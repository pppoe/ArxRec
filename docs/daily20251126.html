<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251125.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FastGS: Training 3D Gaussian Splatting in 100 Seconds", "author": "Shiwei Ren and Tianci Wen and Yongchun Fang and Biao Lu", "abstract": "The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/", "link": "http://arxiv.org/abs/2511.04283v2", "date": "2025-11-25", "relevancy": 3.5177, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7319}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6918}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastGS%3A%20Training%203D%20Gaussian%20Splatting%20in%20100%20Seconds&body=Title%3A%20FastGS%3A%20Training%203D%20Gaussian%20Splatting%20in%20100%20Seconds%0AAuthor%3A%20Shiwei%20Ren%20and%20Tianci%20Wen%20and%20Yongchun%20Fang%20and%20Biao%20Lu%0AAbstract%3A%20The%20dominant%203D%20Gaussian%20splatting%20%283DGS%29%20acceleration%20methods%20fail%20to%20properly%20regulate%20the%20number%20of%20Gaussians%20during%20training%2C%20causing%20redundant%20computational%20time%20overhead.%20In%20this%20paper%2C%20we%20propose%20FastGS%2C%20a%20novel%2C%20simple%2C%20and%20general%20acceleration%20framework%20that%20fully%20considers%20the%20importance%20of%20each%20Gaussian%20based%20on%20multi-view%20consistency%2C%20efficiently%20solving%20the%20trade-off%20between%20training%20time%20and%20rendering%20quality.%20We%20innovatively%20design%20a%20densification%20and%20pruning%20strategy%20based%20on%20multi-view%20consistency%2C%20dispensing%20with%20the%20budgeting%20mechanism.%20Extensive%20experiments%20on%20Mip-NeRF%20360%2C%20Tanks%20%26%20Temples%2C%20and%20Deep%20Blending%20datasets%20demonstrate%20that%20our%20method%20significantly%20outperforms%20the%20state-of-the-art%20methods%20in%20training%20speed%2C%20achieving%20a%203.32%24%5Ctimes%24%20training%20acceleration%20and%20comparable%20rendering%20quality%20compared%20with%20DashGaussian%20on%20the%20Mip-NeRF%20360%20dataset%20and%20a%2015.45%24%5Ctimes%24%20acceleration%20compared%20with%20vanilla%203DGS%20on%20the%20Deep%20Blending%20dataset.%20We%20demonstrate%20that%20FastGS%20exhibits%20strong%20generality%2C%20delivering%202-7%24%5Ctimes%24%20training%20acceleration%20across%20various%20tasks%2C%20including%20dynamic%20scene%20reconstruction%2C%20surface%20reconstruction%2C%20sparse-view%20reconstruction%2C%20large-scale%20reconstruction%2C%20and%20simultaneous%20localization%20and%20mapping.%20The%20project%20page%20is%20available%20at%20https%3A//fastgs.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2511.04283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastGS%253A%2520Training%25203D%2520Gaussian%2520Splatting%2520in%2520100%2520Seconds%26entry.906535625%3DShiwei%2520Ren%2520and%2520Tianci%2520Wen%2520and%2520Yongchun%2520Fang%2520and%2520Biao%2520Lu%26entry.1292438233%3DThe%2520dominant%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520acceleration%2520methods%2520fail%2520to%2520properly%2520regulate%2520the%2520number%2520of%2520Gaussians%2520during%2520training%252C%2520causing%2520redundant%2520computational%2520time%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FastGS%252C%2520a%2520novel%252C%2520simple%252C%2520and%2520general%2520acceleration%2520framework%2520that%2520fully%2520considers%2520the%2520importance%2520of%2520each%2520Gaussian%2520based%2520on%2520multi-view%2520consistency%252C%2520efficiently%2520solving%2520the%2520trade-off%2520between%2520training%2520time%2520and%2520rendering%2520quality.%2520We%2520innovatively%2520design%2520a%2520densification%2520and%2520pruning%2520strategy%2520based%2520on%2520multi-view%2520consistency%252C%2520dispensing%2520with%2520the%2520budgeting%2520mechanism.%2520Extensive%2520experiments%2520on%2520Mip-NeRF%2520360%252C%2520Tanks%2520%2526%2520Temples%252C%2520and%2520Deep%2520Blending%2520datasets%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520the%2520state-of-the-art%2520methods%2520in%2520training%2520speed%252C%2520achieving%2520a%25203.32%2524%255Ctimes%2524%2520training%2520acceleration%2520and%2520comparable%2520rendering%2520quality%2520compared%2520with%2520DashGaussian%2520on%2520the%2520Mip-NeRF%2520360%2520dataset%2520and%2520a%252015.45%2524%255Ctimes%2524%2520acceleration%2520compared%2520with%2520vanilla%25203DGS%2520on%2520the%2520Deep%2520Blending%2520dataset.%2520We%2520demonstrate%2520that%2520FastGS%2520exhibits%2520strong%2520generality%252C%2520delivering%25202-7%2524%255Ctimes%2524%2520training%2520acceleration%2520across%2520various%2520tasks%252C%2520including%2520dynamic%2520scene%2520reconstruction%252C%2520surface%2520reconstruction%252C%2520sparse-view%2520reconstruction%252C%2520large-scale%2520reconstruction%252C%2520and%2520simultaneous%2520localization%2520and%2520mapping.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//fastgs.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.04283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastGS%3A%20Training%203D%20Gaussian%20Splatting%20in%20100%20Seconds&entry.906535625=Shiwei%20Ren%20and%20Tianci%20Wen%20and%20Yongchun%20Fang%20and%20Biao%20Lu&entry.1292438233=The%20dominant%203D%20Gaussian%20splatting%20%283DGS%29%20acceleration%20methods%20fail%20to%20properly%20regulate%20the%20number%20of%20Gaussians%20during%20training%2C%20causing%20redundant%20computational%20time%20overhead.%20In%20this%20paper%2C%20we%20propose%20FastGS%2C%20a%20novel%2C%20simple%2C%20and%20general%20acceleration%20framework%20that%20fully%20considers%20the%20importance%20of%20each%20Gaussian%20based%20on%20multi-view%20consistency%2C%20efficiently%20solving%20the%20trade-off%20between%20training%20time%20and%20rendering%20quality.%20We%20innovatively%20design%20a%20densification%20and%20pruning%20strategy%20based%20on%20multi-view%20consistency%2C%20dispensing%20with%20the%20budgeting%20mechanism.%20Extensive%20experiments%20on%20Mip-NeRF%20360%2C%20Tanks%20%26%20Temples%2C%20and%20Deep%20Blending%20datasets%20demonstrate%20that%20our%20method%20significantly%20outperforms%20the%20state-of-the-art%20methods%20in%20training%20speed%2C%20achieving%20a%203.32%24%5Ctimes%24%20training%20acceleration%20and%20comparable%20rendering%20quality%20compared%20with%20DashGaussian%20on%20the%20Mip-NeRF%20360%20dataset%20and%20a%2015.45%24%5Ctimes%24%20acceleration%20compared%20with%20vanilla%203DGS%20on%20the%20Deep%20Blending%20dataset.%20We%20demonstrate%20that%20FastGS%20exhibits%20strong%20generality%2C%20delivering%202-7%24%5Ctimes%24%20training%20acceleration%20across%20various%20tasks%2C%20including%20dynamic%20scene%20reconstruction%2C%20surface%20reconstruction%2C%20sparse-view%20reconstruction%2C%20large-scale%20reconstruction%2C%20and%20simultaneous%20localization%20and%20mapping.%20The%20project%20page%20is%20available%20at%20https%3A//fastgs.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2511.04283v2&entry.124074799=Read"},
{"title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight", "author": "Yunze Man and Shihao Wang and Guowen Zhang and Johan Bjorck and Zhiqi Li and Liang-Yan Gui and Jim Fan and Jan Kautz and Yu-Xiong Wang and Zhiding Yu", "abstract": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.", "link": "http://arxiv.org/abs/2511.20648v1", "date": "2025-11-25", "relevancy": 3.328, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LocateAnything3D%3A%20Vision-Language%203D%20Detection%20with%20Chain-of-Sight&body=Title%3A%20LocateAnything3D%3A%20Vision-Language%203D%20Detection%20with%20Chain-of-Sight%0AAuthor%3A%20Yunze%20Man%20and%20Shihao%20Wang%20and%20Guowen%20Zhang%20and%20Johan%20Bjorck%20and%20Zhiqi%20Li%20and%20Liang-Yan%20Gui%20and%20Jim%20Fan%20and%20Jan%20Kautz%20and%20Yu-Xiong%20Wang%20and%20Zhiding%20Yu%0AAbstract%3A%20To%20act%20in%20the%20world%2C%20a%20model%20must%20name%20what%20it%20sees%20and%20know%20where%20it%20is%20in%203D.%20Today%27s%20vision-language%20models%20%28VLMs%29%20excel%20at%20open-ended%202D%20description%20and%20grounding%2C%20yet%20multi-object%203D%20detection%20remains%20largely%20missing%20from%20the%20VLM%20toolbox.%20We%20present%20LocateAnything3D%2C%20a%20VLM-native%20recipe%20that%20casts%203D%20detection%20as%20a%20next-token%20prediction%20problem.%20The%20key%20is%20a%20short%2C%20explicit%20Chain-of-Sight%20%28CoS%29%20sequence%20that%20mirrors%20how%20human%20reason%20from%20images%3A%20find%20an%20object%20in%202D%2C%20then%20infer%20its%20distance%2C%20size%2C%20and%20pose.%20The%20decoder%20first%20emits%202D%20detections%20as%20a%20visual%20chain-of-thought%2C%20then%20predicts%203D%20boxes%20under%20an%20easy-to-hard%20curriculum%3A%20across%20objects%2C%20a%20near-to-far%20order%20reduces%20early%20ambiguity%20and%20matches%20ego-centric%20utility%3B%20within%20each%20object%2C%20a%20center-from-camera%2C%20dimensions%2C%20and%20rotation%20factorization%20ranks%20information%20by%20stability%20and%20learnability.%20This%20VLM-native%20interface%20preserves%20open-vocabulary%20and%20visual-prompting%20capability%20without%20specialized%20heads.%20On%20the%20challenging%20Omni3D%20benchmark%2C%20our%20model%20achieves%20state-of-the-art%20results%2C%20with%2049.89%20AP_3D%2C%20surpassing%20the%20previous%20best%20by%20%2B15.51%20absolute%20improvement%20even%20when%20the%20baseline%20is%20given%20ground-truth%202D%20boxes.%20It%20also%20generalizes%20zero-shot%20to%20held-out%20categories%20with%20strong%20robustness.%20By%20turning%203D%20detection%20into%20a%20disciplined%20next-token%20problem%2C%20LocateAnything3D%20offers%20a%20practical%20foundation%20for%20models%20to%20perceive%20in%203D.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocateAnything3D%253A%2520Vision-Language%25203D%2520Detection%2520with%2520Chain-of-Sight%26entry.906535625%3DYunze%2520Man%2520and%2520Shihao%2520Wang%2520and%2520Guowen%2520Zhang%2520and%2520Johan%2520Bjorck%2520and%2520Zhiqi%2520Li%2520and%2520Liang-Yan%2520Gui%2520and%2520Jim%2520Fan%2520and%2520Jan%2520Kautz%2520and%2520Yu-Xiong%2520Wang%2520and%2520Zhiding%2520Yu%26entry.1292438233%3DTo%2520act%2520in%2520the%2520world%252C%2520a%2520model%2520must%2520name%2520what%2520it%2520sees%2520and%2520know%2520where%2520it%2520is%2520in%25203D.%2520Today%2527s%2520vision-language%2520models%2520%2528VLMs%2529%2520excel%2520at%2520open-ended%25202D%2520description%2520and%2520grounding%252C%2520yet%2520multi-object%25203D%2520detection%2520remains%2520largely%2520missing%2520from%2520the%2520VLM%2520toolbox.%2520We%2520present%2520LocateAnything3D%252C%2520a%2520VLM-native%2520recipe%2520that%2520casts%25203D%2520detection%2520as%2520a%2520next-token%2520prediction%2520problem.%2520The%2520key%2520is%2520a%2520short%252C%2520explicit%2520Chain-of-Sight%2520%2528CoS%2529%2520sequence%2520that%2520mirrors%2520how%2520human%2520reason%2520from%2520images%253A%2520find%2520an%2520object%2520in%25202D%252C%2520then%2520infer%2520its%2520distance%252C%2520size%252C%2520and%2520pose.%2520The%2520decoder%2520first%2520emits%25202D%2520detections%2520as%2520a%2520visual%2520chain-of-thought%252C%2520then%2520predicts%25203D%2520boxes%2520under%2520an%2520easy-to-hard%2520curriculum%253A%2520across%2520objects%252C%2520a%2520near-to-far%2520order%2520reduces%2520early%2520ambiguity%2520and%2520matches%2520ego-centric%2520utility%253B%2520within%2520each%2520object%252C%2520a%2520center-from-camera%252C%2520dimensions%252C%2520and%2520rotation%2520factorization%2520ranks%2520information%2520by%2520stability%2520and%2520learnability.%2520This%2520VLM-native%2520interface%2520preserves%2520open-vocabulary%2520and%2520visual-prompting%2520capability%2520without%2520specialized%2520heads.%2520On%2520the%2520challenging%2520Omni3D%2520benchmark%252C%2520our%2520model%2520achieves%2520state-of-the-art%2520results%252C%2520with%252049.89%2520AP_3D%252C%2520surpassing%2520the%2520previous%2520best%2520by%2520%252B15.51%2520absolute%2520improvement%2520even%2520when%2520the%2520baseline%2520is%2520given%2520ground-truth%25202D%2520boxes.%2520It%2520also%2520generalizes%2520zero-shot%2520to%2520held-out%2520categories%2520with%2520strong%2520robustness.%2520By%2520turning%25203D%2520detection%2520into%2520a%2520disciplined%2520next-token%2520problem%252C%2520LocateAnything3D%2520offers%2520a%2520practical%2520foundation%2520for%2520models%2520to%2520perceive%2520in%25203D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocateAnything3D%3A%20Vision-Language%203D%20Detection%20with%20Chain-of-Sight&entry.906535625=Yunze%20Man%20and%20Shihao%20Wang%20and%20Guowen%20Zhang%20and%20Johan%20Bjorck%20and%20Zhiqi%20Li%20and%20Liang-Yan%20Gui%20and%20Jim%20Fan%20and%20Jan%20Kautz%20and%20Yu-Xiong%20Wang%20and%20Zhiding%20Yu&entry.1292438233=To%20act%20in%20the%20world%2C%20a%20model%20must%20name%20what%20it%20sees%20and%20know%20where%20it%20is%20in%203D.%20Today%27s%20vision-language%20models%20%28VLMs%29%20excel%20at%20open-ended%202D%20description%20and%20grounding%2C%20yet%20multi-object%203D%20detection%20remains%20largely%20missing%20from%20the%20VLM%20toolbox.%20We%20present%20LocateAnything3D%2C%20a%20VLM-native%20recipe%20that%20casts%203D%20detection%20as%20a%20next-token%20prediction%20problem.%20The%20key%20is%20a%20short%2C%20explicit%20Chain-of-Sight%20%28CoS%29%20sequence%20that%20mirrors%20how%20human%20reason%20from%20images%3A%20find%20an%20object%20in%202D%2C%20then%20infer%20its%20distance%2C%20size%2C%20and%20pose.%20The%20decoder%20first%20emits%202D%20detections%20as%20a%20visual%20chain-of-thought%2C%20then%20predicts%203D%20boxes%20under%20an%20easy-to-hard%20curriculum%3A%20across%20objects%2C%20a%20near-to-far%20order%20reduces%20early%20ambiguity%20and%20matches%20ego-centric%20utility%3B%20within%20each%20object%2C%20a%20center-from-camera%2C%20dimensions%2C%20and%20rotation%20factorization%20ranks%20information%20by%20stability%20and%20learnability.%20This%20VLM-native%20interface%20preserves%20open-vocabulary%20and%20visual-prompting%20capability%20without%20specialized%20heads.%20On%20the%20challenging%20Omni3D%20benchmark%2C%20our%20model%20achieves%20state-of-the-art%20results%2C%20with%2049.89%20AP_3D%2C%20surpassing%20the%20previous%20best%20by%20%2B15.51%20absolute%20improvement%20even%20when%20the%20baseline%20is%20given%20ground-truth%202D%20boxes.%20It%20also%20generalizes%20zero-shot%20to%20held-out%20categories%20with%20strong%20robustness.%20By%20turning%203D%20detection%20into%20a%20disciplined%20next-token%20problem%2C%20LocateAnything3D%20offers%20a%20practical%20foundation%20for%20models%20to%20perceive%20in%203D.&entry.1838667208=http%3A//arxiv.org/abs/2511.20648v1&entry.124074799=Read"},
{"title": "Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin", "author": "Jo\u00e3o Malheiro Silva and Andy Huynh and Tong Duy Son and Holger Caesar", "abstract": "3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.", "link": "http://arxiv.org/abs/2511.20348v1", "date": "2025-11-25", "relevancy": 3.2814, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6947}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.642}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Material-informed%20Gaussian%20Splatting%20for%203D%20World%20Reconstruction%20in%20a%20Digital%20Twin&body=Title%3A%20Material-informed%20Gaussian%20Splatting%20for%203D%20World%20Reconstruction%20in%20a%20Digital%20Twin%0AAuthor%3A%20Jo%C3%A3o%20Malheiro%20Silva%20and%20Andy%20Huynh%20and%20Tong%20Duy%20Son%20and%20Holger%20Caesar%0AAbstract%3A%203D%20reconstruction%20for%20Digital%20Twins%20often%20relies%20on%20LiDAR-based%20methods%2C%20which%20provide%20accurate%20geometry%20but%20lack%20the%20semantics%20and%20textures%20naturally%20captured%20by%20cameras.%20Traditional%20LiDAR-camera%20fusion%20approaches%20require%20complex%20calibration%20and%20still%20struggle%20with%20certain%20materials%20like%20glass%2C%20which%20are%20visible%20in%20images%20but%20poorly%20represented%20in%20point%20clouds.%20We%20propose%20a%20camera-only%20pipeline%20that%20reconstructs%20scenes%20using%203D%20Gaussian%20Splatting%20from%20multi-view%20images%2C%20extracts%20semantic%20material%20masks%20via%20vision%20models%2C%20converts%20Gaussian%20representations%20to%20mesh%20surfaces%20with%20projected%20material%20labels%2C%20and%20assigns%20physics-based%20material%20properties%20for%20accurate%20sensor%20simulation%20in%20modern%20graphics%20engines%20and%20simulators.%20This%20approach%20combines%20photorealistic%20reconstruction%20with%20physics-based%20material%20assignment%2C%20providing%20sensor%20simulation%20fidelity%20comparable%20to%20LiDAR-camera%20fusion%20while%20eliminating%20hardware%20complexity%20and%20calibration%20requirements.%20We%20validate%20our%20camera-only%20method%20using%20an%20internal%20dataset%20from%20an%20instrumented%20test%20vehicle%2C%20leveraging%20LiDAR%20as%20ground%20truth%20for%20reflectivity%20validation%20alongside%20image%20similarity%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaterial-informed%2520Gaussian%2520Splatting%2520for%25203D%2520World%2520Reconstruction%2520in%2520a%2520Digital%2520Twin%26entry.906535625%3DJo%25C3%25A3o%2520Malheiro%2520Silva%2520and%2520Andy%2520Huynh%2520and%2520Tong%2520Duy%2520Son%2520and%2520Holger%2520Caesar%26entry.1292438233%3D3D%2520reconstruction%2520for%2520Digital%2520Twins%2520often%2520relies%2520on%2520LiDAR-based%2520methods%252C%2520which%2520provide%2520accurate%2520geometry%2520but%2520lack%2520the%2520semantics%2520and%2520textures%2520naturally%2520captured%2520by%2520cameras.%2520Traditional%2520LiDAR-camera%2520fusion%2520approaches%2520require%2520complex%2520calibration%2520and%2520still%2520struggle%2520with%2520certain%2520materials%2520like%2520glass%252C%2520which%2520are%2520visible%2520in%2520images%2520but%2520poorly%2520represented%2520in%2520point%2520clouds.%2520We%2520propose%2520a%2520camera-only%2520pipeline%2520that%2520reconstructs%2520scenes%2520using%25203D%2520Gaussian%2520Splatting%2520from%2520multi-view%2520images%252C%2520extracts%2520semantic%2520material%2520masks%2520via%2520vision%2520models%252C%2520converts%2520Gaussian%2520representations%2520to%2520mesh%2520surfaces%2520with%2520projected%2520material%2520labels%252C%2520and%2520assigns%2520physics-based%2520material%2520properties%2520for%2520accurate%2520sensor%2520simulation%2520in%2520modern%2520graphics%2520engines%2520and%2520simulators.%2520This%2520approach%2520combines%2520photorealistic%2520reconstruction%2520with%2520physics-based%2520material%2520assignment%252C%2520providing%2520sensor%2520simulation%2520fidelity%2520comparable%2520to%2520LiDAR-camera%2520fusion%2520while%2520eliminating%2520hardware%2520complexity%2520and%2520calibration%2520requirements.%2520We%2520validate%2520our%2520camera-only%2520method%2520using%2520an%2520internal%2520dataset%2520from%2520an%2520instrumented%2520test%2520vehicle%252C%2520leveraging%2520LiDAR%2520as%2520ground%2520truth%2520for%2520reflectivity%2520validation%2520alongside%2520image%2520similarity%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Material-informed%20Gaussian%20Splatting%20for%203D%20World%20Reconstruction%20in%20a%20Digital%20Twin&entry.906535625=Jo%C3%A3o%20Malheiro%20Silva%20and%20Andy%20Huynh%20and%20Tong%20Duy%20Son%20and%20Holger%20Caesar&entry.1292438233=3D%20reconstruction%20for%20Digital%20Twins%20often%20relies%20on%20LiDAR-based%20methods%2C%20which%20provide%20accurate%20geometry%20but%20lack%20the%20semantics%20and%20textures%20naturally%20captured%20by%20cameras.%20Traditional%20LiDAR-camera%20fusion%20approaches%20require%20complex%20calibration%20and%20still%20struggle%20with%20certain%20materials%20like%20glass%2C%20which%20are%20visible%20in%20images%20but%20poorly%20represented%20in%20point%20clouds.%20We%20propose%20a%20camera-only%20pipeline%20that%20reconstructs%20scenes%20using%203D%20Gaussian%20Splatting%20from%20multi-view%20images%2C%20extracts%20semantic%20material%20masks%20via%20vision%20models%2C%20converts%20Gaussian%20representations%20to%20mesh%20surfaces%20with%20projected%20material%20labels%2C%20and%20assigns%20physics-based%20material%20properties%20for%20accurate%20sensor%20simulation%20in%20modern%20graphics%20engines%20and%20simulators.%20This%20approach%20combines%20photorealistic%20reconstruction%20with%20physics-based%20material%20assignment%2C%20providing%20sensor%20simulation%20fidelity%20comparable%20to%20LiDAR-camera%20fusion%20while%20eliminating%20hardware%20complexity%20and%20calibration%20requirements.%20We%20validate%20our%20camera-only%20method%20using%20an%20internal%20dataset%20from%20an%20instrumented%20test%20vehicle%2C%20leveraging%20LiDAR%20as%20ground%20truth%20for%20reflectivity%20validation%20alongside%20image%20similarity%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.20348v1&entry.124074799=Read"},
{"title": "ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis", "author": "Advik Sinha and Saurabh Atreya and Aashutosh A and Sk Aziz Ali and Abhijit Das", "abstract": "Until recently, the general corpus of CLIP-type fundamental models has widely explored either the retrieval of short descriptions or the classification of objects in the scene as SINGLE-object image classification task. The same holds for retrieving the image embedding (image retrieval task) given a text prompt. However, real-world scene images exhibit rich compositional structure involving multiple objects and actions. The latest methods in the CLIP-based literature improve class-level discrimination by mining harder negative image-text pairs and by refining permanent text prompts, often using LLMs. However, these improvements remain confined to predefined class lists and do not explicitly model relational or compositional structure. PyramidCLIP partially addresses this gap by aligning global and local visual features, yet it still lacks explicit modeling of inter-object relations. Hence, to further leverage this aspect for scene analysis, the proposed ScenarioCLIP model accepts input texts, grounded relations, and input images, along with focused regions highlighting relations. The proposed model is pretrained on curated scenario data, and finetuned for specialized downstream tasks, such as cross-modal retrieval and fine-grained visual understanding tasks. To address the lack of domain-specific datasets, we generate a novel dataset by extending image-text pairs from existing diverse indoor and outdoor scenario datasets that are publicly available. We used a pipeline of existing language models to ground action, object, and relations, filled by manual and automatic curation. We established a comprehensive benchmark for several scenario-based tasks and compared it with many baseline methods. ScenarioCLIP demonstrates robust zero-shot and finetune performance on various domain-specific tasks. Our code and dataset are available at https://github.com/scenario-clip/ScenarioCLIP", "link": "http://arxiv.org/abs/2511.20274v1", "date": "2025-11-25", "relevancy": 3.2352, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.656}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScenarioCLIP%3A%20Pretrained%20Transferable%20Visual%20Language%20Models%20and%20Action-Genome%20Dataset%20for%20Natural%20Scene%20Analysis&body=Title%3A%20ScenarioCLIP%3A%20Pretrained%20Transferable%20Visual%20Language%20Models%20and%20Action-Genome%20Dataset%20for%20Natural%20Scene%20Analysis%0AAuthor%3A%20Advik%20Sinha%20and%20Saurabh%20Atreya%20and%20Aashutosh%20A%20and%20Sk%20Aziz%20Ali%20and%20Abhijit%20Das%0AAbstract%3A%20Until%20recently%2C%20the%20general%20corpus%20of%20CLIP-type%20fundamental%20models%20has%20widely%20explored%20either%20the%20retrieval%20of%20short%20descriptions%20or%20the%20classification%20of%20objects%20in%20the%20scene%20as%20SINGLE-object%20image%20classification%20task.%20The%20same%20holds%20for%20retrieving%20the%20image%20embedding%20%28image%20retrieval%20task%29%20given%20a%20text%20prompt.%20However%2C%20real-world%20scene%20images%20exhibit%20rich%20compositional%20structure%20involving%20multiple%20objects%20and%20actions.%20The%20latest%20methods%20in%20the%20CLIP-based%20literature%20improve%20class-level%20discrimination%20by%20mining%20harder%20negative%20image-text%20pairs%20and%20by%20refining%20permanent%20text%20prompts%2C%20often%20using%20LLMs.%20However%2C%20these%20improvements%20remain%20confined%20to%20predefined%20class%20lists%20and%20do%20not%20explicitly%20model%20relational%20or%20compositional%20structure.%20PyramidCLIP%20partially%20addresses%20this%20gap%20by%20aligning%20global%20and%20local%20visual%20features%2C%20yet%20it%20still%20lacks%20explicit%20modeling%20of%20inter-object%20relations.%20Hence%2C%20to%20further%20leverage%20this%20aspect%20for%20scene%20analysis%2C%20the%20proposed%20ScenarioCLIP%20model%20accepts%20input%20texts%2C%20grounded%20relations%2C%20and%20input%20images%2C%20along%20with%20focused%20regions%20highlighting%20relations.%20The%20proposed%20model%20is%20pretrained%20on%20curated%20scenario%20data%2C%20and%20finetuned%20for%20specialized%20downstream%20tasks%2C%20such%20as%20cross-modal%20retrieval%20and%20fine-grained%20visual%20understanding%20tasks.%20To%20address%20the%20lack%20of%20domain-specific%20datasets%2C%20we%20generate%20a%20novel%20dataset%20by%20extending%20image-text%20pairs%20from%20existing%20diverse%20indoor%20and%20outdoor%20scenario%20datasets%20that%20are%20publicly%20available.%20We%20used%20a%20pipeline%20of%20existing%20language%20models%20to%20ground%20action%2C%20object%2C%20and%20relations%2C%20filled%20by%20manual%20and%20automatic%20curation.%20We%20established%20a%20comprehensive%20benchmark%20for%20several%20scenario-based%20tasks%20and%20compared%20it%20with%20many%20baseline%20methods.%20ScenarioCLIP%20demonstrates%20robust%20zero-shot%20and%20finetune%20performance%20on%20various%20domain-specific%20tasks.%20Our%20code%20and%20dataset%20are%20available%20at%20https%3A//github.com/scenario-clip/ScenarioCLIP%0ALink%3A%20http%3A//arxiv.org/abs/2511.20274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScenarioCLIP%253A%2520Pretrained%2520Transferable%2520Visual%2520Language%2520Models%2520and%2520Action-Genome%2520Dataset%2520for%2520Natural%2520Scene%2520Analysis%26entry.906535625%3DAdvik%2520Sinha%2520and%2520Saurabh%2520Atreya%2520and%2520Aashutosh%2520A%2520and%2520Sk%2520Aziz%2520Ali%2520and%2520Abhijit%2520Das%26entry.1292438233%3DUntil%2520recently%252C%2520the%2520general%2520corpus%2520of%2520CLIP-type%2520fundamental%2520models%2520has%2520widely%2520explored%2520either%2520the%2520retrieval%2520of%2520short%2520descriptions%2520or%2520the%2520classification%2520of%2520objects%2520in%2520the%2520scene%2520as%2520SINGLE-object%2520image%2520classification%2520task.%2520The%2520same%2520holds%2520for%2520retrieving%2520the%2520image%2520embedding%2520%2528image%2520retrieval%2520task%2529%2520given%2520a%2520text%2520prompt.%2520However%252C%2520real-world%2520scene%2520images%2520exhibit%2520rich%2520compositional%2520structure%2520involving%2520multiple%2520objects%2520and%2520actions.%2520The%2520latest%2520methods%2520in%2520the%2520CLIP-based%2520literature%2520improve%2520class-level%2520discrimination%2520by%2520mining%2520harder%2520negative%2520image-text%2520pairs%2520and%2520by%2520refining%2520permanent%2520text%2520prompts%252C%2520often%2520using%2520LLMs.%2520However%252C%2520these%2520improvements%2520remain%2520confined%2520to%2520predefined%2520class%2520lists%2520and%2520do%2520not%2520explicitly%2520model%2520relational%2520or%2520compositional%2520structure.%2520PyramidCLIP%2520partially%2520addresses%2520this%2520gap%2520by%2520aligning%2520global%2520and%2520local%2520visual%2520features%252C%2520yet%2520it%2520still%2520lacks%2520explicit%2520modeling%2520of%2520inter-object%2520relations.%2520Hence%252C%2520to%2520further%2520leverage%2520this%2520aspect%2520for%2520scene%2520analysis%252C%2520the%2520proposed%2520ScenarioCLIP%2520model%2520accepts%2520input%2520texts%252C%2520grounded%2520relations%252C%2520and%2520input%2520images%252C%2520along%2520with%2520focused%2520regions%2520highlighting%2520relations.%2520The%2520proposed%2520model%2520is%2520pretrained%2520on%2520curated%2520scenario%2520data%252C%2520and%2520finetuned%2520for%2520specialized%2520downstream%2520tasks%252C%2520such%2520as%2520cross-modal%2520retrieval%2520and%2520fine-grained%2520visual%2520understanding%2520tasks.%2520To%2520address%2520the%2520lack%2520of%2520domain-specific%2520datasets%252C%2520we%2520generate%2520a%2520novel%2520dataset%2520by%2520extending%2520image-text%2520pairs%2520from%2520existing%2520diverse%2520indoor%2520and%2520outdoor%2520scenario%2520datasets%2520that%2520are%2520publicly%2520available.%2520We%2520used%2520a%2520pipeline%2520of%2520existing%2520language%2520models%2520to%2520ground%2520action%252C%2520object%252C%2520and%2520relations%252C%2520filled%2520by%2520manual%2520and%2520automatic%2520curation.%2520We%2520established%2520a%2520comprehensive%2520benchmark%2520for%2520several%2520scenario-based%2520tasks%2520and%2520compared%2520it%2520with%2520many%2520baseline%2520methods.%2520ScenarioCLIP%2520demonstrates%2520robust%2520zero-shot%2520and%2520finetune%2520performance%2520on%2520various%2520domain-specific%2520tasks.%2520Our%2520code%2520and%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/scenario-clip/ScenarioCLIP%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScenarioCLIP%3A%20Pretrained%20Transferable%20Visual%20Language%20Models%20and%20Action-Genome%20Dataset%20for%20Natural%20Scene%20Analysis&entry.906535625=Advik%20Sinha%20and%20Saurabh%20Atreya%20and%20Aashutosh%20A%20and%20Sk%20Aziz%20Ali%20and%20Abhijit%20Das&entry.1292438233=Until%20recently%2C%20the%20general%20corpus%20of%20CLIP-type%20fundamental%20models%20has%20widely%20explored%20either%20the%20retrieval%20of%20short%20descriptions%20or%20the%20classification%20of%20objects%20in%20the%20scene%20as%20SINGLE-object%20image%20classification%20task.%20The%20same%20holds%20for%20retrieving%20the%20image%20embedding%20%28image%20retrieval%20task%29%20given%20a%20text%20prompt.%20However%2C%20real-world%20scene%20images%20exhibit%20rich%20compositional%20structure%20involving%20multiple%20objects%20and%20actions.%20The%20latest%20methods%20in%20the%20CLIP-based%20literature%20improve%20class-level%20discrimination%20by%20mining%20harder%20negative%20image-text%20pairs%20and%20by%20refining%20permanent%20text%20prompts%2C%20often%20using%20LLMs.%20However%2C%20these%20improvements%20remain%20confined%20to%20predefined%20class%20lists%20and%20do%20not%20explicitly%20model%20relational%20or%20compositional%20structure.%20PyramidCLIP%20partially%20addresses%20this%20gap%20by%20aligning%20global%20and%20local%20visual%20features%2C%20yet%20it%20still%20lacks%20explicit%20modeling%20of%20inter-object%20relations.%20Hence%2C%20to%20further%20leverage%20this%20aspect%20for%20scene%20analysis%2C%20the%20proposed%20ScenarioCLIP%20model%20accepts%20input%20texts%2C%20grounded%20relations%2C%20and%20input%20images%2C%20along%20with%20focused%20regions%20highlighting%20relations.%20The%20proposed%20model%20is%20pretrained%20on%20curated%20scenario%20data%2C%20and%20finetuned%20for%20specialized%20downstream%20tasks%2C%20such%20as%20cross-modal%20retrieval%20and%20fine-grained%20visual%20understanding%20tasks.%20To%20address%20the%20lack%20of%20domain-specific%20datasets%2C%20we%20generate%20a%20novel%20dataset%20by%20extending%20image-text%20pairs%20from%20existing%20diverse%20indoor%20and%20outdoor%20scenario%20datasets%20that%20are%20publicly%20available.%20We%20used%20a%20pipeline%20of%20existing%20language%20models%20to%20ground%20action%2C%20object%2C%20and%20relations%2C%20filled%20by%20manual%20and%20automatic%20curation.%20We%20established%20a%20comprehensive%20benchmark%20for%20several%20scenario-based%20tasks%20and%20compared%20it%20with%20many%20baseline%20methods.%20ScenarioCLIP%20demonstrates%20robust%20zero-shot%20and%20finetune%20performance%20on%20various%20domain-specific%20tasks.%20Our%20code%20and%20dataset%20are%20available%20at%20https%3A//github.com/scenario-clip/ScenarioCLIP&entry.1838667208=http%3A//arxiv.org/abs/2511.20274v1&entry.124074799=Read"},
{"title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation", "author": "Bowen Xue and Zheng-Peng Duan and Qixin Yan and Wenjing Wang and Hao Liu and Chun-Le Guo and Chongyi Li and Chen Li and Jing Lyu", "abstract": "Generating high-fidelity human videos that match user-specified identities is important yet challenging in the field of generative AI. Existing methods often rely on an excessive number of training parameters and lack compatibility with other AIGC tools. In this paper, we propose Stand-In, a lightweight and plug-and-play framework for identity preservation in video generation. Specifically, we introduce a conditional image branch into the pre-trained video generation model. Identity control is achieved through restricted self-attentions with conditional position mapping. Thanks to these designs, which greatly preserve the pre-trained prior of the video generation model, our approach is able to outperform other full-parameter training methods in video quality and identity preservation, even with just $\\sim$1% additional parameters and only 2000 training pairs. Moreover, our framework can be seamlessly integrated for other tasks, such as subject-driven video generation, pose-referenced video generation, stylization, and face swapping.", "link": "http://arxiv.org/abs/2508.07901v3", "date": "2025-11-25", "relevancy": 3.1652, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7137}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5958}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stand-In%3A%20A%20Lightweight%20and%20Plug-and-Play%20Identity%20Control%20for%20Video%20Generation&body=Title%3A%20Stand-In%3A%20A%20Lightweight%20and%20Plug-and-Play%20Identity%20Control%20for%20Video%20Generation%0AAuthor%3A%20Bowen%20Xue%20and%20Zheng-Peng%20Duan%20and%20Qixin%20Yan%20and%20Wenjing%20Wang%20and%20Hao%20Liu%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li%20and%20Chen%20Li%20and%20Jing%20Lyu%0AAbstract%3A%20Generating%20high-fidelity%20human%20videos%20that%20match%20user-specified%20identities%20is%20important%20yet%20challenging%20in%20the%20field%20of%20generative%20AI.%20Existing%20methods%20often%20rely%20on%20an%20excessive%20number%20of%20training%20parameters%20and%20lack%20compatibility%20with%20other%20AIGC%20tools.%20In%20this%20paper%2C%20we%20propose%20Stand-In%2C%20a%20lightweight%20and%20plug-and-play%20framework%20for%20identity%20preservation%20in%20video%20generation.%20Specifically%2C%20we%20introduce%20a%20conditional%20image%20branch%20into%20the%20pre-trained%20video%20generation%20model.%20Identity%20control%20is%20achieved%20through%20restricted%20self-attentions%20with%20conditional%20position%20mapping.%20Thanks%20to%20these%20designs%2C%20which%20greatly%20preserve%20the%20pre-trained%20prior%20of%20the%20video%20generation%20model%2C%20our%20approach%20is%20able%20to%20outperform%20other%20full-parameter%20training%20methods%20in%20video%20quality%20and%20identity%20preservation%2C%20even%20with%20just%20%24%5Csim%241%25%20additional%20parameters%20and%20only%202000%20training%20pairs.%20Moreover%2C%20our%20framework%20can%20be%20seamlessly%20integrated%20for%20other%20tasks%2C%20such%20as%20subject-driven%20video%20generation%2C%20pose-referenced%20video%20generation%2C%20stylization%2C%20and%20face%20swapping.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07901v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStand-In%253A%2520A%2520Lightweight%2520and%2520Plug-and-Play%2520Identity%2520Control%2520for%2520Video%2520Generation%26entry.906535625%3DBowen%2520Xue%2520and%2520Zheng-Peng%2520Duan%2520and%2520Qixin%2520Yan%2520and%2520Wenjing%2520Wang%2520and%2520Hao%2520Liu%2520and%2520Chun-Le%2520Guo%2520and%2520Chongyi%2520Li%2520and%2520Chen%2520Li%2520and%2520Jing%2520Lyu%26entry.1292438233%3DGenerating%2520high-fidelity%2520human%2520videos%2520that%2520match%2520user-specified%2520identities%2520is%2520important%2520yet%2520challenging%2520in%2520the%2520field%2520of%2520generative%2520AI.%2520Existing%2520methods%2520often%2520rely%2520on%2520an%2520excessive%2520number%2520of%2520training%2520parameters%2520and%2520lack%2520compatibility%2520with%2520other%2520AIGC%2520tools.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Stand-In%252C%2520a%2520lightweight%2520and%2520plug-and-play%2520framework%2520for%2520identity%2520preservation%2520in%2520video%2520generation.%2520Specifically%252C%2520we%2520introduce%2520a%2520conditional%2520image%2520branch%2520into%2520the%2520pre-trained%2520video%2520generation%2520model.%2520Identity%2520control%2520is%2520achieved%2520through%2520restricted%2520self-attentions%2520with%2520conditional%2520position%2520mapping.%2520Thanks%2520to%2520these%2520designs%252C%2520which%2520greatly%2520preserve%2520the%2520pre-trained%2520prior%2520of%2520the%2520video%2520generation%2520model%252C%2520our%2520approach%2520is%2520able%2520to%2520outperform%2520other%2520full-parameter%2520training%2520methods%2520in%2520video%2520quality%2520and%2520identity%2520preservation%252C%2520even%2520with%2520just%2520%2524%255Csim%25241%2525%2520additional%2520parameters%2520and%2520only%25202000%2520training%2520pairs.%2520Moreover%252C%2520our%2520framework%2520can%2520be%2520seamlessly%2520integrated%2520for%2520other%2520tasks%252C%2520such%2520as%2520subject-driven%2520video%2520generation%252C%2520pose-referenced%2520video%2520generation%252C%2520stylization%252C%2520and%2520face%2520swapping.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07901v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stand-In%3A%20A%20Lightweight%20and%20Plug-and-Play%20Identity%20Control%20for%20Video%20Generation&entry.906535625=Bowen%20Xue%20and%20Zheng-Peng%20Duan%20and%20Qixin%20Yan%20and%20Wenjing%20Wang%20and%20Hao%20Liu%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li%20and%20Chen%20Li%20and%20Jing%20Lyu&entry.1292438233=Generating%20high-fidelity%20human%20videos%20that%20match%20user-specified%20identities%20is%20important%20yet%20challenging%20in%20the%20field%20of%20generative%20AI.%20Existing%20methods%20often%20rely%20on%20an%20excessive%20number%20of%20training%20parameters%20and%20lack%20compatibility%20with%20other%20AIGC%20tools.%20In%20this%20paper%2C%20we%20propose%20Stand-In%2C%20a%20lightweight%20and%20plug-and-play%20framework%20for%20identity%20preservation%20in%20video%20generation.%20Specifically%2C%20we%20introduce%20a%20conditional%20image%20branch%20into%20the%20pre-trained%20video%20generation%20model.%20Identity%20control%20is%20achieved%20through%20restricted%20self-attentions%20with%20conditional%20position%20mapping.%20Thanks%20to%20these%20designs%2C%20which%20greatly%20preserve%20the%20pre-trained%20prior%20of%20the%20video%20generation%20model%2C%20our%20approach%20is%20able%20to%20outperform%20other%20full-parameter%20training%20methods%20in%20video%20quality%20and%20identity%20preservation%2C%20even%20with%20just%20%24%5Csim%241%25%20additional%20parameters%20and%20only%202000%20training%20pairs.%20Moreover%2C%20our%20framework%20can%20be%20seamlessly%20integrated%20for%20other%20tasks%2C%20such%20as%20subject-driven%20video%20generation%2C%20pose-referenced%20video%20generation%2C%20stylization%2C%20and%20face%20swapping.&entry.1838667208=http%3A//arxiv.org/abs/2508.07901v3&entry.124074799=Read"},
{"title": "Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement", "author": "Yang Liu and Xilin Zhao and Peisong Wen and Siran Dai and Qingming Huang", "abstract": "Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles. To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation. Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality. This method is training-free and plug-and-play, making it readily applicable to a wide range of video generation models. Experiments on the PhyIQ benchmark show that our method improves the Physics-IQ score from 56.31 to 62.38. We hope this work serves as a preliminary exploration of physics-consistent video generation and may offer insights for future research.", "link": "http://arxiv.org/abs/2511.20280v1", "date": "2025-11-25", "relevancy": 3.1409, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7062}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5929}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20Physics-Grounded%20Video%20Generation%20through%20VLM-Guided%20Iterative%20Self-Refinement&body=Title%3A%20Bootstrapping%20Physics-Grounded%20Video%20Generation%20through%20VLM-Guided%20Iterative%20Self-Refinement%0AAuthor%3A%20Yang%20Liu%20and%20Xilin%20Zhao%20and%20Peisong%20Wen%20and%20Siran%20Dai%20and%20Qingming%20Huang%0AAbstract%3A%20Recent%20progress%20in%20video%20generation%20has%20led%20to%20impressive%20visual%20quality%2C%20yet%20current%20models%20still%20struggle%20to%20produce%20results%20that%20align%20with%20real-world%20physical%20principles.%20To%20this%20end%2C%20we%20propose%20an%20iterative%20self-refinement%20framework%20that%20leverages%20large%20language%20models%20and%20vision-language%20models%20to%20provide%20physics-aware%20guidance%20for%20video%20generation.%20Specifically%2C%20we%20introduce%20a%20multimodal%20chain-of-thought%20%28MM-CoT%29%20process%20that%20refines%20prompts%20based%20on%20feedback%20from%20physical%20inconsistencies%2C%20progressively%20enhancing%20generation%20quality.%20This%20method%20is%20training-free%20and%20plug-and-play%2C%20making%20it%20readily%20applicable%20to%20a%20wide%20range%20of%20video%20generation%20models.%20Experiments%20on%20the%20PhyIQ%20benchmark%20show%20that%20our%20method%20improves%20the%20Physics-IQ%20score%20from%2056.31%20to%2062.38.%20We%20hope%20this%20work%20serves%20as%20a%20preliminary%20exploration%20of%20physics-consistent%20video%20generation%20and%20may%20offer%20insights%20for%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520Physics-Grounded%2520Video%2520Generation%2520through%2520VLM-Guided%2520Iterative%2520Self-Refinement%26entry.906535625%3DYang%2520Liu%2520and%2520Xilin%2520Zhao%2520and%2520Peisong%2520Wen%2520and%2520Siran%2520Dai%2520and%2520Qingming%2520Huang%26entry.1292438233%3DRecent%2520progress%2520in%2520video%2520generation%2520has%2520led%2520to%2520impressive%2520visual%2520quality%252C%2520yet%2520current%2520models%2520still%2520struggle%2520to%2520produce%2520results%2520that%2520align%2520with%2520real-world%2520physical%2520principles.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520iterative%2520self-refinement%2520framework%2520that%2520leverages%2520large%2520language%2520models%2520and%2520vision-language%2520models%2520to%2520provide%2520physics-aware%2520guidance%2520for%2520video%2520generation.%2520Specifically%252C%2520we%2520introduce%2520a%2520multimodal%2520chain-of-thought%2520%2528MM-CoT%2529%2520process%2520that%2520refines%2520prompts%2520based%2520on%2520feedback%2520from%2520physical%2520inconsistencies%252C%2520progressively%2520enhancing%2520generation%2520quality.%2520This%2520method%2520is%2520training-free%2520and%2520plug-and-play%252C%2520making%2520it%2520readily%2520applicable%2520to%2520a%2520wide%2520range%2520of%2520video%2520generation%2520models.%2520Experiments%2520on%2520the%2520PhyIQ%2520benchmark%2520show%2520that%2520our%2520method%2520improves%2520the%2520Physics-IQ%2520score%2520from%252056.31%2520to%252062.38.%2520We%2520hope%2520this%2520work%2520serves%2520as%2520a%2520preliminary%2520exploration%2520of%2520physics-consistent%2520video%2520generation%2520and%2520may%2520offer%2520insights%2520for%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20Physics-Grounded%20Video%20Generation%20through%20VLM-Guided%20Iterative%20Self-Refinement&entry.906535625=Yang%20Liu%20and%20Xilin%20Zhao%20and%20Peisong%20Wen%20and%20Siran%20Dai%20and%20Qingming%20Huang&entry.1292438233=Recent%20progress%20in%20video%20generation%20has%20led%20to%20impressive%20visual%20quality%2C%20yet%20current%20models%20still%20struggle%20to%20produce%20results%20that%20align%20with%20real-world%20physical%20principles.%20To%20this%20end%2C%20we%20propose%20an%20iterative%20self-refinement%20framework%20that%20leverages%20large%20language%20models%20and%20vision-language%20models%20to%20provide%20physics-aware%20guidance%20for%20video%20generation.%20Specifically%2C%20we%20introduce%20a%20multimodal%20chain-of-thought%20%28MM-CoT%29%20process%20that%20refines%20prompts%20based%20on%20feedback%20from%20physical%20inconsistencies%2C%20progressively%20enhancing%20generation%20quality.%20This%20method%20is%20training-free%20and%20plug-and-play%2C%20making%20it%20readily%20applicable%20to%20a%20wide%20range%20of%20video%20generation%20models.%20Experiments%20on%20the%20PhyIQ%20benchmark%20show%20that%20our%20method%20improves%20the%20Physics-IQ%20score%20from%2056.31%20to%2062.38.%20We%20hope%20this%20work%20serves%20as%20a%20preliminary%20exploration%20of%20physics-consistent%20video%20generation%20and%20may%20offer%20insights%20for%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.20280v1&entry.124074799=Read"},
{"title": "VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild", "author": "Xin Ming and Yuxuan Han and Tianyu Huang and Feng Xu", "abstract": "Reconstructing topologically consistent facial geometry is crucial for the digital avatar creation pipelines. Existing methods either require tedious manual efforts, lack generalization to in-the-wild data, or are constrained by the limited expressiveness of 3D Morphable Models. To address these limitations, we propose VGGTFace, an automatic approach that innovatively applies the 3D foundation model, \\emph{i.e.} VGGT, for topologically consistent facial geometry reconstruction from in-the-wild multi-view images captured by everyday users. Our key insight is that, by leveraging VGGT, our method naturally inherits strong generalization ability and expressive power from its large-scale training and point map representation. However, it is unclear how to reconstruct a topologically consistent mesh from VGGT, as the topology information is missing in its prediction. To this end, we augment VGGT with Pixel3DMM for injecting topology information via pixel-aligned UV values. In this manner, we convert the pixel-aligned point map of VGGT to a point cloud with topology. Tailored to this point cloud with known topology, we propose a novel Topology-Aware Bundle Adjustment strategy to fuse them, where we construct a Laplacian energy for the Bundle Adjustment objective. Our method achieves high-quality reconstruction in 10 seconds for 16 views on a single NVIDIA RTX 4090. Experiments demonstrate state-of-the-art results on benchmarks and impressive generalization to in-the-wild data. Code is available at https://github.com/grignarder/vggtface.", "link": "http://arxiv.org/abs/2511.20366v1", "date": "2025-11-25", "relevancy": 3.1252, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6499}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6284}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGGTFace%3A%20Topologically%20Consistent%20Facial%20Geometry%20Reconstruction%20in%20the%20Wild&body=Title%3A%20VGGTFace%3A%20Topologically%20Consistent%20Facial%20Geometry%20Reconstruction%20in%20the%20Wild%0AAuthor%3A%20Xin%20Ming%20and%20Yuxuan%20Han%20and%20Tianyu%20Huang%20and%20Feng%20Xu%0AAbstract%3A%20Reconstructing%20topologically%20consistent%20facial%20geometry%20is%20crucial%20for%20the%20digital%20avatar%20creation%20pipelines.%20Existing%20methods%20either%20require%20tedious%20manual%20efforts%2C%20lack%20generalization%20to%20in-the-wild%20data%2C%20or%20are%20constrained%20by%20the%20limited%20expressiveness%20of%203D%20Morphable%20Models.%20To%20address%20these%20limitations%2C%20we%20propose%20VGGTFace%2C%20an%20automatic%20approach%20that%20innovatively%20applies%20the%203D%20foundation%20model%2C%20%5Cemph%7Bi.e.%7D%20VGGT%2C%20for%20topologically%20consistent%20facial%20geometry%20reconstruction%20from%20in-the-wild%20multi-view%20images%20captured%20by%20everyday%20users.%20Our%20key%20insight%20is%20that%2C%20by%20leveraging%20VGGT%2C%20our%20method%20naturally%20inherits%20strong%20generalization%20ability%20and%20expressive%20power%20from%20its%20large-scale%20training%20and%20point%20map%20representation.%20However%2C%20it%20is%20unclear%20how%20to%20reconstruct%20a%20topologically%20consistent%20mesh%20from%20VGGT%2C%20as%20the%20topology%20information%20is%20missing%20in%20its%20prediction.%20To%20this%20end%2C%20we%20augment%20VGGT%20with%20Pixel3DMM%20for%20injecting%20topology%20information%20via%20pixel-aligned%20UV%20values.%20In%20this%20manner%2C%20we%20convert%20the%20pixel-aligned%20point%20map%20of%20VGGT%20to%20a%20point%20cloud%20with%20topology.%20Tailored%20to%20this%20point%20cloud%20with%20known%20topology%2C%20we%20propose%20a%20novel%20Topology-Aware%20Bundle%20Adjustment%20strategy%20to%20fuse%20them%2C%20where%20we%20construct%20a%20Laplacian%20energy%20for%20the%20Bundle%20Adjustment%20objective.%20Our%20method%20achieves%20high-quality%20reconstruction%20in%2010%20seconds%20for%2016%20views%20on%20a%20single%20NVIDIA%20RTX%204090.%20Experiments%20demonstrate%20state-of-the-art%20results%20on%20benchmarks%20and%20impressive%20generalization%20to%20in-the-wild%20data.%20Code%20is%20available%20at%20https%3A//github.com/grignarder/vggtface.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGGTFace%253A%2520Topologically%2520Consistent%2520Facial%2520Geometry%2520Reconstruction%2520in%2520the%2520Wild%26entry.906535625%3DXin%2520Ming%2520and%2520Yuxuan%2520Han%2520and%2520Tianyu%2520Huang%2520and%2520Feng%2520Xu%26entry.1292438233%3DReconstructing%2520topologically%2520consistent%2520facial%2520geometry%2520is%2520crucial%2520for%2520the%2520digital%2520avatar%2520creation%2520pipelines.%2520Existing%2520methods%2520either%2520require%2520tedious%2520manual%2520efforts%252C%2520lack%2520generalization%2520to%2520in-the-wild%2520data%252C%2520or%2520are%2520constrained%2520by%2520the%2520limited%2520expressiveness%2520of%25203D%2520Morphable%2520Models.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520VGGTFace%252C%2520an%2520automatic%2520approach%2520that%2520innovatively%2520applies%2520the%25203D%2520foundation%2520model%252C%2520%255Cemph%257Bi.e.%257D%2520VGGT%252C%2520for%2520topologically%2520consistent%2520facial%2520geometry%2520reconstruction%2520from%2520in-the-wild%2520multi-view%2520images%2520captured%2520by%2520everyday%2520users.%2520Our%2520key%2520insight%2520is%2520that%252C%2520by%2520leveraging%2520VGGT%252C%2520our%2520method%2520naturally%2520inherits%2520strong%2520generalization%2520ability%2520and%2520expressive%2520power%2520from%2520its%2520large-scale%2520training%2520and%2520point%2520map%2520representation.%2520However%252C%2520it%2520is%2520unclear%2520how%2520to%2520reconstruct%2520a%2520topologically%2520consistent%2520mesh%2520from%2520VGGT%252C%2520as%2520the%2520topology%2520information%2520is%2520missing%2520in%2520its%2520prediction.%2520To%2520this%2520end%252C%2520we%2520augment%2520VGGT%2520with%2520Pixel3DMM%2520for%2520injecting%2520topology%2520information%2520via%2520pixel-aligned%2520UV%2520values.%2520In%2520this%2520manner%252C%2520we%2520convert%2520the%2520pixel-aligned%2520point%2520map%2520of%2520VGGT%2520to%2520a%2520point%2520cloud%2520with%2520topology.%2520Tailored%2520to%2520this%2520point%2520cloud%2520with%2520known%2520topology%252C%2520we%2520propose%2520a%2520novel%2520Topology-Aware%2520Bundle%2520Adjustment%2520strategy%2520to%2520fuse%2520them%252C%2520where%2520we%2520construct%2520a%2520Laplacian%2520energy%2520for%2520the%2520Bundle%2520Adjustment%2520objective.%2520Our%2520method%2520achieves%2520high-quality%2520reconstruction%2520in%252010%2520seconds%2520for%252016%2520views%2520on%2520a%2520single%2520NVIDIA%2520RTX%25204090.%2520Experiments%2520demonstrate%2520state-of-the-art%2520results%2520on%2520benchmarks%2520and%2520impressive%2520generalization%2520to%2520in-the-wild%2520data.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/grignarder/vggtface.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGGTFace%3A%20Topologically%20Consistent%20Facial%20Geometry%20Reconstruction%20in%20the%20Wild&entry.906535625=Xin%20Ming%20and%20Yuxuan%20Han%20and%20Tianyu%20Huang%20and%20Feng%20Xu&entry.1292438233=Reconstructing%20topologically%20consistent%20facial%20geometry%20is%20crucial%20for%20the%20digital%20avatar%20creation%20pipelines.%20Existing%20methods%20either%20require%20tedious%20manual%20efforts%2C%20lack%20generalization%20to%20in-the-wild%20data%2C%20or%20are%20constrained%20by%20the%20limited%20expressiveness%20of%203D%20Morphable%20Models.%20To%20address%20these%20limitations%2C%20we%20propose%20VGGTFace%2C%20an%20automatic%20approach%20that%20innovatively%20applies%20the%203D%20foundation%20model%2C%20%5Cemph%7Bi.e.%7D%20VGGT%2C%20for%20topologically%20consistent%20facial%20geometry%20reconstruction%20from%20in-the-wild%20multi-view%20images%20captured%20by%20everyday%20users.%20Our%20key%20insight%20is%20that%2C%20by%20leveraging%20VGGT%2C%20our%20method%20naturally%20inherits%20strong%20generalization%20ability%20and%20expressive%20power%20from%20its%20large-scale%20training%20and%20point%20map%20representation.%20However%2C%20it%20is%20unclear%20how%20to%20reconstruct%20a%20topologically%20consistent%20mesh%20from%20VGGT%2C%20as%20the%20topology%20information%20is%20missing%20in%20its%20prediction.%20To%20this%20end%2C%20we%20augment%20VGGT%20with%20Pixel3DMM%20for%20injecting%20topology%20information%20via%20pixel-aligned%20UV%20values.%20In%20this%20manner%2C%20we%20convert%20the%20pixel-aligned%20point%20map%20of%20VGGT%20to%20a%20point%20cloud%20with%20topology.%20Tailored%20to%20this%20point%20cloud%20with%20known%20topology%2C%20we%20propose%20a%20novel%20Topology-Aware%20Bundle%20Adjustment%20strategy%20to%20fuse%20them%2C%20where%20we%20construct%20a%20Laplacian%20energy%20for%20the%20Bundle%20Adjustment%20objective.%20Our%20method%20achieves%20high-quality%20reconstruction%20in%2010%20seconds%20for%2016%20views%20on%20a%20single%20NVIDIA%20RTX%204090.%20Experiments%20demonstrate%20state-of-the-art%20results%20on%20benchmarks%20and%20impressive%20generalization%20to%20in-the-wild%20data.%20Code%20is%20available%20at%20https%3A//github.com/grignarder/vggtface.&entry.1838667208=http%3A//arxiv.org/abs/2511.20366v1&entry.124074799=Read"},
{"title": "Personalized Generative Low-light Image Denoising and Enhancement", "author": "Xijun Wang and Prateek Chennuri and Dilshan Godaliyadda and Yu Yuan and Bole Ma and Xingguang Zhang and Hamid R. Sheikh and Stanley Chan", "abstract": "Modern cameras' performance in low-light conditions remains suboptimal due to fundamental limitations in photon shot noise and sensor read noise. Generative image restoration methods have shown promising results compared to traditional approaches, but they suffer from hallucinatory content generation when the signal-to-noise ratio (SNR) is low. Leveraging the availability of personalized photo galleries of the users, we introduce Diffusion-based Personalized Generative Denoising (DiffPGD), a new approach that builds a customized diffusion model for individual users. Our key innovation lies in the development of an identity-consistent physical buffer that extracts the physical attributes of the person from the gallery. This ID-consistent physical buffer serves as a robust prior that can be seamlessly integrated into the diffusion model to restore degraded images without the need for fine-tuning. Over a wide range of low-light testing scenarios, we show that DiffPGD achieves superior image denoising and enhancement performance compared to existing diffusion-based denoising approaches. Our project page can be found at \\href{https://genai-restore.github.io/DiffPGD/}{\\textcolor{purple}{\\textbf{https://genai-restore.github.io/DiffPGD/}}}.", "link": "http://arxiv.org/abs/2412.14327v3", "date": "2025-11-25", "relevancy": 3.0975, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6421}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6129}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Generative%20Low-light%20Image%20Denoising%20and%20Enhancement&body=Title%3A%20Personalized%20Generative%20Low-light%20Image%20Denoising%20and%20Enhancement%0AAuthor%3A%20Xijun%20Wang%20and%20Prateek%20Chennuri%20and%20Dilshan%20Godaliyadda%20and%20Yu%20Yuan%20and%20Bole%20Ma%20and%20Xingguang%20Zhang%20and%20Hamid%20R.%20Sheikh%20and%20Stanley%20Chan%0AAbstract%3A%20Modern%20cameras%27%20performance%20in%20low-light%20conditions%20remains%20suboptimal%20due%20to%20fundamental%20limitations%20in%20photon%20shot%20noise%20and%20sensor%20read%20noise.%20Generative%20image%20restoration%20methods%20have%20shown%20promising%20results%20compared%20to%20traditional%20approaches%2C%20but%20they%20suffer%20from%20hallucinatory%20content%20generation%20when%20the%20signal-to-noise%20ratio%20%28SNR%29%20is%20low.%20Leveraging%20the%20availability%20of%20personalized%20photo%20galleries%20of%20the%20users%2C%20we%20introduce%20Diffusion-based%20Personalized%20Generative%20Denoising%20%28DiffPGD%29%2C%20a%20new%20approach%20that%20builds%20a%20customized%20diffusion%20model%20for%20individual%20users.%20Our%20key%20innovation%20lies%20in%20the%20development%20of%20an%20identity-consistent%20physical%20buffer%20that%20extracts%20the%20physical%20attributes%20of%20the%20person%20from%20the%20gallery.%20This%20ID-consistent%20physical%20buffer%20serves%20as%20a%20robust%20prior%20that%20can%20be%20seamlessly%20integrated%20into%20the%20diffusion%20model%20to%20restore%20degraded%20images%20without%20the%20need%20for%20fine-tuning.%20Over%20a%20wide%20range%20of%20low-light%20testing%20scenarios%2C%20we%20show%20that%20DiffPGD%20achieves%20superior%20image%20denoising%20and%20enhancement%20performance%20compared%20to%20existing%20diffusion-based%20denoising%20approaches.%20Our%20project%20page%20can%20be%20found%20at%20%5Chref%7Bhttps%3A//genai-restore.github.io/DiffPGD/%7D%7B%5Ctextcolor%7Bpurple%7D%7B%5Ctextbf%7Bhttps%3A//genai-restore.github.io/DiffPGD/%7D%7D%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2412.14327v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Generative%2520Low-light%2520Image%2520Denoising%2520and%2520Enhancement%26entry.906535625%3DXijun%2520Wang%2520and%2520Prateek%2520Chennuri%2520and%2520Dilshan%2520Godaliyadda%2520and%2520Yu%2520Yuan%2520and%2520Bole%2520Ma%2520and%2520Xingguang%2520Zhang%2520and%2520Hamid%2520R.%2520Sheikh%2520and%2520Stanley%2520Chan%26entry.1292438233%3DModern%2520cameras%2527%2520performance%2520in%2520low-light%2520conditions%2520remains%2520suboptimal%2520due%2520to%2520fundamental%2520limitations%2520in%2520photon%2520shot%2520noise%2520and%2520sensor%2520read%2520noise.%2520Generative%2520image%2520restoration%2520methods%2520have%2520shown%2520promising%2520results%2520compared%2520to%2520traditional%2520approaches%252C%2520but%2520they%2520suffer%2520from%2520hallucinatory%2520content%2520generation%2520when%2520the%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2520is%2520low.%2520Leveraging%2520the%2520availability%2520of%2520personalized%2520photo%2520galleries%2520of%2520the%2520users%252C%2520we%2520introduce%2520Diffusion-based%2520Personalized%2520Generative%2520Denoising%2520%2528DiffPGD%2529%252C%2520a%2520new%2520approach%2520that%2520builds%2520a%2520customized%2520diffusion%2520model%2520for%2520individual%2520users.%2520Our%2520key%2520innovation%2520lies%2520in%2520the%2520development%2520of%2520an%2520identity-consistent%2520physical%2520buffer%2520that%2520extracts%2520the%2520physical%2520attributes%2520of%2520the%2520person%2520from%2520the%2520gallery.%2520This%2520ID-consistent%2520physical%2520buffer%2520serves%2520as%2520a%2520robust%2520prior%2520that%2520can%2520be%2520seamlessly%2520integrated%2520into%2520the%2520diffusion%2520model%2520to%2520restore%2520degraded%2520images%2520without%2520the%2520need%2520for%2520fine-tuning.%2520Over%2520a%2520wide%2520range%2520of%2520low-light%2520testing%2520scenarios%252C%2520we%2520show%2520that%2520DiffPGD%2520achieves%2520superior%2520image%2520denoising%2520and%2520enhancement%2520performance%2520compared%2520to%2520existing%2520diffusion-based%2520denoising%2520approaches.%2520Our%2520project%2520page%2520can%2520be%2520found%2520at%2520%255Chref%257Bhttps%253A//genai-restore.github.io/DiffPGD/%257D%257B%255Ctextcolor%257Bpurple%257D%257B%255Ctextbf%257Bhttps%253A//genai-restore.github.io/DiffPGD/%257D%257D%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14327v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Generative%20Low-light%20Image%20Denoising%20and%20Enhancement&entry.906535625=Xijun%20Wang%20and%20Prateek%20Chennuri%20and%20Dilshan%20Godaliyadda%20and%20Yu%20Yuan%20and%20Bole%20Ma%20and%20Xingguang%20Zhang%20and%20Hamid%20R.%20Sheikh%20and%20Stanley%20Chan&entry.1292438233=Modern%20cameras%27%20performance%20in%20low-light%20conditions%20remains%20suboptimal%20due%20to%20fundamental%20limitations%20in%20photon%20shot%20noise%20and%20sensor%20read%20noise.%20Generative%20image%20restoration%20methods%20have%20shown%20promising%20results%20compared%20to%20traditional%20approaches%2C%20but%20they%20suffer%20from%20hallucinatory%20content%20generation%20when%20the%20signal-to-noise%20ratio%20%28SNR%29%20is%20low.%20Leveraging%20the%20availability%20of%20personalized%20photo%20galleries%20of%20the%20users%2C%20we%20introduce%20Diffusion-based%20Personalized%20Generative%20Denoising%20%28DiffPGD%29%2C%20a%20new%20approach%20that%20builds%20a%20customized%20diffusion%20model%20for%20individual%20users.%20Our%20key%20innovation%20lies%20in%20the%20development%20of%20an%20identity-consistent%20physical%20buffer%20that%20extracts%20the%20physical%20attributes%20of%20the%20person%20from%20the%20gallery.%20This%20ID-consistent%20physical%20buffer%20serves%20as%20a%20robust%20prior%20that%20can%20be%20seamlessly%20integrated%20into%20the%20diffusion%20model%20to%20restore%20degraded%20images%20without%20the%20need%20for%20fine-tuning.%20Over%20a%20wide%20range%20of%20low-light%20testing%20scenarios%2C%20we%20show%20that%20DiffPGD%20achieves%20superior%20image%20denoising%20and%20enhancement%20performance%20compared%20to%20existing%20diffusion-based%20denoising%20approaches.%20Our%20project%20page%20can%20be%20found%20at%20%5Chref%7Bhttps%3A//genai-restore.github.io/DiffPGD/%7D%7B%5Ctextcolor%7Bpurple%7D%7B%5Ctextbf%7Bhttps%3A//genai-restore.github.io/DiffPGD/%7D%7D%7D.&entry.1838667208=http%3A//arxiv.org/abs/2412.14327v3&entry.124074799=Read"},
{"title": "AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend", "author": "Hengyi Wang and Lourdes Agapito", "abstract": "We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.", "link": "http://arxiv.org/abs/2511.20343v1", "date": "2025-11-25", "relevancy": 3.0962, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6405}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6086}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMB3R%3A%20Accurate%20Feed-forward%20Metric-scale%203D%20Reconstruction%20with%20Backend&body=Title%3A%20AMB3R%3A%20Accurate%20Feed-forward%20Metric-scale%203D%20Reconstruction%20with%20Backend%0AAuthor%3A%20Hengyi%20Wang%20and%20Lourdes%20Agapito%0AAbstract%3A%20We%20present%20AMB3R%2C%20a%20multi-view%20feed-forward%20model%20for%20dense%203D%20reconstruction%20on%20a%20metric-scale%20that%20addresses%20diverse%203D%20vision%20tasks.%20The%20key%20idea%20is%20to%20leverage%20a%20sparse%2C%20yet%20compact%2C%20volumetric%20scene%20representation%20as%20our%20backend%2C%20enabling%20geometric%20reasoning%20with%20spatial%20compactness.%20Although%20trained%20solely%20for%20multi-view%20reconstruction%2C%20we%20demonstrate%20that%20AMB3R%20can%20be%20seamlessly%20extended%20to%20uncalibrated%20visual%20odometry%20%28online%29%20or%20large-scale%20structure%20from%20motion%20without%20the%20need%20for%20task-specific%20fine-tuning%20or%20test-time%20optimization.%20Compared%20to%20prior%20pointmap-based%20models%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20in%20camera%20pose%2C%20depth%2C%20and%20metric-scale%20estimation%2C%203D%20reconstruction%2C%20and%20even%20surpasses%20optimization-based%20SLAM%20and%20SfM%20methods%20with%20dense%20reconstruction%20priors%20on%20common%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMB3R%253A%2520Accurate%2520Feed-forward%2520Metric-scale%25203D%2520Reconstruction%2520with%2520Backend%26entry.906535625%3DHengyi%2520Wang%2520and%2520Lourdes%2520Agapito%26entry.1292438233%3DWe%2520present%2520AMB3R%252C%2520a%2520multi-view%2520feed-forward%2520model%2520for%2520dense%25203D%2520reconstruction%2520on%2520a%2520metric-scale%2520that%2520addresses%2520diverse%25203D%2520vision%2520tasks.%2520The%2520key%2520idea%2520is%2520to%2520leverage%2520a%2520sparse%252C%2520yet%2520compact%252C%2520volumetric%2520scene%2520representation%2520as%2520our%2520backend%252C%2520enabling%2520geometric%2520reasoning%2520with%2520spatial%2520compactness.%2520Although%2520trained%2520solely%2520for%2520multi-view%2520reconstruction%252C%2520we%2520demonstrate%2520that%2520AMB3R%2520can%2520be%2520seamlessly%2520extended%2520to%2520uncalibrated%2520visual%2520odometry%2520%2528online%2529%2520or%2520large-scale%2520structure%2520from%2520motion%2520without%2520the%2520need%2520for%2520task-specific%2520fine-tuning%2520or%2520test-time%2520optimization.%2520Compared%2520to%2520prior%2520pointmap-based%2520models%252C%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520in%2520camera%2520pose%252C%2520depth%252C%2520and%2520metric-scale%2520estimation%252C%25203D%2520reconstruction%252C%2520and%2520even%2520surpasses%2520optimization-based%2520SLAM%2520and%2520SfM%2520methods%2520with%2520dense%2520reconstruction%2520priors%2520on%2520common%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMB3R%3A%20Accurate%20Feed-forward%20Metric-scale%203D%20Reconstruction%20with%20Backend&entry.906535625=Hengyi%20Wang%20and%20Lourdes%20Agapito&entry.1292438233=We%20present%20AMB3R%2C%20a%20multi-view%20feed-forward%20model%20for%20dense%203D%20reconstruction%20on%20a%20metric-scale%20that%20addresses%20diverse%203D%20vision%20tasks.%20The%20key%20idea%20is%20to%20leverage%20a%20sparse%2C%20yet%20compact%2C%20volumetric%20scene%20representation%20as%20our%20backend%2C%20enabling%20geometric%20reasoning%20with%20spatial%20compactness.%20Although%20trained%20solely%20for%20multi-view%20reconstruction%2C%20we%20demonstrate%20that%20AMB3R%20can%20be%20seamlessly%20extended%20to%20uncalibrated%20visual%20odometry%20%28online%29%20or%20large-scale%20structure%20from%20motion%20without%20the%20need%20for%20task-specific%20fine-tuning%20or%20test-time%20optimization.%20Compared%20to%20prior%20pointmap-based%20models%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20in%20camera%20pose%2C%20depth%2C%20and%20metric-scale%20estimation%2C%203D%20reconstruction%2C%20and%20even%20surpasses%20optimization-based%20SLAM%20and%20SfM%20methods%20with%20dense%20reconstruction%20priors%20on%20common%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2511.20343v1&entry.124074799=Read"},
{"title": "Thinking in 360\u00b0: Humanoid Visual Search in the Wild", "author": "Heyang Yu and Yinan Han and Xiangyu Zhang and Baiqiao Yin and Bowen Chang and Xiangyu Han and Xinhao Liu and Jing Zhang and Marco Pavone and Chen Feng and Saining Xie and Yiming Li", "abstract": "Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360\u00b0. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360\u00b0 panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.", "link": "http://arxiv.org/abs/2511.20351v1", "date": "2025-11-25", "relevancy": 3.0931, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20in%20360%C2%B0%3A%20Humanoid%20Visual%20Search%20in%20the%20Wild&body=Title%3A%20Thinking%20in%20360%C2%B0%3A%20Humanoid%20Visual%20Search%20in%20the%20Wild%0AAuthor%3A%20Heyang%20Yu%20and%20Yinan%20Han%20and%20Xiangyu%20Zhang%20and%20Baiqiao%20Yin%20and%20Bowen%20Chang%20and%20Xiangyu%20Han%20and%20Xinhao%20Liu%20and%20Jing%20Zhang%20and%20Marco%20Pavone%20and%20Chen%20Feng%20and%20Saining%20Xie%20and%20Yiming%20Li%0AAbstract%3A%20Humans%20rely%20on%20the%20synergistic%20control%20of%20head%20%28cephalomotor%29%20and%20eye%20%28oculomotor%29%20to%20efficiently%20search%20for%20visual%20information%20in%20360%C2%B0.%20However%2C%20prior%20approaches%20to%20visual%20search%20are%20limited%20to%20a%20static%20image%2C%20neglecting%20the%20physical%20embodiment%20and%20its%20interaction%20with%20the%203D%20world.%20How%20can%20we%20develop%20embodied%20visual%20search%20agents%20as%20efficient%20as%20humans%20while%20bypassing%20the%20constraints%20imposed%20by%20real-world%20hardware%3F%20To%20this%20end%2C%20we%20propose%20humanoid%20visual%20search%20where%20a%20humanoid%20agent%20actively%20rotates%20its%20head%20to%20search%20for%20objects%20or%20paths%20in%20an%20immersive%20world%20represented%20by%20a%20360%C2%B0%20panoramic%20image.%20To%20study%20visual%20search%20in%20visually-crowded%20real-world%20scenarios%2C%20we%20build%20H%2A%20Bench%2C%20a%20new%20benchmark%20that%20moves%20beyond%20household%20scenes%20to%20challenging%20in-the-wild%20scenes%20that%20necessitate%20advanced%20visual-spatial%20reasoning%20capabilities%2C%20such%20as%20transportation%20hubs%2C%20large-scale%20retail%20spaces%2C%20urban%20streets%2C%20and%20public%20institutions.%20Our%20experiments%20first%20reveal%20that%20even%20top-tier%20proprietary%20models%20falter%2C%20achieving%20only%20~30%25%20success%20in%20object%20and%20path%20search.%20We%20then%20use%20post-training%20techniques%20to%20enhance%20the%20open-source%20Qwen2.5-VL%2C%20increasing%20its%20success%20rate%20by%20over%20threefold%20for%20both%20object%20search%20%2814.83%25%20to%2047.38%25%29%20and%20path%20search%20%286.44%25%20to%2024.94%25%29.%20Notably%2C%20the%20lower%20ceiling%20of%20path%20search%20reveals%20its%20inherent%20difficulty%2C%20which%20we%20attribute%20to%20the%20demand%20for%20sophisticated%20spatial%20commonsense.%20Our%20results%20not%20only%20show%20a%20promising%20path%20forward%20but%20also%20quantify%20the%20immense%20challenge%20that%20remains%20in%20building%20MLLM%20agents%20that%20can%20be%20seamlessly%20integrated%20into%20everyday%20human%20life.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520in%2520360%25C2%25B0%253A%2520Humanoid%2520Visual%2520Search%2520in%2520the%2520Wild%26entry.906535625%3DHeyang%2520Yu%2520and%2520Yinan%2520Han%2520and%2520Xiangyu%2520Zhang%2520and%2520Baiqiao%2520Yin%2520and%2520Bowen%2520Chang%2520and%2520Xiangyu%2520Han%2520and%2520Xinhao%2520Liu%2520and%2520Jing%2520Zhang%2520and%2520Marco%2520Pavone%2520and%2520Chen%2520Feng%2520and%2520Saining%2520Xie%2520and%2520Yiming%2520Li%26entry.1292438233%3DHumans%2520rely%2520on%2520the%2520synergistic%2520control%2520of%2520head%2520%2528cephalomotor%2529%2520and%2520eye%2520%2528oculomotor%2529%2520to%2520efficiently%2520search%2520for%2520visual%2520information%2520in%2520360%25C2%25B0.%2520However%252C%2520prior%2520approaches%2520to%2520visual%2520search%2520are%2520limited%2520to%2520a%2520static%2520image%252C%2520neglecting%2520the%2520physical%2520embodiment%2520and%2520its%2520interaction%2520with%2520the%25203D%2520world.%2520How%2520can%2520we%2520develop%2520embodied%2520visual%2520search%2520agents%2520as%2520efficient%2520as%2520humans%2520while%2520bypassing%2520the%2520constraints%2520imposed%2520by%2520real-world%2520hardware%253F%2520To%2520this%2520end%252C%2520we%2520propose%2520humanoid%2520visual%2520search%2520where%2520a%2520humanoid%2520agent%2520actively%2520rotates%2520its%2520head%2520to%2520search%2520for%2520objects%2520or%2520paths%2520in%2520an%2520immersive%2520world%2520represented%2520by%2520a%2520360%25C2%25B0%2520panoramic%2520image.%2520To%2520study%2520visual%2520search%2520in%2520visually-crowded%2520real-world%2520scenarios%252C%2520we%2520build%2520H%252A%2520Bench%252C%2520a%2520new%2520benchmark%2520that%2520moves%2520beyond%2520household%2520scenes%2520to%2520challenging%2520in-the-wild%2520scenes%2520that%2520necessitate%2520advanced%2520visual-spatial%2520reasoning%2520capabilities%252C%2520such%2520as%2520transportation%2520hubs%252C%2520large-scale%2520retail%2520spaces%252C%2520urban%2520streets%252C%2520and%2520public%2520institutions.%2520Our%2520experiments%2520first%2520reveal%2520that%2520even%2520top-tier%2520proprietary%2520models%2520falter%252C%2520achieving%2520only%2520~30%2525%2520success%2520in%2520object%2520and%2520path%2520search.%2520We%2520then%2520use%2520post-training%2520techniques%2520to%2520enhance%2520the%2520open-source%2520Qwen2.5-VL%252C%2520increasing%2520its%2520success%2520rate%2520by%2520over%2520threefold%2520for%2520both%2520object%2520search%2520%252814.83%2525%2520to%252047.38%2525%2529%2520and%2520path%2520search%2520%25286.44%2525%2520to%252024.94%2525%2529.%2520Notably%252C%2520the%2520lower%2520ceiling%2520of%2520path%2520search%2520reveals%2520its%2520inherent%2520difficulty%252C%2520which%2520we%2520attribute%2520to%2520the%2520demand%2520for%2520sophisticated%2520spatial%2520commonsense.%2520Our%2520results%2520not%2520only%2520show%2520a%2520promising%2520path%2520forward%2520but%2520also%2520quantify%2520the%2520immense%2520challenge%2520that%2520remains%2520in%2520building%2520MLLM%2520agents%2520that%2520can%2520be%2520seamlessly%2520integrated%2520into%2520everyday%2520human%2520life.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20in%20360%C2%B0%3A%20Humanoid%20Visual%20Search%20in%20the%20Wild&entry.906535625=Heyang%20Yu%20and%20Yinan%20Han%20and%20Xiangyu%20Zhang%20and%20Baiqiao%20Yin%20and%20Bowen%20Chang%20and%20Xiangyu%20Han%20and%20Xinhao%20Liu%20and%20Jing%20Zhang%20and%20Marco%20Pavone%20and%20Chen%20Feng%20and%20Saining%20Xie%20and%20Yiming%20Li&entry.1292438233=Humans%20rely%20on%20the%20synergistic%20control%20of%20head%20%28cephalomotor%29%20and%20eye%20%28oculomotor%29%20to%20efficiently%20search%20for%20visual%20information%20in%20360%C2%B0.%20However%2C%20prior%20approaches%20to%20visual%20search%20are%20limited%20to%20a%20static%20image%2C%20neglecting%20the%20physical%20embodiment%20and%20its%20interaction%20with%20the%203D%20world.%20How%20can%20we%20develop%20embodied%20visual%20search%20agents%20as%20efficient%20as%20humans%20while%20bypassing%20the%20constraints%20imposed%20by%20real-world%20hardware%3F%20To%20this%20end%2C%20we%20propose%20humanoid%20visual%20search%20where%20a%20humanoid%20agent%20actively%20rotates%20its%20head%20to%20search%20for%20objects%20or%20paths%20in%20an%20immersive%20world%20represented%20by%20a%20360%C2%B0%20panoramic%20image.%20To%20study%20visual%20search%20in%20visually-crowded%20real-world%20scenarios%2C%20we%20build%20H%2A%20Bench%2C%20a%20new%20benchmark%20that%20moves%20beyond%20household%20scenes%20to%20challenging%20in-the-wild%20scenes%20that%20necessitate%20advanced%20visual-spatial%20reasoning%20capabilities%2C%20such%20as%20transportation%20hubs%2C%20large-scale%20retail%20spaces%2C%20urban%20streets%2C%20and%20public%20institutions.%20Our%20experiments%20first%20reveal%20that%20even%20top-tier%20proprietary%20models%20falter%2C%20achieving%20only%20~30%25%20success%20in%20object%20and%20path%20search.%20We%20then%20use%20post-training%20techniques%20to%20enhance%20the%20open-source%20Qwen2.5-VL%2C%20increasing%20its%20success%20rate%20by%20over%20threefold%20for%20both%20object%20search%20%2814.83%25%20to%2047.38%25%29%20and%20path%20search%20%286.44%25%20to%2024.94%25%29.%20Notably%2C%20the%20lower%20ceiling%20of%20path%20search%20reveals%20its%20inherent%20difficulty%2C%20which%20we%20attribute%20to%20the%20demand%20for%20sophisticated%20spatial%20commonsense.%20Our%20results%20not%20only%20show%20a%20promising%20path%20forward%20but%20also%20quantify%20the%20immense%20challenge%20that%20remains%20in%20building%20MLLM%20agents%20that%20can%20be%20seamlessly%20integrated%20into%20everyday%20human%20life.&entry.1838667208=http%3A//arxiv.org/abs/2511.20351v1&entry.124074799=Read"},
{"title": "3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding", "author": "Xiaoye Wang and Chen Tang and Xiangyu Yue and Wei-Hong Li", "abstract": "This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.", "link": "http://arxiv.org/abs/2511.20646v1", "date": "2025-11-25", "relevancy": 3.0705, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6361}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-Aware%20Multi-Task%20Learning%20with%20Cross-View%20Correlations%20for%20Dense%20Scene%20Understanding&body=Title%3A%203D-Aware%20Multi-Task%20Learning%20with%20Cross-View%20Correlations%20for%20Dense%20Scene%20Understanding%0AAuthor%3A%20Xiaoye%20Wang%20and%20Chen%20Tang%20and%20Xiangyu%20Yue%20and%20Wei-Hong%20Li%0AAbstract%3A%20This%20paper%20addresses%20the%20challenge%20of%20training%20a%20single%20network%20to%20jointly%20perform%20multiple%20dense%20prediction%20tasks%2C%20such%20as%20segmentation%20and%20depth%20estimation%2C%20i.e.%2C%20multi-task%20learning%20%28MTL%29.%20Current%20approaches%20mainly%20capture%20cross-task%20relations%20in%20the%202D%20image%20space%2C%20often%20leading%20to%20unstructured%20features%20lacking%203D-awareness.%20We%20argue%20that%203D-awareness%20is%20vital%20for%20modeling%20cross-task%20correlations%20essential%20for%20comprehensive%20scene%20understanding.%20We%20propose%20to%20address%20this%20problem%20by%20integrating%20correlations%20across%20views%2C%20i.e.%2C%20cost%20volume%2C%20as%20geometric%20consistency%20in%20the%20MTL%20network.%20Specifically%2C%20we%20introduce%20a%20lightweight%20Cross-view%20Module%20%28CvM%29%2C%20shared%20across%20tasks%2C%20to%20exchange%20information%20across%20views%20and%20capture%20cross-view%20correlations%2C%20integrated%20with%20a%20feature%20from%20MTL%20encoder%20for%20multi-task%20predictions.%20This%20module%20is%20architecture-agnostic%20and%20can%20be%20applied%20to%20both%20single%20and%20multi-view%20data.%20Extensive%20results%20on%20NYUv2%20and%20PASCAL-Context%20demonstrate%20that%20our%20method%20effectively%20injects%20geometric%20consistency%20into%20existing%20MTL%20methods%20to%20improve%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-Aware%2520Multi-Task%2520Learning%2520with%2520Cross-View%2520Correlations%2520for%2520Dense%2520Scene%2520Understanding%26entry.906535625%3DXiaoye%2520Wang%2520and%2520Chen%2520Tang%2520and%2520Xiangyu%2520Yue%2520and%2520Wei-Hong%2520Li%26entry.1292438233%3DThis%2520paper%2520addresses%2520the%2520challenge%2520of%2520training%2520a%2520single%2520network%2520to%2520jointly%2520perform%2520multiple%2520dense%2520prediction%2520tasks%252C%2520such%2520as%2520segmentation%2520and%2520depth%2520estimation%252C%2520i.e.%252C%2520multi-task%2520learning%2520%2528MTL%2529.%2520Current%2520approaches%2520mainly%2520capture%2520cross-task%2520relations%2520in%2520the%25202D%2520image%2520space%252C%2520often%2520leading%2520to%2520unstructured%2520features%2520lacking%25203D-awareness.%2520We%2520argue%2520that%25203D-awareness%2520is%2520vital%2520for%2520modeling%2520cross-task%2520correlations%2520essential%2520for%2520comprehensive%2520scene%2520understanding.%2520We%2520propose%2520to%2520address%2520this%2520problem%2520by%2520integrating%2520correlations%2520across%2520views%252C%2520i.e.%252C%2520cost%2520volume%252C%2520as%2520geometric%2520consistency%2520in%2520the%2520MTL%2520network.%2520Specifically%252C%2520we%2520introduce%2520a%2520lightweight%2520Cross-view%2520Module%2520%2528CvM%2529%252C%2520shared%2520across%2520tasks%252C%2520to%2520exchange%2520information%2520across%2520views%2520and%2520capture%2520cross-view%2520correlations%252C%2520integrated%2520with%2520a%2520feature%2520from%2520MTL%2520encoder%2520for%2520multi-task%2520predictions.%2520This%2520module%2520is%2520architecture-agnostic%2520and%2520can%2520be%2520applied%2520to%2520both%2520single%2520and%2520multi-view%2520data.%2520Extensive%2520results%2520on%2520NYUv2%2520and%2520PASCAL-Context%2520demonstrate%2520that%2520our%2520method%2520effectively%2520injects%2520geometric%2520consistency%2520into%2520existing%2520MTL%2520methods%2520to%2520improve%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-Aware%20Multi-Task%20Learning%20with%20Cross-View%20Correlations%20for%20Dense%20Scene%20Understanding&entry.906535625=Xiaoye%20Wang%20and%20Chen%20Tang%20and%20Xiangyu%20Yue%20and%20Wei-Hong%20Li&entry.1292438233=This%20paper%20addresses%20the%20challenge%20of%20training%20a%20single%20network%20to%20jointly%20perform%20multiple%20dense%20prediction%20tasks%2C%20such%20as%20segmentation%20and%20depth%20estimation%2C%20i.e.%2C%20multi-task%20learning%20%28MTL%29.%20Current%20approaches%20mainly%20capture%20cross-task%20relations%20in%20the%202D%20image%20space%2C%20often%20leading%20to%20unstructured%20features%20lacking%203D-awareness.%20We%20argue%20that%203D-awareness%20is%20vital%20for%20modeling%20cross-task%20correlations%20essential%20for%20comprehensive%20scene%20understanding.%20We%20propose%20to%20address%20this%20problem%20by%20integrating%20correlations%20across%20views%2C%20i.e.%2C%20cost%20volume%2C%20as%20geometric%20consistency%20in%20the%20MTL%20network.%20Specifically%2C%20we%20introduce%20a%20lightweight%20Cross-view%20Module%20%28CvM%29%2C%20shared%20across%20tasks%2C%20to%20exchange%20information%20across%20views%20and%20capture%20cross-view%20correlations%2C%20integrated%20with%20a%20feature%20from%20MTL%20encoder%20for%20multi-task%20predictions.%20This%20module%20is%20architecture-agnostic%20and%20can%20be%20applied%20to%20both%20single%20and%20multi-view%20data.%20Extensive%20results%20on%20NYUv2%20and%20PASCAL-Context%20demonstrate%20that%20our%20method%20effectively%20injects%20geometric%20consistency%20into%20existing%20MTL%20methods%20to%20improve%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.20646v1&entry.124074799=Read"},
{"title": "GS-Checker: Tampering Localization for 3D Gaussian Splatting", "author": "Haoliang Han and Ziyuan Luo and Jun Qi and Anderson Rocha and Renjie Wan", "abstract": "Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.", "link": "http://arxiv.org/abs/2511.20354v1", "date": "2025-11-25", "relevancy": 3.0297, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6236}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6025}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Checker%3A%20Tampering%20Localization%20for%203D%20Gaussian%20Splatting&body=Title%3A%20GS-Checker%3A%20Tampering%20Localization%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Haoliang%20Han%20and%20Ziyuan%20Luo%20and%20Jun%20Qi%20and%20Anderson%20Rocha%20and%20Renjie%20Wan%0AAbstract%3A%20Recent%20advances%20in%20editing%20technologies%20for%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20made%20it%20simple%20to%20manipulate%203D%20scenes.%20However%2C%20these%20technologies%20raise%20concerns%20about%20potential%20malicious%20manipulation%20of%203D%20content.%20To%20avoid%20such%20malicious%20applications%2C%20localizing%20tampered%20regions%20becomes%20crucial.%20In%20this%20paper%2C%20we%20propose%20GS-Checker%2C%20a%20novel%20method%20for%20locating%20tampered%20areas%20in%203DGS%20models.%20Our%20approach%20integrates%20a%203D%20tampering%20attribute%20into%20the%203D%20Gaussian%20parameters%20to%20indicate%20whether%20the%20Gaussian%20has%20been%20tampered.%20Additionally%2C%20we%20design%20a%203D%20contrastive%20mechanism%20by%20comparing%20the%20similarity%20of%20key%20attributes%20between%203D%20Gaussians%20to%20seek%20tampering%20cues%20at%203D%20level.%20Furthermore%2C%20we%20introduce%20a%20cyclic%20optimization%20strategy%20to%20refine%20the%203D%20tampering%20attribute%2C%20enabling%20more%20accurate%20tampering%20localization.%20Notably%2C%20our%20approach%20does%20not%20require%20expensive%203D%20labels%20for%20supervision.%20Extensive%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20to%20locate%20the%20tampered%203DGS%20area.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Checker%253A%2520Tampering%2520Localization%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DHaoliang%2520Han%2520and%2520Ziyuan%2520Luo%2520and%2520Jun%2520Qi%2520and%2520Anderson%2520Rocha%2520and%2520Renjie%2520Wan%26entry.1292438233%3DRecent%2520advances%2520in%2520editing%2520technologies%2520for%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520made%2520it%2520simple%2520to%2520manipulate%25203D%2520scenes.%2520However%252C%2520these%2520technologies%2520raise%2520concerns%2520about%2520potential%2520malicious%2520manipulation%2520of%25203D%2520content.%2520To%2520avoid%2520such%2520malicious%2520applications%252C%2520localizing%2520tampered%2520regions%2520becomes%2520crucial.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GS-Checker%252C%2520a%2520novel%2520method%2520for%2520locating%2520tampered%2520areas%2520in%25203DGS%2520models.%2520Our%2520approach%2520integrates%2520a%25203D%2520tampering%2520attribute%2520into%2520the%25203D%2520Gaussian%2520parameters%2520to%2520indicate%2520whether%2520the%2520Gaussian%2520has%2520been%2520tampered.%2520Additionally%252C%2520we%2520design%2520a%25203D%2520contrastive%2520mechanism%2520by%2520comparing%2520the%2520similarity%2520of%2520key%2520attributes%2520between%25203D%2520Gaussians%2520to%2520seek%2520tampering%2520cues%2520at%25203D%2520level.%2520Furthermore%252C%2520we%2520introduce%2520a%2520cyclic%2520optimization%2520strategy%2520to%2520refine%2520the%25203D%2520tampering%2520attribute%252C%2520enabling%2520more%2520accurate%2520tampering%2520localization.%2520Notably%252C%2520our%2520approach%2520does%2520not%2520require%2520expensive%25203D%2520labels%2520for%2520supervision.%2520Extensive%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%2520to%2520locate%2520the%2520tampered%25203DGS%2520area.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Checker%3A%20Tampering%20Localization%20for%203D%20Gaussian%20Splatting&entry.906535625=Haoliang%20Han%20and%20Ziyuan%20Luo%20and%20Jun%20Qi%20and%20Anderson%20Rocha%20and%20Renjie%20Wan&entry.1292438233=Recent%20advances%20in%20editing%20technologies%20for%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20made%20it%20simple%20to%20manipulate%203D%20scenes.%20However%2C%20these%20technologies%20raise%20concerns%20about%20potential%20malicious%20manipulation%20of%203D%20content.%20To%20avoid%20such%20malicious%20applications%2C%20localizing%20tampered%20regions%20becomes%20crucial.%20In%20this%20paper%2C%20we%20propose%20GS-Checker%2C%20a%20novel%20method%20for%20locating%20tampered%20areas%20in%203DGS%20models.%20Our%20approach%20integrates%20a%203D%20tampering%20attribute%20into%20the%203D%20Gaussian%20parameters%20to%20indicate%20whether%20the%20Gaussian%20has%20been%20tampered.%20Additionally%2C%20we%20design%20a%203D%20contrastive%20mechanism%20by%20comparing%20the%20similarity%20of%20key%20attributes%20between%203D%20Gaussians%20to%20seek%20tampering%20cues%20at%203D%20level.%20Furthermore%2C%20we%20introduce%20a%20cyclic%20optimization%20strategy%20to%20refine%20the%203D%20tampering%20attribute%2C%20enabling%20more%20accurate%20tampering%20localization.%20Notably%2C%20our%20approach%20does%20not%20require%20expensive%203D%20labels%20for%20supervision.%20Extensive%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method%20to%20locate%20the%20tampered%203DGS%20area.&entry.1838667208=http%3A//arxiv.org/abs/2511.20354v1&entry.124074799=Read"},
{"title": "Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model", "author": "Ziyue Wang and Yayati Jadhav and Peter Pak and Amir Barati Farimani", "abstract": "Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.", "link": "http://arxiv.org/abs/2511.20636v1", "date": "2025-11-25", "relevancy": 3.0034, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6057}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6057}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image2Gcode%3A%20Image-to-G-code%20Generation%20for%20Additive%20Manufacturing%20Using%20Diffusion-Transformer%20Model&body=Title%3A%20Image2Gcode%3A%20Image-to-G-code%20Generation%20for%20Additive%20Manufacturing%20Using%20Diffusion-Transformer%20Model%0AAuthor%3A%20Ziyue%20Wang%20and%20Yayati%20Jadhav%20and%20Peter%20Pak%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20Mechanical%20design%20and%20manufacturing%20workflows%20conventionally%20begin%20with%20conceptual%20design%2C%20followed%20by%20the%20creation%20of%20a%20computer-aided%20design%20%28CAD%29%20model%20and%20fabrication%20through%20material-extrusion%20%28MEX%29%20printing.%20This%20process%20requires%20converting%20CAD%20geometry%20into%20machine-readable%20G-code%20through%20slicing%20and%20path%20planning.%20While%20each%20step%20is%20well%20established%2C%20dependence%20on%20CAD%20modeling%20remains%20a%20major%20bottleneck%3A%20constructing%20object-specific%203D%20geometry%20is%20slow%20and%20poorly%20suited%20to%20rapid%20prototyping.%20Even%20minor%20design%20variations%20typically%20necessitate%20manual%20updates%20in%20CAD%20software%2C%20making%20iteration%20time-consuming%20and%20difficult%20to%20scale.%20To%20address%20this%20limitation%2C%20we%20introduce%20Image2Gcode%2C%20an%20end-to-end%20data-driven%20framework%20that%20bypasses%20the%20CAD%20stage%20and%20generates%20printer-ready%20G-code%20directly%20from%20images%20and%20part%20drawings.%20Instead%20of%20relying%20on%20an%20explicit%203D%20model%2C%20a%20hand-drawn%20or%20captured%202D%20image%20serves%20as%20the%20sole%20input.%20The%20framework%20first%20extracts%20slice-wise%20structural%20cues%20from%20the%20image%20and%20then%20employs%20a%20denoising%20diffusion%20probabilistic%20model%20%28DDPM%29%20over%20G-code%20sequences.%20Through%20iterative%20denoising%2C%20the%20model%20transforms%20Gaussian%20noise%20into%20executable%20print-move%20trajectories%20with%20corresponding%20extrusion%20parameters%2C%20establishing%20a%20direct%20mapping%20from%20visual%20input%20to%20native%20toolpaths.%20By%20producing%20structured%20G-code%20directly%20from%202D%20imagery%2C%20Image2Gcode%20eliminates%20the%20need%20for%20CAD%20or%20STL%20intermediates%2C%20lowering%20the%20entry%20barrier%20for%20additive%20manufacturing%20and%20accelerating%20the%20design-to-fabrication%20cycle.%20This%20approach%20supports%20on-demand%20prototyping%20from%20simple%20sketches%20or%20visual%20references%20and%20integrates%20with%20upstream%202D-to-3D%20reconstruction%20modules%20to%20enable%20an%20automated%20pipeline%20from%20concept%20to%20physical%20artifact.%20The%20result%20is%20a%20flexible%2C%20computationally%20efficient%20framework%20that%20advances%20accessibility%20in%20design%20iteration%2C%20repair%20workflows%2C%20and%20distributed%20manufacturing.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage2Gcode%253A%2520Image-to-G-code%2520Generation%2520for%2520Additive%2520Manufacturing%2520Using%2520Diffusion-Transformer%2520Model%26entry.906535625%3DZiyue%2520Wang%2520and%2520Yayati%2520Jadhav%2520and%2520Peter%2520Pak%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3DMechanical%2520design%2520and%2520manufacturing%2520workflows%2520conventionally%2520begin%2520with%2520conceptual%2520design%252C%2520followed%2520by%2520the%2520creation%2520of%2520a%2520computer-aided%2520design%2520%2528CAD%2529%2520model%2520and%2520fabrication%2520through%2520material-extrusion%2520%2528MEX%2529%2520printing.%2520This%2520process%2520requires%2520converting%2520CAD%2520geometry%2520into%2520machine-readable%2520G-code%2520through%2520slicing%2520and%2520path%2520planning.%2520While%2520each%2520step%2520is%2520well%2520established%252C%2520dependence%2520on%2520CAD%2520modeling%2520remains%2520a%2520major%2520bottleneck%253A%2520constructing%2520object-specific%25203D%2520geometry%2520is%2520slow%2520and%2520poorly%2520suited%2520to%2520rapid%2520prototyping.%2520Even%2520minor%2520design%2520variations%2520typically%2520necessitate%2520manual%2520updates%2520in%2520CAD%2520software%252C%2520making%2520iteration%2520time-consuming%2520and%2520difficult%2520to%2520scale.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520Image2Gcode%252C%2520an%2520end-to-end%2520data-driven%2520framework%2520that%2520bypasses%2520the%2520CAD%2520stage%2520and%2520generates%2520printer-ready%2520G-code%2520directly%2520from%2520images%2520and%2520part%2520drawings.%2520Instead%2520of%2520relying%2520on%2520an%2520explicit%25203D%2520model%252C%2520a%2520hand-drawn%2520or%2520captured%25202D%2520image%2520serves%2520as%2520the%2520sole%2520input.%2520The%2520framework%2520first%2520extracts%2520slice-wise%2520structural%2520cues%2520from%2520the%2520image%2520and%2520then%2520employs%2520a%2520denoising%2520diffusion%2520probabilistic%2520model%2520%2528DDPM%2529%2520over%2520G-code%2520sequences.%2520Through%2520iterative%2520denoising%252C%2520the%2520model%2520transforms%2520Gaussian%2520noise%2520into%2520executable%2520print-move%2520trajectories%2520with%2520corresponding%2520extrusion%2520parameters%252C%2520establishing%2520a%2520direct%2520mapping%2520from%2520visual%2520input%2520to%2520native%2520toolpaths.%2520By%2520producing%2520structured%2520G-code%2520directly%2520from%25202D%2520imagery%252C%2520Image2Gcode%2520eliminates%2520the%2520need%2520for%2520CAD%2520or%2520STL%2520intermediates%252C%2520lowering%2520the%2520entry%2520barrier%2520for%2520additive%2520manufacturing%2520and%2520accelerating%2520the%2520design-to-fabrication%2520cycle.%2520This%2520approach%2520supports%2520on-demand%2520prototyping%2520from%2520simple%2520sketches%2520or%2520visual%2520references%2520and%2520integrates%2520with%2520upstream%25202D-to-3D%2520reconstruction%2520modules%2520to%2520enable%2520an%2520automated%2520pipeline%2520from%2520concept%2520to%2520physical%2520artifact.%2520The%2520result%2520is%2520a%2520flexible%252C%2520computationally%2520efficient%2520framework%2520that%2520advances%2520accessibility%2520in%2520design%2520iteration%252C%2520repair%2520workflows%252C%2520and%2520distributed%2520manufacturing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image2Gcode%3A%20Image-to-G-code%20Generation%20for%20Additive%20Manufacturing%20Using%20Diffusion-Transformer%20Model&entry.906535625=Ziyue%20Wang%20and%20Yayati%20Jadhav%20and%20Peter%20Pak%20and%20Amir%20Barati%20Farimani&entry.1292438233=Mechanical%20design%20and%20manufacturing%20workflows%20conventionally%20begin%20with%20conceptual%20design%2C%20followed%20by%20the%20creation%20of%20a%20computer-aided%20design%20%28CAD%29%20model%20and%20fabrication%20through%20material-extrusion%20%28MEX%29%20printing.%20This%20process%20requires%20converting%20CAD%20geometry%20into%20machine-readable%20G-code%20through%20slicing%20and%20path%20planning.%20While%20each%20step%20is%20well%20established%2C%20dependence%20on%20CAD%20modeling%20remains%20a%20major%20bottleneck%3A%20constructing%20object-specific%203D%20geometry%20is%20slow%20and%20poorly%20suited%20to%20rapid%20prototyping.%20Even%20minor%20design%20variations%20typically%20necessitate%20manual%20updates%20in%20CAD%20software%2C%20making%20iteration%20time-consuming%20and%20difficult%20to%20scale.%20To%20address%20this%20limitation%2C%20we%20introduce%20Image2Gcode%2C%20an%20end-to-end%20data-driven%20framework%20that%20bypasses%20the%20CAD%20stage%20and%20generates%20printer-ready%20G-code%20directly%20from%20images%20and%20part%20drawings.%20Instead%20of%20relying%20on%20an%20explicit%203D%20model%2C%20a%20hand-drawn%20or%20captured%202D%20image%20serves%20as%20the%20sole%20input.%20The%20framework%20first%20extracts%20slice-wise%20structural%20cues%20from%20the%20image%20and%20then%20employs%20a%20denoising%20diffusion%20probabilistic%20model%20%28DDPM%29%20over%20G-code%20sequences.%20Through%20iterative%20denoising%2C%20the%20model%20transforms%20Gaussian%20noise%20into%20executable%20print-move%20trajectories%20with%20corresponding%20extrusion%20parameters%2C%20establishing%20a%20direct%20mapping%20from%20visual%20input%20to%20native%20toolpaths.%20By%20producing%20structured%20G-code%20directly%20from%202D%20imagery%2C%20Image2Gcode%20eliminates%20the%20need%20for%20CAD%20or%20STL%20intermediates%2C%20lowering%20the%20entry%20barrier%20for%20additive%20manufacturing%20and%20accelerating%20the%20design-to-fabrication%20cycle.%20This%20approach%20supports%20on-demand%20prototyping%20from%20simple%20sketches%20or%20visual%20references%20and%20integrates%20with%20upstream%202D-to-3D%20reconstruction%20modules%20to%20enable%20an%20automated%20pipeline%20from%20concept%20to%20physical%20artifact.%20The%20result%20is%20a%20flexible%2C%20computationally%20efficient%20framework%20that%20advances%20accessibility%20in%20design%20iteration%2C%20repair%20workflows%2C%20and%20distributed%20manufacturing.&entry.1838667208=http%3A//arxiv.org/abs/2511.20636v1&entry.124074799=Read"},
{"title": "VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs", "author": "Tianxiang Jiang and Sheng Xia and Yicheng Xu and Linquan Wu and Xiangyu Zeng and Limin Wang and Yu Qiao and Yi Wang", "abstract": "While Multimodal Large Language Models (MLLMs) have become adept at recognizing objects, they often lack the intuitive, human-like understanding of the world's underlying physical and social principles. This high-level vision-grounded semantics, which we term visual knowledge, forms a bridge between perception and reasoning, yet remains an underexplored area in current MLLMs. To systematically evaluate this capability, we present VKnowU, a comprehensive benchmark featuring 1,680 questions in 1,249 videos, covering 8 core types of visual knowledge spanning both world-centric (e.g., intuitive physics) and human-centric (e.g., subjective intentions). Evaluation of 23 SOTA MLLMs reveals that leading models still fall short of human performance, with particularly notable gaps in the world-centric. To bridge this gap, we introduce a new dataset, VKnowQA, and VideoKnow+, a baseline model that explicitly incorporates visual knowledge into MLLMs. VideoKnow+ follows a structured See-Think-Answer paradigm and adopts reinforcement learning with visual knowledge reward, achieving a +3.7% improvement on VKnowU and consistent gains on MVBench, Video-MME, and MMVU. Our work highlights visual knowledge as a missing cornerstone for developing more generalizable MLLMs that can not only see but also truly understand our physical and social worlds.", "link": "http://arxiv.org/abs/2511.20272v1", "date": "2025-11-25", "relevancy": 2.9918, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6146}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VKnowU%3A%20Evaluating%20Visual%20Knowledge%20Understanding%20in%20Multimodal%20LLMs&body=Title%3A%20VKnowU%3A%20Evaluating%20Visual%20Knowledge%20Understanding%20in%20Multimodal%20LLMs%0AAuthor%3A%20Tianxiang%20Jiang%20and%20Sheng%20Xia%20and%20Yicheng%20Xu%20and%20Linquan%20Wu%20and%20Xiangyu%20Zeng%20and%20Limin%20Wang%20and%20Yu%20Qiao%20and%20Yi%20Wang%0AAbstract%3A%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20become%20adept%20at%20recognizing%20objects%2C%20they%20often%20lack%20the%20intuitive%2C%20human-like%20understanding%20of%20the%20world%27s%20underlying%20physical%20and%20social%20principles.%20This%20high-level%20vision-grounded%20semantics%2C%20which%20we%20term%20visual%20knowledge%2C%20forms%20a%20bridge%20between%20perception%20and%20reasoning%2C%20yet%20remains%20an%20underexplored%20area%20in%20current%20MLLMs.%20To%20systematically%20evaluate%20this%20capability%2C%20we%20present%20VKnowU%2C%20a%20comprehensive%20benchmark%20featuring%201%2C680%20questions%20in%201%2C249%20videos%2C%20covering%208%20core%20types%20of%20visual%20knowledge%20spanning%20both%20world-centric%20%28e.g.%2C%20intuitive%20physics%29%20and%20human-centric%20%28e.g.%2C%20subjective%20intentions%29.%20Evaluation%20of%2023%20SOTA%20MLLMs%20reveals%20that%20leading%20models%20still%20fall%20short%20of%20human%20performance%2C%20with%20particularly%20notable%20gaps%20in%20the%20world-centric.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20new%20dataset%2C%20VKnowQA%2C%20and%20VideoKnow%2B%2C%20a%20baseline%20model%20that%20explicitly%20incorporates%20visual%20knowledge%20into%20MLLMs.%20VideoKnow%2B%20follows%20a%20structured%20See-Think-Answer%20paradigm%20and%20adopts%20reinforcement%20learning%20with%20visual%20knowledge%20reward%2C%20achieving%20a%20%2B3.7%25%20improvement%20on%20VKnowU%20and%20consistent%20gains%20on%20MVBench%2C%20Video-MME%2C%20and%20MMVU.%20Our%20work%20highlights%20visual%20knowledge%20as%20a%20missing%20cornerstone%20for%20developing%20more%20generalizable%20MLLMs%20that%20can%20not%20only%20see%20but%20also%20truly%20understand%20our%20physical%20and%20social%20worlds.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVKnowU%253A%2520Evaluating%2520Visual%2520Knowledge%2520Understanding%2520in%2520Multimodal%2520LLMs%26entry.906535625%3DTianxiang%2520Jiang%2520and%2520Sheng%2520Xia%2520and%2520Yicheng%2520Xu%2520and%2520Linquan%2520Wu%2520and%2520Xiangyu%2520Zeng%2520and%2520Limin%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Yi%2520Wang%26entry.1292438233%3DWhile%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520become%2520adept%2520at%2520recognizing%2520objects%252C%2520they%2520often%2520lack%2520the%2520intuitive%252C%2520human-like%2520understanding%2520of%2520the%2520world%2527s%2520underlying%2520physical%2520and%2520social%2520principles.%2520This%2520high-level%2520vision-grounded%2520semantics%252C%2520which%2520we%2520term%2520visual%2520knowledge%252C%2520forms%2520a%2520bridge%2520between%2520perception%2520and%2520reasoning%252C%2520yet%2520remains%2520an%2520underexplored%2520area%2520in%2520current%2520MLLMs.%2520To%2520systematically%2520evaluate%2520this%2520capability%252C%2520we%2520present%2520VKnowU%252C%2520a%2520comprehensive%2520benchmark%2520featuring%25201%252C680%2520questions%2520in%25201%252C249%2520videos%252C%2520covering%25208%2520core%2520types%2520of%2520visual%2520knowledge%2520spanning%2520both%2520world-centric%2520%2528e.g.%252C%2520intuitive%2520physics%2529%2520and%2520human-centric%2520%2528e.g.%252C%2520subjective%2520intentions%2529.%2520Evaluation%2520of%252023%2520SOTA%2520MLLMs%2520reveals%2520that%2520leading%2520models%2520still%2520fall%2520short%2520of%2520human%2520performance%252C%2520with%2520particularly%2520notable%2520gaps%2520in%2520the%2520world-centric.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520new%2520dataset%252C%2520VKnowQA%252C%2520and%2520VideoKnow%252B%252C%2520a%2520baseline%2520model%2520that%2520explicitly%2520incorporates%2520visual%2520knowledge%2520into%2520MLLMs.%2520VideoKnow%252B%2520follows%2520a%2520structured%2520See-Think-Answer%2520paradigm%2520and%2520adopts%2520reinforcement%2520learning%2520with%2520visual%2520knowledge%2520reward%252C%2520achieving%2520a%2520%252B3.7%2525%2520improvement%2520on%2520VKnowU%2520and%2520consistent%2520gains%2520on%2520MVBench%252C%2520Video-MME%252C%2520and%2520MMVU.%2520Our%2520work%2520highlights%2520visual%2520knowledge%2520as%2520a%2520missing%2520cornerstone%2520for%2520developing%2520more%2520generalizable%2520MLLMs%2520that%2520can%2520not%2520only%2520see%2520but%2520also%2520truly%2520understand%2520our%2520physical%2520and%2520social%2520worlds.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VKnowU%3A%20Evaluating%20Visual%20Knowledge%20Understanding%20in%20Multimodal%20LLMs&entry.906535625=Tianxiang%20Jiang%20and%20Sheng%20Xia%20and%20Yicheng%20Xu%20and%20Linquan%20Wu%20and%20Xiangyu%20Zeng%20and%20Limin%20Wang%20and%20Yu%20Qiao%20and%20Yi%20Wang&entry.1292438233=While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20become%20adept%20at%20recognizing%20objects%2C%20they%20often%20lack%20the%20intuitive%2C%20human-like%20understanding%20of%20the%20world%27s%20underlying%20physical%20and%20social%20principles.%20This%20high-level%20vision-grounded%20semantics%2C%20which%20we%20term%20visual%20knowledge%2C%20forms%20a%20bridge%20between%20perception%20and%20reasoning%2C%20yet%20remains%20an%20underexplored%20area%20in%20current%20MLLMs.%20To%20systematically%20evaluate%20this%20capability%2C%20we%20present%20VKnowU%2C%20a%20comprehensive%20benchmark%20featuring%201%2C680%20questions%20in%201%2C249%20videos%2C%20covering%208%20core%20types%20of%20visual%20knowledge%20spanning%20both%20world-centric%20%28e.g.%2C%20intuitive%20physics%29%20and%20human-centric%20%28e.g.%2C%20subjective%20intentions%29.%20Evaluation%20of%2023%20SOTA%20MLLMs%20reveals%20that%20leading%20models%20still%20fall%20short%20of%20human%20performance%2C%20with%20particularly%20notable%20gaps%20in%20the%20world-centric.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20new%20dataset%2C%20VKnowQA%2C%20and%20VideoKnow%2B%2C%20a%20baseline%20model%20that%20explicitly%20incorporates%20visual%20knowledge%20into%20MLLMs.%20VideoKnow%2B%20follows%20a%20structured%20See-Think-Answer%20paradigm%20and%20adopts%20reinforcement%20learning%20with%20visual%20knowledge%20reward%2C%20achieving%20a%20%2B3.7%25%20improvement%20on%20VKnowU%20and%20consistent%20gains%20on%20MVBench%2C%20Video-MME%2C%20and%20MMVU.%20Our%20work%20highlights%20visual%20knowledge%20as%20a%20missing%20cornerstone%20for%20developing%20more%20generalizable%20MLLMs%20that%20can%20not%20only%20see%20but%20also%20truly%20understand%20our%20physical%20and%20social%20worlds.&entry.1838667208=http%3A//arxiv.org/abs/2511.20272v1&entry.124074799=Read"},
{"title": "Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?", "author": "Itay Cohen and Ethan Fetaya and Amir Rosenfeld", "abstract": "Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired \"real\"/\"lookalike\" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.", "link": "http://arxiv.org/abs/2511.19200v2", "date": "2025-11-25", "relevancy": 2.9838, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6118}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Modern%20Vision%20Models%20Understand%20the%20Difference%20Between%20an%20Object%20and%20a%20Look-alike%3F&body=Title%3A%20Can%20Modern%20Vision%20Models%20Understand%20the%20Difference%20Between%20an%20Object%20and%20a%20Look-alike%3F%0AAuthor%3A%20Itay%20Cohen%20and%20Ethan%20Fetaya%20and%20Amir%20Rosenfeld%0AAbstract%3A%20Recent%20advances%20in%20computer%20vision%20have%20yielded%20models%20with%20strong%20performance%20on%20recognition%20benchmarks%3B%20however%2C%20significant%20gaps%20remain%20in%20comparison%20to%20human%20perception.%20One%20subtle%20ability%20is%20to%20judge%20whether%20an%20image%20looks%20like%20a%20given%20object%20without%20being%20an%20instance%20of%20that%20object.%20We%20study%20whether%20vision-language%20models%20such%20as%20CLIP%20capture%20this%20distinction.%20We%20curated%20a%20dataset%20named%20RoLA%20%28Real%20or%20Lookalike%29%20of%20real%20and%20lookalike%20exemplars%20%28e.g.%2C%20toys%2C%20statues%2C%20drawings%2C%20pareidolia%29%20across%20multiple%20categories%2C%20and%20first%20evaluate%20a%20prompt-based%20baseline%20with%20paired%20%22real%22/%22lookalike%22%20prompts.%20We%20then%20estimate%20a%20direction%20in%20CLIP%27s%20embedding%20space%20that%20moves%20representations%20between%20real%20and%20lookalike.%20Applying%20this%20direction%20to%20image%20and%20text%20embeddings%20improves%20discrimination%20in%20cross-modal%20retrieval%20on%20Conceptual12M%2C%20and%20also%20enhances%20captions%20produced%20by%20a%20CLIP%20prefix%20captioner.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19200v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Modern%2520Vision%2520Models%2520Understand%2520the%2520Difference%2520Between%2520an%2520Object%2520and%2520a%2520Look-alike%253F%26entry.906535625%3DItay%2520Cohen%2520and%2520Ethan%2520Fetaya%2520and%2520Amir%2520Rosenfeld%26entry.1292438233%3DRecent%2520advances%2520in%2520computer%2520vision%2520have%2520yielded%2520models%2520with%2520strong%2520performance%2520on%2520recognition%2520benchmarks%253B%2520however%252C%2520significant%2520gaps%2520remain%2520in%2520comparison%2520to%2520human%2520perception.%2520One%2520subtle%2520ability%2520is%2520to%2520judge%2520whether%2520an%2520image%2520looks%2520like%2520a%2520given%2520object%2520without%2520being%2520an%2520instance%2520of%2520that%2520object.%2520We%2520study%2520whether%2520vision-language%2520models%2520such%2520as%2520CLIP%2520capture%2520this%2520distinction.%2520We%2520curated%2520a%2520dataset%2520named%2520RoLA%2520%2528Real%2520or%2520Lookalike%2529%2520of%2520real%2520and%2520lookalike%2520exemplars%2520%2528e.g.%252C%2520toys%252C%2520statues%252C%2520drawings%252C%2520pareidolia%2529%2520across%2520multiple%2520categories%252C%2520and%2520first%2520evaluate%2520a%2520prompt-based%2520baseline%2520with%2520paired%2520%2522real%2522/%2522lookalike%2522%2520prompts.%2520We%2520then%2520estimate%2520a%2520direction%2520in%2520CLIP%2527s%2520embedding%2520space%2520that%2520moves%2520representations%2520between%2520real%2520and%2520lookalike.%2520Applying%2520this%2520direction%2520to%2520image%2520and%2520text%2520embeddings%2520improves%2520discrimination%2520in%2520cross-modal%2520retrieval%2520on%2520Conceptual12M%252C%2520and%2520also%2520enhances%2520captions%2520produced%2520by%2520a%2520CLIP%2520prefix%2520captioner.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19200v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Modern%20Vision%20Models%20Understand%20the%20Difference%20Between%20an%20Object%20and%20a%20Look-alike%3F&entry.906535625=Itay%20Cohen%20and%20Ethan%20Fetaya%20and%20Amir%20Rosenfeld&entry.1292438233=Recent%20advances%20in%20computer%20vision%20have%20yielded%20models%20with%20strong%20performance%20on%20recognition%20benchmarks%3B%20however%2C%20significant%20gaps%20remain%20in%20comparison%20to%20human%20perception.%20One%20subtle%20ability%20is%20to%20judge%20whether%20an%20image%20looks%20like%20a%20given%20object%20without%20being%20an%20instance%20of%20that%20object.%20We%20study%20whether%20vision-language%20models%20such%20as%20CLIP%20capture%20this%20distinction.%20We%20curated%20a%20dataset%20named%20RoLA%20%28Real%20or%20Lookalike%29%20of%20real%20and%20lookalike%20exemplars%20%28e.g.%2C%20toys%2C%20statues%2C%20drawings%2C%20pareidolia%29%20across%20multiple%20categories%2C%20and%20first%20evaluate%20a%20prompt-based%20baseline%20with%20paired%20%22real%22/%22lookalike%22%20prompts.%20We%20then%20estimate%20a%20direction%20in%20CLIP%27s%20embedding%20space%20that%20moves%20representations%20between%20real%20and%20lookalike.%20Applying%20this%20direction%20to%20image%20and%20text%20embeddings%20improves%20discrimination%20in%20cross-modal%20retrieval%20on%20Conceptual12M%2C%20and%20also%20enhances%20captions%20produced%20by%20a%20CLIP%20prefix%20captioner.&entry.1838667208=http%3A//arxiv.org/abs/2511.19200v2&entry.124074799=Read"},
{"title": "Vision-Language Memory for Spatial Reasoning", "author": "Zuntao Liu and Yi Du and Taimeng Fu and Shaoshu Su and Cherie Ho and Chen Wang", "abstract": "Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.", "link": "http://arxiv.org/abs/2511.20644v1", "date": "2025-11-25", "relevancy": 2.9623, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Memory%20for%20Spatial%20Reasoning&body=Title%3A%20Vision-Language%20Memory%20for%20Spatial%20Reasoning%0AAuthor%3A%20Zuntao%20Liu%20and%20Yi%20Du%20and%20Taimeng%20Fu%20and%20Shaoshu%20Su%20and%20Cherie%20Ho%20and%20Chen%20Wang%0AAbstract%3A%20Spatial%20reasoning%20is%20a%20critical%20capability%20for%20intelligent%20robots%2C%20yet%20current%20vision-language%20models%20%28VLMs%29%20still%20fall%20short%20of%20human-level%20performance%20in%20video-based%20spatial%20reasoning.%20This%20gap%20mainly%20stems%20from%20two%20challenges%3A%20a%20semantic-geometric%20misalignment%20that%20prevents%20consistent%203D%20understanding%2C%20and%20the%20absence%20of%20persistent%20memory%20to%20retain%203D%20representation%20and%20understanding%20over%20time.%20To%20address%20these%20limitations%2C%20we%20present%20VLM%24%5E2%24%2C%20a%20Vision-Language%20Model%20with%20persistent%20Memory%20for%20spatial%20reasoning%20with%20a%20view-consistent%2C%203D-aware%20representation%20purely%20from%202D%20video.%20Specifically%2C%20to%20enhance%20long-horizon%20reasoning%2C%20we%20incorporate%20a%20dual-memory%20module%2C%20consisting%20of%20a%20working%20memory%20that%20operates%20as%20a%20sliding%20window%20to%20focus%20on%20immediate%20context%2C%20and%20an%20episodic%20memory%20that%20consolidates%20and%20stores%20critical%20long-term%20information.%20This%20design%20enables%20efficient%20and%20long-horizon%20spatial%20reasoning%20with%20a%20fixed%20computational%20cost.%20Extensive%20experiments%20on%20multiple%20benchmarks%20show%20that%20VLM%24%5E2%24%20achieves%20state-of-the-art%20performance%20among%20video-only%20models%2C%20significantly%20advancing%20the%20frontier%20of%20visual-spatial%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Memory%2520for%2520Spatial%2520Reasoning%26entry.906535625%3DZuntao%2520Liu%2520and%2520Yi%2520Du%2520and%2520Taimeng%2520Fu%2520and%2520Shaoshu%2520Su%2520and%2520Cherie%2520Ho%2520and%2520Chen%2520Wang%26entry.1292438233%3DSpatial%2520reasoning%2520is%2520a%2520critical%2520capability%2520for%2520intelligent%2520robots%252C%2520yet%2520current%2520vision-language%2520models%2520%2528VLMs%2529%2520still%2520fall%2520short%2520of%2520human-level%2520performance%2520in%2520video-based%2520spatial%2520reasoning.%2520This%2520gap%2520mainly%2520stems%2520from%2520two%2520challenges%253A%2520a%2520semantic-geometric%2520misalignment%2520that%2520prevents%2520consistent%25203D%2520understanding%252C%2520and%2520the%2520absence%2520of%2520persistent%2520memory%2520to%2520retain%25203D%2520representation%2520and%2520understanding%2520over%2520time.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520VLM%2524%255E2%2524%252C%2520a%2520Vision-Language%2520Model%2520with%2520persistent%2520Memory%2520for%2520spatial%2520reasoning%2520with%2520a%2520view-consistent%252C%25203D-aware%2520representation%2520purely%2520from%25202D%2520video.%2520Specifically%252C%2520to%2520enhance%2520long-horizon%2520reasoning%252C%2520we%2520incorporate%2520a%2520dual-memory%2520module%252C%2520consisting%2520of%2520a%2520working%2520memory%2520that%2520operates%2520as%2520a%2520sliding%2520window%2520to%2520focus%2520on%2520immediate%2520context%252C%2520and%2520an%2520episodic%2520memory%2520that%2520consolidates%2520and%2520stores%2520critical%2520long-term%2520information.%2520This%2520design%2520enables%2520efficient%2520and%2520long-horizon%2520spatial%2520reasoning%2520with%2520a%2520fixed%2520computational%2520cost.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmarks%2520show%2520that%2520VLM%2524%255E2%2524%2520achieves%2520state-of-the-art%2520performance%2520among%2520video-only%2520models%252C%2520significantly%2520advancing%2520the%2520frontier%2520of%2520visual-spatial%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Memory%20for%20Spatial%20Reasoning&entry.906535625=Zuntao%20Liu%20and%20Yi%20Du%20and%20Taimeng%20Fu%20and%20Shaoshu%20Su%20and%20Cherie%20Ho%20and%20Chen%20Wang&entry.1292438233=Spatial%20reasoning%20is%20a%20critical%20capability%20for%20intelligent%20robots%2C%20yet%20current%20vision-language%20models%20%28VLMs%29%20still%20fall%20short%20of%20human-level%20performance%20in%20video-based%20spatial%20reasoning.%20This%20gap%20mainly%20stems%20from%20two%20challenges%3A%20a%20semantic-geometric%20misalignment%20that%20prevents%20consistent%203D%20understanding%2C%20and%20the%20absence%20of%20persistent%20memory%20to%20retain%203D%20representation%20and%20understanding%20over%20time.%20To%20address%20these%20limitations%2C%20we%20present%20VLM%24%5E2%24%2C%20a%20Vision-Language%20Model%20with%20persistent%20Memory%20for%20spatial%20reasoning%20with%20a%20view-consistent%2C%203D-aware%20representation%20purely%20from%202D%20video.%20Specifically%2C%20to%20enhance%20long-horizon%20reasoning%2C%20we%20incorporate%20a%20dual-memory%20module%2C%20consisting%20of%20a%20working%20memory%20that%20operates%20as%20a%20sliding%20window%20to%20focus%20on%20immediate%20context%2C%20and%20an%20episodic%20memory%20that%20consolidates%20and%20stores%20critical%20long-term%20information.%20This%20design%20enables%20efficient%20and%20long-horizon%20spatial%20reasoning%20with%20a%20fixed%20computational%20cost.%20Extensive%20experiments%20on%20multiple%20benchmarks%20show%20that%20VLM%24%5E2%24%20achieves%20state-of-the-art%20performance%20among%20video-only%20models%2C%20significantly%20advancing%20the%20frontier%20of%20visual-spatial%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2511.20644v1&entry.124074799=Read"},
{"title": "Zoo3D: Zero-Shot 3D Object Detection at Scene Level", "author": "Andrey Lemeshko and Bulat Gabdullin and Nikita Drozdov and Anton Konushin and Danila Rukhovich and Maksim Kolodiazhnyi", "abstract": "3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .", "link": "http://arxiv.org/abs/2511.20253v1", "date": "2025-11-25", "relevancy": 2.9555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6014}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zoo3D%3A%20Zero-Shot%203D%20Object%20Detection%20at%20Scene%20Level&body=Title%3A%20Zoo3D%3A%20Zero-Shot%203D%20Object%20Detection%20at%20Scene%20Level%0AAuthor%3A%20Andrey%20Lemeshko%20and%20Bulat%20Gabdullin%20and%20Nikita%20Drozdov%20and%20Anton%20Konushin%20and%20Danila%20Rukhovich%20and%20Maksim%20Kolodiazhnyi%0AAbstract%3A%203D%20object%20detection%20is%20fundamental%20for%20spatial%20understanding.%20Real-world%20environments%20demand%20models%20capable%20of%20recognizing%20diverse%2C%20previously%20unseen%20objects%2C%20which%20remains%20a%20major%20limitation%20of%20closed-set%20methods.%20Existing%20open-vocabulary%203D%20detectors%20relax%20annotation%20requirements%20but%20still%20depend%20on%20training%20scenes%2C%20either%20as%20point%20clouds%20or%20images.%20We%20take%20this%20a%20step%20further%20by%20introducing%20Zoo3D%2C%20the%20first%20training-free%203D%20object%20detection%20framework.%20Our%20method%20constructs%203D%20bounding%20boxes%20via%20graph%20clustering%20of%202D%20instance%20masks%2C%20then%20assigns%20semantic%20labels%20using%20a%20novel%20open-vocabulary%20module%20with%20best-view%20selection%20and%20view-consensus%20mask%20generation.%20Zoo3D%20operates%20in%20two%20modes%3A%20the%20zero-shot%20Zoo3D%24_0%24%2C%20which%20requires%20no%20training%20at%20all%2C%20and%20the%20self-supervised%20Zoo3D%24_1%24%2C%20which%20refines%203D%20box%20prediction%20by%20training%20a%20class-agnostic%20detector%20on%20Zoo3D%24_0%24-generated%20pseudo%20labels.%20Furthermore%2C%20we%20extend%20Zoo3D%20beyond%20point%20clouds%20to%20work%20directly%20with%20posed%20and%20even%20unposed%20images.%20Across%20ScanNet200%20and%20ARKitScenes%20benchmarks%2C%20both%20Zoo3D%24_0%24%20and%20Zoo3D%24_1%24%20achieve%20state-of-the-art%20results%20in%20open-vocabulary%203D%20object%20detection.%20Remarkably%2C%20our%20zero-shot%20Zoo3D%24_0%24%20outperforms%20all%20existing%20self-supervised%20methods%2C%20hence%20demonstrating%20the%20power%20and%20adaptability%20of%20training-free%2C%20off-the-shelf%20approaches%20for%20real-world%203D%20understanding.%20Code%20is%20available%20at%20https%3A//github.com/col14m/zoo3d%20.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZoo3D%253A%2520Zero-Shot%25203D%2520Object%2520Detection%2520at%2520Scene%2520Level%26entry.906535625%3DAndrey%2520Lemeshko%2520and%2520Bulat%2520Gabdullin%2520and%2520Nikita%2520Drozdov%2520and%2520Anton%2520Konushin%2520and%2520Danila%2520Rukhovich%2520and%2520Maksim%2520Kolodiazhnyi%26entry.1292438233%3D3D%2520object%2520detection%2520is%2520fundamental%2520for%2520spatial%2520understanding.%2520Real-world%2520environments%2520demand%2520models%2520capable%2520of%2520recognizing%2520diverse%252C%2520previously%2520unseen%2520objects%252C%2520which%2520remains%2520a%2520major%2520limitation%2520of%2520closed-set%2520methods.%2520Existing%2520open-vocabulary%25203D%2520detectors%2520relax%2520annotation%2520requirements%2520but%2520still%2520depend%2520on%2520training%2520scenes%252C%2520either%2520as%2520point%2520clouds%2520or%2520images.%2520We%2520take%2520this%2520a%2520step%2520further%2520by%2520introducing%2520Zoo3D%252C%2520the%2520first%2520training-free%25203D%2520object%2520detection%2520framework.%2520Our%2520method%2520constructs%25203D%2520bounding%2520boxes%2520via%2520graph%2520clustering%2520of%25202D%2520instance%2520masks%252C%2520then%2520assigns%2520semantic%2520labels%2520using%2520a%2520novel%2520open-vocabulary%2520module%2520with%2520best-view%2520selection%2520and%2520view-consensus%2520mask%2520generation.%2520Zoo3D%2520operates%2520in%2520two%2520modes%253A%2520the%2520zero-shot%2520Zoo3D%2524_0%2524%252C%2520which%2520requires%2520no%2520training%2520at%2520all%252C%2520and%2520the%2520self-supervised%2520Zoo3D%2524_1%2524%252C%2520which%2520refines%25203D%2520box%2520prediction%2520by%2520training%2520a%2520class-agnostic%2520detector%2520on%2520Zoo3D%2524_0%2524-generated%2520pseudo%2520labels.%2520Furthermore%252C%2520we%2520extend%2520Zoo3D%2520beyond%2520point%2520clouds%2520to%2520work%2520directly%2520with%2520posed%2520and%2520even%2520unposed%2520images.%2520Across%2520ScanNet200%2520and%2520ARKitScenes%2520benchmarks%252C%2520both%2520Zoo3D%2524_0%2524%2520and%2520Zoo3D%2524_1%2524%2520achieve%2520state-of-the-art%2520results%2520in%2520open-vocabulary%25203D%2520object%2520detection.%2520Remarkably%252C%2520our%2520zero-shot%2520Zoo3D%2524_0%2524%2520outperforms%2520all%2520existing%2520self-supervised%2520methods%252C%2520hence%2520demonstrating%2520the%2520power%2520and%2520adaptability%2520of%2520training-free%252C%2520off-the-shelf%2520approaches%2520for%2520real-world%25203D%2520understanding.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/col14m/zoo3d%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zoo3D%3A%20Zero-Shot%203D%20Object%20Detection%20at%20Scene%20Level&entry.906535625=Andrey%20Lemeshko%20and%20Bulat%20Gabdullin%20and%20Nikita%20Drozdov%20and%20Anton%20Konushin%20and%20Danila%20Rukhovich%20and%20Maksim%20Kolodiazhnyi&entry.1292438233=3D%20object%20detection%20is%20fundamental%20for%20spatial%20understanding.%20Real-world%20environments%20demand%20models%20capable%20of%20recognizing%20diverse%2C%20previously%20unseen%20objects%2C%20which%20remains%20a%20major%20limitation%20of%20closed-set%20methods.%20Existing%20open-vocabulary%203D%20detectors%20relax%20annotation%20requirements%20but%20still%20depend%20on%20training%20scenes%2C%20either%20as%20point%20clouds%20or%20images.%20We%20take%20this%20a%20step%20further%20by%20introducing%20Zoo3D%2C%20the%20first%20training-free%203D%20object%20detection%20framework.%20Our%20method%20constructs%203D%20bounding%20boxes%20via%20graph%20clustering%20of%202D%20instance%20masks%2C%20then%20assigns%20semantic%20labels%20using%20a%20novel%20open-vocabulary%20module%20with%20best-view%20selection%20and%20view-consensus%20mask%20generation.%20Zoo3D%20operates%20in%20two%20modes%3A%20the%20zero-shot%20Zoo3D%24_0%24%2C%20which%20requires%20no%20training%20at%20all%2C%20and%20the%20self-supervised%20Zoo3D%24_1%24%2C%20which%20refines%203D%20box%20prediction%20by%20training%20a%20class-agnostic%20detector%20on%20Zoo3D%24_0%24-generated%20pseudo%20labels.%20Furthermore%2C%20we%20extend%20Zoo3D%20beyond%20point%20clouds%20to%20work%20directly%20with%20posed%20and%20even%20unposed%20images.%20Across%20ScanNet200%20and%20ARKitScenes%20benchmarks%2C%20both%20Zoo3D%24_0%24%20and%20Zoo3D%24_1%24%20achieve%20state-of-the-art%20results%20in%20open-vocabulary%203D%20object%20detection.%20Remarkably%2C%20our%20zero-shot%20Zoo3D%24_0%24%20outperforms%20all%20existing%20self-supervised%20methods%2C%20hence%20demonstrating%20the%20power%20and%20adaptability%20of%20training-free%2C%20off-the-shelf%20approaches%20for%20real-world%203D%20understanding.%20Code%20is%20available%20at%20https%3A//github.com/col14m/zoo3d%20.&entry.1838667208=http%3A//arxiv.org/abs/2511.20253v1&entry.124074799=Read"},
{"title": "Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition", "author": "Wei Tang and Zuo-Zheng Wang and Kun Zhang and Tong Wei and Min-Ling Zhang", "abstract": "Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.", "link": "http://arxiv.org/abs/2511.20641v1", "date": "2025-11-25", "relevancy": 2.9457, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6503}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5596}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Power%20of%20Vision-Language%20Models%20for%20Long-Tailed%20Multi-Label%20Visual%20Recognition&body=Title%3A%20Unleashing%20the%20Power%20of%20Vision-Language%20Models%20for%20Long-Tailed%20Multi-Label%20Visual%20Recognition%0AAuthor%3A%20Wei%20Tang%20and%20Zuo-Zheng%20Wang%20and%20Kun%20Zhang%20and%20Tong%20Wei%20and%20Min-Ling%20Zhang%0AAbstract%3A%20Long-tailed%20multi-label%20visual%20recognition%20poses%20a%20significant%20challenge%2C%20as%20images%20typically%20contain%20multiple%20labels%20with%20highly%20imbalanced%20class%20distributions%2C%20leading%20to%20biased%20models%20that%20favor%20head%20classes%20while%20underperforming%20on%20tail%20classes.%20Recent%20efforts%20have%20leveraged%20pre-trained%20vision-language%20models%2C%20such%20as%20CLIP%2C%20alongside%20long-tailed%20learning%20techniques%20to%20exploit%20rich%20visual-textual%20priors%20for%20improved%20performance.%20However%2C%20existing%20methods%20often%20derive%20semantic%20inter-class%20relationships%20directly%20from%20imbalanced%20datasets%2C%20resulting%20in%20unreliable%20correlations%20for%20tail%20classes%20due%20to%20data%20scarcity.%20Moreover%2C%20CLIP%27s%20zero-shot%20paradigm%20is%20optimized%20for%20single-label%20image-text%20matching%2C%20making%20it%20suboptimal%20for%20multi-label%20tasks.%20To%20address%20these%20issues%2C%20we%20propose%20the%20correlation%20adaptation%20prompt%20network%20%28CAPNET%29%2C%20a%20novel%20end-to-end%20framework%20that%20explicitly%20models%20label%20correlations%20from%20CLIP%27s%20textual%20encoder.%20The%20framework%20incorporates%20a%20graph%20convolutional%20network%20for%20label-aware%20propagation%20and%20learnable%20soft%20prompts%20for%20refined%20embeddings.%20It%20utilizes%20a%20distribution-balanced%20Focal%20loss%20with%20class-aware%20re-weighting%20for%20optimized%20training%20under%20imbalance.%20Moreover%2C%20it%20improves%20generalization%20through%20test-time%20ensembling%20and%20realigns%20visual-textual%20modalities%20using%20parameter-efficient%20fine-tuning%20to%20avert%20overfitting%20on%20tail%20classes%20without%20compromising%20head%20class%20performance.%20Extensive%20experiments%20and%20ablation%20studies%20on%20benchmarks%20including%20VOC-LT%2C%20COCO-LT%2C%20and%20NUS-WIDE%20demonstrate%20that%20CAPNET%20achieves%20substantial%20improvements%20over%20state-of-the-art%20methods%2C%20validating%20its%20effectiveness%20for%20real-world%20long-tailed%20multi-label%20visual%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Power%2520of%2520Vision-Language%2520Models%2520for%2520Long-Tailed%2520Multi-Label%2520Visual%2520Recognition%26entry.906535625%3DWei%2520Tang%2520and%2520Zuo-Zheng%2520Wang%2520and%2520Kun%2520Zhang%2520and%2520Tong%2520Wei%2520and%2520Min-Ling%2520Zhang%26entry.1292438233%3DLong-tailed%2520multi-label%2520visual%2520recognition%2520poses%2520a%2520significant%2520challenge%252C%2520as%2520images%2520typically%2520contain%2520multiple%2520labels%2520with%2520highly%2520imbalanced%2520class%2520distributions%252C%2520leading%2520to%2520biased%2520models%2520that%2520favor%2520head%2520classes%2520while%2520underperforming%2520on%2520tail%2520classes.%2520Recent%2520efforts%2520have%2520leveraged%2520pre-trained%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520alongside%2520long-tailed%2520learning%2520techniques%2520to%2520exploit%2520rich%2520visual-textual%2520priors%2520for%2520improved%2520performance.%2520However%252C%2520existing%2520methods%2520often%2520derive%2520semantic%2520inter-class%2520relationships%2520directly%2520from%2520imbalanced%2520datasets%252C%2520resulting%2520in%2520unreliable%2520correlations%2520for%2520tail%2520classes%2520due%2520to%2520data%2520scarcity.%2520Moreover%252C%2520CLIP%2527s%2520zero-shot%2520paradigm%2520is%2520optimized%2520for%2520single-label%2520image-text%2520matching%252C%2520making%2520it%2520suboptimal%2520for%2520multi-label%2520tasks.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520correlation%2520adaptation%2520prompt%2520network%2520%2528CAPNET%2529%252C%2520a%2520novel%2520end-to-end%2520framework%2520that%2520explicitly%2520models%2520label%2520correlations%2520from%2520CLIP%2527s%2520textual%2520encoder.%2520The%2520framework%2520incorporates%2520a%2520graph%2520convolutional%2520network%2520for%2520label-aware%2520propagation%2520and%2520learnable%2520soft%2520prompts%2520for%2520refined%2520embeddings.%2520It%2520utilizes%2520a%2520distribution-balanced%2520Focal%2520loss%2520with%2520class-aware%2520re-weighting%2520for%2520optimized%2520training%2520under%2520imbalance.%2520Moreover%252C%2520it%2520improves%2520generalization%2520through%2520test-time%2520ensembling%2520and%2520realigns%2520visual-textual%2520modalities%2520using%2520parameter-efficient%2520fine-tuning%2520to%2520avert%2520overfitting%2520on%2520tail%2520classes%2520without%2520compromising%2520head%2520class%2520performance.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%2520on%2520benchmarks%2520including%2520VOC-LT%252C%2520COCO-LT%252C%2520and%2520NUS-WIDE%2520demonstrate%2520that%2520CAPNET%2520achieves%2520substantial%2520improvements%2520over%2520state-of-the-art%2520methods%252C%2520validating%2520its%2520effectiveness%2520for%2520real-world%2520long-tailed%2520multi-label%2520visual%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Power%20of%20Vision-Language%20Models%20for%20Long-Tailed%20Multi-Label%20Visual%20Recognition&entry.906535625=Wei%20Tang%20and%20Zuo-Zheng%20Wang%20and%20Kun%20Zhang%20and%20Tong%20Wei%20and%20Min-Ling%20Zhang&entry.1292438233=Long-tailed%20multi-label%20visual%20recognition%20poses%20a%20significant%20challenge%2C%20as%20images%20typically%20contain%20multiple%20labels%20with%20highly%20imbalanced%20class%20distributions%2C%20leading%20to%20biased%20models%20that%20favor%20head%20classes%20while%20underperforming%20on%20tail%20classes.%20Recent%20efforts%20have%20leveraged%20pre-trained%20vision-language%20models%2C%20such%20as%20CLIP%2C%20alongside%20long-tailed%20learning%20techniques%20to%20exploit%20rich%20visual-textual%20priors%20for%20improved%20performance.%20However%2C%20existing%20methods%20often%20derive%20semantic%20inter-class%20relationships%20directly%20from%20imbalanced%20datasets%2C%20resulting%20in%20unreliable%20correlations%20for%20tail%20classes%20due%20to%20data%20scarcity.%20Moreover%2C%20CLIP%27s%20zero-shot%20paradigm%20is%20optimized%20for%20single-label%20image-text%20matching%2C%20making%20it%20suboptimal%20for%20multi-label%20tasks.%20To%20address%20these%20issues%2C%20we%20propose%20the%20correlation%20adaptation%20prompt%20network%20%28CAPNET%29%2C%20a%20novel%20end-to-end%20framework%20that%20explicitly%20models%20label%20correlations%20from%20CLIP%27s%20textual%20encoder.%20The%20framework%20incorporates%20a%20graph%20convolutional%20network%20for%20label-aware%20propagation%20and%20learnable%20soft%20prompts%20for%20refined%20embeddings.%20It%20utilizes%20a%20distribution-balanced%20Focal%20loss%20with%20class-aware%20re-weighting%20for%20optimized%20training%20under%20imbalance.%20Moreover%2C%20it%20improves%20generalization%20through%20test-time%20ensembling%20and%20realigns%20visual-textual%20modalities%20using%20parameter-efficient%20fine-tuning%20to%20avert%20overfitting%20on%20tail%20classes%20without%20compromising%20head%20class%20performance.%20Extensive%20experiments%20and%20ablation%20studies%20on%20benchmarks%20including%20VOC-LT%2C%20COCO-LT%2C%20and%20NUS-WIDE%20demonstrate%20that%20CAPNET%20achieves%20substantial%20improvements%20over%20state-of-the-art%20methods%2C%20validating%20its%20effectiveness%20for%20real-world%20long-tailed%20multi-label%20visual%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2511.20641v1&entry.124074799=Read"},
{"title": "SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation", "author": "Seamie Hayes and Reenu Mohandas and Tim Brophy and Alexandre Boulch and Ganesh Sistu and Ciaran Eising", "abstract": "Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75% reduction in memory footprint, 124% faster inference, and a 5.9% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be made available at https://github.com/seamie6/SuperQuadricOcc.", "link": "http://arxiv.org/abs/2511.17361v2", "date": "2025-11-25", "relevancy": 2.9255, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6298}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.569}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperQuadricOcc%3A%20Multi-Layer%20Gaussian%20Approximation%20of%20Superquadrics%20for%20Real-Time%20Self-Supervised%20Occupancy%20Estimation&body=Title%3A%20SuperQuadricOcc%3A%20Multi-Layer%20Gaussian%20Approximation%20of%20Superquadrics%20for%20Real-Time%20Self-Supervised%20Occupancy%20Estimation%0AAuthor%3A%20Seamie%20Hayes%20and%20Reenu%20Mohandas%20and%20Tim%20Brophy%20and%20Alexandre%20Boulch%20and%20Ganesh%20Sistu%20and%20Ciaran%20Eising%0AAbstract%3A%20Semantic%20occupancy%20estimation%20enables%20comprehensive%20scene%20understanding%20for%20automated%20driving%2C%20providing%20dense%20spatial%20and%20semantic%20information%20essential%20for%20perception%20and%20planning.%20While%20Gaussian%20representations%20have%20been%20widely%20adopted%20in%20self-supervised%20occupancy%20estimation%2C%20the%20deployment%20of%20a%20large%20number%20of%20Gaussian%20primitives%20drastically%20increases%20memory%20requirements%20and%20is%20not%20suitable%20for%20real-time%20inference.%20In%20contrast%2C%20superquadrics%20permit%20reduced%20primitive%20count%20and%20lower%20memory%20requirements%20due%20to%20their%20diverse%20shape%20set.%20However%2C%20implementation%20into%20a%20self-supervised%20occupancy%20model%20is%20nontrivial%20due%20to%20the%20absence%20of%20a%20superquadric%20rasterizer%20to%20enable%20model%20supervision.%20Our%20proposed%20method%2C%20SuperQuadricOcc%2C%20employs%20a%20superquadric-based%20scene%20representation.%20By%20leveraging%20a%20multi-layer%20icosphere-tessellated%20Gaussian%20approximation%20of%20superquadrics%2C%20we%20enable%20Gaussian%20rasterization%20for%20supervision%20during%20training.%20On%20the%20Occ3D%20dataset%2C%20SuperQuadricOcc%20achieves%20a%2075%25%20reduction%20in%20memory%20footprint%2C%20124%25%20faster%20inference%2C%20and%20a%205.9%25%20improvement%20in%20mIoU%20compared%20to%20previous%20Gaussian-based%20methods%2C%20without%20the%20use%20of%20temporal%20labels.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20occupancy%20model%20to%20enable%20real-time%20inference%20while%20maintaining%20competitive%20performance.%20The%20use%20of%20superquadrics%20reduces%20the%20number%20of%20primitives%20required%20for%20scene%20modeling%20by%2084%25%20relative%20to%20Gaussian-based%20approaches.%20Finally%2C%20evaluation%20against%20prior%20methods%20is%20facilitated%20by%20our%20fast%20superquadric%20voxelization%20module.%20The%20code%20will%20be%20made%20available%20at%20https%3A//github.com/seamie6/SuperQuadricOcc.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperQuadricOcc%253A%2520Multi-Layer%2520Gaussian%2520Approximation%2520of%2520Superquadrics%2520for%2520Real-Time%2520Self-Supervised%2520Occupancy%2520Estimation%26entry.906535625%3DSeamie%2520Hayes%2520and%2520Reenu%2520Mohandas%2520and%2520Tim%2520Brophy%2520and%2520Alexandre%2520Boulch%2520and%2520Ganesh%2520Sistu%2520and%2520Ciaran%2520Eising%26entry.1292438233%3DSemantic%2520occupancy%2520estimation%2520enables%2520comprehensive%2520scene%2520understanding%2520for%2520automated%2520driving%252C%2520providing%2520dense%2520spatial%2520and%2520semantic%2520information%2520essential%2520for%2520perception%2520and%2520planning.%2520While%2520Gaussian%2520representations%2520have%2520been%2520widely%2520adopted%2520in%2520self-supervised%2520occupancy%2520estimation%252C%2520the%2520deployment%2520of%2520a%2520large%2520number%2520of%2520Gaussian%2520primitives%2520drastically%2520increases%2520memory%2520requirements%2520and%2520is%2520not%2520suitable%2520for%2520real-time%2520inference.%2520In%2520contrast%252C%2520superquadrics%2520permit%2520reduced%2520primitive%2520count%2520and%2520lower%2520memory%2520requirements%2520due%2520to%2520their%2520diverse%2520shape%2520set.%2520However%252C%2520implementation%2520into%2520a%2520self-supervised%2520occupancy%2520model%2520is%2520nontrivial%2520due%2520to%2520the%2520absence%2520of%2520a%2520superquadric%2520rasterizer%2520to%2520enable%2520model%2520supervision.%2520Our%2520proposed%2520method%252C%2520SuperQuadricOcc%252C%2520employs%2520a%2520superquadric-based%2520scene%2520representation.%2520By%2520leveraging%2520a%2520multi-layer%2520icosphere-tessellated%2520Gaussian%2520approximation%2520of%2520superquadrics%252C%2520we%2520enable%2520Gaussian%2520rasterization%2520for%2520supervision%2520during%2520training.%2520On%2520the%2520Occ3D%2520dataset%252C%2520SuperQuadricOcc%2520achieves%2520a%252075%2525%2520reduction%2520in%2520memory%2520footprint%252C%2520124%2525%2520faster%2520inference%252C%2520and%2520a%25205.9%2525%2520improvement%2520in%2520mIoU%2520compared%2520to%2520previous%2520Gaussian-based%2520methods%252C%2520without%2520the%2520use%2520of%2520temporal%2520labels.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520occupancy%2520model%2520to%2520enable%2520real-time%2520inference%2520while%2520maintaining%2520competitive%2520performance.%2520The%2520use%2520of%2520superquadrics%2520reduces%2520the%2520number%2520of%2520primitives%2520required%2520for%2520scene%2520modeling%2520by%252084%2525%2520relative%2520to%2520Gaussian-based%2520approaches.%2520Finally%252C%2520evaluation%2520against%2520prior%2520methods%2520is%2520facilitated%2520by%2520our%2520fast%2520superquadric%2520voxelization%2520module.%2520The%2520code%2520will%2520be%2520made%2520available%2520at%2520https%253A//github.com/seamie6/SuperQuadricOcc.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperQuadricOcc%3A%20Multi-Layer%20Gaussian%20Approximation%20of%20Superquadrics%20for%20Real-Time%20Self-Supervised%20Occupancy%20Estimation&entry.906535625=Seamie%20Hayes%20and%20Reenu%20Mohandas%20and%20Tim%20Brophy%20and%20Alexandre%20Boulch%20and%20Ganesh%20Sistu%20and%20Ciaran%20Eising&entry.1292438233=Semantic%20occupancy%20estimation%20enables%20comprehensive%20scene%20understanding%20for%20automated%20driving%2C%20providing%20dense%20spatial%20and%20semantic%20information%20essential%20for%20perception%20and%20planning.%20While%20Gaussian%20representations%20have%20been%20widely%20adopted%20in%20self-supervised%20occupancy%20estimation%2C%20the%20deployment%20of%20a%20large%20number%20of%20Gaussian%20primitives%20drastically%20increases%20memory%20requirements%20and%20is%20not%20suitable%20for%20real-time%20inference.%20In%20contrast%2C%20superquadrics%20permit%20reduced%20primitive%20count%20and%20lower%20memory%20requirements%20due%20to%20their%20diverse%20shape%20set.%20However%2C%20implementation%20into%20a%20self-supervised%20occupancy%20model%20is%20nontrivial%20due%20to%20the%20absence%20of%20a%20superquadric%20rasterizer%20to%20enable%20model%20supervision.%20Our%20proposed%20method%2C%20SuperQuadricOcc%2C%20employs%20a%20superquadric-based%20scene%20representation.%20By%20leveraging%20a%20multi-layer%20icosphere-tessellated%20Gaussian%20approximation%20of%20superquadrics%2C%20we%20enable%20Gaussian%20rasterization%20for%20supervision%20during%20training.%20On%20the%20Occ3D%20dataset%2C%20SuperQuadricOcc%20achieves%20a%2075%25%20reduction%20in%20memory%20footprint%2C%20124%25%20faster%20inference%2C%20and%20a%205.9%25%20improvement%20in%20mIoU%20compared%20to%20previous%20Gaussian-based%20methods%2C%20without%20the%20use%20of%20temporal%20labels.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20occupancy%20model%20to%20enable%20real-time%20inference%20while%20maintaining%20competitive%20performance.%20The%20use%20of%20superquadrics%20reduces%20the%20number%20of%20primitives%20required%20for%20scene%20modeling%20by%2084%25%20relative%20to%20Gaussian-based%20approaches.%20Finally%2C%20evaluation%20against%20prior%20methods%20is%20facilitated%20by%20our%20fast%20superquadric%20voxelization%20module.%20The%20code%20will%20be%20made%20available%20at%20https%3A//github.com/seamie6/SuperQuadricOcc.&entry.1838667208=http%3A//arxiv.org/abs/2511.17361v2&entry.124074799=Read"},
{"title": "Multi-view Surface Reconstruction Using Normal and Reflectance Cues", "author": "Robin Bruneau and Baptiste Brument and Yvain Qu\u00e9au and Jean M\u00e9lou and Fran\u00e7ois Bernard Lauze and Jean-Denis Durou and Lilian Calvet", "abstract": "Achieving high-fidelity 3D surface reconstruction while preserving fine details remains challenging, especially in the presence of materials with complex reflectance properties and without a dense-view setup. In this paper, we introduce a versatile framework that incorporates multi-view normal and optionally reflectance maps into radiance-based surface reconstruction. Our approach employs a pixel-wise joint re-parametrization of reflectance and surface normals, representing them as a vector of radiances under simulated, varying illumination. This formulation enables seamless incorporation into standard surface reconstruction pipelines, such as traditional multi-view stereo (MVS) frameworks or modern neural volume rendering (NVR) ones. Combined with the latter, our approach achieves state-of-the-art performance on multi-view photometric stereo (MVPS) benchmark datasets, including DiLiGenT-MV, LUCES-MV and Skoltech3D. In particular, our method excels in reconstructing fine-grained details and handling challenging visibility conditions. The present paper is an extended version of the earlier conference paper by Brument et al. (in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024), featuring an accelerated and more robust algorithm as well as a broader empirical evaluation. The code and data relative to this article is available at https://github.com/RobinBruneau/RNb-NeuS2.", "link": "http://arxiv.org/abs/2506.04115v2", "date": "2025-11-25", "relevancy": 2.9248, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.592}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.592}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Surface%20Reconstruction%20Using%20Normal%20and%20Reflectance%20Cues&body=Title%3A%20Multi-view%20Surface%20Reconstruction%20Using%20Normal%20and%20Reflectance%20Cues%0AAuthor%3A%20Robin%20Bruneau%20and%20Baptiste%20Brument%20and%20Yvain%20Qu%C3%A9au%20and%20Jean%20M%C3%A9lou%20and%20Fran%C3%A7ois%20Bernard%20Lauze%20and%20Jean-Denis%20Durou%20and%20Lilian%20Calvet%0AAbstract%3A%20Achieving%20high-fidelity%203D%20surface%20reconstruction%20while%20preserving%20fine%20details%20remains%20challenging%2C%20especially%20in%20the%20presence%20of%20materials%20with%20complex%20reflectance%20properties%20and%20without%20a%20dense-view%20setup.%20In%20this%20paper%2C%20we%20introduce%20a%20versatile%20framework%20that%20incorporates%20multi-view%20normal%20and%20optionally%20reflectance%20maps%20into%20radiance-based%20surface%20reconstruction.%20Our%20approach%20employs%20a%20pixel-wise%20joint%20re-parametrization%20of%20reflectance%20and%20surface%20normals%2C%20representing%20them%20as%20a%20vector%20of%20radiances%20under%20simulated%2C%20varying%20illumination.%20This%20formulation%20enables%20seamless%20incorporation%20into%20standard%20surface%20reconstruction%20pipelines%2C%20such%20as%20traditional%20multi-view%20stereo%20%28MVS%29%20frameworks%20or%20modern%20neural%20volume%20rendering%20%28NVR%29%20ones.%20Combined%20with%20the%20latter%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20multi-view%20photometric%20stereo%20%28MVPS%29%20benchmark%20datasets%2C%20including%20DiLiGenT-MV%2C%20LUCES-MV%20and%20Skoltech3D.%20In%20particular%2C%20our%20method%20excels%20in%20reconstructing%20fine-grained%20details%20and%20handling%20challenging%20visibility%20conditions.%20The%20present%20paper%20is%20an%20extended%20version%20of%20the%20earlier%20conference%20paper%20by%20Brument%20et%20al.%20%28in%20Proceedings%20of%20the%20IEEE/CVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%2C%202024%29%2C%20featuring%20an%20accelerated%20and%20more%20robust%20algorithm%20as%20well%20as%20a%20broader%20empirical%20evaluation.%20The%20code%20and%20data%20relative%20to%20this%20article%20is%20available%20at%20https%3A//github.com/RobinBruneau/RNb-NeuS2.%0ALink%3A%20http%3A//arxiv.org/abs/2506.04115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Surface%2520Reconstruction%2520Using%2520Normal%2520and%2520Reflectance%2520Cues%26entry.906535625%3DRobin%2520Bruneau%2520and%2520Baptiste%2520Brument%2520and%2520Yvain%2520Qu%25C3%25A9au%2520and%2520Jean%2520M%25C3%25A9lou%2520and%2520Fran%25C3%25A7ois%2520Bernard%2520Lauze%2520and%2520Jean-Denis%2520Durou%2520and%2520Lilian%2520Calvet%26entry.1292438233%3DAchieving%2520high-fidelity%25203D%2520surface%2520reconstruction%2520while%2520preserving%2520fine%2520details%2520remains%2520challenging%252C%2520especially%2520in%2520the%2520presence%2520of%2520materials%2520with%2520complex%2520reflectance%2520properties%2520and%2520without%2520a%2520dense-view%2520setup.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520versatile%2520framework%2520that%2520incorporates%2520multi-view%2520normal%2520and%2520optionally%2520reflectance%2520maps%2520into%2520radiance-based%2520surface%2520reconstruction.%2520Our%2520approach%2520employs%2520a%2520pixel-wise%2520joint%2520re-parametrization%2520of%2520reflectance%2520and%2520surface%2520normals%252C%2520representing%2520them%2520as%2520a%2520vector%2520of%2520radiances%2520under%2520simulated%252C%2520varying%2520illumination.%2520This%2520formulation%2520enables%2520seamless%2520incorporation%2520into%2520standard%2520surface%2520reconstruction%2520pipelines%252C%2520such%2520as%2520traditional%2520multi-view%2520stereo%2520%2528MVS%2529%2520frameworks%2520or%2520modern%2520neural%2520volume%2520rendering%2520%2528NVR%2529%2520ones.%2520Combined%2520with%2520the%2520latter%252C%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520multi-view%2520photometric%2520stereo%2520%2528MVPS%2529%2520benchmark%2520datasets%252C%2520including%2520DiLiGenT-MV%252C%2520LUCES-MV%2520and%2520Skoltech3D.%2520In%2520particular%252C%2520our%2520method%2520excels%2520in%2520reconstructing%2520fine-grained%2520details%2520and%2520handling%2520challenging%2520visibility%2520conditions.%2520The%2520present%2520paper%2520is%2520an%2520extended%2520version%2520of%2520the%2520earlier%2520conference%2520paper%2520by%2520Brument%2520et%2520al.%2520%2528in%2520Proceedings%2520of%2520the%2520IEEE/CVF%2520Conference%2520on%2520Computer%2520Vision%2520and%2520Pattern%2520Recognition%2520%2528CVPR%2529%252C%25202024%2529%252C%2520featuring%2520an%2520accelerated%2520and%2520more%2520robust%2520algorithm%2520as%2520well%2520as%2520a%2520broader%2520empirical%2520evaluation.%2520The%2520code%2520and%2520data%2520relative%2520to%2520this%2520article%2520is%2520available%2520at%2520https%253A//github.com/RobinBruneau/RNb-NeuS2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Surface%20Reconstruction%20Using%20Normal%20and%20Reflectance%20Cues&entry.906535625=Robin%20Bruneau%20and%20Baptiste%20Brument%20and%20Yvain%20Qu%C3%A9au%20and%20Jean%20M%C3%A9lou%20and%20Fran%C3%A7ois%20Bernard%20Lauze%20and%20Jean-Denis%20Durou%20and%20Lilian%20Calvet&entry.1292438233=Achieving%20high-fidelity%203D%20surface%20reconstruction%20while%20preserving%20fine%20details%20remains%20challenging%2C%20especially%20in%20the%20presence%20of%20materials%20with%20complex%20reflectance%20properties%20and%20without%20a%20dense-view%20setup.%20In%20this%20paper%2C%20we%20introduce%20a%20versatile%20framework%20that%20incorporates%20multi-view%20normal%20and%20optionally%20reflectance%20maps%20into%20radiance-based%20surface%20reconstruction.%20Our%20approach%20employs%20a%20pixel-wise%20joint%20re-parametrization%20of%20reflectance%20and%20surface%20normals%2C%20representing%20them%20as%20a%20vector%20of%20radiances%20under%20simulated%2C%20varying%20illumination.%20This%20formulation%20enables%20seamless%20incorporation%20into%20standard%20surface%20reconstruction%20pipelines%2C%20such%20as%20traditional%20multi-view%20stereo%20%28MVS%29%20frameworks%20or%20modern%20neural%20volume%20rendering%20%28NVR%29%20ones.%20Combined%20with%20the%20latter%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20multi-view%20photometric%20stereo%20%28MVPS%29%20benchmark%20datasets%2C%20including%20DiLiGenT-MV%2C%20LUCES-MV%20and%20Skoltech3D.%20In%20particular%2C%20our%20method%20excels%20in%20reconstructing%20fine-grained%20details%20and%20handling%20challenging%20visibility%20conditions.%20The%20present%20paper%20is%20an%20extended%20version%20of%20the%20earlier%20conference%20paper%20by%20Brument%20et%20al.%20%28in%20Proceedings%20of%20the%20IEEE/CVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%28CVPR%29%2C%202024%29%2C%20featuring%20an%20accelerated%20and%20more%20robust%20algorithm%20as%20well%20as%20a%20broader%20empirical%20evaluation.%20The%20code%20and%20data%20relative%20to%20this%20article%20is%20available%20at%20https%3A//github.com/RobinBruneau/RNb-NeuS2.&entry.1838667208=http%3A//arxiv.org/abs/2506.04115v2&entry.124074799=Read"},
{"title": "DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion", "author": "Yinghui Li and Qianyu Zhou and Di Shao and Hao Yang and Ye Zhu and Richard Dazeley and Xuequan Lu", "abstract": "Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.", "link": "http://arxiv.org/abs/2511.20278v1", "date": "2025-11-25", "relevancy": 2.8638, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6017}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5595}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAPointMamba%3A%20Domain%20Adaptive%20Point%20Mamba%20for%20Point%20Cloud%20Completion&body=Title%3A%20DAPointMamba%3A%20Domain%20Adaptive%20Point%20Mamba%20for%20Point%20Cloud%20Completion%0AAuthor%3A%20Yinghui%20Li%20and%20Qianyu%20Zhou%20and%20Di%20Shao%20and%20Hao%20Yang%20and%20Ye%20Zhu%20and%20Richard%20Dazeley%20and%20Xuequan%20Lu%0AAbstract%3A%20Domain%20adaptive%20point%20cloud%20completion%20%28DA%20PCC%29%20aims%20to%20narrow%20the%20geometric%20and%20semantic%20discrepancies%20between%20the%20labeled%20source%20and%20unlabeled%20target%20domains.%20Existing%20methods%20either%20suffer%20from%20limited%20receptive%20fields%20or%20quadratic%20complexity%20due%20to%20using%20CNNs%20or%20vision%20Transformers.%20In%20this%20paper%2C%20we%20present%20the%20first%20work%20that%20studies%20the%20adaptability%20of%20State%20Space%20Models%20%28SSMs%29%20in%20DA%20PCC%20and%20find%20that%20directly%20applying%20SSMs%20to%20DA%20PCC%20will%20encounter%20several%20challenges%3A%20directly%20serializing%203D%20point%20clouds%20into%201D%20sequences%20often%20disrupts%20the%20spatial%20topology%20and%20local%20geometric%20features%20of%20the%20target%20domain.%20Besides%2C%20the%20overlook%20of%20designs%20in%20the%20learning%20domain-agnostic%20representations%20hinders%20the%20adaptation%20performance.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20framework%2C%20DAPointMamba%20for%20DA%20PCC%2C%20that%20exhibits%20strong%20adaptability%20across%20domains%20and%20has%20the%20advantages%20of%20global%20receptive%20fields%20and%20efficient%20linear%20complexity.%20It%20has%20three%20novel%20modules.%20In%20particular%2C%20Cross-Domain%20Patch-Level%20Scanning%20introduces%20patch-level%20geometric%20correspondences%2C%20enabling%20effective%20local%20alignment.%20Cross-Domain%20Spatial%20SSM%20Alignment%20further%20strengthens%20spatial%20consistency%20by%20modulating%20patch%20features%20based%20on%20cross-domain%20similarity%2C%20effectively%20mitigating%20fine-grained%20structural%20discrepancies.%20Cross-Domain%20Channel%20SSM%20Alignment%20actively%20addresses%20global%20semantic%20gaps%20by%20interleaving%20and%20aligning%20feature%20channels.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20benchmarks%20demonstrate%20that%20our%20DAPointMamba%20outperforms%20state-of-the-art%20methods%20with%20less%20computational%20complexity%20and%20inference%20latency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAPointMamba%253A%2520Domain%2520Adaptive%2520Point%2520Mamba%2520for%2520Point%2520Cloud%2520Completion%26entry.906535625%3DYinghui%2520Li%2520and%2520Qianyu%2520Zhou%2520and%2520Di%2520Shao%2520and%2520Hao%2520Yang%2520and%2520Ye%2520Zhu%2520and%2520Richard%2520Dazeley%2520and%2520Xuequan%2520Lu%26entry.1292438233%3DDomain%2520adaptive%2520point%2520cloud%2520completion%2520%2528DA%2520PCC%2529%2520aims%2520to%2520narrow%2520the%2520geometric%2520and%2520semantic%2520discrepancies%2520between%2520the%2520labeled%2520source%2520and%2520unlabeled%2520target%2520domains.%2520Existing%2520methods%2520either%2520suffer%2520from%2520limited%2520receptive%2520fields%2520or%2520quadratic%2520complexity%2520due%2520to%2520using%2520CNNs%2520or%2520vision%2520Transformers.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520work%2520that%2520studies%2520the%2520adaptability%2520of%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520in%2520DA%2520PCC%2520and%2520find%2520that%2520directly%2520applying%2520SSMs%2520to%2520DA%2520PCC%2520will%2520encounter%2520several%2520challenges%253A%2520directly%2520serializing%25203D%2520point%2520clouds%2520into%25201D%2520sequences%2520often%2520disrupts%2520the%2520spatial%2520topology%2520and%2520local%2520geometric%2520features%2520of%2520the%2520target%2520domain.%2520Besides%252C%2520the%2520overlook%2520of%2520designs%2520in%2520the%2520learning%2520domain-agnostic%2520representations%2520hinders%2520the%2520adaptation%2520performance.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520DAPointMamba%2520for%2520DA%2520PCC%252C%2520that%2520exhibits%2520strong%2520adaptability%2520across%2520domains%2520and%2520has%2520the%2520advantages%2520of%2520global%2520receptive%2520fields%2520and%2520efficient%2520linear%2520complexity.%2520It%2520has%2520three%2520novel%2520modules.%2520In%2520particular%252C%2520Cross-Domain%2520Patch-Level%2520Scanning%2520introduces%2520patch-level%2520geometric%2520correspondences%252C%2520enabling%2520effective%2520local%2520alignment.%2520Cross-Domain%2520Spatial%2520SSM%2520Alignment%2520further%2520strengthens%2520spatial%2520consistency%2520by%2520modulating%2520patch%2520features%2520based%2520on%2520cross-domain%2520similarity%252C%2520effectively%2520mitigating%2520fine-grained%2520structural%2520discrepancies.%2520Cross-Domain%2520Channel%2520SSM%2520Alignment%2520actively%2520addresses%2520global%2520semantic%2520gaps%2520by%2520interleaving%2520and%2520aligning%2520feature%2520channels.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520benchmarks%2520demonstrate%2520that%2520our%2520DAPointMamba%2520outperforms%2520state-of-the-art%2520methods%2520with%2520less%2520computational%2520complexity%2520and%2520inference%2520latency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAPointMamba%3A%20Domain%20Adaptive%20Point%20Mamba%20for%20Point%20Cloud%20Completion&entry.906535625=Yinghui%20Li%20and%20Qianyu%20Zhou%20and%20Di%20Shao%20and%20Hao%20Yang%20and%20Ye%20Zhu%20and%20Richard%20Dazeley%20and%20Xuequan%20Lu&entry.1292438233=Domain%20adaptive%20point%20cloud%20completion%20%28DA%20PCC%29%20aims%20to%20narrow%20the%20geometric%20and%20semantic%20discrepancies%20between%20the%20labeled%20source%20and%20unlabeled%20target%20domains.%20Existing%20methods%20either%20suffer%20from%20limited%20receptive%20fields%20or%20quadratic%20complexity%20due%20to%20using%20CNNs%20or%20vision%20Transformers.%20In%20this%20paper%2C%20we%20present%20the%20first%20work%20that%20studies%20the%20adaptability%20of%20State%20Space%20Models%20%28SSMs%29%20in%20DA%20PCC%20and%20find%20that%20directly%20applying%20SSMs%20to%20DA%20PCC%20will%20encounter%20several%20challenges%3A%20directly%20serializing%203D%20point%20clouds%20into%201D%20sequences%20often%20disrupts%20the%20spatial%20topology%20and%20local%20geometric%20features%20of%20the%20target%20domain.%20Besides%2C%20the%20overlook%20of%20designs%20in%20the%20learning%20domain-agnostic%20representations%20hinders%20the%20adaptation%20performance.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20framework%2C%20DAPointMamba%20for%20DA%20PCC%2C%20that%20exhibits%20strong%20adaptability%20across%20domains%20and%20has%20the%20advantages%20of%20global%20receptive%20fields%20and%20efficient%20linear%20complexity.%20It%20has%20three%20novel%20modules.%20In%20particular%2C%20Cross-Domain%20Patch-Level%20Scanning%20introduces%20patch-level%20geometric%20correspondences%2C%20enabling%20effective%20local%20alignment.%20Cross-Domain%20Spatial%20SSM%20Alignment%20further%20strengthens%20spatial%20consistency%20by%20modulating%20patch%20features%20based%20on%20cross-domain%20similarity%2C%20effectively%20mitigating%20fine-grained%20structural%20discrepancies.%20Cross-Domain%20Channel%20SSM%20Alignment%20actively%20addresses%20global%20semantic%20gaps%20by%20interleaving%20and%20aligning%20feature%20channels.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20benchmarks%20demonstrate%20that%20our%20DAPointMamba%20outperforms%20state-of-the-art%20methods%20with%20less%20computational%20complexity%20and%20inference%20latency.&entry.1838667208=http%3A//arxiv.org/abs/2511.20278v1&entry.124074799=Read"},
{"title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering", "author": "Chenhui Gou and Zilong Chen and Zeyu Wang and Feng Li and Deyao Zhu and Zicheng Duan and Kunchang Li and Chaorui Deng and Hongyi Yuan and Haoqi Fan and Cihang Xie and Jianfei Cai and Hamid Rezatofighi", "abstract": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.", "link": "http://arxiv.org/abs/2511.20573v1", "date": "2025-11-25", "relevancy": 2.8602, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5808}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQ-VA%20World%3A%20Towards%20High-Quality%20Visual%20Question-Visual%20Answering&body=Title%3A%20VQ-VA%20World%3A%20Towards%20High-Quality%20Visual%20Question-Visual%20Answering%0AAuthor%3A%20Chenhui%20Gou%20and%20Zilong%20Chen%20and%20Zeyu%20Wang%20and%20Feng%20Li%20and%20Deyao%20Zhu%20and%20Zicheng%20Duan%20and%20Kunchang%20Li%20and%20Chaorui%20Deng%20and%20Hongyi%20Yuan%20and%20Haoqi%20Fan%20and%20Cihang%20Xie%20and%20Jianfei%20Cai%20and%20Hamid%20Rezatofighi%0AAbstract%3A%20This%20paper%20studies%20Visual%20Question-Visual%20Answering%20%28VQ-VA%29%3A%20generating%20an%20image%2C%20rather%20than%20text%2C%20in%20response%20to%20a%20visual%20question%20--%20an%20ability%20that%20has%20recently%20emerged%20in%20proprietary%20systems%20such%20as%20NanoBanana%20and%20GPT-Image.%20To%20also%20bring%20this%20capability%20to%20open-source%20models%2C%20we%20introduce%20VQ-VA%20World%2C%20a%20data-centric%20framework%20built%20around%20an%20agentic%20pipeline%20for%20large-scale%2C%20targeted%20data%20construction.%20Leveraging%20web-scale%20deployment%2C%20this%20pipeline%20crawls%20a%20massive%20amount%20of%20~1.8M%20high-quality%2C%20interleaved%20image-text%20samples%20for%20model%20training.%20For%20evaluation%2C%20we%20further%20release%20IntelligentBench%2C%20a%20human-curated%20benchmark%20that%20systematically%20assesses%20VQ-VA%20along%20the%20aspects%20of%20world%20knowledge%2C%20design%20knowledge%2C%20and%20reasoning.%20Training%20with%20VQ-VA%20World%20data%20yields%20strong%20empirical%20gains%3A%20it%20helps%20LightFusion%20attain%2053.06%20on%20IntelligentBench%2C%20substantially%20surpassing%20the%20best%20prior%20open-source%20baselines%20%28i.e.%2C%207.78%20from%20vanilla%20LightFusion%3B%201.94%20from%20UniWorld-V1%29%2C%20and%20significantly%20narrowing%20the%20gap%20toward%20leading%20proprietary%20systems%20%28e.g.%2C%2081.67%20from%20NanoBanana%3B%2082.64%20from%20GPT-Image%29.%20By%20releasing%20the%20full%20suite%20of%20model%20weights%2C%20datasets%2C%20and%20pipelines%2C%20we%20hope%20to%20stimulate%20future%20research%20on%20VQ-VA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQ-VA%2520World%253A%2520Towards%2520High-Quality%2520Visual%2520Question-Visual%2520Answering%26entry.906535625%3DChenhui%2520Gou%2520and%2520Zilong%2520Chen%2520and%2520Zeyu%2520Wang%2520and%2520Feng%2520Li%2520and%2520Deyao%2520Zhu%2520and%2520Zicheng%2520Duan%2520and%2520Kunchang%2520Li%2520and%2520Chaorui%2520Deng%2520and%2520Hongyi%2520Yuan%2520and%2520Haoqi%2520Fan%2520and%2520Cihang%2520Xie%2520and%2520Jianfei%2520Cai%2520and%2520Hamid%2520Rezatofighi%26entry.1292438233%3DThis%2520paper%2520studies%2520Visual%2520Question-Visual%2520Answering%2520%2528VQ-VA%2529%253A%2520generating%2520an%2520image%252C%2520rather%2520than%2520text%252C%2520in%2520response%2520to%2520a%2520visual%2520question%2520--%2520an%2520ability%2520that%2520has%2520recently%2520emerged%2520in%2520proprietary%2520systems%2520such%2520as%2520NanoBanana%2520and%2520GPT-Image.%2520To%2520also%2520bring%2520this%2520capability%2520to%2520open-source%2520models%252C%2520we%2520introduce%2520VQ-VA%2520World%252C%2520a%2520data-centric%2520framework%2520built%2520around%2520an%2520agentic%2520pipeline%2520for%2520large-scale%252C%2520targeted%2520data%2520construction.%2520Leveraging%2520web-scale%2520deployment%252C%2520this%2520pipeline%2520crawls%2520a%2520massive%2520amount%2520of%2520~1.8M%2520high-quality%252C%2520interleaved%2520image-text%2520samples%2520for%2520model%2520training.%2520For%2520evaluation%252C%2520we%2520further%2520release%2520IntelligentBench%252C%2520a%2520human-curated%2520benchmark%2520that%2520systematically%2520assesses%2520VQ-VA%2520along%2520the%2520aspects%2520of%2520world%2520knowledge%252C%2520design%2520knowledge%252C%2520and%2520reasoning.%2520Training%2520with%2520VQ-VA%2520World%2520data%2520yields%2520strong%2520empirical%2520gains%253A%2520it%2520helps%2520LightFusion%2520attain%252053.06%2520on%2520IntelligentBench%252C%2520substantially%2520surpassing%2520the%2520best%2520prior%2520open-source%2520baselines%2520%2528i.e.%252C%25207.78%2520from%2520vanilla%2520LightFusion%253B%25201.94%2520from%2520UniWorld-V1%2529%252C%2520and%2520significantly%2520narrowing%2520the%2520gap%2520toward%2520leading%2520proprietary%2520systems%2520%2528e.g.%252C%252081.67%2520from%2520NanoBanana%253B%252082.64%2520from%2520GPT-Image%2529.%2520By%2520releasing%2520the%2520full%2520suite%2520of%2520model%2520weights%252C%2520datasets%252C%2520and%2520pipelines%252C%2520we%2520hope%2520to%2520stimulate%2520future%2520research%2520on%2520VQ-VA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQ-VA%20World%3A%20Towards%20High-Quality%20Visual%20Question-Visual%20Answering&entry.906535625=Chenhui%20Gou%20and%20Zilong%20Chen%20and%20Zeyu%20Wang%20and%20Feng%20Li%20and%20Deyao%20Zhu%20and%20Zicheng%20Duan%20and%20Kunchang%20Li%20and%20Chaorui%20Deng%20and%20Hongyi%20Yuan%20and%20Haoqi%20Fan%20and%20Cihang%20Xie%20and%20Jianfei%20Cai%20and%20Hamid%20Rezatofighi&entry.1292438233=This%20paper%20studies%20Visual%20Question-Visual%20Answering%20%28VQ-VA%29%3A%20generating%20an%20image%2C%20rather%20than%20text%2C%20in%20response%20to%20a%20visual%20question%20--%20an%20ability%20that%20has%20recently%20emerged%20in%20proprietary%20systems%20such%20as%20NanoBanana%20and%20GPT-Image.%20To%20also%20bring%20this%20capability%20to%20open-source%20models%2C%20we%20introduce%20VQ-VA%20World%2C%20a%20data-centric%20framework%20built%20around%20an%20agentic%20pipeline%20for%20large-scale%2C%20targeted%20data%20construction.%20Leveraging%20web-scale%20deployment%2C%20this%20pipeline%20crawls%20a%20massive%20amount%20of%20~1.8M%20high-quality%2C%20interleaved%20image-text%20samples%20for%20model%20training.%20For%20evaluation%2C%20we%20further%20release%20IntelligentBench%2C%20a%20human-curated%20benchmark%20that%20systematically%20assesses%20VQ-VA%20along%20the%20aspects%20of%20world%20knowledge%2C%20design%20knowledge%2C%20and%20reasoning.%20Training%20with%20VQ-VA%20World%20data%20yields%20strong%20empirical%20gains%3A%20it%20helps%20LightFusion%20attain%2053.06%20on%20IntelligentBench%2C%20substantially%20surpassing%20the%20best%20prior%20open-source%20baselines%20%28i.e.%2C%207.78%20from%20vanilla%20LightFusion%3B%201.94%20from%20UniWorld-V1%29%2C%20and%20significantly%20narrowing%20the%20gap%20toward%20leading%20proprietary%20systems%20%28e.g.%2C%2081.67%20from%20NanoBanana%3B%2082.64%20from%20GPT-Image%29.%20By%20releasing%20the%20full%20suite%20of%20model%20weights%2C%20datasets%2C%20and%20pipelines%2C%20we%20hope%20to%20stimulate%20future%20research%20on%20VQ-VA.&entry.1838667208=http%3A//arxiv.org/abs/2511.20573v1&entry.124074799=Read"},
{"title": "IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection", "author": "Changjiang Jiang and Wenhui Dong and Zhonghao Zhang and Chenyang Si and Fengchang Yu and Wei Peng and Xinbin Yuan and Yifei Bi and Ming Zhao and Zian Zhou and Caifeng Shan", "abstract": "The rapid development of Artificial Intelligence Generated Content (AIGC) techniques has enabled the creation of high-quality synthetic content, but it also raises significant security concerns. Current detection methods face two major limitations: (1) the lack of multidimensional explainable datasets for generated images and videos. Existing open-source datasets (e.g., WildFake, GenVideo) rely on oversimplified binary annotations, which restrict the explainability and trustworthiness of trained detectors. (2) Prior MLLM-based forgery detectors (e.g., FakeVLM) exhibit insufficiently fine-grained interpretability in their step-by-step reasoning, which hinders reliable localization and explanation. To address these challenges, we introduce Ivy-Fake, the first large-scale multimodal benchmark for explainable AIGC detection. It consists of over 106K richly annotated training samples (images and videos) and 5,000 manually verified evaluation examples, sourced from multiple generative models and real world datasets through a carefully designed pipeline to ensure both diversity and quality. Furthermore, we propose Ivy-xDetector, a reinforcement learning model based on Group Relative Policy Optimization (GRPO), capable of producing explainable reasoning chains and achieving robust performance across multiple synthetic content detection benchmarks. Extensive experiments demonstrate the superiority of our dataset and confirm the effectiveness of our approach. Notably, our method improves performance on GenImage from 86.88% to 96.32%, surpassing prior state-of-the-art methods by a clear margin.", "link": "http://arxiv.org/abs/2506.00979v2", "date": "2025-11-25", "relevancy": 2.8506, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5852}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.582}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IVY-FAKE%3A%20A%20Unified%20Explainable%20Framework%20and%20Benchmark%20for%20Image%20and%20Video%20AIGC%20Detection&body=Title%3A%20IVY-FAKE%3A%20A%20Unified%20Explainable%20Framework%20and%20Benchmark%20for%20Image%20and%20Video%20AIGC%20Detection%0AAuthor%3A%20Changjiang%20Jiang%20and%20Wenhui%20Dong%20and%20Zhonghao%20Zhang%20and%20Chenyang%20Si%20and%20Fengchang%20Yu%20and%20Wei%20Peng%20and%20Xinbin%20Yuan%20and%20Yifei%20Bi%20and%20Ming%20Zhao%20and%20Zian%20Zhou%20and%20Caifeng%20Shan%0AAbstract%3A%20The%20rapid%20development%20of%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20techniques%20has%20enabled%20the%20creation%20of%20high-quality%20synthetic%20content%2C%20but%20it%20also%20raises%20significant%20security%20concerns.%20Current%20detection%20methods%20face%20two%20major%20limitations%3A%20%281%29%20the%20lack%20of%20multidimensional%20explainable%20datasets%20for%20generated%20images%20and%20videos.%20Existing%20open-source%20datasets%20%28e.g.%2C%20WildFake%2C%20GenVideo%29%20rely%20on%20oversimplified%20binary%20annotations%2C%20which%20restrict%20the%20explainability%20and%20trustworthiness%20of%20trained%20detectors.%20%282%29%20Prior%20MLLM-based%20forgery%20detectors%20%28e.g.%2C%20FakeVLM%29%20exhibit%20insufficiently%20fine-grained%20interpretability%20in%20their%20step-by-step%20reasoning%2C%20which%20hinders%20reliable%20localization%20and%20explanation.%20To%20address%20these%20challenges%2C%20we%20introduce%20Ivy-Fake%2C%20the%20first%20large-scale%20multimodal%20benchmark%20for%20explainable%20AIGC%20detection.%20It%20consists%20of%20over%20106K%20richly%20annotated%20training%20samples%20%28images%20and%20videos%29%20and%205%2C000%20manually%20verified%20evaluation%20examples%2C%20sourced%20from%20multiple%20generative%20models%20and%20real%20world%20datasets%20through%20a%20carefully%20designed%20pipeline%20to%20ensure%20both%20diversity%20and%20quality.%20Furthermore%2C%20we%20propose%20Ivy-xDetector%2C%20a%20reinforcement%20learning%20model%20based%20on%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20capable%20of%20producing%20explainable%20reasoning%20chains%20and%20achieving%20robust%20performance%20across%20multiple%20synthetic%20content%20detection%20benchmarks.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20dataset%20and%20confirm%20the%20effectiveness%20of%20our%20approach.%20Notably%2C%20our%20method%20improves%20performance%20on%20GenImage%20from%2086.88%25%20to%2096.32%25%2C%20surpassing%20prior%20state-of-the-art%20methods%20by%20a%20clear%20margin.%0ALink%3A%20http%3A//arxiv.org/abs/2506.00979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIVY-FAKE%253A%2520A%2520Unified%2520Explainable%2520Framework%2520and%2520Benchmark%2520for%2520Image%2520and%2520Video%2520AIGC%2520Detection%26entry.906535625%3DChangjiang%2520Jiang%2520and%2520Wenhui%2520Dong%2520and%2520Zhonghao%2520Zhang%2520and%2520Chenyang%2520Si%2520and%2520Fengchang%2520Yu%2520and%2520Wei%2520Peng%2520and%2520Xinbin%2520Yuan%2520and%2520Yifei%2520Bi%2520and%2520Ming%2520Zhao%2520and%2520Zian%2520Zhou%2520and%2520Caifeng%2520Shan%26entry.1292438233%3DThe%2520rapid%2520development%2520of%2520Artificial%2520Intelligence%2520Generated%2520Content%2520%2528AIGC%2529%2520techniques%2520has%2520enabled%2520the%2520creation%2520of%2520high-quality%2520synthetic%2520content%252C%2520but%2520it%2520also%2520raises%2520significant%2520security%2520concerns.%2520Current%2520detection%2520methods%2520face%2520two%2520major%2520limitations%253A%2520%25281%2529%2520the%2520lack%2520of%2520multidimensional%2520explainable%2520datasets%2520for%2520generated%2520images%2520and%2520videos.%2520Existing%2520open-source%2520datasets%2520%2528e.g.%252C%2520WildFake%252C%2520GenVideo%2529%2520rely%2520on%2520oversimplified%2520binary%2520annotations%252C%2520which%2520restrict%2520the%2520explainability%2520and%2520trustworthiness%2520of%2520trained%2520detectors.%2520%25282%2529%2520Prior%2520MLLM-based%2520forgery%2520detectors%2520%2528e.g.%252C%2520FakeVLM%2529%2520exhibit%2520insufficiently%2520fine-grained%2520interpretability%2520in%2520their%2520step-by-step%2520reasoning%252C%2520which%2520hinders%2520reliable%2520localization%2520and%2520explanation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Ivy-Fake%252C%2520the%2520first%2520large-scale%2520multimodal%2520benchmark%2520for%2520explainable%2520AIGC%2520detection.%2520It%2520consists%2520of%2520over%2520106K%2520richly%2520annotated%2520training%2520samples%2520%2528images%2520and%2520videos%2529%2520and%25205%252C000%2520manually%2520verified%2520evaluation%2520examples%252C%2520sourced%2520from%2520multiple%2520generative%2520models%2520and%2520real%2520world%2520datasets%2520through%2520a%2520carefully%2520designed%2520pipeline%2520to%2520ensure%2520both%2520diversity%2520and%2520quality.%2520Furthermore%252C%2520we%2520propose%2520Ivy-xDetector%252C%2520a%2520reinforcement%2520learning%2520model%2520based%2520on%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520capable%2520of%2520producing%2520explainable%2520reasoning%2520chains%2520and%2520achieving%2520robust%2520performance%2520across%2520multiple%2520synthetic%2520content%2520detection%2520benchmarks.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520dataset%2520and%2520confirm%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Notably%252C%2520our%2520method%2520improves%2520performance%2520on%2520GenImage%2520from%252086.88%2525%2520to%252096.32%2525%252C%2520surpassing%2520prior%2520state-of-the-art%2520methods%2520by%2520a%2520clear%2520margin.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IVY-FAKE%3A%20A%20Unified%20Explainable%20Framework%20and%20Benchmark%20for%20Image%20and%20Video%20AIGC%20Detection&entry.906535625=Changjiang%20Jiang%20and%20Wenhui%20Dong%20and%20Zhonghao%20Zhang%20and%20Chenyang%20Si%20and%20Fengchang%20Yu%20and%20Wei%20Peng%20and%20Xinbin%20Yuan%20and%20Yifei%20Bi%20and%20Ming%20Zhao%20and%20Zian%20Zhou%20and%20Caifeng%20Shan&entry.1292438233=The%20rapid%20development%20of%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20techniques%20has%20enabled%20the%20creation%20of%20high-quality%20synthetic%20content%2C%20but%20it%20also%20raises%20significant%20security%20concerns.%20Current%20detection%20methods%20face%20two%20major%20limitations%3A%20%281%29%20the%20lack%20of%20multidimensional%20explainable%20datasets%20for%20generated%20images%20and%20videos.%20Existing%20open-source%20datasets%20%28e.g.%2C%20WildFake%2C%20GenVideo%29%20rely%20on%20oversimplified%20binary%20annotations%2C%20which%20restrict%20the%20explainability%20and%20trustworthiness%20of%20trained%20detectors.%20%282%29%20Prior%20MLLM-based%20forgery%20detectors%20%28e.g.%2C%20FakeVLM%29%20exhibit%20insufficiently%20fine-grained%20interpretability%20in%20their%20step-by-step%20reasoning%2C%20which%20hinders%20reliable%20localization%20and%20explanation.%20To%20address%20these%20challenges%2C%20we%20introduce%20Ivy-Fake%2C%20the%20first%20large-scale%20multimodal%20benchmark%20for%20explainable%20AIGC%20detection.%20It%20consists%20of%20over%20106K%20richly%20annotated%20training%20samples%20%28images%20and%20videos%29%20and%205%2C000%20manually%20verified%20evaluation%20examples%2C%20sourced%20from%20multiple%20generative%20models%20and%20real%20world%20datasets%20through%20a%20carefully%20designed%20pipeline%20to%20ensure%20both%20diversity%20and%20quality.%20Furthermore%2C%20we%20propose%20Ivy-xDetector%2C%20a%20reinforcement%20learning%20model%20based%20on%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20capable%20of%20producing%20explainable%20reasoning%20chains%20and%20achieving%20robust%20performance%20across%20multiple%20synthetic%20content%20detection%20benchmarks.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20dataset%20and%20confirm%20the%20effectiveness%20of%20our%20approach.%20Notably%2C%20our%20method%20improves%20performance%20on%20GenImage%20from%2086.88%25%20to%2096.32%25%2C%20surpassing%20prior%20state-of-the-art%20methods%20by%20a%20clear%20margin.&entry.1838667208=http%3A//arxiv.org/abs/2506.00979v2&entry.124074799=Read"},
{"title": "DINO-Tok: Adapting DINO for Visual Tokenizers", "author": "Mingkai Jia and Mingxiao Li and Liaoyuan Fan and Tianxing Shi and Jiaxin Guo and Zeming Li and Xiaoyang Guo and Xiao-Xiao Long and Qian Zhang and Ping Tan and Wei Yin", "abstract": "Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a DINO-based visual tokenizer that unifies hierarchical representations into an information-complete latent space. By integrating shallow features that retain fine-grained details with deep features encoding global semantics, DINO-Tok effectively bridges pretrained representations and visual generation. We further analyze the challenges of vector quantization (VQ) in this high-dimensional space, where key information is often lost and codebook collapse occurs. We thus propose a global PCA reweighting mechanism to stabilize VQ and preserve essential information across dimensions. On ImageNet 256$\\times$256, DINO-Tok achieves state-of-the-art reconstruction performance, reaching 28.54 PSNR for autoencoding and 23.98 PSNR for VQ-based modeling, significantly outperforming prior tokenizers and comparable to billion-level data trained models (such as Hunyuan and Wan). These results demonstrate that adapting powerful pretrained vision models like DINO for tokenization enables semantically aligned and high-fidelity latent representations, enabling next-generation visual generative models. Code will be publicly available at https://github.com/MKJia/DINO-Tok.", "link": "http://arxiv.org/abs/2511.20565v1", "date": "2025-11-25", "relevancy": 2.8441, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-Tok%3A%20Adapting%20DINO%20for%20Visual%20Tokenizers&body=Title%3A%20DINO-Tok%3A%20Adapting%20DINO%20for%20Visual%20Tokenizers%0AAuthor%3A%20Mingkai%20Jia%20and%20Mingxiao%20Li%20and%20Liaoyuan%20Fan%20and%20Tianxing%20Shi%20and%20Jiaxin%20Guo%20and%20Zeming%20Li%20and%20Xiaoyang%20Guo%20and%20Xiao-Xiao%20Long%20and%20Qian%20Zhang%20and%20Ping%20Tan%20and%20Wei%20Yin%0AAbstract%3A%20Recent%20advances%20in%20visual%20generation%20have%20highlighted%20the%20rise%20of%20Latent%20Generative%20Models%20%28LGMs%29%2C%20which%20rely%20on%20effective%20visual%20tokenizers%20to%20bridge%20pixels%20and%20semantics.%20However%2C%20existing%20tokenizers%20are%20typically%20trained%20from%20scratch%20and%20struggle%20to%20balance%20semantic%20representation%20and%20reconstruction%20fidelity%2C%20particularly%20in%20high-dimensional%20latent%20spaces.%20In%20this%20work%2C%20we%20introduce%20DINO-Tok%2C%20a%20DINO-based%20visual%20tokenizer%20that%20unifies%20hierarchical%20representations%20into%20an%20information-complete%20latent%20space.%20By%20integrating%20shallow%20features%20that%20retain%20fine-grained%20details%20with%20deep%20features%20encoding%20global%20semantics%2C%20DINO-Tok%20effectively%20bridges%20pretrained%20representations%20and%20visual%20generation.%20We%20further%20analyze%20the%20challenges%20of%20vector%20quantization%20%28VQ%29%20in%20this%20high-dimensional%20space%2C%20where%20key%20information%20is%20often%20lost%20and%20codebook%20collapse%20occurs.%20We%20thus%20propose%20a%20global%20PCA%20reweighting%20mechanism%20to%20stabilize%20VQ%20and%20preserve%20essential%20information%20across%20dimensions.%20On%20ImageNet%20256%24%5Ctimes%24256%2C%20DINO-Tok%20achieves%20state-of-the-art%20reconstruction%20performance%2C%20reaching%2028.54%20PSNR%20for%20autoencoding%20and%2023.98%20PSNR%20for%20VQ-based%20modeling%2C%20significantly%20outperforming%20prior%20tokenizers%20and%20comparable%20to%20billion-level%20data%20trained%20models%20%28such%20as%20Hunyuan%20and%20Wan%29.%20These%20results%20demonstrate%20that%20adapting%20powerful%20pretrained%20vision%20models%20like%20DINO%20for%20tokenization%20enables%20semantically%20aligned%20and%20high-fidelity%20latent%20representations%2C%20enabling%20next-generation%20visual%20generative%20models.%20Code%20will%20be%20publicly%20available%20at%20https%3A//github.com/MKJia/DINO-Tok.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-Tok%253A%2520Adapting%2520DINO%2520for%2520Visual%2520Tokenizers%26entry.906535625%3DMingkai%2520Jia%2520and%2520Mingxiao%2520Li%2520and%2520Liaoyuan%2520Fan%2520and%2520Tianxing%2520Shi%2520and%2520Jiaxin%2520Guo%2520and%2520Zeming%2520Li%2520and%2520Xiaoyang%2520Guo%2520and%2520Xiao-Xiao%2520Long%2520and%2520Qian%2520Zhang%2520and%2520Ping%2520Tan%2520and%2520Wei%2520Yin%26entry.1292438233%3DRecent%2520advances%2520in%2520visual%2520generation%2520have%2520highlighted%2520the%2520rise%2520of%2520Latent%2520Generative%2520Models%2520%2528LGMs%2529%252C%2520which%2520rely%2520on%2520effective%2520visual%2520tokenizers%2520to%2520bridge%2520pixels%2520and%2520semantics.%2520However%252C%2520existing%2520tokenizers%2520are%2520typically%2520trained%2520from%2520scratch%2520and%2520struggle%2520to%2520balance%2520semantic%2520representation%2520and%2520reconstruction%2520fidelity%252C%2520particularly%2520in%2520high-dimensional%2520latent%2520spaces.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DINO-Tok%252C%2520a%2520DINO-based%2520visual%2520tokenizer%2520that%2520unifies%2520hierarchical%2520representations%2520into%2520an%2520information-complete%2520latent%2520space.%2520By%2520integrating%2520shallow%2520features%2520that%2520retain%2520fine-grained%2520details%2520with%2520deep%2520features%2520encoding%2520global%2520semantics%252C%2520DINO-Tok%2520effectively%2520bridges%2520pretrained%2520representations%2520and%2520visual%2520generation.%2520We%2520further%2520analyze%2520the%2520challenges%2520of%2520vector%2520quantization%2520%2528VQ%2529%2520in%2520this%2520high-dimensional%2520space%252C%2520where%2520key%2520information%2520is%2520often%2520lost%2520and%2520codebook%2520collapse%2520occurs.%2520We%2520thus%2520propose%2520a%2520global%2520PCA%2520reweighting%2520mechanism%2520to%2520stabilize%2520VQ%2520and%2520preserve%2520essential%2520information%2520across%2520dimensions.%2520On%2520ImageNet%2520256%2524%255Ctimes%2524256%252C%2520DINO-Tok%2520achieves%2520state-of-the-art%2520reconstruction%2520performance%252C%2520reaching%252028.54%2520PSNR%2520for%2520autoencoding%2520and%252023.98%2520PSNR%2520for%2520VQ-based%2520modeling%252C%2520significantly%2520outperforming%2520prior%2520tokenizers%2520and%2520comparable%2520to%2520billion-level%2520data%2520trained%2520models%2520%2528such%2520as%2520Hunyuan%2520and%2520Wan%2529.%2520These%2520results%2520demonstrate%2520that%2520adapting%2520powerful%2520pretrained%2520vision%2520models%2520like%2520DINO%2520for%2520tokenization%2520enables%2520semantically%2520aligned%2520and%2520high-fidelity%2520latent%2520representations%252C%2520enabling%2520next-generation%2520visual%2520generative%2520models.%2520Code%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/MKJia/DINO-Tok.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-Tok%3A%20Adapting%20DINO%20for%20Visual%20Tokenizers&entry.906535625=Mingkai%20Jia%20and%20Mingxiao%20Li%20and%20Liaoyuan%20Fan%20and%20Tianxing%20Shi%20and%20Jiaxin%20Guo%20and%20Zeming%20Li%20and%20Xiaoyang%20Guo%20and%20Xiao-Xiao%20Long%20and%20Qian%20Zhang%20and%20Ping%20Tan%20and%20Wei%20Yin&entry.1292438233=Recent%20advances%20in%20visual%20generation%20have%20highlighted%20the%20rise%20of%20Latent%20Generative%20Models%20%28LGMs%29%2C%20which%20rely%20on%20effective%20visual%20tokenizers%20to%20bridge%20pixels%20and%20semantics.%20However%2C%20existing%20tokenizers%20are%20typically%20trained%20from%20scratch%20and%20struggle%20to%20balance%20semantic%20representation%20and%20reconstruction%20fidelity%2C%20particularly%20in%20high-dimensional%20latent%20spaces.%20In%20this%20work%2C%20we%20introduce%20DINO-Tok%2C%20a%20DINO-based%20visual%20tokenizer%20that%20unifies%20hierarchical%20representations%20into%20an%20information-complete%20latent%20space.%20By%20integrating%20shallow%20features%20that%20retain%20fine-grained%20details%20with%20deep%20features%20encoding%20global%20semantics%2C%20DINO-Tok%20effectively%20bridges%20pretrained%20representations%20and%20visual%20generation.%20We%20further%20analyze%20the%20challenges%20of%20vector%20quantization%20%28VQ%29%20in%20this%20high-dimensional%20space%2C%20where%20key%20information%20is%20often%20lost%20and%20codebook%20collapse%20occurs.%20We%20thus%20propose%20a%20global%20PCA%20reweighting%20mechanism%20to%20stabilize%20VQ%20and%20preserve%20essential%20information%20across%20dimensions.%20On%20ImageNet%20256%24%5Ctimes%24256%2C%20DINO-Tok%20achieves%20state-of-the-art%20reconstruction%20performance%2C%20reaching%2028.54%20PSNR%20for%20autoencoding%20and%2023.98%20PSNR%20for%20VQ-based%20modeling%2C%20significantly%20outperforming%20prior%20tokenizers%20and%20comparable%20to%20billion-level%20data%20trained%20models%20%28such%20as%20Hunyuan%20and%20Wan%29.%20These%20results%20demonstrate%20that%20adapting%20powerful%20pretrained%20vision%20models%20like%20DINO%20for%20tokenization%20enables%20semantically%20aligned%20and%20high-fidelity%20latent%20representations%2C%20enabling%20next-generation%20visual%20generative%20models.%20Code%20will%20be%20publicly%20available%20at%20https%3A//github.com/MKJia/DINO-Tok.&entry.1838667208=http%3A//arxiv.org/abs/2511.20565v1&entry.124074799=Read"},
{"title": "Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding", "author": "Bowei Pu and Chuanbin Liu and Yifan Ge and Peicheng Zhou and Yiwei Sun and Zhiying Lu and Jiankang Wang and Hongtao Xie", "abstract": "Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.", "link": "http://arxiv.org/abs/2511.18463v2", "date": "2025-11-25", "relevancy": 2.8329, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alternating%20Perception-Reasoning%20for%20Hallucination-Resistant%20Video%20Understanding&body=Title%3A%20Alternating%20Perception-Reasoning%20for%20Hallucination-Resistant%20Video%20Understanding%0AAuthor%3A%20Bowei%20Pu%20and%20Chuanbin%20Liu%20and%20Yifan%20Ge%20and%20Peicheng%20Zhou%20and%20Yiwei%20Sun%20and%20Zhiying%20Lu%20and%20Jiankang%20Wang%20and%20Hongtao%20Xie%0AAbstract%3A%20Sufficient%20visual%20perception%20is%20the%20foundation%20of%20video%20reasoning.%20Nevertheless%2C%20existing%20Video%20Reasoning%20LLMs%20suffer%20from%20perception%20shortcuts%2C%20relying%20on%20a%20flawed%20single-step%20perception%20paradigm.%20This%20paradigm%20describes%20the%20video%20and%20then%20conducts%20reasoning%2C%20which%20runs%20the%20risk%20of%20insufficient%20evidence%20and%20emergent%20hallucinations.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20new%20framework%20that%20integrates%20a%20loop-based%20paradigm%20with%20an%20anti-hallucination%20reward.%20First%2C%20to%20address%20the%20insufficient%20evidence%2C%20we%20introduce%20the%20Perception%20Loop%20Reasoning%20%28PLR%29%20paradigm.%20Instead%20of%20describing%20the%20video%20at%20once%2C%20each%20loop%20requires%20the%20model%20to%20describe%20a%20video%20segment%20with%20precise%20timestamps%2C%20analyze%20this%20segment%2C%20and%20decide%20the%20next%20action.%20Second%2C%20for%20the%20risk%20of%20hallucinations%2C%20the%20Factual-Aware%20Evaluator%20%28FAE%29%20evaluates%20each%20perception%20result%20as%20a%20reliable%20anti-hallucination%20reward.%20This%20reward%20encourages%20the%20model%20to%20provide%20sufficient%20and%20precise%20video%20evidence.%20Our%20FAE%2C%20which%20performs%20comparably%20to%20GPT-4o%2C%20is%20tuned%20on%20our%20AnetHallu-117K%2C%20a%20large-scale%20hallucination%20judgment%20preference%20dataset.%20Extensive%20experiments%20show%20that%20our%20Video-PLR%20achieves%20the%20state-of-the-art%20in%20both%203B%20and%207B%20parameter%20scales%20and%20has%20the%20best%20data%20efficiency.%20Our%20code%2C%20models%2C%20and%20datasets%20are%20released%20on%3A%20https%3A//github.com/BoweiPu/VideoPLR.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlternating%2520Perception-Reasoning%2520for%2520Hallucination-Resistant%2520Video%2520Understanding%26entry.906535625%3DBowei%2520Pu%2520and%2520Chuanbin%2520Liu%2520and%2520Yifan%2520Ge%2520and%2520Peicheng%2520Zhou%2520and%2520Yiwei%2520Sun%2520and%2520Zhiying%2520Lu%2520and%2520Jiankang%2520Wang%2520and%2520Hongtao%2520Xie%26entry.1292438233%3DSufficient%2520visual%2520perception%2520is%2520the%2520foundation%2520of%2520video%2520reasoning.%2520Nevertheless%252C%2520existing%2520Video%2520Reasoning%2520LLMs%2520suffer%2520from%2520perception%2520shortcuts%252C%2520relying%2520on%2520a%2520flawed%2520single-step%2520perception%2520paradigm.%2520This%2520paradigm%2520describes%2520the%2520video%2520and%2520then%2520conducts%2520reasoning%252C%2520which%2520runs%2520the%2520risk%2520of%2520insufficient%2520evidence%2520and%2520emergent%2520hallucinations.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520new%2520framework%2520that%2520integrates%2520a%2520loop-based%2520paradigm%2520with%2520an%2520anti-hallucination%2520reward.%2520First%252C%2520to%2520address%2520the%2520insufficient%2520evidence%252C%2520we%2520introduce%2520the%2520Perception%2520Loop%2520Reasoning%2520%2528PLR%2529%2520paradigm.%2520Instead%2520of%2520describing%2520the%2520video%2520at%2520once%252C%2520each%2520loop%2520requires%2520the%2520model%2520to%2520describe%2520a%2520video%2520segment%2520with%2520precise%2520timestamps%252C%2520analyze%2520this%2520segment%252C%2520and%2520decide%2520the%2520next%2520action.%2520Second%252C%2520for%2520the%2520risk%2520of%2520hallucinations%252C%2520the%2520Factual-Aware%2520Evaluator%2520%2528FAE%2529%2520evaluates%2520each%2520perception%2520result%2520as%2520a%2520reliable%2520anti-hallucination%2520reward.%2520This%2520reward%2520encourages%2520the%2520model%2520to%2520provide%2520sufficient%2520and%2520precise%2520video%2520evidence.%2520Our%2520FAE%252C%2520which%2520performs%2520comparably%2520to%2520GPT-4o%252C%2520is%2520tuned%2520on%2520our%2520AnetHallu-117K%252C%2520a%2520large-scale%2520hallucination%2520judgment%2520preference%2520dataset.%2520Extensive%2520experiments%2520show%2520that%2520our%2520Video-PLR%2520achieves%2520the%2520state-of-the-art%2520in%2520both%25203B%2520and%25207B%2520parameter%2520scales%2520and%2520has%2520the%2520best%2520data%2520efficiency.%2520Our%2520code%252C%2520models%252C%2520and%2520datasets%2520are%2520released%2520on%253A%2520https%253A//github.com/BoweiPu/VideoPLR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alternating%20Perception-Reasoning%20for%20Hallucination-Resistant%20Video%20Understanding&entry.906535625=Bowei%20Pu%20and%20Chuanbin%20Liu%20and%20Yifan%20Ge%20and%20Peicheng%20Zhou%20and%20Yiwei%20Sun%20and%20Zhiying%20Lu%20and%20Jiankang%20Wang%20and%20Hongtao%20Xie&entry.1292438233=Sufficient%20visual%20perception%20is%20the%20foundation%20of%20video%20reasoning.%20Nevertheless%2C%20existing%20Video%20Reasoning%20LLMs%20suffer%20from%20perception%20shortcuts%2C%20relying%20on%20a%20flawed%20single-step%20perception%20paradigm.%20This%20paradigm%20describes%20the%20video%20and%20then%20conducts%20reasoning%2C%20which%20runs%20the%20risk%20of%20insufficient%20evidence%20and%20emergent%20hallucinations.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20new%20framework%20that%20integrates%20a%20loop-based%20paradigm%20with%20an%20anti-hallucination%20reward.%20First%2C%20to%20address%20the%20insufficient%20evidence%2C%20we%20introduce%20the%20Perception%20Loop%20Reasoning%20%28PLR%29%20paradigm.%20Instead%20of%20describing%20the%20video%20at%20once%2C%20each%20loop%20requires%20the%20model%20to%20describe%20a%20video%20segment%20with%20precise%20timestamps%2C%20analyze%20this%20segment%2C%20and%20decide%20the%20next%20action.%20Second%2C%20for%20the%20risk%20of%20hallucinations%2C%20the%20Factual-Aware%20Evaluator%20%28FAE%29%20evaluates%20each%20perception%20result%20as%20a%20reliable%20anti-hallucination%20reward.%20This%20reward%20encourages%20the%20model%20to%20provide%20sufficient%20and%20precise%20video%20evidence.%20Our%20FAE%2C%20which%20performs%20comparably%20to%20GPT-4o%2C%20is%20tuned%20on%20our%20AnetHallu-117K%2C%20a%20large-scale%20hallucination%20judgment%20preference%20dataset.%20Extensive%20experiments%20show%20that%20our%20Video-PLR%20achieves%20the%20state-of-the-art%20in%20both%203B%20and%207B%20parameter%20scales%20and%20has%20the%20best%20data%20efficiency.%20Our%20code%2C%20models%2C%20and%20datasets%20are%20released%20on%3A%20https%3A//github.com/BoweiPu/VideoPLR.&entry.1838667208=http%3A//arxiv.org/abs/2511.18463v2&entry.124074799=Read"},
{"title": "Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search", "author": "Yunqi Zhou and Chengjie Jiang and Chun Yuan and Jing Li", "abstract": "With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8\\% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.", "link": "http://arxiv.org/abs/2511.20460v1", "date": "2025-11-25", "relevancy": 2.8072, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Where%20It%20Matters%3A%20Training-Free%20Ultra-HR%20Remote%20Sensing%20VQA%20via%20Adaptive%20Zoom%20Search&body=Title%3A%20Look%20Where%20It%20Matters%3A%20Training-Free%20Ultra-HR%20Remote%20Sensing%20VQA%20via%20Adaptive%20Zoom%20Search%0AAuthor%3A%20Yunqi%20Zhou%20and%20Chengjie%20Jiang%20and%20Chun%20Yuan%20and%20Jing%20Li%0AAbstract%3A%20With%20advances%20in%20satellite%20constellations%2C%20sensor%20technologies%2C%20and%20imaging%20pipelines%2C%20ultra-high-resolution%20%28Ultra-HR%29%20remote%20sensing%20imagery%20is%20becoming%20increasingly%20widespread.%20However%2C%20current%20remote%20sensing%20foundation%20models%20are%20ill-suited%20to%20such%20inputs%3A%20full-image%20encoding%20exhausts%20token%20and%20memory%20budgets%2C%20while%20resize-based%20preprocessing%20loses%20fine-grained%20and%20answer-critical%20details.%20In%20this%20context%2C%20guiding%20the%20model%20look%20where%20it%20matters%20before%20prediction%20becomes%20crucial.%20Therefore%2C%20we%20present%20ZoomSearch%2C%20a%20training-free%2C%20plug-and-play%20pipeline%20that%20decouples%20%27where%20to%20look%27%20from%20%27how%20to%20answer%27%20for%20Ultra-HR%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RS-VQA%29.%20ZoomSearch%20combines%20Adaptive%20Multi-Branch%20Zoom%20Search%2C%20which%20performs%20a%20hierarchical%20search%20over%20image%20patches%20to%20localize%20query-relevant%20regions%2C%20with%20Layout-Aware%20Patch%20Reassembly%2C%20which%20reorganizes%20the%20selected%20patches%20into%20a%20compact%2C%20layout-faithful%20canvas.%20We%20conduct%20comprehensive%20experiments%20on%20Ultra-HR%20RS-VQA%20benchmarks%20MME-RealWorld-RS%20and%20LRS-VQA%2C%20comparing%20against%20%28i%29%20strong%20general%20foundation%20models%2C%20%28ii%29%20remote%20sensing%20foundation%20models%2C%20%28iii%29%20Ultra-HR%20RS-VQA%20methods%2C%20and%20%28iv%29%20plug-and-play%20search-based%20VQA%20methods.%20When%20integrated%20with%20LLaVA-ov%2C%20ZoomSearch%20attains%20state-of-the-art%20accuracy%20across%20diverse%20tasks%2C%20improving%20the%20LLaVA-ov%20baseline%20by%2026.3%25%20on%20LRS-VQA%20and%20114.8%5C%25%20on%20MME-RealWorld-RS.%20Meanwhile%2C%20it%20achieves%20much%20higher%20inference%20efficiency%2C%20outperforming%20prior%20search-based%20methods%20by%2020%25~44%25%20in%20speed.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Where%2520It%2520Matters%253A%2520Training-Free%2520Ultra-HR%2520Remote%2520Sensing%2520VQA%2520via%2520Adaptive%2520Zoom%2520Search%26entry.906535625%3DYunqi%2520Zhou%2520and%2520Chengjie%2520Jiang%2520and%2520Chun%2520Yuan%2520and%2520Jing%2520Li%26entry.1292438233%3DWith%2520advances%2520in%2520satellite%2520constellations%252C%2520sensor%2520technologies%252C%2520and%2520imaging%2520pipelines%252C%2520ultra-high-resolution%2520%2528Ultra-HR%2529%2520remote%2520sensing%2520imagery%2520is%2520becoming%2520increasingly%2520widespread.%2520However%252C%2520current%2520remote%2520sensing%2520foundation%2520models%2520are%2520ill-suited%2520to%2520such%2520inputs%253A%2520full-image%2520encoding%2520exhausts%2520token%2520and%2520memory%2520budgets%252C%2520while%2520resize-based%2520preprocessing%2520loses%2520fine-grained%2520and%2520answer-critical%2520details.%2520In%2520this%2520context%252C%2520guiding%2520the%2520model%2520look%2520where%2520it%2520matters%2520before%2520prediction%2520becomes%2520crucial.%2520Therefore%252C%2520we%2520present%2520ZoomSearch%252C%2520a%2520training-free%252C%2520plug-and-play%2520pipeline%2520that%2520decouples%2520%2527where%2520to%2520look%2527%2520from%2520%2527how%2520to%2520answer%2527%2520for%2520Ultra-HR%2520Remote%2520Sensing%2520Visual%2520Question%2520Answering%2520%2528RS-VQA%2529.%2520ZoomSearch%2520combines%2520Adaptive%2520Multi-Branch%2520Zoom%2520Search%252C%2520which%2520performs%2520a%2520hierarchical%2520search%2520over%2520image%2520patches%2520to%2520localize%2520query-relevant%2520regions%252C%2520with%2520Layout-Aware%2520Patch%2520Reassembly%252C%2520which%2520reorganizes%2520the%2520selected%2520patches%2520into%2520a%2520compact%252C%2520layout-faithful%2520canvas.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520Ultra-HR%2520RS-VQA%2520benchmarks%2520MME-RealWorld-RS%2520and%2520LRS-VQA%252C%2520comparing%2520against%2520%2528i%2529%2520strong%2520general%2520foundation%2520models%252C%2520%2528ii%2529%2520remote%2520sensing%2520foundation%2520models%252C%2520%2528iii%2529%2520Ultra-HR%2520RS-VQA%2520methods%252C%2520and%2520%2528iv%2529%2520plug-and-play%2520search-based%2520VQA%2520methods.%2520When%2520integrated%2520with%2520LLaVA-ov%252C%2520ZoomSearch%2520attains%2520state-of-the-art%2520accuracy%2520across%2520diverse%2520tasks%252C%2520improving%2520the%2520LLaVA-ov%2520baseline%2520by%252026.3%2525%2520on%2520LRS-VQA%2520and%2520114.8%255C%2525%2520on%2520MME-RealWorld-RS.%2520Meanwhile%252C%2520it%2520achieves%2520much%2520higher%2520inference%2520efficiency%252C%2520outperforming%2520prior%2520search-based%2520methods%2520by%252020%2525~44%2525%2520in%2520speed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Where%20It%20Matters%3A%20Training-Free%20Ultra-HR%20Remote%20Sensing%20VQA%20via%20Adaptive%20Zoom%20Search&entry.906535625=Yunqi%20Zhou%20and%20Chengjie%20Jiang%20and%20Chun%20Yuan%20and%20Jing%20Li&entry.1292438233=With%20advances%20in%20satellite%20constellations%2C%20sensor%20technologies%2C%20and%20imaging%20pipelines%2C%20ultra-high-resolution%20%28Ultra-HR%29%20remote%20sensing%20imagery%20is%20becoming%20increasingly%20widespread.%20However%2C%20current%20remote%20sensing%20foundation%20models%20are%20ill-suited%20to%20such%20inputs%3A%20full-image%20encoding%20exhausts%20token%20and%20memory%20budgets%2C%20while%20resize-based%20preprocessing%20loses%20fine-grained%20and%20answer-critical%20details.%20In%20this%20context%2C%20guiding%20the%20model%20look%20where%20it%20matters%20before%20prediction%20becomes%20crucial.%20Therefore%2C%20we%20present%20ZoomSearch%2C%20a%20training-free%2C%20plug-and-play%20pipeline%20that%20decouples%20%27where%20to%20look%27%20from%20%27how%20to%20answer%27%20for%20Ultra-HR%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RS-VQA%29.%20ZoomSearch%20combines%20Adaptive%20Multi-Branch%20Zoom%20Search%2C%20which%20performs%20a%20hierarchical%20search%20over%20image%20patches%20to%20localize%20query-relevant%20regions%2C%20with%20Layout-Aware%20Patch%20Reassembly%2C%20which%20reorganizes%20the%20selected%20patches%20into%20a%20compact%2C%20layout-faithful%20canvas.%20We%20conduct%20comprehensive%20experiments%20on%20Ultra-HR%20RS-VQA%20benchmarks%20MME-RealWorld-RS%20and%20LRS-VQA%2C%20comparing%20against%20%28i%29%20strong%20general%20foundation%20models%2C%20%28ii%29%20remote%20sensing%20foundation%20models%2C%20%28iii%29%20Ultra-HR%20RS-VQA%20methods%2C%20and%20%28iv%29%20plug-and-play%20search-based%20VQA%20methods.%20When%20integrated%20with%20LLaVA-ov%2C%20ZoomSearch%20attains%20state-of-the-art%20accuracy%20across%20diverse%20tasks%2C%20improving%20the%20LLaVA-ov%20baseline%20by%2026.3%25%20on%20LRS-VQA%20and%20114.8%5C%25%20on%20MME-RealWorld-RS.%20Meanwhile%2C%20it%20achieves%20much%20higher%20inference%20efficiency%2C%20outperforming%20prior%20search-based%20methods%20by%2020%25~44%25%20in%20speed.&entry.1838667208=http%3A//arxiv.org/abs/2511.20460v1&entry.124074799=Read"},
{"title": "When to Think and When to Look: Uncertainty-Guided Lookback", "author": "Jing Bi and Filippos Bellos and Junjia Guo and Yayuan Li and Chao Huang and Yolo Y. Tang and Luchuan Song and Susan Liang and Zhongfei Mark Zhang and Jason J. Corso and Chenliang Xu", "abstract": "Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.", "link": "http://arxiv.org/abs/2511.15613v2", "date": "2025-11-25", "relevancy": 2.7831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20to%20Think%20and%20When%20to%20Look%3A%20Uncertainty-Guided%20Lookback&body=Title%3A%20When%20to%20Think%20and%20When%20to%20Look%3A%20Uncertainty-Guided%20Lookback%0AAuthor%3A%20Jing%20Bi%20and%20Filippos%20Bellos%20and%20Junjia%20Guo%20and%20Yayuan%20Li%20and%20Chao%20Huang%20and%20Yolo%20Y.%20Tang%20and%20Luchuan%20Song%20and%20Susan%20Liang%20and%20Zhongfei%20Mark%20Zhang%20and%20Jason%20J.%20Corso%20and%20Chenliang%20Xu%0AAbstract%3A%20Test-time%20thinking%20%28that%20is%2C%20generating%20explicit%20intermediate%20reasoning%20chains%29%20is%20known%20to%20boost%20performance%20in%20large%20language%20models%20and%20has%20recently%20shown%20strong%20gains%20for%20large%20vision%20language%20models%20%28LVLMs%29.%20However%2C%20despite%20these%20promising%20results%2C%20there%20is%20still%20no%20systematic%20analysis%20of%20how%20thinking%20actually%20affects%20visual%20reasoning.%20We%20provide%20the%20first%20such%20analysis%20with%20a%20large%20scale%2C%20controlled%20comparison%20of%20thinking%20for%20LVLMs%2C%20evaluating%20ten%20variants%20from%20the%20InternVL3.5%20and%20Qwen3-VL%20families%20on%20MMMU-val%20under%20generous%20token%20budgets%20and%20multi%20pass%20decoding.%20We%20show%20that%20more%20thinking%20is%20not%20always%20better%3B%20long%20chains%20often%20yield%20long%20wrong%20trajectories%20that%20ignore%20the%20image%20and%20underperform%20the%20same%20models%20run%20in%20standard%20instruct%20mode.%20A%20deeper%20analysis%20reveals%20that%20certain%20short%20lookback%20phrases%2C%20which%20explicitly%20refer%20back%20to%20the%20image%2C%20are%20strongly%20enriched%20in%20successful%20trajectories%20and%20correlate%20with%20better%20visual%20grounding.%20Building%20on%20this%20insight%2C%20we%20propose%20uncertainty%20guided%20lookback%2C%20a%20training%20free%20decoding%20strategy%20that%20combines%20an%20uncertainty%20signal%20with%20adaptive%20lookback%20prompts%20and%20breadth%20search.%20Our%20method%20improves%20overall%20MMMU%20performance%2C%20delivers%20the%20largest%20gains%20in%20categories%20where%20standard%20thinking%20is%20weak%2C%20and%20outperforms%20several%20strong%20decoding%20baselines%2C%20setting%20a%20new%20state%20of%20the%20art%20under%20fixed%20model%20families%20and%20token%20budgets.%20We%20further%20show%20that%20this%20decoding%20strategy%20generalizes%2C%20yielding%20consistent%20improvements%20on%20five%20additional%20benchmarks%2C%20including%20two%20broad%20multimodal%20suites%20and%20math%20focused%20visual%20reasoning%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520to%2520Think%2520and%2520When%2520to%2520Look%253A%2520Uncertainty-Guided%2520Lookback%26entry.906535625%3DJing%2520Bi%2520and%2520Filippos%2520Bellos%2520and%2520Junjia%2520Guo%2520and%2520Yayuan%2520Li%2520and%2520Chao%2520Huang%2520and%2520Yolo%2520Y.%2520Tang%2520and%2520Luchuan%2520Song%2520and%2520Susan%2520Liang%2520and%2520Zhongfei%2520Mark%2520Zhang%2520and%2520Jason%2520J.%2520Corso%2520and%2520Chenliang%2520Xu%26entry.1292438233%3DTest-time%2520thinking%2520%2528that%2520is%252C%2520generating%2520explicit%2520intermediate%2520reasoning%2520chains%2529%2520is%2520known%2520to%2520boost%2520performance%2520in%2520large%2520language%2520models%2520and%2520has%2520recently%2520shown%2520strong%2520gains%2520for%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529.%2520However%252C%2520despite%2520these%2520promising%2520results%252C%2520there%2520is%2520still%2520no%2520systematic%2520analysis%2520of%2520how%2520thinking%2520actually%2520affects%2520visual%2520reasoning.%2520We%2520provide%2520the%2520first%2520such%2520analysis%2520with%2520a%2520large%2520scale%252C%2520controlled%2520comparison%2520of%2520thinking%2520for%2520LVLMs%252C%2520evaluating%2520ten%2520variants%2520from%2520the%2520InternVL3.5%2520and%2520Qwen3-VL%2520families%2520on%2520MMMU-val%2520under%2520generous%2520token%2520budgets%2520and%2520multi%2520pass%2520decoding.%2520We%2520show%2520that%2520more%2520thinking%2520is%2520not%2520always%2520better%253B%2520long%2520chains%2520often%2520yield%2520long%2520wrong%2520trajectories%2520that%2520ignore%2520the%2520image%2520and%2520underperform%2520the%2520same%2520models%2520run%2520in%2520standard%2520instruct%2520mode.%2520A%2520deeper%2520analysis%2520reveals%2520that%2520certain%2520short%2520lookback%2520phrases%252C%2520which%2520explicitly%2520refer%2520back%2520to%2520the%2520image%252C%2520are%2520strongly%2520enriched%2520in%2520successful%2520trajectories%2520and%2520correlate%2520with%2520better%2520visual%2520grounding.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520uncertainty%2520guided%2520lookback%252C%2520a%2520training%2520free%2520decoding%2520strategy%2520that%2520combines%2520an%2520uncertainty%2520signal%2520with%2520adaptive%2520lookback%2520prompts%2520and%2520breadth%2520search.%2520Our%2520method%2520improves%2520overall%2520MMMU%2520performance%252C%2520delivers%2520the%2520largest%2520gains%2520in%2520categories%2520where%2520standard%2520thinking%2520is%2520weak%252C%2520and%2520outperforms%2520several%2520strong%2520decoding%2520baselines%252C%2520setting%2520a%2520new%2520state%2520of%2520the%2520art%2520under%2520fixed%2520model%2520families%2520and%2520token%2520budgets.%2520We%2520further%2520show%2520that%2520this%2520decoding%2520strategy%2520generalizes%252C%2520yielding%2520consistent%2520improvements%2520on%2520five%2520additional%2520benchmarks%252C%2520including%2520two%2520broad%2520multimodal%2520suites%2520and%2520math%2520focused%2520visual%2520reasoning%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20to%20Think%20and%20When%20to%20Look%3A%20Uncertainty-Guided%20Lookback&entry.906535625=Jing%20Bi%20and%20Filippos%20Bellos%20and%20Junjia%20Guo%20and%20Yayuan%20Li%20and%20Chao%20Huang%20and%20Yolo%20Y.%20Tang%20and%20Luchuan%20Song%20and%20Susan%20Liang%20and%20Zhongfei%20Mark%20Zhang%20and%20Jason%20J.%20Corso%20and%20Chenliang%20Xu&entry.1292438233=Test-time%20thinking%20%28that%20is%2C%20generating%20explicit%20intermediate%20reasoning%20chains%29%20is%20known%20to%20boost%20performance%20in%20large%20language%20models%20and%20has%20recently%20shown%20strong%20gains%20for%20large%20vision%20language%20models%20%28LVLMs%29.%20However%2C%20despite%20these%20promising%20results%2C%20there%20is%20still%20no%20systematic%20analysis%20of%20how%20thinking%20actually%20affects%20visual%20reasoning.%20We%20provide%20the%20first%20such%20analysis%20with%20a%20large%20scale%2C%20controlled%20comparison%20of%20thinking%20for%20LVLMs%2C%20evaluating%20ten%20variants%20from%20the%20InternVL3.5%20and%20Qwen3-VL%20families%20on%20MMMU-val%20under%20generous%20token%20budgets%20and%20multi%20pass%20decoding.%20We%20show%20that%20more%20thinking%20is%20not%20always%20better%3B%20long%20chains%20often%20yield%20long%20wrong%20trajectories%20that%20ignore%20the%20image%20and%20underperform%20the%20same%20models%20run%20in%20standard%20instruct%20mode.%20A%20deeper%20analysis%20reveals%20that%20certain%20short%20lookback%20phrases%2C%20which%20explicitly%20refer%20back%20to%20the%20image%2C%20are%20strongly%20enriched%20in%20successful%20trajectories%20and%20correlate%20with%20better%20visual%20grounding.%20Building%20on%20this%20insight%2C%20we%20propose%20uncertainty%20guided%20lookback%2C%20a%20training%20free%20decoding%20strategy%20that%20combines%20an%20uncertainty%20signal%20with%20adaptive%20lookback%20prompts%20and%20breadth%20search.%20Our%20method%20improves%20overall%20MMMU%20performance%2C%20delivers%20the%20largest%20gains%20in%20categories%20where%20standard%20thinking%20is%20weak%2C%20and%20outperforms%20several%20strong%20decoding%20baselines%2C%20setting%20a%20new%20state%20of%20the%20art%20under%20fixed%20model%20families%20and%20token%20budgets.%20We%20further%20show%20that%20this%20decoding%20strategy%20generalizes%2C%20yielding%20consistent%20improvements%20on%20five%20additional%20benchmarks%2C%20including%20two%20broad%20multimodal%20suites%20and%20math%20focused%20visual%20reasoning%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.15613v2&entry.124074799=Read"},
{"title": "Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models", "author": "Karim Kadry and Abdallah Abdelwahed and Shoaib Goraya and Ajay Manicka and Naravich Chutisilp and Farhad Nezami and Elazer Edelman", "abstract": "We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.", "link": "http://arxiv.org/abs/2511.20587v1", "date": "2025-11-25", "relevancy": 2.7579, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.556}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5494}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anatomica%3A%20Localized%20Control%20over%20Geometric%20and%20Topological%20Properties%20for%20Anatomical%20Diffusion%20Models&body=Title%3A%20Anatomica%3A%20Localized%20Control%20over%20Geometric%20and%20Topological%20Properties%20for%20Anatomical%20Diffusion%20Models%0AAuthor%3A%20Karim%20Kadry%20and%20Abdallah%20Abdelwahed%20and%20Shoaib%20Goraya%20and%20Ajay%20Manicka%20and%20Naravich%20Chutisilp%20and%20Farhad%20Nezami%20and%20Elazer%20Edelman%0AAbstract%3A%20We%20present%20Anatomica%3A%20an%20inference-time%20framework%20for%20generating%20multi-class%20anatomical%20voxel%20maps%20with%20localized%20geo-topological%20control.%20During%20generation%2C%20we%20use%20cuboidal%20control%20domains%20of%20varying%20dimensionality%2C%20location%2C%20and%20shape%20to%20slice%20out%20relevant%20substructures.%20These%20local%20substructures%20are%20used%20to%20compute%20differentiable%20penalty%20functions%20that%20steer%20the%20sample%20towards%20target%20constraints.%20We%20control%20geometric%20features%20such%20as%20size%2C%20shape%2C%20and%20position%20through%20voxel-wise%20moments%2C%20while%20topological%20features%20such%20as%20connected%20components%2C%20loops%2C%20and%20voids%20are%20enforced%20through%20persistent%20homology.%20Lastly%2C%20we%20implement%20Anatomica%20for%20latent%20diffusion%20models%2C%20where%20neural%20field%20decoders%20partially%20extract%20substructures%2C%20enabling%20the%20efficient%20control%20of%20anatomical%20properties.%20Anatomica%20applies%20flexibly%20across%20diverse%20anatomical%20systems%2C%20composing%20constraints%20to%20control%20complex%20structures%20over%20arbitrary%20dimensions%20and%20coordinate%20systems%2C%20thereby%20enabling%20the%20rational%20design%20of%20synthetic%20datasets%20for%20virtual%20trials%20or%20machine%20learning%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatomica%253A%2520Localized%2520Control%2520over%2520Geometric%2520and%2520Topological%2520Properties%2520for%2520Anatomical%2520Diffusion%2520Models%26entry.906535625%3DKarim%2520Kadry%2520and%2520Abdallah%2520Abdelwahed%2520and%2520Shoaib%2520Goraya%2520and%2520Ajay%2520Manicka%2520and%2520Naravich%2520Chutisilp%2520and%2520Farhad%2520Nezami%2520and%2520Elazer%2520Edelman%26entry.1292438233%3DWe%2520present%2520Anatomica%253A%2520an%2520inference-time%2520framework%2520for%2520generating%2520multi-class%2520anatomical%2520voxel%2520maps%2520with%2520localized%2520geo-topological%2520control.%2520During%2520generation%252C%2520we%2520use%2520cuboidal%2520control%2520domains%2520of%2520varying%2520dimensionality%252C%2520location%252C%2520and%2520shape%2520to%2520slice%2520out%2520relevant%2520substructures.%2520These%2520local%2520substructures%2520are%2520used%2520to%2520compute%2520differentiable%2520penalty%2520functions%2520that%2520steer%2520the%2520sample%2520towards%2520target%2520constraints.%2520We%2520control%2520geometric%2520features%2520such%2520as%2520size%252C%2520shape%252C%2520and%2520position%2520through%2520voxel-wise%2520moments%252C%2520while%2520topological%2520features%2520such%2520as%2520connected%2520components%252C%2520loops%252C%2520and%2520voids%2520are%2520enforced%2520through%2520persistent%2520homology.%2520Lastly%252C%2520we%2520implement%2520Anatomica%2520for%2520latent%2520diffusion%2520models%252C%2520where%2520neural%2520field%2520decoders%2520partially%2520extract%2520substructures%252C%2520enabling%2520the%2520efficient%2520control%2520of%2520anatomical%2520properties.%2520Anatomica%2520applies%2520flexibly%2520across%2520diverse%2520anatomical%2520systems%252C%2520composing%2520constraints%2520to%2520control%2520complex%2520structures%2520over%2520arbitrary%2520dimensions%2520and%2520coordinate%2520systems%252C%2520thereby%2520enabling%2520the%2520rational%2520design%2520of%2520synthetic%2520datasets%2520for%2520virtual%2520trials%2520or%2520machine%2520learning%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomica%3A%20Localized%20Control%20over%20Geometric%20and%20Topological%20Properties%20for%20Anatomical%20Diffusion%20Models&entry.906535625=Karim%20Kadry%20and%20Abdallah%20Abdelwahed%20and%20Shoaib%20Goraya%20and%20Ajay%20Manicka%20and%20Naravich%20Chutisilp%20and%20Farhad%20Nezami%20and%20Elazer%20Edelman&entry.1292438233=We%20present%20Anatomica%3A%20an%20inference-time%20framework%20for%20generating%20multi-class%20anatomical%20voxel%20maps%20with%20localized%20geo-topological%20control.%20During%20generation%2C%20we%20use%20cuboidal%20control%20domains%20of%20varying%20dimensionality%2C%20location%2C%20and%20shape%20to%20slice%20out%20relevant%20substructures.%20These%20local%20substructures%20are%20used%20to%20compute%20differentiable%20penalty%20functions%20that%20steer%20the%20sample%20towards%20target%20constraints.%20We%20control%20geometric%20features%20such%20as%20size%2C%20shape%2C%20and%20position%20through%20voxel-wise%20moments%2C%20while%20topological%20features%20such%20as%20connected%20components%2C%20loops%2C%20and%20voids%20are%20enforced%20through%20persistent%20homology.%20Lastly%2C%20we%20implement%20Anatomica%20for%20latent%20diffusion%20models%2C%20where%20neural%20field%20decoders%20partially%20extract%20substructures%2C%20enabling%20the%20efficient%20control%20of%20anatomical%20properties.%20Anatomica%20applies%20flexibly%20across%20diverse%20anatomical%20systems%2C%20composing%20constraints%20to%20control%20complex%20structures%20over%20arbitrary%20dimensions%20and%20coordinate%20systems%2C%20thereby%20enabling%20the%20rational%20design%20of%20synthetic%20datasets%20for%20virtual%20trials%20or%20machine%20learning%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2511.20587v1&entry.124074799=Read"},
{"title": "PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding", "author": "Haoze Zhang and Tianyu Huang and Zichen Wan and Xiaowei Jin and Hongzhi Zhang and Hui Li and Wangmeng Zuo", "abstract": "While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.", "link": "http://arxiv.org/abs/2511.20562v1", "date": "2025-11-25", "relevancy": 2.753, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7656}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6408}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysChoreo%3A%20Physics-Controllable%20Video%20Generation%20with%20Part-Aware%20Semantic%20Grounding&body=Title%3A%20PhysChoreo%3A%20Physics-Controllable%20Video%20Generation%20with%20Part-Aware%20Semantic%20Grounding%0AAuthor%3A%20Haoze%20Zhang%20and%20Tianyu%20Huang%20and%20Zichen%20Wan%20and%20Xiaowei%20Jin%20and%20Hongzhi%20Zhang%20and%20Hui%20Li%20and%20Wangmeng%20Zuo%0AAbstract%3A%20While%20recent%20video%20generation%20models%20have%20achieved%20significant%20visual%20fidelity%2C%20they%20often%20suffer%20from%20the%20lack%20of%20explicit%20physical%20controllability%20and%20plausibility.%20To%20address%20this%2C%20some%20recent%20studies%20attempted%20to%20guide%20the%20video%20generation%20with%20physics-based%20rendering.%20However%2C%20these%20methods%20face%20inherent%20challenges%20in%20accurately%20modeling%20complex%20physical%20properties%20and%20effectively%20control%20ling%20the%20resulting%20physical%20behavior%20over%20extended%20temporal%20sequences.%20In%20this%20work%2C%20we%20introduce%20PhysChoreo%2C%20a%20novel%20framework%20that%20can%20generate%20videos%20with%20diverse%20controllability%20and%20physical%20realism%20from%20a%20single%20image.%20Our%20method%20consists%20of%20two%20stages%3A%20first%2C%20it%20estimates%20the%20static%20initial%20physical%20properties%20of%20all%20objects%20in%20the%20image%20through%20part-aware%20physical%20property%20reconstruction.%20Then%2C%20through%20temporally%20instructed%20and%20physically%20editable%20simulation%2C%20it%20synthesizes%20high-quality%20videos%20with%20rich%20dynamic%20behaviors%20and%20physical%20realism.%20Experimental%20results%20show%20that%20PhysChoreo%20can%20generate%20videos%20with%20rich%20behaviors%20and%20physical%20realism%2C%20outperforming%20state-of-the-art%20methods%20on%20multiple%20evaluation%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysChoreo%253A%2520Physics-Controllable%2520Video%2520Generation%2520with%2520Part-Aware%2520Semantic%2520Grounding%26entry.906535625%3DHaoze%2520Zhang%2520and%2520Tianyu%2520Huang%2520and%2520Zichen%2520Wan%2520and%2520Xiaowei%2520Jin%2520and%2520Hongzhi%2520Zhang%2520and%2520Hui%2520Li%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3DWhile%2520recent%2520video%2520generation%2520models%2520have%2520achieved%2520significant%2520visual%2520fidelity%252C%2520they%2520often%2520suffer%2520from%2520the%2520lack%2520of%2520explicit%2520physical%2520controllability%2520and%2520plausibility.%2520To%2520address%2520this%252C%2520some%2520recent%2520studies%2520attempted%2520to%2520guide%2520the%2520video%2520generation%2520with%2520physics-based%2520rendering.%2520However%252C%2520these%2520methods%2520face%2520inherent%2520challenges%2520in%2520accurately%2520modeling%2520complex%2520physical%2520properties%2520and%2520effectively%2520control%2520ling%2520the%2520resulting%2520physical%2520behavior%2520over%2520extended%2520temporal%2520sequences.%2520In%2520this%2520work%252C%2520we%2520introduce%2520PhysChoreo%252C%2520a%2520novel%2520framework%2520that%2520can%2520generate%2520videos%2520with%2520diverse%2520controllability%2520and%2520physical%2520realism%2520from%2520a%2520single%2520image.%2520Our%2520method%2520consists%2520of%2520two%2520stages%253A%2520first%252C%2520it%2520estimates%2520the%2520static%2520initial%2520physical%2520properties%2520of%2520all%2520objects%2520in%2520the%2520image%2520through%2520part-aware%2520physical%2520property%2520reconstruction.%2520Then%252C%2520through%2520temporally%2520instructed%2520and%2520physically%2520editable%2520simulation%252C%2520it%2520synthesizes%2520high-quality%2520videos%2520with%2520rich%2520dynamic%2520behaviors%2520and%2520physical%2520realism.%2520Experimental%2520results%2520show%2520that%2520PhysChoreo%2520can%2520generate%2520videos%2520with%2520rich%2520behaviors%2520and%2520physical%2520realism%252C%2520outperforming%2520state-of-the-art%2520methods%2520on%2520multiple%2520evaluation%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysChoreo%3A%20Physics-Controllable%20Video%20Generation%20with%20Part-Aware%20Semantic%20Grounding&entry.906535625=Haoze%20Zhang%20and%20Tianyu%20Huang%20and%20Zichen%20Wan%20and%20Xiaowei%20Jin%20and%20Hongzhi%20Zhang%20and%20Hui%20Li%20and%20Wangmeng%20Zuo&entry.1292438233=While%20recent%20video%20generation%20models%20have%20achieved%20significant%20visual%20fidelity%2C%20they%20often%20suffer%20from%20the%20lack%20of%20explicit%20physical%20controllability%20and%20plausibility.%20To%20address%20this%2C%20some%20recent%20studies%20attempted%20to%20guide%20the%20video%20generation%20with%20physics-based%20rendering.%20However%2C%20these%20methods%20face%20inherent%20challenges%20in%20accurately%20modeling%20complex%20physical%20properties%20and%20effectively%20control%20ling%20the%20resulting%20physical%20behavior%20over%20extended%20temporal%20sequences.%20In%20this%20work%2C%20we%20introduce%20PhysChoreo%2C%20a%20novel%20framework%20that%20can%20generate%20videos%20with%20diverse%20controllability%20and%20physical%20realism%20from%20a%20single%20image.%20Our%20method%20consists%20of%20two%20stages%3A%20first%2C%20it%20estimates%20the%20static%20initial%20physical%20properties%20of%20all%20objects%20in%20the%20image%20through%20part-aware%20physical%20property%20reconstruction.%20Then%2C%20through%20temporally%20instructed%20and%20physically%20editable%20simulation%2C%20it%20synthesizes%20high-quality%20videos%20with%20rich%20dynamic%20behaviors%20and%20physical%20realism.%20Experimental%20results%20show%20that%20PhysChoreo%20can%20generate%20videos%20with%20rich%20behaviors%20and%20physical%20realism%2C%20outperforming%20state-of-the-art%20methods%20on%20multiple%20evaluation%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.20562v1&entry.124074799=Read"},
{"title": "Panoptic Captioning: An Equivalence Bridge for Image and Text", "author": "Kun-Yu Lin and Hongjun Wang and Weining Ren and Kai Han", "abstract": "This work introduces panoptic captioning, a novel task striving to seek the minimum text equivalent of images, which has broad potential applications. We take the first step towards panoptic captioning by formulating it as a task of generating a comprehensive textual description for an image, which encapsulates all entities, their respective locations and attributes, relationships among entities, as well as global image state. Through an extensive evaluation, our work reveals that state-of-the-art Multi-modal Large Language Models (MLLMs) have limited performance in solving panoptic captioning. To address this, we propose an effective data engine named PancapEngine to produce high-quality data and a novel method named PancapChain to improve panoptic captioning. Specifically, our PancapEngine first detects diverse categories of entities in images by an elaborate detection suite, and then generates required panoptic captions using entity-aware prompts. Additionally, our PancapChain explicitly decouples the challenging panoptic captioning task into multiple stages and generates panoptic captions step by step. More importantly, we contribute a comprehensive metric named PancapScore and a human-curated test set for reliable model evaluation. Experiments show that our PancapChain-13B model can beat state-of-the-art open-source MLLMs like InternVL-2.5-78B and even surpass proprietary models like GPT-4o and Gemini-2.0-Pro, demonstrating the effectiveness of our data engine and method. Project page: https://visual-ai.github.io/pancap/", "link": "http://arxiv.org/abs/2505.16334v3", "date": "2025-11-25", "relevancy": 2.749, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panoptic%20Captioning%3A%20An%20Equivalence%20Bridge%20for%20Image%20and%20Text&body=Title%3A%20Panoptic%20Captioning%3A%20An%20Equivalence%20Bridge%20for%20Image%20and%20Text%0AAuthor%3A%20Kun-Yu%20Lin%20and%20Hongjun%20Wang%20and%20Weining%20Ren%20and%20Kai%20Han%0AAbstract%3A%20This%20work%20introduces%20panoptic%20captioning%2C%20a%20novel%20task%20striving%20to%20seek%20the%20minimum%20text%20equivalent%20of%20images%2C%20which%20has%20broad%20potential%20applications.%20We%20take%20the%20first%20step%20towards%20panoptic%20captioning%20by%20formulating%20it%20as%20a%20task%20of%20generating%20a%20comprehensive%20textual%20description%20for%20an%20image%2C%20which%20encapsulates%20all%20entities%2C%20their%20respective%20locations%20and%20attributes%2C%20relationships%20among%20entities%2C%20as%20well%20as%20global%20image%20state.%20Through%20an%20extensive%20evaluation%2C%20our%20work%20reveals%20that%20state-of-the-art%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20limited%20performance%20in%20solving%20panoptic%20captioning.%20To%20address%20this%2C%20we%20propose%20an%20effective%20data%20engine%20named%20PancapEngine%20to%20produce%20high-quality%20data%20and%20a%20novel%20method%20named%20PancapChain%20to%20improve%20panoptic%20captioning.%20Specifically%2C%20our%20PancapEngine%20first%20detects%20diverse%20categories%20of%20entities%20in%20images%20by%20an%20elaborate%20detection%20suite%2C%20and%20then%20generates%20required%20panoptic%20captions%20using%20entity-aware%20prompts.%20Additionally%2C%20our%20PancapChain%20explicitly%20decouples%20the%20challenging%20panoptic%20captioning%20task%20into%20multiple%20stages%20and%20generates%20panoptic%20captions%20step%20by%20step.%20More%20importantly%2C%20we%20contribute%20a%20comprehensive%20metric%20named%20PancapScore%20and%20a%20human-curated%20test%20set%20for%20reliable%20model%20evaluation.%20Experiments%20show%20that%20our%20PancapChain-13B%20model%20can%20beat%20state-of-the-art%20open-source%20MLLMs%20like%20InternVL-2.5-78B%20and%20even%20surpass%20proprietary%20models%20like%20GPT-4o%20and%20Gemini-2.0-Pro%2C%20demonstrating%20the%20effectiveness%20of%20our%20data%20engine%20and%20method.%20Project%20page%3A%20https%3A//visual-ai.github.io/pancap/%0ALink%3A%20http%3A//arxiv.org/abs/2505.16334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoptic%2520Captioning%253A%2520An%2520Equivalence%2520Bridge%2520for%2520Image%2520and%2520Text%26entry.906535625%3DKun-Yu%2520Lin%2520and%2520Hongjun%2520Wang%2520and%2520Weining%2520Ren%2520and%2520Kai%2520Han%26entry.1292438233%3DThis%2520work%2520introduces%2520panoptic%2520captioning%252C%2520a%2520novel%2520task%2520striving%2520to%2520seek%2520the%2520minimum%2520text%2520equivalent%2520of%2520images%252C%2520which%2520has%2520broad%2520potential%2520applications.%2520We%2520take%2520the%2520first%2520step%2520towards%2520panoptic%2520captioning%2520by%2520formulating%2520it%2520as%2520a%2520task%2520of%2520generating%2520a%2520comprehensive%2520textual%2520description%2520for%2520an%2520image%252C%2520which%2520encapsulates%2520all%2520entities%252C%2520their%2520respective%2520locations%2520and%2520attributes%252C%2520relationships%2520among%2520entities%252C%2520as%2520well%2520as%2520global%2520image%2520state.%2520Through%2520an%2520extensive%2520evaluation%252C%2520our%2520work%2520reveals%2520that%2520state-of-the-art%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520limited%2520performance%2520in%2520solving%2520panoptic%2520captioning.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520effective%2520data%2520engine%2520named%2520PancapEngine%2520to%2520produce%2520high-quality%2520data%2520and%2520a%2520novel%2520method%2520named%2520PancapChain%2520to%2520improve%2520panoptic%2520captioning.%2520Specifically%252C%2520our%2520PancapEngine%2520first%2520detects%2520diverse%2520categories%2520of%2520entities%2520in%2520images%2520by%2520an%2520elaborate%2520detection%2520suite%252C%2520and%2520then%2520generates%2520required%2520panoptic%2520captions%2520using%2520entity-aware%2520prompts.%2520Additionally%252C%2520our%2520PancapChain%2520explicitly%2520decouples%2520the%2520challenging%2520panoptic%2520captioning%2520task%2520into%2520multiple%2520stages%2520and%2520generates%2520panoptic%2520captions%2520step%2520by%2520step.%2520More%2520importantly%252C%2520we%2520contribute%2520a%2520comprehensive%2520metric%2520named%2520PancapScore%2520and%2520a%2520human-curated%2520test%2520set%2520for%2520reliable%2520model%2520evaluation.%2520Experiments%2520show%2520that%2520our%2520PancapChain-13B%2520model%2520can%2520beat%2520state-of-the-art%2520open-source%2520MLLMs%2520like%2520InternVL-2.5-78B%2520and%2520even%2520surpass%2520proprietary%2520models%2520like%2520GPT-4o%2520and%2520Gemini-2.0-Pro%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520data%2520engine%2520and%2520method.%2520Project%2520page%253A%2520https%253A//visual-ai.github.io/pancap/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panoptic%20Captioning%3A%20An%20Equivalence%20Bridge%20for%20Image%20and%20Text&entry.906535625=Kun-Yu%20Lin%20and%20Hongjun%20Wang%20and%20Weining%20Ren%20and%20Kai%20Han&entry.1292438233=This%20work%20introduces%20panoptic%20captioning%2C%20a%20novel%20task%20striving%20to%20seek%20the%20minimum%20text%20equivalent%20of%20images%2C%20which%20has%20broad%20potential%20applications.%20We%20take%20the%20first%20step%20towards%20panoptic%20captioning%20by%20formulating%20it%20as%20a%20task%20of%20generating%20a%20comprehensive%20textual%20description%20for%20an%20image%2C%20which%20encapsulates%20all%20entities%2C%20their%20respective%20locations%20and%20attributes%2C%20relationships%20among%20entities%2C%20as%20well%20as%20global%20image%20state.%20Through%20an%20extensive%20evaluation%2C%20our%20work%20reveals%20that%20state-of-the-art%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20limited%20performance%20in%20solving%20panoptic%20captioning.%20To%20address%20this%2C%20we%20propose%20an%20effective%20data%20engine%20named%20PancapEngine%20to%20produce%20high-quality%20data%20and%20a%20novel%20method%20named%20PancapChain%20to%20improve%20panoptic%20captioning.%20Specifically%2C%20our%20PancapEngine%20first%20detects%20diverse%20categories%20of%20entities%20in%20images%20by%20an%20elaborate%20detection%20suite%2C%20and%20then%20generates%20required%20panoptic%20captions%20using%20entity-aware%20prompts.%20Additionally%2C%20our%20PancapChain%20explicitly%20decouples%20the%20challenging%20panoptic%20captioning%20task%20into%20multiple%20stages%20and%20generates%20panoptic%20captions%20step%20by%20step.%20More%20importantly%2C%20we%20contribute%20a%20comprehensive%20metric%20named%20PancapScore%20and%20a%20human-curated%20test%20set%20for%20reliable%20model%20evaluation.%20Experiments%20show%20that%20our%20PancapChain-13B%20model%20can%20beat%20state-of-the-art%20open-source%20MLLMs%20like%20InternVL-2.5-78B%20and%20even%20surpass%20proprietary%20models%20like%20GPT-4o%20and%20Gemini-2.0-Pro%2C%20demonstrating%20the%20effectiveness%20of%20our%20data%20engine%20and%20method.%20Project%20page%3A%20https%3A//visual-ai.github.io/pancap/&entry.1838667208=http%3A//arxiv.org/abs/2505.16334v3&entry.124074799=Read"},
{"title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining", "author": "Adhiraj Ghosh and Vishaal Udandarao and Thao Nguyen and Matteo Farina and Mehdi Cherti and Jenia Jitsev and Sewoong Oh and Elisa Ricci and Ludwig Schmidt and Matthias Bethge", "abstract": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.", "link": "http://arxiv.org/abs/2511.20643v1", "date": "2025-11-25", "relevancy": 2.734, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept-Aware%20Batch%20Sampling%20Improves%20Language-Image%20Pretraining&body=Title%3A%20Concept-Aware%20Batch%20Sampling%20Improves%20Language-Image%20Pretraining%0AAuthor%3A%20Adhiraj%20Ghosh%20and%20Vishaal%20Udandarao%20and%20Thao%20Nguyen%20and%20Matteo%20Farina%20and%20Mehdi%20Cherti%20and%20Jenia%20Jitsev%20and%20Sewoong%20Oh%20and%20Elisa%20Ricci%20and%20Ludwig%20Schmidt%20and%20Matthias%20Bethge%0AAbstract%3A%20What%20data%20should%20a%20vision-language%20model%20be%20trained%20on%3F%20To%20answer%20this%20question%2C%20many%20data%20curation%20efforts%20center%20on%20the%20quality%20of%20a%20dataset.%20However%2C%20most%20of%20these%20existing%20methods%20are%20%28i%29%20offline%2C%20i.e.%20they%20produce%20a%20static%20dataset%20from%20a%20set%20of%20predetermined%20filtering%20criteria%2C%20and%20%28ii%29%20concept-agnostic%2C%20i.e.%20they%20use%20model-based%20filters%20which%20induce%20additional%20data%20biases.%20In%20this%20work%2C%20we%20go%20beyond%20such%20offline%2C%20concept-agnostic%20methods%20and%20advocate%20for%20more%20flexible%2C%20task-adaptive%20online%20concept-based%20curation.%20Our%20first%20contribution%20is%20DataConcept%2C%20a%20collection%20of%20128M%20web-crawled%20image-text%20pairs%20annotated%20with%20fine-grained%20details%20about%20their%20concept%20composition.%20Building%20on%20DataConcept%2C%20we%20introduce%20Concept-Aware%20Batch%20Sampling%20%28CABS%29%2C%20a%20simple%20yet%20effective%20batch%20sampling%20framework%20that%20flexibly%20constructs%20batches%20on-the-fly%20based%20on%20specific%20target%20distributions.%20We%20propose%20two%20variants%3A%20%28i%29%20Diversity%20Maximization%20%28CABS-DM%29%20to%20curate%20batches%20with%20a%20broad%20coverage%20of%20available%20concepts%2C%20and%20%28ii%29%20Frequency%20Maximization%20%28CABS-FM%29%20to%20curate%20batches%20with%20high%20object%20multiplicity.%20Through%20extensive%20evaluations%20across%2028%20benchmarks%2C%20we%20demonstrate%20that%20our%20CABS%20method%20significantly%20benefits%20CLIP/SigLIP%20model%20classes%20and%20yields%20highly%20performant%20models.%20Overall%2C%20CABS%20represents%20a%20strong%20open-source%20alternative%20to%20proprietary%20online%20data%20curation%20algorithms%2C%20enabling%20practitioners%20to%20define%20custom%20concept%20distributions%20that%20optimize%20for%20specific%20downstream%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept-Aware%2520Batch%2520Sampling%2520Improves%2520Language-Image%2520Pretraining%26entry.906535625%3DAdhiraj%2520Ghosh%2520and%2520Vishaal%2520Udandarao%2520and%2520Thao%2520Nguyen%2520and%2520Matteo%2520Farina%2520and%2520Mehdi%2520Cherti%2520and%2520Jenia%2520Jitsev%2520and%2520Sewoong%2520Oh%2520and%2520Elisa%2520Ricci%2520and%2520Ludwig%2520Schmidt%2520and%2520Matthias%2520Bethge%26entry.1292438233%3DWhat%2520data%2520should%2520a%2520vision-language%2520model%2520be%2520trained%2520on%253F%2520To%2520answer%2520this%2520question%252C%2520many%2520data%2520curation%2520efforts%2520center%2520on%2520the%2520quality%2520of%2520a%2520dataset.%2520However%252C%2520most%2520of%2520these%2520existing%2520methods%2520are%2520%2528i%2529%2520offline%252C%2520i.e.%2520they%2520produce%2520a%2520static%2520dataset%2520from%2520a%2520set%2520of%2520predetermined%2520filtering%2520criteria%252C%2520and%2520%2528ii%2529%2520concept-agnostic%252C%2520i.e.%2520they%2520use%2520model-based%2520filters%2520which%2520induce%2520additional%2520data%2520biases.%2520In%2520this%2520work%252C%2520we%2520go%2520beyond%2520such%2520offline%252C%2520concept-agnostic%2520methods%2520and%2520advocate%2520for%2520more%2520flexible%252C%2520task-adaptive%2520online%2520concept-based%2520curation.%2520Our%2520first%2520contribution%2520is%2520DataConcept%252C%2520a%2520collection%2520of%2520128M%2520web-crawled%2520image-text%2520pairs%2520annotated%2520with%2520fine-grained%2520details%2520about%2520their%2520concept%2520composition.%2520Building%2520on%2520DataConcept%252C%2520we%2520introduce%2520Concept-Aware%2520Batch%2520Sampling%2520%2528CABS%2529%252C%2520a%2520simple%2520yet%2520effective%2520batch%2520sampling%2520framework%2520that%2520flexibly%2520constructs%2520batches%2520on-the-fly%2520based%2520on%2520specific%2520target%2520distributions.%2520We%2520propose%2520two%2520variants%253A%2520%2528i%2529%2520Diversity%2520Maximization%2520%2528CABS-DM%2529%2520to%2520curate%2520batches%2520with%2520a%2520broad%2520coverage%2520of%2520available%2520concepts%252C%2520and%2520%2528ii%2529%2520Frequency%2520Maximization%2520%2528CABS-FM%2529%2520to%2520curate%2520batches%2520with%2520high%2520object%2520multiplicity.%2520Through%2520extensive%2520evaluations%2520across%252028%2520benchmarks%252C%2520we%2520demonstrate%2520that%2520our%2520CABS%2520method%2520significantly%2520benefits%2520CLIP/SigLIP%2520model%2520classes%2520and%2520yields%2520highly%2520performant%2520models.%2520Overall%252C%2520CABS%2520represents%2520a%2520strong%2520open-source%2520alternative%2520to%2520proprietary%2520online%2520data%2520curation%2520algorithms%252C%2520enabling%2520practitioners%2520to%2520define%2520custom%2520concept%2520distributions%2520that%2520optimize%2520for%2520specific%2520downstream%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept-Aware%20Batch%20Sampling%20Improves%20Language-Image%20Pretraining&entry.906535625=Adhiraj%20Ghosh%20and%20Vishaal%20Udandarao%20and%20Thao%20Nguyen%20and%20Matteo%20Farina%20and%20Mehdi%20Cherti%20and%20Jenia%20Jitsev%20and%20Sewoong%20Oh%20and%20Elisa%20Ricci%20and%20Ludwig%20Schmidt%20and%20Matthias%20Bethge&entry.1292438233=What%20data%20should%20a%20vision-language%20model%20be%20trained%20on%3F%20To%20answer%20this%20question%2C%20many%20data%20curation%20efforts%20center%20on%20the%20quality%20of%20a%20dataset.%20However%2C%20most%20of%20these%20existing%20methods%20are%20%28i%29%20offline%2C%20i.e.%20they%20produce%20a%20static%20dataset%20from%20a%20set%20of%20predetermined%20filtering%20criteria%2C%20and%20%28ii%29%20concept-agnostic%2C%20i.e.%20they%20use%20model-based%20filters%20which%20induce%20additional%20data%20biases.%20In%20this%20work%2C%20we%20go%20beyond%20such%20offline%2C%20concept-agnostic%20methods%20and%20advocate%20for%20more%20flexible%2C%20task-adaptive%20online%20concept-based%20curation.%20Our%20first%20contribution%20is%20DataConcept%2C%20a%20collection%20of%20128M%20web-crawled%20image-text%20pairs%20annotated%20with%20fine-grained%20details%20about%20their%20concept%20composition.%20Building%20on%20DataConcept%2C%20we%20introduce%20Concept-Aware%20Batch%20Sampling%20%28CABS%29%2C%20a%20simple%20yet%20effective%20batch%20sampling%20framework%20that%20flexibly%20constructs%20batches%20on-the-fly%20based%20on%20specific%20target%20distributions.%20We%20propose%20two%20variants%3A%20%28i%29%20Diversity%20Maximization%20%28CABS-DM%29%20to%20curate%20batches%20with%20a%20broad%20coverage%20of%20available%20concepts%2C%20and%20%28ii%29%20Frequency%20Maximization%20%28CABS-FM%29%20to%20curate%20batches%20with%20high%20object%20multiplicity.%20Through%20extensive%20evaluations%20across%2028%20benchmarks%2C%20we%20demonstrate%20that%20our%20CABS%20method%20significantly%20benefits%20CLIP/SigLIP%20model%20classes%20and%20yields%20highly%20performant%20models.%20Overall%2C%20CABS%20represents%20a%20strong%20open-source%20alternative%20to%20proprietary%20online%20data%20curation%20algorithms%2C%20enabling%20practitioners%20to%20define%20custom%20concept%20distributions%20that%20optimize%20for%20specific%20downstream%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.20643v1&entry.124074799=Read"},
{"title": "Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification", "author": "Akshit Pramod Anchan and Jewelith Thomas and Sritama Roy", "abstract": "Developing comprehensive assistive technologies requires the seamless integration of visual and auditory perception. This research evaluates the feasibility of a modular architecture inspired by core functionalities of perceptive systems like 'Smart Eye.' We propose and benchmark three independent sensing modules: a Convolutional Neural Network (CNN) for eye state detection (drowsiness/attention), a deep CNN for facial expression recognition, and a Long Short-Term Memory (LSTM) network for voice-based speaker identification. Utilizing the Eyes Image, FER2013, and customized audio datasets, our models achieved accuracies of 93.0%, 97.8%, and 96.89%, respectively. This study demonstrates that lightweight, domain-specific models can achieve high fidelity on discrete tasks, establishing a validated foundation for future real-time, multimodal integration in resource-constrained assistive devices.", "link": "http://arxiv.org/abs/2511.20474v1", "date": "2025-11-25", "relevancy": 2.7248, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Deep%20Learning%20Framework%20for%20Assistive%20Perception%3A%20Gaze%2C%20Affect%2C%20and%20Speaker%20Identification&body=Title%3A%20Modular%20Deep%20Learning%20Framework%20for%20Assistive%20Perception%3A%20Gaze%2C%20Affect%2C%20and%20Speaker%20Identification%0AAuthor%3A%20Akshit%20Pramod%20Anchan%20and%20Jewelith%20Thomas%20and%20Sritama%20Roy%0AAbstract%3A%20Developing%20comprehensive%20assistive%20technologies%20requires%20the%20seamless%20integration%20of%20visual%20and%20auditory%20perception.%20This%20research%20evaluates%20the%20feasibility%20of%20a%20modular%20architecture%20inspired%20by%20core%20functionalities%20of%20perceptive%20systems%20like%20%27Smart%20Eye.%27%20We%20propose%20and%20benchmark%20three%20independent%20sensing%20modules%3A%20a%20Convolutional%20Neural%20Network%20%28CNN%29%20for%20eye%20state%20detection%20%28drowsiness/attention%29%2C%20a%20deep%20CNN%20for%20facial%20expression%20recognition%2C%20and%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20network%20for%20voice-based%20speaker%20identification.%20Utilizing%20the%20Eyes%20Image%2C%20FER2013%2C%20and%20customized%20audio%20datasets%2C%20our%20models%20achieved%20accuracies%20of%2093.0%25%2C%2097.8%25%2C%20and%2096.89%25%2C%20respectively.%20This%20study%20demonstrates%20that%20lightweight%2C%20domain-specific%20models%20can%20achieve%20high%20fidelity%20on%20discrete%20tasks%2C%20establishing%20a%20validated%20foundation%20for%20future%20real-time%2C%20multimodal%20integration%20in%20resource-constrained%20assistive%20devices.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Deep%2520Learning%2520Framework%2520for%2520Assistive%2520Perception%253A%2520Gaze%252C%2520Affect%252C%2520and%2520Speaker%2520Identification%26entry.906535625%3DAkshit%2520Pramod%2520Anchan%2520and%2520Jewelith%2520Thomas%2520and%2520Sritama%2520Roy%26entry.1292438233%3DDeveloping%2520comprehensive%2520assistive%2520technologies%2520requires%2520the%2520seamless%2520integration%2520of%2520visual%2520and%2520auditory%2520perception.%2520This%2520research%2520evaluates%2520the%2520feasibility%2520of%2520a%2520modular%2520architecture%2520inspired%2520by%2520core%2520functionalities%2520of%2520perceptive%2520systems%2520like%2520%2527Smart%2520Eye.%2527%2520We%2520propose%2520and%2520benchmark%2520three%2520independent%2520sensing%2520modules%253A%2520a%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520for%2520eye%2520state%2520detection%2520%2528drowsiness/attention%2529%252C%2520a%2520deep%2520CNN%2520for%2520facial%2520expression%2520recognition%252C%2520and%2520a%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520network%2520for%2520voice-based%2520speaker%2520identification.%2520Utilizing%2520the%2520Eyes%2520Image%252C%2520FER2013%252C%2520and%2520customized%2520audio%2520datasets%252C%2520our%2520models%2520achieved%2520accuracies%2520of%252093.0%2525%252C%252097.8%2525%252C%2520and%252096.89%2525%252C%2520respectively.%2520This%2520study%2520demonstrates%2520that%2520lightweight%252C%2520domain-specific%2520models%2520can%2520achieve%2520high%2520fidelity%2520on%2520discrete%2520tasks%252C%2520establishing%2520a%2520validated%2520foundation%2520for%2520future%2520real-time%252C%2520multimodal%2520integration%2520in%2520resource-constrained%2520assistive%2520devices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Deep%20Learning%20Framework%20for%20Assistive%20Perception%3A%20Gaze%2C%20Affect%2C%20and%20Speaker%20Identification&entry.906535625=Akshit%20Pramod%20Anchan%20and%20Jewelith%20Thomas%20and%20Sritama%20Roy&entry.1292438233=Developing%20comprehensive%20assistive%20technologies%20requires%20the%20seamless%20integration%20of%20visual%20and%20auditory%20perception.%20This%20research%20evaluates%20the%20feasibility%20of%20a%20modular%20architecture%20inspired%20by%20core%20functionalities%20of%20perceptive%20systems%20like%20%27Smart%20Eye.%27%20We%20propose%20and%20benchmark%20three%20independent%20sensing%20modules%3A%20a%20Convolutional%20Neural%20Network%20%28CNN%29%20for%20eye%20state%20detection%20%28drowsiness/attention%29%2C%20a%20deep%20CNN%20for%20facial%20expression%20recognition%2C%20and%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20network%20for%20voice-based%20speaker%20identification.%20Utilizing%20the%20Eyes%20Image%2C%20FER2013%2C%20and%20customized%20audio%20datasets%2C%20our%20models%20achieved%20accuracies%20of%2093.0%25%2C%2097.8%25%2C%20and%2096.89%25%2C%20respectively.%20This%20study%20demonstrates%20that%20lightweight%2C%20domain-specific%20models%20can%20achieve%20high%20fidelity%20on%20discrete%20tasks%2C%20establishing%20a%20validated%20foundation%20for%20future%20real-time%2C%20multimodal%20integration%20in%20resource-constrained%20assistive%20devices.&entry.1838667208=http%3A//arxiv.org/abs/2511.20474v1&entry.124074799=Read"},
{"title": "Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment", "author": "Zixue Zeng and Xiaoyan Zhao and Matthew Cartier and Tong Yu and Jing Wang and Xin Meng and Zhiyu Sheng and Maryam Satarpour and John M Cormack and Allison Bean and Ryan Nussbaum and Maya Maurer and Emily Landis-Walkenhorst and Dinesh Kumbhare and Kang Kim and Ajay Wasan and Jiantao Pu", "abstract": "We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.", "link": "http://arxiv.org/abs/2501.17690v4", "date": "2025-11-25", "relevancy": 2.7161, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5585}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5495}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation-Aware%20Generative%20Reinforcement%20Network%20%28GRN%29%20for%20Tissue%20Layer%20Segmentation%20in%203-D%20Ultrasound%20Images%20for%20Chronic%20Low-back%20Pain%20%28cLBP%29%20Assessment&body=Title%3A%20Segmentation-Aware%20Generative%20Reinforcement%20Network%20%28GRN%29%20for%20Tissue%20Layer%20Segmentation%20in%203-D%20Ultrasound%20Images%20for%20Chronic%20Low-back%20Pain%20%28cLBP%29%20Assessment%0AAuthor%3A%20Zixue%20Zeng%20and%20Xiaoyan%20Zhao%20and%20Matthew%20Cartier%20and%20Tong%20Yu%20and%20Jing%20Wang%20and%20Xin%20Meng%20and%20Zhiyu%20Sheng%20and%20Maryam%20Satarpour%20and%20John%20M%20Cormack%20and%20Allison%20Bean%20and%20Ryan%20Nussbaum%20and%20Maya%20Maurer%20and%20Emily%20Landis-Walkenhorst%20and%20Dinesh%20Kumbhare%20and%20Kang%20Kim%20and%20Ajay%20Wasan%20and%20Jiantao%20Pu%0AAbstract%3A%20We%20introduce%20a%20novel%20segmentation-aware%20joint%20training%20framework%20called%20generative%20reinforcement%20network%20%28GRN%29%20that%20integrates%20segmentation%20loss%20feedback%20to%20optimize%20both%20image%20generation%20and%20segmentation%20performance%20in%20a%20single%20stage.%20An%20image%20enhancement%20technique%20called%20segmentation-guided%20enhancement%20%28SGE%29%20is%20also%20developed%2C%20where%20the%20generator%20produces%20images%20tailored%20specifically%20for%20the%20segmentation%20model.%20Two%20variants%20of%20GRN%20were%20also%20developed%2C%20including%20GRN%20for%20sample-efficient%20learning%20%28GRN-SEL%29%20and%20GRN%20for%20semi-supervised%20learning%20%28GRN-SSL%29.%20GRN%27s%20performance%20was%20evaluated%20using%20a%20dataset%20of%2069%20fully%20annotated%203D%20ultrasound%20scans%20from%2029%20subjects.%20The%20annotations%20included%20six%20anatomical%20structures%3A%20dermis%2C%20superficial%20fat%2C%20superficial%20fascial%20membrane%20%28SFM%29%2C%20deep%20fat%2C%20deep%20fascial%20membrane%20%28DFM%29%2C%20and%20muscle.%20Our%20results%20show%20that%20GRN-SEL%20with%20SGE%20reduces%20labeling%20efforts%20by%20up%20to%2070%25%20while%20achieving%20a%201.98%25%20improvement%20in%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29%20compared%20to%20models%20trained%20on%20fully%20labeled%20datasets.%20GRN-SEL%20alone%20reduces%20labeling%20efforts%20by%2060%25%2C%20GRN-SSL%20with%20SGE%20decreases%20labeling%20requirements%20by%2070%25%2C%20and%20GRN-SSL%20alone%20by%2060%25%2C%20all%20while%20maintaining%20performance%20comparable%20to%20fully%20supervised%20models.%20These%20findings%20suggest%20the%20effectiveness%20of%20the%20GRN%20framework%20in%20optimizing%20segmentation%20performance%20with%20significantly%20less%20labeled%20data%2C%20offering%20a%20scalable%20and%20efficient%20solution%20for%20ultrasound%20image%20analysis%20and%20reducing%20the%20burdens%20associated%20with%20data%20annotation.%0ALink%3A%20http%3A//arxiv.org/abs/2501.17690v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation-Aware%2520Generative%2520Reinforcement%2520Network%2520%2528GRN%2529%2520for%2520Tissue%2520Layer%2520Segmentation%2520in%25203-D%2520Ultrasound%2520Images%2520for%2520Chronic%2520Low-back%2520Pain%2520%2528cLBP%2529%2520Assessment%26entry.906535625%3DZixue%2520Zeng%2520and%2520Xiaoyan%2520Zhao%2520and%2520Matthew%2520Cartier%2520and%2520Tong%2520Yu%2520and%2520Jing%2520Wang%2520and%2520Xin%2520Meng%2520and%2520Zhiyu%2520Sheng%2520and%2520Maryam%2520Satarpour%2520and%2520John%2520M%2520Cormack%2520and%2520Allison%2520Bean%2520and%2520Ryan%2520Nussbaum%2520and%2520Maya%2520Maurer%2520and%2520Emily%2520Landis-Walkenhorst%2520and%2520Dinesh%2520Kumbhare%2520and%2520Kang%2520Kim%2520and%2520Ajay%2520Wasan%2520and%2520Jiantao%2520Pu%26entry.1292438233%3DWe%2520introduce%2520a%2520novel%2520segmentation-aware%2520joint%2520training%2520framework%2520called%2520generative%2520reinforcement%2520network%2520%2528GRN%2529%2520that%2520integrates%2520segmentation%2520loss%2520feedback%2520to%2520optimize%2520both%2520image%2520generation%2520and%2520segmentation%2520performance%2520in%2520a%2520single%2520stage.%2520An%2520image%2520enhancement%2520technique%2520called%2520segmentation-guided%2520enhancement%2520%2528SGE%2529%2520is%2520also%2520developed%252C%2520where%2520the%2520generator%2520produces%2520images%2520tailored%2520specifically%2520for%2520the%2520segmentation%2520model.%2520Two%2520variants%2520of%2520GRN%2520were%2520also%2520developed%252C%2520including%2520GRN%2520for%2520sample-efficient%2520learning%2520%2528GRN-SEL%2529%2520and%2520GRN%2520for%2520semi-supervised%2520learning%2520%2528GRN-SSL%2529.%2520GRN%2527s%2520performance%2520was%2520evaluated%2520using%2520a%2520dataset%2520of%252069%2520fully%2520annotated%25203D%2520ultrasound%2520scans%2520from%252029%2520subjects.%2520The%2520annotations%2520included%2520six%2520anatomical%2520structures%253A%2520dermis%252C%2520superficial%2520fat%252C%2520superficial%2520fascial%2520membrane%2520%2528SFM%2529%252C%2520deep%2520fat%252C%2520deep%2520fascial%2520membrane%2520%2528DFM%2529%252C%2520and%2520muscle.%2520Our%2520results%2520show%2520that%2520GRN-SEL%2520with%2520SGE%2520reduces%2520labeling%2520efforts%2520by%2520up%2520to%252070%2525%2520while%2520achieving%2520a%25201.98%2525%2520improvement%2520in%2520the%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520compared%2520to%2520models%2520trained%2520on%2520fully%2520labeled%2520datasets.%2520GRN-SEL%2520alone%2520reduces%2520labeling%2520efforts%2520by%252060%2525%252C%2520GRN-SSL%2520with%2520SGE%2520decreases%2520labeling%2520requirements%2520by%252070%2525%252C%2520and%2520GRN-SSL%2520alone%2520by%252060%2525%252C%2520all%2520while%2520maintaining%2520performance%2520comparable%2520to%2520fully%2520supervised%2520models.%2520These%2520findings%2520suggest%2520the%2520effectiveness%2520of%2520the%2520GRN%2520framework%2520in%2520optimizing%2520segmentation%2520performance%2520with%2520significantly%2520less%2520labeled%2520data%252C%2520offering%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%2520ultrasound%2520image%2520analysis%2520and%2520reducing%2520the%2520burdens%2520associated%2520with%2520data%2520annotation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17690v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation-Aware%20Generative%20Reinforcement%20Network%20%28GRN%29%20for%20Tissue%20Layer%20Segmentation%20in%203-D%20Ultrasound%20Images%20for%20Chronic%20Low-back%20Pain%20%28cLBP%29%20Assessment&entry.906535625=Zixue%20Zeng%20and%20Xiaoyan%20Zhao%20and%20Matthew%20Cartier%20and%20Tong%20Yu%20and%20Jing%20Wang%20and%20Xin%20Meng%20and%20Zhiyu%20Sheng%20and%20Maryam%20Satarpour%20and%20John%20M%20Cormack%20and%20Allison%20Bean%20and%20Ryan%20Nussbaum%20and%20Maya%20Maurer%20and%20Emily%20Landis-Walkenhorst%20and%20Dinesh%20Kumbhare%20and%20Kang%20Kim%20and%20Ajay%20Wasan%20and%20Jiantao%20Pu&entry.1292438233=We%20introduce%20a%20novel%20segmentation-aware%20joint%20training%20framework%20called%20generative%20reinforcement%20network%20%28GRN%29%20that%20integrates%20segmentation%20loss%20feedback%20to%20optimize%20both%20image%20generation%20and%20segmentation%20performance%20in%20a%20single%20stage.%20An%20image%20enhancement%20technique%20called%20segmentation-guided%20enhancement%20%28SGE%29%20is%20also%20developed%2C%20where%20the%20generator%20produces%20images%20tailored%20specifically%20for%20the%20segmentation%20model.%20Two%20variants%20of%20GRN%20were%20also%20developed%2C%20including%20GRN%20for%20sample-efficient%20learning%20%28GRN-SEL%29%20and%20GRN%20for%20semi-supervised%20learning%20%28GRN-SSL%29.%20GRN%27s%20performance%20was%20evaluated%20using%20a%20dataset%20of%2069%20fully%20annotated%203D%20ultrasound%20scans%20from%2029%20subjects.%20The%20annotations%20included%20six%20anatomical%20structures%3A%20dermis%2C%20superficial%20fat%2C%20superficial%20fascial%20membrane%20%28SFM%29%2C%20deep%20fat%2C%20deep%20fascial%20membrane%20%28DFM%29%2C%20and%20muscle.%20Our%20results%20show%20that%20GRN-SEL%20with%20SGE%20reduces%20labeling%20efforts%20by%20up%20to%2070%25%20while%20achieving%20a%201.98%25%20improvement%20in%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29%20compared%20to%20models%20trained%20on%20fully%20labeled%20datasets.%20GRN-SEL%20alone%20reduces%20labeling%20efforts%20by%2060%25%2C%20GRN-SSL%20with%20SGE%20decreases%20labeling%20requirements%20by%2070%25%2C%20and%20GRN-SSL%20alone%20by%2060%25%2C%20all%20while%20maintaining%20performance%20comparable%20to%20fully%20supervised%20models.%20These%20findings%20suggest%20the%20effectiveness%20of%20the%20GRN%20framework%20in%20optimizing%20segmentation%20performance%20with%20significantly%20less%20labeled%20data%2C%20offering%20a%20scalable%20and%20efficient%20solution%20for%20ultrasound%20image%20analysis%20and%20reducing%20the%20burdens%20associated%20with%20data%20annotation.&entry.1838667208=http%3A//arxiv.org/abs/2501.17690v4&entry.124074799=Read"},
{"title": "VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning", "author": "Bo Pang and Chenxi Xu and Jierui Ren and Guoping Wang and Sheng Li", "abstract": "Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.", "link": "http://arxiv.org/abs/2511.20422v1", "date": "2025-11-25", "relevancy": 2.7158, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VibraVerse%3A%20A%20Large-Scale%20Geometry-Acoustics%20Alignment%20Dataset%20for%20Physically-Consistent%20Multimodal%20Learning&body=Title%3A%20VibraVerse%3A%20A%20Large-Scale%20Geometry-Acoustics%20Alignment%20Dataset%20for%20Physically-Consistent%20Multimodal%20Learning%0AAuthor%3A%20Bo%20Pang%20and%20Chenxi%20Xu%20and%20Jierui%20Ren%20and%20Guoping%20Wang%20and%20Sheng%20Li%0AAbstract%3A%20Understanding%20the%20physical%20world%20requires%20perceptual%20models%20grounded%20in%20physical%20laws%20rather%20than%20mere%20statistical%20correlations.%20However%2C%20existing%20multimodal%20learning%20frameworks%2C%20focused%20on%20vision%20and%20language%2C%20lack%20physical%20consistency%20and%20overlook%20the%20intrinsic%20causal%20relationships%20among%20an%20object%27s%20geometry%2C%20material%2C%20vibration%20modes%2C%20and%20the%20sounds%20it%20produces.%20We%20introduce%20VibraVerse%2C%20a%20large-scale%20geometry-acoustics%20alignment%20dataset%20that%20explicitly%20bridges%20the%20causal%20chain%20from%203D%20geometry%20-%3E%20physical%20attributes%20-%3E%20modal%20parameters%20-%3E%20acoustic%20signals.%20Each%203D%20model%20has%20explicit%20physical%20properties%20%28density%2C%20Young%27s%20modulus%2C%20Poisson%27s%20ratio%29%20and%20volumetric%20geometry%2C%20from%20which%20modal%20eigenfrequencies%20and%20eigenvectors%20are%20computed%20for%20impact%20sound%20synthesis%20under%20controlled%20excitations.%20To%20establish%20this%20coherence%2C%20we%20introduce%20CLASP%2C%20a%20contrastive%20learning%20framework%20for%20cross-modal%20alignment%20that%20preserves%20the%20causal%20correspondence%20between%20an%20object%27s%20physical%20structure%20and%20its%20acoustic%20response.%20This%20framework%20enforces%20physically%20consistent%20alignment%20across%20modalities%2C%20ensuring%20that%20every%20sample%20is%20coherent%2C%20traceable%20to%20the%20governing%20equations%2C%20and%20embedded%20within%20a%20unified%20representation%20space%20spanning%20shape%2C%20image%2C%20and%20sound.%20Built%20upon%20VibraVerse%2C%20we%20define%20a%20suite%20of%20benchmark%20tasks%20for%20geometry-to-sound%20prediction%2C%20sound-guided%20shape%20reconstruction%2C%20and%20cross-modal%20representation%20learning.%20Extensive%20validations%20on%20these%20tasks%20demonstrate%20that%20models%20trained%20on%20VibraVerse%20exhibit%20superior%20accuracy%2C%20interpretability%2C%20and%20generalization%20across%20modalities.%20These%20results%20establish%20VibraVerse%20as%20a%20benchmark%20for%20physically%20consistent%20and%20causally%20interpretable%20multimodal%20learning%2C%20providing%20a%20foundation%20for%20sound-guided%20embodied%20perception%20and%20a%20deeper%20understanding%20of%20the%20physical%20world.%20The%20dataset%20will%20be%20open-sourced.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibraVerse%253A%2520A%2520Large-Scale%2520Geometry-Acoustics%2520Alignment%2520Dataset%2520for%2520Physically-Consistent%2520Multimodal%2520Learning%26entry.906535625%3DBo%2520Pang%2520and%2520Chenxi%2520Xu%2520and%2520Jierui%2520Ren%2520and%2520Guoping%2520Wang%2520and%2520Sheng%2520Li%26entry.1292438233%3DUnderstanding%2520the%2520physical%2520world%2520requires%2520perceptual%2520models%2520grounded%2520in%2520physical%2520laws%2520rather%2520than%2520mere%2520statistical%2520correlations.%2520However%252C%2520existing%2520multimodal%2520learning%2520frameworks%252C%2520focused%2520on%2520vision%2520and%2520language%252C%2520lack%2520physical%2520consistency%2520and%2520overlook%2520the%2520intrinsic%2520causal%2520relationships%2520among%2520an%2520object%2527s%2520geometry%252C%2520material%252C%2520vibration%2520modes%252C%2520and%2520the%2520sounds%2520it%2520produces.%2520We%2520introduce%2520VibraVerse%252C%2520a%2520large-scale%2520geometry-acoustics%2520alignment%2520dataset%2520that%2520explicitly%2520bridges%2520the%2520causal%2520chain%2520from%25203D%2520geometry%2520-%253E%2520physical%2520attributes%2520-%253E%2520modal%2520parameters%2520-%253E%2520acoustic%2520signals.%2520Each%25203D%2520model%2520has%2520explicit%2520physical%2520properties%2520%2528density%252C%2520Young%2527s%2520modulus%252C%2520Poisson%2527s%2520ratio%2529%2520and%2520volumetric%2520geometry%252C%2520from%2520which%2520modal%2520eigenfrequencies%2520and%2520eigenvectors%2520are%2520computed%2520for%2520impact%2520sound%2520synthesis%2520under%2520controlled%2520excitations.%2520To%2520establish%2520this%2520coherence%252C%2520we%2520introduce%2520CLASP%252C%2520a%2520contrastive%2520learning%2520framework%2520for%2520cross-modal%2520alignment%2520that%2520preserves%2520the%2520causal%2520correspondence%2520between%2520an%2520object%2527s%2520physical%2520structure%2520and%2520its%2520acoustic%2520response.%2520This%2520framework%2520enforces%2520physically%2520consistent%2520alignment%2520across%2520modalities%252C%2520ensuring%2520that%2520every%2520sample%2520is%2520coherent%252C%2520traceable%2520to%2520the%2520governing%2520equations%252C%2520and%2520embedded%2520within%2520a%2520unified%2520representation%2520space%2520spanning%2520shape%252C%2520image%252C%2520and%2520sound.%2520Built%2520upon%2520VibraVerse%252C%2520we%2520define%2520a%2520suite%2520of%2520benchmark%2520tasks%2520for%2520geometry-to-sound%2520prediction%252C%2520sound-guided%2520shape%2520reconstruction%252C%2520and%2520cross-modal%2520representation%2520learning.%2520Extensive%2520validations%2520on%2520these%2520tasks%2520demonstrate%2520that%2520models%2520trained%2520on%2520VibraVerse%2520exhibit%2520superior%2520accuracy%252C%2520interpretability%252C%2520and%2520generalization%2520across%2520modalities.%2520These%2520results%2520establish%2520VibraVerse%2520as%2520a%2520benchmark%2520for%2520physically%2520consistent%2520and%2520causally%2520interpretable%2520multimodal%2520learning%252C%2520providing%2520a%2520foundation%2520for%2520sound-guided%2520embodied%2520perception%2520and%2520a%2520deeper%2520understanding%2520of%2520the%2520physical%2520world.%2520The%2520dataset%2520will%2520be%2520open-sourced.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VibraVerse%3A%20A%20Large-Scale%20Geometry-Acoustics%20Alignment%20Dataset%20for%20Physically-Consistent%20Multimodal%20Learning&entry.906535625=Bo%20Pang%20and%20Chenxi%20Xu%20and%20Jierui%20Ren%20and%20Guoping%20Wang%20and%20Sheng%20Li&entry.1292438233=Understanding%20the%20physical%20world%20requires%20perceptual%20models%20grounded%20in%20physical%20laws%20rather%20than%20mere%20statistical%20correlations.%20However%2C%20existing%20multimodal%20learning%20frameworks%2C%20focused%20on%20vision%20and%20language%2C%20lack%20physical%20consistency%20and%20overlook%20the%20intrinsic%20causal%20relationships%20among%20an%20object%27s%20geometry%2C%20material%2C%20vibration%20modes%2C%20and%20the%20sounds%20it%20produces.%20We%20introduce%20VibraVerse%2C%20a%20large-scale%20geometry-acoustics%20alignment%20dataset%20that%20explicitly%20bridges%20the%20causal%20chain%20from%203D%20geometry%20-%3E%20physical%20attributes%20-%3E%20modal%20parameters%20-%3E%20acoustic%20signals.%20Each%203D%20model%20has%20explicit%20physical%20properties%20%28density%2C%20Young%27s%20modulus%2C%20Poisson%27s%20ratio%29%20and%20volumetric%20geometry%2C%20from%20which%20modal%20eigenfrequencies%20and%20eigenvectors%20are%20computed%20for%20impact%20sound%20synthesis%20under%20controlled%20excitations.%20To%20establish%20this%20coherence%2C%20we%20introduce%20CLASP%2C%20a%20contrastive%20learning%20framework%20for%20cross-modal%20alignment%20that%20preserves%20the%20causal%20correspondence%20between%20an%20object%27s%20physical%20structure%20and%20its%20acoustic%20response.%20This%20framework%20enforces%20physically%20consistent%20alignment%20across%20modalities%2C%20ensuring%20that%20every%20sample%20is%20coherent%2C%20traceable%20to%20the%20governing%20equations%2C%20and%20embedded%20within%20a%20unified%20representation%20space%20spanning%20shape%2C%20image%2C%20and%20sound.%20Built%20upon%20VibraVerse%2C%20we%20define%20a%20suite%20of%20benchmark%20tasks%20for%20geometry-to-sound%20prediction%2C%20sound-guided%20shape%20reconstruction%2C%20and%20cross-modal%20representation%20learning.%20Extensive%20validations%20on%20these%20tasks%20demonstrate%20that%20models%20trained%20on%20VibraVerse%20exhibit%20superior%20accuracy%2C%20interpretability%2C%20and%20generalization%20across%20modalities.%20These%20results%20establish%20VibraVerse%20as%20a%20benchmark%20for%20physically%20consistent%20and%20causally%20interpretable%20multimodal%20learning%2C%20providing%20a%20foundation%20for%20sound-guided%20embodied%20perception%20and%20a%20deeper%20understanding%20of%20the%20physical%20world.%20The%20dataset%20will%20be%20open-sourced.&entry.1838667208=http%3A//arxiv.org/abs/2511.20422v1&entry.124074799=Read"},
{"title": "Dynamic-ICP: Doppler-Aware Iterative Closest Point Registration for Dynamic Scenes", "author": "Dong Wang and Daniel Casado Herraez and Stefan May and Andreas N\u00fcchter", "abstract": "Reliable odometry in highly dynamic environments remains challenging when it relies on ICP-based registration: ICP assumes near-static scenes and degrades in repetitive or low-texture geometry. We introduce Dynamic-ICP, a Doppler-aware registration framework. The method (i) estimates ego motion from per-point Doppler velocity via robust regression and builds a velocity filter, (ii) clusters dynamic objects and reconstructs object-wise translational velocities from ego-compensated radial measurements, (iii) predicts dynamic points with a constant-velocity model, and (iv) aligns scans using a compact objective that combines point-to-plane geometry residual with a translation-invariant, rotation-only Doppler residual. The approach requires no external sensors or sensor-vehicle calibration and operates directly on FMCW LiDAR range and Doppler velocities. We evaluate Dynamic-ICP on three datasets-HeRCULES, HeLiPR, AevaScenes-focusing on highly dynamic scenes. Dynamic-ICP consistently improves rotational stability and translation accuracy over the state-of-the-art methods. Our approach is also simple to integrate into existing pipelines, runs in real time, and provides a lightweight solution for robust registration in dynamic environments. To encourage further research, the code is available at: https://github.com/JMUWRobotics/Dynamic-ICP.", "link": "http://arxiv.org/abs/2511.20292v1", "date": "2025-11-25", "relevancy": 2.71, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5462}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5432}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic-ICP%3A%20Doppler-Aware%20Iterative%20Closest%20Point%20Registration%20for%20Dynamic%20Scenes&body=Title%3A%20Dynamic-ICP%3A%20Doppler-Aware%20Iterative%20Closest%20Point%20Registration%20for%20Dynamic%20Scenes%0AAuthor%3A%20Dong%20Wang%20and%20Daniel%20Casado%20Herraez%20and%20Stefan%20May%20and%20Andreas%20N%C3%BCchter%0AAbstract%3A%20Reliable%20odometry%20in%20highly%20dynamic%20environments%20remains%20challenging%20when%20it%20relies%20on%20ICP-based%20registration%3A%20ICP%20assumes%20near-static%20scenes%20and%20degrades%20in%20repetitive%20or%20low-texture%20geometry.%20We%20introduce%20Dynamic-ICP%2C%20a%20Doppler-aware%20registration%20framework.%20The%20method%20%28i%29%20estimates%20ego%20motion%20from%20per-point%20Doppler%20velocity%20via%20robust%20regression%20and%20builds%20a%20velocity%20filter%2C%20%28ii%29%20clusters%20dynamic%20objects%20and%20reconstructs%20object-wise%20translational%20velocities%20from%20ego-compensated%20radial%20measurements%2C%20%28iii%29%20predicts%20dynamic%20points%20with%20a%20constant-velocity%20model%2C%20and%20%28iv%29%20aligns%20scans%20using%20a%20compact%20objective%20that%20combines%20point-to-plane%20geometry%20residual%20with%20a%20translation-invariant%2C%20rotation-only%20Doppler%20residual.%20The%20approach%20requires%20no%20external%20sensors%20or%20sensor-vehicle%20calibration%20and%20operates%20directly%20on%20FMCW%20LiDAR%20range%20and%20Doppler%20velocities.%20We%20evaluate%20Dynamic-ICP%20on%20three%20datasets-HeRCULES%2C%20HeLiPR%2C%20AevaScenes-focusing%20on%20highly%20dynamic%20scenes.%20Dynamic-ICP%20consistently%20improves%20rotational%20stability%20and%20translation%20accuracy%20over%20the%20state-of-the-art%20methods.%20Our%20approach%20is%20also%20simple%20to%20integrate%20into%20existing%20pipelines%2C%20runs%20in%20real%20time%2C%20and%20provides%20a%20lightweight%20solution%20for%20robust%20registration%20in%20dynamic%20environments.%20To%20encourage%20further%20research%2C%20the%20code%20is%20available%20at%3A%20https%3A//github.com/JMUWRobotics/Dynamic-ICP.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic-ICP%253A%2520Doppler-Aware%2520Iterative%2520Closest%2520Point%2520Registration%2520for%2520Dynamic%2520Scenes%26entry.906535625%3DDong%2520Wang%2520and%2520Daniel%2520Casado%2520Herraez%2520and%2520Stefan%2520May%2520and%2520Andreas%2520N%25C3%25BCchter%26entry.1292438233%3DReliable%2520odometry%2520in%2520highly%2520dynamic%2520environments%2520remains%2520challenging%2520when%2520it%2520relies%2520on%2520ICP-based%2520registration%253A%2520ICP%2520assumes%2520near-static%2520scenes%2520and%2520degrades%2520in%2520repetitive%2520or%2520low-texture%2520geometry.%2520We%2520introduce%2520Dynamic-ICP%252C%2520a%2520Doppler-aware%2520registration%2520framework.%2520The%2520method%2520%2528i%2529%2520estimates%2520ego%2520motion%2520from%2520per-point%2520Doppler%2520velocity%2520via%2520robust%2520regression%2520and%2520builds%2520a%2520velocity%2520filter%252C%2520%2528ii%2529%2520clusters%2520dynamic%2520objects%2520and%2520reconstructs%2520object-wise%2520translational%2520velocities%2520from%2520ego-compensated%2520radial%2520measurements%252C%2520%2528iii%2529%2520predicts%2520dynamic%2520points%2520with%2520a%2520constant-velocity%2520model%252C%2520and%2520%2528iv%2529%2520aligns%2520scans%2520using%2520a%2520compact%2520objective%2520that%2520combines%2520point-to-plane%2520geometry%2520residual%2520with%2520a%2520translation-invariant%252C%2520rotation-only%2520Doppler%2520residual.%2520The%2520approach%2520requires%2520no%2520external%2520sensors%2520or%2520sensor-vehicle%2520calibration%2520and%2520operates%2520directly%2520on%2520FMCW%2520LiDAR%2520range%2520and%2520Doppler%2520velocities.%2520We%2520evaluate%2520Dynamic-ICP%2520on%2520three%2520datasets-HeRCULES%252C%2520HeLiPR%252C%2520AevaScenes-focusing%2520on%2520highly%2520dynamic%2520scenes.%2520Dynamic-ICP%2520consistently%2520improves%2520rotational%2520stability%2520and%2520translation%2520accuracy%2520over%2520the%2520state-of-the-art%2520methods.%2520Our%2520approach%2520is%2520also%2520simple%2520to%2520integrate%2520into%2520existing%2520pipelines%252C%2520runs%2520in%2520real%2520time%252C%2520and%2520provides%2520a%2520lightweight%2520solution%2520for%2520robust%2520registration%2520in%2520dynamic%2520environments.%2520To%2520encourage%2520further%2520research%252C%2520the%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/JMUWRobotics/Dynamic-ICP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic-ICP%3A%20Doppler-Aware%20Iterative%20Closest%20Point%20Registration%20for%20Dynamic%20Scenes&entry.906535625=Dong%20Wang%20and%20Daniel%20Casado%20Herraez%20and%20Stefan%20May%20and%20Andreas%20N%C3%BCchter&entry.1292438233=Reliable%20odometry%20in%20highly%20dynamic%20environments%20remains%20challenging%20when%20it%20relies%20on%20ICP-based%20registration%3A%20ICP%20assumes%20near-static%20scenes%20and%20degrades%20in%20repetitive%20or%20low-texture%20geometry.%20We%20introduce%20Dynamic-ICP%2C%20a%20Doppler-aware%20registration%20framework.%20The%20method%20%28i%29%20estimates%20ego%20motion%20from%20per-point%20Doppler%20velocity%20via%20robust%20regression%20and%20builds%20a%20velocity%20filter%2C%20%28ii%29%20clusters%20dynamic%20objects%20and%20reconstructs%20object-wise%20translational%20velocities%20from%20ego-compensated%20radial%20measurements%2C%20%28iii%29%20predicts%20dynamic%20points%20with%20a%20constant-velocity%20model%2C%20and%20%28iv%29%20aligns%20scans%20using%20a%20compact%20objective%20that%20combines%20point-to-plane%20geometry%20residual%20with%20a%20translation-invariant%2C%20rotation-only%20Doppler%20residual.%20The%20approach%20requires%20no%20external%20sensors%20or%20sensor-vehicle%20calibration%20and%20operates%20directly%20on%20FMCW%20LiDAR%20range%20and%20Doppler%20velocities.%20We%20evaluate%20Dynamic-ICP%20on%20three%20datasets-HeRCULES%2C%20HeLiPR%2C%20AevaScenes-focusing%20on%20highly%20dynamic%20scenes.%20Dynamic-ICP%20consistently%20improves%20rotational%20stability%20and%20translation%20accuracy%20over%20the%20state-of-the-art%20methods.%20Our%20approach%20is%20also%20simple%20to%20integrate%20into%20existing%20pipelines%2C%20runs%20in%20real%20time%2C%20and%20provides%20a%20lightweight%20solution%20for%20robust%20registration%20in%20dynamic%20environments.%20To%20encourage%20further%20research%2C%20the%20code%20is%20available%20at%3A%20https%3A//github.com/JMUWRobotics/Dynamic-ICP.&entry.1838667208=http%3A//arxiv.org/abs/2511.20292v1&entry.124074799=Read"},
{"title": "AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations", "author": "Litian Gong and Fatemeh Bahrani and Yutai Zhou and Amin Banayeeanzade and Jiachen Li and Erdem B\u0131y\u0131k", "abstract": "AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.", "link": "http://arxiv.org/abs/2511.18617v2", "date": "2025-11-25", "relevancy": 2.7083, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5621}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5362}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoFocus-IL%3A%20VLM-based%20Saliency%20Maps%20for%20Data-Efficient%20Visual%20Imitation%20Learning%20without%20Extra%20Human%20Annotations&body=Title%3A%20AutoFocus-IL%3A%20VLM-based%20Saliency%20Maps%20for%20Data-Efficient%20Visual%20Imitation%20Learning%20without%20Extra%20Human%20Annotations%0AAuthor%3A%20Litian%20Gong%20and%20Fatemeh%20Bahrani%20and%20Yutai%20Zhou%20and%20Amin%20Banayeeanzade%20and%20Jiachen%20Li%20and%20Erdem%20B%C4%B1y%C4%B1k%0AAbstract%3A%20AutoFocus-IL%20is%20a%20simple%20yet%20effective%20method%20to%20improve%20data%20efficiency%20and%20generalization%20in%20visual%20imitation%20learning%20by%20guiding%20policies%20to%20attend%20to%20task-relevant%20features%20rather%20than%20distractors%20and%20spurious%20correlations.%20Although%20saliency%20regularization%20has%20emerged%20as%20a%20promising%20way%20to%20achieve%20this%2C%20existing%20approaches%20typically%20require%20costly%20supervision%20such%20as%20human%20gaze%20data%20or%20manual%20saliency%20annotations.%20In%20contrast%2C%20AutoFocus-IL%20leverages%20vision-language%20models%20%28VLMs%29%20to%20automatically%20identify%20and%20track%20key%20objects%20in%20demonstrations%2C%20generating%20temporal%20saliency%20maps%20that%20highlight%20causal%20visual%20signals%20while%20suppressing%20distractors.%20These%20maps%20are%20then%20used%20to%20regularize%20behavior%20cloning%20policies%2C%20yielding%20stronger%20alignment%20between%20visual%20attention%20and%20task-relevant%20cues.%20Experiments%20in%20both%20the%20CARLA%20simulator%20and%20real-robot%20manipulation%20tasks%20demonstrate%20that%20AutoFocus-IL%20not%20only%20outperforms%20standard%20behavior%20cloning%20but%20also%20surpasses%20state-of-the-art%20baselines%20that%20assume%20privileged%20access%20to%20human%20supervision%2C%20such%20as%20gaze%20data.%20Code%2C%20datasets%2C%20and%20trained%20policy%20videos%20are%20available%20at%20https%3A//AutoFocus-IL.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18617v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoFocus-IL%253A%2520VLM-based%2520Saliency%2520Maps%2520for%2520Data-Efficient%2520Visual%2520Imitation%2520Learning%2520without%2520Extra%2520Human%2520Annotations%26entry.906535625%3DLitian%2520Gong%2520and%2520Fatemeh%2520Bahrani%2520and%2520Yutai%2520Zhou%2520and%2520Amin%2520Banayeeanzade%2520and%2520Jiachen%2520Li%2520and%2520Erdem%2520B%25C4%25B1y%25C4%25B1k%26entry.1292438233%3DAutoFocus-IL%2520is%2520a%2520simple%2520yet%2520effective%2520method%2520to%2520improve%2520data%2520efficiency%2520and%2520generalization%2520in%2520visual%2520imitation%2520learning%2520by%2520guiding%2520policies%2520to%2520attend%2520to%2520task-relevant%2520features%2520rather%2520than%2520distractors%2520and%2520spurious%2520correlations.%2520Although%2520saliency%2520regularization%2520has%2520emerged%2520as%2520a%2520promising%2520way%2520to%2520achieve%2520this%252C%2520existing%2520approaches%2520typically%2520require%2520costly%2520supervision%2520such%2520as%2520human%2520gaze%2520data%2520or%2520manual%2520saliency%2520annotations.%2520In%2520contrast%252C%2520AutoFocus-IL%2520leverages%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520automatically%2520identify%2520and%2520track%2520key%2520objects%2520in%2520demonstrations%252C%2520generating%2520temporal%2520saliency%2520maps%2520that%2520highlight%2520causal%2520visual%2520signals%2520while%2520suppressing%2520distractors.%2520These%2520maps%2520are%2520then%2520used%2520to%2520regularize%2520behavior%2520cloning%2520policies%252C%2520yielding%2520stronger%2520alignment%2520between%2520visual%2520attention%2520and%2520task-relevant%2520cues.%2520Experiments%2520in%2520both%2520the%2520CARLA%2520simulator%2520and%2520real-robot%2520manipulation%2520tasks%2520demonstrate%2520that%2520AutoFocus-IL%2520not%2520only%2520outperforms%2520standard%2520behavior%2520cloning%2520but%2520also%2520surpasses%2520state-of-the-art%2520baselines%2520that%2520assume%2520privileged%2520access%2520to%2520human%2520supervision%252C%2520such%2520as%2520gaze%2520data.%2520Code%252C%2520datasets%252C%2520and%2520trained%2520policy%2520videos%2520are%2520available%2520at%2520https%253A//AutoFocus-IL.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18617v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoFocus-IL%3A%20VLM-based%20Saliency%20Maps%20for%20Data-Efficient%20Visual%20Imitation%20Learning%20without%20Extra%20Human%20Annotations&entry.906535625=Litian%20Gong%20and%20Fatemeh%20Bahrani%20and%20Yutai%20Zhou%20and%20Amin%20Banayeeanzade%20and%20Jiachen%20Li%20and%20Erdem%20B%C4%B1y%C4%B1k&entry.1292438233=AutoFocus-IL%20is%20a%20simple%20yet%20effective%20method%20to%20improve%20data%20efficiency%20and%20generalization%20in%20visual%20imitation%20learning%20by%20guiding%20policies%20to%20attend%20to%20task-relevant%20features%20rather%20than%20distractors%20and%20spurious%20correlations.%20Although%20saliency%20regularization%20has%20emerged%20as%20a%20promising%20way%20to%20achieve%20this%2C%20existing%20approaches%20typically%20require%20costly%20supervision%20such%20as%20human%20gaze%20data%20or%20manual%20saliency%20annotations.%20In%20contrast%2C%20AutoFocus-IL%20leverages%20vision-language%20models%20%28VLMs%29%20to%20automatically%20identify%20and%20track%20key%20objects%20in%20demonstrations%2C%20generating%20temporal%20saliency%20maps%20that%20highlight%20causal%20visual%20signals%20while%20suppressing%20distractors.%20These%20maps%20are%20then%20used%20to%20regularize%20behavior%20cloning%20policies%2C%20yielding%20stronger%20alignment%20between%20visual%20attention%20and%20task-relevant%20cues.%20Experiments%20in%20both%20the%20CARLA%20simulator%20and%20real-robot%20manipulation%20tasks%20demonstrate%20that%20AutoFocus-IL%20not%20only%20outperforms%20standard%20behavior%20cloning%20but%20also%20surpasses%20state-of-the-art%20baselines%20that%20assume%20privileged%20access%20to%20human%20supervision%2C%20such%20as%20gaze%20data.%20Code%2C%20datasets%2C%20and%20trained%20policy%20videos%20are%20available%20at%20https%3A//AutoFocus-IL.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2511.18617v2&entry.124074799=Read"},
{"title": "Vision-Language Models for Automated 3D PET/CT Report Generation", "author": "Wenpei Jiao and Kun Shang and Hui Li and Ke Yan and Jiajin Zhang and Guangjie Yang and Lijuan Guo and Yan Wan and Xing Yang and Dakai Jin and Zhaoheng Xie", "abstract": "Positron emission tomography/computed tomography (PET/CT) is essential in oncology, yet the rapid expansion of scanners has outpaced the availability of trained specialists, making automated PET/CT report generation (PETRG) increasingly important for reducing clinical workload. Compared with structural imaging (e.g., X-ray, CT, and MRI), functional PET poses distinct challenges: metabolic patterns vary with tracer physiology, and whole-body 3D contextual information is required rather than local-region interpretation. To advance PETRG, we propose PETRG-3D, an end-to-end 3D dual-branch framework that separately encodes PET and CT volumes and incorporates style-adaptive prompts to mitigate inter-hospital variability in reporting practices. We construct PETRG-Lym, a multi-center lymphoma dataset collected from four hospitals (824 reports w/ 245,509 paired PET/CT slices), and construct AutoPET-RG-Lym, a publicly accessible PETRG benchmark derived from open imaging data but equipped with new expert-written, clinically validated reports (135 cases). To assess clinical utility, we introduce PETRG-Score, a lymphoma-specific evaluation protocol that jointly measures metabolic and structural findings across curated anatomical regions. Experiments show that PETRG-3D substantially outperforms existing methods on both natural language metrics (e.g., +31.49\\% ROUGE-L) and clinical efficacy metrics (e.g., +8.18\\% PET-All), highlighting the benefits of volumetric dual-modality modeling and style-aware prompting. Overall, this work establishes a foundation for future PET/CT-specific models emphasizing disease-aware reasoning and clinically reliable evaluation. Codes, models, and AutoPET-RG-Lym will be released.", "link": "http://arxiv.org/abs/2511.20145v1", "date": "2025-11-25", "relevancy": 2.7013, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5488}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Models%20for%20Automated%203D%20PET/CT%20Report%20Generation&body=Title%3A%20Vision-Language%20Models%20for%20Automated%203D%20PET/CT%20Report%20Generation%0AAuthor%3A%20Wenpei%20Jiao%20and%20Kun%20Shang%20and%20Hui%20Li%20and%20Ke%20Yan%20and%20Jiajin%20Zhang%20and%20Guangjie%20Yang%20and%20Lijuan%20Guo%20and%20Yan%20Wan%20and%20Xing%20Yang%20and%20Dakai%20Jin%20and%20Zhaoheng%20Xie%0AAbstract%3A%20Positron%20emission%20tomography/computed%20tomography%20%28PET/CT%29%20is%20essential%20in%20oncology%2C%20yet%20the%20rapid%20expansion%20of%20scanners%20has%20outpaced%20the%20availability%20of%20trained%20specialists%2C%20making%20automated%20PET/CT%20report%20generation%20%28PETRG%29%20increasingly%20important%20for%20reducing%20clinical%20workload.%20Compared%20with%20structural%20imaging%20%28e.g.%2C%20X-ray%2C%20CT%2C%20and%20MRI%29%2C%20functional%20PET%20poses%20distinct%20challenges%3A%20metabolic%20patterns%20vary%20with%20tracer%20physiology%2C%20and%20whole-body%203D%20contextual%20information%20is%20required%20rather%20than%20local-region%20interpretation.%20To%20advance%20PETRG%2C%20we%20propose%20PETRG-3D%2C%20an%20end-to-end%203D%20dual-branch%20framework%20that%20separately%20encodes%20PET%20and%20CT%20volumes%20and%20incorporates%20style-adaptive%20prompts%20to%20mitigate%20inter-hospital%20variability%20in%20reporting%20practices.%20We%20construct%20PETRG-Lym%2C%20a%20multi-center%20lymphoma%20dataset%20collected%20from%20four%20hospitals%20%28824%20reports%20w/%20245%2C509%20paired%20PET/CT%20slices%29%2C%20and%20construct%20AutoPET-RG-Lym%2C%20a%20publicly%20accessible%20PETRG%20benchmark%20derived%20from%20open%20imaging%20data%20but%20equipped%20with%20new%20expert-written%2C%20clinically%20validated%20reports%20%28135%20cases%29.%20To%20assess%20clinical%20utility%2C%20we%20introduce%20PETRG-Score%2C%20a%20lymphoma-specific%20evaluation%20protocol%20that%20jointly%20measures%20metabolic%20and%20structural%20findings%20across%20curated%20anatomical%20regions.%20Experiments%20show%20that%20PETRG-3D%20substantially%20outperforms%20existing%20methods%20on%20both%20natural%20language%20metrics%20%28e.g.%2C%20%2B31.49%5C%25%20ROUGE-L%29%20and%20clinical%20efficacy%20metrics%20%28e.g.%2C%20%2B8.18%5C%25%20PET-All%29%2C%20highlighting%20the%20benefits%20of%20volumetric%20dual-modality%20modeling%20and%20style-aware%20prompting.%20Overall%2C%20this%20work%20establishes%20a%20foundation%20for%20future%20PET/CT-specific%20models%20emphasizing%20disease-aware%20reasoning%20and%20clinically%20reliable%20evaluation.%20Codes%2C%20models%2C%20and%20AutoPET-RG-Lym%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Models%2520for%2520Automated%25203D%2520PET/CT%2520Report%2520Generation%26entry.906535625%3DWenpei%2520Jiao%2520and%2520Kun%2520Shang%2520and%2520Hui%2520Li%2520and%2520Ke%2520Yan%2520and%2520Jiajin%2520Zhang%2520and%2520Guangjie%2520Yang%2520and%2520Lijuan%2520Guo%2520and%2520Yan%2520Wan%2520and%2520Xing%2520Yang%2520and%2520Dakai%2520Jin%2520and%2520Zhaoheng%2520Xie%26entry.1292438233%3DPositron%2520emission%2520tomography/computed%2520tomography%2520%2528PET/CT%2529%2520is%2520essential%2520in%2520oncology%252C%2520yet%2520the%2520rapid%2520expansion%2520of%2520scanners%2520has%2520outpaced%2520the%2520availability%2520of%2520trained%2520specialists%252C%2520making%2520automated%2520PET/CT%2520report%2520generation%2520%2528PETRG%2529%2520increasingly%2520important%2520for%2520reducing%2520clinical%2520workload.%2520Compared%2520with%2520structural%2520imaging%2520%2528e.g.%252C%2520X-ray%252C%2520CT%252C%2520and%2520MRI%2529%252C%2520functional%2520PET%2520poses%2520distinct%2520challenges%253A%2520metabolic%2520patterns%2520vary%2520with%2520tracer%2520physiology%252C%2520and%2520whole-body%25203D%2520contextual%2520information%2520is%2520required%2520rather%2520than%2520local-region%2520interpretation.%2520To%2520advance%2520PETRG%252C%2520we%2520propose%2520PETRG-3D%252C%2520an%2520end-to-end%25203D%2520dual-branch%2520framework%2520that%2520separately%2520encodes%2520PET%2520and%2520CT%2520volumes%2520and%2520incorporates%2520style-adaptive%2520prompts%2520to%2520mitigate%2520inter-hospital%2520variability%2520in%2520reporting%2520practices.%2520We%2520construct%2520PETRG-Lym%252C%2520a%2520multi-center%2520lymphoma%2520dataset%2520collected%2520from%2520four%2520hospitals%2520%2528824%2520reports%2520w/%2520245%252C509%2520paired%2520PET/CT%2520slices%2529%252C%2520and%2520construct%2520AutoPET-RG-Lym%252C%2520a%2520publicly%2520accessible%2520PETRG%2520benchmark%2520derived%2520from%2520open%2520imaging%2520data%2520but%2520equipped%2520with%2520new%2520expert-written%252C%2520clinically%2520validated%2520reports%2520%2528135%2520cases%2529.%2520To%2520assess%2520clinical%2520utility%252C%2520we%2520introduce%2520PETRG-Score%252C%2520a%2520lymphoma-specific%2520evaluation%2520protocol%2520that%2520jointly%2520measures%2520metabolic%2520and%2520structural%2520findings%2520across%2520curated%2520anatomical%2520regions.%2520Experiments%2520show%2520that%2520PETRG-3D%2520substantially%2520outperforms%2520existing%2520methods%2520on%2520both%2520natural%2520language%2520metrics%2520%2528e.g.%252C%2520%252B31.49%255C%2525%2520ROUGE-L%2529%2520and%2520clinical%2520efficacy%2520metrics%2520%2528e.g.%252C%2520%252B8.18%255C%2525%2520PET-All%2529%252C%2520highlighting%2520the%2520benefits%2520of%2520volumetric%2520dual-modality%2520modeling%2520and%2520style-aware%2520prompting.%2520Overall%252C%2520this%2520work%2520establishes%2520a%2520foundation%2520for%2520future%2520PET/CT-specific%2520models%2520emphasizing%2520disease-aware%2520reasoning%2520and%2520clinically%2520reliable%2520evaluation.%2520Codes%252C%2520models%252C%2520and%2520AutoPET-RG-Lym%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Models%20for%20Automated%203D%20PET/CT%20Report%20Generation&entry.906535625=Wenpei%20Jiao%20and%20Kun%20Shang%20and%20Hui%20Li%20and%20Ke%20Yan%20and%20Jiajin%20Zhang%20and%20Guangjie%20Yang%20and%20Lijuan%20Guo%20and%20Yan%20Wan%20and%20Xing%20Yang%20and%20Dakai%20Jin%20and%20Zhaoheng%20Xie&entry.1292438233=Positron%20emission%20tomography/computed%20tomography%20%28PET/CT%29%20is%20essential%20in%20oncology%2C%20yet%20the%20rapid%20expansion%20of%20scanners%20has%20outpaced%20the%20availability%20of%20trained%20specialists%2C%20making%20automated%20PET/CT%20report%20generation%20%28PETRG%29%20increasingly%20important%20for%20reducing%20clinical%20workload.%20Compared%20with%20structural%20imaging%20%28e.g.%2C%20X-ray%2C%20CT%2C%20and%20MRI%29%2C%20functional%20PET%20poses%20distinct%20challenges%3A%20metabolic%20patterns%20vary%20with%20tracer%20physiology%2C%20and%20whole-body%203D%20contextual%20information%20is%20required%20rather%20than%20local-region%20interpretation.%20To%20advance%20PETRG%2C%20we%20propose%20PETRG-3D%2C%20an%20end-to-end%203D%20dual-branch%20framework%20that%20separately%20encodes%20PET%20and%20CT%20volumes%20and%20incorporates%20style-adaptive%20prompts%20to%20mitigate%20inter-hospital%20variability%20in%20reporting%20practices.%20We%20construct%20PETRG-Lym%2C%20a%20multi-center%20lymphoma%20dataset%20collected%20from%20four%20hospitals%20%28824%20reports%20w/%20245%2C509%20paired%20PET/CT%20slices%29%2C%20and%20construct%20AutoPET-RG-Lym%2C%20a%20publicly%20accessible%20PETRG%20benchmark%20derived%20from%20open%20imaging%20data%20but%20equipped%20with%20new%20expert-written%2C%20clinically%20validated%20reports%20%28135%20cases%29.%20To%20assess%20clinical%20utility%2C%20we%20introduce%20PETRG-Score%2C%20a%20lymphoma-specific%20evaluation%20protocol%20that%20jointly%20measures%20metabolic%20and%20structural%20findings%20across%20curated%20anatomical%20regions.%20Experiments%20show%20that%20PETRG-3D%20substantially%20outperforms%20existing%20methods%20on%20both%20natural%20language%20metrics%20%28e.g.%2C%20%2B31.49%5C%25%20ROUGE-L%29%20and%20clinical%20efficacy%20metrics%20%28e.g.%2C%20%2B8.18%5C%25%20PET-All%29%2C%20highlighting%20the%20benefits%20of%20volumetric%20dual-modality%20modeling%20and%20style-aware%20prompting.%20Overall%2C%20this%20work%20establishes%20a%20foundation%20for%20future%20PET/CT-specific%20models%20emphasizing%20disease-aware%20reasoning%20and%20clinically%20reliable%20evaluation.%20Codes%2C%20models%2C%20and%20AutoPET-RG-Lym%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.20145v1&entry.124074799=Read"},
{"title": "ShelfRectNet: Single View Shelf Image Rectification with Homography Estimation", "author": "Onur Berk Tore and Ibrahim Samil Yalciner and Server Calap", "abstract": "Estimating homography from a single image remains a challenging yet practically valuable task, particularly in domains like retail, where only one viewpoint is typically available for shelf monitoring and product alignment. In this paper, we present a deep learning framework that predicts a 4-point parameterized homography matrix to rectify shelf images captured from arbitrary angles. Our model leverages a ConvNeXt-based backbone for enhanced feature representation and adopts normalized coordinate regression for improved stability. To address data scarcity and promote generalization, we introduce a novel augmentation strategy by modeling and sampling synthetic homographies. Our method achieves a mean corner error of 1.298 pixels on the test set. When compared with both classical computer vision and deep learning-based approaches, our method demonstrates competitive performance in both accuracy and inference speed. Together, these results establish our approach as a robust and efficient solution for realworld single-view rectification. To encourage further research in this domain, we will make our dataset, ShelfRectSet, and code publicly available", "link": "http://arxiv.org/abs/2511.20335v1", "date": "2025-11-25", "relevancy": 2.6953, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5422}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5395}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShelfRectNet%3A%20Single%20View%20Shelf%20Image%20Rectification%20with%20Homography%20Estimation&body=Title%3A%20ShelfRectNet%3A%20Single%20View%20Shelf%20Image%20Rectification%20with%20Homography%20Estimation%0AAuthor%3A%20Onur%20Berk%20Tore%20and%20Ibrahim%20Samil%20Yalciner%20and%20Server%20Calap%0AAbstract%3A%20Estimating%20homography%20from%20a%20single%20image%20remains%20a%20challenging%20yet%20practically%20valuable%20task%2C%20particularly%20in%20domains%20like%20retail%2C%20where%20only%20one%20viewpoint%20is%20typically%20available%20for%20shelf%20monitoring%20and%20product%20alignment.%20In%20this%20paper%2C%20we%20present%20a%20deep%20learning%20framework%20that%20predicts%20a%204-point%20parameterized%20homography%20matrix%20to%20rectify%20shelf%20images%20captured%20from%20arbitrary%20angles.%20Our%20model%20leverages%20a%20ConvNeXt-based%20backbone%20for%20enhanced%20feature%20representation%20and%20adopts%20normalized%20coordinate%20regression%20for%20improved%20stability.%20To%20address%20data%20scarcity%20and%20promote%20generalization%2C%20we%20introduce%20a%20novel%20augmentation%20strategy%20by%20modeling%20and%20sampling%20synthetic%20homographies.%20Our%20method%20achieves%20a%20mean%20corner%20error%20of%201.298%20pixels%20on%20the%20test%20set.%20When%20compared%20with%20both%20classical%20computer%20vision%20and%20deep%20learning-based%20approaches%2C%20our%20method%20demonstrates%20competitive%20performance%20in%20both%20accuracy%20and%20inference%20speed.%20Together%2C%20these%20results%20establish%20our%20approach%20as%20a%20robust%20and%20efficient%20solution%20for%20realworld%20single-view%20rectification.%20To%20encourage%20further%20research%20in%20this%20domain%2C%20we%20will%20make%20our%20dataset%2C%20ShelfRectSet%2C%20and%20code%20publicly%20available%0ALink%3A%20http%3A//arxiv.org/abs/2511.20335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShelfRectNet%253A%2520Single%2520View%2520Shelf%2520Image%2520Rectification%2520with%2520Homography%2520Estimation%26entry.906535625%3DOnur%2520Berk%2520Tore%2520and%2520Ibrahim%2520Samil%2520Yalciner%2520and%2520Server%2520Calap%26entry.1292438233%3DEstimating%2520homography%2520from%2520a%2520single%2520image%2520remains%2520a%2520challenging%2520yet%2520practically%2520valuable%2520task%252C%2520particularly%2520in%2520domains%2520like%2520retail%252C%2520where%2520only%2520one%2520viewpoint%2520is%2520typically%2520available%2520for%2520shelf%2520monitoring%2520and%2520product%2520alignment.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520deep%2520learning%2520framework%2520that%2520predicts%2520a%25204-point%2520parameterized%2520homography%2520matrix%2520to%2520rectify%2520shelf%2520images%2520captured%2520from%2520arbitrary%2520angles.%2520Our%2520model%2520leverages%2520a%2520ConvNeXt-based%2520backbone%2520for%2520enhanced%2520feature%2520representation%2520and%2520adopts%2520normalized%2520coordinate%2520regression%2520for%2520improved%2520stability.%2520To%2520address%2520data%2520scarcity%2520and%2520promote%2520generalization%252C%2520we%2520introduce%2520a%2520novel%2520augmentation%2520strategy%2520by%2520modeling%2520and%2520sampling%2520synthetic%2520homographies.%2520Our%2520method%2520achieves%2520a%2520mean%2520corner%2520error%2520of%25201.298%2520pixels%2520on%2520the%2520test%2520set.%2520When%2520compared%2520with%2520both%2520classical%2520computer%2520vision%2520and%2520deep%2520learning-based%2520approaches%252C%2520our%2520method%2520demonstrates%2520competitive%2520performance%2520in%2520both%2520accuracy%2520and%2520inference%2520speed.%2520Together%252C%2520these%2520results%2520establish%2520our%2520approach%2520as%2520a%2520robust%2520and%2520efficient%2520solution%2520for%2520realworld%2520single-view%2520rectification.%2520To%2520encourage%2520further%2520research%2520in%2520this%2520domain%252C%2520we%2520will%2520make%2520our%2520dataset%252C%2520ShelfRectSet%252C%2520and%2520code%2520publicly%2520available%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShelfRectNet%3A%20Single%20View%20Shelf%20Image%20Rectification%20with%20Homography%20Estimation&entry.906535625=Onur%20Berk%20Tore%20and%20Ibrahim%20Samil%20Yalciner%20and%20Server%20Calap&entry.1292438233=Estimating%20homography%20from%20a%20single%20image%20remains%20a%20challenging%20yet%20practically%20valuable%20task%2C%20particularly%20in%20domains%20like%20retail%2C%20where%20only%20one%20viewpoint%20is%20typically%20available%20for%20shelf%20monitoring%20and%20product%20alignment.%20In%20this%20paper%2C%20we%20present%20a%20deep%20learning%20framework%20that%20predicts%20a%204-point%20parameterized%20homography%20matrix%20to%20rectify%20shelf%20images%20captured%20from%20arbitrary%20angles.%20Our%20model%20leverages%20a%20ConvNeXt-based%20backbone%20for%20enhanced%20feature%20representation%20and%20adopts%20normalized%20coordinate%20regression%20for%20improved%20stability.%20To%20address%20data%20scarcity%20and%20promote%20generalization%2C%20we%20introduce%20a%20novel%20augmentation%20strategy%20by%20modeling%20and%20sampling%20synthetic%20homographies.%20Our%20method%20achieves%20a%20mean%20corner%20error%20of%201.298%20pixels%20on%20the%20test%20set.%20When%20compared%20with%20both%20classical%20computer%20vision%20and%20deep%20learning-based%20approaches%2C%20our%20method%20demonstrates%20competitive%20performance%20in%20both%20accuracy%20and%20inference%20speed.%20Together%2C%20these%20results%20establish%20our%20approach%20as%20a%20robust%20and%20efficient%20solution%20for%20realworld%20single-view%20rectification.%20To%20encourage%20further%20research%20in%20this%20domain%2C%20we%20will%20make%20our%20dataset%2C%20ShelfRectSet%2C%20and%20code%20publicly%20available&entry.1838667208=http%3A//arxiv.org/abs/2511.20335v1&entry.124074799=Read"},
{"title": "Harnessing Vision-Language Models for Time Series Anomaly Detection", "author": "Zelin He and Sarah Alnegheimish and Matthew Reimherr", "abstract": "Time-series anomaly detection (TSAD) has played a vital role in a variety of fields, including healthcare, finance, and sensor-based condition monitoring. Prior methods, which mainly focus on training domain-specific models on numerical data, lack the visual-temporal understanding capacity that human experts have to identify contextual anomalies. To fill this gap, we explore a solution based on vision language models (VLMs). Recent studies have shown the ability of VLMs for visual understanding tasks, yet their direct application to time series has fallen short on both accuracy and efficiency. To harness the power of VLMs for TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening stage built on a relatively lightweight pre-trained vision encoder, which leverages 2D time series representations to accurately localize candidate anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal context and VLM's visual understanding capacity to refine the detection upon the candidates provided by ViT4TS. We show that without any time-series training, VLM4TS outperforms time-series pre-trained and from-scratch baselines in most cases, yielding a 24.6% improvement in F1-max score over the best baseline. Moreover, VLM4TS also consistently outperforms existing language model-based TSAD methods and is on average 36x more efficient in token usage.", "link": "http://arxiv.org/abs/2506.06836v2", "date": "2025-11-25", "relevancy": 2.6503, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Vision-Language%20Models%20for%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20Harnessing%20Vision-Language%20Models%20for%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Zelin%20He%20and%20Sarah%20Alnegheimish%20and%20Matthew%20Reimherr%0AAbstract%3A%20Time-series%20anomaly%20detection%20%28TSAD%29%20has%20played%20a%20vital%20role%20in%20a%20variety%20of%20fields%2C%20including%20healthcare%2C%20finance%2C%20and%20sensor-based%20condition%20monitoring.%20Prior%20methods%2C%20which%20mainly%20focus%20on%20training%20domain-specific%20models%20on%20numerical%20data%2C%20lack%20the%20visual-temporal%20understanding%20capacity%20that%20human%20experts%20have%20to%20identify%20contextual%20anomalies.%20To%20fill%20this%20gap%2C%20we%20explore%20a%20solution%20based%20on%20vision%20language%20models%20%28VLMs%29.%20Recent%20studies%20have%20shown%20the%20ability%20of%20VLMs%20for%20visual%20understanding%20tasks%2C%20yet%20their%20direct%20application%20to%20time%20series%20has%20fallen%20short%20on%20both%20accuracy%20and%20efficiency.%20To%20harness%20the%20power%20of%20VLMs%20for%20TSAD%2C%20we%20propose%20a%20two-stage%20solution%2C%20with%20%281%29%20ViT4TS%2C%20a%20vision-screening%20stage%20built%20on%20a%20relatively%20lightweight%20pre-trained%20vision%20encoder%2C%20which%20leverages%202D%20time%20series%20representations%20to%20accurately%20localize%20candidate%20anomalies%3B%20%282%29%20VLM4TS%2C%20a%20VLM-based%20stage%20that%20integrates%20global%20temporal%20context%20and%20VLM%27s%20visual%20understanding%20capacity%20to%20refine%20the%20detection%20upon%20the%20candidates%20provided%20by%20ViT4TS.%20We%20show%20that%20without%20any%20time-series%20training%2C%20VLM4TS%20outperforms%20time-series%20pre-trained%20and%20from-scratch%20baselines%20in%20most%20cases%2C%20yielding%20a%2024.6%25%20improvement%20in%20F1-max%20score%20over%20the%20best%20baseline.%20Moreover%2C%20VLM4TS%20also%20consistently%20outperforms%20existing%20language%20model-based%20TSAD%20methods%20and%20is%20on%20average%2036x%20more%20efficient%20in%20token%20usage.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Vision-Language%2520Models%2520for%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DZelin%2520He%2520and%2520Sarah%2520Alnegheimish%2520and%2520Matthew%2520Reimherr%26entry.1292438233%3DTime-series%2520anomaly%2520detection%2520%2528TSAD%2529%2520has%2520played%2520a%2520vital%2520role%2520in%2520a%2520variety%2520of%2520fields%252C%2520including%2520healthcare%252C%2520finance%252C%2520and%2520sensor-based%2520condition%2520monitoring.%2520Prior%2520methods%252C%2520which%2520mainly%2520focus%2520on%2520training%2520domain-specific%2520models%2520on%2520numerical%2520data%252C%2520lack%2520the%2520visual-temporal%2520understanding%2520capacity%2520that%2520human%2520experts%2520have%2520to%2520identify%2520contextual%2520anomalies.%2520To%2520fill%2520this%2520gap%252C%2520we%2520explore%2520a%2520solution%2520based%2520on%2520vision%2520language%2520models%2520%2528VLMs%2529.%2520Recent%2520studies%2520have%2520shown%2520the%2520ability%2520of%2520VLMs%2520for%2520visual%2520understanding%2520tasks%252C%2520yet%2520their%2520direct%2520application%2520to%2520time%2520series%2520has%2520fallen%2520short%2520on%2520both%2520accuracy%2520and%2520efficiency.%2520To%2520harness%2520the%2520power%2520of%2520VLMs%2520for%2520TSAD%252C%2520we%2520propose%2520a%2520two-stage%2520solution%252C%2520with%2520%25281%2529%2520ViT4TS%252C%2520a%2520vision-screening%2520stage%2520built%2520on%2520a%2520relatively%2520lightweight%2520pre-trained%2520vision%2520encoder%252C%2520which%2520leverages%25202D%2520time%2520series%2520representations%2520to%2520accurately%2520localize%2520candidate%2520anomalies%253B%2520%25282%2529%2520VLM4TS%252C%2520a%2520VLM-based%2520stage%2520that%2520integrates%2520global%2520temporal%2520context%2520and%2520VLM%2527s%2520visual%2520understanding%2520capacity%2520to%2520refine%2520the%2520detection%2520upon%2520the%2520candidates%2520provided%2520by%2520ViT4TS.%2520We%2520show%2520that%2520without%2520any%2520time-series%2520training%252C%2520VLM4TS%2520outperforms%2520time-series%2520pre-trained%2520and%2520from-scratch%2520baselines%2520in%2520most%2520cases%252C%2520yielding%2520a%252024.6%2525%2520improvement%2520in%2520F1-max%2520score%2520over%2520the%2520best%2520baseline.%2520Moreover%252C%2520VLM4TS%2520also%2520consistently%2520outperforms%2520existing%2520language%2520model-based%2520TSAD%2520methods%2520and%2520is%2520on%2520average%252036x%2520more%2520efficient%2520in%2520token%2520usage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Vision-Language%20Models%20for%20Time%20Series%20Anomaly%20Detection&entry.906535625=Zelin%20He%20and%20Sarah%20Alnegheimish%20and%20Matthew%20Reimherr&entry.1292438233=Time-series%20anomaly%20detection%20%28TSAD%29%20has%20played%20a%20vital%20role%20in%20a%20variety%20of%20fields%2C%20including%20healthcare%2C%20finance%2C%20and%20sensor-based%20condition%20monitoring.%20Prior%20methods%2C%20which%20mainly%20focus%20on%20training%20domain-specific%20models%20on%20numerical%20data%2C%20lack%20the%20visual-temporal%20understanding%20capacity%20that%20human%20experts%20have%20to%20identify%20contextual%20anomalies.%20To%20fill%20this%20gap%2C%20we%20explore%20a%20solution%20based%20on%20vision%20language%20models%20%28VLMs%29.%20Recent%20studies%20have%20shown%20the%20ability%20of%20VLMs%20for%20visual%20understanding%20tasks%2C%20yet%20their%20direct%20application%20to%20time%20series%20has%20fallen%20short%20on%20both%20accuracy%20and%20efficiency.%20To%20harness%20the%20power%20of%20VLMs%20for%20TSAD%2C%20we%20propose%20a%20two-stage%20solution%2C%20with%20%281%29%20ViT4TS%2C%20a%20vision-screening%20stage%20built%20on%20a%20relatively%20lightweight%20pre-trained%20vision%20encoder%2C%20which%20leverages%202D%20time%20series%20representations%20to%20accurately%20localize%20candidate%20anomalies%3B%20%282%29%20VLM4TS%2C%20a%20VLM-based%20stage%20that%20integrates%20global%20temporal%20context%20and%20VLM%27s%20visual%20understanding%20capacity%20to%20refine%20the%20detection%20upon%20the%20candidates%20provided%20by%20ViT4TS.%20We%20show%20that%20without%20any%20time-series%20training%2C%20VLM4TS%20outperforms%20time-series%20pre-trained%20and%20from-scratch%20baselines%20in%20most%20cases%2C%20yielding%20a%2024.6%25%20improvement%20in%20F1-max%20score%20over%20the%20best%20baseline.%20Moreover%2C%20VLM4TS%20also%20consistently%20outperforms%20existing%20language%20model-based%20TSAD%20methods%20and%20is%20on%20average%2036x%20more%20efficient%20in%20token%20usage.&entry.1838667208=http%3A//arxiv.org/abs/2506.06836v2&entry.124074799=Read"},
{"title": "Sparse Techniques for Regression in Deep Gaussian Processes", "author": "Jonas Latz and Aretha L. Teckentrup and Simon Urbainczyk", "abstract": "Gaussian processes (GPs) have gained popularity as flexible machine learning models for regression and function approximation with an in-built method for uncertainty quantification. However, GPs suffer when the amount of training data is large or when the underlying function contains multi-scale features that are difficult to represent by a stationary kernel. To address the former, training of GPs with large-scale data is often performed through inducing point approximations, also known as sparse GP regression (GPR), where the size of the covariance matrices in GPR is reduced considerably through a greedy search on the data set. To aid the latter, deep GPs have gained traction as hierarchical models that resolve multi-scale features by combining multiple GPs. Posterior inference in deep GPs requires a sampling or, more usual, a variational approximation. Variational approximations lead to large-scale stochastic, non-convex optimisation problems and the resulting approximation tends to represent uncertainty incorrectly. In this work, we combine variational learning with MCMC to develop a particle-based expectation-maximisation method to simultaneously find inducing points within the large-scale data (variationally) and accurately train the deep GPs (sampling-based). The result is a highly efficient and accurate methodology for deep GP training on large-scale data. We test our method on standard benchmark problems.", "link": "http://arxiv.org/abs/2505.11355v2", "date": "2025-11-25", "relevancy": 2.6378, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5296}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5279}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Techniques%20for%20Regression%20in%20Deep%20Gaussian%20Processes&body=Title%3A%20Sparse%20Techniques%20for%20Regression%20in%20Deep%20Gaussian%20Processes%0AAuthor%3A%20Jonas%20Latz%20and%20Aretha%20L.%20Teckentrup%20and%20Simon%20Urbainczyk%0AAbstract%3A%20Gaussian%20processes%20%28GPs%29%20have%20gained%20popularity%20as%20flexible%20machine%20learning%20models%20for%20regression%20and%20function%20approximation%20with%20an%20in-built%20method%20for%20uncertainty%20quantification.%20However%2C%20GPs%20suffer%20when%20the%20amount%20of%20training%20data%20is%20large%20or%20when%20the%20underlying%20function%20contains%20multi-scale%20features%20that%20are%20difficult%20to%20represent%20by%20a%20stationary%20kernel.%20To%20address%20the%20former%2C%20training%20of%20GPs%20with%20large-scale%20data%20is%20often%20performed%20through%20inducing%20point%20approximations%2C%20also%20known%20as%20sparse%20GP%20regression%20%28GPR%29%2C%20where%20the%20size%20of%20the%20covariance%20matrices%20in%20GPR%20is%20reduced%20considerably%20through%20a%20greedy%20search%20on%20the%20data%20set.%20To%20aid%20the%20latter%2C%20deep%20GPs%20have%20gained%20traction%20as%20hierarchical%20models%20that%20resolve%20multi-scale%20features%20by%20combining%20multiple%20GPs.%20Posterior%20inference%20in%20deep%20GPs%20requires%20a%20sampling%20or%2C%20more%20usual%2C%20a%20variational%20approximation.%20Variational%20approximations%20lead%20to%20large-scale%20stochastic%2C%20non-convex%20optimisation%20problems%20and%20the%20resulting%20approximation%20tends%20to%20represent%20uncertainty%20incorrectly.%20In%20this%20work%2C%20we%20combine%20variational%20learning%20with%20MCMC%20to%20develop%20a%20particle-based%20expectation-maximisation%20method%20to%20simultaneously%20find%20inducing%20points%20within%20the%20large-scale%20data%20%28variationally%29%20and%20accurately%20train%20the%20deep%20GPs%20%28sampling-based%29.%20The%20result%20is%20a%20highly%20efficient%20and%20accurate%20methodology%20for%20deep%20GP%20training%20on%20large-scale%20data.%20We%20test%20our%20method%20on%20standard%20benchmark%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Techniques%2520for%2520Regression%2520in%2520Deep%2520Gaussian%2520Processes%26entry.906535625%3DJonas%2520Latz%2520and%2520Aretha%2520L.%2520Teckentrup%2520and%2520Simon%2520Urbainczyk%26entry.1292438233%3DGaussian%2520processes%2520%2528GPs%2529%2520have%2520gained%2520popularity%2520as%2520flexible%2520machine%2520learning%2520models%2520for%2520regression%2520and%2520function%2520approximation%2520with%2520an%2520in-built%2520method%2520for%2520uncertainty%2520quantification.%2520However%252C%2520GPs%2520suffer%2520when%2520the%2520amount%2520of%2520training%2520data%2520is%2520large%2520or%2520when%2520the%2520underlying%2520function%2520contains%2520multi-scale%2520features%2520that%2520are%2520difficult%2520to%2520represent%2520by%2520a%2520stationary%2520kernel.%2520To%2520address%2520the%2520former%252C%2520training%2520of%2520GPs%2520with%2520large-scale%2520data%2520is%2520often%2520performed%2520through%2520inducing%2520point%2520approximations%252C%2520also%2520known%2520as%2520sparse%2520GP%2520regression%2520%2528GPR%2529%252C%2520where%2520the%2520size%2520of%2520the%2520covariance%2520matrices%2520in%2520GPR%2520is%2520reduced%2520considerably%2520through%2520a%2520greedy%2520search%2520on%2520the%2520data%2520set.%2520To%2520aid%2520the%2520latter%252C%2520deep%2520GPs%2520have%2520gained%2520traction%2520as%2520hierarchical%2520models%2520that%2520resolve%2520multi-scale%2520features%2520by%2520combining%2520multiple%2520GPs.%2520Posterior%2520inference%2520in%2520deep%2520GPs%2520requires%2520a%2520sampling%2520or%252C%2520more%2520usual%252C%2520a%2520variational%2520approximation.%2520Variational%2520approximations%2520lead%2520to%2520large-scale%2520stochastic%252C%2520non-convex%2520optimisation%2520problems%2520and%2520the%2520resulting%2520approximation%2520tends%2520to%2520represent%2520uncertainty%2520incorrectly.%2520In%2520this%2520work%252C%2520we%2520combine%2520variational%2520learning%2520with%2520MCMC%2520to%2520develop%2520a%2520particle-based%2520expectation-maximisation%2520method%2520to%2520simultaneously%2520find%2520inducing%2520points%2520within%2520the%2520large-scale%2520data%2520%2528variationally%2529%2520and%2520accurately%2520train%2520the%2520deep%2520GPs%2520%2528sampling-based%2529.%2520The%2520result%2520is%2520a%2520highly%2520efficient%2520and%2520accurate%2520methodology%2520for%2520deep%2520GP%2520training%2520on%2520large-scale%2520data.%2520We%2520test%2520our%2520method%2520on%2520standard%2520benchmark%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Techniques%20for%20Regression%20in%20Deep%20Gaussian%20Processes&entry.906535625=Jonas%20Latz%20and%20Aretha%20L.%20Teckentrup%20and%20Simon%20Urbainczyk&entry.1292438233=Gaussian%20processes%20%28GPs%29%20have%20gained%20popularity%20as%20flexible%20machine%20learning%20models%20for%20regression%20and%20function%20approximation%20with%20an%20in-built%20method%20for%20uncertainty%20quantification.%20However%2C%20GPs%20suffer%20when%20the%20amount%20of%20training%20data%20is%20large%20or%20when%20the%20underlying%20function%20contains%20multi-scale%20features%20that%20are%20difficult%20to%20represent%20by%20a%20stationary%20kernel.%20To%20address%20the%20former%2C%20training%20of%20GPs%20with%20large-scale%20data%20is%20often%20performed%20through%20inducing%20point%20approximations%2C%20also%20known%20as%20sparse%20GP%20regression%20%28GPR%29%2C%20where%20the%20size%20of%20the%20covariance%20matrices%20in%20GPR%20is%20reduced%20considerably%20through%20a%20greedy%20search%20on%20the%20data%20set.%20To%20aid%20the%20latter%2C%20deep%20GPs%20have%20gained%20traction%20as%20hierarchical%20models%20that%20resolve%20multi-scale%20features%20by%20combining%20multiple%20GPs.%20Posterior%20inference%20in%20deep%20GPs%20requires%20a%20sampling%20or%2C%20more%20usual%2C%20a%20variational%20approximation.%20Variational%20approximations%20lead%20to%20large-scale%20stochastic%2C%20non-convex%20optimisation%20problems%20and%20the%20resulting%20approximation%20tends%20to%20represent%20uncertainty%20incorrectly.%20In%20this%20work%2C%20we%20combine%20variational%20learning%20with%20MCMC%20to%20develop%20a%20particle-based%20expectation-maximisation%20method%20to%20simultaneously%20find%20inducing%20points%20within%20the%20large-scale%20data%20%28variationally%29%20and%20accurately%20train%20the%20deep%20GPs%20%28sampling-based%29.%20The%20result%20is%20a%20highly%20efficient%20and%20accurate%20methodology%20for%20deep%20GP%20training%20on%20large-scale%20data.%20We%20test%20our%20method%20on%20standard%20benchmark%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2505.11355v2&entry.124074799=Read"},
{"title": "Learning to Generate Human-Human-Object Interactions from Textual Descriptions", "author": "Jeonghyeon Na and Sangwon Baik and Inhee Lee and Junyoung Lee and Hanbyul Joo", "abstract": "The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.", "link": "http://arxiv.org/abs/2511.20446v1", "date": "2025-11-25", "relevancy": 2.6334, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6706}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6654}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%20Human-Human-Object%20Interactions%20from%20Textual%20Descriptions&body=Title%3A%20Learning%20to%20Generate%20Human-Human-Object%20Interactions%20from%20Textual%20Descriptions%0AAuthor%3A%20Jeonghyeon%20Na%20and%20Sangwon%20Baik%20and%20Inhee%20Lee%20and%20Junyoung%20Lee%20and%20Hanbyul%20Joo%0AAbstract%3A%20The%20way%20humans%20interact%20with%20each%20other%2C%20including%20interpersonal%20distances%2C%20spatial%20configuration%2C%20and%20motion%2C%20varies%20significantly%20across%20different%20situations.%20To%20enable%20machines%20to%20understand%20such%20complex%2C%20context-dependent%20behaviors%2C%20it%20is%20essential%20to%20model%20multiple%20people%20in%20relation%20to%20the%20surrounding%20scene%20context.%20In%20this%20paper%2C%20we%20present%20a%20novel%20research%20problem%20to%20model%20the%20correlations%20between%20two%20people%20engaged%20in%20a%20shared%20interaction%20involving%20an%20object.%20We%20refer%20to%20this%20formulation%20as%20Human-Human-Object%20Interactions%20%28HHOIs%29.%20To%20overcome%20the%20lack%20of%20dedicated%20datasets%20for%20HHOIs%2C%20we%20present%20a%20newly%20captured%20HHOIs%20dataset%20and%20a%20method%20to%20synthesize%20HHOI%20data%20by%20leveraging%20image%20generative%20models.%20As%20an%20intermediary%2C%20we%20obtain%20individual%20human-object%20interaction%20%28HOIs%29%20and%20human-human%20interaction%20%28HHIs%29%20from%20the%20HHOIs%2C%20and%20with%20these%20data%2C%20we%20train%20an%20text-to-HOI%20and%20text-to-HHI%20model%20using%20score-based%20diffusion%20model.%20Finally%2C%20we%20present%20a%20unified%20generative%20framework%20that%20integrates%20the%20two%20individual%20model%2C%20capable%20of%20synthesizing%20complete%20HHOIs%20in%20a%20single%20advanced%20sampling%20process.%20Our%20method%20extends%20HHOI%20generation%20to%20multi-human%20settings%2C%20enabling%20interactions%20involving%20more%20than%20two%20individuals.%20Experimental%20results%20show%20that%20our%20method%20generates%20realistic%20HHOIs%20conditioned%20on%20textual%20descriptions%2C%20outperforming%20previous%20approaches%20that%20focus%20only%20on%20single-human%20HOIs.%20Furthermore%2C%20we%20introduce%20multi-human%20motion%20generation%20involving%20objects%20as%20an%20application%20of%20our%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%2520Human-Human-Object%2520Interactions%2520from%2520Textual%2520Descriptions%26entry.906535625%3DJeonghyeon%2520Na%2520and%2520Sangwon%2520Baik%2520and%2520Inhee%2520Lee%2520and%2520Junyoung%2520Lee%2520and%2520Hanbyul%2520Joo%26entry.1292438233%3DThe%2520way%2520humans%2520interact%2520with%2520each%2520other%252C%2520including%2520interpersonal%2520distances%252C%2520spatial%2520configuration%252C%2520and%2520motion%252C%2520varies%2520significantly%2520across%2520different%2520situations.%2520To%2520enable%2520machines%2520to%2520understand%2520such%2520complex%252C%2520context-dependent%2520behaviors%252C%2520it%2520is%2520essential%2520to%2520model%2520multiple%2520people%2520in%2520relation%2520to%2520the%2520surrounding%2520scene%2520context.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520research%2520problem%2520to%2520model%2520the%2520correlations%2520between%2520two%2520people%2520engaged%2520in%2520a%2520shared%2520interaction%2520involving%2520an%2520object.%2520We%2520refer%2520to%2520this%2520formulation%2520as%2520Human-Human-Object%2520Interactions%2520%2528HHOIs%2529.%2520To%2520overcome%2520the%2520lack%2520of%2520dedicated%2520datasets%2520for%2520HHOIs%252C%2520we%2520present%2520a%2520newly%2520captured%2520HHOIs%2520dataset%2520and%2520a%2520method%2520to%2520synthesize%2520HHOI%2520data%2520by%2520leveraging%2520image%2520generative%2520models.%2520As%2520an%2520intermediary%252C%2520we%2520obtain%2520individual%2520human-object%2520interaction%2520%2528HOIs%2529%2520and%2520human-human%2520interaction%2520%2528HHIs%2529%2520from%2520the%2520HHOIs%252C%2520and%2520with%2520these%2520data%252C%2520we%2520train%2520an%2520text-to-HOI%2520and%2520text-to-HHI%2520model%2520using%2520score-based%2520diffusion%2520model.%2520Finally%252C%2520we%2520present%2520a%2520unified%2520generative%2520framework%2520that%2520integrates%2520the%2520two%2520individual%2520model%252C%2520capable%2520of%2520synthesizing%2520complete%2520HHOIs%2520in%2520a%2520single%2520advanced%2520sampling%2520process.%2520Our%2520method%2520extends%2520HHOI%2520generation%2520to%2520multi-human%2520settings%252C%2520enabling%2520interactions%2520involving%2520more%2520than%2520two%2520individuals.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520generates%2520realistic%2520HHOIs%2520conditioned%2520on%2520textual%2520descriptions%252C%2520outperforming%2520previous%2520approaches%2520that%2520focus%2520only%2520on%2520single-human%2520HOIs.%2520Furthermore%252C%2520we%2520introduce%2520multi-human%2520motion%2520generation%2520involving%2520objects%2520as%2520an%2520application%2520of%2520our%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%20Human-Human-Object%20Interactions%20from%20Textual%20Descriptions&entry.906535625=Jeonghyeon%20Na%20and%20Sangwon%20Baik%20and%20Inhee%20Lee%20and%20Junyoung%20Lee%20and%20Hanbyul%20Joo&entry.1292438233=The%20way%20humans%20interact%20with%20each%20other%2C%20including%20interpersonal%20distances%2C%20spatial%20configuration%2C%20and%20motion%2C%20varies%20significantly%20across%20different%20situations.%20To%20enable%20machines%20to%20understand%20such%20complex%2C%20context-dependent%20behaviors%2C%20it%20is%20essential%20to%20model%20multiple%20people%20in%20relation%20to%20the%20surrounding%20scene%20context.%20In%20this%20paper%2C%20we%20present%20a%20novel%20research%20problem%20to%20model%20the%20correlations%20between%20two%20people%20engaged%20in%20a%20shared%20interaction%20involving%20an%20object.%20We%20refer%20to%20this%20formulation%20as%20Human-Human-Object%20Interactions%20%28HHOIs%29.%20To%20overcome%20the%20lack%20of%20dedicated%20datasets%20for%20HHOIs%2C%20we%20present%20a%20newly%20captured%20HHOIs%20dataset%20and%20a%20method%20to%20synthesize%20HHOI%20data%20by%20leveraging%20image%20generative%20models.%20As%20an%20intermediary%2C%20we%20obtain%20individual%20human-object%20interaction%20%28HOIs%29%20and%20human-human%20interaction%20%28HHIs%29%20from%20the%20HHOIs%2C%20and%20with%20these%20data%2C%20we%20train%20an%20text-to-HOI%20and%20text-to-HHI%20model%20using%20score-based%20diffusion%20model.%20Finally%2C%20we%20present%20a%20unified%20generative%20framework%20that%20integrates%20the%20two%20individual%20model%2C%20capable%20of%20synthesizing%20complete%20HHOIs%20in%20a%20single%20advanced%20sampling%20process.%20Our%20method%20extends%20HHOI%20generation%20to%20multi-human%20settings%2C%20enabling%20interactions%20involving%20more%20than%20two%20individuals.%20Experimental%20results%20show%20that%20our%20method%20generates%20realistic%20HHOIs%20conditioned%20on%20textual%20descriptions%2C%20outperforming%20previous%20approaches%20that%20focus%20only%20on%20single-human%20HOIs.%20Furthermore%2C%20we%20introduce%20multi-human%20motion%20generation%20involving%20objects%20as%20an%20application%20of%20our%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2511.20446v1&entry.124074799=Read"},
{"title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs", "author": "Kuniaki Saito and Risa Shinoda and Shohei Tanaka and Tosho Hirasawa and Fumio Okura and Yoshitaka Ushiku", "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.", "link": "http://arxiv.org/abs/2511.20515v1", "date": "2025-11-25", "relevancy": 2.631, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignBench%3A%20Benchmarking%20Fine-Grained%20Image-Text%20Alignment%20with%20Synthetic%20Image-Caption%20Pairs&body=Title%3A%20AlignBench%3A%20Benchmarking%20Fine-Grained%20Image-Text%20Alignment%20with%20Synthetic%20Image-Caption%20Pairs%0AAuthor%3A%20Kuniaki%20Saito%20and%20Risa%20Shinoda%20and%20Shohei%20Tanaka%20and%20Tosho%20Hirasawa%20and%20Fumio%20Okura%20and%20Yoshitaka%20Ushiku%0AAbstract%3A%20Assessing%20image-text%20alignment%20models%20such%20as%20CLIP%20is%20crucial%20for%20bridging%20visual%20and%20linguistic%20representations.%20Yet%20existing%20benchmarks%20rely%20on%20rule-based%20perturbations%20or%20short%20captions%2C%20limiting%20their%20ability%20to%20measure%20fine-grained%20alignment.%20We%20introduce%20AlignBench%2C%20a%20benchmark%20that%20provides%20a%20new%20indicator%20of%20image-text%20alignment%20by%20evaluating%20detailed%20image-caption%20pairs%20generated%20by%20diverse%20image-to-text%20and%20text-to-image%20models.%20Each%20sentence%20is%20annotated%20for%20correctness%2C%20enabling%20direct%20assessment%20of%20VLMs%20as%20alignment%20evaluators.%20Benchmarking%20a%20wide%20range%20of%20decoder-based%20VLMs%20reveals%20three%20key%20findings%3A%20%28i%29%20CLIP-based%20models%2C%20even%20those%20tailored%20for%20compositional%20reasoning%2C%20remain%20nearly%20blind%3B%20%28ii%29%20detectors%20systematically%20over-score%20early%20sentences%3B%20and%20%28iii%29%20they%20show%20strong%20self-preference%2C%20favoring%20their%20own%20outputs%20and%20harming%20detection%20performance.%20Our%20project%20page%20will%20be%20available%20at%20https%3A//dahlian00.github.io/AlignBench/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignBench%253A%2520Benchmarking%2520Fine-Grained%2520Image-Text%2520Alignment%2520with%2520Synthetic%2520Image-Caption%2520Pairs%26entry.906535625%3DKuniaki%2520Saito%2520and%2520Risa%2520Shinoda%2520and%2520Shohei%2520Tanaka%2520and%2520Tosho%2520Hirasawa%2520and%2520Fumio%2520Okura%2520and%2520Yoshitaka%2520Ushiku%26entry.1292438233%3DAssessing%2520image-text%2520alignment%2520models%2520such%2520as%2520CLIP%2520is%2520crucial%2520for%2520bridging%2520visual%2520and%2520linguistic%2520representations.%2520Yet%2520existing%2520benchmarks%2520rely%2520on%2520rule-based%2520perturbations%2520or%2520short%2520captions%252C%2520limiting%2520their%2520ability%2520to%2520measure%2520fine-grained%2520alignment.%2520We%2520introduce%2520AlignBench%252C%2520a%2520benchmark%2520that%2520provides%2520a%2520new%2520indicator%2520of%2520image-text%2520alignment%2520by%2520evaluating%2520detailed%2520image-caption%2520pairs%2520generated%2520by%2520diverse%2520image-to-text%2520and%2520text-to-image%2520models.%2520Each%2520sentence%2520is%2520annotated%2520for%2520correctness%252C%2520enabling%2520direct%2520assessment%2520of%2520VLMs%2520as%2520alignment%2520evaluators.%2520Benchmarking%2520a%2520wide%2520range%2520of%2520decoder-based%2520VLMs%2520reveals%2520three%2520key%2520findings%253A%2520%2528i%2529%2520CLIP-based%2520models%252C%2520even%2520those%2520tailored%2520for%2520compositional%2520reasoning%252C%2520remain%2520nearly%2520blind%253B%2520%2528ii%2529%2520detectors%2520systematically%2520over-score%2520early%2520sentences%253B%2520and%2520%2528iii%2529%2520they%2520show%2520strong%2520self-preference%252C%2520favoring%2520their%2520own%2520outputs%2520and%2520harming%2520detection%2520performance.%2520Our%2520project%2520page%2520will%2520be%2520available%2520at%2520https%253A//dahlian00.github.io/AlignBench/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignBench%3A%20Benchmarking%20Fine-Grained%20Image-Text%20Alignment%20with%20Synthetic%20Image-Caption%20Pairs&entry.906535625=Kuniaki%20Saito%20and%20Risa%20Shinoda%20and%20Shohei%20Tanaka%20and%20Tosho%20Hirasawa%20and%20Fumio%20Okura%20and%20Yoshitaka%20Ushiku&entry.1292438233=Assessing%20image-text%20alignment%20models%20such%20as%20CLIP%20is%20crucial%20for%20bridging%20visual%20and%20linguistic%20representations.%20Yet%20existing%20benchmarks%20rely%20on%20rule-based%20perturbations%20or%20short%20captions%2C%20limiting%20their%20ability%20to%20measure%20fine-grained%20alignment.%20We%20introduce%20AlignBench%2C%20a%20benchmark%20that%20provides%20a%20new%20indicator%20of%20image-text%20alignment%20by%20evaluating%20detailed%20image-caption%20pairs%20generated%20by%20diverse%20image-to-text%20and%20text-to-image%20models.%20Each%20sentence%20is%20annotated%20for%20correctness%2C%20enabling%20direct%20assessment%20of%20VLMs%20as%20alignment%20evaluators.%20Benchmarking%20a%20wide%20range%20of%20decoder-based%20VLMs%20reveals%20three%20key%20findings%3A%20%28i%29%20CLIP-based%20models%2C%20even%20those%20tailored%20for%20compositional%20reasoning%2C%20remain%20nearly%20blind%3B%20%28ii%29%20detectors%20systematically%20over-score%20early%20sentences%3B%20and%20%28iii%29%20they%20show%20strong%20self-preference%2C%20favoring%20their%20own%20outputs%20and%20harming%20detection%20performance.%20Our%20project%20page%20will%20be%20available%20at%20https%3A//dahlian00.github.io/AlignBench/.&entry.1838667208=http%3A//arxiv.org/abs/2511.20515v1&entry.124074799=Read"},
{"title": "Object-Centric Vision Token Pruning for Vision Language Models", "author": "Guangyuan Li and Rongzhen Zhao and Jinhong Deng and Yanbo Wang and Joni Pajarinen", "abstract": "In Vision Language Models (VLMs), vision tokens are quantity-heavy yet information-dispersed compared with language tokens, thus consume too much unnecessary computation. Pruning redundant vision tokens for high VLM inference efficiency has been continuously studied but all existing methods resort to indirect and non-guaranteed ways. We propose OC-VTP, a direct and guaranteed approach to select the most representative vision tokens for high-efficiency yet accuracy-preserving VLM inference. Our OC-VTP requires merely light-weight pre-training of a small object-centric vision token pruner, which can then be inserted into existing VLMs, without fine-tuning of any models on any datasets. It is gauranteed that the most representative vision tokens are kept by minimizing the error in reconstructing the original unpruned tokens from the selected ones. Across any vision pruning ratios, i.e., inference efficiency, our OC-VTP consistently helps mainstream VLMs to preserve the highest inference accuracy. Our pruning also demonstrates interesting interpretability. Our codes are available at https://github.com/GarryLarry010131/OC-VTP.", "link": "http://arxiv.org/abs/2511.20439v1", "date": "2025-11-25", "relevancy": 2.6134, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Centric%20Vision%20Token%20Pruning%20for%20Vision%20Language%20Models&body=Title%3A%20Object-Centric%20Vision%20Token%20Pruning%20for%20Vision%20Language%20Models%0AAuthor%3A%20Guangyuan%20Li%20and%20Rongzhen%20Zhao%20and%20Jinhong%20Deng%20and%20Yanbo%20Wang%20and%20Joni%20Pajarinen%0AAbstract%3A%20In%20Vision%20Language%20Models%20%28VLMs%29%2C%20vision%20tokens%20are%20quantity-heavy%20yet%20information-dispersed%20compared%20with%20language%20tokens%2C%20thus%20consume%20too%20much%20unnecessary%20computation.%20Pruning%20redundant%20vision%20tokens%20for%20high%20VLM%20inference%20efficiency%20has%20been%20continuously%20studied%20but%20all%20existing%20methods%20resort%20to%20indirect%20and%20non-guaranteed%20ways.%20We%20propose%20OC-VTP%2C%20a%20direct%20and%20guaranteed%20approach%20to%20select%20the%20most%20representative%20vision%20tokens%20for%20high-efficiency%20yet%20accuracy-preserving%20VLM%20inference.%20Our%20OC-VTP%20requires%20merely%20light-weight%20pre-training%20of%20a%20small%20object-centric%20vision%20token%20pruner%2C%20which%20can%20then%20be%20inserted%20into%20existing%20VLMs%2C%20without%20fine-tuning%20of%20any%20models%20on%20any%20datasets.%20It%20is%20gauranteed%20that%20the%20most%20representative%20vision%20tokens%20are%20kept%20by%20minimizing%20the%20error%20in%20reconstructing%20the%20original%20unpruned%20tokens%20from%20the%20selected%20ones.%20Across%20any%20vision%20pruning%20ratios%2C%20i.e.%2C%20inference%20efficiency%2C%20our%20OC-VTP%20consistently%20helps%20mainstream%20VLMs%20to%20preserve%20the%20highest%20inference%20accuracy.%20Our%20pruning%20also%20demonstrates%20interesting%20interpretability.%20Our%20codes%20are%20available%20at%20https%3A//github.com/GarryLarry010131/OC-VTP.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Centric%2520Vision%2520Token%2520Pruning%2520for%2520Vision%2520Language%2520Models%26entry.906535625%3DGuangyuan%2520Li%2520and%2520Rongzhen%2520Zhao%2520and%2520Jinhong%2520Deng%2520and%2520Yanbo%2520Wang%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3DIn%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520vision%2520tokens%2520are%2520quantity-heavy%2520yet%2520information-dispersed%2520compared%2520with%2520language%2520tokens%252C%2520thus%2520consume%2520too%2520much%2520unnecessary%2520computation.%2520Pruning%2520redundant%2520vision%2520tokens%2520for%2520high%2520VLM%2520inference%2520efficiency%2520has%2520been%2520continuously%2520studied%2520but%2520all%2520existing%2520methods%2520resort%2520to%2520indirect%2520and%2520non-guaranteed%2520ways.%2520We%2520propose%2520OC-VTP%252C%2520a%2520direct%2520and%2520guaranteed%2520approach%2520to%2520select%2520the%2520most%2520representative%2520vision%2520tokens%2520for%2520high-efficiency%2520yet%2520accuracy-preserving%2520VLM%2520inference.%2520Our%2520OC-VTP%2520requires%2520merely%2520light-weight%2520pre-training%2520of%2520a%2520small%2520object-centric%2520vision%2520token%2520pruner%252C%2520which%2520can%2520then%2520be%2520inserted%2520into%2520existing%2520VLMs%252C%2520without%2520fine-tuning%2520of%2520any%2520models%2520on%2520any%2520datasets.%2520It%2520is%2520gauranteed%2520that%2520the%2520most%2520representative%2520vision%2520tokens%2520are%2520kept%2520by%2520minimizing%2520the%2520error%2520in%2520reconstructing%2520the%2520original%2520unpruned%2520tokens%2520from%2520the%2520selected%2520ones.%2520Across%2520any%2520vision%2520pruning%2520ratios%252C%2520i.e.%252C%2520inference%2520efficiency%252C%2520our%2520OC-VTP%2520consistently%2520helps%2520mainstream%2520VLMs%2520to%2520preserve%2520the%2520highest%2520inference%2520accuracy.%2520Our%2520pruning%2520also%2520demonstrates%2520interesting%2520interpretability.%2520Our%2520codes%2520are%2520available%2520at%2520https%253A//github.com/GarryLarry010131/OC-VTP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Centric%20Vision%20Token%20Pruning%20for%20Vision%20Language%20Models&entry.906535625=Guangyuan%20Li%20and%20Rongzhen%20Zhao%20and%20Jinhong%20Deng%20and%20Yanbo%20Wang%20and%20Joni%20Pajarinen&entry.1292438233=In%20Vision%20Language%20Models%20%28VLMs%29%2C%20vision%20tokens%20are%20quantity-heavy%20yet%20information-dispersed%20compared%20with%20language%20tokens%2C%20thus%20consume%20too%20much%20unnecessary%20computation.%20Pruning%20redundant%20vision%20tokens%20for%20high%20VLM%20inference%20efficiency%20has%20been%20continuously%20studied%20but%20all%20existing%20methods%20resort%20to%20indirect%20and%20non-guaranteed%20ways.%20We%20propose%20OC-VTP%2C%20a%20direct%20and%20guaranteed%20approach%20to%20select%20the%20most%20representative%20vision%20tokens%20for%20high-efficiency%20yet%20accuracy-preserving%20VLM%20inference.%20Our%20OC-VTP%20requires%20merely%20light-weight%20pre-training%20of%20a%20small%20object-centric%20vision%20token%20pruner%2C%20which%20can%20then%20be%20inserted%20into%20existing%20VLMs%2C%20without%20fine-tuning%20of%20any%20models%20on%20any%20datasets.%20It%20is%20gauranteed%20that%20the%20most%20representative%20vision%20tokens%20are%20kept%20by%20minimizing%20the%20error%20in%20reconstructing%20the%20original%20unpruned%20tokens%20from%20the%20selected%20ones.%20Across%20any%20vision%20pruning%20ratios%2C%20i.e.%2C%20inference%20efficiency%2C%20our%20OC-VTP%20consistently%20helps%20mainstream%20VLMs%20to%20preserve%20the%20highest%20inference%20accuracy.%20Our%20pruning%20also%20demonstrates%20interesting%20interpretability.%20Our%20codes%20are%20available%20at%20https%3A//github.com/GarryLarry010131/OC-VTP.&entry.1838667208=http%3A//arxiv.org/abs/2511.20439v1&entry.124074799=Read"},
{"title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions", "author": "Bowen Qin and Chen Yue and Fang Yin and Hui Wang and JG Yao and Jiakang Liu and Jing-Shu Zheng and Miguel Hu Chen and Richeng Xuan and Shibei Meng and Shiqi Zhou and Teng Dai and Tong-Shuai Ren and Wei Cui and Xi Yang and Xialin Du and Xiaojing Xu and Xue Sun and Xuejing Li and Yaming Liu and Yesheng Liu and Ying Liu and Yonghua Lin and Yu Zhao and Yunduo Zhang and Yuwen Luo and Zheqi He and Zhiyuan He and Zhongyuan Wang", "abstract": "We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning models (LRMs) with some preliminary findings. We also release ROME, our evaluation benchmark for vision language models intended to test reasoning from visual clues. We attach links to the benchmark, evaluation data, and other updates on this website: https://flageval-baai.github.io/LRM-Eval/", "link": "http://arxiv.org/abs/2509.17177v3", "date": "2025-11-25", "relevancy": 2.6133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlagEval%20Findings%20Report%3A%20A%20Preliminary%20Evaluation%20of%20Large%20Reasoning%20Models%20on%20Automatically%20Verifiable%20Textual%20and%20Visual%20Questions&body=Title%3A%20FlagEval%20Findings%20Report%3A%20A%20Preliminary%20Evaluation%20of%20Large%20Reasoning%20Models%20on%20Automatically%20Verifiable%20Textual%20and%20Visual%20Questions%0AAuthor%3A%20Bowen%20Qin%20and%20Chen%20Yue%20and%20Fang%20Yin%20and%20Hui%20Wang%20and%20JG%20Yao%20and%20Jiakang%20Liu%20and%20Jing-Shu%20Zheng%20and%20Miguel%20Hu%20Chen%20and%20Richeng%20Xuan%20and%20Shibei%20Meng%20and%20Shiqi%20Zhou%20and%20Teng%20Dai%20and%20Tong-Shuai%20Ren%20and%20Wei%20Cui%20and%20Xi%20Yang%20and%20Xialin%20Du%20and%20Xiaojing%20Xu%20and%20Xue%20Sun%20and%20Xuejing%20Li%20and%20Yaming%20Liu%20and%20Yesheng%20Liu%20and%20Ying%20Liu%20and%20Yonghua%20Lin%20and%20Yu%20Zhao%20and%20Yunduo%20Zhang%20and%20Yuwen%20Luo%20and%20Zheqi%20He%20and%20Zhiyuan%20He%20and%20Zhongyuan%20Wang%0AAbstract%3A%20We%20conduct%20a%20moderate-scale%20contamination-free%20%28to%20some%20extent%29%20evaluation%20of%20current%20large%20reasoning%20models%20%28LRMs%29%20with%20some%20preliminary%20findings.%20We%20also%20release%20ROME%2C%20our%20evaluation%20benchmark%20for%20vision%20language%20models%20intended%20to%20test%20reasoning%20from%20visual%20clues.%20We%20attach%20links%20to%20the%20benchmark%2C%20evaluation%20data%2C%20and%20other%20updates%20on%20this%20website%3A%20https%3A//flageval-baai.github.io/LRM-Eval/%0ALink%3A%20http%3A//arxiv.org/abs/2509.17177v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlagEval%2520Findings%2520Report%253A%2520A%2520Preliminary%2520Evaluation%2520of%2520Large%2520Reasoning%2520Models%2520on%2520Automatically%2520Verifiable%2520Textual%2520and%2520Visual%2520Questions%26entry.906535625%3DBowen%2520Qin%2520and%2520Chen%2520Yue%2520and%2520Fang%2520Yin%2520and%2520Hui%2520Wang%2520and%2520JG%2520Yao%2520and%2520Jiakang%2520Liu%2520and%2520Jing-Shu%2520Zheng%2520and%2520Miguel%2520Hu%2520Chen%2520and%2520Richeng%2520Xuan%2520and%2520Shibei%2520Meng%2520and%2520Shiqi%2520Zhou%2520and%2520Teng%2520Dai%2520and%2520Tong-Shuai%2520Ren%2520and%2520Wei%2520Cui%2520and%2520Xi%2520Yang%2520and%2520Xialin%2520Du%2520and%2520Xiaojing%2520Xu%2520and%2520Xue%2520Sun%2520and%2520Xuejing%2520Li%2520and%2520Yaming%2520Liu%2520and%2520Yesheng%2520Liu%2520and%2520Ying%2520Liu%2520and%2520Yonghua%2520Lin%2520and%2520Yu%2520Zhao%2520and%2520Yunduo%2520Zhang%2520and%2520Yuwen%2520Luo%2520and%2520Zheqi%2520He%2520and%2520Zhiyuan%2520He%2520and%2520Zhongyuan%2520Wang%26entry.1292438233%3DWe%2520conduct%2520a%2520moderate-scale%2520contamination-free%2520%2528to%2520some%2520extent%2529%2520evaluation%2520of%2520current%2520large%2520reasoning%2520models%2520%2528LRMs%2529%2520with%2520some%2520preliminary%2520findings.%2520We%2520also%2520release%2520ROME%252C%2520our%2520evaluation%2520benchmark%2520for%2520vision%2520language%2520models%2520intended%2520to%2520test%2520reasoning%2520from%2520visual%2520clues.%2520We%2520attach%2520links%2520to%2520the%2520benchmark%252C%2520evaluation%2520data%252C%2520and%2520other%2520updates%2520on%2520this%2520website%253A%2520https%253A//flageval-baai.github.io/LRM-Eval/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17177v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlagEval%20Findings%20Report%3A%20A%20Preliminary%20Evaluation%20of%20Large%20Reasoning%20Models%20on%20Automatically%20Verifiable%20Textual%20and%20Visual%20Questions&entry.906535625=Bowen%20Qin%20and%20Chen%20Yue%20and%20Fang%20Yin%20and%20Hui%20Wang%20and%20JG%20Yao%20and%20Jiakang%20Liu%20and%20Jing-Shu%20Zheng%20and%20Miguel%20Hu%20Chen%20and%20Richeng%20Xuan%20and%20Shibei%20Meng%20and%20Shiqi%20Zhou%20and%20Teng%20Dai%20and%20Tong-Shuai%20Ren%20and%20Wei%20Cui%20and%20Xi%20Yang%20and%20Xialin%20Du%20and%20Xiaojing%20Xu%20and%20Xue%20Sun%20and%20Xuejing%20Li%20and%20Yaming%20Liu%20and%20Yesheng%20Liu%20and%20Ying%20Liu%20and%20Yonghua%20Lin%20and%20Yu%20Zhao%20and%20Yunduo%20Zhang%20and%20Yuwen%20Luo%20and%20Zheqi%20He%20and%20Zhiyuan%20He%20and%20Zhongyuan%20Wang&entry.1292438233=We%20conduct%20a%20moderate-scale%20contamination-free%20%28to%20some%20extent%29%20evaluation%20of%20current%20large%20reasoning%20models%20%28LRMs%29%20with%20some%20preliminary%20findings.%20We%20also%20release%20ROME%2C%20our%20evaluation%20benchmark%20for%20vision%20language%20models%20intended%20to%20test%20reasoning%20from%20visual%20clues.%20We%20attach%20links%20to%20the%20benchmark%2C%20evaluation%20data%2C%20and%20other%20updates%20on%20this%20website%3A%20https%3A//flageval-baai.github.io/LRM-Eval/&entry.1838667208=http%3A//arxiv.org/abs/2509.17177v3&entry.124074799=Read"},
{"title": "FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization", "author": "Yuto Suzuki and Paul Awolade and Daniel V. LaBarbera and Farnoush Banaei-Kashani", "abstract": "Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a \"vocabulary selection\" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.", "link": "http://arxiv.org/abs/2511.20510v1", "date": "2025-11-25", "relevancy": 2.6003, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5325}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.516}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRAGMENTA%3A%20End-to-end%20Fragmentation-based%20Generative%20Model%20with%20Agentic%20Tuning%20for%20Drug%20Lead%20Optimization&body=Title%3A%20FRAGMENTA%3A%20End-to-end%20Fragmentation-based%20Generative%20Model%20with%20Agentic%20Tuning%20for%20Drug%20Lead%20Optimization%0AAuthor%3A%20Yuto%20Suzuki%20and%20Paul%20Awolade%20and%20Daniel%20V.%20LaBarbera%20and%20Farnoush%20Banaei-Kashani%0AAbstract%3A%20Molecule%20generation%20using%20generative%20AI%20is%20vital%20for%20drug%20discovery%2C%20yet%20class-specific%20datasets%20often%20contain%20fewer%20than%20100%20training%20examples.%20While%20fragment-based%20models%20handle%20limited%20data%20better%20than%20atom-based%20approaches%2C%20existing%20heuristic%20fragmentation%20limits%20diversity%20and%20misses%20key%20fragments.%20Additionally%2C%20model%20tuning%20typically%20requires%20slow%2C%20indirect%20collaboration%20between%20medicinal%20chemists%20and%20AI%20engineers.%20We%20introduce%20FRAGMENTA%2C%20an%20end-to-end%20framework%20for%20drug%20lead%20optimization%20comprising%3A%201%29%20a%20novel%20generative%20model%20that%20reframes%20fragmentation%20as%20a%20%22vocabulary%20selection%22%20problem%2C%20using%20dynamic%20Q-learning%20to%20jointly%20optimize%20fragmentation%20and%20generation%3B%20and%202%29%20an%20agentic%20AI%20system%20that%20refines%20objectives%20via%20conversational%20feedback%20from%20domain%20experts.%20This%20system%20removes%20the%20AI%20engineer%20from%20the%20loop%20and%20progressively%20learns%20domain%20knowledge%20to%20eventually%20automate%20tuning.%20In%20real-world%20cancer%20drug%20discovery%20experiments%2C%20FRAGMENTA%27s%20Human-Agent%20configuration%20identified%20nearly%20twice%20as%20many%20high-scoring%20molecules%20as%20baselines.%20Furthermore%2C%20the%20fully%20autonomous%20Agent-Agent%20system%20outperformed%20traditional%20Human-Human%20tuning%2C%20demonstrating%20the%20efficacy%20of%20agentic%20tuning%20in%20capturing%20expert%20intent.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRAGMENTA%253A%2520End-to-end%2520Fragmentation-based%2520Generative%2520Model%2520with%2520Agentic%2520Tuning%2520for%2520Drug%2520Lead%2520Optimization%26entry.906535625%3DYuto%2520Suzuki%2520and%2520Paul%2520Awolade%2520and%2520Daniel%2520V.%2520LaBarbera%2520and%2520Farnoush%2520Banaei-Kashani%26entry.1292438233%3DMolecule%2520generation%2520using%2520generative%2520AI%2520is%2520vital%2520for%2520drug%2520discovery%252C%2520yet%2520class-specific%2520datasets%2520often%2520contain%2520fewer%2520than%2520100%2520training%2520examples.%2520While%2520fragment-based%2520models%2520handle%2520limited%2520data%2520better%2520than%2520atom-based%2520approaches%252C%2520existing%2520heuristic%2520fragmentation%2520limits%2520diversity%2520and%2520misses%2520key%2520fragments.%2520Additionally%252C%2520model%2520tuning%2520typically%2520requires%2520slow%252C%2520indirect%2520collaboration%2520between%2520medicinal%2520chemists%2520and%2520AI%2520engineers.%2520We%2520introduce%2520FRAGMENTA%252C%2520an%2520end-to-end%2520framework%2520for%2520drug%2520lead%2520optimization%2520comprising%253A%25201%2529%2520a%2520novel%2520generative%2520model%2520that%2520reframes%2520fragmentation%2520as%2520a%2520%2522vocabulary%2520selection%2522%2520problem%252C%2520using%2520dynamic%2520Q-learning%2520to%2520jointly%2520optimize%2520fragmentation%2520and%2520generation%253B%2520and%25202%2529%2520an%2520agentic%2520AI%2520system%2520that%2520refines%2520objectives%2520via%2520conversational%2520feedback%2520from%2520domain%2520experts.%2520This%2520system%2520removes%2520the%2520AI%2520engineer%2520from%2520the%2520loop%2520and%2520progressively%2520learns%2520domain%2520knowledge%2520to%2520eventually%2520automate%2520tuning.%2520In%2520real-world%2520cancer%2520drug%2520discovery%2520experiments%252C%2520FRAGMENTA%2527s%2520Human-Agent%2520configuration%2520identified%2520nearly%2520twice%2520as%2520many%2520high-scoring%2520molecules%2520as%2520baselines.%2520Furthermore%252C%2520the%2520fully%2520autonomous%2520Agent-Agent%2520system%2520outperformed%2520traditional%2520Human-Human%2520tuning%252C%2520demonstrating%2520the%2520efficacy%2520of%2520agentic%2520tuning%2520in%2520capturing%2520expert%2520intent.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRAGMENTA%3A%20End-to-end%20Fragmentation-based%20Generative%20Model%20with%20Agentic%20Tuning%20for%20Drug%20Lead%20Optimization&entry.906535625=Yuto%20Suzuki%20and%20Paul%20Awolade%20and%20Daniel%20V.%20LaBarbera%20and%20Farnoush%20Banaei-Kashani&entry.1292438233=Molecule%20generation%20using%20generative%20AI%20is%20vital%20for%20drug%20discovery%2C%20yet%20class-specific%20datasets%20often%20contain%20fewer%20than%20100%20training%20examples.%20While%20fragment-based%20models%20handle%20limited%20data%20better%20than%20atom-based%20approaches%2C%20existing%20heuristic%20fragmentation%20limits%20diversity%20and%20misses%20key%20fragments.%20Additionally%2C%20model%20tuning%20typically%20requires%20slow%2C%20indirect%20collaboration%20between%20medicinal%20chemists%20and%20AI%20engineers.%20We%20introduce%20FRAGMENTA%2C%20an%20end-to-end%20framework%20for%20drug%20lead%20optimization%20comprising%3A%201%29%20a%20novel%20generative%20model%20that%20reframes%20fragmentation%20as%20a%20%22vocabulary%20selection%22%20problem%2C%20using%20dynamic%20Q-learning%20to%20jointly%20optimize%20fragmentation%20and%20generation%3B%20and%202%29%20an%20agentic%20AI%20system%20that%20refines%20objectives%20via%20conversational%20feedback%20from%20domain%20experts.%20This%20system%20removes%20the%20AI%20engineer%20from%20the%20loop%20and%20progressively%20learns%20domain%20knowledge%20to%20eventually%20automate%20tuning.%20In%20real-world%20cancer%20drug%20discovery%20experiments%2C%20FRAGMENTA%27s%20Human-Agent%20configuration%20identified%20nearly%20twice%20as%20many%20high-scoring%20molecules%20as%20baselines.%20Furthermore%2C%20the%20fully%20autonomous%20Agent-Agent%20system%20outperformed%20traditional%20Human-Human%20tuning%2C%20demonstrating%20the%20efficacy%20of%20agentic%20tuning%20in%20capturing%20expert%20intent.&entry.1838667208=http%3A//arxiv.org/abs/2511.20510v1&entry.124074799=Read"},
{"title": "OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation", "author": "Qi Jiang and Xiaolong Qian and Yao Gao and Lei Sun and Kailun Yang and Zhonghua Yi and Wenyong Li and Ming-Hsuan Yang and Luc Van Gool and Kaiwei Wang", "abstract": "Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing optical degradation. To improve data scalability, we expand the design specifications to increase the degradation diversity of the lens source, and we sample a more uniform distribution by quantifying the spatial-variation patterns and severity of optical degradation. In terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe optical degradation, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of degradation priors. Experiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state-of-the-art generalization capacity in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large-scale LensLib. The source code and datasets will be made publicly available at https://github.com/zju-jiangqi/OmniLens2.", "link": "http://arxiv.org/abs/2511.17126v3", "date": "2025-11-25", "relevancy": 2.5912, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniLens%2B%2B%3A%20Blind%20Lens%20Aberration%20Correction%20via%20Large%20LensLib%20Pre-Training%20and%20Latent%20PSF%20Representation&body=Title%3A%20OmniLens%2B%2B%3A%20Blind%20Lens%20Aberration%20Correction%20via%20Large%20LensLib%20Pre-Training%20and%20Latent%20PSF%20Representation%0AAuthor%3A%20Qi%20Jiang%20and%20Xiaolong%20Qian%20and%20Yao%20Gao%20and%20Lei%20Sun%20and%20Kailun%20Yang%20and%20Zhonghua%20Yi%20and%20Wenyong%20Li%20and%20Ming-Hsuan%20Yang%20and%20Luc%20Van%20Gool%20and%20Kaiwei%20Wang%0AAbstract%3A%20Emerging%20deep-learning-based%20lens%20library%20pre-training%20%28LensLib-PT%29%20pipeline%20offers%20a%20new%20avenue%20for%20blind%20lens%20aberration%20correction%20by%20training%20a%20universal%20neural%20network%2C%20demonstrating%20strong%20capability%20in%20handling%20diverse%20unknown%20optical%20degradations.%20This%20work%20proposes%20the%20OmniLens%2B%2B%20framework%2C%20which%20resolves%20two%20challenges%20that%20hinder%20the%20generalization%20ability%20of%20existing%20pipelines%3A%20the%20difficulty%20of%20scaling%20data%20and%20the%20absence%20of%20prior%20guidance%20characterizing%20optical%20degradation.%20To%20improve%20data%20scalability%2C%20we%20expand%20the%20design%20specifications%20to%20increase%20the%20degradation%20diversity%20of%20the%20lens%20source%2C%20and%20we%20sample%20a%20more%20uniform%20distribution%20by%20quantifying%20the%20spatial-variation%20patterns%20and%20severity%20of%20optical%20degradation.%20In%20terms%20of%20model%20design%2C%20to%20leverage%20the%20Point%20Spread%20Functions%20%28PSFs%29%2C%20which%20intuitively%20describe%20optical%20degradation%2C%20as%20guidance%20in%20a%20blind%20paradigm%2C%20we%20propose%20the%20Latent%20PSF%20Representation%20%28LPR%29.%20The%20VQVAE%20framework%20is%20introduced%20to%20learn%20latent%20features%20of%20LensLib%27s%20PSFs%2C%20which%20is%20assisted%20by%20modeling%20the%20optical%20degradation%20process%20to%20constrain%20the%20learning%20of%20degradation%20priors.%20Experiments%20on%20diverse%20aberrations%20of%20real-world%20lenses%20and%20synthetic%20LensLib%20show%20that%20OmniLens%2B%2B%20exhibits%20state-of-the-art%20generalization%20capacity%20in%20blind%20aberration%20correction.%20Beyond%20performance%2C%20the%20AODLibpro%20is%20verified%20as%20a%20scalable%20foundation%20for%20more%20effective%20training%20across%20diverse%20aberrations%2C%20and%20LPR%20can%20further%20tap%20the%20potential%20of%20large-scale%20LensLib.%20The%20source%20code%20and%20datasets%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/zju-jiangqi/OmniLens2.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17126v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniLens%252B%252B%253A%2520Blind%2520Lens%2520Aberration%2520Correction%2520via%2520Large%2520LensLib%2520Pre-Training%2520and%2520Latent%2520PSF%2520Representation%26entry.906535625%3DQi%2520Jiang%2520and%2520Xiaolong%2520Qian%2520and%2520Yao%2520Gao%2520and%2520Lei%2520Sun%2520and%2520Kailun%2520Yang%2520and%2520Zhonghua%2520Yi%2520and%2520Wenyong%2520Li%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Luc%2520Van%2520Gool%2520and%2520Kaiwei%2520Wang%26entry.1292438233%3DEmerging%2520deep-learning-based%2520lens%2520library%2520pre-training%2520%2528LensLib-PT%2529%2520pipeline%2520offers%2520a%2520new%2520avenue%2520for%2520blind%2520lens%2520aberration%2520correction%2520by%2520training%2520a%2520universal%2520neural%2520network%252C%2520demonstrating%2520strong%2520capability%2520in%2520handling%2520diverse%2520unknown%2520optical%2520degradations.%2520This%2520work%2520proposes%2520the%2520OmniLens%252B%252B%2520framework%252C%2520which%2520resolves%2520two%2520challenges%2520that%2520hinder%2520the%2520generalization%2520ability%2520of%2520existing%2520pipelines%253A%2520the%2520difficulty%2520of%2520scaling%2520data%2520and%2520the%2520absence%2520of%2520prior%2520guidance%2520characterizing%2520optical%2520degradation.%2520To%2520improve%2520data%2520scalability%252C%2520we%2520expand%2520the%2520design%2520specifications%2520to%2520increase%2520the%2520degradation%2520diversity%2520of%2520the%2520lens%2520source%252C%2520and%2520we%2520sample%2520a%2520more%2520uniform%2520distribution%2520by%2520quantifying%2520the%2520spatial-variation%2520patterns%2520and%2520severity%2520of%2520optical%2520degradation.%2520In%2520terms%2520of%2520model%2520design%252C%2520to%2520leverage%2520the%2520Point%2520Spread%2520Functions%2520%2528PSFs%2529%252C%2520which%2520intuitively%2520describe%2520optical%2520degradation%252C%2520as%2520guidance%2520in%2520a%2520blind%2520paradigm%252C%2520we%2520propose%2520the%2520Latent%2520PSF%2520Representation%2520%2528LPR%2529.%2520The%2520VQVAE%2520framework%2520is%2520introduced%2520to%2520learn%2520latent%2520features%2520of%2520LensLib%2527s%2520PSFs%252C%2520which%2520is%2520assisted%2520by%2520modeling%2520the%2520optical%2520degradation%2520process%2520to%2520constrain%2520the%2520learning%2520of%2520degradation%2520priors.%2520Experiments%2520on%2520diverse%2520aberrations%2520of%2520real-world%2520lenses%2520and%2520synthetic%2520LensLib%2520show%2520that%2520OmniLens%252B%252B%2520exhibits%2520state-of-the-art%2520generalization%2520capacity%2520in%2520blind%2520aberration%2520correction.%2520Beyond%2520performance%252C%2520the%2520AODLibpro%2520is%2520verified%2520as%2520a%2520scalable%2520foundation%2520for%2520more%2520effective%2520training%2520across%2520diverse%2520aberrations%252C%2520and%2520LPR%2520can%2520further%2520tap%2520the%2520potential%2520of%2520large-scale%2520LensLib.%2520The%2520source%2520code%2520and%2520datasets%2520will%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/zju-jiangqi/OmniLens2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17126v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniLens%2B%2B%3A%20Blind%20Lens%20Aberration%20Correction%20via%20Large%20LensLib%20Pre-Training%20and%20Latent%20PSF%20Representation&entry.906535625=Qi%20Jiang%20and%20Xiaolong%20Qian%20and%20Yao%20Gao%20and%20Lei%20Sun%20and%20Kailun%20Yang%20and%20Zhonghua%20Yi%20and%20Wenyong%20Li%20and%20Ming-Hsuan%20Yang%20and%20Luc%20Van%20Gool%20and%20Kaiwei%20Wang&entry.1292438233=Emerging%20deep-learning-based%20lens%20library%20pre-training%20%28LensLib-PT%29%20pipeline%20offers%20a%20new%20avenue%20for%20blind%20lens%20aberration%20correction%20by%20training%20a%20universal%20neural%20network%2C%20demonstrating%20strong%20capability%20in%20handling%20diverse%20unknown%20optical%20degradations.%20This%20work%20proposes%20the%20OmniLens%2B%2B%20framework%2C%20which%20resolves%20two%20challenges%20that%20hinder%20the%20generalization%20ability%20of%20existing%20pipelines%3A%20the%20difficulty%20of%20scaling%20data%20and%20the%20absence%20of%20prior%20guidance%20characterizing%20optical%20degradation.%20To%20improve%20data%20scalability%2C%20we%20expand%20the%20design%20specifications%20to%20increase%20the%20degradation%20diversity%20of%20the%20lens%20source%2C%20and%20we%20sample%20a%20more%20uniform%20distribution%20by%20quantifying%20the%20spatial-variation%20patterns%20and%20severity%20of%20optical%20degradation.%20In%20terms%20of%20model%20design%2C%20to%20leverage%20the%20Point%20Spread%20Functions%20%28PSFs%29%2C%20which%20intuitively%20describe%20optical%20degradation%2C%20as%20guidance%20in%20a%20blind%20paradigm%2C%20we%20propose%20the%20Latent%20PSF%20Representation%20%28LPR%29.%20The%20VQVAE%20framework%20is%20introduced%20to%20learn%20latent%20features%20of%20LensLib%27s%20PSFs%2C%20which%20is%20assisted%20by%20modeling%20the%20optical%20degradation%20process%20to%20constrain%20the%20learning%20of%20degradation%20priors.%20Experiments%20on%20diverse%20aberrations%20of%20real-world%20lenses%20and%20synthetic%20LensLib%20show%20that%20OmniLens%2B%2B%20exhibits%20state-of-the-art%20generalization%20capacity%20in%20blind%20aberration%20correction.%20Beyond%20performance%2C%20the%20AODLibpro%20is%20verified%20as%20a%20scalable%20foundation%20for%20more%20effective%20training%20across%20diverse%20aberrations%2C%20and%20LPR%20can%20further%20tap%20the%20potential%20of%20large-scale%20LensLib.%20The%20source%20code%20and%20datasets%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/zju-jiangqi/OmniLens2.&entry.1838667208=http%3A//arxiv.org/abs/2511.17126v3&entry.124074799=Read"},
{"title": "AdaCap: An Adaptive Contrastive Approach for Small-Data Neural Networks", "author": "Bruno Belucci and Karim Lounici and Katia Meziani", "abstract": "Neural networks struggle on small tabular datasets, where tree-based models remain dominant. We introduce Adaptive Contrastive Approach (AdaCap), a training scheme that combines a permutation-based contrastive loss with a Tikhonov-based closed-form output mapping. Across 85 real-world regression datasets and multiple architectures, AdaCap yields consistent and statistically significant improvements in the small-sample regime, particularly for residual models. A meta-predictor trained on dataset characteristics (size, skewness, noise) accurately anticipates when AdaCap is beneficial. These results show that AdaCap acts as a targeted regularization mechanism, strengthening neural networks precisely where they are most fragile. All results and code are publicly available at https://github.com/BrunoBelucci/adacap.", "link": "http://arxiv.org/abs/2511.20170v1", "date": "2025-11-25", "relevancy": 2.5883, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5394}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaCap%3A%20An%20Adaptive%20Contrastive%20Approach%20for%20Small-Data%20Neural%20Networks&body=Title%3A%20AdaCap%3A%20An%20Adaptive%20Contrastive%20Approach%20for%20Small-Data%20Neural%20Networks%0AAuthor%3A%20Bruno%20Belucci%20and%20Karim%20Lounici%20and%20Katia%20Meziani%0AAbstract%3A%20Neural%20networks%20struggle%20on%20small%20tabular%20datasets%2C%20where%20tree-based%20models%20remain%20dominant.%20We%20introduce%20Adaptive%20Contrastive%20Approach%20%28AdaCap%29%2C%20a%20training%20scheme%20that%20combines%20a%20permutation-based%20contrastive%20loss%20with%20a%20Tikhonov-based%20closed-form%20output%20mapping.%20Across%2085%20real-world%20regression%20datasets%20and%20multiple%20architectures%2C%20AdaCap%20yields%20consistent%20and%20statistically%20significant%20improvements%20in%20the%20small-sample%20regime%2C%20particularly%20for%20residual%20models.%20A%20meta-predictor%20trained%20on%20dataset%20characteristics%20%28size%2C%20skewness%2C%20noise%29%20accurately%20anticipates%20when%20AdaCap%20is%20beneficial.%20These%20results%20show%20that%20AdaCap%20acts%20as%20a%20targeted%20regularization%20mechanism%2C%20strengthening%20neural%20networks%20precisely%20where%20they%20are%20most%20fragile.%20All%20results%20and%20code%20are%20publicly%20available%20at%20https%3A//github.com/BrunoBelucci/adacap.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaCap%253A%2520An%2520Adaptive%2520Contrastive%2520Approach%2520for%2520Small-Data%2520Neural%2520Networks%26entry.906535625%3DBruno%2520Belucci%2520and%2520Karim%2520Lounici%2520and%2520Katia%2520Meziani%26entry.1292438233%3DNeural%2520networks%2520struggle%2520on%2520small%2520tabular%2520datasets%252C%2520where%2520tree-based%2520models%2520remain%2520dominant.%2520We%2520introduce%2520Adaptive%2520Contrastive%2520Approach%2520%2528AdaCap%2529%252C%2520a%2520training%2520scheme%2520that%2520combines%2520a%2520permutation-based%2520contrastive%2520loss%2520with%2520a%2520Tikhonov-based%2520closed-form%2520output%2520mapping.%2520Across%252085%2520real-world%2520regression%2520datasets%2520and%2520multiple%2520architectures%252C%2520AdaCap%2520yields%2520consistent%2520and%2520statistically%2520significant%2520improvements%2520in%2520the%2520small-sample%2520regime%252C%2520particularly%2520for%2520residual%2520models.%2520A%2520meta-predictor%2520trained%2520on%2520dataset%2520characteristics%2520%2528size%252C%2520skewness%252C%2520noise%2529%2520accurately%2520anticipates%2520when%2520AdaCap%2520is%2520beneficial.%2520These%2520results%2520show%2520that%2520AdaCap%2520acts%2520as%2520a%2520targeted%2520regularization%2520mechanism%252C%2520strengthening%2520neural%2520networks%2520precisely%2520where%2520they%2520are%2520most%2520fragile.%2520All%2520results%2520and%2520code%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/BrunoBelucci/adacap.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaCap%3A%20An%20Adaptive%20Contrastive%20Approach%20for%20Small-Data%20Neural%20Networks&entry.906535625=Bruno%20Belucci%20and%20Karim%20Lounici%20and%20Katia%20Meziani&entry.1292438233=Neural%20networks%20struggle%20on%20small%20tabular%20datasets%2C%20where%20tree-based%20models%20remain%20dominant.%20We%20introduce%20Adaptive%20Contrastive%20Approach%20%28AdaCap%29%2C%20a%20training%20scheme%20that%20combines%20a%20permutation-based%20contrastive%20loss%20with%20a%20Tikhonov-based%20closed-form%20output%20mapping.%20Across%2085%20real-world%20regression%20datasets%20and%20multiple%20architectures%2C%20AdaCap%20yields%20consistent%20and%20statistically%20significant%20improvements%20in%20the%20small-sample%20regime%2C%20particularly%20for%20residual%20models.%20A%20meta-predictor%20trained%20on%20dataset%20characteristics%20%28size%2C%20skewness%2C%20noise%29%20accurately%20anticipates%20when%20AdaCap%20is%20beneficial.%20These%20results%20show%20that%20AdaCap%20acts%20as%20a%20targeted%20regularization%20mechanism%2C%20strengthening%20neural%20networks%20precisely%20where%20they%20are%20most%20fragile.%20All%20results%20and%20code%20are%20publicly%20available%20at%20https%3A//github.com/BrunoBelucci/adacap.&entry.1838667208=http%3A//arxiv.org/abs/2511.20170v1&entry.124074799=Read"},
{"title": "E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems", "author": "Rui Xue and Shichao Zhu and Liang Qin and Guangmou Pan and Yang Song and Tianfu Wu", "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.", "link": "http://arxiv.org/abs/2511.20564v1", "date": "2025-11-25", "relevancy": 2.5696, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5436}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.501}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E2E-GRec%3A%20An%20End-to-End%20Joint%20Training%20Framework%20for%20Graph%20Neural%20Networks%20and%20Recommender%20Systems&body=Title%3A%20E2E-GRec%3A%20An%20End-to-End%20Joint%20Training%20Framework%20for%20Graph%20Neural%20Networks%20and%20Recommender%20Systems%0AAuthor%3A%20Rui%20Xue%20and%20Shichao%20Zhu%20and%20Liang%20Qin%20and%20Guangmou%20Pan%20and%20Yang%20Song%20and%20Tianfu%20Wu%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20tools%20for%20modeling%20graph-structured%20data%20and%20have%20been%20widely%20used%20in%20recommender%20systems%2C%20such%20as%20for%20capturing%20complex%20user-item%20and%20item-item%20relations.%20However%2C%20most%20industrial%20deployments%20adopt%20a%20two-stage%20pipeline%3A%20GNNs%20are%20first%20pre-trained%20offline%20to%20generate%20node%20embeddings%2C%20which%20are%20then%20used%20as%20static%20features%20for%20downstream%20recommender%20systems.%20This%20decoupled%20paradigm%20leads%20to%20two%20key%20limitations%3A%20%281%29%20high%20computational%20overhead%2C%20since%20large-scale%20GNN%20inference%20must%20be%20repeatedly%20executed%20to%20refresh%20embeddings%3B%20and%20%282%29%20lack%20of%20joint%20optimization%2C%20as%20the%20gradient%20from%20the%20recommender%20system%20cannot%20directly%20influence%20the%20GNN%20learning%20process%2C%20causing%20the%20GNN%20to%20be%20suboptimally%20informative%20for%20the%20recommendation%20task.%20In%20this%20paper%2C%20we%20propose%20E2E-GRec%2C%20a%20novel%20end-to-end%20training%20framework%20that%20unifies%20GNN%20training%20with%20the%20recommender%20system.%20Our%20framework%20is%20characterized%20by%20three%20key%20components%3A%20%28i%29%20efficient%20subgraph%20sampling%20from%20a%20large-scale%20cross-domain%20heterogeneous%20graph%20to%20ensure%20training%20scalability%20and%20efficiency%3B%20%28ii%29%20a%20Graph%20Feature%20Auto-Encoder%20%28GFAE%29%20serving%20as%20an%20auxiliary%20self-supervised%20task%20to%20guide%20the%20GNN%20to%20learn%20structurally%20meaningful%20embeddings%3B%20and%20%28iii%29%20a%20two-level%20feature%20fusion%20mechanism%20combined%20with%20Gradnorm-based%20dynamic%20loss%20balancing%2C%20which%20stabilizes%20graph-aware%20multi-task%20end-to-end%20training.%20Extensive%20offline%20evaluations%2C%20online%20A/B%20tests%20%28e.g.%2C%20a%20%2B0.133%25%20relative%20improvement%20in%20stay%20duration%2C%20a%200.3171%25%20reduction%20in%20the%20average%20number%20of%20videos%20a%20user%20skips%29%20on%20large-scale%20production%20data%2C%20together%20with%20theoretical%20analysis%2C%20demonstrate%20that%20E2E-GRec%20consistently%20surpasses%20traditional%20approaches%2C%20yielding%20significant%20gains%20across%20multiple%20recommendation%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE2E-GRec%253A%2520An%2520End-to-End%2520Joint%2520Training%2520Framework%2520for%2520Graph%2520Neural%2520Networks%2520and%2520Recommender%2520Systems%26entry.906535625%3DRui%2520Xue%2520and%2520Shichao%2520Zhu%2520and%2520Liang%2520Qin%2520and%2520Guangmou%2520Pan%2520and%2520Yang%2520Song%2520and%2520Tianfu%2520Wu%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520modeling%2520graph-structured%2520data%2520and%2520have%2520been%2520widely%2520used%2520in%2520recommender%2520systems%252C%2520such%2520as%2520for%2520capturing%2520complex%2520user-item%2520and%2520item-item%2520relations.%2520However%252C%2520most%2520industrial%2520deployments%2520adopt%2520a%2520two-stage%2520pipeline%253A%2520GNNs%2520are%2520first%2520pre-trained%2520offline%2520to%2520generate%2520node%2520embeddings%252C%2520which%2520are%2520then%2520used%2520as%2520static%2520features%2520for%2520downstream%2520recommender%2520systems.%2520This%2520decoupled%2520paradigm%2520leads%2520to%2520two%2520key%2520limitations%253A%2520%25281%2529%2520high%2520computational%2520overhead%252C%2520since%2520large-scale%2520GNN%2520inference%2520must%2520be%2520repeatedly%2520executed%2520to%2520refresh%2520embeddings%253B%2520and%2520%25282%2529%2520lack%2520of%2520joint%2520optimization%252C%2520as%2520the%2520gradient%2520from%2520the%2520recommender%2520system%2520cannot%2520directly%2520influence%2520the%2520GNN%2520learning%2520process%252C%2520causing%2520the%2520GNN%2520to%2520be%2520suboptimally%2520informative%2520for%2520the%2520recommendation%2520task.%2520In%2520this%2520paper%252C%2520we%2520propose%2520E2E-GRec%252C%2520a%2520novel%2520end-to-end%2520training%2520framework%2520that%2520unifies%2520GNN%2520training%2520with%2520the%2520recommender%2520system.%2520Our%2520framework%2520is%2520characterized%2520by%2520three%2520key%2520components%253A%2520%2528i%2529%2520efficient%2520subgraph%2520sampling%2520from%2520a%2520large-scale%2520cross-domain%2520heterogeneous%2520graph%2520to%2520ensure%2520training%2520scalability%2520and%2520efficiency%253B%2520%2528ii%2529%2520a%2520Graph%2520Feature%2520Auto-Encoder%2520%2528GFAE%2529%2520serving%2520as%2520an%2520auxiliary%2520self-supervised%2520task%2520to%2520guide%2520the%2520GNN%2520to%2520learn%2520structurally%2520meaningful%2520embeddings%253B%2520and%2520%2528iii%2529%2520a%2520two-level%2520feature%2520fusion%2520mechanism%2520combined%2520with%2520Gradnorm-based%2520dynamic%2520loss%2520balancing%252C%2520which%2520stabilizes%2520graph-aware%2520multi-task%2520end-to-end%2520training.%2520Extensive%2520offline%2520evaluations%252C%2520online%2520A/B%2520tests%2520%2528e.g.%252C%2520a%2520%252B0.133%2525%2520relative%2520improvement%2520in%2520stay%2520duration%252C%2520a%25200.3171%2525%2520reduction%2520in%2520the%2520average%2520number%2520of%2520videos%2520a%2520user%2520skips%2529%2520on%2520large-scale%2520production%2520data%252C%2520together%2520with%2520theoretical%2520analysis%252C%2520demonstrate%2520that%2520E2E-GRec%2520consistently%2520surpasses%2520traditional%2520approaches%252C%2520yielding%2520significant%2520gains%2520across%2520multiple%2520recommendation%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E2E-GRec%3A%20An%20End-to-End%20Joint%20Training%20Framework%20for%20Graph%20Neural%20Networks%20and%20Recommender%20Systems&entry.906535625=Rui%20Xue%20and%20Shichao%20Zhu%20and%20Liang%20Qin%20and%20Guangmou%20Pan%20and%20Yang%20Song%20and%20Tianfu%20Wu&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20tools%20for%20modeling%20graph-structured%20data%20and%20have%20been%20widely%20used%20in%20recommender%20systems%2C%20such%20as%20for%20capturing%20complex%20user-item%20and%20item-item%20relations.%20However%2C%20most%20industrial%20deployments%20adopt%20a%20two-stage%20pipeline%3A%20GNNs%20are%20first%20pre-trained%20offline%20to%20generate%20node%20embeddings%2C%20which%20are%20then%20used%20as%20static%20features%20for%20downstream%20recommender%20systems.%20This%20decoupled%20paradigm%20leads%20to%20two%20key%20limitations%3A%20%281%29%20high%20computational%20overhead%2C%20since%20large-scale%20GNN%20inference%20must%20be%20repeatedly%20executed%20to%20refresh%20embeddings%3B%20and%20%282%29%20lack%20of%20joint%20optimization%2C%20as%20the%20gradient%20from%20the%20recommender%20system%20cannot%20directly%20influence%20the%20GNN%20learning%20process%2C%20causing%20the%20GNN%20to%20be%20suboptimally%20informative%20for%20the%20recommendation%20task.%20In%20this%20paper%2C%20we%20propose%20E2E-GRec%2C%20a%20novel%20end-to-end%20training%20framework%20that%20unifies%20GNN%20training%20with%20the%20recommender%20system.%20Our%20framework%20is%20characterized%20by%20three%20key%20components%3A%20%28i%29%20efficient%20subgraph%20sampling%20from%20a%20large-scale%20cross-domain%20heterogeneous%20graph%20to%20ensure%20training%20scalability%20and%20efficiency%3B%20%28ii%29%20a%20Graph%20Feature%20Auto-Encoder%20%28GFAE%29%20serving%20as%20an%20auxiliary%20self-supervised%20task%20to%20guide%20the%20GNN%20to%20learn%20structurally%20meaningful%20embeddings%3B%20and%20%28iii%29%20a%20two-level%20feature%20fusion%20mechanism%20combined%20with%20Gradnorm-based%20dynamic%20loss%20balancing%2C%20which%20stabilizes%20graph-aware%20multi-task%20end-to-end%20training.%20Extensive%20offline%20evaluations%2C%20online%20A/B%20tests%20%28e.g.%2C%20a%20%2B0.133%25%20relative%20improvement%20in%20stay%20duration%2C%20a%200.3171%25%20reduction%20in%20the%20average%20number%20of%20videos%20a%20user%20skips%29%20on%20large-scale%20production%20data%2C%20together%20with%20theoretical%20analysis%2C%20demonstrate%20that%20E2E-GRec%20consistently%20surpasses%20traditional%20approaches%2C%20yielding%20significant%20gains%20across%20multiple%20recommendation%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.20564v1&entry.124074799=Read"},
{"title": "Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder", "author": "Juexin Zhang and Qifeng Zhong and Ying Weng and Ke Chen", "abstract": "The significant molecular and pathological heterogeneity of glioblastoma, an aggressive brain tumor, complicates diagnosis and patient stratification. While traditional histopathological assessment remains the standard, deep learning offers a promising path toward objective and automated analysis of whole slide images. For the BraTS-Path 2025 Challenge, we developed a method that fine-tunes a pre-trained Vision Transformer (ViT) encoder with a dedicated classification head on the official training dataset. Our model's performance on the online validation set, evaluated via the Synapse platform, yielded a Matthews Correlation Coefficient (MCC) of 0.7064 and an F1-score of 0.7676. On the final test set, the model achieved an MCC of 0.6509 and an F1-score of 0.5330, which secured our team second place in the BraTS-Pathology 2025 Challenge. Our results establish a solid baseline for ViT-based histopathological analysis, and future efforts will focus on bridging the performance gap observed on the unseen validation data.", "link": "http://arxiv.org/abs/2511.20221v1", "date": "2025-11-25", "relevancy": 2.555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch-Level%20Glioblastoma%20Subregion%20Classification%20with%20a%20Contrastive%20Learning-Based%20Encoder&body=Title%3A%20Patch-Level%20Glioblastoma%20Subregion%20Classification%20with%20a%20Contrastive%20Learning-Based%20Encoder%0AAuthor%3A%20Juexin%20Zhang%20and%20Qifeng%20Zhong%20and%20Ying%20Weng%20and%20Ke%20Chen%0AAbstract%3A%20The%20significant%20molecular%20and%20pathological%20heterogeneity%20of%20glioblastoma%2C%20an%20aggressive%20brain%20tumor%2C%20complicates%20diagnosis%20and%20patient%20stratification.%20While%20traditional%20histopathological%20assessment%20remains%20the%20standard%2C%20deep%20learning%20offers%20a%20promising%20path%20toward%20objective%20and%20automated%20analysis%20of%20whole%20slide%20images.%20For%20the%20BraTS-Path%202025%20Challenge%2C%20we%20developed%20a%20method%20that%20fine-tunes%20a%20pre-trained%20Vision%20Transformer%20%28ViT%29%20encoder%20with%20a%20dedicated%20classification%20head%20on%20the%20official%20training%20dataset.%20Our%20model%27s%20performance%20on%20the%20online%20validation%20set%2C%20evaluated%20via%20the%20Synapse%20platform%2C%20yielded%20a%20Matthews%20Correlation%20Coefficient%20%28MCC%29%20of%200.7064%20and%20an%20F1-score%20of%200.7676.%20On%20the%20final%20test%20set%2C%20the%20model%20achieved%20an%20MCC%20of%200.6509%20and%20an%20F1-score%20of%200.5330%2C%20which%20secured%20our%20team%20second%20place%20in%20the%20BraTS-Pathology%202025%20Challenge.%20Our%20results%20establish%20a%20solid%20baseline%20for%20ViT-based%20histopathological%20analysis%2C%20and%20future%20efforts%20will%20focus%20on%20bridging%20the%20performance%20gap%20observed%20on%20the%20unseen%20validation%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch-Level%2520Glioblastoma%2520Subregion%2520Classification%2520with%2520a%2520Contrastive%2520Learning-Based%2520Encoder%26entry.906535625%3DJuexin%2520Zhang%2520and%2520Qifeng%2520Zhong%2520and%2520Ying%2520Weng%2520and%2520Ke%2520Chen%26entry.1292438233%3DThe%2520significant%2520molecular%2520and%2520pathological%2520heterogeneity%2520of%2520glioblastoma%252C%2520an%2520aggressive%2520brain%2520tumor%252C%2520complicates%2520diagnosis%2520and%2520patient%2520stratification.%2520While%2520traditional%2520histopathological%2520assessment%2520remains%2520the%2520standard%252C%2520deep%2520learning%2520offers%2520a%2520promising%2520path%2520toward%2520objective%2520and%2520automated%2520analysis%2520of%2520whole%2520slide%2520images.%2520For%2520the%2520BraTS-Path%25202025%2520Challenge%252C%2520we%2520developed%2520a%2520method%2520that%2520fine-tunes%2520a%2520pre-trained%2520Vision%2520Transformer%2520%2528ViT%2529%2520encoder%2520with%2520a%2520dedicated%2520classification%2520head%2520on%2520the%2520official%2520training%2520dataset.%2520Our%2520model%2527s%2520performance%2520on%2520the%2520online%2520validation%2520set%252C%2520evaluated%2520via%2520the%2520Synapse%2520platform%252C%2520yielded%2520a%2520Matthews%2520Correlation%2520Coefficient%2520%2528MCC%2529%2520of%25200.7064%2520and%2520an%2520F1-score%2520of%25200.7676.%2520On%2520the%2520final%2520test%2520set%252C%2520the%2520model%2520achieved%2520an%2520MCC%2520of%25200.6509%2520and%2520an%2520F1-score%2520of%25200.5330%252C%2520which%2520secured%2520our%2520team%2520second%2520place%2520in%2520the%2520BraTS-Pathology%25202025%2520Challenge.%2520Our%2520results%2520establish%2520a%2520solid%2520baseline%2520for%2520ViT-based%2520histopathological%2520analysis%252C%2520and%2520future%2520efforts%2520will%2520focus%2520on%2520bridging%2520the%2520performance%2520gap%2520observed%2520on%2520the%2520unseen%2520validation%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch-Level%20Glioblastoma%20Subregion%20Classification%20with%20a%20Contrastive%20Learning-Based%20Encoder&entry.906535625=Juexin%20Zhang%20and%20Qifeng%20Zhong%20and%20Ying%20Weng%20and%20Ke%20Chen&entry.1292438233=The%20significant%20molecular%20and%20pathological%20heterogeneity%20of%20glioblastoma%2C%20an%20aggressive%20brain%20tumor%2C%20complicates%20diagnosis%20and%20patient%20stratification.%20While%20traditional%20histopathological%20assessment%20remains%20the%20standard%2C%20deep%20learning%20offers%20a%20promising%20path%20toward%20objective%20and%20automated%20analysis%20of%20whole%20slide%20images.%20For%20the%20BraTS-Path%202025%20Challenge%2C%20we%20developed%20a%20method%20that%20fine-tunes%20a%20pre-trained%20Vision%20Transformer%20%28ViT%29%20encoder%20with%20a%20dedicated%20classification%20head%20on%20the%20official%20training%20dataset.%20Our%20model%27s%20performance%20on%20the%20online%20validation%20set%2C%20evaluated%20via%20the%20Synapse%20platform%2C%20yielded%20a%20Matthews%20Correlation%20Coefficient%20%28MCC%29%20of%200.7064%20and%20an%20F1-score%20of%200.7676.%20On%20the%20final%20test%20set%2C%20the%20model%20achieved%20an%20MCC%20of%200.6509%20and%20an%20F1-score%20of%200.5330%2C%20which%20secured%20our%20team%20second%20place%20in%20the%20BraTS-Pathology%202025%20Challenge.%20Our%20results%20establish%20a%20solid%20baseline%20for%20ViT-based%20histopathological%20analysis%2C%20and%20future%20efforts%20will%20focus%20on%20bridging%20the%20performance%20gap%20observed%20on%20the%20unseen%20validation%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.20221v1&entry.124074799=Read"},
{"title": "Geometry of Decision Making in Language Models", "author": "Abhinav Joshi and Divyanshu Bhatt and Ashutosh Modi", "abstract": "Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \\textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.", "link": "http://arxiv.org/abs/2511.20315v1", "date": "2025-11-25", "relevancy": 2.5502, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20of%20Decision%20Making%20in%20Language%20Models&body=Title%3A%20Geometry%20of%20Decision%20Making%20in%20Language%20Models%0AAuthor%3A%20Abhinav%20Joshi%20and%20Divyanshu%20Bhatt%20and%20Ashutosh%20Modi%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20show%20strong%20generalization%20across%20diverse%20tasks%2C%20yet%20the%20internal%20decision-making%20processes%20behind%20their%20predictions%20remain%20opaque.%20In%20this%20work%2C%20we%20study%20the%20geometry%20of%20hidden%20representations%20in%20LLMs%20through%20the%20lens%20of%20%5Ctextit%7Bintrinsic%20dimension%7D%20%28ID%29%2C%20focusing%20specifically%20on%20decision-making%20dynamics%20in%20a%20multiple-choice%20question%20answering%20%28MCQA%29%20setting.%20We%20perform%20a%20large-scale%20study%2C%20with%2028%20open-weight%20transformer%20models%20and%20estimate%20ID%20across%20layers%20using%20multiple%20estimators%2C%20while%20also%20quantifying%20per-layer%20performance%20on%20MCQA%20tasks.%20Our%20findings%20reveal%20a%20consistent%20ID%20pattern%20across%20models%3A%20early%20layers%20operate%20on%20low-dimensional%20manifolds%2C%20middle%20layers%20expand%20this%20space%2C%20and%20later%20layers%20compress%20it%20again%2C%20converging%20to%20decision-relevant%20representations.%20Together%2C%20these%20results%20suggest%20LLMs%20implicitly%20learn%20to%20project%20linguistic%20inputs%20onto%20structured%2C%20low-dimensional%20manifolds%20aligned%20with%20task-specific%20decisions%2C%20providing%20new%20geometric%20insights%20into%20how%20generalization%20and%20reasoning%20emerge%20in%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520of%2520Decision%2520Making%2520in%2520Language%2520Models%26entry.906535625%3DAbhinav%2520Joshi%2520and%2520Divyanshu%2520Bhatt%2520and%2520Ashutosh%2520Modi%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520strong%2520generalization%2520across%2520diverse%2520tasks%252C%2520yet%2520the%2520internal%2520decision-making%2520processes%2520behind%2520their%2520predictions%2520remain%2520opaque.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520geometry%2520of%2520hidden%2520representations%2520in%2520LLMs%2520through%2520the%2520lens%2520of%2520%255Ctextit%257Bintrinsic%2520dimension%257D%2520%2528ID%2529%252C%2520focusing%2520specifically%2520on%2520decision-making%2520dynamics%2520in%2520a%2520multiple-choice%2520question%2520answering%2520%2528MCQA%2529%2520setting.%2520We%2520perform%2520a%2520large-scale%2520study%252C%2520with%252028%2520open-weight%2520transformer%2520models%2520and%2520estimate%2520ID%2520across%2520layers%2520using%2520multiple%2520estimators%252C%2520while%2520also%2520quantifying%2520per-layer%2520performance%2520on%2520MCQA%2520tasks.%2520Our%2520findings%2520reveal%2520a%2520consistent%2520ID%2520pattern%2520across%2520models%253A%2520early%2520layers%2520operate%2520on%2520low-dimensional%2520manifolds%252C%2520middle%2520layers%2520expand%2520this%2520space%252C%2520and%2520later%2520layers%2520compress%2520it%2520again%252C%2520converging%2520to%2520decision-relevant%2520representations.%2520Together%252C%2520these%2520results%2520suggest%2520LLMs%2520implicitly%2520learn%2520to%2520project%2520linguistic%2520inputs%2520onto%2520structured%252C%2520low-dimensional%2520manifolds%2520aligned%2520with%2520task-specific%2520decisions%252C%2520providing%2520new%2520geometric%2520insights%2520into%2520how%2520generalization%2520and%2520reasoning%2520emerge%2520in%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20of%20Decision%20Making%20in%20Language%20Models&entry.906535625=Abhinav%20Joshi%20and%20Divyanshu%20Bhatt%20and%20Ashutosh%20Modi&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20show%20strong%20generalization%20across%20diverse%20tasks%2C%20yet%20the%20internal%20decision-making%20processes%20behind%20their%20predictions%20remain%20opaque.%20In%20this%20work%2C%20we%20study%20the%20geometry%20of%20hidden%20representations%20in%20LLMs%20through%20the%20lens%20of%20%5Ctextit%7Bintrinsic%20dimension%7D%20%28ID%29%2C%20focusing%20specifically%20on%20decision-making%20dynamics%20in%20a%20multiple-choice%20question%20answering%20%28MCQA%29%20setting.%20We%20perform%20a%20large-scale%20study%2C%20with%2028%20open-weight%20transformer%20models%20and%20estimate%20ID%20across%20layers%20using%20multiple%20estimators%2C%20while%20also%20quantifying%20per-layer%20performance%20on%20MCQA%20tasks.%20Our%20findings%20reveal%20a%20consistent%20ID%20pattern%20across%20models%3A%20early%20layers%20operate%20on%20low-dimensional%20manifolds%2C%20middle%20layers%20expand%20this%20space%2C%20and%20later%20layers%20compress%20it%20again%2C%20converging%20to%20decision-relevant%20representations.%20Together%2C%20these%20results%20suggest%20LLMs%20implicitly%20learn%20to%20project%20linguistic%20inputs%20onto%20structured%2C%20low-dimensional%20manifolds%20aligned%20with%20task-specific%20decisions%2C%20providing%20new%20geometric%20insights%20into%20how%20generalization%20and%20reasoning%20emerge%20in%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.20315v1&entry.124074799=Read"},
{"title": "MotionV2V: Editing Motion in a Video", "author": "Ryan Burgert and Charles Herrmann and Forrester Cole and Michael S Ryoo and Neal Wadhwa and Andrey Voynov and Nataniel Ruiz", "abstract": "While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V", "link": "http://arxiv.org/abs/2511.20640v1", "date": "2025-11-25", "relevancy": 2.5295, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7285}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6203}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionV2V%3A%20Editing%20Motion%20in%20a%20Video&body=Title%3A%20MotionV2V%3A%20Editing%20Motion%20in%20a%20Video%0AAuthor%3A%20Ryan%20Burgert%20and%20Charles%20Herrmann%20and%20Forrester%20Cole%20and%20Michael%20S%20Ryoo%20and%20Neal%20Wadhwa%20and%20Andrey%20Voynov%20and%20Nataniel%20Ruiz%0AAbstract%3A%20While%20generative%20video%20models%20have%20achieved%20remarkable%20fidelity%20and%20consistency%2C%20applying%20these%20capabilities%20to%20video%20editing%20remains%20a%20complex%20challenge.%20Recent%20research%20has%20explored%20motion%20controllability%20as%20a%20means%20to%20enhance%20text-to-video%20generation%20or%20image%20animation%3B%20however%2C%20we%20identify%20precise%20motion%20control%20as%20a%20promising%20yet%20under-explored%20paradigm%20for%20editing%20existing%20videos.%20In%20this%20work%2C%20we%20propose%20modifying%20video%20motion%20by%20directly%20editing%20sparse%20trajectories%20extracted%20from%20the%20input.%20We%20term%20the%20deviation%20between%20input%20and%20output%20trajectories%20a%20%22motion%20edit%22%20and%20demonstrate%20that%20this%20representation%2C%20when%20coupled%20with%20a%20generative%20backbone%2C%20enables%20powerful%20video%20editing%20capabilities.%20To%20achieve%20this%2C%20we%20introduce%20a%20pipeline%20for%20generating%20%22motion%20counterfactuals%22%2C%20video%20pairs%20that%20share%20identical%20content%20but%20distinct%20motion%2C%20and%20we%20fine-tune%20a%20motion-conditioned%20video%20diffusion%20architecture%20on%20this%20dataset.%20Our%20approach%20allows%20for%20edits%20that%20start%20at%20any%20timestamp%20and%20propagate%20naturally.%20In%20a%20four-way%20head-to-head%20user%20study%2C%20our%20model%20achieves%20over%2065%20percent%20preference%20against%20prior%20work.%20Please%20see%20our%20project%20page%3A%20https%3A//ryanndagreat.github.io/MotionV2V%0ALink%3A%20http%3A//arxiv.org/abs/2511.20640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionV2V%253A%2520Editing%2520Motion%2520in%2520a%2520Video%26entry.906535625%3DRyan%2520Burgert%2520and%2520Charles%2520Herrmann%2520and%2520Forrester%2520Cole%2520and%2520Michael%2520S%2520Ryoo%2520and%2520Neal%2520Wadhwa%2520and%2520Andrey%2520Voynov%2520and%2520Nataniel%2520Ruiz%26entry.1292438233%3DWhile%2520generative%2520video%2520models%2520have%2520achieved%2520remarkable%2520fidelity%2520and%2520consistency%252C%2520applying%2520these%2520capabilities%2520to%2520video%2520editing%2520remains%2520a%2520complex%2520challenge.%2520Recent%2520research%2520has%2520explored%2520motion%2520controllability%2520as%2520a%2520means%2520to%2520enhance%2520text-to-video%2520generation%2520or%2520image%2520animation%253B%2520however%252C%2520we%2520identify%2520precise%2520motion%2520control%2520as%2520a%2520promising%2520yet%2520under-explored%2520paradigm%2520for%2520editing%2520existing%2520videos.%2520In%2520this%2520work%252C%2520we%2520propose%2520modifying%2520video%2520motion%2520by%2520directly%2520editing%2520sparse%2520trajectories%2520extracted%2520from%2520the%2520input.%2520We%2520term%2520the%2520deviation%2520between%2520input%2520and%2520output%2520trajectories%2520a%2520%2522motion%2520edit%2522%2520and%2520demonstrate%2520that%2520this%2520representation%252C%2520when%2520coupled%2520with%2520a%2520generative%2520backbone%252C%2520enables%2520powerful%2520video%2520editing%2520capabilities.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520pipeline%2520for%2520generating%2520%2522motion%2520counterfactuals%2522%252C%2520video%2520pairs%2520that%2520share%2520identical%2520content%2520but%2520distinct%2520motion%252C%2520and%2520we%2520fine-tune%2520a%2520motion-conditioned%2520video%2520diffusion%2520architecture%2520on%2520this%2520dataset.%2520Our%2520approach%2520allows%2520for%2520edits%2520that%2520start%2520at%2520any%2520timestamp%2520and%2520propagate%2520naturally.%2520In%2520a%2520four-way%2520head-to-head%2520user%2520study%252C%2520our%2520model%2520achieves%2520over%252065%2520percent%2520preference%2520against%2520prior%2520work.%2520Please%2520see%2520our%2520project%2520page%253A%2520https%253A//ryanndagreat.github.io/MotionV2V%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionV2V%3A%20Editing%20Motion%20in%20a%20Video&entry.906535625=Ryan%20Burgert%20and%20Charles%20Herrmann%20and%20Forrester%20Cole%20and%20Michael%20S%20Ryoo%20and%20Neal%20Wadhwa%20and%20Andrey%20Voynov%20and%20Nataniel%20Ruiz&entry.1292438233=While%20generative%20video%20models%20have%20achieved%20remarkable%20fidelity%20and%20consistency%2C%20applying%20these%20capabilities%20to%20video%20editing%20remains%20a%20complex%20challenge.%20Recent%20research%20has%20explored%20motion%20controllability%20as%20a%20means%20to%20enhance%20text-to-video%20generation%20or%20image%20animation%3B%20however%2C%20we%20identify%20precise%20motion%20control%20as%20a%20promising%20yet%20under-explored%20paradigm%20for%20editing%20existing%20videos.%20In%20this%20work%2C%20we%20propose%20modifying%20video%20motion%20by%20directly%20editing%20sparse%20trajectories%20extracted%20from%20the%20input.%20We%20term%20the%20deviation%20between%20input%20and%20output%20trajectories%20a%20%22motion%20edit%22%20and%20demonstrate%20that%20this%20representation%2C%20when%20coupled%20with%20a%20generative%20backbone%2C%20enables%20powerful%20video%20editing%20capabilities.%20To%20achieve%20this%2C%20we%20introduce%20a%20pipeline%20for%20generating%20%22motion%20counterfactuals%22%2C%20video%20pairs%20that%20share%20identical%20content%20but%20distinct%20motion%2C%20and%20we%20fine-tune%20a%20motion-conditioned%20video%20diffusion%20architecture%20on%20this%20dataset.%20Our%20approach%20allows%20for%20edits%20that%20start%20at%20any%20timestamp%20and%20propagate%20naturally.%20In%20a%20four-way%20head-to-head%20user%20study%2C%20our%20model%20achieves%20over%2065%20percent%20preference%20against%20prior%20work.%20Please%20see%20our%20project%20page%3A%20https%3A//ryanndagreat.github.io/MotionV2V&entry.1838667208=http%3A//arxiv.org/abs/2511.20640v1&entry.124074799=Read"},
{"title": "From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection", "author": "Sidahmed Benabderrahmane and Talal Rahwan", "abstract": "Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.", "link": "http://arxiv.org/abs/2511.20500v1", "date": "2025-11-25", "relevancy": 2.5218, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5168}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5037}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20One%20Attack%20Domain%20to%20Another%3A%20Contrastive%20Transfer%20Learning%20with%20Siamese%20Networks%20for%20APT%20Detection&body=Title%3A%20From%20One%20Attack%20Domain%20to%20Another%3A%20Contrastive%20Transfer%20Learning%20with%20Siamese%20Networks%20for%20APT%20Detection%0AAuthor%3A%20Sidahmed%20Benabderrahmane%20and%20Talal%20Rahwan%0AAbstract%3A%20Advanced%20Persistent%20Threats%20%28APT%29%20pose%20a%20major%20cybersecurity%20challenge%20due%20to%20their%20stealth%2C%20persistence%2C%20and%20adaptability.%20Traditional%20machine%20learning%20detectors%20struggle%20with%20class%20imbalance%2C%20high%20dimensional%20features%2C%20and%20scarce%20real%20world%20traces.%20They%20often%20lack%20transferability-performing%20well%20in%20the%20training%20domain%20but%20degrading%20in%20novel%20attack%20scenarios.%20We%20propose%20a%20hybrid%20transfer%20framework%20that%20integrates%20Transfer%20Learning%2C%20Explainable%20AI%20%28XAI%29%2C%20contrastive%20learning%2C%20and%20Siamese%20networks%20to%20improve%20cross-domain%20generalization.%20An%20attention-based%20autoencoder%20supports%20knowledge%20transfer%20across%20domains%2C%20while%20Shapley%20Additive%20exPlanations%20%28SHAP%29%20select%20stable%2C%20informative%20features%20to%20reduce%20dimensionality%20and%20computational%20cost.%20A%20Siamese%20encoder%20trained%20with%20a%20contrastive%20objective%20aligns%20source%20and%20target%20representations%2C%20increasing%20anomaly%20separability%20and%20mitigating%20feature%20drift.%20We%20evaluate%20on%20real-world%20traces%20from%20the%20DARPA%20Transparent%20Computing%20%28TC%29%20program%20and%20augment%20with%20synthetic%20attack%20scenarios%20to%20test%20robustness.%20Across%20source%20to%20target%20transfers%2C%20the%20approach%20delivers%20improved%20detection%20scores%20with%20classical%20and%20deep%20baselines%2C%20demonstrating%20a%20scalable%2C%20explainable%2C%20and%20transferable%20solution%20for%20APT%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520One%2520Attack%2520Domain%2520to%2520Another%253A%2520Contrastive%2520Transfer%2520Learning%2520with%2520Siamese%2520Networks%2520for%2520APT%2520Detection%26entry.906535625%3DSidahmed%2520Benabderrahmane%2520and%2520Talal%2520Rahwan%26entry.1292438233%3DAdvanced%2520Persistent%2520Threats%2520%2528APT%2529%2520pose%2520a%2520major%2520cybersecurity%2520challenge%2520due%2520to%2520their%2520stealth%252C%2520persistence%252C%2520and%2520adaptability.%2520Traditional%2520machine%2520learning%2520detectors%2520struggle%2520with%2520class%2520imbalance%252C%2520high%2520dimensional%2520features%252C%2520and%2520scarce%2520real%2520world%2520traces.%2520They%2520often%2520lack%2520transferability-performing%2520well%2520in%2520the%2520training%2520domain%2520but%2520degrading%2520in%2520novel%2520attack%2520scenarios.%2520We%2520propose%2520a%2520hybrid%2520transfer%2520framework%2520that%2520integrates%2520Transfer%2520Learning%252C%2520Explainable%2520AI%2520%2528XAI%2529%252C%2520contrastive%2520learning%252C%2520and%2520Siamese%2520networks%2520to%2520improve%2520cross-domain%2520generalization.%2520An%2520attention-based%2520autoencoder%2520supports%2520knowledge%2520transfer%2520across%2520domains%252C%2520while%2520Shapley%2520Additive%2520exPlanations%2520%2528SHAP%2529%2520select%2520stable%252C%2520informative%2520features%2520to%2520reduce%2520dimensionality%2520and%2520computational%2520cost.%2520A%2520Siamese%2520encoder%2520trained%2520with%2520a%2520contrastive%2520objective%2520aligns%2520source%2520and%2520target%2520representations%252C%2520increasing%2520anomaly%2520separability%2520and%2520mitigating%2520feature%2520drift.%2520We%2520evaluate%2520on%2520real-world%2520traces%2520from%2520the%2520DARPA%2520Transparent%2520Computing%2520%2528TC%2529%2520program%2520and%2520augment%2520with%2520synthetic%2520attack%2520scenarios%2520to%2520test%2520robustness.%2520Across%2520source%2520to%2520target%2520transfers%252C%2520the%2520approach%2520delivers%2520improved%2520detection%2520scores%2520with%2520classical%2520and%2520deep%2520baselines%252C%2520demonstrating%2520a%2520scalable%252C%2520explainable%252C%2520and%2520transferable%2520solution%2520for%2520APT%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20One%20Attack%20Domain%20to%20Another%3A%20Contrastive%20Transfer%20Learning%20with%20Siamese%20Networks%20for%20APT%20Detection&entry.906535625=Sidahmed%20Benabderrahmane%20and%20Talal%20Rahwan&entry.1292438233=Advanced%20Persistent%20Threats%20%28APT%29%20pose%20a%20major%20cybersecurity%20challenge%20due%20to%20their%20stealth%2C%20persistence%2C%20and%20adaptability.%20Traditional%20machine%20learning%20detectors%20struggle%20with%20class%20imbalance%2C%20high%20dimensional%20features%2C%20and%20scarce%20real%20world%20traces.%20They%20often%20lack%20transferability-performing%20well%20in%20the%20training%20domain%20but%20degrading%20in%20novel%20attack%20scenarios.%20We%20propose%20a%20hybrid%20transfer%20framework%20that%20integrates%20Transfer%20Learning%2C%20Explainable%20AI%20%28XAI%29%2C%20contrastive%20learning%2C%20and%20Siamese%20networks%20to%20improve%20cross-domain%20generalization.%20An%20attention-based%20autoencoder%20supports%20knowledge%20transfer%20across%20domains%2C%20while%20Shapley%20Additive%20exPlanations%20%28SHAP%29%20select%20stable%2C%20informative%20features%20to%20reduce%20dimensionality%20and%20computational%20cost.%20A%20Siamese%20encoder%20trained%20with%20a%20contrastive%20objective%20aligns%20source%20and%20target%20representations%2C%20increasing%20anomaly%20separability%20and%20mitigating%20feature%20drift.%20We%20evaluate%20on%20real-world%20traces%20from%20the%20DARPA%20Transparent%20Computing%20%28TC%29%20program%20and%20augment%20with%20synthetic%20attack%20scenarios%20to%20test%20robustness.%20Across%20source%20to%20target%20transfers%2C%20the%20approach%20delivers%20improved%20detection%20scores%20with%20classical%20and%20deep%20baselines%2C%20demonstrating%20a%20scalable%2C%20explainable%2C%20and%20transferable%20solution%20for%20APT%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2511.20500v1&entry.124074799=Read"},
{"title": "DecNefLab: A Modular and Interpretable Simulation Framework for Decoded Neurofeedback", "author": "Alexander Olza and Roberto Santana and David Soto", "abstract": "Decoded Neurofeedback (DecNef) is a flourishing non-invasive approach to brain modulation with wide-ranging applications in neuromedicine and cognitive neuroscience. However, progress in DecNef research remains constrained by subject-dependent learning variability, reliance on indirect measures to quantify progress, and the high cost and time demands of experimentation.\n  We present DecNefLab, a modular and interpretable simulation framework that formalizes DecNef as a machine learning problem. Beyond providing a virtual laboratory, DecNefLab enables researchers to model, analyze and understand neurofeedback dynamics. Using latent variable generative models as simulated participants, DecNefLab allows direct observation of internal cognitive states and systematic evaluation of how different protocol designs and subject characteristics influence learning.\n  We demonstrate how this approach can (i) reproduce empirical phenomena of DecNef learning, (ii) identify conditions under which DecNef feedback fails to induce learning, and (iii) guide the design of more robust and reliable DecNef protocols in silico before human implementation.\n  In summary, DecNefLab bridges computational modeling and cognitive neuroscience, offering a principled foundation for methodological innovation, robust protocol design, and ultimately, a deeper understanding of DecNef-based brain modulation.", "link": "http://arxiv.org/abs/2511.14555v2", "date": "2025-11-25", "relevancy": 2.5178, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DecNefLab%3A%20A%20Modular%20and%20Interpretable%20Simulation%20Framework%20for%20Decoded%20Neurofeedback&body=Title%3A%20DecNefLab%3A%20A%20Modular%20and%20Interpretable%20Simulation%20Framework%20for%20Decoded%20Neurofeedback%0AAuthor%3A%20Alexander%20Olza%20and%20Roberto%20Santana%20and%20David%20Soto%0AAbstract%3A%20Decoded%20Neurofeedback%20%28DecNef%29%20is%20a%20flourishing%20non-invasive%20approach%20to%20brain%20modulation%20with%20wide-ranging%20applications%20in%20neuromedicine%20and%20cognitive%20neuroscience.%20However%2C%20progress%20in%20DecNef%20research%20remains%20constrained%20by%20subject-dependent%20learning%20variability%2C%20reliance%20on%20indirect%20measures%20to%20quantify%20progress%2C%20and%20the%20high%20cost%20and%20time%20demands%20of%20experimentation.%0A%20%20We%20present%20DecNefLab%2C%20a%20modular%20and%20interpretable%20simulation%20framework%20that%20formalizes%20DecNef%20as%20a%20machine%20learning%20problem.%20Beyond%20providing%20a%20virtual%20laboratory%2C%20DecNefLab%20enables%20researchers%20to%20model%2C%20analyze%20and%20understand%20neurofeedback%20dynamics.%20Using%20latent%20variable%20generative%20models%20as%20simulated%20participants%2C%20DecNefLab%20allows%20direct%20observation%20of%20internal%20cognitive%20states%20and%20systematic%20evaluation%20of%20how%20different%20protocol%20designs%20and%20subject%20characteristics%20influence%20learning.%0A%20%20We%20demonstrate%20how%20this%20approach%20can%20%28i%29%20reproduce%20empirical%20phenomena%20of%20DecNef%20learning%2C%20%28ii%29%20identify%20conditions%20under%20which%20DecNef%20feedback%20fails%20to%20induce%20learning%2C%20and%20%28iii%29%20guide%20the%20design%20of%20more%20robust%20and%20reliable%20DecNef%20protocols%20in%20silico%20before%20human%20implementation.%0A%20%20In%20summary%2C%20DecNefLab%20bridges%20computational%20modeling%20and%20cognitive%20neuroscience%2C%20offering%20a%20principled%20foundation%20for%20methodological%20innovation%2C%20robust%20protocol%20design%2C%20and%20ultimately%2C%20a%20deeper%20understanding%20of%20DecNef-based%20brain%20modulation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecNefLab%253A%2520A%2520Modular%2520and%2520Interpretable%2520Simulation%2520Framework%2520for%2520Decoded%2520Neurofeedback%26entry.906535625%3DAlexander%2520Olza%2520and%2520Roberto%2520Santana%2520and%2520David%2520Soto%26entry.1292438233%3DDecoded%2520Neurofeedback%2520%2528DecNef%2529%2520is%2520a%2520flourishing%2520non-invasive%2520approach%2520to%2520brain%2520modulation%2520with%2520wide-ranging%2520applications%2520in%2520neuromedicine%2520and%2520cognitive%2520neuroscience.%2520However%252C%2520progress%2520in%2520DecNef%2520research%2520remains%2520constrained%2520by%2520subject-dependent%2520learning%2520variability%252C%2520reliance%2520on%2520indirect%2520measures%2520to%2520quantify%2520progress%252C%2520and%2520the%2520high%2520cost%2520and%2520time%2520demands%2520of%2520experimentation.%250A%2520%2520We%2520present%2520DecNefLab%252C%2520a%2520modular%2520and%2520interpretable%2520simulation%2520framework%2520that%2520formalizes%2520DecNef%2520as%2520a%2520machine%2520learning%2520problem.%2520Beyond%2520providing%2520a%2520virtual%2520laboratory%252C%2520DecNefLab%2520enables%2520researchers%2520to%2520model%252C%2520analyze%2520and%2520understand%2520neurofeedback%2520dynamics.%2520Using%2520latent%2520variable%2520generative%2520models%2520as%2520simulated%2520participants%252C%2520DecNefLab%2520allows%2520direct%2520observation%2520of%2520internal%2520cognitive%2520states%2520and%2520systematic%2520evaluation%2520of%2520how%2520different%2520protocol%2520designs%2520and%2520subject%2520characteristics%2520influence%2520learning.%250A%2520%2520We%2520demonstrate%2520how%2520this%2520approach%2520can%2520%2528i%2529%2520reproduce%2520empirical%2520phenomena%2520of%2520DecNef%2520learning%252C%2520%2528ii%2529%2520identify%2520conditions%2520under%2520which%2520DecNef%2520feedback%2520fails%2520to%2520induce%2520learning%252C%2520and%2520%2528iii%2529%2520guide%2520the%2520design%2520of%2520more%2520robust%2520and%2520reliable%2520DecNef%2520protocols%2520in%2520silico%2520before%2520human%2520implementation.%250A%2520%2520In%2520summary%252C%2520DecNefLab%2520bridges%2520computational%2520modeling%2520and%2520cognitive%2520neuroscience%252C%2520offering%2520a%2520principled%2520foundation%2520for%2520methodological%2520innovation%252C%2520robust%2520protocol%2520design%252C%2520and%2520ultimately%252C%2520a%2520deeper%2520understanding%2520of%2520DecNef-based%2520brain%2520modulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DecNefLab%3A%20A%20Modular%20and%20Interpretable%20Simulation%20Framework%20for%20Decoded%20Neurofeedback&entry.906535625=Alexander%20Olza%20and%20Roberto%20Santana%20and%20David%20Soto&entry.1292438233=Decoded%20Neurofeedback%20%28DecNef%29%20is%20a%20flourishing%20non-invasive%20approach%20to%20brain%20modulation%20with%20wide-ranging%20applications%20in%20neuromedicine%20and%20cognitive%20neuroscience.%20However%2C%20progress%20in%20DecNef%20research%20remains%20constrained%20by%20subject-dependent%20learning%20variability%2C%20reliance%20on%20indirect%20measures%20to%20quantify%20progress%2C%20and%20the%20high%20cost%20and%20time%20demands%20of%20experimentation.%0A%20%20We%20present%20DecNefLab%2C%20a%20modular%20and%20interpretable%20simulation%20framework%20that%20formalizes%20DecNef%20as%20a%20machine%20learning%20problem.%20Beyond%20providing%20a%20virtual%20laboratory%2C%20DecNefLab%20enables%20researchers%20to%20model%2C%20analyze%20and%20understand%20neurofeedback%20dynamics.%20Using%20latent%20variable%20generative%20models%20as%20simulated%20participants%2C%20DecNefLab%20allows%20direct%20observation%20of%20internal%20cognitive%20states%20and%20systematic%20evaluation%20of%20how%20different%20protocol%20designs%20and%20subject%20characteristics%20influence%20learning.%0A%20%20We%20demonstrate%20how%20this%20approach%20can%20%28i%29%20reproduce%20empirical%20phenomena%20of%20DecNef%20learning%2C%20%28ii%29%20identify%20conditions%20under%20which%20DecNef%20feedback%20fails%20to%20induce%20learning%2C%20and%20%28iii%29%20guide%20the%20design%20of%20more%20robust%20and%20reliable%20DecNef%20protocols%20in%20silico%20before%20human%20implementation.%0A%20%20In%20summary%2C%20DecNefLab%20bridges%20computational%20modeling%20and%20cognitive%20neuroscience%2C%20offering%20a%20principled%20foundation%20for%20methodological%20innovation%2C%20robust%20protocol%20design%2C%20and%20ultimately%2C%20a%20deeper%20understanding%20of%20DecNef-based%20brain%20modulation.&entry.1838667208=http%3A//arxiv.org/abs/2511.14555v2&entry.124074799=Read"},
{"title": "Searching Latent Program Spaces", "author": "Matthew V Macfarlane and Clement Bonnet", "abstract": "General intelligence requires systems that acquire new skills efficiently and generalize beyond their training distributions. Although program synthesis approaches have strong generalization power, they face scaling issues due to the large combinatorial spaces that quickly render them impractical, requiring human-generated DSLs or pre-trained priors to narrow this search space. On the other hand, deep learning methods have had high successes, but they lack structured test-time adaptation and rely on heavy stochastic sampling or expensive gradient updates for fine-tuning. In this work, we propose the Latent Program Network (LPN), a novel architecture that builds in test-time search directly into neural models. LPN learns a latent space of implicit programs -- neurally mapping inputs to outputs -- through which it can search using gradients at test time. LPN combines the adaptability of symbolic approaches and the scalability of neural methods. It searches through a compact latent space at test time and bypasses the need for pre-defined domain-specific languages. On a range of programming-by-examples tasks, LPN either outperforms or matches performance compared to in-context learning and test-time training methods. Tested on the ARC-AGI benchmark, we demonstrate that LPN can both learn a compact program space and search through it at test time to adapt to novel tasks. LPN doubles its performance on out-of-distribution tasks when test-time search is switched on.", "link": "http://arxiv.org/abs/2411.08706v3", "date": "2025-11-25", "relevancy": 2.5174, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Searching%20Latent%20Program%20Spaces&body=Title%3A%20Searching%20Latent%20Program%20Spaces%0AAuthor%3A%20Matthew%20V%20Macfarlane%20and%20Clement%20Bonnet%0AAbstract%3A%20General%20intelligence%20requires%20systems%20that%20acquire%20new%20skills%20efficiently%20and%20generalize%20beyond%20their%20training%20distributions.%20Although%20program%20synthesis%20approaches%20have%20strong%20generalization%20power%2C%20they%20face%20scaling%20issues%20due%20to%20the%20large%20combinatorial%20spaces%20that%20quickly%20render%20them%20impractical%2C%20requiring%20human-generated%20DSLs%20or%20pre-trained%20priors%20to%20narrow%20this%20search%20space.%20On%20the%20other%20hand%2C%20deep%20learning%20methods%20have%20had%20high%20successes%2C%20but%20they%20lack%20structured%20test-time%20adaptation%20and%20rely%20on%20heavy%20stochastic%20sampling%20or%20expensive%20gradient%20updates%20for%20fine-tuning.%20In%20this%20work%2C%20we%20propose%20the%20Latent%20Program%20Network%20%28LPN%29%2C%20a%20novel%20architecture%20that%20builds%20in%20test-time%20search%20directly%20into%20neural%20models.%20LPN%20learns%20a%20latent%20space%20of%20implicit%20programs%20--%20neurally%20mapping%20inputs%20to%20outputs%20--%20through%20which%20it%20can%20search%20using%20gradients%20at%20test%20time.%20LPN%20combines%20the%20adaptability%20of%20symbolic%20approaches%20and%20the%20scalability%20of%20neural%20methods.%20It%20searches%20through%20a%20compact%20latent%20space%20at%20test%20time%20and%20bypasses%20the%20need%20for%20pre-defined%20domain-specific%20languages.%20On%20a%20range%20of%20programming-by-examples%20tasks%2C%20LPN%20either%20outperforms%20or%20matches%20performance%20compared%20to%20in-context%20learning%20and%20test-time%20training%20methods.%20Tested%20on%20the%20ARC-AGI%20benchmark%2C%20we%20demonstrate%20that%20LPN%20can%20both%20learn%20a%20compact%20program%20space%20and%20search%20through%20it%20at%20test%20time%20to%20adapt%20to%20novel%20tasks.%20LPN%20doubles%20its%20performance%20on%20out-of-distribution%20tasks%20when%20test-time%20search%20is%20switched%20on.%0ALink%3A%20http%3A//arxiv.org/abs/2411.08706v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearching%2520Latent%2520Program%2520Spaces%26entry.906535625%3DMatthew%2520V%2520Macfarlane%2520and%2520Clement%2520Bonnet%26entry.1292438233%3DGeneral%2520intelligence%2520requires%2520systems%2520that%2520acquire%2520new%2520skills%2520efficiently%2520and%2520generalize%2520beyond%2520their%2520training%2520distributions.%2520Although%2520program%2520synthesis%2520approaches%2520have%2520strong%2520generalization%2520power%252C%2520they%2520face%2520scaling%2520issues%2520due%2520to%2520the%2520large%2520combinatorial%2520spaces%2520that%2520quickly%2520render%2520them%2520impractical%252C%2520requiring%2520human-generated%2520DSLs%2520or%2520pre-trained%2520priors%2520to%2520narrow%2520this%2520search%2520space.%2520On%2520the%2520other%2520hand%252C%2520deep%2520learning%2520methods%2520have%2520had%2520high%2520successes%252C%2520but%2520they%2520lack%2520structured%2520test-time%2520adaptation%2520and%2520rely%2520on%2520heavy%2520stochastic%2520sampling%2520or%2520expensive%2520gradient%2520updates%2520for%2520fine-tuning.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Latent%2520Program%2520Network%2520%2528LPN%2529%252C%2520a%2520novel%2520architecture%2520that%2520builds%2520in%2520test-time%2520search%2520directly%2520into%2520neural%2520models.%2520LPN%2520learns%2520a%2520latent%2520space%2520of%2520implicit%2520programs%2520--%2520neurally%2520mapping%2520inputs%2520to%2520outputs%2520--%2520through%2520which%2520it%2520can%2520search%2520using%2520gradients%2520at%2520test%2520time.%2520LPN%2520combines%2520the%2520adaptability%2520of%2520symbolic%2520approaches%2520and%2520the%2520scalability%2520of%2520neural%2520methods.%2520It%2520searches%2520through%2520a%2520compact%2520latent%2520space%2520at%2520test%2520time%2520and%2520bypasses%2520the%2520need%2520for%2520pre-defined%2520domain-specific%2520languages.%2520On%2520a%2520range%2520of%2520programming-by-examples%2520tasks%252C%2520LPN%2520either%2520outperforms%2520or%2520matches%2520performance%2520compared%2520to%2520in-context%2520learning%2520and%2520test-time%2520training%2520methods.%2520Tested%2520on%2520the%2520ARC-AGI%2520benchmark%252C%2520we%2520demonstrate%2520that%2520LPN%2520can%2520both%2520learn%2520a%2520compact%2520program%2520space%2520and%2520search%2520through%2520it%2520at%2520test%2520time%2520to%2520adapt%2520to%2520novel%2520tasks.%2520LPN%2520doubles%2520its%2520performance%2520on%2520out-of-distribution%2520tasks%2520when%2520test-time%2520search%2520is%2520switched%2520on.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08706v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Searching%20Latent%20Program%20Spaces&entry.906535625=Matthew%20V%20Macfarlane%20and%20Clement%20Bonnet&entry.1292438233=General%20intelligence%20requires%20systems%20that%20acquire%20new%20skills%20efficiently%20and%20generalize%20beyond%20their%20training%20distributions.%20Although%20program%20synthesis%20approaches%20have%20strong%20generalization%20power%2C%20they%20face%20scaling%20issues%20due%20to%20the%20large%20combinatorial%20spaces%20that%20quickly%20render%20them%20impractical%2C%20requiring%20human-generated%20DSLs%20or%20pre-trained%20priors%20to%20narrow%20this%20search%20space.%20On%20the%20other%20hand%2C%20deep%20learning%20methods%20have%20had%20high%20successes%2C%20but%20they%20lack%20structured%20test-time%20adaptation%20and%20rely%20on%20heavy%20stochastic%20sampling%20or%20expensive%20gradient%20updates%20for%20fine-tuning.%20In%20this%20work%2C%20we%20propose%20the%20Latent%20Program%20Network%20%28LPN%29%2C%20a%20novel%20architecture%20that%20builds%20in%20test-time%20search%20directly%20into%20neural%20models.%20LPN%20learns%20a%20latent%20space%20of%20implicit%20programs%20--%20neurally%20mapping%20inputs%20to%20outputs%20--%20through%20which%20it%20can%20search%20using%20gradients%20at%20test%20time.%20LPN%20combines%20the%20adaptability%20of%20symbolic%20approaches%20and%20the%20scalability%20of%20neural%20methods.%20It%20searches%20through%20a%20compact%20latent%20space%20at%20test%20time%20and%20bypasses%20the%20need%20for%20pre-defined%20domain-specific%20languages.%20On%20a%20range%20of%20programming-by-examples%20tasks%2C%20LPN%20either%20outperforms%20or%20matches%20performance%20compared%20to%20in-context%20learning%20and%20test-time%20training%20methods.%20Tested%20on%20the%20ARC-AGI%20benchmark%2C%20we%20demonstrate%20that%20LPN%20can%20both%20learn%20a%20compact%20program%20space%20and%20search%20through%20it%20at%20test%20time%20to%20adapt%20to%20novel%20tasks.%20LPN%20doubles%20its%20performance%20on%20out-of-distribution%20tasks%20when%20test-time%20search%20is%20switched%20on.&entry.1838667208=http%3A//arxiv.org/abs/2411.08706v3&entry.124074799=Read"},
{"title": "SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models", "author": "Wen-Fang Su and Hsiao-Wei Chou and Wen-Yang Lin", "abstract": "Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.", "link": "http://arxiv.org/abs/2511.20143v1", "date": "2025-11-25", "relevancy": 2.5112, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5212}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5005}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEDA%3A%20A%20Self-Adapted%20Entity-Centric%20Data%20Augmentation%20for%20Boosting%20Gird-based%20Discontinuous%20NER%20Models&body=Title%3A%20SEDA%3A%20A%20Self-Adapted%20Entity-Centric%20Data%20Augmentation%20for%20Boosting%20Gird-based%20Discontinuous%20NER%20Models%0AAuthor%3A%20Wen-Fang%20Su%20and%20Hsiao-Wei%20Chou%20and%20Wen-Yang%20Lin%0AAbstract%3A%20Named%20Entity%20Recognition%20%28NER%29%20is%20a%20critical%20task%20in%20natural%20language%20processing%2C%20yet%20it%20remains%20particularly%20challenging%20for%20discontinuous%20entities.%20The%20primary%20difficulty%20lies%20in%20text%20segmentation%2C%20as%20traditional%20methods%20often%20missegment%20or%20entirely%20miss%20cross-sentence%20discontinuous%20entities%2C%20significantly%20affecting%20recognition%20accuracy.%20Therefore%2C%20we%20aim%20to%20address%20the%20segmentation%20and%20omission%20issues%20associated%20with%20such%20entities.%20Recent%20studies%20have%20shown%20that%20grid-tagging%20methods%20are%20effective%20for%20information%20extraction%20due%20to%20their%20flexible%20tagging%20schemes%20and%20robust%20architectures.%20Building%20on%20this%2C%20we%20integrate%20image%20data%20augmentation%20techniques%2C%20such%20as%20cropping%2C%20scaling%2C%20and%20padding%2C%20into%20grid-based%20models%20to%20enhance%20their%20ability%20to%20recognize%20discontinuous%20entities%20and%20handle%20segmentation%20challenges.%20Experimental%20results%20demonstrate%20that%20traditional%20segmentation%20methods%20often%20fail%20to%20capture%20cross-sentence%20discontinuous%20entities%2C%20leading%20to%20decreased%20performance.%20In%20contrast%2C%20our%20augmented%20grid%20models%20achieve%20notable%20improvements.%20Evaluations%20on%20the%20CADEC%2C%20ShARe13%2C%20and%20ShARe14%20datasets%20show%20F1%20score%20gains%20of%201-2.5%25%20overall%20and%203.7-8.4%25%20for%20discontinuous%20entities%2C%20confirming%20the%20effectiveness%20of%20our%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEDA%253A%2520A%2520Self-Adapted%2520Entity-Centric%2520Data%2520Augmentation%2520for%2520Boosting%2520Gird-based%2520Discontinuous%2520NER%2520Models%26entry.906535625%3DWen-Fang%2520Su%2520and%2520Hsiao-Wei%2520Chou%2520and%2520Wen-Yang%2520Lin%26entry.1292438233%3DNamed%2520Entity%2520Recognition%2520%2528NER%2529%2520is%2520a%2520critical%2520task%2520in%2520natural%2520language%2520processing%252C%2520yet%2520it%2520remains%2520particularly%2520challenging%2520for%2520discontinuous%2520entities.%2520The%2520primary%2520difficulty%2520lies%2520in%2520text%2520segmentation%252C%2520as%2520traditional%2520methods%2520often%2520missegment%2520or%2520entirely%2520miss%2520cross-sentence%2520discontinuous%2520entities%252C%2520significantly%2520affecting%2520recognition%2520accuracy.%2520Therefore%252C%2520we%2520aim%2520to%2520address%2520the%2520segmentation%2520and%2520omission%2520issues%2520associated%2520with%2520such%2520entities.%2520Recent%2520studies%2520have%2520shown%2520that%2520grid-tagging%2520methods%2520are%2520effective%2520for%2520information%2520extraction%2520due%2520to%2520their%2520flexible%2520tagging%2520schemes%2520and%2520robust%2520architectures.%2520Building%2520on%2520this%252C%2520we%2520integrate%2520image%2520data%2520augmentation%2520techniques%252C%2520such%2520as%2520cropping%252C%2520scaling%252C%2520and%2520padding%252C%2520into%2520grid-based%2520models%2520to%2520enhance%2520their%2520ability%2520to%2520recognize%2520discontinuous%2520entities%2520and%2520handle%2520segmentation%2520challenges.%2520Experimental%2520results%2520demonstrate%2520that%2520traditional%2520segmentation%2520methods%2520often%2520fail%2520to%2520capture%2520cross-sentence%2520discontinuous%2520entities%252C%2520leading%2520to%2520decreased%2520performance.%2520In%2520contrast%252C%2520our%2520augmented%2520grid%2520models%2520achieve%2520notable%2520improvements.%2520Evaluations%2520on%2520the%2520CADEC%252C%2520ShARe13%252C%2520and%2520ShARe14%2520datasets%2520show%2520F1%2520score%2520gains%2520of%25201-2.5%2525%2520overall%2520and%25203.7-8.4%2525%2520for%2520discontinuous%2520entities%252C%2520confirming%2520the%2520effectiveness%2520of%2520our%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEDA%3A%20A%20Self-Adapted%20Entity-Centric%20Data%20Augmentation%20for%20Boosting%20Gird-based%20Discontinuous%20NER%20Models&entry.906535625=Wen-Fang%20Su%20and%20Hsiao-Wei%20Chou%20and%20Wen-Yang%20Lin&entry.1292438233=Named%20Entity%20Recognition%20%28NER%29%20is%20a%20critical%20task%20in%20natural%20language%20processing%2C%20yet%20it%20remains%20particularly%20challenging%20for%20discontinuous%20entities.%20The%20primary%20difficulty%20lies%20in%20text%20segmentation%2C%20as%20traditional%20methods%20often%20missegment%20or%20entirely%20miss%20cross-sentence%20discontinuous%20entities%2C%20significantly%20affecting%20recognition%20accuracy.%20Therefore%2C%20we%20aim%20to%20address%20the%20segmentation%20and%20omission%20issues%20associated%20with%20such%20entities.%20Recent%20studies%20have%20shown%20that%20grid-tagging%20methods%20are%20effective%20for%20information%20extraction%20due%20to%20their%20flexible%20tagging%20schemes%20and%20robust%20architectures.%20Building%20on%20this%2C%20we%20integrate%20image%20data%20augmentation%20techniques%2C%20such%20as%20cropping%2C%20scaling%2C%20and%20padding%2C%20into%20grid-based%20models%20to%20enhance%20their%20ability%20to%20recognize%20discontinuous%20entities%20and%20handle%20segmentation%20challenges.%20Experimental%20results%20demonstrate%20that%20traditional%20segmentation%20methods%20often%20fail%20to%20capture%20cross-sentence%20discontinuous%20entities%2C%20leading%20to%20decreased%20performance.%20In%20contrast%2C%20our%20augmented%20grid%20models%20achieve%20notable%20improvements.%20Evaluations%20on%20the%20CADEC%2C%20ShARe13%2C%20and%20ShARe14%20datasets%20show%20F1%20score%20gains%20of%201-2.5%25%20overall%20and%203.7-8.4%25%20for%20discontinuous%20entities%2C%20confirming%20the%20effectiveness%20of%20our%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2511.20143v1&entry.124074799=Read"},
{"title": "Interpretable Reward Model via Sparse Autoencoder", "author": "Shuyi Zhang and Wei Shi and Sihang Li and Jiayi Liao and Hengxing Cai and Xiang Wang", "abstract": "Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at https://github.com/schrieffer-z/sarm.", "link": "http://arxiv.org/abs/2508.08746v5", "date": "2025-11-25", "relevancy": 2.4944, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Reward%20Model%20via%20Sparse%20Autoencoder&body=Title%3A%20Interpretable%20Reward%20Model%20via%20Sparse%20Autoencoder%0AAuthor%3A%20Shuyi%20Zhang%20and%20Wei%20Shi%20and%20Sihang%20Li%20and%20Jiayi%20Liao%20and%20Hengxing%20Cai%20and%20Xiang%20Wang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20deployed%20across%20numerous%20fields.%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20leverages%20reward%20models%20%28RMs%29%20as%20proxies%20for%20human%20preferences%20to%20align%20LLM%20behaviors%20with%20human%20values%2C%20making%20the%20accuracy%2C%20reliability%2C%20and%20interpretability%20of%20RMs%20critical%20for%20effective%20alignment.%20However%2C%20traditional%20RMs%20lack%20interpretability%2C%20offer%20limited%20insight%20into%20the%20reasoning%20behind%20reward%20assignments%2C%20and%20are%20inflexible%20toward%20user%20preference%20shifts.%20While%20recent%20multidimensional%20RMs%20aim%20for%20improved%20interpretability%2C%20they%20often%20fail%20to%20provide%20feature-level%20attribution%20and%20require%20costly%20annotations.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20the%20Sparse%20Autoencoder-enhanced%20Reward%20Model%20%28SARM%29%2C%20a%20novel%20architecture%20that%20integrates%20a%20pretrained%20Sparse%20Autoencoder%20%28SAE%29%20into%20a%20reward%20model.%20SARM%20maps%20the%20hidden%20activations%20of%20LLM-based%20RM%20into%20an%20interpretable%2C%20sparse%2C%20and%20monosemantic%20feature%20space%2C%20from%20which%20a%20scalar%20head%20aggregates%20feature%20activations%20to%20produce%20transparent%20and%20conceptually%20meaningful%20reward%20scores.%20Empirical%20evaluations%20demonstrate%20that%20SARM%20facilitates%20direct%20feature-level%20attribution%20of%20reward%20assignments%2C%20allows%20dynamic%20adjustment%20to%20preference%20shifts%2C%20and%20achieves%20superior%20alignment%20performance%20compared%20to%20conventional%20reward%20models.%20Our%20code%20is%20available%20at%20https%3A//github.com/schrieffer-z/sarm.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08746v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Reward%2520Model%2520via%2520Sparse%2520Autoencoder%26entry.906535625%3DShuyi%2520Zhang%2520and%2520Wei%2520Shi%2520and%2520Sihang%2520Li%2520and%2520Jiayi%2520Liao%2520and%2520Hengxing%2520Cai%2520and%2520Xiang%2520Wang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520deployed%2520across%2520numerous%2520fields.%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520leverages%2520reward%2520models%2520%2528RMs%2529%2520as%2520proxies%2520for%2520human%2520preferences%2520to%2520align%2520LLM%2520behaviors%2520with%2520human%2520values%252C%2520making%2520the%2520accuracy%252C%2520reliability%252C%2520and%2520interpretability%2520of%2520RMs%2520critical%2520for%2520effective%2520alignment.%2520However%252C%2520traditional%2520RMs%2520lack%2520interpretability%252C%2520offer%2520limited%2520insight%2520into%2520the%2520reasoning%2520behind%2520reward%2520assignments%252C%2520and%2520are%2520inflexible%2520toward%2520user%2520preference%2520shifts.%2520While%2520recent%2520multidimensional%2520RMs%2520aim%2520for%2520improved%2520interpretability%252C%2520they%2520often%2520fail%2520to%2520provide%2520feature-level%2520attribution%2520and%2520require%2520costly%2520annotations.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520Sparse%2520Autoencoder-enhanced%2520Reward%2520Model%2520%2528SARM%2529%252C%2520a%2520novel%2520architecture%2520that%2520integrates%2520a%2520pretrained%2520Sparse%2520Autoencoder%2520%2528SAE%2529%2520into%2520a%2520reward%2520model.%2520SARM%2520maps%2520the%2520hidden%2520activations%2520of%2520LLM-based%2520RM%2520into%2520an%2520interpretable%252C%2520sparse%252C%2520and%2520monosemantic%2520feature%2520space%252C%2520from%2520which%2520a%2520scalar%2520head%2520aggregates%2520feature%2520activations%2520to%2520produce%2520transparent%2520and%2520conceptually%2520meaningful%2520reward%2520scores.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520SARM%2520facilitates%2520direct%2520feature-level%2520attribution%2520of%2520reward%2520assignments%252C%2520allows%2520dynamic%2520adjustment%2520to%2520preference%2520shifts%252C%2520and%2520achieves%2520superior%2520alignment%2520performance%2520compared%2520to%2520conventional%2520reward%2520models.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/schrieffer-z/sarm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08746v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Reward%20Model%20via%20Sparse%20Autoencoder&entry.906535625=Shuyi%20Zhang%20and%20Wei%20Shi%20and%20Sihang%20Li%20and%20Jiayi%20Liao%20and%20Hengxing%20Cai%20and%20Xiang%20Wang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20deployed%20across%20numerous%20fields.%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20leverages%20reward%20models%20%28RMs%29%20as%20proxies%20for%20human%20preferences%20to%20align%20LLM%20behaviors%20with%20human%20values%2C%20making%20the%20accuracy%2C%20reliability%2C%20and%20interpretability%20of%20RMs%20critical%20for%20effective%20alignment.%20However%2C%20traditional%20RMs%20lack%20interpretability%2C%20offer%20limited%20insight%20into%20the%20reasoning%20behind%20reward%20assignments%2C%20and%20are%20inflexible%20toward%20user%20preference%20shifts.%20While%20recent%20multidimensional%20RMs%20aim%20for%20improved%20interpretability%2C%20they%20often%20fail%20to%20provide%20feature-level%20attribution%20and%20require%20costly%20annotations.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20the%20Sparse%20Autoencoder-enhanced%20Reward%20Model%20%28SARM%29%2C%20a%20novel%20architecture%20that%20integrates%20a%20pretrained%20Sparse%20Autoencoder%20%28SAE%29%20into%20a%20reward%20model.%20SARM%20maps%20the%20hidden%20activations%20of%20LLM-based%20RM%20into%20an%20interpretable%2C%20sparse%2C%20and%20monosemantic%20feature%20space%2C%20from%20which%20a%20scalar%20head%20aggregates%20feature%20activations%20to%20produce%20transparent%20and%20conceptually%20meaningful%20reward%20scores.%20Empirical%20evaluations%20demonstrate%20that%20SARM%20facilitates%20direct%20feature-level%20attribution%20of%20reward%20assignments%2C%20allows%20dynamic%20adjustment%20to%20preference%20shifts%2C%20and%20achieves%20superior%20alignment%20performance%20compared%20to%20conventional%20reward%20models.%20Our%20code%20is%20available%20at%20https%3A//github.com/schrieffer-z/sarm.&entry.1838667208=http%3A//arxiv.org/abs/2508.08746v5&entry.124074799=Read"},
{"title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization", "author": "Tahira Kazimi and Connor Dunlop and Pinar Yanardag", "abstract": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.", "link": "http://arxiv.org/abs/2511.20647v1", "date": "2025-11-25", "relevancy": 2.4846, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6374}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6186}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diverse%20Video%20Generation%20with%20Determinantal%20Point%20Process-Guided%20Policy%20Optimization&body=Title%3A%20Diverse%20Video%20Generation%20with%20Determinantal%20Point%20Process-Guided%20Policy%20Optimization%0AAuthor%3A%20Tahira%20Kazimi%20and%20Connor%20Dunlop%20and%20Pinar%20Yanardag%0AAbstract%3A%20While%20recent%20text-to-video%20%28T2V%29%20diffusion%20models%20have%20achieved%20impressive%20quality%20and%20prompt%20alignment%2C%20they%20often%20produce%20low-diversity%20outputs%20when%20sampling%20multiple%20videos%20from%20a%20single%20text%20prompt.%20We%20tackle%20this%20challenge%20by%20formulating%20it%20as%20a%20set-level%20policy%20optimization%20problem%2C%20with%20the%20goal%20of%20training%20a%20policy%20that%20can%20cover%20the%20diverse%20range%20of%20plausible%20outcomes%20for%20a%20given%20prompt.%20To%20address%20this%2C%20we%20introduce%20DPP-GRPO%2C%20a%20novel%20framework%20for%20diverse%20video%20generation%20that%20combines%20Determinantal%20Point%20Processes%20%28DPPs%29%20and%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20theories%20to%20enforce%20explicit%20reward%20on%20diverse%20generations.%20Our%20objective%20turns%20diversity%20into%20an%20explicit%20signal%20by%20imposing%20diminishing%20returns%20on%20redundant%20samples%20%28via%20DPP%29%20while%20supplies%20groupwise%20feedback%20over%20candidate%20sets%20%28via%20GRPO%29.%20Our%20framework%20is%20plug-and-play%20and%20model-agnostic%2C%20and%20encourages%20diverse%20generations%20across%20visual%20appearance%2C%20camera%20motions%2C%20and%20scene%20structure%20without%20sacrificing%20prompt%20fidelity%20or%20perceptual%20quality.%20We%20implement%20our%20method%20on%20WAN%20and%20CogVideoX%2C%20and%20show%20that%20our%20method%20consistently%20improves%20video%20diversity%20on%20state-of-the-art%20benchmarks%20such%20as%20VBench%2C%20VideoScore%2C%20and%20human%20preference%20studies.%20Moreover%2C%20we%20release%20our%20code%20and%20a%20new%20benchmark%20dataset%20of%2030%2C000%20diverse%20prompts%20to%20support%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverse%2520Video%2520Generation%2520with%2520Determinantal%2520Point%2520Process-Guided%2520Policy%2520Optimization%26entry.906535625%3DTahira%2520Kazimi%2520and%2520Connor%2520Dunlop%2520and%2520Pinar%2520Yanardag%26entry.1292438233%3DWhile%2520recent%2520text-to-video%2520%2528T2V%2529%2520diffusion%2520models%2520have%2520achieved%2520impressive%2520quality%2520and%2520prompt%2520alignment%252C%2520they%2520often%2520produce%2520low-diversity%2520outputs%2520when%2520sampling%2520multiple%2520videos%2520from%2520a%2520single%2520text%2520prompt.%2520We%2520tackle%2520this%2520challenge%2520by%2520formulating%2520it%2520as%2520a%2520set-level%2520policy%2520optimization%2520problem%252C%2520with%2520the%2520goal%2520of%2520training%2520a%2520policy%2520that%2520can%2520cover%2520the%2520diverse%2520range%2520of%2520plausible%2520outcomes%2520for%2520a%2520given%2520prompt.%2520To%2520address%2520this%252C%2520we%2520introduce%2520DPP-GRPO%252C%2520a%2520novel%2520framework%2520for%2520diverse%2520video%2520generation%2520that%2520combines%2520Determinantal%2520Point%2520Processes%2520%2528DPPs%2529%2520and%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520theories%2520to%2520enforce%2520explicit%2520reward%2520on%2520diverse%2520generations.%2520Our%2520objective%2520turns%2520diversity%2520into%2520an%2520explicit%2520signal%2520by%2520imposing%2520diminishing%2520returns%2520on%2520redundant%2520samples%2520%2528via%2520DPP%2529%2520while%2520supplies%2520groupwise%2520feedback%2520over%2520candidate%2520sets%2520%2528via%2520GRPO%2529.%2520Our%2520framework%2520is%2520plug-and-play%2520and%2520model-agnostic%252C%2520and%2520encourages%2520diverse%2520generations%2520across%2520visual%2520appearance%252C%2520camera%2520motions%252C%2520and%2520scene%2520structure%2520without%2520sacrificing%2520prompt%2520fidelity%2520or%2520perceptual%2520quality.%2520We%2520implement%2520our%2520method%2520on%2520WAN%2520and%2520CogVideoX%252C%2520and%2520show%2520that%2520our%2520method%2520consistently%2520improves%2520video%2520diversity%2520on%2520state-of-the-art%2520benchmarks%2520such%2520as%2520VBench%252C%2520VideoScore%252C%2520and%2520human%2520preference%2520studies.%2520Moreover%252C%2520we%2520release%2520our%2520code%2520and%2520a%2520new%2520benchmark%2520dataset%2520of%252030%252C000%2520diverse%2520prompts%2520to%2520support%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diverse%20Video%20Generation%20with%20Determinantal%20Point%20Process-Guided%20Policy%20Optimization&entry.906535625=Tahira%20Kazimi%20and%20Connor%20Dunlop%20and%20Pinar%20Yanardag&entry.1292438233=While%20recent%20text-to-video%20%28T2V%29%20diffusion%20models%20have%20achieved%20impressive%20quality%20and%20prompt%20alignment%2C%20they%20often%20produce%20low-diversity%20outputs%20when%20sampling%20multiple%20videos%20from%20a%20single%20text%20prompt.%20We%20tackle%20this%20challenge%20by%20formulating%20it%20as%20a%20set-level%20policy%20optimization%20problem%2C%20with%20the%20goal%20of%20training%20a%20policy%20that%20can%20cover%20the%20diverse%20range%20of%20plausible%20outcomes%20for%20a%20given%20prompt.%20To%20address%20this%2C%20we%20introduce%20DPP-GRPO%2C%20a%20novel%20framework%20for%20diverse%20video%20generation%20that%20combines%20Determinantal%20Point%20Processes%20%28DPPs%29%20and%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20theories%20to%20enforce%20explicit%20reward%20on%20diverse%20generations.%20Our%20objective%20turns%20diversity%20into%20an%20explicit%20signal%20by%20imposing%20diminishing%20returns%20on%20redundant%20samples%20%28via%20DPP%29%20while%20supplies%20groupwise%20feedback%20over%20candidate%20sets%20%28via%20GRPO%29.%20Our%20framework%20is%20plug-and-play%20and%20model-agnostic%2C%20and%20encourages%20diverse%20generations%20across%20visual%20appearance%2C%20camera%20motions%2C%20and%20scene%20structure%20without%20sacrificing%20prompt%20fidelity%20or%20perceptual%20quality.%20We%20implement%20our%20method%20on%20WAN%20and%20CogVideoX%2C%20and%20show%20that%20our%20method%20consistently%20improves%20video%20diversity%20on%20state-of-the-art%20benchmarks%20such%20as%20VBench%2C%20VideoScore%2C%20and%20human%20preference%20studies.%20Moreover%2C%20we%20release%20our%20code%20and%20a%20new%20benchmark%20dataset%20of%2030%2C000%20diverse%20prompts%20to%20support%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.20647v1&entry.124074799=Read"},
{"title": "Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation", "author": "Andrea Ranieri and Giorgio Palmieri and Silvia Biasotti", "abstract": "This paper addresses the critical need for automated crack detection in the preservation of cultural heritage through semantic segmentation. We present a comparative study of U-Net architectures, using various convolutional neural network (CNN) encoders, for pixel-level crack identification on statues and monuments. A comparative quantitative evaluation is performed on the test set of the OmniCrack30k dataset [1] using popular segmentation metrics including Mean Intersection over Union (mIoU), Dice coefficient, and Jaccard index. This is complemented by an out-of-distribution qualitative evaluation on an unlabeled test set of real-world cracked statues and monuments. Our findings provide valuable insights into the capabilities of different CNN- based encoders for fine-grained crack segmentation. We show that the models exhibit promising generalization capabilities to unseen cultural heritage contexts, despite never having been explicitly trained on images of statues or monuments.", "link": "http://arxiv.org/abs/2511.20541v1", "date": "2025-11-25", "relevancy": 2.4779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Monitoring%20of%20Cultural%20Heritage%20Artifacts%20Using%20Semantic%20Segmentation&body=Title%3A%20Automated%20Monitoring%20of%20Cultural%20Heritage%20Artifacts%20Using%20Semantic%20Segmentation%0AAuthor%3A%20Andrea%20Ranieri%20and%20Giorgio%20Palmieri%20and%20Silvia%20Biasotti%0AAbstract%3A%20This%20paper%20addresses%20the%20critical%20need%20for%20automated%20crack%20detection%20in%20the%20preservation%20of%20cultural%20heritage%20through%20semantic%20segmentation.%20We%20present%20a%20comparative%20study%20of%20U-Net%20architectures%2C%20using%20various%20convolutional%20neural%20network%20%28CNN%29%20encoders%2C%20for%20pixel-level%20crack%20identification%20on%20statues%20and%20monuments.%20A%20comparative%20quantitative%20evaluation%20is%20performed%20on%20the%20test%20set%20of%20the%20OmniCrack30k%20dataset%20%5B1%5D%20using%20popular%20segmentation%20metrics%20including%20Mean%20Intersection%20over%20Union%20%28mIoU%29%2C%20Dice%20coefficient%2C%20and%20Jaccard%20index.%20This%20is%20complemented%20by%20an%20out-of-distribution%20qualitative%20evaluation%20on%20an%20unlabeled%20test%20set%20of%20real-world%20cracked%20statues%20and%20monuments.%20Our%20findings%20provide%20valuable%20insights%20into%20the%20capabilities%20of%20different%20CNN-%20based%20encoders%20for%20fine-grained%20crack%20segmentation.%20We%20show%20that%20the%20models%20exhibit%20promising%20generalization%20capabilities%20to%20unseen%20cultural%20heritage%20contexts%2C%20despite%20never%20having%20been%20explicitly%20trained%20on%20images%20of%20statues%20or%20monuments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Monitoring%2520of%2520Cultural%2520Heritage%2520Artifacts%2520Using%2520Semantic%2520Segmentation%26entry.906535625%3DAndrea%2520Ranieri%2520and%2520Giorgio%2520Palmieri%2520and%2520Silvia%2520Biasotti%26entry.1292438233%3DThis%2520paper%2520addresses%2520the%2520critical%2520need%2520for%2520automated%2520crack%2520detection%2520in%2520the%2520preservation%2520of%2520cultural%2520heritage%2520through%2520semantic%2520segmentation.%2520We%2520present%2520a%2520comparative%2520study%2520of%2520U-Net%2520architectures%252C%2520using%2520various%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520encoders%252C%2520for%2520pixel-level%2520crack%2520identification%2520on%2520statues%2520and%2520monuments.%2520A%2520comparative%2520quantitative%2520evaluation%2520is%2520performed%2520on%2520the%2520test%2520set%2520of%2520the%2520OmniCrack30k%2520dataset%2520%255B1%255D%2520using%2520popular%2520segmentation%2520metrics%2520including%2520Mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%252C%2520Dice%2520coefficient%252C%2520and%2520Jaccard%2520index.%2520This%2520is%2520complemented%2520by%2520an%2520out-of-distribution%2520qualitative%2520evaluation%2520on%2520an%2520unlabeled%2520test%2520set%2520of%2520real-world%2520cracked%2520statues%2520and%2520monuments.%2520Our%2520findings%2520provide%2520valuable%2520insights%2520into%2520the%2520capabilities%2520of%2520different%2520CNN-%2520based%2520encoders%2520for%2520fine-grained%2520crack%2520segmentation.%2520We%2520show%2520that%2520the%2520models%2520exhibit%2520promising%2520generalization%2520capabilities%2520to%2520unseen%2520cultural%2520heritage%2520contexts%252C%2520despite%2520never%2520having%2520been%2520explicitly%2520trained%2520on%2520images%2520of%2520statues%2520or%2520monuments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Monitoring%20of%20Cultural%20Heritage%20Artifacts%20Using%20Semantic%20Segmentation&entry.906535625=Andrea%20Ranieri%20and%20Giorgio%20Palmieri%20and%20Silvia%20Biasotti&entry.1292438233=This%20paper%20addresses%20the%20critical%20need%20for%20automated%20crack%20detection%20in%20the%20preservation%20of%20cultural%20heritage%20through%20semantic%20segmentation.%20We%20present%20a%20comparative%20study%20of%20U-Net%20architectures%2C%20using%20various%20convolutional%20neural%20network%20%28CNN%29%20encoders%2C%20for%20pixel-level%20crack%20identification%20on%20statues%20and%20monuments.%20A%20comparative%20quantitative%20evaluation%20is%20performed%20on%20the%20test%20set%20of%20the%20OmniCrack30k%20dataset%20%5B1%5D%20using%20popular%20segmentation%20metrics%20including%20Mean%20Intersection%20over%20Union%20%28mIoU%29%2C%20Dice%20coefficient%2C%20and%20Jaccard%20index.%20This%20is%20complemented%20by%20an%20out-of-distribution%20qualitative%20evaluation%20on%20an%20unlabeled%20test%20set%20of%20real-world%20cracked%20statues%20and%20monuments.%20Our%20findings%20provide%20valuable%20insights%20into%20the%20capabilities%20of%20different%20CNN-%20based%20encoders%20for%20fine-grained%20crack%20segmentation.%20We%20show%20that%20the%20models%20exhibit%20promising%20generalization%20capabilities%20to%20unseen%20cultural%20heritage%20contexts%2C%20despite%20never%20having%20been%20explicitly%20trained%20on%20images%20of%20statues%20or%20monuments.&entry.1838667208=http%3A//arxiv.org/abs/2511.20541v1&entry.124074799=Read"},
{"title": "AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation", "author": "Xinyue Liang and Zhiyuan Ma and Lingchen Sun and Yanjun Guo and Lei Zhang", "abstract": "Single-image-to-3D models typically follow a sequential generation and reconstruction workflow. However, intermediate multi-view images synthesized by pre-trained generation models often lack cross-view consistency (CVC), significantly degrading 3D reconstruction performance. While recent methods attempt to refine CVC by feeding reconstruction results back into the multi-view generator, these approaches struggle with noisy and unstable reconstruction outputs that limit effective CVC improvement. We introduce AlignCVC, a novel framework that fundamentally re-frames single-image-to-3D generation through distribution alignment rather than relying on strict regression losses. Our key insight is to align both generated and reconstructed multi-view distributions toward the ground-truth multi-view distribution, establishing a principled foundation for improved CVC. Observing that generated images exhibit weak CVC while reconstructed images display strong CVC due to explicit rendering, we propose a soft-hard alignment strategy with distinct objectives for generation and reconstruction models. This approach not only enhances generation quality but also dramatically accelerates inference to as few as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC, seamlessly integrates various multi-view generation models with 3D reconstruction models. Extensive experiments demonstrate the effectiveness and efficiency of AlignCVC for single-image-to-3D generation.", "link": "http://arxiv.org/abs/2506.23150v2", "date": "2025-11-25", "relevancy": 2.4778, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6262}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6262}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignCVC%3A%20Aligning%20Cross-View%20Consistency%20for%20Single-Image-to-3D%20Generation&body=Title%3A%20AlignCVC%3A%20Aligning%20Cross-View%20Consistency%20for%20Single-Image-to-3D%20Generation%0AAuthor%3A%20Xinyue%20Liang%20and%20Zhiyuan%20Ma%20and%20Lingchen%20Sun%20and%20Yanjun%20Guo%20and%20Lei%20Zhang%0AAbstract%3A%20Single-image-to-3D%20models%20typically%20follow%20a%20sequential%20generation%20and%20reconstruction%20workflow.%20However%2C%20intermediate%20multi-view%20images%20synthesized%20by%20pre-trained%20generation%20models%20often%20lack%20cross-view%20consistency%20%28CVC%29%2C%20significantly%20degrading%203D%20reconstruction%20performance.%20While%20recent%20methods%20attempt%20to%20refine%20CVC%20by%20feeding%20reconstruction%20results%20back%20into%20the%20multi-view%20generator%2C%20these%20approaches%20struggle%20with%20noisy%20and%20unstable%20reconstruction%20outputs%20that%20limit%20effective%20CVC%20improvement.%20We%20introduce%20AlignCVC%2C%20a%20novel%20framework%20that%20fundamentally%20re-frames%20single-image-to-3D%20generation%20through%20distribution%20alignment%20rather%20than%20relying%20on%20strict%20regression%20losses.%20Our%20key%20insight%20is%20to%20align%20both%20generated%20and%20reconstructed%20multi-view%20distributions%20toward%20the%20ground-truth%20multi-view%20distribution%2C%20establishing%20a%20principled%20foundation%20for%20improved%20CVC.%20Observing%20that%20generated%20images%20exhibit%20weak%20CVC%20while%20reconstructed%20images%20display%20strong%20CVC%20due%20to%20explicit%20rendering%2C%20we%20propose%20a%20soft-hard%20alignment%20strategy%20with%20distinct%20objectives%20for%20generation%20and%20reconstruction%20models.%20This%20approach%20not%20only%20enhances%20generation%20quality%20but%20also%20dramatically%20accelerates%20inference%20to%20as%20few%20as%204%20steps.%20As%20a%20plug-and-play%20paradigm%2C%20our%20method%2C%20namely%20AlignCVC%2C%20seamlessly%20integrates%20various%20multi-view%20generation%20models%20with%203D%20reconstruction%20models.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20AlignCVC%20for%20single-image-to-3D%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2506.23150v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignCVC%253A%2520Aligning%2520Cross-View%2520Consistency%2520for%2520Single-Image-to-3D%2520Generation%26entry.906535625%3DXinyue%2520Liang%2520and%2520Zhiyuan%2520Ma%2520and%2520Lingchen%2520Sun%2520and%2520Yanjun%2520Guo%2520and%2520Lei%2520Zhang%26entry.1292438233%3DSingle-image-to-3D%2520models%2520typically%2520follow%2520a%2520sequential%2520generation%2520and%2520reconstruction%2520workflow.%2520However%252C%2520intermediate%2520multi-view%2520images%2520synthesized%2520by%2520pre-trained%2520generation%2520models%2520often%2520lack%2520cross-view%2520consistency%2520%2528CVC%2529%252C%2520significantly%2520degrading%25203D%2520reconstruction%2520performance.%2520While%2520recent%2520methods%2520attempt%2520to%2520refine%2520CVC%2520by%2520feeding%2520reconstruction%2520results%2520back%2520into%2520the%2520multi-view%2520generator%252C%2520these%2520approaches%2520struggle%2520with%2520noisy%2520and%2520unstable%2520reconstruction%2520outputs%2520that%2520limit%2520effective%2520CVC%2520improvement.%2520We%2520introduce%2520AlignCVC%252C%2520a%2520novel%2520framework%2520that%2520fundamentally%2520re-frames%2520single-image-to-3D%2520generation%2520through%2520distribution%2520alignment%2520rather%2520than%2520relying%2520on%2520strict%2520regression%2520losses.%2520Our%2520key%2520insight%2520is%2520to%2520align%2520both%2520generated%2520and%2520reconstructed%2520multi-view%2520distributions%2520toward%2520the%2520ground-truth%2520multi-view%2520distribution%252C%2520establishing%2520a%2520principled%2520foundation%2520for%2520improved%2520CVC.%2520Observing%2520that%2520generated%2520images%2520exhibit%2520weak%2520CVC%2520while%2520reconstructed%2520images%2520display%2520strong%2520CVC%2520due%2520to%2520explicit%2520rendering%252C%2520we%2520propose%2520a%2520soft-hard%2520alignment%2520strategy%2520with%2520distinct%2520objectives%2520for%2520generation%2520and%2520reconstruction%2520models.%2520This%2520approach%2520not%2520only%2520enhances%2520generation%2520quality%2520but%2520also%2520dramatically%2520accelerates%2520inference%2520to%2520as%2520few%2520as%25204%2520steps.%2520As%2520a%2520plug-and-play%2520paradigm%252C%2520our%2520method%252C%2520namely%2520AlignCVC%252C%2520seamlessly%2520integrates%2520various%2520multi-view%2520generation%2520models%2520with%25203D%2520reconstruction%2520models.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520AlignCVC%2520for%2520single-image-to-3D%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23150v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignCVC%3A%20Aligning%20Cross-View%20Consistency%20for%20Single-Image-to-3D%20Generation&entry.906535625=Xinyue%20Liang%20and%20Zhiyuan%20Ma%20and%20Lingchen%20Sun%20and%20Yanjun%20Guo%20and%20Lei%20Zhang&entry.1292438233=Single-image-to-3D%20models%20typically%20follow%20a%20sequential%20generation%20and%20reconstruction%20workflow.%20However%2C%20intermediate%20multi-view%20images%20synthesized%20by%20pre-trained%20generation%20models%20often%20lack%20cross-view%20consistency%20%28CVC%29%2C%20significantly%20degrading%203D%20reconstruction%20performance.%20While%20recent%20methods%20attempt%20to%20refine%20CVC%20by%20feeding%20reconstruction%20results%20back%20into%20the%20multi-view%20generator%2C%20these%20approaches%20struggle%20with%20noisy%20and%20unstable%20reconstruction%20outputs%20that%20limit%20effective%20CVC%20improvement.%20We%20introduce%20AlignCVC%2C%20a%20novel%20framework%20that%20fundamentally%20re-frames%20single-image-to-3D%20generation%20through%20distribution%20alignment%20rather%20than%20relying%20on%20strict%20regression%20losses.%20Our%20key%20insight%20is%20to%20align%20both%20generated%20and%20reconstructed%20multi-view%20distributions%20toward%20the%20ground-truth%20multi-view%20distribution%2C%20establishing%20a%20principled%20foundation%20for%20improved%20CVC.%20Observing%20that%20generated%20images%20exhibit%20weak%20CVC%20while%20reconstructed%20images%20display%20strong%20CVC%20due%20to%20explicit%20rendering%2C%20we%20propose%20a%20soft-hard%20alignment%20strategy%20with%20distinct%20objectives%20for%20generation%20and%20reconstruction%20models.%20This%20approach%20not%20only%20enhances%20generation%20quality%20but%20also%20dramatically%20accelerates%20inference%20to%20as%20few%20as%204%20steps.%20As%20a%20plug-and-play%20paradigm%2C%20our%20method%2C%20namely%20AlignCVC%2C%20seamlessly%20integrates%20various%20multi-view%20generation%20models%20with%203D%20reconstruction%20models.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20AlignCVC%20for%20single-image-to-3D%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2506.23150v2&entry.124074799=Read"},
{"title": "Text-guided Controllable Diffusion for Realistic Camouflage Images Generation", "author": "Yuhang Qian and Haiyan Chen and Wentong Li and Ningzhong Liu and Jie Qin", "abstract": "Camouflage Images Generation (CIG) is an emerging research area that focuses on synthesizing images in which objects are harmoniously blended and exhibit high visual consistency with their surroundings. Existing methods perform CIG by either fusing objects into specific backgrounds or outpainting the surroundings via foreground object-guided diffusion. However, they often fail to obtain natural results because they overlook the logical relationship between camouflaged objects and background environments. To address this issue, we propose CT-CIG, a Controllable Text-guided Camouflage Images Generation method that produces realistic and logically plausible camouflage images. Leveraging Large Visual Language Models (VLM), we design a Camouflage-Revealing Dialogue Mechanism (CRDM) to annotate existing camouflage datasets with high-quality text prompts. Subsequently, the constructed image-prompt pairs are utilized to finetune Stable Diffusion, incorporating a lightweight controller to guide the location and shape of camouflaged objects for enhanced camouflage scene fitness. Moreover, we design a Frequency Interaction Refinement Module (FIRM) to capture high-frequency texture features, facilitating the learning of complex camouflage patterns. Extensive experiments, including CLIPScore evaluation and camouflage effectiveness assessment, demonstrate the semantic alignment of our generated text prompts and CT-CIG's ability to produce photorealistic camouflage images.", "link": "http://arxiv.org/abs/2511.20218v1", "date": "2025-11-25", "relevancy": 2.4414, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6296}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5974}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-guided%20Controllable%20Diffusion%20for%20Realistic%20Camouflage%20Images%20Generation&body=Title%3A%20Text-guided%20Controllable%20Diffusion%20for%20Realistic%20Camouflage%20Images%20Generation%0AAuthor%3A%20Yuhang%20Qian%20and%20Haiyan%20Chen%20and%20Wentong%20Li%20and%20Ningzhong%20Liu%20and%20Jie%20Qin%0AAbstract%3A%20Camouflage%20Images%20Generation%20%28CIG%29%20is%20an%20emerging%20research%20area%20that%20focuses%20on%20synthesizing%20images%20in%20which%20objects%20are%20harmoniously%20blended%20and%20exhibit%20high%20visual%20consistency%20with%20their%20surroundings.%20Existing%20methods%20perform%20CIG%20by%20either%20fusing%20objects%20into%20specific%20backgrounds%20or%20outpainting%20the%20surroundings%20via%20foreground%20object-guided%20diffusion.%20However%2C%20they%20often%20fail%20to%20obtain%20natural%20results%20because%20they%20overlook%20the%20logical%20relationship%20between%20camouflaged%20objects%20and%20background%20environments.%20To%20address%20this%20issue%2C%20we%20propose%20CT-CIG%2C%20a%20Controllable%20Text-guided%20Camouflage%20Images%20Generation%20method%20that%20produces%20realistic%20and%20logically%20plausible%20camouflage%20images.%20Leveraging%20Large%20Visual%20Language%20Models%20%28VLM%29%2C%20we%20design%20a%20Camouflage-Revealing%20Dialogue%20Mechanism%20%28CRDM%29%20to%20annotate%20existing%20camouflage%20datasets%20with%20high-quality%20text%20prompts.%20Subsequently%2C%20the%20constructed%20image-prompt%20pairs%20are%20utilized%20to%20finetune%20Stable%20Diffusion%2C%20incorporating%20a%20lightweight%20controller%20to%20guide%20the%20location%20and%20shape%20of%20camouflaged%20objects%20for%20enhanced%20camouflage%20scene%20fitness.%20Moreover%2C%20we%20design%20a%20Frequency%20Interaction%20Refinement%20Module%20%28FIRM%29%20to%20capture%20high-frequency%20texture%20features%2C%20facilitating%20the%20learning%20of%20complex%20camouflage%20patterns.%20Extensive%20experiments%2C%20including%20CLIPScore%20evaluation%20and%20camouflage%20effectiveness%20assessment%2C%20demonstrate%20the%20semantic%20alignment%20of%20our%20generated%20text%20prompts%20and%20CT-CIG%27s%20ability%20to%20produce%20photorealistic%20camouflage%20images.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-guided%2520Controllable%2520Diffusion%2520for%2520Realistic%2520Camouflage%2520Images%2520Generation%26entry.906535625%3DYuhang%2520Qian%2520and%2520Haiyan%2520Chen%2520and%2520Wentong%2520Li%2520and%2520Ningzhong%2520Liu%2520and%2520Jie%2520Qin%26entry.1292438233%3DCamouflage%2520Images%2520Generation%2520%2528CIG%2529%2520is%2520an%2520emerging%2520research%2520area%2520that%2520focuses%2520on%2520synthesizing%2520images%2520in%2520which%2520objects%2520are%2520harmoniously%2520blended%2520and%2520exhibit%2520high%2520visual%2520consistency%2520with%2520their%2520surroundings.%2520Existing%2520methods%2520perform%2520CIG%2520by%2520either%2520fusing%2520objects%2520into%2520specific%2520backgrounds%2520or%2520outpainting%2520the%2520surroundings%2520via%2520foreground%2520object-guided%2520diffusion.%2520However%252C%2520they%2520often%2520fail%2520to%2520obtain%2520natural%2520results%2520because%2520they%2520overlook%2520the%2520logical%2520relationship%2520between%2520camouflaged%2520objects%2520and%2520background%2520environments.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520CT-CIG%252C%2520a%2520Controllable%2520Text-guided%2520Camouflage%2520Images%2520Generation%2520method%2520that%2520produces%2520realistic%2520and%2520logically%2520plausible%2520camouflage%2520images.%2520Leveraging%2520Large%2520Visual%2520Language%2520Models%2520%2528VLM%2529%252C%2520we%2520design%2520a%2520Camouflage-Revealing%2520Dialogue%2520Mechanism%2520%2528CRDM%2529%2520to%2520annotate%2520existing%2520camouflage%2520datasets%2520with%2520high-quality%2520text%2520prompts.%2520Subsequently%252C%2520the%2520constructed%2520image-prompt%2520pairs%2520are%2520utilized%2520to%2520finetune%2520Stable%2520Diffusion%252C%2520incorporating%2520a%2520lightweight%2520controller%2520to%2520guide%2520the%2520location%2520and%2520shape%2520of%2520camouflaged%2520objects%2520for%2520enhanced%2520camouflage%2520scene%2520fitness.%2520Moreover%252C%2520we%2520design%2520a%2520Frequency%2520Interaction%2520Refinement%2520Module%2520%2528FIRM%2529%2520to%2520capture%2520high-frequency%2520texture%2520features%252C%2520facilitating%2520the%2520learning%2520of%2520complex%2520camouflage%2520patterns.%2520Extensive%2520experiments%252C%2520including%2520CLIPScore%2520evaluation%2520and%2520camouflage%2520effectiveness%2520assessment%252C%2520demonstrate%2520the%2520semantic%2520alignment%2520of%2520our%2520generated%2520text%2520prompts%2520and%2520CT-CIG%2527s%2520ability%2520to%2520produce%2520photorealistic%2520camouflage%2520images.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-guided%20Controllable%20Diffusion%20for%20Realistic%20Camouflage%20Images%20Generation&entry.906535625=Yuhang%20Qian%20and%20Haiyan%20Chen%20and%20Wentong%20Li%20and%20Ningzhong%20Liu%20and%20Jie%20Qin&entry.1292438233=Camouflage%20Images%20Generation%20%28CIG%29%20is%20an%20emerging%20research%20area%20that%20focuses%20on%20synthesizing%20images%20in%20which%20objects%20are%20harmoniously%20blended%20and%20exhibit%20high%20visual%20consistency%20with%20their%20surroundings.%20Existing%20methods%20perform%20CIG%20by%20either%20fusing%20objects%20into%20specific%20backgrounds%20or%20outpainting%20the%20surroundings%20via%20foreground%20object-guided%20diffusion.%20However%2C%20they%20often%20fail%20to%20obtain%20natural%20results%20because%20they%20overlook%20the%20logical%20relationship%20between%20camouflaged%20objects%20and%20background%20environments.%20To%20address%20this%20issue%2C%20we%20propose%20CT-CIG%2C%20a%20Controllable%20Text-guided%20Camouflage%20Images%20Generation%20method%20that%20produces%20realistic%20and%20logically%20plausible%20camouflage%20images.%20Leveraging%20Large%20Visual%20Language%20Models%20%28VLM%29%2C%20we%20design%20a%20Camouflage-Revealing%20Dialogue%20Mechanism%20%28CRDM%29%20to%20annotate%20existing%20camouflage%20datasets%20with%20high-quality%20text%20prompts.%20Subsequently%2C%20the%20constructed%20image-prompt%20pairs%20are%20utilized%20to%20finetune%20Stable%20Diffusion%2C%20incorporating%20a%20lightweight%20controller%20to%20guide%20the%20location%20and%20shape%20of%20camouflaged%20objects%20for%20enhanced%20camouflage%20scene%20fitness.%20Moreover%2C%20we%20design%20a%20Frequency%20Interaction%20Refinement%20Module%20%28FIRM%29%20to%20capture%20high-frequency%20texture%20features%2C%20facilitating%20the%20learning%20of%20complex%20camouflage%20patterns.%20Extensive%20experiments%2C%20including%20CLIPScore%20evaluation%20and%20camouflage%20effectiveness%20assessment%2C%20demonstrate%20the%20semantic%20alignment%20of%20our%20generated%20text%20prompts%20and%20CT-CIG%27s%20ability%20to%20produce%20photorealistic%20camouflage%20images.&entry.1838667208=http%3A//arxiv.org/abs/2511.20218v1&entry.124074799=Read"},
{"title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "author": "Wei He and Kai Han and Hang Zhou and Hanting Chen and Zhicheng Liu and Xinghao Chen and Yunhe Wang", "abstract": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "link": "http://arxiv.org/abs/2511.20626v1", "date": "2025-11-25", "relevancy": 2.4295, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5154}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4766}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROOT%3A%20Robust%20Orthogonalized%20Optimizer%20for%20Neural%20Network%20Training&body=Title%3A%20ROOT%3A%20Robust%20Orthogonalized%20Optimizer%20for%20Neural%20Network%20Training%0AAuthor%3A%20Wei%20He%20and%20Kai%20Han%20and%20Hang%20Zhou%20and%20Hanting%20Chen%20and%20Zhicheng%20Liu%20and%20Xinghao%20Chen%20and%20Yunhe%20Wang%0AAbstract%3A%20The%20optimization%20of%20large%20language%20models%20%28LLMs%29%20remains%20a%20critical%20challenge%2C%20particularly%20as%20model%20scaling%20exacerbates%20sensitivity%20to%20algorithmic%20imprecision%20and%20training%20instability.%20Recent%20advances%20in%20optimizers%20have%20improved%20convergence%20efficiency%20through%20momentum%20orthogonalization%2C%20but%20suffer%20from%20two%20key%20robustness%20limitations%3A%20dimensional%20fragility%20in%20orthogonalization%20precision%20and%20vulnerability%20to%20outlier-induced%20noise.%20To%20address%20these%20robustness%20challenges%2C%20we%20introduce%20ROOT%2C%20a%20Robust%20Orthogonalized%20Optimizer%20that%20enhances%20training%20stability%20through%20dual%20robustness%20mechanisms.%20First%2C%20we%20develop%20a%20dimension-robust%20orthogonalization%20scheme%20using%20adaptive%20Newton%20iterations%20with%20fine-grained%20coefficients%20tailored%20to%20specific%20matrix%20sizes%2C%20ensuring%20consistent%20precision%20across%20diverse%20architectural%20configurations.%20Second%2C%20we%20introduce%20an%20optimization-robust%20framework%20via%20proximal%20optimization%20that%20suppresses%20outlier%20noise%20while%20preserving%20meaningful%20gradient%20directions.%20Extensive%20experiments%20demonstrate%20that%20ROOT%20achieves%20significantly%20improved%20robustness%2C%20with%20faster%20convergence%20and%20superior%20final%20performance%20compared%20to%20both%20Muon%20and%20Adam-based%20optimizers%2C%20particularly%20in%20noisy%20and%20non-convex%20scenarios.%20Our%20work%20establishes%20a%20new%20paradigm%20for%20developing%20robust%20and%20precise%20optimizers%20capable%20of%20handling%20the%20complexities%20of%20modern%20large-scale%20model%20training.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/huawei-noah/noah-research/tree/master/ROOT.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROOT%253A%2520Robust%2520Orthogonalized%2520Optimizer%2520for%2520Neural%2520Network%2520Training%26entry.906535625%3DWei%2520He%2520and%2520Kai%2520Han%2520and%2520Hang%2520Zhou%2520and%2520Hanting%2520Chen%2520and%2520Zhicheng%2520Liu%2520and%2520Xinghao%2520Chen%2520and%2520Yunhe%2520Wang%26entry.1292438233%3DThe%2520optimization%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520remains%2520a%2520critical%2520challenge%252C%2520particularly%2520as%2520model%2520scaling%2520exacerbates%2520sensitivity%2520to%2520algorithmic%2520imprecision%2520and%2520training%2520instability.%2520Recent%2520advances%2520in%2520optimizers%2520have%2520improved%2520convergence%2520efficiency%2520through%2520momentum%2520orthogonalization%252C%2520but%2520suffer%2520from%2520two%2520key%2520robustness%2520limitations%253A%2520dimensional%2520fragility%2520in%2520orthogonalization%2520precision%2520and%2520vulnerability%2520to%2520outlier-induced%2520noise.%2520To%2520address%2520these%2520robustness%2520challenges%252C%2520we%2520introduce%2520ROOT%252C%2520a%2520Robust%2520Orthogonalized%2520Optimizer%2520that%2520enhances%2520training%2520stability%2520through%2520dual%2520robustness%2520mechanisms.%2520First%252C%2520we%2520develop%2520a%2520dimension-robust%2520orthogonalization%2520scheme%2520using%2520adaptive%2520Newton%2520iterations%2520with%2520fine-grained%2520coefficients%2520tailored%2520to%2520specific%2520matrix%2520sizes%252C%2520ensuring%2520consistent%2520precision%2520across%2520diverse%2520architectural%2520configurations.%2520Second%252C%2520we%2520introduce%2520an%2520optimization-robust%2520framework%2520via%2520proximal%2520optimization%2520that%2520suppresses%2520outlier%2520noise%2520while%2520preserving%2520meaningful%2520gradient%2520directions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ROOT%2520achieves%2520significantly%2520improved%2520robustness%252C%2520with%2520faster%2520convergence%2520and%2520superior%2520final%2520performance%2520compared%2520to%2520both%2520Muon%2520and%2520Adam-based%2520optimizers%252C%2520particularly%2520in%2520noisy%2520and%2520non-convex%2520scenarios.%2520Our%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%2520developing%2520robust%2520and%2520precise%2520optimizers%2520capable%2520of%2520handling%2520the%2520complexities%2520of%2520modern%2520large-scale%2520model%2520training.%2520The%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/huawei-noah/noah-research/tree/master/ROOT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROOT%3A%20Robust%20Orthogonalized%20Optimizer%20for%20Neural%20Network%20Training&entry.906535625=Wei%20He%20and%20Kai%20Han%20and%20Hang%20Zhou%20and%20Hanting%20Chen%20and%20Zhicheng%20Liu%20and%20Xinghao%20Chen%20and%20Yunhe%20Wang&entry.1292438233=The%20optimization%20of%20large%20language%20models%20%28LLMs%29%20remains%20a%20critical%20challenge%2C%20particularly%20as%20model%20scaling%20exacerbates%20sensitivity%20to%20algorithmic%20imprecision%20and%20training%20instability.%20Recent%20advances%20in%20optimizers%20have%20improved%20convergence%20efficiency%20through%20momentum%20orthogonalization%2C%20but%20suffer%20from%20two%20key%20robustness%20limitations%3A%20dimensional%20fragility%20in%20orthogonalization%20precision%20and%20vulnerability%20to%20outlier-induced%20noise.%20To%20address%20these%20robustness%20challenges%2C%20we%20introduce%20ROOT%2C%20a%20Robust%20Orthogonalized%20Optimizer%20that%20enhances%20training%20stability%20through%20dual%20robustness%20mechanisms.%20First%2C%20we%20develop%20a%20dimension-robust%20orthogonalization%20scheme%20using%20adaptive%20Newton%20iterations%20with%20fine-grained%20coefficients%20tailored%20to%20specific%20matrix%20sizes%2C%20ensuring%20consistent%20precision%20across%20diverse%20architectural%20configurations.%20Second%2C%20we%20introduce%20an%20optimization-robust%20framework%20via%20proximal%20optimization%20that%20suppresses%20outlier%20noise%20while%20preserving%20meaningful%20gradient%20directions.%20Extensive%20experiments%20demonstrate%20that%20ROOT%20achieves%20significantly%20improved%20robustness%2C%20with%20faster%20convergence%20and%20superior%20final%20performance%20compared%20to%20both%20Muon%20and%20Adam-based%20optimizers%2C%20particularly%20in%20noisy%20and%20non-convex%20scenarios.%20Our%20work%20establishes%20a%20new%20paradigm%20for%20developing%20robust%20and%20precise%20optimizers%20capable%20of%20handling%20the%20complexities%20of%20modern%20large-scale%20model%20training.%20The%20code%20will%20be%20available%20at%20https%3A//github.com/huawei-noah/noah-research/tree/master/ROOT.&entry.1838667208=http%3A//arxiv.org/abs/2511.20626v1&entry.124074799=Read"},
{"title": "Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts", "author": "Mosab Rezaei and Mina Rajaei Moghadam and Abdul Rahman Shaikh and Hamed Alhoori and Reva Freedman", "abstract": "Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.", "link": "http://arxiv.org/abs/2511.20459v1", "date": "2025-11-25", "relevancy": 2.4285, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5195}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4805}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%2C%20Evaluation%2C%20and%20Explanation%20of%20Novelists%27%20Styles%20with%20Single-Token%20Prompts&body=Title%3A%20Generation%2C%20Evaluation%2C%20and%20Explanation%20of%20Novelists%27%20Styles%20with%20Single-Token%20Prompts%0AAuthor%3A%20Mosab%20Rezaei%20and%20Mina%20Rajaei%20Moghadam%20and%20Abdul%20Rahman%20Shaikh%20and%20Hamed%20Alhoori%20and%20Reva%20Freedman%0AAbstract%3A%20Recent%20advances%20in%20large%20language%20models%20have%20created%20new%20opportunities%20for%20stylometry%2C%20the%20study%20of%20writing%20styles%20and%20authorship.%20Two%20challenges%2C%20however%2C%20remain%20central%3A%20training%20generative%20models%20when%20no%20paired%20data%20exist%2C%20and%20evaluating%20stylistic%20text%20without%20relying%20only%20on%20human%20judgment.%20In%20this%20work%2C%20we%20present%20a%20framework%20for%20both%20generating%20and%20evaluating%20sentences%20in%20the%20style%20of%2019th-century%20novelists.%20Large%20language%20models%20are%20fine-tuned%20with%20minimal%2C%20single-token%20prompts%20to%20produce%20text%20in%20the%20voices%20of%20authors%20such%20as%20Dickens%2C%20Austen%2C%20Twain%2C%20Alcott%2C%20and%20Melville.%20To%20assess%20these%20generative%20models%2C%20we%20employ%20a%20transformer-based%20detector%20trained%20on%20authentic%20sentences%2C%20using%20it%20both%20as%20a%20classifier%20and%20as%20a%20tool%20for%20stylistic%20explanation.%20We%20complement%20this%20with%20syntactic%20comparisons%20and%20explainable%20AI%20methods%2C%20including%20attention-based%20and%20gradient-based%20analyses%2C%20to%20identify%20the%20linguistic%20cues%20that%20drive%20stylistic%20imitation.%20Our%20findings%20show%20that%20the%20generated%20text%20reflects%20the%20authors%27%20distinctive%20patterns%20and%20that%20AI-based%20evaluation%20offers%20a%20reliable%20alternative%20to%20human%20assessment.%20All%20artifacts%20of%20this%20work%20are%20published%20online.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%252C%2520Evaluation%252C%2520and%2520Explanation%2520of%2520Novelists%2527%2520Styles%2520with%2520Single-Token%2520Prompts%26entry.906535625%3DMosab%2520Rezaei%2520and%2520Mina%2520Rajaei%2520Moghadam%2520and%2520Abdul%2520Rahman%2520Shaikh%2520and%2520Hamed%2520Alhoori%2520and%2520Reva%2520Freedman%26entry.1292438233%3DRecent%2520advances%2520in%2520large%2520language%2520models%2520have%2520created%2520new%2520opportunities%2520for%2520stylometry%252C%2520the%2520study%2520of%2520writing%2520styles%2520and%2520authorship.%2520Two%2520challenges%252C%2520however%252C%2520remain%2520central%253A%2520training%2520generative%2520models%2520when%2520no%2520paired%2520data%2520exist%252C%2520and%2520evaluating%2520stylistic%2520text%2520without%2520relying%2520only%2520on%2520human%2520judgment.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520framework%2520for%2520both%2520generating%2520and%2520evaluating%2520sentences%2520in%2520the%2520style%2520of%252019th-century%2520novelists.%2520Large%2520language%2520models%2520are%2520fine-tuned%2520with%2520minimal%252C%2520single-token%2520prompts%2520to%2520produce%2520text%2520in%2520the%2520voices%2520of%2520authors%2520such%2520as%2520Dickens%252C%2520Austen%252C%2520Twain%252C%2520Alcott%252C%2520and%2520Melville.%2520To%2520assess%2520these%2520generative%2520models%252C%2520we%2520employ%2520a%2520transformer-based%2520detector%2520trained%2520on%2520authentic%2520sentences%252C%2520using%2520it%2520both%2520as%2520a%2520classifier%2520and%2520as%2520a%2520tool%2520for%2520stylistic%2520explanation.%2520We%2520complement%2520this%2520with%2520syntactic%2520comparisons%2520and%2520explainable%2520AI%2520methods%252C%2520including%2520attention-based%2520and%2520gradient-based%2520analyses%252C%2520to%2520identify%2520the%2520linguistic%2520cues%2520that%2520drive%2520stylistic%2520imitation.%2520Our%2520findings%2520show%2520that%2520the%2520generated%2520text%2520reflects%2520the%2520authors%2527%2520distinctive%2520patterns%2520and%2520that%2520AI-based%2520evaluation%2520offers%2520a%2520reliable%2520alternative%2520to%2520human%2520assessment.%2520All%2520artifacts%2520of%2520this%2520work%2520are%2520published%2520online.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%2C%20Evaluation%2C%20and%20Explanation%20of%20Novelists%27%20Styles%20with%20Single-Token%20Prompts&entry.906535625=Mosab%20Rezaei%20and%20Mina%20Rajaei%20Moghadam%20and%20Abdul%20Rahman%20Shaikh%20and%20Hamed%20Alhoori%20and%20Reva%20Freedman&entry.1292438233=Recent%20advances%20in%20large%20language%20models%20have%20created%20new%20opportunities%20for%20stylometry%2C%20the%20study%20of%20writing%20styles%20and%20authorship.%20Two%20challenges%2C%20however%2C%20remain%20central%3A%20training%20generative%20models%20when%20no%20paired%20data%20exist%2C%20and%20evaluating%20stylistic%20text%20without%20relying%20only%20on%20human%20judgment.%20In%20this%20work%2C%20we%20present%20a%20framework%20for%20both%20generating%20and%20evaluating%20sentences%20in%20the%20style%20of%2019th-century%20novelists.%20Large%20language%20models%20are%20fine-tuned%20with%20minimal%2C%20single-token%20prompts%20to%20produce%20text%20in%20the%20voices%20of%20authors%20such%20as%20Dickens%2C%20Austen%2C%20Twain%2C%20Alcott%2C%20and%20Melville.%20To%20assess%20these%20generative%20models%2C%20we%20employ%20a%20transformer-based%20detector%20trained%20on%20authentic%20sentences%2C%20using%20it%20both%20as%20a%20classifier%20and%20as%20a%20tool%20for%20stylistic%20explanation.%20We%20complement%20this%20with%20syntactic%20comparisons%20and%20explainable%20AI%20methods%2C%20including%20attention-based%20and%20gradient-based%20analyses%2C%20to%20identify%20the%20linguistic%20cues%20that%20drive%20stylistic%20imitation.%20Our%20findings%20show%20that%20the%20generated%20text%20reflects%20the%20authors%27%20distinctive%20patterns%20and%20that%20AI-based%20evaluation%20offers%20a%20reliable%20alternative%20to%20human%20assessment.%20All%20artifacts%20of%20this%20work%20are%20published%20online.&entry.1838667208=http%3A//arxiv.org/abs/2511.20459v1&entry.124074799=Read"},
{"title": "Demystifying Higher-Order Graph Neural Networks", "author": "Maciej Besta and Florian Scheidl and Lukas Gianinazzi and Grzegorz Kwasniewski and Shachar Klaiman and J\u00fcrgen M\u00fcller and Torsten Hoefler", "abstract": "Higher-order graph neural networks (HOGNNs) and the related architectures from Topological Deep Learning are an important class of GNN models that harness polyadic relations between vertices beyond plain edges. They have been used to eliminate issues such as over-smoothing or over-squashing, to significantly enhance the accuracy of GNN predictions, to improve the expressiveness of GNN architectures, and for numerous other goals. A plethora of HOGNN models have been introduced, and they come with diverse neural architectures, and even with different notions of what the \"higher-order\" means. This richness makes it very challenging to appropriately analyze and compare HOGNN models, and to decide in what scenario to use specific ones. To alleviate this, we first design an in-depth taxonomy and a blueprint for HOGNNs. This facilitates designing models that maximize performance. Then, we use our taxonomy to analyze and compare the available HOGNN models. The outcomes of our analysis are synthesized in a set of insights that help to select the most beneficial GNN model in a given scenario, and a comprehensive list of challenges and opportunities for further research into more powerful HOGNNs.", "link": "http://arxiv.org/abs/2406.12841v4", "date": "2025-11-25", "relevancy": 2.4278, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4939}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4865}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20Higher-Order%20Graph%20Neural%20Networks&body=Title%3A%20Demystifying%20Higher-Order%20Graph%20Neural%20Networks%0AAuthor%3A%20Maciej%20Besta%20and%20Florian%20Scheidl%20and%20Lukas%20Gianinazzi%20and%20Grzegorz%20Kwasniewski%20and%20Shachar%20Klaiman%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20Torsten%20Hoefler%0AAbstract%3A%20Higher-order%20graph%20neural%20networks%20%28HOGNNs%29%20and%20the%20related%20architectures%20from%20Topological%20Deep%20Learning%20are%20an%20important%20class%20of%20GNN%20models%20that%20harness%20polyadic%20relations%20between%20vertices%20beyond%20plain%20edges.%20They%20have%20been%20used%20to%20eliminate%20issues%20such%20as%20over-smoothing%20or%20over-squashing%2C%20to%20significantly%20enhance%20the%20accuracy%20of%20GNN%20predictions%2C%20to%20improve%20the%20expressiveness%20of%20GNN%20architectures%2C%20and%20for%20numerous%20other%20goals.%20A%20plethora%20of%20HOGNN%20models%20have%20been%20introduced%2C%20and%20they%20come%20with%20diverse%20neural%20architectures%2C%20and%20even%20with%20different%20notions%20of%20what%20the%20%22higher-order%22%20means.%20This%20richness%20makes%20it%20very%20challenging%20to%20appropriately%20analyze%20and%20compare%20HOGNN%20models%2C%20and%20to%20decide%20in%20what%20scenario%20to%20use%20specific%20ones.%20To%20alleviate%20this%2C%20we%20first%20design%20an%20in-depth%20taxonomy%20and%20a%20blueprint%20for%20HOGNNs.%20This%20facilitates%20designing%20models%20that%20maximize%20performance.%20Then%2C%20we%20use%20our%20taxonomy%20to%20analyze%20and%20compare%20the%20available%20HOGNN%20models.%20The%20outcomes%20of%20our%20analysis%20are%20synthesized%20in%20a%20set%20of%20insights%20that%20help%20to%20select%20the%20most%20beneficial%20GNN%20model%20in%20a%20given%20scenario%2C%20and%20a%20comprehensive%20list%20of%20challenges%20and%20opportunities%20for%20further%20research%20into%20more%20powerful%20HOGNNs.%0ALink%3A%20http%3A//arxiv.org/abs/2406.12841v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520Higher-Order%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMaciej%2520Besta%2520and%2520Florian%2520Scheidl%2520and%2520Lukas%2520Gianinazzi%2520and%2520Grzegorz%2520Kwasniewski%2520and%2520Shachar%2520Klaiman%2520and%2520J%25C3%25BCrgen%2520M%25C3%25BCller%2520and%2520Torsten%2520Hoefler%26entry.1292438233%3DHigher-order%2520graph%2520neural%2520networks%2520%2528HOGNNs%2529%2520and%2520the%2520related%2520architectures%2520from%2520Topological%2520Deep%2520Learning%2520are%2520an%2520important%2520class%2520of%2520GNN%2520models%2520that%2520harness%2520polyadic%2520relations%2520between%2520vertices%2520beyond%2520plain%2520edges.%2520They%2520have%2520been%2520used%2520to%2520eliminate%2520issues%2520such%2520as%2520over-smoothing%2520or%2520over-squashing%252C%2520to%2520significantly%2520enhance%2520the%2520accuracy%2520of%2520GNN%2520predictions%252C%2520to%2520improve%2520the%2520expressiveness%2520of%2520GNN%2520architectures%252C%2520and%2520for%2520numerous%2520other%2520goals.%2520A%2520plethora%2520of%2520HOGNN%2520models%2520have%2520been%2520introduced%252C%2520and%2520they%2520come%2520with%2520diverse%2520neural%2520architectures%252C%2520and%2520even%2520with%2520different%2520notions%2520of%2520what%2520the%2520%2522higher-order%2522%2520means.%2520This%2520richness%2520makes%2520it%2520very%2520challenging%2520to%2520appropriately%2520analyze%2520and%2520compare%2520HOGNN%2520models%252C%2520and%2520to%2520decide%2520in%2520what%2520scenario%2520to%2520use%2520specific%2520ones.%2520To%2520alleviate%2520this%252C%2520we%2520first%2520design%2520an%2520in-depth%2520taxonomy%2520and%2520a%2520blueprint%2520for%2520HOGNNs.%2520This%2520facilitates%2520designing%2520models%2520that%2520maximize%2520performance.%2520Then%252C%2520we%2520use%2520our%2520taxonomy%2520to%2520analyze%2520and%2520compare%2520the%2520available%2520HOGNN%2520models.%2520The%2520outcomes%2520of%2520our%2520analysis%2520are%2520synthesized%2520in%2520a%2520set%2520of%2520insights%2520that%2520help%2520to%2520select%2520the%2520most%2520beneficial%2520GNN%2520model%2520in%2520a%2520given%2520scenario%252C%2520and%2520a%2520comprehensive%2520list%2520of%2520challenges%2520and%2520opportunities%2520for%2520further%2520research%2520into%2520more%2520powerful%2520HOGNNs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12841v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20Higher-Order%20Graph%20Neural%20Networks&entry.906535625=Maciej%20Besta%20and%20Florian%20Scheidl%20and%20Lukas%20Gianinazzi%20and%20Grzegorz%20Kwasniewski%20and%20Shachar%20Klaiman%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20Torsten%20Hoefler&entry.1292438233=Higher-order%20graph%20neural%20networks%20%28HOGNNs%29%20and%20the%20related%20architectures%20from%20Topological%20Deep%20Learning%20are%20an%20important%20class%20of%20GNN%20models%20that%20harness%20polyadic%20relations%20between%20vertices%20beyond%20plain%20edges.%20They%20have%20been%20used%20to%20eliminate%20issues%20such%20as%20over-smoothing%20or%20over-squashing%2C%20to%20significantly%20enhance%20the%20accuracy%20of%20GNN%20predictions%2C%20to%20improve%20the%20expressiveness%20of%20GNN%20architectures%2C%20and%20for%20numerous%20other%20goals.%20A%20plethora%20of%20HOGNN%20models%20have%20been%20introduced%2C%20and%20they%20come%20with%20diverse%20neural%20architectures%2C%20and%20even%20with%20different%20notions%20of%20what%20the%20%22higher-order%22%20means.%20This%20richness%20makes%20it%20very%20challenging%20to%20appropriately%20analyze%20and%20compare%20HOGNN%20models%2C%20and%20to%20decide%20in%20what%20scenario%20to%20use%20specific%20ones.%20To%20alleviate%20this%2C%20we%20first%20design%20an%20in-depth%20taxonomy%20and%20a%20blueprint%20for%20HOGNNs.%20This%20facilitates%20designing%20models%20that%20maximize%20performance.%20Then%2C%20we%20use%20our%20taxonomy%20to%20analyze%20and%20compare%20the%20available%20HOGNN%20models.%20The%20outcomes%20of%20our%20analysis%20are%20synthesized%20in%20a%20set%20of%20insights%20that%20help%20to%20select%20the%20most%20beneficial%20GNN%20model%20in%20a%20given%20scenario%2C%20and%20a%20comprehensive%20list%20of%20challenges%20and%20opportunities%20for%20further%20research%20into%20more%20powerful%20HOGNNs.&entry.1838667208=http%3A//arxiv.org/abs/2406.12841v4&entry.124074799=Read"},
{"title": "CORE -- A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment", "author": "Esha Sadia Nasir and Behnaz Elhaminia and Mark Eastwood and Catherine King and Owen Cain and Lorraine Harper and Paul Moss and Dimitrios Chanouzas and David Snead and Nasir Rajpoot and Adam Shephard and Shan E Ahmed Raza", "abstract": "Accurate and efficient registration of whole slide images (WSIs) is essential for high-resolution, nuclei-level analysis in multi-stained tissue slides. We propose a novel coarse-to-fine framework CORE for accurate nuclei-level registration across diverse multimodal whole-slide image (WSI) datasets. The coarse registration stage leverages prompt-based tissue mask extraction to effectively filter out artefacts and non-tissue regions, followed by global alignment using tissue morphology and ac- celerated dense feature matching with a pre-trained feature extractor. From the coarsely aligned slides, nuclei centroids are detected and subjected to fine-grained rigid registration using a custom, shape-aware point-set registration model. Finally, non-rigid alignment at the cellular level is achieved by estimating a non-linear dis- placement field using Coherent Point Drift (CPD). Our approach benefits from automatically generated nuclei that enhance the accuracy of deformable registra- tion and ensure precise nuclei-level correspondence across modalities. The pro- posed model is evaluated on three publicly available WSI registration datasets, and two private datasets. We show that CORE outperforms current state-of-the-art methods in terms of generalisability, precision, and robustness in bright-field and immunofluorescence microscopy WSIs", "link": "http://arxiv.org/abs/2511.03826v3", "date": "2025-11-25", "relevancy": 2.4116, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5322}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4654}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CORE%20--%20A%20Cell-Level%20Coarse-to-Fine%20Image%20Registration%20Engine%20for%20Multi-stain%20Image%20Alignment&body=Title%3A%20CORE%20--%20A%20Cell-Level%20Coarse-to-Fine%20Image%20Registration%20Engine%20for%20Multi-stain%20Image%20Alignment%0AAuthor%3A%20Esha%20Sadia%20Nasir%20and%20Behnaz%20Elhaminia%20and%20Mark%20Eastwood%20and%20Catherine%20King%20and%20Owen%20Cain%20and%20Lorraine%20Harper%20and%20Paul%20Moss%20and%20Dimitrios%20Chanouzas%20and%20David%20Snead%20and%20Nasir%20Rajpoot%20and%20Adam%20Shephard%20and%20Shan%20E%20Ahmed%20Raza%0AAbstract%3A%20Accurate%20and%20efficient%20registration%20of%20whole%20slide%20images%20%28WSIs%29%20is%20essential%20for%20high-resolution%2C%20nuclei-level%20analysis%20in%20multi-stained%20tissue%20slides.%20We%20propose%20a%20novel%20coarse-to-fine%20framework%20CORE%20for%20accurate%20nuclei-level%20registration%20across%20diverse%20multimodal%20whole-slide%20image%20%28WSI%29%20datasets.%20The%20coarse%20registration%20stage%20leverages%20prompt-based%20tissue%20mask%20extraction%20to%20effectively%20filter%20out%20artefacts%20and%20non-tissue%20regions%2C%20followed%20by%20global%20alignment%20using%20tissue%20morphology%20and%20ac-%20celerated%20dense%20feature%20matching%20with%20a%20pre-trained%20feature%20extractor.%20From%20the%20coarsely%20aligned%20slides%2C%20nuclei%20centroids%20are%20detected%20and%20subjected%20to%20fine-grained%20rigid%20registration%20using%20a%20custom%2C%20shape-aware%20point-set%20registration%20model.%20Finally%2C%20non-rigid%20alignment%20at%20the%20cellular%20level%20is%20achieved%20by%20estimating%20a%20non-linear%20dis-%20placement%20field%20using%20Coherent%20Point%20Drift%20%28CPD%29.%20Our%20approach%20benefits%20from%20automatically%20generated%20nuclei%20that%20enhance%20the%20accuracy%20of%20deformable%20registra-%20tion%20and%20ensure%20precise%20nuclei-level%20correspondence%20across%20modalities.%20The%20pro-%20posed%20model%20is%20evaluated%20on%20three%20publicly%20available%20WSI%20registration%20datasets%2C%20and%20two%20private%20datasets.%20We%20show%20that%20CORE%20outperforms%20current%20state-of-the-art%20methods%20in%20terms%20of%20generalisability%2C%20precision%2C%20and%20robustness%20in%20bright-field%20and%20immunofluorescence%20microscopy%20WSIs%0ALink%3A%20http%3A//arxiv.org/abs/2511.03826v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCORE%2520--%2520A%2520Cell-Level%2520Coarse-to-Fine%2520Image%2520Registration%2520Engine%2520for%2520Multi-stain%2520Image%2520Alignment%26entry.906535625%3DEsha%2520Sadia%2520Nasir%2520and%2520Behnaz%2520Elhaminia%2520and%2520Mark%2520Eastwood%2520and%2520Catherine%2520King%2520and%2520Owen%2520Cain%2520and%2520Lorraine%2520Harper%2520and%2520Paul%2520Moss%2520and%2520Dimitrios%2520Chanouzas%2520and%2520David%2520Snead%2520and%2520Nasir%2520Rajpoot%2520and%2520Adam%2520Shephard%2520and%2520Shan%2520E%2520Ahmed%2520Raza%26entry.1292438233%3DAccurate%2520and%2520efficient%2520registration%2520of%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520is%2520essential%2520for%2520high-resolution%252C%2520nuclei-level%2520analysis%2520in%2520multi-stained%2520tissue%2520slides.%2520We%2520propose%2520a%2520novel%2520coarse-to-fine%2520framework%2520CORE%2520for%2520accurate%2520nuclei-level%2520registration%2520across%2520diverse%2520multimodal%2520whole-slide%2520image%2520%2528WSI%2529%2520datasets.%2520The%2520coarse%2520registration%2520stage%2520leverages%2520prompt-based%2520tissue%2520mask%2520extraction%2520to%2520effectively%2520filter%2520out%2520artefacts%2520and%2520non-tissue%2520regions%252C%2520followed%2520by%2520global%2520alignment%2520using%2520tissue%2520morphology%2520and%2520ac-%2520celerated%2520dense%2520feature%2520matching%2520with%2520a%2520pre-trained%2520feature%2520extractor.%2520From%2520the%2520coarsely%2520aligned%2520slides%252C%2520nuclei%2520centroids%2520are%2520detected%2520and%2520subjected%2520to%2520fine-grained%2520rigid%2520registration%2520using%2520a%2520custom%252C%2520shape-aware%2520point-set%2520registration%2520model.%2520Finally%252C%2520non-rigid%2520alignment%2520at%2520the%2520cellular%2520level%2520is%2520achieved%2520by%2520estimating%2520a%2520non-linear%2520dis-%2520placement%2520field%2520using%2520Coherent%2520Point%2520Drift%2520%2528CPD%2529.%2520Our%2520approach%2520benefits%2520from%2520automatically%2520generated%2520nuclei%2520that%2520enhance%2520the%2520accuracy%2520of%2520deformable%2520registra-%2520tion%2520and%2520ensure%2520precise%2520nuclei-level%2520correspondence%2520across%2520modalities.%2520The%2520pro-%2520posed%2520model%2520is%2520evaluated%2520on%2520three%2520publicly%2520available%2520WSI%2520registration%2520datasets%252C%2520and%2520two%2520private%2520datasets.%2520We%2520show%2520that%2520CORE%2520outperforms%2520current%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520generalisability%252C%2520precision%252C%2520and%2520robustness%2520in%2520bright-field%2520and%2520immunofluorescence%2520microscopy%2520WSIs%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03826v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CORE%20--%20A%20Cell-Level%20Coarse-to-Fine%20Image%20Registration%20Engine%20for%20Multi-stain%20Image%20Alignment&entry.906535625=Esha%20Sadia%20Nasir%20and%20Behnaz%20Elhaminia%20and%20Mark%20Eastwood%20and%20Catherine%20King%20and%20Owen%20Cain%20and%20Lorraine%20Harper%20and%20Paul%20Moss%20and%20Dimitrios%20Chanouzas%20and%20David%20Snead%20and%20Nasir%20Rajpoot%20and%20Adam%20Shephard%20and%20Shan%20E%20Ahmed%20Raza&entry.1292438233=Accurate%20and%20efficient%20registration%20of%20whole%20slide%20images%20%28WSIs%29%20is%20essential%20for%20high-resolution%2C%20nuclei-level%20analysis%20in%20multi-stained%20tissue%20slides.%20We%20propose%20a%20novel%20coarse-to-fine%20framework%20CORE%20for%20accurate%20nuclei-level%20registration%20across%20diverse%20multimodal%20whole-slide%20image%20%28WSI%29%20datasets.%20The%20coarse%20registration%20stage%20leverages%20prompt-based%20tissue%20mask%20extraction%20to%20effectively%20filter%20out%20artefacts%20and%20non-tissue%20regions%2C%20followed%20by%20global%20alignment%20using%20tissue%20morphology%20and%20ac-%20celerated%20dense%20feature%20matching%20with%20a%20pre-trained%20feature%20extractor.%20From%20the%20coarsely%20aligned%20slides%2C%20nuclei%20centroids%20are%20detected%20and%20subjected%20to%20fine-grained%20rigid%20registration%20using%20a%20custom%2C%20shape-aware%20point-set%20registration%20model.%20Finally%2C%20non-rigid%20alignment%20at%20the%20cellular%20level%20is%20achieved%20by%20estimating%20a%20non-linear%20dis-%20placement%20field%20using%20Coherent%20Point%20Drift%20%28CPD%29.%20Our%20approach%20benefits%20from%20automatically%20generated%20nuclei%20that%20enhance%20the%20accuracy%20of%20deformable%20registra-%20tion%20and%20ensure%20precise%20nuclei-level%20correspondence%20across%20modalities.%20The%20pro-%20posed%20model%20is%20evaluated%20on%20three%20publicly%20available%20WSI%20registration%20datasets%2C%20and%20two%20private%20datasets.%20We%20show%20that%20CORE%20outperforms%20current%20state-of-the-art%20methods%20in%20terms%20of%20generalisability%2C%20precision%2C%20and%20robustness%20in%20bright-field%20and%20immunofluorescence%20microscopy%20WSIs&entry.1838667208=http%3A//arxiv.org/abs/2511.03826v3&entry.124074799=Read"},
{"title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow", "author": "Jiatao Gu and Ying Shen and Tianrong Chen and Laurent Dinh and Yuyang Wang and Miguel Angel Bautista and David Berthelot and Josh Susskind and Shuangfei Zhai", "abstract": "Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.", "link": "http://arxiv.org/abs/2511.20462v1", "date": "2025-11-25", "relevancy": 2.411, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6404}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5959}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STARFlow-V%3A%20End-to-End%20Video%20Generative%20Modeling%20with%20Normalizing%20Flow&body=Title%3A%20STARFlow-V%3A%20End-to-End%20Video%20Generative%20Modeling%20with%20Normalizing%20Flow%0AAuthor%3A%20Jiatao%20Gu%20and%20Ying%20Shen%20and%20Tianrong%20Chen%20and%20Laurent%20Dinh%20and%20Yuyang%20Wang%20and%20Miguel%20Angel%20Bautista%20and%20David%20Berthelot%20and%20Josh%20Susskind%20and%20Shuangfei%20Zhai%0AAbstract%3A%20Normalizing%20flows%20%28NFs%29%20are%20end-to-end%20likelihood-based%20generative%20models%20for%20continuous%20data%2C%20and%20have%20recently%20regained%20attention%20with%20encouraging%20progress%20on%20image%20generation.%20Yet%20in%20the%20video%20generation%20domain%2C%20where%20spatiotemporal%20complexity%20and%20computational%20cost%20are%20substantially%20higher%2C%20state-of-the-art%20systems%20almost%20exclusively%20rely%20on%20diffusion-based%20models.%20In%20this%20work%2C%20we%20revisit%20this%20design%20space%20by%20presenting%20STARFlow-V%2C%20a%20normalizing%20flow-based%20video%20generator%20with%20substantial%20benefits%20such%20as%20end-to-end%20learning%2C%20robust%20causal%20prediction%2C%20and%20native%20likelihood%20estimation.%20Building%20upon%20the%20recently%20proposed%20STARFlow%2C%20STARFlow-V%20operates%20in%20the%20spatiotemporal%20latent%20space%20with%20a%20global-local%20architecture%20which%20restricts%20causal%20dependencies%20to%20a%20global%20latent%20space%20while%20preserving%20rich%20local%20within-frame%20interactions.%20This%20eases%20error%20accumulation%20over%20time%2C%20a%20common%20pitfall%20of%20standard%20autoregressive%20diffusion%20model%20generation.%20Additionally%2C%20we%20propose%20flow-score%20matching%2C%20which%20equips%20the%20model%20with%20a%20light-weight%20causal%20denoiser%20to%20improve%20the%20video%20generation%20consistency%20in%20an%20autoregressive%20fashion.%20To%20improve%20the%20sampling%20efficiency%2C%20STARFlow-V%20employs%20a%20video-aware%20Jacobi%20iteration%20scheme%20that%20recasts%20inner%20updates%20as%20parallelizable%20iterations%20without%20breaking%20causality.%20Thanks%20to%20the%20invertible%20structure%2C%20the%20same%20model%20can%20natively%20support%20text-to-video%2C%20image-to-video%20as%20well%20as%20video-to-video%20generation%20tasks.%20Empirically%2C%20STARFlow-V%20achieves%20strong%20visual%20fidelity%20and%20temporal%20consistency%20with%20practical%20sampling%20throughput%20relative%20to%20diffusion-based%20baselines.%20These%20results%20present%20the%20first%20evidence%2C%20to%20our%20knowledge%2C%20that%20NFs%20are%20capable%20of%20high-quality%20autoregressive%20video%20generation%2C%20establishing%20them%20as%20a%20promising%20research%20direction%20for%20building%20world%20models.%20Code%20and%20generated%20samples%20are%20available%20at%20https%3A//github.com/apple/ml-starflow.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTARFlow-V%253A%2520End-to-End%2520Video%2520Generative%2520Modeling%2520with%2520Normalizing%2520Flow%26entry.906535625%3DJiatao%2520Gu%2520and%2520Ying%2520Shen%2520and%2520Tianrong%2520Chen%2520and%2520Laurent%2520Dinh%2520and%2520Yuyang%2520Wang%2520and%2520Miguel%2520Angel%2520Bautista%2520and%2520David%2520Berthelot%2520and%2520Josh%2520Susskind%2520and%2520Shuangfei%2520Zhai%26entry.1292438233%3DNormalizing%2520flows%2520%2528NFs%2529%2520are%2520end-to-end%2520likelihood-based%2520generative%2520models%2520for%2520continuous%2520data%252C%2520and%2520have%2520recently%2520regained%2520attention%2520with%2520encouraging%2520progress%2520on%2520image%2520generation.%2520Yet%2520in%2520the%2520video%2520generation%2520domain%252C%2520where%2520spatiotemporal%2520complexity%2520and%2520computational%2520cost%2520are%2520substantially%2520higher%252C%2520state-of-the-art%2520systems%2520almost%2520exclusively%2520rely%2520on%2520diffusion-based%2520models.%2520In%2520this%2520work%252C%2520we%2520revisit%2520this%2520design%2520space%2520by%2520presenting%2520STARFlow-V%252C%2520a%2520normalizing%2520flow-based%2520video%2520generator%2520with%2520substantial%2520benefits%2520such%2520as%2520end-to-end%2520learning%252C%2520robust%2520causal%2520prediction%252C%2520and%2520native%2520likelihood%2520estimation.%2520Building%2520upon%2520the%2520recently%2520proposed%2520STARFlow%252C%2520STARFlow-V%2520operates%2520in%2520the%2520spatiotemporal%2520latent%2520space%2520with%2520a%2520global-local%2520architecture%2520which%2520restricts%2520causal%2520dependencies%2520to%2520a%2520global%2520latent%2520space%2520while%2520preserving%2520rich%2520local%2520within-frame%2520interactions.%2520This%2520eases%2520error%2520accumulation%2520over%2520time%252C%2520a%2520common%2520pitfall%2520of%2520standard%2520autoregressive%2520diffusion%2520model%2520generation.%2520Additionally%252C%2520we%2520propose%2520flow-score%2520matching%252C%2520which%2520equips%2520the%2520model%2520with%2520a%2520light-weight%2520causal%2520denoiser%2520to%2520improve%2520the%2520video%2520generation%2520consistency%2520in%2520an%2520autoregressive%2520fashion.%2520To%2520improve%2520the%2520sampling%2520efficiency%252C%2520STARFlow-V%2520employs%2520a%2520video-aware%2520Jacobi%2520iteration%2520scheme%2520that%2520recasts%2520inner%2520updates%2520as%2520parallelizable%2520iterations%2520without%2520breaking%2520causality.%2520Thanks%2520to%2520the%2520invertible%2520structure%252C%2520the%2520same%2520model%2520can%2520natively%2520support%2520text-to-video%252C%2520image-to-video%2520as%2520well%2520as%2520video-to-video%2520generation%2520tasks.%2520Empirically%252C%2520STARFlow-V%2520achieves%2520strong%2520visual%2520fidelity%2520and%2520temporal%2520consistency%2520with%2520practical%2520sampling%2520throughput%2520relative%2520to%2520diffusion-based%2520baselines.%2520These%2520results%2520present%2520the%2520first%2520evidence%252C%2520to%2520our%2520knowledge%252C%2520that%2520NFs%2520are%2520capable%2520of%2520high-quality%2520autoregressive%2520video%2520generation%252C%2520establishing%2520them%2520as%2520a%2520promising%2520research%2520direction%2520for%2520building%2520world%2520models.%2520Code%2520and%2520generated%2520samples%2520are%2520available%2520at%2520https%253A//github.com/apple/ml-starflow.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STARFlow-V%3A%20End-to-End%20Video%20Generative%20Modeling%20with%20Normalizing%20Flow&entry.906535625=Jiatao%20Gu%20and%20Ying%20Shen%20and%20Tianrong%20Chen%20and%20Laurent%20Dinh%20and%20Yuyang%20Wang%20and%20Miguel%20Angel%20Bautista%20and%20David%20Berthelot%20and%20Josh%20Susskind%20and%20Shuangfei%20Zhai&entry.1292438233=Normalizing%20flows%20%28NFs%29%20are%20end-to-end%20likelihood-based%20generative%20models%20for%20continuous%20data%2C%20and%20have%20recently%20regained%20attention%20with%20encouraging%20progress%20on%20image%20generation.%20Yet%20in%20the%20video%20generation%20domain%2C%20where%20spatiotemporal%20complexity%20and%20computational%20cost%20are%20substantially%20higher%2C%20state-of-the-art%20systems%20almost%20exclusively%20rely%20on%20diffusion-based%20models.%20In%20this%20work%2C%20we%20revisit%20this%20design%20space%20by%20presenting%20STARFlow-V%2C%20a%20normalizing%20flow-based%20video%20generator%20with%20substantial%20benefits%20such%20as%20end-to-end%20learning%2C%20robust%20causal%20prediction%2C%20and%20native%20likelihood%20estimation.%20Building%20upon%20the%20recently%20proposed%20STARFlow%2C%20STARFlow-V%20operates%20in%20the%20spatiotemporal%20latent%20space%20with%20a%20global-local%20architecture%20which%20restricts%20causal%20dependencies%20to%20a%20global%20latent%20space%20while%20preserving%20rich%20local%20within-frame%20interactions.%20This%20eases%20error%20accumulation%20over%20time%2C%20a%20common%20pitfall%20of%20standard%20autoregressive%20diffusion%20model%20generation.%20Additionally%2C%20we%20propose%20flow-score%20matching%2C%20which%20equips%20the%20model%20with%20a%20light-weight%20causal%20denoiser%20to%20improve%20the%20video%20generation%20consistency%20in%20an%20autoregressive%20fashion.%20To%20improve%20the%20sampling%20efficiency%2C%20STARFlow-V%20employs%20a%20video-aware%20Jacobi%20iteration%20scheme%20that%20recasts%20inner%20updates%20as%20parallelizable%20iterations%20without%20breaking%20causality.%20Thanks%20to%20the%20invertible%20structure%2C%20the%20same%20model%20can%20natively%20support%20text-to-video%2C%20image-to-video%20as%20well%20as%20video-to-video%20generation%20tasks.%20Empirically%2C%20STARFlow-V%20achieves%20strong%20visual%20fidelity%20and%20temporal%20consistency%20with%20practical%20sampling%20throughput%20relative%20to%20diffusion-based%20baselines.%20These%20results%20present%20the%20first%20evidence%2C%20to%20our%20knowledge%2C%20that%20NFs%20are%20capable%20of%20high-quality%20autoregressive%20video%20generation%2C%20establishing%20them%20as%20a%20promising%20research%20direction%20for%20building%20world%20models.%20Code%20and%20generated%20samples%20are%20available%20at%20https%3A//github.com/apple/ml-starflow.&entry.1838667208=http%3A//arxiv.org/abs/2511.20462v1&entry.124074799=Read"},
{"title": "LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation", "author": "Cristian Minoccheri and Matthew Hodgman and Haoyuan Ma and Rameez Merchant and Emily Wittrup and Craig Williamson and Kayvan Najarian", "abstract": "Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological emergency with mortality rates exceeding 30%. Transfer learning from related hematoma types represents a potentially valuable but underexplored approach. Although Unet architectures remain the gold standard for medical image segmentation due to their effectiveness on limited datasets, Low-Rank Adaptation (LoRA) methods for parameter-efficient transfer learning have been rarely applied to convolutional neural networks in medical imaging contexts. We implemented a Unet architecture pre-trained on computed tomography scans from 124 traumatic brain injury patients across multiple institutions, then fine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health System using 3-fold cross-validation. We developed a novel CP-LoRA method based on tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA, CP-DoRA) that decompose weight matrices into magnitude and directional components. We compared these approaches against existing LoRA methods (LoRA-C, convLoRA) and standard fine-tuning strategies across different modules on a multi-view Unet model. LoRA-based methods consistently outperformed standard Unet fine-tuning. Performance varied by hemorrhage volume, with all methods showing improved accuracy for larger volumes. CP-LoRA achieved comparable performance to existing methods while using significantly fewer parameters. Over-parameterization with higher ranks consistently yielded better performance than strictly low-rank adaptations. This study demonstrates that transfer learning between hematoma types is feasible and that LoRA-based methods significantly outperform conventional Unet fine-tuning for aneurysmal SAH segmentation.", "link": "http://arxiv.org/abs/2508.01772v3", "date": "2025-11-25", "relevancy": 2.405, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-based%20methods%20on%20Unet%20for%20transfer%20learning%20in%20Subarachnoid%20Hematoma%20Segmentation&body=Title%3A%20LoRA-based%20methods%20on%20Unet%20for%20transfer%20learning%20in%20Subarachnoid%20Hematoma%20Segmentation%0AAuthor%3A%20Cristian%20Minoccheri%20and%20Matthew%20Hodgman%20and%20Haoyuan%20Ma%20and%20Rameez%20Merchant%20and%20Emily%20Wittrup%20and%20Craig%20Williamson%20and%20Kayvan%20Najarian%0AAbstract%3A%20Aneurysmal%20subarachnoid%20hemorrhage%20%28SAH%29%20is%20a%20life-threatening%20neurological%20emergency%20with%20mortality%20rates%20exceeding%2030%25.%20Transfer%20learning%20from%20related%20hematoma%20types%20represents%20a%20potentially%20valuable%20but%20underexplored%20approach.%20Although%20Unet%20architectures%20remain%20the%20gold%20standard%20for%20medical%20image%20segmentation%20due%20to%20their%20effectiveness%20on%20limited%20datasets%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20methods%20for%20parameter-efficient%20transfer%20learning%20have%20been%20rarely%20applied%20to%20convolutional%20neural%20networks%20in%20medical%20imaging%20contexts.%20We%20implemented%20a%20Unet%20architecture%20pre-trained%20on%20computed%20tomography%20scans%20from%20124%20traumatic%20brain%20injury%20patients%20across%20multiple%20institutions%2C%20then%20fine-tuned%20on%2030%20aneurysmal%20SAH%20patients%20from%20the%20University%20of%20Michigan%20Health%20System%20using%203-fold%20cross-validation.%20We%20developed%20a%20novel%20CP-LoRA%20method%20based%20on%20tensor%20CP-decomposition%20and%20introduced%20DoRA%20variants%20%28DoRA-C%2C%20convDoRA%2C%20CP-DoRA%29%20that%20decompose%20weight%20matrices%20into%20magnitude%20and%20directional%20components.%20We%20compared%20these%20approaches%20against%20existing%20LoRA%20methods%20%28LoRA-C%2C%20convLoRA%29%20and%20standard%20fine-tuning%20strategies%20across%20different%20modules%20on%20a%20multi-view%20Unet%20model.%20LoRA-based%20methods%20consistently%20outperformed%20standard%20Unet%20fine-tuning.%20Performance%20varied%20by%20hemorrhage%20volume%2C%20with%20all%20methods%20showing%20improved%20accuracy%20for%20larger%20volumes.%20CP-LoRA%20achieved%20comparable%20performance%20to%20existing%20methods%20while%20using%20significantly%20fewer%20parameters.%20Over-parameterization%20with%20higher%20ranks%20consistently%20yielded%20better%20performance%20than%20strictly%20low-rank%20adaptations.%20This%20study%20demonstrates%20that%20transfer%20learning%20between%20hematoma%20types%20is%20feasible%20and%20that%20LoRA-based%20methods%20significantly%20outperform%20conventional%20Unet%20fine-tuning%20for%20aneurysmal%20SAH%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01772v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-based%2520methods%2520on%2520Unet%2520for%2520transfer%2520learning%2520in%2520Subarachnoid%2520Hematoma%2520Segmentation%26entry.906535625%3DCristian%2520Minoccheri%2520and%2520Matthew%2520Hodgman%2520and%2520Haoyuan%2520Ma%2520and%2520Rameez%2520Merchant%2520and%2520Emily%2520Wittrup%2520and%2520Craig%2520Williamson%2520and%2520Kayvan%2520Najarian%26entry.1292438233%3DAneurysmal%2520subarachnoid%2520hemorrhage%2520%2528SAH%2529%2520is%2520a%2520life-threatening%2520neurological%2520emergency%2520with%2520mortality%2520rates%2520exceeding%252030%2525.%2520Transfer%2520learning%2520from%2520related%2520hematoma%2520types%2520represents%2520a%2520potentially%2520valuable%2520but%2520underexplored%2520approach.%2520Although%2520Unet%2520architectures%2520remain%2520the%2520gold%2520standard%2520for%2520medical%2520image%2520segmentation%2520due%2520to%2520their%2520effectiveness%2520on%2520limited%2520datasets%252C%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520methods%2520for%2520parameter-efficient%2520transfer%2520learning%2520have%2520been%2520rarely%2520applied%2520to%2520convolutional%2520neural%2520networks%2520in%2520medical%2520imaging%2520contexts.%2520We%2520implemented%2520a%2520Unet%2520architecture%2520pre-trained%2520on%2520computed%2520tomography%2520scans%2520from%2520124%2520traumatic%2520brain%2520injury%2520patients%2520across%2520multiple%2520institutions%252C%2520then%2520fine-tuned%2520on%252030%2520aneurysmal%2520SAH%2520patients%2520from%2520the%2520University%2520of%2520Michigan%2520Health%2520System%2520using%25203-fold%2520cross-validation.%2520We%2520developed%2520a%2520novel%2520CP-LoRA%2520method%2520based%2520on%2520tensor%2520CP-decomposition%2520and%2520introduced%2520DoRA%2520variants%2520%2528DoRA-C%252C%2520convDoRA%252C%2520CP-DoRA%2529%2520that%2520decompose%2520weight%2520matrices%2520into%2520magnitude%2520and%2520directional%2520components.%2520We%2520compared%2520these%2520approaches%2520against%2520existing%2520LoRA%2520methods%2520%2528LoRA-C%252C%2520convLoRA%2529%2520and%2520standard%2520fine-tuning%2520strategies%2520across%2520different%2520modules%2520on%2520a%2520multi-view%2520Unet%2520model.%2520LoRA-based%2520methods%2520consistently%2520outperformed%2520standard%2520Unet%2520fine-tuning.%2520Performance%2520varied%2520by%2520hemorrhage%2520volume%252C%2520with%2520all%2520methods%2520showing%2520improved%2520accuracy%2520for%2520larger%2520volumes.%2520CP-LoRA%2520achieved%2520comparable%2520performance%2520to%2520existing%2520methods%2520while%2520using%2520significantly%2520fewer%2520parameters.%2520Over-parameterization%2520with%2520higher%2520ranks%2520consistently%2520yielded%2520better%2520performance%2520than%2520strictly%2520low-rank%2520adaptations.%2520This%2520study%2520demonstrates%2520that%2520transfer%2520learning%2520between%2520hematoma%2520types%2520is%2520feasible%2520and%2520that%2520LoRA-based%2520methods%2520significantly%2520outperform%2520conventional%2520Unet%2520fine-tuning%2520for%2520aneurysmal%2520SAH%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01772v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-based%20methods%20on%20Unet%20for%20transfer%20learning%20in%20Subarachnoid%20Hematoma%20Segmentation&entry.906535625=Cristian%20Minoccheri%20and%20Matthew%20Hodgman%20and%20Haoyuan%20Ma%20and%20Rameez%20Merchant%20and%20Emily%20Wittrup%20and%20Craig%20Williamson%20and%20Kayvan%20Najarian&entry.1292438233=Aneurysmal%20subarachnoid%20hemorrhage%20%28SAH%29%20is%20a%20life-threatening%20neurological%20emergency%20with%20mortality%20rates%20exceeding%2030%25.%20Transfer%20learning%20from%20related%20hematoma%20types%20represents%20a%20potentially%20valuable%20but%20underexplored%20approach.%20Although%20Unet%20architectures%20remain%20the%20gold%20standard%20for%20medical%20image%20segmentation%20due%20to%20their%20effectiveness%20on%20limited%20datasets%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20methods%20for%20parameter-efficient%20transfer%20learning%20have%20been%20rarely%20applied%20to%20convolutional%20neural%20networks%20in%20medical%20imaging%20contexts.%20We%20implemented%20a%20Unet%20architecture%20pre-trained%20on%20computed%20tomography%20scans%20from%20124%20traumatic%20brain%20injury%20patients%20across%20multiple%20institutions%2C%20then%20fine-tuned%20on%2030%20aneurysmal%20SAH%20patients%20from%20the%20University%20of%20Michigan%20Health%20System%20using%203-fold%20cross-validation.%20We%20developed%20a%20novel%20CP-LoRA%20method%20based%20on%20tensor%20CP-decomposition%20and%20introduced%20DoRA%20variants%20%28DoRA-C%2C%20convDoRA%2C%20CP-DoRA%29%20that%20decompose%20weight%20matrices%20into%20magnitude%20and%20directional%20components.%20We%20compared%20these%20approaches%20against%20existing%20LoRA%20methods%20%28LoRA-C%2C%20convLoRA%29%20and%20standard%20fine-tuning%20strategies%20across%20different%20modules%20on%20a%20multi-view%20Unet%20model.%20LoRA-based%20methods%20consistently%20outperformed%20standard%20Unet%20fine-tuning.%20Performance%20varied%20by%20hemorrhage%20volume%2C%20with%20all%20methods%20showing%20improved%20accuracy%20for%20larger%20volumes.%20CP-LoRA%20achieved%20comparable%20performance%20to%20existing%20methods%20while%20using%20significantly%20fewer%20parameters.%20Over-parameterization%20with%20higher%20ranks%20consistently%20yielded%20better%20performance%20than%20strictly%20low-rank%20adaptations.%20This%20study%20demonstrates%20that%20transfer%20learning%20between%20hematoma%20types%20is%20feasible%20and%20that%20LoRA-based%20methods%20significantly%20outperform%20conventional%20Unet%20fine-tuning%20for%20aneurysmal%20SAH%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2508.01772v3&entry.124074799=Read"},
{"title": "Communication-Efficient Learning for Satellite Constellations", "author": "Ruxandra-Stefania Tudose and Moritz H. W. Gr\u00fcss and Grace Ra Kim and Karl H. Johansson and Nicola Bastianello", "abstract": "Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.", "link": "http://arxiv.org/abs/2511.20220v1", "date": "2025-11-25", "relevancy": 2.3945, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4836}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.48}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Learning%20for%20Satellite%20Constellations&body=Title%3A%20Communication-Efficient%20Learning%20for%20Satellite%20Constellations%0AAuthor%3A%20Ruxandra-Stefania%20Tudose%20and%20Moritz%20H.%20W.%20Gr%C3%BCss%20and%20Grace%20Ra%20Kim%20and%20Karl%20H.%20Johansson%20and%20Nicola%20Bastianello%0AAbstract%3A%20Satellite%20constellations%20in%20low-Earth%20orbit%20are%20now%20widespread%2C%20enabling%20positioning%2C%20Earth%20imaging%2C%20and%20communications.%20In%20this%20paper%20we%20address%20the%20solution%20of%20learning%20problems%20using%20these%20satellite%20constellations.%20In%20particular%2C%20we%20focus%20on%20a%20federated%20approach%2C%20where%20satellites%20collect%20and%20locally%20process%20data%2C%20with%20the%20ground%20station%20aggregating%20local%20models.%20We%20focus%20on%20designing%20a%20novel%2C%20communication-efficient%20algorithm%20that%20still%20yields%20accurate%20trained%20models.%20To%20this%20end%2C%20we%20employ%20several%20mechanisms%20to%20reduce%20the%20number%20of%20communications%20with%20the%20ground%20station%20%28local%20training%29%20and%20their%20size%20%28compression%29.%20We%20then%20propose%20an%20error%20feedback%20mechanism%20that%20enhances%20accuracy%2C%20which%20yields%2C%20as%20a%20byproduct%2C%20an%20algorithm-agnostic%20error%20feedback%20scheme%20that%20can%20be%20more%20broadly%20applied.%20We%20analyze%20the%20convergence%20of%20the%20resulting%20algorithm%2C%20and%20compare%20it%20with%20the%20state%20of%20the%20art%20through%20simulations%20in%20a%20realistic%20space%20scenario%2C%20showcasing%20superior%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Learning%2520for%2520Satellite%2520Constellations%26entry.906535625%3DRuxandra-Stefania%2520Tudose%2520and%2520Moritz%2520H.%2520W.%2520Gr%25C3%25BCss%2520and%2520Grace%2520Ra%2520Kim%2520and%2520Karl%2520H.%2520Johansson%2520and%2520Nicola%2520Bastianello%26entry.1292438233%3DSatellite%2520constellations%2520in%2520low-Earth%2520orbit%2520are%2520now%2520widespread%252C%2520enabling%2520positioning%252C%2520Earth%2520imaging%252C%2520and%2520communications.%2520In%2520this%2520paper%2520we%2520address%2520the%2520solution%2520of%2520learning%2520problems%2520using%2520these%2520satellite%2520constellations.%2520In%2520particular%252C%2520we%2520focus%2520on%2520a%2520federated%2520approach%252C%2520where%2520satellites%2520collect%2520and%2520locally%2520process%2520data%252C%2520with%2520the%2520ground%2520station%2520aggregating%2520local%2520models.%2520We%2520focus%2520on%2520designing%2520a%2520novel%252C%2520communication-efficient%2520algorithm%2520that%2520still%2520yields%2520accurate%2520trained%2520models.%2520To%2520this%2520end%252C%2520we%2520employ%2520several%2520mechanisms%2520to%2520reduce%2520the%2520number%2520of%2520communications%2520with%2520the%2520ground%2520station%2520%2528local%2520training%2529%2520and%2520their%2520size%2520%2528compression%2529.%2520We%2520then%2520propose%2520an%2520error%2520feedback%2520mechanism%2520that%2520enhances%2520accuracy%252C%2520which%2520yields%252C%2520as%2520a%2520byproduct%252C%2520an%2520algorithm-agnostic%2520error%2520feedback%2520scheme%2520that%2520can%2520be%2520more%2520broadly%2520applied.%2520We%2520analyze%2520the%2520convergence%2520of%2520the%2520resulting%2520algorithm%252C%2520and%2520compare%2520it%2520with%2520the%2520state%2520of%2520the%2520art%2520through%2520simulations%2520in%2520a%2520realistic%2520space%2520scenario%252C%2520showcasing%2520superior%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Learning%20for%20Satellite%20Constellations&entry.906535625=Ruxandra-Stefania%20Tudose%20and%20Moritz%20H.%20W.%20Gr%C3%BCss%20and%20Grace%20Ra%20Kim%20and%20Karl%20H.%20Johansson%20and%20Nicola%20Bastianello&entry.1292438233=Satellite%20constellations%20in%20low-Earth%20orbit%20are%20now%20widespread%2C%20enabling%20positioning%2C%20Earth%20imaging%2C%20and%20communications.%20In%20this%20paper%20we%20address%20the%20solution%20of%20learning%20problems%20using%20these%20satellite%20constellations.%20In%20particular%2C%20we%20focus%20on%20a%20federated%20approach%2C%20where%20satellites%20collect%20and%20locally%20process%20data%2C%20with%20the%20ground%20station%20aggregating%20local%20models.%20We%20focus%20on%20designing%20a%20novel%2C%20communication-efficient%20algorithm%20that%20still%20yields%20accurate%20trained%20models.%20To%20this%20end%2C%20we%20employ%20several%20mechanisms%20to%20reduce%20the%20number%20of%20communications%20with%20the%20ground%20station%20%28local%20training%29%20and%20their%20size%20%28compression%29.%20We%20then%20propose%20an%20error%20feedback%20mechanism%20that%20enhances%20accuracy%2C%20which%20yields%2C%20as%20a%20byproduct%2C%20an%20algorithm-agnostic%20error%20feedback%20scheme%20that%20can%20be%20more%20broadly%20applied.%20We%20analyze%20the%20convergence%20of%20the%20resulting%20algorithm%2C%20and%20compare%20it%20with%20the%20state%20of%20the%20art%20through%20simulations%20in%20a%20realistic%20space%20scenario%2C%20showcasing%20superior%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.20220v1&entry.124074799=Read"},
{"title": "ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection", "author": "Ruize Ma and Minghong Cai and Yilei Jiang and Jiaming Han and Yi Feng and Yingshui Tan and Xiaoyong Zhu and Bo Zhang and Bo Zheng and Xiangyu Yue", "abstract": "Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.Our code is available at https://github.com/Ruize-Ma/ConceptGuard.", "link": "http://arxiv.org/abs/2511.18780v2", "date": "2025-11-25", "relevancy": 2.3843, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6042}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5969}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConceptGuard%3A%20Proactive%20Safety%20in%20Text-and-Image-to-Video%20Generation%20through%20Multimodal%20Risk%20Detection&body=Title%3A%20ConceptGuard%3A%20Proactive%20Safety%20in%20Text-and-Image-to-Video%20Generation%20through%20Multimodal%20Risk%20Detection%0AAuthor%3A%20Ruize%20Ma%20and%20Minghong%20Cai%20and%20Yilei%20Jiang%20and%20Jiaming%20Han%20and%20Yi%20Feng%20and%20Yingshui%20Tan%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zhang%20and%20Bo%20Zheng%20and%20Xiangyu%20Yue%0AAbstract%3A%20Recent%20progress%20in%20video%20generative%20models%20has%20enabled%20the%20creation%20of%20high-quality%20videos%20from%20multimodal%20prompts%20that%20combine%20text%20and%20images.%20While%20these%20systems%20offer%20enhanced%20controllability%2C%20they%20also%20introduce%20new%20safety%20risks%2C%20as%20harmful%20content%20can%20emerge%20from%20individual%20modalities%20or%20their%20interaction.%20Existing%20safety%20methods%20are%20often%20text-only%2C%20require%20prior%20knowledge%20of%20the%20risk%20category%2C%20or%20operate%20as%20post-generation%20auditors%2C%20struggling%20to%20proactively%20mitigate%20such%20compositional%2C%20multimodal%20risks.%20To%20address%20this%20challenge%2C%20we%20present%20ConceptGuard%2C%20a%20unified%20safeguard%20framework%20for%20proactively%20detecting%20and%20mitigating%20unsafe%20semantics%20in%20multimodal%20video%20generation.%20ConceptGuard%20operates%20in%20two%20stages%3A%20First%2C%20a%20contrastive%20detection%20module%20identifies%20latent%20safety%20risks%20by%20projecting%20fused%20image-text%20inputs%20into%20a%20structured%20concept%20space%3B%20Second%2C%20a%20semantic%20suppression%20mechanism%20steers%20the%20generative%20process%20away%20from%20unsafe%20concepts%20by%20intervening%20in%20the%20prompt%27s%20multimodal%20conditioning.%20To%20support%20the%20development%20and%20rigorous%20evaluation%20of%20this%20framework%2C%20we%20introduce%20two%20novel%20benchmarks%3A%20ConceptRisk%2C%20a%20large-scale%20dataset%20for%20training%20on%20multimodal%20risks%2C%20and%20T2VSafetyBench-TI2V%2C%20the%20first%20benchmark%20adapted%20from%20T2VSafetyBench%20for%20the%20Text-and-Image-to-Video%20%28TI2V%29%20safety%20setting.%20Comprehensive%20experiments%20on%20both%20benchmarks%20show%20that%20ConceptGuard%20consistently%20outperforms%20existing%20baselines%2C%20achieving%20state-of-the-art%20results%20in%20both%20risk%20detection%20and%20safe%20video%20generation.Our%20code%20is%20available%20at%20https%3A//github.com/Ruize-Ma/ConceptGuard.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptGuard%253A%2520Proactive%2520Safety%2520in%2520Text-and-Image-to-Video%2520Generation%2520through%2520Multimodal%2520Risk%2520Detection%26entry.906535625%3DRuize%2520Ma%2520and%2520Minghong%2520Cai%2520and%2520Yilei%2520Jiang%2520and%2520Jiaming%2520Han%2520and%2520Yi%2520Feng%2520and%2520Yingshui%2520Tan%2520and%2520Xiaoyong%2520Zhu%2520and%2520Bo%2520Zhang%2520and%2520Bo%2520Zheng%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3DRecent%2520progress%2520in%2520video%2520generative%2520models%2520has%2520enabled%2520the%2520creation%2520of%2520high-quality%2520videos%2520from%2520multimodal%2520prompts%2520that%2520combine%2520text%2520and%2520images.%2520While%2520these%2520systems%2520offer%2520enhanced%2520controllability%252C%2520they%2520also%2520introduce%2520new%2520safety%2520risks%252C%2520as%2520harmful%2520content%2520can%2520emerge%2520from%2520individual%2520modalities%2520or%2520their%2520interaction.%2520Existing%2520safety%2520methods%2520are%2520often%2520text-only%252C%2520require%2520prior%2520knowledge%2520of%2520the%2520risk%2520category%252C%2520or%2520operate%2520as%2520post-generation%2520auditors%252C%2520struggling%2520to%2520proactively%2520mitigate%2520such%2520compositional%252C%2520multimodal%2520risks.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520ConceptGuard%252C%2520a%2520unified%2520safeguard%2520framework%2520for%2520proactively%2520detecting%2520and%2520mitigating%2520unsafe%2520semantics%2520in%2520multimodal%2520video%2520generation.%2520ConceptGuard%2520operates%2520in%2520two%2520stages%253A%2520First%252C%2520a%2520contrastive%2520detection%2520module%2520identifies%2520latent%2520safety%2520risks%2520by%2520projecting%2520fused%2520image-text%2520inputs%2520into%2520a%2520structured%2520concept%2520space%253B%2520Second%252C%2520a%2520semantic%2520suppression%2520mechanism%2520steers%2520the%2520generative%2520process%2520away%2520from%2520unsafe%2520concepts%2520by%2520intervening%2520in%2520the%2520prompt%2527s%2520multimodal%2520conditioning.%2520To%2520support%2520the%2520development%2520and%2520rigorous%2520evaluation%2520of%2520this%2520framework%252C%2520we%2520introduce%2520two%2520novel%2520benchmarks%253A%2520ConceptRisk%252C%2520a%2520large-scale%2520dataset%2520for%2520training%2520on%2520multimodal%2520risks%252C%2520and%2520T2VSafetyBench-TI2V%252C%2520the%2520first%2520benchmark%2520adapted%2520from%2520T2VSafetyBench%2520for%2520the%2520Text-and-Image-to-Video%2520%2528TI2V%2529%2520safety%2520setting.%2520Comprehensive%2520experiments%2520on%2520both%2520benchmarks%2520show%2520that%2520ConceptGuard%2520consistently%2520outperforms%2520existing%2520baselines%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520both%2520risk%2520detection%2520and%2520safe%2520video%2520generation.Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Ruize-Ma/ConceptGuard.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConceptGuard%3A%20Proactive%20Safety%20in%20Text-and-Image-to-Video%20Generation%20through%20Multimodal%20Risk%20Detection&entry.906535625=Ruize%20Ma%20and%20Minghong%20Cai%20and%20Yilei%20Jiang%20and%20Jiaming%20Han%20and%20Yi%20Feng%20and%20Yingshui%20Tan%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zhang%20and%20Bo%20Zheng%20and%20Xiangyu%20Yue&entry.1292438233=Recent%20progress%20in%20video%20generative%20models%20has%20enabled%20the%20creation%20of%20high-quality%20videos%20from%20multimodal%20prompts%20that%20combine%20text%20and%20images.%20While%20these%20systems%20offer%20enhanced%20controllability%2C%20they%20also%20introduce%20new%20safety%20risks%2C%20as%20harmful%20content%20can%20emerge%20from%20individual%20modalities%20or%20their%20interaction.%20Existing%20safety%20methods%20are%20often%20text-only%2C%20require%20prior%20knowledge%20of%20the%20risk%20category%2C%20or%20operate%20as%20post-generation%20auditors%2C%20struggling%20to%20proactively%20mitigate%20such%20compositional%2C%20multimodal%20risks.%20To%20address%20this%20challenge%2C%20we%20present%20ConceptGuard%2C%20a%20unified%20safeguard%20framework%20for%20proactively%20detecting%20and%20mitigating%20unsafe%20semantics%20in%20multimodal%20video%20generation.%20ConceptGuard%20operates%20in%20two%20stages%3A%20First%2C%20a%20contrastive%20detection%20module%20identifies%20latent%20safety%20risks%20by%20projecting%20fused%20image-text%20inputs%20into%20a%20structured%20concept%20space%3B%20Second%2C%20a%20semantic%20suppression%20mechanism%20steers%20the%20generative%20process%20away%20from%20unsafe%20concepts%20by%20intervening%20in%20the%20prompt%27s%20multimodal%20conditioning.%20To%20support%20the%20development%20and%20rigorous%20evaluation%20of%20this%20framework%2C%20we%20introduce%20two%20novel%20benchmarks%3A%20ConceptRisk%2C%20a%20large-scale%20dataset%20for%20training%20on%20multimodal%20risks%2C%20and%20T2VSafetyBench-TI2V%2C%20the%20first%20benchmark%20adapted%20from%20T2VSafetyBench%20for%20the%20Text-and-Image-to-Video%20%28TI2V%29%20safety%20setting.%20Comprehensive%20experiments%20on%20both%20benchmarks%20show%20that%20ConceptGuard%20consistently%20outperforms%20existing%20baselines%2C%20achieving%20state-of-the-art%20results%20in%20both%20risk%20detection%20and%20safe%20video%20generation.Our%20code%20is%20available%20at%20https%3A//github.com/Ruize-Ma/ConceptGuard.&entry.1838667208=http%3A//arxiv.org/abs/2511.18780v2&entry.124074799=Read"},
{"title": "Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models", "author": "Shamima Hossain", "abstract": "Visual Language Models (VLMs) are powerful generative tools but often produce factually inaccurate outputs due to a lack of robust reasoning capabilities. While extensive research has been conducted on integrating external knowledge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seamlessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leveraging structured knowledge graphs for multi-hop verification using image-captioning task to illustrate our framework. Our approach enables systematic reasoning across multiple steps, including visual entity recognition, knowledge graph traversal, and fact-based caption refinement. We evaluate the framework using hierarchical, triple-based and bullet-point based knowledge representations, analyzing their effectiveness in factual accuracy and logical inference. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions revealing key insights into reasoning patterns and failure modes. This work demonstrates the potential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.", "link": "http://arxiv.org/abs/2511.20531v1", "date": "2025-11-25", "relevancy": 2.3769, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Generation%3A%20Multi-Hop%20Reasoning%20for%20Factual%20Accuracy%20in%20Vision-Language%20Models&body=Title%3A%20Beyond%20Generation%3A%20Multi-Hop%20Reasoning%20for%20Factual%20Accuracy%20in%20Vision-Language%20Models%0AAuthor%3A%20Shamima%20Hossain%0AAbstract%3A%20Visual%20Language%20Models%20%28VLMs%29%20are%20powerful%20generative%20tools%20but%20often%20produce%20factually%20inaccurate%20outputs%20due%20to%20a%20lack%20of%20robust%20reasoning%20capabilities.%20While%20extensive%20research%20has%20been%20conducted%20on%20integrating%20external%20knowledge%20for%20reasoning%20in%20large%20language%20models%20%28LLMs%29%2C%20such%20efforts%20remain%20underexplored%20in%20VLMs%2C%20where%20the%20challenge%20is%20compounded%20by%20the%20need%20to%20bridge%20multiple%20modalities%20seamlessly.%20This%20work%20introduces%20a%20framework%20for%20knowledge-guided%20reasoning%20in%20VLMs%2C%20leveraging%20structured%20knowledge%20graphs%20for%20multi-hop%20verification%20using%20image-captioning%20task%20to%20illustrate%20our%20framework.%20Our%20approach%20enables%20systematic%20reasoning%20across%20multiple%20steps%2C%20including%20visual%20entity%20recognition%2C%20knowledge%20graph%20traversal%2C%20and%20fact-based%20caption%20refinement.%20We%20evaluate%20the%20framework%20using%20hierarchical%2C%20triple-based%20and%20bullet-point%20based%20knowledge%20representations%2C%20analyzing%20their%20effectiveness%20in%20factual%20accuracy%20and%20logical%20inference.%20Empirical%20results%20show%20that%20our%20approach%20improves%20factual%20accuracy%20by%20approximately%2031%25%20on%20preliminary%20experiments%20on%20a%20curated%20dataset%20of%20mixtures%20from%20Google%20Landmarks%20v2%2C%20Conceptual%20captions%20and%20Coco%20captions%20revealing%20key%20insights%20into%20reasoning%20patterns%20and%20failure%20modes.%20This%20work%20demonstrates%20the%20potential%20of%20integrating%20external%20knowledge%20for%20advancing%20reasoning%20in%20VLMs%2C%20paving%20the%20way%20for%20more%20reliable%20and%20knowledgable%20multimodal%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Generation%253A%2520Multi-Hop%2520Reasoning%2520for%2520Factual%2520Accuracy%2520in%2520Vision-Language%2520Models%26entry.906535625%3DShamima%2520Hossain%26entry.1292438233%3DVisual%2520Language%2520Models%2520%2528VLMs%2529%2520are%2520powerful%2520generative%2520tools%2520but%2520often%2520produce%2520factually%2520inaccurate%2520outputs%2520due%2520to%2520a%2520lack%2520of%2520robust%2520reasoning%2520capabilities.%2520While%2520extensive%2520research%2520has%2520been%2520conducted%2520on%2520integrating%2520external%2520knowledge%2520for%2520reasoning%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520such%2520efforts%2520remain%2520underexplored%2520in%2520VLMs%252C%2520where%2520the%2520challenge%2520is%2520compounded%2520by%2520the%2520need%2520to%2520bridge%2520multiple%2520modalities%2520seamlessly.%2520This%2520work%2520introduces%2520a%2520framework%2520for%2520knowledge-guided%2520reasoning%2520in%2520VLMs%252C%2520leveraging%2520structured%2520knowledge%2520graphs%2520for%2520multi-hop%2520verification%2520using%2520image-captioning%2520task%2520to%2520illustrate%2520our%2520framework.%2520Our%2520approach%2520enables%2520systematic%2520reasoning%2520across%2520multiple%2520steps%252C%2520including%2520visual%2520entity%2520recognition%252C%2520knowledge%2520graph%2520traversal%252C%2520and%2520fact-based%2520caption%2520refinement.%2520We%2520evaluate%2520the%2520framework%2520using%2520hierarchical%252C%2520triple-based%2520and%2520bullet-point%2520based%2520knowledge%2520representations%252C%2520analyzing%2520their%2520effectiveness%2520in%2520factual%2520accuracy%2520and%2520logical%2520inference.%2520Empirical%2520results%2520show%2520that%2520our%2520approach%2520improves%2520factual%2520accuracy%2520by%2520approximately%252031%2525%2520on%2520preliminary%2520experiments%2520on%2520a%2520curated%2520dataset%2520of%2520mixtures%2520from%2520Google%2520Landmarks%2520v2%252C%2520Conceptual%2520captions%2520and%2520Coco%2520captions%2520revealing%2520key%2520insights%2520into%2520reasoning%2520patterns%2520and%2520failure%2520modes.%2520This%2520work%2520demonstrates%2520the%2520potential%2520of%2520integrating%2520external%2520knowledge%2520for%2520advancing%2520reasoning%2520in%2520VLMs%252C%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520and%2520knowledgable%2520multimodal%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Generation%3A%20Multi-Hop%20Reasoning%20for%20Factual%20Accuracy%20in%20Vision-Language%20Models&entry.906535625=Shamima%20Hossain&entry.1292438233=Visual%20Language%20Models%20%28VLMs%29%20are%20powerful%20generative%20tools%20but%20often%20produce%20factually%20inaccurate%20outputs%20due%20to%20a%20lack%20of%20robust%20reasoning%20capabilities.%20While%20extensive%20research%20has%20been%20conducted%20on%20integrating%20external%20knowledge%20for%20reasoning%20in%20large%20language%20models%20%28LLMs%29%2C%20such%20efforts%20remain%20underexplored%20in%20VLMs%2C%20where%20the%20challenge%20is%20compounded%20by%20the%20need%20to%20bridge%20multiple%20modalities%20seamlessly.%20This%20work%20introduces%20a%20framework%20for%20knowledge-guided%20reasoning%20in%20VLMs%2C%20leveraging%20structured%20knowledge%20graphs%20for%20multi-hop%20verification%20using%20image-captioning%20task%20to%20illustrate%20our%20framework.%20Our%20approach%20enables%20systematic%20reasoning%20across%20multiple%20steps%2C%20including%20visual%20entity%20recognition%2C%20knowledge%20graph%20traversal%2C%20and%20fact-based%20caption%20refinement.%20We%20evaluate%20the%20framework%20using%20hierarchical%2C%20triple-based%20and%20bullet-point%20based%20knowledge%20representations%2C%20analyzing%20their%20effectiveness%20in%20factual%20accuracy%20and%20logical%20inference.%20Empirical%20results%20show%20that%20our%20approach%20improves%20factual%20accuracy%20by%20approximately%2031%25%20on%20preliminary%20experiments%20on%20a%20curated%20dataset%20of%20mixtures%20from%20Google%20Landmarks%20v2%2C%20Conceptual%20captions%20and%20Coco%20captions%20revealing%20key%20insights%20into%20reasoning%20patterns%20and%20failure%20modes.%20This%20work%20demonstrates%20the%20potential%20of%20integrating%20external%20knowledge%20for%20advancing%20reasoning%20in%20VLMs%2C%20paving%20the%20way%20for%20more%20reliable%20and%20knowledgable%20multimodal%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.20531v1&entry.124074799=Read"},
{"title": "Self-Organization and Spectral Mechanism of Attractor Landscapes in High-Capacity Kernel Hopfield Networks", "author": "Akira Tamamori", "abstract": "Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by unifying the geometric analysis of the attractor landscape with the spectral theory of kernel machines. Using a novel metric, \"Pinnacle Sharpness,\" we first uncover a rich phase diagram of attractor stability, identifying a \"Ridge of Optimization\" where the network achieves maximal robustness under high-load conditions. Phenomenologically, this ridge is characterized by a \"Force Antagonism,\" where a strong driving force is balanced by a collective feedback force. Theoretically, we reveal that this phenomenon arises from a specific reorganization of the weight spectrum, which we term \\textit{Spectral Concentration}. Unlike a simple rank-1 collapse, our analysis shows that the network on the ridge self-organizes into a critical state: the leading eigenvalue is amplified to maximize global stability (Direct Force), while the trailing eigenvalues are preserved to maintain high memory capacity (Indirect Force). These findings provide a complete physical picture of how high-capacity associative memories are formed, demonstrating that optimal performance is achieved by tuning the system to a spectral \"Goldilocks zone\" between rank collapse and diffusion.", "link": "http://arxiv.org/abs/2511.13053v3", "date": "2025-11-25", "relevancy": 2.3681, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4832}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4751}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Organization%20and%20Spectral%20Mechanism%20of%20Attractor%20Landscapes%20in%20High-Capacity%20Kernel%20Hopfield%20Networks&body=Title%3A%20Self-Organization%20and%20Spectral%20Mechanism%20of%20Attractor%20Landscapes%20in%20High-Capacity%20Kernel%20Hopfield%20Networks%0AAuthor%3A%20Akira%20Tamamori%0AAbstract%3A%20Kernel-based%20learning%20methods%20can%20dramatically%20increase%20the%20storage%20capacity%20of%20Hopfield%20networks%2C%20yet%20the%20dynamical%20mechanism%20behind%20this%20enhancement%20remains%20poorly%20understood.%20We%20address%20this%20gap%20by%20unifying%20the%20geometric%20analysis%20of%20the%20attractor%20landscape%20with%20the%20spectral%20theory%20of%20kernel%20machines.%20Using%20a%20novel%20metric%2C%20%22Pinnacle%20Sharpness%2C%22%20we%20first%20uncover%20a%20rich%20phase%20diagram%20of%20attractor%20stability%2C%20identifying%20a%20%22Ridge%20of%20Optimization%22%20where%20the%20network%20achieves%20maximal%20robustness%20under%20high-load%20conditions.%20Phenomenologically%2C%20this%20ridge%20is%20characterized%20by%20a%20%22Force%20Antagonism%2C%22%20where%20a%20strong%20driving%20force%20is%20balanced%20by%20a%20collective%20feedback%20force.%20Theoretically%2C%20we%20reveal%20that%20this%20phenomenon%20arises%20from%20a%20specific%20reorganization%20of%20the%20weight%20spectrum%2C%20which%20we%20term%20%5Ctextit%7BSpectral%20Concentration%7D.%20Unlike%20a%20simple%20rank-1%20collapse%2C%20our%20analysis%20shows%20that%20the%20network%20on%20the%20ridge%20self-organizes%20into%20a%20critical%20state%3A%20the%20leading%20eigenvalue%20is%20amplified%20to%20maximize%20global%20stability%20%28Direct%20Force%29%2C%20while%20the%20trailing%20eigenvalues%20are%20preserved%20to%20maintain%20high%20memory%20capacity%20%28Indirect%20Force%29.%20These%20findings%20provide%20a%20complete%20physical%20picture%20of%20how%20high-capacity%20associative%20memories%20are%20formed%2C%20demonstrating%20that%20optimal%20performance%20is%20achieved%20by%20tuning%20the%20system%20to%20a%20spectral%20%22Goldilocks%20zone%22%20between%20rank%20collapse%20and%20diffusion.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13053v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Organization%2520and%2520Spectral%2520Mechanism%2520of%2520Attractor%2520Landscapes%2520in%2520High-Capacity%2520Kernel%2520Hopfield%2520Networks%26entry.906535625%3DAkira%2520Tamamori%26entry.1292438233%3DKernel-based%2520learning%2520methods%2520can%2520dramatically%2520increase%2520the%2520storage%2520capacity%2520of%2520Hopfield%2520networks%252C%2520yet%2520the%2520dynamical%2520mechanism%2520behind%2520this%2520enhancement%2520remains%2520poorly%2520understood.%2520We%2520address%2520this%2520gap%2520by%2520unifying%2520the%2520geometric%2520analysis%2520of%2520the%2520attractor%2520landscape%2520with%2520the%2520spectral%2520theory%2520of%2520kernel%2520machines.%2520Using%2520a%2520novel%2520metric%252C%2520%2522Pinnacle%2520Sharpness%252C%2522%2520we%2520first%2520uncover%2520a%2520rich%2520phase%2520diagram%2520of%2520attractor%2520stability%252C%2520identifying%2520a%2520%2522Ridge%2520of%2520Optimization%2522%2520where%2520the%2520network%2520achieves%2520maximal%2520robustness%2520under%2520high-load%2520conditions.%2520Phenomenologically%252C%2520this%2520ridge%2520is%2520characterized%2520by%2520a%2520%2522Force%2520Antagonism%252C%2522%2520where%2520a%2520strong%2520driving%2520force%2520is%2520balanced%2520by%2520a%2520collective%2520feedback%2520force.%2520Theoretically%252C%2520we%2520reveal%2520that%2520this%2520phenomenon%2520arises%2520from%2520a%2520specific%2520reorganization%2520of%2520the%2520weight%2520spectrum%252C%2520which%2520we%2520term%2520%255Ctextit%257BSpectral%2520Concentration%257D.%2520Unlike%2520a%2520simple%2520rank-1%2520collapse%252C%2520our%2520analysis%2520shows%2520that%2520the%2520network%2520on%2520the%2520ridge%2520self-organizes%2520into%2520a%2520critical%2520state%253A%2520the%2520leading%2520eigenvalue%2520is%2520amplified%2520to%2520maximize%2520global%2520stability%2520%2528Direct%2520Force%2529%252C%2520while%2520the%2520trailing%2520eigenvalues%2520are%2520preserved%2520to%2520maintain%2520high%2520memory%2520capacity%2520%2528Indirect%2520Force%2529.%2520These%2520findings%2520provide%2520a%2520complete%2520physical%2520picture%2520of%2520how%2520high-capacity%2520associative%2520memories%2520are%2520formed%252C%2520demonstrating%2520that%2520optimal%2520performance%2520is%2520achieved%2520by%2520tuning%2520the%2520system%2520to%2520a%2520spectral%2520%2522Goldilocks%2520zone%2522%2520between%2520rank%2520collapse%2520and%2520diffusion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13053v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Organization%20and%20Spectral%20Mechanism%20of%20Attractor%20Landscapes%20in%20High-Capacity%20Kernel%20Hopfield%20Networks&entry.906535625=Akira%20Tamamori&entry.1292438233=Kernel-based%20learning%20methods%20can%20dramatically%20increase%20the%20storage%20capacity%20of%20Hopfield%20networks%2C%20yet%20the%20dynamical%20mechanism%20behind%20this%20enhancement%20remains%20poorly%20understood.%20We%20address%20this%20gap%20by%20unifying%20the%20geometric%20analysis%20of%20the%20attractor%20landscape%20with%20the%20spectral%20theory%20of%20kernel%20machines.%20Using%20a%20novel%20metric%2C%20%22Pinnacle%20Sharpness%2C%22%20we%20first%20uncover%20a%20rich%20phase%20diagram%20of%20attractor%20stability%2C%20identifying%20a%20%22Ridge%20of%20Optimization%22%20where%20the%20network%20achieves%20maximal%20robustness%20under%20high-load%20conditions.%20Phenomenologically%2C%20this%20ridge%20is%20characterized%20by%20a%20%22Force%20Antagonism%2C%22%20where%20a%20strong%20driving%20force%20is%20balanced%20by%20a%20collective%20feedback%20force.%20Theoretically%2C%20we%20reveal%20that%20this%20phenomenon%20arises%20from%20a%20specific%20reorganization%20of%20the%20weight%20spectrum%2C%20which%20we%20term%20%5Ctextit%7BSpectral%20Concentration%7D.%20Unlike%20a%20simple%20rank-1%20collapse%2C%20our%20analysis%20shows%20that%20the%20network%20on%20the%20ridge%20self-organizes%20into%20a%20critical%20state%3A%20the%20leading%20eigenvalue%20is%20amplified%20to%20maximize%20global%20stability%20%28Direct%20Force%29%2C%20while%20the%20trailing%20eigenvalues%20are%20preserved%20to%20maintain%20high%20memory%20capacity%20%28Indirect%20Force%29.%20These%20findings%20provide%20a%20complete%20physical%20picture%20of%20how%20high-capacity%20associative%20memories%20are%20formed%2C%20demonstrating%20that%20optimal%20performance%20is%20achieved%20by%20tuning%20the%20system%20to%20a%20spectral%20%22Goldilocks%20zone%22%20between%20rank%20collapse%20and%20diffusion.&entry.1838667208=http%3A//arxiv.org/abs/2511.13053v3&entry.124074799=Read"},
{"title": "LightMem: Lightweight and Efficient Memory-Augmented Generation", "author": "Jizhan Fang and Xinle Deng and Haoming Xu and Ziyan Jiang and Yuqi Tang and Ziwen Xu and Shumin Deng and Yunzhi Yao and Mengru Wang and Shuofei Qiao and Huajun Chen and Ningyu Zhang", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. On LongMemEval and LoCoMo, using GPT and Qwen backbones, LightMem consistently surpasses strong baselines, improving QA accuracy by up to 7.7% / 29.3%, reducing total token usage by up to 38x / 20.9x and API calls by up to 30x / 55.5x, while purely online test-time costs are even lower, achieving up to 106x / 117x token reduction and 159x / 310x fewer API calls. The code is available at https://github.com/zjunlp/LightMem.", "link": "http://arxiv.org/abs/2510.18866v2", "date": "2025-11-25", "relevancy": 2.3664, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4754}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation&body=Title%3A%20LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation%0AAuthor%3A%20Jizhan%20Fang%20and%20Xinle%20Deng%20and%20Haoming%20Xu%20and%20Ziyan%20Jiang%20and%20Yuqi%20Tang%20and%20Ziwen%20Xu%20and%20Shumin%20Deng%20and%20Yunzhi%20Yao%20and%20Mengru%20Wang%20and%20Shuofei%20Qiao%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20Despite%20their%20remarkable%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20struggle%20to%20effectively%20leverage%20historical%20interaction%20information%20in%20dynamic%20and%20complex%20environments.%20Memory%20systems%20enable%20LLMs%20to%20move%20beyond%20stateless%20interactions%20by%20introducing%20persistent%20information%20storage%2C%20retrieval%2C%20and%20utilization%20mechanisms.%20However%2C%20existing%20memory%20systems%20often%20introduce%20substantial%20time%20and%20computational%20overhead.%20To%20this%20end%2C%20we%20introduce%20a%20new%20memory%20system%20called%20LightMem%2C%20which%20strikes%20a%20balance%20between%20the%20performance%20and%20efficiency%20of%20memory%20systems.%20Inspired%20by%20the%20Atkinson-Shiffrin%20model%20of%20human%20memory%2C%20LightMem%20organizes%20memory%20into%20three%20complementary%20stages.%20First%2C%20cognition-inspired%20sensory%20memory%20rapidly%20filters%20irrelevant%20information%20through%20lightweight%20compression%20and%20groups%20information%20according%20to%20their%20topics.%20Next%2C%20topic-aware%20short-term%20memory%20consolidates%20these%20topic-based%20groups%2C%20organizing%20and%20summarizing%20content%20for%20more%20structured%20access.%20Finally%2C%20long-term%20memory%20with%20sleep-time%20update%20employs%20an%20offline%20procedure%20that%20decouples%20consolidation%20from%20online%20inference.%20On%20LongMemEval%20and%20LoCoMo%2C%20using%20GPT%20and%20Qwen%20backbones%2C%20LightMem%20consistently%20surpasses%20strong%20baselines%2C%20improving%20QA%20accuracy%20by%20up%20to%207.7%25%20/%2029.3%25%2C%20reducing%20total%20token%20usage%20by%20up%20to%2038x%20/%2020.9x%20and%20API%20calls%20by%20up%20to%2030x%20/%2055.5x%2C%20while%20purely%20online%20test-time%20costs%20are%20even%20lower%2C%20achieving%20up%20to%20106x%20/%20117x%20token%20reduction%20and%20159x%20/%20310x%20fewer%20API%20calls.%20The%20code%20is%20available%20at%20https%3A//github.com/zjunlp/LightMem.%0ALink%3A%20http%3A//arxiv.org/abs/2510.18866v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightMem%253A%2520Lightweight%2520and%2520Efficient%2520Memory-Augmented%2520Generation%26entry.906535625%3DJizhan%2520Fang%2520and%2520Xinle%2520Deng%2520and%2520Haoming%2520Xu%2520and%2520Ziyan%2520Jiang%2520and%2520Yuqi%2520Tang%2520and%2520Ziwen%2520Xu%2520and%2520Shumin%2520Deng%2520and%2520Yunzhi%2520Yao%2520and%2520Mengru%2520Wang%2520and%2520Shuofei%2520Qiao%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3DDespite%2520their%2520remarkable%2520capabilities%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggle%2520to%2520effectively%2520leverage%2520historical%2520interaction%2520information%2520in%2520dynamic%2520and%2520complex%2520environments.%2520Memory%2520systems%2520enable%2520LLMs%2520to%2520move%2520beyond%2520stateless%2520interactions%2520by%2520introducing%2520persistent%2520information%2520storage%252C%2520retrieval%252C%2520and%2520utilization%2520mechanisms.%2520However%252C%2520existing%2520memory%2520systems%2520often%2520introduce%2520substantial%2520time%2520and%2520computational%2520overhead.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520new%2520memory%2520system%2520called%2520LightMem%252C%2520which%2520strikes%2520a%2520balance%2520between%2520the%2520performance%2520and%2520efficiency%2520of%2520memory%2520systems.%2520Inspired%2520by%2520the%2520Atkinson-Shiffrin%2520model%2520of%2520human%2520memory%252C%2520LightMem%2520organizes%2520memory%2520into%2520three%2520complementary%2520stages.%2520First%252C%2520cognition-inspired%2520sensory%2520memory%2520rapidly%2520filters%2520irrelevant%2520information%2520through%2520lightweight%2520compression%2520and%2520groups%2520information%2520according%2520to%2520their%2520topics.%2520Next%252C%2520topic-aware%2520short-term%2520memory%2520consolidates%2520these%2520topic-based%2520groups%252C%2520organizing%2520and%2520summarizing%2520content%2520for%2520more%2520structured%2520access.%2520Finally%252C%2520long-term%2520memory%2520with%2520sleep-time%2520update%2520employs%2520an%2520offline%2520procedure%2520that%2520decouples%2520consolidation%2520from%2520online%2520inference.%2520On%2520LongMemEval%2520and%2520LoCoMo%252C%2520using%2520GPT%2520and%2520Qwen%2520backbones%252C%2520LightMem%2520consistently%2520surpasses%2520strong%2520baselines%252C%2520improving%2520QA%2520accuracy%2520by%2520up%2520to%25207.7%2525%2520/%252029.3%2525%252C%2520reducing%2520total%2520token%2520usage%2520by%2520up%2520to%252038x%2520/%252020.9x%2520and%2520API%2520calls%2520by%2520up%2520to%252030x%2520/%252055.5x%252C%2520while%2520purely%2520online%2520test-time%2520costs%2520are%2520even%2520lower%252C%2520achieving%2520up%2520to%2520106x%2520/%2520117x%2520token%2520reduction%2520and%2520159x%2520/%2520310x%2520fewer%2520API%2520calls.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/zjunlp/LightMem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18866v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation&entry.906535625=Jizhan%20Fang%20and%20Xinle%20Deng%20and%20Haoming%20Xu%20and%20Ziyan%20Jiang%20and%20Yuqi%20Tang%20and%20Ziwen%20Xu%20and%20Shumin%20Deng%20and%20Yunzhi%20Yao%20and%20Mengru%20Wang%20and%20Shuofei%20Qiao%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=Despite%20their%20remarkable%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20struggle%20to%20effectively%20leverage%20historical%20interaction%20information%20in%20dynamic%20and%20complex%20environments.%20Memory%20systems%20enable%20LLMs%20to%20move%20beyond%20stateless%20interactions%20by%20introducing%20persistent%20information%20storage%2C%20retrieval%2C%20and%20utilization%20mechanisms.%20However%2C%20existing%20memory%20systems%20often%20introduce%20substantial%20time%20and%20computational%20overhead.%20To%20this%20end%2C%20we%20introduce%20a%20new%20memory%20system%20called%20LightMem%2C%20which%20strikes%20a%20balance%20between%20the%20performance%20and%20efficiency%20of%20memory%20systems.%20Inspired%20by%20the%20Atkinson-Shiffrin%20model%20of%20human%20memory%2C%20LightMem%20organizes%20memory%20into%20three%20complementary%20stages.%20First%2C%20cognition-inspired%20sensory%20memory%20rapidly%20filters%20irrelevant%20information%20through%20lightweight%20compression%20and%20groups%20information%20according%20to%20their%20topics.%20Next%2C%20topic-aware%20short-term%20memory%20consolidates%20these%20topic-based%20groups%2C%20organizing%20and%20summarizing%20content%20for%20more%20structured%20access.%20Finally%2C%20long-term%20memory%20with%20sleep-time%20update%20employs%20an%20offline%20procedure%20that%20decouples%20consolidation%20from%20online%20inference.%20On%20LongMemEval%20and%20LoCoMo%2C%20using%20GPT%20and%20Qwen%20backbones%2C%20LightMem%20consistently%20surpasses%20strong%20baselines%2C%20improving%20QA%20accuracy%20by%20up%20to%207.7%25%20/%2029.3%25%2C%20reducing%20total%20token%20usage%20by%20up%20to%2038x%20/%2020.9x%20and%20API%20calls%20by%20up%20to%2030x%20/%2055.5x%2C%20while%20purely%20online%20test-time%20costs%20are%20even%20lower%2C%20achieving%20up%20to%20106x%20/%20117x%20token%20reduction%20and%20159x%20/%20310x%20fewer%20API%20calls.%20The%20code%20is%20available%20at%20https%3A//github.com/zjunlp/LightMem.&entry.1838667208=http%3A//arxiv.org/abs/2510.18866v2&entry.124074799=Read"},
{"title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout", "author": "Hidir Yesiltepe and Tuna Han Salih Meral and Adil Kaan Akan and Kaan Oktay and Pinar Yanardag", "abstract": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.", "link": "http://arxiv.org/abs/2511.20649v1", "date": "2025-11-25", "relevancy": 2.3648, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5955}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5925}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinity-RoPE%3A%20Action-Controllable%20Infinite%20Video%20Generation%20Emerges%20From%20Autoregressive%20Self-Rollout&body=Title%3A%20Infinity-RoPE%3A%20Action-Controllable%20Infinite%20Video%20Generation%20Emerges%20From%20Autoregressive%20Self-Rollout%0AAuthor%3A%20Hidir%20Yesiltepe%20and%20Tuna%20Han%20Salih%20Meral%20and%20Adil%20Kaan%20Akan%20and%20Kaan%20Oktay%20and%20Pinar%20Yanardag%0AAbstract%3A%20Current%20autoregressive%20video%20diffusion%20models%20are%20constrained%20by%20three%20core%20bottlenecks%3A%20%28i%29%20the%20finite%20temporal%20horizon%20imposed%20by%20the%20base%20model%27s%203D%20Rotary%20Positional%20Embedding%20%283D-RoPE%29%2C%20%28ii%29%20slow%20prompt%20responsiveness%20in%20maintaining%20fine-grained%20action%20control%20during%20long-form%20rollouts%2C%20and%20%28iii%29%20the%20inability%20to%20realize%20discontinuous%20cinematic%20transitions%20within%20a%20single%20generation%20stream.%20We%20introduce%20%24%5Cinfty%24-RoPE%2C%20a%20unified%20inference-time%20framework%20that%20addresses%20all%20three%20limitations%20through%20three%20interconnected%20components%3A%20Block-Relativistic%20RoPE%2C%20KV%20Flush%2C%20and%20RoPE%20Cut.%20Block-Relativistic%20RoPE%20reformulates%20temporal%20encoding%20as%20a%20moving%20local%20reference%20frame%2C%20where%20each%20newly%20generated%20latent%20block%20is%20rotated%20relative%20to%20the%20base%20model%27s%20maximum%20frame%20horizon%20while%20earlier%20blocks%20are%20rotated%20backward%20to%20preserve%20relative%20temporal%20geometry.%20This%20relativistic%20formulation%20eliminates%20fixed%20temporal%20positions%2C%20enabling%20continuous%20video%20generation%20far%20beyond%20the%20base%20positional%20limits.%20To%20obtain%20fine-grained%20action%20control%20without%20re-encoding%2C%20KV%20Flush%20renews%20the%20KV%20cache%20by%20retaining%20only%20two%20latent%20frames%2C%20the%20global%20sink%20and%20the%20last%20generated%20latent%20frame%2C%20thereby%20ensuring%20immediate%20prompt%20responsiveness.%20Finally%2C%20RoPE%20Cut%20introduces%20controlled%20discontinuities%20in%20temporal%20RoPE%20coordinates%2C%20enabling%20multi-cut%20scene%20transitions%20within%20a%20single%20continuous%20rollout.%20Together%2C%20these%20components%20establish%20%24%5Cinfty%24-RoPE%20as%20a%20training-free%20foundation%20for%20infinite-horizon%2C%20controllable%2C%20and%20cinematic%20video%20diffusion.%20Comprehensive%20experiments%20show%20that%20%24%5Cinfty%24-RoPE%20consistently%20surpasses%20previous%20autoregressive%20models%20in%20overall%20VBench%20scores.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinity-RoPE%253A%2520Action-Controllable%2520Infinite%2520Video%2520Generation%2520Emerges%2520From%2520Autoregressive%2520Self-Rollout%26entry.906535625%3DHidir%2520Yesiltepe%2520and%2520Tuna%2520Han%2520Salih%2520Meral%2520and%2520Adil%2520Kaan%2520Akan%2520and%2520Kaan%2520Oktay%2520and%2520Pinar%2520Yanardag%26entry.1292438233%3DCurrent%2520autoregressive%2520video%2520diffusion%2520models%2520are%2520constrained%2520by%2520three%2520core%2520bottlenecks%253A%2520%2528i%2529%2520the%2520finite%2520temporal%2520horizon%2520imposed%2520by%2520the%2520base%2520model%2527s%25203D%2520Rotary%2520Positional%2520Embedding%2520%25283D-RoPE%2529%252C%2520%2528ii%2529%2520slow%2520prompt%2520responsiveness%2520in%2520maintaining%2520fine-grained%2520action%2520control%2520during%2520long-form%2520rollouts%252C%2520and%2520%2528iii%2529%2520the%2520inability%2520to%2520realize%2520discontinuous%2520cinematic%2520transitions%2520within%2520a%2520single%2520generation%2520stream.%2520We%2520introduce%2520%2524%255Cinfty%2524-RoPE%252C%2520a%2520unified%2520inference-time%2520framework%2520that%2520addresses%2520all%2520three%2520limitations%2520through%2520three%2520interconnected%2520components%253A%2520Block-Relativistic%2520RoPE%252C%2520KV%2520Flush%252C%2520and%2520RoPE%2520Cut.%2520Block-Relativistic%2520RoPE%2520reformulates%2520temporal%2520encoding%2520as%2520a%2520moving%2520local%2520reference%2520frame%252C%2520where%2520each%2520newly%2520generated%2520latent%2520block%2520is%2520rotated%2520relative%2520to%2520the%2520base%2520model%2527s%2520maximum%2520frame%2520horizon%2520while%2520earlier%2520blocks%2520are%2520rotated%2520backward%2520to%2520preserve%2520relative%2520temporal%2520geometry.%2520This%2520relativistic%2520formulation%2520eliminates%2520fixed%2520temporal%2520positions%252C%2520enabling%2520continuous%2520video%2520generation%2520far%2520beyond%2520the%2520base%2520positional%2520limits.%2520To%2520obtain%2520fine-grained%2520action%2520control%2520without%2520re-encoding%252C%2520KV%2520Flush%2520renews%2520the%2520KV%2520cache%2520by%2520retaining%2520only%2520two%2520latent%2520frames%252C%2520the%2520global%2520sink%2520and%2520the%2520last%2520generated%2520latent%2520frame%252C%2520thereby%2520ensuring%2520immediate%2520prompt%2520responsiveness.%2520Finally%252C%2520RoPE%2520Cut%2520introduces%2520controlled%2520discontinuities%2520in%2520temporal%2520RoPE%2520coordinates%252C%2520enabling%2520multi-cut%2520scene%2520transitions%2520within%2520a%2520single%2520continuous%2520rollout.%2520Together%252C%2520these%2520components%2520establish%2520%2524%255Cinfty%2524-RoPE%2520as%2520a%2520training-free%2520foundation%2520for%2520infinite-horizon%252C%2520controllable%252C%2520and%2520cinematic%2520video%2520diffusion.%2520Comprehensive%2520experiments%2520show%2520that%2520%2524%255Cinfty%2524-RoPE%2520consistently%2520surpasses%2520previous%2520autoregressive%2520models%2520in%2520overall%2520VBench%2520scores.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinity-RoPE%3A%20Action-Controllable%20Infinite%20Video%20Generation%20Emerges%20From%20Autoregressive%20Self-Rollout&entry.906535625=Hidir%20Yesiltepe%20and%20Tuna%20Han%20Salih%20Meral%20and%20Adil%20Kaan%20Akan%20and%20Kaan%20Oktay%20and%20Pinar%20Yanardag&entry.1292438233=Current%20autoregressive%20video%20diffusion%20models%20are%20constrained%20by%20three%20core%20bottlenecks%3A%20%28i%29%20the%20finite%20temporal%20horizon%20imposed%20by%20the%20base%20model%27s%203D%20Rotary%20Positional%20Embedding%20%283D-RoPE%29%2C%20%28ii%29%20slow%20prompt%20responsiveness%20in%20maintaining%20fine-grained%20action%20control%20during%20long-form%20rollouts%2C%20and%20%28iii%29%20the%20inability%20to%20realize%20discontinuous%20cinematic%20transitions%20within%20a%20single%20generation%20stream.%20We%20introduce%20%24%5Cinfty%24-RoPE%2C%20a%20unified%20inference-time%20framework%20that%20addresses%20all%20three%20limitations%20through%20three%20interconnected%20components%3A%20Block-Relativistic%20RoPE%2C%20KV%20Flush%2C%20and%20RoPE%20Cut.%20Block-Relativistic%20RoPE%20reformulates%20temporal%20encoding%20as%20a%20moving%20local%20reference%20frame%2C%20where%20each%20newly%20generated%20latent%20block%20is%20rotated%20relative%20to%20the%20base%20model%27s%20maximum%20frame%20horizon%20while%20earlier%20blocks%20are%20rotated%20backward%20to%20preserve%20relative%20temporal%20geometry.%20This%20relativistic%20formulation%20eliminates%20fixed%20temporal%20positions%2C%20enabling%20continuous%20video%20generation%20far%20beyond%20the%20base%20positional%20limits.%20To%20obtain%20fine-grained%20action%20control%20without%20re-encoding%2C%20KV%20Flush%20renews%20the%20KV%20cache%20by%20retaining%20only%20two%20latent%20frames%2C%20the%20global%20sink%20and%20the%20last%20generated%20latent%20frame%2C%20thereby%20ensuring%20immediate%20prompt%20responsiveness.%20Finally%2C%20RoPE%20Cut%20introduces%20controlled%20discontinuities%20in%20temporal%20RoPE%20coordinates%2C%20enabling%20multi-cut%20scene%20transitions%20within%20a%20single%20continuous%20rollout.%20Together%2C%20these%20components%20establish%20%24%5Cinfty%24-RoPE%20as%20a%20training-free%20foundation%20for%20infinite-horizon%2C%20controllable%2C%20and%20cinematic%20video%20diffusion.%20Comprehensive%20experiments%20show%20that%20%24%5Cinfty%24-RoPE%20consistently%20surpasses%20previous%20autoregressive%20models%20in%20overall%20VBench%20scores.&entry.1838667208=http%3A//arxiv.org/abs/2511.20649v1&entry.124074799=Read"},
{"title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward", "author": "Yuwei Niu and Weiyang Jin and Jiaqi Liao and Chaoran Feng and Peng Jin and Bin Lin and Zongjian Li and Bin Zhu and Weihao Yu and Li Yuan", "abstract": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox", "link": "http://arxiv.org/abs/2511.20561v1", "date": "2025-11-25", "relevancy": 2.3595, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5967}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5926}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Understanding%20Inform%20Generation%20in%20Unified%20Multimodal%20Models%3F%20From%20Analysis%20to%20Path%20Forward&body=Title%3A%20Does%20Understanding%20Inform%20Generation%20in%20Unified%20Multimodal%20Models%3F%20From%20Analysis%20to%20Path%20Forward%0AAuthor%3A%20Yuwei%20Niu%20and%20Weiyang%20Jin%20and%20Jiaqi%20Liao%20and%20Chaoran%20Feng%20and%20Peng%20Jin%20and%20Bin%20Lin%20and%20Zongjian%20Li%20and%20Bin%20Zhu%20and%20Weihao%20Yu%20and%20Li%20Yuan%0AAbstract%3A%20Recent%20years%20have%20witnessed%20significant%20progress%20in%20Unified%20Multimodal%20Models%2C%20yet%20a%20fundamental%20question%20remains%3A%20Does%20understanding%20truly%20inform%20generation%3F%20To%20investigate%20this%2C%20we%20introduce%20UniSandbox%2C%20a%20decoupled%20evaluation%20framework%20paired%20with%20controlled%2C%20synthetic%20datasets%20to%20avoid%20data%20leakage%20and%20enable%20detailed%20analysis.%20Our%20findings%20reveal%20a%20significant%20understanding-generation%20gap%2C%20which%20is%20mainly%20reflected%20in%20two%20key%20dimensions%3A%20reasoning%20generation%20and%20knowledge%20transfer.%20Specifically%2C%20for%20reasoning%20generation%20tasks%2C%20we%20observe%20that%20explicit%20Chain-of-Thought%20%28CoT%29%20in%20the%20understanding%20module%20effectively%20bridges%20the%20gap%2C%20and%20further%20demonstrate%20that%20a%20self-training%20approach%20can%20successfully%20internalize%20this%20ability%2C%20enabling%20implicit%20reasoning%20during%20generation.%20Additionally%2C%20for%20knowledge%20transfer%20tasks%2C%20we%20find%20that%20CoT%20assists%20the%20generative%20process%20by%20helping%20retrieve%20newly%20learned%20knowledge%2C%20and%20also%20discover%20that%20query-based%20architectures%20inherently%20exhibit%20latent%20CoT-like%20properties%20that%20affect%20this%20transfer.%20UniSandbox%20provides%20preliminary%20insights%20for%20designing%20future%20unified%20architectures%20and%20training%20strategies%20that%20truly%20bridge%20the%20gap%20between%20understanding%20and%20generation.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/PKU-YuanGroup/UniSandBox%0ALink%3A%20http%3A//arxiv.org/abs/2511.20561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Understanding%2520Inform%2520Generation%2520in%2520Unified%2520Multimodal%2520Models%253F%2520From%2520Analysis%2520to%2520Path%2520Forward%26entry.906535625%3DYuwei%2520Niu%2520and%2520Weiyang%2520Jin%2520and%2520Jiaqi%2520Liao%2520and%2520Chaoran%2520Feng%2520and%2520Peng%2520Jin%2520and%2520Bin%2520Lin%2520and%2520Zongjian%2520Li%2520and%2520Bin%2520Zhu%2520and%2520Weihao%2520Yu%2520and%2520Li%2520Yuan%26entry.1292438233%3DRecent%2520years%2520have%2520witnessed%2520significant%2520progress%2520in%2520Unified%2520Multimodal%2520Models%252C%2520yet%2520a%2520fundamental%2520question%2520remains%253A%2520Does%2520understanding%2520truly%2520inform%2520generation%253F%2520To%2520investigate%2520this%252C%2520we%2520introduce%2520UniSandbox%252C%2520a%2520decoupled%2520evaluation%2520framework%2520paired%2520with%2520controlled%252C%2520synthetic%2520datasets%2520to%2520avoid%2520data%2520leakage%2520and%2520enable%2520detailed%2520analysis.%2520Our%2520findings%2520reveal%2520a%2520significant%2520understanding-generation%2520gap%252C%2520which%2520is%2520mainly%2520reflected%2520in%2520two%2520key%2520dimensions%253A%2520reasoning%2520generation%2520and%2520knowledge%2520transfer.%2520Specifically%252C%2520for%2520reasoning%2520generation%2520tasks%252C%2520we%2520observe%2520that%2520explicit%2520Chain-of-Thought%2520%2528CoT%2529%2520in%2520the%2520understanding%2520module%2520effectively%2520bridges%2520the%2520gap%252C%2520and%2520further%2520demonstrate%2520that%2520a%2520self-training%2520approach%2520can%2520successfully%2520internalize%2520this%2520ability%252C%2520enabling%2520implicit%2520reasoning%2520during%2520generation.%2520Additionally%252C%2520for%2520knowledge%2520transfer%2520tasks%252C%2520we%2520find%2520that%2520CoT%2520assists%2520the%2520generative%2520process%2520by%2520helping%2520retrieve%2520newly%2520learned%2520knowledge%252C%2520and%2520also%2520discover%2520that%2520query-based%2520architectures%2520inherently%2520exhibit%2520latent%2520CoT-like%2520properties%2520that%2520affect%2520this%2520transfer.%2520UniSandbox%2520provides%2520preliminary%2520insights%2520for%2520designing%2520future%2520unified%2520architectures%2520and%2520training%2520strategies%2520that%2520truly%2520bridge%2520the%2520gap%2520between%2520understanding%2520and%2520generation.%2520Code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/PKU-YuanGroup/UniSandBox%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Understanding%20Inform%20Generation%20in%20Unified%20Multimodal%20Models%3F%20From%20Analysis%20to%20Path%20Forward&entry.906535625=Yuwei%20Niu%20and%20Weiyang%20Jin%20and%20Jiaqi%20Liao%20and%20Chaoran%20Feng%20and%20Peng%20Jin%20and%20Bin%20Lin%20and%20Zongjian%20Li%20and%20Bin%20Zhu%20and%20Weihao%20Yu%20and%20Li%20Yuan&entry.1292438233=Recent%20years%20have%20witnessed%20significant%20progress%20in%20Unified%20Multimodal%20Models%2C%20yet%20a%20fundamental%20question%20remains%3A%20Does%20understanding%20truly%20inform%20generation%3F%20To%20investigate%20this%2C%20we%20introduce%20UniSandbox%2C%20a%20decoupled%20evaluation%20framework%20paired%20with%20controlled%2C%20synthetic%20datasets%20to%20avoid%20data%20leakage%20and%20enable%20detailed%20analysis.%20Our%20findings%20reveal%20a%20significant%20understanding-generation%20gap%2C%20which%20is%20mainly%20reflected%20in%20two%20key%20dimensions%3A%20reasoning%20generation%20and%20knowledge%20transfer.%20Specifically%2C%20for%20reasoning%20generation%20tasks%2C%20we%20observe%20that%20explicit%20Chain-of-Thought%20%28CoT%29%20in%20the%20understanding%20module%20effectively%20bridges%20the%20gap%2C%20and%20further%20demonstrate%20that%20a%20self-training%20approach%20can%20successfully%20internalize%20this%20ability%2C%20enabling%20implicit%20reasoning%20during%20generation.%20Additionally%2C%20for%20knowledge%20transfer%20tasks%2C%20we%20find%20that%20CoT%20assists%20the%20generative%20process%20by%20helping%20retrieve%20newly%20learned%20knowledge%2C%20and%20also%20discover%20that%20query-based%20architectures%20inherently%20exhibit%20latent%20CoT-like%20properties%20that%20affect%20this%20transfer.%20UniSandbox%20provides%20preliminary%20insights%20for%20designing%20future%20unified%20architectures%20and%20training%20strategies%20that%20truly%20bridge%20the%20gap%20between%20understanding%20and%20generation.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/PKU-YuanGroup/UniSandBox&entry.1838667208=http%3A//arxiv.org/abs/2511.20561v1&entry.124074799=Read"},
{"title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference", "author": "Jianhao Yuan and Fabio Pizzati and Francesco Pinto and Lars Kunze and Ivan Laptev and Paul Newman and Philip Torr and Daniele De Martini", "abstract": "Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.", "link": "http://arxiv.org/abs/2510.11512v2", "date": "2025-11-25", "relevancy": 2.3548, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5865}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LikePhys%3A%20Evaluating%20Intuitive%20Physics%20Understanding%20in%20Video%20Diffusion%20Models%20via%20Likelihood%20Preference&body=Title%3A%20LikePhys%3A%20Evaluating%20Intuitive%20Physics%20Understanding%20in%20Video%20Diffusion%20Models%20via%20Likelihood%20Preference%0AAuthor%3A%20Jianhao%20Yuan%20and%20Fabio%20Pizzati%20and%20Francesco%20Pinto%20and%20Lars%20Kunze%20and%20Ivan%20Laptev%20and%20Paul%20Newman%20and%20Philip%20Torr%20and%20Daniele%20De%20Martini%0AAbstract%3A%20Intuitive%20physics%20understanding%20in%20video%20diffusion%20models%20plays%20an%20essential%20role%20in%20building%20general-purpose%20physically%20plausible%20world%20simulators%2C%20yet%20accurately%20evaluating%20such%20capacity%20remains%20a%20challenging%20task%20due%20to%20the%20difficulty%20in%20disentangling%20physics%20correctness%20from%20visual%20appearance%20in%20generation.%20To%20the%20end%2C%20we%20introduce%20LikePhys%2C%20a%20training-free%20method%20that%20evaluates%20intuitive%20physics%20in%20video%20diffusion%20models%20by%20distinguishing%20physically%20valid%20and%20impossible%20videos%20using%20the%20denoising%20objective%20as%20an%20ELBO-based%20likelihood%20surrogate%20on%20a%20curated%20dataset%20of%20valid-invalid%20pairs.%20By%20testing%20on%20our%20constructed%20benchmark%20of%20twelve%20scenarios%20spanning%20over%20four%20physics%20domains%2C%20we%20show%20that%20our%20evaluation%20metric%2C%20Plausibility%20Preference%20Error%20%28PPE%29%2C%20demonstrates%20strong%20alignment%20with%20human%20preference%2C%20outperforming%20state-of-the-art%20evaluator%20baselines.%20We%20then%20systematically%20benchmark%20intuitive%20physics%20understanding%20in%20current%20video%20diffusion%20models.%20Our%20study%20further%20analyses%20how%20model%20design%20and%20inference%20settings%20affect%20intuitive%20physics%20understanding%20and%20highlights%20domain-specific%20capacity%20variations%20across%20physical%20laws.%20Empirical%20results%20show%20that%2C%20despite%20current%20models%20struggling%20with%20complex%20and%20chaotic%20dynamics%2C%20there%20is%20a%20clear%20trend%20of%20improvement%20in%20physics%20understanding%20as%20model%20capacity%20and%20inference%20settings%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11512v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLikePhys%253A%2520Evaluating%2520Intuitive%2520Physics%2520Understanding%2520in%2520Video%2520Diffusion%2520Models%2520via%2520Likelihood%2520Preference%26entry.906535625%3DJianhao%2520Yuan%2520and%2520Fabio%2520Pizzati%2520and%2520Francesco%2520Pinto%2520and%2520Lars%2520Kunze%2520and%2520Ivan%2520Laptev%2520and%2520Paul%2520Newman%2520and%2520Philip%2520Torr%2520and%2520Daniele%2520De%2520Martini%26entry.1292438233%3DIntuitive%2520physics%2520understanding%2520in%2520video%2520diffusion%2520models%2520plays%2520an%2520essential%2520role%2520in%2520building%2520general-purpose%2520physically%2520plausible%2520world%2520simulators%252C%2520yet%2520accurately%2520evaluating%2520such%2520capacity%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520difficulty%2520in%2520disentangling%2520physics%2520correctness%2520from%2520visual%2520appearance%2520in%2520generation.%2520To%2520the%2520end%252C%2520we%2520introduce%2520LikePhys%252C%2520a%2520training-free%2520method%2520that%2520evaluates%2520intuitive%2520physics%2520in%2520video%2520diffusion%2520models%2520by%2520distinguishing%2520physically%2520valid%2520and%2520impossible%2520videos%2520using%2520the%2520denoising%2520objective%2520as%2520an%2520ELBO-based%2520likelihood%2520surrogate%2520on%2520a%2520curated%2520dataset%2520of%2520valid-invalid%2520pairs.%2520By%2520testing%2520on%2520our%2520constructed%2520benchmark%2520of%2520twelve%2520scenarios%2520spanning%2520over%2520four%2520physics%2520domains%252C%2520we%2520show%2520that%2520our%2520evaluation%2520metric%252C%2520Plausibility%2520Preference%2520Error%2520%2528PPE%2529%252C%2520demonstrates%2520strong%2520alignment%2520with%2520human%2520preference%252C%2520outperforming%2520state-of-the-art%2520evaluator%2520baselines.%2520We%2520then%2520systematically%2520benchmark%2520intuitive%2520physics%2520understanding%2520in%2520current%2520video%2520diffusion%2520models.%2520Our%2520study%2520further%2520analyses%2520how%2520model%2520design%2520and%2520inference%2520settings%2520affect%2520intuitive%2520physics%2520understanding%2520and%2520highlights%2520domain-specific%2520capacity%2520variations%2520across%2520physical%2520laws.%2520Empirical%2520results%2520show%2520that%252C%2520despite%2520current%2520models%2520struggling%2520with%2520complex%2520and%2520chaotic%2520dynamics%252C%2520there%2520is%2520a%2520clear%2520trend%2520of%2520improvement%2520in%2520physics%2520understanding%2520as%2520model%2520capacity%2520and%2520inference%2520settings%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11512v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LikePhys%3A%20Evaluating%20Intuitive%20Physics%20Understanding%20in%20Video%20Diffusion%20Models%20via%20Likelihood%20Preference&entry.906535625=Jianhao%20Yuan%20and%20Fabio%20Pizzati%20and%20Francesco%20Pinto%20and%20Lars%20Kunze%20and%20Ivan%20Laptev%20and%20Paul%20Newman%20and%20Philip%20Torr%20and%20Daniele%20De%20Martini&entry.1292438233=Intuitive%20physics%20understanding%20in%20video%20diffusion%20models%20plays%20an%20essential%20role%20in%20building%20general-purpose%20physically%20plausible%20world%20simulators%2C%20yet%20accurately%20evaluating%20such%20capacity%20remains%20a%20challenging%20task%20due%20to%20the%20difficulty%20in%20disentangling%20physics%20correctness%20from%20visual%20appearance%20in%20generation.%20To%20the%20end%2C%20we%20introduce%20LikePhys%2C%20a%20training-free%20method%20that%20evaluates%20intuitive%20physics%20in%20video%20diffusion%20models%20by%20distinguishing%20physically%20valid%20and%20impossible%20videos%20using%20the%20denoising%20objective%20as%20an%20ELBO-based%20likelihood%20surrogate%20on%20a%20curated%20dataset%20of%20valid-invalid%20pairs.%20By%20testing%20on%20our%20constructed%20benchmark%20of%20twelve%20scenarios%20spanning%20over%20four%20physics%20domains%2C%20we%20show%20that%20our%20evaluation%20metric%2C%20Plausibility%20Preference%20Error%20%28PPE%29%2C%20demonstrates%20strong%20alignment%20with%20human%20preference%2C%20outperforming%20state-of-the-art%20evaluator%20baselines.%20We%20then%20systematically%20benchmark%20intuitive%20physics%20understanding%20in%20current%20video%20diffusion%20models.%20Our%20study%20further%20analyses%20how%20model%20design%20and%20inference%20settings%20affect%20intuitive%20physics%20understanding%20and%20highlights%20domain-specific%20capacity%20variations%20across%20physical%20laws.%20Empirical%20results%20show%20that%2C%20despite%20current%20models%20struggling%20with%20complex%20and%20chaotic%20dynamics%2C%20there%20is%20a%20clear%20trend%20of%20improvement%20in%20physics%20understanding%20as%20model%20capacity%20and%20inference%20settings%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2510.11512v2&entry.124074799=Read"},
{"title": "SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery", "author": "Da Li and Ji-Ping Jin and Xuanlong Yu and Wei Liu and Xiaodong Cun and Kai Chen and Rui Fan and Jiangang Kong and Shen Xi", "abstract": "Parametric 3D human models such as SMPL have driven significant advances in human pose and shape estimation, yet their simplified kinematics limit biomechanical realism. The recently proposed SKEL model addresses this limitation by re-rigging SMPL with an anatomically accurate skeleton. However, estimating SKEL parameters directly remains challenging due to limited training data, perspective ambiguities, and the inherent complexity of human articulation. We introduce SKEL-CF, a coarse-to-fine framework for SKEL parameter estimation. SKEL-CF employs a transformer-based encoder-decoder architecture, where the encoder predicts coarse camera and SKEL parameters, and the decoder progressively refines them in successive layers. To ensure anatomically consistent supervision, we convert the existing SMPL-based dataset 4DHuman into a SKEL-aligned version, 4DHuman-SKEL, providing high-quality training data for SKEL estimation. In addition, to mitigate depth and scale ambiguities, we explicitly incorporate camera modeling into the SKEL-CF pipeline and demonstrate its importance across diverse viewpoints. Extensive experiments validate the effectiveness of the proposed design. On the challenging MOYO dataset, SKEL-CF achieves 85.0 MPJPE / 51.4 PA-MPJPE, significantly outperforming the previous SKEL-based state-of-the-art HSMR (104.5 / 79.6). These results establish SKEL-CF as a scalable and anatomically faithful framework for human motion analysis, bridging the gap between computer vision and biomechanics. Our implementation is available on the project page: https://pokerman8.github.io/SKEL-CF/.", "link": "http://arxiv.org/abs/2511.20157v1", "date": "2025-11-25", "relevancy": 2.3539, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6072}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKEL-CF%3A%20Coarse-to-Fine%20Biomechanical%20Skeleton%20and%20Surface%20Mesh%20Recovery&body=Title%3A%20SKEL-CF%3A%20Coarse-to-Fine%20Biomechanical%20Skeleton%20and%20Surface%20Mesh%20Recovery%0AAuthor%3A%20Da%20Li%20and%20Ji-Ping%20Jin%20and%20Xuanlong%20Yu%20and%20Wei%20Liu%20and%20Xiaodong%20Cun%20and%20Kai%20Chen%20and%20Rui%20Fan%20and%20Jiangang%20Kong%20and%20Shen%20Xi%0AAbstract%3A%20Parametric%203D%20human%20models%20such%20as%20SMPL%20have%20driven%20significant%20advances%20in%20human%20pose%20and%20shape%20estimation%2C%20yet%20their%20simplified%20kinematics%20limit%20biomechanical%20realism.%20The%20recently%20proposed%20SKEL%20model%20addresses%20this%20limitation%20by%20re-rigging%20SMPL%20with%20an%20anatomically%20accurate%20skeleton.%20However%2C%20estimating%20SKEL%20parameters%20directly%20remains%20challenging%20due%20to%20limited%20training%20data%2C%20perspective%20ambiguities%2C%20and%20the%20inherent%20complexity%20of%20human%20articulation.%20We%20introduce%20SKEL-CF%2C%20a%20coarse-to-fine%20framework%20for%20SKEL%20parameter%20estimation.%20SKEL-CF%20employs%20a%20transformer-based%20encoder-decoder%20architecture%2C%20where%20the%20encoder%20predicts%20coarse%20camera%20and%20SKEL%20parameters%2C%20and%20the%20decoder%20progressively%20refines%20them%20in%20successive%20layers.%20To%20ensure%20anatomically%20consistent%20supervision%2C%20we%20convert%20the%20existing%20SMPL-based%20dataset%204DHuman%20into%20a%20SKEL-aligned%20version%2C%204DHuman-SKEL%2C%20providing%20high-quality%20training%20data%20for%20SKEL%20estimation.%20In%20addition%2C%20to%20mitigate%20depth%20and%20scale%20ambiguities%2C%20we%20explicitly%20incorporate%20camera%20modeling%20into%20the%20SKEL-CF%20pipeline%20and%20demonstrate%20its%20importance%20across%20diverse%20viewpoints.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20the%20proposed%20design.%20On%20the%20challenging%20MOYO%20dataset%2C%20SKEL-CF%20achieves%2085.0%20MPJPE%20/%2051.4%20PA-MPJPE%2C%20significantly%20outperforming%20the%20previous%20SKEL-based%20state-of-the-art%20HSMR%20%28104.5%20/%2079.6%29.%20These%20results%20establish%20SKEL-CF%20as%20a%20scalable%20and%20anatomically%20faithful%20framework%20for%20human%20motion%20analysis%2C%20bridging%20the%20gap%20between%20computer%20vision%20and%20biomechanics.%20Our%20implementation%20is%20available%20on%20the%20project%20page%3A%20https%3A//pokerman8.github.io/SKEL-CF/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKEL-CF%253A%2520Coarse-to-Fine%2520Biomechanical%2520Skeleton%2520and%2520Surface%2520Mesh%2520Recovery%26entry.906535625%3DDa%2520Li%2520and%2520Ji-Ping%2520Jin%2520and%2520Xuanlong%2520Yu%2520and%2520Wei%2520Liu%2520and%2520Xiaodong%2520Cun%2520and%2520Kai%2520Chen%2520and%2520Rui%2520Fan%2520and%2520Jiangang%2520Kong%2520and%2520Shen%2520Xi%26entry.1292438233%3DParametric%25203D%2520human%2520models%2520such%2520as%2520SMPL%2520have%2520driven%2520significant%2520advances%2520in%2520human%2520pose%2520and%2520shape%2520estimation%252C%2520yet%2520their%2520simplified%2520kinematics%2520limit%2520biomechanical%2520realism.%2520The%2520recently%2520proposed%2520SKEL%2520model%2520addresses%2520this%2520limitation%2520by%2520re-rigging%2520SMPL%2520with%2520an%2520anatomically%2520accurate%2520skeleton.%2520However%252C%2520estimating%2520SKEL%2520parameters%2520directly%2520remains%2520challenging%2520due%2520to%2520limited%2520training%2520data%252C%2520perspective%2520ambiguities%252C%2520and%2520the%2520inherent%2520complexity%2520of%2520human%2520articulation.%2520We%2520introduce%2520SKEL-CF%252C%2520a%2520coarse-to-fine%2520framework%2520for%2520SKEL%2520parameter%2520estimation.%2520SKEL-CF%2520employs%2520a%2520transformer-based%2520encoder-decoder%2520architecture%252C%2520where%2520the%2520encoder%2520predicts%2520coarse%2520camera%2520and%2520SKEL%2520parameters%252C%2520and%2520the%2520decoder%2520progressively%2520refines%2520them%2520in%2520successive%2520layers.%2520To%2520ensure%2520anatomically%2520consistent%2520supervision%252C%2520we%2520convert%2520the%2520existing%2520SMPL-based%2520dataset%25204DHuman%2520into%2520a%2520SKEL-aligned%2520version%252C%25204DHuman-SKEL%252C%2520providing%2520high-quality%2520training%2520data%2520for%2520SKEL%2520estimation.%2520In%2520addition%252C%2520to%2520mitigate%2520depth%2520and%2520scale%2520ambiguities%252C%2520we%2520explicitly%2520incorporate%2520camera%2520modeling%2520into%2520the%2520SKEL-CF%2520pipeline%2520and%2520demonstrate%2520its%2520importance%2520across%2520diverse%2520viewpoints.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520design.%2520On%2520the%2520challenging%2520MOYO%2520dataset%252C%2520SKEL-CF%2520achieves%252085.0%2520MPJPE%2520/%252051.4%2520PA-MPJPE%252C%2520significantly%2520outperforming%2520the%2520previous%2520SKEL-based%2520state-of-the-art%2520HSMR%2520%2528104.5%2520/%252079.6%2529.%2520These%2520results%2520establish%2520SKEL-CF%2520as%2520a%2520scalable%2520and%2520anatomically%2520faithful%2520framework%2520for%2520human%2520motion%2520analysis%252C%2520bridging%2520the%2520gap%2520between%2520computer%2520vision%2520and%2520biomechanics.%2520Our%2520implementation%2520is%2520available%2520on%2520the%2520project%2520page%253A%2520https%253A//pokerman8.github.io/SKEL-CF/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKEL-CF%3A%20Coarse-to-Fine%20Biomechanical%20Skeleton%20and%20Surface%20Mesh%20Recovery&entry.906535625=Da%20Li%20and%20Ji-Ping%20Jin%20and%20Xuanlong%20Yu%20and%20Wei%20Liu%20and%20Xiaodong%20Cun%20and%20Kai%20Chen%20and%20Rui%20Fan%20and%20Jiangang%20Kong%20and%20Shen%20Xi&entry.1292438233=Parametric%203D%20human%20models%20such%20as%20SMPL%20have%20driven%20significant%20advances%20in%20human%20pose%20and%20shape%20estimation%2C%20yet%20their%20simplified%20kinematics%20limit%20biomechanical%20realism.%20The%20recently%20proposed%20SKEL%20model%20addresses%20this%20limitation%20by%20re-rigging%20SMPL%20with%20an%20anatomically%20accurate%20skeleton.%20However%2C%20estimating%20SKEL%20parameters%20directly%20remains%20challenging%20due%20to%20limited%20training%20data%2C%20perspective%20ambiguities%2C%20and%20the%20inherent%20complexity%20of%20human%20articulation.%20We%20introduce%20SKEL-CF%2C%20a%20coarse-to-fine%20framework%20for%20SKEL%20parameter%20estimation.%20SKEL-CF%20employs%20a%20transformer-based%20encoder-decoder%20architecture%2C%20where%20the%20encoder%20predicts%20coarse%20camera%20and%20SKEL%20parameters%2C%20and%20the%20decoder%20progressively%20refines%20them%20in%20successive%20layers.%20To%20ensure%20anatomically%20consistent%20supervision%2C%20we%20convert%20the%20existing%20SMPL-based%20dataset%204DHuman%20into%20a%20SKEL-aligned%20version%2C%204DHuman-SKEL%2C%20providing%20high-quality%20training%20data%20for%20SKEL%20estimation.%20In%20addition%2C%20to%20mitigate%20depth%20and%20scale%20ambiguities%2C%20we%20explicitly%20incorporate%20camera%20modeling%20into%20the%20SKEL-CF%20pipeline%20and%20demonstrate%20its%20importance%20across%20diverse%20viewpoints.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20the%20proposed%20design.%20On%20the%20challenging%20MOYO%20dataset%2C%20SKEL-CF%20achieves%2085.0%20MPJPE%20/%2051.4%20PA-MPJPE%2C%20significantly%20outperforming%20the%20previous%20SKEL-based%20state-of-the-art%20HSMR%20%28104.5%20/%2079.6%29.%20These%20results%20establish%20SKEL-CF%20as%20a%20scalable%20and%20anatomically%20faithful%20framework%20for%20human%20motion%20analysis%2C%20bridging%20the%20gap%20between%20computer%20vision%20and%20biomechanics.%20Our%20implementation%20is%20available%20on%20the%20project%20page%3A%20https%3A//pokerman8.github.io/SKEL-CF/.&entry.1838667208=http%3A//arxiv.org/abs/2511.20157v1&entry.124074799=Read"},
{"title": "From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations", "author": "Zhiqing Guo and Dongdong Xi and Songlin Li and Gaobo Yang", "abstract": "Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.", "link": "http://arxiv.org/abs/2511.20359v1", "date": "2025-11-25", "relevancy": 2.343, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6143}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5711}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Passive%20Perception%20to%20Active%20Memory%3A%20A%20Weakly%20Supervised%20Image%20Manipulation%20Localization%20Framework%20Driven%20by%20Coarse-Grained%20Annotations&body=Title%3A%20From%20Passive%20Perception%20to%20Active%20Memory%3A%20A%20Weakly%20Supervised%20Image%20Manipulation%20Localization%20Framework%20Driven%20by%20Coarse-Grained%20Annotations%0AAuthor%3A%20Zhiqing%20Guo%20and%20Dongdong%20Xi%20and%20Songlin%20Li%20and%20Gaobo%20Yang%0AAbstract%3A%20Image%20manipulation%20localization%20%28IML%29%20faces%20a%20fundamental%20trade-off%20between%20minimizing%20annotation%20cost%20and%20achieving%20fine-grained%20localization%20accuracy.%20Existing%20fully-supervised%20IML%20methods%20depend%20heavily%20on%20dense%20pixel-level%20mask%20annotations%2C%20which%20limits%20scalability%20to%20large%20datasets%20or%20real-world%20deployment.In%20contrast%2C%20the%20majority%20of%20existing%20weakly-supervised%20IML%20approaches%20are%20based%20on%20image-level%20labels%2C%20which%20greatly%20reduce%20annotation%20effort%20but%20typically%20lack%20precise%20spatial%20localization.%20To%20address%20this%20dilemma%2C%20we%20propose%20BoxPromptIML%2C%20a%20novel%20weakly-supervised%20IML%20framework%20that%20effectively%20balances%20annotation%20cost%20and%20localization%20performance.%20Specifically%2C%20we%20propose%20a%20coarse%20region%20annotation%20strategy%2C%20which%20can%20generate%20relatively%20accurate%20manipulation%20masks%20at%20lower%20cost.%20To%20improve%20model%20efficiency%20and%20facilitate%20deployment%2C%20we%20further%20design%20an%20efficient%20lightweight%20student%20model%2C%20which%20learns%20to%20perform%20fine-grained%20localization%20through%20knowledge%20distillation%20from%20a%20fixed%20teacher%20model%20based%20on%20the%20Segment%20Anything%20Model%20%28SAM%29.%20Moreover%2C%20inspired%20by%20the%20human%20subconscious%20memory%20mechanism%2C%20our%20feature%20fusion%20module%20employs%20a%20dual-guidance%20strategy%20that%20actively%20contextualizes%20recalled%20prototypical%20patterns%20with%20real-time%20observational%20cues%20derived%20from%20the%20input.%20Instead%20of%20passive%20feature%20extraction%2C%20this%20strategy%20enables%20a%20dynamic%20process%20of%20knowledge%20recollection%2C%20where%20long-term%20memory%20is%20adapted%20to%20the%20specific%20context%20of%20the%20current%20image%2C%20significantly%20enhancing%20localization%20accuracy%20and%20robustness.%20Extensive%20experiments%20across%20both%20in-distribution%20and%20out-of-distribution%20datasets%20show%20that%20BoxPromptIML%20outperforms%20or%20rivals%20fully-supervised%20models%2C%20while%20maintaining%20strong%20generalization%2C%20low%20annotation%20cost%2C%20and%20efficient%20deployment%20characteristics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Passive%2520Perception%2520to%2520Active%2520Memory%253A%2520A%2520Weakly%2520Supervised%2520Image%2520Manipulation%2520Localization%2520Framework%2520Driven%2520by%2520Coarse-Grained%2520Annotations%26entry.906535625%3DZhiqing%2520Guo%2520and%2520Dongdong%2520Xi%2520and%2520Songlin%2520Li%2520and%2520Gaobo%2520Yang%26entry.1292438233%3DImage%2520manipulation%2520localization%2520%2528IML%2529%2520faces%2520a%2520fundamental%2520trade-off%2520between%2520minimizing%2520annotation%2520cost%2520and%2520achieving%2520fine-grained%2520localization%2520accuracy.%2520Existing%2520fully-supervised%2520IML%2520methods%2520depend%2520heavily%2520on%2520dense%2520pixel-level%2520mask%2520annotations%252C%2520which%2520limits%2520scalability%2520to%2520large%2520datasets%2520or%2520real-world%2520deployment.In%2520contrast%252C%2520the%2520majority%2520of%2520existing%2520weakly-supervised%2520IML%2520approaches%2520are%2520based%2520on%2520image-level%2520labels%252C%2520which%2520greatly%2520reduce%2520annotation%2520effort%2520but%2520typically%2520lack%2520precise%2520spatial%2520localization.%2520To%2520address%2520this%2520dilemma%252C%2520we%2520propose%2520BoxPromptIML%252C%2520a%2520novel%2520weakly-supervised%2520IML%2520framework%2520that%2520effectively%2520balances%2520annotation%2520cost%2520and%2520localization%2520performance.%2520Specifically%252C%2520we%2520propose%2520a%2520coarse%2520region%2520annotation%2520strategy%252C%2520which%2520can%2520generate%2520relatively%2520accurate%2520manipulation%2520masks%2520at%2520lower%2520cost.%2520To%2520improve%2520model%2520efficiency%2520and%2520facilitate%2520deployment%252C%2520we%2520further%2520design%2520an%2520efficient%2520lightweight%2520student%2520model%252C%2520which%2520learns%2520to%2520perform%2520fine-grained%2520localization%2520through%2520knowledge%2520distillation%2520from%2520a%2520fixed%2520teacher%2520model%2520based%2520on%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529.%2520Moreover%252C%2520inspired%2520by%2520the%2520human%2520subconscious%2520memory%2520mechanism%252C%2520our%2520feature%2520fusion%2520module%2520employs%2520a%2520dual-guidance%2520strategy%2520that%2520actively%2520contextualizes%2520recalled%2520prototypical%2520patterns%2520with%2520real-time%2520observational%2520cues%2520derived%2520from%2520the%2520input.%2520Instead%2520of%2520passive%2520feature%2520extraction%252C%2520this%2520strategy%2520enables%2520a%2520dynamic%2520process%2520of%2520knowledge%2520recollection%252C%2520where%2520long-term%2520memory%2520is%2520adapted%2520to%2520the%2520specific%2520context%2520of%2520the%2520current%2520image%252C%2520significantly%2520enhancing%2520localization%2520accuracy%2520and%2520robustness.%2520Extensive%2520experiments%2520across%2520both%2520in-distribution%2520and%2520out-of-distribution%2520datasets%2520show%2520that%2520BoxPromptIML%2520outperforms%2520or%2520rivals%2520fully-supervised%2520models%252C%2520while%2520maintaining%2520strong%2520generalization%252C%2520low%2520annotation%2520cost%252C%2520and%2520efficient%2520deployment%2520characteristics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Passive%20Perception%20to%20Active%20Memory%3A%20A%20Weakly%20Supervised%20Image%20Manipulation%20Localization%20Framework%20Driven%20by%20Coarse-Grained%20Annotations&entry.906535625=Zhiqing%20Guo%20and%20Dongdong%20Xi%20and%20Songlin%20Li%20and%20Gaobo%20Yang&entry.1292438233=Image%20manipulation%20localization%20%28IML%29%20faces%20a%20fundamental%20trade-off%20between%20minimizing%20annotation%20cost%20and%20achieving%20fine-grained%20localization%20accuracy.%20Existing%20fully-supervised%20IML%20methods%20depend%20heavily%20on%20dense%20pixel-level%20mask%20annotations%2C%20which%20limits%20scalability%20to%20large%20datasets%20or%20real-world%20deployment.In%20contrast%2C%20the%20majority%20of%20existing%20weakly-supervised%20IML%20approaches%20are%20based%20on%20image-level%20labels%2C%20which%20greatly%20reduce%20annotation%20effort%20but%20typically%20lack%20precise%20spatial%20localization.%20To%20address%20this%20dilemma%2C%20we%20propose%20BoxPromptIML%2C%20a%20novel%20weakly-supervised%20IML%20framework%20that%20effectively%20balances%20annotation%20cost%20and%20localization%20performance.%20Specifically%2C%20we%20propose%20a%20coarse%20region%20annotation%20strategy%2C%20which%20can%20generate%20relatively%20accurate%20manipulation%20masks%20at%20lower%20cost.%20To%20improve%20model%20efficiency%20and%20facilitate%20deployment%2C%20we%20further%20design%20an%20efficient%20lightweight%20student%20model%2C%20which%20learns%20to%20perform%20fine-grained%20localization%20through%20knowledge%20distillation%20from%20a%20fixed%20teacher%20model%20based%20on%20the%20Segment%20Anything%20Model%20%28SAM%29.%20Moreover%2C%20inspired%20by%20the%20human%20subconscious%20memory%20mechanism%2C%20our%20feature%20fusion%20module%20employs%20a%20dual-guidance%20strategy%20that%20actively%20contextualizes%20recalled%20prototypical%20patterns%20with%20real-time%20observational%20cues%20derived%20from%20the%20input.%20Instead%20of%20passive%20feature%20extraction%2C%20this%20strategy%20enables%20a%20dynamic%20process%20of%20knowledge%20recollection%2C%20where%20long-term%20memory%20is%20adapted%20to%20the%20specific%20context%20of%20the%20current%20image%2C%20significantly%20enhancing%20localization%20accuracy%20and%20robustness.%20Extensive%20experiments%20across%20both%20in-distribution%20and%20out-of-distribution%20datasets%20show%20that%20BoxPromptIML%20outperforms%20or%20rivals%20fully-supervised%20models%2C%20while%20maintaining%20strong%20generalization%2C%20low%20annotation%20cost%2C%20and%20efficient%20deployment%20characteristics.&entry.1838667208=http%3A//arxiv.org/abs/2511.20359v1&entry.124074799=Read"},
{"title": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models", "author": "Nathan Roll and Jill Kries and Flora Jin and Catherine Wang and Ann Marie Finley and Meghan Sumner and Cory Shain and Laura Gwilliams", "abstract": "Large language models (LLMs) have emerged as a candidate \"model organism\" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.", "link": "http://arxiv.org/abs/2511.20507v1", "date": "2025-11-25", "relevancy": 2.3425, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Text%20Aphasia%20Battery%20%28TAB%29%3A%20A%20Clinically-Grounded%20Benchmark%20for%20Aphasia-Like%20Deficits%20in%20Language%20Models&body=Title%3A%20The%20Text%20Aphasia%20Battery%20%28TAB%29%3A%20A%20Clinically-Grounded%20Benchmark%20for%20Aphasia-Like%20Deficits%20in%20Language%20Models%0AAuthor%3A%20Nathan%20Roll%20and%20Jill%20Kries%20and%20Flora%20Jin%20and%20Catherine%20Wang%20and%20Ann%20Marie%20Finley%20and%20Meghan%20Sumner%20and%20Cory%20Shain%20and%20Laura%20Gwilliams%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20emerged%20as%20a%20candidate%20%22model%20organism%22%20for%20human%20language%2C%20offering%20an%20unprecedented%20opportunity%20to%20study%20the%20computational%20basis%20of%20linguistic%20disorders%20like%20aphasia.%20However%2C%20traditional%20clinical%20assessments%20are%20ill-suited%20for%20LLMs%2C%20as%20they%20presuppose%20human-like%20pragmatic%20pressures%20and%20probe%20cognitive%20processes%20not%20inherent%20to%20artificial%20architectures.%20We%20introduce%20the%20Text%20Aphasia%20Battery%20%28TAB%29%2C%20a%20text-only%20benchmark%20adapted%20from%20the%20Quick%20Aphasia%20Battery%20%28QAB%29%20to%20assess%20aphasic-like%20deficits%20in%20LLMs.%20The%20TAB%20comprises%20four%20subtests%3A%20Connected%20Text%2C%20Word%20Comprehension%2C%20Sentence%20Comprehension%2C%20and%20Repetition.%20This%20paper%20details%20the%20TAB%27s%20design%2C%20subtests%2C%20and%20scoring%20criteria.%20To%20facilitate%20large-scale%20use%2C%20we%20validate%20an%20automated%20evaluation%20protocol%20using%20Gemini%202.5%20Flash%2C%20which%20achieves%20reliability%20comparable%20to%20expert%20human%20raters%20%28prevalence-weighted%20Cohen%27s%20kappa%20%3D%200.255%20for%20model--consensus%20agreement%20vs.%200.286%20for%20human--human%20agreement%29.%20We%20release%20TAB%20as%20a%20clinically-grounded%2C%20scalable%20framework%20for%20analyzing%20language%20deficits%20in%20artificial%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Text%2520Aphasia%2520Battery%2520%2528TAB%2529%253A%2520A%2520Clinically-Grounded%2520Benchmark%2520for%2520Aphasia-Like%2520Deficits%2520in%2520Language%2520Models%26entry.906535625%3DNathan%2520Roll%2520and%2520Jill%2520Kries%2520and%2520Flora%2520Jin%2520and%2520Catherine%2520Wang%2520and%2520Ann%2520Marie%2520Finley%2520and%2520Meghan%2520Sumner%2520and%2520Cory%2520Shain%2520and%2520Laura%2520Gwilliams%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520a%2520candidate%2520%2522model%2520organism%2522%2520for%2520human%2520language%252C%2520offering%2520an%2520unprecedented%2520opportunity%2520to%2520study%2520the%2520computational%2520basis%2520of%2520linguistic%2520disorders%2520like%2520aphasia.%2520However%252C%2520traditional%2520clinical%2520assessments%2520are%2520ill-suited%2520for%2520LLMs%252C%2520as%2520they%2520presuppose%2520human-like%2520pragmatic%2520pressures%2520and%2520probe%2520cognitive%2520processes%2520not%2520inherent%2520to%2520artificial%2520architectures.%2520We%2520introduce%2520the%2520Text%2520Aphasia%2520Battery%2520%2528TAB%2529%252C%2520a%2520text-only%2520benchmark%2520adapted%2520from%2520the%2520Quick%2520Aphasia%2520Battery%2520%2528QAB%2529%2520to%2520assess%2520aphasic-like%2520deficits%2520in%2520LLMs.%2520The%2520TAB%2520comprises%2520four%2520subtests%253A%2520Connected%2520Text%252C%2520Word%2520Comprehension%252C%2520Sentence%2520Comprehension%252C%2520and%2520Repetition.%2520This%2520paper%2520details%2520the%2520TAB%2527s%2520design%252C%2520subtests%252C%2520and%2520scoring%2520criteria.%2520To%2520facilitate%2520large-scale%2520use%252C%2520we%2520validate%2520an%2520automated%2520evaluation%2520protocol%2520using%2520Gemini%25202.5%2520Flash%252C%2520which%2520achieves%2520reliability%2520comparable%2520to%2520expert%2520human%2520raters%2520%2528prevalence-weighted%2520Cohen%2527s%2520kappa%2520%253D%25200.255%2520for%2520model--consensus%2520agreement%2520vs.%25200.286%2520for%2520human--human%2520agreement%2529.%2520We%2520release%2520TAB%2520as%2520a%2520clinically-grounded%252C%2520scalable%2520framework%2520for%2520analyzing%2520language%2520deficits%2520in%2520artificial%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Text%20Aphasia%20Battery%20%28TAB%29%3A%20A%20Clinically-Grounded%20Benchmark%20for%20Aphasia-Like%20Deficits%20in%20Language%20Models&entry.906535625=Nathan%20Roll%20and%20Jill%20Kries%20and%20Flora%20Jin%20and%20Catherine%20Wang%20and%20Ann%20Marie%20Finley%20and%20Meghan%20Sumner%20and%20Cory%20Shain%20and%20Laura%20Gwilliams&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20emerged%20as%20a%20candidate%20%22model%20organism%22%20for%20human%20language%2C%20offering%20an%20unprecedented%20opportunity%20to%20study%20the%20computational%20basis%20of%20linguistic%20disorders%20like%20aphasia.%20However%2C%20traditional%20clinical%20assessments%20are%20ill-suited%20for%20LLMs%2C%20as%20they%20presuppose%20human-like%20pragmatic%20pressures%20and%20probe%20cognitive%20processes%20not%20inherent%20to%20artificial%20architectures.%20We%20introduce%20the%20Text%20Aphasia%20Battery%20%28TAB%29%2C%20a%20text-only%20benchmark%20adapted%20from%20the%20Quick%20Aphasia%20Battery%20%28QAB%29%20to%20assess%20aphasic-like%20deficits%20in%20LLMs.%20The%20TAB%20comprises%20four%20subtests%3A%20Connected%20Text%2C%20Word%20Comprehension%2C%20Sentence%20Comprehension%2C%20and%20Repetition.%20This%20paper%20details%20the%20TAB%27s%20design%2C%20subtests%2C%20and%20scoring%20criteria.%20To%20facilitate%20large-scale%20use%2C%20we%20validate%20an%20automated%20evaluation%20protocol%20using%20Gemini%202.5%20Flash%2C%20which%20achieves%20reliability%20comparable%20to%20expert%20human%20raters%20%28prevalence-weighted%20Cohen%27s%20kappa%20%3D%200.255%20for%20model--consensus%20agreement%20vs.%200.286%20for%20human--human%20agreement%29.%20We%20release%20TAB%20as%20a%20clinically-grounded%2C%20scalable%20framework%20for%20analyzing%20language%20deficits%20in%20artificial%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.20507v1&entry.124074799=Read"},
{"title": "Graph Kernel Neural Networks", "author": "Luca Cosmo and Giorgia Minello and Alessandro Bicciato and Michael Bronstein and Emanuele Rodol\u00e0 and Luca Rossi and Andrea Torsello", "abstract": "The convolution operator at the core of many modern neural architectures can effectively be seen as performing a dot product between an input matrix and a filter. While this is readily applicable to data such as images, which can be represented as regular grids in the Euclidean space, extending the convolution operator to work on graphs proves more challenging, due to their irregular structure. In this paper, we propose to use graph kernels, i.e. kernel functions that compute an inner product on graphs, to extend the standard convolution operator to the graph domain. This allows us to define an entirely structural model that does not require computing the embedding of the input graph. Our architecture allows to plug-in any type of graph kernels and has the added benefit of providing some interpretability in terms of the structural masks that are learned during the training process, similarly to what happens for convolutional masks in traditional convolutional neural networks. We perform an extensive ablation study to investigate the model hyper-parameters' impact and show that our model achieves competitive performance on standard graph classification and regression datasets.", "link": "http://arxiv.org/abs/2112.07436v3", "date": "2025-11-25", "relevancy": 2.3261, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4919}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4737}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Kernel%20Neural%20Networks&body=Title%3A%20Graph%20Kernel%20Neural%20Networks%0AAuthor%3A%20Luca%20Cosmo%20and%20Giorgia%20Minello%20and%20Alessandro%20Bicciato%20and%20Michael%20Bronstein%20and%20Emanuele%20Rodol%C3%A0%20and%20Luca%20Rossi%20and%20Andrea%20Torsello%0AAbstract%3A%20The%20convolution%20operator%20at%20the%20core%20of%20many%20modern%20neural%20architectures%20can%20effectively%20be%20seen%20as%20performing%20a%20dot%20product%20between%20an%20input%20matrix%20and%20a%20filter.%20While%20this%20is%20readily%20applicable%20to%20data%20such%20as%20images%2C%20which%20can%20be%20represented%20as%20regular%20grids%20in%20the%20Euclidean%20space%2C%20extending%20the%20convolution%20operator%20to%20work%20on%20graphs%20proves%20more%20challenging%2C%20due%20to%20their%20irregular%20structure.%20In%20this%20paper%2C%20we%20propose%20to%20use%20graph%20kernels%2C%20i.e.%20kernel%20functions%20that%20compute%20an%20inner%20product%20on%20graphs%2C%20to%20extend%20the%20standard%20convolution%20operator%20to%20the%20graph%20domain.%20This%20allows%20us%20to%20define%20an%20entirely%20structural%20model%20that%20does%20not%20require%20computing%20the%20embedding%20of%20the%20input%20graph.%20Our%20architecture%20allows%20to%20plug-in%20any%20type%20of%20graph%20kernels%20and%20has%20the%20added%20benefit%20of%20providing%20some%20interpretability%20in%20terms%20of%20the%20structural%20masks%20that%20are%20learned%20during%20the%20training%20process%2C%20similarly%20to%20what%20happens%20for%20convolutional%20masks%20in%20traditional%20convolutional%20neural%20networks.%20We%20perform%20an%20extensive%20ablation%20study%20to%20investigate%20the%20model%20hyper-parameters%27%20impact%20and%20show%20that%20our%20model%20achieves%20competitive%20performance%20on%20standard%20graph%20classification%20and%20regression%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2112.07436v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Kernel%2520Neural%2520Networks%26entry.906535625%3DLuca%2520Cosmo%2520and%2520Giorgia%2520Minello%2520and%2520Alessandro%2520Bicciato%2520and%2520Michael%2520Bronstein%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Luca%2520Rossi%2520and%2520Andrea%2520Torsello%26entry.1292438233%3DThe%2520convolution%2520operator%2520at%2520the%2520core%2520of%2520many%2520modern%2520neural%2520architectures%2520can%2520effectively%2520be%2520seen%2520as%2520performing%2520a%2520dot%2520product%2520between%2520an%2520input%2520matrix%2520and%2520a%2520filter.%2520While%2520this%2520is%2520readily%2520applicable%2520to%2520data%2520such%2520as%2520images%252C%2520which%2520can%2520be%2520represented%2520as%2520regular%2520grids%2520in%2520the%2520Euclidean%2520space%252C%2520extending%2520the%2520convolution%2520operator%2520to%2520work%2520on%2520graphs%2520proves%2520more%2520challenging%252C%2520due%2520to%2520their%2520irregular%2520structure.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520use%2520graph%2520kernels%252C%2520i.e.%2520kernel%2520functions%2520that%2520compute%2520an%2520inner%2520product%2520on%2520graphs%252C%2520to%2520extend%2520the%2520standard%2520convolution%2520operator%2520to%2520the%2520graph%2520domain.%2520This%2520allows%2520us%2520to%2520define%2520an%2520entirely%2520structural%2520model%2520that%2520does%2520not%2520require%2520computing%2520the%2520embedding%2520of%2520the%2520input%2520graph.%2520Our%2520architecture%2520allows%2520to%2520plug-in%2520any%2520type%2520of%2520graph%2520kernels%2520and%2520has%2520the%2520added%2520benefit%2520of%2520providing%2520some%2520interpretability%2520in%2520terms%2520of%2520the%2520structural%2520masks%2520that%2520are%2520learned%2520during%2520the%2520training%2520process%252C%2520similarly%2520to%2520what%2520happens%2520for%2520convolutional%2520masks%2520in%2520traditional%2520convolutional%2520neural%2520networks.%2520We%2520perform%2520an%2520extensive%2520ablation%2520study%2520to%2520investigate%2520the%2520model%2520hyper-parameters%2527%2520impact%2520and%2520show%2520that%2520our%2520model%2520achieves%2520competitive%2520performance%2520on%2520standard%2520graph%2520classification%2520and%2520regression%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2112.07436v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Kernel%20Neural%20Networks&entry.906535625=Luca%20Cosmo%20and%20Giorgia%20Minello%20and%20Alessandro%20Bicciato%20and%20Michael%20Bronstein%20and%20Emanuele%20Rodol%C3%A0%20and%20Luca%20Rossi%20and%20Andrea%20Torsello&entry.1292438233=The%20convolution%20operator%20at%20the%20core%20of%20many%20modern%20neural%20architectures%20can%20effectively%20be%20seen%20as%20performing%20a%20dot%20product%20between%20an%20input%20matrix%20and%20a%20filter.%20While%20this%20is%20readily%20applicable%20to%20data%20such%20as%20images%2C%20which%20can%20be%20represented%20as%20regular%20grids%20in%20the%20Euclidean%20space%2C%20extending%20the%20convolution%20operator%20to%20work%20on%20graphs%20proves%20more%20challenging%2C%20due%20to%20their%20irregular%20structure.%20In%20this%20paper%2C%20we%20propose%20to%20use%20graph%20kernels%2C%20i.e.%20kernel%20functions%20that%20compute%20an%20inner%20product%20on%20graphs%2C%20to%20extend%20the%20standard%20convolution%20operator%20to%20the%20graph%20domain.%20This%20allows%20us%20to%20define%20an%20entirely%20structural%20model%20that%20does%20not%20require%20computing%20the%20embedding%20of%20the%20input%20graph.%20Our%20architecture%20allows%20to%20plug-in%20any%20type%20of%20graph%20kernels%20and%20has%20the%20added%20benefit%20of%20providing%20some%20interpretability%20in%20terms%20of%20the%20structural%20masks%20that%20are%20learned%20during%20the%20training%20process%2C%20similarly%20to%20what%20happens%20for%20convolutional%20masks%20in%20traditional%20convolutional%20neural%20networks.%20We%20perform%20an%20extensive%20ablation%20study%20to%20investigate%20the%20model%20hyper-parameters%27%20impact%20and%20show%20that%20our%20model%20achieves%20competitive%20performance%20on%20standard%20graph%20classification%20and%20regression%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2112.07436v3&entry.124074799=Read"},
{"title": "DUO-TOK: Dual-Track Semantic Music Tokenizer for Vocal-Accompaniment Generation", "author": "Rui Lin and Zhiyue Wu and Jiahe Le and Kangdi Wang and Weixiong Chen and Junyu Dai and Tao Jiang", "abstract": "Duo-Tok is a source-aware dual-codebook tokenizer for vocal-accompaniment music that targets the growing tension between reconstruction quality and language-model (LM) learnability in modern lyrics-to-song systems. Existing codecs either prioritize high-fidelity reconstruction with difficult-to-model acoustic tokens or compress aggressively into semantic tokens that are LM-friendly but lossy, and they rarely make the tokenizer itself aware of dual-track structure. Duo-Tok follows a four-stage, SSL-centered pipeline: we first pretrain a BEST-RQ-style encoder on large-scale audio, then stabilize and factorize the representation with Gaussian replacement noise and multi-task supervision, before freezing the encoder to learn SimVQ-based dual codebooks with hard routing for vocals and accompaniment, and finally training latent diffusion decoders on top of the discrete tokens. Duo-Tok at 0.75 kbps shifts the empirical reconstruction-generation Pareto frontier, achieving the best music-tagging AP and the lowest vocabulary-normalized LM perplexity among compared codecs while maintaining reconstruction quality comparable to state-of-the-art music tokenizers.", "link": "http://arxiv.org/abs/2511.20224v1", "date": "2025-11-25", "relevancy": 2.3259, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUO-TOK%3A%20Dual-Track%20Semantic%20Music%20Tokenizer%20for%20Vocal-Accompaniment%20Generation&body=Title%3A%20DUO-TOK%3A%20Dual-Track%20Semantic%20Music%20Tokenizer%20for%20Vocal-Accompaniment%20Generation%0AAuthor%3A%20Rui%20Lin%20and%20Zhiyue%20Wu%20and%20Jiahe%20Le%20and%20Kangdi%20Wang%20and%20Weixiong%20Chen%20and%20Junyu%20Dai%20and%20Tao%20Jiang%0AAbstract%3A%20Duo-Tok%20is%20a%20source-aware%20dual-codebook%20tokenizer%20for%20vocal-accompaniment%20music%20that%20targets%20the%20growing%20tension%20between%20reconstruction%20quality%20and%20language-model%20%28LM%29%20learnability%20in%20modern%20lyrics-to-song%20systems.%20Existing%20codecs%20either%20prioritize%20high-fidelity%20reconstruction%20with%20difficult-to-model%20acoustic%20tokens%20or%20compress%20aggressively%20into%20semantic%20tokens%20that%20are%20LM-friendly%20but%20lossy%2C%20and%20they%20rarely%20make%20the%20tokenizer%20itself%20aware%20of%20dual-track%20structure.%20Duo-Tok%20follows%20a%20four-stage%2C%20SSL-centered%20pipeline%3A%20we%20first%20pretrain%20a%20BEST-RQ-style%20encoder%20on%20large-scale%20audio%2C%20then%20stabilize%20and%20factorize%20the%20representation%20with%20Gaussian%20replacement%20noise%20and%20multi-task%20supervision%2C%20before%20freezing%20the%20encoder%20to%20learn%20SimVQ-based%20dual%20codebooks%20with%20hard%20routing%20for%20vocals%20and%20accompaniment%2C%20and%20finally%20training%20latent%20diffusion%20decoders%20on%20top%20of%20the%20discrete%20tokens.%20Duo-Tok%20at%200.75%20kbps%20shifts%20the%20empirical%20reconstruction-generation%20Pareto%20frontier%2C%20achieving%20the%20best%20music-tagging%20AP%20and%20the%20lowest%20vocabulary-normalized%20LM%20perplexity%20among%20compared%20codecs%20while%20maintaining%20reconstruction%20quality%20comparable%20to%20state-of-the-art%20music%20tokenizers.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUO-TOK%253A%2520Dual-Track%2520Semantic%2520Music%2520Tokenizer%2520for%2520Vocal-Accompaniment%2520Generation%26entry.906535625%3DRui%2520Lin%2520and%2520Zhiyue%2520Wu%2520and%2520Jiahe%2520Le%2520and%2520Kangdi%2520Wang%2520and%2520Weixiong%2520Chen%2520and%2520Junyu%2520Dai%2520and%2520Tao%2520Jiang%26entry.1292438233%3DDuo-Tok%2520is%2520a%2520source-aware%2520dual-codebook%2520tokenizer%2520for%2520vocal-accompaniment%2520music%2520that%2520targets%2520the%2520growing%2520tension%2520between%2520reconstruction%2520quality%2520and%2520language-model%2520%2528LM%2529%2520learnability%2520in%2520modern%2520lyrics-to-song%2520systems.%2520Existing%2520codecs%2520either%2520prioritize%2520high-fidelity%2520reconstruction%2520with%2520difficult-to-model%2520acoustic%2520tokens%2520or%2520compress%2520aggressively%2520into%2520semantic%2520tokens%2520that%2520are%2520LM-friendly%2520but%2520lossy%252C%2520and%2520they%2520rarely%2520make%2520the%2520tokenizer%2520itself%2520aware%2520of%2520dual-track%2520structure.%2520Duo-Tok%2520follows%2520a%2520four-stage%252C%2520SSL-centered%2520pipeline%253A%2520we%2520first%2520pretrain%2520a%2520BEST-RQ-style%2520encoder%2520on%2520large-scale%2520audio%252C%2520then%2520stabilize%2520and%2520factorize%2520the%2520representation%2520with%2520Gaussian%2520replacement%2520noise%2520and%2520multi-task%2520supervision%252C%2520before%2520freezing%2520the%2520encoder%2520to%2520learn%2520SimVQ-based%2520dual%2520codebooks%2520with%2520hard%2520routing%2520for%2520vocals%2520and%2520accompaniment%252C%2520and%2520finally%2520training%2520latent%2520diffusion%2520decoders%2520on%2520top%2520of%2520the%2520discrete%2520tokens.%2520Duo-Tok%2520at%25200.75%2520kbps%2520shifts%2520the%2520empirical%2520reconstruction-generation%2520Pareto%2520frontier%252C%2520achieving%2520the%2520best%2520music-tagging%2520AP%2520and%2520the%2520lowest%2520vocabulary-normalized%2520LM%2520perplexity%2520among%2520compared%2520codecs%2520while%2520maintaining%2520reconstruction%2520quality%2520comparable%2520to%2520state-of-the-art%2520music%2520tokenizers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUO-TOK%3A%20Dual-Track%20Semantic%20Music%20Tokenizer%20for%20Vocal-Accompaniment%20Generation&entry.906535625=Rui%20Lin%20and%20Zhiyue%20Wu%20and%20Jiahe%20Le%20and%20Kangdi%20Wang%20and%20Weixiong%20Chen%20and%20Junyu%20Dai%20and%20Tao%20Jiang&entry.1292438233=Duo-Tok%20is%20a%20source-aware%20dual-codebook%20tokenizer%20for%20vocal-accompaniment%20music%20that%20targets%20the%20growing%20tension%20between%20reconstruction%20quality%20and%20language-model%20%28LM%29%20learnability%20in%20modern%20lyrics-to-song%20systems.%20Existing%20codecs%20either%20prioritize%20high-fidelity%20reconstruction%20with%20difficult-to-model%20acoustic%20tokens%20or%20compress%20aggressively%20into%20semantic%20tokens%20that%20are%20LM-friendly%20but%20lossy%2C%20and%20they%20rarely%20make%20the%20tokenizer%20itself%20aware%20of%20dual-track%20structure.%20Duo-Tok%20follows%20a%20four-stage%2C%20SSL-centered%20pipeline%3A%20we%20first%20pretrain%20a%20BEST-RQ-style%20encoder%20on%20large-scale%20audio%2C%20then%20stabilize%20and%20factorize%20the%20representation%20with%20Gaussian%20replacement%20noise%20and%20multi-task%20supervision%2C%20before%20freezing%20the%20encoder%20to%20learn%20SimVQ-based%20dual%20codebooks%20with%20hard%20routing%20for%20vocals%20and%20accompaniment%2C%20and%20finally%20training%20latent%20diffusion%20decoders%20on%20top%20of%20the%20discrete%20tokens.%20Duo-Tok%20at%200.75%20kbps%20shifts%20the%20empirical%20reconstruction-generation%20Pareto%20frontier%2C%20achieving%20the%20best%20music-tagging%20AP%20and%20the%20lowest%20vocabulary-normalized%20LM%20perplexity%20among%20compared%20codecs%20while%20maintaining%20reconstruction%20quality%20comparable%20to%20state-of-the-art%20music%20tokenizers.&entry.1838667208=http%3A//arxiv.org/abs/2511.20224v1&entry.124074799=Read"},
{"title": "MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts", "author": "Zilong Huang and Jun He and Xiaobin Huang and Ziyi Xiong and Yang Luo and Junyan Ye and Weijia Li and Yiping Chen and Ting Han", "abstract": "Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.", "link": "http://arxiv.org/abs/2511.20415v1", "date": "2025-11-25", "relevancy": 2.3235, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5915}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5787}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MajutsuCity%3A%20Language-driven%20Aesthetic-adaptive%20City%20Generation%20with%20Controllable%203D%20Assets%20and%20Layouts&body=Title%3A%20MajutsuCity%3A%20Language-driven%20Aesthetic-adaptive%20City%20Generation%20with%20Controllable%203D%20Assets%20and%20Layouts%0AAuthor%3A%20Zilong%20Huang%20and%20Jun%20He%20and%20Xiaobin%20Huang%20and%20Ziyi%20Xiong%20and%20Yang%20Luo%20and%20Junyan%20Ye%20and%20Weijia%20Li%20and%20Yiping%20Chen%20and%20Ting%20Han%0AAbstract%3A%20Generating%20realistic%203D%20cities%20is%20fundamental%20to%20world%20models%2C%20virtual%20reality%2C%20and%20game%20development%2C%20where%20an%20ideal%20urban%20scene%20must%20satisfy%20both%20stylistic%20diversity%2C%20fine-grained%2C%20and%20controllability.%20However%2C%20existing%20methods%20struggle%20to%20balance%20the%20creative%20flexibility%20offered%20by%20text-based%20generation%20with%20the%20object-level%20editability%20enabled%20by%20explicit%20structural%20representations.%20We%20introduce%20MajutsuCity%2C%20a%20natural%20language-driven%20and%20aesthetically%20adaptive%20framework%20for%20synthesizing%20structurally%20consistent%20and%20stylistically%20diverse%203D%20urban%20scenes.%20MajutsuCity%20represents%20a%20city%20as%20a%20composition%20of%20controllable%20layouts%2C%20assets%2C%20and%20materials%2C%20and%20operates%20through%20a%20four-stage%20pipeline.%20To%20extend%20controllability%20beyond%20initial%20generation%2C%20we%20further%20integrate%20MajutsuAgent%2C%20an%20interactive%20language-grounded%20editing%20agent%7D%20that%20supports%20five%20object-level%20operations.%20To%20support%20photorealistic%20and%20customizable%20scene%20synthesis%2C%20we%20also%20construct%20MajutsuDataset%2C%20a%20high-quality%20multimodal%20dataset%7D%20containing%202D%20semantic%20layouts%20and%20height%20maps%2C%20diverse%203D%20building%20assets%2C%20and%20curated%20PBR%20materials%20and%20skyboxes%2C%20each%20accompanied%20by%20detailed%20annotations.%20Meanwhile%2C%20we%20develop%20a%20practical%20set%20of%20evaluation%20metrics%2C%20covering%20key%20dimensions%20such%20as%20structural%20consistency%2C%20scene%20complexity%2C%20material%20fidelity%2C%20and%20lighting%20atmosphere.%20Extensive%20experiments%20demonstrate%20MajutsuCity%20reduces%20layout%20FID%20by%2083.7%25%20compared%20with%20CityDreamer%20and%20by%2020.1%25%20over%20CityCraft.%20Our%20method%20ranks%20first%20across%20all%20AQS%20and%20RDR%20scores%2C%20outperforming%20existing%20methods%20by%20a%20clear%20margin.%20These%20results%20confirm%20MajutsuCity%20as%20a%20new%20state-of-the-art%20in%20geometric%20fidelity%2C%20stylistic%20adaptability%2C%20and%20semantic%20controllability%20for%203D%20city%20generation.%20We%20expect%20our%20framework%20can%20inspire%20new%20avenues%20of%20research%20in%203D%20city%20generation.%20Our%20dataset%20and%20code%20will%20be%20released%20at%20https%3A//github.com/LongHZ140516/MajutsuCity.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMajutsuCity%253A%2520Language-driven%2520Aesthetic-adaptive%2520City%2520Generation%2520with%2520Controllable%25203D%2520Assets%2520and%2520Layouts%26entry.906535625%3DZilong%2520Huang%2520and%2520Jun%2520He%2520and%2520Xiaobin%2520Huang%2520and%2520Ziyi%2520Xiong%2520and%2520Yang%2520Luo%2520and%2520Junyan%2520Ye%2520and%2520Weijia%2520Li%2520and%2520Yiping%2520Chen%2520and%2520Ting%2520Han%26entry.1292438233%3DGenerating%2520realistic%25203D%2520cities%2520is%2520fundamental%2520to%2520world%2520models%252C%2520virtual%2520reality%252C%2520and%2520game%2520development%252C%2520where%2520an%2520ideal%2520urban%2520scene%2520must%2520satisfy%2520both%2520stylistic%2520diversity%252C%2520fine-grained%252C%2520and%2520controllability.%2520However%252C%2520existing%2520methods%2520struggle%2520to%2520balance%2520the%2520creative%2520flexibility%2520offered%2520by%2520text-based%2520generation%2520with%2520the%2520object-level%2520editability%2520enabled%2520by%2520explicit%2520structural%2520representations.%2520We%2520introduce%2520MajutsuCity%252C%2520a%2520natural%2520language-driven%2520and%2520aesthetically%2520adaptive%2520framework%2520for%2520synthesizing%2520structurally%2520consistent%2520and%2520stylistically%2520diverse%25203D%2520urban%2520scenes.%2520MajutsuCity%2520represents%2520a%2520city%2520as%2520a%2520composition%2520of%2520controllable%2520layouts%252C%2520assets%252C%2520and%2520materials%252C%2520and%2520operates%2520through%2520a%2520four-stage%2520pipeline.%2520To%2520extend%2520controllability%2520beyond%2520initial%2520generation%252C%2520we%2520further%2520integrate%2520MajutsuAgent%252C%2520an%2520interactive%2520language-grounded%2520editing%2520agent%257D%2520that%2520supports%2520five%2520object-level%2520operations.%2520To%2520support%2520photorealistic%2520and%2520customizable%2520scene%2520synthesis%252C%2520we%2520also%2520construct%2520MajutsuDataset%252C%2520a%2520high-quality%2520multimodal%2520dataset%257D%2520containing%25202D%2520semantic%2520layouts%2520and%2520height%2520maps%252C%2520diverse%25203D%2520building%2520assets%252C%2520and%2520curated%2520PBR%2520materials%2520and%2520skyboxes%252C%2520each%2520accompanied%2520by%2520detailed%2520annotations.%2520Meanwhile%252C%2520we%2520develop%2520a%2520practical%2520set%2520of%2520evaluation%2520metrics%252C%2520covering%2520key%2520dimensions%2520such%2520as%2520structural%2520consistency%252C%2520scene%2520complexity%252C%2520material%2520fidelity%252C%2520and%2520lighting%2520atmosphere.%2520Extensive%2520experiments%2520demonstrate%2520MajutsuCity%2520reduces%2520layout%2520FID%2520by%252083.7%2525%2520compared%2520with%2520CityDreamer%2520and%2520by%252020.1%2525%2520over%2520CityCraft.%2520Our%2520method%2520ranks%2520first%2520across%2520all%2520AQS%2520and%2520RDR%2520scores%252C%2520outperforming%2520existing%2520methods%2520by%2520a%2520clear%2520margin.%2520These%2520results%2520confirm%2520MajutsuCity%2520as%2520a%2520new%2520state-of-the-art%2520in%2520geometric%2520fidelity%252C%2520stylistic%2520adaptability%252C%2520and%2520semantic%2520controllability%2520for%25203D%2520city%2520generation.%2520We%2520expect%2520our%2520framework%2520can%2520inspire%2520new%2520avenues%2520of%2520research%2520in%25203D%2520city%2520generation.%2520Our%2520dataset%2520and%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/LongHZ140516/MajutsuCity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MajutsuCity%3A%20Language-driven%20Aesthetic-adaptive%20City%20Generation%20with%20Controllable%203D%20Assets%20and%20Layouts&entry.906535625=Zilong%20Huang%20and%20Jun%20He%20and%20Xiaobin%20Huang%20and%20Ziyi%20Xiong%20and%20Yang%20Luo%20and%20Junyan%20Ye%20and%20Weijia%20Li%20and%20Yiping%20Chen%20and%20Ting%20Han&entry.1292438233=Generating%20realistic%203D%20cities%20is%20fundamental%20to%20world%20models%2C%20virtual%20reality%2C%20and%20game%20development%2C%20where%20an%20ideal%20urban%20scene%20must%20satisfy%20both%20stylistic%20diversity%2C%20fine-grained%2C%20and%20controllability.%20However%2C%20existing%20methods%20struggle%20to%20balance%20the%20creative%20flexibility%20offered%20by%20text-based%20generation%20with%20the%20object-level%20editability%20enabled%20by%20explicit%20structural%20representations.%20We%20introduce%20MajutsuCity%2C%20a%20natural%20language-driven%20and%20aesthetically%20adaptive%20framework%20for%20synthesizing%20structurally%20consistent%20and%20stylistically%20diverse%203D%20urban%20scenes.%20MajutsuCity%20represents%20a%20city%20as%20a%20composition%20of%20controllable%20layouts%2C%20assets%2C%20and%20materials%2C%20and%20operates%20through%20a%20four-stage%20pipeline.%20To%20extend%20controllability%20beyond%20initial%20generation%2C%20we%20further%20integrate%20MajutsuAgent%2C%20an%20interactive%20language-grounded%20editing%20agent%7D%20that%20supports%20five%20object-level%20operations.%20To%20support%20photorealistic%20and%20customizable%20scene%20synthesis%2C%20we%20also%20construct%20MajutsuDataset%2C%20a%20high-quality%20multimodal%20dataset%7D%20containing%202D%20semantic%20layouts%20and%20height%20maps%2C%20diverse%203D%20building%20assets%2C%20and%20curated%20PBR%20materials%20and%20skyboxes%2C%20each%20accompanied%20by%20detailed%20annotations.%20Meanwhile%2C%20we%20develop%20a%20practical%20set%20of%20evaluation%20metrics%2C%20covering%20key%20dimensions%20such%20as%20structural%20consistency%2C%20scene%20complexity%2C%20material%20fidelity%2C%20and%20lighting%20atmosphere.%20Extensive%20experiments%20demonstrate%20MajutsuCity%20reduces%20layout%20FID%20by%2083.7%25%20compared%20with%20CityDreamer%20and%20by%2020.1%25%20over%20CityCraft.%20Our%20method%20ranks%20first%20across%20all%20AQS%20and%20RDR%20scores%2C%20outperforming%20existing%20methods%20by%20a%20clear%20margin.%20These%20results%20confirm%20MajutsuCity%20as%20a%20new%20state-of-the-art%20in%20geometric%20fidelity%2C%20stylistic%20adaptability%2C%20and%20semantic%20controllability%20for%203D%20city%20generation.%20We%20expect%20our%20framework%20can%20inspire%20new%20avenues%20of%20research%20in%203D%20city%20generation.%20Our%20dataset%20and%20code%20will%20be%20released%20at%20https%3A//github.com/LongHZ140516/MajutsuCity.&entry.1838667208=http%3A//arxiv.org/abs/2511.20415v1&entry.124074799=Read"},
{"title": "M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo Video Conversion", "author": "Nina Shvetsova and Goutam Bhat and Prune Truong and Hilde Kuehne and Federico Tombari", "abstract": "We tackle the problem of monocular-to-stereo video conversion and propose a novel architecture for inpainting and refinement of the warped right view obtained by depth-based reprojection of the input left view. We extend the Stable Video Diffusion (SVD) model to utilize the input left video, the warped right video, and the disocclusion masks as conditioning input to generate a high-quality right camera view. In order to effectively exploit information from neighboring frames for inpainting, we modify the attention layers in SVD to compute full attention for discoccluded pixels. Our model is trained to generate the right view video in an end-to-end manner without iterative diffusion steps by minimizing image space losses to ensure high-quality generation. Our approach outperforms previous state-of-the-art methods, being ranked best 2.6x more often than the second-place method in a user study, while being 6x faster.", "link": "http://arxiv.org/abs/2505.16565v2", "date": "2025-11-25", "relevancy": 2.3213, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5983}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5851}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M2SVid%3A%20End-to-End%20Inpainting%20and%20Refinement%20for%20Monocular-to-Stereo%20Video%20Conversion&body=Title%3A%20M2SVid%3A%20End-to-End%20Inpainting%20and%20Refinement%20for%20Monocular-to-Stereo%20Video%20Conversion%0AAuthor%3A%20Nina%20Shvetsova%20and%20Goutam%20Bhat%20and%20Prune%20Truong%20and%20Hilde%20Kuehne%20and%20Federico%20Tombari%0AAbstract%3A%20We%20tackle%20the%20problem%20of%20monocular-to-stereo%20video%20conversion%20and%20propose%20a%20novel%20architecture%20for%20inpainting%20and%20refinement%20of%20the%20warped%20right%20view%20obtained%20by%20depth-based%20reprojection%20of%20the%20input%20left%20view.%20We%20extend%20the%20Stable%20Video%20Diffusion%20%28SVD%29%20model%20to%20utilize%20the%20input%20left%20video%2C%20the%20warped%20right%20video%2C%20and%20the%20disocclusion%20masks%20as%20conditioning%20input%20to%20generate%20a%20high-quality%20right%20camera%20view.%20In%20order%20to%20effectively%20exploit%20information%20from%20neighboring%20frames%20for%20inpainting%2C%20we%20modify%20the%20attention%20layers%20in%20SVD%20to%20compute%20full%20attention%20for%20discoccluded%20pixels.%20Our%20model%20is%20trained%20to%20generate%20the%20right%20view%20video%20in%20an%20end-to-end%20manner%20without%20iterative%20diffusion%20steps%20by%20minimizing%20image%20space%20losses%20to%20ensure%20high-quality%20generation.%20Our%20approach%20outperforms%20previous%20state-of-the-art%20methods%2C%20being%20ranked%20best%202.6x%20more%20often%20than%20the%20second-place%20method%20in%20a%20user%20study%2C%20while%20being%206x%20faster.%0ALink%3A%20http%3A//arxiv.org/abs/2505.16565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM2SVid%253A%2520End-to-End%2520Inpainting%2520and%2520Refinement%2520for%2520Monocular-to-Stereo%2520Video%2520Conversion%26entry.906535625%3DNina%2520Shvetsova%2520and%2520Goutam%2520Bhat%2520and%2520Prune%2520Truong%2520and%2520Hilde%2520Kuehne%2520and%2520Federico%2520Tombari%26entry.1292438233%3DWe%2520tackle%2520the%2520problem%2520of%2520monocular-to-stereo%2520video%2520conversion%2520and%2520propose%2520a%2520novel%2520architecture%2520for%2520inpainting%2520and%2520refinement%2520of%2520the%2520warped%2520right%2520view%2520obtained%2520by%2520depth-based%2520reprojection%2520of%2520the%2520input%2520left%2520view.%2520We%2520extend%2520the%2520Stable%2520Video%2520Diffusion%2520%2528SVD%2529%2520model%2520to%2520utilize%2520the%2520input%2520left%2520video%252C%2520the%2520warped%2520right%2520video%252C%2520and%2520the%2520disocclusion%2520masks%2520as%2520conditioning%2520input%2520to%2520generate%2520a%2520high-quality%2520right%2520camera%2520view.%2520In%2520order%2520to%2520effectively%2520exploit%2520information%2520from%2520neighboring%2520frames%2520for%2520inpainting%252C%2520we%2520modify%2520the%2520attention%2520layers%2520in%2520SVD%2520to%2520compute%2520full%2520attention%2520for%2520discoccluded%2520pixels.%2520Our%2520model%2520is%2520trained%2520to%2520generate%2520the%2520right%2520view%2520video%2520in%2520an%2520end-to-end%2520manner%2520without%2520iterative%2520diffusion%2520steps%2520by%2520minimizing%2520image%2520space%2520losses%2520to%2520ensure%2520high-quality%2520generation.%2520Our%2520approach%2520outperforms%2520previous%2520state-of-the-art%2520methods%252C%2520being%2520ranked%2520best%25202.6x%2520more%2520often%2520than%2520the%2520second-place%2520method%2520in%2520a%2520user%2520study%252C%2520while%2520being%25206x%2520faster.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M2SVid%3A%20End-to-End%20Inpainting%20and%20Refinement%20for%20Monocular-to-Stereo%20Video%20Conversion&entry.906535625=Nina%20Shvetsova%20and%20Goutam%20Bhat%20and%20Prune%20Truong%20and%20Hilde%20Kuehne%20and%20Federico%20Tombari&entry.1292438233=We%20tackle%20the%20problem%20of%20monocular-to-stereo%20video%20conversion%20and%20propose%20a%20novel%20architecture%20for%20inpainting%20and%20refinement%20of%20the%20warped%20right%20view%20obtained%20by%20depth-based%20reprojection%20of%20the%20input%20left%20view.%20We%20extend%20the%20Stable%20Video%20Diffusion%20%28SVD%29%20model%20to%20utilize%20the%20input%20left%20video%2C%20the%20warped%20right%20video%2C%20and%20the%20disocclusion%20masks%20as%20conditioning%20input%20to%20generate%20a%20high-quality%20right%20camera%20view.%20In%20order%20to%20effectively%20exploit%20information%20from%20neighboring%20frames%20for%20inpainting%2C%20we%20modify%20the%20attention%20layers%20in%20SVD%20to%20compute%20full%20attention%20for%20discoccluded%20pixels.%20Our%20model%20is%20trained%20to%20generate%20the%20right%20view%20video%20in%20an%20end-to-end%20manner%20without%20iterative%20diffusion%20steps%20by%20minimizing%20image%20space%20losses%20to%20ensure%20high-quality%20generation.%20Our%20approach%20outperforms%20previous%20state-of-the-art%20methods%2C%20being%20ranked%20best%202.6x%20more%20often%20than%20the%20second-place%20method%20in%20a%20user%20study%2C%20while%20being%206x%20faster.&entry.1838667208=http%3A//arxiv.org/abs/2505.16565v2&entry.124074799=Read"},
{"title": "Time-Domain Linear Model-based Framework for Passive Acoustic Mapping of Cavitation Activity", "author": "Tatiana Gelvez-Barrera and Barbara Nicolas and Denis Kouam\u00e9 and Bruno Gilles and Adrian Basarab", "abstract": "Passive acoustic mapping enables the spatial mapping and temporal monitoring of cavitation activity, playing a crucial role in therapeutic ultrasound applications. Most conventional beamforming methods, whether implemented in the time or frequency domains, suffer from limited axial resolution due to the absence of a reference emission onset time. While frequency-domain methods, the most efficient of which are based on the cross-spectral matrix, require long signals for accurate estimation, time-domain methods typically achieve lower spatial resolution. To address these limitations, we propose a linear model-based beamforming framework fully formulated in the time domain. The linear forward model relates a discretized spatiotemporal distribution of cavitation activity to the temporal signals recorded by a probe, explicitly accounting for time-of-flight delays dictated by the acquisition geometry. This model is then inverted using regularization techniques that exploit prior knowledge of cavitation activity in both spatial and temporal domains. Experimental results show that the proposed framework achieves enhanced or competitive cavitation map quality while using only 20\\% of the data typically required by frequency-domain methods. This highlights the substantial gain in data efficiency and the flexibility of our spatiotemporal regularization to adapt to diverse passive cavitation scenarios, outperforming state-of-the-art techniques.", "link": "http://arxiv.org/abs/2511.20551v1", "date": "2025-11-25", "relevancy": 2.3189, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4729}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4592}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-Domain%20Linear%20Model-based%20Framework%20for%20Passive%20Acoustic%20Mapping%20of%20Cavitation%20Activity&body=Title%3A%20Time-Domain%20Linear%20Model-based%20Framework%20for%20Passive%20Acoustic%20Mapping%20of%20Cavitation%20Activity%0AAuthor%3A%20Tatiana%20Gelvez-Barrera%20and%20Barbara%20Nicolas%20and%20Denis%20Kouam%C3%A9%20and%20Bruno%20Gilles%20and%20Adrian%20Basarab%0AAbstract%3A%20Passive%20acoustic%20mapping%20enables%20the%20spatial%20mapping%20and%20temporal%20monitoring%20of%20cavitation%20activity%2C%20playing%20a%20crucial%20role%20in%20therapeutic%20ultrasound%20applications.%20Most%20conventional%20beamforming%20methods%2C%20whether%20implemented%20in%20the%20time%20or%20frequency%20domains%2C%20suffer%20from%20limited%20axial%20resolution%20due%20to%20the%20absence%20of%20a%20reference%20emission%20onset%20time.%20While%20frequency-domain%20methods%2C%20the%20most%20efficient%20of%20which%20are%20based%20on%20the%20cross-spectral%20matrix%2C%20require%20long%20signals%20for%20accurate%20estimation%2C%20time-domain%20methods%20typically%20achieve%20lower%20spatial%20resolution.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20linear%20model-based%20beamforming%20framework%20fully%20formulated%20in%20the%20time%20domain.%20The%20linear%20forward%20model%20relates%20a%20discretized%20spatiotemporal%20distribution%20of%20cavitation%20activity%20to%20the%20temporal%20signals%20recorded%20by%20a%20probe%2C%20explicitly%20accounting%20for%20time-of-flight%20delays%20dictated%20by%20the%20acquisition%20geometry.%20This%20model%20is%20then%20inverted%20using%20regularization%20techniques%20that%20exploit%20prior%20knowledge%20of%20cavitation%20activity%20in%20both%20spatial%20and%20temporal%20domains.%20Experimental%20results%20show%20that%20the%20proposed%20framework%20achieves%20enhanced%20or%20competitive%20cavitation%20map%20quality%20while%20using%20only%2020%5C%25%20of%20the%20data%20typically%20required%20by%20frequency-domain%20methods.%20This%20highlights%20the%20substantial%20gain%20in%20data%20efficiency%20and%20the%20flexibility%20of%20our%20spatiotemporal%20regularization%20to%20adapt%20to%20diverse%20passive%20cavitation%20scenarios%2C%20outperforming%20state-of-the-art%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-Domain%2520Linear%2520Model-based%2520Framework%2520for%2520Passive%2520Acoustic%2520Mapping%2520of%2520Cavitation%2520Activity%26entry.906535625%3DTatiana%2520Gelvez-Barrera%2520and%2520Barbara%2520Nicolas%2520and%2520Denis%2520Kouam%25C3%25A9%2520and%2520Bruno%2520Gilles%2520and%2520Adrian%2520Basarab%26entry.1292438233%3DPassive%2520acoustic%2520mapping%2520enables%2520the%2520spatial%2520mapping%2520and%2520temporal%2520monitoring%2520of%2520cavitation%2520activity%252C%2520playing%2520a%2520crucial%2520role%2520in%2520therapeutic%2520ultrasound%2520applications.%2520Most%2520conventional%2520beamforming%2520methods%252C%2520whether%2520implemented%2520in%2520the%2520time%2520or%2520frequency%2520domains%252C%2520suffer%2520from%2520limited%2520axial%2520resolution%2520due%2520to%2520the%2520absence%2520of%2520a%2520reference%2520emission%2520onset%2520time.%2520While%2520frequency-domain%2520methods%252C%2520the%2520most%2520efficient%2520of%2520which%2520are%2520based%2520on%2520the%2520cross-spectral%2520matrix%252C%2520require%2520long%2520signals%2520for%2520accurate%2520estimation%252C%2520time-domain%2520methods%2520typically%2520achieve%2520lower%2520spatial%2520resolution.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520linear%2520model-based%2520beamforming%2520framework%2520fully%2520formulated%2520in%2520the%2520time%2520domain.%2520The%2520linear%2520forward%2520model%2520relates%2520a%2520discretized%2520spatiotemporal%2520distribution%2520of%2520cavitation%2520activity%2520to%2520the%2520temporal%2520signals%2520recorded%2520by%2520a%2520probe%252C%2520explicitly%2520accounting%2520for%2520time-of-flight%2520delays%2520dictated%2520by%2520the%2520acquisition%2520geometry.%2520This%2520model%2520is%2520then%2520inverted%2520using%2520regularization%2520techniques%2520that%2520exploit%2520prior%2520knowledge%2520of%2520cavitation%2520activity%2520in%2520both%2520spatial%2520and%2520temporal%2520domains.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520framework%2520achieves%2520enhanced%2520or%2520competitive%2520cavitation%2520map%2520quality%2520while%2520using%2520only%252020%255C%2525%2520of%2520the%2520data%2520typically%2520required%2520by%2520frequency-domain%2520methods.%2520This%2520highlights%2520the%2520substantial%2520gain%2520in%2520data%2520efficiency%2520and%2520the%2520flexibility%2520of%2520our%2520spatiotemporal%2520regularization%2520to%2520adapt%2520to%2520diverse%2520passive%2520cavitation%2520scenarios%252C%2520outperforming%2520state-of-the-art%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-Domain%20Linear%20Model-based%20Framework%20for%20Passive%20Acoustic%20Mapping%20of%20Cavitation%20Activity&entry.906535625=Tatiana%20Gelvez-Barrera%20and%20Barbara%20Nicolas%20and%20Denis%20Kouam%C3%A9%20and%20Bruno%20Gilles%20and%20Adrian%20Basarab&entry.1292438233=Passive%20acoustic%20mapping%20enables%20the%20spatial%20mapping%20and%20temporal%20monitoring%20of%20cavitation%20activity%2C%20playing%20a%20crucial%20role%20in%20therapeutic%20ultrasound%20applications.%20Most%20conventional%20beamforming%20methods%2C%20whether%20implemented%20in%20the%20time%20or%20frequency%20domains%2C%20suffer%20from%20limited%20axial%20resolution%20due%20to%20the%20absence%20of%20a%20reference%20emission%20onset%20time.%20While%20frequency-domain%20methods%2C%20the%20most%20efficient%20of%20which%20are%20based%20on%20the%20cross-spectral%20matrix%2C%20require%20long%20signals%20for%20accurate%20estimation%2C%20time-domain%20methods%20typically%20achieve%20lower%20spatial%20resolution.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20linear%20model-based%20beamforming%20framework%20fully%20formulated%20in%20the%20time%20domain.%20The%20linear%20forward%20model%20relates%20a%20discretized%20spatiotemporal%20distribution%20of%20cavitation%20activity%20to%20the%20temporal%20signals%20recorded%20by%20a%20probe%2C%20explicitly%20accounting%20for%20time-of-flight%20delays%20dictated%20by%20the%20acquisition%20geometry.%20This%20model%20is%20then%20inverted%20using%20regularization%20techniques%20that%20exploit%20prior%20knowledge%20of%20cavitation%20activity%20in%20both%20spatial%20and%20temporal%20domains.%20Experimental%20results%20show%20that%20the%20proposed%20framework%20achieves%20enhanced%20or%20competitive%20cavitation%20map%20quality%20while%20using%20only%2020%5C%25%20of%20the%20data%20typically%20required%20by%20frequency-domain%20methods.%20This%20highlights%20the%20substantial%20gain%20in%20data%20efficiency%20and%20the%20flexibility%20of%20our%20spatiotemporal%20regularization%20to%20adapt%20to%20diverse%20passive%20cavitation%20scenarios%2C%20outperforming%20state-of-the-art%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2511.20551v1&entry.124074799=Read"},
{"title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning", "author": "Thinh Dao and Dung Thuy Nguyen and Khoa D Doan and Kok-Seng Wong", "abstract": "Research on backdoor attacks in Federated Learning (FL) has accelerated in recent years, with new attacks and defenses continually proposed in an escalating arms race. However, the evaluation of these methods remains neither standardized nor reliable. First, there are severe inconsistencies in the evaluation settings across studies, and many rely on unrealistic threat models. Second, our code review uncovers semantic bugs in the official codebases of several attacks that artificially inflate their reported performance. These issues raise fundamental questions about whether current methods are truly effective or simply overfitted to narrow experimental setups. We introduce \\textbf{BackFed}, a benchmark designed to standardize and stress-test FL backdoor evaluation by unifying attacks and defenses under a common evaluation framework that mirrors realistic FL deployments. Our benchmark on three representative datasets with three distinct architectures reveals critical limitations of existing methods. Malicious clients often require excessive training time and computation, making them vulnerable to server-enforced time constraints. Meanwhile, several defenses incur severe accuracy degradation or aggregation overhead. Popular defenses and attacks achieve limited performance in our benchmark, which challenges their previous efficacy claims. We establish BackFed as a rigorous and fair evaluation framework that enables more reliable progress in FL backdoor research.", "link": "http://arxiv.org/abs/2507.04903v2", "date": "2025-11-25", "relevancy": 2.308, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4797}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4613}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BackFed%3A%20An%20Efficient%20%26%20Standardized%20Benchmark%20Suite%20for%20Backdoor%20Attacks%20in%20Federated%20Learning&body=Title%3A%20BackFed%3A%20An%20Efficient%20%26%20Standardized%20Benchmark%20Suite%20for%20Backdoor%20Attacks%20in%20Federated%20Learning%0AAuthor%3A%20Thinh%20Dao%20and%20Dung%20Thuy%20Nguyen%20and%20Khoa%20D%20Doan%20and%20Kok-Seng%20Wong%0AAbstract%3A%20Research%20on%20backdoor%20attacks%20in%20Federated%20Learning%20%28FL%29%20has%20accelerated%20in%20recent%20years%2C%20with%20new%20attacks%20and%20defenses%20continually%20proposed%20in%20an%20escalating%20arms%20race.%20However%2C%20the%20evaluation%20of%20these%20methods%20remains%20neither%20standardized%20nor%20reliable.%20First%2C%20there%20are%20severe%20inconsistencies%20in%20the%20evaluation%20settings%20across%20studies%2C%20and%20many%20rely%20on%20unrealistic%20threat%20models.%20Second%2C%20our%20code%20review%20uncovers%20semantic%20bugs%20in%20the%20official%20codebases%20of%20several%20attacks%20that%20artificially%20inflate%20their%20reported%20performance.%20These%20issues%20raise%20fundamental%20questions%20about%20whether%20current%20methods%20are%20truly%20effective%20or%20simply%20overfitted%20to%20narrow%20experimental%20setups.%20We%20introduce%20%5Ctextbf%7BBackFed%7D%2C%20a%20benchmark%20designed%20to%20standardize%20and%20stress-test%20FL%20backdoor%20evaluation%20by%20unifying%20attacks%20and%20defenses%20under%20a%20common%20evaluation%20framework%20that%20mirrors%20realistic%20FL%20deployments.%20Our%20benchmark%20on%20three%20representative%20datasets%20with%20three%20distinct%20architectures%20reveals%20critical%20limitations%20of%20existing%20methods.%20Malicious%20clients%20often%20require%20excessive%20training%20time%20and%20computation%2C%20making%20them%20vulnerable%20to%20server-enforced%20time%20constraints.%20Meanwhile%2C%20several%20defenses%20incur%20severe%20accuracy%20degradation%20or%20aggregation%20overhead.%20Popular%20defenses%20and%20attacks%20achieve%20limited%20performance%20in%20our%20benchmark%2C%20which%20challenges%20their%20previous%20efficacy%20claims.%20We%20establish%20BackFed%20as%20a%20rigorous%20and%20fair%20evaluation%20framework%20that%20enables%20more%20reliable%20progress%20in%20FL%20backdoor%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04903v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackFed%253A%2520An%2520Efficient%2520%2526%2520Standardized%2520Benchmark%2520Suite%2520for%2520Backdoor%2520Attacks%2520in%2520Federated%2520Learning%26entry.906535625%3DThinh%2520Dao%2520and%2520Dung%2520Thuy%2520Nguyen%2520and%2520Khoa%2520D%2520Doan%2520and%2520Kok-Seng%2520Wong%26entry.1292438233%3DResearch%2520on%2520backdoor%2520attacks%2520in%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520accelerated%2520in%2520recent%2520years%252C%2520with%2520new%2520attacks%2520and%2520defenses%2520continually%2520proposed%2520in%2520an%2520escalating%2520arms%2520race.%2520However%252C%2520the%2520evaluation%2520of%2520these%2520methods%2520remains%2520neither%2520standardized%2520nor%2520reliable.%2520First%252C%2520there%2520are%2520severe%2520inconsistencies%2520in%2520the%2520evaluation%2520settings%2520across%2520studies%252C%2520and%2520many%2520rely%2520on%2520unrealistic%2520threat%2520models.%2520Second%252C%2520our%2520code%2520review%2520uncovers%2520semantic%2520bugs%2520in%2520the%2520official%2520codebases%2520of%2520several%2520attacks%2520that%2520artificially%2520inflate%2520their%2520reported%2520performance.%2520These%2520issues%2520raise%2520fundamental%2520questions%2520about%2520whether%2520current%2520methods%2520are%2520truly%2520effective%2520or%2520simply%2520overfitted%2520to%2520narrow%2520experimental%2520setups.%2520We%2520introduce%2520%255Ctextbf%257BBackFed%257D%252C%2520a%2520benchmark%2520designed%2520to%2520standardize%2520and%2520stress-test%2520FL%2520backdoor%2520evaluation%2520by%2520unifying%2520attacks%2520and%2520defenses%2520under%2520a%2520common%2520evaluation%2520framework%2520that%2520mirrors%2520realistic%2520FL%2520deployments.%2520Our%2520benchmark%2520on%2520three%2520representative%2520datasets%2520with%2520three%2520distinct%2520architectures%2520reveals%2520critical%2520limitations%2520of%2520existing%2520methods.%2520Malicious%2520clients%2520often%2520require%2520excessive%2520training%2520time%2520and%2520computation%252C%2520making%2520them%2520vulnerable%2520to%2520server-enforced%2520time%2520constraints.%2520Meanwhile%252C%2520several%2520defenses%2520incur%2520severe%2520accuracy%2520degradation%2520or%2520aggregation%2520overhead.%2520Popular%2520defenses%2520and%2520attacks%2520achieve%2520limited%2520performance%2520in%2520our%2520benchmark%252C%2520which%2520challenges%2520their%2520previous%2520efficacy%2520claims.%2520We%2520establish%2520BackFed%2520as%2520a%2520rigorous%2520and%2520fair%2520evaluation%2520framework%2520that%2520enables%2520more%2520reliable%2520progress%2520in%2520FL%2520backdoor%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04903v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BackFed%3A%20An%20Efficient%20%26%20Standardized%20Benchmark%20Suite%20for%20Backdoor%20Attacks%20in%20Federated%20Learning&entry.906535625=Thinh%20Dao%20and%20Dung%20Thuy%20Nguyen%20and%20Khoa%20D%20Doan%20and%20Kok-Seng%20Wong&entry.1292438233=Research%20on%20backdoor%20attacks%20in%20Federated%20Learning%20%28FL%29%20has%20accelerated%20in%20recent%20years%2C%20with%20new%20attacks%20and%20defenses%20continually%20proposed%20in%20an%20escalating%20arms%20race.%20However%2C%20the%20evaluation%20of%20these%20methods%20remains%20neither%20standardized%20nor%20reliable.%20First%2C%20there%20are%20severe%20inconsistencies%20in%20the%20evaluation%20settings%20across%20studies%2C%20and%20many%20rely%20on%20unrealistic%20threat%20models.%20Second%2C%20our%20code%20review%20uncovers%20semantic%20bugs%20in%20the%20official%20codebases%20of%20several%20attacks%20that%20artificially%20inflate%20their%20reported%20performance.%20These%20issues%20raise%20fundamental%20questions%20about%20whether%20current%20methods%20are%20truly%20effective%20or%20simply%20overfitted%20to%20narrow%20experimental%20setups.%20We%20introduce%20%5Ctextbf%7BBackFed%7D%2C%20a%20benchmark%20designed%20to%20standardize%20and%20stress-test%20FL%20backdoor%20evaluation%20by%20unifying%20attacks%20and%20defenses%20under%20a%20common%20evaluation%20framework%20that%20mirrors%20realistic%20FL%20deployments.%20Our%20benchmark%20on%20three%20representative%20datasets%20with%20three%20distinct%20architectures%20reveals%20critical%20limitations%20of%20existing%20methods.%20Malicious%20clients%20often%20require%20excessive%20training%20time%20and%20computation%2C%20making%20them%20vulnerable%20to%20server-enforced%20time%20constraints.%20Meanwhile%2C%20several%20defenses%20incur%20severe%20accuracy%20degradation%20or%20aggregation%20overhead.%20Popular%20defenses%20and%20attacks%20achieve%20limited%20performance%20in%20our%20benchmark%2C%20which%20challenges%20their%20previous%20efficacy%20claims.%20We%20establish%20BackFed%20as%20a%20rigorous%20and%20fair%20evaluation%20framework%20that%20enables%20more%20reliable%20progress%20in%20FL%20backdoor%20research.&entry.1838667208=http%3A//arxiv.org/abs/2507.04903v2&entry.124074799=Read"},
{"title": "Multi-modal Generative AI: Multi-modal LLMs, Diffusions, and the Unification", "author": "Xin Wang and Yuwei Zhou and Bin Huang and Hong Chen and Wenwu Zhu", "abstract": "Multi-modal generative AI (Artificial Intelligence) has attracted increasing attention from both academia and industry. Particularly, two dominant families of techniques have emerged: i) Multi-modal large language models (LLMs) demonstrate impressive ability for multi-modal understanding; and ii) Diffusion models exhibit remarkable multi-modal powers in terms of multi-modal generation. Therefore, this paper provides a comprehensive overview of multi-modal generative AI, including multi-modal LLMs, diffusions, and the unification for understanding and generation. To lay a solid foundation for unified models, we first provide a detailed review of both multi-modal LLMs and diffusion models respectively, including their probabilistic modeling procedure, multi-modal architecture design, and advanced applications to image/video LLMs as well as text-to-image/video generation. Furthermore, we explore the emerging efforts toward unified models for understanding and generation. To achieve the unification of understanding and generation, we investigate key designs including autoregressive-based and diffusion-based modeling, as well as dense and Mixture-of-Experts (MoE) architectures. We then introduce several strategies for unified models, analyzing their potential advantages and disadvantages. In addition, we summarize the common datasets widely used for multi-modal generative AI pretraining. Last but not least, we present several challenging future research directions which may contribute to the ongoing advancement of multi-modal generative AI.", "link": "http://arxiv.org/abs/2409.14993v3", "date": "2025-11-25", "relevancy": 2.3028, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5826}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.577}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Generative%20AI%3A%20Multi-modal%20LLMs%2C%20Diffusions%2C%20and%20the%20Unification&body=Title%3A%20Multi-modal%20Generative%20AI%3A%20Multi-modal%20LLMs%2C%20Diffusions%2C%20and%20the%20Unification%0AAuthor%3A%20Xin%20Wang%20and%20Yuwei%20Zhou%20and%20Bin%20Huang%20and%20Hong%20Chen%20and%20Wenwu%20Zhu%0AAbstract%3A%20Multi-modal%20generative%20AI%20%28Artificial%20Intelligence%29%20has%20attracted%20increasing%20attention%20from%20both%20academia%20and%20industry.%20Particularly%2C%20two%20dominant%20families%20of%20techniques%20have%20emerged%3A%20i%29%20Multi-modal%20large%20language%20models%20%28LLMs%29%20demonstrate%20impressive%20ability%20for%20multi-modal%20understanding%3B%20and%20ii%29%20Diffusion%20models%20exhibit%20remarkable%20multi-modal%20powers%20in%20terms%20of%20multi-modal%20generation.%20Therefore%2C%20this%20paper%20provides%20a%20comprehensive%20overview%20of%20multi-modal%20generative%20AI%2C%20including%20multi-modal%20LLMs%2C%20diffusions%2C%20and%20the%20unification%20for%20understanding%20and%20generation.%20To%20lay%20a%20solid%20foundation%20for%20unified%20models%2C%20we%20first%20provide%20a%20detailed%20review%20of%20both%20multi-modal%20LLMs%20and%20diffusion%20models%20respectively%2C%20including%20their%20probabilistic%20modeling%20procedure%2C%20multi-modal%20architecture%20design%2C%20and%20advanced%20applications%20to%20image/video%20LLMs%20as%20well%20as%20text-to-image/video%20generation.%20Furthermore%2C%20we%20explore%20the%20emerging%20efforts%20toward%20unified%20models%20for%20understanding%20and%20generation.%20To%20achieve%20the%20unification%20of%20understanding%20and%20generation%2C%20we%20investigate%20key%20designs%20including%20autoregressive-based%20and%20diffusion-based%20modeling%2C%20as%20well%20as%20dense%20and%20Mixture-of-Experts%20%28MoE%29%20architectures.%20We%20then%20introduce%20several%20strategies%20for%20unified%20models%2C%20analyzing%20their%20potential%20advantages%20and%20disadvantages.%20In%20addition%2C%20we%20summarize%20the%20common%20datasets%20widely%20used%20for%20multi-modal%20generative%20AI%20pretraining.%20Last%20but%20not%20least%2C%20we%20present%20several%20challenging%20future%20research%20directions%20which%20may%20contribute%20to%20the%20ongoing%20advancement%20of%20multi-modal%20generative%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2409.14993v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Generative%2520AI%253A%2520Multi-modal%2520LLMs%252C%2520Diffusions%252C%2520and%2520the%2520Unification%26entry.906535625%3DXin%2520Wang%2520and%2520Yuwei%2520Zhou%2520and%2520Bin%2520Huang%2520and%2520Hong%2520Chen%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3DMulti-modal%2520generative%2520AI%2520%2528Artificial%2520Intelligence%2529%2520has%2520attracted%2520increasing%2520attention%2520from%2520both%2520academia%2520and%2520industry.%2520Particularly%252C%2520two%2520dominant%2520families%2520of%2520techniques%2520have%2520emerged%253A%2520i%2529%2520Multi-modal%2520large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520impressive%2520ability%2520for%2520multi-modal%2520understanding%253B%2520and%2520ii%2529%2520Diffusion%2520models%2520exhibit%2520remarkable%2520multi-modal%2520powers%2520in%2520terms%2520of%2520multi-modal%2520generation.%2520Therefore%252C%2520this%2520paper%2520provides%2520a%2520comprehensive%2520overview%2520of%2520multi-modal%2520generative%2520AI%252C%2520including%2520multi-modal%2520LLMs%252C%2520diffusions%252C%2520and%2520the%2520unification%2520for%2520understanding%2520and%2520generation.%2520To%2520lay%2520a%2520solid%2520foundation%2520for%2520unified%2520models%252C%2520we%2520first%2520provide%2520a%2520detailed%2520review%2520of%2520both%2520multi-modal%2520LLMs%2520and%2520diffusion%2520models%2520respectively%252C%2520including%2520their%2520probabilistic%2520modeling%2520procedure%252C%2520multi-modal%2520architecture%2520design%252C%2520and%2520advanced%2520applications%2520to%2520image/video%2520LLMs%2520as%2520well%2520as%2520text-to-image/video%2520generation.%2520Furthermore%252C%2520we%2520explore%2520the%2520emerging%2520efforts%2520toward%2520unified%2520models%2520for%2520understanding%2520and%2520generation.%2520To%2520achieve%2520the%2520unification%2520of%2520understanding%2520and%2520generation%252C%2520we%2520investigate%2520key%2520designs%2520including%2520autoregressive-based%2520and%2520diffusion-based%2520modeling%252C%2520as%2520well%2520as%2520dense%2520and%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures.%2520We%2520then%2520introduce%2520several%2520strategies%2520for%2520unified%2520models%252C%2520analyzing%2520their%2520potential%2520advantages%2520and%2520disadvantages.%2520In%2520addition%252C%2520we%2520summarize%2520the%2520common%2520datasets%2520widely%2520used%2520for%2520multi-modal%2520generative%2520AI%2520pretraining.%2520Last%2520but%2520not%2520least%252C%2520we%2520present%2520several%2520challenging%2520future%2520research%2520directions%2520which%2520may%2520contribute%2520to%2520the%2520ongoing%2520advancement%2520of%2520multi-modal%2520generative%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14993v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Generative%20AI%3A%20Multi-modal%20LLMs%2C%20Diffusions%2C%20and%20the%20Unification&entry.906535625=Xin%20Wang%20and%20Yuwei%20Zhou%20and%20Bin%20Huang%20and%20Hong%20Chen%20and%20Wenwu%20Zhu&entry.1292438233=Multi-modal%20generative%20AI%20%28Artificial%20Intelligence%29%20has%20attracted%20increasing%20attention%20from%20both%20academia%20and%20industry.%20Particularly%2C%20two%20dominant%20families%20of%20techniques%20have%20emerged%3A%20i%29%20Multi-modal%20large%20language%20models%20%28LLMs%29%20demonstrate%20impressive%20ability%20for%20multi-modal%20understanding%3B%20and%20ii%29%20Diffusion%20models%20exhibit%20remarkable%20multi-modal%20powers%20in%20terms%20of%20multi-modal%20generation.%20Therefore%2C%20this%20paper%20provides%20a%20comprehensive%20overview%20of%20multi-modal%20generative%20AI%2C%20including%20multi-modal%20LLMs%2C%20diffusions%2C%20and%20the%20unification%20for%20understanding%20and%20generation.%20To%20lay%20a%20solid%20foundation%20for%20unified%20models%2C%20we%20first%20provide%20a%20detailed%20review%20of%20both%20multi-modal%20LLMs%20and%20diffusion%20models%20respectively%2C%20including%20their%20probabilistic%20modeling%20procedure%2C%20multi-modal%20architecture%20design%2C%20and%20advanced%20applications%20to%20image/video%20LLMs%20as%20well%20as%20text-to-image/video%20generation.%20Furthermore%2C%20we%20explore%20the%20emerging%20efforts%20toward%20unified%20models%20for%20understanding%20and%20generation.%20To%20achieve%20the%20unification%20of%20understanding%20and%20generation%2C%20we%20investigate%20key%20designs%20including%20autoregressive-based%20and%20diffusion-based%20modeling%2C%20as%20well%20as%20dense%20and%20Mixture-of-Experts%20%28MoE%29%20architectures.%20We%20then%20introduce%20several%20strategies%20for%20unified%20models%2C%20analyzing%20their%20potential%20advantages%20and%20disadvantages.%20In%20addition%2C%20we%20summarize%20the%20common%20datasets%20widely%20used%20for%20multi-modal%20generative%20AI%20pretraining.%20Last%20but%20not%20least%2C%20we%20present%20several%20challenging%20future%20research%20directions%20which%20may%20contribute%20to%20the%20ongoing%20advancement%20of%20multi-modal%20generative%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2409.14993v3&entry.124074799=Read"},
{"title": "SelfMOTR: Revisiting MOTR with Self-Generating Detection Priors", "author": "Fabian G\u00fclhan and Emil Mededovic and Yuli Wu and Johannes Stegmaier", "abstract": "Despite progress toward end-to-end tracking with transformer architectures, poor detection performance and the conflict between detection and association in a joint architecture remain critical concerns. Recent approaches aim to mitigate these issues by (i) employing advanced denoising or label assignment strategies, or (ii) incorporating detection priors from external object detectors via distillation or anchor proposal techniques. Inspired by the success of integrating detection priors and by the key insight that MOTR-like models are secretly strong detection models, we introduce SelfMOTR, a novel tracking transformer that relies on self-generated detection priors. Through extensive analysis and ablation studies, we uncover and demonstrate the hidden detection capabilities of MOTR-like models, and present a practical set of tools for leveraging them effectively. On DanceTrack, SelfMOTR achieves strong performance, competing with recent state-of-the-art end-to-end tracking methods.", "link": "http://arxiv.org/abs/2511.20279v1", "date": "2025-11-25", "relevancy": 2.3015, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6258}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfMOTR%3A%20Revisiting%20MOTR%20with%20Self-Generating%20Detection%20Priors&body=Title%3A%20SelfMOTR%3A%20Revisiting%20MOTR%20with%20Self-Generating%20Detection%20Priors%0AAuthor%3A%20Fabian%20G%C3%BClhan%20and%20Emil%20Mededovic%20and%20Yuli%20Wu%20and%20Johannes%20Stegmaier%0AAbstract%3A%20Despite%20progress%20toward%20end-to-end%20tracking%20with%20transformer%20architectures%2C%20poor%20detection%20performance%20and%20the%20conflict%20between%20detection%20and%20association%20in%20a%20joint%20architecture%20remain%20critical%20concerns.%20Recent%20approaches%20aim%20to%20mitigate%20these%20issues%20by%20%28i%29%20employing%20advanced%20denoising%20or%20label%20assignment%20strategies%2C%20or%20%28ii%29%20incorporating%20detection%20priors%20from%20external%20object%20detectors%20via%20distillation%20or%20anchor%20proposal%20techniques.%20Inspired%20by%20the%20success%20of%20integrating%20detection%20priors%20and%20by%20the%20key%20insight%20that%20MOTR-like%20models%20are%20secretly%20strong%20detection%20models%2C%20we%20introduce%20SelfMOTR%2C%20a%20novel%20tracking%20transformer%20that%20relies%20on%20self-generated%20detection%20priors.%20Through%20extensive%20analysis%20and%20ablation%20studies%2C%20we%20uncover%20and%20demonstrate%20the%20hidden%20detection%20capabilities%20of%20MOTR-like%20models%2C%20and%20present%20a%20practical%20set%20of%20tools%20for%20leveraging%20them%20effectively.%20On%20DanceTrack%2C%20SelfMOTR%20achieves%20strong%20performance%2C%20competing%20with%20recent%20state-of-the-art%20end-to-end%20tracking%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfMOTR%253A%2520Revisiting%2520MOTR%2520with%2520Self-Generating%2520Detection%2520Priors%26entry.906535625%3DFabian%2520G%25C3%25BClhan%2520and%2520Emil%2520Mededovic%2520and%2520Yuli%2520Wu%2520and%2520Johannes%2520Stegmaier%26entry.1292438233%3DDespite%2520progress%2520toward%2520end-to-end%2520tracking%2520with%2520transformer%2520architectures%252C%2520poor%2520detection%2520performance%2520and%2520the%2520conflict%2520between%2520detection%2520and%2520association%2520in%2520a%2520joint%2520architecture%2520remain%2520critical%2520concerns.%2520Recent%2520approaches%2520aim%2520to%2520mitigate%2520these%2520issues%2520by%2520%2528i%2529%2520employing%2520advanced%2520denoising%2520or%2520label%2520assignment%2520strategies%252C%2520or%2520%2528ii%2529%2520incorporating%2520detection%2520priors%2520from%2520external%2520object%2520detectors%2520via%2520distillation%2520or%2520anchor%2520proposal%2520techniques.%2520Inspired%2520by%2520the%2520success%2520of%2520integrating%2520detection%2520priors%2520and%2520by%2520the%2520key%2520insight%2520that%2520MOTR-like%2520models%2520are%2520secretly%2520strong%2520detection%2520models%252C%2520we%2520introduce%2520SelfMOTR%252C%2520a%2520novel%2520tracking%2520transformer%2520that%2520relies%2520on%2520self-generated%2520detection%2520priors.%2520Through%2520extensive%2520analysis%2520and%2520ablation%2520studies%252C%2520we%2520uncover%2520and%2520demonstrate%2520the%2520hidden%2520detection%2520capabilities%2520of%2520MOTR-like%2520models%252C%2520and%2520present%2520a%2520practical%2520set%2520of%2520tools%2520for%2520leveraging%2520them%2520effectively.%2520On%2520DanceTrack%252C%2520SelfMOTR%2520achieves%2520strong%2520performance%252C%2520competing%2520with%2520recent%2520state-of-the-art%2520end-to-end%2520tracking%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfMOTR%3A%20Revisiting%20MOTR%20with%20Self-Generating%20Detection%20Priors&entry.906535625=Fabian%20G%C3%BClhan%20and%20Emil%20Mededovic%20and%20Yuli%20Wu%20and%20Johannes%20Stegmaier&entry.1292438233=Despite%20progress%20toward%20end-to-end%20tracking%20with%20transformer%20architectures%2C%20poor%20detection%20performance%20and%20the%20conflict%20between%20detection%20and%20association%20in%20a%20joint%20architecture%20remain%20critical%20concerns.%20Recent%20approaches%20aim%20to%20mitigate%20these%20issues%20by%20%28i%29%20employing%20advanced%20denoising%20or%20label%20assignment%20strategies%2C%20or%20%28ii%29%20incorporating%20detection%20priors%20from%20external%20object%20detectors%20via%20distillation%20or%20anchor%20proposal%20techniques.%20Inspired%20by%20the%20success%20of%20integrating%20detection%20priors%20and%20by%20the%20key%20insight%20that%20MOTR-like%20models%20are%20secretly%20strong%20detection%20models%2C%20we%20introduce%20SelfMOTR%2C%20a%20novel%20tracking%20transformer%20that%20relies%20on%20self-generated%20detection%20priors.%20Through%20extensive%20analysis%20and%20ablation%20studies%2C%20we%20uncover%20and%20demonstrate%20the%20hidden%20detection%20capabilities%20of%20MOTR-like%20models%2C%20and%20present%20a%20practical%20set%20of%20tools%20for%20leveraging%20them%20effectively.%20On%20DanceTrack%2C%20SelfMOTR%20achieves%20strong%20performance%2C%20competing%20with%20recent%20state-of-the-art%20end-to-end%20tracking%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.20279v1&entry.124074799=Read"},
{"title": "Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis", "author": "Mohammad Mahdi and Yuqian Fu and Nedko Savov and Jiancheng Pan and Danda Pani Paudel and Luc Van Gool", "abstract": "Foundation video generation models such as WAN 2.2 exhibit strong text- and image-conditioned synthesis abilities but remain constrained to the same-view generation setting. In this work, we introduce Exo2EgoSyn, an adaptation of WAN 2.2 that unlocks Exocentric-to-Egocentric(Exo2Ego) cross-view video synthesis. Our framework consists of three key modules. Ego-Exo View Alignment(EgoExo-Align) enforces latent-space alignment between exocentric and egocentric first-frame representations, reorienting the generative space from the given exo view toward the ego view. Multi-view Exocentric Video Conditioning (MultiExoCon) aggregates multi-view exocentric videos into a unified conditioning signal, extending WAN2.2 beyond its vanilla single-image or text conditioning. Furthermore, Pose-Aware Latent Injection (PoseInj) injects relative exo-to-ego camera pose information into the latent state, guiding geometry-aware synthesis across viewpoints. Together, these modules enable high-fidelity ego view video generation from third-person observations without retraining from scratch. Experiments on ExoEgo4D validate that Exo2EgoSyn significantly improves Ego2Exo synthesis, paving the way for scalable cross-view video generation with foundation models. Source code and models will be released publicly.", "link": "http://arxiv.org/abs/2511.20186v1", "date": "2025-11-25", "relevancy": 2.2953, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exo2EgoSyn%3A%20Unlocking%20Foundation%20Video%20Generation%20Models%20for%20Exocentric-to-Egocentric%20Video%20Synthesis&body=Title%3A%20Exo2EgoSyn%3A%20Unlocking%20Foundation%20Video%20Generation%20Models%20for%20Exocentric-to-Egocentric%20Video%20Synthesis%0AAuthor%3A%20Mohammad%20Mahdi%20and%20Yuqian%20Fu%20and%20Nedko%20Savov%20and%20Jiancheng%20Pan%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20Foundation%20video%20generation%20models%20such%20as%20WAN%202.2%20exhibit%20strong%20text-%20and%20image-conditioned%20synthesis%20abilities%20but%20remain%20constrained%20to%20the%20same-view%20generation%20setting.%20In%20this%20work%2C%20we%20introduce%20Exo2EgoSyn%2C%20an%20adaptation%20of%20WAN%202.2%20that%20unlocks%20Exocentric-to-Egocentric%28Exo2Ego%29%20cross-view%20video%20synthesis.%20Our%20framework%20consists%20of%20three%20key%20modules.%20Ego-Exo%20View%20Alignment%28EgoExo-Align%29%20enforces%20latent-space%20alignment%20between%20exocentric%20and%20egocentric%20first-frame%20representations%2C%20reorienting%20the%20generative%20space%20from%20the%20given%20exo%20view%20toward%20the%20ego%20view.%20Multi-view%20Exocentric%20Video%20Conditioning%20%28MultiExoCon%29%20aggregates%20multi-view%20exocentric%20videos%20into%20a%20unified%20conditioning%20signal%2C%20extending%20WAN2.2%20beyond%20its%20vanilla%20single-image%20or%20text%20conditioning.%20Furthermore%2C%20Pose-Aware%20Latent%20Injection%20%28PoseInj%29%20injects%20relative%20exo-to-ego%20camera%20pose%20information%20into%20the%20latent%20state%2C%20guiding%20geometry-aware%20synthesis%20across%20viewpoints.%20Together%2C%20these%20modules%20enable%20high-fidelity%20ego%20view%20video%20generation%20from%20third-person%20observations%20without%20retraining%20from%20scratch.%20Experiments%20on%20ExoEgo4D%20validate%20that%20Exo2EgoSyn%20significantly%20improves%20Ego2Exo%20synthesis%2C%20paving%20the%20way%20for%20scalable%20cross-view%20video%20generation%20with%20foundation%20models.%20Source%20code%20and%20models%20will%20be%20released%20publicly.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExo2EgoSyn%253A%2520Unlocking%2520Foundation%2520Video%2520Generation%2520Models%2520for%2520Exocentric-to-Egocentric%2520Video%2520Synthesis%26entry.906535625%3DMohammad%2520Mahdi%2520and%2520Yuqian%2520Fu%2520and%2520Nedko%2520Savov%2520and%2520Jiancheng%2520Pan%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3DFoundation%2520video%2520generation%2520models%2520such%2520as%2520WAN%25202.2%2520exhibit%2520strong%2520text-%2520and%2520image-conditioned%2520synthesis%2520abilities%2520but%2520remain%2520constrained%2520to%2520the%2520same-view%2520generation%2520setting.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Exo2EgoSyn%252C%2520an%2520adaptation%2520of%2520WAN%25202.2%2520that%2520unlocks%2520Exocentric-to-Egocentric%2528Exo2Ego%2529%2520cross-view%2520video%2520synthesis.%2520Our%2520framework%2520consists%2520of%2520three%2520key%2520modules.%2520Ego-Exo%2520View%2520Alignment%2528EgoExo-Align%2529%2520enforces%2520latent-space%2520alignment%2520between%2520exocentric%2520and%2520egocentric%2520first-frame%2520representations%252C%2520reorienting%2520the%2520generative%2520space%2520from%2520the%2520given%2520exo%2520view%2520toward%2520the%2520ego%2520view.%2520Multi-view%2520Exocentric%2520Video%2520Conditioning%2520%2528MultiExoCon%2529%2520aggregates%2520multi-view%2520exocentric%2520videos%2520into%2520a%2520unified%2520conditioning%2520signal%252C%2520extending%2520WAN2.2%2520beyond%2520its%2520vanilla%2520single-image%2520or%2520text%2520conditioning.%2520Furthermore%252C%2520Pose-Aware%2520Latent%2520Injection%2520%2528PoseInj%2529%2520injects%2520relative%2520exo-to-ego%2520camera%2520pose%2520information%2520into%2520the%2520latent%2520state%252C%2520guiding%2520geometry-aware%2520synthesis%2520across%2520viewpoints.%2520Together%252C%2520these%2520modules%2520enable%2520high-fidelity%2520ego%2520view%2520video%2520generation%2520from%2520third-person%2520observations%2520without%2520retraining%2520from%2520scratch.%2520Experiments%2520on%2520ExoEgo4D%2520validate%2520that%2520Exo2EgoSyn%2520significantly%2520improves%2520Ego2Exo%2520synthesis%252C%2520paving%2520the%2520way%2520for%2520scalable%2520cross-view%2520video%2520generation%2520with%2520foundation%2520models.%2520Source%2520code%2520and%2520models%2520will%2520be%2520released%2520publicly.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exo2EgoSyn%3A%20Unlocking%20Foundation%20Video%20Generation%20Models%20for%20Exocentric-to-Egocentric%20Video%20Synthesis&entry.906535625=Mohammad%20Mahdi%20and%20Yuqian%20Fu%20and%20Nedko%20Savov%20and%20Jiancheng%20Pan%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=Foundation%20video%20generation%20models%20such%20as%20WAN%202.2%20exhibit%20strong%20text-%20and%20image-conditioned%20synthesis%20abilities%20but%20remain%20constrained%20to%20the%20same-view%20generation%20setting.%20In%20this%20work%2C%20we%20introduce%20Exo2EgoSyn%2C%20an%20adaptation%20of%20WAN%202.2%20that%20unlocks%20Exocentric-to-Egocentric%28Exo2Ego%29%20cross-view%20video%20synthesis.%20Our%20framework%20consists%20of%20three%20key%20modules.%20Ego-Exo%20View%20Alignment%28EgoExo-Align%29%20enforces%20latent-space%20alignment%20between%20exocentric%20and%20egocentric%20first-frame%20representations%2C%20reorienting%20the%20generative%20space%20from%20the%20given%20exo%20view%20toward%20the%20ego%20view.%20Multi-view%20Exocentric%20Video%20Conditioning%20%28MultiExoCon%29%20aggregates%20multi-view%20exocentric%20videos%20into%20a%20unified%20conditioning%20signal%2C%20extending%20WAN2.2%20beyond%20its%20vanilla%20single-image%20or%20text%20conditioning.%20Furthermore%2C%20Pose-Aware%20Latent%20Injection%20%28PoseInj%29%20injects%20relative%20exo-to-ego%20camera%20pose%20information%20into%20the%20latent%20state%2C%20guiding%20geometry-aware%20synthesis%20across%20viewpoints.%20Together%2C%20these%20modules%20enable%20high-fidelity%20ego%20view%20video%20generation%20from%20third-person%20observations%20without%20retraining%20from%20scratch.%20Experiments%20on%20ExoEgo4D%20validate%20that%20Exo2EgoSyn%20significantly%20improves%20Ego2Exo%20synthesis%2C%20paving%20the%20way%20for%20scalable%20cross-view%20video%20generation%20with%20foundation%20models.%20Source%20code%20and%20models%20will%20be%20released%20publicly.&entry.1838667208=http%3A//arxiv.org/abs/2511.20186v1&entry.124074799=Read"},
{"title": "Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs", "author": "Bao Tang and Shuai Zhang and Yueting Zhu and Jijun Xiang and Xin Yang and Li Yu and Wenyu Liu and Xinggang Wang", "abstract": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.", "link": "http://arxiv.org/abs/2511.20410v1", "date": "2025-11-25", "relevancy": 2.2932, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5883}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.587}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-Free%20Timestep%20Distillation%20via%20Continuous-Time%20Consistency%20with%20Trajectory-Sampled%20Pairs&body=Title%3A%20Image-Free%20Timestep%20Distillation%20via%20Continuous-Time%20Consistency%20with%20Trajectory-Sampled%20Pairs%0AAuthor%3A%20Bao%20Tang%20and%20Shuai%20Zhang%20and%20Yueting%20Zhu%20and%20Jijun%20Xiang%20and%20Xin%20Yang%20and%20Li%20Yu%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20Timestep%20distillation%20is%20an%20effective%20approach%20for%20improving%20the%20generation%20efficiency%20of%20diffusion%20models.%20The%20Consistency%20Model%20%28CM%29%2C%20as%20a%20trajectory-based%20framework%2C%20demonstrates%20significant%20potential%20due%20to%20its%20strong%20theoretical%20foundation%20and%20high-quality%20few-step%20generation.%20Nevertheless%2C%20current%20continuous-time%20consistency%20distillation%20methods%20still%20rely%20heavily%20on%20training%20data%20and%20computational%20resources%2C%20hindering%20their%20deployment%20in%20resource-constrained%20scenarios%20and%20limiting%20their%20scalability%20to%20diverse%20domains.%20To%20address%20this%20issue%2C%20we%20propose%20Trajectory-Backward%20Consistency%20Model%20%28TBCM%29%2C%20which%20eliminates%20the%20dependence%20on%20external%20training%20data%20by%20extracting%20latent%20representations%20directly%20from%20the%20teacher%20model%27s%20generation%20trajectory.%20Unlike%20conventional%20methods%20that%20require%20VAE%20encoding%20and%20large-scale%20datasets%2C%20our%20self-contained%20distillation%20paradigm%20significantly%20improves%20both%20efficiency%20and%20simplicity.%20Moreover%2C%20the%20trajectory-extracted%20samples%20naturally%20bridge%20the%20distribution%20gap%20between%20training%20and%20inference%2C%20thereby%20enabling%20more%20effective%20knowledge%20transfer.%20Empirically%2C%20TBCM%20achieves%206.52%20FID%20and%2028.08%20CLIP%20scores%20on%20MJHQ-30k%20under%20one-step%20generation%2C%20while%20reducing%20training%20time%20by%20approximately%2040%25%20compared%20to%20Sana-Sprint%20and%20saving%20a%20substantial%20amount%20of%20GPU%20memory%2C%20demonstrating%20superior%20efficiency%20without%20sacrificing%20quality.%20We%20further%20reveal%20the%20diffusion-generation%20space%20discrepancy%20in%20continuous-time%20consistency%20distillation%20and%20analyze%20how%20sampling%20strategies%20affect%20distillation%20performance%2C%20offering%20insights%20for%20future%20distillation%20research.%20GitHub%20Link%3A%20https%3A//github.com/hustvl/TBCM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-Free%2520Timestep%2520Distillation%2520via%2520Continuous-Time%2520Consistency%2520with%2520Trajectory-Sampled%2520Pairs%26entry.906535625%3DBao%2520Tang%2520and%2520Shuai%2520Zhang%2520and%2520Yueting%2520Zhu%2520and%2520Jijun%2520Xiang%2520and%2520Xin%2520Yang%2520and%2520Li%2520Yu%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3DTimestep%2520distillation%2520is%2520an%2520effective%2520approach%2520for%2520improving%2520the%2520generation%2520efficiency%2520of%2520diffusion%2520models.%2520The%2520Consistency%2520Model%2520%2528CM%2529%252C%2520as%2520a%2520trajectory-based%2520framework%252C%2520demonstrates%2520significant%2520potential%2520due%2520to%2520its%2520strong%2520theoretical%2520foundation%2520and%2520high-quality%2520few-step%2520generation.%2520Nevertheless%252C%2520current%2520continuous-time%2520consistency%2520distillation%2520methods%2520still%2520rely%2520heavily%2520on%2520training%2520data%2520and%2520computational%2520resources%252C%2520hindering%2520their%2520deployment%2520in%2520resource-constrained%2520scenarios%2520and%2520limiting%2520their%2520scalability%2520to%2520diverse%2520domains.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Trajectory-Backward%2520Consistency%2520Model%2520%2528TBCM%2529%252C%2520which%2520eliminates%2520the%2520dependence%2520on%2520external%2520training%2520data%2520by%2520extracting%2520latent%2520representations%2520directly%2520from%2520the%2520teacher%2520model%2527s%2520generation%2520trajectory.%2520Unlike%2520conventional%2520methods%2520that%2520require%2520VAE%2520encoding%2520and%2520large-scale%2520datasets%252C%2520our%2520self-contained%2520distillation%2520paradigm%2520significantly%2520improves%2520both%2520efficiency%2520and%2520simplicity.%2520Moreover%252C%2520the%2520trajectory-extracted%2520samples%2520naturally%2520bridge%2520the%2520distribution%2520gap%2520between%2520training%2520and%2520inference%252C%2520thereby%2520enabling%2520more%2520effective%2520knowledge%2520transfer.%2520Empirically%252C%2520TBCM%2520achieves%25206.52%2520FID%2520and%252028.08%2520CLIP%2520scores%2520on%2520MJHQ-30k%2520under%2520one-step%2520generation%252C%2520while%2520reducing%2520training%2520time%2520by%2520approximately%252040%2525%2520compared%2520to%2520Sana-Sprint%2520and%2520saving%2520a%2520substantial%2520amount%2520of%2520GPU%2520memory%252C%2520demonstrating%2520superior%2520efficiency%2520without%2520sacrificing%2520quality.%2520We%2520further%2520reveal%2520the%2520diffusion-generation%2520space%2520discrepancy%2520in%2520continuous-time%2520consistency%2520distillation%2520and%2520analyze%2520how%2520sampling%2520strategies%2520affect%2520distillation%2520performance%252C%2520offering%2520insights%2520for%2520future%2520distillation%2520research.%2520GitHub%2520Link%253A%2520https%253A//github.com/hustvl/TBCM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-Free%20Timestep%20Distillation%20via%20Continuous-Time%20Consistency%20with%20Trajectory-Sampled%20Pairs&entry.906535625=Bao%20Tang%20and%20Shuai%20Zhang%20and%20Yueting%20Zhu%20and%20Jijun%20Xiang%20and%20Xin%20Yang%20and%20Li%20Yu%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=Timestep%20distillation%20is%20an%20effective%20approach%20for%20improving%20the%20generation%20efficiency%20of%20diffusion%20models.%20The%20Consistency%20Model%20%28CM%29%2C%20as%20a%20trajectory-based%20framework%2C%20demonstrates%20significant%20potential%20due%20to%20its%20strong%20theoretical%20foundation%20and%20high-quality%20few-step%20generation.%20Nevertheless%2C%20current%20continuous-time%20consistency%20distillation%20methods%20still%20rely%20heavily%20on%20training%20data%20and%20computational%20resources%2C%20hindering%20their%20deployment%20in%20resource-constrained%20scenarios%20and%20limiting%20their%20scalability%20to%20diverse%20domains.%20To%20address%20this%20issue%2C%20we%20propose%20Trajectory-Backward%20Consistency%20Model%20%28TBCM%29%2C%20which%20eliminates%20the%20dependence%20on%20external%20training%20data%20by%20extracting%20latent%20representations%20directly%20from%20the%20teacher%20model%27s%20generation%20trajectory.%20Unlike%20conventional%20methods%20that%20require%20VAE%20encoding%20and%20large-scale%20datasets%2C%20our%20self-contained%20distillation%20paradigm%20significantly%20improves%20both%20efficiency%20and%20simplicity.%20Moreover%2C%20the%20trajectory-extracted%20samples%20naturally%20bridge%20the%20distribution%20gap%20between%20training%20and%20inference%2C%20thereby%20enabling%20more%20effective%20knowledge%20transfer.%20Empirically%2C%20TBCM%20achieves%206.52%20FID%20and%2028.08%20CLIP%20scores%20on%20MJHQ-30k%20under%20one-step%20generation%2C%20while%20reducing%20training%20time%20by%20approximately%2040%25%20compared%20to%20Sana-Sprint%20and%20saving%20a%20substantial%20amount%20of%20GPU%20memory%2C%20demonstrating%20superior%20efficiency%20without%20sacrificing%20quality.%20We%20further%20reveal%20the%20diffusion-generation%20space%20discrepancy%20in%20continuous-time%20consistency%20distillation%20and%20analyze%20how%20sampling%20strategies%20affect%20distillation%20performance%2C%20offering%20insights%20for%20future%20distillation%20research.%20GitHub%20Link%3A%20https%3A//github.com/hustvl/TBCM.&entry.1838667208=http%3A//arxiv.org/abs/2511.20410v1&entry.124074799=Read"},
{"title": "ExDDV: A New Dataset for Explainable Deepfake Detection in Video", "author": "Vlad Hondru and Eduard Hogea and Darian Onchis and Radu Tudor Ionescu", "abstract": "The ever growing realism and quality of generated videos makes it increasingly harder for humans to spot deepfake content, who need to rely more and more on automatic deepfake detectors. However, deepfake detectors are also prone to errors, and their decisions are not explainable, leaving humans vulnerable to deepfake-based fraud and misinformation. To this end, we introduce ExDDV, the first dataset and benchmark for Explainable Deepfake Detection in Video. ExDDV comprises around 5.4K real and deepfake videos that are manually annotated with text descriptions (to explain the artifacts) and clicks (to point out the artifacts). We evaluate a number of vision-language models on ExDDV, performing experiments with various fine-tuning and in-context learning strategies. Our results show that text and click supervision are both required to develop robust explainable models for deepfake videos, which are able to localize and describe the observed artifacts. Our novel dataset and code to reproduce the results are available at https://github.com/vladhondru25/ExDDV.", "link": "http://arxiv.org/abs/2503.14421v2", "date": "2025-11-25", "relevancy": 2.2911, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6027}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5564}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExDDV%3A%20A%20New%20Dataset%20for%20Explainable%20Deepfake%20Detection%20in%20Video&body=Title%3A%20ExDDV%3A%20A%20New%20Dataset%20for%20Explainable%20Deepfake%20Detection%20in%20Video%0AAuthor%3A%20Vlad%20Hondru%20and%20Eduard%20Hogea%20and%20Darian%20Onchis%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20The%20ever%20growing%20realism%20and%20quality%20of%20generated%20videos%20makes%20it%20increasingly%20harder%20for%20humans%20to%20spot%20deepfake%20content%2C%20who%20need%20to%20rely%20more%20and%20more%20on%20automatic%20deepfake%20detectors.%20However%2C%20deepfake%20detectors%20are%20also%20prone%20to%20errors%2C%20and%20their%20decisions%20are%20not%20explainable%2C%20leaving%20humans%20vulnerable%20to%20deepfake-based%20fraud%20and%20misinformation.%20To%20this%20end%2C%20we%20introduce%20ExDDV%2C%20the%20first%20dataset%20and%20benchmark%20for%20Explainable%20Deepfake%20Detection%20in%20Video.%20ExDDV%20comprises%20around%205.4K%20real%20and%20deepfake%20videos%20that%20are%20manually%20annotated%20with%20text%20descriptions%20%28to%20explain%20the%20artifacts%29%20and%20clicks%20%28to%20point%20out%20the%20artifacts%29.%20We%20evaluate%20a%20number%20of%20vision-language%20models%20on%20ExDDV%2C%20performing%20experiments%20with%20various%20fine-tuning%20and%20in-context%20learning%20strategies.%20Our%20results%20show%20that%20text%20and%20click%20supervision%20are%20both%20required%20to%20develop%20robust%20explainable%20models%20for%20deepfake%20videos%2C%20which%20are%20able%20to%20localize%20and%20describe%20the%20observed%20artifacts.%20Our%20novel%20dataset%20and%20code%20to%20reproduce%20the%20results%20are%20available%20at%20https%3A//github.com/vladhondru25/ExDDV.%0ALink%3A%20http%3A//arxiv.org/abs/2503.14421v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExDDV%253A%2520A%2520New%2520Dataset%2520for%2520Explainable%2520Deepfake%2520Detection%2520in%2520Video%26entry.906535625%3DVlad%2520Hondru%2520and%2520Eduard%2520Hogea%2520and%2520Darian%2520Onchis%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3DThe%2520ever%2520growing%2520realism%2520and%2520quality%2520of%2520generated%2520videos%2520makes%2520it%2520increasingly%2520harder%2520for%2520humans%2520to%2520spot%2520deepfake%2520content%252C%2520who%2520need%2520to%2520rely%2520more%2520and%2520more%2520on%2520automatic%2520deepfake%2520detectors.%2520However%252C%2520deepfake%2520detectors%2520are%2520also%2520prone%2520to%2520errors%252C%2520and%2520their%2520decisions%2520are%2520not%2520explainable%252C%2520leaving%2520humans%2520vulnerable%2520to%2520deepfake-based%2520fraud%2520and%2520misinformation.%2520To%2520this%2520end%252C%2520we%2520introduce%2520ExDDV%252C%2520the%2520first%2520dataset%2520and%2520benchmark%2520for%2520Explainable%2520Deepfake%2520Detection%2520in%2520Video.%2520ExDDV%2520comprises%2520around%25205.4K%2520real%2520and%2520deepfake%2520videos%2520that%2520are%2520manually%2520annotated%2520with%2520text%2520descriptions%2520%2528to%2520explain%2520the%2520artifacts%2529%2520and%2520clicks%2520%2528to%2520point%2520out%2520the%2520artifacts%2529.%2520We%2520evaluate%2520a%2520number%2520of%2520vision-language%2520models%2520on%2520ExDDV%252C%2520performing%2520experiments%2520with%2520various%2520fine-tuning%2520and%2520in-context%2520learning%2520strategies.%2520Our%2520results%2520show%2520that%2520text%2520and%2520click%2520supervision%2520are%2520both%2520required%2520to%2520develop%2520robust%2520explainable%2520models%2520for%2520deepfake%2520videos%252C%2520which%2520are%2520able%2520to%2520localize%2520and%2520describe%2520the%2520observed%2520artifacts.%2520Our%2520novel%2520dataset%2520and%2520code%2520to%2520reproduce%2520the%2520results%2520are%2520available%2520at%2520https%253A//github.com/vladhondru25/ExDDV.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14421v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExDDV%3A%20A%20New%20Dataset%20for%20Explainable%20Deepfake%20Detection%20in%20Video&entry.906535625=Vlad%20Hondru%20and%20Eduard%20Hogea%20and%20Darian%20Onchis%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=The%20ever%20growing%20realism%20and%20quality%20of%20generated%20videos%20makes%20it%20increasingly%20harder%20for%20humans%20to%20spot%20deepfake%20content%2C%20who%20need%20to%20rely%20more%20and%20more%20on%20automatic%20deepfake%20detectors.%20However%2C%20deepfake%20detectors%20are%20also%20prone%20to%20errors%2C%20and%20their%20decisions%20are%20not%20explainable%2C%20leaving%20humans%20vulnerable%20to%20deepfake-based%20fraud%20and%20misinformation.%20To%20this%20end%2C%20we%20introduce%20ExDDV%2C%20the%20first%20dataset%20and%20benchmark%20for%20Explainable%20Deepfake%20Detection%20in%20Video.%20ExDDV%20comprises%20around%205.4K%20real%20and%20deepfake%20videos%20that%20are%20manually%20annotated%20with%20text%20descriptions%20%28to%20explain%20the%20artifacts%29%20and%20clicks%20%28to%20point%20out%20the%20artifacts%29.%20We%20evaluate%20a%20number%20of%20vision-language%20models%20on%20ExDDV%2C%20performing%20experiments%20with%20various%20fine-tuning%20and%20in-context%20learning%20strategies.%20Our%20results%20show%20that%20text%20and%20click%20supervision%20are%20both%20required%20to%20develop%20robust%20explainable%20models%20for%20deepfake%20videos%2C%20which%20are%20able%20to%20localize%20and%20describe%20the%20observed%20artifacts.%20Our%20novel%20dataset%20and%20code%20to%20reproduce%20the%20results%20are%20available%20at%20https%3A//github.com/vladhondru25/ExDDV.&entry.1838667208=http%3A//arxiv.org/abs/2503.14421v2&entry.124074799=Read"},
{"title": "CardioComposer: Leveraging Differentiable Geometry for Compositional Control of Anatomical Diffusion Models", "author": "Karim Kadry and Shoaib Goraya and Ajay Manicka and Abdalla Abdelwahed and Naravich Chutisilp and Farhad Nezami and Elazer Edelman", "abstract": "Generative models of 3D cardiovascular anatomy can synthesize informative structures for clinical research and medical device evaluation, but face a trade-off between geometric controllability and realism. We propose CardioComposer: a programmable, inference-time framework for generating multi-class anatomical label maps based on interpretable ellipsoidal primitives. These primitives represent geometric attributes such as the size, shape, and position of discrete substructures. We specifically develop differentiable measurement functions based on voxel-wise geometric moments, enabling loss-based gradient guidance during diffusion model sampling. We demonstrate that these losses can constrain individual geometric attributes in a disentangled manner and provide compositional control over multiple substructures. Finally, we show that our method is compatible with a wide array of anatomical systems containing non-convex substructures, spanning cardiac, vascular, and skeletal organs.", "link": "http://arxiv.org/abs/2509.08015v2", "date": "2025-11-25", "relevancy": 2.2846, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5869}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5797}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CardioComposer%3A%20Leveraging%20Differentiable%20Geometry%20for%20Compositional%20Control%20of%20Anatomical%20Diffusion%20Models&body=Title%3A%20CardioComposer%3A%20Leveraging%20Differentiable%20Geometry%20for%20Compositional%20Control%20of%20Anatomical%20Diffusion%20Models%0AAuthor%3A%20Karim%20Kadry%20and%20Shoaib%20Goraya%20and%20Ajay%20Manicka%20and%20Abdalla%20Abdelwahed%20and%20Naravich%20Chutisilp%20and%20Farhad%20Nezami%20and%20Elazer%20Edelman%0AAbstract%3A%20Generative%20models%20of%203D%20cardiovascular%20anatomy%20can%20synthesize%20informative%20structures%20for%20clinical%20research%20and%20medical%20device%20evaluation%2C%20but%20face%20a%20trade-off%20between%20geometric%20controllability%20and%20realism.%20We%20propose%20CardioComposer%3A%20a%20programmable%2C%20inference-time%20framework%20for%20generating%20multi-class%20anatomical%20label%20maps%20based%20on%20interpretable%20ellipsoidal%20primitives.%20These%20primitives%20represent%20geometric%20attributes%20such%20as%20the%20size%2C%20shape%2C%20and%20position%20of%20discrete%20substructures.%20We%20specifically%20develop%20differentiable%20measurement%20functions%20based%20on%20voxel-wise%20geometric%20moments%2C%20enabling%20loss-based%20gradient%20guidance%20during%20diffusion%20model%20sampling.%20We%20demonstrate%20that%20these%20losses%20can%20constrain%20individual%20geometric%20attributes%20in%20a%20disentangled%20manner%20and%20provide%20compositional%20control%20over%20multiple%20substructures.%20Finally%2C%20we%20show%20that%20our%20method%20is%20compatible%20with%20a%20wide%20array%20of%20anatomical%20systems%20containing%20non-convex%20substructures%2C%20spanning%20cardiac%2C%20vascular%2C%20and%20skeletal%20organs.%0ALink%3A%20http%3A//arxiv.org/abs/2509.08015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCardioComposer%253A%2520Leveraging%2520Differentiable%2520Geometry%2520for%2520Compositional%2520Control%2520of%2520Anatomical%2520Diffusion%2520Models%26entry.906535625%3DKarim%2520Kadry%2520and%2520Shoaib%2520Goraya%2520and%2520Ajay%2520Manicka%2520and%2520Abdalla%2520Abdelwahed%2520and%2520Naravich%2520Chutisilp%2520and%2520Farhad%2520Nezami%2520and%2520Elazer%2520Edelman%26entry.1292438233%3DGenerative%2520models%2520of%25203D%2520cardiovascular%2520anatomy%2520can%2520synthesize%2520informative%2520structures%2520for%2520clinical%2520research%2520and%2520medical%2520device%2520evaluation%252C%2520but%2520face%2520a%2520trade-off%2520between%2520geometric%2520controllability%2520and%2520realism.%2520We%2520propose%2520CardioComposer%253A%2520a%2520programmable%252C%2520inference-time%2520framework%2520for%2520generating%2520multi-class%2520anatomical%2520label%2520maps%2520based%2520on%2520interpretable%2520ellipsoidal%2520primitives.%2520These%2520primitives%2520represent%2520geometric%2520attributes%2520such%2520as%2520the%2520size%252C%2520shape%252C%2520and%2520position%2520of%2520discrete%2520substructures.%2520We%2520specifically%2520develop%2520differentiable%2520measurement%2520functions%2520based%2520on%2520voxel-wise%2520geometric%2520moments%252C%2520enabling%2520loss-based%2520gradient%2520guidance%2520during%2520diffusion%2520model%2520sampling.%2520We%2520demonstrate%2520that%2520these%2520losses%2520can%2520constrain%2520individual%2520geometric%2520attributes%2520in%2520a%2520disentangled%2520manner%2520and%2520provide%2520compositional%2520control%2520over%2520multiple%2520substructures.%2520Finally%252C%2520we%2520show%2520that%2520our%2520method%2520is%2520compatible%2520with%2520a%2520wide%2520array%2520of%2520anatomical%2520systems%2520containing%2520non-convex%2520substructures%252C%2520spanning%2520cardiac%252C%2520vascular%252C%2520and%2520skeletal%2520organs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CardioComposer%3A%20Leveraging%20Differentiable%20Geometry%20for%20Compositional%20Control%20of%20Anatomical%20Diffusion%20Models&entry.906535625=Karim%20Kadry%20and%20Shoaib%20Goraya%20and%20Ajay%20Manicka%20and%20Abdalla%20Abdelwahed%20and%20Naravich%20Chutisilp%20and%20Farhad%20Nezami%20and%20Elazer%20Edelman&entry.1292438233=Generative%20models%20of%203D%20cardiovascular%20anatomy%20can%20synthesize%20informative%20structures%20for%20clinical%20research%20and%20medical%20device%20evaluation%2C%20but%20face%20a%20trade-off%20between%20geometric%20controllability%20and%20realism.%20We%20propose%20CardioComposer%3A%20a%20programmable%2C%20inference-time%20framework%20for%20generating%20multi-class%20anatomical%20label%20maps%20based%20on%20interpretable%20ellipsoidal%20primitives.%20These%20primitives%20represent%20geometric%20attributes%20such%20as%20the%20size%2C%20shape%2C%20and%20position%20of%20discrete%20substructures.%20We%20specifically%20develop%20differentiable%20measurement%20functions%20based%20on%20voxel-wise%20geometric%20moments%2C%20enabling%20loss-based%20gradient%20guidance%20during%20diffusion%20model%20sampling.%20We%20demonstrate%20that%20these%20losses%20can%20constrain%20individual%20geometric%20attributes%20in%20a%20disentangled%20manner%20and%20provide%20compositional%20control%20over%20multiple%20substructures.%20Finally%2C%20we%20show%20that%20our%20method%20is%20compatible%20with%20a%20wide%20array%20of%20anatomical%20systems%20containing%20non-convex%20substructures%2C%20spanning%20cardiac%2C%20vascular%2C%20and%20skeletal%20organs.&entry.1838667208=http%3A//arxiv.org/abs/2509.08015v2&entry.124074799=Read"},
{"title": "BRIC: Bridging Kinematic Plans and Physical Control at Test Time", "author": "Dohun Lim and Minji Kim and Jaewoon Lim and Sungchan Kim", "abstract": "We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.", "link": "http://arxiv.org/abs/2511.20431v1", "date": "2025-11-25", "relevancy": 2.2806, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5846}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRIC%3A%20Bridging%20Kinematic%20Plans%20and%20Physical%20Control%20at%20Test%20Time&body=Title%3A%20BRIC%3A%20Bridging%20Kinematic%20Plans%20and%20Physical%20Control%20at%20Test%20Time%0AAuthor%3A%20Dohun%20Lim%20and%20Minji%20Kim%20and%20Jaewoon%20Lim%20and%20Sungchan%20Kim%0AAbstract%3A%20We%20propose%20BRIC%2C%20a%20novel%20test-time%20adaptation%20%28TTA%29%20framework%20that%20enables%20long-term%20human%20motion%20generation%20by%20resolving%20execution%20discrepancies%20between%20diffusion-based%20kinematic%20motion%20planners%20and%20reinforcement%20learning-based%20physics%20controllers.%20While%20diffusion%20models%20can%20generate%20diverse%20and%20expressive%20motions%20conditioned%20on%20text%20and%20scene%20context%2C%20they%20often%20produce%20physically%20implausible%20outputs%2C%20leading%20to%20execution%20drift%20during%20simulation.%20To%20address%20this%2C%20BRIC%20dynamically%20adapts%20the%20physics%20controller%20to%20noisy%20motion%20plans%20at%20test%20time%2C%20while%20preserving%20pre-trained%20skills%20via%20a%20loss%20function%20that%20mitigates%20catastrophic%20forgetting.%20In%20addition%2C%20BRIC%20introduces%20a%20lightweight%20test-time%20guidance%20mechanism%20that%20steers%20the%20diffusion%20model%20in%20the%20signal%20space%20without%20updating%20its%20parameters.%20By%20combining%20both%20adaptation%20strategies%2C%20BRIC%20ensures%20consistent%20and%20physically%20plausible%20long-term%20executions%20across%20diverse%20environments%20in%20an%20effective%20and%20efficient%20manner.%20We%20validate%20the%20effectiveness%20of%20BRIC%20on%20a%20variety%20of%20long-term%20tasks%2C%20including%20motion%20composition%2C%20obstacle%20avoidance%2C%20and%20human-scene%20interaction%2C%20achieving%20state-of-the-art%20performance%20across%20all%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRIC%253A%2520Bridging%2520Kinematic%2520Plans%2520and%2520Physical%2520Control%2520at%2520Test%2520Time%26entry.906535625%3DDohun%2520Lim%2520and%2520Minji%2520Kim%2520and%2520Jaewoon%2520Lim%2520and%2520Sungchan%2520Kim%26entry.1292438233%3DWe%2520propose%2520BRIC%252C%2520a%2520novel%2520test-time%2520adaptation%2520%2528TTA%2529%2520framework%2520that%2520enables%2520long-term%2520human%2520motion%2520generation%2520by%2520resolving%2520execution%2520discrepancies%2520between%2520diffusion-based%2520kinematic%2520motion%2520planners%2520and%2520reinforcement%2520learning-based%2520physics%2520controllers.%2520While%2520diffusion%2520models%2520can%2520generate%2520diverse%2520and%2520expressive%2520motions%2520conditioned%2520on%2520text%2520and%2520scene%2520context%252C%2520they%2520often%2520produce%2520physically%2520implausible%2520outputs%252C%2520leading%2520to%2520execution%2520drift%2520during%2520simulation.%2520To%2520address%2520this%252C%2520BRIC%2520dynamically%2520adapts%2520the%2520physics%2520controller%2520to%2520noisy%2520motion%2520plans%2520at%2520test%2520time%252C%2520while%2520preserving%2520pre-trained%2520skills%2520via%2520a%2520loss%2520function%2520that%2520mitigates%2520catastrophic%2520forgetting.%2520In%2520addition%252C%2520BRIC%2520introduces%2520a%2520lightweight%2520test-time%2520guidance%2520mechanism%2520that%2520steers%2520the%2520diffusion%2520model%2520in%2520the%2520signal%2520space%2520without%2520updating%2520its%2520parameters.%2520By%2520combining%2520both%2520adaptation%2520strategies%252C%2520BRIC%2520ensures%2520consistent%2520and%2520physically%2520plausible%2520long-term%2520executions%2520across%2520diverse%2520environments%2520in%2520an%2520effective%2520and%2520efficient%2520manner.%2520We%2520validate%2520the%2520effectiveness%2520of%2520BRIC%2520on%2520a%2520variety%2520of%2520long-term%2520tasks%252C%2520including%2520motion%2520composition%252C%2520obstacle%2520avoidance%252C%2520and%2520human-scene%2520interaction%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520all%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRIC%3A%20Bridging%20Kinematic%20Plans%20and%20Physical%20Control%20at%20Test%20Time&entry.906535625=Dohun%20Lim%20and%20Minji%20Kim%20and%20Jaewoon%20Lim%20and%20Sungchan%20Kim&entry.1292438233=We%20propose%20BRIC%2C%20a%20novel%20test-time%20adaptation%20%28TTA%29%20framework%20that%20enables%20long-term%20human%20motion%20generation%20by%20resolving%20execution%20discrepancies%20between%20diffusion-based%20kinematic%20motion%20planners%20and%20reinforcement%20learning-based%20physics%20controllers.%20While%20diffusion%20models%20can%20generate%20diverse%20and%20expressive%20motions%20conditioned%20on%20text%20and%20scene%20context%2C%20they%20often%20produce%20physically%20implausible%20outputs%2C%20leading%20to%20execution%20drift%20during%20simulation.%20To%20address%20this%2C%20BRIC%20dynamically%20adapts%20the%20physics%20controller%20to%20noisy%20motion%20plans%20at%20test%20time%2C%20while%20preserving%20pre-trained%20skills%20via%20a%20loss%20function%20that%20mitigates%20catastrophic%20forgetting.%20In%20addition%2C%20BRIC%20introduces%20a%20lightweight%20test-time%20guidance%20mechanism%20that%20steers%20the%20diffusion%20model%20in%20the%20signal%20space%20without%20updating%20its%20parameters.%20By%20combining%20both%20adaptation%20strategies%2C%20BRIC%20ensures%20consistent%20and%20physically%20plausible%20long-term%20executions%20across%20diverse%20environments%20in%20an%20effective%20and%20efficient%20manner.%20We%20validate%20the%20effectiveness%20of%20BRIC%20on%20a%20variety%20of%20long-term%20tasks%2C%20including%20motion%20composition%2C%20obstacle%20avoidance%2C%20and%20human-scene%20interaction%2C%20achieving%20state-of-the-art%20performance%20across%20all%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.20431v1&entry.124074799=Read"},
{"title": "Safe and Stable Neural Network Dynamical Systems for Robot Motion Planning", "author": "Allen Emmanuel Binny and Mahathi Anand and Hugo T. M. Kussaba and Lingyun Chen and Shreenabh Agrawal and Fares J. Abu-Dakka and Abdalla Swikir", "abstract": "Learning safe and stable robot motions from demonstrations remains a challenge, especially in complex, nonlinear tasks involving dynamic, obstacle-rich environments. In this paper, we propose Safe and Stable Neural Network Dynamical Systems S$^2$-NNDS, a learning-from-demonstration framework that simultaneously learns expressive neural dynamical systems alongside neural Lyapunov stability and barrier safety certificates. Unlike traditional approaches with restrictive polynomial parameterizations, S$^2$-NNDS leverages neural networks to capture complex robot motions providing probabilistic guarantees through split conformal prediction in learned certificates. Experimental results on various 2D and 3D datasets -- including LASA handwriting and demonstrations recorded kinesthetically from the Franka Emika Panda robot -- validate S$^2$-NNDS effectiveness in learning robust, safe, and stable motions from potentially unsafe demonstrations.", "link": "http://arxiv.org/abs/2511.20593v1", "date": "2025-11-25", "relevancy": 1.5708, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5603}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5133}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20and%20Stable%20Neural%20Network%20Dynamical%20Systems%20for%20Robot%20Motion%20Planning&body=Title%3A%20Safe%20and%20Stable%20Neural%20Network%20Dynamical%20Systems%20for%20Robot%20Motion%20Planning%0AAuthor%3A%20Allen%20Emmanuel%20Binny%20and%20Mahathi%20Anand%20and%20Hugo%20T.%20M.%20Kussaba%20and%20Lingyun%20Chen%20and%20Shreenabh%20Agrawal%20and%20Fares%20J.%20Abu-Dakka%20and%20Abdalla%20Swikir%0AAbstract%3A%20Learning%20safe%20and%20stable%20robot%20motions%20from%20demonstrations%20remains%20a%20challenge%2C%20especially%20in%20complex%2C%20nonlinear%20tasks%20involving%20dynamic%2C%20obstacle-rich%20environments.%20In%20this%20paper%2C%20we%20propose%20Safe%20and%20Stable%20Neural%20Network%20Dynamical%20Systems%20S%24%5E2%24-NNDS%2C%20a%20learning-from-demonstration%20framework%20that%20simultaneously%20learns%20expressive%20neural%20dynamical%20systems%20alongside%20neural%20Lyapunov%20stability%20and%20barrier%20safety%20certificates.%20Unlike%20traditional%20approaches%20with%20restrictive%20polynomial%20parameterizations%2C%20S%24%5E2%24-NNDS%20leverages%20neural%20networks%20to%20capture%20complex%20robot%20motions%20providing%20probabilistic%20guarantees%20through%20split%20conformal%20prediction%20in%20learned%20certificates.%20Experimental%20results%20on%20various%202D%20and%203D%20datasets%20--%20including%20LASA%20handwriting%20and%20demonstrations%20recorded%20kinesthetically%20from%20the%20Franka%20Emika%20Panda%20robot%20--%20validate%20S%24%5E2%24-NNDS%20effectiveness%20in%20learning%20robust%2C%20safe%2C%20and%20stable%20motions%20from%20potentially%20unsafe%20demonstrations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520and%2520Stable%2520Neural%2520Network%2520Dynamical%2520Systems%2520for%2520Robot%2520Motion%2520Planning%26entry.906535625%3DAllen%2520Emmanuel%2520Binny%2520and%2520Mahathi%2520Anand%2520and%2520Hugo%2520T.%2520M.%2520Kussaba%2520and%2520Lingyun%2520Chen%2520and%2520Shreenabh%2520Agrawal%2520and%2520Fares%2520J.%2520Abu-Dakka%2520and%2520Abdalla%2520Swikir%26entry.1292438233%3DLearning%2520safe%2520and%2520stable%2520robot%2520motions%2520from%2520demonstrations%2520remains%2520a%2520challenge%252C%2520especially%2520in%2520complex%252C%2520nonlinear%2520tasks%2520involving%2520dynamic%252C%2520obstacle-rich%2520environments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Safe%2520and%2520Stable%2520Neural%2520Network%2520Dynamical%2520Systems%2520S%2524%255E2%2524-NNDS%252C%2520a%2520learning-from-demonstration%2520framework%2520that%2520simultaneously%2520learns%2520expressive%2520neural%2520dynamical%2520systems%2520alongside%2520neural%2520Lyapunov%2520stability%2520and%2520barrier%2520safety%2520certificates.%2520Unlike%2520traditional%2520approaches%2520with%2520restrictive%2520polynomial%2520parameterizations%252C%2520S%2524%255E2%2524-NNDS%2520leverages%2520neural%2520networks%2520to%2520capture%2520complex%2520robot%2520motions%2520providing%2520probabilistic%2520guarantees%2520through%2520split%2520conformal%2520prediction%2520in%2520learned%2520certificates.%2520Experimental%2520results%2520on%2520various%25202D%2520and%25203D%2520datasets%2520--%2520including%2520LASA%2520handwriting%2520and%2520demonstrations%2520recorded%2520kinesthetically%2520from%2520the%2520Franka%2520Emika%2520Panda%2520robot%2520--%2520validate%2520S%2524%255E2%2524-NNDS%2520effectiveness%2520in%2520learning%2520robust%252C%2520safe%252C%2520and%2520stable%2520motions%2520from%2520potentially%2520unsafe%2520demonstrations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20and%20Stable%20Neural%20Network%20Dynamical%20Systems%20for%20Robot%20Motion%20Planning&entry.906535625=Allen%20Emmanuel%20Binny%20and%20Mahathi%20Anand%20and%20Hugo%20T.%20M.%20Kussaba%20and%20Lingyun%20Chen%20and%20Shreenabh%20Agrawal%20and%20Fares%20J.%20Abu-Dakka%20and%20Abdalla%20Swikir&entry.1292438233=Learning%20safe%20and%20stable%20robot%20motions%20from%20demonstrations%20remains%20a%20challenge%2C%20especially%20in%20complex%2C%20nonlinear%20tasks%20involving%20dynamic%2C%20obstacle-rich%20environments.%20In%20this%20paper%2C%20we%20propose%20Safe%20and%20Stable%20Neural%20Network%20Dynamical%20Systems%20S%24%5E2%24-NNDS%2C%20a%20learning-from-demonstration%20framework%20that%20simultaneously%20learns%20expressive%20neural%20dynamical%20systems%20alongside%20neural%20Lyapunov%20stability%20and%20barrier%20safety%20certificates.%20Unlike%20traditional%20approaches%20with%20restrictive%20polynomial%20parameterizations%2C%20S%24%5E2%24-NNDS%20leverages%20neural%20networks%20to%20capture%20complex%20robot%20motions%20providing%20probabilistic%20guarantees%20through%20split%20conformal%20prediction%20in%20learned%20certificates.%20Experimental%20results%20on%20various%202D%20and%203D%20datasets%20--%20including%20LASA%20handwriting%20and%20demonstrations%20recorded%20kinesthetically%20from%20the%20Franka%20Emika%20Panda%20robot%20--%20validate%20S%24%5E2%24-NNDS%20effectiveness%20in%20learning%20robust%2C%20safe%2C%20and%20stable%20motions%20from%20potentially%20unsafe%20demonstrations.&entry.1838667208=http%3A//arxiv.org/abs/2511.20593v1&entry.124074799=Read"},
{"title": "RIS-Assisted Downlink Pinching-Antenna Systems: GNN-Enabled Optimization Approaches", "author": "Changpeng He and Yang Lu and Yanqing Xu and Chong-Yung Chi and Bo Ai and Arumugam Nallanathan", "abstract": "This paper investigates a reconfigurable intelligent surface (RIS)-assisted multi-waveguide pinching-antenna (PA) system (PASS) for multi-user downlink information transmission, motivated by the unknown impact of the integration of emerging PASS and RIS on wireless communications. First, we formulate sum rate (SR) and energy efficiency (EE) maximization problems in a unified framework, subject to constraints on the movable region of PAs, total power budget, and tunable phase of RIS elements. Then, by leveraging a graph-structured topology of the RIS-assisted PASS, a novel three-stage graph neural network (GNN) is proposed, which learns PA positions based on user locations, and RIS phase shifts according to composite channel conditions at the first two stages, respectively, and finally determines beamforming vectors. Specifically, the proposed GNN is achieved through unsupervised training, together with three implementation strategies for its integration with convex optimization, thus offering trade-offs between inference time and solution optimality. Extensive numerical results are provided to validate the effectiveness of the proposed GNN, and to support its unique attributes of viable generalization capability, good performance reliability, and real-time applicability. Moreover, the impact of key parameters on RIS-assisted PASS is illustrated and analyzed.", "link": "http://arxiv.org/abs/2511.20305v1", "date": "2025-11-25", "relevancy": 1.7164, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4178}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIS-Assisted%20Downlink%20Pinching-Antenna%20Systems%3A%20GNN-Enabled%20Optimization%20Approaches&body=Title%3A%20RIS-Assisted%20Downlink%20Pinching-Antenna%20Systems%3A%20GNN-Enabled%20Optimization%20Approaches%0AAuthor%3A%20Changpeng%20He%20and%20Yang%20Lu%20and%20Yanqing%20Xu%20and%20Chong-Yung%20Chi%20and%20Bo%20Ai%20and%20Arumugam%20Nallanathan%0AAbstract%3A%20This%20paper%20investigates%20a%20reconfigurable%20intelligent%20surface%20%28RIS%29-assisted%20multi-waveguide%20pinching-antenna%20%28PA%29%20system%20%28PASS%29%20for%20multi-user%20downlink%20information%20transmission%2C%20motivated%20by%20the%20unknown%20impact%20of%20the%20integration%20of%20emerging%20PASS%20and%20RIS%20on%20wireless%20communications.%20First%2C%20we%20formulate%20sum%20rate%20%28SR%29%20and%20energy%20efficiency%20%28EE%29%20maximization%20problems%20in%20a%20unified%20framework%2C%20subject%20to%20constraints%20on%20the%20movable%20region%20of%20PAs%2C%20total%20power%20budget%2C%20and%20tunable%20phase%20of%20RIS%20elements.%20Then%2C%20by%20leveraging%20a%20graph-structured%20topology%20of%20the%20RIS-assisted%20PASS%2C%20a%20novel%20three-stage%20graph%20neural%20network%20%28GNN%29%20is%20proposed%2C%20which%20learns%20PA%20positions%20based%20on%20user%20locations%2C%20and%20RIS%20phase%20shifts%20according%20to%20composite%20channel%20conditions%20at%20the%20first%20two%20stages%2C%20respectively%2C%20and%20finally%20determines%20beamforming%20vectors.%20Specifically%2C%20the%20proposed%20GNN%20is%20achieved%20through%20unsupervised%20training%2C%20together%20with%20three%20implementation%20strategies%20for%20its%20integration%20with%20convex%20optimization%2C%20thus%20offering%20trade-offs%20between%20inference%20time%20and%20solution%20optimality.%20Extensive%20numerical%20results%20are%20provided%20to%20validate%20the%20effectiveness%20of%20the%20proposed%20GNN%2C%20and%20to%20support%20its%20unique%20attributes%20of%20viable%20generalization%20capability%2C%20good%20performance%20reliability%2C%20and%20real-time%20applicability.%20Moreover%2C%20the%20impact%20of%20key%20parameters%20on%20RIS-assisted%20PASS%20is%20illustrated%20and%20analyzed.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIS-Assisted%2520Downlink%2520Pinching-Antenna%2520Systems%253A%2520GNN-Enabled%2520Optimization%2520Approaches%26entry.906535625%3DChangpeng%2520He%2520and%2520Yang%2520Lu%2520and%2520Yanqing%2520Xu%2520and%2520Chong-Yung%2520Chi%2520and%2520Bo%2520Ai%2520and%2520Arumugam%2520Nallanathan%26entry.1292438233%3DThis%2520paper%2520investigates%2520a%2520reconfigurable%2520intelligent%2520surface%2520%2528RIS%2529-assisted%2520multi-waveguide%2520pinching-antenna%2520%2528PA%2529%2520system%2520%2528PASS%2529%2520for%2520multi-user%2520downlink%2520information%2520transmission%252C%2520motivated%2520by%2520the%2520unknown%2520impact%2520of%2520the%2520integration%2520of%2520emerging%2520PASS%2520and%2520RIS%2520on%2520wireless%2520communications.%2520First%252C%2520we%2520formulate%2520sum%2520rate%2520%2528SR%2529%2520and%2520energy%2520efficiency%2520%2528EE%2529%2520maximization%2520problems%2520in%2520a%2520unified%2520framework%252C%2520subject%2520to%2520constraints%2520on%2520the%2520movable%2520region%2520of%2520PAs%252C%2520total%2520power%2520budget%252C%2520and%2520tunable%2520phase%2520of%2520RIS%2520elements.%2520Then%252C%2520by%2520leveraging%2520a%2520graph-structured%2520topology%2520of%2520the%2520RIS-assisted%2520PASS%252C%2520a%2520novel%2520three-stage%2520graph%2520neural%2520network%2520%2528GNN%2529%2520is%2520proposed%252C%2520which%2520learns%2520PA%2520positions%2520based%2520on%2520user%2520locations%252C%2520and%2520RIS%2520phase%2520shifts%2520according%2520to%2520composite%2520channel%2520conditions%2520at%2520the%2520first%2520two%2520stages%252C%2520respectively%252C%2520and%2520finally%2520determines%2520beamforming%2520vectors.%2520Specifically%252C%2520the%2520proposed%2520GNN%2520is%2520achieved%2520through%2520unsupervised%2520training%252C%2520together%2520with%2520three%2520implementation%2520strategies%2520for%2520its%2520integration%2520with%2520convex%2520optimization%252C%2520thus%2520offering%2520trade-offs%2520between%2520inference%2520time%2520and%2520solution%2520optimality.%2520Extensive%2520numerical%2520results%2520are%2520provided%2520to%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520GNN%252C%2520and%2520to%2520support%2520its%2520unique%2520attributes%2520of%2520viable%2520generalization%2520capability%252C%2520good%2520performance%2520reliability%252C%2520and%2520real-time%2520applicability.%2520Moreover%252C%2520the%2520impact%2520of%2520key%2520parameters%2520on%2520RIS-assisted%2520PASS%2520is%2520illustrated%2520and%2520analyzed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIS-Assisted%20Downlink%20Pinching-Antenna%20Systems%3A%20GNN-Enabled%20Optimization%20Approaches&entry.906535625=Changpeng%20He%20and%20Yang%20Lu%20and%20Yanqing%20Xu%20and%20Chong-Yung%20Chi%20and%20Bo%20Ai%20and%20Arumugam%20Nallanathan&entry.1292438233=This%20paper%20investigates%20a%20reconfigurable%20intelligent%20surface%20%28RIS%29-assisted%20multi-waveguide%20pinching-antenna%20%28PA%29%20system%20%28PASS%29%20for%20multi-user%20downlink%20information%20transmission%2C%20motivated%20by%20the%20unknown%20impact%20of%20the%20integration%20of%20emerging%20PASS%20and%20RIS%20on%20wireless%20communications.%20First%2C%20we%20formulate%20sum%20rate%20%28SR%29%20and%20energy%20efficiency%20%28EE%29%20maximization%20problems%20in%20a%20unified%20framework%2C%20subject%20to%20constraints%20on%20the%20movable%20region%20of%20PAs%2C%20total%20power%20budget%2C%20and%20tunable%20phase%20of%20RIS%20elements.%20Then%2C%20by%20leveraging%20a%20graph-structured%20topology%20of%20the%20RIS-assisted%20PASS%2C%20a%20novel%20three-stage%20graph%20neural%20network%20%28GNN%29%20is%20proposed%2C%20which%20learns%20PA%20positions%20based%20on%20user%20locations%2C%20and%20RIS%20phase%20shifts%20according%20to%20composite%20channel%20conditions%20at%20the%20first%20two%20stages%2C%20respectively%2C%20and%20finally%20determines%20beamforming%20vectors.%20Specifically%2C%20the%20proposed%20GNN%20is%20achieved%20through%20unsupervised%20training%2C%20together%20with%20three%20implementation%20strategies%20for%20its%20integration%20with%20convex%20optimization%2C%20thus%20offering%20trade-offs%20between%20inference%20time%20and%20solution%20optimality.%20Extensive%20numerical%20results%20are%20provided%20to%20validate%20the%20effectiveness%20of%20the%20proposed%20GNN%2C%20and%20to%20support%20its%20unique%20attributes%20of%20viable%20generalization%20capability%2C%20good%20performance%20reliability%2C%20and%20real-time%20applicability.%20Moreover%2C%20the%20impact%20of%20key%20parameters%20on%20RIS-assisted%20PASS%20is%20illustrated%20and%20analyzed.&entry.1838667208=http%3A//arxiv.org/abs/2511.20305v1&entry.124074799=Read"},
{"title": "Universe of Thoughts: Enabling Creative Reasoning with Large Language Models", "author": "Yuto Suzuki and Farnoush Banaei-Kashani", "abstract": "Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \\textit{combinational}, \\textit{exploratory}, and \\textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \\textit{Universe of Thoughts} (or \\textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.", "link": "http://arxiv.org/abs/2511.20471v1", "date": "2025-11-25", "relevancy": 2.1782, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universe%20of%20Thoughts%3A%20Enabling%20Creative%20Reasoning%20with%20Large%20Language%20Models&body=Title%3A%20Universe%20of%20Thoughts%3A%20Enabling%20Creative%20Reasoning%20with%20Large%20Language%20Models%0AAuthor%3A%20Yuto%20Suzuki%20and%20Farnoush%20Banaei-Kashani%0AAbstract%3A%20Reasoning%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20has%20garnered%20increasing%20attention%20due%20to%20outstanding%20performance%20of%20these%20models%20in%20mathematical%20and%20complex%20logical%20tasks.%20Beginning%20with%20the%20Chain-of-Thought%20%28CoT%29%20prompting%20technique%2C%20numerous%20reasoning%20methods%20have%20emerged%20that%20decompose%20problems%20into%20smaller%2C%20sequential%20steps%20%28or%20thoughts%29.%20However%2C%20existing%20reasoning%20models%20focus%20on%20conventional%20problem-solving%20and%20do%20not%20necessarily%20generate%20creative%20solutions%20by%20%60%60creative%20reasoning%27%27.%20In%20domains%20where%20the%20solution%20space%20is%20expansive%20and%20conventional%20solutions%20are%20suboptimal%2C%20such%20as%20drug%20discovery%20or%20business%20strategization%2C%20creative%20reasoning%20to%20discover%20innovative%20solutions%20is%20crucial.%20To%20address%20this%20gap%2C%20first%20we%20introduce%20a%20computational%20framework%20for%20creative%20reasoning%20inspired%20by%20established%20cognitive%20science%20principles.%20With%20this%20framework%2C%20we%20propose%20three%20core%20creative%20reasoning%20paradigms%2C%20namely%2C%20%5Ctextit%7Bcombinational%7D%2C%20%5Ctextit%7Bexploratory%7D%2C%20and%20%5Ctextit%7Btransformative%7D%20reasoning%2C%20where%20each%20offers%20specific%20directions%20for%20systematic%20exploration%20of%20the%20universe%20of%20thoughts%20to%20generate%20creative%20solutions.%20Next%2C%20to%20materialize%20this%20framework%20using%20LLMs%2C%20we%20introduce%20the%20%5Ctextit%7BUniverse%20of%20Thoughts%7D%20%28or%20%5Ctextit%7BUoT%7D%2C%20for%20short%29%2C%20a%20novel%20set%20of%20methods%20to%20implement%20the%20aforementioned%20three%20creative%20processes.%20Finally%2C%20we%20introduce%20three%20novel%20tasks%20that%20necessitate%20creative%20problem-solving%2C%20along%20with%20an%20evaluation%20benchmark%20to%20assess%20creativity%20from%20three%20orthogonal%20perspectives%3A%20feasibility%20as%20constraint%2C%20and%20utility%20and%20novelty%20as%20metrics.%20With%20a%20comparative%20analysis%20against%20the%20state-of-the-art%20%28SOTA%29%20reasoning%20techniques%20as%20well%20as%20representative%20commercial%20models%20with%20reasoning%20capability%2C%20we%20show%20that%20UoT%20demonstrates%20superior%20performance%20in%20creative%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniverse%2520of%2520Thoughts%253A%2520Enabling%2520Creative%2520Reasoning%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DYuto%2520Suzuki%2520and%2520Farnoush%2520Banaei-Kashani%26entry.1292438233%3DReasoning%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520garnered%2520increasing%2520attention%2520due%2520to%2520outstanding%2520performance%2520of%2520these%2520models%2520in%2520mathematical%2520and%2520complex%2520logical%2520tasks.%2520Beginning%2520with%2520the%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520technique%252C%2520numerous%2520reasoning%2520methods%2520have%2520emerged%2520that%2520decompose%2520problems%2520into%2520smaller%252C%2520sequential%2520steps%2520%2528or%2520thoughts%2529.%2520However%252C%2520existing%2520reasoning%2520models%2520focus%2520on%2520conventional%2520problem-solving%2520and%2520do%2520not%2520necessarily%2520generate%2520creative%2520solutions%2520by%2520%2560%2560creative%2520reasoning%2527%2527.%2520In%2520domains%2520where%2520the%2520solution%2520space%2520is%2520expansive%2520and%2520conventional%2520solutions%2520are%2520suboptimal%252C%2520such%2520as%2520drug%2520discovery%2520or%2520business%2520strategization%252C%2520creative%2520reasoning%2520to%2520discover%2520innovative%2520solutions%2520is%2520crucial.%2520To%2520address%2520this%2520gap%252C%2520first%2520we%2520introduce%2520a%2520computational%2520framework%2520for%2520creative%2520reasoning%2520inspired%2520by%2520established%2520cognitive%2520science%2520principles.%2520With%2520this%2520framework%252C%2520we%2520propose%2520three%2520core%2520creative%2520reasoning%2520paradigms%252C%2520namely%252C%2520%255Ctextit%257Bcombinational%257D%252C%2520%255Ctextit%257Bexploratory%257D%252C%2520and%2520%255Ctextit%257Btransformative%257D%2520reasoning%252C%2520where%2520each%2520offers%2520specific%2520directions%2520for%2520systematic%2520exploration%2520of%2520the%2520universe%2520of%2520thoughts%2520to%2520generate%2520creative%2520solutions.%2520Next%252C%2520to%2520materialize%2520this%2520framework%2520using%2520LLMs%252C%2520we%2520introduce%2520the%2520%255Ctextit%257BUniverse%2520of%2520Thoughts%257D%2520%2528or%2520%255Ctextit%257BUoT%257D%252C%2520for%2520short%2529%252C%2520a%2520novel%2520set%2520of%2520methods%2520to%2520implement%2520the%2520aforementioned%2520three%2520creative%2520processes.%2520Finally%252C%2520we%2520introduce%2520three%2520novel%2520tasks%2520that%2520necessitate%2520creative%2520problem-solving%252C%2520along%2520with%2520an%2520evaluation%2520benchmark%2520to%2520assess%2520creativity%2520from%2520three%2520orthogonal%2520perspectives%253A%2520feasibility%2520as%2520constraint%252C%2520and%2520utility%2520and%2520novelty%2520as%2520metrics.%2520With%2520a%2520comparative%2520analysis%2520against%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520reasoning%2520techniques%2520as%2520well%2520as%2520representative%2520commercial%2520models%2520with%2520reasoning%2520capability%252C%2520we%2520show%2520that%2520UoT%2520demonstrates%2520superior%2520performance%2520in%2520creative%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universe%20of%20Thoughts%3A%20Enabling%20Creative%20Reasoning%20with%20Large%20Language%20Models&entry.906535625=Yuto%20Suzuki%20and%20Farnoush%20Banaei-Kashani&entry.1292438233=Reasoning%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20has%20garnered%20increasing%20attention%20due%20to%20outstanding%20performance%20of%20these%20models%20in%20mathematical%20and%20complex%20logical%20tasks.%20Beginning%20with%20the%20Chain-of-Thought%20%28CoT%29%20prompting%20technique%2C%20numerous%20reasoning%20methods%20have%20emerged%20that%20decompose%20problems%20into%20smaller%2C%20sequential%20steps%20%28or%20thoughts%29.%20However%2C%20existing%20reasoning%20models%20focus%20on%20conventional%20problem-solving%20and%20do%20not%20necessarily%20generate%20creative%20solutions%20by%20%60%60creative%20reasoning%27%27.%20In%20domains%20where%20the%20solution%20space%20is%20expansive%20and%20conventional%20solutions%20are%20suboptimal%2C%20such%20as%20drug%20discovery%20or%20business%20strategization%2C%20creative%20reasoning%20to%20discover%20innovative%20solutions%20is%20crucial.%20To%20address%20this%20gap%2C%20first%20we%20introduce%20a%20computational%20framework%20for%20creative%20reasoning%20inspired%20by%20established%20cognitive%20science%20principles.%20With%20this%20framework%2C%20we%20propose%20three%20core%20creative%20reasoning%20paradigms%2C%20namely%2C%20%5Ctextit%7Bcombinational%7D%2C%20%5Ctextit%7Bexploratory%7D%2C%20and%20%5Ctextit%7Btransformative%7D%20reasoning%2C%20where%20each%20offers%20specific%20directions%20for%20systematic%20exploration%20of%20the%20universe%20of%20thoughts%20to%20generate%20creative%20solutions.%20Next%2C%20to%20materialize%20this%20framework%20using%20LLMs%2C%20we%20introduce%20the%20%5Ctextit%7BUniverse%20of%20Thoughts%7D%20%28or%20%5Ctextit%7BUoT%7D%2C%20for%20short%29%2C%20a%20novel%20set%20of%20methods%20to%20implement%20the%20aforementioned%20three%20creative%20processes.%20Finally%2C%20we%20introduce%20three%20novel%20tasks%20that%20necessitate%20creative%20problem-solving%2C%20along%20with%20an%20evaluation%20benchmark%20to%20assess%20creativity%20from%20three%20orthogonal%20perspectives%3A%20feasibility%20as%20constraint%2C%20and%20utility%20and%20novelty%20as%20metrics.%20With%20a%20comparative%20analysis%20against%20the%20state-of-the-art%20%28SOTA%29%20reasoning%20techniques%20as%20well%20as%20representative%20commercial%20models%20with%20reasoning%20capability%2C%20we%20show%20that%20UoT%20demonstrates%20superior%20performance%20in%20creative%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2511.20471v1&entry.124074799=Read"},
{"title": "On Feasible Rewards in Multi-Agent Inverse Reinforcement Learning", "author": "Till Freihaut and Giorgia Ramponi", "abstract": "Multi-agent Inverse Reinforcement Learning (MAIRL) aims to recover agent reward functions from expert demonstrations. We characterize the feasible reward set in Markov games, identifying all reward functions that rationalize a given equilibrium. However, equilibrium-based observations are often ambiguous: a single Nash equilibrium can correspond to many reward structures, potentially changing the game's nature in multi-agent systems. We address this by introducing entropy-regularized Markov games, which yield a unique equilibrium while preserving strategic incentives. For this setting, we provide a sample complexity analysis detailing how errors affect learned policy performance. Our work establishes theoretical foundations and practical insights for MAIRL.", "link": "http://arxiv.org/abs/2411.15046v4", "date": "2025-11-25", "relevancy": 1.81, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Feasible%20Rewards%20in%20Multi-Agent%20Inverse%20Reinforcement%20Learning&body=Title%3A%20On%20Feasible%20Rewards%20in%20Multi-Agent%20Inverse%20Reinforcement%20Learning%0AAuthor%3A%20Till%20Freihaut%20and%20Giorgia%20Ramponi%0AAbstract%3A%20Multi-agent%20Inverse%20Reinforcement%20Learning%20%28MAIRL%29%20aims%20to%20recover%20agent%20reward%20functions%20from%20expert%20demonstrations.%20We%20characterize%20the%20feasible%20reward%20set%20in%20Markov%20games%2C%20identifying%20all%20reward%20functions%20that%20rationalize%20a%20given%20equilibrium.%20However%2C%20equilibrium-based%20observations%20are%20often%20ambiguous%3A%20a%20single%20Nash%20equilibrium%20can%20correspond%20to%20many%20reward%20structures%2C%20potentially%20changing%20the%20game%27s%20nature%20in%20multi-agent%20systems.%20We%20address%20this%20by%20introducing%20entropy-regularized%20Markov%20games%2C%20which%20yield%20a%20unique%20equilibrium%20while%20preserving%20strategic%20incentives.%20For%20this%20setting%2C%20we%20provide%20a%20sample%20complexity%20analysis%20detailing%20how%20errors%20affect%20learned%20policy%20performance.%20Our%20work%20establishes%20theoretical%20foundations%20and%20practical%20insights%20for%20MAIRL.%0ALink%3A%20http%3A//arxiv.org/abs/2411.15046v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Feasible%2520Rewards%2520in%2520Multi-Agent%2520Inverse%2520Reinforcement%2520Learning%26entry.906535625%3DTill%2520Freihaut%2520and%2520Giorgia%2520Ramponi%26entry.1292438233%3DMulti-agent%2520Inverse%2520Reinforcement%2520Learning%2520%2528MAIRL%2529%2520aims%2520to%2520recover%2520agent%2520reward%2520functions%2520from%2520expert%2520demonstrations.%2520We%2520characterize%2520the%2520feasible%2520reward%2520set%2520in%2520Markov%2520games%252C%2520identifying%2520all%2520reward%2520functions%2520that%2520rationalize%2520a%2520given%2520equilibrium.%2520However%252C%2520equilibrium-based%2520observations%2520are%2520often%2520ambiguous%253A%2520a%2520single%2520Nash%2520equilibrium%2520can%2520correspond%2520to%2520many%2520reward%2520structures%252C%2520potentially%2520changing%2520the%2520game%2527s%2520nature%2520in%2520multi-agent%2520systems.%2520We%2520address%2520this%2520by%2520introducing%2520entropy-regularized%2520Markov%2520games%252C%2520which%2520yield%2520a%2520unique%2520equilibrium%2520while%2520preserving%2520strategic%2520incentives.%2520For%2520this%2520setting%252C%2520we%2520provide%2520a%2520sample%2520complexity%2520analysis%2520detailing%2520how%2520errors%2520affect%2520learned%2520policy%2520performance.%2520Our%2520work%2520establishes%2520theoretical%2520foundations%2520and%2520practical%2520insights%2520for%2520MAIRL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15046v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Feasible%20Rewards%20in%20Multi-Agent%20Inverse%20Reinforcement%20Learning&entry.906535625=Till%20Freihaut%20and%20Giorgia%20Ramponi&entry.1292438233=Multi-agent%20Inverse%20Reinforcement%20Learning%20%28MAIRL%29%20aims%20to%20recover%20agent%20reward%20functions%20from%20expert%20demonstrations.%20We%20characterize%20the%20feasible%20reward%20set%20in%20Markov%20games%2C%20identifying%20all%20reward%20functions%20that%20rationalize%20a%20given%20equilibrium.%20However%2C%20equilibrium-based%20observations%20are%20often%20ambiguous%3A%20a%20single%20Nash%20equilibrium%20can%20correspond%20to%20many%20reward%20structures%2C%20potentially%20changing%20the%20game%27s%20nature%20in%20multi-agent%20systems.%20We%20address%20this%20by%20introducing%20entropy-regularized%20Markov%20games%2C%20which%20yield%20a%20unique%20equilibrium%20while%20preserving%20strategic%20incentives.%20For%20this%20setting%2C%20we%20provide%20a%20sample%20complexity%20analysis%20detailing%20how%20errors%20affect%20learned%20policy%20performance.%20Our%20work%20establishes%20theoretical%20foundations%20and%20practical%20insights%20for%20MAIRL.&entry.1838667208=http%3A//arxiv.org/abs/2411.15046v4&entry.124074799=Read"},
{"title": "Improving Language Agents through BREW", "author": "Shashank Kirtania and Param Biyani and Priyanshu Gupta and Yasharth Bajpai and Roshni Iyer and Sumit Gulwani and Gustavo Soares", "abstract": "Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $\u03c4^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.", "link": "http://arxiv.org/abs/2511.20297v1", "date": "2025-11-25", "relevancy": 1.599, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5427}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5356}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Language%20Agents%20through%20BREW&body=Title%3A%20Improving%20Language%20Agents%20through%20BREW%0AAuthor%3A%20Shashank%20Kirtania%20and%20Param%20Biyani%20and%20Priyanshu%20Gupta%20and%20Yasharth%20Bajpai%20and%20Roshni%20Iyer%20and%20Sumit%20Gulwani%20and%20Gustavo%20Soares%0AAbstract%3A%20Large%20Language%20Model%20%28LLM%29-based%20agents%20are%20increasingly%20applied%20to%20tasks%20requiring%20structured%20reasoning%2C%20tool%20use%2C%20and%20environmental%20adaptation%2C%20such%20as%20data%20manipulation%2C%20multistep%20planning%2C%20and%20computer-use%20automation.%20However%2C%20despite%20their%20versatility%2C%20current%20training%20paradigms%20for%20model%20weight%20optimization%20methods%2C%20like%20PPO%20and%20GRPO%2C%20remain%20relatively%20impractical%20with%20their%20high%20computational%20overhead%20for%20rollout%20convergence.%20In%20addition%2C%20the%20resulting%20agent%20policies%20are%20difficult%20to%20interpret%2C%20adapt%2C%20or%20incrementally%20improve.%20To%20address%20this%2C%20we%20investigate%20creating%20and%20refining%20structured%20memory%20of%20experiential%20learning%20of%20an%20agent%20from%20its%20environment%20as%20an%20alternative%20route%20to%20agent%20optimization.%20We%20introduce%20BREW%20%28Bootstrapping%20expeRientially-learned%20Environmental%20knoWledge%29%2C%20a%20framework%20for%20agent%20optimization%20for%20downstream%20tasks%20via%20KB%20construction%20and%20refinement.%20In%20our%20formulation%2C%20we%20introduce%20an%20effective%20method%20for%20partitioning%20agent%20memory%20for%20more%20efficient%20retrieval%20and%20refinement.%20BREW%20uses%20task%20graders%20and%20behavior%20rubrics%20to%20learn%20insights%20while%20leveraging%20state-space%20search%20for%20ensuring%20robustness%20from%20the%20noise%20and%20non-specificity%20in%20natural%20language.%20Empirical%20results%20on%20real%20world%2C%20domain-grounded%20benchmarks%20--%20OSWorld%2C%20%24%CF%84%5E2%24Bench%2C%20and%20SpreadsheetBench%20--%20show%20BREW%20achieves%20%2410-20%5C%25%24%20improvement%20in%20task%20precision%2C%20%2410-15%5C%25%24%20reduction%20in%20API/tool%20calls%20leading%20to%20faster%20execution%20time%2C%20all%20while%20maintaining%20computational%20efficiency%20on%20par%20with%20base%20models.%20Unlike%20prior%20work%20where%20memory%20is%20treated%20as%20static%20context%2C%20we%20establish%20the%20KB%20as%20a%20modular%20and%20controllable%20substrate%20for%20agent%20optimization%20--%20an%20explicit%20lever%20for%20shaping%20behavior%20in%20a%20transparent%2C%20interpretable%2C%20and%20extensible%20manner.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Language%2520Agents%2520through%2520BREW%26entry.906535625%3DShashank%2520Kirtania%2520and%2520Param%2520Biyani%2520and%2520Priyanshu%2520Gupta%2520and%2520Yasharth%2520Bajpai%2520and%2520Roshni%2520Iyer%2520and%2520Sumit%2520Gulwani%2520and%2520Gustavo%2520Soares%26entry.1292438233%3DLarge%2520Language%2520Model%2520%2528LLM%2529-based%2520agents%2520are%2520increasingly%2520applied%2520to%2520tasks%2520requiring%2520structured%2520reasoning%252C%2520tool%2520use%252C%2520and%2520environmental%2520adaptation%252C%2520such%2520as%2520data%2520manipulation%252C%2520multistep%2520planning%252C%2520and%2520computer-use%2520automation.%2520However%252C%2520despite%2520their%2520versatility%252C%2520current%2520training%2520paradigms%2520for%2520model%2520weight%2520optimization%2520methods%252C%2520like%2520PPO%2520and%2520GRPO%252C%2520remain%2520relatively%2520impractical%2520with%2520their%2520high%2520computational%2520overhead%2520for%2520rollout%2520convergence.%2520In%2520addition%252C%2520the%2520resulting%2520agent%2520policies%2520are%2520difficult%2520to%2520interpret%252C%2520adapt%252C%2520or%2520incrementally%2520improve.%2520To%2520address%2520this%252C%2520we%2520investigate%2520creating%2520and%2520refining%2520structured%2520memory%2520of%2520experiential%2520learning%2520of%2520an%2520agent%2520from%2520its%2520environment%2520as%2520an%2520alternative%2520route%2520to%2520agent%2520optimization.%2520We%2520introduce%2520BREW%2520%2528Bootstrapping%2520expeRientially-learned%2520Environmental%2520knoWledge%2529%252C%2520a%2520framework%2520for%2520agent%2520optimization%2520for%2520downstream%2520tasks%2520via%2520KB%2520construction%2520and%2520refinement.%2520In%2520our%2520formulation%252C%2520we%2520introduce%2520an%2520effective%2520method%2520for%2520partitioning%2520agent%2520memory%2520for%2520more%2520efficient%2520retrieval%2520and%2520refinement.%2520BREW%2520uses%2520task%2520graders%2520and%2520behavior%2520rubrics%2520to%2520learn%2520insights%2520while%2520leveraging%2520state-space%2520search%2520for%2520ensuring%2520robustness%2520from%2520the%2520noise%2520and%2520non-specificity%2520in%2520natural%2520language.%2520Empirical%2520results%2520on%2520real%2520world%252C%2520domain-grounded%2520benchmarks%2520--%2520OSWorld%252C%2520%2524%25CF%2584%255E2%2524Bench%252C%2520and%2520SpreadsheetBench%2520--%2520show%2520BREW%2520achieves%2520%252410-20%255C%2525%2524%2520improvement%2520in%2520task%2520precision%252C%2520%252410-15%255C%2525%2524%2520reduction%2520in%2520API/tool%2520calls%2520leading%2520to%2520faster%2520execution%2520time%252C%2520all%2520while%2520maintaining%2520computational%2520efficiency%2520on%2520par%2520with%2520base%2520models.%2520Unlike%2520prior%2520work%2520where%2520memory%2520is%2520treated%2520as%2520static%2520context%252C%2520we%2520establish%2520the%2520KB%2520as%2520a%2520modular%2520and%2520controllable%2520substrate%2520for%2520agent%2520optimization%2520--%2520an%2520explicit%2520lever%2520for%2520shaping%2520behavior%2520in%2520a%2520transparent%252C%2520interpretable%252C%2520and%2520extensible%2520manner.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Language%20Agents%20through%20BREW&entry.906535625=Shashank%20Kirtania%20and%20Param%20Biyani%20and%20Priyanshu%20Gupta%20and%20Yasharth%20Bajpai%20and%20Roshni%20Iyer%20and%20Sumit%20Gulwani%20and%20Gustavo%20Soares&entry.1292438233=Large%20Language%20Model%20%28LLM%29-based%20agents%20are%20increasingly%20applied%20to%20tasks%20requiring%20structured%20reasoning%2C%20tool%20use%2C%20and%20environmental%20adaptation%2C%20such%20as%20data%20manipulation%2C%20multistep%20planning%2C%20and%20computer-use%20automation.%20However%2C%20despite%20their%20versatility%2C%20current%20training%20paradigms%20for%20model%20weight%20optimization%20methods%2C%20like%20PPO%20and%20GRPO%2C%20remain%20relatively%20impractical%20with%20their%20high%20computational%20overhead%20for%20rollout%20convergence.%20In%20addition%2C%20the%20resulting%20agent%20policies%20are%20difficult%20to%20interpret%2C%20adapt%2C%20or%20incrementally%20improve.%20To%20address%20this%2C%20we%20investigate%20creating%20and%20refining%20structured%20memory%20of%20experiential%20learning%20of%20an%20agent%20from%20its%20environment%20as%20an%20alternative%20route%20to%20agent%20optimization.%20We%20introduce%20BREW%20%28Bootstrapping%20expeRientially-learned%20Environmental%20knoWledge%29%2C%20a%20framework%20for%20agent%20optimization%20for%20downstream%20tasks%20via%20KB%20construction%20and%20refinement.%20In%20our%20formulation%2C%20we%20introduce%20an%20effective%20method%20for%20partitioning%20agent%20memory%20for%20more%20efficient%20retrieval%20and%20refinement.%20BREW%20uses%20task%20graders%20and%20behavior%20rubrics%20to%20learn%20insights%20while%20leveraging%20state-space%20search%20for%20ensuring%20robustness%20from%20the%20noise%20and%20non-specificity%20in%20natural%20language.%20Empirical%20results%20on%20real%20world%2C%20domain-grounded%20benchmarks%20--%20OSWorld%2C%20%24%CF%84%5E2%24Bench%2C%20and%20SpreadsheetBench%20--%20show%20BREW%20achieves%20%2410-20%5C%25%24%20improvement%20in%20task%20precision%2C%20%2410-15%5C%25%24%20reduction%20in%20API/tool%20calls%20leading%20to%20faster%20execution%20time%2C%20all%20while%20maintaining%20computational%20efficiency%20on%20par%20with%20base%20models.%20Unlike%20prior%20work%20where%20memory%20is%20treated%20as%20static%20context%2C%20we%20establish%20the%20KB%20as%20a%20modular%20and%20controllable%20substrate%20for%20agent%20optimization%20--%20an%20explicit%20lever%20for%20shaping%20behavior%20in%20a%20transparent%2C%20interpretable%2C%20and%20extensible%20manner.&entry.1838667208=http%3A//arxiv.org/abs/2511.20297v1&entry.124074799=Read"},
{"title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models", "author": "Benji Peng and Keyu Chen and Qian Niu and Ziqian Bi and Ming Liu and Pohsun Feng and Tianyang Wang and Lawrence K. Q. Yan and Yizhu Wen and Yichao Zhang and Caitlyn Heqi Yin and Xinyuan Song", "abstract": "Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.", "link": "http://arxiv.org/abs/2410.15236v3", "date": "2025-11-25", "relevancy": 1.8299, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jailbreaking%20and%20Mitigation%20of%20Vulnerabilities%20in%20Large%20Language%20Models&body=Title%3A%20Jailbreaking%20and%20Mitigation%20of%20Vulnerabilities%20in%20Large%20Language%20Models%0AAuthor%3A%20Benji%20Peng%20and%20Keyu%20Chen%20and%20Qian%20Niu%20and%20Ziqian%20Bi%20and%20Ming%20Liu%20and%20Pohsun%20Feng%20and%20Tianyang%20Wang%20and%20Lawrence%20K.%20Q.%20Yan%20and%20Yizhu%20Wen%20and%20Yichao%20Zhang%20and%20Caitlyn%20Heqi%20Yin%20and%20Xinyuan%20Song%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20transformed%20artificial%20intelligence%20by%20advancing%20natural%20language%20understanding%20and%20generation%2C%20enabling%20applications%20across%20fields%20beyond%20healthcare%2C%20software%20engineering%2C%20and%20conversational%20systems.%20Despite%20these%20advancements%20in%20the%20past%20few%20years%2C%20LLMs%20have%20shown%20considerable%20vulnerabilities%2C%20particularly%20to%20prompt%20injection%20and%20jailbreaking%20attacks.%20This%20review%20analyzes%20the%20state%20of%20research%20on%20these%20vulnerabilities%20and%20presents%20available%20defense%20strategies.%20We%20roughly%20categorize%20attack%20approaches%20into%20prompt-based%2C%20model-based%2C%20multimodal%2C%20and%20multilingual%2C%20covering%20techniques%20such%20as%20adversarial%20prompting%2C%20backdoor%20injections%2C%20and%20cross-modality%20exploits.%20We%20also%20review%20various%20defense%20mechanisms%2C%20including%20prompt%20filtering%2C%20transformation%2C%20alignment%20techniques%2C%20multi-agent%20defenses%2C%20and%20self-regulation%2C%20evaluating%20their%20strengths%20and%20shortcomings.%20We%20also%20discuss%20key%20metrics%20and%20benchmarks%20used%20to%20assess%20LLM%20safety%20and%20robustness%2C%20noting%20challenges%20like%20the%20quantification%20of%20attack%20success%20in%20interactive%20contexts%20and%20biases%20in%20existing%20datasets.%20Identifying%20current%20research%20gaps%2C%20we%20suggest%20future%20directions%20for%20resilient%20alignment%20strategies%2C%20advanced%20defenses%20against%20evolving%20attacks%2C%20automation%20of%20jailbreak%20detection%2C%20and%20consideration%20of%20ethical%20and%20societal%20impacts.%20This%20review%20emphasizes%20the%20need%20for%20continued%20research%20and%20cooperation%20within%20the%20AI%20community%20to%20enhance%20LLM%20security%20and%20ensure%20their%20safe%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2410.15236v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJailbreaking%2520and%2520Mitigation%2520of%2520Vulnerabilities%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DBenji%2520Peng%2520and%2520Keyu%2520Chen%2520and%2520Qian%2520Niu%2520and%2520Ziqian%2520Bi%2520and%2520Ming%2520Liu%2520and%2520Pohsun%2520Feng%2520and%2520Tianyang%2520Wang%2520and%2520Lawrence%2520K.%2520Q.%2520Yan%2520and%2520Yizhu%2520Wen%2520and%2520Yichao%2520Zhang%2520and%2520Caitlyn%2520Heqi%2520Yin%2520and%2520Xinyuan%2520Song%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520transformed%2520artificial%2520intelligence%2520by%2520advancing%2520natural%2520language%2520understanding%2520and%2520generation%252C%2520enabling%2520applications%2520across%2520fields%2520beyond%2520healthcare%252C%2520software%2520engineering%252C%2520and%2520conversational%2520systems.%2520Despite%2520these%2520advancements%2520in%2520the%2520past%2520few%2520years%252C%2520LLMs%2520have%2520shown%2520considerable%2520vulnerabilities%252C%2520particularly%2520to%2520prompt%2520injection%2520and%2520jailbreaking%2520attacks.%2520This%2520review%2520analyzes%2520the%2520state%2520of%2520research%2520on%2520these%2520vulnerabilities%2520and%2520presents%2520available%2520defense%2520strategies.%2520We%2520roughly%2520categorize%2520attack%2520approaches%2520into%2520prompt-based%252C%2520model-based%252C%2520multimodal%252C%2520and%2520multilingual%252C%2520covering%2520techniques%2520such%2520as%2520adversarial%2520prompting%252C%2520backdoor%2520injections%252C%2520and%2520cross-modality%2520exploits.%2520We%2520also%2520review%2520various%2520defense%2520mechanisms%252C%2520including%2520prompt%2520filtering%252C%2520transformation%252C%2520alignment%2520techniques%252C%2520multi-agent%2520defenses%252C%2520and%2520self-regulation%252C%2520evaluating%2520their%2520strengths%2520and%2520shortcomings.%2520We%2520also%2520discuss%2520key%2520metrics%2520and%2520benchmarks%2520used%2520to%2520assess%2520LLM%2520safety%2520and%2520robustness%252C%2520noting%2520challenges%2520like%2520the%2520quantification%2520of%2520attack%2520success%2520in%2520interactive%2520contexts%2520and%2520biases%2520in%2520existing%2520datasets.%2520Identifying%2520current%2520research%2520gaps%252C%2520we%2520suggest%2520future%2520directions%2520for%2520resilient%2520alignment%2520strategies%252C%2520advanced%2520defenses%2520against%2520evolving%2520attacks%252C%2520automation%2520of%2520jailbreak%2520detection%252C%2520and%2520consideration%2520of%2520ethical%2520and%2520societal%2520impacts.%2520This%2520review%2520emphasizes%2520the%2520need%2520for%2520continued%2520research%2520and%2520cooperation%2520within%2520the%2520AI%2520community%2520to%2520enhance%2520LLM%2520security%2520and%2520ensure%2520their%2520safe%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15236v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jailbreaking%20and%20Mitigation%20of%20Vulnerabilities%20in%20Large%20Language%20Models&entry.906535625=Benji%20Peng%20and%20Keyu%20Chen%20and%20Qian%20Niu%20and%20Ziqian%20Bi%20and%20Ming%20Liu%20and%20Pohsun%20Feng%20and%20Tianyang%20Wang%20and%20Lawrence%20K.%20Q.%20Yan%20and%20Yizhu%20Wen%20and%20Yichao%20Zhang%20and%20Caitlyn%20Heqi%20Yin%20and%20Xinyuan%20Song&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20transformed%20artificial%20intelligence%20by%20advancing%20natural%20language%20understanding%20and%20generation%2C%20enabling%20applications%20across%20fields%20beyond%20healthcare%2C%20software%20engineering%2C%20and%20conversational%20systems.%20Despite%20these%20advancements%20in%20the%20past%20few%20years%2C%20LLMs%20have%20shown%20considerable%20vulnerabilities%2C%20particularly%20to%20prompt%20injection%20and%20jailbreaking%20attacks.%20This%20review%20analyzes%20the%20state%20of%20research%20on%20these%20vulnerabilities%20and%20presents%20available%20defense%20strategies.%20We%20roughly%20categorize%20attack%20approaches%20into%20prompt-based%2C%20model-based%2C%20multimodal%2C%20and%20multilingual%2C%20covering%20techniques%20such%20as%20adversarial%20prompting%2C%20backdoor%20injections%2C%20and%20cross-modality%20exploits.%20We%20also%20review%20various%20defense%20mechanisms%2C%20including%20prompt%20filtering%2C%20transformation%2C%20alignment%20techniques%2C%20multi-agent%20defenses%2C%20and%20self-regulation%2C%20evaluating%20their%20strengths%20and%20shortcomings.%20We%20also%20discuss%20key%20metrics%20and%20benchmarks%20used%20to%20assess%20LLM%20safety%20and%20robustness%2C%20noting%20challenges%20like%20the%20quantification%20of%20attack%20success%20in%20interactive%20contexts%20and%20biases%20in%20existing%20datasets.%20Identifying%20current%20research%20gaps%2C%20we%20suggest%20future%20directions%20for%20resilient%20alignment%20strategies%2C%20advanced%20defenses%20against%20evolving%20attacks%2C%20automation%20of%20jailbreak%20detection%2C%20and%20consideration%20of%20ethical%20and%20societal%20impacts.%20This%20review%20emphasizes%20the%20need%20for%20continued%20research%20and%20cooperation%20within%20the%20AI%20community%20to%20enhance%20LLM%20security%20and%20ensure%20their%20safe%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2410.15236v3&entry.124074799=Read"},
{"title": "DiCaP: Distribution-Calibrated Pseudo-labeling for Semi-Supervised Multi-Label Learning", "author": "Bo Han and Zhuoming Li and Xiaoyu Wang and Yaxin Hou and Hui Liu and Junhui Hou and Yuheng Jia", "abstract": "Semi-supervised multi-label learning (SSMLL) aims to address the challenge of limited labeled data in multi-label learning (MLL) by leveraging unlabeled data to improve the model's performance. While pseudo-labeling has become a dominant strategy in SSMLL, most existing methods assign equal weights to all pseudo-labels regardless of their quality, which can amplify the impact of noisy or uncertain predictions and degrade the overall performance. In this paper, we theoretically verify that the optimal weight for a pseudo-label should reflect its correctness likelihood. Empirically, we observe that on the same dataset, the correctness likelihood distribution of unlabeled data remains stable, even as the number of labeled training samples varies. Building on this insight, we propose Distribution-Calibrated Pseudo-labeling (DiCaP), a correctness-aware framework that estimates posterior precision to calibrate pseudo-label weights. We further introduce a dual-thresholding mechanism to separate confident and ambiguous regions: confident samples are pseudo-labeled and weighted accordingly, while ambiguous ones are explored by unsupervised contrastive learning. Experiments conducted on multiple benchmark datasets verify that our method achieves consistent improvements, surpassing state-of-the-art methods by up to 4.27%.", "link": "http://arxiv.org/abs/2511.20225v1", "date": "2025-11-25", "relevancy": 1.5215, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5575}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiCaP%3A%20Distribution-Calibrated%20Pseudo-labeling%20for%20Semi-Supervised%20Multi-Label%20Learning&body=Title%3A%20DiCaP%3A%20Distribution-Calibrated%20Pseudo-labeling%20for%20Semi-Supervised%20Multi-Label%20Learning%0AAuthor%3A%20Bo%20Han%20and%20Zhuoming%20Li%20and%20Xiaoyu%20Wang%20and%20Yaxin%20Hou%20and%20Hui%20Liu%20and%20Junhui%20Hou%20and%20Yuheng%20Jia%0AAbstract%3A%20Semi-supervised%20multi-label%20learning%20%28SSMLL%29%20aims%20to%20address%20the%20challenge%20of%20limited%20labeled%20data%20in%20multi-label%20learning%20%28MLL%29%20by%20leveraging%20unlabeled%20data%20to%20improve%20the%20model%27s%20performance.%20While%20pseudo-labeling%20has%20become%20a%20dominant%20strategy%20in%20SSMLL%2C%20most%20existing%20methods%20assign%20equal%20weights%20to%20all%20pseudo-labels%20regardless%20of%20their%20quality%2C%20which%20can%20amplify%20the%20impact%20of%20noisy%20or%20uncertain%20predictions%20and%20degrade%20the%20overall%20performance.%20In%20this%20paper%2C%20we%20theoretically%20verify%20that%20the%20optimal%20weight%20for%20a%20pseudo-label%20should%20reflect%20its%20correctness%20likelihood.%20Empirically%2C%20we%20observe%20that%20on%20the%20same%20dataset%2C%20the%20correctness%20likelihood%20distribution%20of%20unlabeled%20data%20remains%20stable%2C%20even%20as%20the%20number%20of%20labeled%20training%20samples%20varies.%20Building%20on%20this%20insight%2C%20we%20propose%20Distribution-Calibrated%20Pseudo-labeling%20%28DiCaP%29%2C%20a%20correctness-aware%20framework%20that%20estimates%20posterior%20precision%20to%20calibrate%20pseudo-label%20weights.%20We%20further%20introduce%20a%20dual-thresholding%20mechanism%20to%20separate%20confident%20and%20ambiguous%20regions%3A%20confident%20samples%20are%20pseudo-labeled%20and%20weighted%20accordingly%2C%20while%20ambiguous%20ones%20are%20explored%20by%20unsupervised%20contrastive%20learning.%20Experiments%20conducted%20on%20multiple%20benchmark%20datasets%20verify%20that%20our%20method%20achieves%20consistent%20improvements%2C%20surpassing%20state-of-the-art%20methods%20by%20up%20to%204.27%25.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiCaP%253A%2520Distribution-Calibrated%2520Pseudo-labeling%2520for%2520Semi-Supervised%2520Multi-Label%2520Learning%26entry.906535625%3DBo%2520Han%2520and%2520Zhuoming%2520Li%2520and%2520Xiaoyu%2520Wang%2520and%2520Yaxin%2520Hou%2520and%2520Hui%2520Liu%2520and%2520Junhui%2520Hou%2520and%2520Yuheng%2520Jia%26entry.1292438233%3DSemi-supervised%2520multi-label%2520learning%2520%2528SSMLL%2529%2520aims%2520to%2520address%2520the%2520challenge%2520of%2520limited%2520labeled%2520data%2520in%2520multi-label%2520learning%2520%2528MLL%2529%2520by%2520leveraging%2520unlabeled%2520data%2520to%2520improve%2520the%2520model%2527s%2520performance.%2520While%2520pseudo-labeling%2520has%2520become%2520a%2520dominant%2520strategy%2520in%2520SSMLL%252C%2520most%2520existing%2520methods%2520assign%2520equal%2520weights%2520to%2520all%2520pseudo-labels%2520regardless%2520of%2520their%2520quality%252C%2520which%2520can%2520amplify%2520the%2520impact%2520of%2520noisy%2520or%2520uncertain%2520predictions%2520and%2520degrade%2520the%2520overall%2520performance.%2520In%2520this%2520paper%252C%2520we%2520theoretically%2520verify%2520that%2520the%2520optimal%2520weight%2520for%2520a%2520pseudo-label%2520should%2520reflect%2520its%2520correctness%2520likelihood.%2520Empirically%252C%2520we%2520observe%2520that%2520on%2520the%2520same%2520dataset%252C%2520the%2520correctness%2520likelihood%2520distribution%2520of%2520unlabeled%2520data%2520remains%2520stable%252C%2520even%2520as%2520the%2520number%2520of%2520labeled%2520training%2520samples%2520varies.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520Distribution-Calibrated%2520Pseudo-labeling%2520%2528DiCaP%2529%252C%2520a%2520correctness-aware%2520framework%2520that%2520estimates%2520posterior%2520precision%2520to%2520calibrate%2520pseudo-label%2520weights.%2520We%2520further%2520introduce%2520a%2520dual-thresholding%2520mechanism%2520to%2520separate%2520confident%2520and%2520ambiguous%2520regions%253A%2520confident%2520samples%2520are%2520pseudo-labeled%2520and%2520weighted%2520accordingly%252C%2520while%2520ambiguous%2520ones%2520are%2520explored%2520by%2520unsupervised%2520contrastive%2520learning.%2520Experiments%2520conducted%2520on%2520multiple%2520benchmark%2520datasets%2520verify%2520that%2520our%2520method%2520achieves%2520consistent%2520improvements%252C%2520surpassing%2520state-of-the-art%2520methods%2520by%2520up%2520to%25204.27%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiCaP%3A%20Distribution-Calibrated%20Pseudo-labeling%20for%20Semi-Supervised%20Multi-Label%20Learning&entry.906535625=Bo%20Han%20and%20Zhuoming%20Li%20and%20Xiaoyu%20Wang%20and%20Yaxin%20Hou%20and%20Hui%20Liu%20and%20Junhui%20Hou%20and%20Yuheng%20Jia&entry.1292438233=Semi-supervised%20multi-label%20learning%20%28SSMLL%29%20aims%20to%20address%20the%20challenge%20of%20limited%20labeled%20data%20in%20multi-label%20learning%20%28MLL%29%20by%20leveraging%20unlabeled%20data%20to%20improve%20the%20model%27s%20performance.%20While%20pseudo-labeling%20has%20become%20a%20dominant%20strategy%20in%20SSMLL%2C%20most%20existing%20methods%20assign%20equal%20weights%20to%20all%20pseudo-labels%20regardless%20of%20their%20quality%2C%20which%20can%20amplify%20the%20impact%20of%20noisy%20or%20uncertain%20predictions%20and%20degrade%20the%20overall%20performance.%20In%20this%20paper%2C%20we%20theoretically%20verify%20that%20the%20optimal%20weight%20for%20a%20pseudo-label%20should%20reflect%20its%20correctness%20likelihood.%20Empirically%2C%20we%20observe%20that%20on%20the%20same%20dataset%2C%20the%20correctness%20likelihood%20distribution%20of%20unlabeled%20data%20remains%20stable%2C%20even%20as%20the%20number%20of%20labeled%20training%20samples%20varies.%20Building%20on%20this%20insight%2C%20we%20propose%20Distribution-Calibrated%20Pseudo-labeling%20%28DiCaP%29%2C%20a%20correctness-aware%20framework%20that%20estimates%20posterior%20precision%20to%20calibrate%20pseudo-label%20weights.%20We%20further%20introduce%20a%20dual-thresholding%20mechanism%20to%20separate%20confident%20and%20ambiguous%20regions%3A%20confident%20samples%20are%20pseudo-labeled%20and%20weighted%20accordingly%2C%20while%20ambiguous%20ones%20are%20explored%20by%20unsupervised%20contrastive%20learning.%20Experiments%20conducted%20on%20multiple%20benchmark%20datasets%20verify%20that%20our%20method%20achieves%20consistent%20improvements%2C%20surpassing%20state-of-the-art%20methods%20by%20up%20to%204.27%25.&entry.1838667208=http%3A//arxiv.org/abs/2511.20225v1&entry.124074799=Read"},
{"title": "Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis", "author": "Weichien Liao", "abstract": "High-throughput imaging workflows, such as Parallel Rapid Imaging with Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional real-time processing capabilities. We present a scalable FPGA-based preprocessing pipeline for real-time denoising, implemented via High-Level Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture performs frame subtraction and averaging directly on streamed image data, minimizing latency through burst-mode AXI4 interfaces. The resulting kernel operates below the inter-frame interval, enabling inline denoising and reducing dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale acquisition, this modular FPGA framework offers a practical solution for latency-sensitive imaging workflows in spectroscopy and microscopy.", "link": "http://arxiv.org/abs/2508.14917v2", "date": "2025-11-25", "relevancy": 2.193, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5817}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5487}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20FPGA%20Framework%20for%20Real-Time%20Denoising%20in%20High-Throughput%20Imaging%3A%20A%20DRAM-Optimized%20Pipeline%20using%20High-Level%20Synthesis&body=Title%3A%20Scalable%20FPGA%20Framework%20for%20Real-Time%20Denoising%20in%20High-Throughput%20Imaging%3A%20A%20DRAM-Optimized%20Pipeline%20using%20High-Level%20Synthesis%0AAuthor%3A%20Weichien%20Liao%0AAbstract%3A%20High-throughput%20imaging%20workflows%2C%20such%20as%20Parallel%20Rapid%20Imaging%20with%20Spectroscopic%20Mapping%20%28PRISM%29%2C%20generate%20data%20at%20rates%20that%20exceed%20conventional%20real-time%20processing%20capabilities.%20We%20present%20a%20scalable%20FPGA-based%20preprocessing%20pipeline%20for%20real-time%20denoising%2C%20implemented%20via%20High-Level%20Synthesis%20%28HLS%29%20and%20optimized%20for%20DRAM-backed%20buffering.%20Our%20architecture%20performs%20frame%20subtraction%20and%20averaging%20directly%20on%20streamed%20image%20data%2C%20minimizing%20latency%20through%20burst-mode%20AXI4%20interfaces.%20The%20resulting%20kernel%20operates%20below%20the%20inter-frame%20interval%2C%20enabling%20inline%20denoising%20and%20reducing%20dataset%20size%20for%20downstream%20CPU/GPU%20analysis.%20Validated%20under%20PRISM-scale%20acquisition%2C%20this%20modular%20FPGA%20framework%20offers%20a%20practical%20solution%20for%20latency-sensitive%20imaging%20workflows%20in%20spectroscopy%20and%20microscopy.%0ALink%3A%20http%3A//arxiv.org/abs/2508.14917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520FPGA%2520Framework%2520for%2520Real-Time%2520Denoising%2520in%2520High-Throughput%2520Imaging%253A%2520A%2520DRAM-Optimized%2520Pipeline%2520using%2520High-Level%2520Synthesis%26entry.906535625%3DWeichien%2520Liao%26entry.1292438233%3DHigh-throughput%2520imaging%2520workflows%252C%2520such%2520as%2520Parallel%2520Rapid%2520Imaging%2520with%2520Spectroscopic%2520Mapping%2520%2528PRISM%2529%252C%2520generate%2520data%2520at%2520rates%2520that%2520exceed%2520conventional%2520real-time%2520processing%2520capabilities.%2520We%2520present%2520a%2520scalable%2520FPGA-based%2520preprocessing%2520pipeline%2520for%2520real-time%2520denoising%252C%2520implemented%2520via%2520High-Level%2520Synthesis%2520%2528HLS%2529%2520and%2520optimized%2520for%2520DRAM-backed%2520buffering.%2520Our%2520architecture%2520performs%2520frame%2520subtraction%2520and%2520averaging%2520directly%2520on%2520streamed%2520image%2520data%252C%2520minimizing%2520latency%2520through%2520burst-mode%2520AXI4%2520interfaces.%2520The%2520resulting%2520kernel%2520operates%2520below%2520the%2520inter-frame%2520interval%252C%2520enabling%2520inline%2520denoising%2520and%2520reducing%2520dataset%2520size%2520for%2520downstream%2520CPU/GPU%2520analysis.%2520Validated%2520under%2520PRISM-scale%2520acquisition%252C%2520this%2520modular%2520FPGA%2520framework%2520offers%2520a%2520practical%2520solution%2520for%2520latency-sensitive%2520imaging%2520workflows%2520in%2520spectroscopy%2520and%2520microscopy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20FPGA%20Framework%20for%20Real-Time%20Denoising%20in%20High-Throughput%20Imaging%3A%20A%20DRAM-Optimized%20Pipeline%20using%20High-Level%20Synthesis&entry.906535625=Weichien%20Liao&entry.1292438233=High-throughput%20imaging%20workflows%2C%20such%20as%20Parallel%20Rapid%20Imaging%20with%20Spectroscopic%20Mapping%20%28PRISM%29%2C%20generate%20data%20at%20rates%20that%20exceed%20conventional%20real-time%20processing%20capabilities.%20We%20present%20a%20scalable%20FPGA-based%20preprocessing%20pipeline%20for%20real-time%20denoising%2C%20implemented%20via%20High-Level%20Synthesis%20%28HLS%29%20and%20optimized%20for%20DRAM-backed%20buffering.%20Our%20architecture%20performs%20frame%20subtraction%20and%20averaging%20directly%20on%20streamed%20image%20data%2C%20minimizing%20latency%20through%20burst-mode%20AXI4%20interfaces.%20The%20resulting%20kernel%20operates%20below%20the%20inter-frame%20interval%2C%20enabling%20inline%20denoising%20and%20reducing%20dataset%20size%20for%20downstream%20CPU/GPU%20analysis.%20Validated%20under%20PRISM-scale%20acquisition%2C%20this%20modular%20FPGA%20framework%20offers%20a%20practical%20solution%20for%20latency-sensitive%20imaging%20workflows%20in%20spectroscopy%20and%20microscopy.&entry.1838667208=http%3A//arxiv.org/abs/2508.14917v2&entry.124074799=Read"},
{"title": "Advancing Image Classification with Discrete Diffusion Classification Modeling", "author": "Omer Belhasin and Shelly Golan and Ran El-Yaniv and Michael Elad", "abstract": "Image classification is a well-studied task in computer vision, and yet it remains challenging under high-uncertainty conditions, such as when input images are corrupted or training data are limited. Conventional classification approaches typically train models to directly predict class labels from input images, but this might lead to suboptimal performance in such scenarios. To address this issue, we propose Discrete Diffusion Classification Modeling (DiDiCM), a novel framework that leverages a diffusion-based procedure to model the posterior distribution of class labels conditioned on the input image. DiDiCM supports diffusion-based predictions either on class probabilities or on discrete class labels, providing flexibility in computation and memory trade-offs. We conduct a comprehensive empirical study demonstrating the superior performance of DiDiCM over standard classifiers, showing that a few diffusion iterations achieve higher classification accuracy on the ImageNet dataset compared to baselines, with accuracy gains increasing as the task becomes more challenging. We release our code at https://github.com/omerb01/didicm .", "link": "http://arxiv.org/abs/2511.20263v1", "date": "2025-11-25", "relevancy": 1.7594, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6521}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5785}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Image%20Classification%20with%20Discrete%20Diffusion%20Classification%20Modeling&body=Title%3A%20Advancing%20Image%20Classification%20with%20Discrete%20Diffusion%20Classification%20Modeling%0AAuthor%3A%20Omer%20Belhasin%20and%20Shelly%20Golan%20and%20Ran%20El-Yaniv%20and%20Michael%20Elad%0AAbstract%3A%20Image%20classification%20is%20a%20well-studied%20task%20in%20computer%20vision%2C%20and%20yet%20it%20remains%20challenging%20under%20high-uncertainty%20conditions%2C%20such%20as%20when%20input%20images%20are%20corrupted%20or%20training%20data%20are%20limited.%20Conventional%20classification%20approaches%20typically%20train%20models%20to%20directly%20predict%20class%20labels%20from%20input%20images%2C%20but%20this%20might%20lead%20to%20suboptimal%20performance%20in%20such%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20Discrete%20Diffusion%20Classification%20Modeling%20%28DiDiCM%29%2C%20a%20novel%20framework%20that%20leverages%20a%20diffusion-based%20procedure%20to%20model%20the%20posterior%20distribution%20of%20class%20labels%20conditioned%20on%20the%20input%20image.%20DiDiCM%20supports%20diffusion-based%20predictions%20either%20on%20class%20probabilities%20or%20on%20discrete%20class%20labels%2C%20providing%20flexibility%20in%20computation%20and%20memory%20trade-offs.%20We%20conduct%20a%20comprehensive%20empirical%20study%20demonstrating%20the%20superior%20performance%20of%20DiDiCM%20over%20standard%20classifiers%2C%20showing%20that%20a%20few%20diffusion%20iterations%20achieve%20higher%20classification%20accuracy%20on%20the%20ImageNet%20dataset%20compared%20to%20baselines%2C%20with%20accuracy%20gains%20increasing%20as%20the%20task%20becomes%20more%20challenging.%20We%20release%20our%20code%20at%20https%3A//github.com/omerb01/didicm%20.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Image%2520Classification%2520with%2520Discrete%2520Diffusion%2520Classification%2520Modeling%26entry.906535625%3DOmer%2520Belhasin%2520and%2520Shelly%2520Golan%2520and%2520Ran%2520El-Yaniv%2520and%2520Michael%2520Elad%26entry.1292438233%3DImage%2520classification%2520is%2520a%2520well-studied%2520task%2520in%2520computer%2520vision%252C%2520and%2520yet%2520it%2520remains%2520challenging%2520under%2520high-uncertainty%2520conditions%252C%2520such%2520as%2520when%2520input%2520images%2520are%2520corrupted%2520or%2520training%2520data%2520are%2520limited.%2520Conventional%2520classification%2520approaches%2520typically%2520train%2520models%2520to%2520directly%2520predict%2520class%2520labels%2520from%2520input%2520images%252C%2520but%2520this%2520might%2520lead%2520to%2520suboptimal%2520performance%2520in%2520such%2520scenarios.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Discrete%2520Diffusion%2520Classification%2520Modeling%2520%2528DiDiCM%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520a%2520diffusion-based%2520procedure%2520to%2520model%2520the%2520posterior%2520distribution%2520of%2520class%2520labels%2520conditioned%2520on%2520the%2520input%2520image.%2520DiDiCM%2520supports%2520diffusion-based%2520predictions%2520either%2520on%2520class%2520probabilities%2520or%2520on%2520discrete%2520class%2520labels%252C%2520providing%2520flexibility%2520in%2520computation%2520and%2520memory%2520trade-offs.%2520We%2520conduct%2520a%2520comprehensive%2520empirical%2520study%2520demonstrating%2520the%2520superior%2520performance%2520of%2520DiDiCM%2520over%2520standard%2520classifiers%252C%2520showing%2520that%2520a%2520few%2520diffusion%2520iterations%2520achieve%2520higher%2520classification%2520accuracy%2520on%2520the%2520ImageNet%2520dataset%2520compared%2520to%2520baselines%252C%2520with%2520accuracy%2520gains%2520increasing%2520as%2520the%2520task%2520becomes%2520more%2520challenging.%2520We%2520release%2520our%2520code%2520at%2520https%253A//github.com/omerb01/didicm%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Image%20Classification%20with%20Discrete%20Diffusion%20Classification%20Modeling&entry.906535625=Omer%20Belhasin%20and%20Shelly%20Golan%20and%20Ran%20El-Yaniv%20and%20Michael%20Elad&entry.1292438233=Image%20classification%20is%20a%20well-studied%20task%20in%20computer%20vision%2C%20and%20yet%20it%20remains%20challenging%20under%20high-uncertainty%20conditions%2C%20such%20as%20when%20input%20images%20are%20corrupted%20or%20training%20data%20are%20limited.%20Conventional%20classification%20approaches%20typically%20train%20models%20to%20directly%20predict%20class%20labels%20from%20input%20images%2C%20but%20this%20might%20lead%20to%20suboptimal%20performance%20in%20such%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20Discrete%20Diffusion%20Classification%20Modeling%20%28DiDiCM%29%2C%20a%20novel%20framework%20that%20leverages%20a%20diffusion-based%20procedure%20to%20model%20the%20posterior%20distribution%20of%20class%20labels%20conditioned%20on%20the%20input%20image.%20DiDiCM%20supports%20diffusion-based%20predictions%20either%20on%20class%20probabilities%20or%20on%20discrete%20class%20labels%2C%20providing%20flexibility%20in%20computation%20and%20memory%20trade-offs.%20We%20conduct%20a%20comprehensive%20empirical%20study%20demonstrating%20the%20superior%20performance%20of%20DiDiCM%20over%20standard%20classifiers%2C%20showing%20that%20a%20few%20diffusion%20iterations%20achieve%20higher%20classification%20accuracy%20on%20the%20ImageNet%20dataset%20compared%20to%20baselines%2C%20with%20accuracy%20gains%20increasing%20as%20the%20task%20becomes%20more%20challenging.%20We%20release%20our%20code%20at%20https%3A//github.com/omerb01/didicm%20.&entry.1838667208=http%3A//arxiv.org/abs/2511.20263v1&entry.124074799=Read"},
{"title": "On the Limits of Momentum in Decentralized and Federated Optimization", "author": "Riccardo Zaccone and Sai Praneeth Karimireddy and Carlo Masone", "abstract": "Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $\u0398\\left(1/t\\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.", "link": "http://arxiv.org/abs/2511.20168v1", "date": "2025-11-25", "relevancy": 1.8005, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4638}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4439}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Limits%20of%20Momentum%20in%20Decentralized%20and%20Federated%20Optimization&body=Title%3A%20On%20the%20Limits%20of%20Momentum%20in%20Decentralized%20and%20Federated%20Optimization%0AAuthor%3A%20Riccardo%20Zaccone%20and%20Sai%20Praneeth%20Karimireddy%20and%20Carlo%20Masone%0AAbstract%3A%20Recent%20works%20have%20explored%20the%20use%20of%20momentum%20in%20local%20methods%20to%20enhance%20distributed%20SGD.%20This%20is%20particularly%20appealing%20in%20Federated%20Learning%20%28FL%29%2C%20where%20momentum%20intuitively%20appears%20as%20a%20solution%20to%20mitigate%20the%20effects%20of%20statistical%20heterogeneity.%20Despite%20recent%20progress%20in%20this%20direction%2C%20it%20is%20still%20unclear%20if%20momentum%20can%20guarantee%20convergence%20under%20unbounded%20heterogeneity%20in%20decentralized%20scenarios%2C%20where%20only%20some%20workers%20participate%20at%20each%20round.%20In%20this%20work%20we%20analyze%20momentum%20under%20cyclic%20client%20participation%2C%20and%20theoretically%20prove%20that%20it%20remains%20inevitably%20affected%20by%20statistical%20heterogeneity.%20Similarly%20to%20SGD%2C%20we%20prove%20that%20decreasing%20step-sizes%20do%20not%20help%20either%3A%20in%20fact%2C%20any%20schedule%20decreasing%20faster%20than%20%24%CE%98%5Cleft%281/t%5Cright%29%24%20leads%20to%20convergence%20to%20a%20constant%20value%20that%20depends%20on%20the%20initialization%20and%20the%20heterogeneity%20bound.%20Numerical%20results%20corroborate%20the%20theory%2C%20and%20deep%20learning%20experiments%20confirm%20its%20relevance%20for%20realistic%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Limits%2520of%2520Momentum%2520in%2520Decentralized%2520and%2520Federated%2520Optimization%26entry.906535625%3DRiccardo%2520Zaccone%2520and%2520Sai%2520Praneeth%2520Karimireddy%2520and%2520Carlo%2520Masone%26entry.1292438233%3DRecent%2520works%2520have%2520explored%2520the%2520use%2520of%2520momentum%2520in%2520local%2520methods%2520to%2520enhance%2520distributed%2520SGD.%2520This%2520is%2520particularly%2520appealing%2520in%2520Federated%2520Learning%2520%2528FL%2529%252C%2520where%2520momentum%2520intuitively%2520appears%2520as%2520a%2520solution%2520to%2520mitigate%2520the%2520effects%2520of%2520statistical%2520heterogeneity.%2520Despite%2520recent%2520progress%2520in%2520this%2520direction%252C%2520it%2520is%2520still%2520unclear%2520if%2520momentum%2520can%2520guarantee%2520convergence%2520under%2520unbounded%2520heterogeneity%2520in%2520decentralized%2520scenarios%252C%2520where%2520only%2520some%2520workers%2520participate%2520at%2520each%2520round.%2520In%2520this%2520work%2520we%2520analyze%2520momentum%2520under%2520cyclic%2520client%2520participation%252C%2520and%2520theoretically%2520prove%2520that%2520it%2520remains%2520inevitably%2520affected%2520by%2520statistical%2520heterogeneity.%2520Similarly%2520to%2520SGD%252C%2520we%2520prove%2520that%2520decreasing%2520step-sizes%2520do%2520not%2520help%2520either%253A%2520in%2520fact%252C%2520any%2520schedule%2520decreasing%2520faster%2520than%2520%2524%25CE%2598%255Cleft%25281/t%255Cright%2529%2524%2520leads%2520to%2520convergence%2520to%2520a%2520constant%2520value%2520that%2520depends%2520on%2520the%2520initialization%2520and%2520the%2520heterogeneity%2520bound.%2520Numerical%2520results%2520corroborate%2520the%2520theory%252C%2520and%2520deep%2520learning%2520experiments%2520confirm%2520its%2520relevance%2520for%2520realistic%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Limits%20of%20Momentum%20in%20Decentralized%20and%20Federated%20Optimization&entry.906535625=Riccardo%20Zaccone%20and%20Sai%20Praneeth%20Karimireddy%20and%20Carlo%20Masone&entry.1292438233=Recent%20works%20have%20explored%20the%20use%20of%20momentum%20in%20local%20methods%20to%20enhance%20distributed%20SGD.%20This%20is%20particularly%20appealing%20in%20Federated%20Learning%20%28FL%29%2C%20where%20momentum%20intuitively%20appears%20as%20a%20solution%20to%20mitigate%20the%20effects%20of%20statistical%20heterogeneity.%20Despite%20recent%20progress%20in%20this%20direction%2C%20it%20is%20still%20unclear%20if%20momentum%20can%20guarantee%20convergence%20under%20unbounded%20heterogeneity%20in%20decentralized%20scenarios%2C%20where%20only%20some%20workers%20participate%20at%20each%20round.%20In%20this%20work%20we%20analyze%20momentum%20under%20cyclic%20client%20participation%2C%20and%20theoretically%20prove%20that%20it%20remains%20inevitably%20affected%20by%20statistical%20heterogeneity.%20Similarly%20to%20SGD%2C%20we%20prove%20that%20decreasing%20step-sizes%20do%20not%20help%20either%3A%20in%20fact%2C%20any%20schedule%20decreasing%20faster%20than%20%24%CE%98%5Cleft%281/t%5Cright%29%24%20leads%20to%20convergence%20to%20a%20constant%20value%20that%20depends%20on%20the%20initialization%20and%20the%20heterogeneity%20bound.%20Numerical%20results%20corroborate%20the%20theory%2C%20and%20deep%20learning%20experiments%20confirm%20its%20relevance%20for%20realistic%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2511.20168v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


