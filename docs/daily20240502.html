<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240501.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "author": "Zhexi Peng and Tianjia Shao and Yong Liu and Jingke Zhou and Yin Yang and Jingdong Wang and Kun Zhou", "abstract": "  We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction\nsystem with an RGBD camera for large-scale environments using Gaussian\nsplatting. The system features a compact Gaussian representation and a highly\nefficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be\neither opaque or nearly transparent, with the opaque ones fitting the surface\nand dominant colors, and transparent ones fitting residual colors. By rendering\ndepth in a different way from color rendering, we let a single opaque Gaussian\nwell fit a local surface region without the need of multiple overlapping\nGaussians, hence largely reducing the memory and computation cost. For\non-the-fly Gaussian optimization, we explicitly add Gaussians for three types\nof pixels per frame: newly observed, with large color errors, and with large\ndepth errors. We also categorize all Gaussians into stable and unstable ones,\nwhere the stable Gaussians are expected to well fit previously observed RGBD\nimages and otherwise unstable. We only optimize the unstable Gaussians and only\nrender the pixels occupied by unstable Gaussians. In this way, both the number\nof Gaussians to be optimized and pixels to be rendered are largely reduced, and\nthe optimization can be done in real time. We show real-time reconstructions of\na variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD\nSLAM, our system achieves comparable high-quality reconstruction but with\naround twice the speed and half the memory cost, and shows superior performance\nin the realism of novel view synthesis and camera tracking accuracy.\n", "link": "http://arxiv.org/abs/2404.19706v2", "date": "2024-05-01", "relevancy": 3.6624, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.9948}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6628}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5399}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting&body=Title%3A%20RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting%0AAuthor%3A%20Zhexi%20Peng%20and%20Tianjia%20Shao%20and%20Yong%20Liu%20and%20Jingke%20Zhou%20and%20Yin%20Yang%20and%20Jingdong%20Wang%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20We%20present%20Real-time%20Gaussian%20SLAM%20%28RTG-SLAM%29%2C%20a%20real-time%203D%20reconstruction%0Asystem%20with%20an%20RGBD%20camera%20for%20large-scale%20environments%20using%20Gaussian%0Asplatting.%20The%20system%20features%20a%20compact%20Gaussian%20representation%20and%20a%20highly%0Aefficient%20on-the-fly%20Gaussian%20optimization%20scheme.%20We%20force%20each%20Gaussian%20to%20be%0Aeither%20opaque%20or%20nearly%20transparent%2C%20with%20the%20opaque%20ones%20fitting%20the%20surface%0Aand%20dominant%20colors%2C%20and%20transparent%20ones%20fitting%20residual%20colors.%20By%20rendering%0Adepth%20in%20a%20different%20way%20from%20color%20rendering%2C%20we%20let%20a%20single%20opaque%20Gaussian%0Awell%20fit%20a%20local%20surface%20region%20without%20the%20need%20of%20multiple%20overlapping%0AGaussians%2C%20hence%20largely%20reducing%20the%20memory%20and%20computation%20cost.%20For%0Aon-the-fly%20Gaussian%20optimization%2C%20we%20explicitly%20add%20Gaussians%20for%20three%20types%0Aof%20pixels%20per%20frame%3A%20newly%20observed%2C%20with%20large%20color%20errors%2C%20and%20with%20large%0Adepth%20errors.%20We%20also%20categorize%20all%20Gaussians%20into%20stable%20and%20unstable%20ones%2C%0Awhere%20the%20stable%20Gaussians%20are%20expected%20to%20well%20fit%20previously%20observed%20RGBD%0Aimages%20and%20otherwise%20unstable.%20We%20only%20optimize%20the%20unstable%20Gaussians%20and%20only%0Arender%20the%20pixels%20occupied%20by%20unstable%20Gaussians.%20In%20this%20way%2C%20both%20the%20number%0Aof%20Gaussians%20to%20be%20optimized%20and%20pixels%20to%20be%20rendered%20are%20largely%20reduced%2C%20and%0Athe%20optimization%20can%20be%20done%20in%20real%20time.%20We%20show%20real-time%20reconstructions%20of%0Aa%20variety%20of%20large%20scenes.%20Compared%20with%20the%20state-of-the-art%20NeRF-based%20RGBD%0ASLAM%2C%20our%20system%20achieves%20comparable%20high-quality%20reconstruction%20but%20with%0Aaround%20twice%20the%20speed%20and%20half%20the%20memory%20cost%2C%20and%20shows%20superior%20performance%0Ain%20the%20realism%20of%20novel%20view%20synthesis%20and%20camera%20tracking%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19706v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting&entry.906535625=Zhexi%20Peng%20and%20Tianjia%20Shao%20and%20Yong%20Liu%20and%20Jingke%20Zhou%20and%20Yin%20Yang%20and%20Jingdong%20Wang%20and%20Kun%20Zhou&entry.1292438233=%20%20We%20present%20Real-time%20Gaussian%20SLAM%20%28RTG-SLAM%29%2C%20a%20real-time%203D%20reconstruction%0Asystem%20with%20an%20RGBD%20camera%20for%20large-scale%20environments%20using%20Gaussian%0Asplatting.%20The%20system%20features%20a%20compact%20Gaussian%20representation%20and%20a%20highly%0Aefficient%20on-the-fly%20Gaussian%20optimization%20scheme.%20We%20force%20each%20Gaussian%20to%20be%0Aeither%20opaque%20or%20nearly%20transparent%2C%20with%20the%20opaque%20ones%20fitting%20the%20surface%0Aand%20dominant%20colors%2C%20and%20transparent%20ones%20fitting%20residual%20colors.%20By%20rendering%0Adepth%20in%20a%20different%20way%20from%20color%20rendering%2C%20we%20let%20a%20single%20opaque%20Gaussian%0Awell%20fit%20a%20local%20surface%20region%20without%20the%20need%20of%20multiple%20overlapping%0AGaussians%2C%20hence%20largely%20reducing%20the%20memory%20and%20computation%20cost.%20For%0Aon-the-fly%20Gaussian%20optimization%2C%20we%20explicitly%20add%20Gaussians%20for%20three%20types%0Aof%20pixels%20per%20frame%3A%20newly%20observed%2C%20with%20large%20color%20errors%2C%20and%20with%20large%0Adepth%20errors.%20We%20also%20categorize%20all%20Gaussians%20into%20stable%20and%20unstable%20ones%2C%0Awhere%20the%20stable%20Gaussians%20are%20expected%20to%20well%20fit%20previously%20observed%20RGBD%0Aimages%20and%20otherwise%20unstable.%20We%20only%20optimize%20the%20unstable%20Gaussians%20and%20only%0Arender%20the%20pixels%20occupied%20by%20unstable%20Gaussians.%20In%20this%20way%2C%20both%20the%20number%0Aof%20Gaussians%20to%20be%20optimized%20and%20pixels%20to%20be%20rendered%20are%20largely%20reduced%2C%20and%0Athe%20optimization%20can%20be%20done%20in%20real%20time.%20We%20show%20real-time%20reconstructions%20of%0Aa%20variety%20of%20large%20scenes.%20Compared%20with%20the%20state-of-the-art%20NeRF-based%20RGBD%0ASLAM%2C%20our%20system%20achieves%20comparable%20high-quality%20reconstruction%20but%20with%0Aaround%20twice%20the%20speed%20and%20half%20the%20memory%20cost%2C%20and%20shows%20superior%20performance%0Ain%20the%20realism%20of%20novel%20view%20synthesis%20and%20camera%20tracking%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19706v2&entry.124074799=Read"},
{"title": "NeRF-Guided Unsupervised Learning of RGB-D Registration", "author": "Zhinan Yu and Zheng Qin and Yijie Tang and Yongjun Wang and Renjiao Yi and Chenyang Zhu and Kai Xu", "abstract": "  This paper focuses on training a robust RGB-D registration model without\nground-truth pose supervision. Existing methods usually adopt a pairwise\ntraining strategy based on differentiable rendering, which enforces the\nphotometric and the geometric consistency between the two registered frames as\nsupervision. However, this frame-to-frame framework suffers from poor\nmulti-view consistency due to factors such as lighting changes, geometry\nocclusion and reflective materials. In this paper, we present NeRF-UR, a novel\nframe-to-model optimization framework for unsupervised RGB-D registration.\nInstead of frame-to-frame consistency, we leverage the neural radiance field\n(NeRF) as a global model of the scene and use the consistency between the input\nand the NeRF-rerendered frames for pose optimization. This design can\nsignificantly improve the robustness in scenarios with poor multi-view\nconsistency and provides better learning signal for the registration model.\nFurthermore, to bootstrap the NeRF optimization, we create a synthetic dataset,\nSim-RGBD, through a photo-realistic simulator to warm up the registration\nmodel. By first training the registration model on Sim-RGBD and later\nunsupervisedly fine-tuning on real data, our framework enables distilling the\ncapability of feature extraction and registration from simulation to reality.\nOur method outperforms the state-of-the-art counterparts on two popular indoor\nRGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper\nreproduction.\n", "link": "http://arxiv.org/abs/2405.00507v1", "date": "2024-05-01", "relevancy": 2.9745, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6318}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5978}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5551}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NeRF-Guided%20Unsupervised%20Learning%20of%20RGB-D%20Registration&body=Title%3A%20NeRF-Guided%20Unsupervised%20Learning%20of%20RGB-D%20Registration%0AAuthor%3A%20Zhinan%20Yu%20and%20Zheng%20Qin%20and%20Yijie%20Tang%20and%20Yongjun%20Wang%20and%20Renjiao%20Yi%20and%20Chenyang%20Zhu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20training%20a%20robust%20RGB-D%20registration%20model%20without%0Aground-truth%20pose%20supervision.%20Existing%20methods%20usually%20adopt%20a%20pairwise%0Atraining%20strategy%20based%20on%20differentiable%20rendering%2C%20which%20enforces%20the%0Aphotometric%20and%20the%20geometric%20consistency%20between%20the%20two%20registered%20frames%20as%0Asupervision.%20However%2C%20this%20frame-to-frame%20framework%20suffers%20from%20poor%0Amulti-view%20consistency%20due%20to%20factors%20such%20as%20lighting%20changes%2C%20geometry%0Aocclusion%20and%20reflective%20materials.%20In%20this%20paper%2C%20we%20present%20NeRF-UR%2C%20a%20novel%0Aframe-to-model%20optimization%20framework%20for%20unsupervised%20RGB-D%20registration.%0AInstead%20of%20frame-to-frame%20consistency%2C%20we%20leverage%20the%20neural%20radiance%20field%0A%28NeRF%29%20as%20a%20global%20model%20of%20the%20scene%20and%20use%20the%20consistency%20between%20the%20input%0Aand%20the%20NeRF-rerendered%20frames%20for%20pose%20optimization.%20This%20design%20can%0Asignificantly%20improve%20the%20robustness%20in%20scenarios%20with%20poor%20multi-view%0Aconsistency%20and%20provides%20better%20learning%20signal%20for%20the%20registration%20model.%0AFurthermore%2C%20to%20bootstrap%20the%20NeRF%20optimization%2C%20we%20create%20a%20synthetic%20dataset%2C%0ASim-RGBD%2C%20through%20a%20photo-realistic%20simulator%20to%20warm%20up%20the%20registration%0Amodel.%20By%20first%20training%20the%20registration%20model%20on%20Sim-RGBD%20and%20later%0Aunsupervisedly%20fine-tuning%20on%20real%20data%2C%20our%20framework%20enables%20distilling%20the%0Acapability%20of%20feature%20extraction%20and%20registration%20from%20simulation%20to%20reality.%0AOur%20method%20outperforms%20the%20state-of-the-art%20counterparts%20on%20two%20popular%20indoor%0ARGB-D%20datasets%2C%20ScanNet%20and%203DMatch.%20Code%20and%20models%20will%20be%20released%20for%20paper%0Areproduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00507v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-Guided%20Unsupervised%20Learning%20of%20RGB-D%20Registration&entry.906535625=Zhinan%20Yu%20and%20Zheng%20Qin%20and%20Yijie%20Tang%20and%20Yongjun%20Wang%20and%20Renjiao%20Yi%20and%20Chenyang%20Zhu%20and%20Kai%20Xu&entry.1292438233=%20%20This%20paper%20focuses%20on%20training%20a%20robust%20RGB-D%20registration%20model%20without%0Aground-truth%20pose%20supervision.%20Existing%20methods%20usually%20adopt%20a%20pairwise%0Atraining%20strategy%20based%20on%20differentiable%20rendering%2C%20which%20enforces%20the%0Aphotometric%20and%20the%20geometric%20consistency%20between%20the%20two%20registered%20frames%20as%0Asupervision.%20However%2C%20this%20frame-to-frame%20framework%20suffers%20from%20poor%0Amulti-view%20consistency%20due%20to%20factors%20such%20as%20lighting%20changes%2C%20geometry%0Aocclusion%20and%20reflective%20materials.%20In%20this%20paper%2C%20we%20present%20NeRF-UR%2C%20a%20novel%0Aframe-to-model%20optimization%20framework%20for%20unsupervised%20RGB-D%20registration.%0AInstead%20of%20frame-to-frame%20consistency%2C%20we%20leverage%20the%20neural%20radiance%20field%0A%28NeRF%29%20as%20a%20global%20model%20of%20the%20scene%20and%20use%20the%20consistency%20between%20the%20input%0Aand%20the%20NeRF-rerendered%20frames%20for%20pose%20optimization.%20This%20design%20can%0Asignificantly%20improve%20the%20robustness%20in%20scenarios%20with%20poor%20multi-view%0Aconsistency%20and%20provides%20better%20learning%20signal%20for%20the%20registration%20model.%0AFurthermore%2C%20to%20bootstrap%20the%20NeRF%20optimization%2C%20we%20create%20a%20synthetic%20dataset%2C%0ASim-RGBD%2C%20through%20a%20photo-realistic%20simulator%20to%20warm%20up%20the%20registration%0Amodel.%20By%20first%20training%20the%20registration%20model%20on%20Sim-RGBD%20and%20later%0Aunsupervisedly%20fine-tuning%20on%20real%20data%2C%20our%20framework%20enables%20distilling%20the%0Acapability%20of%20feature%20extraction%20and%20registration%20from%20simulation%20to%20reality.%0AOur%20method%20outperforms%20the%20state-of-the-art%20counterparts%20on%20two%20popular%20indoor%0ARGB-D%20datasets%2C%20ScanNet%20and%203DMatch.%20Code%20and%20models%20will%20be%20released%20for%20paper%0Areproduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00507v1&entry.124074799=Read"},
{"title": "FusionVision: A comprehensive approach of 3D object reconstruction and\n  segmentation from RGB-D cameras using YOLO and fast segment anything", "author": "Safouane El Ghazouali and Youssef Mhirit and Ali Oukhrid and Umberto Michelucci and Hichem Nouira", "abstract": "  In the realm of computer vision, the integration of advanced techniques into\nthe processing of RGB-D camera inputs poses a significant challenge, given the\ninherent complexities arising from diverse environmental conditions and varying\nobject appearances. Therefore, this paper introduces FusionVision, an\nexhaustive pipeline adapted for the robust 3D segmentation of objects in RGB-D\nimagery. Traditional computer vision systems face limitations in simultaneously\ncapturing precise object boundaries and achieving high-precision object\ndetection on depth map as they are mainly proposed for RGB cameras. To address\nthis challenge, FusionVision adopts an integrated approach by merging\nstate-of-the-art object detection techniques, with advanced instance\nsegmentation methods. The integration of these components enables a holistic\n(unified analysis of information obtained from both color \\textit{RGB} and\ndepth \\textit{D} channels) interpretation of RGB-D data, facilitating the\nextraction of comprehensive and accurate object information. The proposed\nFusionVision pipeline employs YOLO for identifying objects within the RGB image\ndomain. Subsequently, FastSAM, an innovative semantic segmentation model, is\napplied to delineate object boundaries, yielding refined segmentation masks.\nThe synergy between these components and their integration into 3D scene\nunderstanding ensures a cohesive fusion of object detection and segmentation,\nenhancing overall precision in 3D object segmentation. The code and pre-trained\nmodels are publicly available at https://github.com/safouaneelg/FusionVision/.\n", "link": "http://arxiv.org/abs/2403.00175v2", "date": "2024-05-01", "relevancy": 2.7589, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5875}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FusionVision%3A%20A%20comprehensive%20approach%20of%203D%20object%20reconstruction%20and%0A%20%20segmentation%20from%20RGB-D%20cameras%20using%20YOLO%20and%20fast%20segment%20anything&body=Title%3A%20FusionVision%3A%20A%20comprehensive%20approach%20of%203D%20object%20reconstruction%20and%0A%20%20segmentation%20from%20RGB-D%20cameras%20using%20YOLO%20and%20fast%20segment%20anything%0AAuthor%3A%20Safouane%20El%20Ghazouali%20and%20Youssef%20Mhirit%20and%20Ali%20Oukhrid%20and%20Umberto%20Michelucci%20and%20Hichem%20Nouira%0AAbstract%3A%20%20%20In%20the%20realm%20of%20computer%20vision%2C%20the%20integration%20of%20advanced%20techniques%20into%0Athe%20processing%20of%20RGB-D%20camera%20inputs%20poses%20a%20significant%20challenge%2C%20given%20the%0Ainherent%20complexities%20arising%20from%20diverse%20environmental%20conditions%20and%20varying%0Aobject%20appearances.%20Therefore%2C%20this%20paper%20introduces%20FusionVision%2C%20an%0Aexhaustive%20pipeline%20adapted%20for%20the%20robust%203D%20segmentation%20of%20objects%20in%20RGB-D%0Aimagery.%20Traditional%20computer%20vision%20systems%20face%20limitations%20in%20simultaneously%0Acapturing%20precise%20object%20boundaries%20and%20achieving%20high-precision%20object%0Adetection%20on%20depth%20map%20as%20they%20are%20mainly%20proposed%20for%20RGB%20cameras.%20To%20address%0Athis%20challenge%2C%20FusionVision%20adopts%20an%20integrated%20approach%20by%20merging%0Astate-of-the-art%20object%20detection%20techniques%2C%20with%20advanced%20instance%0Asegmentation%20methods.%20The%20integration%20of%20these%20components%20enables%20a%20holistic%0A%28unified%20analysis%20of%20information%20obtained%20from%20both%20color%20%5Ctextit%7BRGB%7D%20and%0Adepth%20%5Ctextit%7BD%7D%20channels%29%20interpretation%20of%20RGB-D%20data%2C%20facilitating%20the%0Aextraction%20of%20comprehensive%20and%20accurate%20object%20information.%20The%20proposed%0AFusionVision%20pipeline%20employs%20YOLO%20for%20identifying%20objects%20within%20the%20RGB%20image%0Adomain.%20Subsequently%2C%20FastSAM%2C%20an%20innovative%20semantic%20segmentation%20model%2C%20is%0Aapplied%20to%20delineate%20object%20boundaries%2C%20yielding%20refined%20segmentation%20masks.%0AThe%20synergy%20between%20these%20components%20and%20their%20integration%20into%203D%20scene%0Aunderstanding%20ensures%20a%20cohesive%20fusion%20of%20object%20detection%20and%20segmentation%2C%0Aenhancing%20overall%20precision%20in%203D%20object%20segmentation.%20The%20code%20and%20pre-trained%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/safouaneelg/FusionVision/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00175v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionVision%3A%20A%20comprehensive%20approach%20of%203D%20object%20reconstruction%20and%0A%20%20segmentation%20from%20RGB-D%20cameras%20using%20YOLO%20and%20fast%20segment%20anything&entry.906535625=Safouane%20El%20Ghazouali%20and%20Youssef%20Mhirit%20and%20Ali%20Oukhrid%20and%20Umberto%20Michelucci%20and%20Hichem%20Nouira&entry.1292438233=%20%20In%20the%20realm%20of%20computer%20vision%2C%20the%20integration%20of%20advanced%20techniques%20into%0Athe%20processing%20of%20RGB-D%20camera%20inputs%20poses%20a%20significant%20challenge%2C%20given%20the%0Ainherent%20complexities%20arising%20from%20diverse%20environmental%20conditions%20and%20varying%0Aobject%20appearances.%20Therefore%2C%20this%20paper%20introduces%20FusionVision%2C%20an%0Aexhaustive%20pipeline%20adapted%20for%20the%20robust%203D%20segmentation%20of%20objects%20in%20RGB-D%0Aimagery.%20Traditional%20computer%20vision%20systems%20face%20limitations%20in%20simultaneously%0Acapturing%20precise%20object%20boundaries%20and%20achieving%20high-precision%20object%0Adetection%20on%20depth%20map%20as%20they%20are%20mainly%20proposed%20for%20RGB%20cameras.%20To%20address%0Athis%20challenge%2C%20FusionVision%20adopts%20an%20integrated%20approach%20by%20merging%0Astate-of-the-art%20object%20detection%20techniques%2C%20with%20advanced%20instance%0Asegmentation%20methods.%20The%20integration%20of%20these%20components%20enables%20a%20holistic%0A%28unified%20analysis%20of%20information%20obtained%20from%20both%20color%20%5Ctextit%7BRGB%7D%20and%0Adepth%20%5Ctextit%7BD%7D%20channels%29%20interpretation%20of%20RGB-D%20data%2C%20facilitating%20the%0Aextraction%20of%20comprehensive%20and%20accurate%20object%20information.%20The%20proposed%0AFusionVision%20pipeline%20employs%20YOLO%20for%20identifying%20objects%20within%20the%20RGB%20image%0Adomain.%20Subsequently%2C%20FastSAM%2C%20an%20innovative%20semantic%20segmentation%20model%2C%20is%0Aapplied%20to%20delineate%20object%20boundaries%2C%20yielding%20refined%20segmentation%20masks.%0AThe%20synergy%20between%20these%20components%20and%20their%20integration%20into%203D%20scene%0Aunderstanding%20ensures%20a%20cohesive%20fusion%20of%20object%20detection%20and%20segmentation%2C%0Aenhancing%20overall%20precision%20in%203D%20object%20segmentation.%20The%20code%20and%20pre-trained%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/safouaneelg/FusionVision/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00175v2&entry.124074799=Read"},
{"title": "Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed\n  Image Retrieval", "author": "Young Kyun Jang and Dat Huynh and Ashish Shah and Wen-Kai Chen and Ser-Nam Lim", "abstract": "  Composed Image Retrieval (CIR) is a complex task that retrieves images using\na query, which is configured with an image and a caption that describes desired\nmodifications to that image. Supervised CIR approaches have shown strong\nperformance, but their reliance on expensive manually-annotated datasets\nrestricts their scalability and broader applicability. To address these issues,\nprevious studies have proposed pseudo-word token-based Zero-Shot CIR (ZS-CIR)\nmethods, which utilize a projection module to map images to word tokens.\nHowever, we conjecture that this approach has a downside: the projection module\ndistorts the original image representation and confines the resulting composed\nembeddings to the text-side. In order to resolve this, we introduce a novel\nZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly\nmerge image and text representations by identifying an intermediate embedding\nof both. Furthermore, we introduce Text-Anchored-Tuning (TAT), a method that\nfine-tunes the image encoder while keeping the text encoder fixed. TAT closes\nthe modality gap between images and text, making the Slerp process much more\neffective. Notably, the TAT method is not only efficient in terms of the scale\nof the training dataset and training time, but it also serves as an excellent\ninitial checkpoint for training supervised CIR models, thereby highlighting its\nwider potential. The integration of the Slerp-based ZS-CIR with a TAT-tuned\nmodel enables our approach to deliver state-of-the-art retrieval performance\nacross CIR benchmarks.\n", "link": "http://arxiv.org/abs/2405.00571v1", "date": "2024-05-01", "relevancy": 2.7369, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5444}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5256}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spherical%20Linear%20Interpolation%20and%20Text-Anchoring%20for%20Zero-shot%20Composed%0A%20%20Image%20Retrieval&body=Title%3A%20Spherical%20Linear%20Interpolation%20and%20Text-Anchoring%20for%20Zero-shot%20Composed%0A%20%20Image%20Retrieval%0AAuthor%3A%20Young%20Kyun%20Jang%20and%20Dat%20Huynh%20and%20Ashish%20Shah%20and%20Wen-Kai%20Chen%20and%20Ser-Nam%20Lim%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20complex%20task%20that%20retrieves%20images%20using%0Aa%20query%2C%20which%20is%20configured%20with%20an%20image%20and%20a%20caption%20that%20describes%20desired%0Amodifications%20to%20that%20image.%20Supervised%20CIR%20approaches%20have%20shown%20strong%0Aperformance%2C%20but%20their%20reliance%20on%20expensive%20manually-annotated%20datasets%0Arestricts%20their%20scalability%20and%20broader%20applicability.%20To%20address%20these%20issues%2C%0Aprevious%20studies%20have%20proposed%20pseudo-word%20token-based%20Zero-Shot%20CIR%20%28ZS-CIR%29%0Amethods%2C%20which%20utilize%20a%20projection%20module%20to%20map%20images%20to%20word%20tokens.%0AHowever%2C%20we%20conjecture%20that%20this%20approach%20has%20a%20downside%3A%20the%20projection%20module%0Adistorts%20the%20original%20image%20representation%20and%20confines%20the%20resulting%20composed%0Aembeddings%20to%20the%20text-side.%20In%20order%20to%20resolve%20this%2C%20we%20introduce%20a%20novel%0AZS-CIR%20method%20that%20uses%20Spherical%20Linear%20Interpolation%20%28Slerp%29%20to%20directly%0Amerge%20image%20and%20text%20representations%20by%20identifying%20an%20intermediate%20embedding%0Aof%20both.%20Furthermore%2C%20we%20introduce%20Text-Anchored-Tuning%20%28TAT%29%2C%20a%20method%20that%0Afine-tunes%20the%20image%20encoder%20while%20keeping%20the%20text%20encoder%20fixed.%20TAT%20closes%0Athe%20modality%20gap%20between%20images%20and%20text%2C%20making%20the%20Slerp%20process%20much%20more%0Aeffective.%20Notably%2C%20the%20TAT%20method%20is%20not%20only%20efficient%20in%20terms%20of%20the%20scale%0Aof%20the%20training%20dataset%20and%20training%20time%2C%20but%20it%20also%20serves%20as%20an%20excellent%0Ainitial%20checkpoint%20for%20training%20supervised%20CIR%20models%2C%20thereby%20highlighting%20its%0Awider%20potential.%20The%20integration%20of%20the%20Slerp-based%20ZS-CIR%20with%20a%20TAT-tuned%0Amodel%20enables%20our%20approach%20to%20deliver%20state-of-the-art%20retrieval%20performance%0Aacross%20CIR%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00571v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spherical%20Linear%20Interpolation%20and%20Text-Anchoring%20for%20Zero-shot%20Composed%0A%20%20Image%20Retrieval&entry.906535625=Young%20Kyun%20Jang%20and%20Dat%20Huynh%20and%20Ashish%20Shah%20and%20Wen-Kai%20Chen%20and%20Ser-Nam%20Lim&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20complex%20task%20that%20retrieves%20images%20using%0Aa%20query%2C%20which%20is%20configured%20with%20an%20image%20and%20a%20caption%20that%20describes%20desired%0Amodifications%20to%20that%20image.%20Supervised%20CIR%20approaches%20have%20shown%20strong%0Aperformance%2C%20but%20their%20reliance%20on%20expensive%20manually-annotated%20datasets%0Arestricts%20their%20scalability%20and%20broader%20applicability.%20To%20address%20these%20issues%2C%0Aprevious%20studies%20have%20proposed%20pseudo-word%20token-based%20Zero-Shot%20CIR%20%28ZS-CIR%29%0Amethods%2C%20which%20utilize%20a%20projection%20module%20to%20map%20images%20to%20word%20tokens.%0AHowever%2C%20we%20conjecture%20that%20this%20approach%20has%20a%20downside%3A%20the%20projection%20module%0Adistorts%20the%20original%20image%20representation%20and%20confines%20the%20resulting%20composed%0Aembeddings%20to%20the%20text-side.%20In%20order%20to%20resolve%20this%2C%20we%20introduce%20a%20novel%0AZS-CIR%20method%20that%20uses%20Spherical%20Linear%20Interpolation%20%28Slerp%29%20to%20directly%0Amerge%20image%20and%20text%20representations%20by%20identifying%20an%20intermediate%20embedding%0Aof%20both.%20Furthermore%2C%20we%20introduce%20Text-Anchored-Tuning%20%28TAT%29%2C%20a%20method%20that%0Afine-tunes%20the%20image%20encoder%20while%20keeping%20the%20text%20encoder%20fixed.%20TAT%20closes%0Athe%20modality%20gap%20between%20images%20and%20text%2C%20making%20the%20Slerp%20process%20much%20more%0Aeffective.%20Notably%2C%20the%20TAT%20method%20is%20not%20only%20efficient%20in%20terms%20of%20the%20scale%0Aof%20the%20training%20dataset%20and%20training%20time%2C%20but%20it%20also%20serves%20as%20an%20excellent%0Ainitial%20checkpoint%20for%20training%20supervised%20CIR%20models%2C%20thereby%20highlighting%20its%0Awider%20potential.%20The%20integration%20of%20the%20Slerp-based%20ZS-CIR%20with%20a%20TAT-tuned%0Amodel%20enables%20our%20approach%20to%20deliver%20state-of-the-art%20retrieval%20performance%0Aacross%20CIR%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00571v1&entry.124074799=Read"},
{"title": "UWAFA-GAN: Ultra-Wide-Angle Fluorescein Angiography Transformation via\n  Multi-scale Generation and Registration Enhancement", "author": "Ruiquan Ge and Zhaojie Fang and Pengxue Wei and Zhanghao Chen and Hongyang Jiang and Ahmed Elazab and Wangting Li and Xiang Wan and Shaochong Zhang and Changmiao Wang", "abstract": "  Fundus photography, in combination with the ultra-wide-angle fundus (UWF)\ntechniques, becomes an indispensable diagnostic tool in clinical settings by\noffering a more comprehensive view of the retina. Nonetheless, UWF fluorescein\nangiography (UWF-FA) necessitates the administration of a fluorescent dye via\ninjection into the patient's hand or elbow unlike UWF scanning laser\nophthalmoscopy (UWF-SLO). To mitigate potential adverse effects associated with\ninjections, researchers have proposed the development of cross-modality medical\nimage generation algorithms capable of converting UWF-SLO images into their\nUWF-FA counterparts. Current image generation techniques applied to fundus\nphotography encounter difficulties in producing high-resolution retinal images,\nparticularly in capturing minute vascular lesions. To address these issues, we\nintroduce a novel conditional generative adversarial network (UWAFA-GAN) to\nsynthesize UWF-FA from UWF-SLO. This approach employs multi-scale generators\nand an attention transmit module to efficiently extract both global structures\nand local lesions. Additionally, to counteract the image blurriness issue that\narises from training with misaligned data, a registration module is integrated\nwithin this framework. Our method performs non-trivially on inception scores\nand details generation. Clinical user studies further indicate that the UWF-FA\nimages generated by UWAFA-GAN are clinically comparable to authentic images in\nterms of diagnostic reliability. Empirical evaluations on our proprietary UWF\nimage datasets elucidate that UWAFA-GAN outperforms extant methodologies. The\ncode is accessible at https://github.com/Tinysqua/UWAFA-GAN.\n", "link": "http://arxiv.org/abs/2405.00542v1", "date": "2024-05-01", "relevancy": 2.7231, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5574}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5383}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5382}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UWAFA-GAN%3A%20Ultra-Wide-Angle%20Fluorescein%20Angiography%20Transformation%20via%0A%20%20Multi-scale%20Generation%20and%20Registration%20Enhancement&body=Title%3A%20UWAFA-GAN%3A%20Ultra-Wide-Angle%20Fluorescein%20Angiography%20Transformation%20via%0A%20%20Multi-scale%20Generation%20and%20Registration%20Enhancement%0AAuthor%3A%20Ruiquan%20Ge%20and%20Zhaojie%20Fang%20and%20Pengxue%20Wei%20and%20Zhanghao%20Chen%20and%20Hongyang%20Jiang%20and%20Ahmed%20Elazab%20and%20Wangting%20Li%20and%20Xiang%20Wan%20and%20Shaochong%20Zhang%20and%20Changmiao%20Wang%0AAbstract%3A%20%20%20Fundus%20photography%2C%20in%20combination%20with%20the%20ultra-wide-angle%20fundus%20%28UWF%29%0Atechniques%2C%20becomes%20an%20indispensable%20diagnostic%20tool%20in%20clinical%20settings%20by%0Aoffering%20a%20more%20comprehensive%20view%20of%20the%20retina.%20Nonetheless%2C%20UWF%20fluorescein%0Aangiography%20%28UWF-FA%29%20necessitates%20the%20administration%20of%20a%20fluorescent%20dye%20via%0Ainjection%20into%20the%20patient%27s%20hand%20or%20elbow%20unlike%20UWF%20scanning%20laser%0Aophthalmoscopy%20%28UWF-SLO%29.%20To%20mitigate%20potential%20adverse%20effects%20associated%20with%0Ainjections%2C%20researchers%20have%20proposed%20the%20development%20of%20cross-modality%20medical%0Aimage%20generation%20algorithms%20capable%20of%20converting%20UWF-SLO%20images%20into%20their%0AUWF-FA%20counterparts.%20Current%20image%20generation%20techniques%20applied%20to%20fundus%0Aphotography%20encounter%20difficulties%20in%20producing%20high-resolution%20retinal%20images%2C%0Aparticularly%20in%20capturing%20minute%20vascular%20lesions.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20conditional%20generative%20adversarial%20network%20%28UWAFA-GAN%29%20to%0Asynthesize%20UWF-FA%20from%20UWF-SLO.%20This%20approach%20employs%20multi-scale%20generators%0Aand%20an%20attention%20transmit%20module%20to%20efficiently%20extract%20both%20global%20structures%0Aand%20local%20lesions.%20Additionally%2C%20to%20counteract%20the%20image%20blurriness%20issue%20that%0Aarises%20from%20training%20with%20misaligned%20data%2C%20a%20registration%20module%20is%20integrated%0Awithin%20this%20framework.%20Our%20method%20performs%20non-trivially%20on%20inception%20scores%0Aand%20details%20generation.%20Clinical%20user%20studies%20further%20indicate%20that%20the%20UWF-FA%0Aimages%20generated%20by%20UWAFA-GAN%20are%20clinically%20comparable%20to%20authentic%20images%20in%0Aterms%20of%20diagnostic%20reliability.%20Empirical%20evaluations%20on%20our%20proprietary%20UWF%0Aimage%20datasets%20elucidate%20that%20UWAFA-GAN%20outperforms%20extant%20methodologies.%20The%0Acode%20is%20accessible%20at%20https%3A//github.com/Tinysqua/UWAFA-GAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00542v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UWAFA-GAN%3A%20Ultra-Wide-Angle%20Fluorescein%20Angiography%20Transformation%20via%0A%20%20Multi-scale%20Generation%20and%20Registration%20Enhancement&entry.906535625=Ruiquan%20Ge%20and%20Zhaojie%20Fang%20and%20Pengxue%20Wei%20and%20Zhanghao%20Chen%20and%20Hongyang%20Jiang%20and%20Ahmed%20Elazab%20and%20Wangting%20Li%20and%20Xiang%20Wan%20and%20Shaochong%20Zhang%20and%20Changmiao%20Wang&entry.1292438233=%20%20Fundus%20photography%2C%20in%20combination%20with%20the%20ultra-wide-angle%20fundus%20%28UWF%29%0Atechniques%2C%20becomes%20an%20indispensable%20diagnostic%20tool%20in%20clinical%20settings%20by%0Aoffering%20a%20more%20comprehensive%20view%20of%20the%20retina.%20Nonetheless%2C%20UWF%20fluorescein%0Aangiography%20%28UWF-FA%29%20necessitates%20the%20administration%20of%20a%20fluorescent%20dye%20via%0Ainjection%20into%20the%20patient%27s%20hand%20or%20elbow%20unlike%20UWF%20scanning%20laser%0Aophthalmoscopy%20%28UWF-SLO%29.%20To%20mitigate%20potential%20adverse%20effects%20associated%20with%0Ainjections%2C%20researchers%20have%20proposed%20the%20development%20of%20cross-modality%20medical%0Aimage%20generation%20algorithms%20capable%20of%20converting%20UWF-SLO%20images%20into%20their%0AUWF-FA%20counterparts.%20Current%20image%20generation%20techniques%20applied%20to%20fundus%0Aphotography%20encounter%20difficulties%20in%20producing%20high-resolution%20retinal%20images%2C%0Aparticularly%20in%20capturing%20minute%20vascular%20lesions.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20conditional%20generative%20adversarial%20network%20%28UWAFA-GAN%29%20to%0Asynthesize%20UWF-FA%20from%20UWF-SLO.%20This%20approach%20employs%20multi-scale%20generators%0Aand%20an%20attention%20transmit%20module%20to%20efficiently%20extract%20both%20global%20structures%0Aand%20local%20lesions.%20Additionally%2C%20to%20counteract%20the%20image%20blurriness%20issue%20that%0Aarises%20from%20training%20with%20misaligned%20data%2C%20a%20registration%20module%20is%20integrated%0Awithin%20this%20framework.%20Our%20method%20performs%20non-trivially%20on%20inception%20scores%0Aand%20details%20generation.%20Clinical%20user%20studies%20further%20indicate%20that%20the%20UWF-FA%0Aimages%20generated%20by%20UWAFA-GAN%20are%20clinically%20comparable%20to%20authentic%20images%20in%0Aterms%20of%20diagnostic%20reliability.%20Empirical%20evaluations%20on%20our%20proprietary%20UWF%0Aimage%20datasets%20elucidate%20that%20UWAFA-GAN%20outperforms%20extant%20methodologies.%20The%0Acode%20is%20accessible%20at%20https%3A//github.com/Tinysqua/UWAFA-GAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00542v1&entry.124074799=Read"},
{"title": "Learning to Compose: Improving Object Centric Learning by Injecting\n  Compositionality", "author": "Whie Jung and Jaehoon Yoo and Sungjin Ahn and Seunghoon Hong", "abstract": "  Learning compositional representation is a key aspect of object-centric\nlearning as it enables flexible systematic generalization and supports complex\nvisual reasoning. However, most of the existing approaches rely on\nauto-encoding objective, while the compositionality is implicitly imposed by\nthe architectural or algorithmic bias in the encoder. This misalignment between\nauto-encoding objective and learning compositionality often results in failure\nof capturing meaningful object representations. In this study, we propose a\nnovel objective that explicitly encourages compositionality of the\nrepresentations. Built upon the existing object-centric learning framework\n(e.g., slot attention), our method incorporates additional constraints that an\narbitrary mixture of object representations from two images should be valid by\nmaximizing the likelihood of the composite data. We demonstrate that\nincorporating our objective to the existing framework consistently improves the\nobjective-centric learning and enhances the robustness to the architectural\nchoices.\n", "link": "http://arxiv.org/abs/2405.00646v1", "date": "2024-05-01", "relevancy": 2.7141, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6092}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4967}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Compose%3A%20Improving%20Object%20Centric%20Learning%20by%20Injecting%0A%20%20Compositionality&body=Title%3A%20Learning%20to%20Compose%3A%20Improving%20Object%20Centric%20Learning%20by%20Injecting%0A%20%20Compositionality%0AAuthor%3A%20Whie%20Jung%20and%20Jaehoon%20Yoo%20and%20Sungjin%20Ahn%20and%20Seunghoon%20Hong%0AAbstract%3A%20%20%20Learning%20compositional%20representation%20is%20a%20key%20aspect%20of%20object-centric%0Alearning%20as%20it%20enables%20flexible%20systematic%20generalization%20and%20supports%20complex%0Avisual%20reasoning.%20However%2C%20most%20of%20the%20existing%20approaches%20rely%20on%0Aauto-encoding%20objective%2C%20while%20the%20compositionality%20is%20implicitly%20imposed%20by%0Athe%20architectural%20or%20algorithmic%20bias%20in%20the%20encoder.%20This%20misalignment%20between%0Aauto-encoding%20objective%20and%20learning%20compositionality%20often%20results%20in%20failure%0Aof%20capturing%20meaningful%20object%20representations.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20objective%20that%20explicitly%20encourages%20compositionality%20of%20the%0Arepresentations.%20Built%20upon%20the%20existing%20object-centric%20learning%20framework%0A%28e.g.%2C%20slot%20attention%29%2C%20our%20method%20incorporates%20additional%20constraints%20that%20an%0Aarbitrary%20mixture%20of%20object%20representations%20from%20two%20images%20should%20be%20valid%20by%0Amaximizing%20the%20likelihood%20of%20the%20composite%20data.%20We%20demonstrate%20that%0Aincorporating%20our%20objective%20to%20the%20existing%20framework%20consistently%20improves%20the%0Aobjective-centric%20learning%20and%20enhances%20the%20robustness%20to%20the%20architectural%0Achoices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00646v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Compose%3A%20Improving%20Object%20Centric%20Learning%20by%20Injecting%0A%20%20Compositionality&entry.906535625=Whie%20Jung%20and%20Jaehoon%20Yoo%20and%20Sungjin%20Ahn%20and%20Seunghoon%20Hong&entry.1292438233=%20%20Learning%20compositional%20representation%20is%20a%20key%20aspect%20of%20object-centric%0Alearning%20as%20it%20enables%20flexible%20systematic%20generalization%20and%20supports%20complex%0Avisual%20reasoning.%20However%2C%20most%20of%20the%20existing%20approaches%20rely%20on%0Aauto-encoding%20objective%2C%20while%20the%20compositionality%20is%20implicitly%20imposed%20by%0Athe%20architectural%20or%20algorithmic%20bias%20in%20the%20encoder.%20This%20misalignment%20between%0Aauto-encoding%20objective%20and%20learning%20compositionality%20often%20results%20in%20failure%0Aof%20capturing%20meaningful%20object%20representations.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20objective%20that%20explicitly%20encourages%20compositionality%20of%20the%0Arepresentations.%20Built%20upon%20the%20existing%20object-centric%20learning%20framework%0A%28e.g.%2C%20slot%20attention%29%2C%20our%20method%20incorporates%20additional%20constraints%20that%20an%0Aarbitrary%20mixture%20of%20object%20representations%20from%20two%20images%20should%20be%20valid%20by%0Amaximizing%20the%20likelihood%20of%20the%20composite%20data.%20We%20demonstrate%20that%0Aincorporating%20our%20objective%20to%20the%20existing%20framework%20consistently%20improves%20the%0Aobjective-centric%20learning%20and%20enhances%20the%20robustness%20to%20the%20architectural%0Achoices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00646v1&entry.124074799=Read"},
{"title": "GRASP: A Rehearsal Policy for Efficient Online Continual Learning", "author": "Md Yousuf Harun and Jhair Gallardo and Junyu Chen and Christopher Kanan", "abstract": "  Continual learning (CL) in deep neural networks (DNNs) involves incrementally\naccumulating knowledge in a DNN from a growing data stream. A major challenge\nin CL is that non-stationary data streams cause catastrophic forgetting of\npreviously learned abilities. A popular solution is rehearsal: storing past\nobservations in a buffer and then sampling the buffer to update the DNN.\nUniform sampling in a class-balanced manner is highly effective, and better\nsample selection policies have been elusive. Here, we propose a new sample\nselection policy called GRASP that selects the most prototypical (easy) samples\nfirst and then gradually selects less prototypical (harder) examples. GRASP has\nlittle additional compute or memory overhead compared to uniform selection,\nenabling it to scale to large datasets. Compared to 17 other rehearsal\npolicies, GRASP achieves higher accuracy in CL experiments on ImageNet.\nCompared to uniform balanced sampling, GRASP achieves the same performance with\n40% fewer updates. We also show that GRASP is effective for CL on five text\nclassification datasets.\n", "link": "http://arxiv.org/abs/2308.13646v2", "date": "2024-05-01", "relevancy": 2.6186, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5407}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5317}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4987}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GRASP%3A%20A%20Rehearsal%20Policy%20for%20Efficient%20Online%20Continual%20Learning&body=Title%3A%20GRASP%3A%20A%20Rehearsal%20Policy%20for%20Efficient%20Online%20Continual%20Learning%0AAuthor%3A%20Md%20Yousuf%20Harun%20and%20Jhair%20Gallardo%20and%20Junyu%20Chen%20and%20Christopher%20Kanan%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20in%20deep%20neural%20networks%20%28DNNs%29%20involves%20incrementally%0Aaccumulating%20knowledge%20in%20a%20DNN%20from%20a%20growing%20data%20stream.%20A%20major%20challenge%0Ain%20CL%20is%20that%20non-stationary%20data%20streams%20cause%20catastrophic%20forgetting%20of%0Apreviously%20learned%20abilities.%20A%20popular%20solution%20is%20rehearsal%3A%20storing%20past%0Aobservations%20in%20a%20buffer%20and%20then%20sampling%20the%20buffer%20to%20update%20the%20DNN.%0AUniform%20sampling%20in%20a%20class-balanced%20manner%20is%20highly%20effective%2C%20and%20better%0Asample%20selection%20policies%20have%20been%20elusive.%20Here%2C%20we%20propose%20a%20new%20sample%0Aselection%20policy%20called%20GRASP%20that%20selects%20the%20most%20prototypical%20%28easy%29%20samples%0Afirst%20and%20then%20gradually%20selects%20less%20prototypical%20%28harder%29%20examples.%20GRASP%20has%0Alittle%20additional%20compute%20or%20memory%20overhead%20compared%20to%20uniform%20selection%2C%0Aenabling%20it%20to%20scale%20to%20large%20datasets.%20Compared%20to%2017%20other%20rehearsal%0Apolicies%2C%20GRASP%20achieves%20higher%20accuracy%20in%20CL%20experiments%20on%20ImageNet.%0ACompared%20to%20uniform%20balanced%20sampling%2C%20GRASP%20achieves%20the%20same%20performance%20with%0A40%25%20fewer%20updates.%20We%20also%20show%20that%20GRASP%20is%20effective%20for%20CL%20on%20five%20text%0Aclassification%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13646v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRASP%3A%20A%20Rehearsal%20Policy%20for%20Efficient%20Online%20Continual%20Learning&entry.906535625=Md%20Yousuf%20Harun%20and%20Jhair%20Gallardo%20and%20Junyu%20Chen%20and%20Christopher%20Kanan&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20in%20deep%20neural%20networks%20%28DNNs%29%20involves%20incrementally%0Aaccumulating%20knowledge%20in%20a%20DNN%20from%20a%20growing%20data%20stream.%20A%20major%20challenge%0Ain%20CL%20is%20that%20non-stationary%20data%20streams%20cause%20catastrophic%20forgetting%20of%0Apreviously%20learned%20abilities.%20A%20popular%20solution%20is%20rehearsal%3A%20storing%20past%0Aobservations%20in%20a%20buffer%20and%20then%20sampling%20the%20buffer%20to%20update%20the%20DNN.%0AUniform%20sampling%20in%20a%20class-balanced%20manner%20is%20highly%20effective%2C%20and%20better%0Asample%20selection%20policies%20have%20been%20elusive.%20Here%2C%20we%20propose%20a%20new%20sample%0Aselection%20policy%20called%20GRASP%20that%20selects%20the%20most%20prototypical%20%28easy%29%20samples%0Afirst%20and%20then%20gradually%20selects%20less%20prototypical%20%28harder%29%20examples.%20GRASP%20has%0Alittle%20additional%20compute%20or%20memory%20overhead%20compared%20to%20uniform%20selection%2C%0Aenabling%20it%20to%20scale%20to%20large%20datasets.%20Compared%20to%2017%20other%20rehearsal%0Apolicies%2C%20GRASP%20achieves%20higher%20accuracy%20in%20CL%20experiments%20on%20ImageNet.%0ACompared%20to%20uniform%20balanced%20sampling%2C%20GRASP%20achieves%20the%20same%20performance%20with%0A40%25%20fewer%20updates.%20We%20also%20show%20that%20GRASP%20is%20effective%20for%20CL%20on%20five%20text%0Aclassification%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13646v2&entry.124074799=Read"},
{"title": "Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised\n  Learning", "author": "James Chapman and Lennie Wells and Ana Lawry Aguila", "abstract": "  The Canonical Correlation Analysis (CCA) family of methods is foundational in\nmultiview learning. Regularised linear CCA methods can be seen to generalise\nPartial Least Squares (PLS) and be unified with a Generalized Eigenvalue\nProblem (GEP) framework. However, classical algorithms for these linear methods\nare computationally infeasible for large-scale data. Extensions to Deep CCA\nshow great promise, but current training procedures are slow and complicated.\nFirst we propose a novel unconstrained objective that characterizes the top\nsubspace of GEPs. Our core contribution is a family of fast algorithms for\nstochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying\nstochastic gradient descent (SGD) to the corresponding CCA objectives. Our\nalgorithms show far faster convergence and recover higher correlations than the\nprevious state-of-the-art on all standard CCA and Deep CCA benchmarks. These\nimprovements allow us to perform a first-of-its-kind PLS analysis of an\nextremely large biomedical dataset from the UK Biobank, with over 33,000\nindividuals and 500,000 features. Finally, we apply our algorithms to match the\nperformance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10\nand CIFAR-100 with minimal hyper-parameter tuning, and also present theory to\nclarify the links between these methods and classical CCA, laying the\ngroundwork for future insights.\n", "link": "http://arxiv.org/abs/2310.01012v4", "date": "2024-05-01", "relevancy": 2.5516, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5358}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5003}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning&body=Title%3A%20Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning%0AAuthor%3A%20James%20Chapman%20and%20Lennie%20Wells%20and%20Ana%20Lawry%20Aguila%0AAbstract%3A%20%20%20The%20Canonical%20Correlation%20Analysis%20%28CCA%29%20family%20of%20methods%20is%20foundational%20in%0Amultiview%20learning.%20Regularised%20linear%20CCA%20methods%20can%20be%20seen%20to%20generalise%0APartial%20Least%20Squares%20%28PLS%29%20and%20be%20unified%20with%20a%20Generalized%20Eigenvalue%0AProblem%20%28GEP%29%20framework.%20However%2C%20classical%20algorithms%20for%20these%20linear%20methods%0Aare%20computationally%20infeasible%20for%20large-scale%20data.%20Extensions%20to%20Deep%20CCA%0Ashow%20great%20promise%2C%20but%20current%20training%20procedures%20are%20slow%20and%20complicated.%0AFirst%20we%20propose%20a%20novel%20unconstrained%20objective%20that%20characterizes%20the%20top%0Asubspace%20of%20GEPs.%20Our%20core%20contribution%20is%20a%20family%20of%20fast%20algorithms%20for%0Astochastic%20PLS%2C%20stochastic%20CCA%2C%20and%20Deep%20CCA%2C%20simply%20obtained%20by%20applying%0Astochastic%20gradient%20descent%20%28SGD%29%20to%20the%20corresponding%20CCA%20objectives.%20Our%0Aalgorithms%20show%20far%20faster%20convergence%20and%20recover%20higher%20correlations%20than%20the%0Aprevious%20state-of-the-art%20on%20all%20standard%20CCA%20and%20Deep%20CCA%20benchmarks.%20These%0Aimprovements%20allow%20us%20to%20perform%20a%20first-of-its-kind%20PLS%20analysis%20of%20an%0Aextremely%20large%20biomedical%20dataset%20from%20the%20UK%20Biobank%2C%20with%20over%2033%2C000%0Aindividuals%20and%20500%2C000%20features.%20Finally%2C%20we%20apply%20our%20algorithms%20to%20match%20the%0Aperformance%20of%20%60CCA-family%27%20Self-Supervised%20Learning%20%28SSL%29%20methods%20on%20CIFAR-10%0Aand%20CIFAR-100%20with%20minimal%20hyper-parameter%20tuning%2C%20and%20also%20present%20theory%20to%0Aclarify%20the%20links%20between%20these%20methods%20and%20classical%20CCA%2C%20laying%20the%0Agroundwork%20for%20future%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01012v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning&entry.906535625=James%20Chapman%20and%20Lennie%20Wells%20and%20Ana%20Lawry%20Aguila&entry.1292438233=%20%20The%20Canonical%20Correlation%20Analysis%20%28CCA%29%20family%20of%20methods%20is%20foundational%20in%0Amultiview%20learning.%20Regularised%20linear%20CCA%20methods%20can%20be%20seen%20to%20generalise%0APartial%20Least%20Squares%20%28PLS%29%20and%20be%20unified%20with%20a%20Generalized%20Eigenvalue%0AProblem%20%28GEP%29%20framework.%20However%2C%20classical%20algorithms%20for%20these%20linear%20methods%0Aare%20computationally%20infeasible%20for%20large-scale%20data.%20Extensions%20to%20Deep%20CCA%0Ashow%20great%20promise%2C%20but%20current%20training%20procedures%20are%20slow%20and%20complicated.%0AFirst%20we%20propose%20a%20novel%20unconstrained%20objective%20that%20characterizes%20the%20top%0Asubspace%20of%20GEPs.%20Our%20core%20contribution%20is%20a%20family%20of%20fast%20algorithms%20for%0Astochastic%20PLS%2C%20stochastic%20CCA%2C%20and%20Deep%20CCA%2C%20simply%20obtained%20by%20applying%0Astochastic%20gradient%20descent%20%28SGD%29%20to%20the%20corresponding%20CCA%20objectives.%20Our%0Aalgorithms%20show%20far%20faster%20convergence%20and%20recover%20higher%20correlations%20than%20the%0Aprevious%20state-of-the-art%20on%20all%20standard%20CCA%20and%20Deep%20CCA%20benchmarks.%20These%0Aimprovements%20allow%20us%20to%20perform%20a%20first-of-its-kind%20PLS%20analysis%20of%20an%0Aextremely%20large%20biomedical%20dataset%20from%20the%20UK%20Biobank%2C%20with%20over%2033%2C000%0Aindividuals%20and%20500%2C000%20features.%20Finally%2C%20we%20apply%20our%20algorithms%20to%20match%20the%0Aperformance%20of%20%60CCA-family%27%20Self-Supervised%20Learning%20%28SSL%29%20methods%20on%20CIFAR-10%0Aand%20CIFAR-100%20with%20minimal%20hyper-parameter%20tuning%2C%20and%20also%20present%20theory%20to%0Aclarify%20the%20links%20between%20these%20methods%20and%20classical%20CCA%2C%20laying%20the%0Agroundwork%20for%20future%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01012v4&entry.124074799=Read"},
{"title": "From Empirical Observations to Universality: Dynamics of Deep Learning\n  with Inputs Built on Gaussian mixture", "author": "Jaeyong Bae and Hawoong Jeong", "abstract": "  This study broadens the scope of theoretical frameworks in deep learning by\ndelving into the dynamics of neural networks with inputs that demonstrate the\nstructural characteristics to Gaussian Mixture (GM). We analyzed how the\ndynamics of neural networks under GM-structured inputs diverge from the\npredictions of conventional theories based on simple Gaussian structures. A\nrevelation of our work is the observed convergence of neural network dynamics\ntowards conventional theory even with standardized GM inputs, highlighting an\nunexpected universality. We found that standardization, especially in\nconjunction with certain nonlinear functions, plays a critical role in this\nphenomena. Consequently, despite the complex and varied nature of GM\ndistributions, we demonstrate that neural networks exhibit asymptotic behaviors\nin line with predictions under simple Gaussian frameworks.\n", "link": "http://arxiv.org/abs/2405.00642v1", "date": "2024-05-01", "relevancy": 2.5361, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5428}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4891}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20Empirical%20Observations%20to%20Universality%3A%20Dynamics%20of%20Deep%20Learning%0A%20%20with%20Inputs%20Built%20on%20Gaussian%20mixture&body=Title%3A%20From%20Empirical%20Observations%20to%20Universality%3A%20Dynamics%20of%20Deep%20Learning%0A%20%20with%20Inputs%20Built%20on%20Gaussian%20mixture%0AAuthor%3A%20Jaeyong%20Bae%20and%20Hawoong%20Jeong%0AAbstract%3A%20%20%20This%20study%20broadens%20the%20scope%20of%20theoretical%20frameworks%20in%20deep%20learning%20by%0Adelving%20into%20the%20dynamics%20of%20neural%20networks%20with%20inputs%20that%20demonstrate%20the%0Astructural%20characteristics%20to%20Gaussian%20Mixture%20%28GM%29.%20We%20analyzed%20how%20the%0Adynamics%20of%20neural%20networks%20under%20GM-structured%20inputs%20diverge%20from%20the%0Apredictions%20of%20conventional%20theories%20based%20on%20simple%20Gaussian%20structures.%20A%0Arevelation%20of%20our%20work%20is%20the%20observed%20convergence%20of%20neural%20network%20dynamics%0Atowards%20conventional%20theory%20even%20with%20standardized%20GM%20inputs%2C%20highlighting%20an%0Aunexpected%20universality.%20We%20found%20that%20standardization%2C%20especially%20in%0Aconjunction%20with%20certain%20nonlinear%20functions%2C%20plays%20a%20critical%20role%20in%20this%0Aphenomena.%20Consequently%2C%20despite%20the%20complex%20and%20varied%20nature%20of%20GM%0Adistributions%2C%20we%20demonstrate%20that%20neural%20networks%20exhibit%20asymptotic%20behaviors%0Ain%20line%20with%20predictions%20under%20simple%20Gaussian%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00642v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Empirical%20Observations%20to%20Universality%3A%20Dynamics%20of%20Deep%20Learning%0A%20%20with%20Inputs%20Built%20on%20Gaussian%20mixture&entry.906535625=Jaeyong%20Bae%20and%20Hawoong%20Jeong&entry.1292438233=%20%20This%20study%20broadens%20the%20scope%20of%20theoretical%20frameworks%20in%20deep%20learning%20by%0Adelving%20into%20the%20dynamics%20of%20neural%20networks%20with%20inputs%20that%20demonstrate%20the%0Astructural%20characteristics%20to%20Gaussian%20Mixture%20%28GM%29.%20We%20analyzed%20how%20the%0Adynamics%20of%20neural%20networks%20under%20GM-structured%20inputs%20diverge%20from%20the%0Apredictions%20of%20conventional%20theories%20based%20on%20simple%20Gaussian%20structures.%20A%0Arevelation%20of%20our%20work%20is%20the%20observed%20convergence%20of%20neural%20network%20dynamics%0Atowards%20conventional%20theory%20even%20with%20standardized%20GM%20inputs%2C%20highlighting%20an%0Aunexpected%20universality.%20We%20found%20that%20standardization%2C%20especially%20in%0Aconjunction%20with%20certain%20nonlinear%20functions%2C%20plays%20a%20critical%20role%20in%20this%0Aphenomena.%20Consequently%2C%20despite%20the%20complex%20and%20varied%20nature%20of%20GM%0Adistributions%2C%20we%20demonstrate%20that%20neural%20networks%20exhibit%20asymptotic%20behaviors%0Ain%20line%20with%20predictions%20under%20simple%20Gaussian%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00642v1&entry.124074799=Read"},
{"title": "FlightScope: A Deep Comprehensive Assessment of Aircraft Detection\n  Algorithms in Satellite Imagery", "author": "Safouane El Ghazouali and Arnaud Gucciardi and Nicola Venturi and Michael Rueegsegger and Umberto Michelucci", "abstract": "  Object detection in remotely sensed satellite pictures is fundamental in many\nfields such as biophysical, and environmental monitoring. While deep learning\nalgorithms are constantly evolving, they have been mostly implemented and\ntested on popular ground-based taken photos. This paper critically evaluates\nand compares a suite of advanced object detection algorithms customized for the\ntask of identifying aircraft within satellite imagery. Using the large\nHRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset,\nthis research encompasses an array of methodologies including YOLO versions 5\nand 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from\nscratch. This exhaustive training and validation study reveal YOLOv5 as the\npreeminent model for the specific case of identifying airplanes from remote\nsensing data, showcasing high precision and adaptability across diverse imaging\nconditions. This research highlight the nuanced performance landscapes of these\nalgorithms, with YOLOv5 emerging as a robust solution for aerial object\ndetection, underlining its importance through superior mean average precision,\nRecall, and Intersection over Union scores. The findings described here\nunderscore the fundamental role of algorithm selection aligned with the\nspecific demands of satellite imagery analysis and extend a comprehensive\nframework to evaluate model efficacy. The benchmark toolkit and codes,\navailable via https://github.com/toelt-llc/FlightScope_Bench, aims to further\nexploration and innovation in the realm of remote sensing object detection,\npaving the way for improved analytical methodologies in satellite imagery\napplications.\n", "link": "http://arxiv.org/abs/2404.02877v2", "date": "2024-05-01", "relevancy": 2.5215, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4982}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4976}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FlightScope%3A%20A%20Deep%20Comprehensive%20Assessment%20of%20Aircraft%20Detection%0A%20%20Algorithms%20in%20Satellite%20Imagery&body=Title%3A%20FlightScope%3A%20A%20Deep%20Comprehensive%20Assessment%20of%20Aircraft%20Detection%0A%20%20Algorithms%20in%20Satellite%20Imagery%0AAuthor%3A%20Safouane%20El%20Ghazouali%20and%20Arnaud%20Gucciardi%20and%20Nicola%20Venturi%20and%20Michael%20Rueegsegger%20and%20Umberto%20Michelucci%0AAbstract%3A%20%20%20Object%20detection%20in%20remotely%20sensed%20satellite%20pictures%20is%20fundamental%20in%20many%0Afields%20such%20as%20biophysical%2C%20and%20environmental%20monitoring.%20While%20deep%20learning%0Aalgorithms%20are%20constantly%20evolving%2C%20they%20have%20been%20mostly%20implemented%20and%0Atested%20on%20popular%20ground-based%20taken%20photos.%20This%20paper%20critically%20evaluates%0Aand%20compares%20a%20suite%20of%20advanced%20object%20detection%20algorithms%20customized%20for%20the%0Atask%20of%20identifying%20aircraft%20within%20satellite%20imagery.%20Using%20the%20large%0AHRPlanesV2%20dataset%2C%20together%20with%20a%20rigorous%20validation%20with%20the%20GDIT%20dataset%2C%0Athis%20research%20encompasses%20an%20array%20of%20methodologies%20including%20YOLO%20versions%205%0Aand%208%2C%20Faster%20RCNN%2C%20CenterNet%2C%20RetinaNet%2C%20RTMDet%2C%20and%20DETR%2C%20all%20trained%20from%0Ascratch.%20This%20exhaustive%20training%20and%20validation%20study%20reveal%20YOLOv5%20as%20the%0Apreeminent%20model%20for%20the%20specific%20case%20of%20identifying%20airplanes%20from%20remote%0Asensing%20data%2C%20showcasing%20high%20precision%20and%20adaptability%20across%20diverse%20imaging%0Aconditions.%20This%20research%20highlight%20the%20nuanced%20performance%20landscapes%20of%20these%0Aalgorithms%2C%20with%20YOLOv5%20emerging%20as%20a%20robust%20solution%20for%20aerial%20object%0Adetection%2C%20underlining%20its%20importance%20through%20superior%20mean%20average%20precision%2C%0ARecall%2C%20and%20Intersection%20over%20Union%20scores.%20The%20findings%20described%20here%0Aunderscore%20the%20fundamental%20role%20of%20algorithm%20selection%20aligned%20with%20the%0Aspecific%20demands%20of%20satellite%20imagery%20analysis%20and%20extend%20a%20comprehensive%0Aframework%20to%20evaluate%20model%20efficacy.%20The%20benchmark%20toolkit%20and%20codes%2C%0Aavailable%20via%20https%3A//github.com/toelt-llc/FlightScope_Bench%2C%20aims%20to%20further%0Aexploration%20and%20innovation%20in%20the%20realm%20of%20remote%20sensing%20object%20detection%2C%0Apaving%20the%20way%20for%20improved%20analytical%20methodologies%20in%20satellite%20imagery%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02877v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlightScope%3A%20A%20Deep%20Comprehensive%20Assessment%20of%20Aircraft%20Detection%0A%20%20Algorithms%20in%20Satellite%20Imagery&entry.906535625=Safouane%20El%20Ghazouali%20and%20Arnaud%20Gucciardi%20and%20Nicola%20Venturi%20and%20Michael%20Rueegsegger%20and%20Umberto%20Michelucci&entry.1292438233=%20%20Object%20detection%20in%20remotely%20sensed%20satellite%20pictures%20is%20fundamental%20in%20many%0Afields%20such%20as%20biophysical%2C%20and%20environmental%20monitoring.%20While%20deep%20learning%0Aalgorithms%20are%20constantly%20evolving%2C%20they%20have%20been%20mostly%20implemented%20and%0Atested%20on%20popular%20ground-based%20taken%20photos.%20This%20paper%20critically%20evaluates%0Aand%20compares%20a%20suite%20of%20advanced%20object%20detection%20algorithms%20customized%20for%20the%0Atask%20of%20identifying%20aircraft%20within%20satellite%20imagery.%20Using%20the%20large%0AHRPlanesV2%20dataset%2C%20together%20with%20a%20rigorous%20validation%20with%20the%20GDIT%20dataset%2C%0Athis%20research%20encompasses%20an%20array%20of%20methodologies%20including%20YOLO%20versions%205%0Aand%208%2C%20Faster%20RCNN%2C%20CenterNet%2C%20RetinaNet%2C%20RTMDet%2C%20and%20DETR%2C%20all%20trained%20from%0Ascratch.%20This%20exhaustive%20training%20and%20validation%20study%20reveal%20YOLOv5%20as%20the%0Apreeminent%20model%20for%20the%20specific%20case%20of%20identifying%20airplanes%20from%20remote%0Asensing%20data%2C%20showcasing%20high%20precision%20and%20adaptability%20across%20diverse%20imaging%0Aconditions.%20This%20research%20highlight%20the%20nuanced%20performance%20landscapes%20of%20these%0Aalgorithms%2C%20with%20YOLOv5%20emerging%20as%20a%20robust%20solution%20for%20aerial%20object%0Adetection%2C%20underlining%20its%20importance%20through%20superior%20mean%20average%20precision%2C%0ARecall%2C%20and%20Intersection%20over%20Union%20scores.%20The%20findings%20described%20here%0Aunderscore%20the%20fundamental%20role%20of%20algorithm%20selection%20aligned%20with%20the%0Aspecific%20demands%20of%20satellite%20imagery%20analysis%20and%20extend%20a%20comprehensive%0Aframework%20to%20evaluate%20model%20efficacy.%20The%20benchmark%20toolkit%20and%20codes%2C%0Aavailable%20via%20https%3A//github.com/toelt-llc/FlightScope_Bench%2C%20aims%20to%20further%0Aexploration%20and%20innovation%20in%20the%20realm%20of%20remote%20sensing%20object%20detection%2C%0Apaving%20the%20way%20for%20improved%20analytical%20methodologies%20in%20satellite%20imagery%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02877v2&entry.124074799=Read"},
{"title": "Feature-Aware Noise Contrastive Learning For Unsupervised Red Panda\n  Re-Identification", "author": "Jincheng Zhang and Qijun Zhao and Tie Liu", "abstract": "  To facilitate the re-identification (Re-ID) of individual animals, existing\nmethods primarily focus on maximizing feature similarity within the same\nindividual and enhancing distinctiveness between different individuals.\nHowever, most of them still rely on supervised learning and require substantial\nlabeled data, which is challenging to obtain. To avoid this issue, we propose a\nFeature-Aware Noise Contrastive Learning (FANCL) method to explore an\nunsupervised learning solution, which is then validated on the task of red\npanda re-ID. FANCL employs a Feature-Aware Noise Addition module to produce\nnoised images that conceal critical features and designs two contrastive\nlearning modules to calculate the losses. Firstly, a feature consistency module\nis designed to bridge the gap between the original and noised features.\nSecondly, the neural networks are trained through a cluster contrastive\nlearning module. Through these more challenging learning tasks, FANCL can\nadaptively extract deeper representations of red pandas. The experimental\nresults on a set of red panda images collected in both indoor and outdoor\nenvironments prove that FANCL outperforms several related state-of-the-art\nunsupervised methods, achieving high performance comparable to supervised\nlearning methods.\n", "link": "http://arxiv.org/abs/2405.00468v1", "date": "2024-05-01", "relevancy": 2.5008, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5028}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5018}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4959}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Feature-Aware%20Noise%20Contrastive%20Learning%20For%20Unsupervised%20Red%20Panda%0A%20%20Re-Identification&body=Title%3A%20Feature-Aware%20Noise%20Contrastive%20Learning%20For%20Unsupervised%20Red%20Panda%0A%20%20Re-Identification%0AAuthor%3A%20Jincheng%20Zhang%20and%20Qijun%20Zhao%20and%20Tie%20Liu%0AAbstract%3A%20%20%20To%20facilitate%20the%20re-identification%20%28Re-ID%29%20of%20individual%20animals%2C%20existing%0Amethods%20primarily%20focus%20on%20maximizing%20feature%20similarity%20within%20the%20same%0Aindividual%20and%20enhancing%20distinctiveness%20between%20different%20individuals.%0AHowever%2C%20most%20of%20them%20still%20rely%20on%20supervised%20learning%20and%20require%20substantial%0Alabeled%20data%2C%20which%20is%20challenging%20to%20obtain.%20To%20avoid%20this%20issue%2C%20we%20propose%20a%0AFeature-Aware%20Noise%20Contrastive%20Learning%20%28FANCL%29%20method%20to%20explore%20an%0Aunsupervised%20learning%20solution%2C%20which%20is%20then%20validated%20on%20the%20task%20of%20red%0Apanda%20re-ID.%20FANCL%20employs%20a%20Feature-Aware%20Noise%20Addition%20module%20to%20produce%0Anoised%20images%20that%20conceal%20critical%20features%20and%20designs%20two%20contrastive%0Alearning%20modules%20to%20calculate%20the%20losses.%20Firstly%2C%20a%20feature%20consistency%20module%0Ais%20designed%20to%20bridge%20the%20gap%20between%20the%20original%20and%20noised%20features.%0ASecondly%2C%20the%20neural%20networks%20are%20trained%20through%20a%20cluster%20contrastive%0Alearning%20module.%20Through%20these%20more%20challenging%20learning%20tasks%2C%20FANCL%20can%0Aadaptively%20extract%20deeper%20representations%20of%20red%20pandas.%20The%20experimental%0Aresults%20on%20a%20set%20of%20red%20panda%20images%20collected%20in%20both%20indoor%20and%20outdoor%0Aenvironments%20prove%20that%20FANCL%20outperforms%20several%20related%20state-of-the-art%0Aunsupervised%20methods%2C%20achieving%20high%20performance%20comparable%20to%20supervised%0Alearning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00468v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-Aware%20Noise%20Contrastive%20Learning%20For%20Unsupervised%20Red%20Panda%0A%20%20Re-Identification&entry.906535625=Jincheng%20Zhang%20and%20Qijun%20Zhao%20and%20Tie%20Liu&entry.1292438233=%20%20To%20facilitate%20the%20re-identification%20%28Re-ID%29%20of%20individual%20animals%2C%20existing%0Amethods%20primarily%20focus%20on%20maximizing%20feature%20similarity%20within%20the%20same%0Aindividual%20and%20enhancing%20distinctiveness%20between%20different%20individuals.%0AHowever%2C%20most%20of%20them%20still%20rely%20on%20supervised%20learning%20and%20require%20substantial%0Alabeled%20data%2C%20which%20is%20challenging%20to%20obtain.%20To%20avoid%20this%20issue%2C%20we%20propose%20a%0AFeature-Aware%20Noise%20Contrastive%20Learning%20%28FANCL%29%20method%20to%20explore%20an%0Aunsupervised%20learning%20solution%2C%20which%20is%20then%20validated%20on%20the%20task%20of%20red%0Apanda%20re-ID.%20FANCL%20employs%20a%20Feature-Aware%20Noise%20Addition%20module%20to%20produce%0Anoised%20images%20that%20conceal%20critical%20features%20and%20designs%20two%20contrastive%0Alearning%20modules%20to%20calculate%20the%20losses.%20Firstly%2C%20a%20feature%20consistency%20module%0Ais%20designed%20to%20bridge%20the%20gap%20between%20the%20original%20and%20noised%20features.%0ASecondly%2C%20the%20neural%20networks%20are%20trained%20through%20a%20cluster%20contrastive%0Alearning%20module.%20Through%20these%20more%20challenging%20learning%20tasks%2C%20FANCL%20can%0Aadaptively%20extract%20deeper%20representations%20of%20red%20pandas.%20The%20experimental%0Aresults%20on%20a%20set%20of%20red%20panda%20images%20collected%20in%20both%20indoor%20and%20outdoor%0Aenvironments%20prove%20that%20FANCL%20outperforms%20several%20related%20state-of-the-art%0Aunsupervised%20methods%2C%20achieving%20high%20performance%20comparable%20to%20supervised%0Alearning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00468v1&entry.124074799=Read"},
{"title": "WEST GCN-LSTM: Weighted Stacked Spatio-Temporal Graph Neural Networks\n  for Regional Traffic Forecasting", "author": "Theodoros Theodoropoulos and Angelos-Christos Maroudis and Antonios Makris and Konstantinos Tserpes", "abstract": "  Regional traffic forecasting is a critical challenge in urban mobility, with\napplications to various fields such as the Internet of Everything. In recent\nyears, spatio-temporal graph neural networks have achieved state-of-the-art\nresults in the context of numerous traffic forecasting challenges. This work\naims at expanding upon the conventional spatio-temporal graph neural network\narchitectures in a manner that may facilitate the inclusion of information\nregarding the examined regions, as well as the populations that traverse them,\nin order to establish a more efficient prediction model. The end-product of\nthis scientific endeavour is a novel spatio-temporal graph neural network\narchitecture that is referred to as WEST (WEighted STacked) GCN-LSTM.\nFurthermore, the inclusion of the aforementioned information is conducted via\nthe use of two novel dedicated algorithms that are referred to as the Shared\nBorders Policy and the Adjustable Hops Policy. Through information fusion and\ndistillation, the proposed solution manages to significantly outperform its\ncompetitors in the frame of an experimental evaluation that consists of 19\nforecasting models, across several datasets. Finally, an additional ablation\nstudy determined that each of the components of the proposed solution\ncontributes towards enhancing its overall performance.\n", "link": "http://arxiv.org/abs/2405.00570v1", "date": "2024-05-01", "relevancy": 2.4146, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4924}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.476}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WEST%20GCN-LSTM%3A%20Weighted%20Stacked%20Spatio-Temporal%20Graph%20Neural%20Networks%0A%20%20for%20Regional%20Traffic%20Forecasting&body=Title%3A%20WEST%20GCN-LSTM%3A%20Weighted%20Stacked%20Spatio-Temporal%20Graph%20Neural%20Networks%0A%20%20for%20Regional%20Traffic%20Forecasting%0AAuthor%3A%20Theodoros%20Theodoropoulos%20and%20Angelos-Christos%20Maroudis%20and%20Antonios%20Makris%20and%20Konstantinos%20Tserpes%0AAbstract%3A%20%20%20Regional%20traffic%20forecasting%20is%20a%20critical%20challenge%20in%20urban%20mobility%2C%20with%0Aapplications%20to%20various%20fields%20such%20as%20the%20Internet%20of%20Everything.%20In%20recent%0Ayears%2C%20spatio-temporal%20graph%20neural%20networks%20have%20achieved%20state-of-the-art%0Aresults%20in%20the%20context%20of%20numerous%20traffic%20forecasting%20challenges.%20This%20work%0Aaims%20at%20expanding%20upon%20the%20conventional%20spatio-temporal%20graph%20neural%20network%0Aarchitectures%20in%20a%20manner%20that%20may%20facilitate%20the%20inclusion%20of%20information%0Aregarding%20the%20examined%20regions%2C%20as%20well%20as%20the%20populations%20that%20traverse%20them%2C%0Ain%20order%20to%20establish%20a%20more%20efficient%20prediction%20model.%20The%20end-product%20of%0Athis%20scientific%20endeavour%20is%20a%20novel%20spatio-temporal%20graph%20neural%20network%0Aarchitecture%20that%20is%20referred%20to%20as%20WEST%20%28WEighted%20STacked%29%20GCN-LSTM.%0AFurthermore%2C%20the%20inclusion%20of%20the%20aforementioned%20information%20is%20conducted%20via%0Athe%20use%20of%20two%20novel%20dedicated%20algorithms%20that%20are%20referred%20to%20as%20the%20Shared%0ABorders%20Policy%20and%20the%20Adjustable%20Hops%20Policy.%20Through%20information%20fusion%20and%0Adistillation%2C%20the%20proposed%20solution%20manages%20to%20significantly%20outperform%20its%0Acompetitors%20in%20the%20frame%20of%20an%20experimental%20evaluation%20that%20consists%20of%2019%0Aforecasting%20models%2C%20across%20several%20datasets.%20Finally%2C%20an%20additional%20ablation%0Astudy%20determined%20that%20each%20of%20the%20components%20of%20the%20proposed%20solution%0Acontributes%20towards%20enhancing%20its%20overall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00570v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WEST%20GCN-LSTM%3A%20Weighted%20Stacked%20Spatio-Temporal%20Graph%20Neural%20Networks%0A%20%20for%20Regional%20Traffic%20Forecasting&entry.906535625=Theodoros%20Theodoropoulos%20and%20Angelos-Christos%20Maroudis%20and%20Antonios%20Makris%20and%20Konstantinos%20Tserpes&entry.1292438233=%20%20Regional%20traffic%20forecasting%20is%20a%20critical%20challenge%20in%20urban%20mobility%2C%20with%0Aapplications%20to%20various%20fields%20such%20as%20the%20Internet%20of%20Everything.%20In%20recent%0Ayears%2C%20spatio-temporal%20graph%20neural%20networks%20have%20achieved%20state-of-the-art%0Aresults%20in%20the%20context%20of%20numerous%20traffic%20forecasting%20challenges.%20This%20work%0Aaims%20at%20expanding%20upon%20the%20conventional%20spatio-temporal%20graph%20neural%20network%0Aarchitectures%20in%20a%20manner%20that%20may%20facilitate%20the%20inclusion%20of%20information%0Aregarding%20the%20examined%20regions%2C%20as%20well%20as%20the%20populations%20that%20traverse%20them%2C%0Ain%20order%20to%20establish%20a%20more%20efficient%20prediction%20model.%20The%20end-product%20of%0Athis%20scientific%20endeavour%20is%20a%20novel%20spatio-temporal%20graph%20neural%20network%0Aarchitecture%20that%20is%20referred%20to%20as%20WEST%20%28WEighted%20STacked%29%20GCN-LSTM.%0AFurthermore%2C%20the%20inclusion%20of%20the%20aforementioned%20information%20is%20conducted%20via%0Athe%20use%20of%20two%20novel%20dedicated%20algorithms%20that%20are%20referred%20to%20as%20the%20Shared%0ABorders%20Policy%20and%20the%20Adjustable%20Hops%20Policy.%20Through%20information%20fusion%20and%0Adistillation%2C%20the%20proposed%20solution%20manages%20to%20significantly%20outperform%20its%0Acompetitors%20in%20the%20frame%20of%20an%20experimental%20evaluation%20that%20consists%20of%2019%0Aforecasting%20models%2C%20across%20several%20datasets.%20Finally%2C%20an%20additional%20ablation%0Astudy%20determined%20that%20each%20of%20the%20components%20of%20the%20proposed%20solution%0Acontributes%20towards%20enhancing%20its%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00570v1&entry.124074799=Read"},
{"title": "Spectrally Pruned Gaussian Fields with Neural Compensation", "author": "Runyi Yang and Zhenxin Zhu and Zhou Jiang and Baijun Ye and Xiaoxue Chen and Yifei Zhang and Yuantao Chen and Jian Zhao and Hao Zhao", "abstract": "  Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered\nattention for its fast rendering speed and high rendering quality. However,\nthis comes with high memory consumption, e.g., a well-trained Gaussian field\nmay utilize three million Gaussian primitives and over 700 MB of memory. We\ncredit this high memory footprint to the lack of consideration for the\nrelationship between primitives. In this paper, we propose a memory-efficient\nGaussian field named SUNDAE with spectral pruning and neural compensation. On\none hand, we construct a graph on the set of Gaussian primitives to model their\nrelationship and design a spectral down-sampling module to prune out primitives\nwhile preserving desired signals. On the other hand, to compensate for the\nquality loss of pruning Gaussians, we exploit a lightweight neural network head\nto mix splatted features, which effectively compensates for quality losses\nwhile capturing the relationship between primitives in its weights. We\ndemonstrate the performance of SUNDAE with extensive results. For example,\nSUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla\nGaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB\nmemory, on the Mip-NeRF360 dataset. Codes are publicly available at\nhttps://runyiyang.github.io/projects/SUNDAE/.\n", "link": "http://arxiv.org/abs/2405.00676v1", "date": "2024-05-01", "relevancy": 2.4114, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6464}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6036}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4919}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spectrally%20Pruned%20Gaussian%20Fields%20with%20Neural%20Compensation&body=Title%3A%20Spectrally%20Pruned%20Gaussian%20Fields%20with%20Neural%20Compensation%0AAuthor%3A%20Runyi%20Yang%20and%20Zhenxin%20Zhu%20and%20Zhou%20Jiang%20and%20Baijun%20Ye%20and%20Xiaoxue%20Chen%20and%20Yifei%20Zhang%20and%20Yuantao%20Chen%20and%20Jian%20Zhao%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%2C%20as%20a%20novel%203D%20representation%2C%20has%20garnered%0Aattention%20for%20its%20fast%20rendering%20speed%20and%20high%20rendering%20quality.%20However%2C%0Athis%20comes%20with%20high%20memory%20consumption%2C%20e.g.%2C%20a%20well-trained%20Gaussian%20field%0Amay%20utilize%20three%20million%20Gaussian%20primitives%20and%20over%20700%20MB%20of%20memory.%20We%0Acredit%20this%20high%20memory%20footprint%20to%20the%20lack%20of%20consideration%20for%20the%0Arelationship%20between%20primitives.%20In%20this%20paper%2C%20we%20propose%20a%20memory-efficient%0AGaussian%20field%20named%20SUNDAE%20with%20spectral%20pruning%20and%20neural%20compensation.%20On%0Aone%20hand%2C%20we%20construct%20a%20graph%20on%20the%20set%20of%20Gaussian%20primitives%20to%20model%20their%0Arelationship%20and%20design%20a%20spectral%20down-sampling%20module%20to%20prune%20out%20primitives%0Awhile%20preserving%20desired%20signals.%20On%20the%20other%20hand%2C%20to%20compensate%20for%20the%0Aquality%20loss%20of%20pruning%20Gaussians%2C%20we%20exploit%20a%20lightweight%20neural%20network%20head%0Ato%20mix%20splatted%20features%2C%20which%20effectively%20compensates%20for%20quality%20losses%0Awhile%20capturing%20the%20relationship%20between%20primitives%20in%20its%20weights.%20We%0Ademonstrate%20the%20performance%20of%20SUNDAE%20with%20extensive%20results.%20For%20example%2C%0ASUNDAE%20can%20achieve%2026.80%20PSNR%20at%20145%20FPS%20using%20104%20MB%20memory%20while%20the%20vanilla%0AGaussian%20splatting%20algorithm%20achieves%2025.60%20PSNR%20at%20160%20FPS%20using%20523%20MB%0Amemory%2C%20on%20the%20Mip-NeRF360%20dataset.%20Codes%20are%20publicly%20available%20at%0Ahttps%3A//runyiyang.github.io/projects/SUNDAE/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00676v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectrally%20Pruned%20Gaussian%20Fields%20with%20Neural%20Compensation&entry.906535625=Runyi%20Yang%20and%20Zhenxin%20Zhu%20and%20Zhou%20Jiang%20and%20Baijun%20Ye%20and%20Xiaoxue%20Chen%20and%20Yifei%20Zhang%20and%20Yuantao%20Chen%20and%20Jian%20Zhao%20and%20Hao%20Zhao&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%2C%20as%20a%20novel%203D%20representation%2C%20has%20garnered%0Aattention%20for%20its%20fast%20rendering%20speed%20and%20high%20rendering%20quality.%20However%2C%0Athis%20comes%20with%20high%20memory%20consumption%2C%20e.g.%2C%20a%20well-trained%20Gaussian%20field%0Amay%20utilize%20three%20million%20Gaussian%20primitives%20and%20over%20700%20MB%20of%20memory.%20We%0Acredit%20this%20high%20memory%20footprint%20to%20the%20lack%20of%20consideration%20for%20the%0Arelationship%20between%20primitives.%20In%20this%20paper%2C%20we%20propose%20a%20memory-efficient%0AGaussian%20field%20named%20SUNDAE%20with%20spectral%20pruning%20and%20neural%20compensation.%20On%0Aone%20hand%2C%20we%20construct%20a%20graph%20on%20the%20set%20of%20Gaussian%20primitives%20to%20model%20their%0Arelationship%20and%20design%20a%20spectral%20down-sampling%20module%20to%20prune%20out%20primitives%0Awhile%20preserving%20desired%20signals.%20On%20the%20other%20hand%2C%20to%20compensate%20for%20the%0Aquality%20loss%20of%20pruning%20Gaussians%2C%20we%20exploit%20a%20lightweight%20neural%20network%20head%0Ato%20mix%20splatted%20features%2C%20which%20effectively%20compensates%20for%20quality%20losses%0Awhile%20capturing%20the%20relationship%20between%20primitives%20in%20its%20weights.%20We%0Ademonstrate%20the%20performance%20of%20SUNDAE%20with%20extensive%20results.%20For%20example%2C%0ASUNDAE%20can%20achieve%2026.80%20PSNR%20at%20145%20FPS%20using%20104%20MB%20memory%20while%20the%20vanilla%0AGaussian%20splatting%20algorithm%20achieves%2025.60%20PSNR%20at%20160%20FPS%20using%20523%20MB%0Amemory%2C%20on%20the%20Mip-NeRF360%20dataset.%20Codes%20are%20publicly%20available%20at%0Ahttps%3A//runyiyang.github.io/projects/SUNDAE/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00676v1&entry.124074799=Read"},
{"title": "Geometric Insights into Focal Loss: Reducing Curvature for Enhanced\n  Model Calibration", "author": "Masanari Kimura and Hiroki Naganuma", "abstract": "  The key factor in implementing machine learning algorithms in decision-making\nsituations is not only the accuracy of the model but also its confidence level.\nThe confidence level of a model in a classification problem is often given by\nthe output vector of a softmax function for convenience. However, these values\nare known to deviate significantly from the actual expected model confidence.\nThis problem is called model calibration and has been studied extensively. One\nof the simplest techniques to tackle this task is focal loss, a generalization\nof cross-entropy by introducing one positive parameter. Although many related\nstudies exist because of the simplicity of the idea and its formalization, the\ntheoretical analysis of its behavior is still insufficient. In this study, our\nobjective is to understand the behavior of focal loss by reinterpreting this\nfunction geometrically. Our analysis suggests that focal loss reduces the\ncurvature of the loss surface in training the model. This indicates that\ncurvature may be one of the essential factors in achieving model calibration.\nWe design numerical experiments to support this conjecture to reveal the\nbehavior of focal loss and the relationship between calibration performance and\ncurvature.\n", "link": "http://arxiv.org/abs/2405.00442v1", "date": "2024-05-01", "relevancy": 2.378, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4902}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4698}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4668}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Geometric%20Insights%20into%20Focal%20Loss%3A%20Reducing%20Curvature%20for%20Enhanced%0A%20%20Model%20Calibration&body=Title%3A%20Geometric%20Insights%20into%20Focal%20Loss%3A%20Reducing%20Curvature%20for%20Enhanced%0A%20%20Model%20Calibration%0AAuthor%3A%20Masanari%20Kimura%20and%20Hiroki%20Naganuma%0AAbstract%3A%20%20%20The%20key%20factor%20in%20implementing%20machine%20learning%20algorithms%20in%20decision-making%0Asituations%20is%20not%20only%20the%20accuracy%20of%20the%20model%20but%20also%20its%20confidence%20level.%0AThe%20confidence%20level%20of%20a%20model%20in%20a%20classification%20problem%20is%20often%20given%20by%0Athe%20output%20vector%20of%20a%20softmax%20function%20for%20convenience.%20However%2C%20these%20values%0Aare%20known%20to%20deviate%20significantly%20from%20the%20actual%20expected%20model%20confidence.%0AThis%20problem%20is%20called%20model%20calibration%20and%20has%20been%20studied%20extensively.%20One%0Aof%20the%20simplest%20techniques%20to%20tackle%20this%20task%20is%20focal%20loss%2C%20a%20generalization%0Aof%20cross-entropy%20by%20introducing%20one%20positive%20parameter.%20Although%20many%20related%0Astudies%20exist%20because%20of%20the%20simplicity%20of%20the%20idea%20and%20its%20formalization%2C%20the%0Atheoretical%20analysis%20of%20its%20behavior%20is%20still%20insufficient.%20In%20this%20study%2C%20our%0Aobjective%20is%20to%20understand%20the%20behavior%20of%20focal%20loss%20by%20reinterpreting%20this%0Afunction%20geometrically.%20Our%20analysis%20suggests%20that%20focal%20loss%20reduces%20the%0Acurvature%20of%20the%20loss%20surface%20in%20training%20the%20model.%20This%20indicates%20that%0Acurvature%20may%20be%20one%20of%20the%20essential%20factors%20in%20achieving%20model%20calibration.%0AWe%20design%20numerical%20experiments%20to%20support%20this%20conjecture%20to%20reveal%20the%0Abehavior%20of%20focal%20loss%20and%20the%20relationship%20between%20calibration%20performance%20and%0Acurvature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00442v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Insights%20into%20Focal%20Loss%3A%20Reducing%20Curvature%20for%20Enhanced%0A%20%20Model%20Calibration&entry.906535625=Masanari%20Kimura%20and%20Hiroki%20Naganuma&entry.1292438233=%20%20The%20key%20factor%20in%20implementing%20machine%20learning%20algorithms%20in%20decision-making%0Asituations%20is%20not%20only%20the%20accuracy%20of%20the%20model%20but%20also%20its%20confidence%20level.%0AThe%20confidence%20level%20of%20a%20model%20in%20a%20classification%20problem%20is%20often%20given%20by%0Athe%20output%20vector%20of%20a%20softmax%20function%20for%20convenience.%20However%2C%20these%20values%0Aare%20known%20to%20deviate%20significantly%20from%20the%20actual%20expected%20model%20confidence.%0AThis%20problem%20is%20called%20model%20calibration%20and%20has%20been%20studied%20extensively.%20One%0Aof%20the%20simplest%20techniques%20to%20tackle%20this%20task%20is%20focal%20loss%2C%20a%20generalization%0Aof%20cross-entropy%20by%20introducing%20one%20positive%20parameter.%20Although%20many%20related%0Astudies%20exist%20because%20of%20the%20simplicity%20of%20the%20idea%20and%20its%20formalization%2C%20the%0Atheoretical%20analysis%20of%20its%20behavior%20is%20still%20insufficient.%20In%20this%20study%2C%20our%0Aobjective%20is%20to%20understand%20the%20behavior%20of%20focal%20loss%20by%20reinterpreting%20this%0Afunction%20geometrically.%20Our%20analysis%20suggests%20that%20focal%20loss%20reduces%20the%0Acurvature%20of%20the%20loss%20surface%20in%20training%20the%20model.%20This%20indicates%20that%0Acurvature%20may%20be%20one%20of%20the%20essential%20factors%20in%20achieving%20model%20calibration.%0AWe%20design%20numerical%20experiments%20to%20support%20this%20conjecture%20to%20reveal%20the%0Abehavior%20of%20focal%20loss%20and%20the%20relationship%20between%20calibration%20performance%20and%0Acurvature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00442v1&entry.124074799=Read"},
{"title": "Radar-Based Localization For Autonomous Ground Vehicles In Suburban\n  Neighborhoods", "author": "Andrew J. Kramer and Christoffer Heckman", "abstract": "  For autonomous ground vehicles (AGVs) deployed in suburban neighborhoods and\nother human-centric environments the problem of localization remains a\nfundamental challenge. There are well established methods for localization with\nGPS, lidar, and cameras. But even in ideal conditions these have limitations.\nGPS is not always available and is often not accurate enough on its own, visual\nmethods have difficulty coping with appearance changes due to weather and other\nfactors, and lidar methods are prone to defective solutions due to ambiguous\nscene geometry. Radar on the other hand is not highly susceptible to these\nproblems, owing in part to its longer range. Further, radar is also robust to\nchallenging conditions that interfere with vision and lidar including fog,\nsmoke, rain, and darkness. We present a radar-based localization system that\nincludes a novel method for highly-accurate radar odometry for smooth,\nhigh-frequency relative pose estimation and a novel method for radar-based\nplace recognition and relocalization. We present experiments demonstrating our\nmethods' accuracy and reliability, which are comparable with \\new{other\nmethods' published results for radar localization and we find outperform a\nsimilar method as ours applied to lidar measurements}. Further, we show our\nmethods are lightweight enough to run on common low-power embedded hardware\nwith ample headroom for other autonomy functions.\n", "link": "http://arxiv.org/abs/2405.00600v1", "date": "2024-05-01", "relevancy": 2.2955, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5341}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Radar-Based%20Localization%20For%20Autonomous%20Ground%20Vehicles%20In%20Suburban%0A%20%20Neighborhoods&body=Title%3A%20Radar-Based%20Localization%20For%20Autonomous%20Ground%20Vehicles%20In%20Suburban%0A%20%20Neighborhoods%0AAuthor%3A%20Andrew%20J.%20Kramer%20and%20Christoffer%20Heckman%0AAbstract%3A%20%20%20For%20autonomous%20ground%20vehicles%20%28AGVs%29%20deployed%20in%20suburban%20neighborhoods%20and%0Aother%20human-centric%20environments%20the%20problem%20of%20localization%20remains%20a%0Afundamental%20challenge.%20There%20are%20well%20established%20methods%20for%20localization%20with%0AGPS%2C%20lidar%2C%20and%20cameras.%20But%20even%20in%20ideal%20conditions%20these%20have%20limitations.%0AGPS%20is%20not%20always%20available%20and%20is%20often%20not%20accurate%20enough%20on%20its%20own%2C%20visual%0Amethods%20have%20difficulty%20coping%20with%20appearance%20changes%20due%20to%20weather%20and%20other%0Afactors%2C%20and%20lidar%20methods%20are%20prone%20to%20defective%20solutions%20due%20to%20ambiguous%0Ascene%20geometry.%20Radar%20on%20the%20other%20hand%20is%20not%20highly%20susceptible%20to%20these%0Aproblems%2C%20owing%20in%20part%20to%20its%20longer%20range.%20Further%2C%20radar%20is%20also%20robust%20to%0Achallenging%20conditions%20that%20interfere%20with%20vision%20and%20lidar%20including%20fog%2C%0Asmoke%2C%20rain%2C%20and%20darkness.%20We%20present%20a%20radar-based%20localization%20system%20that%0Aincludes%20a%20novel%20method%20for%20highly-accurate%20radar%20odometry%20for%20smooth%2C%0Ahigh-frequency%20relative%20pose%20estimation%20and%20a%20novel%20method%20for%20radar-based%0Aplace%20recognition%20and%20relocalization.%20We%20present%20experiments%20demonstrating%20our%0Amethods%27%20accuracy%20and%20reliability%2C%20which%20are%20comparable%20with%20%5Cnew%7Bother%0Amethods%27%20published%20results%20for%20radar%20localization%20and%20we%20find%20outperform%20a%0Asimilar%20method%20as%20ours%20applied%20to%20lidar%20measurements%7D.%20Further%2C%20we%20show%20our%0Amethods%20are%20lightweight%20enough%20to%20run%20on%20common%20low-power%20embedded%20hardware%0Awith%20ample%20headroom%20for%20other%20autonomy%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00600v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radar-Based%20Localization%20For%20Autonomous%20Ground%20Vehicles%20In%20Suburban%0A%20%20Neighborhoods&entry.906535625=Andrew%20J.%20Kramer%20and%20Christoffer%20Heckman&entry.1292438233=%20%20For%20autonomous%20ground%20vehicles%20%28AGVs%29%20deployed%20in%20suburban%20neighborhoods%20and%0Aother%20human-centric%20environments%20the%20problem%20of%20localization%20remains%20a%0Afundamental%20challenge.%20There%20are%20well%20established%20methods%20for%20localization%20with%0AGPS%2C%20lidar%2C%20and%20cameras.%20But%20even%20in%20ideal%20conditions%20these%20have%20limitations.%0AGPS%20is%20not%20always%20available%20and%20is%20often%20not%20accurate%20enough%20on%20its%20own%2C%20visual%0Amethods%20have%20difficulty%20coping%20with%20appearance%20changes%20due%20to%20weather%20and%20other%0Afactors%2C%20and%20lidar%20methods%20are%20prone%20to%20defective%20solutions%20due%20to%20ambiguous%0Ascene%20geometry.%20Radar%20on%20the%20other%20hand%20is%20not%20highly%20susceptible%20to%20these%0Aproblems%2C%20owing%20in%20part%20to%20its%20longer%20range.%20Further%2C%20radar%20is%20also%20robust%20to%0Achallenging%20conditions%20that%20interfere%20with%20vision%20and%20lidar%20including%20fog%2C%0Asmoke%2C%20rain%2C%20and%20darkness.%20We%20present%20a%20radar-based%20localization%20system%20that%0Aincludes%20a%20novel%20method%20for%20highly-accurate%20radar%20odometry%20for%20smooth%2C%0Ahigh-frequency%20relative%20pose%20estimation%20and%20a%20novel%20method%20for%20radar-based%0Aplace%20recognition%20and%20relocalization.%20We%20present%20experiments%20demonstrating%20our%0Amethods%27%20accuracy%20and%20reliability%2C%20which%20are%20comparable%20with%20%5Cnew%7Bother%0Amethods%27%20published%20results%20for%20radar%20localization%20and%20we%20find%20outperform%20a%0Asimilar%20method%20as%20ours%20applied%20to%20lidar%20measurements%7D.%20Further%2C%20we%20show%20our%0Amethods%20are%20lightweight%20enough%20to%20run%20on%20common%20low-power%20embedded%20hardware%0Awith%20ample%20headroom%20for%20other%20autonomy%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00600v1&entry.124074799=Read"},
{"title": "SeaTurtleID2022: A long-span dataset for reliable sea turtle\n  re-identification", "author": "Luk\u00e1\u0161 Adam and Vojt\u011bch \u010cerm\u00e1k and Kostas Papafitsoros and Luk\u00e1\u0161 Picek", "abstract": "  This paper introduces the first public large-scale, long-span dataset with\nsea turtle photographs captured in the wild --\n\\href{https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022}{SeaTurtleID2022}.\nThe dataset contains 8729 photographs of 438 unique individuals collected\nwithin 13 years, making it the longest-spanned dataset for animal\nre-identification. All photographs include various annotations, e.g., identity,\nencounter timestamp, and body parts segmentation masks. Instead of standard\n\"random\" splits, the dataset allows for two realistic and ecologically\nmotivated splits: (i) a \\textit{time-aware closed-set} with training,\nvalidation, and test data from different days/years, and (ii) a\n\\textit{time-aware open-set} with new unknown individuals in test and\nvalidation sets. We show that time-aware splits are essential for benchmarking\nre-identification methods, as random splits lead to performance overestimation.\nFurthermore, a baseline instance segmentation and re-identification performance\nover various body parts is provided. Finally, an end-to-end system for sea\nturtle re-identification is proposed and evaluated. The proposed system based\non Hybrid Task Cascade for head instance segmentation and ArcFace-trained\nfeature-extractor achieved an accuracy of 86.8\\%.\n", "link": "http://arxiv.org/abs/2211.10307v4", "date": "2024-05-01", "relevancy": 2.2704, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4729}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4455}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4438}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification&body=Title%3A%20SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification%0AAuthor%3A%20Luk%C3%A1%C5%A1%20Adam%20and%20Vojt%C4%9Bch%20%C4%8Cerm%C3%A1k%20and%20Kostas%20Papafitsoros%20and%20Luk%C3%A1%C5%A1%20Picek%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20first%20public%20large-scale%2C%20long-span%20dataset%20with%0Asea%20turtle%20photographs%20captured%20in%20the%20wild%20--%0A%5Chref%7Bhttps%3A//www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022%7D%7BSeaTurtleID2022%7D.%0AThe%20dataset%20contains%208729%20photographs%20of%20438%20unique%20individuals%20collected%0Awithin%2013%20years%2C%20making%20it%20the%20longest-spanned%20dataset%20for%20animal%0Are-identification.%20All%20photographs%20include%20various%20annotations%2C%20e.g.%2C%20identity%2C%0Aencounter%20timestamp%2C%20and%20body%20parts%20segmentation%20masks.%20Instead%20of%20standard%0A%22random%22%20splits%2C%20the%20dataset%20allows%20for%20two%20realistic%20and%20ecologically%0Amotivated%20splits%3A%20%28i%29%20a%20%5Ctextit%7Btime-aware%20closed-set%7D%20with%20training%2C%0Avalidation%2C%20and%20test%20data%20from%20different%20days/years%2C%20and%20%28ii%29%20a%0A%5Ctextit%7Btime-aware%20open-set%7D%20with%20new%20unknown%20individuals%20in%20test%20and%0Avalidation%20sets.%20We%20show%20that%20time-aware%20splits%20are%20essential%20for%20benchmarking%0Are-identification%20methods%2C%20as%20random%20splits%20lead%20to%20performance%20overestimation.%0AFurthermore%2C%20a%20baseline%20instance%20segmentation%20and%20re-identification%20performance%0Aover%20various%20body%20parts%20is%20provided.%20Finally%2C%20an%20end-to-end%20system%20for%20sea%0Aturtle%20re-identification%20is%20proposed%20and%20evaluated.%20The%20proposed%20system%20based%0Aon%20Hybrid%20Task%20Cascade%20for%20head%20instance%20segmentation%20and%20ArcFace-trained%0Afeature-extractor%20achieved%20an%20accuracy%20of%2086.8%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.10307v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification&entry.906535625=Luk%C3%A1%C5%A1%20Adam%20and%20Vojt%C4%9Bch%20%C4%8Cerm%C3%A1k%20and%20Kostas%20Papafitsoros%20and%20Luk%C3%A1%C5%A1%20Picek&entry.1292438233=%20%20This%20paper%20introduces%20the%20first%20public%20large-scale%2C%20long-span%20dataset%20with%0Asea%20turtle%20photographs%20captured%20in%20the%20wild%20--%0A%5Chref%7Bhttps%3A//www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022%7D%7BSeaTurtleID2022%7D.%0AThe%20dataset%20contains%208729%20photographs%20of%20438%20unique%20individuals%20collected%0Awithin%2013%20years%2C%20making%20it%20the%20longest-spanned%20dataset%20for%20animal%0Are-identification.%20All%20photographs%20include%20various%20annotations%2C%20e.g.%2C%20identity%2C%0Aencounter%20timestamp%2C%20and%20body%20parts%20segmentation%20masks.%20Instead%20of%20standard%0A%22random%22%20splits%2C%20the%20dataset%20allows%20for%20two%20realistic%20and%20ecologically%0Amotivated%20splits%3A%20%28i%29%20a%20%5Ctextit%7Btime-aware%20closed-set%7D%20with%20training%2C%0Avalidation%2C%20and%20test%20data%20from%20different%20days/years%2C%20and%20%28ii%29%20a%0A%5Ctextit%7Btime-aware%20open-set%7D%20with%20new%20unknown%20individuals%20in%20test%20and%0Avalidation%20sets.%20We%20show%20that%20time-aware%20splits%20are%20essential%20for%20benchmarking%0Are-identification%20methods%2C%20as%20random%20splits%20lead%20to%20performance%20overestimation.%0AFurthermore%2C%20a%20baseline%20instance%20segmentation%20and%20re-identification%20performance%0Aover%20various%20body%20parts%20is%20provided.%20Finally%2C%20an%20end-to-end%20system%20for%20sea%0Aturtle%20re-identification%20is%20proposed%20and%20evaluated.%20The%20proposed%20system%20based%0Aon%20Hybrid%20Task%20Cascade%20for%20head%20instance%20segmentation%20and%20ArcFace-trained%0Afeature-extractor%20achieved%20an%20accuracy%20of%2086.8%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.10307v4&entry.124074799=Read"},
{"title": "Lazy Layers to Make Fine-Tuned Diffusion Models More Traceable", "author": "Haozhe Liu and Wentian Zhang and Bing Li and Bernard Ghanem and J\u00fcrgen Schmidhuber", "abstract": "  Foundational generative models should be traceable to protect their owners\nand facilitate safety regulation. To achieve this, traditional approaches embed\nidentifiers based on supervisory trigger-response signals, which are commonly\nknown as backdoor watermarks. They are prone to failure when the model is\nfine-tuned with nontrigger data. Our experiments show that this vulnerability\nis due to energetic changes in only a few 'busy' layers during fine-tuning.\nThis yields a novel arbitrary-in-arbitrary-out (AIAO) strategy that makes\nwatermarks resilient to fine-tuning-based removal. The trigger-response pairs\nof AIAO samples across various neural network depths can be used to construct\nwatermarked subpaths, employing Monte Carlo sampling to achieve stable\nverification results. In addition, unlike the existing methods of designing a\nbackdoor for the input/output space of diffusion models, in our method, we\npropose to embed the backdoor into the feature space of sampled subpaths, where\na mask-controlled trigger function is proposed to preserve the generation\nperformance and ensure the invisibility of the embedded backdoor. Our empirical\nstudies on the MS-COCO, AFHQ, LSUN, CUB-200, and DreamBooth datasets confirm\nthe robustness of AIAO; while the verification rates of other trigger-based\nmethods fall from ~90% to ~70% after fine-tuning, those of our method remain\nconsistently above 90%.\n", "link": "http://arxiv.org/abs/2405.00466v1", "date": "2024-05-01", "relevancy": 2.2659, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5852}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.565}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5483}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lazy%20Layers%20to%20Make%20Fine-Tuned%20Diffusion%20Models%20More%20Traceable&body=Title%3A%20Lazy%20Layers%20to%20Make%20Fine-Tuned%20Diffusion%20Models%20More%20Traceable%0AAuthor%3A%20Haozhe%20Liu%20and%20Wentian%20Zhang%20and%20Bing%20Li%20and%20Bernard%20Ghanem%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Foundational%20generative%20models%20should%20be%20traceable%20to%20protect%20their%20owners%0Aand%20facilitate%20safety%20regulation.%20To%20achieve%20this%2C%20traditional%20approaches%20embed%0Aidentifiers%20based%20on%20supervisory%20trigger-response%20signals%2C%20which%20are%20commonly%0Aknown%20as%20backdoor%20watermarks.%20They%20are%20prone%20to%20failure%20when%20the%20model%20is%0Afine-tuned%20with%20nontrigger%20data.%20Our%20experiments%20show%20that%20this%20vulnerability%0Ais%20due%20to%20energetic%20changes%20in%20only%20a%20few%20%27busy%27%20layers%20during%20fine-tuning.%0AThis%20yields%20a%20novel%20arbitrary-in-arbitrary-out%20%28AIAO%29%20strategy%20that%20makes%0Awatermarks%20resilient%20to%20fine-tuning-based%20removal.%20The%20trigger-response%20pairs%0Aof%20AIAO%20samples%20across%20various%20neural%20network%20depths%20can%20be%20used%20to%20construct%0Awatermarked%20subpaths%2C%20employing%20Monte%20Carlo%20sampling%20to%20achieve%20stable%0Averification%20results.%20In%20addition%2C%20unlike%20the%20existing%20methods%20of%20designing%20a%0Abackdoor%20for%20the%20input/output%20space%20of%20diffusion%20models%2C%20in%20our%20method%2C%20we%0Apropose%20to%20embed%20the%20backdoor%20into%20the%20feature%20space%20of%20sampled%20subpaths%2C%20where%0Aa%20mask-controlled%20trigger%20function%20is%20proposed%20to%20preserve%20the%20generation%0Aperformance%20and%20ensure%20the%20invisibility%20of%20the%20embedded%20backdoor.%20Our%20empirical%0Astudies%20on%20the%20MS-COCO%2C%20AFHQ%2C%20LSUN%2C%20CUB-200%2C%20and%20DreamBooth%20datasets%20confirm%0Athe%20robustness%20of%20AIAO%3B%20while%20the%20verification%20rates%20of%20other%20trigger-based%0Amethods%20fall%20from%20~90%25%20to%20~70%25%20after%20fine-tuning%2C%20those%20of%20our%20method%20remain%0Aconsistently%20above%2090%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00466v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lazy%20Layers%20to%20Make%20Fine-Tuned%20Diffusion%20Models%20More%20Traceable&entry.906535625=Haozhe%20Liu%20and%20Wentian%20Zhang%20and%20Bing%20Li%20and%20Bernard%20Ghanem%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Foundational%20generative%20models%20should%20be%20traceable%20to%20protect%20their%20owners%0Aand%20facilitate%20safety%20regulation.%20To%20achieve%20this%2C%20traditional%20approaches%20embed%0Aidentifiers%20based%20on%20supervisory%20trigger-response%20signals%2C%20which%20are%20commonly%0Aknown%20as%20backdoor%20watermarks.%20They%20are%20prone%20to%20failure%20when%20the%20model%20is%0Afine-tuned%20with%20nontrigger%20data.%20Our%20experiments%20show%20that%20this%20vulnerability%0Ais%20due%20to%20energetic%20changes%20in%20only%20a%20few%20%27busy%27%20layers%20during%20fine-tuning.%0AThis%20yields%20a%20novel%20arbitrary-in-arbitrary-out%20%28AIAO%29%20strategy%20that%20makes%0Awatermarks%20resilient%20to%20fine-tuning-based%20removal.%20The%20trigger-response%20pairs%0Aof%20AIAO%20samples%20across%20various%20neural%20network%20depths%20can%20be%20used%20to%20construct%0Awatermarked%20subpaths%2C%20employing%20Monte%20Carlo%20sampling%20to%20achieve%20stable%0Averification%20results.%20In%20addition%2C%20unlike%20the%20existing%20methods%20of%20designing%20a%0Abackdoor%20for%20the%20input/output%20space%20of%20diffusion%20models%2C%20in%20our%20method%2C%20we%0Apropose%20to%20embed%20the%20backdoor%20into%20the%20feature%20space%20of%20sampled%20subpaths%2C%20where%0Aa%20mask-controlled%20trigger%20function%20is%20proposed%20to%20preserve%20the%20generation%0Aperformance%20and%20ensure%20the%20invisibility%20of%20the%20embedded%20backdoor.%20Our%20empirical%0Astudies%20on%20the%20MS-COCO%2C%20AFHQ%2C%20LSUN%2C%20CUB-200%2C%20and%20DreamBooth%20datasets%20confirm%0Athe%20robustness%20of%20AIAO%3B%20while%20the%20verification%20rates%20of%20other%20trigger-based%0Amethods%20fall%20from%20~90%25%20to%20~70%25%20after%20fine-tuning%2C%20those%20of%20our%20method%20remain%0Aconsistently%20above%2090%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00466v1&entry.124074799=Read"},
{"title": "RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document\n  Abstractive Summarization", "author": "Dongqi Pu and Vera Demberg", "abstract": "  For long document summarization, discourse structure is important to discern\nthe key content of the text and the differences in importance level between\nsentences. Unfortunately, the integration of rhetorical structure theory (RST)\ninto parameter-efficient fine-tuning strategies for long document summarization\nremains unexplored. Therefore, this paper introduces RST-LoRA and proposes four\nRST-aware variants to explicitly incorporate RST into the LoRA model. Our\nempirical evaluation demonstrates that incorporating the type and uncertainty\nof rhetorical relations can complementarily enhance the performance of LoRA in\nsummarization tasks. Furthermore, the best-performing variant we introduced\noutperforms the vanilla LoRA and full-parameter fine-tuning models, as\nconfirmed by multiple automatic and human evaluations, and even surpasses\nprevious state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.00657v1", "date": "2024-05-01", "relevancy": 2.2461, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.437}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RST-LoRA%3A%20A%20Discourse-Aware%20Low-Rank%20Adaptation%20for%20Long%20Document%0A%20%20Abstractive%20Summarization&body=Title%3A%20RST-LoRA%3A%20A%20Discourse-Aware%20Low-Rank%20Adaptation%20for%20Long%20Document%0A%20%20Abstractive%20Summarization%0AAuthor%3A%20Dongqi%20Pu%20and%20Vera%20Demberg%0AAbstract%3A%20%20%20For%20long%20document%20summarization%2C%20discourse%20structure%20is%20important%20to%20discern%0Athe%20key%20content%20of%20the%20text%20and%20the%20differences%20in%20importance%20level%20between%0Asentences.%20Unfortunately%2C%20the%20integration%20of%20rhetorical%20structure%20theory%20%28RST%29%0Ainto%20parameter-efficient%20fine-tuning%20strategies%20for%20long%20document%20summarization%0Aremains%20unexplored.%20Therefore%2C%20this%20paper%20introduces%20RST-LoRA%20and%20proposes%20four%0ARST-aware%20variants%20to%20explicitly%20incorporate%20RST%20into%20the%20LoRA%20model.%20Our%0Aempirical%20evaluation%20demonstrates%20that%20incorporating%20the%20type%20and%20uncertainty%0Aof%20rhetorical%20relations%20can%20complementarily%20enhance%20the%20performance%20of%20LoRA%20in%0Asummarization%20tasks.%20Furthermore%2C%20the%20best-performing%20variant%20we%20introduced%0Aoutperforms%20the%20vanilla%20LoRA%20and%20full-parameter%20fine-tuning%20models%2C%20as%0Aconfirmed%20by%20multiple%20automatic%20and%20human%20evaluations%2C%20and%20even%20surpasses%0Aprevious%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00657v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RST-LoRA%3A%20A%20Discourse-Aware%20Low-Rank%20Adaptation%20for%20Long%20Document%0A%20%20Abstractive%20Summarization&entry.906535625=Dongqi%20Pu%20and%20Vera%20Demberg&entry.1292438233=%20%20For%20long%20document%20summarization%2C%20discourse%20structure%20is%20important%20to%20discern%0Athe%20key%20content%20of%20the%20text%20and%20the%20differences%20in%20importance%20level%20between%0Asentences.%20Unfortunately%2C%20the%20integration%20of%20rhetorical%20structure%20theory%20%28RST%29%0Ainto%20parameter-efficient%20fine-tuning%20strategies%20for%20long%20document%20summarization%0Aremains%20unexplored.%20Therefore%2C%20this%20paper%20introduces%20RST-LoRA%20and%20proposes%20four%0ARST-aware%20variants%20to%20explicitly%20incorporate%20RST%20into%20the%20LoRA%20model.%20Our%0Aempirical%20evaluation%20demonstrates%20that%20incorporating%20the%20type%20and%20uncertainty%0Aof%20rhetorical%20relations%20can%20complementarily%20enhance%20the%20performance%20of%20LoRA%20in%0Asummarization%20tasks.%20Furthermore%2C%20the%20best-performing%20variant%20we%20introduced%0Aoutperforms%20the%20vanilla%20LoRA%20and%20full-parameter%20fine-tuning%20models%2C%20as%0Aconfirmed%20by%20multiple%20automatic%20and%20human%20evaluations%2C%20and%20even%20surpasses%0Aprevious%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00657v1&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Dynamic Graph Neural Networks: Models,\n  Frameworks, Benchmarks, Experiments and Challenges", "author": "ZhengZhao Feng and Rui Wang and TianXing Wang and Mingli Song and Sai Wu and Shuibing He", "abstract": "  Dynamic Graph Neural Networks (GNNs) combine temporal information with GNNs\nto capture structural, temporal, and contextual relationships in dynamic graphs\nsimultaneously, leading to enhanced performance in various applications. As the\ndemand for dynamic GNNs continues to grow, numerous models and frameworks have\nemerged to cater to different application needs. There is a pressing need for a\ncomprehensive survey that evaluates the performance, strengths, and limitations\nof various approaches in this domain. This paper aims to fill this gap by\noffering a thorough comparative analysis and experimental evaluation of dynamic\nGNNs. It covers 81 dynamic GNN models with a novel taxonomy, 12 dynamic GNN\ntraining frameworks, and commonly used benchmarks. We also conduct experimental\nresults from testing representative nine dynamic GNN models and three\nframeworks on six standard graph datasets. Evaluation metrics focus on\nconvergence accuracy, training efficiency, and GPU memory usage, enabling a\nthorough comparison of performance across various models and frameworks. From\nthe analysis and evaluation results, we identify key challenges and offer\nprinciples for future research to enhance the design of models and frameworks\nin the dynamic GNNs field.\n", "link": "http://arxiv.org/abs/2405.00476v1", "date": "2024-05-01", "relevancy": 2.2093, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5642}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5616}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4994}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20of%20Dynamic%20Graph%20Neural%20Networks%3A%20Models%2C%0A%20%20Frameworks%2C%20Benchmarks%2C%20Experiments%20and%20Challenges&body=Title%3A%20A%20Comprehensive%20Survey%20of%20Dynamic%20Graph%20Neural%20Networks%3A%20Models%2C%0A%20%20Frameworks%2C%20Benchmarks%2C%20Experiments%20and%20Challenges%0AAuthor%3A%20ZhengZhao%20Feng%20and%20Rui%20Wang%20and%20TianXing%20Wang%20and%20Mingli%20Song%20and%20Sai%20Wu%20and%20Shuibing%20He%0AAbstract%3A%20%20%20Dynamic%20Graph%20Neural%20Networks%20%28GNNs%29%20combine%20temporal%20information%20with%20GNNs%0Ato%20capture%20structural%2C%20temporal%2C%20and%20contextual%20relationships%20in%20dynamic%20graphs%0Asimultaneously%2C%20leading%20to%20enhanced%20performance%20in%20various%20applications.%20As%20the%0Ademand%20for%20dynamic%20GNNs%20continues%20to%20grow%2C%20numerous%20models%20and%20frameworks%20have%0Aemerged%20to%20cater%20to%20different%20application%20needs.%20There%20is%20a%20pressing%20need%20for%20a%0Acomprehensive%20survey%20that%20evaluates%20the%20performance%2C%20strengths%2C%20and%20limitations%0Aof%20various%20approaches%20in%20this%20domain.%20This%20paper%20aims%20to%20fill%20this%20gap%20by%0Aoffering%20a%20thorough%20comparative%20analysis%20and%20experimental%20evaluation%20of%20dynamic%0AGNNs.%20It%20covers%2081%20dynamic%20GNN%20models%20with%20a%20novel%20taxonomy%2C%2012%20dynamic%20GNN%0Atraining%20frameworks%2C%20and%20commonly%20used%20benchmarks.%20We%20also%20conduct%20experimental%0Aresults%20from%20testing%20representative%20nine%20dynamic%20GNN%20models%20and%20three%0Aframeworks%20on%20six%20standard%20graph%20datasets.%20Evaluation%20metrics%20focus%20on%0Aconvergence%20accuracy%2C%20training%20efficiency%2C%20and%20GPU%20memory%20usage%2C%20enabling%20a%0Athorough%20comparison%20of%20performance%20across%20various%20models%20and%20frameworks.%20From%0Athe%20analysis%20and%20evaluation%20results%2C%20we%20identify%20key%20challenges%20and%20offer%0Aprinciples%20for%20future%20research%20to%20enhance%20the%20design%20of%20models%20and%20frameworks%0Ain%20the%20dynamic%20GNNs%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00476v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Dynamic%20Graph%20Neural%20Networks%3A%20Models%2C%0A%20%20Frameworks%2C%20Benchmarks%2C%20Experiments%20and%20Challenges&entry.906535625=ZhengZhao%20Feng%20and%20Rui%20Wang%20and%20TianXing%20Wang%20and%20Mingli%20Song%20and%20Sai%20Wu%20and%20Shuibing%20He&entry.1292438233=%20%20Dynamic%20Graph%20Neural%20Networks%20%28GNNs%29%20combine%20temporal%20information%20with%20GNNs%0Ato%20capture%20structural%2C%20temporal%2C%20and%20contextual%20relationships%20in%20dynamic%20graphs%0Asimultaneously%2C%20leading%20to%20enhanced%20performance%20in%20various%20applications.%20As%20the%0Ademand%20for%20dynamic%20GNNs%20continues%20to%20grow%2C%20numerous%20models%20and%20frameworks%20have%0Aemerged%20to%20cater%20to%20different%20application%20needs.%20There%20is%20a%20pressing%20need%20for%20a%0Acomprehensive%20survey%20that%20evaluates%20the%20performance%2C%20strengths%2C%20and%20limitations%0Aof%20various%20approaches%20in%20this%20domain.%20This%20paper%20aims%20to%20fill%20this%20gap%20by%0Aoffering%20a%20thorough%20comparative%20analysis%20and%20experimental%20evaluation%20of%20dynamic%0AGNNs.%20It%20covers%2081%20dynamic%20GNN%20models%20with%20a%20novel%20taxonomy%2C%2012%20dynamic%20GNN%0Atraining%20frameworks%2C%20and%20commonly%20used%20benchmarks.%20We%20also%20conduct%20experimental%0Aresults%20from%20testing%20representative%20nine%20dynamic%20GNN%20models%20and%20three%0Aframeworks%20on%20six%20standard%20graph%20datasets.%20Evaluation%20metrics%20focus%20on%0Aconvergence%20accuracy%2C%20training%20efficiency%2C%20and%20GPU%20memory%20usage%2C%20enabling%20a%0Athorough%20comparison%20of%20performance%20across%20various%20models%20and%20frameworks.%20From%0Athe%20analysis%20and%20evaluation%20results%2C%20we%20identify%20key%20challenges%20and%20offer%0Aprinciples%20for%20future%20research%20to%20enhance%20the%20design%20of%20models%20and%20frameworks%0Ain%20the%20dynamic%20GNNs%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00476v1&entry.124074799=Read"},
{"title": "Federated Transfer Component Analysis Towards Effective VNF Profiling", "author": "Xunzheng Zhang and Shadi Moazzeni and Juan Marcelo Parra-Ullauri and Reza Nejabati and Dimitra Simeonidou", "abstract": "  The increasing concerns of knowledge transfer and data privacy challenge the\ntraditional gather-and-analyse paradigm in networks. Specifically, the\nintelligent orchestration of Virtual Network Functions (VNFs) requires\nunderstanding and profiling the resource consumption. However, profiling all\nkinds of VNFs is time-consuming. It is important to consider transferring the\nwell-profiled VNF knowledge to other lack-profiled VNF types while keeping data\nprivate. To this end, this paper proposes a Federated Transfer Component\nAnalysis (FTCA) method between the source and target VNFs. FTCA first trains\nGenerative Adversarial Networks (GANs) based on the source VNF profiling data,\nand the trained GANs model is sent to the target VNF domain. Then, FTCA\nrealizes federated domain adaptation by using the generated source VNF data and\nless target VNF profiling data, while keeping the raw data locally. Experiments\nshow that the proposed FTCA can effectively predict the required resources for\nthe target VNF. Specifically, the RMSE index of the regression model decreases\nby 38.5% and the R-squared metric advances up to 68.6%.\n", "link": "http://arxiv.org/abs/2404.17553v2", "date": "2024-05-01", "relevancy": 2.2072, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4398}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Federated%20Transfer%20Component%20Analysis%20Towards%20Effective%20VNF%20Profiling&body=Title%3A%20Federated%20Transfer%20Component%20Analysis%20Towards%20Effective%20VNF%20Profiling%0AAuthor%3A%20Xunzheng%20Zhang%20and%20Shadi%20Moazzeni%20and%20Juan%20Marcelo%20Parra-Ullauri%20and%20Reza%20Nejabati%20and%20Dimitra%20Simeonidou%0AAbstract%3A%20%20%20The%20increasing%20concerns%20of%20knowledge%20transfer%20and%20data%20privacy%20challenge%20the%0Atraditional%20gather-and-analyse%20paradigm%20in%20networks.%20Specifically%2C%20the%0Aintelligent%20orchestration%20of%20Virtual%20Network%20Functions%20%28VNFs%29%20requires%0Aunderstanding%20and%20profiling%20the%20resource%20consumption.%20However%2C%20profiling%20all%0Akinds%20of%20VNFs%20is%20time-consuming.%20It%20is%20important%20to%20consider%20transferring%20the%0Awell-profiled%20VNF%20knowledge%20to%20other%20lack-profiled%20VNF%20types%20while%20keeping%20data%0Aprivate.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20Federated%20Transfer%20Component%0AAnalysis%20%28FTCA%29%20method%20between%20the%20source%20and%20target%20VNFs.%20FTCA%20first%20trains%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20based%20on%20the%20source%20VNF%20profiling%20data%2C%0Aand%20the%20trained%20GANs%20model%20is%20sent%20to%20the%20target%20VNF%20domain.%20Then%2C%20FTCA%0Arealizes%20federated%20domain%20adaptation%20by%20using%20the%20generated%20source%20VNF%20data%20and%0Aless%20target%20VNF%20profiling%20data%2C%20while%20keeping%20the%20raw%20data%20locally.%20Experiments%0Ashow%20that%20the%20proposed%20FTCA%20can%20effectively%20predict%20the%20required%20resources%20for%0Athe%20target%20VNF.%20Specifically%2C%20the%20RMSE%20index%20of%20the%20regression%20model%20decreases%0Aby%2038.5%25%20and%20the%20R-squared%20metric%20advances%20up%20to%2068.6%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17553v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Transfer%20Component%20Analysis%20Towards%20Effective%20VNF%20Profiling&entry.906535625=Xunzheng%20Zhang%20and%20Shadi%20Moazzeni%20and%20Juan%20Marcelo%20Parra-Ullauri%20and%20Reza%20Nejabati%20and%20Dimitra%20Simeonidou&entry.1292438233=%20%20The%20increasing%20concerns%20of%20knowledge%20transfer%20and%20data%20privacy%20challenge%20the%0Atraditional%20gather-and-analyse%20paradigm%20in%20networks.%20Specifically%2C%20the%0Aintelligent%20orchestration%20of%20Virtual%20Network%20Functions%20%28VNFs%29%20requires%0Aunderstanding%20and%20profiling%20the%20resource%20consumption.%20However%2C%20profiling%20all%0Akinds%20of%20VNFs%20is%20time-consuming.%20It%20is%20important%20to%20consider%20transferring%20the%0Awell-profiled%20VNF%20knowledge%20to%20other%20lack-profiled%20VNF%20types%20while%20keeping%20data%0Aprivate.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20Federated%20Transfer%20Component%0AAnalysis%20%28FTCA%29%20method%20between%20the%20source%20and%20target%20VNFs.%20FTCA%20first%20trains%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20based%20on%20the%20source%20VNF%20profiling%20data%2C%0Aand%20the%20trained%20GANs%20model%20is%20sent%20to%20the%20target%20VNF%20domain.%20Then%2C%20FTCA%0Arealizes%20federated%20domain%20adaptation%20by%20using%20the%20generated%20source%20VNF%20data%20and%0Aless%20target%20VNF%20profiling%20data%2C%20while%20keeping%20the%20raw%20data%20locally.%20Experiments%0Ashow%20that%20the%20proposed%20FTCA%20can%20effectively%20predict%20the%20required%20resources%20for%0Athe%20target%20VNF.%20Specifically%2C%20the%20RMSE%20index%20of%20the%20regression%20model%20decreases%0Aby%2038.5%25%20and%20the%20R-squared%20metric%20advances%20up%20to%2068.6%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17553v2&entry.124074799=Read"},
{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "author": "Kai He and Kaixin Yao and Qixuan Zhang and Lingjie Liu and Jingyi Yu and Lan Xu", "abstract": "  Apparel's significant role in human appearance underscores the importance of\ngarment digitalization for digital human creation. Recent advances in 3D\ncontent creation are pivotal for digital human creation. Nonetheless, garment\ngeneration from text guidance is still nascent. We introduce a text-driven 3D\ngarment generation framework, DressCode, which aims to democratize design for\nnovices and offer immense potential in fashion design, virtual try-on, and\ndigital human creation. We first introduce SewingGPT, a GPT-based architecture\nintegrating cross-attention with text-conditioned embedding to generate sewing\npatterns with text guidance. We then tailor a pre-trained Stable Diffusion to\ngenerate tile-based Physically-based Rendering (PBR) textures for the garments.\nBy leveraging a large language model, our framework generates CG-friendly\ngarments through natural language interaction. It also facilitates pattern\ncompletion and texture editing, streamlining the design process through\nuser-friendly interaction. This framework fosters innovation by allowing\ncreators to freely experiment with designs and incorporate unique elements into\ntheir work. With comprehensive evaluations and comparisons with other\nstate-of-the-art methods, our method showcases superior quality and alignment\nwith input prompts. User studies further validate our high-quality rendering\nresults, highlighting its practical utility and potential in production\nsettings. Our project page is https://IHe-KaiI.github.io/DressCode/.\n", "link": "http://arxiv.org/abs/2401.16465v3", "date": "2024-05-01", "relevancy": 2.1975, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6046}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5395}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5372}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DressCode%3A%20Autoregressively%20Sewing%20and%20Generating%20Garments%20from%20Text%0A%20%20Guidance&body=Title%3A%20DressCode%3A%20Autoregressively%20Sewing%20and%20Generating%20Garments%20from%20Text%0A%20%20Guidance%0AAuthor%3A%20Kai%20He%20and%20Kaixin%20Yao%20and%20Qixuan%20Zhang%20and%20Lingjie%20Liu%20and%20Jingyi%20Yu%20and%20Lan%20Xu%0AAbstract%3A%20%20%20Apparel%27s%20significant%20role%20in%20human%20appearance%20underscores%20the%20importance%20of%0Agarment%20digitalization%20for%20digital%20human%20creation.%20Recent%20advances%20in%203D%0Acontent%20creation%20are%20pivotal%20for%20digital%20human%20creation.%20Nonetheless%2C%20garment%0Ageneration%20from%20text%20guidance%20is%20still%20nascent.%20We%20introduce%20a%20text-driven%203D%0Agarment%20generation%20framework%2C%20DressCode%2C%20which%20aims%20to%20democratize%20design%20for%0Anovices%20and%20offer%20immense%20potential%20in%20fashion%20design%2C%20virtual%20try-on%2C%20and%0Adigital%20human%20creation.%20We%20first%20introduce%20SewingGPT%2C%20a%20GPT-based%20architecture%0Aintegrating%20cross-attention%20with%20text-conditioned%20embedding%20to%20generate%20sewing%0Apatterns%20with%20text%20guidance.%20We%20then%20tailor%20a%20pre-trained%20Stable%20Diffusion%20to%0Agenerate%20tile-based%20Physically-based%20Rendering%20%28PBR%29%20textures%20for%20the%20garments.%0ABy%20leveraging%20a%20large%20language%20model%2C%20our%20framework%20generates%20CG-friendly%0Agarments%20through%20natural%20language%20interaction.%20It%20also%20facilitates%20pattern%0Acompletion%20and%20texture%20editing%2C%20streamlining%20the%20design%20process%20through%0Auser-friendly%20interaction.%20This%20framework%20fosters%20innovation%20by%20allowing%0Acreators%20to%20freely%20experiment%20with%20designs%20and%20incorporate%20unique%20elements%20into%0Atheir%20work.%20With%20comprehensive%20evaluations%20and%20comparisons%20with%20other%0Astate-of-the-art%20methods%2C%20our%20method%20showcases%20superior%20quality%20and%20alignment%0Awith%20input%20prompts.%20User%20studies%20further%20validate%20our%20high-quality%20rendering%0Aresults%2C%20highlighting%20its%20practical%20utility%20and%20potential%20in%20production%0Asettings.%20Our%20project%20page%20is%20https%3A//IHe-KaiI.github.io/DressCode/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16465v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DressCode%3A%20Autoregressively%20Sewing%20and%20Generating%20Garments%20from%20Text%0A%20%20Guidance&entry.906535625=Kai%20He%20and%20Kaixin%20Yao%20and%20Qixuan%20Zhang%20and%20Lingjie%20Liu%20and%20Jingyi%20Yu%20and%20Lan%20Xu&entry.1292438233=%20%20Apparel%27s%20significant%20role%20in%20human%20appearance%20underscores%20the%20importance%20of%0Agarment%20digitalization%20for%20digital%20human%20creation.%20Recent%20advances%20in%203D%0Acontent%20creation%20are%20pivotal%20for%20digital%20human%20creation.%20Nonetheless%2C%20garment%0Ageneration%20from%20text%20guidance%20is%20still%20nascent.%20We%20introduce%20a%20text-driven%203D%0Agarment%20generation%20framework%2C%20DressCode%2C%20which%20aims%20to%20democratize%20design%20for%0Anovices%20and%20offer%20immense%20potential%20in%20fashion%20design%2C%20virtual%20try-on%2C%20and%0Adigital%20human%20creation.%20We%20first%20introduce%20SewingGPT%2C%20a%20GPT-based%20architecture%0Aintegrating%20cross-attention%20with%20text-conditioned%20embedding%20to%20generate%20sewing%0Apatterns%20with%20text%20guidance.%20We%20then%20tailor%20a%20pre-trained%20Stable%20Diffusion%20to%0Agenerate%20tile-based%20Physically-based%20Rendering%20%28PBR%29%20textures%20for%20the%20garments.%0ABy%20leveraging%20a%20large%20language%20model%2C%20our%20framework%20generates%20CG-friendly%0Agarments%20through%20natural%20language%20interaction.%20It%20also%20facilitates%20pattern%0Acompletion%20and%20texture%20editing%2C%20streamlining%20the%20design%20process%20through%0Auser-friendly%20interaction.%20This%20framework%20fosters%20innovation%20by%20allowing%0Acreators%20to%20freely%20experiment%20with%20designs%20and%20incorporate%20unique%20elements%20into%0Atheir%20work.%20With%20comprehensive%20evaluations%20and%20comparisons%20with%20other%0Astate-of-the-art%20methods%2C%20our%20method%20showcases%20superior%20quality%20and%20alignment%0Awith%20input%20prompts.%20User%20studies%20further%20validate%20our%20high-quality%20rendering%0Aresults%2C%20highlighting%20its%20practical%20utility%20and%20potential%20in%20production%0Asettings.%20Our%20project%20page%20is%20https%3A//IHe-KaiI.github.io/DressCode/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16465v3&entry.124074799=Read"},
{"title": "Depth Priors in Removal Neural Radiance Fields", "author": "Zhihao Guo and Peng Wang", "abstract": "  Neural Radiance Fields (NeRF) have shown impressive results in 3D\nreconstruction and generating novel views. A key challenge within NeRF is the\nediting of reconstructed scenes, such as object removal, which requires\nmaintaining consistency across multiple views and ensuring high-quality\nsynthesised perspectives. Previous studies have incorporated depth priors,\ntypically from LiDAR or sparse depth measurements provided by COLMAP, to\nimprove the performance of object removal in NeRF. However, these methods are\neither costly or time-consuming. In this paper, we propose a novel approach\nthat integrates monocular depth estimates with NeRF-based object removal models\nto significantly reduce time consumption and enhance the robustness and quality\nof scene generation and object removal. We conducted a thorough evaluation of\nCOLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy\nin depth map generation. Our findings suggest that COLMAP can serve as an\neffective alternative to a ground truth depth map where such information is\nmissing or costly to obtain. Additionally, we integrated various monocular\ndepth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess\ntheir capacity to improve object removal performance. Our experimental results\nhighlight the potential of monocular depth estimation to substantially improve\nNeRF applications.\n", "link": "http://arxiv.org/abs/2405.00630v1", "date": "2024-05-01", "relevancy": 2.193, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.559}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5428}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5397}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Depth%20Priors%20in%20Removal%20Neural%20Radiance%20Fields&body=Title%3A%20Depth%20Priors%20in%20Removal%20Neural%20Radiance%20Fields%0AAuthor%3A%20Zhihao%20Guo%20and%20Peng%20Wang%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20shown%20impressive%20results%20in%203D%0Areconstruction%20and%20generating%20novel%20views.%20A%20key%20challenge%20within%20NeRF%20is%20the%0Aediting%20of%20reconstructed%20scenes%2C%20such%20as%20object%20removal%2C%20which%20requires%0Amaintaining%20consistency%20across%20multiple%20views%20and%20ensuring%20high-quality%0Asynthesised%20perspectives.%20Previous%20studies%20have%20incorporated%20depth%20priors%2C%0Atypically%20from%20LiDAR%20or%20sparse%20depth%20measurements%20provided%20by%20COLMAP%2C%20to%0Aimprove%20the%20performance%20of%20object%20removal%20in%20NeRF.%20However%2C%20these%20methods%20are%0Aeither%20costly%20or%20time-consuming.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%0Athat%20integrates%20monocular%20depth%20estimates%20with%20NeRF-based%20object%20removal%20models%0Ato%20significantly%20reduce%20time%20consumption%20and%20enhance%20the%20robustness%20and%20quality%0Aof%20scene%20generation%20and%20object%20removal.%20We%20conducted%20a%20thorough%20evaluation%20of%0ACOLMAP%27s%20dense%20depth%20reconstruction%20on%20the%20KITTI%20dataset%20to%20verify%20its%20accuracy%0Ain%20depth%20map%20generation.%20Our%20findings%20suggest%20that%20COLMAP%20can%20serve%20as%20an%0Aeffective%20alternative%20to%20a%20ground%20truth%20depth%20map%20where%20such%20information%20is%0Amissing%20or%20costly%20to%20obtain.%20Additionally%2C%20we%20integrated%20various%20monocular%0Adepth%20estimation%20methods%20into%20the%20removal%20NeRF%20model%2C%20i.e.%2C%20SpinNeRF%2C%20to%20assess%0Atheir%20capacity%20to%20improve%20object%20removal%20performance.%20Our%20experimental%20results%0Ahighlight%20the%20potential%20of%20monocular%20depth%20estimation%20to%20substantially%20improve%0ANeRF%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00630v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Priors%20in%20Removal%20Neural%20Radiance%20Fields&entry.906535625=Zhihao%20Guo%20and%20Peng%20Wang&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20shown%20impressive%20results%20in%203D%0Areconstruction%20and%20generating%20novel%20views.%20A%20key%20challenge%20within%20NeRF%20is%20the%0Aediting%20of%20reconstructed%20scenes%2C%20such%20as%20object%20removal%2C%20which%20requires%0Amaintaining%20consistency%20across%20multiple%20views%20and%20ensuring%20high-quality%0Asynthesised%20perspectives.%20Previous%20studies%20have%20incorporated%20depth%20priors%2C%0Atypically%20from%20LiDAR%20or%20sparse%20depth%20measurements%20provided%20by%20COLMAP%2C%20to%0Aimprove%20the%20performance%20of%20object%20removal%20in%20NeRF.%20However%2C%20these%20methods%20are%0Aeither%20costly%20or%20time-consuming.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%0Athat%20integrates%20monocular%20depth%20estimates%20with%20NeRF-based%20object%20removal%20models%0Ato%20significantly%20reduce%20time%20consumption%20and%20enhance%20the%20robustness%20and%20quality%0Aof%20scene%20generation%20and%20object%20removal.%20We%20conducted%20a%20thorough%20evaluation%20of%0ACOLMAP%27s%20dense%20depth%20reconstruction%20on%20the%20KITTI%20dataset%20to%20verify%20its%20accuracy%0Ain%20depth%20map%20generation.%20Our%20findings%20suggest%20that%20COLMAP%20can%20serve%20as%20an%0Aeffective%20alternative%20to%20a%20ground%20truth%20depth%20map%20where%20such%20information%20is%0Amissing%20or%20costly%20to%20obtain.%20Additionally%2C%20we%20integrated%20various%20monocular%0Adepth%20estimation%20methods%20into%20the%20removal%20NeRF%20model%2C%20i.e.%2C%20SpinNeRF%2C%20to%20assess%0Atheir%20capacity%20to%20improve%20object%20removal%20performance.%20Our%20experimental%20results%0Ahighlight%20the%20potential%20of%20monocular%20depth%20estimation%20to%20substantially%20improve%0ANeRF%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00630v1&entry.124074799=Read"},
{"title": "GraCo: Granularity-Controllable Interactive Segmentation", "author": "Yian Zhao and Kehan Li and Zesen Cheng and Pengchong Qiao and Xiawu Zheng and Rongrong Ji and Chang Liu and Li Yuan and Jie Chen", "abstract": "  Interactive Segmentation (IS) segments specific objects or parts in the image\naccording to user input. Current IS pipelines fall into two categories:\nsingle-granularity output and multi-granularity output. The latter aims to\nalleviate the spatial ambiguity present in the former. However, the\nmulti-granularity output pipeline suffers from limited interaction flexibility\nand produces redundant results. In this work, we introduce\nGranularity-Controllable Interactive Segmentation (GraCo), a novel approach\nthat allows precise control of prediction granularity by introducing additional\nparameters to input. This enhances the customization of the interactive system\nand eliminates redundancy while resolving ambiguity. Nevertheless, the\nexorbitant cost of annotating multi-granularity masks and the lack of available\ndatasets with granularity annotations make it difficult for models to acquire\nthe necessary guidance to control output granularity. To address this problem,\nwe design an any-granularity mask generator that exploits the semantic property\nof the pre-trained IS model to automatically generate abundant mask-granularity\npairs without requiring additional manual annotation. Based on these pairs, we\npropose a granularity-controllable learning strategy that efficiently imparts\nthe granularity controllability to the IS model. Extensive experiments on\nintricate scenarios at object and part levels demonstrate that our GraCo has\nsignificant advantages over previous methods. This highlights the potential of\nGraCo to be a flexible annotation tool, capable of adapting to diverse\nsegmentation scenarios. The project page: https://zhao-yian.github.io/GraCo.\n", "link": "http://arxiv.org/abs/2405.00587v1", "date": "2024-05-01", "relevancy": 2.1918, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5621}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5501}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5329}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GraCo%3A%20Granularity-Controllable%20Interactive%20Segmentation&body=Title%3A%20GraCo%3A%20Granularity-Controllable%20Interactive%20Segmentation%0AAuthor%3A%20Yian%20Zhao%20and%20Kehan%20Li%20and%20Zesen%20Cheng%20and%20Pengchong%20Qiao%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Chang%20Liu%20and%20Li%20Yuan%20and%20Jie%20Chen%0AAbstract%3A%20%20%20Interactive%20Segmentation%20%28IS%29%20segments%20specific%20objects%20or%20parts%20in%20the%20image%0Aaccording%20to%20user%20input.%20Current%20IS%20pipelines%20fall%20into%20two%20categories%3A%0Asingle-granularity%20output%20and%20multi-granularity%20output.%20The%20latter%20aims%20to%0Aalleviate%20the%20spatial%20ambiguity%20present%20in%20the%20former.%20However%2C%20the%0Amulti-granularity%20output%20pipeline%20suffers%20from%20limited%20interaction%20flexibility%0Aand%20produces%20redundant%20results.%20In%20this%20work%2C%20we%20introduce%0AGranularity-Controllable%20Interactive%20Segmentation%20%28GraCo%29%2C%20a%20novel%20approach%0Athat%20allows%20precise%20control%20of%20prediction%20granularity%20by%20introducing%20additional%0Aparameters%20to%20input.%20This%20enhances%20the%20customization%20of%20the%20interactive%20system%0Aand%20eliminates%20redundancy%20while%20resolving%20ambiguity.%20Nevertheless%2C%20the%0Aexorbitant%20cost%20of%20annotating%20multi-granularity%20masks%20and%20the%20lack%20of%20available%0Adatasets%20with%20granularity%20annotations%20make%20it%20difficult%20for%20models%20to%20acquire%0Athe%20necessary%20guidance%20to%20control%20output%20granularity.%20To%20address%20this%20problem%2C%0Awe%20design%20an%20any-granularity%20mask%20generator%20that%20exploits%20the%20semantic%20property%0Aof%20the%20pre-trained%20IS%20model%20to%20automatically%20generate%20abundant%20mask-granularity%0Apairs%20without%20requiring%20additional%20manual%20annotation.%20Based%20on%20these%20pairs%2C%20we%0Apropose%20a%20granularity-controllable%20learning%20strategy%20that%20efficiently%20imparts%0Athe%20granularity%20controllability%20to%20the%20IS%20model.%20Extensive%20experiments%20on%0Aintricate%20scenarios%20at%20object%20and%20part%20levels%20demonstrate%20that%20our%20GraCo%20has%0Asignificant%20advantages%20over%20previous%20methods.%20This%20highlights%20the%20potential%20of%0AGraCo%20to%20be%20a%20flexible%20annotation%20tool%2C%20capable%20of%20adapting%20to%20diverse%0Asegmentation%20scenarios.%20The%20project%20page%3A%20https%3A//zhao-yian.github.io/GraCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00587v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraCo%3A%20Granularity-Controllable%20Interactive%20Segmentation&entry.906535625=Yian%20Zhao%20and%20Kehan%20Li%20and%20Zesen%20Cheng%20and%20Pengchong%20Qiao%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Chang%20Liu%20and%20Li%20Yuan%20and%20Jie%20Chen&entry.1292438233=%20%20Interactive%20Segmentation%20%28IS%29%20segments%20specific%20objects%20or%20parts%20in%20the%20image%0Aaccording%20to%20user%20input.%20Current%20IS%20pipelines%20fall%20into%20two%20categories%3A%0Asingle-granularity%20output%20and%20multi-granularity%20output.%20The%20latter%20aims%20to%0Aalleviate%20the%20spatial%20ambiguity%20present%20in%20the%20former.%20However%2C%20the%0Amulti-granularity%20output%20pipeline%20suffers%20from%20limited%20interaction%20flexibility%0Aand%20produces%20redundant%20results.%20In%20this%20work%2C%20we%20introduce%0AGranularity-Controllable%20Interactive%20Segmentation%20%28GraCo%29%2C%20a%20novel%20approach%0Athat%20allows%20precise%20control%20of%20prediction%20granularity%20by%20introducing%20additional%0Aparameters%20to%20input.%20This%20enhances%20the%20customization%20of%20the%20interactive%20system%0Aand%20eliminates%20redundancy%20while%20resolving%20ambiguity.%20Nevertheless%2C%20the%0Aexorbitant%20cost%20of%20annotating%20multi-granularity%20masks%20and%20the%20lack%20of%20available%0Adatasets%20with%20granularity%20annotations%20make%20it%20difficult%20for%20models%20to%20acquire%0Athe%20necessary%20guidance%20to%20control%20output%20granularity.%20To%20address%20this%20problem%2C%0Awe%20design%20an%20any-granularity%20mask%20generator%20that%20exploits%20the%20semantic%20property%0Aof%20the%20pre-trained%20IS%20model%20to%20automatically%20generate%20abundant%20mask-granularity%0Apairs%20without%20requiring%20additional%20manual%20annotation.%20Based%20on%20these%20pairs%2C%20we%0Apropose%20a%20granularity-controllable%20learning%20strategy%20that%20efficiently%20imparts%0Athe%20granularity%20controllability%20to%20the%20IS%20model.%20Extensive%20experiments%20on%0Aintricate%20scenarios%20at%20object%20and%20part%20levels%20demonstrate%20that%20our%20GraCo%20has%0Asignificant%20advantages%20over%20previous%20methods.%20This%20highlights%20the%20potential%20of%0AGraCo%20to%20be%20a%20flexible%20annotation%20tool%2C%20capable%20of%20adapting%20to%20diverse%0Asegmentation%20scenarios.%20The%20project%20page%3A%20https%3A//zhao-yian.github.io/GraCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00587v1&entry.124074799=Read"},
{"title": "Derivative-based regularization for regression", "author": "Enrico Lopedoto and Maksim Shekhunov and Vitaly Aksenov and Kizito Salako and Tillman Weyde", "abstract": "  In this work, we introduce a novel approach to regularization in\nmultivariable regression problems. Our regularizer, called DLoss, penalises\ndifferences between the model's derivatives and derivatives of the data\ngenerating function as estimated from the training data. We call these\nestimated derivatives data derivatives. The goal of our method is to align the\nmodel to the data, not only in terms of target values but also in terms of the\nderivatives involved. To estimate data derivatives, we select (from the\ntraining data) 2-tuples of input-value pairs, using either nearest neighbour or\nrandom, selection. On synthetic and real datasets, we evaluate the\neffectiveness of adding DLoss, with different weights, to the standard mean\nsquared error loss. The experimental results show that with DLoss (using\nnearest neighbour selection) we obtain, on average, the best rank with respect\nto MSE on validation data sets, compared to no regularization, L2\nregularization, and Dropout.\n", "link": "http://arxiv.org/abs/2405.00555v1", "date": "2024-05-01", "relevancy": 2.1852, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4446}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4346}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4319}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Derivative-based%20regularization%20for%20regression&body=Title%3A%20Derivative-based%20regularization%20for%20regression%0AAuthor%3A%20Enrico%20Lopedoto%20and%20Maksim%20Shekhunov%20and%20Vitaly%20Aksenov%20and%20Kizito%20Salako%20and%20Tillman%20Weyde%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20to%20regularization%20in%0Amultivariable%20regression%20problems.%20Our%20regularizer%2C%20called%20DLoss%2C%20penalises%0Adifferences%20between%20the%20model%27s%20derivatives%20and%20derivatives%20of%20the%20data%0Agenerating%20function%20as%20estimated%20from%20the%20training%20data.%20We%20call%20these%0Aestimated%20derivatives%20data%20derivatives.%20The%20goal%20of%20our%20method%20is%20to%20align%20the%0Amodel%20to%20the%20data%2C%20not%20only%20in%20terms%20of%20target%20values%20but%20also%20in%20terms%20of%20the%0Aderivatives%20involved.%20To%20estimate%20data%20derivatives%2C%20we%20select%20%28from%20the%0Atraining%20data%29%202-tuples%20of%20input-value%20pairs%2C%20using%20either%20nearest%20neighbour%20or%0Arandom%2C%20selection.%20On%20synthetic%20and%20real%20datasets%2C%20we%20evaluate%20the%0Aeffectiveness%20of%20adding%20DLoss%2C%20with%20different%20weights%2C%20to%20the%20standard%20mean%0Asquared%20error%20loss.%20The%20experimental%20results%20show%20that%20with%20DLoss%20%28using%0Anearest%20neighbour%20selection%29%20we%20obtain%2C%20on%20average%2C%20the%20best%20rank%20with%20respect%0Ato%20MSE%20on%20validation%20data%20sets%2C%20compared%20to%20no%20regularization%2C%20L2%0Aregularization%2C%20and%20Dropout.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00555v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Derivative-based%20regularization%20for%20regression&entry.906535625=Enrico%20Lopedoto%20and%20Maksim%20Shekhunov%20and%20Vitaly%20Aksenov%20and%20Kizito%20Salako%20and%20Tillman%20Weyde&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20to%20regularization%20in%0Amultivariable%20regression%20problems.%20Our%20regularizer%2C%20called%20DLoss%2C%20penalises%0Adifferences%20between%20the%20model%27s%20derivatives%20and%20derivatives%20of%20the%20data%0Agenerating%20function%20as%20estimated%20from%20the%20training%20data.%20We%20call%20these%0Aestimated%20derivatives%20data%20derivatives.%20The%20goal%20of%20our%20method%20is%20to%20align%20the%0Amodel%20to%20the%20data%2C%20not%20only%20in%20terms%20of%20target%20values%20but%20also%20in%20terms%20of%20the%0Aderivatives%20involved.%20To%20estimate%20data%20derivatives%2C%20we%20select%20%28from%20the%0Atraining%20data%29%202-tuples%20of%20input-value%20pairs%2C%20using%20either%20nearest%20neighbour%20or%0Arandom%2C%20selection.%20On%20synthetic%20and%20real%20datasets%2C%20we%20evaluate%20the%0Aeffectiveness%20of%20adding%20DLoss%2C%20with%20different%20weights%2C%20to%20the%20standard%20mean%0Asquared%20error%20loss.%20The%20experimental%20results%20show%20that%20with%20DLoss%20%28using%0Anearest%20neighbour%20selection%29%20we%20obtain%2C%20on%20average%2C%20the%20best%20rank%20with%20respect%0Ato%20MSE%20on%20validation%20data%20sets%2C%20compared%20to%20no%20regularization%2C%20L2%0Aregularization%2C%20and%20Dropout.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00555v1&entry.124074799=Read"},
{"title": "The R2D2 deep neural network series paradigm for fast precision imaging\n  in radio astronomy", "author": "Amir Aghabiglou and Chung San Chu and Arwa Dabbech and Yves Wiaux", "abstract": "  Radio-interferometric (RI) imaging entails solving high-resolution\nhigh-dynamic range inverse problems from large data volumes. Recent image\nreconstruction techniques grounded in optimization theory have demonstrated\nremarkable capability for imaging precision, well beyond CLEAN's capability.\nThese range from advanced proximal algorithms propelled by handcrafted\nregularization operators, such as the SARA family, to hybrid plug-and-play\n(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.\nOptimization and PnP structures are however highly iterative, which hinders\ntheir ability to handle the extreme data sizes expected from future\ninstruments. To address this scalability challenge, we introduce a novel deep\nlearning approach, dubbed \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging\". R2D2's reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration's image estimate and associated data residual as inputs. It\nthus takes a hybrid structure between a PnP algorithm and a learned version of\nthe matching pursuit algorithm that underpins CLEAN. We present a comprehensive\nstudy of our approach, featuring its multiple incarnations distinguished by\ntheir DNN architectures. We provide a detailed description of its training\nprocess, targeting a telescope-specific approach. R2D2's capability to deliver\nhigh precision is demonstrated in simulation, across a variety of image and\nobservation settings using the Very Large Array (VLA). Its reconstruction speed\nis also demonstrated: with only few iterations required to clean data residuals\nat dynamic ranges up to 100000, R2D2 opens the door to fast precision imaging.\nR2D2 codes are available in the BASPLib library on GitHub.\n", "link": "http://arxiv.org/abs/2403.05452v3", "date": "2024-05-01", "relevancy": 2.1747, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5755}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5385}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5139}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20R2D2%20deep%20neural%20network%20series%20paradigm%20for%20fast%20precision%20imaging%0A%20%20in%20radio%20astronomy&body=Title%3A%20The%20R2D2%20deep%20neural%20network%20series%20paradigm%20for%20fast%20precision%20imaging%0A%20%20in%20radio%20astronomy%0AAuthor%3A%20Amir%20Aghabiglou%20and%20Chung%20San%20Chu%20and%20Arwa%20Dabbech%20and%20Yves%20Wiaux%0AAbstract%3A%20%20%20Radio-interferometric%20%28RI%29%20imaging%20entails%20solving%20high-resolution%0Ahigh-dynamic%20range%20inverse%20problems%20from%20large%20data%20volumes.%20Recent%20image%0Areconstruction%20techniques%20grounded%20in%20optimization%20theory%20have%20demonstrated%0Aremarkable%20capability%20for%20imaging%20precision%2C%20well%20beyond%20CLEAN%27s%20capability.%0AThese%20range%20from%20advanced%20proximal%20algorithms%20propelled%20by%20handcrafted%0Aregularization%20operators%2C%20such%20as%20the%20SARA%20family%2C%20to%20hybrid%20plug-and-play%0A%28PnP%29%20algorithms%20propelled%20by%20learned%20regularization%20denoisers%2C%20such%20as%20AIRI.%0AOptimization%20and%20PnP%20structures%20are%20however%20highly%20iterative%2C%20which%20hinders%0Atheir%20ability%20to%20handle%20the%20extreme%20data%20sizes%20expected%20from%20future%0Ainstruments.%20To%20address%20this%20scalability%20challenge%2C%20we%20introduce%20a%20novel%20deep%0Alearning%20approach%2C%20dubbed%20%22Residual-to-Residual%20DNN%20series%20for%20high-Dynamic%0Arange%20imaging%22.%20R2D2%27s%20reconstruction%20is%20formed%20as%20a%20series%20of%20residual%20images%2C%0Aiteratively%20estimated%20as%20outputs%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20taking%20the%0Aprevious%20iteration%27s%20image%20estimate%20and%20associated%20data%20residual%20as%20inputs.%20It%0Athus%20takes%20a%20hybrid%20structure%20between%20a%20PnP%20algorithm%20and%20a%20learned%20version%20of%0Athe%20matching%20pursuit%20algorithm%20that%20underpins%20CLEAN.%20We%20present%20a%20comprehensive%0Astudy%20of%20our%20approach%2C%20featuring%20its%20multiple%20incarnations%20distinguished%20by%0Atheir%20DNN%20architectures.%20We%20provide%20a%20detailed%20description%20of%20its%20training%0Aprocess%2C%20targeting%20a%20telescope-specific%20approach.%20R2D2%27s%20capability%20to%20deliver%0Ahigh%20precision%20is%20demonstrated%20in%20simulation%2C%20across%20a%20variety%20of%20image%20and%0Aobservation%20settings%20using%20the%20Very%20Large%20Array%20%28VLA%29.%20Its%20reconstruction%20speed%0Ais%20also%20demonstrated%3A%20with%20only%20few%20iterations%20required%20to%20clean%20data%20residuals%0Aat%20dynamic%20ranges%20up%20to%20100000%2C%20R2D2%20opens%20the%20door%20to%20fast%20precision%20imaging.%0AR2D2%20codes%20are%20available%20in%20the%20BASPLib%20library%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05452v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20R2D2%20deep%20neural%20network%20series%20paradigm%20for%20fast%20precision%20imaging%0A%20%20in%20radio%20astronomy&entry.906535625=Amir%20Aghabiglou%20and%20Chung%20San%20Chu%20and%20Arwa%20Dabbech%20and%20Yves%20Wiaux&entry.1292438233=%20%20Radio-interferometric%20%28RI%29%20imaging%20entails%20solving%20high-resolution%0Ahigh-dynamic%20range%20inverse%20problems%20from%20large%20data%20volumes.%20Recent%20image%0Areconstruction%20techniques%20grounded%20in%20optimization%20theory%20have%20demonstrated%0Aremarkable%20capability%20for%20imaging%20precision%2C%20well%20beyond%20CLEAN%27s%20capability.%0AThese%20range%20from%20advanced%20proximal%20algorithms%20propelled%20by%20handcrafted%0Aregularization%20operators%2C%20such%20as%20the%20SARA%20family%2C%20to%20hybrid%20plug-and-play%0A%28PnP%29%20algorithms%20propelled%20by%20learned%20regularization%20denoisers%2C%20such%20as%20AIRI.%0AOptimization%20and%20PnP%20structures%20are%20however%20highly%20iterative%2C%20which%20hinders%0Atheir%20ability%20to%20handle%20the%20extreme%20data%20sizes%20expected%20from%20future%0Ainstruments.%20To%20address%20this%20scalability%20challenge%2C%20we%20introduce%20a%20novel%20deep%0Alearning%20approach%2C%20dubbed%20%22Residual-to-Residual%20DNN%20series%20for%20high-Dynamic%0Arange%20imaging%22.%20R2D2%27s%20reconstruction%20is%20formed%20as%20a%20series%20of%20residual%20images%2C%0Aiteratively%20estimated%20as%20outputs%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20taking%20the%0Aprevious%20iteration%27s%20image%20estimate%20and%20associated%20data%20residual%20as%20inputs.%20It%0Athus%20takes%20a%20hybrid%20structure%20between%20a%20PnP%20algorithm%20and%20a%20learned%20version%20of%0Athe%20matching%20pursuit%20algorithm%20that%20underpins%20CLEAN.%20We%20present%20a%20comprehensive%0Astudy%20of%20our%20approach%2C%20featuring%20its%20multiple%20incarnations%20distinguished%20by%0Atheir%20DNN%20architectures.%20We%20provide%20a%20detailed%20description%20of%20its%20training%0Aprocess%2C%20targeting%20a%20telescope-specific%20approach.%20R2D2%27s%20capability%20to%20deliver%0Ahigh%20precision%20is%20demonstrated%20in%20simulation%2C%20across%20a%20variety%20of%20image%20and%0Aobservation%20settings%20using%20the%20Very%20Large%20Array%20%28VLA%29.%20Its%20reconstruction%20speed%0Ais%20also%20demonstrated%3A%20with%20only%20few%20iterations%20required%20to%20clean%20data%20residuals%0Aat%20dynamic%20ranges%20up%20to%20100000%2C%20R2D2%20opens%20the%20door%20to%20fast%20precision%20imaging.%0AR2D2%20codes%20are%20available%20in%20the%20BASPLib%20library%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05452v3&entry.124074799=Read"},
{"title": "GAD-Generative Learning for HD Map-Free Autonomous Driving", "author": "Weijian Sun and Yanbo Jia and Qi Zeng and Zihao Liu and Jiang Liao and Yue Li and Xianfeng Li and Bolin Zhao", "abstract": "  Deep-learning-based techniques have been widely adopted for autonomous\ndriving software stacks for mass production in recent years, focusing primarily\non perception modules, with some work extending this method to prediction\nmodules. However, the downstream planning and control modules are still\ndesigned with hefty handcrafted rules, dominated by optimization-based methods\nsuch as quadratic programming or model predictive control. This results in a\nperformance bottleneck for autonomous driving systems in that corner cases\nsimply cannot be solved by enumerating hand-crafted rules. We present a\ndeep-learning-based approach that brings prediction, decision, and planning\nmodules together with the attempt to overcome the rule-based methods'\ndeficiency in real-world applications of autonomous driving, especially for\nurban scenes. The DNN model we proposed is solely trained with 10 hours of\nhuman driver data, and it supports all mass-production ADAS features available\non the market to date. This method is deployed onto a Jiyue test car with no\nmodification to its factory-ready sensor set and compute platform. the\nfeasibility, usability, and commercial potential are demonstrated in this\narticle.\n", "link": "http://arxiv.org/abs/2405.00515v1", "date": "2024-05-01", "relevancy": 2.1697, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5448}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5445}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GAD-Generative%20Learning%20for%20HD%20Map-Free%20Autonomous%20Driving&body=Title%3A%20GAD-Generative%20Learning%20for%20HD%20Map-Free%20Autonomous%20Driving%0AAuthor%3A%20Weijian%20Sun%20and%20Yanbo%20Jia%20and%20Qi%20Zeng%20and%20Zihao%20Liu%20and%20Jiang%20Liao%20and%20Yue%20Li%20and%20Xianfeng%20Li%20and%20Bolin%20Zhao%0AAbstract%3A%20%20%20Deep-learning-based%20techniques%20have%20been%20widely%20adopted%20for%20autonomous%0Adriving%20software%20stacks%20for%20mass%20production%20in%20recent%20years%2C%20focusing%20primarily%0Aon%20perception%20modules%2C%20with%20some%20work%20extending%20this%20method%20to%20prediction%0Amodules.%20However%2C%20the%20downstream%20planning%20and%20control%20modules%20are%20still%0Adesigned%20with%20hefty%20handcrafted%20rules%2C%20dominated%20by%20optimization-based%20methods%0Asuch%20as%20quadratic%20programming%20or%20model%20predictive%20control.%20This%20results%20in%20a%0Aperformance%20bottleneck%20for%20autonomous%20driving%20systems%20in%20that%20corner%20cases%0Asimply%20cannot%20be%20solved%20by%20enumerating%20hand-crafted%20rules.%20We%20present%20a%0Adeep-learning-based%20approach%20that%20brings%20prediction%2C%20decision%2C%20and%20planning%0Amodules%20together%20with%20the%20attempt%20to%20overcome%20the%20rule-based%20methods%27%0Adeficiency%20in%20real-world%20applications%20of%20autonomous%20driving%2C%20especially%20for%0Aurban%20scenes.%20The%20DNN%20model%20we%20proposed%20is%20solely%20trained%20with%2010%20hours%20of%0Ahuman%20driver%20data%2C%20and%20it%20supports%20all%20mass-production%20ADAS%20features%20available%0Aon%20the%20market%20to%20date.%20This%20method%20is%20deployed%20onto%20a%20Jiyue%20test%20car%20with%20no%0Amodification%20to%20its%20factory-ready%20sensor%20set%20and%20compute%20platform.%20the%0Afeasibility%2C%20usability%2C%20and%20commercial%20potential%20are%20demonstrated%20in%20this%0Aarticle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00515v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAD-Generative%20Learning%20for%20HD%20Map-Free%20Autonomous%20Driving&entry.906535625=Weijian%20Sun%20and%20Yanbo%20Jia%20and%20Qi%20Zeng%20and%20Zihao%20Liu%20and%20Jiang%20Liao%20and%20Yue%20Li%20and%20Xianfeng%20Li%20and%20Bolin%20Zhao&entry.1292438233=%20%20Deep-learning-based%20techniques%20have%20been%20widely%20adopted%20for%20autonomous%0Adriving%20software%20stacks%20for%20mass%20production%20in%20recent%20years%2C%20focusing%20primarily%0Aon%20perception%20modules%2C%20with%20some%20work%20extending%20this%20method%20to%20prediction%0Amodules.%20However%2C%20the%20downstream%20planning%20and%20control%20modules%20are%20still%0Adesigned%20with%20hefty%20handcrafted%20rules%2C%20dominated%20by%20optimization-based%20methods%0Asuch%20as%20quadratic%20programming%20or%20model%20predictive%20control.%20This%20results%20in%20a%0Aperformance%20bottleneck%20for%20autonomous%20driving%20systems%20in%20that%20corner%20cases%0Asimply%20cannot%20be%20solved%20by%20enumerating%20hand-crafted%20rules.%20We%20present%20a%0Adeep-learning-based%20approach%20that%20brings%20prediction%2C%20decision%2C%20and%20planning%0Amodules%20together%20with%20the%20attempt%20to%20overcome%20the%20rule-based%20methods%27%0Adeficiency%20in%20real-world%20applications%20of%20autonomous%20driving%2C%20especially%20for%0Aurban%20scenes.%20The%20DNN%20model%20we%20proposed%20is%20solely%20trained%20with%2010%20hours%20of%0Ahuman%20driver%20data%2C%20and%20it%20supports%20all%20mass-production%20ADAS%20features%20available%0Aon%20the%20market%20to%20date.%20This%20method%20is%20deployed%20onto%20a%20Jiyue%20test%20car%20with%20no%0Amodification%20to%20its%20factory-ready%20sensor%20set%20and%20compute%20platform.%20the%0Afeasibility%2C%20usability%2C%20and%20commercial%20potential%20are%20demonstrated%20in%20this%0Aarticle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00515v1&entry.124074799=Read"},
{"title": "QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection", "author": "Nouhaila Innan and Alberto Marchisio and Muhammad Shafique and Mohamed Bennai", "abstract": "  This study introduces the Quantum Federated Neural Network for Financial\nFraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine\nLearning (QML) and quantum computing with Federated Learning (FL) for financial\nfraud detection. Using quantum technologies' computational power and the robust\ndata privacy protections offered by FL, QFNN-FFD emerges as a secure and\nefficient method for identifying fraudulent transactions within the financial\nsector. Implementing a dual-phase training model across distributed clients\nenhances data integrity and enables superior performance metrics, achieving\nprecision rates consistently above 95%. Additionally, QFNN-FFD demonstrates\nexceptional resilience by maintaining an impressive 80% accuracy, highlighting\nits robustness and readiness for real-world applications. This combination of\nhigh performance, security, and robustness against noise positions QFNN-FFD as\na transformative advancement in financial technology solutions and establishes\nit as a new benchmark for privacy-focused fraud detection systems. This\nframework facilitates the broader adoption of secure, quantum-enhanced\nfinancial services and inspires future innovations that could use QML to tackle\ncomplex challenges in other areas requiring high confidentiality and accuracy.\n", "link": "http://arxiv.org/abs/2404.02595v2", "date": "2024-05-01", "relevancy": 2.1516, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4371}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.433}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4209}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20QFNN-FFD%3A%20Quantum%20Federated%20Neural%20Network%20for%20Financial%20Fraud%20Detection&body=Title%3A%20QFNN-FFD%3A%20Quantum%20Federated%20Neural%20Network%20for%20Financial%20Fraud%20Detection%0AAuthor%3A%20Nouhaila%20Innan%20and%20Alberto%20Marchisio%20and%20Muhammad%20Shafique%20and%20Mohamed%20Bennai%0AAbstract%3A%20%20%20This%20study%20introduces%20the%20Quantum%20Federated%20Neural%20Network%20for%20Financial%0AFraud%20Detection%20%28QFNN-FFD%29%2C%20a%20cutting-edge%20framework%20merging%20Quantum%20Machine%0ALearning%20%28QML%29%20and%20quantum%20computing%20with%20Federated%20Learning%20%28FL%29%20for%20financial%0Afraud%20detection.%20Using%20quantum%20technologies%27%20computational%20power%20and%20the%20robust%0Adata%20privacy%20protections%20offered%20by%20FL%2C%20QFNN-FFD%20emerges%20as%20a%20secure%20and%0Aefficient%20method%20for%20identifying%20fraudulent%20transactions%20within%20the%20financial%0Asector.%20Implementing%20a%20dual-phase%20training%20model%20across%20distributed%20clients%0Aenhances%20data%20integrity%20and%20enables%20superior%20performance%20metrics%2C%20achieving%0Aprecision%20rates%20consistently%20above%2095%25.%20Additionally%2C%20QFNN-FFD%20demonstrates%0Aexceptional%20resilience%20by%20maintaining%20an%20impressive%2080%25%20accuracy%2C%20highlighting%0Aits%20robustness%20and%20readiness%20for%20real-world%20applications.%20This%20combination%20of%0Ahigh%20performance%2C%20security%2C%20and%20robustness%20against%20noise%20positions%20QFNN-FFD%20as%0Aa%20transformative%20advancement%20in%20financial%20technology%20solutions%20and%20establishes%0Ait%20as%20a%20new%20benchmark%20for%20privacy-focused%20fraud%20detection%20systems.%20This%0Aframework%20facilitates%20the%20broader%20adoption%20of%20secure%2C%20quantum-enhanced%0Afinancial%20services%20and%20inspires%20future%20innovations%20that%20could%20use%20QML%20to%20tackle%0Acomplex%20challenges%20in%20other%20areas%20requiring%20high%20confidentiality%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02595v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QFNN-FFD%3A%20Quantum%20Federated%20Neural%20Network%20for%20Financial%20Fraud%20Detection&entry.906535625=Nouhaila%20Innan%20and%20Alberto%20Marchisio%20and%20Muhammad%20Shafique%20and%20Mohamed%20Bennai&entry.1292438233=%20%20This%20study%20introduces%20the%20Quantum%20Federated%20Neural%20Network%20for%20Financial%0AFraud%20Detection%20%28QFNN-FFD%29%2C%20a%20cutting-edge%20framework%20merging%20Quantum%20Machine%0ALearning%20%28QML%29%20and%20quantum%20computing%20with%20Federated%20Learning%20%28FL%29%20for%20financial%0Afraud%20detection.%20Using%20quantum%20technologies%27%20computational%20power%20and%20the%20robust%0Adata%20privacy%20protections%20offered%20by%20FL%2C%20QFNN-FFD%20emerges%20as%20a%20secure%20and%0Aefficient%20method%20for%20identifying%20fraudulent%20transactions%20within%20the%20financial%0Asector.%20Implementing%20a%20dual-phase%20training%20model%20across%20distributed%20clients%0Aenhances%20data%20integrity%20and%20enables%20superior%20performance%20metrics%2C%20achieving%0Aprecision%20rates%20consistently%20above%2095%25.%20Additionally%2C%20QFNN-FFD%20demonstrates%0Aexceptional%20resilience%20by%20maintaining%20an%20impressive%2080%25%20accuracy%2C%20highlighting%0Aits%20robustness%20and%20readiness%20for%20real-world%20applications.%20This%20combination%20of%0Ahigh%20performance%2C%20security%2C%20and%20robustness%20against%20noise%20positions%20QFNN-FFD%20as%0Aa%20transformative%20advancement%20in%20financial%20technology%20solutions%20and%20establishes%0Ait%20as%20a%20new%20benchmark%20for%20privacy-focused%20fraud%20detection%20systems.%20This%0Aframework%20facilitates%20the%20broader%20adoption%20of%20secure%2C%20quantum-enhanced%0Afinancial%20services%20and%20inspires%20future%20innovations%20that%20could%20use%20QML%20to%20tackle%0Acomplex%20challenges%20in%20other%20areas%20requiring%20high%20confidentiality%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02595v2&entry.124074799=Read"},
{"title": "Learning to Complement with Multiple Humans", "author": "Zheng Zhang and Cuong Nguyen and Kevin Wells and Thanh-Toan Do and Gustavo Carneiro", "abstract": "  Real-world image classification tasks tend to be complex, where expert\nlabellers are sometimes unsure about the classes present in the images, leading\nto the issue of learning with noisy labels (LNL). The ill-posedness of the LNL\ntask requires the adoption of strong assumptions or the use of multiple noisy\nlabels per training image, resulting in accurate models that work well in\nisolation but fail to optimise human-AI collaborative classification (HAI-CC).\nUnlike such LNL methods, HAI-CC aims to leverage the synergies between human\nexpertise and AI capabilities but requires clean training labels, limiting its\nreal-world applicability. This paper addresses this gap by introducing the\ninnovative Learning to Complement with Multiple Humans (LECOMH) approach.\nLECOMH is designed to learn from noisy labels without depending on clean\nlabels, simultaneously maximising collaborative accuracy while minimising the\ncost of human collaboration, measured by the number of human expert annotations\nrequired per image. Additionally, new benchmarks featuring multiple noisy\nlabels for both training and testing are proposed to evaluate HAI-CC methods.\nThrough quantitative comparisons on these benchmarks, LECOMH consistently\noutperforms competitive HAI-CC approaches, human labellers, multi-rater\nlearning, and noisy-label learning methods across various datasets, offering a\npromising solution for addressing real-world image classification challenges.\n", "link": "http://arxiv.org/abs/2311.13172v2", "date": "2024-05-01", "relevancy": 2.1465, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5464}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5343}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Complement%20with%20Multiple%20Humans&body=Title%3A%20Learning%20to%20Complement%20with%20Multiple%20Humans%0AAuthor%3A%20Zheng%20Zhang%20and%20Cuong%20Nguyen%20and%20Kevin%20Wells%20and%20Thanh-Toan%20Do%20and%20Gustavo%20Carneiro%0AAbstract%3A%20%20%20Real-world%20image%20classification%20tasks%20tend%20to%20be%20complex%2C%20where%20expert%0Alabellers%20are%20sometimes%20unsure%20about%20the%20classes%20present%20in%20the%20images%2C%20leading%0Ato%20the%20issue%20of%20learning%20with%20noisy%20labels%20%28LNL%29.%20The%20ill-posedness%20of%20the%20LNL%0Atask%20requires%20the%20adoption%20of%20strong%20assumptions%20or%20the%20use%20of%20multiple%20noisy%0Alabels%20per%20training%20image%2C%20resulting%20in%20accurate%20models%20that%20work%20well%20in%0Aisolation%20but%20fail%20to%20optimise%20human-AI%20collaborative%20classification%20%28HAI-CC%29.%0AUnlike%20such%20LNL%20methods%2C%20HAI-CC%20aims%20to%20leverage%20the%20synergies%20between%20human%0Aexpertise%20and%20AI%20capabilities%20but%20requires%20clean%20training%20labels%2C%20limiting%20its%0Areal-world%20applicability.%20This%20paper%20addresses%20this%20gap%20by%20introducing%20the%0Ainnovative%20Learning%20to%20Complement%20with%20Multiple%20Humans%20%28LECOMH%29%20approach.%0ALECOMH%20is%20designed%20to%20learn%20from%20noisy%20labels%20without%20depending%20on%20clean%0Alabels%2C%20simultaneously%20maximising%20collaborative%20accuracy%20while%20minimising%20the%0Acost%20of%20human%20collaboration%2C%20measured%20by%20the%20number%20of%20human%20expert%20annotations%0Arequired%20per%20image.%20Additionally%2C%20new%20benchmarks%20featuring%20multiple%20noisy%0Alabels%20for%20both%20training%20and%20testing%20are%20proposed%20to%20evaluate%20HAI-CC%20methods.%0AThrough%20quantitative%20comparisons%20on%20these%20benchmarks%2C%20LECOMH%20consistently%0Aoutperforms%20competitive%20HAI-CC%20approaches%2C%20human%20labellers%2C%20multi-rater%0Alearning%2C%20and%20noisy-label%20learning%20methods%20across%20various%20datasets%2C%20offering%20a%0Apromising%20solution%20for%20addressing%20real-world%20image%20classification%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13172v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Complement%20with%20Multiple%20Humans&entry.906535625=Zheng%20Zhang%20and%20Cuong%20Nguyen%20and%20Kevin%20Wells%20and%20Thanh-Toan%20Do%20and%20Gustavo%20Carneiro&entry.1292438233=%20%20Real-world%20image%20classification%20tasks%20tend%20to%20be%20complex%2C%20where%20expert%0Alabellers%20are%20sometimes%20unsure%20about%20the%20classes%20present%20in%20the%20images%2C%20leading%0Ato%20the%20issue%20of%20learning%20with%20noisy%20labels%20%28LNL%29.%20The%20ill-posedness%20of%20the%20LNL%0Atask%20requires%20the%20adoption%20of%20strong%20assumptions%20or%20the%20use%20of%20multiple%20noisy%0Alabels%20per%20training%20image%2C%20resulting%20in%20accurate%20models%20that%20work%20well%20in%0Aisolation%20but%20fail%20to%20optimise%20human-AI%20collaborative%20classification%20%28HAI-CC%29.%0AUnlike%20such%20LNL%20methods%2C%20HAI-CC%20aims%20to%20leverage%20the%20synergies%20between%20human%0Aexpertise%20and%20AI%20capabilities%20but%20requires%20clean%20training%20labels%2C%20limiting%20its%0Areal-world%20applicability.%20This%20paper%20addresses%20this%20gap%20by%20introducing%20the%0Ainnovative%20Learning%20to%20Complement%20with%20Multiple%20Humans%20%28LECOMH%29%20approach.%0ALECOMH%20is%20designed%20to%20learn%20from%20noisy%20labels%20without%20depending%20on%20clean%0Alabels%2C%20simultaneously%20maximising%20collaborative%20accuracy%20while%20minimising%20the%0Acost%20of%20human%20collaboration%2C%20measured%20by%20the%20number%20of%20human%20expert%20annotations%0Arequired%20per%20image.%20Additionally%2C%20new%20benchmarks%20featuring%20multiple%20noisy%0Alabels%20for%20both%20training%20and%20testing%20are%20proposed%20to%20evaluate%20HAI-CC%20methods.%0AThrough%20quantitative%20comparisons%20on%20these%20benchmarks%2C%20LECOMH%20consistently%0Aoutperforms%20competitive%20HAI-CC%20approaches%2C%20human%20labellers%2C%20multi-rater%0Alearning%2C%20and%20noisy-label%20learning%20methods%20across%20various%20datasets%2C%20offering%20a%0Apromising%20solution%20for%20addressing%20real-world%20image%20classification%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13172v2&entry.124074799=Read"},
{"title": "Volume-Preserving Transformers for Learning Time Series Data with\n  Structure", "author": "Benedikt Brantner and Guillaume de Romemont and Michael Kraus and Zeyuan Li", "abstract": "  Two of the many trends in neural network research of the past few years have\nbeen (i) the learning of dynamical systems, especially with recurrent neural\nnetworks such as long short-term memory networks (LSTMs) and (ii) the\nintroduction of transformer neural networks for natural language processing\n(NLP) tasks. Both of these trends have created enormous amounts of traction,\nparticularly the second one: transformer networks now dominate the field of\nNLP. Even though some work has been performed on the intersection of these two\ntrends, those efforts was largely limited to using the vanilla transformer\ndirectly without adjusting its architecture for the setting of a physical\nsystem. In this work we use a transformer-inspired neural network to learn a\ndynamical system and furthermore (for the first time) imbue it with\nstructure-preserving properties to improve long-term stability. This is shown\nto be of great advantage when applying the neural network to real world\napplications.\n", "link": "http://arxiv.org/abs/2312.11166v2", "date": "2024-05-01", "relevancy": 2.1424, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5174}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Volume-Preserving%20Transformers%20for%20Learning%20Time%20Series%20Data%20with%0A%20%20Structure&body=Title%3A%20Volume-Preserving%20Transformers%20for%20Learning%20Time%20Series%20Data%20with%0A%20%20Structure%0AAuthor%3A%20Benedikt%20Brantner%20and%20Guillaume%20de%20Romemont%20and%20Michael%20Kraus%20and%20Zeyuan%20Li%0AAbstract%3A%20%20%20Two%20of%20the%20many%20trends%20in%20neural%20network%20research%20of%20the%20past%20few%20years%20have%0Abeen%20%28i%29%20the%20learning%20of%20dynamical%20systems%2C%20especially%20with%20recurrent%20neural%0Anetworks%20such%20as%20long%20short-term%20memory%20networks%20%28LSTMs%29%20and%20%28ii%29%20the%0Aintroduction%20of%20transformer%20neural%20networks%20for%20natural%20language%20processing%0A%28NLP%29%20tasks.%20Both%20of%20these%20trends%20have%20created%20enormous%20amounts%20of%20traction%2C%0Aparticularly%20the%20second%20one%3A%20transformer%20networks%20now%20dominate%20the%20field%20of%0ANLP.%20Even%20though%20some%20work%20has%20been%20performed%20on%20the%20intersection%20of%20these%20two%0Atrends%2C%20those%20efforts%20was%20largely%20limited%20to%20using%20the%20vanilla%20transformer%0Adirectly%20without%20adjusting%20its%20architecture%20for%20the%20setting%20of%20a%20physical%0Asystem.%20In%20this%20work%20we%20use%20a%20transformer-inspired%20neural%20network%20to%20learn%20a%0Adynamical%20system%20and%20furthermore%20%28for%20the%20first%20time%29%20imbue%20it%20with%0Astructure-preserving%20properties%20to%20improve%20long-term%20stability.%20This%20is%20shown%0Ato%20be%20of%20great%20advantage%20when%20applying%20the%20neural%20network%20to%20real%20world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11166v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Volume-Preserving%20Transformers%20for%20Learning%20Time%20Series%20Data%20with%0A%20%20Structure&entry.906535625=Benedikt%20Brantner%20and%20Guillaume%20de%20Romemont%20and%20Michael%20Kraus%20and%20Zeyuan%20Li&entry.1292438233=%20%20Two%20of%20the%20many%20trends%20in%20neural%20network%20research%20of%20the%20past%20few%20years%20have%0Abeen%20%28i%29%20the%20learning%20of%20dynamical%20systems%2C%20especially%20with%20recurrent%20neural%0Anetworks%20such%20as%20long%20short-term%20memory%20networks%20%28LSTMs%29%20and%20%28ii%29%20the%0Aintroduction%20of%20transformer%20neural%20networks%20for%20natural%20language%20processing%0A%28NLP%29%20tasks.%20Both%20of%20these%20trends%20have%20created%20enormous%20amounts%20of%20traction%2C%0Aparticularly%20the%20second%20one%3A%20transformer%20networks%20now%20dominate%20the%20field%20of%0ANLP.%20Even%20though%20some%20work%20has%20been%20performed%20on%20the%20intersection%20of%20these%20two%0Atrends%2C%20those%20efforts%20was%20largely%20limited%20to%20using%20the%20vanilla%20transformer%0Adirectly%20without%20adjusting%20its%20architecture%20for%20the%20setting%20of%20a%20physical%0Asystem.%20In%20this%20work%20we%20use%20a%20transformer-inspired%20neural%20network%20to%20learn%20a%0Adynamical%20system%20and%20furthermore%20%28for%20the%20first%20time%29%20imbue%20it%20with%0Astructure-preserving%20properties%20to%20improve%20long-term%20stability.%20This%20is%20shown%0Ato%20be%20of%20great%20advantage%20when%20applying%20the%20neural%20network%20to%20real%20world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11166v2&entry.124074799=Read"},
{"title": "Self-supervised Pre-training of Text Recognizers", "author": "Martin Ki\u0161\u0161 and Michal Hradi\u0161", "abstract": "  In this paper, we investigate self-supervised pre-training methods for\ndocument text recognition. Nowadays, large unlabeled datasets can be collected\nfor many research tasks, including text recognition, but it is costly to\nannotate them. Therefore, methods utilizing unlabeled data are researched. We\nstudy self-supervised pre-training methods based on masked label prediction\nusing three different approaches -- Feature Quantization, VQ-VAE, and\nPost-Quantized AE. We also investigate joint-embedding approaches with VICReg\nand NT-Xent objectives, for which we propose an image shifting technique to\nprevent model collapse where it relies solely on positional encoding while\ncompletely ignoring the input image. We perform our experiments on historical\nhandwritten (Bentham) and historical printed datasets mainly to investigate the\nbenefits of the self-supervised pre-training techniques with different amounts\nof annotated target domain data. We use transfer learning as strong baselines.\nThe evaluation shows that the self-supervised pre-training on data from the\ntarget domain is very effective, but it struggles to outperform transfer\nlearning from closely related domains. This paper is one of the first\nresearches exploring self-supervised pre-training in document text recognition,\nand we believe that it will become a cornerstone for future research in this\narea. We made our implementation of the investigated methods publicly available\nat https://github.com/DCGM/pero-pretraining.\n", "link": "http://arxiv.org/abs/2405.00420v1", "date": "2024-05-01", "relevancy": 2.1343, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5692}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5142}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.493}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Pre-training%20of%20Text%20Recognizers&body=Title%3A%20Self-supervised%20Pre-training%20of%20Text%20Recognizers%0AAuthor%3A%20Martin%20Ki%C5%A1%C5%A1%20and%20Michal%20Hradi%C5%A1%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20self-supervised%20pre-training%20methods%20for%0Adocument%20text%20recognition.%20Nowadays%2C%20large%20unlabeled%20datasets%20can%20be%20collected%0Afor%20many%20research%20tasks%2C%20including%20text%20recognition%2C%20but%20it%20is%20costly%20to%0Aannotate%20them.%20Therefore%2C%20methods%20utilizing%20unlabeled%20data%20are%20researched.%20We%0Astudy%20self-supervised%20pre-training%20methods%20based%20on%20masked%20label%20prediction%0Ausing%20three%20different%20approaches%20--%20Feature%20Quantization%2C%20VQ-VAE%2C%20and%0APost-Quantized%20AE.%20We%20also%20investigate%20joint-embedding%20approaches%20with%20VICReg%0Aand%20NT-Xent%20objectives%2C%20for%20which%20we%20propose%20an%20image%20shifting%20technique%20to%0Aprevent%20model%20collapse%20where%20it%20relies%20solely%20on%20positional%20encoding%20while%0Acompletely%20ignoring%20the%20input%20image.%20We%20perform%20our%20experiments%20on%20historical%0Ahandwritten%20%28Bentham%29%20and%20historical%20printed%20datasets%20mainly%20to%20investigate%20the%0Abenefits%20of%20the%20self-supervised%20pre-training%20techniques%20with%20different%20amounts%0Aof%20annotated%20target%20domain%20data.%20We%20use%20transfer%20learning%20as%20strong%20baselines.%0AThe%20evaluation%20shows%20that%20the%20self-supervised%20pre-training%20on%20data%20from%20the%0Atarget%20domain%20is%20very%20effective%2C%20but%20it%20struggles%20to%20outperform%20transfer%0Alearning%20from%20closely%20related%20domains.%20This%20paper%20is%20one%20of%20the%20first%0Aresearches%20exploring%20self-supervised%20pre-training%20in%20document%20text%20recognition%2C%0Aand%20we%20believe%20that%20it%20will%20become%20a%20cornerstone%20for%20future%20research%20in%20this%0Aarea.%20We%20made%20our%20implementation%20of%20the%20investigated%20methods%20publicly%20available%0Aat%20https%3A//github.com/DCGM/pero-pretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00420v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Pre-training%20of%20Text%20Recognizers&entry.906535625=Martin%20Ki%C5%A1%C5%A1%20and%20Michal%20Hradi%C5%A1&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20self-supervised%20pre-training%20methods%20for%0Adocument%20text%20recognition.%20Nowadays%2C%20large%20unlabeled%20datasets%20can%20be%20collected%0Afor%20many%20research%20tasks%2C%20including%20text%20recognition%2C%20but%20it%20is%20costly%20to%0Aannotate%20them.%20Therefore%2C%20methods%20utilizing%20unlabeled%20data%20are%20researched.%20We%0Astudy%20self-supervised%20pre-training%20methods%20based%20on%20masked%20label%20prediction%0Ausing%20three%20different%20approaches%20--%20Feature%20Quantization%2C%20VQ-VAE%2C%20and%0APost-Quantized%20AE.%20We%20also%20investigate%20joint-embedding%20approaches%20with%20VICReg%0Aand%20NT-Xent%20objectives%2C%20for%20which%20we%20propose%20an%20image%20shifting%20technique%20to%0Aprevent%20model%20collapse%20where%20it%20relies%20solely%20on%20positional%20encoding%20while%0Acompletely%20ignoring%20the%20input%20image.%20We%20perform%20our%20experiments%20on%20historical%0Ahandwritten%20%28Bentham%29%20and%20historical%20printed%20datasets%20mainly%20to%20investigate%20the%0Abenefits%20of%20the%20self-supervised%20pre-training%20techniques%20with%20different%20amounts%0Aof%20annotated%20target%20domain%20data.%20We%20use%20transfer%20learning%20as%20strong%20baselines.%0AThe%20evaluation%20shows%20that%20the%20self-supervised%20pre-training%20on%20data%20from%20the%0Atarget%20domain%20is%20very%20effective%2C%20but%20it%20struggles%20to%20outperform%20transfer%0Alearning%20from%20closely%20related%20domains.%20This%20paper%20is%20one%20of%20the%20first%0Aresearches%20exploring%20self-supervised%20pre-training%20in%20document%20text%20recognition%2C%0Aand%20we%20believe%20that%20it%20will%20become%20a%20cornerstone%20for%20future%20research%20in%20this%0Aarea.%20We%20made%20our%20implementation%20of%20the%20investigated%20methods%20publicly%20available%0Aat%20https%3A//github.com/DCGM/pero-pretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00420v1&entry.124074799=Read"},
{"title": "DmADs-Net: Dense multiscale attention and depth-supervised network for\n  medical image segmentation", "author": "Zhaojin Fu and Zheng Chen and Jinjiang Li and Lu Ren", "abstract": "  Deep learning has made important contributions to the development of medical\nimage segmentation. Convolutional neural networks, as a crucial branch, have\nattracted strong attention from researchers. Through the tireless efforts of\nnumerous researchers, convolutional neural networks have yielded numerous\noutstanding algorithms for processing medical images. The ideas and\narchitectures of these algorithms have also provided important inspiration for\nthe development of later technologies.Through extensive experimentation, we\nhave found that currently mainstream deep learning algorithms are not always\nable to achieve ideal results when processing complex datasets and different\ntypes of datasets. These networks still have room for improvement in lesion\nlocalization and feature extraction. Therefore, we have created the Dense\nMultiscale Attention and Depth-Supervised Network (DmADs-Net).We use ResNet for\nfeature extraction at different depths and create a Multi-scale Convolutional\nFeature Attention Block to improve the network's attention to weak feature\ninformation. The Local Feature Attention Block is created to enable enhanced\nlocal feature attention for high-level semantic information. In addition, in\nthe feature fusion phase, a Feature Refinement and Fusion Block is created to\nenhance the fusion of different semantic information.We validated the\nperformance of the network using five datasets of varying sizes and types.\nResults from comparative experiments show that DmADs-Net outperformed\nmainstream networks. Ablation experiments further demonstrated the\neffectiveness of the created modules and the rationality of the network\narchitecture.\n", "link": "http://arxiv.org/abs/2405.00472v1", "date": "2024-05-01", "relevancy": 2.1309, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5356}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5352}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5291}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DmADs-Net%3A%20Dense%20multiscale%20attention%20and%20depth-supervised%20network%20for%0A%20%20medical%20image%20segmentation&body=Title%3A%20DmADs-Net%3A%20Dense%20multiscale%20attention%20and%20depth-supervised%20network%20for%0A%20%20medical%20image%20segmentation%0AAuthor%3A%20Zhaojin%20Fu%20and%20Zheng%20Chen%20and%20Jinjiang%20Li%20and%20Lu%20Ren%0AAbstract%3A%20%20%20Deep%20learning%20has%20made%20important%20contributions%20to%20the%20development%20of%20medical%0Aimage%20segmentation.%20Convolutional%20neural%20networks%2C%20as%20a%20crucial%20branch%2C%20have%0Aattracted%20strong%20attention%20from%20researchers.%20Through%20the%20tireless%20efforts%20of%0Anumerous%20researchers%2C%20convolutional%20neural%20networks%20have%20yielded%20numerous%0Aoutstanding%20algorithms%20for%20processing%20medical%20images.%20The%20ideas%20and%0Aarchitectures%20of%20these%20algorithms%20have%20also%20provided%20important%20inspiration%20for%0Athe%20development%20of%20later%20technologies.Through%20extensive%20experimentation%2C%20we%0Ahave%20found%20that%20currently%20mainstream%20deep%20learning%20algorithms%20are%20not%20always%0Aable%20to%20achieve%20ideal%20results%20when%20processing%20complex%20datasets%20and%20different%0Atypes%20of%20datasets.%20These%20networks%20still%20have%20room%20for%20improvement%20in%20lesion%0Alocalization%20and%20feature%20extraction.%20Therefore%2C%20we%20have%20created%20the%20Dense%0AMultiscale%20Attention%20and%20Depth-Supervised%20Network%20%28DmADs-Net%29.We%20use%20ResNet%20for%0Afeature%20extraction%20at%20different%20depths%20and%20create%20a%20Multi-scale%20Convolutional%0AFeature%20Attention%20Block%20to%20improve%20the%20network%27s%20attention%20to%20weak%20feature%0Ainformation.%20The%20Local%20Feature%20Attention%20Block%20is%20created%20to%20enable%20enhanced%0Alocal%20feature%20attention%20for%20high-level%20semantic%20information.%20In%20addition%2C%20in%0Athe%20feature%20fusion%20phase%2C%20a%20Feature%20Refinement%20and%20Fusion%20Block%20is%20created%20to%0Aenhance%20the%20fusion%20of%20different%20semantic%20information.We%20validated%20the%0Aperformance%20of%20the%20network%20using%20five%20datasets%20of%20varying%20sizes%20and%20types.%0AResults%20from%20comparative%20experiments%20show%20that%20DmADs-Net%20outperformed%0Amainstream%20networks.%20Ablation%20experiments%20further%20demonstrated%20the%0Aeffectiveness%20of%20the%20created%20modules%20and%20the%20rationality%20of%20the%20network%0Aarchitecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00472v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DmADs-Net%3A%20Dense%20multiscale%20attention%20and%20depth-supervised%20network%20for%0A%20%20medical%20image%20segmentation&entry.906535625=Zhaojin%20Fu%20and%20Zheng%20Chen%20and%20Jinjiang%20Li%20and%20Lu%20Ren&entry.1292438233=%20%20Deep%20learning%20has%20made%20important%20contributions%20to%20the%20development%20of%20medical%0Aimage%20segmentation.%20Convolutional%20neural%20networks%2C%20as%20a%20crucial%20branch%2C%20have%0Aattracted%20strong%20attention%20from%20researchers.%20Through%20the%20tireless%20efforts%20of%0Anumerous%20researchers%2C%20convolutional%20neural%20networks%20have%20yielded%20numerous%0Aoutstanding%20algorithms%20for%20processing%20medical%20images.%20The%20ideas%20and%0Aarchitectures%20of%20these%20algorithms%20have%20also%20provided%20important%20inspiration%20for%0Athe%20development%20of%20later%20technologies.Through%20extensive%20experimentation%2C%20we%0Ahave%20found%20that%20currently%20mainstream%20deep%20learning%20algorithms%20are%20not%20always%0Aable%20to%20achieve%20ideal%20results%20when%20processing%20complex%20datasets%20and%20different%0Atypes%20of%20datasets.%20These%20networks%20still%20have%20room%20for%20improvement%20in%20lesion%0Alocalization%20and%20feature%20extraction.%20Therefore%2C%20we%20have%20created%20the%20Dense%0AMultiscale%20Attention%20and%20Depth-Supervised%20Network%20%28DmADs-Net%29.We%20use%20ResNet%20for%0Afeature%20extraction%20at%20different%20depths%20and%20create%20a%20Multi-scale%20Convolutional%0AFeature%20Attention%20Block%20to%20improve%20the%20network%27s%20attention%20to%20weak%20feature%0Ainformation.%20The%20Local%20Feature%20Attention%20Block%20is%20created%20to%20enable%20enhanced%0Alocal%20feature%20attention%20for%20high-level%20semantic%20information.%20In%20addition%2C%20in%0Athe%20feature%20fusion%20phase%2C%20a%20Feature%20Refinement%20and%20Fusion%20Block%20is%20created%20to%0Aenhance%20the%20fusion%20of%20different%20semantic%20information.We%20validated%20the%0Aperformance%20of%20the%20network%20using%20five%20datasets%20of%20varying%20sizes%20and%20types.%0AResults%20from%20comparative%20experiments%20show%20that%20DmADs-Net%20outperformed%0Amainstream%20networks.%20Ablation%20experiments%20further%20demonstrated%20the%0Aeffectiveness%20of%20the%20created%20modules%20and%20the%20rationality%20of%20the%20network%0Aarchitecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00472v1&entry.124074799=Read"},
{"title": "Grains of Saliency: Optimizing Saliency-based Training of Biometric\n  Attack Detection Models", "author": "Colton R. Crum and Samuel Webster and Adam Czajka", "abstract": "  Incorporating human-perceptual intelligence into model training has shown to\nincrease the generalization capability of models in several difficult biometric\ntasks, such as presentation attack detection (PAD) and detection of synthetic\nsamples. After the initial collection phase, human visual saliency (e.g.,\neye-tracking data, or handwritten annotations) can be integrated into model\ntraining through attention mechanisms, augmented training samples, or through\nhuman perception-related components of loss functions. Despite their successes,\na vital, but seemingly neglected, aspect of any saliency-based training is the\nlevel of salience granularity (e.g., bounding boxes, single saliency maps, or\nsaliency aggregated from multiple subjects) necessary to find a balance between\nreaping the full benefits of human saliency and the cost of its collection. In\nthis paper, we explore several different levels of salience granularity and\ndemonstrate that increased generalization capabilities of PAD and synthetic\nface detection can be achieved by using simple yet effective saliency\npost-processing techniques across several different CNNs.\n", "link": "http://arxiv.org/abs/2405.00650v1", "date": "2024-05-01", "relevancy": 2.1292, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.543}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5275}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5176}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Grains%20of%20Saliency%3A%20Optimizing%20Saliency-based%20Training%20of%20Biometric%0A%20%20Attack%20Detection%20Models&body=Title%3A%20Grains%20of%20Saliency%3A%20Optimizing%20Saliency-based%20Training%20of%20Biometric%0A%20%20Attack%20Detection%20Models%0AAuthor%3A%20Colton%20R.%20Crum%20and%20Samuel%20Webster%20and%20Adam%20Czajka%0AAbstract%3A%20%20%20Incorporating%20human-perceptual%20intelligence%20into%20model%20training%20has%20shown%20to%0Aincrease%20the%20generalization%20capability%20of%20models%20in%20several%20difficult%20biometric%0Atasks%2C%20such%20as%20presentation%20attack%20detection%20%28PAD%29%20and%20detection%20of%20synthetic%0Asamples.%20After%20the%20initial%20collection%20phase%2C%20human%20visual%20saliency%20%28e.g.%2C%0Aeye-tracking%20data%2C%20or%20handwritten%20annotations%29%20can%20be%20integrated%20into%20model%0Atraining%20through%20attention%20mechanisms%2C%20augmented%20training%20samples%2C%20or%20through%0Ahuman%20perception-related%20components%20of%20loss%20functions.%20Despite%20their%20successes%2C%0Aa%20vital%2C%20but%20seemingly%20neglected%2C%20aspect%20of%20any%20saliency-based%20training%20is%20the%0Alevel%20of%20salience%20granularity%20%28e.g.%2C%20bounding%20boxes%2C%20single%20saliency%20maps%2C%20or%0Asaliency%20aggregated%20from%20multiple%20subjects%29%20necessary%20to%20find%20a%20balance%20between%0Areaping%20the%20full%20benefits%20of%20human%20saliency%20and%20the%20cost%20of%20its%20collection.%20In%0Athis%20paper%2C%20we%20explore%20several%20different%20levels%20of%20salience%20granularity%20and%0Ademonstrate%20that%20increased%20generalization%20capabilities%20of%20PAD%20and%20synthetic%0Aface%20detection%20can%20be%20achieved%20by%20using%20simple%20yet%20effective%20saliency%0Apost-processing%20techniques%20across%20several%20different%20CNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00650v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grains%20of%20Saliency%3A%20Optimizing%20Saliency-based%20Training%20of%20Biometric%0A%20%20Attack%20Detection%20Models&entry.906535625=Colton%20R.%20Crum%20and%20Samuel%20Webster%20and%20Adam%20Czajka&entry.1292438233=%20%20Incorporating%20human-perceptual%20intelligence%20into%20model%20training%20has%20shown%20to%0Aincrease%20the%20generalization%20capability%20of%20models%20in%20several%20difficult%20biometric%0Atasks%2C%20such%20as%20presentation%20attack%20detection%20%28PAD%29%20and%20detection%20of%20synthetic%0Asamples.%20After%20the%20initial%20collection%20phase%2C%20human%20visual%20saliency%20%28e.g.%2C%0Aeye-tracking%20data%2C%20or%20handwritten%20annotations%29%20can%20be%20integrated%20into%20model%0Atraining%20through%20attention%20mechanisms%2C%20augmented%20training%20samples%2C%20or%20through%0Ahuman%20perception-related%20components%20of%20loss%20functions.%20Despite%20their%20successes%2C%0Aa%20vital%2C%20but%20seemingly%20neglected%2C%20aspect%20of%20any%20saliency-based%20training%20is%20the%0Alevel%20of%20salience%20granularity%20%28e.g.%2C%20bounding%20boxes%2C%20single%20saliency%20maps%2C%20or%0Asaliency%20aggregated%20from%20multiple%20subjects%29%20necessary%20to%20find%20a%20balance%20between%0Areaping%20the%20full%20benefits%20of%20human%20saliency%20and%20the%20cost%20of%20its%20collection.%20In%0Athis%20paper%2C%20we%20explore%20several%20different%20levels%20of%20salience%20granularity%20and%0Ademonstrate%20that%20increased%20generalization%20capabilities%20of%20PAD%20and%20synthetic%0Aface%20detection%20can%20be%20achieved%20by%20using%20simple%20yet%20effective%20saliency%0Apost-processing%20techniques%20across%20several%20different%20CNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00650v1&entry.124074799=Read"},
{"title": "\"I'm Not Sure, But...\": Examining the Impact of Large Language Models'\n  Uncertainty Expression on User Reliance and Trust", "author": "Sunnie S. Y. Kim and Q. Vera Liao and Mihaela Vorvoreanu and Stephanie Ballard and Jennifer Wortman Vaughan", "abstract": "  Widely deployed large language models (LLMs) can produce convincing yet\nincorrect outputs, potentially misleading users who may rely on them as if they\nwere correct. To reduce such overreliance, there have been calls for LLMs to\ncommunicate their uncertainty to end users. However, there has been little\nempirical work examining how users perceive and act upon LLMs' expressions of\nuncertainty. We explore this question through a large-scale, pre-registered,\nhuman-subject experiment (N=404) in which participants answer medical questions\nwith or without access to responses from a fictional LLM-infused search engine.\nUsing both behavioral and self-reported measures, we examine how different\nnatural language expressions of uncertainty impact participants' reliance,\ntrust, and overall task performance. We find that first-person expressions\n(e.g., \"I'm not sure, but...\") decrease participants' confidence in the system\nand tendency to agree with the system's answers, while increasing participants'\naccuracy. An exploratory analysis suggests that this increase can be attributed\nto reduced (but not fully eliminated) overreliance on incorrect answers. While\nwe observe similar effects for uncertainty expressed from a general perspective\n(e.g., \"It's not clear, but...\"), these effects are weaker and not\nstatistically significant. Our findings suggest that using natural language\nexpressions of uncertainty may be an effective approach for reducing\noverreliance on LLMs, but that the precise language used matters. This\nhighlights the importance of user testing before deploying LLMs at scale.\n", "link": "http://arxiv.org/abs/2405.00623v1", "date": "2024-05-01", "relevancy": 2.1286, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5956}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5048}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20%22I%27m%20Not%20Sure%2C%20But...%22%3A%20Examining%20the%20Impact%20of%20Large%20Language%20Models%27%0A%20%20Uncertainty%20Expression%20on%20User%20Reliance%20and%20Trust&body=Title%3A%20%22I%27m%20Not%20Sure%2C%20But...%22%3A%20Examining%20the%20Impact%20of%20Large%20Language%20Models%27%0A%20%20Uncertainty%20Expression%20on%20User%20Reliance%20and%20Trust%0AAuthor%3A%20Sunnie%20S.%20Y.%20Kim%20and%20Q.%20Vera%20Liao%20and%20Mihaela%20Vorvoreanu%20and%20Stephanie%20Ballard%20and%20Jennifer%20Wortman%20Vaughan%0AAbstract%3A%20%20%20Widely%20deployed%20large%20language%20models%20%28LLMs%29%20can%20produce%20convincing%20yet%0Aincorrect%20outputs%2C%20potentially%20misleading%20users%20who%20may%20rely%20on%20them%20as%20if%20they%0Awere%20correct.%20To%20reduce%20such%20overreliance%2C%20there%20have%20been%20calls%20for%20LLMs%20to%0Acommunicate%20their%20uncertainty%20to%20end%20users.%20However%2C%20there%20has%20been%20little%0Aempirical%20work%20examining%20how%20users%20perceive%20and%20act%20upon%20LLMs%27%20expressions%20of%0Auncertainty.%20We%20explore%20this%20question%20through%20a%20large-scale%2C%20pre-registered%2C%0Ahuman-subject%20experiment%20%28N%3D404%29%20in%20which%20participants%20answer%20medical%20questions%0Awith%20or%20without%20access%20to%20responses%20from%20a%20fictional%20LLM-infused%20search%20engine.%0AUsing%20both%20behavioral%20and%20self-reported%20measures%2C%20we%20examine%20how%20different%0Anatural%20language%20expressions%20of%20uncertainty%20impact%20participants%27%20reliance%2C%0Atrust%2C%20and%20overall%20task%20performance.%20We%20find%20that%20first-person%20expressions%0A%28e.g.%2C%20%22I%27m%20not%20sure%2C%20but...%22%29%20decrease%20participants%27%20confidence%20in%20the%20system%0Aand%20tendency%20to%20agree%20with%20the%20system%27s%20answers%2C%20while%20increasing%20participants%27%0Aaccuracy.%20An%20exploratory%20analysis%20suggests%20that%20this%20increase%20can%20be%20attributed%0Ato%20reduced%20%28but%20not%20fully%20eliminated%29%20overreliance%20on%20incorrect%20answers.%20While%0Awe%20observe%20similar%20effects%20for%20uncertainty%20expressed%20from%20a%20general%20perspective%0A%28e.g.%2C%20%22It%27s%20not%20clear%2C%20but...%22%29%2C%20these%20effects%20are%20weaker%20and%20not%0Astatistically%20significant.%20Our%20findings%20suggest%20that%20using%20natural%20language%0Aexpressions%20of%20uncertainty%20may%20be%20an%20effective%20approach%20for%20reducing%0Aoverreliance%20on%20LLMs%2C%20but%20that%20the%20precise%20language%20used%20matters.%20This%0Ahighlights%20the%20importance%20of%20user%20testing%20before%20deploying%20LLMs%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00623v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22I%27m%20Not%20Sure%2C%20But...%22%3A%20Examining%20the%20Impact%20of%20Large%20Language%20Models%27%0A%20%20Uncertainty%20Expression%20on%20User%20Reliance%20and%20Trust&entry.906535625=Sunnie%20S.%20Y.%20Kim%20and%20Q.%20Vera%20Liao%20and%20Mihaela%20Vorvoreanu%20and%20Stephanie%20Ballard%20and%20Jennifer%20Wortman%20Vaughan&entry.1292438233=%20%20Widely%20deployed%20large%20language%20models%20%28LLMs%29%20can%20produce%20convincing%20yet%0Aincorrect%20outputs%2C%20potentially%20misleading%20users%20who%20may%20rely%20on%20them%20as%20if%20they%0Awere%20correct.%20To%20reduce%20such%20overreliance%2C%20there%20have%20been%20calls%20for%20LLMs%20to%0Acommunicate%20their%20uncertainty%20to%20end%20users.%20However%2C%20there%20has%20been%20little%0Aempirical%20work%20examining%20how%20users%20perceive%20and%20act%20upon%20LLMs%27%20expressions%20of%0Auncertainty.%20We%20explore%20this%20question%20through%20a%20large-scale%2C%20pre-registered%2C%0Ahuman-subject%20experiment%20%28N%3D404%29%20in%20which%20participants%20answer%20medical%20questions%0Awith%20or%20without%20access%20to%20responses%20from%20a%20fictional%20LLM-infused%20search%20engine.%0AUsing%20both%20behavioral%20and%20self-reported%20measures%2C%20we%20examine%20how%20different%0Anatural%20language%20expressions%20of%20uncertainty%20impact%20participants%27%20reliance%2C%0Atrust%2C%20and%20overall%20task%20performance.%20We%20find%20that%20first-person%20expressions%0A%28e.g.%2C%20%22I%27m%20not%20sure%2C%20but...%22%29%20decrease%20participants%27%20confidence%20in%20the%20system%0Aand%20tendency%20to%20agree%20with%20the%20system%27s%20answers%2C%20while%20increasing%20participants%27%0Aaccuracy.%20An%20exploratory%20analysis%20suggests%20that%20this%20increase%20can%20be%20attributed%0Ato%20reduced%20%28but%20not%20fully%20eliminated%29%20overreliance%20on%20incorrect%20answers.%20While%0Awe%20observe%20similar%20effects%20for%20uncertainty%20expressed%20from%20a%20general%20perspective%0A%28e.g.%2C%20%22It%27s%20not%20clear%2C%20but...%22%29%2C%20these%20effects%20are%20weaker%20and%20not%0Astatistically%20significant.%20Our%20findings%20suggest%20that%20using%20natural%20language%0Aexpressions%20of%20uncertainty%20may%20be%20an%20effective%20approach%20for%20reducing%0Aoverreliance%20on%20LLMs%2C%20but%20that%20the%20precise%20language%20used%20matters.%20This%0Ahighlights%20the%20importance%20of%20user%20testing%20before%20deploying%20LLMs%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00623v1&entry.124074799=Read"},
{"title": "Lane Segmentation Refinement with Diffusion Models", "author": "Antonio Ruiz and Andrew Melnik and Dong Wang and Helge Ritter", "abstract": "  The lane graph is a key component for building high-definition (HD) maps and\ncrucial for downstream tasks such as autonomous driving or navigation planning.\nPreviously, He et al. (2022) explored the extraction of the lane-level graph\nfrom aerial imagery utilizing a segmentation based approach. However,\nsegmentation networks struggle to achieve perfect segmentation masks resulting\nin inaccurate lane graph extraction. We explore additional enhancements to\nrefine this segmentation-based approach and extend it with a diffusion\nprobabilistic model (DPM) component. This combination further improves the GEO\nF1 and TOPO F1 scores, which are crucial indicators of the quality of a lane\ngraph, in the undirected graph in non-intersection areas. We conduct\nexperiments on a publicly available dataset, demonstrating that our method\noutperforms the previous approach, particularly in enhancing the connectivity\nof such a graph, as measured by the TOPO F1 score. Moreover, we perform\nablation studies on the individual components of our method to understand their\ncontribution and evaluate their effectiveness.\n", "link": "http://arxiv.org/abs/2405.00620v1", "date": "2024-05-01", "relevancy": 2.1177, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5518}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5251}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5248}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lane%20Segmentation%20Refinement%20with%20Diffusion%20Models&body=Title%3A%20Lane%20Segmentation%20Refinement%20with%20Diffusion%20Models%0AAuthor%3A%20Antonio%20Ruiz%20and%20Andrew%20Melnik%20and%20Dong%20Wang%20and%20Helge%20Ritter%0AAbstract%3A%20%20%20The%20lane%20graph%20is%20a%20key%20component%20for%20building%20high-definition%20%28HD%29%20maps%20and%0Acrucial%20for%20downstream%20tasks%20such%20as%20autonomous%20driving%20or%20navigation%20planning.%0APreviously%2C%20He%20et%20al.%20%282022%29%20explored%20the%20extraction%20of%20the%20lane-level%20graph%0Afrom%20aerial%20imagery%20utilizing%20a%20segmentation%20based%20approach.%20However%2C%0Asegmentation%20networks%20struggle%20to%20achieve%20perfect%20segmentation%20masks%20resulting%0Ain%20inaccurate%20lane%20graph%20extraction.%20We%20explore%20additional%20enhancements%20to%0Arefine%20this%20segmentation-based%20approach%20and%20extend%20it%20with%20a%20diffusion%0Aprobabilistic%20model%20%28DPM%29%20component.%20This%20combination%20further%20improves%20the%20GEO%0AF1%20and%20TOPO%20F1%20scores%2C%20which%20are%20crucial%20indicators%20of%20the%20quality%20of%20a%20lane%0Agraph%2C%20in%20the%20undirected%20graph%20in%20non-intersection%20areas.%20We%20conduct%0Aexperiments%20on%20a%20publicly%20available%20dataset%2C%20demonstrating%20that%20our%20method%0Aoutperforms%20the%20previous%20approach%2C%20particularly%20in%20enhancing%20the%20connectivity%0Aof%20such%20a%20graph%2C%20as%20measured%20by%20the%20TOPO%20F1%20score.%20Moreover%2C%20we%20perform%0Aablation%20studies%20on%20the%20individual%20components%20of%20our%20method%20to%20understand%20their%0Acontribution%20and%20evaluate%20their%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00620v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lane%20Segmentation%20Refinement%20with%20Diffusion%20Models&entry.906535625=Antonio%20Ruiz%20and%20Andrew%20Melnik%20and%20Dong%20Wang%20and%20Helge%20Ritter&entry.1292438233=%20%20The%20lane%20graph%20is%20a%20key%20component%20for%20building%20high-definition%20%28HD%29%20maps%20and%0Acrucial%20for%20downstream%20tasks%20such%20as%20autonomous%20driving%20or%20navigation%20planning.%0APreviously%2C%20He%20et%20al.%20%282022%29%20explored%20the%20extraction%20of%20the%20lane-level%20graph%0Afrom%20aerial%20imagery%20utilizing%20a%20segmentation%20based%20approach.%20However%2C%0Asegmentation%20networks%20struggle%20to%20achieve%20perfect%20segmentation%20masks%20resulting%0Ain%20inaccurate%20lane%20graph%20extraction.%20We%20explore%20additional%20enhancements%20to%0Arefine%20this%20segmentation-based%20approach%20and%20extend%20it%20with%20a%20diffusion%0Aprobabilistic%20model%20%28DPM%29%20component.%20This%20combination%20further%20improves%20the%20GEO%0AF1%20and%20TOPO%20F1%20scores%2C%20which%20are%20crucial%20indicators%20of%20the%20quality%20of%20a%20lane%0Agraph%2C%20in%20the%20undirected%20graph%20in%20non-intersection%20areas.%20We%20conduct%0Aexperiments%20on%20a%20publicly%20available%20dataset%2C%20demonstrating%20that%20our%20method%0Aoutperforms%20the%20previous%20approach%2C%20particularly%20in%20enhancing%20the%20connectivity%0Aof%20such%20a%20graph%2C%20as%20measured%20by%20the%20TOPO%20F1%20score.%20Moreover%2C%20we%20perform%0Aablation%20studies%20on%20the%20individual%20components%20of%20our%20method%20to%20understand%20their%0Acontribution%20and%20evaluate%20their%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00620v1&entry.124074799=Read"},
{"title": "Benchmarking Deep Learning Architectures for Urban Vegetation Point\n  Cloud Semantic Segmentation from MLS", "author": "Aditya Aditya and Bharat Lohani and Jagannath Aryal and Stephan Winter", "abstract": "  Vegetation is crucial for sustainable and resilient cities providing various\necosystem services and well-being of humans. However, vegetation is under\ncritical stress with rapid urbanization and expanding infrastructure\nfootprints. Consequently, mapping of this vegetation is essential in the urban\nenvironment. Recently, deep learning for point cloud semantic segmentation has\nshown significant progress. Advanced models attempt to obtain state-of-the-art\nperformance on benchmark datasets, comprising multiple classes and representing\nreal world scenarios. However, class specific segmentation with respect to\nvegetation points has not been explored. Therefore, selection of a deep\nlearning model for vegetation points segmentation is ambiguous. To address this\nproblem, we provide a comprehensive assessment of point-based deep learning\nmodels for semantic segmentation of vegetation class. We have selected seven\nrepresentative point-based models, namely PointCNN, KPConv (omni-supervised),\nRandLANet, SCFNet, PointNeXt, SPoTr and PointMetaBase. These models are\ninvestigated on three different datasets, specifically Chandigarh, Toronto3D\nand Kerala, which are characterized by diverse nature of vegetation and varying\nscene complexity combined with changing per-point features and class-wise\ncomposition. PointMetaBase and KPConv (omni-supervised) achieve the highest\nmIoU on the Chandigarh (95.24%) and Toronto3D datasets (91.26%), respectively\nwhile PointCNN provides the highest mIoU on the Kerala dataset (85.68%). The\npaper develops a deeper insight, hitherto not reported, into the working of\nthese models for vegetation segmentation and outlines the ingredients that\nshould be included in a model specifically for vegetation segmentation. This\npaper is a step towards the development of a novel architecture for vegetation\npoints segmentation.\n", "link": "http://arxiv.org/abs/2306.10274v3", "date": "2024-05-01", "relevancy": 2.1141, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5173}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5068}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Deep%20Learning%20Architectures%20for%20Urban%20Vegetation%20Point%0A%20%20Cloud%20Semantic%20Segmentation%20from%20MLS&body=Title%3A%20Benchmarking%20Deep%20Learning%20Architectures%20for%20Urban%20Vegetation%20Point%0A%20%20Cloud%20Semantic%20Segmentation%20from%20MLS%0AAuthor%3A%20Aditya%20Aditya%20and%20Bharat%20Lohani%20and%20Jagannath%20Aryal%20and%20Stephan%20Winter%0AAbstract%3A%20%20%20Vegetation%20is%20crucial%20for%20sustainable%20and%20resilient%20cities%20providing%20various%0Aecosystem%20services%20and%20well-being%20of%20humans.%20However%2C%20vegetation%20is%20under%0Acritical%20stress%20with%20rapid%20urbanization%20and%20expanding%20infrastructure%0Afootprints.%20Consequently%2C%20mapping%20of%20this%20vegetation%20is%20essential%20in%20the%20urban%0Aenvironment.%20Recently%2C%20deep%20learning%20for%20point%20cloud%20semantic%20segmentation%20has%0Ashown%20significant%20progress.%20Advanced%20models%20attempt%20to%20obtain%20state-of-the-art%0Aperformance%20on%20benchmark%20datasets%2C%20comprising%20multiple%20classes%20and%20representing%0Areal%20world%20scenarios.%20However%2C%20class%20specific%20segmentation%20with%20respect%20to%0Avegetation%20points%20has%20not%20been%20explored.%20Therefore%2C%20selection%20of%20a%20deep%0Alearning%20model%20for%20vegetation%20points%20segmentation%20is%20ambiguous.%20To%20address%20this%0Aproblem%2C%20we%20provide%20a%20comprehensive%20assessment%20of%20point-based%20deep%20learning%0Amodels%20for%20semantic%20segmentation%20of%20vegetation%20class.%20We%20have%20selected%20seven%0Arepresentative%20point-based%20models%2C%20namely%20PointCNN%2C%20KPConv%20%28omni-supervised%29%2C%0ARandLANet%2C%20SCFNet%2C%20PointNeXt%2C%20SPoTr%20and%20PointMetaBase.%20These%20models%20are%0Ainvestigated%20on%20three%20different%20datasets%2C%20specifically%20Chandigarh%2C%20Toronto3D%0Aand%20Kerala%2C%20which%20are%20characterized%20by%20diverse%20nature%20of%20vegetation%20and%20varying%0Ascene%20complexity%20combined%20with%20changing%20per-point%20features%20and%20class-wise%0Acomposition.%20PointMetaBase%20and%20KPConv%20%28omni-supervised%29%20achieve%20the%20highest%0AmIoU%20on%20the%20Chandigarh%20%2895.24%25%29%20and%20Toronto3D%20datasets%20%2891.26%25%29%2C%20respectively%0Awhile%20PointCNN%20provides%20the%20highest%20mIoU%20on%20the%20Kerala%20dataset%20%2885.68%25%29.%20The%0Apaper%20develops%20a%20deeper%20insight%2C%20hitherto%20not%20reported%2C%20into%20the%20working%20of%0Athese%20models%20for%20vegetation%20segmentation%20and%20outlines%20the%20ingredients%20that%0Ashould%20be%20included%20in%20a%20model%20specifically%20for%20vegetation%20segmentation.%20This%0Apaper%20is%20a%20step%20towards%20the%20development%20of%20a%20novel%20architecture%20for%20vegetation%0Apoints%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10274v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Deep%20Learning%20Architectures%20for%20Urban%20Vegetation%20Point%0A%20%20Cloud%20Semantic%20Segmentation%20from%20MLS&entry.906535625=Aditya%20Aditya%20and%20Bharat%20Lohani%20and%20Jagannath%20Aryal%20and%20Stephan%20Winter&entry.1292438233=%20%20Vegetation%20is%20crucial%20for%20sustainable%20and%20resilient%20cities%20providing%20various%0Aecosystem%20services%20and%20well-being%20of%20humans.%20However%2C%20vegetation%20is%20under%0Acritical%20stress%20with%20rapid%20urbanization%20and%20expanding%20infrastructure%0Afootprints.%20Consequently%2C%20mapping%20of%20this%20vegetation%20is%20essential%20in%20the%20urban%0Aenvironment.%20Recently%2C%20deep%20learning%20for%20point%20cloud%20semantic%20segmentation%20has%0Ashown%20significant%20progress.%20Advanced%20models%20attempt%20to%20obtain%20state-of-the-art%0Aperformance%20on%20benchmark%20datasets%2C%20comprising%20multiple%20classes%20and%20representing%0Areal%20world%20scenarios.%20However%2C%20class%20specific%20segmentation%20with%20respect%20to%0Avegetation%20points%20has%20not%20been%20explored.%20Therefore%2C%20selection%20of%20a%20deep%0Alearning%20model%20for%20vegetation%20points%20segmentation%20is%20ambiguous.%20To%20address%20this%0Aproblem%2C%20we%20provide%20a%20comprehensive%20assessment%20of%20point-based%20deep%20learning%0Amodels%20for%20semantic%20segmentation%20of%20vegetation%20class.%20We%20have%20selected%20seven%0Arepresentative%20point-based%20models%2C%20namely%20PointCNN%2C%20KPConv%20%28omni-supervised%29%2C%0ARandLANet%2C%20SCFNet%2C%20PointNeXt%2C%20SPoTr%20and%20PointMetaBase.%20These%20models%20are%0Ainvestigated%20on%20three%20different%20datasets%2C%20specifically%20Chandigarh%2C%20Toronto3D%0Aand%20Kerala%2C%20which%20are%20characterized%20by%20diverse%20nature%20of%20vegetation%20and%20varying%0Ascene%20complexity%20combined%20with%20changing%20per-point%20features%20and%20class-wise%0Acomposition.%20PointMetaBase%20and%20KPConv%20%28omni-supervised%29%20achieve%20the%20highest%0AmIoU%20on%20the%20Chandigarh%20%2895.24%25%29%20and%20Toronto3D%20datasets%20%2891.26%25%29%2C%20respectively%0Awhile%20PointCNN%20provides%20the%20highest%20mIoU%20on%20the%20Kerala%20dataset%20%2885.68%25%29.%20The%0Apaper%20develops%20a%20deeper%20insight%2C%20hitherto%20not%20reported%2C%20into%20the%20working%20of%0Athese%20models%20for%20vegetation%20segmentation%20and%20outlines%20the%20ingredients%20that%0Ashould%20be%20included%20in%20a%20model%20specifically%20for%20vegetation%20segmentation.%20This%0Apaper%20is%20a%20step%20towards%20the%20development%20of%20a%20novel%20architecture%20for%20vegetation%0Apoints%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10274v3&entry.124074799=Read"},
{"title": "Bridging Dimensions: Confident Reachability for High-Dimensional\n  Controllers", "author": "Yuang Geng and Jake Baldauf and Souradeep Dutta and Chao Huang and Ivan Ruchkin", "abstract": "  Autonomous systems are increasingly implemented using end-to-end\nlearning-based controllers. Such controllers make decisions that are executed\non the real system, with images as one of the primary sensing modalities. Deep\nneural networks form a fundamental building block of such controllers.\nUnfortunately, the existing neural-network verification tools do not scale to\ninputs with thousands of dimensions -- especially when the individual inputs\n(such as pixels) are devoid of clear physical meaning. This paper takes a step\ntowards connecting exhaustive closed-loop verification with high-dimensional\ncontrollers. Our key insight is that the behavior of a high-dimensional\ncontroller can be approximated with several low-dimensional controllers. To\nbalance the approximation accuracy and verifiability of our low-dimensional\ncontrollers, we leverage the latest verification-aware knowledge distillation.\nThen, we inflate low-dimensional reachability results with statistical\napproximation errors, yielding a high-confidence reachability guarantee for the\nhigh-dimensional controller. We investigate two inflation techniques -- based\non trajectories and control actions -- both of which show convincing\nperformance in three OpenAI gym benchmarks.\n", "link": "http://arxiv.org/abs/2311.04843v3", "date": "2024-05-01", "relevancy": 2.1117, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5755}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5204}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5164}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridging%20Dimensions%3A%20Confident%20Reachability%20for%20High-Dimensional%0A%20%20Controllers&body=Title%3A%20Bridging%20Dimensions%3A%20Confident%20Reachability%20for%20High-Dimensional%0A%20%20Controllers%0AAuthor%3A%20Yuang%20Geng%20and%20Jake%20Baldauf%20and%20Souradeep%20Dutta%20and%20Chao%20Huang%20and%20Ivan%20Ruchkin%0AAbstract%3A%20%20%20Autonomous%20systems%20are%20increasingly%20implemented%20using%20end-to-end%0Alearning-based%20controllers.%20Such%20controllers%20make%20decisions%20that%20are%20executed%0Aon%20the%20real%20system%2C%20with%20images%20as%20one%20of%20the%20primary%20sensing%20modalities.%20Deep%0Aneural%20networks%20form%20a%20fundamental%20building%20block%20of%20such%20controllers.%0AUnfortunately%2C%20the%20existing%20neural-network%20verification%20tools%20do%20not%20scale%20to%0Ainputs%20with%20thousands%20of%20dimensions%20--%20especially%20when%20the%20individual%20inputs%0A%28such%20as%20pixels%29%20are%20devoid%20of%20clear%20physical%20meaning.%20This%20paper%20takes%20a%20step%0Atowards%20connecting%20exhaustive%20closed-loop%20verification%20with%20high-dimensional%0Acontrollers.%20Our%20key%20insight%20is%20that%20the%20behavior%20of%20a%20high-dimensional%0Acontroller%20can%20be%20approximated%20with%20several%20low-dimensional%20controllers.%20To%0Abalance%20the%20approximation%20accuracy%20and%20verifiability%20of%20our%20low-dimensional%0Acontrollers%2C%20we%20leverage%20the%20latest%20verification-aware%20knowledge%20distillation.%0AThen%2C%20we%20inflate%20low-dimensional%20reachability%20results%20with%20statistical%0Aapproximation%20errors%2C%20yielding%20a%20high-confidence%20reachability%20guarantee%20for%20the%0Ahigh-dimensional%20controller.%20We%20investigate%20two%20inflation%20techniques%20--%20based%0Aon%20trajectories%20and%20control%20actions%20--%20both%20of%20which%20show%20convincing%0Aperformance%20in%20three%20OpenAI%20gym%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04843v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Dimensions%3A%20Confident%20Reachability%20for%20High-Dimensional%0A%20%20Controllers&entry.906535625=Yuang%20Geng%20and%20Jake%20Baldauf%20and%20Souradeep%20Dutta%20and%20Chao%20Huang%20and%20Ivan%20Ruchkin&entry.1292438233=%20%20Autonomous%20systems%20are%20increasingly%20implemented%20using%20end-to-end%0Alearning-based%20controllers.%20Such%20controllers%20make%20decisions%20that%20are%20executed%0Aon%20the%20real%20system%2C%20with%20images%20as%20one%20of%20the%20primary%20sensing%20modalities.%20Deep%0Aneural%20networks%20form%20a%20fundamental%20building%20block%20of%20such%20controllers.%0AUnfortunately%2C%20the%20existing%20neural-network%20verification%20tools%20do%20not%20scale%20to%0Ainputs%20with%20thousands%20of%20dimensions%20--%20especially%20when%20the%20individual%20inputs%0A%28such%20as%20pixels%29%20are%20devoid%20of%20clear%20physical%20meaning.%20This%20paper%20takes%20a%20step%0Atowards%20connecting%20exhaustive%20closed-loop%20verification%20with%20high-dimensional%0Acontrollers.%20Our%20key%20insight%20is%20that%20the%20behavior%20of%20a%20high-dimensional%0Acontroller%20can%20be%20approximated%20with%20several%20low-dimensional%20controllers.%20To%0Abalance%20the%20approximation%20accuracy%20and%20verifiability%20of%20our%20low-dimensional%0Acontrollers%2C%20we%20leverage%20the%20latest%20verification-aware%20knowledge%20distillation.%0AThen%2C%20we%20inflate%20low-dimensional%20reachability%20results%20with%20statistical%0Aapproximation%20errors%2C%20yielding%20a%20high-confidence%20reachability%20guarantee%20for%20the%0Ahigh-dimensional%20controller.%20We%20investigate%20two%20inflation%20techniques%20--%20based%0Aon%20trajectories%20and%20control%20actions%20--%20both%20of%20which%20show%20convincing%0Aperformance%20in%20three%20OpenAI%20gym%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04843v3&entry.124074799=Read"},
{"title": "In Anticipation of Perfect Deepfake: Identity-anchored Artifact-agnostic\n  Detection under Rebalanced Deepfake Detection Protocol", "author": "Wei-Han Wang and Chin-Yuan Yeh and Hsi-Wen Chen and De-Nian Yang and Ming-Syan Chen", "abstract": "  As deep generative models advance, we anticipate deepfakes achieving\n\"perfection\"-generating no discernible artifacts or noise. However, current\ndeepfake detectors, intentionally or inadvertently, rely on such artifacts for\ndetection, as they are exclusive to deepfakes and absent in genuine examples.\nTo bridge this gap, we introduce the Rebalanced Deepfake Detection Protocol\n(RDDP) to stress-test detectors under balanced scenarios where genuine and\nforged examples bear similar artifacts. We offer two RDDP variants:\nRDDP-WHITEHAT uses white-hat deepfake algorithms to create 'self-deepfakes,'\ngenuine portrait videos with the resemblance of the underlying identity, yet\ncarry similar artifacts to deepfake videos; RDDP-SURROGATE employs surrogate\nfunctions (e.g., Gaussian noise) to process both genuine and forged examples,\nintroducing equivalent noise, thereby sidestepping the need of deepfake\nalgorithms.\n  Towards detecting perfect deepfake videos that aligns with genuine ones, we\npresent ID-Miner, a detector that identifies the puppeteer behind the disguise\nby focusing on motion over artifacts or appearances. As an identity-based\ndetector, it authenticates videos by comparing them with reference footage.\nEquipped with the artifact-agnostic loss at frame-level and the\nidentity-anchored loss at video-level, ID-Miner effectively singles out\nidentity signals amidst distracting variations. Extensive experiments comparing\nID-Miner with 12 baseline detectors under both conventional and RDDP\nevaluations with two deepfake datasets, along with additional qualitative\nstudies, affirm the superiority of our method and the necessity for detectors\ndesigned to counter perfect deepfakes.\n", "link": "http://arxiv.org/abs/2405.00483v1", "date": "2024-05-01", "relevancy": 2.1084, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5665}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4995}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4975}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20In%20Anticipation%20of%20Perfect%20Deepfake%3A%20Identity-anchored%20Artifact-agnostic%0A%20%20Detection%20under%20Rebalanced%20Deepfake%20Detection%20Protocol&body=Title%3A%20In%20Anticipation%20of%20Perfect%20Deepfake%3A%20Identity-anchored%20Artifact-agnostic%0A%20%20Detection%20under%20Rebalanced%20Deepfake%20Detection%20Protocol%0AAuthor%3A%20Wei-Han%20Wang%20and%20Chin-Yuan%20Yeh%20and%20Hsi-Wen%20Chen%20and%20De-Nian%20Yang%20and%20Ming-Syan%20Chen%0AAbstract%3A%20%20%20As%20deep%20generative%20models%20advance%2C%20we%20anticipate%20deepfakes%20achieving%0A%22perfection%22-generating%20no%20discernible%20artifacts%20or%20noise.%20However%2C%20current%0Adeepfake%20detectors%2C%20intentionally%20or%20inadvertently%2C%20rely%20on%20such%20artifacts%20for%0Adetection%2C%20as%20they%20are%20exclusive%20to%20deepfakes%20and%20absent%20in%20genuine%20examples.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20the%20Rebalanced%20Deepfake%20Detection%20Protocol%0A%28RDDP%29%20to%20stress-test%20detectors%20under%20balanced%20scenarios%20where%20genuine%20and%0Aforged%20examples%20bear%20similar%20artifacts.%20We%20offer%20two%20RDDP%20variants%3A%0ARDDP-WHITEHAT%20uses%20white-hat%20deepfake%20algorithms%20to%20create%20%27self-deepfakes%2C%27%0Agenuine%20portrait%20videos%20with%20the%20resemblance%20of%20the%20underlying%20identity%2C%20yet%0Acarry%20similar%20artifacts%20to%20deepfake%20videos%3B%20RDDP-SURROGATE%20employs%20surrogate%0Afunctions%20%28e.g.%2C%20Gaussian%20noise%29%20to%20process%20both%20genuine%20and%20forged%20examples%2C%0Aintroducing%20equivalent%20noise%2C%20thereby%20sidestepping%20the%20need%20of%20deepfake%0Aalgorithms.%0A%20%20Towards%20detecting%20perfect%20deepfake%20videos%20that%20aligns%20with%20genuine%20ones%2C%20we%0Apresent%20ID-Miner%2C%20a%20detector%20that%20identifies%20the%20puppeteer%20behind%20the%20disguise%0Aby%20focusing%20on%20motion%20over%20artifacts%20or%20appearances.%20As%20an%20identity-based%0Adetector%2C%20it%20authenticates%20videos%20by%20comparing%20them%20with%20reference%20footage.%0AEquipped%20with%20the%20artifact-agnostic%20loss%20at%20frame-level%20and%20the%0Aidentity-anchored%20loss%20at%20video-level%2C%20ID-Miner%20effectively%20singles%20out%0Aidentity%20signals%20amidst%20distracting%20variations.%20Extensive%20experiments%20comparing%0AID-Miner%20with%2012%20baseline%20detectors%20under%20both%20conventional%20and%20RDDP%0Aevaluations%20with%20two%20deepfake%20datasets%2C%20along%20with%20additional%20qualitative%0Astudies%2C%20affirm%20the%20superiority%20of%20our%20method%20and%20the%20necessity%20for%20detectors%0Adesigned%20to%20counter%20perfect%20deepfakes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00483v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20Anticipation%20of%20Perfect%20Deepfake%3A%20Identity-anchored%20Artifact-agnostic%0A%20%20Detection%20under%20Rebalanced%20Deepfake%20Detection%20Protocol&entry.906535625=Wei-Han%20Wang%20and%20Chin-Yuan%20Yeh%20and%20Hsi-Wen%20Chen%20and%20De-Nian%20Yang%20and%20Ming-Syan%20Chen&entry.1292438233=%20%20As%20deep%20generative%20models%20advance%2C%20we%20anticipate%20deepfakes%20achieving%0A%22perfection%22-generating%20no%20discernible%20artifacts%20or%20noise.%20However%2C%20current%0Adeepfake%20detectors%2C%20intentionally%20or%20inadvertently%2C%20rely%20on%20such%20artifacts%20for%0Adetection%2C%20as%20they%20are%20exclusive%20to%20deepfakes%20and%20absent%20in%20genuine%20examples.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20the%20Rebalanced%20Deepfake%20Detection%20Protocol%0A%28RDDP%29%20to%20stress-test%20detectors%20under%20balanced%20scenarios%20where%20genuine%20and%0Aforged%20examples%20bear%20similar%20artifacts.%20We%20offer%20two%20RDDP%20variants%3A%0ARDDP-WHITEHAT%20uses%20white-hat%20deepfake%20algorithms%20to%20create%20%27self-deepfakes%2C%27%0Agenuine%20portrait%20videos%20with%20the%20resemblance%20of%20the%20underlying%20identity%2C%20yet%0Acarry%20similar%20artifacts%20to%20deepfake%20videos%3B%20RDDP-SURROGATE%20employs%20surrogate%0Afunctions%20%28e.g.%2C%20Gaussian%20noise%29%20to%20process%20both%20genuine%20and%20forged%20examples%2C%0Aintroducing%20equivalent%20noise%2C%20thereby%20sidestepping%20the%20need%20of%20deepfake%0Aalgorithms.%0A%20%20Towards%20detecting%20perfect%20deepfake%20videos%20that%20aligns%20with%20genuine%20ones%2C%20we%0Apresent%20ID-Miner%2C%20a%20detector%20that%20identifies%20the%20puppeteer%20behind%20the%20disguise%0Aby%20focusing%20on%20motion%20over%20artifacts%20or%20appearances.%20As%20an%20identity-based%0Adetector%2C%20it%20authenticates%20videos%20by%20comparing%20them%20with%20reference%20footage.%0AEquipped%20with%20the%20artifact-agnostic%20loss%20at%20frame-level%20and%20the%0Aidentity-anchored%20loss%20at%20video-level%2C%20ID-Miner%20effectively%20singles%20out%0Aidentity%20signals%20amidst%20distracting%20variations.%20Extensive%20experiments%20comparing%0AID-Miner%20with%2012%20baseline%20detectors%20under%20both%20conventional%20and%20RDDP%0Aevaluations%20with%20two%20deepfake%20datasets%2C%20along%20with%20additional%20qualitative%0Astudies%2C%20affirm%20the%20superiority%20of%20our%20method%20and%20the%20necessity%20for%20detectors%0Adesigned%20to%20counter%20perfect%20deepfakes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00483v1&entry.124074799=Read"},
{"title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A\n  Review", "author": "Nicol\u00f2 Botteghi and Mannes Poel and Christoph Brune", "abstract": "  This review addresses the problem of learning abstract representations of the\nmeasurement data in the context of Deep Reinforcement Learning (DRL). While the\ndata are often ambiguous, high-dimensional, and complex to interpret, many\ndynamical systems can be effectively described by a low-dimensional set of\nstate variables. Discovering these state variables from the data is a crucial\naspect for (i) improving the data efficiency, robustness, and generalization of\nDRL methods, (ii) tackling the curse of dimensionality, and (iii) bringing\ninterpretability and insights into black-box DRL. This review provides a\ncomprehensive and complete overview of unsupervised representation learning in\nDRL by describing the main Deep Learning tools used for learning\nrepresentations of the world, providing a systematic view of the method and\nprinciples, summarizing applications, benchmarks and evaluation strategies, and\ndiscussing open challenges and future directions.\n", "link": "http://arxiv.org/abs/2208.14226v3", "date": "2024-05-01", "relevancy": 2.1073, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5417}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5341}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4713}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Representation%20Learning%20in%20Deep%20Reinforcement%20Learning%3A%20A%0A%20%20Review&body=Title%3A%20Unsupervised%20Representation%20Learning%20in%20Deep%20Reinforcement%20Learning%3A%20A%0A%20%20Review%0AAuthor%3A%20Nicol%C3%B2%20Botteghi%20and%20Mannes%20Poel%20and%20Christoph%20Brune%0AAbstract%3A%20%20%20This%20review%20addresses%20the%20problem%20of%20learning%20abstract%20representations%20of%20the%0Ameasurement%20data%20in%20the%20context%20of%20Deep%20Reinforcement%20Learning%20%28DRL%29.%20While%20the%0Adata%20are%20often%20ambiguous%2C%20high-dimensional%2C%20and%20complex%20to%20interpret%2C%20many%0Adynamical%20systems%20can%20be%20effectively%20described%20by%20a%20low-dimensional%20set%20of%0Astate%20variables.%20Discovering%20these%20state%20variables%20from%20the%20data%20is%20a%20crucial%0Aaspect%20for%20%28i%29%20improving%20the%20data%20efficiency%2C%20robustness%2C%20and%20generalization%20of%0ADRL%20methods%2C%20%28ii%29%20tackling%20the%20curse%20of%20dimensionality%2C%20and%20%28iii%29%20bringing%0Ainterpretability%20and%20insights%20into%20black-box%20DRL.%20This%20review%20provides%20a%0Acomprehensive%20and%20complete%20overview%20of%20unsupervised%20representation%20learning%20in%0ADRL%20by%20describing%20the%20main%20Deep%20Learning%20tools%20used%20for%20learning%0Arepresentations%20of%20the%20world%2C%20providing%20a%20systematic%20view%20of%20the%20method%20and%0Aprinciples%2C%20summarizing%20applications%2C%20benchmarks%20and%20evaluation%20strategies%2C%20and%0Adiscussing%20open%20challenges%20and%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.14226v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Representation%20Learning%20in%20Deep%20Reinforcement%20Learning%3A%20A%0A%20%20Review&entry.906535625=Nicol%C3%B2%20Botteghi%20and%20Mannes%20Poel%20and%20Christoph%20Brune&entry.1292438233=%20%20This%20review%20addresses%20the%20problem%20of%20learning%20abstract%20representations%20of%20the%0Ameasurement%20data%20in%20the%20context%20of%20Deep%20Reinforcement%20Learning%20%28DRL%29.%20While%20the%0Adata%20are%20often%20ambiguous%2C%20high-dimensional%2C%20and%20complex%20to%20interpret%2C%20many%0Adynamical%20systems%20can%20be%20effectively%20described%20by%20a%20low-dimensional%20set%20of%0Astate%20variables.%20Discovering%20these%20state%20variables%20from%20the%20data%20is%20a%20crucial%0Aaspect%20for%20%28i%29%20improving%20the%20data%20efficiency%2C%20robustness%2C%20and%20generalization%20of%0ADRL%20methods%2C%20%28ii%29%20tackling%20the%20curse%20of%20dimensionality%2C%20and%20%28iii%29%20bringing%0Ainterpretability%20and%20insights%20into%20black-box%20DRL.%20This%20review%20provides%20a%0Acomprehensive%20and%20complete%20overview%20of%20unsupervised%20representation%20learning%20in%0ADRL%20by%20describing%20the%20main%20Deep%20Learning%20tools%20used%20for%20learning%0Arepresentations%20of%20the%20world%2C%20providing%20a%20systematic%20view%20of%20the%20method%20and%0Aprinciples%2C%20summarizing%20applications%2C%20benchmarks%20and%20evaluation%20strategies%2C%20and%0Adiscussing%20open%20challenges%20and%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.14226v3&entry.124074799=Read"},
{"title": "ConstrainedZero: Chance-Constrained POMDP Planning using Learned\n  Probabilistic Failure Surrogates and Adaptive Safety Constraints", "author": "Robert J. Moss and Arec Jamgochian and Johannes Fischer and Anthony Corso and Mykel J. Kochenderfer", "abstract": "  To plan safely in uncertain environments, agents must balance utility with\nsafety constraints. Safe planning problems can be modeled as a\nchance-constrained partially observable Markov decision process (CC-POMDP) and\nsolutions often use expensive rollouts or heuristics to estimate the optimal\nvalue and action-selection policy. This work introduces the ConstrainedZero\npolicy iteration algorithm that solves CC-POMDPs in belief space by learning\nneural network approximations of the optimal value and policy with an\nadditional network head that estimates the failure probability given a belief.\nThis failure probability guides safe action selection during online Monte Carlo\ntree search (MCTS). To avoid overemphasizing search based on the failure\nestimates, we introduce $\\Delta$-MCTS, which uses adaptive conformal inference\nto update the failure threshold during planning. The approach is tested on a\nsafety-critical POMDP benchmark, an aircraft collision avoidance system, and\nthe sustainability problem of safe CO$_2$ storage. Results show that by\nseparating safety constraints from the objective we can achieve a target level\nof safety without optimizing the balance between rewards and costs.\n", "link": "http://arxiv.org/abs/2405.00644v1", "date": "2024-05-01", "relevancy": 2.0836, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5411}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5216}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5121}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ConstrainedZero%3A%20Chance-Constrained%20POMDP%20Planning%20using%20Learned%0A%20%20Probabilistic%20Failure%20Surrogates%20and%20Adaptive%20Safety%20Constraints&body=Title%3A%20ConstrainedZero%3A%20Chance-Constrained%20POMDP%20Planning%20using%20Learned%0A%20%20Probabilistic%20Failure%20Surrogates%20and%20Adaptive%20Safety%20Constraints%0AAuthor%3A%20Robert%20J.%20Moss%20and%20Arec%20Jamgochian%20and%20Johannes%20Fischer%20and%20Anthony%20Corso%20and%20Mykel%20J.%20Kochenderfer%0AAbstract%3A%20%20%20To%20plan%20safely%20in%20uncertain%20environments%2C%20agents%20must%20balance%20utility%20with%0Asafety%20constraints.%20Safe%20planning%20problems%20can%20be%20modeled%20as%20a%0Achance-constrained%20partially%20observable%20Markov%20decision%20process%20%28CC-POMDP%29%20and%0Asolutions%20often%20use%20expensive%20rollouts%20or%20heuristics%20to%20estimate%20the%20optimal%0Avalue%20and%20action-selection%20policy.%20This%20work%20introduces%20the%20ConstrainedZero%0Apolicy%20iteration%20algorithm%20that%20solves%20CC-POMDPs%20in%20belief%20space%20by%20learning%0Aneural%20network%20approximations%20of%20the%20optimal%20value%20and%20policy%20with%20an%0Aadditional%20network%20head%20that%20estimates%20the%20failure%20probability%20given%20a%20belief.%0AThis%20failure%20probability%20guides%20safe%20action%20selection%20during%20online%20Monte%20Carlo%0Atree%20search%20%28MCTS%29.%20To%20avoid%20overemphasizing%20search%20based%20on%20the%20failure%0Aestimates%2C%20we%20introduce%20%24%5CDelta%24-MCTS%2C%20which%20uses%20adaptive%20conformal%20inference%0Ato%20update%20the%20failure%20threshold%20during%20planning.%20The%20approach%20is%20tested%20on%20a%0Asafety-critical%20POMDP%20benchmark%2C%20an%20aircraft%20collision%20avoidance%20system%2C%20and%0Athe%20sustainability%20problem%20of%20safe%20CO%24_2%24%20storage.%20Results%20show%20that%20by%0Aseparating%20safety%20constraints%20from%20the%20objective%20we%20can%20achieve%20a%20target%20level%0Aof%20safety%20without%20optimizing%20the%20balance%20between%20rewards%20and%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00644v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConstrainedZero%3A%20Chance-Constrained%20POMDP%20Planning%20using%20Learned%0A%20%20Probabilistic%20Failure%20Surrogates%20and%20Adaptive%20Safety%20Constraints&entry.906535625=Robert%20J.%20Moss%20and%20Arec%20Jamgochian%20and%20Johannes%20Fischer%20and%20Anthony%20Corso%20and%20Mykel%20J.%20Kochenderfer&entry.1292438233=%20%20To%20plan%20safely%20in%20uncertain%20environments%2C%20agents%20must%20balance%20utility%20with%0Asafety%20constraints.%20Safe%20planning%20problems%20can%20be%20modeled%20as%20a%0Achance-constrained%20partially%20observable%20Markov%20decision%20process%20%28CC-POMDP%29%20and%0Asolutions%20often%20use%20expensive%20rollouts%20or%20heuristics%20to%20estimate%20the%20optimal%0Avalue%20and%20action-selection%20policy.%20This%20work%20introduces%20the%20ConstrainedZero%0Apolicy%20iteration%20algorithm%20that%20solves%20CC-POMDPs%20in%20belief%20space%20by%20learning%0Aneural%20network%20approximations%20of%20the%20optimal%20value%20and%20policy%20with%20an%0Aadditional%20network%20head%20that%20estimates%20the%20failure%20probability%20given%20a%20belief.%0AThis%20failure%20probability%20guides%20safe%20action%20selection%20during%20online%20Monte%20Carlo%0Atree%20search%20%28MCTS%29.%20To%20avoid%20overemphasizing%20search%20based%20on%20the%20failure%0Aestimates%2C%20we%20introduce%20%24%5CDelta%24-MCTS%2C%20which%20uses%20adaptive%20conformal%20inference%0Ato%20update%20the%20failure%20threshold%20during%20planning.%20The%20approach%20is%20tested%20on%20a%0Asafety-critical%20POMDP%20benchmark%2C%20an%20aircraft%20collision%20avoidance%20system%2C%20and%0Athe%20sustainability%20problem%20of%20safe%20CO%24_2%24%20storage.%20Results%20show%20that%20by%0Aseparating%20safety%20constraints%20from%20the%20objective%20we%20can%20achieve%20a%20target%20level%0Aof%20safety%20without%20optimizing%20the%20balance%20between%20rewards%20and%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00644v1&entry.124074799=Read"},
{"title": "Swarm Learning: A Survey of Concepts, Applications, and Trends", "author": "Elham Shammar and Xiaohui Cui and Mohammed A. A. Al-qaness", "abstract": "  Deep learning models have raised privacy and security concerns due to their\nreliance on large datasets on central servers. As the number of Internet of\nThings (IoT) devices increases, artificial intelligence (AI) will be crucial\nfor resource management, data processing, and knowledge acquisition. To address\nthose issues, federated learning (FL) has introduced a novel approach to\nbuilding a versatile, large-scale machine learning framework that operates in a\ndecentralized and hardware-agnostic manner. However, FL faces network bandwidth\nlimitations and data breaches. To reduce the central dependency in FL and\nincrease scalability, swarm learning (SL) has been proposed in collaboration\nwith Hewlett Packard Enterprise (HPE). SL represents a decentralized machine\nlearning framework that leverages blockchain technology for secure, scalable,\nand private data management. A blockchain-based network enables the exchange\nand aggregation of model parameters among participants, thus mitigating the\nrisk of a single point of failure and eliminating communication bottlenecks. To\nthe best of our knowledge, this survey is the first to introduce the principles\nof Swarm Learning, its architectural design, and its fields of application. In\naddition, it highlights numerous research avenues that require further\nexploration by academic and industry communities to unlock the full potential\nand applications of SL.\n", "link": "http://arxiv.org/abs/2405.00556v1", "date": "2024-05-01", "relevancy": 2.0753, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4401}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3996}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Swarm%20Learning%3A%20A%20Survey%20of%20Concepts%2C%20Applications%2C%20and%20Trends&body=Title%3A%20Swarm%20Learning%3A%20A%20Survey%20of%20Concepts%2C%20Applications%2C%20and%20Trends%0AAuthor%3A%20Elham%20Shammar%20and%20Xiaohui%20Cui%20and%20Mohammed%20A.%20A.%20Al-qaness%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20raised%20privacy%20and%20security%20concerns%20due%20to%20their%0Areliance%20on%20large%20datasets%20on%20central%20servers.%20As%20the%20number%20of%20Internet%20of%0AThings%20%28IoT%29%20devices%20increases%2C%20artificial%20intelligence%20%28AI%29%20will%20be%20crucial%0Afor%20resource%20management%2C%20data%20processing%2C%20and%20knowledge%20acquisition.%20To%20address%0Athose%20issues%2C%20federated%20learning%20%28FL%29%20has%20introduced%20a%20novel%20approach%20to%0Abuilding%20a%20versatile%2C%20large-scale%20machine%20learning%20framework%20that%20operates%20in%20a%0Adecentralized%20and%20hardware-agnostic%20manner.%20However%2C%20FL%20faces%20network%20bandwidth%0Alimitations%20and%20data%20breaches.%20To%20reduce%20the%20central%20dependency%20in%20FL%20and%0Aincrease%20scalability%2C%20swarm%20learning%20%28SL%29%20has%20been%20proposed%20in%20collaboration%0Awith%20Hewlett%20Packard%20Enterprise%20%28HPE%29.%20SL%20represents%20a%20decentralized%20machine%0Alearning%20framework%20that%20leverages%20blockchain%20technology%20for%20secure%2C%20scalable%2C%0Aand%20private%20data%20management.%20A%20blockchain-based%20network%20enables%20the%20exchange%0Aand%20aggregation%20of%20model%20parameters%20among%20participants%2C%20thus%20mitigating%20the%0Arisk%20of%20a%20single%20point%20of%20failure%20and%20eliminating%20communication%20bottlenecks.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20survey%20is%20the%20first%20to%20introduce%20the%20principles%0Aof%20Swarm%20Learning%2C%20its%20architectural%20design%2C%20and%20its%20fields%20of%20application.%20In%0Aaddition%2C%20it%20highlights%20numerous%20research%20avenues%20that%20require%20further%0Aexploration%20by%20academic%20and%20industry%20communities%20to%20unlock%20the%20full%20potential%0Aand%20applications%20of%20SL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00556v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swarm%20Learning%3A%20A%20Survey%20of%20Concepts%2C%20Applications%2C%20and%20Trends&entry.906535625=Elham%20Shammar%20and%20Xiaohui%20Cui%20and%20Mohammed%20A.%20A.%20Al-qaness&entry.1292438233=%20%20Deep%20learning%20models%20have%20raised%20privacy%20and%20security%20concerns%20due%20to%20their%0Areliance%20on%20large%20datasets%20on%20central%20servers.%20As%20the%20number%20of%20Internet%20of%0AThings%20%28IoT%29%20devices%20increases%2C%20artificial%20intelligence%20%28AI%29%20will%20be%20crucial%0Afor%20resource%20management%2C%20data%20processing%2C%20and%20knowledge%20acquisition.%20To%20address%0Athose%20issues%2C%20federated%20learning%20%28FL%29%20has%20introduced%20a%20novel%20approach%20to%0Abuilding%20a%20versatile%2C%20large-scale%20machine%20learning%20framework%20that%20operates%20in%20a%0Adecentralized%20and%20hardware-agnostic%20manner.%20However%2C%20FL%20faces%20network%20bandwidth%0Alimitations%20and%20data%20breaches.%20To%20reduce%20the%20central%20dependency%20in%20FL%20and%0Aincrease%20scalability%2C%20swarm%20learning%20%28SL%29%20has%20been%20proposed%20in%20collaboration%0Awith%20Hewlett%20Packard%20Enterprise%20%28HPE%29.%20SL%20represents%20a%20decentralized%20machine%0Alearning%20framework%20that%20leverages%20blockchain%20technology%20for%20secure%2C%20scalable%2C%0Aand%20private%20data%20management.%20A%20blockchain-based%20network%20enables%20the%20exchange%0Aand%20aggregation%20of%20model%20parameters%20among%20participants%2C%20thus%20mitigating%20the%0Arisk%20of%20a%20single%20point%20of%20failure%20and%20eliminating%20communication%20bottlenecks.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20survey%20is%20the%20first%20to%20introduce%20the%20principles%0Aof%20Swarm%20Learning%2C%20its%20architectural%20design%2C%20and%20its%20fields%20of%20application.%20In%0Aaddition%2C%20it%20highlights%20numerous%20research%20avenues%20that%20require%20further%0Aexploration%20by%20academic%20and%20industry%20communities%20to%20unlock%20the%20full%20potential%0Aand%20applications%20of%20SL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00556v1&entry.124074799=Read"},
{"title": "NeRF as a Non-Distant Environment Emitter in Physics-based Inverse\n  Rendering", "author": "Jingwang Ling and Ruihan Yu and Feng Xu and Chun Du and Shuang Zhao", "abstract": "  Physics-based inverse rendering enables joint optimization of shape,\nmaterial, and lighting based on captured 2D images. To ensure accurate\nreconstruction, using a light model that closely resembles the captured\nenvironment is essential. Although the widely adopted distant environmental\nlighting model is adequate in many cases, we demonstrate that its inability to\ncapture spatially varying illumination can lead to inaccurate reconstructions\nin many real-world inverse rendering scenarios. To address this limitation, we\nincorporate NeRF as a non-distant environment emitter into the inverse\nrendering pipeline. Additionally, we introduce an emitter importance sampling\ntechnique for NeRF to reduce the rendering variance. Through comparisons on\nboth real and synthetic datasets, our results demonstrate that our NeRF-based\nemitter offers a more precise representation of scene lighting, thereby\nimproving the accuracy of inverse rendering.\n", "link": "http://arxiv.org/abs/2402.04829v2", "date": "2024-05-01", "relevancy": 2.0746, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.53}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5109}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5104}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NeRF%20as%20a%20Non-Distant%20Environment%20Emitter%20in%20Physics-based%20Inverse%0A%20%20Rendering&body=Title%3A%20NeRF%20as%20a%20Non-Distant%20Environment%20Emitter%20in%20Physics-based%20Inverse%0A%20%20Rendering%0AAuthor%3A%20Jingwang%20Ling%20and%20Ruihan%20Yu%20and%20Feng%20Xu%20and%20Chun%20Du%20and%20Shuang%20Zhao%0AAbstract%3A%20%20%20Physics-based%20inverse%20rendering%20enables%20joint%20optimization%20of%20shape%2C%0Amaterial%2C%20and%20lighting%20based%20on%20captured%202D%20images.%20To%20ensure%20accurate%0Areconstruction%2C%20using%20a%20light%20model%20that%20closely%20resembles%20the%20captured%0Aenvironment%20is%20essential.%20Although%20the%20widely%20adopted%20distant%20environmental%0Alighting%20model%20is%20adequate%20in%20many%20cases%2C%20we%20demonstrate%20that%20its%20inability%20to%0Acapture%20spatially%20varying%20illumination%20can%20lead%20to%20inaccurate%20reconstructions%0Ain%20many%20real-world%20inverse%20rendering%20scenarios.%20To%20address%20this%20limitation%2C%20we%0Aincorporate%20NeRF%20as%20a%20non-distant%20environment%20emitter%20into%20the%20inverse%0Arendering%20pipeline.%20Additionally%2C%20we%20introduce%20an%20emitter%20importance%20sampling%0Atechnique%20for%20NeRF%20to%20reduce%20the%20rendering%20variance.%20Through%20comparisons%20on%0Aboth%20real%20and%20synthetic%20datasets%2C%20our%20results%20demonstrate%20that%20our%20NeRF-based%0Aemitter%20offers%20a%20more%20precise%20representation%20of%20scene%20lighting%2C%20thereby%0Aimproving%20the%20accuracy%20of%20inverse%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04829v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF%20as%20a%20Non-Distant%20Environment%20Emitter%20in%20Physics-based%20Inverse%0A%20%20Rendering&entry.906535625=Jingwang%20Ling%20and%20Ruihan%20Yu%20and%20Feng%20Xu%20and%20Chun%20Du%20and%20Shuang%20Zhao&entry.1292438233=%20%20Physics-based%20inverse%20rendering%20enables%20joint%20optimization%20of%20shape%2C%0Amaterial%2C%20and%20lighting%20based%20on%20captured%202D%20images.%20To%20ensure%20accurate%0Areconstruction%2C%20using%20a%20light%20model%20that%20closely%20resembles%20the%20captured%0Aenvironment%20is%20essential.%20Although%20the%20widely%20adopted%20distant%20environmental%0Alighting%20model%20is%20adequate%20in%20many%20cases%2C%20we%20demonstrate%20that%20its%20inability%20to%0Acapture%20spatially%20varying%20illumination%20can%20lead%20to%20inaccurate%20reconstructions%0Ain%20many%20real-world%20inverse%20rendering%20scenarios.%20To%20address%20this%20limitation%2C%20we%0Aincorporate%20NeRF%20as%20a%20non-distant%20environment%20emitter%20into%20the%20inverse%0Arendering%20pipeline.%20Additionally%2C%20we%20introduce%20an%20emitter%20importance%20sampling%0Atechnique%20for%20NeRF%20to%20reduce%20the%20rendering%20variance.%20Through%20comparisons%20on%0Aboth%20real%20and%20synthetic%20datasets%2C%20our%20results%20demonstrate%20that%20our%20NeRF-based%0Aemitter%20offers%20a%20more%20precise%20representation%20of%20scene%20lighting%2C%20thereby%0Aimproving%20the%20accuracy%20of%20inverse%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04829v2&entry.124074799=Read"},
{"title": "IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training", "author": "Che Liu and Sibo Cheng and Miaojing Shi and Anand Shah and Wenjia Bai and Rossella Arcucci", "abstract": "  In the field of medical Vision-Language Pre-training (VLP), significant\nefforts have been devoted to deriving text and image features from both\nclinical reports and associated medical images. However, most existing methods\nmay have overlooked the opportunity in leveraging the inherent hierarchical\nstructure of clinical reports, which are generally split into `findings' for\ndescriptive content and `impressions' for conclusive observation. Instead of\nutilizing this rich, structured format, current medical VLP approaches often\nsimplify the report into either a unified entity or fragmented tokens. In this\nwork, we propose a novel clinical prior guided VLP framework named IMITATE to\nlearn the structure information from medical reports with hierarchical\nvision-language alignment. The framework derives multi-level visual features\nfrom the chest X-ray (CXR) images and separately aligns these features with the\ndescriptive and the conclusive text encoded in the hierarchical medical report.\nFurthermore, a new clinical-informed contrastive loss is introduced for\ncross-modal learning, which accounts for clinical prior knowledge in\nformulating sample correlations in contrastive learning. The proposed model,\nIMITATE, outperforms baseline VLP methods across six different datasets,\nspanning five medical imaging downstream tasks. Comprehensive experimental\nresults highlight the advantages of integrating the hierarchical structure of\nmedical reports for vision-language alignment.\n", "link": "http://arxiv.org/abs/2310.07355v3", "date": "2024-05-01", "relevancy": 2.0738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IMITATE%3A%20Clinical%20Prior%20Guided%20Hierarchical%20Vision-Language%20Pre-training&body=Title%3A%20IMITATE%3A%20Clinical%20Prior%20Guided%20Hierarchical%20Vision-Language%20Pre-training%0AAuthor%3A%20Che%20Liu%20and%20Sibo%20Cheng%20and%20Miaojing%20Shi%20and%20Anand%20Shah%20and%20Wenjia%20Bai%20and%20Rossella%20Arcucci%0AAbstract%3A%20%20%20In%20the%20field%20of%20medical%20Vision-Language%20Pre-training%20%28VLP%29%2C%20significant%0Aefforts%20have%20been%20devoted%20to%20deriving%20text%20and%20image%20features%20from%20both%0Aclinical%20reports%20and%20associated%20medical%20images.%20However%2C%20most%20existing%20methods%0Amay%20have%20overlooked%20the%20opportunity%20in%20leveraging%20the%20inherent%20hierarchical%0Astructure%20of%20clinical%20reports%2C%20which%20are%20generally%20split%20into%20%60findings%27%20for%0Adescriptive%20content%20and%20%60impressions%27%20for%20conclusive%20observation.%20Instead%20of%0Autilizing%20this%20rich%2C%20structured%20format%2C%20current%20medical%20VLP%20approaches%20often%0Asimplify%20the%20report%20into%20either%20a%20unified%20entity%20or%20fragmented%20tokens.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20clinical%20prior%20guided%20VLP%20framework%20named%20IMITATE%20to%0Alearn%20the%20structure%20information%20from%20medical%20reports%20with%20hierarchical%0Avision-language%20alignment.%20The%20framework%20derives%20multi-level%20visual%20features%0Afrom%20the%20chest%20X-ray%20%28CXR%29%20images%20and%20separately%20aligns%20these%20features%20with%20the%0Adescriptive%20and%20the%20conclusive%20text%20encoded%20in%20the%20hierarchical%20medical%20report.%0AFurthermore%2C%20a%20new%20clinical-informed%20contrastive%20loss%20is%20introduced%20for%0Across-modal%20learning%2C%20which%20accounts%20for%20clinical%20prior%20knowledge%20in%0Aformulating%20sample%20correlations%20in%20contrastive%20learning.%20The%20proposed%20model%2C%0AIMITATE%2C%20outperforms%20baseline%20VLP%20methods%20across%20six%20different%20datasets%2C%0Aspanning%20five%20medical%20imaging%20downstream%20tasks.%20Comprehensive%20experimental%0Aresults%20highlight%20the%20advantages%20of%20integrating%20the%20hierarchical%20structure%20of%0Amedical%20reports%20for%20vision-language%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07355v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMITATE%3A%20Clinical%20Prior%20Guided%20Hierarchical%20Vision-Language%20Pre-training&entry.906535625=Che%20Liu%20and%20Sibo%20Cheng%20and%20Miaojing%20Shi%20and%20Anand%20Shah%20and%20Wenjia%20Bai%20and%20Rossella%20Arcucci&entry.1292438233=%20%20In%20the%20field%20of%20medical%20Vision-Language%20Pre-training%20%28VLP%29%2C%20significant%0Aefforts%20have%20been%20devoted%20to%20deriving%20text%20and%20image%20features%20from%20both%0Aclinical%20reports%20and%20associated%20medical%20images.%20However%2C%20most%20existing%20methods%0Amay%20have%20overlooked%20the%20opportunity%20in%20leveraging%20the%20inherent%20hierarchical%0Astructure%20of%20clinical%20reports%2C%20which%20are%20generally%20split%20into%20%60findings%27%20for%0Adescriptive%20content%20and%20%60impressions%27%20for%20conclusive%20observation.%20Instead%20of%0Autilizing%20this%20rich%2C%20structured%20format%2C%20current%20medical%20VLP%20approaches%20often%0Asimplify%20the%20report%20into%20either%20a%20unified%20entity%20or%20fragmented%20tokens.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20clinical%20prior%20guided%20VLP%20framework%20named%20IMITATE%20to%0Alearn%20the%20structure%20information%20from%20medical%20reports%20with%20hierarchical%0Avision-language%20alignment.%20The%20framework%20derives%20multi-level%20visual%20features%0Afrom%20the%20chest%20X-ray%20%28CXR%29%20images%20and%20separately%20aligns%20these%20features%20with%20the%0Adescriptive%20and%20the%20conclusive%20text%20encoded%20in%20the%20hierarchical%20medical%20report.%0AFurthermore%2C%20a%20new%20clinical-informed%20contrastive%20loss%20is%20introduced%20for%0Across-modal%20learning%2C%20which%20accounts%20for%20clinical%20prior%20knowledge%20in%0Aformulating%20sample%20correlations%20in%20contrastive%20learning.%20The%20proposed%20model%2C%0AIMITATE%2C%20outperforms%20baseline%20VLP%20methods%20across%20six%20different%20datasets%2C%0Aspanning%20five%20medical%20imaging%20downstream%20tasks.%20Comprehensive%20experimental%0Aresults%20highlight%20the%20advantages%20of%20integrating%20the%20hierarchical%20structure%20of%0Amedical%20reports%20for%20vision-language%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07355v3&entry.124074799=Read"},
{"title": "HUGO -- Highlighting Unseen Grid Options: Combining Deep Reinforcement\n  Learning with a Heuristic Target Topology Approach", "author": "Malte Lehna and Clara Holzh\u00fcter and Sven Tomforde and Christoph Scholz", "abstract": "  With the growth of Renewable Energy (RE) generation, the operation of power\ngrids has become increasingly complex. One solution is automated grid\noperation, where Deep Reinforcement Learning (DRL) has repeatedly shown\nsignificant potential in Learning to Run a Power Network (L2RPN) challenges.\nHowever, only individual actions at the substation level have been subjected to\ntopology optimization by most existing DRL algorithms. In contrast, we propose\na more holistic approach in this paper by proposing specific Target Topologies\n(TTs) as actions. These topologies are selected based on their robustness. As\npart of this paper, we present a search algorithm to find the TTs and upgrade\nour previously developed DRL agent CurriculumAgent (CAgent) to a novel topology\nagent. We compare the upgrade to the previous CAgent agent and can increase\ntheir scores significantly by 10%. Further, we achieve a 25% better median\nsurvival with our TTs included. Later analysis shows that almost all TTs are\nclose to the base topology, explaining their robustness.\n", "link": "http://arxiv.org/abs/2405.00629v1", "date": "2024-05-01", "relevancy": 2.0571, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.534}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5123}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5083}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HUGO%20--%20Highlighting%20Unseen%20Grid%20Options%3A%20Combining%20Deep%20Reinforcement%0A%20%20Learning%20with%20a%20Heuristic%20Target%20Topology%20Approach&body=Title%3A%20HUGO%20--%20Highlighting%20Unseen%20Grid%20Options%3A%20Combining%20Deep%20Reinforcement%0A%20%20Learning%20with%20a%20Heuristic%20Target%20Topology%20Approach%0AAuthor%3A%20Malte%20Lehna%20and%20Clara%20Holzh%C3%BCter%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20With%20the%20growth%20of%20Renewable%20Energy%20%28RE%29%20generation%2C%20the%20operation%20of%20power%0Agrids%20has%20become%20increasingly%20complex.%20One%20solution%20is%20automated%20grid%0Aoperation%2C%20where%20Deep%20Reinforcement%20Learning%20%28DRL%29%20has%20repeatedly%20shown%0Asignificant%20potential%20in%20Learning%20to%20Run%20a%20Power%20Network%20%28L2RPN%29%20challenges.%0AHowever%2C%20only%20individual%20actions%20at%20the%20substation%20level%20have%20been%20subjected%20to%0Atopology%20optimization%20by%20most%20existing%20DRL%20algorithms.%20In%20contrast%2C%20we%20propose%0Aa%20more%20holistic%20approach%20in%20this%20paper%20by%20proposing%20specific%20Target%20Topologies%0A%28TTs%29%20as%20actions.%20These%20topologies%20are%20selected%20based%20on%20their%20robustness.%20As%0Apart%20of%20this%20paper%2C%20we%20present%20a%20search%20algorithm%20to%20find%20the%20TTs%20and%20upgrade%0Aour%20previously%20developed%20DRL%20agent%20CurriculumAgent%20%28CAgent%29%20to%20a%20novel%20topology%0Aagent.%20We%20compare%20the%20upgrade%20to%20the%20previous%20CAgent%20agent%20and%20can%20increase%0Atheir%20scores%20significantly%20by%2010%25.%20Further%2C%20we%20achieve%20a%2025%25%20better%20median%0Asurvival%20with%20our%20TTs%20included.%20Later%20analysis%20shows%20that%20almost%20all%20TTs%20are%0Aclose%20to%20the%20base%20topology%2C%20explaining%20their%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00629v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HUGO%20--%20Highlighting%20Unseen%20Grid%20Options%3A%20Combining%20Deep%20Reinforcement%0A%20%20Learning%20with%20a%20Heuristic%20Target%20Topology%20Approach&entry.906535625=Malte%20Lehna%20and%20Clara%20Holzh%C3%BCter%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz&entry.1292438233=%20%20With%20the%20growth%20of%20Renewable%20Energy%20%28RE%29%20generation%2C%20the%20operation%20of%20power%0Agrids%20has%20become%20increasingly%20complex.%20One%20solution%20is%20automated%20grid%0Aoperation%2C%20where%20Deep%20Reinforcement%20Learning%20%28DRL%29%20has%20repeatedly%20shown%0Asignificant%20potential%20in%20Learning%20to%20Run%20a%20Power%20Network%20%28L2RPN%29%20challenges.%0AHowever%2C%20only%20individual%20actions%20at%20the%20substation%20level%20have%20been%20subjected%20to%0Atopology%20optimization%20by%20most%20existing%20DRL%20algorithms.%20In%20contrast%2C%20we%20propose%0Aa%20more%20holistic%20approach%20in%20this%20paper%20by%20proposing%20specific%20Target%20Topologies%0A%28TTs%29%20as%20actions.%20These%20topologies%20are%20selected%20based%20on%20their%20robustness.%20As%0Apart%20of%20this%20paper%2C%20we%20present%20a%20search%20algorithm%20to%20find%20the%20TTs%20and%20upgrade%0Aour%20previously%20developed%20DRL%20agent%20CurriculumAgent%20%28CAgent%29%20to%20a%20novel%20topology%0Aagent.%20We%20compare%20the%20upgrade%20to%20the%20previous%20CAgent%20agent%20and%20can%20increase%0Atheir%20scores%20significantly%20by%2010%25.%20Further%2C%20we%20achieve%20a%2025%25%20better%20median%0Asurvival%20with%20our%20TTs%20included.%20Later%20analysis%20shows%20that%20almost%20all%20TTs%20are%0Aclose%20to%20the%20base%20topology%2C%20explaining%20their%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00629v1&entry.124074799=Read"},
{"title": "Koopman-based Deep Learning for Nonlinear System Estimation", "author": "Zexin Sun and Mingyu Chen and John Baillieul", "abstract": "  Nonlinear differential equations are encountered as models of fluid flow,\nspiking neurons, and many other systems of interest in the real world. Common\nfeatures of these systems are that their behaviors are difficult to describe\nexactly and invariably unmodeled dynamics present challenges in making precise\npredictions. In many cases the models exhibit extremely complicated behavior\ndue to bifurcations and chaotic regimes. In this paper, we present a novel\ndata-driven linear estimator that uses Koopman operator theory to extract\nfinite-dimensional representations of complex nonlinear systems. The extracted\nmodel is used together with a deep reinforcement learning network that learns\nthe optimal stepwise actions to predict future states of the original nonlinear\nsystem. Our estimator is also adaptive to a diffeomorphic transformation of the\nnonlinear system which enables transfer learning to compute state estimates of\nthe transformed system without relearning from scratch.\n", "link": "http://arxiv.org/abs/2405.00627v1", "date": "2024-05-01", "relevancy": 2.0464, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5651}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5324}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4695}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Koopman-based%20Deep%20Learning%20for%20Nonlinear%20System%20Estimation&body=Title%3A%20Koopman-based%20Deep%20Learning%20for%20Nonlinear%20System%20Estimation%0AAuthor%3A%20Zexin%20Sun%20and%20Mingyu%20Chen%20and%20John%20Baillieul%0AAbstract%3A%20%20%20Nonlinear%20differential%20equations%20are%20encountered%20as%20models%20of%20fluid%20flow%2C%0Aspiking%20neurons%2C%20and%20many%20other%20systems%20of%20interest%20in%20the%20real%20world.%20Common%0Afeatures%20of%20these%20systems%20are%20that%20their%20behaviors%20are%20difficult%20to%20describe%0Aexactly%20and%20invariably%20unmodeled%20dynamics%20present%20challenges%20in%20making%20precise%0Apredictions.%20In%20many%20cases%20the%20models%20exhibit%20extremely%20complicated%20behavior%0Adue%20to%20bifurcations%20and%20chaotic%20regimes.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Adata-driven%20linear%20estimator%20that%20uses%20Koopman%20operator%20theory%20to%20extract%0Afinite-dimensional%20representations%20of%20complex%20nonlinear%20systems.%20The%20extracted%0Amodel%20is%20used%20together%20with%20a%20deep%20reinforcement%20learning%20network%20that%20learns%0Athe%20optimal%20stepwise%20actions%20to%20predict%20future%20states%20of%20the%20original%20nonlinear%0Asystem.%20Our%20estimator%20is%20also%20adaptive%20to%20a%20diffeomorphic%20transformation%20of%20the%0Anonlinear%20system%20which%20enables%20transfer%20learning%20to%20compute%20state%20estimates%20of%0Athe%20transformed%20system%20without%20relearning%20from%20scratch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00627v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Koopman-based%20Deep%20Learning%20for%20Nonlinear%20System%20Estimation&entry.906535625=Zexin%20Sun%20and%20Mingyu%20Chen%20and%20John%20Baillieul&entry.1292438233=%20%20Nonlinear%20differential%20equations%20are%20encountered%20as%20models%20of%20fluid%20flow%2C%0Aspiking%20neurons%2C%20and%20many%20other%20systems%20of%20interest%20in%20the%20real%20world.%20Common%0Afeatures%20of%20these%20systems%20are%20that%20their%20behaviors%20are%20difficult%20to%20describe%0Aexactly%20and%20invariably%20unmodeled%20dynamics%20present%20challenges%20in%20making%20precise%0Apredictions.%20In%20many%20cases%20the%20models%20exhibit%20extremely%20complicated%20behavior%0Adue%20to%20bifurcations%20and%20chaotic%20regimes.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Adata-driven%20linear%20estimator%20that%20uses%20Koopman%20operator%20theory%20to%20extract%0Afinite-dimensional%20representations%20of%20complex%20nonlinear%20systems.%20The%20extracted%0Amodel%20is%20used%20together%20with%20a%20deep%20reinforcement%20learning%20network%20that%20learns%0Athe%20optimal%20stepwise%20actions%20to%20predict%20future%20states%20of%20the%20original%20nonlinear%0Asystem.%20Our%20estimator%20is%20also%20adaptive%20to%20a%20diffeomorphic%20transformation%20of%20the%0Anonlinear%20system%20which%20enables%20transfer%20learning%20to%20compute%20state%20estimates%20of%0Athe%20transformed%20system%20without%20relearning%20from%20scratch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00627v1&entry.124074799=Read"},
{"title": "The Pyramid of Captions", "author": "Delong Chen and Samuel Cahyawijaya and Etsuko Ishii and Ho Shu Chan and Yejin Bang and Pascale Fung", "abstract": "  We introduce a formal information-theoretic framework for image captioning by\nregarding it as a representation learning task. Our framework defines three key\nobjectives: task sufficiency, minimal redundancy, and human interpretability.\nBuilding upon this foundation, we propose a novel Pyramid of Captions (PoCa)\nmethod, which constructs caption pyramids by generating localized captions for\nzoomed-in image patches and integrating them with global caption information\nusing large language models. This approach leverages intuition that the\ndetailed examination of local patches can reduce error risks and address\ninaccuracies in global captions, either by correcting the hallucination or\nadding missing details. Based on our theoretical framework, we formalize this\nintuition and provide formal proof demonstrating the effectiveness of PoCa\nunder certain assumptions. Empirical tests with various image captioning models\nand large language models show that PoCa consistently yields more informative\nand semantically aligned captions, maintaining brevity and interpretability.\n", "link": "http://arxiv.org/abs/2405.00485v1", "date": "2024-05-01", "relevancy": 2.043, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4925}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4744}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Pyramid%20of%20Captions&body=Title%3A%20The%20Pyramid%20of%20Captions%0AAuthor%3A%20Delong%20Chen%20and%20Samuel%20Cahyawijaya%20and%20Etsuko%20Ishii%20and%20Ho%20Shu%20Chan%20and%20Yejin%20Bang%20and%20Pascale%20Fung%0AAbstract%3A%20%20%20We%20introduce%20a%20formal%20information-theoretic%20framework%20for%20image%20captioning%20by%0Aregarding%20it%20as%20a%20representation%20learning%20task.%20Our%20framework%20defines%20three%20key%0Aobjectives%3A%20task%20sufficiency%2C%20minimal%20redundancy%2C%20and%20human%20interpretability.%0ABuilding%20upon%20this%20foundation%2C%20we%20propose%20a%20novel%20Pyramid%20of%20Captions%20%28PoCa%29%0Amethod%2C%20which%20constructs%20caption%20pyramids%20by%20generating%20localized%20captions%20for%0Azoomed-in%20image%20patches%20and%20integrating%20them%20with%20global%20caption%20information%0Ausing%20large%20language%20models.%20This%20approach%20leverages%20intuition%20that%20the%0Adetailed%20examination%20of%20local%20patches%20can%20reduce%20error%20risks%20and%20address%0Ainaccuracies%20in%20global%20captions%2C%20either%20by%20correcting%20the%20hallucination%20or%0Aadding%20missing%20details.%20Based%20on%20our%20theoretical%20framework%2C%20we%20formalize%20this%0Aintuition%20and%20provide%20formal%20proof%20demonstrating%20the%20effectiveness%20of%20PoCa%0Aunder%20certain%20assumptions.%20Empirical%20tests%20with%20various%20image%20captioning%20models%0Aand%20large%20language%20models%20show%20that%20PoCa%20consistently%20yields%20more%20informative%0Aand%20semantically%20aligned%20captions%2C%20maintaining%20brevity%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00485v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Pyramid%20of%20Captions&entry.906535625=Delong%20Chen%20and%20Samuel%20Cahyawijaya%20and%20Etsuko%20Ishii%20and%20Ho%20Shu%20Chan%20and%20Yejin%20Bang%20and%20Pascale%20Fung&entry.1292438233=%20%20We%20introduce%20a%20formal%20information-theoretic%20framework%20for%20image%20captioning%20by%0Aregarding%20it%20as%20a%20representation%20learning%20task.%20Our%20framework%20defines%20three%20key%0Aobjectives%3A%20task%20sufficiency%2C%20minimal%20redundancy%2C%20and%20human%20interpretability.%0ABuilding%20upon%20this%20foundation%2C%20we%20propose%20a%20novel%20Pyramid%20of%20Captions%20%28PoCa%29%0Amethod%2C%20which%20constructs%20caption%20pyramids%20by%20generating%20localized%20captions%20for%0Azoomed-in%20image%20patches%20and%20integrating%20them%20with%20global%20caption%20information%0Ausing%20large%20language%20models.%20This%20approach%20leverages%20intuition%20that%20the%0Adetailed%20examination%20of%20local%20patches%20can%20reduce%20error%20risks%20and%20address%0Ainaccuracies%20in%20global%20captions%2C%20either%20by%20correcting%20the%20hallucination%20or%0Aadding%20missing%20details.%20Based%20on%20our%20theoretical%20framework%2C%20we%20formalize%20this%0Aintuition%20and%20provide%20formal%20proof%20demonstrating%20the%20effectiveness%20of%20PoCa%0Aunder%20certain%20assumptions.%20Empirical%20tests%20with%20various%20image%20captioning%20models%0Aand%20large%20language%20models%20show%20that%20PoCa%20consistently%20yields%20more%20informative%0Aand%20semantically%20aligned%20captions%2C%20maintaining%20brevity%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00485v1&entry.124074799=Read"},
{"title": "Unbiased Learning to Rank Meets Reality: Lessons from Baidu's\n  Large-Scale Search Dataset", "author": "Philipp Hager and Romain Deffayet and Jean-Michel Renders and Onno Zoeter and Maarten de Rijke", "abstract": "  Unbiased learning-to-rank (ULTR) is a well-established framework for learning\nfrom user clicks, which are often biased by the ranker collecting the data.\nWhile theoretically justified and extensively tested in simulation, ULTR\ntechniques lack empirical validation, especially on modern search engines. The\nBaidu-ULTR dataset released for the WSDM Cup 2023, collected from Baidu's\nsearch engine, offers a rare opportunity to assess the real-world performance\nof prominent ULTR techniques. Despite multiple submissions during the WSDM Cup\n2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the\nobserved improvements stem from applying ULTR or other learning techniques.\n  In this work, we revisit and extend the available experiments on the\nBaidu-ULTR dataset. We find that standard unbiased learning-to-rank techniques\nrobustly improve click predictions but struggle to consistently improve ranking\nperformance, especially considering the stark differences obtained by choice of\nranking loss and query-document features. Our experiments reveal that gains in\nclick prediction do not necessarily translate to enhanced ranking performance\non expert relevance annotations, implying that conclusions strongly depend on\nhow success is measured in this benchmark.\n", "link": "http://arxiv.org/abs/2404.02543v2", "date": "2024-05-01", "relevancy": 2.0425, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5036}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4984}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unbiased%20Learning%20to%20Rank%20Meets%20Reality%3A%20Lessons%20from%20Baidu%27s%0A%20%20Large-Scale%20Search%20Dataset&body=Title%3A%20Unbiased%20Learning%20to%20Rank%20Meets%20Reality%3A%20Lessons%20from%20Baidu%27s%0A%20%20Large-Scale%20Search%20Dataset%0AAuthor%3A%20Philipp%20Hager%20and%20Romain%20Deffayet%20and%20Jean-Michel%20Renders%20and%20Onno%20Zoeter%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20%20%20Unbiased%20learning-to-rank%20%28ULTR%29%20is%20a%20well-established%20framework%20for%20learning%0Afrom%20user%20clicks%2C%20which%20are%20often%20biased%20by%20the%20ranker%20collecting%20the%20data.%0AWhile%20theoretically%20justified%20and%20extensively%20tested%20in%20simulation%2C%20ULTR%0Atechniques%20lack%20empirical%20validation%2C%20especially%20on%20modern%20search%20engines.%20The%0ABaidu-ULTR%20dataset%20released%20for%20the%20WSDM%20Cup%202023%2C%20collected%20from%20Baidu%27s%0Asearch%20engine%2C%20offers%20a%20rare%20opportunity%20to%20assess%20the%20real-world%20performance%0Aof%20prominent%20ULTR%20techniques.%20Despite%20multiple%20submissions%20during%20the%20WSDM%20Cup%0A2023%20and%20the%20subsequent%20NTCIR%20ULTRE-2%20task%2C%20it%20remains%20unclear%20whether%20the%0Aobserved%20improvements%20stem%20from%20applying%20ULTR%20or%20other%20learning%20techniques.%0A%20%20In%20this%20work%2C%20we%20revisit%20and%20extend%20the%20available%20experiments%20on%20the%0ABaidu-ULTR%20dataset.%20We%20find%20that%20standard%20unbiased%20learning-to-rank%20techniques%0Arobustly%20improve%20click%20predictions%20but%20struggle%20to%20consistently%20improve%20ranking%0Aperformance%2C%20especially%20considering%20the%20stark%20differences%20obtained%20by%20choice%20of%0Aranking%20loss%20and%20query-document%20features.%20Our%20experiments%20reveal%20that%20gains%20in%0Aclick%20prediction%20do%20not%20necessarily%20translate%20to%20enhanced%20ranking%20performance%0Aon%20expert%20relevance%20annotations%2C%20implying%20that%20conclusions%20strongly%20depend%20on%0Ahow%20success%20is%20measured%20in%20this%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02543v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unbiased%20Learning%20to%20Rank%20Meets%20Reality%3A%20Lessons%20from%20Baidu%27s%0A%20%20Large-Scale%20Search%20Dataset&entry.906535625=Philipp%20Hager%20and%20Romain%20Deffayet%20and%20Jean-Michel%20Renders%20and%20Onno%20Zoeter%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20Unbiased%20learning-to-rank%20%28ULTR%29%20is%20a%20well-established%20framework%20for%20learning%0Afrom%20user%20clicks%2C%20which%20are%20often%20biased%20by%20the%20ranker%20collecting%20the%20data.%0AWhile%20theoretically%20justified%20and%20extensively%20tested%20in%20simulation%2C%20ULTR%0Atechniques%20lack%20empirical%20validation%2C%20especially%20on%20modern%20search%20engines.%20The%0ABaidu-ULTR%20dataset%20released%20for%20the%20WSDM%20Cup%202023%2C%20collected%20from%20Baidu%27s%0Asearch%20engine%2C%20offers%20a%20rare%20opportunity%20to%20assess%20the%20real-world%20performance%0Aof%20prominent%20ULTR%20techniques.%20Despite%20multiple%20submissions%20during%20the%20WSDM%20Cup%0A2023%20and%20the%20subsequent%20NTCIR%20ULTRE-2%20task%2C%20it%20remains%20unclear%20whether%20the%0Aobserved%20improvements%20stem%20from%20applying%20ULTR%20or%20other%20learning%20techniques.%0A%20%20In%20this%20work%2C%20we%20revisit%20and%20extend%20the%20available%20experiments%20on%20the%0ABaidu-ULTR%20dataset.%20We%20find%20that%20standard%20unbiased%20learning-to-rank%20techniques%0Arobustly%20improve%20click%20predictions%20but%20struggle%20to%20consistently%20improve%20ranking%0Aperformance%2C%20especially%20considering%20the%20stark%20differences%20obtained%20by%20choice%20of%0Aranking%20loss%20and%20query-document%20features.%20Our%20experiments%20reveal%20that%20gains%20in%0Aclick%20prediction%20do%20not%20necessarily%20translate%20to%20enhanced%20ranking%20performance%0Aon%20expert%20relevance%20annotations%2C%20implying%20that%20conclusions%20strongly%20depend%20on%0Ahow%20success%20is%20measured%20in%20this%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02543v2&entry.124074799=Read"},
{"title": "Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for\n  motion modelling in radiotherapy: beyond classic voxel-based methods", "author": "Xia Li and Muheng Li and Antony Lomax and Joachim Buhmann and Ye Zhang", "abstract": "  Background and purpose: Deformable image registration (DIR) is a crucial tool\nin radiotherapy for extracting and modelling organ motion. However, when\nsignificant changes and sliding boundaries are present, it faces compromised\naccuracy and uncertainty, determining the subsequential contour propagation and\ndose accumulation procedures. Materials and methods: We propose an implicit\nneural representation (INR)-based approach modelling motion continuously in\nboth space and time, named Continues-sPatial-Temporal DIR (CPT-DIR). This\nmethod uses a multilayer perception (MLP) network to map 3D coordinate (x,y,z)\nto its corresponding velocity vector (vx,vy,vz). The displacement vectors\n(dx,dy,dz) are then calculated by integrating velocity vectors over time. The\nMLP's parameters can rapidly adapt to new cases without pre-training, enhancing\noptimisation. The DIR's performance was tested on the DIR-Lab dataset of 10\nlung 4DCT cases, using metrics of landmark accuracy (TRE), contour conformity\n(Dice) and image similarity (MAE). Results: The proposed CPT-DIR can reduce\nlandmark TRE from 2.79mm to 0.99mm, outperforming B-splines' results for all\ncases. The MAE of the whole-body region improves from 35.46HU to 28.99HU.\nFurthermore, CPT-DIR surpasses B-splines for accuracy in the sliding boundary\nregion, lowering MAE and increasing Dice coefficients for the ribcage from\n65.65HU and 90.41% to 42.04HU and 90.56%, versus 75.40HU and 89.30% without\nregistration. Meanwhile, CPT-DIR offers significant speed advantages,\ncompleting in under 15 seconds compared to a few minutes with the conventional\nB-splines method. Conclusion: Leveraging the continuous representations, the\nCPT-DIR method significantly enhances registration accuracy, automation and\nspeed, outperforming traditional B-splines in landmark and contour precision,\nparticularly in the challenging areas.\n", "link": "http://arxiv.org/abs/2405.00430v1", "date": "2024-05-01", "relevancy": 1.9952, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4881}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4836}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continuous%20sPatial-Temporal%20Deformable%20Image%20Registration%20%28CPT-DIR%29%20for%0A%20%20motion%20modelling%20in%20radiotherapy%3A%20beyond%20classic%20voxel-based%20methods&body=Title%3A%20Continuous%20sPatial-Temporal%20Deformable%20Image%20Registration%20%28CPT-DIR%29%20for%0A%20%20motion%20modelling%20in%20radiotherapy%3A%20beyond%20classic%20voxel-based%20methods%0AAuthor%3A%20Xia%20Li%20and%20Muheng%20Li%20and%20Antony%20Lomax%20and%20Joachim%20Buhmann%20and%20Ye%20Zhang%0AAbstract%3A%20%20%20Background%20and%20purpose%3A%20Deformable%20image%20registration%20%28DIR%29%20is%20a%20crucial%20tool%0Ain%20radiotherapy%20for%20extracting%20and%20modelling%20organ%20motion.%20However%2C%20when%0Asignificant%20changes%20and%20sliding%20boundaries%20are%20present%2C%20it%20faces%20compromised%0Aaccuracy%20and%20uncertainty%2C%20determining%20the%20subsequential%20contour%20propagation%20and%0Adose%20accumulation%20procedures.%20Materials%20and%20methods%3A%20We%20propose%20an%20implicit%0Aneural%20representation%20%28INR%29-based%20approach%20modelling%20motion%20continuously%20in%0Aboth%20space%20and%20time%2C%20named%20Continues-sPatial-Temporal%20DIR%20%28CPT-DIR%29.%20This%0Amethod%20uses%20a%20multilayer%20perception%20%28MLP%29%20network%20to%20map%203D%20coordinate%20%28x%2Cy%2Cz%29%0Ato%20its%20corresponding%20velocity%20vector%20%28vx%2Cvy%2Cvz%29.%20The%20displacement%20vectors%0A%28dx%2Cdy%2Cdz%29%20are%20then%20calculated%20by%20integrating%20velocity%20vectors%20over%20time.%20The%0AMLP%27s%20parameters%20can%20rapidly%20adapt%20to%20new%20cases%20without%20pre-training%2C%20enhancing%0Aoptimisation.%20The%20DIR%27s%20performance%20was%20tested%20on%20the%20DIR-Lab%20dataset%20of%2010%0Alung%204DCT%20cases%2C%20using%20metrics%20of%20landmark%20accuracy%20%28TRE%29%2C%20contour%20conformity%0A%28Dice%29%20and%20image%20similarity%20%28MAE%29.%20Results%3A%20The%20proposed%20CPT-DIR%20can%20reduce%0Alandmark%20TRE%20from%202.79mm%20to%200.99mm%2C%20outperforming%20B-splines%27%20results%20for%20all%0Acases.%20The%20MAE%20of%20the%20whole-body%20region%20improves%20from%2035.46HU%20to%2028.99HU.%0AFurthermore%2C%20CPT-DIR%20surpasses%20B-splines%20for%20accuracy%20in%20the%20sliding%20boundary%0Aregion%2C%20lowering%20MAE%20and%20increasing%20Dice%20coefficients%20for%20the%20ribcage%20from%0A65.65HU%20and%2090.41%25%20to%2042.04HU%20and%2090.56%25%2C%20versus%2075.40HU%20and%2089.30%25%20without%0Aregistration.%20Meanwhile%2C%20CPT-DIR%20offers%20significant%20speed%20advantages%2C%0Acompleting%20in%20under%2015%20seconds%20compared%20to%20a%20few%20minutes%20with%20the%20conventional%0AB-splines%20method.%20Conclusion%3A%20Leveraging%20the%20continuous%20representations%2C%20the%0ACPT-DIR%20method%20significantly%20enhances%20registration%20accuracy%2C%20automation%20and%0Aspeed%2C%20outperforming%20traditional%20B-splines%20in%20landmark%20and%20contour%20precision%2C%0Aparticularly%20in%20the%20challenging%20areas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00430v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20sPatial-Temporal%20Deformable%20Image%20Registration%20%28CPT-DIR%29%20for%0A%20%20motion%20modelling%20in%20radiotherapy%3A%20beyond%20classic%20voxel-based%20methods&entry.906535625=Xia%20Li%20and%20Muheng%20Li%20and%20Antony%20Lomax%20and%20Joachim%20Buhmann%20and%20Ye%20Zhang&entry.1292438233=%20%20Background%20and%20purpose%3A%20Deformable%20image%20registration%20%28DIR%29%20is%20a%20crucial%20tool%0Ain%20radiotherapy%20for%20extracting%20and%20modelling%20organ%20motion.%20However%2C%20when%0Asignificant%20changes%20and%20sliding%20boundaries%20are%20present%2C%20it%20faces%20compromised%0Aaccuracy%20and%20uncertainty%2C%20determining%20the%20subsequential%20contour%20propagation%20and%0Adose%20accumulation%20procedures.%20Materials%20and%20methods%3A%20We%20propose%20an%20implicit%0Aneural%20representation%20%28INR%29-based%20approach%20modelling%20motion%20continuously%20in%0Aboth%20space%20and%20time%2C%20named%20Continues-sPatial-Temporal%20DIR%20%28CPT-DIR%29.%20This%0Amethod%20uses%20a%20multilayer%20perception%20%28MLP%29%20network%20to%20map%203D%20coordinate%20%28x%2Cy%2Cz%29%0Ato%20its%20corresponding%20velocity%20vector%20%28vx%2Cvy%2Cvz%29.%20The%20displacement%20vectors%0A%28dx%2Cdy%2Cdz%29%20are%20then%20calculated%20by%20integrating%20velocity%20vectors%20over%20time.%20The%0AMLP%27s%20parameters%20can%20rapidly%20adapt%20to%20new%20cases%20without%20pre-training%2C%20enhancing%0Aoptimisation.%20The%20DIR%27s%20performance%20was%20tested%20on%20the%20DIR-Lab%20dataset%20of%2010%0Alung%204DCT%20cases%2C%20using%20metrics%20of%20landmark%20accuracy%20%28TRE%29%2C%20contour%20conformity%0A%28Dice%29%20and%20image%20similarity%20%28MAE%29.%20Results%3A%20The%20proposed%20CPT-DIR%20can%20reduce%0Alandmark%20TRE%20from%202.79mm%20to%200.99mm%2C%20outperforming%20B-splines%27%20results%20for%20all%0Acases.%20The%20MAE%20of%20the%20whole-body%20region%20improves%20from%2035.46HU%20to%2028.99HU.%0AFurthermore%2C%20CPT-DIR%20surpasses%20B-splines%20for%20accuracy%20in%20the%20sliding%20boundary%0Aregion%2C%20lowering%20MAE%20and%20increasing%20Dice%20coefficients%20for%20the%20ribcage%20from%0A65.65HU%20and%2090.41%25%20to%2042.04HU%20and%2090.56%25%2C%20versus%2075.40HU%20and%2089.30%25%20without%0Aregistration.%20Meanwhile%2C%20CPT-DIR%20offers%20significant%20speed%20advantages%2C%0Acompleting%20in%20under%2015%20seconds%20compared%20to%20a%20few%20minutes%20with%20the%20conventional%0AB-splines%20method.%20Conclusion%3A%20Leveraging%20the%20continuous%20representations%2C%20the%0ACPT-DIR%20method%20significantly%20enhances%20registration%20accuracy%2C%20automation%20and%0Aspeed%2C%20outperforming%20traditional%20B-splines%20in%20landmark%20and%20contour%20precision%2C%0Aparticularly%20in%20the%20challenging%20areas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00430v1&entry.124074799=Read"},
{"title": "Machine Learning for Synthetic Data Generation: A Review", "author": "Yingzhou Lu and Minjie Shen and Huazheng Wang and Xiao Wang and Capucine van Rechem and Tianfan Fu and Wenqi Wei", "abstract": "  Machine learning heavily relies on data, but real-world applications often\nencounter various data-related issues. These include data of poor quality,\ninsufficient data points leading to under-fitting of machine learning models,\nand difficulties in data access due to concerns surrounding privacy, safety,\nand regulations. In light of these challenges, the concept of synthetic data\ngeneration emerges as a promising alternative that allows for data sharing and\nutilization in ways that real-world data cannot facilitate. This paper presents\na comprehensive systematic review of existing studies that employ machine\nlearning models for the purpose of generating synthetic data. The review\nencompasses various perspectives, starting with the applications of synthetic\ndata generation, spanning computer vision, speech, natural language processing,\nhealthcare, and business domains. Additionally, it explores different machine\nlearning methods, with particular emphasis on neural network architectures and\ndeep generative models. The paper also addresses the crucial aspects of privacy\nand fairness concerns related to synthetic data generation. Furthermore, this\nstudy identifies the challenges and opportunities prevalent in this emerging\nfield, shedding light on the potential avenues for future research. By delving\ninto the intricacies of synthetic data generation, this paper aims to\ncontribute to the advancement of knowledge and inspire further exploration in\nsynthetic data generation.\n", "link": "http://arxiv.org/abs/2302.04062v7", "date": "2024-05-01", "relevancy": 1.9905, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.519}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4995}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4755}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20for%20Synthetic%20Data%20Generation%3A%20A%20Review&body=Title%3A%20Machine%20Learning%20for%20Synthetic%20Data%20Generation%3A%20A%20Review%0AAuthor%3A%20Yingzhou%20Lu%20and%20Minjie%20Shen%20and%20Huazheng%20Wang%20and%20Xiao%20Wang%20and%20Capucine%20van%20Rechem%20and%20Tianfan%20Fu%20and%20Wenqi%20Wei%0AAbstract%3A%20%20%20Machine%20learning%20heavily%20relies%20on%20data%2C%20but%20real-world%20applications%20often%0Aencounter%20various%20data-related%20issues.%20These%20include%20data%20of%20poor%20quality%2C%0Ainsufficient%20data%20points%20leading%20to%20under-fitting%20of%20machine%20learning%20models%2C%0Aand%20difficulties%20in%20data%20access%20due%20to%20concerns%20surrounding%20privacy%2C%20safety%2C%0Aand%20regulations.%20In%20light%20of%20these%20challenges%2C%20the%20concept%20of%20synthetic%20data%0Ageneration%20emerges%20as%20a%20promising%20alternative%20that%20allows%20for%20data%20sharing%20and%0Autilization%20in%20ways%20that%20real-world%20data%20cannot%20facilitate.%20This%20paper%20presents%0Aa%20comprehensive%20systematic%20review%20of%20existing%20studies%20that%20employ%20machine%0Alearning%20models%20for%20the%20purpose%20of%20generating%20synthetic%20data.%20The%20review%0Aencompasses%20various%20perspectives%2C%20starting%20with%20the%20applications%20of%20synthetic%0Adata%20generation%2C%20spanning%20computer%20vision%2C%20speech%2C%20natural%20language%20processing%2C%0Ahealthcare%2C%20and%20business%20domains.%20Additionally%2C%20it%20explores%20different%20machine%0Alearning%20methods%2C%20with%20particular%20emphasis%20on%20neural%20network%20architectures%20and%0Adeep%20generative%20models.%20The%20paper%20also%20addresses%20the%20crucial%20aspects%20of%20privacy%0Aand%20fairness%20concerns%20related%20to%20synthetic%20data%20generation.%20Furthermore%2C%20this%0Astudy%20identifies%20the%20challenges%20and%20opportunities%20prevalent%20in%20this%20emerging%0Afield%2C%20shedding%20light%20on%20the%20potential%20avenues%20for%20future%20research.%20By%20delving%0Ainto%20the%20intricacies%20of%20synthetic%20data%20generation%2C%20this%20paper%20aims%20to%0Acontribute%20to%20the%20advancement%20of%20knowledge%20and%20inspire%20further%20exploration%20in%0Asynthetic%20data%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.04062v7", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20for%20Synthetic%20Data%20Generation%3A%20A%20Review&entry.906535625=Yingzhou%20Lu%20and%20Minjie%20Shen%20and%20Huazheng%20Wang%20and%20Xiao%20Wang%20and%20Capucine%20van%20Rechem%20and%20Tianfan%20Fu%20and%20Wenqi%20Wei&entry.1292438233=%20%20Machine%20learning%20heavily%20relies%20on%20data%2C%20but%20real-world%20applications%20often%0Aencounter%20various%20data-related%20issues.%20These%20include%20data%20of%20poor%20quality%2C%0Ainsufficient%20data%20points%20leading%20to%20under-fitting%20of%20machine%20learning%20models%2C%0Aand%20difficulties%20in%20data%20access%20due%20to%20concerns%20surrounding%20privacy%2C%20safety%2C%0Aand%20regulations.%20In%20light%20of%20these%20challenges%2C%20the%20concept%20of%20synthetic%20data%0Ageneration%20emerges%20as%20a%20promising%20alternative%20that%20allows%20for%20data%20sharing%20and%0Autilization%20in%20ways%20that%20real-world%20data%20cannot%20facilitate.%20This%20paper%20presents%0Aa%20comprehensive%20systematic%20review%20of%20existing%20studies%20that%20employ%20machine%0Alearning%20models%20for%20the%20purpose%20of%20generating%20synthetic%20data.%20The%20review%0Aencompasses%20various%20perspectives%2C%20starting%20with%20the%20applications%20of%20synthetic%0Adata%20generation%2C%20spanning%20computer%20vision%2C%20speech%2C%20natural%20language%20processing%2C%0Ahealthcare%2C%20and%20business%20domains.%20Additionally%2C%20it%20explores%20different%20machine%0Alearning%20methods%2C%20with%20particular%20emphasis%20on%20neural%20network%20architectures%20and%0Adeep%20generative%20models.%20The%20paper%20also%20addresses%20the%20crucial%20aspects%20of%20privacy%0Aand%20fairness%20concerns%20related%20to%20synthetic%20data%20generation.%20Furthermore%2C%20this%0Astudy%20identifies%20the%20challenges%20and%20opportunities%20prevalent%20in%20this%20emerging%0Afield%2C%20shedding%20light%20on%20the%20potential%20avenues%20for%20future%20research.%20By%20delving%0Ainto%20the%20intricacies%20of%20synthetic%20data%20generation%2C%20this%20paper%20aims%20to%0Acontribute%20to%20the%20advancement%20of%20knowledge%20and%20inspire%20further%20exploration%20in%0Asynthetic%20data%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.04062v7&entry.124074799=Read"},
{"title": "ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural\n  Networks", "author": "Muhammad Kashif and Muhammad Shafique", "abstract": "  In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.\n", "link": "http://arxiv.org/abs/2402.09146v2", "date": "2024-05-01", "relevancy": 1.958, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.515}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4797}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4679}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ResQuNNs%3ATowards%20Enabling%20Deep%20Learning%20in%20Quantum%20Convolution%20Neural%0A%20%20Networks&body=Title%3A%20ResQuNNs%3ATowards%20Enabling%20Deep%20Learning%20in%20Quantum%20Convolution%20Neural%0A%20%20Networks%0AAuthor%3A%20Muhammad%20Kashif%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%20for%20enhancing%20the%20performance%20of%0AQuanvolutional%20Neural%20Networks%20%28QuNNs%29%20by%20introducing%20trainable%20quanvolutional%0Alayers%20and%20addressing%20the%20critical%20challenges%20associated%20with%20them.%20Traditional%0Aquanvolutional%20layers%2C%20although%20beneficial%20for%20feature%20extraction%2C%20have%20largely%0Abeen%20static%2C%20offering%20limited%20adaptability.%20Unlike%20state-of-the-art%2C%20our%0Aresearch%20overcomes%20this%20limitation%20by%20enabling%20training%20within%20these%20layers%2C%0Asignificantly%20increasing%20the%20flexibility%20and%20potential%20of%20QuNNs.%20However%2C%20the%0Aintroduction%20of%20multiple%20trainable%20quanvolutional%20layers%20induces%20complexities%0Ain%20gradient-based%20optimization%2C%20primarily%20due%20to%20the%20difficulty%20in%20accessing%0Agradients%20across%20these%20layers.%20To%20resolve%20this%2C%20we%20propose%20a%20novel%0Aarchitecture%2C%20Residual%20Quanvolutional%20Neural%20Networks%20%28ResQuNNs%29%2C%20leveraging%0Athe%20concept%20of%20residual%20learning%2C%20which%20facilitates%20the%20flow%20of%20gradients%20by%0Aadding%20skip%20connections%20between%20layers.%20By%20inserting%20residual%20blocks%20between%0Aquanvolutional%20layers%2C%20we%20ensure%20enhanced%20gradient%20access%20throughout%20the%0Anetwork%2C%20leading%20to%20improved%20training%20performance.%20Moreover%2C%20we%20provide%0Aempirical%20evidence%20on%20the%20strategic%20placement%20of%20these%20residual%20blocks%20within%0AQuNNs.%20Through%20extensive%20experimentation%2C%20we%20identify%20an%20efficient%0Aconfiguration%20of%20residual%20blocks%2C%20which%20enables%20gradients%20across%20all%20the%20layers%0Ain%20the%20network%20that%20eventually%20results%20in%20efficient%20training.%20Our%20findings%0Asuggest%20that%20the%20precise%20location%20of%20residual%20blocks%20plays%20a%20crucial%20role%20in%0Amaximizing%20the%20performance%20gains%20in%20QuNNs.%20Our%20results%20mark%20a%20substantial%20step%0Aforward%20in%20the%20evolution%20of%20quantum%20deep%20learning%2C%20offering%20new%20avenues%20for%0Aboth%20theoretical%20development%20and%20practical%20quantum%20computing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09146v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResQuNNs%3ATowards%20Enabling%20Deep%20Learning%20in%20Quantum%20Convolution%20Neural%0A%20%20Networks&entry.906535625=Muhammad%20Kashif%20and%20Muhammad%20Shafique&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%20for%20enhancing%20the%20performance%20of%0AQuanvolutional%20Neural%20Networks%20%28QuNNs%29%20by%20introducing%20trainable%20quanvolutional%0Alayers%20and%20addressing%20the%20critical%20challenges%20associated%20with%20them.%20Traditional%0Aquanvolutional%20layers%2C%20although%20beneficial%20for%20feature%20extraction%2C%20have%20largely%0Abeen%20static%2C%20offering%20limited%20adaptability.%20Unlike%20state-of-the-art%2C%20our%0Aresearch%20overcomes%20this%20limitation%20by%20enabling%20training%20within%20these%20layers%2C%0Asignificantly%20increasing%20the%20flexibility%20and%20potential%20of%20QuNNs.%20However%2C%20the%0Aintroduction%20of%20multiple%20trainable%20quanvolutional%20layers%20induces%20complexities%0Ain%20gradient-based%20optimization%2C%20primarily%20due%20to%20the%20difficulty%20in%20accessing%0Agradients%20across%20these%20layers.%20To%20resolve%20this%2C%20we%20propose%20a%20novel%0Aarchitecture%2C%20Residual%20Quanvolutional%20Neural%20Networks%20%28ResQuNNs%29%2C%20leveraging%0Athe%20concept%20of%20residual%20learning%2C%20which%20facilitates%20the%20flow%20of%20gradients%20by%0Aadding%20skip%20connections%20between%20layers.%20By%20inserting%20residual%20blocks%20between%0Aquanvolutional%20layers%2C%20we%20ensure%20enhanced%20gradient%20access%20throughout%20the%0Anetwork%2C%20leading%20to%20improved%20training%20performance.%20Moreover%2C%20we%20provide%0Aempirical%20evidence%20on%20the%20strategic%20placement%20of%20these%20residual%20blocks%20within%0AQuNNs.%20Through%20extensive%20experimentation%2C%20we%20identify%20an%20efficient%0Aconfiguration%20of%20residual%20blocks%2C%20which%20enables%20gradients%20across%20all%20the%20layers%0Ain%20the%20network%20that%20eventually%20results%20in%20efficient%20training.%20Our%20findings%0Asuggest%20that%20the%20precise%20location%20of%20residual%20blocks%20plays%20a%20crucial%20role%20in%0Amaximizing%20the%20performance%20gains%20in%20QuNNs.%20Our%20results%20mark%20a%20substantial%20step%0Aforward%20in%20the%20evolution%20of%20quantum%20deep%20learning%2C%20offering%20new%20avenues%20for%0Aboth%20theoretical%20development%20and%20practical%20quantum%20computing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09146v2&entry.124074799=Read"},
{"title": "PPG-to-ECG Signal Translation for Continuous Atrial Fibrillation\n  Detection via Attention-based Deep State-Space Modeling", "author": "Khuong Vo and Mostafa El-Khamy and Yoojin Choi", "abstract": "  Photoplethysmography (PPG) is a cost-effective and non-invasive technique\nthat utilizes optical methods to measure cardiac physiology. PPG has become\nincreasingly popular in health monitoring and is used in various commercial and\nclinical wearable devices. Compared to electrocardiography (ECG), PPG does not\nprovide substantial clinical diagnostic value, despite the strong correlation\nbetween the two. Here, we propose a subject-independent attention-based deep\nstate-space model (ADSSM) to translate PPG signals to corresponding ECG\nwaveforms. The model is not only robust to noise but also data-efficient by\nincorporating probabilistic prior knowledge. To evaluate our approach, 55\nsubjects' data from the MIMIC-III database were used in their original form,\nand then modified with noise, mimicking real-world scenarios. Our approach was\nproven effective as evidenced by the PR-AUC of 0.986 achieved when inputting\nthe translated ECG signals into an existing atrial fibrillation (AFib)\ndetector. ADSSM enables the integration of ECG's extensive knowledge base and\nPPG's continuous measurement for early diagnosis of cardiovascular disease.\n", "link": "http://arxiv.org/abs/2309.15375v3", "date": "2024-05-01", "relevancy": 1.9508, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4903}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4891}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4845}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PPG-to-ECG%20Signal%20Translation%20for%20Continuous%20Atrial%20Fibrillation%0A%20%20Detection%20via%20Attention-based%20Deep%20State-Space%20Modeling&body=Title%3A%20PPG-to-ECG%20Signal%20Translation%20for%20Continuous%20Atrial%20Fibrillation%0A%20%20Detection%20via%20Attention-based%20Deep%20State-Space%20Modeling%0AAuthor%3A%20Khuong%20Vo%20and%20Mostafa%20El-Khamy%20and%20Yoojin%20Choi%0AAbstract%3A%20%20%20Photoplethysmography%20%28PPG%29%20is%20a%20cost-effective%20and%20non-invasive%20technique%0Athat%20utilizes%20optical%20methods%20to%20measure%20cardiac%20physiology.%20PPG%20has%20become%0Aincreasingly%20popular%20in%20health%20monitoring%20and%20is%20used%20in%20various%20commercial%20and%0Aclinical%20wearable%20devices.%20Compared%20to%20electrocardiography%20%28ECG%29%2C%20PPG%20does%20not%0Aprovide%20substantial%20clinical%20diagnostic%20value%2C%20despite%20the%20strong%20correlation%0Abetween%20the%20two.%20Here%2C%20we%20propose%20a%20subject-independent%20attention-based%20deep%0Astate-space%20model%20%28ADSSM%29%20to%20translate%20PPG%20signals%20to%20corresponding%20ECG%0Awaveforms.%20The%20model%20is%20not%20only%20robust%20to%20noise%20but%20also%20data-efficient%20by%0Aincorporating%20probabilistic%20prior%20knowledge.%20To%20evaluate%20our%20approach%2C%2055%0Asubjects%27%20data%20from%20the%20MIMIC-III%20database%20were%20used%20in%20their%20original%20form%2C%0Aand%20then%20modified%20with%20noise%2C%20mimicking%20real-world%20scenarios.%20Our%20approach%20was%0Aproven%20effective%20as%20evidenced%20by%20the%20PR-AUC%20of%200.986%20achieved%20when%20inputting%0Athe%20translated%20ECG%20signals%20into%20an%20existing%20atrial%20fibrillation%20%28AFib%29%0Adetector.%20ADSSM%20enables%20the%20integration%20of%20ECG%27s%20extensive%20knowledge%20base%20and%0APPG%27s%20continuous%20measurement%20for%20early%20diagnosis%20of%20cardiovascular%20disease.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15375v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PPG-to-ECG%20Signal%20Translation%20for%20Continuous%20Atrial%20Fibrillation%0A%20%20Detection%20via%20Attention-based%20Deep%20State-Space%20Modeling&entry.906535625=Khuong%20Vo%20and%20Mostafa%20El-Khamy%20and%20Yoojin%20Choi&entry.1292438233=%20%20Photoplethysmography%20%28PPG%29%20is%20a%20cost-effective%20and%20non-invasive%20technique%0Athat%20utilizes%20optical%20methods%20to%20measure%20cardiac%20physiology.%20PPG%20has%20become%0Aincreasingly%20popular%20in%20health%20monitoring%20and%20is%20used%20in%20various%20commercial%20and%0Aclinical%20wearable%20devices.%20Compared%20to%20electrocardiography%20%28ECG%29%2C%20PPG%20does%20not%0Aprovide%20substantial%20clinical%20diagnostic%20value%2C%20despite%20the%20strong%20correlation%0Abetween%20the%20two.%20Here%2C%20we%20propose%20a%20subject-independent%20attention-based%20deep%0Astate-space%20model%20%28ADSSM%29%20to%20translate%20PPG%20signals%20to%20corresponding%20ECG%0Awaveforms.%20The%20model%20is%20not%20only%20robust%20to%20noise%20but%20also%20data-efficient%20by%0Aincorporating%20probabilistic%20prior%20knowledge.%20To%20evaluate%20our%20approach%2C%2055%0Asubjects%27%20data%20from%20the%20MIMIC-III%20database%20were%20used%20in%20their%20original%20form%2C%0Aand%20then%20modified%20with%20noise%2C%20mimicking%20real-world%20scenarios.%20Our%20approach%20was%0Aproven%20effective%20as%20evidenced%20by%20the%20PR-AUC%20of%200.986%20achieved%20when%20inputting%0Athe%20translated%20ECG%20signals%20into%20an%20existing%20atrial%20fibrillation%20%28AFib%29%0Adetector.%20ADSSM%20enables%20the%20integration%20of%20ECG%27s%20extensive%20knowledge%20base%20and%0APPG%27s%20continuous%20measurement%20for%20early%20diagnosis%20of%20cardiovascular%20disease.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15375v3&entry.124074799=Read"},
{"title": "Federated Learning with Convex Global and Local Constraints", "author": "Chuan He and Le Peng and Ju Sun", "abstract": "  In practice, many machine learning (ML) problems come with constraints, and\ntheir applied domains involve distributed sensitive data that cannot be shared\nwith others, e.g., in healthcare. Collaborative learning in such practical\nscenarios entails federated learning (FL) for ML problems with constraints, or\nFL with constraints for short. Despite the extensive developments of FL\ntechniques in recent years, these techniques only deal with unconstrained FL\nproblems or FL problems with simple constraints that are amenable to easy\nprojections. There is little work dealing with FL problems with general\nconstraints. To fill this gap, we take the first step toward building an\nalgorithmic framework for solving FL problems with general constraints. In\nparticular, we propose a new FL algorithm for constrained ML problems based on\nthe proximal augmented Lagrangian (AL) method. Assuming convex objective and\nconvex constraints plus other mild conditions, we establish the worst-case\ncomplexity of the proposed algorithm. Our numerical experiments show the\neffectiveness of our algorithm in performing Neyman-Pearson classification and\nfairness-aware learning with nonconvex constraints, in an FL setting.\n", "link": "http://arxiv.org/abs/2310.10117v3", "date": "2024-05-01", "relevancy": 1.9473, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.505}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4827}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20with%20Convex%20Global%20and%20Local%20Constraints&body=Title%3A%20Federated%20Learning%20with%20Convex%20Global%20and%20Local%20Constraints%0AAuthor%3A%20Chuan%20He%20and%20Le%20Peng%20and%20Ju%20Sun%0AAbstract%3A%20%20%20In%20practice%2C%20many%20machine%20learning%20%28ML%29%20problems%20come%20with%20constraints%2C%20and%0Atheir%20applied%20domains%20involve%20distributed%20sensitive%20data%20that%20cannot%20be%20shared%0Awith%20others%2C%20e.g.%2C%20in%20healthcare.%20Collaborative%20learning%20in%20such%20practical%0Ascenarios%20entails%20federated%20learning%20%28FL%29%20for%20ML%20problems%20with%20constraints%2C%20or%0AFL%20with%20constraints%20for%20short.%20Despite%20the%20extensive%20developments%20of%20FL%0Atechniques%20in%20recent%20years%2C%20these%20techniques%20only%20deal%20with%20unconstrained%20FL%0Aproblems%20or%20FL%20problems%20with%20simple%20constraints%20that%20are%20amenable%20to%20easy%0Aprojections.%20There%20is%20little%20work%20dealing%20with%20FL%20problems%20with%20general%0Aconstraints.%20To%20fill%20this%20gap%2C%20we%20take%20the%20first%20step%20toward%20building%20an%0Aalgorithmic%20framework%20for%20solving%20FL%20problems%20with%20general%20constraints.%20In%0Aparticular%2C%20we%20propose%20a%20new%20FL%20algorithm%20for%20constrained%20ML%20problems%20based%20on%0Athe%20proximal%20augmented%20Lagrangian%20%28AL%29%20method.%20Assuming%20convex%20objective%20and%0Aconvex%20constraints%20plus%20other%20mild%20conditions%2C%20we%20establish%20the%20worst-case%0Acomplexity%20of%20the%20proposed%20algorithm.%20Our%20numerical%20experiments%20show%20the%0Aeffectiveness%20of%20our%20algorithm%20in%20performing%20Neyman-Pearson%20classification%20and%0Afairness-aware%20learning%20with%20nonconvex%20constraints%2C%20in%20an%20FL%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10117v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20with%20Convex%20Global%20and%20Local%20Constraints&entry.906535625=Chuan%20He%20and%20Le%20Peng%20and%20Ju%20Sun&entry.1292438233=%20%20In%20practice%2C%20many%20machine%20learning%20%28ML%29%20problems%20come%20with%20constraints%2C%20and%0Atheir%20applied%20domains%20involve%20distributed%20sensitive%20data%20that%20cannot%20be%20shared%0Awith%20others%2C%20e.g.%2C%20in%20healthcare.%20Collaborative%20learning%20in%20such%20practical%0Ascenarios%20entails%20federated%20learning%20%28FL%29%20for%20ML%20problems%20with%20constraints%2C%20or%0AFL%20with%20constraints%20for%20short.%20Despite%20the%20extensive%20developments%20of%20FL%0Atechniques%20in%20recent%20years%2C%20these%20techniques%20only%20deal%20with%20unconstrained%20FL%0Aproblems%20or%20FL%20problems%20with%20simple%20constraints%20that%20are%20amenable%20to%20easy%0Aprojections.%20There%20is%20little%20work%20dealing%20with%20FL%20problems%20with%20general%0Aconstraints.%20To%20fill%20this%20gap%2C%20we%20take%20the%20first%20step%20toward%20building%20an%0Aalgorithmic%20framework%20for%20solving%20FL%20problems%20with%20general%20constraints.%20In%0Aparticular%2C%20we%20propose%20a%20new%20FL%20algorithm%20for%20constrained%20ML%20problems%20based%20on%0Athe%20proximal%20augmented%20Lagrangian%20%28AL%29%20method.%20Assuming%20convex%20objective%20and%0Aconvex%20constraints%20plus%20other%20mild%20conditions%2C%20we%20establish%20the%20worst-case%0Acomplexity%20of%20the%20proposed%20algorithm.%20Our%20numerical%20experiments%20show%20the%0Aeffectiveness%20of%20our%20algorithm%20in%20performing%20Neyman-Pearson%20classification%20and%0Afairness-aware%20learning%20with%20nonconvex%20constraints%2C%20in%20an%20FL%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10117v3&entry.124074799=Read"},
{"title": "Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum\n  Computing", "author": "Jan-Nico Zaech and Martin Danelljan and Tolga Birdal and Luc Van Gool", "abstract": "  Adiabatic quantum computing (AQC) is a promising approach for discrete and\noften NP-hard optimization problems. Current AQCs allow to implement problems\nof research interest, which has sparked the development of quantum\nrepresentations for many computer vision tasks. Despite requiring multiple\nmeasurements from the noisy AQC, current approaches only utilize the best\nmeasurement, discarding information contained in the remaining ones. In this\nwork, we explore the potential of using this information for probabilistic\nbalanced k-means clustering. Instead of discarding non-optimal solutions, we\npropose to use them to compute calibrated posterior probabilities with little\nadditional compute cost. This allows us to identify ambiguous solutions and\ndata points, which we demonstrate on a D-Wave AQC on synthetic tasks and real\nvisual data.\n", "link": "http://arxiv.org/abs/2310.12153v2", "date": "2024-05-01", "relevancy": 1.9392, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4682}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Sampling%20of%20Balanced%20K-Means%20using%20Adiabatic%20Quantum%0A%20%20Computing&body=Title%3A%20Probabilistic%20Sampling%20of%20Balanced%20K-Means%20using%20Adiabatic%20Quantum%0A%20%20Computing%0AAuthor%3A%20Jan-Nico%20Zaech%20and%20Martin%20Danelljan%20and%20Tolga%20Birdal%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Adiabatic%20quantum%20computing%20%28AQC%29%20is%20a%20promising%20approach%20for%20discrete%20and%0Aoften%20NP-hard%20optimization%20problems.%20Current%20AQCs%20allow%20to%20implement%20problems%0Aof%20research%20interest%2C%20which%20has%20sparked%20the%20development%20of%20quantum%0Arepresentations%20for%20many%20computer%20vision%20tasks.%20Despite%20requiring%20multiple%0Ameasurements%20from%20the%20noisy%20AQC%2C%20current%20approaches%20only%20utilize%20the%20best%0Ameasurement%2C%20discarding%20information%20contained%20in%20the%20remaining%20ones.%20In%20this%0Awork%2C%20we%20explore%20the%20potential%20of%20using%20this%20information%20for%20probabilistic%0Abalanced%20k-means%20clustering.%20Instead%20of%20discarding%20non-optimal%20solutions%2C%20we%0Apropose%20to%20use%20them%20to%20compute%20calibrated%20posterior%20probabilities%20with%20little%0Aadditional%20compute%20cost.%20This%20allows%20us%20to%20identify%20ambiguous%20solutions%20and%0Adata%20points%2C%20which%20we%20demonstrate%20on%20a%20D-Wave%20AQC%20on%20synthetic%20tasks%20and%20real%0Avisual%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12153v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Sampling%20of%20Balanced%20K-Means%20using%20Adiabatic%20Quantum%0A%20%20Computing&entry.906535625=Jan-Nico%20Zaech%20and%20Martin%20Danelljan%20and%20Tolga%20Birdal%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Adiabatic%20quantum%20computing%20%28AQC%29%20is%20a%20promising%20approach%20for%20discrete%20and%0Aoften%20NP-hard%20optimization%20problems.%20Current%20AQCs%20allow%20to%20implement%20problems%0Aof%20research%20interest%2C%20which%20has%20sparked%20the%20development%20of%20quantum%0Arepresentations%20for%20many%20computer%20vision%20tasks.%20Despite%20requiring%20multiple%0Ameasurements%20from%20the%20noisy%20AQC%2C%20current%20approaches%20only%20utilize%20the%20best%0Ameasurement%2C%20discarding%20information%20contained%20in%20the%20remaining%20ones.%20In%20this%0Awork%2C%20we%20explore%20the%20potential%20of%20using%20this%20information%20for%20probabilistic%0Abalanced%20k-means%20clustering.%20Instead%20of%20discarding%20non-optimal%20solutions%2C%20we%0Apropose%20to%20use%20them%20to%20compute%20calibrated%20posterior%20probabilities%20with%20little%0Aadditional%20compute%20cost.%20This%20allows%20us%20to%20identify%20ambiguous%20solutions%20and%0Adata%20points%2C%20which%20we%20demonstrate%20on%20a%20D-Wave%20AQC%20on%20synthetic%20tasks%20and%20real%0Avisual%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12153v2&entry.124074799=Read"},
{"title": "Powering In-Database Dynamic Model Slicing for Structured Data Analytics", "author": "Lingze Zeng and Naili Xing and Shaofeng Cai and Gang Chen and Beng Chin Ooi and Jian Pei and Yuncheng Wu", "abstract": "  Relational database management systems (RDBMS) are widely used for the\nstorage and retrieval of structured data. To derive insights beyond statistical\naggregation, we typically have to extract specific subdatasets from the\ndatabase using conventional database operations, and then apply deep neural\nnetworks (DNN) training and inference on these respective subdatasets in a\nseparate machine learning system. The process can be prohibitively expensive,\nespecially when there are a combinatorial number of subdatasets extracted for\ndifferent analytical purposes. This calls for efficient in-database support of\nadvanced analytical methods In this paper, we introduce LEADS, a novel\nSQL-aware dynamic model slicing technique to customize models for subdatasets\nspecified by SQL queries. LEADS improves the predictive modeling of structured\ndata via the mixture of experts (MoE) technique and maintains inference\nefficiency by a SQL-aware gating network. At the core of LEADS is the\nconstruction of a general model with multiple expert sub-models via MoE trained\nover the entire database. This SQL-aware MoE technique scales up the modeling\ncapacity, enhances effectiveness, and preserves efficiency by activating only\nnecessary experts via the gating network during inference. Additionally, we\nintroduce two regularization terms during the training process of LEADS to\nstrike a balance between effectiveness and efficiency. We also design and build\nan in-database inference system, called INDICES, to support end-to-end advanced\nstructured data analytics by non-intrusively incorporating LEADS onto\nPostgreSQL. Our extensive experiments on real-world datasets demonstrate that\nLEADS consistently outperforms baseline models, and INDICES delivers effective\nin-database analytics with a considerable reduction in inference latency\ncompared to traditional solutions.\n", "link": "http://arxiv.org/abs/2405.00568v1", "date": "2024-05-01", "relevancy": 1.9311, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5139}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4606}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4605}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Powering%20In-Database%20Dynamic%20Model%20Slicing%20for%20Structured%20Data%20Analytics&body=Title%3A%20Powering%20In-Database%20Dynamic%20Model%20Slicing%20for%20Structured%20Data%20Analytics%0AAuthor%3A%20Lingze%20Zeng%20and%20Naili%20Xing%20and%20Shaofeng%20Cai%20and%20Gang%20Chen%20and%20Beng%20Chin%20Ooi%20and%20Jian%20Pei%20and%20Yuncheng%20Wu%0AAbstract%3A%20%20%20Relational%20database%20management%20systems%20%28RDBMS%29%20are%20widely%20used%20for%20the%0Astorage%20and%20retrieval%20of%20structured%20data.%20To%20derive%20insights%20beyond%20statistical%0Aaggregation%2C%20we%20typically%20have%20to%20extract%20specific%20subdatasets%20from%20the%0Adatabase%20using%20conventional%20database%20operations%2C%20and%20then%20apply%20deep%20neural%0Anetworks%20%28DNN%29%20training%20and%20inference%20on%20these%20respective%20subdatasets%20in%20a%0Aseparate%20machine%20learning%20system.%20The%20process%20can%20be%20prohibitively%20expensive%2C%0Aespecially%20when%20there%20are%20a%20combinatorial%20number%20of%20subdatasets%20extracted%20for%0Adifferent%20analytical%20purposes.%20This%20calls%20for%20efficient%20in-database%20support%20of%0Aadvanced%20analytical%20methods%20In%20this%20paper%2C%20we%20introduce%20LEADS%2C%20a%20novel%0ASQL-aware%20dynamic%20model%20slicing%20technique%20to%20customize%20models%20for%20subdatasets%0Aspecified%20by%20SQL%20queries.%20LEADS%20improves%20the%20predictive%20modeling%20of%20structured%0Adata%20via%20the%20mixture%20of%20experts%20%28MoE%29%20technique%20and%20maintains%20inference%0Aefficiency%20by%20a%20SQL-aware%20gating%20network.%20At%20the%20core%20of%20LEADS%20is%20the%0Aconstruction%20of%20a%20general%20model%20with%20multiple%20expert%20sub-models%20via%20MoE%20trained%0Aover%20the%20entire%20database.%20This%20SQL-aware%20MoE%20technique%20scales%20up%20the%20modeling%0Acapacity%2C%20enhances%20effectiveness%2C%20and%20preserves%20efficiency%20by%20activating%20only%0Anecessary%20experts%20via%20the%20gating%20network%20during%20inference.%20Additionally%2C%20we%0Aintroduce%20two%20regularization%20terms%20during%20the%20training%20process%20of%20LEADS%20to%0Astrike%20a%20balance%20between%20effectiveness%20and%20efficiency.%20We%20also%20design%20and%20build%0Aan%20in-database%20inference%20system%2C%20called%20INDICES%2C%20to%20support%20end-to-end%20advanced%0Astructured%20data%20analytics%20by%20non-intrusively%20incorporating%20LEADS%20onto%0APostgreSQL.%20Our%20extensive%20experiments%20on%20real-world%20datasets%20demonstrate%20that%0ALEADS%20consistently%20outperforms%20baseline%20models%2C%20and%20INDICES%20delivers%20effective%0Ain-database%20analytics%20with%20a%20considerable%20reduction%20in%20inference%20latency%0Acompared%20to%20traditional%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00568v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Powering%20In-Database%20Dynamic%20Model%20Slicing%20for%20Structured%20Data%20Analytics&entry.906535625=Lingze%20Zeng%20and%20Naili%20Xing%20and%20Shaofeng%20Cai%20and%20Gang%20Chen%20and%20Beng%20Chin%20Ooi%20and%20Jian%20Pei%20and%20Yuncheng%20Wu&entry.1292438233=%20%20Relational%20database%20management%20systems%20%28RDBMS%29%20are%20widely%20used%20for%20the%0Astorage%20and%20retrieval%20of%20structured%20data.%20To%20derive%20insights%20beyond%20statistical%0Aaggregation%2C%20we%20typically%20have%20to%20extract%20specific%20subdatasets%20from%20the%0Adatabase%20using%20conventional%20database%20operations%2C%20and%20then%20apply%20deep%20neural%0Anetworks%20%28DNN%29%20training%20and%20inference%20on%20these%20respective%20subdatasets%20in%20a%0Aseparate%20machine%20learning%20system.%20The%20process%20can%20be%20prohibitively%20expensive%2C%0Aespecially%20when%20there%20are%20a%20combinatorial%20number%20of%20subdatasets%20extracted%20for%0Adifferent%20analytical%20purposes.%20This%20calls%20for%20efficient%20in-database%20support%20of%0Aadvanced%20analytical%20methods%20In%20this%20paper%2C%20we%20introduce%20LEADS%2C%20a%20novel%0ASQL-aware%20dynamic%20model%20slicing%20technique%20to%20customize%20models%20for%20subdatasets%0Aspecified%20by%20SQL%20queries.%20LEADS%20improves%20the%20predictive%20modeling%20of%20structured%0Adata%20via%20the%20mixture%20of%20experts%20%28MoE%29%20technique%20and%20maintains%20inference%0Aefficiency%20by%20a%20SQL-aware%20gating%20network.%20At%20the%20core%20of%20LEADS%20is%20the%0Aconstruction%20of%20a%20general%20model%20with%20multiple%20expert%20sub-models%20via%20MoE%20trained%0Aover%20the%20entire%20database.%20This%20SQL-aware%20MoE%20technique%20scales%20up%20the%20modeling%0Acapacity%2C%20enhances%20effectiveness%2C%20and%20preserves%20efficiency%20by%20activating%20only%0Anecessary%20experts%20via%20the%20gating%20network%20during%20inference.%20Additionally%2C%20we%0Aintroduce%20two%20regularization%20terms%20during%20the%20training%20process%20of%20LEADS%20to%0Astrike%20a%20balance%20between%20effectiveness%20and%20efficiency.%20We%20also%20design%20and%20build%0Aan%20in-database%20inference%20system%2C%20called%20INDICES%2C%20to%20support%20end-to-end%20advanced%0Astructured%20data%20analytics%20by%20non-intrusively%20incorporating%20LEADS%20onto%0APostgreSQL.%20Our%20extensive%20experiments%20on%20real-world%20datasets%20demonstrate%20that%0ALEADS%20consistently%20outperforms%20baseline%20models%2C%20and%20INDICES%20delivers%20effective%0Ain-database%20analytics%20with%20a%20considerable%20reduction%20in%20inference%20latency%0Acompared%20to%20traditional%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00568v1&entry.124074799=Read"},
{"title": "GOLD: Geometry Problem Solver with Natural Language Description", "author": "Jiaxin Zhang and Yashar Moshfeghi", "abstract": "  Addressing the challenge of automated geometry math problem-solving in\nartificial intelligence (AI) involves understanding multi-modal information and\nmathematics. Current methods struggle with accurately interpreting geometry\ndiagrams, which hinders effective problem-solving. To tackle this issue, we\npresent the Geometry problem sOlver with natural Language Description (GOLD)\nmodel. GOLD enhances the extraction of geometric relations by separately\nprocessing symbols and geometric primitives within the diagram. Subsequently,\nit converts the extracted relations into natural language descriptions,\nefficiently utilizing large language models to solve geometry math problems.\nExperiments show that the GOLD model outperforms the Geoformer model, the\nprevious best method on the UniGeo dataset, by achieving accuracy improvements\nof 12.7% and 42.1% in calculation and proving subsets. Additionally, it\nsurpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet,\nby obtaining accuracy enhancements of 1.8% and 3.2%, respectively.\n", "link": "http://arxiv.org/abs/2405.00494v1", "date": "2024-05-01", "relevancy": 1.9282, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4879}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.482}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4798}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GOLD%3A%20Geometry%20Problem%20Solver%20with%20Natural%20Language%20Description&body=Title%3A%20GOLD%3A%20Geometry%20Problem%20Solver%20with%20Natural%20Language%20Description%0AAuthor%3A%20Jiaxin%20Zhang%20and%20Yashar%20Moshfeghi%0AAbstract%3A%20%20%20Addressing%20the%20challenge%20of%20automated%20geometry%20math%20problem-solving%20in%0Aartificial%20intelligence%20%28AI%29%20involves%20understanding%20multi-modal%20information%20and%0Amathematics.%20Current%20methods%20struggle%20with%20accurately%20interpreting%20geometry%0Adiagrams%2C%20which%20hinders%20effective%20problem-solving.%20To%20tackle%20this%20issue%2C%20we%0Apresent%20the%20Geometry%20problem%20sOlver%20with%20natural%20Language%20Description%20%28GOLD%29%0Amodel.%20GOLD%20enhances%20the%20extraction%20of%20geometric%20relations%20by%20separately%0Aprocessing%20symbols%20and%20geometric%20primitives%20within%20the%20diagram.%20Subsequently%2C%0Ait%20converts%20the%20extracted%20relations%20into%20natural%20language%20descriptions%2C%0Aefficiently%20utilizing%20large%20language%20models%20to%20solve%20geometry%20math%20problems.%0AExperiments%20show%20that%20the%20GOLD%20model%20outperforms%20the%20Geoformer%20model%2C%20the%0Aprevious%20best%20method%20on%20the%20UniGeo%20dataset%2C%20by%20achieving%20accuracy%20improvements%0Aof%2012.7%25%20and%2042.1%25%20in%20calculation%20and%20proving%20subsets.%20Additionally%2C%20it%0Asurpasses%20the%20former%20best%20model%20on%20the%20PGPS9K%20and%20Geometry3K%20datasets%2C%20PGPSNet%2C%0Aby%20obtaining%20accuracy%20enhancements%20of%201.8%25%20and%203.2%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00494v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOLD%3A%20Geometry%20Problem%20Solver%20with%20Natural%20Language%20Description&entry.906535625=Jiaxin%20Zhang%20and%20Yashar%20Moshfeghi&entry.1292438233=%20%20Addressing%20the%20challenge%20of%20automated%20geometry%20math%20problem-solving%20in%0Aartificial%20intelligence%20%28AI%29%20involves%20understanding%20multi-modal%20information%20and%0Amathematics.%20Current%20methods%20struggle%20with%20accurately%20interpreting%20geometry%0Adiagrams%2C%20which%20hinders%20effective%20problem-solving.%20To%20tackle%20this%20issue%2C%20we%0Apresent%20the%20Geometry%20problem%20sOlver%20with%20natural%20Language%20Description%20%28GOLD%29%0Amodel.%20GOLD%20enhances%20the%20extraction%20of%20geometric%20relations%20by%20separately%0Aprocessing%20symbols%20and%20geometric%20primitives%20within%20the%20diagram.%20Subsequently%2C%0Ait%20converts%20the%20extracted%20relations%20into%20natural%20language%20descriptions%2C%0Aefficiently%20utilizing%20large%20language%20models%20to%20solve%20geometry%20math%20problems.%0AExperiments%20show%20that%20the%20GOLD%20model%20outperforms%20the%20Geoformer%20model%2C%20the%0Aprevious%20best%20method%20on%20the%20UniGeo%20dataset%2C%20by%20achieving%20accuracy%20improvements%0Aof%2012.7%25%20and%2042.1%25%20in%20calculation%20and%20proving%20subsets.%20Additionally%2C%20it%0Asurpasses%20the%20former%20best%20model%20on%20the%20PGPS9K%20and%20Geometry3K%20datasets%2C%20PGPSNet%2C%0Aby%20obtaining%20accuracy%20enhancements%20of%201.8%25%20and%203.2%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00494v1&entry.124074799=Read"},
{"title": "Robust Semi-supervised Learning via $f$-Divergence and $\u03b1$-R\u00e9nyi\n  Divergence", "author": "Gholamali Aminian and Amirhossien Bagheri and Mahyar JafariNodeh and Radmehr Karimian and Mohammad-Hossein Yassaee", "abstract": "  This paper investigates a range of empirical risk functions and\nregularization methods suitable for self-training methods in semi-supervised\nlearning. These approaches draw inspiration from various divergence measures,\nsuch as $f$-divergences and $\\alpha$-R\\'enyi divergences. Inspired by the\ntheoretical foundations rooted in divergences, i.e., $f$-divergences and\n$\\alpha$-R\\'enyi divergence, we also provide valuable insights to enhance the\nunderstanding of our empirical risk functions and regularization techniques. In\nthe pseudo-labeling and entropy minimization techniques as self-training\nmethods for effective semi-supervised learning, the self-training process has\nsome inherent mismatch between the true label and pseudo-label (noisy\npseudo-labels) and some of our empirical risk functions are robust, concerning\nnoisy pseudo-labels. Under some conditions, our empirical risk functions\ndemonstrate better performance when compared to traditional self-training\nmethods.\n", "link": "http://arxiv.org/abs/2405.00454v1", "date": "2024-05-01", "relevancy": 1.9146, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5005}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4821}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4664}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Semi-supervised%20Learning%20via%20%24f%24-Divergence%20and%20%24%CE%B1%24-R%C3%A9nyi%0A%20%20Divergence&body=Title%3A%20Robust%20Semi-supervised%20Learning%20via%20%24f%24-Divergence%20and%20%24%CE%B1%24-R%C3%A9nyi%0A%20%20Divergence%0AAuthor%3A%20Gholamali%20Aminian%20and%20Amirhossien%20Bagheri%20and%20Mahyar%20JafariNodeh%20and%20Radmehr%20Karimian%20and%20Mohammad-Hossein%20Yassaee%0AAbstract%3A%20%20%20This%20paper%20investigates%20a%20range%20of%20empirical%20risk%20functions%20and%0Aregularization%20methods%20suitable%20for%20self-training%20methods%20in%20semi-supervised%0Alearning.%20These%20approaches%20draw%20inspiration%20from%20various%20divergence%20measures%2C%0Asuch%20as%20%24f%24-divergences%20and%20%24%5Calpha%24-R%5C%27enyi%20divergences.%20Inspired%20by%20the%0Atheoretical%20foundations%20rooted%20in%20divergences%2C%20i.e.%2C%20%24f%24-divergences%20and%0A%24%5Calpha%24-R%5C%27enyi%20divergence%2C%20we%20also%20provide%20valuable%20insights%20to%20enhance%20the%0Aunderstanding%20of%20our%20empirical%20risk%20functions%20and%20regularization%20techniques.%20In%0Athe%20pseudo-labeling%20and%20entropy%20minimization%20techniques%20as%20self-training%0Amethods%20for%20effective%20semi-supervised%20learning%2C%20the%20self-training%20process%20has%0Asome%20inherent%20mismatch%20between%20the%20true%20label%20and%20pseudo-label%20%28noisy%0Apseudo-labels%29%20and%20some%20of%20our%20empirical%20risk%20functions%20are%20robust%2C%20concerning%0Anoisy%20pseudo-labels.%20Under%20some%20conditions%2C%20our%20empirical%20risk%20functions%0Ademonstrate%20better%20performance%20when%20compared%20to%20traditional%20self-training%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00454v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Semi-supervised%20Learning%20via%20%24f%24-Divergence%20and%20%24%CE%B1%24-R%C3%A9nyi%0A%20%20Divergence&entry.906535625=Gholamali%20Aminian%20and%20Amirhossien%20Bagheri%20and%20Mahyar%20JafariNodeh%20and%20Radmehr%20Karimian%20and%20Mohammad-Hossein%20Yassaee&entry.1292438233=%20%20This%20paper%20investigates%20a%20range%20of%20empirical%20risk%20functions%20and%0Aregularization%20methods%20suitable%20for%20self-training%20methods%20in%20semi-supervised%0Alearning.%20These%20approaches%20draw%20inspiration%20from%20various%20divergence%20measures%2C%0Asuch%20as%20%24f%24-divergences%20and%20%24%5Calpha%24-R%5C%27enyi%20divergences.%20Inspired%20by%20the%0Atheoretical%20foundations%20rooted%20in%20divergences%2C%20i.e.%2C%20%24f%24-divergences%20and%0A%24%5Calpha%24-R%5C%27enyi%20divergence%2C%20we%20also%20provide%20valuable%20insights%20to%20enhance%20the%0Aunderstanding%20of%20our%20empirical%20risk%20functions%20and%20regularization%20techniques.%20In%0Athe%20pseudo-labeling%20and%20entropy%20minimization%20techniques%20as%20self-training%0Amethods%20for%20effective%20semi-supervised%20learning%2C%20the%20self-training%20process%20has%0Asome%20inherent%20mismatch%20between%20the%20true%20label%20and%20pseudo-label%20%28noisy%0Apseudo-labels%29%20and%20some%20of%20our%20empirical%20risk%20functions%20are%20robust%2C%20concerning%0Anoisy%20pseudo-labels.%20Under%20some%20conditions%2C%20our%20empirical%20risk%20functions%0Ademonstrate%20better%20performance%20when%20compared%20to%20traditional%20self-training%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00454v1&entry.124074799=Read"},
{"title": "Large Language Models as Zero-shot Dialogue State Tracker through\n  Function Calling", "author": "Zekun Li and Zhiyu Zoey Chen and Mike Ross and Patrick Huber and Seungwhan Moon and Zhaojiang Lin and Xin Luna Dong and Adithya Sagar and Xifeng Yan and Paul A. Crook", "abstract": "  Large language models (LLMs) are increasingly prevalent in conversational\nsystems due to their advanced understanding and generative capabilities in\ngeneral contexts. However, their effectiveness in task-oriented dialogues\n(TOD), which requires not only response generation but also effective dialogue\nstate tracking (DST) within specific tasks and domains, remains less\nsatisfying. In this work, we propose a novel approach FnCTOD for solving DST\nwith LLMs through function calling. This method improves zero-shot DST,\nallowing adaptation to diverse domains without extensive data collection or\nmodel tuning. Our experimental results demonstrate that our approach achieves\nexceptional performance with both modestly sized open-source and also\nproprietary LLMs: with in-context prompting it enables various 7B or 13B\nparameter models to surpass the previous state-of-the-art (SOTA) achieved by\nChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average\njoint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are\nboosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a\nsmall collection of diverse task-oriented dialogues, we can equip modest at\nhttps://github.com/facebookresearch/FnCTOD\n", "link": "http://arxiv.org/abs/2402.10466v2", "date": "2024-05-01", "relevancy": 1.9077, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4915}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4722}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4523}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20as%20Zero-shot%20Dialogue%20State%20Tracker%20through%0A%20%20Function%20Calling&body=Title%3A%20Large%20Language%20Models%20as%20Zero-shot%20Dialogue%20State%20Tracker%20through%0A%20%20Function%20Calling%0AAuthor%3A%20Zekun%20Li%20and%20Zhiyu%20Zoey%20Chen%20and%20Mike%20Ross%20and%20Patrick%20Huber%20and%20Seungwhan%20Moon%20and%20Zhaojiang%20Lin%20and%20Xin%20Luna%20Dong%20and%20Adithya%20Sagar%20and%20Xifeng%20Yan%20and%20Paul%20A.%20Crook%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20prevalent%20in%20conversational%0Asystems%20due%20to%20their%20advanced%20understanding%20and%20generative%20capabilities%20in%0Ageneral%20contexts.%20However%2C%20their%20effectiveness%20in%20task-oriented%20dialogues%0A%28TOD%29%2C%20which%20requires%20not%20only%20response%20generation%20but%20also%20effective%20dialogue%0Astate%20tracking%20%28DST%29%20within%20specific%20tasks%20and%20domains%2C%20remains%20less%0Asatisfying.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20FnCTOD%20for%20solving%20DST%0Awith%20LLMs%20through%20function%20calling.%20This%20method%20improves%20zero-shot%20DST%2C%0Aallowing%20adaptation%20to%20diverse%20domains%20without%20extensive%20data%20collection%20or%0Amodel%20tuning.%20Our%20experimental%20results%20demonstrate%20that%20our%20approach%20achieves%0Aexceptional%20performance%20with%20both%20modestly%20sized%20open-source%20and%20also%0Aproprietary%20LLMs%3A%20with%20in-context%20prompting%20it%20enables%20various%207B%20or%2013B%0Aparameter%20models%20to%20surpass%20the%20previous%20state-of-the-art%20%28SOTA%29%20achieved%20by%0AChatGPT%2C%20and%20improves%20ChatGPT%27s%20performance%20beating%20the%20SOTA%20by%205.6%25%20average%0Ajoint%20goal%20accuracy%20%28JGA%29.%20Individual%20model%20results%20for%20GPT-3.5%20and%20GPT-4%20are%0Aboosted%20by%204.8%25%20and%2014%25%2C%20respectively.%20We%20also%20show%20that%20by%20fine-tuning%20on%20a%0Asmall%20collection%20of%20diverse%20task-oriented%20dialogues%2C%20we%20can%20equip%20modest%20at%0Ahttps%3A//github.com/facebookresearch/FnCTOD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10466v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20as%20Zero-shot%20Dialogue%20State%20Tracker%20through%0A%20%20Function%20Calling&entry.906535625=Zekun%20Li%20and%20Zhiyu%20Zoey%20Chen%20and%20Mike%20Ross%20and%20Patrick%20Huber%20and%20Seungwhan%20Moon%20and%20Zhaojiang%20Lin%20and%20Xin%20Luna%20Dong%20and%20Adithya%20Sagar%20and%20Xifeng%20Yan%20and%20Paul%20A.%20Crook&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20prevalent%20in%20conversational%0Asystems%20due%20to%20their%20advanced%20understanding%20and%20generative%20capabilities%20in%0Ageneral%20contexts.%20However%2C%20their%20effectiveness%20in%20task-oriented%20dialogues%0A%28TOD%29%2C%20which%20requires%20not%20only%20response%20generation%20but%20also%20effective%20dialogue%0Astate%20tracking%20%28DST%29%20within%20specific%20tasks%20and%20domains%2C%20remains%20less%0Asatisfying.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20FnCTOD%20for%20solving%20DST%0Awith%20LLMs%20through%20function%20calling.%20This%20method%20improves%20zero-shot%20DST%2C%0Aallowing%20adaptation%20to%20diverse%20domains%20without%20extensive%20data%20collection%20or%0Amodel%20tuning.%20Our%20experimental%20results%20demonstrate%20that%20our%20approach%20achieves%0Aexceptional%20performance%20with%20both%20modestly%20sized%20open-source%20and%20also%0Aproprietary%20LLMs%3A%20with%20in-context%20prompting%20it%20enables%20various%207B%20or%2013B%0Aparameter%20models%20to%20surpass%20the%20previous%20state-of-the-art%20%28SOTA%29%20achieved%20by%0AChatGPT%2C%20and%20improves%20ChatGPT%27s%20performance%20beating%20the%20SOTA%20by%205.6%25%20average%0Ajoint%20goal%20accuracy%20%28JGA%29.%20Individual%20model%20results%20for%20GPT-3.5%20and%20GPT-4%20are%0Aboosted%20by%204.8%25%20and%2014%25%2C%20respectively.%20We%20also%20show%20that%20by%20fine-tuning%20on%20a%0Asmall%20collection%20of%20diverse%20task-oriented%20dialogues%2C%20we%20can%20equip%20modest%20at%0Ahttps%3A//github.com/facebookresearch/FnCTOD%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10466v2&entry.124074799=Read"},
{"title": "Scaling and renormalization in high-dimensional regression", "author": "Alexander B. Atanasov and Jacob A. Zavatone-Veth and Cengiz Pehlevan", "abstract": "  This paper presents a succinct derivation of the training and generalization\nperformance of a variety of high-dimensional ridge regression models using the\nbasic tools of random matrix theory and free probability. We provide an\nintroduction and review of recent results on these topics, aimed at readers\nwith backgrounds in physics and deep learning. Analytic formulas for the\ntraining and generalization errors are obtained in a few lines of algebra\ndirectly from the properties of the $S$-transform of free probability. This\nallows for a straightforward identification of the sources of power-law scaling\nin model performance. We compute the generalization error of a broad class of\nrandom feature models. We find that in all models, the $S$-transform\ncorresponds to the train-test generalization gap, and yields an analogue of the\ngeneralized-cross-validation estimator. Using these techniques, we derive\nfine-grained bias-variance decompositions for a very general class of random\nfeature models with structured covariates. These novel results allow us to\ndiscover a scaling regime for random feature models where the variance due to\nthe features limits performance in the overparameterized setting. We also\ndemonstrate how anisotropic weight structure in random feature models can limit\nperformance and lead to nontrivial exponents for finite-width corrections in\nthe overparameterized setting. Our results extend and provide a unifying\nperspective on earlier models of neural scaling laws.\n", "link": "http://arxiv.org/abs/2405.00592v1", "date": "2024-05-01", "relevancy": 1.864, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5445}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4541}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scaling%20and%20renormalization%20in%20high-dimensional%20regression&body=Title%3A%20Scaling%20and%20renormalization%20in%20high-dimensional%20regression%0AAuthor%3A%20Alexander%20B.%20Atanasov%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20succinct%20derivation%20of%20the%20training%20and%20generalization%0Aperformance%20of%20a%20variety%20of%20high-dimensional%20ridge%20regression%20models%20using%20the%0Abasic%20tools%20of%20random%20matrix%20theory%20and%20free%20probability.%20We%20provide%20an%0Aintroduction%20and%20review%20of%20recent%20results%20on%20these%20topics%2C%20aimed%20at%20readers%0Awith%20backgrounds%20in%20physics%20and%20deep%20learning.%20Analytic%20formulas%20for%20the%0Atraining%20and%20generalization%20errors%20are%20obtained%20in%20a%20few%20lines%20of%20algebra%0Adirectly%20from%20the%20properties%20of%20the%20%24S%24-transform%20of%20free%20probability.%20This%0Aallows%20for%20a%20straightforward%20identification%20of%20the%20sources%20of%20power-law%20scaling%0Ain%20model%20performance.%20We%20compute%20the%20generalization%20error%20of%20a%20broad%20class%20of%0Arandom%20feature%20models.%20We%20find%20that%20in%20all%20models%2C%20the%20%24S%24-transform%0Acorresponds%20to%20the%20train-test%20generalization%20gap%2C%20and%20yields%20an%20analogue%20of%20the%0Ageneralized-cross-validation%20estimator.%20Using%20these%20techniques%2C%20we%20derive%0Afine-grained%20bias-variance%20decompositions%20for%20a%20very%20general%20class%20of%20random%0Afeature%20models%20with%20structured%20covariates.%20These%20novel%20results%20allow%20us%20to%0Adiscover%20a%20scaling%20regime%20for%20random%20feature%20models%20where%20the%20variance%20due%20to%0Athe%20features%20limits%20performance%20in%20the%20overparameterized%20setting.%20We%20also%0Ademonstrate%20how%20anisotropic%20weight%20structure%20in%20random%20feature%20models%20can%20limit%0Aperformance%20and%20lead%20to%20nontrivial%20exponents%20for%20finite-width%20corrections%20in%0Athe%20overparameterized%20setting.%20Our%20results%20extend%20and%20provide%20a%20unifying%0Aperspective%20on%20earlier%20models%20of%20neural%20scaling%20laws.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00592v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20and%20renormalization%20in%20high-dimensional%20regression&entry.906535625=Alexander%20B.%20Atanasov%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20This%20paper%20presents%20a%20succinct%20derivation%20of%20the%20training%20and%20generalization%0Aperformance%20of%20a%20variety%20of%20high-dimensional%20ridge%20regression%20models%20using%20the%0Abasic%20tools%20of%20random%20matrix%20theory%20and%20free%20probability.%20We%20provide%20an%0Aintroduction%20and%20review%20of%20recent%20results%20on%20these%20topics%2C%20aimed%20at%20readers%0Awith%20backgrounds%20in%20physics%20and%20deep%20learning.%20Analytic%20formulas%20for%20the%0Atraining%20and%20generalization%20errors%20are%20obtained%20in%20a%20few%20lines%20of%20algebra%0Adirectly%20from%20the%20properties%20of%20the%20%24S%24-transform%20of%20free%20probability.%20This%0Aallows%20for%20a%20straightforward%20identification%20of%20the%20sources%20of%20power-law%20scaling%0Ain%20model%20performance.%20We%20compute%20the%20generalization%20error%20of%20a%20broad%20class%20of%0Arandom%20feature%20models.%20We%20find%20that%20in%20all%20models%2C%20the%20%24S%24-transform%0Acorresponds%20to%20the%20train-test%20generalization%20gap%2C%20and%20yields%20an%20analogue%20of%20the%0Ageneralized-cross-validation%20estimator.%20Using%20these%20techniques%2C%20we%20derive%0Afine-grained%20bias-variance%20decompositions%20for%20a%20very%20general%20class%20of%20random%0Afeature%20models%20with%20structured%20covariates.%20These%20novel%20results%20allow%20us%20to%0Adiscover%20a%20scaling%20regime%20for%20random%20feature%20models%20where%20the%20variance%20due%20to%0Athe%20features%20limits%20performance%20in%20the%20overparameterized%20setting.%20We%20also%0Ademonstrate%20how%20anisotropic%20weight%20structure%20in%20random%20feature%20models%20can%20limit%0Aperformance%20and%20lead%20to%20nontrivial%20exponents%20for%20finite-width%20corrections%20in%0Athe%20overparameterized%20setting.%20Our%20results%20extend%20and%20provide%20a%20unifying%0Aperspective%20on%20earlier%20models%20of%20neural%20scaling%20laws.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00592v1&entry.124074799=Read"},
{"title": "Causal Evaluation of Language Models", "author": "Sirui Chen and Bo Peng and Meiqi Chen and Ruiqi Wang and Mengying Xu and Xingyu Zeng and Rui Zhao and Shengjie Zhao and Yu Qiao and Chaochao Lu", "abstract": "  Causal reasoning is viewed as crucial for achieving human-level machine\nintelligence. Recent advances in language models have expanded the horizons of\nartificial intelligence across various domains, sparking inquiries into their\npotential for causal reasoning. In this work, we introduce Causal evaluation of\nLanguage Models (CaLM), which, to the best of our knowledge, is the first\ncomprehensive benchmark for evaluating the causal reasoning capabilities of\nlanguage models. First, we propose the CaLM framework, which establishes a\nfoundational taxonomy consisting of four modules: causal target (i.e., what to\nevaluate), adaptation (i.e., how to obtain the results), metric (i.e., how to\nmeasure the results), and error (i.e., how to analyze the bad results). This\ntaxonomy defines a broad evaluation design space while systematically selecting\ncriteria and priorities. Second, we compose the CaLM dataset, comprising\n126,334 data samples, to provide curated sets of causal targets, adaptations,\nmetrics, and errors, offering extensive coverage for diverse research pursuits.\nThird, we conduct an extensive evaluation of 28 leading language models on a\ncore set of 92 causal targets, 9 adaptations, 7 metrics, and 12 error types.\nFourth, we perform detailed analyses of the evaluation results across various\ndimensions (e.g., adaptation, scale). Fifth, we present 50 high-level empirical\nfindings across 9 dimensions (e.g., model), providing valuable guidance for\nfuture language model development. Finally, we develop a multifaceted platform,\nincluding a website, leaderboards, datasets, and toolkits, to support scalable\nand adaptable assessments. We envision CaLM as an ever-evolving benchmark for\nthe community, systematically updated with new causal targets, adaptations,\nmodels, metrics, and error types to reflect ongoing research advancements.\nProject website is at https://opencausalab.github.io/CaLM.\n", "link": "http://arxiv.org/abs/2405.00622v1", "date": "2024-05-01", "relevancy": 1.8484, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4373}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Causal%20Evaluation%20of%20Language%20Models&body=Title%3A%20Causal%20Evaluation%20of%20Language%20Models%0AAuthor%3A%20Sirui%20Chen%20and%20Bo%20Peng%20and%20Meiqi%20Chen%20and%20Ruiqi%20Wang%20and%20Mengying%20Xu%20and%20Xingyu%20Zeng%20and%20Rui%20Zhao%20and%20Shengjie%20Zhao%20and%20Yu%20Qiao%20and%20Chaochao%20Lu%0AAbstract%3A%20%20%20Causal%20reasoning%20is%20viewed%20as%20crucial%20for%20achieving%20human-level%20machine%0Aintelligence.%20Recent%20advances%20in%20language%20models%20have%20expanded%20the%20horizons%20of%0Aartificial%20intelligence%20across%20various%20domains%2C%20sparking%20inquiries%20into%20their%0Apotential%20for%20causal%20reasoning.%20In%20this%20work%2C%20we%20introduce%20Causal%20evaluation%20of%0ALanguage%20Models%20%28CaLM%29%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%0Acomprehensive%20benchmark%20for%20evaluating%20the%20causal%20reasoning%20capabilities%20of%0Alanguage%20models.%20First%2C%20we%20propose%20the%20CaLM%20framework%2C%20which%20establishes%20a%0Afoundational%20taxonomy%20consisting%20of%20four%20modules%3A%20causal%20target%20%28i.e.%2C%20what%20to%0Aevaluate%29%2C%20adaptation%20%28i.e.%2C%20how%20to%20obtain%20the%20results%29%2C%20metric%20%28i.e.%2C%20how%20to%0Ameasure%20the%20results%29%2C%20and%20error%20%28i.e.%2C%20how%20to%20analyze%20the%20bad%20results%29.%20This%0Ataxonomy%20defines%20a%20broad%20evaluation%20design%20space%20while%20systematically%20selecting%0Acriteria%20and%20priorities.%20Second%2C%20we%20compose%20the%20CaLM%20dataset%2C%20comprising%0A126%2C334%20data%20samples%2C%20to%20provide%20curated%20sets%20of%20causal%20targets%2C%20adaptations%2C%0Ametrics%2C%20and%20errors%2C%20offering%20extensive%20coverage%20for%20diverse%20research%20pursuits.%0AThird%2C%20we%20conduct%20an%20extensive%20evaluation%20of%2028%20leading%20language%20models%20on%20a%0Acore%20set%20of%2092%20causal%20targets%2C%209%20adaptations%2C%207%20metrics%2C%20and%2012%20error%20types.%0AFourth%2C%20we%20perform%20detailed%20analyses%20of%20the%20evaluation%20results%20across%20various%0Adimensions%20%28e.g.%2C%20adaptation%2C%20scale%29.%20Fifth%2C%20we%20present%2050%20high-level%20empirical%0Afindings%20across%209%20dimensions%20%28e.g.%2C%20model%29%2C%20providing%20valuable%20guidance%20for%0Afuture%20language%20model%20development.%20Finally%2C%20we%20develop%20a%20multifaceted%20platform%2C%0Aincluding%20a%20website%2C%20leaderboards%2C%20datasets%2C%20and%20toolkits%2C%20to%20support%20scalable%0Aand%20adaptable%20assessments.%20We%20envision%20CaLM%20as%20an%20ever-evolving%20benchmark%20for%0Athe%20community%2C%20systematically%20updated%20with%20new%20causal%20targets%2C%20adaptations%2C%0Amodels%2C%20metrics%2C%20and%20error%20types%20to%20reflect%20ongoing%20research%20advancements.%0AProject%20website%20is%20at%20https%3A//opencausalab.github.io/CaLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00622v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Evaluation%20of%20Language%20Models&entry.906535625=Sirui%20Chen%20and%20Bo%20Peng%20and%20Meiqi%20Chen%20and%20Ruiqi%20Wang%20and%20Mengying%20Xu%20and%20Xingyu%20Zeng%20and%20Rui%20Zhao%20and%20Shengjie%20Zhao%20and%20Yu%20Qiao%20and%20Chaochao%20Lu&entry.1292438233=%20%20Causal%20reasoning%20is%20viewed%20as%20crucial%20for%20achieving%20human-level%20machine%0Aintelligence.%20Recent%20advances%20in%20language%20models%20have%20expanded%20the%20horizons%20of%0Aartificial%20intelligence%20across%20various%20domains%2C%20sparking%20inquiries%20into%20their%0Apotential%20for%20causal%20reasoning.%20In%20this%20work%2C%20we%20introduce%20Causal%20evaluation%20of%0ALanguage%20Models%20%28CaLM%29%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%0Acomprehensive%20benchmark%20for%20evaluating%20the%20causal%20reasoning%20capabilities%20of%0Alanguage%20models.%20First%2C%20we%20propose%20the%20CaLM%20framework%2C%20which%20establishes%20a%0Afoundational%20taxonomy%20consisting%20of%20four%20modules%3A%20causal%20target%20%28i.e.%2C%20what%20to%0Aevaluate%29%2C%20adaptation%20%28i.e.%2C%20how%20to%20obtain%20the%20results%29%2C%20metric%20%28i.e.%2C%20how%20to%0Ameasure%20the%20results%29%2C%20and%20error%20%28i.e.%2C%20how%20to%20analyze%20the%20bad%20results%29.%20This%0Ataxonomy%20defines%20a%20broad%20evaluation%20design%20space%20while%20systematically%20selecting%0Acriteria%20and%20priorities.%20Second%2C%20we%20compose%20the%20CaLM%20dataset%2C%20comprising%0A126%2C334%20data%20samples%2C%20to%20provide%20curated%20sets%20of%20causal%20targets%2C%20adaptations%2C%0Ametrics%2C%20and%20errors%2C%20offering%20extensive%20coverage%20for%20diverse%20research%20pursuits.%0AThird%2C%20we%20conduct%20an%20extensive%20evaluation%20of%2028%20leading%20language%20models%20on%20a%0Acore%20set%20of%2092%20causal%20targets%2C%209%20adaptations%2C%207%20metrics%2C%20and%2012%20error%20types.%0AFourth%2C%20we%20perform%20detailed%20analyses%20of%20the%20evaluation%20results%20across%20various%0Adimensions%20%28e.g.%2C%20adaptation%2C%20scale%29.%20Fifth%2C%20we%20present%2050%20high-level%20empirical%0Afindings%20across%209%20dimensions%20%28e.g.%2C%20model%29%2C%20providing%20valuable%20guidance%20for%0Afuture%20language%20model%20development.%20Finally%2C%20we%20develop%20a%20multifaceted%20platform%2C%0Aincluding%20a%20website%2C%20leaderboards%2C%20datasets%2C%20and%20toolkits%2C%20to%20support%20scalable%0Aand%20adaptable%20assessments.%20We%20envision%20CaLM%20as%20an%20ever-evolving%20benchmark%20for%0Athe%20community%2C%20systematically%20updated%20with%20new%20causal%20targets%2C%20adaptations%2C%0Amodels%2C%20metrics%2C%20and%20error%20types%20to%20reflect%20ongoing%20research%20advancements.%0AProject%20website%20is%20at%20https%3A//opencausalab.github.io/CaLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00622v1&entry.124074799=Read"},
{"title": "Conformal Risk Control for Ordinal Classification", "author": "Yunpeng Xu and Wenge Guo and Zhi Wei", "abstract": "  As a natural extension to the standard conformal prediction method, several\nconformal risk control methods have been recently developed and applied to\nvarious learning problems. In this work, we seek to control the conformal risk\nin expectation for ordinal classification tasks, which have broad applications\nto many real problems. For this purpose, we firstly formulated the ordinal\nclassification task in the conformal risk control framework, and provided\ntheoretic risk bounds of the risk control method. Then we proposed two types of\nloss functions specially designed for ordinal classification tasks, and\ndeveloped corresponding algorithms to determine the prediction set for each\ncase to control their risks at a desired level. We demonstrated the\neffectiveness of our proposed methods, and analyzed the difference between the\ntwo types of risks on three different datasets, including a simulated dataset,\nthe UTKFace dataset and the diabetic retinopathy detection dataset.\n", "link": "http://arxiv.org/abs/2405.00417v1", "date": "2024-05-01", "relevancy": 1.8476, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.478}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4709}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Conformal%20Risk%20Control%20for%20Ordinal%20Classification&body=Title%3A%20Conformal%20Risk%20Control%20for%20Ordinal%20Classification%0AAuthor%3A%20Yunpeng%20Xu%20and%20Wenge%20Guo%20and%20Zhi%20Wei%0AAbstract%3A%20%20%20As%20a%20natural%20extension%20to%20the%20standard%20conformal%20prediction%20method%2C%20several%0Aconformal%20risk%20control%20methods%20have%20been%20recently%20developed%20and%20applied%20to%0Avarious%20learning%20problems.%20In%20this%20work%2C%20we%20seek%20to%20control%20the%20conformal%20risk%0Ain%20expectation%20for%20ordinal%20classification%20tasks%2C%20which%20have%20broad%20applications%0Ato%20many%20real%20problems.%20For%20this%20purpose%2C%20we%20firstly%20formulated%20the%20ordinal%0Aclassification%20task%20in%20the%20conformal%20risk%20control%20framework%2C%20and%20provided%0Atheoretic%20risk%20bounds%20of%20the%20risk%20control%20method.%20Then%20we%20proposed%20two%20types%20of%0Aloss%20functions%20specially%20designed%20for%20ordinal%20classification%20tasks%2C%20and%0Adeveloped%20corresponding%20algorithms%20to%20determine%20the%20prediction%20set%20for%20each%0Acase%20to%20control%20their%20risks%20at%20a%20desired%20level.%20We%20demonstrated%20the%0Aeffectiveness%20of%20our%20proposed%20methods%2C%20and%20analyzed%20the%20difference%20between%20the%0Atwo%20types%20of%20risks%20on%20three%20different%20datasets%2C%20including%20a%20simulated%20dataset%2C%0Athe%20UTKFace%20dataset%20and%20the%20diabetic%20retinopathy%20detection%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00417v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Risk%20Control%20for%20Ordinal%20Classification&entry.906535625=Yunpeng%20Xu%20and%20Wenge%20Guo%20and%20Zhi%20Wei&entry.1292438233=%20%20As%20a%20natural%20extension%20to%20the%20standard%20conformal%20prediction%20method%2C%20several%0Aconformal%20risk%20control%20methods%20have%20been%20recently%20developed%20and%20applied%20to%0Avarious%20learning%20problems.%20In%20this%20work%2C%20we%20seek%20to%20control%20the%20conformal%20risk%0Ain%20expectation%20for%20ordinal%20classification%20tasks%2C%20which%20have%20broad%20applications%0Ato%20many%20real%20problems.%20For%20this%20purpose%2C%20we%20firstly%20formulated%20the%20ordinal%0Aclassification%20task%20in%20the%20conformal%20risk%20control%20framework%2C%20and%20provided%0Atheoretic%20risk%20bounds%20of%20the%20risk%20control%20method.%20Then%20we%20proposed%20two%20types%20of%0Aloss%20functions%20specially%20designed%20for%20ordinal%20classification%20tasks%2C%20and%0Adeveloped%20corresponding%20algorithms%20to%20determine%20the%20prediction%20set%20for%20each%0Acase%20to%20control%20their%20risks%20at%20a%20desired%20level.%20We%20demonstrated%20the%0Aeffectiveness%20of%20our%20proposed%20methods%2C%20and%20analyzed%20the%20difference%20between%20the%0Atwo%20types%20of%20risks%20on%20three%20different%20datasets%2C%20including%20a%20simulated%20dataset%2C%0Athe%20UTKFace%20dataset%20and%20the%20diabetic%20retinopathy%20detection%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00417v1&entry.124074799=Read"},
{"title": "Shifting Focus with HCEye: Exploring the Dynamics of Visual Highlighting\n  and Cognitive Load on User Attention and Saliency Prediction", "author": "Anwesha Das and Zekun Wu and Iza \u0160krjanec and Anna Maria Feit", "abstract": "  Visual highlighting can guide user attention in complex interfaces. However,\nits effectiveness under limited attentional capacities is underexplored. This\npaper examines the joint impact of visual highlighting (permanent and dynamic)\nand dual-task-induced cognitive load on gaze behaviour. Our analysis, using\neye-movement data from 27 participants viewing 150 unique webpages reveals that\nwhile participants' ability to attend to UI elements decreases with increasing\ncognitive load, dynamic adaptations (i.e., highlighting) remain\nattention-grabbing. The presence of these factors significantly alters what\npeople attend to and thus what is salient. Accordingly, we show that\nstate-of-the-art saliency models increase their performance when accounting for\ndifferent cognitive loads. Our empirical insights, along with our openly\navailable dataset, enhance our understanding of attentional processes in UIs\nunder varying cognitive (and perceptual) loads and open the door for new models\nthat can predict user attention while multitasking.\n", "link": "http://arxiv.org/abs/2404.14232v2", "date": "2024-05-01", "relevancy": 1.8451, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4879}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4601}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Shifting%20Focus%20with%20HCEye%3A%20Exploring%20the%20Dynamics%20of%20Visual%20Highlighting%0A%20%20and%20Cognitive%20Load%20on%20User%20Attention%20and%20Saliency%20Prediction&body=Title%3A%20Shifting%20Focus%20with%20HCEye%3A%20Exploring%20the%20Dynamics%20of%20Visual%20Highlighting%0A%20%20and%20Cognitive%20Load%20on%20User%20Attention%20and%20Saliency%20Prediction%0AAuthor%3A%20Anwesha%20Das%20and%20Zekun%20Wu%20and%20Iza%20%C5%A0krjanec%20and%20Anna%20Maria%20Feit%0AAbstract%3A%20%20%20Visual%20highlighting%20can%20guide%20user%20attention%20in%20complex%20interfaces.%20However%2C%0Aits%20effectiveness%20under%20limited%20attentional%20capacities%20is%20underexplored.%20This%0Apaper%20examines%20the%20joint%20impact%20of%20visual%20highlighting%20%28permanent%20and%20dynamic%29%0Aand%20dual-task-induced%20cognitive%20load%20on%20gaze%20behaviour.%20Our%20analysis%2C%20using%0Aeye-movement%20data%20from%2027%20participants%20viewing%20150%20unique%20webpages%20reveals%20that%0Awhile%20participants%27%20ability%20to%20attend%20to%20UI%20elements%20decreases%20with%20increasing%0Acognitive%20load%2C%20dynamic%20adaptations%20%28i.e.%2C%20highlighting%29%20remain%0Aattention-grabbing.%20The%20presence%20of%20these%20factors%20significantly%20alters%20what%0Apeople%20attend%20to%20and%20thus%20what%20is%20salient.%20Accordingly%2C%20we%20show%20that%0Astate-of-the-art%20saliency%20models%20increase%20their%20performance%20when%20accounting%20for%0Adifferent%20cognitive%20loads.%20Our%20empirical%20insights%2C%20along%20with%20our%20openly%0Aavailable%20dataset%2C%20enhance%20our%20understanding%20of%20attentional%20processes%20in%20UIs%0Aunder%20varying%20cognitive%20%28and%20perceptual%29%20loads%20and%20open%20the%20door%20for%20new%20models%0Athat%20can%20predict%20user%20attention%20while%20multitasking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14232v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shifting%20Focus%20with%20HCEye%3A%20Exploring%20the%20Dynamics%20of%20Visual%20Highlighting%0A%20%20and%20Cognitive%20Load%20on%20User%20Attention%20and%20Saliency%20Prediction&entry.906535625=Anwesha%20Das%20and%20Zekun%20Wu%20and%20Iza%20%C5%A0krjanec%20and%20Anna%20Maria%20Feit&entry.1292438233=%20%20Visual%20highlighting%20can%20guide%20user%20attention%20in%20complex%20interfaces.%20However%2C%0Aits%20effectiveness%20under%20limited%20attentional%20capacities%20is%20underexplored.%20This%0Apaper%20examines%20the%20joint%20impact%20of%20visual%20highlighting%20%28permanent%20and%20dynamic%29%0Aand%20dual-task-induced%20cognitive%20load%20on%20gaze%20behaviour.%20Our%20analysis%2C%20using%0Aeye-movement%20data%20from%2027%20participants%20viewing%20150%20unique%20webpages%20reveals%20that%0Awhile%20participants%27%20ability%20to%20attend%20to%20UI%20elements%20decreases%20with%20increasing%0Acognitive%20load%2C%20dynamic%20adaptations%20%28i.e.%2C%20highlighting%29%20remain%0Aattention-grabbing.%20The%20presence%20of%20these%20factors%20significantly%20alters%20what%0Apeople%20attend%20to%20and%20thus%20what%20is%20salient.%20Accordingly%2C%20we%20show%20that%0Astate-of-the-art%20saliency%20models%20increase%20their%20performance%20when%20accounting%20for%0Adifferent%20cognitive%20loads.%20Our%20empirical%20insights%2C%20along%20with%20our%20openly%0Aavailable%20dataset%2C%20enhance%20our%20understanding%20of%20attentional%20processes%20in%20UIs%0Aunder%20varying%20cognitive%20%28and%20perceptual%29%20loads%20and%20open%20the%20door%20for%20new%20models%0Athat%20can%20predict%20user%20attention%20while%20multitasking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14232v2&entry.124074799=Read"},
{"title": "Explainable Automatic Grading with Neural Additive Models", "author": "Aubrey Condor and Zachary Pardos", "abstract": "  The use of automatic short answer grading (ASAG) models may help alleviate\nthe time burden of grading while encouraging educators to frequently\nincorporate open-ended items in their curriculum. However, current\nstate-of-the-art ASAG models are large neural networks (NN) often described as\n\"black box\", providing no explanation for which characteristics of an input are\nimportant for the produced output. This inexplicable nature can be frustrating\nto teachers and students when trying to interpret, or learn from an\nautomatically-generated grade. To create a powerful yet intelligible ASAG\nmodel, we experiment with a type of model called a Neural Additive Model that\ncombines the performance of a NN with the explainability of an additive model.\nWe use a Knowledge Integration (KI) framework from the learning sciences to\nguide feature engineering to create inputs that reflect whether a student\nincludes certain ideas in their response. We hypothesize that indicating the\ninclusion (or exclusion) of predefined ideas as features will be sufficient for\nthe NAM to have good predictive power and interpretability, as this may guide a\nhuman scorer using a KI rubric. We compare the performance of the NAM with\nanother explainable model, logistic regression, using the same features, and to\na non-explainable neural model, DeBERTa, that does not require feature\nengineering.\n", "link": "http://arxiv.org/abs/2405.00489v1", "date": "2024-05-01", "relevancy": 1.8442, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4705}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4592}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4523}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Explainable%20Automatic%20Grading%20with%20Neural%20Additive%20Models&body=Title%3A%20Explainable%20Automatic%20Grading%20with%20Neural%20Additive%20Models%0AAuthor%3A%20Aubrey%20Condor%20and%20Zachary%20Pardos%0AAbstract%3A%20%20%20The%20use%20of%20automatic%20short%20answer%20grading%20%28ASAG%29%20models%20may%20help%20alleviate%0Athe%20time%20burden%20of%20grading%20while%20encouraging%20educators%20to%20frequently%0Aincorporate%20open-ended%20items%20in%20their%20curriculum.%20However%2C%20current%0Astate-of-the-art%20ASAG%20models%20are%20large%20neural%20networks%20%28NN%29%20often%20described%20as%0A%22black%20box%22%2C%20providing%20no%20explanation%20for%20which%20characteristics%20of%20an%20input%20are%0Aimportant%20for%20the%20produced%20output.%20This%20inexplicable%20nature%20can%20be%20frustrating%0Ato%20teachers%20and%20students%20when%20trying%20to%20interpret%2C%20or%20learn%20from%20an%0Aautomatically-generated%20grade.%20To%20create%20a%20powerful%20yet%20intelligible%20ASAG%0Amodel%2C%20we%20experiment%20with%20a%20type%20of%20model%20called%20a%20Neural%20Additive%20Model%20that%0Acombines%20the%20performance%20of%20a%20NN%20with%20the%20explainability%20of%20an%20additive%20model.%0AWe%20use%20a%20Knowledge%20Integration%20%28KI%29%20framework%20from%20the%20learning%20sciences%20to%0Aguide%20feature%20engineering%20to%20create%20inputs%20that%20reflect%20whether%20a%20student%0Aincludes%20certain%20ideas%20in%20their%20response.%20We%20hypothesize%20that%20indicating%20the%0Ainclusion%20%28or%20exclusion%29%20of%20predefined%20ideas%20as%20features%20will%20be%20sufficient%20for%0Athe%20NAM%20to%20have%20good%20predictive%20power%20and%20interpretability%2C%20as%20this%20may%20guide%20a%0Ahuman%20scorer%20using%20a%20KI%20rubric.%20We%20compare%20the%20performance%20of%20the%20NAM%20with%0Aanother%20explainable%20model%2C%20logistic%20regression%2C%20using%20the%20same%20features%2C%20and%20to%0Aa%20non-explainable%20neural%20model%2C%20DeBERTa%2C%20that%20does%20not%20require%20feature%0Aengineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00489v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Automatic%20Grading%20with%20Neural%20Additive%20Models&entry.906535625=Aubrey%20Condor%20and%20Zachary%20Pardos&entry.1292438233=%20%20The%20use%20of%20automatic%20short%20answer%20grading%20%28ASAG%29%20models%20may%20help%20alleviate%0Athe%20time%20burden%20of%20grading%20while%20encouraging%20educators%20to%20frequently%0Aincorporate%20open-ended%20items%20in%20their%20curriculum.%20However%2C%20current%0Astate-of-the-art%20ASAG%20models%20are%20large%20neural%20networks%20%28NN%29%20often%20described%20as%0A%22black%20box%22%2C%20providing%20no%20explanation%20for%20which%20characteristics%20of%20an%20input%20are%0Aimportant%20for%20the%20produced%20output.%20This%20inexplicable%20nature%20can%20be%20frustrating%0Ato%20teachers%20and%20students%20when%20trying%20to%20interpret%2C%20or%20learn%20from%20an%0Aautomatically-generated%20grade.%20To%20create%20a%20powerful%20yet%20intelligible%20ASAG%0Amodel%2C%20we%20experiment%20with%20a%20type%20of%20model%20called%20a%20Neural%20Additive%20Model%20that%0Acombines%20the%20performance%20of%20a%20NN%20with%20the%20explainability%20of%20an%20additive%20model.%0AWe%20use%20a%20Knowledge%20Integration%20%28KI%29%20framework%20from%20the%20learning%20sciences%20to%0Aguide%20feature%20engineering%20to%20create%20inputs%20that%20reflect%20whether%20a%20student%0Aincludes%20certain%20ideas%20in%20their%20response.%20We%20hypothesize%20that%20indicating%20the%0Ainclusion%20%28or%20exclusion%29%20of%20predefined%20ideas%20as%20features%20will%20be%20sufficient%20for%0Athe%20NAM%20to%20have%20good%20predictive%20power%20and%20interpretability%2C%20as%20this%20may%20guide%20a%0Ahuman%20scorer%20using%20a%20KI%20rubric.%20We%20compare%20the%20performance%20of%20the%20NAM%20with%0Aanother%20explainable%20model%2C%20logistic%20regression%2C%20using%20the%20same%20features%2C%20and%20to%0Aa%20non-explainable%20neural%20model%2C%20DeBERTa%2C%20that%20does%20not%20require%20feature%0Aengineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00489v1&entry.124074799=Read"},
{"title": "PackVFL: Efficient HE Packing for Vertical Federated Learning", "author": "Liu Yang and Shuowei Cai and Di Chai and Junxue Zhang and Han Tian and Yilun Jin and Kun Guo and Kai Chen and Qiang Yang", "abstract": "  As an essential tool of secure distributed machine learning, vertical\nfederated learning (VFL) based on homomorphic encryption (HE) suffers from\nsevere efficiency problems due to data inflation and time-consuming operations.\nTo this core, we propose PackVFL, an efficient VFL framework based on packed HE\n(PackedHE), to accelerate the existing HE-based VFL algorithms. PackVFL packs\nmultiple cleartexts into one ciphertext and supports\nsingle-instruction-multiple-data (SIMD)-style parallelism. We focus on\ndesigning a high-performant matrix multiplication (MatMult) method since it\ntakes up most of the ciphertext computation time in HE-based VFL. Besides,\ndevising the MatMult method is also challenging for PackedHE because a slight\ndifference in the packing way could predominantly affect its computation and\ncommunication costs. Without domain-specific design, directly applying SOTA\nMatMult methods is hard to achieve optimal.\n  Therefore, we make a three-fold design: 1) we systematically explore the\ncurrent design space of MatMult and quantify the complexity of existing\napproaches to provide guidance; 2) we propose a hybrid MatMult method according\nto the unique characteristics of VFL; 3) we adaptively apply our hybrid method\nin representative VFL algorithms, leveraging distinctive algorithmic properties\nto further improve efficiency. As the batch size, feature dimension and model\nsize of VFL scale up to large sizes, PackVFL consistently delivers enhanced\nperformance. Empirically, PackVFL propels existing VFL algorithms to new\nheights, achieving up to a 51.52X end-to-end speedup. This represents a\nsubstantial 34.51X greater speedup compared to the direct application of SOTA\nMatMult methods.\n", "link": "http://arxiv.org/abs/2405.00482v1", "date": "2024-05-01", "relevancy": 1.8343, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.457}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4355}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PackVFL%3A%20Efficient%20HE%20Packing%20for%20Vertical%20Federated%20Learning&body=Title%3A%20PackVFL%3A%20Efficient%20HE%20Packing%20for%20Vertical%20Federated%20Learning%0AAuthor%3A%20Liu%20Yang%20and%20Shuowei%20Cai%20and%20Di%20Chai%20and%20Junxue%20Zhang%20and%20Han%20Tian%20and%20Yilun%20Jin%20and%20Kun%20Guo%20and%20Kai%20Chen%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20As%20an%20essential%20tool%20of%20secure%20distributed%20machine%20learning%2C%20vertical%0Afederated%20learning%20%28VFL%29%20based%20on%20homomorphic%20encryption%20%28HE%29%20suffers%20from%0Asevere%20efficiency%20problems%20due%20to%20data%20inflation%20and%20time-consuming%20operations.%0ATo%20this%20core%2C%20we%20propose%20PackVFL%2C%20an%20efficient%20VFL%20framework%20based%20on%20packed%20HE%0A%28PackedHE%29%2C%20to%20accelerate%20the%20existing%20HE-based%20VFL%20algorithms.%20PackVFL%20packs%0Amultiple%20cleartexts%20into%20one%20ciphertext%20and%20supports%0Asingle-instruction-multiple-data%20%28SIMD%29-style%20parallelism.%20We%20focus%20on%0Adesigning%20a%20high-performant%20matrix%20multiplication%20%28MatMult%29%20method%20since%20it%0Atakes%20up%20most%20of%20the%20ciphertext%20computation%20time%20in%20HE-based%20VFL.%20Besides%2C%0Adevising%20the%20MatMult%20method%20is%20also%20challenging%20for%20PackedHE%20because%20a%20slight%0Adifference%20in%20the%20packing%20way%20could%20predominantly%20affect%20its%20computation%20and%0Acommunication%20costs.%20Without%20domain-specific%20design%2C%20directly%20applying%20SOTA%0AMatMult%20methods%20is%20hard%20to%20achieve%20optimal.%0A%20%20Therefore%2C%20we%20make%20a%20three-fold%20design%3A%201%29%20we%20systematically%20explore%20the%0Acurrent%20design%20space%20of%20MatMult%20and%20quantify%20the%20complexity%20of%20existing%0Aapproaches%20to%20provide%20guidance%3B%202%29%20we%20propose%20a%20hybrid%20MatMult%20method%20according%0Ato%20the%20unique%20characteristics%20of%20VFL%3B%203%29%20we%20adaptively%20apply%20our%20hybrid%20method%0Ain%20representative%20VFL%20algorithms%2C%20leveraging%20distinctive%20algorithmic%20properties%0Ato%20further%20improve%20efficiency.%20As%20the%20batch%20size%2C%20feature%20dimension%20and%20model%0Asize%20of%20VFL%20scale%20up%20to%20large%20sizes%2C%20PackVFL%20consistently%20delivers%20enhanced%0Aperformance.%20Empirically%2C%20PackVFL%20propels%20existing%20VFL%20algorithms%20to%20new%0Aheights%2C%20achieving%20up%20to%20a%2051.52X%20end-to-end%20speedup.%20This%20represents%20a%0Asubstantial%2034.51X%20greater%20speedup%20compared%20to%20the%20direct%20application%20of%20SOTA%0AMatMult%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00482v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PackVFL%3A%20Efficient%20HE%20Packing%20for%20Vertical%20Federated%20Learning&entry.906535625=Liu%20Yang%20and%20Shuowei%20Cai%20and%20Di%20Chai%20and%20Junxue%20Zhang%20and%20Han%20Tian%20and%20Yilun%20Jin%20and%20Kun%20Guo%20and%20Kai%20Chen%20and%20Qiang%20Yang&entry.1292438233=%20%20As%20an%20essential%20tool%20of%20secure%20distributed%20machine%20learning%2C%20vertical%0Afederated%20learning%20%28VFL%29%20based%20on%20homomorphic%20encryption%20%28HE%29%20suffers%20from%0Asevere%20efficiency%20problems%20due%20to%20data%20inflation%20and%20time-consuming%20operations.%0ATo%20this%20core%2C%20we%20propose%20PackVFL%2C%20an%20efficient%20VFL%20framework%20based%20on%20packed%20HE%0A%28PackedHE%29%2C%20to%20accelerate%20the%20existing%20HE-based%20VFL%20algorithms.%20PackVFL%20packs%0Amultiple%20cleartexts%20into%20one%20ciphertext%20and%20supports%0Asingle-instruction-multiple-data%20%28SIMD%29-style%20parallelism.%20We%20focus%20on%0Adesigning%20a%20high-performant%20matrix%20multiplication%20%28MatMult%29%20method%20since%20it%0Atakes%20up%20most%20of%20the%20ciphertext%20computation%20time%20in%20HE-based%20VFL.%20Besides%2C%0Adevising%20the%20MatMult%20method%20is%20also%20challenging%20for%20PackedHE%20because%20a%20slight%0Adifference%20in%20the%20packing%20way%20could%20predominantly%20affect%20its%20computation%20and%0Acommunication%20costs.%20Without%20domain-specific%20design%2C%20directly%20applying%20SOTA%0AMatMult%20methods%20is%20hard%20to%20achieve%20optimal.%0A%20%20Therefore%2C%20we%20make%20a%20three-fold%20design%3A%201%29%20we%20systematically%20explore%20the%0Acurrent%20design%20space%20of%20MatMult%20and%20quantify%20the%20complexity%20of%20existing%0Aapproaches%20to%20provide%20guidance%3B%202%29%20we%20propose%20a%20hybrid%20MatMult%20method%20according%0Ato%20the%20unique%20characteristics%20of%20VFL%3B%203%29%20we%20adaptively%20apply%20our%20hybrid%20method%0Ain%20representative%20VFL%20algorithms%2C%20leveraging%20distinctive%20algorithmic%20properties%0Ato%20further%20improve%20efficiency.%20As%20the%20batch%20size%2C%20feature%20dimension%20and%20model%0Asize%20of%20VFL%20scale%20up%20to%20large%20sizes%2C%20PackVFL%20consistently%20delivers%20enhanced%0Aperformance.%20Empirically%2C%20PackVFL%20propels%20existing%20VFL%20algorithms%20to%20new%0Aheights%2C%20achieving%20up%20to%20a%2051.52X%20end-to-end%20speedup.%20This%20represents%20a%0Asubstantial%2034.51X%20greater%20speedup%20compared%20to%20the%20direct%20application%20of%20SOTA%0AMatMult%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00482v1&entry.124074799=Read"},
{"title": "Queue-based Eco-Driving at Roundabouts with Reinforcement Learning", "author": "Anna-Lena Schlamp and Werner Huber and Stefanie Schmidtner", "abstract": "  We address eco-driving at roundabouts in mixed traffic to enhance traffic\nflow and traffic efficiency in urban areas. The aim is to proactively optimize\nspeed of automated or non-automated connected vehicles (CVs), ensuring both an\nefficient approach and smooth entry into roundabouts. We incorporate the\ntraffic situation ahead, i.e. preceding vehicles and waiting queues. Further,\nwe develop two approaches: a rule-based and an Reinforcement Learning (RL)\nbased eco-driving system, with both using the approach link and information\nfrom conflicting CVs for speed optimization. A fair comparison of rule-based\nand RL-based approaches is performed to explore RL as a viable alternative to\nclassical optimization. Results show that both approaches outperform the\nbaseline. Improvements significantly increase with growing traffic volumes,\nleading to best results on average being obtained at high volumes. Near\ncapacity, performance deteriorates, indicating limited applicability at\ncapacity limits. Examining different CV penetration rates, a decline in\nperformance is observed, but with substantial results still being achieved at\nlower CV rates. RL agents can discover effective policies for speed\noptimization in dynamic roundabout settings, but they do not offer a\nsubstantial advantage over classical approaches, especially at higher traffic\nvolumes or lower CV penetration rates.\n", "link": "http://arxiv.org/abs/2405.00625v1", "date": "2024-05-01", "relevancy": 1.817, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4659}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4554}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4422}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Queue-based%20Eco-Driving%20at%20Roundabouts%20with%20Reinforcement%20Learning&body=Title%3A%20Queue-based%20Eco-Driving%20at%20Roundabouts%20with%20Reinforcement%20Learning%0AAuthor%3A%20Anna-Lena%20Schlamp%20and%20Werner%20Huber%20and%20Stefanie%20Schmidtner%0AAbstract%3A%20%20%20We%20address%20eco-driving%20at%20roundabouts%20in%20mixed%20traffic%20to%20enhance%20traffic%0Aflow%20and%20traffic%20efficiency%20in%20urban%20areas.%20The%20aim%20is%20to%20proactively%20optimize%0Aspeed%20of%20automated%20or%20non-automated%20connected%20vehicles%20%28CVs%29%2C%20ensuring%20both%20an%0Aefficient%20approach%20and%20smooth%20entry%20into%20roundabouts.%20We%20incorporate%20the%0Atraffic%20situation%20ahead%2C%20i.e.%20preceding%20vehicles%20and%20waiting%20queues.%20Further%2C%0Awe%20develop%20two%20approaches%3A%20a%20rule-based%20and%20an%20Reinforcement%20Learning%20%28RL%29%0Abased%20eco-driving%20system%2C%20with%20both%20using%20the%20approach%20link%20and%20information%0Afrom%20conflicting%20CVs%20for%20speed%20optimization.%20A%20fair%20comparison%20of%20rule-based%0Aand%20RL-based%20approaches%20is%20performed%20to%20explore%20RL%20as%20a%20viable%20alternative%20to%0Aclassical%20optimization.%20Results%20show%20that%20both%20approaches%20outperform%20the%0Abaseline.%20Improvements%20significantly%20increase%20with%20growing%20traffic%20volumes%2C%0Aleading%20to%20best%20results%20on%20average%20being%20obtained%20at%20high%20volumes.%20Near%0Acapacity%2C%20performance%20deteriorates%2C%20indicating%20limited%20applicability%20at%0Acapacity%20limits.%20Examining%20different%20CV%20penetration%20rates%2C%20a%20decline%20in%0Aperformance%20is%20observed%2C%20but%20with%20substantial%20results%20still%20being%20achieved%20at%0Alower%20CV%20rates.%20RL%20agents%20can%20discover%20effective%20policies%20for%20speed%0Aoptimization%20in%20dynamic%20roundabout%20settings%2C%20but%20they%20do%20not%20offer%20a%0Asubstantial%20advantage%20over%20classical%20approaches%2C%20especially%20at%20higher%20traffic%0Avolumes%20or%20lower%20CV%20penetration%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00625v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Queue-based%20Eco-Driving%20at%20Roundabouts%20with%20Reinforcement%20Learning&entry.906535625=Anna-Lena%20Schlamp%20and%20Werner%20Huber%20and%20Stefanie%20Schmidtner&entry.1292438233=%20%20We%20address%20eco-driving%20at%20roundabouts%20in%20mixed%20traffic%20to%20enhance%20traffic%0Aflow%20and%20traffic%20efficiency%20in%20urban%20areas.%20The%20aim%20is%20to%20proactively%20optimize%0Aspeed%20of%20automated%20or%20non-automated%20connected%20vehicles%20%28CVs%29%2C%20ensuring%20both%20an%0Aefficient%20approach%20and%20smooth%20entry%20into%20roundabouts.%20We%20incorporate%20the%0Atraffic%20situation%20ahead%2C%20i.e.%20preceding%20vehicles%20and%20waiting%20queues.%20Further%2C%0Awe%20develop%20two%20approaches%3A%20a%20rule-based%20and%20an%20Reinforcement%20Learning%20%28RL%29%0Abased%20eco-driving%20system%2C%20with%20both%20using%20the%20approach%20link%20and%20information%0Afrom%20conflicting%20CVs%20for%20speed%20optimization.%20A%20fair%20comparison%20of%20rule-based%0Aand%20RL-based%20approaches%20is%20performed%20to%20explore%20RL%20as%20a%20viable%20alternative%20to%0Aclassical%20optimization.%20Results%20show%20that%20both%20approaches%20outperform%20the%0Abaseline.%20Improvements%20significantly%20increase%20with%20growing%20traffic%20volumes%2C%0Aleading%20to%20best%20results%20on%20average%20being%20obtained%20at%20high%20volumes.%20Near%0Acapacity%2C%20performance%20deteriorates%2C%20indicating%20limited%20applicability%20at%0Acapacity%20limits.%20Examining%20different%20CV%20penetration%20rates%2C%20a%20decline%20in%0Aperformance%20is%20observed%2C%20but%20with%20substantial%20results%20still%20being%20achieved%20at%0Alower%20CV%20rates.%20RL%20agents%20can%20discover%20effective%20policies%20for%20speed%0Aoptimization%20in%20dynamic%20roundabout%20settings%2C%20but%20they%20do%20not%20offer%20a%0Asubstantial%20advantage%20over%20classical%20approaches%2C%20especially%20at%20higher%20traffic%0Avolumes%20or%20lower%20CV%20penetration%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00625v1&entry.124074799=Read"},
{"title": "No Representation, No Trust: Connecting Representation, Collapse, and\n  Trust Issues in PPO", "author": "Skander Moalla and Andrea Miele and Razvan Pascanu and Caglar Gulcehre", "abstract": "  Reinforcement learning (RL) is inherently rife with non-stationarity since\nthe states and rewards the agent observes during training depend on its\nchanging policy. Therefore, networks in deep RL must be capable of adapting to\nnew observations and fitting new targets. However, previous works have observed\nthat networks in off-policy deep value-based methods exhibit a decrease in\nrepresentation rank, often correlated with an inability to continue learning or\na collapse in performance. Although this phenomenon has generally been\nattributed to neural network learning under non-stationarity, it has been\noverlooked in on-policy policy optimization methods which are often thought\ncapable of training indefinitely. In this work, we empirically study\nrepresentation dynamics in Proximal Policy Optimization (PPO) on the Atari and\nMuJoCo environments, revealing that PPO agents are also affected by feature\nrank deterioration and loss of plasticity. We show that this is aggravated with\nstronger non-stationarity, ultimately driving the actor's performance to\ncollapse, regardless of the performance of the critic. We draw connections\nbetween representation collapse, performance collapse, and trust region issues\nin PPO, and present Proximal Feature Optimization (PFO), a novel auxiliary\nloss, that along with other interventions shows that regularizing the\nrepresentation dynamics improves the performance of PPO agents.\n", "link": "http://arxiv.org/abs/2405.00662v1", "date": "2024-05-01", "relevancy": 1.815, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4652}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4574}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20No%20Representation%2C%20No%20Trust%3A%20Connecting%20Representation%2C%20Collapse%2C%20and%0A%20%20Trust%20Issues%20in%20PPO&body=Title%3A%20No%20Representation%2C%20No%20Trust%3A%20Connecting%20Representation%2C%20Collapse%2C%20and%0A%20%20Trust%20Issues%20in%20PPO%0AAuthor%3A%20Skander%20Moalla%20and%20Andrea%20Miele%20and%20Razvan%20Pascanu%20and%20Caglar%20Gulcehre%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20is%20inherently%20rife%20with%20non-stationarity%20since%0Athe%20states%20and%20rewards%20the%20agent%20observes%20during%20training%20depend%20on%20its%0Achanging%20policy.%20Therefore%2C%20networks%20in%20deep%20RL%20must%20be%20capable%20of%20adapting%20to%0Anew%20observations%20and%20fitting%20new%20targets.%20However%2C%20previous%20works%20have%20observed%0Athat%20networks%20in%20off-policy%20deep%20value-based%20methods%20exhibit%20a%20decrease%20in%0Arepresentation%20rank%2C%20often%20correlated%20with%20an%20inability%20to%20continue%20learning%20or%0Aa%20collapse%20in%20performance.%20Although%20this%20phenomenon%20has%20generally%20been%0Aattributed%20to%20neural%20network%20learning%20under%20non-stationarity%2C%20it%20has%20been%0Aoverlooked%20in%20on-policy%20policy%20optimization%20methods%20which%20are%20often%20thought%0Acapable%20of%20training%20indefinitely.%20In%20this%20work%2C%20we%20empirically%20study%0Arepresentation%20dynamics%20in%20Proximal%20Policy%20Optimization%20%28PPO%29%20on%20the%20Atari%20and%0AMuJoCo%20environments%2C%20revealing%20that%20PPO%20agents%20are%20also%20affected%20by%20feature%0Arank%20deterioration%20and%20loss%20of%20plasticity.%20We%20show%20that%20this%20is%20aggravated%20with%0Astronger%20non-stationarity%2C%20ultimately%20driving%20the%20actor%27s%20performance%20to%0Acollapse%2C%20regardless%20of%20the%20performance%20of%20the%20critic.%20We%20draw%20connections%0Abetween%20representation%20collapse%2C%20performance%20collapse%2C%20and%20trust%20region%20issues%0Ain%20PPO%2C%20and%20present%20Proximal%20Feature%20Optimization%20%28PFO%29%2C%20a%20novel%20auxiliary%0Aloss%2C%20that%20along%20with%20other%20interventions%20shows%20that%20regularizing%20the%0Arepresentation%20dynamics%20improves%20the%20performance%20of%20PPO%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00662v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Representation%2C%20No%20Trust%3A%20Connecting%20Representation%2C%20Collapse%2C%20and%0A%20%20Trust%20Issues%20in%20PPO&entry.906535625=Skander%20Moalla%20and%20Andrea%20Miele%20and%20Razvan%20Pascanu%20and%20Caglar%20Gulcehre&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20is%20inherently%20rife%20with%20non-stationarity%20since%0Athe%20states%20and%20rewards%20the%20agent%20observes%20during%20training%20depend%20on%20its%0Achanging%20policy.%20Therefore%2C%20networks%20in%20deep%20RL%20must%20be%20capable%20of%20adapting%20to%0Anew%20observations%20and%20fitting%20new%20targets.%20However%2C%20previous%20works%20have%20observed%0Athat%20networks%20in%20off-policy%20deep%20value-based%20methods%20exhibit%20a%20decrease%20in%0Arepresentation%20rank%2C%20often%20correlated%20with%20an%20inability%20to%20continue%20learning%20or%0Aa%20collapse%20in%20performance.%20Although%20this%20phenomenon%20has%20generally%20been%0Aattributed%20to%20neural%20network%20learning%20under%20non-stationarity%2C%20it%20has%20been%0Aoverlooked%20in%20on-policy%20policy%20optimization%20methods%20which%20are%20often%20thought%0Acapable%20of%20training%20indefinitely.%20In%20this%20work%2C%20we%20empirically%20study%0Arepresentation%20dynamics%20in%20Proximal%20Policy%20Optimization%20%28PPO%29%20on%20the%20Atari%20and%0AMuJoCo%20environments%2C%20revealing%20that%20PPO%20agents%20are%20also%20affected%20by%20feature%0Arank%20deterioration%20and%20loss%20of%20plasticity.%20We%20show%20that%20this%20is%20aggravated%20with%0Astronger%20non-stationarity%2C%20ultimately%20driving%20the%20actor%27s%20performance%20to%0Acollapse%2C%20regardless%20of%20the%20performance%20of%20the%20critic.%20We%20draw%20connections%0Abetween%20representation%20collapse%2C%20performance%20collapse%2C%20and%20trust%20region%20issues%0Ain%20PPO%2C%20and%20present%20Proximal%20Feature%20Optimization%20%28PFO%29%2C%20a%20novel%20auxiliary%0Aloss%2C%20that%20along%20with%20other%20interventions%20shows%20that%20regularizing%20the%0Arepresentation%20dynamics%20improves%20the%20performance%20of%20PPO%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00662v1&entry.124074799=Read"},
{"title": "Towards Safe Large Language Models for Medicine", "author": "Tessa Han and Aounon Kumar and Chirag Agarwal and Himabindu Lakkaraju", "abstract": "  As large language models (LLMs) develop ever-improving capabilities and are\napplied in real-world settings, it is important to understand their safety.\nWhile initial steps have been taken to evaluate the safety of general-knowledge\nLLMs, exposing some weaknesses, the safety of medical LLMs has not been\nsufficiently evaluated despite their high risks to personal health and safety,\npublic health and safety, patient rights, and human rights. To address this\ngap, we conduct, to our knowledge, the first study of its kind to evaluate and\nimprove the safety of medical LLMs. We find that 1) current medical LLMs do not\nmeet standards of general or medical safety, as they readily comply with\nharmful requests and that 2) fine-tuning medical LLMs on safety demonstrations\nsignificantly improves their safety, reducing their tendency to comply with\nharmful requests. In addition, we present a definition of medical safety for\nLLMs and develop a benchmark dataset to evaluate and train for medical safety\nin LLMs. Poised at the intersection of research on machine learning safety and\nmedical machine learning, this work casts light on the status quo of the safety\nof medical LLMs and motivates future work in this area, mitigating the risks of\nharm of LLMs in medicine.\n", "link": "http://arxiv.org/abs/2403.03744v2", "date": "2024-05-01", "relevancy": 1.8122, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4432}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4312}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Safe%20Large%20Language%20Models%20for%20Medicine&body=Title%3A%20Towards%20Safe%20Large%20Language%20Models%20for%20Medicine%0AAuthor%3A%20Tessa%20Han%20and%20Aounon%20Kumar%20and%20Chirag%20Agarwal%20and%20Himabindu%20Lakkaraju%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20develop%20ever-improving%20capabilities%20and%20are%0Aapplied%20in%20real-world%20settings%2C%20it%20is%20important%20to%20understand%20their%20safety.%0AWhile%20initial%20steps%20have%20been%20taken%20to%20evaluate%20the%20safety%20of%20general-knowledge%0ALLMs%2C%20exposing%20some%20weaknesses%2C%20the%20safety%20of%20medical%20LLMs%20has%20not%20been%0Asufficiently%20evaluated%20despite%20their%20high%20risks%20to%20personal%20health%20and%20safety%2C%0Apublic%20health%20and%20safety%2C%20patient%20rights%2C%20and%20human%20rights.%20To%20address%20this%0Agap%2C%20we%20conduct%2C%20to%20our%20knowledge%2C%20the%20first%20study%20of%20its%20kind%20to%20evaluate%20and%0Aimprove%20the%20safety%20of%20medical%20LLMs.%20We%20find%20that%201%29%20current%20medical%20LLMs%20do%20not%0Ameet%20standards%20of%20general%20or%20medical%20safety%2C%20as%20they%20readily%20comply%20with%0Aharmful%20requests%20and%20that%202%29%20fine-tuning%20medical%20LLMs%20on%20safety%20demonstrations%0Asignificantly%20improves%20their%20safety%2C%20reducing%20their%20tendency%20to%20comply%20with%0Aharmful%20requests.%20In%20addition%2C%20we%20present%20a%20definition%20of%20medical%20safety%20for%0ALLMs%20and%20develop%20a%20benchmark%20dataset%20to%20evaluate%20and%20train%20for%20medical%20safety%0Ain%20LLMs.%20Poised%20at%20the%20intersection%20of%20research%20on%20machine%20learning%20safety%20and%0Amedical%20machine%20learning%2C%20this%20work%20casts%20light%20on%20the%20status%20quo%20of%20the%20safety%0Aof%20medical%20LLMs%20and%20motivates%20future%20work%20in%20this%20area%2C%20mitigating%20the%20risks%20of%0Aharm%20of%20LLMs%20in%20medicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03744v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Safe%20Large%20Language%20Models%20for%20Medicine&entry.906535625=Tessa%20Han%20and%20Aounon%20Kumar%20and%20Chirag%20Agarwal%20and%20Himabindu%20Lakkaraju&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20develop%20ever-improving%20capabilities%20and%20are%0Aapplied%20in%20real-world%20settings%2C%20it%20is%20important%20to%20understand%20their%20safety.%0AWhile%20initial%20steps%20have%20been%20taken%20to%20evaluate%20the%20safety%20of%20general-knowledge%0ALLMs%2C%20exposing%20some%20weaknesses%2C%20the%20safety%20of%20medical%20LLMs%20has%20not%20been%0Asufficiently%20evaluated%20despite%20their%20high%20risks%20to%20personal%20health%20and%20safety%2C%0Apublic%20health%20and%20safety%2C%20patient%20rights%2C%20and%20human%20rights.%20To%20address%20this%0Agap%2C%20we%20conduct%2C%20to%20our%20knowledge%2C%20the%20first%20study%20of%20its%20kind%20to%20evaluate%20and%0Aimprove%20the%20safety%20of%20medical%20LLMs.%20We%20find%20that%201%29%20current%20medical%20LLMs%20do%20not%0Ameet%20standards%20of%20general%20or%20medical%20safety%2C%20as%20they%20readily%20comply%20with%0Aharmful%20requests%20and%20that%202%29%20fine-tuning%20medical%20LLMs%20on%20safety%20demonstrations%0Asignificantly%20improves%20their%20safety%2C%20reducing%20their%20tendency%20to%20comply%20with%0Aharmful%20requests.%20In%20addition%2C%20we%20present%20a%20definition%20of%20medical%20safety%20for%0ALLMs%20and%20develop%20a%20benchmark%20dataset%20to%20evaluate%20and%20train%20for%20medical%20safety%0Ain%20LLMs.%20Poised%20at%20the%20intersection%20of%20research%20on%20machine%20learning%20safety%20and%0Amedical%20machine%20learning%2C%20this%20work%20casts%20light%20on%20the%20status%20quo%20of%20the%20safety%0Aof%20medical%20LLMs%20and%20motivates%20future%20work%20in%20this%20area%2C%20mitigating%20the%20risks%20of%0Aharm%20of%20LLMs%20in%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03744v2&entry.124074799=Read"},
{"title": "Capabilities of Gemini Models in Medicine", "author": "Khaled Saab and Tao Tu and Wei-Hung Weng and Ryutaro Tanno and David Stutz and Ellery Wulczyn and Fan Zhang and Tim Strother and Chunjong Park and Elahe Vedadi and Juanma Zambrano Chaves and Szu-Yeu Hu and Mike Schaekermann and Aishwarya Kamath and Yong Cheng and David G. T. Barrett and Cathy Cheung and Basil Mustafa and Anil Palepu and Daniel McDuff and Le Hou and Tomer Golany and Luyang Liu and Jean-baptiste Alayrac and Neil Houlsby and Nenad Tomasev and Jan Freyberg and Charles Lau and Jonas Kemp and Jeremy Lai and Shekoofeh Azizi and Kimberly Kanada and SiWai Man and Kavita Kulkarni and Ruoxi Sun and Siamak Shakeri and Luheng He and Ben Caine and Albert Webson and Natasha Latysheva and Melvin Johnson and Philip Mansfield and Jian Lu and Ehud Rivlin and Jesper Anderson and Bradley Green and Renee Wong and Jonathan Krause and Jonathon Shlens and Ewa Dominowska and S. M. Ali Eslami and Katherine Chou and Claire Cui and Oriol Vinyals and Koray Kavukcuoglu and James Manyika and Jeff Dean and Demis Hassabis and Yossi Matias and Dale Webster and Joelle Barral and Greg Corrado and Christopher Semturs and S. Sara Mahdavi and Juraj Gottweis and Alan Karthikesalingam and Vivek Natarajan", "abstract": "  Excellence in a wide variety of medical applications poses considerable\nchallenges for AI, requiring advanced reasoning, access to up-to-date medical\nknowledge and understanding of complex multimodal data. Gemini models, with\nstrong general capabilities in multimodal and long-context reasoning, offer\nexciting possibilities in medicine. Building on these core strengths of Gemini,\nwe introduce Med-Gemini, a family of highly capable multimodal models that are\nspecialized in medicine with the ability to seamlessly use web search, and that\ncan be efficiently tailored to novel modalities using custom encoders. We\nevaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art\n(SoTA) performance on 10 of them, and surpass the GPT-4 model family on every\nbenchmark where a direct comparison is viable, often by a wide margin. On the\npopular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves\nSoTA performance of 91.1% accuracy, using a novel uncertainty-guided search\nstrategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU\n(health & medicine), Med-Gemini improves over GPT-4V by an average relative\nmargin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context\ncapabilities through SoTA performance on a needle-in-a-haystack retrieval task\nfrom long de-identified health records and medical video question answering,\nsurpassing prior bespoke methods using only in-context learning. Finally,\nMed-Gemini's performance suggests real-world utility by surpassing human\nexperts on tasks such as medical text summarization, alongside demonstrations\nof promising potential for multimodal medical dialogue, medical research and\neducation. Taken together, our results offer compelling evidence for\nMed-Gemini's potential, although further rigorous evaluation will be crucial\nbefore real-world deployment in this safety-critical domain.\n", "link": "http://arxiv.org/abs/2404.18416v2", "date": "2024-05-01", "relevancy": 1.8089, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4461}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Capabilities%20of%20Gemini%20Models%20in%20Medicine&body=Title%3A%20Capabilities%20of%20Gemini%20Models%20in%20Medicine%0AAuthor%3A%20Khaled%20Saab%20and%20Tao%20Tu%20and%20Wei-Hung%20Weng%20and%20Ryutaro%20Tanno%20and%20David%20Stutz%20and%20Ellery%20Wulczyn%20and%20Fan%20Zhang%20and%20Tim%20Strother%20and%20Chunjong%20Park%20and%20Elahe%20Vedadi%20and%20Juanma%20Zambrano%20Chaves%20and%20Szu-Yeu%20Hu%20and%20Mike%20Schaekermann%20and%20Aishwarya%20Kamath%20and%20Yong%20Cheng%20and%20David%20G.%20T.%20Barrett%20and%20Cathy%20Cheung%20and%20Basil%20Mustafa%20and%20Anil%20Palepu%20and%20Daniel%20McDuff%20and%20Le%20Hou%20and%20Tomer%20Golany%20and%20Luyang%20Liu%20and%20Jean-baptiste%20Alayrac%20and%20Neil%20Houlsby%20and%20Nenad%20Tomasev%20and%20Jan%20Freyberg%20and%20Charles%20Lau%20and%20Jonas%20Kemp%20and%20Jeremy%20Lai%20and%20Shekoofeh%20Azizi%20and%20Kimberly%20Kanada%20and%20SiWai%20Man%20and%20Kavita%20Kulkarni%20and%20Ruoxi%20Sun%20and%20Siamak%20Shakeri%20and%20Luheng%20He%20and%20Ben%20Caine%20and%20Albert%20Webson%20and%20Natasha%20Latysheva%20and%20Melvin%20Johnson%20and%20Philip%20Mansfield%20and%20Jian%20Lu%20and%20Ehud%20Rivlin%20and%20Jesper%20Anderson%20and%20Bradley%20Green%20and%20Renee%20Wong%20and%20Jonathan%20Krause%20and%20Jonathon%20Shlens%20and%20Ewa%20Dominowska%20and%20S.%20M.%20Ali%20Eslami%20and%20Katherine%20Chou%20and%20Claire%20Cui%20and%20Oriol%20Vinyals%20and%20Koray%20Kavukcuoglu%20and%20James%20Manyika%20and%20Jeff%20Dean%20and%20Demis%20Hassabis%20and%20Yossi%20Matias%20and%20Dale%20Webster%20and%20Joelle%20Barral%20and%20Greg%20Corrado%20and%20Christopher%20Semturs%20and%20S.%20Sara%20Mahdavi%20and%20Juraj%20Gottweis%20and%20Alan%20Karthikesalingam%20and%20Vivek%20Natarajan%0AAbstract%3A%20%20%20Excellence%20in%20a%20wide%20variety%20of%20medical%20applications%20poses%20considerable%0Achallenges%20for%20AI%2C%20requiring%20advanced%20reasoning%2C%20access%20to%20up-to-date%20medical%0Aknowledge%20and%20understanding%20of%20complex%20multimodal%20data.%20Gemini%20models%2C%20with%0Astrong%20general%20capabilities%20in%20multimodal%20and%20long-context%20reasoning%2C%20offer%0Aexciting%20possibilities%20in%20medicine.%20Building%20on%20these%20core%20strengths%20of%20Gemini%2C%0Awe%20introduce%20Med-Gemini%2C%20a%20family%20of%20highly%20capable%20multimodal%20models%20that%20are%0Aspecialized%20in%20medicine%20with%20the%20ability%20to%20seamlessly%20use%20web%20search%2C%20and%20that%0Acan%20be%20efficiently%20tailored%20to%20novel%20modalities%20using%20custom%20encoders.%20We%0Aevaluate%20Med-Gemini%20on%2014%20medical%20benchmarks%2C%20establishing%20new%20state-of-the-art%0A%28SoTA%29%20performance%20on%2010%20of%20them%2C%20and%20surpass%20the%20GPT-4%20model%20family%20on%20every%0Abenchmark%20where%20a%20direct%20comparison%20is%20viable%2C%20often%20by%20a%20wide%20margin.%20On%20the%0Apopular%20MedQA%20%28USMLE%29%20benchmark%2C%20our%20best-performing%20Med-Gemini%20model%20achieves%0ASoTA%20performance%20of%2091.1%25%20accuracy%2C%20using%20a%20novel%20uncertainty-guided%20search%0Astrategy.%20On%207%20multimodal%20benchmarks%20including%20NEJM%20Image%20Challenges%20and%20MMMU%0A%28health%20%26%20medicine%29%2C%20Med-Gemini%20improves%20over%20GPT-4V%20by%20an%20average%20relative%0Amargin%20of%2044.5%25.%20We%20demonstrate%20the%20effectiveness%20of%20Med-Gemini%27s%20long-context%0Acapabilities%20through%20SoTA%20performance%20on%20a%20needle-in-a-haystack%20retrieval%20task%0Afrom%20long%20de-identified%20health%20records%20and%20medical%20video%20question%20answering%2C%0Asurpassing%20prior%20bespoke%20methods%20using%20only%20in-context%20learning.%20Finally%2C%0AMed-Gemini%27s%20performance%20suggests%20real-world%20utility%20by%20surpassing%20human%0Aexperts%20on%20tasks%20such%20as%20medical%20text%20summarization%2C%20alongside%20demonstrations%0Aof%20promising%20potential%20for%20multimodal%20medical%20dialogue%2C%20medical%20research%20and%0Aeducation.%20Taken%20together%2C%20our%20results%20offer%20compelling%20evidence%20for%0AMed-Gemini%27s%20potential%2C%20although%20further%20rigorous%20evaluation%20will%20be%20crucial%0Abefore%20real-world%20deployment%20in%20this%20safety-critical%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18416v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capabilities%20of%20Gemini%20Models%20in%20Medicine&entry.906535625=Khaled%20Saab%20and%20Tao%20Tu%20and%20Wei-Hung%20Weng%20and%20Ryutaro%20Tanno%20and%20David%20Stutz%20and%20Ellery%20Wulczyn%20and%20Fan%20Zhang%20and%20Tim%20Strother%20and%20Chunjong%20Park%20and%20Elahe%20Vedadi%20and%20Juanma%20Zambrano%20Chaves%20and%20Szu-Yeu%20Hu%20and%20Mike%20Schaekermann%20and%20Aishwarya%20Kamath%20and%20Yong%20Cheng%20and%20David%20G.%20T.%20Barrett%20and%20Cathy%20Cheung%20and%20Basil%20Mustafa%20and%20Anil%20Palepu%20and%20Daniel%20McDuff%20and%20Le%20Hou%20and%20Tomer%20Golany%20and%20Luyang%20Liu%20and%20Jean-baptiste%20Alayrac%20and%20Neil%20Houlsby%20and%20Nenad%20Tomasev%20and%20Jan%20Freyberg%20and%20Charles%20Lau%20and%20Jonas%20Kemp%20and%20Jeremy%20Lai%20and%20Shekoofeh%20Azizi%20and%20Kimberly%20Kanada%20and%20SiWai%20Man%20and%20Kavita%20Kulkarni%20and%20Ruoxi%20Sun%20and%20Siamak%20Shakeri%20and%20Luheng%20He%20and%20Ben%20Caine%20and%20Albert%20Webson%20and%20Natasha%20Latysheva%20and%20Melvin%20Johnson%20and%20Philip%20Mansfield%20and%20Jian%20Lu%20and%20Ehud%20Rivlin%20and%20Jesper%20Anderson%20and%20Bradley%20Green%20and%20Renee%20Wong%20and%20Jonathan%20Krause%20and%20Jonathon%20Shlens%20and%20Ewa%20Dominowska%20and%20S.%20M.%20Ali%20Eslami%20and%20Katherine%20Chou%20and%20Claire%20Cui%20and%20Oriol%20Vinyals%20and%20Koray%20Kavukcuoglu%20and%20James%20Manyika%20and%20Jeff%20Dean%20and%20Demis%20Hassabis%20and%20Yossi%20Matias%20and%20Dale%20Webster%20and%20Joelle%20Barral%20and%20Greg%20Corrado%20and%20Christopher%20Semturs%20and%20S.%20Sara%20Mahdavi%20and%20Juraj%20Gottweis%20and%20Alan%20Karthikesalingam%20and%20Vivek%20Natarajan&entry.1292438233=%20%20Excellence%20in%20a%20wide%20variety%20of%20medical%20applications%20poses%20considerable%0Achallenges%20for%20AI%2C%20requiring%20advanced%20reasoning%2C%20access%20to%20up-to-date%20medical%0Aknowledge%20and%20understanding%20of%20complex%20multimodal%20data.%20Gemini%20models%2C%20with%0Astrong%20general%20capabilities%20in%20multimodal%20and%20long-context%20reasoning%2C%20offer%0Aexciting%20possibilities%20in%20medicine.%20Building%20on%20these%20core%20strengths%20of%20Gemini%2C%0Awe%20introduce%20Med-Gemini%2C%20a%20family%20of%20highly%20capable%20multimodal%20models%20that%20are%0Aspecialized%20in%20medicine%20with%20the%20ability%20to%20seamlessly%20use%20web%20search%2C%20and%20that%0Acan%20be%20efficiently%20tailored%20to%20novel%20modalities%20using%20custom%20encoders.%20We%0Aevaluate%20Med-Gemini%20on%2014%20medical%20benchmarks%2C%20establishing%20new%20state-of-the-art%0A%28SoTA%29%20performance%20on%2010%20of%20them%2C%20and%20surpass%20the%20GPT-4%20model%20family%20on%20every%0Abenchmark%20where%20a%20direct%20comparison%20is%20viable%2C%20often%20by%20a%20wide%20margin.%20On%20the%0Apopular%20MedQA%20%28USMLE%29%20benchmark%2C%20our%20best-performing%20Med-Gemini%20model%20achieves%0ASoTA%20performance%20of%2091.1%25%20accuracy%2C%20using%20a%20novel%20uncertainty-guided%20search%0Astrategy.%20On%207%20multimodal%20benchmarks%20including%20NEJM%20Image%20Challenges%20and%20MMMU%0A%28health%20%26%20medicine%29%2C%20Med-Gemini%20improves%20over%20GPT-4V%20by%20an%20average%20relative%0Amargin%20of%2044.5%25.%20We%20demonstrate%20the%20effectiveness%20of%20Med-Gemini%27s%20long-context%0Acapabilities%20through%20SoTA%20performance%20on%20a%20needle-in-a-haystack%20retrieval%20task%0Afrom%20long%20de-identified%20health%20records%20and%20medical%20video%20question%20answering%2C%0Asurpassing%20prior%20bespoke%20methods%20using%20only%20in-context%20learning.%20Finally%2C%0AMed-Gemini%27s%20performance%20suggests%20real-world%20utility%20by%20surpassing%20human%0Aexperts%20on%20tasks%20such%20as%20medical%20text%20summarization%2C%20alongside%20demonstrations%0Aof%20promising%20potential%20for%20multimodal%20medical%20dialogue%2C%20medical%20research%20and%0Aeducation.%20Taken%20together%2C%20our%20results%20offer%20compelling%20evidence%20for%0AMed-Gemini%27s%20potential%2C%20although%20further%20rigorous%20evaluation%20will%20be%20crucial%0Abefore%20real-world%20deployment%20in%20this%20safety-critical%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18416v2&entry.124074799=Read"},
{"title": "Discovering robust biomarkers of neurological disorders from functional\n  MRI using graph neural networks: A Review", "author": "Yi Hao Chan and Deepank Girish and Sukrit Gupta and Jing Xia and Chockalingam Kasi and Yinan He and Conghao Wang and Jagath C. Rajapakse", "abstract": "  Graph neural networks (GNN) have emerged as a popular tool for modelling\nfunctional magnetic resonance imaging (fMRI) datasets. Many recent studies have\nreported significant improvements in disorder classification performance via\nmore sophisticated GNN designs and highlighted salient features that could be\npotential biomarkers of the disorder. In this review, we provide an overview of\nhow GNN and model explainability techniques have been applied on fMRI datasets\nfor disorder prediction tasks, with a particular emphasis on the robustness of\nbiomarkers produced for neurodegenerative diseases and neuropsychiatric\ndisorders. We found that while most studies have performant models, salient\nfeatures highlighted in these studies vary greatly across studies on the same\ndisorder and little has been done to evaluate their robustness. To address\nthese issues, we suggest establishing new standards that are based on objective\nevaluation metrics to determine the robustness of these potential biomarkers.\nWe further highlight gaps in the existing literature and put together a\nprediction-attribution-evaluation framework that could set the foundations for\nfuture research on improving the robustness of potential biomarkers discovered\nvia GNNs.\n", "link": "http://arxiv.org/abs/2405.00577v1", "date": "2024-05-01", "relevancy": 1.806, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4684}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4423}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4383}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Discovering%20robust%20biomarkers%20of%20neurological%20disorders%20from%20functional%0A%20%20MRI%20using%20graph%20neural%20networks%3A%20A%20Review&body=Title%3A%20Discovering%20robust%20biomarkers%20of%20neurological%20disorders%20from%20functional%0A%20%20MRI%20using%20graph%20neural%20networks%3A%20A%20Review%0AAuthor%3A%20Yi%20Hao%20Chan%20and%20Deepank%20Girish%20and%20Sukrit%20Gupta%20and%20Jing%20Xia%20and%20Chockalingam%20Kasi%20and%20Yinan%20He%20and%20Conghao%20Wang%20and%20Jagath%20C.%20Rajapakse%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNN%29%20have%20emerged%20as%20a%20popular%20tool%20for%20modelling%0Afunctional%20magnetic%20resonance%20imaging%20%28fMRI%29%20datasets.%20Many%20recent%20studies%20have%0Areported%20significant%20improvements%20in%20disorder%20classification%20performance%20via%0Amore%20sophisticated%20GNN%20designs%20and%20highlighted%20salient%20features%20that%20could%20be%0Apotential%20biomarkers%20of%20the%20disorder.%20In%20this%20review%2C%20we%20provide%20an%20overview%20of%0Ahow%20GNN%20and%20model%20explainability%20techniques%20have%20been%20applied%20on%20fMRI%20datasets%0Afor%20disorder%20prediction%20tasks%2C%20with%20a%20particular%20emphasis%20on%20the%20robustness%20of%0Abiomarkers%20produced%20for%20neurodegenerative%20diseases%20and%20neuropsychiatric%0Adisorders.%20We%20found%20that%20while%20most%20studies%20have%20performant%20models%2C%20salient%0Afeatures%20highlighted%20in%20these%20studies%20vary%20greatly%20across%20studies%20on%20the%20same%0Adisorder%20and%20little%20has%20been%20done%20to%20evaluate%20their%20robustness.%20To%20address%0Athese%20issues%2C%20we%20suggest%20establishing%20new%20standards%20that%20are%20based%20on%20objective%0Aevaluation%20metrics%20to%20determine%20the%20robustness%20of%20these%20potential%20biomarkers.%0AWe%20further%20highlight%20gaps%20in%20the%20existing%20literature%20and%20put%20together%20a%0Aprediction-attribution-evaluation%20framework%20that%20could%20set%20the%20foundations%20for%0Afuture%20research%20on%20improving%20the%20robustness%20of%20potential%20biomarkers%20discovered%0Avia%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00577v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20robust%20biomarkers%20of%20neurological%20disorders%20from%20functional%0A%20%20MRI%20using%20graph%20neural%20networks%3A%20A%20Review&entry.906535625=Yi%20Hao%20Chan%20and%20Deepank%20Girish%20and%20Sukrit%20Gupta%20and%20Jing%20Xia%20and%20Chockalingam%20Kasi%20and%20Yinan%20He%20and%20Conghao%20Wang%20and%20Jagath%20C.%20Rajapakse&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNN%29%20have%20emerged%20as%20a%20popular%20tool%20for%20modelling%0Afunctional%20magnetic%20resonance%20imaging%20%28fMRI%29%20datasets.%20Many%20recent%20studies%20have%0Areported%20significant%20improvements%20in%20disorder%20classification%20performance%20via%0Amore%20sophisticated%20GNN%20designs%20and%20highlighted%20salient%20features%20that%20could%20be%0Apotential%20biomarkers%20of%20the%20disorder.%20In%20this%20review%2C%20we%20provide%20an%20overview%20of%0Ahow%20GNN%20and%20model%20explainability%20techniques%20have%20been%20applied%20on%20fMRI%20datasets%0Afor%20disorder%20prediction%20tasks%2C%20with%20a%20particular%20emphasis%20on%20the%20robustness%20of%0Abiomarkers%20produced%20for%20neurodegenerative%20diseases%20and%20neuropsychiatric%0Adisorders.%20We%20found%20that%20while%20most%20studies%20have%20performant%20models%2C%20salient%0Afeatures%20highlighted%20in%20these%20studies%20vary%20greatly%20across%20studies%20on%20the%20same%0Adisorder%20and%20little%20has%20been%20done%20to%20evaluate%20their%20robustness.%20To%20address%0Athese%20issues%2C%20we%20suggest%20establishing%20new%20standards%20that%20are%20based%20on%20objective%0Aevaluation%20metrics%20to%20determine%20the%20robustness%20of%20these%20potential%20biomarkers.%0AWe%20further%20highlight%20gaps%20in%20the%20existing%20literature%20and%20put%20together%20a%0Aprediction-attribution-evaluation%20framework%20that%20could%20set%20the%20foundations%20for%0Afuture%20research%20on%20improving%20the%20robustness%20of%20potential%20biomarkers%20discovered%0Avia%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00577v1&entry.124074799=Read"},
{"title": "Self-Play Preference Optimization for Language Model Alignment", "author": "Yue Wu and Zhiqing Sun and Huizhuo Yuan and Kaixuan Ji and Yiming Yang and Quanquan Gu", "abstract": "  Traditional reinforcement learning from human feedback (RLHF) approaches\nrelying on parametric models like the Bradley-Terry model fall short in\ncapturing the intransitivity and irrationality in human preferences. Recent\nadvancements suggest that directly working with preference probabilities can\nyield a more accurate reflection of human preferences, enabling more flexible\nand accurate language model alignment. In this paper, we propose a\nself-play-based method for language model alignment, which treats the problem\nas a constant-sum two-player game aimed at identifying the Nash equilibrium\npolicy. Our approach, dubbed \\textit{Self-Play Preference Optimization} (SPPO),\napproximates the Nash equilibrium through iterative policy updates and enjoys\ntheoretical convergence guarantee. Our method can effectively increase the\nlog-likelihood of the chosen response and decrease that of the rejected\nresponse, which cannot be trivially achieved by symmetric pairwise loss such as\nDirect Preference Optimization (DPO) and Identity Preference Optimization\n(IPO). In our experiments, using only 60k prompts (without responses) from the\nUltraFeedback dataset and without any prompt augmentation, by leveraging a\npre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain\na model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the\nstate-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on\nAlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and\nthe Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved\nwithout additional external supervision (e.g., responses, preferences, etc.)\nfrom GPT-4 or other stronger language models.\n", "link": "http://arxiv.org/abs/2405.00675v1", "date": "2024-05-01", "relevancy": 1.7962, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4791}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4491}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.437}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Play%20Preference%20Optimization%20for%20Language%20Model%20Alignment&body=Title%3A%20Self-Play%20Preference%20Optimization%20for%20Language%20Model%20Alignment%0AAuthor%3A%20Yue%20Wu%20and%20Zhiqing%20Sun%20and%20Huizhuo%20Yuan%20and%20Kaixuan%20Ji%20and%20Yiming%20Yang%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Traditional%20reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20approaches%0Arelying%20on%20parametric%20models%20like%20the%20Bradley-Terry%20model%20fall%20short%20in%0Acapturing%20the%20intransitivity%20and%20irrationality%20in%20human%20preferences.%20Recent%0Aadvancements%20suggest%20that%20directly%20working%20with%20preference%20probabilities%20can%0Ayield%20a%20more%20accurate%20reflection%20of%20human%20preferences%2C%20enabling%20more%20flexible%0Aand%20accurate%20language%20model%20alignment.%20In%20this%20paper%2C%20we%20propose%20a%0Aself-play-based%20method%20for%20language%20model%20alignment%2C%20which%20treats%20the%20problem%0Aas%20a%20constant-sum%20two-player%20game%20aimed%20at%20identifying%20the%20Nash%20equilibrium%0Apolicy.%20Our%20approach%2C%20dubbed%20%5Ctextit%7BSelf-Play%20Preference%20Optimization%7D%20%28SPPO%29%2C%0Aapproximates%20the%20Nash%20equilibrium%20through%20iterative%20policy%20updates%20and%20enjoys%0Atheoretical%20convergence%20guarantee.%20Our%20method%20can%20effectively%20increase%20the%0Alog-likelihood%20of%20the%20chosen%20response%20and%20decrease%20that%20of%20the%20rejected%0Aresponse%2C%20which%20cannot%20be%20trivially%20achieved%20by%20symmetric%20pairwise%20loss%20such%20as%0ADirect%20Preference%20Optimization%20%28DPO%29%20and%20Identity%20Preference%20Optimization%0A%28IPO%29.%20In%20our%20experiments%2C%20using%20only%2060k%20prompts%20%28without%20responses%29%20from%20the%0AUltraFeedback%20dataset%20and%20without%20any%20prompt%20augmentation%2C%20by%20leveraging%20a%0Apre-trained%20preference%20model%20PairRM%20with%20only%200.4B%20parameters%2C%20SPPO%20can%20obtain%0Aa%20model%20from%20fine-tuning%20Mistral-7B-Instruct-v0.2%20that%20achieves%20the%0Astate-of-the-art%20length-controlled%20win-rate%20of%2028.53%25%20against%20GPT-4-Turbo%20on%0AAlpacaEval%202.0.%20It%20also%20outperforms%20the%20%28iterative%29%20DPO%20and%20IPO%20on%20MT-Bench%20and%0Athe%20Open%20LLM%20Leaderboard.%20Notably%2C%20the%20strong%20performance%20of%20SPPO%20is%20achieved%0Awithout%20additional%20external%20supervision%20%28e.g.%2C%20responses%2C%20preferences%2C%20etc.%29%0Afrom%20GPT-4%20or%20other%20stronger%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00675v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Play%20Preference%20Optimization%20for%20Language%20Model%20Alignment&entry.906535625=Yue%20Wu%20and%20Zhiqing%20Sun%20and%20Huizhuo%20Yuan%20and%20Kaixuan%20Ji%20and%20Yiming%20Yang%20and%20Quanquan%20Gu&entry.1292438233=%20%20Traditional%20reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20approaches%0Arelying%20on%20parametric%20models%20like%20the%20Bradley-Terry%20model%20fall%20short%20in%0Acapturing%20the%20intransitivity%20and%20irrationality%20in%20human%20preferences.%20Recent%0Aadvancements%20suggest%20that%20directly%20working%20with%20preference%20probabilities%20can%0Ayield%20a%20more%20accurate%20reflection%20of%20human%20preferences%2C%20enabling%20more%20flexible%0Aand%20accurate%20language%20model%20alignment.%20In%20this%20paper%2C%20we%20propose%20a%0Aself-play-based%20method%20for%20language%20model%20alignment%2C%20which%20treats%20the%20problem%0Aas%20a%20constant-sum%20two-player%20game%20aimed%20at%20identifying%20the%20Nash%20equilibrium%0Apolicy.%20Our%20approach%2C%20dubbed%20%5Ctextit%7BSelf-Play%20Preference%20Optimization%7D%20%28SPPO%29%2C%0Aapproximates%20the%20Nash%20equilibrium%20through%20iterative%20policy%20updates%20and%20enjoys%0Atheoretical%20convergence%20guarantee.%20Our%20method%20can%20effectively%20increase%20the%0Alog-likelihood%20of%20the%20chosen%20response%20and%20decrease%20that%20of%20the%20rejected%0Aresponse%2C%20which%20cannot%20be%20trivially%20achieved%20by%20symmetric%20pairwise%20loss%20such%20as%0ADirect%20Preference%20Optimization%20%28DPO%29%20and%20Identity%20Preference%20Optimization%0A%28IPO%29.%20In%20our%20experiments%2C%20using%20only%2060k%20prompts%20%28without%20responses%29%20from%20the%0AUltraFeedback%20dataset%20and%20without%20any%20prompt%20augmentation%2C%20by%20leveraging%20a%0Apre-trained%20preference%20model%20PairRM%20with%20only%200.4B%20parameters%2C%20SPPO%20can%20obtain%0Aa%20model%20from%20fine-tuning%20Mistral-7B-Instruct-v0.2%20that%20achieves%20the%0Astate-of-the-art%20length-controlled%20win-rate%20of%2028.53%25%20against%20GPT-4-Turbo%20on%0AAlpacaEval%202.0.%20It%20also%20outperforms%20the%20%28iterative%29%20DPO%20and%20IPO%20on%20MT-Bench%20and%0Athe%20Open%20LLM%20Leaderboard.%20Notably%2C%20the%20strong%20performance%20of%20SPPO%20is%20achieved%0Awithout%20additional%20external%20supervision%20%28e.g.%2C%20responses%2C%20preferences%2C%20etc.%29%0Afrom%20GPT-4%20or%20other%20stronger%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00675v1&entry.124074799=Read"},
{"title": "RGB$\\leftrightarrow$X: Image decomposition and synthesis using material-\n  and lighting-aware diffusion models", "author": "Zheng Zeng and Valentin Deschaintre and Iliyan Georgiev and Yannick Hold-Geoffroy and Yiwei Hu and Fujun Luan and Ling-Qi Yan and Milo\u0161 Ha\u0161an", "abstract": "  The three areas of realistic forward rendering, per-pixel inverse rendering,\nand generative image synthesis may seem like separate and unrelated sub-fields\nof graphics and vision. However, recent work has demonstrated improved\nestimation of per-pixel intrinsic channels (albedo, roughness, metallicity)\nbased on a diffusion architecture; we call this the RGB$\\rightarrow$X problem.\nWe further show that the reverse problem of synthesizing realistic images given\nintrinsic channels, X$\\rightarrow$RGB, can also be addressed in a diffusion\nframework.\n  Focusing on the image domain of interior scenes, we introduce an improved\ndiffusion model for RGB$\\rightarrow$X, which also estimates lighting, as well\nas the first diffusion X$\\rightarrow$RGB model capable of synthesizing\nrealistic images from (full or partial) intrinsic channels. Our\nX$\\rightarrow$RGB model explores a middle ground between traditional rendering\nand generative models: we can specify only certain appearance properties that\nshould be followed, and give freedom to the model to hallucinate a plausible\nversion of the rest.\n  This flexibility makes it possible to use a mix of heterogeneous training\ndatasets, which differ in the available channels. We use multiple existing\ndatasets and extend them with our own synthetic and real data, resulting in a\nmodel capable of extracting scene properties better than previous work and of\ngenerating highly realistic images of interior scenes.\n", "link": "http://arxiv.org/abs/2405.00666v1", "date": "2024-05-01", "relevancy": 1.7884, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.645}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5893}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5793}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RGB%24%5Cleftrightarrow%24X%3A%20Image%20decomposition%20and%20synthesis%20using%20material-%0A%20%20and%20lighting-aware%20diffusion%20models&body=Title%3A%20RGB%24%5Cleftrightarrow%24X%3A%20Image%20decomposition%20and%20synthesis%20using%20material-%0A%20%20and%20lighting-aware%20diffusion%20models%0AAuthor%3A%20Zheng%20Zeng%20and%20Valentin%20Deschaintre%20and%20Iliyan%20Georgiev%20and%20Yannick%20Hold-Geoffroy%20and%20Yiwei%20Hu%20and%20Fujun%20Luan%20and%20Ling-Qi%20Yan%20and%20Milo%C5%A1%20Ha%C5%A1an%0AAbstract%3A%20%20%20The%20three%20areas%20of%20realistic%20forward%20rendering%2C%20per-pixel%20inverse%20rendering%2C%0Aand%20generative%20image%20synthesis%20may%20seem%20like%20separate%20and%20unrelated%20sub-fields%0Aof%20graphics%20and%20vision.%20However%2C%20recent%20work%20has%20demonstrated%20improved%0Aestimation%20of%20per-pixel%20intrinsic%20channels%20%28albedo%2C%20roughness%2C%20metallicity%29%0Abased%20on%20a%20diffusion%20architecture%3B%20we%20call%20this%20the%20RGB%24%5Crightarrow%24X%20problem.%0AWe%20further%20show%20that%20the%20reverse%20problem%20of%20synthesizing%20realistic%20images%20given%0Aintrinsic%20channels%2C%20X%24%5Crightarrow%24RGB%2C%20can%20also%20be%20addressed%20in%20a%20diffusion%0Aframework.%0A%20%20Focusing%20on%20the%20image%20domain%20of%20interior%20scenes%2C%20we%20introduce%20an%20improved%0Adiffusion%20model%20for%20RGB%24%5Crightarrow%24X%2C%20which%20also%20estimates%20lighting%2C%20as%20well%0Aas%20the%20first%20diffusion%20X%24%5Crightarrow%24RGB%20model%20capable%20of%20synthesizing%0Arealistic%20images%20from%20%28full%20or%20partial%29%20intrinsic%20channels.%20Our%0AX%24%5Crightarrow%24RGB%20model%20explores%20a%20middle%20ground%20between%20traditional%20rendering%0Aand%20generative%20models%3A%20we%20can%20specify%20only%20certain%20appearance%20properties%20that%0Ashould%20be%20followed%2C%20and%20give%20freedom%20to%20the%20model%20to%20hallucinate%20a%20plausible%0Aversion%20of%20the%20rest.%0A%20%20This%20flexibility%20makes%20it%20possible%20to%20use%20a%20mix%20of%20heterogeneous%20training%0Adatasets%2C%20which%20differ%20in%20the%20available%20channels.%20We%20use%20multiple%20existing%0Adatasets%20and%20extend%20them%20with%20our%20own%20synthetic%20and%20real%20data%2C%20resulting%20in%20a%0Amodel%20capable%20of%20extracting%20scene%20properties%20better%20than%20previous%20work%20and%20of%0Agenerating%20highly%20realistic%20images%20of%20interior%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00666v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB%24%5Cleftrightarrow%24X%3A%20Image%20decomposition%20and%20synthesis%20using%20material-%0A%20%20and%20lighting-aware%20diffusion%20models&entry.906535625=Zheng%20Zeng%20and%20Valentin%20Deschaintre%20and%20Iliyan%20Georgiev%20and%20Yannick%20Hold-Geoffroy%20and%20Yiwei%20Hu%20and%20Fujun%20Luan%20and%20Ling-Qi%20Yan%20and%20Milo%C5%A1%20Ha%C5%A1an&entry.1292438233=%20%20The%20three%20areas%20of%20realistic%20forward%20rendering%2C%20per-pixel%20inverse%20rendering%2C%0Aand%20generative%20image%20synthesis%20may%20seem%20like%20separate%20and%20unrelated%20sub-fields%0Aof%20graphics%20and%20vision.%20However%2C%20recent%20work%20has%20demonstrated%20improved%0Aestimation%20of%20per-pixel%20intrinsic%20channels%20%28albedo%2C%20roughness%2C%20metallicity%29%0Abased%20on%20a%20diffusion%20architecture%3B%20we%20call%20this%20the%20RGB%24%5Crightarrow%24X%20problem.%0AWe%20further%20show%20that%20the%20reverse%20problem%20of%20synthesizing%20realistic%20images%20given%0Aintrinsic%20channels%2C%20X%24%5Crightarrow%24RGB%2C%20can%20also%20be%20addressed%20in%20a%20diffusion%0Aframework.%0A%20%20Focusing%20on%20the%20image%20domain%20of%20interior%20scenes%2C%20we%20introduce%20an%20improved%0Adiffusion%20model%20for%20RGB%24%5Crightarrow%24X%2C%20which%20also%20estimates%20lighting%2C%20as%20well%0Aas%20the%20first%20diffusion%20X%24%5Crightarrow%24RGB%20model%20capable%20of%20synthesizing%0Arealistic%20images%20from%20%28full%20or%20partial%29%20intrinsic%20channels.%20Our%0AX%24%5Crightarrow%24RGB%20model%20explores%20a%20middle%20ground%20between%20traditional%20rendering%0Aand%20generative%20models%3A%20we%20can%20specify%20only%20certain%20appearance%20properties%20that%0Ashould%20be%20followed%2C%20and%20give%20freedom%20to%20the%20model%20to%20hallucinate%20a%20plausible%0Aversion%20of%20the%20rest.%0A%20%20This%20flexibility%20makes%20it%20possible%20to%20use%20a%20mix%20of%20heterogeneous%20training%0Adatasets%2C%20which%20differ%20in%20the%20available%20channels.%20We%20use%20multiple%20existing%0Adatasets%20and%20extend%20them%20with%20our%20own%20synthetic%20and%20real%20data%2C%20resulting%20in%20a%0Amodel%20capable%20of%20extracting%20scene%20properties%20better%20than%20previous%20work%20and%20of%0Agenerating%20highly%20realistic%20images%20of%20interior%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00666v1&entry.124074799=Read"},
{"title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of\n  Diverse Models", "author": "Pat Verga and Sebastian Hofstatter and Sophia Althammer and Yixuan Su and Aleksandra Piktus and Arkady Arkhangorodsky and Minjie Xu and Naomi White and Patrick Lewis", "abstract": "  As Large Language Models (LLMs) have become more advanced, they have outpaced\nour abilities to accurately evaluate their quality. Not only is finding data to\nadequately probe particular model properties difficult, but evaluating the\ncorrectness of a model's freeform generation alone is a challenge. To address\nthis, many evaluations now rely on using LLMs themselves as judges to score the\nquality of outputs from other LLMs. Evaluations most commonly use a single\nlarge model like GPT4. While this method has grown in popularity, it is costly,\nhas been shown to introduce intramodel bias, and in this work, we find that\nvery large models are often unnecessary. We propose instead to evaluate models\nusing a Panel of LLm evaluators (PoLL). Across three distinct judge settings\nand spanning six different datasets, we find that using a PoLL composed of a\nlarger number of smaller models outperforms a single large judge, exhibits less\nintra-model bias due to its composition of disjoint model families, and does so\nwhile being over seven times less expensive.\n", "link": "http://arxiv.org/abs/2404.18796v2", "date": "2024-05-01", "relevancy": 1.7876, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4667}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.441}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Replacing%20Judges%20with%20Juries%3A%20Evaluating%20LLM%20Generations%20with%20a%20Panel%20of%0A%20%20Diverse%20Models&body=Title%3A%20Replacing%20Judges%20with%20Juries%3A%20Evaluating%20LLM%20Generations%20with%20a%20Panel%20of%0A%20%20Diverse%20Models%0AAuthor%3A%20Pat%20Verga%20and%20Sebastian%20Hofstatter%20and%20Sophia%20Althammer%20and%20Yixuan%20Su%20and%20Aleksandra%20Piktus%20and%20Arkady%20Arkhangorodsky%20and%20Minjie%20Xu%20and%20Naomi%20White%20and%20Patrick%20Lewis%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20more%20advanced%2C%20they%20have%20outpaced%0Aour%20abilities%20to%20accurately%20evaluate%20their%20quality.%20Not%20only%20is%20finding%20data%20to%0Aadequately%20probe%20particular%20model%20properties%20difficult%2C%20but%20evaluating%20the%0Acorrectness%20of%20a%20model%27s%20freeform%20generation%20alone%20is%20a%20challenge.%20To%20address%0Athis%2C%20many%20evaluations%20now%20rely%20on%20using%20LLMs%20themselves%20as%20judges%20to%20score%20the%0Aquality%20of%20outputs%20from%20other%20LLMs.%20Evaluations%20most%20commonly%20use%20a%20single%0Alarge%20model%20like%20GPT4.%20While%20this%20method%20has%20grown%20in%20popularity%2C%20it%20is%20costly%2C%0Ahas%20been%20shown%20to%20introduce%20intramodel%20bias%2C%20and%20in%20this%20work%2C%20we%20find%20that%0Avery%20large%20models%20are%20often%20unnecessary.%20We%20propose%20instead%20to%20evaluate%20models%0Ausing%20a%20Panel%20of%20LLm%20evaluators%20%28PoLL%29.%20Across%20three%20distinct%20judge%20settings%0Aand%20spanning%20six%20different%20datasets%2C%20we%20find%20that%20using%20a%20PoLL%20composed%20of%20a%0Alarger%20number%20of%20smaller%20models%20outperforms%20a%20single%20large%20judge%2C%20exhibits%20less%0Aintra-model%20bias%20due%20to%20its%20composition%20of%20disjoint%20model%20families%2C%20and%20does%20so%0Awhile%20being%20over%20seven%20times%20less%20expensive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18796v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Replacing%20Judges%20with%20Juries%3A%20Evaluating%20LLM%20Generations%20with%20a%20Panel%20of%0A%20%20Diverse%20Models&entry.906535625=Pat%20Verga%20and%20Sebastian%20Hofstatter%20and%20Sophia%20Althammer%20and%20Yixuan%20Su%20and%20Aleksandra%20Piktus%20and%20Arkady%20Arkhangorodsky%20and%20Minjie%20Xu%20and%20Naomi%20White%20and%20Patrick%20Lewis&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20more%20advanced%2C%20they%20have%20outpaced%0Aour%20abilities%20to%20accurately%20evaluate%20their%20quality.%20Not%20only%20is%20finding%20data%20to%0Aadequately%20probe%20particular%20model%20properties%20difficult%2C%20but%20evaluating%20the%0Acorrectness%20of%20a%20model%27s%20freeform%20generation%20alone%20is%20a%20challenge.%20To%20address%0Athis%2C%20many%20evaluations%20now%20rely%20on%20using%20LLMs%20themselves%20as%20judges%20to%20score%20the%0Aquality%20of%20outputs%20from%20other%20LLMs.%20Evaluations%20most%20commonly%20use%20a%20single%0Alarge%20model%20like%20GPT4.%20While%20this%20method%20has%20grown%20in%20popularity%2C%20it%20is%20costly%2C%0Ahas%20been%20shown%20to%20introduce%20intramodel%20bias%2C%20and%20in%20this%20work%2C%20we%20find%20that%0Avery%20large%20models%20are%20often%20unnecessary.%20We%20propose%20instead%20to%20evaluate%20models%0Ausing%20a%20Panel%20of%20LLm%20evaluators%20%28PoLL%29.%20Across%20three%20distinct%20judge%20settings%0Aand%20spanning%20six%20different%20datasets%2C%20we%20find%20that%20using%20a%20PoLL%20composed%20of%20a%0Alarger%20number%20of%20smaller%20models%20outperforms%20a%20single%20large%20judge%2C%20exhibits%20less%0Aintra-model%20bias%20due%20to%20its%20composition%20of%20disjoint%20model%20families%2C%20and%20does%20so%0Awhile%20being%20over%20seven%20times%20less%20expensive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18796v2&entry.124074799=Read"},
{"title": "Multigroup Robustness", "author": "Lunjia Hu and Charlotte Peale and Judy Hanwen Shen", "abstract": "  To address the shortcomings of real-world datasets, robust learning\nalgorithms have been designed to overcome arbitrary and indiscriminate data\ncorruption. However, practical processes of gathering data may lead to patterns\nof data corruption that are localized to specific partitions of the training\ndataset. Motivated by critical applications where the learned model is deployed\nto make predictions about people from a rich collection of overlapping\nsubpopulations, we initiate the study of multigroup robust algorithms whose\nrobustness guarantees for each subpopulation only degrade with the amount of\ndata corruption inside that subpopulation. When the data corruption is not\ndistributed uniformly over subpopulations, our algorithms provide more\nmeaningful robustness guarantees than standard guarantees that are oblivious to\nhow the data corruption and the affected subpopulations are related. Our\ntechniques establish a new connection between multigroup fairness and\nrobustness.\n", "link": "http://arxiv.org/abs/2405.00614v1", "date": "2024-05-01", "relevancy": 1.78, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4639}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4342}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4304}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multigroup%20Robustness&body=Title%3A%20Multigroup%20Robustness%0AAuthor%3A%20Lunjia%20Hu%20and%20Charlotte%20Peale%20and%20Judy%20Hanwen%20Shen%0AAbstract%3A%20%20%20To%20address%20the%20shortcomings%20of%20real-world%20datasets%2C%20robust%20learning%0Aalgorithms%20have%20been%20designed%20to%20overcome%20arbitrary%20and%20indiscriminate%20data%0Acorruption.%20However%2C%20practical%20processes%20of%20gathering%20data%20may%20lead%20to%20patterns%0Aof%20data%20corruption%20that%20are%20localized%20to%20specific%20partitions%20of%20the%20training%0Adataset.%20Motivated%20by%20critical%20applications%20where%20the%20learned%20model%20is%20deployed%0Ato%20make%20predictions%20about%20people%20from%20a%20rich%20collection%20of%20overlapping%0Asubpopulations%2C%20we%20initiate%20the%20study%20of%20multigroup%20robust%20algorithms%20whose%0Arobustness%20guarantees%20for%20each%20subpopulation%20only%20degrade%20with%20the%20amount%20of%0Adata%20corruption%20inside%20that%20subpopulation.%20When%20the%20data%20corruption%20is%20not%0Adistributed%20uniformly%20over%20subpopulations%2C%20our%20algorithms%20provide%20more%0Ameaningful%20robustness%20guarantees%20than%20standard%20guarantees%20that%20are%20oblivious%20to%0Ahow%20the%20data%20corruption%20and%20the%20affected%20subpopulations%20are%20related.%20Our%0Atechniques%20establish%20a%20new%20connection%20between%20multigroup%20fairness%20and%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00614v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multigroup%20Robustness&entry.906535625=Lunjia%20Hu%20and%20Charlotte%20Peale%20and%20Judy%20Hanwen%20Shen&entry.1292438233=%20%20To%20address%20the%20shortcomings%20of%20real-world%20datasets%2C%20robust%20learning%0Aalgorithms%20have%20been%20designed%20to%20overcome%20arbitrary%20and%20indiscriminate%20data%0Acorruption.%20However%2C%20practical%20processes%20of%20gathering%20data%20may%20lead%20to%20patterns%0Aof%20data%20corruption%20that%20are%20localized%20to%20specific%20partitions%20of%20the%20training%0Adataset.%20Motivated%20by%20critical%20applications%20where%20the%20learned%20model%20is%20deployed%0Ato%20make%20predictions%20about%20people%20from%20a%20rich%20collection%20of%20overlapping%0Asubpopulations%2C%20we%20initiate%20the%20study%20of%20multigroup%20robust%20algorithms%20whose%0Arobustness%20guarantees%20for%20each%20subpopulation%20only%20degrade%20with%20the%20amount%20of%0Adata%20corruption%20inside%20that%20subpopulation.%20When%20the%20data%20corruption%20is%20not%0Adistributed%20uniformly%20over%20subpopulations%2C%20our%20algorithms%20provide%20more%0Ameaningful%20robustness%20guarantees%20than%20standard%20guarantees%20that%20are%20oblivious%20to%0Ahow%20the%20data%20corruption%20and%20the%20affected%20subpopulations%20are%20related.%20Our%0Atechniques%20establish%20a%20new%20connection%20between%20multigroup%20fairness%20and%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00614v1&entry.124074799=Read"},
{"title": "A Survey of Graph Neural Networks for Social Recommender Systems", "author": "Kartik Sharma and Yeon-Chang Lee and Sivagami Nambi and Aditya Salian and Shlok Shah and Sang-Wook Kim and Srijan Kumar", "abstract": "  Social recommender systems (SocialRS) simultaneously leverage the\nuser-to-item interactions as well as the user-to-user social relations for the\ntask of generating item recommendations to users. Additionally exploiting\nsocial relations is clearly effective in understanding users' tastes due to the\neffects of homophily and social influence. For this reason, SocialRS has\nincreasingly attracted attention. In particular, with the advance of graph\nneural networks (GNN), many GNN-based SocialRS methods have been developed\nrecently. Therefore, we conduct a comprehensive and systematic review of the\nliterature on GNN-based SocialRS. In this survey, we first identify 84 papers\non GNN-based SocialRS after annotating 2151 papers by following the PRISMA\nframework (preferred reporting items for systematic reviews and meta-analyses).\nThen, we comprehensively review them in terms of their inputs and architectures\nto propose a novel taxonomy: (1) input taxonomy includes 5 groups of input type\nnotations and 7 groups of input representation notations; (2) architecture\ntaxonomy includes 8 groups of GNN encoder notations, 2 groups of decoder\nnotations, and 12 groups of loss function notations. We classify the GNN-based\nSocialRS methods into several categories as per the taxonomy and describe their\ndetails. Furthermore, we summarize benchmark datasets and metrics widely used\nto evaluate the GNN-based SocialRS methods. Finally, we conclude this survey by\npresenting some future research directions. GitHub repository with the curated\nlist of papers are available at\nhttps://github.com/claws-lab/awesome-GNN-social-recsys.\n", "link": "http://arxiv.org/abs/2212.04481v3", "date": "2024-05-01", "relevancy": 1.7304, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4471}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4279}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.42}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Graph%20Neural%20Networks%20for%20Social%20Recommender%20Systems&body=Title%3A%20A%20Survey%20of%20Graph%20Neural%20Networks%20for%20Social%20Recommender%20Systems%0AAuthor%3A%20Kartik%20Sharma%20and%20Yeon-Chang%20Lee%20and%20Sivagami%20Nambi%20and%20Aditya%20Salian%20and%20Shlok%20Shah%20and%20Sang-Wook%20Kim%20and%20Srijan%20Kumar%0AAbstract%3A%20%20%20Social%20recommender%20systems%20%28SocialRS%29%20simultaneously%20leverage%20the%0Auser-to-item%20interactions%20as%20well%20as%20the%20user-to-user%20social%20relations%20for%20the%0Atask%20of%20generating%20item%20recommendations%20to%20users.%20Additionally%20exploiting%0Asocial%20relations%20is%20clearly%20effective%20in%20understanding%20users%27%20tastes%20due%20to%20the%0Aeffects%20of%20homophily%20and%20social%20influence.%20For%20this%20reason%2C%20SocialRS%20has%0Aincreasingly%20attracted%20attention.%20In%20particular%2C%20with%20the%20advance%20of%20graph%0Aneural%20networks%20%28GNN%29%2C%20many%20GNN-based%20SocialRS%20methods%20have%20been%20developed%0Arecently.%20Therefore%2C%20we%20conduct%20a%20comprehensive%20and%20systematic%20review%20of%20the%0Aliterature%20on%20GNN-based%20SocialRS.%20In%20this%20survey%2C%20we%20first%20identify%2084%20papers%0Aon%20GNN-based%20SocialRS%20after%20annotating%202151%20papers%20by%20following%20the%20PRISMA%0Aframework%20%28preferred%20reporting%20items%20for%20systematic%20reviews%20and%20meta-analyses%29.%0AThen%2C%20we%20comprehensively%20review%20them%20in%20terms%20of%20their%20inputs%20and%20architectures%0Ato%20propose%20a%20novel%20taxonomy%3A%20%281%29%20input%20taxonomy%20includes%205%20groups%20of%20input%20type%0Anotations%20and%207%20groups%20of%20input%20representation%20notations%3B%20%282%29%20architecture%0Ataxonomy%20includes%208%20groups%20of%20GNN%20encoder%20notations%2C%202%20groups%20of%20decoder%0Anotations%2C%20and%2012%20groups%20of%20loss%20function%20notations.%20We%20classify%20the%20GNN-based%0ASocialRS%20methods%20into%20several%20categories%20as%20per%20the%20taxonomy%20and%20describe%20their%0Adetails.%20Furthermore%2C%20we%20summarize%20benchmark%20datasets%20and%20metrics%20widely%20used%0Ato%20evaluate%20the%20GNN-based%20SocialRS%20methods.%20Finally%2C%20we%20conclude%20this%20survey%20by%0Apresenting%20some%20future%20research%20directions.%20GitHub%20repository%20with%20the%20curated%0Alist%20of%20papers%20are%20available%20at%0Ahttps%3A//github.com/claws-lab/awesome-GNN-social-recsys.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.04481v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Graph%20Neural%20Networks%20for%20Social%20Recommender%20Systems&entry.906535625=Kartik%20Sharma%20and%20Yeon-Chang%20Lee%20and%20Sivagami%20Nambi%20and%20Aditya%20Salian%20and%20Shlok%20Shah%20and%20Sang-Wook%20Kim%20and%20Srijan%20Kumar&entry.1292438233=%20%20Social%20recommender%20systems%20%28SocialRS%29%20simultaneously%20leverage%20the%0Auser-to-item%20interactions%20as%20well%20as%20the%20user-to-user%20social%20relations%20for%20the%0Atask%20of%20generating%20item%20recommendations%20to%20users.%20Additionally%20exploiting%0Asocial%20relations%20is%20clearly%20effective%20in%20understanding%20users%27%20tastes%20due%20to%20the%0Aeffects%20of%20homophily%20and%20social%20influence.%20For%20this%20reason%2C%20SocialRS%20has%0Aincreasingly%20attracted%20attention.%20In%20particular%2C%20with%20the%20advance%20of%20graph%0Aneural%20networks%20%28GNN%29%2C%20many%20GNN-based%20SocialRS%20methods%20have%20been%20developed%0Arecently.%20Therefore%2C%20we%20conduct%20a%20comprehensive%20and%20systematic%20review%20of%20the%0Aliterature%20on%20GNN-based%20SocialRS.%20In%20this%20survey%2C%20we%20first%20identify%2084%20papers%0Aon%20GNN-based%20SocialRS%20after%20annotating%202151%20papers%20by%20following%20the%20PRISMA%0Aframework%20%28preferred%20reporting%20items%20for%20systematic%20reviews%20and%20meta-analyses%29.%0AThen%2C%20we%20comprehensively%20review%20them%20in%20terms%20of%20their%20inputs%20and%20architectures%0Ato%20propose%20a%20novel%20taxonomy%3A%20%281%29%20input%20taxonomy%20includes%205%20groups%20of%20input%20type%0Anotations%20and%207%20groups%20of%20input%20representation%20notations%3B%20%282%29%20architecture%0Ataxonomy%20includes%208%20groups%20of%20GNN%20encoder%20notations%2C%202%20groups%20of%20decoder%0Anotations%2C%20and%2012%20groups%20of%20loss%20function%20notations.%20We%20classify%20the%20GNN-based%0ASocialRS%20methods%20into%20several%20categories%20as%20per%20the%20taxonomy%20and%20describe%20their%0Adetails.%20Furthermore%2C%20we%20summarize%20benchmark%20datasets%20and%20metrics%20widely%20used%0Ato%20evaluate%20the%20GNN-based%20SocialRS%20methods.%20Finally%2C%20we%20conclude%20this%20survey%20by%0Apresenting%20some%20future%20research%20directions.%20GitHub%20repository%20with%20the%20curated%0Alist%20of%20papers%20are%20available%20at%0Ahttps%3A//github.com/claws-lab/awesome-GNN-social-recsys.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.04481v3&entry.124074799=Read"},
{"title": "Robustness of graph embedding methods for community detection", "author": "Zhi-Feng Wei and Pablo Moriano and Ramakrishnan Kannan", "abstract": "  This study investigates the robustness of graph embedding methods for\ncommunity detection in the face of network perturbations, specifically edge\ndeletions. Graph embedding techniques, which represent nodes as low-dimensional\nvectors, are widely used for various graph machine learning tasks due to their\nability to capture structural properties of networks effectively. However, the\nimpact of perturbations on the performance of these methods remains relatively\nunderstudied. The research considers state-of-the-art graph embedding methods\nfrom two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random\nwalk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on\nboth synthetic and real-world networks, the study reveals varying degrees of\nrobustness within each family of graph embedding methods. The robustness is\nfound to be influenced by factors such as network size, initial community\npartition strength, and the type of perturbation. Notably, node2vec and LLE\nconsistently demonstrate higher robustness for community detection across\ndifferent scenarios, including networks with degree and community size\nheterogeneity. These findings highlight the importance of selecting an\nappropriate graph embedding method based on the specific characteristics of the\nnetwork and the task at hand, particularly in scenarios where robustness to\nperturbations is crucial.\n", "link": "http://arxiv.org/abs/2405.00636v1", "date": "2024-05-01", "relevancy": 1.7293, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4332}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4326}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4313}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robustness%20of%20graph%20embedding%20methods%20for%20community%20detection&body=Title%3A%20Robustness%20of%20graph%20embedding%20methods%20for%20community%20detection%0AAuthor%3A%20Zhi-Feng%20Wei%20and%20Pablo%20Moriano%20and%20Ramakrishnan%20Kannan%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20robustness%20of%20graph%20embedding%20methods%20for%0Acommunity%20detection%20in%20the%20face%20of%20network%20perturbations%2C%20specifically%20edge%0Adeletions.%20Graph%20embedding%20techniques%2C%20which%20represent%20nodes%20as%20low-dimensional%0Avectors%2C%20are%20widely%20used%20for%20various%20graph%20machine%20learning%20tasks%20due%20to%20their%0Aability%20to%20capture%20structural%20properties%20of%20networks%20effectively.%20However%2C%20the%0Aimpact%20of%20perturbations%20on%20the%20performance%20of%20these%20methods%20remains%20relatively%0Aunderstudied.%20The%20research%20considers%20state-of-the-art%20graph%20embedding%20methods%0Afrom%20two%20families%3A%20matrix%20factorization%20%28e.g.%2C%20LE%2C%20LLE%2C%20HOPE%2C%20M-NMF%29%20and%20random%0Awalk-based%20%28e.g.%2C%20DeepWalk%2C%20LINE%2C%20node2vec%29.%20Through%20experiments%20conducted%20on%0Aboth%20synthetic%20and%20real-world%20networks%2C%20the%20study%20reveals%20varying%20degrees%20of%0Arobustness%20within%20each%20family%20of%20graph%20embedding%20methods.%20The%20robustness%20is%0Afound%20to%20be%20influenced%20by%20factors%20such%20as%20network%20size%2C%20initial%20community%0Apartition%20strength%2C%20and%20the%20type%20of%20perturbation.%20Notably%2C%20node2vec%20and%20LLE%0Aconsistently%20demonstrate%20higher%20robustness%20for%20community%20detection%20across%0Adifferent%20scenarios%2C%20including%20networks%20with%20degree%20and%20community%20size%0Aheterogeneity.%20These%20findings%20highlight%20the%20importance%20of%20selecting%20an%0Aappropriate%20graph%20embedding%20method%20based%20on%20the%20specific%20characteristics%20of%20the%0Anetwork%20and%20the%20task%20at%20hand%2C%20particularly%20in%20scenarios%20where%20robustness%20to%0Aperturbations%20is%20crucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00636v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20of%20graph%20embedding%20methods%20for%20community%20detection&entry.906535625=Zhi-Feng%20Wei%20and%20Pablo%20Moriano%20and%20Ramakrishnan%20Kannan&entry.1292438233=%20%20This%20study%20investigates%20the%20robustness%20of%20graph%20embedding%20methods%20for%0Acommunity%20detection%20in%20the%20face%20of%20network%20perturbations%2C%20specifically%20edge%0Adeletions.%20Graph%20embedding%20techniques%2C%20which%20represent%20nodes%20as%20low-dimensional%0Avectors%2C%20are%20widely%20used%20for%20various%20graph%20machine%20learning%20tasks%20due%20to%20their%0Aability%20to%20capture%20structural%20properties%20of%20networks%20effectively.%20However%2C%20the%0Aimpact%20of%20perturbations%20on%20the%20performance%20of%20these%20methods%20remains%20relatively%0Aunderstudied.%20The%20research%20considers%20state-of-the-art%20graph%20embedding%20methods%0Afrom%20two%20families%3A%20matrix%20factorization%20%28e.g.%2C%20LE%2C%20LLE%2C%20HOPE%2C%20M-NMF%29%20and%20random%0Awalk-based%20%28e.g.%2C%20DeepWalk%2C%20LINE%2C%20node2vec%29.%20Through%20experiments%20conducted%20on%0Aboth%20synthetic%20and%20real-world%20networks%2C%20the%20study%20reveals%20varying%20degrees%20of%0Arobustness%20within%20each%20family%20of%20graph%20embedding%20methods.%20The%20robustness%20is%0Afound%20to%20be%20influenced%20by%20factors%20such%20as%20network%20size%2C%20initial%20community%0Apartition%20strength%2C%20and%20the%20type%20of%20perturbation.%20Notably%2C%20node2vec%20and%20LLE%0Aconsistently%20demonstrate%20higher%20robustness%20for%20community%20detection%20across%0Adifferent%20scenarios%2C%20including%20networks%20with%20degree%20and%20community%20size%0Aheterogeneity.%20These%20findings%20highlight%20the%20importance%20of%20selecting%20an%0Aappropriate%20graph%20embedding%20method%20based%20on%20the%20specific%20characteristics%20of%20the%0Anetwork%20and%20the%20task%20at%20hand%2C%20particularly%20in%20scenarios%20where%20robustness%20to%0Aperturbations%20is%20crucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00636v1&entry.124074799=Read"},
{"title": "Enhancing Surgical Robots with Embodied Intelligence for Autonomous\n  Ultrasound Scanning", "author": "Huan Xu and Jinlin Wu and Guanglin Cao and Zhen Lei and Zhen Chen and Hongbin Liu", "abstract": "  Ultrasound robots are increasingly used in medical diagnostics and early\ndisease screening. However, current ultrasound robots lack the intelligence to\nunderstand human intentions and instructions, hindering autonomous ultrasound\nscanning. To solve this problem, we propose a novel Ultrasound Embodied\nIntelligence system that equips ultrasound robots with the large language model\n(LLM) and domain knowledge, thereby improving the efficiency of ultrasound\nrobots. Specifically, we first design an ultrasound operation knowledge\ndatabase to add expertise in ultrasound scanning to the LLM, enabling the LLM\nto perform precise motion planning. Furthermore, we devise a dynamic ultrasound\nscanning strategy based on a \\textit{think-observe-execute} prompt engineering,\nallowing LLMs to dynamically adjust motion planning strategies during the\nscanning procedures. Extensive experiments demonstrate that our system\nsignificantly improves ultrasound scan efficiency and quality from verbal\ncommands. This advancement in autonomous medical scanning technology\ncontributes to non-invasive diagnostics and streamlined medical workflows.\n", "link": "http://arxiv.org/abs/2405.00461v1", "date": "2024-05-01", "relevancy": 1.6897, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5346}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Surgical%20Robots%20with%20Embodied%20Intelligence%20for%20Autonomous%0A%20%20Ultrasound%20Scanning&body=Title%3A%20Enhancing%20Surgical%20Robots%20with%20Embodied%20Intelligence%20for%20Autonomous%0A%20%20Ultrasound%20Scanning%0AAuthor%3A%20Huan%20Xu%20and%20Jinlin%20Wu%20and%20Guanglin%20Cao%20and%20Zhen%20Lei%20and%20Zhen%20Chen%20and%20Hongbin%20Liu%0AAbstract%3A%20%20%20Ultrasound%20robots%20are%20increasingly%20used%20in%20medical%20diagnostics%20and%20early%0Adisease%20screening.%20However%2C%20current%20ultrasound%20robots%20lack%20the%20intelligence%20to%0Aunderstand%20human%20intentions%20and%20instructions%2C%20hindering%20autonomous%20ultrasound%0Ascanning.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20novel%20Ultrasound%20Embodied%0AIntelligence%20system%20that%20equips%20ultrasound%20robots%20with%20the%20large%20language%20model%0A%28LLM%29%20and%20domain%20knowledge%2C%20thereby%20improving%20the%20efficiency%20of%20ultrasound%0Arobots.%20Specifically%2C%20we%20first%20design%20an%20ultrasound%20operation%20knowledge%0Adatabase%20to%20add%20expertise%20in%20ultrasound%20scanning%20to%20the%20LLM%2C%20enabling%20the%20LLM%0Ato%20perform%20precise%20motion%20planning.%20Furthermore%2C%20we%20devise%20a%20dynamic%20ultrasound%0Ascanning%20strategy%20based%20on%20a%20%5Ctextit%7Bthink-observe-execute%7D%20prompt%20engineering%2C%0Aallowing%20LLMs%20to%20dynamically%20adjust%20motion%20planning%20strategies%20during%20the%0Ascanning%20procedures.%20Extensive%20experiments%20demonstrate%20that%20our%20system%0Asignificantly%20improves%20ultrasound%20scan%20efficiency%20and%20quality%20from%20verbal%0Acommands.%20This%20advancement%20in%20autonomous%20medical%20scanning%20technology%0Acontributes%20to%20non-invasive%20diagnostics%20and%20streamlined%20medical%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00461v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Surgical%20Robots%20with%20Embodied%20Intelligence%20for%20Autonomous%0A%20%20Ultrasound%20Scanning&entry.906535625=Huan%20Xu%20and%20Jinlin%20Wu%20and%20Guanglin%20Cao%20and%20Zhen%20Lei%20and%20Zhen%20Chen%20and%20Hongbin%20Liu&entry.1292438233=%20%20Ultrasound%20robots%20are%20increasingly%20used%20in%20medical%20diagnostics%20and%20early%0Adisease%20screening.%20However%2C%20current%20ultrasound%20robots%20lack%20the%20intelligence%20to%0Aunderstand%20human%20intentions%20and%20instructions%2C%20hindering%20autonomous%20ultrasound%0Ascanning.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20novel%20Ultrasound%20Embodied%0AIntelligence%20system%20that%20equips%20ultrasound%20robots%20with%20the%20large%20language%20model%0A%28LLM%29%20and%20domain%20knowledge%2C%20thereby%20improving%20the%20efficiency%20of%20ultrasound%0Arobots.%20Specifically%2C%20we%20first%20design%20an%20ultrasound%20operation%20knowledge%0Adatabase%20to%20add%20expertise%20in%20ultrasound%20scanning%20to%20the%20LLM%2C%20enabling%20the%20LLM%0Ato%20perform%20precise%20motion%20planning.%20Furthermore%2C%20we%20devise%20a%20dynamic%20ultrasound%0Ascanning%20strategy%20based%20on%20a%20%5Ctextit%7Bthink-observe-execute%7D%20prompt%20engineering%2C%0Aallowing%20LLMs%20to%20dynamically%20adjust%20motion%20planning%20strategies%20during%20the%0Ascanning%20procedures.%20Extensive%20experiments%20demonstrate%20that%20our%20system%0Asignificantly%20improves%20ultrasound%20scan%20efficiency%20and%20quality%20from%20verbal%0Acommands.%20This%20advancement%20in%20autonomous%20medical%20scanning%20technology%0Acontributes%20to%20non-invasive%20diagnostics%20and%20streamlined%20medical%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00461v1&entry.124074799=Read"},
{"title": "TexSliders: Diffusion-Based Texture Editing in CLIP Space", "author": "Julia Guerrero-Viu and Milos Hasan and Arthur Roullier and Midhun Harikumar and Yiwei Hu and Paul Guerrero and Diego Gutierrez and Belen Masia and Valentin Deschaintre", "abstract": "  Generative models have enabled intuitive image creation and manipulation\nusing natural language. In particular, diffusion models have recently shown\nremarkable results for natural image editing. In this work, we propose to apply\ndiffusion techniques to edit textures, a specific class of images that are an\nessential part of 3D content creation pipelines. We analyze existing editing\nmethods and show that they are not directly applicable to textures, since their\ncommon underlying approach, manipulating attention maps, is unsuitable for the\ntexture domain. To address this, we propose a novel approach that instead\nmanipulates CLIP image embeddings to condition the diffusion generation. We\ndefine editing directions using simple text prompts (e.g., \"aged wood\" to \"new\nwood\") and map these to CLIP image embedding space using a texture prior, with\na sampling-based approach that gives us identity-preserving directions in CLIP\nspace. To further improve identity preservation, we project these directions to\na CLIP subspace that minimizes identity variations resulting from entangled\ntexture attributes. Our editing pipeline facilitates the creation of arbitrary\nsliders using natural language prompts only, with no ground-truth annotated\ndata necessary.\n", "link": "http://arxiv.org/abs/2405.00672v1", "date": "2024-05-01", "relevancy": 1.6863, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5994}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5662}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TexSliders%3A%20Diffusion-Based%20Texture%20Editing%20in%20CLIP%20Space&body=Title%3A%20TexSliders%3A%20Diffusion-Based%20Texture%20Editing%20in%20CLIP%20Space%0AAuthor%3A%20Julia%20Guerrero-Viu%20and%20Milos%20Hasan%20and%20Arthur%20Roullier%20and%20Midhun%20Harikumar%20and%20Yiwei%20Hu%20and%20Paul%20Guerrero%20and%20Diego%20Gutierrez%20and%20Belen%20Masia%20and%20Valentin%20Deschaintre%0AAbstract%3A%20%20%20Generative%20models%20have%20enabled%20intuitive%20image%20creation%20and%20manipulation%0Ausing%20natural%20language.%20In%20particular%2C%20diffusion%20models%20have%20recently%20shown%0Aremarkable%20results%20for%20natural%20image%20editing.%20In%20this%20work%2C%20we%20propose%20to%20apply%0Adiffusion%20techniques%20to%20edit%20textures%2C%20a%20specific%20class%20of%20images%20that%20are%20an%0Aessential%20part%20of%203D%20content%20creation%20pipelines.%20We%20analyze%20existing%20editing%0Amethods%20and%20show%20that%20they%20are%20not%20directly%20applicable%20to%20textures%2C%20since%20their%0Acommon%20underlying%20approach%2C%20manipulating%20attention%20maps%2C%20is%20unsuitable%20for%20the%0Atexture%20domain.%20To%20address%20this%2C%20we%20propose%20a%20novel%20approach%20that%20instead%0Amanipulates%20CLIP%20image%20embeddings%20to%20condition%20the%20diffusion%20generation.%20We%0Adefine%20editing%20directions%20using%20simple%20text%20prompts%20%28e.g.%2C%20%22aged%20wood%22%20to%20%22new%0Awood%22%29%20and%20map%20these%20to%20CLIP%20image%20embedding%20space%20using%20a%20texture%20prior%2C%20with%0Aa%20sampling-based%20approach%20that%20gives%20us%20identity-preserving%20directions%20in%20CLIP%0Aspace.%20To%20further%20improve%20identity%20preservation%2C%20we%20project%20these%20directions%20to%0Aa%20CLIP%20subspace%20that%20minimizes%20identity%20variations%20resulting%20from%20entangled%0Atexture%20attributes.%20Our%20editing%20pipeline%20facilitates%20the%20creation%20of%20arbitrary%0Asliders%20using%20natural%20language%20prompts%20only%2C%20with%20no%20ground-truth%20annotated%0Adata%20necessary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00672v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexSliders%3A%20Diffusion-Based%20Texture%20Editing%20in%20CLIP%20Space&entry.906535625=Julia%20Guerrero-Viu%20and%20Milos%20Hasan%20and%20Arthur%20Roullier%20and%20Midhun%20Harikumar%20and%20Yiwei%20Hu%20and%20Paul%20Guerrero%20and%20Diego%20Gutierrez%20and%20Belen%20Masia%20and%20Valentin%20Deschaintre&entry.1292438233=%20%20Generative%20models%20have%20enabled%20intuitive%20image%20creation%20and%20manipulation%0Ausing%20natural%20language.%20In%20particular%2C%20diffusion%20models%20have%20recently%20shown%0Aremarkable%20results%20for%20natural%20image%20editing.%20In%20this%20work%2C%20we%20propose%20to%20apply%0Adiffusion%20techniques%20to%20edit%20textures%2C%20a%20specific%20class%20of%20images%20that%20are%20an%0Aessential%20part%20of%203D%20content%20creation%20pipelines.%20We%20analyze%20existing%20editing%0Amethods%20and%20show%20that%20they%20are%20not%20directly%20applicable%20to%20textures%2C%20since%20their%0Acommon%20underlying%20approach%2C%20manipulating%20attention%20maps%2C%20is%20unsuitable%20for%20the%0Atexture%20domain.%20To%20address%20this%2C%20we%20propose%20a%20novel%20approach%20that%20instead%0Amanipulates%20CLIP%20image%20embeddings%20to%20condition%20the%20diffusion%20generation.%20We%0Adefine%20editing%20directions%20using%20simple%20text%20prompts%20%28e.g.%2C%20%22aged%20wood%22%20to%20%22new%0Awood%22%29%20and%20map%20these%20to%20CLIP%20image%20embedding%20space%20using%20a%20texture%20prior%2C%20with%0Aa%20sampling-based%20approach%20that%20gives%20us%20identity-preserving%20directions%20in%20CLIP%0Aspace.%20To%20further%20improve%20identity%20preservation%2C%20we%20project%20these%20directions%20to%0Aa%20CLIP%20subspace%20that%20minimizes%20identity%20variations%20resulting%20from%20entangled%0Atexture%20attributes.%20Our%20editing%20pipeline%20facilitates%20the%20creation%20of%20arbitrary%0Asliders%20using%20natural%20language%20prompts%20only%2C%20with%20no%20ground-truth%20annotated%0Adata%20necessary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00672v1&entry.124074799=Read"},
{"title": "Semantic-guided modeling of spatial relation and object co-occurrence\n  for indoor scene recognition", "author": "Chuanxin Song and Hanbo Wu and Xin Ma", "abstract": "  Exploring the semantic context in scene images is essential for indoor scene\nrecognition. However, due to the diverse intra-class spatial layouts and the\ncoexisting inter-class objects, modeling contextual relationships to adapt\nvarious image characteristics is a great challenge. Existing contextual\nmodeling methods for scene recognition exhibit two limitations: 1) They\ntypically model only one kind of spatial relationship among objects within\nscenes in an artificially predefined manner, with limited exploration of\ndiverse spatial layouts. 2) They often overlook the differences in coexisting\nobjects across different scenes, suppressing scene recognition performance. To\novercome these limitations, we propose SpaCoNet, which simultaneously models\nSpatial relation and Co-occurrence of objects guided by semantic segmentation.\nFirstly, the Semantic Spatial Relation Module (SSRM) is constructed to model\nscene spatial features. With the help of semantic segmentation, this module\ndecouples the spatial information from the scene image and thoroughly explores\nall spatial relationships among objects in an end-to-end manner. Secondly, both\nspatial features from the SSRM and deep features from the Image Feature\nExtraction Module are allocated to each object, so as to distinguish the\ncoexisting object across different scenes. Finally, utilizing the\ndiscriminative features above, we design a Global-Local Dependency Module to\nexplore the long-range co-occurrence among objects, and further generate a\nsemantic-guided feature representation for indoor scene recognition.\nExperimental results on three widely used scene datasets demonstrate the\neffectiveness and generality of the proposed method.\n", "link": "http://arxiv.org/abs/2305.12661v3", "date": "2024-05-01", "relevancy": 1.6852, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5762}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5577}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5572}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantic-guided%20modeling%20of%20spatial%20relation%20and%20object%20co-occurrence%0A%20%20for%20indoor%20scene%20recognition&body=Title%3A%20Semantic-guided%20modeling%20of%20spatial%20relation%20and%20object%20co-occurrence%0A%20%20for%20indoor%20scene%20recognition%0AAuthor%3A%20Chuanxin%20Song%20and%20Hanbo%20Wu%20and%20Xin%20Ma%0AAbstract%3A%20%20%20Exploring%20the%20semantic%20context%20in%20scene%20images%20is%20essential%20for%20indoor%20scene%0Arecognition.%20However%2C%20due%20to%20the%20diverse%20intra-class%20spatial%20layouts%20and%20the%0Acoexisting%20inter-class%20objects%2C%20modeling%20contextual%20relationships%20to%20adapt%0Avarious%20image%20characteristics%20is%20a%20great%20challenge.%20Existing%20contextual%0Amodeling%20methods%20for%20scene%20recognition%20exhibit%20two%20limitations%3A%201%29%20They%0Atypically%20model%20only%20one%20kind%20of%20spatial%20relationship%20among%20objects%20within%0Ascenes%20in%20an%20artificially%20predefined%20manner%2C%20with%20limited%20exploration%20of%0Adiverse%20spatial%20layouts.%202%29%20They%20often%20overlook%20the%20differences%20in%20coexisting%0Aobjects%20across%20different%20scenes%2C%20suppressing%20scene%20recognition%20performance.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20SpaCoNet%2C%20which%20simultaneously%20models%0ASpatial%20relation%20and%20Co-occurrence%20of%20objects%20guided%20by%20semantic%20segmentation.%0AFirstly%2C%20the%20Semantic%20Spatial%20Relation%20Module%20%28SSRM%29%20is%20constructed%20to%20model%0Ascene%20spatial%20features.%20With%20the%20help%20of%20semantic%20segmentation%2C%20this%20module%0Adecouples%20the%20spatial%20information%20from%20the%20scene%20image%20and%20thoroughly%20explores%0Aall%20spatial%20relationships%20among%20objects%20in%20an%20end-to-end%20manner.%20Secondly%2C%20both%0Aspatial%20features%20from%20the%20SSRM%20and%20deep%20features%20from%20the%20Image%20Feature%0AExtraction%20Module%20are%20allocated%20to%20each%20object%2C%20so%20as%20to%20distinguish%20the%0Acoexisting%20object%20across%20different%20scenes.%20Finally%2C%20utilizing%20the%0Adiscriminative%20features%20above%2C%20we%20design%20a%20Global-Local%20Dependency%20Module%20to%0Aexplore%20the%20long-range%20co-occurrence%20among%20objects%2C%20and%20further%20generate%20a%0Asemantic-guided%20feature%20representation%20for%20indoor%20scene%20recognition.%0AExperimental%20results%20on%20three%20widely%20used%20scene%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20generality%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12661v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-guided%20modeling%20of%20spatial%20relation%20and%20object%20co-occurrence%0A%20%20for%20indoor%20scene%20recognition&entry.906535625=Chuanxin%20Song%20and%20Hanbo%20Wu%20and%20Xin%20Ma&entry.1292438233=%20%20Exploring%20the%20semantic%20context%20in%20scene%20images%20is%20essential%20for%20indoor%20scene%0Arecognition.%20However%2C%20due%20to%20the%20diverse%20intra-class%20spatial%20layouts%20and%20the%0Acoexisting%20inter-class%20objects%2C%20modeling%20contextual%20relationships%20to%20adapt%0Avarious%20image%20characteristics%20is%20a%20great%20challenge.%20Existing%20contextual%0Amodeling%20methods%20for%20scene%20recognition%20exhibit%20two%20limitations%3A%201%29%20They%0Atypically%20model%20only%20one%20kind%20of%20spatial%20relationship%20among%20objects%20within%0Ascenes%20in%20an%20artificially%20predefined%20manner%2C%20with%20limited%20exploration%20of%0Adiverse%20spatial%20layouts.%202%29%20They%20often%20overlook%20the%20differences%20in%20coexisting%0Aobjects%20across%20different%20scenes%2C%20suppressing%20scene%20recognition%20performance.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20SpaCoNet%2C%20which%20simultaneously%20models%0ASpatial%20relation%20and%20Co-occurrence%20of%20objects%20guided%20by%20semantic%20segmentation.%0AFirstly%2C%20the%20Semantic%20Spatial%20Relation%20Module%20%28SSRM%29%20is%20constructed%20to%20model%0Ascene%20spatial%20features.%20With%20the%20help%20of%20semantic%20segmentation%2C%20this%20module%0Adecouples%20the%20spatial%20information%20from%20the%20scene%20image%20and%20thoroughly%20explores%0Aall%20spatial%20relationships%20among%20objects%20in%20an%20end-to-end%20manner.%20Secondly%2C%20both%0Aspatial%20features%20from%20the%20SSRM%20and%20deep%20features%20from%20the%20Image%20Feature%0AExtraction%20Module%20are%20allocated%20to%20each%20object%2C%20so%20as%20to%20distinguish%20the%0Acoexisting%20object%20across%20different%20scenes.%20Finally%2C%20utilizing%20the%0Adiscriminative%20features%20above%2C%20we%20design%20a%20Global-Local%20Dependency%20Module%20to%0Aexplore%20the%20long-range%20co-occurrence%20among%20objects%2C%20and%20further%20generate%20a%0Asemantic-guided%20feature%20representation%20for%20indoor%20scene%20recognition.%0AExperimental%20results%20on%20three%20widely%20used%20scene%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20generality%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12661v3&entry.124074799=Read"},
{"title": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs", "author": "Nicolas Gorlo and Lukas Schmid and Luca Carlone", "abstract": "  We present a novel approach for long-term human trajectory prediction, which\nis essential for long-horizon robot planning in human-populated environments.\nState-of-the-art human trajectory prediction methods are limited by their focus\non collision avoidance and short-term planning, and their inability to model\ncomplex interactions of humans with the environment. In contrast, our approach\novercomes these limitations by predicting sequences of human interactions with\nthe environment and using this information to guide trajectory predictions over\na horizon of up to 60s. We leverage Large Language Models (LLMs) to predict\ninteractions with the environment by conditioning the LLM prediction on rich\ncontextual information about the scene. This information is given as a 3D\nDynamic Scene Graph that encodes the geometry, semantics, and traversability of\nthe environment into a hierarchical representation. We then ground these\ninteraction sequences into multi-modal spatio-temporal distributions over human\npositions using a probabilistic approach based on continuous-time Markov\nChains. To evaluate our approach, we introduce a new semi-synthetic dataset of\nlong-term human trajectories in complex indoor environments, which also\nincludes annotations of human-object interactions. We show in thorough\nexperimental evaluations that our approach achieves a 54% lower average\nnegative log-likelihood (NLL) and a 26.5% lower Best-of-20 displacement error\ncompared to the best non-privileged baselines for a time horizon of 60s.\n", "link": "http://arxiv.org/abs/2405.00552v1", "date": "2024-05-01", "relevancy": 1.6851, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5893}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5734}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.546}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Long-Term%20Human%20Trajectory%20Prediction%20using%203D%20Dynamic%20Scene%20Graphs&body=Title%3A%20Long-Term%20Human%20Trajectory%20Prediction%20using%203D%20Dynamic%20Scene%20Graphs%0AAuthor%3A%20Nicolas%20Gorlo%20and%20Lukas%20Schmid%20and%20Luca%20Carlone%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20long-term%20human%20trajectory%20prediction%2C%20which%0Ais%20essential%20for%20long-horizon%20robot%20planning%20in%20human-populated%20environments.%0AState-of-the-art%20human%20trajectory%20prediction%20methods%20are%20limited%20by%20their%20focus%0Aon%20collision%20avoidance%20and%20short-term%20planning%2C%20and%20their%20inability%20to%20model%0Acomplex%20interactions%20of%20humans%20with%20the%20environment.%20In%20contrast%2C%20our%20approach%0Aovercomes%20these%20limitations%20by%20predicting%20sequences%20of%20human%20interactions%20with%0Athe%20environment%20and%20using%20this%20information%20to%20guide%20trajectory%20predictions%20over%0Aa%20horizon%20of%20up%20to%2060s.%20We%20leverage%20Large%20Language%20Models%20%28LLMs%29%20to%20predict%0Ainteractions%20with%20the%20environment%20by%20conditioning%20the%20LLM%20prediction%20on%20rich%0Acontextual%20information%20about%20the%20scene.%20This%20information%20is%20given%20as%20a%203D%0ADynamic%20Scene%20Graph%20that%20encodes%20the%20geometry%2C%20semantics%2C%20and%20traversability%20of%0Athe%20environment%20into%20a%20hierarchical%20representation.%20We%20then%20ground%20these%0Ainteraction%20sequences%20into%20multi-modal%20spatio-temporal%20distributions%20over%20human%0Apositions%20using%20a%20probabilistic%20approach%20based%20on%20continuous-time%20Markov%0AChains.%20To%20evaluate%20our%20approach%2C%20we%20introduce%20a%20new%20semi-synthetic%20dataset%20of%0Along-term%20human%20trajectories%20in%20complex%20indoor%20environments%2C%20which%20also%0Aincludes%20annotations%20of%20human-object%20interactions.%20We%20show%20in%20thorough%0Aexperimental%20evaluations%20that%20our%20approach%20achieves%20a%2054%25%20lower%20average%0Anegative%20log-likelihood%20%28NLL%29%20and%20a%2026.5%25%20lower%20Best-of-20%20displacement%20error%0Acompared%20to%20the%20best%20non-privileged%20baselines%20for%20a%20time%20horizon%20of%2060s.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00552v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Term%20Human%20Trajectory%20Prediction%20using%203D%20Dynamic%20Scene%20Graphs&entry.906535625=Nicolas%20Gorlo%20and%20Lukas%20Schmid%20and%20Luca%20Carlone&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20long-term%20human%20trajectory%20prediction%2C%20which%0Ais%20essential%20for%20long-horizon%20robot%20planning%20in%20human-populated%20environments.%0AState-of-the-art%20human%20trajectory%20prediction%20methods%20are%20limited%20by%20their%20focus%0Aon%20collision%20avoidance%20and%20short-term%20planning%2C%20and%20their%20inability%20to%20model%0Acomplex%20interactions%20of%20humans%20with%20the%20environment.%20In%20contrast%2C%20our%20approach%0Aovercomes%20these%20limitations%20by%20predicting%20sequences%20of%20human%20interactions%20with%0Athe%20environment%20and%20using%20this%20information%20to%20guide%20trajectory%20predictions%20over%0Aa%20horizon%20of%20up%20to%2060s.%20We%20leverage%20Large%20Language%20Models%20%28LLMs%29%20to%20predict%0Ainteractions%20with%20the%20environment%20by%20conditioning%20the%20LLM%20prediction%20on%20rich%0Acontextual%20information%20about%20the%20scene.%20This%20information%20is%20given%20as%20a%203D%0ADynamic%20Scene%20Graph%20that%20encodes%20the%20geometry%2C%20semantics%2C%20and%20traversability%20of%0Athe%20environment%20into%20a%20hierarchical%20representation.%20We%20then%20ground%20these%0Ainteraction%20sequences%20into%20multi-modal%20spatio-temporal%20distributions%20over%20human%0Apositions%20using%20a%20probabilistic%20approach%20based%20on%20continuous-time%20Markov%0AChains.%20To%20evaluate%20our%20approach%2C%20we%20introduce%20a%20new%20semi-synthetic%20dataset%20of%0Along-term%20human%20trajectories%20in%20complex%20indoor%20environments%2C%20which%20also%0Aincludes%20annotations%20of%20human-object%20interactions.%20We%20show%20in%20thorough%0Aexperimental%20evaluations%20that%20our%20approach%20achieves%20a%2054%25%20lower%20average%0Anegative%20log-likelihood%20%28NLL%29%20and%20a%2026.5%25%20lower%20Best-of-20%20displacement%20error%0Acompared%20to%20the%20best%20non-privileged%20baselines%20for%20a%20time%20horizon%20of%2060s.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00552v1&entry.124074799=Read"},
{"title": "MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion\n  Generation", "author": "Xujie Zhang and Ente Lin and Xiu Li and Yuxuan Luo and Michael Kampffmeyer and Xin Dong and Xiaodan Liang", "abstract": "  This paper introduces MMTryon, a multi-modal multi-reference VIrtual Try-ON\n(VITON) framework, which can generate high-quality compositional try-on results\nby taking as inputs a text instruction and multiple garment images. Our MMTryon\nmainly addresses two problems overlooked in prior literature: 1) Support of\nmultiple try-on items and dressing styleExisting methods are commonly designed\nfor single-item try-on tasks (e.g., upper/lower garments, dresses) and fall\nshort on customizing dressing styles (e.g., zipped/unzipped, tuck-in/tuck-out,\netc.) 2) Segmentation Dependency. They further heavily rely on\ncategory-specific segmentation models to identify the replacement regions, with\nsegmentation errors directly leading to significant artifacts in the try-on\nresults. For the first issue, our MMTryon introduces a novel multi-modality and\nmulti-reference attention mechanism to combine the garment information from\nreference images and dressing-style information from text instructions.\nBesides, to remove the segmentation dependency, MMTryon uses a parsing-free\ngarment encoder and leverages a novel scalable data generation pipeline to\nconvert existing VITON datasets to a form that allows MMTryon to be trained\nwithout requiring any explicit segmentation. Extensive experiments on\nhigh-resolution benchmarks and in-the-wild test sets demonstrate MMTryon's\nsuperiority over existing SOTA methods both qualitatively and quantitatively.\nBesides, MMTryon's impressive performance on multi-items and style-controllable\nvirtual try-on scenarios and its ability to try on any outfit in a large\nvariety of scenarios from any source image, opens up a new avenue for future\ninvestigation in the fashion community.\n", "link": "http://arxiv.org/abs/2405.00448v1", "date": "2024-05-01", "relevancy": 1.6752, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6312}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5377}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MMTryon%3A%20Multi-Modal%20Multi-Reference%20Control%20for%20High-Quality%20Fashion%0A%20%20Generation&body=Title%3A%20MMTryon%3A%20Multi-Modal%20Multi-Reference%20Control%20for%20High-Quality%20Fashion%0A%20%20Generation%0AAuthor%3A%20Xujie%20Zhang%20and%20Ente%20Lin%20and%20Xiu%20Li%20and%20Yuxuan%20Luo%20and%20Michael%20Kampffmeyer%20and%20Xin%20Dong%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20This%20paper%20introduces%20MMTryon%2C%20a%20multi-modal%20multi-reference%20VIrtual%20Try-ON%0A%28VITON%29%20framework%2C%20which%20can%20generate%20high-quality%20compositional%20try-on%20results%0Aby%20taking%20as%20inputs%20a%20text%20instruction%20and%20multiple%20garment%20images.%20Our%20MMTryon%0Amainly%20addresses%20two%20problems%20overlooked%20in%20prior%20literature%3A%201%29%20Support%20of%0Amultiple%20try-on%20items%20and%20dressing%20styleExisting%20methods%20are%20commonly%20designed%0Afor%20single-item%20try-on%20tasks%20%28e.g.%2C%20upper/lower%20garments%2C%20dresses%29%20and%20fall%0Ashort%20on%20customizing%20dressing%20styles%20%28e.g.%2C%20zipped/unzipped%2C%20tuck-in/tuck-out%2C%0Aetc.%29%202%29%20Segmentation%20Dependency.%20They%20further%20heavily%20rely%20on%0Acategory-specific%20segmentation%20models%20to%20identify%20the%20replacement%20regions%2C%20with%0Asegmentation%20errors%20directly%20leading%20to%20significant%20artifacts%20in%20the%20try-on%0Aresults.%20For%20the%20first%20issue%2C%20our%20MMTryon%20introduces%20a%20novel%20multi-modality%20and%0Amulti-reference%20attention%20mechanism%20to%20combine%20the%20garment%20information%20from%0Areference%20images%20and%20dressing-style%20information%20from%20text%20instructions.%0ABesides%2C%20to%20remove%20the%20segmentation%20dependency%2C%20MMTryon%20uses%20a%20parsing-free%0Agarment%20encoder%20and%20leverages%20a%20novel%20scalable%20data%20generation%20pipeline%20to%0Aconvert%20existing%20VITON%20datasets%20to%20a%20form%20that%20allows%20MMTryon%20to%20be%20trained%0Awithout%20requiring%20any%20explicit%20segmentation.%20Extensive%20experiments%20on%0Ahigh-resolution%20benchmarks%20and%20in-the-wild%20test%20sets%20demonstrate%20MMTryon%27s%0Asuperiority%20over%20existing%20SOTA%20methods%20both%20qualitatively%20and%20quantitatively.%0ABesides%2C%20MMTryon%27s%20impressive%20performance%20on%20multi-items%20and%20style-controllable%0Avirtual%20try-on%20scenarios%20and%20its%20ability%20to%20try%20on%20any%20outfit%20in%20a%20large%0Avariety%20of%20scenarios%20from%20any%20source%20image%2C%20opens%20up%20a%20new%20avenue%20for%20future%0Ainvestigation%20in%20the%20fashion%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00448v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMTryon%3A%20Multi-Modal%20Multi-Reference%20Control%20for%20High-Quality%20Fashion%0A%20%20Generation&entry.906535625=Xujie%20Zhang%20and%20Ente%20Lin%20and%20Xiu%20Li%20and%20Yuxuan%20Luo%20and%20Michael%20Kampffmeyer%20and%20Xin%20Dong%20and%20Xiaodan%20Liang&entry.1292438233=%20%20This%20paper%20introduces%20MMTryon%2C%20a%20multi-modal%20multi-reference%20VIrtual%20Try-ON%0A%28VITON%29%20framework%2C%20which%20can%20generate%20high-quality%20compositional%20try-on%20results%0Aby%20taking%20as%20inputs%20a%20text%20instruction%20and%20multiple%20garment%20images.%20Our%20MMTryon%0Amainly%20addresses%20two%20problems%20overlooked%20in%20prior%20literature%3A%201%29%20Support%20of%0Amultiple%20try-on%20items%20and%20dressing%20styleExisting%20methods%20are%20commonly%20designed%0Afor%20single-item%20try-on%20tasks%20%28e.g.%2C%20upper/lower%20garments%2C%20dresses%29%20and%20fall%0Ashort%20on%20customizing%20dressing%20styles%20%28e.g.%2C%20zipped/unzipped%2C%20tuck-in/tuck-out%2C%0Aetc.%29%202%29%20Segmentation%20Dependency.%20They%20further%20heavily%20rely%20on%0Acategory-specific%20segmentation%20models%20to%20identify%20the%20replacement%20regions%2C%20with%0Asegmentation%20errors%20directly%20leading%20to%20significant%20artifacts%20in%20the%20try-on%0Aresults.%20For%20the%20first%20issue%2C%20our%20MMTryon%20introduces%20a%20novel%20multi-modality%20and%0Amulti-reference%20attention%20mechanism%20to%20combine%20the%20garment%20information%20from%0Areference%20images%20and%20dressing-style%20information%20from%20text%20instructions.%0ABesides%2C%20to%20remove%20the%20segmentation%20dependency%2C%20MMTryon%20uses%20a%20parsing-free%0Agarment%20encoder%20and%20leverages%20a%20novel%20scalable%20data%20generation%20pipeline%20to%0Aconvert%20existing%20VITON%20datasets%20to%20a%20form%20that%20allows%20MMTryon%20to%20be%20trained%0Awithout%20requiring%20any%20explicit%20segmentation.%20Extensive%20experiments%20on%0Ahigh-resolution%20benchmarks%20and%20in-the-wild%20test%20sets%20demonstrate%20MMTryon%27s%0Asuperiority%20over%20existing%20SOTA%20methods%20both%20qualitatively%20and%20quantitatively.%0ABesides%2C%20MMTryon%27s%20impressive%20performance%20on%20multi-items%20and%20style-controllable%0Avirtual%20try-on%20scenarios%20and%20its%20ability%20to%20try%20on%20any%20outfit%20in%20a%20large%0Avariety%20of%20scenarios%20from%20any%20source%20image%2C%20opens%20up%20a%20new%20avenue%20for%20future%0Ainvestigation%20in%20the%20fashion%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00448v1&entry.124074799=Read"},
{"title": "Watching Grass Grow: Long-term Visual Navigation and Mission Planning\n  for Autonomous Biodiversity Monitoring", "author": "Matthew Gadd and Daniele De Martini and Luke Pitt and Wayne Tubby and Matthew Towlson and Chris Prahacs and Oliver Bartlett and John Jackson and Man Qi and Paul Newman and Andrew Hector and Roberto Salguero-G\u00f3mez and Nick Hawes", "abstract": "  We describe a challenging robotics deployment in a complex ecosystem to\nmonitor a rich plant community. The study site is dominated by dynamic\ngrassland vegetation and is thus visually ambiguous and liable to drastic\nappearance change over the course of a day and especially through the growing\nseason. This dynamism and complexity in appearance seriously impact the\nstability of the robotics platform, as localisation is a foundational part of\nthat control loop, and so routes must be carefully taught and retaught until\nautonomy is robust and repeatable. Our system is demonstrated over a 6-week\nperiod monitoring the response of grass species to experimental climate change\nmanipulations. We also discuss the applicability of our pipeline to monitor\nbiodiversity in other complex natural settings.\n", "link": "http://arxiv.org/abs/2404.10446v2", "date": "2024-05-01", "relevancy": 1.6694, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5816}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5528}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5479}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Watching%20Grass%20Grow%3A%20Long-term%20Visual%20Navigation%20and%20Mission%20Planning%0A%20%20for%20Autonomous%20Biodiversity%20Monitoring&body=Title%3A%20Watching%20Grass%20Grow%3A%20Long-term%20Visual%20Navigation%20and%20Mission%20Planning%0A%20%20for%20Autonomous%20Biodiversity%20Monitoring%0AAuthor%3A%20Matthew%20Gadd%20and%20Daniele%20De%20Martini%20and%20Luke%20Pitt%20and%20Wayne%20Tubby%20and%20Matthew%20Towlson%20and%20Chris%20Prahacs%20and%20Oliver%20Bartlett%20and%20John%20Jackson%20and%20Man%20Qi%20and%20Paul%20Newman%20and%20Andrew%20Hector%20and%20Roberto%20Salguero-G%C3%B3mez%20and%20Nick%20Hawes%0AAbstract%3A%20%20%20We%20describe%20a%20challenging%20robotics%20deployment%20in%20a%20complex%20ecosystem%20to%0Amonitor%20a%20rich%20plant%20community.%20The%20study%20site%20is%20dominated%20by%20dynamic%0Agrassland%20vegetation%20and%20is%20thus%20visually%20ambiguous%20and%20liable%20to%20drastic%0Aappearance%20change%20over%20the%20course%20of%20a%20day%20and%20especially%20through%20the%20growing%0Aseason.%20This%20dynamism%20and%20complexity%20in%20appearance%20seriously%20impact%20the%0Astability%20of%20the%20robotics%20platform%2C%20as%20localisation%20is%20a%20foundational%20part%20of%0Athat%20control%20loop%2C%20and%20so%20routes%20must%20be%20carefully%20taught%20and%20retaught%20until%0Aautonomy%20is%20robust%20and%20repeatable.%20Our%20system%20is%20demonstrated%20over%20a%206-week%0Aperiod%20monitoring%20the%20response%20of%20grass%20species%20to%20experimental%20climate%20change%0Amanipulations.%20We%20also%20discuss%20the%20applicability%20of%20our%20pipeline%20to%20monitor%0Abiodiversity%20in%20other%20complex%20natural%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10446v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Watching%20Grass%20Grow%3A%20Long-term%20Visual%20Navigation%20and%20Mission%20Planning%0A%20%20for%20Autonomous%20Biodiversity%20Monitoring&entry.906535625=Matthew%20Gadd%20and%20Daniele%20De%20Martini%20and%20Luke%20Pitt%20and%20Wayne%20Tubby%20and%20Matthew%20Towlson%20and%20Chris%20Prahacs%20and%20Oliver%20Bartlett%20and%20John%20Jackson%20and%20Man%20Qi%20and%20Paul%20Newman%20and%20Andrew%20Hector%20and%20Roberto%20Salguero-G%C3%B3mez%20and%20Nick%20Hawes&entry.1292438233=%20%20We%20describe%20a%20challenging%20robotics%20deployment%20in%20a%20complex%20ecosystem%20to%0Amonitor%20a%20rich%20plant%20community.%20The%20study%20site%20is%20dominated%20by%20dynamic%0Agrassland%20vegetation%20and%20is%20thus%20visually%20ambiguous%20and%20liable%20to%20drastic%0Aappearance%20change%20over%20the%20course%20of%20a%20day%20and%20especially%20through%20the%20growing%0Aseason.%20This%20dynamism%20and%20complexity%20in%20appearance%20seriously%20impact%20the%0Astability%20of%20the%20robotics%20platform%2C%20as%20localisation%20is%20a%20foundational%20part%20of%0Athat%20control%20loop%2C%20and%20so%20routes%20must%20be%20carefully%20taught%20and%20retaught%20until%0Aautonomy%20is%20robust%20and%20repeatable.%20Our%20system%20is%20demonstrated%20over%20a%206-week%0Aperiod%20monitoring%20the%20response%20of%20grass%20species%20to%20experimental%20climate%20change%0Amanipulations.%20We%20also%20discuss%20the%20applicability%20of%20our%20pipeline%20to%20monitor%0Abiodiversity%20in%20other%20complex%20natural%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10446v2&entry.124074799=Read"},
{"title": "Fuzzy Intelligent System for Student Software Project Evaluation", "author": "Anna Ogorodova and Pakizar Shamoi and Aron Karatayev", "abstract": "  Developing software projects allows students to put knowledge into practice\nand gain teamwork skills. However, assessing student performance in\nproject-oriented courses poses significant challenges, particularly as the size\nof classes increases. The current paper introduces a fuzzy intelligent system\ndesigned to evaluate academic software projects using object-oriented\nprogramming and design course as an example. To establish evaluation criteria,\nwe first conducted a survey of student project teams (n=31) and faculty (n=3)\nto identify key parameters and their applicable ranges. The selected criteria -\nclean code, use of inheritance, and functionality - were selected as essential\nfor assessing the quality of academic software projects. These criteria were\nthen represented as fuzzy variables with corresponding fuzzy sets.\nCollaborating with three experts, including one professor and two course\ninstructors, we defined a set of fuzzy rules for a fuzzy inference system. This\nsystem processes the input criteria to produce a quantifiable measure of\nproject success. The system demonstrated promising results in automating the\nevaluation of projects. Our approach standardizes project evaluations and helps\nto reduce the subjective bias in manual grading.\n", "link": "http://arxiv.org/abs/2405.00453v1", "date": "2024-05-01", "relevancy": 1.6544, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4336}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4221}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3902}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fuzzy%20Intelligent%20System%20for%20Student%20Software%20Project%20Evaluation&body=Title%3A%20Fuzzy%20Intelligent%20System%20for%20Student%20Software%20Project%20Evaluation%0AAuthor%3A%20Anna%20Ogorodova%20and%20Pakizar%20Shamoi%20and%20Aron%20Karatayev%0AAbstract%3A%20%20%20Developing%20software%20projects%20allows%20students%20to%20put%20knowledge%20into%20practice%0Aand%20gain%20teamwork%20skills.%20However%2C%20assessing%20student%20performance%20in%0Aproject-oriented%20courses%20poses%20significant%20challenges%2C%20particularly%20as%20the%20size%0Aof%20classes%20increases.%20The%20current%20paper%20introduces%20a%20fuzzy%20intelligent%20system%0Adesigned%20to%20evaluate%20academic%20software%20projects%20using%20object-oriented%0Aprogramming%20and%20design%20course%20as%20an%20example.%20To%20establish%20evaluation%20criteria%2C%0Awe%20first%20conducted%20a%20survey%20of%20student%20project%20teams%20%28n%3D31%29%20and%20faculty%20%28n%3D3%29%0Ato%20identify%20key%20parameters%20and%20their%20applicable%20ranges.%20The%20selected%20criteria%20-%0Aclean%20code%2C%20use%20of%20inheritance%2C%20and%20functionality%20-%20were%20selected%20as%20essential%0Afor%20assessing%20the%20quality%20of%20academic%20software%20projects.%20These%20criteria%20were%0Athen%20represented%20as%20fuzzy%20variables%20with%20corresponding%20fuzzy%20sets.%0ACollaborating%20with%20three%20experts%2C%20including%20one%20professor%20and%20two%20course%0Ainstructors%2C%20we%20defined%20a%20set%20of%20fuzzy%20rules%20for%20a%20fuzzy%20inference%20system.%20This%0Asystem%20processes%20the%20input%20criteria%20to%20produce%20a%20quantifiable%20measure%20of%0Aproject%20success.%20The%20system%20demonstrated%20promising%20results%20in%20automating%20the%0Aevaluation%20of%20projects.%20Our%20approach%20standardizes%20project%20evaluations%20and%20helps%0Ato%20reduce%20the%20subjective%20bias%20in%20manual%20grading.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00453v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fuzzy%20Intelligent%20System%20for%20Student%20Software%20Project%20Evaluation&entry.906535625=Anna%20Ogorodova%20and%20Pakizar%20Shamoi%20and%20Aron%20Karatayev&entry.1292438233=%20%20Developing%20software%20projects%20allows%20students%20to%20put%20knowledge%20into%20practice%0Aand%20gain%20teamwork%20skills.%20However%2C%20assessing%20student%20performance%20in%0Aproject-oriented%20courses%20poses%20significant%20challenges%2C%20particularly%20as%20the%20size%0Aof%20classes%20increases.%20The%20current%20paper%20introduces%20a%20fuzzy%20intelligent%20system%0Adesigned%20to%20evaluate%20academic%20software%20projects%20using%20object-oriented%0Aprogramming%20and%20design%20course%20as%20an%20example.%20To%20establish%20evaluation%20criteria%2C%0Awe%20first%20conducted%20a%20survey%20of%20student%20project%20teams%20%28n%3D31%29%20and%20faculty%20%28n%3D3%29%0Ato%20identify%20key%20parameters%20and%20their%20applicable%20ranges.%20The%20selected%20criteria%20-%0Aclean%20code%2C%20use%20of%20inheritance%2C%20and%20functionality%20-%20were%20selected%20as%20essential%0Afor%20assessing%20the%20quality%20of%20academic%20software%20projects.%20These%20criteria%20were%0Athen%20represented%20as%20fuzzy%20variables%20with%20corresponding%20fuzzy%20sets.%0ACollaborating%20with%20three%20experts%2C%20including%20one%20professor%20and%20two%20course%0Ainstructors%2C%20we%20defined%20a%20set%20of%20fuzzy%20rules%20for%20a%20fuzzy%20inference%20system.%20This%0Asystem%20processes%20the%20input%20criteria%20to%20produce%20a%20quantifiable%20measure%20of%0Aproject%20success.%20The%20system%20demonstrated%20promising%20results%20in%20automating%20the%0Aevaluation%20of%20projects.%20Our%20approach%20standardizes%20project%20evaluations%20and%20helps%0Ato%20reduce%20the%20subjective%20bias%20in%20manual%20grading.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00453v1&entry.124074799=Read"},
{"title": "Closed-Loop Koopman Operator Approximation", "author": "Steven Dahdah and James Richard Forbes", "abstract": "  This paper proposes a method to identify a Koopman model of a\nfeedback-controlled system given a known controller. The Koopman operator\nallows a nonlinear system to be rewritten as an infinite-dimensional linear\nsystem by viewing it in terms of an infinite set of lifting functions. A\nfinite-dimensional approximation of the Koopman operator can be identified from\ndata by choosing a finite subset of lifting functions and solving a regression\nproblem in the lifted space. Existing methods are designed to identify\nopen-loop systems. However, it is impractical or impossible to run experiments\non some systems, such as unstable systems, in an open-loop fashion. The\nproposed method leverages the linearity of the Koopman operator, along with\nknowledge of the controller and the structure of the closed-loop system, to\nsimultaneously identify the closed-loop and plant systems. The advantages of\nthe proposed closed-loop Koopman operator approximation method are demonstrated\nin simulation using a Duffing oscillator and experimentally using a rotary\ninverted pendulum system. An open-source software implementation of the\nproposed method is publicly available, along with the experimental dataset\ngenerated for this paper.\n", "link": "http://arxiv.org/abs/2303.15318v3", "date": "2024-05-01", "relevancy": 1.6526, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4644}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4156}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3902}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Closed-Loop%20Koopman%20Operator%20Approximation&body=Title%3A%20Closed-Loop%20Koopman%20Operator%20Approximation%0AAuthor%3A%20Steven%20Dahdah%20and%20James%20Richard%20Forbes%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20method%20to%20identify%20a%20Koopman%20model%20of%20a%0Afeedback-controlled%20system%20given%20a%20known%20controller.%20The%20Koopman%20operator%0Aallows%20a%20nonlinear%20system%20to%20be%20rewritten%20as%20an%20infinite-dimensional%20linear%0Asystem%20by%20viewing%20it%20in%20terms%20of%20an%20infinite%20set%20of%20lifting%20functions.%20A%0Afinite-dimensional%20approximation%20of%20the%20Koopman%20operator%20can%20be%20identified%20from%0Adata%20by%20choosing%20a%20finite%20subset%20of%20lifting%20functions%20and%20solving%20a%20regression%0Aproblem%20in%20the%20lifted%20space.%20Existing%20methods%20are%20designed%20to%20identify%0Aopen-loop%20systems.%20However%2C%20it%20is%20impractical%20or%20impossible%20to%20run%20experiments%0Aon%20some%20systems%2C%20such%20as%20unstable%20systems%2C%20in%20an%20open-loop%20fashion.%20The%0Aproposed%20method%20leverages%20the%20linearity%20of%20the%20Koopman%20operator%2C%20along%20with%0Aknowledge%20of%20the%20controller%20and%20the%20structure%20of%20the%20closed-loop%20system%2C%20to%0Asimultaneously%20identify%20the%20closed-loop%20and%20plant%20systems.%20The%20advantages%20of%0Athe%20proposed%20closed-loop%20Koopman%20operator%20approximation%20method%20are%20demonstrated%0Ain%20simulation%20using%20a%20Duffing%20oscillator%20and%20experimentally%20using%20a%20rotary%0Ainverted%20pendulum%20system.%20An%20open-source%20software%20implementation%20of%20the%0Aproposed%20method%20is%20publicly%20available%2C%20along%20with%20the%20experimental%20dataset%0Agenerated%20for%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15318v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Loop%20Koopman%20Operator%20Approximation&entry.906535625=Steven%20Dahdah%20and%20James%20Richard%20Forbes&entry.1292438233=%20%20This%20paper%20proposes%20a%20method%20to%20identify%20a%20Koopman%20model%20of%20a%0Afeedback-controlled%20system%20given%20a%20known%20controller.%20The%20Koopman%20operator%0Aallows%20a%20nonlinear%20system%20to%20be%20rewritten%20as%20an%20infinite-dimensional%20linear%0Asystem%20by%20viewing%20it%20in%20terms%20of%20an%20infinite%20set%20of%20lifting%20functions.%20A%0Afinite-dimensional%20approximation%20of%20the%20Koopman%20operator%20can%20be%20identified%20from%0Adata%20by%20choosing%20a%20finite%20subset%20of%20lifting%20functions%20and%20solving%20a%20regression%0Aproblem%20in%20the%20lifted%20space.%20Existing%20methods%20are%20designed%20to%20identify%0Aopen-loop%20systems.%20However%2C%20it%20is%20impractical%20or%20impossible%20to%20run%20experiments%0Aon%20some%20systems%2C%20such%20as%20unstable%20systems%2C%20in%20an%20open-loop%20fashion.%20The%0Aproposed%20method%20leverages%20the%20linearity%20of%20the%20Koopman%20operator%2C%20along%20with%0Aknowledge%20of%20the%20controller%20and%20the%20structure%20of%20the%20closed-loop%20system%2C%20to%0Asimultaneously%20identify%20the%20closed-loop%20and%20plant%20systems.%20The%20advantages%20of%0Athe%20proposed%20closed-loop%20Koopman%20operator%20approximation%20method%20are%20demonstrated%0Ain%20simulation%20using%20a%20Duffing%20oscillator%20and%20experimentally%20using%20a%20rotary%0Ainverted%20pendulum%20system.%20An%20open-source%20software%20implementation%20of%20the%0Aproposed%20method%20is%20publicly%20available%2C%20along%20with%20the%20experimental%20dataset%0Agenerated%20for%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15318v3&entry.124074799=Read"},
{"title": "EALD-MLLM: Emotion Analysis in Long-sequential and De-identity videos\n  with Multi-modal Large Language Model", "author": "Deng Li and Xin Liu and Bohao Xing and Baiqiang Xia and Yuan Zong and Bihan Wen and Heikki K\u00e4lvi\u00e4inen", "abstract": "  Emotion AI is the ability of computers to understand human emotional states.\nExisting works have achieved promising progress, but two limitations remain to\nbe solved: 1) Previous studies have been more focused on short sequential video\nemotion analysis while overlooking long sequential video. However, the emotions\nin short sequential videos only reflect instantaneous emotions, which may be\ndeliberately guided or hidden. In contrast, long sequential videos can reveal\nauthentic emotions; 2) Previous studies commonly utilize various signals such\nas facial, speech, and even sensitive biological signals (e.g.,\nelectrocardiogram). However, due to the increasing demand for privacy,\ndeveloping Emotion AI without relying on sensitive signals is becoming\nimportant. To address the aforementioned limitations, in this paper, we\nconstruct a dataset for Emotion Analysis in Long-sequential and De-identity\nvideos called EALD by collecting and processing the sequences of athletes'\npost-match interviews. In addition to providing annotations of the overall\nemotional state of each video, we also provide the Non-Facial Body Language\n(NFBL) annotations for each player. NFBL is an inner-driven emotional\nexpression and can serve as an identity-free clue to understanding the\nemotional state. Moreover, we provide a simple but effective baseline for\nfurther research. More precisely, we evaluate the Multimodal Large Language\nModels (MLLMs) with de-identification signals (e.g., visual, speech, and NFBLs)\nto perform emotion analysis. Our experimental results demonstrate that: 1)\nMLLMs can achieve comparable, even better performance than the supervised\nsingle-modal models, even in a zero-shot scenario; 2) NFBL is an important cue\nin long sequential emotion analysis. EALD will be available on the open-source\nplatform.\n", "link": "http://arxiv.org/abs/2405.00574v1", "date": "2024-05-01", "relevancy": 1.6512, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5815}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EALD-MLLM%3A%20Emotion%20Analysis%20in%20Long-sequential%20and%20De-identity%20videos%0A%20%20with%20Multi-modal%20Large%20Language%20Model&body=Title%3A%20EALD-MLLM%3A%20Emotion%20Analysis%20in%20Long-sequential%20and%20De-identity%20videos%0A%20%20with%20Multi-modal%20Large%20Language%20Model%0AAuthor%3A%20Deng%20Li%20and%20Xin%20Liu%20and%20Bohao%20Xing%20and%20Baiqiang%20Xia%20and%20Yuan%20Zong%20and%20Bihan%20Wen%20and%20Heikki%20K%C3%A4lvi%C3%A4inen%0AAbstract%3A%20%20%20Emotion%20AI%20is%20the%20ability%20of%20computers%20to%20understand%20human%20emotional%20states.%0AExisting%20works%20have%20achieved%20promising%20progress%2C%20but%20two%20limitations%20remain%20to%0Abe%20solved%3A%201%29%20Previous%20studies%20have%20been%20more%20focused%20on%20short%20sequential%20video%0Aemotion%20analysis%20while%20overlooking%20long%20sequential%20video.%20However%2C%20the%20emotions%0Ain%20short%20sequential%20videos%20only%20reflect%20instantaneous%20emotions%2C%20which%20may%20be%0Adeliberately%20guided%20or%20hidden.%20In%20contrast%2C%20long%20sequential%20videos%20can%20reveal%0Aauthentic%20emotions%3B%202%29%20Previous%20studies%20commonly%20utilize%20various%20signals%20such%0Aas%20facial%2C%20speech%2C%20and%20even%20sensitive%20biological%20signals%20%28e.g.%2C%0Aelectrocardiogram%29.%20However%2C%20due%20to%20the%20increasing%20demand%20for%20privacy%2C%0Adeveloping%20Emotion%20AI%20without%20relying%20on%20sensitive%20signals%20is%20becoming%0Aimportant.%20To%20address%20the%20aforementioned%20limitations%2C%20in%20this%20paper%2C%20we%0Aconstruct%20a%20dataset%20for%20Emotion%20Analysis%20in%20Long-sequential%20and%20De-identity%0Avideos%20called%20EALD%20by%20collecting%20and%20processing%20the%20sequences%20of%20athletes%27%0Apost-match%20interviews.%20In%20addition%20to%20providing%20annotations%20of%20the%20overall%0Aemotional%20state%20of%20each%20video%2C%20we%20also%20provide%20the%20Non-Facial%20Body%20Language%0A%28NFBL%29%20annotations%20for%20each%20player.%20NFBL%20is%20an%20inner-driven%20emotional%0Aexpression%20and%20can%20serve%20as%20an%20identity-free%20clue%20to%20understanding%20the%0Aemotional%20state.%20Moreover%2C%20we%20provide%20a%20simple%20but%20effective%20baseline%20for%0Afurther%20research.%20More%20precisely%2C%20we%20evaluate%20the%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20with%20de-identification%20signals%20%28e.g.%2C%20visual%2C%20speech%2C%20and%20NFBLs%29%0Ato%20perform%20emotion%20analysis.%20Our%20experimental%20results%20demonstrate%20that%3A%201%29%0AMLLMs%20can%20achieve%20comparable%2C%20even%20better%20performance%20than%20the%20supervised%0Asingle-modal%20models%2C%20even%20in%20a%20zero-shot%20scenario%3B%202%29%20NFBL%20is%20an%20important%20cue%0Ain%20long%20sequential%20emotion%20analysis.%20EALD%20will%20be%20available%20on%20the%20open-source%0Aplatform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00574v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EALD-MLLM%3A%20Emotion%20Analysis%20in%20Long-sequential%20and%20De-identity%20videos%0A%20%20with%20Multi-modal%20Large%20Language%20Model&entry.906535625=Deng%20Li%20and%20Xin%20Liu%20and%20Bohao%20Xing%20and%20Baiqiang%20Xia%20and%20Yuan%20Zong%20and%20Bihan%20Wen%20and%20Heikki%20K%C3%A4lvi%C3%A4inen&entry.1292438233=%20%20Emotion%20AI%20is%20the%20ability%20of%20computers%20to%20understand%20human%20emotional%20states.%0AExisting%20works%20have%20achieved%20promising%20progress%2C%20but%20two%20limitations%20remain%20to%0Abe%20solved%3A%201%29%20Previous%20studies%20have%20been%20more%20focused%20on%20short%20sequential%20video%0Aemotion%20analysis%20while%20overlooking%20long%20sequential%20video.%20However%2C%20the%20emotions%0Ain%20short%20sequential%20videos%20only%20reflect%20instantaneous%20emotions%2C%20which%20may%20be%0Adeliberately%20guided%20or%20hidden.%20In%20contrast%2C%20long%20sequential%20videos%20can%20reveal%0Aauthentic%20emotions%3B%202%29%20Previous%20studies%20commonly%20utilize%20various%20signals%20such%0Aas%20facial%2C%20speech%2C%20and%20even%20sensitive%20biological%20signals%20%28e.g.%2C%0Aelectrocardiogram%29.%20However%2C%20due%20to%20the%20increasing%20demand%20for%20privacy%2C%0Adeveloping%20Emotion%20AI%20without%20relying%20on%20sensitive%20signals%20is%20becoming%0Aimportant.%20To%20address%20the%20aforementioned%20limitations%2C%20in%20this%20paper%2C%20we%0Aconstruct%20a%20dataset%20for%20Emotion%20Analysis%20in%20Long-sequential%20and%20De-identity%0Avideos%20called%20EALD%20by%20collecting%20and%20processing%20the%20sequences%20of%20athletes%27%0Apost-match%20interviews.%20In%20addition%20to%20providing%20annotations%20of%20the%20overall%0Aemotional%20state%20of%20each%20video%2C%20we%20also%20provide%20the%20Non-Facial%20Body%20Language%0A%28NFBL%29%20annotations%20for%20each%20player.%20NFBL%20is%20an%20inner-driven%20emotional%0Aexpression%20and%20can%20serve%20as%20an%20identity-free%20clue%20to%20understanding%20the%0Aemotional%20state.%20Moreover%2C%20we%20provide%20a%20simple%20but%20effective%20baseline%20for%0Afurther%20research.%20More%20precisely%2C%20we%20evaluate%20the%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20with%20de-identification%20signals%20%28e.g.%2C%20visual%2C%20speech%2C%20and%20NFBLs%29%0Ato%20perform%20emotion%20analysis.%20Our%20experimental%20results%20demonstrate%20that%3A%201%29%0AMLLMs%20can%20achieve%20comparable%2C%20even%20better%20performance%20than%20the%20supervised%0Asingle-modal%20models%2C%20even%20in%20a%20zero-shot%20scenario%3B%202%29%20NFBL%20is%20an%20important%20cue%0Ain%20long%20sequential%20emotion%20analysis.%20EALD%20will%20be%20available%20on%20the%20open-source%0Aplatform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00574v1&entry.124074799=Read"},
{"title": "Predictive Accuracy-Based Active Learning for Medical Image Segmentation", "author": "Jun Shi and Shulan Ruan and Ziqi Zhu and Minfan Zhao and Hong An and Xudong Xue and Bing Yan", "abstract": "  Active learning is considered a viable solution to alleviate the\ncontradiction between the high dependency of deep learning-based segmentation\nmethods on annotated data and the expensive pixel-level annotation cost of\nmedical images. However, most existing methods suffer from unreliable\nuncertainty assessment and the struggle to balance diversity and\ninformativeness, leading to poor performance in segmentation tasks. In\nresponse, we propose an efficient Predictive Accuracy-based Active Learning\n(PAAL) method for medical image segmentation, first introducing predictive\naccuracy to define uncertainty. Specifically, PAAL mainly consists of an\nAccuracy Predictor (AP) and a Weighted Polling Strategy (WPS). The former is an\nattached learnable module that can accurately predict the segmentation accuracy\nof unlabeled samples relative to the target model with the predicted posterior\nprobability. The latter provides an efficient hybrid querying scheme by\ncombining predicted accuracy and feature representation, aiming to ensure the\nuncertainty and diversity of the acquired samples. Extensive experiment results\non multiple datasets demonstrate the superiority of PAAL. PAAL achieves\ncomparable accuracy to fully annotated data while reducing annotation costs by\napproximately 50% to 80%, showcasing significant potential in clinical\napplications. The code is available at https://github.com/shijun18/PAAL-MedSeg.\n", "link": "http://arxiv.org/abs/2405.00452v1", "date": "2024-05-01", "relevancy": 1.6392, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6047}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.542}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Predictive%20Accuracy-Based%20Active%20Learning%20for%20Medical%20Image%20Segmentation&body=Title%3A%20Predictive%20Accuracy-Based%20Active%20Learning%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Jun%20Shi%20and%20Shulan%20Ruan%20and%20Ziqi%20Zhu%20and%20Minfan%20Zhao%20and%20Hong%20An%20and%20Xudong%20Xue%20and%20Bing%20Yan%0AAbstract%3A%20%20%20Active%20learning%20is%20considered%20a%20viable%20solution%20to%20alleviate%20the%0Acontradiction%20between%20the%20high%20dependency%20of%20deep%20learning-based%20segmentation%0Amethods%20on%20annotated%20data%20and%20the%20expensive%20pixel-level%20annotation%20cost%20of%0Amedical%20images.%20However%2C%20most%20existing%20methods%20suffer%20from%20unreliable%0Auncertainty%20assessment%20and%20the%20struggle%20to%20balance%20diversity%20and%0Ainformativeness%2C%20leading%20to%20poor%20performance%20in%20segmentation%20tasks.%20In%0Aresponse%2C%20we%20propose%20an%20efficient%20Predictive%20Accuracy-based%20Active%20Learning%0A%28PAAL%29%20method%20for%20medical%20image%20segmentation%2C%20first%20introducing%20predictive%0Aaccuracy%20to%20define%20uncertainty.%20Specifically%2C%20PAAL%20mainly%20consists%20of%20an%0AAccuracy%20Predictor%20%28AP%29%20and%20a%20Weighted%20Polling%20Strategy%20%28WPS%29.%20The%20former%20is%20an%0Aattached%20learnable%20module%20that%20can%20accurately%20predict%20the%20segmentation%20accuracy%0Aof%20unlabeled%20samples%20relative%20to%20the%20target%20model%20with%20the%20predicted%20posterior%0Aprobability.%20The%20latter%20provides%20an%20efficient%20hybrid%20querying%20scheme%20by%0Acombining%20predicted%20accuracy%20and%20feature%20representation%2C%20aiming%20to%20ensure%20the%0Auncertainty%20and%20diversity%20of%20the%20acquired%20samples.%20Extensive%20experiment%20results%0Aon%20multiple%20datasets%20demonstrate%20the%20superiority%20of%20PAAL.%20PAAL%20achieves%0Acomparable%20accuracy%20to%20fully%20annotated%20data%20while%20reducing%20annotation%20costs%20by%0Aapproximately%2050%25%20to%2080%25%2C%20showcasing%20significant%20potential%20in%20clinical%0Aapplications.%20The%20code%20is%20available%20at%20https%3A//github.com/shijun18/PAAL-MedSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00452v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Accuracy-Based%20Active%20Learning%20for%20Medical%20Image%20Segmentation&entry.906535625=Jun%20Shi%20and%20Shulan%20Ruan%20and%20Ziqi%20Zhu%20and%20Minfan%20Zhao%20and%20Hong%20An%20and%20Xudong%20Xue%20and%20Bing%20Yan&entry.1292438233=%20%20Active%20learning%20is%20considered%20a%20viable%20solution%20to%20alleviate%20the%0Acontradiction%20between%20the%20high%20dependency%20of%20deep%20learning-based%20segmentation%0Amethods%20on%20annotated%20data%20and%20the%20expensive%20pixel-level%20annotation%20cost%20of%0Amedical%20images.%20However%2C%20most%20existing%20methods%20suffer%20from%20unreliable%0Auncertainty%20assessment%20and%20the%20struggle%20to%20balance%20diversity%20and%0Ainformativeness%2C%20leading%20to%20poor%20performance%20in%20segmentation%20tasks.%20In%0Aresponse%2C%20we%20propose%20an%20efficient%20Predictive%20Accuracy-based%20Active%20Learning%0A%28PAAL%29%20method%20for%20medical%20image%20segmentation%2C%20first%20introducing%20predictive%0Aaccuracy%20to%20define%20uncertainty.%20Specifically%2C%20PAAL%20mainly%20consists%20of%20an%0AAccuracy%20Predictor%20%28AP%29%20and%20a%20Weighted%20Polling%20Strategy%20%28WPS%29.%20The%20former%20is%20an%0Aattached%20learnable%20module%20that%20can%20accurately%20predict%20the%20segmentation%20accuracy%0Aof%20unlabeled%20samples%20relative%20to%20the%20target%20model%20with%20the%20predicted%20posterior%0Aprobability.%20The%20latter%20provides%20an%20efficient%20hybrid%20querying%20scheme%20by%0Acombining%20predicted%20accuracy%20and%20feature%20representation%2C%20aiming%20to%20ensure%20the%0Auncertainty%20and%20diversity%20of%20the%20acquired%20samples.%20Extensive%20experiment%20results%0Aon%20multiple%20datasets%20demonstrate%20the%20superiority%20of%20PAAL.%20PAAL%20achieves%0Acomparable%20accuracy%20to%20fully%20annotated%20data%20while%20reducing%20annotation%20costs%20by%0Aapproximately%2050%25%20to%2080%25%2C%20showcasing%20significant%20potential%20in%20clinical%0Aapplications.%20The%20code%20is%20available%20at%20https%3A//github.com/shijun18/PAAL-MedSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00452v1&entry.124074799=Read"},
{"title": "U-Nets as Belief Propagation: Efficient Classification, Denoising, and\n  Diffusion in Generative Hierarchical Models", "author": "Song Mei", "abstract": "  U-Nets are among the most widely used architectures in computer vision,\nrenowned for their exceptional performance in applications such as image\nsegmentation, denoising, and diffusion modeling. However, a theoretical\nexplanation of the U-Net architecture design has not yet been fully\nestablished.\n  This paper introduces a novel interpretation of the U-Net architecture by\nstudying certain generative hierarchical models, which are tree-structured\ngraphical models extensively utilized in both language and image domains. With\ntheir encoder-decoder structure, long skip connections, and pooling and\nup-sampling layers, we demonstrate how U-Nets can naturally implement the\nbelief propagation denoising algorithm in such generative hierarchical models,\nthereby efficiently approximating the denoising functions. This leads to an\nefficient sample complexity bound for learning the denoising function using\nU-Nets within these models. Additionally, we discuss the broader implications\nof these findings for diffusion models in generative hierarchical models. We\nalso demonstrate that the conventional architecture of convolutional neural\nnetworks (ConvNets) is ideally suited for classification tasks within these\nmodels. This offers a unified view of the roles of ConvNets and U-Nets,\nhighlighting the versatility of generative hierarchical models in modeling\ncomplex data distributions across language and image domains.\n", "link": "http://arxiv.org/abs/2404.18444v2", "date": "2024-05-01", "relevancy": 1.6363, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5532}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5439}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5429}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20U-Nets%20as%20Belief%20Propagation%3A%20Efficient%20Classification%2C%20Denoising%2C%20and%0A%20%20Diffusion%20in%20Generative%20Hierarchical%20Models&body=Title%3A%20U-Nets%20as%20Belief%20Propagation%3A%20Efficient%20Classification%2C%20Denoising%2C%20and%0A%20%20Diffusion%20in%20Generative%20Hierarchical%20Models%0AAuthor%3A%20Song%20Mei%0AAbstract%3A%20%20%20U-Nets%20are%20among%20the%20most%20widely%20used%20architectures%20in%20computer%20vision%2C%0Arenowned%20for%20their%20exceptional%20performance%20in%20applications%20such%20as%20image%0Asegmentation%2C%20denoising%2C%20and%20diffusion%20modeling.%20However%2C%20a%20theoretical%0Aexplanation%20of%20the%20U-Net%20architecture%20design%20has%20not%20yet%20been%20fully%0Aestablished.%0A%20%20This%20paper%20introduces%20a%20novel%20interpretation%20of%20the%20U-Net%20architecture%20by%0Astudying%20certain%20generative%20hierarchical%20models%2C%20which%20are%20tree-structured%0Agraphical%20models%20extensively%20utilized%20in%20both%20language%20and%20image%20domains.%20With%0Atheir%20encoder-decoder%20structure%2C%20long%20skip%20connections%2C%20and%20pooling%20and%0Aup-sampling%20layers%2C%20we%20demonstrate%20how%20U-Nets%20can%20naturally%20implement%20the%0Abelief%20propagation%20denoising%20algorithm%20in%20such%20generative%20hierarchical%20models%2C%0Athereby%20efficiently%20approximating%20the%20denoising%20functions.%20This%20leads%20to%20an%0Aefficient%20sample%20complexity%20bound%20for%20learning%20the%20denoising%20function%20using%0AU-Nets%20within%20these%20models.%20Additionally%2C%20we%20discuss%20the%20broader%20implications%0Aof%20these%20findings%20for%20diffusion%20models%20in%20generative%20hierarchical%20models.%20We%0Aalso%20demonstrate%20that%20the%20conventional%20architecture%20of%20convolutional%20neural%0Anetworks%20%28ConvNets%29%20is%20ideally%20suited%20for%20classification%20tasks%20within%20these%0Amodels.%20This%20offers%20a%20unified%20view%20of%20the%20roles%20of%20ConvNets%20and%20U-Nets%2C%0Ahighlighting%20the%20versatility%20of%20generative%20hierarchical%20models%20in%20modeling%0Acomplex%20data%20distributions%20across%20language%20and%20image%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18444v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-Nets%20as%20Belief%20Propagation%3A%20Efficient%20Classification%2C%20Denoising%2C%20and%0A%20%20Diffusion%20in%20Generative%20Hierarchical%20Models&entry.906535625=Song%20Mei&entry.1292438233=%20%20U-Nets%20are%20among%20the%20most%20widely%20used%20architectures%20in%20computer%20vision%2C%0Arenowned%20for%20their%20exceptional%20performance%20in%20applications%20such%20as%20image%0Asegmentation%2C%20denoising%2C%20and%20diffusion%20modeling.%20However%2C%20a%20theoretical%0Aexplanation%20of%20the%20U-Net%20architecture%20design%20has%20not%20yet%20been%20fully%0Aestablished.%0A%20%20This%20paper%20introduces%20a%20novel%20interpretation%20of%20the%20U-Net%20architecture%20by%0Astudying%20certain%20generative%20hierarchical%20models%2C%20which%20are%20tree-structured%0Agraphical%20models%20extensively%20utilized%20in%20both%20language%20and%20image%20domains.%20With%0Atheir%20encoder-decoder%20structure%2C%20long%20skip%20connections%2C%20and%20pooling%20and%0Aup-sampling%20layers%2C%20we%20demonstrate%20how%20U-Nets%20can%20naturally%20implement%20the%0Abelief%20propagation%20denoising%20algorithm%20in%20such%20generative%20hierarchical%20models%2C%0Athereby%20efficiently%20approximating%20the%20denoising%20functions.%20This%20leads%20to%20an%0Aefficient%20sample%20complexity%20bound%20for%20learning%20the%20denoising%20function%20using%0AU-Nets%20within%20these%20models.%20Additionally%2C%20we%20discuss%20the%20broader%20implications%0Aof%20these%20findings%20for%20diffusion%20models%20in%20generative%20hierarchical%20models.%20We%0Aalso%20demonstrate%20that%20the%20conventional%20architecture%20of%20convolutional%20neural%0Anetworks%20%28ConvNets%29%20is%20ideally%20suited%20for%20classification%20tasks%20within%20these%0Amodels.%20This%20offers%20a%20unified%20view%20of%20the%20roles%20of%20ConvNets%20and%20U-Nets%2C%0Ahighlighting%20the%20versatility%20of%20generative%20hierarchical%20models%20in%20modeling%0Acomplex%20data%20distributions%20across%20language%20and%20image%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18444v2&entry.124074799=Read"},
{"title": "Cross-Validation Conformal Risk Control", "author": "Kfir M. Cohen and Sangwoo Park and Osvaldo Simeone and Shlomo Shamai", "abstract": "  Conformal risk control (CRC) is a recently proposed technique that applies\npost-hoc to a conventional point predictor to provide calibration guarantees.\nGeneralizing conformal prediction (CP), with CRC, calibration is ensured for a\nset predictor that is extracted from the point predictor to control a risk\nfunction such as the probability of miscoverage or the false negative rate. The\noriginal CRC requires the available data set to be split between training and\nvalidation data sets. This can be problematic when data availability is\nlimited, resulting in inefficient set predictors. In this paper, a novel CRC\nmethod is introduced that is based on cross-validation, rather than on\nvalidation as the original CRC. The proposed cross-validation CRC (CV-CRC)\nextends a version of the jackknife-minmax from CP to CRC, allowing for the\ncontrol of a broader range of risk functions. CV-CRC is proved to offer\ntheoretical guarantees on the average risk of the set predictor. Furthermore,\nnumerical experiments show that CV-CRC can reduce the average set size with\nrespect to CRC when the available data are limited.\n", "link": "http://arxiv.org/abs/2401.11974v2", "date": "2024-05-01", "relevancy": 1.6188, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4073}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.406}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4024}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-Validation%20Conformal%20Risk%20Control&body=Title%3A%20Cross-Validation%20Conformal%20Risk%20Control%0AAuthor%3A%20Kfir%20M.%20Cohen%20and%20Sangwoo%20Park%20and%20Osvaldo%20Simeone%20and%20Shlomo%20Shamai%0AAbstract%3A%20%20%20Conformal%20risk%20control%20%28CRC%29%20is%20a%20recently%20proposed%20technique%20that%20applies%0Apost-hoc%20to%20a%20conventional%20point%20predictor%20to%20provide%20calibration%20guarantees.%0AGeneralizing%20conformal%20prediction%20%28CP%29%2C%20with%20CRC%2C%20calibration%20is%20ensured%20for%20a%0Aset%20predictor%20that%20is%20extracted%20from%20the%20point%20predictor%20to%20control%20a%20risk%0Afunction%20such%20as%20the%20probability%20of%20miscoverage%20or%20the%20false%20negative%20rate.%20The%0Aoriginal%20CRC%20requires%20the%20available%20data%20set%20to%20be%20split%20between%20training%20and%0Avalidation%20data%20sets.%20This%20can%20be%20problematic%20when%20data%20availability%20is%0Alimited%2C%20resulting%20in%20inefficient%20set%20predictors.%20In%20this%20paper%2C%20a%20novel%20CRC%0Amethod%20is%20introduced%20that%20is%20based%20on%20cross-validation%2C%20rather%20than%20on%0Avalidation%20as%20the%20original%20CRC.%20The%20proposed%20cross-validation%20CRC%20%28CV-CRC%29%0Aextends%20a%20version%20of%20the%20jackknife-minmax%20from%20CP%20to%20CRC%2C%20allowing%20for%20the%0Acontrol%20of%20a%20broader%20range%20of%20risk%20functions.%20CV-CRC%20is%20proved%20to%20offer%0Atheoretical%20guarantees%20on%20the%20average%20risk%20of%20the%20set%20predictor.%20Furthermore%2C%0Anumerical%20experiments%20show%20that%20CV-CRC%20can%20reduce%20the%20average%20set%20size%20with%0Arespect%20to%20CRC%20when%20the%20available%20data%20are%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11974v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Validation%20Conformal%20Risk%20Control&entry.906535625=Kfir%20M.%20Cohen%20and%20Sangwoo%20Park%20and%20Osvaldo%20Simeone%20and%20Shlomo%20Shamai&entry.1292438233=%20%20Conformal%20risk%20control%20%28CRC%29%20is%20a%20recently%20proposed%20technique%20that%20applies%0Apost-hoc%20to%20a%20conventional%20point%20predictor%20to%20provide%20calibration%20guarantees.%0AGeneralizing%20conformal%20prediction%20%28CP%29%2C%20with%20CRC%2C%20calibration%20is%20ensured%20for%20a%0Aset%20predictor%20that%20is%20extracted%20from%20the%20point%20predictor%20to%20control%20a%20risk%0Afunction%20such%20as%20the%20probability%20of%20miscoverage%20or%20the%20false%20negative%20rate.%20The%0Aoriginal%20CRC%20requires%20the%20available%20data%20set%20to%20be%20split%20between%20training%20and%0Avalidation%20data%20sets.%20This%20can%20be%20problematic%20when%20data%20availability%20is%0Alimited%2C%20resulting%20in%20inefficient%20set%20predictors.%20In%20this%20paper%2C%20a%20novel%20CRC%0Amethod%20is%20introduced%20that%20is%20based%20on%20cross-validation%2C%20rather%20than%20on%0Avalidation%20as%20the%20original%20CRC.%20The%20proposed%20cross-validation%20CRC%20%28CV-CRC%29%0Aextends%20a%20version%20of%20the%20jackknife-minmax%20from%20CP%20to%20CRC%2C%20allowing%20for%20the%0Acontrol%20of%20a%20broader%20range%20of%20risk%20functions.%20CV-CRC%20is%20proved%20to%20offer%0Atheoretical%20guarantees%20on%20the%20average%20risk%20of%20the%20set%20predictor.%20Furthermore%2C%0Anumerical%20experiments%20show%20that%20CV-CRC%20can%20reduce%20the%20average%20set%20size%20with%0Arespect%20to%20CRC%20when%20the%20available%20data%20are%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11974v2&entry.124074799=Read"},
{"title": "Separation capacity of linear reservoirs with random connectivity matrix", "author": "Youness Boutaib", "abstract": "  We argue that the success of reservoir computing lies within the separation\ncapacity of the reservoirs and show that the expected separation capacity of\nrandom linear reservoirs is fully characterised by the spectral decomposition\nof an associated generalised matrix of moments. Of particular interest are\nreservoirs with Gaussian matrices that are either symmetric or whose entries\nare all independent. In the symmetric case, we prove that the separation\ncapacity always deteriorates with time; while for short inputs, separation with\nlarge reservoirs is best achieved when the entries of the matrix are scaled\nwith a factor $\\rho_T/\\sqrt{N}$, where $N$ is the dimension of the reservoir\nand $\\rho_T$ depends on the maximum length of the input time series. In the\ni.i.d. case, we establish that optimal separation with large reservoirs is\nconsistently achieved when the entries of the reservoir matrix are scaled with\nthe exact factor $1/\\sqrt{N}$. We further give upper bounds on the quality of\nseparation in function of the length of the time series. We complement this\nanalysis with an investigation of the likelihood of this separation and the\nimpact of the chosen architecture on separation consistency.\n", "link": "http://arxiv.org/abs/2404.17429v2", "date": "2024-05-01", "relevancy": 1.61, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4101}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4018}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Separation%20capacity%20of%20linear%20reservoirs%20with%20random%20connectivity%20matrix&body=Title%3A%20Separation%20capacity%20of%20linear%20reservoirs%20with%20random%20connectivity%20matrix%0AAuthor%3A%20Youness%20Boutaib%0AAbstract%3A%20%20%20We%20argue%20that%20the%20success%20of%20reservoir%20computing%20lies%20within%20the%20separation%0Acapacity%20of%20the%20reservoirs%20and%20show%20that%20the%20expected%20separation%20capacity%20of%0Arandom%20linear%20reservoirs%20is%20fully%20characterised%20by%20the%20spectral%20decomposition%0Aof%20an%20associated%20generalised%20matrix%20of%20moments.%20Of%20particular%20interest%20are%0Areservoirs%20with%20Gaussian%20matrices%20that%20are%20either%20symmetric%20or%20whose%20entries%0Aare%20all%20independent.%20In%20the%20symmetric%20case%2C%20we%20prove%20that%20the%20separation%0Acapacity%20always%20deteriorates%20with%20time%3B%20while%20for%20short%20inputs%2C%20separation%20with%0Alarge%20reservoirs%20is%20best%20achieved%20when%20the%20entries%20of%20the%20matrix%20are%20scaled%0Awith%20a%20factor%20%24%5Crho_T/%5Csqrt%7BN%7D%24%2C%20where%20%24N%24%20is%20the%20dimension%20of%20the%20reservoir%0Aand%20%24%5Crho_T%24%20depends%20on%20the%20maximum%20length%20of%20the%20input%20time%20series.%20In%20the%0Ai.i.d.%20case%2C%20we%20establish%20that%20optimal%20separation%20with%20large%20reservoirs%20is%0Aconsistently%20achieved%20when%20the%20entries%20of%20the%20reservoir%20matrix%20are%20scaled%20with%0Athe%20exact%20factor%20%241/%5Csqrt%7BN%7D%24.%20We%20further%20give%20upper%20bounds%20on%20the%20quality%20of%0Aseparation%20in%20function%20of%20the%20length%20of%20the%20time%20series.%20We%20complement%20this%0Aanalysis%20with%20an%20investigation%20of%20the%20likelihood%20of%20this%20separation%20and%20the%0Aimpact%20of%20the%20chosen%20architecture%20on%20separation%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17429v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separation%20capacity%20of%20linear%20reservoirs%20with%20random%20connectivity%20matrix&entry.906535625=Youness%20Boutaib&entry.1292438233=%20%20We%20argue%20that%20the%20success%20of%20reservoir%20computing%20lies%20within%20the%20separation%0Acapacity%20of%20the%20reservoirs%20and%20show%20that%20the%20expected%20separation%20capacity%20of%0Arandom%20linear%20reservoirs%20is%20fully%20characterised%20by%20the%20spectral%20decomposition%0Aof%20an%20associated%20generalised%20matrix%20of%20moments.%20Of%20particular%20interest%20are%0Areservoirs%20with%20Gaussian%20matrices%20that%20are%20either%20symmetric%20or%20whose%20entries%0Aare%20all%20independent.%20In%20the%20symmetric%20case%2C%20we%20prove%20that%20the%20separation%0Acapacity%20always%20deteriorates%20with%20time%3B%20while%20for%20short%20inputs%2C%20separation%20with%0Alarge%20reservoirs%20is%20best%20achieved%20when%20the%20entries%20of%20the%20matrix%20are%20scaled%0Awith%20a%20factor%20%24%5Crho_T/%5Csqrt%7BN%7D%24%2C%20where%20%24N%24%20is%20the%20dimension%20of%20the%20reservoir%0Aand%20%24%5Crho_T%24%20depends%20on%20the%20maximum%20length%20of%20the%20input%20time%20series.%20In%20the%0Ai.i.d.%20case%2C%20we%20establish%20that%20optimal%20separation%20with%20large%20reservoirs%20is%0Aconsistently%20achieved%20when%20the%20entries%20of%20the%20reservoir%20matrix%20are%20scaled%20with%0Athe%20exact%20factor%20%241/%5Csqrt%7BN%7D%24.%20We%20further%20give%20upper%20bounds%20on%20the%20quality%20of%0Aseparation%20in%20function%20of%20the%20length%20of%20the%20time%20series.%20We%20complement%20this%0Aanalysis%20with%20an%20investigation%20of%20the%20likelihood%20of%20this%20separation%20and%20the%0Aimpact%20of%20the%20chosen%20architecture%20on%20separation%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17429v2&entry.124074799=Read"},
{"title": "Adapting Pretrained Networks for Image Quality Assessment on High\n  Dynamic Range Displays", "author": "Andrei Chubarau and Hyunjin Yoo and Tara Akhavan and James Clark", "abstract": "  Conventional image quality metrics (IQMs), such as PSNR and SSIM, are\ndesigned for perceptually uniform gamma-encoded pixel values and cannot be\ndirectly applied to perceptually non-uniform linear high-dynamic-range (HDR)\ncolors. Similarly, most of the available datasets consist of\nstandard-dynamic-range (SDR) images collected in standard and possibly\nuncontrolled viewing conditions. Popular pre-trained neural networks are\nlikewise intended for SDR inputs, restricting their direct application to HDR\ncontent. On the other hand, training HDR models from scratch is challenging due\nto limited available HDR data. In this work, we explore more effective\napproaches for training deep learning-based models for image quality assessment\n(IQA) on HDR data. We leverage networks pre-trained on SDR data (source domain)\nand re-target these models to HDR (target domain) with additional fine-tuning\nand domain adaptation. We validate our methods on the available HDR IQA\ndatasets, demonstrating that models trained with our combined recipe outperform\nprevious baselines, converge much quicker, and reliably generalize to HDR\ninputs.\n", "link": "http://arxiv.org/abs/2405.00670v1", "date": "2024-05-01", "relevancy": 1.5977, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5446}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5299}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5288}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adapting%20Pretrained%20Networks%20for%20Image%20Quality%20Assessment%20on%20High%0A%20%20Dynamic%20Range%20Displays&body=Title%3A%20Adapting%20Pretrained%20Networks%20for%20Image%20Quality%20Assessment%20on%20High%0A%20%20Dynamic%20Range%20Displays%0AAuthor%3A%20Andrei%20Chubarau%20and%20Hyunjin%20Yoo%20and%20Tara%20Akhavan%20and%20James%20Clark%0AAbstract%3A%20%20%20Conventional%20image%20quality%20metrics%20%28IQMs%29%2C%20such%20as%20PSNR%20and%20SSIM%2C%20are%0Adesigned%20for%20perceptually%20uniform%20gamma-encoded%20pixel%20values%20and%20cannot%20be%0Adirectly%20applied%20to%20perceptually%20non-uniform%20linear%20high-dynamic-range%20%28HDR%29%0Acolors.%20Similarly%2C%20most%20of%20the%20available%20datasets%20consist%20of%0Astandard-dynamic-range%20%28SDR%29%20images%20collected%20in%20standard%20and%20possibly%0Auncontrolled%20viewing%20conditions.%20Popular%20pre-trained%20neural%20networks%20are%0Alikewise%20intended%20for%20SDR%20inputs%2C%20restricting%20their%20direct%20application%20to%20HDR%0Acontent.%20On%20the%20other%20hand%2C%20training%20HDR%20models%20from%20scratch%20is%20challenging%20due%0Ato%20limited%20available%20HDR%20data.%20In%20this%20work%2C%20we%20explore%20more%20effective%0Aapproaches%20for%20training%20deep%20learning-based%20models%20for%20image%20quality%20assessment%0A%28IQA%29%20on%20HDR%20data.%20We%20leverage%20networks%20pre-trained%20on%20SDR%20data%20%28source%20domain%29%0Aand%20re-target%20these%20models%20to%20HDR%20%28target%20domain%29%20with%20additional%20fine-tuning%0Aand%20domain%20adaptation.%20We%20validate%20our%20methods%20on%20the%20available%20HDR%20IQA%0Adatasets%2C%20demonstrating%20that%20models%20trained%20with%20our%20combined%20recipe%20outperform%0Aprevious%20baselines%2C%20converge%20much%20quicker%2C%20and%20reliably%20generalize%20to%20HDR%0Ainputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00670v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Pretrained%20Networks%20for%20Image%20Quality%20Assessment%20on%20High%0A%20%20Dynamic%20Range%20Displays&entry.906535625=Andrei%20Chubarau%20and%20Hyunjin%20Yoo%20and%20Tara%20Akhavan%20and%20James%20Clark&entry.1292438233=%20%20Conventional%20image%20quality%20metrics%20%28IQMs%29%2C%20such%20as%20PSNR%20and%20SSIM%2C%20are%0Adesigned%20for%20perceptually%20uniform%20gamma-encoded%20pixel%20values%20and%20cannot%20be%0Adirectly%20applied%20to%20perceptually%20non-uniform%20linear%20high-dynamic-range%20%28HDR%29%0Acolors.%20Similarly%2C%20most%20of%20the%20available%20datasets%20consist%20of%0Astandard-dynamic-range%20%28SDR%29%20images%20collected%20in%20standard%20and%20possibly%0Auncontrolled%20viewing%20conditions.%20Popular%20pre-trained%20neural%20networks%20are%0Alikewise%20intended%20for%20SDR%20inputs%2C%20restricting%20their%20direct%20application%20to%20HDR%0Acontent.%20On%20the%20other%20hand%2C%20training%20HDR%20models%20from%20scratch%20is%20challenging%20due%0Ato%20limited%20available%20HDR%20data.%20In%20this%20work%2C%20we%20explore%20more%20effective%0Aapproaches%20for%20training%20deep%20learning-based%20models%20for%20image%20quality%20assessment%0A%28IQA%29%20on%20HDR%20data.%20We%20leverage%20networks%20pre-trained%20on%20SDR%20data%20%28source%20domain%29%0Aand%20re-target%20these%20models%20to%20HDR%20%28target%20domain%29%20with%20additional%20fine-tuning%0Aand%20domain%20adaptation.%20We%20validate%20our%20methods%20on%20the%20available%20HDR%20IQA%0Adatasets%2C%20demonstrating%20that%20models%20trained%20with%20our%20combined%20recipe%20outperform%0Aprevious%20baselines%2C%20converge%20much%20quicker%2C%20and%20reliably%20generalize%20to%20HDR%0Ainputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00670v1&entry.124074799=Read"},
{"title": "New Benchmark Dataset and Fine-Grained Cross-Modal Fusion Framework for\n  Vietnamese Multimodal Aspect-Category Sentiment Analysis", "author": "Quy Hoang Nguyen and Minh-Van Truong Nguyen and Kiet Van Nguyen", "abstract": "  The emergence of multimodal data on social media platforms presents new\nopportunities to better understand user sentiments toward a given aspect.\nHowever, existing multimodal datasets for Aspect-Category Sentiment Analysis\n(ACSA) often focus on textual annotations, neglecting fine-grained information\nin images. Consequently, these datasets fail to fully exploit the richness\ninherent in multimodal. To address this, we introduce a new Vietnamese\nmultimodal dataset, named ViMACSA, which consists of 4,876 text-image pairs\nwith 14,618 fine-grained annotations for both text and image in the hotel\ndomain. Additionally, we propose a Fine-Grained Cross-Modal Fusion Framework\n(FCMF) that effectively learns both intra- and inter-modality interactions and\nthen fuses these information to produce a unified multimodal representation.\nExperimental results show that our framework outperforms SOTA models on the\nViMACSA dataset, achieving the highest F1 score of 79.73%. We also explore\ncharacteristics and challenges in Vietnamese multimodal sentiment analysis,\nincluding misspellings, abbreviations, and the complexities of the Vietnamese\nlanguage. This work contributes both a benchmark dataset and a new framework\nthat leverages fine-grained multimodal information to improve multimodal\naspect-category sentiment analysis. Our dataset is available for research\npurposes:\nhttps://github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis.\n", "link": "http://arxiv.org/abs/2405.00543v1", "date": "2024-05-01", "relevancy": 1.5921, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5392}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4993}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20New%20Benchmark%20Dataset%20and%20Fine-Grained%20Cross-Modal%20Fusion%20Framework%20for%0A%20%20Vietnamese%20Multimodal%20Aspect-Category%20Sentiment%20Analysis&body=Title%3A%20New%20Benchmark%20Dataset%20and%20Fine-Grained%20Cross-Modal%20Fusion%20Framework%20for%0A%20%20Vietnamese%20Multimodal%20Aspect-Category%20Sentiment%20Analysis%0AAuthor%3A%20Quy%20Hoang%20Nguyen%20and%20Minh-Van%20Truong%20Nguyen%20and%20Kiet%20Van%20Nguyen%0AAbstract%3A%20%20%20The%20emergence%20of%20multimodal%20data%20on%20social%20media%20platforms%20presents%20new%0Aopportunities%20to%20better%20understand%20user%20sentiments%20toward%20a%20given%20aspect.%0AHowever%2C%20existing%20multimodal%20datasets%20for%20Aspect-Category%20Sentiment%20Analysis%0A%28ACSA%29%20often%20focus%20on%20textual%20annotations%2C%20neglecting%20fine-grained%20information%0Ain%20images.%20Consequently%2C%20these%20datasets%20fail%20to%20fully%20exploit%20the%20richness%0Ainherent%20in%20multimodal.%20To%20address%20this%2C%20we%20introduce%20a%20new%20Vietnamese%0Amultimodal%20dataset%2C%20named%20ViMACSA%2C%20which%20consists%20of%204%2C876%20text-image%20pairs%0Awith%2014%2C618%20fine-grained%20annotations%20for%20both%20text%20and%20image%20in%20the%20hotel%0Adomain.%20Additionally%2C%20we%20propose%20a%20Fine-Grained%20Cross-Modal%20Fusion%20Framework%0A%28FCMF%29%20that%20effectively%20learns%20both%20intra-%20and%20inter-modality%20interactions%20and%0Athen%20fuses%20these%20information%20to%20produce%20a%20unified%20multimodal%20representation.%0AExperimental%20results%20show%20that%20our%20framework%20outperforms%20SOTA%20models%20on%20the%0AViMACSA%20dataset%2C%20achieving%20the%20highest%20F1%20score%20of%2079.73%25.%20We%20also%20explore%0Acharacteristics%20and%20challenges%20in%20Vietnamese%20multimodal%20sentiment%20analysis%2C%0Aincluding%20misspellings%2C%20abbreviations%2C%20and%20the%20complexities%20of%20the%20Vietnamese%0Alanguage.%20This%20work%20contributes%20both%20a%20benchmark%20dataset%20and%20a%20new%20framework%0Athat%20leverages%20fine-grained%20multimodal%20information%20to%20improve%20multimodal%0Aaspect-category%20sentiment%20analysis.%20Our%20dataset%20is%20available%20for%20research%0Apurposes%3A%0Ahttps%3A//github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00543v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Benchmark%20Dataset%20and%20Fine-Grained%20Cross-Modal%20Fusion%20Framework%20for%0A%20%20Vietnamese%20Multimodal%20Aspect-Category%20Sentiment%20Analysis&entry.906535625=Quy%20Hoang%20Nguyen%20and%20Minh-Van%20Truong%20Nguyen%20and%20Kiet%20Van%20Nguyen&entry.1292438233=%20%20The%20emergence%20of%20multimodal%20data%20on%20social%20media%20platforms%20presents%20new%0Aopportunities%20to%20better%20understand%20user%20sentiments%20toward%20a%20given%20aspect.%0AHowever%2C%20existing%20multimodal%20datasets%20for%20Aspect-Category%20Sentiment%20Analysis%0A%28ACSA%29%20often%20focus%20on%20textual%20annotations%2C%20neglecting%20fine-grained%20information%0Ain%20images.%20Consequently%2C%20these%20datasets%20fail%20to%20fully%20exploit%20the%20richness%0Ainherent%20in%20multimodal.%20To%20address%20this%2C%20we%20introduce%20a%20new%20Vietnamese%0Amultimodal%20dataset%2C%20named%20ViMACSA%2C%20which%20consists%20of%204%2C876%20text-image%20pairs%0Awith%2014%2C618%20fine-grained%20annotations%20for%20both%20text%20and%20image%20in%20the%20hotel%0Adomain.%20Additionally%2C%20we%20propose%20a%20Fine-Grained%20Cross-Modal%20Fusion%20Framework%0A%28FCMF%29%20that%20effectively%20learns%20both%20intra-%20and%20inter-modality%20interactions%20and%0Athen%20fuses%20these%20information%20to%20produce%20a%20unified%20multimodal%20representation.%0AExperimental%20results%20show%20that%20our%20framework%20outperforms%20SOTA%20models%20on%20the%0AViMACSA%20dataset%2C%20achieving%20the%20highest%20F1%20score%20of%2079.73%25.%20We%20also%20explore%0Acharacteristics%20and%20challenges%20in%20Vietnamese%20multimodal%20sentiment%20analysis%2C%0Aincluding%20misspellings%2C%20abbreviations%2C%20and%20the%20complexities%20of%20the%20Vietnamese%0Alanguage.%20This%20work%20contributes%20both%20a%20benchmark%20dataset%20and%20a%20new%20framework%0Athat%20leverages%20fine-grained%20multimodal%20information%20to%20improve%20multimodal%0Aaspect-category%20sentiment%20analysis.%20Our%20dataset%20is%20available%20for%20research%0Apurposes%3A%0Ahttps%3A//github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00543v1&entry.124074799=Read"},
{"title": "Multi-Robot Strategies for Communication-Constrained Exploration and\n  Electrostatic Anomaly Characterization", "author": "Gjosse Zijlstra and Karen L. Aplin and Edmund R. Hunt", "abstract": "  Exploration of extreme or remote environments such as Mars is often\nrecognized as an opportunity for multi-robot systems. However, this poses\nchallenges for maintaining robust inter-robot communication without preexisting\ninfrastructure. It may be that robots can only share information when they are\nphysically in close proximity with each other. At the same time, atmospheric\nphenomena such as dust devils are poorly understood and characterization of\ntheir electrostatic properties is of scientific interest. We perform a\ncomparative analysis of two multi-robot communication strategies: a distributed\napproach, with pairwise intermittent rendezvous, and a centralized, fixed base\nstation approach. We also introduce and evaluate the effectiveness of an\nalgorithm designed to predict the location and strength of electrostatic\nanomalies, assuming robot proximity. Using an agent-based simulation, we assess\nthe performance of these strategies in a 2D grid cell representation of a\nMartian environment. Results indicate that a decentralized rendezvous system\nconsistently outperforms a fixed base station system in terms of exploration\nspeed and in reducing the risk of data loss. We also find that inter-robot data\nsharing improves performance when trying to predict the location and strength\nof an electrostatic anomaly. These findings indicate the importance of\nappropriate communication strategies for efficient multi-robot science\nmissions.\n", "link": "http://arxiv.org/abs/2405.00586v1", "date": "2024-05-01", "relevancy": 1.5899, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6193}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5763}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4757}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Robot%20Strategies%20for%20Communication-Constrained%20Exploration%20and%0A%20%20Electrostatic%20Anomaly%20Characterization&body=Title%3A%20Multi-Robot%20Strategies%20for%20Communication-Constrained%20Exploration%20and%0A%20%20Electrostatic%20Anomaly%20Characterization%0AAuthor%3A%20Gjosse%20Zijlstra%20and%20Karen%20L.%20Aplin%20and%20Edmund%20R.%20Hunt%0AAbstract%3A%20%20%20Exploration%20of%20extreme%20or%20remote%20environments%20such%20as%20Mars%20is%20often%0Arecognized%20as%20an%20opportunity%20for%20multi-robot%20systems.%20However%2C%20this%20poses%0Achallenges%20for%20maintaining%20robust%20inter-robot%20communication%20without%20preexisting%0Ainfrastructure.%20It%20may%20be%20that%20robots%20can%20only%20share%20information%20when%20they%20are%0Aphysically%20in%20close%20proximity%20with%20each%20other.%20At%20the%20same%20time%2C%20atmospheric%0Aphenomena%20such%20as%20dust%20devils%20are%20poorly%20understood%20and%20characterization%20of%0Atheir%20electrostatic%20properties%20is%20of%20scientific%20interest.%20We%20perform%20a%0Acomparative%20analysis%20of%20two%20multi-robot%20communication%20strategies%3A%20a%20distributed%0Aapproach%2C%20with%20pairwise%20intermittent%20rendezvous%2C%20and%20a%20centralized%2C%20fixed%20base%0Astation%20approach.%20We%20also%20introduce%20and%20evaluate%20the%20effectiveness%20of%20an%0Aalgorithm%20designed%20to%20predict%20the%20location%20and%20strength%20of%20electrostatic%0Aanomalies%2C%20assuming%20robot%20proximity.%20Using%20an%20agent-based%20simulation%2C%20we%20assess%0Athe%20performance%20of%20these%20strategies%20in%20a%202D%20grid%20cell%20representation%20of%20a%0AMartian%20environment.%20Results%20indicate%20that%20a%20decentralized%20rendezvous%20system%0Aconsistently%20outperforms%20a%20fixed%20base%20station%20system%20in%20terms%20of%20exploration%0Aspeed%20and%20in%20reducing%20the%20risk%20of%20data%20loss.%20We%20also%20find%20that%20inter-robot%20data%0Asharing%20improves%20performance%20when%20trying%20to%20predict%20the%20location%20and%20strength%0Aof%20an%20electrostatic%20anomaly.%20These%20findings%20indicate%20the%20importance%20of%0Aappropriate%20communication%20strategies%20for%20efficient%20multi-robot%20science%0Amissions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00586v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Robot%20Strategies%20for%20Communication-Constrained%20Exploration%20and%0A%20%20Electrostatic%20Anomaly%20Characterization&entry.906535625=Gjosse%20Zijlstra%20and%20Karen%20L.%20Aplin%20and%20Edmund%20R.%20Hunt&entry.1292438233=%20%20Exploration%20of%20extreme%20or%20remote%20environments%20such%20as%20Mars%20is%20often%0Arecognized%20as%20an%20opportunity%20for%20multi-robot%20systems.%20However%2C%20this%20poses%0Achallenges%20for%20maintaining%20robust%20inter-robot%20communication%20without%20preexisting%0Ainfrastructure.%20It%20may%20be%20that%20robots%20can%20only%20share%20information%20when%20they%20are%0Aphysically%20in%20close%20proximity%20with%20each%20other.%20At%20the%20same%20time%2C%20atmospheric%0Aphenomena%20such%20as%20dust%20devils%20are%20poorly%20understood%20and%20characterization%20of%0Atheir%20electrostatic%20properties%20is%20of%20scientific%20interest.%20We%20perform%20a%0Acomparative%20analysis%20of%20two%20multi-robot%20communication%20strategies%3A%20a%20distributed%0Aapproach%2C%20with%20pairwise%20intermittent%20rendezvous%2C%20and%20a%20centralized%2C%20fixed%20base%0Astation%20approach.%20We%20also%20introduce%20and%20evaluate%20the%20effectiveness%20of%20an%0Aalgorithm%20designed%20to%20predict%20the%20location%20and%20strength%20of%20electrostatic%0Aanomalies%2C%20assuming%20robot%20proximity.%20Using%20an%20agent-based%20simulation%2C%20we%20assess%0Athe%20performance%20of%20these%20strategies%20in%20a%202D%20grid%20cell%20representation%20of%20a%0AMartian%20environment.%20Results%20indicate%20that%20a%20decentralized%20rendezvous%20system%0Aconsistently%20outperforms%20a%20fixed%20base%20station%20system%20in%20terms%20of%20exploration%0Aspeed%20and%20in%20reducing%20the%20risk%20of%20data%20loss.%20We%20also%20find%20that%20inter-robot%20data%0Asharing%20improves%20performance%20when%20trying%20to%20predict%20the%20location%20and%20strength%0Aof%20an%20electrostatic%20anomaly.%20These%20findings%20indicate%20the%20importance%20of%0Aappropriate%20communication%20strategies%20for%20efficient%20multi-robot%20science%0Amissions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00586v1&entry.124074799=Read"},
{"title": "Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest\n  Monitoring", "author": "Sizhuo Li and Dimitri Gominski and Martin Brandt and Xiaoye Tong and Philippe Ciais", "abstract": "  Image-level regression is an important task in Earth observation, where\nvisual domain and label shifts are a core challenge hampering generalization.\nHowever, cross-domain regression with remote sensing data remains understudied\ndue to the absence of suited datasets. We introduce a new dataset with aerial\nand satellite imagery in five countries with three forest-related regression\ntasks. To match real-world applicative interests, we compare methods through a\nrestrictive setup where no prior on the target domain is available during\ntraining, and models are adapted with limited information during testing.\nBuilding on the assumption that ordered relationships generalize better, we\npropose manifold diffusion for regression as a strong baseline for transduction\nin low-data regimes. Our comparison highlights the comparative advantages of\ninductive and transductive methods in cross-domain regression.\n", "link": "http://arxiv.org/abs/2405.00514v1", "date": "2024-05-01", "relevancy": 1.5832, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5382}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5179}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5113}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Get%20Your%20Embedding%20Space%20in%20Order%3A%20Domain-Adaptive%20Regression%20for%20Forest%0A%20%20Monitoring&body=Title%3A%20Get%20Your%20Embedding%20Space%20in%20Order%3A%20Domain-Adaptive%20Regression%20for%20Forest%0A%20%20Monitoring%0AAuthor%3A%20Sizhuo%20Li%20and%20Dimitri%20Gominski%20and%20Martin%20Brandt%20and%20Xiaoye%20Tong%20and%20Philippe%20Ciais%0AAbstract%3A%20%20%20Image-level%20regression%20is%20an%20important%20task%20in%20Earth%20observation%2C%20where%0Avisual%20domain%20and%20label%20shifts%20are%20a%20core%20challenge%20hampering%20generalization.%0AHowever%2C%20cross-domain%20regression%20with%20remote%20sensing%20data%20remains%20understudied%0Adue%20to%20the%20absence%20of%20suited%20datasets.%20We%20introduce%20a%20new%20dataset%20with%20aerial%0Aand%20satellite%20imagery%20in%20five%20countries%20with%20three%20forest-related%20regression%0Atasks.%20To%20match%20real-world%20applicative%20interests%2C%20we%20compare%20methods%20through%20a%0Arestrictive%20setup%20where%20no%20prior%20on%20the%20target%20domain%20is%20available%20during%0Atraining%2C%20and%20models%20are%20adapted%20with%20limited%20information%20during%20testing.%0ABuilding%20on%20the%20assumption%20that%20ordered%20relationships%20generalize%20better%2C%20we%0Apropose%20manifold%20diffusion%20for%20regression%20as%20a%20strong%20baseline%20for%20transduction%0Ain%20low-data%20regimes.%20Our%20comparison%20highlights%20the%20comparative%20advantages%20of%0Ainductive%20and%20transductive%20methods%20in%20cross-domain%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00514v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Get%20Your%20Embedding%20Space%20in%20Order%3A%20Domain-Adaptive%20Regression%20for%20Forest%0A%20%20Monitoring&entry.906535625=Sizhuo%20Li%20and%20Dimitri%20Gominski%20and%20Martin%20Brandt%20and%20Xiaoye%20Tong%20and%20Philippe%20Ciais&entry.1292438233=%20%20Image-level%20regression%20is%20an%20important%20task%20in%20Earth%20observation%2C%20where%0Avisual%20domain%20and%20label%20shifts%20are%20a%20core%20challenge%20hampering%20generalization.%0AHowever%2C%20cross-domain%20regression%20with%20remote%20sensing%20data%20remains%20understudied%0Adue%20to%20the%20absence%20of%20suited%20datasets.%20We%20introduce%20a%20new%20dataset%20with%20aerial%0Aand%20satellite%20imagery%20in%20five%20countries%20with%20three%20forest-related%20regression%0Atasks.%20To%20match%20real-world%20applicative%20interests%2C%20we%20compare%20methods%20through%20a%0Arestrictive%20setup%20where%20no%20prior%20on%20the%20target%20domain%20is%20available%20during%0Atraining%2C%20and%20models%20are%20adapted%20with%20limited%20information%20during%20testing.%0ABuilding%20on%20the%20assumption%20that%20ordered%20relationships%20generalize%20better%2C%20we%0Apropose%20manifold%20diffusion%20for%20regression%20as%20a%20strong%20baseline%20for%20transduction%0Ain%20low-data%20regimes.%20Our%20comparison%20highlights%20the%20comparative%20advantages%20of%0Ainductive%20and%20transductive%20methods%20in%20cross-domain%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00514v1&entry.124074799=Read"},
{"title": "Attention is All They Need: Exploring the Media Archaeology of the\n  Computer Vision Research Paper", "author": "Samuel Goree and Gabriel Appleby and David Crandall and Norman Su", "abstract": "  Research papers, in addition to textual documents, are a designed interface\nthrough which researchers communicate. Recently, rapid growth has transformed\nthat interface in many fields of computing. In this work, we examine the\neffects of this growth from a media archaeology perspective, through the\nchanges to figures and tables in research papers. Specifically, we study these\nchanges in computer vision over the past decade, as the deep learning\nrevolution has driven unprecedented growth in the discipline. We ground our\ninvestigation through interviews with veteran researchers spanning computer\nvision, graphics and visualization. Our analysis focuses on the research\nattention economy: how research paper elements contribute towards advertising,\nmeasuring and disseminating an increasingly commodified ``contribution.''\nThrough this work, we seek to motivate future discussion surrounding the design\nof both the research paper itself as well as the larger sociotechnical research\npublishing system, including tools for finding, reading and writing research\npapers.\n", "link": "http://arxiv.org/abs/2209.11200v2", "date": "2024-05-01", "relevancy": 0.8773, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.457}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4322}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4267}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attention%20is%20All%20They%20Need%3A%20Exploring%20the%20Media%20Archaeology%20of%20the%0A%20%20Computer%20Vision%20Research%20Paper&body=Title%3A%20Attention%20is%20All%20They%20Need%3A%20Exploring%20the%20Media%20Archaeology%20of%20the%0A%20%20Computer%20Vision%20Research%20Paper%0AAuthor%3A%20Samuel%20Goree%20and%20Gabriel%20Appleby%20and%20David%20Crandall%20and%20Norman%20Su%0AAbstract%3A%20%20%20Research%20papers%2C%20in%20addition%20to%20textual%20documents%2C%20are%20a%20designed%20interface%0Athrough%20which%20researchers%20communicate.%20Recently%2C%20rapid%20growth%20has%20transformed%0Athat%20interface%20in%20many%20fields%20of%20computing.%20In%20this%20work%2C%20we%20examine%20the%0Aeffects%20of%20this%20growth%20from%20a%20media%20archaeology%20perspective%2C%20through%20the%0Achanges%20to%20figures%20and%20tables%20in%20research%20papers.%20Specifically%2C%20we%20study%20these%0Achanges%20in%20computer%20vision%20over%20the%20past%20decade%2C%20as%20the%20deep%20learning%0Arevolution%20has%20driven%20unprecedented%20growth%20in%20the%20discipline.%20We%20ground%20our%0Ainvestigation%20through%20interviews%20with%20veteran%20researchers%20spanning%20computer%0Avision%2C%20graphics%20and%20visualization.%20Our%20analysis%20focuses%20on%20the%20research%0Aattention%20economy%3A%20how%20research%20paper%20elements%20contribute%20towards%20advertising%2C%0Ameasuring%20and%20disseminating%20an%20increasingly%20commodified%20%60%60contribution.%27%27%0AThrough%20this%20work%2C%20we%20seek%20to%20motivate%20future%20discussion%20surrounding%20the%20design%0Aof%20both%20the%20research%20paper%20itself%20as%20well%20as%20the%20larger%20sociotechnical%20research%0Apublishing%20system%2C%20including%20tools%20for%20finding%2C%20reading%20and%20writing%20research%0Apapers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.11200v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20is%20All%20They%20Need%3A%20Exploring%20the%20Media%20Archaeology%20of%20the%0A%20%20Computer%20Vision%20Research%20Paper&entry.906535625=Samuel%20Goree%20and%20Gabriel%20Appleby%20and%20David%20Crandall%20and%20Norman%20Su&entry.1292438233=%20%20Research%20papers%2C%20in%20addition%20to%20textual%20documents%2C%20are%20a%20designed%20interface%0Athrough%20which%20researchers%20communicate.%20Recently%2C%20rapid%20growth%20has%20transformed%0Athat%20interface%20in%20many%20fields%20of%20computing.%20In%20this%20work%2C%20we%20examine%20the%0Aeffects%20of%20this%20growth%20from%20a%20media%20archaeology%20perspective%2C%20through%20the%0Achanges%20to%20figures%20and%20tables%20in%20research%20papers.%20Specifically%2C%20we%20study%20these%0Achanges%20in%20computer%20vision%20over%20the%20past%20decade%2C%20as%20the%20deep%20learning%0Arevolution%20has%20driven%20unprecedented%20growth%20in%20the%20discipline.%20We%20ground%20our%0Ainvestigation%20through%20interviews%20with%20veteran%20researchers%20spanning%20computer%0Avision%2C%20graphics%20and%20visualization.%20Our%20analysis%20focuses%20on%20the%20research%0Aattention%20economy%3A%20how%20research%20paper%20elements%20contribute%20towards%20advertising%2C%0Ameasuring%20and%20disseminating%20an%20increasingly%20commodified%20%60%60contribution.%27%27%0AThrough%20this%20work%2C%20we%20seek%20to%20motivate%20future%20discussion%20surrounding%20the%20design%0Aof%20both%20the%20research%20paper%20itself%20as%20well%20as%20the%20larger%20sociotechnical%20research%0Apublishing%20system%2C%20including%20tools%20for%20finding%2C%20reading%20and%20writing%20research%0Apapers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.11200v2&entry.124074799=Read"},
{"title": "ULLER: A Unified Language for Learning and Reasoning", "author": "Emile van Krieken and Samy Badreddine and Robin Manhaeve and Eleonora Giunchiglia", "abstract": "  The field of neuro-symbolic artificial intelligence (NeSy), which combines\nlearning and reasoning, has recently experienced significant growth. There now\nare a wide variety of NeSy frameworks, each with its own specific language for\nexpressing background knowledge and how to relate it to neural networks. This\nheterogeneity hinders accessibility for newcomers and makes comparing different\nNeSy frameworks challenging. We propose a unified language for NeSy, which we\ncall ULLER, a Unified Language for LEarning and Reasoning. ULLER encompasses a\nwide variety of settings, while ensuring that knowledge described in it can be\nused in existing NeSy systems. ULLER has a neuro-symbolic first-order syntax\nfor which we provide example semantics including classical, fuzzy, and\nprobabilistic logics. We believe ULLER is a first step towards making NeSy\nresearch more accessible and comparable, paving the way for libraries that\nstreamline training and evaluation across a multitude of semantics, knowledge\nbases, and NeSy systems.\n", "link": "http://arxiv.org/abs/2405.00532v1", "date": "2024-05-01", "relevancy": 1.3641, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4506}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4328}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ULLER%3A%20A%20Unified%20Language%20for%20Learning%20and%20Reasoning&body=Title%3A%20ULLER%3A%20A%20Unified%20Language%20for%20Learning%20and%20Reasoning%0AAuthor%3A%20Emile%20van%20Krieken%20and%20Samy%20Badreddine%20and%20Robin%20Manhaeve%20and%20Eleonora%20Giunchiglia%0AAbstract%3A%20%20%20The%20field%20of%20neuro-symbolic%20artificial%20intelligence%20%28NeSy%29%2C%20which%20combines%0Alearning%20and%20reasoning%2C%20has%20recently%20experienced%20significant%20growth.%20There%20now%0Aare%20a%20wide%20variety%20of%20NeSy%20frameworks%2C%20each%20with%20its%20own%20specific%20language%20for%0Aexpressing%20background%20knowledge%20and%20how%20to%20relate%20it%20to%20neural%20networks.%20This%0Aheterogeneity%20hinders%20accessibility%20for%20newcomers%20and%20makes%20comparing%20different%0ANeSy%20frameworks%20challenging.%20We%20propose%20a%20unified%20language%20for%20NeSy%2C%20which%20we%0Acall%20ULLER%2C%20a%20Unified%20Language%20for%20LEarning%20and%20Reasoning.%20ULLER%20encompasses%20a%0Awide%20variety%20of%20settings%2C%20while%20ensuring%20that%20knowledge%20described%20in%20it%20can%20be%0Aused%20in%20existing%20NeSy%20systems.%20ULLER%20has%20a%20neuro-symbolic%20first-order%20syntax%0Afor%20which%20we%20provide%20example%20semantics%20including%20classical%2C%20fuzzy%2C%20and%0Aprobabilistic%20logics.%20We%20believe%20ULLER%20is%20a%20first%20step%20towards%20making%20NeSy%0Aresearch%20more%20accessible%20and%20comparable%2C%20paving%20the%20way%20for%20libraries%20that%0Astreamline%20training%20and%20evaluation%20across%20a%20multitude%20of%20semantics%2C%20knowledge%0Abases%2C%20and%20NeSy%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00532v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ULLER%3A%20A%20Unified%20Language%20for%20Learning%20and%20Reasoning&entry.906535625=Emile%20van%20Krieken%20and%20Samy%20Badreddine%20and%20Robin%20Manhaeve%20and%20Eleonora%20Giunchiglia&entry.1292438233=%20%20The%20field%20of%20neuro-symbolic%20artificial%20intelligence%20%28NeSy%29%2C%20which%20combines%0Alearning%20and%20reasoning%2C%20has%20recently%20experienced%20significant%20growth.%20There%20now%0Aare%20a%20wide%20variety%20of%20NeSy%20frameworks%2C%20each%20with%20its%20own%20specific%20language%20for%0Aexpressing%20background%20knowledge%20and%20how%20to%20relate%20it%20to%20neural%20networks.%20This%0Aheterogeneity%20hinders%20accessibility%20for%20newcomers%20and%20makes%20comparing%20different%0ANeSy%20frameworks%20challenging.%20We%20propose%20a%20unified%20language%20for%20NeSy%2C%20which%20we%0Acall%20ULLER%2C%20a%20Unified%20Language%20for%20LEarning%20and%20Reasoning.%20ULLER%20encompasses%20a%0Awide%20variety%20of%20settings%2C%20while%20ensuring%20that%20knowledge%20described%20in%20it%20can%20be%0Aused%20in%20existing%20NeSy%20systems.%20ULLER%20has%20a%20neuro-symbolic%20first-order%20syntax%0Afor%20which%20we%20provide%20example%20semantics%20including%20classical%2C%20fuzzy%2C%20and%0Aprobabilistic%20logics.%20We%20believe%20ULLER%20is%20a%20first%20step%20towards%20making%20NeSy%0Aresearch%20more%20accessible%20and%20comparable%2C%20paving%20the%20way%20for%20libraries%20that%0Astreamline%20training%20and%20evaluation%20across%20a%20multitude%20of%20semantics%2C%20knowledge%0Abases%2C%20and%20NeSy%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00532v1&entry.124074799=Read"},
{"title": "GenCast: Diffusion-based ensemble forecasting for medium-range weather", "author": "Ilan Price and Alvaro Sanchez-Gonzalez and Ferran Alet and Tom R. Andersson and Andrew El-Kadi and Dominic Masters and Timo Ewalds and Jacklynn Stott and Shakir Mohamed and Peter Battaglia and Remi Lam and Matthew Willson", "abstract": "  Weather forecasts are fundamentally uncertain, so predicting the range of\nprobable weather scenarios is crucial for important decisions, from warning the\npublic about hazardous weather, to planning renewable energy use. Here, we\nintroduce GenCast, a probabilistic weather model with greater skill and speed\nthan the top operational medium-range weather forecast in the world, the\nEuropean Centre for Medium-Range Forecasts (ECMWF)'s ensemble forecast, ENS.\nUnlike traditional approaches, which are based on numerical weather prediction\n(NWP), GenCast is a machine learning weather prediction (MLWP) method, trained\non decades of reanalysis data. GenCast generates an ensemble of stochastic\n15-day global forecasts, at 12-hour steps and 0.25 degree latitude-longitude\nresolution, for over 80 surface and atmospheric variables, in 8 minutes. It has\ngreater skill than ENS on 97.4% of 1320 targets we evaluated, and better\npredicts extreme weather, tropical cyclones, and wind power production. This\nwork helps open the next chapter in operational weather forecasting, where\ncritical weather-dependent decisions are made with greater accuracy and\nefficiency.\n", "link": "http://arxiv.org/abs/2312.15796v2", "date": "2024-05-01", "relevancy": 1.3096, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4502}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.436}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4031}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GenCast%3A%20Diffusion-based%20ensemble%20forecasting%20for%20medium-range%20weather&body=Title%3A%20GenCast%3A%20Diffusion-based%20ensemble%20forecasting%20for%20medium-range%20weather%0AAuthor%3A%20Ilan%20Price%20and%20Alvaro%20Sanchez-Gonzalez%20and%20Ferran%20Alet%20and%20Tom%20R.%20Andersson%20and%20Andrew%20El-Kadi%20and%20Dominic%20Masters%20and%20Timo%20Ewalds%20and%20Jacklynn%20Stott%20and%20Shakir%20Mohamed%20and%20Peter%20Battaglia%20and%20Remi%20Lam%20and%20Matthew%20Willson%0AAbstract%3A%20%20%20Weather%20forecasts%20are%20fundamentally%20uncertain%2C%20so%20predicting%20the%20range%20of%0Aprobable%20weather%20scenarios%20is%20crucial%20for%20important%20decisions%2C%20from%20warning%20the%0Apublic%20about%20hazardous%20weather%2C%20to%20planning%20renewable%20energy%20use.%20Here%2C%20we%0Aintroduce%20GenCast%2C%20a%20probabilistic%20weather%20model%20with%20greater%20skill%20and%20speed%0Athan%20the%20top%20operational%20medium-range%20weather%20forecast%20in%20the%20world%2C%20the%0AEuropean%20Centre%20for%20Medium-Range%20Forecasts%20%28ECMWF%29%27s%20ensemble%20forecast%2C%20ENS.%0AUnlike%20traditional%20approaches%2C%20which%20are%20based%20on%20numerical%20weather%20prediction%0A%28NWP%29%2C%20GenCast%20is%20a%20machine%20learning%20weather%20prediction%20%28MLWP%29%20method%2C%20trained%0Aon%20decades%20of%20reanalysis%20data.%20GenCast%20generates%20an%20ensemble%20of%20stochastic%0A15-day%20global%20forecasts%2C%20at%2012-hour%20steps%20and%200.25%20degree%20latitude-longitude%0Aresolution%2C%20for%20over%2080%20surface%20and%20atmospheric%20variables%2C%20in%208%20minutes.%20It%20has%0Agreater%20skill%20than%20ENS%20on%2097.4%25%20of%201320%20targets%20we%20evaluated%2C%20and%20better%0Apredicts%20extreme%20weather%2C%20tropical%20cyclones%2C%20and%20wind%20power%20production.%20This%0Awork%20helps%20open%20the%20next%20chapter%20in%20operational%20weather%20forecasting%2C%20where%0Acritical%20weather-dependent%20decisions%20are%20made%20with%20greater%20accuracy%20and%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15796v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenCast%3A%20Diffusion-based%20ensemble%20forecasting%20for%20medium-range%20weather&entry.906535625=Ilan%20Price%20and%20Alvaro%20Sanchez-Gonzalez%20and%20Ferran%20Alet%20and%20Tom%20R.%20Andersson%20and%20Andrew%20El-Kadi%20and%20Dominic%20Masters%20and%20Timo%20Ewalds%20and%20Jacklynn%20Stott%20and%20Shakir%20Mohamed%20and%20Peter%20Battaglia%20and%20Remi%20Lam%20and%20Matthew%20Willson&entry.1292438233=%20%20Weather%20forecasts%20are%20fundamentally%20uncertain%2C%20so%20predicting%20the%20range%20of%0Aprobable%20weather%20scenarios%20is%20crucial%20for%20important%20decisions%2C%20from%20warning%20the%0Apublic%20about%20hazardous%20weather%2C%20to%20planning%20renewable%20energy%20use.%20Here%2C%20we%0Aintroduce%20GenCast%2C%20a%20probabilistic%20weather%20model%20with%20greater%20skill%20and%20speed%0Athan%20the%20top%20operational%20medium-range%20weather%20forecast%20in%20the%20world%2C%20the%0AEuropean%20Centre%20for%20Medium-Range%20Forecasts%20%28ECMWF%29%27s%20ensemble%20forecast%2C%20ENS.%0AUnlike%20traditional%20approaches%2C%20which%20are%20based%20on%20numerical%20weather%20prediction%0A%28NWP%29%2C%20GenCast%20is%20a%20machine%20learning%20weather%20prediction%20%28MLWP%29%20method%2C%20trained%0Aon%20decades%20of%20reanalysis%20data.%20GenCast%20generates%20an%20ensemble%20of%20stochastic%0A15-day%20global%20forecasts%2C%20at%2012-hour%20steps%20and%200.25%20degree%20latitude-longitude%0Aresolution%2C%20for%20over%2080%20surface%20and%20atmospheric%20variables%2C%20in%208%20minutes.%20It%20has%0Agreater%20skill%20than%20ENS%20on%2097.4%25%20of%201320%20targets%20we%20evaluated%2C%20and%20better%0Apredicts%20extreme%20weather%2C%20tropical%20cyclones%2C%20and%20wind%20power%20production.%20This%0Awork%20helps%20open%20the%20next%20chapter%20in%20operational%20weather%20forecasting%2C%20where%0Acritical%20weather-dependent%20decisions%20are%20made%20with%20greater%20accuracy%20and%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15796v2&entry.124074799=Read"},
{"title": "Hidden yet quantifiable: A lower bound for confounding strength using\n  randomized trials", "author": "Piersilvio De Bartolomeis and Javier Abad and Konstantin Donhauser and Fanny Yang", "abstract": "  In the era of fast-paced precision medicine, observational studies play a\nmajor role in properly evaluating new treatments in clinical practice. Yet,\nunobserved confounding can significantly compromise causal conclusions drawn\nfrom non-randomized data. We propose a novel strategy that leverages randomized\ntrials to quantify unobserved confounding. First, we design a statistical test\nto detect unobserved confounding with strength above a given threshold. Then,\nwe use the test to estimate an asymptotically valid lower bound on the\nunobserved confounding strength. We evaluate the power and validity of our\nstatistical test on several synthetic and semi-synthetic datasets. Further, we\nshow how our lower bound can correctly identify the absence and presence of\nunobserved confounding in a real-world setting.\n", "link": "http://arxiv.org/abs/2312.03871v3", "date": "2024-05-01", "relevancy": 1.2384, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.417}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4063}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hidden%20yet%20quantifiable%3A%20A%20lower%20bound%20for%20confounding%20strength%20using%0A%20%20randomized%20trials&body=Title%3A%20Hidden%20yet%20quantifiable%3A%20A%20lower%20bound%20for%20confounding%20strength%20using%0A%20%20randomized%20trials%0AAuthor%3A%20Piersilvio%20De%20Bartolomeis%20and%20Javier%20Abad%20and%20Konstantin%20Donhauser%20and%20Fanny%20Yang%0AAbstract%3A%20%20%20In%20the%20era%20of%20fast-paced%20precision%20medicine%2C%20observational%20studies%20play%20a%0Amajor%20role%20in%20properly%20evaluating%20new%20treatments%20in%20clinical%20practice.%20Yet%2C%0Aunobserved%20confounding%20can%20significantly%20compromise%20causal%20conclusions%20drawn%0Afrom%20non-randomized%20data.%20We%20propose%20a%20novel%20strategy%20that%20leverages%20randomized%0Atrials%20to%20quantify%20unobserved%20confounding.%20First%2C%20we%20design%20a%20statistical%20test%0Ato%20detect%20unobserved%20confounding%20with%20strength%20above%20a%20given%20threshold.%20Then%2C%0Awe%20use%20the%20test%20to%20estimate%20an%20asymptotically%20valid%20lower%20bound%20on%20the%0Aunobserved%20confounding%20strength.%20We%20evaluate%20the%20power%20and%20validity%20of%20our%0Astatistical%20test%20on%20several%20synthetic%20and%20semi-synthetic%20datasets.%20Further%2C%20we%0Ashow%20how%20our%20lower%20bound%20can%20correctly%20identify%20the%20absence%20and%20presence%20of%0Aunobserved%20confounding%20in%20a%20real-world%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03871v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hidden%20yet%20quantifiable%3A%20A%20lower%20bound%20for%20confounding%20strength%20using%0A%20%20randomized%20trials&entry.906535625=Piersilvio%20De%20Bartolomeis%20and%20Javier%20Abad%20and%20Konstantin%20Donhauser%20and%20Fanny%20Yang&entry.1292438233=%20%20In%20the%20era%20of%20fast-paced%20precision%20medicine%2C%20observational%20studies%20play%20a%0Amajor%20role%20in%20properly%20evaluating%20new%20treatments%20in%20clinical%20practice.%20Yet%2C%0Aunobserved%20confounding%20can%20significantly%20compromise%20causal%20conclusions%20drawn%0Afrom%20non-randomized%20data.%20We%20propose%20a%20novel%20strategy%20that%20leverages%20randomized%0Atrials%20to%20quantify%20unobserved%20confounding.%20First%2C%20we%20design%20a%20statistical%20test%0Ato%20detect%20unobserved%20confounding%20with%20strength%20above%20a%20given%20threshold.%20Then%2C%0Awe%20use%20the%20test%20to%20estimate%20an%20asymptotically%20valid%20lower%20bound%20on%20the%0Aunobserved%20confounding%20strength.%20We%20evaluate%20the%20power%20and%20validity%20of%20our%0Astatistical%20test%20on%20several%20synthetic%20and%20semi-synthetic%20datasets.%20Further%2C%20we%0Ashow%20how%20our%20lower%20bound%20can%20correctly%20identify%20the%20absence%20and%20presence%20of%0Aunobserved%20confounding%20in%20a%20real-world%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03871v3&entry.124074799=Read"},
{"title": "Blurring Diffusion Models", "author": "Emiel Hoogeboom and Tim Salimans", "abstract": "  Recently, Rissanen et al., (2022) have presented a new type of diffusion\nprocess for generative modeling based on heat dissipation, or blurring, as an\nalternative to isotropic Gaussian diffusion. Here, we show that blurring can\nequivalently be defined through a Gaussian diffusion process with non-isotropic\nnoise. In making this connection, we bridge the gap between inverse heat\ndissipation and denoising diffusion, and we shed light on the inductive bias\nthat results from this modeling choice. Finally, we propose a generalized class\nof diffusion models that offers the best of both standard Gaussian denoising\ndiffusion and inverse heat dissipation, which we call Blurring Diffusion\nModels.\n", "link": "http://arxiv.org/abs/2209.05557v3", "date": "2024-05-01", "relevancy": 1.0956, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5704}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5652}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5078}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Blurring%20Diffusion%20Models&body=Title%3A%20Blurring%20Diffusion%20Models%0AAuthor%3A%20Emiel%20Hoogeboom%20and%20Tim%20Salimans%0AAbstract%3A%20%20%20Recently%2C%20Rissanen%20et%20al.%2C%20%282022%29%20have%20presented%20a%20new%20type%20of%20diffusion%0Aprocess%20for%20generative%20modeling%20based%20on%20heat%20dissipation%2C%20or%20blurring%2C%20as%20an%0Aalternative%20to%20isotropic%20Gaussian%20diffusion.%20Here%2C%20we%20show%20that%20blurring%20can%0Aequivalently%20be%20defined%20through%20a%20Gaussian%20diffusion%20process%20with%20non-isotropic%0Anoise.%20In%20making%20this%20connection%2C%20we%20bridge%20the%20gap%20between%20inverse%20heat%0Adissipation%20and%20denoising%20diffusion%2C%20and%20we%20shed%20light%20on%20the%20inductive%20bias%0Athat%20results%20from%20this%20modeling%20choice.%20Finally%2C%20we%20propose%20a%20generalized%20class%0Aof%20diffusion%20models%20that%20offers%20the%20best%20of%20both%20standard%20Gaussian%20denoising%0Adiffusion%20and%20inverse%20heat%20dissipation%2C%20which%20we%20call%20Blurring%20Diffusion%0AModels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.05557v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blurring%20Diffusion%20Models&entry.906535625=Emiel%20Hoogeboom%20and%20Tim%20Salimans&entry.1292438233=%20%20Recently%2C%20Rissanen%20et%20al.%2C%20%282022%29%20have%20presented%20a%20new%20type%20of%20diffusion%0Aprocess%20for%20generative%20modeling%20based%20on%20heat%20dissipation%2C%20or%20blurring%2C%20as%20an%0Aalternative%20to%20isotropic%20Gaussian%20diffusion.%20Here%2C%20we%20show%20that%20blurring%20can%0Aequivalently%20be%20defined%20through%20a%20Gaussian%20diffusion%20process%20with%20non-isotropic%0Anoise.%20In%20making%20this%20connection%2C%20we%20bridge%20the%20gap%20between%20inverse%20heat%0Adissipation%20and%20denoising%20diffusion%2C%20and%20we%20shed%20light%20on%20the%20inductive%20bias%0Athat%20results%20from%20this%20modeling%20choice.%20Finally%2C%20we%20propose%20a%20generalized%20class%0Aof%20diffusion%20models%20that%20offers%20the%20best%20of%20both%20standard%20Gaussian%20denoising%0Adiffusion%20and%20inverse%20heat%20dissipation%2C%20which%20we%20call%20Blurring%20Diffusion%0AModels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.05557v3&entry.124074799=Read"},
{"title": "ODBO: Bayesian Optimization with Search Space Prescreening for Directed\n  Protein Evolution", "author": "Lixue Cheng and Ziyi Yang and Changyu Hsieh and Benben Liao and Shengyu Zhang", "abstract": "  Directed evolution is a versatile technique in protein engineering that\nmimics the process of natural selection by iteratively alternating between\nmutagenesis and screening in order to search for sequences that optimize a\ngiven property of interest, such as catalytic activity and binding affinity to\na specified target. However, the space of possible proteins is too large to\nsearch exhaustively in the laboratory, and functional proteins are scarce in\nthe vast sequence space. Machine learning (ML) approaches can accelerate\ndirected evolution by learning to map protein sequences to functions without\nbuilding a detailed model of the underlying physics, chemistry and biological\npathways. Despite the great potentials held by these ML methods, they encounter\nsevere challenges in identifying the most suitable sequences for a targeted\nfunction. These failures can be attributed to the common practice of adopting a\nhigh-dimensional feature representation for protein sequences and inefficient\nsearch methods. To address these issues, we propose an efficient, experimental\ndesign-oriented closed-loop optimization framework for protein directed\nevolution, termed ODBO, which employs a combination of novel low-dimensional\nprotein encoding strategy and Bayesian optimization enhanced with search space\nprescreening via outlier detection. We further design an initial sample\nselection strategy to minimize the number of experimental samples for training\nML models. We conduct and report four protein directed evolution experiments\nthat substantiate the capability of the proposed framework for finding of the\nvariants with properties of interest. We expect the ODBO framework to greatly\nreduce the experimental cost and time cost of directed evolution, and can be\nfurther generalized as a powerful tool for adaptive experimental design in a\nbroader context.\n", "link": "http://arxiv.org/abs/2205.09548v6", "date": "2024-05-01", "relevancy": 0.9609, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4814}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4506}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ODBO%3A%20Bayesian%20Optimization%20with%20Search%20Space%20Prescreening%20for%20Directed%0A%20%20Protein%20Evolution&body=Title%3A%20ODBO%3A%20Bayesian%20Optimization%20with%20Search%20Space%20Prescreening%20for%20Directed%0A%20%20Protein%20Evolution%0AAuthor%3A%20Lixue%20Cheng%20and%20Ziyi%20Yang%20and%20Changyu%20Hsieh%20and%20Benben%20Liao%20and%20Shengyu%20Zhang%0AAbstract%3A%20%20%20Directed%20evolution%20is%20a%20versatile%20technique%20in%20protein%20engineering%20that%0Amimics%20the%20process%20of%20natural%20selection%20by%20iteratively%20alternating%20between%0Amutagenesis%20and%20screening%20in%20order%20to%20search%20for%20sequences%20that%20optimize%20a%0Agiven%20property%20of%20interest%2C%20such%20as%20catalytic%20activity%20and%20binding%20affinity%20to%0Aa%20specified%20target.%20However%2C%20the%20space%20of%20possible%20proteins%20is%20too%20large%20to%0Asearch%20exhaustively%20in%20the%20laboratory%2C%20and%20functional%20proteins%20are%20scarce%20in%0Athe%20vast%20sequence%20space.%20Machine%20learning%20%28ML%29%20approaches%20can%20accelerate%0Adirected%20evolution%20by%20learning%20to%20map%20protein%20sequences%20to%20functions%20without%0Abuilding%20a%20detailed%20model%20of%20the%20underlying%20physics%2C%20chemistry%20and%20biological%0Apathways.%20Despite%20the%20great%20potentials%20held%20by%20these%20ML%20methods%2C%20they%20encounter%0Asevere%20challenges%20in%20identifying%20the%20most%20suitable%20sequences%20for%20a%20targeted%0Afunction.%20These%20failures%20can%20be%20attributed%20to%20the%20common%20practice%20of%20adopting%20a%0Ahigh-dimensional%20feature%20representation%20for%20protein%20sequences%20and%20inefficient%0Asearch%20methods.%20To%20address%20these%20issues%2C%20we%20propose%20an%20efficient%2C%20experimental%0Adesign-oriented%20closed-loop%20optimization%20framework%20for%20protein%20directed%0Aevolution%2C%20termed%20ODBO%2C%20which%20employs%20a%20combination%20of%20novel%20low-dimensional%0Aprotein%20encoding%20strategy%20and%20Bayesian%20optimization%20enhanced%20with%20search%20space%0Aprescreening%20via%20outlier%20detection.%20We%20further%20design%20an%20initial%20sample%0Aselection%20strategy%20to%20minimize%20the%20number%20of%20experimental%20samples%20for%20training%0AML%20models.%20We%20conduct%20and%20report%20four%20protein%20directed%20evolution%20experiments%0Athat%20substantiate%20the%20capability%20of%20the%20proposed%20framework%20for%20finding%20of%20the%0Avariants%20with%20properties%20of%20interest.%20We%20expect%20the%20ODBO%20framework%20to%20greatly%0Areduce%20the%20experimental%20cost%20and%20time%20cost%20of%20directed%20evolution%2C%20and%20can%20be%0Afurther%20generalized%20as%20a%20powerful%20tool%20for%20adaptive%20experimental%20design%20in%20a%0Abroader%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.09548v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODBO%3A%20Bayesian%20Optimization%20with%20Search%20Space%20Prescreening%20for%20Directed%0A%20%20Protein%20Evolution&entry.906535625=Lixue%20Cheng%20and%20Ziyi%20Yang%20and%20Changyu%20Hsieh%20and%20Benben%20Liao%20and%20Shengyu%20Zhang&entry.1292438233=%20%20Directed%20evolution%20is%20a%20versatile%20technique%20in%20protein%20engineering%20that%0Amimics%20the%20process%20of%20natural%20selection%20by%20iteratively%20alternating%20between%0Amutagenesis%20and%20screening%20in%20order%20to%20search%20for%20sequences%20that%20optimize%20a%0Agiven%20property%20of%20interest%2C%20such%20as%20catalytic%20activity%20and%20binding%20affinity%20to%0Aa%20specified%20target.%20However%2C%20the%20space%20of%20possible%20proteins%20is%20too%20large%20to%0Asearch%20exhaustively%20in%20the%20laboratory%2C%20and%20functional%20proteins%20are%20scarce%20in%0Athe%20vast%20sequence%20space.%20Machine%20learning%20%28ML%29%20approaches%20can%20accelerate%0Adirected%20evolution%20by%20learning%20to%20map%20protein%20sequences%20to%20functions%20without%0Abuilding%20a%20detailed%20model%20of%20the%20underlying%20physics%2C%20chemistry%20and%20biological%0Apathways.%20Despite%20the%20great%20potentials%20held%20by%20these%20ML%20methods%2C%20they%20encounter%0Asevere%20challenges%20in%20identifying%20the%20most%20suitable%20sequences%20for%20a%20targeted%0Afunction.%20These%20failures%20can%20be%20attributed%20to%20the%20common%20practice%20of%20adopting%20a%0Ahigh-dimensional%20feature%20representation%20for%20protein%20sequences%20and%20inefficient%0Asearch%20methods.%20To%20address%20these%20issues%2C%20we%20propose%20an%20efficient%2C%20experimental%0Adesign-oriented%20closed-loop%20optimization%20framework%20for%20protein%20directed%0Aevolution%2C%20termed%20ODBO%2C%20which%20employs%20a%20combination%20of%20novel%20low-dimensional%0Aprotein%20encoding%20strategy%20and%20Bayesian%20optimization%20enhanced%20with%20search%20space%0Aprescreening%20via%20outlier%20detection.%20We%20further%20design%20an%20initial%20sample%0Aselection%20strategy%20to%20minimize%20the%20number%20of%20experimental%20samples%20for%20training%0AML%20models.%20We%20conduct%20and%20report%20four%20protein%20directed%20evolution%20experiments%0Athat%20substantiate%20the%20capability%20of%20the%20proposed%20framework%20for%20finding%20of%20the%0Avariants%20with%20properties%20of%20interest.%20We%20expect%20the%20ODBO%20framework%20to%20greatly%0Areduce%20the%20experimental%20cost%20and%20time%20cost%20of%20directed%20evolution%2C%20and%20can%20be%0Afurther%20generalized%20as%20a%20powerful%20tool%20for%20adaptive%20experimental%20design%20in%20a%0Abroader%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.09548v6&entry.124074799=Read"},
{"title": "Influence Maximization with Unknown Individual Effect on General Network", "author": "Xinyan Su and Zhiheng Zhang and Jiyan Qiu and Jun Li", "abstract": "  The identification of a seed set to maximize information spread in a network\nis crucial, a concept known as Influence Maximization (IM). Elegant IM\nalgorithms could naturally extend to cases where each node is equipped with\nspecific weight, referred to as individual effect, to measure the node's\nimportance. Prevailing literature has typically assumed that the individual\neffect remains constant during the cascade process. However, this assumption is\nnot always feasible, as the individual effect of each node is primarily\nevaluated by the difference between the outputs in the activated and\nnon-activated states, with one of these states always being unobservable after\npropagation. Moreover, the individual effect is sensitive to the environmental\ninformation provided by surrounding nodes. To address these challenges, we\nextend the consideration of IM to a broader scenario involving general networks\nwith dynamic node individual effects, leveraging causality techniques. In our\npaper, we address this through the development of a Causal Influence\nMaximization (CauIM) algorithm. Theoretically, for CauIM, we present the\ngeneralized lower bound of influence spread and provide robustness analysis.\nEmpirically, in synthetic and real-world experiments, we demonstrate the\neffectiveness and robustness of CauIM, along with a novel acceleration\ntechnique.\n", "link": "http://arxiv.org/abs/2301.12226v2", "date": "2024-05-01", "relevancy": 1.3262, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4292}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.421}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Influence%20Maximization%20with%20Unknown%20Individual%20Effect%20on%20General%20Network&body=Title%3A%20Influence%20Maximization%20with%20Unknown%20Individual%20Effect%20on%20General%20Network%0AAuthor%3A%20Xinyan%20Su%20and%20Zhiheng%20Zhang%20and%20Jiyan%20Qiu%20and%20Jun%20Li%0AAbstract%3A%20%20%20The%20identification%20of%20a%20seed%20set%20to%20maximize%20information%20spread%20in%20a%20network%0Ais%20crucial%2C%20a%20concept%20known%20as%20Influence%20Maximization%20%28IM%29.%20Elegant%20IM%0Aalgorithms%20could%20naturally%20extend%20to%20cases%20where%20each%20node%20is%20equipped%20with%0Aspecific%20weight%2C%20referred%20to%20as%20individual%20effect%2C%20to%20measure%20the%20node%27s%0Aimportance.%20Prevailing%20literature%20has%20typically%20assumed%20that%20the%20individual%0Aeffect%20remains%20constant%20during%20the%20cascade%20process.%20However%2C%20this%20assumption%20is%0Anot%20always%20feasible%2C%20as%20the%20individual%20effect%20of%20each%20node%20is%20primarily%0Aevaluated%20by%20the%20difference%20between%20the%20outputs%20in%20the%20activated%20and%0Anon-activated%20states%2C%20with%20one%20of%20these%20states%20always%20being%20unobservable%20after%0Apropagation.%20Moreover%2C%20the%20individual%20effect%20is%20sensitive%20to%20the%20environmental%0Ainformation%20provided%20by%20surrounding%20nodes.%20To%20address%20these%20challenges%2C%20we%0Aextend%20the%20consideration%20of%20IM%20to%20a%20broader%20scenario%20involving%20general%20networks%0Awith%20dynamic%20node%20individual%20effects%2C%20leveraging%20causality%20techniques.%20In%20our%0Apaper%2C%20we%20address%20this%20through%20the%20development%20of%20a%20Causal%20Influence%0AMaximization%20%28CauIM%29%20algorithm.%20Theoretically%2C%20for%20CauIM%2C%20we%20present%20the%0Ageneralized%20lower%20bound%20of%20influence%20spread%20and%20provide%20robustness%20analysis.%0AEmpirically%2C%20in%20synthetic%20and%20real-world%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20and%20robustness%20of%20CauIM%2C%20along%20with%20a%20novel%20acceleration%0Atechnique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.12226v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influence%20Maximization%20with%20Unknown%20Individual%20Effect%20on%20General%20Network&entry.906535625=Xinyan%20Su%20and%20Zhiheng%20Zhang%20and%20Jiyan%20Qiu%20and%20Jun%20Li&entry.1292438233=%20%20The%20identification%20of%20a%20seed%20set%20to%20maximize%20information%20spread%20in%20a%20network%0Ais%20crucial%2C%20a%20concept%20known%20as%20Influence%20Maximization%20%28IM%29.%20Elegant%20IM%0Aalgorithms%20could%20naturally%20extend%20to%20cases%20where%20each%20node%20is%20equipped%20with%0Aspecific%20weight%2C%20referred%20to%20as%20individual%20effect%2C%20to%20measure%20the%20node%27s%0Aimportance.%20Prevailing%20literature%20has%20typically%20assumed%20that%20the%20individual%0Aeffect%20remains%20constant%20during%20the%20cascade%20process.%20However%2C%20this%20assumption%20is%0Anot%20always%20feasible%2C%20as%20the%20individual%20effect%20of%20each%20node%20is%20primarily%0Aevaluated%20by%20the%20difference%20between%20the%20outputs%20in%20the%20activated%20and%0Anon-activated%20states%2C%20with%20one%20of%20these%20states%20always%20being%20unobservable%20after%0Apropagation.%20Moreover%2C%20the%20individual%20effect%20is%20sensitive%20to%20the%20environmental%0Ainformation%20provided%20by%20surrounding%20nodes.%20To%20address%20these%20challenges%2C%20we%0Aextend%20the%20consideration%20of%20IM%20to%20a%20broader%20scenario%20involving%20general%20networks%0Awith%20dynamic%20node%20individual%20effects%2C%20leveraging%20causality%20techniques.%20In%20our%0Apaper%2C%20we%20address%20this%20through%20the%20development%20of%20a%20Causal%20Influence%0AMaximization%20%28CauIM%29%20algorithm.%20Theoretically%2C%20for%20CauIM%2C%20we%20present%20the%0Ageneralized%20lower%20bound%20of%20influence%20spread%20and%20provide%20robustness%20analysis.%0AEmpirically%2C%20in%20synthetic%20and%20real-world%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20and%20robustness%20of%20CauIM%2C%20along%20with%20a%20novel%20acceleration%0Atechnique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.12226v2&entry.124074799=Read"},
{"title": "Reverse Training to Nurse the Reversal Curse", "author": "Olga Golovneva and Zeyuan Allen-Zhu and Jason Weston and Sainbayar Sukhbaatar", "abstract": "  Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.\n", "link": "http://arxiv.org/abs/2403.13799v2", "date": "2024-05-01", "relevancy": 1.4019, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4873}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4695}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4418}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reverse%20Training%20to%20Nurse%20the%20Reversal%20Curse&body=Title%3A%20Reverse%20Training%20to%20Nurse%20the%20Reversal%20Curse%0AAuthor%3A%20Olga%20Golovneva%20and%20Zeyuan%20Allen-Zhu%20and%20Jason%20Weston%20and%20Sainbayar%20Sukhbaatar%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20a%20surprising%20failure%3A%20when%20trained%20on%20%22A%0Ahas%20a%20feature%20B%22%2C%20they%20do%20not%20generalize%20to%20%22B%20is%20a%20feature%20of%20A%22%2C%20which%20is%0Atermed%20the%20Reversal%20Curse.%20Even%20when%20training%20with%20trillions%20of%20tokens%20this%0Aissue%20still%20appears%20due%20to%20Zipf%27s%20law%20-%20hence%20even%20if%20we%20train%20on%20the%20entire%0Ainternet.%20This%20work%20proposes%20an%20alternative%20training%20scheme%2C%20called%20reverse%0Atraining%2C%20whereby%20all%20words%20are%20used%20twice%2C%20doubling%20the%20amount%20of%20available%0Atokens.%20The%20LLM%20is%20trained%20in%20both%20forward%20and%20reverse%20directions%20by%20reversing%0Athe%20training%20strings%20while%20preserving%20%28i.e.%2C%20not%20reversing%29%20chosen%20substrings%2C%0Asuch%20as%20entities.%20We%20show%20that%20data-matched%20reverse-trained%20models%20provide%0Asuperior%20performance%20to%20standard%20models%20on%20standard%20tasks%2C%20and%20compute-matched%0Areverse-trained%20models%20provide%20far%20superior%20performance%20on%20reversal%20tasks%2C%0Ahelping%20resolve%20the%20reversal%20curse%20issue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13799v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reverse%20Training%20to%20Nurse%20the%20Reversal%20Curse&entry.906535625=Olga%20Golovneva%20and%20Zeyuan%20Allen-Zhu%20and%20Jason%20Weston%20and%20Sainbayar%20Sukhbaatar&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20a%20surprising%20failure%3A%20when%20trained%20on%20%22A%0Ahas%20a%20feature%20B%22%2C%20they%20do%20not%20generalize%20to%20%22B%20is%20a%20feature%20of%20A%22%2C%20which%20is%0Atermed%20the%20Reversal%20Curse.%20Even%20when%20training%20with%20trillions%20of%20tokens%20this%0Aissue%20still%20appears%20due%20to%20Zipf%27s%20law%20-%20hence%20even%20if%20we%20train%20on%20the%20entire%0Ainternet.%20This%20work%20proposes%20an%20alternative%20training%20scheme%2C%20called%20reverse%0Atraining%2C%20whereby%20all%20words%20are%20used%20twice%2C%20doubling%20the%20amount%20of%20available%0Atokens.%20The%20LLM%20is%20trained%20in%20both%20forward%20and%20reverse%20directions%20by%20reversing%0Athe%20training%20strings%20while%20preserving%20%28i.e.%2C%20not%20reversing%29%20chosen%20substrings%2C%0Asuch%20as%20entities.%20We%20show%20that%20data-matched%20reverse-trained%20models%20provide%0Asuperior%20performance%20to%20standard%20models%20on%20standard%20tasks%2C%20and%20compute-matched%0Areverse-trained%20models%20provide%20far%20superior%20performance%20on%20reversal%20tasks%2C%0Ahelping%20resolve%20the%20reversal%20curse%20issue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13799v2&entry.124074799=Read"},
{"title": "CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions", "author": "Donghee Choi and Mogan Gim and Donghyeon Park and Mujeen Sung and Hyunjae Kim and Jaewoo Kang and Jihun Choi", "abstract": "  This paper introduces CookingSense, a descriptive collection of knowledge\nassertions in the culinary domain extracted from various sources, including web\ndata, scientific papers, and recipes, from which knowledge covering a broad\nrange of aspects is acquired. CookingSense is constructed through a series of\ndictionary-based filtering and language model-based semantic filtering\ntechniques, which results in a rich knowledgebase of multidisciplinary\nfood-related assertions. Additionally, we present FoodBench, a novel benchmark\nto evaluate culinary decision support systems. From evaluations with FoodBench,\nwe empirically prove that CookingSense improves the performance of retrieval\naugmented language models. We also validate the quality and variety of\nassertions in CookingSense through qualitative analysis.\n", "link": "http://arxiv.org/abs/2405.00523v1", "date": "2024-05-01", "relevancy": 0.8266, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4293}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4082}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4023}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CookingSense%3A%20A%20Culinary%20Knowledgebase%20with%20Multidisciplinary%20Assertions&body=Title%3A%20CookingSense%3A%20A%20Culinary%20Knowledgebase%20with%20Multidisciplinary%20Assertions%0AAuthor%3A%20Donghee%20Choi%20and%20Mogan%20Gim%20and%20Donghyeon%20Park%20and%20Mujeen%20Sung%20and%20Hyunjae%20Kim%20and%20Jaewoo%20Kang%20and%20Jihun%20Choi%0AAbstract%3A%20%20%20This%20paper%20introduces%20CookingSense%2C%20a%20descriptive%20collection%20of%20knowledge%0Aassertions%20in%20the%20culinary%20domain%20extracted%20from%20various%20sources%2C%20including%20web%0Adata%2C%20scientific%20papers%2C%20and%20recipes%2C%20from%20which%20knowledge%20covering%20a%20broad%0Arange%20of%20aspects%20is%20acquired.%20CookingSense%20is%20constructed%20through%20a%20series%20of%0Adictionary-based%20filtering%20and%20language%20model-based%20semantic%20filtering%0Atechniques%2C%20which%20results%20in%20a%20rich%20knowledgebase%20of%20multidisciplinary%0Afood-related%20assertions.%20Additionally%2C%20we%20present%20FoodBench%2C%20a%20novel%20benchmark%0Ato%20evaluate%20culinary%20decision%20support%20systems.%20From%20evaluations%20with%20FoodBench%2C%0Awe%20empirically%20prove%20that%20CookingSense%20improves%20the%20performance%20of%20retrieval%0Aaugmented%20language%20models.%20We%20also%20validate%20the%20quality%20and%20variety%20of%0Aassertions%20in%20CookingSense%20through%20qualitative%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00523v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CookingSense%3A%20A%20Culinary%20Knowledgebase%20with%20Multidisciplinary%20Assertions&entry.906535625=Donghee%20Choi%20and%20Mogan%20Gim%20and%20Donghyeon%20Park%20and%20Mujeen%20Sung%20and%20Hyunjae%20Kim%20and%20Jaewoo%20Kang%20and%20Jihun%20Choi&entry.1292438233=%20%20This%20paper%20introduces%20CookingSense%2C%20a%20descriptive%20collection%20of%20knowledge%0Aassertions%20in%20the%20culinary%20domain%20extracted%20from%20various%20sources%2C%20including%20web%0Adata%2C%20scientific%20papers%2C%20and%20recipes%2C%20from%20which%20knowledge%20covering%20a%20broad%0Arange%20of%20aspects%20is%20acquired.%20CookingSense%20is%20constructed%20through%20a%20series%20of%0Adictionary-based%20filtering%20and%20language%20model-based%20semantic%20filtering%0Atechniques%2C%20which%20results%20in%20a%20rich%20knowledgebase%20of%20multidisciplinary%0Afood-related%20assertions.%20Additionally%2C%20we%20present%20FoodBench%2C%20a%20novel%20benchmark%0Ato%20evaluate%20culinary%20decision%20support%20systems.%20From%20evaluations%20with%20FoodBench%2C%0Awe%20empirically%20prove%20that%20CookingSense%20improves%20the%20performance%20of%20retrieval%0Aaugmented%20language%20models.%20We%20also%20validate%20the%20quality%20and%20variety%20of%0Aassertions%20in%20CookingSense%20through%20qualitative%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00523v1&entry.124074799=Read"},
{"title": "Detection of ransomware attacks using federated learning based on the\n  CNN model", "author": "Hong-Nhung Nguyen and Ha-Thanh Nguyen and Damien Lescos", "abstract": "  Computing is still under a significant threat from ransomware, which\nnecessitates prompt action to prevent it. Ransomware attacks can have a\nnegative impact on how smart grids, particularly digital substations. In\naddition to examining a ransomware detection method using artificial\nintelligence (AI), this paper offers a ransomware attack modeling technique\nthat targets the disrupted operation of a digital substation. The first, binary\ndata is transformed into image data and fed into the convolution neural network\nmodel using federated learning. The experimental findings demonstrate that the\nsuggested technique detects ransomware with a high accuracy rate.\n", "link": "http://arxiv.org/abs/2405.00418v1", "date": "2024-05-01", "relevancy": 1.2462, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4238}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4167}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4038}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Detection%20of%20ransomware%20attacks%20using%20federated%20learning%20based%20on%20the%0A%20%20CNN%20model&body=Title%3A%20Detection%20of%20ransomware%20attacks%20using%20federated%20learning%20based%20on%20the%0A%20%20CNN%20model%0AAuthor%3A%20Hong-Nhung%20Nguyen%20and%20Ha-Thanh%20Nguyen%20and%20Damien%20Lescos%0AAbstract%3A%20%20%20Computing%20is%20still%20under%20a%20significant%20threat%20from%20ransomware%2C%20which%0Anecessitates%20prompt%20action%20to%20prevent%20it.%20Ransomware%20attacks%20can%20have%20a%0Anegative%20impact%20on%20how%20smart%20grids%2C%20particularly%20digital%20substations.%20In%0Aaddition%20to%20examining%20a%20ransomware%20detection%20method%20using%20artificial%0Aintelligence%20%28AI%29%2C%20this%20paper%20offers%20a%20ransomware%20attack%20modeling%20technique%0Athat%20targets%20the%20disrupted%20operation%20of%20a%20digital%20substation.%20The%20first%2C%20binary%0Adata%20is%20transformed%20into%20image%20data%20and%20fed%20into%20the%20convolution%20neural%20network%0Amodel%20using%20federated%20learning.%20The%20experimental%20findings%20demonstrate%20that%20the%0Asuggested%20technique%20detects%20ransomware%20with%20a%20high%20accuracy%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00418v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20of%20ransomware%20attacks%20using%20federated%20learning%20based%20on%20the%0A%20%20CNN%20model&entry.906535625=Hong-Nhung%20Nguyen%20and%20Ha-Thanh%20Nguyen%20and%20Damien%20Lescos&entry.1292438233=%20%20Computing%20is%20still%20under%20a%20significant%20threat%20from%20ransomware%2C%20which%0Anecessitates%20prompt%20action%20to%20prevent%20it.%20Ransomware%20attacks%20can%20have%20a%0Anegative%20impact%20on%20how%20smart%20grids%2C%20particularly%20digital%20substations.%20In%0Aaddition%20to%20examining%20a%20ransomware%20detection%20method%20using%20artificial%0Aintelligence%20%28AI%29%2C%20this%20paper%20offers%20a%20ransomware%20attack%20modeling%20technique%0Athat%20targets%20the%20disrupted%20operation%20of%20a%20digital%20substation.%20The%20first%2C%20binary%0Adata%20is%20transformed%20into%20image%20data%20and%20fed%20into%20the%20convolution%20neural%20network%0Amodel%20using%20federated%20learning.%20The%20experimental%20findings%20demonstrate%20that%20the%0Asuggested%20technique%20detects%20ransomware%20with%20a%20high%20accuracy%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00418v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


