<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251211.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting", "author": "Madhav Agarwal and Mingtian Zhang and Laura Sevilla-Lara and Steven McDonagh", "abstract": "Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.", "link": "http://arxiv.org/abs/2512.10939v1", "date": "2025-12-11", "relevancy": 3.5888, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7508}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7508}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianHeadTalk%3A%20Wobble-Free%203D%20Talking%20Heads%20with%20Audio%20Driven%20Gaussian%20Splatting&body=Title%3A%20GaussianHeadTalk%3A%20Wobble-Free%203D%20Talking%20Heads%20with%20Audio%20Driven%20Gaussian%20Splatting%0AAuthor%3A%20Madhav%20Agarwal%20and%20Mingtian%20Zhang%20and%20Laura%20Sevilla-Lara%20and%20Steven%20McDonagh%0AAbstract%3A%20Speech-driven%20talking%20heads%20have%20recently%20emerged%20and%20enable%20interactive%20avatars.%20However%2C%20real-world%20applications%20are%20limited%2C%20as%20current%20methods%20achieve%20high%20visual%20fidelity%20but%20slow%20or%20fast%20yet%20temporally%20unstable.%20Diffusion%20methods%20provide%20realistic%20image%20generation%2C%20yet%20struggle%20with%20oneshot%20settings.%20Gaussian%20Splatting%20approaches%20are%20real-time%2C%20yet%20inaccuracies%20in%20facial%20tracking%2C%20or%20inconsistent%20Gaussian%20mappings%2C%20lead%20to%20unstable%20outputs%20and%20video%20artifacts%20that%20are%20detrimental%20to%20realistic%20use%20cases.%20We%20address%20this%20problem%20by%20mapping%20Gaussian%20Splatting%20using%203D%20Morphable%20Models%20to%20generate%20person-specific%20avatars.%20We%20introduce%20transformer-based%20prediction%20of%20model%20parameters%2C%20directly%20from%20audio%2C%20to%20drive%20temporal%20consistency.%20From%20monocular%20video%20and%20independent%20audio%20speech%20inputs%2C%20our%20method%20enables%20generation%20of%20real-time%20talking%20head%20videos%20where%20we%20report%20competitive%20quantitative%20and%20qualitative%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianHeadTalk%253A%2520Wobble-Free%25203D%2520Talking%2520Heads%2520with%2520Audio%2520Driven%2520Gaussian%2520Splatting%26entry.906535625%3DMadhav%2520Agarwal%2520and%2520Mingtian%2520Zhang%2520and%2520Laura%2520Sevilla-Lara%2520and%2520Steven%2520McDonagh%26entry.1292438233%3DSpeech-driven%2520talking%2520heads%2520have%2520recently%2520emerged%2520and%2520enable%2520interactive%2520avatars.%2520However%252C%2520real-world%2520applications%2520are%2520limited%252C%2520as%2520current%2520methods%2520achieve%2520high%2520visual%2520fidelity%2520but%2520slow%2520or%2520fast%2520yet%2520temporally%2520unstable.%2520Diffusion%2520methods%2520provide%2520realistic%2520image%2520generation%252C%2520yet%2520struggle%2520with%2520oneshot%2520settings.%2520Gaussian%2520Splatting%2520approaches%2520are%2520real-time%252C%2520yet%2520inaccuracies%2520in%2520facial%2520tracking%252C%2520or%2520inconsistent%2520Gaussian%2520mappings%252C%2520lead%2520to%2520unstable%2520outputs%2520and%2520video%2520artifacts%2520that%2520are%2520detrimental%2520to%2520realistic%2520use%2520cases.%2520We%2520address%2520this%2520problem%2520by%2520mapping%2520Gaussian%2520Splatting%2520using%25203D%2520Morphable%2520Models%2520to%2520generate%2520person-specific%2520avatars.%2520We%2520introduce%2520transformer-based%2520prediction%2520of%2520model%2520parameters%252C%2520directly%2520from%2520audio%252C%2520to%2520drive%2520temporal%2520consistency.%2520From%2520monocular%2520video%2520and%2520independent%2520audio%2520speech%2520inputs%252C%2520our%2520method%2520enables%2520generation%2520of%2520real-time%2520talking%2520head%2520videos%2520where%2520we%2520report%2520competitive%2520quantitative%2520and%2520qualitative%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianHeadTalk%3A%20Wobble-Free%203D%20Talking%20Heads%20with%20Audio%20Driven%20Gaussian%20Splatting&entry.906535625=Madhav%20Agarwal%20and%20Mingtian%20Zhang%20and%20Laura%20Sevilla-Lara%20and%20Steven%20McDonagh&entry.1292438233=Speech-driven%20talking%20heads%20have%20recently%20emerged%20and%20enable%20interactive%20avatars.%20However%2C%20real-world%20applications%20are%20limited%2C%20as%20current%20methods%20achieve%20high%20visual%20fidelity%20but%20slow%20or%20fast%20yet%20temporally%20unstable.%20Diffusion%20methods%20provide%20realistic%20image%20generation%2C%20yet%20struggle%20with%20oneshot%20settings.%20Gaussian%20Splatting%20approaches%20are%20real-time%2C%20yet%20inaccuracies%20in%20facial%20tracking%2C%20or%20inconsistent%20Gaussian%20mappings%2C%20lead%20to%20unstable%20outputs%20and%20video%20artifacts%20that%20are%20detrimental%20to%20realistic%20use%20cases.%20We%20address%20this%20problem%20by%20mapping%20Gaussian%20Splatting%20using%203D%20Morphable%20Models%20to%20generate%20person-specific%20avatars.%20We%20introduce%20transformer-based%20prediction%20of%20model%20parameters%2C%20directly%20from%20audio%2C%20to%20drive%20temporal%20consistency.%20From%20monocular%20video%20and%20independent%20audio%20speech%20inputs%2C%20our%20method%20enables%20generation%20of%20real-time%20talking%20head%20videos%20where%20we%20report%20competitive%20quantitative%20and%20qualitative%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.10939v1&entry.124074799=Read"},
{"title": "From Generated Human Videos to Physically Plausible Robot Trajectories", "author": "James Ni and Zekai Wang and Wei Lin and Amir Bar and Yann LeCun and Trevor Darrell and Jitendra Malik and Roei Herzig", "abstract": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.", "link": "http://arxiv.org/abs/2512.05094v2", "date": "2025-12-11", "relevancy": 3.3172, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6827}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6691}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Generated%20Human%20Videos%20to%20Physically%20Plausible%20Robot%20Trajectories&body=Title%3A%20From%20Generated%20Human%20Videos%20to%20Physically%20Plausible%20Robot%20Trajectories%0AAuthor%3A%20James%20Ni%20and%20Zekai%20Wang%20and%20Wei%20Lin%20and%20Amir%20Bar%20and%20Yann%20LeCun%20and%20Trevor%20Darrell%20and%20Jitendra%20Malik%20and%20Roei%20Herzig%0AAbstract%3A%20Video%20generation%20models%20are%20rapidly%20improving%20in%20their%20ability%20to%20synthesize%20human%20actions%20in%20novel%20contexts%2C%20holding%20the%20potential%20to%20serve%20as%20high-level%20planners%20for%20contextual%20robot%20control.%20To%20realize%20this%20potential%2C%20a%20key%20research%20question%20remains%20open%3A%20how%20can%20a%20humanoid%20execute%20the%20human%20actions%20from%20generated%20videos%20in%20a%20zero-shot%20manner%3F%20This%20challenge%20arises%20because%20generated%20videos%20are%20often%20noisy%20and%20exhibit%20morphological%20distortions%20that%20make%20direct%20imitation%20difficult%20compared%20to%20real%20video.%20To%20address%20this%2C%20we%20introduce%20a%20two-stage%20pipeline.%20First%2C%20we%20lift%20video%20pixels%20into%20a%204D%20human%20representation%20and%20then%20retarget%20to%20the%20humanoid%20morphology.%20Second%2C%20we%20propose%20GenMimic-a%20physics-aware%20reinforcement%20learning%20policy%20conditioned%20on%203D%20keypoints%2C%20and%20trained%20with%20symmetry%20regularization%20and%20keypoint-weighted%20tracking%20rewards.%20As%20a%20result%2C%20GenMimic%20can%20mimic%20human%20actions%20from%20noisy%2C%20generated%20videos.%20We%20curate%20GenMimicBench%2C%20a%20synthetic%20human-motion%20dataset%20generated%20using%20two%20video%20generation%20models%20across%20a%20spectrum%20of%20actions%20and%20contexts%2C%20establishing%20a%20benchmark%20for%20assessing%20zero-shot%20generalization%20and%20policy%20robustness.%20Extensive%20experiments%20demonstrate%20improvements%20over%20strong%20baselines%20in%20simulation%20and%20confirm%20coherent%2C%20physically%20stable%20motion%20tracking%20on%20a%20Unitree%20G1%20humanoid%20robot%20without%20fine-tuning.%20This%20work%20offers%20a%20promising%20path%20to%20realizing%20the%20potential%20of%20video%20generation%20models%20as%20high-level%20policies%20for%20robot%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Generated%2520Human%2520Videos%2520to%2520Physically%2520Plausible%2520Robot%2520Trajectories%26entry.906535625%3DJames%2520Ni%2520and%2520Zekai%2520Wang%2520and%2520Wei%2520Lin%2520and%2520Amir%2520Bar%2520and%2520Yann%2520LeCun%2520and%2520Trevor%2520Darrell%2520and%2520Jitendra%2520Malik%2520and%2520Roei%2520Herzig%26entry.1292438233%3DVideo%2520generation%2520models%2520are%2520rapidly%2520improving%2520in%2520their%2520ability%2520to%2520synthesize%2520human%2520actions%2520in%2520novel%2520contexts%252C%2520holding%2520the%2520potential%2520to%2520serve%2520as%2520high-level%2520planners%2520for%2520contextual%2520robot%2520control.%2520To%2520realize%2520this%2520potential%252C%2520a%2520key%2520research%2520question%2520remains%2520open%253A%2520how%2520can%2520a%2520humanoid%2520execute%2520the%2520human%2520actions%2520from%2520generated%2520videos%2520in%2520a%2520zero-shot%2520manner%253F%2520This%2520challenge%2520arises%2520because%2520generated%2520videos%2520are%2520often%2520noisy%2520and%2520exhibit%2520morphological%2520distortions%2520that%2520make%2520direct%2520imitation%2520difficult%2520compared%2520to%2520real%2520video.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520two-stage%2520pipeline.%2520First%252C%2520we%2520lift%2520video%2520pixels%2520into%2520a%25204D%2520human%2520representation%2520and%2520then%2520retarget%2520to%2520the%2520humanoid%2520morphology.%2520Second%252C%2520we%2520propose%2520GenMimic-a%2520physics-aware%2520reinforcement%2520learning%2520policy%2520conditioned%2520on%25203D%2520keypoints%252C%2520and%2520trained%2520with%2520symmetry%2520regularization%2520and%2520keypoint-weighted%2520tracking%2520rewards.%2520As%2520a%2520result%252C%2520GenMimic%2520can%2520mimic%2520human%2520actions%2520from%2520noisy%252C%2520generated%2520videos.%2520We%2520curate%2520GenMimicBench%252C%2520a%2520synthetic%2520human-motion%2520dataset%2520generated%2520using%2520two%2520video%2520generation%2520models%2520across%2520a%2520spectrum%2520of%2520actions%2520and%2520contexts%252C%2520establishing%2520a%2520benchmark%2520for%2520assessing%2520zero-shot%2520generalization%2520and%2520policy%2520robustness.%2520Extensive%2520experiments%2520demonstrate%2520improvements%2520over%2520strong%2520baselines%2520in%2520simulation%2520and%2520confirm%2520coherent%252C%2520physically%2520stable%2520motion%2520tracking%2520on%2520a%2520Unitree%2520G1%2520humanoid%2520robot%2520without%2520fine-tuning.%2520This%2520work%2520offers%2520a%2520promising%2520path%2520to%2520realizing%2520the%2520potential%2520of%2520video%2520generation%2520models%2520as%2520high-level%2520policies%2520for%2520robot%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Generated%20Human%20Videos%20to%20Physically%20Plausible%20Robot%20Trajectories&entry.906535625=James%20Ni%20and%20Zekai%20Wang%20and%20Wei%20Lin%20and%20Amir%20Bar%20and%20Yann%20LeCun%20and%20Trevor%20Darrell%20and%20Jitendra%20Malik%20and%20Roei%20Herzig&entry.1292438233=Video%20generation%20models%20are%20rapidly%20improving%20in%20their%20ability%20to%20synthesize%20human%20actions%20in%20novel%20contexts%2C%20holding%20the%20potential%20to%20serve%20as%20high-level%20planners%20for%20contextual%20robot%20control.%20To%20realize%20this%20potential%2C%20a%20key%20research%20question%20remains%20open%3A%20how%20can%20a%20humanoid%20execute%20the%20human%20actions%20from%20generated%20videos%20in%20a%20zero-shot%20manner%3F%20This%20challenge%20arises%20because%20generated%20videos%20are%20often%20noisy%20and%20exhibit%20morphological%20distortions%20that%20make%20direct%20imitation%20difficult%20compared%20to%20real%20video.%20To%20address%20this%2C%20we%20introduce%20a%20two-stage%20pipeline.%20First%2C%20we%20lift%20video%20pixels%20into%20a%204D%20human%20representation%20and%20then%20retarget%20to%20the%20humanoid%20morphology.%20Second%2C%20we%20propose%20GenMimic-a%20physics-aware%20reinforcement%20learning%20policy%20conditioned%20on%203D%20keypoints%2C%20and%20trained%20with%20symmetry%20regularization%20and%20keypoint-weighted%20tracking%20rewards.%20As%20a%20result%2C%20GenMimic%20can%20mimic%20human%20actions%20from%20noisy%2C%20generated%20videos.%20We%20curate%20GenMimicBench%2C%20a%20synthetic%20human-motion%20dataset%20generated%20using%20two%20video%20generation%20models%20across%20a%20spectrum%20of%20actions%20and%20contexts%2C%20establishing%20a%20benchmark%20for%20assessing%20zero-shot%20generalization%20and%20policy%20robustness.%20Extensive%20experiments%20demonstrate%20improvements%20over%20strong%20baselines%20in%20simulation%20and%20confirm%20coherent%2C%20physically%20stable%20motion%20tracking%20on%20a%20Unitree%20G1%20humanoid%20robot%20without%20fine-tuning.%20This%20work%20offers%20a%20promising%20path%20to%20realizing%20the%20potential%20of%20video%20generation%20models%20as%20high-level%20policies%20for%20robot%20control.&entry.1838667208=http%3A//arxiv.org/abs/2512.05094v2&entry.124074799=Read"},
{"title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "author": "Siyu Chen and Ting Han and Chengzheng Fu and Changshe Zhang and Chaolei Wang and Jinhe Su and Guorong Cai and Meiliu Wu", "abstract": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in semantic segmentation (DGSS) highlight a subtle complementarity that motivates Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS aims to generate pixel-level masks for unseen categories while maintaining robustness across unseen domains, a critical capability for real-world scenarios such as autonomous driving in adverse conditions. We introduce Vireo, a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS and DGSS for the first time. Vireo builds upon the frozen Visual Foundation Models (VFMs) and incorporates scene geometry via Depth VFMs to extract domain-invariant structural features. To bridge the gap between visual and textual modalities under domain shift, we propose three key components: (1) GeoText Prompts, which align geometric features with language cues and progressively refine VFM encoder representations; (2) Coarse Mask Prior Embedding (CMPE) for enhancing gradient flow for faster convergence and stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH), which fuses refined structural and semantic features for robust prediction. Comprehensive evaluation on these components demonstrates the effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art performance and surpasses existing methods by a large margin in both domain generalization and open-vocabulary recognition, offering a unified and scalable solution for robust visual understanding in diverse and dynamic environments. Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "link": "http://arxiv.org/abs/2506.09881v3", "date": "2025-12-11", "relevancy": 3.0558, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6334}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation&body=Title%3A%20Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation%0AAuthor%3A%20Siyu%20Chen%20and%20Ting%20Han%20and%20Chengzheng%20Fu%20and%20Changshe%20Zhang%20and%20Chaolei%20Wang%20and%20Jinhe%20Su%20and%20Guorong%20Cai%20and%20Meiliu%20Wu%0AAbstract%3A%20Open-Vocabulary%20semantic%20segmentation%20%28OVSS%29%20and%20domain%20generalization%20in%20semantic%20segmentation%20%28DGSS%29%20highlight%20a%20subtle%20complementarity%20that%20motivates%20Open-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation%20%28OV-DGSS%29.%20OV-DGSS%20aims%20to%20generate%20pixel-level%20masks%20for%20unseen%20categories%20while%20maintaining%20robustness%20across%20unseen%20domains%2C%20a%20critical%20capability%20for%20real-world%20scenarios%20such%20as%20autonomous%20driving%20in%20adverse%20conditions.%20We%20introduce%20Vireo%2C%20a%20novel%20single-stage%20framework%20for%20OV-DGSS%20that%20unifies%20the%20strengths%20of%20OVSS%20and%20DGSS%20for%20the%20first%20time.%20Vireo%20builds%20upon%20the%20frozen%20Visual%20Foundation%20Models%20%28VFMs%29%20and%20incorporates%20scene%20geometry%20via%20Depth%20VFMs%20to%20extract%20domain-invariant%20structural%20features.%20To%20bridge%20the%20gap%20between%20visual%20and%20textual%20modalities%20under%20domain%20shift%2C%20we%20propose%20three%20key%20components%3A%20%281%29%20GeoText%20Prompts%2C%20which%20align%20geometric%20features%20with%20language%20cues%20and%20progressively%20refine%20VFM%20encoder%20representations%3B%20%282%29%20Coarse%20Mask%20Prior%20Embedding%20%28CMPE%29%20for%20enhancing%20gradient%20flow%20for%20faster%20convergence%20and%20stronger%20textual%20influence%3B%20and%20%283%29%20the%20Domain-Open-Vocabulary%20Vector%20Embedding%20Head%20%28DOV-VEH%29%2C%20which%20fuses%20refined%20structural%20and%20semantic%20features%20for%20robust%20prediction.%20Comprehensive%20evaluation%20on%20these%20components%20demonstrates%20the%20effectiveness%20of%20our%20designs.%20Our%20proposed%20Vireo%20achieves%20the%20state-of-the-art%20performance%20and%20surpasses%20existing%20methods%20by%20a%20large%20margin%20in%20both%20domain%20generalization%20and%20open-vocabulary%20recognition%2C%20offering%20a%20unified%20and%20scalable%20solution%20for%20robust%20visual%20understanding%20in%20diverse%20and%20dynamic%20environments.%20Code%20is%20available%20at%20https%3A//github.com/anonymouse-9c53tp182bvz/Vireo.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09881v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Depth%2520and%2520Language%2520for%2520Open-Vocabulary%2520Domain-Generalized%2520Semantic%2520Segmentation%26entry.906535625%3DSiyu%2520Chen%2520and%2520Ting%2520Han%2520and%2520Chengzheng%2520Fu%2520and%2520Changshe%2520Zhang%2520and%2520Chaolei%2520Wang%2520and%2520Jinhe%2520Su%2520and%2520Guorong%2520Cai%2520and%2520Meiliu%2520Wu%26entry.1292438233%3DOpen-Vocabulary%2520semantic%2520segmentation%2520%2528OVSS%2529%2520and%2520domain%2520generalization%2520in%2520semantic%2520segmentation%2520%2528DGSS%2529%2520highlight%2520a%2520subtle%2520complementarity%2520that%2520motivates%2520Open-Vocabulary%2520Domain-Generalized%2520Semantic%2520Segmentation%2520%2528OV-DGSS%2529.%2520OV-DGSS%2520aims%2520to%2520generate%2520pixel-level%2520masks%2520for%2520unseen%2520categories%2520while%2520maintaining%2520robustness%2520across%2520unseen%2520domains%252C%2520a%2520critical%2520capability%2520for%2520real-world%2520scenarios%2520such%2520as%2520autonomous%2520driving%2520in%2520adverse%2520conditions.%2520We%2520introduce%2520Vireo%252C%2520a%2520novel%2520single-stage%2520framework%2520for%2520OV-DGSS%2520that%2520unifies%2520the%2520strengths%2520of%2520OVSS%2520and%2520DGSS%2520for%2520the%2520first%2520time.%2520Vireo%2520builds%2520upon%2520the%2520frozen%2520Visual%2520Foundation%2520Models%2520%2528VFMs%2529%2520and%2520incorporates%2520scene%2520geometry%2520via%2520Depth%2520VFMs%2520to%2520extract%2520domain-invariant%2520structural%2520features.%2520To%2520bridge%2520the%2520gap%2520between%2520visual%2520and%2520textual%2520modalities%2520under%2520domain%2520shift%252C%2520we%2520propose%2520three%2520key%2520components%253A%2520%25281%2529%2520GeoText%2520Prompts%252C%2520which%2520align%2520geometric%2520features%2520with%2520language%2520cues%2520and%2520progressively%2520refine%2520VFM%2520encoder%2520representations%253B%2520%25282%2529%2520Coarse%2520Mask%2520Prior%2520Embedding%2520%2528CMPE%2529%2520for%2520enhancing%2520gradient%2520flow%2520for%2520faster%2520convergence%2520and%2520stronger%2520textual%2520influence%253B%2520and%2520%25283%2529%2520the%2520Domain-Open-Vocabulary%2520Vector%2520Embedding%2520Head%2520%2528DOV-VEH%2529%252C%2520which%2520fuses%2520refined%2520structural%2520and%2520semantic%2520features%2520for%2520robust%2520prediction.%2520Comprehensive%2520evaluation%2520on%2520these%2520components%2520demonstrates%2520the%2520effectiveness%2520of%2520our%2520designs.%2520Our%2520proposed%2520Vireo%2520achieves%2520the%2520state-of-the-art%2520performance%2520and%2520surpasses%2520existing%2520methods%2520by%2520a%2520large%2520margin%2520in%2520both%2520domain%2520generalization%2520and%2520open-vocabulary%2520recognition%252C%2520offering%2520a%2520unified%2520and%2520scalable%2520solution%2520for%2520robust%2520visual%2520understanding%2520in%2520diverse%2520and%2520dynamic%2520environments.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/anonymouse-9c53tp182bvz/Vireo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09881v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation&entry.906535625=Siyu%20Chen%20and%20Ting%20Han%20and%20Chengzheng%20Fu%20and%20Changshe%20Zhang%20and%20Chaolei%20Wang%20and%20Jinhe%20Su%20and%20Guorong%20Cai%20and%20Meiliu%20Wu&entry.1292438233=Open-Vocabulary%20semantic%20segmentation%20%28OVSS%29%20and%20domain%20generalization%20in%20semantic%20segmentation%20%28DGSS%29%20highlight%20a%20subtle%20complementarity%20that%20motivates%20Open-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation%20%28OV-DGSS%29.%20OV-DGSS%20aims%20to%20generate%20pixel-level%20masks%20for%20unseen%20categories%20while%20maintaining%20robustness%20across%20unseen%20domains%2C%20a%20critical%20capability%20for%20real-world%20scenarios%20such%20as%20autonomous%20driving%20in%20adverse%20conditions.%20We%20introduce%20Vireo%2C%20a%20novel%20single-stage%20framework%20for%20OV-DGSS%20that%20unifies%20the%20strengths%20of%20OVSS%20and%20DGSS%20for%20the%20first%20time.%20Vireo%20builds%20upon%20the%20frozen%20Visual%20Foundation%20Models%20%28VFMs%29%20and%20incorporates%20scene%20geometry%20via%20Depth%20VFMs%20to%20extract%20domain-invariant%20structural%20features.%20To%20bridge%20the%20gap%20between%20visual%20and%20textual%20modalities%20under%20domain%20shift%2C%20we%20propose%20three%20key%20components%3A%20%281%29%20GeoText%20Prompts%2C%20which%20align%20geometric%20features%20with%20language%20cues%20and%20progressively%20refine%20VFM%20encoder%20representations%3B%20%282%29%20Coarse%20Mask%20Prior%20Embedding%20%28CMPE%29%20for%20enhancing%20gradient%20flow%20for%20faster%20convergence%20and%20stronger%20textual%20influence%3B%20and%20%283%29%20the%20Domain-Open-Vocabulary%20Vector%20Embedding%20Head%20%28DOV-VEH%29%2C%20which%20fuses%20refined%20structural%20and%20semantic%20features%20for%20robust%20prediction.%20Comprehensive%20evaluation%20on%20these%20components%20demonstrates%20the%20effectiveness%20of%20our%20designs.%20Our%20proposed%20Vireo%20achieves%20the%20state-of-the-art%20performance%20and%20surpasses%20existing%20methods%20by%20a%20large%20margin%20in%20both%20domain%20generalization%20and%20open-vocabulary%20recognition%2C%20offering%20a%20unified%20and%20scalable%20solution%20for%20robust%20visual%20understanding%20in%20diverse%20and%20dynamic%20environments.%20Code%20is%20available%20at%20https%3A//github.com/anonymouse-9c53tp182bvz/Vireo.&entry.1838667208=http%3A//arxiv.org/abs/2506.09881v3&entry.124074799=Read"},
{"title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training", "author": "Qitao Zhao and Hao Tan and Qianqian Wang and Sai Bi and Kai Zhang and Kalyan Sunkavalli and Shubham Tulsiani and Hanwen Jiang", "abstract": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.", "link": "http://arxiv.org/abs/2512.10950v1", "date": "2025-12-11", "relevancy": 3.0496, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6126}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E-RayZer%3A%20Self-supervised%203D%20Reconstruction%20as%20Spatial%20Visual%20Pre-training&body=Title%3A%20E-RayZer%3A%20Self-supervised%203D%20Reconstruction%20as%20Spatial%20Visual%20Pre-training%0AAuthor%3A%20Qitao%20Zhao%20and%20Hao%20Tan%20and%20Qianqian%20Wang%20and%20Sai%20Bi%20and%20Kai%20Zhang%20and%20Kalyan%20Sunkavalli%20and%20Shubham%20Tulsiani%20and%20Hanwen%20Jiang%0AAbstract%3A%20Self-supervised%20pre-training%20has%20revolutionized%20foundation%20models%20for%20languages%2C%20individual%202D%20images%20and%20videos%2C%20but%20remains%20largely%20unexplored%20for%20learning%203D-aware%20representations%20from%20multi-view%20images.%20In%20this%20paper%2C%20we%20present%20E-RayZer%2C%20a%20self-supervised%20large%203D%20Vision%20model%20that%20learns%20truly%203D-aware%20representations%20directly%20from%20unlabeled%20images.%20Unlike%20prior%20self-supervised%20methods%20such%20as%20RayZer%20that%20infer%203D%20indirectly%20through%20latent-space%20view%20synthesis%2C%20E-RayZer%20operates%20directly%20in%203D%20space%2C%20performing%20self-supervised%203D%20reconstruction%20with%20Explicit%20geometry.%20This%20formulation%20eliminates%20shortcut%20solutions%20and%20yields%20representations%20that%20are%20geometrically%20grounded.%20To%20ensure%20convergence%20and%20scalability%2C%20we%20introduce%20a%20novel%20fine-grained%20learning%20curriculum%20that%20organizes%20training%20from%20easy%20to%20hard%20samples%20and%20harmonizes%20heterogeneous%20data%20sources%20in%20an%20entirely%20unsupervised%20manner.%20Experiments%20demonstrate%20that%20E-RayZer%20significantly%20outperforms%20RayZer%20on%20pose%20estimation%2C%20matches%20or%20sometimes%20surpasses%20fully%20supervised%20reconstruction%20models%20such%20as%20VGGT.%20Furthermore%2C%20its%20learned%20representations%20outperform%20leading%20visual%20pre-training%20models%20%28e.g.%2C%20DINOv3%2C%20CroCo%20v2%2C%20VideoMAE%20V2%2C%20and%20RayZer%29%20when%20transferring%20to%203D%20downstream%20tasks%2C%20establishing%20E-RayZer%20as%20a%20new%20paradigm%20for%203D-aware%20visual%20pre-training.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE-RayZer%253A%2520Self-supervised%25203D%2520Reconstruction%2520as%2520Spatial%2520Visual%2520Pre-training%26entry.906535625%3DQitao%2520Zhao%2520and%2520Hao%2520Tan%2520and%2520Qianqian%2520Wang%2520and%2520Sai%2520Bi%2520and%2520Kai%2520Zhang%2520and%2520Kalyan%2520Sunkavalli%2520and%2520Shubham%2520Tulsiani%2520and%2520Hanwen%2520Jiang%26entry.1292438233%3DSelf-supervised%2520pre-training%2520has%2520revolutionized%2520foundation%2520models%2520for%2520languages%252C%2520individual%25202D%2520images%2520and%2520videos%252C%2520but%2520remains%2520largely%2520unexplored%2520for%2520learning%25203D-aware%2520representations%2520from%2520multi-view%2520images.%2520In%2520this%2520paper%252C%2520we%2520present%2520E-RayZer%252C%2520a%2520self-supervised%2520large%25203D%2520Vision%2520model%2520that%2520learns%2520truly%25203D-aware%2520representations%2520directly%2520from%2520unlabeled%2520images.%2520Unlike%2520prior%2520self-supervised%2520methods%2520such%2520as%2520RayZer%2520that%2520infer%25203D%2520indirectly%2520through%2520latent-space%2520view%2520synthesis%252C%2520E-RayZer%2520operates%2520directly%2520in%25203D%2520space%252C%2520performing%2520self-supervised%25203D%2520reconstruction%2520with%2520Explicit%2520geometry.%2520This%2520formulation%2520eliminates%2520shortcut%2520solutions%2520and%2520yields%2520representations%2520that%2520are%2520geometrically%2520grounded.%2520To%2520ensure%2520convergence%2520and%2520scalability%252C%2520we%2520introduce%2520a%2520novel%2520fine-grained%2520learning%2520curriculum%2520that%2520organizes%2520training%2520from%2520easy%2520to%2520hard%2520samples%2520and%2520harmonizes%2520heterogeneous%2520data%2520sources%2520in%2520an%2520entirely%2520unsupervised%2520manner.%2520Experiments%2520demonstrate%2520that%2520E-RayZer%2520significantly%2520outperforms%2520RayZer%2520on%2520pose%2520estimation%252C%2520matches%2520or%2520sometimes%2520surpasses%2520fully%2520supervised%2520reconstruction%2520models%2520such%2520as%2520VGGT.%2520Furthermore%252C%2520its%2520learned%2520representations%2520outperform%2520leading%2520visual%2520pre-training%2520models%2520%2528e.g.%252C%2520DINOv3%252C%2520CroCo%2520v2%252C%2520VideoMAE%2520V2%252C%2520and%2520RayZer%2529%2520when%2520transferring%2520to%25203D%2520downstream%2520tasks%252C%2520establishing%2520E-RayZer%2520as%2520a%2520new%2520paradigm%2520for%25203D-aware%2520visual%2520pre-training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-RayZer%3A%20Self-supervised%203D%20Reconstruction%20as%20Spatial%20Visual%20Pre-training&entry.906535625=Qitao%20Zhao%20and%20Hao%20Tan%20and%20Qianqian%20Wang%20and%20Sai%20Bi%20and%20Kai%20Zhang%20and%20Kalyan%20Sunkavalli%20and%20Shubham%20Tulsiani%20and%20Hanwen%20Jiang&entry.1292438233=Self-supervised%20pre-training%20has%20revolutionized%20foundation%20models%20for%20languages%2C%20individual%202D%20images%20and%20videos%2C%20but%20remains%20largely%20unexplored%20for%20learning%203D-aware%20representations%20from%20multi-view%20images.%20In%20this%20paper%2C%20we%20present%20E-RayZer%2C%20a%20self-supervised%20large%203D%20Vision%20model%20that%20learns%20truly%203D-aware%20representations%20directly%20from%20unlabeled%20images.%20Unlike%20prior%20self-supervised%20methods%20such%20as%20RayZer%20that%20infer%203D%20indirectly%20through%20latent-space%20view%20synthesis%2C%20E-RayZer%20operates%20directly%20in%203D%20space%2C%20performing%20self-supervised%203D%20reconstruction%20with%20Explicit%20geometry.%20This%20formulation%20eliminates%20shortcut%20solutions%20and%20yields%20representations%20that%20are%20geometrically%20grounded.%20To%20ensure%20convergence%20and%20scalability%2C%20we%20introduce%20a%20novel%20fine-grained%20learning%20curriculum%20that%20organizes%20training%20from%20easy%20to%20hard%20samples%20and%20harmonizes%20heterogeneous%20data%20sources%20in%20an%20entirely%20unsupervised%20manner.%20Experiments%20demonstrate%20that%20E-RayZer%20significantly%20outperforms%20RayZer%20on%20pose%20estimation%2C%20matches%20or%20sometimes%20surpasses%20fully%20supervised%20reconstruction%20models%20such%20as%20VGGT.%20Furthermore%2C%20its%20learned%20representations%20outperform%20leading%20visual%20pre-training%20models%20%28e.g.%2C%20DINOv3%2C%20CroCo%20v2%2C%20VideoMAE%20V2%2C%20and%20RayZer%29%20when%20transferring%20to%203D%20downstream%20tasks%2C%20establishing%20E-RayZer%20as%20a%20new%20paradigm%20for%203D-aware%20visual%20pre-training.&entry.1838667208=http%3A//arxiv.org/abs/2512.10950v1&entry.124074799=Read"},
{"title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model", "author": "Yukai Shi and Weiyu Li and Zihao Wang and Hongyang Li and Xingyu Chen and Ping Tan and Lei Zhang", "abstract": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.", "link": "http://arxiv.org/abs/2512.10957v1", "date": "2025-12-11", "relevancy": 3.0089, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6047}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneMaker%3A%20Open-set%203D%20Scene%20Generation%20with%20Decoupled%20De-occlusion%20and%20Pose%20Estimation%20Model&body=Title%3A%20SceneMaker%3A%20Open-set%203D%20Scene%20Generation%20with%20Decoupled%20De-occlusion%20and%20Pose%20Estimation%20Model%0AAuthor%3A%20Yukai%20Shi%20and%20Weiyu%20Li%20and%20Zihao%20Wang%20and%20Hongyang%20Li%20and%20Xingyu%20Chen%20and%20Ping%20Tan%20and%20Lei%20Zhang%0AAbstract%3A%20We%20propose%20a%20decoupled%203D%20scene%20generation%20framework%20called%20SceneMaker%20in%20this%20work.%20Due%20to%20the%20lack%20of%20sufficient%20open-set%20de-occlusion%20and%20pose%20estimation%20priors%2C%20existing%20methods%20struggle%20to%20simultaneously%20produce%20high-quality%20geometry%20and%20accurate%20poses%20under%20severe%20occlusion%20and%20open-set%20settings.%20To%20address%20these%20issues%2C%20we%20first%20decouple%20the%20de-occlusion%20model%20from%203D%20object%20generation%2C%20and%20enhance%20it%20by%20leveraging%20image%20datasets%20and%20collected%20de-occlusion%20datasets%20for%20much%20more%20diverse%20open-set%20occlusion%20patterns.%20Then%2C%20we%20propose%20a%20unified%20pose%20estimation%20model%20that%20integrates%20global%20and%20local%20mechanisms%20for%20both%20self-attention%20and%20cross-attention%20to%20improve%20accuracy.%20Besides%2C%20we%20construct%20an%20open-set%203D%20scene%20dataset%20to%20further%20extend%20the%20generalization%20of%20the%20pose%20estimation%20model.%20Comprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20decoupled%20framework%20on%20both%20indoor%20and%20open-set%20scenes.%20Our%20codes%20and%20datasets%20is%20released%20at%20https%3A//idea-research.github.io/SceneMaker/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneMaker%253A%2520Open-set%25203D%2520Scene%2520Generation%2520with%2520Decoupled%2520De-occlusion%2520and%2520Pose%2520Estimation%2520Model%26entry.906535625%3DYukai%2520Shi%2520and%2520Weiyu%2520Li%2520and%2520Zihao%2520Wang%2520and%2520Hongyang%2520Li%2520and%2520Xingyu%2520Chen%2520and%2520Ping%2520Tan%2520and%2520Lei%2520Zhang%26entry.1292438233%3DWe%2520propose%2520a%2520decoupled%25203D%2520scene%2520generation%2520framework%2520called%2520SceneMaker%2520in%2520this%2520work.%2520Due%2520to%2520the%2520lack%2520of%2520sufficient%2520open-set%2520de-occlusion%2520and%2520pose%2520estimation%2520priors%252C%2520existing%2520methods%2520struggle%2520to%2520simultaneously%2520produce%2520high-quality%2520geometry%2520and%2520accurate%2520poses%2520under%2520severe%2520occlusion%2520and%2520open-set%2520settings.%2520To%2520address%2520these%2520issues%252C%2520we%2520first%2520decouple%2520the%2520de-occlusion%2520model%2520from%25203D%2520object%2520generation%252C%2520and%2520enhance%2520it%2520by%2520leveraging%2520image%2520datasets%2520and%2520collected%2520de-occlusion%2520datasets%2520for%2520much%2520more%2520diverse%2520open-set%2520occlusion%2520patterns.%2520Then%252C%2520we%2520propose%2520a%2520unified%2520pose%2520estimation%2520model%2520that%2520integrates%2520global%2520and%2520local%2520mechanisms%2520for%2520both%2520self-attention%2520and%2520cross-attention%2520to%2520improve%2520accuracy.%2520Besides%252C%2520we%2520construct%2520an%2520open-set%25203D%2520scene%2520dataset%2520to%2520further%2520extend%2520the%2520generalization%2520of%2520the%2520pose%2520estimation%2520model.%2520Comprehensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520decoupled%2520framework%2520on%2520both%2520indoor%2520and%2520open-set%2520scenes.%2520Our%2520codes%2520and%2520datasets%2520is%2520released%2520at%2520https%253A//idea-research.github.io/SceneMaker/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneMaker%3A%20Open-set%203D%20Scene%20Generation%20with%20Decoupled%20De-occlusion%20and%20Pose%20Estimation%20Model&entry.906535625=Yukai%20Shi%20and%20Weiyu%20Li%20and%20Zihao%20Wang%20and%20Hongyang%20Li%20and%20Xingyu%20Chen%20and%20Ping%20Tan%20and%20Lei%20Zhang&entry.1292438233=We%20propose%20a%20decoupled%203D%20scene%20generation%20framework%20called%20SceneMaker%20in%20this%20work.%20Due%20to%20the%20lack%20of%20sufficient%20open-set%20de-occlusion%20and%20pose%20estimation%20priors%2C%20existing%20methods%20struggle%20to%20simultaneously%20produce%20high-quality%20geometry%20and%20accurate%20poses%20under%20severe%20occlusion%20and%20open-set%20settings.%20To%20address%20these%20issues%2C%20we%20first%20decouple%20the%20de-occlusion%20model%20from%203D%20object%20generation%2C%20and%20enhance%20it%20by%20leveraging%20image%20datasets%20and%20collected%20de-occlusion%20datasets%20for%20much%20more%20diverse%20open-set%20occlusion%20patterns.%20Then%2C%20we%20propose%20a%20unified%20pose%20estimation%20model%20that%20integrates%20global%20and%20local%20mechanisms%20for%20both%20self-attention%20and%20cross-attention%20to%20improve%20accuracy.%20Besides%2C%20we%20construct%20an%20open-set%203D%20scene%20dataset%20to%20further%20extend%20the%20generalization%20of%20the%20pose%20estimation%20model.%20Comprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20decoupled%20framework%20on%20both%20indoor%20and%20open-set%20scenes.%20Our%20codes%20and%20datasets%20is%20released%20at%20https%3A//idea-research.github.io/SceneMaker/.&entry.1838667208=http%3A//arxiv.org/abs/2512.10957v1&entry.124074799=Read"},
{"title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos", "author": "Kehong Gong and Zhengyu Wen and Weixia He and Mingxi Xu and Qi Wang and Ning Zhang and Zhengyu Li and Dongze Lian and Wei Zhao and Xiaoyu He and Mingyuan Zhang", "abstract": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/", "link": "http://arxiv.org/abs/2512.10881v1", "date": "2025-12-11", "relevancy": 3.0049, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6258}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.611}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoCapAnything%3A%20Unified%203D%20Motion%20Capture%20for%20Arbitrary%20Skeletons%20from%20Monocular%20Videos&body=Title%3A%20MoCapAnything%3A%20Unified%203D%20Motion%20Capture%20for%20Arbitrary%20Skeletons%20from%20Monocular%20Videos%0AAuthor%3A%20Kehong%20Gong%20and%20Zhengyu%20Wen%20and%20Weixia%20He%20and%20Mingxi%20Xu%20and%20Qi%20Wang%20and%20Ning%20Zhang%20and%20Zhengyu%20Li%20and%20Dongze%20Lian%20and%20Wei%20Zhao%20and%20Xiaoyu%20He%20and%20Mingyuan%20Zhang%0AAbstract%3A%20Motion%20capture%20now%20underpins%20content%20creation%20far%20beyond%20digital%20humans%2C%20yet%20most%20existing%20pipelines%20remain%20species-%20or%20template-specific.%20We%20formalize%20this%20gap%20as%20Category-Agnostic%20Motion%20Capture%20%28CAMoCap%29%3A%20given%20a%20monocular%20video%20and%20an%20arbitrary%20rigged%203D%20asset%20as%20a%20prompt%2C%20the%20goal%20is%20to%20reconstruct%20a%20rotation-based%20animation%20such%20as%20BVH%20that%20directly%20drives%20the%20specific%20asset.%20We%20present%20MoCapAnything%2C%20a%20reference-guided%2C%20factorized%20framework%20that%20first%20predicts%203D%20joint%20trajectories%20and%20then%20recovers%20asset-specific%20rotations%20via%20constraint-aware%20inverse%20kinematics.%20The%20system%20contains%20three%20learnable%20modules%20and%20a%20lightweight%20IK%20stage%3A%20%281%29%20a%20Reference%20Prompt%20Encoder%20that%20extracts%20per-joint%20queries%20from%20the%20asset%27s%20skeleton%2C%20mesh%2C%20and%20rendered%20images%3B%20%282%29%20a%20Video%20Feature%20Extractor%20that%20computes%20dense%20visual%20descriptors%20and%20reconstructs%20a%20coarse%204D%20deforming%20mesh%20to%20bridge%20the%20gap%20between%20video%20and%20joint%20space%3B%20and%20%283%29%20a%20Unified%20Motion%20Decoder%20that%20fuses%20these%20cues%20to%20produce%20temporally%20coherent%20trajectories.%20We%20also%20curate%20Truebones%20Zoo%20with%201038%20motion%20clips%2C%20each%20providing%20a%20standardized%20skeleton-mesh-render%20triad.%20Experiments%20on%20both%20in-domain%20benchmarks%20and%20in-the-wild%20videos%20show%20that%20MoCapAnything%20delivers%20high-quality%20skeletal%20animations%20and%20exhibits%20meaningful%20cross-species%20retargeting%20across%20heterogeneous%20rigs%2C%20enabling%20scalable%2C%20prompt-driven%203D%20motion%20capture%20for%20arbitrary%20assets.%20Project%20page%3A%20https%3A//animotionlab.github.io/MoCapAnything/%0ALink%3A%20http%3A//arxiv.org/abs/2512.10881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoCapAnything%253A%2520Unified%25203D%2520Motion%2520Capture%2520for%2520Arbitrary%2520Skeletons%2520from%2520Monocular%2520Videos%26entry.906535625%3DKehong%2520Gong%2520and%2520Zhengyu%2520Wen%2520and%2520Weixia%2520He%2520and%2520Mingxi%2520Xu%2520and%2520Qi%2520Wang%2520and%2520Ning%2520Zhang%2520and%2520Zhengyu%2520Li%2520and%2520Dongze%2520Lian%2520and%2520Wei%2520Zhao%2520and%2520Xiaoyu%2520He%2520and%2520Mingyuan%2520Zhang%26entry.1292438233%3DMotion%2520capture%2520now%2520underpins%2520content%2520creation%2520far%2520beyond%2520digital%2520humans%252C%2520yet%2520most%2520existing%2520pipelines%2520remain%2520species-%2520or%2520template-specific.%2520We%2520formalize%2520this%2520gap%2520as%2520Category-Agnostic%2520Motion%2520Capture%2520%2528CAMoCap%2529%253A%2520given%2520a%2520monocular%2520video%2520and%2520an%2520arbitrary%2520rigged%25203D%2520asset%2520as%2520a%2520prompt%252C%2520the%2520goal%2520is%2520to%2520reconstruct%2520a%2520rotation-based%2520animation%2520such%2520as%2520BVH%2520that%2520directly%2520drives%2520the%2520specific%2520asset.%2520We%2520present%2520MoCapAnything%252C%2520a%2520reference-guided%252C%2520factorized%2520framework%2520that%2520first%2520predicts%25203D%2520joint%2520trajectories%2520and%2520then%2520recovers%2520asset-specific%2520rotations%2520via%2520constraint-aware%2520inverse%2520kinematics.%2520The%2520system%2520contains%2520three%2520learnable%2520modules%2520and%2520a%2520lightweight%2520IK%2520stage%253A%2520%25281%2529%2520a%2520Reference%2520Prompt%2520Encoder%2520that%2520extracts%2520per-joint%2520queries%2520from%2520the%2520asset%2527s%2520skeleton%252C%2520mesh%252C%2520and%2520rendered%2520images%253B%2520%25282%2529%2520a%2520Video%2520Feature%2520Extractor%2520that%2520computes%2520dense%2520visual%2520descriptors%2520and%2520reconstructs%2520a%2520coarse%25204D%2520deforming%2520mesh%2520to%2520bridge%2520the%2520gap%2520between%2520video%2520and%2520joint%2520space%253B%2520and%2520%25283%2529%2520a%2520Unified%2520Motion%2520Decoder%2520that%2520fuses%2520these%2520cues%2520to%2520produce%2520temporally%2520coherent%2520trajectories.%2520We%2520also%2520curate%2520Truebones%2520Zoo%2520with%25201038%2520motion%2520clips%252C%2520each%2520providing%2520a%2520standardized%2520skeleton-mesh-render%2520triad.%2520Experiments%2520on%2520both%2520in-domain%2520benchmarks%2520and%2520in-the-wild%2520videos%2520show%2520that%2520MoCapAnything%2520delivers%2520high-quality%2520skeletal%2520animations%2520and%2520exhibits%2520meaningful%2520cross-species%2520retargeting%2520across%2520heterogeneous%2520rigs%252C%2520enabling%2520scalable%252C%2520prompt-driven%25203D%2520motion%2520capture%2520for%2520arbitrary%2520assets.%2520Project%2520page%253A%2520https%253A//animotionlab.github.io/MoCapAnything/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoCapAnything%3A%20Unified%203D%20Motion%20Capture%20for%20Arbitrary%20Skeletons%20from%20Monocular%20Videos&entry.906535625=Kehong%20Gong%20and%20Zhengyu%20Wen%20and%20Weixia%20He%20and%20Mingxi%20Xu%20and%20Qi%20Wang%20and%20Ning%20Zhang%20and%20Zhengyu%20Li%20and%20Dongze%20Lian%20and%20Wei%20Zhao%20and%20Xiaoyu%20He%20and%20Mingyuan%20Zhang&entry.1292438233=Motion%20capture%20now%20underpins%20content%20creation%20far%20beyond%20digital%20humans%2C%20yet%20most%20existing%20pipelines%20remain%20species-%20or%20template-specific.%20We%20formalize%20this%20gap%20as%20Category-Agnostic%20Motion%20Capture%20%28CAMoCap%29%3A%20given%20a%20monocular%20video%20and%20an%20arbitrary%20rigged%203D%20asset%20as%20a%20prompt%2C%20the%20goal%20is%20to%20reconstruct%20a%20rotation-based%20animation%20such%20as%20BVH%20that%20directly%20drives%20the%20specific%20asset.%20We%20present%20MoCapAnything%2C%20a%20reference-guided%2C%20factorized%20framework%20that%20first%20predicts%203D%20joint%20trajectories%20and%20then%20recovers%20asset-specific%20rotations%20via%20constraint-aware%20inverse%20kinematics.%20The%20system%20contains%20three%20learnable%20modules%20and%20a%20lightweight%20IK%20stage%3A%20%281%29%20a%20Reference%20Prompt%20Encoder%20that%20extracts%20per-joint%20queries%20from%20the%20asset%27s%20skeleton%2C%20mesh%2C%20and%20rendered%20images%3B%20%282%29%20a%20Video%20Feature%20Extractor%20that%20computes%20dense%20visual%20descriptors%20and%20reconstructs%20a%20coarse%204D%20deforming%20mesh%20to%20bridge%20the%20gap%20between%20video%20and%20joint%20space%3B%20and%20%283%29%20a%20Unified%20Motion%20Decoder%20that%20fuses%20these%20cues%20to%20produce%20temporally%20coherent%20trajectories.%20We%20also%20curate%20Truebones%20Zoo%20with%201038%20motion%20clips%2C%20each%20providing%20a%20standardized%20skeleton-mesh-render%20triad.%20Experiments%20on%20both%20in-domain%20benchmarks%20and%20in-the-wild%20videos%20show%20that%20MoCapAnything%20delivers%20high-quality%20skeletal%20animations%20and%20exhibits%20meaningful%20cross-species%20retargeting%20across%20heterogeneous%20rigs%2C%20enabling%20scalable%2C%20prompt-driven%203D%20motion%20capture%20for%20arbitrary%20assets.%20Project%20page%3A%20https%3A//animotionlab.github.io/MoCapAnything/&entry.1838667208=http%3A//arxiv.org/abs/2512.10881v1&entry.124074799=Read"},
{"title": "PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning", "author": "Jianqi Chen and Biao Zhang and Xiangjun Tang and Peter Wonka", "abstract": "6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .", "link": "http://arxiv.org/abs/2512.10840v1", "date": "2025-12-11", "relevancy": 2.9527, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5942}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5933}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseGAM%3A%20Robust%20Unseen%20Object%20Pose%20Estimation%20via%20Geometry-Aware%20Multi-View%20Reasoning&body=Title%3A%20PoseGAM%3A%20Robust%20Unseen%20Object%20Pose%20Estimation%20via%20Geometry-Aware%20Multi-View%20Reasoning%0AAuthor%3A%20Jianqi%20Chen%20and%20Biao%20Zhang%20and%20Xiangjun%20Tang%20and%20Peter%20Wonka%0AAbstract%3A%206D%20object%20pose%20estimation%2C%20which%20predicts%20the%20transformation%20of%20an%20object%20relative%20to%20the%20camera%2C%20remains%20challenging%20for%20unseen%20objects.%20Existing%20approaches%20typically%20rely%20on%20explicitly%20constructing%20feature%20correspondences%20between%20the%20query%20image%20and%20either%20the%20object%20model%20or%20template%20images.%20In%20this%20work%2C%20we%20propose%20PoseGAM%2C%20a%20geometry-aware%20multi-view%20framework%20that%20directly%20predicts%20object%20pose%20from%20a%20query%20image%20and%20multiple%20template%20images%2C%20eliminating%20the%20need%20for%20explicit%20matching.%20Built%20upon%20recent%20multi-view-based%20foundation%20model%20architectures%2C%20the%20method%20integrates%20object%20geometry%20information%20through%20two%20complementary%20mechanisms%3A%20explicit%20point-based%20geometry%20and%20learned%20features%20from%20geometry%20representation%20networks.%20In%20addition%2C%20we%20construct%20a%20large-scale%20synthetic%20dataset%20containing%20more%20than%20190k%20objects%20under%20diverse%20environmental%20conditions%20to%20enhance%20robustness%20and%20generalization.%20Extensive%20evaluations%20across%20multiple%20benchmarks%20demonstrate%20our%20state-of-the-art%20performance%2C%20yielding%20an%20average%20AR%20improvement%20of%205.1%25%20over%20prior%20methods%20and%20achieving%20up%20to%2017.6%25%20gains%20on%20individual%20datasets%2C%20indicating%20strong%20generalization%20to%20unseen%20objects.%20Project%20page%3A%20https%3A//windvchen.github.io/PoseGAM/%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseGAM%253A%2520Robust%2520Unseen%2520Object%2520Pose%2520Estimation%2520via%2520Geometry-Aware%2520Multi-View%2520Reasoning%26entry.906535625%3DJianqi%2520Chen%2520and%2520Biao%2520Zhang%2520and%2520Xiangjun%2520Tang%2520and%2520Peter%2520Wonka%26entry.1292438233%3D6D%2520object%2520pose%2520estimation%252C%2520which%2520predicts%2520the%2520transformation%2520of%2520an%2520object%2520relative%2520to%2520the%2520camera%252C%2520remains%2520challenging%2520for%2520unseen%2520objects.%2520Existing%2520approaches%2520typically%2520rely%2520on%2520explicitly%2520constructing%2520feature%2520correspondences%2520between%2520the%2520query%2520image%2520and%2520either%2520the%2520object%2520model%2520or%2520template%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520PoseGAM%252C%2520a%2520geometry-aware%2520multi-view%2520framework%2520that%2520directly%2520predicts%2520object%2520pose%2520from%2520a%2520query%2520image%2520and%2520multiple%2520template%2520images%252C%2520eliminating%2520the%2520need%2520for%2520explicit%2520matching.%2520Built%2520upon%2520recent%2520multi-view-based%2520foundation%2520model%2520architectures%252C%2520the%2520method%2520integrates%2520object%2520geometry%2520information%2520through%2520two%2520complementary%2520mechanisms%253A%2520explicit%2520point-based%2520geometry%2520and%2520learned%2520features%2520from%2520geometry%2520representation%2520networks.%2520In%2520addition%252C%2520we%2520construct%2520a%2520large-scale%2520synthetic%2520dataset%2520containing%2520more%2520than%2520190k%2520objects%2520under%2520diverse%2520environmental%2520conditions%2520to%2520enhance%2520robustness%2520and%2520generalization.%2520Extensive%2520evaluations%2520across%2520multiple%2520benchmarks%2520demonstrate%2520our%2520state-of-the-art%2520performance%252C%2520yielding%2520an%2520average%2520AR%2520improvement%2520of%25205.1%2525%2520over%2520prior%2520methods%2520and%2520achieving%2520up%2520to%252017.6%2525%2520gains%2520on%2520individual%2520datasets%252C%2520indicating%2520strong%2520generalization%2520to%2520unseen%2520objects.%2520Project%2520page%253A%2520https%253A//windvchen.github.io/PoseGAM/%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseGAM%3A%20Robust%20Unseen%20Object%20Pose%20Estimation%20via%20Geometry-Aware%20Multi-View%20Reasoning&entry.906535625=Jianqi%20Chen%20and%20Biao%20Zhang%20and%20Xiangjun%20Tang%20and%20Peter%20Wonka&entry.1292438233=6D%20object%20pose%20estimation%2C%20which%20predicts%20the%20transformation%20of%20an%20object%20relative%20to%20the%20camera%2C%20remains%20challenging%20for%20unseen%20objects.%20Existing%20approaches%20typically%20rely%20on%20explicitly%20constructing%20feature%20correspondences%20between%20the%20query%20image%20and%20either%20the%20object%20model%20or%20template%20images.%20In%20this%20work%2C%20we%20propose%20PoseGAM%2C%20a%20geometry-aware%20multi-view%20framework%20that%20directly%20predicts%20object%20pose%20from%20a%20query%20image%20and%20multiple%20template%20images%2C%20eliminating%20the%20need%20for%20explicit%20matching.%20Built%20upon%20recent%20multi-view-based%20foundation%20model%20architectures%2C%20the%20method%20integrates%20object%20geometry%20information%20through%20two%20complementary%20mechanisms%3A%20explicit%20point-based%20geometry%20and%20learned%20features%20from%20geometry%20representation%20networks.%20In%20addition%2C%20we%20construct%20a%20large-scale%20synthetic%20dataset%20containing%20more%20than%20190k%20objects%20under%20diverse%20environmental%20conditions%20to%20enhance%20robustness%20and%20generalization.%20Extensive%20evaluations%20across%20multiple%20benchmarks%20demonstrate%20our%20state-of-the-art%20performance%2C%20yielding%20an%20average%20AR%20improvement%20of%205.1%25%20over%20prior%20methods%20and%20achieving%20up%20to%2017.6%25%20gains%20on%20individual%20datasets%2C%20indicating%20strong%20generalization%20to%20unseen%20objects.%20Project%20page%3A%20https%3A//windvchen.github.io/PoseGAM/%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.10840v1&entry.124074799=Read"},
{"title": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching", "author": "Javier Villena Toro and Mehdi Tarkian", "abstract": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.", "link": "http://arxiv.org/abs/2512.10674v1", "date": "2025-12-11", "relevancy": 2.9176, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5995}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5844}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geo6DPose%3A%20Fast%20Zero-Shot%206D%20Object%20Pose%20Estimation%20via%20Geometry-Filtered%20Feature%20Matching&body=Title%3A%20Geo6DPose%3A%20Fast%20Zero-Shot%206D%20Object%20Pose%20Estimation%20via%20Geometry-Filtered%20Feature%20Matching%0AAuthor%3A%20Javier%20Villena%20Toro%20and%20Mehdi%20Tarkian%0AAbstract%3A%20Recent%20progress%20in%20zero-shot%206D%20object%20pose%20estimation%20has%20been%20driven%20largely%20by%20large-scale%20models%20and%20cloud-based%20inference.%20However%2C%20these%20approaches%20often%20introduce%20high%20latency%2C%20elevated%20energy%20consumption%2C%20and%20deployment%20risks%20related%20to%20connectivity%2C%20cost%2C%20and%20data%20governance%3B%20factors%20that%20conflict%20with%20the%20practical%20constraints%20of%20real-world%20robotics%2C%20where%20compute%20is%20limited%20and%20on-device%20inference%20is%20frequently%20required.%20We%20introduce%20Geo6DPose%2C%20a%20lightweight%2C%20fully%20local%2C%20and%20training-free%20pipeline%20for%20zero-shot%206D%20pose%20estimation%20that%20trades%20model%20scale%20for%20geometric%20reliability.%20Our%20method%20combines%20foundation%20model%20visual%20features%20with%20a%20geometric%20filtering%20strategy%3A%20Similarity%20maps%20are%20computed%20between%20onboarded%20template%20DINO%20descriptors%20and%20scene%20patches%2C%20and%20mutual%20correspondences%20are%20established%20by%20projecting%20scene%20patch%20centers%20to%203D%20and%20template%20descriptors%20to%20the%20object%20model%20coordinate%20system.%20Final%20poses%20are%20recovered%20via%20correspondence-driven%20RANSAC%20and%20ranked%20using%20a%20weighted%20geometric%20alignment%20metric%20that%20jointly%20accounts%20for%20reprojection%20consistency%20and%20spatial%20support%2C%20improving%20robustness%20to%20noise%2C%20clutter%2C%20and%20partial%20visibility.%20Geo6DPose%20achieves%20sub-second%20inference%20on%20a%20single%20commodity%20GPU%20while%20matching%20the%20average%20recall%20of%20significantly%20larger%20zero-shot%20baselines%20%2853.7%20AR%2C%201.08%20FPS%29.%20It%20requires%20no%20training%2C%20fine-tuning%2C%20or%20network%20access%2C%20and%20remains%20compatible%20with%20evolving%20foundation%20backbones%2C%20advancing%20practical%2C%20fully%20local%206D%20perception%20for%20robotic%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeo6DPose%253A%2520Fast%2520Zero-Shot%25206D%2520Object%2520Pose%2520Estimation%2520via%2520Geometry-Filtered%2520Feature%2520Matching%26entry.906535625%3DJavier%2520Villena%2520Toro%2520and%2520Mehdi%2520Tarkian%26entry.1292438233%3DRecent%2520progress%2520in%2520zero-shot%25206D%2520object%2520pose%2520estimation%2520has%2520been%2520driven%2520largely%2520by%2520large-scale%2520models%2520and%2520cloud-based%2520inference.%2520However%252C%2520these%2520approaches%2520often%2520introduce%2520high%2520latency%252C%2520elevated%2520energy%2520consumption%252C%2520and%2520deployment%2520risks%2520related%2520to%2520connectivity%252C%2520cost%252C%2520and%2520data%2520governance%253B%2520factors%2520that%2520conflict%2520with%2520the%2520practical%2520constraints%2520of%2520real-world%2520robotics%252C%2520where%2520compute%2520is%2520limited%2520and%2520on-device%2520inference%2520is%2520frequently%2520required.%2520We%2520introduce%2520Geo6DPose%252C%2520a%2520lightweight%252C%2520fully%2520local%252C%2520and%2520training-free%2520pipeline%2520for%2520zero-shot%25206D%2520pose%2520estimation%2520that%2520trades%2520model%2520scale%2520for%2520geometric%2520reliability.%2520Our%2520method%2520combines%2520foundation%2520model%2520visual%2520features%2520with%2520a%2520geometric%2520filtering%2520strategy%253A%2520Similarity%2520maps%2520are%2520computed%2520between%2520onboarded%2520template%2520DINO%2520descriptors%2520and%2520scene%2520patches%252C%2520and%2520mutual%2520correspondences%2520are%2520established%2520by%2520projecting%2520scene%2520patch%2520centers%2520to%25203D%2520and%2520template%2520descriptors%2520to%2520the%2520object%2520model%2520coordinate%2520system.%2520Final%2520poses%2520are%2520recovered%2520via%2520correspondence-driven%2520RANSAC%2520and%2520ranked%2520using%2520a%2520weighted%2520geometric%2520alignment%2520metric%2520that%2520jointly%2520accounts%2520for%2520reprojection%2520consistency%2520and%2520spatial%2520support%252C%2520improving%2520robustness%2520to%2520noise%252C%2520clutter%252C%2520and%2520partial%2520visibility.%2520Geo6DPose%2520achieves%2520sub-second%2520inference%2520on%2520a%2520single%2520commodity%2520GPU%2520while%2520matching%2520the%2520average%2520recall%2520of%2520significantly%2520larger%2520zero-shot%2520baselines%2520%252853.7%2520AR%252C%25201.08%2520FPS%2529.%2520It%2520requires%2520no%2520training%252C%2520fine-tuning%252C%2520or%2520network%2520access%252C%2520and%2520remains%2520compatible%2520with%2520evolving%2520foundation%2520backbones%252C%2520advancing%2520practical%252C%2520fully%2520local%25206D%2520perception%2520for%2520robotic%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geo6DPose%3A%20Fast%20Zero-Shot%206D%20Object%20Pose%20Estimation%20via%20Geometry-Filtered%20Feature%20Matching&entry.906535625=Javier%20Villena%20Toro%20and%20Mehdi%20Tarkian&entry.1292438233=Recent%20progress%20in%20zero-shot%206D%20object%20pose%20estimation%20has%20been%20driven%20largely%20by%20large-scale%20models%20and%20cloud-based%20inference.%20However%2C%20these%20approaches%20often%20introduce%20high%20latency%2C%20elevated%20energy%20consumption%2C%20and%20deployment%20risks%20related%20to%20connectivity%2C%20cost%2C%20and%20data%20governance%3B%20factors%20that%20conflict%20with%20the%20practical%20constraints%20of%20real-world%20robotics%2C%20where%20compute%20is%20limited%20and%20on-device%20inference%20is%20frequently%20required.%20We%20introduce%20Geo6DPose%2C%20a%20lightweight%2C%20fully%20local%2C%20and%20training-free%20pipeline%20for%20zero-shot%206D%20pose%20estimation%20that%20trades%20model%20scale%20for%20geometric%20reliability.%20Our%20method%20combines%20foundation%20model%20visual%20features%20with%20a%20geometric%20filtering%20strategy%3A%20Similarity%20maps%20are%20computed%20between%20onboarded%20template%20DINO%20descriptors%20and%20scene%20patches%2C%20and%20mutual%20correspondences%20are%20established%20by%20projecting%20scene%20patch%20centers%20to%203D%20and%20template%20descriptors%20to%20the%20object%20model%20coordinate%20system.%20Final%20poses%20are%20recovered%20via%20correspondence-driven%20RANSAC%20and%20ranked%20using%20a%20weighted%20geometric%20alignment%20metric%20that%20jointly%20accounts%20for%20reprojection%20consistency%20and%20spatial%20support%2C%20improving%20robustness%20to%20noise%2C%20clutter%2C%20and%20partial%20visibility.%20Geo6DPose%20achieves%20sub-second%20inference%20on%20a%20single%20commodity%20GPU%20while%20matching%20the%20average%20recall%20of%20significantly%20larger%20zero-shot%20baselines%20%2853.7%20AR%2C%201.08%20FPS%29.%20It%20requires%20no%20training%2C%20fine-tuning%2C%20or%20network%20access%2C%20and%20remains%20compatible%20with%20evolving%20foundation%20backbones%2C%20advancing%20practical%2C%20fully%20local%206D%20perception%20for%20robotic%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2512.10674v1&entry.124074799=Read"},
{"title": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models", "author": "Shengao Wang and Wenqi Wang and Zecheng Wang and Max Whitton and Michael Wakeham and Arjun Chandra and Joey Huang and Pengyue Zhu and Helen Chen and David Li and Jeffrey Li and Shawn Li and Andrew Zagula and Amy Zhao and Andrew Zhu and Sayaka Nakamura and Yuki Yamamoto and Jerry Jun Yokono and Aaron Mueller and Bryan A. Plummer and Kate Saenko and Venkatesh Saligrama and Boqing Gong", "abstract": "Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.", "link": "http://arxiv.org/abs/2512.10932v1", "date": "2025-12-11", "relevancy": 2.9063, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BabyVLM-V2%3A%20Toward%20Developmentally%20Grounded%20Pretraining%20and%20Benchmarking%20of%20Vision%20Foundation%20Models&body=Title%3A%20BabyVLM-V2%3A%20Toward%20Developmentally%20Grounded%20Pretraining%20and%20Benchmarking%20of%20Vision%20Foundation%20Models%0AAuthor%3A%20Shengao%20Wang%20and%20Wenqi%20Wang%20and%20Zecheng%20Wang%20and%20Max%20Whitton%20and%20Michael%20Wakeham%20and%20Arjun%20Chandra%20and%20Joey%20Huang%20and%20Pengyue%20Zhu%20and%20Helen%20Chen%20and%20David%20Li%20and%20Jeffrey%20Li%20and%20Shawn%20Li%20and%20Andrew%20Zagula%20and%20Amy%20Zhao%20and%20Andrew%20Zhu%20and%20Sayaka%20Nakamura%20and%20Yuki%20Yamamoto%20and%20Jerry%20Jun%20Yokono%20and%20Aaron%20Mueller%20and%20Bryan%20A.%20Plummer%20and%20Kate%20Saenko%20and%20Venkatesh%20Saligrama%20and%20Boqing%20Gong%0AAbstract%3A%20Early%20children%27s%20developmental%20trajectories%20set%20up%20a%20natural%20goal%20for%20sample-efficient%20pretraining%20of%20vision%20foundation%20models.%20We%20introduce%20BabyVLM-V2%2C%20a%20developmentally%20grounded%20framework%20for%20infant-inspired%20vision-language%20modeling%20that%20extensively%20improves%20upon%20BabyVLM-V1%20through%20a%20longitudinal%2C%20multifaceted%20pretraining%20set%2C%20a%20versatile%20model%2C%20and%2C%20most%20importantly%2C%20DevCV%20Toolbox%20for%20cognitive%20evaluation.%20The%20pretraining%20set%20maximizes%20coverage%20while%20minimizing%20curation%20of%20a%20longitudinal%2C%20infant-centric%20audiovisual%20corpus%2C%20yielding%20video-utterance%2C%20image-utterance%2C%20and%20multi-turn%20conversational%20data%20that%20mirror%20infant%20experiences.%20DevCV%20Toolbox%20adapts%20all%20vision-related%20measures%20of%20the%20recently%20released%20NIH%20Baby%20Toolbox%20into%20a%20benchmark%20suite%20of%20ten%20multimodal%20tasks%2C%20covering%20spatial%20reasoning%2C%20memory%2C%20and%20vocabulary%20understanding%20aligned%20with%20early%20children%27s%20capabilities.%20Experimental%20results%20show%20that%20a%20compact%20model%20pretrained%20from%20scratch%20can%20achieve%20competitive%20performance%20on%20DevCV%20Toolbox%2C%20outperforming%20GPT-4o%20on%20some%20tasks.%20We%20hope%20the%20principled%2C%20unified%20BabyVLM-V2%20framework%20will%20accelerate%20research%20in%20developmentally%20plausible%20pretraining%20of%20vision%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBabyVLM-V2%253A%2520Toward%2520Developmentally%2520Grounded%2520Pretraining%2520and%2520Benchmarking%2520of%2520Vision%2520Foundation%2520Models%26entry.906535625%3DShengao%2520Wang%2520and%2520Wenqi%2520Wang%2520and%2520Zecheng%2520Wang%2520and%2520Max%2520Whitton%2520and%2520Michael%2520Wakeham%2520and%2520Arjun%2520Chandra%2520and%2520Joey%2520Huang%2520and%2520Pengyue%2520Zhu%2520and%2520Helen%2520Chen%2520and%2520David%2520Li%2520and%2520Jeffrey%2520Li%2520and%2520Shawn%2520Li%2520and%2520Andrew%2520Zagula%2520and%2520Amy%2520Zhao%2520and%2520Andrew%2520Zhu%2520and%2520Sayaka%2520Nakamura%2520and%2520Yuki%2520Yamamoto%2520and%2520Jerry%2520Jun%2520Yokono%2520and%2520Aaron%2520Mueller%2520and%2520Bryan%2520A.%2520Plummer%2520and%2520Kate%2520Saenko%2520and%2520Venkatesh%2520Saligrama%2520and%2520Boqing%2520Gong%26entry.1292438233%3DEarly%2520children%2527s%2520developmental%2520trajectories%2520set%2520up%2520a%2520natural%2520goal%2520for%2520sample-efficient%2520pretraining%2520of%2520vision%2520foundation%2520models.%2520We%2520introduce%2520BabyVLM-V2%252C%2520a%2520developmentally%2520grounded%2520framework%2520for%2520infant-inspired%2520vision-language%2520modeling%2520that%2520extensively%2520improves%2520upon%2520BabyVLM-V1%2520through%2520a%2520longitudinal%252C%2520multifaceted%2520pretraining%2520set%252C%2520a%2520versatile%2520model%252C%2520and%252C%2520most%2520importantly%252C%2520DevCV%2520Toolbox%2520for%2520cognitive%2520evaluation.%2520The%2520pretraining%2520set%2520maximizes%2520coverage%2520while%2520minimizing%2520curation%2520of%2520a%2520longitudinal%252C%2520infant-centric%2520audiovisual%2520corpus%252C%2520yielding%2520video-utterance%252C%2520image-utterance%252C%2520and%2520multi-turn%2520conversational%2520data%2520that%2520mirror%2520infant%2520experiences.%2520DevCV%2520Toolbox%2520adapts%2520all%2520vision-related%2520measures%2520of%2520the%2520recently%2520released%2520NIH%2520Baby%2520Toolbox%2520into%2520a%2520benchmark%2520suite%2520of%2520ten%2520multimodal%2520tasks%252C%2520covering%2520spatial%2520reasoning%252C%2520memory%252C%2520and%2520vocabulary%2520understanding%2520aligned%2520with%2520early%2520children%2527s%2520capabilities.%2520Experimental%2520results%2520show%2520that%2520a%2520compact%2520model%2520pretrained%2520from%2520scratch%2520can%2520achieve%2520competitive%2520performance%2520on%2520DevCV%2520Toolbox%252C%2520outperforming%2520GPT-4o%2520on%2520some%2520tasks.%2520We%2520hope%2520the%2520principled%252C%2520unified%2520BabyVLM-V2%2520framework%2520will%2520accelerate%2520research%2520in%2520developmentally%2520plausible%2520pretraining%2520of%2520vision%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BabyVLM-V2%3A%20Toward%20Developmentally%20Grounded%20Pretraining%20and%20Benchmarking%20of%20Vision%20Foundation%20Models&entry.906535625=Shengao%20Wang%20and%20Wenqi%20Wang%20and%20Zecheng%20Wang%20and%20Max%20Whitton%20and%20Michael%20Wakeham%20and%20Arjun%20Chandra%20and%20Joey%20Huang%20and%20Pengyue%20Zhu%20and%20Helen%20Chen%20and%20David%20Li%20and%20Jeffrey%20Li%20and%20Shawn%20Li%20and%20Andrew%20Zagula%20and%20Amy%20Zhao%20and%20Andrew%20Zhu%20and%20Sayaka%20Nakamura%20and%20Yuki%20Yamamoto%20and%20Jerry%20Jun%20Yokono%20and%20Aaron%20Mueller%20and%20Bryan%20A.%20Plummer%20and%20Kate%20Saenko%20and%20Venkatesh%20Saligrama%20and%20Boqing%20Gong&entry.1292438233=Early%20children%27s%20developmental%20trajectories%20set%20up%20a%20natural%20goal%20for%20sample-efficient%20pretraining%20of%20vision%20foundation%20models.%20We%20introduce%20BabyVLM-V2%2C%20a%20developmentally%20grounded%20framework%20for%20infant-inspired%20vision-language%20modeling%20that%20extensively%20improves%20upon%20BabyVLM-V1%20through%20a%20longitudinal%2C%20multifaceted%20pretraining%20set%2C%20a%20versatile%20model%2C%20and%2C%20most%20importantly%2C%20DevCV%20Toolbox%20for%20cognitive%20evaluation.%20The%20pretraining%20set%20maximizes%20coverage%20while%20minimizing%20curation%20of%20a%20longitudinal%2C%20infant-centric%20audiovisual%20corpus%2C%20yielding%20video-utterance%2C%20image-utterance%2C%20and%20multi-turn%20conversational%20data%20that%20mirror%20infant%20experiences.%20DevCV%20Toolbox%20adapts%20all%20vision-related%20measures%20of%20the%20recently%20released%20NIH%20Baby%20Toolbox%20into%20a%20benchmark%20suite%20of%20ten%20multimodal%20tasks%2C%20covering%20spatial%20reasoning%2C%20memory%2C%20and%20vocabulary%20understanding%20aligned%20with%20early%20children%27s%20capabilities.%20Experimental%20results%20show%20that%20a%20compact%20model%20pretrained%20from%20scratch%20can%20achieve%20competitive%20performance%20on%20DevCV%20Toolbox%2C%20outperforming%20GPT-4o%20on%20some%20tasks.%20We%20hope%20the%20principled%2C%20unified%20BabyVLM-V2%20framework%20will%20accelerate%20research%20in%20developmentally%20plausible%20pretraining%20of%20vision%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.10932v1&entry.124074799=Read"},
{"title": "Panoramic Out-of-Distribution Segmentation", "author": "Mengfei Duan and Yuheng Zhang and Yihong Cao and Fei Teng and Kai Luo and Jiaming Zhang and Kailun Yang and Zhiyong Li", "abstract": "Panoramic imaging enables capturing 360\u00b0 images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception, which is critical to applications, such as autonomous driving and augmented reality, etc. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to pixel distortions and background clutter. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), with the aim of achieving comprehensive and safe scene understanding. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities and advances the development of panoramic understanding. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.", "link": "http://arxiv.org/abs/2505.03539v3", "date": "2025-12-11", "relevancy": 2.8697, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panoramic%20Out-of-Distribution%20Segmentation&body=Title%3A%20Panoramic%20Out-of-Distribution%20Segmentation%0AAuthor%3A%20Mengfei%20Duan%20and%20Yuheng%20Zhang%20and%20Yihong%20Cao%20and%20Fei%20Teng%20and%20Kai%20Luo%20and%20Jiaming%20Zhang%20and%20Kailun%20Yang%20and%20Zhiyong%20Li%0AAbstract%3A%20Panoramic%20imaging%20enables%20capturing%20360%C2%B0%20images%20with%20an%20ultra-wide%20Field-of-View%20%28FoV%29%20for%20dense%20omnidirectional%20perception%2C%20which%20is%20critical%20to%20applications%2C%20such%20as%20autonomous%20driving%20and%20augmented%20reality%2C%20etc.%20However%2C%20current%20panoramic%20semantic%20segmentation%20methods%20fail%20to%20identify%20outliers%2C%20and%20pinhole%20Out-of-distribution%20Segmentation%20%28OoS%29%20models%20perform%20unsatisfactorily%20in%20the%20panoramic%20domain%20due%20to%20pixel%20distortions%20and%20background%20clutter.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20new%20task%2C%20Panoramic%20Out-of-distribution%20Segmentation%20%28PanOoS%29%2C%20with%20the%20aim%20of%20achieving%20comprehensive%20and%20safe%20scene%20understanding.%20Furthermore%2C%20we%20propose%20the%20first%20solution%2C%20POS%2C%20which%20adapts%20to%20the%20characteristics%20of%20panoramic%20images%20through%20text-guided%20prompt%20distribution%20learning.%20Specifically%2C%20POS%20integrates%20a%20disentanglement%20strategy%20designed%20to%20materialize%20the%20cross-domain%20generalization%20capability%20of%20CLIP.%20The%20proposed%20Prompt-based%20Restoration%20Attention%20%28PRA%29%20optimizes%20semantic%20decoding%20by%20prompt%20guidance%20and%20self-adaptive%20correction%2C%20while%20Bilevel%20Prompt%20Distribution%20Learning%20%28BPDL%29%20refines%20the%20manifold%20of%20per-pixel%20mask%20embeddings%20via%20semantic%20prototype%20supervision.%20Besides%2C%20to%20compensate%20for%20the%20scarcity%20of%20PanOoS%20datasets%2C%20we%20establish%20two%20benchmarks%3A%20DenseOoS%2C%20which%20features%20diverse%20outliers%20in%20complex%20environments%2C%20and%20QuadOoS%2C%20captured%20by%20a%20quadruped%20robot%20with%20a%20panoramic%20annular%20lens%20system.%20Extensive%20experiments%20demonstrate%20superior%20performance%20of%20POS%2C%20with%20AuPRC%20improving%20by%2034.25%25%20and%20FPR95%20decreasing%20by%2021.42%25%20on%20DenseOoS%2C%20outperforming%20state-of-the-art%20pinhole-OoS%20methods.%20Moreover%2C%20POS%20achieves%20leading%20closed-set%20segmentation%20capabilities%20and%20advances%20the%20development%20of%20panoramic%20understanding.%20Code%20and%20datasets%20will%20be%20available%20at%20https%3A//github.com/MengfeiD/PanOoS.%0ALink%3A%20http%3A//arxiv.org/abs/2505.03539v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoramic%2520Out-of-Distribution%2520Segmentation%26entry.906535625%3DMengfei%2520Duan%2520and%2520Yuheng%2520Zhang%2520and%2520Yihong%2520Cao%2520and%2520Fei%2520Teng%2520and%2520Kai%2520Luo%2520and%2520Jiaming%2520Zhang%2520and%2520Kailun%2520Yang%2520and%2520Zhiyong%2520Li%26entry.1292438233%3DPanoramic%2520imaging%2520enables%2520capturing%2520360%25C2%25B0%2520images%2520with%2520an%2520ultra-wide%2520Field-of-View%2520%2528FoV%2529%2520for%2520dense%2520omnidirectional%2520perception%252C%2520which%2520is%2520critical%2520to%2520applications%252C%2520such%2520as%2520autonomous%2520driving%2520and%2520augmented%2520reality%252C%2520etc.%2520However%252C%2520current%2520panoramic%2520semantic%2520segmentation%2520methods%2520fail%2520to%2520identify%2520outliers%252C%2520and%2520pinhole%2520Out-of-distribution%2520Segmentation%2520%2528OoS%2529%2520models%2520perform%2520unsatisfactorily%2520in%2520the%2520panoramic%2520domain%2520due%2520to%2520pixel%2520distortions%2520and%2520background%2520clutter.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520new%2520task%252C%2520Panoramic%2520Out-of-distribution%2520Segmentation%2520%2528PanOoS%2529%252C%2520with%2520the%2520aim%2520of%2520achieving%2520comprehensive%2520and%2520safe%2520scene%2520understanding.%2520Furthermore%252C%2520we%2520propose%2520the%2520first%2520solution%252C%2520POS%252C%2520which%2520adapts%2520to%2520the%2520characteristics%2520of%2520panoramic%2520images%2520through%2520text-guided%2520prompt%2520distribution%2520learning.%2520Specifically%252C%2520POS%2520integrates%2520a%2520disentanglement%2520strategy%2520designed%2520to%2520materialize%2520the%2520cross-domain%2520generalization%2520capability%2520of%2520CLIP.%2520The%2520proposed%2520Prompt-based%2520Restoration%2520Attention%2520%2528PRA%2529%2520optimizes%2520semantic%2520decoding%2520by%2520prompt%2520guidance%2520and%2520self-adaptive%2520correction%252C%2520while%2520Bilevel%2520Prompt%2520Distribution%2520Learning%2520%2528BPDL%2529%2520refines%2520the%2520manifold%2520of%2520per-pixel%2520mask%2520embeddings%2520via%2520semantic%2520prototype%2520supervision.%2520Besides%252C%2520to%2520compensate%2520for%2520the%2520scarcity%2520of%2520PanOoS%2520datasets%252C%2520we%2520establish%2520two%2520benchmarks%253A%2520DenseOoS%252C%2520which%2520features%2520diverse%2520outliers%2520in%2520complex%2520environments%252C%2520and%2520QuadOoS%252C%2520captured%2520by%2520a%2520quadruped%2520robot%2520with%2520a%2520panoramic%2520annular%2520lens%2520system.%2520Extensive%2520experiments%2520demonstrate%2520superior%2520performance%2520of%2520POS%252C%2520with%2520AuPRC%2520improving%2520by%252034.25%2525%2520and%2520FPR95%2520decreasing%2520by%252021.42%2525%2520on%2520DenseOoS%252C%2520outperforming%2520state-of-the-art%2520pinhole-OoS%2520methods.%2520Moreover%252C%2520POS%2520achieves%2520leading%2520closed-set%2520segmentation%2520capabilities%2520and%2520advances%2520the%2520development%2520of%2520panoramic%2520understanding.%2520Code%2520and%2520datasets%2520will%2520be%2520available%2520at%2520https%253A//github.com/MengfeiD/PanOoS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03539v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panoramic%20Out-of-Distribution%20Segmentation&entry.906535625=Mengfei%20Duan%20and%20Yuheng%20Zhang%20and%20Yihong%20Cao%20and%20Fei%20Teng%20and%20Kai%20Luo%20and%20Jiaming%20Zhang%20and%20Kailun%20Yang%20and%20Zhiyong%20Li&entry.1292438233=Panoramic%20imaging%20enables%20capturing%20360%C2%B0%20images%20with%20an%20ultra-wide%20Field-of-View%20%28FoV%29%20for%20dense%20omnidirectional%20perception%2C%20which%20is%20critical%20to%20applications%2C%20such%20as%20autonomous%20driving%20and%20augmented%20reality%2C%20etc.%20However%2C%20current%20panoramic%20semantic%20segmentation%20methods%20fail%20to%20identify%20outliers%2C%20and%20pinhole%20Out-of-distribution%20Segmentation%20%28OoS%29%20models%20perform%20unsatisfactorily%20in%20the%20panoramic%20domain%20due%20to%20pixel%20distortions%20and%20background%20clutter.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20new%20task%2C%20Panoramic%20Out-of-distribution%20Segmentation%20%28PanOoS%29%2C%20with%20the%20aim%20of%20achieving%20comprehensive%20and%20safe%20scene%20understanding.%20Furthermore%2C%20we%20propose%20the%20first%20solution%2C%20POS%2C%20which%20adapts%20to%20the%20characteristics%20of%20panoramic%20images%20through%20text-guided%20prompt%20distribution%20learning.%20Specifically%2C%20POS%20integrates%20a%20disentanglement%20strategy%20designed%20to%20materialize%20the%20cross-domain%20generalization%20capability%20of%20CLIP.%20The%20proposed%20Prompt-based%20Restoration%20Attention%20%28PRA%29%20optimizes%20semantic%20decoding%20by%20prompt%20guidance%20and%20self-adaptive%20correction%2C%20while%20Bilevel%20Prompt%20Distribution%20Learning%20%28BPDL%29%20refines%20the%20manifold%20of%20per-pixel%20mask%20embeddings%20via%20semantic%20prototype%20supervision.%20Besides%2C%20to%20compensate%20for%20the%20scarcity%20of%20PanOoS%20datasets%2C%20we%20establish%20two%20benchmarks%3A%20DenseOoS%2C%20which%20features%20diverse%20outliers%20in%20complex%20environments%2C%20and%20QuadOoS%2C%20captured%20by%20a%20quadruped%20robot%20with%20a%20panoramic%20annular%20lens%20system.%20Extensive%20experiments%20demonstrate%20superior%20performance%20of%20POS%2C%20with%20AuPRC%20improving%20by%2034.25%25%20and%20FPR95%20decreasing%20by%2021.42%25%20on%20DenseOoS%2C%20outperforming%20state-of-the-art%20pinhole-OoS%20methods.%20Moreover%2C%20POS%20achieves%20leading%20closed-set%20segmentation%20capabilities%20and%20advances%20the%20development%20of%20panoramic%20understanding.%20Code%20and%20datasets%20will%20be%20available%20at%20https%3A//github.com/MengfeiD/PanOoS.&entry.1838667208=http%3A//arxiv.org/abs/2505.03539v3&entry.124074799=Read"},
{"title": "Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA", "author": "Pasquale De Marinis and Gennaro Vessio and Giovanna Castellano", "abstract": "Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder's limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \\textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder's generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.", "link": "http://arxiv.org/abs/2512.10521v1", "date": "2025-12-11", "relevancy": 2.8474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Take%20a%20Peek%3A%20Efficient%20Encoder%20Adaptation%20for%20Few-Shot%20Semantic%20Segmentation%20via%20LoRA&body=Title%3A%20Take%20a%20Peek%3A%20Efficient%20Encoder%20Adaptation%20for%20Few-Shot%20Semantic%20Segmentation%20via%20LoRA%0AAuthor%3A%20Pasquale%20De%20Marinis%20and%20Gennaro%20Vessio%20and%20Giovanna%20Castellano%0AAbstract%3A%20Few-shot%20semantic%20segmentation%20%28FSS%29%20aims%20to%20segment%20novel%20classes%20in%20query%20images%20using%20only%20a%20small%20annotated%20support%20set.%20While%20prior%20research%20has%20mainly%20focused%20on%20improving%20decoders%2C%20the%20encoder%27s%20limited%20ability%20to%20extract%20meaningful%20features%20for%20unseen%20classes%20remains%20a%20key%20bottleneck.%20In%20this%20work%2C%20we%20introduce%20%5Ctextit%7BTake%20a%20Peek%7D%20%28TaP%29%2C%20a%20simple%20yet%20effective%20method%20that%20enhances%20encoder%20adaptability%20for%20both%20FSS%20and%20cross-domain%20FSS%20%28CD-FSS%29.%20TaP%20leverages%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20fine-tune%20the%20encoder%20on%20the%20support%20set%20with%20minimal%20computational%20overhead%2C%20enabling%20fast%20adaptation%20to%20novel%20classes%20while%20mitigating%20catastrophic%20forgetting.%20Our%20method%20is%20model-agnostic%20and%20can%20be%20seamlessly%20integrated%20into%20existing%20FSS%20pipelines.%20Extensive%20experiments%20across%20multiple%20benchmarks--including%20COCO%20%2420%5Ei%24%2C%20Pascal%20%245%5Ei%24%2C%20and%20cross-domain%20datasets%20such%20as%20DeepGlobe%2C%20ISIC%2C%20and%20Chest%20X-ray--demonstrate%20that%20TaP%20consistently%20improves%20segmentation%20performance%20across%20diverse%20models%20and%20shot%20settings.%20Notably%2C%20TaP%20delivers%20significant%20gains%20in%20complex%20multi-class%20scenarios%2C%20highlighting%20its%20practical%20effectiveness%20in%20realistic%20settings.%20A%20rank%20sensitivity%20analysis%20also%20shows%20that%20strong%20performance%20can%20be%20achieved%20even%20with%20low-rank%20adaptations%2C%20ensuring%20computational%20efficiency.%20By%20addressing%20a%20critical%20limitation%20in%20FSS--the%20encoder%27s%20generalization%20to%20novel%20classes--TaP%20paves%20the%20way%20toward%20more%20robust%2C%20efficient%2C%20and%20generalizable%20segmentation%20systems.%20The%20code%20is%20available%20at%20https%3A//github.com/pasqualedem/TakeAPeek.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTake%2520a%2520Peek%253A%2520Efficient%2520Encoder%2520Adaptation%2520for%2520Few-Shot%2520Semantic%2520Segmentation%2520via%2520LoRA%26entry.906535625%3DPasquale%2520De%2520Marinis%2520and%2520Gennaro%2520Vessio%2520and%2520Giovanna%2520Castellano%26entry.1292438233%3DFew-shot%2520semantic%2520segmentation%2520%2528FSS%2529%2520aims%2520to%2520segment%2520novel%2520classes%2520in%2520query%2520images%2520using%2520only%2520a%2520small%2520annotated%2520support%2520set.%2520While%2520prior%2520research%2520has%2520mainly%2520focused%2520on%2520improving%2520decoders%252C%2520the%2520encoder%2527s%2520limited%2520ability%2520to%2520extract%2520meaningful%2520features%2520for%2520unseen%2520classes%2520remains%2520a%2520key%2520bottleneck.%2520In%2520this%2520work%252C%2520we%2520introduce%2520%255Ctextit%257BTake%2520a%2520Peek%257D%2520%2528TaP%2529%252C%2520a%2520simple%2520yet%2520effective%2520method%2520that%2520enhances%2520encoder%2520adaptability%2520for%2520both%2520FSS%2520and%2520cross-domain%2520FSS%2520%2528CD-FSS%2529.%2520TaP%2520leverages%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520fine-tune%2520the%2520encoder%2520on%2520the%2520support%2520set%2520with%2520minimal%2520computational%2520overhead%252C%2520enabling%2520fast%2520adaptation%2520to%2520novel%2520classes%2520while%2520mitigating%2520catastrophic%2520forgetting.%2520Our%2520method%2520is%2520model-agnostic%2520and%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520FSS%2520pipelines.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks--including%2520COCO%2520%252420%255Ei%2524%252C%2520Pascal%2520%25245%255Ei%2524%252C%2520and%2520cross-domain%2520datasets%2520such%2520as%2520DeepGlobe%252C%2520ISIC%252C%2520and%2520Chest%2520X-ray--demonstrate%2520that%2520TaP%2520consistently%2520improves%2520segmentation%2520performance%2520across%2520diverse%2520models%2520and%2520shot%2520settings.%2520Notably%252C%2520TaP%2520delivers%2520significant%2520gains%2520in%2520complex%2520multi-class%2520scenarios%252C%2520highlighting%2520its%2520practical%2520effectiveness%2520in%2520realistic%2520settings.%2520A%2520rank%2520sensitivity%2520analysis%2520also%2520shows%2520that%2520strong%2520performance%2520can%2520be%2520achieved%2520even%2520with%2520low-rank%2520adaptations%252C%2520ensuring%2520computational%2520efficiency.%2520By%2520addressing%2520a%2520critical%2520limitation%2520in%2520FSS--the%2520encoder%2527s%2520generalization%2520to%2520novel%2520classes--TaP%2520paves%2520the%2520way%2520toward%2520more%2520robust%252C%2520efficient%252C%2520and%2520generalizable%2520segmentation%2520systems.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/pasqualedem/TakeAPeek.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Take%20a%20Peek%3A%20Efficient%20Encoder%20Adaptation%20for%20Few-Shot%20Semantic%20Segmentation%20via%20LoRA&entry.906535625=Pasquale%20De%20Marinis%20and%20Gennaro%20Vessio%20and%20Giovanna%20Castellano&entry.1292438233=Few-shot%20semantic%20segmentation%20%28FSS%29%20aims%20to%20segment%20novel%20classes%20in%20query%20images%20using%20only%20a%20small%20annotated%20support%20set.%20While%20prior%20research%20has%20mainly%20focused%20on%20improving%20decoders%2C%20the%20encoder%27s%20limited%20ability%20to%20extract%20meaningful%20features%20for%20unseen%20classes%20remains%20a%20key%20bottleneck.%20In%20this%20work%2C%20we%20introduce%20%5Ctextit%7BTake%20a%20Peek%7D%20%28TaP%29%2C%20a%20simple%20yet%20effective%20method%20that%20enhances%20encoder%20adaptability%20for%20both%20FSS%20and%20cross-domain%20FSS%20%28CD-FSS%29.%20TaP%20leverages%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20fine-tune%20the%20encoder%20on%20the%20support%20set%20with%20minimal%20computational%20overhead%2C%20enabling%20fast%20adaptation%20to%20novel%20classes%20while%20mitigating%20catastrophic%20forgetting.%20Our%20method%20is%20model-agnostic%20and%20can%20be%20seamlessly%20integrated%20into%20existing%20FSS%20pipelines.%20Extensive%20experiments%20across%20multiple%20benchmarks--including%20COCO%20%2420%5Ei%24%2C%20Pascal%20%245%5Ei%24%2C%20and%20cross-domain%20datasets%20such%20as%20DeepGlobe%2C%20ISIC%2C%20and%20Chest%20X-ray--demonstrate%20that%20TaP%20consistently%20improves%20segmentation%20performance%20across%20diverse%20models%20and%20shot%20settings.%20Notably%2C%20TaP%20delivers%20significant%20gains%20in%20complex%20multi-class%20scenarios%2C%20highlighting%20its%20practical%20effectiveness%20in%20realistic%20settings.%20A%20rank%20sensitivity%20analysis%20also%20shows%20that%20strong%20performance%20can%20be%20achieved%20even%20with%20low-rank%20adaptations%2C%20ensuring%20computational%20efficiency.%20By%20addressing%20a%20critical%20limitation%20in%20FSS--the%20encoder%27s%20generalization%20to%20novel%20classes--TaP%20paves%20the%20way%20toward%20more%20robust%2C%20efficient%2C%20and%20generalizable%20segmentation%20systems.%20The%20code%20is%20available%20at%20https%3A//github.com/pasqualedem/TakeAPeek.&entry.1838667208=http%3A//arxiv.org/abs/2512.10521v1&entry.124074799=Read"},
{"title": "SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence", "author": "Haoning Wu and Xiao Huang and Yaohui Chen and Ya Zhang and Yanfeng Wang and Weidi Xie", "abstract": "Existing evaluations of multimodal large language models (MLLMs) on spatial intelligence are typically fragmented and limited in scope. In this work, we aim to conduct a holistic assessment of the spatial understanding capabilities of modern MLLMs and propose complementary data-driven and agent-based solutions. Specifically, we make the following contributions: (i) we introduce SpatialScore, to our knowledge, the most comprehensive and diverse benchmark for multimodal spatial intelligence to date. It covers multiple visual data types, input modalities, and question-answering formats, and contains approximately 5K manually verified samples spanning 30 distinct tasks; (ii) using SpatialScore, we extensively evaluate 40 representative MLLMs, revealing persistent challenges and a substantial gap between current models and human-level spatial intelligence; (iii) to advance model capabilities, we construct SpatialCorpus, a large-scale training resource with 331K multimodal QA samples that supports fine-tuning on spatial reasoning tasks and significantly improves the performance of existing models (e.g., Qwen3-VL); (iv) to complement this data-driven route with a training-free paradigm, we develop SpatialAgent, a multi-agent system equipped with 12 specialized spatial perception tools that supports both Plan-Execute and ReAct reasoning, enabling substantial gains in spatial reasoning without additional model training. Extensive experiments and in-depth analyses demonstrate the effectiveness of our benchmark, corpus, and agent framework. We expect these resources to serve as a solid foundation for advancing MLLMs toward human-level spatial intelligence. All data, code, and models will be released to the research community.", "link": "http://arxiv.org/abs/2505.17012v2", "date": "2025-12-11", "relevancy": 2.8245, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5735}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialScore%3A%20Towards%20Comprehensive%20Evaluation%20for%20Spatial%20Intelligence&body=Title%3A%20SpatialScore%3A%20Towards%20Comprehensive%20Evaluation%20for%20Spatial%20Intelligence%0AAuthor%3A%20Haoning%20Wu%20and%20Xiao%20Huang%20and%20Yaohui%20Chen%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%0AAbstract%3A%20Existing%20evaluations%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20on%20spatial%20intelligence%20are%20typically%20fragmented%20and%20limited%20in%20scope.%20In%20this%20work%2C%20we%20aim%20to%20conduct%20a%20holistic%20assessment%20of%20the%20spatial%20understanding%20capabilities%20of%20modern%20MLLMs%20and%20propose%20complementary%20data-driven%20and%20agent-based%20solutions.%20Specifically%2C%20we%20make%20the%20following%20contributions%3A%20%28i%29%20we%20introduce%20SpatialScore%2C%20to%20our%20knowledge%2C%20the%20most%20comprehensive%20and%20diverse%20benchmark%20for%20multimodal%20spatial%20intelligence%20to%20date.%20It%20covers%20multiple%20visual%20data%20types%2C%20input%20modalities%2C%20and%20question-answering%20formats%2C%20and%20contains%20approximately%205K%20manually%20verified%20samples%20spanning%2030%20distinct%20tasks%3B%20%28ii%29%20using%20SpatialScore%2C%20we%20extensively%20evaluate%2040%20representative%20MLLMs%2C%20revealing%20persistent%20challenges%20and%20a%20substantial%20gap%20between%20current%20models%20and%20human-level%20spatial%20intelligence%3B%20%28iii%29%20to%20advance%20model%20capabilities%2C%20we%20construct%20SpatialCorpus%2C%20a%20large-scale%20training%20resource%20with%20331K%20multimodal%20QA%20samples%20that%20supports%20fine-tuning%20on%20spatial%20reasoning%20tasks%20and%20significantly%20improves%20the%20performance%20of%20existing%20models%20%28e.g.%2C%20Qwen3-VL%29%3B%20%28iv%29%20to%20complement%20this%20data-driven%20route%20with%20a%20training-free%20paradigm%2C%20we%20develop%20SpatialAgent%2C%20a%20multi-agent%20system%20equipped%20with%2012%20specialized%20spatial%20perception%20tools%20that%20supports%20both%20Plan-Execute%20and%20ReAct%20reasoning%2C%20enabling%20substantial%20gains%20in%20spatial%20reasoning%20without%20additional%20model%20training.%20Extensive%20experiments%20and%20in-depth%20analyses%20demonstrate%20the%20effectiveness%20of%20our%20benchmark%2C%20corpus%2C%20and%20agent%20framework.%20We%20expect%20these%20resources%20to%20serve%20as%20a%20solid%20foundation%20for%20advancing%20MLLMs%20toward%20human-level%20spatial%20intelligence.%20All%20data%2C%20code%2C%20and%20models%20will%20be%20released%20to%20the%20research%20community.%0ALink%3A%20http%3A//arxiv.org/abs/2505.17012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialScore%253A%2520Towards%2520Comprehensive%2520Evaluation%2520for%2520Spatial%2520Intelligence%26entry.906535625%3DHaoning%2520Wu%2520and%2520Xiao%2520Huang%2520and%2520Yaohui%2520Chen%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%2520and%2520Weidi%2520Xie%26entry.1292438233%3DExisting%2520evaluations%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520on%2520spatial%2520intelligence%2520are%2520typically%2520fragmented%2520and%2520limited%2520in%2520scope.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520conduct%2520a%2520holistic%2520assessment%2520of%2520the%2520spatial%2520understanding%2520capabilities%2520of%2520modern%2520MLLMs%2520and%2520propose%2520complementary%2520data-driven%2520and%2520agent-based%2520solutions.%2520Specifically%252C%2520we%2520make%2520the%2520following%2520contributions%253A%2520%2528i%2529%2520we%2520introduce%2520SpatialScore%252C%2520to%2520our%2520knowledge%252C%2520the%2520most%2520comprehensive%2520and%2520diverse%2520benchmark%2520for%2520multimodal%2520spatial%2520intelligence%2520to%2520date.%2520It%2520covers%2520multiple%2520visual%2520data%2520types%252C%2520input%2520modalities%252C%2520and%2520question-answering%2520formats%252C%2520and%2520contains%2520approximately%25205K%2520manually%2520verified%2520samples%2520spanning%252030%2520distinct%2520tasks%253B%2520%2528ii%2529%2520using%2520SpatialScore%252C%2520we%2520extensively%2520evaluate%252040%2520representative%2520MLLMs%252C%2520revealing%2520persistent%2520challenges%2520and%2520a%2520substantial%2520gap%2520between%2520current%2520models%2520and%2520human-level%2520spatial%2520intelligence%253B%2520%2528iii%2529%2520to%2520advance%2520model%2520capabilities%252C%2520we%2520construct%2520SpatialCorpus%252C%2520a%2520large-scale%2520training%2520resource%2520with%2520331K%2520multimodal%2520QA%2520samples%2520that%2520supports%2520fine-tuning%2520on%2520spatial%2520reasoning%2520tasks%2520and%2520significantly%2520improves%2520the%2520performance%2520of%2520existing%2520models%2520%2528e.g.%252C%2520Qwen3-VL%2529%253B%2520%2528iv%2529%2520to%2520complement%2520this%2520data-driven%2520route%2520with%2520a%2520training-free%2520paradigm%252C%2520we%2520develop%2520SpatialAgent%252C%2520a%2520multi-agent%2520system%2520equipped%2520with%252012%2520specialized%2520spatial%2520perception%2520tools%2520that%2520supports%2520both%2520Plan-Execute%2520and%2520ReAct%2520reasoning%252C%2520enabling%2520substantial%2520gains%2520in%2520spatial%2520reasoning%2520without%2520additional%2520model%2520training.%2520Extensive%2520experiments%2520and%2520in-depth%2520analyses%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520benchmark%252C%2520corpus%252C%2520and%2520agent%2520framework.%2520We%2520expect%2520these%2520resources%2520to%2520serve%2520as%2520a%2520solid%2520foundation%2520for%2520advancing%2520MLLMs%2520toward%2520human-level%2520spatial%2520intelligence.%2520All%2520data%252C%2520code%252C%2520and%2520models%2520will%2520be%2520released%2520to%2520the%2520research%2520community.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialScore%3A%20Towards%20Comprehensive%20Evaluation%20for%20Spatial%20Intelligence&entry.906535625=Haoning%20Wu%20and%20Xiao%20Huang%20and%20Yaohui%20Chen%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie&entry.1292438233=Existing%20evaluations%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20on%20spatial%20intelligence%20are%20typically%20fragmented%20and%20limited%20in%20scope.%20In%20this%20work%2C%20we%20aim%20to%20conduct%20a%20holistic%20assessment%20of%20the%20spatial%20understanding%20capabilities%20of%20modern%20MLLMs%20and%20propose%20complementary%20data-driven%20and%20agent-based%20solutions.%20Specifically%2C%20we%20make%20the%20following%20contributions%3A%20%28i%29%20we%20introduce%20SpatialScore%2C%20to%20our%20knowledge%2C%20the%20most%20comprehensive%20and%20diverse%20benchmark%20for%20multimodal%20spatial%20intelligence%20to%20date.%20It%20covers%20multiple%20visual%20data%20types%2C%20input%20modalities%2C%20and%20question-answering%20formats%2C%20and%20contains%20approximately%205K%20manually%20verified%20samples%20spanning%2030%20distinct%20tasks%3B%20%28ii%29%20using%20SpatialScore%2C%20we%20extensively%20evaluate%2040%20representative%20MLLMs%2C%20revealing%20persistent%20challenges%20and%20a%20substantial%20gap%20between%20current%20models%20and%20human-level%20spatial%20intelligence%3B%20%28iii%29%20to%20advance%20model%20capabilities%2C%20we%20construct%20SpatialCorpus%2C%20a%20large-scale%20training%20resource%20with%20331K%20multimodal%20QA%20samples%20that%20supports%20fine-tuning%20on%20spatial%20reasoning%20tasks%20and%20significantly%20improves%20the%20performance%20of%20existing%20models%20%28e.g.%2C%20Qwen3-VL%29%3B%20%28iv%29%20to%20complement%20this%20data-driven%20route%20with%20a%20training-free%20paradigm%2C%20we%20develop%20SpatialAgent%2C%20a%20multi-agent%20system%20equipped%20with%2012%20specialized%20spatial%20perception%20tools%20that%20supports%20both%20Plan-Execute%20and%20ReAct%20reasoning%2C%20enabling%20substantial%20gains%20in%20spatial%20reasoning%20without%20additional%20model%20training.%20Extensive%20experiments%20and%20in-depth%20analyses%20demonstrate%20the%20effectiveness%20of%20our%20benchmark%2C%20corpus%2C%20and%20agent%20framework.%20We%20expect%20these%20resources%20to%20serve%20as%20a%20solid%20foundation%20for%20advancing%20MLLMs%20toward%20human-level%20spatial%20intelligence.%20All%20data%2C%20code%2C%20and%20models%20will%20be%20released%20to%20the%20research%20community.&entry.1838667208=http%3A//arxiv.org/abs/2505.17012v2&entry.124074799=Read"},
{"title": "Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding", "author": "Yuchen Feng and Zhenyu Zhang and Naibin Gu and Yilong Chen and Peng Fu and Zheng Lin and Shuohuan Wang and Yu Sun and Hua Wu and Weiping Wang and Haifeng Wang", "abstract": "Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential \"blink-like\" process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.", "link": "http://arxiv.org/abs/2512.10548v1", "date": "2025-12-11", "relevancy": 2.8149, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5651}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blink%3A%20Dynamic%20Visual%20Token%20Resolution%20for%20Enhanced%20Multimodal%20Understanding&body=Title%3A%20Blink%3A%20Dynamic%20Visual%20Token%20Resolution%20for%20Enhanced%20Multimodal%20Understanding%0AAuthor%3A%20Yuchen%20Feng%20and%20Zhenyu%20Zhang%20and%20Naibin%20Gu%20and%20Yilong%20Chen%20and%20Peng%20Fu%20and%20Zheng%20Lin%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Hua%20Wu%20and%20Weiping%20Wang%20and%20Haifeng%20Wang%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20on%20various%20vision-language%20tasks%2C%20yet%20their%20visual%20perception%20remains%20limited.%20Humans%2C%20in%20comparison%2C%20perceive%20complex%20scenes%20efficiently%20by%20dynamically%20scanning%20and%20focusing%20on%20salient%20regions%20in%20a%20sequential%20%22blink-like%22%20process.%20Motivated%20by%20this%20strategy%2C%20we%20first%20investigate%20whether%20MLLMs%20exhibit%20similar%20behavior.%20Our%20pilot%20analysis%20reveals%20that%20MLLMs%20naturally%20attend%20to%20different%20visual%20regions%20across%20layers%20and%20that%20selectively%20allocating%20more%20computation%20to%20salient%20tokens%20can%20enhance%20visual%20perception.%20Building%20on%20this%20insight%2C%20we%20propose%20Blink%2C%20a%20dynamic%20visual%20token%20resolution%20framework%20that%20emulates%20the%20human-inspired%20process%20within%20a%20single%20forward%20pass.%20Specifically%2C%20Blink%20includes%20two%20modules%3A%20saliency-guided%20scanning%20and%20dynamic%20token%20resolution.%20It%20first%20estimates%20the%20saliency%20of%20visual%20tokens%20in%20each%20layer%20based%20on%20the%20attention%20map%2C%20and%20extends%20important%20tokens%20through%20a%20plug-and-play%20token%20super-resolution%20%28TokenSR%29%20module.%20In%20the%20next%20layer%2C%20it%20drops%20the%20extended%20tokens%20when%20they%20lose%20focus.%20This%20dynamic%20mechanism%20balances%20broad%20exploration%20and%20fine-grained%20focus%2C%20thereby%20enhancing%20visual%20perception%20adaptively%20and%20efficiently.%20Extensive%20experiments%20validate%20Blink%2C%20demonstrating%20its%20effectiveness%20in%20enhancing%20visual%20perception%20and%20multimodal%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlink%253A%2520Dynamic%2520Visual%2520Token%2520Resolution%2520for%2520Enhanced%2520Multimodal%2520Understanding%26entry.906535625%3DYuchen%2520Feng%2520and%2520Zhenyu%2520Zhang%2520and%2520Naibin%2520Gu%2520and%2520Yilong%2520Chen%2520and%2520Peng%2520Fu%2520and%2520Zheng%2520Lin%2520and%2520Shuohuan%2520Wang%2520and%2520Yu%2520Sun%2520and%2520Hua%2520Wu%2520and%2520Weiping%2520Wang%2520and%2520Haifeng%2520Wang%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520on%2520various%2520vision-language%2520tasks%252C%2520yet%2520their%2520visual%2520perception%2520remains%2520limited.%2520Humans%252C%2520in%2520comparison%252C%2520perceive%2520complex%2520scenes%2520efficiently%2520by%2520dynamically%2520scanning%2520and%2520focusing%2520on%2520salient%2520regions%2520in%2520a%2520sequential%2520%2522blink-like%2522%2520process.%2520Motivated%2520by%2520this%2520strategy%252C%2520we%2520first%2520investigate%2520whether%2520MLLMs%2520exhibit%2520similar%2520behavior.%2520Our%2520pilot%2520analysis%2520reveals%2520that%2520MLLMs%2520naturally%2520attend%2520to%2520different%2520visual%2520regions%2520across%2520layers%2520and%2520that%2520selectively%2520allocating%2520more%2520computation%2520to%2520salient%2520tokens%2520can%2520enhance%2520visual%2520perception.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520Blink%252C%2520a%2520dynamic%2520visual%2520token%2520resolution%2520framework%2520that%2520emulates%2520the%2520human-inspired%2520process%2520within%2520a%2520single%2520forward%2520pass.%2520Specifically%252C%2520Blink%2520includes%2520two%2520modules%253A%2520saliency-guided%2520scanning%2520and%2520dynamic%2520token%2520resolution.%2520It%2520first%2520estimates%2520the%2520saliency%2520of%2520visual%2520tokens%2520in%2520each%2520layer%2520based%2520on%2520the%2520attention%2520map%252C%2520and%2520extends%2520important%2520tokens%2520through%2520a%2520plug-and-play%2520token%2520super-resolution%2520%2528TokenSR%2529%2520module.%2520In%2520the%2520next%2520layer%252C%2520it%2520drops%2520the%2520extended%2520tokens%2520when%2520they%2520lose%2520focus.%2520This%2520dynamic%2520mechanism%2520balances%2520broad%2520exploration%2520and%2520fine-grained%2520focus%252C%2520thereby%2520enhancing%2520visual%2520perception%2520adaptively%2520and%2520efficiently.%2520Extensive%2520experiments%2520validate%2520Blink%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520enhancing%2520visual%2520perception%2520and%2520multimodal%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blink%3A%20Dynamic%20Visual%20Token%20Resolution%20for%20Enhanced%20Multimodal%20Understanding&entry.906535625=Yuchen%20Feng%20and%20Zhenyu%20Zhang%20and%20Naibin%20Gu%20and%20Yilong%20Chen%20and%20Peng%20Fu%20and%20Zheng%20Lin%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Hua%20Wu%20and%20Weiping%20Wang%20and%20Haifeng%20Wang&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20on%20various%20vision-language%20tasks%2C%20yet%20their%20visual%20perception%20remains%20limited.%20Humans%2C%20in%20comparison%2C%20perceive%20complex%20scenes%20efficiently%20by%20dynamically%20scanning%20and%20focusing%20on%20salient%20regions%20in%20a%20sequential%20%22blink-like%22%20process.%20Motivated%20by%20this%20strategy%2C%20we%20first%20investigate%20whether%20MLLMs%20exhibit%20similar%20behavior.%20Our%20pilot%20analysis%20reveals%20that%20MLLMs%20naturally%20attend%20to%20different%20visual%20regions%20across%20layers%20and%20that%20selectively%20allocating%20more%20computation%20to%20salient%20tokens%20can%20enhance%20visual%20perception.%20Building%20on%20this%20insight%2C%20we%20propose%20Blink%2C%20a%20dynamic%20visual%20token%20resolution%20framework%20that%20emulates%20the%20human-inspired%20process%20within%20a%20single%20forward%20pass.%20Specifically%2C%20Blink%20includes%20two%20modules%3A%20saliency-guided%20scanning%20and%20dynamic%20token%20resolution.%20It%20first%20estimates%20the%20saliency%20of%20visual%20tokens%20in%20each%20layer%20based%20on%20the%20attention%20map%2C%20and%20extends%20important%20tokens%20through%20a%20plug-and-play%20token%20super-resolution%20%28TokenSR%29%20module.%20In%20the%20next%20layer%2C%20it%20drops%20the%20extended%20tokens%20when%20they%20lose%20focus.%20This%20dynamic%20mechanism%20balances%20broad%20exploration%20and%20fine-grained%20focus%2C%20thereby%20enhancing%20visual%20perception%20adaptively%20and%20efficiently.%20Extensive%20experiments%20validate%20Blink%2C%20demonstrating%20its%20effectiveness%20in%20enhancing%20visual%20perception%20and%20multimodal%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2512.10548v1&entry.124074799=Read"},
{"title": "LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models", "author": "Qingqiao Hu and Weimin Lyu and Meilong Xu and Kehan Qi and Xiaoling Hu and Saumya Gupta and Jiawei Zhou and Chao Chen", "abstract": "Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.", "link": "http://arxiv.org/abs/2512.05391v2", "date": "2025-12-11", "relevancy": 2.7981, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoC-Path%3A%20Learning%20to%20Compress%20for%20Pathology%20Multimodal%20Large%20Language%20Models&body=Title%3A%20LoC-Path%3A%20Learning%20to%20Compress%20for%20Pathology%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Qingqiao%20Hu%20and%20Weimin%20Lyu%20and%20Meilong%20Xu%20and%20Kehan%20Qi%20and%20Xiaoling%20Hu%20and%20Saumya%20Gupta%20and%20Jiawei%20Zhou%20and%20Chao%20Chen%0AAbstract%3A%20Whole%20Slide%20Image%20%28WSI%29%20understanding%20is%20fundamentally%20challenging%20due%20to%20its%20gigapixel%20scale%20and%20the%20extreme%20sparsity%20of%20diagnostically%20relevant%20regions.%20Unlike%20human%20experts%20who%20primarily%20rely%20on%20key%20areas%20to%20arrive%20at%20a%20diagnosis%2C%20existing%20slide-level%20multimodal%20large%20language%20models%20%28MLLMs%29%20for%20pathology%20rely%20on%20heavy%20slide-level%20encoders%20that%20process%20thousands%20of%20patch%20features%20in%20a%20brute-force%20manner%2C%20resulting%20in%20excessive%20computational%20cost.%20In%20this%20work%2C%20we%20revisit%20the%20WSI-language%20modeling%20paradigm%20and%20show%20that%20tile-level%20features%20exhibit%20strong%20global%20and%20local%20redundancy%2C%20whereas%20only%20a%20small%20subset%20of%20tiles%20are%20truly%20task-relevant.%20Motivated%20by%20this%20observation%2C%20we%20introduce%20an%20efficient%20MLLM%20framework%2C%20called%20LoC-Path%2C%20that%20replaces%20the%20expensive%20slide-level%20encoder%20with%20redundancy-reducing%20modules.%20We%20first%20design%20a%20Sparse%20Token%20Merger%20%28STM%29%20and%20an%20MAE-pretrained%20resampler%20to%20remove%20local%20redundancy%20and%20compress%20globally%20redundant%20tile%20tokens%20into%20a%20compact%20slide-level%20representation%20set.%20We%20then%20propose%20a%20Cross-Attention%20Routing%20Adapter%20%28CARA%29%20and%20a%20Token%20Importance%20Scorer%20%28TIS%29%20to%20integrate%20the%20compressed%20visual%20representation%20with%20the%20language%20model%20in%20a%20computation-efficient%20manner.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%20performance%20comparable%20to%20existing%20state-of-the-art%20whole-slide%20MLLMs%2C%20while%20requiring%20significantly%20lower%20computation%20and%20memory.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoC-Path%253A%2520Learning%2520to%2520Compress%2520for%2520Pathology%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DQingqiao%2520Hu%2520and%2520Weimin%2520Lyu%2520and%2520Meilong%2520Xu%2520and%2520Kehan%2520Qi%2520and%2520Xiaoling%2520Hu%2520and%2520Saumya%2520Gupta%2520and%2520Jiawei%2520Zhou%2520and%2520Chao%2520Chen%26entry.1292438233%3DWhole%2520Slide%2520Image%2520%2528WSI%2529%2520understanding%2520is%2520fundamentally%2520challenging%2520due%2520to%2520its%2520gigapixel%2520scale%2520and%2520the%2520extreme%2520sparsity%2520of%2520diagnostically%2520relevant%2520regions.%2520Unlike%2520human%2520experts%2520who%2520primarily%2520rely%2520on%2520key%2520areas%2520to%2520arrive%2520at%2520a%2520diagnosis%252C%2520existing%2520slide-level%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520for%2520pathology%2520rely%2520on%2520heavy%2520slide-level%2520encoders%2520that%2520process%2520thousands%2520of%2520patch%2520features%2520in%2520a%2520brute-force%2520manner%252C%2520resulting%2520in%2520excessive%2520computational%2520cost.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520WSI-language%2520modeling%2520paradigm%2520and%2520show%2520that%2520tile-level%2520features%2520exhibit%2520strong%2520global%2520and%2520local%2520redundancy%252C%2520whereas%2520only%2520a%2520small%2520subset%2520of%2520tiles%2520are%2520truly%2520task-relevant.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520introduce%2520an%2520efficient%2520MLLM%2520framework%252C%2520called%2520LoC-Path%252C%2520that%2520replaces%2520the%2520expensive%2520slide-level%2520encoder%2520with%2520redundancy-reducing%2520modules.%2520We%2520first%2520design%2520a%2520Sparse%2520Token%2520Merger%2520%2528STM%2529%2520and%2520an%2520MAE-pretrained%2520resampler%2520to%2520remove%2520local%2520redundancy%2520and%2520compress%2520globally%2520redundant%2520tile%2520tokens%2520into%2520a%2520compact%2520slide-level%2520representation%2520set.%2520We%2520then%2520propose%2520a%2520Cross-Attention%2520Routing%2520Adapter%2520%2528CARA%2529%2520and%2520a%2520Token%2520Importance%2520Scorer%2520%2528TIS%2529%2520to%2520integrate%2520the%2520compressed%2520visual%2520representation%2520with%2520the%2520language%2520model%2520in%2520a%2520computation-efficient%2520manner.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520performance%2520comparable%2520to%2520existing%2520state-of-the-art%2520whole-slide%2520MLLMs%252C%2520while%2520requiring%2520significantly%2520lower%2520computation%2520and%2520memory.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoC-Path%3A%20Learning%20to%20Compress%20for%20Pathology%20Multimodal%20Large%20Language%20Models&entry.906535625=Qingqiao%20Hu%20and%20Weimin%20Lyu%20and%20Meilong%20Xu%20and%20Kehan%20Qi%20and%20Xiaoling%20Hu%20and%20Saumya%20Gupta%20and%20Jiawei%20Zhou%20and%20Chao%20Chen&entry.1292438233=Whole%20Slide%20Image%20%28WSI%29%20understanding%20is%20fundamentally%20challenging%20due%20to%20its%20gigapixel%20scale%20and%20the%20extreme%20sparsity%20of%20diagnostically%20relevant%20regions.%20Unlike%20human%20experts%20who%20primarily%20rely%20on%20key%20areas%20to%20arrive%20at%20a%20diagnosis%2C%20existing%20slide-level%20multimodal%20large%20language%20models%20%28MLLMs%29%20for%20pathology%20rely%20on%20heavy%20slide-level%20encoders%20that%20process%20thousands%20of%20patch%20features%20in%20a%20brute-force%20manner%2C%20resulting%20in%20excessive%20computational%20cost.%20In%20this%20work%2C%20we%20revisit%20the%20WSI-language%20modeling%20paradigm%20and%20show%20that%20tile-level%20features%20exhibit%20strong%20global%20and%20local%20redundancy%2C%20whereas%20only%20a%20small%20subset%20of%20tiles%20are%20truly%20task-relevant.%20Motivated%20by%20this%20observation%2C%20we%20introduce%20an%20efficient%20MLLM%20framework%2C%20called%20LoC-Path%2C%20that%20replaces%20the%20expensive%20slide-level%20encoder%20with%20redundancy-reducing%20modules.%20We%20first%20design%20a%20Sparse%20Token%20Merger%20%28STM%29%20and%20an%20MAE-pretrained%20resampler%20to%20remove%20local%20redundancy%20and%20compress%20globally%20redundant%20tile%20tokens%20into%20a%20compact%20slide-level%20representation%20set.%20We%20then%20propose%20a%20Cross-Attention%20Routing%20Adapter%20%28CARA%29%20and%20a%20Token%20Importance%20Scorer%20%28TIS%29%20to%20integrate%20the%20compressed%20visual%20representation%20with%20the%20language%20model%20in%20a%20computation-efficient%20manner.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%20performance%20comparable%20to%20existing%20state-of-the-art%20whole-slide%20MLLMs%2C%20while%20requiring%20significantly%20lower%20computation%20and%20memory.&entry.1838667208=http%3A//arxiv.org/abs/2512.05391v2&entry.124074799=Read"},
{"title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis", "author": "Xiang Fan and Sharath Girish and Vivek Ramanujan and Chaoyang Wang and Ashkan Mirzaei and Petr Sushko and Aliaksandr Siarohin and Sergey Tulyakov and Ranjay Krishna", "abstract": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/", "link": "http://arxiv.org/abs/2512.10940v1", "date": "2025-12-11", "relevancy": 2.7968, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7037}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7037}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniView%3A%20An%20All-Seeing%20Diffusion%20Model%20for%203D%20and%204D%20View%20Synthesis&body=Title%3A%20OmniView%3A%20An%20All-Seeing%20Diffusion%20Model%20for%203D%20and%204D%20View%20Synthesis%0AAuthor%3A%20Xiang%20Fan%20and%20Sharath%20Girish%20and%20Vivek%20Ramanujan%20and%20Chaoyang%20Wang%20and%20Ashkan%20Mirzaei%20and%20Petr%20Sushko%20and%20Aliaksandr%20Siarohin%20and%20Sergey%20Tulyakov%20and%20Ranjay%20Krishna%0AAbstract%3A%20Prior%20approaches%20injecting%20camera%20control%20into%20diffusion%20models%20have%20focused%20on%20specific%20subsets%20of%204D%20consistency%20tasks%3A%20novel%20view%20synthesis%2C%20text-to-video%20with%20camera%20control%2C%20image-to-video%2C%20amongst%20others.%20Therefore%2C%20these%20fragmented%20approaches%20are%20trained%20on%20disjoint%20slices%20of%20available%203D/4D%20data.%20We%20introduce%20OmniView%2C%20a%20unified%20framework%20that%20generalizes%20across%20a%20wide%20range%20of%204D%20consistency%20tasks.%20Our%20method%20separately%20represents%20space%2C%20time%2C%20and%20view%20conditions%2C%20enabling%20flexible%20combinations%20of%20these%20inputs.%20For%20example%2C%20OmniView%20can%20synthesize%20novel%20views%20from%20static%2C%20dynamic%2C%20and%20multiview%20inputs%2C%20extrapolate%20trajectories%20forward%20and%20backward%20in%20time%2C%20and%20create%20videos%20from%20text%20or%20image%20prompts%20with%20full%20camera%20control.%20OmniView%20is%20competitive%20with%20task-specific%20models%20across%20diverse%20benchmarks%20and%20metrics%2C%20improving%20image%20quality%20scores%20among%20camera-conditioned%20diffusion%20models%20by%20up%20to%2033%5C%25%20in%20multiview%20NVS%20LLFF%20dataset%2C%2060%5C%25%20in%20dynamic%20NVS%20Neural%203D%20Video%20benchmark%2C%2020%5C%25%20in%20static%20camera%20control%20on%20RE-10K%2C%20and%20reducing%20camera%20trajectory%20errors%20by%204x%20in%20text-conditioned%20video%20generation.%20With%20strong%20generalizability%20in%20one%20model%2C%20OmniView%20demonstrates%20the%20feasibility%20of%20a%20generalist%204D%20video%20model.%20Project%20page%20is%20available%20at%20https%3A//snap-research.github.io/OmniView/%0ALink%3A%20http%3A//arxiv.org/abs/2512.10940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniView%253A%2520An%2520All-Seeing%2520Diffusion%2520Model%2520for%25203D%2520and%25204D%2520View%2520Synthesis%26entry.906535625%3DXiang%2520Fan%2520and%2520Sharath%2520Girish%2520and%2520Vivek%2520Ramanujan%2520and%2520Chaoyang%2520Wang%2520and%2520Ashkan%2520Mirzaei%2520and%2520Petr%2520Sushko%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Sergey%2520Tulyakov%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3DPrior%2520approaches%2520injecting%2520camera%2520control%2520into%2520diffusion%2520models%2520have%2520focused%2520on%2520specific%2520subsets%2520of%25204D%2520consistency%2520tasks%253A%2520novel%2520view%2520synthesis%252C%2520text-to-video%2520with%2520camera%2520control%252C%2520image-to-video%252C%2520amongst%2520others.%2520Therefore%252C%2520these%2520fragmented%2520approaches%2520are%2520trained%2520on%2520disjoint%2520slices%2520of%2520available%25203D/4D%2520data.%2520We%2520introduce%2520OmniView%252C%2520a%2520unified%2520framework%2520that%2520generalizes%2520across%2520a%2520wide%2520range%2520of%25204D%2520consistency%2520tasks.%2520Our%2520method%2520separately%2520represents%2520space%252C%2520time%252C%2520and%2520view%2520conditions%252C%2520enabling%2520flexible%2520combinations%2520of%2520these%2520inputs.%2520For%2520example%252C%2520OmniView%2520can%2520synthesize%2520novel%2520views%2520from%2520static%252C%2520dynamic%252C%2520and%2520multiview%2520inputs%252C%2520extrapolate%2520trajectories%2520forward%2520and%2520backward%2520in%2520time%252C%2520and%2520create%2520videos%2520from%2520text%2520or%2520image%2520prompts%2520with%2520full%2520camera%2520control.%2520OmniView%2520is%2520competitive%2520with%2520task-specific%2520models%2520across%2520diverse%2520benchmarks%2520and%2520metrics%252C%2520improving%2520image%2520quality%2520scores%2520among%2520camera-conditioned%2520diffusion%2520models%2520by%2520up%2520to%252033%255C%2525%2520in%2520multiview%2520NVS%2520LLFF%2520dataset%252C%252060%255C%2525%2520in%2520dynamic%2520NVS%2520Neural%25203D%2520Video%2520benchmark%252C%252020%255C%2525%2520in%2520static%2520camera%2520control%2520on%2520RE-10K%252C%2520and%2520reducing%2520camera%2520trajectory%2520errors%2520by%25204x%2520in%2520text-conditioned%2520video%2520generation.%2520With%2520strong%2520generalizability%2520in%2520one%2520model%252C%2520OmniView%2520demonstrates%2520the%2520feasibility%2520of%2520a%2520generalist%25204D%2520video%2520model.%2520Project%2520page%2520is%2520available%2520at%2520https%253A//snap-research.github.io/OmniView/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniView%3A%20An%20All-Seeing%20Diffusion%20Model%20for%203D%20and%204D%20View%20Synthesis&entry.906535625=Xiang%20Fan%20and%20Sharath%20Girish%20and%20Vivek%20Ramanujan%20and%20Chaoyang%20Wang%20and%20Ashkan%20Mirzaei%20and%20Petr%20Sushko%20and%20Aliaksandr%20Siarohin%20and%20Sergey%20Tulyakov%20and%20Ranjay%20Krishna&entry.1292438233=Prior%20approaches%20injecting%20camera%20control%20into%20diffusion%20models%20have%20focused%20on%20specific%20subsets%20of%204D%20consistency%20tasks%3A%20novel%20view%20synthesis%2C%20text-to-video%20with%20camera%20control%2C%20image-to-video%2C%20amongst%20others.%20Therefore%2C%20these%20fragmented%20approaches%20are%20trained%20on%20disjoint%20slices%20of%20available%203D/4D%20data.%20We%20introduce%20OmniView%2C%20a%20unified%20framework%20that%20generalizes%20across%20a%20wide%20range%20of%204D%20consistency%20tasks.%20Our%20method%20separately%20represents%20space%2C%20time%2C%20and%20view%20conditions%2C%20enabling%20flexible%20combinations%20of%20these%20inputs.%20For%20example%2C%20OmniView%20can%20synthesize%20novel%20views%20from%20static%2C%20dynamic%2C%20and%20multiview%20inputs%2C%20extrapolate%20trajectories%20forward%20and%20backward%20in%20time%2C%20and%20create%20videos%20from%20text%20or%20image%20prompts%20with%20full%20camera%20control.%20OmniView%20is%20competitive%20with%20task-specific%20models%20across%20diverse%20benchmarks%20and%20metrics%2C%20improving%20image%20quality%20scores%20among%20camera-conditioned%20diffusion%20models%20by%20up%20to%2033%5C%25%20in%20multiview%20NVS%20LLFF%20dataset%2C%2060%5C%25%20in%20dynamic%20NVS%20Neural%203D%20Video%20benchmark%2C%2020%5C%25%20in%20static%20camera%20control%20on%20RE-10K%2C%20and%20reducing%20camera%20trajectory%20errors%20by%204x%20in%20text-conditioned%20video%20generation.%20With%20strong%20generalizability%20in%20one%20model%2C%20OmniView%20demonstrates%20the%20feasibility%20of%20a%20generalist%204D%20video%20model.%20Project%20page%20is%20available%20at%20https%3A//snap-research.github.io/OmniView/&entry.1838667208=http%3A//arxiv.org/abs/2512.10940v1&entry.124074799=Read"},
{"title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language", "author": "Delong Chen and Mustafa Shukor and Theo Moutakanni and Willy Chung and Jade Yu and Tejaswi Kasarla and Allen Bolourchi and Yann LeCun and Pascale Fung", "abstract": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.", "link": "http://arxiv.org/abs/2512.10942v1", "date": "2025-12-11", "relevancy": 2.7797, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VL-JEPA%3A%20Joint%20Embedding%20Predictive%20Architecture%20for%20Vision-language&body=Title%3A%20VL-JEPA%3A%20Joint%20Embedding%20Predictive%20Architecture%20for%20Vision-language%0AAuthor%3A%20Delong%20Chen%20and%20Mustafa%20Shukor%20and%20Theo%20Moutakanni%20and%20Willy%20Chung%20and%20Jade%20Yu%20and%20Tejaswi%20Kasarla%20and%20Allen%20Bolourchi%20and%20Yann%20LeCun%20and%20Pascale%20Fung%0AAbstract%3A%20We%20introduce%20VL-JEPA%2C%20a%20vision-language%20model%20built%20on%20a%20Joint%20Embedding%20Predictive%20Architecture%20%28JEPA%29.%20Instead%20of%20autoregressively%20generating%20tokens%20as%20in%20classical%20VLMs%2C%20VL-JEPA%20predicts%20continuous%20embeddings%20of%20the%20target%20texts.%20By%20learning%20in%20an%20abstract%20representation%20space%2C%20the%20model%20focuses%20on%20task-relevant%20semantics%20while%20abstracting%20away%20surface-level%20linguistic%20variability.%20In%20a%20strictly%20controlled%20comparison%20against%20standard%20token-space%20VLM%20training%20with%20the%20same%20vision%20encoder%20and%20training%20data%2C%20VL-JEPA%20achieves%20stronger%20performance%20while%20having%2050%25%20fewer%20trainable%20parameters.%20At%20inference%20time%2C%20a%20lightweight%20text%20decoder%20is%20invoked%20only%20when%20needed%20to%20translate%20VL-JEPA%20predicted%20embeddings%20into%20text.%20We%20show%20that%20VL-JEPA%20natively%20supports%20selective%20decoding%20that%20reduces%20the%20number%20of%20decoding%20operations%20by%202.85x%20while%20maintaining%20similar%20performance%20compared%20to%20non-adaptive%20uniform%20decoding.%20Beyond%20generation%2C%20the%20VL-JEPA%27s%20embedding%20space%20naturally%20supports%20open-vocabulary%20classification%2C%20text-to-video%20retrieval%2C%20and%20discriminative%20VQA%20without%20any%20architecture%20modification.%20On%20eight%20video%20classification%20and%20eight%20video%20retrieval%20datasets%2C%20the%20average%20performance%20VL-JEPA%20surpasses%20that%20of%20CLIP%2C%20SigLIP2%2C%20and%20Perception%20Encoder.%20At%20the%20same%20time%2C%20the%20model%20achieves%20comparable%20performance%20as%20classical%20VLMs%20%28InstructBLIP%2C%20QwenVL%29%20on%20four%20VQA%20datasets%3A%20GQA%2C%20TallyQA%2C%20POPE%20and%20POPEv2%2C%20despite%20only%20having%201.6B%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVL-JEPA%253A%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520for%2520Vision-language%26entry.906535625%3DDelong%2520Chen%2520and%2520Mustafa%2520Shukor%2520and%2520Theo%2520Moutakanni%2520and%2520Willy%2520Chung%2520and%2520Jade%2520Yu%2520and%2520Tejaswi%2520Kasarla%2520and%2520Allen%2520Bolourchi%2520and%2520Yann%2520LeCun%2520and%2520Pascale%2520Fung%26entry.1292438233%3DWe%2520introduce%2520VL-JEPA%252C%2520a%2520vision-language%2520model%2520built%2520on%2520a%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520%2528JEPA%2529.%2520Instead%2520of%2520autoregressively%2520generating%2520tokens%2520as%2520in%2520classical%2520VLMs%252C%2520VL-JEPA%2520predicts%2520continuous%2520embeddings%2520of%2520the%2520target%2520texts.%2520By%2520learning%2520in%2520an%2520abstract%2520representation%2520space%252C%2520the%2520model%2520focuses%2520on%2520task-relevant%2520semantics%2520while%2520abstracting%2520away%2520surface-level%2520linguistic%2520variability.%2520In%2520a%2520strictly%2520controlled%2520comparison%2520against%2520standard%2520token-space%2520VLM%2520training%2520with%2520the%2520same%2520vision%2520encoder%2520and%2520training%2520data%252C%2520VL-JEPA%2520achieves%2520stronger%2520performance%2520while%2520having%252050%2525%2520fewer%2520trainable%2520parameters.%2520At%2520inference%2520time%252C%2520a%2520lightweight%2520text%2520decoder%2520is%2520invoked%2520only%2520when%2520needed%2520to%2520translate%2520VL-JEPA%2520predicted%2520embeddings%2520into%2520text.%2520We%2520show%2520that%2520VL-JEPA%2520natively%2520supports%2520selective%2520decoding%2520that%2520reduces%2520the%2520number%2520of%2520decoding%2520operations%2520by%25202.85x%2520while%2520maintaining%2520similar%2520performance%2520compared%2520to%2520non-adaptive%2520uniform%2520decoding.%2520Beyond%2520generation%252C%2520the%2520VL-JEPA%2527s%2520embedding%2520space%2520naturally%2520supports%2520open-vocabulary%2520classification%252C%2520text-to-video%2520retrieval%252C%2520and%2520discriminative%2520VQA%2520without%2520any%2520architecture%2520modification.%2520On%2520eight%2520video%2520classification%2520and%2520eight%2520video%2520retrieval%2520datasets%252C%2520the%2520average%2520performance%2520VL-JEPA%2520surpasses%2520that%2520of%2520CLIP%252C%2520SigLIP2%252C%2520and%2520Perception%2520Encoder.%2520At%2520the%2520same%2520time%252C%2520the%2520model%2520achieves%2520comparable%2520performance%2520as%2520classical%2520VLMs%2520%2528InstructBLIP%252C%2520QwenVL%2529%2520on%2520four%2520VQA%2520datasets%253A%2520GQA%252C%2520TallyQA%252C%2520POPE%2520and%2520POPEv2%252C%2520despite%2520only%2520having%25201.6B%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VL-JEPA%3A%20Joint%20Embedding%20Predictive%20Architecture%20for%20Vision-language&entry.906535625=Delong%20Chen%20and%20Mustafa%20Shukor%20and%20Theo%20Moutakanni%20and%20Willy%20Chung%20and%20Jade%20Yu%20and%20Tejaswi%20Kasarla%20and%20Allen%20Bolourchi%20and%20Yann%20LeCun%20and%20Pascale%20Fung&entry.1292438233=We%20introduce%20VL-JEPA%2C%20a%20vision-language%20model%20built%20on%20a%20Joint%20Embedding%20Predictive%20Architecture%20%28JEPA%29.%20Instead%20of%20autoregressively%20generating%20tokens%20as%20in%20classical%20VLMs%2C%20VL-JEPA%20predicts%20continuous%20embeddings%20of%20the%20target%20texts.%20By%20learning%20in%20an%20abstract%20representation%20space%2C%20the%20model%20focuses%20on%20task-relevant%20semantics%20while%20abstracting%20away%20surface-level%20linguistic%20variability.%20In%20a%20strictly%20controlled%20comparison%20against%20standard%20token-space%20VLM%20training%20with%20the%20same%20vision%20encoder%20and%20training%20data%2C%20VL-JEPA%20achieves%20stronger%20performance%20while%20having%2050%25%20fewer%20trainable%20parameters.%20At%20inference%20time%2C%20a%20lightweight%20text%20decoder%20is%20invoked%20only%20when%20needed%20to%20translate%20VL-JEPA%20predicted%20embeddings%20into%20text.%20We%20show%20that%20VL-JEPA%20natively%20supports%20selective%20decoding%20that%20reduces%20the%20number%20of%20decoding%20operations%20by%202.85x%20while%20maintaining%20similar%20performance%20compared%20to%20non-adaptive%20uniform%20decoding.%20Beyond%20generation%2C%20the%20VL-JEPA%27s%20embedding%20space%20naturally%20supports%20open-vocabulary%20classification%2C%20text-to-video%20retrieval%2C%20and%20discriminative%20VQA%20without%20any%20architecture%20modification.%20On%20eight%20video%20classification%20and%20eight%20video%20retrieval%20datasets%2C%20the%20average%20performance%20VL-JEPA%20surpasses%20that%20of%20CLIP%2C%20SigLIP2%2C%20and%20Perception%20Encoder.%20At%20the%20same%20time%2C%20the%20model%20achieves%20comparable%20performance%20as%20classical%20VLMs%20%28InstructBLIP%2C%20QwenVL%29%20on%20four%20VQA%20datasets%3A%20GQA%2C%20TallyQA%2C%20POPE%20and%20POPEv2%2C%20despite%20only%20having%201.6B%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2512.10942v1&entry.124074799=Read"},
{"title": "Sharp Monocular View Synthesis in Less Than a Second", "author": "Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\u00ebl Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun", "abstract": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp", "link": "http://arxiv.org/abs/2512.10685v1", "date": "2025-12-11", "relevancy": 2.7653, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5647}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5543}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharp%20Monocular%20View%20Synthesis%20in%20Less%20Than%20a%20Second&body=Title%3A%20Sharp%20Monocular%20View%20Synthesis%20in%20Less%20Than%20a%20Second%0AAuthor%3A%20Lars%20Mescheder%20and%20Wei%20Dong%20and%20Shiwei%20Li%20and%20Xuyang%20Bai%20and%20Marcel%20Santos%20and%20Peiyun%20Hu%20and%20Bruno%20Lecouat%20and%20Mingmin%20Zhen%20and%20Ama%C3%ABl%20Delaunoy%20and%20Tian%20Fang%20and%20Yanghai%20Tsin%20and%20Stephan%20R.%20Richter%20and%20Vladlen%20Koltun%0AAbstract%3A%20We%20present%20SHARP%2C%20an%20approach%20to%20photorealistic%20view%20synthesis%20from%20a%20single%20image.%20Given%20a%20single%20photograph%2C%20SHARP%20regresses%20the%20parameters%20of%20a%203D%20Gaussian%20representation%20of%20the%20depicted%20scene.%20This%20is%20done%20in%20less%20than%20a%20second%20on%20a%20standard%20GPU%20via%20a%20single%20feedforward%20pass%20through%20a%20neural%20network.%20The%203D%20Gaussian%20representation%20produced%20by%20SHARP%20can%20then%20be%20rendered%20in%20real%20time%2C%20yielding%20high-resolution%20photorealistic%20images%20for%20nearby%20views.%20The%20representation%20is%20metric%2C%20with%20absolute%20scale%2C%20supporting%20metric%20camera%20movements.%20Experimental%20results%20demonstrate%20that%20SHARP%20delivers%20robust%20zero-shot%20generalization%20across%20datasets.%20It%20sets%20a%20new%20state%20of%20the%20art%20on%20multiple%20datasets%2C%20reducing%20LPIPS%20by%2025-34%25%20and%20DISTS%20by%2021-43%25%20versus%20the%20best%20prior%20model%2C%20while%20lowering%20the%20synthesis%20time%20by%20three%20orders%20of%20magnitude.%20Code%20and%20weights%20are%20provided%20at%20https%3A//github.com/apple/ml-sharp%0ALink%3A%20http%3A//arxiv.org/abs/2512.10685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharp%2520Monocular%2520View%2520Synthesis%2520in%2520Less%2520Than%2520a%2520Second%26entry.906535625%3DLars%2520Mescheder%2520and%2520Wei%2520Dong%2520and%2520Shiwei%2520Li%2520and%2520Xuyang%2520Bai%2520and%2520Marcel%2520Santos%2520and%2520Peiyun%2520Hu%2520and%2520Bruno%2520Lecouat%2520and%2520Mingmin%2520Zhen%2520and%2520Ama%25C3%25ABl%2520Delaunoy%2520and%2520Tian%2520Fang%2520and%2520Yanghai%2520Tsin%2520and%2520Stephan%2520R.%2520Richter%2520and%2520Vladlen%2520Koltun%26entry.1292438233%3DWe%2520present%2520SHARP%252C%2520an%2520approach%2520to%2520photorealistic%2520view%2520synthesis%2520from%2520a%2520single%2520image.%2520Given%2520a%2520single%2520photograph%252C%2520SHARP%2520regresses%2520the%2520parameters%2520of%2520a%25203D%2520Gaussian%2520representation%2520of%2520the%2520depicted%2520scene.%2520This%2520is%2520done%2520in%2520less%2520than%2520a%2520second%2520on%2520a%2520standard%2520GPU%2520via%2520a%2520single%2520feedforward%2520pass%2520through%2520a%2520neural%2520network.%2520The%25203D%2520Gaussian%2520representation%2520produced%2520by%2520SHARP%2520can%2520then%2520be%2520rendered%2520in%2520real%2520time%252C%2520yielding%2520high-resolution%2520photorealistic%2520images%2520for%2520nearby%2520views.%2520The%2520representation%2520is%2520metric%252C%2520with%2520absolute%2520scale%252C%2520supporting%2520metric%2520camera%2520movements.%2520Experimental%2520results%2520demonstrate%2520that%2520SHARP%2520delivers%2520robust%2520zero-shot%2520generalization%2520across%2520datasets.%2520It%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520multiple%2520datasets%252C%2520reducing%2520LPIPS%2520by%252025-34%2525%2520and%2520DISTS%2520by%252021-43%2525%2520versus%2520the%2520best%2520prior%2520model%252C%2520while%2520lowering%2520the%2520synthesis%2520time%2520by%2520three%2520orders%2520of%2520magnitude.%2520Code%2520and%2520weights%2520are%2520provided%2520at%2520https%253A//github.com/apple/ml-sharp%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20Monocular%20View%20Synthesis%20in%20Less%20Than%20a%20Second&entry.906535625=Lars%20Mescheder%20and%20Wei%20Dong%20and%20Shiwei%20Li%20and%20Xuyang%20Bai%20and%20Marcel%20Santos%20and%20Peiyun%20Hu%20and%20Bruno%20Lecouat%20and%20Mingmin%20Zhen%20and%20Ama%C3%ABl%20Delaunoy%20and%20Tian%20Fang%20and%20Yanghai%20Tsin%20and%20Stephan%20R.%20Richter%20and%20Vladlen%20Koltun&entry.1292438233=We%20present%20SHARP%2C%20an%20approach%20to%20photorealistic%20view%20synthesis%20from%20a%20single%20image.%20Given%20a%20single%20photograph%2C%20SHARP%20regresses%20the%20parameters%20of%20a%203D%20Gaussian%20representation%20of%20the%20depicted%20scene.%20This%20is%20done%20in%20less%20than%20a%20second%20on%20a%20standard%20GPU%20via%20a%20single%20feedforward%20pass%20through%20a%20neural%20network.%20The%203D%20Gaussian%20representation%20produced%20by%20SHARP%20can%20then%20be%20rendered%20in%20real%20time%2C%20yielding%20high-resolution%20photorealistic%20images%20for%20nearby%20views.%20The%20representation%20is%20metric%2C%20with%20absolute%20scale%2C%20supporting%20metric%20camera%20movements.%20Experimental%20results%20demonstrate%20that%20SHARP%20delivers%20robust%20zero-shot%20generalization%20across%20datasets.%20It%20sets%20a%20new%20state%20of%20the%20art%20on%20multiple%20datasets%2C%20reducing%20LPIPS%20by%2025-34%25%20and%20DISTS%20by%2021-43%25%20versus%20the%20best%20prior%20model%2C%20while%20lowering%20the%20synthesis%20time%20by%20three%20orders%20of%20magnitude.%20Code%20and%20weights%20are%20provided%20at%20https%3A//github.com/apple/ml-sharp&entry.1838667208=http%3A//arxiv.org/abs/2512.10685v1&entry.124074799=Read"},
{"title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders", "author": "Akshay Kulkarni and Tsui-Wei Weng and Vivek Narayanaswamy and Shusen Liu and Wesam A. Sakla and Kowshik Thopalli", "abstract": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.", "link": "http://arxiv.org/abs/2512.10805v1", "date": "2025-12-11", "relevancy": 2.7589, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20and%20Steerable%20Concept%20Bottleneck%20Sparse%20Autoencoders&body=Title%3A%20Interpretable%20and%20Steerable%20Concept%20Bottleneck%20Sparse%20Autoencoders%0AAuthor%3A%20Akshay%20Kulkarni%20and%20Tsui-Wei%20Weng%20and%20Vivek%20Narayanaswamy%20and%20Shusen%20Liu%20and%20Wesam%20A.%20Sakla%20and%20Kowshik%20Thopalli%0AAbstract%3A%20Sparse%20autoencoders%20%28SAEs%29%20promise%20a%20unified%20approach%20for%20mechanistic%20interpretability%2C%20concept%20discovery%2C%20and%20model%20steering%20in%20LLMs%20and%20LVLMs.%20However%2C%20realizing%20this%20potential%20requires%20that%20the%20learned%20features%20be%20both%20interpretable%20and%20steerable.%20To%20that%20end%2C%20we%20introduce%20two%20new%20computationally%20inexpensive%20interpretability%20and%20steerability%20metrics%20and%20conduct%20a%20systematic%20analysis%20on%20LVLMs.%20Our%20analysis%20uncovers%20two%20observations%3B%20%28i%29%20a%20majority%20of%20SAE%20neurons%20exhibit%20either%20low%20interpretability%20or%20low%20steerability%20or%20both%2C%20rendering%20them%20ineffective%20for%20downstream%20use%3B%20and%20%28ii%29%20due%20to%20the%20unsupervised%20nature%20of%20SAEs%2C%20user-desired%20concepts%20are%20often%20absent%20in%20the%20learned%20dictionary%2C%20thus%20limiting%20their%20practical%20utility.%20To%20address%20these%20limitations%2C%20we%20propose%20Concept%20Bottleneck%20Sparse%20Autoencoders%20%28CB-SAE%29%20-%20a%20novel%20post-hoc%20framework%20that%20prunes%20low-utility%20neurons%20and%20augments%20the%20latent%20space%20with%20a%20lightweight%20concept%20bottleneck%20aligned%20to%20a%20user-defined%20concept%20set.%20The%20resulting%20CB-SAE%20improves%20interpretability%20by%20%2B32.1%25%20and%20steerability%20by%20%2B14.5%25%20across%20LVLMs%20and%20image%20generation%20tasks.%20We%20will%20make%20our%20code%20and%20model%20weights%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520and%2520Steerable%2520Concept%2520Bottleneck%2520Sparse%2520Autoencoders%26entry.906535625%3DAkshay%2520Kulkarni%2520and%2520Tsui-Wei%2520Weng%2520and%2520Vivek%2520Narayanaswamy%2520and%2520Shusen%2520Liu%2520and%2520Wesam%2520A.%2520Sakla%2520and%2520Kowshik%2520Thopalli%26entry.1292438233%3DSparse%2520autoencoders%2520%2528SAEs%2529%2520promise%2520a%2520unified%2520approach%2520for%2520mechanistic%2520interpretability%252C%2520concept%2520discovery%252C%2520and%2520model%2520steering%2520in%2520LLMs%2520and%2520LVLMs.%2520However%252C%2520realizing%2520this%2520potential%2520requires%2520that%2520the%2520learned%2520features%2520be%2520both%2520interpretable%2520and%2520steerable.%2520To%2520that%2520end%252C%2520we%2520introduce%2520two%2520new%2520computationally%2520inexpensive%2520interpretability%2520and%2520steerability%2520metrics%2520and%2520conduct%2520a%2520systematic%2520analysis%2520on%2520LVLMs.%2520Our%2520analysis%2520uncovers%2520two%2520observations%253B%2520%2528i%2529%2520a%2520majority%2520of%2520SAE%2520neurons%2520exhibit%2520either%2520low%2520interpretability%2520or%2520low%2520steerability%2520or%2520both%252C%2520rendering%2520them%2520ineffective%2520for%2520downstream%2520use%253B%2520and%2520%2528ii%2529%2520due%2520to%2520the%2520unsupervised%2520nature%2520of%2520SAEs%252C%2520user-desired%2520concepts%2520are%2520often%2520absent%2520in%2520the%2520learned%2520dictionary%252C%2520thus%2520limiting%2520their%2520practical%2520utility.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Concept%2520Bottleneck%2520Sparse%2520Autoencoders%2520%2528CB-SAE%2529%2520-%2520a%2520novel%2520post-hoc%2520framework%2520that%2520prunes%2520low-utility%2520neurons%2520and%2520augments%2520the%2520latent%2520space%2520with%2520a%2520lightweight%2520concept%2520bottleneck%2520aligned%2520to%2520a%2520user-defined%2520concept%2520set.%2520The%2520resulting%2520CB-SAE%2520improves%2520interpretability%2520by%2520%252B32.1%2525%2520and%2520steerability%2520by%2520%252B14.5%2525%2520across%2520LVLMs%2520and%2520image%2520generation%2520tasks.%2520We%2520will%2520make%2520our%2520code%2520and%2520model%2520weights%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20and%20Steerable%20Concept%20Bottleneck%20Sparse%20Autoencoders&entry.906535625=Akshay%20Kulkarni%20and%20Tsui-Wei%20Weng%20and%20Vivek%20Narayanaswamy%20and%20Shusen%20Liu%20and%20Wesam%20A.%20Sakla%20and%20Kowshik%20Thopalli&entry.1292438233=Sparse%20autoencoders%20%28SAEs%29%20promise%20a%20unified%20approach%20for%20mechanistic%20interpretability%2C%20concept%20discovery%2C%20and%20model%20steering%20in%20LLMs%20and%20LVLMs.%20However%2C%20realizing%20this%20potential%20requires%20that%20the%20learned%20features%20be%20both%20interpretable%20and%20steerable.%20To%20that%20end%2C%20we%20introduce%20two%20new%20computationally%20inexpensive%20interpretability%20and%20steerability%20metrics%20and%20conduct%20a%20systematic%20analysis%20on%20LVLMs.%20Our%20analysis%20uncovers%20two%20observations%3B%20%28i%29%20a%20majority%20of%20SAE%20neurons%20exhibit%20either%20low%20interpretability%20or%20low%20steerability%20or%20both%2C%20rendering%20them%20ineffective%20for%20downstream%20use%3B%20and%20%28ii%29%20due%20to%20the%20unsupervised%20nature%20of%20SAEs%2C%20user-desired%20concepts%20are%20often%20absent%20in%20the%20learned%20dictionary%2C%20thus%20limiting%20their%20practical%20utility.%20To%20address%20these%20limitations%2C%20we%20propose%20Concept%20Bottleneck%20Sparse%20Autoencoders%20%28CB-SAE%29%20-%20a%20novel%20post-hoc%20framework%20that%20prunes%20low-utility%20neurons%20and%20augments%20the%20latent%20space%20with%20a%20lightweight%20concept%20bottleneck%20aligned%20to%20a%20user-defined%20concept%20set.%20The%20resulting%20CB-SAE%20improves%20interpretability%20by%20%2B32.1%25%20and%20steerability%20by%20%2B14.5%25%20across%20LVLMs%20and%20image%20generation%20tasks.%20We%20will%20make%20our%20code%20and%20model%20weights%20available.&entry.1838667208=http%3A//arxiv.org/abs/2512.10805v1&entry.124074799=Read"},
{"title": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data", "author": "Ivo Bueno and Ruikun Hou and Babette B\u00fchler and Tim F\u00fctterer and James Drimalla and Jonathan Kyle Foster and Peter Youngs and Peter Gerjets and Ulrich Trautwein and Enkelejda Kasneci", "abstract": "Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.", "link": "http://arxiv.org/abs/2512.00087v2", "date": "2025-12-11", "relevancy": 2.7569, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Automated%20Recognition%20of%20Instructional%20Activity%20and%20Discourse%20from%20Multimodal%20Classroom%20Data&body=Title%3A%20Exploring%20Automated%20Recognition%20of%20Instructional%20Activity%20and%20Discourse%20from%20Multimodal%20Classroom%20Data%0AAuthor%3A%20Ivo%20Bueno%20and%20Ruikun%20Hou%20and%20Babette%20B%C3%BChler%20and%20Tim%20F%C3%BCtterer%20and%20James%20Drimalla%20and%20Jonathan%20Kyle%20Foster%20and%20Peter%20Youngs%20and%20Peter%20Gerjets%20and%20Ulrich%20Trautwein%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20Observation%20of%20classroom%20interactions%20can%20provide%20concrete%20feedback%20to%20teachers%2C%20but%20current%20methods%20rely%20on%20manual%20annotation%2C%20which%20is%20resource-intensive%20and%20hard%20to%20scale.%20This%20work%20explores%20AI-driven%20analysis%20of%20classroom%20recordings%2C%20focusing%20on%20multimodal%20instructional%20activity%20and%20discourse%20recognition%20as%20a%20foundation%20for%20actionable%20feedback.%20Using%20a%20densely%20annotated%20dataset%20of%20164%20hours%20of%20video%20and%2068%20lesson%20transcripts%2C%20we%20design%20parallel%2C%20modality-specific%20pipelines.%20For%20video%2C%20we%20evaluate%20zero-shot%20multimodal%20LLMs%2C%20fine-tuned%20vision-language%20models%2C%20and%20self-supervised%20video%20transformers%20on%2024%20activity%20labels.%20For%20transcripts%2C%20we%20fine-tune%20a%20transformer-based%20classifier%20with%20contextualized%20inputs%20and%20compare%20it%20against%20prompting-based%20LLMs%20on%2019%20discourse%20labels.%20To%20handle%20class%20imbalance%20and%20multi-label%20complexity%2C%20we%20apply%20per-label%20thresholding%2C%20context%20windows%2C%20and%20imbalance-aware%20loss%20functions.%20The%20results%20show%20that%20fine-tuned%20models%20consistently%20outperform%20prompting-based%20approaches%2C%20achieving%20macro-F1%20scores%20of%200.577%20for%20video%20and%200.460%20for%20transcripts.%20These%20results%20demonstrate%20the%20feasibility%20of%20automated%20classroom%20analysis%20and%20establish%20a%20foundation%20for%20scalable%20teacher%20feedback%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00087v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Automated%2520Recognition%2520of%2520Instructional%2520Activity%2520and%2520Discourse%2520from%2520Multimodal%2520Classroom%2520Data%26entry.906535625%3DIvo%2520Bueno%2520and%2520Ruikun%2520Hou%2520and%2520Babette%2520B%25C3%25BChler%2520and%2520Tim%2520F%25C3%25BCtterer%2520and%2520James%2520Drimalla%2520and%2520Jonathan%2520Kyle%2520Foster%2520and%2520Peter%2520Youngs%2520and%2520Peter%2520Gerjets%2520and%2520Ulrich%2520Trautwein%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3DObservation%2520of%2520classroom%2520interactions%2520can%2520provide%2520concrete%2520feedback%2520to%2520teachers%252C%2520but%2520current%2520methods%2520rely%2520on%2520manual%2520annotation%252C%2520which%2520is%2520resource-intensive%2520and%2520hard%2520to%2520scale.%2520This%2520work%2520explores%2520AI-driven%2520analysis%2520of%2520classroom%2520recordings%252C%2520focusing%2520on%2520multimodal%2520instructional%2520activity%2520and%2520discourse%2520recognition%2520as%2520a%2520foundation%2520for%2520actionable%2520feedback.%2520Using%2520a%2520densely%2520annotated%2520dataset%2520of%2520164%2520hours%2520of%2520video%2520and%252068%2520lesson%2520transcripts%252C%2520we%2520design%2520parallel%252C%2520modality-specific%2520pipelines.%2520For%2520video%252C%2520we%2520evaluate%2520zero-shot%2520multimodal%2520LLMs%252C%2520fine-tuned%2520vision-language%2520models%252C%2520and%2520self-supervised%2520video%2520transformers%2520on%252024%2520activity%2520labels.%2520For%2520transcripts%252C%2520we%2520fine-tune%2520a%2520transformer-based%2520classifier%2520with%2520contextualized%2520inputs%2520and%2520compare%2520it%2520against%2520prompting-based%2520LLMs%2520on%252019%2520discourse%2520labels.%2520To%2520handle%2520class%2520imbalance%2520and%2520multi-label%2520complexity%252C%2520we%2520apply%2520per-label%2520thresholding%252C%2520context%2520windows%252C%2520and%2520imbalance-aware%2520loss%2520functions.%2520The%2520results%2520show%2520that%2520fine-tuned%2520models%2520consistently%2520outperform%2520prompting-based%2520approaches%252C%2520achieving%2520macro-F1%2520scores%2520of%25200.577%2520for%2520video%2520and%25200.460%2520for%2520transcripts.%2520These%2520results%2520demonstrate%2520the%2520feasibility%2520of%2520automated%2520classroom%2520analysis%2520and%2520establish%2520a%2520foundation%2520for%2520scalable%2520teacher%2520feedback%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00087v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Automated%20Recognition%20of%20Instructional%20Activity%20and%20Discourse%20from%20Multimodal%20Classroom%20Data&entry.906535625=Ivo%20Bueno%20and%20Ruikun%20Hou%20and%20Babette%20B%C3%BChler%20and%20Tim%20F%C3%BCtterer%20and%20James%20Drimalla%20and%20Jonathan%20Kyle%20Foster%20and%20Peter%20Youngs%20and%20Peter%20Gerjets%20and%20Ulrich%20Trautwein%20and%20Enkelejda%20Kasneci&entry.1292438233=Observation%20of%20classroom%20interactions%20can%20provide%20concrete%20feedback%20to%20teachers%2C%20but%20current%20methods%20rely%20on%20manual%20annotation%2C%20which%20is%20resource-intensive%20and%20hard%20to%20scale.%20This%20work%20explores%20AI-driven%20analysis%20of%20classroom%20recordings%2C%20focusing%20on%20multimodal%20instructional%20activity%20and%20discourse%20recognition%20as%20a%20foundation%20for%20actionable%20feedback.%20Using%20a%20densely%20annotated%20dataset%20of%20164%20hours%20of%20video%20and%2068%20lesson%20transcripts%2C%20we%20design%20parallel%2C%20modality-specific%20pipelines.%20For%20video%2C%20we%20evaluate%20zero-shot%20multimodal%20LLMs%2C%20fine-tuned%20vision-language%20models%2C%20and%20self-supervised%20video%20transformers%20on%2024%20activity%20labels.%20For%20transcripts%2C%20we%20fine-tune%20a%20transformer-based%20classifier%20with%20contextualized%20inputs%20and%20compare%20it%20against%20prompting-based%20LLMs%20on%2019%20discourse%20labels.%20To%20handle%20class%20imbalance%20and%20multi-label%20complexity%2C%20we%20apply%20per-label%20thresholding%2C%20context%20windows%2C%20and%20imbalance-aware%20loss%20functions.%20The%20results%20show%20that%20fine-tuned%20models%20consistently%20outperform%20prompting-based%20approaches%2C%20achieving%20macro-F1%20scores%20of%200.577%20for%20video%20and%200.460%20for%20transcripts.%20These%20results%20demonstrate%20the%20feasibility%20of%20automated%20classroom%20analysis%20and%20establish%20a%20foundation%20for%20scalable%20teacher%20feedback%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.00087v2&entry.124074799=Read"},
{"title": "Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data", "author": "Jessica Plassmann and Nicolas Schuler and Georg von Freymann and Michael Schuth", "abstract": "Shearography is a non-destructive testing method for detecting subsurface defects, offering high sensitivity and full-field inspection capabilities. However, its industrial adoption remains limited due to the need for expert interpretation. To reduce reliance on labeled data and manual evaluation, this study explores unsupervised learning methods for automated anomaly detection in shearographic images. Three architectures are evaluated: a fully connected autoencoder, a convolutional autoencoder, and a student-teacher feature matching model. All models are trained solely on defect-free data. A controlled dataset was developed using a custom specimen with reproducible defect patterns, enabling systematic acquisition of shearographic measurements under both ideal and realistic deformation conditions. Two training subsets were defined: one containing only undistorted, defect-free samples, and one additionally including globally deformed, yet defect-free, data. The latter simulates practical inspection conditions by incorporating deformation-induced fringe patterns that may obscure localized anomalies. The models are evaluated in terms of binary classification and, for the student-teacher model, spatial defect localization. Results show that the student-teacher approach achieves superior classification robustness and enables precise localization. Compared to the autoencoder-based models, it demonstrates improved separability of feature representations, as visualized through t-SNE embeddings. Additionally, a YOLOv8 model trained on labeled defect data serves as a reference to benchmark localization quality. This study underscores the potential of unsupervised deep learning for scalable, label-efficient shearographic inspection in industrial environments.", "link": "http://arxiv.org/abs/2511.02541v2", "date": "2025-12-11", "relevancy": 2.746, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5941}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5355}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Learning%20for%20Industrial%20Defect%20Detection%3A%20A%20Case%20Study%20on%20Shearographic%20Data&body=Title%3A%20Unsupervised%20Learning%20for%20Industrial%20Defect%20Detection%3A%20A%20Case%20Study%20on%20Shearographic%20Data%0AAuthor%3A%20Jessica%20Plassmann%20and%20Nicolas%20Schuler%20and%20Georg%20von%20Freymann%20and%20Michael%20Schuth%0AAbstract%3A%20Shearography%20is%20a%20non-destructive%20testing%20method%20for%20detecting%20subsurface%20defects%2C%20offering%20high%20sensitivity%20and%20full-field%20inspection%20capabilities.%20However%2C%20its%20industrial%20adoption%20remains%20limited%20due%20to%20the%20need%20for%20expert%20interpretation.%20To%20reduce%20reliance%20on%20labeled%20data%20and%20manual%20evaluation%2C%20this%20study%20explores%20unsupervised%20learning%20methods%20for%20automated%20anomaly%20detection%20in%20shearographic%20images.%20Three%20architectures%20are%20evaluated%3A%20a%20fully%20connected%20autoencoder%2C%20a%20convolutional%20autoencoder%2C%20and%20a%20student-teacher%20feature%20matching%20model.%20All%20models%20are%20trained%20solely%20on%20defect-free%20data.%20A%20controlled%20dataset%20was%20developed%20using%20a%20custom%20specimen%20with%20reproducible%20defect%20patterns%2C%20enabling%20systematic%20acquisition%20of%20shearographic%20measurements%20under%20both%20ideal%20and%20realistic%20deformation%20conditions.%20Two%20training%20subsets%20were%20defined%3A%20one%20containing%20only%20undistorted%2C%20defect-free%20samples%2C%20and%20one%20additionally%20including%20globally%20deformed%2C%20yet%20defect-free%2C%20data.%20The%20latter%20simulates%20practical%20inspection%20conditions%20by%20incorporating%20deformation-induced%20fringe%20patterns%20that%20may%20obscure%20localized%20anomalies.%20The%20models%20are%20evaluated%20in%20terms%20of%20binary%20classification%20and%2C%20for%20the%20student-teacher%20model%2C%20spatial%20defect%20localization.%20Results%20show%20that%20the%20student-teacher%20approach%20achieves%20superior%20classification%20robustness%20and%20enables%20precise%20localization.%20Compared%20to%20the%20autoencoder-based%20models%2C%20it%20demonstrates%20improved%20separability%20of%20feature%20representations%2C%20as%20visualized%20through%20t-SNE%20embeddings.%20Additionally%2C%20a%20YOLOv8%20model%20trained%20on%20labeled%20defect%20data%20serves%20as%20a%20reference%20to%20benchmark%20localization%20quality.%20This%20study%20underscores%20the%20potential%20of%20unsupervised%20deep%20learning%20for%20scalable%2C%20label-efficient%20shearographic%20inspection%20in%20industrial%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.02541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Learning%2520for%2520Industrial%2520Defect%2520Detection%253A%2520A%2520Case%2520Study%2520on%2520Shearographic%2520Data%26entry.906535625%3DJessica%2520Plassmann%2520and%2520Nicolas%2520Schuler%2520and%2520Georg%2520von%2520Freymann%2520and%2520Michael%2520Schuth%26entry.1292438233%3DShearography%2520is%2520a%2520non-destructive%2520testing%2520method%2520for%2520detecting%2520subsurface%2520defects%252C%2520offering%2520high%2520sensitivity%2520and%2520full-field%2520inspection%2520capabilities.%2520However%252C%2520its%2520industrial%2520adoption%2520remains%2520limited%2520due%2520to%2520the%2520need%2520for%2520expert%2520interpretation.%2520To%2520reduce%2520reliance%2520on%2520labeled%2520data%2520and%2520manual%2520evaluation%252C%2520this%2520study%2520explores%2520unsupervised%2520learning%2520methods%2520for%2520automated%2520anomaly%2520detection%2520in%2520shearographic%2520images.%2520Three%2520architectures%2520are%2520evaluated%253A%2520a%2520fully%2520connected%2520autoencoder%252C%2520a%2520convolutional%2520autoencoder%252C%2520and%2520a%2520student-teacher%2520feature%2520matching%2520model.%2520All%2520models%2520are%2520trained%2520solely%2520on%2520defect-free%2520data.%2520A%2520controlled%2520dataset%2520was%2520developed%2520using%2520a%2520custom%2520specimen%2520with%2520reproducible%2520defect%2520patterns%252C%2520enabling%2520systematic%2520acquisition%2520of%2520shearographic%2520measurements%2520under%2520both%2520ideal%2520and%2520realistic%2520deformation%2520conditions.%2520Two%2520training%2520subsets%2520were%2520defined%253A%2520one%2520containing%2520only%2520undistorted%252C%2520defect-free%2520samples%252C%2520and%2520one%2520additionally%2520including%2520globally%2520deformed%252C%2520yet%2520defect-free%252C%2520data.%2520The%2520latter%2520simulates%2520practical%2520inspection%2520conditions%2520by%2520incorporating%2520deformation-induced%2520fringe%2520patterns%2520that%2520may%2520obscure%2520localized%2520anomalies.%2520The%2520models%2520are%2520evaluated%2520in%2520terms%2520of%2520binary%2520classification%2520and%252C%2520for%2520the%2520student-teacher%2520model%252C%2520spatial%2520defect%2520localization.%2520Results%2520show%2520that%2520the%2520student-teacher%2520approach%2520achieves%2520superior%2520classification%2520robustness%2520and%2520enables%2520precise%2520localization.%2520Compared%2520to%2520the%2520autoencoder-based%2520models%252C%2520it%2520demonstrates%2520improved%2520separability%2520of%2520feature%2520representations%252C%2520as%2520visualized%2520through%2520t-SNE%2520embeddings.%2520Additionally%252C%2520a%2520YOLOv8%2520model%2520trained%2520on%2520labeled%2520defect%2520data%2520serves%2520as%2520a%2520reference%2520to%2520benchmark%2520localization%2520quality.%2520This%2520study%2520underscores%2520the%2520potential%2520of%2520unsupervised%2520deep%2520learning%2520for%2520scalable%252C%2520label-efficient%2520shearographic%2520inspection%2520in%2520industrial%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Learning%20for%20Industrial%20Defect%20Detection%3A%20A%20Case%20Study%20on%20Shearographic%20Data&entry.906535625=Jessica%20Plassmann%20and%20Nicolas%20Schuler%20and%20Georg%20von%20Freymann%20and%20Michael%20Schuth&entry.1292438233=Shearography%20is%20a%20non-destructive%20testing%20method%20for%20detecting%20subsurface%20defects%2C%20offering%20high%20sensitivity%20and%20full-field%20inspection%20capabilities.%20However%2C%20its%20industrial%20adoption%20remains%20limited%20due%20to%20the%20need%20for%20expert%20interpretation.%20To%20reduce%20reliance%20on%20labeled%20data%20and%20manual%20evaluation%2C%20this%20study%20explores%20unsupervised%20learning%20methods%20for%20automated%20anomaly%20detection%20in%20shearographic%20images.%20Three%20architectures%20are%20evaluated%3A%20a%20fully%20connected%20autoencoder%2C%20a%20convolutional%20autoencoder%2C%20and%20a%20student-teacher%20feature%20matching%20model.%20All%20models%20are%20trained%20solely%20on%20defect-free%20data.%20A%20controlled%20dataset%20was%20developed%20using%20a%20custom%20specimen%20with%20reproducible%20defect%20patterns%2C%20enabling%20systematic%20acquisition%20of%20shearographic%20measurements%20under%20both%20ideal%20and%20realistic%20deformation%20conditions.%20Two%20training%20subsets%20were%20defined%3A%20one%20containing%20only%20undistorted%2C%20defect-free%20samples%2C%20and%20one%20additionally%20including%20globally%20deformed%2C%20yet%20defect-free%2C%20data.%20The%20latter%20simulates%20practical%20inspection%20conditions%20by%20incorporating%20deformation-induced%20fringe%20patterns%20that%20may%20obscure%20localized%20anomalies.%20The%20models%20are%20evaluated%20in%20terms%20of%20binary%20classification%20and%2C%20for%20the%20student-teacher%20model%2C%20spatial%20defect%20localization.%20Results%20show%20that%20the%20student-teacher%20approach%20achieves%20superior%20classification%20robustness%20and%20enables%20precise%20localization.%20Compared%20to%20the%20autoencoder-based%20models%2C%20it%20demonstrates%20improved%20separability%20of%20feature%20representations%2C%20as%20visualized%20through%20t-SNE%20embeddings.%20Additionally%2C%20a%20YOLOv8%20model%20trained%20on%20labeled%20defect%20data%20serves%20as%20a%20reference%20to%20benchmark%20localization%20quality.%20This%20study%20underscores%20the%20potential%20of%20unsupervised%20deep%20learning%20for%20scalable%2C%20label-efficient%20shearographic%20inspection%20in%20industrial%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.02541v2&entry.124074799=Read"},
{"title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat", "author": "Qi Lin and Weikai Xu and Lisi Chen and Bin Dai", "abstract": "With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.", "link": "http://arxiv.org/abs/2506.01524v2", "date": "2025-12-11", "relevancy": 2.7196, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5699}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-VAE%3A%20A%20Variational%20Auto%20Encoding%20Framework%20Towards%20Fine-Grained%20Control%20over%20Human-Like%20Chat&body=Title%3A%20V-VAE%3A%20A%20Variational%20Auto%20Encoding%20Framework%20Towards%20Fine-Grained%20Control%20over%20Human-Like%20Chat%0AAuthor%3A%20Qi%20Lin%20and%20Weikai%20Xu%20and%20Lisi%20Chen%20and%20Bin%20Dai%0AAbstract%3A%20With%20the%20continued%20proliferation%20of%20Large%20Language%20Model%20%28LLM%29%20based%20chatbots%2C%20there%20is%20a%20growing%20demand%20for%20generating%20responses%20that%20are%20not%20only%20linguistically%20fluent%20but%20also%20consistently%20aligned%20with%20persona-specific%20traits%20in%20conversations.%20However%2C%20existing%20role-play%20and%20persona-based%20chat%20approaches%20rely%20heavily%20on%20static%20role%20descriptions%2C%20coarse-grained%20signal%20space%2C%20and%20low-quality%20synthetic%20data%2C%20which%20fail%20to%20capture%20dynamic%20fine-grained%20details%20in%20human-like%20chat.%20Human-like%20chat%20requires%20modeling%20subtle%20latent%20traits%2C%20such%20as%20emotional%20tone%2C%20situational%20awareness%2C%20and%20evolving%20personality%2C%20which%20are%20difficult%20to%20predefine%20and%20cannot%20be%20easily%20learned%20from%20synthetic%20or%20distillation-based%20data.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20Verbal%20Variational%20Auto-Encoding%20%28V-VAE%29%20framework%2C%20containing%20a%20variational%20auto-encoding%20module%20and%20fine-grained%20control%20space%20which%20dynamically%20adapts%20dialogue%20behaviour%20based%20on%20fine-grained%2C%20interpretable%20latent%20variables%20across%20talking%20style%2C%20interaction%20patterns%2C%20and%20personal%20attributes.%20We%20also%20construct%20a%20high-quality%20dataset%2C%20HumanChatData%2C%20and%20benchmark%20HumanChatBench%20to%20address%20the%20scarcity%20of%20high-quality%20data%20in%20the%20human-like%20domain.%20Experiments%20show%20that%20LLMs%20based%20on%20V-VAE%20consistently%20outperform%20standard%20baselines%20on%20HumanChatBench%20and%20DialogBench%2C%20which%20further%20demonstrates%20the%20effectiveness%20of%20V-VAE%20and%20HumanChatData.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01524v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-VAE%253A%2520A%2520Variational%2520Auto%2520Encoding%2520Framework%2520Towards%2520Fine-Grained%2520Control%2520over%2520Human-Like%2520Chat%26entry.906535625%3DQi%2520Lin%2520and%2520Weikai%2520Xu%2520and%2520Lisi%2520Chen%2520and%2520Bin%2520Dai%26entry.1292438233%3DWith%2520the%2520continued%2520proliferation%2520of%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520chatbots%252C%2520there%2520is%2520a%2520growing%2520demand%2520for%2520generating%2520responses%2520that%2520are%2520not%2520only%2520linguistically%2520fluent%2520but%2520also%2520consistently%2520aligned%2520with%2520persona-specific%2520traits%2520in%2520conversations.%2520However%252C%2520existing%2520role-play%2520and%2520persona-based%2520chat%2520approaches%2520rely%2520heavily%2520on%2520static%2520role%2520descriptions%252C%2520coarse-grained%2520signal%2520space%252C%2520and%2520low-quality%2520synthetic%2520data%252C%2520which%2520fail%2520to%2520capture%2520dynamic%2520fine-grained%2520details%2520in%2520human-like%2520chat.%2520Human-like%2520chat%2520requires%2520modeling%2520subtle%2520latent%2520traits%252C%2520such%2520as%2520emotional%2520tone%252C%2520situational%2520awareness%252C%2520and%2520evolving%2520personality%252C%2520which%2520are%2520difficult%2520to%2520predefine%2520and%2520cannot%2520be%2520easily%2520learned%2520from%2520synthetic%2520or%2520distillation-based%2520data.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520Verbal%2520Variational%2520Auto-Encoding%2520%2528V-VAE%2529%2520framework%252C%2520containing%2520a%2520variational%2520auto-encoding%2520module%2520and%2520fine-grained%2520control%2520space%2520which%2520dynamically%2520adapts%2520dialogue%2520behaviour%2520based%2520on%2520fine-grained%252C%2520interpretable%2520latent%2520variables%2520across%2520talking%2520style%252C%2520interaction%2520patterns%252C%2520and%2520personal%2520attributes.%2520We%2520also%2520construct%2520a%2520high-quality%2520dataset%252C%2520HumanChatData%252C%2520and%2520benchmark%2520HumanChatBench%2520to%2520address%2520the%2520scarcity%2520of%2520high-quality%2520data%2520in%2520the%2520human-like%2520domain.%2520Experiments%2520show%2520that%2520LLMs%2520based%2520on%2520V-VAE%2520consistently%2520outperform%2520standard%2520baselines%2520on%2520HumanChatBench%2520and%2520DialogBench%252C%2520which%2520further%2520demonstrates%2520the%2520effectiveness%2520of%2520V-VAE%2520and%2520HumanChatData.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01524v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-VAE%3A%20A%20Variational%20Auto%20Encoding%20Framework%20Towards%20Fine-Grained%20Control%20over%20Human-Like%20Chat&entry.906535625=Qi%20Lin%20and%20Weikai%20Xu%20and%20Lisi%20Chen%20and%20Bin%20Dai&entry.1292438233=With%20the%20continued%20proliferation%20of%20Large%20Language%20Model%20%28LLM%29%20based%20chatbots%2C%20there%20is%20a%20growing%20demand%20for%20generating%20responses%20that%20are%20not%20only%20linguistically%20fluent%20but%20also%20consistently%20aligned%20with%20persona-specific%20traits%20in%20conversations.%20However%2C%20existing%20role-play%20and%20persona-based%20chat%20approaches%20rely%20heavily%20on%20static%20role%20descriptions%2C%20coarse-grained%20signal%20space%2C%20and%20low-quality%20synthetic%20data%2C%20which%20fail%20to%20capture%20dynamic%20fine-grained%20details%20in%20human-like%20chat.%20Human-like%20chat%20requires%20modeling%20subtle%20latent%20traits%2C%20such%20as%20emotional%20tone%2C%20situational%20awareness%2C%20and%20evolving%20personality%2C%20which%20are%20difficult%20to%20predefine%20and%20cannot%20be%20easily%20learned%20from%20synthetic%20or%20distillation-based%20data.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20Verbal%20Variational%20Auto-Encoding%20%28V-VAE%29%20framework%2C%20containing%20a%20variational%20auto-encoding%20module%20and%20fine-grained%20control%20space%20which%20dynamically%20adapts%20dialogue%20behaviour%20based%20on%20fine-grained%2C%20interpretable%20latent%20variables%20across%20talking%20style%2C%20interaction%20patterns%2C%20and%20personal%20attributes.%20We%20also%20construct%20a%20high-quality%20dataset%2C%20HumanChatData%2C%20and%20benchmark%20HumanChatBench%20to%20address%20the%20scarcity%20of%20high-quality%20data%20in%20the%20human-like%20domain.%20Experiments%20show%20that%20LLMs%20based%20on%20V-VAE%20consistently%20outperform%20standard%20baselines%20on%20HumanChatBench%20and%20DialogBench%2C%20which%20further%20demonstrates%20the%20effectiveness%20of%20V-VAE%20and%20HumanChatData.&entry.1838667208=http%3A//arxiv.org/abs/2506.01524v2&entry.124074799=Read"},
{"title": "Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval", "author": "J. Xiao and Y. Guo and X. Zi and K. Thiyagarajan and C. Moreira and M. Prasad", "abstract": "Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \\textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\\%, nearly doubling the 23.86\\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.", "link": "http://arxiv.org/abs/2512.10596v1", "date": "2025-12-11", "relevancy": 2.7026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Pixels%3A%20A%20Training-Free%2C%20Text-to-Text%20Framework%20for%20Remote%20Sensing%20Image%20Retrieval&body=Title%3A%20Beyond%20Pixels%3A%20A%20Training-Free%2C%20Text-to-Text%20Framework%20for%20Remote%20Sensing%20Image%20Retrieval%0AAuthor%3A%20J.%20Xiao%20and%20Y.%20Guo%20and%20X.%20Zi%20and%20K.%20Thiyagarajan%20and%20C.%20Moreira%20and%20M.%20Prasad%0AAbstract%3A%20Semantic%20retrieval%20of%20remote%20sensing%20%28RS%29%20images%20is%20a%20critical%20task%20fundamentally%20challenged%20by%20the%20%5Ctextquote%7Bsemantic%20gap%7D%2C%20the%20discrepancy%20between%20a%20model%27s%20low-level%20visual%20features%20and%20high-level%20human%20concepts.%20While%20large%20Vision-Language%20Models%20%28VLMs%29%20offer%20a%20promising%20path%20to%20bridge%20this%20gap%2C%20existing%20methods%20often%20rely%20on%20costly%2C%20domain-specific%20training%2C%20and%20there%20is%20a%20lack%20of%20benchmarks%20to%20evaluate%20the%20practical%20utility%20of%20VLM-generated%20text%20in%20a%20zero-shot%20retrieval%20context.%20To%20address%20this%20research%20gap%2C%20we%20introduce%20the%20Remote%20Sensing%20Rich%20Text%20%28RSRT%29%20dataset%2C%20a%20new%20benchmark%20featuring%20multiple%20structured%20captions%20per%20image.%20Based%20on%20this%20dataset%2C%20we%20propose%20a%20fully%20training-free%2C%20text-only%20retrieval%20reference%20called%20TRSLLaVA.%20Our%20methodology%20reformulates%20cross-modal%20retrieval%20as%20a%20text-to-text%20%28T2T%29%20matching%20problem%2C%20leveraging%20rich%20text%20descriptions%20as%20queries%20against%20a%20database%20of%20VLM-generated%20captions%20within%20a%20unified%20textual%20embedding%20space.%20This%20approach%20completely%20bypasses%20model%20training%20or%20fine-tuning.%20Experiments%20on%20the%20RSITMD%20and%20RSICD%20benchmarks%20show%20our%20training-free%20method%20is%20highly%20competitive%20with%20state-of-the-art%20supervised%20models.%20For%20instance%2C%20on%20RSITMD%2C%20our%20method%20achieves%20a%20mean%20Recall%20of%2042.62%5C%25%2C%20nearly%20doubling%20the%2023.86%5C%25%20of%20the%20standard%20zero-shot%20CLIP%20baseline%20and%20surpassing%20several%20top%20supervised%20models.%20This%20validates%20that%20high-quality%20semantic%20representation%20through%20structured%20text%20provides%20a%20powerful%20and%20cost-effective%20paradigm%20for%20remote%20sensing%20image%20retrieval.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Pixels%253A%2520A%2520Training-Free%252C%2520Text-to-Text%2520Framework%2520for%2520Remote%2520Sensing%2520Image%2520Retrieval%26entry.906535625%3DJ.%2520Xiao%2520and%2520Y.%2520Guo%2520and%2520X.%2520Zi%2520and%2520K.%2520Thiyagarajan%2520and%2520C.%2520Moreira%2520and%2520M.%2520Prasad%26entry.1292438233%3DSemantic%2520retrieval%2520of%2520remote%2520sensing%2520%2528RS%2529%2520images%2520is%2520a%2520critical%2520task%2520fundamentally%2520challenged%2520by%2520the%2520%255Ctextquote%257Bsemantic%2520gap%257D%252C%2520the%2520discrepancy%2520between%2520a%2520model%2527s%2520low-level%2520visual%2520features%2520and%2520high-level%2520human%2520concepts.%2520While%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520offer%2520a%2520promising%2520path%2520to%2520bridge%2520this%2520gap%252C%2520existing%2520methods%2520often%2520rely%2520on%2520costly%252C%2520domain-specific%2520training%252C%2520and%2520there%2520is%2520a%2520lack%2520of%2520benchmarks%2520to%2520evaluate%2520the%2520practical%2520utility%2520of%2520VLM-generated%2520text%2520in%2520a%2520zero-shot%2520retrieval%2520context.%2520To%2520address%2520this%2520research%2520gap%252C%2520we%2520introduce%2520the%2520Remote%2520Sensing%2520Rich%2520Text%2520%2528RSRT%2529%2520dataset%252C%2520a%2520new%2520benchmark%2520featuring%2520multiple%2520structured%2520captions%2520per%2520image.%2520Based%2520on%2520this%2520dataset%252C%2520we%2520propose%2520a%2520fully%2520training-free%252C%2520text-only%2520retrieval%2520reference%2520called%2520TRSLLaVA.%2520Our%2520methodology%2520reformulates%2520cross-modal%2520retrieval%2520as%2520a%2520text-to-text%2520%2528T2T%2529%2520matching%2520problem%252C%2520leveraging%2520rich%2520text%2520descriptions%2520as%2520queries%2520against%2520a%2520database%2520of%2520VLM-generated%2520captions%2520within%2520a%2520unified%2520textual%2520embedding%2520space.%2520This%2520approach%2520completely%2520bypasses%2520model%2520training%2520or%2520fine-tuning.%2520Experiments%2520on%2520the%2520RSITMD%2520and%2520RSICD%2520benchmarks%2520show%2520our%2520training-free%2520method%2520is%2520highly%2520competitive%2520with%2520state-of-the-art%2520supervised%2520models.%2520For%2520instance%252C%2520on%2520RSITMD%252C%2520our%2520method%2520achieves%2520a%2520mean%2520Recall%2520of%252042.62%255C%2525%252C%2520nearly%2520doubling%2520the%252023.86%255C%2525%2520of%2520the%2520standard%2520zero-shot%2520CLIP%2520baseline%2520and%2520surpassing%2520several%2520top%2520supervised%2520models.%2520This%2520validates%2520that%2520high-quality%2520semantic%2520representation%2520through%2520structured%2520text%2520provides%2520a%2520powerful%2520and%2520cost-effective%2520paradigm%2520for%2520remote%2520sensing%2520image%2520retrieval.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Pixels%3A%20A%20Training-Free%2C%20Text-to-Text%20Framework%20for%20Remote%20Sensing%20Image%20Retrieval&entry.906535625=J.%20Xiao%20and%20Y.%20Guo%20and%20X.%20Zi%20and%20K.%20Thiyagarajan%20and%20C.%20Moreira%20and%20M.%20Prasad&entry.1292438233=Semantic%20retrieval%20of%20remote%20sensing%20%28RS%29%20images%20is%20a%20critical%20task%20fundamentally%20challenged%20by%20the%20%5Ctextquote%7Bsemantic%20gap%7D%2C%20the%20discrepancy%20between%20a%20model%27s%20low-level%20visual%20features%20and%20high-level%20human%20concepts.%20While%20large%20Vision-Language%20Models%20%28VLMs%29%20offer%20a%20promising%20path%20to%20bridge%20this%20gap%2C%20existing%20methods%20often%20rely%20on%20costly%2C%20domain-specific%20training%2C%20and%20there%20is%20a%20lack%20of%20benchmarks%20to%20evaluate%20the%20practical%20utility%20of%20VLM-generated%20text%20in%20a%20zero-shot%20retrieval%20context.%20To%20address%20this%20research%20gap%2C%20we%20introduce%20the%20Remote%20Sensing%20Rich%20Text%20%28RSRT%29%20dataset%2C%20a%20new%20benchmark%20featuring%20multiple%20structured%20captions%20per%20image.%20Based%20on%20this%20dataset%2C%20we%20propose%20a%20fully%20training-free%2C%20text-only%20retrieval%20reference%20called%20TRSLLaVA.%20Our%20methodology%20reformulates%20cross-modal%20retrieval%20as%20a%20text-to-text%20%28T2T%29%20matching%20problem%2C%20leveraging%20rich%20text%20descriptions%20as%20queries%20against%20a%20database%20of%20VLM-generated%20captions%20within%20a%20unified%20textual%20embedding%20space.%20This%20approach%20completely%20bypasses%20model%20training%20or%20fine-tuning.%20Experiments%20on%20the%20RSITMD%20and%20RSICD%20benchmarks%20show%20our%20training-free%20method%20is%20highly%20competitive%20with%20state-of-the-art%20supervised%20models.%20For%20instance%2C%20on%20RSITMD%2C%20our%20method%20achieves%20a%20mean%20Recall%20of%2042.62%5C%25%2C%20nearly%20doubling%20the%2023.86%5C%25%20of%20the%20standard%20zero-shot%20CLIP%20baseline%20and%20surpassing%20several%20top%20supervised%20models.%20This%20validates%20that%20high-quality%20semantic%20representation%20through%20structured%20text%20provides%20a%20powerful%20and%20cost-effective%20paradigm%20for%20remote%20sensing%20image%20retrieval.&entry.1838667208=http%3A//arxiv.org/abs/2512.10596v1&entry.124074799=Read"},
{"title": "Grounding Everything in Tokens for Multimodal Large Language Models", "author": "Xiangxuan Ren and Zhongdao Wang and Liping Hou and Pin Tang and Guoqing Wang and Chao Ma", "abstract": "Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.", "link": "http://arxiv.org/abs/2512.10554v1", "date": "2025-12-11", "relevancy": 2.6931, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5542}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Everything%20in%20Tokens%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Grounding%20Everything%20in%20Tokens%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Xiangxuan%20Ren%20and%20Zhongdao%20Wang%20and%20Liping%20Hou%20and%20Pin%20Tang%20and%20Guoqing%20Wang%20and%20Chao%20Ma%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%20advancements%20in%20vision%20understanding%20and%20reasoning.%20However%2C%20the%20autoregressive%20Transformer%20architecture%20used%20by%20MLLMs%20requries%20tokenization%20on%20input%20images%2C%20which%20limits%20their%20ability%20to%20accurately%20ground%20objects%20within%20the%202D%20image%20space.%20This%20raises%20an%20important%20question%3A%20how%20can%20sequential%20language%20tokens%20be%20improved%20to%20better%20ground%20objects%20in%202D%20spatial%20space%20for%20MLLMs%3F%20To%20address%20this%2C%20we%20present%20a%20spatial%20representation%20method%20for%20grounding%20objects%2C%20namely%20GETok%2C%20that%20integrates%20a%20specialized%20vocabulary%20of%20learnable%20tokens%20into%20MLLMs.%20GETok%20first%20uses%20grid%20tokens%20to%20partition%20the%20image%20plane%20into%20structured%20spatial%20anchors%2C%20and%20then%20exploits%20offset%20tokens%20to%20enable%20precise%20and%20iterative%20refinement%20of%20localization%20predictions.%20By%20embedding%20spatial%20relationships%20directly%20into%20tokens%2C%20GETok%20significantly%20advances%20MLLMs%20in%20native%202D%20space%20reasoning%20without%20modifying%20the%20autoregressive%20architecture.%20Extensive%20experiments%20demonstrate%20that%20GETok%20achieves%20superior%20performance%20over%20the%20state-of-the-art%20methods%20across%20various%20referring%20tasks%20in%20both%20supervised%20fine-tuning%20and%20reinforcement%20learning%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Everything%2520in%2520Tokens%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DXiangxuan%2520Ren%2520and%2520Zhongdao%2520Wang%2520and%2520Liping%2520Hou%2520and%2520Pin%2520Tang%2520and%2520Guoqing%2520Wang%2520and%2520Chao%2520Ma%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520made%2520significant%2520advancements%2520in%2520vision%2520understanding%2520and%2520reasoning.%2520However%252C%2520the%2520autoregressive%2520Transformer%2520architecture%2520used%2520by%2520MLLMs%2520requries%2520tokenization%2520on%2520input%2520images%252C%2520which%2520limits%2520their%2520ability%2520to%2520accurately%2520ground%2520objects%2520within%2520the%25202D%2520image%2520space.%2520This%2520raises%2520an%2520important%2520question%253A%2520how%2520can%2520sequential%2520language%2520tokens%2520be%2520improved%2520to%2520better%2520ground%2520objects%2520in%25202D%2520spatial%2520space%2520for%2520MLLMs%253F%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520spatial%2520representation%2520method%2520for%2520grounding%2520objects%252C%2520namely%2520GETok%252C%2520that%2520integrates%2520a%2520specialized%2520vocabulary%2520of%2520learnable%2520tokens%2520into%2520MLLMs.%2520GETok%2520first%2520uses%2520grid%2520tokens%2520to%2520partition%2520the%2520image%2520plane%2520into%2520structured%2520spatial%2520anchors%252C%2520and%2520then%2520exploits%2520offset%2520tokens%2520to%2520enable%2520precise%2520and%2520iterative%2520refinement%2520of%2520localization%2520predictions.%2520By%2520embedding%2520spatial%2520relationships%2520directly%2520into%2520tokens%252C%2520GETok%2520significantly%2520advances%2520MLLMs%2520in%2520native%25202D%2520space%2520reasoning%2520without%2520modifying%2520the%2520autoregressive%2520architecture.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GETok%2520achieves%2520superior%2520performance%2520over%2520the%2520state-of-the-art%2520methods%2520across%2520various%2520referring%2520tasks%2520in%2520both%2520supervised%2520fine-tuning%2520and%2520reinforcement%2520learning%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Everything%20in%20Tokens%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Xiangxuan%20Ren%20and%20Zhongdao%20Wang%20and%20Liping%20Hou%20and%20Pin%20Tang%20and%20Guoqing%20Wang%20and%20Chao%20Ma&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%20advancements%20in%20vision%20understanding%20and%20reasoning.%20However%2C%20the%20autoregressive%20Transformer%20architecture%20used%20by%20MLLMs%20requries%20tokenization%20on%20input%20images%2C%20which%20limits%20their%20ability%20to%20accurately%20ground%20objects%20within%20the%202D%20image%20space.%20This%20raises%20an%20important%20question%3A%20how%20can%20sequential%20language%20tokens%20be%20improved%20to%20better%20ground%20objects%20in%202D%20spatial%20space%20for%20MLLMs%3F%20To%20address%20this%2C%20we%20present%20a%20spatial%20representation%20method%20for%20grounding%20objects%2C%20namely%20GETok%2C%20that%20integrates%20a%20specialized%20vocabulary%20of%20learnable%20tokens%20into%20MLLMs.%20GETok%20first%20uses%20grid%20tokens%20to%20partition%20the%20image%20plane%20into%20structured%20spatial%20anchors%2C%20and%20then%20exploits%20offset%20tokens%20to%20enable%20precise%20and%20iterative%20refinement%20of%20localization%20predictions.%20By%20embedding%20spatial%20relationships%20directly%20into%20tokens%2C%20GETok%20significantly%20advances%20MLLMs%20in%20native%202D%20space%20reasoning%20without%20modifying%20the%20autoregressive%20architecture.%20Extensive%20experiments%20demonstrate%20that%20GETok%20achieves%20superior%20performance%20over%20the%20state-of-the-art%20methods%20across%20various%20referring%20tasks%20in%20both%20supervised%20fine-tuning%20and%20reinforcement%20learning%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.10554v1&entry.124074799=Read"},
{"title": "THeGAU: Type-Aware Heterogeneous Graph Autoencoder and Augmentation", "author": "Ming-Yi Hong and Miao-Chen Chiang and Youchen Teng and Yu-Hsiang Wang and Chih-Yu Wang and Che Lin", "abstract": "Heterogeneous Graph Neural Networks (HGNNs) are effective for modeling Heterogeneous Information Networks (HINs), which encode complex multi-typed entities and relations. However, HGNNs often suffer from type information loss and structural noise, limiting their representational fidelity and generalization. We propose THeGAU, a model-agnostic framework that combines a type-aware graph autoencoder with guided graph augmentation to improve node classification. THeGAU reconstructs schema-valid edges as an auxiliary task to preserve node-type semantics and introduces a decoder-driven augmentation mechanism to selectively refine noisy structures. This joint design enhances robustness, accuracy, and efficiency while significantly reducing computational overhead. Extensive experiments on three benchmark HIN datasets (IMDB, ACM, and DBLP) demonstrate that THeGAU consistently outperforms existing HGNN methods, achieving state-of-the-art performance across multiple backbones.", "link": "http://arxiv.org/abs/2512.10589v1", "date": "2025-12-11", "relevancy": 2.6864, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5691}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5225}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20THeGAU%3A%20Type-Aware%20Heterogeneous%20Graph%20Autoencoder%20and%20Augmentation&body=Title%3A%20THeGAU%3A%20Type-Aware%20Heterogeneous%20Graph%20Autoencoder%20and%20Augmentation%0AAuthor%3A%20Ming-Yi%20Hong%20and%20Miao-Chen%20Chiang%20and%20Youchen%20Teng%20and%20Yu-Hsiang%20Wang%20and%20Chih-Yu%20Wang%20and%20Che%20Lin%0AAbstract%3A%20Heterogeneous%20Graph%20Neural%20Networks%20%28HGNNs%29%20are%20effective%20for%20modeling%20Heterogeneous%20Information%20Networks%20%28HINs%29%2C%20which%20encode%20complex%20multi-typed%20entities%20and%20relations.%20However%2C%20HGNNs%20often%20suffer%20from%20type%20information%20loss%20and%20structural%20noise%2C%20limiting%20their%20representational%20fidelity%20and%20generalization.%20We%20propose%20THeGAU%2C%20a%20model-agnostic%20framework%20that%20combines%20a%20type-aware%20graph%20autoencoder%20with%20guided%20graph%20augmentation%20to%20improve%20node%20classification.%20THeGAU%20reconstructs%20schema-valid%20edges%20as%20an%20auxiliary%20task%20to%20preserve%20node-type%20semantics%20and%20introduces%20a%20decoder-driven%20augmentation%20mechanism%20to%20selectively%20refine%20noisy%20structures.%20This%20joint%20design%20enhances%20robustness%2C%20accuracy%2C%20and%20efficiency%20while%20significantly%20reducing%20computational%20overhead.%20Extensive%20experiments%20on%20three%20benchmark%20HIN%20datasets%20%28IMDB%2C%20ACM%2C%20and%20DBLP%29%20demonstrate%20that%20THeGAU%20consistently%20outperforms%20existing%20HGNN%20methods%2C%20achieving%20state-of-the-art%20performance%20across%20multiple%20backbones.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTHeGAU%253A%2520Type-Aware%2520Heterogeneous%2520Graph%2520Autoencoder%2520and%2520Augmentation%26entry.906535625%3DMing-Yi%2520Hong%2520and%2520Miao-Chen%2520Chiang%2520and%2520Youchen%2520Teng%2520and%2520Yu-Hsiang%2520Wang%2520and%2520Chih-Yu%2520Wang%2520and%2520Che%2520Lin%26entry.1292438233%3DHeterogeneous%2520Graph%2520Neural%2520Networks%2520%2528HGNNs%2529%2520are%2520effective%2520for%2520modeling%2520Heterogeneous%2520Information%2520Networks%2520%2528HINs%2529%252C%2520which%2520encode%2520complex%2520multi-typed%2520entities%2520and%2520relations.%2520However%252C%2520HGNNs%2520often%2520suffer%2520from%2520type%2520information%2520loss%2520and%2520structural%2520noise%252C%2520limiting%2520their%2520representational%2520fidelity%2520and%2520generalization.%2520We%2520propose%2520THeGAU%252C%2520a%2520model-agnostic%2520framework%2520that%2520combines%2520a%2520type-aware%2520graph%2520autoencoder%2520with%2520guided%2520graph%2520augmentation%2520to%2520improve%2520node%2520classification.%2520THeGAU%2520reconstructs%2520schema-valid%2520edges%2520as%2520an%2520auxiliary%2520task%2520to%2520preserve%2520node-type%2520semantics%2520and%2520introduces%2520a%2520decoder-driven%2520augmentation%2520mechanism%2520to%2520selectively%2520refine%2520noisy%2520structures.%2520This%2520joint%2520design%2520enhances%2520robustness%252C%2520accuracy%252C%2520and%2520efficiency%2520while%2520significantly%2520reducing%2520computational%2520overhead.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520HIN%2520datasets%2520%2528IMDB%252C%2520ACM%252C%2520and%2520DBLP%2529%2520demonstrate%2520that%2520THeGAU%2520consistently%2520outperforms%2520existing%2520HGNN%2520methods%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520multiple%2520backbones.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=THeGAU%3A%20Type-Aware%20Heterogeneous%20Graph%20Autoencoder%20and%20Augmentation&entry.906535625=Ming-Yi%20Hong%20and%20Miao-Chen%20Chiang%20and%20Youchen%20Teng%20and%20Yu-Hsiang%20Wang%20and%20Chih-Yu%20Wang%20and%20Che%20Lin&entry.1292438233=Heterogeneous%20Graph%20Neural%20Networks%20%28HGNNs%29%20are%20effective%20for%20modeling%20Heterogeneous%20Information%20Networks%20%28HINs%29%2C%20which%20encode%20complex%20multi-typed%20entities%20and%20relations.%20However%2C%20HGNNs%20often%20suffer%20from%20type%20information%20loss%20and%20structural%20noise%2C%20limiting%20their%20representational%20fidelity%20and%20generalization.%20We%20propose%20THeGAU%2C%20a%20model-agnostic%20framework%20that%20combines%20a%20type-aware%20graph%20autoencoder%20with%20guided%20graph%20augmentation%20to%20improve%20node%20classification.%20THeGAU%20reconstructs%20schema-valid%20edges%20as%20an%20auxiliary%20task%20to%20preserve%20node-type%20semantics%20and%20introduces%20a%20decoder-driven%20augmentation%20mechanism%20to%20selectively%20refine%20noisy%20structures.%20This%20joint%20design%20enhances%20robustness%2C%20accuracy%2C%20and%20efficiency%20while%20significantly%20reducing%20computational%20overhead.%20Extensive%20experiments%20on%20three%20benchmark%20HIN%20datasets%20%28IMDB%2C%20ACM%2C%20and%20DBLP%29%20demonstrate%20that%20THeGAU%20consistently%20outperforms%20existing%20HGNN%20methods%2C%20achieving%20state-of-the-art%20performance%20across%20multiple%20backbones.&entry.1838667208=http%3A//arxiv.org/abs/2512.10589v1&entry.124074799=Read"},
{"title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders", "author": "Qingsen Ma and Dianyun Wang and Jiaming Lyu and Yaoye Wang and Lechen Ning and Sujie Zhu and Zhenbo Xu and Liuyu Xiang and Huining Li and Huijia Wu and Zhaofeng He", "abstract": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.", "link": "http://arxiv.org/abs/2512.10547v1", "date": "2025-12-11", "relevancy": 2.6825, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20the%20Address%20Book%3A%20Dissecting%20the%20Sparse%20Semantic%20Structure%20of%20LLM%20Key-Value%20Caches%20via%20Sparse%20Autoencoders&body=Title%3A%20Unlocking%20the%20Address%20Book%3A%20Dissecting%20the%20Sparse%20Semantic%20Structure%20of%20LLM%20Key-Value%20Caches%20via%20Sparse%20Autoencoders%0AAuthor%3A%20Qingsen%20Ma%20and%20Dianyun%20Wang%20and%20Jiaming%20Lyu%20and%20Yaoye%20Wang%20and%20Lechen%20Ning%20and%20Sujie%20Zhu%20and%20Zhenbo%20Xu%20and%20Liuyu%20Xiang%20and%20Huining%20Li%20and%20Huijia%20Wu%20and%20Zhaofeng%20He%0AAbstract%3A%20The%20Key-Value%20%28KV%29%20cache%20is%20the%20primary%20memory%20bottleneck%20in%20long-context%20Large%20Language%20Models%2C%20yet%20it%20is%20typically%20treated%20as%20an%20opaque%20numerical%20tensor.%20In%20this%20work%2C%20we%20propose%20%5Ctextbf%7BSTA-Attention%7D%2C%20a%20framework%20that%20utilizes%20Top-K%20Sparse%20Autoencoders%20%28SAEs%29%20to%20decompose%20the%20KV%20cache%20into%20interpretable%20%60%60semantic%20atoms.%27%27%20Unlike%20standard%20%24L_1%24-regularized%20SAEs%2C%20our%20Top-K%20approach%20eliminates%20shrinkage%20bias%2C%20preserving%20the%20precise%20dot-product%20geometry%20required%20for%20attention.%20Our%20analysis%20uncovers%20a%20fundamental%20%5Ctextbf%7BKey-Value%20Asymmetry%7D%3A%20while%20Key%20vectors%20serve%20as%20highly%20sparse%20routers%20dominated%20by%20a%20%60%60Semantic%20Elbow%2C%27%27%20deep%20Value%20vectors%20carry%20dense%20content%20payloads%20requiring%20a%20larger%20budget.%20Based%20on%20this%20structure%2C%20we%20introduce%20a%20Dual-Budget%20Strategy%20that%20selectively%20preserves%20the%20most%20informative%20semantic%20components%20while%20filtering%20representational%20noise.%20Experiments%20on%20Yi-6B%2C%20Mistral-7B%2C%20Qwen2.5-32B%2C%20and%20others%20show%20that%20our%20semantic%20reconstructions%20maintain%20perplexity%20and%20zero-shot%20performance%20comparable%20to%20the%20original%20models%2C%20effectively%20bridging%20the%20gap%20between%20mechanistic%20interpretability%20and%20faithful%20attention%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520the%2520Address%2520Book%253A%2520Dissecting%2520the%2520Sparse%2520Semantic%2520Structure%2520of%2520LLM%2520Key-Value%2520Caches%2520via%2520Sparse%2520Autoencoders%26entry.906535625%3DQingsen%2520Ma%2520and%2520Dianyun%2520Wang%2520and%2520Jiaming%2520Lyu%2520and%2520Yaoye%2520Wang%2520and%2520Lechen%2520Ning%2520and%2520Sujie%2520Zhu%2520and%2520Zhenbo%2520Xu%2520and%2520Liuyu%2520Xiang%2520and%2520Huining%2520Li%2520and%2520Huijia%2520Wu%2520and%2520Zhaofeng%2520He%26entry.1292438233%3DThe%2520Key-Value%2520%2528KV%2529%2520cache%2520is%2520the%2520primary%2520memory%2520bottleneck%2520in%2520long-context%2520Large%2520Language%2520Models%252C%2520yet%2520it%2520is%2520typically%2520treated%2520as%2520an%2520opaque%2520numerical%2520tensor.%2520In%2520this%2520work%252C%2520we%2520propose%2520%255Ctextbf%257BSTA-Attention%257D%252C%2520a%2520framework%2520that%2520utilizes%2520Top-K%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520to%2520decompose%2520the%2520KV%2520cache%2520into%2520interpretable%2520%2560%2560semantic%2520atoms.%2527%2527%2520Unlike%2520standard%2520%2524L_1%2524-regularized%2520SAEs%252C%2520our%2520Top-K%2520approach%2520eliminates%2520shrinkage%2520bias%252C%2520preserving%2520the%2520precise%2520dot-product%2520geometry%2520required%2520for%2520attention.%2520Our%2520analysis%2520uncovers%2520a%2520fundamental%2520%255Ctextbf%257BKey-Value%2520Asymmetry%257D%253A%2520while%2520Key%2520vectors%2520serve%2520as%2520highly%2520sparse%2520routers%2520dominated%2520by%2520a%2520%2560%2560Semantic%2520Elbow%252C%2527%2527%2520deep%2520Value%2520vectors%2520carry%2520dense%2520content%2520payloads%2520requiring%2520a%2520larger%2520budget.%2520Based%2520on%2520this%2520structure%252C%2520we%2520introduce%2520a%2520Dual-Budget%2520Strategy%2520that%2520selectively%2520preserves%2520the%2520most%2520informative%2520semantic%2520components%2520while%2520filtering%2520representational%2520noise.%2520Experiments%2520on%2520Yi-6B%252C%2520Mistral-7B%252C%2520Qwen2.5-32B%252C%2520and%2520others%2520show%2520that%2520our%2520semantic%2520reconstructions%2520maintain%2520perplexity%2520and%2520zero-shot%2520performance%2520comparable%2520to%2520the%2520original%2520models%252C%2520effectively%2520bridging%2520the%2520gap%2520between%2520mechanistic%2520interpretability%2520and%2520faithful%2520attention%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20the%20Address%20Book%3A%20Dissecting%20the%20Sparse%20Semantic%20Structure%20of%20LLM%20Key-Value%20Caches%20via%20Sparse%20Autoencoders&entry.906535625=Qingsen%20Ma%20and%20Dianyun%20Wang%20and%20Jiaming%20Lyu%20and%20Yaoye%20Wang%20and%20Lechen%20Ning%20and%20Sujie%20Zhu%20and%20Zhenbo%20Xu%20and%20Liuyu%20Xiang%20and%20Huining%20Li%20and%20Huijia%20Wu%20and%20Zhaofeng%20He&entry.1292438233=The%20Key-Value%20%28KV%29%20cache%20is%20the%20primary%20memory%20bottleneck%20in%20long-context%20Large%20Language%20Models%2C%20yet%20it%20is%20typically%20treated%20as%20an%20opaque%20numerical%20tensor.%20In%20this%20work%2C%20we%20propose%20%5Ctextbf%7BSTA-Attention%7D%2C%20a%20framework%20that%20utilizes%20Top-K%20Sparse%20Autoencoders%20%28SAEs%29%20to%20decompose%20the%20KV%20cache%20into%20interpretable%20%60%60semantic%20atoms.%27%27%20Unlike%20standard%20%24L_1%24-regularized%20SAEs%2C%20our%20Top-K%20approach%20eliminates%20shrinkage%20bias%2C%20preserving%20the%20precise%20dot-product%20geometry%20required%20for%20attention.%20Our%20analysis%20uncovers%20a%20fundamental%20%5Ctextbf%7BKey-Value%20Asymmetry%7D%3A%20while%20Key%20vectors%20serve%20as%20highly%20sparse%20routers%20dominated%20by%20a%20%60%60Semantic%20Elbow%2C%27%27%20deep%20Value%20vectors%20carry%20dense%20content%20payloads%20requiring%20a%20larger%20budget.%20Based%20on%20this%20structure%2C%20we%20introduce%20a%20Dual-Budget%20Strategy%20that%20selectively%20preserves%20the%20most%20informative%20semantic%20components%20while%20filtering%20representational%20noise.%20Experiments%20on%20Yi-6B%2C%20Mistral-7B%2C%20Qwen2.5-32B%2C%20and%20others%20show%20that%20our%20semantic%20reconstructions%20maintain%20perplexity%20and%20zero-shot%20performance%20comparable%20to%20the%20original%20models%2C%20effectively%20bridging%20the%20gap%20between%20mechanistic%20interpretability%20and%20faithful%20attention%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.10547v1&entry.124074799=Read"},
{"title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization", "author": "Tsai-Shien Chen and Aliaksandr Siarohin and Guocheng Gordon Qian and Kuan-Chieh Jackson Wang and Egor Nemchinov and Moayed Haji-Ali and Riza Alp Guler and Willi Menapace and Ivan Skorokhodov and Anil Kag and Jun-Yan Zhu and Sergey Tulyakov", "abstract": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.", "link": "http://arxiv.org/abs/2512.10955v1", "date": "2025-12-11", "relevancy": 2.6817, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Attribute%3A%20Open-vocabulary%20Attribute%20Encoder%20for%20Visual%20Concept%20Personalization&body=Title%3A%20Omni-Attribute%3A%20Open-vocabulary%20Attribute%20Encoder%20for%20Visual%20Concept%20Personalization%0AAuthor%3A%20Tsai-Shien%20Chen%20and%20Aliaksandr%20Siarohin%20and%20Guocheng%20Gordon%20Qian%20and%20Kuan-Chieh%20Jackson%20Wang%20and%20Egor%20Nemchinov%20and%20Moayed%20Haji-Ali%20and%20Riza%20Alp%20Guler%20and%20Willi%20Menapace%20and%20Ivan%20Skorokhodov%20and%20Anil%20Kag%20and%20Jun-Yan%20Zhu%20and%20Sergey%20Tulyakov%0AAbstract%3A%20Visual%20concept%20personalization%20aims%20to%20transfer%20only%20specific%20image%20attributes%2C%20such%20as%20identity%2C%20expression%2C%20lighting%2C%20and%20style%2C%20into%20unseen%20contexts.%20However%2C%20existing%20methods%20rely%20on%20holistic%20embeddings%20from%20general-purpose%20image%20encoders%2C%20which%20entangle%20multiple%20visual%20factors%20and%20make%20it%20difficult%20to%20isolate%20a%20single%20attribute.%20This%20often%20leads%20to%20information%20leakage%20and%20incoherent%20synthesis.%20To%20address%20this%20limitation%2C%20we%20introduce%20Omni-Attribute%2C%20the%20first%20open-vocabulary%20image%20attribute%20encoder%20designed%20to%20learn%20high-fidelity%2C%20attribute-specific%20representations.%20Our%20approach%20jointly%20designs%20the%20data%20and%20model%3A%20%28i%29%20we%20curate%20semantically%20linked%20image%20pairs%20annotated%20with%20positive%20and%20negative%20attributes%20to%20explicitly%20teach%20the%20encoder%20what%20to%20preserve%20or%20suppress%3B%20and%20%28ii%29%20we%20adopt%20a%20dual-objective%20training%20paradigm%20that%20balances%20generative%20fidelity%20with%20contrastive%20disentanglement.%20The%20resulting%20embeddings%20prove%20effective%20for%20open-vocabulary%20attribute%20retrieval%2C%20personalization%2C%20and%20compositional%20generation%2C%20achieving%20state-of-the-art%20performance%20across%20multiple%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Attribute%253A%2520Open-vocabulary%2520Attribute%2520Encoder%2520for%2520Visual%2520Concept%2520Personalization%26entry.906535625%3DTsai-Shien%2520Chen%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Guocheng%2520Gordon%2520Qian%2520and%2520Kuan-Chieh%2520Jackson%2520Wang%2520and%2520Egor%2520Nemchinov%2520and%2520Moayed%2520Haji-Ali%2520and%2520Riza%2520Alp%2520Guler%2520and%2520Willi%2520Menapace%2520and%2520Ivan%2520Skorokhodov%2520and%2520Anil%2520Kag%2520and%2520Jun-Yan%2520Zhu%2520and%2520Sergey%2520Tulyakov%26entry.1292438233%3DVisual%2520concept%2520personalization%2520aims%2520to%2520transfer%2520only%2520specific%2520image%2520attributes%252C%2520such%2520as%2520identity%252C%2520expression%252C%2520lighting%252C%2520and%2520style%252C%2520into%2520unseen%2520contexts.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520holistic%2520embeddings%2520from%2520general-purpose%2520image%2520encoders%252C%2520which%2520entangle%2520multiple%2520visual%2520factors%2520and%2520make%2520it%2520difficult%2520to%2520isolate%2520a%2520single%2520attribute.%2520This%2520often%2520leads%2520to%2520information%2520leakage%2520and%2520incoherent%2520synthesis.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520Omni-Attribute%252C%2520the%2520first%2520open-vocabulary%2520image%2520attribute%2520encoder%2520designed%2520to%2520learn%2520high-fidelity%252C%2520attribute-specific%2520representations.%2520Our%2520approach%2520jointly%2520designs%2520the%2520data%2520and%2520model%253A%2520%2528i%2529%2520we%2520curate%2520semantically%2520linked%2520image%2520pairs%2520annotated%2520with%2520positive%2520and%2520negative%2520attributes%2520to%2520explicitly%2520teach%2520the%2520encoder%2520what%2520to%2520preserve%2520or%2520suppress%253B%2520and%2520%2528ii%2529%2520we%2520adopt%2520a%2520dual-objective%2520training%2520paradigm%2520that%2520balances%2520generative%2520fidelity%2520with%2520contrastive%2520disentanglement.%2520The%2520resulting%2520embeddings%2520prove%2520effective%2520for%2520open-vocabulary%2520attribute%2520retrieval%252C%2520personalization%252C%2520and%2520compositional%2520generation%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520multiple%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Attribute%3A%20Open-vocabulary%20Attribute%20Encoder%20for%20Visual%20Concept%20Personalization&entry.906535625=Tsai-Shien%20Chen%20and%20Aliaksandr%20Siarohin%20and%20Guocheng%20Gordon%20Qian%20and%20Kuan-Chieh%20Jackson%20Wang%20and%20Egor%20Nemchinov%20and%20Moayed%20Haji-Ali%20and%20Riza%20Alp%20Guler%20and%20Willi%20Menapace%20and%20Ivan%20Skorokhodov%20and%20Anil%20Kag%20and%20Jun-Yan%20Zhu%20and%20Sergey%20Tulyakov&entry.1292438233=Visual%20concept%20personalization%20aims%20to%20transfer%20only%20specific%20image%20attributes%2C%20such%20as%20identity%2C%20expression%2C%20lighting%2C%20and%20style%2C%20into%20unseen%20contexts.%20However%2C%20existing%20methods%20rely%20on%20holistic%20embeddings%20from%20general-purpose%20image%20encoders%2C%20which%20entangle%20multiple%20visual%20factors%20and%20make%20it%20difficult%20to%20isolate%20a%20single%20attribute.%20This%20often%20leads%20to%20information%20leakage%20and%20incoherent%20synthesis.%20To%20address%20this%20limitation%2C%20we%20introduce%20Omni-Attribute%2C%20the%20first%20open-vocabulary%20image%20attribute%20encoder%20designed%20to%20learn%20high-fidelity%2C%20attribute-specific%20representations.%20Our%20approach%20jointly%20designs%20the%20data%20and%20model%3A%20%28i%29%20we%20curate%20semantically%20linked%20image%20pairs%20annotated%20with%20positive%20and%20negative%20attributes%20to%20explicitly%20teach%20the%20encoder%20what%20to%20preserve%20or%20suppress%3B%20and%20%28ii%29%20we%20adopt%20a%20dual-objective%20training%20paradigm%20that%20balances%20generative%20fidelity%20with%20contrastive%20disentanglement.%20The%20resulting%20embeddings%20prove%20effective%20for%20open-vocabulary%20attribute%20retrieval%2C%20personalization%2C%20and%20compositional%20generation%2C%20achieving%20state-of-the-art%20performance%20across%20multiple%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.10955v1&entry.124074799=Read"},
{"title": "Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models", "author": "Amartya Roy and Elamparithy M and Kripabandhu Ghosh and Ponnurangam Kumaraguru and Adrian de Wynter", "abstract": "In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.", "link": "http://arxiv.org/abs/2512.10561v1", "date": "2025-12-11", "relevancy": 2.6655, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5596}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Reasoning%20Favors%20Encoders%3A%20On%20The%20Limits%20of%20Decoder-Only%20Models&body=Title%3A%20Causal%20Reasoning%20Favors%20Encoders%3A%20On%20The%20Limits%20of%20Decoder-Only%20Models%0AAuthor%3A%20Amartya%20Roy%20and%20Elamparithy%20M%20and%20Kripabandhu%20Ghosh%20and%20Ponnurangam%20Kumaraguru%20and%20Adrian%20de%20Wynter%0AAbstract%3A%20In%20context%20learning%20%28ICL%29%20underpins%20recent%20advances%20in%20large%20language%20models%20%28LLMs%29%2C%20although%20its%20role%20and%20performance%20in%20causal%20reasoning%20remains%20unclear.%20Causal%20reasoning%20demands%20multihop%20composition%20and%20strict%20conjunctive%20control%2C%20and%20reliance%20on%20spurious%20lexical%20relations%20of%20the%20input%20could%20provide%20misleading%20results.%20We%20hypothesize%20that%2C%20due%20to%20their%20ability%20to%20project%20the%20input%20into%20a%20latent%20space%2C%20encoder%20and%20encoder%20decoder%20architectures%20are%20better%20suited%20for%20said%20multihop%20conjunctive%20reasoning%20versus%20decoder%20only%20models.%20To%20do%20this%2C%20we%20compare%20fine-tuned%20versions%20of%20all%20the%20aforementioned%20architectures%20with%20zero%20and%20few%20shot%20ICL%20in%20both%20natural%20language%20and%20non%20natural%20language%20scenarios.%20We%20find%20that%20ICL%20alone%20is%20insufficient%20for%20reliable%20causal%20reasoning%2C%20often%20overfocusing%20on%20irrelevant%20input%20features.%20In%20particular%2C%20decoder%20only%20models%20are%20noticeably%20brittle%20to%20distributional%20shifts%2C%20while%20finetuned%20encoder%20and%20encoder%20decoder%20models%20can%20generalize%20more%20robustly%20across%20our%20tests%2C%20including%20the%20non%20natural%20language%20split.%20Both%20architectures%20are%20only%20matched%20or%20surpassed%20by%20decoder%20only%20architectures%20at%20large%20scales.%20We%20conclude%20by%20noting%20that%20for%20cost%20effective%2C%20short%20horizon%20robust%20causal%20reasoning%2C%20encoder%20or%20encoder%20decoder%20architectures%20with%20targeted%20finetuning%20are%20preferable.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Reasoning%2520Favors%2520Encoders%253A%2520On%2520The%2520Limits%2520of%2520Decoder-Only%2520Models%26entry.906535625%3DAmartya%2520Roy%2520and%2520Elamparithy%2520M%2520and%2520Kripabandhu%2520Ghosh%2520and%2520Ponnurangam%2520Kumaraguru%2520and%2520Adrian%2520de%2520Wynter%26entry.1292438233%3DIn%2520context%2520learning%2520%2528ICL%2529%2520underpins%2520recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520although%2520its%2520role%2520and%2520performance%2520in%2520causal%2520reasoning%2520remains%2520unclear.%2520Causal%2520reasoning%2520demands%2520multihop%2520composition%2520and%2520strict%2520conjunctive%2520control%252C%2520and%2520reliance%2520on%2520spurious%2520lexical%2520relations%2520of%2520the%2520input%2520could%2520provide%2520misleading%2520results.%2520We%2520hypothesize%2520that%252C%2520due%2520to%2520their%2520ability%2520to%2520project%2520the%2520input%2520into%2520a%2520latent%2520space%252C%2520encoder%2520and%2520encoder%2520decoder%2520architectures%2520are%2520better%2520suited%2520for%2520said%2520multihop%2520conjunctive%2520reasoning%2520versus%2520decoder%2520only%2520models.%2520To%2520do%2520this%252C%2520we%2520compare%2520fine-tuned%2520versions%2520of%2520all%2520the%2520aforementioned%2520architectures%2520with%2520zero%2520and%2520few%2520shot%2520ICL%2520in%2520both%2520natural%2520language%2520and%2520non%2520natural%2520language%2520scenarios.%2520We%2520find%2520that%2520ICL%2520alone%2520is%2520insufficient%2520for%2520reliable%2520causal%2520reasoning%252C%2520often%2520overfocusing%2520on%2520irrelevant%2520input%2520features.%2520In%2520particular%252C%2520decoder%2520only%2520models%2520are%2520noticeably%2520brittle%2520to%2520distributional%2520shifts%252C%2520while%2520finetuned%2520encoder%2520and%2520encoder%2520decoder%2520models%2520can%2520generalize%2520more%2520robustly%2520across%2520our%2520tests%252C%2520including%2520the%2520non%2520natural%2520language%2520split.%2520Both%2520architectures%2520are%2520only%2520matched%2520or%2520surpassed%2520by%2520decoder%2520only%2520architectures%2520at%2520large%2520scales.%2520We%2520conclude%2520by%2520noting%2520that%2520for%2520cost%2520effective%252C%2520short%2520horizon%2520robust%2520causal%2520reasoning%252C%2520encoder%2520or%2520encoder%2520decoder%2520architectures%2520with%2520targeted%2520finetuning%2520are%2520preferable.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Reasoning%20Favors%20Encoders%3A%20On%20The%20Limits%20of%20Decoder-Only%20Models&entry.906535625=Amartya%20Roy%20and%20Elamparithy%20M%20and%20Kripabandhu%20Ghosh%20and%20Ponnurangam%20Kumaraguru%20and%20Adrian%20de%20Wynter&entry.1292438233=In%20context%20learning%20%28ICL%29%20underpins%20recent%20advances%20in%20large%20language%20models%20%28LLMs%29%2C%20although%20its%20role%20and%20performance%20in%20causal%20reasoning%20remains%20unclear.%20Causal%20reasoning%20demands%20multihop%20composition%20and%20strict%20conjunctive%20control%2C%20and%20reliance%20on%20spurious%20lexical%20relations%20of%20the%20input%20could%20provide%20misleading%20results.%20We%20hypothesize%20that%2C%20due%20to%20their%20ability%20to%20project%20the%20input%20into%20a%20latent%20space%2C%20encoder%20and%20encoder%20decoder%20architectures%20are%20better%20suited%20for%20said%20multihop%20conjunctive%20reasoning%20versus%20decoder%20only%20models.%20To%20do%20this%2C%20we%20compare%20fine-tuned%20versions%20of%20all%20the%20aforementioned%20architectures%20with%20zero%20and%20few%20shot%20ICL%20in%20both%20natural%20language%20and%20non%20natural%20language%20scenarios.%20We%20find%20that%20ICL%20alone%20is%20insufficient%20for%20reliable%20causal%20reasoning%2C%20often%20overfocusing%20on%20irrelevant%20input%20features.%20In%20particular%2C%20decoder%20only%20models%20are%20noticeably%20brittle%20to%20distributional%20shifts%2C%20while%20finetuned%20encoder%20and%20encoder%20decoder%20models%20can%20generalize%20more%20robustly%20across%20our%20tests%2C%20including%20the%20non%20natural%20language%20split.%20Both%20architectures%20are%20only%20matched%20or%20surpassed%20by%20decoder%20only%20architectures%20at%20large%20scales.%20We%20conclude%20by%20noting%20that%20for%20cost%20effective%2C%20short%20horizon%20robust%20causal%20reasoning%2C%20encoder%20or%20encoder%20decoder%20architectures%20with%20targeted%20finetuning%20are%20preferable.&entry.1838667208=http%3A//arxiv.org/abs/2512.10561v1&entry.124074799=Read"},
{"title": "Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks", "author": "Meher Md Saad", "abstract": "Isolated Sign Language Recognition (ISLR) is critical for bridging the communication gap between the Deaf and Hard-of-Hearing (DHH) community and the hearing world. However, robust ISLR is fundamentally constrained by data scarcity and the long-tail distribution of sign vocabulary, where gathering sufficient examples for thousands of unique signs is prohibitively expensive. Standard classification approaches struggle under these conditions, often overfitting to frequent classes while failing to generalize to rare ones. To address this bottleneck, we propose a Few-Shot Prototypical Network framework adapted for a skeleton based encoder. Unlike traditional classifiers that learn fixed decision boundaries, our approach utilizes episodic training to learn a semantic metric space where signs are classified based on their proximity to dynamic class prototypes. We integrate a Spatiotemporal Graph Convolutional Network (ST-GCN) with a novel Multi-Scale Temporal Aggregation (MSTA) module to capture both rapid and fluid motion dynamics. Experimental results on the WLASL dataset demonstrate the superiority of this metric learning paradigm: our model achieves 43.75% Top-1 and 77.10% Top-5 accuracy on the test set. Crucially, this outperforms a standard classification baseline sharing the identical backbone architecture by over 13%, proving that the prototypical training strategy effectively outperforms in a data scarce situation where standard classification fails. Furthermore, the model exhibits strong zero-shot generalization, achieving nearly 30% accuracy on the unseen SignASL dataset without fine-tuning, offering a scalable pathway for recognizing extensive sign vocabularies with limited data.", "link": "http://arxiv.org/abs/2512.10562v1", "date": "2025-12-11", "relevancy": 2.6647, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5481}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5322}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20American%20Sign%20Language%20Recognition%20via%20Few-Shot%20Prototypical%20Networks&body=Title%3A%20Data-Efficient%20American%20Sign%20Language%20Recognition%20via%20Few-Shot%20Prototypical%20Networks%0AAuthor%3A%20Meher%20Md%20Saad%0AAbstract%3A%20Isolated%20Sign%20Language%20Recognition%20%28ISLR%29%20is%20critical%20for%20bridging%20the%20communication%20gap%20between%20the%20Deaf%20and%20Hard-of-Hearing%20%28DHH%29%20community%20and%20the%20hearing%20world.%20However%2C%20robust%20ISLR%20is%20fundamentally%20constrained%20by%20data%20scarcity%20and%20the%20long-tail%20distribution%20of%20sign%20vocabulary%2C%20where%20gathering%20sufficient%20examples%20for%20thousands%20of%20unique%20signs%20is%20prohibitively%20expensive.%20Standard%20classification%20approaches%20struggle%20under%20these%20conditions%2C%20often%20overfitting%20to%20frequent%20classes%20while%20failing%20to%20generalize%20to%20rare%20ones.%20To%20address%20this%20bottleneck%2C%20we%20propose%20a%20Few-Shot%20Prototypical%20Network%20framework%20adapted%20for%20a%20skeleton%20based%20encoder.%20Unlike%20traditional%20classifiers%20that%20learn%20fixed%20decision%20boundaries%2C%20our%20approach%20utilizes%20episodic%20training%20to%20learn%20a%20semantic%20metric%20space%20where%20signs%20are%20classified%20based%20on%20their%20proximity%20to%20dynamic%20class%20prototypes.%20We%20integrate%20a%20Spatiotemporal%20Graph%20Convolutional%20Network%20%28ST-GCN%29%20with%20a%20novel%20Multi-Scale%20Temporal%20Aggregation%20%28MSTA%29%20module%20to%20capture%20both%20rapid%20and%20fluid%20motion%20dynamics.%20Experimental%20results%20on%20the%20WLASL%20dataset%20demonstrate%20the%20superiority%20of%20this%20metric%20learning%20paradigm%3A%20our%20model%20achieves%2043.75%25%20Top-1%20and%2077.10%25%20Top-5%20accuracy%20on%20the%20test%20set.%20Crucially%2C%20this%20outperforms%20a%20standard%20classification%20baseline%20sharing%20the%20identical%20backbone%20architecture%20by%20over%2013%25%2C%20proving%20that%20the%20prototypical%20training%20strategy%20effectively%20outperforms%20in%20a%20data%20scarce%20situation%20where%20standard%20classification%20fails.%20Furthermore%2C%20the%20model%20exhibits%20strong%20zero-shot%20generalization%2C%20achieving%20nearly%2030%25%20accuracy%20on%20the%20unseen%20SignASL%20dataset%20without%20fine-tuning%2C%20offering%20a%20scalable%20pathway%20for%20recognizing%20extensive%20sign%20vocabularies%20with%20limited%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Efficient%2520American%2520Sign%2520Language%2520Recognition%2520via%2520Few-Shot%2520Prototypical%2520Networks%26entry.906535625%3DMeher%2520Md%2520Saad%26entry.1292438233%3DIsolated%2520Sign%2520Language%2520Recognition%2520%2528ISLR%2529%2520is%2520critical%2520for%2520bridging%2520the%2520communication%2520gap%2520between%2520the%2520Deaf%2520and%2520Hard-of-Hearing%2520%2528DHH%2529%2520community%2520and%2520the%2520hearing%2520world.%2520However%252C%2520robust%2520ISLR%2520is%2520fundamentally%2520constrained%2520by%2520data%2520scarcity%2520and%2520the%2520long-tail%2520distribution%2520of%2520sign%2520vocabulary%252C%2520where%2520gathering%2520sufficient%2520examples%2520for%2520thousands%2520of%2520unique%2520signs%2520is%2520prohibitively%2520expensive.%2520Standard%2520classification%2520approaches%2520struggle%2520under%2520these%2520conditions%252C%2520often%2520overfitting%2520to%2520frequent%2520classes%2520while%2520failing%2520to%2520generalize%2520to%2520rare%2520ones.%2520To%2520address%2520this%2520bottleneck%252C%2520we%2520propose%2520a%2520Few-Shot%2520Prototypical%2520Network%2520framework%2520adapted%2520for%2520a%2520skeleton%2520based%2520encoder.%2520Unlike%2520traditional%2520classifiers%2520that%2520learn%2520fixed%2520decision%2520boundaries%252C%2520our%2520approach%2520utilizes%2520episodic%2520training%2520to%2520learn%2520a%2520semantic%2520metric%2520space%2520where%2520signs%2520are%2520classified%2520based%2520on%2520their%2520proximity%2520to%2520dynamic%2520class%2520prototypes.%2520We%2520integrate%2520a%2520Spatiotemporal%2520Graph%2520Convolutional%2520Network%2520%2528ST-GCN%2529%2520with%2520a%2520novel%2520Multi-Scale%2520Temporal%2520Aggregation%2520%2528MSTA%2529%2520module%2520to%2520capture%2520both%2520rapid%2520and%2520fluid%2520motion%2520dynamics.%2520Experimental%2520results%2520on%2520the%2520WLASL%2520dataset%2520demonstrate%2520the%2520superiority%2520of%2520this%2520metric%2520learning%2520paradigm%253A%2520our%2520model%2520achieves%252043.75%2525%2520Top-1%2520and%252077.10%2525%2520Top-5%2520accuracy%2520on%2520the%2520test%2520set.%2520Crucially%252C%2520this%2520outperforms%2520a%2520standard%2520classification%2520baseline%2520sharing%2520the%2520identical%2520backbone%2520architecture%2520by%2520over%252013%2525%252C%2520proving%2520that%2520the%2520prototypical%2520training%2520strategy%2520effectively%2520outperforms%2520in%2520a%2520data%2520scarce%2520situation%2520where%2520standard%2520classification%2520fails.%2520Furthermore%252C%2520the%2520model%2520exhibits%2520strong%2520zero-shot%2520generalization%252C%2520achieving%2520nearly%252030%2525%2520accuracy%2520on%2520the%2520unseen%2520SignASL%2520dataset%2520without%2520fine-tuning%252C%2520offering%2520a%2520scalable%2520pathway%2520for%2520recognizing%2520extensive%2520sign%2520vocabularies%2520with%2520limited%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20American%20Sign%20Language%20Recognition%20via%20Few-Shot%20Prototypical%20Networks&entry.906535625=Meher%20Md%20Saad&entry.1292438233=Isolated%20Sign%20Language%20Recognition%20%28ISLR%29%20is%20critical%20for%20bridging%20the%20communication%20gap%20between%20the%20Deaf%20and%20Hard-of-Hearing%20%28DHH%29%20community%20and%20the%20hearing%20world.%20However%2C%20robust%20ISLR%20is%20fundamentally%20constrained%20by%20data%20scarcity%20and%20the%20long-tail%20distribution%20of%20sign%20vocabulary%2C%20where%20gathering%20sufficient%20examples%20for%20thousands%20of%20unique%20signs%20is%20prohibitively%20expensive.%20Standard%20classification%20approaches%20struggle%20under%20these%20conditions%2C%20often%20overfitting%20to%20frequent%20classes%20while%20failing%20to%20generalize%20to%20rare%20ones.%20To%20address%20this%20bottleneck%2C%20we%20propose%20a%20Few-Shot%20Prototypical%20Network%20framework%20adapted%20for%20a%20skeleton%20based%20encoder.%20Unlike%20traditional%20classifiers%20that%20learn%20fixed%20decision%20boundaries%2C%20our%20approach%20utilizes%20episodic%20training%20to%20learn%20a%20semantic%20metric%20space%20where%20signs%20are%20classified%20based%20on%20their%20proximity%20to%20dynamic%20class%20prototypes.%20We%20integrate%20a%20Spatiotemporal%20Graph%20Convolutional%20Network%20%28ST-GCN%29%20with%20a%20novel%20Multi-Scale%20Temporal%20Aggregation%20%28MSTA%29%20module%20to%20capture%20both%20rapid%20and%20fluid%20motion%20dynamics.%20Experimental%20results%20on%20the%20WLASL%20dataset%20demonstrate%20the%20superiority%20of%20this%20metric%20learning%20paradigm%3A%20our%20model%20achieves%2043.75%25%20Top-1%20and%2077.10%25%20Top-5%20accuracy%20on%20the%20test%20set.%20Crucially%2C%20this%20outperforms%20a%20standard%20classification%20baseline%20sharing%20the%20identical%20backbone%20architecture%20by%20over%2013%25%2C%20proving%20that%20the%20prototypical%20training%20strategy%20effectively%20outperforms%20in%20a%20data%20scarce%20situation%20where%20standard%20classification%20fails.%20Furthermore%2C%20the%20model%20exhibits%20strong%20zero-shot%20generalization%2C%20achieving%20nearly%2030%25%20accuracy%20on%20the%20unseen%20SignASL%20dataset%20without%20fine-tuning%2C%20offering%20a%20scalable%20pathway%20for%20recognizing%20extensive%20sign%20vocabularies%20with%20limited%20data.&entry.1838667208=http%3A//arxiv.org/abs/2512.10562v1&entry.124074799=Read"},
{"title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance", "author": "Peiying Zhang and Nanxuan Zhao and Matthew Fisher and Yiran Xu and Jing Liao and Difan Liu", "abstract": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.", "link": "http://arxiv.org/abs/2512.10894v1", "date": "2025-12-11", "relevancy": 2.6398, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5498}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DuetSVG%3A%20Unified%20Multimodal%20SVG%20Generation%20with%20Internal%20Visual%20Guidance&body=Title%3A%20DuetSVG%3A%20Unified%20Multimodal%20SVG%20Generation%20with%20Internal%20Visual%20Guidance%0AAuthor%3A%20Peiying%20Zhang%20and%20Nanxuan%20Zhao%20and%20Matthew%20Fisher%20and%20Yiran%20Xu%20and%20Jing%20Liao%20and%20Difan%20Liu%0AAbstract%3A%20Recent%20vision-language%20model%20%28VLM%29-based%20approaches%20have%20achieved%20impressive%20results%20on%20SVG%20generation.%20However%2C%20because%20they%20generate%20only%20text%20and%20lack%20visual%20signals%20during%20decoding%2C%20they%20often%20struggle%20with%20complex%20semantics%20and%20fail%20to%20produce%20visually%20appealing%20or%20geometrically%20coherent%20SVGs.%20We%20introduce%20DuetSVG%2C%20a%20unified%20multimodal%20model%20that%20jointly%20generates%20image%20tokens%20and%20corresponding%20SVG%20tokens%20in%20an%20end-to-end%20manner.%20DuetSVG%20is%20trained%20on%20both%20image%20and%20SVG%20datasets.%20At%20inference%2C%20we%20apply%20a%20novel%20test-time%20scaling%20strategy%20that%20leverages%20the%20model%27s%20native%20visual%20predictions%20as%20guidance%20to%20improve%20SVG%20decoding%20quality.%20Extensive%20experiments%20show%20that%20our%20method%20outperforms%20existing%20methods%2C%20producing%20visually%20faithful%2C%20semantically%20aligned%2C%20and%20syntactically%20clean%20SVGs%20across%20a%20wide%20range%20of%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuetSVG%253A%2520Unified%2520Multimodal%2520SVG%2520Generation%2520with%2520Internal%2520Visual%2520Guidance%26entry.906535625%3DPeiying%2520Zhang%2520and%2520Nanxuan%2520Zhao%2520and%2520Matthew%2520Fisher%2520and%2520Yiran%2520Xu%2520and%2520Jing%2520Liao%2520and%2520Difan%2520Liu%26entry.1292438233%3DRecent%2520vision-language%2520model%2520%2528VLM%2529-based%2520approaches%2520have%2520achieved%2520impressive%2520results%2520on%2520SVG%2520generation.%2520However%252C%2520because%2520they%2520generate%2520only%2520text%2520and%2520lack%2520visual%2520signals%2520during%2520decoding%252C%2520they%2520often%2520struggle%2520with%2520complex%2520semantics%2520and%2520fail%2520to%2520produce%2520visually%2520appealing%2520or%2520geometrically%2520coherent%2520SVGs.%2520We%2520introduce%2520DuetSVG%252C%2520a%2520unified%2520multimodal%2520model%2520that%2520jointly%2520generates%2520image%2520tokens%2520and%2520corresponding%2520SVG%2520tokens%2520in%2520an%2520end-to-end%2520manner.%2520DuetSVG%2520is%2520trained%2520on%2520both%2520image%2520and%2520SVG%2520datasets.%2520At%2520inference%252C%2520we%2520apply%2520a%2520novel%2520test-time%2520scaling%2520strategy%2520that%2520leverages%2520the%2520model%2527s%2520native%2520visual%2520predictions%2520as%2520guidance%2520to%2520improve%2520SVG%2520decoding%2520quality.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520existing%2520methods%252C%2520producing%2520visually%2520faithful%252C%2520semantically%2520aligned%252C%2520and%2520syntactically%2520clean%2520SVGs%2520across%2520a%2520wide%2520range%2520of%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DuetSVG%3A%20Unified%20Multimodal%20SVG%20Generation%20with%20Internal%20Visual%20Guidance&entry.906535625=Peiying%20Zhang%20and%20Nanxuan%20Zhao%20and%20Matthew%20Fisher%20and%20Yiran%20Xu%20and%20Jing%20Liao%20and%20Difan%20Liu&entry.1292438233=Recent%20vision-language%20model%20%28VLM%29-based%20approaches%20have%20achieved%20impressive%20results%20on%20SVG%20generation.%20However%2C%20because%20they%20generate%20only%20text%20and%20lack%20visual%20signals%20during%20decoding%2C%20they%20often%20struggle%20with%20complex%20semantics%20and%20fail%20to%20produce%20visually%20appealing%20or%20geometrically%20coherent%20SVGs.%20We%20introduce%20DuetSVG%2C%20a%20unified%20multimodal%20model%20that%20jointly%20generates%20image%20tokens%20and%20corresponding%20SVG%20tokens%20in%20an%20end-to-end%20manner.%20DuetSVG%20is%20trained%20on%20both%20image%20and%20SVG%20datasets.%20At%20inference%2C%20we%20apply%20a%20novel%20test-time%20scaling%20strategy%20that%20leverages%20the%20model%27s%20native%20visual%20predictions%20as%20guidance%20to%20improve%20SVG%20decoding%20quality.%20Extensive%20experiments%20show%20that%20our%20method%20outperforms%20existing%20methods%2C%20producing%20visually%20faithful%2C%20semantically%20aligned%2C%20and%20syntactically%20clean%20SVGs%20across%20a%20wide%20range%20of%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.10894v1&entry.124074799=Read"},
{"title": "3D Blood Pulsation Maps", "author": "Maurice Rohr and Tobias Reinhardt and Tizian Dege and Justus Thies and Christoph Hoog Antink", "abstract": "We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps. These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging. Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis. Pulse3DFace consists of raw videos from 15 subjects recorded at 30 Hz with an RGB camera from 23 viewpoints, blood pulse reference measurements, and facial 3D scans generated using monocular structure-from-motion techniques. It also includes processed 3D pulsation maps compatible with the texture space of the 3D head model FLAME. These maps provide signal-to-noise ratio, local pulse amplitude, phase information, and supplementary data. We offer a comprehensive evaluation of the dataset's illumination conditions, map consistency, and its ability to capture physiologically meaningful features in the facial and neck skin regions.", "link": "http://arxiv.org/abs/2512.10517v1", "date": "2025-12-11", "relevancy": 2.6395, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.543}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5203}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Blood%20Pulsation%20Maps&body=Title%3A%203D%20Blood%20Pulsation%20Maps%0AAuthor%3A%20Maurice%20Rohr%20and%20Tobias%20Reinhardt%20and%20Tizian%20Dege%20and%20Justus%20Thies%20and%20Christoph%20Hoog%20Antink%0AAbstract%3A%20We%20present%20Pulse3DFace%2C%20the%20first%20dataset%20of%20its%20kind%20for%20estimating%203D%20blood%20pulsation%20maps.%20These%20maps%20can%20be%20used%20to%20develop%20models%20of%20dynamic%20facial%20blood%20pulsation%2C%20enabling%20the%20creation%20of%20synthetic%20video%20data%20to%20improve%20and%20validate%20remote%20pulse%20estimation%20methods%20via%20photoplethysmography%20imaging.%20Additionally%2C%20the%20dataset%20facilitates%20research%20into%20novel%20multi-view-based%20approaches%20for%20mitigating%20illumination%20effects%20in%20blood%20pulsation%20analysis.%20Pulse3DFace%20consists%20of%20raw%20videos%20from%2015%20subjects%20recorded%20at%2030%20Hz%20with%20an%20RGB%20camera%20from%2023%20viewpoints%2C%20blood%20pulse%20reference%20measurements%2C%20and%20facial%203D%20scans%20generated%20using%20monocular%20structure-from-motion%20techniques.%20It%20also%20includes%20processed%203D%20pulsation%20maps%20compatible%20with%20the%20texture%20space%20of%20the%203D%20head%20model%20FLAME.%20These%20maps%20provide%20signal-to-noise%20ratio%2C%20local%20pulse%20amplitude%2C%20phase%20information%2C%20and%20supplementary%20data.%20We%20offer%20a%20comprehensive%20evaluation%20of%20the%20dataset%27s%20illumination%20conditions%2C%20map%20consistency%2C%20and%20its%20ability%20to%20capture%20physiologically%20meaningful%20features%20in%20the%20facial%20and%20neck%20skin%20regions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Blood%2520Pulsation%2520Maps%26entry.906535625%3DMaurice%2520Rohr%2520and%2520Tobias%2520Reinhardt%2520and%2520Tizian%2520Dege%2520and%2520Justus%2520Thies%2520and%2520Christoph%2520Hoog%2520Antink%26entry.1292438233%3DWe%2520present%2520Pulse3DFace%252C%2520the%2520first%2520dataset%2520of%2520its%2520kind%2520for%2520estimating%25203D%2520blood%2520pulsation%2520maps.%2520These%2520maps%2520can%2520be%2520used%2520to%2520develop%2520models%2520of%2520dynamic%2520facial%2520blood%2520pulsation%252C%2520enabling%2520the%2520creation%2520of%2520synthetic%2520video%2520data%2520to%2520improve%2520and%2520validate%2520remote%2520pulse%2520estimation%2520methods%2520via%2520photoplethysmography%2520imaging.%2520Additionally%252C%2520the%2520dataset%2520facilitates%2520research%2520into%2520novel%2520multi-view-based%2520approaches%2520for%2520mitigating%2520illumination%2520effects%2520in%2520blood%2520pulsation%2520analysis.%2520Pulse3DFace%2520consists%2520of%2520raw%2520videos%2520from%252015%2520subjects%2520recorded%2520at%252030%2520Hz%2520with%2520an%2520RGB%2520camera%2520from%252023%2520viewpoints%252C%2520blood%2520pulse%2520reference%2520measurements%252C%2520and%2520facial%25203D%2520scans%2520generated%2520using%2520monocular%2520structure-from-motion%2520techniques.%2520It%2520also%2520includes%2520processed%25203D%2520pulsation%2520maps%2520compatible%2520with%2520the%2520texture%2520space%2520of%2520the%25203D%2520head%2520model%2520FLAME.%2520These%2520maps%2520provide%2520signal-to-noise%2520ratio%252C%2520local%2520pulse%2520amplitude%252C%2520phase%2520information%252C%2520and%2520supplementary%2520data.%2520We%2520offer%2520a%2520comprehensive%2520evaluation%2520of%2520the%2520dataset%2527s%2520illumination%2520conditions%252C%2520map%2520consistency%252C%2520and%2520its%2520ability%2520to%2520capture%2520physiologically%2520meaningful%2520features%2520in%2520the%2520facial%2520and%2520neck%2520skin%2520regions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Blood%20Pulsation%20Maps&entry.906535625=Maurice%20Rohr%20and%20Tobias%20Reinhardt%20and%20Tizian%20Dege%20and%20Justus%20Thies%20and%20Christoph%20Hoog%20Antink&entry.1292438233=We%20present%20Pulse3DFace%2C%20the%20first%20dataset%20of%20its%20kind%20for%20estimating%203D%20blood%20pulsation%20maps.%20These%20maps%20can%20be%20used%20to%20develop%20models%20of%20dynamic%20facial%20blood%20pulsation%2C%20enabling%20the%20creation%20of%20synthetic%20video%20data%20to%20improve%20and%20validate%20remote%20pulse%20estimation%20methods%20via%20photoplethysmography%20imaging.%20Additionally%2C%20the%20dataset%20facilitates%20research%20into%20novel%20multi-view-based%20approaches%20for%20mitigating%20illumination%20effects%20in%20blood%20pulsation%20analysis.%20Pulse3DFace%20consists%20of%20raw%20videos%20from%2015%20subjects%20recorded%20at%2030%20Hz%20with%20an%20RGB%20camera%20from%2023%20viewpoints%2C%20blood%20pulse%20reference%20measurements%2C%20and%20facial%203D%20scans%20generated%20using%20monocular%20structure-from-motion%20techniques.%20It%20also%20includes%20processed%203D%20pulsation%20maps%20compatible%20with%20the%20texture%20space%20of%20the%203D%20head%20model%20FLAME.%20These%20maps%20provide%20signal-to-noise%20ratio%2C%20local%20pulse%20amplitude%2C%20phase%20information%2C%20and%20supplementary%20data.%20We%20offer%20a%20comprehensive%20evaluation%20of%20the%20dataset%27s%20illumination%20conditions%2C%20map%20consistency%2C%20and%20its%20ability%20to%20capture%20physiologically%20meaningful%20features%20in%20the%20facial%20and%20neck%20skin%20regions.&entry.1838667208=http%3A//arxiv.org/abs/2512.10517v1&entry.124074799=Read"},
{"title": "Self-Ensemble Post Learning for Noisy Domain Generalization", "author": "Wang Lu and Jindong Wang", "abstract": "While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.", "link": "http://arxiv.org/abs/2512.10818v1", "date": "2025-12-11", "relevancy": 2.631, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5323}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.53}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Ensemble%20Post%20Learning%20for%20Noisy%20Domain%20Generalization&body=Title%3A%20Self-Ensemble%20Post%20Learning%20for%20Noisy%20Domain%20Generalization%0AAuthor%3A%20Wang%20Lu%20and%20Jindong%20Wang%0AAbstract%3A%20While%20computer%20vision%20and%20machine%20learning%20have%20made%20great%20progress%2C%20their%20robustness%20is%20still%20challenged%20by%20two%20key%20issues%3A%20data%20distribution%20shift%20and%20label%20noise.%20When%20domain%20generalization%20%28DG%29%20encounters%20noise%2C%20noisy%20labels%20further%20exacerbate%20the%20emergence%20of%20spurious%20features%20in%20deep%20layers%2C%20i.e.%20spurious%20feature%20enlargement%2C%20leading%20to%20a%20degradation%20in%20the%20performance%20of%20existing%20algorithms.%20This%20paper%2C%20starting%20from%20domain%20generalization%2C%20explores%20how%20to%20make%20existing%20methods%20rework%20when%20meeting%20noise.%20We%20find%20that%20the%20latent%20features%20inside%20the%20model%20have%20certain%20discriminative%20capabilities%2C%20and%20different%20latent%20features%20focus%20on%20different%20parts%20of%20the%20image.%20Based%20on%20these%20observations%2C%20we%20propose%20the%20Self-Ensemble%20Post%20Learning%20approach%20%28SEPL%29%20to%20diversify%20features%20which%20can%20be%20leveraged.%20Specifically%2C%20SEPL%20consists%20of%20two%20parts%3A%20feature%20probing%20training%20and%20prediction%20ensemble%20inference.%20It%20leverages%20intermediate%20feature%20representations%20within%20the%20model%20architecture%2C%20training%20multiple%20probing%20classifiers%20to%20fully%20exploit%20the%20capabilities%20of%20pre-trained%20models%2C%20while%20the%20final%20predictions%20are%20obtained%20through%20the%20integration%20of%20outputs%20from%20these%20diverse%20classification%20heads.%20Considering%20the%20presence%20of%20noisy%20labels%2C%20we%20employ%20semi-supervised%20algorithms%20to%20train%20probing%20classifiers.%20Given%20that%20different%20probing%20classifiers%20focus%20on%20different%20areas%2C%20we%20integrate%20their%20predictions%20using%20a%20crowdsourcing%20inference%20approach.%20Extensive%20experimental%20evaluations%20demonstrate%20that%20the%20proposed%20method%20not%20only%20enhances%20the%20robustness%20of%20existing%20methods%20but%20also%20exhibits%20significant%20potential%20for%20real-world%20applications%20with%20high%20flexibility.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Ensemble%2520Post%2520Learning%2520for%2520Noisy%2520Domain%2520Generalization%26entry.906535625%3DWang%2520Lu%2520and%2520Jindong%2520Wang%26entry.1292438233%3DWhile%2520computer%2520vision%2520and%2520machine%2520learning%2520have%2520made%2520great%2520progress%252C%2520their%2520robustness%2520is%2520still%2520challenged%2520by%2520two%2520key%2520issues%253A%2520data%2520distribution%2520shift%2520and%2520label%2520noise.%2520When%2520domain%2520generalization%2520%2528DG%2529%2520encounters%2520noise%252C%2520noisy%2520labels%2520further%2520exacerbate%2520the%2520emergence%2520of%2520spurious%2520features%2520in%2520deep%2520layers%252C%2520i.e.%2520spurious%2520feature%2520enlargement%252C%2520leading%2520to%2520a%2520degradation%2520in%2520the%2520performance%2520of%2520existing%2520algorithms.%2520This%2520paper%252C%2520starting%2520from%2520domain%2520generalization%252C%2520explores%2520how%2520to%2520make%2520existing%2520methods%2520rework%2520when%2520meeting%2520noise.%2520We%2520find%2520that%2520the%2520latent%2520features%2520inside%2520the%2520model%2520have%2520certain%2520discriminative%2520capabilities%252C%2520and%2520different%2520latent%2520features%2520focus%2520on%2520different%2520parts%2520of%2520the%2520image.%2520Based%2520on%2520these%2520observations%252C%2520we%2520propose%2520the%2520Self-Ensemble%2520Post%2520Learning%2520approach%2520%2528SEPL%2529%2520to%2520diversify%2520features%2520which%2520can%2520be%2520leveraged.%2520Specifically%252C%2520SEPL%2520consists%2520of%2520two%2520parts%253A%2520feature%2520probing%2520training%2520and%2520prediction%2520ensemble%2520inference.%2520It%2520leverages%2520intermediate%2520feature%2520representations%2520within%2520the%2520model%2520architecture%252C%2520training%2520multiple%2520probing%2520classifiers%2520to%2520fully%2520exploit%2520the%2520capabilities%2520of%2520pre-trained%2520models%252C%2520while%2520the%2520final%2520predictions%2520are%2520obtained%2520through%2520the%2520integration%2520of%2520outputs%2520from%2520these%2520diverse%2520classification%2520heads.%2520Considering%2520the%2520presence%2520of%2520noisy%2520labels%252C%2520we%2520employ%2520semi-supervised%2520algorithms%2520to%2520train%2520probing%2520classifiers.%2520Given%2520that%2520different%2520probing%2520classifiers%2520focus%2520on%2520different%2520areas%252C%2520we%2520integrate%2520their%2520predictions%2520using%2520a%2520crowdsourcing%2520inference%2520approach.%2520Extensive%2520experimental%2520evaluations%2520demonstrate%2520that%2520the%2520proposed%2520method%2520not%2520only%2520enhances%2520the%2520robustness%2520of%2520existing%2520methods%2520but%2520also%2520exhibits%2520significant%2520potential%2520for%2520real-world%2520applications%2520with%2520high%2520flexibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Ensemble%20Post%20Learning%20for%20Noisy%20Domain%20Generalization&entry.906535625=Wang%20Lu%20and%20Jindong%20Wang&entry.1292438233=While%20computer%20vision%20and%20machine%20learning%20have%20made%20great%20progress%2C%20their%20robustness%20is%20still%20challenged%20by%20two%20key%20issues%3A%20data%20distribution%20shift%20and%20label%20noise.%20When%20domain%20generalization%20%28DG%29%20encounters%20noise%2C%20noisy%20labels%20further%20exacerbate%20the%20emergence%20of%20spurious%20features%20in%20deep%20layers%2C%20i.e.%20spurious%20feature%20enlargement%2C%20leading%20to%20a%20degradation%20in%20the%20performance%20of%20existing%20algorithms.%20This%20paper%2C%20starting%20from%20domain%20generalization%2C%20explores%20how%20to%20make%20existing%20methods%20rework%20when%20meeting%20noise.%20We%20find%20that%20the%20latent%20features%20inside%20the%20model%20have%20certain%20discriminative%20capabilities%2C%20and%20different%20latent%20features%20focus%20on%20different%20parts%20of%20the%20image.%20Based%20on%20these%20observations%2C%20we%20propose%20the%20Self-Ensemble%20Post%20Learning%20approach%20%28SEPL%29%20to%20diversify%20features%20which%20can%20be%20leveraged.%20Specifically%2C%20SEPL%20consists%20of%20two%20parts%3A%20feature%20probing%20training%20and%20prediction%20ensemble%20inference.%20It%20leverages%20intermediate%20feature%20representations%20within%20the%20model%20architecture%2C%20training%20multiple%20probing%20classifiers%20to%20fully%20exploit%20the%20capabilities%20of%20pre-trained%20models%2C%20while%20the%20final%20predictions%20are%20obtained%20through%20the%20integration%20of%20outputs%20from%20these%20diverse%20classification%20heads.%20Considering%20the%20presence%20of%20noisy%20labels%2C%20we%20employ%20semi-supervised%20algorithms%20to%20train%20probing%20classifiers.%20Given%20that%20different%20probing%20classifiers%20focus%20on%20different%20areas%2C%20we%20integrate%20their%20predictions%20using%20a%20crowdsourcing%20inference%20approach.%20Extensive%20experimental%20evaluations%20demonstrate%20that%20the%20proposed%20method%20not%20only%20enhances%20the%20robustness%20of%20existing%20methods%20but%20also%20exhibits%20significant%20potential%20for%20real-world%20applications%20with%20high%20flexibility.&entry.1838667208=http%3A//arxiv.org/abs/2512.10818v1&entry.124074799=Read"},
{"title": "Dual Cluster Contrastive learning for Object Re-Identification", "author": "Hantao Yao and Changsheng Xu", "abstract": "Recently, cluster contrastive learning has been proven effective for object ReID by computing the contrastive loss between the individual features and the cluster memory. However, existing methods that use the individual features to momentum update the cluster memory will fluctuate over the training examples, especially for the outlier samples. Unlike the individual-based updating mechanism, the centroid-based updating mechanism that applies the mean feature of each cluster to update the cluster memory can reduce the impact of individual samples. Therefore, we formulate the individual-based updating and centroid-based updating mechanisms in a unified cluster contrastive framework, named Dual Cluster Contrastive framework (DCC), which maintains two types of memory banks: individual and centroid cluster memory banks. Significantly, the individual cluster memory considers just one individual at a time to take a single step for updating. The centroid cluster memory applies the mean feature of each cluster to update the corresponding cluster memory. During optimization, besides the vallina contrastive loss of each memory, a cross-view consistency constraint is applied to exchange the benefits of two memories for generating a discriminative description for the object ReID. Note that DCC can be easily applied for unsupervised or supervised object ReID by using ground-truth labels or the generated pseudo-labels. Extensive experiments on three benchmarks, \\emph{e.g.,} Market-1501, MSMT17, and VeRi-776, under \\textbf{supervised Object ReID} and \\textbf{unsupervised Object ReID} demonstrate the superiority of the proposed DCC.", "link": "http://arxiv.org/abs/2112.04662v4", "date": "2025-12-11", "relevancy": 2.6143, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Cluster%20Contrastive%20learning%20for%20Object%20Re-Identification&body=Title%3A%20Dual%20Cluster%20Contrastive%20learning%20for%20Object%20Re-Identification%0AAuthor%3A%20Hantao%20Yao%20and%20Changsheng%20Xu%0AAbstract%3A%20Recently%2C%20cluster%20contrastive%20learning%20has%20been%20proven%20effective%20for%20object%20ReID%20by%20computing%20the%20contrastive%20loss%20between%20the%20individual%20features%20and%20the%20cluster%20memory.%20However%2C%20existing%20methods%20that%20use%20the%20individual%20features%20to%20momentum%20update%20the%20cluster%20memory%20will%20fluctuate%20over%20the%20training%20examples%2C%20especially%20for%20the%20outlier%20samples.%20Unlike%20the%20individual-based%20updating%20mechanism%2C%20the%20centroid-based%20updating%20mechanism%20that%20applies%20the%20mean%20feature%20of%20each%20cluster%20to%20update%20the%20cluster%20memory%20can%20reduce%20the%20impact%20of%20individual%20samples.%20Therefore%2C%20we%20formulate%20the%20individual-based%20updating%20and%20centroid-based%20updating%20mechanisms%20in%20a%20unified%20cluster%20contrastive%20framework%2C%20named%20Dual%20Cluster%20Contrastive%20framework%20%28DCC%29%2C%20which%20maintains%20two%20types%20of%20memory%20banks%3A%20individual%20and%20centroid%20cluster%20memory%20banks.%20Significantly%2C%20the%20individual%20cluster%20memory%20considers%20just%20one%20individual%20at%20a%20time%20to%20take%20a%20single%20step%20for%20updating.%20The%20centroid%20cluster%20memory%20applies%20the%20mean%20feature%20of%20each%20cluster%20to%20update%20the%20corresponding%20cluster%20memory.%20During%20optimization%2C%20besides%20the%20vallina%20contrastive%20loss%20of%20each%20memory%2C%20a%20cross-view%20consistency%20constraint%20is%20applied%20to%20exchange%20the%20benefits%20of%20two%20memories%20for%20generating%20a%20discriminative%20description%20for%20the%20object%20ReID.%20Note%20that%20DCC%20can%20be%20easily%20applied%20for%20unsupervised%20or%20supervised%20object%20ReID%20by%20using%20ground-truth%20labels%20or%20the%20generated%20pseudo-labels.%20Extensive%20experiments%20on%20three%20benchmarks%2C%20%5Cemph%7Be.g.%2C%7D%20Market-1501%2C%20MSMT17%2C%20and%20VeRi-776%2C%20under%20%5Ctextbf%7Bsupervised%20Object%20ReID%7D%20and%20%5Ctextbf%7Bunsupervised%20Object%20ReID%7D%20demonstrate%20the%20superiority%20of%20the%20proposed%20DCC.%0ALink%3A%20http%3A//arxiv.org/abs/2112.04662v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Cluster%2520Contrastive%2520learning%2520for%2520Object%2520Re-Identification%26entry.906535625%3DHantao%2520Yao%2520and%2520Changsheng%2520Xu%26entry.1292438233%3DRecently%252C%2520cluster%2520contrastive%2520learning%2520has%2520been%2520proven%2520effective%2520for%2520object%2520ReID%2520by%2520computing%2520the%2520contrastive%2520loss%2520between%2520the%2520individual%2520features%2520and%2520the%2520cluster%2520memory.%2520However%252C%2520existing%2520methods%2520that%2520use%2520the%2520individual%2520features%2520to%2520momentum%2520update%2520the%2520cluster%2520memory%2520will%2520fluctuate%2520over%2520the%2520training%2520examples%252C%2520especially%2520for%2520the%2520outlier%2520samples.%2520Unlike%2520the%2520individual-based%2520updating%2520mechanism%252C%2520the%2520centroid-based%2520updating%2520mechanism%2520that%2520applies%2520the%2520mean%2520feature%2520of%2520each%2520cluster%2520to%2520update%2520the%2520cluster%2520memory%2520can%2520reduce%2520the%2520impact%2520of%2520individual%2520samples.%2520Therefore%252C%2520we%2520formulate%2520the%2520individual-based%2520updating%2520and%2520centroid-based%2520updating%2520mechanisms%2520in%2520a%2520unified%2520cluster%2520contrastive%2520framework%252C%2520named%2520Dual%2520Cluster%2520Contrastive%2520framework%2520%2528DCC%2529%252C%2520which%2520maintains%2520two%2520types%2520of%2520memory%2520banks%253A%2520individual%2520and%2520centroid%2520cluster%2520memory%2520banks.%2520Significantly%252C%2520the%2520individual%2520cluster%2520memory%2520considers%2520just%2520one%2520individual%2520at%2520a%2520time%2520to%2520take%2520a%2520single%2520step%2520for%2520updating.%2520The%2520centroid%2520cluster%2520memory%2520applies%2520the%2520mean%2520feature%2520of%2520each%2520cluster%2520to%2520update%2520the%2520corresponding%2520cluster%2520memory.%2520During%2520optimization%252C%2520besides%2520the%2520vallina%2520contrastive%2520loss%2520of%2520each%2520memory%252C%2520a%2520cross-view%2520consistency%2520constraint%2520is%2520applied%2520to%2520exchange%2520the%2520benefits%2520of%2520two%2520memories%2520for%2520generating%2520a%2520discriminative%2520description%2520for%2520the%2520object%2520ReID.%2520Note%2520that%2520DCC%2520can%2520be%2520easily%2520applied%2520for%2520unsupervised%2520or%2520supervised%2520object%2520ReID%2520by%2520using%2520ground-truth%2520labels%2520or%2520the%2520generated%2520pseudo-labels.%2520Extensive%2520experiments%2520on%2520three%2520benchmarks%252C%2520%255Cemph%257Be.g.%252C%257D%2520Market-1501%252C%2520MSMT17%252C%2520and%2520VeRi-776%252C%2520under%2520%255Ctextbf%257Bsupervised%2520Object%2520ReID%257D%2520and%2520%255Ctextbf%257Bunsupervised%2520Object%2520ReID%257D%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520DCC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2112.04662v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Cluster%20Contrastive%20learning%20for%20Object%20Re-Identification&entry.906535625=Hantao%20Yao%20and%20Changsheng%20Xu&entry.1292438233=Recently%2C%20cluster%20contrastive%20learning%20has%20been%20proven%20effective%20for%20object%20ReID%20by%20computing%20the%20contrastive%20loss%20between%20the%20individual%20features%20and%20the%20cluster%20memory.%20However%2C%20existing%20methods%20that%20use%20the%20individual%20features%20to%20momentum%20update%20the%20cluster%20memory%20will%20fluctuate%20over%20the%20training%20examples%2C%20especially%20for%20the%20outlier%20samples.%20Unlike%20the%20individual-based%20updating%20mechanism%2C%20the%20centroid-based%20updating%20mechanism%20that%20applies%20the%20mean%20feature%20of%20each%20cluster%20to%20update%20the%20cluster%20memory%20can%20reduce%20the%20impact%20of%20individual%20samples.%20Therefore%2C%20we%20formulate%20the%20individual-based%20updating%20and%20centroid-based%20updating%20mechanisms%20in%20a%20unified%20cluster%20contrastive%20framework%2C%20named%20Dual%20Cluster%20Contrastive%20framework%20%28DCC%29%2C%20which%20maintains%20two%20types%20of%20memory%20banks%3A%20individual%20and%20centroid%20cluster%20memory%20banks.%20Significantly%2C%20the%20individual%20cluster%20memory%20considers%20just%20one%20individual%20at%20a%20time%20to%20take%20a%20single%20step%20for%20updating.%20The%20centroid%20cluster%20memory%20applies%20the%20mean%20feature%20of%20each%20cluster%20to%20update%20the%20corresponding%20cluster%20memory.%20During%20optimization%2C%20besides%20the%20vallina%20contrastive%20loss%20of%20each%20memory%2C%20a%20cross-view%20consistency%20constraint%20is%20applied%20to%20exchange%20the%20benefits%20of%20two%20memories%20for%20generating%20a%20discriminative%20description%20for%20the%20object%20ReID.%20Note%20that%20DCC%20can%20be%20easily%20applied%20for%20unsupervised%20or%20supervised%20object%20ReID%20by%20using%20ground-truth%20labels%20or%20the%20generated%20pseudo-labels.%20Extensive%20experiments%20on%20three%20benchmarks%2C%20%5Cemph%7Be.g.%2C%7D%20Market-1501%2C%20MSMT17%2C%20and%20VeRi-776%2C%20under%20%5Ctextbf%7Bsupervised%20Object%20ReID%7D%20and%20%5Ctextbf%7Bunsupervised%20Object%20ReID%7D%20demonstrate%20the%20superiority%20of%20the%20proposed%20DCC.&entry.1838667208=http%3A//arxiv.org/abs/2112.04662v4&entry.124074799=Read"},
{"title": "Generalized Spherical Neural Operators: Green's Function Formulation", "author": "Hao Tang and Hao Chen and Chao Li", "abstract": "Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.", "link": "http://arxiv.org/abs/2512.10723v1", "date": "2025-12-11", "relevancy": 2.5818, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5258}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5193}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Spherical%20Neural%20Operators%3A%20Green%27s%20Function%20Formulation&body=Title%3A%20Generalized%20Spherical%20Neural%20Operators%3A%20Green%27s%20Function%20Formulation%0AAuthor%3A%20Hao%20Tang%20and%20Hao%20Chen%20and%20Chao%20Li%0AAbstract%3A%20Neural%20operators%20offer%20powerful%20approaches%20for%20solving%20parametric%20partial%20differential%20equations%2C%20but%20extending%20them%20to%20spherical%20domains%20remains%20challenging%20due%20to%20the%20need%20to%20preserve%20intrinsic%20geometry%20while%20avoiding%20distortions%20that%20break%20rotational%20consistency.%20Existing%20spherical%20operators%20rely%20on%20rotational%20equivariance%20but%20often%20lack%20the%20flexibility%20for%20real-world%20complexity.%20We%20propose%20a%20general%20operator-design%20framework%20based%20on%20the%20designable%20spherical%20Green%27s%20function%20and%20its%20harmonic%20expansion%2C%20establishing%20a%20solid%20operator-theoretic%20foundation%20for%20spherical%20learning.%20Based%20on%20this%2C%20we%20propose%20an%20absolute%20and%20relative%20position-dependent%20Green%27s%20function%20that%20enables%20flexible%20balance%20of%20equivariance%20and%20invariance%20for%20real-world%20modeling.%20The%20resulting%20operator%2C%20Green%27s-function%20Spherical%20Neural%20Operator%20%28GSNO%29%20with%20a%20novel%20spectral%20learning%20method%2C%20can%20adapt%20to%20anisotropic%2C%20constraint-rich%20systems%20while%20retaining%20spectral%20efficiency.%20To%20exploit%20GSNO%2C%20we%20develop%20GSHNet%2C%20a%20hierarchical%20architecture%20that%20combines%20multi-scale%20spectral%20modeling%20with%20spherical%20up-down%20sampling%2C%20enhancing%20global%20feature%20representation.%20Evaluations%20on%20diffusion%20MRI%2C%20shallow%20water%20dynamics%2C%20and%20global%20weather%20forecasting%2C%20GSNO%20and%20GSHNet%20consistently%20outperform%20state-of-the-art%20methods.%20Our%20results%20position%20GSNO%20as%20a%20principled%20and%20general%20framework%20for%20spherical%20operator%20learning%2C%20bridging%20rigorous%20theory%20with%20real-world%20complexity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Spherical%2520Neural%2520Operators%253A%2520Green%2527s%2520Function%2520Formulation%26entry.906535625%3DHao%2520Tang%2520and%2520Hao%2520Chen%2520and%2520Chao%2520Li%26entry.1292438233%3DNeural%2520operators%2520offer%2520powerful%2520approaches%2520for%2520solving%2520parametric%2520partial%2520differential%2520equations%252C%2520but%2520extending%2520them%2520to%2520spherical%2520domains%2520remains%2520challenging%2520due%2520to%2520the%2520need%2520to%2520preserve%2520intrinsic%2520geometry%2520while%2520avoiding%2520distortions%2520that%2520break%2520rotational%2520consistency.%2520Existing%2520spherical%2520operators%2520rely%2520on%2520rotational%2520equivariance%2520but%2520often%2520lack%2520the%2520flexibility%2520for%2520real-world%2520complexity.%2520We%2520propose%2520a%2520general%2520operator-design%2520framework%2520based%2520on%2520the%2520designable%2520spherical%2520Green%2527s%2520function%2520and%2520its%2520harmonic%2520expansion%252C%2520establishing%2520a%2520solid%2520operator-theoretic%2520foundation%2520for%2520spherical%2520learning.%2520Based%2520on%2520this%252C%2520we%2520propose%2520an%2520absolute%2520and%2520relative%2520position-dependent%2520Green%2527s%2520function%2520that%2520enables%2520flexible%2520balance%2520of%2520equivariance%2520and%2520invariance%2520for%2520real-world%2520modeling.%2520The%2520resulting%2520operator%252C%2520Green%2527s-function%2520Spherical%2520Neural%2520Operator%2520%2528GSNO%2529%2520with%2520a%2520novel%2520spectral%2520learning%2520method%252C%2520can%2520adapt%2520to%2520anisotropic%252C%2520constraint-rich%2520systems%2520while%2520retaining%2520spectral%2520efficiency.%2520To%2520exploit%2520GSNO%252C%2520we%2520develop%2520GSHNet%252C%2520a%2520hierarchical%2520architecture%2520that%2520combines%2520multi-scale%2520spectral%2520modeling%2520with%2520spherical%2520up-down%2520sampling%252C%2520enhancing%2520global%2520feature%2520representation.%2520Evaluations%2520on%2520diffusion%2520MRI%252C%2520shallow%2520water%2520dynamics%252C%2520and%2520global%2520weather%2520forecasting%252C%2520GSNO%2520and%2520GSHNet%2520consistently%2520outperform%2520state-of-the-art%2520methods.%2520Our%2520results%2520position%2520GSNO%2520as%2520a%2520principled%2520and%2520general%2520framework%2520for%2520spherical%2520operator%2520learning%252C%2520bridging%2520rigorous%2520theory%2520with%2520real-world%2520complexity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Spherical%20Neural%20Operators%3A%20Green%27s%20Function%20Formulation&entry.906535625=Hao%20Tang%20and%20Hao%20Chen%20and%20Chao%20Li&entry.1292438233=Neural%20operators%20offer%20powerful%20approaches%20for%20solving%20parametric%20partial%20differential%20equations%2C%20but%20extending%20them%20to%20spherical%20domains%20remains%20challenging%20due%20to%20the%20need%20to%20preserve%20intrinsic%20geometry%20while%20avoiding%20distortions%20that%20break%20rotational%20consistency.%20Existing%20spherical%20operators%20rely%20on%20rotational%20equivariance%20but%20often%20lack%20the%20flexibility%20for%20real-world%20complexity.%20We%20propose%20a%20general%20operator-design%20framework%20based%20on%20the%20designable%20spherical%20Green%27s%20function%20and%20its%20harmonic%20expansion%2C%20establishing%20a%20solid%20operator-theoretic%20foundation%20for%20spherical%20learning.%20Based%20on%20this%2C%20we%20propose%20an%20absolute%20and%20relative%20position-dependent%20Green%27s%20function%20that%20enables%20flexible%20balance%20of%20equivariance%20and%20invariance%20for%20real-world%20modeling.%20The%20resulting%20operator%2C%20Green%27s-function%20Spherical%20Neural%20Operator%20%28GSNO%29%20with%20a%20novel%20spectral%20learning%20method%2C%20can%20adapt%20to%20anisotropic%2C%20constraint-rich%20systems%20while%20retaining%20spectral%20efficiency.%20To%20exploit%20GSNO%2C%20we%20develop%20GSHNet%2C%20a%20hierarchical%20architecture%20that%20combines%20multi-scale%20spectral%20modeling%20with%20spherical%20up-down%20sampling%2C%20enhancing%20global%20feature%20representation.%20Evaluations%20on%20diffusion%20MRI%2C%20shallow%20water%20dynamics%2C%20and%20global%20weather%20forecasting%2C%20GSNO%20and%20GSHNet%20consistently%20outperform%20state-of-the-art%20methods.%20Our%20results%20position%20GSNO%20as%20a%20principled%20and%20general%20framework%20for%20spherical%20operator%20learning%2C%20bridging%20rigorous%20theory%20with%20real-world%20complexity.&entry.1838667208=http%3A//arxiv.org/abs/2512.10723v1&entry.124074799=Read"},
{"title": "Virtual camera detection: Catching video injection attacks in remote biometric systems", "author": "Daniyar Kurmankhojayev and Andrei Shadrikov and Dmitrii Gordin and Mikhail Shkorin and Danijar Gabdullin and Aigerim Kambetbayeva and Kanat Kuatov", "abstract": "Face anti-spoofing (FAS) is a vital component of remote biometric authentication systems based on facial recognition, increasingly used across web-based applications. Among emerging threats, video injection attacks -- facilitated by technologies such as deepfakes and virtual camera software -- pose significant challenges to system integrity. While virtual camera detection (VCD) has shown potential as a countermeasure, existing literature offers limited insight into its practical implementation and evaluation. This study introduces a machine learning-based approach to VCD, with a focus on its design and validation. The model is trained on metadata collected during sessions with authentic users. Empirical results demonstrate its effectiveness in identifying video injection attempts and reducing the risk of malicious users bypassing FAS systems.", "link": "http://arxiv.org/abs/2512.10653v1", "date": "2025-12-11", "relevancy": 2.5669, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5641}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4976}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtual%20camera%20detection%3A%20Catching%20video%20injection%20attacks%20in%20remote%20biometric%20systems&body=Title%3A%20Virtual%20camera%20detection%3A%20Catching%20video%20injection%20attacks%20in%20remote%20biometric%20systems%0AAuthor%3A%20Daniyar%20Kurmankhojayev%20and%20Andrei%20Shadrikov%20and%20Dmitrii%20Gordin%20and%20Mikhail%20Shkorin%20and%20Danijar%20Gabdullin%20and%20Aigerim%20Kambetbayeva%20and%20Kanat%20Kuatov%0AAbstract%3A%20Face%20anti-spoofing%20%28FAS%29%20is%20a%20vital%20component%20of%20remote%20biometric%20authentication%20systems%20based%20on%20facial%20recognition%2C%20increasingly%20used%20across%20web-based%20applications.%20Among%20emerging%20threats%2C%20video%20injection%20attacks%20--%20facilitated%20by%20technologies%20such%20as%20deepfakes%20and%20virtual%20camera%20software%20--%20pose%20significant%20challenges%20to%20system%20integrity.%20While%20virtual%20camera%20detection%20%28VCD%29%20has%20shown%20potential%20as%20a%20countermeasure%2C%20existing%20literature%20offers%20limited%20insight%20into%20its%20practical%20implementation%20and%20evaluation.%20This%20study%20introduces%20a%20machine%20learning-based%20approach%20to%20VCD%2C%20with%20a%20focus%20on%20its%20design%20and%20validation.%20The%20model%20is%20trained%20on%20metadata%20collected%20during%20sessions%20with%20authentic%20users.%20Empirical%20results%20demonstrate%20its%20effectiveness%20in%20identifying%20video%20injection%20attempts%20and%20reducing%20the%20risk%20of%20malicious%20users%20bypassing%20FAS%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtual%2520camera%2520detection%253A%2520Catching%2520video%2520injection%2520attacks%2520in%2520remote%2520biometric%2520systems%26entry.906535625%3DDaniyar%2520Kurmankhojayev%2520and%2520Andrei%2520Shadrikov%2520and%2520Dmitrii%2520Gordin%2520and%2520Mikhail%2520Shkorin%2520and%2520Danijar%2520Gabdullin%2520and%2520Aigerim%2520Kambetbayeva%2520and%2520Kanat%2520Kuatov%26entry.1292438233%3DFace%2520anti-spoofing%2520%2528FAS%2529%2520is%2520a%2520vital%2520component%2520of%2520remote%2520biometric%2520authentication%2520systems%2520based%2520on%2520facial%2520recognition%252C%2520increasingly%2520used%2520across%2520web-based%2520applications.%2520Among%2520emerging%2520threats%252C%2520video%2520injection%2520attacks%2520--%2520facilitated%2520by%2520technologies%2520such%2520as%2520deepfakes%2520and%2520virtual%2520camera%2520software%2520--%2520pose%2520significant%2520challenges%2520to%2520system%2520integrity.%2520While%2520virtual%2520camera%2520detection%2520%2528VCD%2529%2520has%2520shown%2520potential%2520as%2520a%2520countermeasure%252C%2520existing%2520literature%2520offers%2520limited%2520insight%2520into%2520its%2520practical%2520implementation%2520and%2520evaluation.%2520This%2520study%2520introduces%2520a%2520machine%2520learning-based%2520approach%2520to%2520VCD%252C%2520with%2520a%2520focus%2520on%2520its%2520design%2520and%2520validation.%2520The%2520model%2520is%2520trained%2520on%2520metadata%2520collected%2520during%2520sessions%2520with%2520authentic%2520users.%2520Empirical%2520results%2520demonstrate%2520its%2520effectiveness%2520in%2520identifying%2520video%2520injection%2520attempts%2520and%2520reducing%2520the%2520risk%2520of%2520malicious%2520users%2520bypassing%2520FAS%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtual%20camera%20detection%3A%20Catching%20video%20injection%20attacks%20in%20remote%20biometric%20systems&entry.906535625=Daniyar%20Kurmankhojayev%20and%20Andrei%20Shadrikov%20and%20Dmitrii%20Gordin%20and%20Mikhail%20Shkorin%20and%20Danijar%20Gabdullin%20and%20Aigerim%20Kambetbayeva%20and%20Kanat%20Kuatov&entry.1292438233=Face%20anti-spoofing%20%28FAS%29%20is%20a%20vital%20component%20of%20remote%20biometric%20authentication%20systems%20based%20on%20facial%20recognition%2C%20increasingly%20used%20across%20web-based%20applications.%20Among%20emerging%20threats%2C%20video%20injection%20attacks%20--%20facilitated%20by%20technologies%20such%20as%20deepfakes%20and%20virtual%20camera%20software%20--%20pose%20significant%20challenges%20to%20system%20integrity.%20While%20virtual%20camera%20detection%20%28VCD%29%20has%20shown%20potential%20as%20a%20countermeasure%2C%20existing%20literature%20offers%20limited%20insight%20into%20its%20practical%20implementation%20and%20evaluation.%20This%20study%20introduces%20a%20machine%20learning-based%20approach%20to%20VCD%2C%20with%20a%20focus%20on%20its%20design%20and%20validation.%20The%20model%20is%20trained%20on%20metadata%20collected%20during%20sessions%20with%20authentic%20users.%20Empirical%20results%20demonstrate%20its%20effectiveness%20in%20identifying%20video%20injection%20attempts%20and%20reducing%20the%20risk%20of%20malicious%20users%20bypassing%20FAS%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.10653v1&entry.124074799=Read"},
{"title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation", "author": "Sharath Girish and Viacheslav Ivanov and Tsai-Shien Chen and Hao Chen and Aliaksandr Siarohin and Sergey Tulyakov", "abstract": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT", "link": "http://arxiv.org/abs/2512.10943v1", "date": "2025-12-11", "relevancy": 2.5594, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6834}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6511}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlcheMinT%3A%20Fine-grained%20Temporal%20Control%20for%20Multi-Reference%20Consistent%20Video%20Generation&body=Title%3A%20AlcheMinT%3A%20Fine-grained%20Temporal%20Control%20for%20Multi-Reference%20Consistent%20Video%20Generation%0AAuthor%3A%20Sharath%20Girish%20and%20Viacheslav%20Ivanov%20and%20Tsai-Shien%20Chen%20and%20Hao%20Chen%20and%20Aliaksandr%20Siarohin%20and%20Sergey%20Tulyakov%0AAbstract%3A%20Recent%20advances%20in%20subject-driven%20video%20generation%20with%20large%20diffusion%20models%20have%20enabled%20personalized%20content%20synthesis%20conditioned%20on%20user-provided%20subjects.%20However%2C%20existing%20methods%20lack%20fine-grained%20temporal%20control%20over%20subject%20appearance%20and%20disappearance%2C%20which%20are%20essential%20for%20applications%20such%20as%20compositional%20video%20synthesis%2C%20storyboarding%2C%20and%20controllable%20animation.%20We%20propose%20AlcheMinT%2C%20a%20unified%20framework%20that%20introduces%20explicit%20timestamps%20conditioning%20for%20subject-driven%20video%20generation.%20Our%20approach%20introduces%20a%20novel%20positional%20encoding%20mechanism%20that%20unlocks%20the%20encoding%20of%20temporal%20intervals%2C%20associated%20in%20our%20case%20with%20subject%20identities%2C%20while%20seamlessly%20integrating%20with%20the%20pretrained%20video%20generation%20model%20positional%20embeddings.%20Additionally%2C%20we%20incorporate%20subject-descriptive%20text%20tokens%20to%20strengthen%20binding%20between%20visual%20identity%20and%20video%20captions%2C%20mitigating%20ambiguity%20during%20generation.%20Through%20token-wise%20concatenation%2C%20AlcheMinT%20avoids%20any%20additional%20cross-attention%20modules%20and%20incurs%20negligible%20parameter%20overhead.%20We%20establish%20a%20benchmark%20evaluating%20multiple%20subject%20identity%20preservation%2C%20video%20fidelity%2C%20and%20temporal%20adherence.%20Experimental%20results%20demonstrate%20that%20AlcheMinT%20achieves%20visual%20quality%20matching%20state-of-the-art%20video%20personalization%20methods%2C%20while%2C%20for%20the%20first%20time%2C%20enabling%20precise%20temporal%20control%20over%20multi-subject%20generation%20within%20videos.%20Project%20page%20is%20at%20https%3A//snap-research.github.io/Video-AlcheMinT%0ALink%3A%20http%3A//arxiv.org/abs/2512.10943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlcheMinT%253A%2520Fine-grained%2520Temporal%2520Control%2520for%2520Multi-Reference%2520Consistent%2520Video%2520Generation%26entry.906535625%3DSharath%2520Girish%2520and%2520Viacheslav%2520Ivanov%2520and%2520Tsai-Shien%2520Chen%2520and%2520Hao%2520Chen%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Sergey%2520Tulyakov%26entry.1292438233%3DRecent%2520advances%2520in%2520subject-driven%2520video%2520generation%2520with%2520large%2520diffusion%2520models%2520have%2520enabled%2520personalized%2520content%2520synthesis%2520conditioned%2520on%2520user-provided%2520subjects.%2520However%252C%2520existing%2520methods%2520lack%2520fine-grained%2520temporal%2520control%2520over%2520subject%2520appearance%2520and%2520disappearance%252C%2520which%2520are%2520essential%2520for%2520applications%2520such%2520as%2520compositional%2520video%2520synthesis%252C%2520storyboarding%252C%2520and%2520controllable%2520animation.%2520We%2520propose%2520AlcheMinT%252C%2520a%2520unified%2520framework%2520that%2520introduces%2520explicit%2520timestamps%2520conditioning%2520for%2520subject-driven%2520video%2520generation.%2520Our%2520approach%2520introduces%2520a%2520novel%2520positional%2520encoding%2520mechanism%2520that%2520unlocks%2520the%2520encoding%2520of%2520temporal%2520intervals%252C%2520associated%2520in%2520our%2520case%2520with%2520subject%2520identities%252C%2520while%2520seamlessly%2520integrating%2520with%2520the%2520pretrained%2520video%2520generation%2520model%2520positional%2520embeddings.%2520Additionally%252C%2520we%2520incorporate%2520subject-descriptive%2520text%2520tokens%2520to%2520strengthen%2520binding%2520between%2520visual%2520identity%2520and%2520video%2520captions%252C%2520mitigating%2520ambiguity%2520during%2520generation.%2520Through%2520token-wise%2520concatenation%252C%2520AlcheMinT%2520avoids%2520any%2520additional%2520cross-attention%2520modules%2520and%2520incurs%2520negligible%2520parameter%2520overhead.%2520We%2520establish%2520a%2520benchmark%2520evaluating%2520multiple%2520subject%2520identity%2520preservation%252C%2520video%2520fidelity%252C%2520and%2520temporal%2520adherence.%2520Experimental%2520results%2520demonstrate%2520that%2520AlcheMinT%2520achieves%2520visual%2520quality%2520matching%2520state-of-the-art%2520video%2520personalization%2520methods%252C%2520while%252C%2520for%2520the%2520first%2520time%252C%2520enabling%2520precise%2520temporal%2520control%2520over%2520multi-subject%2520generation%2520within%2520videos.%2520Project%2520page%2520is%2520at%2520https%253A//snap-research.github.io/Video-AlcheMinT%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlcheMinT%3A%20Fine-grained%20Temporal%20Control%20for%20Multi-Reference%20Consistent%20Video%20Generation&entry.906535625=Sharath%20Girish%20and%20Viacheslav%20Ivanov%20and%20Tsai-Shien%20Chen%20and%20Hao%20Chen%20and%20Aliaksandr%20Siarohin%20and%20Sergey%20Tulyakov&entry.1292438233=Recent%20advances%20in%20subject-driven%20video%20generation%20with%20large%20diffusion%20models%20have%20enabled%20personalized%20content%20synthesis%20conditioned%20on%20user-provided%20subjects.%20However%2C%20existing%20methods%20lack%20fine-grained%20temporal%20control%20over%20subject%20appearance%20and%20disappearance%2C%20which%20are%20essential%20for%20applications%20such%20as%20compositional%20video%20synthesis%2C%20storyboarding%2C%20and%20controllable%20animation.%20We%20propose%20AlcheMinT%2C%20a%20unified%20framework%20that%20introduces%20explicit%20timestamps%20conditioning%20for%20subject-driven%20video%20generation.%20Our%20approach%20introduces%20a%20novel%20positional%20encoding%20mechanism%20that%20unlocks%20the%20encoding%20of%20temporal%20intervals%2C%20associated%20in%20our%20case%20with%20subject%20identities%2C%20while%20seamlessly%20integrating%20with%20the%20pretrained%20video%20generation%20model%20positional%20embeddings.%20Additionally%2C%20we%20incorporate%20subject-descriptive%20text%20tokens%20to%20strengthen%20binding%20between%20visual%20identity%20and%20video%20captions%2C%20mitigating%20ambiguity%20during%20generation.%20Through%20token-wise%20concatenation%2C%20AlcheMinT%20avoids%20any%20additional%20cross-attention%20modules%20and%20incurs%20negligible%20parameter%20overhead.%20We%20establish%20a%20benchmark%20evaluating%20multiple%20subject%20identity%20preservation%2C%20video%20fidelity%2C%20and%20temporal%20adherence.%20Experimental%20results%20demonstrate%20that%20AlcheMinT%20achieves%20visual%20quality%20matching%20state-of-the-art%20video%20personalization%20methods%2C%20while%2C%20for%20the%20first%20time%2C%20enabling%20precise%20temporal%20control%20over%20multi-subject%20generation%20within%20videos.%20Project%20page%20is%20at%20https%3A//snap-research.github.io/Video-AlcheMinT&entry.1838667208=http%3A//arxiv.org/abs/2512.10943v1&entry.124074799=Read"},
{"title": "DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction", "author": "Yifan Song and Fenglin Yu and Yihong Luo and Xingjian Tao and Siya Qiu and Kai Han and Jing Tang", "abstract": "Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.", "link": "http://arxiv.org/abs/2512.06356v2", "date": "2025-12-11", "relevancy": 2.5535, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5197}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5085}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DDFI%3A%20Diverse%20and%20Distribution-aware%20Missing%20Feature%20Imputation%20via%20Two-step%20Reconstruction&body=Title%3A%20DDFI%3A%20Diverse%20and%20Distribution-aware%20Missing%20Feature%20Imputation%20via%20Two-step%20Reconstruction%0AAuthor%3A%20Yifan%20Song%20and%20Fenglin%20Yu%20and%20Yihong%20Luo%20and%20Xingjian%20Tao%20and%20Siya%20Qiu%20and%20Kai%20Han%20and%20Jing%20Tang%0AAbstract%3A%20Incomplete%20node%20features%20are%20ubiquitous%20in%20real-world%20scenarios%2C%20e.g.%2C%20the%20attributes%20of%20web%20users%20may%20be%20partly%20private%2C%20which%20causes%20the%20performance%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20decline%20significantly.%20Feature%20propagation%20%28FP%29%20is%20a%20well-known%20method%20that%20performs%20well%20for%20imputation%20of%20missing%20node%20features%20on%20graphs%2C%20but%20it%20still%20has%20the%20following%20three%20issues%3A%201%29%20it%20struggles%20with%20graphs%20that%20are%20not%20fully%20connected%2C%202%29%20imputed%20features%20face%20the%20over-smoothing%20problem%2C%20and%203%29%20FP%20is%20tailored%20for%20transductive%20tasks%2C%20overlooking%20the%20feature%20distribution%20shift%20in%20inductive%20tasks.%20To%20address%20these%20challenges%2C%20we%20introduce%20DDFI%2C%20a%20Diverse%20and%20Distribution-aware%20Missing%20Feature%20Imputation%20method%20that%20combines%20feature%20propagation%20with%20a%20graph-based%20Masked%20AutoEncoder%20%28MAE%29%20in%20a%20nontrivial%20manner.%20It%20first%20designs%20a%20simple%20yet%20effective%20algorithm%2C%20namely%20Co-Label%20Linking%20%28CLL%29%2C%20that%20randomly%20connects%20nodes%20in%20the%20training%20set%20with%20the%20same%20label%20to%20enhance%20the%20performance%20on%20graphs%20with%20numerous%20connected%20components.%20Then%20we%20develop%20a%20novel%20two-step%20representation%20generation%20process%20at%20the%20inference%20stage.%20Specifically%2C%20instead%20of%20directly%20using%20FP-imputed%20features%20as%20input%20during%20inference%2C%20DDFI%20further%20reconstructs%20the%20features%20through%20the%20whole%20MAE%20to%20reduce%20feature%20distribution%20shift%20in%20the%20inductive%20tasks%20and%20enhance%20the%20diversity%20of%20node%20features.%20Meanwhile%2C%20since%20existing%20feature%20imputation%20methods%20for%20graphs%20only%20evaluate%20by%20simulating%20the%20missing%20scenes%20with%20manually%20masking%20the%20features%2C%20we%20collect%20a%20new%20dataset%20called%20Sailing%20from%20the%20records%20of%20voyages%20that%20contains%20naturally%20missing%20features%20to%20help%20better%20evaluate%20the%20effectiveness.%20Extensive%20experiments%20conducted%20on%20six%20public%20datasets%20and%20Sailing%20show%20that%20DDFI%20outperforms%20the%20state-of-the-art%20methods%20under%20both%20transductive%20and%20inductive%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06356v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDDFI%253A%2520Diverse%2520and%2520Distribution-aware%2520Missing%2520Feature%2520Imputation%2520via%2520Two-step%2520Reconstruction%26entry.906535625%3DYifan%2520Song%2520and%2520Fenglin%2520Yu%2520and%2520Yihong%2520Luo%2520and%2520Xingjian%2520Tao%2520and%2520Siya%2520Qiu%2520and%2520Kai%2520Han%2520and%2520Jing%2520Tang%26entry.1292438233%3DIncomplete%2520node%2520features%2520are%2520ubiquitous%2520in%2520real-world%2520scenarios%252C%2520e.g.%252C%2520the%2520attributes%2520of%2520web%2520users%2520may%2520be%2520partly%2520private%252C%2520which%2520causes%2520the%2520performance%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520decline%2520significantly.%2520Feature%2520propagation%2520%2528FP%2529%2520is%2520a%2520well-known%2520method%2520that%2520performs%2520well%2520for%2520imputation%2520of%2520missing%2520node%2520features%2520on%2520graphs%252C%2520but%2520it%2520still%2520has%2520the%2520following%2520three%2520issues%253A%25201%2529%2520it%2520struggles%2520with%2520graphs%2520that%2520are%2520not%2520fully%2520connected%252C%25202%2529%2520imputed%2520features%2520face%2520the%2520over-smoothing%2520problem%252C%2520and%25203%2529%2520FP%2520is%2520tailored%2520for%2520transductive%2520tasks%252C%2520overlooking%2520the%2520feature%2520distribution%2520shift%2520in%2520inductive%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520DDFI%252C%2520a%2520Diverse%2520and%2520Distribution-aware%2520Missing%2520Feature%2520Imputation%2520method%2520that%2520combines%2520feature%2520propagation%2520with%2520a%2520graph-based%2520Masked%2520AutoEncoder%2520%2528MAE%2529%2520in%2520a%2520nontrivial%2520manner.%2520It%2520first%2520designs%2520a%2520simple%2520yet%2520effective%2520algorithm%252C%2520namely%2520Co-Label%2520Linking%2520%2528CLL%2529%252C%2520that%2520randomly%2520connects%2520nodes%2520in%2520the%2520training%2520set%2520with%2520the%2520same%2520label%2520to%2520enhance%2520the%2520performance%2520on%2520graphs%2520with%2520numerous%2520connected%2520components.%2520Then%2520we%2520develop%2520a%2520novel%2520two-step%2520representation%2520generation%2520process%2520at%2520the%2520inference%2520stage.%2520Specifically%252C%2520instead%2520of%2520directly%2520using%2520FP-imputed%2520features%2520as%2520input%2520during%2520inference%252C%2520DDFI%2520further%2520reconstructs%2520the%2520features%2520through%2520the%2520whole%2520MAE%2520to%2520reduce%2520feature%2520distribution%2520shift%2520in%2520the%2520inductive%2520tasks%2520and%2520enhance%2520the%2520diversity%2520of%2520node%2520features.%2520Meanwhile%252C%2520since%2520existing%2520feature%2520imputation%2520methods%2520for%2520graphs%2520only%2520evaluate%2520by%2520simulating%2520the%2520missing%2520scenes%2520with%2520manually%2520masking%2520the%2520features%252C%2520we%2520collect%2520a%2520new%2520dataset%2520called%2520Sailing%2520from%2520the%2520records%2520of%2520voyages%2520that%2520contains%2520naturally%2520missing%2520features%2520to%2520help%2520better%2520evaluate%2520the%2520effectiveness.%2520Extensive%2520experiments%2520conducted%2520on%2520six%2520public%2520datasets%2520and%2520Sailing%2520show%2520that%2520DDFI%2520outperforms%2520the%2520state-of-the-art%2520methods%2520under%2520both%2520transductive%2520and%2520inductive%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06356v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDFI%3A%20Diverse%20and%20Distribution-aware%20Missing%20Feature%20Imputation%20via%20Two-step%20Reconstruction&entry.906535625=Yifan%20Song%20and%20Fenglin%20Yu%20and%20Yihong%20Luo%20and%20Xingjian%20Tao%20and%20Siya%20Qiu%20and%20Kai%20Han%20and%20Jing%20Tang&entry.1292438233=Incomplete%20node%20features%20are%20ubiquitous%20in%20real-world%20scenarios%2C%20e.g.%2C%20the%20attributes%20of%20web%20users%20may%20be%20partly%20private%2C%20which%20causes%20the%20performance%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20decline%20significantly.%20Feature%20propagation%20%28FP%29%20is%20a%20well-known%20method%20that%20performs%20well%20for%20imputation%20of%20missing%20node%20features%20on%20graphs%2C%20but%20it%20still%20has%20the%20following%20three%20issues%3A%201%29%20it%20struggles%20with%20graphs%20that%20are%20not%20fully%20connected%2C%202%29%20imputed%20features%20face%20the%20over-smoothing%20problem%2C%20and%203%29%20FP%20is%20tailored%20for%20transductive%20tasks%2C%20overlooking%20the%20feature%20distribution%20shift%20in%20inductive%20tasks.%20To%20address%20these%20challenges%2C%20we%20introduce%20DDFI%2C%20a%20Diverse%20and%20Distribution-aware%20Missing%20Feature%20Imputation%20method%20that%20combines%20feature%20propagation%20with%20a%20graph-based%20Masked%20AutoEncoder%20%28MAE%29%20in%20a%20nontrivial%20manner.%20It%20first%20designs%20a%20simple%20yet%20effective%20algorithm%2C%20namely%20Co-Label%20Linking%20%28CLL%29%2C%20that%20randomly%20connects%20nodes%20in%20the%20training%20set%20with%20the%20same%20label%20to%20enhance%20the%20performance%20on%20graphs%20with%20numerous%20connected%20components.%20Then%20we%20develop%20a%20novel%20two-step%20representation%20generation%20process%20at%20the%20inference%20stage.%20Specifically%2C%20instead%20of%20directly%20using%20FP-imputed%20features%20as%20input%20during%20inference%2C%20DDFI%20further%20reconstructs%20the%20features%20through%20the%20whole%20MAE%20to%20reduce%20feature%20distribution%20shift%20in%20the%20inductive%20tasks%20and%20enhance%20the%20diversity%20of%20node%20features.%20Meanwhile%2C%20since%20existing%20feature%20imputation%20methods%20for%20graphs%20only%20evaluate%20by%20simulating%20the%20missing%20scenes%20with%20manually%20masking%20the%20features%2C%20we%20collect%20a%20new%20dataset%20called%20Sailing%20from%20the%20records%20of%20voyages%20that%20contains%20naturally%20missing%20features%20to%20help%20better%20evaluate%20the%20effectiveness.%20Extensive%20experiments%20conducted%20on%20six%20public%20datasets%20and%20Sailing%20show%20that%20DDFI%20outperforms%20the%20state-of-the-art%20methods%20under%20both%20transductive%20and%20inductive%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.06356v2&entry.124074799=Read"},
{"title": "MaskedManipulator: Versatile Whole-Body Manipulation", "author": "Chen Tessler and Yifeng Jiang and Erwin Coumans and Zhengyi Luo and Gal Chechik and Xue Bin Peng", "abstract": "We tackle the challenges of synthesizing versatile, physically simulated human motions for full-body object manipulation. Unlike prior methods that are focused on detailed motion tracking, trajectory following, or teleoperation, our framework enables users to specify versatile high-level objectives such as target object poses or body poses. To achieve this, we introduce MaskedManipulator, a generative control policy distilled from a tracking controller trained on large-scale human motion capture data. This two-stage learning process allows the system to perform complex interaction behaviors, while providing intuitive user control over both character and object motions. MaskedManipulator produces goal-directed manipulation behaviors that expand the scope of interactive animation systems beyond task-specific solutions.", "link": "http://arxiv.org/abs/2505.19086v3", "date": "2025-12-11", "relevancy": 2.537, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6832}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5998}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaskedManipulator%3A%20Versatile%20Whole-Body%20Manipulation&body=Title%3A%20MaskedManipulator%3A%20Versatile%20Whole-Body%20Manipulation%0AAuthor%3A%20Chen%20Tessler%20and%20Yifeng%20Jiang%20and%20Erwin%20Coumans%20and%20Zhengyi%20Luo%20and%20Gal%20Chechik%20and%20Xue%20Bin%20Peng%0AAbstract%3A%20We%20tackle%20the%20challenges%20of%20synthesizing%20versatile%2C%20physically%20simulated%20human%20motions%20for%20full-body%20object%20manipulation.%20Unlike%20prior%20methods%20that%20are%20focused%20on%20detailed%20motion%20tracking%2C%20trajectory%20following%2C%20or%20teleoperation%2C%20our%20framework%20enables%20users%20to%20specify%20versatile%20high-level%20objectives%20such%20as%20target%20object%20poses%20or%20body%20poses.%20To%20achieve%20this%2C%20we%20introduce%20MaskedManipulator%2C%20a%20generative%20control%20policy%20distilled%20from%20a%20tracking%20controller%20trained%20on%20large-scale%20human%20motion%20capture%20data.%20This%20two-stage%20learning%20process%20allows%20the%20system%20to%20perform%20complex%20interaction%20behaviors%2C%20while%20providing%20intuitive%20user%20control%20over%20both%20character%20and%20object%20motions.%20MaskedManipulator%20produces%20goal-directed%20manipulation%20behaviors%20that%20expand%20the%20scope%20of%20interactive%20animation%20systems%20beyond%20task-specific%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2505.19086v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaskedManipulator%253A%2520Versatile%2520Whole-Body%2520Manipulation%26entry.906535625%3DChen%2520Tessler%2520and%2520Yifeng%2520Jiang%2520and%2520Erwin%2520Coumans%2520and%2520Zhengyi%2520Luo%2520and%2520Gal%2520Chechik%2520and%2520Xue%2520Bin%2520Peng%26entry.1292438233%3DWe%2520tackle%2520the%2520challenges%2520of%2520synthesizing%2520versatile%252C%2520physically%2520simulated%2520human%2520motions%2520for%2520full-body%2520object%2520manipulation.%2520Unlike%2520prior%2520methods%2520that%2520are%2520focused%2520on%2520detailed%2520motion%2520tracking%252C%2520trajectory%2520following%252C%2520or%2520teleoperation%252C%2520our%2520framework%2520enables%2520users%2520to%2520specify%2520versatile%2520high-level%2520objectives%2520such%2520as%2520target%2520object%2520poses%2520or%2520body%2520poses.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520MaskedManipulator%252C%2520a%2520generative%2520control%2520policy%2520distilled%2520from%2520a%2520tracking%2520controller%2520trained%2520on%2520large-scale%2520human%2520motion%2520capture%2520data.%2520This%2520two-stage%2520learning%2520process%2520allows%2520the%2520system%2520to%2520perform%2520complex%2520interaction%2520behaviors%252C%2520while%2520providing%2520intuitive%2520user%2520control%2520over%2520both%2520character%2520and%2520object%2520motions.%2520MaskedManipulator%2520produces%2520goal-directed%2520manipulation%2520behaviors%2520that%2520expand%2520the%2520scope%2520of%2520interactive%2520animation%2520systems%2520beyond%2520task-specific%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19086v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskedManipulator%3A%20Versatile%20Whole-Body%20Manipulation&entry.906535625=Chen%20Tessler%20and%20Yifeng%20Jiang%20and%20Erwin%20Coumans%20and%20Zhengyi%20Luo%20and%20Gal%20Chechik%20and%20Xue%20Bin%20Peng&entry.1292438233=We%20tackle%20the%20challenges%20of%20synthesizing%20versatile%2C%20physically%20simulated%20human%20motions%20for%20full-body%20object%20manipulation.%20Unlike%20prior%20methods%20that%20are%20focused%20on%20detailed%20motion%20tracking%2C%20trajectory%20following%2C%20or%20teleoperation%2C%20our%20framework%20enables%20users%20to%20specify%20versatile%20high-level%20objectives%20such%20as%20target%20object%20poses%20or%20body%20poses.%20To%20achieve%20this%2C%20we%20introduce%20MaskedManipulator%2C%20a%20generative%20control%20policy%20distilled%20from%20a%20tracking%20controller%20trained%20on%20large-scale%20human%20motion%20capture%20data.%20This%20two-stage%20learning%20process%20allows%20the%20system%20to%20perform%20complex%20interaction%20behaviors%2C%20while%20providing%20intuitive%20user%20control%20over%20both%20character%20and%20object%20motions.%20MaskedManipulator%20produces%20goal-directed%20manipulation%20behaviors%20that%20expand%20the%20scope%20of%20interactive%20animation%20systems%20beyond%20task-specific%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2505.19086v3&entry.124074799=Read"},
{"title": "Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning", "author": "Benjamin Gundersen and Nicolas Deperrois and Samuel Ruiperez-Campillo and Thomas M. Sutter and Julia E. Vogt and Michael Moor and Farhad Nooralahzadeh and Michael Krauthammer", "abstract": "Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning (\"thinking\") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.", "link": "http://arxiv.org/abs/2512.10691v1", "date": "2025-12-11", "relevancy": 2.5355, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5225}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Radiology%20Report%20Generation%20and%20Visual%20Grounding%20using%20Reinforcement%20Learning&body=Title%3A%20Enhancing%20Radiology%20Report%20Generation%20and%20Visual%20Grounding%20using%20Reinforcement%20Learning%0AAuthor%3A%20Benjamin%20Gundersen%20and%20Nicolas%20Deperrois%20and%20Samuel%20Ruiperez-Campillo%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt%20and%20Michael%20Moor%20and%20Farhad%20Nooralahzadeh%20and%20Michael%20Krauthammer%0AAbstract%3A%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20improved%20Chest%20X-ray%20%28CXR%29%20interpretation%20in%20multiple%20aspects.%20However%2C%20many%20medical%20VLMs%20rely%20solely%20on%20supervised%20fine-tuning%20%28SFT%29%2C%20which%20optimizes%20next-token%20prediction%20without%20evaluating%20answer%20quality.%20In%20contrast%2C%20reinforcement%20learning%20%28RL%29%20can%20incorporate%20task-specific%20feedback%2C%20and%20its%20combination%20with%20explicit%20intermediate%20reasoning%20%28%22thinking%22%29%20has%20demonstrated%20substantial%20gains%20on%20verifiable%20math%20and%20coding%20tasks.%20To%20investigate%20the%20effects%20of%20RL%20and%20thinking%20in%20a%20CXR%20VLM%2C%20we%20perform%20large-scale%20SFT%20on%20CXR%20data%20to%20build%20an%20updated%20RadVLM%20based%20on%20Qwen3-VL%2C%20followed%20by%20a%20cold-start%20SFT%20stage%20that%20equips%20the%20model%20with%20basic%20thinking%20ability.%20We%20then%20apply%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20with%20clinically%20grounded%2C%20task-specific%20rewards%20for%20report%20generation%20and%20visual%20grounding%2C%20and%20run%20matched%20RL%20experiments%20on%20both%20domain-specific%20and%20general-domain%20Qwen3-VL%20variants%2C%20with%20and%20without%20thinking.%20Across%20these%20settings%2C%20we%20find%20that%20while%20strong%20SFT%20remains%20crucial%20for%20high%20base%20performance%2C%20RL%20provides%20additional%20gains%20on%20both%20tasks%2C%20whereas%20explicit%20thinking%20does%20not%20appear%20to%20further%20improve%20results.%20Under%20a%20unified%20evaluation%20pipeline%2C%20the%20RL-optimized%20RadVLM%20models%20outperform%20their%20baseline%20counterparts%20and%20reach%20state-of-the-art%20performance%20on%20both%20report%20generation%20and%20grounding%2C%20highlighting%20clinically%20aligned%20RL%20as%20a%20powerful%20complement%20to%20SFT%20for%20medical%20VLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Radiology%2520Report%2520Generation%2520and%2520Visual%2520Grounding%2520using%2520Reinforcement%2520Learning%26entry.906535625%3DBenjamin%2520Gundersen%2520and%2520Nicolas%2520Deperrois%2520and%2520Samuel%2520Ruiperez-Campillo%2520and%2520Thomas%2520M.%2520Sutter%2520and%2520Julia%2520E.%2520Vogt%2520and%2520Michael%2520Moor%2520and%2520Farhad%2520Nooralahzadeh%2520and%2520Michael%2520Krauthammer%26entry.1292438233%3DRecent%2520advances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520improved%2520Chest%2520X-ray%2520%2528CXR%2529%2520interpretation%2520in%2520multiple%2520aspects.%2520However%252C%2520many%2520medical%2520VLMs%2520rely%2520solely%2520on%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520which%2520optimizes%2520next-token%2520prediction%2520without%2520evaluating%2520answer%2520quality.%2520In%2520contrast%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520can%2520incorporate%2520task-specific%2520feedback%252C%2520and%2520its%2520combination%2520with%2520explicit%2520intermediate%2520reasoning%2520%2528%2522thinking%2522%2529%2520has%2520demonstrated%2520substantial%2520gains%2520on%2520verifiable%2520math%2520and%2520coding%2520tasks.%2520To%2520investigate%2520the%2520effects%2520of%2520RL%2520and%2520thinking%2520in%2520a%2520CXR%2520VLM%252C%2520we%2520perform%2520large-scale%2520SFT%2520on%2520CXR%2520data%2520to%2520build%2520an%2520updated%2520RadVLM%2520based%2520on%2520Qwen3-VL%252C%2520followed%2520by%2520a%2520cold-start%2520SFT%2520stage%2520that%2520equips%2520the%2520model%2520with%2520basic%2520thinking%2520ability.%2520We%2520then%2520apply%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520with%2520clinically%2520grounded%252C%2520task-specific%2520rewards%2520for%2520report%2520generation%2520and%2520visual%2520grounding%252C%2520and%2520run%2520matched%2520RL%2520experiments%2520on%2520both%2520domain-specific%2520and%2520general-domain%2520Qwen3-VL%2520variants%252C%2520with%2520and%2520without%2520thinking.%2520Across%2520these%2520settings%252C%2520we%2520find%2520that%2520while%2520strong%2520SFT%2520remains%2520crucial%2520for%2520high%2520base%2520performance%252C%2520RL%2520provides%2520additional%2520gains%2520on%2520both%2520tasks%252C%2520whereas%2520explicit%2520thinking%2520does%2520not%2520appear%2520to%2520further%2520improve%2520results.%2520Under%2520a%2520unified%2520evaluation%2520pipeline%252C%2520the%2520RL-optimized%2520RadVLM%2520models%2520outperform%2520their%2520baseline%2520counterparts%2520and%2520reach%2520state-of-the-art%2520performance%2520on%2520both%2520report%2520generation%2520and%2520grounding%252C%2520highlighting%2520clinically%2520aligned%2520RL%2520as%2520a%2520powerful%2520complement%2520to%2520SFT%2520for%2520medical%2520VLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Radiology%20Report%20Generation%20and%20Visual%20Grounding%20using%20Reinforcement%20Learning&entry.906535625=Benjamin%20Gundersen%20and%20Nicolas%20Deperrois%20and%20Samuel%20Ruiperez-Campillo%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt%20and%20Michael%20Moor%20and%20Farhad%20Nooralahzadeh%20and%20Michael%20Krauthammer&entry.1292438233=Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20improved%20Chest%20X-ray%20%28CXR%29%20interpretation%20in%20multiple%20aspects.%20However%2C%20many%20medical%20VLMs%20rely%20solely%20on%20supervised%20fine-tuning%20%28SFT%29%2C%20which%20optimizes%20next-token%20prediction%20without%20evaluating%20answer%20quality.%20In%20contrast%2C%20reinforcement%20learning%20%28RL%29%20can%20incorporate%20task-specific%20feedback%2C%20and%20its%20combination%20with%20explicit%20intermediate%20reasoning%20%28%22thinking%22%29%20has%20demonstrated%20substantial%20gains%20on%20verifiable%20math%20and%20coding%20tasks.%20To%20investigate%20the%20effects%20of%20RL%20and%20thinking%20in%20a%20CXR%20VLM%2C%20we%20perform%20large-scale%20SFT%20on%20CXR%20data%20to%20build%20an%20updated%20RadVLM%20based%20on%20Qwen3-VL%2C%20followed%20by%20a%20cold-start%20SFT%20stage%20that%20equips%20the%20model%20with%20basic%20thinking%20ability.%20We%20then%20apply%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20with%20clinically%20grounded%2C%20task-specific%20rewards%20for%20report%20generation%20and%20visual%20grounding%2C%20and%20run%20matched%20RL%20experiments%20on%20both%20domain-specific%20and%20general-domain%20Qwen3-VL%20variants%2C%20with%20and%20without%20thinking.%20Across%20these%20settings%2C%20we%20find%20that%20while%20strong%20SFT%20remains%20crucial%20for%20high%20base%20performance%2C%20RL%20provides%20additional%20gains%20on%20both%20tasks%2C%20whereas%20explicit%20thinking%20does%20not%20appear%20to%20further%20improve%20results.%20Under%20a%20unified%20evaluation%20pipeline%2C%20the%20RL-optimized%20RadVLM%20models%20outperform%20their%20baseline%20counterparts%20and%20reach%20state-of-the-art%20performance%20on%20both%20report%20generation%20and%20grounding%2C%20highlighting%20clinically%20aligned%20RL%20as%20a%20powerful%20complement%20to%20SFT%20for%20medical%20VLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.10691v1&entry.124074799=Read"},
{"title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology", "author": "Akash Kundu and Rishika Goswami", "abstract": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety", "link": "http://arxiv.org/abs/2506.18156v3", "date": "2025-12-11", "relevancy": 2.5185, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Through%20the%20Human%20Lens%3A%20Investigating%20Cognitive%20Theories%20in%20Machine%20Psychology&body=Title%3A%20AI%20Through%20the%20Human%20Lens%3A%20Investigating%20Cognitive%20Theories%20in%20Machine%20Psychology%0AAuthor%3A%20Akash%20Kundu%20and%20Rishika%20Goswami%0AAbstract%3A%20We%20investigate%20whether%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20human-like%20cognitive%20patterns%20under%20four%20established%20frameworks%20from%20psychology%3A%20Thematic%20Apperception%20Test%20%28TAT%29%2C%20Framing%20Bias%2C%20Moral%20Foundations%20Theory%20%28MFT%29%2C%20and%20Cognitive%20Dissonance.%20We%20evaluated%20several%20proprietary%20and%20open-source%20models%20using%20structured%20prompts%20and%20automated%20scoring.%20Our%20findings%20reveal%20that%20these%20models%20often%20produce%20coherent%20narratives%2C%20show%20susceptibility%20to%20positive%20framing%2C%20exhibit%20moral%20judgments%20aligned%20with%20Liberty/Oppression%20concerns%2C%20and%20demonstrate%20self-contradictions%20tempered%20by%20extensive%20rationalization.%20Such%20behaviors%20mirror%20human%20cognitive%20tendencies%20yet%20are%20shaped%20by%20their%20training%20data%20and%20alignment%20methods.%20We%20discuss%20the%20implications%20for%20AI%20transparency%2C%20ethical%20deployment%2C%20and%20future%20work%20that%20bridges%20cognitive%20psychology%20and%20AI%20safety%0ALink%3A%20http%3A//arxiv.org/abs/2506.18156v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Through%2520the%2520Human%2520Lens%253A%2520Investigating%2520Cognitive%2520Theories%2520in%2520Machine%2520Psychology%26entry.906535625%3DAkash%2520Kundu%2520and%2520Rishika%2520Goswami%26entry.1292438233%3DWe%2520investigate%2520whether%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520human-like%2520cognitive%2520patterns%2520under%2520four%2520established%2520frameworks%2520from%2520psychology%253A%2520Thematic%2520Apperception%2520Test%2520%2528TAT%2529%252C%2520Framing%2520Bias%252C%2520Moral%2520Foundations%2520Theory%2520%2528MFT%2529%252C%2520and%2520Cognitive%2520Dissonance.%2520We%2520evaluated%2520several%2520proprietary%2520and%2520open-source%2520models%2520using%2520structured%2520prompts%2520and%2520automated%2520scoring.%2520Our%2520findings%2520reveal%2520that%2520these%2520models%2520often%2520produce%2520coherent%2520narratives%252C%2520show%2520susceptibility%2520to%2520positive%2520framing%252C%2520exhibit%2520moral%2520judgments%2520aligned%2520with%2520Liberty/Oppression%2520concerns%252C%2520and%2520demonstrate%2520self-contradictions%2520tempered%2520by%2520extensive%2520rationalization.%2520Such%2520behaviors%2520mirror%2520human%2520cognitive%2520tendencies%2520yet%2520are%2520shaped%2520by%2520their%2520training%2520data%2520and%2520alignment%2520methods.%2520We%2520discuss%2520the%2520implications%2520for%2520AI%2520transparency%252C%2520ethical%2520deployment%252C%2520and%2520future%2520work%2520that%2520bridges%2520cognitive%2520psychology%2520and%2520AI%2520safety%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18156v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Through%20the%20Human%20Lens%3A%20Investigating%20Cognitive%20Theories%20in%20Machine%20Psychology&entry.906535625=Akash%20Kundu%20and%20Rishika%20Goswami&entry.1292438233=We%20investigate%20whether%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20human-like%20cognitive%20patterns%20under%20four%20established%20frameworks%20from%20psychology%3A%20Thematic%20Apperception%20Test%20%28TAT%29%2C%20Framing%20Bias%2C%20Moral%20Foundations%20Theory%20%28MFT%29%2C%20and%20Cognitive%20Dissonance.%20We%20evaluated%20several%20proprietary%20and%20open-source%20models%20using%20structured%20prompts%20and%20automated%20scoring.%20Our%20findings%20reveal%20that%20these%20models%20often%20produce%20coherent%20narratives%2C%20show%20susceptibility%20to%20positive%20framing%2C%20exhibit%20moral%20judgments%20aligned%20with%20Liberty/Oppression%20concerns%2C%20and%20demonstrate%20self-contradictions%20tempered%20by%20extensive%20rationalization.%20Such%20behaviors%20mirror%20human%20cognitive%20tendencies%20yet%20are%20shaped%20by%20their%20training%20data%20and%20alignment%20methods.%20We%20discuss%20the%20implications%20for%20AI%20transparency%2C%20ethical%20deployment%2C%20and%20future%20work%20that%20bridges%20cognitive%20psychology%20and%20AI%20safety&entry.1838667208=http%3A//arxiv.org/abs/2506.18156v3&entry.124074799=Read"},
{"title": "Salient Object Detection in Complex Weather Conditions via Noise Indicators", "author": "Quan Chen and Xiaokai Yang and Tingyu Wang and Rongfeng Lu and Xichun Sheng and Yaoqi Sun and Chenggang Yan", "abstract": "Salient object detection (SOD), a foundational task in computer vision, has advanced from single-modal to multi-modal paradigms to enhance generalization. However, most existing SOD methods assume low-noise visual conditions, overlooking the degradation of segmentation accuracy caused by weather-induced noise in real-world scenarios. In this paper, we propose a SOD framework tailored for diverse weather conditions, encompassing a specific encoder and a replaceable decoder. To enable handling of varying weather noises, we introduce a one-hot vector as a noise indicator to represent different weather types and design a Noise Indicator Fusion Module (NIFM). The NIFM takes both semantic features and the noise indicator as dual inputs and is inserted between consecutive stages of the encoder to embed weather-aware priors via adaptive feature modulation. Critically, the proposed specific encoder retains compatibility with mainstream SOD decoders. Extensive experiments are conducted on the WXSOD dataset under varying training data scales (100%, 50%, 30% of the full training set), three encoder and seven decoder configurations. Results show that the proposed SOD framework (particularly the NIFM-enhanced specific encoder) improves segmentation accuracy under complex weather conditions compared to a vanilla encoder.", "link": "http://arxiv.org/abs/2512.10592v1", "date": "2025-12-11", "relevancy": 2.5169, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Salient%20Object%20Detection%20in%20Complex%20Weather%20Conditions%20via%20Noise%20Indicators&body=Title%3A%20Salient%20Object%20Detection%20in%20Complex%20Weather%20Conditions%20via%20Noise%20Indicators%0AAuthor%3A%20Quan%20Chen%20and%20Xiaokai%20Yang%20and%20Tingyu%20Wang%20and%20Rongfeng%20Lu%20and%20Xichun%20Sheng%20and%20Yaoqi%20Sun%20and%20Chenggang%20Yan%0AAbstract%3A%20Salient%20object%20detection%20%28SOD%29%2C%20a%20foundational%20task%20in%20computer%20vision%2C%20has%20advanced%20from%20single-modal%20to%20multi-modal%20paradigms%20to%20enhance%20generalization.%20However%2C%20most%20existing%20SOD%20methods%20assume%20low-noise%20visual%20conditions%2C%20overlooking%20the%20degradation%20of%20segmentation%20accuracy%20caused%20by%20weather-induced%20noise%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20SOD%20framework%20tailored%20for%20diverse%20weather%20conditions%2C%20encompassing%20a%20specific%20encoder%20and%20a%20replaceable%20decoder.%20To%20enable%20handling%20of%20varying%20weather%20noises%2C%20we%20introduce%20a%20one-hot%20vector%20as%20a%20noise%20indicator%20to%20represent%20different%20weather%20types%20and%20design%20a%20Noise%20Indicator%20Fusion%20Module%20%28NIFM%29.%20The%20NIFM%20takes%20both%20semantic%20features%20and%20the%20noise%20indicator%20as%20dual%20inputs%20and%20is%20inserted%20between%20consecutive%20stages%20of%20the%20encoder%20to%20embed%20weather-aware%20priors%20via%20adaptive%20feature%20modulation.%20Critically%2C%20the%20proposed%20specific%20encoder%20retains%20compatibility%20with%20mainstream%20SOD%20decoders.%20Extensive%20experiments%20are%20conducted%20on%20the%20WXSOD%20dataset%20under%20varying%20training%20data%20scales%20%28100%25%2C%2050%25%2C%2030%25%20of%20the%20full%20training%20set%29%2C%20three%20encoder%20and%20seven%20decoder%20configurations.%20Results%20show%20that%20the%20proposed%20SOD%20framework%20%28particularly%20the%20NIFM-enhanced%20specific%20encoder%29%20improves%20segmentation%20accuracy%20under%20complex%20weather%20conditions%20compared%20to%20a%20vanilla%20encoder.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSalient%2520Object%2520Detection%2520in%2520Complex%2520Weather%2520Conditions%2520via%2520Noise%2520Indicators%26entry.906535625%3DQuan%2520Chen%2520and%2520Xiaokai%2520Yang%2520and%2520Tingyu%2520Wang%2520and%2520Rongfeng%2520Lu%2520and%2520Xichun%2520Sheng%2520and%2520Yaoqi%2520Sun%2520and%2520Chenggang%2520Yan%26entry.1292438233%3DSalient%2520object%2520detection%2520%2528SOD%2529%252C%2520a%2520foundational%2520task%2520in%2520computer%2520vision%252C%2520has%2520advanced%2520from%2520single-modal%2520to%2520multi-modal%2520paradigms%2520to%2520enhance%2520generalization.%2520However%252C%2520most%2520existing%2520SOD%2520methods%2520assume%2520low-noise%2520visual%2520conditions%252C%2520overlooking%2520the%2520degradation%2520of%2520segmentation%2520accuracy%2520caused%2520by%2520weather-induced%2520noise%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520SOD%2520framework%2520tailored%2520for%2520diverse%2520weather%2520conditions%252C%2520encompassing%2520a%2520specific%2520encoder%2520and%2520a%2520replaceable%2520decoder.%2520To%2520enable%2520handling%2520of%2520varying%2520weather%2520noises%252C%2520we%2520introduce%2520a%2520one-hot%2520vector%2520as%2520a%2520noise%2520indicator%2520to%2520represent%2520different%2520weather%2520types%2520and%2520design%2520a%2520Noise%2520Indicator%2520Fusion%2520Module%2520%2528NIFM%2529.%2520The%2520NIFM%2520takes%2520both%2520semantic%2520features%2520and%2520the%2520noise%2520indicator%2520as%2520dual%2520inputs%2520and%2520is%2520inserted%2520between%2520consecutive%2520stages%2520of%2520the%2520encoder%2520to%2520embed%2520weather-aware%2520priors%2520via%2520adaptive%2520feature%2520modulation.%2520Critically%252C%2520the%2520proposed%2520specific%2520encoder%2520retains%2520compatibility%2520with%2520mainstream%2520SOD%2520decoders.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520the%2520WXSOD%2520dataset%2520under%2520varying%2520training%2520data%2520scales%2520%2528100%2525%252C%252050%2525%252C%252030%2525%2520of%2520the%2520full%2520training%2520set%2529%252C%2520three%2520encoder%2520and%2520seven%2520decoder%2520configurations.%2520Results%2520show%2520that%2520the%2520proposed%2520SOD%2520framework%2520%2528particularly%2520the%2520NIFM-enhanced%2520specific%2520encoder%2529%2520improves%2520segmentation%2520accuracy%2520under%2520complex%2520weather%2520conditions%2520compared%2520to%2520a%2520vanilla%2520encoder.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Salient%20Object%20Detection%20in%20Complex%20Weather%20Conditions%20via%20Noise%20Indicators&entry.906535625=Quan%20Chen%20and%20Xiaokai%20Yang%20and%20Tingyu%20Wang%20and%20Rongfeng%20Lu%20and%20Xichun%20Sheng%20and%20Yaoqi%20Sun%20and%20Chenggang%20Yan&entry.1292438233=Salient%20object%20detection%20%28SOD%29%2C%20a%20foundational%20task%20in%20computer%20vision%2C%20has%20advanced%20from%20single-modal%20to%20multi-modal%20paradigms%20to%20enhance%20generalization.%20However%2C%20most%20existing%20SOD%20methods%20assume%20low-noise%20visual%20conditions%2C%20overlooking%20the%20degradation%20of%20segmentation%20accuracy%20caused%20by%20weather-induced%20noise%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20SOD%20framework%20tailored%20for%20diverse%20weather%20conditions%2C%20encompassing%20a%20specific%20encoder%20and%20a%20replaceable%20decoder.%20To%20enable%20handling%20of%20varying%20weather%20noises%2C%20we%20introduce%20a%20one-hot%20vector%20as%20a%20noise%20indicator%20to%20represent%20different%20weather%20types%20and%20design%20a%20Noise%20Indicator%20Fusion%20Module%20%28NIFM%29.%20The%20NIFM%20takes%20both%20semantic%20features%20and%20the%20noise%20indicator%20as%20dual%20inputs%20and%20is%20inserted%20between%20consecutive%20stages%20of%20the%20encoder%20to%20embed%20weather-aware%20priors%20via%20adaptive%20feature%20modulation.%20Critically%2C%20the%20proposed%20specific%20encoder%20retains%20compatibility%20with%20mainstream%20SOD%20decoders.%20Extensive%20experiments%20are%20conducted%20on%20the%20WXSOD%20dataset%20under%20varying%20training%20data%20scales%20%28100%25%2C%2050%25%2C%2030%25%20of%20the%20full%20training%20set%29%2C%20three%20encoder%20and%20seven%20decoder%20configurations.%20Results%20show%20that%20the%20proposed%20SOD%20framework%20%28particularly%20the%20NIFM-enhanced%20specific%20encoder%29%20improves%20segmentation%20accuracy%20under%20complex%20weather%20conditions%20compared%20to%20a%20vanilla%20encoder.&entry.1838667208=http%3A//arxiv.org/abs/2512.10592v1&entry.124074799=Read"},
{"title": "SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation", "author": "Kehong Gong and Zhengyu Wen and Mingxi Xu and Weixia He and Qi Wang and Ning Zhang and Zhengyu Li and Chenbin Li and Dongze Lian and Wei Zhao and Xiaoyu He and Mingyuan Zhang", "abstract": "Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/", "link": "http://arxiv.org/abs/2512.10860v1", "date": "2025-12-11", "relevancy": 2.5167, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6345}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6305}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWiT-4D%3A%20Sliding-Window%20Transformer%20for%20Lossless%20and%20Parameter-Free%20Temporal%204D%20Generation&body=Title%3A%20SWiT-4D%3A%20Sliding-Window%20Transformer%20for%20Lossless%20and%20Parameter-Free%20Temporal%204D%20Generation%0AAuthor%3A%20Kehong%20Gong%20and%20Zhengyu%20Wen%20and%20Mingxi%20Xu%20and%20Weixia%20He%20and%20Qi%20Wang%20and%20Ning%20Zhang%20and%20Zhengyu%20Li%20and%20Chenbin%20Li%20and%20Dongze%20Lian%20and%20Wei%20Zhao%20and%20Xiaoyu%20He%20and%20Mingyuan%20Zhang%0AAbstract%3A%20Despite%20significant%20progress%20in%204D%20content%20generation%2C%20the%20conversion%20of%20monocular%20videos%20into%20high-quality%20animated%203D%20assets%20with%20explicit%204D%20meshes%20remains%20considerably%20challenging.%20The%20scarcity%20of%20large-scale%2C%20naturally%20captured%204D%20mesh%20datasets%20further%20limits%20the%20ability%20to%20train%20generalizable%20video-to-4D%20models%20from%20scratch%20in%20a%20purely%20data-driven%20manner.%20Meanwhile%2C%20advances%20in%20image-to-3D%20generation%2C%20supported%20by%20extensive%20datasets%2C%20offer%20powerful%20prior%20models%20that%20can%20be%20leveraged.%20To%20better%20utilize%20these%20priors%20while%20minimizing%20reliance%20on%204D%20supervision%2C%20we%20introduce%20SWiT-4D%2C%20a%20Sliding-Window%20Transformer%20for%20lossless%2C%20parameter-free%20temporal%204D%20mesh%20generation.%20SWiT-4D%20integrates%20seamlessly%20with%20any%20Diffusion%20Transformer%20%28DiT%29-based%20image-to-3D%20generator%2C%20adding%20spatial-temporal%20modeling%20across%20video%20frames%20while%20preserving%20the%20original%20single-image%20forward%20process%2C%20enabling%204D%20mesh%20reconstruction%20from%20videos%20of%20arbitrary%20length.%20To%20recover%20global%20translation%2C%20we%20further%20introduce%20an%20optimization-based%20trajectory%20module%20tailored%20for%20static-camera%20monocular%20videos.%20SWiT-4D%20demonstrates%20strong%20data%20efficiency%3A%20with%20only%20a%20single%20short%20%28%3C10s%29%20video%20for%20fine-tuning%2C%20it%20achieves%20high-fidelity%20geometry%20and%20stable%20temporal%20consistency%2C%20indicating%20practical%20deployability%20under%20extremely%20limited%204D%20supervision.%20Comprehensive%20experiments%20on%20both%20in-domain%20zoo-test%20sets%20and%20challenging%20out-of-domain%20benchmarks%20%28C4D%2C%20Objaverse%2C%20and%20in-the-wild%20videos%29%20show%20that%20SWiT-4D%20consistently%20outperforms%20existing%20baselines%20in%20temporal%20smoothness.%20Project%20page%3A%20https%3A//animotionlab.github.io/SWIT4D/%0ALink%3A%20http%3A//arxiv.org/abs/2512.10860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWiT-4D%253A%2520Sliding-Window%2520Transformer%2520for%2520Lossless%2520and%2520Parameter-Free%2520Temporal%25204D%2520Generation%26entry.906535625%3DKehong%2520Gong%2520and%2520Zhengyu%2520Wen%2520and%2520Mingxi%2520Xu%2520and%2520Weixia%2520He%2520and%2520Qi%2520Wang%2520and%2520Ning%2520Zhang%2520and%2520Zhengyu%2520Li%2520and%2520Chenbin%2520Li%2520and%2520Dongze%2520Lian%2520and%2520Wei%2520Zhao%2520and%2520Xiaoyu%2520He%2520and%2520Mingyuan%2520Zhang%26entry.1292438233%3DDespite%2520significant%2520progress%2520in%25204D%2520content%2520generation%252C%2520the%2520conversion%2520of%2520monocular%2520videos%2520into%2520high-quality%2520animated%25203D%2520assets%2520with%2520explicit%25204D%2520meshes%2520remains%2520considerably%2520challenging.%2520The%2520scarcity%2520of%2520large-scale%252C%2520naturally%2520captured%25204D%2520mesh%2520datasets%2520further%2520limits%2520the%2520ability%2520to%2520train%2520generalizable%2520video-to-4D%2520models%2520from%2520scratch%2520in%2520a%2520purely%2520data-driven%2520manner.%2520Meanwhile%252C%2520advances%2520in%2520image-to-3D%2520generation%252C%2520supported%2520by%2520extensive%2520datasets%252C%2520offer%2520powerful%2520prior%2520models%2520that%2520can%2520be%2520leveraged.%2520To%2520better%2520utilize%2520these%2520priors%2520while%2520minimizing%2520reliance%2520on%25204D%2520supervision%252C%2520we%2520introduce%2520SWiT-4D%252C%2520a%2520Sliding-Window%2520Transformer%2520for%2520lossless%252C%2520parameter-free%2520temporal%25204D%2520mesh%2520generation.%2520SWiT-4D%2520integrates%2520seamlessly%2520with%2520any%2520Diffusion%2520Transformer%2520%2528DiT%2529-based%2520image-to-3D%2520generator%252C%2520adding%2520spatial-temporal%2520modeling%2520across%2520video%2520frames%2520while%2520preserving%2520the%2520original%2520single-image%2520forward%2520process%252C%2520enabling%25204D%2520mesh%2520reconstruction%2520from%2520videos%2520of%2520arbitrary%2520length.%2520To%2520recover%2520global%2520translation%252C%2520we%2520further%2520introduce%2520an%2520optimization-based%2520trajectory%2520module%2520tailored%2520for%2520static-camera%2520monocular%2520videos.%2520SWiT-4D%2520demonstrates%2520strong%2520data%2520efficiency%253A%2520with%2520only%2520a%2520single%2520short%2520%2528%253C10s%2529%2520video%2520for%2520fine-tuning%252C%2520it%2520achieves%2520high-fidelity%2520geometry%2520and%2520stable%2520temporal%2520consistency%252C%2520indicating%2520practical%2520deployability%2520under%2520extremely%2520limited%25204D%2520supervision.%2520Comprehensive%2520experiments%2520on%2520both%2520in-domain%2520zoo-test%2520sets%2520and%2520challenging%2520out-of-domain%2520benchmarks%2520%2528C4D%252C%2520Objaverse%252C%2520and%2520in-the-wild%2520videos%2529%2520show%2520that%2520SWiT-4D%2520consistently%2520outperforms%2520existing%2520baselines%2520in%2520temporal%2520smoothness.%2520Project%2520page%253A%2520https%253A//animotionlab.github.io/SWIT4D/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWiT-4D%3A%20Sliding-Window%20Transformer%20for%20Lossless%20and%20Parameter-Free%20Temporal%204D%20Generation&entry.906535625=Kehong%20Gong%20and%20Zhengyu%20Wen%20and%20Mingxi%20Xu%20and%20Weixia%20He%20and%20Qi%20Wang%20and%20Ning%20Zhang%20and%20Zhengyu%20Li%20and%20Chenbin%20Li%20and%20Dongze%20Lian%20and%20Wei%20Zhao%20and%20Xiaoyu%20He%20and%20Mingyuan%20Zhang&entry.1292438233=Despite%20significant%20progress%20in%204D%20content%20generation%2C%20the%20conversion%20of%20monocular%20videos%20into%20high-quality%20animated%203D%20assets%20with%20explicit%204D%20meshes%20remains%20considerably%20challenging.%20The%20scarcity%20of%20large-scale%2C%20naturally%20captured%204D%20mesh%20datasets%20further%20limits%20the%20ability%20to%20train%20generalizable%20video-to-4D%20models%20from%20scratch%20in%20a%20purely%20data-driven%20manner.%20Meanwhile%2C%20advances%20in%20image-to-3D%20generation%2C%20supported%20by%20extensive%20datasets%2C%20offer%20powerful%20prior%20models%20that%20can%20be%20leveraged.%20To%20better%20utilize%20these%20priors%20while%20minimizing%20reliance%20on%204D%20supervision%2C%20we%20introduce%20SWiT-4D%2C%20a%20Sliding-Window%20Transformer%20for%20lossless%2C%20parameter-free%20temporal%204D%20mesh%20generation.%20SWiT-4D%20integrates%20seamlessly%20with%20any%20Diffusion%20Transformer%20%28DiT%29-based%20image-to-3D%20generator%2C%20adding%20spatial-temporal%20modeling%20across%20video%20frames%20while%20preserving%20the%20original%20single-image%20forward%20process%2C%20enabling%204D%20mesh%20reconstruction%20from%20videos%20of%20arbitrary%20length.%20To%20recover%20global%20translation%2C%20we%20further%20introduce%20an%20optimization-based%20trajectory%20module%20tailored%20for%20static-camera%20monocular%20videos.%20SWiT-4D%20demonstrates%20strong%20data%20efficiency%3A%20with%20only%20a%20single%20short%20%28%3C10s%29%20video%20for%20fine-tuning%2C%20it%20achieves%20high-fidelity%20geometry%20and%20stable%20temporal%20consistency%2C%20indicating%20practical%20deployability%20under%20extremely%20limited%204D%20supervision.%20Comprehensive%20experiments%20on%20both%20in-domain%20zoo-test%20sets%20and%20challenging%20out-of-domain%20benchmarks%20%28C4D%2C%20Objaverse%2C%20and%20in-the-wild%20videos%29%20show%20that%20SWiT-4D%20consistently%20outperforms%20existing%20baselines%20in%20temporal%20smoothness.%20Project%20page%3A%20https%3A//animotionlab.github.io/SWIT4D/&entry.1838667208=http%3A//arxiv.org/abs/2512.10860v1&entry.124074799=Read"},
{"title": "HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification", "author": "Mostafa Anoosha and Zeinab Dehghani and Kuniko Paxton and Koorosh Aslansefat and Dhavalkumar Thakker", "abstract": "Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.", "link": "http://arxiv.org/abs/2512.10701v1", "date": "2025-12-11", "relevancy": 2.5041, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5056}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4989}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridVFL%3A%20Disentangled%20Feature%20Learning%20for%20Edge-Enabled%20Vertical%20Federated%20Multimodal%20Classification&body=Title%3A%20HybridVFL%3A%20Disentangled%20Feature%20Learning%20for%20Edge-Enabled%20Vertical%20Federated%20Multimodal%20Classification%0AAuthor%3A%20Mostafa%20Anoosha%20and%20Zeinab%20Dehghani%20and%20Kuniko%20Paxton%20and%20Koorosh%20Aslansefat%20and%20Dhavalkumar%20Thakker%0AAbstract%3A%20Vertical%20Federated%20Learning%20%28VFL%29%20offers%20a%20privacy-preserving%20paradigm%20for%20Edge%20AI%20scenarios%20like%20mobile%20health%20diagnostics%2C%20where%20sensitive%20multimodal%20data%20reside%20on%20distributed%2C%20resource-constrained%20devices.%20Yet%2C%20standard%20VFL%20systems%20often%20suffer%20performance%20limitations%20due%20to%20simplistic%20feature%20fusion.%20This%20paper%20introduces%20HybridVFL%2C%20a%20novel%20framework%20designed%20to%20overcome%20this%20bottleneck%20by%20employing%20client-side%20feature%20disentanglement%20paired%20with%20a%20server-side%20cross-modal%20transformer%20for%20context-aware%20fusion.%20Through%20systematic%20evaluation%20on%20the%20multimodal%20HAM10000%20skin%20lesion%20dataset%2C%20we%20demonstrate%20that%20HybridVFL%20significantly%20outperforms%20standard%20federated%20baselines%2C%20validating%20the%20criticality%20of%20advanced%20fusion%20mechanisms%20in%20robust%2C%20privacy-preserving%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridVFL%253A%2520Disentangled%2520Feature%2520Learning%2520for%2520Edge-Enabled%2520Vertical%2520Federated%2520Multimodal%2520Classification%26entry.906535625%3DMostafa%2520Anoosha%2520and%2520Zeinab%2520Dehghani%2520and%2520Kuniko%2520Paxton%2520and%2520Koorosh%2520Aslansefat%2520and%2520Dhavalkumar%2520Thakker%26entry.1292438233%3DVertical%2520Federated%2520Learning%2520%2528VFL%2529%2520offers%2520a%2520privacy-preserving%2520paradigm%2520for%2520Edge%2520AI%2520scenarios%2520like%2520mobile%2520health%2520diagnostics%252C%2520where%2520sensitive%2520multimodal%2520data%2520reside%2520on%2520distributed%252C%2520resource-constrained%2520devices.%2520Yet%252C%2520standard%2520VFL%2520systems%2520often%2520suffer%2520performance%2520limitations%2520due%2520to%2520simplistic%2520feature%2520fusion.%2520This%2520paper%2520introduces%2520HybridVFL%252C%2520a%2520novel%2520framework%2520designed%2520to%2520overcome%2520this%2520bottleneck%2520by%2520employing%2520client-side%2520feature%2520disentanglement%2520paired%2520with%2520a%2520server-side%2520cross-modal%2520transformer%2520for%2520context-aware%2520fusion.%2520Through%2520systematic%2520evaluation%2520on%2520the%2520multimodal%2520HAM10000%2520skin%2520lesion%2520dataset%252C%2520we%2520demonstrate%2520that%2520HybridVFL%2520significantly%2520outperforms%2520standard%2520federated%2520baselines%252C%2520validating%2520the%2520criticality%2520of%2520advanced%2520fusion%2520mechanisms%2520in%2520robust%252C%2520privacy-preserving%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridVFL%3A%20Disentangled%20Feature%20Learning%20for%20Edge-Enabled%20Vertical%20Federated%20Multimodal%20Classification&entry.906535625=Mostafa%20Anoosha%20and%20Zeinab%20Dehghani%20and%20Kuniko%20Paxton%20and%20Koorosh%20Aslansefat%20and%20Dhavalkumar%20Thakker&entry.1292438233=Vertical%20Federated%20Learning%20%28VFL%29%20offers%20a%20privacy-preserving%20paradigm%20for%20Edge%20AI%20scenarios%20like%20mobile%20health%20diagnostics%2C%20where%20sensitive%20multimodal%20data%20reside%20on%20distributed%2C%20resource-constrained%20devices.%20Yet%2C%20standard%20VFL%20systems%20often%20suffer%20performance%20limitations%20due%20to%20simplistic%20feature%20fusion.%20This%20paper%20introduces%20HybridVFL%2C%20a%20novel%20framework%20designed%20to%20overcome%20this%20bottleneck%20by%20employing%20client-side%20feature%20disentanglement%20paired%20with%20a%20server-side%20cross-modal%20transformer%20for%20context-aware%20fusion.%20Through%20systematic%20evaluation%20on%20the%20multimodal%20HAM10000%20skin%20lesion%20dataset%2C%20we%20demonstrate%20that%20HybridVFL%20significantly%20outperforms%20standard%20federated%20baselines%2C%20validating%20the%20criticality%20of%20advanced%20fusion%20mechanisms%20in%20robust%2C%20privacy-preserving%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.10701v1&entry.124074799=Read"},
{"title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "author": "Zijian Wu and Lingkai Kong and Wenwei Zhang and Songyang Gao and Yuzhe Gu and Zhongrui Cai and Tianyou Ma and Yuhong Liu and Zhi Wang and Runyuan Ma and Guangyu Wang and Wei Li and Conghui He and Dahua Lin and Kai Chen", "abstract": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.", "link": "http://arxiv.org/abs/2512.10756v1", "date": "2025-12-11", "relevancy": 2.5039, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPV%3A%20Outcome-based%20Process%20Verifier%20for%20Efficient%20Long%20Chain-of-Thought%20Verification&body=Title%3A%20OPV%3A%20Outcome-based%20Process%20Verifier%20for%20Efficient%20Long%20Chain-of-Thought%20Verification%0AAuthor%3A%20Zijian%20Wu%20and%20Lingkai%20Kong%20and%20Wenwei%20Zhang%20and%20Songyang%20Gao%20and%20Yuzhe%20Gu%20and%20Zhongrui%20Cai%20and%20Tianyou%20Ma%20and%20Yuhong%20Liu%20and%20Zhi%20Wang%20and%20Runyuan%20Ma%20and%20Guangyu%20Wang%20and%20Wei%20Li%20and%20Conghui%20He%20and%20Dahua%20Lin%20and%20Kai%20Chen%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20significant%20progress%20in%20solving%20complex%20reasoning%20tasks%20by%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29.%20This%20advancement%20is%20also%20inseparable%20from%20the%20oversight%20automated%20by%20reliable%20verifiers.%20However%2C%20current%20outcome-based%20verifiers%20%28OVs%29%20are%20unable%20to%20inspect%20the%20unreliable%20intermediate%20steps%20in%20the%20long%20reasoning%20chains%20of%20thought%20%28CoTs%29.%20Meanwhile%2C%20current%20process-based%20verifiers%20%28PVs%29%20have%20difficulties%20in%20reliably%20detecting%20errors%20in%20the%20complex%20long%20CoTs%2C%20limited%20by%20the%20scarcity%20of%20high-quality%20annotations%20due%20to%20the%20prohibitive%20costs%20of%20human%20annotations.%20Therefore%2C%20we%20propose%20the%20Outcome-based%20Process%20Verifier%20%28OPV%29%2C%20which%20verifies%20the%20rationale%20process%20of%20summarized%20outcomes%20from%20long%20CoTs%20to%20achieve%20both%20accurate%20and%20efficient%20verification%20and%20enable%20large-scale%20annotation.%20To%20empower%20the%20proposed%20verifier%2C%20we%20adopt%20an%20iterative%20active%20learning%20framework%20with%20expert%20annotations%20to%20progressively%20improve%20the%20verification%20capability%20of%20OPV%20with%20fewer%20annotation%20costs.%20Specifically%2C%20in%20each%20iteration%2C%20the%20most%20uncertain%20cases%20of%20the%20current%20best%20OPV%20are%20annotated%20and%20then%20subsequently%20used%20to%20train%20a%20new%20OPV%20through%20Rejection%20Fine-Tuning%20%28RFT%29%20and%20RLVR%20for%20the%20next%20round.%20Extensive%20experiments%20demonstrate%20OPV%27s%20superior%20performance%20and%20broad%20applicability.%20It%20achieves%20new%20state-of-the-art%20results%20on%20our%20held-out%20OPV-Bench%2C%20outperforming%20much%20larger%20open-source%20models%20such%20as%20Qwen3-Max-Preview%20with%20an%20F1%20score%20of%2083.1%20compared%20to%2076.3.%20Furthermore%2C%20OPV%20effectively%20detects%20false%20positives%20within%20synthetic%20dataset%2C%20closely%20align%20with%20expert%20assessment.%20When%20collaborating%20with%20policy%20models%2C%20OPV%20consistently%20yields%20performance%20gains%2C%20e.g.%2C%20raising%20the%20accuracy%20of%20DeepSeek-R1-Distill-Qwen-32B%20from%2055.2%25%20to%2073.3%25%20on%20AIME2025%20as%20the%20compute%20budget%20scales.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPV%253A%2520Outcome-based%2520Process%2520Verifier%2520for%2520Efficient%2520Long%2520Chain-of-Thought%2520Verification%26entry.906535625%3DZijian%2520Wu%2520and%2520Lingkai%2520Kong%2520and%2520Wenwei%2520Zhang%2520and%2520Songyang%2520Gao%2520and%2520Yuzhe%2520Gu%2520and%2520Zhongrui%2520Cai%2520and%2520Tianyou%2520Ma%2520and%2520Yuhong%2520Liu%2520and%2520Zhi%2520Wang%2520and%2520Runyuan%2520Ma%2520and%2520Guangyu%2520Wang%2520and%2520Wei%2520Li%2520and%2520Conghui%2520He%2520and%2520Dahua%2520Lin%2520and%2520Kai%2520Chen%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520significant%2520progress%2520in%2520solving%2520complex%2520reasoning%2520tasks%2520by%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529.%2520This%2520advancement%2520is%2520also%2520inseparable%2520from%2520the%2520oversight%2520automated%2520by%2520reliable%2520verifiers.%2520However%252C%2520current%2520outcome-based%2520verifiers%2520%2528OVs%2529%2520are%2520unable%2520to%2520inspect%2520the%2520unreliable%2520intermediate%2520steps%2520in%2520the%2520long%2520reasoning%2520chains%2520of%2520thought%2520%2528CoTs%2529.%2520Meanwhile%252C%2520current%2520process-based%2520verifiers%2520%2528PVs%2529%2520have%2520difficulties%2520in%2520reliably%2520detecting%2520errors%2520in%2520the%2520complex%2520long%2520CoTs%252C%2520limited%2520by%2520the%2520scarcity%2520of%2520high-quality%2520annotations%2520due%2520to%2520the%2520prohibitive%2520costs%2520of%2520human%2520annotations.%2520Therefore%252C%2520we%2520propose%2520the%2520Outcome-based%2520Process%2520Verifier%2520%2528OPV%2529%252C%2520which%2520verifies%2520the%2520rationale%2520process%2520of%2520summarized%2520outcomes%2520from%2520long%2520CoTs%2520to%2520achieve%2520both%2520accurate%2520and%2520efficient%2520verification%2520and%2520enable%2520large-scale%2520annotation.%2520To%2520empower%2520the%2520proposed%2520verifier%252C%2520we%2520adopt%2520an%2520iterative%2520active%2520learning%2520framework%2520with%2520expert%2520annotations%2520to%2520progressively%2520improve%2520the%2520verification%2520capability%2520of%2520OPV%2520with%2520fewer%2520annotation%2520costs.%2520Specifically%252C%2520in%2520each%2520iteration%252C%2520the%2520most%2520uncertain%2520cases%2520of%2520the%2520current%2520best%2520OPV%2520are%2520annotated%2520and%2520then%2520subsequently%2520used%2520to%2520train%2520a%2520new%2520OPV%2520through%2520Rejection%2520Fine-Tuning%2520%2528RFT%2529%2520and%2520RLVR%2520for%2520the%2520next%2520round.%2520Extensive%2520experiments%2520demonstrate%2520OPV%2527s%2520superior%2520performance%2520and%2520broad%2520applicability.%2520It%2520achieves%2520new%2520state-of-the-art%2520results%2520on%2520our%2520held-out%2520OPV-Bench%252C%2520outperforming%2520much%2520larger%2520open-source%2520models%2520such%2520as%2520Qwen3-Max-Preview%2520with%2520an%2520F1%2520score%2520of%252083.1%2520compared%2520to%252076.3.%2520Furthermore%252C%2520OPV%2520effectively%2520detects%2520false%2520positives%2520within%2520synthetic%2520dataset%252C%2520closely%2520align%2520with%2520expert%2520assessment.%2520When%2520collaborating%2520with%2520policy%2520models%252C%2520OPV%2520consistently%2520yields%2520performance%2520gains%252C%2520e.g.%252C%2520raising%2520the%2520accuracy%2520of%2520DeepSeek-R1-Distill-Qwen-32B%2520from%252055.2%2525%2520to%252073.3%2525%2520on%2520AIME2025%2520as%2520the%2520compute%2520budget%2520scales.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPV%3A%20Outcome-based%20Process%20Verifier%20for%20Efficient%20Long%20Chain-of-Thought%20Verification&entry.906535625=Zijian%20Wu%20and%20Lingkai%20Kong%20and%20Wenwei%20Zhang%20and%20Songyang%20Gao%20and%20Yuzhe%20Gu%20and%20Zhongrui%20Cai%20and%20Tianyou%20Ma%20and%20Yuhong%20Liu%20and%20Zhi%20Wang%20and%20Runyuan%20Ma%20and%20Guangyu%20Wang%20and%20Wei%20Li%20and%20Conghui%20He%20and%20Dahua%20Lin%20and%20Kai%20Chen&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20achieved%20significant%20progress%20in%20solving%20complex%20reasoning%20tasks%20by%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29.%20This%20advancement%20is%20also%20inseparable%20from%20the%20oversight%20automated%20by%20reliable%20verifiers.%20However%2C%20current%20outcome-based%20verifiers%20%28OVs%29%20are%20unable%20to%20inspect%20the%20unreliable%20intermediate%20steps%20in%20the%20long%20reasoning%20chains%20of%20thought%20%28CoTs%29.%20Meanwhile%2C%20current%20process-based%20verifiers%20%28PVs%29%20have%20difficulties%20in%20reliably%20detecting%20errors%20in%20the%20complex%20long%20CoTs%2C%20limited%20by%20the%20scarcity%20of%20high-quality%20annotations%20due%20to%20the%20prohibitive%20costs%20of%20human%20annotations.%20Therefore%2C%20we%20propose%20the%20Outcome-based%20Process%20Verifier%20%28OPV%29%2C%20which%20verifies%20the%20rationale%20process%20of%20summarized%20outcomes%20from%20long%20CoTs%20to%20achieve%20both%20accurate%20and%20efficient%20verification%20and%20enable%20large-scale%20annotation.%20To%20empower%20the%20proposed%20verifier%2C%20we%20adopt%20an%20iterative%20active%20learning%20framework%20with%20expert%20annotations%20to%20progressively%20improve%20the%20verification%20capability%20of%20OPV%20with%20fewer%20annotation%20costs.%20Specifically%2C%20in%20each%20iteration%2C%20the%20most%20uncertain%20cases%20of%20the%20current%20best%20OPV%20are%20annotated%20and%20then%20subsequently%20used%20to%20train%20a%20new%20OPV%20through%20Rejection%20Fine-Tuning%20%28RFT%29%20and%20RLVR%20for%20the%20next%20round.%20Extensive%20experiments%20demonstrate%20OPV%27s%20superior%20performance%20and%20broad%20applicability.%20It%20achieves%20new%20state-of-the-art%20results%20on%20our%20held-out%20OPV-Bench%2C%20outperforming%20much%20larger%20open-source%20models%20such%20as%20Qwen3-Max-Preview%20with%20an%20F1%20score%20of%2083.1%20compared%20to%2076.3.%20Furthermore%2C%20OPV%20effectively%20detects%20false%20positives%20within%20synthetic%20dataset%2C%20closely%20align%20with%20expert%20assessment.%20When%20collaborating%20with%20policy%20models%2C%20OPV%20consistently%20yields%20performance%20gains%2C%20e.g.%2C%20raising%20the%20accuracy%20of%20DeepSeek-R1-Distill-Qwen-32B%20from%2055.2%25%20to%2073.3%25%20on%20AIME2025%20as%20the%20compute%20budget%20scales.&entry.1838667208=http%3A//arxiv.org/abs/2512.10756v1&entry.124074799=Read"},
{"title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction", "author": "Jay Karhade and Nikhil Keetha and Yuchen Zhang and Tanisha Gupta and Akash Sharma and Sebastian Scherer and Deva Ramanan", "abstract": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.", "link": "http://arxiv.org/abs/2512.10935v1", "date": "2025-12-11", "relevancy": 2.485, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any4D%3A%20Unified%20Feed-Forward%20Metric%204D%20Reconstruction&body=Title%3A%20Any4D%3A%20Unified%20Feed-Forward%20Metric%204D%20Reconstruction%0AAuthor%3A%20Jay%20Karhade%20and%20Nikhil%20Keetha%20and%20Yuchen%20Zhang%20and%20Tanisha%20Gupta%20and%20Akash%20Sharma%20and%20Sebastian%20Scherer%20and%20Deva%20Ramanan%0AAbstract%3A%20We%20present%20Any4D%2C%20a%20scalable%20multi-view%20transformer%20for%20metric-scale%2C%20dense%20feed-forward%204D%20reconstruction.%20Any4D%20directly%20generates%20per-pixel%20motion%20and%20geometry%20predictions%20for%20N%20frames%2C%20in%20contrast%20to%20prior%20work%20that%20typically%20focuses%20on%20either%202-view%20dense%20scene%20flow%20or%20sparse%203D%20point%20tracking.%20Moreover%2C%20unlike%20other%20recent%20methods%20for%204D%20reconstruction%20from%20monocular%20RGB%20videos%2C%20Any4D%20can%20process%20additional%20modalities%20and%20sensors%20such%20as%20RGB-D%20frames%2C%20IMU-based%20egomotion%2C%20and%20Radar%20Doppler%20measurements%2C%20when%20available.%20One%20of%20the%20key%20innovations%20that%20allows%20for%20such%20a%20flexible%20framework%20is%20a%20modular%20representation%20of%20a%204D%20scene%3B%20specifically%2C%20per-view%204D%20predictions%20are%20encoded%20using%20a%20variety%20of%20egocentric%20factors%20%28depthmaps%20and%20camera%20intrinsics%29%20represented%20in%20local%20camera%20coordinates%2C%20and%20allocentric%20factors%20%28camera%20extrinsics%20and%20scene%20flow%29%20represented%20in%20global%20world%20coordinates.%20We%20achieve%20superior%20performance%20across%20diverse%20setups%20-%20both%20in%20terms%20of%20accuracy%20%282-3X%20lower%20error%29%20and%20compute%20efficiency%20%2815X%20faster%29%2C%20opening%20avenues%20for%20multiple%20downstream%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny4D%253A%2520Unified%2520Feed-Forward%2520Metric%25204D%2520Reconstruction%26entry.906535625%3DJay%2520Karhade%2520and%2520Nikhil%2520Keetha%2520and%2520Yuchen%2520Zhang%2520and%2520Tanisha%2520Gupta%2520and%2520Akash%2520Sharma%2520and%2520Sebastian%2520Scherer%2520and%2520Deva%2520Ramanan%26entry.1292438233%3DWe%2520present%2520Any4D%252C%2520a%2520scalable%2520multi-view%2520transformer%2520for%2520metric-scale%252C%2520dense%2520feed-forward%25204D%2520reconstruction.%2520Any4D%2520directly%2520generates%2520per-pixel%2520motion%2520and%2520geometry%2520predictions%2520for%2520N%2520frames%252C%2520in%2520contrast%2520to%2520prior%2520work%2520that%2520typically%2520focuses%2520on%2520either%25202-view%2520dense%2520scene%2520flow%2520or%2520sparse%25203D%2520point%2520tracking.%2520Moreover%252C%2520unlike%2520other%2520recent%2520methods%2520for%25204D%2520reconstruction%2520from%2520monocular%2520RGB%2520videos%252C%2520Any4D%2520can%2520process%2520additional%2520modalities%2520and%2520sensors%2520such%2520as%2520RGB-D%2520frames%252C%2520IMU-based%2520egomotion%252C%2520and%2520Radar%2520Doppler%2520measurements%252C%2520when%2520available.%2520One%2520of%2520the%2520key%2520innovations%2520that%2520allows%2520for%2520such%2520a%2520flexible%2520framework%2520is%2520a%2520modular%2520representation%2520of%2520a%25204D%2520scene%253B%2520specifically%252C%2520per-view%25204D%2520predictions%2520are%2520encoded%2520using%2520a%2520variety%2520of%2520egocentric%2520factors%2520%2528depthmaps%2520and%2520camera%2520intrinsics%2529%2520represented%2520in%2520local%2520camera%2520coordinates%252C%2520and%2520allocentric%2520factors%2520%2528camera%2520extrinsics%2520and%2520scene%2520flow%2529%2520represented%2520in%2520global%2520world%2520coordinates.%2520We%2520achieve%2520superior%2520performance%2520across%2520diverse%2520setups%2520-%2520both%2520in%2520terms%2520of%2520accuracy%2520%25282-3X%2520lower%2520error%2529%2520and%2520compute%2520efficiency%2520%252815X%2520faster%2529%252C%2520opening%2520avenues%2520for%2520multiple%2520downstream%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any4D%3A%20Unified%20Feed-Forward%20Metric%204D%20Reconstruction&entry.906535625=Jay%20Karhade%20and%20Nikhil%20Keetha%20and%20Yuchen%20Zhang%20and%20Tanisha%20Gupta%20and%20Akash%20Sharma%20and%20Sebastian%20Scherer%20and%20Deva%20Ramanan&entry.1292438233=We%20present%20Any4D%2C%20a%20scalable%20multi-view%20transformer%20for%20metric-scale%2C%20dense%20feed-forward%204D%20reconstruction.%20Any4D%20directly%20generates%20per-pixel%20motion%20and%20geometry%20predictions%20for%20N%20frames%2C%20in%20contrast%20to%20prior%20work%20that%20typically%20focuses%20on%20either%202-view%20dense%20scene%20flow%20or%20sparse%203D%20point%20tracking.%20Moreover%2C%20unlike%20other%20recent%20methods%20for%204D%20reconstruction%20from%20monocular%20RGB%20videos%2C%20Any4D%20can%20process%20additional%20modalities%20and%20sensors%20such%20as%20RGB-D%20frames%2C%20IMU-based%20egomotion%2C%20and%20Radar%20Doppler%20measurements%2C%20when%20available.%20One%20of%20the%20key%20innovations%20that%20allows%20for%20such%20a%20flexible%20framework%20is%20a%20modular%20representation%20of%20a%204D%20scene%3B%20specifically%2C%20per-view%204D%20predictions%20are%20encoded%20using%20a%20variety%20of%20egocentric%20factors%20%28depthmaps%20and%20camera%20intrinsics%29%20represented%20in%20local%20camera%20coordinates%2C%20and%20allocentric%20factors%20%28camera%20extrinsics%20and%20scene%20flow%29%20represented%20in%20global%20world%20coordinates.%20We%20achieve%20superior%20performance%20across%20diverse%20setups%20-%20both%20in%20terms%20of%20accuracy%20%282-3X%20lower%20error%29%20and%20compute%20efficiency%20%2815X%20faster%29%2C%20opening%20avenues%20for%20multiple%20downstream%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.10935v1&entry.124074799=Read"},
{"title": "LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation", "author": "Lin Du and Lu Bai and Jincheng Li and Lixin Cui and Hangyuan Du and Lichi Zhang and Yuting Chen and Zhao Li", "abstract": "Graph Neural Networks (GNNs) have emerged as a dominant paradigm for graph classification. Specifically, most existing GNNs mainly rely on the message passing strategy between neighbor nodes, where the expressivity is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Although a number of k-WL-based GNNs have been proposed to overcome this limitation, their computational cost increases rapidly with k, significantly restricting the practical applicability. Moreover, since the k-WL models mainly operate on node tuples, these k-WL-based GNNs cannot retain fine-grained node- or edge-level semantics required by attribution methods (e.g., Integrated Gradients), leading to the less interpretable problem. To overcome the above shortcomings, in this paper, we propose a novel Line Graph Aggregation Network (LGAN), that constructs a line graph from the induced subgraph centered at each node to perform the higher-order aggregation. We theoretically prove that the LGAN not only possesses the greater expressive power than the 2-WL under injective aggregation assumptions, but also has lower time complexity. Empirical evaluations on benchmarks demonstrate that the LGAN outperforms state-of-the-art k-WL-based GNNs, while offering better interpretability.", "link": "http://arxiv.org/abs/2512.10735v1", "date": "2025-12-11", "relevancy": 2.4782, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4989}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LGAN%3A%20An%20Efficient%20High-Order%20Graph%20Neural%20Network%20via%20the%20Line%20Graph%20Aggregation&body=Title%3A%20LGAN%3A%20An%20Efficient%20High-Order%20Graph%20Neural%20Network%20via%20the%20Line%20Graph%20Aggregation%0AAuthor%3A%20Lin%20Du%20and%20Lu%20Bai%20and%20Jincheng%20Li%20and%20Lixin%20Cui%20and%20Hangyuan%20Du%20and%20Lichi%20Zhang%20and%20Yuting%20Chen%20and%20Zhao%20Li%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20a%20dominant%20paradigm%20for%20graph%20classification.%20Specifically%2C%20most%20existing%20GNNs%20mainly%20rely%20on%20the%20message%20passing%20strategy%20between%20neighbor%20nodes%2C%20where%20the%20expressivity%20is%20limited%20by%20the%201-dimensional%20Weisfeiler-Lehman%20%281-WL%29%20test.%20Although%20a%20number%20of%20k-WL-based%20GNNs%20have%20been%20proposed%20to%20overcome%20this%20limitation%2C%20their%20computational%20cost%20increases%20rapidly%20with%20k%2C%20significantly%20restricting%20the%20practical%20applicability.%20Moreover%2C%20since%20the%20k-WL%20models%20mainly%20operate%20on%20node%20tuples%2C%20these%20k-WL-based%20GNNs%20cannot%20retain%20fine-grained%20node-%20or%20edge-level%20semantics%20required%20by%20attribution%20methods%20%28e.g.%2C%20Integrated%20Gradients%29%2C%20leading%20to%20the%20less%20interpretable%20problem.%20To%20overcome%20the%20above%20shortcomings%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20Line%20Graph%20Aggregation%20Network%20%28LGAN%29%2C%20that%20constructs%20a%20line%20graph%20from%20the%20induced%20subgraph%20centered%20at%20each%20node%20to%20perform%20the%20higher-order%20aggregation.%20We%20theoretically%20prove%20that%20the%20LGAN%20not%20only%20possesses%20the%20greater%20expressive%20power%20than%20the%202-WL%20under%20injective%20aggregation%20assumptions%2C%20but%20also%20has%20lower%20time%20complexity.%20Empirical%20evaluations%20on%20benchmarks%20demonstrate%20that%20the%20LGAN%20outperforms%20state-of-the-art%20k-WL-based%20GNNs%2C%20while%20offering%20better%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLGAN%253A%2520An%2520Efficient%2520High-Order%2520Graph%2520Neural%2520Network%2520via%2520the%2520Line%2520Graph%2520Aggregation%26entry.906535625%3DLin%2520Du%2520and%2520Lu%2520Bai%2520and%2520Jincheng%2520Li%2520and%2520Lixin%2520Cui%2520and%2520Hangyuan%2520Du%2520and%2520Lichi%2520Zhang%2520and%2520Yuting%2520Chen%2520and%2520Zhao%2520Li%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520a%2520dominant%2520paradigm%2520for%2520graph%2520classification.%2520Specifically%252C%2520most%2520existing%2520GNNs%2520mainly%2520rely%2520on%2520the%2520message%2520passing%2520strategy%2520between%2520neighbor%2520nodes%252C%2520where%2520the%2520expressivity%2520is%2520limited%2520by%2520the%25201-dimensional%2520Weisfeiler-Lehman%2520%25281-WL%2529%2520test.%2520Although%2520a%2520number%2520of%2520k-WL-based%2520GNNs%2520have%2520been%2520proposed%2520to%2520overcome%2520this%2520limitation%252C%2520their%2520computational%2520cost%2520increases%2520rapidly%2520with%2520k%252C%2520significantly%2520restricting%2520the%2520practical%2520applicability.%2520Moreover%252C%2520since%2520the%2520k-WL%2520models%2520mainly%2520operate%2520on%2520node%2520tuples%252C%2520these%2520k-WL-based%2520GNNs%2520cannot%2520retain%2520fine-grained%2520node-%2520or%2520edge-level%2520semantics%2520required%2520by%2520attribution%2520methods%2520%2528e.g.%252C%2520Integrated%2520Gradients%2529%252C%2520leading%2520to%2520the%2520less%2520interpretable%2520problem.%2520To%2520overcome%2520the%2520above%2520shortcomings%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Line%2520Graph%2520Aggregation%2520Network%2520%2528LGAN%2529%252C%2520that%2520constructs%2520a%2520line%2520graph%2520from%2520the%2520induced%2520subgraph%2520centered%2520at%2520each%2520node%2520to%2520perform%2520the%2520higher-order%2520aggregation.%2520We%2520theoretically%2520prove%2520that%2520the%2520LGAN%2520not%2520only%2520possesses%2520the%2520greater%2520expressive%2520power%2520than%2520the%25202-WL%2520under%2520injective%2520aggregation%2520assumptions%252C%2520but%2520also%2520has%2520lower%2520time%2520complexity.%2520Empirical%2520evaluations%2520on%2520benchmarks%2520demonstrate%2520that%2520the%2520LGAN%2520outperforms%2520state-of-the-art%2520k-WL-based%2520GNNs%252C%2520while%2520offering%2520better%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LGAN%3A%20An%20Efficient%20High-Order%20Graph%20Neural%20Network%20via%20the%20Line%20Graph%20Aggregation&entry.906535625=Lin%20Du%20and%20Lu%20Bai%20and%20Jincheng%20Li%20and%20Lixin%20Cui%20and%20Hangyuan%20Du%20and%20Lichi%20Zhang%20and%20Yuting%20Chen%20and%20Zhao%20Li&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20a%20dominant%20paradigm%20for%20graph%20classification.%20Specifically%2C%20most%20existing%20GNNs%20mainly%20rely%20on%20the%20message%20passing%20strategy%20between%20neighbor%20nodes%2C%20where%20the%20expressivity%20is%20limited%20by%20the%201-dimensional%20Weisfeiler-Lehman%20%281-WL%29%20test.%20Although%20a%20number%20of%20k-WL-based%20GNNs%20have%20been%20proposed%20to%20overcome%20this%20limitation%2C%20their%20computational%20cost%20increases%20rapidly%20with%20k%2C%20significantly%20restricting%20the%20practical%20applicability.%20Moreover%2C%20since%20the%20k-WL%20models%20mainly%20operate%20on%20node%20tuples%2C%20these%20k-WL-based%20GNNs%20cannot%20retain%20fine-grained%20node-%20or%20edge-level%20semantics%20required%20by%20attribution%20methods%20%28e.g.%2C%20Integrated%20Gradients%29%2C%20leading%20to%20the%20less%20interpretable%20problem.%20To%20overcome%20the%20above%20shortcomings%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20Line%20Graph%20Aggregation%20Network%20%28LGAN%29%2C%20that%20constructs%20a%20line%20graph%20from%20the%20induced%20subgraph%20centered%20at%20each%20node%20to%20perform%20the%20higher-order%20aggregation.%20We%20theoretically%20prove%20that%20the%20LGAN%20not%20only%20possesses%20the%20greater%20expressive%20power%20than%20the%202-WL%20under%20injective%20aggregation%20assumptions%2C%20but%20also%20has%20lower%20time%20complexity.%20Empirical%20evaluations%20on%20benchmarks%20demonstrate%20that%20the%20LGAN%20outperforms%20state-of-the-art%20k-WL-based%20GNNs%2C%20while%20offering%20better%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2512.10735v1&entry.124074799=Read"},
{"title": "Mr. Virgil: Learning Multi-robot Visual-range Relative Localization", "author": "Si Wang and Zhehan Li and Jiadong Lu and Rong Xiong and Yanjun Cao and Yue Wang", "abstract": "Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.", "link": "http://arxiv.org/abs/2512.10540v1", "date": "2025-12-11", "relevancy": 2.4516, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6147}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6146}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mr.%20Virgil%3A%20Learning%20Multi-robot%20Visual-range%20Relative%20Localization&body=Title%3A%20Mr.%20Virgil%3A%20Learning%20Multi-robot%20Visual-range%20Relative%20Localization%0AAuthor%3A%20Si%20Wang%20and%20Zhehan%20Li%20and%20Jiadong%20Lu%20and%20Rong%20Xiong%20and%20Yanjun%20Cao%20and%20Yue%20Wang%0AAbstract%3A%20Ultra-wideband%20%28UWB%29-vision%20fusion%20localization%20has%20achieved%20extensive%20applications%20in%20the%20domain%20of%20multi-agent%20relative%20localization.%20The%20challenging%20matching%20problem%20between%20robots%20and%20visual%20detection%20renders%20existing%20methods%20highly%20dependent%20on%20identity-encoded%20hardware%20or%20delicate%20tuning%20algorithms.%20Overconfident%20yet%20erroneous%20matches%20may%20bring%20about%20irreversible%20damage%20to%20the%20localization%20system.%20To%20address%20this%20issue%2C%20we%20introduce%20Mr.%20Virgil%2C%20an%20end-to-end%20learning%20multi-robot%20visual-range%20relative%20localization%20framework%2C%20consisting%20of%20a%20graph%20neural%20network%20for%20data%20association%20between%20UWB%20rangings%20and%20visual%20detections%2C%20and%20a%20differentiable%20pose%20graph%20optimization%20%28PGO%29%20back-end.%20The%20graph-based%20front-end%20supplies%20robust%20matching%20results%2C%20accurate%20initial%20position%20predictions%2C%20and%20credible%20uncertainty%20estimates%2C%20which%20are%20subsequently%20integrated%20into%20the%20PGO%20back-end%20to%20elevate%20the%20accuracy%20of%20the%20final%20pose%20estimation.%20Additionally%2C%20a%20decentralized%20system%20is%20implemented%20for%20real-world%20applications.%20Experiments%20spanning%20varying%20robot%20numbers%2C%20simulation%20and%20real-world%2C%20occlusion%20and%20non-occlusion%20conditions%20showcase%20the%20stability%20and%20exactitude%20under%20various%20scenes%20compared%20to%20conventional%20methods.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/HiOnes/Mr-Virgil.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMr.%2520Virgil%253A%2520Learning%2520Multi-robot%2520Visual-range%2520Relative%2520Localization%26entry.906535625%3DSi%2520Wang%2520and%2520Zhehan%2520Li%2520and%2520Jiadong%2520Lu%2520and%2520Rong%2520Xiong%2520and%2520Yanjun%2520Cao%2520and%2520Yue%2520Wang%26entry.1292438233%3DUltra-wideband%2520%2528UWB%2529-vision%2520fusion%2520localization%2520has%2520achieved%2520extensive%2520applications%2520in%2520the%2520domain%2520of%2520multi-agent%2520relative%2520localization.%2520The%2520challenging%2520matching%2520problem%2520between%2520robots%2520and%2520visual%2520detection%2520renders%2520existing%2520methods%2520highly%2520dependent%2520on%2520identity-encoded%2520hardware%2520or%2520delicate%2520tuning%2520algorithms.%2520Overconfident%2520yet%2520erroneous%2520matches%2520may%2520bring%2520about%2520irreversible%2520damage%2520to%2520the%2520localization%2520system.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Mr.%2520Virgil%252C%2520an%2520end-to-end%2520learning%2520multi-robot%2520visual-range%2520relative%2520localization%2520framework%252C%2520consisting%2520of%2520a%2520graph%2520neural%2520network%2520for%2520data%2520association%2520between%2520UWB%2520rangings%2520and%2520visual%2520detections%252C%2520and%2520a%2520differentiable%2520pose%2520graph%2520optimization%2520%2528PGO%2529%2520back-end.%2520The%2520graph-based%2520front-end%2520supplies%2520robust%2520matching%2520results%252C%2520accurate%2520initial%2520position%2520predictions%252C%2520and%2520credible%2520uncertainty%2520estimates%252C%2520which%2520are%2520subsequently%2520integrated%2520into%2520the%2520PGO%2520back-end%2520to%2520elevate%2520the%2520accuracy%2520of%2520the%2520final%2520pose%2520estimation.%2520Additionally%252C%2520a%2520decentralized%2520system%2520is%2520implemented%2520for%2520real-world%2520applications.%2520Experiments%2520spanning%2520varying%2520robot%2520numbers%252C%2520simulation%2520and%2520real-world%252C%2520occlusion%2520and%2520non-occlusion%2520conditions%2520showcase%2520the%2520stability%2520and%2520exactitude%2520under%2520various%2520scenes%2520compared%2520to%2520conventional%2520methods.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/HiOnes/Mr-Virgil.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mr.%20Virgil%3A%20Learning%20Multi-robot%20Visual-range%20Relative%20Localization&entry.906535625=Si%20Wang%20and%20Zhehan%20Li%20and%20Jiadong%20Lu%20and%20Rong%20Xiong%20and%20Yanjun%20Cao%20and%20Yue%20Wang&entry.1292438233=Ultra-wideband%20%28UWB%29-vision%20fusion%20localization%20has%20achieved%20extensive%20applications%20in%20the%20domain%20of%20multi-agent%20relative%20localization.%20The%20challenging%20matching%20problem%20between%20robots%20and%20visual%20detection%20renders%20existing%20methods%20highly%20dependent%20on%20identity-encoded%20hardware%20or%20delicate%20tuning%20algorithms.%20Overconfident%20yet%20erroneous%20matches%20may%20bring%20about%20irreversible%20damage%20to%20the%20localization%20system.%20To%20address%20this%20issue%2C%20we%20introduce%20Mr.%20Virgil%2C%20an%20end-to-end%20learning%20multi-robot%20visual-range%20relative%20localization%20framework%2C%20consisting%20of%20a%20graph%20neural%20network%20for%20data%20association%20between%20UWB%20rangings%20and%20visual%20detections%2C%20and%20a%20differentiable%20pose%20graph%20optimization%20%28PGO%29%20back-end.%20The%20graph-based%20front-end%20supplies%20robust%20matching%20results%2C%20accurate%20initial%20position%20predictions%2C%20and%20credible%20uncertainty%20estimates%2C%20which%20are%20subsequently%20integrated%20into%20the%20PGO%20back-end%20to%20elevate%20the%20accuracy%20of%20the%20final%20pose%20estimation.%20Additionally%2C%20a%20decentralized%20system%20is%20implemented%20for%20real-world%20applications.%20Experiments%20spanning%20varying%20robot%20numbers%2C%20simulation%20and%20real-world%2C%20occlusion%20and%20non-occlusion%20conditions%20showcase%20the%20stability%20and%20exactitude%20under%20various%20scenes%20compared%20to%20conventional%20methods.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/HiOnes/Mr-Virgil.&entry.1838667208=http%3A//arxiv.org/abs/2512.10540v1&entry.124074799=Read"},
{"title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving", "author": "Jiawei Yang and Ziyu Chen and Yurong You and Yan Wang and Yiming Li and Yuxiao Chen and Boyi Li and Boris Ivanovic and Marco Pavone and Yue Wang", "abstract": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.", "link": "http://arxiv.org/abs/2512.10947v1", "date": "2025-12-11", "relevancy": 2.4428, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20and%20Effective%20Multi-Camera%20Encoding%20for%20End-to-End%20Driving&body=Title%3A%20Towards%20Efficient%20and%20Effective%20Multi-Camera%20Encoding%20for%20End-to-End%20Driving%0AAuthor%3A%20Jiawei%20Yang%20and%20Ziyu%20Chen%20and%20Yurong%20You%20and%20Yan%20Wang%20and%20Yiming%20Li%20and%20Yuxiao%20Chen%20and%20Boyi%20Li%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Yue%20Wang%0AAbstract%3A%20We%20present%20Flex%2C%20an%20efficient%20and%20effective%20scene%20encoder%20that%20addresses%20the%20computational%20bottleneck%20of%20processing%20high-volume%20multi-camera%20data%20in%20end-to-end%20autonomous%20driving.%20Flex%20employs%20a%20small%20set%20of%20learnable%20scene%20tokens%20to%20jointly%20encode%20information%20from%20all%20image%20tokens%20across%20different%20cameras%20and%20timesteps.%20By%20design%2C%20our%20approach%20is%20geometry-agnostic%2C%20learning%20a%20compact%20scene%20representation%20directly%20from%20data%20without%20relying%20on%20the%20explicit%203D%20inductive%20biases%2C%20such%20as%20Bird-Eye-View%20%28BEV%29%2C%20occupancy%20or%20tri-plane%20representations%2C%20which%20are%20common%20in%20prior%20work.%20This%20holistic%20encoding%20strategy%20aggressively%20compresses%20the%20visual%20input%20for%20the%20downstream%20Large%20Language%20Model%20%28LLM%29%20based%20policy%20model.%20Evaluated%20on%20a%20large-scale%20proprietary%20dataset%20of%2020%2C000%20driving%20hours%2C%20our%20Flex%20achieves%202.2x%20greater%20inference%20throughput%20while%20improving%20driving%20performance%20by%20a%20large%20margin%20compared%20to%20state-of-the-art%20methods.%20Furthermore%2C%20we%20show%20that%20these%20compact%20scene%20tokens%20develop%20an%20emergent%20capability%20for%20scene%20decomposition%20without%20any%20explicit%20supervision.%20Our%20findings%20challenge%20the%20prevailing%20assumption%20that%203D%20priors%20are%20necessary%2C%20demonstrating%20that%20a%20data-driven%2C%20joint%20encoding%20strategy%20offers%20a%20more%20scalable%2C%20efficient%20and%20effective%20path%20for%20future%20autonomous%20driving%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520and%2520Effective%2520Multi-Camera%2520Encoding%2520for%2520End-to-End%2520Driving%26entry.906535625%3DJiawei%2520Yang%2520and%2520Ziyu%2520Chen%2520and%2520Yurong%2520You%2520and%2520Yan%2520Wang%2520and%2520Yiming%2520Li%2520and%2520Yuxiao%2520Chen%2520and%2520Boyi%2520Li%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Yue%2520Wang%26entry.1292438233%3DWe%2520present%2520Flex%252C%2520an%2520efficient%2520and%2520effective%2520scene%2520encoder%2520that%2520addresses%2520the%2520computational%2520bottleneck%2520of%2520processing%2520high-volume%2520multi-camera%2520data%2520in%2520end-to-end%2520autonomous%2520driving.%2520Flex%2520employs%2520a%2520small%2520set%2520of%2520learnable%2520scene%2520tokens%2520to%2520jointly%2520encode%2520information%2520from%2520all%2520image%2520tokens%2520across%2520different%2520cameras%2520and%2520timesteps.%2520By%2520design%252C%2520our%2520approach%2520is%2520geometry-agnostic%252C%2520learning%2520a%2520compact%2520scene%2520representation%2520directly%2520from%2520data%2520without%2520relying%2520on%2520the%2520explicit%25203D%2520inductive%2520biases%252C%2520such%2520as%2520Bird-Eye-View%2520%2528BEV%2529%252C%2520occupancy%2520or%2520tri-plane%2520representations%252C%2520which%2520are%2520common%2520in%2520prior%2520work.%2520This%2520holistic%2520encoding%2520strategy%2520aggressively%2520compresses%2520the%2520visual%2520input%2520for%2520the%2520downstream%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520policy%2520model.%2520Evaluated%2520on%2520a%2520large-scale%2520proprietary%2520dataset%2520of%252020%252C000%2520driving%2520hours%252C%2520our%2520Flex%2520achieves%25202.2x%2520greater%2520inference%2520throughput%2520while%2520improving%2520driving%2520performance%2520by%2520a%2520large%2520margin%2520compared%2520to%2520state-of-the-art%2520methods.%2520Furthermore%252C%2520we%2520show%2520that%2520these%2520compact%2520scene%2520tokens%2520develop%2520an%2520emergent%2520capability%2520for%2520scene%2520decomposition%2520without%2520any%2520explicit%2520supervision.%2520Our%2520findings%2520challenge%2520the%2520prevailing%2520assumption%2520that%25203D%2520priors%2520are%2520necessary%252C%2520demonstrating%2520that%2520a%2520data-driven%252C%2520joint%2520encoding%2520strategy%2520offers%2520a%2520more%2520scalable%252C%2520efficient%2520and%2520effective%2520path%2520for%2520future%2520autonomous%2520driving%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20and%20Effective%20Multi-Camera%20Encoding%20for%20End-to-End%20Driving&entry.906535625=Jiawei%20Yang%20and%20Ziyu%20Chen%20and%20Yurong%20You%20and%20Yan%20Wang%20and%20Yiming%20Li%20and%20Yuxiao%20Chen%20and%20Boyi%20Li%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Yue%20Wang&entry.1292438233=We%20present%20Flex%2C%20an%20efficient%20and%20effective%20scene%20encoder%20that%20addresses%20the%20computational%20bottleneck%20of%20processing%20high-volume%20multi-camera%20data%20in%20end-to-end%20autonomous%20driving.%20Flex%20employs%20a%20small%20set%20of%20learnable%20scene%20tokens%20to%20jointly%20encode%20information%20from%20all%20image%20tokens%20across%20different%20cameras%20and%20timesteps.%20By%20design%2C%20our%20approach%20is%20geometry-agnostic%2C%20learning%20a%20compact%20scene%20representation%20directly%20from%20data%20without%20relying%20on%20the%20explicit%203D%20inductive%20biases%2C%20such%20as%20Bird-Eye-View%20%28BEV%29%2C%20occupancy%20or%20tri-plane%20representations%2C%20which%20are%20common%20in%20prior%20work.%20This%20holistic%20encoding%20strategy%20aggressively%20compresses%20the%20visual%20input%20for%20the%20downstream%20Large%20Language%20Model%20%28LLM%29%20based%20policy%20model.%20Evaluated%20on%20a%20large-scale%20proprietary%20dataset%20of%2020%2C000%20driving%20hours%2C%20our%20Flex%20achieves%202.2x%20greater%20inference%20throughput%20while%20improving%20driving%20performance%20by%20a%20large%20margin%20compared%20to%20state-of-the-art%20methods.%20Furthermore%2C%20we%20show%20that%20these%20compact%20scene%20tokens%20develop%20an%20emergent%20capability%20for%20scene%20decomposition%20without%20any%20explicit%20supervision.%20Our%20findings%20challenge%20the%20prevailing%20assumption%20that%203D%20priors%20are%20necessary%2C%20demonstrating%20that%20a%20data-driven%2C%20joint%20encoding%20strategy%20offers%20a%20more%20scalable%2C%20efficient%20and%20effective%20path%20for%20future%20autonomous%20driving%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.10947v1&entry.124074799=Read"},
{"title": "Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration", "author": "Ahmed Khaled and Satyen Kale and Arthur Douillard and Chi Jin and Rob Fergus and Manzil Zaheer", "abstract": "Modern machine learning often requires training with large batch size, distributed data, and massively parallel compute hardware (like mobile and other edge devices or distributed data centers). Communication becomes a major bottleneck in such settings but methods like Local Stochastic Gradient Descent (Local SGD) show great promise in reducing this additional communication overhead. Local SGD consists of three parts: a local optimization process, an aggregation mechanism, and an outer optimizer that uses the aggregated updates from the nodes to produce a new model. While there exists an extensive literature on understanding the impact of hyperparameters in the local optimization process, the choice of outer optimizer and its hyperparameters is less clear. We study the role of the outer optimizer in Local SGD, and prove new convergence guarantees for the algorithm. In particular, we show that tuning the outer learning rate allows us to (a) trade off between optimization error and stochastic gradient noise variance, and (b) make up for ill-tuning of the inner learning rate. Our theory suggests that the outer learning rate should sometimes be set to values greater than $1$. We extend our results to settings where we use momentum in the outer optimizer, and we show a similar role for the momentum-adjusted outer learning rate. We also study acceleration in the outer optimizer and show that it improves the convergence rate as a function of the number of communication rounds, improving upon the convergence rate of prior algorithms that apply acceleration locally. Finally, we also introduce a novel data-dependent analysis of Local SGD that yields further insights on outer learning rate tuning. We conduct comprehensive experiments with standard language models and various outer optimizers to validate our theory.", "link": "http://arxiv.org/abs/2509.10439v2", "date": "2025-12-11", "relevancy": 2.441, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.509}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4829}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Outer%20Optimizers%20in%20Local%20SGD%3A%20Learning%20Rates%2C%20Momentum%2C%20and%20Acceleration&body=Title%3A%20Understanding%20Outer%20Optimizers%20in%20Local%20SGD%3A%20Learning%20Rates%2C%20Momentum%2C%20and%20Acceleration%0AAuthor%3A%20Ahmed%20Khaled%20and%20Satyen%20Kale%20and%20Arthur%20Douillard%20and%20Chi%20Jin%20and%20Rob%20Fergus%20and%20Manzil%20Zaheer%0AAbstract%3A%20Modern%20machine%20learning%20often%20requires%20training%20with%20large%20batch%20size%2C%20distributed%20data%2C%20and%20massively%20parallel%20compute%20hardware%20%28like%20mobile%20and%20other%20edge%20devices%20or%20distributed%20data%20centers%29.%20Communication%20becomes%20a%20major%20bottleneck%20in%20such%20settings%20but%20methods%20like%20Local%20Stochastic%20Gradient%20Descent%20%28Local%20SGD%29%20show%20great%20promise%20in%20reducing%20this%20additional%20communication%20overhead.%20Local%20SGD%20consists%20of%20three%20parts%3A%20a%20local%20optimization%20process%2C%20an%20aggregation%20mechanism%2C%20and%20an%20outer%20optimizer%20that%20uses%20the%20aggregated%20updates%20from%20the%20nodes%20to%20produce%20a%20new%20model.%20While%20there%20exists%20an%20extensive%20literature%20on%20understanding%20the%20impact%20of%20hyperparameters%20in%20the%20local%20optimization%20process%2C%20the%20choice%20of%20outer%20optimizer%20and%20its%20hyperparameters%20is%20less%20clear.%20We%20study%20the%20role%20of%20the%20outer%20optimizer%20in%20Local%20SGD%2C%20and%20prove%20new%20convergence%20guarantees%20for%20the%20algorithm.%20In%20particular%2C%20we%20show%20that%20tuning%20the%20outer%20learning%20rate%20allows%20us%20to%20%28a%29%20trade%20off%20between%20optimization%20error%20and%20stochastic%20gradient%20noise%20variance%2C%20and%20%28b%29%20make%20up%20for%20ill-tuning%20of%20the%20inner%20learning%20rate.%20Our%20theory%20suggests%20that%20the%20outer%20learning%20rate%20should%20sometimes%20be%20set%20to%20values%20greater%20than%20%241%24.%20We%20extend%20our%20results%20to%20settings%20where%20we%20use%20momentum%20in%20the%20outer%20optimizer%2C%20and%20we%20show%20a%20similar%20role%20for%20the%20momentum-adjusted%20outer%20learning%20rate.%20We%20also%20study%20acceleration%20in%20the%20outer%20optimizer%20and%20show%20that%20it%20improves%20the%20convergence%20rate%20as%20a%20function%20of%20the%20number%20of%20communication%20rounds%2C%20improving%20upon%20the%20convergence%20rate%20of%20prior%20algorithms%20that%20apply%20acceleration%20locally.%20Finally%2C%20we%20also%20introduce%20a%20novel%20data-dependent%20analysis%20of%20Local%20SGD%20that%20yields%20further%20insights%20on%20outer%20learning%20rate%20tuning.%20We%20conduct%20comprehensive%20experiments%20with%20standard%20language%20models%20and%20various%20outer%20optimizers%20to%20validate%20our%20theory.%0ALink%3A%20http%3A//arxiv.org/abs/2509.10439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Outer%2520Optimizers%2520in%2520Local%2520SGD%253A%2520Learning%2520Rates%252C%2520Momentum%252C%2520and%2520Acceleration%26entry.906535625%3DAhmed%2520Khaled%2520and%2520Satyen%2520Kale%2520and%2520Arthur%2520Douillard%2520and%2520Chi%2520Jin%2520and%2520Rob%2520Fergus%2520and%2520Manzil%2520Zaheer%26entry.1292438233%3DModern%2520machine%2520learning%2520often%2520requires%2520training%2520with%2520large%2520batch%2520size%252C%2520distributed%2520data%252C%2520and%2520massively%2520parallel%2520compute%2520hardware%2520%2528like%2520mobile%2520and%2520other%2520edge%2520devices%2520or%2520distributed%2520data%2520centers%2529.%2520Communication%2520becomes%2520a%2520major%2520bottleneck%2520in%2520such%2520settings%2520but%2520methods%2520like%2520Local%2520Stochastic%2520Gradient%2520Descent%2520%2528Local%2520SGD%2529%2520show%2520great%2520promise%2520in%2520reducing%2520this%2520additional%2520communication%2520overhead.%2520Local%2520SGD%2520consists%2520of%2520three%2520parts%253A%2520a%2520local%2520optimization%2520process%252C%2520an%2520aggregation%2520mechanism%252C%2520and%2520an%2520outer%2520optimizer%2520that%2520uses%2520the%2520aggregated%2520updates%2520from%2520the%2520nodes%2520to%2520produce%2520a%2520new%2520model.%2520While%2520there%2520exists%2520an%2520extensive%2520literature%2520on%2520understanding%2520the%2520impact%2520of%2520hyperparameters%2520in%2520the%2520local%2520optimization%2520process%252C%2520the%2520choice%2520of%2520outer%2520optimizer%2520and%2520its%2520hyperparameters%2520is%2520less%2520clear.%2520We%2520study%2520the%2520role%2520of%2520the%2520outer%2520optimizer%2520in%2520Local%2520SGD%252C%2520and%2520prove%2520new%2520convergence%2520guarantees%2520for%2520the%2520algorithm.%2520In%2520particular%252C%2520we%2520show%2520that%2520tuning%2520the%2520outer%2520learning%2520rate%2520allows%2520us%2520to%2520%2528a%2529%2520trade%2520off%2520between%2520optimization%2520error%2520and%2520stochastic%2520gradient%2520noise%2520variance%252C%2520and%2520%2528b%2529%2520make%2520up%2520for%2520ill-tuning%2520of%2520the%2520inner%2520learning%2520rate.%2520Our%2520theory%2520suggests%2520that%2520the%2520outer%2520learning%2520rate%2520should%2520sometimes%2520be%2520set%2520to%2520values%2520greater%2520than%2520%25241%2524.%2520We%2520extend%2520our%2520results%2520to%2520settings%2520where%2520we%2520use%2520momentum%2520in%2520the%2520outer%2520optimizer%252C%2520and%2520we%2520show%2520a%2520similar%2520role%2520for%2520the%2520momentum-adjusted%2520outer%2520learning%2520rate.%2520We%2520also%2520study%2520acceleration%2520in%2520the%2520outer%2520optimizer%2520and%2520show%2520that%2520it%2520improves%2520the%2520convergence%2520rate%2520as%2520a%2520function%2520of%2520the%2520number%2520of%2520communication%2520rounds%252C%2520improving%2520upon%2520the%2520convergence%2520rate%2520of%2520prior%2520algorithms%2520that%2520apply%2520acceleration%2520locally.%2520Finally%252C%2520we%2520also%2520introduce%2520a%2520novel%2520data-dependent%2520analysis%2520of%2520Local%2520SGD%2520that%2520yields%2520further%2520insights%2520on%2520outer%2520learning%2520rate%2520tuning.%2520We%2520conduct%2520comprehensive%2520experiments%2520with%2520standard%2520language%2520models%2520and%2520various%2520outer%2520optimizers%2520to%2520validate%2520our%2520theory.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Outer%20Optimizers%20in%20Local%20SGD%3A%20Learning%20Rates%2C%20Momentum%2C%20and%20Acceleration&entry.906535625=Ahmed%20Khaled%20and%20Satyen%20Kale%20and%20Arthur%20Douillard%20and%20Chi%20Jin%20and%20Rob%20Fergus%20and%20Manzil%20Zaheer&entry.1292438233=Modern%20machine%20learning%20often%20requires%20training%20with%20large%20batch%20size%2C%20distributed%20data%2C%20and%20massively%20parallel%20compute%20hardware%20%28like%20mobile%20and%20other%20edge%20devices%20or%20distributed%20data%20centers%29.%20Communication%20becomes%20a%20major%20bottleneck%20in%20such%20settings%20but%20methods%20like%20Local%20Stochastic%20Gradient%20Descent%20%28Local%20SGD%29%20show%20great%20promise%20in%20reducing%20this%20additional%20communication%20overhead.%20Local%20SGD%20consists%20of%20three%20parts%3A%20a%20local%20optimization%20process%2C%20an%20aggregation%20mechanism%2C%20and%20an%20outer%20optimizer%20that%20uses%20the%20aggregated%20updates%20from%20the%20nodes%20to%20produce%20a%20new%20model.%20While%20there%20exists%20an%20extensive%20literature%20on%20understanding%20the%20impact%20of%20hyperparameters%20in%20the%20local%20optimization%20process%2C%20the%20choice%20of%20outer%20optimizer%20and%20its%20hyperparameters%20is%20less%20clear.%20We%20study%20the%20role%20of%20the%20outer%20optimizer%20in%20Local%20SGD%2C%20and%20prove%20new%20convergence%20guarantees%20for%20the%20algorithm.%20In%20particular%2C%20we%20show%20that%20tuning%20the%20outer%20learning%20rate%20allows%20us%20to%20%28a%29%20trade%20off%20between%20optimization%20error%20and%20stochastic%20gradient%20noise%20variance%2C%20and%20%28b%29%20make%20up%20for%20ill-tuning%20of%20the%20inner%20learning%20rate.%20Our%20theory%20suggests%20that%20the%20outer%20learning%20rate%20should%20sometimes%20be%20set%20to%20values%20greater%20than%20%241%24.%20We%20extend%20our%20results%20to%20settings%20where%20we%20use%20momentum%20in%20the%20outer%20optimizer%2C%20and%20we%20show%20a%20similar%20role%20for%20the%20momentum-adjusted%20outer%20learning%20rate.%20We%20also%20study%20acceleration%20in%20the%20outer%20optimizer%20and%20show%20that%20it%20improves%20the%20convergence%20rate%20as%20a%20function%20of%20the%20number%20of%20communication%20rounds%2C%20improving%20upon%20the%20convergence%20rate%20of%20prior%20algorithms%20that%20apply%20acceleration%20locally.%20Finally%2C%20we%20also%20introduce%20a%20novel%20data-dependent%20analysis%20of%20Local%20SGD%20that%20yields%20further%20insights%20on%20outer%20learning%20rate%20tuning.%20We%20conduct%20comprehensive%20experiments%20with%20standard%20language%20models%20and%20various%20outer%20optimizers%20to%20validate%20our%20theory.&entry.1838667208=http%3A//arxiv.org/abs/2509.10439v2&entry.124074799=Read"},
{"title": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis", "author": "Zijian Gu and Yuxi Liu and Zhenhao Zhang and Song Wang", "abstract": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms. Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.", "link": "http://arxiv.org/abs/2512.03477v2", "date": "2025-12-11", "relevancy": 2.4246, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.485}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness-Aware%20Fine-Tuning%20of%20Vision-Language%20Models%20for%20Medical%20Glaucoma%20Diagnosis&body=Title%3A%20Fairness-Aware%20Fine-Tuning%20of%20Vision-Language%20Models%20for%20Medical%20Glaucoma%20Diagnosis%0AAuthor%3A%20Zijian%20Gu%20and%20Yuxi%20Liu%20and%20Zhenhao%20Zhang%20and%20Song%20Wang%0AAbstract%3A%20Vision-language%20models%20achieve%20expert-level%20performance%20on%20medical%20imaging%20tasks%20but%20exhibit%20significant%20diagnostic%20accuracy%20disparities%20across%20demographic%20groups.%20We%20introduce%20fairness-aware%20Low-Rank%20Adaptation%20for%20medical%20VLMs%2C%20combining%20parameter%20efficiency%20with%20explicit%20fairness%20optimization.%20Our%20key%20algorithmic%20contribution%20is%20a%20differentiable%20MaxAccGap%20loss%20that%20enables%20end-to-end%20optimization%20of%20accuracy%20parity%20across%20demographic%20groups.%20We%20propose%20three%20methods%3A%20FR-LoRA%20integrates%20MaxAccGap%20regularization%20into%20the%20training%20objective%2C%20GR-LoRA%20applies%20inverse%20frequency%20weighting%20to%20balance%20gradient%20contributions%2C%20and%20Hybrid-LoRA%20combines%20both%20mechanisms.%20Evaluated%20on%2010%2C000%20glaucoma%20fundus%20images%2C%20GR-LoRA%20reduces%20diagnostic%20accuracy%20disparities%20by%2069%25%20while%20maintaining%2053.15%25%20overall%20accuracy.%20Ablation%20studies%20reveal%20that%20strong%20regularization%20strength%20achieves%20optimal%20fairness%20with%20minimal%20accuracy%20trade-off%2C%20and%20race-specific%20optimization%20yields%2060%25%20disparity%20reduction.%20Our%20approach%20requires%20only%200.24%25%20trainable%20parameters%2C%20enabling%20practical%20deployment%20of%20fair%20medical%20AI%20in%20resource-constrained%20healthcare%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness-Aware%2520Fine-Tuning%2520of%2520Vision-Language%2520Models%2520for%2520Medical%2520Glaucoma%2520Diagnosis%26entry.906535625%3DZijian%2520Gu%2520and%2520Yuxi%2520Liu%2520and%2520Zhenhao%2520Zhang%2520and%2520Song%2520Wang%26entry.1292438233%3DVision-language%2520models%2520achieve%2520expert-level%2520performance%2520on%2520medical%2520imaging%2520tasks%2520but%2520exhibit%2520significant%2520diagnostic%2520accuracy%2520disparities%2520across%2520demographic%2520groups.%2520We%2520introduce%2520fairness-aware%2520Low-Rank%2520Adaptation%2520for%2520medical%2520VLMs%252C%2520combining%2520parameter%2520efficiency%2520with%2520explicit%2520fairness%2520optimization.%2520Our%2520key%2520algorithmic%2520contribution%2520is%2520a%2520differentiable%2520MaxAccGap%2520loss%2520that%2520enables%2520end-to-end%2520optimization%2520of%2520accuracy%2520parity%2520across%2520demographic%2520groups.%2520We%2520propose%2520three%2520methods%253A%2520FR-LoRA%2520integrates%2520MaxAccGap%2520regularization%2520into%2520the%2520training%2520objective%252C%2520GR-LoRA%2520applies%2520inverse%2520frequency%2520weighting%2520to%2520balance%2520gradient%2520contributions%252C%2520and%2520Hybrid-LoRA%2520combines%2520both%2520mechanisms.%2520Evaluated%2520on%252010%252C000%2520glaucoma%2520fundus%2520images%252C%2520GR-LoRA%2520reduces%2520diagnostic%2520accuracy%2520disparities%2520by%252069%2525%2520while%2520maintaining%252053.15%2525%2520overall%2520accuracy.%2520Ablation%2520studies%2520reveal%2520that%2520strong%2520regularization%2520strength%2520achieves%2520optimal%2520fairness%2520with%2520minimal%2520accuracy%2520trade-off%252C%2520and%2520race-specific%2520optimization%2520yields%252060%2525%2520disparity%2520reduction.%2520Our%2520approach%2520requires%2520only%25200.24%2525%2520trainable%2520parameters%252C%2520enabling%2520practical%2520deployment%2520of%2520fair%2520medical%2520AI%2520in%2520resource-constrained%2520healthcare%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness-Aware%20Fine-Tuning%20of%20Vision-Language%20Models%20for%20Medical%20Glaucoma%20Diagnosis&entry.906535625=Zijian%20Gu%20and%20Yuxi%20Liu%20and%20Zhenhao%20Zhang%20and%20Song%20Wang&entry.1292438233=Vision-language%20models%20achieve%20expert-level%20performance%20on%20medical%20imaging%20tasks%20but%20exhibit%20significant%20diagnostic%20accuracy%20disparities%20across%20demographic%20groups.%20We%20introduce%20fairness-aware%20Low-Rank%20Adaptation%20for%20medical%20VLMs%2C%20combining%20parameter%20efficiency%20with%20explicit%20fairness%20optimization.%20Our%20key%20algorithmic%20contribution%20is%20a%20differentiable%20MaxAccGap%20loss%20that%20enables%20end-to-end%20optimization%20of%20accuracy%20parity%20across%20demographic%20groups.%20We%20propose%20three%20methods%3A%20FR-LoRA%20integrates%20MaxAccGap%20regularization%20into%20the%20training%20objective%2C%20GR-LoRA%20applies%20inverse%20frequency%20weighting%20to%20balance%20gradient%20contributions%2C%20and%20Hybrid-LoRA%20combines%20both%20mechanisms.%20Evaluated%20on%2010%2C000%20glaucoma%20fundus%20images%2C%20GR-LoRA%20reduces%20diagnostic%20accuracy%20disparities%20by%2069%25%20while%20maintaining%2053.15%25%20overall%20accuracy.%20Ablation%20studies%20reveal%20that%20strong%20regularization%20strength%20achieves%20optimal%20fairness%20with%20minimal%20accuracy%20trade-off%2C%20and%20race-specific%20optimization%20yields%2060%25%20disparity%20reduction.%20Our%20approach%20requires%20only%200.24%25%20trainable%20parameters%2C%20enabling%20practical%20deployment%20of%20fair%20medical%20AI%20in%20resource-constrained%20healthcare%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.03477v2&entry.124074799=Read"},
{"title": "Lightweight Model Attribution and Detection of Synthetic Speech via Audio Residual Fingerprints", "author": "Mat\u00edas Pizarro and Mike Laszkiewicz and Dorothea Kolossa and Asja Fischer", "abstract": "As speech generation technologies advance, so do risks of impersonation, misinformation, and spoofing. We present a lightweight, training-free approach for detecting synthetic speech and attributing it to its source model. Our method addresses three tasks: (1) single-model attribution in an open-world setting, (2) multi-model attribution in a closed-world setting, and (3) real vs. synthetic speech classification. The core idea is simple: we compute standardized average residuals--the difference between an audio signal and its filtered version--to extract model-agnostic fingerprints that capture synthesis artifacts. Experiments across multiple synthesis systems and languages show AUROC scores above 99%, with strong reliability even when only a subset of model outputs is available. The method maintains high performance under common audio distortions, including echo and moderate background noise, while data augmentation can improve results in more challenging conditions. In addition, out-of-domain detection is performed using Mahalanobis distances to in-domain residual fingerprints, achieving an F1 score of 0.91 on unseen models, reinforcing the method's efficiency, generalizability, and suitability for digital forensics and security applications.", "link": "http://arxiv.org/abs/2411.14013v4", "date": "2025-12-11", "relevancy": 2.4023, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4899}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4772}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Model%20Attribution%20and%20Detection%20of%20Synthetic%20Speech%20via%20Audio%20Residual%20Fingerprints&body=Title%3A%20Lightweight%20Model%20Attribution%20and%20Detection%20of%20Synthetic%20Speech%20via%20Audio%20Residual%20Fingerprints%0AAuthor%3A%20Mat%C3%ADas%20Pizarro%20and%20Mike%20Laszkiewicz%20and%20Dorothea%20Kolossa%20and%20Asja%20Fischer%0AAbstract%3A%20As%20speech%20generation%20technologies%20advance%2C%20so%20do%20risks%20of%20impersonation%2C%20misinformation%2C%20and%20spoofing.%20We%20present%20a%20lightweight%2C%20training-free%20approach%20for%20detecting%20synthetic%20speech%20and%20attributing%20it%20to%20its%20source%20model.%20Our%20method%20addresses%20three%20tasks%3A%20%281%29%20single-model%20attribution%20in%20an%20open-world%20setting%2C%20%282%29%20multi-model%20attribution%20in%20a%20closed-world%20setting%2C%20and%20%283%29%20real%20vs.%20synthetic%20speech%20classification.%20The%20core%20idea%20is%20simple%3A%20we%20compute%20standardized%20average%20residuals--the%20difference%20between%20an%20audio%20signal%20and%20its%20filtered%20version--to%20extract%20model-agnostic%20fingerprints%20that%20capture%20synthesis%20artifacts.%20Experiments%20across%20multiple%20synthesis%20systems%20and%20languages%20show%20AUROC%20scores%20above%2099%25%2C%20with%20strong%20reliability%20even%20when%20only%20a%20subset%20of%20model%20outputs%20is%20available.%20The%20method%20maintains%20high%20performance%20under%20common%20audio%20distortions%2C%20including%20echo%20and%20moderate%20background%20noise%2C%20while%20data%20augmentation%20can%20improve%20results%20in%20more%20challenging%20conditions.%20In%20addition%2C%20out-of-domain%20detection%20is%20performed%20using%20Mahalanobis%20distances%20to%20in-domain%20residual%20fingerprints%2C%20achieving%20an%20F1%20score%20of%200.91%20on%20unseen%20models%2C%20reinforcing%20the%20method%27s%20efficiency%2C%20generalizability%2C%20and%20suitability%20for%20digital%20forensics%20and%20security%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2411.14013v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Model%2520Attribution%2520and%2520Detection%2520of%2520Synthetic%2520Speech%2520via%2520Audio%2520Residual%2520Fingerprints%26entry.906535625%3DMat%25C3%25ADas%2520Pizarro%2520and%2520Mike%2520Laszkiewicz%2520and%2520Dorothea%2520Kolossa%2520and%2520Asja%2520Fischer%26entry.1292438233%3DAs%2520speech%2520generation%2520technologies%2520advance%252C%2520so%2520do%2520risks%2520of%2520impersonation%252C%2520misinformation%252C%2520and%2520spoofing.%2520We%2520present%2520a%2520lightweight%252C%2520training-free%2520approach%2520for%2520detecting%2520synthetic%2520speech%2520and%2520attributing%2520it%2520to%2520its%2520source%2520model.%2520Our%2520method%2520addresses%2520three%2520tasks%253A%2520%25281%2529%2520single-model%2520attribution%2520in%2520an%2520open-world%2520setting%252C%2520%25282%2529%2520multi-model%2520attribution%2520in%2520a%2520closed-world%2520setting%252C%2520and%2520%25283%2529%2520real%2520vs.%2520synthetic%2520speech%2520classification.%2520The%2520core%2520idea%2520is%2520simple%253A%2520we%2520compute%2520standardized%2520average%2520residuals--the%2520difference%2520between%2520an%2520audio%2520signal%2520and%2520its%2520filtered%2520version--to%2520extract%2520model-agnostic%2520fingerprints%2520that%2520capture%2520synthesis%2520artifacts.%2520Experiments%2520across%2520multiple%2520synthesis%2520systems%2520and%2520languages%2520show%2520AUROC%2520scores%2520above%252099%2525%252C%2520with%2520strong%2520reliability%2520even%2520when%2520only%2520a%2520subset%2520of%2520model%2520outputs%2520is%2520available.%2520The%2520method%2520maintains%2520high%2520performance%2520under%2520common%2520audio%2520distortions%252C%2520including%2520echo%2520and%2520moderate%2520background%2520noise%252C%2520while%2520data%2520augmentation%2520can%2520improve%2520results%2520in%2520more%2520challenging%2520conditions.%2520In%2520addition%252C%2520out-of-domain%2520detection%2520is%2520performed%2520using%2520Mahalanobis%2520distances%2520to%2520in-domain%2520residual%2520fingerprints%252C%2520achieving%2520an%2520F1%2520score%2520of%25200.91%2520on%2520unseen%2520models%252C%2520reinforcing%2520the%2520method%2527s%2520efficiency%252C%2520generalizability%252C%2520and%2520suitability%2520for%2520digital%2520forensics%2520and%2520security%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14013v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Model%20Attribution%20and%20Detection%20of%20Synthetic%20Speech%20via%20Audio%20Residual%20Fingerprints&entry.906535625=Mat%C3%ADas%20Pizarro%20and%20Mike%20Laszkiewicz%20and%20Dorothea%20Kolossa%20and%20Asja%20Fischer&entry.1292438233=As%20speech%20generation%20technologies%20advance%2C%20so%20do%20risks%20of%20impersonation%2C%20misinformation%2C%20and%20spoofing.%20We%20present%20a%20lightweight%2C%20training-free%20approach%20for%20detecting%20synthetic%20speech%20and%20attributing%20it%20to%20its%20source%20model.%20Our%20method%20addresses%20three%20tasks%3A%20%281%29%20single-model%20attribution%20in%20an%20open-world%20setting%2C%20%282%29%20multi-model%20attribution%20in%20a%20closed-world%20setting%2C%20and%20%283%29%20real%20vs.%20synthetic%20speech%20classification.%20The%20core%20idea%20is%20simple%3A%20we%20compute%20standardized%20average%20residuals--the%20difference%20between%20an%20audio%20signal%20and%20its%20filtered%20version--to%20extract%20model-agnostic%20fingerprints%20that%20capture%20synthesis%20artifacts.%20Experiments%20across%20multiple%20synthesis%20systems%20and%20languages%20show%20AUROC%20scores%20above%2099%25%2C%20with%20strong%20reliability%20even%20when%20only%20a%20subset%20of%20model%20outputs%20is%20available.%20The%20method%20maintains%20high%20performance%20under%20common%20audio%20distortions%2C%20including%20echo%20and%20moderate%20background%20noise%2C%20while%20data%20augmentation%20can%20improve%20results%20in%20more%20challenging%20conditions.%20In%20addition%2C%20out-of-domain%20detection%20is%20performed%20using%20Mahalanobis%20distances%20to%20in-domain%20residual%20fingerprints%2C%20achieving%20an%20F1%20score%20of%200.91%20on%20unseen%20models%2C%20reinforcing%20the%20method%27s%20efficiency%2C%20generalizability%2C%20and%20suitability%20for%20digital%20forensics%20and%20security%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2411.14013v4&entry.124074799=Read"},
{"title": "PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction", "author": "Brandon Smock and Valerie Faucon-Morin and Max Sokolov and Libin Liang and Tayyibah Khanam and Maury Courtland", "abstract": "Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.", "link": "http://arxiv.org/abs/2512.10888v1", "date": "2025-12-11", "relevancy": 2.3986, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PubTables-v2%3A%20A%20new%20large-scale%20dataset%20for%20full-page%20and%20multi-page%20table%20extraction&body=Title%3A%20PubTables-v2%3A%20A%20new%20large-scale%20dataset%20for%20full-page%20and%20multi-page%20table%20extraction%0AAuthor%3A%20Brandon%20Smock%20and%20Valerie%20Faucon-Morin%20and%20Max%20Sokolov%20and%20Libin%20Liang%20and%20Tayyibah%20Khanam%20and%20Maury%20Courtland%0AAbstract%3A%20Table%20extraction%20%28TE%29%20is%20a%20key%20challenge%20in%20visual%20document%20understanding.%20Traditional%20approaches%20detect%20tables%20first%2C%20then%20recognize%20their%20structure.%20Recently%2C%20interest%20has%20surged%20in%20developing%20methods%2C%20such%20as%20vision-language%20models%20%28VLMs%29%2C%20that%20can%20extract%20tables%20directly%20in%20their%20full%20page%20or%20document%20context.%20However%2C%20progress%20has%20been%20difficult%20to%20demonstrate%20due%20to%20a%20lack%20of%20annotated%20data.%20To%20address%20this%2C%20we%20create%20a%20new%20large-scale%20dataset%2C%20PubTables-v2.%20PubTables-v2%20supports%20a%20number%20of%20current%20challenging%20table%20extraction%20tasks.%20Notably%2C%20it%20is%20the%20first%20large-scale%20benchmark%20for%20multi-page%20table%20structure%20recognition.%20We%20demonstrate%20its%20usefulness%20by%20evaluating%20domain-specialized%20VLMs%20on%20these%20tasks%20and%20highlighting%20current%20progress.%20Finally%2C%20we%20use%20PubTables-v2%20to%20create%20the%20Page-Object%20Table%20Transformer%20%28POTATR%29%2C%20an%20image-to-graph%20extension%20of%20the%20Table%20Transformer%20to%20comprehensive%20page-level%20TE.%20Data%2C%20code%2C%20and%20trained%20models%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPubTables-v2%253A%2520A%2520new%2520large-scale%2520dataset%2520for%2520full-page%2520and%2520multi-page%2520table%2520extraction%26entry.906535625%3DBrandon%2520Smock%2520and%2520Valerie%2520Faucon-Morin%2520and%2520Max%2520Sokolov%2520and%2520Libin%2520Liang%2520and%2520Tayyibah%2520Khanam%2520and%2520Maury%2520Courtland%26entry.1292438233%3DTable%2520extraction%2520%2528TE%2529%2520is%2520a%2520key%2520challenge%2520in%2520visual%2520document%2520understanding.%2520Traditional%2520approaches%2520detect%2520tables%2520first%252C%2520then%2520recognize%2520their%2520structure.%2520Recently%252C%2520interest%2520has%2520surged%2520in%2520developing%2520methods%252C%2520such%2520as%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520that%2520can%2520extract%2520tables%2520directly%2520in%2520their%2520full%2520page%2520or%2520document%2520context.%2520However%252C%2520progress%2520has%2520been%2520difficult%2520to%2520demonstrate%2520due%2520to%2520a%2520lack%2520of%2520annotated%2520data.%2520To%2520address%2520this%252C%2520we%2520create%2520a%2520new%2520large-scale%2520dataset%252C%2520PubTables-v2.%2520PubTables-v2%2520supports%2520a%2520number%2520of%2520current%2520challenging%2520table%2520extraction%2520tasks.%2520Notably%252C%2520it%2520is%2520the%2520first%2520large-scale%2520benchmark%2520for%2520multi-page%2520table%2520structure%2520recognition.%2520We%2520demonstrate%2520its%2520usefulness%2520by%2520evaluating%2520domain-specialized%2520VLMs%2520on%2520these%2520tasks%2520and%2520highlighting%2520current%2520progress.%2520Finally%252C%2520we%2520use%2520PubTables-v2%2520to%2520create%2520the%2520Page-Object%2520Table%2520Transformer%2520%2528POTATR%2529%252C%2520an%2520image-to-graph%2520extension%2520of%2520the%2520Table%2520Transformer%2520to%2520comprehensive%2520page-level%2520TE.%2520Data%252C%2520code%252C%2520and%2520trained%2520models%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PubTables-v2%3A%20A%20new%20large-scale%20dataset%20for%20full-page%20and%20multi-page%20table%20extraction&entry.906535625=Brandon%20Smock%20and%20Valerie%20Faucon-Morin%20and%20Max%20Sokolov%20and%20Libin%20Liang%20and%20Tayyibah%20Khanam%20and%20Maury%20Courtland&entry.1292438233=Table%20extraction%20%28TE%29%20is%20a%20key%20challenge%20in%20visual%20document%20understanding.%20Traditional%20approaches%20detect%20tables%20first%2C%20then%20recognize%20their%20structure.%20Recently%2C%20interest%20has%20surged%20in%20developing%20methods%2C%20such%20as%20vision-language%20models%20%28VLMs%29%2C%20that%20can%20extract%20tables%20directly%20in%20their%20full%20page%20or%20document%20context.%20However%2C%20progress%20has%20been%20difficult%20to%20demonstrate%20due%20to%20a%20lack%20of%20annotated%20data.%20To%20address%20this%2C%20we%20create%20a%20new%20large-scale%20dataset%2C%20PubTables-v2.%20PubTables-v2%20supports%20a%20number%20of%20current%20challenging%20table%20extraction%20tasks.%20Notably%2C%20it%20is%20the%20first%20large-scale%20benchmark%20for%20multi-page%20table%20structure%20recognition.%20We%20demonstrate%20its%20usefulness%20by%20evaluating%20domain-specialized%20VLMs%20on%20these%20tasks%20and%20highlighting%20current%20progress.%20Finally%2C%20we%20use%20PubTables-v2%20to%20create%20the%20Page-Object%20Table%20Transformer%20%28POTATR%29%2C%20an%20image-to-graph%20extension%20of%20the%20Table%20Transformer%20to%20comprehensive%20page-level%20TE.%20Data%2C%20code%2C%20and%20trained%20models%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2512.10888v1&entry.124074799=Read"},
{"title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence", "author": "Jingli Lin and Runsen Xu and Shaohao Zhu and Sihan Yang and Peizhou Cao and Yunlong Ran and Miao Hu and Chenming Zhu and Yiman Xie and Yilin Long and Wenbo Hu and Dahua Lin and Tai Wang and Jiangmiao Pang", "abstract": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.", "link": "http://arxiv.org/abs/2512.10863v1", "date": "2025-12-11", "relevancy": 2.3961, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMSI-Video-Bench%3A%20A%20Holistic%20Benchmark%20for%20Video-Based%20Spatial%20Intelligence&body=Title%3A%20MMSI-Video-Bench%3A%20A%20Holistic%20Benchmark%20for%20Video-Based%20Spatial%20Intelligence%0AAuthor%3A%20Jingli%20Lin%20and%20Runsen%20Xu%20and%20Shaohao%20Zhu%20and%20Sihan%20Yang%20and%20Peizhou%20Cao%20and%20Yunlong%20Ran%20and%20Miao%20Hu%20and%20Chenming%20Zhu%20and%20Yiman%20Xie%20and%20Yilin%20Long%20and%20Wenbo%20Hu%20and%20Dahua%20Lin%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang%0AAbstract%3A%20Spatial%20understanding%20over%20continuous%20visual%20input%20is%20crucial%20for%20MLLMs%20to%20evolve%20into%20general-purpose%20assistants%20in%20physical%20environments.%20Yet%20there%20is%20still%20no%20comprehensive%20benchmark%20that%20holistically%20assesses%20the%20progress%20toward%20this%20goal.%20In%20this%20work%2C%20we%20introduce%20MMSI-Video-Bench%2C%20a%20fully%20human-annotated%20benchmark%20for%20video-based%20spatial%20intelligence%20in%20MLLMs.%20It%20operationalizes%20a%20four-level%20framework%2C%20Perception%2C%20Planning%2C%20Prediction%2C%20and%20Cross-Video%20Reasoning%2C%20through%201%2C106%20questions%20grounded%20in%201%2C278%20clips%20from%2025%20datasets%20and%20in-house%20videos.%20Each%20item%20is%20carefully%20designed%20and%20reviewed%20by%203DV%20experts%20with%20explanatory%20rationales%20to%20ensure%20precise%2C%20unambiguous%20grounding.%20Leveraging%20its%20diverse%20data%20sources%20and%20holistic%20task%20coverage%2C%20MMSI-Video-Bench%20also%20supports%20three%20domain-oriented%20sub-benchmarks%20%28Indoor%20Scene%20Perception%20Bench%2C%20Robot%20Bench%20and%20Grounding%20Bench%29%20for%20targeted%20capability%20assessment.%20We%20evaluate%2025%20strong%20open-source%20and%20proprietary%20MLLMs%2C%20revealing%20a%20striking%20human--AI%20gap%3A%20many%20models%20perform%20near%20chance%2C%20and%20the%20best%20reasoning%20model%20lags%20humans%20by%20nearly%2060%25.%20We%20further%20find%20that%20spatially%20fine-tuned%20models%20still%20fail%20to%20generalize%20effectively%20on%20our%20benchmark.%20Fine-grained%20error%20analysis%20exposes%20systematic%20failures%20in%20geometric%20reasoning%2C%20motion%20grounding%2C%20long-horizon%20prediction%2C%20and%20cross-video%20correspondence.%20We%20also%20show%20that%20typical%20frame-sampling%20strategies%20transfer%20poorly%20to%20our%20reasoning-intensive%20benchmark%2C%20and%20that%20neither%203D%20spatial%20cues%20nor%20chain-of-thought%20prompting%20yields%20meaningful%20gains.%20We%20expect%20our%20benchmark%20to%20establish%20a%20solid%20testbed%20for%20advancing%20video-based%20spatial%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMSI-Video-Bench%253A%2520A%2520Holistic%2520Benchmark%2520for%2520Video-Based%2520Spatial%2520Intelligence%26entry.906535625%3DJingli%2520Lin%2520and%2520Runsen%2520Xu%2520and%2520Shaohao%2520Zhu%2520and%2520Sihan%2520Yang%2520and%2520Peizhou%2520Cao%2520and%2520Yunlong%2520Ran%2520and%2520Miao%2520Hu%2520and%2520Chenming%2520Zhu%2520and%2520Yiman%2520Xie%2520and%2520Yilin%2520Long%2520and%2520Wenbo%2520Hu%2520and%2520Dahua%2520Lin%2520and%2520Tai%2520Wang%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DSpatial%2520understanding%2520over%2520continuous%2520visual%2520input%2520is%2520crucial%2520for%2520MLLMs%2520to%2520evolve%2520into%2520general-purpose%2520assistants%2520in%2520physical%2520environments.%2520Yet%2520there%2520is%2520still%2520no%2520comprehensive%2520benchmark%2520that%2520holistically%2520assesses%2520the%2520progress%2520toward%2520this%2520goal.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MMSI-Video-Bench%252C%2520a%2520fully%2520human-annotated%2520benchmark%2520for%2520video-based%2520spatial%2520intelligence%2520in%2520MLLMs.%2520It%2520operationalizes%2520a%2520four-level%2520framework%252C%2520Perception%252C%2520Planning%252C%2520Prediction%252C%2520and%2520Cross-Video%2520Reasoning%252C%2520through%25201%252C106%2520questions%2520grounded%2520in%25201%252C278%2520clips%2520from%252025%2520datasets%2520and%2520in-house%2520videos.%2520Each%2520item%2520is%2520carefully%2520designed%2520and%2520reviewed%2520by%25203DV%2520experts%2520with%2520explanatory%2520rationales%2520to%2520ensure%2520precise%252C%2520unambiguous%2520grounding.%2520Leveraging%2520its%2520diverse%2520data%2520sources%2520and%2520holistic%2520task%2520coverage%252C%2520MMSI-Video-Bench%2520also%2520supports%2520three%2520domain-oriented%2520sub-benchmarks%2520%2528Indoor%2520Scene%2520Perception%2520Bench%252C%2520Robot%2520Bench%2520and%2520Grounding%2520Bench%2529%2520for%2520targeted%2520capability%2520assessment.%2520We%2520evaluate%252025%2520strong%2520open-source%2520and%2520proprietary%2520MLLMs%252C%2520revealing%2520a%2520striking%2520human--AI%2520gap%253A%2520many%2520models%2520perform%2520near%2520chance%252C%2520and%2520the%2520best%2520reasoning%2520model%2520lags%2520humans%2520by%2520nearly%252060%2525.%2520We%2520further%2520find%2520that%2520spatially%2520fine-tuned%2520models%2520still%2520fail%2520to%2520generalize%2520effectively%2520on%2520our%2520benchmark.%2520Fine-grained%2520error%2520analysis%2520exposes%2520systematic%2520failures%2520in%2520geometric%2520reasoning%252C%2520motion%2520grounding%252C%2520long-horizon%2520prediction%252C%2520and%2520cross-video%2520correspondence.%2520We%2520also%2520show%2520that%2520typical%2520frame-sampling%2520strategies%2520transfer%2520poorly%2520to%2520our%2520reasoning-intensive%2520benchmark%252C%2520and%2520that%2520neither%25203D%2520spatial%2520cues%2520nor%2520chain-of-thought%2520prompting%2520yields%2520meaningful%2520gains.%2520We%2520expect%2520our%2520benchmark%2520to%2520establish%2520a%2520solid%2520testbed%2520for%2520advancing%2520video-based%2520spatial%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMSI-Video-Bench%3A%20A%20Holistic%20Benchmark%20for%20Video-Based%20Spatial%20Intelligence&entry.906535625=Jingli%20Lin%20and%20Runsen%20Xu%20and%20Shaohao%20Zhu%20and%20Sihan%20Yang%20and%20Peizhou%20Cao%20and%20Yunlong%20Ran%20and%20Miao%20Hu%20and%20Chenming%20Zhu%20and%20Yiman%20Xie%20and%20Yilin%20Long%20and%20Wenbo%20Hu%20and%20Dahua%20Lin%20and%20Tai%20Wang%20and%20Jiangmiao%20Pang&entry.1292438233=Spatial%20understanding%20over%20continuous%20visual%20input%20is%20crucial%20for%20MLLMs%20to%20evolve%20into%20general-purpose%20assistants%20in%20physical%20environments.%20Yet%20there%20is%20still%20no%20comprehensive%20benchmark%20that%20holistically%20assesses%20the%20progress%20toward%20this%20goal.%20In%20this%20work%2C%20we%20introduce%20MMSI-Video-Bench%2C%20a%20fully%20human-annotated%20benchmark%20for%20video-based%20spatial%20intelligence%20in%20MLLMs.%20It%20operationalizes%20a%20four-level%20framework%2C%20Perception%2C%20Planning%2C%20Prediction%2C%20and%20Cross-Video%20Reasoning%2C%20through%201%2C106%20questions%20grounded%20in%201%2C278%20clips%20from%2025%20datasets%20and%20in-house%20videos.%20Each%20item%20is%20carefully%20designed%20and%20reviewed%20by%203DV%20experts%20with%20explanatory%20rationales%20to%20ensure%20precise%2C%20unambiguous%20grounding.%20Leveraging%20its%20diverse%20data%20sources%20and%20holistic%20task%20coverage%2C%20MMSI-Video-Bench%20also%20supports%20three%20domain-oriented%20sub-benchmarks%20%28Indoor%20Scene%20Perception%20Bench%2C%20Robot%20Bench%20and%20Grounding%20Bench%29%20for%20targeted%20capability%20assessment.%20We%20evaluate%2025%20strong%20open-source%20and%20proprietary%20MLLMs%2C%20revealing%20a%20striking%20human--AI%20gap%3A%20many%20models%20perform%20near%20chance%2C%20and%20the%20best%20reasoning%20model%20lags%20humans%20by%20nearly%2060%25.%20We%20further%20find%20that%20spatially%20fine-tuned%20models%20still%20fail%20to%20generalize%20effectively%20on%20our%20benchmark.%20Fine-grained%20error%20analysis%20exposes%20systematic%20failures%20in%20geometric%20reasoning%2C%20motion%20grounding%2C%20long-horizon%20prediction%2C%20and%20cross-video%20correspondence.%20We%20also%20show%20that%20typical%20frame-sampling%20strategies%20transfer%20poorly%20to%20our%20reasoning-intensive%20benchmark%2C%20and%20that%20neither%203D%20spatial%20cues%20nor%20chain-of-thought%20prompting%20yields%20meaningful%20gains.%20We%20expect%20our%20benchmark%20to%20establish%20a%20solid%20testbed%20for%20advancing%20video-based%20spatial%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2512.10863v1&entry.124074799=Read"},
{"title": "Motion Planning for Safe Landing of a Human-Piloted Parafoil", "author": "Maximillian Fainkich and Kiril Solovey and Anna Clarke", "abstract": "Most skydiving accidents occur during the parafoil-piloting and landing stages and result from human lapses in judgment while piloting the parafoil. Training of novice pilots is protracted due to the lack of functional and easily accessible training simulators. Moreover, work on parafoil trajectory planning suitable for aiding human training remains limited. To bridge this gap, we study the problem of computing safe trajectories for human-piloted parafoil flight and examine how such trajectories fare against human-generated solutions. For the algorithmic part, we adapt the sampling-based motion planner Stable Sparse RRT (SST) by Li et al., to cope with the problem constraints while minimizing the bank angle (control effort) as a proxy for safety. We then compare the computer-generated solutions with data from human-generated parafoil flight, where the algorithm offers a relative cost improvement of 20\\%-80\\% over the performance of the human pilot. We observe that human pilots tend to, first, close the horizontal distance to the landing area, and then address the vertical gap by spiraling down to the suitable altitude for starting a landing maneuver. The algorithm considered here makes smoother and more gradual descents, arriving at the landing area at the precise altitude necessary for the final approach while maintaining safety constraints. Overall, the study demonstrates the potential of computer-generated guidelines, rather than traditional rules of thumb, which can be integrated into future simulators to train pilots for safer and more cost-effective flights.", "link": "http://arxiv.org/abs/2512.10595v1", "date": "2025-12-11", "relevancy": 2.3955, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5006}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4791}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20Planning%20for%20Safe%20Landing%20of%20a%20Human-Piloted%20Parafoil&body=Title%3A%20Motion%20Planning%20for%20Safe%20Landing%20of%20a%20Human-Piloted%20Parafoil%0AAuthor%3A%20Maximillian%20Fainkich%20and%20Kiril%20Solovey%20and%20Anna%20Clarke%0AAbstract%3A%20Most%20skydiving%20accidents%20occur%20during%20the%20parafoil-piloting%20and%20landing%20stages%20and%20result%20from%20human%20lapses%20in%20judgment%20while%20piloting%20the%20parafoil.%20Training%20of%20novice%20pilots%20is%20protracted%20due%20to%20the%20lack%20of%20functional%20and%20easily%20accessible%20training%20simulators.%20Moreover%2C%20work%20on%20parafoil%20trajectory%20planning%20suitable%20for%20aiding%20human%20training%20remains%20limited.%20To%20bridge%20this%20gap%2C%20we%20study%20the%20problem%20of%20computing%20safe%20trajectories%20for%20human-piloted%20parafoil%20flight%20and%20examine%20how%20such%20trajectories%20fare%20against%20human-generated%20solutions.%20For%20the%20algorithmic%20part%2C%20we%20adapt%20the%20sampling-based%20motion%20planner%20Stable%20Sparse%20RRT%20%28SST%29%20by%20Li%20et%20al.%2C%20to%20cope%20with%20the%20problem%20constraints%20while%20minimizing%20the%20bank%20angle%20%28control%20effort%29%20as%20a%20proxy%20for%20safety.%20We%20then%20compare%20the%20computer-generated%20solutions%20with%20data%20from%20human-generated%20parafoil%20flight%2C%20where%20the%20algorithm%20offers%20a%20relative%20cost%20improvement%20of%2020%5C%25-80%5C%25%20over%20the%20performance%20of%20the%20human%20pilot.%20We%20observe%20that%20human%20pilots%20tend%20to%2C%20first%2C%20close%20the%20horizontal%20distance%20to%20the%20landing%20area%2C%20and%20then%20address%20the%20vertical%20gap%20by%20spiraling%20down%20to%20the%20suitable%20altitude%20for%20starting%20a%20landing%20maneuver.%20The%20algorithm%20considered%20here%20makes%20smoother%20and%20more%20gradual%20descents%2C%20arriving%20at%20the%20landing%20area%20at%20the%20precise%20altitude%20necessary%20for%20the%20final%20approach%20while%20maintaining%20safety%20constraints.%20Overall%2C%20the%20study%20demonstrates%20the%20potential%20of%20computer-generated%20guidelines%2C%20rather%20than%20traditional%20rules%20of%20thumb%2C%20which%20can%20be%20integrated%20into%20future%20simulators%20to%20train%20pilots%20for%20safer%20and%20more%20cost-effective%20flights.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520Planning%2520for%2520Safe%2520Landing%2520of%2520a%2520Human-Piloted%2520Parafoil%26entry.906535625%3DMaximillian%2520Fainkich%2520and%2520Kiril%2520Solovey%2520and%2520Anna%2520Clarke%26entry.1292438233%3DMost%2520skydiving%2520accidents%2520occur%2520during%2520the%2520parafoil-piloting%2520and%2520landing%2520stages%2520and%2520result%2520from%2520human%2520lapses%2520in%2520judgment%2520while%2520piloting%2520the%2520parafoil.%2520Training%2520of%2520novice%2520pilots%2520is%2520protracted%2520due%2520to%2520the%2520lack%2520of%2520functional%2520and%2520easily%2520accessible%2520training%2520simulators.%2520Moreover%252C%2520work%2520on%2520parafoil%2520trajectory%2520planning%2520suitable%2520for%2520aiding%2520human%2520training%2520remains%2520limited.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520study%2520the%2520problem%2520of%2520computing%2520safe%2520trajectories%2520for%2520human-piloted%2520parafoil%2520flight%2520and%2520examine%2520how%2520such%2520trajectories%2520fare%2520against%2520human-generated%2520solutions.%2520For%2520the%2520algorithmic%2520part%252C%2520we%2520adapt%2520the%2520sampling-based%2520motion%2520planner%2520Stable%2520Sparse%2520RRT%2520%2528SST%2529%2520by%2520Li%2520et%2520al.%252C%2520to%2520cope%2520with%2520the%2520problem%2520constraints%2520while%2520minimizing%2520the%2520bank%2520angle%2520%2528control%2520effort%2529%2520as%2520a%2520proxy%2520for%2520safety.%2520We%2520then%2520compare%2520the%2520computer-generated%2520solutions%2520with%2520data%2520from%2520human-generated%2520parafoil%2520flight%252C%2520where%2520the%2520algorithm%2520offers%2520a%2520relative%2520cost%2520improvement%2520of%252020%255C%2525-80%255C%2525%2520over%2520the%2520performance%2520of%2520the%2520human%2520pilot.%2520We%2520observe%2520that%2520human%2520pilots%2520tend%2520to%252C%2520first%252C%2520close%2520the%2520horizontal%2520distance%2520to%2520the%2520landing%2520area%252C%2520and%2520then%2520address%2520the%2520vertical%2520gap%2520by%2520spiraling%2520down%2520to%2520the%2520suitable%2520altitude%2520for%2520starting%2520a%2520landing%2520maneuver.%2520The%2520algorithm%2520considered%2520here%2520makes%2520smoother%2520and%2520more%2520gradual%2520descents%252C%2520arriving%2520at%2520the%2520landing%2520area%2520at%2520the%2520precise%2520altitude%2520necessary%2520for%2520the%2520final%2520approach%2520while%2520maintaining%2520safety%2520constraints.%2520Overall%252C%2520the%2520study%2520demonstrates%2520the%2520potential%2520of%2520computer-generated%2520guidelines%252C%2520rather%2520than%2520traditional%2520rules%2520of%2520thumb%252C%2520which%2520can%2520be%2520integrated%2520into%2520future%2520simulators%2520to%2520train%2520pilots%2520for%2520safer%2520and%2520more%2520cost-effective%2520flights.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Planning%20for%20Safe%20Landing%20of%20a%20Human-Piloted%20Parafoil&entry.906535625=Maximillian%20Fainkich%20and%20Kiril%20Solovey%20and%20Anna%20Clarke&entry.1292438233=Most%20skydiving%20accidents%20occur%20during%20the%20parafoil-piloting%20and%20landing%20stages%20and%20result%20from%20human%20lapses%20in%20judgment%20while%20piloting%20the%20parafoil.%20Training%20of%20novice%20pilots%20is%20protracted%20due%20to%20the%20lack%20of%20functional%20and%20easily%20accessible%20training%20simulators.%20Moreover%2C%20work%20on%20parafoil%20trajectory%20planning%20suitable%20for%20aiding%20human%20training%20remains%20limited.%20To%20bridge%20this%20gap%2C%20we%20study%20the%20problem%20of%20computing%20safe%20trajectories%20for%20human-piloted%20parafoil%20flight%20and%20examine%20how%20such%20trajectories%20fare%20against%20human-generated%20solutions.%20For%20the%20algorithmic%20part%2C%20we%20adapt%20the%20sampling-based%20motion%20planner%20Stable%20Sparse%20RRT%20%28SST%29%20by%20Li%20et%20al.%2C%20to%20cope%20with%20the%20problem%20constraints%20while%20minimizing%20the%20bank%20angle%20%28control%20effort%29%20as%20a%20proxy%20for%20safety.%20We%20then%20compare%20the%20computer-generated%20solutions%20with%20data%20from%20human-generated%20parafoil%20flight%2C%20where%20the%20algorithm%20offers%20a%20relative%20cost%20improvement%20of%2020%5C%25-80%5C%25%20over%20the%20performance%20of%20the%20human%20pilot.%20We%20observe%20that%20human%20pilots%20tend%20to%2C%20first%2C%20close%20the%20horizontal%20distance%20to%20the%20landing%20area%2C%20and%20then%20address%20the%20vertical%20gap%20by%20spiraling%20down%20to%20the%20suitable%20altitude%20for%20starting%20a%20landing%20maneuver.%20The%20algorithm%20considered%20here%20makes%20smoother%20and%20more%20gradual%20descents%2C%20arriving%20at%20the%20landing%20area%20at%20the%20precise%20altitude%20necessary%20for%20the%20final%20approach%20while%20maintaining%20safety%20constraints.%20Overall%2C%20the%20study%20demonstrates%20the%20potential%20of%20computer-generated%20guidelines%2C%20rather%20than%20traditional%20rules%20of%20thumb%2C%20which%20can%20be%20integrated%20into%20future%20simulators%20to%20train%20pilots%20for%20safer%20and%20more%20cost-effective%20flights.&entry.1838667208=http%3A//arxiv.org/abs/2512.10595v1&entry.124074799=Read"},
{"title": "Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces", "author": "Bishoy Galoaa and Xiangyu Bai and Sarah Ostadabbas", "abstract": "We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.", "link": "http://arxiv.org/abs/2512.10617v1", "date": "2025-12-11", "relevancy": 2.3826, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6229}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5787}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lang2Motion%3A%20Bridging%20Language%20and%20Motion%20through%20Joint%20Embedding%20Spaces&body=Title%3A%20Lang2Motion%3A%20Bridging%20Language%20and%20Motion%20through%20Joint%20Embedding%20Spaces%0AAuthor%3A%20Bishoy%20Galoaa%20and%20Xiangyu%20Bai%20and%20Sarah%20Ostadabbas%0AAbstract%3A%20We%20present%20Lang2Motion%2C%20a%20framework%20for%20language-guided%20point%20trajectory%20generation%20by%20aligning%20motion%20manifolds%20with%20joint%20embedding%20spaces.%20Unlike%20prior%20work%20focusing%20on%20human%20motion%20or%20video%20synthesis%2C%20we%20generate%20explicit%20trajectories%20for%20arbitrary%20objects%20using%20motion%20extracted%20from%20real-world%20videos%20via%20point%20tracking.%20Our%20transformer-based%20auto-encoder%20learns%20trajectory%20representations%20through%20dual%20supervision%3A%20textual%20motion%20descriptions%20and%20rendered%20trajectory%20visualizations%2C%20both%20mapped%20through%20CLIP%27s%20frozen%20encoders.%20Lang2Motion%20achieves%2034.2%25%20Recall%401%20on%20text-to-trajectory%20retrieval%2C%20outperforming%20video-based%20methods%20by%2012.5%20points%2C%20and%20improves%20motion%20accuracy%20by%2033-52%25%20%2812.4%20ADE%20vs%2018.3-25.3%29%20compared%20to%20video%20generation%20baselines.%20We%20demonstrate%2088.3%25%20Top-1%20accuracy%20on%20human%20action%20recognition%20despite%20training%20only%20on%20diverse%20object%20motions%2C%20showing%20effective%20transfer%20across%20motion%20domains.%20Lang2Motion%20supports%20style%20transfer%2C%20semantic%20interpolation%2C%20and%20latent-space%20editing%20through%20CLIP-aligned%20trajectory%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLang2Motion%253A%2520Bridging%2520Language%2520and%2520Motion%2520through%2520Joint%2520Embedding%2520Spaces%26entry.906535625%3DBishoy%2520Galoaa%2520and%2520Xiangyu%2520Bai%2520and%2520Sarah%2520Ostadabbas%26entry.1292438233%3DWe%2520present%2520Lang2Motion%252C%2520a%2520framework%2520for%2520language-guided%2520point%2520trajectory%2520generation%2520by%2520aligning%2520motion%2520manifolds%2520with%2520joint%2520embedding%2520spaces.%2520Unlike%2520prior%2520work%2520focusing%2520on%2520human%2520motion%2520or%2520video%2520synthesis%252C%2520we%2520generate%2520explicit%2520trajectories%2520for%2520arbitrary%2520objects%2520using%2520motion%2520extracted%2520from%2520real-world%2520videos%2520via%2520point%2520tracking.%2520Our%2520transformer-based%2520auto-encoder%2520learns%2520trajectory%2520representations%2520through%2520dual%2520supervision%253A%2520textual%2520motion%2520descriptions%2520and%2520rendered%2520trajectory%2520visualizations%252C%2520both%2520mapped%2520through%2520CLIP%2527s%2520frozen%2520encoders.%2520Lang2Motion%2520achieves%252034.2%2525%2520Recall%25401%2520on%2520text-to-trajectory%2520retrieval%252C%2520outperforming%2520video-based%2520methods%2520by%252012.5%2520points%252C%2520and%2520improves%2520motion%2520accuracy%2520by%252033-52%2525%2520%252812.4%2520ADE%2520vs%252018.3-25.3%2529%2520compared%2520to%2520video%2520generation%2520baselines.%2520We%2520demonstrate%252088.3%2525%2520Top-1%2520accuracy%2520on%2520human%2520action%2520recognition%2520despite%2520training%2520only%2520on%2520diverse%2520object%2520motions%252C%2520showing%2520effective%2520transfer%2520across%2520motion%2520domains.%2520Lang2Motion%2520supports%2520style%2520transfer%252C%2520semantic%2520interpolation%252C%2520and%2520latent-space%2520editing%2520through%2520CLIP-aligned%2520trajectory%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lang2Motion%3A%20Bridging%20Language%20and%20Motion%20through%20Joint%20Embedding%20Spaces&entry.906535625=Bishoy%20Galoaa%20and%20Xiangyu%20Bai%20and%20Sarah%20Ostadabbas&entry.1292438233=We%20present%20Lang2Motion%2C%20a%20framework%20for%20language-guided%20point%20trajectory%20generation%20by%20aligning%20motion%20manifolds%20with%20joint%20embedding%20spaces.%20Unlike%20prior%20work%20focusing%20on%20human%20motion%20or%20video%20synthesis%2C%20we%20generate%20explicit%20trajectories%20for%20arbitrary%20objects%20using%20motion%20extracted%20from%20real-world%20videos%20via%20point%20tracking.%20Our%20transformer-based%20auto-encoder%20learns%20trajectory%20representations%20through%20dual%20supervision%3A%20textual%20motion%20descriptions%20and%20rendered%20trajectory%20visualizations%2C%20both%20mapped%20through%20CLIP%27s%20frozen%20encoders.%20Lang2Motion%20achieves%2034.2%25%20Recall%401%20on%20text-to-trajectory%20retrieval%2C%20outperforming%20video-based%20methods%20by%2012.5%20points%2C%20and%20improves%20motion%20accuracy%20by%2033-52%25%20%2812.4%20ADE%20vs%2018.3-25.3%29%20compared%20to%20video%20generation%20baselines.%20We%20demonstrate%2088.3%25%20Top-1%20accuracy%20on%20human%20action%20recognition%20despite%20training%20only%20on%20diverse%20object%20motions%2C%20showing%20effective%20transfer%20across%20motion%20domains.%20Lang2Motion%20supports%20style%20transfer%2C%20semantic%20interpolation%2C%20and%20latent-space%20editing%20through%20CLIP-aligned%20trajectory%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2512.10617v1&entry.124074799=Read"},
{"title": "SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving", "author": "Peizheng Li and Zhenghao Zhang and David Holtz and Hang Yu and Yutong Yang and Yuzhi Lai and Rui Song and Andreas Geiger and Andreas Zell", "abstract": "End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.", "link": "http://arxiv.org/abs/2512.10719v1", "date": "2025-12-11", "relevancy": 2.3784, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5989}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaceDrive%3A%20Infusing%20Spatial%20Awareness%20into%20VLM-based%20Autonomous%20Driving&body=Title%3A%20SpaceDrive%3A%20Infusing%20Spatial%20Awareness%20into%20VLM-based%20Autonomous%20Driving%0AAuthor%3A%20Peizheng%20Li%20and%20Zhenghao%20Zhang%20and%20David%20Holtz%20and%20Hang%20Yu%20and%20Yutong%20Yang%20and%20Yuzhi%20Lai%20and%20Rui%20Song%20and%20Andreas%20Geiger%20and%20Andreas%20Zell%0AAbstract%3A%20End-to-end%20autonomous%20driving%20methods%20built%20on%20vision%20language%20models%20%28VLMs%29%20have%20undergone%20rapid%20development%20driven%20by%20their%20universal%20visual%20understanding%20and%20strong%20reasoning%20capabilities%20obtained%20from%20the%20large-scale%20pretraining.%20However%2C%20we%20find%20that%20current%20VLMs%20struggle%20to%20understand%20fine-grained%203D%20spatial%20relationships%20which%20is%20a%20fundamental%20requirement%20for%20systems%20interacting%20with%20the%20physical%20world.%20To%20address%20this%20issue%2C%20we%20propose%20SpaceDrive%2C%20a%20spatial-aware%20VLM-based%20driving%20framework%20that%20treats%20spatial%20information%20as%20explicit%20positional%20encodings%20%28PEs%29%20instead%20of%20textual%20digit%20tokens%2C%20enabling%20joint%20reasoning%20over%20semantic%20and%20spatial%20representations.%20SpaceDrive%20employs%20a%20universal%20positional%20encoder%20to%20all%203D%20coordinates%20derived%20from%20multi-view%20depth%20estimation%2C%20historical%20ego-states%2C%20and%20text%20prompts.%20These%203D%20PEs%20are%20first%20superimposed%20to%20augment%20the%20corresponding%202D%20visual%20tokens.%20Meanwhile%2C%20they%20serve%20as%20a%20task-agnostic%20coordinate%20representation%2C%20replacing%20the%20digit-wise%20numerical%20tokens%20as%20both%20inputs%20and%20outputs%20for%20the%20VLM.%20This%20mechanism%20enables%20the%20model%20to%20better%20index%20specific%20visual%20semantics%20in%20spatial%20reasoning%20and%20directly%20regress%20trajectory%20coordinates%20rather%20than%20generating%20digit-by-digit%2C%20thereby%20enhancing%20planning%20accuracy.%20Extensive%20experiments%20validate%20that%20SpaceDrive%20achieves%20state-of-the-art%20open-loop%20performance%20on%20the%20nuScenes%20dataset%20and%20the%20second-best%20Driving%20Score%20of%2078.02%20on%20the%20Bench2Drive%20closed-loop%20benchmark%20over%20existing%20VLM-based%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaceDrive%253A%2520Infusing%2520Spatial%2520Awareness%2520into%2520VLM-based%2520Autonomous%2520Driving%26entry.906535625%3DPeizheng%2520Li%2520and%2520Zhenghao%2520Zhang%2520and%2520David%2520Holtz%2520and%2520Hang%2520Yu%2520and%2520Yutong%2520Yang%2520and%2520Yuzhi%2520Lai%2520and%2520Rui%2520Song%2520and%2520Andreas%2520Geiger%2520and%2520Andreas%2520Zell%26entry.1292438233%3DEnd-to-end%2520autonomous%2520driving%2520methods%2520built%2520on%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520undergone%2520rapid%2520development%2520driven%2520by%2520their%2520universal%2520visual%2520understanding%2520and%2520strong%2520reasoning%2520capabilities%2520obtained%2520from%2520the%2520large-scale%2520pretraining.%2520However%252C%2520we%2520find%2520that%2520current%2520VLMs%2520struggle%2520to%2520understand%2520fine-grained%25203D%2520spatial%2520relationships%2520which%2520is%2520a%2520fundamental%2520requirement%2520for%2520systems%2520interacting%2520with%2520the%2520physical%2520world.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520SpaceDrive%252C%2520a%2520spatial-aware%2520VLM-based%2520driving%2520framework%2520that%2520treats%2520spatial%2520information%2520as%2520explicit%2520positional%2520encodings%2520%2528PEs%2529%2520instead%2520of%2520textual%2520digit%2520tokens%252C%2520enabling%2520joint%2520reasoning%2520over%2520semantic%2520and%2520spatial%2520representations.%2520SpaceDrive%2520employs%2520a%2520universal%2520positional%2520encoder%2520to%2520all%25203D%2520coordinates%2520derived%2520from%2520multi-view%2520depth%2520estimation%252C%2520historical%2520ego-states%252C%2520and%2520text%2520prompts.%2520These%25203D%2520PEs%2520are%2520first%2520superimposed%2520to%2520augment%2520the%2520corresponding%25202D%2520visual%2520tokens.%2520Meanwhile%252C%2520they%2520serve%2520as%2520a%2520task-agnostic%2520coordinate%2520representation%252C%2520replacing%2520the%2520digit-wise%2520numerical%2520tokens%2520as%2520both%2520inputs%2520and%2520outputs%2520for%2520the%2520VLM.%2520This%2520mechanism%2520enables%2520the%2520model%2520to%2520better%2520index%2520specific%2520visual%2520semantics%2520in%2520spatial%2520reasoning%2520and%2520directly%2520regress%2520trajectory%2520coordinates%2520rather%2520than%2520generating%2520digit-by-digit%252C%2520thereby%2520enhancing%2520planning%2520accuracy.%2520Extensive%2520experiments%2520validate%2520that%2520SpaceDrive%2520achieves%2520state-of-the-art%2520open-loop%2520performance%2520on%2520the%2520nuScenes%2520dataset%2520and%2520the%2520second-best%2520Driving%2520Score%2520of%252078.02%2520on%2520the%2520Bench2Drive%2520closed-loop%2520benchmark%2520over%2520existing%2520VLM-based%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaceDrive%3A%20Infusing%20Spatial%20Awareness%20into%20VLM-based%20Autonomous%20Driving&entry.906535625=Peizheng%20Li%20and%20Zhenghao%20Zhang%20and%20David%20Holtz%20and%20Hang%20Yu%20and%20Yutong%20Yang%20and%20Yuzhi%20Lai%20and%20Rui%20Song%20and%20Andreas%20Geiger%20and%20Andreas%20Zell&entry.1292438233=End-to-end%20autonomous%20driving%20methods%20built%20on%20vision%20language%20models%20%28VLMs%29%20have%20undergone%20rapid%20development%20driven%20by%20their%20universal%20visual%20understanding%20and%20strong%20reasoning%20capabilities%20obtained%20from%20the%20large-scale%20pretraining.%20However%2C%20we%20find%20that%20current%20VLMs%20struggle%20to%20understand%20fine-grained%203D%20spatial%20relationships%20which%20is%20a%20fundamental%20requirement%20for%20systems%20interacting%20with%20the%20physical%20world.%20To%20address%20this%20issue%2C%20we%20propose%20SpaceDrive%2C%20a%20spatial-aware%20VLM-based%20driving%20framework%20that%20treats%20spatial%20information%20as%20explicit%20positional%20encodings%20%28PEs%29%20instead%20of%20textual%20digit%20tokens%2C%20enabling%20joint%20reasoning%20over%20semantic%20and%20spatial%20representations.%20SpaceDrive%20employs%20a%20universal%20positional%20encoder%20to%20all%203D%20coordinates%20derived%20from%20multi-view%20depth%20estimation%2C%20historical%20ego-states%2C%20and%20text%20prompts.%20These%203D%20PEs%20are%20first%20superimposed%20to%20augment%20the%20corresponding%202D%20visual%20tokens.%20Meanwhile%2C%20they%20serve%20as%20a%20task-agnostic%20coordinate%20representation%2C%20replacing%20the%20digit-wise%20numerical%20tokens%20as%20both%20inputs%20and%20outputs%20for%20the%20VLM.%20This%20mechanism%20enables%20the%20model%20to%20better%20index%20specific%20visual%20semantics%20in%20spatial%20reasoning%20and%20directly%20regress%20trajectory%20coordinates%20rather%20than%20generating%20digit-by-digit%2C%20thereby%20enhancing%20planning%20accuracy.%20Extensive%20experiments%20validate%20that%20SpaceDrive%20achieves%20state-of-the-art%20open-loop%20performance%20on%20the%20nuScenes%20dataset%20and%20the%20second-best%20Driving%20Score%20of%2078.02%20on%20the%20Bench2Drive%20closed-loop%20benchmark%20over%20existing%20VLM-based%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.10719v1&entry.124074799=Read"},
{"title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos", "author": "Yulu Gan and Ligeng Zhu and Dandan Shan and Baifeng Shi and Hongxu Yin and Boris Ivanovic and Song Han and Trevor Darrell and Jitendra Malik and Marco Pavone and Boyi Li", "abstract": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.", "link": "http://arxiv.org/abs/2512.10927v1", "date": "2025-12-11", "relevancy": 2.3737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5978}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoundationMotion%3A%20Auto-Labeling%20and%20Reasoning%20about%20Spatial%20Movement%20in%20Videos&body=Title%3A%20FoundationMotion%3A%20Auto-Labeling%20and%20Reasoning%20about%20Spatial%20Movement%20in%20Videos%0AAuthor%3A%20Yulu%20Gan%20and%20Ligeng%20Zhu%20and%20Dandan%20Shan%20and%20Baifeng%20Shi%20and%20Hongxu%20Yin%20and%20Boris%20Ivanovic%20and%20Song%20Han%20and%20Trevor%20Darrell%20and%20Jitendra%20Malik%20and%20Marco%20Pavone%20and%20Boyi%20Li%0AAbstract%3A%20Motion%20understanding%20is%20fundamental%20to%20physical%20reasoning%2C%20enabling%20models%20to%20infer%20dynamics%20and%20predict%20future%20states.%20However%2C%20state-of-the-art%20models%20still%20struggle%20on%20recent%20motion%20benchmarks%2C%20primarily%20due%20to%20the%20scarcity%20of%20large-scale%2C%20fine-grained%20motion%20datasets.%20Existing%20motion%20datasets%20are%20often%20constructed%20from%20costly%20manual%20annotation%2C%20severely%20limiting%20scalability.%20To%20address%20this%20challenge%2C%20we%20introduce%20FoundationMotion%2C%20a%20fully%20automated%20data%20curation%20pipeline%20that%20constructs%20large-scale%20motion%20datasets.%20Our%20approach%20first%20detects%20and%20tracks%20objects%20in%20videos%20to%20extract%20their%20trajectories%2C%20then%20leverages%20these%20trajectories%20and%20video%20frames%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20generate%20fine-grained%20captions%20and%20diverse%20question-answer%20pairs%20about%20motion%20and%20spatial%20reasoning.%20Using%20datasets%20produced%20by%20this%20pipeline%2C%20we%20fine-tune%20open-source%20models%20including%20NVILA-Video-15B%20and%20Qwen2.5-7B%2C%20achieving%20substantial%20improvements%20in%20motion%20understanding%20without%20compromising%20performance%20on%20other%20tasks.%20Notably%2C%20our%20models%20outperform%20strong%20closed-source%20baselines%20like%20Gemini-2.5%20Flash%20and%20large%20open-source%20models%20such%20as%20Qwen2.5-VL-72B%20across%20diverse%20motion%20understanding%20datasets%20and%20benchmarks.%20FoundationMotion%20thus%20provides%20a%20scalable%20solution%20for%20curating%20fine-grained%20motion%20datasets%20that%20enable%20effective%20fine-tuning%20of%20diverse%20models%20to%20enhance%20motion%20understanding%20and%20spatial%20reasoning%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundationMotion%253A%2520Auto-Labeling%2520and%2520Reasoning%2520about%2520Spatial%2520Movement%2520in%2520Videos%26entry.906535625%3DYulu%2520Gan%2520and%2520Ligeng%2520Zhu%2520and%2520Dandan%2520Shan%2520and%2520Baifeng%2520Shi%2520and%2520Hongxu%2520Yin%2520and%2520Boris%2520Ivanovic%2520and%2520Song%2520Han%2520and%2520Trevor%2520Darrell%2520and%2520Jitendra%2520Malik%2520and%2520Marco%2520Pavone%2520and%2520Boyi%2520Li%26entry.1292438233%3DMotion%2520understanding%2520is%2520fundamental%2520to%2520physical%2520reasoning%252C%2520enabling%2520models%2520to%2520infer%2520dynamics%2520and%2520predict%2520future%2520states.%2520However%252C%2520state-of-the-art%2520models%2520still%2520struggle%2520on%2520recent%2520motion%2520benchmarks%252C%2520primarily%2520due%2520to%2520the%2520scarcity%2520of%2520large-scale%252C%2520fine-grained%2520motion%2520datasets.%2520Existing%2520motion%2520datasets%2520are%2520often%2520constructed%2520from%2520costly%2520manual%2520annotation%252C%2520severely%2520limiting%2520scalability.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520FoundationMotion%252C%2520a%2520fully%2520automated%2520data%2520curation%2520pipeline%2520that%2520constructs%2520large-scale%2520motion%2520datasets.%2520Our%2520approach%2520first%2520detects%2520and%2520tracks%2520objects%2520in%2520videos%2520to%2520extract%2520their%2520trajectories%252C%2520then%2520leverages%2520these%2520trajectories%2520and%2520video%2520frames%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520generate%2520fine-grained%2520captions%2520and%2520diverse%2520question-answer%2520pairs%2520about%2520motion%2520and%2520spatial%2520reasoning.%2520Using%2520datasets%2520produced%2520by%2520this%2520pipeline%252C%2520we%2520fine-tune%2520open-source%2520models%2520including%2520NVILA-Video-15B%2520and%2520Qwen2.5-7B%252C%2520achieving%2520substantial%2520improvements%2520in%2520motion%2520understanding%2520without%2520compromising%2520performance%2520on%2520other%2520tasks.%2520Notably%252C%2520our%2520models%2520outperform%2520strong%2520closed-source%2520baselines%2520like%2520Gemini-2.5%2520Flash%2520and%2520large%2520open-source%2520models%2520such%2520as%2520Qwen2.5-VL-72B%2520across%2520diverse%2520motion%2520understanding%2520datasets%2520and%2520benchmarks.%2520FoundationMotion%2520thus%2520provides%2520a%2520scalable%2520solution%2520for%2520curating%2520fine-grained%2520motion%2520datasets%2520that%2520enable%2520effective%2520fine-tuning%2520of%2520diverse%2520models%2520to%2520enhance%2520motion%2520understanding%2520and%2520spatial%2520reasoning%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoundationMotion%3A%20Auto-Labeling%20and%20Reasoning%20about%20Spatial%20Movement%20in%20Videos&entry.906535625=Yulu%20Gan%20and%20Ligeng%20Zhu%20and%20Dandan%20Shan%20and%20Baifeng%20Shi%20and%20Hongxu%20Yin%20and%20Boris%20Ivanovic%20and%20Song%20Han%20and%20Trevor%20Darrell%20and%20Jitendra%20Malik%20and%20Marco%20Pavone%20and%20Boyi%20Li&entry.1292438233=Motion%20understanding%20is%20fundamental%20to%20physical%20reasoning%2C%20enabling%20models%20to%20infer%20dynamics%20and%20predict%20future%20states.%20However%2C%20state-of-the-art%20models%20still%20struggle%20on%20recent%20motion%20benchmarks%2C%20primarily%20due%20to%20the%20scarcity%20of%20large-scale%2C%20fine-grained%20motion%20datasets.%20Existing%20motion%20datasets%20are%20often%20constructed%20from%20costly%20manual%20annotation%2C%20severely%20limiting%20scalability.%20To%20address%20this%20challenge%2C%20we%20introduce%20FoundationMotion%2C%20a%20fully%20automated%20data%20curation%20pipeline%20that%20constructs%20large-scale%20motion%20datasets.%20Our%20approach%20first%20detects%20and%20tracks%20objects%20in%20videos%20to%20extract%20their%20trajectories%2C%20then%20leverages%20these%20trajectories%20and%20video%20frames%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20generate%20fine-grained%20captions%20and%20diverse%20question-answer%20pairs%20about%20motion%20and%20spatial%20reasoning.%20Using%20datasets%20produced%20by%20this%20pipeline%2C%20we%20fine-tune%20open-source%20models%20including%20NVILA-Video-15B%20and%20Qwen2.5-7B%2C%20achieving%20substantial%20improvements%20in%20motion%20understanding%20without%20compromising%20performance%20on%20other%20tasks.%20Notably%2C%20our%20models%20outperform%20strong%20closed-source%20baselines%20like%20Gemini-2.5%20Flash%20and%20large%20open-source%20models%20such%20as%20Qwen2.5-VL-72B%20across%20diverse%20motion%20understanding%20datasets%20and%20benchmarks.%20FoundationMotion%20thus%20provides%20a%20scalable%20solution%20for%20curating%20fine-grained%20motion%20datasets%20that%20enable%20effective%20fine-tuning%20of%20diverse%20models%20to%20enhance%20motion%20understanding%20and%20spatial%20reasoning%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2512.10927v1&entry.124074799=Read"},
{"title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "author": "Ke Xing and Xiaojie Jin and Longfei Li and Yuyang Yin and Hanwen Liang and Guixun Luo and Chen Fang and Jue Wang and Konstantinos N. Plataniotis and Yao Zhao and Yunchao Wei", "abstract": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "link": "http://arxiv.org/abs/2512.09363v2", "date": "2025-12-11", "relevancy": 2.3722, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6383}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5933}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StereoWorld%3A%20Geometry-Aware%20Monocular-to-Stereo%20Video%20Generation&body=Title%3A%20StereoWorld%3A%20Geometry-Aware%20Monocular-to-Stereo%20Video%20Generation%0AAuthor%3A%20Ke%20Xing%20and%20Xiaojie%20Jin%20and%20Longfei%20Li%20and%20Yuyang%20Yin%20and%20Hanwen%20Liang%20and%20Guixun%20Luo%20and%20Chen%20Fang%20and%20Jue%20Wang%20and%20Konstantinos%20N.%20Plataniotis%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%0AAbstract%3A%20The%20growing%20adoption%20of%20XR%20devices%20has%20fueled%20strong%20demand%20for%20high-quality%20stereo%20video%2C%20yet%20its%20production%20remains%20costly%20and%20artifact-prone.%20To%20address%20this%20challenge%2C%20we%20present%20StereoWorld%2C%20an%20end-to-end%20framework%20that%20repurposes%20a%20pretrained%20video%20generator%20for%20high-fidelity%20monocular-to-stereo%20video%20generation.%20Our%20framework%20jointly%20conditions%20the%20model%20on%20the%20monocular%20video%20input%20while%20explicitly%20supervising%20the%20generation%20with%20a%20geometry-aware%20regularization%20to%20ensure%203D%20structural%20fidelity.%20A%20spatio-temporal%20tiling%20scheme%20is%20further%20integrated%20to%20enable%20efficient%2C%20high-resolution%20synthesis.%20To%20enable%20large-scale%20training%20and%20evaluation%2C%20we%20curate%20a%20high-definition%20stereo%20video%20dataset%20containing%20over%2011M%20frames%20aligned%20to%20natural%20human%20interpupillary%20distance%20%28IPD%29.%20Extensive%20experiments%20demonstrate%20that%20StereoWorld%20substantially%20outperforms%20prior%20methods%2C%20generating%20stereo%20videos%20with%20superior%20visual%20fidelity%20and%20geometric%20consistency.%20The%20project%20webpage%20is%20available%20at%20https%3A//ke-xing.github.io/StereoWorld/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09363v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereoWorld%253A%2520Geometry-Aware%2520Monocular-to-Stereo%2520Video%2520Generation%26entry.906535625%3DKe%2520Xing%2520and%2520Xiaojie%2520Jin%2520and%2520Longfei%2520Li%2520and%2520Yuyang%2520Yin%2520and%2520Hanwen%2520Liang%2520and%2520Guixun%2520Luo%2520and%2520Chen%2520Fang%2520and%2520Jue%2520Wang%2520and%2520Konstantinos%2520N.%2520Plataniotis%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%26entry.1292438233%3DThe%2520growing%2520adoption%2520of%2520XR%2520devices%2520has%2520fueled%2520strong%2520demand%2520for%2520high-quality%2520stereo%2520video%252C%2520yet%2520its%2520production%2520remains%2520costly%2520and%2520artifact-prone.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520StereoWorld%252C%2520an%2520end-to-end%2520framework%2520that%2520repurposes%2520a%2520pretrained%2520video%2520generator%2520for%2520high-fidelity%2520monocular-to-stereo%2520video%2520generation.%2520Our%2520framework%2520jointly%2520conditions%2520the%2520model%2520on%2520the%2520monocular%2520video%2520input%2520while%2520explicitly%2520supervising%2520the%2520generation%2520with%2520a%2520geometry-aware%2520regularization%2520to%2520ensure%25203D%2520structural%2520fidelity.%2520A%2520spatio-temporal%2520tiling%2520scheme%2520is%2520further%2520integrated%2520to%2520enable%2520efficient%252C%2520high-resolution%2520synthesis.%2520To%2520enable%2520large-scale%2520training%2520and%2520evaluation%252C%2520we%2520curate%2520a%2520high-definition%2520stereo%2520video%2520dataset%2520containing%2520over%252011M%2520frames%2520aligned%2520to%2520natural%2520human%2520interpupillary%2520distance%2520%2528IPD%2529.%2520Extensive%2520experiments%2520demonstrate%2520that%2520StereoWorld%2520substantially%2520outperforms%2520prior%2520methods%252C%2520generating%2520stereo%2520videos%2520with%2520superior%2520visual%2520fidelity%2520and%2520geometric%2520consistency.%2520The%2520project%2520webpage%2520is%2520available%2520at%2520https%253A//ke-xing.github.io/StereoWorld/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09363v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StereoWorld%3A%20Geometry-Aware%20Monocular-to-Stereo%20Video%20Generation&entry.906535625=Ke%20Xing%20and%20Xiaojie%20Jin%20and%20Longfei%20Li%20and%20Yuyang%20Yin%20and%20Hanwen%20Liang%20and%20Guixun%20Luo%20and%20Chen%20Fang%20and%20Jue%20Wang%20and%20Konstantinos%20N.%20Plataniotis%20and%20Yao%20Zhao%20and%20Yunchao%20Wei&entry.1292438233=The%20growing%20adoption%20of%20XR%20devices%20has%20fueled%20strong%20demand%20for%20high-quality%20stereo%20video%2C%20yet%20its%20production%20remains%20costly%20and%20artifact-prone.%20To%20address%20this%20challenge%2C%20we%20present%20StereoWorld%2C%20an%20end-to-end%20framework%20that%20repurposes%20a%20pretrained%20video%20generator%20for%20high-fidelity%20monocular-to-stereo%20video%20generation.%20Our%20framework%20jointly%20conditions%20the%20model%20on%20the%20monocular%20video%20input%20while%20explicitly%20supervising%20the%20generation%20with%20a%20geometry-aware%20regularization%20to%20ensure%203D%20structural%20fidelity.%20A%20spatio-temporal%20tiling%20scheme%20is%20further%20integrated%20to%20enable%20efficient%2C%20high-resolution%20synthesis.%20To%20enable%20large-scale%20training%20and%20evaluation%2C%20we%20curate%20a%20high-definition%20stereo%20video%20dataset%20containing%20over%2011M%20frames%20aligned%20to%20natural%20human%20interpupillary%20distance%20%28IPD%29.%20Extensive%20experiments%20demonstrate%20that%20StereoWorld%20substantially%20outperforms%20prior%20methods%2C%20generating%20stereo%20videos%20with%20superior%20visual%20fidelity%20and%20geometric%20consistency.%20The%20project%20webpage%20is%20available%20at%20https%3A//ke-xing.github.io/StereoWorld/.&entry.1838667208=http%3A//arxiv.org/abs/2512.09363v2&entry.124074799=Read"},
{"title": "CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding", "author": "Fevziye Irem Eyiokur and Dogucan Yaman and Haz\u0131m Kemal Ekenel and Alexander Waibel", "abstract": "We address Embodied Reference Understanding, the task of predicting the object a person in the scene refers to through pointing gesture and language. This requires multimodal reasoning over text, visual pointing cues, and scene context, yet existing methods often fail to fully exploit visual disambiguation signals. We also observe that while the referent often aligns with the head-to-fingertip direction, in many cases it aligns more closely with the wrist-to-fingertip direction, making a single-line assumption overly limiting. To address this, we propose a dual-model framework, where one model learns from the head-to-fingertip direction and the other from the wrist-to-fingertip direction. We introduce a Gaussian ray heatmap representation of these lines and use them as input to provide a strong supervisory signal that encourages the model to better attend to pointing cues. To fuse their complementary strengths, we present the CLIP-Aware Pointing Ensemble module, which performs a hybrid ensemble guided by CLIP features. We further incorporate an auxiliary object center prediction head to enhance referent localization. We validate our approach on YouRefIt, achieving 75.0 mAP at 0.25 IoU, alongside state-of-the-art CLIP and C_D scores, and demonstrate its generality on unseen CAESAR and ISL Pointing, showing robust performance across benchmarks.", "link": "http://arxiv.org/abs/2507.21888v4", "date": "2025-12-11", "relevancy": 2.3718, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6143}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%20Embodied%20Reference%20Understanding&body=Title%3A%20CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%20Embodied%20Reference%20Understanding%0AAuthor%3A%20Fevziye%20Irem%20Eyiokur%20and%20Dogucan%20Yaman%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel%0AAbstract%3A%20We%20address%20Embodied%20Reference%20Understanding%2C%20the%20task%20of%20predicting%20the%20object%20a%20person%20in%20the%20scene%20refers%20to%20through%20pointing%20gesture%20and%20language.%20This%20requires%20multimodal%20reasoning%20over%20text%2C%20visual%20pointing%20cues%2C%20and%20scene%20context%2C%20yet%20existing%20methods%20often%20fail%20to%20fully%20exploit%20visual%20disambiguation%20signals.%20We%20also%20observe%20that%20while%20the%20referent%20often%20aligns%20with%20the%20head-to-fingertip%20direction%2C%20in%20many%20cases%20it%20aligns%20more%20closely%20with%20the%20wrist-to-fingertip%20direction%2C%20making%20a%20single-line%20assumption%20overly%20limiting.%20To%20address%20this%2C%20we%20propose%20a%20dual-model%20framework%2C%20where%20one%20model%20learns%20from%20the%20head-to-fingertip%20direction%20and%20the%20other%20from%20the%20wrist-to-fingertip%20direction.%20We%20introduce%20a%20Gaussian%20ray%20heatmap%20representation%20of%20these%20lines%20and%20use%20them%20as%20input%20to%20provide%20a%20strong%20supervisory%20signal%20that%20encourages%20the%20model%20to%20better%20attend%20to%20pointing%20cues.%20To%20fuse%20their%20complementary%20strengths%2C%20we%20present%20the%20CLIP-Aware%20Pointing%20Ensemble%20module%2C%20which%20performs%20a%20hybrid%20ensemble%20guided%20by%20CLIP%20features.%20We%20further%20incorporate%20an%20auxiliary%20object%20center%20prediction%20head%20to%20enhance%20referent%20localization.%20We%20validate%20our%20approach%20on%20YouRefIt%2C%20achieving%2075.0%20mAP%20at%200.25%20IoU%2C%20alongside%20state-of-the-art%20CLIP%20and%20C_D%20scores%2C%20and%20demonstrate%20its%20generality%20on%20unseen%20CAESAR%20and%20ISL%20Pointing%2C%20showing%20robust%20performance%20across%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2507.21888v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPE%253A%2520A%2520CLIP-Aware%2520Pointing%2520Ensemble%2520of%2520Complementary%2520Heatmap%2520Cues%2520for%2520Embodied%2520Reference%2520Understanding%26entry.906535625%3DFevziye%2520Irem%2520Eyiokur%2520and%2520Dogucan%2520Yaman%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Alexander%2520Waibel%26entry.1292438233%3DWe%2520address%2520Embodied%2520Reference%2520Understanding%252C%2520the%2520task%2520of%2520predicting%2520the%2520object%2520a%2520person%2520in%2520the%2520scene%2520refers%2520to%2520through%2520pointing%2520gesture%2520and%2520language.%2520This%2520requires%2520multimodal%2520reasoning%2520over%2520text%252C%2520visual%2520pointing%2520cues%252C%2520and%2520scene%2520context%252C%2520yet%2520existing%2520methods%2520often%2520fail%2520to%2520fully%2520exploit%2520visual%2520disambiguation%2520signals.%2520We%2520also%2520observe%2520that%2520while%2520the%2520referent%2520often%2520aligns%2520with%2520the%2520head-to-fingertip%2520direction%252C%2520in%2520many%2520cases%2520it%2520aligns%2520more%2520closely%2520with%2520the%2520wrist-to-fingertip%2520direction%252C%2520making%2520a%2520single-line%2520assumption%2520overly%2520limiting.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520dual-model%2520framework%252C%2520where%2520one%2520model%2520learns%2520from%2520the%2520head-to-fingertip%2520direction%2520and%2520the%2520other%2520from%2520the%2520wrist-to-fingertip%2520direction.%2520We%2520introduce%2520a%2520Gaussian%2520ray%2520heatmap%2520representation%2520of%2520these%2520lines%2520and%2520use%2520them%2520as%2520input%2520to%2520provide%2520a%2520strong%2520supervisory%2520signal%2520that%2520encourages%2520the%2520model%2520to%2520better%2520attend%2520to%2520pointing%2520cues.%2520To%2520fuse%2520their%2520complementary%2520strengths%252C%2520we%2520present%2520the%2520CLIP-Aware%2520Pointing%2520Ensemble%2520module%252C%2520which%2520performs%2520a%2520hybrid%2520ensemble%2520guided%2520by%2520CLIP%2520features.%2520We%2520further%2520incorporate%2520an%2520auxiliary%2520object%2520center%2520prediction%2520head%2520to%2520enhance%2520referent%2520localization.%2520We%2520validate%2520our%2520approach%2520on%2520YouRefIt%252C%2520achieving%252075.0%2520mAP%2520at%25200.25%2520IoU%252C%2520alongside%2520state-of-the-art%2520CLIP%2520and%2520C_D%2520scores%252C%2520and%2520demonstrate%2520its%2520generality%2520on%2520unseen%2520CAESAR%2520and%2520ISL%2520Pointing%252C%2520showing%2520robust%2520performance%2520across%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21888v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%20Embodied%20Reference%20Understanding&entry.906535625=Fevziye%20Irem%20Eyiokur%20and%20Dogucan%20Yaman%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel&entry.1292438233=We%20address%20Embodied%20Reference%20Understanding%2C%20the%20task%20of%20predicting%20the%20object%20a%20person%20in%20the%20scene%20refers%20to%20through%20pointing%20gesture%20and%20language.%20This%20requires%20multimodal%20reasoning%20over%20text%2C%20visual%20pointing%20cues%2C%20and%20scene%20context%2C%20yet%20existing%20methods%20often%20fail%20to%20fully%20exploit%20visual%20disambiguation%20signals.%20We%20also%20observe%20that%20while%20the%20referent%20often%20aligns%20with%20the%20head-to-fingertip%20direction%2C%20in%20many%20cases%20it%20aligns%20more%20closely%20with%20the%20wrist-to-fingertip%20direction%2C%20making%20a%20single-line%20assumption%20overly%20limiting.%20To%20address%20this%2C%20we%20propose%20a%20dual-model%20framework%2C%20where%20one%20model%20learns%20from%20the%20head-to-fingertip%20direction%20and%20the%20other%20from%20the%20wrist-to-fingertip%20direction.%20We%20introduce%20a%20Gaussian%20ray%20heatmap%20representation%20of%20these%20lines%20and%20use%20them%20as%20input%20to%20provide%20a%20strong%20supervisory%20signal%20that%20encourages%20the%20model%20to%20better%20attend%20to%20pointing%20cues.%20To%20fuse%20their%20complementary%20strengths%2C%20we%20present%20the%20CLIP-Aware%20Pointing%20Ensemble%20module%2C%20which%20performs%20a%20hybrid%20ensemble%20guided%20by%20CLIP%20features.%20We%20further%20incorporate%20an%20auxiliary%20object%20center%20prediction%20head%20to%20enhance%20referent%20localization.%20We%20validate%20our%20approach%20on%20YouRefIt%2C%20achieving%2075.0%20mAP%20at%200.25%20IoU%2C%20alongside%20state-of-the-art%20CLIP%20and%20C_D%20scores%2C%20and%20demonstrate%20its%20generality%20on%20unseen%20CAESAR%20and%20ISL%20Pointing%2C%20showing%20robust%20performance%20across%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2507.21888v4&entry.124074799=Read"},
{"title": "Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification", "author": "Liang Peng and Haopeng Liu and Yixuan Ye and Cheng Liu and Wenjun Shen and Si Wu and Hau-San Wong", "abstract": "Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.", "link": "http://arxiv.org/abs/2512.10640v1", "date": "2025-12-11", "relevancy": 2.3587, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4753}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4713}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refinement%20Contrastive%20Learning%20of%20Cell-Gene%20Associations%20for%20Unsupervised%20Cell%20Type%20Identification&body=Title%3A%20Refinement%20Contrastive%20Learning%20of%20Cell-Gene%20Associations%20for%20Unsupervised%20Cell%20Type%20Identification%0AAuthor%3A%20Liang%20Peng%20and%20Haopeng%20Liu%20and%20Yixuan%20Ye%20and%20Cheng%20Liu%20and%20Wenjun%20Shen%20and%20Si%20Wu%20and%20Hau-San%20Wong%0AAbstract%3A%20Unsupervised%20cell%20type%20identification%20is%20crucial%20for%20uncovering%20and%20characterizing%20heterogeneous%20populations%20in%20single%20cell%20omics%20studies.%20Although%20a%20range%20of%20clustering%20methods%20have%20been%20developed%2C%20most%20focus%20exclusively%20on%20intrinsic%20cellular%20structure%20and%20ignore%20the%20pivotal%20role%20of%20cell-gene%20associations%2C%20which%20limits%20their%20ability%20to%20distinguish%20closely%20related%20cell%20types.%20To%20this%20end%2C%20we%20propose%20a%20Refinement%20Contrastive%20Learning%20framework%20%28scRCL%29%20that%20explicitly%20incorporates%20cell-gene%20interactions%20to%20derive%20more%20informative%20representations.%20Specifically%2C%20we%20introduce%20two%20contrastive%20distribution%20alignment%20components%20that%20reveal%20reliable%20intrinsic%20cellular%20structures%20by%20effectively%20exploiting%20cell-cell%20structural%20relationships.%20Additionally%2C%20we%20develop%20a%20refinement%20module%20that%20integrates%20gene-correlation%20structure%20learning%20to%20enhance%20cell%20embeddings%20by%20capturing%20underlying%20cell-gene%20associations.%20This%20module%20strengthens%20connections%20between%20cells%20and%20their%20associated%20genes%2C%20refining%20the%20representation%20learning%20to%20exploiting%20biologically%20meaningful%20relationships.%20Extensive%20experiments%20on%20several%20single-cell%20RNA-seq%20and%20spatial%20transcriptomics%20benchmark%20datasets%20demonstrate%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20baselines%20in%20cell-type%20identification%20accuracy.%20Moreover%2C%20downstream%20biological%20analyses%20confirm%20that%20the%20recovered%20cell%20populations%20exhibit%20coherent%20gene-expression%20signatures%2C%20further%20validating%20the%20biological%20relevance%20of%20our%20approach.%20The%20code%20is%20available%20at%20https%3A//github.com/THPengL/scRCL.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefinement%2520Contrastive%2520Learning%2520of%2520Cell-Gene%2520Associations%2520for%2520Unsupervised%2520Cell%2520Type%2520Identification%26entry.906535625%3DLiang%2520Peng%2520and%2520Haopeng%2520Liu%2520and%2520Yixuan%2520Ye%2520and%2520Cheng%2520Liu%2520and%2520Wenjun%2520Shen%2520and%2520Si%2520Wu%2520and%2520Hau-San%2520Wong%26entry.1292438233%3DUnsupervised%2520cell%2520type%2520identification%2520is%2520crucial%2520for%2520uncovering%2520and%2520characterizing%2520heterogeneous%2520populations%2520in%2520single%2520cell%2520omics%2520studies.%2520Although%2520a%2520range%2520of%2520clustering%2520methods%2520have%2520been%2520developed%252C%2520most%2520focus%2520exclusively%2520on%2520intrinsic%2520cellular%2520structure%2520and%2520ignore%2520the%2520pivotal%2520role%2520of%2520cell-gene%2520associations%252C%2520which%2520limits%2520their%2520ability%2520to%2520distinguish%2520closely%2520related%2520cell%2520types.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520Refinement%2520Contrastive%2520Learning%2520framework%2520%2528scRCL%2529%2520that%2520explicitly%2520incorporates%2520cell-gene%2520interactions%2520to%2520derive%2520more%2520informative%2520representations.%2520Specifically%252C%2520we%2520introduce%2520two%2520contrastive%2520distribution%2520alignment%2520components%2520that%2520reveal%2520reliable%2520intrinsic%2520cellular%2520structures%2520by%2520effectively%2520exploiting%2520cell-cell%2520structural%2520relationships.%2520Additionally%252C%2520we%2520develop%2520a%2520refinement%2520module%2520that%2520integrates%2520gene-correlation%2520structure%2520learning%2520to%2520enhance%2520cell%2520embeddings%2520by%2520capturing%2520underlying%2520cell-gene%2520associations.%2520This%2520module%2520strengthens%2520connections%2520between%2520cells%2520and%2520their%2520associated%2520genes%252C%2520refining%2520the%2520representation%2520learning%2520to%2520exploiting%2520biologically%2520meaningful%2520relationships.%2520Extensive%2520experiments%2520on%2520several%2520single-cell%2520RNA-seq%2520and%2520spatial%2520transcriptomics%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520cell-type%2520identification%2520accuracy.%2520Moreover%252C%2520downstream%2520biological%2520analyses%2520confirm%2520that%2520the%2520recovered%2520cell%2520populations%2520exhibit%2520coherent%2520gene-expression%2520signatures%252C%2520further%2520validating%2520the%2520biological%2520relevance%2520of%2520our%2520approach.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/THPengL/scRCL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refinement%20Contrastive%20Learning%20of%20Cell-Gene%20Associations%20for%20Unsupervised%20Cell%20Type%20Identification&entry.906535625=Liang%20Peng%20and%20Haopeng%20Liu%20and%20Yixuan%20Ye%20and%20Cheng%20Liu%20and%20Wenjun%20Shen%20and%20Si%20Wu%20and%20Hau-San%20Wong&entry.1292438233=Unsupervised%20cell%20type%20identification%20is%20crucial%20for%20uncovering%20and%20characterizing%20heterogeneous%20populations%20in%20single%20cell%20omics%20studies.%20Although%20a%20range%20of%20clustering%20methods%20have%20been%20developed%2C%20most%20focus%20exclusively%20on%20intrinsic%20cellular%20structure%20and%20ignore%20the%20pivotal%20role%20of%20cell-gene%20associations%2C%20which%20limits%20their%20ability%20to%20distinguish%20closely%20related%20cell%20types.%20To%20this%20end%2C%20we%20propose%20a%20Refinement%20Contrastive%20Learning%20framework%20%28scRCL%29%20that%20explicitly%20incorporates%20cell-gene%20interactions%20to%20derive%20more%20informative%20representations.%20Specifically%2C%20we%20introduce%20two%20contrastive%20distribution%20alignment%20components%20that%20reveal%20reliable%20intrinsic%20cellular%20structures%20by%20effectively%20exploiting%20cell-cell%20structural%20relationships.%20Additionally%2C%20we%20develop%20a%20refinement%20module%20that%20integrates%20gene-correlation%20structure%20learning%20to%20enhance%20cell%20embeddings%20by%20capturing%20underlying%20cell-gene%20associations.%20This%20module%20strengthens%20connections%20between%20cells%20and%20their%20associated%20genes%2C%20refining%20the%20representation%20learning%20to%20exploiting%20biologically%20meaningful%20relationships.%20Extensive%20experiments%20on%20several%20single-cell%20RNA-seq%20and%20spatial%20transcriptomics%20benchmark%20datasets%20demonstrate%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20baselines%20in%20cell-type%20identification%20accuracy.%20Moreover%2C%20downstream%20biological%20analyses%20confirm%20that%20the%20recovered%20cell%20populations%20exhibit%20coherent%20gene-expression%20signatures%2C%20further%20validating%20the%20biological%20relevance%20of%20our%20approach.%20The%20code%20is%20available%20at%20https%3A//github.com/THPengL/scRCL.&entry.1838667208=http%3A//arxiv.org/abs/2512.10640v1&entry.124074799=Read"},
{"title": "If generative AI is the answer, what is the question?", "author": "Ambuj Tewari", "abstract": "Beginning with text and images, generative AI has expanded to audio, video, computer code, and molecules. Yet, if generative AI is the answer, what is the question? We explore the foundations of generation as a distinct machine learning task with connections to prediction, compression, and decision-making. We survey five major generative model families: autoregressive models, variational autoencoders, normalizing flows, generative adversarial networks, and diffusion models. We then introduce a probabilistic framework that emphasizes the distinction between density estimation and generation. We review a game-theoretic framework with a two-player adversary-learner setup to study generation. We discuss post-training modifications that prepare generative models for deployment. We end by highlighting some important topics in socially responsible generation such as privacy, detection of AI-generated content, and copyright and IP. We adopt a task-first framing of generation, focusing on what generation is as a machine learning problem, rather than only on how models implement it.", "link": "http://arxiv.org/abs/2509.06120v2", "date": "2025-12-11", "relevancy": 2.3569, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.635}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5666}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20If%20generative%20AI%20is%20the%20answer%2C%20what%20is%20the%20question%3F&body=Title%3A%20If%20generative%20AI%20is%20the%20answer%2C%20what%20is%20the%20question%3F%0AAuthor%3A%20Ambuj%20Tewari%0AAbstract%3A%20Beginning%20with%20text%20and%20images%2C%20generative%20AI%20has%20expanded%20to%20audio%2C%20video%2C%20computer%20code%2C%20and%20molecules.%20Yet%2C%20if%20generative%20AI%20is%20the%20answer%2C%20what%20is%20the%20question%3F%20We%20explore%20the%20foundations%20of%20generation%20as%20a%20distinct%20machine%20learning%20task%20with%20connections%20to%20prediction%2C%20compression%2C%20and%20decision-making.%20We%20survey%20five%20major%20generative%20model%20families%3A%20autoregressive%20models%2C%20variational%20autoencoders%2C%20normalizing%20flows%2C%20generative%20adversarial%20networks%2C%20and%20diffusion%20models.%20We%20then%20introduce%20a%20probabilistic%20framework%20that%20emphasizes%20the%20distinction%20between%20density%20estimation%20and%20generation.%20We%20review%20a%20game-theoretic%20framework%20with%20a%20two-player%20adversary-learner%20setup%20to%20study%20generation.%20We%20discuss%20post-training%20modifications%20that%20prepare%20generative%20models%20for%20deployment.%20We%20end%20by%20highlighting%20some%20important%20topics%20in%20socially%20responsible%20generation%20such%20as%20privacy%2C%20detection%20of%20AI-generated%20content%2C%20and%20copyright%20and%20IP.%20We%20adopt%20a%20task-first%20framing%20of%20generation%2C%20focusing%20on%20what%20generation%20is%20as%20a%20machine%20learning%20problem%2C%20rather%20than%20only%20on%20how%20models%20implement%20it.%0ALink%3A%20http%3A//arxiv.org/abs/2509.06120v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIf%2520generative%2520AI%2520is%2520the%2520answer%252C%2520what%2520is%2520the%2520question%253F%26entry.906535625%3DAmbuj%2520Tewari%26entry.1292438233%3DBeginning%2520with%2520text%2520and%2520images%252C%2520generative%2520AI%2520has%2520expanded%2520to%2520audio%252C%2520video%252C%2520computer%2520code%252C%2520and%2520molecules.%2520Yet%252C%2520if%2520generative%2520AI%2520is%2520the%2520answer%252C%2520what%2520is%2520the%2520question%253F%2520We%2520explore%2520the%2520foundations%2520of%2520generation%2520as%2520a%2520distinct%2520machine%2520learning%2520task%2520with%2520connections%2520to%2520prediction%252C%2520compression%252C%2520and%2520decision-making.%2520We%2520survey%2520five%2520major%2520generative%2520model%2520families%253A%2520autoregressive%2520models%252C%2520variational%2520autoencoders%252C%2520normalizing%2520flows%252C%2520generative%2520adversarial%2520networks%252C%2520and%2520diffusion%2520models.%2520We%2520then%2520introduce%2520a%2520probabilistic%2520framework%2520that%2520emphasizes%2520the%2520distinction%2520between%2520density%2520estimation%2520and%2520generation.%2520We%2520review%2520a%2520game-theoretic%2520framework%2520with%2520a%2520two-player%2520adversary-learner%2520setup%2520to%2520study%2520generation.%2520We%2520discuss%2520post-training%2520modifications%2520that%2520prepare%2520generative%2520models%2520for%2520deployment.%2520We%2520end%2520by%2520highlighting%2520some%2520important%2520topics%2520in%2520socially%2520responsible%2520generation%2520such%2520as%2520privacy%252C%2520detection%2520of%2520AI-generated%2520content%252C%2520and%2520copyright%2520and%2520IP.%2520We%2520adopt%2520a%2520task-first%2520framing%2520of%2520generation%252C%2520focusing%2520on%2520what%2520generation%2520is%2520as%2520a%2520machine%2520learning%2520problem%252C%2520rather%2520than%2520only%2520on%2520how%2520models%2520implement%2520it.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06120v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=If%20generative%20AI%20is%20the%20answer%2C%20what%20is%20the%20question%3F&entry.906535625=Ambuj%20Tewari&entry.1292438233=Beginning%20with%20text%20and%20images%2C%20generative%20AI%20has%20expanded%20to%20audio%2C%20video%2C%20computer%20code%2C%20and%20molecules.%20Yet%2C%20if%20generative%20AI%20is%20the%20answer%2C%20what%20is%20the%20question%3F%20We%20explore%20the%20foundations%20of%20generation%20as%20a%20distinct%20machine%20learning%20task%20with%20connections%20to%20prediction%2C%20compression%2C%20and%20decision-making.%20We%20survey%20five%20major%20generative%20model%20families%3A%20autoregressive%20models%2C%20variational%20autoencoders%2C%20normalizing%20flows%2C%20generative%20adversarial%20networks%2C%20and%20diffusion%20models.%20We%20then%20introduce%20a%20probabilistic%20framework%20that%20emphasizes%20the%20distinction%20between%20density%20estimation%20and%20generation.%20We%20review%20a%20game-theoretic%20framework%20with%20a%20two-player%20adversary-learner%20setup%20to%20study%20generation.%20We%20discuss%20post-training%20modifications%20that%20prepare%20generative%20models%20for%20deployment.%20We%20end%20by%20highlighting%20some%20important%20topics%20in%20socially%20responsible%20generation%20such%20as%20privacy%2C%20detection%20of%20AI-generated%20content%2C%20and%20copyright%20and%20IP.%20We%20adopt%20a%20task-first%20framing%20of%20generation%2C%20focusing%20on%20what%20generation%20is%20as%20a%20machine%20learning%20problem%2C%20rather%20than%20only%20on%20how%20models%20implement%20it.&entry.1838667208=http%3A//arxiv.org/abs/2509.06120v2&entry.124074799=Read"},
{"title": "Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach", "author": "Yueying Ke", "abstract": "Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification, predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.\n  We introduce DiatomCascadeNet (H-COFGS), a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.\n  H-COFGS matches flat baselines at the species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5% of misclassified species are correctly predicted at the genus level, versus 67.2% for flat baselines. H-COFGS reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).\n  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.", "link": "http://arxiv.org/abs/2512.06613v2", "date": "2025-12-11", "relevancy": 2.3542, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4842}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4697}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Deep%20Learning%20for%20Diatom%20Image%20Classification%3A%20A%20Multi-Level%20Taxonomic%20Approach&body=Title%3A%20Hierarchical%20Deep%20Learning%20for%20Diatom%20Image%20Classification%3A%20A%20Multi-Level%20Taxonomic%20Approach%0AAuthor%3A%20Yueying%20Ke%0AAbstract%3A%20Accurate%20taxonomic%20identification%20of%20diatoms%20is%20essential%20for%20aquatic%20ecosystem%20monitoring%2C%20yet%20conventional%20methods%20depend%20heavily%20on%20expert%20taxonomists.%20Recent%20deep%20learning%20approaches%20improve%20automation%2C%20but%20most%20treat%20diatom%20recognition%20as%20flat%20classification%2C%20predicting%20only%20one%20taxonomic%20rank.%20We%20investigate%20whether%20embedding%20taxonomic%20hierarchy%20into%20neural%20network%20architectures%20can%20improve%20both%20accuracy%20and%20error%20locality.%0A%20%20We%20introduce%20DiatomCascadeNet%20%28H-COFGS%29%2C%20a%20hierarchical%20convolutional%20network%20with%20five%20cascaded%20heads%20that%20jointly%20predict%20class%2C%20order%2C%20family%2C%20genus%2C%20and%20species.%20Each%20head%20receives%20shared%20backbone%20features%20and%20probability%20distributions%20from%20higher%20levels%2C%20with%20binary%20masks%20restricting%20predictions%20to%20valid%20descendants%20during%20training%20and%20inference.%20Using%20a%20filtered%20dataset%20of%201%2C456%20diatom%20images%20covering%2082%20species%2C%20we%20compare%20hierarchical%20and%20flat%20models%20under%20identical%20settings.%0A%20%20H-COFGS%20matches%20flat%20baselines%20at%20the%20species%20level%20%2869.4%25%20accuracy%29%20while%20outperforming%20at%20all%20upper%20taxonomic%20levels.%20When%20species%20predictions%20fail%2C%20errors%20remain%20taxonomically%20local%3A%2092.5%25%20of%20misclassified%20species%20are%20correctly%20predicted%20at%20the%20genus%20level%2C%20versus%2067.2%25%20for%20flat%20baselines.%20H-COFGS%20reduces%20mean%20taxonomic%20distance%20by%2038.2%25%20%281.209%20vs.%201.955%29.%0A%20%20Progressive%20training%20reveals%20bidirectional%20mechanisms%3A%20hierarchical%20constraint%20masks%20operate%20top-down%20to%20constrain%20prediction%20space%2C%20while%20gradients%20from%20fine-grained%20levels%20propagate%20bottom-up%20through%20the%20shared%20backbone%2C%20refining%20features.%20This%20improves%20class%20accuracy%20from%2096.2%25%20to%2099.5%25%20and%20yields%206-8%25%20gains%20at%20upper%20levels%2C%20producing%20more%20robust%2C%20interpretable%2C%20and%20biologically%20aligned%20predictions%20for%20multi-level%20taxonomic%20classification.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Deep%2520Learning%2520for%2520Diatom%2520Image%2520Classification%253A%2520A%2520Multi-Level%2520Taxonomic%2520Approach%26entry.906535625%3DYueying%2520Ke%26entry.1292438233%3DAccurate%2520taxonomic%2520identification%2520of%2520diatoms%2520is%2520essential%2520for%2520aquatic%2520ecosystem%2520monitoring%252C%2520yet%2520conventional%2520methods%2520depend%2520heavily%2520on%2520expert%2520taxonomists.%2520Recent%2520deep%2520learning%2520approaches%2520improve%2520automation%252C%2520but%2520most%2520treat%2520diatom%2520recognition%2520as%2520flat%2520classification%252C%2520predicting%2520only%2520one%2520taxonomic%2520rank.%2520We%2520investigate%2520whether%2520embedding%2520taxonomic%2520hierarchy%2520into%2520neural%2520network%2520architectures%2520can%2520improve%2520both%2520accuracy%2520and%2520error%2520locality.%250A%2520%2520We%2520introduce%2520DiatomCascadeNet%2520%2528H-COFGS%2529%252C%2520a%2520hierarchical%2520convolutional%2520network%2520with%2520five%2520cascaded%2520heads%2520that%2520jointly%2520predict%2520class%252C%2520order%252C%2520family%252C%2520genus%252C%2520and%2520species.%2520Each%2520head%2520receives%2520shared%2520backbone%2520features%2520and%2520probability%2520distributions%2520from%2520higher%2520levels%252C%2520with%2520binary%2520masks%2520restricting%2520predictions%2520to%2520valid%2520descendants%2520during%2520training%2520and%2520inference.%2520Using%2520a%2520filtered%2520dataset%2520of%25201%252C456%2520diatom%2520images%2520covering%252082%2520species%252C%2520we%2520compare%2520hierarchical%2520and%2520flat%2520models%2520under%2520identical%2520settings.%250A%2520%2520H-COFGS%2520matches%2520flat%2520baselines%2520at%2520the%2520species%2520level%2520%252869.4%2525%2520accuracy%2529%2520while%2520outperforming%2520at%2520all%2520upper%2520taxonomic%2520levels.%2520When%2520species%2520predictions%2520fail%252C%2520errors%2520remain%2520taxonomically%2520local%253A%252092.5%2525%2520of%2520misclassified%2520species%2520are%2520correctly%2520predicted%2520at%2520the%2520genus%2520level%252C%2520versus%252067.2%2525%2520for%2520flat%2520baselines.%2520H-COFGS%2520reduces%2520mean%2520taxonomic%2520distance%2520by%252038.2%2525%2520%25281.209%2520vs.%25201.955%2529.%250A%2520%2520Progressive%2520training%2520reveals%2520bidirectional%2520mechanisms%253A%2520hierarchical%2520constraint%2520masks%2520operate%2520top-down%2520to%2520constrain%2520prediction%2520space%252C%2520while%2520gradients%2520from%2520fine-grained%2520levels%2520propagate%2520bottom-up%2520through%2520the%2520shared%2520backbone%252C%2520refining%2520features.%2520This%2520improves%2520class%2520accuracy%2520from%252096.2%2525%2520to%252099.5%2525%2520and%2520yields%25206-8%2525%2520gains%2520at%2520upper%2520levels%252C%2520producing%2520more%2520robust%252C%2520interpretable%252C%2520and%2520biologically%2520aligned%2520predictions%2520for%2520multi-level%2520taxonomic%2520classification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Deep%20Learning%20for%20Diatom%20Image%20Classification%3A%20A%20Multi-Level%20Taxonomic%20Approach&entry.906535625=Yueying%20Ke&entry.1292438233=Accurate%20taxonomic%20identification%20of%20diatoms%20is%20essential%20for%20aquatic%20ecosystem%20monitoring%2C%20yet%20conventional%20methods%20depend%20heavily%20on%20expert%20taxonomists.%20Recent%20deep%20learning%20approaches%20improve%20automation%2C%20but%20most%20treat%20diatom%20recognition%20as%20flat%20classification%2C%20predicting%20only%20one%20taxonomic%20rank.%20We%20investigate%20whether%20embedding%20taxonomic%20hierarchy%20into%20neural%20network%20architectures%20can%20improve%20both%20accuracy%20and%20error%20locality.%0A%20%20We%20introduce%20DiatomCascadeNet%20%28H-COFGS%29%2C%20a%20hierarchical%20convolutional%20network%20with%20five%20cascaded%20heads%20that%20jointly%20predict%20class%2C%20order%2C%20family%2C%20genus%2C%20and%20species.%20Each%20head%20receives%20shared%20backbone%20features%20and%20probability%20distributions%20from%20higher%20levels%2C%20with%20binary%20masks%20restricting%20predictions%20to%20valid%20descendants%20during%20training%20and%20inference.%20Using%20a%20filtered%20dataset%20of%201%2C456%20diatom%20images%20covering%2082%20species%2C%20we%20compare%20hierarchical%20and%20flat%20models%20under%20identical%20settings.%0A%20%20H-COFGS%20matches%20flat%20baselines%20at%20the%20species%20level%20%2869.4%25%20accuracy%29%20while%20outperforming%20at%20all%20upper%20taxonomic%20levels.%20When%20species%20predictions%20fail%2C%20errors%20remain%20taxonomically%20local%3A%2092.5%25%20of%20misclassified%20species%20are%20correctly%20predicted%20at%20the%20genus%20level%2C%20versus%2067.2%25%20for%20flat%20baselines.%20H-COFGS%20reduces%20mean%20taxonomic%20distance%20by%2038.2%25%20%281.209%20vs.%201.955%29.%0A%20%20Progressive%20training%20reveals%20bidirectional%20mechanisms%3A%20hierarchical%20constraint%20masks%20operate%20top-down%20to%20constrain%20prediction%20space%2C%20while%20gradients%20from%20fine-grained%20levels%20propagate%20bottom-up%20through%20the%20shared%20backbone%2C%20refining%20features.%20This%20improves%20class%20accuracy%20from%2096.2%25%20to%2099.5%25%20and%20yields%206-8%25%20gains%20at%20upper%20levels%2C%20producing%20more%20robust%2C%20interpretable%2C%20and%20biologically%20aligned%20predictions%20for%20multi-level%20taxonomic%20classification.&entry.1838667208=http%3A//arxiv.org/abs/2512.06613v2&entry.124074799=Read"},
{"title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World", "author": "Ao Liang and Lingdong Kong and Tianyi Yan and Hongsi Liu and Wesley Yang and Ziqi Huang and Wei Yin and Jialong Zuo and Yixuan Hu and Dekai Zhu and Dongyue Lu and Youquan Liu and Guangfeng Jiang and Linfeng Li and Xiangtai Li and Long Zhuo and Lai Xing Ng and Benoit R. Cottereau and Changxin Gao and Liang Pan and Wei Tsang Ooi and Ziwei Liu", "abstract": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.", "link": "http://arxiv.org/abs/2512.10958v1", "date": "2025-12-11", "relevancy": 2.3334, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldLens%3A%20Full-Spectrum%20Evaluations%20of%20Driving%20World%20Models%20in%20Real%20World&body=Title%3A%20WorldLens%3A%20Full-Spectrum%20Evaluations%20of%20Driving%20World%20Models%20in%20Real%20World%0AAuthor%3A%20Ao%20Liang%20and%20Lingdong%20Kong%20and%20Tianyi%20Yan%20and%20Hongsi%20Liu%20and%20Wesley%20Yang%20and%20Ziqi%20Huang%20and%20Wei%20Yin%20and%20Jialong%20Zuo%20and%20Yixuan%20Hu%20and%20Dekai%20Zhu%20and%20Dongyue%20Lu%20and%20Youquan%20Liu%20and%20Guangfeng%20Jiang%20and%20Linfeng%20Li%20and%20Xiangtai%20Li%20and%20Long%20Zhuo%20and%20Lai%20Xing%20Ng%20and%20Benoit%20R.%20Cottereau%20and%20Changxin%20Gao%20and%20Liang%20Pan%20and%20Wei%20Tsang%20Ooi%20and%20Ziwei%20Liu%0AAbstract%3A%20Generative%20world%20models%20are%20reshaping%20embodied%20AI%2C%20enabling%20agents%20to%20synthesize%20realistic%204D%20driving%20environments%20that%20look%20convincing%20but%20often%20fail%20physically%20or%20behaviorally.%20Despite%20rapid%20progress%2C%20the%20field%20still%20lacks%20a%20unified%20way%20to%20assess%20whether%20generated%20worlds%20preserve%20geometry%2C%20obey%20physics%2C%20or%20support%20reliable%20control.%20We%20introduce%20WorldLens%2C%20a%20full-spectrum%20benchmark%20evaluating%20how%20well%20a%20model%20builds%2C%20understands%2C%20and%20behaves%20within%20its%20generated%20world.%20It%20spans%20five%20aspects%20--%20Generation%2C%20Reconstruction%2C%20Action-Following%2C%20Downstream%20Task%2C%20and%20Human%20Preference%20--%20jointly%20covering%20visual%20realism%2C%20geometric%20consistency%2C%20physical%20plausibility%2C%20and%20functional%20reliability.%20Across%20these%20dimensions%2C%20no%20existing%20world%20model%20excels%20universally%3A%20those%20with%20strong%20textures%20often%20violate%20physics%2C%20while%20geometry-stable%20ones%20lack%20behavioral%20fidelity.%20To%20align%20objective%20metrics%20with%20human%20judgment%2C%20we%20further%20construct%20WorldLens-26K%2C%20a%20large-scale%20dataset%20of%20human-annotated%20videos%20with%20numerical%20scores%20and%20textual%20rationales%2C%20and%20develop%20WorldLens-Agent%2C%20an%20evaluation%20model%20distilled%20from%20these%20annotations%20to%20enable%20scalable%2C%20explainable%20scoring.%20Together%2C%20the%20benchmark%2C%20dataset%2C%20and%20agent%20form%20a%20unified%20ecosystem%20for%20measuring%20world%20fidelity%20--%20standardizing%20how%20future%20models%20are%20judged%20not%20only%20by%20how%20real%20they%20look%2C%20but%20by%20how%20real%20they%20behave.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldLens%253A%2520Full-Spectrum%2520Evaluations%2520of%2520Driving%2520World%2520Models%2520in%2520Real%2520World%26entry.906535625%3DAo%2520Liang%2520and%2520Lingdong%2520Kong%2520and%2520Tianyi%2520Yan%2520and%2520Hongsi%2520Liu%2520and%2520Wesley%2520Yang%2520and%2520Ziqi%2520Huang%2520and%2520Wei%2520Yin%2520and%2520Jialong%2520Zuo%2520and%2520Yixuan%2520Hu%2520and%2520Dekai%2520Zhu%2520and%2520Dongyue%2520Lu%2520and%2520Youquan%2520Liu%2520and%2520Guangfeng%2520Jiang%2520and%2520Linfeng%2520Li%2520and%2520Xiangtai%2520Li%2520and%2520Long%2520Zhuo%2520and%2520Lai%2520Xing%2520Ng%2520and%2520Benoit%2520R.%2520Cottereau%2520and%2520Changxin%2520Gao%2520and%2520Liang%2520Pan%2520and%2520Wei%2520Tsang%2520Ooi%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DGenerative%2520world%2520models%2520are%2520reshaping%2520embodied%2520AI%252C%2520enabling%2520agents%2520to%2520synthesize%2520realistic%25204D%2520driving%2520environments%2520that%2520look%2520convincing%2520but%2520often%2520fail%2520physically%2520or%2520behaviorally.%2520Despite%2520rapid%2520progress%252C%2520the%2520field%2520still%2520lacks%2520a%2520unified%2520way%2520to%2520assess%2520whether%2520generated%2520worlds%2520preserve%2520geometry%252C%2520obey%2520physics%252C%2520or%2520support%2520reliable%2520control.%2520We%2520introduce%2520WorldLens%252C%2520a%2520full-spectrum%2520benchmark%2520evaluating%2520how%2520well%2520a%2520model%2520builds%252C%2520understands%252C%2520and%2520behaves%2520within%2520its%2520generated%2520world.%2520It%2520spans%2520five%2520aspects%2520--%2520Generation%252C%2520Reconstruction%252C%2520Action-Following%252C%2520Downstream%2520Task%252C%2520and%2520Human%2520Preference%2520--%2520jointly%2520covering%2520visual%2520realism%252C%2520geometric%2520consistency%252C%2520physical%2520plausibility%252C%2520and%2520functional%2520reliability.%2520Across%2520these%2520dimensions%252C%2520no%2520existing%2520world%2520model%2520excels%2520universally%253A%2520those%2520with%2520strong%2520textures%2520often%2520violate%2520physics%252C%2520while%2520geometry-stable%2520ones%2520lack%2520behavioral%2520fidelity.%2520To%2520align%2520objective%2520metrics%2520with%2520human%2520judgment%252C%2520we%2520further%2520construct%2520WorldLens-26K%252C%2520a%2520large-scale%2520dataset%2520of%2520human-annotated%2520videos%2520with%2520numerical%2520scores%2520and%2520textual%2520rationales%252C%2520and%2520develop%2520WorldLens-Agent%252C%2520an%2520evaluation%2520model%2520distilled%2520from%2520these%2520annotations%2520to%2520enable%2520scalable%252C%2520explainable%2520scoring.%2520Together%252C%2520the%2520benchmark%252C%2520dataset%252C%2520and%2520agent%2520form%2520a%2520unified%2520ecosystem%2520for%2520measuring%2520world%2520fidelity%2520--%2520standardizing%2520how%2520future%2520models%2520are%2520judged%2520not%2520only%2520by%2520how%2520real%2520they%2520look%252C%2520but%2520by%2520how%2520real%2520they%2520behave.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldLens%3A%20Full-Spectrum%20Evaluations%20of%20Driving%20World%20Models%20in%20Real%20World&entry.906535625=Ao%20Liang%20and%20Lingdong%20Kong%20and%20Tianyi%20Yan%20and%20Hongsi%20Liu%20and%20Wesley%20Yang%20and%20Ziqi%20Huang%20and%20Wei%20Yin%20and%20Jialong%20Zuo%20and%20Yixuan%20Hu%20and%20Dekai%20Zhu%20and%20Dongyue%20Lu%20and%20Youquan%20Liu%20and%20Guangfeng%20Jiang%20and%20Linfeng%20Li%20and%20Xiangtai%20Li%20and%20Long%20Zhuo%20and%20Lai%20Xing%20Ng%20and%20Benoit%20R.%20Cottereau%20and%20Changxin%20Gao%20and%20Liang%20Pan%20and%20Wei%20Tsang%20Ooi%20and%20Ziwei%20Liu&entry.1292438233=Generative%20world%20models%20are%20reshaping%20embodied%20AI%2C%20enabling%20agents%20to%20synthesize%20realistic%204D%20driving%20environments%20that%20look%20convincing%20but%20often%20fail%20physically%20or%20behaviorally.%20Despite%20rapid%20progress%2C%20the%20field%20still%20lacks%20a%20unified%20way%20to%20assess%20whether%20generated%20worlds%20preserve%20geometry%2C%20obey%20physics%2C%20or%20support%20reliable%20control.%20We%20introduce%20WorldLens%2C%20a%20full-spectrum%20benchmark%20evaluating%20how%20well%20a%20model%20builds%2C%20understands%2C%20and%20behaves%20within%20its%20generated%20world.%20It%20spans%20five%20aspects%20--%20Generation%2C%20Reconstruction%2C%20Action-Following%2C%20Downstream%20Task%2C%20and%20Human%20Preference%20--%20jointly%20covering%20visual%20realism%2C%20geometric%20consistency%2C%20physical%20plausibility%2C%20and%20functional%20reliability.%20Across%20these%20dimensions%2C%20no%20existing%20world%20model%20excels%20universally%3A%20those%20with%20strong%20textures%20often%20violate%20physics%2C%20while%20geometry-stable%20ones%20lack%20behavioral%20fidelity.%20To%20align%20objective%20metrics%20with%20human%20judgment%2C%20we%20further%20construct%20WorldLens-26K%2C%20a%20large-scale%20dataset%20of%20human-annotated%20videos%20with%20numerical%20scores%20and%20textual%20rationales%2C%20and%20develop%20WorldLens-Agent%2C%20an%20evaluation%20model%20distilled%20from%20these%20annotations%20to%20enable%20scalable%2C%20explainable%20scoring.%20Together%2C%20the%20benchmark%2C%20dataset%2C%20and%20agent%20form%20a%20unified%20ecosystem%20for%20measuring%20world%20fidelity%20--%20standardizing%20how%20future%20models%20are%20judged%20not%20only%20by%20how%20real%20they%20look%2C%20but%20by%20how%20real%20they%20behave.&entry.1838667208=http%3A//arxiv.org/abs/2512.10958v1&entry.124074799=Read"},
{"title": "Authority Backdoor: A Certifiable Backdoor Mechanism for Authoring DNNs", "author": "Han Yang and Shaofeng Li and Tian Dong and Xiangyu Xu and Guangchi Liu and Zhen Ling", "abstract": "Deep Neural Networks (DNNs), as valuable intellectual property, face unauthorized use. Existing protections, such as digital watermarking, are largely passive; they provide only post-hoc ownership verification and cannot actively prevent the illicit use of a stolen model. This work proposes a proactive protection scheme, dubbed ``Authority Backdoor,\" which embeds access constraints directly into the model. In particular, the scheme utilizes a backdoor learning framework to intrinsically lock a model's utility, such that it performs normally only in the presence of a specific trigger (e.g., a hardware fingerprint). But in its absence, the DNN's performance degrades to be useless. To further enhance the security of the proposed authority scheme, the certifiable robustness is integrated to prevent an adaptive attacker from removing the implanted backdoor. The resulting framework establishes a secure authority mechanism for DNNs, combining access control with certifiable robustness against adversarial attacks. Extensive experiments on diverse architectures and datasets validate the effectiveness and certifiable robustness of the proposed framework.", "link": "http://arxiv.org/abs/2512.10600v1", "date": "2025-12-11", "relevancy": 2.3297, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4992}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4573}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Authority%20Backdoor%3A%20A%20Certifiable%20Backdoor%20Mechanism%20for%20Authoring%20DNNs&body=Title%3A%20Authority%20Backdoor%3A%20A%20Certifiable%20Backdoor%20Mechanism%20for%20Authoring%20DNNs%0AAuthor%3A%20Han%20Yang%20and%20Shaofeng%20Li%20and%20Tian%20Dong%20and%20Xiangyu%20Xu%20and%20Guangchi%20Liu%20and%20Zhen%20Ling%0AAbstract%3A%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20as%20valuable%20intellectual%20property%2C%20face%20unauthorized%20use.%20Existing%20protections%2C%20such%20as%20digital%20watermarking%2C%20are%20largely%20passive%3B%20they%20provide%20only%20post-hoc%20ownership%20verification%20and%20cannot%20actively%20prevent%20the%20illicit%20use%20of%20a%20stolen%20model.%20This%20work%20proposes%20a%20proactive%20protection%20scheme%2C%20dubbed%20%60%60Authority%20Backdoor%2C%22%20which%20embeds%20access%20constraints%20directly%20into%20the%20model.%20In%20particular%2C%20the%20scheme%20utilizes%20a%20backdoor%20learning%20framework%20to%20intrinsically%20lock%20a%20model%27s%20utility%2C%20such%20that%20it%20performs%20normally%20only%20in%20the%20presence%20of%20a%20specific%20trigger%20%28e.g.%2C%20a%20hardware%20fingerprint%29.%20But%20in%20its%20absence%2C%20the%20DNN%27s%20performance%20degrades%20to%20be%20useless.%20To%20further%20enhance%20the%20security%20of%20the%20proposed%20authority%20scheme%2C%20the%20certifiable%20robustness%20is%20integrated%20to%20prevent%20an%20adaptive%20attacker%20from%20removing%20the%20implanted%20backdoor.%20The%20resulting%20framework%20establishes%20a%20secure%20authority%20mechanism%20for%20DNNs%2C%20combining%20access%20control%20with%20certifiable%20robustness%20against%20adversarial%20attacks.%20Extensive%20experiments%20on%20diverse%20architectures%20and%20datasets%20validate%20the%20effectiveness%20and%20certifiable%20robustness%20of%20the%20proposed%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuthority%2520Backdoor%253A%2520A%2520Certifiable%2520Backdoor%2520Mechanism%2520for%2520Authoring%2520DNNs%26entry.906535625%3DHan%2520Yang%2520and%2520Shaofeng%2520Li%2520and%2520Tian%2520Dong%2520and%2520Xiangyu%2520Xu%2520and%2520Guangchi%2520Liu%2520and%2520Zhen%2520Ling%26entry.1292438233%3DDeep%2520Neural%2520Networks%2520%2528DNNs%2529%252C%2520as%2520valuable%2520intellectual%2520property%252C%2520face%2520unauthorized%2520use.%2520Existing%2520protections%252C%2520such%2520as%2520digital%2520watermarking%252C%2520are%2520largely%2520passive%253B%2520they%2520provide%2520only%2520post-hoc%2520ownership%2520verification%2520and%2520cannot%2520actively%2520prevent%2520the%2520illicit%2520use%2520of%2520a%2520stolen%2520model.%2520This%2520work%2520proposes%2520a%2520proactive%2520protection%2520scheme%252C%2520dubbed%2520%2560%2560Authority%2520Backdoor%252C%2522%2520which%2520embeds%2520access%2520constraints%2520directly%2520into%2520the%2520model.%2520In%2520particular%252C%2520the%2520scheme%2520utilizes%2520a%2520backdoor%2520learning%2520framework%2520to%2520intrinsically%2520lock%2520a%2520model%2527s%2520utility%252C%2520such%2520that%2520it%2520performs%2520normally%2520only%2520in%2520the%2520presence%2520of%2520a%2520specific%2520trigger%2520%2528e.g.%252C%2520a%2520hardware%2520fingerprint%2529.%2520But%2520in%2520its%2520absence%252C%2520the%2520DNN%2527s%2520performance%2520degrades%2520to%2520be%2520useless.%2520To%2520further%2520enhance%2520the%2520security%2520of%2520the%2520proposed%2520authority%2520scheme%252C%2520the%2520certifiable%2520robustness%2520is%2520integrated%2520to%2520prevent%2520an%2520adaptive%2520attacker%2520from%2520removing%2520the%2520implanted%2520backdoor.%2520The%2520resulting%2520framework%2520establishes%2520a%2520secure%2520authority%2520mechanism%2520for%2520DNNs%252C%2520combining%2520access%2520control%2520with%2520certifiable%2520robustness%2520against%2520adversarial%2520attacks.%2520Extensive%2520experiments%2520on%2520diverse%2520architectures%2520and%2520datasets%2520validate%2520the%2520effectiveness%2520and%2520certifiable%2520robustness%2520of%2520the%2520proposed%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Authority%20Backdoor%3A%20A%20Certifiable%20Backdoor%20Mechanism%20for%20Authoring%20DNNs&entry.906535625=Han%20Yang%20and%20Shaofeng%20Li%20and%20Tian%20Dong%20and%20Xiangyu%20Xu%20and%20Guangchi%20Liu%20and%20Zhen%20Ling&entry.1292438233=Deep%20Neural%20Networks%20%28DNNs%29%2C%20as%20valuable%20intellectual%20property%2C%20face%20unauthorized%20use.%20Existing%20protections%2C%20such%20as%20digital%20watermarking%2C%20are%20largely%20passive%3B%20they%20provide%20only%20post-hoc%20ownership%20verification%20and%20cannot%20actively%20prevent%20the%20illicit%20use%20of%20a%20stolen%20model.%20This%20work%20proposes%20a%20proactive%20protection%20scheme%2C%20dubbed%20%60%60Authority%20Backdoor%2C%22%20which%20embeds%20access%20constraints%20directly%20into%20the%20model.%20In%20particular%2C%20the%20scheme%20utilizes%20a%20backdoor%20learning%20framework%20to%20intrinsically%20lock%20a%20model%27s%20utility%2C%20such%20that%20it%20performs%20normally%20only%20in%20the%20presence%20of%20a%20specific%20trigger%20%28e.g.%2C%20a%20hardware%20fingerprint%29.%20But%20in%20its%20absence%2C%20the%20DNN%27s%20performance%20degrades%20to%20be%20useless.%20To%20further%20enhance%20the%20security%20of%20the%20proposed%20authority%20scheme%2C%20the%20certifiable%20robustness%20is%20integrated%20to%20prevent%20an%20adaptive%20attacker%20from%20removing%20the%20implanted%20backdoor.%20The%20resulting%20framework%20establishes%20a%20secure%20authority%20mechanism%20for%20DNNs%2C%20combining%20access%20control%20with%20certifiable%20robustness%20against%20adversarial%20attacks.%20Extensive%20experiments%20on%20diverse%20architectures%20and%20datasets%20validate%20the%20effectiveness%20and%20certifiable%20robustness%20of%20the%20proposed%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.10600v1&entry.124074799=Read"},
{"title": "FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale", "author": "Isabelle Lee and Sarah Liaw and Dani Yogatama", "abstract": "Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.", "link": "http://arxiv.org/abs/2505.14932v2", "date": "2025-12-11", "relevancy": 2.3275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOL-Traces%3A%20Verified%20First-Order%20Logic%20Reasoning%20Traces%20at%20Scale&body=Title%3A%20FOL-Traces%3A%20Verified%20First-Order%20Logic%20Reasoning%20Traces%20at%20Scale%0AAuthor%3A%20Isabelle%20Lee%20and%20Sarah%20Liaw%20and%20Dani%20Yogatama%0AAbstract%3A%20Reasoning%20in%20language%20models%20is%20difficult%20to%20evaluate%3A%20natural-language%20traces%20are%20unverifiable%2C%20symbolic%20datasets%20too%20small%2C%20and%20most%20benchmarks%20conflate%20heuristics%20with%20inference.%20We%20present%20FOL-Traces%2C%20the%20first%20large-scale%20dataset%20of%20programmatically%20verified%20reasoning%20traces%2C%20enabling%20rigorous%20evaluation%20of%20structured%20logical%20inference.%20We%20also%20propose%20two%20challenging%20and%20comprehensive%20diagnostic%20tasks-masked%20operation%20prediction%20and%20step%20completion-that%20directly%20probe%20syntactic%20awareness%20and%20process%20fidelity.%20FOL-Traces%20serves%20as%20a%20scalable%20testbed%20for%20rigorously%20studying%20how%20models%20perform%20structured%20logical%20inference.%20Systematic%20experiments%20with%205%20reasoning%20LLMs%20show%20that%20the%20dataset%20remains%20challenging%3A%20models%20only%20reach%20around%2045.7%25%20accuracy%20on%20masked%20operation%20prediction%20and%20around%2027%25%20on%20two-step%20completion.%0ALink%3A%20http%3A//arxiv.org/abs/2505.14932v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOL-Traces%253A%2520Verified%2520First-Order%2520Logic%2520Reasoning%2520Traces%2520at%2520Scale%26entry.906535625%3DIsabelle%2520Lee%2520and%2520Sarah%2520Liaw%2520and%2520Dani%2520Yogatama%26entry.1292438233%3DReasoning%2520in%2520language%2520models%2520is%2520difficult%2520to%2520evaluate%253A%2520natural-language%2520traces%2520are%2520unverifiable%252C%2520symbolic%2520datasets%2520too%2520small%252C%2520and%2520most%2520benchmarks%2520conflate%2520heuristics%2520with%2520inference.%2520We%2520present%2520FOL-Traces%252C%2520the%2520first%2520large-scale%2520dataset%2520of%2520programmatically%2520verified%2520reasoning%2520traces%252C%2520enabling%2520rigorous%2520evaluation%2520of%2520structured%2520logical%2520inference.%2520We%2520also%2520propose%2520two%2520challenging%2520and%2520comprehensive%2520diagnostic%2520tasks-masked%2520operation%2520prediction%2520and%2520step%2520completion-that%2520directly%2520probe%2520syntactic%2520awareness%2520and%2520process%2520fidelity.%2520FOL-Traces%2520serves%2520as%2520a%2520scalable%2520testbed%2520for%2520rigorously%2520studying%2520how%2520models%2520perform%2520structured%2520logical%2520inference.%2520Systematic%2520experiments%2520with%25205%2520reasoning%2520LLMs%2520show%2520that%2520the%2520dataset%2520remains%2520challenging%253A%2520models%2520only%2520reach%2520around%252045.7%2525%2520accuracy%2520on%2520masked%2520operation%2520prediction%2520and%2520around%252027%2525%2520on%2520two-step%2520completion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14932v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOL-Traces%3A%20Verified%20First-Order%20Logic%20Reasoning%20Traces%20at%20Scale&entry.906535625=Isabelle%20Lee%20and%20Sarah%20Liaw%20and%20Dani%20Yogatama&entry.1292438233=Reasoning%20in%20language%20models%20is%20difficult%20to%20evaluate%3A%20natural-language%20traces%20are%20unverifiable%2C%20symbolic%20datasets%20too%20small%2C%20and%20most%20benchmarks%20conflate%20heuristics%20with%20inference.%20We%20present%20FOL-Traces%2C%20the%20first%20large-scale%20dataset%20of%20programmatically%20verified%20reasoning%20traces%2C%20enabling%20rigorous%20evaluation%20of%20structured%20logical%20inference.%20We%20also%20propose%20two%20challenging%20and%20comprehensive%20diagnostic%20tasks-masked%20operation%20prediction%20and%20step%20completion-that%20directly%20probe%20syntactic%20awareness%20and%20process%20fidelity.%20FOL-Traces%20serves%20as%20a%20scalable%20testbed%20for%20rigorously%20studying%20how%20models%20perform%20structured%20logical%20inference.%20Systematic%20experiments%20with%205%20reasoning%20LLMs%20show%20that%20the%20dataset%20remains%20challenging%3A%20models%20only%20reach%20around%2045.7%25%20accuracy%20on%20masked%20operation%20prediction%20and%20around%2027%25%20on%20two-step%20completion.&entry.1838667208=http%3A//arxiv.org/abs/2505.14932v2&entry.124074799=Read"},
{"title": "Hierarchical Dataset Selection for High-Quality Data Sharing", "author": "Xiaona Zhou and Yingyan Zeng and Ran Jin and Ismini Lourentzou", "abstract": "The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.", "link": "http://arxiv.org/abs/2512.10952v1", "date": "2025-12-11", "relevancy": 2.3089, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4594}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Dataset%20Selection%20for%20High-Quality%20Data%20Sharing&body=Title%3A%20Hierarchical%20Dataset%20Selection%20for%20High-Quality%20Data%20Sharing%0AAuthor%3A%20Xiaona%20Zhou%20and%20Yingyan%20Zeng%20and%20Ran%20Jin%20and%20Ismini%20Lourentzou%0AAbstract%3A%20The%20success%20of%20modern%20machine%20learning%20hinges%20on%20access%20to%20high-quality%20training%20data.%20In%20many%20real-world%20scenarios%2C%20such%20as%20acquiring%20data%20from%20public%20repositories%20or%20sharing%20across%20institutions%2C%20data%20is%20naturally%20organized%20into%20discrete%20datasets%20that%20vary%20in%20relevance%2C%20quality%2C%20and%20utility.%20Selecting%20which%20repositories%20or%20institutions%20to%20search%20for%20useful%20datasets%2C%20and%20which%20datasets%20to%20incorporate%20into%20model%20training%20are%20therefore%20critical%20decisions%2C%20yet%20most%20existing%20methods%20select%20individual%20samples%20and%20treat%20all%20data%20as%20equally%20relevant%2C%20ignoring%20differences%20between%20datasets%20and%20their%20sources.%20In%20this%20work%2C%20we%20formalize%20the%20task%20of%20dataset%20selection%3A%20selecting%20entire%20datasets%20from%20a%20large%2C%20heterogeneous%20pool%20to%20improve%20downstream%20performance%20under%20resource%20constraints.%20We%20propose%20Dataset%20Selection%20via%20Hierarchies%20%28DaSH%29%2C%20a%20dataset%20selection%20method%20that%20models%20utility%20at%20both%20dataset%20and%20group%20%28e.g.%2C%20collections%2C%20institutions%29%20levels%2C%20enabling%20efficient%20generalization%20from%20limited%20observations.%20Across%20two%20public%20benchmarks%20%28Digit-Five%20and%20DomainNet%29%2C%20DaSH%20outperforms%20state-of-the-art%20data%20selection%20baselines%20by%20up%20to%2026.2%25%20in%20accuracy%2C%20while%20requiring%20significantly%20fewer%20exploration%20steps.%20Ablations%20show%20DaSH%20is%20robust%20to%20low-resource%20settings%20and%20lack%20of%20relevant%20datasets%2C%20making%20it%20suitable%20for%20scalable%20and%20adaptive%20dataset%20selection%20in%20practical%20multi-source%20learning%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Dataset%2520Selection%2520for%2520High-Quality%2520Data%2520Sharing%26entry.906535625%3DXiaona%2520Zhou%2520and%2520Yingyan%2520Zeng%2520and%2520Ran%2520Jin%2520and%2520Ismini%2520Lourentzou%26entry.1292438233%3DThe%2520success%2520of%2520modern%2520machine%2520learning%2520hinges%2520on%2520access%2520to%2520high-quality%2520training%2520data.%2520In%2520many%2520real-world%2520scenarios%252C%2520such%2520as%2520acquiring%2520data%2520from%2520public%2520repositories%2520or%2520sharing%2520across%2520institutions%252C%2520data%2520is%2520naturally%2520organized%2520into%2520discrete%2520datasets%2520that%2520vary%2520in%2520relevance%252C%2520quality%252C%2520and%2520utility.%2520Selecting%2520which%2520repositories%2520or%2520institutions%2520to%2520search%2520for%2520useful%2520datasets%252C%2520and%2520which%2520datasets%2520to%2520incorporate%2520into%2520model%2520training%2520are%2520therefore%2520critical%2520decisions%252C%2520yet%2520most%2520existing%2520methods%2520select%2520individual%2520samples%2520and%2520treat%2520all%2520data%2520as%2520equally%2520relevant%252C%2520ignoring%2520differences%2520between%2520datasets%2520and%2520their%2520sources.%2520In%2520this%2520work%252C%2520we%2520formalize%2520the%2520task%2520of%2520dataset%2520selection%253A%2520selecting%2520entire%2520datasets%2520from%2520a%2520large%252C%2520heterogeneous%2520pool%2520to%2520improve%2520downstream%2520performance%2520under%2520resource%2520constraints.%2520We%2520propose%2520Dataset%2520Selection%2520via%2520Hierarchies%2520%2528DaSH%2529%252C%2520a%2520dataset%2520selection%2520method%2520that%2520models%2520utility%2520at%2520both%2520dataset%2520and%2520group%2520%2528e.g.%252C%2520collections%252C%2520institutions%2529%2520levels%252C%2520enabling%2520efficient%2520generalization%2520from%2520limited%2520observations.%2520Across%2520two%2520public%2520benchmarks%2520%2528Digit-Five%2520and%2520DomainNet%2529%252C%2520DaSH%2520outperforms%2520state-of-the-art%2520data%2520selection%2520baselines%2520by%2520up%2520to%252026.2%2525%2520in%2520accuracy%252C%2520while%2520requiring%2520significantly%2520fewer%2520exploration%2520steps.%2520Ablations%2520show%2520DaSH%2520is%2520robust%2520to%2520low-resource%2520settings%2520and%2520lack%2520of%2520relevant%2520datasets%252C%2520making%2520it%2520suitable%2520for%2520scalable%2520and%2520adaptive%2520dataset%2520selection%2520in%2520practical%2520multi-source%2520learning%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Dataset%20Selection%20for%20High-Quality%20Data%20Sharing&entry.906535625=Xiaona%20Zhou%20and%20Yingyan%20Zeng%20and%20Ran%20Jin%20and%20Ismini%20Lourentzou&entry.1292438233=The%20success%20of%20modern%20machine%20learning%20hinges%20on%20access%20to%20high-quality%20training%20data.%20In%20many%20real-world%20scenarios%2C%20such%20as%20acquiring%20data%20from%20public%20repositories%20or%20sharing%20across%20institutions%2C%20data%20is%20naturally%20organized%20into%20discrete%20datasets%20that%20vary%20in%20relevance%2C%20quality%2C%20and%20utility.%20Selecting%20which%20repositories%20or%20institutions%20to%20search%20for%20useful%20datasets%2C%20and%20which%20datasets%20to%20incorporate%20into%20model%20training%20are%20therefore%20critical%20decisions%2C%20yet%20most%20existing%20methods%20select%20individual%20samples%20and%20treat%20all%20data%20as%20equally%20relevant%2C%20ignoring%20differences%20between%20datasets%20and%20their%20sources.%20In%20this%20work%2C%20we%20formalize%20the%20task%20of%20dataset%20selection%3A%20selecting%20entire%20datasets%20from%20a%20large%2C%20heterogeneous%20pool%20to%20improve%20downstream%20performance%20under%20resource%20constraints.%20We%20propose%20Dataset%20Selection%20via%20Hierarchies%20%28DaSH%29%2C%20a%20dataset%20selection%20method%20that%20models%20utility%20at%20both%20dataset%20and%20group%20%28e.g.%2C%20collections%2C%20institutions%29%20levels%2C%20enabling%20efficient%20generalization%20from%20limited%20observations.%20Across%20two%20public%20benchmarks%20%28Digit-Five%20and%20DomainNet%29%2C%20DaSH%20outperforms%20state-of-the-art%20data%20selection%20baselines%20by%20up%20to%2026.2%25%20in%20accuracy%2C%20while%20requiring%20significantly%20fewer%20exploration%20steps.%20Ablations%20show%20DaSH%20is%20robust%20to%20low-resource%20settings%20and%20lack%20of%20relevant%20datasets%2C%20making%20it%20suitable%20for%20scalable%20and%20adaptive%20dataset%20selection%20in%20practical%20multi-source%20learning%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2512.10952v1&entry.124074799=Read"},
{"title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "author": "Itay Dreyfuss and Antonio Abu Nassar and Samuel Ackerman and Axel Ben David and Rami Katan and Orna Raz and Marcel Zalmanovici", "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "link": "http://arxiv.org/abs/2512.10713v1", "date": "2025-12-11", "relevancy": 2.2864, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACIFIC%3A%20a%20framework%20for%20generating%20benchmarks%20to%20check%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code&body=Title%3A%20PACIFIC%3A%20a%20framework%20for%20generating%20benchmarks%20to%20check%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code%0AAuthor%3A%20Itay%20Dreyfuss%20and%20Antonio%20Abu%20Nassar%20and%20Samuel%20Ackerman%20and%20Axel%20Ben%20David%20and%20Rami%20Katan%20and%20Orna%20Raz%20and%20Marcel%20Zalmanovici%0AAbstract%3A%20Large%20Language%20Model%20%28LLM%29-based%20code%20assistants%20have%20emerged%20as%20a%20powerful%20application%20of%20generative%20AI%2C%20demonstrating%20impressive%20capabilities%20in%20code%20generation%20and%20comprehension.%20A%20key%20requirement%20for%20these%20systems%20is%20their%20ability%20to%20accurately%20follow%20user%20instructions.%20We%20present%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code%20%28PACIFIC%29%2C%20a%20novel%20framework%20designed%20to%20automatically%20generate%20benchmarks%20that%20rigorously%20assess%20sequential%20instruction-following%20and%20code%20dry-running%20capabilities%20in%20LLMs%2C%20while%20allowing%20control%20over%20benchmark%20difficulty.%20PACIFIC%20produces%20benchmark%20variants%20with%20clearly%20defined%20expected%20outputs%2C%20enabling%20straightforward%20and%20reliable%20evaluation%20through%20simple%20output%20comparisons.%20In%20contrast%20to%20existing%20approaches%20that%20often%20rely%20on%20tool%20usage%20or%20agentic%20behavior%2C%20our%20work%20isolates%20and%20evaluates%20the%20LLM%27s%20intrinsic%20ability%20to%20reason%20through%20code%20behavior%20step-by-step%20without%20execution%20%28dry%20running%29%20and%20to%20follow%20instructions.%20Furthermore%2C%20our%20framework%20mitigates%20training%20data%20contamination%20by%20facilitating%20effortless%20generation%20of%20novel%20benchmark%20variations.%20We%20validate%20our%20framework%20by%20generating%20a%20suite%20of%20benchmarks%20spanning%20a%20range%20of%20difficulty%20levels%20and%20evaluating%20multiple%20state-of-the-art%20LLMs.%20Our%20results%20demonstrate%20that%20PACIFIC%20can%20produce%20increasingly%20challenging%20benchmarks%20that%20effectively%20differentiate%20instruction-following%20and%20dry%20running%20capabilities%2C%20even%20among%20advanced%20models.%20Overall%2C%20our%20framework%20offers%20a%20scalable%2C%20contamination-resilient%20methodology%20for%20assessing%20core%20competencies%20of%20LLMs%20in%20code-related%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACIFIC%253A%2520a%2520framework%2520for%2520generating%2520benchmarks%2520to%2520check%2520Precise%2520Automatically%2520Checked%2520Instruction%2520Following%2520In%2520Code%26entry.906535625%3DItay%2520Dreyfuss%2520and%2520Antonio%2520Abu%2520Nassar%2520and%2520Samuel%2520Ackerman%2520and%2520Axel%2520Ben%2520David%2520and%2520Rami%2520Katan%2520and%2520Orna%2520Raz%2520and%2520Marcel%2520Zalmanovici%26entry.1292438233%3DLarge%2520Language%2520Model%2520%2528LLM%2529-based%2520code%2520assistants%2520have%2520emerged%2520as%2520a%2520powerful%2520application%2520of%2520generative%2520AI%252C%2520demonstrating%2520impressive%2520capabilities%2520in%2520code%2520generation%2520and%2520comprehension.%2520A%2520key%2520requirement%2520for%2520these%2520systems%2520is%2520their%2520ability%2520to%2520accurately%2520follow%2520user%2520instructions.%2520We%2520present%2520Precise%2520Automatically%2520Checked%2520Instruction%2520Following%2520In%2520Code%2520%2528PACIFIC%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%2520automatically%2520generate%2520benchmarks%2520that%2520rigorously%2520assess%2520sequential%2520instruction-following%2520and%2520code%2520dry-running%2520capabilities%2520in%2520LLMs%252C%2520while%2520allowing%2520control%2520over%2520benchmark%2520difficulty.%2520PACIFIC%2520produces%2520benchmark%2520variants%2520with%2520clearly%2520defined%2520expected%2520outputs%252C%2520enabling%2520straightforward%2520and%2520reliable%2520evaluation%2520through%2520simple%2520output%2520comparisons.%2520In%2520contrast%2520to%2520existing%2520approaches%2520that%2520often%2520rely%2520on%2520tool%2520usage%2520or%2520agentic%2520behavior%252C%2520our%2520work%2520isolates%2520and%2520evaluates%2520the%2520LLM%2527s%2520intrinsic%2520ability%2520to%2520reason%2520through%2520code%2520behavior%2520step-by-step%2520without%2520execution%2520%2528dry%2520running%2529%2520and%2520to%2520follow%2520instructions.%2520Furthermore%252C%2520our%2520framework%2520mitigates%2520training%2520data%2520contamination%2520by%2520facilitating%2520effortless%2520generation%2520of%2520novel%2520benchmark%2520variations.%2520We%2520validate%2520our%2520framework%2520by%2520generating%2520a%2520suite%2520of%2520benchmarks%2520spanning%2520a%2520range%2520of%2520difficulty%2520levels%2520and%2520evaluating%2520multiple%2520state-of-the-art%2520LLMs.%2520Our%2520results%2520demonstrate%2520that%2520PACIFIC%2520can%2520produce%2520increasingly%2520challenging%2520benchmarks%2520that%2520effectively%2520differentiate%2520instruction-following%2520and%2520dry%2520running%2520capabilities%252C%2520even%2520among%2520advanced%2520models.%2520Overall%252C%2520our%2520framework%2520offers%2520a%2520scalable%252C%2520contamination-resilient%2520methodology%2520for%2520assessing%2520core%2520competencies%2520of%2520LLMs%2520in%2520code-related%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACIFIC%3A%20a%20framework%20for%20generating%20benchmarks%20to%20check%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code&entry.906535625=Itay%20Dreyfuss%20and%20Antonio%20Abu%20Nassar%20and%20Samuel%20Ackerman%20and%20Axel%20Ben%20David%20and%20Rami%20Katan%20and%20Orna%20Raz%20and%20Marcel%20Zalmanovici&entry.1292438233=Large%20Language%20Model%20%28LLM%29-based%20code%20assistants%20have%20emerged%20as%20a%20powerful%20application%20of%20generative%20AI%2C%20demonstrating%20impressive%20capabilities%20in%20code%20generation%20and%20comprehension.%20A%20key%20requirement%20for%20these%20systems%20is%20their%20ability%20to%20accurately%20follow%20user%20instructions.%20We%20present%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code%20%28PACIFIC%29%2C%20a%20novel%20framework%20designed%20to%20automatically%20generate%20benchmarks%20that%20rigorously%20assess%20sequential%20instruction-following%20and%20code%20dry-running%20capabilities%20in%20LLMs%2C%20while%20allowing%20control%20over%20benchmark%20difficulty.%20PACIFIC%20produces%20benchmark%20variants%20with%20clearly%20defined%20expected%20outputs%2C%20enabling%20straightforward%20and%20reliable%20evaluation%20through%20simple%20output%20comparisons.%20In%20contrast%20to%20existing%20approaches%20that%20often%20rely%20on%20tool%20usage%20or%20agentic%20behavior%2C%20our%20work%20isolates%20and%20evaluates%20the%20LLM%27s%20intrinsic%20ability%20to%20reason%20through%20code%20behavior%20step-by-step%20without%20execution%20%28dry%20running%29%20and%20to%20follow%20instructions.%20Furthermore%2C%20our%20framework%20mitigates%20training%20data%20contamination%20by%20facilitating%20effortless%20generation%20of%20novel%20benchmark%20variations.%20We%20validate%20our%20framework%20by%20generating%20a%20suite%20of%20benchmarks%20spanning%20a%20range%20of%20difficulty%20levels%20and%20evaluating%20multiple%20state-of-the-art%20LLMs.%20Our%20results%20demonstrate%20that%20PACIFIC%20can%20produce%20increasingly%20challenging%20benchmarks%20that%20effectively%20differentiate%20instruction-following%20and%20dry%20running%20capabilities%2C%20even%20among%20advanced%20models.%20Overall%2C%20our%20framework%20offers%20a%20scalable%2C%20contamination-resilient%20methodology%20for%20assessing%20core%20competencies%20of%20LLMs%20in%20code-related%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.10713v1&entry.124074799=Read"},
{"title": "Neural Ranging Inertial Odometry", "author": "Si Wang and Bingqi Shen and Fei Wang and Yanjun Cao and Rong Xiong and Yue Wang", "abstract": "Ultra-wideband (UWB) has shown promising potential in GPS-denied localization thanks to its lightweight and drift-free characteristics, while the accuracy is limited in real scenarios due to its sensitivity to sensor arrangement and non-Gaussian pattern induced by multi-path or multi-signal interference, which commonly occurs in many typical applications like long tunnels. We introduce a novel neural fusion framework for ranging inertial odometry which involves a graph attention UWB network and a recurrent neural inertial network. Our graph net learns scene-relevant ranging patterns and adapts to any number of anchors or tags, realizing accurate positioning without calibration. Additionally, the integration of least squares and the incorporation of nominal frame enhance overall performance and scalability. The effectiveness and robustness of our methods are validated through extensive experiments on both public and self-collected datasets, spanning indoor, outdoor, and tunnel environments. The results demonstrate the superiority of our proposed IR-ULSG in handling challenging conditions, including scenarios outside the convex envelope and cases where only a single anchor is available.", "link": "http://arxiv.org/abs/2512.10531v1", "date": "2025-12-11", "relevancy": 2.2812, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5874}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5666}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Ranging%20Inertial%20Odometry&body=Title%3A%20Neural%20Ranging%20Inertial%20Odometry%0AAuthor%3A%20Si%20Wang%20and%20Bingqi%20Shen%20and%20Fei%20Wang%20and%20Yanjun%20Cao%20and%20Rong%20Xiong%20and%20Yue%20Wang%0AAbstract%3A%20Ultra-wideband%20%28UWB%29%20has%20shown%20promising%20potential%20in%20GPS-denied%20localization%20thanks%20to%20its%20lightweight%20and%20drift-free%20characteristics%2C%20while%20the%20accuracy%20is%20limited%20in%20real%20scenarios%20due%20to%20its%20sensitivity%20to%20sensor%20arrangement%20and%20non-Gaussian%20pattern%20induced%20by%20multi-path%20or%20multi-signal%20interference%2C%20which%20commonly%20occurs%20in%20many%20typical%20applications%20like%20long%20tunnels.%20We%20introduce%20a%20novel%20neural%20fusion%20framework%20for%20ranging%20inertial%20odometry%20which%20involves%20a%20graph%20attention%20UWB%20network%20and%20a%20recurrent%20neural%20inertial%20network.%20Our%20graph%20net%20learns%20scene-relevant%20ranging%20patterns%20and%20adapts%20to%20any%20number%20of%20anchors%20or%20tags%2C%20realizing%20accurate%20positioning%20without%20calibration.%20Additionally%2C%20the%20integration%20of%20least%20squares%20and%20the%20incorporation%20of%20nominal%20frame%20enhance%20overall%20performance%20and%20scalability.%20The%20effectiveness%20and%20robustness%20of%20our%20methods%20are%20validated%20through%20extensive%20experiments%20on%20both%20public%20and%20self-collected%20datasets%2C%20spanning%20indoor%2C%20outdoor%2C%20and%20tunnel%20environments.%20The%20results%20demonstrate%20the%20superiority%20of%20our%20proposed%20IR-ULSG%20in%20handling%20challenging%20conditions%2C%20including%20scenarios%20outside%20the%20convex%20envelope%20and%20cases%20where%20only%20a%20single%20anchor%20is%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Ranging%2520Inertial%2520Odometry%26entry.906535625%3DSi%2520Wang%2520and%2520Bingqi%2520Shen%2520and%2520Fei%2520Wang%2520and%2520Yanjun%2520Cao%2520and%2520Rong%2520Xiong%2520and%2520Yue%2520Wang%26entry.1292438233%3DUltra-wideband%2520%2528UWB%2529%2520has%2520shown%2520promising%2520potential%2520in%2520GPS-denied%2520localization%2520thanks%2520to%2520its%2520lightweight%2520and%2520drift-free%2520characteristics%252C%2520while%2520the%2520accuracy%2520is%2520limited%2520in%2520real%2520scenarios%2520due%2520to%2520its%2520sensitivity%2520to%2520sensor%2520arrangement%2520and%2520non-Gaussian%2520pattern%2520induced%2520by%2520multi-path%2520or%2520multi-signal%2520interference%252C%2520which%2520commonly%2520occurs%2520in%2520many%2520typical%2520applications%2520like%2520long%2520tunnels.%2520We%2520introduce%2520a%2520novel%2520neural%2520fusion%2520framework%2520for%2520ranging%2520inertial%2520odometry%2520which%2520involves%2520a%2520graph%2520attention%2520UWB%2520network%2520and%2520a%2520recurrent%2520neural%2520inertial%2520network.%2520Our%2520graph%2520net%2520learns%2520scene-relevant%2520ranging%2520patterns%2520and%2520adapts%2520to%2520any%2520number%2520of%2520anchors%2520or%2520tags%252C%2520realizing%2520accurate%2520positioning%2520without%2520calibration.%2520Additionally%252C%2520the%2520integration%2520of%2520least%2520squares%2520and%2520the%2520incorporation%2520of%2520nominal%2520frame%2520enhance%2520overall%2520performance%2520and%2520scalability.%2520The%2520effectiveness%2520and%2520robustness%2520of%2520our%2520methods%2520are%2520validated%2520through%2520extensive%2520experiments%2520on%2520both%2520public%2520and%2520self-collected%2520datasets%252C%2520spanning%2520indoor%252C%2520outdoor%252C%2520and%2520tunnel%2520environments.%2520The%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520IR-ULSG%2520in%2520handling%2520challenging%2520conditions%252C%2520including%2520scenarios%2520outside%2520the%2520convex%2520envelope%2520and%2520cases%2520where%2520only%2520a%2520single%2520anchor%2520is%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Ranging%20Inertial%20Odometry&entry.906535625=Si%20Wang%20and%20Bingqi%20Shen%20and%20Fei%20Wang%20and%20Yanjun%20Cao%20and%20Rong%20Xiong%20and%20Yue%20Wang&entry.1292438233=Ultra-wideband%20%28UWB%29%20has%20shown%20promising%20potential%20in%20GPS-denied%20localization%20thanks%20to%20its%20lightweight%20and%20drift-free%20characteristics%2C%20while%20the%20accuracy%20is%20limited%20in%20real%20scenarios%20due%20to%20its%20sensitivity%20to%20sensor%20arrangement%20and%20non-Gaussian%20pattern%20induced%20by%20multi-path%20or%20multi-signal%20interference%2C%20which%20commonly%20occurs%20in%20many%20typical%20applications%20like%20long%20tunnels.%20We%20introduce%20a%20novel%20neural%20fusion%20framework%20for%20ranging%20inertial%20odometry%20which%20involves%20a%20graph%20attention%20UWB%20network%20and%20a%20recurrent%20neural%20inertial%20network.%20Our%20graph%20net%20learns%20scene-relevant%20ranging%20patterns%20and%20adapts%20to%20any%20number%20of%20anchors%20or%20tags%2C%20realizing%20accurate%20positioning%20without%20calibration.%20Additionally%2C%20the%20integration%20of%20least%20squares%20and%20the%20incorporation%20of%20nominal%20frame%20enhance%20overall%20performance%20and%20scalability.%20The%20effectiveness%20and%20robustness%20of%20our%20methods%20are%20validated%20through%20extensive%20experiments%20on%20both%20public%20and%20self-collected%20datasets%2C%20spanning%20indoor%2C%20outdoor%2C%20and%20tunnel%20environments.%20The%20results%20demonstrate%20the%20superiority%20of%20our%20proposed%20IR-ULSG%20in%20handling%20challenging%20conditions%2C%20including%20scenarios%20outside%20the%20convex%20envelope%20and%20cases%20where%20only%20a%20single%20anchor%20is%20available.&entry.1838667208=http%3A//arxiv.org/abs/2512.10531v1&entry.124074799=Read"},
{"title": "Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner", "author": "Haojie Zheng and Shuchen Weng and Jingqi Liu and Siqi Yang and Boxin Shi and Xinlong Wang", "abstract": "Recent advancements in video generation highlight that realistic audio-visual synchronization is crucial for engaging content creation. However, existing video editing methods largely overlook audio-visual synchronization and lack the fine-grained spatial and temporal controllability required for precise instance-level edits. In this paper, we propose AVI-Edit, a framework for audio-sync video instance editing. We propose a granularity-aware mask refiner that iteratively refines coarse user-provided masks into precise instance-level regions. We further design a self-feedback audio agent to curate high-quality audio guidance, providing fine-grained temporal control. To facilitate this task, we additionally construct a large-scale dataset with instance-centric correspondence and comprehensive annotations. Extensive experiments demonstrate that AVI-Edit outperforms state-of-the-art methods in visual quality, condition following, and audio-visual synchronization. Project page: https://hjzheng.net/projects/AVI-Edit/.", "link": "http://arxiv.org/abs/2512.10571v1", "date": "2025-12-11", "relevancy": 2.277, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5779}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.576}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-sync%20Video%20Instance%20Editing%20with%20Granularity-Aware%20Mask%20Refiner&body=Title%3A%20Audio-sync%20Video%20Instance%20Editing%20with%20Granularity-Aware%20Mask%20Refiner%0AAuthor%3A%20Haojie%20Zheng%20and%20Shuchen%20Weng%20and%20Jingqi%20Liu%20and%20Siqi%20Yang%20and%20Boxin%20Shi%20and%20Xinlong%20Wang%0AAbstract%3A%20Recent%20advancements%20in%20video%20generation%20highlight%20that%20realistic%20audio-visual%20synchronization%20is%20crucial%20for%20engaging%20content%20creation.%20However%2C%20existing%20video%20editing%20methods%20largely%20overlook%20audio-visual%20synchronization%20and%20lack%20the%20fine-grained%20spatial%20and%20temporal%20controllability%20required%20for%20precise%20instance-level%20edits.%20In%20this%20paper%2C%20we%20propose%20AVI-Edit%2C%20a%20framework%20for%20audio-sync%20video%20instance%20editing.%20We%20propose%20a%20granularity-aware%20mask%20refiner%20that%20iteratively%20refines%20coarse%20user-provided%20masks%20into%20precise%20instance-level%20regions.%20We%20further%20design%20a%20self-feedback%20audio%20agent%20to%20curate%20high-quality%20audio%20guidance%2C%20providing%20fine-grained%20temporal%20control.%20To%20facilitate%20this%20task%2C%20we%20additionally%20construct%20a%20large-scale%20dataset%20with%20instance-centric%20correspondence%20and%20comprehensive%20annotations.%20Extensive%20experiments%20demonstrate%20that%20AVI-Edit%20outperforms%20state-of-the-art%20methods%20in%20visual%20quality%2C%20condition%20following%2C%20and%20audio-visual%20synchronization.%20Project%20page%3A%20https%3A//hjzheng.net/projects/AVI-Edit/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-sync%2520Video%2520Instance%2520Editing%2520with%2520Granularity-Aware%2520Mask%2520Refiner%26entry.906535625%3DHaojie%2520Zheng%2520and%2520Shuchen%2520Weng%2520and%2520Jingqi%2520Liu%2520and%2520Siqi%2520Yang%2520and%2520Boxin%2520Shi%2520and%2520Xinlong%2520Wang%26entry.1292438233%3DRecent%2520advancements%2520in%2520video%2520generation%2520highlight%2520that%2520realistic%2520audio-visual%2520synchronization%2520is%2520crucial%2520for%2520engaging%2520content%2520creation.%2520However%252C%2520existing%2520video%2520editing%2520methods%2520largely%2520overlook%2520audio-visual%2520synchronization%2520and%2520lack%2520the%2520fine-grained%2520spatial%2520and%2520temporal%2520controllability%2520required%2520for%2520precise%2520instance-level%2520edits.%2520In%2520this%2520paper%252C%2520we%2520propose%2520AVI-Edit%252C%2520a%2520framework%2520for%2520audio-sync%2520video%2520instance%2520editing.%2520We%2520propose%2520a%2520granularity-aware%2520mask%2520refiner%2520that%2520iteratively%2520refines%2520coarse%2520user-provided%2520masks%2520into%2520precise%2520instance-level%2520regions.%2520We%2520further%2520design%2520a%2520self-feedback%2520audio%2520agent%2520to%2520curate%2520high-quality%2520audio%2520guidance%252C%2520providing%2520fine-grained%2520temporal%2520control.%2520To%2520facilitate%2520this%2520task%252C%2520we%2520additionally%2520construct%2520a%2520large-scale%2520dataset%2520with%2520instance-centric%2520correspondence%2520and%2520comprehensive%2520annotations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520AVI-Edit%2520outperforms%2520state-of-the-art%2520methods%2520in%2520visual%2520quality%252C%2520condition%2520following%252C%2520and%2520audio-visual%2520synchronization.%2520Project%2520page%253A%2520https%253A//hjzheng.net/projects/AVI-Edit/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-sync%20Video%20Instance%20Editing%20with%20Granularity-Aware%20Mask%20Refiner&entry.906535625=Haojie%20Zheng%20and%20Shuchen%20Weng%20and%20Jingqi%20Liu%20and%20Siqi%20Yang%20and%20Boxin%20Shi%20and%20Xinlong%20Wang&entry.1292438233=Recent%20advancements%20in%20video%20generation%20highlight%20that%20realistic%20audio-visual%20synchronization%20is%20crucial%20for%20engaging%20content%20creation.%20However%2C%20existing%20video%20editing%20methods%20largely%20overlook%20audio-visual%20synchronization%20and%20lack%20the%20fine-grained%20spatial%20and%20temporal%20controllability%20required%20for%20precise%20instance-level%20edits.%20In%20this%20paper%2C%20we%20propose%20AVI-Edit%2C%20a%20framework%20for%20audio-sync%20video%20instance%20editing.%20We%20propose%20a%20granularity-aware%20mask%20refiner%20that%20iteratively%20refines%20coarse%20user-provided%20masks%20into%20precise%20instance-level%20regions.%20We%20further%20design%20a%20self-feedback%20audio%20agent%20to%20curate%20high-quality%20audio%20guidance%2C%20providing%20fine-grained%20temporal%20control.%20To%20facilitate%20this%20task%2C%20we%20additionally%20construct%20a%20large-scale%20dataset%20with%20instance-centric%20correspondence%20and%20comprehensive%20annotations.%20Extensive%20experiments%20demonstrate%20that%20AVI-Edit%20outperforms%20state-of-the-art%20methods%20in%20visual%20quality%2C%20condition%20following%2C%20and%20audio-visual%20synchronization.%20Project%20page%3A%20https%3A//hjzheng.net/projects/AVI-Edit/.&entry.1838667208=http%3A//arxiv.org/abs/2512.10571v1&entry.124074799=Read"},
{"title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model", "author": "Dian Zheng and Manyuan Zhang and Hongyu Li and Kai Zou and Hongbo Liu and Ziyu Guo and Kaituo Feng and Yexin Liu and Ying Luo and Yan Feng and Peng Pei and Xunliang Cai and Hongsheng Li", "abstract": "Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.", "link": "http://arxiv.org/abs/2511.22663v2", "date": "2025-12-11", "relevancy": 2.2748, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5809}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architecture%20Decoupling%20Is%20Not%20All%20You%20Need%20For%20Unified%20Multimodal%20Model&body=Title%3A%20Architecture%20Decoupling%20Is%20Not%20All%20You%20Need%20For%20Unified%20Multimodal%20Model%0AAuthor%3A%20Dian%20Zheng%20and%20Manyuan%20Zhang%20and%20Hongyu%20Li%20and%20Kai%20Zou%20and%20Hongbo%20Liu%20and%20Ziyu%20Guo%20and%20Kaituo%20Feng%20and%20Yexin%20Liu%20and%20Ying%20Luo%20and%20Yan%20Feng%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Hongsheng%20Li%0AAbstract%3A%20Unified%20multimodal%20models%20for%20image%20generation%20and%20understanding%20represent%20a%20significant%20step%20toward%20AGI%20and%20have%20attracted%20widespread%20attention%20from%20researchers.%20The%20main%20challenge%20of%20this%20task%20lies%20in%20the%20difficulty%20in%20establishing%20an%20optimal%20training%20paradigm%20due%20to%20inherent%20conflicting%20targets%20in%20understanding%20and%20generation%20tasks.%20To%20alleviate%20these%20conflicts%20and%20pursue%20higher%20performance%2C%20many%20researchers%20adopt%20varying%20degrees%20of%20model%20decoupling%20%28e.g.%2C%20Double%20image%20encoders%2C%20MOE/MOT%20architecture%2C%20or%20frozen%20MLLM%29.%20However%2C%20excessive%20model%20decoupling%20can%20lead%20to%20the%20loss%20of%20interleave%20generation%20ability%2C%20undermining%20the%20original%20intent%20of%20unified%20models.%20In%20this%20work%2C%20we%20aim%20to%20explore%20how%20to%20mitigate%20task%20conflicts%20without%20resorting%20to%20model%20decoupling.%20Firstly%2C%20we%20analyze%20why%20decoupling%20alleviates%20conflicts%20by%20studying%20the%20cross-modal%20attention%20behavior%20of%20models.%20We%20observe%20that%20model%20decoupling%20essentially%20drives%20models%20toward%20task-specific%20multimodal%20interaction%20patterns%2C%20as%20seen%20in%20Qwen-VL%20and%20HunyuanImage%2C%20and%20that%20the%20more%20thorough%20the%20decoupling%2C%20the%20more%20consistent%20the%20behavior%20becomes.%20Motivated%20by%20this%20observation%2C%20we%20propose%20Attention%20Interaction%20Alignment%20%28AIA%29%20loss%2C%20which%20explicitly%20learns%20Task-Specific%20multimodal%20interaction%20patterns%20during%20training.%20To%20demonstrate%20the%20generalizability%20of%20our%20AIA%20loss%2C%20we%20apply%20it%20to%20Emu3%20and%20Janus-Pro%20during%20SFT%20and%20post-training%20stage%20respectively.%20Without%20bells%20and%20whistles%2C%20AIA%20not%20only%20refines%20cross-modal%20attention%20patterns%2C%20but%20also%20boosts%20both%20generation%20and%20understanding%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.22663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitecture%2520Decoupling%2520Is%2520Not%2520All%2520You%2520Need%2520For%2520Unified%2520Multimodal%2520Model%26entry.906535625%3DDian%2520Zheng%2520and%2520Manyuan%2520Zhang%2520and%2520Hongyu%2520Li%2520and%2520Kai%2520Zou%2520and%2520Hongbo%2520Liu%2520and%2520Ziyu%2520Guo%2520and%2520Kaituo%2520Feng%2520and%2520Yexin%2520Liu%2520and%2520Ying%2520Luo%2520and%2520Yan%2520Feng%2520and%2520Peng%2520Pei%2520and%2520Xunliang%2520Cai%2520and%2520Hongsheng%2520Li%26entry.1292438233%3DUnified%2520multimodal%2520models%2520for%2520image%2520generation%2520and%2520understanding%2520represent%2520a%2520significant%2520step%2520toward%2520AGI%2520and%2520have%2520attracted%2520widespread%2520attention%2520from%2520researchers.%2520The%2520main%2520challenge%2520of%2520this%2520task%2520lies%2520in%2520the%2520difficulty%2520in%2520establishing%2520an%2520optimal%2520training%2520paradigm%2520due%2520to%2520inherent%2520conflicting%2520targets%2520in%2520understanding%2520and%2520generation%2520tasks.%2520To%2520alleviate%2520these%2520conflicts%2520and%2520pursue%2520higher%2520performance%252C%2520many%2520researchers%2520adopt%2520varying%2520degrees%2520of%2520model%2520decoupling%2520%2528e.g.%252C%2520Double%2520image%2520encoders%252C%2520MOE/MOT%2520architecture%252C%2520or%2520frozen%2520MLLM%2529.%2520However%252C%2520excessive%2520model%2520decoupling%2520can%2520lead%2520to%2520the%2520loss%2520of%2520interleave%2520generation%2520ability%252C%2520undermining%2520the%2520original%2520intent%2520of%2520unified%2520models.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520explore%2520how%2520to%2520mitigate%2520task%2520conflicts%2520without%2520resorting%2520to%2520model%2520decoupling.%2520Firstly%252C%2520we%2520analyze%2520why%2520decoupling%2520alleviates%2520conflicts%2520by%2520studying%2520the%2520cross-modal%2520attention%2520behavior%2520of%2520models.%2520We%2520observe%2520that%2520model%2520decoupling%2520essentially%2520drives%2520models%2520toward%2520task-specific%2520multimodal%2520interaction%2520patterns%252C%2520as%2520seen%2520in%2520Qwen-VL%2520and%2520HunyuanImage%252C%2520and%2520that%2520the%2520more%2520thorough%2520the%2520decoupling%252C%2520the%2520more%2520consistent%2520the%2520behavior%2520becomes.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520Attention%2520Interaction%2520Alignment%2520%2528AIA%2529%2520loss%252C%2520which%2520explicitly%2520learns%2520Task-Specific%2520multimodal%2520interaction%2520patterns%2520during%2520training.%2520To%2520demonstrate%2520the%2520generalizability%2520of%2520our%2520AIA%2520loss%252C%2520we%2520apply%2520it%2520to%2520Emu3%2520and%2520Janus-Pro%2520during%2520SFT%2520and%2520post-training%2520stage%2520respectively.%2520Without%2520bells%2520and%2520whistles%252C%2520AIA%2520not%2520only%2520refines%2520cross-modal%2520attention%2520patterns%252C%2520but%2520also%2520boosts%2520both%2520generation%2520and%2520understanding%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.22663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architecture%20Decoupling%20Is%20Not%20All%20You%20Need%20For%20Unified%20Multimodal%20Model&entry.906535625=Dian%20Zheng%20and%20Manyuan%20Zhang%20and%20Hongyu%20Li%20and%20Kai%20Zou%20and%20Hongbo%20Liu%20and%20Ziyu%20Guo%20and%20Kaituo%20Feng%20and%20Yexin%20Liu%20and%20Ying%20Luo%20and%20Yan%20Feng%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Hongsheng%20Li&entry.1292438233=Unified%20multimodal%20models%20for%20image%20generation%20and%20understanding%20represent%20a%20significant%20step%20toward%20AGI%20and%20have%20attracted%20widespread%20attention%20from%20researchers.%20The%20main%20challenge%20of%20this%20task%20lies%20in%20the%20difficulty%20in%20establishing%20an%20optimal%20training%20paradigm%20due%20to%20inherent%20conflicting%20targets%20in%20understanding%20and%20generation%20tasks.%20To%20alleviate%20these%20conflicts%20and%20pursue%20higher%20performance%2C%20many%20researchers%20adopt%20varying%20degrees%20of%20model%20decoupling%20%28e.g.%2C%20Double%20image%20encoders%2C%20MOE/MOT%20architecture%2C%20or%20frozen%20MLLM%29.%20However%2C%20excessive%20model%20decoupling%20can%20lead%20to%20the%20loss%20of%20interleave%20generation%20ability%2C%20undermining%20the%20original%20intent%20of%20unified%20models.%20In%20this%20work%2C%20we%20aim%20to%20explore%20how%20to%20mitigate%20task%20conflicts%20without%20resorting%20to%20model%20decoupling.%20Firstly%2C%20we%20analyze%20why%20decoupling%20alleviates%20conflicts%20by%20studying%20the%20cross-modal%20attention%20behavior%20of%20models.%20We%20observe%20that%20model%20decoupling%20essentially%20drives%20models%20toward%20task-specific%20multimodal%20interaction%20patterns%2C%20as%20seen%20in%20Qwen-VL%20and%20HunyuanImage%2C%20and%20that%20the%20more%20thorough%20the%20decoupling%2C%20the%20more%20consistent%20the%20behavior%20becomes.%20Motivated%20by%20this%20observation%2C%20we%20propose%20Attention%20Interaction%20Alignment%20%28AIA%29%20loss%2C%20which%20explicitly%20learns%20Task-Specific%20multimodal%20interaction%20patterns%20during%20training.%20To%20demonstrate%20the%20generalizability%20of%20our%20AIA%20loss%2C%20we%20apply%20it%20to%20Emu3%20and%20Janus-Pro%20during%20SFT%20and%20post-training%20stage%20respectively.%20Without%20bells%20and%20whistles%2C%20AIA%20not%20only%20refines%20cross-modal%20attention%20patterns%2C%20but%20also%20boosts%20both%20generation%20and%20understanding%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.22663v2&entry.124074799=Read"},
{"title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images", "author": "Matias Cosarinsky and Nicolas Gaggion and Rodrigo Echeveste and Enzo Ferrante", "abstract": "Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.", "link": "http://arxiv.org/abs/2512.10715v1", "date": "2025-12-11", "relevancy": 2.2631, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.643}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5957}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CheXmask-U%3A%20Quantifying%20uncertainty%20in%20landmark-based%20anatomical%20segmentation%20for%20X-ray%20images&body=Title%3A%20CheXmask-U%3A%20Quantifying%20uncertainty%20in%20landmark-based%20anatomical%20segmentation%20for%20X-ray%20images%0AAuthor%3A%20Matias%20Cosarinsky%20and%20Nicolas%20Gaggion%20and%20Rodrigo%20Echeveste%20and%20Enzo%20Ferrante%0AAbstract%3A%20Uncertainty%20estimation%20is%20essential%20for%20the%20safe%20clinical%20deployment%20of%20medical%20image%20segmentation%20systems%2C%20enabling%20the%20identification%20of%20unreliable%20predictions%20and%20supporting%20human%20oversight.%20While%20prior%20work%20has%20largely%20focused%20on%20pixel-level%20uncertainty%2C%20landmark-based%20segmentation%20offers%20inherent%20topological%20guarantees%20yet%20remains%20underexplored%20from%20an%20uncertainty%20perspective.%20In%20this%20work%2C%20we%20study%20uncertainty%20estimation%20for%20anatomical%20landmark-based%20segmentation%20on%20chest%20X-rays.%20Inspired%20by%20hybrid%20neural%20network%20architectures%20that%20combine%20standard%20image%20convolutional%20encoders%20with%20graph-based%20generative%20decoders%2C%20and%20leveraging%20their%20variational%20latent%20space%2C%20we%20derive%20two%20complementary%20measures%3A%20%28i%29%20latent%20uncertainty%2C%20captured%20directly%20from%20the%20learned%20distribution%20parameters%2C%20and%20%28ii%29%20predictive%20uncertainty%2C%20obtained%20by%20generating%20multiple%20stochastic%20output%20predictions%20from%20latent%20samples.%20Through%20controlled%20corruption%20experiments%20we%20show%20that%20both%20uncertainty%20measures%20increase%20with%20perturbation%20severity%2C%20reflecting%20both%20global%20and%20local%20degradation.%20We%20demonstrate%20that%20these%20uncertainty%20signals%20can%20identify%20unreliable%20predictions%20by%20comparing%20with%20manual%20ground-truth%2C%20and%20support%20out-of-distribution%20detection%20on%20the%20CheXmask%20dataset.%20More%20importantly%2C%20we%20release%20CheXmask-U%20%28huggingface.co/datasets/mcosarinsky/CheXmask-U%29%2C%20a%20large%20scale%20dataset%20of%20657%2C566%20chest%20X-ray%20landmark%20segmentations%20with%20per-node%20uncertainty%20estimates%2C%20enabling%20researchers%20to%20account%20for%20spatial%20variations%20in%20segmentation%20quality%20when%20using%20these%20anatomical%20masks.%20Our%20findings%20establish%20uncertainty%20estimation%20as%20a%20promising%20direction%20to%20enhance%20robustness%20and%20safe%20deployment%20of%20landmark-based%20anatomical%20segmentation%20methods%20in%20chest%20X-ray.%20A%20fully%20working%20interactive%20demo%20of%20the%20method%20is%20available%20at%20huggingface.co/spaces/matiasky/CheXmask-U%20and%20the%20source%20code%20at%20github.com/mcosarinsky/CheXmask-U.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCheXmask-U%253A%2520Quantifying%2520uncertainty%2520in%2520landmark-based%2520anatomical%2520segmentation%2520for%2520X-ray%2520images%26entry.906535625%3DMatias%2520Cosarinsky%2520and%2520Nicolas%2520Gaggion%2520and%2520Rodrigo%2520Echeveste%2520and%2520Enzo%2520Ferrante%26entry.1292438233%3DUncertainty%2520estimation%2520is%2520essential%2520for%2520the%2520safe%2520clinical%2520deployment%2520of%2520medical%2520image%2520segmentation%2520systems%252C%2520enabling%2520the%2520identification%2520of%2520unreliable%2520predictions%2520and%2520supporting%2520human%2520oversight.%2520While%2520prior%2520work%2520has%2520largely%2520focused%2520on%2520pixel-level%2520uncertainty%252C%2520landmark-based%2520segmentation%2520offers%2520inherent%2520topological%2520guarantees%2520yet%2520remains%2520underexplored%2520from%2520an%2520uncertainty%2520perspective.%2520In%2520this%2520work%252C%2520we%2520study%2520uncertainty%2520estimation%2520for%2520anatomical%2520landmark-based%2520segmentation%2520on%2520chest%2520X-rays.%2520Inspired%2520by%2520hybrid%2520neural%2520network%2520architectures%2520that%2520combine%2520standard%2520image%2520convolutional%2520encoders%2520with%2520graph-based%2520generative%2520decoders%252C%2520and%2520leveraging%2520their%2520variational%2520latent%2520space%252C%2520we%2520derive%2520two%2520complementary%2520measures%253A%2520%2528i%2529%2520latent%2520uncertainty%252C%2520captured%2520directly%2520from%2520the%2520learned%2520distribution%2520parameters%252C%2520and%2520%2528ii%2529%2520predictive%2520uncertainty%252C%2520obtained%2520by%2520generating%2520multiple%2520stochastic%2520output%2520predictions%2520from%2520latent%2520samples.%2520Through%2520controlled%2520corruption%2520experiments%2520we%2520show%2520that%2520both%2520uncertainty%2520measures%2520increase%2520with%2520perturbation%2520severity%252C%2520reflecting%2520both%2520global%2520and%2520local%2520degradation.%2520We%2520demonstrate%2520that%2520these%2520uncertainty%2520signals%2520can%2520identify%2520unreliable%2520predictions%2520by%2520comparing%2520with%2520manual%2520ground-truth%252C%2520and%2520support%2520out-of-distribution%2520detection%2520on%2520the%2520CheXmask%2520dataset.%2520More%2520importantly%252C%2520we%2520release%2520CheXmask-U%2520%2528huggingface.co/datasets/mcosarinsky/CheXmask-U%2529%252C%2520a%2520large%2520scale%2520dataset%2520of%2520657%252C566%2520chest%2520X-ray%2520landmark%2520segmentations%2520with%2520per-node%2520uncertainty%2520estimates%252C%2520enabling%2520researchers%2520to%2520account%2520for%2520spatial%2520variations%2520in%2520segmentation%2520quality%2520when%2520using%2520these%2520anatomical%2520masks.%2520Our%2520findings%2520establish%2520uncertainty%2520estimation%2520as%2520a%2520promising%2520direction%2520to%2520enhance%2520robustness%2520and%2520safe%2520deployment%2520of%2520landmark-based%2520anatomical%2520segmentation%2520methods%2520in%2520chest%2520X-ray.%2520A%2520fully%2520working%2520interactive%2520demo%2520of%2520the%2520method%2520is%2520available%2520at%2520huggingface.co/spaces/matiasky/CheXmask-U%2520and%2520the%2520source%2520code%2520at%2520github.com/mcosarinsky/CheXmask-U.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CheXmask-U%3A%20Quantifying%20uncertainty%20in%20landmark-based%20anatomical%20segmentation%20for%20X-ray%20images&entry.906535625=Matias%20Cosarinsky%20and%20Nicolas%20Gaggion%20and%20Rodrigo%20Echeveste%20and%20Enzo%20Ferrante&entry.1292438233=Uncertainty%20estimation%20is%20essential%20for%20the%20safe%20clinical%20deployment%20of%20medical%20image%20segmentation%20systems%2C%20enabling%20the%20identification%20of%20unreliable%20predictions%20and%20supporting%20human%20oversight.%20While%20prior%20work%20has%20largely%20focused%20on%20pixel-level%20uncertainty%2C%20landmark-based%20segmentation%20offers%20inherent%20topological%20guarantees%20yet%20remains%20underexplored%20from%20an%20uncertainty%20perspective.%20In%20this%20work%2C%20we%20study%20uncertainty%20estimation%20for%20anatomical%20landmark-based%20segmentation%20on%20chest%20X-rays.%20Inspired%20by%20hybrid%20neural%20network%20architectures%20that%20combine%20standard%20image%20convolutional%20encoders%20with%20graph-based%20generative%20decoders%2C%20and%20leveraging%20their%20variational%20latent%20space%2C%20we%20derive%20two%20complementary%20measures%3A%20%28i%29%20latent%20uncertainty%2C%20captured%20directly%20from%20the%20learned%20distribution%20parameters%2C%20and%20%28ii%29%20predictive%20uncertainty%2C%20obtained%20by%20generating%20multiple%20stochastic%20output%20predictions%20from%20latent%20samples.%20Through%20controlled%20corruption%20experiments%20we%20show%20that%20both%20uncertainty%20measures%20increase%20with%20perturbation%20severity%2C%20reflecting%20both%20global%20and%20local%20degradation.%20We%20demonstrate%20that%20these%20uncertainty%20signals%20can%20identify%20unreliable%20predictions%20by%20comparing%20with%20manual%20ground-truth%2C%20and%20support%20out-of-distribution%20detection%20on%20the%20CheXmask%20dataset.%20More%20importantly%2C%20we%20release%20CheXmask-U%20%28huggingface.co/datasets/mcosarinsky/CheXmask-U%29%2C%20a%20large%20scale%20dataset%20of%20657%2C566%20chest%20X-ray%20landmark%20segmentations%20with%20per-node%20uncertainty%20estimates%2C%20enabling%20researchers%20to%20account%20for%20spatial%20variations%20in%20segmentation%20quality%20when%20using%20these%20anatomical%20masks.%20Our%20findings%20establish%20uncertainty%20estimation%20as%20a%20promising%20direction%20to%20enhance%20robustness%20and%20safe%20deployment%20of%20landmark-based%20anatomical%20segmentation%20methods%20in%20chest%20X-ray.%20A%20fully%20working%20interactive%20demo%20of%20the%20method%20is%20available%20at%20huggingface.co/spaces/matiasky/CheXmask-U%20and%20the%20source%20code%20at%20github.com/mcosarinsky/CheXmask-U.&entry.1838667208=http%3A//arxiv.org/abs/2512.10715v1&entry.124074799=Read"},
{"title": "Multi-Granular Node Pruning for Circuit Discovery", "author": "Muhammad Umair Haider and Hammad Rizwan and Hassan Sajjad and A. B. Siddique", "abstract": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.", "link": "http://arxiv.org/abs/2512.10903v1", "date": "2025-12-11", "relevancy": 2.2596, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4563}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Granular%20Node%20Pruning%20for%20Circuit%20Discovery&body=Title%3A%20Multi-Granular%20Node%20Pruning%20for%20Circuit%20Discovery%0AAuthor%3A%20Muhammad%20Umair%20Haider%20and%20Hammad%20Rizwan%20and%20Hassan%20Sajjad%20and%20A.%20B.%20Siddique%0AAbstract%3A%20Circuit%20discovery%20aims%20to%20identify%20minimal%20subnetworks%20that%20are%20responsible%20for%20specific%20behaviors%20in%20large%20language%20models%20%28LLMs%29.%20Existing%20approaches%20primarily%20rely%20on%20iterative%20edge%20pruning%2C%20which%20is%20computationally%20expensive%20and%20limited%20to%20coarse-grained%20units%20such%20as%20attention%20heads%20or%20MLP%20blocks%2C%20overlooking%20finer%20structures%20like%20individual%20neurons.%20We%20propose%20a%20node-level%20pruning%20framework%20for%20circuit%20discovery%20that%20addresses%20both%20scalability%20and%20granularity%20limitations.%20Our%20method%20introduces%20learnable%20masks%20across%20multiple%20levels%20of%20granularity%2C%20from%20entire%20blocks%20to%20individual%20neurons%2C%20within%20a%20unified%20optimization%20objective.%20Granularity-specific%20sparsity%20penalties%20guide%20the%20pruning%20process%2C%20allowing%20a%20comprehensive%20compression%20in%20a%20single%20fine-tuning%20run.%20Empirically%2C%20our%20approach%20identifies%20circuits%20that%20are%20smaller%20in%20nodes%20than%20those%20discovered%20by%20prior%20methods%3B%20moreover%2C%20we%20demonstrate%20that%20many%20neurons%20deemed%20important%20by%20coarse%20methods%20are%20actually%20irrelevant%2C%20while%20still%20maintaining%20task%20performance.%20Furthermore%2C%20our%20method%20has%20a%20significantly%20lower%20memory%20footprint%2C%205-10x%2C%20as%20it%20does%20not%20require%20keeping%20intermediate%20activations%20in%20the%20memory%20to%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Granular%2520Node%2520Pruning%2520for%2520Circuit%2520Discovery%26entry.906535625%3DMuhammad%2520Umair%2520Haider%2520and%2520Hammad%2520Rizwan%2520and%2520Hassan%2520Sajjad%2520and%2520A.%2520B.%2520Siddique%26entry.1292438233%3DCircuit%2520discovery%2520aims%2520to%2520identify%2520minimal%2520subnetworks%2520that%2520are%2520responsible%2520for%2520specific%2520behaviors%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Existing%2520approaches%2520primarily%2520rely%2520on%2520iterative%2520edge%2520pruning%252C%2520which%2520is%2520computationally%2520expensive%2520and%2520limited%2520to%2520coarse-grained%2520units%2520such%2520as%2520attention%2520heads%2520or%2520MLP%2520blocks%252C%2520overlooking%2520finer%2520structures%2520like%2520individual%2520neurons.%2520We%2520propose%2520a%2520node-level%2520pruning%2520framework%2520for%2520circuit%2520discovery%2520that%2520addresses%2520both%2520scalability%2520and%2520granularity%2520limitations.%2520Our%2520method%2520introduces%2520learnable%2520masks%2520across%2520multiple%2520levels%2520of%2520granularity%252C%2520from%2520entire%2520blocks%2520to%2520individual%2520neurons%252C%2520within%2520a%2520unified%2520optimization%2520objective.%2520Granularity-specific%2520sparsity%2520penalties%2520guide%2520the%2520pruning%2520process%252C%2520allowing%2520a%2520comprehensive%2520compression%2520in%2520a%2520single%2520fine-tuning%2520run.%2520Empirically%252C%2520our%2520approach%2520identifies%2520circuits%2520that%2520are%2520smaller%2520in%2520nodes%2520than%2520those%2520discovered%2520by%2520prior%2520methods%253B%2520moreover%252C%2520we%2520demonstrate%2520that%2520many%2520neurons%2520deemed%2520important%2520by%2520coarse%2520methods%2520are%2520actually%2520irrelevant%252C%2520while%2520still%2520maintaining%2520task%2520performance.%2520Furthermore%252C%2520our%2520method%2520has%2520a%2520significantly%2520lower%2520memory%2520footprint%252C%25205-10x%252C%2520as%2520it%2520does%2520not%2520require%2520keeping%2520intermediate%2520activations%2520in%2520the%2520memory%2520to%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Granular%20Node%20Pruning%20for%20Circuit%20Discovery&entry.906535625=Muhammad%20Umair%20Haider%20and%20Hammad%20Rizwan%20and%20Hassan%20Sajjad%20and%20A.%20B.%20Siddique&entry.1292438233=Circuit%20discovery%20aims%20to%20identify%20minimal%20subnetworks%20that%20are%20responsible%20for%20specific%20behaviors%20in%20large%20language%20models%20%28LLMs%29.%20Existing%20approaches%20primarily%20rely%20on%20iterative%20edge%20pruning%2C%20which%20is%20computationally%20expensive%20and%20limited%20to%20coarse-grained%20units%20such%20as%20attention%20heads%20or%20MLP%20blocks%2C%20overlooking%20finer%20structures%20like%20individual%20neurons.%20We%20propose%20a%20node-level%20pruning%20framework%20for%20circuit%20discovery%20that%20addresses%20both%20scalability%20and%20granularity%20limitations.%20Our%20method%20introduces%20learnable%20masks%20across%20multiple%20levels%20of%20granularity%2C%20from%20entire%20blocks%20to%20individual%20neurons%2C%20within%20a%20unified%20optimization%20objective.%20Granularity-specific%20sparsity%20penalties%20guide%20the%20pruning%20process%2C%20allowing%20a%20comprehensive%20compression%20in%20a%20single%20fine-tuning%20run.%20Empirically%2C%20our%20approach%20identifies%20circuits%20that%20are%20smaller%20in%20nodes%20than%20those%20discovered%20by%20prior%20methods%3B%20moreover%2C%20we%20demonstrate%20that%20many%20neurons%20deemed%20important%20by%20coarse%20methods%20are%20actually%20irrelevant%2C%20while%20still%20maintaining%20task%20performance.%20Furthermore%2C%20our%20method%20has%20a%20significantly%20lower%20memory%20footprint%2C%205-10x%2C%20as%20it%20does%20not%20require%20keeping%20intermediate%20activations%20in%20the%20memory%20to%20work.&entry.1838667208=http%3A//arxiv.org/abs/2512.10903v1&entry.124074799=Read"},
{"title": "Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS/UWB/IMU Fusion: A Comparison of EKF, FGO, and PF", "author": "Jiaqiang Zhang and Xianjia Yu and Sier Ha and Paola Torrico Moron and Sahar Salimpour and Farhad Kerama and Haizhou Zhang and Tomi Westerlund", "abstract": "Accurate and continuous pedestrian positioning across outdoor-indoor environments remains challenging because GNSS, UWB, and inertial PDR are complementary yet individually fragile under signal blockage, multipath, and drift. This paper presents a unified GNSS/UWB/IMU fusion framework for seamless pedestrian localization and provides a controlled comparison of three probabilistic back-ends: an error-state extended Kalman filter, sliding-window factor graph optimization, and a particle filter. The system uses chest-mounted IMU-based PDR as the motion backbone and integrates absolute updates from GNSS outdoors and UWB indoors. To enhance transition robustness and mitigate urban GNSS degradation, we introduce a lightweight map-based feasibility constraint derived from OpenStreetMap building footprints, treating most building interiors as non-navigable while allowing motion inside a designated UWB-instrumented building. The framework is implemented in ROS 2 and runs in real time on a wearable platform, with visualization in Foxglove. We evaluate three scenarios: indoor (UWB+PDR), outdoor (GNSS+PDR), and seamless outdoor-indoor (GNSS+UWB+PDR). Results show that the ESKF provides the most consistent overall performance in our implementation.", "link": "http://arxiv.org/abs/2512.10480v1", "date": "2025-12-11", "relevancy": 2.2545, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5987}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5512}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seamless%20Outdoor-Indoor%20Pedestrian%20Positioning%20System%20with%20GNSS/UWB/IMU%20Fusion%3A%20A%20Comparison%20of%20EKF%2C%20FGO%2C%20and%20PF&body=Title%3A%20Seamless%20Outdoor-Indoor%20Pedestrian%20Positioning%20System%20with%20GNSS/UWB/IMU%20Fusion%3A%20A%20Comparison%20of%20EKF%2C%20FGO%2C%20and%20PF%0AAuthor%3A%20Jiaqiang%20Zhang%20and%20Xianjia%20Yu%20and%20Sier%20Ha%20and%20Paola%20Torrico%20Moron%20and%20Sahar%20Salimpour%20and%20Farhad%20Kerama%20and%20Haizhou%20Zhang%20and%20Tomi%20Westerlund%0AAbstract%3A%20Accurate%20and%20continuous%20pedestrian%20positioning%20across%20outdoor-indoor%20environments%20remains%20challenging%20because%20GNSS%2C%20UWB%2C%20and%20inertial%20PDR%20are%20complementary%20yet%20individually%20fragile%20under%20signal%20blockage%2C%20multipath%2C%20and%20drift.%20This%20paper%20presents%20a%20unified%20GNSS/UWB/IMU%20fusion%20framework%20for%20seamless%20pedestrian%20localization%20and%20provides%20a%20controlled%20comparison%20of%20three%20probabilistic%20back-ends%3A%20an%20error-state%20extended%20Kalman%20filter%2C%20sliding-window%20factor%20graph%20optimization%2C%20and%20a%20particle%20filter.%20The%20system%20uses%20chest-mounted%20IMU-based%20PDR%20as%20the%20motion%20backbone%20and%20integrates%20absolute%20updates%20from%20GNSS%20outdoors%20and%20UWB%20indoors.%20To%20enhance%20transition%20robustness%20and%20mitigate%20urban%20GNSS%20degradation%2C%20we%20introduce%20a%20lightweight%20map-based%20feasibility%20constraint%20derived%20from%20OpenStreetMap%20building%20footprints%2C%20treating%20most%20building%20interiors%20as%20non-navigable%20while%20allowing%20motion%20inside%20a%20designated%20UWB-instrumented%20building.%20The%20framework%20is%20implemented%20in%20ROS%202%20and%20runs%20in%20real%20time%20on%20a%20wearable%20platform%2C%20with%20visualization%20in%20Foxglove.%20We%20evaluate%20three%20scenarios%3A%20indoor%20%28UWB%2BPDR%29%2C%20outdoor%20%28GNSS%2BPDR%29%2C%20and%20seamless%20outdoor-indoor%20%28GNSS%2BUWB%2BPDR%29.%20Results%20show%20that%20the%20ESKF%20provides%20the%20most%20consistent%20overall%20performance%20in%20our%20implementation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeamless%2520Outdoor-Indoor%2520Pedestrian%2520Positioning%2520System%2520with%2520GNSS/UWB/IMU%2520Fusion%253A%2520A%2520Comparison%2520of%2520EKF%252C%2520FGO%252C%2520and%2520PF%26entry.906535625%3DJiaqiang%2520Zhang%2520and%2520Xianjia%2520Yu%2520and%2520Sier%2520Ha%2520and%2520Paola%2520Torrico%2520Moron%2520and%2520Sahar%2520Salimpour%2520and%2520Farhad%2520Kerama%2520and%2520Haizhou%2520Zhang%2520and%2520Tomi%2520Westerlund%26entry.1292438233%3DAccurate%2520and%2520continuous%2520pedestrian%2520positioning%2520across%2520outdoor-indoor%2520environments%2520remains%2520challenging%2520because%2520GNSS%252C%2520UWB%252C%2520and%2520inertial%2520PDR%2520are%2520complementary%2520yet%2520individually%2520fragile%2520under%2520signal%2520blockage%252C%2520multipath%252C%2520and%2520drift.%2520This%2520paper%2520presents%2520a%2520unified%2520GNSS/UWB/IMU%2520fusion%2520framework%2520for%2520seamless%2520pedestrian%2520localization%2520and%2520provides%2520a%2520controlled%2520comparison%2520of%2520three%2520probabilistic%2520back-ends%253A%2520an%2520error-state%2520extended%2520Kalman%2520filter%252C%2520sliding-window%2520factor%2520graph%2520optimization%252C%2520and%2520a%2520particle%2520filter.%2520The%2520system%2520uses%2520chest-mounted%2520IMU-based%2520PDR%2520as%2520the%2520motion%2520backbone%2520and%2520integrates%2520absolute%2520updates%2520from%2520GNSS%2520outdoors%2520and%2520UWB%2520indoors.%2520To%2520enhance%2520transition%2520robustness%2520and%2520mitigate%2520urban%2520GNSS%2520degradation%252C%2520we%2520introduce%2520a%2520lightweight%2520map-based%2520feasibility%2520constraint%2520derived%2520from%2520OpenStreetMap%2520building%2520footprints%252C%2520treating%2520most%2520building%2520interiors%2520as%2520non-navigable%2520while%2520allowing%2520motion%2520inside%2520a%2520designated%2520UWB-instrumented%2520building.%2520The%2520framework%2520is%2520implemented%2520in%2520ROS%25202%2520and%2520runs%2520in%2520real%2520time%2520on%2520a%2520wearable%2520platform%252C%2520with%2520visualization%2520in%2520Foxglove.%2520We%2520evaluate%2520three%2520scenarios%253A%2520indoor%2520%2528UWB%252BPDR%2529%252C%2520outdoor%2520%2528GNSS%252BPDR%2529%252C%2520and%2520seamless%2520outdoor-indoor%2520%2528GNSS%252BUWB%252BPDR%2529.%2520Results%2520show%2520that%2520the%2520ESKF%2520provides%2520the%2520most%2520consistent%2520overall%2520performance%2520in%2520our%2520implementation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seamless%20Outdoor-Indoor%20Pedestrian%20Positioning%20System%20with%20GNSS/UWB/IMU%20Fusion%3A%20A%20Comparison%20of%20EKF%2C%20FGO%2C%20and%20PF&entry.906535625=Jiaqiang%20Zhang%20and%20Xianjia%20Yu%20and%20Sier%20Ha%20and%20Paola%20Torrico%20Moron%20and%20Sahar%20Salimpour%20and%20Farhad%20Kerama%20and%20Haizhou%20Zhang%20and%20Tomi%20Westerlund&entry.1292438233=Accurate%20and%20continuous%20pedestrian%20positioning%20across%20outdoor-indoor%20environments%20remains%20challenging%20because%20GNSS%2C%20UWB%2C%20and%20inertial%20PDR%20are%20complementary%20yet%20individually%20fragile%20under%20signal%20blockage%2C%20multipath%2C%20and%20drift.%20This%20paper%20presents%20a%20unified%20GNSS/UWB/IMU%20fusion%20framework%20for%20seamless%20pedestrian%20localization%20and%20provides%20a%20controlled%20comparison%20of%20three%20probabilistic%20back-ends%3A%20an%20error-state%20extended%20Kalman%20filter%2C%20sliding-window%20factor%20graph%20optimization%2C%20and%20a%20particle%20filter.%20The%20system%20uses%20chest-mounted%20IMU-based%20PDR%20as%20the%20motion%20backbone%20and%20integrates%20absolute%20updates%20from%20GNSS%20outdoors%20and%20UWB%20indoors.%20To%20enhance%20transition%20robustness%20and%20mitigate%20urban%20GNSS%20degradation%2C%20we%20introduce%20a%20lightweight%20map-based%20feasibility%20constraint%20derived%20from%20OpenStreetMap%20building%20footprints%2C%20treating%20most%20building%20interiors%20as%20non-navigable%20while%20allowing%20motion%20inside%20a%20designated%20UWB-instrumented%20building.%20The%20framework%20is%20implemented%20in%20ROS%202%20and%20runs%20in%20real%20time%20on%20a%20wearable%20platform%2C%20with%20visualization%20in%20Foxglove.%20We%20evaluate%20three%20scenarios%3A%20indoor%20%28UWB%2BPDR%29%2C%20outdoor%20%28GNSS%2BPDR%29%2C%20and%20seamless%20outdoor-indoor%20%28GNSS%2BUWB%2BPDR%29.%20Results%20show%20that%20the%20ESKF%20provides%20the%20most%20consistent%20overall%20performance%20in%20our%20implementation.&entry.1838667208=http%3A//arxiv.org/abs/2512.10480v1&entry.124074799=Read"},
{"title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation", "author": "Hao Xing and Kai Zhe Boey and Yuankai Wu and Darius Burschka and Gordon Cheng", "abstract": "Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.", "link": "http://arxiv.org/abs/2507.00752v2", "date": "2025-12-11", "relevancy": 2.2532, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5747}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Graph%20Convolutional%20Network%20with%20Sinusoidal%20Encoding%20for%20Robust%20Human%20Action%20Segmentation&body=Title%3A%20Multi-Modal%20Graph%20Convolutional%20Network%20with%20Sinusoidal%20Encoding%20for%20Robust%20Human%20Action%20Segmentation%0AAuthor%3A%20Hao%20Xing%20and%20Kai%20Zhe%20Boey%20and%20Yuankai%20Wu%20and%20Darius%20Burschka%20and%20Gordon%20Cheng%0AAbstract%3A%20Accurate%20temporal%20segmentation%20of%20human%20actions%20is%20critical%20for%20intelligent%20robots%20in%20collaborative%20settings%2C%20where%20a%20precise%20understanding%20of%20sub-activity%20labels%20and%20their%20temporal%20structure%20is%20essential.%20However%2C%20the%20inherent%20noise%20in%20both%20human%20pose%20estimation%20and%20object%20detection%20often%20leads%20to%20over-segmentation%20errors%2C%20disrupting%20the%20coherence%20of%20action%20sequences.%20To%20address%20this%2C%20we%20propose%20a%20Multi-Modal%20Graph%20Convolutional%20Network%20%28MMGCN%29%20that%20integrates%20low-frame-rate%20%28e.g.%2C%201%20fps%29%20visual%20data%20with%20high-frame-rate%20%28e.g.%2C%2030%20fps%29%20motion%20data%20%28skeleton%20and%20object%20detections%29%20to%20mitigate%20fragmentation.%20Our%20framework%20introduces%20three%20key%20contributions.%20First%2C%20a%20sinusoidal%20encoding%20strategy%20that%20maps%203D%20skeleton%20coordinates%20into%20a%20continuous%20sin-cos%20space%20to%20enhance%20spatial%20representation%20robustness.%20Second%2C%20a%20temporal%20graph%20fusion%20module%20that%20aligns%20multi-modal%20inputs%20with%20differing%20resolutions%20via%20hierarchical%20feature%20aggregation%2C%20Third%2C%20inspired%20by%20the%20smooth%20transitions%20inherent%20to%20human%20actions%2C%20we%20design%20SmoothLabelMix%2C%20a%20data%20augmentation%20technique%20that%20mixes%20input%20sequences%20and%20labels%20to%20generate%20synthetic%20training%20examples%20with%20gradual%20action%20transitions%2C%20enhancing%20temporal%20consistency%20in%20predictions%20and%20reducing%20over-segmentation%20artifacts.%0A%20%20Extensive%20experiments%20on%20the%20Bimanual%20Actions%20Dataset%2C%20a%20public%20benchmark%20for%20human-object%20interaction%20understanding%2C%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20methods%2C%20especially%20in%20action%20segmentation%20accuracy%2C%20achieving%20F1%4010%3A%2094.5%25%20and%20F1%4025%3A%2092.8%25.%0ALink%3A%20http%3A//arxiv.org/abs/2507.00752v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Graph%2520Convolutional%2520Network%2520with%2520Sinusoidal%2520Encoding%2520for%2520Robust%2520Human%2520Action%2520Segmentation%26entry.906535625%3DHao%2520Xing%2520and%2520Kai%2520Zhe%2520Boey%2520and%2520Yuankai%2520Wu%2520and%2520Darius%2520Burschka%2520and%2520Gordon%2520Cheng%26entry.1292438233%3DAccurate%2520temporal%2520segmentation%2520of%2520human%2520actions%2520is%2520critical%2520for%2520intelligent%2520robots%2520in%2520collaborative%2520settings%252C%2520where%2520a%2520precise%2520understanding%2520of%2520sub-activity%2520labels%2520and%2520their%2520temporal%2520structure%2520is%2520essential.%2520However%252C%2520the%2520inherent%2520noise%2520in%2520both%2520human%2520pose%2520estimation%2520and%2520object%2520detection%2520often%2520leads%2520to%2520over-segmentation%2520errors%252C%2520disrupting%2520the%2520coherence%2520of%2520action%2520sequences.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Multi-Modal%2520Graph%2520Convolutional%2520Network%2520%2528MMGCN%2529%2520that%2520integrates%2520low-frame-rate%2520%2528e.g.%252C%25201%2520fps%2529%2520visual%2520data%2520with%2520high-frame-rate%2520%2528e.g.%252C%252030%2520fps%2529%2520motion%2520data%2520%2528skeleton%2520and%2520object%2520detections%2529%2520to%2520mitigate%2520fragmentation.%2520Our%2520framework%2520introduces%2520three%2520key%2520contributions.%2520First%252C%2520a%2520sinusoidal%2520encoding%2520strategy%2520that%2520maps%25203D%2520skeleton%2520coordinates%2520into%2520a%2520continuous%2520sin-cos%2520space%2520to%2520enhance%2520spatial%2520representation%2520robustness.%2520Second%252C%2520a%2520temporal%2520graph%2520fusion%2520module%2520that%2520aligns%2520multi-modal%2520inputs%2520with%2520differing%2520resolutions%2520via%2520hierarchical%2520feature%2520aggregation%252C%2520Third%252C%2520inspired%2520by%2520the%2520smooth%2520transitions%2520inherent%2520to%2520human%2520actions%252C%2520we%2520design%2520SmoothLabelMix%252C%2520a%2520data%2520augmentation%2520technique%2520that%2520mixes%2520input%2520sequences%2520and%2520labels%2520to%2520generate%2520synthetic%2520training%2520examples%2520with%2520gradual%2520action%2520transitions%252C%2520enhancing%2520temporal%2520consistency%2520in%2520predictions%2520and%2520reducing%2520over-segmentation%2520artifacts.%250A%2520%2520Extensive%2520experiments%2520on%2520the%2520Bimanual%2520Actions%2520Dataset%252C%2520a%2520public%2520benchmark%2520for%2520human-object%2520interaction%2520understanding%252C%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%2520methods%252C%2520especially%2520in%2520action%2520segmentation%2520accuracy%252C%2520achieving%2520F1%254010%253A%252094.5%2525%2520and%2520F1%254025%253A%252092.8%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00752v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Graph%20Convolutional%20Network%20with%20Sinusoidal%20Encoding%20for%20Robust%20Human%20Action%20Segmentation&entry.906535625=Hao%20Xing%20and%20Kai%20Zhe%20Boey%20and%20Yuankai%20Wu%20and%20Darius%20Burschka%20and%20Gordon%20Cheng&entry.1292438233=Accurate%20temporal%20segmentation%20of%20human%20actions%20is%20critical%20for%20intelligent%20robots%20in%20collaborative%20settings%2C%20where%20a%20precise%20understanding%20of%20sub-activity%20labels%20and%20their%20temporal%20structure%20is%20essential.%20However%2C%20the%20inherent%20noise%20in%20both%20human%20pose%20estimation%20and%20object%20detection%20often%20leads%20to%20over-segmentation%20errors%2C%20disrupting%20the%20coherence%20of%20action%20sequences.%20To%20address%20this%2C%20we%20propose%20a%20Multi-Modal%20Graph%20Convolutional%20Network%20%28MMGCN%29%20that%20integrates%20low-frame-rate%20%28e.g.%2C%201%20fps%29%20visual%20data%20with%20high-frame-rate%20%28e.g.%2C%2030%20fps%29%20motion%20data%20%28skeleton%20and%20object%20detections%29%20to%20mitigate%20fragmentation.%20Our%20framework%20introduces%20three%20key%20contributions.%20First%2C%20a%20sinusoidal%20encoding%20strategy%20that%20maps%203D%20skeleton%20coordinates%20into%20a%20continuous%20sin-cos%20space%20to%20enhance%20spatial%20representation%20robustness.%20Second%2C%20a%20temporal%20graph%20fusion%20module%20that%20aligns%20multi-modal%20inputs%20with%20differing%20resolutions%20via%20hierarchical%20feature%20aggregation%2C%20Third%2C%20inspired%20by%20the%20smooth%20transitions%20inherent%20to%20human%20actions%2C%20we%20design%20SmoothLabelMix%2C%20a%20data%20augmentation%20technique%20that%20mixes%20input%20sequences%20and%20labels%20to%20generate%20synthetic%20training%20examples%20with%20gradual%20action%20transitions%2C%20enhancing%20temporal%20consistency%20in%20predictions%20and%20reducing%20over-segmentation%20artifacts.%0A%20%20Extensive%20experiments%20on%20the%20Bimanual%20Actions%20Dataset%2C%20a%20public%20benchmark%20for%20human-object%20interaction%20understanding%2C%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20methods%2C%20especially%20in%20action%20segmentation%20accuracy%2C%20achieving%20F1%4010%3A%2094.5%25%20and%20F1%4025%3A%2092.8%25.&entry.1838667208=http%3A//arxiv.org/abs/2507.00752v2&entry.124074799=Read"},
{"title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models", "author": "Zongzhao Li and Xiangzhe Kong and Jiahui Su and Zongyang Ma and Mingze Li and Songyou Li and Yuelin Zhang and Yu Rong and Tingyang Xu and Deli Zhao and Wenbing Huang", "abstract": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.", "link": "http://arxiv.org/abs/2512.10867v1", "date": "2025-12-11", "relevancy": 2.2525, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5746}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Macro%20to%20Micro%3A%20Benchmarking%20Microscopic%20Spatial%20Intelligence%20on%20Molecules%20via%20Vision-Language%20Models&body=Title%3A%20From%20Macro%20to%20Micro%3A%20Benchmarking%20Microscopic%20Spatial%20Intelligence%20on%20Molecules%20via%20Vision-Language%20Models%0AAuthor%3A%20Zongzhao%20Li%20and%20Xiangzhe%20Kong%20and%20Jiahui%20Su%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yuelin%20Zhang%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Deli%20Zhao%20and%20Wenbing%20Huang%0AAbstract%3A%20This%20paper%20introduces%20the%20concept%20of%20Microscopic%20Spatial%20Intelligence%20%28MiSI%29%2C%20the%20capability%20to%20perceive%20and%20reason%20about%20the%20spatial%20relationships%20of%20invisible%20microscopic%20entities%2C%20which%20is%20fundamental%20to%20scientific%20discovery.%20To%20assess%20the%20potential%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20this%20domain%2C%20we%20propose%20a%20systematic%20benchmark%20framework%20MiSI-Bench.%20This%20framework%20features%20over%20163%2C000%20question-answer%20pairs%20and%20587%2C000%20images%20derived%20from%20approximately%204%2C000%20molecular%20structures%2C%20covering%20nine%20complementary%20tasks%20that%20evaluate%20abilities%20ranging%20from%20elementary%20spatial%20transformations%20to%20complex%20relational%20identifications.%20Experimental%20results%20reveal%20that%20current%20state-of-the-art%20VLMs%20perform%20significantly%20below%20human%20level%20on%20this%20benchmark.%20However%2C%20a%20fine-tuned%207B%20model%20demonstrates%20substantial%20potential%2C%20even%20surpassing%20humans%20in%20spatial%20transformation%20tasks%2C%20while%20its%20poor%20performance%20in%20scientifically-grounded%20tasks%20like%20hydrogen%20bond%20recognition%20underscores%20the%20necessity%20of%20integrating%20explicit%20domain%20knowledge%20for%20progress%20toward%20scientific%20AGI.%20The%20datasets%20are%20available%20at%20https%3A//huggingface.co/datasets/zongzhao/MiSI-bench.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Macro%2520to%2520Micro%253A%2520Benchmarking%2520Microscopic%2520Spatial%2520Intelligence%2520on%2520Molecules%2520via%2520Vision-Language%2520Models%26entry.906535625%3DZongzhao%2520Li%2520and%2520Xiangzhe%2520Kong%2520and%2520Jiahui%2520Su%2520and%2520Zongyang%2520Ma%2520and%2520Mingze%2520Li%2520and%2520Songyou%2520Li%2520and%2520Yuelin%2520Zhang%2520and%2520Yu%2520Rong%2520and%2520Tingyang%2520Xu%2520and%2520Deli%2520Zhao%2520and%2520Wenbing%2520Huang%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520concept%2520of%2520Microscopic%2520Spatial%2520Intelligence%2520%2528MiSI%2529%252C%2520the%2520capability%2520to%2520perceive%2520and%2520reason%2520about%2520the%2520spatial%2520relationships%2520of%2520invisible%2520microscopic%2520entities%252C%2520which%2520is%2520fundamental%2520to%2520scientific%2520discovery.%2520To%2520assess%2520the%2520potential%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520in%2520this%2520domain%252C%2520we%2520propose%2520a%2520systematic%2520benchmark%2520framework%2520MiSI-Bench.%2520This%2520framework%2520features%2520over%2520163%252C000%2520question-answer%2520pairs%2520and%2520587%252C000%2520images%2520derived%2520from%2520approximately%25204%252C000%2520molecular%2520structures%252C%2520covering%2520nine%2520complementary%2520tasks%2520that%2520evaluate%2520abilities%2520ranging%2520from%2520elementary%2520spatial%2520transformations%2520to%2520complex%2520relational%2520identifications.%2520Experimental%2520results%2520reveal%2520that%2520current%2520state-of-the-art%2520VLMs%2520perform%2520significantly%2520below%2520human%2520level%2520on%2520this%2520benchmark.%2520However%252C%2520a%2520fine-tuned%25207B%2520model%2520demonstrates%2520substantial%2520potential%252C%2520even%2520surpassing%2520humans%2520in%2520spatial%2520transformation%2520tasks%252C%2520while%2520its%2520poor%2520performance%2520in%2520scientifically-grounded%2520tasks%2520like%2520hydrogen%2520bond%2520recognition%2520underscores%2520the%2520necessity%2520of%2520integrating%2520explicit%2520domain%2520knowledge%2520for%2520progress%2520toward%2520scientific%2520AGI.%2520The%2520datasets%2520are%2520available%2520at%2520https%253A//huggingface.co/datasets/zongzhao/MiSI-bench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Macro%20to%20Micro%3A%20Benchmarking%20Microscopic%20Spatial%20Intelligence%20on%20Molecules%20via%20Vision-Language%20Models&entry.906535625=Zongzhao%20Li%20and%20Xiangzhe%20Kong%20and%20Jiahui%20Su%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yuelin%20Zhang%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Deli%20Zhao%20and%20Wenbing%20Huang&entry.1292438233=This%20paper%20introduces%20the%20concept%20of%20Microscopic%20Spatial%20Intelligence%20%28MiSI%29%2C%20the%20capability%20to%20perceive%20and%20reason%20about%20the%20spatial%20relationships%20of%20invisible%20microscopic%20entities%2C%20which%20is%20fundamental%20to%20scientific%20discovery.%20To%20assess%20the%20potential%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20this%20domain%2C%20we%20propose%20a%20systematic%20benchmark%20framework%20MiSI-Bench.%20This%20framework%20features%20over%20163%2C000%20question-answer%20pairs%20and%20587%2C000%20images%20derived%20from%20approximately%204%2C000%20molecular%20structures%2C%20covering%20nine%20complementary%20tasks%20that%20evaluate%20abilities%20ranging%20from%20elementary%20spatial%20transformations%20to%20complex%20relational%20identifications.%20Experimental%20results%20reveal%20that%20current%20state-of-the-art%20VLMs%20perform%20significantly%20below%20human%20level%20on%20this%20benchmark.%20However%2C%20a%20fine-tuned%207B%20model%20demonstrates%20substantial%20potential%2C%20even%20surpassing%20humans%20in%20spatial%20transformation%20tasks%2C%20while%20its%20poor%20performance%20in%20scientifically-grounded%20tasks%20like%20hydrogen%20bond%20recognition%20underscores%20the%20necessity%20of%20integrating%20explicit%20domain%20knowledge%20for%20progress%20toward%20scientific%20AGI.%20The%20datasets%20are%20available%20at%20https%3A//huggingface.co/datasets/zongzhao/MiSI-bench.&entry.1838667208=http%3A//arxiv.org/abs/2512.10867v1&entry.124074799=Read"},
{"title": "Optimal transport unlocks end-to-end learning for single-molecule localization", "author": "Romain Seailles and Jean-Baptiste Masson and Jean Ponce and Julien Mairal", "abstract": "Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.", "link": "http://arxiv.org/abs/2512.10683v1", "date": "2025-12-11", "relevancy": 2.2505, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5975}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5688}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20transport%20unlocks%20end-to-end%20learning%20for%20single-molecule%20localization&body=Title%3A%20Optimal%20transport%20unlocks%20end-to-end%20learning%20for%20single-molecule%20localization%0AAuthor%3A%20Romain%20Seailles%20and%20Jean-Baptiste%20Masson%20and%20Jean%20Ponce%20and%20Julien%20Mairal%0AAbstract%3A%20Single-molecule%20localization%20microscopy%20%28SMLM%29%20allows%20reconstructing%20biology-relevant%20structures%20beyond%20the%20diffraction%20limit%20by%20detecting%20and%20localizing%20individual%20fluorophores%20--%20fluorescent%20molecules%20stained%20onto%20the%20observed%20specimen%20--%20over%20time%20to%20reconstruct%20super-resolved%20images.%20Currently%2C%20efficient%20SMLM%20requires%20non-overlapping%20emitting%20fluorophores%2C%20leading%20to%20long%20acquisition%20times%20that%20hinders%20live-cell%20imaging.%20Recent%20deep-learning%20approaches%20can%20handle%20denser%20emissions%2C%20but%20they%20rely%20on%20variants%20of%20non-maximum%20suppression%20%28NMS%29%20layers%2C%20which%20are%20unfortunately%20non-differentiable%20and%20may%20discard%20true%20positives%20with%20their%20local%20fusion%20strategy.%20In%20this%20presentation%2C%20we%20reformulate%20the%20SMLM%20training%20objective%20as%20a%20set-matching%20problem%2C%20deriving%20an%20optimal-transport%20loss%20that%20eliminates%20the%20need%20for%20NMS%20during%20inference%20and%20enables%20end-to-end%20training.%20Additionally%2C%20we%20propose%20an%20iterative%20neural%20network%20that%20integrates%20knowledge%20of%20the%20microscope%27s%20optical%20system%20inside%20our%20model.%20Experiments%20on%20synthetic%20benchmarks%20and%20real%20biological%20data%20show%20that%20both%20our%20new%20loss%20function%20and%20architecture%20surpass%20the%20state%20of%20the%20art%20at%20moderate%20and%20high%20emitter%20densities.%20Code%20is%20available%20at%20https%3A//github.com/RSLLES/SHOT.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520transport%2520unlocks%2520end-to-end%2520learning%2520for%2520single-molecule%2520localization%26entry.906535625%3DRomain%2520Seailles%2520and%2520Jean-Baptiste%2520Masson%2520and%2520Jean%2520Ponce%2520and%2520Julien%2520Mairal%26entry.1292438233%3DSingle-molecule%2520localization%2520microscopy%2520%2528SMLM%2529%2520allows%2520reconstructing%2520biology-relevant%2520structures%2520beyond%2520the%2520diffraction%2520limit%2520by%2520detecting%2520and%2520localizing%2520individual%2520fluorophores%2520--%2520fluorescent%2520molecules%2520stained%2520onto%2520the%2520observed%2520specimen%2520--%2520over%2520time%2520to%2520reconstruct%2520super-resolved%2520images.%2520Currently%252C%2520efficient%2520SMLM%2520requires%2520non-overlapping%2520emitting%2520fluorophores%252C%2520leading%2520to%2520long%2520acquisition%2520times%2520that%2520hinders%2520live-cell%2520imaging.%2520Recent%2520deep-learning%2520approaches%2520can%2520handle%2520denser%2520emissions%252C%2520but%2520they%2520rely%2520on%2520variants%2520of%2520non-maximum%2520suppression%2520%2528NMS%2529%2520layers%252C%2520which%2520are%2520unfortunately%2520non-differentiable%2520and%2520may%2520discard%2520true%2520positives%2520with%2520their%2520local%2520fusion%2520strategy.%2520In%2520this%2520presentation%252C%2520we%2520reformulate%2520the%2520SMLM%2520training%2520objective%2520as%2520a%2520set-matching%2520problem%252C%2520deriving%2520an%2520optimal-transport%2520loss%2520that%2520eliminates%2520the%2520need%2520for%2520NMS%2520during%2520inference%2520and%2520enables%2520end-to-end%2520training.%2520Additionally%252C%2520we%2520propose%2520an%2520iterative%2520neural%2520network%2520that%2520integrates%2520knowledge%2520of%2520the%2520microscope%2527s%2520optical%2520system%2520inside%2520our%2520model.%2520Experiments%2520on%2520synthetic%2520benchmarks%2520and%2520real%2520biological%2520data%2520show%2520that%2520both%2520our%2520new%2520loss%2520function%2520and%2520architecture%2520surpass%2520the%2520state%2520of%2520the%2520art%2520at%2520moderate%2520and%2520high%2520emitter%2520densities.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/RSLLES/SHOT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20transport%20unlocks%20end-to-end%20learning%20for%20single-molecule%20localization&entry.906535625=Romain%20Seailles%20and%20Jean-Baptiste%20Masson%20and%20Jean%20Ponce%20and%20Julien%20Mairal&entry.1292438233=Single-molecule%20localization%20microscopy%20%28SMLM%29%20allows%20reconstructing%20biology-relevant%20structures%20beyond%20the%20diffraction%20limit%20by%20detecting%20and%20localizing%20individual%20fluorophores%20--%20fluorescent%20molecules%20stained%20onto%20the%20observed%20specimen%20--%20over%20time%20to%20reconstruct%20super-resolved%20images.%20Currently%2C%20efficient%20SMLM%20requires%20non-overlapping%20emitting%20fluorophores%2C%20leading%20to%20long%20acquisition%20times%20that%20hinders%20live-cell%20imaging.%20Recent%20deep-learning%20approaches%20can%20handle%20denser%20emissions%2C%20but%20they%20rely%20on%20variants%20of%20non-maximum%20suppression%20%28NMS%29%20layers%2C%20which%20are%20unfortunately%20non-differentiable%20and%20may%20discard%20true%20positives%20with%20their%20local%20fusion%20strategy.%20In%20this%20presentation%2C%20we%20reformulate%20the%20SMLM%20training%20objective%20as%20a%20set-matching%20problem%2C%20deriving%20an%20optimal-transport%20loss%20that%20eliminates%20the%20need%20for%20NMS%20during%20inference%20and%20enables%20end-to-end%20training.%20Additionally%2C%20we%20propose%20an%20iterative%20neural%20network%20that%20integrates%20knowledge%20of%20the%20microscope%27s%20optical%20system%20inside%20our%20model.%20Experiments%20on%20synthetic%20benchmarks%20and%20real%20biological%20data%20show%20that%20both%20our%20new%20loss%20function%20and%20architecture%20surpass%20the%20state%20of%20the%20art%20at%20moderate%20and%20high%20emitter%20densities.%20Code%20is%20available%20at%20https%3A//github.com/RSLLES/SHOT.&entry.1838667208=http%3A//arxiv.org/abs/2512.10683v1&entry.124074799=Read"},
{"title": "Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality", "author": "Lingjing Kong and Shaoan Xie and Guangyi Chen and Yuewen Sun and Xiangchen Song and Eric P. Xing and Kun Zhang", "abstract": "Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.", "link": "http://arxiv.org/abs/2512.10720v1", "date": "2025-12-11", "relevancy": 2.2168, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5769}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5629}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Black%20Box%3A%20Identifiable%20Interpretation%20and%20Control%20in%20Generative%20Models%20via%20Causal%20Minimality&body=Title%3A%20Beyond%20the%20Black%20Box%3A%20Identifiable%20Interpretation%20and%20Control%20in%20Generative%20Models%20via%20Causal%20Minimality%0AAuthor%3A%20Lingjing%20Kong%20and%20Shaoan%20Xie%20and%20Guangyi%20Chen%20and%20Yuewen%20Sun%20and%20Xiangchen%20Song%20and%20Eric%20P.%20Xing%20and%20Kun%20Zhang%0AAbstract%3A%20Deep%20generative%20models%2C%20while%20revolutionizing%20fields%20like%20image%20and%20text%20generation%2C%20largely%20operate%20as%20opaque%20black%20boxes%2C%20hindering%20human%20understanding%2C%20control%2C%20and%20alignment.%20While%20methods%20like%20sparse%20autoencoders%20%28SAEs%29%20show%20remarkable%20empirical%20success%2C%20they%20often%20lack%20theoretical%20guarantees%2C%20risking%20subjective%20insights.%20Our%20primary%20objective%20is%20to%20establish%20a%20principled%20foundation%20for%20interpretable%20generative%20models.%20We%20demonstrate%20that%20the%20principle%20of%20causal%20minimality%20--%20favoring%20the%20simplest%20causal%20explanation%20--%20can%20endow%20the%20latent%20representations%20of%20diffusion%20vision%20and%20autoregressive%20language%20models%20with%20clear%20causal%20interpretation%20and%20robust%2C%20component-wise%20identifiable%20control.%20We%20introduce%20a%20novel%20theoretical%20framework%20for%20hierarchical%20selection%20models%2C%20where%20higher-level%20concepts%20emerge%20from%20the%20constrained%20composition%20of%20lower-level%20variables%2C%20better%20capturing%20the%20complex%20dependencies%20in%20data%20generation.%20Under%20theoretically%20derived%20minimality%20conditions%20%28manifesting%20as%20sparsity%20or%20compression%20constraints%29%2C%20we%20show%20that%20learned%20representations%20can%20be%20equivalent%20to%20the%20true%20latent%20variables%20of%20the%20data-generating%20process.%20Empirically%2C%20applying%20these%20constraints%20to%20leading%20generative%20models%20allows%20us%20to%20extract%20their%20innate%20hierarchical%20concept%20graphs%2C%20offering%20fresh%20insights%20into%20their%20internal%20knowledge%20organization.%20Furthermore%2C%20these%20causally%20grounded%20concepts%20serve%20as%20levers%20for%20fine-grained%20model%20steering%2C%20paving%20the%20way%20for%20transparent%2C%20reliable%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Black%2520Box%253A%2520Identifiable%2520Interpretation%2520and%2520Control%2520in%2520Generative%2520Models%2520via%2520Causal%2520Minimality%26entry.906535625%3DLingjing%2520Kong%2520and%2520Shaoan%2520Xie%2520and%2520Guangyi%2520Chen%2520and%2520Yuewen%2520Sun%2520and%2520Xiangchen%2520Song%2520and%2520Eric%2520P.%2520Xing%2520and%2520Kun%2520Zhang%26entry.1292438233%3DDeep%2520generative%2520models%252C%2520while%2520revolutionizing%2520fields%2520like%2520image%2520and%2520text%2520generation%252C%2520largely%2520operate%2520as%2520opaque%2520black%2520boxes%252C%2520hindering%2520human%2520understanding%252C%2520control%252C%2520and%2520alignment.%2520While%2520methods%2520like%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520show%2520remarkable%2520empirical%2520success%252C%2520they%2520often%2520lack%2520theoretical%2520guarantees%252C%2520risking%2520subjective%2520insights.%2520Our%2520primary%2520objective%2520is%2520to%2520establish%2520a%2520principled%2520foundation%2520for%2520interpretable%2520generative%2520models.%2520We%2520demonstrate%2520that%2520the%2520principle%2520of%2520causal%2520minimality%2520--%2520favoring%2520the%2520simplest%2520causal%2520explanation%2520--%2520can%2520endow%2520the%2520latent%2520representations%2520of%2520diffusion%2520vision%2520and%2520autoregressive%2520language%2520models%2520with%2520clear%2520causal%2520interpretation%2520and%2520robust%252C%2520component-wise%2520identifiable%2520control.%2520We%2520introduce%2520a%2520novel%2520theoretical%2520framework%2520for%2520hierarchical%2520selection%2520models%252C%2520where%2520higher-level%2520concepts%2520emerge%2520from%2520the%2520constrained%2520composition%2520of%2520lower-level%2520variables%252C%2520better%2520capturing%2520the%2520complex%2520dependencies%2520in%2520data%2520generation.%2520Under%2520theoretically%2520derived%2520minimality%2520conditions%2520%2528manifesting%2520as%2520sparsity%2520or%2520compression%2520constraints%2529%252C%2520we%2520show%2520that%2520learned%2520representations%2520can%2520be%2520equivalent%2520to%2520the%2520true%2520latent%2520variables%2520of%2520the%2520data-generating%2520process.%2520Empirically%252C%2520applying%2520these%2520constraints%2520to%2520leading%2520generative%2520models%2520allows%2520us%2520to%2520extract%2520their%2520innate%2520hierarchical%2520concept%2520graphs%252C%2520offering%2520fresh%2520insights%2520into%2520their%2520internal%2520knowledge%2520organization.%2520Furthermore%252C%2520these%2520causally%2520grounded%2520concepts%2520serve%2520as%2520levers%2520for%2520fine-grained%2520model%2520steering%252C%2520paving%2520the%2520way%2520for%2520transparent%252C%2520reliable%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Black%20Box%3A%20Identifiable%20Interpretation%20and%20Control%20in%20Generative%20Models%20via%20Causal%20Minimality&entry.906535625=Lingjing%20Kong%20and%20Shaoan%20Xie%20and%20Guangyi%20Chen%20and%20Yuewen%20Sun%20and%20Xiangchen%20Song%20and%20Eric%20P.%20Xing%20and%20Kun%20Zhang&entry.1292438233=Deep%20generative%20models%2C%20while%20revolutionizing%20fields%20like%20image%20and%20text%20generation%2C%20largely%20operate%20as%20opaque%20black%20boxes%2C%20hindering%20human%20understanding%2C%20control%2C%20and%20alignment.%20While%20methods%20like%20sparse%20autoencoders%20%28SAEs%29%20show%20remarkable%20empirical%20success%2C%20they%20often%20lack%20theoretical%20guarantees%2C%20risking%20subjective%20insights.%20Our%20primary%20objective%20is%20to%20establish%20a%20principled%20foundation%20for%20interpretable%20generative%20models.%20We%20demonstrate%20that%20the%20principle%20of%20causal%20minimality%20--%20favoring%20the%20simplest%20causal%20explanation%20--%20can%20endow%20the%20latent%20representations%20of%20diffusion%20vision%20and%20autoregressive%20language%20models%20with%20clear%20causal%20interpretation%20and%20robust%2C%20component-wise%20identifiable%20control.%20We%20introduce%20a%20novel%20theoretical%20framework%20for%20hierarchical%20selection%20models%2C%20where%20higher-level%20concepts%20emerge%20from%20the%20constrained%20composition%20of%20lower-level%20variables%2C%20better%20capturing%20the%20complex%20dependencies%20in%20data%20generation.%20Under%20theoretically%20derived%20minimality%20conditions%20%28manifesting%20as%20sparsity%20or%20compression%20constraints%29%2C%20we%20show%20that%20learned%20representations%20can%20be%20equivalent%20to%20the%20true%20latent%20variables%20of%20the%20data-generating%20process.%20Empirically%2C%20applying%20these%20constraints%20to%20leading%20generative%20models%20allows%20us%20to%20extract%20their%20innate%20hierarchical%20concept%20graphs%2C%20offering%20fresh%20insights%20into%20their%20internal%20knowledge%20organization.%20Furthermore%2C%20these%20causally%20grounded%20concepts%20serve%20as%20levers%20for%20fine-grained%20model%20steering%2C%20paving%20the%20way%20for%20transparent%2C%20reliable%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.10720v1&entry.124074799=Read"},
{"title": "XDen-1K: A Density Field Dataset of Real-World Objects", "author": "Jingxuan Zhang and Tianqi Yu and Yatu Zhang and Jinze Wu and Kaixin Yao and Jingyang Liu and Yuyao Zhang and Jiayuan Gu and Jingyi Yu", "abstract": "A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.", "link": "http://arxiv.org/abs/2512.10668v1", "date": "2025-12-11", "relevancy": 2.2092, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XDen-1K%3A%20A%20Density%20Field%20Dataset%20of%20Real-World%20Objects&body=Title%3A%20XDen-1K%3A%20A%20Density%20Field%20Dataset%20of%20Real-World%20Objects%0AAuthor%3A%20Jingxuan%20Zhang%20and%20Tianqi%20Yu%20and%20Yatu%20Zhang%20and%20Jinze%20Wu%20and%20Kaixin%20Yao%20and%20Jingyang%20Liu%20and%20Yuyao%20Zhang%20and%20Jiayuan%20Gu%20and%20Jingyi%20Yu%0AAbstract%3A%20A%20deep%20understanding%20of%20the%20physical%20world%20is%20a%20central%20goal%20for%20embodied%20AI%20and%20realistic%20simulation.%20While%20current%20models%20excel%20at%20capturing%20an%20object%27s%20surface%20geometry%20and%20appearance%2C%20they%20largely%20neglect%20its%20internal%20physical%20properties.%20This%20omission%20is%20critical%2C%20as%20properties%20like%20volumetric%20density%20are%20fundamental%20for%20predicting%20an%20object%27s%20center%20of%20mass%2C%20stability%2C%20and%20interaction%20dynamics%20in%20applications%20ranging%20from%20robotic%20manipulation%20to%20physical%20simulation.%20The%20primary%20bottleneck%20has%20been%20the%20absence%20of%20large-scale%2C%20real-world%20data.%20To%20bridge%20this%20gap%2C%20we%20introduce%20XDen-1K%2C%20the%20first%20large-scale%2C%20multi-modal%20dataset%20designed%20for%20real-world%20physical%20property%20estimation%2C%20with%20a%20particular%20focus%20on%20volumetric%20density.%20The%20core%20of%20this%20dataset%20consists%20of%201%2C000%20real-world%20objects%20across%20148%20categories%2C%20for%20which%20we%20provide%20comprehensive%20multi-modal%20data%2C%20including%20a%20high-resolution%203D%20geometric%20model%20with%20part-level%20annotations%20and%20a%20corresponding%20set%20of%20real-world%20biplanar%20X-ray%20scans.%20Building%20upon%20this%20data%2C%20we%20introduce%20a%20novel%20optimization%20framework%20that%20recovers%20a%20high-fidelity%20volumetric%20density%20field%20of%20each%20object%20from%20its%20sparse%20X-ray%20views.%20To%20demonstrate%20its%20practical%20value%2C%20we%20add%20X-ray%20images%20as%20a%20conditioning%20signal%20to%20an%20existing%20segmentation%20network%20and%20perform%20volumetric%20segmentation.%20Furthermore%2C%20we%20conduct%20experiments%20on%20downstream%20robotics%20tasks.%20The%20results%20show%20that%20leveraging%20the%20dataset%20can%20effectively%20improve%20the%20accuracy%20of%20center-of-mass%20estimation%20and%20the%20success%20rate%20of%20robotic%20manipulation.%20We%20believe%20XDen-1K%20will%20serve%20as%20a%20foundational%20resource%20and%20a%20challenging%20new%20benchmark%2C%20catalyzing%20future%20research%20in%20physically%20grounded%20visual%20inference%20and%20embodied%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXDen-1K%253A%2520A%2520Density%2520Field%2520Dataset%2520of%2520Real-World%2520Objects%26entry.906535625%3DJingxuan%2520Zhang%2520and%2520Tianqi%2520Yu%2520and%2520Yatu%2520Zhang%2520and%2520Jinze%2520Wu%2520and%2520Kaixin%2520Yao%2520and%2520Jingyang%2520Liu%2520and%2520Yuyao%2520Zhang%2520and%2520Jiayuan%2520Gu%2520and%2520Jingyi%2520Yu%26entry.1292438233%3DA%2520deep%2520understanding%2520of%2520the%2520physical%2520world%2520is%2520a%2520central%2520goal%2520for%2520embodied%2520AI%2520and%2520realistic%2520simulation.%2520While%2520current%2520models%2520excel%2520at%2520capturing%2520an%2520object%2527s%2520surface%2520geometry%2520and%2520appearance%252C%2520they%2520largely%2520neglect%2520its%2520internal%2520physical%2520properties.%2520This%2520omission%2520is%2520critical%252C%2520as%2520properties%2520like%2520volumetric%2520density%2520are%2520fundamental%2520for%2520predicting%2520an%2520object%2527s%2520center%2520of%2520mass%252C%2520stability%252C%2520and%2520interaction%2520dynamics%2520in%2520applications%2520ranging%2520from%2520robotic%2520manipulation%2520to%2520physical%2520simulation.%2520The%2520primary%2520bottleneck%2520has%2520been%2520the%2520absence%2520of%2520large-scale%252C%2520real-world%2520data.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520XDen-1K%252C%2520the%2520first%2520large-scale%252C%2520multi-modal%2520dataset%2520designed%2520for%2520real-world%2520physical%2520property%2520estimation%252C%2520with%2520a%2520particular%2520focus%2520on%2520volumetric%2520density.%2520The%2520core%2520of%2520this%2520dataset%2520consists%2520of%25201%252C000%2520real-world%2520objects%2520across%2520148%2520categories%252C%2520for%2520which%2520we%2520provide%2520comprehensive%2520multi-modal%2520data%252C%2520including%2520a%2520high-resolution%25203D%2520geometric%2520model%2520with%2520part-level%2520annotations%2520and%2520a%2520corresponding%2520set%2520of%2520real-world%2520biplanar%2520X-ray%2520scans.%2520Building%2520upon%2520this%2520data%252C%2520we%2520introduce%2520a%2520novel%2520optimization%2520framework%2520that%2520recovers%2520a%2520high-fidelity%2520volumetric%2520density%2520field%2520of%2520each%2520object%2520from%2520its%2520sparse%2520X-ray%2520views.%2520To%2520demonstrate%2520its%2520practical%2520value%252C%2520we%2520add%2520X-ray%2520images%2520as%2520a%2520conditioning%2520signal%2520to%2520an%2520existing%2520segmentation%2520network%2520and%2520perform%2520volumetric%2520segmentation.%2520Furthermore%252C%2520we%2520conduct%2520experiments%2520on%2520downstream%2520robotics%2520tasks.%2520The%2520results%2520show%2520that%2520leveraging%2520the%2520dataset%2520can%2520effectively%2520improve%2520the%2520accuracy%2520of%2520center-of-mass%2520estimation%2520and%2520the%2520success%2520rate%2520of%2520robotic%2520manipulation.%2520We%2520believe%2520XDen-1K%2520will%2520serve%2520as%2520a%2520foundational%2520resource%2520and%2520a%2520challenging%2520new%2520benchmark%252C%2520catalyzing%2520future%2520research%2520in%2520physically%2520grounded%2520visual%2520inference%2520and%2520embodied%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XDen-1K%3A%20A%20Density%20Field%20Dataset%20of%20Real-World%20Objects&entry.906535625=Jingxuan%20Zhang%20and%20Tianqi%20Yu%20and%20Yatu%20Zhang%20and%20Jinze%20Wu%20and%20Kaixin%20Yao%20and%20Jingyang%20Liu%20and%20Yuyao%20Zhang%20and%20Jiayuan%20Gu%20and%20Jingyi%20Yu&entry.1292438233=A%20deep%20understanding%20of%20the%20physical%20world%20is%20a%20central%20goal%20for%20embodied%20AI%20and%20realistic%20simulation.%20While%20current%20models%20excel%20at%20capturing%20an%20object%27s%20surface%20geometry%20and%20appearance%2C%20they%20largely%20neglect%20its%20internal%20physical%20properties.%20This%20omission%20is%20critical%2C%20as%20properties%20like%20volumetric%20density%20are%20fundamental%20for%20predicting%20an%20object%27s%20center%20of%20mass%2C%20stability%2C%20and%20interaction%20dynamics%20in%20applications%20ranging%20from%20robotic%20manipulation%20to%20physical%20simulation.%20The%20primary%20bottleneck%20has%20been%20the%20absence%20of%20large-scale%2C%20real-world%20data.%20To%20bridge%20this%20gap%2C%20we%20introduce%20XDen-1K%2C%20the%20first%20large-scale%2C%20multi-modal%20dataset%20designed%20for%20real-world%20physical%20property%20estimation%2C%20with%20a%20particular%20focus%20on%20volumetric%20density.%20The%20core%20of%20this%20dataset%20consists%20of%201%2C000%20real-world%20objects%20across%20148%20categories%2C%20for%20which%20we%20provide%20comprehensive%20multi-modal%20data%2C%20including%20a%20high-resolution%203D%20geometric%20model%20with%20part-level%20annotations%20and%20a%20corresponding%20set%20of%20real-world%20biplanar%20X-ray%20scans.%20Building%20upon%20this%20data%2C%20we%20introduce%20a%20novel%20optimization%20framework%20that%20recovers%20a%20high-fidelity%20volumetric%20density%20field%20of%20each%20object%20from%20its%20sparse%20X-ray%20views.%20To%20demonstrate%20its%20practical%20value%2C%20we%20add%20X-ray%20images%20as%20a%20conditioning%20signal%20to%20an%20existing%20segmentation%20network%20and%20perform%20volumetric%20segmentation.%20Furthermore%2C%20we%20conduct%20experiments%20on%20downstream%20robotics%20tasks.%20The%20results%20show%20that%20leveraging%20the%20dataset%20can%20effectively%20improve%20the%20accuracy%20of%20center-of-mass%20estimation%20and%20the%20success%20rate%20of%20robotic%20manipulation.%20We%20believe%20XDen-1K%20will%20serve%20as%20a%20foundational%20resource%20and%20a%20challenging%20new%20benchmark%2C%20catalyzing%20future%20research%20in%20physically%20grounded%20visual%20inference%20and%20embodied%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2512.10668v1&entry.124074799=Read"},
{"title": "IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation", "author": "Yuan-Ming Li and Qize Yang and Nan Lei and Shenghao Fu and Ling-An Zeng and Jian-Fang Hu and Xihan Wei and Wei-Shi Zheng", "abstract": "Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.", "link": "http://arxiv.org/abs/2512.10730v1", "date": "2025-12-11", "relevancy": 2.2045, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5571}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5522}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRG-MotionLLM%3A%20Interleaving%20Motion%20Generation%2C%20Assessment%20and%20Refinement%20for%20Text-to-Motion%20Generation&body=Title%3A%20IRG-MotionLLM%3A%20Interleaving%20Motion%20Generation%2C%20Assessment%20and%20Refinement%20for%20Text-to-Motion%20Generation%0AAuthor%3A%20Yuan-Ming%20Li%20and%20Qize%20Yang%20and%20Nan%20Lei%20and%20Shenghao%20Fu%20and%20Ling-An%20Zeng%20and%20Jian-Fang%20Hu%20and%20Xihan%20Wei%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20Recent%20advances%20in%20motion-aware%20large%20language%20models%20have%20shown%20remarkable%20promise%20for%20unifying%20motion%20understanding%20and%20generation%20tasks.%20However%2C%20these%20models%20typically%20treat%20understanding%20and%20generation%20separately%2C%20limiting%20the%20mutual%20benefits%20that%20could%20arise%20from%20interactive%20feedback%20between%20tasks.%20In%20this%20work%2C%20we%20reveal%20that%20motion%20assessment%20and%20refinement%20tasks%20act%20as%20crucial%20bridges%20to%20enable%20bidirectional%20knowledge%20flow%20between%20understanding%20and%20generation.%20Leveraging%20this%20insight%2C%20we%20propose%20Interleaved%20Reasoning%20for%20Motion%20Generation%20%28IRMoGen%29%2C%20a%20novel%20paradigm%20that%20tightly%20couples%20motion%20generation%20with%20assessment%20and%20refinement%20through%20iterative%20text-motion%20dialogue.%20To%20realize%20this%2C%20we%20introduce%20IRG-MotionLLM%2C%20the%20first%20model%20that%20seamlessly%20interleaves%20motion%20generation%2C%20assessment%2C%20and%20refinement%20to%20improve%20generation%20performance.%20IRG-MotionLLM%20is%20developed%20progressively%20with%20a%20novel%20three-stage%20training%20scheme%2C%20initializing%20and%20subsequently%20enhancing%20native%20IRMoGen%20capabilities.%20To%20facilitate%20this%20development%2C%20we%20construct%20an%20automated%20data%20engine%20to%20synthesize%20interleaved%20reasoning%20annotations%20from%20existing%20text-motion%20datasets.%20Extensive%20experiments%20demonstrate%20that%3A%20%28i%29%20Assessment%20and%20refinement%20tasks%20significantly%20improve%20text-motion%20alignment%3B%20%28ii%29%20Interleaving%20motion%20generation%2C%20assessment%2C%20and%20refinement%20steps%20yields%20consistent%20performance%20gains%20across%20training%20stages%3B%20and%20%28iii%29%20IRG-MotionLLM%20clearly%20outperforms%20the%20baseline%20model%20and%20achieves%20advanced%20performance%20on%20standard%20text-to-motion%20generation%20benchmarks.%20Cross-evaluator%20testing%20further%20validates%20its%20effectiveness.%20Code%20%26%20Data%3A%20https%3A//github.com/HumanMLLM/IRG-MotionLLM/tree/main.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRG-MotionLLM%253A%2520Interleaving%2520Motion%2520Generation%252C%2520Assessment%2520and%2520Refinement%2520for%2520Text-to-Motion%2520Generation%26entry.906535625%3DYuan-Ming%2520Li%2520and%2520Qize%2520Yang%2520and%2520Nan%2520Lei%2520and%2520Shenghao%2520Fu%2520and%2520Ling-An%2520Zeng%2520and%2520Jian-Fang%2520Hu%2520and%2520Xihan%2520Wei%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3DRecent%2520advances%2520in%2520motion-aware%2520large%2520language%2520models%2520have%2520shown%2520remarkable%2520promise%2520for%2520unifying%2520motion%2520understanding%2520and%2520generation%2520tasks.%2520However%252C%2520these%2520models%2520typically%2520treat%2520understanding%2520and%2520generation%2520separately%252C%2520limiting%2520the%2520mutual%2520benefits%2520that%2520could%2520arise%2520from%2520interactive%2520feedback%2520between%2520tasks.%2520In%2520this%2520work%252C%2520we%2520reveal%2520that%2520motion%2520assessment%2520and%2520refinement%2520tasks%2520act%2520as%2520crucial%2520bridges%2520to%2520enable%2520bidirectional%2520knowledge%2520flow%2520between%2520understanding%2520and%2520generation.%2520Leveraging%2520this%2520insight%252C%2520we%2520propose%2520Interleaved%2520Reasoning%2520for%2520Motion%2520Generation%2520%2528IRMoGen%2529%252C%2520a%2520novel%2520paradigm%2520that%2520tightly%2520couples%2520motion%2520generation%2520with%2520assessment%2520and%2520refinement%2520through%2520iterative%2520text-motion%2520dialogue.%2520To%2520realize%2520this%252C%2520we%2520introduce%2520IRG-MotionLLM%252C%2520the%2520first%2520model%2520that%2520seamlessly%2520interleaves%2520motion%2520generation%252C%2520assessment%252C%2520and%2520refinement%2520to%2520improve%2520generation%2520performance.%2520IRG-MotionLLM%2520is%2520developed%2520progressively%2520with%2520a%2520novel%2520three-stage%2520training%2520scheme%252C%2520initializing%2520and%2520subsequently%2520enhancing%2520native%2520IRMoGen%2520capabilities.%2520To%2520facilitate%2520this%2520development%252C%2520we%2520construct%2520an%2520automated%2520data%2520engine%2520to%2520synthesize%2520interleaved%2520reasoning%2520annotations%2520from%2520existing%2520text-motion%2520datasets.%2520Extensive%2520experiments%2520demonstrate%2520that%253A%2520%2528i%2529%2520Assessment%2520and%2520refinement%2520tasks%2520significantly%2520improve%2520text-motion%2520alignment%253B%2520%2528ii%2529%2520Interleaving%2520motion%2520generation%252C%2520assessment%252C%2520and%2520refinement%2520steps%2520yields%2520consistent%2520performance%2520gains%2520across%2520training%2520stages%253B%2520and%2520%2528iii%2529%2520IRG-MotionLLM%2520clearly%2520outperforms%2520the%2520baseline%2520model%2520and%2520achieves%2520advanced%2520performance%2520on%2520standard%2520text-to-motion%2520generation%2520benchmarks.%2520Cross-evaluator%2520testing%2520further%2520validates%2520its%2520effectiveness.%2520Code%2520%2526%2520Data%253A%2520https%253A//github.com/HumanMLLM/IRG-MotionLLM/tree/main.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRG-MotionLLM%3A%20Interleaving%20Motion%20Generation%2C%20Assessment%20and%20Refinement%20for%20Text-to-Motion%20Generation&entry.906535625=Yuan-Ming%20Li%20and%20Qize%20Yang%20and%20Nan%20Lei%20and%20Shenghao%20Fu%20and%20Ling-An%20Zeng%20and%20Jian-Fang%20Hu%20and%20Xihan%20Wei%20and%20Wei-Shi%20Zheng&entry.1292438233=Recent%20advances%20in%20motion-aware%20large%20language%20models%20have%20shown%20remarkable%20promise%20for%20unifying%20motion%20understanding%20and%20generation%20tasks.%20However%2C%20these%20models%20typically%20treat%20understanding%20and%20generation%20separately%2C%20limiting%20the%20mutual%20benefits%20that%20could%20arise%20from%20interactive%20feedback%20between%20tasks.%20In%20this%20work%2C%20we%20reveal%20that%20motion%20assessment%20and%20refinement%20tasks%20act%20as%20crucial%20bridges%20to%20enable%20bidirectional%20knowledge%20flow%20between%20understanding%20and%20generation.%20Leveraging%20this%20insight%2C%20we%20propose%20Interleaved%20Reasoning%20for%20Motion%20Generation%20%28IRMoGen%29%2C%20a%20novel%20paradigm%20that%20tightly%20couples%20motion%20generation%20with%20assessment%20and%20refinement%20through%20iterative%20text-motion%20dialogue.%20To%20realize%20this%2C%20we%20introduce%20IRG-MotionLLM%2C%20the%20first%20model%20that%20seamlessly%20interleaves%20motion%20generation%2C%20assessment%2C%20and%20refinement%20to%20improve%20generation%20performance.%20IRG-MotionLLM%20is%20developed%20progressively%20with%20a%20novel%20three-stage%20training%20scheme%2C%20initializing%20and%20subsequently%20enhancing%20native%20IRMoGen%20capabilities.%20To%20facilitate%20this%20development%2C%20we%20construct%20an%20automated%20data%20engine%20to%20synthesize%20interleaved%20reasoning%20annotations%20from%20existing%20text-motion%20datasets.%20Extensive%20experiments%20demonstrate%20that%3A%20%28i%29%20Assessment%20and%20refinement%20tasks%20significantly%20improve%20text-motion%20alignment%3B%20%28ii%29%20Interleaving%20motion%20generation%2C%20assessment%2C%20and%20refinement%20steps%20yields%20consistent%20performance%20gains%20across%20training%20stages%3B%20and%20%28iii%29%20IRG-MotionLLM%20clearly%20outperforms%20the%20baseline%20model%20and%20achieves%20advanced%20performance%20on%20standard%20text-to-motion%20generation%20benchmarks.%20Cross-evaluator%20testing%20further%20validates%20its%20effectiveness.%20Code%20%26%20Data%3A%20https%3A//github.com/HumanMLLM/IRG-MotionLLM/tree/main.&entry.1838667208=http%3A//arxiv.org/abs/2512.10730v1&entry.124074799=Read"},
{"title": "Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network", "author": "Khurram Ashfaq and Muhammad Tariq Mahmood", "abstract": "Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.", "link": "http://arxiv.org/abs/2512.10498v1", "date": "2025-12-11", "relevancy": 2.2037, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5598}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5488}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Shape%20from%20Focus%20via%20Multiscale%20Directional%20Dilated%20Laplacian%20and%20Recurrent%20Network&body=Title%3A%20Robust%20Shape%20from%20Focus%20via%20Multiscale%20Directional%20Dilated%20Laplacian%20and%20Recurrent%20Network%0AAuthor%3A%20Khurram%20Ashfaq%20and%20Muhammad%20Tariq%20Mahmood%0AAbstract%3A%20Shape-from-Focus%20%28SFF%29%20is%20a%20passive%20depth%20estimation%20technique%20that%20infers%20scene%20depth%20by%20analyzing%20focus%20variations%20in%20a%20focal%20stack.%20Most%20recent%20deep%20learning-based%20SFF%20methods%20typically%20operate%20in%20two%20stages%3A%20first%2C%20they%20extract%20focus%20volumes%20%28a%20per%20pixel%20representation%20of%20focus%20likelihood%20across%20the%20focal%20stack%29%20using%20heavy%20feature%20encoders%3B%20then%2C%20they%20estimate%20depth%20via%20a%20simple%20one-step%20aggregation%20technique%20that%20often%20introduces%20artifacts%20and%20amplifies%20noise%20in%20the%20depth%20map.%20To%20address%20these%20issues%2C%20we%20propose%20a%20hybrid%20framework.%20Our%20method%20computes%20multi-scale%20focus%20volumes%20traditionally%20using%20handcrafted%20Directional%20Dilated%20Laplacian%20%28DDL%29%20kernels%2C%20which%20capture%20long-range%20and%20directional%20focus%20variations%20to%20form%20robust%20focus%20volumes.%20These%20focus%20volumes%20are%20then%20fed%20into%20a%20lightweight%2C%20multi-scale%20GRU-based%20depth%20extraction%20module%20that%20iteratively%20refines%20an%20initial%20depth%20estimate%20at%20a%20lower%20resolution%20for%20computational%20efficiency.%20Finally%2C%20a%20learned%20convex%20upsampling%20module%20within%20our%20recurrent%20network%20reconstructs%20high-resolution%20depth%20maps%20while%20preserving%20fine%20scene%20details%20and%20sharp%20boundaries.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20deep%20learning%20and%20traditional%20methods%2C%20achieving%20superior%20accuracy%20and%20generalization%20across%20diverse%20focal%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Shape%2520from%2520Focus%2520via%2520Multiscale%2520Directional%2520Dilated%2520Laplacian%2520and%2520Recurrent%2520Network%26entry.906535625%3DKhurram%2520Ashfaq%2520and%2520Muhammad%2520Tariq%2520Mahmood%26entry.1292438233%3DShape-from-Focus%2520%2528SFF%2529%2520is%2520a%2520passive%2520depth%2520estimation%2520technique%2520that%2520infers%2520scene%2520depth%2520by%2520analyzing%2520focus%2520variations%2520in%2520a%2520focal%2520stack.%2520Most%2520recent%2520deep%2520learning-based%2520SFF%2520methods%2520typically%2520operate%2520in%2520two%2520stages%253A%2520first%252C%2520they%2520extract%2520focus%2520volumes%2520%2528a%2520per%2520pixel%2520representation%2520of%2520focus%2520likelihood%2520across%2520the%2520focal%2520stack%2529%2520using%2520heavy%2520feature%2520encoders%253B%2520then%252C%2520they%2520estimate%2520depth%2520via%2520a%2520simple%2520one-step%2520aggregation%2520technique%2520that%2520often%2520introduces%2520artifacts%2520and%2520amplifies%2520noise%2520in%2520the%2520depth%2520map.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520hybrid%2520framework.%2520Our%2520method%2520computes%2520multi-scale%2520focus%2520volumes%2520traditionally%2520using%2520handcrafted%2520Directional%2520Dilated%2520Laplacian%2520%2528DDL%2529%2520kernels%252C%2520which%2520capture%2520long-range%2520and%2520directional%2520focus%2520variations%2520to%2520form%2520robust%2520focus%2520volumes.%2520These%2520focus%2520volumes%2520are%2520then%2520fed%2520into%2520a%2520lightweight%252C%2520multi-scale%2520GRU-based%2520depth%2520extraction%2520module%2520that%2520iteratively%2520refines%2520an%2520initial%2520depth%2520estimate%2520at%2520a%2520lower%2520resolution%2520for%2520computational%2520efficiency.%2520Finally%252C%2520a%2520learned%2520convex%2520upsampling%2520module%2520within%2520our%2520recurrent%2520network%2520reconstructs%2520high-resolution%2520depth%2520maps%2520while%2520preserving%2520fine%2520scene%2520details%2520and%2520sharp%2520boundaries.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%2520deep%2520learning%2520and%2520traditional%2520methods%252C%2520achieving%2520superior%2520accuracy%2520and%2520generalization%2520across%2520diverse%2520focal%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Shape%20from%20Focus%20via%20Multiscale%20Directional%20Dilated%20Laplacian%20and%20Recurrent%20Network&entry.906535625=Khurram%20Ashfaq%20and%20Muhammad%20Tariq%20Mahmood&entry.1292438233=Shape-from-Focus%20%28SFF%29%20is%20a%20passive%20depth%20estimation%20technique%20that%20infers%20scene%20depth%20by%20analyzing%20focus%20variations%20in%20a%20focal%20stack.%20Most%20recent%20deep%20learning-based%20SFF%20methods%20typically%20operate%20in%20two%20stages%3A%20first%2C%20they%20extract%20focus%20volumes%20%28a%20per%20pixel%20representation%20of%20focus%20likelihood%20across%20the%20focal%20stack%29%20using%20heavy%20feature%20encoders%3B%20then%2C%20they%20estimate%20depth%20via%20a%20simple%20one-step%20aggregation%20technique%20that%20often%20introduces%20artifacts%20and%20amplifies%20noise%20in%20the%20depth%20map.%20To%20address%20these%20issues%2C%20we%20propose%20a%20hybrid%20framework.%20Our%20method%20computes%20multi-scale%20focus%20volumes%20traditionally%20using%20handcrafted%20Directional%20Dilated%20Laplacian%20%28DDL%29%20kernels%2C%20which%20capture%20long-range%20and%20directional%20focus%20variations%20to%20form%20robust%20focus%20volumes.%20These%20focus%20volumes%20are%20then%20fed%20into%20a%20lightweight%2C%20multi-scale%20GRU-based%20depth%20extraction%20module%20that%20iteratively%20refines%20an%20initial%20depth%20estimate%20at%20a%20lower%20resolution%20for%20computational%20efficiency.%20Finally%2C%20a%20learned%20convex%20upsampling%20module%20within%20our%20recurrent%20network%20reconstructs%20high-resolution%20depth%20maps%20while%20preserving%20fine%20scene%20details%20and%20sharp%20boundaries.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20deep%20learning%20and%20traditional%20methods%2C%20achieving%20superior%20accuracy%20and%20generalization%20across%20diverse%20focal%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2512.10498v1&entry.124074799=Read"},
{"title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models", "author": "Tianyi Zhou and Johanne Medina and Sanjay Chawla", "abstract": "Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.", "link": "http://arxiv.org/abs/2508.08139v2", "date": "2025-12-11", "relevancy": 2.2034, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5803}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5789}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Detect%20Their%20Confabulations%3F%20Estimating%20Reliability%20in%20Uncertainty-Aware%20Language%20Models&body=Title%3A%20Can%20LLMs%20Detect%20Their%20Confabulations%3F%20Estimating%20Reliability%20in%20Uncertainty-Aware%20Language%20Models%0AAuthor%3A%20Tianyi%20Zhou%20and%20Johanne%20Medina%20and%20Sanjay%20Chawla%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20prone%20to%20generating%20fluent%20but%20incorrect%20content%2C%20known%20as%20confabulation%2C%20which%20poses%20increasing%20risks%20in%20multi-turn%20or%20agentic%20applications%20where%20outputs%20may%20be%20reused%20as%20context.%20In%20this%20work%2C%20we%20investigate%20how%20in-context%20information%20influences%20model%20behavior%20and%20whether%20LLMs%20can%20identify%20their%20unreliable%20responses.%20We%20propose%20a%20reliability%20estimation%20that%20leverages%20token-level%20uncertainty%20to%20guide%20the%20aggregation%20of%20internal%20model%20representations.%20Specifically%2C%20we%20compute%20aleatoric%20and%20epistemic%20uncertainty%20from%20output%20logits%20to%20identify%20salient%20tokens%20and%20aggregate%20their%20hidden%20states%20into%20compact%20representations%20for%20response-level%20reliability%20prediction.%20Through%20controlled%20experiments%20on%20open%20QA%20benchmarks%2C%20we%20find%20that%20correct%20in-context%20information%20improves%20both%20answer%20accuracy%20and%20model%20confidence%2C%20while%20misleading%20context%20often%20induces%20confidently%20incorrect%20responses%2C%20revealing%20a%20misalignment%20between%20uncertainty%20and%20correctness.%20Our%20probing-based%20method%20captures%20these%20shifts%20in%20model%20behavior%20and%20improves%20the%20detection%20of%20unreliable%20outputs%20across%20multiple%20open-source%20LLMs.%20These%20results%20underscore%20the%20limitations%20of%20direct%20uncertainty%20signals%20and%20highlight%20the%20potential%20of%20uncertainty-guided%20probing%20for%20reliability-aware%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Detect%2520Their%2520Confabulations%253F%2520Estimating%2520Reliability%2520in%2520Uncertainty-Aware%2520Language%2520Models%26entry.906535625%3DTianyi%2520Zhou%2520and%2520Johanne%2520Medina%2520and%2520Sanjay%2520Chawla%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520prone%2520to%2520generating%2520fluent%2520but%2520incorrect%2520content%252C%2520known%2520as%2520confabulation%252C%2520which%2520poses%2520increasing%2520risks%2520in%2520multi-turn%2520or%2520agentic%2520applications%2520where%2520outputs%2520may%2520be%2520reused%2520as%2520context.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520in-context%2520information%2520influences%2520model%2520behavior%2520and%2520whether%2520LLMs%2520can%2520identify%2520their%2520unreliable%2520responses.%2520We%2520propose%2520a%2520reliability%2520estimation%2520that%2520leverages%2520token-level%2520uncertainty%2520to%2520guide%2520the%2520aggregation%2520of%2520internal%2520model%2520representations.%2520Specifically%252C%2520we%2520compute%2520aleatoric%2520and%2520epistemic%2520uncertainty%2520from%2520output%2520logits%2520to%2520identify%2520salient%2520tokens%2520and%2520aggregate%2520their%2520hidden%2520states%2520into%2520compact%2520representations%2520for%2520response-level%2520reliability%2520prediction.%2520Through%2520controlled%2520experiments%2520on%2520open%2520QA%2520benchmarks%252C%2520we%2520find%2520that%2520correct%2520in-context%2520information%2520improves%2520both%2520answer%2520accuracy%2520and%2520model%2520confidence%252C%2520while%2520misleading%2520context%2520often%2520induces%2520confidently%2520incorrect%2520responses%252C%2520revealing%2520a%2520misalignment%2520between%2520uncertainty%2520and%2520correctness.%2520Our%2520probing-based%2520method%2520captures%2520these%2520shifts%2520in%2520model%2520behavior%2520and%2520improves%2520the%2520detection%2520of%2520unreliable%2520outputs%2520across%2520multiple%2520open-source%2520LLMs.%2520These%2520results%2520underscore%2520the%2520limitations%2520of%2520direct%2520uncertainty%2520signals%2520and%2520highlight%2520the%2520potential%2520of%2520uncertainty-guided%2520probing%2520for%2520reliability-aware%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Detect%20Their%20Confabulations%3F%20Estimating%20Reliability%20in%20Uncertainty-Aware%20Language%20Models&entry.906535625=Tianyi%20Zhou%20and%20Johanne%20Medina%20and%20Sanjay%20Chawla&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20prone%20to%20generating%20fluent%20but%20incorrect%20content%2C%20known%20as%20confabulation%2C%20which%20poses%20increasing%20risks%20in%20multi-turn%20or%20agentic%20applications%20where%20outputs%20may%20be%20reused%20as%20context.%20In%20this%20work%2C%20we%20investigate%20how%20in-context%20information%20influences%20model%20behavior%20and%20whether%20LLMs%20can%20identify%20their%20unreliable%20responses.%20We%20propose%20a%20reliability%20estimation%20that%20leverages%20token-level%20uncertainty%20to%20guide%20the%20aggregation%20of%20internal%20model%20representations.%20Specifically%2C%20we%20compute%20aleatoric%20and%20epistemic%20uncertainty%20from%20output%20logits%20to%20identify%20salient%20tokens%20and%20aggregate%20their%20hidden%20states%20into%20compact%20representations%20for%20response-level%20reliability%20prediction.%20Through%20controlled%20experiments%20on%20open%20QA%20benchmarks%2C%20we%20find%20that%20correct%20in-context%20information%20improves%20both%20answer%20accuracy%20and%20model%20confidence%2C%20while%20misleading%20context%20often%20induces%20confidently%20incorrect%20responses%2C%20revealing%20a%20misalignment%20between%20uncertainty%20and%20correctness.%20Our%20probing-based%20method%20captures%20these%20shifts%20in%20model%20behavior%20and%20improves%20the%20detection%20of%20unreliable%20outputs%20across%20multiple%20open-source%20LLMs.%20These%20results%20underscore%20the%20limitations%20of%20direct%20uncertainty%20signals%20and%20highlight%20the%20potential%20of%20uncertainty-guided%20probing%20for%20reliability-aware%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2508.08139v2&entry.124074799=Read"},
{"title": "COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators", "author": "Wei Fang and Chiyao Wang and Wenshuai Ma and Hui Liu and Jianqiang Hu and Xiaona Niu and Yi Chu and Mingming Zhang and Jingxiao Yang and Dongwei Zhang and Zelin Li and Pengyun Liu and Jiawei Zheng and Pengke Zhang and Chaoshi Qin and Wangang Guo and Bin Wang and Yugang Xue and Wei Zhang and Zikuan Wang and Rui Zhu and Yihui Cao and Quanmao Lu and Rui Meng and Yan Li", "abstract": "Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.\n  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.\n  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.\n  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.", "link": "http://arxiv.org/abs/2512.10702v1", "date": "2025-12-11", "relevancy": 2.2027, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4418}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4418}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMPARE%3A%20Clinical%20Optimization%20with%20Modular%20Planning%20and%20Assessment%20via%20RAG-Enhanced%20AI-OCT%3A%20Superior%20Decision%20Support%20for%20Percutaneous%20Coronary%20Intervention%20Compared%20to%20ChatGPT-5%20and%20Junior%20Operators&body=Title%3A%20COMPARE%3A%20Clinical%20Optimization%20with%20Modular%20Planning%20and%20Assessment%20via%20RAG-Enhanced%20AI-OCT%3A%20Superior%20Decision%20Support%20for%20Percutaneous%20Coronary%20Intervention%20Compared%20to%20ChatGPT-5%20and%20Junior%20Operators%0AAuthor%3A%20Wei%20Fang%20and%20Chiyao%20Wang%20and%20Wenshuai%20Ma%20and%20Hui%20Liu%20and%20Jianqiang%20Hu%20and%20Xiaona%20Niu%20and%20Yi%20Chu%20and%20Mingming%20Zhang%20and%20Jingxiao%20Yang%20and%20Dongwei%20Zhang%20and%20Zelin%20Li%20and%20Pengyun%20Liu%20and%20Jiawei%20Zheng%20and%20Pengke%20Zhang%20and%20Chaoshi%20Qin%20and%20Wangang%20Guo%20and%20Bin%20Wang%20and%20Yugang%20Xue%20and%20Wei%20Zhang%20and%20Zikuan%20Wang%20and%20Rui%20Zhu%20and%20Yihui%20Cao%20and%20Quanmao%20Lu%20and%20Rui%20Meng%20and%20Yan%20Li%0AAbstract%3A%20Background%3A%20While%20intravascular%20imaging%2C%20particularly%20optical%20coherence%20tomography%20%28OCT%29%2C%20improves%20percutaneous%20coronary%20intervention%20%28PCI%29%20outcomes%2C%20its%20interpretation%20is%20operator-dependent.%20General-purpose%20artificial%20intelligence%20%28AI%29%20shows%20promise%20but%20lacks%20domain-specific%20reliability.%20We%20evaluated%20the%20performance%20of%20CA-GPT%2C%20a%20novel%20large%20model%20deployed%20on%20an%20AI-OCT%20system%2C%20against%20that%20of%20the%20general-purpose%20ChatGPT-5%20and%20junior%20physicians%20for%20OCT-guided%20PCI%20planning%20and%20assessment.%0A%20%20Methods%3A%20In%20this%20single-center%20analysis%20of%2096%20patients%20who%20underwent%20OCT-guided%20PCI%2C%20the%20procedural%20decisions%20generated%20by%20the%20CA-GPT%2C%20ChatGPT-5%2C%20and%20junior%20physicians%20were%20compared%20with%20an%20expert-derived%20procedural%20record.%20Agreement%20was%20assessed%20using%20ten%20pre-specified%20metrics%20across%20pre-PCI%20and%20post-PCI%20phases.%0A%20%20Results%3A%20For%20pre-PCI%20planning%2C%20CA-GPT%20demonstrated%20significantly%20higher%20median%20agreement%20scores%20%285%5BIQR%203.75-5%5D%29%20compared%20to%20both%20ChatGPT-5%20%283%5B2-4%5D%2C%20P%3C0.001%29%20and%20junior%20physicians%20%284%5B3-4%5D%2C%20P%3C0.001%29.%20CA-GPT%20significantly%20outperformed%20ChatGPT-5%20across%20all%20individual%20pre-PCI%20metrics%20and%20showed%20superior%20performance%20to%20junior%20physicians%20in%20stent%20diameter%20%2890.3%25%20vs.%2072.2%25%2C%20P%3C0.05%29%20and%20length%20selection%20%2880.6%25%20vs.%2052.8%25%2C%20P%3C0.01%29.%20In%20post-PCI%20assessment%2C%20CA-GPT%20maintained%20excellent%20overall%20agreement%20%285%5B4.75-5%5D%29%2C%20significantly%20higher%20than%20both%20ChatGPT-5%20%284%5B4-5%5D%2C%20P%3C0.001%29%20and%20junior%20physicians%20%285%5B4-5%5D%2C%20P%3C0.05%29.%20Subgroup%20analysis%20confirmed%20CA-GPT%27s%20robust%20performance%20advantage%20in%20complex%20scenarios.%0A%20%20Conclusion%3A%20The%20CA-GPT-based%20AI-OCT%20system%20achieved%20superior%20decision-making%20agreement%20versus%20a%20general-purpose%20large%20language%20model%20and%20junior%20physicians%20across%20both%20PCI%20planning%20and%20assessment%20phases.%20This%20approach%20provides%20a%20standardized%20and%20reliable%20method%20for%20intravascular%20imaging%20interpretation%2C%20demonstrating%20significant%20potential%20to%20augment%20operator%20expertise%20and%20optimize%20OCT-guided%20PCI.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMPARE%253A%2520Clinical%2520Optimization%2520with%2520Modular%2520Planning%2520and%2520Assessment%2520via%2520RAG-Enhanced%2520AI-OCT%253A%2520Superior%2520Decision%2520Support%2520for%2520Percutaneous%2520Coronary%2520Intervention%2520Compared%2520to%2520ChatGPT-5%2520and%2520Junior%2520Operators%26entry.906535625%3DWei%2520Fang%2520and%2520Chiyao%2520Wang%2520and%2520Wenshuai%2520Ma%2520and%2520Hui%2520Liu%2520and%2520Jianqiang%2520Hu%2520and%2520Xiaona%2520Niu%2520and%2520Yi%2520Chu%2520and%2520Mingming%2520Zhang%2520and%2520Jingxiao%2520Yang%2520and%2520Dongwei%2520Zhang%2520and%2520Zelin%2520Li%2520and%2520Pengyun%2520Liu%2520and%2520Jiawei%2520Zheng%2520and%2520Pengke%2520Zhang%2520and%2520Chaoshi%2520Qin%2520and%2520Wangang%2520Guo%2520and%2520Bin%2520Wang%2520and%2520Yugang%2520Xue%2520and%2520Wei%2520Zhang%2520and%2520Zikuan%2520Wang%2520and%2520Rui%2520Zhu%2520and%2520Yihui%2520Cao%2520and%2520Quanmao%2520Lu%2520and%2520Rui%2520Meng%2520and%2520Yan%2520Li%26entry.1292438233%3DBackground%253A%2520While%2520intravascular%2520imaging%252C%2520particularly%2520optical%2520coherence%2520tomography%2520%2528OCT%2529%252C%2520improves%2520percutaneous%2520coronary%2520intervention%2520%2528PCI%2529%2520outcomes%252C%2520its%2520interpretation%2520is%2520operator-dependent.%2520General-purpose%2520artificial%2520intelligence%2520%2528AI%2529%2520shows%2520promise%2520but%2520lacks%2520domain-specific%2520reliability.%2520We%2520evaluated%2520the%2520performance%2520of%2520CA-GPT%252C%2520a%2520novel%2520large%2520model%2520deployed%2520on%2520an%2520AI-OCT%2520system%252C%2520against%2520that%2520of%2520the%2520general-purpose%2520ChatGPT-5%2520and%2520junior%2520physicians%2520for%2520OCT-guided%2520PCI%2520planning%2520and%2520assessment.%250A%2520%2520Methods%253A%2520In%2520this%2520single-center%2520analysis%2520of%252096%2520patients%2520who%2520underwent%2520OCT-guided%2520PCI%252C%2520the%2520procedural%2520decisions%2520generated%2520by%2520the%2520CA-GPT%252C%2520ChatGPT-5%252C%2520and%2520junior%2520physicians%2520were%2520compared%2520with%2520an%2520expert-derived%2520procedural%2520record.%2520Agreement%2520was%2520assessed%2520using%2520ten%2520pre-specified%2520metrics%2520across%2520pre-PCI%2520and%2520post-PCI%2520phases.%250A%2520%2520Results%253A%2520For%2520pre-PCI%2520planning%252C%2520CA-GPT%2520demonstrated%2520significantly%2520higher%2520median%2520agreement%2520scores%2520%25285%255BIQR%25203.75-5%255D%2529%2520compared%2520to%2520both%2520ChatGPT-5%2520%25283%255B2-4%255D%252C%2520P%253C0.001%2529%2520and%2520junior%2520physicians%2520%25284%255B3-4%255D%252C%2520P%253C0.001%2529.%2520CA-GPT%2520significantly%2520outperformed%2520ChatGPT-5%2520across%2520all%2520individual%2520pre-PCI%2520metrics%2520and%2520showed%2520superior%2520performance%2520to%2520junior%2520physicians%2520in%2520stent%2520diameter%2520%252890.3%2525%2520vs.%252072.2%2525%252C%2520P%253C0.05%2529%2520and%2520length%2520selection%2520%252880.6%2525%2520vs.%252052.8%2525%252C%2520P%253C0.01%2529.%2520In%2520post-PCI%2520assessment%252C%2520CA-GPT%2520maintained%2520excellent%2520overall%2520agreement%2520%25285%255B4.75-5%255D%2529%252C%2520significantly%2520higher%2520than%2520both%2520ChatGPT-5%2520%25284%255B4-5%255D%252C%2520P%253C0.001%2529%2520and%2520junior%2520physicians%2520%25285%255B4-5%255D%252C%2520P%253C0.05%2529.%2520Subgroup%2520analysis%2520confirmed%2520CA-GPT%2527s%2520robust%2520performance%2520advantage%2520in%2520complex%2520scenarios.%250A%2520%2520Conclusion%253A%2520The%2520CA-GPT-based%2520AI-OCT%2520system%2520achieved%2520superior%2520decision-making%2520agreement%2520versus%2520a%2520general-purpose%2520large%2520language%2520model%2520and%2520junior%2520physicians%2520across%2520both%2520PCI%2520planning%2520and%2520assessment%2520phases.%2520This%2520approach%2520provides%2520a%2520standardized%2520and%2520reliable%2520method%2520for%2520intravascular%2520imaging%2520interpretation%252C%2520demonstrating%2520significant%2520potential%2520to%2520augment%2520operator%2520expertise%2520and%2520optimize%2520OCT-guided%2520PCI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMPARE%3A%20Clinical%20Optimization%20with%20Modular%20Planning%20and%20Assessment%20via%20RAG-Enhanced%20AI-OCT%3A%20Superior%20Decision%20Support%20for%20Percutaneous%20Coronary%20Intervention%20Compared%20to%20ChatGPT-5%20and%20Junior%20Operators&entry.906535625=Wei%20Fang%20and%20Chiyao%20Wang%20and%20Wenshuai%20Ma%20and%20Hui%20Liu%20and%20Jianqiang%20Hu%20and%20Xiaona%20Niu%20and%20Yi%20Chu%20and%20Mingming%20Zhang%20and%20Jingxiao%20Yang%20and%20Dongwei%20Zhang%20and%20Zelin%20Li%20and%20Pengyun%20Liu%20and%20Jiawei%20Zheng%20and%20Pengke%20Zhang%20and%20Chaoshi%20Qin%20and%20Wangang%20Guo%20and%20Bin%20Wang%20and%20Yugang%20Xue%20and%20Wei%20Zhang%20and%20Zikuan%20Wang%20and%20Rui%20Zhu%20and%20Yihui%20Cao%20and%20Quanmao%20Lu%20and%20Rui%20Meng%20and%20Yan%20Li&entry.1292438233=Background%3A%20While%20intravascular%20imaging%2C%20particularly%20optical%20coherence%20tomography%20%28OCT%29%2C%20improves%20percutaneous%20coronary%20intervention%20%28PCI%29%20outcomes%2C%20its%20interpretation%20is%20operator-dependent.%20General-purpose%20artificial%20intelligence%20%28AI%29%20shows%20promise%20but%20lacks%20domain-specific%20reliability.%20We%20evaluated%20the%20performance%20of%20CA-GPT%2C%20a%20novel%20large%20model%20deployed%20on%20an%20AI-OCT%20system%2C%20against%20that%20of%20the%20general-purpose%20ChatGPT-5%20and%20junior%20physicians%20for%20OCT-guided%20PCI%20planning%20and%20assessment.%0A%20%20Methods%3A%20In%20this%20single-center%20analysis%20of%2096%20patients%20who%20underwent%20OCT-guided%20PCI%2C%20the%20procedural%20decisions%20generated%20by%20the%20CA-GPT%2C%20ChatGPT-5%2C%20and%20junior%20physicians%20were%20compared%20with%20an%20expert-derived%20procedural%20record.%20Agreement%20was%20assessed%20using%20ten%20pre-specified%20metrics%20across%20pre-PCI%20and%20post-PCI%20phases.%0A%20%20Results%3A%20For%20pre-PCI%20planning%2C%20CA-GPT%20demonstrated%20significantly%20higher%20median%20agreement%20scores%20%285%5BIQR%203.75-5%5D%29%20compared%20to%20both%20ChatGPT-5%20%283%5B2-4%5D%2C%20P%3C0.001%29%20and%20junior%20physicians%20%284%5B3-4%5D%2C%20P%3C0.001%29.%20CA-GPT%20significantly%20outperformed%20ChatGPT-5%20across%20all%20individual%20pre-PCI%20metrics%20and%20showed%20superior%20performance%20to%20junior%20physicians%20in%20stent%20diameter%20%2890.3%25%20vs.%2072.2%25%2C%20P%3C0.05%29%20and%20length%20selection%20%2880.6%25%20vs.%2052.8%25%2C%20P%3C0.01%29.%20In%20post-PCI%20assessment%2C%20CA-GPT%20maintained%20excellent%20overall%20agreement%20%285%5B4.75-5%5D%29%2C%20significantly%20higher%20than%20both%20ChatGPT-5%20%284%5B4-5%5D%2C%20P%3C0.001%29%20and%20junior%20physicians%20%285%5B4-5%5D%2C%20P%3C0.05%29.%20Subgroup%20analysis%20confirmed%20CA-GPT%27s%20robust%20performance%20advantage%20in%20complex%20scenarios.%0A%20%20Conclusion%3A%20The%20CA-GPT-based%20AI-OCT%20system%20achieved%20superior%20decision-making%20agreement%20versus%20a%20general-purpose%20large%20language%20model%20and%20junior%20physicians%20across%20both%20PCI%20planning%20and%20assessment%20phases.%20This%20approach%20provides%20a%20standardized%20and%20reliable%20method%20for%20intravascular%20imaging%20interpretation%2C%20demonstrating%20significant%20potential%20to%20augment%20operator%20expertise%20and%20optimize%20OCT-guided%20PCI.&entry.1838667208=http%3A//arxiv.org/abs/2512.10702v1&entry.124074799=Read"},
{"title": "Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments", "author": "Atahan Cilan and Atay \u00d6zg\u00f6vde", "abstract": "This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.", "link": "http://arxiv.org/abs/2512.10835v1", "date": "2025-12-11", "relevancy": 2.2007, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5769}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5483}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Controllable%20and%20Diverse%20Player%20Behaviors%20in%20Multi-Agent%20Environments&body=Title%3A%20Learning%20Controllable%20and%20Diverse%20Player%20Behaviors%20in%20Multi-Agent%20Environments%0AAuthor%3A%20Atahan%20Cilan%20and%20Atay%20%C3%96zg%C3%B6vde%0AAbstract%3A%20This%20paper%20introduces%20a%20reinforcement%20learning%20framework%20that%20enables%20controllable%20and%20diverse%20player%20behaviors%20without%20relying%20on%20human%20gameplay%20data.%20Existing%20approaches%20often%20require%20large-scale%20player%20trajectories%2C%20train%20separate%20models%20for%20different%20player%20types%2C%20or%20provide%20no%20direct%20mapping%20between%20interpretable%20behavioral%20parameters%20and%20the%20learned%20policy%2C%20limiting%20their%20scalability%20and%20controllability.%20We%20define%20player%20behavior%20in%20an%20N-dimensional%20continuous%20space%20and%20uniformly%20sample%20target%20behavior%20vectors%20from%20a%20region%20that%20encompasses%20the%20subset%20representing%20real%20human%20styles.%20During%20training%2C%20each%20agent%20receives%20both%20its%20current%20and%20target%20behavior%20vectors%20as%20input%2C%20and%20the%20reward%20is%20based%20on%20the%20normalized%20reduction%20in%20distance%20between%20them.%20This%20allows%20the%20policy%20to%20learn%20how%20actions%20influence%20behavioral%20statistics%2C%20enabling%20smooth%20control%20over%20attributes%20such%20as%20aggressiveness%2C%20mobility%2C%20and%20cooperativeness.%20A%20single%20PPO-based%20multi-agent%20policy%20can%20reproduce%20new%20or%20unseen%20play%20styles%20without%20retraining.%20Experiments%20conducted%20in%20a%20custom%20multi-player%20Unity%20game%20show%20that%20the%20proposed%20framework%20produces%20significantly%20greater%20behavioral%20diversity%20than%20a%20win-only%20baseline%20and%20reliably%20matches%20specified%20behavior%20vectors%20across%20diverse%20targets.%20The%20method%20offers%20a%20scalable%20solution%20for%20automated%20playtesting%2C%20game%20balancing%2C%20human-like%20behavior%20simulation%2C%20and%20replacing%20disconnected%20players%20in%20online%20games.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Controllable%2520and%2520Diverse%2520Player%2520Behaviors%2520in%2520Multi-Agent%2520Environments%26entry.906535625%3DAtahan%2520Cilan%2520and%2520Atay%2520%25C3%2596zg%25C3%25B6vde%26entry.1292438233%3DThis%2520paper%2520introduces%2520a%2520reinforcement%2520learning%2520framework%2520that%2520enables%2520controllable%2520and%2520diverse%2520player%2520behaviors%2520without%2520relying%2520on%2520human%2520gameplay%2520data.%2520Existing%2520approaches%2520often%2520require%2520large-scale%2520player%2520trajectories%252C%2520train%2520separate%2520models%2520for%2520different%2520player%2520types%252C%2520or%2520provide%2520no%2520direct%2520mapping%2520between%2520interpretable%2520behavioral%2520parameters%2520and%2520the%2520learned%2520policy%252C%2520limiting%2520their%2520scalability%2520and%2520controllability.%2520We%2520define%2520player%2520behavior%2520in%2520an%2520N-dimensional%2520continuous%2520space%2520and%2520uniformly%2520sample%2520target%2520behavior%2520vectors%2520from%2520a%2520region%2520that%2520encompasses%2520the%2520subset%2520representing%2520real%2520human%2520styles.%2520During%2520training%252C%2520each%2520agent%2520receives%2520both%2520its%2520current%2520and%2520target%2520behavior%2520vectors%2520as%2520input%252C%2520and%2520the%2520reward%2520is%2520based%2520on%2520the%2520normalized%2520reduction%2520in%2520distance%2520between%2520them.%2520This%2520allows%2520the%2520policy%2520to%2520learn%2520how%2520actions%2520influence%2520behavioral%2520statistics%252C%2520enabling%2520smooth%2520control%2520over%2520attributes%2520such%2520as%2520aggressiveness%252C%2520mobility%252C%2520and%2520cooperativeness.%2520A%2520single%2520PPO-based%2520multi-agent%2520policy%2520can%2520reproduce%2520new%2520or%2520unseen%2520play%2520styles%2520without%2520retraining.%2520Experiments%2520conducted%2520in%2520a%2520custom%2520multi-player%2520Unity%2520game%2520show%2520that%2520the%2520proposed%2520framework%2520produces%2520significantly%2520greater%2520behavioral%2520diversity%2520than%2520a%2520win-only%2520baseline%2520and%2520reliably%2520matches%2520specified%2520behavior%2520vectors%2520across%2520diverse%2520targets.%2520The%2520method%2520offers%2520a%2520scalable%2520solution%2520for%2520automated%2520playtesting%252C%2520game%2520balancing%252C%2520human-like%2520behavior%2520simulation%252C%2520and%2520replacing%2520disconnected%2520players%2520in%2520online%2520games.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Controllable%20and%20Diverse%20Player%20Behaviors%20in%20Multi-Agent%20Environments&entry.906535625=Atahan%20Cilan%20and%20Atay%20%C3%96zg%C3%B6vde&entry.1292438233=This%20paper%20introduces%20a%20reinforcement%20learning%20framework%20that%20enables%20controllable%20and%20diverse%20player%20behaviors%20without%20relying%20on%20human%20gameplay%20data.%20Existing%20approaches%20often%20require%20large-scale%20player%20trajectories%2C%20train%20separate%20models%20for%20different%20player%20types%2C%20or%20provide%20no%20direct%20mapping%20between%20interpretable%20behavioral%20parameters%20and%20the%20learned%20policy%2C%20limiting%20their%20scalability%20and%20controllability.%20We%20define%20player%20behavior%20in%20an%20N-dimensional%20continuous%20space%20and%20uniformly%20sample%20target%20behavior%20vectors%20from%20a%20region%20that%20encompasses%20the%20subset%20representing%20real%20human%20styles.%20During%20training%2C%20each%20agent%20receives%20both%20its%20current%20and%20target%20behavior%20vectors%20as%20input%2C%20and%20the%20reward%20is%20based%20on%20the%20normalized%20reduction%20in%20distance%20between%20them.%20This%20allows%20the%20policy%20to%20learn%20how%20actions%20influence%20behavioral%20statistics%2C%20enabling%20smooth%20control%20over%20attributes%20such%20as%20aggressiveness%2C%20mobility%2C%20and%20cooperativeness.%20A%20single%20PPO-based%20multi-agent%20policy%20can%20reproduce%20new%20or%20unseen%20play%20styles%20without%20retraining.%20Experiments%20conducted%20in%20a%20custom%20multi-player%20Unity%20game%20show%20that%20the%20proposed%20framework%20produces%20significantly%20greater%20behavioral%20diversity%20than%20a%20win-only%20baseline%20and%20reliably%20matches%20specified%20behavior%20vectors%20across%20diverse%20targets.%20The%20method%20offers%20a%20scalable%20solution%20for%20automated%20playtesting%2C%20game%20balancing%2C%20human-like%20behavior%20simulation%2C%20and%20replacing%20disconnected%20players%20in%20online%20games.&entry.1838667208=http%3A//arxiv.org/abs/2512.10835v1&entry.124074799=Read"},
{"title": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data", "author": "Bidyapati Pradhan and Surajit Dasgupta and Amit Kumar Saha and Omkar Anustoop and Sriram Puttagunta and Vipul Mittal and Gopal Sarda", "abstract": "The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.", "link": "http://arxiv.org/abs/2508.15432v3", "date": "2025-12-11", "relevancy": 2.1945, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5578}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5472}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyGra%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data&body=Title%3A%20SyGra%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data%0AAuthor%3A%20Bidyapati%20Pradhan%20and%20Surajit%20Dasgupta%20and%20Amit%20Kumar%20Saha%20and%20Omkar%20Anustoop%20and%20Sriram%20Puttagunta%20and%20Vipul%20Mittal%20and%20Gopal%20Sarda%0AAbstract%3A%20The%20advancement%20of%20large%20language%20models%20%28LLMs%29%20is%20critically%20dependent%20on%20the%20availability%20of%20high-quality%20datasets%20for%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20alignment%20tasks%20like%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20etc.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%20synthetic%20data%20generation%20framework%20that%20facilitates%20scalable%2C%20configurable%2C%20and%20high-fidelity%20generation%20of%20synthetic%20data%20tailored%20for%20these%20training%20paradigms.%20Our%20approach%20employs%20a%20modular%20and%20configuration-based%20pipeline%20capable%20of%20modeling%20complex%20dialogue%20flows%20with%20minimal%20manual%20intervention.%20This%20framework%20uses%20a%20dual-stage%20quality%20tagging%20mechanism%2C%20combining%20heuristic%20rules%20and%20LLM-based%20evaluations%2C%20to%20automatically%20filter%20and%20score%20data%20extracted%20from%20OASST-formatted%20conversations%2C%20ensuring%20the%20curation%20of%20high-quality%20dialogue%20samples.%20The%20resulting%20datasets%20are%20structured%20under%20a%20flexible%20schema%20supporting%20both%20SFT%20and%20DPO%20use%20cases%2C%20enabling%20seamless%20integration%20into%20diverse%20training%20workflows.%20Together%2C%20these%20innovations%20offer%20a%20robust%20solution%20for%20generating%20and%20managing%20synthetic%20conversational%20data%20at%20scale%2C%20significantly%20reducing%20the%20overhead%20of%20data%20preparation%20in%20LLM%20training%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2508.15432v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyGra%253A%2520A%2520Unified%2520Graph-Based%2520Framework%2520for%2520Scalable%2520Generation%252C%2520Quality%2520Tagging%252C%2520and%2520Management%2520of%2520Synthetic%2520Data%26entry.906535625%3DBidyapati%2520Pradhan%2520and%2520Surajit%2520Dasgupta%2520and%2520Amit%2520Kumar%2520Saha%2520and%2520Omkar%2520Anustoop%2520and%2520Sriram%2520Puttagunta%2520and%2520Vipul%2520Mittal%2520and%2520Gopal%2520Sarda%26entry.1292438233%3DThe%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520critically%2520dependent%2520on%2520the%2520availability%2520of%2520high-quality%2520datasets%2520for%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%252C%2520alignment%2520tasks%2520like%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%252C%2520etc.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520comprehensive%2520synthetic%2520data%2520generation%2520framework%2520that%2520facilitates%2520scalable%252C%2520configurable%252C%2520and%2520high-fidelity%2520generation%2520of%2520synthetic%2520data%2520tailored%2520for%2520these%2520training%2520paradigms.%2520Our%2520approach%2520employs%2520a%2520modular%2520and%2520configuration-based%2520pipeline%2520capable%2520of%2520modeling%2520complex%2520dialogue%2520flows%2520with%2520minimal%2520manual%2520intervention.%2520This%2520framework%2520uses%2520a%2520dual-stage%2520quality%2520tagging%2520mechanism%252C%2520combining%2520heuristic%2520rules%2520and%2520LLM-based%2520evaluations%252C%2520to%2520automatically%2520filter%2520and%2520score%2520data%2520extracted%2520from%2520OASST-formatted%2520conversations%252C%2520ensuring%2520the%2520curation%2520of%2520high-quality%2520dialogue%2520samples.%2520The%2520resulting%2520datasets%2520are%2520structured%2520under%2520a%2520flexible%2520schema%2520supporting%2520both%2520SFT%2520and%2520DPO%2520use%2520cases%252C%2520enabling%2520seamless%2520integration%2520into%2520diverse%2520training%2520workflows.%2520Together%252C%2520these%2520innovations%2520offer%2520a%2520robust%2520solution%2520for%2520generating%2520and%2520managing%2520synthetic%2520conversational%2520data%2520at%2520scale%252C%2520significantly%2520reducing%2520the%2520overhead%2520of%2520data%2520preparation%2520in%2520LLM%2520training%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15432v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyGra%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data&entry.906535625=Bidyapati%20Pradhan%20and%20Surajit%20Dasgupta%20and%20Amit%20Kumar%20Saha%20and%20Omkar%20Anustoop%20and%20Sriram%20Puttagunta%20and%20Vipul%20Mittal%20and%20Gopal%20Sarda&entry.1292438233=The%20advancement%20of%20large%20language%20models%20%28LLMs%29%20is%20critically%20dependent%20on%20the%20availability%20of%20high-quality%20datasets%20for%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20alignment%20tasks%20like%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20etc.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%20synthetic%20data%20generation%20framework%20that%20facilitates%20scalable%2C%20configurable%2C%20and%20high-fidelity%20generation%20of%20synthetic%20data%20tailored%20for%20these%20training%20paradigms.%20Our%20approach%20employs%20a%20modular%20and%20configuration-based%20pipeline%20capable%20of%20modeling%20complex%20dialogue%20flows%20with%20minimal%20manual%20intervention.%20This%20framework%20uses%20a%20dual-stage%20quality%20tagging%20mechanism%2C%20combining%20heuristic%20rules%20and%20LLM-based%20evaluations%2C%20to%20automatically%20filter%20and%20score%20data%20extracted%20from%20OASST-formatted%20conversations%2C%20ensuring%20the%20curation%20of%20high-quality%20dialogue%20samples.%20The%20resulting%20datasets%20are%20structured%20under%20a%20flexible%20schema%20supporting%20both%20SFT%20and%20DPO%20use%20cases%2C%20enabling%20seamless%20integration%20into%20diverse%20training%20workflows.%20Together%2C%20these%20innovations%20offer%20a%20robust%20solution%20for%20generating%20and%20managing%20synthetic%20conversational%20data%20at%20scale%2C%20significantly%20reducing%20the%20overhead%20of%20data%20preparation%20in%20LLM%20training%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2508.15432v3&entry.124074799=Read"},
{"title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality", "author": "Aileen Cheng and Alon Jacovi and Amir Globerson and Ben Golan and Charles Kwong and Chris Alberti and Connie Tao and Eyal Ben-David and Gaurav Singh Tomar and Lukas Haas and Yonatan Bitton and Adam Bloniarz and Aijun Bai and Andrew Wang and Anfal Siddiqui and Arturo Bajuelos Castillo and Aviel Atias and Chang Liu and Corey Fry and Daniel Balle and Deepanway Ghosal and Doron Kukliansky and Dror Marcus and Elena Gribovskaya and Eran Ofek and Honglei Zhuang and Itay Laish and Jan Ackermann and Lily Wang and Meg Risdal and Megan Barnes and Michael Fink and Mohamed Amin and Moran Ambar and Natan Potikha and Nikita Gupta and Nitzan Katz and Noam Velan and Ofir Roval and Ori Ram and Polina Zablotskaia and Prathamesh Bang and Priyanka Agrawal and Rakesh Ghiya and Sanjay Ganapathy and Simon Baumgartner and Sofia Erell and Sushant Prakash and Thibault Sellam and Vikram Rao and Xuanhui Wang and Yaroslav Akulov and Yulong Yang and Zhen Yang and Zhixin Lai and Zhongru Wu and Anca Dragan and Avinatan Hassidim and Fernando Pereira and Slav Petrov and Srinivasan Venkatachary and Tulsee Doshi and Yossi Matias and Sasha Goldshtein and Dipanjan Das", "abstract": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .", "link": "http://arxiv.org/abs/2512.10791v1", "date": "2025-12-11", "relevancy": 2.1889, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4472}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20FACTS%20Leaderboard%3A%20A%20Comprehensive%20Benchmark%20for%20Large%20Language%20Model%20Factuality&body=Title%3A%20The%20FACTS%20Leaderboard%3A%20A%20Comprehensive%20Benchmark%20for%20Large%20Language%20Model%20Factuality%0AAuthor%3A%20Aileen%20Cheng%20and%20Alon%20Jacovi%20and%20Amir%20Globerson%20and%20Ben%20Golan%20and%20Charles%20Kwong%20and%20Chris%20Alberti%20and%20Connie%20Tao%20and%20Eyal%20Ben-David%20and%20Gaurav%20Singh%20Tomar%20and%20Lukas%20Haas%20and%20Yonatan%20Bitton%20and%20Adam%20Bloniarz%20and%20Aijun%20Bai%20and%20Andrew%20Wang%20and%20Anfal%20Siddiqui%20and%20Arturo%20Bajuelos%20Castillo%20and%20Aviel%20Atias%20and%20Chang%20Liu%20and%20Corey%20Fry%20and%20Daniel%20Balle%20and%20Deepanway%20Ghosal%20and%20Doron%20Kukliansky%20and%20Dror%20Marcus%20and%20Elena%20Gribovskaya%20and%20Eran%20Ofek%20and%20Honglei%20Zhuang%20and%20Itay%20Laish%20and%20Jan%20Ackermann%20and%20Lily%20Wang%20and%20Meg%20Risdal%20and%20Megan%20Barnes%20and%20Michael%20Fink%20and%20Mohamed%20Amin%20and%20Moran%20Ambar%20and%20Natan%20Potikha%20and%20Nikita%20Gupta%20and%20Nitzan%20Katz%20and%20Noam%20Velan%20and%20Ofir%20Roval%20and%20Ori%20Ram%20and%20Polina%20Zablotskaia%20and%20Prathamesh%20Bang%20and%20Priyanka%20Agrawal%20and%20Rakesh%20Ghiya%20and%20Sanjay%20Ganapathy%20and%20Simon%20Baumgartner%20and%20Sofia%20Erell%20and%20Sushant%20Prakash%20and%20Thibault%20Sellam%20and%20Vikram%20Rao%20and%20Xuanhui%20Wang%20and%20Yaroslav%20Akulov%20and%20Yulong%20Yang%20and%20Zhen%20Yang%20and%20Zhixin%20Lai%20and%20Zhongru%20Wu%20and%20Anca%20Dragan%20and%20Avinatan%20Hassidim%20and%20Fernando%20Pereira%20and%20Slav%20Petrov%20and%20Srinivasan%20Venkatachary%20and%20Tulsee%20Doshi%20and%20Yossi%20Matias%20and%20Sasha%20Goldshtein%20and%20Dipanjan%20Das%0AAbstract%3A%20We%20introduce%20The%20FACTS%20Leaderboard%2C%20an%20online%20leaderboard%20suite%20and%20associated%20set%20of%20benchmarks%20that%20comprehensively%20evaluates%20the%20ability%20of%20language%20models%20to%20generate%20factually%20accurate%20text%20across%20diverse%20scenarios.%20The%20suite%20provides%20a%20holistic%20measure%20of%20factuality%20by%20aggregating%20the%20performance%20of%20models%20on%20four%20distinct%20sub-leaderboards%3A%20%281%29%20FACTS%20Multimodal%2C%20which%20measures%20the%20factuality%20of%20responses%20to%20image-based%20questions%3B%20%282%29%20FACTS%20Parametric%2C%20which%20assesses%20models%27%20world%20knowledge%20by%20answering%20closed-book%20factoid%20questions%20from%20internal%20parameters%3B%20%283%29%20FACTS%20Search%2C%20which%20evaluates%20factuality%20in%20information-seeking%20scenarios%2C%20where%20the%20model%20must%20use%20a%20search%20API%3B%20and%20%284%29%20FACTS%20Grounding%20%28v2%29%2C%20which%20evaluates%20whether%20long-form%20responses%20are%20grounded%20in%20provided%20documents%2C%20featuring%20significantly%20improved%20judge%20models.%20Each%20sub-leaderboard%20employs%20automated%20judge%20models%20to%20score%20model%20responses%2C%20and%20the%20final%20suite%20score%20is%20an%20average%20of%20the%20four%20components%2C%20designed%20to%20provide%20a%20robust%20and%20balanced%20assessment%20of%20a%20model%27s%20overall%20factuality.%20The%20FACTS%20Leaderboard%20Suite%20will%20be%20actively%20maintained%2C%20containing%20both%20public%20and%20private%20splits%20to%20allow%20for%20external%20participation%20while%20guarding%20its%20integrity.%20It%20can%20be%20found%20at%20https%3A//www.kaggle.com/benchmarks/google/facts%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520FACTS%2520Leaderboard%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Large%2520Language%2520Model%2520Factuality%26entry.906535625%3DAileen%2520Cheng%2520and%2520Alon%2520Jacovi%2520and%2520Amir%2520Globerson%2520and%2520Ben%2520Golan%2520and%2520Charles%2520Kwong%2520and%2520Chris%2520Alberti%2520and%2520Connie%2520Tao%2520and%2520Eyal%2520Ben-David%2520and%2520Gaurav%2520Singh%2520Tomar%2520and%2520Lukas%2520Haas%2520and%2520Yonatan%2520Bitton%2520and%2520Adam%2520Bloniarz%2520and%2520Aijun%2520Bai%2520and%2520Andrew%2520Wang%2520and%2520Anfal%2520Siddiqui%2520and%2520Arturo%2520Bajuelos%2520Castillo%2520and%2520Aviel%2520Atias%2520and%2520Chang%2520Liu%2520and%2520Corey%2520Fry%2520and%2520Daniel%2520Balle%2520and%2520Deepanway%2520Ghosal%2520and%2520Doron%2520Kukliansky%2520and%2520Dror%2520Marcus%2520and%2520Elena%2520Gribovskaya%2520and%2520Eran%2520Ofek%2520and%2520Honglei%2520Zhuang%2520and%2520Itay%2520Laish%2520and%2520Jan%2520Ackermann%2520and%2520Lily%2520Wang%2520and%2520Meg%2520Risdal%2520and%2520Megan%2520Barnes%2520and%2520Michael%2520Fink%2520and%2520Mohamed%2520Amin%2520and%2520Moran%2520Ambar%2520and%2520Natan%2520Potikha%2520and%2520Nikita%2520Gupta%2520and%2520Nitzan%2520Katz%2520and%2520Noam%2520Velan%2520and%2520Ofir%2520Roval%2520and%2520Ori%2520Ram%2520and%2520Polina%2520Zablotskaia%2520and%2520Prathamesh%2520Bang%2520and%2520Priyanka%2520Agrawal%2520and%2520Rakesh%2520Ghiya%2520and%2520Sanjay%2520Ganapathy%2520and%2520Simon%2520Baumgartner%2520and%2520Sofia%2520Erell%2520and%2520Sushant%2520Prakash%2520and%2520Thibault%2520Sellam%2520and%2520Vikram%2520Rao%2520and%2520Xuanhui%2520Wang%2520and%2520Yaroslav%2520Akulov%2520and%2520Yulong%2520Yang%2520and%2520Zhen%2520Yang%2520and%2520Zhixin%2520Lai%2520and%2520Zhongru%2520Wu%2520and%2520Anca%2520Dragan%2520and%2520Avinatan%2520Hassidim%2520and%2520Fernando%2520Pereira%2520and%2520Slav%2520Petrov%2520and%2520Srinivasan%2520Venkatachary%2520and%2520Tulsee%2520Doshi%2520and%2520Yossi%2520Matias%2520and%2520Sasha%2520Goldshtein%2520and%2520Dipanjan%2520Das%26entry.1292438233%3DWe%2520introduce%2520The%2520FACTS%2520Leaderboard%252C%2520an%2520online%2520leaderboard%2520suite%2520and%2520associated%2520set%2520of%2520benchmarks%2520that%2520comprehensively%2520evaluates%2520the%2520ability%2520of%2520language%2520models%2520to%2520generate%2520factually%2520accurate%2520text%2520across%2520diverse%2520scenarios.%2520The%2520suite%2520provides%2520a%2520holistic%2520measure%2520of%2520factuality%2520by%2520aggregating%2520the%2520performance%2520of%2520models%2520on%2520four%2520distinct%2520sub-leaderboards%253A%2520%25281%2529%2520FACTS%2520Multimodal%252C%2520which%2520measures%2520the%2520factuality%2520of%2520responses%2520to%2520image-based%2520questions%253B%2520%25282%2529%2520FACTS%2520Parametric%252C%2520which%2520assesses%2520models%2527%2520world%2520knowledge%2520by%2520answering%2520closed-book%2520factoid%2520questions%2520from%2520internal%2520parameters%253B%2520%25283%2529%2520FACTS%2520Search%252C%2520which%2520evaluates%2520factuality%2520in%2520information-seeking%2520scenarios%252C%2520where%2520the%2520model%2520must%2520use%2520a%2520search%2520API%253B%2520and%2520%25284%2529%2520FACTS%2520Grounding%2520%2528v2%2529%252C%2520which%2520evaluates%2520whether%2520long-form%2520responses%2520are%2520grounded%2520in%2520provided%2520documents%252C%2520featuring%2520significantly%2520improved%2520judge%2520models.%2520Each%2520sub-leaderboard%2520employs%2520automated%2520judge%2520models%2520to%2520score%2520model%2520responses%252C%2520and%2520the%2520final%2520suite%2520score%2520is%2520an%2520average%2520of%2520the%2520four%2520components%252C%2520designed%2520to%2520provide%2520a%2520robust%2520and%2520balanced%2520assessment%2520of%2520a%2520model%2527s%2520overall%2520factuality.%2520The%2520FACTS%2520Leaderboard%2520Suite%2520will%2520be%2520actively%2520maintained%252C%2520containing%2520both%2520public%2520and%2520private%2520splits%2520to%2520allow%2520for%2520external%2520participation%2520while%2520guarding%2520its%2520integrity.%2520It%2520can%2520be%2520found%2520at%2520https%253A//www.kaggle.com/benchmarks/google/facts%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20FACTS%20Leaderboard%3A%20A%20Comprehensive%20Benchmark%20for%20Large%20Language%20Model%20Factuality&entry.906535625=Aileen%20Cheng%20and%20Alon%20Jacovi%20and%20Amir%20Globerson%20and%20Ben%20Golan%20and%20Charles%20Kwong%20and%20Chris%20Alberti%20and%20Connie%20Tao%20and%20Eyal%20Ben-David%20and%20Gaurav%20Singh%20Tomar%20and%20Lukas%20Haas%20and%20Yonatan%20Bitton%20and%20Adam%20Bloniarz%20and%20Aijun%20Bai%20and%20Andrew%20Wang%20and%20Anfal%20Siddiqui%20and%20Arturo%20Bajuelos%20Castillo%20and%20Aviel%20Atias%20and%20Chang%20Liu%20and%20Corey%20Fry%20and%20Daniel%20Balle%20and%20Deepanway%20Ghosal%20and%20Doron%20Kukliansky%20and%20Dror%20Marcus%20and%20Elena%20Gribovskaya%20and%20Eran%20Ofek%20and%20Honglei%20Zhuang%20and%20Itay%20Laish%20and%20Jan%20Ackermann%20and%20Lily%20Wang%20and%20Meg%20Risdal%20and%20Megan%20Barnes%20and%20Michael%20Fink%20and%20Mohamed%20Amin%20and%20Moran%20Ambar%20and%20Natan%20Potikha%20and%20Nikita%20Gupta%20and%20Nitzan%20Katz%20and%20Noam%20Velan%20and%20Ofir%20Roval%20and%20Ori%20Ram%20and%20Polina%20Zablotskaia%20and%20Prathamesh%20Bang%20and%20Priyanka%20Agrawal%20and%20Rakesh%20Ghiya%20and%20Sanjay%20Ganapathy%20and%20Simon%20Baumgartner%20and%20Sofia%20Erell%20and%20Sushant%20Prakash%20and%20Thibault%20Sellam%20and%20Vikram%20Rao%20and%20Xuanhui%20Wang%20and%20Yaroslav%20Akulov%20and%20Yulong%20Yang%20and%20Zhen%20Yang%20and%20Zhixin%20Lai%20and%20Zhongru%20Wu%20and%20Anca%20Dragan%20and%20Avinatan%20Hassidim%20and%20Fernando%20Pereira%20and%20Slav%20Petrov%20and%20Srinivasan%20Venkatachary%20and%20Tulsee%20Doshi%20and%20Yossi%20Matias%20and%20Sasha%20Goldshtein%20and%20Dipanjan%20Das&entry.1292438233=We%20introduce%20The%20FACTS%20Leaderboard%2C%20an%20online%20leaderboard%20suite%20and%20associated%20set%20of%20benchmarks%20that%20comprehensively%20evaluates%20the%20ability%20of%20language%20models%20to%20generate%20factually%20accurate%20text%20across%20diverse%20scenarios.%20The%20suite%20provides%20a%20holistic%20measure%20of%20factuality%20by%20aggregating%20the%20performance%20of%20models%20on%20four%20distinct%20sub-leaderboards%3A%20%281%29%20FACTS%20Multimodal%2C%20which%20measures%20the%20factuality%20of%20responses%20to%20image-based%20questions%3B%20%282%29%20FACTS%20Parametric%2C%20which%20assesses%20models%27%20world%20knowledge%20by%20answering%20closed-book%20factoid%20questions%20from%20internal%20parameters%3B%20%283%29%20FACTS%20Search%2C%20which%20evaluates%20factuality%20in%20information-seeking%20scenarios%2C%20where%20the%20model%20must%20use%20a%20search%20API%3B%20and%20%284%29%20FACTS%20Grounding%20%28v2%29%2C%20which%20evaluates%20whether%20long-form%20responses%20are%20grounded%20in%20provided%20documents%2C%20featuring%20significantly%20improved%20judge%20models.%20Each%20sub-leaderboard%20employs%20automated%20judge%20models%20to%20score%20model%20responses%2C%20and%20the%20final%20suite%20score%20is%20an%20average%20of%20the%20four%20components%2C%20designed%20to%20provide%20a%20robust%20and%20balanced%20assessment%20of%20a%20model%27s%20overall%20factuality.%20The%20FACTS%20Leaderboard%20Suite%20will%20be%20actively%20maintained%2C%20containing%20both%20public%20and%20private%20splits%20to%20allow%20for%20external%20participation%20while%20guarding%20its%20integrity.%20It%20can%20be%20found%20at%20https%3A//www.kaggle.com/benchmarks/google/facts%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.10791v1&entry.124074799=Read"},
{"title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation", "author": "Henghui Ding and Chang Liu and Shuting He and Kaining Ying and Xudong Jiang and Chen Change Loy and Yu-Gang Jiang", "abstract": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/", "link": "http://arxiv.org/abs/2512.10945v1", "date": "2025-12-11", "relevancy": 2.1869, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeViS%3A%20A%20Multi-Modal%20Dataset%20for%20Referring%20Motion%20Expression%20Video%20Segmentation&body=Title%3A%20MeViS%3A%20A%20Multi-Modal%20Dataset%20for%20Referring%20Motion%20Expression%20Video%20Segmentation%0AAuthor%3A%20Henghui%20Ding%20and%20Chang%20Liu%20and%20Shuting%20He%20and%20Kaining%20Ying%20and%20Xudong%20Jiang%20and%20Chen%20Change%20Loy%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20This%20paper%20proposes%20a%20large-scale%20multi-modal%20dataset%20for%20referring%20motion%20expression%20video%20segmentation%2C%20focusing%20on%20segmenting%20and%20tracking%20target%20objects%20in%20videos%20based%20on%20language%20description%20of%20objects%27%20motions.%20Existing%20referring%20video%20segmentation%20datasets%20often%20focus%20on%20salient%20objects%20and%20use%20language%20expressions%20rich%20in%20static%20attributes%2C%20potentially%20allowing%20the%20target%20object%20to%20be%20identified%20in%20a%20single%20frame.%20Such%20datasets%20underemphasize%20the%20role%20of%20motion%20in%20both%20videos%20and%20languages.%20To%20explore%20the%20feasibility%20of%20using%20motion%20expressions%20and%20motion%20reasoning%20clues%20for%20pixel-level%20video%20understanding%2C%20we%20introduce%20MeViS%2C%20a%20dataset%20containing%2033%2C072%20human-annotated%20motion%20expressions%20in%20both%20text%20and%20audio%2C%20covering%208%2C171%20objects%20in%202%2C006%20videos%20of%20complex%20scenarios.%20We%20benchmark%2015%20existing%20methods%20across%204%20tasks%20supported%20by%20MeViS%2C%20including%206%20referring%20video%20object%20segmentation%20%28RVOS%29%20methods%2C%203%20audio-guided%20video%20object%20segmentation%20%28AVOS%29%20methods%2C%202%20referring%20multi-object%20tracking%20%28RMOT%29%20methods%2C%20and%204%20video%20captioning%20methods%20for%20the%20newly%20introduced%20referring%20motion%20expression%20generation%20%28RMEG%29%20task.%20The%20results%20demonstrate%20weaknesses%20and%20limitations%20of%20existing%20methods%20in%20addressing%20motion%20expression-guided%20video%20understanding.%20We%20further%20analyze%20the%20challenges%20and%20propose%20an%20approach%20LMPM%2B%2B%20for%20RVOS/AVOS/RMOT%20that%20achieves%20new%20state-of-the-art%20results.%20Our%20dataset%20provides%20a%20platform%20that%20facilitates%20the%20development%20of%20motion%20expression-guided%20video%20understanding%20algorithms%20in%20complex%20video%20scenes.%20The%20proposed%20MeViS%20dataset%20and%20the%20method%27s%20source%20code%20are%20publicly%20available%20at%20https%3A//henghuiding.com/MeViS/%0ALink%3A%20http%3A//arxiv.org/abs/2512.10945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeViS%253A%2520A%2520Multi-Modal%2520Dataset%2520for%2520Referring%2520Motion%2520Expression%2520Video%2520Segmentation%26entry.906535625%3DHenghui%2520Ding%2520and%2520Chang%2520Liu%2520and%2520Shuting%2520He%2520and%2520Kaining%2520Ying%2520and%2520Xudong%2520Jiang%2520and%2520Chen%2520Change%2520Loy%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%2520large-scale%2520multi-modal%2520dataset%2520for%2520referring%2520motion%2520expression%2520video%2520segmentation%252C%2520focusing%2520on%2520segmenting%2520and%2520tracking%2520target%2520objects%2520in%2520videos%2520based%2520on%2520language%2520description%2520of%2520objects%2527%2520motions.%2520Existing%2520referring%2520video%2520segmentation%2520datasets%2520often%2520focus%2520on%2520salient%2520objects%2520and%2520use%2520language%2520expressions%2520rich%2520in%2520static%2520attributes%252C%2520potentially%2520allowing%2520the%2520target%2520object%2520to%2520be%2520identified%2520in%2520a%2520single%2520frame.%2520Such%2520datasets%2520underemphasize%2520the%2520role%2520of%2520motion%2520in%2520both%2520videos%2520and%2520languages.%2520To%2520explore%2520the%2520feasibility%2520of%2520using%2520motion%2520expressions%2520and%2520motion%2520reasoning%2520clues%2520for%2520pixel-level%2520video%2520understanding%252C%2520we%2520introduce%2520MeViS%252C%2520a%2520dataset%2520containing%252033%252C072%2520human-annotated%2520motion%2520expressions%2520in%2520both%2520text%2520and%2520audio%252C%2520covering%25208%252C171%2520objects%2520in%25202%252C006%2520videos%2520of%2520complex%2520scenarios.%2520We%2520benchmark%252015%2520existing%2520methods%2520across%25204%2520tasks%2520supported%2520by%2520MeViS%252C%2520including%25206%2520referring%2520video%2520object%2520segmentation%2520%2528RVOS%2529%2520methods%252C%25203%2520audio-guided%2520video%2520object%2520segmentation%2520%2528AVOS%2529%2520methods%252C%25202%2520referring%2520multi-object%2520tracking%2520%2528RMOT%2529%2520methods%252C%2520and%25204%2520video%2520captioning%2520methods%2520for%2520the%2520newly%2520introduced%2520referring%2520motion%2520expression%2520generation%2520%2528RMEG%2529%2520task.%2520The%2520results%2520demonstrate%2520weaknesses%2520and%2520limitations%2520of%2520existing%2520methods%2520in%2520addressing%2520motion%2520expression-guided%2520video%2520understanding.%2520We%2520further%2520analyze%2520the%2520challenges%2520and%2520propose%2520an%2520approach%2520LMPM%252B%252B%2520for%2520RVOS/AVOS/RMOT%2520that%2520achieves%2520new%2520state-of-the-art%2520results.%2520Our%2520dataset%2520provides%2520a%2520platform%2520that%2520facilitates%2520the%2520development%2520of%2520motion%2520expression-guided%2520video%2520understanding%2520algorithms%2520in%2520complex%2520video%2520scenes.%2520The%2520proposed%2520MeViS%2520dataset%2520and%2520the%2520method%2527s%2520source%2520code%2520are%2520publicly%2520available%2520at%2520https%253A//henghuiding.com/MeViS/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeViS%3A%20A%20Multi-Modal%20Dataset%20for%20Referring%20Motion%20Expression%20Video%20Segmentation&entry.906535625=Henghui%20Ding%20and%20Chang%20Liu%20and%20Shuting%20He%20and%20Kaining%20Ying%20and%20Xudong%20Jiang%20and%20Chen%20Change%20Loy%20and%20Yu-Gang%20Jiang&entry.1292438233=This%20paper%20proposes%20a%20large-scale%20multi-modal%20dataset%20for%20referring%20motion%20expression%20video%20segmentation%2C%20focusing%20on%20segmenting%20and%20tracking%20target%20objects%20in%20videos%20based%20on%20language%20description%20of%20objects%27%20motions.%20Existing%20referring%20video%20segmentation%20datasets%20often%20focus%20on%20salient%20objects%20and%20use%20language%20expressions%20rich%20in%20static%20attributes%2C%20potentially%20allowing%20the%20target%20object%20to%20be%20identified%20in%20a%20single%20frame.%20Such%20datasets%20underemphasize%20the%20role%20of%20motion%20in%20both%20videos%20and%20languages.%20To%20explore%20the%20feasibility%20of%20using%20motion%20expressions%20and%20motion%20reasoning%20clues%20for%20pixel-level%20video%20understanding%2C%20we%20introduce%20MeViS%2C%20a%20dataset%20containing%2033%2C072%20human-annotated%20motion%20expressions%20in%20both%20text%20and%20audio%2C%20covering%208%2C171%20objects%20in%202%2C006%20videos%20of%20complex%20scenarios.%20We%20benchmark%2015%20existing%20methods%20across%204%20tasks%20supported%20by%20MeViS%2C%20including%206%20referring%20video%20object%20segmentation%20%28RVOS%29%20methods%2C%203%20audio-guided%20video%20object%20segmentation%20%28AVOS%29%20methods%2C%202%20referring%20multi-object%20tracking%20%28RMOT%29%20methods%2C%20and%204%20video%20captioning%20methods%20for%20the%20newly%20introduced%20referring%20motion%20expression%20generation%20%28RMEG%29%20task.%20The%20results%20demonstrate%20weaknesses%20and%20limitations%20of%20existing%20methods%20in%20addressing%20motion%20expression-guided%20video%20understanding.%20We%20further%20analyze%20the%20challenges%20and%20propose%20an%20approach%20LMPM%2B%2B%20for%20RVOS/AVOS/RMOT%20that%20achieves%20new%20state-of-the-art%20results.%20Our%20dataset%20provides%20a%20platform%20that%20facilitates%20the%20development%20of%20motion%20expression-guided%20video%20understanding%20algorithms%20in%20complex%20video%20scenes.%20The%20proposed%20MeViS%20dataset%20and%20the%20method%27s%20source%20code%20are%20publicly%20available%20at%20https%3A//henghuiding.com/MeViS/&entry.1838667208=http%3A//arxiv.org/abs/2512.10945v1&entry.124074799=Read"},
{"title": "Bidirectional Normalizing Flow: From Data to Noise and Back", "author": "Yiyang Lu and Qiao Sun and Xianbang Wang and Zhicheng Jiang and Hanhong Zhao and Kaiming He", "abstract": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.", "link": "http://arxiv.org/abs/2512.10953v1", "date": "2025-12-11", "relevancy": 2.161, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6766}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5361}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bidirectional%20Normalizing%20Flow%3A%20From%20Data%20to%20Noise%20and%20Back&body=Title%3A%20Bidirectional%20Normalizing%20Flow%3A%20From%20Data%20to%20Noise%20and%20Back%0AAuthor%3A%20Yiyang%20Lu%20and%20Qiao%20Sun%20and%20Xianbang%20Wang%20and%20Zhicheng%20Jiang%20and%20Hanhong%20Zhao%20and%20Kaiming%20He%0AAbstract%3A%20Normalizing%20Flows%20%28NFs%29%20have%20been%20established%20as%20a%20principled%20framework%20for%20generative%20modeling.%20Standard%20NFs%20consist%20of%20a%20forward%20process%20and%20a%20reverse%20process%3A%20the%20forward%20process%20maps%20data%20to%20noise%2C%20while%20the%20reverse%20process%20generates%20samples%20by%20inverting%20it.%20Typical%20NF%20forward%20transformations%20are%20constrained%20by%20explicit%20invertibility%2C%20ensuring%20that%20the%20reverse%20process%20can%20serve%20as%20their%20exact%20analytic%20inverse.%20Recent%20developments%20in%20TARFlow%20and%20its%20variants%20have%20revitalized%20NF%20methods%20by%20combining%20Transformers%20and%20autoregressive%20flows%2C%20but%20have%20also%20exposed%20causal%20decoding%20as%20a%20major%20bottleneck.%20In%20this%20work%2C%20we%20introduce%20Bidirectional%20Normalizing%20Flow%20%28%24%5Ctextbf%7BBiFlow%7D%24%29%2C%20a%20framework%20that%20removes%20the%20need%20for%20an%20exact%20analytic%20inverse.%20BiFlow%20learns%20a%20reverse%20model%20that%20approximates%20the%20underlying%20noise-to-data%20inverse%20mapping%2C%20enabling%20more%20flexible%20loss%20functions%20and%20architectures.%20Experiments%20on%20ImageNet%20demonstrate%20that%20BiFlow%2C%20compared%20to%20its%20causal%20decoding%20counterpart%2C%20improves%20generation%20quality%20while%20accelerating%20sampling%20by%20up%20to%20two%20orders%20of%20magnitude.%20BiFlow%20yields%20state-of-the-art%20results%20among%20NF-based%20methods%20and%20competitive%20performance%20among%20single-evaluation%20%28%221-NFE%22%29%20methods.%20Following%20recent%20encouraging%20progress%20on%20NFs%2C%20we%20hope%20our%20work%20will%20draw%20further%20attention%20to%20this%20classical%20paradigm.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBidirectional%2520Normalizing%2520Flow%253A%2520From%2520Data%2520to%2520Noise%2520and%2520Back%26entry.906535625%3DYiyang%2520Lu%2520and%2520Qiao%2520Sun%2520and%2520Xianbang%2520Wang%2520and%2520Zhicheng%2520Jiang%2520and%2520Hanhong%2520Zhao%2520and%2520Kaiming%2520He%26entry.1292438233%3DNormalizing%2520Flows%2520%2528NFs%2529%2520have%2520been%2520established%2520as%2520a%2520principled%2520framework%2520for%2520generative%2520modeling.%2520Standard%2520NFs%2520consist%2520of%2520a%2520forward%2520process%2520and%2520a%2520reverse%2520process%253A%2520the%2520forward%2520process%2520maps%2520data%2520to%2520noise%252C%2520while%2520the%2520reverse%2520process%2520generates%2520samples%2520by%2520inverting%2520it.%2520Typical%2520NF%2520forward%2520transformations%2520are%2520constrained%2520by%2520explicit%2520invertibility%252C%2520ensuring%2520that%2520the%2520reverse%2520process%2520can%2520serve%2520as%2520their%2520exact%2520analytic%2520inverse.%2520Recent%2520developments%2520in%2520TARFlow%2520and%2520its%2520variants%2520have%2520revitalized%2520NF%2520methods%2520by%2520combining%2520Transformers%2520and%2520autoregressive%2520flows%252C%2520but%2520have%2520also%2520exposed%2520causal%2520decoding%2520as%2520a%2520major%2520bottleneck.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Bidirectional%2520Normalizing%2520Flow%2520%2528%2524%255Ctextbf%257BBiFlow%257D%2524%2529%252C%2520a%2520framework%2520that%2520removes%2520the%2520need%2520for%2520an%2520exact%2520analytic%2520inverse.%2520BiFlow%2520learns%2520a%2520reverse%2520model%2520that%2520approximates%2520the%2520underlying%2520noise-to-data%2520inverse%2520mapping%252C%2520enabling%2520more%2520flexible%2520loss%2520functions%2520and%2520architectures.%2520Experiments%2520on%2520ImageNet%2520demonstrate%2520that%2520BiFlow%252C%2520compared%2520to%2520its%2520causal%2520decoding%2520counterpart%252C%2520improves%2520generation%2520quality%2520while%2520accelerating%2520sampling%2520by%2520up%2520to%2520two%2520orders%2520of%2520magnitude.%2520BiFlow%2520yields%2520state-of-the-art%2520results%2520among%2520NF-based%2520methods%2520and%2520competitive%2520performance%2520among%2520single-evaluation%2520%2528%25221-NFE%2522%2529%2520methods.%2520Following%2520recent%2520encouraging%2520progress%2520on%2520NFs%252C%2520we%2520hope%2520our%2520work%2520will%2520draw%2520further%2520attention%2520to%2520this%2520classical%2520paradigm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidirectional%20Normalizing%20Flow%3A%20From%20Data%20to%20Noise%20and%20Back&entry.906535625=Yiyang%20Lu%20and%20Qiao%20Sun%20and%20Xianbang%20Wang%20and%20Zhicheng%20Jiang%20and%20Hanhong%20Zhao%20and%20Kaiming%20He&entry.1292438233=Normalizing%20Flows%20%28NFs%29%20have%20been%20established%20as%20a%20principled%20framework%20for%20generative%20modeling.%20Standard%20NFs%20consist%20of%20a%20forward%20process%20and%20a%20reverse%20process%3A%20the%20forward%20process%20maps%20data%20to%20noise%2C%20while%20the%20reverse%20process%20generates%20samples%20by%20inverting%20it.%20Typical%20NF%20forward%20transformations%20are%20constrained%20by%20explicit%20invertibility%2C%20ensuring%20that%20the%20reverse%20process%20can%20serve%20as%20their%20exact%20analytic%20inverse.%20Recent%20developments%20in%20TARFlow%20and%20its%20variants%20have%20revitalized%20NF%20methods%20by%20combining%20Transformers%20and%20autoregressive%20flows%2C%20but%20have%20also%20exposed%20causal%20decoding%20as%20a%20major%20bottleneck.%20In%20this%20work%2C%20we%20introduce%20Bidirectional%20Normalizing%20Flow%20%28%24%5Ctextbf%7BBiFlow%7D%24%29%2C%20a%20framework%20that%20removes%20the%20need%20for%20an%20exact%20analytic%20inverse.%20BiFlow%20learns%20a%20reverse%20model%20that%20approximates%20the%20underlying%20noise-to-data%20inverse%20mapping%2C%20enabling%20more%20flexible%20loss%20functions%20and%20architectures.%20Experiments%20on%20ImageNet%20demonstrate%20that%20BiFlow%2C%20compared%20to%20its%20causal%20decoding%20counterpart%2C%20improves%20generation%20quality%20while%20accelerating%20sampling%20by%20up%20to%20two%20orders%20of%20magnitude.%20BiFlow%20yields%20state-of-the-art%20results%20among%20NF-based%20methods%20and%20competitive%20performance%20among%20single-evaluation%20%28%221-NFE%22%29%20methods.%20Following%20recent%20encouraging%20progress%20on%20NFs%2C%20we%20hope%20our%20work%20will%20draw%20further%20attention%20to%20this%20classical%20paradigm.&entry.1838667208=http%3A//arxiv.org/abs/2512.10953v1&entry.124074799=Read"},
{"title": "Luxical: High-Speed Lexical-Dense Text Embeddings", "author": " DatologyAI and  : and Luke Merrick and Alex Fang and Aldo Carranza and Alvin Deng and Amro Abbas and Brett Larsen and Cody Blakeney and Darren Teh and David Schwab and Fan Pan and Haakon Mongstad and Haoli Yin and Jack Urbanek and Jason Lee and Jason Telanoff and Josh Wills and Kaleigh Mentzer and Paul Burstein and Parth Doshi and Paul Burnstein and Pratyush Maini and Ricardo Monti and Rishabh Adiga and Scott Loftin and Siddharth Joshi and Spandan Das and Tony Jiang and Vineeth Dorna and Zhengping Wang and Bogdan Gaza and Ari Morcos and Matthew Leavitt", "abstract": "Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed \"lexical-dense\" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.", "link": "http://arxiv.org/abs/2512.09015v2", "date": "2025-12-11", "relevancy": 2.1558, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Luxical%3A%20High-Speed%20Lexical-Dense%20Text%20Embeddings&body=Title%3A%20Luxical%3A%20High-Speed%20Lexical-Dense%20Text%20Embeddings%0AAuthor%3A%20%20DatologyAI%20and%20%20%3A%20and%20Luke%20Merrick%20and%20Alex%20Fang%20and%20Aldo%20Carranza%20and%20Alvin%20Deng%20and%20Amro%20Abbas%20and%20Brett%20Larsen%20and%20Cody%20Blakeney%20and%20Darren%20Teh%20and%20David%20Schwab%20and%20Fan%20Pan%20and%20Haakon%20Mongstad%20and%20Haoli%20Yin%20and%20Jack%20Urbanek%20and%20Jason%20Lee%20and%20Jason%20Telanoff%20and%20Josh%20Wills%20and%20Kaleigh%20Mentzer%20and%20Paul%20Burstein%20and%20Parth%20Doshi%20and%20Paul%20Burnstein%20and%20Pratyush%20Maini%20and%20Ricardo%20Monti%20and%20Rishabh%20Adiga%20and%20Scott%20Loftin%20and%20Siddharth%20Joshi%20and%20Spandan%20Das%20and%20Tony%20Jiang%20and%20Vineeth%20Dorna%20and%20Zhengping%20Wang%20and%20Bogdan%20Gaza%20and%20Ari%20Morcos%20and%20Matthew%20Leavitt%0AAbstract%3A%20Frontier%20language%20model%20quality%20increasingly%20hinges%20on%20our%20ability%20to%20organize%20web-scale%20text%20corpora%20for%20training.%20Today%27s%20dominant%20tools%20trade%20off%20speed%20and%20flexibility%3A%20lexical%20classifiers%20%28e.g.%2C%20FastText%29%20are%20fast%20but%20limited%20to%20producing%20classification%20output%20scores%2C%20while%20the%20vector-valued%20outputs%20of%20transformer%20text%20embedding%20models%20flexibly%20support%20numerous%20workflows%20%28e.g.%2C%20clustering%2C%20classification%2C%20and%20retrieval%29%20but%20are%20computationally%20expensive%20to%20produce.%20We%20introduce%20Luxical%2C%20a%20library%20for%20high-speed%20%22lexical-dense%22%20text%20embeddings%20that%20aims%20to%20recover%20the%20best%20properties%20of%20both%20approaches%20for%20web-scale%20text%20organization.%20Luxical%20combines%20sparse%20TF--IDF%20features%2C%20a%20small%20ReLU%20network%2C%20and%20a%20knowledge%20distillation%20training%20regimen%20to%20approximate%20large%20transformer%20embedding%20models%20at%20a%20fraction%20of%20their%20operational%20cost.%20In%20this%20technical%20report%2C%20we%20describe%20the%20Luxical%20architecture%20and%20training%20objective%20and%20evaluate%20a%20concrete%20Luxical%20model%20in%20two%20disparate%20applications%3A%20a%20targeted%20webcrawl%20document%20retrieval%20test%20and%20an%20end-to-end%20language%20model%20data%20curation%20task%20grounded%20in%20text%20classification.%20In%20these%20tasks%20we%20demonstrate%20speedups%20ranging%20from%203x%20to%20100x%20over%20varying-sized%20neural%20baselines%2C%20and%20comparable%20to%20FastText%20model%20inference%20during%20the%20data%20curation%20task.%20On%20these%20evaluations%2C%20the%20tested%20Luxical%20model%20illustrates%20favorable%20compute/quality%20trade-offs%20for%20large-scale%20text%20organization%2C%20matching%20the%20quality%20of%20neural%20baselines.%20Luxical%20is%20available%20as%20open-source%20software%20at%20https%3A//github.com/datologyai/luxical.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLuxical%253A%2520High-Speed%2520Lexical-Dense%2520Text%2520Embeddings%26entry.906535625%3D%2520DatologyAI%2520and%2520%2520%253A%2520and%2520Luke%2520Merrick%2520and%2520Alex%2520Fang%2520and%2520Aldo%2520Carranza%2520and%2520Alvin%2520Deng%2520and%2520Amro%2520Abbas%2520and%2520Brett%2520Larsen%2520and%2520Cody%2520Blakeney%2520and%2520Darren%2520Teh%2520and%2520David%2520Schwab%2520and%2520Fan%2520Pan%2520and%2520Haakon%2520Mongstad%2520and%2520Haoli%2520Yin%2520and%2520Jack%2520Urbanek%2520and%2520Jason%2520Lee%2520and%2520Jason%2520Telanoff%2520and%2520Josh%2520Wills%2520and%2520Kaleigh%2520Mentzer%2520and%2520Paul%2520Burstein%2520and%2520Parth%2520Doshi%2520and%2520Paul%2520Burnstein%2520and%2520Pratyush%2520Maini%2520and%2520Ricardo%2520Monti%2520and%2520Rishabh%2520Adiga%2520and%2520Scott%2520Loftin%2520and%2520Siddharth%2520Joshi%2520and%2520Spandan%2520Das%2520and%2520Tony%2520Jiang%2520and%2520Vineeth%2520Dorna%2520and%2520Zhengping%2520Wang%2520and%2520Bogdan%2520Gaza%2520and%2520Ari%2520Morcos%2520and%2520Matthew%2520Leavitt%26entry.1292438233%3DFrontier%2520language%2520model%2520quality%2520increasingly%2520hinges%2520on%2520our%2520ability%2520to%2520organize%2520web-scale%2520text%2520corpora%2520for%2520training.%2520Today%2527s%2520dominant%2520tools%2520trade%2520off%2520speed%2520and%2520flexibility%253A%2520lexical%2520classifiers%2520%2528e.g.%252C%2520FastText%2529%2520are%2520fast%2520but%2520limited%2520to%2520producing%2520classification%2520output%2520scores%252C%2520while%2520the%2520vector-valued%2520outputs%2520of%2520transformer%2520text%2520embedding%2520models%2520flexibly%2520support%2520numerous%2520workflows%2520%2528e.g.%252C%2520clustering%252C%2520classification%252C%2520and%2520retrieval%2529%2520but%2520are%2520computationally%2520expensive%2520to%2520produce.%2520We%2520introduce%2520Luxical%252C%2520a%2520library%2520for%2520high-speed%2520%2522lexical-dense%2522%2520text%2520embeddings%2520that%2520aims%2520to%2520recover%2520the%2520best%2520properties%2520of%2520both%2520approaches%2520for%2520web-scale%2520text%2520organization.%2520Luxical%2520combines%2520sparse%2520TF--IDF%2520features%252C%2520a%2520small%2520ReLU%2520network%252C%2520and%2520a%2520knowledge%2520distillation%2520training%2520regimen%2520to%2520approximate%2520large%2520transformer%2520embedding%2520models%2520at%2520a%2520fraction%2520of%2520their%2520operational%2520cost.%2520In%2520this%2520technical%2520report%252C%2520we%2520describe%2520the%2520Luxical%2520architecture%2520and%2520training%2520objective%2520and%2520evaluate%2520a%2520concrete%2520Luxical%2520model%2520in%2520two%2520disparate%2520applications%253A%2520a%2520targeted%2520webcrawl%2520document%2520retrieval%2520test%2520and%2520an%2520end-to-end%2520language%2520model%2520data%2520curation%2520task%2520grounded%2520in%2520text%2520classification.%2520In%2520these%2520tasks%2520we%2520demonstrate%2520speedups%2520ranging%2520from%25203x%2520to%2520100x%2520over%2520varying-sized%2520neural%2520baselines%252C%2520and%2520comparable%2520to%2520FastText%2520model%2520inference%2520during%2520the%2520data%2520curation%2520task.%2520On%2520these%2520evaluations%252C%2520the%2520tested%2520Luxical%2520model%2520illustrates%2520favorable%2520compute/quality%2520trade-offs%2520for%2520large-scale%2520text%2520organization%252C%2520matching%2520the%2520quality%2520of%2520neural%2520baselines.%2520Luxical%2520is%2520available%2520as%2520open-source%2520software%2520at%2520https%253A//github.com/datologyai/luxical.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Luxical%3A%20High-Speed%20Lexical-Dense%20Text%20Embeddings&entry.906535625=%20DatologyAI%20and%20%20%3A%20and%20Luke%20Merrick%20and%20Alex%20Fang%20and%20Aldo%20Carranza%20and%20Alvin%20Deng%20and%20Amro%20Abbas%20and%20Brett%20Larsen%20and%20Cody%20Blakeney%20and%20Darren%20Teh%20and%20David%20Schwab%20and%20Fan%20Pan%20and%20Haakon%20Mongstad%20and%20Haoli%20Yin%20and%20Jack%20Urbanek%20and%20Jason%20Lee%20and%20Jason%20Telanoff%20and%20Josh%20Wills%20and%20Kaleigh%20Mentzer%20and%20Paul%20Burstein%20and%20Parth%20Doshi%20and%20Paul%20Burnstein%20and%20Pratyush%20Maini%20and%20Ricardo%20Monti%20and%20Rishabh%20Adiga%20and%20Scott%20Loftin%20and%20Siddharth%20Joshi%20and%20Spandan%20Das%20and%20Tony%20Jiang%20and%20Vineeth%20Dorna%20and%20Zhengping%20Wang%20and%20Bogdan%20Gaza%20and%20Ari%20Morcos%20and%20Matthew%20Leavitt&entry.1292438233=Frontier%20language%20model%20quality%20increasingly%20hinges%20on%20our%20ability%20to%20organize%20web-scale%20text%20corpora%20for%20training.%20Today%27s%20dominant%20tools%20trade%20off%20speed%20and%20flexibility%3A%20lexical%20classifiers%20%28e.g.%2C%20FastText%29%20are%20fast%20but%20limited%20to%20producing%20classification%20output%20scores%2C%20while%20the%20vector-valued%20outputs%20of%20transformer%20text%20embedding%20models%20flexibly%20support%20numerous%20workflows%20%28e.g.%2C%20clustering%2C%20classification%2C%20and%20retrieval%29%20but%20are%20computationally%20expensive%20to%20produce.%20We%20introduce%20Luxical%2C%20a%20library%20for%20high-speed%20%22lexical-dense%22%20text%20embeddings%20that%20aims%20to%20recover%20the%20best%20properties%20of%20both%20approaches%20for%20web-scale%20text%20organization.%20Luxical%20combines%20sparse%20TF--IDF%20features%2C%20a%20small%20ReLU%20network%2C%20and%20a%20knowledge%20distillation%20training%20regimen%20to%20approximate%20large%20transformer%20embedding%20models%20at%20a%20fraction%20of%20their%20operational%20cost.%20In%20this%20technical%20report%2C%20we%20describe%20the%20Luxical%20architecture%20and%20training%20objective%20and%20evaluate%20a%20concrete%20Luxical%20model%20in%20two%20disparate%20applications%3A%20a%20targeted%20webcrawl%20document%20retrieval%20test%20and%20an%20end-to-end%20language%20model%20data%20curation%20task%20grounded%20in%20text%20classification.%20In%20these%20tasks%20we%20demonstrate%20speedups%20ranging%20from%203x%20to%20100x%20over%20varying-sized%20neural%20baselines%2C%20and%20comparable%20to%20FastText%20model%20inference%20during%20the%20data%20curation%20task.%20On%20these%20evaluations%2C%20the%20tested%20Luxical%20model%20illustrates%20favorable%20compute/quality%20trade-offs%20for%20large-scale%20text%20organization%2C%20matching%20the%20quality%20of%20neural%20baselines.%20Luxical%20is%20available%20as%20open-source%20software%20at%20https%3A//github.com/datologyai/luxical.&entry.1838667208=http%3A//arxiv.org/abs/2512.09015v2&entry.124074799=Read"},
{"title": "LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation", "author": "Tianyu Zhou and Junyi Tang and Zehui Li and Dahong Qian and Suncheng Xiang", "abstract": "Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.", "link": "http://arxiv.org/abs/2512.10750v1", "date": "2025-12-11", "relevancy": 2.1511, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5771}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5382}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LDP%3A%20Parameter-Efficient%20Fine-Tuning%20of%20Multimodal%20LLM%20for%20Medical%20Report%20Generation&body=Title%3A%20LDP%3A%20Parameter-Efficient%20Fine-Tuning%20of%20Multimodal%20LLM%20for%20Medical%20Report%20Generation%0AAuthor%3A%20Tianyu%20Zhou%20and%20Junyi%20Tang%20and%20Zehui%20Li%20and%20Dahong%20Qian%20and%20Suncheng%20Xiang%0AAbstract%3A%20Colonoscopic%20polyp%20diagnosis%20is%20pivotal%20for%20early%20colorectal%20cancer%20detection%2C%20yet%20traditional%20automated%20reporting%20suffers%20from%20inconsistencies%20and%20hallucinations%20due%20to%20the%20scarcity%20of%20high-quality%20multimodal%20medical%20data.%20To%20bridge%20this%20gap%2C%20we%20propose%20LDP%2C%20a%20novel%20framework%20leveraging%20multimodal%20large%20language%20models%20%28MLLMs%29%20for%20professional%20polyp%20diagnosis%20report%20generation.%20Specifically%2C%20we%20curate%20MMEndo%2C%20a%20multimodal%20endoscopic%20dataset%20comprising%20expert-annotated%20colonoscopy%20image-text%20pairs.%20We%20fine-tune%20the%20Qwen2-VL-7B%20backbone%20using%20Parameter-Efficient%20Fine-Tuning%20%28LoRA%29%20and%20align%20it%20with%20clinical%20standards%20via%20Direct%20Preference%20Optimization%20%28DPO%29.%20Extensive%20experiments%20show%20that%20our%20LDP%20outperforms%20existing%20baselines%20on%20both%20automated%20metrics%20and%20rigorous%20clinical%20expert%20evaluations%20%28achieving%20a%20Physician%20Score%20of%207.2/10%29%2C%20significantly%20reducing%20training%20computational%20costs%20by%20833x%20compared%20to%20full%20fine-tuning.%20The%20proposed%20solution%20offers%20a%20scalable%2C%20clinically%20viable%20path%20for%20primary%20healthcare%2C%20with%20additional%20validation%20on%20the%20IU-XRay%20dataset%20confirming%20its%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLDP%253A%2520Parameter-Efficient%2520Fine-Tuning%2520of%2520Multimodal%2520LLM%2520for%2520Medical%2520Report%2520Generation%26entry.906535625%3DTianyu%2520Zhou%2520and%2520Junyi%2520Tang%2520and%2520Zehui%2520Li%2520and%2520Dahong%2520Qian%2520and%2520Suncheng%2520Xiang%26entry.1292438233%3DColonoscopic%2520polyp%2520diagnosis%2520is%2520pivotal%2520for%2520early%2520colorectal%2520cancer%2520detection%252C%2520yet%2520traditional%2520automated%2520reporting%2520suffers%2520from%2520inconsistencies%2520and%2520hallucinations%2520due%2520to%2520the%2520scarcity%2520of%2520high-quality%2520multimodal%2520medical%2520data.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520LDP%252C%2520a%2520novel%2520framework%2520leveraging%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520for%2520professional%2520polyp%2520diagnosis%2520report%2520generation.%2520Specifically%252C%2520we%2520curate%2520MMEndo%252C%2520a%2520multimodal%2520endoscopic%2520dataset%2520comprising%2520expert-annotated%2520colonoscopy%2520image-text%2520pairs.%2520We%2520fine-tune%2520the%2520Qwen2-VL-7B%2520backbone%2520using%2520Parameter-Efficient%2520Fine-Tuning%2520%2528LoRA%2529%2520and%2520align%2520it%2520with%2520clinical%2520standards%2520via%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529.%2520Extensive%2520experiments%2520show%2520that%2520our%2520LDP%2520outperforms%2520existing%2520baselines%2520on%2520both%2520automated%2520metrics%2520and%2520rigorous%2520clinical%2520expert%2520evaluations%2520%2528achieving%2520a%2520Physician%2520Score%2520of%25207.2/10%2529%252C%2520significantly%2520reducing%2520training%2520computational%2520costs%2520by%2520833x%2520compared%2520to%2520full%2520fine-tuning.%2520The%2520proposed%2520solution%2520offers%2520a%2520scalable%252C%2520clinically%2520viable%2520path%2520for%2520primary%2520healthcare%252C%2520with%2520additional%2520validation%2520on%2520the%2520IU-XRay%2520dataset%2520confirming%2520its%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LDP%3A%20Parameter-Efficient%20Fine-Tuning%20of%20Multimodal%20LLM%20for%20Medical%20Report%20Generation&entry.906535625=Tianyu%20Zhou%20and%20Junyi%20Tang%20and%20Zehui%20Li%20and%20Dahong%20Qian%20and%20Suncheng%20Xiang&entry.1292438233=Colonoscopic%20polyp%20diagnosis%20is%20pivotal%20for%20early%20colorectal%20cancer%20detection%2C%20yet%20traditional%20automated%20reporting%20suffers%20from%20inconsistencies%20and%20hallucinations%20due%20to%20the%20scarcity%20of%20high-quality%20multimodal%20medical%20data.%20To%20bridge%20this%20gap%2C%20we%20propose%20LDP%2C%20a%20novel%20framework%20leveraging%20multimodal%20large%20language%20models%20%28MLLMs%29%20for%20professional%20polyp%20diagnosis%20report%20generation.%20Specifically%2C%20we%20curate%20MMEndo%2C%20a%20multimodal%20endoscopic%20dataset%20comprising%20expert-annotated%20colonoscopy%20image-text%20pairs.%20We%20fine-tune%20the%20Qwen2-VL-7B%20backbone%20using%20Parameter-Efficient%20Fine-Tuning%20%28LoRA%29%20and%20align%20it%20with%20clinical%20standards%20via%20Direct%20Preference%20Optimization%20%28DPO%29.%20Extensive%20experiments%20show%20that%20our%20LDP%20outperforms%20existing%20baselines%20on%20both%20automated%20metrics%20and%20rigorous%20clinical%20expert%20evaluations%20%28achieving%20a%20Physician%20Score%20of%207.2/10%29%2C%20significantly%20reducing%20training%20computational%20costs%20by%20833x%20compared%20to%20full%20fine-tuning.%20The%20proposed%20solution%20offers%20a%20scalable%2C%20clinically%20viable%20path%20for%20primary%20healthcare%2C%20with%20additional%20validation%20on%20the%20IU-XRay%20dataset%20confirming%20its%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2512.10750v1&entry.124074799=Read"},
{"title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs", "author": "Minghao LI and Ruihang Wang and Rui Tan and Yonggang Wen", "abstract": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.", "link": "http://arxiv.org/abs/2512.10611v1", "date": "2025-12-11", "relevancy": 2.1493, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5487}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5365}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phythesis%3A%20Physics-Guided%20Evolutionary%20Scene%20Synthesis%20for%20Energy-Efficient%20Data%20Center%20Design%20via%20LLMs&body=Title%3A%20Phythesis%3A%20Physics-Guided%20Evolutionary%20Scene%20Synthesis%20for%20Energy-Efficient%20Data%20Center%20Design%20via%20LLMs%0AAuthor%3A%20Minghao%20LI%20and%20Ruihang%20Wang%20and%20Rui%20Tan%20and%20Yonggang%20Wen%0AAbstract%3A%20Data%20center%20%28DC%29%20infrastructure%20serves%20as%20the%20backbone%20to%20support%20the%20escalating%20demand%20for%20computing%20capacity.%20Traditional%20design%20methodologies%20that%20blend%20human%20expertise%20with%20specialized%20simulation%20tools%20scale%20poorly%20with%20the%20increasing%20system%20complexity.%20Recent%20studies%20adopt%20generative%20artificial%20intelligence%20to%20design%20plausible%20human-centric%20indoor%20layouts.%20However%2C%20they%20do%20not%20consider%20the%20underlying%20physics%2C%20making%20them%20unsuitable%20for%20the%20DC%20design%20that%20sets%20quantifiable%20operational%20objectives%20and%20strict%20physical%20constraints.%20To%20bridge%20the%20gap%2C%20we%20propose%20Phythesis%2C%20a%20novel%20framework%20that%20synergizes%20large%20language%20models%20%28LLMs%29%20and%20physics-guided%20evolutionary%20optimization%20to%20automate%20simulation-ready%20%28SimReady%29%20scene%20synthesis%20for%20energy-efficient%20DC%20design.%20Phythesis%20employs%20an%20iterative%20bi-level%20optimization%20architecture%2C%20where%20%28i%29%20the%20LLM-driven%20optimization%20level%20generates%20physically%20plausible%20three-dimensional%20layouts%20and%20self-criticizes%20them%20to%20refine%20the%20scene%20topology%2C%20and%20%28ii%29%20the%20physics-informed%20optimization%20level%20identifies%20the%20optimal%20asset%20parameters%20and%20selects%20the%20best%20asset%20combination.%20Experiments%20on%20three%20generation%20scales%20show%20that%20Phythesis%20achieves%2057.3%25%20generation%20success%20rate%20increase%20and%2011.5%25%20power%20usage%20effectiveness%20%28PUE%29%20improvement%2C%20compared%20with%20the%20vanilla%20LLM-based%20solution.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhythesis%253A%2520Physics-Guided%2520Evolutionary%2520Scene%2520Synthesis%2520for%2520Energy-Efficient%2520Data%2520Center%2520Design%2520via%2520LLMs%26entry.906535625%3DMinghao%2520LI%2520and%2520Ruihang%2520Wang%2520and%2520Rui%2520Tan%2520and%2520Yonggang%2520Wen%26entry.1292438233%3DData%2520center%2520%2528DC%2529%2520infrastructure%2520serves%2520as%2520the%2520backbone%2520to%2520support%2520the%2520escalating%2520demand%2520for%2520computing%2520capacity.%2520Traditional%2520design%2520methodologies%2520that%2520blend%2520human%2520expertise%2520with%2520specialized%2520simulation%2520tools%2520scale%2520poorly%2520with%2520the%2520increasing%2520system%2520complexity.%2520Recent%2520studies%2520adopt%2520generative%2520artificial%2520intelligence%2520to%2520design%2520plausible%2520human-centric%2520indoor%2520layouts.%2520However%252C%2520they%2520do%2520not%2520consider%2520the%2520underlying%2520physics%252C%2520making%2520them%2520unsuitable%2520for%2520the%2520DC%2520design%2520that%2520sets%2520quantifiable%2520operational%2520objectives%2520and%2520strict%2520physical%2520constraints.%2520To%2520bridge%2520the%2520gap%252C%2520we%2520propose%2520Phythesis%252C%2520a%2520novel%2520framework%2520that%2520synergizes%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520physics-guided%2520evolutionary%2520optimization%2520to%2520automate%2520simulation-ready%2520%2528SimReady%2529%2520scene%2520synthesis%2520for%2520energy-efficient%2520DC%2520design.%2520Phythesis%2520employs%2520an%2520iterative%2520bi-level%2520optimization%2520architecture%252C%2520where%2520%2528i%2529%2520the%2520LLM-driven%2520optimization%2520level%2520generates%2520physically%2520plausible%2520three-dimensional%2520layouts%2520and%2520self-criticizes%2520them%2520to%2520refine%2520the%2520scene%2520topology%252C%2520and%2520%2528ii%2529%2520the%2520physics-informed%2520optimization%2520level%2520identifies%2520the%2520optimal%2520asset%2520parameters%2520and%2520selects%2520the%2520best%2520asset%2520combination.%2520Experiments%2520on%2520three%2520generation%2520scales%2520show%2520that%2520Phythesis%2520achieves%252057.3%2525%2520generation%2520success%2520rate%2520increase%2520and%252011.5%2525%2520power%2520usage%2520effectiveness%2520%2528PUE%2529%2520improvement%252C%2520compared%2520with%2520the%2520vanilla%2520LLM-based%2520solution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phythesis%3A%20Physics-Guided%20Evolutionary%20Scene%20Synthesis%20for%20Energy-Efficient%20Data%20Center%20Design%20via%20LLMs&entry.906535625=Minghao%20LI%20and%20Ruihang%20Wang%20and%20Rui%20Tan%20and%20Yonggang%20Wen&entry.1292438233=Data%20center%20%28DC%29%20infrastructure%20serves%20as%20the%20backbone%20to%20support%20the%20escalating%20demand%20for%20computing%20capacity.%20Traditional%20design%20methodologies%20that%20blend%20human%20expertise%20with%20specialized%20simulation%20tools%20scale%20poorly%20with%20the%20increasing%20system%20complexity.%20Recent%20studies%20adopt%20generative%20artificial%20intelligence%20to%20design%20plausible%20human-centric%20indoor%20layouts.%20However%2C%20they%20do%20not%20consider%20the%20underlying%20physics%2C%20making%20them%20unsuitable%20for%20the%20DC%20design%20that%20sets%20quantifiable%20operational%20objectives%20and%20strict%20physical%20constraints.%20To%20bridge%20the%20gap%2C%20we%20propose%20Phythesis%2C%20a%20novel%20framework%20that%20synergizes%20large%20language%20models%20%28LLMs%29%20and%20physics-guided%20evolutionary%20optimization%20to%20automate%20simulation-ready%20%28SimReady%29%20scene%20synthesis%20for%20energy-efficient%20DC%20design.%20Phythesis%20employs%20an%20iterative%20bi-level%20optimization%20architecture%2C%20where%20%28i%29%20the%20LLM-driven%20optimization%20level%20generates%20physically%20plausible%20three-dimensional%20layouts%20and%20self-criticizes%20them%20to%20refine%20the%20scene%20topology%2C%20and%20%28ii%29%20the%20physics-informed%20optimization%20level%20identifies%20the%20optimal%20asset%20parameters%20and%20selects%20the%20best%20asset%20combination.%20Experiments%20on%20three%20generation%20scales%20show%20that%20Phythesis%20achieves%2057.3%25%20generation%20success%20rate%20increase%20and%2011.5%25%20power%20usage%20effectiveness%20%28PUE%29%20improvement%2C%20compared%20with%20the%20vanilla%20LLM-based%20solution.&entry.1838667208=http%3A//arxiv.org/abs/2512.10611v1&entry.124074799=Read"},
{"title": "ENMA: Tokenwise Autoregression for Generative Neural PDE Operators", "author": "Armand Kassa\u00ef Koupa\u00ef and Lise Le Boudec and Louis Serrano and Patrick Gallinari", "abstract": "Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete-as is often the case-a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs.", "link": "http://arxiv.org/abs/2506.06158v3", "date": "2025-12-11", "relevancy": 2.1475, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5606}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5278}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators&body=Title%3A%20ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators%0AAuthor%3A%20Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Lise%20Le%20Boudec%20and%20Louis%20Serrano%20and%20Patrick%20Gallinari%0AAbstract%3A%20Solving%20time-dependent%20parametric%20partial%20differential%20equations%20%28PDEs%29%20remains%20a%20fundamental%20challenge%20for%20neural%20solvers%2C%20particularly%20when%20generalizing%20across%20a%20wide%20range%20of%20physical%20parameters%20and%20dynamics.%20When%20data%20is%20uncertain%20or%20incomplete-as%20is%20often%20the%20case-a%20natural%20approach%20is%20to%20turn%20to%20generative%20models.%20We%20introduce%20ENMA%2C%20a%20generative%20neural%20operator%20designed%20to%20model%20spatio-temporal%20dynamics%20arising%20from%20physical%20phenomena.%20ENMA%20predicts%20future%20dynamics%20in%20a%20compressed%20latent%20space%20using%20a%20generative%20masked%20autoregressive%20transformer%20trained%20with%20flow%20matching%20loss%2C%20enabling%20tokenwise%20generation.%20Irregularly%20sampled%20spatial%20observations%20are%20encoded%20into%20uniform%20latent%20representations%20via%20attention%20mechanisms%20and%20further%20compressed%20through%20a%20spatio-temporal%20convolutional%20encoder.%20This%20allows%20ENMA%20to%20perform%20in-context%20learning%20at%20inference%20time%20by%20conditioning%20on%20either%20past%20states%20of%20the%20target%20trajectory%20or%20auxiliary%20context%20trajectories%20with%20similar%20dynamics.%20The%20result%20is%20a%20robust%20and%20adaptable%20framework%20that%20generalizes%20to%20new%20PDE%20regimes%20and%20supports%20one-shot%20surrogate%20modeling%20of%20time-dependent%20parametric%20PDEs.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06158v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENMA%253A%2520Tokenwise%2520Autoregression%2520for%2520Generative%2520Neural%2520PDE%2520Operators%26entry.906535625%3DArmand%2520Kassa%25C3%25AF%2520Koupa%25C3%25AF%2520and%2520Lise%2520Le%2520Boudec%2520and%2520Louis%2520Serrano%2520and%2520Patrick%2520Gallinari%26entry.1292438233%3DSolving%2520time-dependent%2520parametric%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520remains%2520a%2520fundamental%2520challenge%2520for%2520neural%2520solvers%252C%2520particularly%2520when%2520generalizing%2520across%2520a%2520wide%2520range%2520of%2520physical%2520parameters%2520and%2520dynamics.%2520When%2520data%2520is%2520uncertain%2520or%2520incomplete-as%2520is%2520often%2520the%2520case-a%2520natural%2520approach%2520is%2520to%2520turn%2520to%2520generative%2520models.%2520We%2520introduce%2520ENMA%252C%2520a%2520generative%2520neural%2520operator%2520designed%2520to%2520model%2520spatio-temporal%2520dynamics%2520arising%2520from%2520physical%2520phenomena.%2520ENMA%2520predicts%2520future%2520dynamics%2520in%2520a%2520compressed%2520latent%2520space%2520using%2520a%2520generative%2520masked%2520autoregressive%2520transformer%2520trained%2520with%2520flow%2520matching%2520loss%252C%2520enabling%2520tokenwise%2520generation.%2520Irregularly%2520sampled%2520spatial%2520observations%2520are%2520encoded%2520into%2520uniform%2520latent%2520representations%2520via%2520attention%2520mechanisms%2520and%2520further%2520compressed%2520through%2520a%2520spatio-temporal%2520convolutional%2520encoder.%2520This%2520allows%2520ENMA%2520to%2520perform%2520in-context%2520learning%2520at%2520inference%2520time%2520by%2520conditioning%2520on%2520either%2520past%2520states%2520of%2520the%2520target%2520trajectory%2520or%2520auxiliary%2520context%2520trajectories%2520with%2520similar%2520dynamics.%2520The%2520result%2520is%2520a%2520robust%2520and%2520adaptable%2520framework%2520that%2520generalizes%2520to%2520new%2520PDE%2520regimes%2520and%2520supports%2520one-shot%2520surrogate%2520modeling%2520of%2520time-dependent%2520parametric%2520PDEs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06158v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENMA%3A%20Tokenwise%20Autoregression%20for%20Generative%20Neural%20PDE%20Operators&entry.906535625=Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Lise%20Le%20Boudec%20and%20Louis%20Serrano%20and%20Patrick%20Gallinari&entry.1292438233=Solving%20time-dependent%20parametric%20partial%20differential%20equations%20%28PDEs%29%20remains%20a%20fundamental%20challenge%20for%20neural%20solvers%2C%20particularly%20when%20generalizing%20across%20a%20wide%20range%20of%20physical%20parameters%20and%20dynamics.%20When%20data%20is%20uncertain%20or%20incomplete-as%20is%20often%20the%20case-a%20natural%20approach%20is%20to%20turn%20to%20generative%20models.%20We%20introduce%20ENMA%2C%20a%20generative%20neural%20operator%20designed%20to%20model%20spatio-temporal%20dynamics%20arising%20from%20physical%20phenomena.%20ENMA%20predicts%20future%20dynamics%20in%20a%20compressed%20latent%20space%20using%20a%20generative%20masked%20autoregressive%20transformer%20trained%20with%20flow%20matching%20loss%2C%20enabling%20tokenwise%20generation.%20Irregularly%20sampled%20spatial%20observations%20are%20encoded%20into%20uniform%20latent%20representations%20via%20attention%20mechanisms%20and%20further%20compressed%20through%20a%20spatio-temporal%20convolutional%20encoder.%20This%20allows%20ENMA%20to%20perform%20in-context%20learning%20at%20inference%20time%20by%20conditioning%20on%20either%20past%20states%20of%20the%20target%20trajectory%20or%20auxiliary%20context%20trajectories%20with%20similar%20dynamics.%20The%20result%20is%20a%20robust%20and%20adaptable%20framework%20that%20generalizes%20to%20new%20PDE%20regimes%20and%20supports%20one-shot%20surrogate%20modeling%20of%20time-dependent%20parametric%20PDEs.&entry.1838667208=http%3A//arxiv.org/abs/2506.06158v3&entry.124074799=Read"},
{"title": "On the Stabilization of Rigid Formations on Regular Curves", "author": "Mohamed Elobaid and Shinkyu Park and Eric Feron", "abstract": "This work deals with the problem of stabilizing a multi-agent rigid formation on a general class of planar curves. Namely, we seek to stabilize an equilateral polygonal formation on closed planar differentiable curves after a path sweep. The task of finding an inscribed regular polygon centered at the point of interest is solved via a randomized multi-start Newton-Like algorithm for which one is able to ascertain the existence of a minimizer. Then we design a continuous feedback law that guarantees convergence to, and sufficient sweeping of the curve, followed by convergence to the desired formation vertices while ensuring inter-agent avoidance. The proposed approach is validated through numerical simulations for different classes of curves and different rigid formations. Code: https://github.com/mebbaid/paper-elobaid-ifacwc-2026", "link": "http://arxiv.org/abs/2512.10700v1", "date": "2025-12-11", "relevancy": 2.0495, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4171}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.407}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Stabilization%20of%20Rigid%20Formations%20on%20Regular%20Curves&body=Title%3A%20On%20the%20Stabilization%20of%20Rigid%20Formations%20on%20Regular%20Curves%0AAuthor%3A%20Mohamed%20Elobaid%20and%20Shinkyu%20Park%20and%20Eric%20Feron%0AAbstract%3A%20This%20work%20deals%20with%20the%20problem%20of%20stabilizing%20a%20multi-agent%20rigid%20formation%20on%20a%20general%20class%20of%20planar%20curves.%20Namely%2C%20we%20seek%20to%20stabilize%20an%20equilateral%20polygonal%20formation%20on%20closed%20planar%20differentiable%20curves%20after%20a%20path%20sweep.%20The%20task%20of%20finding%20an%20inscribed%20regular%20polygon%20centered%20at%20the%20point%20of%20interest%20is%20solved%20via%20a%20randomized%20multi-start%20Newton-Like%20algorithm%20for%20which%20one%20is%20able%20to%20ascertain%20the%20existence%20of%20a%20minimizer.%20Then%20we%20design%20a%20continuous%20feedback%20law%20that%20guarantees%20convergence%20to%2C%20and%20sufficient%20sweeping%20of%20the%20curve%2C%20followed%20by%20convergence%20to%20the%20desired%20formation%20vertices%20while%20ensuring%20inter-agent%20avoidance.%20The%20proposed%20approach%20is%20validated%20through%20numerical%20simulations%20for%20different%20classes%20of%20curves%20and%20different%20rigid%20formations.%20Code%3A%20https%3A//github.com/mebbaid/paper-elobaid-ifacwc-2026%0ALink%3A%20http%3A//arxiv.org/abs/2512.10700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Stabilization%2520of%2520Rigid%2520Formations%2520on%2520Regular%2520Curves%26entry.906535625%3DMohamed%2520Elobaid%2520and%2520Shinkyu%2520Park%2520and%2520Eric%2520Feron%26entry.1292438233%3DThis%2520work%2520deals%2520with%2520the%2520problem%2520of%2520stabilizing%2520a%2520multi-agent%2520rigid%2520formation%2520on%2520a%2520general%2520class%2520of%2520planar%2520curves.%2520Namely%252C%2520we%2520seek%2520to%2520stabilize%2520an%2520equilateral%2520polygonal%2520formation%2520on%2520closed%2520planar%2520differentiable%2520curves%2520after%2520a%2520path%2520sweep.%2520The%2520task%2520of%2520finding%2520an%2520inscribed%2520regular%2520polygon%2520centered%2520at%2520the%2520point%2520of%2520interest%2520is%2520solved%2520via%2520a%2520randomized%2520multi-start%2520Newton-Like%2520algorithm%2520for%2520which%2520one%2520is%2520able%2520to%2520ascertain%2520the%2520existence%2520of%2520a%2520minimizer.%2520Then%2520we%2520design%2520a%2520continuous%2520feedback%2520law%2520that%2520guarantees%2520convergence%2520to%252C%2520and%2520sufficient%2520sweeping%2520of%2520the%2520curve%252C%2520followed%2520by%2520convergence%2520to%2520the%2520desired%2520formation%2520vertices%2520while%2520ensuring%2520inter-agent%2520avoidance.%2520The%2520proposed%2520approach%2520is%2520validated%2520through%2520numerical%2520simulations%2520for%2520different%2520classes%2520of%2520curves%2520and%2520different%2520rigid%2520formations.%2520Code%253A%2520https%253A//github.com/mebbaid/paper-elobaid-ifacwc-2026%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Stabilization%20of%20Rigid%20Formations%20on%20Regular%20Curves&entry.906535625=Mohamed%20Elobaid%20and%20Shinkyu%20Park%20and%20Eric%20Feron&entry.1292438233=This%20work%20deals%20with%20the%20problem%20of%20stabilizing%20a%20multi-agent%20rigid%20formation%20on%20a%20general%20class%20of%20planar%20curves.%20Namely%2C%20we%20seek%20to%20stabilize%20an%20equilateral%20polygonal%20formation%20on%20closed%20planar%20differentiable%20curves%20after%20a%20path%20sweep.%20The%20task%20of%20finding%20an%20inscribed%20regular%20polygon%20centered%20at%20the%20point%20of%20interest%20is%20solved%20via%20a%20randomized%20multi-start%20Newton-Like%20algorithm%20for%20which%20one%20is%20able%20to%20ascertain%20the%20existence%20of%20a%20minimizer.%20Then%20we%20design%20a%20continuous%20feedback%20law%20that%20guarantees%20convergence%20to%2C%20and%20sufficient%20sweeping%20of%20the%20curve%2C%20followed%20by%20convergence%20to%20the%20desired%20formation%20vertices%20while%20ensuring%20inter-agent%20avoidance.%20The%20proposed%20approach%20is%20validated%20through%20numerical%20simulations%20for%20different%20classes%20of%20curves%20and%20different%20rigid%20formations.%20Code%3A%20https%3A//github.com/mebbaid/paper-elobaid-ifacwc-2026&entry.1838667208=http%3A//arxiv.org/abs/2512.10700v1&entry.124074799=Read"},
{"title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving", "author": "Yifang Xu and Jiahao Cui and Feipeng Cai and Zhihao Zhu and Hanlin Shang and Shan Luan and Mingwang Xu and Neng Zhang and Yaoyi Li and Jia Cai and Siyu Zhu", "abstract": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.", "link": "http://arxiv.org/abs/2512.06112v2", "date": "2025-12-11", "relevancy": 1.6028, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.538}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5306}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WAM-Flow%3A%20Parallel%20Coarse-to-Fine%20Motion%20Planning%20via%20Discrete%20Flow%20Matching%20for%20Autonomous%20Driving&body=Title%3A%20WAM-Flow%3A%20Parallel%20Coarse-to-Fine%20Motion%20Planning%20via%20Discrete%20Flow%20Matching%20for%20Autonomous%20Driving%0AAuthor%3A%20Yifang%20Xu%20and%20Jiahao%20Cui%20and%20Feipeng%20Cai%20and%20Zhihao%20Zhu%20and%20Hanlin%20Shang%20and%20Shan%20Luan%20and%20Mingwang%20Xu%20and%20Neng%20Zhang%20and%20Yaoyi%20Li%20and%20Jia%20Cai%20and%20Siyu%20Zhu%0AAbstract%3A%20We%20introduce%20WAM-Flow%2C%20a%20vision-language-action%20%28VLA%29%20model%20that%20casts%20ego-trajectory%20planning%20as%20discrete%20flow%20matching%20over%20a%20structured%20token%20space.%20In%20contrast%20to%20autoregressive%20decoders%2C%20WAM-Flow%20performs%20fully%20parallel%2C%20bidirectional%20denoising%2C%20enabling%20coarse-to-fine%20refinement%20with%20a%20tunable%20compute-accuracy%20trade-off.%20Specifically%2C%20the%20approach%20combines%20a%20metric-aligned%20numerical%20tokenizer%20that%20preserves%20scalar%20geometry%20via%20triplet-margin%20learning%2C%20a%20geometry-aware%20flow%20objective%20and%20a%20simulator-guided%20GRPO%20alignment%20that%20integrates%20safety%2C%20ego%20progress%2C%20and%20comfort%20rewards%20while%20retaining%20parallel%20generation.%20A%20multi-stage%20adaptation%20converts%20a%20pre-trained%20auto-regressive%20backbone%20%28Janus-1.5B%29%20from%20causal%20decoding%20to%20non-causal%20flow%20model%20and%20strengthens%20road-scene%20competence%20through%20continued%20multimodal%20pretraining.%20Thanks%20to%20the%20inherent%20nature%20of%20consistency%20model%20training%20and%20parallel%20decoding%20inference%2C%20WAM-Flow%20achieves%20superior%20closed-loop%20performance%20against%20autoregressive%20and%20diffusion-based%20VLA%20baselines%2C%20with%201-step%20inference%20attaining%2089.1%20PDMS%20and%205-step%20inference%20reaching%2090.3%20PDMS%20on%20NAVSIM%20v1%20benchmark.%20These%20results%20establish%20discrete%20flow%20matching%20as%20a%20new%20promising%20paradigm%20for%20end-to-end%20autonomous%20driving.%20The%20code%20will%20be%20publicly%20available%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWAM-Flow%253A%2520Parallel%2520Coarse-to-Fine%2520Motion%2520Planning%2520via%2520Discrete%2520Flow%2520Matching%2520for%2520Autonomous%2520Driving%26entry.906535625%3DYifang%2520Xu%2520and%2520Jiahao%2520Cui%2520and%2520Feipeng%2520Cai%2520and%2520Zhihao%2520Zhu%2520and%2520Hanlin%2520Shang%2520and%2520Shan%2520Luan%2520and%2520Mingwang%2520Xu%2520and%2520Neng%2520Zhang%2520and%2520Yaoyi%2520Li%2520and%2520Jia%2520Cai%2520and%2520Siyu%2520Zhu%26entry.1292438233%3DWe%2520introduce%2520WAM-Flow%252C%2520a%2520vision-language-action%2520%2528VLA%2529%2520model%2520that%2520casts%2520ego-trajectory%2520planning%2520as%2520discrete%2520flow%2520matching%2520over%2520a%2520structured%2520token%2520space.%2520In%2520contrast%2520to%2520autoregressive%2520decoders%252C%2520WAM-Flow%2520performs%2520fully%2520parallel%252C%2520bidirectional%2520denoising%252C%2520enabling%2520coarse-to-fine%2520refinement%2520with%2520a%2520tunable%2520compute-accuracy%2520trade-off.%2520Specifically%252C%2520the%2520approach%2520combines%2520a%2520metric-aligned%2520numerical%2520tokenizer%2520that%2520preserves%2520scalar%2520geometry%2520via%2520triplet-margin%2520learning%252C%2520a%2520geometry-aware%2520flow%2520objective%2520and%2520a%2520simulator-guided%2520GRPO%2520alignment%2520that%2520integrates%2520safety%252C%2520ego%2520progress%252C%2520and%2520comfort%2520rewards%2520while%2520retaining%2520parallel%2520generation.%2520A%2520multi-stage%2520adaptation%2520converts%2520a%2520pre-trained%2520auto-regressive%2520backbone%2520%2528Janus-1.5B%2529%2520from%2520causal%2520decoding%2520to%2520non-causal%2520flow%2520model%2520and%2520strengthens%2520road-scene%2520competence%2520through%2520continued%2520multimodal%2520pretraining.%2520Thanks%2520to%2520the%2520inherent%2520nature%2520of%2520consistency%2520model%2520training%2520and%2520parallel%2520decoding%2520inference%252C%2520WAM-Flow%2520achieves%2520superior%2520closed-loop%2520performance%2520against%2520autoregressive%2520and%2520diffusion-based%2520VLA%2520baselines%252C%2520with%25201-step%2520inference%2520attaining%252089.1%2520PDMS%2520and%25205-step%2520inference%2520reaching%252090.3%2520PDMS%2520on%2520NAVSIM%2520v1%2520benchmark.%2520These%2520results%2520establish%2520discrete%2520flow%2520matching%2520as%2520a%2520new%2520promising%2520paradigm%2520for%2520end-to-end%2520autonomous%2520driving.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WAM-Flow%3A%20Parallel%20Coarse-to-Fine%20Motion%20Planning%20via%20Discrete%20Flow%20Matching%20for%20Autonomous%20Driving&entry.906535625=Yifang%20Xu%20and%20Jiahao%20Cui%20and%20Feipeng%20Cai%20and%20Zhihao%20Zhu%20and%20Hanlin%20Shang%20and%20Shan%20Luan%20and%20Mingwang%20Xu%20and%20Neng%20Zhang%20and%20Yaoyi%20Li%20and%20Jia%20Cai%20and%20Siyu%20Zhu&entry.1292438233=We%20introduce%20WAM-Flow%2C%20a%20vision-language-action%20%28VLA%29%20model%20that%20casts%20ego-trajectory%20planning%20as%20discrete%20flow%20matching%20over%20a%20structured%20token%20space.%20In%20contrast%20to%20autoregressive%20decoders%2C%20WAM-Flow%20performs%20fully%20parallel%2C%20bidirectional%20denoising%2C%20enabling%20coarse-to-fine%20refinement%20with%20a%20tunable%20compute-accuracy%20trade-off.%20Specifically%2C%20the%20approach%20combines%20a%20metric-aligned%20numerical%20tokenizer%20that%20preserves%20scalar%20geometry%20via%20triplet-margin%20learning%2C%20a%20geometry-aware%20flow%20objective%20and%20a%20simulator-guided%20GRPO%20alignment%20that%20integrates%20safety%2C%20ego%20progress%2C%20and%20comfort%20rewards%20while%20retaining%20parallel%20generation.%20A%20multi-stage%20adaptation%20converts%20a%20pre-trained%20auto-regressive%20backbone%20%28Janus-1.5B%29%20from%20causal%20decoding%20to%20non-causal%20flow%20model%20and%20strengthens%20road-scene%20competence%20through%20continued%20multimodal%20pretraining.%20Thanks%20to%20the%20inherent%20nature%20of%20consistency%20model%20training%20and%20parallel%20decoding%20inference%2C%20WAM-Flow%20achieves%20superior%20closed-loop%20performance%20against%20autoregressive%20and%20diffusion-based%20VLA%20baselines%2C%20with%201-step%20inference%20attaining%2089.1%20PDMS%20and%205-step%20inference%20reaching%2090.3%20PDMS%20on%20NAVSIM%20v1%20benchmark.%20These%20results%20establish%20discrete%20flow%20matching%20as%20a%20new%20promising%20paradigm%20for%20end-to-end%20autonomous%20driving.%20The%20code%20will%20be%20publicly%20available%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2512.06112v2&entry.124074799=Read"},
{"title": "Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach", "author": "C. Bosco and U. Minora and D. de Rigo and J. Pingsdorf and R. Cortinovis", "abstract": "This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.", "link": "http://arxiv.org/abs/2512.10633v1", "date": "2025-12-11", "relevancy": 1.7002, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4469}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4312}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supporting%20Migration%20Policies%20with%20Forecasts%3A%20Illegal%20Border%20Crossings%20in%20Europe%20through%20a%20Mixed%20Approach&body=Title%3A%20Supporting%20Migration%20Policies%20with%20Forecasts%3A%20Illegal%20Border%20Crossings%20in%20Europe%20through%20a%20Mixed%20Approach%0AAuthor%3A%20C.%20Bosco%20and%20U.%20Minora%20and%20D.%20de%20Rigo%20and%20J.%20Pingsdorf%20and%20R.%20Cortinovis%0AAbstract%3A%20This%20paper%20presents%20a%20mixed-methodology%20to%20forecast%20illegal%20border%20crossings%20in%20Europe%20across%20five%20key%20migratory%20routes%2C%20with%20a%20one-year%20time%20horizon.%20The%20methodology%20integrates%20machine%20learning%20techniques%20with%20qualitative%20insights%20from%20migration%20experts.%20This%20approach%20aims%20at%20improving%20the%20predictive%20capacity%20of%20data-driven%20models%20through%20the%20inclusion%20of%20a%20human-assessed%20covariate%2C%20an%20innovation%20that%20addresses%20challenges%20posed%20by%20sudden%20shifts%20in%20migration%20patterns%20and%20limitations%20in%20traditional%20datasets.%20The%20proposed%20methodology%20responds%20directly%20to%20the%20forecasting%20needs%20outlined%20in%20the%20EU%20Pact%20on%20Migration%20and%20Asylum%2C%20supporting%20the%20Asylum%20and%20Migration%20Management%20Regulation%20%28AMMR%29.%20It%20is%20designed%20to%20provide%20policy-relevant%20forecasts%20that%20inform%20strategic%20decisions%2C%20early%20warning%20systems%2C%20and%20solidarity%20mechanisms%20among%20EU%20Member%20States.%20By%20joining%20data-driven%20modeling%20with%20expert%20judgment%2C%20this%20work%20aligns%20with%20existing%20academic%20recommendations%20and%20introduces%20a%20novel%20operational%20tool%20tailored%20for%20EU%20migration%20governance.%20The%20methodology%20is%20tested%20and%20validated%20with%20known%20data%20to%20demonstrate%20its%20applicability%20and%20reliability%20in%20migration-related%20policy%20context.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupporting%2520Migration%2520Policies%2520with%2520Forecasts%253A%2520Illegal%2520Border%2520Crossings%2520in%2520Europe%2520through%2520a%2520Mixed%2520Approach%26entry.906535625%3DC.%2520Bosco%2520and%2520U.%2520Minora%2520and%2520D.%2520de%2520Rigo%2520and%2520J.%2520Pingsdorf%2520and%2520R.%2520Cortinovis%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520mixed-methodology%2520to%2520forecast%2520illegal%2520border%2520crossings%2520in%2520Europe%2520across%2520five%2520key%2520migratory%2520routes%252C%2520with%2520a%2520one-year%2520time%2520horizon.%2520The%2520methodology%2520integrates%2520machine%2520learning%2520techniques%2520with%2520qualitative%2520insights%2520from%2520migration%2520experts.%2520This%2520approach%2520aims%2520at%2520improving%2520the%2520predictive%2520capacity%2520of%2520data-driven%2520models%2520through%2520the%2520inclusion%2520of%2520a%2520human-assessed%2520covariate%252C%2520an%2520innovation%2520that%2520addresses%2520challenges%2520posed%2520by%2520sudden%2520shifts%2520in%2520migration%2520patterns%2520and%2520limitations%2520in%2520traditional%2520datasets.%2520The%2520proposed%2520methodology%2520responds%2520directly%2520to%2520the%2520forecasting%2520needs%2520outlined%2520in%2520the%2520EU%2520Pact%2520on%2520Migration%2520and%2520Asylum%252C%2520supporting%2520the%2520Asylum%2520and%2520Migration%2520Management%2520Regulation%2520%2528AMMR%2529.%2520It%2520is%2520designed%2520to%2520provide%2520policy-relevant%2520forecasts%2520that%2520inform%2520strategic%2520decisions%252C%2520early%2520warning%2520systems%252C%2520and%2520solidarity%2520mechanisms%2520among%2520EU%2520Member%2520States.%2520By%2520joining%2520data-driven%2520modeling%2520with%2520expert%2520judgment%252C%2520this%2520work%2520aligns%2520with%2520existing%2520academic%2520recommendations%2520and%2520introduces%2520a%2520novel%2520operational%2520tool%2520tailored%2520for%2520EU%2520migration%2520governance.%2520The%2520methodology%2520is%2520tested%2520and%2520validated%2520with%2520known%2520data%2520to%2520demonstrate%2520its%2520applicability%2520and%2520reliability%2520in%2520migration-related%2520policy%2520context.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supporting%20Migration%20Policies%20with%20Forecasts%3A%20Illegal%20Border%20Crossings%20in%20Europe%20through%20a%20Mixed%20Approach&entry.906535625=C.%20Bosco%20and%20U.%20Minora%20and%20D.%20de%20Rigo%20and%20J.%20Pingsdorf%20and%20R.%20Cortinovis&entry.1292438233=This%20paper%20presents%20a%20mixed-methodology%20to%20forecast%20illegal%20border%20crossings%20in%20Europe%20across%20five%20key%20migratory%20routes%2C%20with%20a%20one-year%20time%20horizon.%20The%20methodology%20integrates%20machine%20learning%20techniques%20with%20qualitative%20insights%20from%20migration%20experts.%20This%20approach%20aims%20at%20improving%20the%20predictive%20capacity%20of%20data-driven%20models%20through%20the%20inclusion%20of%20a%20human-assessed%20covariate%2C%20an%20innovation%20that%20addresses%20challenges%20posed%20by%20sudden%20shifts%20in%20migration%20patterns%20and%20limitations%20in%20traditional%20datasets.%20The%20proposed%20methodology%20responds%20directly%20to%20the%20forecasting%20needs%20outlined%20in%20the%20EU%20Pact%20on%20Migration%20and%20Asylum%2C%20supporting%20the%20Asylum%20and%20Migration%20Management%20Regulation%20%28AMMR%29.%20It%20is%20designed%20to%20provide%20policy-relevant%20forecasts%20that%20inform%20strategic%20decisions%2C%20early%20warning%20systems%2C%20and%20solidarity%20mechanisms%20among%20EU%20Member%20States.%20By%20joining%20data-driven%20modeling%20with%20expert%20judgment%2C%20this%20work%20aligns%20with%20existing%20academic%20recommendations%20and%20introduces%20a%20novel%20operational%20tool%20tailored%20for%20EU%20migration%20governance.%20The%20methodology%20is%20tested%20and%20validated%20with%20known%20data%20to%20demonstrate%20its%20applicability%20and%20reliability%20in%20migration-related%20policy%20context.&entry.1838667208=http%3A//arxiv.org/abs/2512.10633v1&entry.124074799=Read"},
{"title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce", "author": "Rui Min and Zile Qiao and Ze Xu and Jiawen Zhai and Wenyu Gao and Xuanzhong Chen and Haozhen Sun and Zhen Zhang and Xinyu Wang and Hong Zhou and Wenbiao Yin and Bo Zhang and Xuan Zhou and Ming Yan and Yong Jiang and Haicheng Liu and Liang Ding and Ling Zou and Yi R. Fung and Yalong Li and Pengjun Xie", "abstract": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.", "link": "http://arxiv.org/abs/2512.08868v2", "date": "2025-12-11", "relevancy": 1.8453, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EcomBench%3A%20Towards%20Holistic%20Evaluation%20of%20Foundation%20Agents%20in%20E-commerce&body=Title%3A%20EcomBench%3A%20Towards%20Holistic%20Evaluation%20of%20Foundation%20Agents%20in%20E-commerce%0AAuthor%3A%20Rui%20Min%20and%20Zile%20Qiao%20and%20Ze%20Xu%20and%20Jiawen%20Zhai%20and%20Wenyu%20Gao%20and%20Xuanzhong%20Chen%20and%20Haozhen%20Sun%20and%20Zhen%20Zhang%20and%20Xinyu%20Wang%20and%20Hong%20Zhou%20and%20Wenbiao%20Yin%20and%20Bo%20Zhang%20and%20Xuan%20Zhou%20and%20Ming%20Yan%20and%20Yong%20Jiang%20and%20Haicheng%20Liu%20and%20Liang%20Ding%20and%20Ling%20Zou%20and%20Yi%20R.%20Fung%20and%20Yalong%20Li%20and%20Pengjun%20Xie%0AAbstract%3A%20Foundation%20agents%20have%20rapidly%20advanced%20in%20their%20ability%20to%20reason%20and%20interact%20with%20real%20environments%2C%20making%20the%20evaluation%20of%20their%20core%20capabilities%20increasingly%20important.%20While%20many%20benchmarks%20have%20been%20developed%20to%20assess%20agent%20performance%2C%20most%20concentrate%20on%20academic%20settings%20or%20artificially%20designed%20scenarios%20while%20overlooking%20the%20challenges%20that%20arise%20in%20real%20applications.%20To%20address%20this%20issue%2C%20we%20focus%20on%20a%20highly%20practical%20real-world%20setting%2C%20the%20e-commerce%20domain%2C%20which%20involves%20a%20large%20volume%20of%20diverse%20user%20interactions%2C%20dynamic%20market%20conditions%2C%20and%20tasks%20directly%20tied%20to%20real%20decision-making%20processes.%20To%20this%20end%2C%20we%20introduce%20EcomBench%2C%20a%20holistic%20E-commerce%20Benchmark%20designed%20to%20evaluate%20agent%20performance%20in%20realistic%20e-commerce%20environments.%20EcomBench%20is%20built%20from%20genuine%20user%20demands%20embedded%20in%20leading%20global%20e-commerce%20ecosystems%20and%20is%20carefully%20curated%20and%20annotated%20through%20human%20experts%20to%20ensure%20clarity%2C%20accuracy%2C%20and%20domain%20relevance.%20It%20covers%20multiple%20task%20categories%20within%20e-commerce%20scenarios%20and%20defines%20three%20difficulty%20levels%20that%20evaluate%20agents%20on%20key%20capabilities%20such%20as%20deep%20information%20retrieval%2C%20multi-step%20reasoning%2C%20and%20cross-source%20knowledge%20integration.%20By%20grounding%20evaluation%20in%20real%20e-commerce%20contexts%2C%20EcomBench%20provides%20a%20rigorous%20and%20dynamic%20testbed%20for%20measuring%20the%20practical%20capabilities%20of%20agents%20in%20modern%20e-commerce.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEcomBench%253A%2520Towards%2520Holistic%2520Evaluation%2520of%2520Foundation%2520Agents%2520in%2520E-commerce%26entry.906535625%3DRui%2520Min%2520and%2520Zile%2520Qiao%2520and%2520Ze%2520Xu%2520and%2520Jiawen%2520Zhai%2520and%2520Wenyu%2520Gao%2520and%2520Xuanzhong%2520Chen%2520and%2520Haozhen%2520Sun%2520and%2520Zhen%2520Zhang%2520and%2520Xinyu%2520Wang%2520and%2520Hong%2520Zhou%2520and%2520Wenbiao%2520Yin%2520and%2520Bo%2520Zhang%2520and%2520Xuan%2520Zhou%2520and%2520Ming%2520Yan%2520and%2520Yong%2520Jiang%2520and%2520Haicheng%2520Liu%2520and%2520Liang%2520Ding%2520and%2520Ling%2520Zou%2520and%2520Yi%2520R.%2520Fung%2520and%2520Yalong%2520Li%2520and%2520Pengjun%2520Xie%26entry.1292438233%3DFoundation%2520agents%2520have%2520rapidly%2520advanced%2520in%2520their%2520ability%2520to%2520reason%2520and%2520interact%2520with%2520real%2520environments%252C%2520making%2520the%2520evaluation%2520of%2520their%2520core%2520capabilities%2520increasingly%2520important.%2520While%2520many%2520benchmarks%2520have%2520been%2520developed%2520to%2520assess%2520agent%2520performance%252C%2520most%2520concentrate%2520on%2520academic%2520settings%2520or%2520artificially%2520designed%2520scenarios%2520while%2520overlooking%2520the%2520challenges%2520that%2520arise%2520in%2520real%2520applications.%2520To%2520address%2520this%2520issue%252C%2520we%2520focus%2520on%2520a%2520highly%2520practical%2520real-world%2520setting%252C%2520the%2520e-commerce%2520domain%252C%2520which%2520involves%2520a%2520large%2520volume%2520of%2520diverse%2520user%2520interactions%252C%2520dynamic%2520market%2520conditions%252C%2520and%2520tasks%2520directly%2520tied%2520to%2520real%2520decision-making%2520processes.%2520To%2520this%2520end%252C%2520we%2520introduce%2520EcomBench%252C%2520a%2520holistic%2520E-commerce%2520Benchmark%2520designed%2520to%2520evaluate%2520agent%2520performance%2520in%2520realistic%2520e-commerce%2520environments.%2520EcomBench%2520is%2520built%2520from%2520genuine%2520user%2520demands%2520embedded%2520in%2520leading%2520global%2520e-commerce%2520ecosystems%2520and%2520is%2520carefully%2520curated%2520and%2520annotated%2520through%2520human%2520experts%2520to%2520ensure%2520clarity%252C%2520accuracy%252C%2520and%2520domain%2520relevance.%2520It%2520covers%2520multiple%2520task%2520categories%2520within%2520e-commerce%2520scenarios%2520and%2520defines%2520three%2520difficulty%2520levels%2520that%2520evaluate%2520agents%2520on%2520key%2520capabilities%2520such%2520as%2520deep%2520information%2520retrieval%252C%2520multi-step%2520reasoning%252C%2520and%2520cross-source%2520knowledge%2520integration.%2520By%2520grounding%2520evaluation%2520in%2520real%2520e-commerce%2520contexts%252C%2520EcomBench%2520provides%2520a%2520rigorous%2520and%2520dynamic%2520testbed%2520for%2520measuring%2520the%2520practical%2520capabilities%2520of%2520agents%2520in%2520modern%2520e-commerce.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EcomBench%3A%20Towards%20Holistic%20Evaluation%20of%20Foundation%20Agents%20in%20E-commerce&entry.906535625=Rui%20Min%20and%20Zile%20Qiao%20and%20Ze%20Xu%20and%20Jiawen%20Zhai%20and%20Wenyu%20Gao%20and%20Xuanzhong%20Chen%20and%20Haozhen%20Sun%20and%20Zhen%20Zhang%20and%20Xinyu%20Wang%20and%20Hong%20Zhou%20and%20Wenbiao%20Yin%20and%20Bo%20Zhang%20and%20Xuan%20Zhou%20and%20Ming%20Yan%20and%20Yong%20Jiang%20and%20Haicheng%20Liu%20and%20Liang%20Ding%20and%20Ling%20Zou%20and%20Yi%20R.%20Fung%20and%20Yalong%20Li%20and%20Pengjun%20Xie&entry.1292438233=Foundation%20agents%20have%20rapidly%20advanced%20in%20their%20ability%20to%20reason%20and%20interact%20with%20real%20environments%2C%20making%20the%20evaluation%20of%20their%20core%20capabilities%20increasingly%20important.%20While%20many%20benchmarks%20have%20been%20developed%20to%20assess%20agent%20performance%2C%20most%20concentrate%20on%20academic%20settings%20or%20artificially%20designed%20scenarios%20while%20overlooking%20the%20challenges%20that%20arise%20in%20real%20applications.%20To%20address%20this%20issue%2C%20we%20focus%20on%20a%20highly%20practical%20real-world%20setting%2C%20the%20e-commerce%20domain%2C%20which%20involves%20a%20large%20volume%20of%20diverse%20user%20interactions%2C%20dynamic%20market%20conditions%2C%20and%20tasks%20directly%20tied%20to%20real%20decision-making%20processes.%20To%20this%20end%2C%20we%20introduce%20EcomBench%2C%20a%20holistic%20E-commerce%20Benchmark%20designed%20to%20evaluate%20agent%20performance%20in%20realistic%20e-commerce%20environments.%20EcomBench%20is%20built%20from%20genuine%20user%20demands%20embedded%20in%20leading%20global%20e-commerce%20ecosystems%20and%20is%20carefully%20curated%20and%20annotated%20through%20human%20experts%20to%20ensure%20clarity%2C%20accuracy%2C%20and%20domain%20relevance.%20It%20covers%20multiple%20task%20categories%20within%20e-commerce%20scenarios%20and%20defines%20three%20difficulty%20levels%20that%20evaluate%20agents%20on%20key%20capabilities%20such%20as%20deep%20information%20retrieval%2C%20multi-step%20reasoning%2C%20and%20cross-source%20knowledge%20integration.%20By%20grounding%20evaluation%20in%20real%20e-commerce%20contexts%2C%20EcomBench%20provides%20a%20rigorous%20and%20dynamic%20testbed%20for%20measuring%20the%20practical%20capabilities%20of%20agents%20in%20modern%20e-commerce.&entry.1838667208=http%3A//arxiv.org/abs/2512.08868v2&entry.124074799=Read"},
{"title": "Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation", "author": "Kevin Glocker and K\u00e4triin Kukk and Romina Oji and Marcel Bollmann and Marco Kuhlmann and Jenny Kunz", "abstract": "Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.", "link": "http://arxiv.org/abs/2512.10772v1", "date": "2025-12-11", "relevancy": 1.9511, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grow%20Up%20and%20Merge%3A%20Scaling%20Strategies%20for%20Efficient%20Language%20Adaptation&body=Title%3A%20Grow%20Up%20and%20Merge%3A%20Scaling%20Strategies%20for%20Efficient%20Language%20Adaptation%0AAuthor%3A%20Kevin%20Glocker%20and%20K%C3%A4triin%20Kukk%20and%20Romina%20Oji%20and%20Marcel%20Bollmann%20and%20Marco%20Kuhlmann%20and%20Jenny%20Kunz%0AAbstract%3A%20Achieving%20high-performing%20language%20models%20which%20include%20medium-%20and%20lower-resource%20languages%20remains%20a%20challenge.%20Massively%20multilingual%20models%20still%20underperform%20compared%20to%20language-specific%20adaptations%2C%20especially%20at%20smaller%20model%20scales.%20In%20this%20work%2C%20we%20investigate%20scaling%20as%20an%20efficient%20strategy%20for%20adapting%20pretrained%20models%20to%20new%20target%20languages.%20Through%20comprehensive%20scaling%20ablations%20with%20approximately%20FLOP-matched%20models%2C%20we%20test%20whether%20upscaling%20an%20English%20base%20model%20enables%20more%20effective%20and%20resource-efficient%20adaptation%20than%20standard%20continued%20pretraining.%20We%20find%20that%2C%20once%20exposed%20to%20sufficient%20target-language%20data%2C%20larger%20upscaled%20models%20can%20match%20or%20surpass%20the%20performance%20of%20smaller%20models%20continually%20pretrained%20on%20much%20more%20data%2C%20demonstrating%20the%20benefits%20of%20scaling%20for%20data%20efficiency.%20Scaling%20also%20helps%20preserve%20the%20base%20model%27s%20capabilities%20in%20English%2C%20thus%20reducing%20catastrophic%20forgetting.%20Finally%2C%20we%20explore%20whether%20such%20scaled%2C%20language-specific%20models%20can%20be%20merged%20to%20construct%20modular%20and%20flexible%20multilingual%20systems.%20We%20find%20that%20while%20merging%20remains%20less%20effective%20than%20joint%20multilingual%20training%2C%20upscaled%20merges%20perform%20better%20than%20smaller%20ones.%20We%20observe%20large%20performance%20differences%20across%20merging%20methods%2C%20suggesting%20potential%20for%20improvement%20through%20merging%20approaches%20specialized%20for%20language-level%20integration.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrow%2520Up%2520and%2520Merge%253A%2520Scaling%2520Strategies%2520for%2520Efficient%2520Language%2520Adaptation%26entry.906535625%3DKevin%2520Glocker%2520and%2520K%25C3%25A4triin%2520Kukk%2520and%2520Romina%2520Oji%2520and%2520Marcel%2520Bollmann%2520and%2520Marco%2520Kuhlmann%2520and%2520Jenny%2520Kunz%26entry.1292438233%3DAchieving%2520high-performing%2520language%2520models%2520which%2520include%2520medium-%2520and%2520lower-resource%2520languages%2520remains%2520a%2520challenge.%2520Massively%2520multilingual%2520models%2520still%2520underperform%2520compared%2520to%2520language-specific%2520adaptations%252C%2520especially%2520at%2520smaller%2520model%2520scales.%2520In%2520this%2520work%252C%2520we%2520investigate%2520scaling%2520as%2520an%2520efficient%2520strategy%2520for%2520adapting%2520pretrained%2520models%2520to%2520new%2520target%2520languages.%2520Through%2520comprehensive%2520scaling%2520ablations%2520with%2520approximately%2520FLOP-matched%2520models%252C%2520we%2520test%2520whether%2520upscaling%2520an%2520English%2520base%2520model%2520enables%2520more%2520effective%2520and%2520resource-efficient%2520adaptation%2520than%2520standard%2520continued%2520pretraining.%2520We%2520find%2520that%252C%2520once%2520exposed%2520to%2520sufficient%2520target-language%2520data%252C%2520larger%2520upscaled%2520models%2520can%2520match%2520or%2520surpass%2520the%2520performance%2520of%2520smaller%2520models%2520continually%2520pretrained%2520on%2520much%2520more%2520data%252C%2520demonstrating%2520the%2520benefits%2520of%2520scaling%2520for%2520data%2520efficiency.%2520Scaling%2520also%2520helps%2520preserve%2520the%2520base%2520model%2527s%2520capabilities%2520in%2520English%252C%2520thus%2520reducing%2520catastrophic%2520forgetting.%2520Finally%252C%2520we%2520explore%2520whether%2520such%2520scaled%252C%2520language-specific%2520models%2520can%2520be%2520merged%2520to%2520construct%2520modular%2520and%2520flexible%2520multilingual%2520systems.%2520We%2520find%2520that%2520while%2520merging%2520remains%2520less%2520effective%2520than%2520joint%2520multilingual%2520training%252C%2520upscaled%2520merges%2520perform%2520better%2520than%2520smaller%2520ones.%2520We%2520observe%2520large%2520performance%2520differences%2520across%2520merging%2520methods%252C%2520suggesting%2520potential%2520for%2520improvement%2520through%2520merging%2520approaches%2520specialized%2520for%2520language-level%2520integration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grow%20Up%20and%20Merge%3A%20Scaling%20Strategies%20for%20Efficient%20Language%20Adaptation&entry.906535625=Kevin%20Glocker%20and%20K%C3%A4triin%20Kukk%20and%20Romina%20Oji%20and%20Marcel%20Bollmann%20and%20Marco%20Kuhlmann%20and%20Jenny%20Kunz&entry.1292438233=Achieving%20high-performing%20language%20models%20which%20include%20medium-%20and%20lower-resource%20languages%20remains%20a%20challenge.%20Massively%20multilingual%20models%20still%20underperform%20compared%20to%20language-specific%20adaptations%2C%20especially%20at%20smaller%20model%20scales.%20In%20this%20work%2C%20we%20investigate%20scaling%20as%20an%20efficient%20strategy%20for%20adapting%20pretrained%20models%20to%20new%20target%20languages.%20Through%20comprehensive%20scaling%20ablations%20with%20approximately%20FLOP-matched%20models%2C%20we%20test%20whether%20upscaling%20an%20English%20base%20model%20enables%20more%20effective%20and%20resource-efficient%20adaptation%20than%20standard%20continued%20pretraining.%20We%20find%20that%2C%20once%20exposed%20to%20sufficient%20target-language%20data%2C%20larger%20upscaled%20models%20can%20match%20or%20surpass%20the%20performance%20of%20smaller%20models%20continually%20pretrained%20on%20much%20more%20data%2C%20demonstrating%20the%20benefits%20of%20scaling%20for%20data%20efficiency.%20Scaling%20also%20helps%20preserve%20the%20base%20model%27s%20capabilities%20in%20English%2C%20thus%20reducing%20catastrophic%20forgetting.%20Finally%2C%20we%20explore%20whether%20such%20scaled%2C%20language-specific%20models%20can%20be%20merged%20to%20construct%20modular%20and%20flexible%20multilingual%20systems.%20We%20find%20that%20while%20merging%20remains%20less%20effective%20than%20joint%20multilingual%20training%2C%20upscaled%20merges%20perform%20better%20than%20smaller%20ones.%20We%20observe%20large%20performance%20differences%20across%20merging%20methods%2C%20suggesting%20potential%20for%20improvement%20through%20merging%20approaches%20specialized%20for%20language-level%20integration.&entry.1838667208=http%3A//arxiv.org/abs/2512.10772v1&entry.124074799=Read"},
{"title": "FE-MCFormer: An interpretable fault diagnosis framework for rotating machinery under strong noise based on time-frequency fusion transformer", "author": "Yuhan Yuan and Xiaomo Jiang and Haibin Yang and Haixin Zhao and Shengbo Wang and Xueyu Cheng and Jigang Meng and Shuhua Yang", "abstract": "Many fault diagnosis methods of rotating machines are based on discriminative features extracted from signals collected from the key components such as bearings. However, under complex operating conditions, periodic impulsive characteristics in the signal related to weak fault information are often obscured by noise interference. Consequently, existing approaches struggle to learn interpretable fault-related features in such scenarios. This paper proposes a novel transformer framework (FE-MCFormer) to extract interpretable time-frequency features, with the aim of improving the fault detection accuracy and intrepretability of rotating machines under strong noise. First, a Fourier adaptive reconstruction embedding layer is introduced as a global information encoder in the model. Subsequently, a time-frequency fusion module is designed, further improve the model robustness and interpretability. The effectiveness of FE-MCFormer in machine fault diagnosis is validated through three case studies.", "link": "http://arxiv.org/abs/2505.06285v2", "date": "2025-12-11", "relevancy": 1.3732, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4743}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4717}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FE-MCFormer%3A%20An%20interpretable%20fault%20diagnosis%20framework%20for%20rotating%20machinery%20under%20strong%20noise%20based%20on%20time-frequency%20fusion%20transformer&body=Title%3A%20FE-MCFormer%3A%20An%20interpretable%20fault%20diagnosis%20framework%20for%20rotating%20machinery%20under%20strong%20noise%20based%20on%20time-frequency%20fusion%20transformer%0AAuthor%3A%20Yuhan%20Yuan%20and%20Xiaomo%20Jiang%20and%20Haibin%20Yang%20and%20Haixin%20Zhao%20and%20Shengbo%20Wang%20and%20Xueyu%20Cheng%20and%20Jigang%20Meng%20and%20Shuhua%20Yang%0AAbstract%3A%20Many%20fault%20diagnosis%20methods%20of%20rotating%20machines%20are%20based%20on%20discriminative%20features%20extracted%20from%20signals%20collected%20from%20the%20key%20components%20such%20as%20bearings.%20However%2C%20under%20complex%20operating%20conditions%2C%20periodic%20impulsive%20characteristics%20in%20the%20signal%20related%20to%20weak%20fault%20information%20are%20often%20obscured%20by%20noise%20interference.%20Consequently%2C%20existing%20approaches%20struggle%20to%20learn%20interpretable%20fault-related%20features%20in%20such%20scenarios.%20This%20paper%20proposes%20a%20novel%20transformer%20framework%20%28FE-MCFormer%29%20to%20extract%20interpretable%20time-frequency%20features%2C%20with%20the%20aim%20of%20improving%20the%20fault%20detection%20accuracy%20and%20intrepretability%20of%20rotating%20machines%20under%20strong%20noise.%20First%2C%20a%20Fourier%20adaptive%20reconstruction%20embedding%20layer%20is%20introduced%20as%20a%20global%20information%20encoder%20in%20the%20model.%20Subsequently%2C%20a%20time-frequency%20fusion%20module%20is%20designed%2C%20further%20improve%20the%20model%20robustness%20and%20interpretability.%20The%20effectiveness%20of%20FE-MCFormer%20in%20machine%20fault%20diagnosis%20is%20validated%20through%20three%20case%20studies.%0ALink%3A%20http%3A//arxiv.org/abs/2505.06285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFE-MCFormer%253A%2520An%2520interpretable%2520fault%2520diagnosis%2520framework%2520for%2520rotating%2520machinery%2520under%2520strong%2520noise%2520based%2520on%2520time-frequency%2520fusion%2520transformer%26entry.906535625%3DYuhan%2520Yuan%2520and%2520Xiaomo%2520Jiang%2520and%2520Haibin%2520Yang%2520and%2520Haixin%2520Zhao%2520and%2520Shengbo%2520Wang%2520and%2520Xueyu%2520Cheng%2520and%2520Jigang%2520Meng%2520and%2520Shuhua%2520Yang%26entry.1292438233%3DMany%2520fault%2520diagnosis%2520methods%2520of%2520rotating%2520machines%2520are%2520based%2520on%2520discriminative%2520features%2520extracted%2520from%2520signals%2520collected%2520from%2520the%2520key%2520components%2520such%2520as%2520bearings.%2520However%252C%2520under%2520complex%2520operating%2520conditions%252C%2520periodic%2520impulsive%2520characteristics%2520in%2520the%2520signal%2520related%2520to%2520weak%2520fault%2520information%2520are%2520often%2520obscured%2520by%2520noise%2520interference.%2520Consequently%252C%2520existing%2520approaches%2520struggle%2520to%2520learn%2520interpretable%2520fault-related%2520features%2520in%2520such%2520scenarios.%2520This%2520paper%2520proposes%2520a%2520novel%2520transformer%2520framework%2520%2528FE-MCFormer%2529%2520to%2520extract%2520interpretable%2520time-frequency%2520features%252C%2520with%2520the%2520aim%2520of%2520improving%2520the%2520fault%2520detection%2520accuracy%2520and%2520intrepretability%2520of%2520rotating%2520machines%2520under%2520strong%2520noise.%2520First%252C%2520a%2520Fourier%2520adaptive%2520reconstruction%2520embedding%2520layer%2520is%2520introduced%2520as%2520a%2520global%2520information%2520encoder%2520in%2520the%2520model.%2520Subsequently%252C%2520a%2520time-frequency%2520fusion%2520module%2520is%2520designed%252C%2520further%2520improve%2520the%2520model%2520robustness%2520and%2520interpretability.%2520The%2520effectiveness%2520of%2520FE-MCFormer%2520in%2520machine%2520fault%2520diagnosis%2520is%2520validated%2520through%2520three%2520case%2520studies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FE-MCFormer%3A%20An%20interpretable%20fault%20diagnosis%20framework%20for%20rotating%20machinery%20under%20strong%20noise%20based%20on%20time-frequency%20fusion%20transformer&entry.906535625=Yuhan%20Yuan%20and%20Xiaomo%20Jiang%20and%20Haibin%20Yang%20and%20Haixin%20Zhao%20and%20Shengbo%20Wang%20and%20Xueyu%20Cheng%20and%20Jigang%20Meng%20and%20Shuhua%20Yang&entry.1292438233=Many%20fault%20diagnosis%20methods%20of%20rotating%20machines%20are%20based%20on%20discriminative%20features%20extracted%20from%20signals%20collected%20from%20the%20key%20components%20such%20as%20bearings.%20However%2C%20under%20complex%20operating%20conditions%2C%20periodic%20impulsive%20characteristics%20in%20the%20signal%20related%20to%20weak%20fault%20information%20are%20often%20obscured%20by%20noise%20interference.%20Consequently%2C%20existing%20approaches%20struggle%20to%20learn%20interpretable%20fault-related%20features%20in%20such%20scenarios.%20This%20paper%20proposes%20a%20novel%20transformer%20framework%20%28FE-MCFormer%29%20to%20extract%20interpretable%20time-frequency%20features%2C%20with%20the%20aim%20of%20improving%20the%20fault%20detection%20accuracy%20and%20intrepretability%20of%20rotating%20machines%20under%20strong%20noise.%20First%2C%20a%20Fourier%20adaptive%20reconstruction%20embedding%20layer%20is%20introduced%20as%20a%20global%20information%20encoder%20in%20the%20model.%20Subsequently%2C%20a%20time-frequency%20fusion%20module%20is%20designed%2C%20further%20improve%20the%20model%20robustness%20and%20interpretability.%20The%20effectiveness%20of%20FE-MCFormer%20in%20machine%20fault%20diagnosis%20is%20validated%20through%20three%20case%20studies.&entry.1838667208=http%3A//arxiv.org/abs/2505.06285v2&entry.124074799=Read"},
{"title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation", "author": "Lim Chien Her and Ming Yan and Yunshu Bai and Ruihao Li and Hao Zhang", "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.", "link": "http://arxiv.org/abs/2512.10501v1", "date": "2025-12-11", "relevancy": 1.7811, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6055}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5868}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%203D%20Map%20Generation%20with%20LLM%20Agents%3A%20A%20Dual-Agent%20Architecture%20for%20Procedural%20Content%20Generation&body=Title%3A%20Zero-shot%203D%20Map%20Generation%20with%20LLM%20Agents%3A%20A%20Dual-Agent%20Architecture%20for%20Procedural%20Content%20Generation%0AAuthor%3A%20Lim%20Chien%20Her%20and%20Ming%20Yan%20and%20Yunshu%20Bai%20and%20Ruihao%20Li%20and%20Hao%20Zhang%0AAbstract%3A%20Procedural%20Content%20Generation%20%28PCG%29%20offers%20scalable%20methods%20for%20algorithmically%20creating%20complex%2C%20customizable%20worlds.%20However%2C%20controlling%20these%20pipelines%20requires%20the%20precise%20configuration%20of%20opaque%20technical%20parameters.%20We%20propose%20a%20training-free%20architecture%20that%20utilizes%20LLM%20agents%20for%20zero-shot%20PCG%20parameter%20configuration.%20While%20Large%20Language%20Models%20%28LLMs%29%20promise%20a%20natural%20language%20interface%20for%20PCG%20tools%2C%20off-the-shelf%20models%20often%20fail%20to%20bridge%20the%20semantic%20gap%20between%20abstract%20user%20instructions%20and%20strict%20parameter%20specifications.%20Our%20system%20pairs%20an%20Actor%20agent%20with%20a%20Critic%20agent%2C%20enabling%20an%20iterative%20workflow%20where%20the%20system%20autonomously%20reasons%20over%20tool%20parameters%20and%20refines%20configurations%20to%20progressively%20align%20with%20human%20design%20preferences.%20We%20validate%20this%20approach%20on%20the%20generation%20of%20various%203D%20maps%2C%20establishing%20a%20new%20benchmark%20for%20instruction-following%20in%20PCG.%20Experiments%20demonstrate%20that%20our%20approach%20outperforms%20single-agent%20baselines%2C%20producing%20diverse%20and%20structurally%20valid%20environments%20from%20natural%20language%20descriptions.%20These%20results%20demonstrate%20that%20off-the-shelf%20LLMs%20can%20be%20effectively%20repurposed%20as%20generalized%20agents%20for%20arbitrary%20PCG%20tools.%20By%20shifting%20the%20burden%20from%20model%20training%20to%20architectural%20reasoning%2C%20our%20method%20offers%20a%20scalable%20framework%20for%20mastering%20complex%20software%20without%20task-specific%20fine-tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%25203D%2520Map%2520Generation%2520with%2520LLM%2520Agents%253A%2520A%2520Dual-Agent%2520Architecture%2520for%2520Procedural%2520Content%2520Generation%26entry.906535625%3DLim%2520Chien%2520Her%2520and%2520Ming%2520Yan%2520and%2520Yunshu%2520Bai%2520and%2520Ruihao%2520Li%2520and%2520Hao%2520Zhang%26entry.1292438233%3DProcedural%2520Content%2520Generation%2520%2528PCG%2529%2520offers%2520scalable%2520methods%2520for%2520algorithmically%2520creating%2520complex%252C%2520customizable%2520worlds.%2520However%252C%2520controlling%2520these%2520pipelines%2520requires%2520the%2520precise%2520configuration%2520of%2520opaque%2520technical%2520parameters.%2520We%2520propose%2520a%2520training-free%2520architecture%2520that%2520utilizes%2520LLM%2520agents%2520for%2520zero-shot%2520PCG%2520parameter%2520configuration.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520promise%2520a%2520natural%2520language%2520interface%2520for%2520PCG%2520tools%252C%2520off-the-shelf%2520models%2520often%2520fail%2520to%2520bridge%2520the%2520semantic%2520gap%2520between%2520abstract%2520user%2520instructions%2520and%2520strict%2520parameter%2520specifications.%2520Our%2520system%2520pairs%2520an%2520Actor%2520agent%2520with%2520a%2520Critic%2520agent%252C%2520enabling%2520an%2520iterative%2520workflow%2520where%2520the%2520system%2520autonomously%2520reasons%2520over%2520tool%2520parameters%2520and%2520refines%2520configurations%2520to%2520progressively%2520align%2520with%2520human%2520design%2520preferences.%2520We%2520validate%2520this%2520approach%2520on%2520the%2520generation%2520of%2520various%25203D%2520maps%252C%2520establishing%2520a%2520new%2520benchmark%2520for%2520instruction-following%2520in%2520PCG.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520single-agent%2520baselines%252C%2520producing%2520diverse%2520and%2520structurally%2520valid%2520environments%2520from%2520natural%2520language%2520descriptions.%2520These%2520results%2520demonstrate%2520that%2520off-the-shelf%2520LLMs%2520can%2520be%2520effectively%2520repurposed%2520as%2520generalized%2520agents%2520for%2520arbitrary%2520PCG%2520tools.%2520By%2520shifting%2520the%2520burden%2520from%2520model%2520training%2520to%2520architectural%2520reasoning%252C%2520our%2520method%2520offers%2520a%2520scalable%2520framework%2520for%2520mastering%2520complex%2520software%2520without%2520task-specific%2520fine-tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%203D%20Map%20Generation%20with%20LLM%20Agents%3A%20A%20Dual-Agent%20Architecture%20for%20Procedural%20Content%20Generation&entry.906535625=Lim%20Chien%20Her%20and%20Ming%20Yan%20and%20Yunshu%20Bai%20and%20Ruihao%20Li%20and%20Hao%20Zhang&entry.1292438233=Procedural%20Content%20Generation%20%28PCG%29%20offers%20scalable%20methods%20for%20algorithmically%20creating%20complex%2C%20customizable%20worlds.%20However%2C%20controlling%20these%20pipelines%20requires%20the%20precise%20configuration%20of%20opaque%20technical%20parameters.%20We%20propose%20a%20training-free%20architecture%20that%20utilizes%20LLM%20agents%20for%20zero-shot%20PCG%20parameter%20configuration.%20While%20Large%20Language%20Models%20%28LLMs%29%20promise%20a%20natural%20language%20interface%20for%20PCG%20tools%2C%20off-the-shelf%20models%20often%20fail%20to%20bridge%20the%20semantic%20gap%20between%20abstract%20user%20instructions%20and%20strict%20parameter%20specifications.%20Our%20system%20pairs%20an%20Actor%20agent%20with%20a%20Critic%20agent%2C%20enabling%20an%20iterative%20workflow%20where%20the%20system%20autonomously%20reasons%20over%20tool%20parameters%20and%20refines%20configurations%20to%20progressively%20align%20with%20human%20design%20preferences.%20We%20validate%20this%20approach%20on%20the%20generation%20of%20various%203D%20maps%2C%20establishing%20a%20new%20benchmark%20for%20instruction-following%20in%20PCG.%20Experiments%20demonstrate%20that%20our%20approach%20outperforms%20single-agent%20baselines%2C%20producing%20diverse%20and%20structurally%20valid%20environments%20from%20natural%20language%20descriptions.%20These%20results%20demonstrate%20that%20off-the-shelf%20LLMs%20can%20be%20effectively%20repurposed%20as%20generalized%20agents%20for%20arbitrary%20PCG%20tools.%20By%20shifting%20the%20burden%20from%20model%20training%20to%20architectural%20reasoning%2C%20our%20method%20offers%20a%20scalable%20framework%20for%20mastering%20complex%20software%20without%20task-specific%20fine-tuning.&entry.1838667208=http%3A//arxiv.org/abs/2512.10501v1&entry.124074799=Read"},
{"title": "A Differentiable Digital Twin of Distributed Link Scheduling for Contention-Aware Networking", "author": "Zhongyuan Zhao and Yujun Ming and Kevin Chan and Ananthram Swami and Santiago Segarra", "abstract": "Many routing and flow optimization problems in wired networks can be solved efficiently using minimum cost flow formulations. However, this approach does not extend to wireless multi-hop networks, where the assumptions of fixed link capacity and linear cost structure collapse due to contention for shared spectrum resources. The key challenge is that the long-term capacity of a wireless link becomes a non-linear function of its network context, including network topology, link quality, and the traffic assigned to neighboring links. In this work, we pursue a new direction of modeling wireless network under randomized medium access control by developing an analytical network digital twin (NDT) that predicts link duty cycles from network context. We generalize randomized contention as finding a Maximal Independent Set (MIS) on the conflict graph using weighted Luby's algorithm, derive an analytical model of link duty cycles, and introduce an iterative procedure that resolves the circular dependency among duty cycle, link capacity, and contention probability. Our numerical experiments show that the proposed NDT accurately predicts link duty cycles and congestion patterns with up to a 5000x speedup over packet-level simulation, and enables us to optimize link scheduling using gradient descent for reduced congestion and radio footprint.", "link": "http://arxiv.org/abs/2512.10874v1", "date": "2025-12-11", "relevancy": 1.3925, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4799}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4629}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Differentiable%20Digital%20Twin%20of%20Distributed%20Link%20Scheduling%20for%20Contention-Aware%20Networking&body=Title%3A%20A%20Differentiable%20Digital%20Twin%20of%20Distributed%20Link%20Scheduling%20for%20Contention-Aware%20Networking%0AAuthor%3A%20Zhongyuan%20Zhao%20and%20Yujun%20Ming%20and%20Kevin%20Chan%20and%20Ananthram%20Swami%20and%20Santiago%20Segarra%0AAbstract%3A%20Many%20routing%20and%20flow%20optimization%20problems%20in%20wired%20networks%20can%20be%20solved%20efficiently%20using%20minimum%20cost%20flow%20formulations.%20However%2C%20this%20approach%20does%20not%20extend%20to%20wireless%20multi-hop%20networks%2C%20where%20the%20assumptions%20of%20fixed%20link%20capacity%20and%20linear%20cost%20structure%20collapse%20due%20to%20contention%20for%20shared%20spectrum%20resources.%20The%20key%20challenge%20is%20that%20the%20long-term%20capacity%20of%20a%20wireless%20link%20becomes%20a%20non-linear%20function%20of%20its%20network%20context%2C%20including%20network%20topology%2C%20link%20quality%2C%20and%20the%20traffic%20assigned%20to%20neighboring%20links.%20In%20this%20work%2C%20we%20pursue%20a%20new%20direction%20of%20modeling%20wireless%20network%20under%20randomized%20medium%20access%20control%20by%20developing%20an%20analytical%20network%20digital%20twin%20%28NDT%29%20that%20predicts%20link%20duty%20cycles%20from%20network%20context.%20We%20generalize%20randomized%20contention%20as%20finding%20a%20Maximal%20Independent%20Set%20%28MIS%29%20on%20the%20conflict%20graph%20using%20weighted%20Luby%27s%20algorithm%2C%20derive%20an%20analytical%20model%20of%20link%20duty%20cycles%2C%20and%20introduce%20an%20iterative%20procedure%20that%20resolves%20the%20circular%20dependency%20among%20duty%20cycle%2C%20link%20capacity%2C%20and%20contention%20probability.%20Our%20numerical%20experiments%20show%20that%20the%20proposed%20NDT%20accurately%20predicts%20link%20duty%20cycles%20and%20congestion%20patterns%20with%20up%20to%20a%205000x%20speedup%20over%20packet-level%20simulation%2C%20and%20enables%20us%20to%20optimize%20link%20scheduling%20using%20gradient%20descent%20for%20reduced%20congestion%20and%20radio%20footprint.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Differentiable%2520Digital%2520Twin%2520of%2520Distributed%2520Link%2520Scheduling%2520for%2520Contention-Aware%2520Networking%26entry.906535625%3DZhongyuan%2520Zhao%2520and%2520Yujun%2520Ming%2520and%2520Kevin%2520Chan%2520and%2520Ananthram%2520Swami%2520and%2520Santiago%2520Segarra%26entry.1292438233%3DMany%2520routing%2520and%2520flow%2520optimization%2520problems%2520in%2520wired%2520networks%2520can%2520be%2520solved%2520efficiently%2520using%2520minimum%2520cost%2520flow%2520formulations.%2520However%252C%2520this%2520approach%2520does%2520not%2520extend%2520to%2520wireless%2520multi-hop%2520networks%252C%2520where%2520the%2520assumptions%2520of%2520fixed%2520link%2520capacity%2520and%2520linear%2520cost%2520structure%2520collapse%2520due%2520to%2520contention%2520for%2520shared%2520spectrum%2520resources.%2520The%2520key%2520challenge%2520is%2520that%2520the%2520long-term%2520capacity%2520of%2520a%2520wireless%2520link%2520becomes%2520a%2520non-linear%2520function%2520of%2520its%2520network%2520context%252C%2520including%2520network%2520topology%252C%2520link%2520quality%252C%2520and%2520the%2520traffic%2520assigned%2520to%2520neighboring%2520links.%2520In%2520this%2520work%252C%2520we%2520pursue%2520a%2520new%2520direction%2520of%2520modeling%2520wireless%2520network%2520under%2520randomized%2520medium%2520access%2520control%2520by%2520developing%2520an%2520analytical%2520network%2520digital%2520twin%2520%2528NDT%2529%2520that%2520predicts%2520link%2520duty%2520cycles%2520from%2520network%2520context.%2520We%2520generalize%2520randomized%2520contention%2520as%2520finding%2520a%2520Maximal%2520Independent%2520Set%2520%2528MIS%2529%2520on%2520the%2520conflict%2520graph%2520using%2520weighted%2520Luby%2527s%2520algorithm%252C%2520derive%2520an%2520analytical%2520model%2520of%2520link%2520duty%2520cycles%252C%2520and%2520introduce%2520an%2520iterative%2520procedure%2520that%2520resolves%2520the%2520circular%2520dependency%2520among%2520duty%2520cycle%252C%2520link%2520capacity%252C%2520and%2520contention%2520probability.%2520Our%2520numerical%2520experiments%2520show%2520that%2520the%2520proposed%2520NDT%2520accurately%2520predicts%2520link%2520duty%2520cycles%2520and%2520congestion%2520patterns%2520with%2520up%2520to%2520a%25205000x%2520speedup%2520over%2520packet-level%2520simulation%252C%2520and%2520enables%2520us%2520to%2520optimize%2520link%2520scheduling%2520using%2520gradient%2520descent%2520for%2520reduced%2520congestion%2520and%2520radio%2520footprint.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Differentiable%20Digital%20Twin%20of%20Distributed%20Link%20Scheduling%20for%20Contention-Aware%20Networking&entry.906535625=Zhongyuan%20Zhao%20and%20Yujun%20Ming%20and%20Kevin%20Chan%20and%20Ananthram%20Swami%20and%20Santiago%20Segarra&entry.1292438233=Many%20routing%20and%20flow%20optimization%20problems%20in%20wired%20networks%20can%20be%20solved%20efficiently%20using%20minimum%20cost%20flow%20formulations.%20However%2C%20this%20approach%20does%20not%20extend%20to%20wireless%20multi-hop%20networks%2C%20where%20the%20assumptions%20of%20fixed%20link%20capacity%20and%20linear%20cost%20structure%20collapse%20due%20to%20contention%20for%20shared%20spectrum%20resources.%20The%20key%20challenge%20is%20that%20the%20long-term%20capacity%20of%20a%20wireless%20link%20becomes%20a%20non-linear%20function%20of%20its%20network%20context%2C%20including%20network%20topology%2C%20link%20quality%2C%20and%20the%20traffic%20assigned%20to%20neighboring%20links.%20In%20this%20work%2C%20we%20pursue%20a%20new%20direction%20of%20modeling%20wireless%20network%20under%20randomized%20medium%20access%20control%20by%20developing%20an%20analytical%20network%20digital%20twin%20%28NDT%29%20that%20predicts%20link%20duty%20cycles%20from%20network%20context.%20We%20generalize%20randomized%20contention%20as%20finding%20a%20Maximal%20Independent%20Set%20%28MIS%29%20on%20the%20conflict%20graph%20using%20weighted%20Luby%27s%20algorithm%2C%20derive%20an%20analytical%20model%20of%20link%20duty%20cycles%2C%20and%20introduce%20an%20iterative%20procedure%20that%20resolves%20the%20circular%20dependency%20among%20duty%20cycle%2C%20link%20capacity%2C%20and%20contention%20probability.%20Our%20numerical%20experiments%20show%20that%20the%20proposed%20NDT%20accurately%20predicts%20link%20duty%20cycles%20and%20congestion%20patterns%20with%20up%20to%20a%205000x%20speedup%20over%20packet-level%20simulation%2C%20and%20enables%20us%20to%20optimize%20link%20scheduling%20using%20gradient%20descent%20for%20reduced%20congestion%20and%20radio%20footprint.&entry.1838667208=http%3A//arxiv.org/abs/2512.10874v1&entry.124074799=Read"},
{"title": "V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions", "author": "Mumuksh Tayal and Manan Tayal and Aditya Singh and Shishir Kolathaya and Ravi Prakash", "abstract": "Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.", "link": "http://arxiv.org/abs/2512.10822v1", "date": "2025-12-11", "relevancy": 2.0199, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5338}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-OCBF%3A%20Learning%20Safety%20Filters%20from%20Offline%20Data%20via%20Value-Guided%20Offline%20Control%20Barrier%20Functions&body=Title%3A%20V-OCBF%3A%20Learning%20Safety%20Filters%20from%20Offline%20Data%20via%20Value-Guided%20Offline%20Control%20Barrier%20Functions%0AAuthor%3A%20Mumuksh%20Tayal%20and%20Manan%20Tayal%20and%20Aditya%20Singh%20and%20Shishir%20Kolathaya%20and%20Ravi%20Prakash%0AAbstract%3A%20Ensuring%20safety%20in%20autonomous%20systems%20requires%20controllers%20that%20satisfy%20hard%2C%20state-wise%20constraints%20without%20relying%20on%20online%20interaction.%20While%20existing%20Safe%20Offline%20RL%20methods%20typically%20enforce%20soft%20expected-cost%20constraints%2C%20they%20do%20not%20guarantee%20forward%20invariance.%20Conversely%2C%20Control%20Barrier%20Functions%20%28CBFs%29%20provide%20rigorous%20safety%20guarantees%20but%20usually%20depend%20on%20expert-designed%20barrier%20functions%20or%20full%20knowledge%20of%20the%20system%20dynamics.%20We%20introduce%20Value-Guided%20Offline%20Control%20Barrier%20Functions%20%28V-OCBF%29%2C%20a%20framework%20that%20learns%20a%20neural%20CBF%20entirely%20from%20offline%20demonstrations.%20Unlike%20prior%20approaches%2C%20V-OCBF%20does%20not%20assume%20access%20to%20the%20dynamics%20model%3B%20instead%2C%20it%20derives%20a%20recursive%20finite-difference%20barrier%20update%2C%20enabling%20model-free%20learning%20of%20a%20barrier%20that%20propagates%20safety%20information%20over%20time.%20Moreover%2C%20V-OCBF%20incorporates%20an%20expectile-based%20objective%20that%20avoids%20querying%20the%20barrier%20on%20out-of-distribution%20actions%20and%20restricts%20updates%20to%20the%20dataset-supported%20action%20set.%20The%20learned%20barrier%20is%20then%20used%20with%20a%20Quadratic%20Program%20%28QP%29%20formulation%20to%20synthesize%20real-time%20safe%20control.%20Across%20multiple%20case%20studies%2C%20V-OCBF%20yields%20substantially%20fewer%20safety%20violations%20than%20baseline%20methods%20while%20maintaining%20strong%20task%20performance%2C%20highlighting%20its%20scalability%20for%20offline%20synthesis%20of%20safety-critical%20controllers%20without%20online%20interaction%20or%20hand-engineered%20barriers.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-OCBF%253A%2520Learning%2520Safety%2520Filters%2520from%2520Offline%2520Data%2520via%2520Value-Guided%2520Offline%2520Control%2520Barrier%2520Functions%26entry.906535625%3DMumuksh%2520Tayal%2520and%2520Manan%2520Tayal%2520and%2520Aditya%2520Singh%2520and%2520Shishir%2520Kolathaya%2520and%2520Ravi%2520Prakash%26entry.1292438233%3DEnsuring%2520safety%2520in%2520autonomous%2520systems%2520requires%2520controllers%2520that%2520satisfy%2520hard%252C%2520state-wise%2520constraints%2520without%2520relying%2520on%2520online%2520interaction.%2520While%2520existing%2520Safe%2520Offline%2520RL%2520methods%2520typically%2520enforce%2520soft%2520expected-cost%2520constraints%252C%2520they%2520do%2520not%2520guarantee%2520forward%2520invariance.%2520Conversely%252C%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529%2520provide%2520rigorous%2520safety%2520guarantees%2520but%2520usually%2520depend%2520on%2520expert-designed%2520barrier%2520functions%2520or%2520full%2520knowledge%2520of%2520the%2520system%2520dynamics.%2520We%2520introduce%2520Value-Guided%2520Offline%2520Control%2520Barrier%2520Functions%2520%2528V-OCBF%2529%252C%2520a%2520framework%2520that%2520learns%2520a%2520neural%2520CBF%2520entirely%2520from%2520offline%2520demonstrations.%2520Unlike%2520prior%2520approaches%252C%2520V-OCBF%2520does%2520not%2520assume%2520access%2520to%2520the%2520dynamics%2520model%253B%2520instead%252C%2520it%2520derives%2520a%2520recursive%2520finite-difference%2520barrier%2520update%252C%2520enabling%2520model-free%2520learning%2520of%2520a%2520barrier%2520that%2520propagates%2520safety%2520information%2520over%2520time.%2520Moreover%252C%2520V-OCBF%2520incorporates%2520an%2520expectile-based%2520objective%2520that%2520avoids%2520querying%2520the%2520barrier%2520on%2520out-of-distribution%2520actions%2520and%2520restricts%2520updates%2520to%2520the%2520dataset-supported%2520action%2520set.%2520The%2520learned%2520barrier%2520is%2520then%2520used%2520with%2520a%2520Quadratic%2520Program%2520%2528QP%2529%2520formulation%2520to%2520synthesize%2520real-time%2520safe%2520control.%2520Across%2520multiple%2520case%2520studies%252C%2520V-OCBF%2520yields%2520substantially%2520fewer%2520safety%2520violations%2520than%2520baseline%2520methods%2520while%2520maintaining%2520strong%2520task%2520performance%252C%2520highlighting%2520its%2520scalability%2520for%2520offline%2520synthesis%2520of%2520safety-critical%2520controllers%2520without%2520online%2520interaction%2520or%2520hand-engineered%2520barriers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-OCBF%3A%20Learning%20Safety%20Filters%20from%20Offline%20Data%20via%20Value-Guided%20Offline%20Control%20Barrier%20Functions&entry.906535625=Mumuksh%20Tayal%20and%20Manan%20Tayal%20and%20Aditya%20Singh%20and%20Shishir%20Kolathaya%20and%20Ravi%20Prakash&entry.1292438233=Ensuring%20safety%20in%20autonomous%20systems%20requires%20controllers%20that%20satisfy%20hard%2C%20state-wise%20constraints%20without%20relying%20on%20online%20interaction.%20While%20existing%20Safe%20Offline%20RL%20methods%20typically%20enforce%20soft%20expected-cost%20constraints%2C%20they%20do%20not%20guarantee%20forward%20invariance.%20Conversely%2C%20Control%20Barrier%20Functions%20%28CBFs%29%20provide%20rigorous%20safety%20guarantees%20but%20usually%20depend%20on%20expert-designed%20barrier%20functions%20or%20full%20knowledge%20of%20the%20system%20dynamics.%20We%20introduce%20Value-Guided%20Offline%20Control%20Barrier%20Functions%20%28V-OCBF%29%2C%20a%20framework%20that%20learns%20a%20neural%20CBF%20entirely%20from%20offline%20demonstrations.%20Unlike%20prior%20approaches%2C%20V-OCBF%20does%20not%20assume%20access%20to%20the%20dynamics%20model%3B%20instead%2C%20it%20derives%20a%20recursive%20finite-difference%20barrier%20update%2C%20enabling%20model-free%20learning%20of%20a%20barrier%20that%20propagates%20safety%20information%20over%20time.%20Moreover%2C%20V-OCBF%20incorporates%20an%20expectile-based%20objective%20that%20avoids%20querying%20the%20barrier%20on%20out-of-distribution%20actions%20and%20restricts%20updates%20to%20the%20dataset-supported%20action%20set.%20The%20learned%20barrier%20is%20then%20used%20with%20a%20Quadratic%20Program%20%28QP%29%20formulation%20to%20synthesize%20real-time%20safe%20control.%20Across%20multiple%20case%20studies%2C%20V-OCBF%20yields%20substantially%20fewer%20safety%20violations%20than%20baseline%20methods%20while%20maintaining%20strong%20task%20performance%2C%20highlighting%20its%20scalability%20for%20offline%20synthesis%20of%20safety-critical%20controllers%20without%20online%20interaction%20or%20hand-engineered%20barriers.&entry.1838667208=http%3A//arxiv.org/abs/2512.10822v1&entry.124074799=Read"},
{"title": "Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration", "author": "Wenlong Jiao and Heyang Lee and Ping Wang and Pengfei Zhu and Qinghua Hu and Dongwei Ren", "abstract": "All-in-one image restoration aims to handle diverse degradations (e.g., noise, blur, adverse weather) within a unified framework, yet existing methods increasingly rely on complex architectures (e.g., Mixture-of-Experts, diffusion models) and elaborate degradation prompt strategies. In this work, we reveal a critical insight: well-crafted feature extraction inherently encodes degradation-carrying information, and a symmetric U-Net architecture is sufficient to unleash these cues effectively. By aligning feature scales across encoder-decoder and enabling streamlined cross-scale propagation, our symmetric design preserves intrinsic degradation signals robustly, rendering simple additive fusion in skip connections sufficient for state-of-the-art performance. Our primary baseline, SymUNet, is built on this symmetric U-Net and achieves better results across benchmark datasets than existing approaches while reducing computational cost. We further propose a semantic enhanced variant, SE-SymUNet, which integrates direct semantic injection from frozen CLIP features via simple cross-attention to explicitly amplify degradation priors. Extensive experiments on several benchmarks validate the superiority of our methods. Both baselines SymUNet and SE-SymUNet establish simpler and stronger foundations for future advancements in all-in-one image restoration. The source code is available at https://github.com/WenlongJiao/SymUNet.", "link": "http://arxiv.org/abs/2512.10581v1", "date": "2025-12-11", "relevancy": 2.1391, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5696}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5282}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20Degradation-Carrying%20Features%20in%20Symmetric%20U-Net%3A%20Simpler%20and%20Stronger%20Baselines%20for%20All-in-One%20Image%20Restoration&body=Title%3A%20Unleashing%20Degradation-Carrying%20Features%20in%20Symmetric%20U-Net%3A%20Simpler%20and%20Stronger%20Baselines%20for%20All-in-One%20Image%20Restoration%0AAuthor%3A%20Wenlong%20Jiao%20and%20Heyang%20Lee%20and%20Ping%20Wang%20and%20Pengfei%20Zhu%20and%20Qinghua%20Hu%20and%20Dongwei%20Ren%0AAbstract%3A%20All-in-one%20image%20restoration%20aims%20to%20handle%20diverse%20degradations%20%28e.g.%2C%20noise%2C%20blur%2C%20adverse%20weather%29%20within%20a%20unified%20framework%2C%20yet%20existing%20methods%20increasingly%20rely%20on%20complex%20architectures%20%28e.g.%2C%20Mixture-of-Experts%2C%20diffusion%20models%29%20and%20elaborate%20degradation%20prompt%20strategies.%20In%20this%20work%2C%20we%20reveal%20a%20critical%20insight%3A%20well-crafted%20feature%20extraction%20inherently%20encodes%20degradation-carrying%20information%2C%20and%20a%20symmetric%20U-Net%20architecture%20is%20sufficient%20to%20unleash%20these%20cues%20effectively.%20By%20aligning%20feature%20scales%20across%20encoder-decoder%20and%20enabling%20streamlined%20cross-scale%20propagation%2C%20our%20symmetric%20design%20preserves%20intrinsic%20degradation%20signals%20robustly%2C%20rendering%20simple%20additive%20fusion%20in%20skip%20connections%20sufficient%20for%20state-of-the-art%20performance.%20Our%20primary%20baseline%2C%20SymUNet%2C%20is%20built%20on%20this%20symmetric%20U-Net%20and%20achieves%20better%20results%20across%20benchmark%20datasets%20than%20existing%20approaches%20while%20reducing%20computational%20cost.%20We%20further%20propose%20a%20semantic%20enhanced%20variant%2C%20SE-SymUNet%2C%20which%20integrates%20direct%20semantic%20injection%20from%20frozen%20CLIP%20features%20via%20simple%20cross-attention%20to%20explicitly%20amplify%20degradation%20priors.%20Extensive%20experiments%20on%20several%20benchmarks%20validate%20the%20superiority%20of%20our%20methods.%20Both%20baselines%20SymUNet%20and%20SE-SymUNet%20establish%20simpler%20and%20stronger%20foundations%20for%20future%20advancements%20in%20all-in-one%20image%20restoration.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/WenlongJiao/SymUNet.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520Degradation-Carrying%2520Features%2520in%2520Symmetric%2520U-Net%253A%2520Simpler%2520and%2520Stronger%2520Baselines%2520for%2520All-in-One%2520Image%2520Restoration%26entry.906535625%3DWenlong%2520Jiao%2520and%2520Heyang%2520Lee%2520and%2520Ping%2520Wang%2520and%2520Pengfei%2520Zhu%2520and%2520Qinghua%2520Hu%2520and%2520Dongwei%2520Ren%26entry.1292438233%3DAll-in-one%2520image%2520restoration%2520aims%2520to%2520handle%2520diverse%2520degradations%2520%2528e.g.%252C%2520noise%252C%2520blur%252C%2520adverse%2520weather%2529%2520within%2520a%2520unified%2520framework%252C%2520yet%2520existing%2520methods%2520increasingly%2520rely%2520on%2520complex%2520architectures%2520%2528e.g.%252C%2520Mixture-of-Experts%252C%2520diffusion%2520models%2529%2520and%2520elaborate%2520degradation%2520prompt%2520strategies.%2520In%2520this%2520work%252C%2520we%2520reveal%2520a%2520critical%2520insight%253A%2520well-crafted%2520feature%2520extraction%2520inherently%2520encodes%2520degradation-carrying%2520information%252C%2520and%2520a%2520symmetric%2520U-Net%2520architecture%2520is%2520sufficient%2520to%2520unleash%2520these%2520cues%2520effectively.%2520By%2520aligning%2520feature%2520scales%2520across%2520encoder-decoder%2520and%2520enabling%2520streamlined%2520cross-scale%2520propagation%252C%2520our%2520symmetric%2520design%2520preserves%2520intrinsic%2520degradation%2520signals%2520robustly%252C%2520rendering%2520simple%2520additive%2520fusion%2520in%2520skip%2520connections%2520sufficient%2520for%2520state-of-the-art%2520performance.%2520Our%2520primary%2520baseline%252C%2520SymUNet%252C%2520is%2520built%2520on%2520this%2520symmetric%2520U-Net%2520and%2520achieves%2520better%2520results%2520across%2520benchmark%2520datasets%2520than%2520existing%2520approaches%2520while%2520reducing%2520computational%2520cost.%2520We%2520further%2520propose%2520a%2520semantic%2520enhanced%2520variant%252C%2520SE-SymUNet%252C%2520which%2520integrates%2520direct%2520semantic%2520injection%2520from%2520frozen%2520CLIP%2520features%2520via%2520simple%2520cross-attention%2520to%2520explicitly%2520amplify%2520degradation%2520priors.%2520Extensive%2520experiments%2520on%2520several%2520benchmarks%2520validate%2520the%2520superiority%2520of%2520our%2520methods.%2520Both%2520baselines%2520SymUNet%2520and%2520SE-SymUNet%2520establish%2520simpler%2520and%2520stronger%2520foundations%2520for%2520future%2520advancements%2520in%2520all-in-one%2520image%2520restoration.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/WenlongJiao/SymUNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20Degradation-Carrying%20Features%20in%20Symmetric%20U-Net%3A%20Simpler%20and%20Stronger%20Baselines%20for%20All-in-One%20Image%20Restoration&entry.906535625=Wenlong%20Jiao%20and%20Heyang%20Lee%20and%20Ping%20Wang%20and%20Pengfei%20Zhu%20and%20Qinghua%20Hu%20and%20Dongwei%20Ren&entry.1292438233=All-in-one%20image%20restoration%20aims%20to%20handle%20diverse%20degradations%20%28e.g.%2C%20noise%2C%20blur%2C%20adverse%20weather%29%20within%20a%20unified%20framework%2C%20yet%20existing%20methods%20increasingly%20rely%20on%20complex%20architectures%20%28e.g.%2C%20Mixture-of-Experts%2C%20diffusion%20models%29%20and%20elaborate%20degradation%20prompt%20strategies.%20In%20this%20work%2C%20we%20reveal%20a%20critical%20insight%3A%20well-crafted%20feature%20extraction%20inherently%20encodes%20degradation-carrying%20information%2C%20and%20a%20symmetric%20U-Net%20architecture%20is%20sufficient%20to%20unleash%20these%20cues%20effectively.%20By%20aligning%20feature%20scales%20across%20encoder-decoder%20and%20enabling%20streamlined%20cross-scale%20propagation%2C%20our%20symmetric%20design%20preserves%20intrinsic%20degradation%20signals%20robustly%2C%20rendering%20simple%20additive%20fusion%20in%20skip%20connections%20sufficient%20for%20state-of-the-art%20performance.%20Our%20primary%20baseline%2C%20SymUNet%2C%20is%20built%20on%20this%20symmetric%20U-Net%20and%20achieves%20better%20results%20across%20benchmark%20datasets%20than%20existing%20approaches%20while%20reducing%20computational%20cost.%20We%20further%20propose%20a%20semantic%20enhanced%20variant%2C%20SE-SymUNet%2C%20which%20integrates%20direct%20semantic%20injection%20from%20frozen%20CLIP%20features%20via%20simple%20cross-attention%20to%20explicitly%20amplify%20degradation%20priors.%20Extensive%20experiments%20on%20several%20benchmarks%20validate%20the%20superiority%20of%20our%20methods.%20Both%20baselines%20SymUNet%20and%20SE-SymUNet%20establish%20simpler%20and%20stronger%20foundations%20for%20future%20advancements%20in%20all-in-one%20image%20restoration.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/WenlongJiao/SymUNet.&entry.1838667208=http%3A//arxiv.org/abs/2512.10581v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


