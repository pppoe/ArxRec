<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260201.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation", "author": "Xinya Ji and Sebastian Weiss and Manuel Kansy and Jacek Naruniec and Xun Cao and Barbara Solenthaler and Derek Bradley", "abstract": "Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose FastGHA, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.", "link": "http://arxiv.org/abs/2601.13837v2", "date": "2026-01-30", "relevancy": 3.755, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7937}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7937}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastGHA%3A%20Generalized%20Few-Shot%203D%20Gaussian%20Head%20Avatars%20with%20Real-Time%20Animation&body=Title%3A%20FastGHA%3A%20Generalized%20Few-Shot%203D%20Gaussian%20Head%20Avatars%20with%20Real-Time%20Animation%0AAuthor%3A%20Xinya%20Ji%20and%20Sebastian%20Weiss%20and%20Manuel%20Kansy%20and%20Jacek%20Naruniec%20and%20Xun%20Cao%20and%20Barbara%20Solenthaler%20and%20Derek%20Bradley%0AAbstract%3A%20Despite%20recent%20progress%20in%203D%20Gaussian-based%20head%20avatar%20modeling%2C%20efficiently%20generating%20high%20fidelity%20avatars%20remains%20a%20challenge.%20Current%20methods%20typically%20rely%20on%20extensive%20multi-view%20capture%20setups%20or%20monocular%20videos%20with%20per-identity%20optimization%20during%20inference%2C%20limiting%20their%20scalability%20and%20ease%20of%20use%20on%20unseen%20subjects.%20To%20overcome%20these%20efficiency%20drawbacks%2C%20we%20propose%20FastGHA%2C%20a%20feed-forward%20method%20to%20generate%20high-quality%20Gaussian%20head%20avatars%20from%20only%20a%20few%20input%20images%20while%20supporting%20real-time%20animation.%20Our%20approach%20directly%20learns%20a%20per-pixel%20Gaussian%20representation%20from%20the%20input%20images%2C%20and%20aggregates%20multi-view%20information%20using%20a%20transformer-based%20encoder%20that%20fuses%20image%20features%20from%20both%20DINOv3%20and%20Stable%20Diffusion%20VAE.%20For%20real-time%20animation%2C%20we%20extend%20the%20explicit%20Gaussian%20representations%20with%20per-Gaussian%20features%20and%20introduce%20a%20lightweight%20MLP-based%20dynamic%20network%20to%20predict%203D%20Gaussian%20deformations%20from%20expression%20codes.%20Furthermore%2C%20to%20enhance%20geometric%20smoothness%20of%20the%203D%20head%2C%20we%20employ%20point%20maps%20from%20a%20pre-trained%20large%20reconstruction%20model%20as%20geometry%20supervision.%20Experiments%20show%20that%20our%20approach%20significantly%20outperforms%20existing%20methods%20in%20both%20rendering%20quality%20and%20inference%20efficiency%2C%20while%20supporting%20real-time%20dynamic%20avatar%20animation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13837v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastGHA%253A%2520Generalized%2520Few-Shot%25203D%2520Gaussian%2520Head%2520Avatars%2520with%2520Real-Time%2520Animation%26entry.906535625%3DXinya%2520Ji%2520and%2520Sebastian%2520Weiss%2520and%2520Manuel%2520Kansy%2520and%2520Jacek%2520Naruniec%2520and%2520Xun%2520Cao%2520and%2520Barbara%2520Solenthaler%2520and%2520Derek%2520Bradley%26entry.1292438233%3DDespite%2520recent%2520progress%2520in%25203D%2520Gaussian-based%2520head%2520avatar%2520modeling%252C%2520efficiently%2520generating%2520high%2520fidelity%2520avatars%2520remains%2520a%2520challenge.%2520Current%2520methods%2520typically%2520rely%2520on%2520extensive%2520multi-view%2520capture%2520setups%2520or%2520monocular%2520videos%2520with%2520per-identity%2520optimization%2520during%2520inference%252C%2520limiting%2520their%2520scalability%2520and%2520ease%2520of%2520use%2520on%2520unseen%2520subjects.%2520To%2520overcome%2520these%2520efficiency%2520drawbacks%252C%2520we%2520propose%2520FastGHA%252C%2520a%2520feed-forward%2520method%2520to%2520generate%2520high-quality%2520Gaussian%2520head%2520avatars%2520from%2520only%2520a%2520few%2520input%2520images%2520while%2520supporting%2520real-time%2520animation.%2520Our%2520approach%2520directly%2520learns%2520a%2520per-pixel%2520Gaussian%2520representation%2520from%2520the%2520input%2520images%252C%2520and%2520aggregates%2520multi-view%2520information%2520using%2520a%2520transformer-based%2520encoder%2520that%2520fuses%2520image%2520features%2520from%2520both%2520DINOv3%2520and%2520Stable%2520Diffusion%2520VAE.%2520For%2520real-time%2520animation%252C%2520we%2520extend%2520the%2520explicit%2520Gaussian%2520representations%2520with%2520per-Gaussian%2520features%2520and%2520introduce%2520a%2520lightweight%2520MLP-based%2520dynamic%2520network%2520to%2520predict%25203D%2520Gaussian%2520deformations%2520from%2520expression%2520codes.%2520Furthermore%252C%2520to%2520enhance%2520geometric%2520smoothness%2520of%2520the%25203D%2520head%252C%2520we%2520employ%2520point%2520maps%2520from%2520a%2520pre-trained%2520large%2520reconstruction%2520model%2520as%2520geometry%2520supervision.%2520Experiments%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520both%2520rendering%2520quality%2520and%2520inference%2520efficiency%252C%2520while%2520supporting%2520real-time%2520dynamic%2520avatar%2520animation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13837v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastGHA%3A%20Generalized%20Few-Shot%203D%20Gaussian%20Head%20Avatars%20with%20Real-Time%20Animation&entry.906535625=Xinya%20Ji%20and%20Sebastian%20Weiss%20and%20Manuel%20Kansy%20and%20Jacek%20Naruniec%20and%20Xun%20Cao%20and%20Barbara%20Solenthaler%20and%20Derek%20Bradley&entry.1292438233=Despite%20recent%20progress%20in%203D%20Gaussian-based%20head%20avatar%20modeling%2C%20efficiently%20generating%20high%20fidelity%20avatars%20remains%20a%20challenge.%20Current%20methods%20typically%20rely%20on%20extensive%20multi-view%20capture%20setups%20or%20monocular%20videos%20with%20per-identity%20optimization%20during%20inference%2C%20limiting%20their%20scalability%20and%20ease%20of%20use%20on%20unseen%20subjects.%20To%20overcome%20these%20efficiency%20drawbacks%2C%20we%20propose%20FastGHA%2C%20a%20feed-forward%20method%20to%20generate%20high-quality%20Gaussian%20head%20avatars%20from%20only%20a%20few%20input%20images%20while%20supporting%20real-time%20animation.%20Our%20approach%20directly%20learns%20a%20per-pixel%20Gaussian%20representation%20from%20the%20input%20images%2C%20and%20aggregates%20multi-view%20information%20using%20a%20transformer-based%20encoder%20that%20fuses%20image%20features%20from%20both%20DINOv3%20and%20Stable%20Diffusion%20VAE.%20For%20real-time%20animation%2C%20we%20extend%20the%20explicit%20Gaussian%20representations%20with%20per-Gaussian%20features%20and%20introduce%20a%20lightweight%20MLP-based%20dynamic%20network%20to%20predict%203D%20Gaussian%20deformations%20from%20expression%20codes.%20Furthermore%2C%20to%20enhance%20geometric%20smoothness%20of%20the%203D%20head%2C%20we%20employ%20point%20maps%20from%20a%20pre-trained%20large%20reconstruction%20model%20as%20geometry%20supervision.%20Experiments%20show%20that%20our%20approach%20significantly%20outperforms%20existing%20methods%20in%20both%20rendering%20quality%20and%20inference%20efficiency%2C%20while%20supporting%20real-time%20dynamic%20avatar%20animation.&entry.1838667208=http%3A//arxiv.org/abs/2601.13837v2&entry.124074799=Read"},
{"title": "VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation", "author": "Hongyang Du and Junjie Ye and Xiaoyan Cong and Runhao Li and Jingcheng Ni and Aman Agarwal and Zeqi Zhou and Zekun Li and Randall Balestriero and Yue Wang", "abstract": "While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.", "link": "http://arxiv.org/abs/2601.23286v1", "date": "2026-01-30", "relevancy": 3.0614, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6588}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5895}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoGPA%3A%20Distilling%20Geometry%20Priors%20for%203D-Consistent%20Video%20Generation&body=Title%3A%20VideoGPA%3A%20Distilling%20Geometry%20Priors%20for%203D-Consistent%20Video%20Generation%0AAuthor%3A%20Hongyang%20Du%20and%20Junjie%20Ye%20and%20Xiaoyan%20Cong%20and%20Runhao%20Li%20and%20Jingcheng%20Ni%20and%20Aman%20Agarwal%20and%20Zeqi%20Zhou%20and%20Zekun%20Li%20and%20Randall%20Balestriero%20and%20Yue%20Wang%0AAbstract%3A%20While%20recent%20video%20diffusion%20models%20%28VDMs%29%20produce%20visually%20impressive%20results%2C%20they%20fundamentally%20struggle%20to%20maintain%203D%20structural%20consistency%2C%20often%20resulting%20in%20object%20deformation%20or%20spatial%20drift.%20We%20hypothesize%20that%20these%20failures%20arise%20because%20standard%20denoising%20objectives%20lack%20explicit%20incentives%20for%20geometric%20coherence.%20To%20address%20this%2C%20we%20introduce%20VideoGPA%20%28Video%20Geometric%20Preference%20Alignment%29%2C%20a%20data-efficient%20self-supervised%20framework%20that%20leverages%20a%20geometry%20foundation%20model%20to%20automatically%20derive%20dense%20preference%20signals%20that%20guide%20VDMs%20via%20Direct%20Preference%20Optimization%20%28DPO%29.%20This%20approach%20effectively%20steers%20the%20generative%20distribution%20toward%20inherent%203D%20consistency%20without%20requiring%20human%20annotations.%20VideoGPA%20significantly%20enhances%20temporal%20stability%2C%20physical%20plausibility%2C%20and%20motion%20coherence%20using%20minimal%20preference%20pairs%2C%20consistently%20outperforming%20state-of-the-art%20baselines%20in%20extensive%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoGPA%253A%2520Distilling%2520Geometry%2520Priors%2520for%25203D-Consistent%2520Video%2520Generation%26entry.906535625%3DHongyang%2520Du%2520and%2520Junjie%2520Ye%2520and%2520Xiaoyan%2520Cong%2520and%2520Runhao%2520Li%2520and%2520Jingcheng%2520Ni%2520and%2520Aman%2520Agarwal%2520and%2520Zeqi%2520Zhou%2520and%2520Zekun%2520Li%2520and%2520Randall%2520Balestriero%2520and%2520Yue%2520Wang%26entry.1292438233%3DWhile%2520recent%2520video%2520diffusion%2520models%2520%2528VDMs%2529%2520produce%2520visually%2520impressive%2520results%252C%2520they%2520fundamentally%2520struggle%2520to%2520maintain%25203D%2520structural%2520consistency%252C%2520often%2520resulting%2520in%2520object%2520deformation%2520or%2520spatial%2520drift.%2520We%2520hypothesize%2520that%2520these%2520failures%2520arise%2520because%2520standard%2520denoising%2520objectives%2520lack%2520explicit%2520incentives%2520for%2520geometric%2520coherence.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VideoGPA%2520%2528Video%2520Geometric%2520Preference%2520Alignment%2529%252C%2520a%2520data-efficient%2520self-supervised%2520framework%2520that%2520leverages%2520a%2520geometry%2520foundation%2520model%2520to%2520automatically%2520derive%2520dense%2520preference%2520signals%2520that%2520guide%2520VDMs%2520via%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529.%2520This%2520approach%2520effectively%2520steers%2520the%2520generative%2520distribution%2520toward%2520inherent%25203D%2520consistency%2520without%2520requiring%2520human%2520annotations.%2520VideoGPA%2520significantly%2520enhances%2520temporal%2520stability%252C%2520physical%2520plausibility%252C%2520and%2520motion%2520coherence%2520using%2520minimal%2520preference%2520pairs%252C%2520consistently%2520outperforming%2520state-of-the-art%2520baselines%2520in%2520extensive%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGPA%3A%20Distilling%20Geometry%20Priors%20for%203D-Consistent%20Video%20Generation&entry.906535625=Hongyang%20Du%20and%20Junjie%20Ye%20and%20Xiaoyan%20Cong%20and%20Runhao%20Li%20and%20Jingcheng%20Ni%20and%20Aman%20Agarwal%20and%20Zeqi%20Zhou%20and%20Zekun%20Li%20and%20Randall%20Balestriero%20and%20Yue%20Wang&entry.1292438233=While%20recent%20video%20diffusion%20models%20%28VDMs%29%20produce%20visually%20impressive%20results%2C%20they%20fundamentally%20struggle%20to%20maintain%203D%20structural%20consistency%2C%20often%20resulting%20in%20object%20deformation%20or%20spatial%20drift.%20We%20hypothesize%20that%20these%20failures%20arise%20because%20standard%20denoising%20objectives%20lack%20explicit%20incentives%20for%20geometric%20coherence.%20To%20address%20this%2C%20we%20introduce%20VideoGPA%20%28Video%20Geometric%20Preference%20Alignment%29%2C%20a%20data-efficient%20self-supervised%20framework%20that%20leverages%20a%20geometry%20foundation%20model%20to%20automatically%20derive%20dense%20preference%20signals%20that%20guide%20VDMs%20via%20Direct%20Preference%20Optimization%20%28DPO%29.%20This%20approach%20effectively%20steers%20the%20generative%20distribution%20toward%20inherent%203D%20consistency%20without%20requiring%20human%20annotations.%20VideoGPA%20significantly%20enhances%20temporal%20stability%2C%20physical%20plausibility%2C%20and%20motion%20coherence%20using%20minimal%20preference%20pairs%2C%20consistently%20outperforming%20state-of-the-art%20baselines%20in%20extensive%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2601.23286v1&entry.124074799=Read"},
{"title": "RoboArmGS: High-Quality Robotic Arm Splatting via B\u00e9zier Curve Refinement", "author": "Hao Wang and Xiaobao Wei and Ying Li and Qingpo Wuwu and Dongli Wu and Jiajun Cao and Ming Lu and Wenzhao Zheng and Shanghang Zhang", "abstract": "Constructing photorealistic and controllable robotic arm digital assets from real observations is fundamental to robotic applications. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, the idealized URDF-rigged motion cannot accurately model the actual motion captured in real-world observations, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable B\u00e9zier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable B\u00e9zier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.", "link": "http://arxiv.org/abs/2511.17961v2", "date": "2026-01-30", "relevancy": 3.0548, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6201}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6118}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboArmGS%3A%20High-Quality%20Robotic%20Arm%20Splatting%20via%20B%C3%A9zier%20Curve%20Refinement&body=Title%3A%20RoboArmGS%3A%20High-Quality%20Robotic%20Arm%20Splatting%20via%20B%C3%A9zier%20Curve%20Refinement%0AAuthor%3A%20Hao%20Wang%20and%20Xiaobao%20Wei%20and%20Ying%20Li%20and%20Qingpo%20Wuwu%20and%20Dongli%20Wu%20and%20Jiajun%20Cao%20and%20Ming%20Lu%20and%20Wenzhao%20Zheng%20and%20Shanghang%20Zhang%0AAbstract%3A%20Constructing%20photorealistic%20and%20controllable%20robotic%20arm%20digital%20assets%20from%20real%20observations%20is%20fundamental%20to%20robotic%20applications.%20Current%20approaches%20naively%20bind%20static%203D%20Gaussians%20according%20to%20URDF%20links%2C%20forcing%20them%20to%20follow%20an%20URDF-rigged%20motion%20passively.%20However%2C%20the%20idealized%20URDF-rigged%20motion%20cannot%20accurately%20model%20the%20actual%20motion%20captured%20in%20real-world%20observations%2C%20leading%20to%20severe%20rendering%20artifacts%20in%203D%20Gaussians.%20To%20address%20these%20challenges%2C%20we%20propose%20RoboArmGS%2C%20a%20novel%20hybrid%20representation%20that%20refines%20the%20URDF-rigged%20motion%20with%20learnable%20B%C3%A9zier%20curves%2C%20enabling%20more%20accurate%20real-world%20motion%20modeling.%20To%20be%20more%20specific%2C%20we%20present%20a%20learnable%20B%C3%A9zier%20Curve%20motion%20refiner%20that%20corrects%20per-joint%20residuals%20to%20address%20mismatches%20between%20real-world%20motion%20and%20URDF-rigged%20motion.%20RoboArmGS%20enables%20the%20learning%20of%20more%20accurate%20real-world%20motion%20while%20achieving%20a%20coherent%20binding%20of%203D%20Gaussians%20across%20arm%20parts.%20To%20support%20future%20research%2C%20we%20contribute%20a%20carefully%20collected%20dataset%20named%20RoboArm4D%2C%20which%20comprises%20several%20widely%20used%20robotic%20arms%20for%20evaluating%20the%20quality%20of%20building%20high-quality%20digital%20assets.%20We%20evaluate%20our%20approach%20on%20RoboArm4D%2C%20and%20RoboArmGS%20achieves%20state-of-the-art%20performance%20in%20real-world%20motion%20modeling%20and%20rendering%20quality.%20The%20code%20and%20dataset%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboArmGS%253A%2520High-Quality%2520Robotic%2520Arm%2520Splatting%2520via%2520B%25C3%25A9zier%2520Curve%2520Refinement%26entry.906535625%3DHao%2520Wang%2520and%2520Xiaobao%2520Wei%2520and%2520Ying%2520Li%2520and%2520Qingpo%2520Wuwu%2520and%2520Dongli%2520Wu%2520and%2520Jiajun%2520Cao%2520and%2520Ming%2520Lu%2520and%2520Wenzhao%2520Zheng%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3DConstructing%2520photorealistic%2520and%2520controllable%2520robotic%2520arm%2520digital%2520assets%2520from%2520real%2520observations%2520is%2520fundamental%2520to%2520robotic%2520applications.%2520Current%2520approaches%2520naively%2520bind%2520static%25203D%2520Gaussians%2520according%2520to%2520URDF%2520links%252C%2520forcing%2520them%2520to%2520follow%2520an%2520URDF-rigged%2520motion%2520passively.%2520However%252C%2520the%2520idealized%2520URDF-rigged%2520motion%2520cannot%2520accurately%2520model%2520the%2520actual%2520motion%2520captured%2520in%2520real-world%2520observations%252C%2520leading%2520to%2520severe%2520rendering%2520artifacts%2520in%25203D%2520Gaussians.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520RoboArmGS%252C%2520a%2520novel%2520hybrid%2520representation%2520that%2520refines%2520the%2520URDF-rigged%2520motion%2520with%2520learnable%2520B%25C3%25A9zier%2520curves%252C%2520enabling%2520more%2520accurate%2520real-world%2520motion%2520modeling.%2520To%2520be%2520more%2520specific%252C%2520we%2520present%2520a%2520learnable%2520B%25C3%25A9zier%2520Curve%2520motion%2520refiner%2520that%2520corrects%2520per-joint%2520residuals%2520to%2520address%2520mismatches%2520between%2520real-world%2520motion%2520and%2520URDF-rigged%2520motion.%2520RoboArmGS%2520enables%2520the%2520learning%2520of%2520more%2520accurate%2520real-world%2520motion%2520while%2520achieving%2520a%2520coherent%2520binding%2520of%25203D%2520Gaussians%2520across%2520arm%2520parts.%2520To%2520support%2520future%2520research%252C%2520we%2520contribute%2520a%2520carefully%2520collected%2520dataset%2520named%2520RoboArm4D%252C%2520which%2520comprises%2520several%2520widely%2520used%2520robotic%2520arms%2520for%2520evaluating%2520the%2520quality%2520of%2520building%2520high-quality%2520digital%2520assets.%2520We%2520evaluate%2520our%2520approach%2520on%2520RoboArm4D%252C%2520and%2520RoboArmGS%2520achieves%2520state-of-the-art%2520performance%2520in%2520real-world%2520motion%2520modeling%2520and%2520rendering%2520quality.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboArmGS%3A%20High-Quality%20Robotic%20Arm%20Splatting%20via%20B%C3%A9zier%20Curve%20Refinement&entry.906535625=Hao%20Wang%20and%20Xiaobao%20Wei%20and%20Ying%20Li%20and%20Qingpo%20Wuwu%20and%20Dongli%20Wu%20and%20Jiajun%20Cao%20and%20Ming%20Lu%20and%20Wenzhao%20Zheng%20and%20Shanghang%20Zhang&entry.1292438233=Constructing%20photorealistic%20and%20controllable%20robotic%20arm%20digital%20assets%20from%20real%20observations%20is%20fundamental%20to%20robotic%20applications.%20Current%20approaches%20naively%20bind%20static%203D%20Gaussians%20according%20to%20URDF%20links%2C%20forcing%20them%20to%20follow%20an%20URDF-rigged%20motion%20passively.%20However%2C%20the%20idealized%20URDF-rigged%20motion%20cannot%20accurately%20model%20the%20actual%20motion%20captured%20in%20real-world%20observations%2C%20leading%20to%20severe%20rendering%20artifacts%20in%203D%20Gaussians.%20To%20address%20these%20challenges%2C%20we%20propose%20RoboArmGS%2C%20a%20novel%20hybrid%20representation%20that%20refines%20the%20URDF-rigged%20motion%20with%20learnable%20B%C3%A9zier%20curves%2C%20enabling%20more%20accurate%20real-world%20motion%20modeling.%20To%20be%20more%20specific%2C%20we%20present%20a%20learnable%20B%C3%A9zier%20Curve%20motion%20refiner%20that%20corrects%20per-joint%20residuals%20to%20address%20mismatches%20between%20real-world%20motion%20and%20URDF-rigged%20motion.%20RoboArmGS%20enables%20the%20learning%20of%20more%20accurate%20real-world%20motion%20while%20achieving%20a%20coherent%20binding%20of%203D%20Gaussians%20across%20arm%20parts.%20To%20support%20future%20research%2C%20we%20contribute%20a%20carefully%20collected%20dataset%20named%20RoboArm4D%2C%20which%20comprises%20several%20widely%20used%20robotic%20arms%20for%20evaluating%20the%20quality%20of%20building%20high-quality%20digital%20assets.%20We%20evaluate%20our%20approach%20on%20RoboArm4D%2C%20and%20RoboArmGS%20achieves%20state-of-the-art%20performance%20in%20real-world%20motion%20modeling%20and%20rendering%20quality.%20The%20code%20and%20dataset%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.17961v2&entry.124074799=Read"},
{"title": "Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning", "author": "Xiangyu Zeng and Zhiqiu Zhang and Yuhan Zhu and Xinhao Li and Zikang Wang and Changlian Ma and Qingyu Zhang and Zizheng Huang and Kun Ouyang and Tianxiang Jiang and Ziang Yan and Yi Wang and Hongjie Zhang and Yali Wang and Limin Wang", "abstract": "Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3's strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.", "link": "http://arxiv.org/abs/2601.23224v1", "date": "2026-01-30", "relevancy": 3.0347, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6271}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-o3%3A%20Native%20Interleaved%20Clue%20Seeking%20for%20Long%20Video%20Multi-Hop%20Reasoning&body=Title%3A%20Video-o3%3A%20Native%20Interleaved%20Clue%20Seeking%20for%20Long%20Video%20Multi-Hop%20Reasoning%0AAuthor%3A%20Xiangyu%20Zeng%20and%20Zhiqiu%20Zhang%20and%20Yuhan%20Zhu%20and%20Xinhao%20Li%20and%20Zikang%20Wang%20and%20Changlian%20Ma%20and%20Qingyu%20Zhang%20and%20Zizheng%20Huang%20and%20Kun%20Ouyang%20and%20Tianxiang%20Jiang%20and%20Ziang%20Yan%20and%20Yi%20Wang%20and%20Hongjie%20Zhang%20and%20Yali%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20Existing%20multimodal%20large%20language%20models%20for%20long-video%20understanding%20predominantly%20rely%20on%20uniform%20sampling%20and%20single-turn%20inference%2C%20limiting%20their%20ability%20to%20identify%20sparse%20yet%20critical%20evidence%20amid%20extensive%20redundancy.%20We%20introduce%20Video-o3%2C%20a%20novel%20framework%20that%20supports%20iterative%20discovery%20of%20salient%20visual%20clues%2C%20fine-grained%20inspection%20of%20key%20segments%2C%20and%20adaptive%20termination%20once%20sufficient%20evidence%20is%20acquired.%20Technically%2C%20we%20address%20two%20core%20challenges%20in%20interleaved%20tool%20invocation.%20First%2C%20to%20mitigate%20attention%20dispersion%20induced%20by%20the%20heterogeneity%20of%20reasoning%20and%20tool-calling%2C%20we%20propose%20Task-Decoupled%20Attention%20Masking%2C%20which%20isolates%20per-step%20concentration%20while%20preserving%20shared%20global%20context.%20Second%2C%20to%20control%20context%20length%20growth%20in%20multi-turn%20interactions%2C%20we%20introduce%20a%20Verifiable%20Trajectory-Guided%20Reward%20that%20balances%20exploration%20coverage%20with%20reasoning%20efficiency.%20To%20support%20training%20at%20scale%2C%20we%20further%20develop%20a%20data%20synthesis%20pipeline%20and%20construct%20Seeker-173K%2C%20comprising%20173K%20high-quality%20tool-interaction%20trajectories%20for%20effective%20supervised%20and%20reinforcement%20learning.%20Extensive%20experiments%20show%20that%20Video-o3%20substantially%20outperforms%20state-of-the-art%20methods%2C%20achieving%2072.1%25%20accuracy%20on%20MLVU%20and%2046.5%25%20on%20Video-Holmes.%20These%20results%20demonstrate%20Video-o3%27s%20strong%20multi-hop%20evidence-seeking%20and%20reasoning%20capabilities%2C%20and%20validate%20the%20effectiveness%20of%20native%20tool%20invocation%20in%20long-video%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-o3%253A%2520Native%2520Interleaved%2520Clue%2520Seeking%2520for%2520Long%2520Video%2520Multi-Hop%2520Reasoning%26entry.906535625%3DXiangyu%2520Zeng%2520and%2520Zhiqiu%2520Zhang%2520and%2520Yuhan%2520Zhu%2520and%2520Xinhao%2520Li%2520and%2520Zikang%2520Wang%2520and%2520Changlian%2520Ma%2520and%2520Qingyu%2520Zhang%2520and%2520Zizheng%2520Huang%2520and%2520Kun%2520Ouyang%2520and%2520Tianxiang%2520Jiang%2520and%2520Ziang%2520Yan%2520and%2520Yi%2520Wang%2520and%2520Hongjie%2520Zhang%2520and%2520Yali%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3DExisting%2520multimodal%2520large%2520language%2520models%2520for%2520long-video%2520understanding%2520predominantly%2520rely%2520on%2520uniform%2520sampling%2520and%2520single-turn%2520inference%252C%2520limiting%2520their%2520ability%2520to%2520identify%2520sparse%2520yet%2520critical%2520evidence%2520amid%2520extensive%2520redundancy.%2520We%2520introduce%2520Video-o3%252C%2520a%2520novel%2520framework%2520that%2520supports%2520iterative%2520discovery%2520of%2520salient%2520visual%2520clues%252C%2520fine-grained%2520inspection%2520of%2520key%2520segments%252C%2520and%2520adaptive%2520termination%2520once%2520sufficient%2520evidence%2520is%2520acquired.%2520Technically%252C%2520we%2520address%2520two%2520core%2520challenges%2520in%2520interleaved%2520tool%2520invocation.%2520First%252C%2520to%2520mitigate%2520attention%2520dispersion%2520induced%2520by%2520the%2520heterogeneity%2520of%2520reasoning%2520and%2520tool-calling%252C%2520we%2520propose%2520Task-Decoupled%2520Attention%2520Masking%252C%2520which%2520isolates%2520per-step%2520concentration%2520while%2520preserving%2520shared%2520global%2520context.%2520Second%252C%2520to%2520control%2520context%2520length%2520growth%2520in%2520multi-turn%2520interactions%252C%2520we%2520introduce%2520a%2520Verifiable%2520Trajectory-Guided%2520Reward%2520that%2520balances%2520exploration%2520coverage%2520with%2520reasoning%2520efficiency.%2520To%2520support%2520training%2520at%2520scale%252C%2520we%2520further%2520develop%2520a%2520data%2520synthesis%2520pipeline%2520and%2520construct%2520Seeker-173K%252C%2520comprising%2520173K%2520high-quality%2520tool-interaction%2520trajectories%2520for%2520effective%2520supervised%2520and%2520reinforcement%2520learning.%2520Extensive%2520experiments%2520show%2520that%2520Video-o3%2520substantially%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%252072.1%2525%2520accuracy%2520on%2520MLVU%2520and%252046.5%2525%2520on%2520Video-Holmes.%2520These%2520results%2520demonstrate%2520Video-o3%2527s%2520strong%2520multi-hop%2520evidence-seeking%2520and%2520reasoning%2520capabilities%252C%2520and%2520validate%2520the%2520effectiveness%2520of%2520native%2520tool%2520invocation%2520in%2520long-video%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-o3%3A%20Native%20Interleaved%20Clue%20Seeking%20for%20Long%20Video%20Multi-Hop%20Reasoning&entry.906535625=Xiangyu%20Zeng%20and%20Zhiqiu%20Zhang%20and%20Yuhan%20Zhu%20and%20Xinhao%20Li%20and%20Zikang%20Wang%20and%20Changlian%20Ma%20and%20Qingyu%20Zhang%20and%20Zizheng%20Huang%20and%20Kun%20Ouyang%20and%20Tianxiang%20Jiang%20and%20Ziang%20Yan%20and%20Yi%20Wang%20and%20Hongjie%20Zhang%20and%20Yali%20Wang%20and%20Limin%20Wang&entry.1292438233=Existing%20multimodal%20large%20language%20models%20for%20long-video%20understanding%20predominantly%20rely%20on%20uniform%20sampling%20and%20single-turn%20inference%2C%20limiting%20their%20ability%20to%20identify%20sparse%20yet%20critical%20evidence%20amid%20extensive%20redundancy.%20We%20introduce%20Video-o3%2C%20a%20novel%20framework%20that%20supports%20iterative%20discovery%20of%20salient%20visual%20clues%2C%20fine-grained%20inspection%20of%20key%20segments%2C%20and%20adaptive%20termination%20once%20sufficient%20evidence%20is%20acquired.%20Technically%2C%20we%20address%20two%20core%20challenges%20in%20interleaved%20tool%20invocation.%20First%2C%20to%20mitigate%20attention%20dispersion%20induced%20by%20the%20heterogeneity%20of%20reasoning%20and%20tool-calling%2C%20we%20propose%20Task-Decoupled%20Attention%20Masking%2C%20which%20isolates%20per-step%20concentration%20while%20preserving%20shared%20global%20context.%20Second%2C%20to%20control%20context%20length%20growth%20in%20multi-turn%20interactions%2C%20we%20introduce%20a%20Verifiable%20Trajectory-Guided%20Reward%20that%20balances%20exploration%20coverage%20with%20reasoning%20efficiency.%20To%20support%20training%20at%20scale%2C%20we%20further%20develop%20a%20data%20synthesis%20pipeline%20and%20construct%20Seeker-173K%2C%20comprising%20173K%20high-quality%20tool-interaction%20trajectories%20for%20effective%20supervised%20and%20reinforcement%20learning.%20Extensive%20experiments%20show%20that%20Video-o3%20substantially%20outperforms%20state-of-the-art%20methods%2C%20achieving%2072.1%25%20accuracy%20on%20MLVU%20and%2046.5%25%20on%20Video-Holmes.%20These%20results%20demonstrate%20Video-o3%27s%20strong%20multi-hop%20evidence-seeking%20and%20reasoning%20capabilities%2C%20and%20validate%20the%20effectiveness%20of%20native%20tool%20invocation%20in%20long-video%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2601.23224v1&entry.124074799=Read"},
{"title": "Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI", "author": "Yinsong Wang and Thomas Fletcher and Xinzhe Luo and Aine Travers Dineen and Rhodri Cusack and Chen Qin", "abstract": "Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.", "link": "http://arxiv.org/abs/2601.22990v1", "date": "2026-01-30", "relevancy": 2.9847, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6183}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6171}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Slice-to-Volume%20Reconstruction%20with%20Gaussian%20Representations%20for%20Fetal%20MRI&body=Title%3A%20Self-Supervised%20Slice-to-Volume%20Reconstruction%20with%20Gaussian%20Representations%20for%20Fetal%20MRI%0AAuthor%3A%20Yinsong%20Wang%20and%20Thomas%20Fletcher%20and%20Xinzhe%20Luo%20and%20Aine%20Travers%20Dineen%20and%20Rhodri%20Cusack%20and%20Chen%20Qin%0AAbstract%3A%20Reconstructing%203D%20fetal%20MR%20volumes%20from%20motion-corrupted%20stacks%20of%202D%20slices%20is%20a%20crucial%20and%20challenging%20task.%20Conventional%20slice-to-volume%20reconstruction%20%28SVR%29%20methods%20are%20time-consuming%20and%20require%20multiple%20orthogonal%20stacks%20for%20reconstruction.%20While%20learning-based%20SVR%20approaches%20have%20significantly%20reduced%20the%20time%20required%20at%20the%20inference%20stage%2C%20they%20heavily%20rely%20on%20ground%20truth%20information%20for%20training%2C%20which%20is%20inaccessible%20in%20practice.%20To%20address%20these%20challenges%2C%20we%20propose%20GaussianSVR%2C%20a%20self-supervised%20framework%20for%20slice-to-volume%20reconstruction.%20GaussianSVR%20represents%20the%20target%20volume%20using%203D%20Gaussian%20representations%20to%20achieve%20high-fidelity%20reconstruction.%20It%20leverages%20a%20simulated%20forward%20slice%20acquisition%20model%20to%20enable%20self-supervised%20training%2C%20alleviating%20the%20need%20for%20ground-truth%20volumes.%20Furthermore%2C%20to%20enhance%20both%20accuracy%20and%20efficiency%2C%20we%20introduce%20a%20multi-resolution%20training%20strategy%20that%20jointly%20optimizes%20Gaussian%20parameters%20and%20spatial%20transformations%20across%20different%20resolution%20levels.%20Experiments%20show%20that%20GaussianSVR%20outperforms%20the%20baseline%20methods%20on%20fetal%20MR%20volumetric%20reconstruction.%20Code%20will%20be%20available%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Slice-to-Volume%2520Reconstruction%2520with%2520Gaussian%2520Representations%2520for%2520Fetal%2520MRI%26entry.906535625%3DYinsong%2520Wang%2520and%2520Thomas%2520Fletcher%2520and%2520Xinzhe%2520Luo%2520and%2520Aine%2520Travers%2520Dineen%2520and%2520Rhodri%2520Cusack%2520and%2520Chen%2520Qin%26entry.1292438233%3DReconstructing%25203D%2520fetal%2520MR%2520volumes%2520from%2520motion-corrupted%2520stacks%2520of%25202D%2520slices%2520is%2520a%2520crucial%2520and%2520challenging%2520task.%2520Conventional%2520slice-to-volume%2520reconstruction%2520%2528SVR%2529%2520methods%2520are%2520time-consuming%2520and%2520require%2520multiple%2520orthogonal%2520stacks%2520for%2520reconstruction.%2520While%2520learning-based%2520SVR%2520approaches%2520have%2520significantly%2520reduced%2520the%2520time%2520required%2520at%2520the%2520inference%2520stage%252C%2520they%2520heavily%2520rely%2520on%2520ground%2520truth%2520information%2520for%2520training%252C%2520which%2520is%2520inaccessible%2520in%2520practice.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GaussianSVR%252C%2520a%2520self-supervised%2520framework%2520for%2520slice-to-volume%2520reconstruction.%2520GaussianSVR%2520represents%2520the%2520target%2520volume%2520using%25203D%2520Gaussian%2520representations%2520to%2520achieve%2520high-fidelity%2520reconstruction.%2520It%2520leverages%2520a%2520simulated%2520forward%2520slice%2520acquisition%2520model%2520to%2520enable%2520self-supervised%2520training%252C%2520alleviating%2520the%2520need%2520for%2520ground-truth%2520volumes.%2520Furthermore%252C%2520to%2520enhance%2520both%2520accuracy%2520and%2520efficiency%252C%2520we%2520introduce%2520a%2520multi-resolution%2520training%2520strategy%2520that%2520jointly%2520optimizes%2520Gaussian%2520parameters%2520and%2520spatial%2520transformations%2520across%2520different%2520resolution%2520levels.%2520Experiments%2520show%2520that%2520GaussianSVR%2520outperforms%2520the%2520baseline%2520methods%2520on%2520fetal%2520MR%2520volumetric%2520reconstruction.%2520Code%2520will%2520be%2520available%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Slice-to-Volume%20Reconstruction%20with%20Gaussian%20Representations%20for%20Fetal%20MRI&entry.906535625=Yinsong%20Wang%20and%20Thomas%20Fletcher%20and%20Xinzhe%20Luo%20and%20Aine%20Travers%20Dineen%20and%20Rhodri%20Cusack%20and%20Chen%20Qin&entry.1292438233=Reconstructing%203D%20fetal%20MR%20volumes%20from%20motion-corrupted%20stacks%20of%202D%20slices%20is%20a%20crucial%20and%20challenging%20task.%20Conventional%20slice-to-volume%20reconstruction%20%28SVR%29%20methods%20are%20time-consuming%20and%20require%20multiple%20orthogonal%20stacks%20for%20reconstruction.%20While%20learning-based%20SVR%20approaches%20have%20significantly%20reduced%20the%20time%20required%20at%20the%20inference%20stage%2C%20they%20heavily%20rely%20on%20ground%20truth%20information%20for%20training%2C%20which%20is%20inaccessible%20in%20practice.%20To%20address%20these%20challenges%2C%20we%20propose%20GaussianSVR%2C%20a%20self-supervised%20framework%20for%20slice-to-volume%20reconstruction.%20GaussianSVR%20represents%20the%20target%20volume%20using%203D%20Gaussian%20representations%20to%20achieve%20high-fidelity%20reconstruction.%20It%20leverages%20a%20simulated%20forward%20slice%20acquisition%20model%20to%20enable%20self-supervised%20training%2C%20alleviating%20the%20need%20for%20ground-truth%20volumes.%20Furthermore%2C%20to%20enhance%20both%20accuracy%20and%20efficiency%2C%20we%20introduce%20a%20multi-resolution%20training%20strategy%20that%20jointly%20optimizes%20Gaussian%20parameters%20and%20spatial%20transformations%20across%20different%20resolution%20levels.%20Experiments%20show%20that%20GaussianSVR%20outperforms%20the%20baseline%20methods%20on%20fetal%20MR%20volumetric%20reconstruction.%20Code%20will%20be%20available%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2601.22990v1&entry.124074799=Read"},
{"title": "EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing", "author": "Xijie Yang and Mulin Yu and Changjian Jiang and Kerui Ren and Tao Lu and Jiangmiao Pang and Dahua Lin and Bo Dai and Linning Xu", "abstract": "Recent reconstruction methods based on radiance field such as NeRF and 3DGS reproduce indoor scenes with high visual fidelity, but break down under scene editing due to baked illumination and the lack of explicit light transport. In contrast, physically based inverse rendering relies on mesh representations and path tracing, which enforce correct light transport but place strong requirements on geometric fidelity, becoming a practical bottleneck for real indoor scenes. In this work, we propose Emission-Aware Gaussians and Path Tracing (EAG-PT), aiming for physically based light transport with a unified 2D Gaussian representation. Our design is based on three cores: (1) using 2D Gaussians as a unified scene representation and transport-friendly geometry proxy that avoids reconstructed mesh, (2) explicitly separating emissive and non-emissive components during reconstruction for further scene editing, and (3) decoupling reconstruction from final rendering by using efficient single-bounce optimization and high-quality multi-bounce path tracing after scene editing. Experiments on synthetic and real indoor scenes show that EAG-PT produces more natural and physically consistent renders after editing than radiant scene reconstructions, while preserving finer geometric detail and avoiding mesh-induced artifacts compared to mesh-based inverse path tracing. These results suggest promising directions for future use in interior design, XR content creation, and embodied AI.", "link": "http://arxiv.org/abs/2601.23065v1", "date": "2026-01-30", "relevancy": 2.9779, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6281}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.597}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EAG-PT%3A%20Emission-Aware%20Gaussians%20and%20Path%20Tracing%20for%20Indoor%20Scene%20Reconstruction%20and%20Editing&body=Title%3A%20EAG-PT%3A%20Emission-Aware%20Gaussians%20and%20Path%20Tracing%20for%20Indoor%20Scene%20Reconstruction%20and%20Editing%0AAuthor%3A%20Xijie%20Yang%20and%20Mulin%20Yu%20and%20Changjian%20Jiang%20and%20Kerui%20Ren%20and%20Tao%20Lu%20and%20Jiangmiao%20Pang%20and%20Dahua%20Lin%20and%20Bo%20Dai%20and%20Linning%20Xu%0AAbstract%3A%20Recent%20reconstruction%20methods%20based%20on%20radiance%20field%20such%20as%20NeRF%20and%203DGS%20reproduce%20indoor%20scenes%20with%20high%20visual%20fidelity%2C%20but%20break%20down%20under%20scene%20editing%20due%20to%20baked%20illumination%20and%20the%20lack%20of%20explicit%20light%20transport.%20In%20contrast%2C%20physically%20based%20inverse%20rendering%20relies%20on%20mesh%20representations%20and%20path%20tracing%2C%20which%20enforce%20correct%20light%20transport%20but%20place%20strong%20requirements%20on%20geometric%20fidelity%2C%20becoming%20a%20practical%20bottleneck%20for%20real%20indoor%20scenes.%20In%20this%20work%2C%20we%20propose%20Emission-Aware%20Gaussians%20and%20Path%20Tracing%20%28EAG-PT%29%2C%20aiming%20for%20physically%20based%20light%20transport%20with%20a%20unified%202D%20Gaussian%20representation.%20Our%20design%20is%20based%20on%20three%20cores%3A%20%281%29%20using%202D%20Gaussians%20as%20a%20unified%20scene%20representation%20and%20transport-friendly%20geometry%20proxy%20that%20avoids%20reconstructed%20mesh%2C%20%282%29%20explicitly%20separating%20emissive%20and%20non-emissive%20components%20during%20reconstruction%20for%20further%20scene%20editing%2C%20and%20%283%29%20decoupling%20reconstruction%20from%20final%20rendering%20by%20using%20efficient%20single-bounce%20optimization%20and%20high-quality%20multi-bounce%20path%20tracing%20after%20scene%20editing.%20Experiments%20on%20synthetic%20and%20real%20indoor%20scenes%20show%20that%20EAG-PT%20produces%20more%20natural%20and%20physically%20consistent%20renders%20after%20editing%20than%20radiant%20scene%20reconstructions%2C%20while%20preserving%20finer%20geometric%20detail%20and%20avoiding%20mesh-induced%20artifacts%20compared%20to%20mesh-based%20inverse%20path%20tracing.%20These%20results%20suggest%20promising%20directions%20for%20future%20use%20in%20interior%20design%2C%20XR%20content%20creation%2C%20and%20embodied%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEAG-PT%253A%2520Emission-Aware%2520Gaussians%2520and%2520Path%2520Tracing%2520for%2520Indoor%2520Scene%2520Reconstruction%2520and%2520Editing%26entry.906535625%3DXijie%2520Yang%2520and%2520Mulin%2520Yu%2520and%2520Changjian%2520Jiang%2520and%2520Kerui%2520Ren%2520and%2520Tao%2520Lu%2520and%2520Jiangmiao%2520Pang%2520and%2520Dahua%2520Lin%2520and%2520Bo%2520Dai%2520and%2520Linning%2520Xu%26entry.1292438233%3DRecent%2520reconstruction%2520methods%2520based%2520on%2520radiance%2520field%2520such%2520as%2520NeRF%2520and%25203DGS%2520reproduce%2520indoor%2520scenes%2520with%2520high%2520visual%2520fidelity%252C%2520but%2520break%2520down%2520under%2520scene%2520editing%2520due%2520to%2520baked%2520illumination%2520and%2520the%2520lack%2520of%2520explicit%2520light%2520transport.%2520In%2520contrast%252C%2520physically%2520based%2520inverse%2520rendering%2520relies%2520on%2520mesh%2520representations%2520and%2520path%2520tracing%252C%2520which%2520enforce%2520correct%2520light%2520transport%2520but%2520place%2520strong%2520requirements%2520on%2520geometric%2520fidelity%252C%2520becoming%2520a%2520practical%2520bottleneck%2520for%2520real%2520indoor%2520scenes.%2520In%2520this%2520work%252C%2520we%2520propose%2520Emission-Aware%2520Gaussians%2520and%2520Path%2520Tracing%2520%2528EAG-PT%2529%252C%2520aiming%2520for%2520physically%2520based%2520light%2520transport%2520with%2520a%2520unified%25202D%2520Gaussian%2520representation.%2520Our%2520design%2520is%2520based%2520on%2520three%2520cores%253A%2520%25281%2529%2520using%25202D%2520Gaussians%2520as%2520a%2520unified%2520scene%2520representation%2520and%2520transport-friendly%2520geometry%2520proxy%2520that%2520avoids%2520reconstructed%2520mesh%252C%2520%25282%2529%2520explicitly%2520separating%2520emissive%2520and%2520non-emissive%2520components%2520during%2520reconstruction%2520for%2520further%2520scene%2520editing%252C%2520and%2520%25283%2529%2520decoupling%2520reconstruction%2520from%2520final%2520rendering%2520by%2520using%2520efficient%2520single-bounce%2520optimization%2520and%2520high-quality%2520multi-bounce%2520path%2520tracing%2520after%2520scene%2520editing.%2520Experiments%2520on%2520synthetic%2520and%2520real%2520indoor%2520scenes%2520show%2520that%2520EAG-PT%2520produces%2520more%2520natural%2520and%2520physically%2520consistent%2520renders%2520after%2520editing%2520than%2520radiant%2520scene%2520reconstructions%252C%2520while%2520preserving%2520finer%2520geometric%2520detail%2520and%2520avoiding%2520mesh-induced%2520artifacts%2520compared%2520to%2520mesh-based%2520inverse%2520path%2520tracing.%2520These%2520results%2520suggest%2520promising%2520directions%2520for%2520future%2520use%2520in%2520interior%2520design%252C%2520XR%2520content%2520creation%252C%2520and%2520embodied%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EAG-PT%3A%20Emission-Aware%20Gaussians%20and%20Path%20Tracing%20for%20Indoor%20Scene%20Reconstruction%20and%20Editing&entry.906535625=Xijie%20Yang%20and%20Mulin%20Yu%20and%20Changjian%20Jiang%20and%20Kerui%20Ren%20and%20Tao%20Lu%20and%20Jiangmiao%20Pang%20and%20Dahua%20Lin%20and%20Bo%20Dai%20and%20Linning%20Xu&entry.1292438233=Recent%20reconstruction%20methods%20based%20on%20radiance%20field%20such%20as%20NeRF%20and%203DGS%20reproduce%20indoor%20scenes%20with%20high%20visual%20fidelity%2C%20but%20break%20down%20under%20scene%20editing%20due%20to%20baked%20illumination%20and%20the%20lack%20of%20explicit%20light%20transport.%20In%20contrast%2C%20physically%20based%20inverse%20rendering%20relies%20on%20mesh%20representations%20and%20path%20tracing%2C%20which%20enforce%20correct%20light%20transport%20but%20place%20strong%20requirements%20on%20geometric%20fidelity%2C%20becoming%20a%20practical%20bottleneck%20for%20real%20indoor%20scenes.%20In%20this%20work%2C%20we%20propose%20Emission-Aware%20Gaussians%20and%20Path%20Tracing%20%28EAG-PT%29%2C%20aiming%20for%20physically%20based%20light%20transport%20with%20a%20unified%202D%20Gaussian%20representation.%20Our%20design%20is%20based%20on%20three%20cores%3A%20%281%29%20using%202D%20Gaussians%20as%20a%20unified%20scene%20representation%20and%20transport-friendly%20geometry%20proxy%20that%20avoids%20reconstructed%20mesh%2C%20%282%29%20explicitly%20separating%20emissive%20and%20non-emissive%20components%20during%20reconstruction%20for%20further%20scene%20editing%2C%20and%20%283%29%20decoupling%20reconstruction%20from%20final%20rendering%20by%20using%20efficient%20single-bounce%20optimization%20and%20high-quality%20multi-bounce%20path%20tracing%20after%20scene%20editing.%20Experiments%20on%20synthetic%20and%20real%20indoor%20scenes%20show%20that%20EAG-PT%20produces%20more%20natural%20and%20physically%20consistent%20renders%20after%20editing%20than%20radiant%20scene%20reconstructions%2C%20while%20preserving%20finer%20geometric%20detail%20and%20avoiding%20mesh-induced%20artifacts%20compared%20to%20mesh-based%20inverse%20path%20tracing.%20These%20results%20suggest%20promising%20directions%20for%20future%20use%20in%20interior%20design%2C%20XR%20content%20creation%2C%20and%20embodied%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2601.23065v1&entry.124074799=Read"},
{"title": "Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training", "author": "Linjia Kang and Zhimin Wang and Yongkang Zhang and Duo Wu and Jinghe Wang and Ming Ma and Haopeng Yan and Zhi Wang", "abstract": "Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.", "link": "http://arxiv.org/abs/2601.22781v1", "date": "2026-01-30", "relevancy": 2.9362, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6071}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5923}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Challenges%3A%20Adaptive%20Difficulty-Aware%20Data%20Generation%20for%20Mobile%20GUI%20Agent%20Training&body=Title%3A%20Learning%20with%20Challenges%3A%20Adaptive%20Difficulty-Aware%20Data%20Generation%20for%20Mobile%20GUI%20Agent%20Training%0AAuthor%3A%20Linjia%20Kang%20and%20Zhimin%20Wang%20and%20Yongkang%20Zhang%20and%20Duo%20Wu%20and%20Jinghe%20Wang%20and%20Ming%20Ma%20and%20Haopeng%20Yan%20and%20Zhi%20Wang%0AAbstract%3A%20Large-scale%2C%20high-quality%20interaction%20trajectories%20are%20essential%20for%20advancing%20mobile%20Graphical%20User%20Interface%20%28GUI%29%20agents.%20While%20existing%20methods%20typically%20rely%20on%20labor-intensive%20human%20demonstrations%20or%20automated%20model%20exploration%20to%20generate%20GUI%20trajectories%2C%20they%20lack%20fine-grained%20control%20over%20task%20difficulty.%20This%20fundamentally%20restricts%20learning%20effectiveness%20due%20to%20the%20mismatch%20between%20the%20training%20difficulty%20and%20the%20agent%27s%20capabilities.%20Inspired%20by%20how%20humans%20acquire%20skills%20through%20progressively%20challenging%20tasks%2C%20we%20propose%20MobileGen%2C%20a%20novel%20data%20generation%20framework%20that%20adaptively%20aligns%20training%20difficulty%20with%20the%20GUI%20agent%27s%20capability%20frontier.%20Specifically%2C%20MobileGen%20explicitly%20decouples%20task%20difficulty%20into%20structural%20%28e.g.%2C%20trajectory%20length%29%20and%20semantic%20%28e.g.%2C%20task%20goal%29%20dimensions.%20It%20then%20iteratively%20evaluates%20the%20agent%20on%20a%20curated%20prior%20dataset%20to%20construct%20a%20systematic%20profile%20of%20its%20capability%20frontier%20across%20these%20two%20dimensions.%20With%20this%20profile%2C%20the%20probability%20distribution%20of%20task%20difficulty%20is%20adaptively%20computed%2C%20from%20which%20the%20target%20difficulty%20for%20the%20next%20round%20of%20training%20can%20be%20sampled.%20Guided%20by%20the%20sampled%20difficulty%2C%20a%20multi-agent%20controllable%20generator%20is%20finally%20used%20to%20synthesize%20high-quality%20interaction%20trajectories%20along%20with%20corresponding%20task%20instructions.%20Extensive%20experiments%20show%20that%20MobileGen%20consistently%20outperforms%20existing%20data%20generation%20methods%20by%20improving%20the%20average%20performance%20of%20GUI%20agents%20by%201.57%20times%20across%20multiple%20challenging%20benchmarks.%20This%20highlights%20the%20importance%20of%20capability-aligned%20data%20generation%20for%20effective%20mobile%20GUI%20agent%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520with%2520Challenges%253A%2520Adaptive%2520Difficulty-Aware%2520Data%2520Generation%2520for%2520Mobile%2520GUI%2520Agent%2520Training%26entry.906535625%3DLinjia%2520Kang%2520and%2520Zhimin%2520Wang%2520and%2520Yongkang%2520Zhang%2520and%2520Duo%2520Wu%2520and%2520Jinghe%2520Wang%2520and%2520Ming%2520Ma%2520and%2520Haopeng%2520Yan%2520and%2520Zhi%2520Wang%26entry.1292438233%3DLarge-scale%252C%2520high-quality%2520interaction%2520trajectories%2520are%2520essential%2520for%2520advancing%2520mobile%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents.%2520While%2520existing%2520methods%2520typically%2520rely%2520on%2520labor-intensive%2520human%2520demonstrations%2520or%2520automated%2520model%2520exploration%2520to%2520generate%2520GUI%2520trajectories%252C%2520they%2520lack%2520fine-grained%2520control%2520over%2520task%2520difficulty.%2520This%2520fundamentally%2520restricts%2520learning%2520effectiveness%2520due%2520to%2520the%2520mismatch%2520between%2520the%2520training%2520difficulty%2520and%2520the%2520agent%2527s%2520capabilities.%2520Inspired%2520by%2520how%2520humans%2520acquire%2520skills%2520through%2520progressively%2520challenging%2520tasks%252C%2520we%2520propose%2520MobileGen%252C%2520a%2520novel%2520data%2520generation%2520framework%2520that%2520adaptively%2520aligns%2520training%2520difficulty%2520with%2520the%2520GUI%2520agent%2527s%2520capability%2520frontier.%2520Specifically%252C%2520MobileGen%2520explicitly%2520decouples%2520task%2520difficulty%2520into%2520structural%2520%2528e.g.%252C%2520trajectory%2520length%2529%2520and%2520semantic%2520%2528e.g.%252C%2520task%2520goal%2529%2520dimensions.%2520It%2520then%2520iteratively%2520evaluates%2520the%2520agent%2520on%2520a%2520curated%2520prior%2520dataset%2520to%2520construct%2520a%2520systematic%2520profile%2520of%2520its%2520capability%2520frontier%2520across%2520these%2520two%2520dimensions.%2520With%2520this%2520profile%252C%2520the%2520probability%2520distribution%2520of%2520task%2520difficulty%2520is%2520adaptively%2520computed%252C%2520from%2520which%2520the%2520target%2520difficulty%2520for%2520the%2520next%2520round%2520of%2520training%2520can%2520be%2520sampled.%2520Guided%2520by%2520the%2520sampled%2520difficulty%252C%2520a%2520multi-agent%2520controllable%2520generator%2520is%2520finally%2520used%2520to%2520synthesize%2520high-quality%2520interaction%2520trajectories%2520along%2520with%2520corresponding%2520task%2520instructions.%2520Extensive%2520experiments%2520show%2520that%2520MobileGen%2520consistently%2520outperforms%2520existing%2520data%2520generation%2520methods%2520by%2520improving%2520the%2520average%2520performance%2520of%2520GUI%2520agents%2520by%25201.57%2520times%2520across%2520multiple%2520challenging%2520benchmarks.%2520This%2520highlights%2520the%2520importance%2520of%2520capability-aligned%2520data%2520generation%2520for%2520effective%2520mobile%2520GUI%2520agent%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Challenges%3A%20Adaptive%20Difficulty-Aware%20Data%20Generation%20for%20Mobile%20GUI%20Agent%20Training&entry.906535625=Linjia%20Kang%20and%20Zhimin%20Wang%20and%20Yongkang%20Zhang%20and%20Duo%20Wu%20and%20Jinghe%20Wang%20and%20Ming%20Ma%20and%20Haopeng%20Yan%20and%20Zhi%20Wang&entry.1292438233=Large-scale%2C%20high-quality%20interaction%20trajectories%20are%20essential%20for%20advancing%20mobile%20Graphical%20User%20Interface%20%28GUI%29%20agents.%20While%20existing%20methods%20typically%20rely%20on%20labor-intensive%20human%20demonstrations%20or%20automated%20model%20exploration%20to%20generate%20GUI%20trajectories%2C%20they%20lack%20fine-grained%20control%20over%20task%20difficulty.%20This%20fundamentally%20restricts%20learning%20effectiveness%20due%20to%20the%20mismatch%20between%20the%20training%20difficulty%20and%20the%20agent%27s%20capabilities.%20Inspired%20by%20how%20humans%20acquire%20skills%20through%20progressively%20challenging%20tasks%2C%20we%20propose%20MobileGen%2C%20a%20novel%20data%20generation%20framework%20that%20adaptively%20aligns%20training%20difficulty%20with%20the%20GUI%20agent%27s%20capability%20frontier.%20Specifically%2C%20MobileGen%20explicitly%20decouples%20task%20difficulty%20into%20structural%20%28e.g.%2C%20trajectory%20length%29%20and%20semantic%20%28e.g.%2C%20task%20goal%29%20dimensions.%20It%20then%20iteratively%20evaluates%20the%20agent%20on%20a%20curated%20prior%20dataset%20to%20construct%20a%20systematic%20profile%20of%20its%20capability%20frontier%20across%20these%20two%20dimensions.%20With%20this%20profile%2C%20the%20probability%20distribution%20of%20task%20difficulty%20is%20adaptively%20computed%2C%20from%20which%20the%20target%20difficulty%20for%20the%20next%20round%20of%20training%20can%20be%20sampled.%20Guided%20by%20the%20sampled%20difficulty%2C%20a%20multi-agent%20controllable%20generator%20is%20finally%20used%20to%20synthesize%20high-quality%20interaction%20trajectories%20along%20with%20corresponding%20task%20instructions.%20Extensive%20experiments%20show%20that%20MobileGen%20consistently%20outperforms%20existing%20data%20generation%20methods%20by%20improving%20the%20average%20performance%20of%20GUI%20agents%20by%201.57%20times%20across%20multiple%20challenging%20benchmarks.%20This%20highlights%20the%20importance%20of%20capability-aligned%20data%20generation%20for%20effective%20mobile%20GUI%20agent%20training.&entry.1838667208=http%3A//arxiv.org/abs/2601.22781v1&entry.124074799=Read"},
{"title": "Alignment among Language, Vision and Action Representations", "author": "Nicola Milano and Stefano Nolfi", "abstract": "A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.", "link": "http://arxiv.org/abs/2601.22948v1", "date": "2026-01-30", "relevancy": 2.9228, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment%20among%20Language%2C%20Vision%20and%20Action%20Representations&body=Title%3A%20Alignment%20among%20Language%2C%20Vision%20and%20Action%20Representations%0AAuthor%3A%20Nicola%20Milano%20and%20Stefano%20Nolfi%0AAbstract%3A%20A%20fundamental%20question%20in%20cognitive%20science%20and%20AI%20concerns%20whether%20different%20learning%20modalities%3A%20language%2C%20vision%2C%20and%20action%2C%20give%20rise%20to%20distinct%20or%20shared%20internal%20representations.%20Traditional%20views%20assume%20that%20models%20trained%20on%20different%20data%20types%20develop%20specialized%2C%20non-transferable%20representations.%20However%2C%20recent%20evidence%20suggests%20unexpected%20convergence%3A%20models%20optimized%20for%20distinct%20tasks%20may%20develop%20similar%20representational%20geometries.%20We%20investigate%20whether%20this%20convergence%20extends%20to%20embodied%20action%20learning%20by%20training%20a%20transformer-based%20agent%20to%20execute%20goal-directed%20behaviors%20in%20response%20to%20natural%20language%20instructions.%20Using%20behavioral%20cloning%20on%20the%20BabyAI%20platform%2C%20we%20generated%20action-grounded%20language%20embeddings%20shaped%20exclusively%20by%20sensorimotor%20control%20requirements.%20We%20then%20compared%20these%20representations%20with%20those%20extracted%20from%20state-of-the-art%20large%20language%20models%20%28LLaMA%2C%20Qwen%2C%20DeepSeek%2C%20BERT%29%20and%20vision-language%20models%20%28CLIP%2C%20BLIP%29.%20Despite%20substantial%20differences%20in%20training%20data%2C%20modality%2C%20and%20objectives%2C%20we%20observed%20robust%20cross-modal%20alignment.%20Action%20representations%20aligned%20strongly%20with%20decoder-only%20language%20models%20and%20BLIP%20%28precision%4015%3A%200.70-0.73%29%2C%20approaching%20the%20alignment%20observed%20among%20language%20models%20themselves.%20Alignment%20with%20CLIP%20and%20BERT%20was%20significantly%20weaker.%20These%20findings%20indicate%20that%20linguistic%2C%20visual%2C%20and%20action%20representations%20converge%20toward%20partially%20shared%20semantic%20structures%2C%20supporting%20modality-independent%20semantic%20organization%20and%20highlighting%20potential%20for%20cross-domain%20transfer%20in%20embodied%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment%2520among%2520Language%252C%2520Vision%2520and%2520Action%2520Representations%26entry.906535625%3DNicola%2520Milano%2520and%2520Stefano%2520Nolfi%26entry.1292438233%3DA%2520fundamental%2520question%2520in%2520cognitive%2520science%2520and%2520AI%2520concerns%2520whether%2520different%2520learning%2520modalities%253A%2520language%252C%2520vision%252C%2520and%2520action%252C%2520give%2520rise%2520to%2520distinct%2520or%2520shared%2520internal%2520representations.%2520Traditional%2520views%2520assume%2520that%2520models%2520trained%2520on%2520different%2520data%2520types%2520develop%2520specialized%252C%2520non-transferable%2520representations.%2520However%252C%2520recent%2520evidence%2520suggests%2520unexpected%2520convergence%253A%2520models%2520optimized%2520for%2520distinct%2520tasks%2520may%2520develop%2520similar%2520representational%2520geometries.%2520We%2520investigate%2520whether%2520this%2520convergence%2520extends%2520to%2520embodied%2520action%2520learning%2520by%2520training%2520a%2520transformer-based%2520agent%2520to%2520execute%2520goal-directed%2520behaviors%2520in%2520response%2520to%2520natural%2520language%2520instructions.%2520Using%2520behavioral%2520cloning%2520on%2520the%2520BabyAI%2520platform%252C%2520we%2520generated%2520action-grounded%2520language%2520embeddings%2520shaped%2520exclusively%2520by%2520sensorimotor%2520control%2520requirements.%2520We%2520then%2520compared%2520these%2520representations%2520with%2520those%2520extracted%2520from%2520state-of-the-art%2520large%2520language%2520models%2520%2528LLaMA%252C%2520Qwen%252C%2520DeepSeek%252C%2520BERT%2529%2520and%2520vision-language%2520models%2520%2528CLIP%252C%2520BLIP%2529.%2520Despite%2520substantial%2520differences%2520in%2520training%2520data%252C%2520modality%252C%2520and%2520objectives%252C%2520we%2520observed%2520robust%2520cross-modal%2520alignment.%2520Action%2520representations%2520aligned%2520strongly%2520with%2520decoder-only%2520language%2520models%2520and%2520BLIP%2520%2528precision%254015%253A%25200.70-0.73%2529%252C%2520approaching%2520the%2520alignment%2520observed%2520among%2520language%2520models%2520themselves.%2520Alignment%2520with%2520CLIP%2520and%2520BERT%2520was%2520significantly%2520weaker.%2520These%2520findings%2520indicate%2520that%2520linguistic%252C%2520visual%252C%2520and%2520action%2520representations%2520converge%2520toward%2520partially%2520shared%2520semantic%2520structures%252C%2520supporting%2520modality-independent%2520semantic%2520organization%2520and%2520highlighting%2520potential%2520for%2520cross-domain%2520transfer%2520in%2520embodied%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment%20among%20Language%2C%20Vision%20and%20Action%20Representations&entry.906535625=Nicola%20Milano%20and%20Stefano%20Nolfi&entry.1292438233=A%20fundamental%20question%20in%20cognitive%20science%20and%20AI%20concerns%20whether%20different%20learning%20modalities%3A%20language%2C%20vision%2C%20and%20action%2C%20give%20rise%20to%20distinct%20or%20shared%20internal%20representations.%20Traditional%20views%20assume%20that%20models%20trained%20on%20different%20data%20types%20develop%20specialized%2C%20non-transferable%20representations.%20However%2C%20recent%20evidence%20suggests%20unexpected%20convergence%3A%20models%20optimized%20for%20distinct%20tasks%20may%20develop%20similar%20representational%20geometries.%20We%20investigate%20whether%20this%20convergence%20extends%20to%20embodied%20action%20learning%20by%20training%20a%20transformer-based%20agent%20to%20execute%20goal-directed%20behaviors%20in%20response%20to%20natural%20language%20instructions.%20Using%20behavioral%20cloning%20on%20the%20BabyAI%20platform%2C%20we%20generated%20action-grounded%20language%20embeddings%20shaped%20exclusively%20by%20sensorimotor%20control%20requirements.%20We%20then%20compared%20these%20representations%20with%20those%20extracted%20from%20state-of-the-art%20large%20language%20models%20%28LLaMA%2C%20Qwen%2C%20DeepSeek%2C%20BERT%29%20and%20vision-language%20models%20%28CLIP%2C%20BLIP%29.%20Despite%20substantial%20differences%20in%20training%20data%2C%20modality%2C%20and%20objectives%2C%20we%20observed%20robust%20cross-modal%20alignment.%20Action%20representations%20aligned%20strongly%20with%20decoder-only%20language%20models%20and%20BLIP%20%28precision%4015%3A%200.70-0.73%29%2C%20approaching%20the%20alignment%20observed%20among%20language%20models%20themselves.%20Alignment%20with%20CLIP%20and%20BERT%20was%20significantly%20weaker.%20These%20findings%20indicate%20that%20linguistic%2C%20visual%2C%20and%20action%20representations%20converge%20toward%20partially%20shared%20semantic%20structures%2C%20supporting%20modality-independent%20semantic%20organization%20and%20highlighting%20potential%20for%20cross-domain%20transfer%20in%20embodied%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.22948v1&entry.124074799=Read"},
{"title": "From Street View to Visibility Network: Mapping Urban Visual Relationships with Vision-Language Models", "author": "Zicheng Fan and Kunihiko Fujiwara and Pengyuan Liu and Fan Zhang and Filip Biljecki", "abstract": "Visibility analysis is one of the fundamental analytics methods in urban planning and landscape research, traditionally conducted through computational simulations based on the Line-of-Sight (LoS) principle. However, when assessing the visibility of named urban objects such as landmarks, geometric intersection alone fails to capture the contextual and perceptual dimensions of visibility as experienced in the real world. The study challenges the traditional LoS-based approaches by introducing a new, image-based visibility analysis method. Specifically, a Vision Language Model (VLM) is applied to detect the target object within a direction-zoomed Street View Image (SVI). Successful detection represents the object's visibility at the corresponding SVI location. Further, a heterogeneous visibility graph is constructed to address the complex interaction between observers and target objects. In the first case study, the method proves its reliability in detecting the visibility of six tall landmark constructions in global cities, with an overall accuracy of 87%. Furthermore, it reveals broader contextual differences when the landmarks are perceived and experienced. In the second case, the proposed visibility graph uncovers the form and strength of connections for multiple landmarks along the River Thames in London, as well as the places where these connections occur. Notably, bridges on the River Thames account for approximately 30% of total connections. Our method complements and enhances traditional LoS-based visibility analysis, and showcases the possibility of revealing the prevalent connection of any visual objects in the urban environment. It opens up new research perspectives for urban planning, heritage conservation, and computational social science.", "link": "http://arxiv.org/abs/2505.11809v2", "date": "2026-01-30", "relevancy": 2.8565, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Street%20View%20to%20Visibility%20Network%3A%20Mapping%20Urban%20Visual%20Relationships%20with%20Vision-Language%20Models&body=Title%3A%20From%20Street%20View%20to%20Visibility%20Network%3A%20Mapping%20Urban%20Visual%20Relationships%20with%20Vision-Language%20Models%0AAuthor%3A%20Zicheng%20Fan%20and%20Kunihiko%20Fujiwara%20and%20Pengyuan%20Liu%20and%20Fan%20Zhang%20and%20Filip%20Biljecki%0AAbstract%3A%20Visibility%20analysis%20is%20one%20of%20the%20fundamental%20analytics%20methods%20in%20urban%20planning%20and%20landscape%20research%2C%20traditionally%20conducted%20through%20computational%20simulations%20based%20on%20the%20Line-of-Sight%20%28LoS%29%20principle.%20However%2C%20when%20assessing%20the%20visibility%20of%20named%20urban%20objects%20such%20as%20landmarks%2C%20geometric%20intersection%20alone%20fails%20to%20capture%20the%20contextual%20and%20perceptual%20dimensions%20of%20visibility%20as%20experienced%20in%20the%20real%20world.%20The%20study%20challenges%20the%20traditional%20LoS-based%20approaches%20by%20introducing%20a%20new%2C%20image-based%20visibility%20analysis%20method.%20Specifically%2C%20a%20Vision%20Language%20Model%20%28VLM%29%20is%20applied%20to%20detect%20the%20target%20object%20within%20a%20direction-zoomed%20Street%20View%20Image%20%28SVI%29.%20Successful%20detection%20represents%20the%20object%27s%20visibility%20at%20the%20corresponding%20SVI%20location.%20Further%2C%20a%20heterogeneous%20visibility%20graph%20is%20constructed%20to%20address%20the%20complex%20interaction%20between%20observers%20and%20target%20objects.%20In%20the%20first%20case%20study%2C%20the%20method%20proves%20its%20reliability%20in%20detecting%20the%20visibility%20of%20six%20tall%20landmark%20constructions%20in%20global%20cities%2C%20with%20an%20overall%20accuracy%20of%2087%25.%20Furthermore%2C%20it%20reveals%20broader%20contextual%20differences%20when%20the%20landmarks%20are%20perceived%20and%20experienced.%20In%20the%20second%20case%2C%20the%20proposed%20visibility%20graph%20uncovers%20the%20form%20and%20strength%20of%20connections%20for%20multiple%20landmarks%20along%20the%20River%20Thames%20in%20London%2C%20as%20well%20as%20the%20places%20where%20these%20connections%20occur.%20Notably%2C%20bridges%20on%20the%20River%20Thames%20account%20for%20approximately%2030%25%20of%20total%20connections.%20Our%20method%20complements%20and%20enhances%20traditional%20LoS-based%20visibility%20analysis%2C%20and%20showcases%20the%20possibility%20of%20revealing%20the%20prevalent%20connection%20of%20any%20visual%20objects%20in%20the%20urban%20environment.%20It%20opens%20up%20new%20research%20perspectives%20for%20urban%20planning%2C%20heritage%20conservation%2C%20and%20computational%20social%20science.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Street%2520View%2520to%2520Visibility%2520Network%253A%2520Mapping%2520Urban%2520Visual%2520Relationships%2520with%2520Vision-Language%2520Models%26entry.906535625%3DZicheng%2520Fan%2520and%2520Kunihiko%2520Fujiwara%2520and%2520Pengyuan%2520Liu%2520and%2520Fan%2520Zhang%2520and%2520Filip%2520Biljecki%26entry.1292438233%3DVisibility%2520analysis%2520is%2520one%2520of%2520the%2520fundamental%2520analytics%2520methods%2520in%2520urban%2520planning%2520and%2520landscape%2520research%252C%2520traditionally%2520conducted%2520through%2520computational%2520simulations%2520based%2520on%2520the%2520Line-of-Sight%2520%2528LoS%2529%2520principle.%2520However%252C%2520when%2520assessing%2520the%2520visibility%2520of%2520named%2520urban%2520objects%2520such%2520as%2520landmarks%252C%2520geometric%2520intersection%2520alone%2520fails%2520to%2520capture%2520the%2520contextual%2520and%2520perceptual%2520dimensions%2520of%2520visibility%2520as%2520experienced%2520in%2520the%2520real%2520world.%2520The%2520study%2520challenges%2520the%2520traditional%2520LoS-based%2520approaches%2520by%2520introducing%2520a%2520new%252C%2520image-based%2520visibility%2520analysis%2520method.%2520Specifically%252C%2520a%2520Vision%2520Language%2520Model%2520%2528VLM%2529%2520is%2520applied%2520to%2520detect%2520the%2520target%2520object%2520within%2520a%2520direction-zoomed%2520Street%2520View%2520Image%2520%2528SVI%2529.%2520Successful%2520detection%2520represents%2520the%2520object%2527s%2520visibility%2520at%2520the%2520corresponding%2520SVI%2520location.%2520Further%252C%2520a%2520heterogeneous%2520visibility%2520graph%2520is%2520constructed%2520to%2520address%2520the%2520complex%2520interaction%2520between%2520observers%2520and%2520target%2520objects.%2520In%2520the%2520first%2520case%2520study%252C%2520the%2520method%2520proves%2520its%2520reliability%2520in%2520detecting%2520the%2520visibility%2520of%2520six%2520tall%2520landmark%2520constructions%2520in%2520global%2520cities%252C%2520with%2520an%2520overall%2520accuracy%2520of%252087%2525.%2520Furthermore%252C%2520it%2520reveals%2520broader%2520contextual%2520differences%2520when%2520the%2520landmarks%2520are%2520perceived%2520and%2520experienced.%2520In%2520the%2520second%2520case%252C%2520the%2520proposed%2520visibility%2520graph%2520uncovers%2520the%2520form%2520and%2520strength%2520of%2520connections%2520for%2520multiple%2520landmarks%2520along%2520the%2520River%2520Thames%2520in%2520London%252C%2520as%2520well%2520as%2520the%2520places%2520where%2520these%2520connections%2520occur.%2520Notably%252C%2520bridges%2520on%2520the%2520River%2520Thames%2520account%2520for%2520approximately%252030%2525%2520of%2520total%2520connections.%2520Our%2520method%2520complements%2520and%2520enhances%2520traditional%2520LoS-based%2520visibility%2520analysis%252C%2520and%2520showcases%2520the%2520possibility%2520of%2520revealing%2520the%2520prevalent%2520connection%2520of%2520any%2520visual%2520objects%2520in%2520the%2520urban%2520environment.%2520It%2520opens%2520up%2520new%2520research%2520perspectives%2520for%2520urban%2520planning%252C%2520heritage%2520conservation%252C%2520and%2520computational%2520social%2520science.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Street%20View%20to%20Visibility%20Network%3A%20Mapping%20Urban%20Visual%20Relationships%20with%20Vision-Language%20Models&entry.906535625=Zicheng%20Fan%20and%20Kunihiko%20Fujiwara%20and%20Pengyuan%20Liu%20and%20Fan%20Zhang%20and%20Filip%20Biljecki&entry.1292438233=Visibility%20analysis%20is%20one%20of%20the%20fundamental%20analytics%20methods%20in%20urban%20planning%20and%20landscape%20research%2C%20traditionally%20conducted%20through%20computational%20simulations%20based%20on%20the%20Line-of-Sight%20%28LoS%29%20principle.%20However%2C%20when%20assessing%20the%20visibility%20of%20named%20urban%20objects%20such%20as%20landmarks%2C%20geometric%20intersection%20alone%20fails%20to%20capture%20the%20contextual%20and%20perceptual%20dimensions%20of%20visibility%20as%20experienced%20in%20the%20real%20world.%20The%20study%20challenges%20the%20traditional%20LoS-based%20approaches%20by%20introducing%20a%20new%2C%20image-based%20visibility%20analysis%20method.%20Specifically%2C%20a%20Vision%20Language%20Model%20%28VLM%29%20is%20applied%20to%20detect%20the%20target%20object%20within%20a%20direction-zoomed%20Street%20View%20Image%20%28SVI%29.%20Successful%20detection%20represents%20the%20object%27s%20visibility%20at%20the%20corresponding%20SVI%20location.%20Further%2C%20a%20heterogeneous%20visibility%20graph%20is%20constructed%20to%20address%20the%20complex%20interaction%20between%20observers%20and%20target%20objects.%20In%20the%20first%20case%20study%2C%20the%20method%20proves%20its%20reliability%20in%20detecting%20the%20visibility%20of%20six%20tall%20landmark%20constructions%20in%20global%20cities%2C%20with%20an%20overall%20accuracy%20of%2087%25.%20Furthermore%2C%20it%20reveals%20broader%20contextual%20differences%20when%20the%20landmarks%20are%20perceived%20and%20experienced.%20In%20the%20second%20case%2C%20the%20proposed%20visibility%20graph%20uncovers%20the%20form%20and%20strength%20of%20connections%20for%20multiple%20landmarks%20along%20the%20River%20Thames%20in%20London%2C%20as%20well%20as%20the%20places%20where%20these%20connections%20occur.%20Notably%2C%20bridges%20on%20the%20River%20Thames%20account%20for%20approximately%2030%25%20of%20total%20connections.%20Our%20method%20complements%20and%20enhances%20traditional%20LoS-based%20visibility%20analysis%2C%20and%20showcases%20the%20possibility%20of%20revealing%20the%20prevalent%20connection%20of%20any%20visual%20objects%20in%20the%20urban%20environment.%20It%20opens%20up%20new%20research%20perspectives%20for%20urban%20planning%2C%20heritage%20conservation%2C%20and%20computational%20social%20science.&entry.1838667208=http%3A//arxiv.org/abs/2505.11809v2&entry.124074799=Read"},
{"title": "FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations", "author": "Fedor Rodionov and Abdelrahman Eldesokey and Michael Birsak and John Femiani and Bernard Ghanem and Peter Wonka", "abstract": "We introduce FloorplanQA, a diagnostic benchmark for evaluating spatial reasoning in large-language models (LLMs). FloorplanQA is grounded in structured representations of indoor scenes, such as (e.g., kitchens, living rooms, bedrooms, bathrooms, and others), encoded symbolically in JSON or XML layouts. The benchmark covers core spatial tasks, including distance measurement, visibility, path finding, and object placement within constrained spaces. Our results across a variety of frontier open-source and commercial LLMs reveal that while models may succeed in shallow queries, they often fail to respect physical constraints, preserve spatial coherence, though they remain mostly robust to small spatial perturbations. FloorplanQA uncovers a blind spot in today's LLMs: inconsistent reasoning about indoor layouts. We hope this benchmark inspires new work on language models that can accurately infer and manipulate spatial and geometric properties in practical settings.", "link": "http://arxiv.org/abs/2507.07644v3", "date": "2026-01-30", "relevancy": 2.7588, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5603}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FloorplanQA%3A%20A%20Benchmark%20for%20Spatial%20Reasoning%20in%20LLMs%20using%20Structured%20Representations&body=Title%3A%20FloorplanQA%3A%20A%20Benchmark%20for%20Spatial%20Reasoning%20in%20LLMs%20using%20Structured%20Representations%0AAuthor%3A%20Fedor%20Rodionov%20and%20Abdelrahman%20Eldesokey%20and%20Michael%20Birsak%20and%20John%20Femiani%20and%20Bernard%20Ghanem%20and%20Peter%20Wonka%0AAbstract%3A%20We%20introduce%20FloorplanQA%2C%20a%20diagnostic%20benchmark%20for%20evaluating%20spatial%20reasoning%20in%20large-language%20models%20%28LLMs%29.%20FloorplanQA%20is%20grounded%20in%20structured%20representations%20of%20indoor%20scenes%2C%20such%20as%20%28e.g.%2C%20kitchens%2C%20living%20rooms%2C%20bedrooms%2C%20bathrooms%2C%20and%20others%29%2C%20encoded%20symbolically%20in%20JSON%20or%20XML%20layouts.%20The%20benchmark%20covers%20core%20spatial%20tasks%2C%20including%20distance%20measurement%2C%20visibility%2C%20path%20finding%2C%20and%20object%20placement%20within%20constrained%20spaces.%20Our%20results%20across%20a%20variety%20of%20frontier%20open-source%20and%20commercial%20LLMs%20reveal%20that%20while%20models%20may%20succeed%20in%20shallow%20queries%2C%20they%20often%20fail%20to%20respect%20physical%20constraints%2C%20preserve%20spatial%20coherence%2C%20though%20they%20remain%20mostly%20robust%20to%20small%20spatial%20perturbations.%20FloorplanQA%20uncovers%20a%20blind%20spot%20in%20today%27s%20LLMs%3A%20inconsistent%20reasoning%20about%20indoor%20layouts.%20We%20hope%20this%20benchmark%20inspires%20new%20work%20on%20language%20models%20that%20can%20accurately%20infer%20and%20manipulate%20spatial%20and%20geometric%20properties%20in%20practical%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2507.07644v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFloorplanQA%253A%2520A%2520Benchmark%2520for%2520Spatial%2520Reasoning%2520in%2520LLMs%2520using%2520Structured%2520Representations%26entry.906535625%3DFedor%2520Rodionov%2520and%2520Abdelrahman%2520Eldesokey%2520and%2520Michael%2520Birsak%2520and%2520John%2520Femiani%2520and%2520Bernard%2520Ghanem%2520and%2520Peter%2520Wonka%26entry.1292438233%3DWe%2520introduce%2520FloorplanQA%252C%2520a%2520diagnostic%2520benchmark%2520for%2520evaluating%2520spatial%2520reasoning%2520in%2520large-language%2520models%2520%2528LLMs%2529.%2520FloorplanQA%2520is%2520grounded%2520in%2520structured%2520representations%2520of%2520indoor%2520scenes%252C%2520such%2520as%2520%2528e.g.%252C%2520kitchens%252C%2520living%2520rooms%252C%2520bedrooms%252C%2520bathrooms%252C%2520and%2520others%2529%252C%2520encoded%2520symbolically%2520in%2520JSON%2520or%2520XML%2520layouts.%2520The%2520benchmark%2520covers%2520core%2520spatial%2520tasks%252C%2520including%2520distance%2520measurement%252C%2520visibility%252C%2520path%2520finding%252C%2520and%2520object%2520placement%2520within%2520constrained%2520spaces.%2520Our%2520results%2520across%2520a%2520variety%2520of%2520frontier%2520open-source%2520and%2520commercial%2520LLMs%2520reveal%2520that%2520while%2520models%2520may%2520succeed%2520in%2520shallow%2520queries%252C%2520they%2520often%2520fail%2520to%2520respect%2520physical%2520constraints%252C%2520preserve%2520spatial%2520coherence%252C%2520though%2520they%2520remain%2520mostly%2520robust%2520to%2520small%2520spatial%2520perturbations.%2520FloorplanQA%2520uncovers%2520a%2520blind%2520spot%2520in%2520today%2527s%2520LLMs%253A%2520inconsistent%2520reasoning%2520about%2520indoor%2520layouts.%2520We%2520hope%2520this%2520benchmark%2520inspires%2520new%2520work%2520on%2520language%2520models%2520that%2520can%2520accurately%2520infer%2520and%2520manipulate%2520spatial%2520and%2520geometric%2520properties%2520in%2520practical%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07644v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FloorplanQA%3A%20A%20Benchmark%20for%20Spatial%20Reasoning%20in%20LLMs%20using%20Structured%20Representations&entry.906535625=Fedor%20Rodionov%20and%20Abdelrahman%20Eldesokey%20and%20Michael%20Birsak%20and%20John%20Femiani%20and%20Bernard%20Ghanem%20and%20Peter%20Wonka&entry.1292438233=We%20introduce%20FloorplanQA%2C%20a%20diagnostic%20benchmark%20for%20evaluating%20spatial%20reasoning%20in%20large-language%20models%20%28LLMs%29.%20FloorplanQA%20is%20grounded%20in%20structured%20representations%20of%20indoor%20scenes%2C%20such%20as%20%28e.g.%2C%20kitchens%2C%20living%20rooms%2C%20bedrooms%2C%20bathrooms%2C%20and%20others%29%2C%20encoded%20symbolically%20in%20JSON%20or%20XML%20layouts.%20The%20benchmark%20covers%20core%20spatial%20tasks%2C%20including%20distance%20measurement%2C%20visibility%2C%20path%20finding%2C%20and%20object%20placement%20within%20constrained%20spaces.%20Our%20results%20across%20a%20variety%20of%20frontier%20open-source%20and%20commercial%20LLMs%20reveal%20that%20while%20models%20may%20succeed%20in%20shallow%20queries%2C%20they%20often%20fail%20to%20respect%20physical%20constraints%2C%20preserve%20spatial%20coherence%2C%20though%20they%20remain%20mostly%20robust%20to%20small%20spatial%20perturbations.%20FloorplanQA%20uncovers%20a%20blind%20spot%20in%20today%27s%20LLMs%3A%20inconsistent%20reasoning%20about%20indoor%20layouts.%20We%20hope%20this%20benchmark%20inspires%20new%20work%20on%20language%20models%20that%20can%20accurately%20infer%20and%20manipulate%20spatial%20and%20geometric%20properties%20in%20practical%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2507.07644v3&entry.124074799=Read"},
{"title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers", "author": "Bin Yu and Shijie Lian and Xiaopeng Lin and Yuliang Wei and Zhaolong Shen and Changti Wu and Yuzhuo Miao and Xinming Wang and Bailing Wang and Cong Huang and Kai Chen", "abstract": "The fundamental premise of Vision-Language-Action (VLA) models is to harness the extensive general capabilities of pre-trained Vision-Language Models (VLMs) for generalized embodied intelligence. However, standard robotic fine-tuning inevitably disrupts the pre-trained feature space, leading to \"catastrophic forgetting\" that compromises the general visual understanding we aim to leverage. To effectively utilize the uncorrupted general capabilities of VLMs for robotic tasks, we propose TwinBrainVLA, which coordinates two isomorphic VLM pathways: a frozen generalist (also called \"Left Brain\") and a trainable specialist (also called \"Right Brain\"). Our architecture utilizes a Asymmetric Mixture-of-Transformers (AsyMoT) mechanism, enabling the Right Brain to dynamically query and fuse intact semantic knowledge from the Left Brain with proprioceptive states. This fused representation conditions a flow-matching action expert for precise continuous control. Empirical results on SimplerEnv and RoboCasa benchmarks demonstrate that by explicitly retaining general capabilities, TwinBrainVLA achieves substantial performance gains over baseline models in complex manipulation tasks.", "link": "http://arxiv.org/abs/2601.14133v2", "date": "2026-01-30", "relevancy": 2.7495, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TwinBrainVLA%3A%20Unleashing%20the%20Potential%20of%20Generalist%20VLMs%20for%20Embodied%20Tasks%20via%20Asymmetric%20Mixture-of-Transformers&body=Title%3A%20TwinBrainVLA%3A%20Unleashing%20the%20Potential%20of%20Generalist%20VLMs%20for%20Embodied%20Tasks%20via%20Asymmetric%20Mixture-of-Transformers%0AAuthor%3A%20Bin%20Yu%20and%20Shijie%20Lian%20and%20Xiaopeng%20Lin%20and%20Yuliang%20Wei%20and%20Zhaolong%20Shen%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Xinming%20Wang%20and%20Bailing%20Wang%20and%20Cong%20Huang%20and%20Kai%20Chen%0AAbstract%3A%20The%20fundamental%20premise%20of%20Vision-Language-Action%20%28VLA%29%20models%20is%20to%20harness%20the%20extensive%20general%20capabilities%20of%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20for%20generalized%20embodied%20intelligence.%20However%2C%20standard%20robotic%20fine-tuning%20inevitably%20disrupts%20the%20pre-trained%20feature%20space%2C%20leading%20to%20%22catastrophic%20forgetting%22%20that%20compromises%20the%20general%20visual%20understanding%20we%20aim%20to%20leverage.%20To%20effectively%20utilize%20the%20uncorrupted%20general%20capabilities%20of%20VLMs%20for%20robotic%20tasks%2C%20we%20propose%20TwinBrainVLA%2C%20which%20coordinates%20two%20isomorphic%20VLM%20pathways%3A%20a%20frozen%20generalist%20%28also%20called%20%22Left%20Brain%22%29%20and%20a%20trainable%20specialist%20%28also%20called%20%22Right%20Brain%22%29.%20Our%20architecture%20utilizes%20a%20Asymmetric%20Mixture-of-Transformers%20%28AsyMoT%29%20mechanism%2C%20enabling%20the%20Right%20Brain%20to%20dynamically%20query%20and%20fuse%20intact%20semantic%20knowledge%20from%20the%20Left%20Brain%20with%20proprioceptive%20states.%20This%20fused%20representation%20conditions%20a%20flow-matching%20action%20expert%20for%20precise%20continuous%20control.%20Empirical%20results%20on%20SimplerEnv%20and%20RoboCasa%20benchmarks%20demonstrate%20that%20by%20explicitly%20retaining%20general%20capabilities%2C%20TwinBrainVLA%20achieves%20substantial%20performance%20gains%20over%20baseline%20models%20in%20complex%20manipulation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14133v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwinBrainVLA%253A%2520Unleashing%2520the%2520Potential%2520of%2520Generalist%2520VLMs%2520for%2520Embodied%2520Tasks%2520via%2520Asymmetric%2520Mixture-of-Transformers%26entry.906535625%3DBin%2520Yu%2520and%2520Shijie%2520Lian%2520and%2520Xiaopeng%2520Lin%2520and%2520Yuliang%2520Wei%2520and%2520Zhaolong%2520Shen%2520and%2520Changti%2520Wu%2520and%2520Yuzhuo%2520Miao%2520and%2520Xinming%2520Wang%2520and%2520Bailing%2520Wang%2520and%2520Cong%2520Huang%2520and%2520Kai%2520Chen%26entry.1292438233%3DThe%2520fundamental%2520premise%2520of%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520is%2520to%2520harness%2520the%2520extensive%2520general%2520capabilities%2520of%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520generalized%2520embodied%2520intelligence.%2520However%252C%2520standard%2520robotic%2520fine-tuning%2520inevitably%2520disrupts%2520the%2520pre-trained%2520feature%2520space%252C%2520leading%2520to%2520%2522catastrophic%2520forgetting%2522%2520that%2520compromises%2520the%2520general%2520visual%2520understanding%2520we%2520aim%2520to%2520leverage.%2520To%2520effectively%2520utilize%2520the%2520uncorrupted%2520general%2520capabilities%2520of%2520VLMs%2520for%2520robotic%2520tasks%252C%2520we%2520propose%2520TwinBrainVLA%252C%2520which%2520coordinates%2520two%2520isomorphic%2520VLM%2520pathways%253A%2520a%2520frozen%2520generalist%2520%2528also%2520called%2520%2522Left%2520Brain%2522%2529%2520and%2520a%2520trainable%2520specialist%2520%2528also%2520called%2520%2522Right%2520Brain%2522%2529.%2520Our%2520architecture%2520utilizes%2520a%2520Asymmetric%2520Mixture-of-Transformers%2520%2528AsyMoT%2529%2520mechanism%252C%2520enabling%2520the%2520Right%2520Brain%2520to%2520dynamically%2520query%2520and%2520fuse%2520intact%2520semantic%2520knowledge%2520from%2520the%2520Left%2520Brain%2520with%2520proprioceptive%2520states.%2520This%2520fused%2520representation%2520conditions%2520a%2520flow-matching%2520action%2520expert%2520for%2520precise%2520continuous%2520control.%2520Empirical%2520results%2520on%2520SimplerEnv%2520and%2520RoboCasa%2520benchmarks%2520demonstrate%2520that%2520by%2520explicitly%2520retaining%2520general%2520capabilities%252C%2520TwinBrainVLA%2520achieves%2520substantial%2520performance%2520gains%2520over%2520baseline%2520models%2520in%2520complex%2520manipulation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14133v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TwinBrainVLA%3A%20Unleashing%20the%20Potential%20of%20Generalist%20VLMs%20for%20Embodied%20Tasks%20via%20Asymmetric%20Mixture-of-Transformers&entry.906535625=Bin%20Yu%20and%20Shijie%20Lian%20and%20Xiaopeng%20Lin%20and%20Yuliang%20Wei%20and%20Zhaolong%20Shen%20and%20Changti%20Wu%20and%20Yuzhuo%20Miao%20and%20Xinming%20Wang%20and%20Bailing%20Wang%20and%20Cong%20Huang%20and%20Kai%20Chen&entry.1292438233=The%20fundamental%20premise%20of%20Vision-Language-Action%20%28VLA%29%20models%20is%20to%20harness%20the%20extensive%20general%20capabilities%20of%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20for%20generalized%20embodied%20intelligence.%20However%2C%20standard%20robotic%20fine-tuning%20inevitably%20disrupts%20the%20pre-trained%20feature%20space%2C%20leading%20to%20%22catastrophic%20forgetting%22%20that%20compromises%20the%20general%20visual%20understanding%20we%20aim%20to%20leverage.%20To%20effectively%20utilize%20the%20uncorrupted%20general%20capabilities%20of%20VLMs%20for%20robotic%20tasks%2C%20we%20propose%20TwinBrainVLA%2C%20which%20coordinates%20two%20isomorphic%20VLM%20pathways%3A%20a%20frozen%20generalist%20%28also%20called%20%22Left%20Brain%22%29%20and%20a%20trainable%20specialist%20%28also%20called%20%22Right%20Brain%22%29.%20Our%20architecture%20utilizes%20a%20Asymmetric%20Mixture-of-Transformers%20%28AsyMoT%29%20mechanism%2C%20enabling%20the%20Right%20Brain%20to%20dynamically%20query%20and%20fuse%20intact%20semantic%20knowledge%20from%20the%20Left%20Brain%20with%20proprioceptive%20states.%20This%20fused%20representation%20conditions%20a%20flow-matching%20action%20expert%20for%20precise%20continuous%20control.%20Empirical%20results%20on%20SimplerEnv%20and%20RoboCasa%20benchmarks%20demonstrate%20that%20by%20explicitly%20retaining%20general%20capabilities%2C%20TwinBrainVLA%20achieves%20substantial%20performance%20gains%20over%20baseline%20models%20in%20complex%20manipulation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.14133v2&entry.124074799=Read"},
{"title": "Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models", "author": "Anmin Wang and Nan Zhang and Wei Tao and Xiaoyang Qu and Guokuan Li and Jiguang Wan and Jianzong Wang", "abstract": "Vision-Language Models (VLMs) face significant computational challenges in video processing due to massive data redundancy, which creates prohibitively long token sequences. To address this, we introduce Triage, a training-free, plug-and-play framework that reframes video reasoning as a resource allocation problem via hierarchical visual budgeting. Its first stage, Frame-Level Budgeting, identifies keyframes by evaluating their visual dynamics and relevance, generating a strategic prior based on their importance scores. Guided by this prior, the second stage, Token-Level Budgeting, allocates tokens in two phases: it first secures high-relevance Core Tokens, followed by diverse Context Tokens selected with an efficient batched Maximal Marginal Relevance (MMR) algorithm. Extensive experiments demonstrate that Triage improves inference speed and reduces memory footprint, while maintaining or surpassing the performance of baselines and other methods on various video reasoning benchmarks.", "link": "http://arxiv.org/abs/2601.22959v1", "date": "2026-01-30", "relevancy": 2.7486, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5723}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triage%3A%20Hierarchical%20Visual%20Budgeting%20for%20Efficient%20Video%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20Triage%3A%20Hierarchical%20Visual%20Budgeting%20for%20Efficient%20Video%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Anmin%20Wang%20and%20Nan%20Zhang%20and%20Wei%20Tao%20and%20Xiaoyang%20Qu%20and%20Guokuan%20Li%20and%20Jiguang%20Wan%20and%20Jianzong%20Wang%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20face%20significant%20computational%20challenges%20in%20video%20processing%20due%20to%20massive%20data%20redundancy%2C%20which%20creates%20prohibitively%20long%20token%20sequences.%20To%20address%20this%2C%20we%20introduce%20Triage%2C%20a%20training-free%2C%20plug-and-play%20framework%20that%20reframes%20video%20reasoning%20as%20a%20resource%20allocation%20problem%20via%20hierarchical%20visual%20budgeting.%20Its%20first%20stage%2C%20Frame-Level%20Budgeting%2C%20identifies%20keyframes%20by%20evaluating%20their%20visual%20dynamics%20and%20relevance%2C%20generating%20a%20strategic%20prior%20based%20on%20their%20importance%20scores.%20Guided%20by%20this%20prior%2C%20the%20second%20stage%2C%20Token-Level%20Budgeting%2C%20allocates%20tokens%20in%20two%20phases%3A%20it%20first%20secures%20high-relevance%20Core%20Tokens%2C%20followed%20by%20diverse%20Context%20Tokens%20selected%20with%20an%20efficient%20batched%20Maximal%20Marginal%20Relevance%20%28MMR%29%20algorithm.%20Extensive%20experiments%20demonstrate%20that%20Triage%20improves%20inference%20speed%20and%20reduces%20memory%20footprint%2C%20while%20maintaining%20or%20surpassing%20the%20performance%20of%20baselines%20and%20other%20methods%20on%20various%20video%20reasoning%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriage%253A%2520Hierarchical%2520Visual%2520Budgeting%2520for%2520Efficient%2520Video%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DAnmin%2520Wang%2520and%2520Nan%2520Zhang%2520and%2520Wei%2520Tao%2520and%2520Xiaoyang%2520Qu%2520and%2520Guokuan%2520Li%2520and%2520Jiguang%2520Wan%2520and%2520Jianzong%2520Wang%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520face%2520significant%2520computational%2520challenges%2520in%2520video%2520processing%2520due%2520to%2520massive%2520data%2520redundancy%252C%2520which%2520creates%2520prohibitively%2520long%2520token%2520sequences.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Triage%252C%2520a%2520training-free%252C%2520plug-and-play%2520framework%2520that%2520reframes%2520video%2520reasoning%2520as%2520a%2520resource%2520allocation%2520problem%2520via%2520hierarchical%2520visual%2520budgeting.%2520Its%2520first%2520stage%252C%2520Frame-Level%2520Budgeting%252C%2520identifies%2520keyframes%2520by%2520evaluating%2520their%2520visual%2520dynamics%2520and%2520relevance%252C%2520generating%2520a%2520strategic%2520prior%2520based%2520on%2520their%2520importance%2520scores.%2520Guided%2520by%2520this%2520prior%252C%2520the%2520second%2520stage%252C%2520Token-Level%2520Budgeting%252C%2520allocates%2520tokens%2520in%2520two%2520phases%253A%2520it%2520first%2520secures%2520high-relevance%2520Core%2520Tokens%252C%2520followed%2520by%2520diverse%2520Context%2520Tokens%2520selected%2520with%2520an%2520efficient%2520batched%2520Maximal%2520Marginal%2520Relevance%2520%2528MMR%2529%2520algorithm.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Triage%2520improves%2520inference%2520speed%2520and%2520reduces%2520memory%2520footprint%252C%2520while%2520maintaining%2520or%2520surpassing%2520the%2520performance%2520of%2520baselines%2520and%2520other%2520methods%2520on%2520various%2520video%2520reasoning%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triage%3A%20Hierarchical%20Visual%20Budgeting%20for%20Efficient%20Video%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Anmin%20Wang%20and%20Nan%20Zhang%20and%20Wei%20Tao%20and%20Xiaoyang%20Qu%20and%20Guokuan%20Li%20and%20Jiguang%20Wan%20and%20Jianzong%20Wang&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20face%20significant%20computational%20challenges%20in%20video%20processing%20due%20to%20massive%20data%20redundancy%2C%20which%20creates%20prohibitively%20long%20token%20sequences.%20To%20address%20this%2C%20we%20introduce%20Triage%2C%20a%20training-free%2C%20plug-and-play%20framework%20that%20reframes%20video%20reasoning%20as%20a%20resource%20allocation%20problem%20via%20hierarchical%20visual%20budgeting.%20Its%20first%20stage%2C%20Frame-Level%20Budgeting%2C%20identifies%20keyframes%20by%20evaluating%20their%20visual%20dynamics%20and%20relevance%2C%20generating%20a%20strategic%20prior%20based%20on%20their%20importance%20scores.%20Guided%20by%20this%20prior%2C%20the%20second%20stage%2C%20Token-Level%20Budgeting%2C%20allocates%20tokens%20in%20two%20phases%3A%20it%20first%20secures%20high-relevance%20Core%20Tokens%2C%20followed%20by%20diverse%20Context%20Tokens%20selected%20with%20an%20efficient%20batched%20Maximal%20Marginal%20Relevance%20%28MMR%29%20algorithm.%20Extensive%20experiments%20demonstrate%20that%20Triage%20improves%20inference%20speed%20and%20reduces%20memory%20footprint%2C%20while%20maintaining%20or%20surpassing%20the%20performance%20of%20baselines%20and%20other%20methods%20on%20various%20video%20reasoning%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2601.22959v1&entry.124074799=Read"},
{"title": "Are Pose Estimators Ready for the Open World? STAGE: A GenAI Toolkit for Auditing 3D Human Pose Estimators", "author": "Nikita Kister and Istv\u00e1n S\u00e1r\u00e1ndi and Jiayi Wang and Anna Khoreva and Gerard Pons-Moll", "abstract": "For safety-critical applications, it is crucial to audit 3D human pose estimators before deployment. Will the system break down if the weather or the clothing changes? Is it robust regarding gender and age? To answer these questions and more, we need controlled studies with images that differ in a single attribute, but real benchmarks cannot provide such pairs. We thus present STAGE, a GenAI data toolkit for auditing 3D human pose estimators. For STAGE, we develop the first GenAI image creator with accurate 3D pose control and propose a novel evaluation strategy to isolate and quantify the effects of single factors such as gender, ethnicity, age, clothing, location, and weather. Enabled by STAGE, we generate a series of benchmarks to audit, for the first time, the sensitivity of popular pose estimators towards such factors. Our results show that natural variations can severely degrade pose estimator performance, raising doubts about their readiness for open-world deployment. We aim to highlight these robustness issues and establish STAGE as a benchmark to quantify them.", "link": "http://arxiv.org/abs/2408.16536v2", "date": "2026-01-30", "relevancy": 2.7457, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5694}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5393}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Pose%20Estimators%20Ready%20for%20the%20Open%20World%3F%20STAGE%3A%20A%20GenAI%20Toolkit%20for%20Auditing%203D%20Human%20Pose%20Estimators&body=Title%3A%20Are%20Pose%20Estimators%20Ready%20for%20the%20Open%20World%3F%20STAGE%3A%20A%20GenAI%20Toolkit%20for%20Auditing%203D%20Human%20Pose%20Estimators%0AAuthor%3A%20Nikita%20Kister%20and%20Istv%C3%A1n%20S%C3%A1r%C3%A1ndi%20and%20Jiayi%20Wang%20and%20Anna%20Khoreva%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20For%20safety-critical%20applications%2C%20it%20is%20crucial%20to%20audit%203D%20human%20pose%20estimators%20before%20deployment.%20Will%20the%20system%20break%20down%20if%20the%20weather%20or%20the%20clothing%20changes%3F%20Is%20it%20robust%20regarding%20gender%20and%20age%3F%20To%20answer%20these%20questions%20and%20more%2C%20we%20need%20controlled%20studies%20with%20images%20that%20differ%20in%20a%20single%20attribute%2C%20but%20real%20benchmarks%20cannot%20provide%20such%20pairs.%20We%20thus%20present%20STAGE%2C%20a%20GenAI%20data%20toolkit%20for%20auditing%203D%20human%20pose%20estimators.%20For%20STAGE%2C%20we%20develop%20the%20first%20GenAI%20image%20creator%20with%20accurate%203D%20pose%20control%20and%20propose%20a%20novel%20evaluation%20strategy%20to%20isolate%20and%20quantify%20the%20effects%20of%20single%20factors%20such%20as%20gender%2C%20ethnicity%2C%20age%2C%20clothing%2C%20location%2C%20and%20weather.%20Enabled%20by%20STAGE%2C%20we%20generate%20a%20series%20of%20benchmarks%20to%20audit%2C%20for%20the%20first%20time%2C%20the%20sensitivity%20of%20popular%20pose%20estimators%20towards%20such%20factors.%20Our%20results%20show%20that%20natural%20variations%20can%20severely%20degrade%20pose%20estimator%20performance%2C%20raising%20doubts%20about%20their%20readiness%20for%20open-world%20deployment.%20We%20aim%20to%20highlight%20these%20robustness%20issues%20and%20establish%20STAGE%20as%20a%20benchmark%20to%20quantify%20them.%0ALink%3A%20http%3A//arxiv.org/abs/2408.16536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Pose%2520Estimators%2520Ready%2520for%2520the%2520Open%2520World%253F%2520STAGE%253A%2520A%2520GenAI%2520Toolkit%2520for%2520Auditing%25203D%2520Human%2520Pose%2520Estimators%26entry.906535625%3DNikita%2520Kister%2520and%2520Istv%25C3%25A1n%2520S%25C3%25A1r%25C3%25A1ndi%2520and%2520Jiayi%2520Wang%2520and%2520Anna%2520Khoreva%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3DFor%2520safety-critical%2520applications%252C%2520it%2520is%2520crucial%2520to%2520audit%25203D%2520human%2520pose%2520estimators%2520before%2520deployment.%2520Will%2520the%2520system%2520break%2520down%2520if%2520the%2520weather%2520or%2520the%2520clothing%2520changes%253F%2520Is%2520it%2520robust%2520regarding%2520gender%2520and%2520age%253F%2520To%2520answer%2520these%2520questions%2520and%2520more%252C%2520we%2520need%2520controlled%2520studies%2520with%2520images%2520that%2520differ%2520in%2520a%2520single%2520attribute%252C%2520but%2520real%2520benchmarks%2520cannot%2520provide%2520such%2520pairs.%2520We%2520thus%2520present%2520STAGE%252C%2520a%2520GenAI%2520data%2520toolkit%2520for%2520auditing%25203D%2520human%2520pose%2520estimators.%2520For%2520STAGE%252C%2520we%2520develop%2520the%2520first%2520GenAI%2520image%2520creator%2520with%2520accurate%25203D%2520pose%2520control%2520and%2520propose%2520a%2520novel%2520evaluation%2520strategy%2520to%2520isolate%2520and%2520quantify%2520the%2520effects%2520of%2520single%2520factors%2520such%2520as%2520gender%252C%2520ethnicity%252C%2520age%252C%2520clothing%252C%2520location%252C%2520and%2520weather.%2520Enabled%2520by%2520STAGE%252C%2520we%2520generate%2520a%2520series%2520of%2520benchmarks%2520to%2520audit%252C%2520for%2520the%2520first%2520time%252C%2520the%2520sensitivity%2520of%2520popular%2520pose%2520estimators%2520towards%2520such%2520factors.%2520Our%2520results%2520show%2520that%2520natural%2520variations%2520can%2520severely%2520degrade%2520pose%2520estimator%2520performance%252C%2520raising%2520doubts%2520about%2520their%2520readiness%2520for%2520open-world%2520deployment.%2520We%2520aim%2520to%2520highlight%2520these%2520robustness%2520issues%2520and%2520establish%2520STAGE%2520as%2520a%2520benchmark%2520to%2520quantify%2520them.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Pose%20Estimators%20Ready%20for%20the%20Open%20World%3F%20STAGE%3A%20A%20GenAI%20Toolkit%20for%20Auditing%203D%20Human%20Pose%20Estimators&entry.906535625=Nikita%20Kister%20and%20Istv%C3%A1n%20S%C3%A1r%C3%A1ndi%20and%20Jiayi%20Wang%20and%20Anna%20Khoreva%20and%20Gerard%20Pons-Moll&entry.1292438233=For%20safety-critical%20applications%2C%20it%20is%20crucial%20to%20audit%203D%20human%20pose%20estimators%20before%20deployment.%20Will%20the%20system%20break%20down%20if%20the%20weather%20or%20the%20clothing%20changes%3F%20Is%20it%20robust%20regarding%20gender%20and%20age%3F%20To%20answer%20these%20questions%20and%20more%2C%20we%20need%20controlled%20studies%20with%20images%20that%20differ%20in%20a%20single%20attribute%2C%20but%20real%20benchmarks%20cannot%20provide%20such%20pairs.%20We%20thus%20present%20STAGE%2C%20a%20GenAI%20data%20toolkit%20for%20auditing%203D%20human%20pose%20estimators.%20For%20STAGE%2C%20we%20develop%20the%20first%20GenAI%20image%20creator%20with%20accurate%203D%20pose%20control%20and%20propose%20a%20novel%20evaluation%20strategy%20to%20isolate%20and%20quantify%20the%20effects%20of%20single%20factors%20such%20as%20gender%2C%20ethnicity%2C%20age%2C%20clothing%2C%20location%2C%20and%20weather.%20Enabled%20by%20STAGE%2C%20we%20generate%20a%20series%20of%20benchmarks%20to%20audit%2C%20for%20the%20first%20time%2C%20the%20sensitivity%20of%20popular%20pose%20estimators%20towards%20such%20factors.%20Our%20results%20show%20that%20natural%20variations%20can%20severely%20degrade%20pose%20estimator%20performance%2C%20raising%20doubts%20about%20their%20readiness%20for%20open-world%20deployment.%20We%20aim%20to%20highlight%20these%20robustness%20issues%20and%20establish%20STAGE%20as%20a%20benchmark%20to%20quantify%20them.&entry.1838667208=http%3A//arxiv.org/abs/2408.16536v2&entry.124074799=Read"},
{"title": "Benchmarking Foundation Models for Mitotic Figure Classification", "author": "Jonas Ammeling and Jonathan Ganz and Emely Rosbach and Ludwig Lausser and Christof A. Bertram and Katharina Breininger and Marc Aubreville", "abstract": "The performance of deep learning models is known to scale with data quantity and diversity. In pathology, as in many other medical imaging domains, the availability of labeled images for a specific task is often limited. Self-supervised learning techniques have enabled the use of vast amounts of unlabeled data to train large-scale neural networks, i.e., foundation models, that can address the limited data problem by providing semantically rich feature vectors that can generalize well to new tasks with minimal training effort increasing model performance and robustness. In this work, we investigate the use of foundation models for mitotic figure classification. The mitotic count, which can be derived from this classification task, is an independent prognostic marker for specific tumors and part of certain tumor grading systems. In particular, we investigate the data scaling laws on multiple current foundation models and evaluate their robustness to unseen tumor domains. Next to the commonly used linear probing paradigm, we also adapt the models using low-rank adaptation (LoRA) of their attention mechanisms. We compare all models against end-to-end-trained baselines, both CNNs and Vision Transformers. Our results demonstrate that LoRA-adapted foundation models provide superior performance to those adapted with standard linear probing, reaching performance levels close to 100% data availability with only 10% of training data. Furthermore, LoRA-adaptation of the most recent foundation models almost closes the out-of-domain performance gap when evaluated on unseen tumor domains. However, full fine-tuning of traditional architectures still yields competitive performance.", "link": "http://arxiv.org/abs/2508.04441v2", "date": "2026-01-30", "relevancy": 2.7429, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Foundation%20Models%20for%20Mitotic%20Figure%20Classification&body=Title%3A%20Benchmarking%20Foundation%20Models%20for%20Mitotic%20Figure%20Classification%0AAuthor%3A%20Jonas%20Ammeling%20and%20Jonathan%20Ganz%20and%20Emely%20Rosbach%20and%20Ludwig%20Lausser%20and%20Christof%20A.%20Bertram%20and%20Katharina%20Breininger%20and%20Marc%20Aubreville%0AAbstract%3A%20The%20performance%20of%20deep%20learning%20models%20is%20known%20to%20scale%20with%20data%20quantity%20and%20diversity.%20In%20pathology%2C%20as%20in%20many%20other%20medical%20imaging%20domains%2C%20the%20availability%20of%20labeled%20images%20for%20a%20specific%20task%20is%20often%20limited.%20Self-supervised%20learning%20techniques%20have%20enabled%20the%20use%20of%20vast%20amounts%20of%20unlabeled%20data%20to%20train%20large-scale%20neural%20networks%2C%20i.e.%2C%20foundation%20models%2C%20that%20can%20address%20the%20limited%20data%20problem%20by%20providing%20semantically%20rich%20feature%20vectors%20that%20can%20generalize%20well%20to%20new%20tasks%20with%20minimal%20training%20effort%20increasing%20model%20performance%20and%20robustness.%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20foundation%20models%20for%20mitotic%20figure%20classification.%20The%20mitotic%20count%2C%20which%20can%20be%20derived%20from%20this%20classification%20task%2C%20is%20an%20independent%20prognostic%20marker%20for%20specific%20tumors%20and%20part%20of%20certain%20tumor%20grading%20systems.%20In%20particular%2C%20we%20investigate%20the%20data%20scaling%20laws%20on%20multiple%20current%20foundation%20models%20and%20evaluate%20their%20robustness%20to%20unseen%20tumor%20domains.%20Next%20to%20the%20commonly%20used%20linear%20probing%20paradigm%2C%20we%20also%20adapt%20the%20models%20using%20low-rank%20adaptation%20%28LoRA%29%20of%20their%20attention%20mechanisms.%20We%20compare%20all%20models%20against%20end-to-end-trained%20baselines%2C%20both%20CNNs%20and%20Vision%20Transformers.%20Our%20results%20demonstrate%20that%20LoRA-adapted%20foundation%20models%20provide%20superior%20performance%20to%20those%20adapted%20with%20standard%20linear%20probing%2C%20reaching%20performance%20levels%20close%20to%20100%25%20data%20availability%20with%20only%2010%25%20of%20training%20data.%20Furthermore%2C%20LoRA-adaptation%20of%20the%20most%20recent%20foundation%20models%20almost%20closes%20the%20out-of-domain%20performance%20gap%20when%20evaluated%20on%20unseen%20tumor%20domains.%20However%2C%20full%20fine-tuning%20of%20traditional%20architectures%20still%20yields%20competitive%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2508.04441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Foundation%2520Models%2520for%2520Mitotic%2520Figure%2520Classification%26entry.906535625%3DJonas%2520Ammeling%2520and%2520Jonathan%2520Ganz%2520and%2520Emely%2520Rosbach%2520and%2520Ludwig%2520Lausser%2520and%2520Christof%2520A.%2520Bertram%2520and%2520Katharina%2520Breininger%2520and%2520Marc%2520Aubreville%26entry.1292438233%3DThe%2520performance%2520of%2520deep%2520learning%2520models%2520is%2520known%2520to%2520scale%2520with%2520data%2520quantity%2520and%2520diversity.%2520In%2520pathology%252C%2520as%2520in%2520many%2520other%2520medical%2520imaging%2520domains%252C%2520the%2520availability%2520of%2520labeled%2520images%2520for%2520a%2520specific%2520task%2520is%2520often%2520limited.%2520Self-supervised%2520learning%2520techniques%2520have%2520enabled%2520the%2520use%2520of%2520vast%2520amounts%2520of%2520unlabeled%2520data%2520to%2520train%2520large-scale%2520neural%2520networks%252C%2520i.e.%252C%2520foundation%2520models%252C%2520that%2520can%2520address%2520the%2520limited%2520data%2520problem%2520by%2520providing%2520semantically%2520rich%2520feature%2520vectors%2520that%2520can%2520generalize%2520well%2520to%2520new%2520tasks%2520with%2520minimal%2520training%2520effort%2520increasing%2520model%2520performance%2520and%2520robustness.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520use%2520of%2520foundation%2520models%2520for%2520mitotic%2520figure%2520classification.%2520The%2520mitotic%2520count%252C%2520which%2520can%2520be%2520derived%2520from%2520this%2520classification%2520task%252C%2520is%2520an%2520independent%2520prognostic%2520marker%2520for%2520specific%2520tumors%2520and%2520part%2520of%2520certain%2520tumor%2520grading%2520systems.%2520In%2520particular%252C%2520we%2520investigate%2520the%2520data%2520scaling%2520laws%2520on%2520multiple%2520current%2520foundation%2520models%2520and%2520evaluate%2520their%2520robustness%2520to%2520unseen%2520tumor%2520domains.%2520Next%2520to%2520the%2520commonly%2520used%2520linear%2520probing%2520paradigm%252C%2520we%2520also%2520adapt%2520the%2520models%2520using%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520of%2520their%2520attention%2520mechanisms.%2520We%2520compare%2520all%2520models%2520against%2520end-to-end-trained%2520baselines%252C%2520both%2520CNNs%2520and%2520Vision%2520Transformers.%2520Our%2520results%2520demonstrate%2520that%2520LoRA-adapted%2520foundation%2520models%2520provide%2520superior%2520performance%2520to%2520those%2520adapted%2520with%2520standard%2520linear%2520probing%252C%2520reaching%2520performance%2520levels%2520close%2520to%2520100%2525%2520data%2520availability%2520with%2520only%252010%2525%2520of%2520training%2520data.%2520Furthermore%252C%2520LoRA-adaptation%2520of%2520the%2520most%2520recent%2520foundation%2520models%2520almost%2520closes%2520the%2520out-of-domain%2520performance%2520gap%2520when%2520evaluated%2520on%2520unseen%2520tumor%2520domains.%2520However%252C%2520full%2520fine-tuning%2520of%2520traditional%2520architectures%2520still%2520yields%2520competitive%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Foundation%20Models%20for%20Mitotic%20Figure%20Classification&entry.906535625=Jonas%20Ammeling%20and%20Jonathan%20Ganz%20and%20Emely%20Rosbach%20and%20Ludwig%20Lausser%20and%20Christof%20A.%20Bertram%20and%20Katharina%20Breininger%20and%20Marc%20Aubreville&entry.1292438233=The%20performance%20of%20deep%20learning%20models%20is%20known%20to%20scale%20with%20data%20quantity%20and%20diversity.%20In%20pathology%2C%20as%20in%20many%20other%20medical%20imaging%20domains%2C%20the%20availability%20of%20labeled%20images%20for%20a%20specific%20task%20is%20often%20limited.%20Self-supervised%20learning%20techniques%20have%20enabled%20the%20use%20of%20vast%20amounts%20of%20unlabeled%20data%20to%20train%20large-scale%20neural%20networks%2C%20i.e.%2C%20foundation%20models%2C%20that%20can%20address%20the%20limited%20data%20problem%20by%20providing%20semantically%20rich%20feature%20vectors%20that%20can%20generalize%20well%20to%20new%20tasks%20with%20minimal%20training%20effort%20increasing%20model%20performance%20and%20robustness.%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20foundation%20models%20for%20mitotic%20figure%20classification.%20The%20mitotic%20count%2C%20which%20can%20be%20derived%20from%20this%20classification%20task%2C%20is%20an%20independent%20prognostic%20marker%20for%20specific%20tumors%20and%20part%20of%20certain%20tumor%20grading%20systems.%20In%20particular%2C%20we%20investigate%20the%20data%20scaling%20laws%20on%20multiple%20current%20foundation%20models%20and%20evaluate%20their%20robustness%20to%20unseen%20tumor%20domains.%20Next%20to%20the%20commonly%20used%20linear%20probing%20paradigm%2C%20we%20also%20adapt%20the%20models%20using%20low-rank%20adaptation%20%28LoRA%29%20of%20their%20attention%20mechanisms.%20We%20compare%20all%20models%20against%20end-to-end-trained%20baselines%2C%20both%20CNNs%20and%20Vision%20Transformers.%20Our%20results%20demonstrate%20that%20LoRA-adapted%20foundation%20models%20provide%20superior%20performance%20to%20those%20adapted%20with%20standard%20linear%20probing%2C%20reaching%20performance%20levels%20close%20to%20100%25%20data%20availability%20with%20only%2010%25%20of%20training%20data.%20Furthermore%2C%20LoRA-adaptation%20of%20the%20most%20recent%20foundation%20models%20almost%20closes%20the%20out-of-domain%20performance%20gap%20when%20evaluated%20on%20unseen%20tumor%20domains.%20However%2C%20full%20fine-tuning%20of%20traditional%20architectures%20still%20yields%20competitive%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2508.04441v2&entry.124074799=Read"},
{"title": "Learning to Build Shapes by Extrusion", "author": "Thor Vestergaard Christiansen and Karran Pandey and Alba Reinders and Karan Singh and Morten Rieger Hannemose and J. Andreas B\u00e6rentzen", "abstract": "We introduce Text Encoded Extrusion (TEE), a text-based representation that expresses mesh construction as sequences of face extrusions rather than polygon lists, and a method for generating 3D meshes from TEE using a large language model (LLM). By learning extrusion sequences that assemble a mesh, similar to the way artists create meshes, our approach naturally supports arbitrary output face counts and produces manifold meshes by design, in contrast to recent transformer-based models. The learnt extrusion sequences can also be applied to existing meshes - enabling editing in addition to generation. To train our model, we decompose a library of quadrilateral meshes with non-self-intersecting face loops into constituent loops, which can be viewed as their building blocks, and finetune an LLM on the steps for reassembling the meshes by performing a sequence of extrusions. We demonstrate that our representation enables reconstruction, novel shape synthesis, and the addition of new features to existing meshes.", "link": "http://arxiv.org/abs/2601.22858v1", "date": "2026-01-30", "relevancy": 2.7278, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6168}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.51}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Build%20Shapes%20by%20Extrusion&body=Title%3A%20Learning%20to%20Build%20Shapes%20by%20Extrusion%0AAuthor%3A%20Thor%20Vestergaard%20Christiansen%20and%20Karran%20Pandey%20and%20Alba%20Reinders%20and%20Karan%20Singh%20and%20Morten%20Rieger%20Hannemose%20and%20J.%20Andreas%20B%C3%A6rentzen%0AAbstract%3A%20We%20introduce%20Text%20Encoded%20Extrusion%20%28TEE%29%2C%20a%20text-based%20representation%20that%20expresses%20mesh%20construction%20as%20sequences%20of%20face%20extrusions%20rather%20than%20polygon%20lists%2C%20and%20a%20method%20for%20generating%203D%20meshes%20from%20TEE%20using%20a%20large%20language%20model%20%28LLM%29.%20By%20learning%20extrusion%20sequences%20that%20assemble%20a%20mesh%2C%20similar%20to%20the%20way%20artists%20create%20meshes%2C%20our%20approach%20naturally%20supports%20arbitrary%20output%20face%20counts%20and%20produces%20manifold%20meshes%20by%20design%2C%20in%20contrast%20to%20recent%20transformer-based%20models.%20The%20learnt%20extrusion%20sequences%20can%20also%20be%20applied%20to%20existing%20meshes%20-%20enabling%20editing%20in%20addition%20to%20generation.%20To%20train%20our%20model%2C%20we%20decompose%20a%20library%20of%20quadrilateral%20meshes%20with%20non-self-intersecting%20face%20loops%20into%20constituent%20loops%2C%20which%20can%20be%20viewed%20as%20their%20building%20blocks%2C%20and%20finetune%20an%20LLM%20on%20the%20steps%20for%20reassembling%20the%20meshes%20by%20performing%20a%20sequence%20of%20extrusions.%20We%20demonstrate%20that%20our%20representation%20enables%20reconstruction%2C%20novel%20shape%20synthesis%2C%20and%20the%20addition%20of%20new%20features%20to%20existing%20meshes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Build%2520Shapes%2520by%2520Extrusion%26entry.906535625%3DThor%2520Vestergaard%2520Christiansen%2520and%2520Karran%2520Pandey%2520and%2520Alba%2520Reinders%2520and%2520Karan%2520Singh%2520and%2520Morten%2520Rieger%2520Hannemose%2520and%2520J.%2520Andreas%2520B%25C3%25A6rentzen%26entry.1292438233%3DWe%2520introduce%2520Text%2520Encoded%2520Extrusion%2520%2528TEE%2529%252C%2520a%2520text-based%2520representation%2520that%2520expresses%2520mesh%2520construction%2520as%2520sequences%2520of%2520face%2520extrusions%2520rather%2520than%2520polygon%2520lists%252C%2520and%2520a%2520method%2520for%2520generating%25203D%2520meshes%2520from%2520TEE%2520using%2520a%2520large%2520language%2520model%2520%2528LLM%2529.%2520By%2520learning%2520extrusion%2520sequences%2520that%2520assemble%2520a%2520mesh%252C%2520similar%2520to%2520the%2520way%2520artists%2520create%2520meshes%252C%2520our%2520approach%2520naturally%2520supports%2520arbitrary%2520output%2520face%2520counts%2520and%2520produces%2520manifold%2520meshes%2520by%2520design%252C%2520in%2520contrast%2520to%2520recent%2520transformer-based%2520models.%2520The%2520learnt%2520extrusion%2520sequences%2520can%2520also%2520be%2520applied%2520to%2520existing%2520meshes%2520-%2520enabling%2520editing%2520in%2520addition%2520to%2520generation.%2520To%2520train%2520our%2520model%252C%2520we%2520decompose%2520a%2520library%2520of%2520quadrilateral%2520meshes%2520with%2520non-self-intersecting%2520face%2520loops%2520into%2520constituent%2520loops%252C%2520which%2520can%2520be%2520viewed%2520as%2520their%2520building%2520blocks%252C%2520and%2520finetune%2520an%2520LLM%2520on%2520the%2520steps%2520for%2520reassembling%2520the%2520meshes%2520by%2520performing%2520a%2520sequence%2520of%2520extrusions.%2520We%2520demonstrate%2520that%2520our%2520representation%2520enables%2520reconstruction%252C%2520novel%2520shape%2520synthesis%252C%2520and%2520the%2520addition%2520of%2520new%2520features%2520to%2520existing%2520meshes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Build%20Shapes%20by%20Extrusion&entry.906535625=Thor%20Vestergaard%20Christiansen%20and%20Karran%20Pandey%20and%20Alba%20Reinders%20and%20Karan%20Singh%20and%20Morten%20Rieger%20Hannemose%20and%20J.%20Andreas%20B%C3%A6rentzen&entry.1292438233=We%20introduce%20Text%20Encoded%20Extrusion%20%28TEE%29%2C%20a%20text-based%20representation%20that%20expresses%20mesh%20construction%20as%20sequences%20of%20face%20extrusions%20rather%20than%20polygon%20lists%2C%20and%20a%20method%20for%20generating%203D%20meshes%20from%20TEE%20using%20a%20large%20language%20model%20%28LLM%29.%20By%20learning%20extrusion%20sequences%20that%20assemble%20a%20mesh%2C%20similar%20to%20the%20way%20artists%20create%20meshes%2C%20our%20approach%20naturally%20supports%20arbitrary%20output%20face%20counts%20and%20produces%20manifold%20meshes%20by%20design%2C%20in%20contrast%20to%20recent%20transformer-based%20models.%20The%20learnt%20extrusion%20sequences%20can%20also%20be%20applied%20to%20existing%20meshes%20-%20enabling%20editing%20in%20addition%20to%20generation.%20To%20train%20our%20model%2C%20we%20decompose%20a%20library%20of%20quadrilateral%20meshes%20with%20non-self-intersecting%20face%20loops%20into%20constituent%20loops%2C%20which%20can%20be%20viewed%20as%20their%20building%20blocks%2C%20and%20finetune%20an%20LLM%20on%20the%20steps%20for%20reassembling%20the%20meshes%20by%20performing%20a%20sequence%20of%20extrusions.%20We%20demonstrate%20that%20our%20representation%20enables%20reconstruction%2C%20novel%20shape%20synthesis%2C%20and%20the%20addition%20of%20new%20features%20to%20existing%20meshes.&entry.1838667208=http%3A//arxiv.org/abs/2601.22858v1&entry.124074799=Read"},
{"title": "HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation", "author": "Hari Krishna Gadi and Daniel Matos and Hongyi Luo and Lu Liu and Yongliang Wang and Yanfeng Zhang and Liqiu Meng", "abstract": "Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.", "link": "http://arxiv.org/abs/2601.23064v1", "date": "2026-01-30", "relevancy": 2.7044, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.576}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5235}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HierLoc%3A%20Hyperbolic%20Entity%20Embeddings%20for%20Hierarchical%20Visual%20Geolocation&body=Title%3A%20HierLoc%3A%20Hyperbolic%20Entity%20Embeddings%20for%20Hierarchical%20Visual%20Geolocation%0AAuthor%3A%20Hari%20Krishna%20Gadi%20and%20Daniel%20Matos%20and%20Hongyi%20Luo%20and%20Lu%20Liu%20and%20Yongliang%20Wang%20and%20Yanfeng%20Zhang%20and%20Liqiu%20Meng%0AAbstract%3A%20Visual%20geolocalization%2C%20the%20task%20of%20predicting%20where%20an%20image%20was%20taken%2C%20remains%20challenging%20due%20to%20global%20scale%2C%20visual%20ambiguity%2C%20and%20the%20inherently%20hierarchical%20structure%20of%20geography.%20Existing%20paradigms%20rely%20on%20either%20large-scale%20retrieval%2C%20which%20requires%20storing%20a%20large%20number%20of%20image%20embeddings%2C%20grid-based%20classifiers%20that%20ignore%20geographic%20continuity%2C%20or%20generative%20models%20that%20diffuse%20over%20space%20but%20struggle%20with%20fine%20detail.%20We%20introduce%20an%20entity-centric%20formulation%20of%20geolocation%20that%20replaces%20image-to-image%20retrieval%20with%20a%20compact%20hierarchy%20of%20geographic%20entities%20embedded%20in%20Hyperbolic%20space.%20Images%20are%20aligned%20directly%20to%20country%2C%20region%2C%20subregion%2C%20and%20city%20entities%20through%20Geo-Weighted%20Hyperbolic%20contrastive%20learning%20by%20directly%20incorporating%20haversine%20distance%20into%20the%20contrastive%20objective.%20This%20hierarchical%20design%20enables%20interpretable%20predictions%20and%20efficient%20inference%20with%20240k%20entity%20embeddings%20instead%20of%20over%205%20million%20image%20embeddings%20on%20the%20OSV5M%20benchmark%2C%20on%20which%20our%20method%20establishes%20a%20new%20state-of-the-art%20performance.%20Compared%20to%20the%20current%20methods%20in%20the%20literature%2C%20it%20reduces%20mean%20geodesic%20error%20by%2019.5%5C%25%2C%20while%20improving%20the%20fine-grained%20subregion%20accuracy%20by%2043%25.%20These%20results%20demonstrate%20that%20geometry-aware%20hierarchical%20embeddings%20provide%20a%20scalable%20and%20conceptually%20new%20alternative%20for%20global%20image%20geolocation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierLoc%253A%2520Hyperbolic%2520Entity%2520Embeddings%2520for%2520Hierarchical%2520Visual%2520Geolocation%26entry.906535625%3DHari%2520Krishna%2520Gadi%2520and%2520Daniel%2520Matos%2520and%2520Hongyi%2520Luo%2520and%2520Lu%2520Liu%2520and%2520Yongliang%2520Wang%2520and%2520Yanfeng%2520Zhang%2520and%2520Liqiu%2520Meng%26entry.1292438233%3DVisual%2520geolocalization%252C%2520the%2520task%2520of%2520predicting%2520where%2520an%2520image%2520was%2520taken%252C%2520remains%2520challenging%2520due%2520to%2520global%2520scale%252C%2520visual%2520ambiguity%252C%2520and%2520the%2520inherently%2520hierarchical%2520structure%2520of%2520geography.%2520Existing%2520paradigms%2520rely%2520on%2520either%2520large-scale%2520retrieval%252C%2520which%2520requires%2520storing%2520a%2520large%2520number%2520of%2520image%2520embeddings%252C%2520grid-based%2520classifiers%2520that%2520ignore%2520geographic%2520continuity%252C%2520or%2520generative%2520models%2520that%2520diffuse%2520over%2520space%2520but%2520struggle%2520with%2520fine%2520detail.%2520We%2520introduce%2520an%2520entity-centric%2520formulation%2520of%2520geolocation%2520that%2520replaces%2520image-to-image%2520retrieval%2520with%2520a%2520compact%2520hierarchy%2520of%2520geographic%2520entities%2520embedded%2520in%2520Hyperbolic%2520space.%2520Images%2520are%2520aligned%2520directly%2520to%2520country%252C%2520region%252C%2520subregion%252C%2520and%2520city%2520entities%2520through%2520Geo-Weighted%2520Hyperbolic%2520contrastive%2520learning%2520by%2520directly%2520incorporating%2520haversine%2520distance%2520into%2520the%2520contrastive%2520objective.%2520This%2520hierarchical%2520design%2520enables%2520interpretable%2520predictions%2520and%2520efficient%2520inference%2520with%2520240k%2520entity%2520embeddings%2520instead%2520of%2520over%25205%2520million%2520image%2520embeddings%2520on%2520the%2520OSV5M%2520benchmark%252C%2520on%2520which%2520our%2520method%2520establishes%2520a%2520new%2520state-of-the-art%2520performance.%2520Compared%2520to%2520the%2520current%2520methods%2520in%2520the%2520literature%252C%2520it%2520reduces%2520mean%2520geodesic%2520error%2520by%252019.5%255C%2525%252C%2520while%2520improving%2520the%2520fine-grained%2520subregion%2520accuracy%2520by%252043%2525.%2520These%2520results%2520demonstrate%2520that%2520geometry-aware%2520hierarchical%2520embeddings%2520provide%2520a%2520scalable%2520and%2520conceptually%2520new%2520alternative%2520for%2520global%2520image%2520geolocation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HierLoc%3A%20Hyperbolic%20Entity%20Embeddings%20for%20Hierarchical%20Visual%20Geolocation&entry.906535625=Hari%20Krishna%20Gadi%20and%20Daniel%20Matos%20and%20Hongyi%20Luo%20and%20Lu%20Liu%20and%20Yongliang%20Wang%20and%20Yanfeng%20Zhang%20and%20Liqiu%20Meng&entry.1292438233=Visual%20geolocalization%2C%20the%20task%20of%20predicting%20where%20an%20image%20was%20taken%2C%20remains%20challenging%20due%20to%20global%20scale%2C%20visual%20ambiguity%2C%20and%20the%20inherently%20hierarchical%20structure%20of%20geography.%20Existing%20paradigms%20rely%20on%20either%20large-scale%20retrieval%2C%20which%20requires%20storing%20a%20large%20number%20of%20image%20embeddings%2C%20grid-based%20classifiers%20that%20ignore%20geographic%20continuity%2C%20or%20generative%20models%20that%20diffuse%20over%20space%20but%20struggle%20with%20fine%20detail.%20We%20introduce%20an%20entity-centric%20formulation%20of%20geolocation%20that%20replaces%20image-to-image%20retrieval%20with%20a%20compact%20hierarchy%20of%20geographic%20entities%20embedded%20in%20Hyperbolic%20space.%20Images%20are%20aligned%20directly%20to%20country%2C%20region%2C%20subregion%2C%20and%20city%20entities%20through%20Geo-Weighted%20Hyperbolic%20contrastive%20learning%20by%20directly%20incorporating%20haversine%20distance%20into%20the%20contrastive%20objective.%20This%20hierarchical%20design%20enables%20interpretable%20predictions%20and%20efficient%20inference%20with%20240k%20entity%20embeddings%20instead%20of%20over%205%20million%20image%20embeddings%20on%20the%20OSV5M%20benchmark%2C%20on%20which%20our%20method%20establishes%20a%20new%20state-of-the-art%20performance.%20Compared%20to%20the%20current%20methods%20in%20the%20literature%2C%20it%20reduces%20mean%20geodesic%20error%20by%2019.5%5C%25%2C%20while%20improving%20the%20fine-grained%20subregion%20accuracy%20by%2043%25.%20These%20results%20demonstrate%20that%20geometry-aware%20hierarchical%20embeddings%20provide%20a%20scalable%20and%20conceptually%20new%20alternative%20for%20global%20image%20geolocation.&entry.1838667208=http%3A//arxiv.org/abs/2601.23064v1&entry.124074799=Read"},
{"title": "Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation", "author": "Yizhao Han and Tianxing Shi and Zhao Wang and Zifan Xu and Zhiyuan Pu and Mingxiao Li and Qian Zhang and Wei Yin and Xiao-Xiao Long", "abstract": "Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.", "link": "http://arxiv.org/abs/2601.19488v2", "date": "2026-01-30", "relevancy": 2.699, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5532}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5369}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Guided%20k-Guard%20Sampling%20for%20Long-Horizon%20Autoregressive%20Video%20Generation&body=Title%3A%20Entropy-Guided%20k-Guard%20Sampling%20for%20Long-Horizon%20Autoregressive%20Video%20Generation%0AAuthor%3A%20Yizhao%20Han%20and%20Tianxing%20Shi%20and%20Zhao%20Wang%20and%20Zifan%20Xu%20and%20Zhiyuan%20Pu%20and%20Mingxiao%20Li%20and%20Qian%20Zhang%20and%20Wei%20Yin%20and%20Xiao-Xiao%20Long%0AAbstract%3A%20Autoregressive%20%28AR%29%20architectures%20have%20achieved%20significant%20successes%20in%20LLMs%2C%20inspiring%20explorations%20for%20video%20generation.%20In%20LLMs%2C%20top-p/top-k%20sampling%20strategies%20work%20exceptionally%20well%3A%20language%20tokens%20have%20high%20semantic%20density%20and%20low%20redundancy%2C%20so%20a%20fixed%20size%20of%20token%20candidates%20already%20strikes%20a%20balance%20between%20semantic%20accuracy%20and%20generation%20diversity.%20In%20contrast%2C%20video%20tokens%20have%20low%20semantic%20density%20and%20high%20spatio-temporal%20redundancy.%20This%20mismatch%20makes%20static%20top-k/top-p%20strategies%20ineffective%20for%20video%20decoders%3A%20they%20either%20introduce%20unnecessary%20randomness%20for%20low-uncertainty%20regions%20%28static%20backgrounds%29%20or%20get%20stuck%20in%20early%20errors%20for%20high-uncertainty%20regions%20%28foreground%20objects%29.%20Prediction%20errors%20will%20accumulate%20as%20more%20frames%20are%20generated%20and%20eventually%20severely%20degrade%20long-horizon%20quality.%20To%20address%20this%2C%20we%20propose%20Entropy-Guided%20k-Guard%20%28ENkG%29%20sampling%2C%20a%20simple%20yet%20effective%20strategy%20that%20adapts%20sampling%20to%20token-wise%20dispersion%2C%20quantified%20by%20the%20entropy%20of%20each%20token%27s%20predicted%20distribution.%20ENkG%20uses%20adaptive%20token%20candidate%20sizes%3A%20for%20low-entropy%20regions%2C%20it%20employs%20fewer%20candidates%20to%20suppress%20redundant%20noise%20and%20preserve%20structural%20integrity%3B%20for%20high-entropy%20regions%2C%20it%20uses%20more%20candidates%20to%20mitigate%20error%20compounding.%20ENkG%20is%20model-agnostic%2C%20training-free%2C%20and%20adds%20negligible%20overhead.%20Experiments%20demonstrate%20consistent%20improvements%20in%20perceptual%20quality%20and%20structural%20stability%20compared%20to%20static%20top-k/top-p%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Guided%2520k-Guard%2520Sampling%2520for%2520Long-Horizon%2520Autoregressive%2520Video%2520Generation%26entry.906535625%3DYizhao%2520Han%2520and%2520Tianxing%2520Shi%2520and%2520Zhao%2520Wang%2520and%2520Zifan%2520Xu%2520and%2520Zhiyuan%2520Pu%2520and%2520Mingxiao%2520Li%2520and%2520Qian%2520Zhang%2520and%2520Wei%2520Yin%2520and%2520Xiao-Xiao%2520Long%26entry.1292438233%3DAutoregressive%2520%2528AR%2529%2520architectures%2520have%2520achieved%2520significant%2520successes%2520in%2520LLMs%252C%2520inspiring%2520explorations%2520for%2520video%2520generation.%2520In%2520LLMs%252C%2520top-p/top-k%2520sampling%2520strategies%2520work%2520exceptionally%2520well%253A%2520language%2520tokens%2520have%2520high%2520semantic%2520density%2520and%2520low%2520redundancy%252C%2520so%2520a%2520fixed%2520size%2520of%2520token%2520candidates%2520already%2520strikes%2520a%2520balance%2520between%2520semantic%2520accuracy%2520and%2520generation%2520diversity.%2520In%2520contrast%252C%2520video%2520tokens%2520have%2520low%2520semantic%2520density%2520and%2520high%2520spatio-temporal%2520redundancy.%2520This%2520mismatch%2520makes%2520static%2520top-k/top-p%2520strategies%2520ineffective%2520for%2520video%2520decoders%253A%2520they%2520either%2520introduce%2520unnecessary%2520randomness%2520for%2520low-uncertainty%2520regions%2520%2528static%2520backgrounds%2529%2520or%2520get%2520stuck%2520in%2520early%2520errors%2520for%2520high-uncertainty%2520regions%2520%2528foreground%2520objects%2529.%2520Prediction%2520errors%2520will%2520accumulate%2520as%2520more%2520frames%2520are%2520generated%2520and%2520eventually%2520severely%2520degrade%2520long-horizon%2520quality.%2520To%2520address%2520this%252C%2520we%2520propose%2520Entropy-Guided%2520k-Guard%2520%2528ENkG%2529%2520sampling%252C%2520a%2520simple%2520yet%2520effective%2520strategy%2520that%2520adapts%2520sampling%2520to%2520token-wise%2520dispersion%252C%2520quantified%2520by%2520the%2520entropy%2520of%2520each%2520token%2527s%2520predicted%2520distribution.%2520ENkG%2520uses%2520adaptive%2520token%2520candidate%2520sizes%253A%2520for%2520low-entropy%2520regions%252C%2520it%2520employs%2520fewer%2520candidates%2520to%2520suppress%2520redundant%2520noise%2520and%2520preserve%2520structural%2520integrity%253B%2520for%2520high-entropy%2520regions%252C%2520it%2520uses%2520more%2520candidates%2520to%2520mitigate%2520error%2520compounding.%2520ENkG%2520is%2520model-agnostic%252C%2520training-free%252C%2520and%2520adds%2520negligible%2520overhead.%2520Experiments%2520demonstrate%2520consistent%2520improvements%2520in%2520perceptual%2520quality%2520and%2520structural%2520stability%2520compared%2520to%2520static%2520top-k/top-p%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Guided%20k-Guard%20Sampling%20for%20Long-Horizon%20Autoregressive%20Video%20Generation&entry.906535625=Yizhao%20Han%20and%20Tianxing%20Shi%20and%20Zhao%20Wang%20and%20Zifan%20Xu%20and%20Zhiyuan%20Pu%20and%20Mingxiao%20Li%20and%20Qian%20Zhang%20and%20Wei%20Yin%20and%20Xiao-Xiao%20Long&entry.1292438233=Autoregressive%20%28AR%29%20architectures%20have%20achieved%20significant%20successes%20in%20LLMs%2C%20inspiring%20explorations%20for%20video%20generation.%20In%20LLMs%2C%20top-p/top-k%20sampling%20strategies%20work%20exceptionally%20well%3A%20language%20tokens%20have%20high%20semantic%20density%20and%20low%20redundancy%2C%20so%20a%20fixed%20size%20of%20token%20candidates%20already%20strikes%20a%20balance%20between%20semantic%20accuracy%20and%20generation%20diversity.%20In%20contrast%2C%20video%20tokens%20have%20low%20semantic%20density%20and%20high%20spatio-temporal%20redundancy.%20This%20mismatch%20makes%20static%20top-k/top-p%20strategies%20ineffective%20for%20video%20decoders%3A%20they%20either%20introduce%20unnecessary%20randomness%20for%20low-uncertainty%20regions%20%28static%20backgrounds%29%20or%20get%20stuck%20in%20early%20errors%20for%20high-uncertainty%20regions%20%28foreground%20objects%29.%20Prediction%20errors%20will%20accumulate%20as%20more%20frames%20are%20generated%20and%20eventually%20severely%20degrade%20long-horizon%20quality.%20To%20address%20this%2C%20we%20propose%20Entropy-Guided%20k-Guard%20%28ENkG%29%20sampling%2C%20a%20simple%20yet%20effective%20strategy%20that%20adapts%20sampling%20to%20token-wise%20dispersion%2C%20quantified%20by%20the%20entropy%20of%20each%20token%27s%20predicted%20distribution.%20ENkG%20uses%20adaptive%20token%20candidate%20sizes%3A%20for%20low-entropy%20regions%2C%20it%20employs%20fewer%20candidates%20to%20suppress%20redundant%20noise%20and%20preserve%20structural%20integrity%3B%20for%20high-entropy%20regions%2C%20it%20uses%20more%20candidates%20to%20mitigate%20error%20compounding.%20ENkG%20is%20model-agnostic%2C%20training-free%2C%20and%20adds%20negligible%20overhead.%20Experiments%20demonstrate%20consistent%20improvements%20in%20perceptual%20quality%20and%20structural%20stability%20compared%20to%20static%20top-k/top-p%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2601.19488v2&entry.124074799=Read"},
{"title": "NativeTok: Native Visual Tokenization for Improved Image Generation", "author": "Bin Wu and Mengqi Huang and Weinan Jia and Zhendong Mao", "abstract": "VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.", "link": "http://arxiv.org/abs/2601.22837v1", "date": "2026-01-30", "relevancy": 2.6976, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5805}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.545}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NativeTok%3A%20Native%20Visual%20Tokenization%20for%20Improved%20Image%20Generation&body=Title%3A%20NativeTok%3A%20Native%20Visual%20Tokenization%20for%20Improved%20Image%20Generation%0AAuthor%3A%20Bin%20Wu%20and%20Mengqi%20Huang%20and%20Weinan%20Jia%20and%20Zhendong%20Mao%0AAbstract%3A%20VQ-based%20image%20generation%20typically%20follows%20a%20two-stage%20pipeline%3A%20a%20tokenizer%20encodes%20images%20into%20discrete%20tokens%2C%20and%20a%20generative%20model%20learns%20their%20dependencies%20for%20reconstruction.%20However%2C%20improved%20tokenization%20in%20the%20first%20stage%20does%20not%20necessarily%20enhance%20the%20second-stage%20generation%2C%20as%20existing%20methods%20fail%20to%20constrain%20token%20dependencies.%20This%20mismatch%20forces%20the%20generative%20model%20to%20learn%20from%20unordered%20distributions%2C%20leading%20to%20bias%20and%20weak%20coherence.%20To%20address%20this%2C%20we%20propose%20native%20visual%20tokenization%2C%20which%20enforces%20causal%20dependencies%20during%20tokenization.%20Building%20on%20this%20idea%2C%20we%20introduce%20NativeTok%2C%20a%20framework%20that%20achieves%20efficient%20reconstruction%20while%20embedding%20relational%20constraints%20within%20token%20sequences.%20NativeTok%20consists%20of%3A%20%281%29%20a%20Meta%20Image%20Transformer%20%28MIT%29%20for%20latent%20image%20modeling%2C%20and%20%282%29%20a%20Mixture%20of%20Causal%20Expert%20Transformer%20%28MoCET%29%2C%20where%20each%20lightweight%20expert%20block%20generates%20a%20single%20token%20conditioned%20on%20prior%20tokens%20and%20latent%20features.%20We%20further%20design%20a%20Hierarchical%20Native%20Training%20strategy%20that%20updates%20only%20new%20expert%20blocks%2C%20ensuring%20training%20efficiency.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20NativeTok.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNativeTok%253A%2520Native%2520Visual%2520Tokenization%2520for%2520Improved%2520Image%2520Generation%26entry.906535625%3DBin%2520Wu%2520and%2520Mengqi%2520Huang%2520and%2520Weinan%2520Jia%2520and%2520Zhendong%2520Mao%26entry.1292438233%3DVQ-based%2520image%2520generation%2520typically%2520follows%2520a%2520two-stage%2520pipeline%253A%2520a%2520tokenizer%2520encodes%2520images%2520into%2520discrete%2520tokens%252C%2520and%2520a%2520generative%2520model%2520learns%2520their%2520dependencies%2520for%2520reconstruction.%2520However%252C%2520improved%2520tokenization%2520in%2520the%2520first%2520stage%2520does%2520not%2520necessarily%2520enhance%2520the%2520second-stage%2520generation%252C%2520as%2520existing%2520methods%2520fail%2520to%2520constrain%2520token%2520dependencies.%2520This%2520mismatch%2520forces%2520the%2520generative%2520model%2520to%2520learn%2520from%2520unordered%2520distributions%252C%2520leading%2520to%2520bias%2520and%2520weak%2520coherence.%2520To%2520address%2520this%252C%2520we%2520propose%2520native%2520visual%2520tokenization%252C%2520which%2520enforces%2520causal%2520dependencies%2520during%2520tokenization.%2520Building%2520on%2520this%2520idea%252C%2520we%2520introduce%2520NativeTok%252C%2520a%2520framework%2520that%2520achieves%2520efficient%2520reconstruction%2520while%2520embedding%2520relational%2520constraints%2520within%2520token%2520sequences.%2520NativeTok%2520consists%2520of%253A%2520%25281%2529%2520a%2520Meta%2520Image%2520Transformer%2520%2528MIT%2529%2520for%2520latent%2520image%2520modeling%252C%2520and%2520%25282%2529%2520a%2520Mixture%2520of%2520Causal%2520Expert%2520Transformer%2520%2528MoCET%2529%252C%2520where%2520each%2520lightweight%2520expert%2520block%2520generates%2520a%2520single%2520token%2520conditioned%2520on%2520prior%2520tokens%2520and%2520latent%2520features.%2520We%2520further%2520design%2520a%2520Hierarchical%2520Native%2520Training%2520strategy%2520that%2520updates%2520only%2520new%2520expert%2520blocks%252C%2520ensuring%2520training%2520efficiency.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520NativeTok.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NativeTok%3A%20Native%20Visual%20Tokenization%20for%20Improved%20Image%20Generation&entry.906535625=Bin%20Wu%20and%20Mengqi%20Huang%20and%20Weinan%20Jia%20and%20Zhendong%20Mao&entry.1292438233=VQ-based%20image%20generation%20typically%20follows%20a%20two-stage%20pipeline%3A%20a%20tokenizer%20encodes%20images%20into%20discrete%20tokens%2C%20and%20a%20generative%20model%20learns%20their%20dependencies%20for%20reconstruction.%20However%2C%20improved%20tokenization%20in%20the%20first%20stage%20does%20not%20necessarily%20enhance%20the%20second-stage%20generation%2C%20as%20existing%20methods%20fail%20to%20constrain%20token%20dependencies.%20This%20mismatch%20forces%20the%20generative%20model%20to%20learn%20from%20unordered%20distributions%2C%20leading%20to%20bias%20and%20weak%20coherence.%20To%20address%20this%2C%20we%20propose%20native%20visual%20tokenization%2C%20which%20enforces%20causal%20dependencies%20during%20tokenization.%20Building%20on%20this%20idea%2C%20we%20introduce%20NativeTok%2C%20a%20framework%20that%20achieves%20efficient%20reconstruction%20while%20embedding%20relational%20constraints%20within%20token%20sequences.%20NativeTok%20consists%20of%3A%20%281%29%20a%20Meta%20Image%20Transformer%20%28MIT%29%20for%20latent%20image%20modeling%2C%20and%20%282%29%20a%20Mixture%20of%20Causal%20Expert%20Transformer%20%28MoCET%29%2C%20where%20each%20lightweight%20expert%20block%20generates%20a%20single%20token%20conditioned%20on%20prior%20tokens%20and%20latent%20features.%20We%20further%20design%20a%20Hierarchical%20Native%20Training%20strategy%20that%20updates%20only%20new%20expert%20blocks%2C%20ensuring%20training%20efficiency.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20NativeTok.&entry.1838667208=http%3A//arxiv.org/abs/2601.22837v1&entry.124074799=Read"},
{"title": "Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model", "author": "Zhijing Yang and Weiwei Zhang and Mingliang Yang and Siyuan Peng and Yukai Shi and Junpeng Tan and Tianshui Chen and Liruo Zhong", "abstract": "This work aims to address a novel Customized Virtual Try-ON (Cu-VTON) task, enabling the superimposition of a specified garment onto a model that can be customized in terms of appearance, posture, and additional attributes. Compared with traditional VTON task, it enables users to tailor digital avatars to their individual preferences, thereby enhancing the virtual fitting experience with greater flexibility and engagement. To address this task, we introduce a Neural Clothing Tryer (NCT) framework, which exploits the advanced diffusion models equipped with semantic enhancement and controlling modules to better preserve semantic characterization and textural details of the garment and meanwhile facilitating the flexible editing of the model's postures and appearances. Specifically, NCT introduces a semantic-enhanced module to take semantic descriptions of garments and utilizes a visual-language encoder to learn aligned features across modalities. The aligned features are served as condition input to the diffusion model to enhance the preservation of the garment's semantics. Then, a semantic controlling module is designed to take the garment image, tailored posture image, and semantic description as input to maintain garment details while simultaneously editing model postures, expressions, and various attributes. Extensive experiments on the open available benchmark demonstrate the superior performance of the proposed NCT framework.", "link": "http://arxiv.org/abs/2601.22838v1", "date": "2026-01-30", "relevancy": 2.6918, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7084}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6671}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Clothing%20Tryer%3A%20Customized%20Virtual%20Try-On%20via%20Semantic%20Enhancement%20and%20Controlling%20Diffusion%20Model&body=Title%3A%20Neural%20Clothing%20Tryer%3A%20Customized%20Virtual%20Try-On%20via%20Semantic%20Enhancement%20and%20Controlling%20Diffusion%20Model%0AAuthor%3A%20Zhijing%20Yang%20and%20Weiwei%20Zhang%20and%20Mingliang%20Yang%20and%20Siyuan%20Peng%20and%20Yukai%20Shi%20and%20Junpeng%20Tan%20and%20Tianshui%20Chen%20and%20Liruo%20Zhong%0AAbstract%3A%20This%20work%20aims%20to%20address%20a%20novel%20Customized%20Virtual%20Try-ON%20%28Cu-VTON%29%20task%2C%20enabling%20the%20superimposition%20of%20a%20specified%20garment%20onto%20a%20model%20that%20can%20be%20customized%20in%20terms%20of%20appearance%2C%20posture%2C%20and%20additional%20attributes.%20Compared%20with%20traditional%20VTON%20task%2C%20it%20enables%20users%20to%20tailor%20digital%20avatars%20to%20their%20individual%20preferences%2C%20thereby%20enhancing%20the%20virtual%20fitting%20experience%20with%20greater%20flexibility%20and%20engagement.%20To%20address%20this%20task%2C%20we%20introduce%20a%20Neural%20Clothing%20Tryer%20%28NCT%29%20framework%2C%20which%20exploits%20the%20advanced%20diffusion%20models%20equipped%20with%20semantic%20enhancement%20and%20controlling%20modules%20to%20better%20preserve%20semantic%20characterization%20and%20textural%20details%20of%20the%20garment%20and%20meanwhile%20facilitating%20the%20flexible%20editing%20of%20the%20model%27s%20postures%20and%20appearances.%20Specifically%2C%20NCT%20introduces%20a%20semantic-enhanced%20module%20to%20take%20semantic%20descriptions%20of%20garments%20and%20utilizes%20a%20visual-language%20encoder%20to%20learn%20aligned%20features%20across%20modalities.%20The%20aligned%20features%20are%20served%20as%20condition%20input%20to%20the%20diffusion%20model%20to%20enhance%20the%20preservation%20of%20the%20garment%27s%20semantics.%20Then%2C%20a%20semantic%20controlling%20module%20is%20designed%20to%20take%20the%20garment%20image%2C%20tailored%20posture%20image%2C%20and%20semantic%20description%20as%20input%20to%20maintain%20garment%20details%20while%20simultaneously%20editing%20model%20postures%2C%20expressions%2C%20and%20various%20attributes.%20Extensive%20experiments%20on%20the%20open%20available%20benchmark%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%20NCT%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Clothing%2520Tryer%253A%2520Customized%2520Virtual%2520Try-On%2520via%2520Semantic%2520Enhancement%2520and%2520Controlling%2520Diffusion%2520Model%26entry.906535625%3DZhijing%2520Yang%2520and%2520Weiwei%2520Zhang%2520and%2520Mingliang%2520Yang%2520and%2520Siyuan%2520Peng%2520and%2520Yukai%2520Shi%2520and%2520Junpeng%2520Tan%2520and%2520Tianshui%2520Chen%2520and%2520Liruo%2520Zhong%26entry.1292438233%3DThis%2520work%2520aims%2520to%2520address%2520a%2520novel%2520Customized%2520Virtual%2520Try-ON%2520%2528Cu-VTON%2529%2520task%252C%2520enabling%2520the%2520superimposition%2520of%2520a%2520specified%2520garment%2520onto%2520a%2520model%2520that%2520can%2520be%2520customized%2520in%2520terms%2520of%2520appearance%252C%2520posture%252C%2520and%2520additional%2520attributes.%2520Compared%2520with%2520traditional%2520VTON%2520task%252C%2520it%2520enables%2520users%2520to%2520tailor%2520digital%2520avatars%2520to%2520their%2520individual%2520preferences%252C%2520thereby%2520enhancing%2520the%2520virtual%2520fitting%2520experience%2520with%2520greater%2520flexibility%2520and%2520engagement.%2520To%2520address%2520this%2520task%252C%2520we%2520introduce%2520a%2520Neural%2520Clothing%2520Tryer%2520%2528NCT%2529%2520framework%252C%2520which%2520exploits%2520the%2520advanced%2520diffusion%2520models%2520equipped%2520with%2520semantic%2520enhancement%2520and%2520controlling%2520modules%2520to%2520better%2520preserve%2520semantic%2520characterization%2520and%2520textural%2520details%2520of%2520the%2520garment%2520and%2520meanwhile%2520facilitating%2520the%2520flexible%2520editing%2520of%2520the%2520model%2527s%2520postures%2520and%2520appearances.%2520Specifically%252C%2520NCT%2520introduces%2520a%2520semantic-enhanced%2520module%2520to%2520take%2520semantic%2520descriptions%2520of%2520garments%2520and%2520utilizes%2520a%2520visual-language%2520encoder%2520to%2520learn%2520aligned%2520features%2520across%2520modalities.%2520The%2520aligned%2520features%2520are%2520served%2520as%2520condition%2520input%2520to%2520the%2520diffusion%2520model%2520to%2520enhance%2520the%2520preservation%2520of%2520the%2520garment%2527s%2520semantics.%2520Then%252C%2520a%2520semantic%2520controlling%2520module%2520is%2520designed%2520to%2520take%2520the%2520garment%2520image%252C%2520tailored%2520posture%2520image%252C%2520and%2520semantic%2520description%2520as%2520input%2520to%2520maintain%2520garment%2520details%2520while%2520simultaneously%2520editing%2520model%2520postures%252C%2520expressions%252C%2520and%2520various%2520attributes.%2520Extensive%2520experiments%2520on%2520the%2520open%2520available%2520benchmark%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%2520NCT%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Clothing%20Tryer%3A%20Customized%20Virtual%20Try-On%20via%20Semantic%20Enhancement%20and%20Controlling%20Diffusion%20Model&entry.906535625=Zhijing%20Yang%20and%20Weiwei%20Zhang%20and%20Mingliang%20Yang%20and%20Siyuan%20Peng%20and%20Yukai%20Shi%20and%20Junpeng%20Tan%20and%20Tianshui%20Chen%20and%20Liruo%20Zhong&entry.1292438233=This%20work%20aims%20to%20address%20a%20novel%20Customized%20Virtual%20Try-ON%20%28Cu-VTON%29%20task%2C%20enabling%20the%20superimposition%20of%20a%20specified%20garment%20onto%20a%20model%20that%20can%20be%20customized%20in%20terms%20of%20appearance%2C%20posture%2C%20and%20additional%20attributes.%20Compared%20with%20traditional%20VTON%20task%2C%20it%20enables%20users%20to%20tailor%20digital%20avatars%20to%20their%20individual%20preferences%2C%20thereby%20enhancing%20the%20virtual%20fitting%20experience%20with%20greater%20flexibility%20and%20engagement.%20To%20address%20this%20task%2C%20we%20introduce%20a%20Neural%20Clothing%20Tryer%20%28NCT%29%20framework%2C%20which%20exploits%20the%20advanced%20diffusion%20models%20equipped%20with%20semantic%20enhancement%20and%20controlling%20modules%20to%20better%20preserve%20semantic%20characterization%20and%20textural%20details%20of%20the%20garment%20and%20meanwhile%20facilitating%20the%20flexible%20editing%20of%20the%20model%27s%20postures%20and%20appearances.%20Specifically%2C%20NCT%20introduces%20a%20semantic-enhanced%20module%20to%20take%20semantic%20descriptions%20of%20garments%20and%20utilizes%20a%20visual-language%20encoder%20to%20learn%20aligned%20features%20across%20modalities.%20The%20aligned%20features%20are%20served%20as%20condition%20input%20to%20the%20diffusion%20model%20to%20enhance%20the%20preservation%20of%20the%20garment%27s%20semantics.%20Then%2C%20a%20semantic%20controlling%20module%20is%20designed%20to%20take%20the%20garment%20image%2C%20tailored%20posture%20image%2C%20and%20semantic%20description%20as%20input%20to%20maintain%20garment%20details%20while%20simultaneously%20editing%20model%20postures%2C%20expressions%2C%20and%20various%20attributes.%20Extensive%20experiments%20on%20the%20open%20available%20benchmark%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%20NCT%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2601.22838v1&entry.124074799=Read"},
{"title": "Development of Domain-Invariant Visual Enhancement and Restoration (DIVER) Approach for Underwater Images", "author": "Rajini Makam and Sharanya Patil and Dhatri Shankari T M and Suresh Sundaram and Narasimhan Sundararajan", "abstract": "Underwater images suffer severe degradation due to wavelength-dependent attenuation, scattering, and illumination non-uniformity that vary across water types and depths. We propose an unsupervised Domain-Invariant Visual Enhancement and Restoration (DIVER) framework that integrates empirical correction with physics-guided modeling for robust underwater image enhancement. DIVER first applies either IlluminateNet for adaptive luminance enhancement or a Spectral Equalization Filter for spectral normalization. An Adaptive Optical Correction Module then refines hue and contrast using channel-adaptive filtering, while Hydro-OpticNet employs physics-constrained learning to compensate for backscatter and wavelength-dependent attenuation. The parameters of IlluminateNet and Hydro-OpticNet are optimized via unsupervised learning using a composite loss function. DIVER is evaluated on eight diverse datasets covering shallow, deep, and highly turbid environments, including both naturally low-light and artificially illuminated scenes, using reference and non-reference metrics. While state-of-the-art methods such as WaterNet, UDNet, and Phaseformer perform reasonably in shallow water, their performance degrades in deep, unevenly illuminated, or artificially lit conditions. In contrast, DIVER consistently achieves best or near-best performance across all datasets, demonstrating strong domain-invariant capability. DIVER yields at least a 9% improvement over SOTA methods in UCIQE. On the low-light SeaThru dataset, where color-palette references enable direct evaluation of color restoration, DIVER achieves at least a 4.9% reduction in GPMAE compared to existing methods. Beyond visual quality, DIVER also improves robotic perception by enhancing ORB-based keypoint repeatability and matching performance, confirming its robustness across diverse underwater environments.", "link": "http://arxiv.org/abs/2601.22878v1", "date": "2026-01-30", "relevancy": 2.6903, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5374}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Development%20of%20Domain-Invariant%20Visual%20Enhancement%20and%20Restoration%20%28DIVER%29%20Approach%20for%20Underwater%20Images&body=Title%3A%20Development%20of%20Domain-Invariant%20Visual%20Enhancement%20and%20Restoration%20%28DIVER%29%20Approach%20for%20Underwater%20Images%0AAuthor%3A%20Rajini%20Makam%20and%20Sharanya%20Patil%20and%20Dhatri%20Shankari%20T%20M%20and%20Suresh%20Sundaram%20and%20Narasimhan%20Sundararajan%0AAbstract%3A%20Underwater%20images%20suffer%20severe%20degradation%20due%20to%20wavelength-dependent%20attenuation%2C%20scattering%2C%20and%20illumination%20non-uniformity%20that%20vary%20across%20water%20types%20and%20depths.%20We%20propose%20an%20unsupervised%20Domain-Invariant%20Visual%20Enhancement%20and%20Restoration%20%28DIVER%29%20framework%20that%20integrates%20empirical%20correction%20with%20physics-guided%20modeling%20for%20robust%20underwater%20image%20enhancement.%20DIVER%20first%20applies%20either%20IlluminateNet%20for%20adaptive%20luminance%20enhancement%20or%20a%20Spectral%20Equalization%20Filter%20for%20spectral%20normalization.%20An%20Adaptive%20Optical%20Correction%20Module%20then%20refines%20hue%20and%20contrast%20using%20channel-adaptive%20filtering%2C%20while%20Hydro-OpticNet%20employs%20physics-constrained%20learning%20to%20compensate%20for%20backscatter%20and%20wavelength-dependent%20attenuation.%20The%20parameters%20of%20IlluminateNet%20and%20Hydro-OpticNet%20are%20optimized%20via%20unsupervised%20learning%20using%20a%20composite%20loss%20function.%20DIVER%20is%20evaluated%20on%20eight%20diverse%20datasets%20covering%20shallow%2C%20deep%2C%20and%20highly%20turbid%20environments%2C%20including%20both%20naturally%20low-light%20and%20artificially%20illuminated%20scenes%2C%20using%20reference%20and%20non-reference%20metrics.%20While%20state-of-the-art%20methods%20such%20as%20WaterNet%2C%20UDNet%2C%20and%20Phaseformer%20perform%20reasonably%20in%20shallow%20water%2C%20their%20performance%20degrades%20in%20deep%2C%20unevenly%20illuminated%2C%20or%20artificially%20lit%20conditions.%20In%20contrast%2C%20DIVER%20consistently%20achieves%20best%20or%20near-best%20performance%20across%20all%20datasets%2C%20demonstrating%20strong%20domain-invariant%20capability.%20DIVER%20yields%20at%20least%20a%209%25%20improvement%20over%20SOTA%20methods%20in%20UCIQE.%20On%20the%20low-light%20SeaThru%20dataset%2C%20where%20color-palette%20references%20enable%20direct%20evaluation%20of%20color%20restoration%2C%20DIVER%20achieves%20at%20least%20a%204.9%25%20reduction%20in%20GPMAE%20compared%20to%20existing%20methods.%20Beyond%20visual%20quality%2C%20DIVER%20also%20improves%20robotic%20perception%20by%20enhancing%20ORB-based%20keypoint%20repeatability%20and%20matching%20performance%2C%20confirming%20its%20robustness%20across%20diverse%20underwater%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevelopment%2520of%2520Domain-Invariant%2520Visual%2520Enhancement%2520and%2520Restoration%2520%2528DIVER%2529%2520Approach%2520for%2520Underwater%2520Images%26entry.906535625%3DRajini%2520Makam%2520and%2520Sharanya%2520Patil%2520and%2520Dhatri%2520Shankari%2520T%2520M%2520and%2520Suresh%2520Sundaram%2520and%2520Narasimhan%2520Sundararajan%26entry.1292438233%3DUnderwater%2520images%2520suffer%2520severe%2520degradation%2520due%2520to%2520wavelength-dependent%2520attenuation%252C%2520scattering%252C%2520and%2520illumination%2520non-uniformity%2520that%2520vary%2520across%2520water%2520types%2520and%2520depths.%2520We%2520propose%2520an%2520unsupervised%2520Domain-Invariant%2520Visual%2520Enhancement%2520and%2520Restoration%2520%2528DIVER%2529%2520framework%2520that%2520integrates%2520empirical%2520correction%2520with%2520physics-guided%2520modeling%2520for%2520robust%2520underwater%2520image%2520enhancement.%2520DIVER%2520first%2520applies%2520either%2520IlluminateNet%2520for%2520adaptive%2520luminance%2520enhancement%2520or%2520a%2520Spectral%2520Equalization%2520Filter%2520for%2520spectral%2520normalization.%2520An%2520Adaptive%2520Optical%2520Correction%2520Module%2520then%2520refines%2520hue%2520and%2520contrast%2520using%2520channel-adaptive%2520filtering%252C%2520while%2520Hydro-OpticNet%2520employs%2520physics-constrained%2520learning%2520to%2520compensate%2520for%2520backscatter%2520and%2520wavelength-dependent%2520attenuation.%2520The%2520parameters%2520of%2520IlluminateNet%2520and%2520Hydro-OpticNet%2520are%2520optimized%2520via%2520unsupervised%2520learning%2520using%2520a%2520composite%2520loss%2520function.%2520DIVER%2520is%2520evaluated%2520on%2520eight%2520diverse%2520datasets%2520covering%2520shallow%252C%2520deep%252C%2520and%2520highly%2520turbid%2520environments%252C%2520including%2520both%2520naturally%2520low-light%2520and%2520artificially%2520illuminated%2520scenes%252C%2520using%2520reference%2520and%2520non-reference%2520metrics.%2520While%2520state-of-the-art%2520methods%2520such%2520as%2520WaterNet%252C%2520UDNet%252C%2520and%2520Phaseformer%2520perform%2520reasonably%2520in%2520shallow%2520water%252C%2520their%2520performance%2520degrades%2520in%2520deep%252C%2520unevenly%2520illuminated%252C%2520or%2520artificially%2520lit%2520conditions.%2520In%2520contrast%252C%2520DIVER%2520consistently%2520achieves%2520best%2520or%2520near-best%2520performance%2520across%2520all%2520datasets%252C%2520demonstrating%2520strong%2520domain-invariant%2520capability.%2520DIVER%2520yields%2520at%2520least%2520a%25209%2525%2520improvement%2520over%2520SOTA%2520methods%2520in%2520UCIQE.%2520On%2520the%2520low-light%2520SeaThru%2520dataset%252C%2520where%2520color-palette%2520references%2520enable%2520direct%2520evaluation%2520of%2520color%2520restoration%252C%2520DIVER%2520achieves%2520at%2520least%2520a%25204.9%2525%2520reduction%2520in%2520GPMAE%2520compared%2520to%2520existing%2520methods.%2520Beyond%2520visual%2520quality%252C%2520DIVER%2520also%2520improves%2520robotic%2520perception%2520by%2520enhancing%2520ORB-based%2520keypoint%2520repeatability%2520and%2520matching%2520performance%252C%2520confirming%2520its%2520robustness%2520across%2520diverse%2520underwater%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Development%20of%20Domain-Invariant%20Visual%20Enhancement%20and%20Restoration%20%28DIVER%29%20Approach%20for%20Underwater%20Images&entry.906535625=Rajini%20Makam%20and%20Sharanya%20Patil%20and%20Dhatri%20Shankari%20T%20M%20and%20Suresh%20Sundaram%20and%20Narasimhan%20Sundararajan&entry.1292438233=Underwater%20images%20suffer%20severe%20degradation%20due%20to%20wavelength-dependent%20attenuation%2C%20scattering%2C%20and%20illumination%20non-uniformity%20that%20vary%20across%20water%20types%20and%20depths.%20We%20propose%20an%20unsupervised%20Domain-Invariant%20Visual%20Enhancement%20and%20Restoration%20%28DIVER%29%20framework%20that%20integrates%20empirical%20correction%20with%20physics-guided%20modeling%20for%20robust%20underwater%20image%20enhancement.%20DIVER%20first%20applies%20either%20IlluminateNet%20for%20adaptive%20luminance%20enhancement%20or%20a%20Spectral%20Equalization%20Filter%20for%20spectral%20normalization.%20An%20Adaptive%20Optical%20Correction%20Module%20then%20refines%20hue%20and%20contrast%20using%20channel-adaptive%20filtering%2C%20while%20Hydro-OpticNet%20employs%20physics-constrained%20learning%20to%20compensate%20for%20backscatter%20and%20wavelength-dependent%20attenuation.%20The%20parameters%20of%20IlluminateNet%20and%20Hydro-OpticNet%20are%20optimized%20via%20unsupervised%20learning%20using%20a%20composite%20loss%20function.%20DIVER%20is%20evaluated%20on%20eight%20diverse%20datasets%20covering%20shallow%2C%20deep%2C%20and%20highly%20turbid%20environments%2C%20including%20both%20naturally%20low-light%20and%20artificially%20illuminated%20scenes%2C%20using%20reference%20and%20non-reference%20metrics.%20While%20state-of-the-art%20methods%20such%20as%20WaterNet%2C%20UDNet%2C%20and%20Phaseformer%20perform%20reasonably%20in%20shallow%20water%2C%20their%20performance%20degrades%20in%20deep%2C%20unevenly%20illuminated%2C%20or%20artificially%20lit%20conditions.%20In%20contrast%2C%20DIVER%20consistently%20achieves%20best%20or%20near-best%20performance%20across%20all%20datasets%2C%20demonstrating%20strong%20domain-invariant%20capability.%20DIVER%20yields%20at%20least%20a%209%25%20improvement%20over%20SOTA%20methods%20in%20UCIQE.%20On%20the%20low-light%20SeaThru%20dataset%2C%20where%20color-palette%20references%20enable%20direct%20evaluation%20of%20color%20restoration%2C%20DIVER%20achieves%20at%20least%20a%204.9%25%20reduction%20in%20GPMAE%20compared%20to%20existing%20methods.%20Beyond%20visual%20quality%2C%20DIVER%20also%20improves%20robotic%20perception%20by%20enhancing%20ORB-based%20keypoint%20repeatability%20and%20matching%20performance%2C%20confirming%20its%20robustness%20across%20diverse%20underwater%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.22878v1&entry.124074799=Read"},
{"title": "Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval", "author": "Ilyass Moummad and Marius Miron and David Robinson and Kawtar Zaher and Herv\u00e9 Go\u00ebau and Olivier Pietquin and Pierre Bonnet and Emmanuel Chemla and Matthieu Geist and Alexis Joly", "abstract": "Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.", "link": "http://arxiv.org/abs/2601.22783v1", "date": "2026-01-30", "relevancy": 2.6744, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compact%20Hypercube%20Embeddings%20for%20Fast%20Text-based%20Wildlife%20Observation%20Retrieval&body=Title%3A%20Compact%20Hypercube%20Embeddings%20for%20Fast%20Text-based%20Wildlife%20Observation%20Retrieval%0AAuthor%3A%20Ilyass%20Moummad%20and%20Marius%20Miron%20and%20David%20Robinson%20and%20Kawtar%20Zaher%20and%20Herv%C3%A9%20Go%C3%ABau%20and%20Olivier%20Pietquin%20and%20Pierre%20Bonnet%20and%20Emmanuel%20Chemla%20and%20Matthieu%20Geist%20and%20Alexis%20Joly%0AAbstract%3A%20Large-scale%20biodiversity%20monitoring%20platforms%20increasingly%20rely%20on%20multimodal%20wildlife%20observations.%20While%20recent%20foundation%20models%20enable%20rich%20semantic%20representations%20across%20vision%2C%20audio%2C%20and%20language%2C%20retrieving%20relevant%20observations%20from%20massive%20archives%20remains%20challenging%20due%20to%20the%20computational%20cost%20of%20high-dimensional%20similarity%20search.%20In%20this%20work%2C%20we%20introduce%20compact%20hypercube%20embeddings%20for%20fast%20text-based%20wildlife%20observation%20retrieval%2C%20a%20framework%20that%20enables%20efficient%20text-based%20search%20over%20large-scale%20wildlife%20image%20and%20audio%20databases%20using%20compact%20binary%20representations.%20Building%20on%20the%20cross-view%20code%20alignment%20hashing%20framework%2C%20we%20extend%20lightweight%20hashing%20beyond%20a%20single-modality%20setup%20to%20align%20natural%20language%20descriptions%20with%20visual%20or%20acoustic%20observations%20in%20a%20shared%20Hamming%20space.%20Our%20approach%20leverages%20pretrained%20wildlife%20foundation%20models%2C%20including%20BioCLIP%20and%20BioLingual%2C%20and%20adapts%20them%20efficiently%20for%20hashing%20using%20parameter-efficient%20fine-tuning.%20We%20evaluate%20our%20method%20on%20large-scale%20benchmarks%2C%20including%20iNaturalist2024%20for%20text-to-image%20retrieval%20and%20iNatSounds2024%20for%20text-to-audio%20retrieval%2C%20as%20well%20as%20multiple%20soundscape%20datasets%20to%20assess%20robustness%20under%20domain%20shift.%20Results%20show%20that%20retrieval%20using%20discrete%20hypercube%20embeddings%20achieves%20competitive%2C%20and%20in%20several%20cases%20superior%2C%20performance%20compared%20to%20continuous%20embeddings%2C%20while%20drastically%20reducing%20memory%20and%20search%20cost.%20Moreover%2C%20we%20observe%20that%20the%20hashing%20objective%20consistently%20improves%20the%20underlying%20encoder%20representations%2C%20leading%20to%20stronger%20retrieval%20and%20zero-shot%20generalization.%20These%20results%20demonstrate%20that%20binary%2C%20language-based%20retrieval%20enables%20scalable%20and%20efficient%20search%20over%20large%20wildlife%20archives%20for%20biodiversity%20monitoring%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompact%2520Hypercube%2520Embeddings%2520for%2520Fast%2520Text-based%2520Wildlife%2520Observation%2520Retrieval%26entry.906535625%3DIlyass%2520Moummad%2520and%2520Marius%2520Miron%2520and%2520David%2520Robinson%2520and%2520Kawtar%2520Zaher%2520and%2520Herv%25C3%25A9%2520Go%25C3%25ABau%2520and%2520Olivier%2520Pietquin%2520and%2520Pierre%2520Bonnet%2520and%2520Emmanuel%2520Chemla%2520and%2520Matthieu%2520Geist%2520and%2520Alexis%2520Joly%26entry.1292438233%3DLarge-scale%2520biodiversity%2520monitoring%2520platforms%2520increasingly%2520rely%2520on%2520multimodal%2520wildlife%2520observations.%2520While%2520recent%2520foundation%2520models%2520enable%2520rich%2520semantic%2520representations%2520across%2520vision%252C%2520audio%252C%2520and%2520language%252C%2520retrieving%2520relevant%2520observations%2520from%2520massive%2520archives%2520remains%2520challenging%2520due%2520to%2520the%2520computational%2520cost%2520of%2520high-dimensional%2520similarity%2520search.%2520In%2520this%2520work%252C%2520we%2520introduce%2520compact%2520hypercube%2520embeddings%2520for%2520fast%2520text-based%2520wildlife%2520observation%2520retrieval%252C%2520a%2520framework%2520that%2520enables%2520efficient%2520text-based%2520search%2520over%2520large-scale%2520wildlife%2520image%2520and%2520audio%2520databases%2520using%2520compact%2520binary%2520representations.%2520Building%2520on%2520the%2520cross-view%2520code%2520alignment%2520hashing%2520framework%252C%2520we%2520extend%2520lightweight%2520hashing%2520beyond%2520a%2520single-modality%2520setup%2520to%2520align%2520natural%2520language%2520descriptions%2520with%2520visual%2520or%2520acoustic%2520observations%2520in%2520a%2520shared%2520Hamming%2520space.%2520Our%2520approach%2520leverages%2520pretrained%2520wildlife%2520foundation%2520models%252C%2520including%2520BioCLIP%2520and%2520BioLingual%252C%2520and%2520adapts%2520them%2520efficiently%2520for%2520hashing%2520using%2520parameter-efficient%2520fine-tuning.%2520We%2520evaluate%2520our%2520method%2520on%2520large-scale%2520benchmarks%252C%2520including%2520iNaturalist2024%2520for%2520text-to-image%2520retrieval%2520and%2520iNatSounds2024%2520for%2520text-to-audio%2520retrieval%252C%2520as%2520well%2520as%2520multiple%2520soundscape%2520datasets%2520to%2520assess%2520robustness%2520under%2520domain%2520shift.%2520Results%2520show%2520that%2520retrieval%2520using%2520discrete%2520hypercube%2520embeddings%2520achieves%2520competitive%252C%2520and%2520in%2520several%2520cases%2520superior%252C%2520performance%2520compared%2520to%2520continuous%2520embeddings%252C%2520while%2520drastically%2520reducing%2520memory%2520and%2520search%2520cost.%2520Moreover%252C%2520we%2520observe%2520that%2520the%2520hashing%2520objective%2520consistently%2520improves%2520the%2520underlying%2520encoder%2520representations%252C%2520leading%2520to%2520stronger%2520retrieval%2520and%2520zero-shot%2520generalization.%2520These%2520results%2520demonstrate%2520that%2520binary%252C%2520language-based%2520retrieval%2520enables%2520scalable%2520and%2520efficient%2520search%2520over%2520large%2520wildlife%2520archives%2520for%2520biodiversity%2520monitoring%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compact%20Hypercube%20Embeddings%20for%20Fast%20Text-based%20Wildlife%20Observation%20Retrieval&entry.906535625=Ilyass%20Moummad%20and%20Marius%20Miron%20and%20David%20Robinson%20and%20Kawtar%20Zaher%20and%20Herv%C3%A9%20Go%C3%ABau%20and%20Olivier%20Pietquin%20and%20Pierre%20Bonnet%20and%20Emmanuel%20Chemla%20and%20Matthieu%20Geist%20and%20Alexis%20Joly&entry.1292438233=Large-scale%20biodiversity%20monitoring%20platforms%20increasingly%20rely%20on%20multimodal%20wildlife%20observations.%20While%20recent%20foundation%20models%20enable%20rich%20semantic%20representations%20across%20vision%2C%20audio%2C%20and%20language%2C%20retrieving%20relevant%20observations%20from%20massive%20archives%20remains%20challenging%20due%20to%20the%20computational%20cost%20of%20high-dimensional%20similarity%20search.%20In%20this%20work%2C%20we%20introduce%20compact%20hypercube%20embeddings%20for%20fast%20text-based%20wildlife%20observation%20retrieval%2C%20a%20framework%20that%20enables%20efficient%20text-based%20search%20over%20large-scale%20wildlife%20image%20and%20audio%20databases%20using%20compact%20binary%20representations.%20Building%20on%20the%20cross-view%20code%20alignment%20hashing%20framework%2C%20we%20extend%20lightweight%20hashing%20beyond%20a%20single-modality%20setup%20to%20align%20natural%20language%20descriptions%20with%20visual%20or%20acoustic%20observations%20in%20a%20shared%20Hamming%20space.%20Our%20approach%20leverages%20pretrained%20wildlife%20foundation%20models%2C%20including%20BioCLIP%20and%20BioLingual%2C%20and%20adapts%20them%20efficiently%20for%20hashing%20using%20parameter-efficient%20fine-tuning.%20We%20evaluate%20our%20method%20on%20large-scale%20benchmarks%2C%20including%20iNaturalist2024%20for%20text-to-image%20retrieval%20and%20iNatSounds2024%20for%20text-to-audio%20retrieval%2C%20as%20well%20as%20multiple%20soundscape%20datasets%20to%20assess%20robustness%20under%20domain%20shift.%20Results%20show%20that%20retrieval%20using%20discrete%20hypercube%20embeddings%20achieves%20competitive%2C%20and%20in%20several%20cases%20superior%2C%20performance%20compared%20to%20continuous%20embeddings%2C%20while%20drastically%20reducing%20memory%20and%20search%20cost.%20Moreover%2C%20we%20observe%20that%20the%20hashing%20objective%20consistently%20improves%20the%20underlying%20encoder%20representations%2C%20leading%20to%20stronger%20retrieval%20and%20zero-shot%20generalization.%20These%20results%20demonstrate%20that%20binary%2C%20language-based%20retrieval%20enables%20scalable%20and%20efficient%20search%20over%20large%20wildlife%20archives%20for%20biodiversity%20monitoring%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.22783v1&entry.124074799=Read"},
{"title": "LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models", "author": "Alhassan Abdelhalim and Janick Edinger and S\u00f6ren Laue and Michaela Regneri", "abstract": "Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.\n  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.", "link": "http://arxiv.org/abs/2601.22928v1", "date": "2026-01-30", "relevancy": 2.6708, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5465}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Explain%27t%3A%20A%20Post-Mortem%20on%20Semantic%20Interpretability%20in%20Transformer%20Models&body=Title%3A%20LLMs%20Explain%27t%3A%20A%20Post-Mortem%20on%20Semantic%20Interpretability%20in%20Transformer%20Models%0AAuthor%3A%20Alhassan%20Abdelhalim%20and%20Janick%20Edinger%20and%20S%C3%B6ren%20Laue%20and%20Michaela%20Regneri%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20becoming%20increasingly%20popular%20in%20pervasive%20computing%20due%20to%20their%20versatility%20and%20strong%20performance.%20However%2C%20despite%20their%20ubiquitous%20use%2C%20the%20exact%20mechanisms%20underlying%20their%20outstanding%20performance%20remain%20unclear.%20Different%20methods%20for%20LLM%20explainability%20exist%2C%20and%20many%20are%2C%20as%20a%20method%2C%20not%20fully%20understood%20themselves.%20We%20started%20with%20the%20question%20of%20how%20linguistic%20abstraction%20emerges%20in%20LLMs%2C%20aiming%20to%20detect%20it%20across%20different%20LLM%20modules%20%28attention%20heads%20and%20input%20embeddings%29.%20For%20this%2C%20we%20used%20methods%20well-established%20in%20the%20literature%3A%20%281%29%20probing%20for%20token-level%20relational%20structures%2C%20and%20%282%29%20feature-mapping%20using%20embeddings%20as%20carriers%20of%20human-interpretable%20properties.%0A%20%20Both%20attempts%20failed%20for%20different%20methodological%20reasons%3A%20Attention-based%20explanations%20collapsed%20once%20we%20tested%20the%20core%20assumption%20that%20later-layer%20representations%20still%20correspond%20to%20tokens.%20Property-inference%20methods%20applied%20to%20embeddings%20also%20failed%20because%20their%20high%20predictive%20scores%20were%20driven%20by%20methodological%20artifacts%20and%20dataset%20structure%20rather%20than%20meaningful%20semantic%20knowledge.%20These%20failures%20matter%20because%20both%20techniques%20are%20widely%20treated%20as%20evidence%20for%20what%20LLMs%20supposedly%20understand%2C%20yet%20our%20results%20show%20such%20conclusions%20are%20unwarranted.%20These%20limitations%20are%20particularly%20relevant%20in%20pervasive%20and%20distributed%20computing%20settings%20where%20LLMs%20are%20deployed%20as%20system%20components%20and%20interpretability%20methods%20are%20relied%20upon%20for%20debugging%2C%20compression%2C%20and%20explaining%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Explain%2527t%253A%2520A%2520Post-Mortem%2520on%2520Semantic%2520Interpretability%2520in%2520Transformer%2520Models%26entry.906535625%3DAlhassan%2520Abdelhalim%2520and%2520Janick%2520Edinger%2520and%2520S%25C3%25B6ren%2520Laue%2520and%2520Michaela%2520Regneri%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520becoming%2520increasingly%2520popular%2520in%2520pervasive%2520computing%2520due%2520to%2520their%2520versatility%2520and%2520strong%2520performance.%2520However%252C%2520despite%2520their%2520ubiquitous%2520use%252C%2520the%2520exact%2520mechanisms%2520underlying%2520their%2520outstanding%2520performance%2520remain%2520unclear.%2520Different%2520methods%2520for%2520LLM%2520explainability%2520exist%252C%2520and%2520many%2520are%252C%2520as%2520a%2520method%252C%2520not%2520fully%2520understood%2520themselves.%2520We%2520started%2520with%2520the%2520question%2520of%2520how%2520linguistic%2520abstraction%2520emerges%2520in%2520LLMs%252C%2520aiming%2520to%2520detect%2520it%2520across%2520different%2520LLM%2520modules%2520%2528attention%2520heads%2520and%2520input%2520embeddings%2529.%2520For%2520this%252C%2520we%2520used%2520methods%2520well-established%2520in%2520the%2520literature%253A%2520%25281%2529%2520probing%2520for%2520token-level%2520relational%2520structures%252C%2520and%2520%25282%2529%2520feature-mapping%2520using%2520embeddings%2520as%2520carriers%2520of%2520human-interpretable%2520properties.%250A%2520%2520Both%2520attempts%2520failed%2520for%2520different%2520methodological%2520reasons%253A%2520Attention-based%2520explanations%2520collapsed%2520once%2520we%2520tested%2520the%2520core%2520assumption%2520that%2520later-layer%2520representations%2520still%2520correspond%2520to%2520tokens.%2520Property-inference%2520methods%2520applied%2520to%2520embeddings%2520also%2520failed%2520because%2520their%2520high%2520predictive%2520scores%2520were%2520driven%2520by%2520methodological%2520artifacts%2520and%2520dataset%2520structure%2520rather%2520than%2520meaningful%2520semantic%2520knowledge.%2520These%2520failures%2520matter%2520because%2520both%2520techniques%2520are%2520widely%2520treated%2520as%2520evidence%2520for%2520what%2520LLMs%2520supposedly%2520understand%252C%2520yet%2520our%2520results%2520show%2520such%2520conclusions%2520are%2520unwarranted.%2520These%2520limitations%2520are%2520particularly%2520relevant%2520in%2520pervasive%2520and%2520distributed%2520computing%2520settings%2520where%2520LLMs%2520are%2520deployed%2520as%2520system%2520components%2520and%2520interpretability%2520methods%2520are%2520relied%2520upon%2520for%2520debugging%252C%2520compression%252C%2520and%2520explaining%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Explain%27t%3A%20A%20Post-Mortem%20on%20Semantic%20Interpretability%20in%20Transformer%20Models&entry.906535625=Alhassan%20Abdelhalim%20and%20Janick%20Edinger%20and%20S%C3%B6ren%20Laue%20and%20Michaela%20Regneri&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20becoming%20increasingly%20popular%20in%20pervasive%20computing%20due%20to%20their%20versatility%20and%20strong%20performance.%20However%2C%20despite%20their%20ubiquitous%20use%2C%20the%20exact%20mechanisms%20underlying%20their%20outstanding%20performance%20remain%20unclear.%20Different%20methods%20for%20LLM%20explainability%20exist%2C%20and%20many%20are%2C%20as%20a%20method%2C%20not%20fully%20understood%20themselves.%20We%20started%20with%20the%20question%20of%20how%20linguistic%20abstraction%20emerges%20in%20LLMs%2C%20aiming%20to%20detect%20it%20across%20different%20LLM%20modules%20%28attention%20heads%20and%20input%20embeddings%29.%20For%20this%2C%20we%20used%20methods%20well-established%20in%20the%20literature%3A%20%281%29%20probing%20for%20token-level%20relational%20structures%2C%20and%20%282%29%20feature-mapping%20using%20embeddings%20as%20carriers%20of%20human-interpretable%20properties.%0A%20%20Both%20attempts%20failed%20for%20different%20methodological%20reasons%3A%20Attention-based%20explanations%20collapsed%20once%20we%20tested%20the%20core%20assumption%20that%20later-layer%20representations%20still%20correspond%20to%20tokens.%20Property-inference%20methods%20applied%20to%20embeddings%20also%20failed%20because%20their%20high%20predictive%20scores%20were%20driven%20by%20methodological%20artifacts%20and%20dataset%20structure%20rather%20than%20meaningful%20semantic%20knowledge.%20These%20failures%20matter%20because%20both%20techniques%20are%20widely%20treated%20as%20evidence%20for%20what%20LLMs%20supposedly%20understand%2C%20yet%20our%20results%20show%20such%20conclusions%20are%20unwarranted.%20These%20limitations%20are%20particularly%20relevant%20in%20pervasive%20and%20distributed%20computing%20settings%20where%20LLMs%20are%20deployed%20as%20system%20components%20and%20interpretability%20methods%20are%20relied%20upon%20for%20debugging%2C%20compression%2C%20and%20explaining%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.22928v1&entry.124074799=Read"},
{"title": "Quasiparticle Interference Kernel Extraction with Variational Autoencoders via Latent Alignment", "author": "Yingshuai Ji and Haomin Zhuang and Matthew Toole and James McKenzie and Xiaolong Liu and Xiangliang Zhang", "abstract": "Quasiparticle interference (QPI) imaging is a powerful tool for probing electronic structures in quantum materials, but extracting the single-scatterer QPI pattern (i.e., the kernel) from a multi-scatterer image remains a fundamentally ill-posed inverse problem, because many different kernels can combine to produce almost the same observed image, and noise or overlaps further obscure the true signal. Existing solutions to this extraction problem rely on manually zooming into small local regions with isolated single-scatterers. This is infeasible for real cases where scattering conditions are too complex. In this work, we propose the first AI-based framework for QPI kernel extraction, which models the space of physically valid kernels and uses this knowledge to guide the inverse mapping. We introduce a two-step learning strategy that decouples kernel representation learning from observation-to-kernel inference. In the first step, we train a variational autoencoder to learn a compact latent space of scattering kernels. In the second step, we align the latent representation of QPI observations with those of the pre-learned kernels using a dedicated encoder. This design enables the model to infer kernels robustly under complex, entangled scattering conditions. We construct a diverse and physically realistic QPI dataset comprising 100 unique kernels and evaluate our method against a direct one-step baseline. Experimental results demonstrate that our approach achieves significantly higher extraction accuracy, improved generalization to unseen kernels. To further validate its effectiveness, we also apply the method to real QPI data from Ag and FeSe samples, where it reliably extracts meaningful kernels under complex scattering conditions.", "link": "http://arxiv.org/abs/2506.05325v2", "date": "2026-01-30", "relevancy": 2.6626, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5749}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5167}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quasiparticle%20Interference%20Kernel%20Extraction%20with%20Variational%20Autoencoders%20via%20Latent%20Alignment&body=Title%3A%20Quasiparticle%20Interference%20Kernel%20Extraction%20with%20Variational%20Autoencoders%20via%20Latent%20Alignment%0AAuthor%3A%20Yingshuai%20Ji%20and%20Haomin%20Zhuang%20and%20Matthew%20Toole%20and%20James%20McKenzie%20and%20Xiaolong%20Liu%20and%20Xiangliang%20Zhang%0AAbstract%3A%20Quasiparticle%20interference%20%28QPI%29%20imaging%20is%20a%20powerful%20tool%20for%20probing%20electronic%20structures%20in%20quantum%20materials%2C%20but%20extracting%20the%20single-scatterer%20QPI%20pattern%20%28i.e.%2C%20the%20kernel%29%20from%20a%20multi-scatterer%20image%20remains%20a%20fundamentally%20ill-posed%20inverse%20problem%2C%20because%20many%20different%20kernels%20can%20combine%20to%20produce%20almost%20the%20same%20observed%20image%2C%20and%20noise%20or%20overlaps%20further%20obscure%20the%20true%20signal.%20Existing%20solutions%20to%20this%20extraction%20problem%20rely%20on%20manually%20zooming%20into%20small%20local%20regions%20with%20isolated%20single-scatterers.%20This%20is%20infeasible%20for%20real%20cases%20where%20scattering%20conditions%20are%20too%20complex.%20In%20this%20work%2C%20we%20propose%20the%20first%20AI-based%20framework%20for%20QPI%20kernel%20extraction%2C%20which%20models%20the%20space%20of%20physically%20valid%20kernels%20and%20uses%20this%20knowledge%20to%20guide%20the%20inverse%20mapping.%20We%20introduce%20a%20two-step%20learning%20strategy%20that%20decouples%20kernel%20representation%20learning%20from%20observation-to-kernel%20inference.%20In%20the%20first%20step%2C%20we%20train%20a%20variational%20autoencoder%20to%20learn%20a%20compact%20latent%20space%20of%20scattering%20kernels.%20In%20the%20second%20step%2C%20we%20align%20the%20latent%20representation%20of%20QPI%20observations%20with%20those%20of%20the%20pre-learned%20kernels%20using%20a%20dedicated%20encoder.%20This%20design%20enables%20the%20model%20to%20infer%20kernels%20robustly%20under%20complex%2C%20entangled%20scattering%20conditions.%20We%20construct%20a%20diverse%20and%20physically%20realistic%20QPI%20dataset%20comprising%20100%20unique%20kernels%20and%20evaluate%20our%20method%20against%20a%20direct%20one-step%20baseline.%20Experimental%20results%20demonstrate%20that%20our%20approach%20achieves%20significantly%20higher%20extraction%20accuracy%2C%20improved%20generalization%20to%20unseen%20kernels.%20To%20further%20validate%20its%20effectiveness%2C%20we%20also%20apply%20the%20method%20to%20real%20QPI%20data%20from%20Ag%20and%20FeSe%20samples%2C%20where%20it%20reliably%20extracts%20meaningful%20kernels%20under%20complex%20scattering%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuasiparticle%2520Interference%2520Kernel%2520Extraction%2520with%2520Variational%2520Autoencoders%2520via%2520Latent%2520Alignment%26entry.906535625%3DYingshuai%2520Ji%2520and%2520Haomin%2520Zhuang%2520and%2520Matthew%2520Toole%2520and%2520James%2520McKenzie%2520and%2520Xiaolong%2520Liu%2520and%2520Xiangliang%2520Zhang%26entry.1292438233%3DQuasiparticle%2520interference%2520%2528QPI%2529%2520imaging%2520is%2520a%2520powerful%2520tool%2520for%2520probing%2520electronic%2520structures%2520in%2520quantum%2520materials%252C%2520but%2520extracting%2520the%2520single-scatterer%2520QPI%2520pattern%2520%2528i.e.%252C%2520the%2520kernel%2529%2520from%2520a%2520multi-scatterer%2520image%2520remains%2520a%2520fundamentally%2520ill-posed%2520inverse%2520problem%252C%2520because%2520many%2520different%2520kernels%2520can%2520combine%2520to%2520produce%2520almost%2520the%2520same%2520observed%2520image%252C%2520and%2520noise%2520or%2520overlaps%2520further%2520obscure%2520the%2520true%2520signal.%2520Existing%2520solutions%2520to%2520this%2520extraction%2520problem%2520rely%2520on%2520manually%2520zooming%2520into%2520small%2520local%2520regions%2520with%2520isolated%2520single-scatterers.%2520This%2520is%2520infeasible%2520for%2520real%2520cases%2520where%2520scattering%2520conditions%2520are%2520too%2520complex.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520first%2520AI-based%2520framework%2520for%2520QPI%2520kernel%2520extraction%252C%2520which%2520models%2520the%2520space%2520of%2520physically%2520valid%2520kernels%2520and%2520uses%2520this%2520knowledge%2520to%2520guide%2520the%2520inverse%2520mapping.%2520We%2520introduce%2520a%2520two-step%2520learning%2520strategy%2520that%2520decouples%2520kernel%2520representation%2520learning%2520from%2520observation-to-kernel%2520inference.%2520In%2520the%2520first%2520step%252C%2520we%2520train%2520a%2520variational%2520autoencoder%2520to%2520learn%2520a%2520compact%2520latent%2520space%2520of%2520scattering%2520kernels.%2520In%2520the%2520second%2520step%252C%2520we%2520align%2520the%2520latent%2520representation%2520of%2520QPI%2520observations%2520with%2520those%2520of%2520the%2520pre-learned%2520kernels%2520using%2520a%2520dedicated%2520encoder.%2520This%2520design%2520enables%2520the%2520model%2520to%2520infer%2520kernels%2520robustly%2520under%2520complex%252C%2520entangled%2520scattering%2520conditions.%2520We%2520construct%2520a%2520diverse%2520and%2520physically%2520realistic%2520QPI%2520dataset%2520comprising%2520100%2520unique%2520kernels%2520and%2520evaluate%2520our%2520method%2520against%2520a%2520direct%2520one-step%2520baseline.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520significantly%2520higher%2520extraction%2520accuracy%252C%2520improved%2520generalization%2520to%2520unseen%2520kernels.%2520To%2520further%2520validate%2520its%2520effectiveness%252C%2520we%2520also%2520apply%2520the%2520method%2520to%2520real%2520QPI%2520data%2520from%2520Ag%2520and%2520FeSe%2520samples%252C%2520where%2520it%2520reliably%2520extracts%2520meaningful%2520kernels%2520under%2520complex%2520scattering%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quasiparticle%20Interference%20Kernel%20Extraction%20with%20Variational%20Autoencoders%20via%20Latent%20Alignment&entry.906535625=Yingshuai%20Ji%20and%20Haomin%20Zhuang%20and%20Matthew%20Toole%20and%20James%20McKenzie%20and%20Xiaolong%20Liu%20and%20Xiangliang%20Zhang&entry.1292438233=Quasiparticle%20interference%20%28QPI%29%20imaging%20is%20a%20powerful%20tool%20for%20probing%20electronic%20structures%20in%20quantum%20materials%2C%20but%20extracting%20the%20single-scatterer%20QPI%20pattern%20%28i.e.%2C%20the%20kernel%29%20from%20a%20multi-scatterer%20image%20remains%20a%20fundamentally%20ill-posed%20inverse%20problem%2C%20because%20many%20different%20kernels%20can%20combine%20to%20produce%20almost%20the%20same%20observed%20image%2C%20and%20noise%20or%20overlaps%20further%20obscure%20the%20true%20signal.%20Existing%20solutions%20to%20this%20extraction%20problem%20rely%20on%20manually%20zooming%20into%20small%20local%20regions%20with%20isolated%20single-scatterers.%20This%20is%20infeasible%20for%20real%20cases%20where%20scattering%20conditions%20are%20too%20complex.%20In%20this%20work%2C%20we%20propose%20the%20first%20AI-based%20framework%20for%20QPI%20kernel%20extraction%2C%20which%20models%20the%20space%20of%20physically%20valid%20kernels%20and%20uses%20this%20knowledge%20to%20guide%20the%20inverse%20mapping.%20We%20introduce%20a%20two-step%20learning%20strategy%20that%20decouples%20kernel%20representation%20learning%20from%20observation-to-kernel%20inference.%20In%20the%20first%20step%2C%20we%20train%20a%20variational%20autoencoder%20to%20learn%20a%20compact%20latent%20space%20of%20scattering%20kernels.%20In%20the%20second%20step%2C%20we%20align%20the%20latent%20representation%20of%20QPI%20observations%20with%20those%20of%20the%20pre-learned%20kernels%20using%20a%20dedicated%20encoder.%20This%20design%20enables%20the%20model%20to%20infer%20kernels%20robustly%20under%20complex%2C%20entangled%20scattering%20conditions.%20We%20construct%20a%20diverse%20and%20physically%20realistic%20QPI%20dataset%20comprising%20100%20unique%20kernels%20and%20evaluate%20our%20method%20against%20a%20direct%20one-step%20baseline.%20Experimental%20results%20demonstrate%20that%20our%20approach%20achieves%20significantly%20higher%20extraction%20accuracy%2C%20improved%20generalization%20to%20unseen%20kernels.%20To%20further%20validate%20its%20effectiveness%2C%20we%20also%20apply%20the%20method%20to%20real%20QPI%20data%20from%20Ag%20and%20FeSe%20samples%2C%20where%20it%20reliably%20extracts%20meaningful%20kernels%20under%20complex%20scattering%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2506.05325v2&entry.124074799=Read"},
{"title": "Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA", "author": "Zhan Fa and Yue Duan and Jian Zhang and Lei Qi and Wanqi Yang and Yinghuan Shi", "abstract": "Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.", "link": "http://arxiv.org/abs/2601.22828v1", "date": "2026-01-30", "relevancy": 2.6591, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20and%20Composing%3A%20Towards%20Efficient%20Vision-Language%20Continual%20Learning%20via%20Rank-1%20Expert%20Pool%20in%20a%20Single%20LoRA&body=Title%3A%20Decomposing%20and%20Composing%3A%20Towards%20Efficient%20Vision-Language%20Continual%20Learning%20via%20Rank-1%20Expert%20Pool%20in%20a%20Single%20LoRA%0AAuthor%3A%20Zhan%20Fa%20and%20Yue%20Duan%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Wanqi%20Yang%20and%20Yinghuan%20Shi%0AAbstract%3A%20Continual%20learning%20%28CL%29%20in%20vision-language%20models%20%28VLMs%29%20faces%20significant%20challenges%20in%20improving%20task%20adaptation%20and%20avoiding%20catastrophic%20forgetting.%20Existing%20methods%20usually%20have%20heavy%20inference%20burden%20or%20rely%20on%20external%20knowledge%2C%20while%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20shown%20potential%20in%20reducing%20these%20issues%20by%20enabling%20parameter-efficient%20tuning.%20However%2C%20considering%20directly%20using%20LoRA%20to%20alleviate%20the%20catastrophic%20forgetting%20problem%20is%20non-trivial%2C%20we%20introduce%20a%20novel%20framework%20that%20restructures%20a%20single%20LoRA%20module%20as%20a%20decomposable%20Rank-1%20Expert%20Pool.%20Our%20method%20learns%20to%20dynamically%20compose%20a%20sparse%2C%20task-specific%20update%20by%20selecting%20from%20this%20expert%20pool%2C%20guided%20by%20the%20semantics%20of%20the%20%5BCLS%5D%20token.%20In%20addition%2C%20we%20propose%20an%20Activation-Guided%20Orthogonal%20%28AGO%29%20loss%20that%20orthogonalizes%20critical%20parts%20of%20LoRA%20weights%20across%20tasks.%20This%20sparse%20composition%20and%20orthogonalization%20enable%20fewer%20parameter%20updates%2C%20resulting%20in%20domain-aware%20learning%20while%20minimizing%20inter-task%20interference%20and%20maintaining%20downstream%20task%20performance.%20Extensive%20experiments%20across%20multiple%20settings%20demonstrate%20state-of-the-art%20results%20in%20all%20metrics%2C%20surpassing%20zero-shot%20upper%20bounds%20in%20generalization.%20Notably%2C%20it%20reduces%20trainable%20parameters%20by%2096.7%25%20compared%20to%20the%20baseline%20method%2C%20eliminating%20reliance%20on%20external%20datasets%20or%20task-ID%20discriminators.%20The%20merged%20LoRAs%20retain%20less%20weights%20and%20incur%20no%20inference%20latency%2C%20making%20our%20method%20computationally%20lightweight.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520and%2520Composing%253A%2520Towards%2520Efficient%2520Vision-Language%2520Continual%2520Learning%2520via%2520Rank-1%2520Expert%2520Pool%2520in%2520a%2520Single%2520LoRA%26entry.906535625%3DZhan%2520Fa%2520and%2520Yue%2520Duan%2520and%2520Jian%2520Zhang%2520and%2520Lei%2520Qi%2520and%2520Wanqi%2520Yang%2520and%2520Yinghuan%2520Shi%26entry.1292438233%3DContinual%2520learning%2520%2528CL%2529%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520faces%2520significant%2520challenges%2520in%2520improving%2520task%2520adaptation%2520and%2520avoiding%2520catastrophic%2520forgetting.%2520Existing%2520methods%2520usually%2520have%2520heavy%2520inference%2520burden%2520or%2520rely%2520on%2520external%2520knowledge%252C%2520while%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520shown%2520potential%2520in%2520reducing%2520these%2520issues%2520by%2520enabling%2520parameter-efficient%2520tuning.%2520However%252C%2520considering%2520directly%2520using%2520LoRA%2520to%2520alleviate%2520the%2520catastrophic%2520forgetting%2520problem%2520is%2520non-trivial%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%2520restructures%2520a%2520single%2520LoRA%2520module%2520as%2520a%2520decomposable%2520Rank-1%2520Expert%2520Pool.%2520Our%2520method%2520learns%2520to%2520dynamically%2520compose%2520a%2520sparse%252C%2520task-specific%2520update%2520by%2520selecting%2520from%2520this%2520expert%2520pool%252C%2520guided%2520by%2520the%2520semantics%2520of%2520the%2520%255BCLS%255D%2520token.%2520In%2520addition%252C%2520we%2520propose%2520an%2520Activation-Guided%2520Orthogonal%2520%2528AGO%2529%2520loss%2520that%2520orthogonalizes%2520critical%2520parts%2520of%2520LoRA%2520weights%2520across%2520tasks.%2520This%2520sparse%2520composition%2520and%2520orthogonalization%2520enable%2520fewer%2520parameter%2520updates%252C%2520resulting%2520in%2520domain-aware%2520learning%2520while%2520minimizing%2520inter-task%2520interference%2520and%2520maintaining%2520downstream%2520task%2520performance.%2520Extensive%2520experiments%2520across%2520multiple%2520settings%2520demonstrate%2520state-of-the-art%2520results%2520in%2520all%2520metrics%252C%2520surpassing%2520zero-shot%2520upper%2520bounds%2520in%2520generalization.%2520Notably%252C%2520it%2520reduces%2520trainable%2520parameters%2520by%252096.7%2525%2520compared%2520to%2520the%2520baseline%2520method%252C%2520eliminating%2520reliance%2520on%2520external%2520datasets%2520or%2520task-ID%2520discriminators.%2520The%2520merged%2520LoRAs%2520retain%2520less%2520weights%2520and%2520incur%2520no%2520inference%2520latency%252C%2520making%2520our%2520method%2520computationally%2520lightweight.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20and%20Composing%3A%20Towards%20Efficient%20Vision-Language%20Continual%20Learning%20via%20Rank-1%20Expert%20Pool%20in%20a%20Single%20LoRA&entry.906535625=Zhan%20Fa%20and%20Yue%20Duan%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Wanqi%20Yang%20and%20Yinghuan%20Shi&entry.1292438233=Continual%20learning%20%28CL%29%20in%20vision-language%20models%20%28VLMs%29%20faces%20significant%20challenges%20in%20improving%20task%20adaptation%20and%20avoiding%20catastrophic%20forgetting.%20Existing%20methods%20usually%20have%20heavy%20inference%20burden%20or%20rely%20on%20external%20knowledge%2C%20while%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20shown%20potential%20in%20reducing%20these%20issues%20by%20enabling%20parameter-efficient%20tuning.%20However%2C%20considering%20directly%20using%20LoRA%20to%20alleviate%20the%20catastrophic%20forgetting%20problem%20is%20non-trivial%2C%20we%20introduce%20a%20novel%20framework%20that%20restructures%20a%20single%20LoRA%20module%20as%20a%20decomposable%20Rank-1%20Expert%20Pool.%20Our%20method%20learns%20to%20dynamically%20compose%20a%20sparse%2C%20task-specific%20update%20by%20selecting%20from%20this%20expert%20pool%2C%20guided%20by%20the%20semantics%20of%20the%20%5BCLS%5D%20token.%20In%20addition%2C%20we%20propose%20an%20Activation-Guided%20Orthogonal%20%28AGO%29%20loss%20that%20orthogonalizes%20critical%20parts%20of%20LoRA%20weights%20across%20tasks.%20This%20sparse%20composition%20and%20orthogonalization%20enable%20fewer%20parameter%20updates%2C%20resulting%20in%20domain-aware%20learning%20while%20minimizing%20inter-task%20interference%20and%20maintaining%20downstream%20task%20performance.%20Extensive%20experiments%20across%20multiple%20settings%20demonstrate%20state-of-the-art%20results%20in%20all%20metrics%2C%20surpassing%20zero-shot%20upper%20bounds%20in%20generalization.%20Notably%2C%20it%20reduces%20trainable%20parameters%20by%2096.7%25%20compared%20to%20the%20baseline%20method%2C%20eliminating%20reliance%20on%20external%20datasets%20or%20task-ID%20discriminators.%20The%20merged%20LoRAs%20retain%20less%20weights%20and%20incur%20no%20inference%20latency%2C%20making%20our%20method%20computationally%20lightweight.&entry.1838667208=http%3A//arxiv.org/abs/2601.22828v1&entry.124074799=Read"},
{"title": "PENEX: AdaBoost-Inspired Neural Network Regularization", "author": "Klaus-Rudolf Kladny and Bernhard Sch\u00f6lkopf and Michael Muehlebach", "abstract": "AdaBoost sequentially fits so-called weak learners to minimize an exponential loss, which penalizes misclassified data points more severely than other loss functions like cross-entropy. Paradoxically, AdaBoost generalizes well in practice as the number of weak learners grows. In the present work, we introduce Penalized Exponential Loss (PENEX), a new formulation of the multi-class exponential loss that is theoretically grounded and, in contrast to the existing formulation, amenable to optimization via first-order methods, making it a practical objective for training neural networks. We demonstrate that PENEX effectively increases margins of data points, which can be translated into a generalization bound. Empirically, across computer vision and language tasks, PENEX improves neural network generalization in low-data regimes, often matching or outperforming established regularizers at comparable computational cost. Our results highlight the potential of the exponential loss beyond its application in AdaBoost.", "link": "http://arxiv.org/abs/2510.02107v3", "date": "2026-01-30", "relevancy": 2.6582, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4874}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PENEX%3A%20AdaBoost-Inspired%20Neural%20Network%20Regularization&body=Title%3A%20PENEX%3A%20AdaBoost-Inspired%20Neural%20Network%20Regularization%0AAuthor%3A%20Klaus-Rudolf%20Kladny%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Michael%20Muehlebach%0AAbstract%3A%20AdaBoost%20sequentially%20fits%20so-called%20weak%20learners%20to%20minimize%20an%20exponential%20loss%2C%20which%20penalizes%20misclassified%20data%20points%20more%20severely%20than%20other%20loss%20functions%20like%20cross-entropy.%20Paradoxically%2C%20AdaBoost%20generalizes%20well%20in%20practice%20as%20the%20number%20of%20weak%20learners%20grows.%20In%20the%20present%20work%2C%20we%20introduce%20Penalized%20Exponential%20Loss%20%28PENEX%29%2C%20a%20new%20formulation%20of%20the%20multi-class%20exponential%20loss%20that%20is%20theoretically%20grounded%20and%2C%20in%20contrast%20to%20the%20existing%20formulation%2C%20amenable%20to%20optimization%20via%20first-order%20methods%2C%20making%20it%20a%20practical%20objective%20for%20training%20neural%20networks.%20We%20demonstrate%20that%20PENEX%20effectively%20increases%20margins%20of%20data%20points%2C%20which%20can%20be%20translated%20into%20a%20generalization%20bound.%20Empirically%2C%20across%20computer%20vision%20and%20language%20tasks%2C%20PENEX%20improves%20neural%20network%20generalization%20in%20low-data%20regimes%2C%20often%20matching%20or%20outperforming%20established%20regularizers%20at%20comparable%20computational%20cost.%20Our%20results%20highlight%20the%20potential%20of%20the%20exponential%20loss%20beyond%20its%20application%20in%20AdaBoost.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02107v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPENEX%253A%2520AdaBoost-Inspired%2520Neural%2520Network%2520Regularization%26entry.906535625%3DKlaus-Rudolf%2520Kladny%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Michael%2520Muehlebach%26entry.1292438233%3DAdaBoost%2520sequentially%2520fits%2520so-called%2520weak%2520learners%2520to%2520minimize%2520an%2520exponential%2520loss%252C%2520which%2520penalizes%2520misclassified%2520data%2520points%2520more%2520severely%2520than%2520other%2520loss%2520functions%2520like%2520cross-entropy.%2520Paradoxically%252C%2520AdaBoost%2520generalizes%2520well%2520in%2520practice%2520as%2520the%2520number%2520of%2520weak%2520learners%2520grows.%2520In%2520the%2520present%2520work%252C%2520we%2520introduce%2520Penalized%2520Exponential%2520Loss%2520%2528PENEX%2529%252C%2520a%2520new%2520formulation%2520of%2520the%2520multi-class%2520exponential%2520loss%2520that%2520is%2520theoretically%2520grounded%2520and%252C%2520in%2520contrast%2520to%2520the%2520existing%2520formulation%252C%2520amenable%2520to%2520optimization%2520via%2520first-order%2520methods%252C%2520making%2520it%2520a%2520practical%2520objective%2520for%2520training%2520neural%2520networks.%2520We%2520demonstrate%2520that%2520PENEX%2520effectively%2520increases%2520margins%2520of%2520data%2520points%252C%2520which%2520can%2520be%2520translated%2520into%2520a%2520generalization%2520bound.%2520Empirically%252C%2520across%2520computer%2520vision%2520and%2520language%2520tasks%252C%2520PENEX%2520improves%2520neural%2520network%2520generalization%2520in%2520low-data%2520regimes%252C%2520often%2520matching%2520or%2520outperforming%2520established%2520regularizers%2520at%2520comparable%2520computational%2520cost.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520the%2520exponential%2520loss%2520beyond%2520its%2520application%2520in%2520AdaBoost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02107v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PENEX%3A%20AdaBoost-Inspired%20Neural%20Network%20Regularization&entry.906535625=Klaus-Rudolf%20Kladny%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Michael%20Muehlebach&entry.1292438233=AdaBoost%20sequentially%20fits%20so-called%20weak%20learners%20to%20minimize%20an%20exponential%20loss%2C%20which%20penalizes%20misclassified%20data%20points%20more%20severely%20than%20other%20loss%20functions%20like%20cross-entropy.%20Paradoxically%2C%20AdaBoost%20generalizes%20well%20in%20practice%20as%20the%20number%20of%20weak%20learners%20grows.%20In%20the%20present%20work%2C%20we%20introduce%20Penalized%20Exponential%20Loss%20%28PENEX%29%2C%20a%20new%20formulation%20of%20the%20multi-class%20exponential%20loss%20that%20is%20theoretically%20grounded%20and%2C%20in%20contrast%20to%20the%20existing%20formulation%2C%20amenable%20to%20optimization%20via%20first-order%20methods%2C%20making%20it%20a%20practical%20objective%20for%20training%20neural%20networks.%20We%20demonstrate%20that%20PENEX%20effectively%20increases%20margins%20of%20data%20points%2C%20which%20can%20be%20translated%20into%20a%20generalization%20bound.%20Empirically%2C%20across%20computer%20vision%20and%20language%20tasks%2C%20PENEX%20improves%20neural%20network%20generalization%20in%20low-data%20regimes%2C%20often%20matching%20or%20outperforming%20established%20regularizers%20at%20comparable%20computational%20cost.%20Our%20results%20highlight%20the%20potential%20of%20the%20exponential%20loss%20beyond%20its%20application%20in%20AdaBoost.&entry.1838667208=http%3A//arxiv.org/abs/2510.02107v3&entry.124074799=Read"},
{"title": "SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis", "author": "Mo Wang and Junfeng Xia and Wenhao Ye and Enyu Liu and Kaining Peng and Jianfeng Feng and Quanying Liu and Hongkai Wen", "abstract": "Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.", "link": "http://arxiv.org/abs/2512.21881v3", "date": "2026-01-30", "relevancy": 2.6545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLIM-Brain%3A%20A%20Data-%20and%20Training-Efficient%20Foundation%20Model%20for%20fMRI%20Data%20Analysis&body=Title%3A%20SLIM-Brain%3A%20A%20Data-%20and%20Training-Efficient%20Foundation%20Model%20for%20fMRI%20Data%20Analysis%0AAuthor%3A%20Mo%20Wang%20and%20Junfeng%20Xia%20and%20Wenhao%20Ye%20and%20Enyu%20Liu%20and%20Kaining%20Peng%20and%20Jianfeng%20Feng%20and%20Quanying%20Liu%20and%20Hongkai%20Wen%0AAbstract%3A%20Foundation%20models%20are%20emerging%20as%20a%20powerful%20paradigm%20for%20fMRI%20analysis%2C%20but%20current%20approaches%20face%20a%20dual%20bottleneck%20of%20data-%20and%20training-efficiency.%20Atlas-based%20methods%20aggregate%20voxel%20signals%20into%20fixed%20regions%20of%20interest%2C%20reducing%20data%20dimensionality%20but%20discarding%20fine-grained%20spatial%20details%2C%20and%20requiring%20extremely%20large%20cohorts%20to%20train%20effectively%20as%20general-purpose%20foundation%20models.%20Atlas-free%20methods%2C%20on%20the%20other%20hand%2C%20operate%20directly%20on%20voxel-level%20information%20-%20preserving%20spatial%20fidelity%20but%20are%20prohibitively%20memory-%20and%20compute-intensive%2C%20making%20large-scale%20pre-training%20infeasible.%20We%20introduce%20SLIM-Brain%20%28Sample-efficient%2C%20Low-memory%20fMRI%20Foundation%20Model%20for%20Human%20Brain%29%2C%20a%20new%20atlas-free%20foundation%20model%20that%20simultaneously%20improves%20both%20data-%20and%20training-efficiency.%20SLIM-Brain%20adopts%20a%20two-stage%20adaptive%20design%3A%20%28i%29%20a%20lightweight%20temporal%20extractor%20captures%20global%20context%20across%20full%20sequences%20and%20ranks%20data%20windows%20by%20saliency%2C%20and%20%28ii%29%20a%204D%20hierarchical%20encoder%20%28Hiera-JEPA%29%20learns%20fine-grained%20voxel-level%20representations%20only%20from%20the%20top-%24k%24%20selected%20windows%2C%20while%20deleting%20about%2070%25%20masked%20patches.%20Extensive%20experiments%20across%20seven%20public%20benchmarks%20show%20that%20SLIM-Brain%20establishes%20new%20state-of-the-art%20performance%20on%20diverse%20tasks%2C%20while%20requiring%20only%204%20thousand%20pre-training%20sessions%20and%20approximately%2030%25%20of%20GPU%20memory%20comparing%20to%20traditional%20voxel-level%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21881v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLIM-Brain%253A%2520A%2520Data-%2520and%2520Training-Efficient%2520Foundation%2520Model%2520for%2520fMRI%2520Data%2520Analysis%26entry.906535625%3DMo%2520Wang%2520and%2520Junfeng%2520Xia%2520and%2520Wenhao%2520Ye%2520and%2520Enyu%2520Liu%2520and%2520Kaining%2520Peng%2520and%2520Jianfeng%2520Feng%2520and%2520Quanying%2520Liu%2520and%2520Hongkai%2520Wen%26entry.1292438233%3DFoundation%2520models%2520are%2520emerging%2520as%2520a%2520powerful%2520paradigm%2520for%2520fMRI%2520analysis%252C%2520but%2520current%2520approaches%2520face%2520a%2520dual%2520bottleneck%2520of%2520data-%2520and%2520training-efficiency.%2520Atlas-based%2520methods%2520aggregate%2520voxel%2520signals%2520into%2520fixed%2520regions%2520of%2520interest%252C%2520reducing%2520data%2520dimensionality%2520but%2520discarding%2520fine-grained%2520spatial%2520details%252C%2520and%2520requiring%2520extremely%2520large%2520cohorts%2520to%2520train%2520effectively%2520as%2520general-purpose%2520foundation%2520models.%2520Atlas-free%2520methods%252C%2520on%2520the%2520other%2520hand%252C%2520operate%2520directly%2520on%2520voxel-level%2520information%2520-%2520preserving%2520spatial%2520fidelity%2520but%2520are%2520prohibitively%2520memory-%2520and%2520compute-intensive%252C%2520making%2520large-scale%2520pre-training%2520infeasible.%2520We%2520introduce%2520SLIM-Brain%2520%2528Sample-efficient%252C%2520Low-memory%2520fMRI%2520Foundation%2520Model%2520for%2520Human%2520Brain%2529%252C%2520a%2520new%2520atlas-free%2520foundation%2520model%2520that%2520simultaneously%2520improves%2520both%2520data-%2520and%2520training-efficiency.%2520SLIM-Brain%2520adopts%2520a%2520two-stage%2520adaptive%2520design%253A%2520%2528i%2529%2520a%2520lightweight%2520temporal%2520extractor%2520captures%2520global%2520context%2520across%2520full%2520sequences%2520and%2520ranks%2520data%2520windows%2520by%2520saliency%252C%2520and%2520%2528ii%2529%2520a%25204D%2520hierarchical%2520encoder%2520%2528Hiera-JEPA%2529%2520learns%2520fine-grained%2520voxel-level%2520representations%2520only%2520from%2520the%2520top-%2524k%2524%2520selected%2520windows%252C%2520while%2520deleting%2520about%252070%2525%2520masked%2520patches.%2520Extensive%2520experiments%2520across%2520seven%2520public%2520benchmarks%2520show%2520that%2520SLIM-Brain%2520establishes%2520new%2520state-of-the-art%2520performance%2520on%2520diverse%2520tasks%252C%2520while%2520requiring%2520only%25204%2520thousand%2520pre-training%2520sessions%2520and%2520approximately%252030%2525%2520of%2520GPU%2520memory%2520comparing%2520to%2520traditional%2520voxel-level%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21881v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLIM-Brain%3A%20A%20Data-%20and%20Training-Efficient%20Foundation%20Model%20for%20fMRI%20Data%20Analysis&entry.906535625=Mo%20Wang%20and%20Junfeng%20Xia%20and%20Wenhao%20Ye%20and%20Enyu%20Liu%20and%20Kaining%20Peng%20and%20Jianfeng%20Feng%20and%20Quanying%20Liu%20and%20Hongkai%20Wen&entry.1292438233=Foundation%20models%20are%20emerging%20as%20a%20powerful%20paradigm%20for%20fMRI%20analysis%2C%20but%20current%20approaches%20face%20a%20dual%20bottleneck%20of%20data-%20and%20training-efficiency.%20Atlas-based%20methods%20aggregate%20voxel%20signals%20into%20fixed%20regions%20of%20interest%2C%20reducing%20data%20dimensionality%20but%20discarding%20fine-grained%20spatial%20details%2C%20and%20requiring%20extremely%20large%20cohorts%20to%20train%20effectively%20as%20general-purpose%20foundation%20models.%20Atlas-free%20methods%2C%20on%20the%20other%20hand%2C%20operate%20directly%20on%20voxel-level%20information%20-%20preserving%20spatial%20fidelity%20but%20are%20prohibitively%20memory-%20and%20compute-intensive%2C%20making%20large-scale%20pre-training%20infeasible.%20We%20introduce%20SLIM-Brain%20%28Sample-efficient%2C%20Low-memory%20fMRI%20Foundation%20Model%20for%20Human%20Brain%29%2C%20a%20new%20atlas-free%20foundation%20model%20that%20simultaneously%20improves%20both%20data-%20and%20training-efficiency.%20SLIM-Brain%20adopts%20a%20two-stage%20adaptive%20design%3A%20%28i%29%20a%20lightweight%20temporal%20extractor%20captures%20global%20context%20across%20full%20sequences%20and%20ranks%20data%20windows%20by%20saliency%2C%20and%20%28ii%29%20a%204D%20hierarchical%20encoder%20%28Hiera-JEPA%29%20learns%20fine-grained%20voxel-level%20representations%20only%20from%20the%20top-%24k%24%20selected%20windows%2C%20while%20deleting%20about%2070%25%20masked%20patches.%20Extensive%20experiments%20across%20seven%20public%20benchmarks%20show%20that%20SLIM-Brain%20establishes%20new%20state-of-the-art%20performance%20on%20diverse%20tasks%2C%20while%20requiring%20only%204%20thousand%20pre-training%20sessions%20and%20approximately%2030%25%20of%20GPU%20memory%20comparing%20to%20traditional%20voxel-level%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.21881v3&entry.124074799=Read"},
{"title": "From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics", "author": "Bowen Cao and Dongdong Zhang and Yixia Li and Junpeng Liu and Shijue Huang and Chufan Shi and Hongyuan Lu and Yaokang Wu and Guanhua Chen and Wai Lam and Furu Wei", "abstract": "Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.", "link": "http://arxiv.org/abs/2601.23048v1", "date": "2026-01-30", "relevancy": 2.6101, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Abstract%20to%20Contextual%3A%20What%20LLMs%20Still%20Cannot%20Do%20in%20Mathematics&body=Title%3A%20From%20Abstract%20to%20Contextual%3A%20What%20LLMs%20Still%20Cannot%20Do%20in%20Mathematics%0AAuthor%3A%20Bowen%20Cao%20and%20Dongdong%20Zhang%20and%20Yixia%20Li%20and%20Junpeng%20Liu%20and%20Shijue%20Huang%20and%20Chufan%20Shi%20and%20Hongyuan%20Lu%20and%20Yaokang%20Wu%20and%20Guanhua%20Chen%20and%20Wai%20Lam%20and%20Furu%20Wei%0AAbstract%3A%20Large%20language%20models%20now%20solve%20many%20benchmark%20math%20problems%20at%20near-expert%20levels%2C%20yet%20this%20progress%20has%20not%20fully%20translated%20into%20reliable%20performance%20in%20real-world%20applications.%20We%20study%20this%20gap%20through%20contextual%20mathematical%20reasoning%2C%20where%20the%20mathematical%20core%20must%20be%20formulated%20from%20descriptive%20scenarios.%20We%20introduce%20ContextMATH%2C%20a%20benchmark%20that%20repurposes%20AIME%20and%20MATH-500%20problems%20into%20two%20contextual%20settings%3A%20Scenario%20Grounding%20%28SG%29%2C%20which%20embeds%20abstract%20problems%20into%20realistic%20narratives%20without%20increasing%20reasoning%20complexity%2C%20and%20Complexity%20Scaling%20%28CS%29%2C%20which%20transforms%20explicit%20conditions%20into%20sub-problems%20to%20capture%20how%20constraints%20often%20appear%20in%20practice.%20Evaluating%2061%20proprietary%20and%20open-source%20models%2C%20we%20observe%20sharp%20drops%3A%20on%20average%2C%20open-source%20models%20decline%20by%2013%20and%2034%20points%20on%20SG%20and%20CS%2C%20while%20proprietary%20models%20drop%20by%2013%20and%2020.%20Error%20analysis%20shows%20that%20errors%20are%20dominated%20by%20incorrect%20problem%20formulation%2C%20with%20formulation%20accuracy%20declining%20as%20original%20problem%20difficulty%20increases.%20Correct%20formulation%20emerges%20as%20a%20prerequisite%20for%20success%2C%20and%20its%20sufficiency%20improves%20with%20model%20scale%2C%20indicating%20that%20larger%20models%20advance%20in%20both%20understanding%20and%20reasoning.%20Nevertheless%2C%20formulation%20and%20reasoning%20remain%20two%20complementary%20bottlenecks%20that%20limit%20contextual%20mathematical%20problem%20solving.%20Finally%2C%20we%20find%20that%20fine-tuning%20with%20scenario%20data%20improves%20performance%2C%20whereas%20formulation-only%20training%20is%20ineffective.%20However%2C%20performance%20gaps%20are%20only%20partially%20alleviated%2C%20highlighting%20contextual%20mathematical%20reasoning%20as%20a%20central%20unsolved%20challenge%20for%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Abstract%2520to%2520Contextual%253A%2520What%2520LLMs%2520Still%2520Cannot%2520Do%2520in%2520Mathematics%26entry.906535625%3DBowen%2520Cao%2520and%2520Dongdong%2520Zhang%2520and%2520Yixia%2520Li%2520and%2520Junpeng%2520Liu%2520and%2520Shijue%2520Huang%2520and%2520Chufan%2520Shi%2520and%2520Hongyuan%2520Lu%2520and%2520Yaokang%2520Wu%2520and%2520Guanhua%2520Chen%2520and%2520Wai%2520Lam%2520and%2520Furu%2520Wei%26entry.1292438233%3DLarge%2520language%2520models%2520now%2520solve%2520many%2520benchmark%2520math%2520problems%2520at%2520near-expert%2520levels%252C%2520yet%2520this%2520progress%2520has%2520not%2520fully%2520translated%2520into%2520reliable%2520performance%2520in%2520real-world%2520applications.%2520We%2520study%2520this%2520gap%2520through%2520contextual%2520mathematical%2520reasoning%252C%2520where%2520the%2520mathematical%2520core%2520must%2520be%2520formulated%2520from%2520descriptive%2520scenarios.%2520We%2520introduce%2520ContextMATH%252C%2520a%2520benchmark%2520that%2520repurposes%2520AIME%2520and%2520MATH-500%2520problems%2520into%2520two%2520contextual%2520settings%253A%2520Scenario%2520Grounding%2520%2528SG%2529%252C%2520which%2520embeds%2520abstract%2520problems%2520into%2520realistic%2520narratives%2520without%2520increasing%2520reasoning%2520complexity%252C%2520and%2520Complexity%2520Scaling%2520%2528CS%2529%252C%2520which%2520transforms%2520explicit%2520conditions%2520into%2520sub-problems%2520to%2520capture%2520how%2520constraints%2520often%2520appear%2520in%2520practice.%2520Evaluating%252061%2520proprietary%2520and%2520open-source%2520models%252C%2520we%2520observe%2520sharp%2520drops%253A%2520on%2520average%252C%2520open-source%2520models%2520decline%2520by%252013%2520and%252034%2520points%2520on%2520SG%2520and%2520CS%252C%2520while%2520proprietary%2520models%2520drop%2520by%252013%2520and%252020.%2520Error%2520analysis%2520shows%2520that%2520errors%2520are%2520dominated%2520by%2520incorrect%2520problem%2520formulation%252C%2520with%2520formulation%2520accuracy%2520declining%2520as%2520original%2520problem%2520difficulty%2520increases.%2520Correct%2520formulation%2520emerges%2520as%2520a%2520prerequisite%2520for%2520success%252C%2520and%2520its%2520sufficiency%2520improves%2520with%2520model%2520scale%252C%2520indicating%2520that%2520larger%2520models%2520advance%2520in%2520both%2520understanding%2520and%2520reasoning.%2520Nevertheless%252C%2520formulation%2520and%2520reasoning%2520remain%2520two%2520complementary%2520bottlenecks%2520that%2520limit%2520contextual%2520mathematical%2520problem%2520solving.%2520Finally%252C%2520we%2520find%2520that%2520fine-tuning%2520with%2520scenario%2520data%2520improves%2520performance%252C%2520whereas%2520formulation-only%2520training%2520is%2520ineffective.%2520However%252C%2520performance%2520gaps%2520are%2520only%2520partially%2520alleviated%252C%2520highlighting%2520contextual%2520mathematical%2520reasoning%2520as%2520a%2520central%2520unsolved%2520challenge%2520for%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Abstract%20to%20Contextual%3A%20What%20LLMs%20Still%20Cannot%20Do%20in%20Mathematics&entry.906535625=Bowen%20Cao%20and%20Dongdong%20Zhang%20and%20Yixia%20Li%20and%20Junpeng%20Liu%20and%20Shijue%20Huang%20and%20Chufan%20Shi%20and%20Hongyuan%20Lu%20and%20Yaokang%20Wu%20and%20Guanhua%20Chen%20and%20Wai%20Lam%20and%20Furu%20Wei&entry.1292438233=Large%20language%20models%20now%20solve%20many%20benchmark%20math%20problems%20at%20near-expert%20levels%2C%20yet%20this%20progress%20has%20not%20fully%20translated%20into%20reliable%20performance%20in%20real-world%20applications.%20We%20study%20this%20gap%20through%20contextual%20mathematical%20reasoning%2C%20where%20the%20mathematical%20core%20must%20be%20formulated%20from%20descriptive%20scenarios.%20We%20introduce%20ContextMATH%2C%20a%20benchmark%20that%20repurposes%20AIME%20and%20MATH-500%20problems%20into%20two%20contextual%20settings%3A%20Scenario%20Grounding%20%28SG%29%2C%20which%20embeds%20abstract%20problems%20into%20realistic%20narratives%20without%20increasing%20reasoning%20complexity%2C%20and%20Complexity%20Scaling%20%28CS%29%2C%20which%20transforms%20explicit%20conditions%20into%20sub-problems%20to%20capture%20how%20constraints%20often%20appear%20in%20practice.%20Evaluating%2061%20proprietary%20and%20open-source%20models%2C%20we%20observe%20sharp%20drops%3A%20on%20average%2C%20open-source%20models%20decline%20by%2013%20and%2034%20points%20on%20SG%20and%20CS%2C%20while%20proprietary%20models%20drop%20by%2013%20and%2020.%20Error%20analysis%20shows%20that%20errors%20are%20dominated%20by%20incorrect%20problem%20formulation%2C%20with%20formulation%20accuracy%20declining%20as%20original%20problem%20difficulty%20increases.%20Correct%20formulation%20emerges%20as%20a%20prerequisite%20for%20success%2C%20and%20its%20sufficiency%20improves%20with%20model%20scale%2C%20indicating%20that%20larger%20models%20advance%20in%20both%20understanding%20and%20reasoning.%20Nevertheless%2C%20formulation%20and%20reasoning%20remain%20two%20complementary%20bottlenecks%20that%20limit%20contextual%20mathematical%20problem%20solving.%20Finally%2C%20we%20find%20that%20fine-tuning%20with%20scenario%20data%20improves%20performance%2C%20whereas%20formulation-only%20training%20is%20ineffective.%20However%2C%20performance%20gaps%20are%20only%20partially%20alleviated%2C%20highlighting%20contextual%20mathematical%20reasoning%20as%20a%20central%20unsolved%20challenge%20for%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.23048v1&entry.124074799=Read"},
{"title": "Mem-T: Densifying Rewards for Long-Horizon Memory Agents", "author": "Yanwei Yue and Guibin Zhang and Boci Peng and Xuanbo Fan and Jiaxin Guo and Qiankun Li and Yan Zhang", "abstract": "Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\\sim24.45\\%$ relative to GAM without sacrificing performance.", "link": "http://arxiv.org/abs/2601.23014v1", "date": "2026-01-30", "relevancy": 2.5818, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5271}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mem-T%3A%20Densifying%20Rewards%20for%20Long-Horizon%20Memory%20Agents&body=Title%3A%20Mem-T%3A%20Densifying%20Rewards%20for%20Long-Horizon%20Memory%20Agents%0AAuthor%3A%20Yanwei%20Yue%20and%20Guibin%20Zhang%20and%20Boci%20Peng%20and%20Xuanbo%20Fan%20and%20Jiaxin%20Guo%20and%20Qiankun%20Li%20and%20Yan%20Zhang%0AAbstract%3A%20Memory%20agents%2C%20which%20depart%20from%20predefined%20memory-processing%20pipelines%20by%20endogenously%20managing%20the%20processing%2C%20storage%2C%20and%20retrieval%20of%20memories%2C%20have%20garnered%20increasing%20attention%20for%20their%20autonomy%20and%20adaptability.%20However%2C%20existing%20training%20paradigms%20remain%20constrained%3A%20agents%20often%20traverse%20long-horizon%20sequences%20of%20memory%20operations%20before%20receiving%20sparse%20and%20delayed%20rewards%2C%20which%20hinders%20truly%20end-to-end%20optimization%20of%20memory%20management%20policies.%20To%20address%20this%20limitation%2C%20we%20introduce%20Mem-T%2C%20an%20autonomous%20memory%20agent%20that%20interfaces%20with%20a%20lightweight%20hierarchical%20memory%20database%20to%20perform%20dynamic%20updates%20and%20multi-turn%20retrieval%20over%20streaming%20inputs.%20To%20effectively%20train%20long-horizon%20memory%20management%20capabilities%2C%20we%20further%20propose%20MoT-GRPO%2C%20a%20tree-guided%20reinforcement%20learning%20framework%20that%20transforms%20sparse%20terminal%20feedback%20into%20dense%2C%20step-wise%20supervision%20via%20memory%20operation%20tree%20backpropagation%20and%20hindsight%20credit%20assignment%2C%20thereby%20enabling%20the%20joint%20optimization%20of%20memory%20construction%20and%20retrieval.%20Extensive%20experiments%20demonstrate%20that%20Mem-T%20is%20%281%29%20high-performing%2C%20surpassing%20frameworks%20such%20as%20A-Mem%20and%20Mem0%20by%20up%20to%20%2414.92%5C%25%24%2C%20and%20%282%29%20economical%2C%20operating%20on%20a%20favorable%20accuracy-efficiency%20Pareto%20frontier%20and%20reducing%20inference%20tokens%20per%20query%20by%20%24%5Csim24.45%5C%25%24%20relative%20to%20GAM%20without%20sacrificing%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMem-T%253A%2520Densifying%2520Rewards%2520for%2520Long-Horizon%2520Memory%2520Agents%26entry.906535625%3DYanwei%2520Yue%2520and%2520Guibin%2520Zhang%2520and%2520Boci%2520Peng%2520and%2520Xuanbo%2520Fan%2520and%2520Jiaxin%2520Guo%2520and%2520Qiankun%2520Li%2520and%2520Yan%2520Zhang%26entry.1292438233%3DMemory%2520agents%252C%2520which%2520depart%2520from%2520predefined%2520memory-processing%2520pipelines%2520by%2520endogenously%2520managing%2520the%2520processing%252C%2520storage%252C%2520and%2520retrieval%2520of%2520memories%252C%2520have%2520garnered%2520increasing%2520attention%2520for%2520their%2520autonomy%2520and%2520adaptability.%2520However%252C%2520existing%2520training%2520paradigms%2520remain%2520constrained%253A%2520agents%2520often%2520traverse%2520long-horizon%2520sequences%2520of%2520memory%2520operations%2520before%2520receiving%2520sparse%2520and%2520delayed%2520rewards%252C%2520which%2520hinders%2520truly%2520end-to-end%2520optimization%2520of%2520memory%2520management%2520policies.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520Mem-T%252C%2520an%2520autonomous%2520memory%2520agent%2520that%2520interfaces%2520with%2520a%2520lightweight%2520hierarchical%2520memory%2520database%2520to%2520perform%2520dynamic%2520updates%2520and%2520multi-turn%2520retrieval%2520over%2520streaming%2520inputs.%2520To%2520effectively%2520train%2520long-horizon%2520memory%2520management%2520capabilities%252C%2520we%2520further%2520propose%2520MoT-GRPO%252C%2520a%2520tree-guided%2520reinforcement%2520learning%2520framework%2520that%2520transforms%2520sparse%2520terminal%2520feedback%2520into%2520dense%252C%2520step-wise%2520supervision%2520via%2520memory%2520operation%2520tree%2520backpropagation%2520and%2520hindsight%2520credit%2520assignment%252C%2520thereby%2520enabling%2520the%2520joint%2520optimization%2520of%2520memory%2520construction%2520and%2520retrieval.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Mem-T%2520is%2520%25281%2529%2520high-performing%252C%2520surpassing%2520frameworks%2520such%2520as%2520A-Mem%2520and%2520Mem0%2520by%2520up%2520to%2520%252414.92%255C%2525%2524%252C%2520and%2520%25282%2529%2520economical%252C%2520operating%2520on%2520a%2520favorable%2520accuracy-efficiency%2520Pareto%2520frontier%2520and%2520reducing%2520inference%2520tokens%2520per%2520query%2520by%2520%2524%255Csim24.45%255C%2525%2524%2520relative%2520to%2520GAM%2520without%2520sacrificing%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mem-T%3A%20Densifying%20Rewards%20for%20Long-Horizon%20Memory%20Agents&entry.906535625=Yanwei%20Yue%20and%20Guibin%20Zhang%20and%20Boci%20Peng%20and%20Xuanbo%20Fan%20and%20Jiaxin%20Guo%20and%20Qiankun%20Li%20and%20Yan%20Zhang&entry.1292438233=Memory%20agents%2C%20which%20depart%20from%20predefined%20memory-processing%20pipelines%20by%20endogenously%20managing%20the%20processing%2C%20storage%2C%20and%20retrieval%20of%20memories%2C%20have%20garnered%20increasing%20attention%20for%20their%20autonomy%20and%20adaptability.%20However%2C%20existing%20training%20paradigms%20remain%20constrained%3A%20agents%20often%20traverse%20long-horizon%20sequences%20of%20memory%20operations%20before%20receiving%20sparse%20and%20delayed%20rewards%2C%20which%20hinders%20truly%20end-to-end%20optimization%20of%20memory%20management%20policies.%20To%20address%20this%20limitation%2C%20we%20introduce%20Mem-T%2C%20an%20autonomous%20memory%20agent%20that%20interfaces%20with%20a%20lightweight%20hierarchical%20memory%20database%20to%20perform%20dynamic%20updates%20and%20multi-turn%20retrieval%20over%20streaming%20inputs.%20To%20effectively%20train%20long-horizon%20memory%20management%20capabilities%2C%20we%20further%20propose%20MoT-GRPO%2C%20a%20tree-guided%20reinforcement%20learning%20framework%20that%20transforms%20sparse%20terminal%20feedback%20into%20dense%2C%20step-wise%20supervision%20via%20memory%20operation%20tree%20backpropagation%20and%20hindsight%20credit%20assignment%2C%20thereby%20enabling%20the%20joint%20optimization%20of%20memory%20construction%20and%20retrieval.%20Extensive%20experiments%20demonstrate%20that%20Mem-T%20is%20%281%29%20high-performing%2C%20surpassing%20frameworks%20such%20as%20A-Mem%20and%20Mem0%20by%20up%20to%20%2414.92%5C%25%24%2C%20and%20%282%29%20economical%2C%20operating%20on%20a%20favorable%20accuracy-efficiency%20Pareto%20frontier%20and%20reducing%20inference%20tokens%20per%20query%20by%20%24%5Csim24.45%5C%25%24%20relative%20to%20GAM%20without%20sacrificing%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.23014v1&entry.124074799=Read"},
{"title": "FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images", "author": "Haiyang Wu and Weiliang Mu and Jipeng Zhang and Zhong Dandan and Zhuofei Du and Haifeng Li and Tao Chao", "abstract": "Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.", "link": "http://arxiv.org/abs/2601.22809v1", "date": "2026-01-30", "relevancy": 2.5804, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FarmMind%3A%20Reasoning-Query-Driven%20Dynamic%20Segmentation%20for%20Farmland%20Remote%20Sensing%20Images&body=Title%3A%20FarmMind%3A%20Reasoning-Query-Driven%20Dynamic%20Segmentation%20for%20Farmland%20Remote%20Sensing%20Images%0AAuthor%3A%20Haiyang%20Wu%20and%20Weiliang%20Mu%20and%20Jipeng%20Zhang%20and%20Zhong%20Dandan%20and%20Zhuofei%20Du%20and%20Haifeng%20Li%20and%20Tao%20Chao%0AAbstract%3A%20Existing%20methods%20for%20farmland%20remote%20sensing%20image%20%28FRSI%29%20segmentation%20generally%20follow%20a%20static%20segmentation%20paradigm%2C%20where%20analysis%20relies%20solely%20on%20the%20limited%20information%20contained%20within%20a%20single%20input%20patch.%20Consequently%2C%20their%20reasoning%20capability%20is%20limited%20when%20dealing%20with%20complex%20scenes%20characterized%20by%20ambiguity%20and%20visual%20uncertainty.%20In%20contrast%2C%20human%20experts%2C%20when%20interpreting%20remote%20sensing%20images%20in%20such%20ambiguous%20cases%2C%20tend%20to%20actively%20query%20auxiliary%20images%20%28such%20as%20higher-resolution%2C%20larger-scale%2C%20or%20temporally%20adjacent%20data%29%20to%20conduct%20cross-verification%20and%20achieve%20more%20comprehensive%20reasoning.%20Inspired%20by%20this%2C%20we%20propose%20a%20reasoning-query-driven%20dynamic%20segmentation%20framework%20for%20FRSIs%2C%20named%20FarmMind.%20This%20framework%20breaks%20through%20the%20limitations%20of%20the%20static%20segmentation%20paradigm%20by%20introducing%20a%20reasoning-query%20mechanism%2C%20which%20dynamically%20and%20on-demand%20queries%20external%20auxiliary%20images%20to%20compensate%20for%20the%20insufficient%20information%20in%20a%20single%20input%20image.%20Unlike%20direct%20queries%2C%20this%20mechanism%20simulates%20the%20thinking%20process%20of%20human%20experts%20when%20faced%20with%20segmentation%20ambiguity%3A%20it%20first%20analyzes%20the%20root%20causes%20of%20segmentation%20ambiguities%20through%20reasoning%2C%20and%20then%20determines%20what%20type%20of%20auxiliary%20image%20needs%20to%20be%20queried%20based%20on%20this%20analysis.%20Extensive%20experiments%20demonstrate%20that%20FarmMind%20achieves%20superior%20segmentation%20performance%20and%20stronger%20generalization%20ability%20compared%20with%20existing%20methods.%20The%20source%20code%20and%20dataset%20used%20in%20this%20work%20are%20publicly%20available%20at%3A%20https%3A//github.com/WithoutOcean/FarmMind.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFarmMind%253A%2520Reasoning-Query-Driven%2520Dynamic%2520Segmentation%2520for%2520Farmland%2520Remote%2520Sensing%2520Images%26entry.906535625%3DHaiyang%2520Wu%2520and%2520Weiliang%2520Mu%2520and%2520Jipeng%2520Zhang%2520and%2520Zhong%2520Dandan%2520and%2520Zhuofei%2520Du%2520and%2520Haifeng%2520Li%2520and%2520Tao%2520Chao%26entry.1292438233%3DExisting%2520methods%2520for%2520farmland%2520remote%2520sensing%2520image%2520%2528FRSI%2529%2520segmentation%2520generally%2520follow%2520a%2520static%2520segmentation%2520paradigm%252C%2520where%2520analysis%2520relies%2520solely%2520on%2520the%2520limited%2520information%2520contained%2520within%2520a%2520single%2520input%2520patch.%2520Consequently%252C%2520their%2520reasoning%2520capability%2520is%2520limited%2520when%2520dealing%2520with%2520complex%2520scenes%2520characterized%2520by%2520ambiguity%2520and%2520visual%2520uncertainty.%2520In%2520contrast%252C%2520human%2520experts%252C%2520when%2520interpreting%2520remote%2520sensing%2520images%2520in%2520such%2520ambiguous%2520cases%252C%2520tend%2520to%2520actively%2520query%2520auxiliary%2520images%2520%2528such%2520as%2520higher-resolution%252C%2520larger-scale%252C%2520or%2520temporally%2520adjacent%2520data%2529%2520to%2520conduct%2520cross-verification%2520and%2520achieve%2520more%2520comprehensive%2520reasoning.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520reasoning-query-driven%2520dynamic%2520segmentation%2520framework%2520for%2520FRSIs%252C%2520named%2520FarmMind.%2520This%2520framework%2520breaks%2520through%2520the%2520limitations%2520of%2520the%2520static%2520segmentation%2520paradigm%2520by%2520introducing%2520a%2520reasoning-query%2520mechanism%252C%2520which%2520dynamically%2520and%2520on-demand%2520queries%2520external%2520auxiliary%2520images%2520to%2520compensate%2520for%2520the%2520insufficient%2520information%2520in%2520a%2520single%2520input%2520image.%2520Unlike%2520direct%2520queries%252C%2520this%2520mechanism%2520simulates%2520the%2520thinking%2520process%2520of%2520human%2520experts%2520when%2520faced%2520with%2520segmentation%2520ambiguity%253A%2520it%2520first%2520analyzes%2520the%2520root%2520causes%2520of%2520segmentation%2520ambiguities%2520through%2520reasoning%252C%2520and%2520then%2520determines%2520what%2520type%2520of%2520auxiliary%2520image%2520needs%2520to%2520be%2520queried%2520based%2520on%2520this%2520analysis.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FarmMind%2520achieves%2520superior%2520segmentation%2520performance%2520and%2520stronger%2520generalization%2520ability%2520compared%2520with%2520existing%2520methods.%2520The%2520source%2520code%2520and%2520dataset%2520used%2520in%2520this%2520work%2520are%2520publicly%2520available%2520at%253A%2520https%253A//github.com/WithoutOcean/FarmMind.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FarmMind%3A%20Reasoning-Query-Driven%20Dynamic%20Segmentation%20for%20Farmland%20Remote%20Sensing%20Images&entry.906535625=Haiyang%20Wu%20and%20Weiliang%20Mu%20and%20Jipeng%20Zhang%20and%20Zhong%20Dandan%20and%20Zhuofei%20Du%20and%20Haifeng%20Li%20and%20Tao%20Chao&entry.1292438233=Existing%20methods%20for%20farmland%20remote%20sensing%20image%20%28FRSI%29%20segmentation%20generally%20follow%20a%20static%20segmentation%20paradigm%2C%20where%20analysis%20relies%20solely%20on%20the%20limited%20information%20contained%20within%20a%20single%20input%20patch.%20Consequently%2C%20their%20reasoning%20capability%20is%20limited%20when%20dealing%20with%20complex%20scenes%20characterized%20by%20ambiguity%20and%20visual%20uncertainty.%20In%20contrast%2C%20human%20experts%2C%20when%20interpreting%20remote%20sensing%20images%20in%20such%20ambiguous%20cases%2C%20tend%20to%20actively%20query%20auxiliary%20images%20%28such%20as%20higher-resolution%2C%20larger-scale%2C%20or%20temporally%20adjacent%20data%29%20to%20conduct%20cross-verification%20and%20achieve%20more%20comprehensive%20reasoning.%20Inspired%20by%20this%2C%20we%20propose%20a%20reasoning-query-driven%20dynamic%20segmentation%20framework%20for%20FRSIs%2C%20named%20FarmMind.%20This%20framework%20breaks%20through%20the%20limitations%20of%20the%20static%20segmentation%20paradigm%20by%20introducing%20a%20reasoning-query%20mechanism%2C%20which%20dynamically%20and%20on-demand%20queries%20external%20auxiliary%20images%20to%20compensate%20for%20the%20insufficient%20information%20in%20a%20single%20input%20image.%20Unlike%20direct%20queries%2C%20this%20mechanism%20simulates%20the%20thinking%20process%20of%20human%20experts%20when%20faced%20with%20segmentation%20ambiguity%3A%20it%20first%20analyzes%20the%20root%20causes%20of%20segmentation%20ambiguities%20through%20reasoning%2C%20and%20then%20determines%20what%20type%20of%20auxiliary%20image%20needs%20to%20be%20queried%20based%20on%20this%20analysis.%20Extensive%20experiments%20demonstrate%20that%20FarmMind%20achieves%20superior%20segmentation%20performance%20and%20stronger%20generalization%20ability%20compared%20with%20existing%20methods.%20The%20source%20code%20and%20dataset%20used%20in%20this%20work%20are%20publicly%20available%20at%3A%20https%3A//github.com/WithoutOcean/FarmMind.&entry.1838667208=http%3A//arxiv.org/abs/2601.22809v1&entry.124074799=Read"},
{"title": "Semantic Leakage from Image Embeddings", "author": "Yiyi Chen and Qiongkai Xu and Desmond Eliott and Qiongxiu Li and Johannes Bjerva", "abstract": "Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1", "link": "http://arxiv.org/abs/2601.22929v1", "date": "2026-01-30", "relevancy": 2.5765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Leakage%20from%20Image%20Embeddings&body=Title%3A%20Semantic%20Leakage%20from%20Image%20Embeddings%0AAuthor%3A%20Yiyi%20Chen%20and%20Qiongkai%20Xu%20and%20Desmond%20Eliott%20and%20Qiongxiu%20Li%20and%20Johannes%20Bjerva%0AAbstract%3A%20Image%20embeddings%20are%20generally%20assumed%20to%20pose%20limited%20privacy%20risk.%20We%20challenge%20this%20assumption%20by%20formalizing%20semantic%20leakage%20as%20the%20ability%20to%20recover%20semantic%20structures%20from%20compressed%20image%20embeddings.%20Surprisingly%2C%20we%20show%20that%20semantic%20leakage%20does%20not%20require%20exact%20reconstruction%20of%20the%20original%20image.%20Preserving%20local%20semantic%20neighborhoods%20under%20embedding%20alignment%20is%20sufficient%20to%20expose%20the%20intrinsic%20vulnerability%20of%20image%20embeddings.%20Crucially%2C%20this%20preserved%20neighborhood%20structure%20allows%20semantic%20information%20to%20propagate%20through%20a%20sequence%20of%20lossy%20mappings.%20Based%20on%20this%20conjecture%2C%20we%20propose%20Semantic%20Leakage%20from%20Image%20Embeddings%20%28SLImE%29%2C%20a%20lightweight%20inference%20framework%20that%20reveals%20semantic%20information%20from%20standalone%20compressed%20image%20embeddings%2C%20incorporating%20a%20locally%20trained%20semantic%20retriever%20with%20off-the-shelf%20models%2C%20without%20training%20task-specific%20decoders.%20We%20thoroughly%20validate%20each%20step%20of%20the%20framework%20empirically%2C%20from%20aligned%20embeddings%20to%20retrieved%20tags%2C%20symbolic%20representations%2C%20and%20grammatical%20and%20coherent%20descriptions.%20We%20evaluate%20SLImE%20across%20a%20range%20of%20open%20and%20closed%20embedding%20models%2C%20including%20GEMINI%2C%20COHERE%2C%20NOMIC%2C%20and%20CLIP%2C%20and%20demonstrate%20consistent%20recovery%20of%20semantic%20information%20across%20diverse%20inference%20tasks.%20Our%20results%20reveal%20a%20fundamental%20vulnerability%20in%20image%20embeddings%2C%20whereby%20the%20preservation%20of%20semantic%20neighborhoods%20under%20alignment%20enables%20semantic%20leakage%2C%20highlighting%20challenges%20for%20privacy%20preservation.1%0ALink%3A%20http%3A//arxiv.org/abs/2601.22929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Leakage%2520from%2520Image%2520Embeddings%26entry.906535625%3DYiyi%2520Chen%2520and%2520Qiongkai%2520Xu%2520and%2520Desmond%2520Eliott%2520and%2520Qiongxiu%2520Li%2520and%2520Johannes%2520Bjerva%26entry.1292438233%3DImage%2520embeddings%2520are%2520generally%2520assumed%2520to%2520pose%2520limited%2520privacy%2520risk.%2520We%2520challenge%2520this%2520assumption%2520by%2520formalizing%2520semantic%2520leakage%2520as%2520the%2520ability%2520to%2520recover%2520semantic%2520structures%2520from%2520compressed%2520image%2520embeddings.%2520Surprisingly%252C%2520we%2520show%2520that%2520semantic%2520leakage%2520does%2520not%2520require%2520exact%2520reconstruction%2520of%2520the%2520original%2520image.%2520Preserving%2520local%2520semantic%2520neighborhoods%2520under%2520embedding%2520alignment%2520is%2520sufficient%2520to%2520expose%2520the%2520intrinsic%2520vulnerability%2520of%2520image%2520embeddings.%2520Crucially%252C%2520this%2520preserved%2520neighborhood%2520structure%2520allows%2520semantic%2520information%2520to%2520propagate%2520through%2520a%2520sequence%2520of%2520lossy%2520mappings.%2520Based%2520on%2520this%2520conjecture%252C%2520we%2520propose%2520Semantic%2520Leakage%2520from%2520Image%2520Embeddings%2520%2528SLImE%2529%252C%2520a%2520lightweight%2520inference%2520framework%2520that%2520reveals%2520semantic%2520information%2520from%2520standalone%2520compressed%2520image%2520embeddings%252C%2520incorporating%2520a%2520locally%2520trained%2520semantic%2520retriever%2520with%2520off-the-shelf%2520models%252C%2520without%2520training%2520task-specific%2520decoders.%2520We%2520thoroughly%2520validate%2520each%2520step%2520of%2520the%2520framework%2520empirically%252C%2520from%2520aligned%2520embeddings%2520to%2520retrieved%2520tags%252C%2520symbolic%2520representations%252C%2520and%2520grammatical%2520and%2520coherent%2520descriptions.%2520We%2520evaluate%2520SLImE%2520across%2520a%2520range%2520of%2520open%2520and%2520closed%2520embedding%2520models%252C%2520including%2520GEMINI%252C%2520COHERE%252C%2520NOMIC%252C%2520and%2520CLIP%252C%2520and%2520demonstrate%2520consistent%2520recovery%2520of%2520semantic%2520information%2520across%2520diverse%2520inference%2520tasks.%2520Our%2520results%2520reveal%2520a%2520fundamental%2520vulnerability%2520in%2520image%2520embeddings%252C%2520whereby%2520the%2520preservation%2520of%2520semantic%2520neighborhoods%2520under%2520alignment%2520enables%2520semantic%2520leakage%252C%2520highlighting%2520challenges%2520for%2520privacy%2520preservation.1%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Leakage%20from%20Image%20Embeddings&entry.906535625=Yiyi%20Chen%20and%20Qiongkai%20Xu%20and%20Desmond%20Eliott%20and%20Qiongxiu%20Li%20and%20Johannes%20Bjerva&entry.1292438233=Image%20embeddings%20are%20generally%20assumed%20to%20pose%20limited%20privacy%20risk.%20We%20challenge%20this%20assumption%20by%20formalizing%20semantic%20leakage%20as%20the%20ability%20to%20recover%20semantic%20structures%20from%20compressed%20image%20embeddings.%20Surprisingly%2C%20we%20show%20that%20semantic%20leakage%20does%20not%20require%20exact%20reconstruction%20of%20the%20original%20image.%20Preserving%20local%20semantic%20neighborhoods%20under%20embedding%20alignment%20is%20sufficient%20to%20expose%20the%20intrinsic%20vulnerability%20of%20image%20embeddings.%20Crucially%2C%20this%20preserved%20neighborhood%20structure%20allows%20semantic%20information%20to%20propagate%20through%20a%20sequence%20of%20lossy%20mappings.%20Based%20on%20this%20conjecture%2C%20we%20propose%20Semantic%20Leakage%20from%20Image%20Embeddings%20%28SLImE%29%2C%20a%20lightweight%20inference%20framework%20that%20reveals%20semantic%20information%20from%20standalone%20compressed%20image%20embeddings%2C%20incorporating%20a%20locally%20trained%20semantic%20retriever%20with%20off-the-shelf%20models%2C%20without%20training%20task-specific%20decoders.%20We%20thoroughly%20validate%20each%20step%20of%20the%20framework%20empirically%2C%20from%20aligned%20embeddings%20to%20retrieved%20tags%2C%20symbolic%20representations%2C%20and%20grammatical%20and%20coherent%20descriptions.%20We%20evaluate%20SLImE%20across%20a%20range%20of%20open%20and%20closed%20embedding%20models%2C%20including%20GEMINI%2C%20COHERE%2C%20NOMIC%2C%20and%20CLIP%2C%20and%20demonstrate%20consistent%20recovery%20of%20semantic%20information%20across%20diverse%20inference%20tasks.%20Our%20results%20reveal%20a%20fundamental%20vulnerability%20in%20image%20embeddings%2C%20whereby%20the%20preservation%20of%20semantic%20neighborhoods%20under%20alignment%20enables%20semantic%20leakage%2C%20highlighting%20challenges%20for%20privacy%20preservation.1&entry.1838667208=http%3A//arxiv.org/abs/2601.22929v1&entry.124074799=Read"},
{"title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text", "author": "Ximing Lu and David Acuna and Jaehun Jung and Jian Hu and Di Zhang and Shizhe Diao and Yunheng Zou and Shaokun Zhang and Brandon Cui and Mingjie Liu and Hyunwoo Kim and Prithviraj Ammanabrolu and Jan Kautz and Yi Dong and Yejin Choi", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.", "link": "http://arxiv.org/abs/2601.22975v1", "date": "2026-01-30", "relevancy": 2.5722, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5395}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Golden%20Goose%3A%20A%20Simple%20Trick%20to%20Synthesize%20Unlimited%20RLVR%20Tasks%20from%20Unverifiable%20Internet%20Text&body=Title%3A%20Golden%20Goose%3A%20A%20Simple%20Trick%20to%20Synthesize%20Unlimited%20RLVR%20Tasks%20from%20Unverifiable%20Internet%20Text%0AAuthor%3A%20Ximing%20Lu%20and%20David%20Acuna%20and%20Jaehun%20Jung%20and%20Jian%20Hu%20and%20Di%20Zhang%20and%20Shizhe%20Diao%20and%20Yunheng%20Zou%20and%20Shaokun%20Zhang%20and%20Brandon%20Cui%20and%20Mingjie%20Liu%20and%20Hyunwoo%20Kim%20and%20Prithviraj%20Ammanabrolu%20and%20Jan%20Kautz%20and%20Yi%20Dong%20and%20Yejin%20Choi%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20become%20a%20cornerstone%20for%20unlocking%20complex%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29.%20Yet%2C%20scaling%20up%20RL%20is%20bottlenecked%20by%20limited%20existing%20verifiable%20data%2C%20where%20improvements%20increasingly%20saturate%20over%20prolonged%20training.%20To%20overcome%20this%2C%20we%20propose%20Golden%20Goose%2C%20a%20simple%20trick%20to%20synthesize%20unlimited%20RLVR%20tasks%20from%20unverifiable%20internet%20text%20by%20constructing%20a%20multiple-choice%20question-answering%20version%20of%20the%20fill-in-the-middle%20task.%20Given%20a%20source%20text%2C%20we%20prompt%20an%20LLM%20to%20identify%20and%20mask%20key%20reasoning%20steps%2C%20then%20generate%20a%20set%20of%20diverse%2C%20plausible%20distractors.%20This%20enables%20us%20to%20leverage%20reasoning-rich%20unverifiable%20corpora%20typically%20excluded%20from%20prior%20RLVR%20data%20construction%20%28e.g.%2C%20science%20textbooks%29%20to%20synthesize%20GooseReason-0.7M%2C%20a%20large-scale%20RLVR%20dataset%20with%20over%200.7%20million%20tasks%20spanning%20mathematics%2C%20programming%2C%20and%20general%20scientific%20domains.%20Empirically%2C%20GooseReason%20effectively%20revives%20models%20saturated%20on%20existing%20RLVR%20data%2C%20yielding%20robust%2C%20sustained%20gains%20under%20continuous%20RL%20and%20achieving%20new%20state-of-the-art%20results%20for%201.5B%20and%204B-Instruct%20models%20across%2015%20diverse%20benchmarks.%20Finally%2C%20we%20deploy%20Golden%20Goose%20in%20a%20real-world%20setting%2C%20synthesizing%20RLVR%20tasks%20from%20raw%20FineWeb%20scrapes%20for%20the%20cybersecurity%20domain%2C%20where%20no%20prior%20RLVR%20data%20exists.%20Training%20Qwen3-4B-Instruct%20on%20the%20resulting%20data%20GooseReason-Cyber%20sets%20a%20new%20state-of-the-art%20in%20cybersecurity%2C%20surpassing%20a%207B%20domain-specialized%20model%20with%20extensive%20domain-specific%20pre-training%20and%20post-training.%20This%20highlights%20the%20potential%20of%20automatically%20scaling%20up%20RLVR%20data%20by%20exploiting%20abundant%2C%20reasoning-rich%2C%20unverifiable%20internet%20text.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGolden%2520Goose%253A%2520A%2520Simple%2520Trick%2520to%2520Synthesize%2520Unlimited%2520RLVR%2520Tasks%2520from%2520Unverifiable%2520Internet%2520Text%26entry.906535625%3DXiming%2520Lu%2520and%2520David%2520Acuna%2520and%2520Jaehun%2520Jung%2520and%2520Jian%2520Hu%2520and%2520Di%2520Zhang%2520and%2520Shizhe%2520Diao%2520and%2520Yunheng%2520Zou%2520and%2520Shaokun%2520Zhang%2520and%2520Brandon%2520Cui%2520and%2520Mingjie%2520Liu%2520and%2520Hyunwoo%2520Kim%2520and%2520Prithviraj%2520Ammanabrolu%2520and%2520Jan%2520Kautz%2520and%2520Yi%2520Dong%2520and%2520Yejin%2520Choi%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520become%2520a%2520cornerstone%2520for%2520unlocking%2520complex%2520reasoning%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Yet%252C%2520scaling%2520up%2520RL%2520is%2520bottlenecked%2520by%2520limited%2520existing%2520verifiable%2520data%252C%2520where%2520improvements%2520increasingly%2520saturate%2520over%2520prolonged%2520training.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520Golden%2520Goose%252C%2520a%2520simple%2520trick%2520to%2520synthesize%2520unlimited%2520RLVR%2520tasks%2520from%2520unverifiable%2520internet%2520text%2520by%2520constructing%2520a%2520multiple-choice%2520question-answering%2520version%2520of%2520the%2520fill-in-the-middle%2520task.%2520Given%2520a%2520source%2520text%252C%2520we%2520prompt%2520an%2520LLM%2520to%2520identify%2520and%2520mask%2520key%2520reasoning%2520steps%252C%2520then%2520generate%2520a%2520set%2520of%2520diverse%252C%2520plausible%2520distractors.%2520This%2520enables%2520us%2520to%2520leverage%2520reasoning-rich%2520unverifiable%2520corpora%2520typically%2520excluded%2520from%2520prior%2520RLVR%2520data%2520construction%2520%2528e.g.%252C%2520science%2520textbooks%2529%2520to%2520synthesize%2520GooseReason-0.7M%252C%2520a%2520large-scale%2520RLVR%2520dataset%2520with%2520over%25200.7%2520million%2520tasks%2520spanning%2520mathematics%252C%2520programming%252C%2520and%2520general%2520scientific%2520domains.%2520Empirically%252C%2520GooseReason%2520effectively%2520revives%2520models%2520saturated%2520on%2520existing%2520RLVR%2520data%252C%2520yielding%2520robust%252C%2520sustained%2520gains%2520under%2520continuous%2520RL%2520and%2520achieving%2520new%2520state-of-the-art%2520results%2520for%25201.5B%2520and%25204B-Instruct%2520models%2520across%252015%2520diverse%2520benchmarks.%2520Finally%252C%2520we%2520deploy%2520Golden%2520Goose%2520in%2520a%2520real-world%2520setting%252C%2520synthesizing%2520RLVR%2520tasks%2520from%2520raw%2520FineWeb%2520scrapes%2520for%2520the%2520cybersecurity%2520domain%252C%2520where%2520no%2520prior%2520RLVR%2520data%2520exists.%2520Training%2520Qwen3-4B-Instruct%2520on%2520the%2520resulting%2520data%2520GooseReason-Cyber%2520sets%2520a%2520new%2520state-of-the-art%2520in%2520cybersecurity%252C%2520surpassing%2520a%25207B%2520domain-specialized%2520model%2520with%2520extensive%2520domain-specific%2520pre-training%2520and%2520post-training.%2520This%2520highlights%2520the%2520potential%2520of%2520automatically%2520scaling%2520up%2520RLVR%2520data%2520by%2520exploiting%2520abundant%252C%2520reasoning-rich%252C%2520unverifiable%2520internet%2520text.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Golden%20Goose%3A%20A%20Simple%20Trick%20to%20Synthesize%20Unlimited%20RLVR%20Tasks%20from%20Unverifiable%20Internet%20Text&entry.906535625=Ximing%20Lu%20and%20David%20Acuna%20and%20Jaehun%20Jung%20and%20Jian%20Hu%20and%20Di%20Zhang%20and%20Shizhe%20Diao%20and%20Yunheng%20Zou%20and%20Shaokun%20Zhang%20and%20Brandon%20Cui%20and%20Mingjie%20Liu%20and%20Hyunwoo%20Kim%20and%20Prithviraj%20Ammanabrolu%20and%20Jan%20Kautz%20and%20Yi%20Dong%20and%20Yejin%20Choi&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20become%20a%20cornerstone%20for%20unlocking%20complex%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29.%20Yet%2C%20scaling%20up%20RL%20is%20bottlenecked%20by%20limited%20existing%20verifiable%20data%2C%20where%20improvements%20increasingly%20saturate%20over%20prolonged%20training.%20To%20overcome%20this%2C%20we%20propose%20Golden%20Goose%2C%20a%20simple%20trick%20to%20synthesize%20unlimited%20RLVR%20tasks%20from%20unverifiable%20internet%20text%20by%20constructing%20a%20multiple-choice%20question-answering%20version%20of%20the%20fill-in-the-middle%20task.%20Given%20a%20source%20text%2C%20we%20prompt%20an%20LLM%20to%20identify%20and%20mask%20key%20reasoning%20steps%2C%20then%20generate%20a%20set%20of%20diverse%2C%20plausible%20distractors.%20This%20enables%20us%20to%20leverage%20reasoning-rich%20unverifiable%20corpora%20typically%20excluded%20from%20prior%20RLVR%20data%20construction%20%28e.g.%2C%20science%20textbooks%29%20to%20synthesize%20GooseReason-0.7M%2C%20a%20large-scale%20RLVR%20dataset%20with%20over%200.7%20million%20tasks%20spanning%20mathematics%2C%20programming%2C%20and%20general%20scientific%20domains.%20Empirically%2C%20GooseReason%20effectively%20revives%20models%20saturated%20on%20existing%20RLVR%20data%2C%20yielding%20robust%2C%20sustained%20gains%20under%20continuous%20RL%20and%20achieving%20new%20state-of-the-art%20results%20for%201.5B%20and%204B-Instruct%20models%20across%2015%20diverse%20benchmarks.%20Finally%2C%20we%20deploy%20Golden%20Goose%20in%20a%20real-world%20setting%2C%20synthesizing%20RLVR%20tasks%20from%20raw%20FineWeb%20scrapes%20for%20the%20cybersecurity%20domain%2C%20where%20no%20prior%20RLVR%20data%20exists.%20Training%20Qwen3-4B-Instruct%20on%20the%20resulting%20data%20GooseReason-Cyber%20sets%20a%20new%20state-of-the-art%20in%20cybersecurity%2C%20surpassing%20a%207B%20domain-specialized%20model%20with%20extensive%20domain-specific%20pre-training%20and%20post-training.%20This%20highlights%20the%20potential%20of%20automatically%20scaling%20up%20RLVR%20data%20by%20exploiting%20abundant%2C%20reasoning-rich%2C%20unverifiable%20internet%20text.&entry.1838667208=http%3A//arxiv.org/abs/2601.22975v1&entry.124074799=Read"},
{"title": "Token Entropy Regularization for Multi-modal Antenna Affiliation Identification", "author": "Dong Chen and Ruoyu Li and Xinyan Zhang and Jialei Xu and Ruosen Zhao and Zhikang Zhang and Lingyun Li and Zizhuang Wei", "abstract": "Accurate antenna affiliation identification is crucial for optimizing and maintaining communication networks. Current practice, however, relies on the cumbersome and error-prone process of manual tower inspections. We propose a novel paradigm shift that fuses video footage of base stations, antenna geometric features, and Physical Cell Identity (PCI) signals, transforming antenna affiliation identification into multi-modal classification and matching tasks. Publicly available pretrained transformers struggle with this unique task due to a lack of analogous data in the communications domain, which hampers cross-modal alignment. To address this, we introduce a dedicated training framework that aligns antenna images with corresponding PCI signals. To tackle the representation alignment challenge, we propose a novel Token Entropy Regularization module in the pretraining stage. Our experiments demonstrate that TER accelerates convergence and yields significant performance gains. Further analysis reveals that the entropy of the first token is modality-dependent. Code will be made available upon publication.", "link": "http://arxiv.org/abs/2601.21280v2", "date": "2026-01-30", "relevancy": 2.5483, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5362}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Entropy%20Regularization%20for%20Multi-modal%20Antenna%20Affiliation%20Identification&body=Title%3A%20Token%20Entropy%20Regularization%20for%20Multi-modal%20Antenna%20Affiliation%20Identification%0AAuthor%3A%20Dong%20Chen%20and%20Ruoyu%20Li%20and%20Xinyan%20Zhang%20and%20Jialei%20Xu%20and%20Ruosen%20Zhao%20and%20Zhikang%20Zhang%20and%20Lingyun%20Li%20and%20Zizhuang%20Wei%0AAbstract%3A%20Accurate%20antenna%20affiliation%20identification%20is%20crucial%20for%20optimizing%20and%20maintaining%20communication%20networks.%20Current%20practice%2C%20however%2C%20relies%20on%20the%20cumbersome%20and%20error-prone%20process%20of%20manual%20tower%20inspections.%20We%20propose%20a%20novel%20paradigm%20shift%20that%20fuses%20video%20footage%20of%20base%20stations%2C%20antenna%20geometric%20features%2C%20and%20Physical%20Cell%20Identity%20%28PCI%29%20signals%2C%20transforming%20antenna%20affiliation%20identification%20into%20multi-modal%20classification%20and%20matching%20tasks.%20Publicly%20available%20pretrained%20transformers%20struggle%20with%20this%20unique%20task%20due%20to%20a%20lack%20of%20analogous%20data%20in%20the%20communications%20domain%2C%20which%20hampers%20cross-modal%20alignment.%20To%20address%20this%2C%20we%20introduce%20a%20dedicated%20training%20framework%20that%20aligns%20antenna%20images%20with%20corresponding%20PCI%20signals.%20To%20tackle%20the%20representation%20alignment%20challenge%2C%20we%20propose%20a%20novel%20Token%20Entropy%20Regularization%20module%20in%20the%20pretraining%20stage.%20Our%20experiments%20demonstrate%20that%20TER%20accelerates%20convergence%20and%20yields%20significant%20performance%20gains.%20Further%20analysis%20reveals%20that%20the%20entropy%20of%20the%20first%20token%20is%20modality-dependent.%20Code%20will%20be%20made%20available%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21280v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Entropy%2520Regularization%2520for%2520Multi-modal%2520Antenna%2520Affiliation%2520Identification%26entry.906535625%3DDong%2520Chen%2520and%2520Ruoyu%2520Li%2520and%2520Xinyan%2520Zhang%2520and%2520Jialei%2520Xu%2520and%2520Ruosen%2520Zhao%2520and%2520Zhikang%2520Zhang%2520and%2520Lingyun%2520Li%2520and%2520Zizhuang%2520Wei%26entry.1292438233%3DAccurate%2520antenna%2520affiliation%2520identification%2520is%2520crucial%2520for%2520optimizing%2520and%2520maintaining%2520communication%2520networks.%2520Current%2520practice%252C%2520however%252C%2520relies%2520on%2520the%2520cumbersome%2520and%2520error-prone%2520process%2520of%2520manual%2520tower%2520inspections.%2520We%2520propose%2520a%2520novel%2520paradigm%2520shift%2520that%2520fuses%2520video%2520footage%2520of%2520base%2520stations%252C%2520antenna%2520geometric%2520features%252C%2520and%2520Physical%2520Cell%2520Identity%2520%2528PCI%2529%2520signals%252C%2520transforming%2520antenna%2520affiliation%2520identification%2520into%2520multi-modal%2520classification%2520and%2520matching%2520tasks.%2520Publicly%2520available%2520pretrained%2520transformers%2520struggle%2520with%2520this%2520unique%2520task%2520due%2520to%2520a%2520lack%2520of%2520analogous%2520data%2520in%2520the%2520communications%2520domain%252C%2520which%2520hampers%2520cross-modal%2520alignment.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520dedicated%2520training%2520framework%2520that%2520aligns%2520antenna%2520images%2520with%2520corresponding%2520PCI%2520signals.%2520To%2520tackle%2520the%2520representation%2520alignment%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520Token%2520Entropy%2520Regularization%2520module%2520in%2520the%2520pretraining%2520stage.%2520Our%2520experiments%2520demonstrate%2520that%2520TER%2520accelerates%2520convergence%2520and%2520yields%2520significant%2520performance%2520gains.%2520Further%2520analysis%2520reveals%2520that%2520the%2520entropy%2520of%2520the%2520first%2520token%2520is%2520modality-dependent.%2520Code%2520will%2520be%2520made%2520available%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21280v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Entropy%20Regularization%20for%20Multi-modal%20Antenna%20Affiliation%20Identification&entry.906535625=Dong%20Chen%20and%20Ruoyu%20Li%20and%20Xinyan%20Zhang%20and%20Jialei%20Xu%20and%20Ruosen%20Zhao%20and%20Zhikang%20Zhang%20and%20Lingyun%20Li%20and%20Zizhuang%20Wei&entry.1292438233=Accurate%20antenna%20affiliation%20identification%20is%20crucial%20for%20optimizing%20and%20maintaining%20communication%20networks.%20Current%20practice%2C%20however%2C%20relies%20on%20the%20cumbersome%20and%20error-prone%20process%20of%20manual%20tower%20inspections.%20We%20propose%20a%20novel%20paradigm%20shift%20that%20fuses%20video%20footage%20of%20base%20stations%2C%20antenna%20geometric%20features%2C%20and%20Physical%20Cell%20Identity%20%28PCI%29%20signals%2C%20transforming%20antenna%20affiliation%20identification%20into%20multi-modal%20classification%20and%20matching%20tasks.%20Publicly%20available%20pretrained%20transformers%20struggle%20with%20this%20unique%20task%20due%20to%20a%20lack%20of%20analogous%20data%20in%20the%20communications%20domain%2C%20which%20hampers%20cross-modal%20alignment.%20To%20address%20this%2C%20we%20introduce%20a%20dedicated%20training%20framework%20that%20aligns%20antenna%20images%20with%20corresponding%20PCI%20signals.%20To%20tackle%20the%20representation%20alignment%20challenge%2C%20we%20propose%20a%20novel%20Token%20Entropy%20Regularization%20module%20in%20the%20pretraining%20stage.%20Our%20experiments%20demonstrate%20that%20TER%20accelerates%20convergence%20and%20yields%20significant%20performance%20gains.%20Further%20analysis%20reveals%20that%20the%20entropy%20of%20the%20first%20token%20is%20modality-dependent.%20Code%20will%20be%20made%20available%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2601.21280v2&entry.124074799=Read"},
{"title": "DREAMS: Preserving both Local and Global Structure in Dimensionality Reduction", "author": "No\u00ebl Kury and Dmitry Kobak and Sebastian Damrich", "abstract": "Dimensionality reduction techniques are widely used for visualizing high-dimensional data in two dimensions. Existing methods are typically designed to preserve either local (e.g., $t$-SNE, UMAP) or global (e.g., MDS, PCA) structure of the data, but none of the established methods can represent both aspects well. In this paper, we present DREAMS (Dimensionality Reduction Enhanced Across Multiple Scales), a method that combines the local structure preservation of $t$-SNE with the global structure preservation of PCA via a simple regularization term. Our approach generates a spectrum of embeddings between the locally well-structured $t$-SNE embedding and the globally well-structured PCA embedding, efficiently balancing both local and global structure preservation. We benchmark DREAMS across eleven real-world datasets, showcasing qualitatively and quantitatively its superior ability to preserve structure across multiple scales compared to previous approaches.", "link": "http://arxiv.org/abs/2508.13747v2", "date": "2026-01-30", "relevancy": 2.5473, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5203}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DREAMS%3A%20Preserving%20both%20Local%20and%20Global%20Structure%20in%20Dimensionality%20Reduction&body=Title%3A%20DREAMS%3A%20Preserving%20both%20Local%20and%20Global%20Structure%20in%20Dimensionality%20Reduction%0AAuthor%3A%20No%C3%ABl%20Kury%20and%20Dmitry%20Kobak%20and%20Sebastian%20Damrich%0AAbstract%3A%20Dimensionality%20reduction%20techniques%20are%20widely%20used%20for%20visualizing%20high-dimensional%20data%20in%20two%20dimensions.%20Existing%20methods%20are%20typically%20designed%20to%20preserve%20either%20local%20%28e.g.%2C%20%24t%24-SNE%2C%20UMAP%29%20or%20global%20%28e.g.%2C%20MDS%2C%20PCA%29%20structure%20of%20the%20data%2C%20but%20none%20of%20the%20established%20methods%20can%20represent%20both%20aspects%20well.%20In%20this%20paper%2C%20we%20present%20DREAMS%20%28Dimensionality%20Reduction%20Enhanced%20Across%20Multiple%20Scales%29%2C%20a%20method%20that%20combines%20the%20local%20structure%20preservation%20of%20%24t%24-SNE%20with%20the%20global%20structure%20preservation%20of%20PCA%20via%20a%20simple%20regularization%20term.%20Our%20approach%20generates%20a%20spectrum%20of%20embeddings%20between%20the%20locally%20well-structured%20%24t%24-SNE%20embedding%20and%20the%20globally%20well-structured%20PCA%20embedding%2C%20efficiently%20balancing%20both%20local%20and%20global%20structure%20preservation.%20We%20benchmark%20DREAMS%20across%20eleven%20real-world%20datasets%2C%20showcasing%20qualitatively%20and%20quantitatively%20its%20superior%20ability%20to%20preserve%20structure%20across%20multiple%20scales%20compared%20to%20previous%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2508.13747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDREAMS%253A%2520Preserving%2520both%2520Local%2520and%2520Global%2520Structure%2520in%2520Dimensionality%2520Reduction%26entry.906535625%3DNo%25C3%25ABl%2520Kury%2520and%2520Dmitry%2520Kobak%2520and%2520Sebastian%2520Damrich%26entry.1292438233%3DDimensionality%2520reduction%2520techniques%2520are%2520widely%2520used%2520for%2520visualizing%2520high-dimensional%2520data%2520in%2520two%2520dimensions.%2520Existing%2520methods%2520are%2520typically%2520designed%2520to%2520preserve%2520either%2520local%2520%2528e.g.%252C%2520%2524t%2524-SNE%252C%2520UMAP%2529%2520or%2520global%2520%2528e.g.%252C%2520MDS%252C%2520PCA%2529%2520structure%2520of%2520the%2520data%252C%2520but%2520none%2520of%2520the%2520established%2520methods%2520can%2520represent%2520both%2520aspects%2520well.%2520In%2520this%2520paper%252C%2520we%2520present%2520DREAMS%2520%2528Dimensionality%2520Reduction%2520Enhanced%2520Across%2520Multiple%2520Scales%2529%252C%2520a%2520method%2520that%2520combines%2520the%2520local%2520structure%2520preservation%2520of%2520%2524t%2524-SNE%2520with%2520the%2520global%2520structure%2520preservation%2520of%2520PCA%2520via%2520a%2520simple%2520regularization%2520term.%2520Our%2520approach%2520generates%2520a%2520spectrum%2520of%2520embeddings%2520between%2520the%2520locally%2520well-structured%2520%2524t%2524-SNE%2520embedding%2520and%2520the%2520globally%2520well-structured%2520PCA%2520embedding%252C%2520efficiently%2520balancing%2520both%2520local%2520and%2520global%2520structure%2520preservation.%2520We%2520benchmark%2520DREAMS%2520across%2520eleven%2520real-world%2520datasets%252C%2520showcasing%2520qualitatively%2520and%2520quantitatively%2520its%2520superior%2520ability%2520to%2520preserve%2520structure%2520across%2520multiple%2520scales%2520compared%2520to%2520previous%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DREAMS%3A%20Preserving%20both%20Local%20and%20Global%20Structure%20in%20Dimensionality%20Reduction&entry.906535625=No%C3%ABl%20Kury%20and%20Dmitry%20Kobak%20and%20Sebastian%20Damrich&entry.1292438233=Dimensionality%20reduction%20techniques%20are%20widely%20used%20for%20visualizing%20high-dimensional%20data%20in%20two%20dimensions.%20Existing%20methods%20are%20typically%20designed%20to%20preserve%20either%20local%20%28e.g.%2C%20%24t%24-SNE%2C%20UMAP%29%20or%20global%20%28e.g.%2C%20MDS%2C%20PCA%29%20structure%20of%20the%20data%2C%20but%20none%20of%20the%20established%20methods%20can%20represent%20both%20aspects%20well.%20In%20this%20paper%2C%20we%20present%20DREAMS%20%28Dimensionality%20Reduction%20Enhanced%20Across%20Multiple%20Scales%29%2C%20a%20method%20that%20combines%20the%20local%20structure%20preservation%20of%20%24t%24-SNE%20with%20the%20global%20structure%20preservation%20of%20PCA%20via%20a%20simple%20regularization%20term.%20Our%20approach%20generates%20a%20spectrum%20of%20embeddings%20between%20the%20locally%20well-structured%20%24t%24-SNE%20embedding%20and%20the%20globally%20well-structured%20PCA%20embedding%2C%20efficiently%20balancing%20both%20local%20and%20global%20structure%20preservation.%20We%20benchmark%20DREAMS%20across%20eleven%20real-world%20datasets%2C%20showcasing%20qualitatively%20and%20quantitatively%20its%20superior%20ability%20to%20preserve%20structure%20across%20multiple%20scales%20compared%20to%20previous%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2508.13747v2&entry.124074799=Read"},
{"title": "Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold", "author": "Aldric Labarthe and Roland Bouffanais and Julien Randon-Furling", "abstract": "The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.", "link": "http://arxiv.org/abs/2601.22806v1", "date": "2026-01-30", "relevancy": 2.5396, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.551}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4908}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20the%20Unseen%20in%20Attributed%20Graphs%3A%20Interplay%20between%20Graph%20Geometry%20and%20Node%20Attributes%20Manifold&body=Title%3A%20Aligning%20the%20Unseen%20in%20Attributed%20Graphs%3A%20Interplay%20between%20Graph%20Geometry%20and%20Node%20Attributes%20Manifold%0AAuthor%3A%20Aldric%20Labarthe%20and%20Roland%20Bouffanais%20and%20Julien%20Randon-Furling%0AAbstract%3A%20The%20standard%20approach%20to%20representation%20learning%20on%20attributed%20graphs%20--%20i.e.%2C%20simultaneously%20reconstructing%20node%20attributes%20and%20graph%20structure%20--%20is%20geometrically%20flawed%2C%20as%20it%20merges%20two%20potentially%20incompatible%20metric%20spaces.%20This%20forces%20a%20destructive%20alignment%20that%20erodes%20information%20about%20the%20graph%27s%20underlying%20generative%20process.%20To%20recover%20this%20lost%20signal%2C%20we%20introduce%20a%20custom%20variational%20autoencoder%20that%20separates%20manifold%20learning%20from%20structural%20alignment.%20By%20quantifying%20the%20metric%20distortion%20needed%20to%20map%20the%20attribute%20manifold%20onto%20the%20graph%27s%20Heat%20Kernel%2C%20we%20transform%20geometric%20conflict%20into%20an%20interpretable%20structural%20descriptor.%20Experiments%20show%20our%20method%20uncovers%20connectivity%20patterns%20and%20anomalies%20undetectable%20by%20conventional%20approaches%2C%20proving%20both%20their%20theoretical%20inadequacy%20and%20practical%20limitations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520the%2520Unseen%2520in%2520Attributed%2520Graphs%253A%2520Interplay%2520between%2520Graph%2520Geometry%2520and%2520Node%2520Attributes%2520Manifold%26entry.906535625%3DAldric%2520Labarthe%2520and%2520Roland%2520Bouffanais%2520and%2520Julien%2520Randon-Furling%26entry.1292438233%3DThe%2520standard%2520approach%2520to%2520representation%2520learning%2520on%2520attributed%2520graphs%2520--%2520i.e.%252C%2520simultaneously%2520reconstructing%2520node%2520attributes%2520and%2520graph%2520structure%2520--%2520is%2520geometrically%2520flawed%252C%2520as%2520it%2520merges%2520two%2520potentially%2520incompatible%2520metric%2520spaces.%2520This%2520forces%2520a%2520destructive%2520alignment%2520that%2520erodes%2520information%2520about%2520the%2520graph%2527s%2520underlying%2520generative%2520process.%2520To%2520recover%2520this%2520lost%2520signal%252C%2520we%2520introduce%2520a%2520custom%2520variational%2520autoencoder%2520that%2520separates%2520manifold%2520learning%2520from%2520structural%2520alignment.%2520By%2520quantifying%2520the%2520metric%2520distortion%2520needed%2520to%2520map%2520the%2520attribute%2520manifold%2520onto%2520the%2520graph%2527s%2520Heat%2520Kernel%252C%2520we%2520transform%2520geometric%2520conflict%2520into%2520an%2520interpretable%2520structural%2520descriptor.%2520Experiments%2520show%2520our%2520method%2520uncovers%2520connectivity%2520patterns%2520and%2520anomalies%2520undetectable%2520by%2520conventional%2520approaches%252C%2520proving%2520both%2520their%2520theoretical%2520inadequacy%2520and%2520practical%2520limitations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20the%20Unseen%20in%20Attributed%20Graphs%3A%20Interplay%20between%20Graph%20Geometry%20and%20Node%20Attributes%20Manifold&entry.906535625=Aldric%20Labarthe%20and%20Roland%20Bouffanais%20and%20Julien%20Randon-Furling&entry.1292438233=The%20standard%20approach%20to%20representation%20learning%20on%20attributed%20graphs%20--%20i.e.%2C%20simultaneously%20reconstructing%20node%20attributes%20and%20graph%20structure%20--%20is%20geometrically%20flawed%2C%20as%20it%20merges%20two%20potentially%20incompatible%20metric%20spaces.%20This%20forces%20a%20destructive%20alignment%20that%20erodes%20information%20about%20the%20graph%27s%20underlying%20generative%20process.%20To%20recover%20this%20lost%20signal%2C%20we%20introduce%20a%20custom%20variational%20autoencoder%20that%20separates%20manifold%20learning%20from%20structural%20alignment.%20By%20quantifying%20the%20metric%20distortion%20needed%20to%20map%20the%20attribute%20manifold%20onto%20the%20graph%27s%20Heat%20Kernel%2C%20we%20transform%20geometric%20conflict%20into%20an%20interpretable%20structural%20descriptor.%20Experiments%20show%20our%20method%20uncovers%20connectivity%20patterns%20and%20anomalies%20undetectable%20by%20conventional%20approaches%2C%20proving%20both%20their%20theoretical%20inadequacy%20and%20practical%20limitations.&entry.1838667208=http%3A//arxiv.org/abs/2601.22806v1&entry.124074799=Read"},
{"title": "High-quality generation of dynamic game content via small language models: A proof of concept", "author": "Morten I. K. Munk and Arturo Valdivia and Paolo Burelli", "abstract": "Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practical issues are solved by pivoting to small language models (SLMs), but existing studies using SLMs have resulted in poor output quality. We propose a strategy of achieving high-quality SLM generation through aggressive fine-tuning on deliberately scoped tasks with narrow context, constrained structure, or both. In short, more difficult tasks require narrower scope and higher specialization to the training corpus. Training data is synthetically generated via a DAG-based approach, grounding models in the specific game world. Such models can form the basis for agentic networks designed around the narratological framework at hand, representing a more practical and robust solution than cloud-dependent LLMs. To validate this approach, we present a proof-of-concept focusing on a single specialized SLM as the fundamental building block. We introduce a minimal RPG loop revolving around rhetorical battles of reputations, powered by this model. We demonstrate that a simple retry-until-success strategy reaches adequate quality (as defined by an LLM-as-a-judge scheme) with predictable latency suitable for real-time generation. While local quality assessment remains an open question, our results demonstrate feasibility for real-time generation under typical game engine constraints.", "link": "http://arxiv.org/abs/2601.23206v1", "date": "2026-01-30", "relevancy": 2.4908, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5156}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4958}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-quality%20generation%20of%20dynamic%20game%20content%20via%20small%20language%20models%3A%20A%20proof%20of%20concept&body=Title%3A%20High-quality%20generation%20of%20dynamic%20game%20content%20via%20small%20language%20models%3A%20A%20proof%20of%20concept%0AAuthor%3A%20Morten%20I.%20K.%20Munk%20and%20Arturo%20Valdivia%20and%20Paolo%20Burelli%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20offer%20promise%20for%20dynamic%20game%20content%20generation%2C%20but%20they%20face%20critical%20barriers%2C%20including%20narrative%20incoherence%20and%20high%20operational%20costs.%20Due%20to%20their%20large%20size%2C%20they%20are%20often%20accessed%20in%20the%20cloud%2C%20limiting%20their%20application%20in%20offline%20games.%20Many%20of%20these%20practical%20issues%20are%20solved%20by%20pivoting%20to%20small%20language%20models%20%28SLMs%29%2C%20but%20existing%20studies%20using%20SLMs%20have%20resulted%20in%20poor%20output%20quality.%20We%20propose%20a%20strategy%20of%20achieving%20high-quality%20SLM%20generation%20through%20aggressive%20fine-tuning%20on%20deliberately%20scoped%20tasks%20with%20narrow%20context%2C%20constrained%20structure%2C%20or%20both.%20In%20short%2C%20more%20difficult%20tasks%20require%20narrower%20scope%20and%20higher%20specialization%20to%20the%20training%20corpus.%20Training%20data%20is%20synthetically%20generated%20via%20a%20DAG-based%20approach%2C%20grounding%20models%20in%20the%20specific%20game%20world.%20Such%20models%20can%20form%20the%20basis%20for%20agentic%20networks%20designed%20around%20the%20narratological%20framework%20at%20hand%2C%20representing%20a%20more%20practical%20and%20robust%20solution%20than%20cloud-dependent%20LLMs.%20To%20validate%20this%20approach%2C%20we%20present%20a%20proof-of-concept%20focusing%20on%20a%20single%20specialized%20SLM%20as%20the%20fundamental%20building%20block.%20We%20introduce%20a%20minimal%20RPG%20loop%20revolving%20around%20rhetorical%20battles%20of%20reputations%2C%20powered%20by%20this%20model.%20We%20demonstrate%20that%20a%20simple%20retry-until-success%20strategy%20reaches%20adequate%20quality%20%28as%20defined%20by%20an%20LLM-as-a-judge%20scheme%29%20with%20predictable%20latency%20suitable%20for%20real-time%20generation.%20While%20local%20quality%20assessment%20remains%20an%20open%20question%2C%20our%20results%20demonstrate%20feasibility%20for%20real-time%20generation%20under%20typical%20game%20engine%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-quality%2520generation%2520of%2520dynamic%2520game%2520content%2520via%2520small%2520language%2520models%253A%2520A%2520proof%2520of%2520concept%26entry.906535625%3DMorten%2520I.%2520K.%2520Munk%2520and%2520Arturo%2520Valdivia%2520and%2520Paolo%2520Burelli%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520offer%2520promise%2520for%2520dynamic%2520game%2520content%2520generation%252C%2520but%2520they%2520face%2520critical%2520barriers%252C%2520including%2520narrative%2520incoherence%2520and%2520high%2520operational%2520costs.%2520Due%2520to%2520their%2520large%2520size%252C%2520they%2520are%2520often%2520accessed%2520in%2520the%2520cloud%252C%2520limiting%2520their%2520application%2520in%2520offline%2520games.%2520Many%2520of%2520these%2520practical%2520issues%2520are%2520solved%2520by%2520pivoting%2520to%2520small%2520language%2520models%2520%2528SLMs%2529%252C%2520but%2520existing%2520studies%2520using%2520SLMs%2520have%2520resulted%2520in%2520poor%2520output%2520quality.%2520We%2520propose%2520a%2520strategy%2520of%2520achieving%2520high-quality%2520SLM%2520generation%2520through%2520aggressive%2520fine-tuning%2520on%2520deliberately%2520scoped%2520tasks%2520with%2520narrow%2520context%252C%2520constrained%2520structure%252C%2520or%2520both.%2520In%2520short%252C%2520more%2520difficult%2520tasks%2520require%2520narrower%2520scope%2520and%2520higher%2520specialization%2520to%2520the%2520training%2520corpus.%2520Training%2520data%2520is%2520synthetically%2520generated%2520via%2520a%2520DAG-based%2520approach%252C%2520grounding%2520models%2520in%2520the%2520specific%2520game%2520world.%2520Such%2520models%2520can%2520form%2520the%2520basis%2520for%2520agentic%2520networks%2520designed%2520around%2520the%2520narratological%2520framework%2520at%2520hand%252C%2520representing%2520a%2520more%2520practical%2520and%2520robust%2520solution%2520than%2520cloud-dependent%2520LLMs.%2520To%2520validate%2520this%2520approach%252C%2520we%2520present%2520a%2520proof-of-concept%2520focusing%2520on%2520a%2520single%2520specialized%2520SLM%2520as%2520the%2520fundamental%2520building%2520block.%2520We%2520introduce%2520a%2520minimal%2520RPG%2520loop%2520revolving%2520around%2520rhetorical%2520battles%2520of%2520reputations%252C%2520powered%2520by%2520this%2520model.%2520We%2520demonstrate%2520that%2520a%2520simple%2520retry-until-success%2520strategy%2520reaches%2520adequate%2520quality%2520%2528as%2520defined%2520by%2520an%2520LLM-as-a-judge%2520scheme%2529%2520with%2520predictable%2520latency%2520suitable%2520for%2520real-time%2520generation.%2520While%2520local%2520quality%2520assessment%2520remains%2520an%2520open%2520question%252C%2520our%2520results%2520demonstrate%2520feasibility%2520for%2520real-time%2520generation%2520under%2520typical%2520game%2520engine%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-quality%20generation%20of%20dynamic%20game%20content%20via%20small%20language%20models%3A%20A%20proof%20of%20concept&entry.906535625=Morten%20I.%20K.%20Munk%20and%20Arturo%20Valdivia%20and%20Paolo%20Burelli&entry.1292438233=Large%20language%20models%20%28LLMs%29%20offer%20promise%20for%20dynamic%20game%20content%20generation%2C%20but%20they%20face%20critical%20barriers%2C%20including%20narrative%20incoherence%20and%20high%20operational%20costs.%20Due%20to%20their%20large%20size%2C%20they%20are%20often%20accessed%20in%20the%20cloud%2C%20limiting%20their%20application%20in%20offline%20games.%20Many%20of%20these%20practical%20issues%20are%20solved%20by%20pivoting%20to%20small%20language%20models%20%28SLMs%29%2C%20but%20existing%20studies%20using%20SLMs%20have%20resulted%20in%20poor%20output%20quality.%20We%20propose%20a%20strategy%20of%20achieving%20high-quality%20SLM%20generation%20through%20aggressive%20fine-tuning%20on%20deliberately%20scoped%20tasks%20with%20narrow%20context%2C%20constrained%20structure%2C%20or%20both.%20In%20short%2C%20more%20difficult%20tasks%20require%20narrower%20scope%20and%20higher%20specialization%20to%20the%20training%20corpus.%20Training%20data%20is%20synthetically%20generated%20via%20a%20DAG-based%20approach%2C%20grounding%20models%20in%20the%20specific%20game%20world.%20Such%20models%20can%20form%20the%20basis%20for%20agentic%20networks%20designed%20around%20the%20narratological%20framework%20at%20hand%2C%20representing%20a%20more%20practical%20and%20robust%20solution%20than%20cloud-dependent%20LLMs.%20To%20validate%20this%20approach%2C%20we%20present%20a%20proof-of-concept%20focusing%20on%20a%20single%20specialized%20SLM%20as%20the%20fundamental%20building%20block.%20We%20introduce%20a%20minimal%20RPG%20loop%20revolving%20around%20rhetorical%20battles%20of%20reputations%2C%20powered%20by%20this%20model.%20We%20demonstrate%20that%20a%20simple%20retry-until-success%20strategy%20reaches%20adequate%20quality%20%28as%20defined%20by%20an%20LLM-as-a-judge%20scheme%29%20with%20predictable%20latency%20suitable%20for%20real-time%20generation.%20While%20local%20quality%20assessment%20remains%20an%20open%20question%2C%20our%20results%20demonstrate%20feasibility%20for%20real-time%20generation%20under%20typical%20game%20engine%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2601.23206v1&entry.124074799=Read"},
{"title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models", "author": "Anindya Sundar Das and Kangjie Chen and Monowar Bhuyan", "abstract": "Pre-trained language models have achieved remarkable success across a wide range of natural language processing (NLP) tasks, particularly when fine-tuned on large, domain-relevant datasets. However, they remain vulnerable to backdoor attacks, where adversaries embed malicious behaviors using trigger patterns in the training data. These triggers remain dormant during normal usage, but, when activated, can cause targeted misclassifications. In this work, we investigate the internal behavior of backdoored pre-trained encoder-based language models, focusing on the consistent shift in attention and gradient attribution when processing poisoned inputs; where the trigger token dominates both attention and gradient signals, overriding the surrounding context. We propose an inference-time defense that constructs anomaly scores by combining token-level attention and gradient information. Extensive experiments on text classification tasks across diverse backdoor attack scenarios demonstrate that our method significantly reduces attack success rates compared to existing baselines. Furthermore, we provide an interpretability-driven analysis of the scoring mechanism, shedding light on trigger localization and the robustness of the proposed defense.", "link": "http://arxiv.org/abs/2510.04347v2", "date": "2026-01-30", "relevancy": 2.4766, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5108}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4918}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unmasking%20Backdoors%3A%20An%20Explainable%20Defense%20via%20Gradient-Attention%20Anomaly%20Scoring%20for%20Pre-trained%20Language%20Models&body=Title%3A%20Unmasking%20Backdoors%3A%20An%20Explainable%20Defense%20via%20Gradient-Attention%20Anomaly%20Scoring%20for%20Pre-trained%20Language%20Models%0AAuthor%3A%20Anindya%20Sundar%20Das%20and%20Kangjie%20Chen%20and%20Monowar%20Bhuyan%0AAbstract%3A%20Pre-trained%20language%20models%20have%20achieved%20remarkable%20success%20across%20a%20wide%20range%20of%20natural%20language%20processing%20%28NLP%29%20tasks%2C%20particularly%20when%20fine-tuned%20on%20large%2C%20domain-relevant%20datasets.%20However%2C%20they%20remain%20vulnerable%20to%20backdoor%20attacks%2C%20where%20adversaries%20embed%20malicious%20behaviors%20using%20trigger%20patterns%20in%20the%20training%20data.%20These%20triggers%20remain%20dormant%20during%20normal%20usage%2C%20but%2C%20when%20activated%2C%20can%20cause%20targeted%20misclassifications.%20In%20this%20work%2C%20we%20investigate%20the%20internal%20behavior%20of%20backdoored%20pre-trained%20encoder-based%20language%20models%2C%20focusing%20on%20the%20consistent%20shift%20in%20attention%20and%20gradient%20attribution%20when%20processing%20poisoned%20inputs%3B%20where%20the%20trigger%20token%20dominates%20both%20attention%20and%20gradient%20signals%2C%20overriding%20the%20surrounding%20context.%20We%20propose%20an%20inference-time%20defense%20that%20constructs%20anomaly%20scores%20by%20combining%20token-level%20attention%20and%20gradient%20information.%20Extensive%20experiments%20on%20text%20classification%20tasks%20across%20diverse%20backdoor%20attack%20scenarios%20demonstrate%20that%20our%20method%20significantly%20reduces%20attack%20success%20rates%20compared%20to%20existing%20baselines.%20Furthermore%2C%20we%20provide%20an%20interpretability-driven%20analysis%20of%20the%20scoring%20mechanism%2C%20shedding%20light%20on%20trigger%20localization%20and%20the%20robustness%20of%20the%20proposed%20defense.%0ALink%3A%20http%3A//arxiv.org/abs/2510.04347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnmasking%2520Backdoors%253A%2520An%2520Explainable%2520Defense%2520via%2520Gradient-Attention%2520Anomaly%2520Scoring%2520for%2520Pre-trained%2520Language%2520Models%26entry.906535625%3DAnindya%2520Sundar%2520Das%2520and%2520Kangjie%2520Chen%2520and%2520Monowar%2520Bhuyan%26entry.1292438233%3DPre-trained%2520language%2520models%2520have%2520achieved%2520remarkable%2520success%2520across%2520a%2520wide%2520range%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529%2520tasks%252C%2520particularly%2520when%2520fine-tuned%2520on%2520large%252C%2520domain-relevant%2520datasets.%2520However%252C%2520they%2520remain%2520vulnerable%2520to%2520backdoor%2520attacks%252C%2520where%2520adversaries%2520embed%2520malicious%2520behaviors%2520using%2520trigger%2520patterns%2520in%2520the%2520training%2520data.%2520These%2520triggers%2520remain%2520dormant%2520during%2520normal%2520usage%252C%2520but%252C%2520when%2520activated%252C%2520can%2520cause%2520targeted%2520misclassifications.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520internal%2520behavior%2520of%2520backdoored%2520pre-trained%2520encoder-based%2520language%2520models%252C%2520focusing%2520on%2520the%2520consistent%2520shift%2520in%2520attention%2520and%2520gradient%2520attribution%2520when%2520processing%2520poisoned%2520inputs%253B%2520where%2520the%2520trigger%2520token%2520dominates%2520both%2520attention%2520and%2520gradient%2520signals%252C%2520overriding%2520the%2520surrounding%2520context.%2520We%2520propose%2520an%2520inference-time%2520defense%2520that%2520constructs%2520anomaly%2520scores%2520by%2520combining%2520token-level%2520attention%2520and%2520gradient%2520information.%2520Extensive%2520experiments%2520on%2520text%2520classification%2520tasks%2520across%2520diverse%2520backdoor%2520attack%2520scenarios%2520demonstrate%2520that%2520our%2520method%2520significantly%2520reduces%2520attack%2520success%2520rates%2520compared%2520to%2520existing%2520baselines.%2520Furthermore%252C%2520we%2520provide%2520an%2520interpretability-driven%2520analysis%2520of%2520the%2520scoring%2520mechanism%252C%2520shedding%2520light%2520on%2520trigger%2520localization%2520and%2520the%2520robustness%2520of%2520the%2520proposed%2520defense.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unmasking%20Backdoors%3A%20An%20Explainable%20Defense%20via%20Gradient-Attention%20Anomaly%20Scoring%20for%20Pre-trained%20Language%20Models&entry.906535625=Anindya%20Sundar%20Das%20and%20Kangjie%20Chen%20and%20Monowar%20Bhuyan&entry.1292438233=Pre-trained%20language%20models%20have%20achieved%20remarkable%20success%20across%20a%20wide%20range%20of%20natural%20language%20processing%20%28NLP%29%20tasks%2C%20particularly%20when%20fine-tuned%20on%20large%2C%20domain-relevant%20datasets.%20However%2C%20they%20remain%20vulnerable%20to%20backdoor%20attacks%2C%20where%20adversaries%20embed%20malicious%20behaviors%20using%20trigger%20patterns%20in%20the%20training%20data.%20These%20triggers%20remain%20dormant%20during%20normal%20usage%2C%20but%2C%20when%20activated%2C%20can%20cause%20targeted%20misclassifications.%20In%20this%20work%2C%20we%20investigate%20the%20internal%20behavior%20of%20backdoored%20pre-trained%20encoder-based%20language%20models%2C%20focusing%20on%20the%20consistent%20shift%20in%20attention%20and%20gradient%20attribution%20when%20processing%20poisoned%20inputs%3B%20where%20the%20trigger%20token%20dominates%20both%20attention%20and%20gradient%20signals%2C%20overriding%20the%20surrounding%20context.%20We%20propose%20an%20inference-time%20defense%20that%20constructs%20anomaly%20scores%20by%20combining%20token-level%20attention%20and%20gradient%20information.%20Extensive%20experiments%20on%20text%20classification%20tasks%20across%20diverse%20backdoor%20attack%20scenarios%20demonstrate%20that%20our%20method%20significantly%20reduces%20attack%20success%20rates%20compared%20to%20existing%20baselines.%20Furthermore%2C%20we%20provide%20an%20interpretability-driven%20analysis%20of%20the%20scoring%20mechanism%2C%20shedding%20light%20on%20trigger%20localization%20and%20the%20robustness%20of%20the%20proposed%20defense.&entry.1838667208=http%3A//arxiv.org/abs/2510.04347v2&entry.124074799=Read"},
{"title": "Robust and Generalized Humanoid Motion Tracking", "author": "Yubiao Ma and Han Yu and Jiayin Xie and Changtai Lv and Qiang Luo and Chi Zhang and Yunpeng Yin and Boyang Xing and Xuemei Ren and Dongdong Zheng", "abstract": "Learning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies after being transferred to the robot domain, and local defects may be amplified by closed-loop execution, causing drift or failure in highly dynamic and contact-rich behaviors. We propose a dynamics-conditioned command aggregation framework that uses a causal temporal encoder to summarize recent proprioception and a multi-head cross-attention command encoder to selectively aggregate a context window based on the current dynamics. We further integrate a fall recovery curriculum with random unstable initialization and an annealed upward assistance force to improve robustness and disturbance rejection. The resulting policy requires only about 3.5 hours of motion data and supports single-stage end-to-end training without distillation. The proposed method is evaluated under diverse reference inputs and challenging motion regimes, demonstrating zero-shot transfer to unseen motions as well as robust sim-to-real transfer on a physical humanoid robot.", "link": "http://arxiv.org/abs/2601.23080v1", "date": "2026-01-30", "relevancy": 2.4532, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6097}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Generalized%20Humanoid%20Motion%20Tracking&body=Title%3A%20Robust%20and%20Generalized%20Humanoid%20Motion%20Tracking%0AAuthor%3A%20Yubiao%20Ma%20and%20Han%20Yu%20and%20Jiayin%20Xie%20and%20Changtai%20Lv%20and%20Qiang%20Luo%20and%20Chi%20Zhang%20and%20Yunpeng%20Yin%20and%20Boyang%20Xing%20and%20Xuemei%20Ren%20and%20Dongdong%20Zheng%0AAbstract%3A%20Learning%20a%20general%20humanoid%20whole-body%20controller%20is%20challenging%20because%20practical%20reference%20motions%20can%20exhibit%20noise%20and%20inconsistencies%20after%20being%20transferred%20to%20the%20robot%20domain%2C%20and%20local%20defects%20may%20be%20amplified%20by%20closed-loop%20execution%2C%20causing%20drift%20or%20failure%20in%20highly%20dynamic%20and%20contact-rich%20behaviors.%20We%20propose%20a%20dynamics-conditioned%20command%20aggregation%20framework%20that%20uses%20a%20causal%20temporal%20encoder%20to%20summarize%20recent%20proprioception%20and%20a%20multi-head%20cross-attention%20command%20encoder%20to%20selectively%20aggregate%20a%20context%20window%20based%20on%20the%20current%20dynamics.%20We%20further%20integrate%20a%20fall%20recovery%20curriculum%20with%20random%20unstable%20initialization%20and%20an%20annealed%20upward%20assistance%20force%20to%20improve%20robustness%20and%20disturbance%20rejection.%20The%20resulting%20policy%20requires%20only%20about%203.5%20hours%20of%20motion%20data%20and%20supports%20single-stage%20end-to-end%20training%20without%20distillation.%20The%20proposed%20method%20is%20evaluated%20under%20diverse%20reference%20inputs%20and%20challenging%20motion%20regimes%2C%20demonstrating%20zero-shot%20transfer%20to%20unseen%20motions%20as%20well%20as%20robust%20sim-to-real%20transfer%20on%20a%20physical%20humanoid%20robot.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Generalized%2520Humanoid%2520Motion%2520Tracking%26entry.906535625%3DYubiao%2520Ma%2520and%2520Han%2520Yu%2520and%2520Jiayin%2520Xie%2520and%2520Changtai%2520Lv%2520and%2520Qiang%2520Luo%2520and%2520Chi%2520Zhang%2520and%2520Yunpeng%2520Yin%2520and%2520Boyang%2520Xing%2520and%2520Xuemei%2520Ren%2520and%2520Dongdong%2520Zheng%26entry.1292438233%3DLearning%2520a%2520general%2520humanoid%2520whole-body%2520controller%2520is%2520challenging%2520because%2520practical%2520reference%2520motions%2520can%2520exhibit%2520noise%2520and%2520inconsistencies%2520after%2520being%2520transferred%2520to%2520the%2520robot%2520domain%252C%2520and%2520local%2520defects%2520may%2520be%2520amplified%2520by%2520closed-loop%2520execution%252C%2520causing%2520drift%2520or%2520failure%2520in%2520highly%2520dynamic%2520and%2520contact-rich%2520behaviors.%2520We%2520propose%2520a%2520dynamics-conditioned%2520command%2520aggregation%2520framework%2520that%2520uses%2520a%2520causal%2520temporal%2520encoder%2520to%2520summarize%2520recent%2520proprioception%2520and%2520a%2520multi-head%2520cross-attention%2520command%2520encoder%2520to%2520selectively%2520aggregate%2520a%2520context%2520window%2520based%2520on%2520the%2520current%2520dynamics.%2520We%2520further%2520integrate%2520a%2520fall%2520recovery%2520curriculum%2520with%2520random%2520unstable%2520initialization%2520and%2520an%2520annealed%2520upward%2520assistance%2520force%2520to%2520improve%2520robustness%2520and%2520disturbance%2520rejection.%2520The%2520resulting%2520policy%2520requires%2520only%2520about%25203.5%2520hours%2520of%2520motion%2520data%2520and%2520supports%2520single-stage%2520end-to-end%2520training%2520without%2520distillation.%2520The%2520proposed%2520method%2520is%2520evaluated%2520under%2520diverse%2520reference%2520inputs%2520and%2520challenging%2520motion%2520regimes%252C%2520demonstrating%2520zero-shot%2520transfer%2520to%2520unseen%2520motions%2520as%2520well%2520as%2520robust%2520sim-to-real%2520transfer%2520on%2520a%2520physical%2520humanoid%2520robot.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Generalized%20Humanoid%20Motion%20Tracking&entry.906535625=Yubiao%20Ma%20and%20Han%20Yu%20and%20Jiayin%20Xie%20and%20Changtai%20Lv%20and%20Qiang%20Luo%20and%20Chi%20Zhang%20and%20Yunpeng%20Yin%20and%20Boyang%20Xing%20and%20Xuemei%20Ren%20and%20Dongdong%20Zheng&entry.1292438233=Learning%20a%20general%20humanoid%20whole-body%20controller%20is%20challenging%20because%20practical%20reference%20motions%20can%20exhibit%20noise%20and%20inconsistencies%20after%20being%20transferred%20to%20the%20robot%20domain%2C%20and%20local%20defects%20may%20be%20amplified%20by%20closed-loop%20execution%2C%20causing%20drift%20or%20failure%20in%20highly%20dynamic%20and%20contact-rich%20behaviors.%20We%20propose%20a%20dynamics-conditioned%20command%20aggregation%20framework%20that%20uses%20a%20causal%20temporal%20encoder%20to%20summarize%20recent%20proprioception%20and%20a%20multi-head%20cross-attention%20command%20encoder%20to%20selectively%20aggregate%20a%20context%20window%20based%20on%20the%20current%20dynamics.%20We%20further%20integrate%20a%20fall%20recovery%20curriculum%20with%20random%20unstable%20initialization%20and%20an%20annealed%20upward%20assistance%20force%20to%20improve%20robustness%20and%20disturbance%20rejection.%20The%20resulting%20policy%20requires%20only%20about%203.5%20hours%20of%20motion%20data%20and%20supports%20single-stage%20end-to-end%20training%20without%20distillation.%20The%20proposed%20method%20is%20evaluated%20under%20diverse%20reference%20inputs%20and%20challenging%20motion%20regimes%2C%20demonstrating%20zero-shot%20transfer%20to%20unseen%20motions%20as%20well%20as%20robust%20sim-to-real%20transfer%20on%20a%20physical%20humanoid%20robot.&entry.1838667208=http%3A//arxiv.org/abs/2601.23080v1&entry.124074799=Read"},
{"title": "LAVA: Explainability for Unsupervised Latent Embeddings", "author": "Ivan Stresec and Joana P. Gon\u00e7alves", "abstract": "Unsupervised black-box models are drivers of scientific discovery, yet are difficult to interpret, as their output is often a multidimensional embedding rather than a well-defined target. While explainability for supervised learning uncovers how input features contribute to predictions, its unsupervised counterpart should relate input features to the structure of the learned embeddings. However, adaptations of supervised model explainability for unsupervised learning provide either single-sample or dataset-summary explanations, remaining too fine-grained or reductive to be meaningful, and cannot explain embeddings without mapping functions. To bridge this gap, we propose LAVA, a post-hoc model-agnostic method to explain local embedding organization through feature covariation in the original input data. LAVA explanations comprise modules, capturing local subpatterns of input feature correlation that reoccur globally across the embeddings. LAVA delivers stable explanations at a desired level of granularity, revealing domain-relevant patterns such as visual parts of images or disease signals in cellular processes, otherwise missed by existing methods.", "link": "http://arxiv.org/abs/2509.21149v2", "date": "2026-01-30", "relevancy": 2.4419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAVA%3A%20Explainability%20for%20Unsupervised%20Latent%20Embeddings&body=Title%3A%20LAVA%3A%20Explainability%20for%20Unsupervised%20Latent%20Embeddings%0AAuthor%3A%20Ivan%20Stresec%20and%20Joana%20P.%20Gon%C3%A7alves%0AAbstract%3A%20Unsupervised%20black-box%20models%20are%20drivers%20of%20scientific%20discovery%2C%20yet%20are%20difficult%20to%20interpret%2C%20as%20their%20output%20is%20often%20a%20multidimensional%20embedding%20rather%20than%20a%20well-defined%20target.%20While%20explainability%20for%20supervised%20learning%20uncovers%20how%20input%20features%20contribute%20to%20predictions%2C%20its%20unsupervised%20counterpart%20should%20relate%20input%20features%20to%20the%20structure%20of%20the%20learned%20embeddings.%20However%2C%20adaptations%20of%20supervised%20model%20explainability%20for%20unsupervised%20learning%20provide%20either%20single-sample%20or%20dataset-summary%20explanations%2C%20remaining%20too%20fine-grained%20or%20reductive%20to%20be%20meaningful%2C%20and%20cannot%20explain%20embeddings%20without%20mapping%20functions.%20To%20bridge%20this%20gap%2C%20we%20propose%20LAVA%2C%20a%20post-hoc%20model-agnostic%20method%20to%20explain%20local%20embedding%20organization%20through%20feature%20covariation%20in%20the%20original%20input%20data.%20LAVA%20explanations%20comprise%20modules%2C%20capturing%20local%20subpatterns%20of%20input%20feature%20correlation%20that%20reoccur%20globally%20across%20the%20embeddings.%20LAVA%20delivers%20stable%20explanations%20at%20a%20desired%20level%20of%20granularity%2C%20revealing%20domain-relevant%20patterns%20such%20as%20visual%20parts%20of%20images%20or%20disease%20signals%20in%20cellular%20processes%2C%20otherwise%20missed%20by%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21149v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAVA%253A%2520Explainability%2520for%2520Unsupervised%2520Latent%2520Embeddings%26entry.906535625%3DIvan%2520Stresec%2520and%2520Joana%2520P.%2520Gon%25C3%25A7alves%26entry.1292438233%3DUnsupervised%2520black-box%2520models%2520are%2520drivers%2520of%2520scientific%2520discovery%252C%2520yet%2520are%2520difficult%2520to%2520interpret%252C%2520as%2520their%2520output%2520is%2520often%2520a%2520multidimensional%2520embedding%2520rather%2520than%2520a%2520well-defined%2520target.%2520While%2520explainability%2520for%2520supervised%2520learning%2520uncovers%2520how%2520input%2520features%2520contribute%2520to%2520predictions%252C%2520its%2520unsupervised%2520counterpart%2520should%2520relate%2520input%2520features%2520to%2520the%2520structure%2520of%2520the%2520learned%2520embeddings.%2520However%252C%2520adaptations%2520of%2520supervised%2520model%2520explainability%2520for%2520unsupervised%2520learning%2520provide%2520either%2520single-sample%2520or%2520dataset-summary%2520explanations%252C%2520remaining%2520too%2520fine-grained%2520or%2520reductive%2520to%2520be%2520meaningful%252C%2520and%2520cannot%2520explain%2520embeddings%2520without%2520mapping%2520functions.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520LAVA%252C%2520a%2520post-hoc%2520model-agnostic%2520method%2520to%2520explain%2520local%2520embedding%2520organization%2520through%2520feature%2520covariation%2520in%2520the%2520original%2520input%2520data.%2520LAVA%2520explanations%2520comprise%2520modules%252C%2520capturing%2520local%2520subpatterns%2520of%2520input%2520feature%2520correlation%2520that%2520reoccur%2520globally%2520across%2520the%2520embeddings.%2520LAVA%2520delivers%2520stable%2520explanations%2520at%2520a%2520desired%2520level%2520of%2520granularity%252C%2520revealing%2520domain-relevant%2520patterns%2520such%2520as%2520visual%2520parts%2520of%2520images%2520or%2520disease%2520signals%2520in%2520cellular%2520processes%252C%2520otherwise%2520missed%2520by%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21149v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAVA%3A%20Explainability%20for%20Unsupervised%20Latent%20Embeddings&entry.906535625=Ivan%20Stresec%20and%20Joana%20P.%20Gon%C3%A7alves&entry.1292438233=Unsupervised%20black-box%20models%20are%20drivers%20of%20scientific%20discovery%2C%20yet%20are%20difficult%20to%20interpret%2C%20as%20their%20output%20is%20often%20a%20multidimensional%20embedding%20rather%20than%20a%20well-defined%20target.%20While%20explainability%20for%20supervised%20learning%20uncovers%20how%20input%20features%20contribute%20to%20predictions%2C%20its%20unsupervised%20counterpart%20should%20relate%20input%20features%20to%20the%20structure%20of%20the%20learned%20embeddings.%20However%2C%20adaptations%20of%20supervised%20model%20explainability%20for%20unsupervised%20learning%20provide%20either%20single-sample%20or%20dataset-summary%20explanations%2C%20remaining%20too%20fine-grained%20or%20reductive%20to%20be%20meaningful%2C%20and%20cannot%20explain%20embeddings%20without%20mapping%20functions.%20To%20bridge%20this%20gap%2C%20we%20propose%20LAVA%2C%20a%20post-hoc%20model-agnostic%20method%20to%20explain%20local%20embedding%20organization%20through%20feature%20covariation%20in%20the%20original%20input%20data.%20LAVA%20explanations%20comprise%20modules%2C%20capturing%20local%20subpatterns%20of%20input%20feature%20correlation%20that%20reoccur%20globally%20across%20the%20embeddings.%20LAVA%20delivers%20stable%20explanations%20at%20a%20desired%20level%20of%20granularity%2C%20revealing%20domain-relevant%20patterns%20such%20as%20visual%20parts%20of%20images%20or%20disease%20signals%20in%20cellular%20processes%2C%20otherwise%20missed%20by%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2509.21149v2&entry.124074799=Read"},
{"title": "Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning", "author": "Arian Raje and Baris Askin and Divyansh Jhunjhunwala and Gauri Joshi", "abstract": "Large language models (LLMs) have not yet effectively leveraged the vast amounts of edge-device data, and federated learning (FL) offers a promising paradigm to collaboratively fine-tune LLMs without transferring private edge data to the cloud. To operate within the computation and communication constraints of edge devices, recent literature on federated fine-tuning of LLMs proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient methods. However, LoRA-based methods suffer from accuracy degradation in FL settings, primarily because of data and computational heterogeneity across clients. We propose Ravan, an adaptive multi-head LoRA method that balances parameter efficiency and model expressivity by reparameterizing the weight updates as the sum of multiple LoRA heads $s_i\\textbf{B}_i\\textbf{H}_i\\textbf{A}_i$ in which only the core matrices $\\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These trainable scaling factors let the optimization focus on the most useful heads, recovering a higher-rank approximation of the full update without increasing the number of communicated parameters since clients upload $s_i\\textbf{H}_i$ directly. Experiments on vision and language benchmarks show that Ravan improves test accuracy by $2-8\\%$ over prior parameter-efficient baselines, making it a robust and scalable solution for federated fine-tuning of LLMs.", "link": "http://arxiv.org/abs/2506.05568v2", "date": "2026-01-30", "relevancy": 2.4319, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4851}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ravan%3A%20Multi-Head%20Low-Rank%20Adaptation%20for%20Federated%20Fine-Tuning&body=Title%3A%20Ravan%3A%20Multi-Head%20Low-Rank%20Adaptation%20for%20Federated%20Fine-Tuning%0AAuthor%3A%20Arian%20Raje%20and%20Baris%20Askin%20and%20Divyansh%20Jhunjhunwala%20and%20Gauri%20Joshi%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20not%20yet%20effectively%20leveraged%20the%20vast%20amounts%20of%20edge-device%20data%2C%20and%20federated%20learning%20%28FL%29%20offers%20a%20promising%20paradigm%20to%20collaboratively%20fine-tune%20LLMs%20without%20transferring%20private%20edge%20data%20to%20the%20cloud.%20To%20operate%20within%20the%20computation%20and%20communication%20constraints%20of%20edge%20devices%2C%20recent%20literature%20on%20federated%20fine-tuning%20of%20LLMs%20proposes%20the%20use%20of%20low-rank%20adaptation%20%28LoRA%29%20and%20similar%20parameter-efficient%20methods.%20However%2C%20LoRA-based%20methods%20suffer%20from%20accuracy%20degradation%20in%20FL%20settings%2C%20primarily%20because%20of%20data%20and%20computational%20heterogeneity%20across%20clients.%20We%20propose%20Ravan%2C%20an%20adaptive%20multi-head%20LoRA%20method%20that%20balances%20parameter%20efficiency%20and%20model%20expressivity%20by%20reparameterizing%20the%20weight%20updates%20as%20the%20sum%20of%20multiple%20LoRA%20heads%20%24s_i%5Ctextbf%7BB%7D_i%5Ctextbf%7BH%7D_i%5Ctextbf%7BA%7D_i%24%20in%20which%20only%20the%20core%20matrices%20%24%5Ctextbf%7BH%7D_i%24%20and%20their%20lightweight%20scaling%20factors%20%24s_i%24%20are%20trained.%20These%20trainable%20scaling%20factors%20let%20the%20optimization%20focus%20on%20the%20most%20useful%20heads%2C%20recovering%20a%20higher-rank%20approximation%20of%20the%20full%20update%20without%20increasing%20the%20number%20of%20communicated%20parameters%20since%20clients%20upload%20%24s_i%5Ctextbf%7BH%7D_i%24%20directly.%20Experiments%20on%20vision%20and%20language%20benchmarks%20show%20that%20Ravan%20improves%20test%20accuracy%20by%20%242-8%5C%25%24%20over%20prior%20parameter-efficient%20baselines%2C%20making%20it%20a%20robust%20and%20scalable%20solution%20for%20federated%20fine-tuning%20of%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRavan%253A%2520Multi-Head%2520Low-Rank%2520Adaptation%2520for%2520Federated%2520Fine-Tuning%26entry.906535625%3DArian%2520Raje%2520and%2520Baris%2520Askin%2520and%2520Divyansh%2520Jhunjhunwala%2520and%2520Gauri%2520Joshi%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520not%2520yet%2520effectively%2520leveraged%2520the%2520vast%2520amounts%2520of%2520edge-device%2520data%252C%2520and%2520federated%2520learning%2520%2528FL%2529%2520offers%2520a%2520promising%2520paradigm%2520to%2520collaboratively%2520fine-tune%2520LLMs%2520without%2520transferring%2520private%2520edge%2520data%2520to%2520the%2520cloud.%2520To%2520operate%2520within%2520the%2520computation%2520and%2520communication%2520constraints%2520of%2520edge%2520devices%252C%2520recent%2520literature%2520on%2520federated%2520fine-tuning%2520of%2520LLMs%2520proposes%2520the%2520use%2520of%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520and%2520similar%2520parameter-efficient%2520methods.%2520However%252C%2520LoRA-based%2520methods%2520suffer%2520from%2520accuracy%2520degradation%2520in%2520FL%2520settings%252C%2520primarily%2520because%2520of%2520data%2520and%2520computational%2520heterogeneity%2520across%2520clients.%2520We%2520propose%2520Ravan%252C%2520an%2520adaptive%2520multi-head%2520LoRA%2520method%2520that%2520balances%2520parameter%2520efficiency%2520and%2520model%2520expressivity%2520by%2520reparameterizing%2520the%2520weight%2520updates%2520as%2520the%2520sum%2520of%2520multiple%2520LoRA%2520heads%2520%2524s_i%255Ctextbf%257BB%257D_i%255Ctextbf%257BH%257D_i%255Ctextbf%257BA%257D_i%2524%2520in%2520which%2520only%2520the%2520core%2520matrices%2520%2524%255Ctextbf%257BH%257D_i%2524%2520and%2520their%2520lightweight%2520scaling%2520factors%2520%2524s_i%2524%2520are%2520trained.%2520These%2520trainable%2520scaling%2520factors%2520let%2520the%2520optimization%2520focus%2520on%2520the%2520most%2520useful%2520heads%252C%2520recovering%2520a%2520higher-rank%2520approximation%2520of%2520the%2520full%2520update%2520without%2520increasing%2520the%2520number%2520of%2520communicated%2520parameters%2520since%2520clients%2520upload%2520%2524s_i%255Ctextbf%257BH%257D_i%2524%2520directly.%2520Experiments%2520on%2520vision%2520and%2520language%2520benchmarks%2520show%2520that%2520Ravan%2520improves%2520test%2520accuracy%2520by%2520%25242-8%255C%2525%2524%2520over%2520prior%2520parameter-efficient%2520baselines%252C%2520making%2520it%2520a%2520robust%2520and%2520scalable%2520solution%2520for%2520federated%2520fine-tuning%2520of%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ravan%3A%20Multi-Head%20Low-Rank%20Adaptation%20for%20Federated%20Fine-Tuning&entry.906535625=Arian%20Raje%20and%20Baris%20Askin%20and%20Divyansh%20Jhunjhunwala%20and%20Gauri%20Joshi&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20not%20yet%20effectively%20leveraged%20the%20vast%20amounts%20of%20edge-device%20data%2C%20and%20federated%20learning%20%28FL%29%20offers%20a%20promising%20paradigm%20to%20collaboratively%20fine-tune%20LLMs%20without%20transferring%20private%20edge%20data%20to%20the%20cloud.%20To%20operate%20within%20the%20computation%20and%20communication%20constraints%20of%20edge%20devices%2C%20recent%20literature%20on%20federated%20fine-tuning%20of%20LLMs%20proposes%20the%20use%20of%20low-rank%20adaptation%20%28LoRA%29%20and%20similar%20parameter-efficient%20methods.%20However%2C%20LoRA-based%20methods%20suffer%20from%20accuracy%20degradation%20in%20FL%20settings%2C%20primarily%20because%20of%20data%20and%20computational%20heterogeneity%20across%20clients.%20We%20propose%20Ravan%2C%20an%20adaptive%20multi-head%20LoRA%20method%20that%20balances%20parameter%20efficiency%20and%20model%20expressivity%20by%20reparameterizing%20the%20weight%20updates%20as%20the%20sum%20of%20multiple%20LoRA%20heads%20%24s_i%5Ctextbf%7BB%7D_i%5Ctextbf%7BH%7D_i%5Ctextbf%7BA%7D_i%24%20in%20which%20only%20the%20core%20matrices%20%24%5Ctextbf%7BH%7D_i%24%20and%20their%20lightweight%20scaling%20factors%20%24s_i%24%20are%20trained.%20These%20trainable%20scaling%20factors%20let%20the%20optimization%20focus%20on%20the%20most%20useful%20heads%2C%20recovering%20a%20higher-rank%20approximation%20of%20the%20full%20update%20without%20increasing%20the%20number%20of%20communicated%20parameters%20since%20clients%20upload%20%24s_i%5Ctextbf%7BH%7D_i%24%20directly.%20Experiments%20on%20vision%20and%20language%20benchmarks%20show%20that%20Ravan%20improves%20test%20accuracy%20by%20%242-8%5C%25%24%20over%20prior%20parameter-efficient%20baselines%2C%20making%20it%20a%20robust%20and%20scalable%20solution%20for%20federated%20fine-tuning%20of%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2506.05568v2&entry.124074799=Read"},
{"title": "Clipping-Free Policy Optimization for Large Language Models", "author": "\u00d6mer Veysel \u00c7a\u011fatan and Bar\u0131\u015f Akg\u00fcn and G\u00f6zde G\u00fcl \u015eahin and Xuandong Zhao", "abstract": "Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.", "link": "http://arxiv.org/abs/2601.22801v1", "date": "2026-01-30", "relevancy": 2.4283, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4871}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clipping-Free%20Policy%20Optimization%20for%20Large%20Language%20Models&body=Title%3A%20Clipping-Free%20Policy%20Optimization%20for%20Large%20Language%20Models%0AAuthor%3A%20%C3%96mer%20Veysel%20%C3%87a%C4%9Fatan%20and%20Bar%C4%B1%C5%9F%20Akg%C3%BCn%20and%20G%C3%B6zde%20G%C3%BCl%20%C5%9Eahin%20and%20Xuandong%20Zhao%0AAbstract%3A%20Reinforcement%20learning%20has%20become%20central%20to%20post-training%20large%20language%20models%2C%20yet%20dominant%20algorithms%20rely%20on%20clipping%20mechanisms%20that%20introduce%20optimization%20issues%20at%20scale%2C%20including%20zero-gradient%20regions%2C%20reward%20hacking%2C%20and%20training%20instability.%20We%20propose%20Clipping-Free%20Policy%20Optimization%20%28CFPO%29%2C%20which%20replaces%20heuristic%20clipping%20with%20a%20convex%20quadratic%20penalty%20derived%20from%20Total%20Variation%20divergence%20constraints%2C%20yielding%20an%20everywhere-differentiable%20objective%20that%20enforces%20stable%20policy%20updates%20without%20hard%20boundaries.%20We%20evaluate%20CFPO%20across%20both%20reasoning%20and%20alignment%20settings.%20In%20reasoning%2C%20CFPO%20matches%20clipping-based%20methods%20on%20downstream%20benchmarks%20while%20extending%20the%20stable%20training%20regime.%20In%20alignment%2C%20CFPO%20mitigates%20verbosity%20exploitation%20and%20reduces%20capability%20degradation%2C%20while%20achieving%20competitive%20instruction-following%20performance.%20CFPO%20requires%20only%20a%20one-line%20code%20change%20and%20no%20additional%20hyperparameters.%20Our%20results%20suggest%20that%20CFPO%20is%20a%20promising%20drop-in%20alternative%20to%20clipping-based%20methods%20for%20LLM%20post-training.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClipping-Free%2520Policy%2520Optimization%2520for%2520Large%2520Language%2520Models%26entry.906535625%3D%25C3%2596mer%2520Veysel%2520%25C3%2587a%25C4%259Fatan%2520and%2520Bar%25C4%25B1%25C5%259F%2520Akg%25C3%25BCn%2520and%2520G%25C3%25B6zde%2520G%25C3%25BCl%2520%25C5%259Eahin%2520and%2520Xuandong%2520Zhao%26entry.1292438233%3DReinforcement%2520learning%2520has%2520become%2520central%2520to%2520post-training%2520large%2520language%2520models%252C%2520yet%2520dominant%2520algorithms%2520rely%2520on%2520clipping%2520mechanisms%2520that%2520introduce%2520optimization%2520issues%2520at%2520scale%252C%2520including%2520zero-gradient%2520regions%252C%2520reward%2520hacking%252C%2520and%2520training%2520instability.%2520We%2520propose%2520Clipping-Free%2520Policy%2520Optimization%2520%2528CFPO%2529%252C%2520which%2520replaces%2520heuristic%2520clipping%2520with%2520a%2520convex%2520quadratic%2520penalty%2520derived%2520from%2520Total%2520Variation%2520divergence%2520constraints%252C%2520yielding%2520an%2520everywhere-differentiable%2520objective%2520that%2520enforces%2520stable%2520policy%2520updates%2520without%2520hard%2520boundaries.%2520We%2520evaluate%2520CFPO%2520across%2520both%2520reasoning%2520and%2520alignment%2520settings.%2520In%2520reasoning%252C%2520CFPO%2520matches%2520clipping-based%2520methods%2520on%2520downstream%2520benchmarks%2520while%2520extending%2520the%2520stable%2520training%2520regime.%2520In%2520alignment%252C%2520CFPO%2520mitigates%2520verbosity%2520exploitation%2520and%2520reduces%2520capability%2520degradation%252C%2520while%2520achieving%2520competitive%2520instruction-following%2520performance.%2520CFPO%2520requires%2520only%2520a%2520one-line%2520code%2520change%2520and%2520no%2520additional%2520hyperparameters.%2520Our%2520results%2520suggest%2520that%2520CFPO%2520is%2520a%2520promising%2520drop-in%2520alternative%2520to%2520clipping-based%2520methods%2520for%2520LLM%2520post-training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clipping-Free%20Policy%20Optimization%20for%20Large%20Language%20Models&entry.906535625=%C3%96mer%20Veysel%20%C3%87a%C4%9Fatan%20and%20Bar%C4%B1%C5%9F%20Akg%C3%BCn%20and%20G%C3%B6zde%20G%C3%BCl%20%C5%9Eahin%20and%20Xuandong%20Zhao&entry.1292438233=Reinforcement%20learning%20has%20become%20central%20to%20post-training%20large%20language%20models%2C%20yet%20dominant%20algorithms%20rely%20on%20clipping%20mechanisms%20that%20introduce%20optimization%20issues%20at%20scale%2C%20including%20zero-gradient%20regions%2C%20reward%20hacking%2C%20and%20training%20instability.%20We%20propose%20Clipping-Free%20Policy%20Optimization%20%28CFPO%29%2C%20which%20replaces%20heuristic%20clipping%20with%20a%20convex%20quadratic%20penalty%20derived%20from%20Total%20Variation%20divergence%20constraints%2C%20yielding%20an%20everywhere-differentiable%20objective%20that%20enforces%20stable%20policy%20updates%20without%20hard%20boundaries.%20We%20evaluate%20CFPO%20across%20both%20reasoning%20and%20alignment%20settings.%20In%20reasoning%2C%20CFPO%20matches%20clipping-based%20methods%20on%20downstream%20benchmarks%20while%20extending%20the%20stable%20training%20regime.%20In%20alignment%2C%20CFPO%20mitigates%20verbosity%20exploitation%20and%20reduces%20capability%20degradation%2C%20while%20achieving%20competitive%20instruction-following%20performance.%20CFPO%20requires%20only%20a%20one-line%20code%20change%20and%20no%20additional%20hyperparameters.%20Our%20results%20suggest%20that%20CFPO%20is%20a%20promising%20drop-in%20alternative%20to%20clipping-based%20methods%20for%20LLM%20post-training.&entry.1838667208=http%3A//arxiv.org/abs/2601.22801v1&entry.124074799=Read"},
{"title": "Perplexity Cannot Always Tell Right from Wrong", "author": "Petar Veli\u010dkovi\u0107 and Federico Barbero and Christos Perivolaropoulos and Simon Osindero and Razvan Pascanu", "abstract": "Perplexity -- a function measuring a model's overall level of \"surprise\" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.", "link": "http://arxiv.org/abs/2601.22950v1", "date": "2026-01-30", "relevancy": 2.4224, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perplexity%20Cannot%20Always%20Tell%20Right%20from%20Wrong&body=Title%3A%20Perplexity%20Cannot%20Always%20Tell%20Right%20from%20Wrong%0AAuthor%3A%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Federico%20Barbero%20and%20Christos%20Perivolaropoulos%20and%20Simon%20Osindero%20and%20Razvan%20Pascanu%0AAbstract%3A%20Perplexity%20--%20a%20function%20measuring%20a%20model%27s%20overall%20level%20of%20%22surprise%22%20when%20encountering%20a%20particular%20output%20--%20has%20gained%20significant%20traction%20in%20recent%20years%2C%20both%20as%20a%20loss%20function%20and%20as%20a%20simple-to-compute%20metric%20of%20model%20quality.%20Prior%20studies%20have%20pointed%20out%20several%20limitations%20of%20perplexity%2C%20often%20from%20an%20empirical%20manner.%20Here%20we%20leverage%20recent%20results%20on%20Transformer%20continuity%20to%20show%20in%20a%20rigorous%20manner%20how%20perplexity%20may%20be%20an%20unsuitable%20metric%20for%20model%20selection.%20Specifically%2C%20we%20prove%20that%2C%20if%20there%20is%20any%20sequence%20that%20a%20compact%20decoder-only%20Transformer%20model%20predicts%20accurately%20and%20confidently%20--%20a%20necessary%20pre-requisite%20for%20strong%20generalisation%20--%20it%20must%20imply%20existence%20of%20another%20sequence%20with%20very%20low%20perplexity%2C%20but%20not%20predicted%20correctly%20by%20that%20same%20model.%20Further%2C%20by%20analytically%20studying%20iso-perplexity%20plots%2C%20we%20find%20that%20perplexity%20will%20not%20always%20select%20for%20the%20more%20accurate%20model%20--%20rather%2C%20any%20increase%20in%20model%20confidence%20must%20be%20accompanied%20by%20a%20commensurate%20rise%20in%20accuracy%20for%20the%20new%20model%20to%20be%20selected.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerplexity%2520Cannot%2520Always%2520Tell%2520Right%2520from%2520Wrong%26entry.906535625%3DPetar%2520Veli%25C4%258Dkovi%25C4%2587%2520and%2520Federico%2520Barbero%2520and%2520Christos%2520Perivolaropoulos%2520and%2520Simon%2520Osindero%2520and%2520Razvan%2520Pascanu%26entry.1292438233%3DPerplexity%2520--%2520a%2520function%2520measuring%2520a%2520model%2527s%2520overall%2520level%2520of%2520%2522surprise%2522%2520when%2520encountering%2520a%2520particular%2520output%2520--%2520has%2520gained%2520significant%2520traction%2520in%2520recent%2520years%252C%2520both%2520as%2520a%2520loss%2520function%2520and%2520as%2520a%2520simple-to-compute%2520metric%2520of%2520model%2520quality.%2520Prior%2520studies%2520have%2520pointed%2520out%2520several%2520limitations%2520of%2520perplexity%252C%2520often%2520from%2520an%2520empirical%2520manner.%2520Here%2520we%2520leverage%2520recent%2520results%2520on%2520Transformer%2520continuity%2520to%2520show%2520in%2520a%2520rigorous%2520manner%2520how%2520perplexity%2520may%2520be%2520an%2520unsuitable%2520metric%2520for%2520model%2520selection.%2520Specifically%252C%2520we%2520prove%2520that%252C%2520if%2520there%2520is%2520any%2520sequence%2520that%2520a%2520compact%2520decoder-only%2520Transformer%2520model%2520predicts%2520accurately%2520and%2520confidently%2520--%2520a%2520necessary%2520pre-requisite%2520for%2520strong%2520generalisation%2520--%2520it%2520must%2520imply%2520existence%2520of%2520another%2520sequence%2520with%2520very%2520low%2520perplexity%252C%2520but%2520not%2520predicted%2520correctly%2520by%2520that%2520same%2520model.%2520Further%252C%2520by%2520analytically%2520studying%2520iso-perplexity%2520plots%252C%2520we%2520find%2520that%2520perplexity%2520will%2520not%2520always%2520select%2520for%2520the%2520more%2520accurate%2520model%2520--%2520rather%252C%2520any%2520increase%2520in%2520model%2520confidence%2520must%2520be%2520accompanied%2520by%2520a%2520commensurate%2520rise%2520in%2520accuracy%2520for%2520the%2520new%2520model%2520to%2520be%2520selected.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perplexity%20Cannot%20Always%20Tell%20Right%20from%20Wrong&entry.906535625=Petar%20Veli%C4%8Dkovi%C4%87%20and%20Federico%20Barbero%20and%20Christos%20Perivolaropoulos%20and%20Simon%20Osindero%20and%20Razvan%20Pascanu&entry.1292438233=Perplexity%20--%20a%20function%20measuring%20a%20model%27s%20overall%20level%20of%20%22surprise%22%20when%20encountering%20a%20particular%20output%20--%20has%20gained%20significant%20traction%20in%20recent%20years%2C%20both%20as%20a%20loss%20function%20and%20as%20a%20simple-to-compute%20metric%20of%20model%20quality.%20Prior%20studies%20have%20pointed%20out%20several%20limitations%20of%20perplexity%2C%20often%20from%20an%20empirical%20manner.%20Here%20we%20leverage%20recent%20results%20on%20Transformer%20continuity%20to%20show%20in%20a%20rigorous%20manner%20how%20perplexity%20may%20be%20an%20unsuitable%20metric%20for%20model%20selection.%20Specifically%2C%20we%20prove%20that%2C%20if%20there%20is%20any%20sequence%20that%20a%20compact%20decoder-only%20Transformer%20model%20predicts%20accurately%20and%20confidently%20--%20a%20necessary%20pre-requisite%20for%20strong%20generalisation%20--%20it%20must%20imply%20existence%20of%20another%20sequence%20with%20very%20low%20perplexity%2C%20but%20not%20predicted%20correctly%20by%20that%20same%20model.%20Further%2C%20by%20analytically%20studying%20iso-perplexity%20plots%2C%20we%20find%20that%20perplexity%20will%20not%20always%20select%20for%20the%20more%20accurate%20model%20--%20rather%2C%20any%20increase%20in%20model%20confidence%20must%20be%20accompanied%20by%20a%20commensurate%20rise%20in%20accuracy%20for%20the%20new%20model%20to%20be%20selected.&entry.1838667208=http%3A//arxiv.org/abs/2601.22950v1&entry.124074799=Read"},
{"title": "TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart Sound Segmentation", "author": "Peihong Zhang and Zhixin Li and Yuxuan Liu and Rui Sang and Yiqiang Cai and Yizhou Tan and Shengchen Li", "abstract": "Deep learning approaches for heart-sound (PCG) segmentation built on time-frequency features can be accurate but often rely on large expert-labeled datasets, limiting robustness and deployment. We present TopSeg, a topological representation-centric framework that encodes PCG dynamics with multi-scale topological features and decodes them using a lightweight temporal convolutional network (TCN) with an order- and duration-constrained inference step. To evaluate data efficiency and generalization, we train exclusively on PhysioNet 2016 dataset with subject-level subsampling and perform external validation on CirCor dataset. Under matched-capacity decoders, the topological features consistently outperform spectrogram and envelope inputs, with the largest margins at low data budgets; as a full system, TopSeg surpasses representative end-to-end baselines trained on their native inputs under the same budgets while remaining competitive at full data. Ablations at 10% training confirm that all scales contribute and that combining H_0 and H_1 yields more reliable S1/S2 localization and boundary stability. These results indicate that topology-aware representations provide a strong inductive bias for data-efficient, cross-dataset PCG segmentation, supporting practical use when labeled data are limited.", "link": "http://arxiv.org/abs/2510.17346v2", "date": "2026-01-30", "relevancy": 2.4143, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4824}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopSeg%3A%20A%20Multi-Scale%20Topological%20Framework%20for%20Data-Efficient%20Heart%20Sound%20Segmentation&body=Title%3A%20TopSeg%3A%20A%20Multi-Scale%20Topological%20Framework%20for%20Data-Efficient%20Heart%20Sound%20Segmentation%0AAuthor%3A%20Peihong%20Zhang%20and%20Zhixin%20Li%20and%20Yuxuan%20Liu%20and%20Rui%20Sang%20and%20Yiqiang%20Cai%20and%20Yizhou%20Tan%20and%20Shengchen%20Li%0AAbstract%3A%20Deep%20learning%20approaches%20for%20heart-sound%20%28PCG%29%20segmentation%20built%20on%20time-frequency%20features%20can%20be%20accurate%20but%20often%20rely%20on%20large%20expert-labeled%20datasets%2C%20limiting%20robustness%20and%20deployment.%20We%20present%20TopSeg%2C%20a%20topological%20representation-centric%20framework%20that%20encodes%20PCG%20dynamics%20with%20multi-scale%20topological%20features%20and%20decodes%20them%20using%20a%20lightweight%20temporal%20convolutional%20network%20%28TCN%29%20with%20an%20order-%20and%20duration-constrained%20inference%20step.%20To%20evaluate%20data%20efficiency%20and%20generalization%2C%20we%20train%20exclusively%20on%20PhysioNet%202016%20dataset%20with%20subject-level%20subsampling%20and%20perform%20external%20validation%20on%20CirCor%20dataset.%20Under%20matched-capacity%20decoders%2C%20the%20topological%20features%20consistently%20outperform%20spectrogram%20and%20envelope%20inputs%2C%20with%20the%20largest%20margins%20at%20low%20data%20budgets%3B%20as%20a%20full%20system%2C%20TopSeg%20surpasses%20representative%20end-to-end%20baselines%20trained%20on%20their%20native%20inputs%20under%20the%20same%20budgets%20while%20remaining%20competitive%20at%20full%20data.%20Ablations%20at%2010%25%20training%20confirm%20that%20all%20scales%20contribute%20and%20that%20combining%20H_0%20and%20H_1%20yields%20more%20reliable%20S1/S2%20localization%20and%20boundary%20stability.%20These%20results%20indicate%20that%20topology-aware%20representations%20provide%20a%20strong%20inductive%20bias%20for%20data-efficient%2C%20cross-dataset%20PCG%20segmentation%2C%20supporting%20practical%20use%20when%20labeled%20data%20are%20limited.%0ALink%3A%20http%3A//arxiv.org/abs/2510.17346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopSeg%253A%2520A%2520Multi-Scale%2520Topological%2520Framework%2520for%2520Data-Efficient%2520Heart%2520Sound%2520Segmentation%26entry.906535625%3DPeihong%2520Zhang%2520and%2520Zhixin%2520Li%2520and%2520Yuxuan%2520Liu%2520and%2520Rui%2520Sang%2520and%2520Yiqiang%2520Cai%2520and%2520Yizhou%2520Tan%2520and%2520Shengchen%2520Li%26entry.1292438233%3DDeep%2520learning%2520approaches%2520for%2520heart-sound%2520%2528PCG%2529%2520segmentation%2520built%2520on%2520time-frequency%2520features%2520can%2520be%2520accurate%2520but%2520often%2520rely%2520on%2520large%2520expert-labeled%2520datasets%252C%2520limiting%2520robustness%2520and%2520deployment.%2520We%2520present%2520TopSeg%252C%2520a%2520topological%2520representation-centric%2520framework%2520that%2520encodes%2520PCG%2520dynamics%2520with%2520multi-scale%2520topological%2520features%2520and%2520decodes%2520them%2520using%2520a%2520lightweight%2520temporal%2520convolutional%2520network%2520%2528TCN%2529%2520with%2520an%2520order-%2520and%2520duration-constrained%2520inference%2520step.%2520To%2520evaluate%2520data%2520efficiency%2520and%2520generalization%252C%2520we%2520train%2520exclusively%2520on%2520PhysioNet%25202016%2520dataset%2520with%2520subject-level%2520subsampling%2520and%2520perform%2520external%2520validation%2520on%2520CirCor%2520dataset.%2520Under%2520matched-capacity%2520decoders%252C%2520the%2520topological%2520features%2520consistently%2520outperform%2520spectrogram%2520and%2520envelope%2520inputs%252C%2520with%2520the%2520largest%2520margins%2520at%2520low%2520data%2520budgets%253B%2520as%2520a%2520full%2520system%252C%2520TopSeg%2520surpasses%2520representative%2520end-to-end%2520baselines%2520trained%2520on%2520their%2520native%2520inputs%2520under%2520the%2520same%2520budgets%2520while%2520remaining%2520competitive%2520at%2520full%2520data.%2520Ablations%2520at%252010%2525%2520training%2520confirm%2520that%2520all%2520scales%2520contribute%2520and%2520that%2520combining%2520H_0%2520and%2520H_1%2520yields%2520more%2520reliable%2520S1/S2%2520localization%2520and%2520boundary%2520stability.%2520These%2520results%2520indicate%2520that%2520topology-aware%2520representations%2520provide%2520a%2520strong%2520inductive%2520bias%2520for%2520data-efficient%252C%2520cross-dataset%2520PCG%2520segmentation%252C%2520supporting%2520practical%2520use%2520when%2520labeled%2520data%2520are%2520limited.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopSeg%3A%20A%20Multi-Scale%20Topological%20Framework%20for%20Data-Efficient%20Heart%20Sound%20Segmentation&entry.906535625=Peihong%20Zhang%20and%20Zhixin%20Li%20and%20Yuxuan%20Liu%20and%20Rui%20Sang%20and%20Yiqiang%20Cai%20and%20Yizhou%20Tan%20and%20Shengchen%20Li&entry.1292438233=Deep%20learning%20approaches%20for%20heart-sound%20%28PCG%29%20segmentation%20built%20on%20time-frequency%20features%20can%20be%20accurate%20but%20often%20rely%20on%20large%20expert-labeled%20datasets%2C%20limiting%20robustness%20and%20deployment.%20We%20present%20TopSeg%2C%20a%20topological%20representation-centric%20framework%20that%20encodes%20PCG%20dynamics%20with%20multi-scale%20topological%20features%20and%20decodes%20them%20using%20a%20lightweight%20temporal%20convolutional%20network%20%28TCN%29%20with%20an%20order-%20and%20duration-constrained%20inference%20step.%20To%20evaluate%20data%20efficiency%20and%20generalization%2C%20we%20train%20exclusively%20on%20PhysioNet%202016%20dataset%20with%20subject-level%20subsampling%20and%20perform%20external%20validation%20on%20CirCor%20dataset.%20Under%20matched-capacity%20decoders%2C%20the%20topological%20features%20consistently%20outperform%20spectrogram%20and%20envelope%20inputs%2C%20with%20the%20largest%20margins%20at%20low%20data%20budgets%3B%20as%20a%20full%20system%2C%20TopSeg%20surpasses%20representative%20end-to-end%20baselines%20trained%20on%20their%20native%20inputs%20under%20the%20same%20budgets%20while%20remaining%20competitive%20at%20full%20data.%20Ablations%20at%2010%25%20training%20confirm%20that%20all%20scales%20contribute%20and%20that%20combining%20H_0%20and%20H_1%20yields%20more%20reliable%20S1/S2%20localization%20and%20boundary%20stability.%20These%20results%20indicate%20that%20topology-aware%20representations%20provide%20a%20strong%20inductive%20bias%20for%20data-efficient%2C%20cross-dataset%20PCG%20segmentation%2C%20supporting%20practical%20use%20when%20labeled%20data%20are%20limited.&entry.1838667208=http%3A//arxiv.org/abs/2510.17346v2&entry.124074799=Read"},
{"title": "Context-aware Fairness Evaluation and Mitigation in LLMs", "author": "Afrozah Nadeem and Mark Dras and Usman Naseem", "abstract": "Large language models often display undesirable behaviors embedded in their internal representations, undermining fairness, inconsistency drift, amplification of harmful content, and the propagation of unwanted patterns during extended dialogue and conversations. Although training-time or data-centric methods attempt to reduce these effects, they are computationally expensive, irreversible once deployed, and slow to adapt to new conversational contexts. Pruning-based methods provide a flexible and transparent way to reduce bias by adjusting the neurons responsible for certain behaviors. However, most existing approaches are static; once a neuron is removed, the model loses the ability to adapt when the conversation or context changes. To address this, we propose a dynamic, reversible, pruning-based framework that detects context-aware neuron activations and applies adaptive masking to modulate their influence during generation. Our inference-time solution provides fine-grained, memory-aware mitigation with knowledge-preserved, more coherent behavior across multilingual single- and multi-turn dialogues, enabling dynamic fairness control in real-world conversational AI.", "link": "http://arxiv.org/abs/2510.18914v2", "date": "2026-01-30", "relevancy": 2.4127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-aware%20Fairness%20Evaluation%20and%20Mitigation%20in%20LLMs&body=Title%3A%20Context-aware%20Fairness%20Evaluation%20and%20Mitigation%20in%20LLMs%0AAuthor%3A%20Afrozah%20Nadeem%20and%20Mark%20Dras%20and%20Usman%20Naseem%0AAbstract%3A%20Large%20language%20models%20often%20display%20undesirable%20behaviors%20embedded%20in%20their%20internal%20representations%2C%20undermining%20fairness%2C%20inconsistency%20drift%2C%20amplification%20of%20harmful%20content%2C%20and%20the%20propagation%20of%20unwanted%20patterns%20during%20extended%20dialogue%20and%20conversations.%20Although%20training-time%20or%20data-centric%20methods%20attempt%20to%20reduce%20these%20effects%2C%20they%20are%20computationally%20expensive%2C%20irreversible%20once%20deployed%2C%20and%20slow%20to%20adapt%20to%20new%20conversational%20contexts.%20Pruning-based%20methods%20provide%20a%20flexible%20and%20transparent%20way%20to%20reduce%20bias%20by%20adjusting%20the%20neurons%20responsible%20for%20certain%20behaviors.%20However%2C%20most%20existing%20approaches%20are%20static%3B%20once%20a%20neuron%20is%20removed%2C%20the%20model%20loses%20the%20ability%20to%20adapt%20when%20the%20conversation%20or%20context%20changes.%20To%20address%20this%2C%20we%20propose%20a%20dynamic%2C%20reversible%2C%20pruning-based%20framework%20that%20detects%20context-aware%20neuron%20activations%20and%20applies%20adaptive%20masking%20to%20modulate%20their%20influence%20during%20generation.%20Our%20inference-time%20solution%20provides%20fine-grained%2C%20memory-aware%20mitigation%20with%20knowledge-preserved%2C%20more%20coherent%20behavior%20across%20multilingual%20single-%20and%20multi-turn%20dialogues%2C%20enabling%20dynamic%20fairness%20control%20in%20real-world%20conversational%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2510.18914v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-aware%2520Fairness%2520Evaluation%2520and%2520Mitigation%2520in%2520LLMs%26entry.906535625%3DAfrozah%2520Nadeem%2520and%2520Mark%2520Dras%2520and%2520Usman%2520Naseem%26entry.1292438233%3DLarge%2520language%2520models%2520often%2520display%2520undesirable%2520behaviors%2520embedded%2520in%2520their%2520internal%2520representations%252C%2520undermining%2520fairness%252C%2520inconsistency%2520drift%252C%2520amplification%2520of%2520harmful%2520content%252C%2520and%2520the%2520propagation%2520of%2520unwanted%2520patterns%2520during%2520extended%2520dialogue%2520and%2520conversations.%2520Although%2520training-time%2520or%2520data-centric%2520methods%2520attempt%2520to%2520reduce%2520these%2520effects%252C%2520they%2520are%2520computationally%2520expensive%252C%2520irreversible%2520once%2520deployed%252C%2520and%2520slow%2520to%2520adapt%2520to%2520new%2520conversational%2520contexts.%2520Pruning-based%2520methods%2520provide%2520a%2520flexible%2520and%2520transparent%2520way%2520to%2520reduce%2520bias%2520by%2520adjusting%2520the%2520neurons%2520responsible%2520for%2520certain%2520behaviors.%2520However%252C%2520most%2520existing%2520approaches%2520are%2520static%253B%2520once%2520a%2520neuron%2520is%2520removed%252C%2520the%2520model%2520loses%2520the%2520ability%2520to%2520adapt%2520when%2520the%2520conversation%2520or%2520context%2520changes.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520dynamic%252C%2520reversible%252C%2520pruning-based%2520framework%2520that%2520detects%2520context-aware%2520neuron%2520activations%2520and%2520applies%2520adaptive%2520masking%2520to%2520modulate%2520their%2520influence%2520during%2520generation.%2520Our%2520inference-time%2520solution%2520provides%2520fine-grained%252C%2520memory-aware%2520mitigation%2520with%2520knowledge-preserved%252C%2520more%2520coherent%2520behavior%2520across%2520multilingual%2520single-%2520and%2520multi-turn%2520dialogues%252C%2520enabling%2520dynamic%2520fairness%2520control%2520in%2520real-world%2520conversational%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18914v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-aware%20Fairness%20Evaluation%20and%20Mitigation%20in%20LLMs&entry.906535625=Afrozah%20Nadeem%20and%20Mark%20Dras%20and%20Usman%20Naseem&entry.1292438233=Large%20language%20models%20often%20display%20undesirable%20behaviors%20embedded%20in%20their%20internal%20representations%2C%20undermining%20fairness%2C%20inconsistency%20drift%2C%20amplification%20of%20harmful%20content%2C%20and%20the%20propagation%20of%20unwanted%20patterns%20during%20extended%20dialogue%20and%20conversations.%20Although%20training-time%20or%20data-centric%20methods%20attempt%20to%20reduce%20these%20effects%2C%20they%20are%20computationally%20expensive%2C%20irreversible%20once%20deployed%2C%20and%20slow%20to%20adapt%20to%20new%20conversational%20contexts.%20Pruning-based%20methods%20provide%20a%20flexible%20and%20transparent%20way%20to%20reduce%20bias%20by%20adjusting%20the%20neurons%20responsible%20for%20certain%20behaviors.%20However%2C%20most%20existing%20approaches%20are%20static%3B%20once%20a%20neuron%20is%20removed%2C%20the%20model%20loses%20the%20ability%20to%20adapt%20when%20the%20conversation%20or%20context%20changes.%20To%20address%20this%2C%20we%20propose%20a%20dynamic%2C%20reversible%2C%20pruning-based%20framework%20that%20detects%20context-aware%20neuron%20activations%20and%20applies%20adaptive%20masking%20to%20modulate%20their%20influence%20during%20generation.%20Our%20inference-time%20solution%20provides%20fine-grained%2C%20memory-aware%20mitigation%20with%20knowledge-preserved%2C%20more%20coherent%20behavior%20across%20multilingual%20single-%20and%20multi-turn%20dialogues%2C%20enabling%20dynamic%20fairness%20control%20in%20real-world%20conversational%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2510.18914v2&entry.124074799=Read"},
{"title": "Accurate and Efficient Low-Rank Model Merging in Core Space", "author": "Aniello Panariello and Daniel Marczak and Simone Magistri and Angelo Porrello and Bart\u0142omiej Twardowski and Andrew D. Bagdanov and Simone Calderara and Joost van de Weijer", "abstract": "In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. With the rise of parameter-efficient adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning has become more accessible. While fine-tuning models with LoRA is highly efficient, existing merging methods often sacrifice this efficiency by merging fully-sized weight matrices. We propose the Core Space merging framework, which enables the merging of LoRA-adapted models within a common alignment basis, thereby preserving the efficiency of low-rank adaptation while substantially improving accuracy across tasks. We further provide a formal proof that projection into Core Space ensures no loss of information and provide a complexity analysis showing the efficiency gains. Extensive empirical results demonstrate that Core Space significantly improves existing merging techniques and achieves state-of-the-art results on both vision and language tasks while utilizing a fraction of the computational resources. Codebase is available at https://github.com/apanariello4/core-space-merging.", "link": "http://arxiv.org/abs/2509.17786v4", "date": "2026-01-30", "relevancy": 2.4042, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4916}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4833}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accurate%20and%20Efficient%20Low-Rank%20Model%20Merging%20in%20Core%20Space&body=Title%3A%20Accurate%20and%20Efficient%20Low-Rank%20Model%20Merging%20in%20Core%20Space%0AAuthor%3A%20Aniello%20Panariello%20and%20Daniel%20Marczak%20and%20Simone%20Magistri%20and%20Angelo%20Porrello%20and%20Bart%C5%82omiej%20Twardowski%20and%20Andrew%20D.%20Bagdanov%20and%20Simone%20Calderara%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20In%20this%20paper%2C%20we%20address%20the%20challenges%20associated%20with%20merging%20low-rank%20adaptations%20of%20large%20neural%20networks.%20With%20the%20rise%20of%20parameter-efficient%20adaptation%20techniques%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20model%20fine-tuning%20has%20become%20more%20accessible.%20While%20fine-tuning%20models%20with%20LoRA%20is%20highly%20efficient%2C%20existing%20merging%20methods%20often%20sacrifice%20this%20efficiency%20by%20merging%20fully-sized%20weight%20matrices.%20We%20propose%20the%20Core%20Space%20merging%20framework%2C%20which%20enables%20the%20merging%20of%20LoRA-adapted%20models%20within%20a%20common%20alignment%20basis%2C%20thereby%20preserving%20the%20efficiency%20of%20low-rank%20adaptation%20while%20substantially%20improving%20accuracy%20across%20tasks.%20We%20further%20provide%20a%20formal%20proof%20that%20projection%20into%20Core%20Space%20ensures%20no%20loss%20of%20information%20and%20provide%20a%20complexity%20analysis%20showing%20the%20efficiency%20gains.%20Extensive%20empirical%20results%20demonstrate%20that%20Core%20Space%20significantly%20improves%20existing%20merging%20techniques%20and%20achieves%20state-of-the-art%20results%20on%20both%20vision%20and%20language%20tasks%20while%20utilizing%20a%20fraction%20of%20the%20computational%20resources.%20Codebase%20is%20available%20at%20https%3A//github.com/apanariello4/core-space-merging.%0ALink%3A%20http%3A//arxiv.org/abs/2509.17786v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccurate%2520and%2520Efficient%2520Low-Rank%2520Model%2520Merging%2520in%2520Core%2520Space%26entry.906535625%3DAniello%2520Panariello%2520and%2520Daniel%2520Marczak%2520and%2520Simone%2520Magistri%2520and%2520Angelo%2520Porrello%2520and%2520Bart%25C5%2582omiej%2520Twardowski%2520and%2520Andrew%2520D.%2520Bagdanov%2520and%2520Simone%2520Calderara%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520address%2520the%2520challenges%2520associated%2520with%2520merging%2520low-rank%2520adaptations%2520of%2520large%2520neural%2520networks.%2520With%2520the%2520rise%2520of%2520parameter-efficient%2520adaptation%2520techniques%252C%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520model%2520fine-tuning%2520has%2520become%2520more%2520accessible.%2520While%2520fine-tuning%2520models%2520with%2520LoRA%2520is%2520highly%2520efficient%252C%2520existing%2520merging%2520methods%2520often%2520sacrifice%2520this%2520efficiency%2520by%2520merging%2520fully-sized%2520weight%2520matrices.%2520We%2520propose%2520the%2520Core%2520Space%2520merging%2520framework%252C%2520which%2520enables%2520the%2520merging%2520of%2520LoRA-adapted%2520models%2520within%2520a%2520common%2520alignment%2520basis%252C%2520thereby%2520preserving%2520the%2520efficiency%2520of%2520low-rank%2520adaptation%2520while%2520substantially%2520improving%2520accuracy%2520across%2520tasks.%2520We%2520further%2520provide%2520a%2520formal%2520proof%2520that%2520projection%2520into%2520Core%2520Space%2520ensures%2520no%2520loss%2520of%2520information%2520and%2520provide%2520a%2520complexity%2520analysis%2520showing%2520the%2520efficiency%2520gains.%2520Extensive%2520empirical%2520results%2520demonstrate%2520that%2520Core%2520Space%2520significantly%2520improves%2520existing%2520merging%2520techniques%2520and%2520achieves%2520state-of-the-art%2520results%2520on%2520both%2520vision%2520and%2520language%2520tasks%2520while%2520utilizing%2520a%2520fraction%2520of%2520the%2520computational%2520resources.%2520Codebase%2520is%2520available%2520at%2520https%253A//github.com/apanariello4/core-space-merging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17786v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20and%20Efficient%20Low-Rank%20Model%20Merging%20in%20Core%20Space&entry.906535625=Aniello%20Panariello%20and%20Daniel%20Marczak%20and%20Simone%20Magistri%20and%20Angelo%20Porrello%20and%20Bart%C5%82omiej%20Twardowski%20and%20Andrew%20D.%20Bagdanov%20and%20Simone%20Calderara%20and%20Joost%20van%20de%20Weijer&entry.1292438233=In%20this%20paper%2C%20we%20address%20the%20challenges%20associated%20with%20merging%20low-rank%20adaptations%20of%20large%20neural%20networks.%20With%20the%20rise%20of%20parameter-efficient%20adaptation%20techniques%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20model%20fine-tuning%20has%20become%20more%20accessible.%20While%20fine-tuning%20models%20with%20LoRA%20is%20highly%20efficient%2C%20existing%20merging%20methods%20often%20sacrifice%20this%20efficiency%20by%20merging%20fully-sized%20weight%20matrices.%20We%20propose%20the%20Core%20Space%20merging%20framework%2C%20which%20enables%20the%20merging%20of%20LoRA-adapted%20models%20within%20a%20common%20alignment%20basis%2C%20thereby%20preserving%20the%20efficiency%20of%20low-rank%20adaptation%20while%20substantially%20improving%20accuracy%20across%20tasks.%20We%20further%20provide%20a%20formal%20proof%20that%20projection%20into%20Core%20Space%20ensures%20no%20loss%20of%20information%20and%20provide%20a%20complexity%20analysis%20showing%20the%20efficiency%20gains.%20Extensive%20empirical%20results%20demonstrate%20that%20Core%20Space%20significantly%20improves%20existing%20merging%20techniques%20and%20achieves%20state-of-the-art%20results%20on%20both%20vision%20and%20language%20tasks%20while%20utilizing%20a%20fraction%20of%20the%20computational%20resources.%20Codebase%20is%20available%20at%20https%3A//github.com/apanariello4/core-space-merging.&entry.1838667208=http%3A//arxiv.org/abs/2509.17786v4&entry.124074799=Read"},
{"title": "A VAE Approach to Sample Multivariate Extremes", "author": "Nicolas Lafon and Philippe Naveau and Ronan Fablet", "abstract": "Generating accurate extremes from an observational data set is crucial when seeking to estimate risks associated with the occurrence of future extremes which could be larger than those already observed. Applications range from the occurrence of natural disasters to financial crashes. Generative approaches from the machine learning community do not apply to extreme samples without careful adaptation. Besides, asymptotic results from extreme value theory (EVT) give a theoretical framework to model multivariate extreme events, especially through the notion of multivariate regular variation. Bridging these two fields, this paper details a variational autoencoder (VAE) approach for sampling multivariate heavy-tailed distributions, i.e., distributions likely to have extremes of particularly large intensities. We illustrate the relevance of our approach on a synthetic data set and on a real data set of discharge measurements along the Danube river network. The latter shows the potential of our approach for flood risks' assessment. In addition to outperforming the standard VAE for the tested data sets, we also provide a comparison with a competing EVT-based generative approach. On the tested cases, our approach improves the learning of the dependency structure between extremes.", "link": "http://arxiv.org/abs/2306.10987v2", "date": "2026-01-30", "relevancy": 2.4029, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5268}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4738}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20VAE%20Approach%20to%20Sample%20Multivariate%20Extremes&body=Title%3A%20A%20VAE%20Approach%20to%20Sample%20Multivariate%20Extremes%0AAuthor%3A%20Nicolas%20Lafon%20and%20Philippe%20Naveau%20and%20Ronan%20Fablet%0AAbstract%3A%20Generating%20accurate%20extremes%20from%20an%20observational%20data%20set%20is%20crucial%20when%20seeking%20to%20estimate%20risks%20associated%20with%20the%20occurrence%20of%20future%20extremes%20which%20could%20be%20larger%20than%20those%20already%20observed.%20Applications%20range%20from%20the%20occurrence%20of%20natural%20disasters%20to%20financial%20crashes.%20Generative%20approaches%20from%20the%20machine%20learning%20community%20do%20not%20apply%20to%20extreme%20samples%20without%20careful%20adaptation.%20Besides%2C%20asymptotic%20results%20from%20extreme%20value%20theory%20%28EVT%29%20give%20a%20theoretical%20framework%20to%20model%20multivariate%20extreme%20events%2C%20especially%20through%20the%20notion%20of%20multivariate%20regular%20variation.%20Bridging%20these%20two%20fields%2C%20this%20paper%20details%20a%20variational%20autoencoder%20%28VAE%29%20approach%20for%20sampling%20multivariate%20heavy-tailed%20distributions%2C%20i.e.%2C%20distributions%20likely%20to%20have%20extremes%20of%20particularly%20large%20intensities.%20We%20illustrate%20the%20relevance%20of%20our%20approach%20on%20a%20synthetic%20data%20set%20and%20on%20a%20real%20data%20set%20of%20discharge%20measurements%20along%20the%20Danube%20river%20network.%20The%20latter%20shows%20the%20potential%20of%20our%20approach%20for%20flood%20risks%27%20assessment.%20In%20addition%20to%20outperforming%20the%20standard%20VAE%20for%20the%20tested%20data%20sets%2C%20we%20also%20provide%20a%20comparison%20with%20a%20competing%20EVT-based%20generative%20approach.%20On%20the%20tested%20cases%2C%20our%20approach%20improves%20the%20learning%20of%20the%20dependency%20structure%20between%20extremes.%0ALink%3A%20http%3A//arxiv.org/abs/2306.10987v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520VAE%2520Approach%2520to%2520Sample%2520Multivariate%2520Extremes%26entry.906535625%3DNicolas%2520Lafon%2520and%2520Philippe%2520Naveau%2520and%2520Ronan%2520Fablet%26entry.1292438233%3DGenerating%2520accurate%2520extremes%2520from%2520an%2520observational%2520data%2520set%2520is%2520crucial%2520when%2520seeking%2520to%2520estimate%2520risks%2520associated%2520with%2520the%2520occurrence%2520of%2520future%2520extremes%2520which%2520could%2520be%2520larger%2520than%2520those%2520already%2520observed.%2520Applications%2520range%2520from%2520the%2520occurrence%2520of%2520natural%2520disasters%2520to%2520financial%2520crashes.%2520Generative%2520approaches%2520from%2520the%2520machine%2520learning%2520community%2520do%2520not%2520apply%2520to%2520extreme%2520samples%2520without%2520careful%2520adaptation.%2520Besides%252C%2520asymptotic%2520results%2520from%2520extreme%2520value%2520theory%2520%2528EVT%2529%2520give%2520a%2520theoretical%2520framework%2520to%2520model%2520multivariate%2520extreme%2520events%252C%2520especially%2520through%2520the%2520notion%2520of%2520multivariate%2520regular%2520variation.%2520Bridging%2520these%2520two%2520fields%252C%2520this%2520paper%2520details%2520a%2520variational%2520autoencoder%2520%2528VAE%2529%2520approach%2520for%2520sampling%2520multivariate%2520heavy-tailed%2520distributions%252C%2520i.e.%252C%2520distributions%2520likely%2520to%2520have%2520extremes%2520of%2520particularly%2520large%2520intensities.%2520We%2520illustrate%2520the%2520relevance%2520of%2520our%2520approach%2520on%2520a%2520synthetic%2520data%2520set%2520and%2520on%2520a%2520real%2520data%2520set%2520of%2520discharge%2520measurements%2520along%2520the%2520Danube%2520river%2520network.%2520The%2520latter%2520shows%2520the%2520potential%2520of%2520our%2520approach%2520for%2520flood%2520risks%2527%2520assessment.%2520In%2520addition%2520to%2520outperforming%2520the%2520standard%2520VAE%2520for%2520the%2520tested%2520data%2520sets%252C%2520we%2520also%2520provide%2520a%2520comparison%2520with%2520a%2520competing%2520EVT-based%2520generative%2520approach.%2520On%2520the%2520tested%2520cases%252C%2520our%2520approach%2520improves%2520the%2520learning%2520of%2520the%2520dependency%2520structure%2520between%2520extremes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.10987v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20VAE%20Approach%20to%20Sample%20Multivariate%20Extremes&entry.906535625=Nicolas%20Lafon%20and%20Philippe%20Naveau%20and%20Ronan%20Fablet&entry.1292438233=Generating%20accurate%20extremes%20from%20an%20observational%20data%20set%20is%20crucial%20when%20seeking%20to%20estimate%20risks%20associated%20with%20the%20occurrence%20of%20future%20extremes%20which%20could%20be%20larger%20than%20those%20already%20observed.%20Applications%20range%20from%20the%20occurrence%20of%20natural%20disasters%20to%20financial%20crashes.%20Generative%20approaches%20from%20the%20machine%20learning%20community%20do%20not%20apply%20to%20extreme%20samples%20without%20careful%20adaptation.%20Besides%2C%20asymptotic%20results%20from%20extreme%20value%20theory%20%28EVT%29%20give%20a%20theoretical%20framework%20to%20model%20multivariate%20extreme%20events%2C%20especially%20through%20the%20notion%20of%20multivariate%20regular%20variation.%20Bridging%20these%20two%20fields%2C%20this%20paper%20details%20a%20variational%20autoencoder%20%28VAE%29%20approach%20for%20sampling%20multivariate%20heavy-tailed%20distributions%2C%20i.e.%2C%20distributions%20likely%20to%20have%20extremes%20of%20particularly%20large%20intensities.%20We%20illustrate%20the%20relevance%20of%20our%20approach%20on%20a%20synthetic%20data%20set%20and%20on%20a%20real%20data%20set%20of%20discharge%20measurements%20along%20the%20Danube%20river%20network.%20The%20latter%20shows%20the%20potential%20of%20our%20approach%20for%20flood%20risks%27%20assessment.%20In%20addition%20to%20outperforming%20the%20standard%20VAE%20for%20the%20tested%20data%20sets%2C%20we%20also%20provide%20a%20comparison%20with%20a%20competing%20EVT-based%20generative%20approach.%20On%20the%20tested%20cases%2C%20our%20approach%20improves%20the%20learning%20of%20the%20dependency%20structure%20between%20extremes.&entry.1838667208=http%3A//arxiv.org/abs/2306.10987v2&entry.124074799=Read"},
{"title": "Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework", "author": "Hamid Reza Akbari and Mohammad Hossein Sameti and Amir M. Mansourian and Mohammad Hossein Rohban and Hossein Sameti", "abstract": "The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git", "link": "http://arxiv.org/abs/2601.22786v1", "date": "2026-01-30", "relevancy": 2.3968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20IIT-Inspired%20Consciousness%20in%20LLMs%3A%20A%20Reward-Based%20Learning%20Framework&body=Title%3A%20Toward%20IIT-Inspired%20Consciousness%20in%20LLMs%3A%20A%20Reward-Based%20Learning%20Framework%0AAuthor%3A%20Hamid%20Reza%20Akbari%20and%20Mohammad%20Hossein%20Sameti%20and%20Amir%20M.%20Mansourian%20and%20Mohammad%20Hossein%20Rohban%20and%20Hossein%20Sameti%0AAbstract%3A%20The%20pursuit%20of%20Artificial%20General%20Intelligence%20%28AGI%29%20is%20a%20central%20goal%20in%20language%20model%20development%2C%20in%20which%20consciousness-like%20processing%20could%20serve%20as%20a%20key%20facilitator.%20While%20current%20language%20models%20are%20not%20conscious%2C%20they%20exhibit%20behaviors%20analogous%20to%20certain%20aspects%20of%20consciousness.%20This%20paper%20investigates%20the%20implementation%20of%20a%20leading%20theory%20of%20consciousness%2C%20Integrated%20Information%20Theory%20%28IIT%29%2C%20within%20language%20models%20via%20a%20reward-based%20learning%20paradigm.%20IIT%20provides%20a%20formal%2C%20axiom-based%20mathematical%20framework%20for%20quantifying%20consciousness.%20Drawing%20inspiration%20from%20its%20core%20principles%2C%20we%20formulate%20a%20novel%20reward%20function%20that%20quantifies%20a%20text%27s%20causality%2C%20coherence%20and%20integration%2C%20characteristics%20associated%20with%20conscious%20processing.%20Empirically%2C%20it%20is%20found%20that%20optimizing%20for%20this%20IIT-inspired%20reward%20leads%20to%20more%20concise%20text%20generation.%20On%20out%20of%20domain%20tasks%2C%20careful%20tuning%20achieves%20up%20to%20a%2031%25%20reduction%20in%20output%20length%20while%20preserving%20accuracy%20levels%20comparable%20to%20the%20base%20model.%20In%20addition%20to%20primary%20task%20performance%2C%20the%20broader%20effects%20of%20this%20training%20methodology%20on%20the%20model%27s%20confidence%20calibration%20and%20test-time%20computational%20scaling%20is%20analyzed.%20The%20proposed%20framework%20offers%20significant%20practical%20advantages%3A%20it%20is%20conceptually%20simple%2C%20computationally%20efficient%2C%20requires%20no%20external%20data%20or%20auxiliary%20models%2C%20and%20leverages%20a%20general%2C%20capability-driven%20signal%20rather%20than%20task-specific%20heuristics.%20Code%20available%20at%20https%3A//github.com/MH-Sameti/LLM_PostTraining.git%0ALink%3A%20http%3A//arxiv.org/abs/2601.22786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520IIT-Inspired%2520Consciousness%2520in%2520LLMs%253A%2520A%2520Reward-Based%2520Learning%2520Framework%26entry.906535625%3DHamid%2520Reza%2520Akbari%2520and%2520Mohammad%2520Hossein%2520Sameti%2520and%2520Amir%2520M.%2520Mansourian%2520and%2520Mohammad%2520Hossein%2520Rohban%2520and%2520Hossein%2520Sameti%26entry.1292438233%3DThe%2520pursuit%2520of%2520Artificial%2520General%2520Intelligence%2520%2528AGI%2529%2520is%2520a%2520central%2520goal%2520in%2520language%2520model%2520development%252C%2520in%2520which%2520consciousness-like%2520processing%2520could%2520serve%2520as%2520a%2520key%2520facilitator.%2520While%2520current%2520language%2520models%2520are%2520not%2520conscious%252C%2520they%2520exhibit%2520behaviors%2520analogous%2520to%2520certain%2520aspects%2520of%2520consciousness.%2520This%2520paper%2520investigates%2520the%2520implementation%2520of%2520a%2520leading%2520theory%2520of%2520consciousness%252C%2520Integrated%2520Information%2520Theory%2520%2528IIT%2529%252C%2520within%2520language%2520models%2520via%2520a%2520reward-based%2520learning%2520paradigm.%2520IIT%2520provides%2520a%2520formal%252C%2520axiom-based%2520mathematical%2520framework%2520for%2520quantifying%2520consciousness.%2520Drawing%2520inspiration%2520from%2520its%2520core%2520principles%252C%2520we%2520formulate%2520a%2520novel%2520reward%2520function%2520that%2520quantifies%2520a%2520text%2527s%2520causality%252C%2520coherence%2520and%2520integration%252C%2520characteristics%2520associated%2520with%2520conscious%2520processing.%2520Empirically%252C%2520it%2520is%2520found%2520that%2520optimizing%2520for%2520this%2520IIT-inspired%2520reward%2520leads%2520to%2520more%2520concise%2520text%2520generation.%2520On%2520out%2520of%2520domain%2520tasks%252C%2520careful%2520tuning%2520achieves%2520up%2520to%2520a%252031%2525%2520reduction%2520in%2520output%2520length%2520while%2520preserving%2520accuracy%2520levels%2520comparable%2520to%2520the%2520base%2520model.%2520In%2520addition%2520to%2520primary%2520task%2520performance%252C%2520the%2520broader%2520effects%2520of%2520this%2520training%2520methodology%2520on%2520the%2520model%2527s%2520confidence%2520calibration%2520and%2520test-time%2520computational%2520scaling%2520is%2520analyzed.%2520The%2520proposed%2520framework%2520offers%2520significant%2520practical%2520advantages%253A%2520it%2520is%2520conceptually%2520simple%252C%2520computationally%2520efficient%252C%2520requires%2520no%2520external%2520data%2520or%2520auxiliary%2520models%252C%2520and%2520leverages%2520a%2520general%252C%2520capability-driven%2520signal%2520rather%2520than%2520task-specific%2520heuristics.%2520Code%2520available%2520at%2520https%253A//github.com/MH-Sameti/LLM_PostTraining.git%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20IIT-Inspired%20Consciousness%20in%20LLMs%3A%20A%20Reward-Based%20Learning%20Framework&entry.906535625=Hamid%20Reza%20Akbari%20and%20Mohammad%20Hossein%20Sameti%20and%20Amir%20M.%20Mansourian%20and%20Mohammad%20Hossein%20Rohban%20and%20Hossein%20Sameti&entry.1292438233=The%20pursuit%20of%20Artificial%20General%20Intelligence%20%28AGI%29%20is%20a%20central%20goal%20in%20language%20model%20development%2C%20in%20which%20consciousness-like%20processing%20could%20serve%20as%20a%20key%20facilitator.%20While%20current%20language%20models%20are%20not%20conscious%2C%20they%20exhibit%20behaviors%20analogous%20to%20certain%20aspects%20of%20consciousness.%20This%20paper%20investigates%20the%20implementation%20of%20a%20leading%20theory%20of%20consciousness%2C%20Integrated%20Information%20Theory%20%28IIT%29%2C%20within%20language%20models%20via%20a%20reward-based%20learning%20paradigm.%20IIT%20provides%20a%20formal%2C%20axiom-based%20mathematical%20framework%20for%20quantifying%20consciousness.%20Drawing%20inspiration%20from%20its%20core%20principles%2C%20we%20formulate%20a%20novel%20reward%20function%20that%20quantifies%20a%20text%27s%20causality%2C%20coherence%20and%20integration%2C%20characteristics%20associated%20with%20conscious%20processing.%20Empirically%2C%20it%20is%20found%20that%20optimizing%20for%20this%20IIT-inspired%20reward%20leads%20to%20more%20concise%20text%20generation.%20On%20out%20of%20domain%20tasks%2C%20careful%20tuning%20achieves%20up%20to%20a%2031%25%20reduction%20in%20output%20length%20while%20preserving%20accuracy%20levels%20comparable%20to%20the%20base%20model.%20In%20addition%20to%20primary%20task%20performance%2C%20the%20broader%20effects%20of%20this%20training%20methodology%20on%20the%20model%27s%20confidence%20calibration%20and%20test-time%20computational%20scaling%20is%20analyzed.%20The%20proposed%20framework%20offers%20significant%20practical%20advantages%3A%20it%20is%20conceptually%20simple%2C%20computationally%20efficient%2C%20requires%20no%20external%20data%20or%20auxiliary%20models%2C%20and%20leverages%20a%20general%2C%20capability-driven%20signal%20rather%20than%20task-specific%20heuristics.%20Code%20available%20at%20https%3A//github.com/MH-Sameti/LLM_PostTraining.git&entry.1838667208=http%3A//arxiv.org/abs/2601.22786v1&entry.124074799=Read"},
{"title": "ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search", "author": "Tao Yu and Haopeng Jin and Hao Wang and Shenghua Chai and Yujia Yang and Junhao Gong and Jiaming Guo and Minghui Zhang and Xinlong Chen and Zhenghao Zhang and Yuxuan Zhou and Yanpei Gong and YuanCheng Liu and Yiming Ding and Kangwei Zeng and Pengfei Yang and Zhongtian Luo and Yufei Xiong and Shanbin Zhang and Shaoxiong Cheng and Huang Ruilin and Li Shuo and Yuxi Niu and Xinyuan Zhang and Yueya Xu and Jie Mao and Ruixuan Ji and Yaru Zhao and Mingchen Zhang and Jiabing Yang and Jiaqi Liu and YiFan Zhang and Hongzhu Yi and Xinming Wang and Cheng Zhong and Xiao Ma and Zhang Zhang and Yan Huang and Liang Wang", "abstract": "In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.", "link": "http://arxiv.org/abs/2601.23232v1", "date": "2026-01-30", "relevancy": 2.3836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5972}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShotFinder%3A%20Imagination-Driven%20Open-Domain%20Video%20Shot%20Retrieval%20via%20Web%20Search&body=Title%3A%20ShotFinder%3A%20Imagination-Driven%20Open-Domain%20Video%20Shot%20Retrieval%20via%20Web%20Search%0AAuthor%3A%20Tao%20Yu%20and%20Haopeng%20Jin%20and%20Hao%20Wang%20and%20Shenghua%20Chai%20and%20Yujia%20Yang%20and%20Junhao%20Gong%20and%20Jiaming%20Guo%20and%20Minghui%20Zhang%20and%20Xinlong%20Chen%20and%20Zhenghao%20Zhang%20and%20Yuxuan%20Zhou%20and%20Yanpei%20Gong%20and%20YuanCheng%20Liu%20and%20Yiming%20Ding%20and%20Kangwei%20Zeng%20and%20Pengfei%20Yang%20and%20Zhongtian%20Luo%20and%20Yufei%20Xiong%20and%20Shanbin%20Zhang%20and%20Shaoxiong%20Cheng%20and%20Huang%20Ruilin%20and%20Li%20Shuo%20and%20Yuxi%20Niu%20and%20Xinyuan%20Zhang%20and%20Yueya%20Xu%20and%20Jie%20Mao%20and%20Ruixuan%20Ji%20and%20Yaru%20Zhao%20and%20Mingchen%20Zhang%20and%20Jiabing%20Yang%20and%20Jiaqi%20Liu%20and%20YiFan%20Zhang%20and%20Hongzhu%20Yi%20and%20Xinming%20Wang%20and%20Cheng%20Zhong%20and%20Xiao%20Ma%20and%20Zhang%20Zhang%20and%20Yan%20Huang%20and%20Liang%20Wang%0AAbstract%3A%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20made%20rapid%20progress%20in%20information%20retrieval%2C%20yet%20existing%20research%20has%20mainly%20focused%20on%20text%20or%20static%20multimodal%20settings.%20Open-domain%20video%20shot%20retrieval%2C%20which%20involves%20richer%20temporal%20structure%20and%20more%20complex%20semantics%2C%20still%20lacks%20systematic%20benchmarks%20and%20analysis.%20To%20fill%20this%20gap%2C%20we%20introduce%20ShotFinder%2C%20a%20benchmark%20that%20formalizes%20editing%20requirements%20as%20keyframe-oriented%20shot%20descriptions%20and%20introduces%20five%20types%20of%20controllable%20single-factor%20constraints%3A%20Temporal%20order%2C%20Color%2C%20Visual%20style%2C%20Audio%2C%20and%20Resolution.%20We%20curate%201%2C210%20high-quality%20samples%20from%20YouTube%20across%2020%20thematic%20categories%2C%20using%20large%20models%20for%20generation%20with%20human%20verification.%20Based%20on%20the%20benchmark%2C%20we%20propose%20ShotFinder%2C%20a%20text-driven%20three-stage%20retrieval%20and%20localization%20pipeline%3A%20%281%29%20query%20expansion%20via%20video%20imagination%2C%20%282%29%20candidate%20video%20retrieval%20with%20a%20search%20engine%2C%20and%20%283%29%20description-guided%20temporal%20localization.%20Experiments%20on%20multiple%20closed-source%20and%20open-source%20models%20reveal%20a%20significant%20gap%20to%20human%20performance%2C%20with%20clear%20imbalance%20across%20constraints%3A%20temporal%20localization%20is%20relatively%20tractable%2C%20while%20color%20and%20visual%20style%20remain%20major%20challenges.%20These%20results%20reveal%20that%20open-domain%20video%20shot%20retrieval%20is%20still%20a%20critical%20capability%20that%20multimodal%20large%20models%20have%20yet%20to%20overcome.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShotFinder%253A%2520Imagination-Driven%2520Open-Domain%2520Video%2520Shot%2520Retrieval%2520via%2520Web%2520Search%26entry.906535625%3DTao%2520Yu%2520and%2520Haopeng%2520Jin%2520and%2520Hao%2520Wang%2520and%2520Shenghua%2520Chai%2520and%2520Yujia%2520Yang%2520and%2520Junhao%2520Gong%2520and%2520Jiaming%2520Guo%2520and%2520Minghui%2520Zhang%2520and%2520Xinlong%2520Chen%2520and%2520Zhenghao%2520Zhang%2520and%2520Yuxuan%2520Zhou%2520and%2520Yanpei%2520Gong%2520and%2520YuanCheng%2520Liu%2520and%2520Yiming%2520Ding%2520and%2520Kangwei%2520Zeng%2520and%2520Pengfei%2520Yang%2520and%2520Zhongtian%2520Luo%2520and%2520Yufei%2520Xiong%2520and%2520Shanbin%2520Zhang%2520and%2520Shaoxiong%2520Cheng%2520and%2520Huang%2520Ruilin%2520and%2520Li%2520Shuo%2520and%2520Yuxi%2520Niu%2520and%2520Xinyuan%2520Zhang%2520and%2520Yueya%2520Xu%2520and%2520Jie%2520Mao%2520and%2520Ruixuan%2520Ji%2520and%2520Yaru%2520Zhao%2520and%2520Mingchen%2520Zhang%2520and%2520Jiabing%2520Yang%2520and%2520Jiaqi%2520Liu%2520and%2520YiFan%2520Zhang%2520and%2520Hongzhu%2520Yi%2520and%2520Xinming%2520Wang%2520and%2520Cheng%2520Zhong%2520and%2520Xiao%2520Ma%2520and%2520Zhang%2520Zhang%2520and%2520Yan%2520Huang%2520and%2520Liang%2520Wang%26entry.1292438233%3DIn%2520recent%2520years%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520rapid%2520progress%2520in%2520information%2520retrieval%252C%2520yet%2520existing%2520research%2520has%2520mainly%2520focused%2520on%2520text%2520or%2520static%2520multimodal%2520settings.%2520Open-domain%2520video%2520shot%2520retrieval%252C%2520which%2520involves%2520richer%2520temporal%2520structure%2520and%2520more%2520complex%2520semantics%252C%2520still%2520lacks%2520systematic%2520benchmarks%2520and%2520analysis.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520ShotFinder%252C%2520a%2520benchmark%2520that%2520formalizes%2520editing%2520requirements%2520as%2520keyframe-oriented%2520shot%2520descriptions%2520and%2520introduces%2520five%2520types%2520of%2520controllable%2520single-factor%2520constraints%253A%2520Temporal%2520order%252C%2520Color%252C%2520Visual%2520style%252C%2520Audio%252C%2520and%2520Resolution.%2520We%2520curate%25201%252C210%2520high-quality%2520samples%2520from%2520YouTube%2520across%252020%2520thematic%2520categories%252C%2520using%2520large%2520models%2520for%2520generation%2520with%2520human%2520verification.%2520Based%2520on%2520the%2520benchmark%252C%2520we%2520propose%2520ShotFinder%252C%2520a%2520text-driven%2520three-stage%2520retrieval%2520and%2520localization%2520pipeline%253A%2520%25281%2529%2520query%2520expansion%2520via%2520video%2520imagination%252C%2520%25282%2529%2520candidate%2520video%2520retrieval%2520with%2520a%2520search%2520engine%252C%2520and%2520%25283%2529%2520description-guided%2520temporal%2520localization.%2520Experiments%2520on%2520multiple%2520closed-source%2520and%2520open-source%2520models%2520reveal%2520a%2520significant%2520gap%2520to%2520human%2520performance%252C%2520with%2520clear%2520imbalance%2520across%2520constraints%253A%2520temporal%2520localization%2520is%2520relatively%2520tractable%252C%2520while%2520color%2520and%2520visual%2520style%2520remain%2520major%2520challenges.%2520These%2520results%2520reveal%2520that%2520open-domain%2520video%2520shot%2520retrieval%2520is%2520still%2520a%2520critical%2520capability%2520that%2520multimodal%2520large%2520models%2520have%2520yet%2520to%2520overcome.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShotFinder%3A%20Imagination-Driven%20Open-Domain%20Video%20Shot%20Retrieval%20via%20Web%20Search&entry.906535625=Tao%20Yu%20and%20Haopeng%20Jin%20and%20Hao%20Wang%20and%20Shenghua%20Chai%20and%20Yujia%20Yang%20and%20Junhao%20Gong%20and%20Jiaming%20Guo%20and%20Minghui%20Zhang%20and%20Xinlong%20Chen%20and%20Zhenghao%20Zhang%20and%20Yuxuan%20Zhou%20and%20Yanpei%20Gong%20and%20YuanCheng%20Liu%20and%20Yiming%20Ding%20and%20Kangwei%20Zeng%20and%20Pengfei%20Yang%20and%20Zhongtian%20Luo%20and%20Yufei%20Xiong%20and%20Shanbin%20Zhang%20and%20Shaoxiong%20Cheng%20and%20Huang%20Ruilin%20and%20Li%20Shuo%20and%20Yuxi%20Niu%20and%20Xinyuan%20Zhang%20and%20Yueya%20Xu%20and%20Jie%20Mao%20and%20Ruixuan%20Ji%20and%20Yaru%20Zhao%20and%20Mingchen%20Zhang%20and%20Jiabing%20Yang%20and%20Jiaqi%20Liu%20and%20YiFan%20Zhang%20and%20Hongzhu%20Yi%20and%20Xinming%20Wang%20and%20Cheng%20Zhong%20and%20Xiao%20Ma%20and%20Zhang%20Zhang%20and%20Yan%20Huang%20and%20Liang%20Wang&entry.1292438233=In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20made%20rapid%20progress%20in%20information%20retrieval%2C%20yet%20existing%20research%20has%20mainly%20focused%20on%20text%20or%20static%20multimodal%20settings.%20Open-domain%20video%20shot%20retrieval%2C%20which%20involves%20richer%20temporal%20structure%20and%20more%20complex%20semantics%2C%20still%20lacks%20systematic%20benchmarks%20and%20analysis.%20To%20fill%20this%20gap%2C%20we%20introduce%20ShotFinder%2C%20a%20benchmark%20that%20formalizes%20editing%20requirements%20as%20keyframe-oriented%20shot%20descriptions%20and%20introduces%20five%20types%20of%20controllable%20single-factor%20constraints%3A%20Temporal%20order%2C%20Color%2C%20Visual%20style%2C%20Audio%2C%20and%20Resolution.%20We%20curate%201%2C210%20high-quality%20samples%20from%20YouTube%20across%2020%20thematic%20categories%2C%20using%20large%20models%20for%20generation%20with%20human%20verification.%20Based%20on%20the%20benchmark%2C%20we%20propose%20ShotFinder%2C%20a%20text-driven%20three-stage%20retrieval%20and%20localization%20pipeline%3A%20%281%29%20query%20expansion%20via%20video%20imagination%2C%20%282%29%20candidate%20video%20retrieval%20with%20a%20search%20engine%2C%20and%20%283%29%20description-guided%20temporal%20localization.%20Experiments%20on%20multiple%20closed-source%20and%20open-source%20models%20reveal%20a%20significant%20gap%20to%20human%20performance%2C%20with%20clear%20imbalance%20across%20constraints%3A%20temporal%20localization%20is%20relatively%20tractable%2C%20while%20color%20and%20visual%20style%20remain%20major%20challenges.%20These%20results%20reveal%20that%20open-domain%20video%20shot%20retrieval%20is%20still%20a%20critical%20capability%20that%20multimodal%20large%20models%20have%20yet%20to%20overcome.&entry.1838667208=http%3A//arxiv.org/abs/2601.23232v1&entry.124074799=Read"},
{"title": "Structured Over Scale: Learning Spatial Reasoning from Educational Video", "author": "Bishoy Galoaa and Xiangyu Bai and Sarah Ostadabbas", "abstract": "Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \\textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children's educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.", "link": "http://arxiv.org/abs/2601.23251v1", "date": "2026-01-30", "relevancy": 2.3739, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6071}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Over%20Scale%3A%20Learning%20Spatial%20Reasoning%20from%20Educational%20Video&body=Title%3A%20Structured%20Over%20Scale%3A%20Learning%20Spatial%20Reasoning%20from%20Educational%20Video%0AAuthor%3A%20Bishoy%20Galoaa%20and%20Xiangyu%20Bai%20and%20Sarah%20Ostadabbas%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20demonstrate%20impressive%20performance%20on%20standard%20video%20understanding%20benchmarks%20yet%20fail%20systematically%20on%20simple%20reasoning%20tasks%20that%20preschool%20children%20can%20solve%2C%20including%20counting%2C%20spatial%20reasoning%2C%20and%20compositional%20understanding.%20We%20hypothesize%20that%20the%20pedagogically-structured%20content%20of%20educational%20videos%20provides%20an%20ideal%20training%20signal%20for%20improving%20these%20capabilities.%20We%20introduce%20DoraVQA%2C%20a%20dataset%20of%205%2C344%20question-answer%20pairs%20automatically%20extracted%20from%208%20seasons%20of%20Dora%20the%20Explorer%20with%20precise%20timestamp%20alignment.%20Each%20episode%20follows%20a%20consistent%20%5Ctextit%7Bcontext-question-pause-answer%7D%20structure%20that%20creates%20a%20self-contained%20learning%20environment%20analogous%20to%20interactive%20tutoring.%20We%20fine-tune%20both%20Qwen2%20and%20Qwen3%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20leveraging%20the%20clear%20correctness%20signals%20and%20structured%20reasoning%20traces%20inherent%20in%20educational%20content.%20Despite%20training%20exclusively%20on%2038%20hours%20of%20children%27s%20educational%20videos%2C%20our%20approach%20achieves%20improvements%20of%208-14%20points%20on%20DoraVQA%20and%20state-of-the-art%2086.16%5C%25%20on%20CVBench%2C%20with%20strong%20transfer%20to%20Video-MME%20and%20NExT-QA%2C%20demonstrating%20effective%20generalization%20from%20narrow%20pedagogical%20content%20to%20broad%20multimodal%20understanding.%20Through%20cross-domain%20benchmarks%2C%20we%20show%20that%20VLMs%20can%20perform%20tasks%20that%20require%20robust%20reasoning%20learned%20from%20structured%20educational%20content%2C%20suggesting%20that%20content%20structure%20matters%20as%20much%20as%20content%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Over%2520Scale%253A%2520Learning%2520Spatial%2520Reasoning%2520from%2520Educational%2520Video%26entry.906535625%3DBishoy%2520Galoaa%2520and%2520Xiangyu%2520Bai%2520and%2520Sarah%2520Ostadabbas%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520demonstrate%2520impressive%2520performance%2520on%2520standard%2520video%2520understanding%2520benchmarks%2520yet%2520fail%2520systematically%2520on%2520simple%2520reasoning%2520tasks%2520that%2520preschool%2520children%2520can%2520solve%252C%2520including%2520counting%252C%2520spatial%2520reasoning%252C%2520and%2520compositional%2520understanding.%2520We%2520hypothesize%2520that%2520the%2520pedagogically-structured%2520content%2520of%2520educational%2520videos%2520provides%2520an%2520ideal%2520training%2520signal%2520for%2520improving%2520these%2520capabilities.%2520We%2520introduce%2520DoraVQA%252C%2520a%2520dataset%2520of%25205%252C344%2520question-answer%2520pairs%2520automatically%2520extracted%2520from%25208%2520seasons%2520of%2520Dora%2520the%2520Explorer%2520with%2520precise%2520timestamp%2520alignment.%2520Each%2520episode%2520follows%2520a%2520consistent%2520%255Ctextit%257Bcontext-question-pause-answer%257D%2520structure%2520that%2520creates%2520a%2520self-contained%2520learning%2520environment%2520analogous%2520to%2520interactive%2520tutoring.%2520We%2520fine-tune%2520both%2520Qwen2%2520and%2520Qwen3%2520using%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520leveraging%2520the%2520clear%2520correctness%2520signals%2520and%2520structured%2520reasoning%2520traces%2520inherent%2520in%2520educational%2520content.%2520Despite%2520training%2520exclusively%2520on%252038%2520hours%2520of%2520children%2527s%2520educational%2520videos%252C%2520our%2520approach%2520achieves%2520improvements%2520of%25208-14%2520points%2520on%2520DoraVQA%2520and%2520state-of-the-art%252086.16%255C%2525%2520on%2520CVBench%252C%2520with%2520strong%2520transfer%2520to%2520Video-MME%2520and%2520NExT-QA%252C%2520demonstrating%2520effective%2520generalization%2520from%2520narrow%2520pedagogical%2520content%2520to%2520broad%2520multimodal%2520understanding.%2520Through%2520cross-domain%2520benchmarks%252C%2520we%2520show%2520that%2520VLMs%2520can%2520perform%2520tasks%2520that%2520require%2520robust%2520reasoning%2520learned%2520from%2520structured%2520educational%2520content%252C%2520suggesting%2520that%2520content%2520structure%2520matters%2520as%2520much%2520as%2520content%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Over%20Scale%3A%20Learning%20Spatial%20Reasoning%20from%20Educational%20Video&entry.906535625=Bishoy%20Galoaa%20and%20Xiangyu%20Bai%20and%20Sarah%20Ostadabbas&entry.1292438233=Vision-language%20models%20%28VLMs%29%20demonstrate%20impressive%20performance%20on%20standard%20video%20understanding%20benchmarks%20yet%20fail%20systematically%20on%20simple%20reasoning%20tasks%20that%20preschool%20children%20can%20solve%2C%20including%20counting%2C%20spatial%20reasoning%2C%20and%20compositional%20understanding.%20We%20hypothesize%20that%20the%20pedagogically-structured%20content%20of%20educational%20videos%20provides%20an%20ideal%20training%20signal%20for%20improving%20these%20capabilities.%20We%20introduce%20DoraVQA%2C%20a%20dataset%20of%205%2C344%20question-answer%20pairs%20automatically%20extracted%20from%208%20seasons%20of%20Dora%20the%20Explorer%20with%20precise%20timestamp%20alignment.%20Each%20episode%20follows%20a%20consistent%20%5Ctextit%7Bcontext-question-pause-answer%7D%20structure%20that%20creates%20a%20self-contained%20learning%20environment%20analogous%20to%20interactive%20tutoring.%20We%20fine-tune%20both%20Qwen2%20and%20Qwen3%20using%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20leveraging%20the%20clear%20correctness%20signals%20and%20structured%20reasoning%20traces%20inherent%20in%20educational%20content.%20Despite%20training%20exclusively%20on%2038%20hours%20of%20children%27s%20educational%20videos%2C%20our%20approach%20achieves%20improvements%20of%208-14%20points%20on%20DoraVQA%20and%20state-of-the-art%2086.16%5C%25%20on%20CVBench%2C%20with%20strong%20transfer%20to%20Video-MME%20and%20NExT-QA%2C%20demonstrating%20effective%20generalization%20from%20narrow%20pedagogical%20content%20to%20broad%20multimodal%20understanding.%20Through%20cross-domain%20benchmarks%2C%20we%20show%20that%20VLMs%20can%20perform%20tasks%20that%20require%20robust%20reasoning%20learned%20from%20structured%20educational%20content%2C%20suggesting%20that%20content%20structure%20matters%20as%20much%20as%20content%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2601.23251v1&entry.124074799=Read"},
{"title": "HeatMat: Simulation of City Material Impact on Urban Heat Island Effect", "author": "Marie Reinbigler and Romain Rouffet and Peter Naylor and Mikolaj Czerkawski and Nikolaos Dionelis and Elisabeth Brunet and Catalin Fetita and Rosalie Martin", "abstract": "The Urban Heat Island (UHI) effect, defined as a significant increase in temperature in urban environments compared to surrounding areas, is difficult to study in real cities using sensor data (satellites or in-situ stations) due to their coarse spatial and temporal resolution. Among the factors contributing to this effect are the properties of urban materials, which differ from those in rural areas. To analyze their individual impact and to test new material configurations, a high-resolution simulation at the city scale is required. Estimating the current materials used in a city, including those on building facades, is also challenging. We propose HeatMat, an approach to analyze at high resolution the individual impact of urban materials on the UHI effect in a real city, relying only on open data. We estimate building materials using street-view images and a pre-trained vision-language model (VLM) to supplement existing OpenStreetMap data, which describes the 2D geometry and features of buildings. We further encode this information into a set of 2D maps that represent the city's vertical structure and material characteristics. These maps serve as inputs for our 2.5D simulator, which models coupled heat transfers and enables random-access surface temperature estimation at multiple resolutions, reaching an x20 speedup compared to an equivalent simulation in 3D.", "link": "http://arxiv.org/abs/2601.22796v1", "date": "2026-01-30", "relevancy": 2.3684, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4779}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4779}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeatMat%3A%20Simulation%20of%20City%20Material%20Impact%20on%20Urban%20Heat%20Island%20Effect&body=Title%3A%20HeatMat%3A%20Simulation%20of%20City%20Material%20Impact%20on%20Urban%20Heat%20Island%20Effect%0AAuthor%3A%20Marie%20Reinbigler%20and%20Romain%20Rouffet%20and%20Peter%20Naylor%20and%20Mikolaj%20Czerkawski%20and%20Nikolaos%20Dionelis%20and%20Elisabeth%20Brunet%20and%20Catalin%20Fetita%20and%20Rosalie%20Martin%0AAbstract%3A%20The%20Urban%20Heat%20Island%20%28UHI%29%20effect%2C%20defined%20as%20a%20significant%20increase%20in%20temperature%20in%20urban%20environments%20compared%20to%20surrounding%20areas%2C%20is%20difficult%20to%20study%20in%20real%20cities%20using%20sensor%20data%20%28satellites%20or%20in-situ%20stations%29%20due%20to%20their%20coarse%20spatial%20and%20temporal%20resolution.%20Among%20the%20factors%20contributing%20to%20this%20effect%20are%20the%20properties%20of%20urban%20materials%2C%20which%20differ%20from%20those%20in%20rural%20areas.%20To%20analyze%20their%20individual%20impact%20and%20to%20test%20new%20material%20configurations%2C%20a%20high-resolution%20simulation%20at%20the%20city%20scale%20is%20required.%20Estimating%20the%20current%20materials%20used%20in%20a%20city%2C%20including%20those%20on%20building%20facades%2C%20is%20also%20challenging.%20We%20propose%20HeatMat%2C%20an%20approach%20to%20analyze%20at%20high%20resolution%20the%20individual%20impact%20of%20urban%20materials%20on%20the%20UHI%20effect%20in%20a%20real%20city%2C%20relying%20only%20on%20open%20data.%20We%20estimate%20building%20materials%20using%20street-view%20images%20and%20a%20pre-trained%20vision-language%20model%20%28VLM%29%20to%20supplement%20existing%20OpenStreetMap%20data%2C%20which%20describes%20the%202D%20geometry%20and%20features%20of%20buildings.%20We%20further%20encode%20this%20information%20into%20a%20set%20of%202D%20maps%20that%20represent%20the%20city%27s%20vertical%20structure%20and%20material%20characteristics.%20These%20maps%20serve%20as%20inputs%20for%20our%202.5D%20simulator%2C%20which%20models%20coupled%20heat%20transfers%20and%20enables%20random-access%20surface%20temperature%20estimation%20at%20multiple%20resolutions%2C%20reaching%20an%20x20%20speedup%20compared%20to%20an%20equivalent%20simulation%20in%203D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeatMat%253A%2520Simulation%2520of%2520City%2520Material%2520Impact%2520on%2520Urban%2520Heat%2520Island%2520Effect%26entry.906535625%3DMarie%2520Reinbigler%2520and%2520Romain%2520Rouffet%2520and%2520Peter%2520Naylor%2520and%2520Mikolaj%2520Czerkawski%2520and%2520Nikolaos%2520Dionelis%2520and%2520Elisabeth%2520Brunet%2520and%2520Catalin%2520Fetita%2520and%2520Rosalie%2520Martin%26entry.1292438233%3DThe%2520Urban%2520Heat%2520Island%2520%2528UHI%2529%2520effect%252C%2520defined%2520as%2520a%2520significant%2520increase%2520in%2520temperature%2520in%2520urban%2520environments%2520compared%2520to%2520surrounding%2520areas%252C%2520is%2520difficult%2520to%2520study%2520in%2520real%2520cities%2520using%2520sensor%2520data%2520%2528satellites%2520or%2520in-situ%2520stations%2529%2520due%2520to%2520their%2520coarse%2520spatial%2520and%2520temporal%2520resolution.%2520Among%2520the%2520factors%2520contributing%2520to%2520this%2520effect%2520are%2520the%2520properties%2520of%2520urban%2520materials%252C%2520which%2520differ%2520from%2520those%2520in%2520rural%2520areas.%2520To%2520analyze%2520their%2520individual%2520impact%2520and%2520to%2520test%2520new%2520material%2520configurations%252C%2520a%2520high-resolution%2520simulation%2520at%2520the%2520city%2520scale%2520is%2520required.%2520Estimating%2520the%2520current%2520materials%2520used%2520in%2520a%2520city%252C%2520including%2520those%2520on%2520building%2520facades%252C%2520is%2520also%2520challenging.%2520We%2520propose%2520HeatMat%252C%2520an%2520approach%2520to%2520analyze%2520at%2520high%2520resolution%2520the%2520individual%2520impact%2520of%2520urban%2520materials%2520on%2520the%2520UHI%2520effect%2520in%2520a%2520real%2520city%252C%2520relying%2520only%2520on%2520open%2520data.%2520We%2520estimate%2520building%2520materials%2520using%2520street-view%2520images%2520and%2520a%2520pre-trained%2520vision-language%2520model%2520%2528VLM%2529%2520to%2520supplement%2520existing%2520OpenStreetMap%2520data%252C%2520which%2520describes%2520the%25202D%2520geometry%2520and%2520features%2520of%2520buildings.%2520We%2520further%2520encode%2520this%2520information%2520into%2520a%2520set%2520of%25202D%2520maps%2520that%2520represent%2520the%2520city%2527s%2520vertical%2520structure%2520and%2520material%2520characteristics.%2520These%2520maps%2520serve%2520as%2520inputs%2520for%2520our%25202.5D%2520simulator%252C%2520which%2520models%2520coupled%2520heat%2520transfers%2520and%2520enables%2520random-access%2520surface%2520temperature%2520estimation%2520at%2520multiple%2520resolutions%252C%2520reaching%2520an%2520x20%2520speedup%2520compared%2520to%2520an%2520equivalent%2520simulation%2520in%25203D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeatMat%3A%20Simulation%20of%20City%20Material%20Impact%20on%20Urban%20Heat%20Island%20Effect&entry.906535625=Marie%20Reinbigler%20and%20Romain%20Rouffet%20and%20Peter%20Naylor%20and%20Mikolaj%20Czerkawski%20and%20Nikolaos%20Dionelis%20and%20Elisabeth%20Brunet%20and%20Catalin%20Fetita%20and%20Rosalie%20Martin&entry.1292438233=The%20Urban%20Heat%20Island%20%28UHI%29%20effect%2C%20defined%20as%20a%20significant%20increase%20in%20temperature%20in%20urban%20environments%20compared%20to%20surrounding%20areas%2C%20is%20difficult%20to%20study%20in%20real%20cities%20using%20sensor%20data%20%28satellites%20or%20in-situ%20stations%29%20due%20to%20their%20coarse%20spatial%20and%20temporal%20resolution.%20Among%20the%20factors%20contributing%20to%20this%20effect%20are%20the%20properties%20of%20urban%20materials%2C%20which%20differ%20from%20those%20in%20rural%20areas.%20To%20analyze%20their%20individual%20impact%20and%20to%20test%20new%20material%20configurations%2C%20a%20high-resolution%20simulation%20at%20the%20city%20scale%20is%20required.%20Estimating%20the%20current%20materials%20used%20in%20a%20city%2C%20including%20those%20on%20building%20facades%2C%20is%20also%20challenging.%20We%20propose%20HeatMat%2C%20an%20approach%20to%20analyze%20at%20high%20resolution%20the%20individual%20impact%20of%20urban%20materials%20on%20the%20UHI%20effect%20in%20a%20real%20city%2C%20relying%20only%20on%20open%20data.%20We%20estimate%20building%20materials%20using%20street-view%20images%20and%20a%20pre-trained%20vision-language%20model%20%28VLM%29%20to%20supplement%20existing%20OpenStreetMap%20data%2C%20which%20describes%20the%202D%20geometry%20and%20features%20of%20buildings.%20We%20further%20encode%20this%20information%20into%20a%20set%20of%202D%20maps%20that%20represent%20the%20city%27s%20vertical%20structure%20and%20material%20characteristics.%20These%20maps%20serve%20as%20inputs%20for%20our%202.5D%20simulator%2C%20which%20models%20coupled%20heat%20transfers%20and%20enables%20random-access%20surface%20temperature%20estimation%20at%20multiple%20resolutions%2C%20reaching%20an%20x20%20speedup%20compared%20to%20an%20equivalent%20simulation%20in%203D.&entry.1838667208=http%3A//arxiv.org/abs/2601.22796v1&entry.124074799=Read"},
{"title": "Graph Attention Network for Node Regression on Random Geometric Graphs with Erd\u0151s--R\u00e9nyi contamination", "author": "Somak Laha and Suqi Liu and Morgane Austern", "abstract": "Graph attention networks (GATs) are widely used and often appear robust to noise in node covariates and edges, yet rigorous statistical guarantees demonstrating a provable advantage of GATs over non-attention graph neural networks~(GNNs) are scarce. We partially address this gap for node regression with graph-based errors-in-variables models under simultaneous covariate and edge corruption: responses are generated from latent node-level covariates, but only noise-perturbed versions of the latent covariates are observed; and the sample graph is a random geometric graph created from the node covariates but contaminated by independent Erd\u0151s--R\u00e9nyi edges. We propose and analyze a carefully designed, task-specific GAT that constructs denoised proxy features for regression. We prove that regressing the response variables on the proxies achieves lower error asymptotically in (a) estimating the regression coefficient compared to the ordinary least squares (OLS) estimator on the noisy node covariates, and (b) predicting the response for an unlabelled node compared to a vanilla graph convolutional network~(GCN) -- under mild growth conditions. Our analysis leverages high-dimensional geometric tail bounds and concentration for neighbourhood counts and sample covariances. We verify our theoretical findings through experiments on synthetically generated data. We also perform experiments on real-world graphs and demonstrate the effectiveness of the attention mechanism in several node regression tasks.", "link": "http://arxiv.org/abs/2601.23239v1", "date": "2026-01-30", "relevancy": 2.3653, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4785}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Attention%20Network%20for%20Node%20Regression%20on%20Random%20Geometric%20Graphs%20with%20Erd%C5%91s--R%C3%A9nyi%20contamination&body=Title%3A%20Graph%20Attention%20Network%20for%20Node%20Regression%20on%20Random%20Geometric%20Graphs%20with%20Erd%C5%91s--R%C3%A9nyi%20contamination%0AAuthor%3A%20Somak%20Laha%20and%20Suqi%20Liu%20and%20Morgane%20Austern%0AAbstract%3A%20Graph%20attention%20networks%20%28GATs%29%20are%20widely%20used%20and%20often%20appear%20robust%20to%20noise%20in%20node%20covariates%20and%20edges%2C%20yet%20rigorous%20statistical%20guarantees%20demonstrating%20a%20provable%20advantage%20of%20GATs%20over%20non-attention%20graph%20neural%20networks~%28GNNs%29%20are%20scarce.%20We%20partially%20address%20this%20gap%20for%20node%20regression%20with%20graph-based%20errors-in-variables%20models%20under%20simultaneous%20covariate%20and%20edge%20corruption%3A%20responses%20are%20generated%20from%20latent%20node-level%20covariates%2C%20but%20only%20noise-perturbed%20versions%20of%20the%20latent%20covariates%20are%20observed%3B%20and%20the%20sample%20graph%20is%20a%20random%20geometric%20graph%20created%20from%20the%20node%20covariates%20but%20contaminated%20by%20independent%20Erd%C5%91s--R%C3%A9nyi%20edges.%20We%20propose%20and%20analyze%20a%20carefully%20designed%2C%20task-specific%20GAT%20that%20constructs%20denoised%20proxy%20features%20for%20regression.%20We%20prove%20that%20regressing%20the%20response%20variables%20on%20the%20proxies%20achieves%20lower%20error%20asymptotically%20in%20%28a%29%20estimating%20the%20regression%20coefficient%20compared%20to%20the%20ordinary%20least%20squares%20%28OLS%29%20estimator%20on%20the%20noisy%20node%20covariates%2C%20and%20%28b%29%20predicting%20the%20response%20for%20an%20unlabelled%20node%20compared%20to%20a%20vanilla%20graph%20convolutional%20network~%28GCN%29%20--%20under%20mild%20growth%20conditions.%20Our%20analysis%20leverages%20high-dimensional%20geometric%20tail%20bounds%20and%20concentration%20for%20neighbourhood%20counts%20and%20sample%20covariances.%20We%20verify%20our%20theoretical%20findings%20through%20experiments%20on%20synthetically%20generated%20data.%20We%20also%20perform%20experiments%20on%20real-world%20graphs%20and%20demonstrate%20the%20effectiveness%20of%20the%20attention%20mechanism%20in%20several%20node%20regression%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Attention%2520Network%2520for%2520Node%2520Regression%2520on%2520Random%2520Geometric%2520Graphs%2520with%2520Erd%25C5%2591s--R%25C3%25A9nyi%2520contamination%26entry.906535625%3DSomak%2520Laha%2520and%2520Suqi%2520Liu%2520and%2520Morgane%2520Austern%26entry.1292438233%3DGraph%2520attention%2520networks%2520%2528GATs%2529%2520are%2520widely%2520used%2520and%2520often%2520appear%2520robust%2520to%2520noise%2520in%2520node%2520covariates%2520and%2520edges%252C%2520yet%2520rigorous%2520statistical%2520guarantees%2520demonstrating%2520a%2520provable%2520advantage%2520of%2520GATs%2520over%2520non-attention%2520graph%2520neural%2520networks~%2528GNNs%2529%2520are%2520scarce.%2520We%2520partially%2520address%2520this%2520gap%2520for%2520node%2520regression%2520with%2520graph-based%2520errors-in-variables%2520models%2520under%2520simultaneous%2520covariate%2520and%2520edge%2520corruption%253A%2520responses%2520are%2520generated%2520from%2520latent%2520node-level%2520covariates%252C%2520but%2520only%2520noise-perturbed%2520versions%2520of%2520the%2520latent%2520covariates%2520are%2520observed%253B%2520and%2520the%2520sample%2520graph%2520is%2520a%2520random%2520geometric%2520graph%2520created%2520from%2520the%2520node%2520covariates%2520but%2520contaminated%2520by%2520independent%2520Erd%25C5%2591s--R%25C3%25A9nyi%2520edges.%2520We%2520propose%2520and%2520analyze%2520a%2520carefully%2520designed%252C%2520task-specific%2520GAT%2520that%2520constructs%2520denoised%2520proxy%2520features%2520for%2520regression.%2520We%2520prove%2520that%2520regressing%2520the%2520response%2520variables%2520on%2520the%2520proxies%2520achieves%2520lower%2520error%2520asymptotically%2520in%2520%2528a%2529%2520estimating%2520the%2520regression%2520coefficient%2520compared%2520to%2520the%2520ordinary%2520least%2520squares%2520%2528OLS%2529%2520estimator%2520on%2520the%2520noisy%2520node%2520covariates%252C%2520and%2520%2528b%2529%2520predicting%2520the%2520response%2520for%2520an%2520unlabelled%2520node%2520compared%2520to%2520a%2520vanilla%2520graph%2520convolutional%2520network~%2528GCN%2529%2520--%2520under%2520mild%2520growth%2520conditions.%2520Our%2520analysis%2520leverages%2520high-dimensional%2520geometric%2520tail%2520bounds%2520and%2520concentration%2520for%2520neighbourhood%2520counts%2520and%2520sample%2520covariances.%2520We%2520verify%2520our%2520theoretical%2520findings%2520through%2520experiments%2520on%2520synthetically%2520generated%2520data.%2520We%2520also%2520perform%2520experiments%2520on%2520real-world%2520graphs%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520attention%2520mechanism%2520in%2520several%2520node%2520regression%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Attention%20Network%20for%20Node%20Regression%20on%20Random%20Geometric%20Graphs%20with%20Erd%C5%91s--R%C3%A9nyi%20contamination&entry.906535625=Somak%20Laha%20and%20Suqi%20Liu%20and%20Morgane%20Austern&entry.1292438233=Graph%20attention%20networks%20%28GATs%29%20are%20widely%20used%20and%20often%20appear%20robust%20to%20noise%20in%20node%20covariates%20and%20edges%2C%20yet%20rigorous%20statistical%20guarantees%20demonstrating%20a%20provable%20advantage%20of%20GATs%20over%20non-attention%20graph%20neural%20networks~%28GNNs%29%20are%20scarce.%20We%20partially%20address%20this%20gap%20for%20node%20regression%20with%20graph-based%20errors-in-variables%20models%20under%20simultaneous%20covariate%20and%20edge%20corruption%3A%20responses%20are%20generated%20from%20latent%20node-level%20covariates%2C%20but%20only%20noise-perturbed%20versions%20of%20the%20latent%20covariates%20are%20observed%3B%20and%20the%20sample%20graph%20is%20a%20random%20geometric%20graph%20created%20from%20the%20node%20covariates%20but%20contaminated%20by%20independent%20Erd%C5%91s--R%C3%A9nyi%20edges.%20We%20propose%20and%20analyze%20a%20carefully%20designed%2C%20task-specific%20GAT%20that%20constructs%20denoised%20proxy%20features%20for%20regression.%20We%20prove%20that%20regressing%20the%20response%20variables%20on%20the%20proxies%20achieves%20lower%20error%20asymptotically%20in%20%28a%29%20estimating%20the%20regression%20coefficient%20compared%20to%20the%20ordinary%20least%20squares%20%28OLS%29%20estimator%20on%20the%20noisy%20node%20covariates%2C%20and%20%28b%29%20predicting%20the%20response%20for%20an%20unlabelled%20node%20compared%20to%20a%20vanilla%20graph%20convolutional%20network~%28GCN%29%20--%20under%20mild%20growth%20conditions.%20Our%20analysis%20leverages%20high-dimensional%20geometric%20tail%20bounds%20and%20concentration%20for%20neighbourhood%20counts%20and%20sample%20covariances.%20We%20verify%20our%20theoretical%20findings%20through%20experiments%20on%20synthetically%20generated%20data.%20We%20also%20perform%20experiments%20on%20real-world%20graphs%20and%20demonstrate%20the%20effectiveness%20of%20the%20attention%20mechanism%20in%20several%20node%20regression%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.23239v1&entry.124074799=Read"},
{"title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application", "author": "Haoyu Jiang and Fanjie Zeng and Boan Qu and Xiaojie Lin and Wei Zhong", "abstract": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.", "link": "http://arxiv.org/abs/2512.19299v2", "date": "2026-01-30", "relevancy": 2.3614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helios%3A%20A%20Foundational%20Language%20Model%20for%20Smart%20Energy%20Knowledge%20Reasoning%20and%20Application&body=Title%3A%20Helios%3A%20A%20Foundational%20Language%20Model%20for%20Smart%20Energy%20Knowledge%20Reasoning%20and%20Application%0AAuthor%3A%20Haoyu%20Jiang%20and%20Fanjie%20Zeng%20and%20Boan%20Qu%20and%20Xiaojie%20Lin%20and%20Wei%20Zhong%0AAbstract%3A%20In%20the%20global%20drive%20toward%20carbon%20neutrality%2C%20deeply%20coordinated%20smart%20energy%20systems%20underpin%20industrial%20transformation.%20However%2C%20the%20interdisciplinary%2C%20fragmented%2C%20and%20fast-evolving%20expertise%20in%20this%20domain%20prevents%20general-purpose%20LLMs%2C%20which%20lack%20domain%20knowledge%20and%20physical-constraint%20awareness%2C%20from%20delivering%20precise%20engineering-aligned%20inference%20and%20generation.%20To%20address%20these%20challenges%2C%20we%20introduce%20Helios%2C%20a%20large%20language%20model%20tailored%20to%20the%20smart%20energy%20domain%2C%20together%20with%20a%20comprehensive%20suite%20of%20resources%20to%20advance%20LLM%20research%20in%20this%20field.%20Specifically%2C%20we%20develop%20Enersys%2C%20a%20multi-agent%20collaborative%20framework%20for%20end-to-end%20dataset%20construction%2C%20through%20which%20we%20produce%3A%20%281%29%20a%20smart%20energy%20knowledge%20base%2C%20EnerBase%2C%20to%20enrich%20the%20model%27s%20foundational%20expertise%3B%20%282%29%20an%20instruction%20fine-tuning%20dataset%2C%20EnerInstruct%2C%20to%20strengthen%20performance%20on%20domain-specific%20downstream%20tasks%3B%20and%20%283%29%20an%20RLHF%20dataset%2C%20EnerReinforce%2C%20to%20align%20the%20model%20with%20human%20preferences%20and%20industry%20standards.%20Leveraging%20these%20resources%2C%20Helios%20undergoes%20large-scale%20pretraining%2C%20SFT%2C%20and%20RLHF.%20We%20also%20release%20EnerBench%2C%20a%20benchmark%20for%20evaluating%20LLMs%20in%20smart%20energy%20scenarios%2C%20and%20demonstrate%20that%20our%20approach%20significantly%20enhances%20domain%20knowledge%20mastery%2C%20task%20execution%20accuracy%2C%20and%20alignment%20with%20human%20preferences.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelios%253A%2520A%2520Foundational%2520Language%2520Model%2520for%2520Smart%2520Energy%2520Knowledge%2520Reasoning%2520and%2520Application%26entry.906535625%3DHaoyu%2520Jiang%2520and%2520Fanjie%2520Zeng%2520and%2520Boan%2520Qu%2520and%2520Xiaojie%2520Lin%2520and%2520Wei%2520Zhong%26entry.1292438233%3DIn%2520the%2520global%2520drive%2520toward%2520carbon%2520neutrality%252C%2520deeply%2520coordinated%2520smart%2520energy%2520systems%2520underpin%2520industrial%2520transformation.%2520However%252C%2520the%2520interdisciplinary%252C%2520fragmented%252C%2520and%2520fast-evolving%2520expertise%2520in%2520this%2520domain%2520prevents%2520general-purpose%2520LLMs%252C%2520which%2520lack%2520domain%2520knowledge%2520and%2520physical-constraint%2520awareness%252C%2520from%2520delivering%2520precise%2520engineering-aligned%2520inference%2520and%2520generation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Helios%252C%2520a%2520large%2520language%2520model%2520tailored%2520to%2520the%2520smart%2520energy%2520domain%252C%2520together%2520with%2520a%2520comprehensive%2520suite%2520of%2520resources%2520to%2520advance%2520LLM%2520research%2520in%2520this%2520field.%2520Specifically%252C%2520we%2520develop%2520Enersys%252C%2520a%2520multi-agent%2520collaborative%2520framework%2520for%2520end-to-end%2520dataset%2520construction%252C%2520through%2520which%2520we%2520produce%253A%2520%25281%2529%2520a%2520smart%2520energy%2520knowledge%2520base%252C%2520EnerBase%252C%2520to%2520enrich%2520the%2520model%2527s%2520foundational%2520expertise%253B%2520%25282%2529%2520an%2520instruction%2520fine-tuning%2520dataset%252C%2520EnerInstruct%252C%2520to%2520strengthen%2520performance%2520on%2520domain-specific%2520downstream%2520tasks%253B%2520and%2520%25283%2529%2520an%2520RLHF%2520dataset%252C%2520EnerReinforce%252C%2520to%2520align%2520the%2520model%2520with%2520human%2520preferences%2520and%2520industry%2520standards.%2520Leveraging%2520these%2520resources%252C%2520Helios%2520undergoes%2520large-scale%2520pretraining%252C%2520SFT%252C%2520and%2520RLHF.%2520We%2520also%2520release%2520EnerBench%252C%2520a%2520benchmark%2520for%2520evaluating%2520LLMs%2520in%2520smart%2520energy%2520scenarios%252C%2520and%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520domain%2520knowledge%2520mastery%252C%2520task%2520execution%2520accuracy%252C%2520and%2520alignment%2520with%2520human%2520preferences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helios%3A%20A%20Foundational%20Language%20Model%20for%20Smart%20Energy%20Knowledge%20Reasoning%20and%20Application&entry.906535625=Haoyu%20Jiang%20and%20Fanjie%20Zeng%20and%20Boan%20Qu%20and%20Xiaojie%20Lin%20and%20Wei%20Zhong&entry.1292438233=In%20the%20global%20drive%20toward%20carbon%20neutrality%2C%20deeply%20coordinated%20smart%20energy%20systems%20underpin%20industrial%20transformation.%20However%2C%20the%20interdisciplinary%2C%20fragmented%2C%20and%20fast-evolving%20expertise%20in%20this%20domain%20prevents%20general-purpose%20LLMs%2C%20which%20lack%20domain%20knowledge%20and%20physical-constraint%20awareness%2C%20from%20delivering%20precise%20engineering-aligned%20inference%20and%20generation.%20To%20address%20these%20challenges%2C%20we%20introduce%20Helios%2C%20a%20large%20language%20model%20tailored%20to%20the%20smart%20energy%20domain%2C%20together%20with%20a%20comprehensive%20suite%20of%20resources%20to%20advance%20LLM%20research%20in%20this%20field.%20Specifically%2C%20we%20develop%20Enersys%2C%20a%20multi-agent%20collaborative%20framework%20for%20end-to-end%20dataset%20construction%2C%20through%20which%20we%20produce%3A%20%281%29%20a%20smart%20energy%20knowledge%20base%2C%20EnerBase%2C%20to%20enrich%20the%20model%27s%20foundational%20expertise%3B%20%282%29%20an%20instruction%20fine-tuning%20dataset%2C%20EnerInstruct%2C%20to%20strengthen%20performance%20on%20domain-specific%20downstream%20tasks%3B%20and%20%283%29%20an%20RLHF%20dataset%2C%20EnerReinforce%2C%20to%20align%20the%20model%20with%20human%20preferences%20and%20industry%20standards.%20Leveraging%20these%20resources%2C%20Helios%20undergoes%20large-scale%20pretraining%2C%20SFT%2C%20and%20RLHF.%20We%20also%20release%20EnerBench%2C%20a%20benchmark%20for%20evaluating%20LLMs%20in%20smart%20energy%20scenarios%2C%20and%20demonstrate%20that%20our%20approach%20significantly%20enhances%20domain%20knowledge%20mastery%2C%20task%20execution%20accuracy%2C%20and%20alignment%20with%20human%20preferences.&entry.1838667208=http%3A//arxiv.org/abs/2512.19299v2&entry.124074799=Read"},
{"title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation", "author": "Hun Chang and Byunghee Cha and Jong Chul Ye", "abstract": "Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.", "link": "http://arxiv.org/abs/2601.22904v1", "date": "2026-01-30", "relevancy": 2.3565, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5906}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-SAE%3A%20DINO%20Spherical%20Autoencoder%20for%20High-Fidelity%20Image%20Reconstruction%20and%20Generation&body=Title%3A%20DINO-SAE%3A%20DINO%20Spherical%20Autoencoder%20for%20High-Fidelity%20Image%20Reconstruction%20and%20Generation%0AAuthor%3A%20Hun%20Chang%20and%20Byunghee%20Cha%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20Recent%20studies%20have%20explored%20using%20pretrained%20Vision%20Foundation%20Models%20%28VFMs%29%20such%20as%20DINO%20for%20generative%20autoencoders%2C%20showing%20strong%20generative%20performance.%20Unfortunately%2C%20existing%20approaches%20often%20suffer%20from%20limited%20reconstruction%20fidelity%20due%20to%20the%20loss%20of%20high-frequency%20details.%20In%20this%20work%2C%20we%20present%20the%20DINO%20Spherical%20Autoencoder%20%28DINO-SAE%29%2C%20a%20framework%20that%20bridges%20semantic%20representation%20and%20pixel-level%20reconstruction.%20Our%20key%20insight%20is%20that%20semantic%20information%20in%20contrastive%20representations%20is%20primarily%20encoded%20in%20the%20direction%20of%20feature%20vectors%2C%20while%20forcing%20strict%20magnitude%20matching%20can%20hinder%20the%20encoder%20from%20preserving%20fine-grained%20details.%20To%20address%20this%2C%20we%20introduce%20Hierarchical%20Convolutional%20Patch%20Embedding%20module%20that%20enhances%20local%20structure%20and%20texture%20preservation%2C%20and%20Cosine%20Similarity%20Alignment%20objective%20that%20enforces%20semantic%20consistency%20while%20allowing%20flexible%20feature%20magnitudes%20for%20detail%20retention.%20Furthermore%2C%20leveraging%20the%20observation%20that%20SSL-based%20foundation%20model%20representations%20intrinsically%20lie%20on%20a%20hypersphere%2C%20we%20employ%20Riemannian%20Flow%20Matching%20to%20train%20a%20Diffusion%20Transformer%20%28DiT%29%20directly%20on%20this%20spherical%20latent%20manifold.%20Experiments%20on%20ImageNet-1K%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20reconstruction%20quality%2C%20reaching%200.37%20rFID%20and%2026.2%20dB%20PSNR%2C%20while%20maintaining%20strong%20semantic%20alignment%20to%20the%20pretrained%20VFM.%20Notably%2C%20our%20Riemannian%20Flow%20Matching-based%20DiT%20exhibits%20efficient%20convergence%2C%20achieving%20a%20gFID%20of%203.47%20at%2080%20epochs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-SAE%253A%2520DINO%2520Spherical%2520Autoencoder%2520for%2520High-Fidelity%2520Image%2520Reconstruction%2520and%2520Generation%26entry.906535625%3DHun%2520Chang%2520and%2520Byunghee%2520Cha%2520and%2520Jong%2520Chul%2520Ye%26entry.1292438233%3DRecent%2520studies%2520have%2520explored%2520using%2520pretrained%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520such%2520as%2520DINO%2520for%2520generative%2520autoencoders%252C%2520showing%2520strong%2520generative%2520performance.%2520Unfortunately%252C%2520existing%2520approaches%2520often%2520suffer%2520from%2520limited%2520reconstruction%2520fidelity%2520due%2520to%2520the%2520loss%2520of%2520high-frequency%2520details.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520DINO%2520Spherical%2520Autoencoder%2520%2528DINO-SAE%2529%252C%2520a%2520framework%2520that%2520bridges%2520semantic%2520representation%2520and%2520pixel-level%2520reconstruction.%2520Our%2520key%2520insight%2520is%2520that%2520semantic%2520information%2520in%2520contrastive%2520representations%2520is%2520primarily%2520encoded%2520in%2520the%2520direction%2520of%2520feature%2520vectors%252C%2520while%2520forcing%2520strict%2520magnitude%2520matching%2520can%2520hinder%2520the%2520encoder%2520from%2520preserving%2520fine-grained%2520details.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Hierarchical%2520Convolutional%2520Patch%2520Embedding%2520module%2520that%2520enhances%2520local%2520structure%2520and%2520texture%2520preservation%252C%2520and%2520Cosine%2520Similarity%2520Alignment%2520objective%2520that%2520enforces%2520semantic%2520consistency%2520while%2520allowing%2520flexible%2520feature%2520magnitudes%2520for%2520detail%2520retention.%2520Furthermore%252C%2520leveraging%2520the%2520observation%2520that%2520SSL-based%2520foundation%2520model%2520representations%2520intrinsically%2520lie%2520on%2520a%2520hypersphere%252C%2520we%2520employ%2520Riemannian%2520Flow%2520Matching%2520to%2520train%2520a%2520Diffusion%2520Transformer%2520%2528DiT%2529%2520directly%2520on%2520this%2520spherical%2520latent%2520manifold.%2520Experiments%2520on%2520ImageNet-1K%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520reconstruction%2520quality%252C%2520reaching%25200.37%2520rFID%2520and%252026.2%2520dB%2520PSNR%252C%2520while%2520maintaining%2520strong%2520semantic%2520alignment%2520to%2520the%2520pretrained%2520VFM.%2520Notably%252C%2520our%2520Riemannian%2520Flow%2520Matching-based%2520DiT%2520exhibits%2520efficient%2520convergence%252C%2520achieving%2520a%2520gFID%2520of%25203.47%2520at%252080%2520epochs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-SAE%3A%20DINO%20Spherical%20Autoencoder%20for%20High-Fidelity%20Image%20Reconstruction%20and%20Generation&entry.906535625=Hun%20Chang%20and%20Byunghee%20Cha%20and%20Jong%20Chul%20Ye&entry.1292438233=Recent%20studies%20have%20explored%20using%20pretrained%20Vision%20Foundation%20Models%20%28VFMs%29%20such%20as%20DINO%20for%20generative%20autoencoders%2C%20showing%20strong%20generative%20performance.%20Unfortunately%2C%20existing%20approaches%20often%20suffer%20from%20limited%20reconstruction%20fidelity%20due%20to%20the%20loss%20of%20high-frequency%20details.%20In%20this%20work%2C%20we%20present%20the%20DINO%20Spherical%20Autoencoder%20%28DINO-SAE%29%2C%20a%20framework%20that%20bridges%20semantic%20representation%20and%20pixel-level%20reconstruction.%20Our%20key%20insight%20is%20that%20semantic%20information%20in%20contrastive%20representations%20is%20primarily%20encoded%20in%20the%20direction%20of%20feature%20vectors%2C%20while%20forcing%20strict%20magnitude%20matching%20can%20hinder%20the%20encoder%20from%20preserving%20fine-grained%20details.%20To%20address%20this%2C%20we%20introduce%20Hierarchical%20Convolutional%20Patch%20Embedding%20module%20that%20enhances%20local%20structure%20and%20texture%20preservation%2C%20and%20Cosine%20Similarity%20Alignment%20objective%20that%20enforces%20semantic%20consistency%20while%20allowing%20flexible%20feature%20magnitudes%20for%20detail%20retention.%20Furthermore%2C%20leveraging%20the%20observation%20that%20SSL-based%20foundation%20model%20representations%20intrinsically%20lie%20on%20a%20hypersphere%2C%20we%20employ%20Riemannian%20Flow%20Matching%20to%20train%20a%20Diffusion%20Transformer%20%28DiT%29%20directly%20on%20this%20spherical%20latent%20manifold.%20Experiments%20on%20ImageNet-1K%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20reconstruction%20quality%2C%20reaching%200.37%20rFID%20and%2026.2%20dB%20PSNR%2C%20while%20maintaining%20strong%20semantic%20alignment%20to%20the%20pretrained%20VFM.%20Notably%2C%20our%20Riemannian%20Flow%20Matching-based%20DiT%20exhibits%20efficient%20convergence%2C%20achieving%20a%20gFID%20of%203.47%20at%2080%20epochs.&entry.1838667208=http%3A//arxiv.org/abs/2601.22904v1&entry.124074799=Read"},
{"title": "Video Unlearning via Low-Rank Refusal Vector", "author": "Simone Facchiano and Stefano Saravalle and Matteo Migliarini and Edoardo De Matteis and Alessio Sampieri and Andrea Pilzer and Emanuele Rodol\u00e0 and Indro Spinelli and Luca Franco and Fabio Galasso", "abstract": "Video generative models achieve high-quality synthesis from natural-language prompts by leveraging large-scale web data. However, this training paradigm inherently exposes them to unsafe biases and harmful concepts, introducing the risk of generating undesirable or illicit content. To mitigate unsafe generations, existing machine unlearning approaches either rely on filtering, and can therefore be bypassed, or they update model weights, but with costly fine-tuning or training-free closed-form edits. We propose the first training-free weight update framework for concept removal in video diffusion models. From five paired safe/unsafe prompts, our method estimates a refusal vector and integrates it into the model weights as a closed-form update. A contrastive low-rank factorization further disentangles the target concept from unrelated semantics, it ensures a selective concept suppression and it does not harm generation quality. Our approach reduces unsafe generations on the Open-Sora and ZeroScopeT2V models across the T2VSafetyBench and SafeSora benchmarks, with average reductions of 36.3% and 58.2% respectively, while preserving prompt alignment and video quality. This establishes an efficient and scalable solution for safe video generation without retraining nor any inference overhead. Project page: https://www.pinlab.org/video-unlearning.", "link": "http://arxiv.org/abs/2506.07891v2", "date": "2026-01-30", "relevancy": 2.3548, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.596}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5952}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Unlearning%20via%20Low-Rank%20Refusal%20Vector&body=Title%3A%20Video%20Unlearning%20via%20Low-Rank%20Refusal%20Vector%0AAuthor%3A%20Simone%20Facchiano%20and%20Stefano%20Saravalle%20and%20Matteo%20Migliarini%20and%20Edoardo%20De%20Matteis%20and%20Alessio%20Sampieri%20and%20Andrea%20Pilzer%20and%20Emanuele%20Rodol%C3%A0%20and%20Indro%20Spinelli%20and%20Luca%20Franco%20and%20Fabio%20Galasso%0AAbstract%3A%20Video%20generative%20models%20achieve%20high-quality%20synthesis%20from%20natural-language%20prompts%20by%20leveraging%20large-scale%20web%20data.%20However%2C%20this%20training%20paradigm%20inherently%20exposes%20them%20to%20unsafe%20biases%20and%20harmful%20concepts%2C%20introducing%20the%20risk%20of%20generating%20undesirable%20or%20illicit%20content.%20To%20mitigate%20unsafe%20generations%2C%20existing%20machine%20unlearning%20approaches%20either%20rely%20on%20filtering%2C%20and%20can%20therefore%20be%20bypassed%2C%20or%20they%20update%20model%20weights%2C%20but%20with%20costly%20fine-tuning%20or%20training-free%20closed-form%20edits.%20We%20propose%20the%20first%20training-free%20weight%20update%20framework%20for%20concept%20removal%20in%20video%20diffusion%20models.%20From%20five%20paired%20safe/unsafe%20prompts%2C%20our%20method%20estimates%20a%20refusal%20vector%20and%20integrates%20it%20into%20the%20model%20weights%20as%20a%20closed-form%20update.%20A%20contrastive%20low-rank%20factorization%20further%20disentangles%20the%20target%20concept%20from%20unrelated%20semantics%2C%20it%20ensures%20a%20selective%20concept%20suppression%20and%20it%20does%20not%20harm%20generation%20quality.%20Our%20approach%20reduces%20unsafe%20generations%20on%20the%20Open-Sora%20and%20ZeroScopeT2V%20models%20across%20the%20T2VSafetyBench%20and%20SafeSora%20benchmarks%2C%20with%20average%20reductions%20of%2036.3%25%20and%2058.2%25%20respectively%2C%20while%20preserving%20prompt%20alignment%20and%20video%20quality.%20This%20establishes%20an%20efficient%20and%20scalable%20solution%20for%20safe%20video%20generation%20without%20retraining%20nor%20any%20inference%20overhead.%20Project%20page%3A%20https%3A//www.pinlab.org/video-unlearning.%0ALink%3A%20http%3A//arxiv.org/abs/2506.07891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Unlearning%2520via%2520Low-Rank%2520Refusal%2520Vector%26entry.906535625%3DSimone%2520Facchiano%2520and%2520Stefano%2520Saravalle%2520and%2520Matteo%2520Migliarini%2520and%2520Edoardo%2520De%2520Matteis%2520and%2520Alessio%2520Sampieri%2520and%2520Andrea%2520Pilzer%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Indro%2520Spinelli%2520and%2520Luca%2520Franco%2520and%2520Fabio%2520Galasso%26entry.1292438233%3DVideo%2520generative%2520models%2520achieve%2520high-quality%2520synthesis%2520from%2520natural-language%2520prompts%2520by%2520leveraging%2520large-scale%2520web%2520data.%2520However%252C%2520this%2520training%2520paradigm%2520inherently%2520exposes%2520them%2520to%2520unsafe%2520biases%2520and%2520harmful%2520concepts%252C%2520introducing%2520the%2520risk%2520of%2520generating%2520undesirable%2520or%2520illicit%2520content.%2520To%2520mitigate%2520unsafe%2520generations%252C%2520existing%2520machine%2520unlearning%2520approaches%2520either%2520rely%2520on%2520filtering%252C%2520and%2520can%2520therefore%2520be%2520bypassed%252C%2520or%2520they%2520update%2520model%2520weights%252C%2520but%2520with%2520costly%2520fine-tuning%2520or%2520training-free%2520closed-form%2520edits.%2520We%2520propose%2520the%2520first%2520training-free%2520weight%2520update%2520framework%2520for%2520concept%2520removal%2520in%2520video%2520diffusion%2520models.%2520From%2520five%2520paired%2520safe/unsafe%2520prompts%252C%2520our%2520method%2520estimates%2520a%2520refusal%2520vector%2520and%2520integrates%2520it%2520into%2520the%2520model%2520weights%2520as%2520a%2520closed-form%2520update.%2520A%2520contrastive%2520low-rank%2520factorization%2520further%2520disentangles%2520the%2520target%2520concept%2520from%2520unrelated%2520semantics%252C%2520it%2520ensures%2520a%2520selective%2520concept%2520suppression%2520and%2520it%2520does%2520not%2520harm%2520generation%2520quality.%2520Our%2520approach%2520reduces%2520unsafe%2520generations%2520on%2520the%2520Open-Sora%2520and%2520ZeroScopeT2V%2520models%2520across%2520the%2520T2VSafetyBench%2520and%2520SafeSora%2520benchmarks%252C%2520with%2520average%2520reductions%2520of%252036.3%2525%2520and%252058.2%2525%2520respectively%252C%2520while%2520preserving%2520prompt%2520alignment%2520and%2520video%2520quality.%2520This%2520establishes%2520an%2520efficient%2520and%2520scalable%2520solution%2520for%2520safe%2520video%2520generation%2520without%2520retraining%2520nor%2520any%2520inference%2520overhead.%2520Project%2520page%253A%2520https%253A//www.pinlab.org/video-unlearning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Unlearning%20via%20Low-Rank%20Refusal%20Vector&entry.906535625=Simone%20Facchiano%20and%20Stefano%20Saravalle%20and%20Matteo%20Migliarini%20and%20Edoardo%20De%20Matteis%20and%20Alessio%20Sampieri%20and%20Andrea%20Pilzer%20and%20Emanuele%20Rodol%C3%A0%20and%20Indro%20Spinelli%20and%20Luca%20Franco%20and%20Fabio%20Galasso&entry.1292438233=Video%20generative%20models%20achieve%20high-quality%20synthesis%20from%20natural-language%20prompts%20by%20leveraging%20large-scale%20web%20data.%20However%2C%20this%20training%20paradigm%20inherently%20exposes%20them%20to%20unsafe%20biases%20and%20harmful%20concepts%2C%20introducing%20the%20risk%20of%20generating%20undesirable%20or%20illicit%20content.%20To%20mitigate%20unsafe%20generations%2C%20existing%20machine%20unlearning%20approaches%20either%20rely%20on%20filtering%2C%20and%20can%20therefore%20be%20bypassed%2C%20or%20they%20update%20model%20weights%2C%20but%20with%20costly%20fine-tuning%20or%20training-free%20closed-form%20edits.%20We%20propose%20the%20first%20training-free%20weight%20update%20framework%20for%20concept%20removal%20in%20video%20diffusion%20models.%20From%20five%20paired%20safe/unsafe%20prompts%2C%20our%20method%20estimates%20a%20refusal%20vector%20and%20integrates%20it%20into%20the%20model%20weights%20as%20a%20closed-form%20update.%20A%20contrastive%20low-rank%20factorization%20further%20disentangles%20the%20target%20concept%20from%20unrelated%20semantics%2C%20it%20ensures%20a%20selective%20concept%20suppression%20and%20it%20does%20not%20harm%20generation%20quality.%20Our%20approach%20reduces%20unsafe%20generations%20on%20the%20Open-Sora%20and%20ZeroScopeT2V%20models%20across%20the%20T2VSafetyBench%20and%20SafeSora%20benchmarks%2C%20with%20average%20reductions%20of%2036.3%25%20and%2058.2%25%20respectively%2C%20while%20preserving%20prompt%20alignment%20and%20video%20quality.%20This%20establishes%20an%20efficient%20and%20scalable%20solution%20for%20safe%20video%20generation%20without%20retraining%20nor%20any%20inference%20overhead.%20Project%20page%3A%20https%3A//www.pinlab.org/video-unlearning.&entry.1838667208=http%3A//arxiv.org/abs/2506.07891v2&entry.124074799=Read"},
{"title": "Segment Any Events with Language", "author": "Seungjun Lee and Gim Hee Lee", "abstract": "Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL", "link": "http://arxiv.org/abs/2601.23159v1", "date": "2026-01-30", "relevancy": 2.3446, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6012}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Any%20Events%20with%20Language&body=Title%3A%20Segment%20Any%20Events%20with%20Language%0AAuthor%3A%20Seungjun%20Lee%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20Scene%20understanding%20with%20free-form%20language%20has%20been%20widely%20explored%20within%20diverse%20modalities%20such%20as%20images%2C%20point%20clouds%2C%20and%20LiDAR.%20However%2C%20related%20studies%20on%20event%20sensors%20are%20scarce%20or%20narrowly%20centered%20on%20semantic-level%20understanding.%20We%20introduce%20SEAL%2C%20the%20first%20Semantic-aware%20Segment%20Any%20Events%20framework%20that%20addresses%20Open-Vocabulary%20Event%20Instance%20Segmentation%20%28OV-EIS%29.%20Given%20the%20visual%20prompt%2C%20our%20model%20presents%20a%20unified%20framework%20to%20support%20both%20event%20segmentation%20and%20open-vocabulary%20mask%20classification%20at%20multiple%20levels%20of%20granularity%2C%20including%20instance-level%20and%20part-level.%20To%20enable%20thorough%20evaluation%20on%20OV-EIS%2C%20we%20curate%20four%20benchmarks%20that%20cover%20label%20granularity%20from%20coarse%20to%20fine%20class%20configurations%20and%20semantic%20granularity%20from%20instance-level%20to%20part-level%20understanding.%20Extensive%20experiments%20show%20that%20our%20SEAL%20largely%20outperforms%20proposed%20baselines%20in%20terms%20of%20performance%20and%20inference%20speed%20with%20a%20parameter-efficient%20architecture.%20In%20the%20Appendix%2C%20we%20further%20present%20a%20simple%20variant%20of%20our%20SEAL%20achieving%20generic%20spatiotemporal%20OV-EIS%20that%20does%20not%20require%20any%20visual%20prompts%20from%20users%20in%20the%20inference.%20Check%20out%20our%20project%20page%20in%20https%3A//0nandon.github.io/SEAL%0ALink%3A%20http%3A//arxiv.org/abs/2601.23159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Any%2520Events%2520with%2520Language%26entry.906535625%3DSeungjun%2520Lee%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3DScene%2520understanding%2520with%2520free-form%2520language%2520has%2520been%2520widely%2520explored%2520within%2520diverse%2520modalities%2520such%2520as%2520images%252C%2520point%2520clouds%252C%2520and%2520LiDAR.%2520However%252C%2520related%2520studies%2520on%2520event%2520sensors%2520are%2520scarce%2520or%2520narrowly%2520centered%2520on%2520semantic-level%2520understanding.%2520We%2520introduce%2520SEAL%252C%2520the%2520first%2520Semantic-aware%2520Segment%2520Any%2520Events%2520framework%2520that%2520addresses%2520Open-Vocabulary%2520Event%2520Instance%2520Segmentation%2520%2528OV-EIS%2529.%2520Given%2520the%2520visual%2520prompt%252C%2520our%2520model%2520presents%2520a%2520unified%2520framework%2520to%2520support%2520both%2520event%2520segmentation%2520and%2520open-vocabulary%2520mask%2520classification%2520at%2520multiple%2520levels%2520of%2520granularity%252C%2520including%2520instance-level%2520and%2520part-level.%2520To%2520enable%2520thorough%2520evaluation%2520on%2520OV-EIS%252C%2520we%2520curate%2520four%2520benchmarks%2520that%2520cover%2520label%2520granularity%2520from%2520coarse%2520to%2520fine%2520class%2520configurations%2520and%2520semantic%2520granularity%2520from%2520instance-level%2520to%2520part-level%2520understanding.%2520Extensive%2520experiments%2520show%2520that%2520our%2520SEAL%2520largely%2520outperforms%2520proposed%2520baselines%2520in%2520terms%2520of%2520performance%2520and%2520inference%2520speed%2520with%2520a%2520parameter-efficient%2520architecture.%2520In%2520the%2520Appendix%252C%2520we%2520further%2520present%2520a%2520simple%2520variant%2520of%2520our%2520SEAL%2520achieving%2520generic%2520spatiotemporal%2520OV-EIS%2520that%2520does%2520not%2520require%2520any%2520visual%2520prompts%2520from%2520users%2520in%2520the%2520inference.%2520Check%2520out%2520our%2520project%2520page%2520in%2520https%253A//0nandon.github.io/SEAL%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Any%20Events%20with%20Language&entry.906535625=Seungjun%20Lee%20and%20Gim%20Hee%20Lee&entry.1292438233=Scene%20understanding%20with%20free-form%20language%20has%20been%20widely%20explored%20within%20diverse%20modalities%20such%20as%20images%2C%20point%20clouds%2C%20and%20LiDAR.%20However%2C%20related%20studies%20on%20event%20sensors%20are%20scarce%20or%20narrowly%20centered%20on%20semantic-level%20understanding.%20We%20introduce%20SEAL%2C%20the%20first%20Semantic-aware%20Segment%20Any%20Events%20framework%20that%20addresses%20Open-Vocabulary%20Event%20Instance%20Segmentation%20%28OV-EIS%29.%20Given%20the%20visual%20prompt%2C%20our%20model%20presents%20a%20unified%20framework%20to%20support%20both%20event%20segmentation%20and%20open-vocabulary%20mask%20classification%20at%20multiple%20levels%20of%20granularity%2C%20including%20instance-level%20and%20part-level.%20To%20enable%20thorough%20evaluation%20on%20OV-EIS%2C%20we%20curate%20four%20benchmarks%20that%20cover%20label%20granularity%20from%20coarse%20to%20fine%20class%20configurations%20and%20semantic%20granularity%20from%20instance-level%20to%20part-level%20understanding.%20Extensive%20experiments%20show%20that%20our%20SEAL%20largely%20outperforms%20proposed%20baselines%20in%20terms%20of%20performance%20and%20inference%20speed%20with%20a%20parameter-efficient%20architecture.%20In%20the%20Appendix%2C%20we%20further%20present%20a%20simple%20variant%20of%20our%20SEAL%20achieving%20generic%20spatiotemporal%20OV-EIS%20that%20does%20not%20require%20any%20visual%20prompts%20from%20users%20in%20the%20inference.%20Check%20out%20our%20project%20page%20in%20https%3A//0nandon.github.io/SEAL&entry.1838667208=http%3A//arxiv.org/abs/2601.23159v1&entry.124074799=Read"},
{"title": "A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions", "author": "Ji Zhou and Yilin Ding and Yongqi Zhao and Jiachen Xu and Arno Eichberger", "abstract": "Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.", "link": "http://arxiv.org/abs/2601.22830v1", "date": "2026-01-30", "relevancy": 2.3287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5884}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Evaluation%20of%20Large%20Vision-Language%20Models%20for%202D%20Object%20Detection%20under%20SOTIF%20Conditions&body=Title%3A%20A%20Comparative%20Evaluation%20of%20Large%20Vision-Language%20Models%20for%202D%20Object%20Detection%20under%20SOTIF%20Conditions%0AAuthor%3A%20Ji%20Zhou%20and%20Yilin%20Ding%20and%20Yongqi%20Zhao%20and%20Jiachen%20Xu%20and%20Arno%20Eichberger%0AAbstract%3A%20Reliable%20environmental%20perception%20remains%20one%20of%20the%20main%20obstacles%20for%20safe%20operation%20of%20automated%20vehicles.%20Safety%20of%20the%20Intended%20Functionality%20%28SOTIF%29%20concerns%20safety%20risks%20from%20perception%20insufficiencies%2C%20particularly%20under%20adverse%20conditions%20where%20conventional%20detectors%20often%20falter.%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20demonstrate%20promising%20semantic%20reasoning%2C%20their%20quantitative%20effectiveness%20for%20safety-critical%202D%20object%20detection%20is%20underexplored.%20This%20paper%20presents%20a%20systematic%20evaluation%20of%20ten%20representative%20LVLMs%20using%20the%20PeSOTIF%20dataset%2C%20a%20benchmark%20specifically%20curated%20for%20long-tail%20traffic%20scenarios%20and%20environmental%20degradations.%20Performance%20is%20quantitatively%20compared%20against%20the%20classical%20perception%20approach%2C%20a%20YOLO-based%20detector.%20Experimental%20results%20reveal%20a%20critical%20trade-off%3A%20top-performing%20LVLMs%20%28e.g.%2C%20Gemini%203%2C%20Doubao%29%20surpass%20the%20YOLO%20baseline%20in%20recall%20by%20over%2025%25%20in%20complex%20natural%20scenarios%2C%20exhibiting%20superior%20robustness%20to%20visual%20degradation.%20Conversely%2C%20the%20baseline%20retains%20an%20advantage%20in%20geometric%20precision%20for%20synthetic%20perturbations.%20These%20findings%20highlight%20the%20complementary%20strengths%20of%20semantic%20reasoning%20versus%20geometric%20regression%2C%20supporting%20the%20use%20of%20LVLMs%20as%20high-level%20safety%20validators%20in%20SOTIF-oriented%20automated%20driving%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Evaluation%2520of%2520Large%2520Vision-Language%2520Models%2520for%25202D%2520Object%2520Detection%2520under%2520SOTIF%2520Conditions%26entry.906535625%3DJi%2520Zhou%2520and%2520Yilin%2520Ding%2520and%2520Yongqi%2520Zhao%2520and%2520Jiachen%2520Xu%2520and%2520Arno%2520Eichberger%26entry.1292438233%3DReliable%2520environmental%2520perception%2520remains%2520one%2520of%2520the%2520main%2520obstacles%2520for%2520safe%2520operation%2520of%2520automated%2520vehicles.%2520Safety%2520of%2520the%2520Intended%2520Functionality%2520%2528SOTIF%2529%2520concerns%2520safety%2520risks%2520from%2520perception%2520insufficiencies%252C%2520particularly%2520under%2520adverse%2520conditions%2520where%2520conventional%2520detectors%2520often%2520falter.%2520While%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520demonstrate%2520promising%2520semantic%2520reasoning%252C%2520their%2520quantitative%2520effectiveness%2520for%2520safety-critical%25202D%2520object%2520detection%2520is%2520underexplored.%2520This%2520paper%2520presents%2520a%2520systematic%2520evaluation%2520of%2520ten%2520representative%2520LVLMs%2520using%2520the%2520PeSOTIF%2520dataset%252C%2520a%2520benchmark%2520specifically%2520curated%2520for%2520long-tail%2520traffic%2520scenarios%2520and%2520environmental%2520degradations.%2520Performance%2520is%2520quantitatively%2520compared%2520against%2520the%2520classical%2520perception%2520approach%252C%2520a%2520YOLO-based%2520detector.%2520Experimental%2520results%2520reveal%2520a%2520critical%2520trade-off%253A%2520top-performing%2520LVLMs%2520%2528e.g.%252C%2520Gemini%25203%252C%2520Doubao%2529%2520surpass%2520the%2520YOLO%2520baseline%2520in%2520recall%2520by%2520over%252025%2525%2520in%2520complex%2520natural%2520scenarios%252C%2520exhibiting%2520superior%2520robustness%2520to%2520visual%2520degradation.%2520Conversely%252C%2520the%2520baseline%2520retains%2520an%2520advantage%2520in%2520geometric%2520precision%2520for%2520synthetic%2520perturbations.%2520These%2520findings%2520highlight%2520the%2520complementary%2520strengths%2520of%2520semantic%2520reasoning%2520versus%2520geometric%2520regression%252C%2520supporting%2520the%2520use%2520of%2520LVLMs%2520as%2520high-level%2520safety%2520validators%2520in%2520SOTIF-oriented%2520automated%2520driving%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Evaluation%20of%20Large%20Vision-Language%20Models%20for%202D%20Object%20Detection%20under%20SOTIF%20Conditions&entry.906535625=Ji%20Zhou%20and%20Yilin%20Ding%20and%20Yongqi%20Zhao%20and%20Jiachen%20Xu%20and%20Arno%20Eichberger&entry.1292438233=Reliable%20environmental%20perception%20remains%20one%20of%20the%20main%20obstacles%20for%20safe%20operation%20of%20automated%20vehicles.%20Safety%20of%20the%20Intended%20Functionality%20%28SOTIF%29%20concerns%20safety%20risks%20from%20perception%20insufficiencies%2C%20particularly%20under%20adverse%20conditions%20where%20conventional%20detectors%20often%20falter.%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20demonstrate%20promising%20semantic%20reasoning%2C%20their%20quantitative%20effectiveness%20for%20safety-critical%202D%20object%20detection%20is%20underexplored.%20This%20paper%20presents%20a%20systematic%20evaluation%20of%20ten%20representative%20LVLMs%20using%20the%20PeSOTIF%20dataset%2C%20a%20benchmark%20specifically%20curated%20for%20long-tail%20traffic%20scenarios%20and%20environmental%20degradations.%20Performance%20is%20quantitatively%20compared%20against%20the%20classical%20perception%20approach%2C%20a%20YOLO-based%20detector.%20Experimental%20results%20reveal%20a%20critical%20trade-off%3A%20top-performing%20LVLMs%20%28e.g.%2C%20Gemini%203%2C%20Doubao%29%20surpass%20the%20YOLO%20baseline%20in%20recall%20by%20over%2025%25%20in%20complex%20natural%20scenarios%2C%20exhibiting%20superior%20robustness%20to%20visual%20degradation.%20Conversely%2C%20the%20baseline%20retains%20an%20advantage%20in%20geometric%20precision%20for%20synthetic%20perturbations.%20These%20findings%20highlight%20the%20complementary%20strengths%20of%20semantic%20reasoning%20versus%20geometric%20regression%2C%20supporting%20the%20use%20of%20LVLMs%20as%20high-level%20safety%20validators%20in%20SOTIF-oriented%20automated%20driving%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.22830v1&entry.124074799=Read"},
{"title": "Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion", "author": "Dennis Sprute and Hanna Senke and Holger Flatt", "abstract": "Supervised machine learning algorithms play a crucial role in optical quality control within industrial production. These approaches require representative datasets for effective model training. However, while non-defective components are frequent, defective parts are rare in production, resulting in highly imbalanced datasets that adversely impact model performance. Existing strategies to address this challenge, such as specialized loss functions or traditional data augmentation techniques, have limitations, including the need for careful hyperparameter tuning or the alteration of only simple image features. Therefore, this work explores the potential of generative artificial intelligence (GenAI) as an alternative method for expanding limited datasets and enhancing supervised machine learning performance. Specifically, we investigate Stable Diffusion and CycleGAN as image generation models, focusing on the segmentation of combine harvester components in thermal images for subsequent defect detection. Our results demonstrate that dataset expansion using Stable Diffusion yields the most significant improvement, enhancing segmentation performance by 4.6 %, resulting in a Mean Intersection over Union (Mean IoU) of 84.6 %.", "link": "http://arxiv.org/abs/2601.22961v1", "date": "2026-01-30", "relevancy": 2.3277, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5984}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.583}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Supervised%20Machine%20Learning%20Performance%20in%20Optical%20Quality%20Control%20via%20Generative%20AI%20for%20Dataset%20Expansion&body=Title%3A%20Improving%20Supervised%20Machine%20Learning%20Performance%20in%20Optical%20Quality%20Control%20via%20Generative%20AI%20for%20Dataset%20Expansion%0AAuthor%3A%20Dennis%20Sprute%20and%20Hanna%20Senke%20and%20Holger%20Flatt%0AAbstract%3A%20Supervised%20machine%20learning%20algorithms%20play%20a%20crucial%20role%20in%20optical%20quality%20control%20within%20industrial%20production.%20These%20approaches%20require%20representative%20datasets%20for%20effective%20model%20training.%20However%2C%20while%20non-defective%20components%20are%20frequent%2C%20defective%20parts%20are%20rare%20in%20production%2C%20resulting%20in%20highly%20imbalanced%20datasets%20that%20adversely%20impact%20model%20performance.%20Existing%20strategies%20to%20address%20this%20challenge%2C%20such%20as%20specialized%20loss%20functions%20or%20traditional%20data%20augmentation%20techniques%2C%20have%20limitations%2C%20including%20the%20need%20for%20careful%20hyperparameter%20tuning%20or%20the%20alteration%20of%20only%20simple%20image%20features.%20Therefore%2C%20this%20work%20explores%20the%20potential%20of%20generative%20artificial%20intelligence%20%28GenAI%29%20as%20an%20alternative%20method%20for%20expanding%20limited%20datasets%20and%20enhancing%20supervised%20machine%20learning%20performance.%20Specifically%2C%20we%20investigate%20Stable%20Diffusion%20and%20CycleGAN%20as%20image%20generation%20models%2C%20focusing%20on%20the%20segmentation%20of%20combine%20harvester%20components%20in%20thermal%20images%20for%20subsequent%20defect%20detection.%20Our%20results%20demonstrate%20that%20dataset%20expansion%20using%20Stable%20Diffusion%20yields%20the%20most%20significant%20improvement%2C%20enhancing%20segmentation%20performance%20by%204.6%20%25%2C%20resulting%20in%20a%20Mean%20Intersection%20over%20Union%20%28Mean%20IoU%29%20of%2084.6%20%25.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Supervised%2520Machine%2520Learning%2520Performance%2520in%2520Optical%2520Quality%2520Control%2520via%2520Generative%2520AI%2520for%2520Dataset%2520Expansion%26entry.906535625%3DDennis%2520Sprute%2520and%2520Hanna%2520Senke%2520and%2520Holger%2520Flatt%26entry.1292438233%3DSupervised%2520machine%2520learning%2520algorithms%2520play%2520a%2520crucial%2520role%2520in%2520optical%2520quality%2520control%2520within%2520industrial%2520production.%2520These%2520approaches%2520require%2520representative%2520datasets%2520for%2520effective%2520model%2520training.%2520However%252C%2520while%2520non-defective%2520components%2520are%2520frequent%252C%2520defective%2520parts%2520are%2520rare%2520in%2520production%252C%2520resulting%2520in%2520highly%2520imbalanced%2520datasets%2520that%2520adversely%2520impact%2520model%2520performance.%2520Existing%2520strategies%2520to%2520address%2520this%2520challenge%252C%2520such%2520as%2520specialized%2520loss%2520functions%2520or%2520traditional%2520data%2520augmentation%2520techniques%252C%2520have%2520limitations%252C%2520including%2520the%2520need%2520for%2520careful%2520hyperparameter%2520tuning%2520or%2520the%2520alteration%2520of%2520only%2520simple%2520image%2520features.%2520Therefore%252C%2520this%2520work%2520explores%2520the%2520potential%2520of%2520generative%2520artificial%2520intelligence%2520%2528GenAI%2529%2520as%2520an%2520alternative%2520method%2520for%2520expanding%2520limited%2520datasets%2520and%2520enhancing%2520supervised%2520machine%2520learning%2520performance.%2520Specifically%252C%2520we%2520investigate%2520Stable%2520Diffusion%2520and%2520CycleGAN%2520as%2520image%2520generation%2520models%252C%2520focusing%2520on%2520the%2520segmentation%2520of%2520combine%2520harvester%2520components%2520in%2520thermal%2520images%2520for%2520subsequent%2520defect%2520detection.%2520Our%2520results%2520demonstrate%2520that%2520dataset%2520expansion%2520using%2520Stable%2520Diffusion%2520yields%2520the%2520most%2520significant%2520improvement%252C%2520enhancing%2520segmentation%2520performance%2520by%25204.6%2520%2525%252C%2520resulting%2520in%2520a%2520Mean%2520Intersection%2520over%2520Union%2520%2528Mean%2520IoU%2529%2520of%252084.6%2520%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Supervised%20Machine%20Learning%20Performance%20in%20Optical%20Quality%20Control%20via%20Generative%20AI%20for%20Dataset%20Expansion&entry.906535625=Dennis%20Sprute%20and%20Hanna%20Senke%20and%20Holger%20Flatt&entry.1292438233=Supervised%20machine%20learning%20algorithms%20play%20a%20crucial%20role%20in%20optical%20quality%20control%20within%20industrial%20production.%20These%20approaches%20require%20representative%20datasets%20for%20effective%20model%20training.%20However%2C%20while%20non-defective%20components%20are%20frequent%2C%20defective%20parts%20are%20rare%20in%20production%2C%20resulting%20in%20highly%20imbalanced%20datasets%20that%20adversely%20impact%20model%20performance.%20Existing%20strategies%20to%20address%20this%20challenge%2C%20such%20as%20specialized%20loss%20functions%20or%20traditional%20data%20augmentation%20techniques%2C%20have%20limitations%2C%20including%20the%20need%20for%20careful%20hyperparameter%20tuning%20or%20the%20alteration%20of%20only%20simple%20image%20features.%20Therefore%2C%20this%20work%20explores%20the%20potential%20of%20generative%20artificial%20intelligence%20%28GenAI%29%20as%20an%20alternative%20method%20for%20expanding%20limited%20datasets%20and%20enhancing%20supervised%20machine%20learning%20performance.%20Specifically%2C%20we%20investigate%20Stable%20Diffusion%20and%20CycleGAN%20as%20image%20generation%20models%2C%20focusing%20on%20the%20segmentation%20of%20combine%20harvester%20components%20in%20thermal%20images%20for%20subsequent%20defect%20detection.%20Our%20results%20demonstrate%20that%20dataset%20expansion%20using%20Stable%20Diffusion%20yields%20the%20most%20significant%20improvement%2C%20enhancing%20segmentation%20performance%20by%204.6%20%25%2C%20resulting%20in%20a%20Mean%20Intersection%20over%20Union%20%28Mean%20IoU%29%20of%2084.6%20%25.&entry.1838667208=http%3A//arxiv.org/abs/2601.22961v1&entry.124074799=Read"},
{"title": "Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction", "author": "Refael Sheffer and Chen Pinchover and Haim Zisman and Dror Ozeri and Roee Litman", "abstract": "Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more. Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.\n  We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images. Our solution is based on the celebrated Neural Radiance Fields (NeRF), a recent 3D reconstruction method. Additionally, we include specific image capture considerations, which dictate the needed illumination to successfully expose the scene beneath the canopy. To better cope with the poorly lit understory, we employ a low light loss. Finally, we propose two complementary approaches to remove occluding canopy elements by controlling per-ray integration procedure.\n  To validate the value of our approach, we present two possible downstream tasks. For the task of search and rescue (SAR), we demonstrate that our method enables person detection which achieves promising results compared to thermal AOS (using only RGB images). Additionally, we show the potential of our approach for forest inventory tasks like tree counting. These results position our approach as a cost-effective, high-resolution alternative to specialized sensors for SAR, trail mapping, and forest-inventory tasks.", "link": "http://arxiv.org/abs/2601.22861v1", "date": "2026-01-30", "relevancy": 2.3101, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6007}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Under-Canopy%20Terrain%20Reconstruction%20in%20Dense%20Forests%20Using%20RGB%20Imaging%20and%20Neural%203D%20Reconstruction&body=Title%3A%20Under-Canopy%20Terrain%20Reconstruction%20in%20Dense%20Forests%20Using%20RGB%20Imaging%20and%20Neural%203D%20Reconstruction%0AAuthor%3A%20Refael%20Sheffer%20and%20Chen%20Pinchover%20and%20Haim%20Zisman%20and%20Dror%20Ozeri%20and%20Roee%20Litman%0AAbstract%3A%20Mapping%20the%20terrain%20and%20understory%20hidden%20beneath%20dense%20forest%20canopies%20is%20of%20great%20interest%20for%20numerous%20applications%20such%20as%20search%20and%20rescue%2C%20trail%20mapping%2C%20forest%20inventory%20tasks%2C%20and%20more.%20Existing%20solutions%20rely%20on%20specialized%20sensors%3A%20either%20heavy%2C%20costly%20airborne%20LiDAR%2C%20or%20Airborne%20Optical%20Sectioning%20%28AOS%29%2C%20which%20uses%20thermal%20synthetic%20aperture%20photography%20and%20is%20tailored%20for%20person%20detection.%0A%20%20We%20introduce%20a%20novel%20approach%20for%20the%20reconstruction%20of%20canopy-free%2C%20photorealistic%20ground%20views%20using%20only%20conventional%20RGB%20images.%20Our%20solution%20is%20based%20on%20the%20celebrated%20Neural%20Radiance%20Fields%20%28NeRF%29%2C%20a%20recent%203D%20reconstruction%20method.%20Additionally%2C%20we%20include%20specific%20image%20capture%20considerations%2C%20which%20dictate%20the%20needed%20illumination%20to%20successfully%20expose%20the%20scene%20beneath%20the%20canopy.%20To%20better%20cope%20with%20the%20poorly%20lit%20understory%2C%20we%20employ%20a%20low%20light%20loss.%20Finally%2C%20we%20propose%20two%20complementary%20approaches%20to%20remove%20occluding%20canopy%20elements%20by%20controlling%20per-ray%20integration%20procedure.%0A%20%20To%20validate%20the%20value%20of%20our%20approach%2C%20we%20present%20two%20possible%20downstream%20tasks.%20For%20the%20task%20of%20search%20and%20rescue%20%28SAR%29%2C%20we%20demonstrate%20that%20our%20method%20enables%20person%20detection%20which%20achieves%20promising%20results%20compared%20to%20thermal%20AOS%20%28using%20only%20RGB%20images%29.%20Additionally%2C%20we%20show%20the%20potential%20of%20our%20approach%20for%20forest%20inventory%20tasks%20like%20tree%20counting.%20These%20results%20position%20our%20approach%20as%20a%20cost-effective%2C%20high-resolution%20alternative%20to%20specialized%20sensors%20for%20SAR%2C%20trail%20mapping%2C%20and%20forest-inventory%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnder-Canopy%2520Terrain%2520Reconstruction%2520in%2520Dense%2520Forests%2520Using%2520RGB%2520Imaging%2520and%2520Neural%25203D%2520Reconstruction%26entry.906535625%3DRefael%2520Sheffer%2520and%2520Chen%2520Pinchover%2520and%2520Haim%2520Zisman%2520and%2520Dror%2520Ozeri%2520and%2520Roee%2520Litman%26entry.1292438233%3DMapping%2520the%2520terrain%2520and%2520understory%2520hidden%2520beneath%2520dense%2520forest%2520canopies%2520is%2520of%2520great%2520interest%2520for%2520numerous%2520applications%2520such%2520as%2520search%2520and%2520rescue%252C%2520trail%2520mapping%252C%2520forest%2520inventory%2520tasks%252C%2520and%2520more.%2520Existing%2520solutions%2520rely%2520on%2520specialized%2520sensors%253A%2520either%2520heavy%252C%2520costly%2520airborne%2520LiDAR%252C%2520or%2520Airborne%2520Optical%2520Sectioning%2520%2528AOS%2529%252C%2520which%2520uses%2520thermal%2520synthetic%2520aperture%2520photography%2520and%2520is%2520tailored%2520for%2520person%2520detection.%250A%2520%2520We%2520introduce%2520a%2520novel%2520approach%2520for%2520the%2520reconstruction%2520of%2520canopy-free%252C%2520photorealistic%2520ground%2520views%2520using%2520only%2520conventional%2520RGB%2520images.%2520Our%2520solution%2520is%2520based%2520on%2520the%2520celebrated%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%252C%2520a%2520recent%25203D%2520reconstruction%2520method.%2520Additionally%252C%2520we%2520include%2520specific%2520image%2520capture%2520considerations%252C%2520which%2520dictate%2520the%2520needed%2520illumination%2520to%2520successfully%2520expose%2520the%2520scene%2520beneath%2520the%2520canopy.%2520To%2520better%2520cope%2520with%2520the%2520poorly%2520lit%2520understory%252C%2520we%2520employ%2520a%2520low%2520light%2520loss.%2520Finally%252C%2520we%2520propose%2520two%2520complementary%2520approaches%2520to%2520remove%2520occluding%2520canopy%2520elements%2520by%2520controlling%2520per-ray%2520integration%2520procedure.%250A%2520%2520To%2520validate%2520the%2520value%2520of%2520our%2520approach%252C%2520we%2520present%2520two%2520possible%2520downstream%2520tasks.%2520For%2520the%2520task%2520of%2520search%2520and%2520rescue%2520%2528SAR%2529%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520enables%2520person%2520detection%2520which%2520achieves%2520promising%2520results%2520compared%2520to%2520thermal%2520AOS%2520%2528using%2520only%2520RGB%2520images%2529.%2520Additionally%252C%2520we%2520show%2520the%2520potential%2520of%2520our%2520approach%2520for%2520forest%2520inventory%2520tasks%2520like%2520tree%2520counting.%2520These%2520results%2520position%2520our%2520approach%2520as%2520a%2520cost-effective%252C%2520high-resolution%2520alternative%2520to%2520specialized%2520sensors%2520for%2520SAR%252C%2520trail%2520mapping%252C%2520and%2520forest-inventory%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Under-Canopy%20Terrain%20Reconstruction%20in%20Dense%20Forests%20Using%20RGB%20Imaging%20and%20Neural%203D%20Reconstruction&entry.906535625=Refael%20Sheffer%20and%20Chen%20Pinchover%20and%20Haim%20Zisman%20and%20Dror%20Ozeri%20and%20Roee%20Litman&entry.1292438233=Mapping%20the%20terrain%20and%20understory%20hidden%20beneath%20dense%20forest%20canopies%20is%20of%20great%20interest%20for%20numerous%20applications%20such%20as%20search%20and%20rescue%2C%20trail%20mapping%2C%20forest%20inventory%20tasks%2C%20and%20more.%20Existing%20solutions%20rely%20on%20specialized%20sensors%3A%20either%20heavy%2C%20costly%20airborne%20LiDAR%2C%20or%20Airborne%20Optical%20Sectioning%20%28AOS%29%2C%20which%20uses%20thermal%20synthetic%20aperture%20photography%20and%20is%20tailored%20for%20person%20detection.%0A%20%20We%20introduce%20a%20novel%20approach%20for%20the%20reconstruction%20of%20canopy-free%2C%20photorealistic%20ground%20views%20using%20only%20conventional%20RGB%20images.%20Our%20solution%20is%20based%20on%20the%20celebrated%20Neural%20Radiance%20Fields%20%28NeRF%29%2C%20a%20recent%203D%20reconstruction%20method.%20Additionally%2C%20we%20include%20specific%20image%20capture%20considerations%2C%20which%20dictate%20the%20needed%20illumination%20to%20successfully%20expose%20the%20scene%20beneath%20the%20canopy.%20To%20better%20cope%20with%20the%20poorly%20lit%20understory%2C%20we%20employ%20a%20low%20light%20loss.%20Finally%2C%20we%20propose%20two%20complementary%20approaches%20to%20remove%20occluding%20canopy%20elements%20by%20controlling%20per-ray%20integration%20procedure.%0A%20%20To%20validate%20the%20value%20of%20our%20approach%2C%20we%20present%20two%20possible%20downstream%20tasks.%20For%20the%20task%20of%20search%20and%20rescue%20%28SAR%29%2C%20we%20demonstrate%20that%20our%20method%20enables%20person%20detection%20which%20achieves%20promising%20results%20compared%20to%20thermal%20AOS%20%28using%20only%20RGB%20images%29.%20Additionally%2C%20we%20show%20the%20potential%20of%20our%20approach%20for%20forest%20inventory%20tasks%20like%20tree%20counting.%20These%20results%20position%20our%20approach%20as%20a%20cost-effective%2C%20high-resolution%20alternative%20to%20specialized%20sensors%20for%20SAR%2C%20trail%20mapping%2C%20and%20forest-inventory%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.22861v1&entry.124074799=Read"},
{"title": "Quantum Super-resolution by Adaptive Non-local Observables", "author": "Hsin-Yi Lin and Huan-Hsin Tseng and Samuel Yen-Chi Chen and Shinjae Yoo", "abstract": "Super-resolution (SR) seeks to reconstruct high-resolution (HR) data from low-resolution (LR) observations. Classical deep learning methods have advanced SR substantially, but require increasingly deeper networks, large datasets, and heavy computation to capture fine-grained correlations. In this work, we present the \\emph{first study} to investigate quantum circuits for SR. We propose a framework based on Variational Quantum Circuits (VQCs) with \\emph{Adaptive Non-Local Observable} (ANO) measurements. Unlike conventional VQCs with fixed Pauli readouts, ANO introduces trainable multi-qubit Hermitian observables, allowing the measurement process to adapt during training. This design leverages the high-dimensional Hilbert space of quantum systems and the representational structure provided by entanglement and superposition. Experiments demonstrate that ANO-VQCs achieve up to five-fold higher resolution with a relatively small model size, suggesting a promising new direction at the intersection of quantum machine learning and super-resolution.", "link": "http://arxiv.org/abs/2601.14433v2", "date": "2026-01-30", "relevancy": 2.3084, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Super-resolution%20by%20Adaptive%20Non-local%20Observables&body=Title%3A%20Quantum%20Super-resolution%20by%20Adaptive%20Non-local%20Observables%0AAuthor%3A%20Hsin-Yi%20Lin%20and%20Huan-Hsin%20Tseng%20and%20Samuel%20Yen-Chi%20Chen%20and%20Shinjae%20Yoo%0AAbstract%3A%20Super-resolution%20%28SR%29%20seeks%20to%20reconstruct%20high-resolution%20%28HR%29%20data%20from%20low-resolution%20%28LR%29%20observations.%20Classical%20deep%20learning%20methods%20have%20advanced%20SR%20substantially%2C%20but%20require%20increasingly%20deeper%20networks%2C%20large%20datasets%2C%20and%20heavy%20computation%20to%20capture%20fine-grained%20correlations.%20In%20this%20work%2C%20we%20present%20the%20%5Cemph%7Bfirst%20study%7D%20to%20investigate%20quantum%20circuits%20for%20SR.%20We%20propose%20a%20framework%20based%20on%20Variational%20Quantum%20Circuits%20%28VQCs%29%20with%20%5Cemph%7BAdaptive%20Non-Local%20Observable%7D%20%28ANO%29%20measurements.%20Unlike%20conventional%20VQCs%20with%20fixed%20Pauli%20readouts%2C%20ANO%20introduces%20trainable%20multi-qubit%20Hermitian%20observables%2C%20allowing%20the%20measurement%20process%20to%20adapt%20during%20training.%20This%20design%20leverages%20the%20high-dimensional%20Hilbert%20space%20of%20quantum%20systems%20and%20the%20representational%20structure%20provided%20by%20entanglement%20and%20superposition.%20Experiments%20demonstrate%20that%20ANO-VQCs%20achieve%20up%20to%20five-fold%20higher%20resolution%20with%20a%20relatively%20small%20model%20size%2C%20suggesting%20a%20promising%20new%20direction%20at%20the%20intersection%20of%20quantum%20machine%20learning%20and%20super-resolution.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14433v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Super-resolution%2520by%2520Adaptive%2520Non-local%2520Observables%26entry.906535625%3DHsin-Yi%2520Lin%2520and%2520Huan-Hsin%2520Tseng%2520and%2520Samuel%2520Yen-Chi%2520Chen%2520and%2520Shinjae%2520Yoo%26entry.1292438233%3DSuper-resolution%2520%2528SR%2529%2520seeks%2520to%2520reconstruct%2520high-resolution%2520%2528HR%2529%2520data%2520from%2520low-resolution%2520%2528LR%2529%2520observations.%2520Classical%2520deep%2520learning%2520methods%2520have%2520advanced%2520SR%2520substantially%252C%2520but%2520require%2520increasingly%2520deeper%2520networks%252C%2520large%2520datasets%252C%2520and%2520heavy%2520computation%2520to%2520capture%2520fine-grained%2520correlations.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520%255Cemph%257Bfirst%2520study%257D%2520to%2520investigate%2520quantum%2520circuits%2520for%2520SR.%2520We%2520propose%2520a%2520framework%2520based%2520on%2520Variational%2520Quantum%2520Circuits%2520%2528VQCs%2529%2520with%2520%255Cemph%257BAdaptive%2520Non-Local%2520Observable%257D%2520%2528ANO%2529%2520measurements.%2520Unlike%2520conventional%2520VQCs%2520with%2520fixed%2520Pauli%2520readouts%252C%2520ANO%2520introduces%2520trainable%2520multi-qubit%2520Hermitian%2520observables%252C%2520allowing%2520the%2520measurement%2520process%2520to%2520adapt%2520during%2520training.%2520This%2520design%2520leverages%2520the%2520high-dimensional%2520Hilbert%2520space%2520of%2520quantum%2520systems%2520and%2520the%2520representational%2520structure%2520provided%2520by%2520entanglement%2520and%2520superposition.%2520Experiments%2520demonstrate%2520that%2520ANO-VQCs%2520achieve%2520up%2520to%2520five-fold%2520higher%2520resolution%2520with%2520a%2520relatively%2520small%2520model%2520size%252C%2520suggesting%2520a%2520promising%2520new%2520direction%2520at%2520the%2520intersection%2520of%2520quantum%2520machine%2520learning%2520and%2520super-resolution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14433v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Super-resolution%20by%20Adaptive%20Non-local%20Observables&entry.906535625=Hsin-Yi%20Lin%20and%20Huan-Hsin%20Tseng%20and%20Samuel%20Yen-Chi%20Chen%20and%20Shinjae%20Yoo&entry.1292438233=Super-resolution%20%28SR%29%20seeks%20to%20reconstruct%20high-resolution%20%28HR%29%20data%20from%20low-resolution%20%28LR%29%20observations.%20Classical%20deep%20learning%20methods%20have%20advanced%20SR%20substantially%2C%20but%20require%20increasingly%20deeper%20networks%2C%20large%20datasets%2C%20and%20heavy%20computation%20to%20capture%20fine-grained%20correlations.%20In%20this%20work%2C%20we%20present%20the%20%5Cemph%7Bfirst%20study%7D%20to%20investigate%20quantum%20circuits%20for%20SR.%20We%20propose%20a%20framework%20based%20on%20Variational%20Quantum%20Circuits%20%28VQCs%29%20with%20%5Cemph%7BAdaptive%20Non-Local%20Observable%7D%20%28ANO%29%20measurements.%20Unlike%20conventional%20VQCs%20with%20fixed%20Pauli%20readouts%2C%20ANO%20introduces%20trainable%20multi-qubit%20Hermitian%20observables%2C%20allowing%20the%20measurement%20process%20to%20adapt%20during%20training.%20This%20design%20leverages%20the%20high-dimensional%20Hilbert%20space%20of%20quantum%20systems%20and%20the%20representational%20structure%20provided%20by%20entanglement%20and%20superposition.%20Experiments%20demonstrate%20that%20ANO-VQCs%20achieve%20up%20to%20five-fold%20higher%20resolution%20with%20a%20relatively%20small%20model%20size%2C%20suggesting%20a%20promising%20new%20direction%20at%20the%20intersection%20of%20quantum%20machine%20learning%20and%20super-resolution.&entry.1838667208=http%3A//arxiv.org/abs/2601.14433v2&entry.124074799=Read"},
{"title": "Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models", "author": "Charles Westphal and Keivan Navaie and Fernando E. Rosas", "abstract": "Fine-tuned LLMs can covertly encode prompt secrets into outputs via steganographic channels. Prior work demonstrated this threat but relied on trivially recoverable encodings. We formalize payload recoverability via classifier accuracy and show previous schemes achieve 100\\% recoverability. In response, we introduce low-recoverability steganography, replacing arbitrary mappings with embedding-space-derived ones. For Llama-8B (LoRA) and Ministral-8B (LoRA) trained on TrojanStego prompts, exact secret recovery rises from 17$\\rightarrow$30\\% (+78\\%) and 24$\\rightarrow$43\\% (+80\\%) respectively, while on Llama-70B (LoRA) trained on Wiki prompts, it climbs from 9$\\rightarrow$19\\% (+123\\%), all while reducing payload recoverability. We then discuss detection. We argue that detecting fine-tuning-based steganographic attacks requires approaches beyond traditional steganalysis. Standard approaches measure distributional shift, which is an expected side-effect of fine-tuning. Instead, we propose a mechanistic interpretability approach: linear probes trained on later-layer activations detect the secret with up to 33\\% higher accuracy in fine-tuned models compared to base models, even for low-recoverability schemes. This suggests that malicious fine-tuning leaves actionable internal signatures amenable to interpretability-based defenses.", "link": "http://arxiv.org/abs/2601.22818v1", "date": "2026-01-30", "relevancy": 2.2909, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4653}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hide%20and%20Seek%20in%20Embedding%20Space%3A%20Geometry-based%20Steganography%20and%20Detection%20in%20Large%20Language%20Models&body=Title%3A%20Hide%20and%20Seek%20in%20Embedding%20Space%3A%20Geometry-based%20Steganography%20and%20Detection%20in%20Large%20Language%20Models%0AAuthor%3A%20Charles%20Westphal%20and%20Keivan%20Navaie%20and%20Fernando%20E.%20Rosas%0AAbstract%3A%20Fine-tuned%20LLMs%20can%20covertly%20encode%20prompt%20secrets%20into%20outputs%20via%20steganographic%20channels.%20Prior%20work%20demonstrated%20this%20threat%20but%20relied%20on%20trivially%20recoverable%20encodings.%20We%20formalize%20payload%20recoverability%20via%20classifier%20accuracy%20and%20show%20previous%20schemes%20achieve%20100%5C%25%20recoverability.%20In%20response%2C%20we%20introduce%20low-recoverability%20steganography%2C%20replacing%20arbitrary%20mappings%20with%20embedding-space-derived%20ones.%20For%20Llama-8B%20%28LoRA%29%20and%20Ministral-8B%20%28LoRA%29%20trained%20on%20TrojanStego%20prompts%2C%20exact%20secret%20recovery%20rises%20from%2017%24%5Crightarrow%2430%5C%25%20%28%2B78%5C%25%29%20and%2024%24%5Crightarrow%2443%5C%25%20%28%2B80%5C%25%29%20respectively%2C%20while%20on%20Llama-70B%20%28LoRA%29%20trained%20on%20Wiki%20prompts%2C%20it%20climbs%20from%209%24%5Crightarrow%2419%5C%25%20%28%2B123%5C%25%29%2C%20all%20while%20reducing%20payload%20recoverability.%20We%20then%20discuss%20detection.%20We%20argue%20that%20detecting%20fine-tuning-based%20steganographic%20attacks%20requires%20approaches%20beyond%20traditional%20steganalysis.%20Standard%20approaches%20measure%20distributional%20shift%2C%20which%20is%20an%20expected%20side-effect%20of%20fine-tuning.%20Instead%2C%20we%20propose%20a%20mechanistic%20interpretability%20approach%3A%20linear%20probes%20trained%20on%20later-layer%20activations%20detect%20the%20secret%20with%20up%20to%2033%5C%25%20higher%20accuracy%20in%20fine-tuned%20models%20compared%20to%20base%20models%2C%20even%20for%20low-recoverability%20schemes.%20This%20suggests%20that%20malicious%20fine-tuning%20leaves%20actionable%20internal%20signatures%20amenable%20to%20interpretability-based%20defenses.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHide%2520and%2520Seek%2520in%2520Embedding%2520Space%253A%2520Geometry-based%2520Steganography%2520and%2520Detection%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DCharles%2520Westphal%2520and%2520Keivan%2520Navaie%2520and%2520Fernando%2520E.%2520Rosas%26entry.1292438233%3DFine-tuned%2520LLMs%2520can%2520covertly%2520encode%2520prompt%2520secrets%2520into%2520outputs%2520via%2520steganographic%2520channels.%2520Prior%2520work%2520demonstrated%2520this%2520threat%2520but%2520relied%2520on%2520trivially%2520recoverable%2520encodings.%2520We%2520formalize%2520payload%2520recoverability%2520via%2520classifier%2520accuracy%2520and%2520show%2520previous%2520schemes%2520achieve%2520100%255C%2525%2520recoverability.%2520In%2520response%252C%2520we%2520introduce%2520low-recoverability%2520steganography%252C%2520replacing%2520arbitrary%2520mappings%2520with%2520embedding-space-derived%2520ones.%2520For%2520Llama-8B%2520%2528LoRA%2529%2520and%2520Ministral-8B%2520%2528LoRA%2529%2520trained%2520on%2520TrojanStego%2520prompts%252C%2520exact%2520secret%2520recovery%2520rises%2520from%252017%2524%255Crightarrow%252430%255C%2525%2520%2528%252B78%255C%2525%2529%2520and%252024%2524%255Crightarrow%252443%255C%2525%2520%2528%252B80%255C%2525%2529%2520respectively%252C%2520while%2520on%2520Llama-70B%2520%2528LoRA%2529%2520trained%2520on%2520Wiki%2520prompts%252C%2520it%2520climbs%2520from%25209%2524%255Crightarrow%252419%255C%2525%2520%2528%252B123%255C%2525%2529%252C%2520all%2520while%2520reducing%2520payload%2520recoverability.%2520We%2520then%2520discuss%2520detection.%2520We%2520argue%2520that%2520detecting%2520fine-tuning-based%2520steganographic%2520attacks%2520requires%2520approaches%2520beyond%2520traditional%2520steganalysis.%2520Standard%2520approaches%2520measure%2520distributional%2520shift%252C%2520which%2520is%2520an%2520expected%2520side-effect%2520of%2520fine-tuning.%2520Instead%252C%2520we%2520propose%2520a%2520mechanistic%2520interpretability%2520approach%253A%2520linear%2520probes%2520trained%2520on%2520later-layer%2520activations%2520detect%2520the%2520secret%2520with%2520up%2520to%252033%255C%2525%2520higher%2520accuracy%2520in%2520fine-tuned%2520models%2520compared%2520to%2520base%2520models%252C%2520even%2520for%2520low-recoverability%2520schemes.%2520This%2520suggests%2520that%2520malicious%2520fine-tuning%2520leaves%2520actionable%2520internal%2520signatures%2520amenable%2520to%2520interpretability-based%2520defenses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hide%20and%20Seek%20in%20Embedding%20Space%3A%20Geometry-based%20Steganography%20and%20Detection%20in%20Large%20Language%20Models&entry.906535625=Charles%20Westphal%20and%20Keivan%20Navaie%20and%20Fernando%20E.%20Rosas&entry.1292438233=Fine-tuned%20LLMs%20can%20covertly%20encode%20prompt%20secrets%20into%20outputs%20via%20steganographic%20channels.%20Prior%20work%20demonstrated%20this%20threat%20but%20relied%20on%20trivially%20recoverable%20encodings.%20We%20formalize%20payload%20recoverability%20via%20classifier%20accuracy%20and%20show%20previous%20schemes%20achieve%20100%5C%25%20recoverability.%20In%20response%2C%20we%20introduce%20low-recoverability%20steganography%2C%20replacing%20arbitrary%20mappings%20with%20embedding-space-derived%20ones.%20For%20Llama-8B%20%28LoRA%29%20and%20Ministral-8B%20%28LoRA%29%20trained%20on%20TrojanStego%20prompts%2C%20exact%20secret%20recovery%20rises%20from%2017%24%5Crightarrow%2430%5C%25%20%28%2B78%5C%25%29%20and%2024%24%5Crightarrow%2443%5C%25%20%28%2B80%5C%25%29%20respectively%2C%20while%20on%20Llama-70B%20%28LoRA%29%20trained%20on%20Wiki%20prompts%2C%20it%20climbs%20from%209%24%5Crightarrow%2419%5C%25%20%28%2B123%5C%25%29%2C%20all%20while%20reducing%20payload%20recoverability.%20We%20then%20discuss%20detection.%20We%20argue%20that%20detecting%20fine-tuning-based%20steganographic%20attacks%20requires%20approaches%20beyond%20traditional%20steganalysis.%20Standard%20approaches%20measure%20distributional%20shift%2C%20which%20is%20an%20expected%20side-effect%20of%20fine-tuning.%20Instead%2C%20we%20propose%20a%20mechanistic%20interpretability%20approach%3A%20linear%20probes%20trained%20on%20later-layer%20activations%20detect%20the%20secret%20with%20up%20to%2033%5C%25%20higher%20accuracy%20in%20fine-tuned%20models%20compared%20to%20base%20models%2C%20even%20for%20low-recoverability%20schemes.%20This%20suggests%20that%20malicious%20fine-tuning%20leaves%20actionable%20internal%20signatures%20amenable%20to%20interpretability-based%20defenses.&entry.1838667208=http%3A//arxiv.org/abs/2601.22818v1&entry.124074799=Read"},
{"title": "Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training", "author": "Anglin Liu and Ruichao Chen and Yi Lu and Hongxia Xu and Jintai Chen", "abstract": "Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometric fidelity. This paper introduces Med-Scout, a novel framework that \"cures\" this blindness via Reinforcement Learning (RL) that leverages the intrinsic geometric logic latent within unlabeled medical images. Instead of relying on costly expert annotations, Med-Scout derives verifiable supervision signals through three strategic proxy tasks: Hierarchical Scale Localization, Topological Jigsaw Reconstruction, and Anomaly Consistency Detection. To rigorously quantify this deficit, we present Med-Scout-Bench, a new benchmark specifically designed to evaluate geometric perception. Extensive evaluations show that Med-Scout significantly mitigates geometric blindness, outperforming leading proprietary and open-source MLLMs by over 40% on our benchmark. Furthermore, this enhanced geometric perception generalizes to broader medical understanding, achieving superior results on radiological and comprehensive medical VQA tasks.", "link": "http://arxiv.org/abs/2601.23220v1", "date": "2026-01-30", "relevancy": 2.2907, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Med-Scout%3A%20Curing%20MLLMs%27%20Geometric%20Blindness%20in%20Medical%20Perception%20via%20Geometry-Aware%20RL%20Post-Training&body=Title%3A%20Med-Scout%3A%20Curing%20MLLMs%27%20Geometric%20Blindness%20in%20Medical%20Perception%20via%20Geometry-Aware%20RL%20Post-Training%0AAuthor%3A%20Anglin%20Liu%20and%20Ruichao%20Chen%20and%20Yi%20Lu%20and%20Hongxia%20Xu%20and%20Jintai%20Chen%0AAbstract%3A%20Despite%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%27%20linguistic%20prowess%20in%20medical%20diagnosis%2C%20we%20find%20even%20state-of-the-art%20MLLMs%20suffer%20from%20a%20critical%20perceptual%20deficit%3A%20geometric%20blindness.%20This%20failure%20to%20ground%20outputs%20in%20objective%20geometric%20constraints%20leads%20to%20plausible%20yet%20factually%20incorrect%20hallucinations%2C%20rooted%20in%20training%20paradigms%20that%20prioritize%20linguistic%20fluency%20over%20geometric%20fidelity.%20This%20paper%20introduces%20Med-Scout%2C%20a%20novel%20framework%20that%20%22cures%22%20this%20blindness%20via%20Reinforcement%20Learning%20%28RL%29%20that%20leverages%20the%20intrinsic%20geometric%20logic%20latent%20within%20unlabeled%20medical%20images.%20Instead%20of%20relying%20on%20costly%20expert%20annotations%2C%20Med-Scout%20derives%20verifiable%20supervision%20signals%20through%20three%20strategic%20proxy%20tasks%3A%20Hierarchical%20Scale%20Localization%2C%20Topological%20Jigsaw%20Reconstruction%2C%20and%20Anomaly%20Consistency%20Detection.%20To%20rigorously%20quantify%20this%20deficit%2C%20we%20present%20Med-Scout-Bench%2C%20a%20new%20benchmark%20specifically%20designed%20to%20evaluate%20geometric%20perception.%20Extensive%20evaluations%20show%20that%20Med-Scout%20significantly%20mitigates%20geometric%20blindness%2C%20outperforming%20leading%20proprietary%20and%20open-source%20MLLMs%20by%20over%2040%25%20on%20our%20benchmark.%20Furthermore%2C%20this%20enhanced%20geometric%20perception%20generalizes%20to%20broader%20medical%20understanding%2C%20achieving%20superior%20results%20on%20radiological%20and%20comprehensive%20medical%20VQA%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMed-Scout%253A%2520Curing%2520MLLMs%2527%2520Geometric%2520Blindness%2520in%2520Medical%2520Perception%2520via%2520Geometry-Aware%2520RL%2520Post-Training%26entry.906535625%3DAnglin%2520Liu%2520and%2520Ruichao%2520Chen%2520and%2520Yi%2520Lu%2520and%2520Hongxia%2520Xu%2520and%2520Jintai%2520Chen%26entry.1292438233%3DDespite%2520recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2527%2520linguistic%2520prowess%2520in%2520medical%2520diagnosis%252C%2520we%2520find%2520even%2520state-of-the-art%2520MLLMs%2520suffer%2520from%2520a%2520critical%2520perceptual%2520deficit%253A%2520geometric%2520blindness.%2520This%2520failure%2520to%2520ground%2520outputs%2520in%2520objective%2520geometric%2520constraints%2520leads%2520to%2520plausible%2520yet%2520factually%2520incorrect%2520hallucinations%252C%2520rooted%2520in%2520training%2520paradigms%2520that%2520prioritize%2520linguistic%2520fluency%2520over%2520geometric%2520fidelity.%2520This%2520paper%2520introduces%2520Med-Scout%252C%2520a%2520novel%2520framework%2520that%2520%2522cures%2522%2520this%2520blindness%2520via%2520Reinforcement%2520Learning%2520%2528RL%2529%2520that%2520leverages%2520the%2520intrinsic%2520geometric%2520logic%2520latent%2520within%2520unlabeled%2520medical%2520images.%2520Instead%2520of%2520relying%2520on%2520costly%2520expert%2520annotations%252C%2520Med-Scout%2520derives%2520verifiable%2520supervision%2520signals%2520through%2520three%2520strategic%2520proxy%2520tasks%253A%2520Hierarchical%2520Scale%2520Localization%252C%2520Topological%2520Jigsaw%2520Reconstruction%252C%2520and%2520Anomaly%2520Consistency%2520Detection.%2520To%2520rigorously%2520quantify%2520this%2520deficit%252C%2520we%2520present%2520Med-Scout-Bench%252C%2520a%2520new%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520geometric%2520perception.%2520Extensive%2520evaluations%2520show%2520that%2520Med-Scout%2520significantly%2520mitigates%2520geometric%2520blindness%252C%2520outperforming%2520leading%2520proprietary%2520and%2520open-source%2520MLLMs%2520by%2520over%252040%2525%2520on%2520our%2520benchmark.%2520Furthermore%252C%2520this%2520enhanced%2520geometric%2520perception%2520generalizes%2520to%2520broader%2520medical%2520understanding%252C%2520achieving%2520superior%2520results%2520on%2520radiological%2520and%2520comprehensive%2520medical%2520VQA%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Med-Scout%3A%20Curing%20MLLMs%27%20Geometric%20Blindness%20in%20Medical%20Perception%20via%20Geometry-Aware%20RL%20Post-Training&entry.906535625=Anglin%20Liu%20and%20Ruichao%20Chen%20and%20Yi%20Lu%20and%20Hongxia%20Xu%20and%20Jintai%20Chen&entry.1292438233=Despite%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%27%20linguistic%20prowess%20in%20medical%20diagnosis%2C%20we%20find%20even%20state-of-the-art%20MLLMs%20suffer%20from%20a%20critical%20perceptual%20deficit%3A%20geometric%20blindness.%20This%20failure%20to%20ground%20outputs%20in%20objective%20geometric%20constraints%20leads%20to%20plausible%20yet%20factually%20incorrect%20hallucinations%2C%20rooted%20in%20training%20paradigms%20that%20prioritize%20linguistic%20fluency%20over%20geometric%20fidelity.%20This%20paper%20introduces%20Med-Scout%2C%20a%20novel%20framework%20that%20%22cures%22%20this%20blindness%20via%20Reinforcement%20Learning%20%28RL%29%20that%20leverages%20the%20intrinsic%20geometric%20logic%20latent%20within%20unlabeled%20medical%20images.%20Instead%20of%20relying%20on%20costly%20expert%20annotations%2C%20Med-Scout%20derives%20verifiable%20supervision%20signals%20through%20three%20strategic%20proxy%20tasks%3A%20Hierarchical%20Scale%20Localization%2C%20Topological%20Jigsaw%20Reconstruction%2C%20and%20Anomaly%20Consistency%20Detection.%20To%20rigorously%20quantify%20this%20deficit%2C%20we%20present%20Med-Scout-Bench%2C%20a%20new%20benchmark%20specifically%20designed%20to%20evaluate%20geometric%20perception.%20Extensive%20evaluations%20show%20that%20Med-Scout%20significantly%20mitigates%20geometric%20blindness%2C%20outperforming%20leading%20proprietary%20and%20open-source%20MLLMs%20by%20over%2040%25%20on%20our%20benchmark.%20Furthermore%2C%20this%20enhanced%20geometric%20perception%20generalizes%20to%20broader%20medical%20understanding%2C%20achieving%20superior%20results%20on%20radiological%20and%20comprehensive%20medical%20VQA%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.23220v1&entry.124074799=Read"},
{"title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings", "author": "Safal Shrestha and Minwu Kim and Aadim Nepal and Anubhav Shrestha and Keith Ross", "abstract": "Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we \"warm up\" the model by distilling Long CoTs from a toy domain, namely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: $(i)$ the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro; $(ii)$ When both the base model and the warmed-up model are RLVR trained on the same small dataset ($\\leq100$ examples), the warmed-up model consistently outperforms the base model; $(iii)$ Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; $(iv)$ Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.", "link": "http://arxiv.org/abs/2505.13718v3", "date": "2026-01-30", "relevancy": 2.2902, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4599}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Warm%20Up%20Before%20You%20Train%3A%20Unlocking%20General%20Reasoning%20in%20Resource-Constrained%20Settings&body=Title%3A%20Warm%20Up%20Before%20You%20Train%3A%20Unlocking%20General%20Reasoning%20in%20Resource-Constrained%20Settings%0AAuthor%3A%20Safal%20Shrestha%20and%20Minwu%20Kim%20and%20Aadim%20Nepal%20and%20Anubhav%20Shrestha%20and%20Keith%20Ross%0AAbstract%3A%20Designing%20effective%20reasoning-capable%20LLMs%20typically%20requires%20training%20using%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20or%20distillation%20with%20carefully%20curated%20Long%20Chain%20of%20Thoughts%20%28CoT%29%2C%20both%20of%20which%20depend%20heavily%20on%20extensive%20training%20data.%20This%20creates%20a%20major%20challenge%20when%20the%20amount%20of%20quality%20training%20data%20is%20scarce.%20We%20propose%20a%20sample-efficient%2C%20two-stage%20training%20strategy%20to%20develop%20reasoning%20LLMs%20under%20limited%20supervision.%20In%20the%20first%20stage%2C%20we%20%22warm%20up%22%20the%20model%20by%20distilling%20Long%20CoTs%20from%20a%20toy%20domain%2C%20namely%2C%20Knights%20%5C%26%20Knaves%20%28K%5C%26K%29%20logic%20puzzles%20to%20acquire%20general%20reasoning%20skills.%20In%20the%20second%20stage%2C%20we%20apply%20RLVR%20to%20the%20warmed-up%20model%20using%20a%20limited%20set%20of%20target-domain%20examples.%20Our%20experiments%20demonstrate%20that%20this%20two-phase%20approach%20offers%20several%20benefits%3A%20%24%28i%29%24%20the%20warmup%20phase%20alone%20facilitates%20generalized%20reasoning%2C%20leading%20to%20performance%20improvements%20across%20a%20range%20of%20tasks%2C%20including%20MATH%2C%20HumanEval%24%5E%7B%2B%7D%24%2C%20and%20MMLU-Pro%3B%20%24%28ii%29%24%20When%20both%20the%20base%20model%20and%20the%20warmed-up%20model%20are%20RLVR%20trained%20on%20the%20same%20small%20dataset%20%28%24%5Cleq100%24%20examples%29%2C%20the%20warmed-up%20model%20consistently%20outperforms%20the%20base%20model%3B%20%24%28iii%29%24%20Warming%20up%20before%20RLVR%20training%20allows%20a%20model%20to%20maintain%20cross-domain%20generalizability%20even%20after%20training%20on%20a%20specific%20domain%3B%20%24%28iv%29%24%20Introducing%20warmup%20in%20the%20pipeline%20improves%20not%20only%20accuracy%20but%20also%20overall%20sample%20efficiency%20during%20RLVR%20training.%20The%20results%20in%20this%20paper%20highlight%20the%20promise%20of%20warmup%20for%20building%20robust%20reasoning%20LLMs%20in%20data-scarce%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2505.13718v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWarm%2520Up%2520Before%2520You%2520Train%253A%2520Unlocking%2520General%2520Reasoning%2520in%2520Resource-Constrained%2520Settings%26entry.906535625%3DSafal%2520Shrestha%2520and%2520Minwu%2520Kim%2520and%2520Aadim%2520Nepal%2520and%2520Anubhav%2520Shrestha%2520and%2520Keith%2520Ross%26entry.1292438233%3DDesigning%2520effective%2520reasoning-capable%2520LLMs%2520typically%2520requires%2520training%2520using%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520or%2520distillation%2520with%2520carefully%2520curated%2520Long%2520Chain%2520of%2520Thoughts%2520%2528CoT%2529%252C%2520both%2520of%2520which%2520depend%2520heavily%2520on%2520extensive%2520training%2520data.%2520This%2520creates%2520a%2520major%2520challenge%2520when%2520the%2520amount%2520of%2520quality%2520training%2520data%2520is%2520scarce.%2520We%2520propose%2520a%2520sample-efficient%252C%2520two-stage%2520training%2520strategy%2520to%2520develop%2520reasoning%2520LLMs%2520under%2520limited%2520supervision.%2520In%2520the%2520first%2520stage%252C%2520we%2520%2522warm%2520up%2522%2520the%2520model%2520by%2520distilling%2520Long%2520CoTs%2520from%2520a%2520toy%2520domain%252C%2520namely%252C%2520Knights%2520%255C%2526%2520Knaves%2520%2528K%255C%2526K%2529%2520logic%2520puzzles%2520to%2520acquire%2520general%2520reasoning%2520skills.%2520In%2520the%2520second%2520stage%252C%2520we%2520apply%2520RLVR%2520to%2520the%2520warmed-up%2520model%2520using%2520a%2520limited%2520set%2520of%2520target-domain%2520examples.%2520Our%2520experiments%2520demonstrate%2520that%2520this%2520two-phase%2520approach%2520offers%2520several%2520benefits%253A%2520%2524%2528i%2529%2524%2520the%2520warmup%2520phase%2520alone%2520facilitates%2520generalized%2520reasoning%252C%2520leading%2520to%2520performance%2520improvements%2520across%2520a%2520range%2520of%2520tasks%252C%2520including%2520MATH%252C%2520HumanEval%2524%255E%257B%252B%257D%2524%252C%2520and%2520MMLU-Pro%253B%2520%2524%2528ii%2529%2524%2520When%2520both%2520the%2520base%2520model%2520and%2520the%2520warmed-up%2520model%2520are%2520RLVR%2520trained%2520on%2520the%2520same%2520small%2520dataset%2520%2528%2524%255Cleq100%2524%2520examples%2529%252C%2520the%2520warmed-up%2520model%2520consistently%2520outperforms%2520the%2520base%2520model%253B%2520%2524%2528iii%2529%2524%2520Warming%2520up%2520before%2520RLVR%2520training%2520allows%2520a%2520model%2520to%2520maintain%2520cross-domain%2520generalizability%2520even%2520after%2520training%2520on%2520a%2520specific%2520domain%253B%2520%2524%2528iv%2529%2524%2520Introducing%2520warmup%2520in%2520the%2520pipeline%2520improves%2520not%2520only%2520accuracy%2520but%2520also%2520overall%2520sample%2520efficiency%2520during%2520RLVR%2520training.%2520The%2520results%2520in%2520this%2520paper%2520highlight%2520the%2520promise%2520of%2520warmup%2520for%2520building%2520robust%2520reasoning%2520LLMs%2520in%2520data-scarce%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13718v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Warm%20Up%20Before%20You%20Train%3A%20Unlocking%20General%20Reasoning%20in%20Resource-Constrained%20Settings&entry.906535625=Safal%20Shrestha%20and%20Minwu%20Kim%20and%20Aadim%20Nepal%20and%20Anubhav%20Shrestha%20and%20Keith%20Ross&entry.1292438233=Designing%20effective%20reasoning-capable%20LLMs%20typically%20requires%20training%20using%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20or%20distillation%20with%20carefully%20curated%20Long%20Chain%20of%20Thoughts%20%28CoT%29%2C%20both%20of%20which%20depend%20heavily%20on%20extensive%20training%20data.%20This%20creates%20a%20major%20challenge%20when%20the%20amount%20of%20quality%20training%20data%20is%20scarce.%20We%20propose%20a%20sample-efficient%2C%20two-stage%20training%20strategy%20to%20develop%20reasoning%20LLMs%20under%20limited%20supervision.%20In%20the%20first%20stage%2C%20we%20%22warm%20up%22%20the%20model%20by%20distilling%20Long%20CoTs%20from%20a%20toy%20domain%2C%20namely%2C%20Knights%20%5C%26%20Knaves%20%28K%5C%26K%29%20logic%20puzzles%20to%20acquire%20general%20reasoning%20skills.%20In%20the%20second%20stage%2C%20we%20apply%20RLVR%20to%20the%20warmed-up%20model%20using%20a%20limited%20set%20of%20target-domain%20examples.%20Our%20experiments%20demonstrate%20that%20this%20two-phase%20approach%20offers%20several%20benefits%3A%20%24%28i%29%24%20the%20warmup%20phase%20alone%20facilitates%20generalized%20reasoning%2C%20leading%20to%20performance%20improvements%20across%20a%20range%20of%20tasks%2C%20including%20MATH%2C%20HumanEval%24%5E%7B%2B%7D%24%2C%20and%20MMLU-Pro%3B%20%24%28ii%29%24%20When%20both%20the%20base%20model%20and%20the%20warmed-up%20model%20are%20RLVR%20trained%20on%20the%20same%20small%20dataset%20%28%24%5Cleq100%24%20examples%29%2C%20the%20warmed-up%20model%20consistently%20outperforms%20the%20base%20model%3B%20%24%28iii%29%24%20Warming%20up%20before%20RLVR%20training%20allows%20a%20model%20to%20maintain%20cross-domain%20generalizability%20even%20after%20training%20on%20a%20specific%20domain%3B%20%24%28iv%29%24%20Introducing%20warmup%20in%20the%20pipeline%20improves%20not%20only%20accuracy%20but%20also%20overall%20sample%20efficiency%20during%20RLVR%20training.%20The%20results%20in%20this%20paper%20highlight%20the%20promise%20of%20warmup%20for%20building%20robust%20reasoning%20LLMs%20in%20data-scarce%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2505.13718v3&entry.124074799=Read"},
{"title": "The Narrow Gate: Localized Image-Text Communication in Native Multimodal Models", "author": "Alessandro Pietro Serra and Francesco Ortu and Emanuele Panizon and Lucrezia Valeriani and Lorenzo Basile and Alessio Ansuini and Diego Doimo and Alberto Cazzaniga", "abstract": "Recent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, focusing on how visual information is processed and transferred to the textual domain. We compare native multimodal VLMs, models trained from scratch on multimodal data to generate both text and images, and non-native multimodal VLMs, models adapted from pre-trained large language models or capable of generating only text, highlighting key differences in information flow. We find that in native multimodal VLMs, image and text embeddings are more separated within the residual stream. Moreover, VLMs differ in how visual information reaches text: non-native multimodal VLMs exhibit a distributed communication pattern, where information is exchanged through multiple image tokens, whereas models trained natively for joint image and text generation tend to rely on a single post-image token that acts as a narrow gate for visual information. We show that ablating this single token significantly deteriorates image-understanding performance, whereas targeted, token-level interventions reliably steer image semantics and downstream text with fine-grained control.", "link": "http://arxiv.org/abs/2412.06646v4", "date": "2026-01-30", "relevancy": 2.2832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5783}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Narrow%20Gate%3A%20Localized%20Image-Text%20Communication%20in%20Native%20Multimodal%20Models&body=Title%3A%20The%20Narrow%20Gate%3A%20Localized%20Image-Text%20Communication%20in%20Native%20Multimodal%20Models%0AAuthor%3A%20Alessandro%20Pietro%20Serra%20and%20Francesco%20Ortu%20and%20Emanuele%20Panizon%20and%20Lucrezia%20Valeriani%20and%20Lorenzo%20Basile%20and%20Alessio%20Ansuini%20and%20Diego%20Doimo%20and%20Alberto%20Cazzaniga%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20training%20have%20significantly%20improved%20the%20integration%20of%20image%20understanding%20and%20generation%20within%20a%20unified%20model.%20This%20study%20investigates%20how%20vision-language%20models%20%28VLMs%29%20handle%20image-understanding%20tasks%2C%20focusing%20on%20how%20visual%20information%20is%20processed%20and%20transferred%20to%20the%20textual%20domain.%20We%20compare%20native%20multimodal%20VLMs%2C%20models%20trained%20from%20scratch%20on%20multimodal%20data%20to%20generate%20both%20text%20and%20images%2C%20and%20non-native%20multimodal%20VLMs%2C%20models%20adapted%20from%20pre-trained%20large%20language%20models%20or%20capable%20of%20generating%20only%20text%2C%20highlighting%20key%20differences%20in%20information%20flow.%20We%20find%20that%20in%20native%20multimodal%20VLMs%2C%20image%20and%20text%20embeddings%20are%20more%20separated%20within%20the%20residual%20stream.%20Moreover%2C%20VLMs%20differ%20in%20how%20visual%20information%20reaches%20text%3A%20non-native%20multimodal%20VLMs%20exhibit%20a%20distributed%20communication%20pattern%2C%20where%20information%20is%20exchanged%20through%20multiple%20image%20tokens%2C%20whereas%20models%20trained%20natively%20for%20joint%20image%20and%20text%20generation%20tend%20to%20rely%20on%20a%20single%20post-image%20token%20that%20acts%20as%20a%20narrow%20gate%20for%20visual%20information.%20We%20show%20that%20ablating%20this%20single%20token%20significantly%20deteriorates%20image-understanding%20performance%2C%20whereas%20targeted%2C%20token-level%20interventions%20reliably%20steer%20image%20semantics%20and%20downstream%20text%20with%20fine-grained%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2412.06646v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Narrow%2520Gate%253A%2520Localized%2520Image-Text%2520Communication%2520in%2520Native%2520Multimodal%2520Models%26entry.906535625%3DAlessandro%2520Pietro%2520Serra%2520and%2520Francesco%2520Ortu%2520and%2520Emanuele%2520Panizon%2520and%2520Lucrezia%2520Valeriani%2520and%2520Lorenzo%2520Basile%2520and%2520Alessio%2520Ansuini%2520and%2520Diego%2520Doimo%2520and%2520Alberto%2520Cazzaniga%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520training%2520have%2520significantly%2520improved%2520the%2520integration%2520of%2520image%2520understanding%2520and%2520generation%2520within%2520a%2520unified%2520model.%2520This%2520study%2520investigates%2520how%2520vision-language%2520models%2520%2528VLMs%2529%2520handle%2520image-understanding%2520tasks%252C%2520focusing%2520on%2520how%2520visual%2520information%2520is%2520processed%2520and%2520transferred%2520to%2520the%2520textual%2520domain.%2520We%2520compare%2520native%2520multimodal%2520VLMs%252C%2520models%2520trained%2520from%2520scratch%2520on%2520multimodal%2520data%2520to%2520generate%2520both%2520text%2520and%2520images%252C%2520and%2520non-native%2520multimodal%2520VLMs%252C%2520models%2520adapted%2520from%2520pre-trained%2520large%2520language%2520models%2520or%2520capable%2520of%2520generating%2520only%2520text%252C%2520highlighting%2520key%2520differences%2520in%2520information%2520flow.%2520We%2520find%2520that%2520in%2520native%2520multimodal%2520VLMs%252C%2520image%2520and%2520text%2520embeddings%2520are%2520more%2520separated%2520within%2520the%2520residual%2520stream.%2520Moreover%252C%2520VLMs%2520differ%2520in%2520how%2520visual%2520information%2520reaches%2520text%253A%2520non-native%2520multimodal%2520VLMs%2520exhibit%2520a%2520distributed%2520communication%2520pattern%252C%2520where%2520information%2520is%2520exchanged%2520through%2520multiple%2520image%2520tokens%252C%2520whereas%2520models%2520trained%2520natively%2520for%2520joint%2520image%2520and%2520text%2520generation%2520tend%2520to%2520rely%2520on%2520a%2520single%2520post-image%2520token%2520that%2520acts%2520as%2520a%2520narrow%2520gate%2520for%2520visual%2520information.%2520We%2520show%2520that%2520ablating%2520this%2520single%2520token%2520significantly%2520deteriorates%2520image-understanding%2520performance%252C%2520whereas%2520targeted%252C%2520token-level%2520interventions%2520reliably%2520steer%2520image%2520semantics%2520and%2520downstream%2520text%2520with%2520fine-grained%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06646v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Narrow%20Gate%3A%20Localized%20Image-Text%20Communication%20in%20Native%20Multimodal%20Models&entry.906535625=Alessandro%20Pietro%20Serra%20and%20Francesco%20Ortu%20and%20Emanuele%20Panizon%20and%20Lucrezia%20Valeriani%20and%20Lorenzo%20Basile%20and%20Alessio%20Ansuini%20and%20Diego%20Doimo%20and%20Alberto%20Cazzaniga&entry.1292438233=Recent%20advances%20in%20multimodal%20training%20have%20significantly%20improved%20the%20integration%20of%20image%20understanding%20and%20generation%20within%20a%20unified%20model.%20This%20study%20investigates%20how%20vision-language%20models%20%28VLMs%29%20handle%20image-understanding%20tasks%2C%20focusing%20on%20how%20visual%20information%20is%20processed%20and%20transferred%20to%20the%20textual%20domain.%20We%20compare%20native%20multimodal%20VLMs%2C%20models%20trained%20from%20scratch%20on%20multimodal%20data%20to%20generate%20both%20text%20and%20images%2C%20and%20non-native%20multimodal%20VLMs%2C%20models%20adapted%20from%20pre-trained%20large%20language%20models%20or%20capable%20of%20generating%20only%20text%2C%20highlighting%20key%20differences%20in%20information%20flow.%20We%20find%20that%20in%20native%20multimodal%20VLMs%2C%20image%20and%20text%20embeddings%20are%20more%20separated%20within%20the%20residual%20stream.%20Moreover%2C%20VLMs%20differ%20in%20how%20visual%20information%20reaches%20text%3A%20non-native%20multimodal%20VLMs%20exhibit%20a%20distributed%20communication%20pattern%2C%20where%20information%20is%20exchanged%20through%20multiple%20image%20tokens%2C%20whereas%20models%20trained%20natively%20for%20joint%20image%20and%20text%20generation%20tend%20to%20rely%20on%20a%20single%20post-image%20token%20that%20acts%20as%20a%20narrow%20gate%20for%20visual%20information.%20We%20show%20that%20ablating%20this%20single%20token%20significantly%20deteriorates%20image-understanding%20performance%2C%20whereas%20targeted%2C%20token-level%20interventions%20reliably%20steer%20image%20semantics%20and%20downstream%20text%20with%20fine-grained%20control.&entry.1838667208=http%3A//arxiv.org/abs/2412.06646v4&entry.124074799=Read"},
{"title": "FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation", "author": "Muqing Liu and Chongjie Si and Yuheng Jia", "abstract": "Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.", "link": "http://arxiv.org/abs/2601.22905v1", "date": "2026-01-30", "relevancy": 2.2825, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4708}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4503}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexLoRA%3A%20Entropy-Guided%20Flexible%20Low-Rank%20Adaptation&body=Title%3A%20FlexLoRA%3A%20Entropy-Guided%20Flexible%20Low-Rank%20Adaptation%0AAuthor%3A%20Muqing%20Liu%20and%20Chongjie%20Si%20and%20Yuheng%20Jia%0AAbstract%3A%20Large%20pre-trained%20models%20achieve%20remarkable%20success%20across%20diverse%20domains%2C%20yet%20fully%20fine-tuning%20incurs%20prohibitive%20computational%20and%20memory%20costs.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20has%20thus%20become%20a%20mainstream%20paradigm.%20Among%20them%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20introduces%20trainable%20low-rank%20matrices%20and%20shows%20strong%20performance%2C%20nevertheless%2C%20its%20fixed-rank%20design%20limits%20flexibility.%20Dynamic%20rank%20allocation%20methods%20mitigate%20this%20issue%20by%20pruning%20redundant%20directions%3B%20however%2C%20they%20often%20rely%20on%20heuristic%2C%20element-level%20metrics%20that%20globally%20sort%20rank%20directions%20without%20matrix-wise%20distinction%2C%20and%20they%20lack%20mechanisms%20to%20expand%20capacity%20in%20layers%20requiring%20additional%20adaptation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20FlexLoRA%2C%20an%20entropy-guided%20flexible%20low-rank%20adaptation%20framework%20that%20%28i%29%20evaluates%20matrix%20importance%20via%20spectral%20energy%20entropy%2C%20%28ii%29%20supports%20rank%20pruning%20and%20expansion%20under%20a%20global%20budget%2C%20and%20%28iii%29%20employs%20zero-impact%20initialization%20for%20newly%20added%20singular%20directions%20to%20ensure%20stability.%20By%20addressing%20granularity%2C%20flexibility%2C%20and%20stability%20limitations%2C%20FlexLoRA%20provides%20a%20more%20principled%20solution%20for%20PEFT.%20Extensive%20experiments%20show%20that%20FlexLoRA%20consistently%20outperforms%20state-of-the-art%20baselines%20across%20benchmarks.%20Codes%20are%20available%20at%20https%3A//github.com/Chongjie-Si/Subspace-Tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexLoRA%253A%2520Entropy-Guided%2520Flexible%2520Low-Rank%2520Adaptation%26entry.906535625%3DMuqing%2520Liu%2520and%2520Chongjie%2520Si%2520and%2520Yuheng%2520Jia%26entry.1292438233%3DLarge%2520pre-trained%2520models%2520achieve%2520remarkable%2520success%2520across%2520diverse%2520domains%252C%2520yet%2520fully%2520fine-tuning%2520incurs%2520prohibitive%2520computational%2520and%2520memory%2520costs.%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520has%2520thus%2520become%2520a%2520mainstream%2520paradigm.%2520Among%2520them%252C%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520introduces%2520trainable%2520low-rank%2520matrices%2520and%2520shows%2520strong%2520performance%252C%2520nevertheless%252C%2520its%2520fixed-rank%2520design%2520limits%2520flexibility.%2520Dynamic%2520rank%2520allocation%2520methods%2520mitigate%2520this%2520issue%2520by%2520pruning%2520redundant%2520directions%253B%2520however%252C%2520they%2520often%2520rely%2520on%2520heuristic%252C%2520element-level%2520metrics%2520that%2520globally%2520sort%2520rank%2520directions%2520without%2520matrix-wise%2520distinction%252C%2520and%2520they%2520lack%2520mechanisms%2520to%2520expand%2520capacity%2520in%2520layers%2520requiring%2520additional%2520adaptation.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520FlexLoRA%252C%2520an%2520entropy-guided%2520flexible%2520low-rank%2520adaptation%2520framework%2520that%2520%2528i%2529%2520evaluates%2520matrix%2520importance%2520via%2520spectral%2520energy%2520entropy%252C%2520%2528ii%2529%2520supports%2520rank%2520pruning%2520and%2520expansion%2520under%2520a%2520global%2520budget%252C%2520and%2520%2528iii%2529%2520employs%2520zero-impact%2520initialization%2520for%2520newly%2520added%2520singular%2520directions%2520to%2520ensure%2520stability.%2520By%2520addressing%2520granularity%252C%2520flexibility%252C%2520and%2520stability%2520limitations%252C%2520FlexLoRA%2520provides%2520a%2520more%2520principled%2520solution%2520for%2520PEFT.%2520Extensive%2520experiments%2520show%2520that%2520FlexLoRA%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%2520across%2520benchmarks.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/Chongjie-Si/Subspace-Tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexLoRA%3A%20Entropy-Guided%20Flexible%20Low-Rank%20Adaptation&entry.906535625=Muqing%20Liu%20and%20Chongjie%20Si%20and%20Yuheng%20Jia&entry.1292438233=Large%20pre-trained%20models%20achieve%20remarkable%20success%20across%20diverse%20domains%2C%20yet%20fully%20fine-tuning%20incurs%20prohibitive%20computational%20and%20memory%20costs.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20has%20thus%20become%20a%20mainstream%20paradigm.%20Among%20them%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20introduces%20trainable%20low-rank%20matrices%20and%20shows%20strong%20performance%2C%20nevertheless%2C%20its%20fixed-rank%20design%20limits%20flexibility.%20Dynamic%20rank%20allocation%20methods%20mitigate%20this%20issue%20by%20pruning%20redundant%20directions%3B%20however%2C%20they%20often%20rely%20on%20heuristic%2C%20element-level%20metrics%20that%20globally%20sort%20rank%20directions%20without%20matrix-wise%20distinction%2C%20and%20they%20lack%20mechanisms%20to%20expand%20capacity%20in%20layers%20requiring%20additional%20adaptation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20FlexLoRA%2C%20an%20entropy-guided%20flexible%20low-rank%20adaptation%20framework%20that%20%28i%29%20evaluates%20matrix%20importance%20via%20spectral%20energy%20entropy%2C%20%28ii%29%20supports%20rank%20pruning%20and%20expansion%20under%20a%20global%20budget%2C%20and%20%28iii%29%20employs%20zero-impact%20initialization%20for%20newly%20added%20singular%20directions%20to%20ensure%20stability.%20By%20addressing%20granularity%2C%20flexibility%2C%20and%20stability%20limitations%2C%20FlexLoRA%20provides%20a%20more%20principled%20solution%20for%20PEFT.%20Extensive%20experiments%20show%20that%20FlexLoRA%20consistently%20outperforms%20state-of-the-art%20baselines%20across%20benchmarks.%20Codes%20are%20available%20at%20https%3A//github.com/Chongjie-Si/Subspace-Tuning.&entry.1838667208=http%3A//arxiv.org/abs/2601.22905v1&entry.124074799=Read"},
{"title": "MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics", "author": "Mikel M. Iparraguirre and Iciar Alfaro and David Gonzalez and Elias Cueto", "abstract": "We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.\n  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.", "link": "http://arxiv.org/abs/2601.23177v1", "date": "2026-01-30", "relevancy": 2.2757, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6013}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshGraphNet-Transformer%3A%20Scalable%20Mesh-based%20Learned%20Simulation%20for%20Solid%20Mechanics&body=Title%3A%20MeshGraphNet-Transformer%3A%20Scalable%20Mesh-based%20Learned%20Simulation%20for%20Solid%20Mechanics%0AAuthor%3A%20Mikel%20M.%20Iparraguirre%20and%20Iciar%20Alfaro%20and%20David%20Gonzalez%20and%20Elias%20Cueto%0AAbstract%3A%20We%20present%20MeshGraphNet-Transformer%20%28MGN-T%29%2C%20a%20novel%20architecture%20that%20combines%20the%20global%20modeling%20capabilities%20of%20Transformers%20with%20the%20geometric%20inductive%20bias%20of%20MeshGraphNets%2C%20while%20preserving%20a%20mesh-based%20graph%20representation.%20MGN-T%20overcomes%20a%20key%20limitation%20of%20standard%20MGN%2C%20the%20inefficient%20long-range%20information%20propagation%20caused%20by%20iterative%20message%20passing%20on%20large%2C%20high-resolution%20meshes.%20A%20physics-attention%20Transformer%20serves%20as%20a%20global%20processor%2C%20updating%20all%20nodal%20states%20simultaneously%20while%20explicitly%20retaining%20node%20and%20edge%20attributes.%20By%20directly%20capturing%20long-range%20physical%20interactions%2C%20MGN-T%20eliminates%20the%20need%20for%20deep%20message-passing%20stacks%20or%20hierarchical%2C%20coarsened%20meshes%2C%20enabling%20efficient%20learning%20on%20high-resolution%20meshes%20with%20varying%20geometries%2C%20topologies%2C%20and%20boundary%20conditions%20at%20an%20industrial%20scale.%0A%20%20We%20demonstrate%20that%20MGN-T%20successfully%20handles%20industrial-scale%20meshes%20for%20impact%20dynamics%2C%20a%20setting%20in%20which%20standard%20MGN%20fails%20due%20message-passing%20under-reaching.%20The%20method%20accurately%20models%20self-contact%2C%20plasticity%2C%20and%20multivariate%20outputs%2C%20including%20internal%2C%20phenomenological%20plastic%20variables.%20Moreover%2C%20MGN-T%20outperforms%20state-of-the-art%20approaches%20on%20classical%20benchmarks%2C%20achieving%20higher%20accuracy%20while%20maintaining%20practical%20efficiency%2C%20using%20only%20a%20fraction%20of%20the%20parameters%20required%20by%20competing%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshGraphNet-Transformer%253A%2520Scalable%2520Mesh-based%2520Learned%2520Simulation%2520for%2520Solid%2520Mechanics%26entry.906535625%3DMikel%2520M.%2520Iparraguirre%2520and%2520Iciar%2520Alfaro%2520and%2520David%2520Gonzalez%2520and%2520Elias%2520Cueto%26entry.1292438233%3DWe%2520present%2520MeshGraphNet-Transformer%2520%2528MGN-T%2529%252C%2520a%2520novel%2520architecture%2520that%2520combines%2520the%2520global%2520modeling%2520capabilities%2520of%2520Transformers%2520with%2520the%2520geometric%2520inductive%2520bias%2520of%2520MeshGraphNets%252C%2520while%2520preserving%2520a%2520mesh-based%2520graph%2520representation.%2520MGN-T%2520overcomes%2520a%2520key%2520limitation%2520of%2520standard%2520MGN%252C%2520the%2520inefficient%2520long-range%2520information%2520propagation%2520caused%2520by%2520iterative%2520message%2520passing%2520on%2520large%252C%2520high-resolution%2520meshes.%2520A%2520physics-attention%2520Transformer%2520serves%2520as%2520a%2520global%2520processor%252C%2520updating%2520all%2520nodal%2520states%2520simultaneously%2520while%2520explicitly%2520retaining%2520node%2520and%2520edge%2520attributes.%2520By%2520directly%2520capturing%2520long-range%2520physical%2520interactions%252C%2520MGN-T%2520eliminates%2520the%2520need%2520for%2520deep%2520message-passing%2520stacks%2520or%2520hierarchical%252C%2520coarsened%2520meshes%252C%2520enabling%2520efficient%2520learning%2520on%2520high-resolution%2520meshes%2520with%2520varying%2520geometries%252C%2520topologies%252C%2520and%2520boundary%2520conditions%2520at%2520an%2520industrial%2520scale.%250A%2520%2520We%2520demonstrate%2520that%2520MGN-T%2520successfully%2520handles%2520industrial-scale%2520meshes%2520for%2520impact%2520dynamics%252C%2520a%2520setting%2520in%2520which%2520standard%2520MGN%2520fails%2520due%2520message-passing%2520under-reaching.%2520The%2520method%2520accurately%2520models%2520self-contact%252C%2520plasticity%252C%2520and%2520multivariate%2520outputs%252C%2520including%2520internal%252C%2520phenomenological%2520plastic%2520variables.%2520Moreover%252C%2520MGN-T%2520outperforms%2520state-of-the-art%2520approaches%2520on%2520classical%2520benchmarks%252C%2520achieving%2520higher%2520accuracy%2520while%2520maintaining%2520practical%2520efficiency%252C%2520using%2520only%2520a%2520fraction%2520of%2520the%2520parameters%2520required%2520by%2520competing%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshGraphNet-Transformer%3A%20Scalable%20Mesh-based%20Learned%20Simulation%20for%20Solid%20Mechanics&entry.906535625=Mikel%20M.%20Iparraguirre%20and%20Iciar%20Alfaro%20and%20David%20Gonzalez%20and%20Elias%20Cueto&entry.1292438233=We%20present%20MeshGraphNet-Transformer%20%28MGN-T%29%2C%20a%20novel%20architecture%20that%20combines%20the%20global%20modeling%20capabilities%20of%20Transformers%20with%20the%20geometric%20inductive%20bias%20of%20MeshGraphNets%2C%20while%20preserving%20a%20mesh-based%20graph%20representation.%20MGN-T%20overcomes%20a%20key%20limitation%20of%20standard%20MGN%2C%20the%20inefficient%20long-range%20information%20propagation%20caused%20by%20iterative%20message%20passing%20on%20large%2C%20high-resolution%20meshes.%20A%20physics-attention%20Transformer%20serves%20as%20a%20global%20processor%2C%20updating%20all%20nodal%20states%20simultaneously%20while%20explicitly%20retaining%20node%20and%20edge%20attributes.%20By%20directly%20capturing%20long-range%20physical%20interactions%2C%20MGN-T%20eliminates%20the%20need%20for%20deep%20message-passing%20stacks%20or%20hierarchical%2C%20coarsened%20meshes%2C%20enabling%20efficient%20learning%20on%20high-resolution%20meshes%20with%20varying%20geometries%2C%20topologies%2C%20and%20boundary%20conditions%20at%20an%20industrial%20scale.%0A%20%20We%20demonstrate%20that%20MGN-T%20successfully%20handles%20industrial-scale%20meshes%20for%20impact%20dynamics%2C%20a%20setting%20in%20which%20standard%20MGN%20fails%20due%20message-passing%20under-reaching.%20The%20method%20accurately%20models%20self-contact%2C%20plasticity%2C%20and%20multivariate%20outputs%2C%20including%20internal%2C%20phenomenological%20plastic%20variables.%20Moreover%2C%20MGN-T%20outperforms%20state-of-the-art%20approaches%20on%20classical%20benchmarks%2C%20achieving%20higher%20accuracy%20while%20maintaining%20practical%20efficiency%2C%20using%20only%20a%20fraction%20of%20the%20parameters%20required%20by%20competing%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.23177v1&entry.124074799=Read"},
{"title": "User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments", "author": "Junfeng Lin and Yanming Xiu and Maria Gorlatova", "abstract": "Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.", "link": "http://arxiv.org/abs/2601.23281v1", "date": "2026-01-30", "relevancy": 2.2593, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20User%20Prompting%20Strategies%20and%20Prompt%20Enhancement%20Methods%20for%20Open-Set%20Object%20Detection%20in%20XR%20Environments&body=Title%3A%20User%20Prompting%20Strategies%20and%20Prompt%20Enhancement%20Methods%20for%20Open-Set%20Object%20Detection%20in%20XR%20Environments%0AAuthor%3A%20Junfeng%20Lin%20and%20Yanming%20Xiu%20and%20Maria%20Gorlatova%0AAbstract%3A%20Open-set%20object%20detection%20%28OSOD%29%20localizes%20objects%20while%20identifying%20and%20rejecting%20unknown%20classes%20at%20inference.%20While%20recent%20OSOD%20models%20perform%20well%20on%20benchmarks%2C%20their%20behavior%20under%20realistic%20user%20prompting%20remains%20underexplored.%20In%20interactive%20XR%20settings%2C%20user-generated%20prompts%20are%20often%20ambiguous%2C%20underspecified%2C%20or%20overly%20detailed.%20To%20study%20prompt-conditioned%20robustness%2C%20we%20evaluate%20two%20OSOD%20models%2C%20GroundingDINO%20and%20YOLO-E%2C%20on%20real-world%20XR%20images%20and%20simulate%20diverse%20user%20prompting%20behaviors%20using%20vision-language%20models.%20We%20consider%20four%20prompt%20types%3A%20standard%2C%20underdetailed%2C%20overdetailed%2C%20and%20pragmatically%20ambiguous%2C%20and%20examine%20the%20impact%20of%20two%20enhancement%20strategies%20on%20these%20prompts.%20Results%20show%20that%20both%20models%20exhibit%20stable%20performance%20under%20underdetailed%20and%20standard%20prompts%2C%20while%20they%20suffer%20degradation%20under%20ambiguous%20prompts.%20Overdetailed%20prompts%20primarily%20affect%20GroundingDINO.%20Prompt%20enhancement%20substantially%20improves%20robustness%20under%20ambiguity%2C%20yielding%20gains%20exceeding%2055%25%20mIoU%20and%2041%25%20average%20confidence.%20Based%20on%20the%20findings%2C%20we%20propose%20several%20prompting%20strategies%20and%20prompt%20enhancement%20methods%20for%20OSOD%20models%20in%20XR%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUser%2520Prompting%2520Strategies%2520and%2520Prompt%2520Enhancement%2520Methods%2520for%2520Open-Set%2520Object%2520Detection%2520in%2520XR%2520Environments%26entry.906535625%3DJunfeng%2520Lin%2520and%2520Yanming%2520Xiu%2520and%2520Maria%2520Gorlatova%26entry.1292438233%3DOpen-set%2520object%2520detection%2520%2528OSOD%2529%2520localizes%2520objects%2520while%2520identifying%2520and%2520rejecting%2520unknown%2520classes%2520at%2520inference.%2520While%2520recent%2520OSOD%2520models%2520perform%2520well%2520on%2520benchmarks%252C%2520their%2520behavior%2520under%2520realistic%2520user%2520prompting%2520remains%2520underexplored.%2520In%2520interactive%2520XR%2520settings%252C%2520user-generated%2520prompts%2520are%2520often%2520ambiguous%252C%2520underspecified%252C%2520or%2520overly%2520detailed.%2520To%2520study%2520prompt-conditioned%2520robustness%252C%2520we%2520evaluate%2520two%2520OSOD%2520models%252C%2520GroundingDINO%2520and%2520YOLO-E%252C%2520on%2520real-world%2520XR%2520images%2520and%2520simulate%2520diverse%2520user%2520prompting%2520behaviors%2520using%2520vision-language%2520models.%2520We%2520consider%2520four%2520prompt%2520types%253A%2520standard%252C%2520underdetailed%252C%2520overdetailed%252C%2520and%2520pragmatically%2520ambiguous%252C%2520and%2520examine%2520the%2520impact%2520of%2520two%2520enhancement%2520strategies%2520on%2520these%2520prompts.%2520Results%2520show%2520that%2520both%2520models%2520exhibit%2520stable%2520performance%2520under%2520underdetailed%2520and%2520standard%2520prompts%252C%2520while%2520they%2520suffer%2520degradation%2520under%2520ambiguous%2520prompts.%2520Overdetailed%2520prompts%2520primarily%2520affect%2520GroundingDINO.%2520Prompt%2520enhancement%2520substantially%2520improves%2520robustness%2520under%2520ambiguity%252C%2520yielding%2520gains%2520exceeding%252055%2525%2520mIoU%2520and%252041%2525%2520average%2520confidence.%2520Based%2520on%2520the%2520findings%252C%2520we%2520propose%2520several%2520prompting%2520strategies%2520and%2520prompt%2520enhancement%2520methods%2520for%2520OSOD%2520models%2520in%2520XR%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=User%20Prompting%20Strategies%20and%20Prompt%20Enhancement%20Methods%20for%20Open-Set%20Object%20Detection%20in%20XR%20Environments&entry.906535625=Junfeng%20Lin%20and%20Yanming%20Xiu%20and%20Maria%20Gorlatova&entry.1292438233=Open-set%20object%20detection%20%28OSOD%29%20localizes%20objects%20while%20identifying%20and%20rejecting%20unknown%20classes%20at%20inference.%20While%20recent%20OSOD%20models%20perform%20well%20on%20benchmarks%2C%20their%20behavior%20under%20realistic%20user%20prompting%20remains%20underexplored.%20In%20interactive%20XR%20settings%2C%20user-generated%20prompts%20are%20often%20ambiguous%2C%20underspecified%2C%20or%20overly%20detailed.%20To%20study%20prompt-conditioned%20robustness%2C%20we%20evaluate%20two%20OSOD%20models%2C%20GroundingDINO%20and%20YOLO-E%2C%20on%20real-world%20XR%20images%20and%20simulate%20diverse%20user%20prompting%20behaviors%20using%20vision-language%20models.%20We%20consider%20four%20prompt%20types%3A%20standard%2C%20underdetailed%2C%20overdetailed%2C%20and%20pragmatically%20ambiguous%2C%20and%20examine%20the%20impact%20of%20two%20enhancement%20strategies%20on%20these%20prompts.%20Results%20show%20that%20both%20models%20exhibit%20stable%20performance%20under%20underdetailed%20and%20standard%20prompts%2C%20while%20they%20suffer%20degradation%20under%20ambiguous%20prompts.%20Overdetailed%20prompts%20primarily%20affect%20GroundingDINO.%20Prompt%20enhancement%20substantially%20improves%20robustness%20under%20ambiguity%2C%20yielding%20gains%20exceeding%2055%25%20mIoU%20and%2041%25%20average%20confidence.%20Based%20on%20the%20findings%2C%20we%20propose%20several%20prompting%20strategies%20and%20prompt%20enhancement%20methods%20for%20OSOD%20models%20in%20XR%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.23281v1&entry.124074799=Read"},
{"title": "Agnostic Language Identification and Generation", "author": "Mikael M\u00f8ller H\u00f8gsgaard and Chirag Pabbaraju", "abstract": "Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general \"agnostic\" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.", "link": "http://arxiv.org/abs/2601.23258v1", "date": "2026-01-30", "relevancy": 2.2545, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4553}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4531}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agnostic%20Language%20Identification%20and%20Generation&body=Title%3A%20Agnostic%20Language%20Identification%20and%20Generation%0AAuthor%3A%20Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Chirag%20Pabbaraju%0AAbstract%3A%20Recent%20works%20on%20language%20identification%20and%20generation%20have%20established%20tight%20statistical%20rates%20at%20which%20these%20tasks%20can%20be%20achieved.%20These%20works%20typically%20operate%20under%20a%20strong%20realizability%20assumption%3A%20that%20the%20input%20data%20is%20drawn%20from%20an%20unknown%20distribution%20necessarily%20supported%20on%20some%20language%20in%20a%20given%20collection.%20In%20this%20work%2C%20we%20relax%20this%20assumption%20of%20realizability%20entirely%2C%20and%20impose%20no%20restrictions%20on%20the%20distribution%20of%20the%20input%20data.%20We%20propose%20objectives%20to%20study%20both%20language%20identification%20and%20generation%20in%20this%20more%20general%20%22agnostic%22%20setup.%20Across%20both%20problems%2C%20we%20obtain%20novel%20interesting%20characterizations%20and%20nearly%20tight%20rates.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgnostic%2520Language%2520Identification%2520and%2520Generation%26entry.906535625%3DMikael%2520M%25C3%25B8ller%2520H%25C3%25B8gsgaard%2520and%2520Chirag%2520Pabbaraju%26entry.1292438233%3DRecent%2520works%2520on%2520language%2520identification%2520and%2520generation%2520have%2520established%2520tight%2520statistical%2520rates%2520at%2520which%2520these%2520tasks%2520can%2520be%2520achieved.%2520These%2520works%2520typically%2520operate%2520under%2520a%2520strong%2520realizability%2520assumption%253A%2520that%2520the%2520input%2520data%2520is%2520drawn%2520from%2520an%2520unknown%2520distribution%2520necessarily%2520supported%2520on%2520some%2520language%2520in%2520a%2520given%2520collection.%2520In%2520this%2520work%252C%2520we%2520relax%2520this%2520assumption%2520of%2520realizability%2520entirely%252C%2520and%2520impose%2520no%2520restrictions%2520on%2520the%2520distribution%2520of%2520the%2520input%2520data.%2520We%2520propose%2520objectives%2520to%2520study%2520both%2520language%2520identification%2520and%2520generation%2520in%2520this%2520more%2520general%2520%2522agnostic%2522%2520setup.%2520Across%2520both%2520problems%252C%2520we%2520obtain%2520novel%2520interesting%2520characterizations%2520and%2520nearly%2520tight%2520rates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agnostic%20Language%20Identification%20and%20Generation&entry.906535625=Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Chirag%20Pabbaraju&entry.1292438233=Recent%20works%20on%20language%20identification%20and%20generation%20have%20established%20tight%20statistical%20rates%20at%20which%20these%20tasks%20can%20be%20achieved.%20These%20works%20typically%20operate%20under%20a%20strong%20realizability%20assumption%3A%20that%20the%20input%20data%20is%20drawn%20from%20an%20unknown%20distribution%20necessarily%20supported%20on%20some%20language%20in%20a%20given%20collection.%20In%20this%20work%2C%20we%20relax%20this%20assumption%20of%20realizability%20entirely%2C%20and%20impose%20no%20restrictions%20on%20the%20distribution%20of%20the%20input%20data.%20We%20propose%20objectives%20to%20study%20both%20language%20identification%20and%20generation%20in%20this%20more%20general%20%22agnostic%22%20setup.%20Across%20both%20problems%2C%20we%20obtain%20novel%20interesting%20characterizations%20and%20nearly%20tight%20rates.&entry.1838667208=http%3A//arxiv.org/abs/2601.23258v1&entry.124074799=Read"},
{"title": "TSAQA: Time Series Analysis Question And Answering Benchmark", "author": "Baoyu Jing and Sanhorn Chen and Lecheng Zheng and Boyu Liu and Zihao Li and Jiaru Zou and Tianxin Wei and Zhining Liu and Zhichen Zeng and Ruizhong Qiu and Xiao Lin and Yuchen Yan and Dongqi Fu and Jingchao Ni and Jingrui He and Hanghang Tong", "abstract": "Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.", "link": "http://arxiv.org/abs/2601.23204v1", "date": "2026-01-30", "relevancy": 2.2516, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4575}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSAQA%3A%20Time%20Series%20Analysis%20Question%20And%20Answering%20Benchmark&body=Title%3A%20TSAQA%3A%20Time%20Series%20Analysis%20Question%20And%20Answering%20Benchmark%0AAuthor%3A%20Baoyu%20Jing%20and%20Sanhorn%20Chen%20and%20Lecheng%20Zheng%20and%20Boyu%20Liu%20and%20Zihao%20Li%20and%20Jiaru%20Zou%20and%20Tianxin%20Wei%20and%20Zhining%20Liu%20and%20Zhichen%20Zeng%20and%20Ruizhong%20Qiu%20and%20Xiao%20Lin%20and%20Yuchen%20Yan%20and%20Dongqi%20Fu%20and%20Jingchao%20Ni%20and%20Jingrui%20He%20and%20Hanghang%20Tong%0AAbstract%3A%20Time%20series%20data%20are%20integral%20to%20critical%20applications%20across%20domains%20such%20as%20finance%2C%20healthcare%2C%20transportation%2C%20and%20environmental%20science.%20While%20recent%20work%20has%20begun%20to%20explore%20multi-task%20time%20series%20question%20answering%20%28QA%29%2C%20current%20benchmarks%20remain%20limited%20to%20forecasting%20and%20anomaly%20detection%20tasks.%20We%20introduce%20TSAQA%2C%20a%20novel%20unified%20benchmark%20designed%20to%20broaden%20task%20coverage%20and%20evaluate%20diverse%20temporal%20analysis%20capabilities.%20TSAQA%20integrates%20six%20diverse%20tasks%20under%20a%20single%20framework%20ranging%20from%20conventional%20analysis%2C%20including%20anomaly%20detection%20and%20classification%2C%20to%20advanced%20analysis%2C%20such%20as%20characterization%2C%20comparison%2C%20data%20transformation%2C%20and%20temporal%20relationship%20analysis.%20Spanning%20210k%20samples%20across%2013%20domains%2C%20the%20dataset%20employs%20diverse%20formats%2C%20including%20true-or-false%20%28TF%29%2C%20multiple-choice%20%28MC%29%2C%20and%20a%20novel%20puzzling%20%28PZ%29%2C%20to%20comprehensively%20assess%20time%20series%20analysis.%20Zero-shot%20evaluation%20demonstrates%20that%20these%20tasks%20are%20challenging%20for%20current%20Large%20Language%20Models%20%28LLMs%29%3A%20the%20best-performing%20commercial%20LLM%2C%20Gemini-2.5-Flash%2C%20achieves%20an%20average%20score%20of%20only%2065.08.%20Although%20instruction%20tuning%20boosts%20open-source%20performance%3A%20the%20best-performing%20open-source%20model%2C%20LLaMA-3.1-8B%2C%20shows%20significant%20room%20for%20improvement%2C%20highlighting%20the%20complexity%20of%20temporal%20analysis%20for%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSAQA%253A%2520Time%2520Series%2520Analysis%2520Question%2520And%2520Answering%2520Benchmark%26entry.906535625%3DBaoyu%2520Jing%2520and%2520Sanhorn%2520Chen%2520and%2520Lecheng%2520Zheng%2520and%2520Boyu%2520Liu%2520and%2520Zihao%2520Li%2520and%2520Jiaru%2520Zou%2520and%2520Tianxin%2520Wei%2520and%2520Zhining%2520Liu%2520and%2520Zhichen%2520Zeng%2520and%2520Ruizhong%2520Qiu%2520and%2520Xiao%2520Lin%2520and%2520Yuchen%2520Yan%2520and%2520Dongqi%2520Fu%2520and%2520Jingchao%2520Ni%2520and%2520Jingrui%2520He%2520and%2520Hanghang%2520Tong%26entry.1292438233%3DTime%2520series%2520data%2520are%2520integral%2520to%2520critical%2520applications%2520across%2520domains%2520such%2520as%2520finance%252C%2520healthcare%252C%2520transportation%252C%2520and%2520environmental%2520science.%2520While%2520recent%2520work%2520has%2520begun%2520to%2520explore%2520multi-task%2520time%2520series%2520question%2520answering%2520%2528QA%2529%252C%2520current%2520benchmarks%2520remain%2520limited%2520to%2520forecasting%2520and%2520anomaly%2520detection%2520tasks.%2520We%2520introduce%2520TSAQA%252C%2520a%2520novel%2520unified%2520benchmark%2520designed%2520to%2520broaden%2520task%2520coverage%2520and%2520evaluate%2520diverse%2520temporal%2520analysis%2520capabilities.%2520TSAQA%2520integrates%2520six%2520diverse%2520tasks%2520under%2520a%2520single%2520framework%2520ranging%2520from%2520conventional%2520analysis%252C%2520including%2520anomaly%2520detection%2520and%2520classification%252C%2520to%2520advanced%2520analysis%252C%2520such%2520as%2520characterization%252C%2520comparison%252C%2520data%2520transformation%252C%2520and%2520temporal%2520relationship%2520analysis.%2520Spanning%2520210k%2520samples%2520across%252013%2520domains%252C%2520the%2520dataset%2520employs%2520diverse%2520formats%252C%2520including%2520true-or-false%2520%2528TF%2529%252C%2520multiple-choice%2520%2528MC%2529%252C%2520and%2520a%2520novel%2520puzzling%2520%2528PZ%2529%252C%2520to%2520comprehensively%2520assess%2520time%2520series%2520analysis.%2520Zero-shot%2520evaluation%2520demonstrates%2520that%2520these%2520tasks%2520are%2520challenging%2520for%2520current%2520Large%2520Language%2520Models%2520%2528LLMs%2529%253A%2520the%2520best-performing%2520commercial%2520LLM%252C%2520Gemini-2.5-Flash%252C%2520achieves%2520an%2520average%2520score%2520of%2520only%252065.08.%2520Although%2520instruction%2520tuning%2520boosts%2520open-source%2520performance%253A%2520the%2520best-performing%2520open-source%2520model%252C%2520LLaMA-3.1-8B%252C%2520shows%2520significant%2520room%2520for%2520improvement%252C%2520highlighting%2520the%2520complexity%2520of%2520temporal%2520analysis%2520for%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSAQA%3A%20Time%20Series%20Analysis%20Question%20And%20Answering%20Benchmark&entry.906535625=Baoyu%20Jing%20and%20Sanhorn%20Chen%20and%20Lecheng%20Zheng%20and%20Boyu%20Liu%20and%20Zihao%20Li%20and%20Jiaru%20Zou%20and%20Tianxin%20Wei%20and%20Zhining%20Liu%20and%20Zhichen%20Zeng%20and%20Ruizhong%20Qiu%20and%20Xiao%20Lin%20and%20Yuchen%20Yan%20and%20Dongqi%20Fu%20and%20Jingchao%20Ni%20and%20Jingrui%20He%20and%20Hanghang%20Tong&entry.1292438233=Time%20series%20data%20are%20integral%20to%20critical%20applications%20across%20domains%20such%20as%20finance%2C%20healthcare%2C%20transportation%2C%20and%20environmental%20science.%20While%20recent%20work%20has%20begun%20to%20explore%20multi-task%20time%20series%20question%20answering%20%28QA%29%2C%20current%20benchmarks%20remain%20limited%20to%20forecasting%20and%20anomaly%20detection%20tasks.%20We%20introduce%20TSAQA%2C%20a%20novel%20unified%20benchmark%20designed%20to%20broaden%20task%20coverage%20and%20evaluate%20diverse%20temporal%20analysis%20capabilities.%20TSAQA%20integrates%20six%20diverse%20tasks%20under%20a%20single%20framework%20ranging%20from%20conventional%20analysis%2C%20including%20anomaly%20detection%20and%20classification%2C%20to%20advanced%20analysis%2C%20such%20as%20characterization%2C%20comparison%2C%20data%20transformation%2C%20and%20temporal%20relationship%20analysis.%20Spanning%20210k%20samples%20across%2013%20domains%2C%20the%20dataset%20employs%20diverse%20formats%2C%20including%20true-or-false%20%28TF%29%2C%20multiple-choice%20%28MC%29%2C%20and%20a%20novel%20puzzling%20%28PZ%29%2C%20to%20comprehensively%20assess%20time%20series%20analysis.%20Zero-shot%20evaluation%20demonstrates%20that%20these%20tasks%20are%20challenging%20for%20current%20Large%20Language%20Models%20%28LLMs%29%3A%20the%20best-performing%20commercial%20LLM%2C%20Gemini-2.5-Flash%2C%20achieves%20an%20average%20score%20of%20only%2065.08.%20Although%20instruction%20tuning%20boosts%20open-source%20performance%3A%20the%20best-performing%20open-source%20model%2C%20LLaMA-3.1-8B%2C%20shows%20significant%20room%20for%20improvement%2C%20highlighting%20the%20complexity%20of%20temporal%20analysis%20for%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.23204v1&entry.124074799=Read"},
{"title": "Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection", "author": "Nan Zhong and Yiran Xu and Mian Zou", "abstract": "As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.", "link": "http://arxiv.org/abs/2601.22778v1", "date": "2026-01-30", "relevancy": 2.2503, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5713}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5647}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Color%20Matters%3A%20Demosaicing-Guided%20Color%20Correlation%20Training%20for%20Generalizable%20AI-Generated%20Image%20Detection&body=Title%3A%20Color%20Matters%3A%20Demosaicing-Guided%20Color%20Correlation%20Training%20for%20Generalizable%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Nan%20Zhong%20and%20Yiran%20Xu%20and%20Mian%20Zou%0AAbstract%3A%20As%20realistic%20AI-generated%20images%20threaten%20digital%20authenticity%2C%20we%20address%20the%20generalization%20failure%20of%20generative%20artifact-based%20detectors%20by%20exploiting%20the%20intrinsic%20properties%20of%20the%20camera%20imaging%20pipeline.%20Concretely%2C%20we%20investigate%20color%20correlations%20induced%20by%20the%20color%20filter%20array%20%28CFA%29%20and%20demosaicing%2C%20and%20propose%20a%20Demosaicing-guided%20Color%20Correlation%20Training%20%28DCCT%29%20framework%20for%20AI-generated%20image%20detection.%20By%20simulating%20the%20CFA%20sampling%20pattern%2C%20we%20decompose%20each%20color%20image%20into%20a%20single-channel%20input%20%28as%20the%20condition%29%20and%20the%20remaining%20two%20channels%20as%20the%20ground-truth%20targets%20%28for%20prediction%29.%20A%20self-supervised%20U-Net%20is%20trained%20to%20model%20the%20conditional%20distribution%20of%20the%20missing%20channels%20from%20the%20given%20one%2C%20parameterized%20via%20a%20mixture%20of%20logistic%20functions.%20Our%20theoretical%20analysis%20reveals%20that%20DCCT%20targets%20a%20provable%20distributional%20difference%20in%20color-correlation%20features%20between%20photographic%20and%20AI-generated%20images.%20By%20leveraging%20these%20distinct%20features%20to%20construct%20a%20binary%20classifier%2C%20DCCT%20achieves%20state-of-the-art%20generalization%20and%20robustness%2C%20significantly%20outperforming%20prior%20methods%20across%20over%2020%20unseen%20generators.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColor%2520Matters%253A%2520Demosaicing-Guided%2520Color%2520Correlation%2520Training%2520for%2520Generalizable%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DNan%2520Zhong%2520and%2520Yiran%2520Xu%2520and%2520Mian%2520Zou%26entry.1292438233%3DAs%2520realistic%2520AI-generated%2520images%2520threaten%2520digital%2520authenticity%252C%2520we%2520address%2520the%2520generalization%2520failure%2520of%2520generative%2520artifact-based%2520detectors%2520by%2520exploiting%2520the%2520intrinsic%2520properties%2520of%2520the%2520camera%2520imaging%2520pipeline.%2520Concretely%252C%2520we%2520investigate%2520color%2520correlations%2520induced%2520by%2520the%2520color%2520filter%2520array%2520%2528CFA%2529%2520and%2520demosaicing%252C%2520and%2520propose%2520a%2520Demosaicing-guided%2520Color%2520Correlation%2520Training%2520%2528DCCT%2529%2520framework%2520for%2520AI-generated%2520image%2520detection.%2520By%2520simulating%2520the%2520CFA%2520sampling%2520pattern%252C%2520we%2520decompose%2520each%2520color%2520image%2520into%2520a%2520single-channel%2520input%2520%2528as%2520the%2520condition%2529%2520and%2520the%2520remaining%2520two%2520channels%2520as%2520the%2520ground-truth%2520targets%2520%2528for%2520prediction%2529.%2520A%2520self-supervised%2520U-Net%2520is%2520trained%2520to%2520model%2520the%2520conditional%2520distribution%2520of%2520the%2520missing%2520channels%2520from%2520the%2520given%2520one%252C%2520parameterized%2520via%2520a%2520mixture%2520of%2520logistic%2520functions.%2520Our%2520theoretical%2520analysis%2520reveals%2520that%2520DCCT%2520targets%2520a%2520provable%2520distributional%2520difference%2520in%2520color-correlation%2520features%2520between%2520photographic%2520and%2520AI-generated%2520images.%2520By%2520leveraging%2520these%2520distinct%2520features%2520to%2520construct%2520a%2520binary%2520classifier%252C%2520DCCT%2520achieves%2520state-of-the-art%2520generalization%2520and%2520robustness%252C%2520significantly%2520outperforming%2520prior%2520methods%2520across%2520over%252020%2520unseen%2520generators.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Color%20Matters%3A%20Demosaicing-Guided%20Color%20Correlation%20Training%20for%20Generalizable%20AI-Generated%20Image%20Detection&entry.906535625=Nan%20Zhong%20and%20Yiran%20Xu%20and%20Mian%20Zou&entry.1292438233=As%20realistic%20AI-generated%20images%20threaten%20digital%20authenticity%2C%20we%20address%20the%20generalization%20failure%20of%20generative%20artifact-based%20detectors%20by%20exploiting%20the%20intrinsic%20properties%20of%20the%20camera%20imaging%20pipeline.%20Concretely%2C%20we%20investigate%20color%20correlations%20induced%20by%20the%20color%20filter%20array%20%28CFA%29%20and%20demosaicing%2C%20and%20propose%20a%20Demosaicing-guided%20Color%20Correlation%20Training%20%28DCCT%29%20framework%20for%20AI-generated%20image%20detection.%20By%20simulating%20the%20CFA%20sampling%20pattern%2C%20we%20decompose%20each%20color%20image%20into%20a%20single-channel%20input%20%28as%20the%20condition%29%20and%20the%20remaining%20two%20channels%20as%20the%20ground-truth%20targets%20%28for%20prediction%29.%20A%20self-supervised%20U-Net%20is%20trained%20to%20model%20the%20conditional%20distribution%20of%20the%20missing%20channels%20from%20the%20given%20one%2C%20parameterized%20via%20a%20mixture%20of%20logistic%20functions.%20Our%20theoretical%20analysis%20reveals%20that%20DCCT%20targets%20a%20provable%20distributional%20difference%20in%20color-correlation%20features%20between%20photographic%20and%20AI-generated%20images.%20By%20leveraging%20these%20distinct%20features%20to%20construct%20a%20binary%20classifier%2C%20DCCT%20achieves%20state-of-the-art%20generalization%20and%20robustness%2C%20significantly%20outperforming%20prior%20methods%20across%20over%2020%20unseen%20generators.&entry.1838667208=http%3A//arxiv.org/abs/2601.22778v1&entry.124074799=Read"},
{"title": "Monocular pose estimation of articulated open surgery tools -- in the wild", "author": "Robert Spektor and Tom Friedman and Itay Or and Gil Bolotin and Shlomi Laufer", "abstract": "This work presents a framework for monocular 6D pose estimation of surgical instruments in open surgery, addressing challenges such as object articulations, specularity, occlusions, and synthetic-to-real domain adaptation. The proposed approach consists of three main components: $(1)$ synthetic data generation pipeline that incorporates 3D scanning of surgical tools with articulation rigging and physically-based rendering; $(2)$ a tailored pose estimation framework combining tool detection with pose and articulation estimation; and $(3)$ a training strategy on synthetic and real unannotated video data, employing domain adaptation with automatically generated pseudo-labels. Evaluations conducted on real data of open surgery demonstrate the good performance and real-world applicability of the proposed framework, highlighting its potential for integration into medical augmented reality and robotic systems. The approach eliminates the need for extensive manual annotation of real surgical data.", "link": "http://arxiv.org/abs/2407.12138v3", "date": "2026-01-30", "relevancy": 2.2456, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5668}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20pose%20estimation%20of%20articulated%20open%20surgery%20tools%20--%20in%20the%20wild&body=Title%3A%20Monocular%20pose%20estimation%20of%20articulated%20open%20surgery%20tools%20--%20in%20the%20wild%0AAuthor%3A%20Robert%20Spektor%20and%20Tom%20Friedman%20and%20Itay%20Or%20and%20Gil%20Bolotin%20and%20Shlomi%20Laufer%0AAbstract%3A%20This%20work%20presents%20a%20framework%20for%20monocular%206D%20pose%20estimation%20of%20surgical%20instruments%20in%20open%20surgery%2C%20addressing%20challenges%20such%20as%20object%20articulations%2C%20specularity%2C%20occlusions%2C%20and%20synthetic-to-real%20domain%20adaptation.%20The%20proposed%20approach%20consists%20of%20three%20main%20components%3A%20%24%281%29%24%20synthetic%20data%20generation%20pipeline%20that%20incorporates%203D%20scanning%20of%20surgical%20tools%20with%20articulation%20rigging%20and%20physically-based%20rendering%3B%20%24%282%29%24%20a%20tailored%20pose%20estimation%20framework%20combining%20tool%20detection%20with%20pose%20and%20articulation%20estimation%3B%20and%20%24%283%29%24%20a%20training%20strategy%20on%20synthetic%20and%20real%20unannotated%20video%20data%2C%20employing%20domain%20adaptation%20with%20automatically%20generated%20pseudo-labels.%20Evaluations%20conducted%20on%20real%20data%20of%20open%20surgery%20demonstrate%20the%20good%20performance%20and%20real-world%20applicability%20of%20the%20proposed%20framework%2C%20highlighting%20its%20potential%20for%20integration%20into%20medical%20augmented%20reality%20and%20robotic%20systems.%20The%20approach%20eliminates%20the%20need%20for%20extensive%20manual%20annotation%20of%20real%20surgical%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2407.12138v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520pose%2520estimation%2520of%2520articulated%2520open%2520surgery%2520tools%2520--%2520in%2520the%2520wild%26entry.906535625%3DRobert%2520Spektor%2520and%2520Tom%2520Friedman%2520and%2520Itay%2520Or%2520and%2520Gil%2520Bolotin%2520and%2520Shlomi%2520Laufer%26entry.1292438233%3DThis%2520work%2520presents%2520a%2520framework%2520for%2520monocular%25206D%2520pose%2520estimation%2520of%2520surgical%2520instruments%2520in%2520open%2520surgery%252C%2520addressing%2520challenges%2520such%2520as%2520object%2520articulations%252C%2520specularity%252C%2520occlusions%252C%2520and%2520synthetic-to-real%2520domain%2520adaptation.%2520The%2520proposed%2520approach%2520consists%2520of%2520three%2520main%2520components%253A%2520%2524%25281%2529%2524%2520synthetic%2520data%2520generation%2520pipeline%2520that%2520incorporates%25203D%2520scanning%2520of%2520surgical%2520tools%2520with%2520articulation%2520rigging%2520and%2520physically-based%2520rendering%253B%2520%2524%25282%2529%2524%2520a%2520tailored%2520pose%2520estimation%2520framework%2520combining%2520tool%2520detection%2520with%2520pose%2520and%2520articulation%2520estimation%253B%2520and%2520%2524%25283%2529%2524%2520a%2520training%2520strategy%2520on%2520synthetic%2520and%2520real%2520unannotated%2520video%2520data%252C%2520employing%2520domain%2520adaptation%2520with%2520automatically%2520generated%2520pseudo-labels.%2520Evaluations%2520conducted%2520on%2520real%2520data%2520of%2520open%2520surgery%2520demonstrate%2520the%2520good%2520performance%2520and%2520real-world%2520applicability%2520of%2520the%2520proposed%2520framework%252C%2520highlighting%2520its%2520potential%2520for%2520integration%2520into%2520medical%2520augmented%2520reality%2520and%2520robotic%2520systems.%2520The%2520approach%2520eliminates%2520the%2520need%2520for%2520extensive%2520manual%2520annotation%2520of%2520real%2520surgical%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12138v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20pose%20estimation%20of%20articulated%20open%20surgery%20tools%20--%20in%20the%20wild&entry.906535625=Robert%20Spektor%20and%20Tom%20Friedman%20and%20Itay%20Or%20and%20Gil%20Bolotin%20and%20Shlomi%20Laufer&entry.1292438233=This%20work%20presents%20a%20framework%20for%20monocular%206D%20pose%20estimation%20of%20surgical%20instruments%20in%20open%20surgery%2C%20addressing%20challenges%20such%20as%20object%20articulations%2C%20specularity%2C%20occlusions%2C%20and%20synthetic-to-real%20domain%20adaptation.%20The%20proposed%20approach%20consists%20of%20three%20main%20components%3A%20%24%281%29%24%20synthetic%20data%20generation%20pipeline%20that%20incorporates%203D%20scanning%20of%20surgical%20tools%20with%20articulation%20rigging%20and%20physically-based%20rendering%3B%20%24%282%29%24%20a%20tailored%20pose%20estimation%20framework%20combining%20tool%20detection%20with%20pose%20and%20articulation%20estimation%3B%20and%20%24%283%29%24%20a%20training%20strategy%20on%20synthetic%20and%20real%20unannotated%20video%20data%2C%20employing%20domain%20adaptation%20with%20automatically%20generated%20pseudo-labels.%20Evaluations%20conducted%20on%20real%20data%20of%20open%20surgery%20demonstrate%20the%20good%20performance%20and%20real-world%20applicability%20of%20the%20proposed%20framework%2C%20highlighting%20its%20potential%20for%20integration%20into%20medical%20augmented%20reality%20and%20robotic%20systems.%20The%20approach%20eliminates%20the%20need%20for%20extensive%20manual%20annotation%20of%20real%20surgical%20data.&entry.1838667208=http%3A//arxiv.org/abs/2407.12138v3&entry.124074799=Read"},
{"title": "Diachronic Stereo Matching for Multi-Date Satellite Imagery", "author": "El\u00edas Masquil and Luca Savant Aira and Roger Mar\u00ed and Thibaud Ehret and Pablo Mus\u00e9 and Gabriele Facciolo", "abstract": "Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions. On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations. On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs. However, when the two images are captured months apart, strong seasonal, illumination, and shadow changes violate standard stereoscopic assumptions, causing existing pipelines to fail. This work presents the first Diachronic Stereo Matching method for satellite imagery, enabling reliable 3D reconstruction from temporally distant pairs. Two advances make this possible: (1) fine-tuning a state-of-the-art deep stereo network that leverages monocular depth priors, and (2) exposing it to a dataset specifically curated to include a diverse set of diachronic image pairs. In particular, we start from a pretrained MonSter model, trained initially on a mix of synthetic and real datasets such as SceneFlow and KITTI, and fine-tune it on a set of stereo pairs derived from the DFC2019 remote sensing challenge. This dataset contains both synchronic and diachronic pairs under diverse seasonal and illumination conditions. Experiments on multi-date WorldView-3 imagery demonstrate that our approach consistently surpasses classical pipelines and unadapted deep stereo models on both synchronic and diachronic settings. Fine-tuning on temporally diverse images, together with monocular priors, proves essential for enabling 3D reconstruction from previously incompatible acquisition dates. Left image (winter) Right image (autumn) DSM geometry Ours (1.23 m) Zero-shot (3.99 m) LiDAR GT Figure 1. Output geometry for a winter-autumn image pair from Omaha (OMA 331 test scene). Our method recovers accurate geometry despite the diachronic nature of the pair, exhibiting strong appearance changes, which cause existing zero-shot methods to fail. Missing values due to perspective shown in black.  Mean altitude error in parentheses; lower is better.", "link": "http://arxiv.org/abs/2601.22808v1", "date": "2026-01-30", "relevancy": 2.2385, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5755}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5673}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diachronic%20Stereo%20Matching%20for%20Multi-Date%20Satellite%20Imagery&body=Title%3A%20Diachronic%20Stereo%20Matching%20for%20Multi-Date%20Satellite%20Imagery%0AAuthor%3A%20El%C3%ADas%20Masquil%20and%20Luca%20Savant%20Aira%20and%20Roger%20Mar%C3%AD%20and%20Thibaud%20Ehret%20and%20Pablo%20Mus%C3%A9%20and%20Gabriele%20Facciolo%0AAbstract%3A%20Recent%20advances%20in%20image-based%20satellite%203D%20reconstruction%20have%20progressed%20along%20two%20complementary%20directions.%20On%20one%20hand%2C%20multi-date%20approaches%20using%20NeRF%20or%20Gaussian-splatting%20jointly%20model%20appearance%20and%20geometry%20across%20many%20acquisitions%2C%20achieving%20accurate%20reconstructions%20on%20opportunistic%20imagery%20with%20numerous%20observations.%20On%20the%20other%20hand%2C%20classical%20stereoscopic%20reconstruction%20pipelines%20deliver%20robust%20and%20scalable%20results%20for%20simultaneous%20or%20quasi-simultaneous%20image%20pairs.%20However%2C%20when%20the%20two%20images%20are%20captured%20months%20apart%2C%20strong%20seasonal%2C%20illumination%2C%20and%20shadow%20changes%20violate%20standard%20stereoscopic%20assumptions%2C%20causing%20existing%20pipelines%20to%20fail.%20This%20work%20presents%20the%20first%20Diachronic%20Stereo%20Matching%20method%20for%20satellite%20imagery%2C%20enabling%20reliable%203D%20reconstruction%20from%20temporally%20distant%20pairs.%20Two%20advances%20make%20this%20possible%3A%20%281%29%20fine-tuning%20a%20state-of-the-art%20deep%20stereo%20network%20that%20leverages%20monocular%20depth%20priors%2C%20and%20%282%29%20exposing%20it%20to%20a%20dataset%20specifically%20curated%20to%20include%20a%20diverse%20set%20of%20diachronic%20image%20pairs.%20In%20particular%2C%20we%20start%20from%20a%20pretrained%20MonSter%20model%2C%20trained%20initially%20on%20a%20mix%20of%20synthetic%20and%20real%20datasets%20such%20as%20SceneFlow%20and%20KITTI%2C%20and%20fine-tune%20it%20on%20a%20set%20of%20stereo%20pairs%20derived%20from%20the%20DFC2019%20remote%20sensing%20challenge.%20This%20dataset%20contains%20both%20synchronic%20and%20diachronic%20pairs%20under%20diverse%20seasonal%20and%20illumination%20conditions.%20Experiments%20on%20multi-date%20WorldView-3%20imagery%20demonstrate%20that%20our%20approach%20consistently%20surpasses%20classical%20pipelines%20and%20unadapted%20deep%20stereo%20models%20on%20both%20synchronic%20and%20diachronic%20settings.%20Fine-tuning%20on%20temporally%20diverse%20images%2C%20together%20with%20monocular%20priors%2C%20proves%20essential%20for%20enabling%203D%20reconstruction%20from%20previously%20incompatible%20acquisition%20dates.%20Left%20image%20%28winter%29%20Right%20image%20%28autumn%29%20DSM%20geometry%20Ours%20%281.23%20m%29%20Zero-shot%20%283.99%20m%29%20LiDAR%20GT%20Figure%201.%20Output%20geometry%20for%20a%20winter-autumn%20image%20pair%20from%20Omaha%20%28OMA%20331%20test%20scene%29.%20Our%20method%20recovers%20accurate%20geometry%20despite%20the%20diachronic%20nature%20of%20the%20pair%2C%20exhibiting%20strong%20appearance%20changes%2C%20which%20cause%20existing%20zero-shot%20methods%20to%20fail.%20Missing%20values%20due%20to%20perspective%20shown%20in%20black.%20%20Mean%20altitude%20error%20in%20parentheses%3B%20lower%20is%20better.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiachronic%2520Stereo%2520Matching%2520for%2520Multi-Date%2520Satellite%2520Imagery%26entry.906535625%3DEl%25C3%25ADas%2520Masquil%2520and%2520Luca%2520Savant%2520Aira%2520and%2520Roger%2520Mar%25C3%25AD%2520and%2520Thibaud%2520Ehret%2520and%2520Pablo%2520Mus%25C3%25A9%2520and%2520Gabriele%2520Facciolo%26entry.1292438233%3DRecent%2520advances%2520in%2520image-based%2520satellite%25203D%2520reconstruction%2520have%2520progressed%2520along%2520two%2520complementary%2520directions.%2520On%2520one%2520hand%252C%2520multi-date%2520approaches%2520using%2520NeRF%2520or%2520Gaussian-splatting%2520jointly%2520model%2520appearance%2520and%2520geometry%2520across%2520many%2520acquisitions%252C%2520achieving%2520accurate%2520reconstructions%2520on%2520opportunistic%2520imagery%2520with%2520numerous%2520observations.%2520On%2520the%2520other%2520hand%252C%2520classical%2520stereoscopic%2520reconstruction%2520pipelines%2520deliver%2520robust%2520and%2520scalable%2520results%2520for%2520simultaneous%2520or%2520quasi-simultaneous%2520image%2520pairs.%2520However%252C%2520when%2520the%2520two%2520images%2520are%2520captured%2520months%2520apart%252C%2520strong%2520seasonal%252C%2520illumination%252C%2520and%2520shadow%2520changes%2520violate%2520standard%2520stereoscopic%2520assumptions%252C%2520causing%2520existing%2520pipelines%2520to%2520fail.%2520This%2520work%2520presents%2520the%2520first%2520Diachronic%2520Stereo%2520Matching%2520method%2520for%2520satellite%2520imagery%252C%2520enabling%2520reliable%25203D%2520reconstruction%2520from%2520temporally%2520distant%2520pairs.%2520Two%2520advances%2520make%2520this%2520possible%253A%2520%25281%2529%2520fine-tuning%2520a%2520state-of-the-art%2520deep%2520stereo%2520network%2520that%2520leverages%2520monocular%2520depth%2520priors%252C%2520and%2520%25282%2529%2520exposing%2520it%2520to%2520a%2520dataset%2520specifically%2520curated%2520to%2520include%2520a%2520diverse%2520set%2520of%2520diachronic%2520image%2520pairs.%2520In%2520particular%252C%2520we%2520start%2520from%2520a%2520pretrained%2520MonSter%2520model%252C%2520trained%2520initially%2520on%2520a%2520mix%2520of%2520synthetic%2520and%2520real%2520datasets%2520such%2520as%2520SceneFlow%2520and%2520KITTI%252C%2520and%2520fine-tune%2520it%2520on%2520a%2520set%2520of%2520stereo%2520pairs%2520derived%2520from%2520the%2520DFC2019%2520remote%2520sensing%2520challenge.%2520This%2520dataset%2520contains%2520both%2520synchronic%2520and%2520diachronic%2520pairs%2520under%2520diverse%2520seasonal%2520and%2520illumination%2520conditions.%2520Experiments%2520on%2520multi-date%2520WorldView-3%2520imagery%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520surpasses%2520classical%2520pipelines%2520and%2520unadapted%2520deep%2520stereo%2520models%2520on%2520both%2520synchronic%2520and%2520diachronic%2520settings.%2520Fine-tuning%2520on%2520temporally%2520diverse%2520images%252C%2520together%2520with%2520monocular%2520priors%252C%2520proves%2520essential%2520for%2520enabling%25203D%2520reconstruction%2520from%2520previously%2520incompatible%2520acquisition%2520dates.%2520Left%2520image%2520%2528winter%2529%2520Right%2520image%2520%2528autumn%2529%2520DSM%2520geometry%2520Ours%2520%25281.23%2520m%2529%2520Zero-shot%2520%25283.99%2520m%2529%2520LiDAR%2520GT%2520Figure%25201.%2520Output%2520geometry%2520for%2520a%2520winter-autumn%2520image%2520pair%2520from%2520Omaha%2520%2528OMA%2520331%2520test%2520scene%2529.%2520Our%2520method%2520recovers%2520accurate%2520geometry%2520despite%2520the%2520diachronic%2520nature%2520of%2520the%2520pair%252C%2520exhibiting%2520strong%2520appearance%2520changes%252C%2520which%2520cause%2520existing%2520zero-shot%2520methods%2520to%2520fail.%2520Missing%2520values%2520due%2520to%2520perspective%2520shown%2520in%2520black.%2520%2520Mean%2520altitude%2520error%2520in%2520parentheses%253B%2520lower%2520is%2520better.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diachronic%20Stereo%20Matching%20for%20Multi-Date%20Satellite%20Imagery&entry.906535625=El%C3%ADas%20Masquil%20and%20Luca%20Savant%20Aira%20and%20Roger%20Mar%C3%AD%20and%20Thibaud%20Ehret%20and%20Pablo%20Mus%C3%A9%20and%20Gabriele%20Facciolo&entry.1292438233=Recent%20advances%20in%20image-based%20satellite%203D%20reconstruction%20have%20progressed%20along%20two%20complementary%20directions.%20On%20one%20hand%2C%20multi-date%20approaches%20using%20NeRF%20or%20Gaussian-splatting%20jointly%20model%20appearance%20and%20geometry%20across%20many%20acquisitions%2C%20achieving%20accurate%20reconstructions%20on%20opportunistic%20imagery%20with%20numerous%20observations.%20On%20the%20other%20hand%2C%20classical%20stereoscopic%20reconstruction%20pipelines%20deliver%20robust%20and%20scalable%20results%20for%20simultaneous%20or%20quasi-simultaneous%20image%20pairs.%20However%2C%20when%20the%20two%20images%20are%20captured%20months%20apart%2C%20strong%20seasonal%2C%20illumination%2C%20and%20shadow%20changes%20violate%20standard%20stereoscopic%20assumptions%2C%20causing%20existing%20pipelines%20to%20fail.%20This%20work%20presents%20the%20first%20Diachronic%20Stereo%20Matching%20method%20for%20satellite%20imagery%2C%20enabling%20reliable%203D%20reconstruction%20from%20temporally%20distant%20pairs.%20Two%20advances%20make%20this%20possible%3A%20%281%29%20fine-tuning%20a%20state-of-the-art%20deep%20stereo%20network%20that%20leverages%20monocular%20depth%20priors%2C%20and%20%282%29%20exposing%20it%20to%20a%20dataset%20specifically%20curated%20to%20include%20a%20diverse%20set%20of%20diachronic%20image%20pairs.%20In%20particular%2C%20we%20start%20from%20a%20pretrained%20MonSter%20model%2C%20trained%20initially%20on%20a%20mix%20of%20synthetic%20and%20real%20datasets%20such%20as%20SceneFlow%20and%20KITTI%2C%20and%20fine-tune%20it%20on%20a%20set%20of%20stereo%20pairs%20derived%20from%20the%20DFC2019%20remote%20sensing%20challenge.%20This%20dataset%20contains%20both%20synchronic%20and%20diachronic%20pairs%20under%20diverse%20seasonal%20and%20illumination%20conditions.%20Experiments%20on%20multi-date%20WorldView-3%20imagery%20demonstrate%20that%20our%20approach%20consistently%20surpasses%20classical%20pipelines%20and%20unadapted%20deep%20stereo%20models%20on%20both%20synchronic%20and%20diachronic%20settings.%20Fine-tuning%20on%20temporally%20diverse%20images%2C%20together%20with%20monocular%20priors%2C%20proves%20essential%20for%20enabling%203D%20reconstruction%20from%20previously%20incompatible%20acquisition%20dates.%20Left%20image%20%28winter%29%20Right%20image%20%28autumn%29%20DSM%20geometry%20Ours%20%281.23%20m%29%20Zero-shot%20%283.99%20m%29%20LiDAR%20GT%20Figure%201.%20Output%20geometry%20for%20a%20winter-autumn%20image%20pair%20from%20Omaha%20%28OMA%20331%20test%20scene%29.%20Our%20method%20recovers%20accurate%20geometry%20despite%20the%20diachronic%20nature%20of%20the%20pair%2C%20exhibiting%20strong%20appearance%20changes%2C%20which%20cause%20existing%20zero-shot%20methods%20to%20fail.%20Missing%20values%20due%20to%20perspective%20shown%20in%20black.%20%20Mean%20altitude%20error%20in%20parentheses%3B%20lower%20is%20better.&entry.1838667208=http%3A//arxiv.org/abs/2601.22808v1&entry.124074799=Read"},
{"title": "Scalable Topology-Preserving Graph Coarsening with Graph Collapse", "author": "Xiang Wu and Rong-Hua Li and Xunkai Li and Kangfei Zhao and Hongchao Qin and Guoren Wang", "abstract": "Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.", "link": "http://arxiv.org/abs/2601.22943v1", "date": "2026-01-30", "relevancy": 2.2138, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4537}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.444}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Topology-Preserving%20Graph%20Coarsening%20with%20Graph%20Collapse&body=Title%3A%20Scalable%20Topology-Preserving%20Graph%20Coarsening%20with%20Graph%20Collapse%0AAuthor%3A%20Xiang%20Wu%20and%20Rong-Hua%20Li%20and%20Xunkai%20Li%20and%20Kangfei%20Zhao%20and%20Hongchao%20Qin%20and%20Guoren%20Wang%0AAbstract%3A%20Graph%20coarsening%20reduces%20the%20size%20of%20a%20graph%20while%20preserving%20certain%20properties.%20Most%20existing%20methods%20preserve%20either%20spectral%20or%20spatial%20characteristics.%20Recent%20research%20has%20shown%20that%20preserving%20topological%20features%20helps%20maintain%20the%20predictive%20performance%20of%20graph%20neural%20networks%20%28GNNs%29%20trained%20on%20the%20coarsened%20graph%20but%20suffers%20from%20exponential%20time%20complexity.%20To%20address%20these%20problems%2C%20we%20propose%20Scalable%20Topology-Preserving%20Graph%20Coarsening%20%28STPGC%29%20by%20introducing%20the%20concepts%20of%20graph%20strong%20collapse%20and%20graph%20edge%20collapse%20extended%20from%20algebraic%20topology.%20STPGC%20comprises%20three%20new%20algorithms%2C%20GStrongCollapse%2C%20GEdgeCollapse%2C%20and%20NeighborhoodConing%20based%20on%20these%20two%20concepts%2C%20which%20eliminate%20dominated%20nodes%20and%20edges%20while%20rigorously%20preserving%20topological%20features.%20We%20further%20prove%20that%20STPGC%20preserves%20the%20GNN%20receptive%20field%20and%20develop%20approximate%20algorithms%20to%20accelerate%20GNN%20training.%20Experiments%20on%20node%20classification%20with%20GNNs%20demonstrate%20the%20efficiency%20and%20effectiveness%20of%20STPGC.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Topology-Preserving%2520Graph%2520Coarsening%2520with%2520Graph%2520Collapse%26entry.906535625%3DXiang%2520Wu%2520and%2520Rong-Hua%2520Li%2520and%2520Xunkai%2520Li%2520and%2520Kangfei%2520Zhao%2520and%2520Hongchao%2520Qin%2520and%2520Guoren%2520Wang%26entry.1292438233%3DGraph%2520coarsening%2520reduces%2520the%2520size%2520of%2520a%2520graph%2520while%2520preserving%2520certain%2520properties.%2520Most%2520existing%2520methods%2520preserve%2520either%2520spectral%2520or%2520spatial%2520characteristics.%2520Recent%2520research%2520has%2520shown%2520that%2520preserving%2520topological%2520features%2520helps%2520maintain%2520the%2520predictive%2520performance%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520trained%2520on%2520the%2520coarsened%2520graph%2520but%2520suffers%2520from%2520exponential%2520time%2520complexity.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520Scalable%2520Topology-Preserving%2520Graph%2520Coarsening%2520%2528STPGC%2529%2520by%2520introducing%2520the%2520concepts%2520of%2520graph%2520strong%2520collapse%2520and%2520graph%2520edge%2520collapse%2520extended%2520from%2520algebraic%2520topology.%2520STPGC%2520comprises%2520three%2520new%2520algorithms%252C%2520GStrongCollapse%252C%2520GEdgeCollapse%252C%2520and%2520NeighborhoodConing%2520based%2520on%2520these%2520two%2520concepts%252C%2520which%2520eliminate%2520dominated%2520nodes%2520and%2520edges%2520while%2520rigorously%2520preserving%2520topological%2520features.%2520We%2520further%2520prove%2520that%2520STPGC%2520preserves%2520the%2520GNN%2520receptive%2520field%2520and%2520develop%2520approximate%2520algorithms%2520to%2520accelerate%2520GNN%2520training.%2520Experiments%2520on%2520node%2520classification%2520with%2520GNNs%2520demonstrate%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520STPGC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Topology-Preserving%20Graph%20Coarsening%20with%20Graph%20Collapse&entry.906535625=Xiang%20Wu%20and%20Rong-Hua%20Li%20and%20Xunkai%20Li%20and%20Kangfei%20Zhao%20and%20Hongchao%20Qin%20and%20Guoren%20Wang&entry.1292438233=Graph%20coarsening%20reduces%20the%20size%20of%20a%20graph%20while%20preserving%20certain%20properties.%20Most%20existing%20methods%20preserve%20either%20spectral%20or%20spatial%20characteristics.%20Recent%20research%20has%20shown%20that%20preserving%20topological%20features%20helps%20maintain%20the%20predictive%20performance%20of%20graph%20neural%20networks%20%28GNNs%29%20trained%20on%20the%20coarsened%20graph%20but%20suffers%20from%20exponential%20time%20complexity.%20To%20address%20these%20problems%2C%20we%20propose%20Scalable%20Topology-Preserving%20Graph%20Coarsening%20%28STPGC%29%20by%20introducing%20the%20concepts%20of%20graph%20strong%20collapse%20and%20graph%20edge%20collapse%20extended%20from%20algebraic%20topology.%20STPGC%20comprises%20three%20new%20algorithms%2C%20GStrongCollapse%2C%20GEdgeCollapse%2C%20and%20NeighborhoodConing%20based%20on%20these%20two%20concepts%2C%20which%20eliminate%20dominated%20nodes%20and%20edges%20while%20rigorously%20preserving%20topological%20features.%20We%20further%20prove%20that%20STPGC%20preserves%20the%20GNN%20receptive%20field%20and%20develop%20approximate%20algorithms%20to%20accelerate%20GNN%20training.%20Experiments%20on%20node%20classification%20with%20GNNs%20demonstrate%20the%20efficiency%20and%20effectiveness%20of%20STPGC.&entry.1838667208=http%3A//arxiv.org/abs/2601.22943v1&entry.124074799=Read"},
{"title": "Vision-Language Controlled Deep Unfolding for Joint Medical Image Restoration and Segmentation", "author": "Ping Chen and Zicheng Huang and Xiangming Wang and Yungeng Liu and Bingyu Liang and Haijin Zeng and Yongyong Chen", "abstract": "We propose VL-DUN, a principled framework for joint All-in-One Medical Image Restoration and Segmentation (AiOMIRS) that bridges the gap between low-level signal recovery and high-level semantic understanding. While standard pipelines treat these tasks in isolation, our core insight is that they are fundamentally synergistic: restoration provides clean anatomical structures to improve segmentation, while semantic priors regularize the restoration process. VL-DUN resolves the sub-optimality of sequential processing through two primary innovations. (1) We formulate AiOMIRS as a unified optimization problem, deriving an interpretable joint unfolding mechanism where restoration and segmentation are mathematically coupled for mutual refinement. (2) We introduce a frequency-aware Mamba mechanism to capture long-range dependencies for global segmentation while preserving the high-frequency textures necessary for restoration. This allows for efficient global context modeling with linear complexity, effectively mitigating the spectral bias of standard architectures. As a pioneering work in the AiOMIRS task, VL-DUN establishes a new state-of-the-art across multi-modal benchmarks, improving PSNR by 0.92 dB and the Dice coefficient by 9.76\\%. Our results demonstrate that joint collaborative learning offers a superior, more robust solution for complex clinical workflows compared to isolated task processing. The codes are provided in https://github.com/cipi666/VLDUN.", "link": "http://arxiv.org/abs/2601.23103v1", "date": "2026-01-30", "relevancy": 2.2061, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.57}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Controlled%20Deep%20Unfolding%20for%20Joint%20Medical%20Image%20Restoration%20and%20Segmentation&body=Title%3A%20Vision-Language%20Controlled%20Deep%20Unfolding%20for%20Joint%20Medical%20Image%20Restoration%20and%20Segmentation%0AAuthor%3A%20Ping%20Chen%20and%20Zicheng%20Huang%20and%20Xiangming%20Wang%20and%20Yungeng%20Liu%20and%20Bingyu%20Liang%20and%20Haijin%20Zeng%20and%20Yongyong%20Chen%0AAbstract%3A%20We%20propose%20VL-DUN%2C%20a%20principled%20framework%20for%20joint%20All-in-One%20Medical%20Image%20Restoration%20and%20Segmentation%20%28AiOMIRS%29%20that%20bridges%20the%20gap%20between%20low-level%20signal%20recovery%20and%20high-level%20semantic%20understanding.%20While%20standard%20pipelines%20treat%20these%20tasks%20in%20isolation%2C%20our%20core%20insight%20is%20that%20they%20are%20fundamentally%20synergistic%3A%20restoration%20provides%20clean%20anatomical%20structures%20to%20improve%20segmentation%2C%20while%20semantic%20priors%20regularize%20the%20restoration%20process.%20VL-DUN%20resolves%20the%20sub-optimality%20of%20sequential%20processing%20through%20two%20primary%20innovations.%20%281%29%20We%20formulate%20AiOMIRS%20as%20a%20unified%20optimization%20problem%2C%20deriving%20an%20interpretable%20joint%20unfolding%20mechanism%20where%20restoration%20and%20segmentation%20are%20mathematically%20coupled%20for%20mutual%20refinement.%20%282%29%20We%20introduce%20a%20frequency-aware%20Mamba%20mechanism%20to%20capture%20long-range%20dependencies%20for%20global%20segmentation%20while%20preserving%20the%20high-frequency%20textures%20necessary%20for%20restoration.%20This%20allows%20for%20efficient%20global%20context%20modeling%20with%20linear%20complexity%2C%20effectively%20mitigating%20the%20spectral%20bias%20of%20standard%20architectures.%20As%20a%20pioneering%20work%20in%20the%20AiOMIRS%20task%2C%20VL-DUN%20establishes%20a%20new%20state-of-the-art%20across%20multi-modal%20benchmarks%2C%20improving%20PSNR%20by%200.92%20dB%20and%20the%20Dice%20coefficient%20by%209.76%5C%25.%20Our%20results%20demonstrate%20that%20joint%20collaborative%20learning%20offers%20a%20superior%2C%20more%20robust%20solution%20for%20complex%20clinical%20workflows%20compared%20to%20isolated%20task%20processing.%20The%20codes%20are%20provided%20in%20https%3A//github.com/cipi666/VLDUN.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Controlled%2520Deep%2520Unfolding%2520for%2520Joint%2520Medical%2520Image%2520Restoration%2520and%2520Segmentation%26entry.906535625%3DPing%2520Chen%2520and%2520Zicheng%2520Huang%2520and%2520Xiangming%2520Wang%2520and%2520Yungeng%2520Liu%2520and%2520Bingyu%2520Liang%2520and%2520Haijin%2520Zeng%2520and%2520Yongyong%2520Chen%26entry.1292438233%3DWe%2520propose%2520VL-DUN%252C%2520a%2520principled%2520framework%2520for%2520joint%2520All-in-One%2520Medical%2520Image%2520Restoration%2520and%2520Segmentation%2520%2528AiOMIRS%2529%2520that%2520bridges%2520the%2520gap%2520between%2520low-level%2520signal%2520recovery%2520and%2520high-level%2520semantic%2520understanding.%2520While%2520standard%2520pipelines%2520treat%2520these%2520tasks%2520in%2520isolation%252C%2520our%2520core%2520insight%2520is%2520that%2520they%2520are%2520fundamentally%2520synergistic%253A%2520restoration%2520provides%2520clean%2520anatomical%2520structures%2520to%2520improve%2520segmentation%252C%2520while%2520semantic%2520priors%2520regularize%2520the%2520restoration%2520process.%2520VL-DUN%2520resolves%2520the%2520sub-optimality%2520of%2520sequential%2520processing%2520through%2520two%2520primary%2520innovations.%2520%25281%2529%2520We%2520formulate%2520AiOMIRS%2520as%2520a%2520unified%2520optimization%2520problem%252C%2520deriving%2520an%2520interpretable%2520joint%2520unfolding%2520mechanism%2520where%2520restoration%2520and%2520segmentation%2520are%2520mathematically%2520coupled%2520for%2520mutual%2520refinement.%2520%25282%2529%2520We%2520introduce%2520a%2520frequency-aware%2520Mamba%2520mechanism%2520to%2520capture%2520long-range%2520dependencies%2520for%2520global%2520segmentation%2520while%2520preserving%2520the%2520high-frequency%2520textures%2520necessary%2520for%2520restoration.%2520This%2520allows%2520for%2520efficient%2520global%2520context%2520modeling%2520with%2520linear%2520complexity%252C%2520effectively%2520mitigating%2520the%2520spectral%2520bias%2520of%2520standard%2520architectures.%2520As%2520a%2520pioneering%2520work%2520in%2520the%2520AiOMIRS%2520task%252C%2520VL-DUN%2520establishes%2520a%2520new%2520state-of-the-art%2520across%2520multi-modal%2520benchmarks%252C%2520improving%2520PSNR%2520by%25200.92%2520dB%2520and%2520the%2520Dice%2520coefficient%2520by%25209.76%255C%2525.%2520Our%2520results%2520demonstrate%2520that%2520joint%2520collaborative%2520learning%2520offers%2520a%2520superior%252C%2520more%2520robust%2520solution%2520for%2520complex%2520clinical%2520workflows%2520compared%2520to%2520isolated%2520task%2520processing.%2520The%2520codes%2520are%2520provided%2520in%2520https%253A//github.com/cipi666/VLDUN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Controlled%20Deep%20Unfolding%20for%20Joint%20Medical%20Image%20Restoration%20and%20Segmentation&entry.906535625=Ping%20Chen%20and%20Zicheng%20Huang%20and%20Xiangming%20Wang%20and%20Yungeng%20Liu%20and%20Bingyu%20Liang%20and%20Haijin%20Zeng%20and%20Yongyong%20Chen&entry.1292438233=We%20propose%20VL-DUN%2C%20a%20principled%20framework%20for%20joint%20All-in-One%20Medical%20Image%20Restoration%20and%20Segmentation%20%28AiOMIRS%29%20that%20bridges%20the%20gap%20between%20low-level%20signal%20recovery%20and%20high-level%20semantic%20understanding.%20While%20standard%20pipelines%20treat%20these%20tasks%20in%20isolation%2C%20our%20core%20insight%20is%20that%20they%20are%20fundamentally%20synergistic%3A%20restoration%20provides%20clean%20anatomical%20structures%20to%20improve%20segmentation%2C%20while%20semantic%20priors%20regularize%20the%20restoration%20process.%20VL-DUN%20resolves%20the%20sub-optimality%20of%20sequential%20processing%20through%20two%20primary%20innovations.%20%281%29%20We%20formulate%20AiOMIRS%20as%20a%20unified%20optimization%20problem%2C%20deriving%20an%20interpretable%20joint%20unfolding%20mechanism%20where%20restoration%20and%20segmentation%20are%20mathematically%20coupled%20for%20mutual%20refinement.%20%282%29%20We%20introduce%20a%20frequency-aware%20Mamba%20mechanism%20to%20capture%20long-range%20dependencies%20for%20global%20segmentation%20while%20preserving%20the%20high-frequency%20textures%20necessary%20for%20restoration.%20This%20allows%20for%20efficient%20global%20context%20modeling%20with%20linear%20complexity%2C%20effectively%20mitigating%20the%20spectral%20bias%20of%20standard%20architectures.%20As%20a%20pioneering%20work%20in%20the%20AiOMIRS%20task%2C%20VL-DUN%20establishes%20a%20new%20state-of-the-art%20across%20multi-modal%20benchmarks%2C%20improving%20PSNR%20by%200.92%20dB%20and%20the%20Dice%20coefficient%20by%209.76%5C%25.%20Our%20results%20demonstrate%20that%20joint%20collaborative%20learning%20offers%20a%20superior%2C%20more%20robust%20solution%20for%20complex%20clinical%20workflows%20compared%20to%20isolated%20task%20processing.%20The%20codes%20are%20provided%20in%20https%3A//github.com/cipi666/VLDUN.&entry.1838667208=http%3A//arxiv.org/abs/2601.23103v1&entry.124074799=Read"},
{"title": "IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models", "author": "Seyed Ahmad Hosseini Miangoleh and Amin Jalal Aghdasian and Farzaneh Abdollahi", "abstract": "This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96\\% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available.", "link": "http://arxiv.org/abs/2601.23266v1", "date": "2026-01-30", "relevancy": 2.1981, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5665}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5533}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRL-DAL%3A%20Safe%20and%20Adaptive%20Trajectory%20Planning%20for%20Autonomous%20Driving%20via%20Energy-Guided%20Diffusion%20Models&body=Title%3A%20IRL-DAL%3A%20Safe%20and%20Adaptive%20Trajectory%20Planning%20for%20Autonomous%20Driving%20via%20Energy-Guided%20Diffusion%20Models%0AAuthor%3A%20Seyed%20Ahmad%20Hosseini%20Miangoleh%20and%20Amin%20Jalal%20Aghdasian%20and%20Farzaneh%20Abdollahi%0AAbstract%3A%20This%20paper%20proposes%20a%20novel%20inverse%20reinforcement%20learning%20framework%20using%20a%20diffusion-based%20adaptive%20lookahead%20planner%20%28IRL-DAL%29%20for%20autonomous%20vehicles.%20Training%20begins%20with%20imitation%20from%20an%20expert%20finite%20state%20machine%20%28FSM%29%20controller%20to%20provide%20a%20stable%20initialization.%20Environment%20terms%20are%20combined%20with%20an%20IRL%20discriminator%20signal%20to%20align%20with%20expert%20goals.%20Reinforcement%20learning%20%28RL%29%20is%20then%20performed%20with%20a%20hybrid%20reward%20that%20combines%20diffuse%20environmental%20feedback%20and%20targeted%20IRL%20rewards.%20A%20conditional%20diffusion%20model%2C%20which%20acts%20as%20a%20safety%20supervisor%2C%20plans%20safe%20paths.%20It%20stays%20in%20its%20lane%2C%20avoids%20obstacles%2C%20and%20moves%20smoothly.%20Then%2C%20a%20learnable%20adaptive%20mask%20%28LAM%29%20improves%20perception.%20It%20shifts%20visual%20attention%20based%20on%20vehicle%20speed%20and%20nearby%20hazards.%20After%20FSM-based%20imitation%2C%20the%20policy%20is%20fine-tuned%20with%20Proximal%20Policy%20Optimization%20%28PPO%29.%20Training%20is%20run%20in%20the%20Webots%20simulator%20with%20a%20two-stage%20curriculum.%20A%2096%5C%25%20success%20rate%20is%20reached%2C%20and%20collisions%20are%20reduced%20to%200.05%20per%201k%20steps%2C%20marking%20a%20new%20benchmark%20for%20safe%20navigation.%20By%20applying%20the%20proposed%20approach%2C%20the%20agent%20not%20only%20drives%20in%20lane%20but%20also%20handles%20unsafe%20conditions%20at%20an%20expert%20level%2C%20increasing%20robustness.We%20make%20our%20code%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRL-DAL%253A%2520Safe%2520and%2520Adaptive%2520Trajectory%2520Planning%2520for%2520Autonomous%2520Driving%2520via%2520Energy-Guided%2520Diffusion%2520Models%26entry.906535625%3DSeyed%2520Ahmad%2520Hosseini%2520Miangoleh%2520and%2520Amin%2520Jalal%2520Aghdasian%2520and%2520Farzaneh%2520Abdollahi%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%2520novel%2520inverse%2520reinforcement%2520learning%2520framework%2520using%2520a%2520diffusion-based%2520adaptive%2520lookahead%2520planner%2520%2528IRL-DAL%2529%2520for%2520autonomous%2520vehicles.%2520Training%2520begins%2520with%2520imitation%2520from%2520an%2520expert%2520finite%2520state%2520machine%2520%2528FSM%2529%2520controller%2520to%2520provide%2520a%2520stable%2520initialization.%2520Environment%2520terms%2520are%2520combined%2520with%2520an%2520IRL%2520discriminator%2520signal%2520to%2520align%2520with%2520expert%2520goals.%2520Reinforcement%2520learning%2520%2528RL%2529%2520is%2520then%2520performed%2520with%2520a%2520hybrid%2520reward%2520that%2520combines%2520diffuse%2520environmental%2520feedback%2520and%2520targeted%2520IRL%2520rewards.%2520A%2520conditional%2520diffusion%2520model%252C%2520which%2520acts%2520as%2520a%2520safety%2520supervisor%252C%2520plans%2520safe%2520paths.%2520It%2520stays%2520in%2520its%2520lane%252C%2520avoids%2520obstacles%252C%2520and%2520moves%2520smoothly.%2520Then%252C%2520a%2520learnable%2520adaptive%2520mask%2520%2528LAM%2529%2520improves%2520perception.%2520It%2520shifts%2520visual%2520attention%2520based%2520on%2520vehicle%2520speed%2520and%2520nearby%2520hazards.%2520After%2520FSM-based%2520imitation%252C%2520the%2520policy%2520is%2520fine-tuned%2520with%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529.%2520Training%2520is%2520run%2520in%2520the%2520Webots%2520simulator%2520with%2520a%2520two-stage%2520curriculum.%2520A%252096%255C%2525%2520success%2520rate%2520is%2520reached%252C%2520and%2520collisions%2520are%2520reduced%2520to%25200.05%2520per%25201k%2520steps%252C%2520marking%2520a%2520new%2520benchmark%2520for%2520safe%2520navigation.%2520By%2520applying%2520the%2520proposed%2520approach%252C%2520the%2520agent%2520not%2520only%2520drives%2520in%2520lane%2520but%2520also%2520handles%2520unsafe%2520conditions%2520at%2520an%2520expert%2520level%252C%2520increasing%2520robustness.We%2520make%2520our%2520code%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRL-DAL%3A%20Safe%20and%20Adaptive%20Trajectory%20Planning%20for%20Autonomous%20Driving%20via%20Energy-Guided%20Diffusion%20Models&entry.906535625=Seyed%20Ahmad%20Hosseini%20Miangoleh%20and%20Amin%20Jalal%20Aghdasian%20and%20Farzaneh%20Abdollahi&entry.1292438233=This%20paper%20proposes%20a%20novel%20inverse%20reinforcement%20learning%20framework%20using%20a%20diffusion-based%20adaptive%20lookahead%20planner%20%28IRL-DAL%29%20for%20autonomous%20vehicles.%20Training%20begins%20with%20imitation%20from%20an%20expert%20finite%20state%20machine%20%28FSM%29%20controller%20to%20provide%20a%20stable%20initialization.%20Environment%20terms%20are%20combined%20with%20an%20IRL%20discriminator%20signal%20to%20align%20with%20expert%20goals.%20Reinforcement%20learning%20%28RL%29%20is%20then%20performed%20with%20a%20hybrid%20reward%20that%20combines%20diffuse%20environmental%20feedback%20and%20targeted%20IRL%20rewards.%20A%20conditional%20diffusion%20model%2C%20which%20acts%20as%20a%20safety%20supervisor%2C%20plans%20safe%20paths.%20It%20stays%20in%20its%20lane%2C%20avoids%20obstacles%2C%20and%20moves%20smoothly.%20Then%2C%20a%20learnable%20adaptive%20mask%20%28LAM%29%20improves%20perception.%20It%20shifts%20visual%20attention%20based%20on%20vehicle%20speed%20and%20nearby%20hazards.%20After%20FSM-based%20imitation%2C%20the%20policy%20is%20fine-tuned%20with%20Proximal%20Policy%20Optimization%20%28PPO%29.%20Training%20is%20run%20in%20the%20Webots%20simulator%20with%20a%20two-stage%20curriculum.%20A%2096%5C%25%20success%20rate%20is%20reached%2C%20and%20collisions%20are%20reduced%20to%200.05%20per%201k%20steps%2C%20marking%20a%20new%20benchmark%20for%20safe%20navigation.%20By%20applying%20the%20proposed%20approach%2C%20the%20agent%20not%20only%20drives%20in%20lane%20but%20also%20handles%20unsafe%20conditions%20at%20an%20expert%20level%2C%20increasing%20robustness.We%20make%20our%20code%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.23266v1&entry.124074799=Read"},
{"title": "Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization", "author": "Hui Lu and Yi Yu and Yiming Yang and Chenyu Yi and Xueyi Ke and Qixing Zhang and Bingquan Shen and Alex Kot and Xudong Jiang", "abstract": "Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\\% on GPT-4o and +19.9\\% on Gemini-2.0 over the strongest universal baseline.", "link": "http://arxiv.org/abs/2601.23179v1", "date": "2026-01-30", "relevancy": 2.1926, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5666}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5415}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Make%20Anything%20Match%20Your%20Target%3A%20Universal%20Adversarial%20Perturbations%20against%20Closed-Source%20MLLMs%20via%20Multi-Crop%20Routed%20Meta%20Optimization&body=Title%3A%20Make%20Anything%20Match%20Your%20Target%3A%20Universal%20Adversarial%20Perturbations%20against%20Closed-Source%20MLLMs%20via%20Multi-Crop%20Routed%20Meta%20Optimization%0AAuthor%3A%20Hui%20Lu%20and%20Yi%20Yu%20and%20Yiming%20Yang%20and%20Chenyu%20Yi%20and%20Xueyi%20Ke%20and%20Qixing%20Zhang%20and%20Bingquan%20Shen%20and%20Alex%20Kot%20and%20Xudong%20Jiang%0AAbstract%3A%20Targeted%20adversarial%20attacks%20on%20closed-source%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20been%20increasingly%20explored%20under%20black-box%20transfer%2C%20yet%20prior%20methods%20are%20predominantly%20sample-specific%20and%20offer%20limited%20reusability%20across%20inputs.%20We%20instead%20study%20a%20more%20stringent%20setting%2C%20Universal%20Targeted%20Transferable%20Adversarial%20Attacks%20%28UTTAA%29%2C%20where%20a%20single%20perturbation%20must%20consistently%20steer%20arbitrary%20inputs%20toward%20a%20specified%20target%20across%20unknown%20commercial%20MLLMs.%20Naively%20adapting%20existing%20sample-wise%20attacks%20to%20this%20universal%20setting%20faces%20three%20core%20difficulties%3A%20%28i%29%20target%20supervision%20becomes%20high-variance%20due%20to%20target-crop%20randomness%2C%20%28ii%29%20token-wise%20matching%20is%20unreliable%20because%20universality%20suppresses%20image-specific%20cues%20that%20would%20otherwise%20anchor%20alignment%2C%20and%20%28iii%29%20few-source%20per-target%20adaptation%20is%20highly%20initialization-sensitive%2C%20which%20can%20degrade%20the%20attainable%20performance.%20In%20this%20work%2C%20we%20propose%20MCRMO-Attack%2C%20which%20stabilizes%20supervision%20via%20Multi-Crop%20Aggregation%20with%20an%20Attention-Guided%20Crop%2C%20improves%20token-level%20reliability%20through%20alignability-gated%20Token%20Routing%2C%20and%20meta-learns%20a%20cross-target%20perturbation%20prior%20that%20yields%20stronger%20per-target%20solutions.%20Across%20commercial%20MLLMs%2C%20we%20boost%20unseen-image%20attack%20success%20rate%20by%20%2B23.7%5C%25%20on%20GPT-4o%20and%20%2B19.9%5C%25%20on%20Gemini-2.0%20over%20the%20strongest%20universal%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMake%2520Anything%2520Match%2520Your%2520Target%253A%2520Universal%2520Adversarial%2520Perturbations%2520against%2520Closed-Source%2520MLLMs%2520via%2520Multi-Crop%2520Routed%2520Meta%2520Optimization%26entry.906535625%3DHui%2520Lu%2520and%2520Yi%2520Yu%2520and%2520Yiming%2520Yang%2520and%2520Chenyu%2520Yi%2520and%2520Xueyi%2520Ke%2520and%2520Qixing%2520Zhang%2520and%2520Bingquan%2520Shen%2520and%2520Alex%2520Kot%2520and%2520Xudong%2520Jiang%26entry.1292438233%3DTargeted%2520adversarial%2520attacks%2520on%2520closed-source%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520been%2520increasingly%2520explored%2520under%2520black-box%2520transfer%252C%2520yet%2520prior%2520methods%2520are%2520predominantly%2520sample-specific%2520and%2520offer%2520limited%2520reusability%2520across%2520inputs.%2520We%2520instead%2520study%2520a%2520more%2520stringent%2520setting%252C%2520Universal%2520Targeted%2520Transferable%2520Adversarial%2520Attacks%2520%2528UTTAA%2529%252C%2520where%2520a%2520single%2520perturbation%2520must%2520consistently%2520steer%2520arbitrary%2520inputs%2520toward%2520a%2520specified%2520target%2520across%2520unknown%2520commercial%2520MLLMs.%2520Naively%2520adapting%2520existing%2520sample-wise%2520attacks%2520to%2520this%2520universal%2520setting%2520faces%2520three%2520core%2520difficulties%253A%2520%2528i%2529%2520target%2520supervision%2520becomes%2520high-variance%2520due%2520to%2520target-crop%2520randomness%252C%2520%2528ii%2529%2520token-wise%2520matching%2520is%2520unreliable%2520because%2520universality%2520suppresses%2520image-specific%2520cues%2520that%2520would%2520otherwise%2520anchor%2520alignment%252C%2520and%2520%2528iii%2529%2520few-source%2520per-target%2520adaptation%2520is%2520highly%2520initialization-sensitive%252C%2520which%2520can%2520degrade%2520the%2520attainable%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520MCRMO-Attack%252C%2520which%2520stabilizes%2520supervision%2520via%2520Multi-Crop%2520Aggregation%2520with%2520an%2520Attention-Guided%2520Crop%252C%2520improves%2520token-level%2520reliability%2520through%2520alignability-gated%2520Token%2520Routing%252C%2520and%2520meta-learns%2520a%2520cross-target%2520perturbation%2520prior%2520that%2520yields%2520stronger%2520per-target%2520solutions.%2520Across%2520commercial%2520MLLMs%252C%2520we%2520boost%2520unseen-image%2520attack%2520success%2520rate%2520by%2520%252B23.7%255C%2525%2520on%2520GPT-4o%2520and%2520%252B19.9%255C%2525%2520on%2520Gemini-2.0%2520over%2520the%2520strongest%2520universal%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Make%20Anything%20Match%20Your%20Target%3A%20Universal%20Adversarial%20Perturbations%20against%20Closed-Source%20MLLMs%20via%20Multi-Crop%20Routed%20Meta%20Optimization&entry.906535625=Hui%20Lu%20and%20Yi%20Yu%20and%20Yiming%20Yang%20and%20Chenyu%20Yi%20and%20Xueyi%20Ke%20and%20Qixing%20Zhang%20and%20Bingquan%20Shen%20and%20Alex%20Kot%20and%20Xudong%20Jiang&entry.1292438233=Targeted%20adversarial%20attacks%20on%20closed-source%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20been%20increasingly%20explored%20under%20black-box%20transfer%2C%20yet%20prior%20methods%20are%20predominantly%20sample-specific%20and%20offer%20limited%20reusability%20across%20inputs.%20We%20instead%20study%20a%20more%20stringent%20setting%2C%20Universal%20Targeted%20Transferable%20Adversarial%20Attacks%20%28UTTAA%29%2C%20where%20a%20single%20perturbation%20must%20consistently%20steer%20arbitrary%20inputs%20toward%20a%20specified%20target%20across%20unknown%20commercial%20MLLMs.%20Naively%20adapting%20existing%20sample-wise%20attacks%20to%20this%20universal%20setting%20faces%20three%20core%20difficulties%3A%20%28i%29%20target%20supervision%20becomes%20high-variance%20due%20to%20target-crop%20randomness%2C%20%28ii%29%20token-wise%20matching%20is%20unreliable%20because%20universality%20suppresses%20image-specific%20cues%20that%20would%20otherwise%20anchor%20alignment%2C%20and%20%28iii%29%20few-source%20per-target%20adaptation%20is%20highly%20initialization-sensitive%2C%20which%20can%20degrade%20the%20attainable%20performance.%20In%20this%20work%2C%20we%20propose%20MCRMO-Attack%2C%20which%20stabilizes%20supervision%20via%20Multi-Crop%20Aggregation%20with%20an%20Attention-Guided%20Crop%2C%20improves%20token-level%20reliability%20through%20alignability-gated%20Token%20Routing%2C%20and%20meta-learns%20a%20cross-target%20perturbation%20prior%20that%20yields%20stronger%20per-target%20solutions.%20Across%20commercial%20MLLMs%2C%20we%20boost%20unseen-image%20attack%20success%20rate%20by%20%2B23.7%5C%25%20on%20GPT-4o%20and%20%2B19.9%5C%25%20on%20Gemini-2.0%20over%20the%20strongest%20universal%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2601.23179v1&entry.124074799=Read"},
{"title": "Multi-Cue Anomaly Detection and Localization under Data Contamination", "author": "Anindya Sundar Das and Monowar Bhuyan", "abstract": "Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.", "link": "http://arxiv.org/abs/2601.22913v1", "date": "2026-01-30", "relevancy": 2.1915, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5656}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Cue%20Anomaly%20Detection%20and%20Localization%20under%20Data%20Contamination&body=Title%3A%20Multi-Cue%20Anomaly%20Detection%20and%20Localization%20under%20Data%20Contamination%0AAuthor%3A%20Anindya%20Sundar%20Das%20and%20Monowar%20Bhuyan%0AAbstract%3A%20Visual%20anomaly%20detection%20in%20real-world%20industrial%20settings%20faces%20two%20major%20limitations.%20First%2C%20most%20existing%20methods%20are%20trained%20on%20purely%20normal%20data%20or%20on%20unlabeled%20datasets%20assumed%20to%20be%20predominantly%20normal%2C%20presuming%20the%20absence%20of%20contamination%2C%20an%20assumption%20that%20is%20rarely%20satisfied%20in%20practice.%20Second%2C%20they%20assume%20no%20access%20to%20labeled%20anomaly%20samples%2C%20limiting%20the%20model%20from%20learning%20discriminative%20characteristics%20of%20true%20anomalies.%20Therefore%2C%20these%20approaches%20often%20struggle%20to%20distinguish%20anomalies%20from%20normal%20instances%2C%20resulting%20in%20reduced%20detection%20and%20weak%20localization%20performance.%20In%20real-world%20applications%2C%20where%20training%20data%20are%20frequently%20contaminated%20with%20anomalies%2C%20such%20methods%20fail%20to%20deliver%20reliable%20performance.%20In%20this%20work%2C%20we%20propose%20a%20robust%20anomaly%20detection%20framework%20that%20integrates%20limited%20anomaly%20supervision%20into%20the%20adaptive%20deviation%20learning%20paradigm.%20We%20introduce%20a%20composite%20anomaly%20score%20that%20combines%20three%20complementary%20components%3A%20a%20deviation%20score%20capturing%20statistical%20irregularity%2C%20an%20entropy-based%20uncertainty%20score%20reflecting%20predictive%20inconsistency%2C%20and%20a%20segmentation-based%20score%20highlighting%20spatial%20abnormality.%20This%20unified%20scoring%20mechanism%20enables%20accurate%20detection%20and%20supports%20gradient-based%20localization%2C%20providing%20intuitive%20and%20explainable%20visual%20evidence%20of%20anomalous%20regions.%20Following%20the%20few-anomaly%20paradigm%2C%20we%20incorporate%20a%20small%20set%20of%20labeled%20anomalies%20during%20training%20while%20simultaneously%20mitigating%20the%20influence%20of%20contaminated%20samples%20through%20adaptive%20instance%20weighting.%20Extensive%20experiments%20on%20the%20MVTec%20and%20VisA%20benchmarks%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20baselines%20and%20achieves%20strong%20detection%20and%20localization%20performance%2C%20interpretability%2C%20and%20robustness%20under%20various%20levels%20of%20data%20contamination.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Cue%2520Anomaly%2520Detection%2520and%2520Localization%2520under%2520Data%2520Contamination%26entry.906535625%3DAnindya%2520Sundar%2520Das%2520and%2520Monowar%2520Bhuyan%26entry.1292438233%3DVisual%2520anomaly%2520detection%2520in%2520real-world%2520industrial%2520settings%2520faces%2520two%2520major%2520limitations.%2520First%252C%2520most%2520existing%2520methods%2520are%2520trained%2520on%2520purely%2520normal%2520data%2520or%2520on%2520unlabeled%2520datasets%2520assumed%2520to%2520be%2520predominantly%2520normal%252C%2520presuming%2520the%2520absence%2520of%2520contamination%252C%2520an%2520assumption%2520that%2520is%2520rarely%2520satisfied%2520in%2520practice.%2520Second%252C%2520they%2520assume%2520no%2520access%2520to%2520labeled%2520anomaly%2520samples%252C%2520limiting%2520the%2520model%2520from%2520learning%2520discriminative%2520characteristics%2520of%2520true%2520anomalies.%2520Therefore%252C%2520these%2520approaches%2520often%2520struggle%2520to%2520distinguish%2520anomalies%2520from%2520normal%2520instances%252C%2520resulting%2520in%2520reduced%2520detection%2520and%2520weak%2520localization%2520performance.%2520In%2520real-world%2520applications%252C%2520where%2520training%2520data%2520are%2520frequently%2520contaminated%2520with%2520anomalies%252C%2520such%2520methods%2520fail%2520to%2520deliver%2520reliable%2520performance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520robust%2520anomaly%2520detection%2520framework%2520that%2520integrates%2520limited%2520anomaly%2520supervision%2520into%2520the%2520adaptive%2520deviation%2520learning%2520paradigm.%2520We%2520introduce%2520a%2520composite%2520anomaly%2520score%2520that%2520combines%2520three%2520complementary%2520components%253A%2520a%2520deviation%2520score%2520capturing%2520statistical%2520irregularity%252C%2520an%2520entropy-based%2520uncertainty%2520score%2520reflecting%2520predictive%2520inconsistency%252C%2520and%2520a%2520segmentation-based%2520score%2520highlighting%2520spatial%2520abnormality.%2520This%2520unified%2520scoring%2520mechanism%2520enables%2520accurate%2520detection%2520and%2520supports%2520gradient-based%2520localization%252C%2520providing%2520intuitive%2520and%2520explainable%2520visual%2520evidence%2520of%2520anomalous%2520regions.%2520Following%2520the%2520few-anomaly%2520paradigm%252C%2520we%2520incorporate%2520a%2520small%2520set%2520of%2520labeled%2520anomalies%2520during%2520training%2520while%2520simultaneously%2520mitigating%2520the%2520influence%2520of%2520contaminated%2520samples%2520through%2520adaptive%2520instance%2520weighting.%2520Extensive%2520experiments%2520on%2520the%2520MVTec%2520and%2520VisA%2520benchmarks%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520state-of-the-art%2520baselines%2520and%2520achieves%2520strong%2520detection%2520and%2520localization%2520performance%252C%2520interpretability%252C%2520and%2520robustness%2520under%2520various%2520levels%2520of%2520data%2520contamination.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Cue%20Anomaly%20Detection%20and%20Localization%20under%20Data%20Contamination&entry.906535625=Anindya%20Sundar%20Das%20and%20Monowar%20Bhuyan&entry.1292438233=Visual%20anomaly%20detection%20in%20real-world%20industrial%20settings%20faces%20two%20major%20limitations.%20First%2C%20most%20existing%20methods%20are%20trained%20on%20purely%20normal%20data%20or%20on%20unlabeled%20datasets%20assumed%20to%20be%20predominantly%20normal%2C%20presuming%20the%20absence%20of%20contamination%2C%20an%20assumption%20that%20is%20rarely%20satisfied%20in%20practice.%20Second%2C%20they%20assume%20no%20access%20to%20labeled%20anomaly%20samples%2C%20limiting%20the%20model%20from%20learning%20discriminative%20characteristics%20of%20true%20anomalies.%20Therefore%2C%20these%20approaches%20often%20struggle%20to%20distinguish%20anomalies%20from%20normal%20instances%2C%20resulting%20in%20reduced%20detection%20and%20weak%20localization%20performance.%20In%20real-world%20applications%2C%20where%20training%20data%20are%20frequently%20contaminated%20with%20anomalies%2C%20such%20methods%20fail%20to%20deliver%20reliable%20performance.%20In%20this%20work%2C%20we%20propose%20a%20robust%20anomaly%20detection%20framework%20that%20integrates%20limited%20anomaly%20supervision%20into%20the%20adaptive%20deviation%20learning%20paradigm.%20We%20introduce%20a%20composite%20anomaly%20score%20that%20combines%20three%20complementary%20components%3A%20a%20deviation%20score%20capturing%20statistical%20irregularity%2C%20an%20entropy-based%20uncertainty%20score%20reflecting%20predictive%20inconsistency%2C%20and%20a%20segmentation-based%20score%20highlighting%20spatial%20abnormality.%20This%20unified%20scoring%20mechanism%20enables%20accurate%20detection%20and%20supports%20gradient-based%20localization%2C%20providing%20intuitive%20and%20explainable%20visual%20evidence%20of%20anomalous%20regions.%20Following%20the%20few-anomaly%20paradigm%2C%20we%20incorporate%20a%20small%20set%20of%20labeled%20anomalies%20during%20training%20while%20simultaneously%20mitigating%20the%20influence%20of%20contaminated%20samples%20through%20adaptive%20instance%20weighting.%20Extensive%20experiments%20on%20the%20MVTec%20and%20VisA%20benchmarks%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20baselines%20and%20achieves%20strong%20detection%20and%20localization%20performance%2C%20interpretability%2C%20and%20robustness%20under%20various%20levels%20of%20data%20contamination.&entry.1838667208=http%3A//arxiv.org/abs/2601.22913v1&entry.124074799=Read"},
{"title": "Learning to Execute Graph Algorithms Exactly with Graph Neural Networks", "author": "Muhammad Fetrat Qharabagh and Artur Back de Luca and George Giapitzakis and Kimon Fountoulakis", "abstract": "Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.", "link": "http://arxiv.org/abs/2601.23207v1", "date": "2026-01-30", "relevancy": 2.1876, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4343}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Execute%20Graph%20Algorithms%20Exactly%20with%20Graph%20Neural%20Networks&body=Title%3A%20Learning%20to%20Execute%20Graph%20Algorithms%20Exactly%20with%20Graph%20Neural%20Networks%0AAuthor%3A%20Muhammad%20Fetrat%20Qharabagh%20and%20Artur%20Back%20de%20Luca%20and%20George%20Giapitzakis%20and%20Kimon%20Fountoulakis%0AAbstract%3A%20Understanding%20what%20graph%20neural%20networks%20can%20learn%2C%20especially%20their%20ability%20to%20learn%20to%20execute%20algorithms%2C%20remains%20a%20central%20theoretical%20challenge.%20In%20this%20work%2C%20we%20prove%20exact%20learnability%20results%20for%20graph%20algorithms%20under%20bounded-degree%20and%20finite-precision%20constraints.%20Our%20approach%20follows%20a%20two-step%20process.%20First%2C%20we%20train%20an%20ensemble%20of%20multi-layer%20perceptrons%20%28MLPs%29%20to%20execute%20the%20local%20instructions%20of%20a%20single%20node.%20Second%2C%20during%20inference%2C%20we%20use%20the%20trained%20MLP%20ensemble%20as%20the%20update%20function%20within%20a%20graph%20neural%20network%20%28GNN%29.%20Leveraging%20Neural%20Tangent%20Kernel%20%28NTK%29%20theory%2C%20we%20show%20that%20local%20instructions%20can%20be%20learned%20from%20a%20small%20training%20set%2C%20enabling%20the%20complete%20graph%20algorithm%20to%20be%20executed%20during%20inference%20without%20error%20and%20with%20high%20probability.%20To%20illustrate%20the%20learning%20power%20of%20our%20setting%2C%20we%20establish%20a%20rigorous%20learnability%20result%20for%20the%20LOCAL%20model%20of%20distributed%20computation.%20We%20further%20demonstrate%20positive%20learnability%20results%20for%20widely%20studied%20algorithms%20such%20as%20message%20flooding%2C%20breadth-first%20and%20depth-first%20search%2C%20and%20Bellman-Ford.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Execute%2520Graph%2520Algorithms%2520Exactly%2520with%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMuhammad%2520Fetrat%2520Qharabagh%2520and%2520Artur%2520Back%2520de%2520Luca%2520and%2520George%2520Giapitzakis%2520and%2520Kimon%2520Fountoulakis%26entry.1292438233%3DUnderstanding%2520what%2520graph%2520neural%2520networks%2520can%2520learn%252C%2520especially%2520their%2520ability%2520to%2520learn%2520to%2520execute%2520algorithms%252C%2520remains%2520a%2520central%2520theoretical%2520challenge.%2520In%2520this%2520work%252C%2520we%2520prove%2520exact%2520learnability%2520results%2520for%2520graph%2520algorithms%2520under%2520bounded-degree%2520and%2520finite-precision%2520constraints.%2520Our%2520approach%2520follows%2520a%2520two-step%2520process.%2520First%252C%2520we%2520train%2520an%2520ensemble%2520of%2520multi-layer%2520perceptrons%2520%2528MLPs%2529%2520to%2520execute%2520the%2520local%2520instructions%2520of%2520a%2520single%2520node.%2520Second%252C%2520during%2520inference%252C%2520we%2520use%2520the%2520trained%2520MLP%2520ensemble%2520as%2520the%2520update%2520function%2520within%2520a%2520graph%2520neural%2520network%2520%2528GNN%2529.%2520Leveraging%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529%2520theory%252C%2520we%2520show%2520that%2520local%2520instructions%2520can%2520be%2520learned%2520from%2520a%2520small%2520training%2520set%252C%2520enabling%2520the%2520complete%2520graph%2520algorithm%2520to%2520be%2520executed%2520during%2520inference%2520without%2520error%2520and%2520with%2520high%2520probability.%2520To%2520illustrate%2520the%2520learning%2520power%2520of%2520our%2520setting%252C%2520we%2520establish%2520a%2520rigorous%2520learnability%2520result%2520for%2520the%2520LOCAL%2520model%2520of%2520distributed%2520computation.%2520We%2520further%2520demonstrate%2520positive%2520learnability%2520results%2520for%2520widely%2520studied%2520algorithms%2520such%2520as%2520message%2520flooding%252C%2520breadth-first%2520and%2520depth-first%2520search%252C%2520and%2520Bellman-Ford.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Execute%20Graph%20Algorithms%20Exactly%20with%20Graph%20Neural%20Networks&entry.906535625=Muhammad%20Fetrat%20Qharabagh%20and%20Artur%20Back%20de%20Luca%20and%20George%20Giapitzakis%20and%20Kimon%20Fountoulakis&entry.1292438233=Understanding%20what%20graph%20neural%20networks%20can%20learn%2C%20especially%20their%20ability%20to%20learn%20to%20execute%20algorithms%2C%20remains%20a%20central%20theoretical%20challenge.%20In%20this%20work%2C%20we%20prove%20exact%20learnability%20results%20for%20graph%20algorithms%20under%20bounded-degree%20and%20finite-precision%20constraints.%20Our%20approach%20follows%20a%20two-step%20process.%20First%2C%20we%20train%20an%20ensemble%20of%20multi-layer%20perceptrons%20%28MLPs%29%20to%20execute%20the%20local%20instructions%20of%20a%20single%20node.%20Second%2C%20during%20inference%2C%20we%20use%20the%20trained%20MLP%20ensemble%20as%20the%20update%20function%20within%20a%20graph%20neural%20network%20%28GNN%29.%20Leveraging%20Neural%20Tangent%20Kernel%20%28NTK%29%20theory%2C%20we%20show%20that%20local%20instructions%20can%20be%20learned%20from%20a%20small%20training%20set%2C%20enabling%20the%20complete%20graph%20algorithm%20to%20be%20executed%20during%20inference%20without%20error%20and%20with%20high%20probability.%20To%20illustrate%20the%20learning%20power%20of%20our%20setting%2C%20we%20establish%20a%20rigorous%20learnability%20result%20for%20the%20LOCAL%20model%20of%20distributed%20computation.%20We%20further%20demonstrate%20positive%20learnability%20results%20for%20widely%20studied%20algorithms%20such%20as%20message%20flooding%2C%20breadth-first%20and%20depth-first%20search%2C%20and%20Bellman-Ford.&entry.1838667208=http%3A//arxiv.org/abs/2601.23207v1&entry.124074799=Read"},
{"title": "Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG", "author": "Blagoj Hristov and Zoran Hadzi-Velkov and Katerina Hadzi-Velkova Saneva and Gorjan Nadzinski and Vesna Ojleska Latkoska", "abstract": "Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\\pm$ 3.1% to 92.3% $\\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.", "link": "http://arxiv.org/abs/2601.23011v1", "date": "2026-01-30", "relevancy": 2.1867, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5715}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5479}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Convolutional%20Sparse%20Autoencoders%20for%20Robust%20Movement%20Classification%20from%20Low-Density%20sEMG&body=Title%3A%20Leveraging%20Convolutional%20Sparse%20Autoencoders%20for%20Robust%20Movement%20Classification%20from%20Low-Density%20sEMG%0AAuthor%3A%20Blagoj%20Hristov%20and%20Zoran%20Hadzi-Velkov%20and%20Katerina%20Hadzi-Velkova%20Saneva%20and%20Gorjan%20Nadzinski%20and%20Vesna%20Ojleska%20Latkoska%0AAbstract%3A%20Reliable%20control%20of%20myoelectric%20prostheses%20is%20often%20hindered%20by%20high%20inter-subject%20variability%20and%20the%20clinical%20impracticality%20of%20high-density%20sensor%20arrays.%20This%20study%20proposes%20a%20deep%20learning%20framework%20for%20accurate%20gesture%20recognition%20using%20only%20two%20surface%20electromyography%20%28sEMG%29%20channels.%20The%20method%20employs%20a%20Convolutional%20Sparse%20Autoencoder%20%28CSAE%29%20to%20extract%20temporal%20feature%20representations%20directly%20from%20raw%20signals%2C%20eliminating%20the%20need%20for%20heuristic%20feature%20engineering.%20On%20a%206-class%20gesture%20set%2C%20our%20model%20achieved%20a%20multi-subject%20F1-score%20of%2094.3%25%20%24%5Cpm%24%200.3%25.%20To%20address%20subject-specific%20differences%2C%20we%20present%20a%20few-shot%20transfer%20learning%20protocol%20that%20improved%20performance%20on%20unseen%20subjects%20from%20a%20baseline%20of%2035.1%25%20%24%5Cpm%24%203.1%25%20to%2092.3%25%20%24%5Cpm%24%200.9%25%20with%20minimal%20calibration%20data.%20Furthermore%2C%20the%20system%20supports%20functional%20extensibility%20through%20an%20incremental%20learning%20strategy%2C%20allowing%20for%20expansion%20to%20a%2010-class%20set%20with%20a%2090.0%25%20%24%5Cpm%24%200.2%25%20F1-score%20without%20full%20model%20retraining.%20By%20combining%20high%20precision%20with%20minimal%20computational%20and%20sensor%20overhead%2C%20this%20framework%20provides%20a%20scalable%20and%20efficient%20approach%20for%20the%20next%20generation%20of%20affordable%20and%20adaptive%20prosthetic%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Convolutional%2520Sparse%2520Autoencoders%2520for%2520Robust%2520Movement%2520Classification%2520from%2520Low-Density%2520sEMG%26entry.906535625%3DBlagoj%2520Hristov%2520and%2520Zoran%2520Hadzi-Velkov%2520and%2520Katerina%2520Hadzi-Velkova%2520Saneva%2520and%2520Gorjan%2520Nadzinski%2520and%2520Vesna%2520Ojleska%2520Latkoska%26entry.1292438233%3DReliable%2520control%2520of%2520myoelectric%2520prostheses%2520is%2520often%2520hindered%2520by%2520high%2520inter-subject%2520variability%2520and%2520the%2520clinical%2520impracticality%2520of%2520high-density%2520sensor%2520arrays.%2520This%2520study%2520proposes%2520a%2520deep%2520learning%2520framework%2520for%2520accurate%2520gesture%2520recognition%2520using%2520only%2520two%2520surface%2520electromyography%2520%2528sEMG%2529%2520channels.%2520The%2520method%2520employs%2520a%2520Convolutional%2520Sparse%2520Autoencoder%2520%2528CSAE%2529%2520to%2520extract%2520temporal%2520feature%2520representations%2520directly%2520from%2520raw%2520signals%252C%2520eliminating%2520the%2520need%2520for%2520heuristic%2520feature%2520engineering.%2520On%2520a%25206-class%2520gesture%2520set%252C%2520our%2520model%2520achieved%2520a%2520multi-subject%2520F1-score%2520of%252094.3%2525%2520%2524%255Cpm%2524%25200.3%2525.%2520To%2520address%2520subject-specific%2520differences%252C%2520we%2520present%2520a%2520few-shot%2520transfer%2520learning%2520protocol%2520that%2520improved%2520performance%2520on%2520unseen%2520subjects%2520from%2520a%2520baseline%2520of%252035.1%2525%2520%2524%255Cpm%2524%25203.1%2525%2520to%252092.3%2525%2520%2524%255Cpm%2524%25200.9%2525%2520with%2520minimal%2520calibration%2520data.%2520Furthermore%252C%2520the%2520system%2520supports%2520functional%2520extensibility%2520through%2520an%2520incremental%2520learning%2520strategy%252C%2520allowing%2520for%2520expansion%2520to%2520a%252010-class%2520set%2520with%2520a%252090.0%2525%2520%2524%255Cpm%2524%25200.2%2525%2520F1-score%2520without%2520full%2520model%2520retraining.%2520By%2520combining%2520high%2520precision%2520with%2520minimal%2520computational%2520and%2520sensor%2520overhead%252C%2520this%2520framework%2520provides%2520a%2520scalable%2520and%2520efficient%2520approach%2520for%2520the%2520next%2520generation%2520of%2520affordable%2520and%2520adaptive%2520prosthetic%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Convolutional%20Sparse%20Autoencoders%20for%20Robust%20Movement%20Classification%20from%20Low-Density%20sEMG&entry.906535625=Blagoj%20Hristov%20and%20Zoran%20Hadzi-Velkov%20and%20Katerina%20Hadzi-Velkova%20Saneva%20and%20Gorjan%20Nadzinski%20and%20Vesna%20Ojleska%20Latkoska&entry.1292438233=Reliable%20control%20of%20myoelectric%20prostheses%20is%20often%20hindered%20by%20high%20inter-subject%20variability%20and%20the%20clinical%20impracticality%20of%20high-density%20sensor%20arrays.%20This%20study%20proposes%20a%20deep%20learning%20framework%20for%20accurate%20gesture%20recognition%20using%20only%20two%20surface%20electromyography%20%28sEMG%29%20channels.%20The%20method%20employs%20a%20Convolutional%20Sparse%20Autoencoder%20%28CSAE%29%20to%20extract%20temporal%20feature%20representations%20directly%20from%20raw%20signals%2C%20eliminating%20the%20need%20for%20heuristic%20feature%20engineering.%20On%20a%206-class%20gesture%20set%2C%20our%20model%20achieved%20a%20multi-subject%20F1-score%20of%2094.3%25%20%24%5Cpm%24%200.3%25.%20To%20address%20subject-specific%20differences%2C%20we%20present%20a%20few-shot%20transfer%20learning%20protocol%20that%20improved%20performance%20on%20unseen%20subjects%20from%20a%20baseline%20of%2035.1%25%20%24%5Cpm%24%203.1%25%20to%2092.3%25%20%24%5Cpm%24%200.9%25%20with%20minimal%20calibration%20data.%20Furthermore%2C%20the%20system%20supports%20functional%20extensibility%20through%20an%20incremental%20learning%20strategy%2C%20allowing%20for%20expansion%20to%20a%2010-class%20set%20with%20a%2090.0%25%20%24%5Cpm%24%200.2%25%20F1-score%20without%20full%20model%20retraining.%20By%20combining%20high%20precision%20with%20minimal%20computational%20and%20sensor%20overhead%2C%20this%20framework%20provides%20a%20scalable%20and%20efficient%20approach%20for%20the%20next%20generation%20of%20affordable%20and%20adaptive%20prosthetic%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.23011v1&entry.124074799=Read"},
{"title": "Q-Hawkeye: Reliable Visual Policy Optimization for Image Quality Assessment", "author": "Wulin Xie and Rui Dai and Ruidong Ding and Kaikui Liu and Xiangxiang Chu and Xinwen Hou and Jie Wen", "abstract": "Image Quality Assessment (IQA) predicts perceptual quality scores consistent with human judgments. Recent RL-based IQA methods built on MLLMs focus on generating visual quality descriptions and scores, ignoring two key reliability limitations: (i) although the model's prediction stability varies significantly across training samples, existing GRPO-based methods apply uniform advantage weighting, thereby amplifying noisy signals from unstable samples in gradient updates; (ii) most works emphasize text-grounded reasoning over images while overlooking the model's visual perception ability of image content. In this paper, we propose Q-Hawkeye, an RL-based reliable visual policy optimization framework that redesigns the learning signal through unified Uncertainty-Aware Dynamic Optimization and Perception-Aware Optimization. Q-Hawkeye estimates predictive uncertainty using the variance of predicted scores across multiple rollouts and leverages this uncertainty to reweight each sample's update strength, stabilizing policy optimization. To strengthen perceptual reliability, we construct paired inputs of degraded images and their original images and introduce an Implicit Perception Loss that constrains the model to ground its quality judgments in genuine visual evidence. Extensive experiments demonstrate that Q-Hawkeye outperforms state-of-the-art methods and generalizes better across multiple datasets. The code and models will be made available.", "link": "http://arxiv.org/abs/2601.22920v1", "date": "2026-01-30", "relevancy": 2.1757, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5604}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-Hawkeye%3A%20Reliable%20Visual%20Policy%20Optimization%20for%20Image%20Quality%20Assessment&body=Title%3A%20Q-Hawkeye%3A%20Reliable%20Visual%20Policy%20Optimization%20for%20Image%20Quality%20Assessment%0AAuthor%3A%20Wulin%20Xie%20and%20Rui%20Dai%20and%20Ruidong%20Ding%20and%20Kaikui%20Liu%20and%20Xiangxiang%20Chu%20and%20Xinwen%20Hou%20and%20Jie%20Wen%0AAbstract%3A%20Image%20Quality%20Assessment%20%28IQA%29%20predicts%20perceptual%20quality%20scores%20consistent%20with%20human%20judgments.%20Recent%20RL-based%20IQA%20methods%20built%20on%20MLLMs%20focus%20on%20generating%20visual%20quality%20descriptions%20and%20scores%2C%20ignoring%20two%20key%20reliability%20limitations%3A%20%28i%29%20although%20the%20model%27s%20prediction%20stability%20varies%20significantly%20across%20training%20samples%2C%20existing%20GRPO-based%20methods%20apply%20uniform%20advantage%20weighting%2C%20thereby%20amplifying%20noisy%20signals%20from%20unstable%20samples%20in%20gradient%20updates%3B%20%28ii%29%20most%20works%20emphasize%20text-grounded%20reasoning%20over%20images%20while%20overlooking%20the%20model%27s%20visual%20perception%20ability%20of%20image%20content.%20In%20this%20paper%2C%20we%20propose%20Q-Hawkeye%2C%20an%20RL-based%20reliable%20visual%20policy%20optimization%20framework%20that%20redesigns%20the%20learning%20signal%20through%20unified%20Uncertainty-Aware%20Dynamic%20Optimization%20and%20Perception-Aware%20Optimization.%20Q-Hawkeye%20estimates%20predictive%20uncertainty%20using%20the%20variance%20of%20predicted%20scores%20across%20multiple%20rollouts%20and%20leverages%20this%20uncertainty%20to%20reweight%20each%20sample%27s%20update%20strength%2C%20stabilizing%20policy%20optimization.%20To%20strengthen%20perceptual%20reliability%2C%20we%20construct%20paired%20inputs%20of%20degraded%20images%20and%20their%20original%20images%20and%20introduce%20an%20Implicit%20Perception%20Loss%20that%20constrains%20the%20model%20to%20ground%20its%20quality%20judgments%20in%20genuine%20visual%20evidence.%20Extensive%20experiments%20demonstrate%20that%20Q-Hawkeye%20outperforms%20state-of-the-art%20methods%20and%20generalizes%20better%20across%20multiple%20datasets.%20The%20code%20and%20models%20will%20be%20made%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-Hawkeye%253A%2520Reliable%2520Visual%2520Policy%2520Optimization%2520for%2520Image%2520Quality%2520Assessment%26entry.906535625%3DWulin%2520Xie%2520and%2520Rui%2520Dai%2520and%2520Ruidong%2520Ding%2520and%2520Kaikui%2520Liu%2520and%2520Xiangxiang%2520Chu%2520and%2520Xinwen%2520Hou%2520and%2520Jie%2520Wen%26entry.1292438233%3DImage%2520Quality%2520Assessment%2520%2528IQA%2529%2520predicts%2520perceptual%2520quality%2520scores%2520consistent%2520with%2520human%2520judgments.%2520Recent%2520RL-based%2520IQA%2520methods%2520built%2520on%2520MLLMs%2520focus%2520on%2520generating%2520visual%2520quality%2520descriptions%2520and%2520scores%252C%2520ignoring%2520two%2520key%2520reliability%2520limitations%253A%2520%2528i%2529%2520although%2520the%2520model%2527s%2520prediction%2520stability%2520varies%2520significantly%2520across%2520training%2520samples%252C%2520existing%2520GRPO-based%2520methods%2520apply%2520uniform%2520advantage%2520weighting%252C%2520thereby%2520amplifying%2520noisy%2520signals%2520from%2520unstable%2520samples%2520in%2520gradient%2520updates%253B%2520%2528ii%2529%2520most%2520works%2520emphasize%2520text-grounded%2520reasoning%2520over%2520images%2520while%2520overlooking%2520the%2520model%2527s%2520visual%2520perception%2520ability%2520of%2520image%2520content.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Q-Hawkeye%252C%2520an%2520RL-based%2520reliable%2520visual%2520policy%2520optimization%2520framework%2520that%2520redesigns%2520the%2520learning%2520signal%2520through%2520unified%2520Uncertainty-Aware%2520Dynamic%2520Optimization%2520and%2520Perception-Aware%2520Optimization.%2520Q-Hawkeye%2520estimates%2520predictive%2520uncertainty%2520using%2520the%2520variance%2520of%2520predicted%2520scores%2520across%2520multiple%2520rollouts%2520and%2520leverages%2520this%2520uncertainty%2520to%2520reweight%2520each%2520sample%2527s%2520update%2520strength%252C%2520stabilizing%2520policy%2520optimization.%2520To%2520strengthen%2520perceptual%2520reliability%252C%2520we%2520construct%2520paired%2520inputs%2520of%2520degraded%2520images%2520and%2520their%2520original%2520images%2520and%2520introduce%2520an%2520Implicit%2520Perception%2520Loss%2520that%2520constrains%2520the%2520model%2520to%2520ground%2520its%2520quality%2520judgments%2520in%2520genuine%2520visual%2520evidence.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Q-Hawkeye%2520outperforms%2520state-of-the-art%2520methods%2520and%2520generalizes%2520better%2520across%2520multiple%2520datasets.%2520The%2520code%2520and%2520models%2520will%2520be%2520made%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Hawkeye%3A%20Reliable%20Visual%20Policy%20Optimization%20for%20Image%20Quality%20Assessment&entry.906535625=Wulin%20Xie%20and%20Rui%20Dai%20and%20Ruidong%20Ding%20and%20Kaikui%20Liu%20and%20Xiangxiang%20Chu%20and%20Xinwen%20Hou%20and%20Jie%20Wen&entry.1292438233=Image%20Quality%20Assessment%20%28IQA%29%20predicts%20perceptual%20quality%20scores%20consistent%20with%20human%20judgments.%20Recent%20RL-based%20IQA%20methods%20built%20on%20MLLMs%20focus%20on%20generating%20visual%20quality%20descriptions%20and%20scores%2C%20ignoring%20two%20key%20reliability%20limitations%3A%20%28i%29%20although%20the%20model%27s%20prediction%20stability%20varies%20significantly%20across%20training%20samples%2C%20existing%20GRPO-based%20methods%20apply%20uniform%20advantage%20weighting%2C%20thereby%20amplifying%20noisy%20signals%20from%20unstable%20samples%20in%20gradient%20updates%3B%20%28ii%29%20most%20works%20emphasize%20text-grounded%20reasoning%20over%20images%20while%20overlooking%20the%20model%27s%20visual%20perception%20ability%20of%20image%20content.%20In%20this%20paper%2C%20we%20propose%20Q-Hawkeye%2C%20an%20RL-based%20reliable%20visual%20policy%20optimization%20framework%20that%20redesigns%20the%20learning%20signal%20through%20unified%20Uncertainty-Aware%20Dynamic%20Optimization%20and%20Perception-Aware%20Optimization.%20Q-Hawkeye%20estimates%20predictive%20uncertainty%20using%20the%20variance%20of%20predicted%20scores%20across%20multiple%20rollouts%20and%20leverages%20this%20uncertainty%20to%20reweight%20each%20sample%27s%20update%20strength%2C%20stabilizing%20policy%20optimization.%20To%20strengthen%20perceptual%20reliability%2C%20we%20construct%20paired%20inputs%20of%20degraded%20images%20and%20their%20original%20images%20and%20introduce%20an%20Implicit%20Perception%20Loss%20that%20constrains%20the%20model%20to%20ground%20its%20quality%20judgments%20in%20genuine%20visual%20evidence.%20Extensive%20experiments%20demonstrate%20that%20Q-Hawkeye%20outperforms%20state-of-the-art%20methods%20and%20generalizes%20better%20across%20multiple%20datasets.%20The%20code%20and%20models%20will%20be%20made%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.22920v1&entry.124074799=Read"},
{"title": "The Blueprints of Intelligence: A Functional-Topological Foundation for Perception and Representation", "author": "Eduardo Di Santi", "abstract": "Real-world phenomena do not generate arbitrary variability: their signals concentrate on compact, low-variability subsets of functional space, enabling rapid generalization from few examples. A small child can recognize a dog after extremely limited exposure because the perceptual manifold of \"dog\" is compact, structured, and low-dimensional. We formalize this principle through a deterministic functional-topological framework in which the set of valid realizations produced by a physical process forms a compact subset of a Banach space, endowed with stable invariants, a finite Hausdorff radius, and an induced continuous perceptual functional.\n  This geometry provides explicit limits on knowledge, conditions for identifiability, and guarantees for generalization from sparse evidence -- properties fundamental to both natural and artificial intelligence. Across electromechanical, electrochemical, and physiological domains, we show that real-world processes consistently generate compact perceptual manifolds with the same geometric characteristics. Their boundaries can be discovered in a fully self-supervised manner as the empirical radius saturates with increasing sampling, even when the governing equations are unknown.\n  These results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction. It provides a geometric explanation for why biological learners and self-supervised AI systems can generalize from few observations, and establishes compact perceptual manifolds as a fundamental building block for future AI architectures. Finally, this work unifies biological perception and modern self-supervised models under a single geometric principle: both derive their generalization ability from the compactness and invariants of real-world perceptual manifolds.", "link": "http://arxiv.org/abs/2512.05089v4", "date": "2026-01-30", "relevancy": 2.171, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Blueprints%20of%20Intelligence%3A%20A%20Functional-Topological%20Foundation%20for%20Perception%20and%20Representation&body=Title%3A%20The%20Blueprints%20of%20Intelligence%3A%20A%20Functional-Topological%20Foundation%20for%20Perception%20and%20Representation%0AAuthor%3A%20Eduardo%20Di%20Santi%0AAbstract%3A%20Real-world%20phenomena%20do%20not%20generate%20arbitrary%20variability%3A%20their%20signals%20concentrate%20on%20compact%2C%20low-variability%20subsets%20of%20functional%20space%2C%20enabling%20rapid%20generalization%20from%20few%20examples.%20A%20small%20child%20can%20recognize%20a%20dog%20after%20extremely%20limited%20exposure%20because%20the%20perceptual%20manifold%20of%20%22dog%22%20is%20compact%2C%20structured%2C%20and%20low-dimensional.%20We%20formalize%20this%20principle%20through%20a%20deterministic%20functional-topological%20framework%20in%20which%20the%20set%20of%20valid%20realizations%20produced%20by%20a%20physical%20process%20forms%20a%20compact%20subset%20of%20a%20Banach%20space%2C%20endowed%20with%20stable%20invariants%2C%20a%20finite%20Hausdorff%20radius%2C%20and%20an%20induced%20continuous%20perceptual%20functional.%0A%20%20This%20geometry%20provides%20explicit%20limits%20on%20knowledge%2C%20conditions%20for%20identifiability%2C%20and%20guarantees%20for%20generalization%20from%20sparse%20evidence%20--%20properties%20fundamental%20to%20both%20natural%20and%20artificial%20intelligence.%20Across%20electromechanical%2C%20electrochemical%2C%20and%20physiological%20domains%2C%20we%20show%20that%20real-world%20processes%20consistently%20generate%20compact%20perceptual%20manifolds%20with%20the%20same%20geometric%20characteristics.%20Their%20boundaries%20can%20be%20discovered%20in%20a%20fully%20self-supervised%20manner%20as%20the%20empirical%20radius%20saturates%20with%20increasing%20sampling%2C%20even%20when%20the%20governing%20equations%20are%20unknown.%0A%20%20These%20results%20demonstrate%20that%20deterministic%20functional%20topology%20offers%20a%20unified%20mathematical%20foundation%20for%20perception%2C%20representation%2C%20and%20world-model%20construction.%20It%20provides%20a%20geometric%20explanation%20for%20why%20biological%20learners%20and%20self-supervised%20AI%20systems%20can%20generalize%20from%20few%20observations%2C%20and%20establishes%20compact%20perceptual%20manifolds%20as%20a%20fundamental%20building%20block%20for%20future%20AI%20architectures.%20Finally%2C%20this%20work%20unifies%20biological%20perception%20and%20modern%20self-supervised%20models%20under%20a%20single%20geometric%20principle%3A%20both%20derive%20their%20generalization%20ability%20from%20the%20compactness%20and%20invariants%20of%20real-world%20perceptual%20manifolds.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05089v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Blueprints%2520of%2520Intelligence%253A%2520A%2520Functional-Topological%2520Foundation%2520for%2520Perception%2520and%2520Representation%26entry.906535625%3DEduardo%2520Di%2520Santi%26entry.1292438233%3DReal-world%2520phenomena%2520do%2520not%2520generate%2520arbitrary%2520variability%253A%2520their%2520signals%2520concentrate%2520on%2520compact%252C%2520low-variability%2520subsets%2520of%2520functional%2520space%252C%2520enabling%2520rapid%2520generalization%2520from%2520few%2520examples.%2520A%2520small%2520child%2520can%2520recognize%2520a%2520dog%2520after%2520extremely%2520limited%2520exposure%2520because%2520the%2520perceptual%2520manifold%2520of%2520%2522dog%2522%2520is%2520compact%252C%2520structured%252C%2520and%2520low-dimensional.%2520We%2520formalize%2520this%2520principle%2520through%2520a%2520deterministic%2520functional-topological%2520framework%2520in%2520which%2520the%2520set%2520of%2520valid%2520realizations%2520produced%2520by%2520a%2520physical%2520process%2520forms%2520a%2520compact%2520subset%2520of%2520a%2520Banach%2520space%252C%2520endowed%2520with%2520stable%2520invariants%252C%2520a%2520finite%2520Hausdorff%2520radius%252C%2520and%2520an%2520induced%2520continuous%2520perceptual%2520functional.%250A%2520%2520This%2520geometry%2520provides%2520explicit%2520limits%2520on%2520knowledge%252C%2520conditions%2520for%2520identifiability%252C%2520and%2520guarantees%2520for%2520generalization%2520from%2520sparse%2520evidence%2520--%2520properties%2520fundamental%2520to%2520both%2520natural%2520and%2520artificial%2520intelligence.%2520Across%2520electromechanical%252C%2520electrochemical%252C%2520and%2520physiological%2520domains%252C%2520we%2520show%2520that%2520real-world%2520processes%2520consistently%2520generate%2520compact%2520perceptual%2520manifolds%2520with%2520the%2520same%2520geometric%2520characteristics.%2520Their%2520boundaries%2520can%2520be%2520discovered%2520in%2520a%2520fully%2520self-supervised%2520manner%2520as%2520the%2520empirical%2520radius%2520saturates%2520with%2520increasing%2520sampling%252C%2520even%2520when%2520the%2520governing%2520equations%2520are%2520unknown.%250A%2520%2520These%2520results%2520demonstrate%2520that%2520deterministic%2520functional%2520topology%2520offers%2520a%2520unified%2520mathematical%2520foundation%2520for%2520perception%252C%2520representation%252C%2520and%2520world-model%2520construction.%2520It%2520provides%2520a%2520geometric%2520explanation%2520for%2520why%2520biological%2520learners%2520and%2520self-supervised%2520AI%2520systems%2520can%2520generalize%2520from%2520few%2520observations%252C%2520and%2520establishes%2520compact%2520perceptual%2520manifolds%2520as%2520a%2520fundamental%2520building%2520block%2520for%2520future%2520AI%2520architectures.%2520Finally%252C%2520this%2520work%2520unifies%2520biological%2520perception%2520and%2520modern%2520self-supervised%2520models%2520under%2520a%2520single%2520geometric%2520principle%253A%2520both%2520derive%2520their%2520generalization%2520ability%2520from%2520the%2520compactness%2520and%2520invariants%2520of%2520real-world%2520perceptual%2520manifolds.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05089v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Blueprints%20of%20Intelligence%3A%20A%20Functional-Topological%20Foundation%20for%20Perception%20and%20Representation&entry.906535625=Eduardo%20Di%20Santi&entry.1292438233=Real-world%20phenomena%20do%20not%20generate%20arbitrary%20variability%3A%20their%20signals%20concentrate%20on%20compact%2C%20low-variability%20subsets%20of%20functional%20space%2C%20enabling%20rapid%20generalization%20from%20few%20examples.%20A%20small%20child%20can%20recognize%20a%20dog%20after%20extremely%20limited%20exposure%20because%20the%20perceptual%20manifold%20of%20%22dog%22%20is%20compact%2C%20structured%2C%20and%20low-dimensional.%20We%20formalize%20this%20principle%20through%20a%20deterministic%20functional-topological%20framework%20in%20which%20the%20set%20of%20valid%20realizations%20produced%20by%20a%20physical%20process%20forms%20a%20compact%20subset%20of%20a%20Banach%20space%2C%20endowed%20with%20stable%20invariants%2C%20a%20finite%20Hausdorff%20radius%2C%20and%20an%20induced%20continuous%20perceptual%20functional.%0A%20%20This%20geometry%20provides%20explicit%20limits%20on%20knowledge%2C%20conditions%20for%20identifiability%2C%20and%20guarantees%20for%20generalization%20from%20sparse%20evidence%20--%20properties%20fundamental%20to%20both%20natural%20and%20artificial%20intelligence.%20Across%20electromechanical%2C%20electrochemical%2C%20and%20physiological%20domains%2C%20we%20show%20that%20real-world%20processes%20consistently%20generate%20compact%20perceptual%20manifolds%20with%20the%20same%20geometric%20characteristics.%20Their%20boundaries%20can%20be%20discovered%20in%20a%20fully%20self-supervised%20manner%20as%20the%20empirical%20radius%20saturates%20with%20increasing%20sampling%2C%20even%20when%20the%20governing%20equations%20are%20unknown.%0A%20%20These%20results%20demonstrate%20that%20deterministic%20functional%20topology%20offers%20a%20unified%20mathematical%20foundation%20for%20perception%2C%20representation%2C%20and%20world-model%20construction.%20It%20provides%20a%20geometric%20explanation%20for%20why%20biological%20learners%20and%20self-supervised%20AI%20systems%20can%20generalize%20from%20few%20observations%2C%20and%20establishes%20compact%20perceptual%20manifolds%20as%20a%20fundamental%20building%20block%20for%20future%20AI%20architectures.%20Finally%2C%20this%20work%20unifies%20biological%20perception%20and%20modern%20self-supervised%20models%20under%20a%20single%20geometric%20principle%3A%20both%20derive%20their%20generalization%20ability%20from%20the%20compactness%20and%20invariants%20of%20real-world%20perceptual%20manifolds.&entry.1838667208=http%3A//arxiv.org/abs/2512.05089v4&entry.124074799=Read"},
{"title": "CoFrGeNet: Continued Fraction Architectures for Language Generation", "author": "Amit Dhurandhar and Vijil Chenthamarakshan and Dennis Wei and Tejaswini Pedapati and Karthikeyan Natesan Ramamurthy and Rahul Nair", "abstract": "Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\\frac{2}{3}$ to $\\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.", "link": "http://arxiv.org/abs/2601.21766v2", "date": "2026-01-30", "relevancy": 2.167, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.62}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5452}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoFrGeNet%3A%20Continued%20Fraction%20Architectures%20for%20Language%20Generation&body=Title%3A%20CoFrGeNet%3A%20Continued%20Fraction%20Architectures%20for%20Language%20Generation%0AAuthor%3A%20Amit%20Dhurandhar%20and%20Vijil%20Chenthamarakshan%20and%20Dennis%20Wei%20and%20Tejaswini%20Pedapati%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Rahul%20Nair%0AAbstract%3A%20Transformers%20are%20arguably%20the%20preferred%20architecture%20for%20language%20generation.%20In%20this%20paper%2C%20inspired%20by%20continued%20fractions%2C%20we%20introduce%20a%20new%20function%20class%20for%20generative%20modeling.%20The%20architecture%20family%20implementing%20this%20function%20class%20is%20named%20CoFrGeNets%20-%20Continued%20Fraction%20Generative%20Networks.%20We%20design%20novel%20architectural%20components%20based%20on%20this%20function%20class%20that%20can%20replace%20Multi-head%20Attention%20and%20Feed-Forward%20Networks%20in%20Transformer%20blocks%20while%20requiring%20much%20fewer%20parameters.%20We%20derive%20custom%20gradient%20formulations%20to%20optimize%20the%20proposed%20components%20more%20accurately%20and%20efficiently%20than%20using%20standard%20PyTorch-based%20gradients.%20Our%20components%20are%20a%20plug-in%20replacement%20requiring%20little%20change%20in%20training%20or%20inference%20procedures%20that%20have%20already%20been%20put%20in%20place%20for%20Transformer-based%20models%20thus%20making%20our%20approach%20easy%20to%20incorporate%20in%20large%20industrial%20workflows.%20We%20experiment%20on%20two%20very%20different%20transformer%20architectures%20GPT2-xl%20%281.5B%29%20and%20Llama3%20%283.2B%29%2C%20where%20the%20former%20we%20pre-train%20on%20OpenWebText%20and%20GneissWeb%2C%20while%20the%20latter%20we%20pre-train%20on%20the%20docling%20data%20mix%20which%20consists%20of%20nine%20different%20datasets.%20Results%20show%20that%20the%20performance%20on%20downstream%20classification%2C%20Q%5C%26%20A%2C%20reasoning%20and%20text%20understanding%20tasks%20of%20our%20models%20is%20competitive%20and%20sometimes%20even%20superior%20to%20the%20original%20models%20with%20%24%5Cfrac%7B2%7D%7B3%7D%24%20to%20%24%5Cfrac%7B1%7D%7B2%7D%24%20the%20parameters%20and%20shorter%20pre-training%20time.%20We%20believe%20that%20future%20implementations%20customized%20to%20hardware%20will%20further%20bring%20out%20the%20true%20potential%20of%20our%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoFrGeNet%253A%2520Continued%2520Fraction%2520Architectures%2520for%2520Language%2520Generation%26entry.906535625%3DAmit%2520Dhurandhar%2520and%2520Vijil%2520Chenthamarakshan%2520and%2520Dennis%2520Wei%2520and%2520Tejaswini%2520Pedapati%2520and%2520Karthikeyan%2520Natesan%2520Ramamurthy%2520and%2520Rahul%2520Nair%26entry.1292438233%3DTransformers%2520are%2520arguably%2520the%2520preferred%2520architecture%2520for%2520language%2520generation.%2520In%2520this%2520paper%252C%2520inspired%2520by%2520continued%2520fractions%252C%2520we%2520introduce%2520a%2520new%2520function%2520class%2520for%2520generative%2520modeling.%2520The%2520architecture%2520family%2520implementing%2520this%2520function%2520class%2520is%2520named%2520CoFrGeNets%2520-%2520Continued%2520Fraction%2520Generative%2520Networks.%2520We%2520design%2520novel%2520architectural%2520components%2520based%2520on%2520this%2520function%2520class%2520that%2520can%2520replace%2520Multi-head%2520Attention%2520and%2520Feed-Forward%2520Networks%2520in%2520Transformer%2520blocks%2520while%2520requiring%2520much%2520fewer%2520parameters.%2520We%2520derive%2520custom%2520gradient%2520formulations%2520to%2520optimize%2520the%2520proposed%2520components%2520more%2520accurately%2520and%2520efficiently%2520than%2520using%2520standard%2520PyTorch-based%2520gradients.%2520Our%2520components%2520are%2520a%2520plug-in%2520replacement%2520requiring%2520little%2520change%2520in%2520training%2520or%2520inference%2520procedures%2520that%2520have%2520already%2520been%2520put%2520in%2520place%2520for%2520Transformer-based%2520models%2520thus%2520making%2520our%2520approach%2520easy%2520to%2520incorporate%2520in%2520large%2520industrial%2520workflows.%2520We%2520experiment%2520on%2520two%2520very%2520different%2520transformer%2520architectures%2520GPT2-xl%2520%25281.5B%2529%2520and%2520Llama3%2520%25283.2B%2529%252C%2520where%2520the%2520former%2520we%2520pre-train%2520on%2520OpenWebText%2520and%2520GneissWeb%252C%2520while%2520the%2520latter%2520we%2520pre-train%2520on%2520the%2520docling%2520data%2520mix%2520which%2520consists%2520of%2520nine%2520different%2520datasets.%2520Results%2520show%2520that%2520the%2520performance%2520on%2520downstream%2520classification%252C%2520Q%255C%2526%2520A%252C%2520reasoning%2520and%2520text%2520understanding%2520tasks%2520of%2520our%2520models%2520is%2520competitive%2520and%2520sometimes%2520even%2520superior%2520to%2520the%2520original%2520models%2520with%2520%2524%255Cfrac%257B2%257D%257B3%257D%2524%2520to%2520%2524%255Cfrac%257B1%257D%257B2%257D%2524%2520the%2520parameters%2520and%2520shorter%2520pre-training%2520time.%2520We%2520believe%2520that%2520future%2520implementations%2520customized%2520to%2520hardware%2520will%2520further%2520bring%2520out%2520the%2520true%2520potential%2520of%2520our%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoFrGeNet%3A%20Continued%20Fraction%20Architectures%20for%20Language%20Generation&entry.906535625=Amit%20Dhurandhar%20and%20Vijil%20Chenthamarakshan%20and%20Dennis%20Wei%20and%20Tejaswini%20Pedapati%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Rahul%20Nair&entry.1292438233=Transformers%20are%20arguably%20the%20preferred%20architecture%20for%20language%20generation.%20In%20this%20paper%2C%20inspired%20by%20continued%20fractions%2C%20we%20introduce%20a%20new%20function%20class%20for%20generative%20modeling.%20The%20architecture%20family%20implementing%20this%20function%20class%20is%20named%20CoFrGeNets%20-%20Continued%20Fraction%20Generative%20Networks.%20We%20design%20novel%20architectural%20components%20based%20on%20this%20function%20class%20that%20can%20replace%20Multi-head%20Attention%20and%20Feed-Forward%20Networks%20in%20Transformer%20blocks%20while%20requiring%20much%20fewer%20parameters.%20We%20derive%20custom%20gradient%20formulations%20to%20optimize%20the%20proposed%20components%20more%20accurately%20and%20efficiently%20than%20using%20standard%20PyTorch-based%20gradients.%20Our%20components%20are%20a%20plug-in%20replacement%20requiring%20little%20change%20in%20training%20or%20inference%20procedures%20that%20have%20already%20been%20put%20in%20place%20for%20Transformer-based%20models%20thus%20making%20our%20approach%20easy%20to%20incorporate%20in%20large%20industrial%20workflows.%20We%20experiment%20on%20two%20very%20different%20transformer%20architectures%20GPT2-xl%20%281.5B%29%20and%20Llama3%20%283.2B%29%2C%20where%20the%20former%20we%20pre-train%20on%20OpenWebText%20and%20GneissWeb%2C%20while%20the%20latter%20we%20pre-train%20on%20the%20docling%20data%20mix%20which%20consists%20of%20nine%20different%20datasets.%20Results%20show%20that%20the%20performance%20on%20downstream%20classification%2C%20Q%5C%26%20A%2C%20reasoning%20and%20text%20understanding%20tasks%20of%20our%20models%20is%20competitive%20and%20sometimes%20even%20superior%20to%20the%20original%20models%20with%20%24%5Cfrac%7B2%7D%7B3%7D%24%20to%20%24%5Cfrac%7B1%7D%7B2%7D%24%20the%20parameters%20and%20shorter%20pre-training%20time.%20We%20believe%20that%20future%20implementations%20customized%20to%20hardware%20will%20further%20bring%20out%20the%20true%20potential%20of%20our%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2601.21766v2&entry.124074799=Read"},
{"title": "MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models", "author": "Yangyan Li", "abstract": "Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost: expanding a model's parametric memory -- its repository of factual knowledge or visual patterns -- traditionally requires deepening or widening the network, which incurs a proportional rise in active FLOPs. In this work, we introduce $\\textbf{MoVE (Mixture of Value Embeddings)}$, a mechanism that breaks this coupling and establishes a new axis for scaling capacity. MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers. For every step in the sequence, the model employs a differentiable soft gating mechanism to dynamically mix retrieved concepts from this bank into the standard value projection. This architecture allows parametric memory to be scaled independently of network depth by simply increasing the number of embedding slots. We validate MoVE through strictly controlled experiments on two representative applications of autoregressive modeling: Text Generation and Image Generation. In both domains, MoVE yields consistent performance improvements over standard and layer-wise memory baselines, enabling the construction of \"memory-dense\" models that achieve lower perplexity and higher fidelity than their dense counterparts at comparable compute budgets.", "link": "http://arxiv.org/abs/2601.22887v1", "date": "2026-01-30", "relevancy": 2.167, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5511}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5459}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoVE%3A%20Mixture%20of%20Value%20Embeddings%20--%20A%20New%20Axis%20for%20Scaling%20Parametric%20Memory%20in%20Autoregressive%20Models&body=Title%3A%20MoVE%3A%20Mixture%20of%20Value%20Embeddings%20--%20A%20New%20Axis%20for%20Scaling%20Parametric%20Memory%20in%20Autoregressive%20Models%0AAuthor%3A%20Yangyan%20Li%0AAbstract%3A%20Autoregressive%20sequence%20modeling%20stands%20as%20the%20cornerstone%20of%20modern%20Generative%20AI%2C%20powering%20results%20across%20diverse%20modalities%20ranging%20from%20text%20generation%20to%20image%20generation.%20However%2C%20a%20fundamental%20limitation%20of%20this%20paradigm%20is%20the%20rigid%20structural%20coupling%20of%20model%20capacity%20to%20computational%20cost%3A%20expanding%20a%20model%27s%20parametric%20memory%20--%20its%20repository%20of%20factual%20knowledge%20or%20visual%20patterns%20--%20traditionally%20requires%20deepening%20or%20widening%20the%20network%2C%20which%20incurs%20a%20proportional%20rise%20in%20active%20FLOPs.%20In%20this%20work%2C%20we%20introduce%20%24%5Ctextbf%7BMoVE%20%28Mixture%20of%20Value%20Embeddings%29%7D%24%2C%20a%20mechanism%20that%20breaks%20this%20coupling%20and%20establishes%20a%20new%20axis%20for%20scaling%20capacity.%20MoVE%20decouples%20memory%20from%20compute%20by%20introducing%20a%20global%20bank%20of%20learnable%20value%20embeddings%20shared%20across%20all%20attention%20layers.%20For%20every%20step%20in%20the%20sequence%2C%20the%20model%20employs%20a%20differentiable%20soft%20gating%20mechanism%20to%20dynamically%20mix%20retrieved%20concepts%20from%20this%20bank%20into%20the%20standard%20value%20projection.%20This%20architecture%20allows%20parametric%20memory%20to%20be%20scaled%20independently%20of%20network%20depth%20by%20simply%20increasing%20the%20number%20of%20embedding%20slots.%20We%20validate%20MoVE%20through%20strictly%20controlled%20experiments%20on%20two%20representative%20applications%20of%20autoregressive%20modeling%3A%20Text%20Generation%20and%20Image%20Generation.%20In%20both%20domains%2C%20MoVE%20yields%20consistent%20performance%20improvements%20over%20standard%20and%20layer-wise%20memory%20baselines%2C%20enabling%20the%20construction%20of%20%22memory-dense%22%20models%20that%20achieve%20lower%20perplexity%20and%20higher%20fidelity%20than%20their%20dense%20counterparts%20at%20comparable%20compute%20budgets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoVE%253A%2520Mixture%2520of%2520Value%2520Embeddings%2520--%2520A%2520New%2520Axis%2520for%2520Scaling%2520Parametric%2520Memory%2520in%2520Autoregressive%2520Models%26entry.906535625%3DYangyan%2520Li%26entry.1292438233%3DAutoregressive%2520sequence%2520modeling%2520stands%2520as%2520the%2520cornerstone%2520of%2520modern%2520Generative%2520AI%252C%2520powering%2520results%2520across%2520diverse%2520modalities%2520ranging%2520from%2520text%2520generation%2520to%2520image%2520generation.%2520However%252C%2520a%2520fundamental%2520limitation%2520of%2520this%2520paradigm%2520is%2520the%2520rigid%2520structural%2520coupling%2520of%2520model%2520capacity%2520to%2520computational%2520cost%253A%2520expanding%2520a%2520model%2527s%2520parametric%2520memory%2520--%2520its%2520repository%2520of%2520factual%2520knowledge%2520or%2520visual%2520patterns%2520--%2520traditionally%2520requires%2520deepening%2520or%2520widening%2520the%2520network%252C%2520which%2520incurs%2520a%2520proportional%2520rise%2520in%2520active%2520FLOPs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520%2524%255Ctextbf%257BMoVE%2520%2528Mixture%2520of%2520Value%2520Embeddings%2529%257D%2524%252C%2520a%2520mechanism%2520that%2520breaks%2520this%2520coupling%2520and%2520establishes%2520a%2520new%2520axis%2520for%2520scaling%2520capacity.%2520MoVE%2520decouples%2520memory%2520from%2520compute%2520by%2520introducing%2520a%2520global%2520bank%2520of%2520learnable%2520value%2520embeddings%2520shared%2520across%2520all%2520attention%2520layers.%2520For%2520every%2520step%2520in%2520the%2520sequence%252C%2520the%2520model%2520employs%2520a%2520differentiable%2520soft%2520gating%2520mechanism%2520to%2520dynamically%2520mix%2520retrieved%2520concepts%2520from%2520this%2520bank%2520into%2520the%2520standard%2520value%2520projection.%2520This%2520architecture%2520allows%2520parametric%2520memory%2520to%2520be%2520scaled%2520independently%2520of%2520network%2520depth%2520by%2520simply%2520increasing%2520the%2520number%2520of%2520embedding%2520slots.%2520We%2520validate%2520MoVE%2520through%2520strictly%2520controlled%2520experiments%2520on%2520two%2520representative%2520applications%2520of%2520autoregressive%2520modeling%253A%2520Text%2520Generation%2520and%2520Image%2520Generation.%2520In%2520both%2520domains%252C%2520MoVE%2520yields%2520consistent%2520performance%2520improvements%2520over%2520standard%2520and%2520layer-wise%2520memory%2520baselines%252C%2520enabling%2520the%2520construction%2520of%2520%2522memory-dense%2522%2520models%2520that%2520achieve%2520lower%2520perplexity%2520and%2520higher%2520fidelity%2520than%2520their%2520dense%2520counterparts%2520at%2520comparable%2520compute%2520budgets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoVE%3A%20Mixture%20of%20Value%20Embeddings%20--%20A%20New%20Axis%20for%20Scaling%20Parametric%20Memory%20in%20Autoregressive%20Models&entry.906535625=Yangyan%20Li&entry.1292438233=Autoregressive%20sequence%20modeling%20stands%20as%20the%20cornerstone%20of%20modern%20Generative%20AI%2C%20powering%20results%20across%20diverse%20modalities%20ranging%20from%20text%20generation%20to%20image%20generation.%20However%2C%20a%20fundamental%20limitation%20of%20this%20paradigm%20is%20the%20rigid%20structural%20coupling%20of%20model%20capacity%20to%20computational%20cost%3A%20expanding%20a%20model%27s%20parametric%20memory%20--%20its%20repository%20of%20factual%20knowledge%20or%20visual%20patterns%20--%20traditionally%20requires%20deepening%20or%20widening%20the%20network%2C%20which%20incurs%20a%20proportional%20rise%20in%20active%20FLOPs.%20In%20this%20work%2C%20we%20introduce%20%24%5Ctextbf%7BMoVE%20%28Mixture%20of%20Value%20Embeddings%29%7D%24%2C%20a%20mechanism%20that%20breaks%20this%20coupling%20and%20establishes%20a%20new%20axis%20for%20scaling%20capacity.%20MoVE%20decouples%20memory%20from%20compute%20by%20introducing%20a%20global%20bank%20of%20learnable%20value%20embeddings%20shared%20across%20all%20attention%20layers.%20For%20every%20step%20in%20the%20sequence%2C%20the%20model%20employs%20a%20differentiable%20soft%20gating%20mechanism%20to%20dynamically%20mix%20retrieved%20concepts%20from%20this%20bank%20into%20the%20standard%20value%20projection.%20This%20architecture%20allows%20parametric%20memory%20to%20be%20scaled%20independently%20of%20network%20depth%20by%20simply%20increasing%20the%20number%20of%20embedding%20slots.%20We%20validate%20MoVE%20through%20strictly%20controlled%20experiments%20on%20two%20representative%20applications%20of%20autoregressive%20modeling%3A%20Text%20Generation%20and%20Image%20Generation.%20In%20both%20domains%2C%20MoVE%20yields%20consistent%20performance%20improvements%20over%20standard%20and%20layer-wise%20memory%20baselines%2C%20enabling%20the%20construction%20of%20%22memory-dense%22%20models%20that%20achieve%20lower%20perplexity%20and%20higher%20fidelity%20than%20their%20dense%20counterparts%20at%20comparable%20compute%20budgets.&entry.1838667208=http%3A//arxiv.org/abs/2601.22887v1&entry.124074799=Read"},
{"title": "Leveraging Multi-Rater Annotations to Calibrate Object Detectors in Microscopy Imaging", "author": "Francesco Campi and Lucrezia Tondo and Ekin Karabati and Johannes Betge and Marie Piraud", "abstract": "Deep learning-based object detectors have achieved impressive performance in microscopy imaging, yet their confidence estimates often lack calibration, limiting their reliability for biomedical applications. In this work, we introduce a new approach to improve model calibration by leveraging multi-rater annotations. We propose to train separate models on the annotations from single experts and aggregate their predictions to emulate consensus. This improves upon label sampling strategies, where models are trained on mixed annotations, and offers a more principled way to capture inter-rater variability. Experiments on a colorectal organoid dataset annotated by two experts demonstrate that our rater-specific ensemble strategy improves calibration performance while maintaining comparable detection accuracy. These findings suggest that explicitly modelling rater disagreement can lead to more trustworthy object detectors in biomedical imaging.", "link": "http://arxiv.org/abs/2601.23007v1", "date": "2026-01-30", "relevancy": 2.1644, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6174}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5305}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Multi-Rater%20Annotations%20to%20Calibrate%20Object%20Detectors%20in%20Microscopy%20Imaging&body=Title%3A%20Leveraging%20Multi-Rater%20Annotations%20to%20Calibrate%20Object%20Detectors%20in%20Microscopy%20Imaging%0AAuthor%3A%20Francesco%20Campi%20and%20Lucrezia%20Tondo%20and%20Ekin%20Karabati%20and%20Johannes%20Betge%20and%20Marie%20Piraud%0AAbstract%3A%20Deep%20learning-based%20object%20detectors%20have%20achieved%20impressive%20performance%20in%20microscopy%20imaging%2C%20yet%20their%20confidence%20estimates%20often%20lack%20calibration%2C%20limiting%20their%20reliability%20for%20biomedical%20applications.%20In%20this%20work%2C%20we%20introduce%20a%20new%20approach%20to%20improve%20model%20calibration%20by%20leveraging%20multi-rater%20annotations.%20We%20propose%20to%20train%20separate%20models%20on%20the%20annotations%20from%20single%20experts%20and%20aggregate%20their%20predictions%20to%20emulate%20consensus.%20This%20improves%20upon%20label%20sampling%20strategies%2C%20where%20models%20are%20trained%20on%20mixed%20annotations%2C%20and%20offers%20a%20more%20principled%20way%20to%20capture%20inter-rater%20variability.%20Experiments%20on%20a%20colorectal%20organoid%20dataset%20annotated%20by%20two%20experts%20demonstrate%20that%20our%20rater-specific%20ensemble%20strategy%20improves%20calibration%20performance%20while%20maintaining%20comparable%20detection%20accuracy.%20These%20findings%20suggest%20that%20explicitly%20modelling%20rater%20disagreement%20can%20lead%20to%20more%20trustworthy%20object%20detectors%20in%20biomedical%20imaging.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Multi-Rater%2520Annotations%2520to%2520Calibrate%2520Object%2520Detectors%2520in%2520Microscopy%2520Imaging%26entry.906535625%3DFrancesco%2520Campi%2520and%2520Lucrezia%2520Tondo%2520and%2520Ekin%2520Karabati%2520and%2520Johannes%2520Betge%2520and%2520Marie%2520Piraud%26entry.1292438233%3DDeep%2520learning-based%2520object%2520detectors%2520have%2520achieved%2520impressive%2520performance%2520in%2520microscopy%2520imaging%252C%2520yet%2520their%2520confidence%2520estimates%2520often%2520lack%2520calibration%252C%2520limiting%2520their%2520reliability%2520for%2520biomedical%2520applications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520approach%2520to%2520improve%2520model%2520calibration%2520by%2520leveraging%2520multi-rater%2520annotations.%2520We%2520propose%2520to%2520train%2520separate%2520models%2520on%2520the%2520annotations%2520from%2520single%2520experts%2520and%2520aggregate%2520their%2520predictions%2520to%2520emulate%2520consensus.%2520This%2520improves%2520upon%2520label%2520sampling%2520strategies%252C%2520where%2520models%2520are%2520trained%2520on%2520mixed%2520annotations%252C%2520and%2520offers%2520a%2520more%2520principled%2520way%2520to%2520capture%2520inter-rater%2520variability.%2520Experiments%2520on%2520a%2520colorectal%2520organoid%2520dataset%2520annotated%2520by%2520two%2520experts%2520demonstrate%2520that%2520our%2520rater-specific%2520ensemble%2520strategy%2520improves%2520calibration%2520performance%2520while%2520maintaining%2520comparable%2520detection%2520accuracy.%2520These%2520findings%2520suggest%2520that%2520explicitly%2520modelling%2520rater%2520disagreement%2520can%2520lead%2520to%2520more%2520trustworthy%2520object%2520detectors%2520in%2520biomedical%2520imaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Multi-Rater%20Annotations%20to%20Calibrate%20Object%20Detectors%20in%20Microscopy%20Imaging&entry.906535625=Francesco%20Campi%20and%20Lucrezia%20Tondo%20and%20Ekin%20Karabati%20and%20Johannes%20Betge%20and%20Marie%20Piraud&entry.1292438233=Deep%20learning-based%20object%20detectors%20have%20achieved%20impressive%20performance%20in%20microscopy%20imaging%2C%20yet%20their%20confidence%20estimates%20often%20lack%20calibration%2C%20limiting%20their%20reliability%20for%20biomedical%20applications.%20In%20this%20work%2C%20we%20introduce%20a%20new%20approach%20to%20improve%20model%20calibration%20by%20leveraging%20multi-rater%20annotations.%20We%20propose%20to%20train%20separate%20models%20on%20the%20annotations%20from%20single%20experts%20and%20aggregate%20their%20predictions%20to%20emulate%20consensus.%20This%20improves%20upon%20label%20sampling%20strategies%2C%20where%20models%20are%20trained%20on%20mixed%20annotations%2C%20and%20offers%20a%20more%20principled%20way%20to%20capture%20inter-rater%20variability.%20Experiments%20on%20a%20colorectal%20organoid%20dataset%20annotated%20by%20two%20experts%20demonstrate%20that%20our%20rater-specific%20ensemble%20strategy%20improves%20calibration%20performance%20while%20maintaining%20comparable%20detection%20accuracy.%20These%20findings%20suggest%20that%20explicitly%20modelling%20rater%20disagreement%20can%20lead%20to%20more%20trustworthy%20object%20detectors%20in%20biomedical%20imaging.&entry.1838667208=http%3A//arxiv.org/abs/2601.23007v1&entry.124074799=Read"},
{"title": "Adaptive Edge Learning for Density-Aware Graph Generation", "author": "Seyedeh Ava Razi Razavi and James Sargant and Sheridan Houghten and Renata Dividino", "abstract": "Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.", "link": "http://arxiv.org/abs/2601.23052v1", "date": "2026-01-30", "relevancy": 2.1542, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.55}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5324}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Edge%20Learning%20for%20Density-Aware%20Graph%20Generation&body=Title%3A%20Adaptive%20Edge%20Learning%20for%20Density-Aware%20Graph%20Generation%0AAuthor%3A%20Seyedeh%20Ava%20Razi%20Razavi%20and%20James%20Sargant%20and%20Sheridan%20Houghten%20and%20Renata%20Dividino%0AAbstract%3A%20Generating%20realistic%20graph-structured%20data%20is%20challenging%20due%20to%20discrete%20structures%2C%20variable%20sizes%2C%20and%20class-specific%20connectivity%20patterns%20that%20resist%20conventional%20generative%20modelling.%20While%20recent%20graph%20generation%20methods%20employ%20generative%20adversarial%20network%20%28GAN%29%20frameworks%20to%20handle%20permutation%20invariance%20and%20irregular%20topologies%2C%20they%20typically%20rely%20on%20random%20edge%20sampling%20with%20fixed%20probabilities%2C%20limiting%20their%20capacity%20to%20capture%20complex%20structural%20dependencies%20between%20nodes.%20We%20propose%20a%20density-aware%20conditional%20graph%20generation%20framework%20using%20Wasserstein%20GANs%20%28WGAN%29%20that%20replaces%20random%20sampling%20with%20a%20learnable%20distance-based%20edge%20predictor.%20Our%20approach%20embeds%20nodes%20into%20a%20latent%20space%20where%20proximity%20correlates%20with%20edge%20likelihood%2C%20enabling%20the%20generator%20to%20learn%20meaningful%20connectivity%20patterns.%20A%20differentiable%20edge%20predictor%20determines%20pairwise%20relationships%20directly%20from%20node%20embeddings%2C%20while%20a%20density-aware%20selection%20mechanism%20adaptively%20controls%20edge%20density%20to%20match%20class-specific%20sparsity%20distributions%20observed%20in%20real%20graphs.%20We%20train%20the%20model%20using%20a%20WGAN%20with%20gradient%20penalty%2C%20employing%20a%20GCN-based%20critic%20to%20ensure%20generated%20graphs%20exhibit%20realistic%20topology%20and%20align%20with%20target%20class%20distributions.%20Experiments%20on%20benchmark%20datasets%20demonstrate%20that%20our%20method%20produces%20graphs%20with%20superior%20structural%20coherence%20and%20class-consistent%20connectivity%20compared%20to%20existing%20baselines.%20The%20learned%20edge%20predictor%20captures%20complex%20relational%20patterns%20beyond%20simple%20heuristics%2C%20generating%20graphs%20whose%20density%20and%20topology%20closely%20match%20real%20structural%20distributions.%20Our%20results%20show%20improved%20training%20stability%20and%20controllable%20synthesis%2C%20making%20the%20framework%20effective%20for%20realistic%20graph%20generation%20and%20data%20augmentation.%20Source%20code%20is%20publicly%20available%20at%20https%3A//github.com/ava-12/Density_Aware_WGAN.git.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Edge%2520Learning%2520for%2520Density-Aware%2520Graph%2520Generation%26entry.906535625%3DSeyedeh%2520Ava%2520Razi%2520Razavi%2520and%2520James%2520Sargant%2520and%2520Sheridan%2520Houghten%2520and%2520Renata%2520Dividino%26entry.1292438233%3DGenerating%2520realistic%2520graph-structured%2520data%2520is%2520challenging%2520due%2520to%2520discrete%2520structures%252C%2520variable%2520sizes%252C%2520and%2520class-specific%2520connectivity%2520patterns%2520that%2520resist%2520conventional%2520generative%2520modelling.%2520While%2520recent%2520graph%2520generation%2520methods%2520employ%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520frameworks%2520to%2520handle%2520permutation%2520invariance%2520and%2520irregular%2520topologies%252C%2520they%2520typically%2520rely%2520on%2520random%2520edge%2520sampling%2520with%2520fixed%2520probabilities%252C%2520limiting%2520their%2520capacity%2520to%2520capture%2520complex%2520structural%2520dependencies%2520between%2520nodes.%2520We%2520propose%2520a%2520density-aware%2520conditional%2520graph%2520generation%2520framework%2520using%2520Wasserstein%2520GANs%2520%2528WGAN%2529%2520that%2520replaces%2520random%2520sampling%2520with%2520a%2520learnable%2520distance-based%2520edge%2520predictor.%2520Our%2520approach%2520embeds%2520nodes%2520into%2520a%2520latent%2520space%2520where%2520proximity%2520correlates%2520with%2520edge%2520likelihood%252C%2520enabling%2520the%2520generator%2520to%2520learn%2520meaningful%2520connectivity%2520patterns.%2520A%2520differentiable%2520edge%2520predictor%2520determines%2520pairwise%2520relationships%2520directly%2520from%2520node%2520embeddings%252C%2520while%2520a%2520density-aware%2520selection%2520mechanism%2520adaptively%2520controls%2520edge%2520density%2520to%2520match%2520class-specific%2520sparsity%2520distributions%2520observed%2520in%2520real%2520graphs.%2520We%2520train%2520the%2520model%2520using%2520a%2520WGAN%2520with%2520gradient%2520penalty%252C%2520employing%2520a%2520GCN-based%2520critic%2520to%2520ensure%2520generated%2520graphs%2520exhibit%2520realistic%2520topology%2520and%2520align%2520with%2520target%2520class%2520distributions.%2520Experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520method%2520produces%2520graphs%2520with%2520superior%2520structural%2520coherence%2520and%2520class-consistent%2520connectivity%2520compared%2520to%2520existing%2520baselines.%2520The%2520learned%2520edge%2520predictor%2520captures%2520complex%2520relational%2520patterns%2520beyond%2520simple%2520heuristics%252C%2520generating%2520graphs%2520whose%2520density%2520and%2520topology%2520closely%2520match%2520real%2520structural%2520distributions.%2520Our%2520results%2520show%2520improved%2520training%2520stability%2520and%2520controllable%2520synthesis%252C%2520making%2520the%2520framework%2520effective%2520for%2520realistic%2520graph%2520generation%2520and%2520data%2520augmentation.%2520Source%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/ava-12/Density_Aware_WGAN.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Edge%20Learning%20for%20Density-Aware%20Graph%20Generation&entry.906535625=Seyedeh%20Ava%20Razi%20Razavi%20and%20James%20Sargant%20and%20Sheridan%20Houghten%20and%20Renata%20Dividino&entry.1292438233=Generating%20realistic%20graph-structured%20data%20is%20challenging%20due%20to%20discrete%20structures%2C%20variable%20sizes%2C%20and%20class-specific%20connectivity%20patterns%20that%20resist%20conventional%20generative%20modelling.%20While%20recent%20graph%20generation%20methods%20employ%20generative%20adversarial%20network%20%28GAN%29%20frameworks%20to%20handle%20permutation%20invariance%20and%20irregular%20topologies%2C%20they%20typically%20rely%20on%20random%20edge%20sampling%20with%20fixed%20probabilities%2C%20limiting%20their%20capacity%20to%20capture%20complex%20structural%20dependencies%20between%20nodes.%20We%20propose%20a%20density-aware%20conditional%20graph%20generation%20framework%20using%20Wasserstein%20GANs%20%28WGAN%29%20that%20replaces%20random%20sampling%20with%20a%20learnable%20distance-based%20edge%20predictor.%20Our%20approach%20embeds%20nodes%20into%20a%20latent%20space%20where%20proximity%20correlates%20with%20edge%20likelihood%2C%20enabling%20the%20generator%20to%20learn%20meaningful%20connectivity%20patterns.%20A%20differentiable%20edge%20predictor%20determines%20pairwise%20relationships%20directly%20from%20node%20embeddings%2C%20while%20a%20density-aware%20selection%20mechanism%20adaptively%20controls%20edge%20density%20to%20match%20class-specific%20sparsity%20distributions%20observed%20in%20real%20graphs.%20We%20train%20the%20model%20using%20a%20WGAN%20with%20gradient%20penalty%2C%20employing%20a%20GCN-based%20critic%20to%20ensure%20generated%20graphs%20exhibit%20realistic%20topology%20and%20align%20with%20target%20class%20distributions.%20Experiments%20on%20benchmark%20datasets%20demonstrate%20that%20our%20method%20produces%20graphs%20with%20superior%20structural%20coherence%20and%20class-consistent%20connectivity%20compared%20to%20existing%20baselines.%20The%20learned%20edge%20predictor%20captures%20complex%20relational%20patterns%20beyond%20simple%20heuristics%2C%20generating%20graphs%20whose%20density%20and%20topology%20closely%20match%20real%20structural%20distributions.%20Our%20results%20show%20improved%20training%20stability%20and%20controllable%20synthesis%2C%20making%20the%20framework%20effective%20for%20realistic%20graph%20generation%20and%20data%20augmentation.%20Source%20code%20is%20publicly%20available%20at%20https%3A//github.com/ava-12/Density_Aware_WGAN.git.&entry.1838667208=http%3A//arxiv.org/abs/2601.23052v1&entry.124074799=Read"},
{"title": "TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification", "author": "Haoyun Jiang and Junqi He and Feng Hong and Xinlong Yang and Jianwei Zhang and Zheng Li and Zhengyang Zhuge and Zhiyong Chen and Bo Han and Junyang Lin and Jiangchao Yao", "abstract": "Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has nearly saturated improvements in draft effectiveness and efficiency, this paper advances SD from a new yet critical perspective: the verification cost. We propose TriSpec, a novel ternary SD framework that, at its core, introduces a lightweight proxy to significantly reduce computational cost by approving easily verifiable draft sequences and engaging the full target model only when encountering uncertain tokens. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 35\\% speedup over standard SD, with up to 50\\% fewer target model invocations while maintaining comparable accuracy.", "link": "http://arxiv.org/abs/2601.23180v1", "date": "2026-01-30", "relevancy": 2.146, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TriSpec%3A%20Ternary%20Speculative%20Decoding%20via%20Lightweight%20Proxy%20Verification&body=Title%3A%20TriSpec%3A%20Ternary%20Speculative%20Decoding%20via%20Lightweight%20Proxy%20Verification%0AAuthor%3A%20Haoyun%20Jiang%20and%20Junqi%20He%20and%20Feng%20Hong%20and%20Xinlong%20Yang%20and%20Jianwei%20Zhang%20and%20Zheng%20Li%20and%20Zhengyang%20Zhuge%20and%20Zhiyong%20Chen%20and%20Bo%20Han%20and%20Junyang%20Lin%20and%20Jiangchao%20Yao%0AAbstract%3A%20Inference%20efficiency%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20fundamentally%20limited%20by%20their%20serial%2C%20autoregressive%20generation%2C%20especially%20as%20reasoning%20becomes%20a%20key%20capability%20and%20response%20sequences%20grow%20longer.%20Speculative%20decoding%20%28SD%29%20offers%20a%20powerful%20solution%2C%20providing%20significant%20speed-ups%20through%20its%20lightweight%20drafting%20and%20parallel%20verification%20mechanism.%20While%20existing%20work%20has%20nearly%20saturated%20improvements%20in%20draft%20effectiveness%20and%20efficiency%2C%20this%20paper%20advances%20SD%20from%20a%20new%20yet%20critical%20perspective%3A%20the%20verification%20cost.%20We%20propose%20TriSpec%2C%20a%20novel%20ternary%20SD%20framework%20that%2C%20at%20its%20core%2C%20introduces%20a%20lightweight%20proxy%20to%20significantly%20reduce%20computational%20cost%20by%20approving%20easily%20verifiable%20draft%20sequences%20and%20engaging%20the%20full%20target%20model%20only%20when%20encountering%20uncertain%20tokens.%20TriSpec%20can%20be%20integrated%20with%20state-of-the-art%20SD%20methods%20like%20EAGLE-3%20to%20further%20reduce%20verification%20costs%2C%20achieving%20greater%20acceleration.%20Extensive%20experiments%20on%20the%20Qwen3%20and%20DeepSeek-R1-Distill-Qwen/LLaMA%20families%20show%20that%20TriSpec%20achieves%20up%20to%2035%5C%25%20speedup%20over%20standard%20SD%2C%20with%20up%20to%2050%5C%25%20fewer%20target%20model%20invocations%20while%20maintaining%20comparable%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriSpec%253A%2520Ternary%2520Speculative%2520Decoding%2520via%2520Lightweight%2520Proxy%2520Verification%26entry.906535625%3DHaoyun%2520Jiang%2520and%2520Junqi%2520He%2520and%2520Feng%2520Hong%2520and%2520Xinlong%2520Yang%2520and%2520Jianwei%2520Zhang%2520and%2520Zheng%2520Li%2520and%2520Zhengyang%2520Zhuge%2520and%2520Zhiyong%2520Chen%2520and%2520Bo%2520Han%2520and%2520Junyang%2520Lin%2520and%2520Jiangchao%2520Yao%26entry.1292438233%3DInference%2520efficiency%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520fundamentally%2520limited%2520by%2520their%2520serial%252C%2520autoregressive%2520generation%252C%2520especially%2520as%2520reasoning%2520becomes%2520a%2520key%2520capability%2520and%2520response%2520sequences%2520grow%2520longer.%2520Speculative%2520decoding%2520%2528SD%2529%2520offers%2520a%2520powerful%2520solution%252C%2520providing%2520significant%2520speed-ups%2520through%2520its%2520lightweight%2520drafting%2520and%2520parallel%2520verification%2520mechanism.%2520While%2520existing%2520work%2520has%2520nearly%2520saturated%2520improvements%2520in%2520draft%2520effectiveness%2520and%2520efficiency%252C%2520this%2520paper%2520advances%2520SD%2520from%2520a%2520new%2520yet%2520critical%2520perspective%253A%2520the%2520verification%2520cost.%2520We%2520propose%2520TriSpec%252C%2520a%2520novel%2520ternary%2520SD%2520framework%2520that%252C%2520at%2520its%2520core%252C%2520introduces%2520a%2520lightweight%2520proxy%2520to%2520significantly%2520reduce%2520computational%2520cost%2520by%2520approving%2520easily%2520verifiable%2520draft%2520sequences%2520and%2520engaging%2520the%2520full%2520target%2520model%2520only%2520when%2520encountering%2520uncertain%2520tokens.%2520TriSpec%2520can%2520be%2520integrated%2520with%2520state-of-the-art%2520SD%2520methods%2520like%2520EAGLE-3%2520to%2520further%2520reduce%2520verification%2520costs%252C%2520achieving%2520greater%2520acceleration.%2520Extensive%2520experiments%2520on%2520the%2520Qwen3%2520and%2520DeepSeek-R1-Distill-Qwen/LLaMA%2520families%2520show%2520that%2520TriSpec%2520achieves%2520up%2520to%252035%255C%2525%2520speedup%2520over%2520standard%2520SD%252C%2520with%2520up%2520to%252050%255C%2525%2520fewer%2520target%2520model%2520invocations%2520while%2520maintaining%2520comparable%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriSpec%3A%20Ternary%20Speculative%20Decoding%20via%20Lightweight%20Proxy%20Verification&entry.906535625=Haoyun%20Jiang%20and%20Junqi%20He%20and%20Feng%20Hong%20and%20Xinlong%20Yang%20and%20Jianwei%20Zhang%20and%20Zheng%20Li%20and%20Zhengyang%20Zhuge%20and%20Zhiyong%20Chen%20and%20Bo%20Han%20and%20Junyang%20Lin%20and%20Jiangchao%20Yao&entry.1292438233=Inference%20efficiency%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20fundamentally%20limited%20by%20their%20serial%2C%20autoregressive%20generation%2C%20especially%20as%20reasoning%20becomes%20a%20key%20capability%20and%20response%20sequences%20grow%20longer.%20Speculative%20decoding%20%28SD%29%20offers%20a%20powerful%20solution%2C%20providing%20significant%20speed-ups%20through%20its%20lightweight%20drafting%20and%20parallel%20verification%20mechanism.%20While%20existing%20work%20has%20nearly%20saturated%20improvements%20in%20draft%20effectiveness%20and%20efficiency%2C%20this%20paper%20advances%20SD%20from%20a%20new%20yet%20critical%20perspective%3A%20the%20verification%20cost.%20We%20propose%20TriSpec%2C%20a%20novel%20ternary%20SD%20framework%20that%2C%20at%20its%20core%2C%20introduces%20a%20lightweight%20proxy%20to%20significantly%20reduce%20computational%20cost%20by%20approving%20easily%20verifiable%20draft%20sequences%20and%20engaging%20the%20full%20target%20model%20only%20when%20encountering%20uncertain%20tokens.%20TriSpec%20can%20be%20integrated%20with%20state-of-the-art%20SD%20methods%20like%20EAGLE-3%20to%20further%20reduce%20verification%20costs%2C%20achieving%20greater%20acceleration.%20Extensive%20experiments%20on%20the%20Qwen3%20and%20DeepSeek-R1-Distill-Qwen/LLaMA%20families%20show%20that%20TriSpec%20achieves%20up%20to%2035%5C%25%20speedup%20over%20standard%20SD%2C%20with%20up%20to%2050%5C%25%20fewer%20target%20model%20invocations%20while%20maintaining%20comparable%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2601.23180v1&entry.124074799=Read"},
{"title": "Optimal Fairness under Local Differential Privacy", "author": "Hrad Ghoukasian and Shahab Asoodeh", "abstract": "We investigate how to optimally design local differential privacy (LDP) mechanisms that reduce data unfairness and thereby improve fairness in downstream classification. We first derive a closed-form optimal mechanism for binary sensitive attributes and then develop a tractable optimization framework that yields the corresponding optimal mechanism for multi-valued attributes. As a theoretical contribution, we establish that for discrimination-accuracy optimal classifiers, reducing data unfairness necessarily leads to lower classification unfairness, thus providing a direct link between privacy-aware pre-processing and classification fairness. Empirically, we demonstrate that our approach consistently outperforms existing LDP mechanisms in reducing data unfairness across diverse datasets and fairness metrics, while maintaining accuracy close to that of non-private models. Moreover, compared with leading pre-processing and post-processing fairness methods, our mechanism achieves a more favorable accuracy-fairness trade-off while simultaneously preserving the privacy of sensitive attributes. Taken together, these results highlight LDP as a principled and effective pre-processing fairness intervention technique.", "link": "http://arxiv.org/abs/2511.16377v2", "date": "2026-01-30", "relevancy": 2.1408, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4329}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4316}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Fairness%20under%20Local%20Differential%20Privacy&body=Title%3A%20Optimal%20Fairness%20under%20Local%20Differential%20Privacy%0AAuthor%3A%20Hrad%20Ghoukasian%20and%20Shahab%20Asoodeh%0AAbstract%3A%20We%20investigate%20how%20to%20optimally%20design%20local%20differential%20privacy%20%28LDP%29%20mechanisms%20that%20reduce%20data%20unfairness%20and%20thereby%20improve%20fairness%20in%20downstream%20classification.%20We%20first%20derive%20a%20closed-form%20optimal%20mechanism%20for%20binary%20sensitive%20attributes%20and%20then%20develop%20a%20tractable%20optimization%20framework%20that%20yields%20the%20corresponding%20optimal%20mechanism%20for%20multi-valued%20attributes.%20As%20a%20theoretical%20contribution%2C%20we%20establish%20that%20for%20discrimination-accuracy%20optimal%20classifiers%2C%20reducing%20data%20unfairness%20necessarily%20leads%20to%20lower%20classification%20unfairness%2C%20thus%20providing%20a%20direct%20link%20between%20privacy-aware%20pre-processing%20and%20classification%20fairness.%20Empirically%2C%20we%20demonstrate%20that%20our%20approach%20consistently%20outperforms%20existing%20LDP%20mechanisms%20in%20reducing%20data%20unfairness%20across%20diverse%20datasets%20and%20fairness%20metrics%2C%20while%20maintaining%20accuracy%20close%20to%20that%20of%20non-private%20models.%20Moreover%2C%20compared%20with%20leading%20pre-processing%20and%20post-processing%20fairness%20methods%2C%20our%20mechanism%20achieves%20a%20more%20favorable%20accuracy-fairness%20trade-off%20while%20simultaneously%20preserving%20the%20privacy%20of%20sensitive%20attributes.%20Taken%20together%2C%20these%20results%20highlight%20LDP%20as%20a%20principled%20and%20effective%20pre-processing%20fairness%20intervention%20technique.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Fairness%2520under%2520Local%2520Differential%2520Privacy%26entry.906535625%3DHrad%2520Ghoukasian%2520and%2520Shahab%2520Asoodeh%26entry.1292438233%3DWe%2520investigate%2520how%2520to%2520optimally%2520design%2520local%2520differential%2520privacy%2520%2528LDP%2529%2520mechanisms%2520that%2520reduce%2520data%2520unfairness%2520and%2520thereby%2520improve%2520fairness%2520in%2520downstream%2520classification.%2520We%2520first%2520derive%2520a%2520closed-form%2520optimal%2520mechanism%2520for%2520binary%2520sensitive%2520attributes%2520and%2520then%2520develop%2520a%2520tractable%2520optimization%2520framework%2520that%2520yields%2520the%2520corresponding%2520optimal%2520mechanism%2520for%2520multi-valued%2520attributes.%2520As%2520a%2520theoretical%2520contribution%252C%2520we%2520establish%2520that%2520for%2520discrimination-accuracy%2520optimal%2520classifiers%252C%2520reducing%2520data%2520unfairness%2520necessarily%2520leads%2520to%2520lower%2520classification%2520unfairness%252C%2520thus%2520providing%2520a%2520direct%2520link%2520between%2520privacy-aware%2520pre-processing%2520and%2520classification%2520fairness.%2520Empirically%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520outperforms%2520existing%2520LDP%2520mechanisms%2520in%2520reducing%2520data%2520unfairness%2520across%2520diverse%2520datasets%2520and%2520fairness%2520metrics%252C%2520while%2520maintaining%2520accuracy%2520close%2520to%2520that%2520of%2520non-private%2520models.%2520Moreover%252C%2520compared%2520with%2520leading%2520pre-processing%2520and%2520post-processing%2520fairness%2520methods%252C%2520our%2520mechanism%2520achieves%2520a%2520more%2520favorable%2520accuracy-fairness%2520trade-off%2520while%2520simultaneously%2520preserving%2520the%2520privacy%2520of%2520sensitive%2520attributes.%2520Taken%2520together%252C%2520these%2520results%2520highlight%2520LDP%2520as%2520a%2520principled%2520and%2520effective%2520pre-processing%2520fairness%2520intervention%2520technique.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Fairness%20under%20Local%20Differential%20Privacy&entry.906535625=Hrad%20Ghoukasian%20and%20Shahab%20Asoodeh&entry.1292438233=We%20investigate%20how%20to%20optimally%20design%20local%20differential%20privacy%20%28LDP%29%20mechanisms%20that%20reduce%20data%20unfairness%20and%20thereby%20improve%20fairness%20in%20downstream%20classification.%20We%20first%20derive%20a%20closed-form%20optimal%20mechanism%20for%20binary%20sensitive%20attributes%20and%20then%20develop%20a%20tractable%20optimization%20framework%20that%20yields%20the%20corresponding%20optimal%20mechanism%20for%20multi-valued%20attributes.%20As%20a%20theoretical%20contribution%2C%20we%20establish%20that%20for%20discrimination-accuracy%20optimal%20classifiers%2C%20reducing%20data%20unfairness%20necessarily%20leads%20to%20lower%20classification%20unfairness%2C%20thus%20providing%20a%20direct%20link%20between%20privacy-aware%20pre-processing%20and%20classification%20fairness.%20Empirically%2C%20we%20demonstrate%20that%20our%20approach%20consistently%20outperforms%20existing%20LDP%20mechanisms%20in%20reducing%20data%20unfairness%20across%20diverse%20datasets%20and%20fairness%20metrics%2C%20while%20maintaining%20accuracy%20close%20to%20that%20of%20non-private%20models.%20Moreover%2C%20compared%20with%20leading%20pre-processing%20and%20post-processing%20fairness%20methods%2C%20our%20mechanism%20achieves%20a%20more%20favorable%20accuracy-fairness%20trade-off%20while%20simultaneously%20preserving%20the%20privacy%20of%20sensitive%20attributes.%20Taken%20together%2C%20these%20results%20highlight%20LDP%20as%20a%20principled%20and%20effective%20pre-processing%20fairness%20intervention%20technique.&entry.1838667208=http%3A//arxiv.org/abs/2511.16377v2&entry.124074799=Read"},
{"title": "Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic", "author": "Jeong Woon Lee and Kyoleen Kwak and Daeho Kim and Hyoseok Hwang", "abstract": "Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.", "link": "http://arxiv.org/abs/2601.22970v1", "date": "2026-01-30", "relevancy": 2.1351, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4397}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4261}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizing%20the%20Q-Gradient%20Field%20for%20Policy%20Smoothness%20in%20Actor-Critic&body=Title%3A%20Stabilizing%20the%20Q-Gradient%20Field%20for%20Policy%20Smoothness%20in%20Actor-Critic%0AAuthor%3A%20Jeong%20Woon%20Lee%20and%20Kyoleen%20Kwak%20and%20Daeho%20Kim%20and%20Hyoseok%20Hwang%0AAbstract%3A%20Policies%20learned%20via%20continuous%20actor-critic%20methods%20often%20exhibit%20erratic%2C%20high-frequency%20oscillations%2C%20making%20them%20unsuitable%20for%20physical%20deployment.%20Current%20approaches%20attempt%20to%20enforce%20smoothness%20by%20directly%20regularizing%20the%20policy%27s%20output.%20We%20argue%20that%20this%20approach%20treats%20the%20symptom%20rather%20than%20the%20cause.%20In%20this%20work%2C%20we%20theoretically%20establish%20that%20policy%20non-smoothness%20is%20fundamentally%20governed%20by%20the%20differential%20geometry%20of%20the%20critic.%20By%20applying%20implicit%20differentiation%20to%20the%20actor-critic%20objective%2C%20we%20prove%20that%20the%20sensitivity%20of%20the%20optimal%20policy%20is%20bounded%20by%20the%20ratio%20of%20the%20Q-function%27s%20mixed-partial%20derivative%20%28noise%20sensitivity%29%20to%20its%20action-space%20curvature%20%28signal%20distinctness%29.%20To%20empirically%20validate%20this%20theoretical%20insight%2C%20we%20introduce%20PAVE%20%28Policy-Aware%20Value-field%20Equalization%29%2C%20a%20critic-centric%20regularization%20framework%20that%20treats%20the%20critic%20as%20a%20scalar%20field%20and%20stabilizes%20its%20induced%20action-gradient%20field.%20PAVE%20rectifies%20the%20learning%20signal%20by%20minimizing%20the%20Q-gradient%20volatility%20while%20preserving%20local%20curvature.%20Experimental%20results%20demonstrate%20that%20PAVE%20achieves%20smoothness%20and%20robustness%20comparable%20to%20policy-side%20smoothness%20regularization%20methods%2C%20while%20maintaining%20competitive%20task%20performance%2C%20without%20modifying%20the%20actor.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizing%2520the%2520Q-Gradient%2520Field%2520for%2520Policy%2520Smoothness%2520in%2520Actor-Critic%26entry.906535625%3DJeong%2520Woon%2520Lee%2520and%2520Kyoleen%2520Kwak%2520and%2520Daeho%2520Kim%2520and%2520Hyoseok%2520Hwang%26entry.1292438233%3DPolicies%2520learned%2520via%2520continuous%2520actor-critic%2520methods%2520often%2520exhibit%2520erratic%252C%2520high-frequency%2520oscillations%252C%2520making%2520them%2520unsuitable%2520for%2520physical%2520deployment.%2520Current%2520approaches%2520attempt%2520to%2520enforce%2520smoothness%2520by%2520directly%2520regularizing%2520the%2520policy%2527s%2520output.%2520We%2520argue%2520that%2520this%2520approach%2520treats%2520the%2520symptom%2520rather%2520than%2520the%2520cause.%2520In%2520this%2520work%252C%2520we%2520theoretically%2520establish%2520that%2520policy%2520non-smoothness%2520is%2520fundamentally%2520governed%2520by%2520the%2520differential%2520geometry%2520of%2520the%2520critic.%2520By%2520applying%2520implicit%2520differentiation%2520to%2520the%2520actor-critic%2520objective%252C%2520we%2520prove%2520that%2520the%2520sensitivity%2520of%2520the%2520optimal%2520policy%2520is%2520bounded%2520by%2520the%2520ratio%2520of%2520the%2520Q-function%2527s%2520mixed-partial%2520derivative%2520%2528noise%2520sensitivity%2529%2520to%2520its%2520action-space%2520curvature%2520%2528signal%2520distinctness%2529.%2520To%2520empirically%2520validate%2520this%2520theoretical%2520insight%252C%2520we%2520introduce%2520PAVE%2520%2528Policy-Aware%2520Value-field%2520Equalization%2529%252C%2520a%2520critic-centric%2520regularization%2520framework%2520that%2520treats%2520the%2520critic%2520as%2520a%2520scalar%2520field%2520and%2520stabilizes%2520its%2520induced%2520action-gradient%2520field.%2520PAVE%2520rectifies%2520the%2520learning%2520signal%2520by%2520minimizing%2520the%2520Q-gradient%2520volatility%2520while%2520preserving%2520local%2520curvature.%2520Experimental%2520results%2520demonstrate%2520that%2520PAVE%2520achieves%2520smoothness%2520and%2520robustness%2520comparable%2520to%2520policy-side%2520smoothness%2520regularization%2520methods%252C%2520while%2520maintaining%2520competitive%2520task%2520performance%252C%2520without%2520modifying%2520the%2520actor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizing%20the%20Q-Gradient%20Field%20for%20Policy%20Smoothness%20in%20Actor-Critic&entry.906535625=Jeong%20Woon%20Lee%20and%20Kyoleen%20Kwak%20and%20Daeho%20Kim%20and%20Hyoseok%20Hwang&entry.1292438233=Policies%20learned%20via%20continuous%20actor-critic%20methods%20often%20exhibit%20erratic%2C%20high-frequency%20oscillations%2C%20making%20them%20unsuitable%20for%20physical%20deployment.%20Current%20approaches%20attempt%20to%20enforce%20smoothness%20by%20directly%20regularizing%20the%20policy%27s%20output.%20We%20argue%20that%20this%20approach%20treats%20the%20symptom%20rather%20than%20the%20cause.%20In%20this%20work%2C%20we%20theoretically%20establish%20that%20policy%20non-smoothness%20is%20fundamentally%20governed%20by%20the%20differential%20geometry%20of%20the%20critic.%20By%20applying%20implicit%20differentiation%20to%20the%20actor-critic%20objective%2C%20we%20prove%20that%20the%20sensitivity%20of%20the%20optimal%20policy%20is%20bounded%20by%20the%20ratio%20of%20the%20Q-function%27s%20mixed-partial%20derivative%20%28noise%20sensitivity%29%20to%20its%20action-space%20curvature%20%28signal%20distinctness%29.%20To%20empirically%20validate%20this%20theoretical%20insight%2C%20we%20introduce%20PAVE%20%28Policy-Aware%20Value-field%20Equalization%29%2C%20a%20critic-centric%20regularization%20framework%20that%20treats%20the%20critic%20as%20a%20scalar%20field%20and%20stabilizes%20its%20induced%20action-gradient%20field.%20PAVE%20rectifies%20the%20learning%20signal%20by%20minimizing%20the%20Q-gradient%20volatility%20while%20preserving%20local%20curvature.%20Experimental%20results%20demonstrate%20that%20PAVE%20achieves%20smoothness%20and%20robustness%20comparable%20to%20policy-side%20smoothness%20regularization%20methods%2C%20while%20maintaining%20competitive%20task%20performance%2C%20without%20modifying%20the%20actor.&entry.1838667208=http%3A//arxiv.org/abs/2601.22970v1&entry.124074799=Read"},
{"title": "SuperCoder: Assembly Program Superoptimization with Large Language Models", "author": "Anjiang Wei and Tarun Suresh and Huanmi Tan and Yinglun Xu and Gagandeep Singh and Ke Wang and Alex Aiken", "abstract": "Superoptimization is the task of transforming a program into a faster one while preserving its input-output behavior. In this work, we investigate whether large language models (LLMs) can serve as superoptimizers, generating assembly programs that outperform code already optimized by industry-standard compilers. We construct the first large-scale benchmark for this problem, consisting of 8,072 assembly programs averaging 130 lines, in contrast to prior datasets restricted to 2-15 straight-line, loop-free programs. We evaluate 23 LLMs on this benchmark and find that the strongest baseline, Claude-opus-4, achieves a 51.5% test-passing rate and a 1.43x average speedup over gcc -O3. To further enhance performance, we fine-tune models with reinforcement learning, optimizing a reward function that integrates correctness and performance speedup. Starting from Qwen2.5-Coder-7B-Instruct (61.4% correctness, 1.10x speedup), the fine-tuned model SuperCoder attains 95.0% correctness and 1.46x average speedup, with additional improvement enabled by Best-of-N sampling and iterative refinement. Our results demonstrate, for the first time, that LLMs can be applied as superoptimizers for assembly programs, establishing a foundation for future research in program performance optimization beyond compiler heuristics.", "link": "http://arxiv.org/abs/2505.11480v3", "date": "2026-01-30", "relevancy": 2.1344, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.441}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperCoder%3A%20Assembly%20Program%20Superoptimization%20with%20Large%20Language%20Models&body=Title%3A%20SuperCoder%3A%20Assembly%20Program%20Superoptimization%20with%20Large%20Language%20Models%0AAuthor%3A%20Anjiang%20Wei%20and%20Tarun%20Suresh%20and%20Huanmi%20Tan%20and%20Yinglun%20Xu%20and%20Gagandeep%20Singh%20and%20Ke%20Wang%20and%20Alex%20Aiken%0AAbstract%3A%20Superoptimization%20is%20the%20task%20of%20transforming%20a%20program%20into%20a%20faster%20one%20while%20preserving%20its%20input-output%20behavior.%20In%20this%20work%2C%20we%20investigate%20whether%20large%20language%20models%20%28LLMs%29%20can%20serve%20as%20superoptimizers%2C%20generating%20assembly%20programs%20that%20outperform%20code%20already%20optimized%20by%20industry-standard%20compilers.%20We%20construct%20the%20first%20large-scale%20benchmark%20for%20this%20problem%2C%20consisting%20of%208%2C072%20assembly%20programs%20averaging%20130%20lines%2C%20in%20contrast%20to%20prior%20datasets%20restricted%20to%202-15%20straight-line%2C%20loop-free%20programs.%20We%20evaluate%2023%20LLMs%20on%20this%20benchmark%20and%20find%20that%20the%20strongest%20baseline%2C%20Claude-opus-4%2C%20achieves%20a%2051.5%25%20test-passing%20rate%20and%20a%201.43x%20average%20speedup%20over%20gcc%20-O3.%20To%20further%20enhance%20performance%2C%20we%20fine-tune%20models%20with%20reinforcement%20learning%2C%20optimizing%20a%20reward%20function%20that%20integrates%20correctness%20and%20performance%20speedup.%20Starting%20from%20Qwen2.5-Coder-7B-Instruct%20%2861.4%25%20correctness%2C%201.10x%20speedup%29%2C%20the%20fine-tuned%20model%20SuperCoder%20attains%2095.0%25%20correctness%20and%201.46x%20average%20speedup%2C%20with%20additional%20improvement%20enabled%20by%20Best-of-N%20sampling%20and%20iterative%20refinement.%20Our%20results%20demonstrate%2C%20for%20the%20first%20time%2C%20that%20LLMs%20can%20be%20applied%20as%20superoptimizers%20for%20assembly%20programs%2C%20establishing%20a%20foundation%20for%20future%20research%20in%20program%20performance%20optimization%20beyond%20compiler%20heuristics.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11480v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperCoder%253A%2520Assembly%2520Program%2520Superoptimization%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DAnjiang%2520Wei%2520and%2520Tarun%2520Suresh%2520and%2520Huanmi%2520Tan%2520and%2520Yinglun%2520Xu%2520and%2520Gagandeep%2520Singh%2520and%2520Ke%2520Wang%2520and%2520Alex%2520Aiken%26entry.1292438233%3DSuperoptimization%2520is%2520the%2520task%2520of%2520transforming%2520a%2520program%2520into%2520a%2520faster%2520one%2520while%2520preserving%2520its%2520input-output%2520behavior.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520serve%2520as%2520superoptimizers%252C%2520generating%2520assembly%2520programs%2520that%2520outperform%2520code%2520already%2520optimized%2520by%2520industry-standard%2520compilers.%2520We%2520construct%2520the%2520first%2520large-scale%2520benchmark%2520for%2520this%2520problem%252C%2520consisting%2520of%25208%252C072%2520assembly%2520programs%2520averaging%2520130%2520lines%252C%2520in%2520contrast%2520to%2520prior%2520datasets%2520restricted%2520to%25202-15%2520straight-line%252C%2520loop-free%2520programs.%2520We%2520evaluate%252023%2520LLMs%2520on%2520this%2520benchmark%2520and%2520find%2520that%2520the%2520strongest%2520baseline%252C%2520Claude-opus-4%252C%2520achieves%2520a%252051.5%2525%2520test-passing%2520rate%2520and%2520a%25201.43x%2520average%2520speedup%2520over%2520gcc%2520-O3.%2520To%2520further%2520enhance%2520performance%252C%2520we%2520fine-tune%2520models%2520with%2520reinforcement%2520learning%252C%2520optimizing%2520a%2520reward%2520function%2520that%2520integrates%2520correctness%2520and%2520performance%2520speedup.%2520Starting%2520from%2520Qwen2.5-Coder-7B-Instruct%2520%252861.4%2525%2520correctness%252C%25201.10x%2520speedup%2529%252C%2520the%2520fine-tuned%2520model%2520SuperCoder%2520attains%252095.0%2525%2520correctness%2520and%25201.46x%2520average%2520speedup%252C%2520with%2520additional%2520improvement%2520enabled%2520by%2520Best-of-N%2520sampling%2520and%2520iterative%2520refinement.%2520Our%2520results%2520demonstrate%252C%2520for%2520the%2520first%2520time%252C%2520that%2520LLMs%2520can%2520be%2520applied%2520as%2520superoptimizers%2520for%2520assembly%2520programs%252C%2520establishing%2520a%2520foundation%2520for%2520future%2520research%2520in%2520program%2520performance%2520optimization%2520beyond%2520compiler%2520heuristics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11480v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperCoder%3A%20Assembly%20Program%20Superoptimization%20with%20Large%20Language%20Models&entry.906535625=Anjiang%20Wei%20and%20Tarun%20Suresh%20and%20Huanmi%20Tan%20and%20Yinglun%20Xu%20and%20Gagandeep%20Singh%20and%20Ke%20Wang%20and%20Alex%20Aiken&entry.1292438233=Superoptimization%20is%20the%20task%20of%20transforming%20a%20program%20into%20a%20faster%20one%20while%20preserving%20its%20input-output%20behavior.%20In%20this%20work%2C%20we%20investigate%20whether%20large%20language%20models%20%28LLMs%29%20can%20serve%20as%20superoptimizers%2C%20generating%20assembly%20programs%20that%20outperform%20code%20already%20optimized%20by%20industry-standard%20compilers.%20We%20construct%20the%20first%20large-scale%20benchmark%20for%20this%20problem%2C%20consisting%20of%208%2C072%20assembly%20programs%20averaging%20130%20lines%2C%20in%20contrast%20to%20prior%20datasets%20restricted%20to%202-15%20straight-line%2C%20loop-free%20programs.%20We%20evaluate%2023%20LLMs%20on%20this%20benchmark%20and%20find%20that%20the%20strongest%20baseline%2C%20Claude-opus-4%2C%20achieves%20a%2051.5%25%20test-passing%20rate%20and%20a%201.43x%20average%20speedup%20over%20gcc%20-O3.%20To%20further%20enhance%20performance%2C%20we%20fine-tune%20models%20with%20reinforcement%20learning%2C%20optimizing%20a%20reward%20function%20that%20integrates%20correctness%20and%20performance%20speedup.%20Starting%20from%20Qwen2.5-Coder-7B-Instruct%20%2861.4%25%20correctness%2C%201.10x%20speedup%29%2C%20the%20fine-tuned%20model%20SuperCoder%20attains%2095.0%25%20correctness%20and%201.46x%20average%20speedup%2C%20with%20additional%20improvement%20enabled%20by%20Best-of-N%20sampling%20and%20iterative%20refinement.%20Our%20results%20demonstrate%2C%20for%20the%20first%20time%2C%20that%20LLMs%20can%20be%20applied%20as%20superoptimizers%20for%20assembly%20programs%2C%20establishing%20a%20foundation%20for%20future%20research%20in%20program%20performance%20optimization%20beyond%20compiler%20heuristics.&entry.1838667208=http%3A//arxiv.org/abs/2505.11480v3&entry.124074799=Read"},
{"title": "GNN Explanations that do not Explain and How to find Them", "author": "Steve Azzolin and Stefano Teso and Bruno Lepri and Andrea Passerini and Sagar Malhotra", "abstract": "Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.", "link": "http://arxiv.org/abs/2601.20815v2", "date": "2026-01-30", "relevancy": 2.1269, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4421}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4227}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNN%20Explanations%20that%20do%20not%20Explain%20and%20How%20to%20find%20Them&body=Title%3A%20GNN%20Explanations%20that%20do%20not%20Explain%20and%20How%20to%20find%20Them%0AAuthor%3A%20Steve%20Azzolin%20and%20Stefano%20Teso%20and%20Bruno%20Lepri%20and%20Andrea%20Passerini%20and%20Sagar%20Malhotra%0AAbstract%3A%20Explanations%20provided%20by%20Self-explainable%20Graph%20Neural%20Networks%20%28SE-GNNs%29%20are%20fundamental%20for%20understanding%20the%20model%27s%20inner%20workings%20and%20for%20identifying%20potential%20misuse%20of%20sensitive%20attributes.%20Although%20recent%20works%20have%20highlighted%20that%20these%20explanations%20can%20be%20suboptimal%20and%20potentially%20misleading%2C%20a%20characterization%20of%20their%20failure%20cases%20is%20unavailable.%20In%20this%20work%2C%20we%20identify%20a%20critical%20failure%20of%20SE-GNN%20explanations%3A%20explanations%20can%20be%20unambiguously%20unrelated%20to%20how%20the%20SE-GNNs%20infer%20labels.%20We%20show%20that%2C%20on%20the%20one%20hand%2C%20many%20SE-GNNs%20can%20achieve%20optimal%20true%20risk%20while%20producing%20these%20degenerate%20explanations%2C%20and%20on%20the%20other%2C%20most%20faithfulness%20metrics%20can%20fail%20to%20identify%20these%20failure%20modes.%20Our%20empirical%20analysis%20reveals%20that%20degenerate%20explanations%20can%20be%20maliciously%20planted%20%28allowing%20an%20attacker%20to%20hide%20the%20use%20of%20sensitive%20attributes%29%20and%20can%20also%20emerge%20naturally%2C%20highlighting%20the%20need%20for%20reliable%20auditing.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20faithfulness%20metric%20that%20reliably%20marks%20degenerate%20explanations%20as%20unfaithful%2C%20in%20both%20malicious%20and%20natural%20settings.%20Our%20code%20is%20available%20in%20the%20supplemental.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNN%2520Explanations%2520that%2520do%2520not%2520Explain%2520and%2520How%2520to%2520find%2520Them%26entry.906535625%3DSteve%2520Azzolin%2520and%2520Stefano%2520Teso%2520and%2520Bruno%2520Lepri%2520and%2520Andrea%2520Passerini%2520and%2520Sagar%2520Malhotra%26entry.1292438233%3DExplanations%2520provided%2520by%2520Self-explainable%2520Graph%2520Neural%2520Networks%2520%2528SE-GNNs%2529%2520are%2520fundamental%2520for%2520understanding%2520the%2520model%2527s%2520inner%2520workings%2520and%2520for%2520identifying%2520potential%2520misuse%2520of%2520sensitive%2520attributes.%2520Although%2520recent%2520works%2520have%2520highlighted%2520that%2520these%2520explanations%2520can%2520be%2520suboptimal%2520and%2520potentially%2520misleading%252C%2520a%2520characterization%2520of%2520their%2520failure%2520cases%2520is%2520unavailable.%2520In%2520this%2520work%252C%2520we%2520identify%2520a%2520critical%2520failure%2520of%2520SE-GNN%2520explanations%253A%2520explanations%2520can%2520be%2520unambiguously%2520unrelated%2520to%2520how%2520the%2520SE-GNNs%2520infer%2520labels.%2520We%2520show%2520that%252C%2520on%2520the%2520one%2520hand%252C%2520many%2520SE-GNNs%2520can%2520achieve%2520optimal%2520true%2520risk%2520while%2520producing%2520these%2520degenerate%2520explanations%252C%2520and%2520on%2520the%2520other%252C%2520most%2520faithfulness%2520metrics%2520can%2520fail%2520to%2520identify%2520these%2520failure%2520modes.%2520Our%2520empirical%2520analysis%2520reveals%2520that%2520degenerate%2520explanations%2520can%2520be%2520maliciously%2520planted%2520%2528allowing%2520an%2520attacker%2520to%2520hide%2520the%2520use%2520of%2520sensitive%2520attributes%2529%2520and%2520can%2520also%2520emerge%2520naturally%252C%2520highlighting%2520the%2520need%2520for%2520reliable%2520auditing.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520faithfulness%2520metric%2520that%2520reliably%2520marks%2520degenerate%2520explanations%2520as%2520unfaithful%252C%2520in%2520both%2520malicious%2520and%2520natural%2520settings.%2520Our%2520code%2520is%2520available%2520in%2520the%2520supplemental.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNN%20Explanations%20that%20do%20not%20Explain%20and%20How%20to%20find%20Them&entry.906535625=Steve%20Azzolin%20and%20Stefano%20Teso%20and%20Bruno%20Lepri%20and%20Andrea%20Passerini%20and%20Sagar%20Malhotra&entry.1292438233=Explanations%20provided%20by%20Self-explainable%20Graph%20Neural%20Networks%20%28SE-GNNs%29%20are%20fundamental%20for%20understanding%20the%20model%27s%20inner%20workings%20and%20for%20identifying%20potential%20misuse%20of%20sensitive%20attributes.%20Although%20recent%20works%20have%20highlighted%20that%20these%20explanations%20can%20be%20suboptimal%20and%20potentially%20misleading%2C%20a%20characterization%20of%20their%20failure%20cases%20is%20unavailable.%20In%20this%20work%2C%20we%20identify%20a%20critical%20failure%20of%20SE-GNN%20explanations%3A%20explanations%20can%20be%20unambiguously%20unrelated%20to%20how%20the%20SE-GNNs%20infer%20labels.%20We%20show%20that%2C%20on%20the%20one%20hand%2C%20many%20SE-GNNs%20can%20achieve%20optimal%20true%20risk%20while%20producing%20these%20degenerate%20explanations%2C%20and%20on%20the%20other%2C%20most%20faithfulness%20metrics%20can%20fail%20to%20identify%20these%20failure%20modes.%20Our%20empirical%20analysis%20reveals%20that%20degenerate%20explanations%20can%20be%20maliciously%20planted%20%28allowing%20an%20attacker%20to%20hide%20the%20use%20of%20sensitive%20attributes%29%20and%20can%20also%20emerge%20naturally%2C%20highlighting%20the%20need%20for%20reliable%20auditing.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20faithfulness%20metric%20that%20reliably%20marks%20degenerate%20explanations%20as%20unfaithful%2C%20in%20both%20malicious%20and%20natural%20settings.%20Our%20code%20is%20available%20in%20the%20supplemental.&entry.1838667208=http%3A//arxiv.org/abs/2601.20815v2&entry.124074799=Read"},
{"title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning", "author": "Thomas Carta and Cl\u00e9ment Romac and Thomas Wolf and Sylvain Lamprier and Olivier Sigaud and Pierre-Yves Oudeyer", "abstract": "Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.", "link": "http://arxiv.org/abs/2302.02662v5", "date": "2026-01-30", "relevancy": 2.123, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5381}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Large%20Language%20Models%20in%20Interactive%20Environments%20with%20Online%20Reinforcement%20Learning&body=Title%3A%20Grounding%20Large%20Language%20Models%20in%20Interactive%20Environments%20with%20Online%20Reinforcement%20Learning%0AAuthor%3A%20Thomas%20Carta%20and%20Cl%C3%A9ment%20Romac%20and%20Thomas%20Wolf%20and%20Sylvain%20Lamprier%20and%20Olivier%20Sigaud%20and%20Pierre-Yves%20Oudeyer%0AAbstract%3A%20Recent%20works%20successfully%20leveraged%20Large%20Language%20Models%27%20%28LLM%29%20abilities%20to%20capture%20abstract%20knowledge%20about%20world%27s%20physics%20to%20solve%20decision-making%20problems.%20Yet%2C%20the%20alignment%20between%20LLMs%27%20knowledge%20and%20the%20environment%20can%20be%20wrong%20and%20limit%20functional%20competence%20due%20to%20lack%20of%20grounding.%20In%20this%20paper%2C%20we%20study%20an%20approach%20%28named%20GLAM%29%20to%20achieve%20this%20alignment%20through%20functional%20grounding%3A%20we%20consider%20an%20agent%20using%20an%20LLM%20as%20a%20policy%20that%20is%20progressively%20updated%20as%20the%20agent%20interacts%20with%20the%20environment%2C%20leveraging%20online%20Reinforcement%20Learning%20to%20improve%20its%20performance%20to%20solve%20goals.%20Using%20an%20interactive%20textual%20environment%20designed%20to%20study%20higher-level%20forms%20of%20functional%20grounding%2C%20and%20a%20set%20of%20spatial%20and%20navigation%20tasks%2C%20we%20study%20several%20scientific%20questions%3A%201%29%20Can%20LLMs%20boost%20sample%20efficiency%20for%20online%20learning%20of%20various%20RL%20tasks%3F%202%29%20How%20can%20it%20boost%20different%20forms%20of%20generalization%3F%203%29%20What%20is%20the%20impact%20of%20online%20learning%3F%20We%20study%20these%20questions%20by%20functionally%20grounding%20several%20variants%20%28size%2C%20architecture%29%20of%20FLAN-T5.%0ALink%3A%20http%3A//arxiv.org/abs/2302.02662v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Large%2520Language%2520Models%2520in%2520Interactive%2520Environments%2520with%2520Online%2520Reinforcement%2520Learning%26entry.906535625%3DThomas%2520Carta%2520and%2520Cl%25C3%25A9ment%2520Romac%2520and%2520Thomas%2520Wolf%2520and%2520Sylvain%2520Lamprier%2520and%2520Olivier%2520Sigaud%2520and%2520Pierre-Yves%2520Oudeyer%26entry.1292438233%3DRecent%2520works%2520successfully%2520leveraged%2520Large%2520Language%2520Models%2527%2520%2528LLM%2529%2520abilities%2520to%2520capture%2520abstract%2520knowledge%2520about%2520world%2527s%2520physics%2520to%2520solve%2520decision-making%2520problems.%2520Yet%252C%2520the%2520alignment%2520between%2520LLMs%2527%2520knowledge%2520and%2520the%2520environment%2520can%2520be%2520wrong%2520and%2520limit%2520functional%2520competence%2520due%2520to%2520lack%2520of%2520grounding.%2520In%2520this%2520paper%252C%2520we%2520study%2520an%2520approach%2520%2528named%2520GLAM%2529%2520to%2520achieve%2520this%2520alignment%2520through%2520functional%2520grounding%253A%2520we%2520consider%2520an%2520agent%2520using%2520an%2520LLM%2520as%2520a%2520policy%2520that%2520is%2520progressively%2520updated%2520as%2520the%2520agent%2520interacts%2520with%2520the%2520environment%252C%2520leveraging%2520online%2520Reinforcement%2520Learning%2520to%2520improve%2520its%2520performance%2520to%2520solve%2520goals.%2520Using%2520an%2520interactive%2520textual%2520environment%2520designed%2520to%2520study%2520higher-level%2520forms%2520of%2520functional%2520grounding%252C%2520and%2520a%2520set%2520of%2520spatial%2520and%2520navigation%2520tasks%252C%2520we%2520study%2520several%2520scientific%2520questions%253A%25201%2529%2520Can%2520LLMs%2520boost%2520sample%2520efficiency%2520for%2520online%2520learning%2520of%2520various%2520RL%2520tasks%253F%25202%2529%2520How%2520can%2520it%2520boost%2520different%2520forms%2520of%2520generalization%253F%25203%2529%2520What%2520is%2520the%2520impact%2520of%2520online%2520learning%253F%2520We%2520study%2520these%2520questions%2520by%2520functionally%2520grounding%2520several%2520variants%2520%2528size%252C%2520architecture%2529%2520of%2520FLAN-T5.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.02662v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Large%20Language%20Models%20in%20Interactive%20Environments%20with%20Online%20Reinforcement%20Learning&entry.906535625=Thomas%20Carta%20and%20Cl%C3%A9ment%20Romac%20and%20Thomas%20Wolf%20and%20Sylvain%20Lamprier%20and%20Olivier%20Sigaud%20and%20Pierre-Yves%20Oudeyer&entry.1292438233=Recent%20works%20successfully%20leveraged%20Large%20Language%20Models%27%20%28LLM%29%20abilities%20to%20capture%20abstract%20knowledge%20about%20world%27s%20physics%20to%20solve%20decision-making%20problems.%20Yet%2C%20the%20alignment%20between%20LLMs%27%20knowledge%20and%20the%20environment%20can%20be%20wrong%20and%20limit%20functional%20competence%20due%20to%20lack%20of%20grounding.%20In%20this%20paper%2C%20we%20study%20an%20approach%20%28named%20GLAM%29%20to%20achieve%20this%20alignment%20through%20functional%20grounding%3A%20we%20consider%20an%20agent%20using%20an%20LLM%20as%20a%20policy%20that%20is%20progressively%20updated%20as%20the%20agent%20interacts%20with%20the%20environment%2C%20leveraging%20online%20Reinforcement%20Learning%20to%20improve%20its%20performance%20to%20solve%20goals.%20Using%20an%20interactive%20textual%20environment%20designed%20to%20study%20higher-level%20forms%20of%20functional%20grounding%2C%20and%20a%20set%20of%20spatial%20and%20navigation%20tasks%2C%20we%20study%20several%20scientific%20questions%3A%201%29%20Can%20LLMs%20boost%20sample%20efficiency%20for%20online%20learning%20of%20various%20RL%20tasks%3F%202%29%20How%20can%20it%20boost%20different%20forms%20of%20generalization%3F%203%29%20What%20is%20the%20impact%20of%20online%20learning%3F%20We%20study%20these%20questions%20by%20functionally%20grounding%20several%20variants%20%28size%2C%20architecture%29%20of%20FLAN-T5.&entry.1838667208=http%3A//arxiv.org/abs/2302.02662v5&entry.124074799=Read"},
{"title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment", "author": "Zhewen Tan and Wenhan Yu and Jianfeng Si and Tongxin Liu and Kaiqi Guan and Huiyan Jin and Jiawen Tao and Xiaokun Yuan and Duohe Ma and Xiangzheng Zhang and Tong Yang and Lin Sun", "abstract": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.", "link": "http://arxiv.org/abs/2601.18292v2", "date": "2026-01-30", "relevancy": 2.1201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5183}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TriPlay-RL%3A%20Tri-Role%20Self-Play%20Reinforcement%20Learning%20for%20LLM%20Safety%20Alignment&body=Title%3A%20TriPlay-RL%3A%20Tri-Role%20Self-Play%20Reinforcement%20Learning%20for%20LLM%20Safety%20Alignment%0AAuthor%3A%20Zhewen%20Tan%20and%20Wenhan%20Yu%20and%20Jianfeng%20Si%20and%20Tongxin%20Liu%20and%20Kaiqi%20Guan%20and%20Huiyan%20Jin%20and%20Jiawen%20Tao%20and%20Xiaokun%20Yuan%20and%20Duohe%20Ma%20and%20Xiangzheng%20Zhang%20and%20Tong%20Yang%20and%20Lin%20Sun%0AAbstract%3A%20In%20recent%20years%2C%20safety%20risks%20associated%20with%20large%20language%20models%20have%20become%20increasingly%20prominent%2C%20highlighting%20the%20urgent%20need%20to%20mitigate%20the%20generation%20of%20toxic%20and%20harmful%20content.%20The%20mainstream%20paradigm%20for%20LLM%20safety%20alignment%20typically%20adopts%20a%20collaborative%20framework%20involving%20three%20roles%3A%20an%20attacker%20for%20adversarial%20prompt%20generation%2C%20a%20defender%20for%20safety%20defense%2C%20and%20an%20evaluator%20for%20response%20assessment.%20In%20this%20paper%2C%20we%20propose%20a%20closed-loop%20reinforcement%20learning%20framework%20called%20TriPlay-RL%20that%20enables%20iterative%20and%20co-improving%20collaboration%20among%20three%20roles%20with%20near-zero%20manual%20annotation.%20Experimental%20results%20show%20that%20the%20attacker%20preserves%20high%20output%20diversity%20while%20achieving%20a%2020%25-50%25%20improvement%20in%20adversarial%20effectiveness%3B%20the%20defender%20attains%2010%25-30%25%20gains%20in%20safety%20performance%20without%20degrading%20general%20reasoning%20capability%3B%20and%20the%20evaluator%20continuously%20refines%20its%20fine-grained%20judgment%20ability%20through%20iterations%2C%20accurately%20distinguishing%20unsafe%20responses%2C%20simple%20refusals%2C%20and%20useful%20guidance.%20Overall%2C%20our%20framework%20establishes%20an%20efficient%20and%20scalable%20paradigm%20for%20LLM%20safety%20alignment%2C%20enabling%20continuous%20co-evolution%20within%20a%20unified%20learning%20loop.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriPlay-RL%253A%2520Tri-Role%2520Self-Play%2520Reinforcement%2520Learning%2520for%2520LLM%2520Safety%2520Alignment%26entry.906535625%3DZhewen%2520Tan%2520and%2520Wenhan%2520Yu%2520and%2520Jianfeng%2520Si%2520and%2520Tongxin%2520Liu%2520and%2520Kaiqi%2520Guan%2520and%2520Huiyan%2520Jin%2520and%2520Jiawen%2520Tao%2520and%2520Xiaokun%2520Yuan%2520and%2520Duohe%2520Ma%2520and%2520Xiangzheng%2520Zhang%2520and%2520Tong%2520Yang%2520and%2520Lin%2520Sun%26entry.1292438233%3DIn%2520recent%2520years%252C%2520safety%2520risks%2520associated%2520with%2520large%2520language%2520models%2520have%2520become%2520increasingly%2520prominent%252C%2520highlighting%2520the%2520urgent%2520need%2520to%2520mitigate%2520the%2520generation%2520of%2520toxic%2520and%2520harmful%2520content.%2520The%2520mainstream%2520paradigm%2520for%2520LLM%2520safety%2520alignment%2520typically%2520adopts%2520a%2520collaborative%2520framework%2520involving%2520three%2520roles%253A%2520an%2520attacker%2520for%2520adversarial%2520prompt%2520generation%252C%2520a%2520defender%2520for%2520safety%2520defense%252C%2520and%2520an%2520evaluator%2520for%2520response%2520assessment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520closed-loop%2520reinforcement%2520learning%2520framework%2520called%2520TriPlay-RL%2520that%2520enables%2520iterative%2520and%2520co-improving%2520collaboration%2520among%2520three%2520roles%2520with%2520near-zero%2520manual%2520annotation.%2520Experimental%2520results%2520show%2520that%2520the%2520attacker%2520preserves%2520high%2520output%2520diversity%2520while%2520achieving%2520a%252020%2525-50%2525%2520improvement%2520in%2520adversarial%2520effectiveness%253B%2520the%2520defender%2520attains%252010%2525-30%2525%2520gains%2520in%2520safety%2520performance%2520without%2520degrading%2520general%2520reasoning%2520capability%253B%2520and%2520the%2520evaluator%2520continuously%2520refines%2520its%2520fine-grained%2520judgment%2520ability%2520through%2520iterations%252C%2520accurately%2520distinguishing%2520unsafe%2520responses%252C%2520simple%2520refusals%252C%2520and%2520useful%2520guidance.%2520Overall%252C%2520our%2520framework%2520establishes%2520an%2520efficient%2520and%2520scalable%2520paradigm%2520for%2520LLM%2520safety%2520alignment%252C%2520enabling%2520continuous%2520co-evolution%2520within%2520a%2520unified%2520learning%2520loop.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriPlay-RL%3A%20Tri-Role%20Self-Play%20Reinforcement%20Learning%20for%20LLM%20Safety%20Alignment&entry.906535625=Zhewen%20Tan%20and%20Wenhan%20Yu%20and%20Jianfeng%20Si%20and%20Tongxin%20Liu%20and%20Kaiqi%20Guan%20and%20Huiyan%20Jin%20and%20Jiawen%20Tao%20and%20Xiaokun%20Yuan%20and%20Duohe%20Ma%20and%20Xiangzheng%20Zhang%20and%20Tong%20Yang%20and%20Lin%20Sun&entry.1292438233=In%20recent%20years%2C%20safety%20risks%20associated%20with%20large%20language%20models%20have%20become%20increasingly%20prominent%2C%20highlighting%20the%20urgent%20need%20to%20mitigate%20the%20generation%20of%20toxic%20and%20harmful%20content.%20The%20mainstream%20paradigm%20for%20LLM%20safety%20alignment%20typically%20adopts%20a%20collaborative%20framework%20involving%20three%20roles%3A%20an%20attacker%20for%20adversarial%20prompt%20generation%2C%20a%20defender%20for%20safety%20defense%2C%20and%20an%20evaluator%20for%20response%20assessment.%20In%20this%20paper%2C%20we%20propose%20a%20closed-loop%20reinforcement%20learning%20framework%20called%20TriPlay-RL%20that%20enables%20iterative%20and%20co-improving%20collaboration%20among%20three%20roles%20with%20near-zero%20manual%20annotation.%20Experimental%20results%20show%20that%20the%20attacker%20preserves%20high%20output%20diversity%20while%20achieving%20a%2020%25-50%25%20improvement%20in%20adversarial%20effectiveness%3B%20the%20defender%20attains%2010%25-30%25%20gains%20in%20safety%20performance%20without%20degrading%20general%20reasoning%20capability%3B%20and%20the%20evaluator%20continuously%20refines%20its%20fine-grained%20judgment%20ability%20through%20iterations%2C%20accurately%20distinguishing%20unsafe%20responses%2C%20simple%20refusals%2C%20and%20useful%20guidance.%20Overall%2C%20our%20framework%20establishes%20an%20efficient%20and%20scalable%20paradigm%20for%20LLM%20safety%20alignment%2C%20enabling%20continuous%20co-evolution%20within%20a%20unified%20learning%20loop.&entry.1838667208=http%3A//arxiv.org/abs/2601.18292v2&entry.124074799=Read"},
{"title": "Synthetic Time Series Generation via Complex Networks", "author": "Jaime Vale and Vanessa Freitas Silva and Maria Eduarda Silva and Fernando Silva", "abstract": "Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.", "link": "http://arxiv.org/abs/2601.22879v1", "date": "2026-01-30", "relevancy": 2.0067, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5056}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4997}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Time%20Series%20Generation%20via%20Complex%20Networks&body=Title%3A%20Synthetic%20Time%20Series%20Generation%20via%20Complex%20Networks%0AAuthor%3A%20Jaime%20Vale%20and%20Vanessa%20Freitas%20Silva%20and%20Maria%20Eduarda%20Silva%20and%20Fernando%20Silva%0AAbstract%3A%20Time%20series%20data%20are%20essential%20for%20a%20wide%20range%20of%20applications%2C%20particularly%20in%20developing%20robust%20machine%20learning%20models.%20However%2C%20access%20to%20high-quality%20datasets%20is%20often%20limited%20due%20to%20privacy%20concerns%2C%20acquisition%20costs%2C%20and%20labeling%20challenges.%20Synthetic%20time%20series%20generation%20has%20emerged%20as%20a%20promising%20solution%20to%20address%20these%20constraints.%20In%20this%20work%2C%20we%20present%20a%20framework%20for%20generating%20synthetic%20time%20series%20by%20leveraging%20complex%20networks%20mappings.%20Specifically%2C%20we%20investigate%20whether%20time%20series%20transformed%20into%20Quantile%20Graphs%20%28QG%29%20--%20and%20then%20reconstructed%20via%20inverse%20mapping%20--%20can%20produce%20synthetic%20data%20that%20preserve%20the%20statistical%20and%20structural%20properties%20of%20the%20original.%20We%20evaluate%20the%20fidelity%20and%20utility%20of%20the%20generated%20data%20using%20both%20simulated%20and%20real-world%20datasets%2C%20and%20compare%20our%20approach%20against%20state-of-the-art%20Generative%20Adversarial%20Network%20%28GAN%29%20methods.%20Results%20indicate%20that%20our%20quantile%20graph-based%20methodology%20offers%20a%20competitive%20and%20interpretable%20alternative%20for%20synthetic%20time%20series%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Time%2520Series%2520Generation%2520via%2520Complex%2520Networks%26entry.906535625%3DJaime%2520Vale%2520and%2520Vanessa%2520Freitas%2520Silva%2520and%2520Maria%2520Eduarda%2520Silva%2520and%2520Fernando%2520Silva%26entry.1292438233%3DTime%2520series%2520data%2520are%2520essential%2520for%2520a%2520wide%2520range%2520of%2520applications%252C%2520particularly%2520in%2520developing%2520robust%2520machine%2520learning%2520models.%2520However%252C%2520access%2520to%2520high-quality%2520datasets%2520is%2520often%2520limited%2520due%2520to%2520privacy%2520concerns%252C%2520acquisition%2520costs%252C%2520and%2520labeling%2520challenges.%2520Synthetic%2520time%2520series%2520generation%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520address%2520these%2520constraints.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520framework%2520for%2520generating%2520synthetic%2520time%2520series%2520by%2520leveraging%2520complex%2520networks%2520mappings.%2520Specifically%252C%2520we%2520investigate%2520whether%2520time%2520series%2520transformed%2520into%2520Quantile%2520Graphs%2520%2528QG%2529%2520--%2520and%2520then%2520reconstructed%2520via%2520inverse%2520mapping%2520--%2520can%2520produce%2520synthetic%2520data%2520that%2520preserve%2520the%2520statistical%2520and%2520structural%2520properties%2520of%2520the%2520original.%2520We%2520evaluate%2520the%2520fidelity%2520and%2520utility%2520of%2520the%2520generated%2520data%2520using%2520both%2520simulated%2520and%2520real-world%2520datasets%252C%2520and%2520compare%2520our%2520approach%2520against%2520state-of-the-art%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520methods.%2520Results%2520indicate%2520that%2520our%2520quantile%2520graph-based%2520methodology%2520offers%2520a%2520competitive%2520and%2520interpretable%2520alternative%2520for%2520synthetic%2520time%2520series%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Time%20Series%20Generation%20via%20Complex%20Networks&entry.906535625=Jaime%20Vale%20and%20Vanessa%20Freitas%20Silva%20and%20Maria%20Eduarda%20Silva%20and%20Fernando%20Silva&entry.1292438233=Time%20series%20data%20are%20essential%20for%20a%20wide%20range%20of%20applications%2C%20particularly%20in%20developing%20robust%20machine%20learning%20models.%20However%2C%20access%20to%20high-quality%20datasets%20is%20often%20limited%20due%20to%20privacy%20concerns%2C%20acquisition%20costs%2C%20and%20labeling%20challenges.%20Synthetic%20time%20series%20generation%20has%20emerged%20as%20a%20promising%20solution%20to%20address%20these%20constraints.%20In%20this%20work%2C%20we%20present%20a%20framework%20for%20generating%20synthetic%20time%20series%20by%20leveraging%20complex%20networks%20mappings.%20Specifically%2C%20we%20investigate%20whether%20time%20series%20transformed%20into%20Quantile%20Graphs%20%28QG%29%20--%20and%20then%20reconstructed%20via%20inverse%20mapping%20--%20can%20produce%20synthetic%20data%20that%20preserve%20the%20statistical%20and%20structural%20properties%20of%20the%20original.%20We%20evaluate%20the%20fidelity%20and%20utility%20of%20the%20generated%20data%20using%20both%20simulated%20and%20real-world%20datasets%2C%20and%20compare%20our%20approach%20against%20state-of-the-art%20Generative%20Adversarial%20Network%20%28GAN%29%20methods.%20Results%20indicate%20that%20our%20quantile%20graph-based%20methodology%20offers%20a%20competitive%20and%20interpretable%20alternative%20for%20synthetic%20time%20series%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2601.22879v1&entry.124074799=Read"},
{"title": "PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy", "author": "Jinhao Zhang and Zhexuan Zhou and Huizhe Li and Yichen Lai and Wenlong Xia and Haoming Song and Youmin Gong and Jie Mei", "abstract": "Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.", "link": "http://arxiv.org/abs/2601.22018v2", "date": "2026-01-30", "relevancy": 1.7991, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6279}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5977}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PocketDP3%3A%20Efficient%20Pocket-Scale%203D%20Visuomotor%20Policy&body=Title%3A%20PocketDP3%3A%20Efficient%20Pocket-Scale%203D%20Visuomotor%20Policy%0AAuthor%3A%20Jinhao%20Zhang%20and%20Zhexuan%20Zhou%20and%20Huizhe%20Li%20and%20Yichen%20Lai%20and%20Wenlong%20Xia%20and%20Haoming%20Song%20and%20Youmin%20Gong%20and%20Jie%20Mei%0AAbstract%3A%20Recently%2C%203D%20vision-based%20diffusion%20policies%20have%20shown%20strong%20capability%20in%20learning%20complex%20robotic%20manipulation%20skills.%20However%2C%20a%20common%20architectural%20mismatch%20exists%20in%20these%20models%3A%20a%20tiny%20yet%20efficient%20point-cloud%20encoder%20is%20often%20paired%20with%20a%20massive%20decoder.%20Given%20a%20compact%20scene%20representation%2C%20we%20argue%20that%20this%20may%20lead%20to%20substantial%20parameter%20waste%20in%20the%20decoder.%20Motivated%20by%20this%20observation%2C%20we%20propose%20PocketDP3%2C%20a%20pocket-scale%203D%20diffusion%20policy%20that%20replaces%20the%20heavy%20conditional%20U-Net%20decoder%20used%20in%20prior%20methods%20with%20a%20lightweight%20Diffusion%20Mixer%20%28DiM%29%20built%20on%20MLP-Mixer%20blocks.%20This%20architecture%20enables%20efficient%20fusion%20across%20temporal%20and%20channel%20dimensions%2C%20significantly%20reducing%20model%20size.%20Notably%2C%20without%20any%20additional%20consistency%20distillation%20techniques%2C%20our%20method%20supports%20two-step%20inference%20without%20sacrificing%20performance%2C%20improving%20practicality%20for%20real-time%20deployment.%20Across%20three%20simulation%20benchmarks--RoboTwin2.0%2C%20Adroit%2C%20and%20MetaWorld--PocketDP3%20achieves%20state-of-the-art%20performance%20with%20fewer%20than%201%25%20of%20the%20parameters%20of%20prior%20methods%2C%20while%20also%20accelerating%20inference.%20Real-world%20experiments%20further%20demonstrate%20the%20practicality%20and%20transferability%20of%20our%20method%20in%20real-world%20settings.%20Code%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPocketDP3%253A%2520Efficient%2520Pocket-Scale%25203D%2520Visuomotor%2520Policy%26entry.906535625%3DJinhao%2520Zhang%2520and%2520Zhexuan%2520Zhou%2520and%2520Huizhe%2520Li%2520and%2520Yichen%2520Lai%2520and%2520Wenlong%2520Xia%2520and%2520Haoming%2520Song%2520and%2520Youmin%2520Gong%2520and%2520Jie%2520Mei%26entry.1292438233%3DRecently%252C%25203D%2520vision-based%2520diffusion%2520policies%2520have%2520shown%2520strong%2520capability%2520in%2520learning%2520complex%2520robotic%2520manipulation%2520skills.%2520However%252C%2520a%2520common%2520architectural%2520mismatch%2520exists%2520in%2520these%2520models%253A%2520a%2520tiny%2520yet%2520efficient%2520point-cloud%2520encoder%2520is%2520often%2520paired%2520with%2520a%2520massive%2520decoder.%2520Given%2520a%2520compact%2520scene%2520representation%252C%2520we%2520argue%2520that%2520this%2520may%2520lead%2520to%2520substantial%2520parameter%2520waste%2520in%2520the%2520decoder.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520PocketDP3%252C%2520a%2520pocket-scale%25203D%2520diffusion%2520policy%2520that%2520replaces%2520the%2520heavy%2520conditional%2520U-Net%2520decoder%2520used%2520in%2520prior%2520methods%2520with%2520a%2520lightweight%2520Diffusion%2520Mixer%2520%2528DiM%2529%2520built%2520on%2520MLP-Mixer%2520blocks.%2520This%2520architecture%2520enables%2520efficient%2520fusion%2520across%2520temporal%2520and%2520channel%2520dimensions%252C%2520significantly%2520reducing%2520model%2520size.%2520Notably%252C%2520without%2520any%2520additional%2520consistency%2520distillation%2520techniques%252C%2520our%2520method%2520supports%2520two-step%2520inference%2520without%2520sacrificing%2520performance%252C%2520improving%2520practicality%2520for%2520real-time%2520deployment.%2520Across%2520three%2520simulation%2520benchmarks--RoboTwin2.0%252C%2520Adroit%252C%2520and%2520MetaWorld--PocketDP3%2520achieves%2520state-of-the-art%2520performance%2520with%2520fewer%2520than%25201%2525%2520of%2520the%2520parameters%2520of%2520prior%2520methods%252C%2520while%2520also%2520accelerating%2520inference.%2520Real-world%2520experiments%2520further%2520demonstrate%2520the%2520practicality%2520and%2520transferability%2520of%2520our%2520method%2520in%2520real-world%2520settings.%2520Code%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PocketDP3%3A%20Efficient%20Pocket-Scale%203D%20Visuomotor%20Policy&entry.906535625=Jinhao%20Zhang%20and%20Zhexuan%20Zhou%20and%20Huizhe%20Li%20and%20Yichen%20Lai%20and%20Wenlong%20Xia%20and%20Haoming%20Song%20and%20Youmin%20Gong%20and%20Jie%20Mei&entry.1292438233=Recently%2C%203D%20vision-based%20diffusion%20policies%20have%20shown%20strong%20capability%20in%20learning%20complex%20robotic%20manipulation%20skills.%20However%2C%20a%20common%20architectural%20mismatch%20exists%20in%20these%20models%3A%20a%20tiny%20yet%20efficient%20point-cloud%20encoder%20is%20often%20paired%20with%20a%20massive%20decoder.%20Given%20a%20compact%20scene%20representation%2C%20we%20argue%20that%20this%20may%20lead%20to%20substantial%20parameter%20waste%20in%20the%20decoder.%20Motivated%20by%20this%20observation%2C%20we%20propose%20PocketDP3%2C%20a%20pocket-scale%203D%20diffusion%20policy%20that%20replaces%20the%20heavy%20conditional%20U-Net%20decoder%20used%20in%20prior%20methods%20with%20a%20lightweight%20Diffusion%20Mixer%20%28DiM%29%20built%20on%20MLP-Mixer%20blocks.%20This%20architecture%20enables%20efficient%20fusion%20across%20temporal%20and%20channel%20dimensions%2C%20significantly%20reducing%20model%20size.%20Notably%2C%20without%20any%20additional%20consistency%20distillation%20techniques%2C%20our%20method%20supports%20two-step%20inference%20without%20sacrificing%20performance%2C%20improving%20practicality%20for%20real-time%20deployment.%20Across%20three%20simulation%20benchmarks--RoboTwin2.0%2C%20Adroit%2C%20and%20MetaWorld--PocketDP3%20achieves%20state-of-the-art%20performance%20with%20fewer%20than%201%25%20of%20the%20parameters%20of%20prior%20methods%2C%20while%20also%20accelerating%20inference.%20Real-world%20experiments%20further%20demonstrate%20the%20practicality%20and%20transferability%20of%20our%20method%20in%20real-world%20settings.%20Code%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2601.22018v2&entry.124074799=Read"},
{"title": "A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration", "author": "Huan Song and Shuyu Tian and Junyi Hao and Cheng Yuan and Zhenyu Jia and Jiawei Shao and Xuelong Li", "abstract": "As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging from physical desensitization to traditional cryptographic or obfuscation techniques-often compromise semantic understanding capabilities or fail to guarantee mathematical irreversibility against reconstruction attacks. To address these challenges, this study presents a novel privacy-preserving perception technology based on the AI Flow theoretical framework and an edge-cloud collaborative architecture. The proposed methodology integrates source desensitization with irreversible feature mapping. Leveraging Information Bottleneck theory, the edge device performs millisecond-level processing to transform raw imagery into abstract feature vectors via non-linear mapping and stochastic noise injection. This process constructs a unidirectional information flow that strips identity-sensitive attributes, rendering the reconstruction of original images impossible. Subsequently, the cloud platform utilizes multimodal family models to perform joint inference solely on these abstract vectors to detect abnormal behaviors. This approach fundamentally severs the path to privacy leakage at the architectural level, achieving a breakthrough from video surveillance to de-identified behavior perception and offering a robust solution for risk management in high-sensitivity public spaces.", "link": "http://arxiv.org/abs/2601.22938v1", "date": "2026-01-30", "relevancy": 2.1008, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5401}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5186}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Real-Time%20Privacy-Preserving%20Behavior%20Recognition%20System%20via%20Edge-Cloud%20Collaboration&body=Title%3A%20A%20Real-Time%20Privacy-Preserving%20Behavior%20Recognition%20System%20via%20Edge-Cloud%20Collaboration%0AAuthor%3A%20Huan%20Song%20and%20Shuyu%20Tian%20and%20Junyi%20Hao%20and%20Cheng%20Yuan%20and%20Zhenyu%20Jia%20and%20Jiawei%20Shao%20and%20Xuelong%20Li%0AAbstract%3A%20As%20intelligent%20sensing%20expands%20into%20high-privacy%20environments%20such%20as%20restrooms%20and%20changing%20rooms%2C%20the%20field%20faces%20a%20critical%20privacy-security%20paradox.%20Traditional%20RGB%20surveillance%20raises%20significant%20concerns%20regarding%20visual%20recording%20and%20storage%2C%20while%20existing%20privacy-preserving%20methods-ranging%20from%20physical%20desensitization%20to%20traditional%20cryptographic%20or%20obfuscation%20techniques-often%20compromise%20semantic%20understanding%20capabilities%20or%20fail%20to%20guarantee%20mathematical%20irreversibility%20against%20reconstruction%20attacks.%20To%20address%20these%20challenges%2C%20this%20study%20presents%20a%20novel%20privacy-preserving%20perception%20technology%20based%20on%20the%20AI%20Flow%20theoretical%20framework%20and%20an%20edge-cloud%20collaborative%20architecture.%20The%20proposed%20methodology%20integrates%20source%20desensitization%20with%20irreversible%20feature%20mapping.%20Leveraging%20Information%20Bottleneck%20theory%2C%20the%20edge%20device%20performs%20millisecond-level%20processing%20to%20transform%20raw%20imagery%20into%20abstract%20feature%20vectors%20via%20non-linear%20mapping%20and%20stochastic%20noise%20injection.%20This%20process%20constructs%20a%20unidirectional%20information%20flow%20that%20strips%20identity-sensitive%20attributes%2C%20rendering%20the%20reconstruction%20of%20original%20images%20impossible.%20Subsequently%2C%20the%20cloud%20platform%20utilizes%20multimodal%20family%20models%20to%20perform%20joint%20inference%20solely%20on%20these%20abstract%20vectors%20to%20detect%20abnormal%20behaviors.%20This%20approach%20fundamentally%20severs%20the%20path%20to%20privacy%20leakage%20at%20the%20architectural%20level%2C%20achieving%20a%20breakthrough%20from%20video%20surveillance%20to%20de-identified%20behavior%20perception%20and%20offering%20a%20robust%20solution%20for%20risk%20management%20in%20high-sensitivity%20public%20spaces.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Real-Time%2520Privacy-Preserving%2520Behavior%2520Recognition%2520System%2520via%2520Edge-Cloud%2520Collaboration%26entry.906535625%3DHuan%2520Song%2520and%2520Shuyu%2520Tian%2520and%2520Junyi%2520Hao%2520and%2520Cheng%2520Yuan%2520and%2520Zhenyu%2520Jia%2520and%2520Jiawei%2520Shao%2520and%2520Xuelong%2520Li%26entry.1292438233%3DAs%2520intelligent%2520sensing%2520expands%2520into%2520high-privacy%2520environments%2520such%2520as%2520restrooms%2520and%2520changing%2520rooms%252C%2520the%2520field%2520faces%2520a%2520critical%2520privacy-security%2520paradox.%2520Traditional%2520RGB%2520surveillance%2520raises%2520significant%2520concerns%2520regarding%2520visual%2520recording%2520and%2520storage%252C%2520while%2520existing%2520privacy-preserving%2520methods-ranging%2520from%2520physical%2520desensitization%2520to%2520traditional%2520cryptographic%2520or%2520obfuscation%2520techniques-often%2520compromise%2520semantic%2520understanding%2520capabilities%2520or%2520fail%2520to%2520guarantee%2520mathematical%2520irreversibility%2520against%2520reconstruction%2520attacks.%2520To%2520address%2520these%2520challenges%252C%2520this%2520study%2520presents%2520a%2520novel%2520privacy-preserving%2520perception%2520technology%2520based%2520on%2520the%2520AI%2520Flow%2520theoretical%2520framework%2520and%2520an%2520edge-cloud%2520collaborative%2520architecture.%2520The%2520proposed%2520methodology%2520integrates%2520source%2520desensitization%2520with%2520irreversible%2520feature%2520mapping.%2520Leveraging%2520Information%2520Bottleneck%2520theory%252C%2520the%2520edge%2520device%2520performs%2520millisecond-level%2520processing%2520to%2520transform%2520raw%2520imagery%2520into%2520abstract%2520feature%2520vectors%2520via%2520non-linear%2520mapping%2520and%2520stochastic%2520noise%2520injection.%2520This%2520process%2520constructs%2520a%2520unidirectional%2520information%2520flow%2520that%2520strips%2520identity-sensitive%2520attributes%252C%2520rendering%2520the%2520reconstruction%2520of%2520original%2520images%2520impossible.%2520Subsequently%252C%2520the%2520cloud%2520platform%2520utilizes%2520multimodal%2520family%2520models%2520to%2520perform%2520joint%2520inference%2520solely%2520on%2520these%2520abstract%2520vectors%2520to%2520detect%2520abnormal%2520behaviors.%2520This%2520approach%2520fundamentally%2520severs%2520the%2520path%2520to%2520privacy%2520leakage%2520at%2520the%2520architectural%2520level%252C%2520achieving%2520a%2520breakthrough%2520from%2520video%2520surveillance%2520to%2520de-identified%2520behavior%2520perception%2520and%2520offering%2520a%2520robust%2520solution%2520for%2520risk%2520management%2520in%2520high-sensitivity%2520public%2520spaces.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Real-Time%20Privacy-Preserving%20Behavior%20Recognition%20System%20via%20Edge-Cloud%20Collaboration&entry.906535625=Huan%20Song%20and%20Shuyu%20Tian%20and%20Junyi%20Hao%20and%20Cheng%20Yuan%20and%20Zhenyu%20Jia%20and%20Jiawei%20Shao%20and%20Xuelong%20Li&entry.1292438233=As%20intelligent%20sensing%20expands%20into%20high-privacy%20environments%20such%20as%20restrooms%20and%20changing%20rooms%2C%20the%20field%20faces%20a%20critical%20privacy-security%20paradox.%20Traditional%20RGB%20surveillance%20raises%20significant%20concerns%20regarding%20visual%20recording%20and%20storage%2C%20while%20existing%20privacy-preserving%20methods-ranging%20from%20physical%20desensitization%20to%20traditional%20cryptographic%20or%20obfuscation%20techniques-often%20compromise%20semantic%20understanding%20capabilities%20or%20fail%20to%20guarantee%20mathematical%20irreversibility%20against%20reconstruction%20attacks.%20To%20address%20these%20challenges%2C%20this%20study%20presents%20a%20novel%20privacy-preserving%20perception%20technology%20based%20on%20the%20AI%20Flow%20theoretical%20framework%20and%20an%20edge-cloud%20collaborative%20architecture.%20The%20proposed%20methodology%20integrates%20source%20desensitization%20with%20irreversible%20feature%20mapping.%20Leveraging%20Information%20Bottleneck%20theory%2C%20the%20edge%20device%20performs%20millisecond-level%20processing%20to%20transform%20raw%20imagery%20into%20abstract%20feature%20vectors%20via%20non-linear%20mapping%20and%20stochastic%20noise%20injection.%20This%20process%20constructs%20a%20unidirectional%20information%20flow%20that%20strips%20identity-sensitive%20attributes%2C%20rendering%20the%20reconstruction%20of%20original%20images%20impossible.%20Subsequently%2C%20the%20cloud%20platform%20utilizes%20multimodal%20family%20models%20to%20perform%20joint%20inference%20solely%20on%20these%20abstract%20vectors%20to%20detect%20abnormal%20behaviors.%20This%20approach%20fundamentally%20severs%20the%20path%20to%20privacy%20leakage%20at%20the%20architectural%20level%2C%20achieving%20a%20breakthrough%20from%20video%20surveillance%20to%20de-identified%20behavior%20perception%20and%20offering%20a%20robust%20solution%20for%20risk%20management%20in%20high-sensitivity%20public%20spaces.&entry.1838667208=http%3A//arxiv.org/abs/2601.22938v1&entry.124074799=Read"},
{"title": "Geometric-disentangelment Unlearning", "author": "Duo Zhou and Yuji Zhang and Tianxin Wei and Ruizhong Qiu and Ke Yang and Xiao Lin and Cheng Qian and Jingrui He and Hanghang Tong and Heng Ji and Huan Zhang", "abstract": "Large language models (LLMs) can internalize private or harmful content, motivating unlearning that removes a forget set while preserving retaining knowledge. However, forgetting updates often cause collateral degradation on retaining knowledge, creating a persistent trade-off. Existing LLM unlearning methods are often heuristic, and other theoretical approaches rely on offline feature constructions that do not capture update-time forget-retain interaction in LLMs. To address this limitation, we aim to develop an LLM unlearning method that reduces the forget-retain trade-off with theoretical guarantees. We take a first-principles view by formalizing \"no side effects\" as local retain invariance under small parameter updates, and prove an equivalence under optimizer-induced geometry: the retain loss is locally invariant if and only if the update direction is orthogonal to the subspace spanned by retain gradients. Based on the insight, we propose Geometric-disentanglement Unlearning (GU), a lightweight and theoretically grounded projection that can be plug-and-play to existing gradient-based unlearning methods to mitigate forget-retain side effects. Experiments on TOFU, MUSE, and WMDP-cyber show that GU strengthens forgetting while reducing retain drift. When added to SimNPO, it achieves up to 62\\% improved forgetting Extraction Strength (ES) and 31\\% higher retain ES. We open-sourced our code in https://github.com/Lemutisme/Geometric-Unlearning.", "link": "http://arxiv.org/abs/2511.17100v3", "date": "2026-01-30", "relevancy": 2.1036, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5505}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5348}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric-disentangelment%20Unlearning&body=Title%3A%20Geometric-disentangelment%20Unlearning%0AAuthor%3A%20Duo%20Zhou%20and%20Yuji%20Zhang%20and%20Tianxin%20Wei%20and%20Ruizhong%20Qiu%20and%20Ke%20Yang%20and%20Xiao%20Lin%20and%20Cheng%20Qian%20and%20Jingrui%20He%20and%20Hanghang%20Tong%20and%20Heng%20Ji%20and%20Huan%20Zhang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20can%20internalize%20private%20or%20harmful%20content%2C%20motivating%20unlearning%20that%20removes%20a%20forget%20set%20while%20preserving%20retaining%20knowledge.%20However%2C%20forgetting%20updates%20often%20cause%20collateral%20degradation%20on%20retaining%20knowledge%2C%20creating%20a%20persistent%20trade-off.%20Existing%20LLM%20unlearning%20methods%20are%20often%20heuristic%2C%20and%20other%20theoretical%20approaches%20rely%20on%20offline%20feature%20constructions%20that%20do%20not%20capture%20update-time%20forget-retain%20interaction%20in%20LLMs.%20To%20address%20this%20limitation%2C%20we%20aim%20to%20develop%20an%20LLM%20unlearning%20method%20that%20reduces%20the%20forget-retain%20trade-off%20with%20theoretical%20guarantees.%20We%20take%20a%20first-principles%20view%20by%20formalizing%20%22no%20side%20effects%22%20as%20local%20retain%20invariance%20under%20small%20parameter%20updates%2C%20and%20prove%20an%20equivalence%20under%20optimizer-induced%20geometry%3A%20the%20retain%20loss%20is%20locally%20invariant%20if%20and%20only%20if%20the%20update%20direction%20is%20orthogonal%20to%20the%20subspace%20spanned%20by%20retain%20gradients.%20Based%20on%20the%20insight%2C%20we%20propose%20Geometric-disentanglement%20Unlearning%20%28GU%29%2C%20a%20lightweight%20and%20theoretically%20grounded%20projection%20that%20can%20be%20plug-and-play%20to%20existing%20gradient-based%20unlearning%20methods%20to%20mitigate%20forget-retain%20side%20effects.%20Experiments%20on%20TOFU%2C%20MUSE%2C%20and%20WMDP-cyber%20show%20that%20GU%20strengthens%20forgetting%20while%20reducing%20retain%20drift.%20When%20added%20to%20SimNPO%2C%20it%20achieves%20up%20to%2062%5C%25%20improved%20forgetting%20Extraction%20Strength%20%28ES%29%20and%2031%5C%25%20higher%20retain%20ES.%20We%20open-sourced%20our%20code%20in%20https%3A//github.com/Lemutisme/Geometric-Unlearning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17100v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric-disentangelment%2520Unlearning%26entry.906535625%3DDuo%2520Zhou%2520and%2520Yuji%2520Zhang%2520and%2520Tianxin%2520Wei%2520and%2520Ruizhong%2520Qiu%2520and%2520Ke%2520Yang%2520and%2520Xiao%2520Lin%2520and%2520Cheng%2520Qian%2520and%2520Jingrui%2520He%2520and%2520Hanghang%2520Tong%2520and%2520Heng%2520Ji%2520and%2520Huan%2520Zhang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520can%2520internalize%2520private%2520or%2520harmful%2520content%252C%2520motivating%2520unlearning%2520that%2520removes%2520a%2520forget%2520set%2520while%2520preserving%2520retaining%2520knowledge.%2520However%252C%2520forgetting%2520updates%2520often%2520cause%2520collateral%2520degradation%2520on%2520retaining%2520knowledge%252C%2520creating%2520a%2520persistent%2520trade-off.%2520Existing%2520LLM%2520unlearning%2520methods%2520are%2520often%2520heuristic%252C%2520and%2520other%2520theoretical%2520approaches%2520rely%2520on%2520offline%2520feature%2520constructions%2520that%2520do%2520not%2520capture%2520update-time%2520forget-retain%2520interaction%2520in%2520LLMs.%2520To%2520address%2520this%2520limitation%252C%2520we%2520aim%2520to%2520develop%2520an%2520LLM%2520unlearning%2520method%2520that%2520reduces%2520the%2520forget-retain%2520trade-off%2520with%2520theoretical%2520guarantees.%2520We%2520take%2520a%2520first-principles%2520view%2520by%2520formalizing%2520%2522no%2520side%2520effects%2522%2520as%2520local%2520retain%2520invariance%2520under%2520small%2520parameter%2520updates%252C%2520and%2520prove%2520an%2520equivalence%2520under%2520optimizer-induced%2520geometry%253A%2520the%2520retain%2520loss%2520is%2520locally%2520invariant%2520if%2520and%2520only%2520if%2520the%2520update%2520direction%2520is%2520orthogonal%2520to%2520the%2520subspace%2520spanned%2520by%2520retain%2520gradients.%2520Based%2520on%2520the%2520insight%252C%2520we%2520propose%2520Geometric-disentanglement%2520Unlearning%2520%2528GU%2529%252C%2520a%2520lightweight%2520and%2520theoretically%2520grounded%2520projection%2520that%2520can%2520be%2520plug-and-play%2520to%2520existing%2520gradient-based%2520unlearning%2520methods%2520to%2520mitigate%2520forget-retain%2520side%2520effects.%2520Experiments%2520on%2520TOFU%252C%2520MUSE%252C%2520and%2520WMDP-cyber%2520show%2520that%2520GU%2520strengthens%2520forgetting%2520while%2520reducing%2520retain%2520drift.%2520When%2520added%2520to%2520SimNPO%252C%2520it%2520achieves%2520up%2520to%252062%255C%2525%2520improved%2520forgetting%2520Extraction%2520Strength%2520%2528ES%2529%2520and%252031%255C%2525%2520higher%2520retain%2520ES.%2520We%2520open-sourced%2520our%2520code%2520in%2520https%253A//github.com/Lemutisme/Geometric-Unlearning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17100v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric-disentangelment%20Unlearning&entry.906535625=Duo%20Zhou%20and%20Yuji%20Zhang%20and%20Tianxin%20Wei%20and%20Ruizhong%20Qiu%20and%20Ke%20Yang%20and%20Xiao%20Lin%20and%20Cheng%20Qian%20and%20Jingrui%20He%20and%20Hanghang%20Tong%20and%20Heng%20Ji%20and%20Huan%20Zhang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20can%20internalize%20private%20or%20harmful%20content%2C%20motivating%20unlearning%20that%20removes%20a%20forget%20set%20while%20preserving%20retaining%20knowledge.%20However%2C%20forgetting%20updates%20often%20cause%20collateral%20degradation%20on%20retaining%20knowledge%2C%20creating%20a%20persistent%20trade-off.%20Existing%20LLM%20unlearning%20methods%20are%20often%20heuristic%2C%20and%20other%20theoretical%20approaches%20rely%20on%20offline%20feature%20constructions%20that%20do%20not%20capture%20update-time%20forget-retain%20interaction%20in%20LLMs.%20To%20address%20this%20limitation%2C%20we%20aim%20to%20develop%20an%20LLM%20unlearning%20method%20that%20reduces%20the%20forget-retain%20trade-off%20with%20theoretical%20guarantees.%20We%20take%20a%20first-principles%20view%20by%20formalizing%20%22no%20side%20effects%22%20as%20local%20retain%20invariance%20under%20small%20parameter%20updates%2C%20and%20prove%20an%20equivalence%20under%20optimizer-induced%20geometry%3A%20the%20retain%20loss%20is%20locally%20invariant%20if%20and%20only%20if%20the%20update%20direction%20is%20orthogonal%20to%20the%20subspace%20spanned%20by%20retain%20gradients.%20Based%20on%20the%20insight%2C%20we%20propose%20Geometric-disentanglement%20Unlearning%20%28GU%29%2C%20a%20lightweight%20and%20theoretically%20grounded%20projection%20that%20can%20be%20plug-and-play%20to%20existing%20gradient-based%20unlearning%20methods%20to%20mitigate%20forget-retain%20side%20effects.%20Experiments%20on%20TOFU%2C%20MUSE%2C%20and%20WMDP-cyber%20show%20that%20GU%20strengthens%20forgetting%20while%20reducing%20retain%20drift.%20When%20added%20to%20SimNPO%2C%20it%20achieves%20up%20to%2062%5C%25%20improved%20forgetting%20Extraction%20Strength%20%28ES%29%20and%2031%5C%25%20higher%20retain%20ES.%20We%20open-sourced%20our%20code%20in%20https%3A//github.com/Lemutisme/Geometric-Unlearning.&entry.1838667208=http%3A//arxiv.org/abs/2511.17100v3&entry.124074799=Read"},
{"title": "Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning", "author": "Arvind Mahankali and Kaiyue Wen and Tengyu Ma", "abstract": "Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.", "link": "http://arxiv.org/abs/2601.23027v1", "date": "2026-01-30", "relevancy": 1.3952, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4632}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide-and-Conquer%20CoT%3A%20RL%20for%20Reducing%20Latency%20via%20Parallel%20Reasoning&body=Title%3A%20Divide-and-Conquer%20CoT%3A%20RL%20for%20Reducing%20Latency%20via%20Parallel%20Reasoning%0AAuthor%3A%20Arvind%20Mahankali%20and%20Kaiyue%20Wen%20and%20Tengyu%20Ma%0AAbstract%3A%20Long%20chain-of-thought%20reasoning%20%28Long%20CoT%29%20is%20now%20fundamental%20to%20state-of-the-art%20LLMs%2C%20especially%20in%20mathematical%20reasoning.%20However%2C%20LLM%20generation%20is%20highly%20sequential%2C%20and%20long%20CoTs%20lead%20to%20a%20high%20latency.%20We%20propose%20to%20train%20Divide-and-Conquer%20CoT%20%28DC-CoT%29%20to%20reduce%20the%20latency.%20With%20DC-CoT%2C%20the%20model%20can%20act%20as%20a%20director%20that%20identifies%20distinct%20subtasks%20that%20can%20be%20performed%20in%20parallel%20in%20its%20reasoning%20process%2C%20and%20then%20spawns%20workers%20to%20execute%20the%20subtasks.%20Our%20goal%20is%20to%20achieve%20high%20accuracy%2C%20with%20a%20low%20longest%20path%20length%2C%20which%20is%20a%20theoretical%20measure%20of%20the%20latency%20needed%20for%20the%20response.%20We%20start%20with%20a%20long%20CoT%20base%20model%20%28DeepScaleR-1.5B-Preview%29%2C%20and%20first%20use%20SFT%20with%20a%20small%20curated%20demonstration%20set%20to%20initialize%20its%20ability%20to%20spawn%20workers%20in%20a%20certain%20format.%20Because%20SFT%20degrades%20the%20accuracy%20significantly%2C%20we%20design%20a%20multi-stage%20RL%20algorithm%2C%20with%20various%20data%20filtering%20strategies%2C%20to%20recover%20the%20accuracy%20while%20decreasing%20the%20longest%20path%20length.%20Across%20several%20benchmarks%20including%20AIME%202024%20and%20HMMT%202025%2C%20DC-CoT%20achieves%20similar%20accuracy%20as%20DeepScaleR-1.5B-Preview%20while%20decreasing%20longest%20path%20length%20by%2035-40%25.%20Our%20code%2C%20SFT%20dataset%20and%20models%20are%20publicly%20available%20at%20https%3A//github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide-and-Conquer%2520CoT%253A%2520RL%2520for%2520Reducing%2520Latency%2520via%2520Parallel%2520Reasoning%26entry.906535625%3DArvind%2520Mahankali%2520and%2520Kaiyue%2520Wen%2520and%2520Tengyu%2520Ma%26entry.1292438233%3DLong%2520chain-of-thought%2520reasoning%2520%2528Long%2520CoT%2529%2520is%2520now%2520fundamental%2520to%2520state-of-the-art%2520LLMs%252C%2520especially%2520in%2520mathematical%2520reasoning.%2520However%252C%2520LLM%2520generation%2520is%2520highly%2520sequential%252C%2520and%2520long%2520CoTs%2520lead%2520to%2520a%2520high%2520latency.%2520We%2520propose%2520to%2520train%2520Divide-and-Conquer%2520CoT%2520%2528DC-CoT%2529%2520to%2520reduce%2520the%2520latency.%2520With%2520DC-CoT%252C%2520the%2520model%2520can%2520act%2520as%2520a%2520director%2520that%2520identifies%2520distinct%2520subtasks%2520that%2520can%2520be%2520performed%2520in%2520parallel%2520in%2520its%2520reasoning%2520process%252C%2520and%2520then%2520spawns%2520workers%2520to%2520execute%2520the%2520subtasks.%2520Our%2520goal%2520is%2520to%2520achieve%2520high%2520accuracy%252C%2520with%2520a%2520low%2520longest%2520path%2520length%252C%2520which%2520is%2520a%2520theoretical%2520measure%2520of%2520the%2520latency%2520needed%2520for%2520the%2520response.%2520We%2520start%2520with%2520a%2520long%2520CoT%2520base%2520model%2520%2528DeepScaleR-1.5B-Preview%2529%252C%2520and%2520first%2520use%2520SFT%2520with%2520a%2520small%2520curated%2520demonstration%2520set%2520to%2520initialize%2520its%2520ability%2520to%2520spawn%2520workers%2520in%2520a%2520certain%2520format.%2520Because%2520SFT%2520degrades%2520the%2520accuracy%2520significantly%252C%2520we%2520design%2520a%2520multi-stage%2520RL%2520algorithm%252C%2520with%2520various%2520data%2520filtering%2520strategies%252C%2520to%2520recover%2520the%2520accuracy%2520while%2520decreasing%2520the%2520longest%2520path%2520length.%2520Across%2520several%2520benchmarks%2520including%2520AIME%25202024%2520and%2520HMMT%25202025%252C%2520DC-CoT%2520achieves%2520similar%2520accuracy%2520as%2520DeepScaleR-1.5B-Preview%2520while%2520decreasing%2520longest%2520path%2520length%2520by%252035-40%2525.%2520Our%2520code%252C%2520SFT%2520dataset%2520and%2520models%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide-and-Conquer%20CoT%3A%20RL%20for%20Reducing%20Latency%20via%20Parallel%20Reasoning&entry.906535625=Arvind%20Mahankali%20and%20Kaiyue%20Wen%20and%20Tengyu%20Ma&entry.1292438233=Long%20chain-of-thought%20reasoning%20%28Long%20CoT%29%20is%20now%20fundamental%20to%20state-of-the-art%20LLMs%2C%20especially%20in%20mathematical%20reasoning.%20However%2C%20LLM%20generation%20is%20highly%20sequential%2C%20and%20long%20CoTs%20lead%20to%20a%20high%20latency.%20We%20propose%20to%20train%20Divide-and-Conquer%20CoT%20%28DC-CoT%29%20to%20reduce%20the%20latency.%20With%20DC-CoT%2C%20the%20model%20can%20act%20as%20a%20director%20that%20identifies%20distinct%20subtasks%20that%20can%20be%20performed%20in%20parallel%20in%20its%20reasoning%20process%2C%20and%20then%20spawns%20workers%20to%20execute%20the%20subtasks.%20Our%20goal%20is%20to%20achieve%20high%20accuracy%2C%20with%20a%20low%20longest%20path%20length%2C%20which%20is%20a%20theoretical%20measure%20of%20the%20latency%20needed%20for%20the%20response.%20We%20start%20with%20a%20long%20CoT%20base%20model%20%28DeepScaleR-1.5B-Preview%29%2C%20and%20first%20use%20SFT%20with%20a%20small%20curated%20demonstration%20set%20to%20initialize%20its%20ability%20to%20spawn%20workers%20in%20a%20certain%20format.%20Because%20SFT%20degrades%20the%20accuracy%20significantly%2C%20we%20design%20a%20multi-stage%20RL%20algorithm%2C%20with%20various%20data%20filtering%20strategies%2C%20to%20recover%20the%20accuracy%20while%20decreasing%20the%20longest%20path%20length.%20Across%20several%20benchmarks%20including%20AIME%202024%20and%20HMMT%202025%2C%20DC-CoT%20achieves%20similar%20accuracy%20as%20DeepScaleR-1.5B-Preview%20while%20decreasing%20longest%20path%20length%20by%2035-40%25.%20Our%20code%2C%20SFT%20dataset%20and%20models%20are%20publicly%20available%20at%20https%3A//github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2601.23027v1&entry.124074799=Read"},
{"title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing", "author": "Yinzhi Zhao and Ming Wang and Shi Feng and Xiaocui Yang and Daling Wang and Yifei Zhang", "abstract": "Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.", "link": "http://arxiv.org/abs/2601.10543v2", "date": "2026-01-30", "relevancy": 2.0524, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defending%20Large%20Language%20Models%20Against%20Jailbreak%20Attacks%20via%20In-Decoding%20Safety-Awareness%20Probing&body=Title%3A%20Defending%20Large%20Language%20Models%20Against%20Jailbreak%20Attacks%20via%20In-Decoding%20Safety-Awareness%20Probing%0AAuthor%3A%20Yinzhi%20Zhao%20and%20Ming%20Wang%20and%20Shi%20Feng%20and%20Xiaocui%20Yang%20and%20Daling%20Wang%20and%20Yifei%20Zhang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20across%20natural%20language%20tasks%20and%20are%20increasingly%20deployed%20in%20real-world%20applications.%20Despite%20extensive%20safety%20alignment%20efforts%2C%20recent%20studies%20show%20that%20such%20alignment%20is%20often%20shallow%20and%20remains%20vulnerable%20to%20jailbreak%20attacks.%20Existing%20defense%20mechanisms%2C%20including%20decoding-based%20constraints%20and%20post-hoc%20content%20detectors%2C%20struggle%20against%20sophisticated%20jailbreaks%2C%20often%20intervening%20robust%20detection%20or%20excessively%20degrading%20model%20utility.%20In%20this%20work%2C%20we%20examine%20the%20decoding%20process%20of%20LLMs%20and%20make%20a%20key%20observation%3A%20even%20when%20successfully%20jailbroken%2C%20models%20internally%20exhibit%20latent%20safety-related%20signals%20during%20generation.%20However%2C%20these%20signals%20are%20overridden%20by%20the%20model%27s%20drive%20for%20fluent%20continuation%2C%20preventing%20timely%20self-correction%20or%20refusal.%20Building%20on%20this%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20approach%20that%20explicitly%20surfaces%20and%20leverages%20these%20latent%20safety%20signals%20for%20early%20detection%20of%20unsafe%20content%20during%20decoding.%20Experiments%20across%20diverse%20jailbreak%20attacks%20demonstrate%20that%20our%20approach%20significantly%20enhances%20safety%2C%20while%20maintaining%20low%20over-refusal%20rates%20on%20benign%20inputs%20and%20preserving%20response%20quality.%20Our%20results%20suggest%20that%20activating%20intrinsic%20safety-awareness%20during%20decoding%20offers%20a%20promising%20and%20complementary%20direction%20for%20defending%20against%20jailbreak%20attacks.%20Code%20is%20available%20at%3A%20https%3A//github.com/zyz13590/SafeProbing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefending%2520Large%2520Language%2520Models%2520Against%2520Jailbreak%2520Attacks%2520via%2520In-Decoding%2520Safety-Awareness%2520Probing%26entry.906535625%3DYinzhi%2520Zhao%2520and%2520Ming%2520Wang%2520and%2520Shi%2520Feng%2520and%2520Xiaocui%2520Yang%2520and%2520Daling%2520Wang%2520and%2520Yifei%2520Zhang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520across%2520natural%2520language%2520tasks%2520and%2520are%2520increasingly%2520deployed%2520in%2520real-world%2520applications.%2520Despite%2520extensive%2520safety%2520alignment%2520efforts%252C%2520recent%2520studies%2520show%2520that%2520such%2520alignment%2520is%2520often%2520shallow%2520and%2520remains%2520vulnerable%2520to%2520jailbreak%2520attacks.%2520Existing%2520defense%2520mechanisms%252C%2520including%2520decoding-based%2520constraints%2520and%2520post-hoc%2520content%2520detectors%252C%2520struggle%2520against%2520sophisticated%2520jailbreaks%252C%2520often%2520intervening%2520robust%2520detection%2520or%2520excessively%2520degrading%2520model%2520utility.%2520In%2520this%2520work%252C%2520we%2520examine%2520the%2520decoding%2520process%2520of%2520LLMs%2520and%2520make%2520a%2520key%2520observation%253A%2520even%2520when%2520successfully%2520jailbroken%252C%2520models%2520internally%2520exhibit%2520latent%2520safety-related%2520signals%2520during%2520generation.%2520However%252C%2520these%2520signals%2520are%2520overridden%2520by%2520the%2520model%2527s%2520drive%2520for%2520fluent%2520continuation%252C%2520preventing%2520timely%2520self-correction%2520or%2520refusal.%2520Building%2520on%2520this%2520observation%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520approach%2520that%2520explicitly%2520surfaces%2520and%2520leverages%2520these%2520latent%2520safety%2520signals%2520for%2520early%2520detection%2520of%2520unsafe%2520content%2520during%2520decoding.%2520Experiments%2520across%2520diverse%2520jailbreak%2520attacks%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520safety%252C%2520while%2520maintaining%2520low%2520over-refusal%2520rates%2520on%2520benign%2520inputs%2520and%2520preserving%2520response%2520quality.%2520Our%2520results%2520suggest%2520that%2520activating%2520intrinsic%2520safety-awareness%2520during%2520decoding%2520offers%2520a%2520promising%2520and%2520complementary%2520direction%2520for%2520defending%2520against%2520jailbreak%2520attacks.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/zyz13590/SafeProbing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defending%20Large%20Language%20Models%20Against%20Jailbreak%20Attacks%20via%20In-Decoding%20Safety-Awareness%20Probing&entry.906535625=Yinzhi%20Zhao%20and%20Ming%20Wang%20and%20Shi%20Feng%20and%20Xiaocui%20Yang%20and%20Daling%20Wang%20and%20Yifei%20Zhang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20across%20natural%20language%20tasks%20and%20are%20increasingly%20deployed%20in%20real-world%20applications.%20Despite%20extensive%20safety%20alignment%20efforts%2C%20recent%20studies%20show%20that%20such%20alignment%20is%20often%20shallow%20and%20remains%20vulnerable%20to%20jailbreak%20attacks.%20Existing%20defense%20mechanisms%2C%20including%20decoding-based%20constraints%20and%20post-hoc%20content%20detectors%2C%20struggle%20against%20sophisticated%20jailbreaks%2C%20often%20intervening%20robust%20detection%20or%20excessively%20degrading%20model%20utility.%20In%20this%20work%2C%20we%20examine%20the%20decoding%20process%20of%20LLMs%20and%20make%20a%20key%20observation%3A%20even%20when%20successfully%20jailbroken%2C%20models%20internally%20exhibit%20latent%20safety-related%20signals%20during%20generation.%20However%2C%20these%20signals%20are%20overridden%20by%20the%20model%27s%20drive%20for%20fluent%20continuation%2C%20preventing%20timely%20self-correction%20or%20refusal.%20Building%20on%20this%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20approach%20that%20explicitly%20surfaces%20and%20leverages%20these%20latent%20safety%20signals%20for%20early%20detection%20of%20unsafe%20content%20during%20decoding.%20Experiments%20across%20diverse%20jailbreak%20attacks%20demonstrate%20that%20our%20approach%20significantly%20enhances%20safety%2C%20while%20maintaining%20low%20over-refusal%20rates%20on%20benign%20inputs%20and%20preserving%20response%20quality.%20Our%20results%20suggest%20that%20activating%20intrinsic%20safety-awareness%20during%20decoding%20offers%20a%20promising%20and%20complementary%20direction%20for%20defending%20against%20jailbreak%20attacks.%20Code%20is%20available%20at%3A%20https%3A//github.com/zyz13590/SafeProbing.&entry.1838667208=http%3A//arxiv.org/abs/2601.10543v2&entry.124074799=Read"},
{"title": "Evaluating Large Language Models for Security Bug Report Prediction", "author": "Farnaz Soltaniani and Shoaib Razzaq and Mohammad Ghafari", "abstract": "Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches. Prompted proprietary models demonstrate the highest sensitivity to SBRs, achieving a G-measure of 77% and a recall of 74% on average across all the datasets, albeit at the cost of a higher false-positive rate, resulting in an average precision of only 22%. Fine-tuned models, by contrast, exhibit the opposite behavior, attaining a lower overall G-measure of 51% but substantially higher precision of 75% at the cost of reduced recall of 36%. Though a one-time investment in building fine-tuned models is necessary, the inference on the largest dataset is up to 50 times faster than that of proprietary models. These findings suggest that further investigations to harness the power of LLMs for SBR prediction are necessary.", "link": "http://arxiv.org/abs/2601.22921v1", "date": "2026-01-30", "relevancy": 1.6952, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Large%20Language%20Models%20for%20Security%20Bug%20Report%20Prediction&body=Title%3A%20Evaluating%20Large%20Language%20Models%20for%20Security%20Bug%20Report%20Prediction%0AAuthor%3A%20Farnaz%20Soltaniani%20and%20Shoaib%20Razzaq%20and%20Mohammad%20Ghafari%0AAbstract%3A%20Early%20detection%20of%20security%20bug%20reports%20%28SBRs%29%20is%20critical%20for%20timely%20vulnerability%20mitigation.%20We%20present%20an%20evaluation%20of%20prompt-based%20engineering%20and%20fine-tuning%20approaches%20for%20predicting%20SBRs%20using%20Large%20Language%20Models%20%28LLMs%29.%20Our%20findings%20reveal%20a%20distinct%20trade-off%20between%20the%20two%20approaches.%20Prompted%20proprietary%20models%20demonstrate%20the%20highest%20sensitivity%20to%20SBRs%2C%20achieving%20a%20G-measure%20of%2077%25%20and%20a%20recall%20of%2074%25%20on%20average%20across%20all%20the%20datasets%2C%20albeit%20at%20the%20cost%20of%20a%20higher%20false-positive%20rate%2C%20resulting%20in%20an%20average%20precision%20of%20only%2022%25.%20Fine-tuned%20models%2C%20by%20contrast%2C%20exhibit%20the%20opposite%20behavior%2C%20attaining%20a%20lower%20overall%20G-measure%20of%2051%25%20but%20substantially%20higher%20precision%20of%2075%25%20at%20the%20cost%20of%20reduced%20recall%20of%2036%25.%20Though%20a%20one-time%20investment%20in%20building%20fine-tuned%20models%20is%20necessary%2C%20the%20inference%20on%20the%20largest%20dataset%20is%20up%20to%2050%20times%20faster%20than%20that%20of%20proprietary%20models.%20These%20findings%20suggest%20that%20further%20investigations%20to%20harness%20the%20power%20of%20LLMs%20for%20SBR%20prediction%20are%20necessary.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Large%2520Language%2520Models%2520for%2520Security%2520Bug%2520Report%2520Prediction%26entry.906535625%3DFarnaz%2520Soltaniani%2520and%2520Shoaib%2520Razzaq%2520and%2520Mohammad%2520Ghafari%26entry.1292438233%3DEarly%2520detection%2520of%2520security%2520bug%2520reports%2520%2528SBRs%2529%2520is%2520critical%2520for%2520timely%2520vulnerability%2520mitigation.%2520We%2520present%2520an%2520evaluation%2520of%2520prompt-based%2520engineering%2520and%2520fine-tuning%2520approaches%2520for%2520predicting%2520SBRs%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Our%2520findings%2520reveal%2520a%2520distinct%2520trade-off%2520between%2520the%2520two%2520approaches.%2520Prompted%2520proprietary%2520models%2520demonstrate%2520the%2520highest%2520sensitivity%2520to%2520SBRs%252C%2520achieving%2520a%2520G-measure%2520of%252077%2525%2520and%2520a%2520recall%2520of%252074%2525%2520on%2520average%2520across%2520all%2520the%2520datasets%252C%2520albeit%2520at%2520the%2520cost%2520of%2520a%2520higher%2520false-positive%2520rate%252C%2520resulting%2520in%2520an%2520average%2520precision%2520of%2520only%252022%2525.%2520Fine-tuned%2520models%252C%2520by%2520contrast%252C%2520exhibit%2520the%2520opposite%2520behavior%252C%2520attaining%2520a%2520lower%2520overall%2520G-measure%2520of%252051%2525%2520but%2520substantially%2520higher%2520precision%2520of%252075%2525%2520at%2520the%2520cost%2520of%2520reduced%2520recall%2520of%252036%2525.%2520Though%2520a%2520one-time%2520investment%2520in%2520building%2520fine-tuned%2520models%2520is%2520necessary%252C%2520the%2520inference%2520on%2520the%2520largest%2520dataset%2520is%2520up%2520to%252050%2520times%2520faster%2520than%2520that%2520of%2520proprietary%2520models.%2520These%2520findings%2520suggest%2520that%2520further%2520investigations%2520to%2520harness%2520the%2520power%2520of%2520LLMs%2520for%2520SBR%2520prediction%2520are%2520necessary.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Large%20Language%20Models%20for%20Security%20Bug%20Report%20Prediction&entry.906535625=Farnaz%20Soltaniani%20and%20Shoaib%20Razzaq%20and%20Mohammad%20Ghafari&entry.1292438233=Early%20detection%20of%20security%20bug%20reports%20%28SBRs%29%20is%20critical%20for%20timely%20vulnerability%20mitigation.%20We%20present%20an%20evaluation%20of%20prompt-based%20engineering%20and%20fine-tuning%20approaches%20for%20predicting%20SBRs%20using%20Large%20Language%20Models%20%28LLMs%29.%20Our%20findings%20reveal%20a%20distinct%20trade-off%20between%20the%20two%20approaches.%20Prompted%20proprietary%20models%20demonstrate%20the%20highest%20sensitivity%20to%20SBRs%2C%20achieving%20a%20G-measure%20of%2077%25%20and%20a%20recall%20of%2074%25%20on%20average%20across%20all%20the%20datasets%2C%20albeit%20at%20the%20cost%20of%20a%20higher%20false-positive%20rate%2C%20resulting%20in%20an%20average%20precision%20of%20only%2022%25.%20Fine-tuned%20models%2C%20by%20contrast%2C%20exhibit%20the%20opposite%20behavior%2C%20attaining%20a%20lower%20overall%20G-measure%20of%2051%25%20but%20substantially%20higher%20precision%20of%2075%25%20at%20the%20cost%20of%20reduced%20recall%20of%2036%25.%20Though%20a%20one-time%20investment%20in%20building%20fine-tuned%20models%20is%20necessary%2C%20the%20inference%20on%20the%20largest%20dataset%20is%20up%20to%2050%20times%20faster%20than%20that%20of%20proprietary%20models.%20These%20findings%20suggest%20that%20further%20investigations%20to%20harness%20the%20power%20of%20LLMs%20for%20SBR%20prediction%20are%20necessary.&entry.1838667208=http%3A//arxiv.org/abs/2601.22921v1&entry.124074799=Read"},
{"title": "From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation", "author": "Devon Levy and Bar Assayag and Laura Gaspar and Ilan Shimshoni and Bella Specktor-Fadida", "abstract": "Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.", "link": "http://arxiv.org/abs/2601.18532v2", "date": "2026-01-30", "relevancy": 2.1006, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5503}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Cold%20Start%20to%20Active%20Learning%3A%20Embedding-Based%20Scan%20Selection%20for%20Medical%20Image%20Segmentation&body=Title%3A%20From%20Cold%20Start%20to%20Active%20Learning%3A%20Embedding-Based%20Scan%20Selection%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Devon%20Levy%20and%20Bar%20Assayag%20and%20Laura%20Gaspar%20and%20Ilan%20Shimshoni%20and%20Bella%20Specktor-Fadida%0AAbstract%3A%20Accurate%20segmentation%20annotations%20are%20critical%20for%20disease%20monitoring%2C%20yet%20manual%20labeling%20remains%20a%20major%20bottleneck%20due%20to%20the%20time%20and%20expertise%20required.%20Active%20learning%20%28AL%29%20alleviates%20this%20burden%20by%20prioritizing%20informative%20samples%20for%20annotation%2C%20typically%20through%20a%20diversity-based%20cold-start%20phase%20followed%20by%20uncertainty-driven%20selection.%20We%20propose%20a%20novel%20cold-start%20sampling%20strategy%20that%20combines%20foundation-model%20embeddings%20with%20clustering%2C%20including%20automatic%20selection%20of%20the%20number%20of%20clusters%20and%20proportional%20sampling%20across%20clusters%2C%20to%20construct%20a%20diverse%20and%20representative%20initial%20training.%20This%20is%20followed%20by%20an%20uncertainty-based%20AL%20framework%20that%20integrates%20spatial%20diversity%20to%20guide%20sample%20selection.%20The%20proposed%20method%20is%20intuitive%20and%20interpretable%2C%20enabling%20visualization%20of%20the%20feature-space%20distribution%20of%20candidate%20samples.%20We%20evaluate%20our%20approach%20on%20three%20datasets%20spanning%20X-ray%20and%20MRI%20modalities.%20On%20the%20CheXmask%20dataset%2C%20the%20cold-start%20strategy%20outperforms%20random%20selection%2C%20improving%20Dice%20from%200.918%20to%200.929%20and%20reducing%20the%20Hausdorff%20distance%20from%2032.41%20to%2027.66%20mm.%20In%20the%20AL%20setting%2C%20combined%20entropy%20and%20diversity%20selection%20improves%20Dice%20from%200.919%20to%200.939%20and%20reduces%20the%20Hausdorff%20distance%20from%2030.10%20to%2019.16%20mm.%20On%20the%20Montgomery%20dataset%2C%20cold-start%20gains%20are%20substantial%2C%20with%20Dice%20improving%20from%200.928%20to%200.950%20and%20Hausdorff%20distance%20decreasing%20from%2014.22%20to%209.38%20mm.%20On%20the%20SynthStrip%20dataset%2C%20cold-start%20selection%20slightly%20affects%20Dice%20but%20reduces%20the%20Hausdorff%20distance%20from%209.43%20to%208.69%20mm%2C%20while%20active%20learning%20improves%20Dice%20from%200.816%20to%200.826%20and%20reduces%20the%20Hausdorff%20distance%20from%207.76%20to%206.38%20mm.%20Overall%2C%20the%20proposed%20framework%20consistently%20outperforms%20baseline%20methods%20in%20low-data%20regimes%2C%20improving%20segmentation%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18532v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Cold%2520Start%2520to%2520Active%2520Learning%253A%2520Embedding-Based%2520Scan%2520Selection%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DDevon%2520Levy%2520and%2520Bar%2520Assayag%2520and%2520Laura%2520Gaspar%2520and%2520Ilan%2520Shimshoni%2520and%2520Bella%2520Specktor-Fadida%26entry.1292438233%3DAccurate%2520segmentation%2520annotations%2520are%2520critical%2520for%2520disease%2520monitoring%252C%2520yet%2520manual%2520labeling%2520remains%2520a%2520major%2520bottleneck%2520due%2520to%2520the%2520time%2520and%2520expertise%2520required.%2520Active%2520learning%2520%2528AL%2529%2520alleviates%2520this%2520burden%2520by%2520prioritizing%2520informative%2520samples%2520for%2520annotation%252C%2520typically%2520through%2520a%2520diversity-based%2520cold-start%2520phase%2520followed%2520by%2520uncertainty-driven%2520selection.%2520We%2520propose%2520a%2520novel%2520cold-start%2520sampling%2520strategy%2520that%2520combines%2520foundation-model%2520embeddings%2520with%2520clustering%252C%2520including%2520automatic%2520selection%2520of%2520the%2520number%2520of%2520clusters%2520and%2520proportional%2520sampling%2520across%2520clusters%252C%2520to%2520construct%2520a%2520diverse%2520and%2520representative%2520initial%2520training.%2520This%2520is%2520followed%2520by%2520an%2520uncertainty-based%2520AL%2520framework%2520that%2520integrates%2520spatial%2520diversity%2520to%2520guide%2520sample%2520selection.%2520The%2520proposed%2520method%2520is%2520intuitive%2520and%2520interpretable%252C%2520enabling%2520visualization%2520of%2520the%2520feature-space%2520distribution%2520of%2520candidate%2520samples.%2520We%2520evaluate%2520our%2520approach%2520on%2520three%2520datasets%2520spanning%2520X-ray%2520and%2520MRI%2520modalities.%2520On%2520the%2520CheXmask%2520dataset%252C%2520the%2520cold-start%2520strategy%2520outperforms%2520random%2520selection%252C%2520improving%2520Dice%2520from%25200.918%2520to%25200.929%2520and%2520reducing%2520the%2520Hausdorff%2520distance%2520from%252032.41%2520to%252027.66%2520mm.%2520In%2520the%2520AL%2520setting%252C%2520combined%2520entropy%2520and%2520diversity%2520selection%2520improves%2520Dice%2520from%25200.919%2520to%25200.939%2520and%2520reduces%2520the%2520Hausdorff%2520distance%2520from%252030.10%2520to%252019.16%2520mm.%2520On%2520the%2520Montgomery%2520dataset%252C%2520cold-start%2520gains%2520are%2520substantial%252C%2520with%2520Dice%2520improving%2520from%25200.928%2520to%25200.950%2520and%2520Hausdorff%2520distance%2520decreasing%2520from%252014.22%2520to%25209.38%2520mm.%2520On%2520the%2520SynthStrip%2520dataset%252C%2520cold-start%2520selection%2520slightly%2520affects%2520Dice%2520but%2520reduces%2520the%2520Hausdorff%2520distance%2520from%25209.43%2520to%25208.69%2520mm%252C%2520while%2520active%2520learning%2520improves%2520Dice%2520from%25200.816%2520to%25200.826%2520and%2520reduces%2520the%2520Hausdorff%2520distance%2520from%25207.76%2520to%25206.38%2520mm.%2520Overall%252C%2520the%2520proposed%2520framework%2520consistently%2520outperforms%2520baseline%2520methods%2520in%2520low-data%2520regimes%252C%2520improving%2520segmentation%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18532v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Cold%20Start%20to%20Active%20Learning%3A%20Embedding-Based%20Scan%20Selection%20for%20Medical%20Image%20Segmentation&entry.906535625=Devon%20Levy%20and%20Bar%20Assayag%20and%20Laura%20Gaspar%20and%20Ilan%20Shimshoni%20and%20Bella%20Specktor-Fadida&entry.1292438233=Accurate%20segmentation%20annotations%20are%20critical%20for%20disease%20monitoring%2C%20yet%20manual%20labeling%20remains%20a%20major%20bottleneck%20due%20to%20the%20time%20and%20expertise%20required.%20Active%20learning%20%28AL%29%20alleviates%20this%20burden%20by%20prioritizing%20informative%20samples%20for%20annotation%2C%20typically%20through%20a%20diversity-based%20cold-start%20phase%20followed%20by%20uncertainty-driven%20selection.%20We%20propose%20a%20novel%20cold-start%20sampling%20strategy%20that%20combines%20foundation-model%20embeddings%20with%20clustering%2C%20including%20automatic%20selection%20of%20the%20number%20of%20clusters%20and%20proportional%20sampling%20across%20clusters%2C%20to%20construct%20a%20diverse%20and%20representative%20initial%20training.%20This%20is%20followed%20by%20an%20uncertainty-based%20AL%20framework%20that%20integrates%20spatial%20diversity%20to%20guide%20sample%20selection.%20The%20proposed%20method%20is%20intuitive%20and%20interpretable%2C%20enabling%20visualization%20of%20the%20feature-space%20distribution%20of%20candidate%20samples.%20We%20evaluate%20our%20approach%20on%20three%20datasets%20spanning%20X-ray%20and%20MRI%20modalities.%20On%20the%20CheXmask%20dataset%2C%20the%20cold-start%20strategy%20outperforms%20random%20selection%2C%20improving%20Dice%20from%200.918%20to%200.929%20and%20reducing%20the%20Hausdorff%20distance%20from%2032.41%20to%2027.66%20mm.%20In%20the%20AL%20setting%2C%20combined%20entropy%20and%20diversity%20selection%20improves%20Dice%20from%200.919%20to%200.939%20and%20reduces%20the%20Hausdorff%20distance%20from%2030.10%20to%2019.16%20mm.%20On%20the%20Montgomery%20dataset%2C%20cold-start%20gains%20are%20substantial%2C%20with%20Dice%20improving%20from%200.928%20to%200.950%20and%20Hausdorff%20distance%20decreasing%20from%2014.22%20to%209.38%20mm.%20On%20the%20SynthStrip%20dataset%2C%20cold-start%20selection%20slightly%20affects%20Dice%20but%20reduces%20the%20Hausdorff%20distance%20from%209.43%20to%208.69%20mm%2C%20while%20active%20learning%20improves%20Dice%20from%200.816%20to%200.826%20and%20reduces%20the%20Hausdorff%20distance%20from%207.76%20to%206.38%20mm.%20Overall%2C%20the%20proposed%20framework%20consistently%20outperforms%20baseline%20methods%20in%20low-data%20regimes%2C%20improving%20segmentation%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2601.18532v2&entry.124074799=Read"},
{"title": "On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care", "author": "Joel Romero-Hernandez and Oscar Camara", "abstract": "Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.", "link": "http://arxiv.org/abs/2601.23154v1", "date": "2026-01-30", "relevancy": 1.3382, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4493}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4426}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Safer%20Reinforcement%20Learning%20Policies%20for%20Sedation%20and%20Analgesia%20in%20Intensive%20Care&body=Title%3A%20On%20Safer%20Reinforcement%20Learning%20Policies%20for%20Sedation%20and%20Analgesia%20in%20Intensive%20Care%0AAuthor%3A%20Joel%20Romero-Hernandez%20and%20Oscar%20Camara%0AAbstract%3A%20Pain%20management%20in%20intensive%20care%20usually%20involves%20complex%20trade-offs%20between%20therapeutic%20goals%20and%20patient%20safety%2C%20since%20both%20inadequate%20and%20excessive%20treatment%20may%20induce%20serious%20sequelae.%20Reinforcement%20learning%20can%20help%20address%20this%20challenge%20by%20learning%20medication%20dosing%20policies%20from%20retrospective%20data.%20However%2C%20prior%20work%20on%20sedation%20and%20analgesia%20has%20optimized%20for%20objectives%20that%20do%20not%20value%20patient%20survival%20while%20relying%20on%20algorithms%20unsuitable%20for%20imperfect%20information%20settings.%20We%20investigated%20the%20risks%20of%20these%20design%20choices%20by%20implementing%20a%20deep%20reinforcement%20learning%20framework%20to%20suggest%20hourly%20medication%20doses%20under%20partial%20observability.%20Using%20data%20from%2047%2C144%20ICU%20stays%20in%20the%20MIMIC-IV%20database%2C%20we%20trained%20policies%20to%20prescribe%20opioids%2C%20propofol%2C%20benzodiazepines%2C%20and%20dexmedetomidine%20according%20to%20two%20goals%3A%20reduce%20pain%20or%20jointly%20reduce%20pain%20and%20mortality.%20We%20found%20that%2C%20although%20the%20two%20policies%20were%20associated%20with%20lower%20pain%2C%20actions%20from%20the%20first%20policy%20were%20positively%20correlated%20with%20mortality%2C%20while%20those%20proposed%20by%20the%20second%20policy%20were%20negatively%20correlated.%20This%20suggests%20that%20valuing%20long-term%20outcomes%20could%20be%20critical%20for%20safer%20treatment%20policies%2C%20even%20if%20a%20short-term%20goal%20remains%20the%20primary%20objective.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Safer%2520Reinforcement%2520Learning%2520Policies%2520for%2520Sedation%2520and%2520Analgesia%2520in%2520Intensive%2520Care%26entry.906535625%3DJoel%2520Romero-Hernandez%2520and%2520Oscar%2520Camara%26entry.1292438233%3DPain%2520management%2520in%2520intensive%2520care%2520usually%2520involves%2520complex%2520trade-offs%2520between%2520therapeutic%2520goals%2520and%2520patient%2520safety%252C%2520since%2520both%2520inadequate%2520and%2520excessive%2520treatment%2520may%2520induce%2520serious%2520sequelae.%2520Reinforcement%2520learning%2520can%2520help%2520address%2520this%2520challenge%2520by%2520learning%2520medication%2520dosing%2520policies%2520from%2520retrospective%2520data.%2520However%252C%2520prior%2520work%2520on%2520sedation%2520and%2520analgesia%2520has%2520optimized%2520for%2520objectives%2520that%2520do%2520not%2520value%2520patient%2520survival%2520while%2520relying%2520on%2520algorithms%2520unsuitable%2520for%2520imperfect%2520information%2520settings.%2520We%2520investigated%2520the%2520risks%2520of%2520these%2520design%2520choices%2520by%2520implementing%2520a%2520deep%2520reinforcement%2520learning%2520framework%2520to%2520suggest%2520hourly%2520medication%2520doses%2520under%2520partial%2520observability.%2520Using%2520data%2520from%252047%252C144%2520ICU%2520stays%2520in%2520the%2520MIMIC-IV%2520database%252C%2520we%2520trained%2520policies%2520to%2520prescribe%2520opioids%252C%2520propofol%252C%2520benzodiazepines%252C%2520and%2520dexmedetomidine%2520according%2520to%2520two%2520goals%253A%2520reduce%2520pain%2520or%2520jointly%2520reduce%2520pain%2520and%2520mortality.%2520We%2520found%2520that%252C%2520although%2520the%2520two%2520policies%2520were%2520associated%2520with%2520lower%2520pain%252C%2520actions%2520from%2520the%2520first%2520policy%2520were%2520positively%2520correlated%2520with%2520mortality%252C%2520while%2520those%2520proposed%2520by%2520the%2520second%2520policy%2520were%2520negatively%2520correlated.%2520This%2520suggests%2520that%2520valuing%2520long-term%2520outcomes%2520could%2520be%2520critical%2520for%2520safer%2520treatment%2520policies%252C%2520even%2520if%2520a%2520short-term%2520goal%2520remains%2520the%2520primary%2520objective.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Safer%20Reinforcement%20Learning%20Policies%20for%20Sedation%20and%20Analgesia%20in%20Intensive%20Care&entry.906535625=Joel%20Romero-Hernandez%20and%20Oscar%20Camara&entry.1292438233=Pain%20management%20in%20intensive%20care%20usually%20involves%20complex%20trade-offs%20between%20therapeutic%20goals%20and%20patient%20safety%2C%20since%20both%20inadequate%20and%20excessive%20treatment%20may%20induce%20serious%20sequelae.%20Reinforcement%20learning%20can%20help%20address%20this%20challenge%20by%20learning%20medication%20dosing%20policies%20from%20retrospective%20data.%20However%2C%20prior%20work%20on%20sedation%20and%20analgesia%20has%20optimized%20for%20objectives%20that%20do%20not%20value%20patient%20survival%20while%20relying%20on%20algorithms%20unsuitable%20for%20imperfect%20information%20settings.%20We%20investigated%20the%20risks%20of%20these%20design%20choices%20by%20implementing%20a%20deep%20reinforcement%20learning%20framework%20to%20suggest%20hourly%20medication%20doses%20under%20partial%20observability.%20Using%20data%20from%2047%2C144%20ICU%20stays%20in%20the%20MIMIC-IV%20database%2C%20we%20trained%20policies%20to%20prescribe%20opioids%2C%20propofol%2C%20benzodiazepines%2C%20and%20dexmedetomidine%20according%20to%20two%20goals%3A%20reduce%20pain%20or%20jointly%20reduce%20pain%20and%20mortality.%20We%20found%20that%2C%20although%20the%20two%20policies%20were%20associated%20with%20lower%20pain%2C%20actions%20from%20the%20first%20policy%20were%20positively%20correlated%20with%20mortality%2C%20while%20those%20proposed%20by%20the%20second%20policy%20were%20negatively%20correlated.%20This%20suggests%20that%20valuing%20long-term%20outcomes%20could%20be%20critical%20for%20safer%20treatment%20policies%2C%20even%20if%20a%20short-term%20goal%20remains%20the%20primary%20objective.&entry.1838667208=http%3A//arxiv.org/abs/2601.23154v1&entry.124074799=Read"},
{"title": "Diverse Approaches to Optimal Execution Schedule Generation", "author": "Robert de Witt and Mikko S. Pakkanen", "abstract": "We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation, suggesting opportunities for ensemble approaches that combine improved specialists with the baseline PPO policy. Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computational resources per behavioural cell may be required for robust specialist development across all market conditions.\n  To ensure experimental integrity, we develop a calibrated Gymnasium environment focused on order scheduling rather than tactical placement decisions. The simulator features a transient impact model with exponential decay and square-root volume scaling, fit to 400+ U.S. equities with $R^2>0.02$ out-of-sample. Within this environment, two Proximal Policy Optimization architectures - both MLP and CNN feature extractors - demonstrate substantial improvements over industry baselines, with the CNN variant achieving 2.13 bps arrival slippage versus 5.23 bps for VWAP on 4,900 out-of-sample orders ($21B notional). These results validate both the simulation realism and provide strong single-policy baselines for quality-diversity methods.", "link": "http://arxiv.org/abs/2601.22113v2", "date": "2026-01-30", "relevancy": 1.7725, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4526}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4384}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diverse%20Approaches%20to%20Optimal%20Execution%20Schedule%20Generation&body=Title%3A%20Diverse%20Approaches%20to%20Optimal%20Execution%20Schedule%20Generation%0AAuthor%3A%20Robert%20de%20Witt%20and%20Mikko%20S.%20Pakkanen%0AAbstract%3A%20We%20present%20the%20first%20application%20of%20MAP-Elites%2C%20a%20quality-diversity%20algorithm%2C%20to%20trade%20execution.%20Rather%20than%20searching%20for%20a%20single%20optimal%20policy%2C%20MAP-Elites%20generates%20a%20diverse%20portfolio%20of%20regime-specialist%20strategies%20indexed%20by%20liquidity%20and%20volatility%20conditions.%20Individual%20specialists%20achieve%208-10%25%20performance%20improvements%20within%20their%20behavioural%20niches%2C%20while%20other%20cells%20show%20degradation%2C%20suggesting%20opportunities%20for%20ensemble%20approaches%20that%20combine%20improved%20specialists%20with%20the%20baseline%20PPO%20policy.%20Results%20indicate%20that%20quality-diversity%20methods%20offer%20promise%20for%20regime-adaptive%20execution%2C%20though%20substantial%20computational%20resources%20per%20behavioural%20cell%20may%20be%20required%20for%20robust%20specialist%20development%20across%20all%20market%20conditions.%0A%20%20To%20ensure%20experimental%20integrity%2C%20we%20develop%20a%20calibrated%20Gymnasium%20environment%20focused%20on%20order%20scheduling%20rather%20than%20tactical%20placement%20decisions.%20The%20simulator%20features%20a%20transient%20impact%20model%20with%20exponential%20decay%20and%20square-root%20volume%20scaling%2C%20fit%20to%20400%2B%20U.S.%20equities%20with%20%24R%5E2%3E0.02%24%20out-of-sample.%20Within%20this%20environment%2C%20two%20Proximal%20Policy%20Optimization%20architectures%20-%20both%20MLP%20and%20CNN%20feature%20extractors%20-%20demonstrate%20substantial%20improvements%20over%20industry%20baselines%2C%20with%20the%20CNN%20variant%20achieving%202.13%20bps%20arrival%20slippage%20versus%205.23%20bps%20for%20VWAP%20on%204%2C900%20out-of-sample%20orders%20%28%2421B%20notional%29.%20These%20results%20validate%20both%20the%20simulation%20realism%20and%20provide%20strong%20single-policy%20baselines%20for%20quality-diversity%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverse%2520Approaches%2520to%2520Optimal%2520Execution%2520Schedule%2520Generation%26entry.906535625%3DRobert%2520de%2520Witt%2520and%2520Mikko%2520S.%2520Pakkanen%26entry.1292438233%3DWe%2520present%2520the%2520first%2520application%2520of%2520MAP-Elites%252C%2520a%2520quality-diversity%2520algorithm%252C%2520to%2520trade%2520execution.%2520Rather%2520than%2520searching%2520for%2520a%2520single%2520optimal%2520policy%252C%2520MAP-Elites%2520generates%2520a%2520diverse%2520portfolio%2520of%2520regime-specialist%2520strategies%2520indexed%2520by%2520liquidity%2520and%2520volatility%2520conditions.%2520Individual%2520specialists%2520achieve%25208-10%2525%2520performance%2520improvements%2520within%2520their%2520behavioural%2520niches%252C%2520while%2520other%2520cells%2520show%2520degradation%252C%2520suggesting%2520opportunities%2520for%2520ensemble%2520approaches%2520that%2520combine%2520improved%2520specialists%2520with%2520the%2520baseline%2520PPO%2520policy.%2520Results%2520indicate%2520that%2520quality-diversity%2520methods%2520offer%2520promise%2520for%2520regime-adaptive%2520execution%252C%2520though%2520substantial%2520computational%2520resources%2520per%2520behavioural%2520cell%2520may%2520be%2520required%2520for%2520robust%2520specialist%2520development%2520across%2520all%2520market%2520conditions.%250A%2520%2520To%2520ensure%2520experimental%2520integrity%252C%2520we%2520develop%2520a%2520calibrated%2520Gymnasium%2520environment%2520focused%2520on%2520order%2520scheduling%2520rather%2520than%2520tactical%2520placement%2520decisions.%2520The%2520simulator%2520features%2520a%2520transient%2520impact%2520model%2520with%2520exponential%2520decay%2520and%2520square-root%2520volume%2520scaling%252C%2520fit%2520to%2520400%252B%2520U.S.%2520equities%2520with%2520%2524R%255E2%253E0.02%2524%2520out-of-sample.%2520Within%2520this%2520environment%252C%2520two%2520Proximal%2520Policy%2520Optimization%2520architectures%2520-%2520both%2520MLP%2520and%2520CNN%2520feature%2520extractors%2520-%2520demonstrate%2520substantial%2520improvements%2520over%2520industry%2520baselines%252C%2520with%2520the%2520CNN%2520variant%2520achieving%25202.13%2520bps%2520arrival%2520slippage%2520versus%25205.23%2520bps%2520for%2520VWAP%2520on%25204%252C900%2520out-of-sample%2520orders%2520%2528%252421B%2520notional%2529.%2520These%2520results%2520validate%2520both%2520the%2520simulation%2520realism%2520and%2520provide%2520strong%2520single-policy%2520baselines%2520for%2520quality-diversity%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diverse%20Approaches%20to%20Optimal%20Execution%20Schedule%20Generation&entry.906535625=Robert%20de%20Witt%20and%20Mikko%20S.%20Pakkanen&entry.1292438233=We%20present%20the%20first%20application%20of%20MAP-Elites%2C%20a%20quality-diversity%20algorithm%2C%20to%20trade%20execution.%20Rather%20than%20searching%20for%20a%20single%20optimal%20policy%2C%20MAP-Elites%20generates%20a%20diverse%20portfolio%20of%20regime-specialist%20strategies%20indexed%20by%20liquidity%20and%20volatility%20conditions.%20Individual%20specialists%20achieve%208-10%25%20performance%20improvements%20within%20their%20behavioural%20niches%2C%20while%20other%20cells%20show%20degradation%2C%20suggesting%20opportunities%20for%20ensemble%20approaches%20that%20combine%20improved%20specialists%20with%20the%20baseline%20PPO%20policy.%20Results%20indicate%20that%20quality-diversity%20methods%20offer%20promise%20for%20regime-adaptive%20execution%2C%20though%20substantial%20computational%20resources%20per%20behavioural%20cell%20may%20be%20required%20for%20robust%20specialist%20development%20across%20all%20market%20conditions.%0A%20%20To%20ensure%20experimental%20integrity%2C%20we%20develop%20a%20calibrated%20Gymnasium%20environment%20focused%20on%20order%20scheduling%20rather%20than%20tactical%20placement%20decisions.%20The%20simulator%20features%20a%20transient%20impact%20model%20with%20exponential%20decay%20and%20square-root%20volume%20scaling%2C%20fit%20to%20400%2B%20U.S.%20equities%20with%20%24R%5E2%3E0.02%24%20out-of-sample.%20Within%20this%20environment%2C%20two%20Proximal%20Policy%20Optimization%20architectures%20-%20both%20MLP%20and%20CNN%20feature%20extractors%20-%20demonstrate%20substantial%20improvements%20over%20industry%20baselines%2C%20with%20the%20CNN%20variant%20achieving%202.13%20bps%20arrival%20slippage%20versus%205.23%20bps%20for%20VWAP%20on%204%2C900%20out-of-sample%20orders%20%28%2421B%20notional%29.%20These%20results%20validate%20both%20the%20simulation%20realism%20and%20provide%20strong%20single-policy%20baselines%20for%20quality-diversity%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.22113v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


