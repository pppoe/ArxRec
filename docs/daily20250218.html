<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250217.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with\n  Pose Guidance from Text", "author": "Gyumin Shim and Sangmin Lee and Jaegul Choo", "abstract": "  In this paper, we introduce GaussianMotion, a novel human rendering model\nthat generates fully animatable scenes aligned with textual descriptions using\nGaussian Splatting. Although existing methods achieve reasonable text-to-3D\ngeneration of human bodies using various 3D representations, they often face\nlimitations in fidelity and efficiency, or primarily focus on static models\nwith limited pose control. In contrast, our method generates fully animatable\n3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score\ndistillation, achieving high fidelity and efficient rendering for arbitrary\nposes. By densely generating diverse random poses during optimization, our\ndeformable 3D human model learns to capture a wide range of natural motions\ndistilled from a pose-conditioned diffusion model in an end-to-end manner.\nFurthermore, we propose Adaptive Score Distillation that effectively balances\nrealistic detail and smoothness to achieve optimal 3D results. Experimental\nresults demonstrate that our approach outperforms existing baselines by\nproducing high-quality textures in both static and animated results, and by\ngenerating diverse 3D human models from various textual inputs.\n", "link": "http://arxiv.org/abs/2502.11642v1", "date": "2025-02-17", "relevancy": 3.4823, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7031}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7031}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianMotion%3A%20End-to-End%20Learning%20of%20Animatable%20Gaussian%20Avatars%20with%0A%20%20Pose%20Guidance%20from%20Text&body=Title%3A%20GaussianMotion%3A%20End-to-End%20Learning%20of%20Animatable%20Gaussian%20Avatars%20with%0A%20%20Pose%20Guidance%20from%20Text%0AAuthor%3A%20Gyumin%20Shim%20and%20Sangmin%20Lee%20and%20Jaegul%20Choo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20GaussianMotion%2C%20a%20novel%20human%20rendering%20model%0Athat%20generates%20fully%20animatable%20scenes%20aligned%20with%20textual%20descriptions%20using%0AGaussian%20Splatting.%20Although%20existing%20methods%20achieve%20reasonable%20text-to-3D%0Ageneration%20of%20human%20bodies%20using%20various%203D%20representations%2C%20they%20often%20face%0Alimitations%20in%20fidelity%20and%20efficiency%2C%20or%20primarily%20focus%20on%20static%20models%0Awith%20limited%20pose%20control.%20In%20contrast%2C%20our%20method%20generates%20fully%20animatable%0A3D%20avatars%20by%20combining%20deformable%203D%20Gaussian%20Splatting%20with%20text-to-3D%20score%0Adistillation%2C%20achieving%20high%20fidelity%20and%20efficient%20rendering%20for%20arbitrary%0Aposes.%20By%20densely%20generating%20diverse%20random%20poses%20during%20optimization%2C%20our%0Adeformable%203D%20human%20model%20learns%20to%20capture%20a%20wide%20range%20of%20natural%20motions%0Adistilled%20from%20a%20pose-conditioned%20diffusion%20model%20in%20an%20end-to-end%20manner.%0AFurthermore%2C%20we%20propose%20Adaptive%20Score%20Distillation%20that%20effectively%20balances%0Arealistic%20detail%20and%20smoothness%20to%20achieve%20optimal%203D%20results.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20outperforms%20existing%20baselines%20by%0Aproducing%20high-quality%20textures%20in%20both%20static%20and%20animated%20results%2C%20and%20by%0Agenerating%20diverse%203D%20human%20models%20from%20various%20textual%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianMotion%253A%2520End-to-End%2520Learning%2520of%2520Animatable%2520Gaussian%2520Avatars%2520with%250A%2520%2520Pose%2520Guidance%2520from%2520Text%26entry.906535625%3DGyumin%2520Shim%2520and%2520Sangmin%2520Lee%2520and%2520Jaegul%2520Choo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GaussianMotion%252C%2520a%2520novel%2520human%2520rendering%2520model%250Athat%2520generates%2520fully%2520animatable%2520scenes%2520aligned%2520with%2520textual%2520descriptions%2520using%250AGaussian%2520Splatting.%2520Although%2520existing%2520methods%2520achieve%2520reasonable%2520text-to-3D%250Ageneration%2520of%2520human%2520bodies%2520using%2520various%25203D%2520representations%252C%2520they%2520often%2520face%250Alimitations%2520in%2520fidelity%2520and%2520efficiency%252C%2520or%2520primarily%2520focus%2520on%2520static%2520models%250Awith%2520limited%2520pose%2520control.%2520In%2520contrast%252C%2520our%2520method%2520generates%2520fully%2520animatable%250A3D%2520avatars%2520by%2520combining%2520deformable%25203D%2520Gaussian%2520Splatting%2520with%2520text-to-3D%2520score%250Adistillation%252C%2520achieving%2520high%2520fidelity%2520and%2520efficient%2520rendering%2520for%2520arbitrary%250Aposes.%2520By%2520densely%2520generating%2520diverse%2520random%2520poses%2520during%2520optimization%252C%2520our%250Adeformable%25203D%2520human%2520model%2520learns%2520to%2520capture%2520a%2520wide%2520range%2520of%2520natural%2520motions%250Adistilled%2520from%2520a%2520pose-conditioned%2520diffusion%2520model%2520in%2520an%2520end-to-end%2520manner.%250AFurthermore%252C%2520we%2520propose%2520Adaptive%2520Score%2520Distillation%2520that%2520effectively%2520balances%250Arealistic%2520detail%2520and%2520smoothness%2520to%2520achieve%2520optimal%25203D%2520results.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520existing%2520baselines%2520by%250Aproducing%2520high-quality%2520textures%2520in%2520both%2520static%2520and%2520animated%2520results%252C%2520and%2520by%250Agenerating%2520diverse%25203D%2520human%2520models%2520from%2520various%2520textual%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianMotion%3A%20End-to-End%20Learning%20of%20Animatable%20Gaussian%20Avatars%20with%0A%20%20Pose%20Guidance%20from%20Text&entry.906535625=Gyumin%20Shim%20and%20Sangmin%20Lee%20and%20Jaegul%20Choo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20GaussianMotion%2C%20a%20novel%20human%20rendering%20model%0Athat%20generates%20fully%20animatable%20scenes%20aligned%20with%20textual%20descriptions%20using%0AGaussian%20Splatting.%20Although%20existing%20methods%20achieve%20reasonable%20text-to-3D%0Ageneration%20of%20human%20bodies%20using%20various%203D%20representations%2C%20they%20often%20face%0Alimitations%20in%20fidelity%20and%20efficiency%2C%20or%20primarily%20focus%20on%20static%20models%0Awith%20limited%20pose%20control.%20In%20contrast%2C%20our%20method%20generates%20fully%20animatable%0A3D%20avatars%20by%20combining%20deformable%203D%20Gaussian%20Splatting%20with%20text-to-3D%20score%0Adistillation%2C%20achieving%20high%20fidelity%20and%20efficient%20rendering%20for%20arbitrary%0Aposes.%20By%20densely%20generating%20diverse%20random%20poses%20during%20optimization%2C%20our%0Adeformable%203D%20human%20model%20learns%20to%20capture%20a%20wide%20range%20of%20natural%20motions%0Adistilled%20from%20a%20pose-conditioned%20diffusion%20model%20in%20an%20end-to-end%20manner.%0AFurthermore%2C%20we%20propose%20Adaptive%20Score%20Distillation%20that%20effectively%20balances%0Arealistic%20detail%20and%20smoothness%20to%20achieve%20optimal%203D%20results.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20outperforms%20existing%20baselines%20by%0Aproducing%20high-quality%20textures%20in%20both%20static%20and%20animated%20results%2C%20and%20by%0Agenerating%20diverse%203D%20human%20models%20from%20various%20textual%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11642v1&entry.124074799=Read"},
{"title": "ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View\n  Gaussian Optimization", "author": "Onat \u015eahin and Mohammad Altillawi and George Eskandar and Carlos Carbone and Ziyuan Liu", "abstract": "  Recent advances in diffusion models have significantly improved 3D\ngeneration, enabling the use of assets generated from an image for embodied AI\nsimulations. However, the one-to-many nature of the image-to-3D problem limits\ntheir use due to inconsistent content and quality across views. Previous models\noptimize a 3D model by sampling views from a view-conditioned diffusion prior,\nbut diffusion models cannot guarantee view consistency. Instead, we present\nConsistentDreamer, where we first generate a set of fixed multi-view prior\nimages and sample random views between them with another diffusion model\nthrough a score distillation sampling (SDS) loss. Thereby, we limit the\ndiscrepancies between the views guided by the SDS loss and ensure a consistent\nrough shape. In each iteration, we also use our generated multi-view prior\nimages for fine-detail reconstruction. To balance between the rough shape and\nthe fine-detail optimizations, we introduce dynamic task-dependent weights\nbased on homoscedastic uncertainty, updated automatically in each iteration.\nAdditionally, we employ opacity, depth distortion, and normal alignment losses\nto refine the surface for mesh extraction. Our method ensures better view\nconsistency and visual quality compared to the state-of-the-art.\n", "link": "http://arxiv.org/abs/2502.09278v2", "date": "2025-02-17", "relevancy": 3.3748, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6905}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6672}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConsistentDreamer%3A%20View-Consistent%20Meshes%20Through%20Balanced%20Multi-View%0A%20%20Gaussian%20Optimization&body=Title%3A%20ConsistentDreamer%3A%20View-Consistent%20Meshes%20Through%20Balanced%20Multi-View%0A%20%20Gaussian%20Optimization%0AAuthor%3A%20Onat%20%C5%9Eahin%20and%20Mohammad%20Altillawi%20and%20George%20Eskandar%20and%20Carlos%20Carbone%20and%20Ziyuan%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20significantly%20improved%203D%0Ageneration%2C%20enabling%20the%20use%20of%20assets%20generated%20from%20an%20image%20for%20embodied%20AI%0Asimulations.%20However%2C%20the%20one-to-many%20nature%20of%20the%20image-to-3D%20problem%20limits%0Atheir%20use%20due%20to%20inconsistent%20content%20and%20quality%20across%20views.%20Previous%20models%0Aoptimize%20a%203D%20model%20by%20sampling%20views%20from%20a%20view-conditioned%20diffusion%20prior%2C%0Abut%20diffusion%20models%20cannot%20guarantee%20view%20consistency.%20Instead%2C%20we%20present%0AConsistentDreamer%2C%20where%20we%20first%20generate%20a%20set%20of%20fixed%20multi-view%20prior%0Aimages%20and%20sample%20random%20views%20between%20them%20with%20another%20diffusion%20model%0Athrough%20a%20score%20distillation%20sampling%20%28SDS%29%20loss.%20Thereby%2C%20we%20limit%20the%0Adiscrepancies%20between%20the%20views%20guided%20by%20the%20SDS%20loss%20and%20ensure%20a%20consistent%0Arough%20shape.%20In%20each%20iteration%2C%20we%20also%20use%20our%20generated%20multi-view%20prior%0Aimages%20for%20fine-detail%20reconstruction.%20To%20balance%20between%20the%20rough%20shape%20and%0Athe%20fine-detail%20optimizations%2C%20we%20introduce%20dynamic%20task-dependent%20weights%0Abased%20on%20homoscedastic%20uncertainty%2C%20updated%20automatically%20in%20each%20iteration.%0AAdditionally%2C%20we%20employ%20opacity%2C%20depth%20distortion%2C%20and%20normal%20alignment%20losses%0Ato%20refine%20the%20surface%20for%20mesh%20extraction.%20Our%20method%20ensures%20better%20view%0Aconsistency%20and%20visual%20quality%20compared%20to%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistentDreamer%253A%2520View-Consistent%2520Meshes%2520Through%2520Balanced%2520Multi-View%250A%2520%2520Gaussian%2520Optimization%26entry.906535625%3DOnat%2520%25C5%259Eahin%2520and%2520Mohammad%2520Altillawi%2520and%2520George%2520Eskandar%2520and%2520Carlos%2520Carbone%2520and%2520Ziyuan%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520significantly%2520improved%25203D%250Ageneration%252C%2520enabling%2520the%2520use%2520of%2520assets%2520generated%2520from%2520an%2520image%2520for%2520embodied%2520AI%250Asimulations.%2520However%252C%2520the%2520one-to-many%2520nature%2520of%2520the%2520image-to-3D%2520problem%2520limits%250Atheir%2520use%2520due%2520to%2520inconsistent%2520content%2520and%2520quality%2520across%2520views.%2520Previous%2520models%250Aoptimize%2520a%25203D%2520model%2520by%2520sampling%2520views%2520from%2520a%2520view-conditioned%2520diffusion%2520prior%252C%250Abut%2520diffusion%2520models%2520cannot%2520guarantee%2520view%2520consistency.%2520Instead%252C%2520we%2520present%250AConsistentDreamer%252C%2520where%2520we%2520first%2520generate%2520a%2520set%2520of%2520fixed%2520multi-view%2520prior%250Aimages%2520and%2520sample%2520random%2520views%2520between%2520them%2520with%2520another%2520diffusion%2520model%250Athrough%2520a%2520score%2520distillation%2520sampling%2520%2528SDS%2529%2520loss.%2520Thereby%252C%2520we%2520limit%2520the%250Adiscrepancies%2520between%2520the%2520views%2520guided%2520by%2520the%2520SDS%2520loss%2520and%2520ensure%2520a%2520consistent%250Arough%2520shape.%2520In%2520each%2520iteration%252C%2520we%2520also%2520use%2520our%2520generated%2520multi-view%2520prior%250Aimages%2520for%2520fine-detail%2520reconstruction.%2520To%2520balance%2520between%2520the%2520rough%2520shape%2520and%250Athe%2520fine-detail%2520optimizations%252C%2520we%2520introduce%2520dynamic%2520task-dependent%2520weights%250Abased%2520on%2520homoscedastic%2520uncertainty%252C%2520updated%2520automatically%2520in%2520each%2520iteration.%250AAdditionally%252C%2520we%2520employ%2520opacity%252C%2520depth%2520distortion%252C%2520and%2520normal%2520alignment%2520losses%250Ato%2520refine%2520the%2520surface%2520for%2520mesh%2520extraction.%2520Our%2520method%2520ensures%2520better%2520view%250Aconsistency%2520and%2520visual%2520quality%2520compared%2520to%2520the%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConsistentDreamer%3A%20View-Consistent%20Meshes%20Through%20Balanced%20Multi-View%0A%20%20Gaussian%20Optimization&entry.906535625=Onat%20%C5%9Eahin%20and%20Mohammad%20Altillawi%20and%20George%20Eskandar%20and%20Carlos%20Carbone%20and%20Ziyuan%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20significantly%20improved%203D%0Ageneration%2C%20enabling%20the%20use%20of%20assets%20generated%20from%20an%20image%20for%20embodied%20AI%0Asimulations.%20However%2C%20the%20one-to-many%20nature%20of%20the%20image-to-3D%20problem%20limits%0Atheir%20use%20due%20to%20inconsistent%20content%20and%20quality%20across%20views.%20Previous%20models%0Aoptimize%20a%203D%20model%20by%20sampling%20views%20from%20a%20view-conditioned%20diffusion%20prior%2C%0Abut%20diffusion%20models%20cannot%20guarantee%20view%20consistency.%20Instead%2C%20we%20present%0AConsistentDreamer%2C%20where%20we%20first%20generate%20a%20set%20of%20fixed%20multi-view%20prior%0Aimages%20and%20sample%20random%20views%20between%20them%20with%20another%20diffusion%20model%0Athrough%20a%20score%20distillation%20sampling%20%28SDS%29%20loss.%20Thereby%2C%20we%20limit%20the%0Adiscrepancies%20between%20the%20views%20guided%20by%20the%20SDS%20loss%20and%20ensure%20a%20consistent%0Arough%20shape.%20In%20each%20iteration%2C%20we%20also%20use%20our%20generated%20multi-view%20prior%0Aimages%20for%20fine-detail%20reconstruction.%20To%20balance%20between%20the%20rough%20shape%20and%0Athe%20fine-detail%20optimizations%2C%20we%20introduce%20dynamic%20task-dependent%20weights%0Abased%20on%20homoscedastic%20uncertainty%2C%20updated%20automatically%20in%20each%20iteration.%0AAdditionally%2C%20we%20employ%20opacity%2C%20depth%20distortion%2C%20and%20normal%20alignment%20losses%0Ato%20refine%20the%20surface%20for%20mesh%20extraction.%20Our%20method%20ensures%20better%20view%0Aconsistency%20and%20visual%20quality%20compared%20to%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09278v2&entry.124074799=Read"},
{"title": "HumanGif: Single-View Human Diffusion with Generative Prior", "author": "Shoukang Hu and Takuya Narihira and Kazumi Fukuda and Ryosuke Sawata and Takashi Shibuya and Yuki Mitsufuji", "abstract": "  While previous single-view-based 3D human reconstruction methods made\nsignificant progress in novel view synthesis, it remains a challenge to\nsynthesize both view-consistent and pose-consistent results for animatable\nhuman avatars from a single image input. Motivated by the success of 2D\ncharacter animation, we propose <strong>HumanGif</strong>, a single-view human\ndiffusion model with generative prior. Specifically, we formulate the\nsingle-view-based 3D human novel view and pose synthesis as a\nsingle-view-conditioned human diffusion process, utilizing generative priors\nfrom foundational diffusion models. To ensure fine-grained and consistent novel\nview and pose synthesis, we introduce a Human NeRF module in HumanGif to learn\nspatially aligned features from the input image, implicitly capturing the\nrelative camera and human pose transformation. Furthermore, we introduce an\nimage-level loss during optimization to bridge the gap between latent and image\nspaces in diffusion models. Extensive experiments on RenderPeople and\nDNA-Rendering datasets demonstrate that HumanGif achieves the best perceptual\nperformance, with better generalizability for novel view and pose synthesis.\n", "link": "http://arxiv.org/abs/2502.12080v1", "date": "2025-02-17", "relevancy": 3.3479, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.739}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.649}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanGif%3A%20Single-View%20Human%20Diffusion%20with%20Generative%20Prior&body=Title%3A%20HumanGif%3A%20Single-View%20Human%20Diffusion%20with%20Generative%20Prior%0AAuthor%3A%20Shoukang%20Hu%20and%20Takuya%20Narihira%20and%20Kazumi%20Fukuda%20and%20Ryosuke%20Sawata%20and%20Takashi%20Shibuya%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20While%20previous%20single-view-based%203D%20human%20reconstruction%20methods%20made%0Asignificant%20progress%20in%20novel%20view%20synthesis%2C%20it%20remains%20a%20challenge%20to%0Asynthesize%20both%20view-consistent%20and%20pose-consistent%20results%20for%20animatable%0Ahuman%20avatars%20from%20a%20single%20image%20input.%20Motivated%20by%20the%20success%20of%202D%0Acharacter%20animation%2C%20we%20propose%20%3Cstrong%3EHumanGif%3C/strong%3E%2C%20a%20single-view%20human%0Adiffusion%20model%20with%20generative%20prior.%20Specifically%2C%20we%20formulate%20the%0Asingle-view-based%203D%20human%20novel%20view%20and%20pose%20synthesis%20as%20a%0Asingle-view-conditioned%20human%20diffusion%20process%2C%20utilizing%20generative%20priors%0Afrom%20foundational%20diffusion%20models.%20To%20ensure%20fine-grained%20and%20consistent%20novel%0Aview%20and%20pose%20synthesis%2C%20we%20introduce%20a%20Human%20NeRF%20module%20in%20HumanGif%20to%20learn%0Aspatially%20aligned%20features%20from%20the%20input%20image%2C%20implicitly%20capturing%20the%0Arelative%20camera%20and%20human%20pose%20transformation.%20Furthermore%2C%20we%20introduce%20an%0Aimage-level%20loss%20during%20optimization%20to%20bridge%20the%20gap%20between%20latent%20and%20image%0Aspaces%20in%20diffusion%20models.%20Extensive%20experiments%20on%20RenderPeople%20and%0ADNA-Rendering%20datasets%20demonstrate%20that%20HumanGif%20achieves%20the%20best%20perceptual%0Aperformance%2C%20with%20better%20generalizability%20for%20novel%20view%20and%20pose%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanGif%253A%2520Single-View%2520Human%2520Diffusion%2520with%2520Generative%2520Prior%26entry.906535625%3DShoukang%2520Hu%2520and%2520Takuya%2520Narihira%2520and%2520Kazumi%2520Fukuda%2520and%2520Ryosuke%2520Sawata%2520and%2520Takashi%2520Shibuya%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520While%2520previous%2520single-view-based%25203D%2520human%2520reconstruction%2520methods%2520made%250Asignificant%2520progress%2520in%2520novel%2520view%2520synthesis%252C%2520it%2520remains%2520a%2520challenge%2520to%250Asynthesize%2520both%2520view-consistent%2520and%2520pose-consistent%2520results%2520for%2520animatable%250Ahuman%2520avatars%2520from%2520a%2520single%2520image%2520input.%2520Motivated%2520by%2520the%2520success%2520of%25202D%250Acharacter%2520animation%252C%2520we%2520propose%2520%253Cstrong%253EHumanGif%253C/strong%253E%252C%2520a%2520single-view%2520human%250Adiffusion%2520model%2520with%2520generative%2520prior.%2520Specifically%252C%2520we%2520formulate%2520the%250Asingle-view-based%25203D%2520human%2520novel%2520view%2520and%2520pose%2520synthesis%2520as%2520a%250Asingle-view-conditioned%2520human%2520diffusion%2520process%252C%2520utilizing%2520generative%2520priors%250Afrom%2520foundational%2520diffusion%2520models.%2520To%2520ensure%2520fine-grained%2520and%2520consistent%2520novel%250Aview%2520and%2520pose%2520synthesis%252C%2520we%2520introduce%2520a%2520Human%2520NeRF%2520module%2520in%2520HumanGif%2520to%2520learn%250Aspatially%2520aligned%2520features%2520from%2520the%2520input%2520image%252C%2520implicitly%2520capturing%2520the%250Arelative%2520camera%2520and%2520human%2520pose%2520transformation.%2520Furthermore%252C%2520we%2520introduce%2520an%250Aimage-level%2520loss%2520during%2520optimization%2520to%2520bridge%2520the%2520gap%2520between%2520latent%2520and%2520image%250Aspaces%2520in%2520diffusion%2520models.%2520Extensive%2520experiments%2520on%2520RenderPeople%2520and%250ADNA-Rendering%2520datasets%2520demonstrate%2520that%2520HumanGif%2520achieves%2520the%2520best%2520perceptual%250Aperformance%252C%2520with%2520better%2520generalizability%2520for%2520novel%2520view%2520and%2520pose%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanGif%3A%20Single-View%20Human%20Diffusion%20with%20Generative%20Prior&entry.906535625=Shoukang%20Hu%20and%20Takuya%20Narihira%20and%20Kazumi%20Fukuda%20and%20Ryosuke%20Sawata%20and%20Takashi%20Shibuya%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20While%20previous%20single-view-based%203D%20human%20reconstruction%20methods%20made%0Asignificant%20progress%20in%20novel%20view%20synthesis%2C%20it%20remains%20a%20challenge%20to%0Asynthesize%20both%20view-consistent%20and%20pose-consistent%20results%20for%20animatable%0Ahuman%20avatars%20from%20a%20single%20image%20input.%20Motivated%20by%20the%20success%20of%202D%0Acharacter%20animation%2C%20we%20propose%20%3Cstrong%3EHumanGif%3C/strong%3E%2C%20a%20single-view%20human%0Adiffusion%20model%20with%20generative%20prior.%20Specifically%2C%20we%20formulate%20the%0Asingle-view-based%203D%20human%20novel%20view%20and%20pose%20synthesis%20as%20a%0Asingle-view-conditioned%20human%20diffusion%20process%2C%20utilizing%20generative%20priors%0Afrom%20foundational%20diffusion%20models.%20To%20ensure%20fine-grained%20and%20consistent%20novel%0Aview%20and%20pose%20synthesis%2C%20we%20introduce%20a%20Human%20NeRF%20module%20in%20HumanGif%20to%20learn%0Aspatially%20aligned%20features%20from%20the%20input%20image%2C%20implicitly%20capturing%20the%0Arelative%20camera%20and%20human%20pose%20transformation.%20Furthermore%2C%20we%20introduce%20an%0Aimage-level%20loss%20during%20optimization%20to%20bridge%20the%20gap%20between%20latent%20and%20image%0Aspaces%20in%20diffusion%20models.%20Extensive%20experiments%20on%20RenderPeople%20and%0ADNA-Rendering%20datasets%20demonstrate%20that%20HumanGif%20achieves%20the%20best%20perceptual%0Aperformance%2C%20with%20better%20generalizability%20for%20novel%20view%20and%20pose%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12080v1&entry.124074799=Read"},
{"title": "MVTokenFlow: High-quality 4D Content Generation using Multiview Token\n  Flow", "author": "Hanzhuo Huang and Yuan Liu and Ge Zheng and Jiepeng Wang and Zhiyang Dou and Sibei Yang", "abstract": "  In this paper, we present MVTokenFlow for high-quality 4D content creation\nfrom monocular videos. Recent advancements in generative models such as video\ndiffusion models and multiview diffusion models enable us to create videos or\n3D models. However, extending these generative models for dynamic 4D content\ncreation is still a challenging task that requires the generated content to be\nconsistent spatially and temporally. To address this challenge, MVTokenFlow\nutilizes the multiview diffusion model to generate multiview images on\ndifferent timesteps, which attains spatial consistency across different\nviewpoints and allows us to reconstruct a reasonable coarse 4D field. Then,\nMVTokenFlow further regenerates all the multiview images using the rendered 2D\nflows as guidance. The 2D flows effectively associate pixels from different\ntimesteps and improve the temporal consistency by reusing tokens in the\nregeneration process. Finally, the regenerated images are spatiotemporally\nconsistent and utilized to refine the coarse 4D field to get a high-quality 4D\nfield. Experiments demonstrate the effectiveness of our design and show\nsignificantly improved quality than baseline methods.\n", "link": "http://arxiv.org/abs/2502.11697v1", "date": "2025-02-17", "relevancy": 3.1711, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6444}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6291}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVTokenFlow%3A%20High-quality%204D%20Content%20Generation%20using%20Multiview%20Token%0A%20%20Flow&body=Title%3A%20MVTokenFlow%3A%20High-quality%204D%20Content%20Generation%20using%20Multiview%20Token%0A%20%20Flow%0AAuthor%3A%20Hanzhuo%20Huang%20and%20Yuan%20Liu%20and%20Ge%20Zheng%20and%20Jiepeng%20Wang%20and%20Zhiyang%20Dou%20and%20Sibei%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20MVTokenFlow%20for%20high-quality%204D%20content%20creation%0Afrom%20monocular%20videos.%20Recent%20advancements%20in%20generative%20models%20such%20as%20video%0Adiffusion%20models%20and%20multiview%20diffusion%20models%20enable%20us%20to%20create%20videos%20or%0A3D%20models.%20However%2C%20extending%20these%20generative%20models%20for%20dynamic%204D%20content%0Acreation%20is%20still%20a%20challenging%20task%20that%20requires%20the%20generated%20content%20to%20be%0Aconsistent%20spatially%20and%20temporally.%20To%20address%20this%20challenge%2C%20MVTokenFlow%0Autilizes%20the%20multiview%20diffusion%20model%20to%20generate%20multiview%20images%20on%0Adifferent%20timesteps%2C%20which%20attains%20spatial%20consistency%20across%20different%0Aviewpoints%20and%20allows%20us%20to%20reconstruct%20a%20reasonable%20coarse%204D%20field.%20Then%2C%0AMVTokenFlow%20further%20regenerates%20all%20the%20multiview%20images%20using%20the%20rendered%202D%0Aflows%20as%20guidance.%20The%202D%20flows%20effectively%20associate%20pixels%20from%20different%0Atimesteps%20and%20improve%20the%20temporal%20consistency%20by%20reusing%20tokens%20in%20the%0Aregeneration%20process.%20Finally%2C%20the%20regenerated%20images%20are%20spatiotemporally%0Aconsistent%20and%20utilized%20to%20refine%20the%20coarse%204D%20field%20to%20get%20a%20high-quality%204D%0Afield.%20Experiments%20demonstrate%20the%20effectiveness%20of%20our%20design%20and%20show%0Asignificantly%20improved%20quality%20than%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVTokenFlow%253A%2520High-quality%25204D%2520Content%2520Generation%2520using%2520Multiview%2520Token%250A%2520%2520Flow%26entry.906535625%3DHanzhuo%2520Huang%2520and%2520Yuan%2520Liu%2520and%2520Ge%2520Zheng%2520and%2520Jiepeng%2520Wang%2520and%2520Zhiyang%2520Dou%2520and%2520Sibei%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520MVTokenFlow%2520for%2520high-quality%25204D%2520content%2520creation%250Afrom%2520monocular%2520videos.%2520Recent%2520advancements%2520in%2520generative%2520models%2520such%2520as%2520video%250Adiffusion%2520models%2520and%2520multiview%2520diffusion%2520models%2520enable%2520us%2520to%2520create%2520videos%2520or%250A3D%2520models.%2520However%252C%2520extending%2520these%2520generative%2520models%2520for%2520dynamic%25204D%2520content%250Acreation%2520is%2520still%2520a%2520challenging%2520task%2520that%2520requires%2520the%2520generated%2520content%2520to%2520be%250Aconsistent%2520spatially%2520and%2520temporally.%2520To%2520address%2520this%2520challenge%252C%2520MVTokenFlow%250Autilizes%2520the%2520multiview%2520diffusion%2520model%2520to%2520generate%2520multiview%2520images%2520on%250Adifferent%2520timesteps%252C%2520which%2520attains%2520spatial%2520consistency%2520across%2520different%250Aviewpoints%2520and%2520allows%2520us%2520to%2520reconstruct%2520a%2520reasonable%2520coarse%25204D%2520field.%2520Then%252C%250AMVTokenFlow%2520further%2520regenerates%2520all%2520the%2520multiview%2520images%2520using%2520the%2520rendered%25202D%250Aflows%2520as%2520guidance.%2520The%25202D%2520flows%2520effectively%2520associate%2520pixels%2520from%2520different%250Atimesteps%2520and%2520improve%2520the%2520temporal%2520consistency%2520by%2520reusing%2520tokens%2520in%2520the%250Aregeneration%2520process.%2520Finally%252C%2520the%2520regenerated%2520images%2520are%2520spatiotemporally%250Aconsistent%2520and%2520utilized%2520to%2520refine%2520the%2520coarse%25204D%2520field%2520to%2520get%2520a%2520high-quality%25204D%250Afield.%2520Experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520design%2520and%2520show%250Asignificantly%2520improved%2520quality%2520than%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVTokenFlow%3A%20High-quality%204D%20Content%20Generation%20using%20Multiview%20Token%0A%20%20Flow&entry.906535625=Hanzhuo%20Huang%20and%20Yuan%20Liu%20and%20Ge%20Zheng%20and%20Jiepeng%20Wang%20and%20Zhiyang%20Dou%20and%20Sibei%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20MVTokenFlow%20for%20high-quality%204D%20content%20creation%0Afrom%20monocular%20videos.%20Recent%20advancements%20in%20generative%20models%20such%20as%20video%0Adiffusion%20models%20and%20multiview%20diffusion%20models%20enable%20us%20to%20create%20videos%20or%0A3D%20models.%20However%2C%20extending%20these%20generative%20models%20for%20dynamic%204D%20content%0Acreation%20is%20still%20a%20challenging%20task%20that%20requires%20the%20generated%20content%20to%20be%0Aconsistent%20spatially%20and%20temporally.%20To%20address%20this%20challenge%2C%20MVTokenFlow%0Autilizes%20the%20multiview%20diffusion%20model%20to%20generate%20multiview%20images%20on%0Adifferent%20timesteps%2C%20which%20attains%20spatial%20consistency%20across%20different%0Aviewpoints%20and%20allows%20us%20to%20reconstruct%20a%20reasonable%20coarse%204D%20field.%20Then%2C%0AMVTokenFlow%20further%20regenerates%20all%20the%20multiview%20images%20using%20the%20rendered%202D%0Aflows%20as%20guidance.%20The%202D%20flows%20effectively%20associate%20pixels%20from%20different%0Atimesteps%20and%20improve%20the%20temporal%20consistency%20by%20reusing%20tokens%20in%20the%0Aregeneration%20process.%20Finally%2C%20the%20regenerated%20images%20are%20spatiotemporally%0Aconsistent%20and%20utilized%20to%20refine%20the%20coarse%204D%20field%20to%20get%20a%20high-quality%204D%0Afield.%20Experiments%20demonstrate%20the%20effectiveness%20of%20our%20design%20and%20show%0Asignificantly%20improved%20quality%20than%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11697v1&entry.124074799=Read"},
{"title": "3D Gaussian Inpainting with Depth-Guided Cross-View Consistency", "author": "Sheng-Yu Huang and Zi-Ting Chou and Yu-Chiang Frank Wang", "abstract": "  When performing 3D inpainting using novel-view rendering methods like Neural\nRadiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture\nand geometry consistency across camera views has been a challenge. In this\npaper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided\nCross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided\nby the rendered depth information from each training view, our 3DGIC exploits\nbackground pixels visible across different views for updating the inpainting\nmask, allowing us to refine the 3DGS for inpainting purposes.Through extensive\nexperiments on benchmark datasets, we confirm that our 3DGIC outperforms\ncurrent state-of-the-art 3D inpainting methods quantitatively and\nqualitatively.\n", "link": "http://arxiv.org/abs/2502.11801v1", "date": "2025-02-17", "relevancy": 3.0786, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6349}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6099}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Inpainting%20with%20Depth-Guided%20Cross-View%20Consistency&body=Title%3A%203D%20Gaussian%20Inpainting%20with%20Depth-Guided%20Cross-View%20Consistency%0AAuthor%3A%20Sheng-Yu%20Huang%20and%20Zi-Ting%20Chou%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20%20%20When%20performing%203D%20inpainting%20using%20novel-view%20rendering%20methods%20like%20Neural%0ARadiance%20Field%20%28NeRF%29%20or%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20how%20to%20achieve%20texture%0Aand%20geometry%20consistency%20across%20camera%20views%20has%20been%20a%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20framework%20of%203D%20Gaussian%20Inpainting%20with%20Depth-Guided%0ACross-View%20Consistency%20%283DGIC%29%20for%20cross-view%20consistent%203D%20inpainting.%20Guided%0Aby%20the%20rendered%20depth%20information%20from%20each%20training%20view%2C%20our%203DGIC%20exploits%0Abackground%20pixels%20visible%20across%20different%20views%20for%20updating%20the%20inpainting%0Amask%2C%20allowing%20us%20to%20refine%20the%203DGS%20for%20inpainting%20purposes.Through%20extensive%0Aexperiments%20on%20benchmark%20datasets%2C%20we%20confirm%20that%20our%203DGIC%20outperforms%0Acurrent%20state-of-the-art%203D%20inpainting%20methods%20quantitatively%20and%0Aqualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Inpainting%2520with%2520Depth-Guided%2520Cross-View%2520Consistency%26entry.906535625%3DSheng-Yu%2520Huang%2520and%2520Zi-Ting%2520Chou%2520and%2520Yu-Chiang%2520Frank%2520Wang%26entry.1292438233%3D%2520%2520When%2520performing%25203D%2520inpainting%2520using%2520novel-view%2520rendering%2520methods%2520like%2520Neural%250ARadiance%2520Field%2520%2528NeRF%2529%2520or%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520how%2520to%2520achieve%2520texture%250Aand%2520geometry%2520consistency%2520across%2520camera%2520views%2520has%2520been%2520a%2520challenge.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520framework%2520of%25203D%2520Gaussian%2520Inpainting%2520with%2520Depth-Guided%250ACross-View%2520Consistency%2520%25283DGIC%2529%2520for%2520cross-view%2520consistent%25203D%2520inpainting.%2520Guided%250Aby%2520the%2520rendered%2520depth%2520information%2520from%2520each%2520training%2520view%252C%2520our%25203DGIC%2520exploits%250Abackground%2520pixels%2520visible%2520across%2520different%2520views%2520for%2520updating%2520the%2520inpainting%250Amask%252C%2520allowing%2520us%2520to%2520refine%2520the%25203DGS%2520for%2520inpainting%2520purposes.Through%2520extensive%250Aexperiments%2520on%2520benchmark%2520datasets%252C%2520we%2520confirm%2520that%2520our%25203DGIC%2520outperforms%250Acurrent%2520state-of-the-art%25203D%2520inpainting%2520methods%2520quantitatively%2520and%250Aqualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Inpainting%20with%20Depth-Guided%20Cross-View%20Consistency&entry.906535625=Sheng-Yu%20Huang%20and%20Zi-Ting%20Chou%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20When%20performing%203D%20inpainting%20using%20novel-view%20rendering%20methods%20like%20Neural%0ARadiance%20Field%20%28NeRF%29%20or%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20how%20to%20achieve%20texture%0Aand%20geometry%20consistency%20across%20camera%20views%20has%20been%20a%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20framework%20of%203D%20Gaussian%20Inpainting%20with%20Depth-Guided%0ACross-View%20Consistency%20%283DGIC%29%20for%20cross-view%20consistent%203D%20inpainting.%20Guided%0Aby%20the%20rendered%20depth%20information%20from%20each%20training%20view%2C%20our%203DGIC%20exploits%0Abackground%20pixels%20visible%20across%20different%20views%20for%20updating%20the%20inpainting%0Amask%2C%20allowing%20us%20to%20refine%20the%203DGS%20for%20inpainting%20purposes.Through%20extensive%0Aexperiments%20on%20benchmark%20datasets%2C%20we%20confirm%20that%20our%203DGIC%20outperforms%0Acurrent%20state-of-the-art%203D%20inpainting%20methods%20quantitatively%20and%0Aqualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11801v1&entry.124074799=Read"},
{"title": "Understanding Long Videos with Multimodal Language Models", "author": "Kanchana Ranasinghe and Xiang Li and Kumara Kahatapitiya and Michael S. Ryoo", "abstract": "  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n", "link": "http://arxiv.org/abs/2403.16998v3", "date": "2025-02-17", "relevancy": 3.0678, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models&body=Title%3A%20Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models%0AAuthor%3A%20Kanchana%20Ranasinghe%20and%20Xiang%20Li%20and%20Kumara%20Kahatapitiya%20and%20Michael%20S.%20Ryoo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20allowed%20recent%20LLM-based%20approaches%20to%0Aachieve%20excellent%20performance%20on%20long-video%20understanding%20benchmarks.%20We%0Ainvestigate%20how%20extensive%20world%20knowledge%20and%20strong%20reasoning%20skills%20of%0Aunderlying%20LLMs%20influence%20this%20strong%20performance.%20Surprisingly%2C%20we%20discover%0Athat%20LLM-based%20approaches%20can%20yield%20surprisingly%20good%20accuracy%20on%20long-video%0Atasks%20with%20limited%20video%20information%2C%20sometimes%20even%20with%20no%20video%20specific%0Ainformation.%20Building%20on%20this%2C%20we%20exploring%20injecting%20video-specific%0Ainformation%20into%20an%20LLM-based%20framework.%20We%20utilize%20off-the-shelf%20vision%20tools%0Ato%20extract%20three%20object-centric%20information%20modalities%20from%20videos%20and%20then%0Aleverage%20natural%20language%20as%20a%20medium%20for%20fusing%20this%20information.%20Our%0Aresulting%20Multimodal%20Video%20Understanding%20%28MVU%29%20framework%20demonstrates%0Astate-of-the-art%20performance%20across%20multiple%20video%20understanding%20benchmarks.%0AStrong%20performance%20also%20on%20robotics%20domain%20tasks%20establish%20its%20strong%0Agenerality.%20Our%20code%20will%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16998v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Long%2520Videos%2520with%2520Multimodal%2520Language%2520Models%26entry.906535625%3DKanchana%2520Ranasinghe%2520and%2520Xiang%2520Li%2520and%2520Kumara%2520Kahatapitiya%2520and%2520Michael%2520S.%2520Ryoo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520allowed%2520recent%2520LLM-based%2520approaches%2520to%250Aachieve%2520excellent%2520performance%2520on%2520long-video%2520understanding%2520benchmarks.%2520We%250Ainvestigate%2520how%2520extensive%2520world%2520knowledge%2520and%2520strong%2520reasoning%2520skills%2520of%250Aunderlying%2520LLMs%2520influence%2520this%2520strong%2520performance.%2520Surprisingly%252C%2520we%2520discover%250Athat%2520LLM-based%2520approaches%2520can%2520yield%2520surprisingly%2520good%2520accuracy%2520on%2520long-video%250Atasks%2520with%2520limited%2520video%2520information%252C%2520sometimes%2520even%2520with%2520no%2520video%2520specific%250Ainformation.%2520Building%2520on%2520this%252C%2520we%2520exploring%2520injecting%2520video-specific%250Ainformation%2520into%2520an%2520LLM-based%2520framework.%2520We%2520utilize%2520off-the-shelf%2520vision%2520tools%250Ato%2520extract%2520three%2520object-centric%2520information%2520modalities%2520from%2520videos%2520and%2520then%250Aleverage%2520natural%2520language%2520as%2520a%2520medium%2520for%2520fusing%2520this%2520information.%2520Our%250Aresulting%2520Multimodal%2520Video%2520Understanding%2520%2528MVU%2529%2520framework%2520demonstrates%250Astate-of-the-art%2520performance%2520across%2520multiple%2520video%2520understanding%2520benchmarks.%250AStrong%2520performance%2520also%2520on%2520robotics%2520domain%2520tasks%2520establish%2520its%2520strong%250Agenerality.%2520Our%2520code%2520will%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16998v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Long%20Videos%20with%20Multimodal%20Language%20Models&entry.906535625=Kanchana%20Ranasinghe%20and%20Xiang%20Li%20and%20Kumara%20Kahatapitiya%20and%20Michael%20S.%20Ryoo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20allowed%20recent%20LLM-based%20approaches%20to%0Aachieve%20excellent%20performance%20on%20long-video%20understanding%20benchmarks.%20We%0Ainvestigate%20how%20extensive%20world%20knowledge%20and%20strong%20reasoning%20skills%20of%0Aunderlying%20LLMs%20influence%20this%20strong%20performance.%20Surprisingly%2C%20we%20discover%0Athat%20LLM-based%20approaches%20can%20yield%20surprisingly%20good%20accuracy%20on%20long-video%0Atasks%20with%20limited%20video%20information%2C%20sometimes%20even%20with%20no%20video%20specific%0Ainformation.%20Building%20on%20this%2C%20we%20exploring%20injecting%20video-specific%0Ainformation%20into%20an%20LLM-based%20framework.%20We%20utilize%20off-the-shelf%20vision%20tools%0Ato%20extract%20three%20object-centric%20information%20modalities%20from%20videos%20and%20then%0Aleverage%20natural%20language%20as%20a%20medium%20for%20fusing%20this%20information.%20Our%0Aresulting%20Multimodal%20Video%20Understanding%20%28MVU%29%20framework%20demonstrates%0Astate-of-the-art%20performance%20across%20multiple%20video%20understanding%20benchmarks.%0AStrong%20performance%20also%20on%20robotics%20domain%20tasks%20establish%20its%20strong%0Agenerality.%20Our%20code%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16998v3&entry.124074799=Read"},
{"title": "Language Models Can See Better: Visual Contrastive Decoding For LLM\n  Multimodal Reasoning", "author": "Yuqi Pang and Bowen Yang and Haoqin Tu and Yun Cao and Zeyu Zhang", "abstract": "  Although Large Language Models (LLMs) excel in reasoning and generation for\nlanguage tasks, they are not specifically designed for multimodal challenges.\nTraining Multimodal Large Language Models (MLLMs), however, is\nresource-intensive and constrained by various training limitations. In this\npaper, we propose the Modular-based Visual Contrastive Decoding (MVCD)\nframework to move this obstacle. Our framework leverages LLMs' In-Context\nLearning (ICL) capability and the proposed visual contrastive-example decoding\n(CED), specifically tailored for this framework, without requiring any\nadditional training. By converting visual signals into text and focusing on\ncontrastive output distributions during decoding, we can highlight the new\ninformation introduced by contextual examples, explore their connections, and\navoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual\nperception to make it see and reason over the input visuals. To demonstrate\nMVCD's effectiveness, we conduct experiments with four LLMs across five\nquestion answering datasets. Our results not only show consistent improvement\nin model accuracy but well explain the effective components inside our decoding\nstrategy. Our code will be available at https://github.com/Pbhgit/MVCD.\n", "link": "http://arxiv.org/abs/2502.11751v1", "date": "2025-02-17", "relevancy": 3.0174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20Can%20See%20Better%3A%20Visual%20Contrastive%20Decoding%20For%20LLM%0A%20%20Multimodal%20Reasoning&body=Title%3A%20Language%20Models%20Can%20See%20Better%3A%20Visual%20Contrastive%20Decoding%20For%20LLM%0A%20%20Multimodal%20Reasoning%0AAuthor%3A%20Yuqi%20Pang%20and%20Bowen%20Yang%20and%20Haoqin%20Tu%20and%20Yun%20Cao%20and%20Zeyu%20Zhang%0AAbstract%3A%20%20%20Although%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20reasoning%20and%20generation%20for%0Alanguage%20tasks%2C%20they%20are%20not%20specifically%20designed%20for%20multimodal%20challenges.%0ATraining%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20however%2C%20is%0Aresource-intensive%20and%20constrained%20by%20various%20training%20limitations.%20In%20this%0Apaper%2C%20we%20propose%20the%20Modular-based%20Visual%20Contrastive%20Decoding%20%28MVCD%29%0Aframework%20to%20move%20this%20obstacle.%20Our%20framework%20leverages%20LLMs%27%20In-Context%0ALearning%20%28ICL%29%20capability%20and%20the%20proposed%20visual%20contrastive-example%20decoding%0A%28CED%29%2C%20specifically%20tailored%20for%20this%20framework%2C%20without%20requiring%20any%0Aadditional%20training.%20By%20converting%20visual%20signals%20into%20text%20and%20focusing%20on%0Acontrastive%20output%20distributions%20during%20decoding%2C%20we%20can%20highlight%20the%20new%0Ainformation%20introduced%20by%20contextual%20examples%2C%20explore%20their%20connections%2C%20and%0Aavoid%20over-reliance%20on%20prior%20encoded%20knowledge.%20MVCD%20enhances%20LLMs%27%20visual%0Aperception%20to%20make%20it%20see%20and%20reason%20over%20the%20input%20visuals.%20To%20demonstrate%0AMVCD%27s%20effectiveness%2C%20we%20conduct%20experiments%20with%20four%20LLMs%20across%20five%0Aquestion%20answering%20datasets.%20Our%20results%20not%20only%20show%20consistent%20improvement%0Ain%20model%20accuracy%20but%20well%20explain%20the%20effective%20components%20inside%20our%20decoding%0Astrategy.%20Our%20code%20will%20be%20available%20at%20https%3A//github.com/Pbhgit/MVCD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520Can%2520See%2520Better%253A%2520Visual%2520Contrastive%2520Decoding%2520For%2520LLM%250A%2520%2520Multimodal%2520Reasoning%26entry.906535625%3DYuqi%2520Pang%2520and%2520Bowen%2520Yang%2520and%2520Haoqin%2520Tu%2520and%2520Yun%2520Cao%2520and%2520Zeyu%2520Zhang%26entry.1292438233%3D%2520%2520Although%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520reasoning%2520and%2520generation%2520for%250Alanguage%2520tasks%252C%2520they%2520are%2520not%2520specifically%2520designed%2520for%2520multimodal%2520challenges.%250ATraining%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520however%252C%2520is%250Aresource-intensive%2520and%2520constrained%2520by%2520various%2520training%2520limitations.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520Modular-based%2520Visual%2520Contrastive%2520Decoding%2520%2528MVCD%2529%250Aframework%2520to%2520move%2520this%2520obstacle.%2520Our%2520framework%2520leverages%2520LLMs%2527%2520In-Context%250ALearning%2520%2528ICL%2529%2520capability%2520and%2520the%2520proposed%2520visual%2520contrastive-example%2520decoding%250A%2528CED%2529%252C%2520specifically%2520tailored%2520for%2520this%2520framework%252C%2520without%2520requiring%2520any%250Aadditional%2520training.%2520By%2520converting%2520visual%2520signals%2520into%2520text%2520and%2520focusing%2520on%250Acontrastive%2520output%2520distributions%2520during%2520decoding%252C%2520we%2520can%2520highlight%2520the%2520new%250Ainformation%2520introduced%2520by%2520contextual%2520examples%252C%2520explore%2520their%2520connections%252C%2520and%250Aavoid%2520over-reliance%2520on%2520prior%2520encoded%2520knowledge.%2520MVCD%2520enhances%2520LLMs%2527%2520visual%250Aperception%2520to%2520make%2520it%2520see%2520and%2520reason%2520over%2520the%2520input%2520visuals.%2520To%2520demonstrate%250AMVCD%2527s%2520effectiveness%252C%2520we%2520conduct%2520experiments%2520with%2520four%2520LLMs%2520across%2520five%250Aquestion%2520answering%2520datasets.%2520Our%2520results%2520not%2520only%2520show%2520consistent%2520improvement%250Ain%2520model%2520accuracy%2520but%2520well%2520explain%2520the%2520effective%2520components%2520inside%2520our%2520decoding%250Astrategy.%2520Our%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/Pbhgit/MVCD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Can%20See%20Better%3A%20Visual%20Contrastive%20Decoding%20For%20LLM%0A%20%20Multimodal%20Reasoning&entry.906535625=Yuqi%20Pang%20and%20Bowen%20Yang%20and%20Haoqin%20Tu%20and%20Yun%20Cao%20and%20Zeyu%20Zhang&entry.1292438233=%20%20Although%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20reasoning%20and%20generation%20for%0Alanguage%20tasks%2C%20they%20are%20not%20specifically%20designed%20for%20multimodal%20challenges.%0ATraining%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20however%2C%20is%0Aresource-intensive%20and%20constrained%20by%20various%20training%20limitations.%20In%20this%0Apaper%2C%20we%20propose%20the%20Modular-based%20Visual%20Contrastive%20Decoding%20%28MVCD%29%0Aframework%20to%20move%20this%20obstacle.%20Our%20framework%20leverages%20LLMs%27%20In-Context%0ALearning%20%28ICL%29%20capability%20and%20the%20proposed%20visual%20contrastive-example%20decoding%0A%28CED%29%2C%20specifically%20tailored%20for%20this%20framework%2C%20without%20requiring%20any%0Aadditional%20training.%20By%20converting%20visual%20signals%20into%20text%20and%20focusing%20on%0Acontrastive%20output%20distributions%20during%20decoding%2C%20we%20can%20highlight%20the%20new%0Ainformation%20introduced%20by%20contextual%20examples%2C%20explore%20their%20connections%2C%20and%0Aavoid%20over-reliance%20on%20prior%20encoded%20knowledge.%20MVCD%20enhances%20LLMs%27%20visual%0Aperception%20to%20make%20it%20see%20and%20reason%20over%20the%20input%20visuals.%20To%20demonstrate%0AMVCD%27s%20effectiveness%2C%20we%20conduct%20experiments%20with%20four%20LLMs%20across%20five%0Aquestion%20answering%20datasets.%20Our%20results%20not%20only%20show%20consistent%20improvement%0Ain%20model%20accuracy%20but%20well%20explain%20the%20effective%20components%20inside%20our%20decoding%0Astrategy.%20Our%20code%20will%20be%20available%20at%20https%3A//github.com/Pbhgit/MVCD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11751v1&entry.124074799=Read"},
{"title": "Defining and Evaluating Visual Language Models' Basic Spatial Abilities:\n  A Perspective from Psychometrics", "author": "Wenrui Xu and Dalin Lyu and Weihang Wang and Jie Feng and Chen Gao and Yong Li", "abstract": "  The Theory of Multiple Intelligences underscores the hierarchical nature of\ncognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer\na psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual\nLanguage Models (VLMs): Spatial Perception, Spatial Relation, Spatial\nOrientation, Mental Rotation, and Spatial Visualization. Benchmarking 13\nmainstream VLMs through nine validated psychometric experiments reveals\nsignificant gaps versus humans (average score 24.95 vs. 68.38), with three key\nfindings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,\nweakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller\nmodels such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading\n(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought\n(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from\narchitectural constraints. Identified barriers include weak geometry encoding\nand missing dynamic simulation. By linking psychometric BSAs to VLM\ncapabilities, we provide a diagnostic toolkit for spatial intelligence\nevaluation, methodological foundations for embodied AI development, and a\ncognitive science-informed roadmap for achieving human-like spatial\nintelligence.\n", "link": "http://arxiv.org/abs/2502.11859v1", "date": "2025-02-17", "relevancy": 2.9576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6235}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defining%20and%20Evaluating%20Visual%20Language%20Models%27%20Basic%20Spatial%20Abilities%3A%0A%20%20A%20Perspective%20from%20Psychometrics&body=Title%3A%20Defining%20and%20Evaluating%20Visual%20Language%20Models%27%20Basic%20Spatial%20Abilities%3A%0A%20%20A%20Perspective%20from%20Psychometrics%0AAuthor%3A%20Wenrui%20Xu%20and%20Dalin%20Lyu%20and%20Weihang%20Wang%20and%20Jie%20Feng%20and%20Chen%20Gao%20and%20Yong%20Li%0AAbstract%3A%20%20%20The%20Theory%20of%20Multiple%20Intelligences%20underscores%20the%20hierarchical%20nature%20of%0Acognitive%20capabilities.%20To%20advance%20Spatial%20Artificial%20Intelligence%2C%20we%20pioneer%0Aa%20psychometric%20framework%20defining%20five%20Basic%20Spatial%20Abilities%20%28BSAs%29%20in%20Visual%0ALanguage%20Models%20%28VLMs%29%3A%20Spatial%20Perception%2C%20Spatial%20Relation%2C%20Spatial%0AOrientation%2C%20Mental%20Rotation%2C%20and%20Spatial%20Visualization.%20Benchmarking%2013%0Amainstream%20VLMs%20through%20nine%20validated%20psychometric%20experiments%20reveals%0Asignificant%20gaps%20versus%20humans%20%28average%20score%2024.95%20vs.%2068.38%29%2C%20with%20three%20key%0Afindings%3A%201%29%20VLMs%20mirror%20human%20hierarchies%20%28strongest%20in%202D%20orientation%2C%0Aweakest%20in%203D%20rotation%29%20with%20independent%20BSAs%20%28Pearson%27s%20r%3C0.4%29%3B%202%29%20Smaller%0Amodels%20such%20as%20Qwen2-VL-7B%20surpass%20larger%20counterparts%2C%20with%20Qwen%20leading%0A%2830.82%29%20and%20InternVL2%20lagging%20%2819.6%29%3B%203%29%20Interventions%20like%20chain-of-thought%0A%280.100%20accuracy%20gain%29%20and%205-shot%20training%20%280.259%20improvement%29%20show%20limits%20from%0Aarchitectural%20constraints.%20Identified%20barriers%20include%20weak%20geometry%20encoding%0Aand%20missing%20dynamic%20simulation.%20By%20linking%20psychometric%20BSAs%20to%20VLM%0Acapabilities%2C%20we%20provide%20a%20diagnostic%20toolkit%20for%20spatial%20intelligence%0Aevaluation%2C%20methodological%20foundations%20for%20embodied%20AI%20development%2C%20and%20a%0Acognitive%20science-informed%20roadmap%20for%20achieving%20human-like%20spatial%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefining%2520and%2520Evaluating%2520Visual%2520Language%2520Models%2527%2520Basic%2520Spatial%2520Abilities%253A%250A%2520%2520A%2520Perspective%2520from%2520Psychometrics%26entry.906535625%3DWenrui%2520Xu%2520and%2520Dalin%2520Lyu%2520and%2520Weihang%2520Wang%2520and%2520Jie%2520Feng%2520and%2520Chen%2520Gao%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520The%2520Theory%2520of%2520Multiple%2520Intelligences%2520underscores%2520the%2520hierarchical%2520nature%2520of%250Acognitive%2520capabilities.%2520To%2520advance%2520Spatial%2520Artificial%2520Intelligence%252C%2520we%2520pioneer%250Aa%2520psychometric%2520framework%2520defining%2520five%2520Basic%2520Spatial%2520Abilities%2520%2528BSAs%2529%2520in%2520Visual%250ALanguage%2520Models%2520%2528VLMs%2529%253A%2520Spatial%2520Perception%252C%2520Spatial%2520Relation%252C%2520Spatial%250AOrientation%252C%2520Mental%2520Rotation%252C%2520and%2520Spatial%2520Visualization.%2520Benchmarking%252013%250Amainstream%2520VLMs%2520through%2520nine%2520validated%2520psychometric%2520experiments%2520reveals%250Asignificant%2520gaps%2520versus%2520humans%2520%2528average%2520score%252024.95%2520vs.%252068.38%2529%252C%2520with%2520three%2520key%250Afindings%253A%25201%2529%2520VLMs%2520mirror%2520human%2520hierarchies%2520%2528strongest%2520in%25202D%2520orientation%252C%250Aweakest%2520in%25203D%2520rotation%2529%2520with%2520independent%2520BSAs%2520%2528Pearson%2527s%2520r%253C0.4%2529%253B%25202%2529%2520Smaller%250Amodels%2520such%2520as%2520Qwen2-VL-7B%2520surpass%2520larger%2520counterparts%252C%2520with%2520Qwen%2520leading%250A%252830.82%2529%2520and%2520InternVL2%2520lagging%2520%252819.6%2529%253B%25203%2529%2520Interventions%2520like%2520chain-of-thought%250A%25280.100%2520accuracy%2520gain%2529%2520and%25205-shot%2520training%2520%25280.259%2520improvement%2529%2520show%2520limits%2520from%250Aarchitectural%2520constraints.%2520Identified%2520barriers%2520include%2520weak%2520geometry%2520encoding%250Aand%2520missing%2520dynamic%2520simulation.%2520By%2520linking%2520psychometric%2520BSAs%2520to%2520VLM%250Acapabilities%252C%2520we%2520provide%2520a%2520diagnostic%2520toolkit%2520for%2520spatial%2520intelligence%250Aevaluation%252C%2520methodological%2520foundations%2520for%2520embodied%2520AI%2520development%252C%2520and%2520a%250Acognitive%2520science-informed%2520roadmap%2520for%2520achieving%2520human-like%2520spatial%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defining%20and%20Evaluating%20Visual%20Language%20Models%27%20Basic%20Spatial%20Abilities%3A%0A%20%20A%20Perspective%20from%20Psychometrics&entry.906535625=Wenrui%20Xu%20and%20Dalin%20Lyu%20and%20Weihang%20Wang%20and%20Jie%20Feng%20and%20Chen%20Gao%20and%20Yong%20Li&entry.1292438233=%20%20The%20Theory%20of%20Multiple%20Intelligences%20underscores%20the%20hierarchical%20nature%20of%0Acognitive%20capabilities.%20To%20advance%20Spatial%20Artificial%20Intelligence%2C%20we%20pioneer%0Aa%20psychometric%20framework%20defining%20five%20Basic%20Spatial%20Abilities%20%28BSAs%29%20in%20Visual%0ALanguage%20Models%20%28VLMs%29%3A%20Spatial%20Perception%2C%20Spatial%20Relation%2C%20Spatial%0AOrientation%2C%20Mental%20Rotation%2C%20and%20Spatial%20Visualization.%20Benchmarking%2013%0Amainstream%20VLMs%20through%20nine%20validated%20psychometric%20experiments%20reveals%0Asignificant%20gaps%20versus%20humans%20%28average%20score%2024.95%20vs.%2068.38%29%2C%20with%20three%20key%0Afindings%3A%201%29%20VLMs%20mirror%20human%20hierarchies%20%28strongest%20in%202D%20orientation%2C%0Aweakest%20in%203D%20rotation%29%20with%20independent%20BSAs%20%28Pearson%27s%20r%3C0.4%29%3B%202%29%20Smaller%0Amodels%20such%20as%20Qwen2-VL-7B%20surpass%20larger%20counterparts%2C%20with%20Qwen%20leading%0A%2830.82%29%20and%20InternVL2%20lagging%20%2819.6%29%3B%203%29%20Interventions%20like%20chain-of-thought%0A%280.100%20accuracy%20gain%29%20and%205-shot%20training%20%280.259%20improvement%29%20show%20limits%20from%0Aarchitectural%20constraints.%20Identified%20barriers%20include%20weak%20geometry%20encoding%0Aand%20missing%20dynamic%20simulation.%20By%20linking%20psychometric%20BSAs%20to%20VLM%0Acapabilities%2C%20we%20provide%20a%20diagnostic%20toolkit%20for%20spatial%20intelligence%0Aevaluation%2C%20methodological%20foundations%20for%20embodied%20AI%20development%2C%20and%20a%0Acognitive%20science-informed%20roadmap%20for%20achieving%20human-like%20spatial%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11859v1&entry.124074799=Read"},
{"title": "Better Language Models Exhibit Higher Visual Alignment", "author": "Jona Ruthardt and Gertjan J. Burghouts and Serge Belongie and Yuki M. Asano", "abstract": "  How well do text-only Large Language Models (LLMs) naturally align with the\nvisual world? We provide the first direct analysis by utilizing frozen text\nrepresentations in a discriminative vision-language model framework and\nmeasuring zero-shot generalization on unseen classes. We find decoder-based\nLLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs\nreliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs\nleads to strong gains in cross-lingual settings, where our approach surpasses\nCLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves\nboth robustness and generalization and also significantly reduces the need for\npaired data and compute, making vision-language models more accessible and\nadaptable.\n", "link": "http://arxiv.org/abs/2410.07173v2", "date": "2025-02-17", "relevancy": 2.931, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Language%20Models%20Exhibit%20Higher%20Visual%20Alignment&body=Title%3A%20Better%20Language%20Models%20Exhibit%20Higher%20Visual%20Alignment%0AAuthor%3A%20Jona%20Ruthardt%20and%20Gertjan%20J.%20Burghouts%20and%20Serge%20Belongie%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20How%20well%20do%20text-only%20Large%20Language%20Models%20%28LLMs%29%20naturally%20align%20with%20the%0Avisual%20world%3F%20We%20provide%20the%20first%20direct%20analysis%20by%20utilizing%20frozen%20text%0Arepresentations%20in%20a%20discriminative%20vision-language%20model%20framework%20and%0Ameasuring%20zero-shot%20generalization%20on%20unseen%20classes.%20We%20find%20decoder-based%0ALLMs%20exhibit%20high%20intrinsic%20visual%20alignment.%20In%20particular%2C%20more%20capable%20LLMs%0Areliably%20demonstrate%20stronger%20generalization.%20Moreover%2C%20utilizing%20frozen%20LLMs%0Aleads%20to%20strong%20gains%20in%20cross-lingual%20settings%2C%20where%20our%20approach%20surpasses%0ACLIP%27s%20accuracy%20of%201.4%25%20with%2038.7%25%20for%20Chinese.%20Our%20proposed%20method%20improves%0Aboth%20robustness%20and%20generalization%20and%20also%20significantly%20reduces%20the%20need%20for%0Apaired%20data%20and%20compute%2C%20making%20vision-language%20models%20more%20accessible%20and%0Aadaptable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Language%2520Models%2520Exhibit%2520Higher%2520Visual%2520Alignment%26entry.906535625%3DJona%2520Ruthardt%2520and%2520Gertjan%2520J.%2520Burghouts%2520and%2520Serge%2520Belongie%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520How%2520well%2520do%2520text-only%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520naturally%2520align%2520with%2520the%250Avisual%2520world%253F%2520We%2520provide%2520the%2520first%2520direct%2520analysis%2520by%2520utilizing%2520frozen%2520text%250Arepresentations%2520in%2520a%2520discriminative%2520vision-language%2520model%2520framework%2520and%250Ameasuring%2520zero-shot%2520generalization%2520on%2520unseen%2520classes.%2520We%2520find%2520decoder-based%250ALLMs%2520exhibit%2520high%2520intrinsic%2520visual%2520alignment.%2520In%2520particular%252C%2520more%2520capable%2520LLMs%250Areliably%2520demonstrate%2520stronger%2520generalization.%2520Moreover%252C%2520utilizing%2520frozen%2520LLMs%250Aleads%2520to%2520strong%2520gains%2520in%2520cross-lingual%2520settings%252C%2520where%2520our%2520approach%2520surpasses%250ACLIP%2527s%2520accuracy%2520of%25201.4%2525%2520with%252038.7%2525%2520for%2520Chinese.%2520Our%2520proposed%2520method%2520improves%250Aboth%2520robustness%2520and%2520generalization%2520and%2520also%2520significantly%2520reduces%2520the%2520need%2520for%250Apaired%2520data%2520and%2520compute%252C%2520making%2520vision-language%2520models%2520more%2520accessible%2520and%250Aadaptable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Language%20Models%20Exhibit%20Higher%20Visual%20Alignment&entry.906535625=Jona%20Ruthardt%20and%20Gertjan%20J.%20Burghouts%20and%20Serge%20Belongie%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20How%20well%20do%20text-only%20Large%20Language%20Models%20%28LLMs%29%20naturally%20align%20with%20the%0Avisual%20world%3F%20We%20provide%20the%20first%20direct%20analysis%20by%20utilizing%20frozen%20text%0Arepresentations%20in%20a%20discriminative%20vision-language%20model%20framework%20and%0Ameasuring%20zero-shot%20generalization%20on%20unseen%20classes.%20We%20find%20decoder-based%0ALLMs%20exhibit%20high%20intrinsic%20visual%20alignment.%20In%20particular%2C%20more%20capable%20LLMs%0Areliably%20demonstrate%20stronger%20generalization.%20Moreover%2C%20utilizing%20frozen%20LLMs%0Aleads%20to%20strong%20gains%20in%20cross-lingual%20settings%2C%20where%20our%20approach%20surpasses%0ACLIP%27s%20accuracy%20of%201.4%25%20with%2038.7%25%20for%20Chinese.%20Our%20proposed%20method%20improves%0Aboth%20robustness%20and%20generalization%20and%20also%20significantly%20reduces%20the%20need%20for%0Apaired%20data%20and%20compute%2C%20making%20vision-language%20models%20more%20accessible%20and%0Aadaptable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07173v2&entry.124074799=Read"},
{"title": "From Open-Vocabulary to Vocabulary-Free Semantic Segmentation", "author": "Klara Reichard and Giulia Rizzoli and Stefano Gasperini and Lukas Hoyer and Pietro Zanuttigh and Nassir Navab and Federico Tombari", "abstract": "  Open-vocabulary semantic segmentation enables models to identify novel object\ncategories beyond their training data. While this flexibility represents a\nsignificant advancement, current approaches still rely on manually specified\nclass names as input, creating an inherent bottleneck in real-world\napplications. This work proposes a Vocabulary-Free Semantic Segmentation\npipeline, eliminating the need for predefined class vocabularies. Specifically,\nwe address the chicken-and-egg problem where users need knowledge of all\npotential objects within a scene to identify them, yet the purpose of\nsegmentation is often to discover these objects. The proposed approach\nleverages Vision-Language Models to automatically recognize objects and\ngenerate appropriate class names, aiming to solve the challenge of class\nspecification and naming quality. Through extensive experiments on several\npublic datasets, we highlight the crucial role of the text encoder in model\nperformance, particularly when the image text classes are paired with generated\ndescriptions. Despite the challenges introduced by the sensitivity of the\nsegmentation text encoder to false negatives within the class tagging process,\nwhich adds complexity to the task, we demonstrate that our fully automated\npipeline significantly enhances vocabulary-free segmentation accuracy across\ndiverse real-world scenarios.\n", "link": "http://arxiv.org/abs/2502.11891v1", "date": "2025-02-17", "relevancy": 2.9206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Open-Vocabulary%20to%20Vocabulary-Free%20Semantic%20Segmentation&body=Title%3A%20From%20Open-Vocabulary%20to%20Vocabulary-Free%20Semantic%20Segmentation%0AAuthor%3A%20Klara%20Reichard%20and%20Giulia%20Rizzoli%20and%20Stefano%20Gasperini%20and%20Lukas%20Hoyer%20and%20Pietro%20Zanuttigh%20and%20Nassir%20Navab%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Open-vocabulary%20semantic%20segmentation%20enables%20models%20to%20identify%20novel%20object%0Acategories%20beyond%20their%20training%20data.%20While%20this%20flexibility%20represents%20a%0Asignificant%20advancement%2C%20current%20approaches%20still%20rely%20on%20manually%20specified%0Aclass%20names%20as%20input%2C%20creating%20an%20inherent%20bottleneck%20in%20real-world%0Aapplications.%20This%20work%20proposes%20a%20Vocabulary-Free%20Semantic%20Segmentation%0Apipeline%2C%20eliminating%20the%20need%20for%20predefined%20class%20vocabularies.%20Specifically%2C%0Awe%20address%20the%20chicken-and-egg%20problem%20where%20users%20need%20knowledge%20of%20all%0Apotential%20objects%20within%20a%20scene%20to%20identify%20them%2C%20yet%20the%20purpose%20of%0Asegmentation%20is%20often%20to%20discover%20these%20objects.%20The%20proposed%20approach%0Aleverages%20Vision-Language%20Models%20to%20automatically%20recognize%20objects%20and%0Agenerate%20appropriate%20class%20names%2C%20aiming%20to%20solve%20the%20challenge%20of%20class%0Aspecification%20and%20naming%20quality.%20Through%20extensive%20experiments%20on%20several%0Apublic%20datasets%2C%20we%20highlight%20the%20crucial%20role%20of%20the%20text%20encoder%20in%20model%0Aperformance%2C%20particularly%20when%20the%20image%20text%20classes%20are%20paired%20with%20generated%0Adescriptions.%20Despite%20the%20challenges%20introduced%20by%20the%20sensitivity%20of%20the%0Asegmentation%20text%20encoder%20to%20false%20negatives%20within%20the%20class%20tagging%20process%2C%0Awhich%20adds%20complexity%20to%20the%20task%2C%20we%20demonstrate%20that%20our%20fully%20automated%0Apipeline%20significantly%20enhances%20vocabulary-free%20segmentation%20accuracy%20across%0Adiverse%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Open-Vocabulary%2520to%2520Vocabulary-Free%2520Semantic%2520Segmentation%26entry.906535625%3DKlara%2520Reichard%2520and%2520Giulia%2520Rizzoli%2520and%2520Stefano%2520Gasperini%2520and%2520Lukas%2520Hoyer%2520and%2520Pietro%2520Zanuttigh%2520and%2520Nassir%2520Navab%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520Open-vocabulary%2520semantic%2520segmentation%2520enables%2520models%2520to%2520identify%2520novel%2520object%250Acategories%2520beyond%2520their%2520training%2520data.%2520While%2520this%2520flexibility%2520represents%2520a%250Asignificant%2520advancement%252C%2520current%2520approaches%2520still%2520rely%2520on%2520manually%2520specified%250Aclass%2520names%2520as%2520input%252C%2520creating%2520an%2520inherent%2520bottleneck%2520in%2520real-world%250Aapplications.%2520This%2520work%2520proposes%2520a%2520Vocabulary-Free%2520Semantic%2520Segmentation%250Apipeline%252C%2520eliminating%2520the%2520need%2520for%2520predefined%2520class%2520vocabularies.%2520Specifically%252C%250Awe%2520address%2520the%2520chicken-and-egg%2520problem%2520where%2520users%2520need%2520knowledge%2520of%2520all%250Apotential%2520objects%2520within%2520a%2520scene%2520to%2520identify%2520them%252C%2520yet%2520the%2520purpose%2520of%250Asegmentation%2520is%2520often%2520to%2520discover%2520these%2520objects.%2520The%2520proposed%2520approach%250Aleverages%2520Vision-Language%2520Models%2520to%2520automatically%2520recognize%2520objects%2520and%250Agenerate%2520appropriate%2520class%2520names%252C%2520aiming%2520to%2520solve%2520the%2520challenge%2520of%2520class%250Aspecification%2520and%2520naming%2520quality.%2520Through%2520extensive%2520experiments%2520on%2520several%250Apublic%2520datasets%252C%2520we%2520highlight%2520the%2520crucial%2520role%2520of%2520the%2520text%2520encoder%2520in%2520model%250Aperformance%252C%2520particularly%2520when%2520the%2520image%2520text%2520classes%2520are%2520paired%2520with%2520generated%250Adescriptions.%2520Despite%2520the%2520challenges%2520introduced%2520by%2520the%2520sensitivity%2520of%2520the%250Asegmentation%2520text%2520encoder%2520to%2520false%2520negatives%2520within%2520the%2520class%2520tagging%2520process%252C%250Awhich%2520adds%2520complexity%2520to%2520the%2520task%252C%2520we%2520demonstrate%2520that%2520our%2520fully%2520automated%250Apipeline%2520significantly%2520enhances%2520vocabulary-free%2520segmentation%2520accuracy%2520across%250Adiverse%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Open-Vocabulary%20to%20Vocabulary-Free%20Semantic%20Segmentation&entry.906535625=Klara%20Reichard%20and%20Giulia%20Rizzoli%20and%20Stefano%20Gasperini%20and%20Lukas%20Hoyer%20and%20Pietro%20Zanuttigh%20and%20Nassir%20Navab%20and%20Federico%20Tombari&entry.1292438233=%20%20Open-vocabulary%20semantic%20segmentation%20enables%20models%20to%20identify%20novel%20object%0Acategories%20beyond%20their%20training%20data.%20While%20this%20flexibility%20represents%20a%0Asignificant%20advancement%2C%20current%20approaches%20still%20rely%20on%20manually%20specified%0Aclass%20names%20as%20input%2C%20creating%20an%20inherent%20bottleneck%20in%20real-world%0Aapplications.%20This%20work%20proposes%20a%20Vocabulary-Free%20Semantic%20Segmentation%0Apipeline%2C%20eliminating%20the%20need%20for%20predefined%20class%20vocabularies.%20Specifically%2C%0Awe%20address%20the%20chicken-and-egg%20problem%20where%20users%20need%20knowledge%20of%20all%0Apotential%20objects%20within%20a%20scene%20to%20identify%20them%2C%20yet%20the%20purpose%20of%0Asegmentation%20is%20often%20to%20discover%20these%20objects.%20The%20proposed%20approach%0Aleverages%20Vision-Language%20Models%20to%20automatically%20recognize%20objects%20and%0Agenerate%20appropriate%20class%20names%2C%20aiming%20to%20solve%20the%20challenge%20of%20class%0Aspecification%20and%20naming%20quality.%20Through%20extensive%20experiments%20on%20several%0Apublic%20datasets%2C%20we%20highlight%20the%20crucial%20role%20of%20the%20text%20encoder%20in%20model%0Aperformance%2C%20particularly%20when%20the%20image%20text%20classes%20are%20paired%20with%20generated%0Adescriptions.%20Despite%20the%20challenges%20introduced%20by%20the%20sensitivity%20of%20the%0Asegmentation%20text%20encoder%20to%20false%20negatives%20within%20the%20class%20tagging%20process%2C%0Awhich%20adds%20complexity%20to%20the%20task%2C%20we%20demonstrate%20that%20our%20fully%20automated%0Apipeline%20significantly%20enhances%20vocabulary-free%20segmentation%20accuracy%20across%0Adiverse%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11891v1&entry.124074799=Read"},
{"title": "MagicArticulate: Make Your 3D Models Articulation-Ready", "author": "Chaoyue Song and Jianfeng Zhang and Xiu Li and Fan Yang and Yiwen Chen and Zhongcong Xu and Jun Hao Liew and Xiaoyang Guo and Fayao Liu and Jiashi Feng and Guosheng Lin", "abstract": "  With the explosive growth of 3D content creation, there is an increasing\ndemand for automatically converting static 3D models into articulation-ready\nversions that support realistic animation. Traditional approaches rely heavily\non manual annotation, which is both time-consuming and labor-intensive.\nMoreover, the lack of large-scale benchmarks has hindered the development of\nlearning-based solutions. In this work, we present MagicArticulate, an\neffective framework that automatically transforms static 3D models into\narticulation-ready assets. Our key contributions are threefold. First, we\nintroduce Articulation-XL, a large-scale benchmark containing over 33k 3D\nmodels with high-quality articulation annotations, carefully curated from\nObjaverse-XL. Second, we propose a novel skeleton generation method that\nformulates the task as a sequence modeling problem, leveraging an\nauto-regressive transformer to naturally handle varying numbers of bones or\njoints within skeletons and their inherent dependencies across different 3D\nmodels. Third, we predict skinning weights using a functional diffusion process\nthat incorporates volumetric geodesic distance priors between vertices and\njoints. Extensive experiments demonstrate that MagicArticulate significantly\noutperforms existing methods across diverse object categories, achieving\nhigh-quality articulation that enables realistic animation. Project page:\nhttps://chaoyuesong.github.io/MagicArticulate.\n", "link": "http://arxiv.org/abs/2502.12135v1", "date": "2025-02-17", "relevancy": 2.8748, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6232}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5614}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicArticulate%3A%20Make%20Your%203D%20Models%20Articulation-Ready&body=Title%3A%20MagicArticulate%3A%20Make%20Your%203D%20Models%20Articulation-Ready%0AAuthor%3A%20Chaoyue%20Song%20and%20Jianfeng%20Zhang%20and%20Xiu%20Li%20and%20Fan%20Yang%20and%20Yiwen%20Chen%20and%20Zhongcong%20Xu%20and%20Jun%20Hao%20Liew%20and%20Xiaoyang%20Guo%20and%20Fayao%20Liu%20and%20Jiashi%20Feng%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20With%20the%20explosive%20growth%20of%203D%20content%20creation%2C%20there%20is%20an%20increasing%0Ademand%20for%20automatically%20converting%20static%203D%20models%20into%20articulation-ready%0Aversions%20that%20support%20realistic%20animation.%20Traditional%20approaches%20rely%20heavily%0Aon%20manual%20annotation%2C%20which%20is%20both%20time-consuming%20and%20labor-intensive.%0AMoreover%2C%20the%20lack%20of%20large-scale%20benchmarks%20has%20hindered%20the%20development%20of%0Alearning-based%20solutions.%20In%20this%20work%2C%20we%20present%20MagicArticulate%2C%20an%0Aeffective%20framework%20that%20automatically%20transforms%20static%203D%20models%20into%0Aarticulation-ready%20assets.%20Our%20key%20contributions%20are%20threefold.%20First%2C%20we%0Aintroduce%20Articulation-XL%2C%20a%20large-scale%20benchmark%20containing%20over%2033k%203D%0Amodels%20with%20high-quality%20articulation%20annotations%2C%20carefully%20curated%20from%0AObjaverse-XL.%20Second%2C%20we%20propose%20a%20novel%20skeleton%20generation%20method%20that%0Aformulates%20the%20task%20as%20a%20sequence%20modeling%20problem%2C%20leveraging%20an%0Aauto-regressive%20transformer%20to%20naturally%20handle%20varying%20numbers%20of%20bones%20or%0Ajoints%20within%20skeletons%20and%20their%20inherent%20dependencies%20across%20different%203D%0Amodels.%20Third%2C%20we%20predict%20skinning%20weights%20using%20a%20functional%20diffusion%20process%0Athat%20incorporates%20volumetric%20geodesic%20distance%20priors%20between%20vertices%20and%0Ajoints.%20Extensive%20experiments%20demonstrate%20that%20MagicArticulate%20significantly%0Aoutperforms%20existing%20methods%20across%20diverse%20object%20categories%2C%20achieving%0Ahigh-quality%20articulation%20that%20enables%20realistic%20animation.%20Project%20page%3A%0Ahttps%3A//chaoyuesong.github.io/MagicArticulate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicArticulate%253A%2520Make%2520Your%25203D%2520Models%2520Articulation-Ready%26entry.906535625%3DChaoyue%2520Song%2520and%2520Jianfeng%2520Zhang%2520and%2520Xiu%2520Li%2520and%2520Fan%2520Yang%2520and%2520Yiwen%2520Chen%2520and%2520Zhongcong%2520Xu%2520and%2520Jun%2520Hao%2520Liew%2520and%2520Xiaoyang%2520Guo%2520and%2520Fayao%2520Liu%2520and%2520Jiashi%2520Feng%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520With%2520the%2520explosive%2520growth%2520of%25203D%2520content%2520creation%252C%2520there%2520is%2520an%2520increasing%250Ademand%2520for%2520automatically%2520converting%2520static%25203D%2520models%2520into%2520articulation-ready%250Aversions%2520that%2520support%2520realistic%2520animation.%2520Traditional%2520approaches%2520rely%2520heavily%250Aon%2520manual%2520annotation%252C%2520which%2520is%2520both%2520time-consuming%2520and%2520labor-intensive.%250AMoreover%252C%2520the%2520lack%2520of%2520large-scale%2520benchmarks%2520has%2520hindered%2520the%2520development%2520of%250Alearning-based%2520solutions.%2520In%2520this%2520work%252C%2520we%2520present%2520MagicArticulate%252C%2520an%250Aeffective%2520framework%2520that%2520automatically%2520transforms%2520static%25203D%2520models%2520into%250Aarticulation-ready%2520assets.%2520Our%2520key%2520contributions%2520are%2520threefold.%2520First%252C%2520we%250Aintroduce%2520Articulation-XL%252C%2520a%2520large-scale%2520benchmark%2520containing%2520over%252033k%25203D%250Amodels%2520with%2520high-quality%2520articulation%2520annotations%252C%2520carefully%2520curated%2520from%250AObjaverse-XL.%2520Second%252C%2520we%2520propose%2520a%2520novel%2520skeleton%2520generation%2520method%2520that%250Aformulates%2520the%2520task%2520as%2520a%2520sequence%2520modeling%2520problem%252C%2520leveraging%2520an%250Aauto-regressive%2520transformer%2520to%2520naturally%2520handle%2520varying%2520numbers%2520of%2520bones%2520or%250Ajoints%2520within%2520skeletons%2520and%2520their%2520inherent%2520dependencies%2520across%2520different%25203D%250Amodels.%2520Third%252C%2520we%2520predict%2520skinning%2520weights%2520using%2520a%2520functional%2520diffusion%2520process%250Athat%2520incorporates%2520volumetric%2520geodesic%2520distance%2520priors%2520between%2520vertices%2520and%250Ajoints.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MagicArticulate%2520significantly%250Aoutperforms%2520existing%2520methods%2520across%2520diverse%2520object%2520categories%252C%2520achieving%250Ahigh-quality%2520articulation%2520that%2520enables%2520realistic%2520animation.%2520Project%2520page%253A%250Ahttps%253A//chaoyuesong.github.io/MagicArticulate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicArticulate%3A%20Make%20Your%203D%20Models%20Articulation-Ready&entry.906535625=Chaoyue%20Song%20and%20Jianfeng%20Zhang%20and%20Xiu%20Li%20and%20Fan%20Yang%20and%20Yiwen%20Chen%20and%20Zhongcong%20Xu%20and%20Jun%20Hao%20Liew%20and%20Xiaoyang%20Guo%20and%20Fayao%20Liu%20and%20Jiashi%20Feng%20and%20Guosheng%20Lin&entry.1292438233=%20%20With%20the%20explosive%20growth%20of%203D%20content%20creation%2C%20there%20is%20an%20increasing%0Ademand%20for%20automatically%20converting%20static%203D%20models%20into%20articulation-ready%0Aversions%20that%20support%20realistic%20animation.%20Traditional%20approaches%20rely%20heavily%0Aon%20manual%20annotation%2C%20which%20is%20both%20time-consuming%20and%20labor-intensive.%0AMoreover%2C%20the%20lack%20of%20large-scale%20benchmarks%20has%20hindered%20the%20development%20of%0Alearning-based%20solutions.%20In%20this%20work%2C%20we%20present%20MagicArticulate%2C%20an%0Aeffective%20framework%20that%20automatically%20transforms%20static%203D%20models%20into%0Aarticulation-ready%20assets.%20Our%20key%20contributions%20are%20threefold.%20First%2C%20we%0Aintroduce%20Articulation-XL%2C%20a%20large-scale%20benchmark%20containing%20over%2033k%203D%0Amodels%20with%20high-quality%20articulation%20annotations%2C%20carefully%20curated%20from%0AObjaverse-XL.%20Second%2C%20we%20propose%20a%20novel%20skeleton%20generation%20method%20that%0Aformulates%20the%20task%20as%20a%20sequence%20modeling%20problem%2C%20leveraging%20an%0Aauto-regressive%20transformer%20to%20naturally%20handle%20varying%20numbers%20of%20bones%20or%0Ajoints%20within%20skeletons%20and%20their%20inherent%20dependencies%20across%20different%203D%0Amodels.%20Third%2C%20we%20predict%20skinning%20weights%20using%20a%20functional%20diffusion%20process%0Athat%20incorporates%20volumetric%20geodesic%20distance%20priors%20between%20vertices%20and%0Ajoints.%20Extensive%20experiments%20demonstrate%20that%20MagicArticulate%20significantly%0Aoutperforms%20existing%20methods%20across%20diverse%20object%20categories%2C%20achieving%0Ahigh-quality%20articulation%20that%20enables%20realistic%20animation.%20Project%20page%3A%0Ahttps%3A//chaoyuesong.github.io/MagicArticulate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12135v1&entry.124074799=Read"},
{"title": "Scalable Vision Language Model Training via High Quality Data Curation", "author": "Hongyuan Dong and Zijian Kang and Weijie Yin and Xiao Liang and Chao Feng and Jiao Ran", "abstract": "  In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning\nvia High QuaLity Data Curation), an open-source vision language model (VLM)\nseries achieving state-of-the-art (SOTA) performance in 2B and 8B parameters.\nThe following three key improvements contribute to SAIL-VL's leading\nperformance: (1) Scalable high-quality visual understanding data construction:\nWe implement a data construction pipeline to enable hundred-million-scale\nhigh-quality recaption data annotation, and the resulted dataset SAIL-Caption\nis validated to be of the highest data quality compared with opensource\nalternatives. (2) Scalable Pretraining with High-Quality Visual Understanding\nData: We scale SAIL-VL's pretraining budget up to 655B tokens and show that\neven a 2B VLM benefits from scaled up training data sizes, exhibiting expected\ndata size scaling laws in visual understanding and instruction following\nperformance. (3) Scalable SFT via data quantity and complexity scaling: We\ncurate a high-quality SFT dataset collection which outperforms opensource\nalternatives in data quantity scaling effectiveness. We also demonstrate that\ntraining with progressively higher-complexity data surpasses baseline one-stage\ntraining by a large margin. SAIL-VL series models achieve the highest average\nscore in 18 widely used VLM benchmarks in our evaluation, with the 2B model\ntakes the top position over VLMs of comparable sizes on OpenCompass 2024\n(https://rank.opencompass.org.cn/leaderboard-multimodal) demonstrating robust\nvisual comprehension abilities. SAIL-VL series models are released at\nHuggingFace (https://huggingface.co/BytedanceDouyinContent).\n", "link": "http://arxiv.org/abs/2501.05952v2", "date": "2025-02-17", "relevancy": 2.8637, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5753}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Vision%20Language%20Model%20Training%20via%20High%20Quality%20Data%20Curation&body=Title%3A%20Scalable%20Vision%20Language%20Model%20Training%20via%20High%20Quality%20Data%20Curation%0AAuthor%3A%20Hongyuan%20Dong%20and%20Zijian%20Kang%20and%20Weijie%20Yin%20and%20Xiao%20Liang%20and%20Chao%20Feng%20and%20Jiao%20Ran%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20SAIL-VL%20%28ScAlable%20Vision%20Language%20Model%20TraIning%0Avia%20High%20QuaLity%20Data%20Curation%29%2C%20an%20open-source%20vision%20language%20model%20%28VLM%29%0Aseries%20achieving%20state-of-the-art%20%28SOTA%29%20performance%20in%202B%20and%208B%20parameters.%0AThe%20following%20three%20key%20improvements%20contribute%20to%20SAIL-VL%27s%20leading%0Aperformance%3A%20%281%29%20Scalable%20high-quality%20visual%20understanding%20data%20construction%3A%0AWe%20implement%20a%20data%20construction%20pipeline%20to%20enable%20hundred-million-scale%0Ahigh-quality%20recaption%20data%20annotation%2C%20and%20the%20resulted%20dataset%20SAIL-Caption%0Ais%20validated%20to%20be%20of%20the%20highest%20data%20quality%20compared%20with%20opensource%0Aalternatives.%20%282%29%20Scalable%20Pretraining%20with%20High-Quality%20Visual%20Understanding%0AData%3A%20We%20scale%20SAIL-VL%27s%20pretraining%20budget%20up%20to%20655B%20tokens%20and%20show%20that%0Aeven%20a%202B%20VLM%20benefits%20from%20scaled%20up%20training%20data%20sizes%2C%20exhibiting%20expected%0Adata%20size%20scaling%20laws%20in%20visual%20understanding%20and%20instruction%20following%0Aperformance.%20%283%29%20Scalable%20SFT%20via%20data%20quantity%20and%20complexity%20scaling%3A%20We%0Acurate%20a%20high-quality%20SFT%20dataset%20collection%20which%20outperforms%20opensource%0Aalternatives%20in%20data%20quantity%20scaling%20effectiveness.%20We%20also%20demonstrate%20that%0Atraining%20with%20progressively%20higher-complexity%20data%20surpasses%20baseline%20one-stage%0Atraining%20by%20a%20large%20margin.%20SAIL-VL%20series%20models%20achieve%20the%20highest%20average%0Ascore%20in%2018%20widely%20used%20VLM%20benchmarks%20in%20our%20evaluation%2C%20with%20the%202B%20model%0Atakes%20the%20top%20position%20over%20VLMs%20of%20comparable%20sizes%20on%20OpenCompass%202024%0A%28https%3A//rank.opencompass.org.cn/leaderboard-multimodal%29%20demonstrating%20robust%0Avisual%20comprehension%20abilities.%20SAIL-VL%20series%20models%20are%20released%20at%0AHuggingFace%20%28https%3A//huggingface.co/BytedanceDouyinContent%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Vision%2520Language%2520Model%2520Training%2520via%2520High%2520Quality%2520Data%2520Curation%26entry.906535625%3DHongyuan%2520Dong%2520and%2520Zijian%2520Kang%2520and%2520Weijie%2520Yin%2520and%2520Xiao%2520Liang%2520and%2520Chao%2520Feng%2520and%2520Jiao%2520Ran%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SAIL-VL%2520%2528ScAlable%2520Vision%2520Language%2520Model%2520TraIning%250Avia%2520High%2520QuaLity%2520Data%2520Curation%2529%252C%2520an%2520open-source%2520vision%2520language%2520model%2520%2528VLM%2529%250Aseries%2520achieving%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520in%25202B%2520and%25208B%2520parameters.%250AThe%2520following%2520three%2520key%2520improvements%2520contribute%2520to%2520SAIL-VL%2527s%2520leading%250Aperformance%253A%2520%25281%2529%2520Scalable%2520high-quality%2520visual%2520understanding%2520data%2520construction%253A%250AWe%2520implement%2520a%2520data%2520construction%2520pipeline%2520to%2520enable%2520hundred-million-scale%250Ahigh-quality%2520recaption%2520data%2520annotation%252C%2520and%2520the%2520resulted%2520dataset%2520SAIL-Caption%250Ais%2520validated%2520to%2520be%2520of%2520the%2520highest%2520data%2520quality%2520compared%2520with%2520opensource%250Aalternatives.%2520%25282%2529%2520Scalable%2520Pretraining%2520with%2520High-Quality%2520Visual%2520Understanding%250AData%253A%2520We%2520scale%2520SAIL-VL%2527s%2520pretraining%2520budget%2520up%2520to%2520655B%2520tokens%2520and%2520show%2520that%250Aeven%2520a%25202B%2520VLM%2520benefits%2520from%2520scaled%2520up%2520training%2520data%2520sizes%252C%2520exhibiting%2520expected%250Adata%2520size%2520scaling%2520laws%2520in%2520visual%2520understanding%2520and%2520instruction%2520following%250Aperformance.%2520%25283%2529%2520Scalable%2520SFT%2520via%2520data%2520quantity%2520and%2520complexity%2520scaling%253A%2520We%250Acurate%2520a%2520high-quality%2520SFT%2520dataset%2520collection%2520which%2520outperforms%2520opensource%250Aalternatives%2520in%2520data%2520quantity%2520scaling%2520effectiveness.%2520We%2520also%2520demonstrate%2520that%250Atraining%2520with%2520progressively%2520higher-complexity%2520data%2520surpasses%2520baseline%2520one-stage%250Atraining%2520by%2520a%2520large%2520margin.%2520SAIL-VL%2520series%2520models%2520achieve%2520the%2520highest%2520average%250Ascore%2520in%252018%2520widely%2520used%2520VLM%2520benchmarks%2520in%2520our%2520evaluation%252C%2520with%2520the%25202B%2520model%250Atakes%2520the%2520top%2520position%2520over%2520VLMs%2520of%2520comparable%2520sizes%2520on%2520OpenCompass%25202024%250A%2528https%253A//rank.opencompass.org.cn/leaderboard-multimodal%2529%2520demonstrating%2520robust%250Avisual%2520comprehension%2520abilities.%2520SAIL-VL%2520series%2520models%2520are%2520released%2520at%250AHuggingFace%2520%2528https%253A//huggingface.co/BytedanceDouyinContent%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Vision%20Language%20Model%20Training%20via%20High%20Quality%20Data%20Curation&entry.906535625=Hongyuan%20Dong%20and%20Zijian%20Kang%20and%20Weijie%20Yin%20and%20Xiao%20Liang%20and%20Chao%20Feng%20and%20Jiao%20Ran&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20SAIL-VL%20%28ScAlable%20Vision%20Language%20Model%20TraIning%0Avia%20High%20QuaLity%20Data%20Curation%29%2C%20an%20open-source%20vision%20language%20model%20%28VLM%29%0Aseries%20achieving%20state-of-the-art%20%28SOTA%29%20performance%20in%202B%20and%208B%20parameters.%0AThe%20following%20three%20key%20improvements%20contribute%20to%20SAIL-VL%27s%20leading%0Aperformance%3A%20%281%29%20Scalable%20high-quality%20visual%20understanding%20data%20construction%3A%0AWe%20implement%20a%20data%20construction%20pipeline%20to%20enable%20hundred-million-scale%0Ahigh-quality%20recaption%20data%20annotation%2C%20and%20the%20resulted%20dataset%20SAIL-Caption%0Ais%20validated%20to%20be%20of%20the%20highest%20data%20quality%20compared%20with%20opensource%0Aalternatives.%20%282%29%20Scalable%20Pretraining%20with%20High-Quality%20Visual%20Understanding%0AData%3A%20We%20scale%20SAIL-VL%27s%20pretraining%20budget%20up%20to%20655B%20tokens%20and%20show%20that%0Aeven%20a%202B%20VLM%20benefits%20from%20scaled%20up%20training%20data%20sizes%2C%20exhibiting%20expected%0Adata%20size%20scaling%20laws%20in%20visual%20understanding%20and%20instruction%20following%0Aperformance.%20%283%29%20Scalable%20SFT%20via%20data%20quantity%20and%20complexity%20scaling%3A%20We%0Acurate%20a%20high-quality%20SFT%20dataset%20collection%20which%20outperforms%20opensource%0Aalternatives%20in%20data%20quantity%20scaling%20effectiveness.%20We%20also%20demonstrate%20that%0Atraining%20with%20progressively%20higher-complexity%20data%20surpasses%20baseline%20one-stage%0Atraining%20by%20a%20large%20margin.%20SAIL-VL%20series%20models%20achieve%20the%20highest%20average%0Ascore%20in%2018%20widely%20used%20VLM%20benchmarks%20in%20our%20evaluation%2C%20with%20the%202B%20model%0Atakes%20the%20top%20position%20over%20VLMs%20of%20comparable%20sizes%20on%20OpenCompass%202024%0A%28https%3A//rank.opencompass.org.cn/leaderboard-multimodal%29%20demonstrating%20robust%0Avisual%20comprehension%20abilities.%20SAIL-VL%20series%20models%20are%20released%20at%0AHuggingFace%20%28https%3A//huggingface.co/BytedanceDouyinContent%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05952v2&entry.124074799=Read"},
{"title": "DINeuro: Distilling Knowledge from 2D Natural Images via Deformable\n  Tubular Transferring Strategy for 3D Neuron Reconstruction", "author": "Yik San Cheng and Runkai Zhao and Heng Wang and Hanchuan Peng and Yui Lo and Yuqian Chen and Lauren J. O'Donnell and Weidong Cai", "abstract": "  Reconstructing neuron morphology from 3D light microscope imaging data is\ncritical to aid neuroscientists in analyzing brain networks and neuroanatomy.\nWith the boost from deep learning techniques, a variety of learning-based\nsegmentation models have been developed to enhance the signal-to-noise ratio of\nraw neuron images as a pre-processing step in the reconstruction workflow.\nHowever, most existing models directly encode the latent representative\nfeatures of volumetric neuron data but neglect their intrinsic morphological\nknowledge. To address this limitation, we design a novel framework that\ndistills the prior knowledge from a 2D Vision Transformer pre-trained on\nextensive 2D natural images to facilitate neuronal morphological learning of\nour 3D Vision Transformer. To bridge the knowledge gap between the 2D natural\nimage and 3D microscopic morphologic domains, we propose a deformable tubular\ntransferring strategy that adapts the pre-trained 2D natural knowledge to the\ninherent tubular characteristics of neuronal structure in the latent embedding\nspace. The experimental results on the Janelia dataset of the BigNeuron project\ndemonstrate that our method achieves a segmentation performance improvement of\n4.53% in mean Dice and 3.56% in mean 95% Hausdorff distance.\n", "link": "http://arxiv.org/abs/2410.22078v2", "date": "2025-02-17", "relevancy": 2.8557, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINeuro%3A%20Distilling%20Knowledge%20from%202D%20Natural%20Images%20via%20Deformable%0A%20%20Tubular%20Transferring%20Strategy%20for%203D%20Neuron%20Reconstruction&body=Title%3A%20DINeuro%3A%20Distilling%20Knowledge%20from%202D%20Natural%20Images%20via%20Deformable%0A%20%20Tubular%20Transferring%20Strategy%20for%203D%20Neuron%20Reconstruction%0AAuthor%3A%20Yik%20San%20Cheng%20and%20Runkai%20Zhao%20and%20Heng%20Wang%20and%20Hanchuan%20Peng%20and%20Yui%20Lo%20and%20Yuqian%20Chen%20and%20Lauren%20J.%20O%27Donnell%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Reconstructing%20neuron%20morphology%20from%203D%20light%20microscope%20imaging%20data%20is%0Acritical%20to%20aid%20neuroscientists%20in%20analyzing%20brain%20networks%20and%20neuroanatomy.%0AWith%20the%20boost%20from%20deep%20learning%20techniques%2C%20a%20variety%20of%20learning-based%0Asegmentation%20models%20have%20been%20developed%20to%20enhance%20the%20signal-to-noise%20ratio%20of%0Araw%20neuron%20images%20as%20a%20pre-processing%20step%20in%20the%20reconstruction%20workflow.%0AHowever%2C%20most%20existing%20models%20directly%20encode%20the%20latent%20representative%0Afeatures%20of%20volumetric%20neuron%20data%20but%20neglect%20their%20intrinsic%20morphological%0Aknowledge.%20To%20address%20this%20limitation%2C%20we%20design%20a%20novel%20framework%20that%0Adistills%20the%20prior%20knowledge%20from%20a%202D%20Vision%20Transformer%20pre-trained%20on%0Aextensive%202D%20natural%20images%20to%20facilitate%20neuronal%20morphological%20learning%20of%0Aour%203D%20Vision%20Transformer.%20To%20bridge%20the%20knowledge%20gap%20between%20the%202D%20natural%0Aimage%20and%203D%20microscopic%20morphologic%20domains%2C%20we%20propose%20a%20deformable%20tubular%0Atransferring%20strategy%20that%20adapts%20the%20pre-trained%202D%20natural%20knowledge%20to%20the%0Ainherent%20tubular%20characteristics%20of%20neuronal%20structure%20in%20the%20latent%20embedding%0Aspace.%20The%20experimental%20results%20on%20the%20Janelia%20dataset%20of%20the%20BigNeuron%20project%0Ademonstrate%20that%20our%20method%20achieves%20a%20segmentation%20performance%20improvement%20of%0A4.53%25%20in%20mean%20Dice%20and%203.56%25%20in%20mean%2095%25%20Hausdorff%20distance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINeuro%253A%2520Distilling%2520Knowledge%2520from%25202D%2520Natural%2520Images%2520via%2520Deformable%250A%2520%2520Tubular%2520Transferring%2520Strategy%2520for%25203D%2520Neuron%2520Reconstruction%26entry.906535625%3DYik%2520San%2520Cheng%2520and%2520Runkai%2520Zhao%2520and%2520Heng%2520Wang%2520and%2520Hanchuan%2520Peng%2520and%2520Yui%2520Lo%2520and%2520Yuqian%2520Chen%2520and%2520Lauren%2520J.%2520O%2527Donnell%2520and%2520Weidong%2520Cai%26entry.1292438233%3D%2520%2520Reconstructing%2520neuron%2520morphology%2520from%25203D%2520light%2520microscope%2520imaging%2520data%2520is%250Acritical%2520to%2520aid%2520neuroscientists%2520in%2520analyzing%2520brain%2520networks%2520and%2520neuroanatomy.%250AWith%2520the%2520boost%2520from%2520deep%2520learning%2520techniques%252C%2520a%2520variety%2520of%2520learning-based%250Asegmentation%2520models%2520have%2520been%2520developed%2520to%2520enhance%2520the%2520signal-to-noise%2520ratio%2520of%250Araw%2520neuron%2520images%2520as%2520a%2520pre-processing%2520step%2520in%2520the%2520reconstruction%2520workflow.%250AHowever%252C%2520most%2520existing%2520models%2520directly%2520encode%2520the%2520latent%2520representative%250Afeatures%2520of%2520volumetric%2520neuron%2520data%2520but%2520neglect%2520their%2520intrinsic%2520morphological%250Aknowledge.%2520To%2520address%2520this%2520limitation%252C%2520we%2520design%2520a%2520novel%2520framework%2520that%250Adistills%2520the%2520prior%2520knowledge%2520from%2520a%25202D%2520Vision%2520Transformer%2520pre-trained%2520on%250Aextensive%25202D%2520natural%2520images%2520to%2520facilitate%2520neuronal%2520morphological%2520learning%2520of%250Aour%25203D%2520Vision%2520Transformer.%2520To%2520bridge%2520the%2520knowledge%2520gap%2520between%2520the%25202D%2520natural%250Aimage%2520and%25203D%2520microscopic%2520morphologic%2520domains%252C%2520we%2520propose%2520a%2520deformable%2520tubular%250Atransferring%2520strategy%2520that%2520adapts%2520the%2520pre-trained%25202D%2520natural%2520knowledge%2520to%2520the%250Ainherent%2520tubular%2520characteristics%2520of%2520neuronal%2520structure%2520in%2520the%2520latent%2520embedding%250Aspace.%2520The%2520experimental%2520results%2520on%2520the%2520Janelia%2520dataset%2520of%2520the%2520BigNeuron%2520project%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520a%2520segmentation%2520performance%2520improvement%2520of%250A4.53%2525%2520in%2520mean%2520Dice%2520and%25203.56%2525%2520in%2520mean%252095%2525%2520Hausdorff%2520distance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINeuro%3A%20Distilling%20Knowledge%20from%202D%20Natural%20Images%20via%20Deformable%0A%20%20Tubular%20Transferring%20Strategy%20for%203D%20Neuron%20Reconstruction&entry.906535625=Yik%20San%20Cheng%20and%20Runkai%20Zhao%20and%20Heng%20Wang%20and%20Hanchuan%20Peng%20and%20Yui%20Lo%20and%20Yuqian%20Chen%20and%20Lauren%20J.%20O%27Donnell%20and%20Weidong%20Cai&entry.1292438233=%20%20Reconstructing%20neuron%20morphology%20from%203D%20light%20microscope%20imaging%20data%20is%0Acritical%20to%20aid%20neuroscientists%20in%20analyzing%20brain%20networks%20and%20neuroanatomy.%0AWith%20the%20boost%20from%20deep%20learning%20techniques%2C%20a%20variety%20of%20learning-based%0Asegmentation%20models%20have%20been%20developed%20to%20enhance%20the%20signal-to-noise%20ratio%20of%0Araw%20neuron%20images%20as%20a%20pre-processing%20step%20in%20the%20reconstruction%20workflow.%0AHowever%2C%20most%20existing%20models%20directly%20encode%20the%20latent%20representative%0Afeatures%20of%20volumetric%20neuron%20data%20but%20neglect%20their%20intrinsic%20morphological%0Aknowledge.%20To%20address%20this%20limitation%2C%20we%20design%20a%20novel%20framework%20that%0Adistills%20the%20prior%20knowledge%20from%20a%202D%20Vision%20Transformer%20pre-trained%20on%0Aextensive%202D%20natural%20images%20to%20facilitate%20neuronal%20morphological%20learning%20of%0Aour%203D%20Vision%20Transformer.%20To%20bridge%20the%20knowledge%20gap%20between%20the%202D%20natural%0Aimage%20and%203D%20microscopic%20morphologic%20domains%2C%20we%20propose%20a%20deformable%20tubular%0Atransferring%20strategy%20that%20adapts%20the%20pre-trained%202D%20natural%20knowledge%20to%20the%0Ainherent%20tubular%20characteristics%20of%20neuronal%20structure%20in%20the%20latent%20embedding%0Aspace.%20The%20experimental%20results%20on%20the%20Janelia%20dataset%20of%20the%20BigNeuron%20project%0Ademonstrate%20that%20our%20method%20achieves%20a%20segmentation%20performance%20improvement%20of%0A4.53%25%20in%20mean%20Dice%20and%203.56%25%20in%20mean%2095%25%20Hausdorff%20distance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22078v2&entry.124074799=Read"},
{"title": "Understanding Figurative Meaning through Explainable Visual Entailment", "author": "Arkadiy Saakyan and Shreyas Kulkarni and Tuhin Chakrabarty and Smaranda Muresan", "abstract": "  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of the capabilities of these models when presented\nwith images and captions containing figurative meaning, such as metaphors or\nhumor. To close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present in the image, in the caption, or both. Using a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning (hallucination and incomplete or unsound reasoning) across classes of\nmodels via human evaluation.\n", "link": "http://arxiv.org/abs/2405.01474v3", "date": "2025-02-17", "relevancy": 2.8372, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Figurative%20Meaning%20through%20Explainable%20Visual%20Entailment&body=Title%3A%20Understanding%20Figurative%20Meaning%20through%20Explainable%20Visual%20Entailment%0AAuthor%3A%20Arkadiy%20Saakyan%20and%20Shreyas%20Kulkarni%20and%20Tuhin%20Chakrabarty%20and%20Smaranda%20Muresan%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20in%0Atasks%20requiring%20a%20fine-grained%20understanding%20of%20literal%20meaning%20in%20images%20and%0Atext%2C%20such%20as%20visual%20question-answering%20or%20visual%20entailment.%20However%2C%20there%0Ahas%20been%20little%20exploration%20of%20the%20capabilities%20of%20these%20models%20when%20presented%0Awith%20images%20and%20captions%20containing%20figurative%20meaning%2C%20such%20as%20metaphors%20or%0Ahumor.%20To%20close%20this%20gap%2C%20we%20propose%20a%20new%20task%20framing%20the%20figurative%20meaning%0Aunderstanding%20problem%20as%20an%20explainable%20visual%20entailment%20task%2C%20where%20the%20model%0Ahas%20to%20predict%20whether%20the%20image%20%28premise%29%20entails%20a%20caption%20%28hypothesis%29%20and%0Ajustify%20the%20predicted%20label%20with%20a%20textual%20explanation.%20The%20figurative%0Aphenomena%20can%20be%20present%20in%20the%20image%2C%20in%20the%20caption%2C%20or%20both.%20Using%20a%0Ahuman-AI%20collaboration%20approach%2C%20we%20build%20the%20accompanying%20expert-verified%0Adataset%20V-FLUTE%2C%20containing%206%2C027%20%7Bimage%2C%20caption%2C%20label%2C%20explanation%7D%0Ainstances%20spanning%20five%20diverse%20figurative%20phenomena%3A%20metaphors%2C%20similes%2C%0Aidioms%2C%20sarcasm%2C%20and%20humor.%20Through%20automatic%20evaluation%2C%20we%20find%20that%20VLMs%0Astruggle%20to%20generalize%20from%20literal%20to%20figurative%20meaning%2C%20particularly%20when%20it%0Ais%20present%20in%20images.%20Further%2C%20we%20identify%20common%20types%20of%20errors%20in%20VLM%0Areasoning%20%28hallucination%20and%20incomplete%20or%20unsound%20reasoning%29%20across%20classes%20of%0Amodels%20via%20human%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01474v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Figurative%2520Meaning%2520through%2520Explainable%2520Visual%2520Entailment%26entry.906535625%3DArkadiy%2520Saakyan%2520and%2520Shreyas%2520Kulkarni%2520and%2520Tuhin%2520Chakrabarty%2520and%2520Smaranda%2520Muresan%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520in%250Atasks%2520requiring%2520a%2520fine-grained%2520understanding%2520of%2520literal%2520meaning%2520in%2520images%2520and%250Atext%252C%2520such%2520as%2520visual%2520question-answering%2520or%2520visual%2520entailment.%2520However%252C%2520there%250Ahas%2520been%2520little%2520exploration%2520of%2520the%2520capabilities%2520of%2520these%2520models%2520when%2520presented%250Awith%2520images%2520and%2520captions%2520containing%2520figurative%2520meaning%252C%2520such%2520as%2520metaphors%2520or%250Ahumor.%2520To%2520close%2520this%2520gap%252C%2520we%2520propose%2520a%2520new%2520task%2520framing%2520the%2520figurative%2520meaning%250Aunderstanding%2520problem%2520as%2520an%2520explainable%2520visual%2520entailment%2520task%252C%2520where%2520the%2520model%250Ahas%2520to%2520predict%2520whether%2520the%2520image%2520%2528premise%2529%2520entails%2520a%2520caption%2520%2528hypothesis%2529%2520and%250Ajustify%2520the%2520predicted%2520label%2520with%2520a%2520textual%2520explanation.%2520The%2520figurative%250Aphenomena%2520can%2520be%2520present%2520in%2520the%2520image%252C%2520in%2520the%2520caption%252C%2520or%2520both.%2520Using%2520a%250Ahuman-AI%2520collaboration%2520approach%252C%2520we%2520build%2520the%2520accompanying%2520expert-verified%250Adataset%2520V-FLUTE%252C%2520containing%25206%252C027%2520%257Bimage%252C%2520caption%252C%2520label%252C%2520explanation%257D%250Ainstances%2520spanning%2520five%2520diverse%2520figurative%2520phenomena%253A%2520metaphors%252C%2520similes%252C%250Aidioms%252C%2520sarcasm%252C%2520and%2520humor.%2520Through%2520automatic%2520evaluation%252C%2520we%2520find%2520that%2520VLMs%250Astruggle%2520to%2520generalize%2520from%2520literal%2520to%2520figurative%2520meaning%252C%2520particularly%2520when%2520it%250Ais%2520present%2520in%2520images.%2520Further%252C%2520we%2520identify%2520common%2520types%2520of%2520errors%2520in%2520VLM%250Areasoning%2520%2528hallucination%2520and%2520incomplete%2520or%2520unsound%2520reasoning%2529%2520across%2520classes%2520of%250Amodels%2520via%2520human%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01474v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Figurative%20Meaning%20through%20Explainable%20Visual%20Entailment&entry.906535625=Arkadiy%20Saakyan%20and%20Shreyas%20Kulkarni%20and%20Tuhin%20Chakrabarty%20and%20Smaranda%20Muresan&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20in%0Atasks%20requiring%20a%20fine-grained%20understanding%20of%20literal%20meaning%20in%20images%20and%0Atext%2C%20such%20as%20visual%20question-answering%20or%20visual%20entailment.%20However%2C%20there%0Ahas%20been%20little%20exploration%20of%20the%20capabilities%20of%20these%20models%20when%20presented%0Awith%20images%20and%20captions%20containing%20figurative%20meaning%2C%20such%20as%20metaphors%20or%0Ahumor.%20To%20close%20this%20gap%2C%20we%20propose%20a%20new%20task%20framing%20the%20figurative%20meaning%0Aunderstanding%20problem%20as%20an%20explainable%20visual%20entailment%20task%2C%20where%20the%20model%0Ahas%20to%20predict%20whether%20the%20image%20%28premise%29%20entails%20a%20caption%20%28hypothesis%29%20and%0Ajustify%20the%20predicted%20label%20with%20a%20textual%20explanation.%20The%20figurative%0Aphenomena%20can%20be%20present%20in%20the%20image%2C%20in%20the%20caption%2C%20or%20both.%20Using%20a%0Ahuman-AI%20collaboration%20approach%2C%20we%20build%20the%20accompanying%20expert-verified%0Adataset%20V-FLUTE%2C%20containing%206%2C027%20%7Bimage%2C%20caption%2C%20label%2C%20explanation%7D%0Ainstances%20spanning%20five%20diverse%20figurative%20phenomena%3A%20metaphors%2C%20similes%2C%0Aidioms%2C%20sarcasm%2C%20and%20humor.%20Through%20automatic%20evaluation%2C%20we%20find%20that%20VLMs%0Astruggle%20to%20generalize%20from%20literal%20to%20figurative%20meaning%2C%20particularly%20when%20it%0Ais%20present%20in%20images.%20Further%2C%20we%20identify%20common%20types%20of%20errors%20in%20VLM%0Areasoning%20%28hallucination%20and%20incomplete%20or%20unsound%20reasoning%29%20across%20classes%20of%0Amodels%20via%20human%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01474v3&entry.124074799=Read"},
{"title": "Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?", "author": "Gon\u00e7alo Gomes and Chrysoula Zerva and Bruno Martins", "abstract": "  The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.\n", "link": "http://arxiv.org/abs/2502.06600v2", "date": "2025-02-17", "relevancy": 2.8198, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Multilingual%20Image%20Captioning%3A%20How%20far%20can%20we%20get%20with%0A%20%20CLIP%20models%3F&body=Title%3A%20Evaluation%20of%20Multilingual%20Image%20Captioning%3A%20How%20far%20can%20we%20get%20with%0A%20%20CLIP%20models%3F%0AAuthor%3A%20Gon%C3%A7alo%20Gomes%20and%20Chrysoula%20Zerva%20and%20Bruno%20Martins%0AAbstract%3A%20%20%20The%20evaluation%20of%20image%20captions%2C%20looking%20at%20both%20linguistic%20fluency%20and%0Asemantic%20correspondence%20to%20visual%20contents%2C%20has%20witnessed%20a%20significant%20effort.%0AStill%2C%20despite%20advancements%20such%20as%20the%20CLIPScore%20metric%2C%20multilingual%0Acaptioning%20evaluation%20has%20remained%20relatively%20unexplored.%20This%20work%20presents%0Aseveral%20strategies%2C%20and%20extensive%20experiments%2C%20related%20to%20evaluating%20CLIPScore%0Avariants%20in%20multilingual%20settings.%20To%20address%20the%20lack%20of%20multilingual%20test%0Adata%2C%20we%20consider%20two%20different%20strategies%3A%20%281%29%20using%20quality%20aware%0Amachine-translated%20datasets%20with%20human%20judgements%2C%20and%20%282%29%20re-purposing%0Amultilingual%20datasets%20that%20target%20semantic%20inference%20and%20reasoning.%20Our%20results%0Ahighlight%20the%20potential%20of%20finetuned%20multilingual%20models%20to%20generalize%20across%0Alanguages%20and%20to%20handle%20complex%20linguistic%20challenges.%20Tests%20with%0Amachine-translated%20data%20show%20that%20multilingual%20CLIPScore%20models%20can%20maintain%20a%0Ahigh%20correlation%20with%20human%20judgements%20across%20different%20languages%2C%20and%0Aadditional%20tests%20with%20natively%20multilingual%20and%20multicultural%20data%20further%0Aattest%20to%20the%20high-quality%20assessments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06600v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Multilingual%2520Image%2520Captioning%253A%2520How%2520far%2520can%2520we%2520get%2520with%250A%2520%2520CLIP%2520models%253F%26entry.906535625%3DGon%25C3%25A7alo%2520Gomes%2520and%2520Chrysoula%2520Zerva%2520and%2520Bruno%2520Martins%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520image%2520captions%252C%2520looking%2520at%2520both%2520linguistic%2520fluency%2520and%250Asemantic%2520correspondence%2520to%2520visual%2520contents%252C%2520has%2520witnessed%2520a%2520significant%2520effort.%250AStill%252C%2520despite%2520advancements%2520such%2520as%2520the%2520CLIPScore%2520metric%252C%2520multilingual%250Acaptioning%2520evaluation%2520has%2520remained%2520relatively%2520unexplored.%2520This%2520work%2520presents%250Aseveral%2520strategies%252C%2520and%2520extensive%2520experiments%252C%2520related%2520to%2520evaluating%2520CLIPScore%250Avariants%2520in%2520multilingual%2520settings.%2520To%2520address%2520the%2520lack%2520of%2520multilingual%2520test%250Adata%252C%2520we%2520consider%2520two%2520different%2520strategies%253A%2520%25281%2529%2520using%2520quality%2520aware%250Amachine-translated%2520datasets%2520with%2520human%2520judgements%252C%2520and%2520%25282%2529%2520re-purposing%250Amultilingual%2520datasets%2520that%2520target%2520semantic%2520inference%2520and%2520reasoning.%2520Our%2520results%250Ahighlight%2520the%2520potential%2520of%2520finetuned%2520multilingual%2520models%2520to%2520generalize%2520across%250Alanguages%2520and%2520to%2520handle%2520complex%2520linguistic%2520challenges.%2520Tests%2520with%250Amachine-translated%2520data%2520show%2520that%2520multilingual%2520CLIPScore%2520models%2520can%2520maintain%2520a%250Ahigh%2520correlation%2520with%2520human%2520judgements%2520across%2520different%2520languages%252C%2520and%250Aadditional%2520tests%2520with%2520natively%2520multilingual%2520and%2520multicultural%2520data%2520further%250Aattest%2520to%2520the%2520high-quality%2520assessments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06600v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Multilingual%20Image%20Captioning%3A%20How%20far%20can%20we%20get%20with%0A%20%20CLIP%20models%3F&entry.906535625=Gon%C3%A7alo%20Gomes%20and%20Chrysoula%20Zerva%20and%20Bruno%20Martins&entry.1292438233=%20%20The%20evaluation%20of%20image%20captions%2C%20looking%20at%20both%20linguistic%20fluency%20and%0Asemantic%20correspondence%20to%20visual%20contents%2C%20has%20witnessed%20a%20significant%20effort.%0AStill%2C%20despite%20advancements%20such%20as%20the%20CLIPScore%20metric%2C%20multilingual%0Acaptioning%20evaluation%20has%20remained%20relatively%20unexplored.%20This%20work%20presents%0Aseveral%20strategies%2C%20and%20extensive%20experiments%2C%20related%20to%20evaluating%20CLIPScore%0Avariants%20in%20multilingual%20settings.%20To%20address%20the%20lack%20of%20multilingual%20test%0Adata%2C%20we%20consider%20two%20different%20strategies%3A%20%281%29%20using%20quality%20aware%0Amachine-translated%20datasets%20with%20human%20judgements%2C%20and%20%282%29%20re-purposing%0Amultilingual%20datasets%20that%20target%20semantic%20inference%20and%20reasoning.%20Our%20results%0Ahighlight%20the%20potential%20of%20finetuned%20multilingual%20models%20to%20generalize%20across%0Alanguages%20and%20to%20handle%20complex%20linguistic%20challenges.%20Tests%20with%0Amachine-translated%20data%20show%20that%20multilingual%20CLIPScore%20models%20can%20maintain%20a%0Ahigh%20correlation%20with%20human%20judgements%20across%20different%20languages%2C%20and%0Aadditional%20tests%20with%20natively%20multilingual%20and%20multicultural%20data%20further%0Aattest%20to%20the%20high-quality%20assessments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06600v2&entry.124074799=Read"},
{"title": "LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked\n  Entities", "author": "Florian Sestak and Artur Toshev and Andreas F\u00fcrst and G\u00fcnter Klambauer and Andreas Mayr and Johannes Brandstetter", "abstract": "  Generative models are spearheading recent progress in deep learning, showing\nstrong promise for trajectory sampling in dynamical systems as well. However,\nwhile latent space modeling paradigms have transformed image and video\ngeneration, similar approaches are more difficult for most dynamical systems.\nSuch systems -- from chemical molecule structures to collective human behavior\n-- are described by interactions of entities, making them inherently linked to\nconnectivity patterns and the traceability of entities over time. Our approach,\nLaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked\nEntities), combines the advantages of graph neural networks, i.e., the\ntraceability of entities across time-steps, with the efficiency and scalability\nof recent advances in image and video generation, where pre-trained encoder and\ndecoder are frozen to enable generative modeling in the latent space. The core\nidea of LaM-SLidE is to introduce identifier representations (IDs) to allow for\nretrieval of entity properties, e.g., entity coordinates, from latent system\nrepresentations and thus enables traceability. Experimentally, across different\ndomains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,\nand generalizability. (Code is available at\nhttps://github.com/ml-jku/LaM-SLidE)\n", "link": "http://arxiv.org/abs/2502.12128v1", "date": "2025-02-17", "relevancy": 2.8144, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5762}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5639}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaM-SLidE%3A%20Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%0A%20%20Entities&body=Title%3A%20LaM-SLidE%3A%20Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%0A%20%20Entities%0AAuthor%3A%20Florian%20Sestak%20and%20Artur%20Toshev%20and%20Andreas%20F%C3%BCrst%20and%20G%C3%BCnter%20Klambauer%20and%20Andreas%20Mayr%20and%20Johannes%20Brandstetter%0AAbstract%3A%20%20%20Generative%20models%20are%20spearheading%20recent%20progress%20in%20deep%20learning%2C%20showing%0Astrong%20promise%20for%20trajectory%20sampling%20in%20dynamical%20systems%20as%20well.%20However%2C%0Awhile%20latent%20space%20modeling%20paradigms%20have%20transformed%20image%20and%20video%0Ageneration%2C%20similar%20approaches%20are%20more%20difficult%20for%20most%20dynamical%20systems.%0ASuch%20systems%20--%20from%20chemical%20molecule%20structures%20to%20collective%20human%20behavior%0A--%20are%20described%20by%20interactions%20of%20entities%2C%20making%20them%20inherently%20linked%20to%0Aconnectivity%20patterns%20and%20the%20traceability%20of%20entities%20over%20time.%20Our%20approach%2C%0ALaM-SLidE%20%28Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%0AEntities%29%2C%20combines%20the%20advantages%20of%20graph%20neural%20networks%2C%20i.e.%2C%20the%0Atraceability%20of%20entities%20across%20time-steps%2C%20with%20the%20efficiency%20and%20scalability%0Aof%20recent%20advances%20in%20image%20and%20video%20generation%2C%20where%20pre-trained%20encoder%20and%0Adecoder%20are%20frozen%20to%20enable%20generative%20modeling%20in%20the%20latent%20space.%20The%20core%0Aidea%20of%20LaM-SLidE%20is%20to%20introduce%20identifier%20representations%20%28IDs%29%20to%20allow%20for%0Aretrieval%20of%20entity%20properties%2C%20e.g.%2C%20entity%20coordinates%2C%20from%20latent%20system%0Arepresentations%20and%20thus%20enables%20traceability.%20Experimentally%2C%20across%20different%0Adomains%2C%20we%20show%20that%20LaM-SLidE%20performs%20favorably%20in%20terms%20of%20speed%2C%20accuracy%2C%0Aand%20generalizability.%20%28Code%20is%20available%20at%0Ahttps%3A//github.com/ml-jku/LaM-SLidE%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaM-SLidE%253A%2520Latent%2520Space%2520Modeling%2520of%2520Spatial%2520Dynamical%2520Systems%2520via%2520Linked%250A%2520%2520Entities%26entry.906535625%3DFlorian%2520Sestak%2520and%2520Artur%2520Toshev%2520and%2520Andreas%2520F%25C3%25BCrst%2520and%2520G%25C3%25BCnter%2520Klambauer%2520and%2520Andreas%2520Mayr%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3D%2520%2520Generative%2520models%2520are%2520spearheading%2520recent%2520progress%2520in%2520deep%2520learning%252C%2520showing%250Astrong%2520promise%2520for%2520trajectory%2520sampling%2520in%2520dynamical%2520systems%2520as%2520well.%2520However%252C%250Awhile%2520latent%2520space%2520modeling%2520paradigms%2520have%2520transformed%2520image%2520and%2520video%250Ageneration%252C%2520similar%2520approaches%2520are%2520more%2520difficult%2520for%2520most%2520dynamical%2520systems.%250ASuch%2520systems%2520--%2520from%2520chemical%2520molecule%2520structures%2520to%2520collective%2520human%2520behavior%250A--%2520are%2520described%2520by%2520interactions%2520of%2520entities%252C%2520making%2520them%2520inherently%2520linked%2520to%250Aconnectivity%2520patterns%2520and%2520the%2520traceability%2520of%2520entities%2520over%2520time.%2520Our%2520approach%252C%250ALaM-SLidE%2520%2528Latent%2520Space%2520Modeling%2520of%2520Spatial%2520Dynamical%2520Systems%2520via%2520Linked%250AEntities%2529%252C%2520combines%2520the%2520advantages%2520of%2520graph%2520neural%2520networks%252C%2520i.e.%252C%2520the%250Atraceability%2520of%2520entities%2520across%2520time-steps%252C%2520with%2520the%2520efficiency%2520and%2520scalability%250Aof%2520recent%2520advances%2520in%2520image%2520and%2520video%2520generation%252C%2520where%2520pre-trained%2520encoder%2520and%250Adecoder%2520are%2520frozen%2520to%2520enable%2520generative%2520modeling%2520in%2520the%2520latent%2520space.%2520The%2520core%250Aidea%2520of%2520LaM-SLidE%2520is%2520to%2520introduce%2520identifier%2520representations%2520%2528IDs%2529%2520to%2520allow%2520for%250Aretrieval%2520of%2520entity%2520properties%252C%2520e.g.%252C%2520entity%2520coordinates%252C%2520from%2520latent%2520system%250Arepresentations%2520and%2520thus%2520enables%2520traceability.%2520Experimentally%252C%2520across%2520different%250Adomains%252C%2520we%2520show%2520that%2520LaM-SLidE%2520performs%2520favorably%2520in%2520terms%2520of%2520speed%252C%2520accuracy%252C%250Aand%2520generalizability.%2520%2528Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ml-jku/LaM-SLidE%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaM-SLidE%3A%20Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%0A%20%20Entities&entry.906535625=Florian%20Sestak%20and%20Artur%20Toshev%20and%20Andreas%20F%C3%BCrst%20and%20G%C3%BCnter%20Klambauer%20and%20Andreas%20Mayr%20and%20Johannes%20Brandstetter&entry.1292438233=%20%20Generative%20models%20are%20spearheading%20recent%20progress%20in%20deep%20learning%2C%20showing%0Astrong%20promise%20for%20trajectory%20sampling%20in%20dynamical%20systems%20as%20well.%20However%2C%0Awhile%20latent%20space%20modeling%20paradigms%20have%20transformed%20image%20and%20video%0Ageneration%2C%20similar%20approaches%20are%20more%20difficult%20for%20most%20dynamical%20systems.%0ASuch%20systems%20--%20from%20chemical%20molecule%20structures%20to%20collective%20human%20behavior%0A--%20are%20described%20by%20interactions%20of%20entities%2C%20making%20them%20inherently%20linked%20to%0Aconnectivity%20patterns%20and%20the%20traceability%20of%20entities%20over%20time.%20Our%20approach%2C%0ALaM-SLidE%20%28Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%0AEntities%29%2C%20combines%20the%20advantages%20of%20graph%20neural%20networks%2C%20i.e.%2C%20the%0Atraceability%20of%20entities%20across%20time-steps%2C%20with%20the%20efficiency%20and%20scalability%0Aof%20recent%20advances%20in%20image%20and%20video%20generation%2C%20where%20pre-trained%20encoder%20and%0Adecoder%20are%20frozen%20to%20enable%20generative%20modeling%20in%20the%20latent%20space.%20The%20core%0Aidea%20of%20LaM-SLidE%20is%20to%20introduce%20identifier%20representations%20%28IDs%29%20to%20allow%20for%0Aretrieval%20of%20entity%20properties%2C%20e.g.%2C%20entity%20coordinates%2C%20from%20latent%20system%0Arepresentations%20and%20thus%20enables%20traceability.%20Experimentally%2C%20across%20different%0Adomains%2C%20we%20show%20that%20LaM-SLidE%20performs%20favorably%20in%20terms%20of%20speed%2C%20accuracy%2C%0Aand%20generalizability.%20%28Code%20is%20available%20at%0Ahttps%3A//github.com/ml-jku/LaM-SLidE%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12128v1&entry.124074799=Read"},
{"title": "Vision CNNs trained to estimate spatial latents learned similar\n  ventral-stream-aligned representations", "author": "Yudi Xie and Weichen Huang and Esther Alter and Jeremy Schwartz and Joshua B. Tenenbaum and James J. DiCarlo", "abstract": "  Studies of the functional role of the primate ventral visual stream have\ntraditionally focused on object categorization, often ignoring -- despite much\nprior evidence -- its role in estimating \"spatial\" latents such as object\nposition and pose. Most leading ventral stream models are derived by optimizing\nnetworks for object categorization, which seems to imply that the ventral\nstream is also derived under such an objective. Here, we explore an alternative\nhypothesis: Might the ventral stream be optimized for estimating spatial\nlatents? And a closely related question: How different -- if at all -- are\nrepresentations learned from spatial latent estimation compared to\ncategorization? To ask these questions, we leveraged synthetic image datasets\ngenerated by a 3D graphic engine and trained convolutional neural networks\n(CNNs) to estimate different combinations of spatial and category latents. We\nfound that models trained to estimate just a few spatial latents achieve neural\nalignment scores comparable to those trained on hundreds of categories, and the\nspatial latent performance of models strongly correlates with their neural\nalignment. Spatial latent and category-trained models have very similar -- but\nnot identical -- internal representations, especially in their early and middle\nlayers. We provide evidence that this convergence is partly driven by\nnon-target latent variability in the training data, which facilitates the\nimplicit learning of representations of those non-target latents. Taken\ntogether, these results suggest that many training objectives, such as spatial\nlatents, can lead to similar models aligned neurally with the ventral stream.\nThus, one should not assume that the ventral stream is optimized for object\ncategorization only. As a field, we need to continue to sharpen our measures of\ncomparing models to brains to better understand the functional roles of the\nventral stream.\n", "link": "http://arxiv.org/abs/2412.09115v2", "date": "2025-02-17", "relevancy": 2.7895, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20CNNs%20trained%20to%20estimate%20spatial%20latents%20learned%20similar%0A%20%20ventral-stream-aligned%20representations&body=Title%3A%20Vision%20CNNs%20trained%20to%20estimate%20spatial%20latents%20learned%20similar%0A%20%20ventral-stream-aligned%20representations%0AAuthor%3A%20Yudi%20Xie%20and%20Weichen%20Huang%20and%20Esther%20Alter%20and%20Jeremy%20Schwartz%20and%20Joshua%20B.%20Tenenbaum%20and%20James%20J.%20DiCarlo%0AAbstract%3A%20%20%20Studies%20of%20the%20functional%20role%20of%20the%20primate%20ventral%20visual%20stream%20have%0Atraditionally%20focused%20on%20object%20categorization%2C%20often%20ignoring%20--%20despite%20much%0Aprior%20evidence%20--%20its%20role%20in%20estimating%20%22spatial%22%20latents%20such%20as%20object%0Aposition%20and%20pose.%20Most%20leading%20ventral%20stream%20models%20are%20derived%20by%20optimizing%0Anetworks%20for%20object%20categorization%2C%20which%20seems%20to%20imply%20that%20the%20ventral%0Astream%20is%20also%20derived%20under%20such%20an%20objective.%20Here%2C%20we%20explore%20an%20alternative%0Ahypothesis%3A%20Might%20the%20ventral%20stream%20be%20optimized%20for%20estimating%20spatial%0Alatents%3F%20And%20a%20closely%20related%20question%3A%20How%20different%20--%20if%20at%20all%20--%20are%0Arepresentations%20learned%20from%20spatial%20latent%20estimation%20compared%20to%0Acategorization%3F%20To%20ask%20these%20questions%2C%20we%20leveraged%20synthetic%20image%20datasets%0Agenerated%20by%20a%203D%20graphic%20engine%20and%20trained%20convolutional%20neural%20networks%0A%28CNNs%29%20to%20estimate%20different%20combinations%20of%20spatial%20and%20category%20latents.%20We%0Afound%20that%20models%20trained%20to%20estimate%20just%20a%20few%20spatial%20latents%20achieve%20neural%0Aalignment%20scores%20comparable%20to%20those%20trained%20on%20hundreds%20of%20categories%2C%20and%20the%0Aspatial%20latent%20performance%20of%20models%20strongly%20correlates%20with%20their%20neural%0Aalignment.%20Spatial%20latent%20and%20category-trained%20models%20have%20very%20similar%20--%20but%0Anot%20identical%20--%20internal%20representations%2C%20especially%20in%20their%20early%20and%20middle%0Alayers.%20We%20provide%20evidence%20that%20this%20convergence%20is%20partly%20driven%20by%0Anon-target%20latent%20variability%20in%20the%20training%20data%2C%20which%20facilitates%20the%0Aimplicit%20learning%20of%20representations%20of%20those%20non-target%20latents.%20Taken%0Atogether%2C%20these%20results%20suggest%20that%20many%20training%20objectives%2C%20such%20as%20spatial%0Alatents%2C%20can%20lead%20to%20similar%20models%20aligned%20neurally%20with%20the%20ventral%20stream.%0AThus%2C%20one%20should%20not%20assume%20that%20the%20ventral%20stream%20is%20optimized%20for%20object%0Acategorization%20only.%20As%20a%20field%2C%20we%20need%20to%20continue%20to%20sharpen%20our%20measures%20of%0Acomparing%20models%20to%20brains%20to%20better%20understand%20the%20functional%20roles%20of%20the%0Aventral%20stream.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520CNNs%2520trained%2520to%2520estimate%2520spatial%2520latents%2520learned%2520similar%250A%2520%2520ventral-stream-aligned%2520representations%26entry.906535625%3DYudi%2520Xie%2520and%2520Weichen%2520Huang%2520and%2520Esther%2520Alter%2520and%2520Jeremy%2520Schwartz%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520James%2520J.%2520DiCarlo%26entry.1292438233%3D%2520%2520Studies%2520of%2520the%2520functional%2520role%2520of%2520the%2520primate%2520ventral%2520visual%2520stream%2520have%250Atraditionally%2520focused%2520on%2520object%2520categorization%252C%2520often%2520ignoring%2520--%2520despite%2520much%250Aprior%2520evidence%2520--%2520its%2520role%2520in%2520estimating%2520%2522spatial%2522%2520latents%2520such%2520as%2520object%250Aposition%2520and%2520pose.%2520Most%2520leading%2520ventral%2520stream%2520models%2520are%2520derived%2520by%2520optimizing%250Anetworks%2520for%2520object%2520categorization%252C%2520which%2520seems%2520to%2520imply%2520that%2520the%2520ventral%250Astream%2520is%2520also%2520derived%2520under%2520such%2520an%2520objective.%2520Here%252C%2520we%2520explore%2520an%2520alternative%250Ahypothesis%253A%2520Might%2520the%2520ventral%2520stream%2520be%2520optimized%2520for%2520estimating%2520spatial%250Alatents%253F%2520And%2520a%2520closely%2520related%2520question%253A%2520How%2520different%2520--%2520if%2520at%2520all%2520--%2520are%250Arepresentations%2520learned%2520from%2520spatial%2520latent%2520estimation%2520compared%2520to%250Acategorization%253F%2520To%2520ask%2520these%2520questions%252C%2520we%2520leveraged%2520synthetic%2520image%2520datasets%250Agenerated%2520by%2520a%25203D%2520graphic%2520engine%2520and%2520trained%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520to%2520estimate%2520different%2520combinations%2520of%2520spatial%2520and%2520category%2520latents.%2520We%250Afound%2520that%2520models%2520trained%2520to%2520estimate%2520just%2520a%2520few%2520spatial%2520latents%2520achieve%2520neural%250Aalignment%2520scores%2520comparable%2520to%2520those%2520trained%2520on%2520hundreds%2520of%2520categories%252C%2520and%2520the%250Aspatial%2520latent%2520performance%2520of%2520models%2520strongly%2520correlates%2520with%2520their%2520neural%250Aalignment.%2520Spatial%2520latent%2520and%2520category-trained%2520models%2520have%2520very%2520similar%2520--%2520but%250Anot%2520identical%2520--%2520internal%2520representations%252C%2520especially%2520in%2520their%2520early%2520and%2520middle%250Alayers.%2520We%2520provide%2520evidence%2520that%2520this%2520convergence%2520is%2520partly%2520driven%2520by%250Anon-target%2520latent%2520variability%2520in%2520the%2520training%2520data%252C%2520which%2520facilitates%2520the%250Aimplicit%2520learning%2520of%2520representations%2520of%2520those%2520non-target%2520latents.%2520Taken%250Atogether%252C%2520these%2520results%2520suggest%2520that%2520many%2520training%2520objectives%252C%2520such%2520as%2520spatial%250Alatents%252C%2520can%2520lead%2520to%2520similar%2520models%2520aligned%2520neurally%2520with%2520the%2520ventral%2520stream.%250AThus%252C%2520one%2520should%2520not%2520assume%2520that%2520the%2520ventral%2520stream%2520is%2520optimized%2520for%2520object%250Acategorization%2520only.%2520As%2520a%2520field%252C%2520we%2520need%2520to%2520continue%2520to%2520sharpen%2520our%2520measures%2520of%250Acomparing%2520models%2520to%2520brains%2520to%2520better%2520understand%2520the%2520functional%2520roles%2520of%2520the%250Aventral%2520stream.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20CNNs%20trained%20to%20estimate%20spatial%20latents%20learned%20similar%0A%20%20ventral-stream-aligned%20representations&entry.906535625=Yudi%20Xie%20and%20Weichen%20Huang%20and%20Esther%20Alter%20and%20Jeremy%20Schwartz%20and%20Joshua%20B.%20Tenenbaum%20and%20James%20J.%20DiCarlo&entry.1292438233=%20%20Studies%20of%20the%20functional%20role%20of%20the%20primate%20ventral%20visual%20stream%20have%0Atraditionally%20focused%20on%20object%20categorization%2C%20often%20ignoring%20--%20despite%20much%0Aprior%20evidence%20--%20its%20role%20in%20estimating%20%22spatial%22%20latents%20such%20as%20object%0Aposition%20and%20pose.%20Most%20leading%20ventral%20stream%20models%20are%20derived%20by%20optimizing%0Anetworks%20for%20object%20categorization%2C%20which%20seems%20to%20imply%20that%20the%20ventral%0Astream%20is%20also%20derived%20under%20such%20an%20objective.%20Here%2C%20we%20explore%20an%20alternative%0Ahypothesis%3A%20Might%20the%20ventral%20stream%20be%20optimized%20for%20estimating%20spatial%0Alatents%3F%20And%20a%20closely%20related%20question%3A%20How%20different%20--%20if%20at%20all%20--%20are%0Arepresentations%20learned%20from%20spatial%20latent%20estimation%20compared%20to%0Acategorization%3F%20To%20ask%20these%20questions%2C%20we%20leveraged%20synthetic%20image%20datasets%0Agenerated%20by%20a%203D%20graphic%20engine%20and%20trained%20convolutional%20neural%20networks%0A%28CNNs%29%20to%20estimate%20different%20combinations%20of%20spatial%20and%20category%20latents.%20We%0Afound%20that%20models%20trained%20to%20estimate%20just%20a%20few%20spatial%20latents%20achieve%20neural%0Aalignment%20scores%20comparable%20to%20those%20trained%20on%20hundreds%20of%20categories%2C%20and%20the%0Aspatial%20latent%20performance%20of%20models%20strongly%20correlates%20with%20their%20neural%0Aalignment.%20Spatial%20latent%20and%20category-trained%20models%20have%20very%20similar%20--%20but%0Anot%20identical%20--%20internal%20representations%2C%20especially%20in%20their%20early%20and%20middle%0Alayers.%20We%20provide%20evidence%20that%20this%20convergence%20is%20partly%20driven%20by%0Anon-target%20latent%20variability%20in%20the%20training%20data%2C%20which%20facilitates%20the%0Aimplicit%20learning%20of%20representations%20of%20those%20non-target%20latents.%20Taken%0Atogether%2C%20these%20results%20suggest%20that%20many%20training%20objectives%2C%20such%20as%20spatial%0Alatents%2C%20can%20lead%20to%20similar%20models%20aligned%20neurally%20with%20the%20ventral%20stream.%0AThus%2C%20one%20should%20not%20assume%20that%20the%20ventral%20stream%20is%20optimized%20for%20object%0Acategorization%20only.%20As%20a%20field%2C%20we%20need%20to%20continue%20to%20sharpen%20our%20measures%20of%0Acomparing%20models%20to%20brains%20to%20better%20understand%20the%20functional%20roles%20of%20the%0Aventral%20stream.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09115v2&entry.124074799=Read"},
{"title": "VRoPE: Rotary Position Embedding for Video Large Language Models", "author": "Zikang Liu and Longteng Guo and Yepeng Tang and Junxian Cai and Kai Ma and Xi Chen and Jing Liu", "abstract": "  Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Our approach restructures positional\nindices to preserve spatial coherence and ensure a smooth transition between\nvideo and text tokens. Additionally, we introduce a more balanced encoding\nstrategy that mitigates attention biases, ensuring a more uniform distribution\nof spatial focus. Extensive experiments on Vicuna and Qwen2 across different\nmodel scales demonstrate that VRoPE consistently outperforms previous RoPE\nvariants, achieving significant improvements in video understanding, temporal\nreasoning, and retrieval tasks. Code will be available at\nhttps://github.com/johncaged/VRoPE\n", "link": "http://arxiv.org/abs/2502.11664v1", "date": "2025-02-17", "relevancy": 2.78, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5585}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRoPE%3A%20Rotary%20Position%20Embedding%20for%20Video%20Large%20Language%20Models&body=Title%3A%20VRoPE%3A%20Rotary%20Position%20Embedding%20for%20Video%20Large%20Language%20Models%0AAuthor%3A%20Zikang%20Liu%20and%20Longteng%20Guo%20and%20Yepeng%20Tang%20and%20Junxian%20Cai%20and%20Kai%20Ma%20and%20Xi%20Chen%20and%20Jing%20Liu%0AAbstract%3A%20%20%20Rotary%20Position%20Embedding%20%28RoPE%29%20has%20shown%20strong%20performance%20in%20text-based%0ALarge%20Language%20Models%20%28LLMs%29%2C%20but%20extending%20it%20to%20video%20remains%20a%20challenge%20due%0Ato%20the%20intricate%20spatiotemporal%20structure%20of%20video%20frames.%20Existing%0Aadaptations%2C%20such%20as%20RoPE-3D%2C%20attempt%20to%20encode%20spatial%20and%20temporal%20dimensions%0Aseparately%20but%20suffer%20from%20two%20major%20limitations%3A%20positional%20bias%20in%20attention%0Adistribution%20and%20disruptions%20in%20video-text%20transitions.%20To%20overcome%20these%0Aissues%2C%20we%20propose%20Video%20Rotary%20Position%20Embedding%20%28VRoPE%29%2C%20a%20novel%20positional%0Aencoding%20method%20tailored%20for%20Video-LLMs.%20Our%20approach%20restructures%20positional%0Aindices%20to%20preserve%20spatial%20coherence%20and%20ensure%20a%20smooth%20transition%20between%0Avideo%20and%20text%20tokens.%20Additionally%2C%20we%20introduce%20a%20more%20balanced%20encoding%0Astrategy%20that%20mitigates%20attention%20biases%2C%20ensuring%20a%20more%20uniform%20distribution%0Aof%20spatial%20focus.%20Extensive%20experiments%20on%20Vicuna%20and%20Qwen2%20across%20different%0Amodel%20scales%20demonstrate%20that%20VRoPE%20consistently%20outperforms%20previous%20RoPE%0Avariants%2C%20achieving%20significant%20improvements%20in%20video%20understanding%2C%20temporal%0Areasoning%2C%20and%20retrieval%20tasks.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/johncaged/VRoPE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRoPE%253A%2520Rotary%2520Position%2520Embedding%2520for%2520Video%2520Large%2520Language%2520Models%26entry.906535625%3DZikang%2520Liu%2520and%2520Longteng%2520Guo%2520and%2520Yepeng%2520Tang%2520and%2520Junxian%2520Cai%2520and%2520Kai%2520Ma%2520and%2520Xi%2520Chen%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520Rotary%2520Position%2520Embedding%2520%2528RoPE%2529%2520has%2520shown%2520strong%2520performance%2520in%2520text-based%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520but%2520extending%2520it%2520to%2520video%2520remains%2520a%2520challenge%2520due%250Ato%2520the%2520intricate%2520spatiotemporal%2520structure%2520of%2520video%2520frames.%2520Existing%250Aadaptations%252C%2520such%2520as%2520RoPE-3D%252C%2520attempt%2520to%2520encode%2520spatial%2520and%2520temporal%2520dimensions%250Aseparately%2520but%2520suffer%2520from%2520two%2520major%2520limitations%253A%2520positional%2520bias%2520in%2520attention%250Adistribution%2520and%2520disruptions%2520in%2520video-text%2520transitions.%2520To%2520overcome%2520these%250Aissues%252C%2520we%2520propose%2520Video%2520Rotary%2520Position%2520Embedding%2520%2528VRoPE%2529%252C%2520a%2520novel%2520positional%250Aencoding%2520method%2520tailored%2520for%2520Video-LLMs.%2520Our%2520approach%2520restructures%2520positional%250Aindices%2520to%2520preserve%2520spatial%2520coherence%2520and%2520ensure%2520a%2520smooth%2520transition%2520between%250Avideo%2520and%2520text%2520tokens.%2520Additionally%252C%2520we%2520introduce%2520a%2520more%2520balanced%2520encoding%250Astrategy%2520that%2520mitigates%2520attention%2520biases%252C%2520ensuring%2520a%2520more%2520uniform%2520distribution%250Aof%2520spatial%2520focus.%2520Extensive%2520experiments%2520on%2520Vicuna%2520and%2520Qwen2%2520across%2520different%250Amodel%2520scales%2520demonstrate%2520that%2520VRoPE%2520consistently%2520outperforms%2520previous%2520RoPE%250Avariants%252C%2520achieving%2520significant%2520improvements%2520in%2520video%2520understanding%252C%2520temporal%250Areasoning%252C%2520and%2520retrieval%2520tasks.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/johncaged/VRoPE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRoPE%3A%20Rotary%20Position%20Embedding%20for%20Video%20Large%20Language%20Models&entry.906535625=Zikang%20Liu%20and%20Longteng%20Guo%20and%20Yepeng%20Tang%20and%20Junxian%20Cai%20and%20Kai%20Ma%20and%20Xi%20Chen%20and%20Jing%20Liu&entry.1292438233=%20%20Rotary%20Position%20Embedding%20%28RoPE%29%20has%20shown%20strong%20performance%20in%20text-based%0ALarge%20Language%20Models%20%28LLMs%29%2C%20but%20extending%20it%20to%20video%20remains%20a%20challenge%20due%0Ato%20the%20intricate%20spatiotemporal%20structure%20of%20video%20frames.%20Existing%0Aadaptations%2C%20such%20as%20RoPE-3D%2C%20attempt%20to%20encode%20spatial%20and%20temporal%20dimensions%0Aseparately%20but%20suffer%20from%20two%20major%20limitations%3A%20positional%20bias%20in%20attention%0Adistribution%20and%20disruptions%20in%20video-text%20transitions.%20To%20overcome%20these%0Aissues%2C%20we%20propose%20Video%20Rotary%20Position%20Embedding%20%28VRoPE%29%2C%20a%20novel%20positional%0Aencoding%20method%20tailored%20for%20Video-LLMs.%20Our%20approach%20restructures%20positional%0Aindices%20to%20preserve%20spatial%20coherence%20and%20ensure%20a%20smooth%20transition%20between%0Avideo%20and%20text%20tokens.%20Additionally%2C%20we%20introduce%20a%20more%20balanced%20encoding%0Astrategy%20that%20mitigates%20attention%20biases%2C%20ensuring%20a%20more%20uniform%20distribution%0Aof%20spatial%20focus.%20Extensive%20experiments%20on%20Vicuna%20and%20Qwen2%20across%20different%0Amodel%20scales%20demonstrate%20that%20VRoPE%20consistently%20outperforms%20previous%20RoPE%0Avariants%2C%20achieving%20significant%20improvements%20in%20video%20understanding%2C%20temporal%0Areasoning%2C%20and%20retrieval%20tasks.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/johncaged/VRoPE%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11664v1&entry.124074799=Read"},
{"title": "\"See the World, Discover Knowledge\": A Chinese Factuality Evaluation for\n  Large Vision Language Models", "author": "Jihao Gu and Yingyao Wang and Pi Bu and Chen Wang and Ziming Wang and Tengtao Song and Donglai Wei and Jiale Yuan and Yingxiu Zhao and Yancheng He and Shilong Li and Jiaheng Liu and Meng Cao and Jun Song and Yingshui Tan and Xiang Li and Wenbo Su and Zhicheng Zheng and Xiaoyong Zhu and Bo Zheng", "abstract": "  The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.\n", "link": "http://arxiv.org/abs/2502.11718v1", "date": "2025-02-17", "relevancy": 2.7744, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%20Factuality%20Evaluation%20for%0A%20%20Large%20Vision%20Language%20Models&body=Title%3A%20%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%20Factuality%20Evaluation%20for%0A%20%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Jihao%20Gu%20and%20Yingyao%20Wang%20and%20Pi%20Bu%20and%20Chen%20Wang%20and%20Ziming%20Wang%20and%20Tengtao%20Song%20and%20Donglai%20Wei%20and%20Jiale%20Yuan%20and%20Yingxiu%20Zhao%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Jiaheng%20Liu%20and%20Meng%20Cao%20and%20Jun%20Song%20and%20Yingshui%20Tan%20and%20Xiang%20Li%20and%20Wenbo%20Su%20and%20Zhicheng%20Zheng%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20The%20evaluation%20of%20factual%20accuracy%20in%20large%20vision%20language%20models%20%28LVLMs%29%0Ahas%20lagged%20behind%20their%20rapid%20development%2C%20making%20it%20challenging%20to%20fully%0Areflect%20these%20models%27%20knowledge%20capacity%20and%20reliability.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20first%20factuality-based%20visual%20question-answering%20benchmark%20in%0AChinese%2C%20named%20ChineseSimpleVQA%2C%20aimed%20at%20assessing%20the%20visual%20factuality%20of%0ALVLMs%20across%208%20major%20topics%20and%2056%20subtopics.%20The%20key%20features%20of%20this%0Abenchmark%20include%20a%20focus%20on%20the%20Chinese%20language%2C%20diverse%20knowledge%20types%2C%20a%0Amulti-hop%20question%20construction%2C%20high-quality%20data%2C%20static%20consistency%2C%20and%0Aeasy-to-evaluate%20through%20short%20answers.%20Moreover%2C%20we%20contribute%20a%20rigorous%20data%0Aconstruction%20pipeline%20and%20decouple%20the%20visual%20factuality%20into%20two%20parts%3A%20seeing%0Athe%20world%20%28i.e.%2C%20object%20recognition%29%20and%20discovering%20knowledge.%20This%20decoupling%0Aallows%20us%20to%20analyze%20the%20capability%20boundaries%20and%20execution%20mechanisms%20of%0ALVLMs.%20Subsequently%2C%20we%20evaluate%2034%20advanced%20open-source%20and%20closed-source%0Amodels%2C%20revealing%20critical%20performance%20gaps%20within%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522See%2520the%2520World%252C%2520Discover%2520Knowledge%2522%253A%2520A%2520Chinese%2520Factuality%2520Evaluation%2520for%250A%2520%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DJihao%2520Gu%2520and%2520Yingyao%2520Wang%2520and%2520Pi%2520Bu%2520and%2520Chen%2520Wang%2520and%2520Ziming%2520Wang%2520and%2520Tengtao%2520Song%2520and%2520Donglai%2520Wei%2520and%2520Jiale%2520Yuan%2520and%2520Yingxiu%2520Zhao%2520and%2520Yancheng%2520He%2520and%2520Shilong%2520Li%2520and%2520Jiaheng%2520Liu%2520and%2520Meng%2520Cao%2520and%2520Jun%2520Song%2520and%2520Yingshui%2520Tan%2520and%2520Xiang%2520Li%2520and%2520Wenbo%2520Su%2520and%2520Zhicheng%2520Zheng%2520and%2520Xiaoyong%2520Zhu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520factual%2520accuracy%2520in%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%250Ahas%2520lagged%2520behind%2520their%2520rapid%2520development%252C%2520making%2520it%2520challenging%2520to%2520fully%250Areflect%2520these%2520models%2527%2520knowledge%2520capacity%2520and%2520reliability.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520first%2520factuality-based%2520visual%2520question-answering%2520benchmark%2520in%250AChinese%252C%2520named%2520ChineseSimpleVQA%252C%2520aimed%2520at%2520assessing%2520the%2520visual%2520factuality%2520of%250ALVLMs%2520across%25208%2520major%2520topics%2520and%252056%2520subtopics.%2520The%2520key%2520features%2520of%2520this%250Abenchmark%2520include%2520a%2520focus%2520on%2520the%2520Chinese%2520language%252C%2520diverse%2520knowledge%2520types%252C%2520a%250Amulti-hop%2520question%2520construction%252C%2520high-quality%2520data%252C%2520static%2520consistency%252C%2520and%250Aeasy-to-evaluate%2520through%2520short%2520answers.%2520Moreover%252C%2520we%2520contribute%2520a%2520rigorous%2520data%250Aconstruction%2520pipeline%2520and%2520decouple%2520the%2520visual%2520factuality%2520into%2520two%2520parts%253A%2520seeing%250Athe%2520world%2520%2528i.e.%252C%2520object%2520recognition%2529%2520and%2520discovering%2520knowledge.%2520This%2520decoupling%250Aallows%2520us%2520to%2520analyze%2520the%2520capability%2520boundaries%2520and%2520execution%2520mechanisms%2520of%250ALVLMs.%2520Subsequently%252C%2520we%2520evaluate%252034%2520advanced%2520open-source%2520and%2520closed-source%250Amodels%252C%2520revealing%2520critical%2520performance%2520gaps%2520within%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%20Factuality%20Evaluation%20for%0A%20%20Large%20Vision%20Language%20Models&entry.906535625=Jihao%20Gu%20and%20Yingyao%20Wang%20and%20Pi%20Bu%20and%20Chen%20Wang%20and%20Ziming%20Wang%20and%20Tengtao%20Song%20and%20Donglai%20Wei%20and%20Jiale%20Yuan%20and%20Yingxiu%20Zhao%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Jiaheng%20Liu%20and%20Meng%20Cao%20and%20Jun%20Song%20and%20Yingshui%20Tan%20and%20Xiang%20Li%20and%20Wenbo%20Su%20and%20Zhicheng%20Zheng%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng&entry.1292438233=%20%20The%20evaluation%20of%20factual%20accuracy%20in%20large%20vision%20language%20models%20%28LVLMs%29%0Ahas%20lagged%20behind%20their%20rapid%20development%2C%20making%20it%20challenging%20to%20fully%0Areflect%20these%20models%27%20knowledge%20capacity%20and%20reliability.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20first%20factuality-based%20visual%20question-answering%20benchmark%20in%0AChinese%2C%20named%20ChineseSimpleVQA%2C%20aimed%20at%20assessing%20the%20visual%20factuality%20of%0ALVLMs%20across%208%20major%20topics%20and%2056%20subtopics.%20The%20key%20features%20of%20this%0Abenchmark%20include%20a%20focus%20on%20the%20Chinese%20language%2C%20diverse%20knowledge%20types%2C%20a%0Amulti-hop%20question%20construction%2C%20high-quality%20data%2C%20static%20consistency%2C%20and%0Aeasy-to-evaluate%20through%20short%20answers.%20Moreover%2C%20we%20contribute%20a%20rigorous%20data%0Aconstruction%20pipeline%20and%20decouple%20the%20visual%20factuality%20into%20two%20parts%3A%20seeing%0Athe%20world%20%28i.e.%2C%20object%20recognition%29%20and%20discovering%20knowledge.%20This%20decoupling%0Aallows%20us%20to%20analyze%20the%20capability%20boundaries%20and%20execution%20mechanisms%20of%0ALVLMs.%20Subsequently%2C%20we%20evaluate%2034%20advanced%20open-source%20and%20closed-source%0Amodels%2C%20revealing%20critical%20performance%20gaps%20within%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11718v1&entry.124074799=Read"},
{"title": "Memory-based Ensemble Learning in CMR Semantic Segmentation", "author": "Yiwei Liu and Ziyi Wu and Liang Zhong and Lingyi Wen and Yuankai Wu", "abstract": "  Existing models typically segment either the entire 3D frame or 2D slices\nindependently to derive clinical functional metrics from ventricular\nsegmentation in cardiac cine sequences. While performing well overall, they\nstruggle at the end slices. To address this, we leverage spatial continuity to\nextract global uncertainty from segmentation variance and use it as memory in\nour ensemble learning method, Streaming, for classifier weighting, balancing\noverall and end-slice performance. Additionally, we introduce the End\nCoefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M&Ms\ndatasets show that our framework achieves near-state-of-the-art Dice Similarity\nCoefficient (DSC) and outperforms all models on end-slice performance,\nimproving patient-specific segmentation accuracy.\n", "link": "http://arxiv.org/abs/2502.09269v2", "date": "2025-02-17", "relevancy": 2.7647, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5554}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-based%20Ensemble%20Learning%20in%20CMR%20Semantic%20Segmentation&body=Title%3A%20Memory-based%20Ensemble%20Learning%20in%20CMR%20Semantic%20Segmentation%0AAuthor%3A%20Yiwei%20Liu%20and%20Ziyi%20Wu%20and%20Liang%20Zhong%20and%20Lingyi%20Wen%20and%20Yuankai%20Wu%0AAbstract%3A%20%20%20Existing%20models%20typically%20segment%20either%20the%20entire%203D%20frame%20or%202D%20slices%0Aindependently%20to%20derive%20clinical%20functional%20metrics%20from%20ventricular%0Asegmentation%20in%20cardiac%20cine%20sequences.%20While%20performing%20well%20overall%2C%20they%0Astruggle%20at%20the%20end%20slices.%20To%20address%20this%2C%20we%20leverage%20spatial%20continuity%20to%0Aextract%20global%20uncertainty%20from%20segmentation%20variance%20and%20use%20it%20as%20memory%20in%0Aour%20ensemble%20learning%20method%2C%20Streaming%2C%20for%20classifier%20weighting%2C%20balancing%0Aoverall%20and%20end-slice%20performance.%20Additionally%2C%20we%20introduce%20the%20End%0ACoefficient%20%28EC%29%20to%20quantify%20end-slice%20accuracy.%20Experiments%20on%20ACDC%20and%20M%26Ms%0Adatasets%20show%20that%20our%20framework%20achieves%20near-state-of-the-art%20Dice%20Similarity%0ACoefficient%20%28DSC%29%20and%20outperforms%20all%20models%20on%20end-slice%20performance%2C%0Aimproving%20patient-specific%20segmentation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-based%2520Ensemble%2520Learning%2520in%2520CMR%2520Semantic%2520Segmentation%26entry.906535625%3DYiwei%2520Liu%2520and%2520Ziyi%2520Wu%2520and%2520Liang%2520Zhong%2520and%2520Lingyi%2520Wen%2520and%2520Yuankai%2520Wu%26entry.1292438233%3D%2520%2520Existing%2520models%2520typically%2520segment%2520either%2520the%2520entire%25203D%2520frame%2520or%25202D%2520slices%250Aindependently%2520to%2520derive%2520clinical%2520functional%2520metrics%2520from%2520ventricular%250Asegmentation%2520in%2520cardiac%2520cine%2520sequences.%2520While%2520performing%2520well%2520overall%252C%2520they%250Astruggle%2520at%2520the%2520end%2520slices.%2520To%2520address%2520this%252C%2520we%2520leverage%2520spatial%2520continuity%2520to%250Aextract%2520global%2520uncertainty%2520from%2520segmentation%2520variance%2520and%2520use%2520it%2520as%2520memory%2520in%250Aour%2520ensemble%2520learning%2520method%252C%2520Streaming%252C%2520for%2520classifier%2520weighting%252C%2520balancing%250Aoverall%2520and%2520end-slice%2520performance.%2520Additionally%252C%2520we%2520introduce%2520the%2520End%250ACoefficient%2520%2528EC%2529%2520to%2520quantify%2520end-slice%2520accuracy.%2520Experiments%2520on%2520ACDC%2520and%2520M%2526Ms%250Adatasets%2520show%2520that%2520our%2520framework%2520achieves%2520near-state-of-the-art%2520Dice%2520Similarity%250ACoefficient%2520%2528DSC%2529%2520and%2520outperforms%2520all%2520models%2520on%2520end-slice%2520performance%252C%250Aimproving%2520patient-specific%2520segmentation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-based%20Ensemble%20Learning%20in%20CMR%20Semantic%20Segmentation&entry.906535625=Yiwei%20Liu%20and%20Ziyi%20Wu%20and%20Liang%20Zhong%20and%20Lingyi%20Wen%20and%20Yuankai%20Wu&entry.1292438233=%20%20Existing%20models%20typically%20segment%20either%20the%20entire%203D%20frame%20or%202D%20slices%0Aindependently%20to%20derive%20clinical%20functional%20metrics%20from%20ventricular%0Asegmentation%20in%20cardiac%20cine%20sequences.%20While%20performing%20well%20overall%2C%20they%0Astruggle%20at%20the%20end%20slices.%20To%20address%20this%2C%20we%20leverage%20spatial%20continuity%20to%0Aextract%20global%20uncertainty%20from%20segmentation%20variance%20and%20use%20it%20as%20memory%20in%0Aour%20ensemble%20learning%20method%2C%20Streaming%2C%20for%20classifier%20weighting%2C%20balancing%0Aoverall%20and%20end-slice%20performance.%20Additionally%2C%20we%20introduce%20the%20End%0ACoefficient%20%28EC%29%20to%20quantify%20end-slice%20accuracy.%20Experiments%20on%20ACDC%20and%20M%26Ms%0Adatasets%20show%20that%20our%20framework%20achieves%20near-state-of-the-art%20Dice%20Similarity%0ACoefficient%20%28DSC%29%20and%20outperforms%20all%20models%20on%20end-slice%20performance%2C%0Aimproving%20patient-specific%20segmentation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09269v2&entry.124074799=Read"},
{"title": "Range and Bird's Eye View Fused Cross-Modal Visual Place Recognition", "author": "Jianyi Peng and Fan Lu and Bin Li and Yuan Huang and Sanqing Qu and Guang Chen", "abstract": "  Image-to-point cloud cross-modal Visual Place Recognition (VPR) is a\nchallenging task where the query is an RGB image, and the database samples are\nLiDAR point clouds. Compared to single-modal VPR, this approach benefits from\nthe widespread availability of RGB cameras and the robustness of point clouds\nin providing accurate spatial geometry and distance information. However,\ncurrent methods rely on intermediate modalities that capture either the\nvertical or horizontal field of view, limiting their ability to fully exploit\nthe complementary information from both sensors. In this work, we propose an\ninnovative initial retrieval + re-rank method that effectively combines\ninformation from range (or RGB) images and Bird's Eye View (BEV) images. Our\napproach relies solely on a computationally efficient global descriptor\nsimilarity search process to achieve re-ranking. Additionally, we introduce a\nnovel similarity label supervision technique to maximize the utility of limited\ntraining data. Specifically, we employ points average distance to approximate\nappearance similarity and incorporate an adaptive margin, based on similarity\ndifferences, into the vanilla triplet loss. Experimental results on the KITTI\ndataset demonstrate that our method significantly outperforms state-of-the-art\napproaches.\n", "link": "http://arxiv.org/abs/2502.11742v1", "date": "2025-02-17", "relevancy": 2.728, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Range%20and%20Bird%27s%20Eye%20View%20Fused%20Cross-Modal%20Visual%20Place%20Recognition&body=Title%3A%20Range%20and%20Bird%27s%20Eye%20View%20Fused%20Cross-Modal%20Visual%20Place%20Recognition%0AAuthor%3A%20Jianyi%20Peng%20and%20Fan%20Lu%20and%20Bin%20Li%20and%20Yuan%20Huang%20and%20Sanqing%20Qu%20and%20Guang%20Chen%0AAbstract%3A%20%20%20Image-to-point%20cloud%20cross-modal%20Visual%20Place%20Recognition%20%28VPR%29%20is%20a%0Achallenging%20task%20where%20the%20query%20is%20an%20RGB%20image%2C%20and%20the%20database%20samples%20are%0ALiDAR%20point%20clouds.%20Compared%20to%20single-modal%20VPR%2C%20this%20approach%20benefits%20from%0Athe%20widespread%20availability%20of%20RGB%20cameras%20and%20the%20robustness%20of%20point%20clouds%0Ain%20providing%20accurate%20spatial%20geometry%20and%20distance%20information.%20However%2C%0Acurrent%20methods%20rely%20on%20intermediate%20modalities%20that%20capture%20either%20the%0Avertical%20or%20horizontal%20field%20of%20view%2C%20limiting%20their%20ability%20to%20fully%20exploit%0Athe%20complementary%20information%20from%20both%20sensors.%20In%20this%20work%2C%20we%20propose%20an%0Ainnovative%20initial%20retrieval%20%2B%20re-rank%20method%20that%20effectively%20combines%0Ainformation%20from%20range%20%28or%20RGB%29%20images%20and%20Bird%27s%20Eye%20View%20%28BEV%29%20images.%20Our%0Aapproach%20relies%20solely%20on%20a%20computationally%20efficient%20global%20descriptor%0Asimilarity%20search%20process%20to%20achieve%20re-ranking.%20Additionally%2C%20we%20introduce%20a%0Anovel%20similarity%20label%20supervision%20technique%20to%20maximize%20the%20utility%20of%20limited%0Atraining%20data.%20Specifically%2C%20we%20employ%20points%20average%20distance%20to%20approximate%0Aappearance%20similarity%20and%20incorporate%20an%20adaptive%20margin%2C%20based%20on%20similarity%0Adifferences%2C%20into%20the%20vanilla%20triplet%20loss.%20Experimental%20results%20on%20the%20KITTI%0Adataset%20demonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRange%2520and%2520Bird%2527s%2520Eye%2520View%2520Fused%2520Cross-Modal%2520Visual%2520Place%2520Recognition%26entry.906535625%3DJianyi%2520Peng%2520and%2520Fan%2520Lu%2520and%2520Bin%2520Li%2520and%2520Yuan%2520Huang%2520and%2520Sanqing%2520Qu%2520and%2520Guang%2520Chen%26entry.1292438233%3D%2520%2520Image-to-point%2520cloud%2520cross-modal%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520is%2520a%250Achallenging%2520task%2520where%2520the%2520query%2520is%2520an%2520RGB%2520image%252C%2520and%2520the%2520database%2520samples%2520are%250ALiDAR%2520point%2520clouds.%2520Compared%2520to%2520single-modal%2520VPR%252C%2520this%2520approach%2520benefits%2520from%250Athe%2520widespread%2520availability%2520of%2520RGB%2520cameras%2520and%2520the%2520robustness%2520of%2520point%2520clouds%250Ain%2520providing%2520accurate%2520spatial%2520geometry%2520and%2520distance%2520information.%2520However%252C%250Acurrent%2520methods%2520rely%2520on%2520intermediate%2520modalities%2520that%2520capture%2520either%2520the%250Avertical%2520or%2520horizontal%2520field%2520of%2520view%252C%2520limiting%2520their%2520ability%2520to%2520fully%2520exploit%250Athe%2520complementary%2520information%2520from%2520both%2520sensors.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%250Ainnovative%2520initial%2520retrieval%2520%252B%2520re-rank%2520method%2520that%2520effectively%2520combines%250Ainformation%2520from%2520range%2520%2528or%2520RGB%2529%2520images%2520and%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520images.%2520Our%250Aapproach%2520relies%2520solely%2520on%2520a%2520computationally%2520efficient%2520global%2520descriptor%250Asimilarity%2520search%2520process%2520to%2520achieve%2520re-ranking.%2520Additionally%252C%2520we%2520introduce%2520a%250Anovel%2520similarity%2520label%2520supervision%2520technique%2520to%2520maximize%2520the%2520utility%2520of%2520limited%250Atraining%2520data.%2520Specifically%252C%2520we%2520employ%2520points%2520average%2520distance%2520to%2520approximate%250Aappearance%2520similarity%2520and%2520incorporate%2520an%2520adaptive%2520margin%252C%2520based%2520on%2520similarity%250Adifferences%252C%2520into%2520the%2520vanilla%2520triplet%2520loss.%2520Experimental%2520results%2520on%2520the%2520KITTI%250Adataset%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520state-of-the-art%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Range%20and%20Bird%27s%20Eye%20View%20Fused%20Cross-Modal%20Visual%20Place%20Recognition&entry.906535625=Jianyi%20Peng%20and%20Fan%20Lu%20and%20Bin%20Li%20and%20Yuan%20Huang%20and%20Sanqing%20Qu%20and%20Guang%20Chen&entry.1292438233=%20%20Image-to-point%20cloud%20cross-modal%20Visual%20Place%20Recognition%20%28VPR%29%20is%20a%0Achallenging%20task%20where%20the%20query%20is%20an%20RGB%20image%2C%20and%20the%20database%20samples%20are%0ALiDAR%20point%20clouds.%20Compared%20to%20single-modal%20VPR%2C%20this%20approach%20benefits%20from%0Athe%20widespread%20availability%20of%20RGB%20cameras%20and%20the%20robustness%20of%20point%20clouds%0Ain%20providing%20accurate%20spatial%20geometry%20and%20distance%20information.%20However%2C%0Acurrent%20methods%20rely%20on%20intermediate%20modalities%20that%20capture%20either%20the%0Avertical%20or%20horizontal%20field%20of%20view%2C%20limiting%20their%20ability%20to%20fully%20exploit%0Athe%20complementary%20information%20from%20both%20sensors.%20In%20this%20work%2C%20we%20propose%20an%0Ainnovative%20initial%20retrieval%20%2B%20re-rank%20method%20that%20effectively%20combines%0Ainformation%20from%20range%20%28or%20RGB%29%20images%20and%20Bird%27s%20Eye%20View%20%28BEV%29%20images.%20Our%0Aapproach%20relies%20solely%20on%20a%20computationally%20efficient%20global%20descriptor%0Asimilarity%20search%20process%20to%20achieve%20re-ranking.%20Additionally%2C%20we%20introduce%20a%0Anovel%20similarity%20label%20supervision%20technique%20to%20maximize%20the%20utility%20of%20limited%0Atraining%20data.%20Specifically%2C%20we%20employ%20points%20average%20distance%20to%20approximate%0Aappearance%20similarity%20and%20incorporate%20an%20adaptive%20margin%2C%20based%20on%20similarity%0Adifferences%2C%20into%20the%20vanilla%20triplet%20loss.%20Experimental%20results%20on%20the%20KITTI%0Adataset%20demonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11742v1&entry.124074799=Read"},
{"title": "Masked Latent Prediction and Classification for Self-Supervised Audio\n  Representation Learning", "author": "Aurian Quelennec and Pierre Chouteau and Geoffroy Peeters and Slim Essid", "abstract": "  Recently, self-supervised learning methods based on masked latent prediction\nhave proven to encode input data into powerful representations. However, during\ntraining, the learned latent space can be further transformed to extract\nhigher-level information that could be more suited for downstream\nclassification tasks. Therefore, we propose a new method: MAsked latenT\nPrediction And Classification (MATPAC), which is trained with two pretext tasks\nsolved jointly. As in previous work, the first pretext task is a masked latent\nprediction task, ensuring a robust input representation in the latent space.\nThe second one is unsupervised classification, which utilises the latent\nrepresentations of the first pretext task to match probability distributions\nbetween a teacher and a student. We validate the MATPAC method by comparing it\nto other state-of-the-art proposals and conducting ablations studies. MATPAC\nreaches state-of-the-art self-supervised learning results on reference audio\nclassification datasets such as OpenMIC, GTZAN, ESC-50 and US8K and outperforms\ncomparable supervised methods results for musical auto-tagging on\nMagna-tag-a-tune.\n", "link": "http://arxiv.org/abs/2502.12031v1", "date": "2025-02-17", "relevancy": 2.691, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5665}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5259}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Latent%20Prediction%20and%20Classification%20for%20Self-Supervised%20Audio%0A%20%20Representation%20Learning&body=Title%3A%20Masked%20Latent%20Prediction%20and%20Classification%20for%20Self-Supervised%20Audio%0A%20%20Representation%20Learning%0AAuthor%3A%20Aurian%20Quelennec%20and%20Pierre%20Chouteau%20and%20Geoffroy%20Peeters%20and%20Slim%20Essid%0AAbstract%3A%20%20%20Recently%2C%20self-supervised%20learning%20methods%20based%20on%20masked%20latent%20prediction%0Ahave%20proven%20to%20encode%20input%20data%20into%20powerful%20representations.%20However%2C%20during%0Atraining%2C%20the%20learned%20latent%20space%20can%20be%20further%20transformed%20to%20extract%0Ahigher-level%20information%20that%20could%20be%20more%20suited%20for%20downstream%0Aclassification%20tasks.%20Therefore%2C%20we%20propose%20a%20new%20method%3A%20MAsked%20latenT%0APrediction%20And%20Classification%20%28MATPAC%29%2C%20which%20is%20trained%20with%20two%20pretext%20tasks%0Asolved%20jointly.%20As%20in%20previous%20work%2C%20the%20first%20pretext%20task%20is%20a%20masked%20latent%0Aprediction%20task%2C%20ensuring%20a%20robust%20input%20representation%20in%20the%20latent%20space.%0AThe%20second%20one%20is%20unsupervised%20classification%2C%20which%20utilises%20the%20latent%0Arepresentations%20of%20the%20first%20pretext%20task%20to%20match%20probability%20distributions%0Abetween%20a%20teacher%20and%20a%20student.%20We%20validate%20the%20MATPAC%20method%20by%20comparing%20it%0Ato%20other%20state-of-the-art%20proposals%20and%20conducting%20ablations%20studies.%20MATPAC%0Areaches%20state-of-the-art%20self-supervised%20learning%20results%20on%20reference%20audio%0Aclassification%20datasets%20such%20as%20OpenMIC%2C%20GTZAN%2C%20ESC-50%20and%20US8K%20and%20outperforms%0Acomparable%20supervised%20methods%20results%20for%20musical%20auto-tagging%20on%0AMagna-tag-a-tune.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Latent%2520Prediction%2520and%2520Classification%2520for%2520Self-Supervised%2520Audio%250A%2520%2520Representation%2520Learning%26entry.906535625%3DAurian%2520Quelennec%2520and%2520Pierre%2520Chouteau%2520and%2520Geoffroy%2520Peeters%2520and%2520Slim%2520Essid%26entry.1292438233%3D%2520%2520Recently%252C%2520self-supervised%2520learning%2520methods%2520based%2520on%2520masked%2520latent%2520prediction%250Ahave%2520proven%2520to%2520encode%2520input%2520data%2520into%2520powerful%2520representations.%2520However%252C%2520during%250Atraining%252C%2520the%2520learned%2520latent%2520space%2520can%2520be%2520further%2520transformed%2520to%2520extract%250Ahigher-level%2520information%2520that%2520could%2520be%2520more%2520suited%2520for%2520downstream%250Aclassification%2520tasks.%2520Therefore%252C%2520we%2520propose%2520a%2520new%2520method%253A%2520MAsked%2520latenT%250APrediction%2520And%2520Classification%2520%2528MATPAC%2529%252C%2520which%2520is%2520trained%2520with%2520two%2520pretext%2520tasks%250Asolved%2520jointly.%2520As%2520in%2520previous%2520work%252C%2520the%2520first%2520pretext%2520task%2520is%2520a%2520masked%2520latent%250Aprediction%2520task%252C%2520ensuring%2520a%2520robust%2520input%2520representation%2520in%2520the%2520latent%2520space.%250AThe%2520second%2520one%2520is%2520unsupervised%2520classification%252C%2520which%2520utilises%2520the%2520latent%250Arepresentations%2520of%2520the%2520first%2520pretext%2520task%2520to%2520match%2520probability%2520distributions%250Abetween%2520a%2520teacher%2520and%2520a%2520student.%2520We%2520validate%2520the%2520MATPAC%2520method%2520by%2520comparing%2520it%250Ato%2520other%2520state-of-the-art%2520proposals%2520and%2520conducting%2520ablations%2520studies.%2520MATPAC%250Areaches%2520state-of-the-art%2520self-supervised%2520learning%2520results%2520on%2520reference%2520audio%250Aclassification%2520datasets%2520such%2520as%2520OpenMIC%252C%2520GTZAN%252C%2520ESC-50%2520and%2520US8K%2520and%2520outperforms%250Acomparable%2520supervised%2520methods%2520results%2520for%2520musical%2520auto-tagging%2520on%250AMagna-tag-a-tune.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Latent%20Prediction%20and%20Classification%20for%20Self-Supervised%20Audio%0A%20%20Representation%20Learning&entry.906535625=Aurian%20Quelennec%20and%20Pierre%20Chouteau%20and%20Geoffroy%20Peeters%20and%20Slim%20Essid&entry.1292438233=%20%20Recently%2C%20self-supervised%20learning%20methods%20based%20on%20masked%20latent%20prediction%0Ahave%20proven%20to%20encode%20input%20data%20into%20powerful%20representations.%20However%2C%20during%0Atraining%2C%20the%20learned%20latent%20space%20can%20be%20further%20transformed%20to%20extract%0Ahigher-level%20information%20that%20could%20be%20more%20suited%20for%20downstream%0Aclassification%20tasks.%20Therefore%2C%20we%20propose%20a%20new%20method%3A%20MAsked%20latenT%0APrediction%20And%20Classification%20%28MATPAC%29%2C%20which%20is%20trained%20with%20two%20pretext%20tasks%0Asolved%20jointly.%20As%20in%20previous%20work%2C%20the%20first%20pretext%20task%20is%20a%20masked%20latent%0Aprediction%20task%2C%20ensuring%20a%20robust%20input%20representation%20in%20the%20latent%20space.%0AThe%20second%20one%20is%20unsupervised%20classification%2C%20which%20utilises%20the%20latent%0Arepresentations%20of%20the%20first%20pretext%20task%20to%20match%20probability%20distributions%0Abetween%20a%20teacher%20and%20a%20student.%20We%20validate%20the%20MATPAC%20method%20by%20comparing%20it%0Ato%20other%20state-of-the-art%20proposals%20and%20conducting%20ablations%20studies.%20MATPAC%0Areaches%20state-of-the-art%20self-supervised%20learning%20results%20on%20reference%20audio%0Aclassification%20datasets%20such%20as%20OpenMIC%2C%20GTZAN%2C%20ESC-50%20and%20US8K%20and%20outperforms%0Acomparable%20supervised%20methods%20results%20for%20musical%20auto-tagging%20on%0AMagna-tag-a-tune.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12031v1&entry.124074799=Read"},
{"title": "HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation", "author": "Tianwei Lin and Wenqiao Zhang and Sijing Li and Yuqian Yuan and Binhe Yu and Haoyuan Li and Wanggui He and Hao Jiang and Mengze Li and Xiaohui Song and Siliang Tang and Jun Xiao and Hui Lin and Yueting Zhuang and Beng Chin Ooi", "abstract": "  We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.\n", "link": "http://arxiv.org/abs/2502.09838v2", "date": "2025-02-17", "relevancy": 2.6892, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HealthGPT%3A%20A%20Medical%20Large%20Vision-Language%20Model%20for%20Unifying%0A%20%20Comprehension%20and%20Generation%20via%20Heterogeneous%20Knowledge%20Adaptation&body=Title%3A%20HealthGPT%3A%20A%20Medical%20Large%20Vision-Language%20Model%20for%20Unifying%0A%20%20Comprehension%20and%20Generation%20via%20Heterogeneous%20Knowledge%20Adaptation%0AAuthor%3A%20Tianwei%20Lin%20and%20Wenqiao%20Zhang%20and%20Sijing%20Li%20and%20Yuqian%20Yuan%20and%20Binhe%20Yu%20and%20Haoyuan%20Li%20and%20Wanggui%20He%20and%20Hao%20Jiang%20and%20Mengze%20Li%20and%20Xiaohui%20Song%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Hui%20Lin%20and%20Yueting%20Zhuang%20and%20Beng%20Chin%20Ooi%0AAbstract%3A%20%20%20We%20present%20HealthGPT%2C%20a%20powerful%20Medical%20Large%20Vision-Language%20Model%0A%28Med-LVLM%29%20that%20integrates%20medical%20visual%20comprehension%20and%20generation%0Acapabilities%20within%20a%20unified%20autoregressive%20paradigm.%20Our%20bootstrapping%0Aphilosophy%20is%20to%20progressively%20adapt%20heterogeneous%20comprehension%20and%20generation%0Aknowledge%20to%20pre-trained%20large%20language%20models%20%28LLMs%29.%20This%20is%20achieved%20through%0Aa%20novel%20heterogeneous%20low-rank%20adaptation%20%28H-LoRA%29%20technique%2C%20which%20is%0Acomplemented%20by%20a%20tailored%20hierarchical%20visual%20perception%20approach%20and%20a%0Athree-stage%20learning%20strategy.%20To%20effectively%20learn%20the%20HealthGPT%2C%20we%20devise%20a%0Acomprehensive%20medical%20domain-specific%20comprehension%20and%20generation%20dataset%0Acalled%20VL-Health.%20Experimental%20results%20demonstrate%20exceptional%20performance%20and%0Ascalability%20of%20HealthGPT%20in%20medical%20visual%20unified%20tasks.%20Our%20project%20can%20be%0Aaccessed%20at%20https%3A//github.com/DCDmllm/HealthGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHealthGPT%253A%2520A%2520Medical%2520Large%2520Vision-Language%2520Model%2520for%2520Unifying%250A%2520%2520Comprehension%2520and%2520Generation%2520via%2520Heterogeneous%2520Knowledge%2520Adaptation%26entry.906535625%3DTianwei%2520Lin%2520and%2520Wenqiao%2520Zhang%2520and%2520Sijing%2520Li%2520and%2520Yuqian%2520Yuan%2520and%2520Binhe%2520Yu%2520and%2520Haoyuan%2520Li%2520and%2520Wanggui%2520He%2520and%2520Hao%2520Jiang%2520and%2520Mengze%2520Li%2520and%2520Xiaohui%2520Song%2520and%2520Siliang%2520Tang%2520and%2520Jun%2520Xiao%2520and%2520Hui%2520Lin%2520and%2520Yueting%2520Zhuang%2520and%2520Beng%2520Chin%2520Ooi%26entry.1292438233%3D%2520%2520We%2520present%2520HealthGPT%252C%2520a%2520powerful%2520Medical%2520Large%2520Vision-Language%2520Model%250A%2528Med-LVLM%2529%2520that%2520integrates%2520medical%2520visual%2520comprehension%2520and%2520generation%250Acapabilities%2520within%2520a%2520unified%2520autoregressive%2520paradigm.%2520Our%2520bootstrapping%250Aphilosophy%2520is%2520to%2520progressively%2520adapt%2520heterogeneous%2520comprehension%2520and%2520generation%250Aknowledge%2520to%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529.%2520This%2520is%2520achieved%2520through%250Aa%2520novel%2520heterogeneous%2520low-rank%2520adaptation%2520%2528H-LoRA%2529%2520technique%252C%2520which%2520is%250Acomplemented%2520by%2520a%2520tailored%2520hierarchical%2520visual%2520perception%2520approach%2520and%2520a%250Athree-stage%2520learning%2520strategy.%2520To%2520effectively%2520learn%2520the%2520HealthGPT%252C%2520we%2520devise%2520a%250Acomprehensive%2520medical%2520domain-specific%2520comprehension%2520and%2520generation%2520dataset%250Acalled%2520VL-Health.%2520Experimental%2520results%2520demonstrate%2520exceptional%2520performance%2520and%250Ascalability%2520of%2520HealthGPT%2520in%2520medical%2520visual%2520unified%2520tasks.%2520Our%2520project%2520can%2520be%250Aaccessed%2520at%2520https%253A//github.com/DCDmllm/HealthGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HealthGPT%3A%20A%20Medical%20Large%20Vision-Language%20Model%20for%20Unifying%0A%20%20Comprehension%20and%20Generation%20via%20Heterogeneous%20Knowledge%20Adaptation&entry.906535625=Tianwei%20Lin%20and%20Wenqiao%20Zhang%20and%20Sijing%20Li%20and%20Yuqian%20Yuan%20and%20Binhe%20Yu%20and%20Haoyuan%20Li%20and%20Wanggui%20He%20and%20Hao%20Jiang%20and%20Mengze%20Li%20and%20Xiaohui%20Song%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Hui%20Lin%20and%20Yueting%20Zhuang%20and%20Beng%20Chin%20Ooi&entry.1292438233=%20%20We%20present%20HealthGPT%2C%20a%20powerful%20Medical%20Large%20Vision-Language%20Model%0A%28Med-LVLM%29%20that%20integrates%20medical%20visual%20comprehension%20and%20generation%0Acapabilities%20within%20a%20unified%20autoregressive%20paradigm.%20Our%20bootstrapping%0Aphilosophy%20is%20to%20progressively%20adapt%20heterogeneous%20comprehension%20and%20generation%0Aknowledge%20to%20pre-trained%20large%20language%20models%20%28LLMs%29.%20This%20is%20achieved%20through%0Aa%20novel%20heterogeneous%20low-rank%20adaptation%20%28H-LoRA%29%20technique%2C%20which%20is%0Acomplemented%20by%20a%20tailored%20hierarchical%20visual%20perception%20approach%20and%20a%0Athree-stage%20learning%20strategy.%20To%20effectively%20learn%20the%20HealthGPT%2C%20we%20devise%20a%0Acomprehensive%20medical%20domain-specific%20comprehension%20and%20generation%20dataset%0Acalled%20VL-Health.%20Experimental%20results%20demonstrate%20exceptional%20performance%20and%0Ascalability%20of%20HealthGPT%20in%20medical%20visual%20unified%20tasks.%20Our%20project%20can%20be%0Aaccessed%20at%20https%3A//github.com/DCDmllm/HealthGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09838v2&entry.124074799=Read"},
{"title": "Revealing Bias Formation in Deep Neural Networks Through the Geometric\n  Mechanisms of Human Visual Decoupling", "author": "Yanbiao Ma and Bowei Liu and Wei Dai and Jiayi Chen and Shuo Li", "abstract": "  Deep neural networks (DNNs) often exhibit biases toward certain categories\nduring object recognition, even under balanced training data conditions. The\nintrinsic mechanisms underlying these biases remain unclear. Inspired by the\nhuman visual system, which decouples object manifolds through hierarchical\nprocessing to achieve object recognition, we propose a geometric analysis\nframework linking the geometric complexity of class-specific perceptual\nmanifolds in DNNs to model bias. Our findings reveal that differences in\ngeometric complexity can lead to varying recognition capabilities across\ncategories, introducing biases. To support this analysis, we present the\nPerceptual-Manifold-Geometry library, designed for calculating the geometric\nproperties of perceptual manifolds.\n", "link": "http://arxiv.org/abs/2502.11809v1", "date": "2025-02-17", "relevancy": 2.6887, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5381}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5378}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20Bias%20Formation%20in%20Deep%20Neural%20Networks%20Through%20the%20Geometric%0A%20%20Mechanisms%20of%20Human%20Visual%20Decoupling&body=Title%3A%20Revealing%20Bias%20Formation%20in%20Deep%20Neural%20Networks%20Through%20the%20Geometric%0A%20%20Mechanisms%20of%20Human%20Visual%20Decoupling%0AAuthor%3A%20Yanbiao%20Ma%20and%20Bowei%20Liu%20and%20Wei%20Dai%20and%20Jiayi%20Chen%20and%20Shuo%20Li%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20often%20exhibit%20biases%20toward%20certain%20categories%0Aduring%20object%20recognition%2C%20even%20under%20balanced%20training%20data%20conditions.%20The%0Aintrinsic%20mechanisms%20underlying%20these%20biases%20remain%20unclear.%20Inspired%20by%20the%0Ahuman%20visual%20system%2C%20which%20decouples%20object%20manifolds%20through%20hierarchical%0Aprocessing%20to%20achieve%20object%20recognition%2C%20we%20propose%20a%20geometric%20analysis%0Aframework%20linking%20the%20geometric%20complexity%20of%20class-specific%20perceptual%0Amanifolds%20in%20DNNs%20to%20model%20bias.%20Our%20findings%20reveal%20that%20differences%20in%0Ageometric%20complexity%20can%20lead%20to%20varying%20recognition%20capabilities%20across%0Acategories%2C%20introducing%20biases.%20To%20support%20this%20analysis%2C%20we%20present%20the%0APerceptual-Manifold-Geometry%20library%2C%20designed%20for%20calculating%20the%20geometric%0Aproperties%20of%20perceptual%20manifolds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520Bias%2520Formation%2520in%2520Deep%2520Neural%2520Networks%2520Through%2520the%2520Geometric%250A%2520%2520Mechanisms%2520of%2520Human%2520Visual%2520Decoupling%26entry.906535625%3DYanbiao%2520Ma%2520and%2520Bowei%2520Liu%2520and%2520Wei%2520Dai%2520and%2520Jiayi%2520Chen%2520and%2520Shuo%2520Li%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520often%2520exhibit%2520biases%2520toward%2520certain%2520categories%250Aduring%2520object%2520recognition%252C%2520even%2520under%2520balanced%2520training%2520data%2520conditions.%2520The%250Aintrinsic%2520mechanisms%2520underlying%2520these%2520biases%2520remain%2520unclear.%2520Inspired%2520by%2520the%250Ahuman%2520visual%2520system%252C%2520which%2520decouples%2520object%2520manifolds%2520through%2520hierarchical%250Aprocessing%2520to%2520achieve%2520object%2520recognition%252C%2520we%2520propose%2520a%2520geometric%2520analysis%250Aframework%2520linking%2520the%2520geometric%2520complexity%2520of%2520class-specific%2520perceptual%250Amanifolds%2520in%2520DNNs%2520to%2520model%2520bias.%2520Our%2520findings%2520reveal%2520that%2520differences%2520in%250Ageometric%2520complexity%2520can%2520lead%2520to%2520varying%2520recognition%2520capabilities%2520across%250Acategories%252C%2520introducing%2520biases.%2520To%2520support%2520this%2520analysis%252C%2520we%2520present%2520the%250APerceptual-Manifold-Geometry%2520library%252C%2520designed%2520for%2520calculating%2520the%2520geometric%250Aproperties%2520of%2520perceptual%2520manifolds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20Bias%20Formation%20in%20Deep%20Neural%20Networks%20Through%20the%20Geometric%0A%20%20Mechanisms%20of%20Human%20Visual%20Decoupling&entry.906535625=Yanbiao%20Ma%20and%20Bowei%20Liu%20and%20Wei%20Dai%20and%20Jiayi%20Chen%20and%20Shuo%20Li&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20often%20exhibit%20biases%20toward%20certain%20categories%0Aduring%20object%20recognition%2C%20even%20under%20balanced%20training%20data%20conditions.%20The%0Aintrinsic%20mechanisms%20underlying%20these%20biases%20remain%20unclear.%20Inspired%20by%20the%0Ahuman%20visual%20system%2C%20which%20decouples%20object%20manifolds%20through%20hierarchical%0Aprocessing%20to%20achieve%20object%20recognition%2C%20we%20propose%20a%20geometric%20analysis%0Aframework%20linking%20the%20geometric%20complexity%20of%20class-specific%20perceptual%0Amanifolds%20in%20DNNs%20to%20model%20bias.%20Our%20findings%20reveal%20that%20differences%20in%0Ageometric%20complexity%20can%20lead%20to%20varying%20recognition%20capabilities%20across%0Acategories%2C%20introducing%20biases.%20To%20support%20this%20analysis%2C%20we%20present%20the%0APerceptual-Manifold-Geometry%20library%2C%20designed%20for%20calculating%20the%20geometric%0Aproperties%20of%20perceptual%20manifolds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11809v1&entry.124074799=Read"},
{"title": "GraphEval36K: Benchmarking Coding and Reasoning Capabilities of Large\n  Language Models on Graph Datasets", "author": "Qiming Wu and Zichen Chen and Will Corcoran and Misha Sra and Ambuj K. Singh", "abstract": "  Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to manipulate, program, and reason about\nstructured data, especially graphs. We introduce GraphEval36K, the first\ncomprehensive graph dataset, comprising 40 graph coding problems and 36,900\ntest cases to evaluate the ability of LLMs on graph problem-solving. Our\ndataset is categorized into eight primary and four sub-categories to ensure a\nthorough evaluation across different types of graphs. We benchmark ten LLMs,\nfinding that private models outperform open-source ones, though the gap is\nnarrowing. We also analyze the performance of LLMs across directed vs\nundirected graphs, different kinds of graph concepts, and network models.\nFurthermore, to improve the usability of our evaluation framework, we propose\nStructured Symbolic Decomposition (SSD), an instruction-based method designed\nto enhance LLM performance on complex graph tasks. Results show that SSD\nimproves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and\nClaude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.\n", "link": "http://arxiv.org/abs/2406.16176v2", "date": "2025-02-17", "relevancy": 2.6834, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphEval36K%3A%20Benchmarking%20Coding%20and%20Reasoning%20Capabilities%20of%20Large%0A%20%20Language%20Models%20on%20Graph%20Datasets&body=Title%3A%20GraphEval36K%3A%20Benchmarking%20Coding%20and%20Reasoning%20Capabilities%20of%20Large%0A%20%20Language%20Models%20on%20Graph%20Datasets%0AAuthor%3A%20Qiming%20Wu%20and%20Zichen%20Chen%20and%20Will%20Corcoran%20and%20Misha%20Sra%20and%20Ambuj%20K.%20Singh%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20%28NLP%29%2C%20demonstrating%20significant%20capabilities%20in%20processing%0Aand%20understanding%20text%20data.%20However%2C%20recent%20studies%20have%20identified%0Alimitations%20in%20LLMs%27%20ability%20to%20manipulate%2C%20program%2C%20and%20reason%20about%0Astructured%20data%2C%20especially%20graphs.%20We%20introduce%20GraphEval36K%2C%20the%20first%0Acomprehensive%20graph%20dataset%2C%20comprising%2040%20graph%20coding%20problems%20and%2036%2C900%0Atest%20cases%20to%20evaluate%20the%20ability%20of%20LLMs%20on%20graph%20problem-solving.%20Our%0Adataset%20is%20categorized%20into%20eight%20primary%20and%20four%20sub-categories%20to%20ensure%20a%0Athorough%20evaluation%20across%20different%20types%20of%20graphs.%20We%20benchmark%20ten%20LLMs%2C%0Afinding%20that%20private%20models%20outperform%20open-source%20ones%2C%20though%20the%20gap%20is%0Anarrowing.%20We%20also%20analyze%20the%20performance%20of%20LLMs%20across%20directed%20vs%0Aundirected%20graphs%2C%20different%20kinds%20of%20graph%20concepts%2C%20and%20network%20models.%0AFurthermore%2C%20to%20improve%20the%20usability%20of%20our%20evaluation%20framework%2C%20we%20propose%0AStructured%20Symbolic%20Decomposition%20%28SSD%29%2C%20an%20instruction-based%20method%20designed%0Ato%20enhance%20LLM%20performance%20on%20complex%20graph%20tasks.%20Results%20show%20that%20SSD%0Aimproves%20the%20average%20passing%20rate%20of%20GPT-4%2C%20GPT-4o%2C%20Gemini-Pro%20and%0AClaude-3-Sonnet%20by%208.38%25%2C%206.78%25%2C%2029.28%25%20and%2025.28%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphEval36K%253A%2520Benchmarking%2520Coding%2520and%2520Reasoning%2520Capabilities%2520of%2520Large%250A%2520%2520Language%2520Models%2520on%2520Graph%2520Datasets%26entry.906535625%3DQiming%2520Wu%2520and%2520Zichen%2520Chen%2520and%2520Will%2520Corcoran%2520and%2520Misha%2520Sra%2520and%2520Ambuj%2520K.%2520Singh%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%252C%2520demonstrating%2520significant%2520capabilities%2520in%2520processing%250Aand%2520understanding%2520text%2520data.%2520However%252C%2520recent%2520studies%2520have%2520identified%250Alimitations%2520in%2520LLMs%2527%2520ability%2520to%2520manipulate%252C%2520program%252C%2520and%2520reason%2520about%250Astructured%2520data%252C%2520especially%2520graphs.%2520We%2520introduce%2520GraphEval36K%252C%2520the%2520first%250Acomprehensive%2520graph%2520dataset%252C%2520comprising%252040%2520graph%2520coding%2520problems%2520and%252036%252C900%250Atest%2520cases%2520to%2520evaluate%2520the%2520ability%2520of%2520LLMs%2520on%2520graph%2520problem-solving.%2520Our%250Adataset%2520is%2520categorized%2520into%2520eight%2520primary%2520and%2520four%2520sub-categories%2520to%2520ensure%2520a%250Athorough%2520evaluation%2520across%2520different%2520types%2520of%2520graphs.%2520We%2520benchmark%2520ten%2520LLMs%252C%250Afinding%2520that%2520private%2520models%2520outperform%2520open-source%2520ones%252C%2520though%2520the%2520gap%2520is%250Anarrowing.%2520We%2520also%2520analyze%2520the%2520performance%2520of%2520LLMs%2520across%2520directed%2520vs%250Aundirected%2520graphs%252C%2520different%2520kinds%2520of%2520graph%2520concepts%252C%2520and%2520network%2520models.%250AFurthermore%252C%2520to%2520improve%2520the%2520usability%2520of%2520our%2520evaluation%2520framework%252C%2520we%2520propose%250AStructured%2520Symbolic%2520Decomposition%2520%2528SSD%2529%252C%2520an%2520instruction-based%2520method%2520designed%250Ato%2520enhance%2520LLM%2520performance%2520on%2520complex%2520graph%2520tasks.%2520Results%2520show%2520that%2520SSD%250Aimproves%2520the%2520average%2520passing%2520rate%2520of%2520GPT-4%252C%2520GPT-4o%252C%2520Gemini-Pro%2520and%250AClaude-3-Sonnet%2520by%25208.38%2525%252C%25206.78%2525%252C%252029.28%2525%2520and%252025.28%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphEval36K%3A%20Benchmarking%20Coding%20and%20Reasoning%20Capabilities%20of%20Large%0A%20%20Language%20Models%20on%20Graph%20Datasets&entry.906535625=Qiming%20Wu%20and%20Zichen%20Chen%20and%20Will%20Corcoran%20and%20Misha%20Sra%20and%20Ambuj%20K.%20Singh&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20%28NLP%29%2C%20demonstrating%20significant%20capabilities%20in%20processing%0Aand%20understanding%20text%20data.%20However%2C%20recent%20studies%20have%20identified%0Alimitations%20in%20LLMs%27%20ability%20to%20manipulate%2C%20program%2C%20and%20reason%20about%0Astructured%20data%2C%20especially%20graphs.%20We%20introduce%20GraphEval36K%2C%20the%20first%0Acomprehensive%20graph%20dataset%2C%20comprising%2040%20graph%20coding%20problems%20and%2036%2C900%0Atest%20cases%20to%20evaluate%20the%20ability%20of%20LLMs%20on%20graph%20problem-solving.%20Our%0Adataset%20is%20categorized%20into%20eight%20primary%20and%20four%20sub-categories%20to%20ensure%20a%0Athorough%20evaluation%20across%20different%20types%20of%20graphs.%20We%20benchmark%20ten%20LLMs%2C%0Afinding%20that%20private%20models%20outperform%20open-source%20ones%2C%20though%20the%20gap%20is%0Anarrowing.%20We%20also%20analyze%20the%20performance%20of%20LLMs%20across%20directed%20vs%0Aundirected%20graphs%2C%20different%20kinds%20of%20graph%20concepts%2C%20and%20network%20models.%0AFurthermore%2C%20to%20improve%20the%20usability%20of%20our%20evaluation%20framework%2C%20we%20propose%0AStructured%20Symbolic%20Decomposition%20%28SSD%29%2C%20an%20instruction-based%20method%20designed%0Ato%20enhance%20LLM%20performance%20on%20complex%20graph%20tasks.%20Results%20show%20that%20SSD%0Aimproves%20the%20average%20passing%20rate%20of%20GPT-4%2C%20GPT-4o%2C%20Gemini-Pro%20and%0AClaude-3-Sonnet%20by%208.38%25%2C%206.78%25%2C%2029.28%25%20and%2025.28%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16176v2&entry.124074799=Read"},
{"title": "Exploiting Task Relationships for Continual Learning Using\n  Transferability-Aware Task Embeddings", "author": "Yanru Wu and Xiangyu Chen and Jianning Wang and Enming Zhang and Hanbing Liu and Yang Li", "abstract": "  Continual learning (CL) has been an essential topic in the contemporary\napplication of deep neural networks, where catastrophic forgetting (CF) can\nimpede a model's ability to acquire knowledge progressively. Existing CL\nstrategies primarily address CF by regularizing model updates or separating\ntask-specific and shared components. However, these methods focus on task model\nelements while overlooking the potential of leveraging inter-task relationships\nfor learning enhancement. To address this, we propose a transferability-aware\ntask embedding named H-embedding and train a hypernet under its guidance to\nlearn task-conditioned model weights for CL tasks. Particularly, H-embedding is\nintroduced based on an information theoretical transferability measure and is\ndesigned to be online and easy to compute. The framework is also characterized\nby notable practicality, which only requires storing a low-dimensional task\nembedding for each task, and can be efficiently trained in an end-to-end way.\nExtensive evaluations and experimental analyses on datasets including Permuted\nMNIST, Cifar10/100, and ImageNet-R demonstrate that our framework performs\nprominently compared to various baseline methods, displaying great potential in\nexploiting intrinsic task relationships.\n", "link": "http://arxiv.org/abs/2502.11609v1", "date": "2025-02-17", "relevancy": 2.6803, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.568}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5204}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Task%20Relationships%20for%20Continual%20Learning%20Using%0A%20%20Transferability-Aware%20Task%20Embeddings&body=Title%3A%20Exploiting%20Task%20Relationships%20for%20Continual%20Learning%20Using%0A%20%20Transferability-Aware%20Task%20Embeddings%0AAuthor%3A%20Yanru%20Wu%20and%20Xiangyu%20Chen%20and%20Jianning%20Wang%20and%20Enming%20Zhang%20and%20Hanbing%20Liu%20and%20Yang%20Li%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20has%20been%20an%20essential%20topic%20in%20the%20contemporary%0Aapplication%20of%20deep%20neural%20networks%2C%20where%20catastrophic%20forgetting%20%28CF%29%20can%0Aimpede%20a%20model%27s%20ability%20to%20acquire%20knowledge%20progressively.%20Existing%20CL%0Astrategies%20primarily%20address%20CF%20by%20regularizing%20model%20updates%20or%20separating%0Atask-specific%20and%20shared%20components.%20However%2C%20these%20methods%20focus%20on%20task%20model%0Aelements%20while%20overlooking%20the%20potential%20of%20leveraging%20inter-task%20relationships%0Afor%20learning%20enhancement.%20To%20address%20this%2C%20we%20propose%20a%20transferability-aware%0Atask%20embedding%20named%20H-embedding%20and%20train%20a%20hypernet%20under%20its%20guidance%20to%0Alearn%20task-conditioned%20model%20weights%20for%20CL%20tasks.%20Particularly%2C%20H-embedding%20is%0Aintroduced%20based%20on%20an%20information%20theoretical%20transferability%20measure%20and%20is%0Adesigned%20to%20be%20online%20and%20easy%20to%20compute.%20The%20framework%20is%20also%20characterized%0Aby%20notable%20practicality%2C%20which%20only%20requires%20storing%20a%20low-dimensional%20task%0Aembedding%20for%20each%20task%2C%20and%20can%20be%20efficiently%20trained%20in%20an%20end-to-end%20way.%0AExtensive%20evaluations%20and%20experimental%20analyses%20on%20datasets%20including%20Permuted%0AMNIST%2C%20Cifar10/100%2C%20and%20ImageNet-R%20demonstrate%20that%20our%20framework%20performs%0Aprominently%20compared%20to%20various%20baseline%20methods%2C%20displaying%20great%20potential%20in%0Aexploiting%20intrinsic%20task%20relationships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Task%2520Relationships%2520for%2520Continual%2520Learning%2520Using%250A%2520%2520Transferability-Aware%2520Task%2520Embeddings%26entry.906535625%3DYanru%2520Wu%2520and%2520Xiangyu%2520Chen%2520and%2520Jianning%2520Wang%2520and%2520Enming%2520Zhang%2520and%2520Hanbing%2520Liu%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520has%2520been%2520an%2520essential%2520topic%2520in%2520the%2520contemporary%250Aapplication%2520of%2520deep%2520neural%2520networks%252C%2520where%2520catastrophic%2520forgetting%2520%2528CF%2529%2520can%250Aimpede%2520a%2520model%2527s%2520ability%2520to%2520acquire%2520knowledge%2520progressively.%2520Existing%2520CL%250Astrategies%2520primarily%2520address%2520CF%2520by%2520regularizing%2520model%2520updates%2520or%2520separating%250Atask-specific%2520and%2520shared%2520components.%2520However%252C%2520these%2520methods%2520focus%2520on%2520task%2520model%250Aelements%2520while%2520overlooking%2520the%2520potential%2520of%2520leveraging%2520inter-task%2520relationships%250Afor%2520learning%2520enhancement.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520transferability-aware%250Atask%2520embedding%2520named%2520H-embedding%2520and%2520train%2520a%2520hypernet%2520under%2520its%2520guidance%2520to%250Alearn%2520task-conditioned%2520model%2520weights%2520for%2520CL%2520tasks.%2520Particularly%252C%2520H-embedding%2520is%250Aintroduced%2520based%2520on%2520an%2520information%2520theoretical%2520transferability%2520measure%2520and%2520is%250Adesigned%2520to%2520be%2520online%2520and%2520easy%2520to%2520compute.%2520The%2520framework%2520is%2520also%2520characterized%250Aby%2520notable%2520practicality%252C%2520which%2520only%2520requires%2520storing%2520a%2520low-dimensional%2520task%250Aembedding%2520for%2520each%2520task%252C%2520and%2520can%2520be%2520efficiently%2520trained%2520in%2520an%2520end-to-end%2520way.%250AExtensive%2520evaluations%2520and%2520experimental%2520analyses%2520on%2520datasets%2520including%2520Permuted%250AMNIST%252C%2520Cifar10/100%252C%2520and%2520ImageNet-R%2520demonstrate%2520that%2520our%2520framework%2520performs%250Aprominently%2520compared%2520to%2520various%2520baseline%2520methods%252C%2520displaying%2520great%2520potential%2520in%250Aexploiting%2520intrinsic%2520task%2520relationships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Task%20Relationships%20for%20Continual%20Learning%20Using%0A%20%20Transferability-Aware%20Task%20Embeddings&entry.906535625=Yanru%20Wu%20and%20Xiangyu%20Chen%20and%20Jianning%20Wang%20and%20Enming%20Zhang%20and%20Hanbing%20Liu%20and%20Yang%20Li&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20has%20been%20an%20essential%20topic%20in%20the%20contemporary%0Aapplication%20of%20deep%20neural%20networks%2C%20where%20catastrophic%20forgetting%20%28CF%29%20can%0Aimpede%20a%20model%27s%20ability%20to%20acquire%20knowledge%20progressively.%20Existing%20CL%0Astrategies%20primarily%20address%20CF%20by%20regularizing%20model%20updates%20or%20separating%0Atask-specific%20and%20shared%20components.%20However%2C%20these%20methods%20focus%20on%20task%20model%0Aelements%20while%20overlooking%20the%20potential%20of%20leveraging%20inter-task%20relationships%0Afor%20learning%20enhancement.%20To%20address%20this%2C%20we%20propose%20a%20transferability-aware%0Atask%20embedding%20named%20H-embedding%20and%20train%20a%20hypernet%20under%20its%20guidance%20to%0Alearn%20task-conditioned%20model%20weights%20for%20CL%20tasks.%20Particularly%2C%20H-embedding%20is%0Aintroduced%20based%20on%20an%20information%20theoretical%20transferability%20measure%20and%20is%0Adesigned%20to%20be%20online%20and%20easy%20to%20compute.%20The%20framework%20is%20also%20characterized%0Aby%20notable%20practicality%2C%20which%20only%20requires%20storing%20a%20low-dimensional%20task%0Aembedding%20for%20each%20task%2C%20and%20can%20be%20efficiently%20trained%20in%20an%20end-to-end%20way.%0AExtensive%20evaluations%20and%20experimental%20analyses%20on%20datasets%20including%20Permuted%0AMNIST%2C%20Cifar10/100%2C%20and%20ImageNet-R%20demonstrate%20that%20our%20framework%20performs%0Aprominently%20compared%20to%20various%20baseline%20methods%2C%20displaying%20great%20potential%20in%0Aexploiting%20intrinsic%20task%20relationships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11609v1&entry.124074799=Read"},
{"title": "Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual\n  Metrics", "author": "Francesco Croce and Christian Schlarmann and Naman Deep Singh and Matthias Hein", "abstract": "  Measuring perceptual similarity is a key tool in computer vision. In recent\nyears perceptual metrics based on features extracted from neural networks with\nlarge and diverse training sets, e.g. CLIP, have become popular. At the same\ntime, the metrics extracted from features of neural networks are not\nadversarially robust. In this paper we show that adversarially robust CLIP\nmodels, called R-CLIP$_\\textrm{F}$, obtained by unsupervised adversarial\nfine-tuning induce a better and adversarially robust perceptual metric that\noutperforms existing metrics in a zero-shot setting, and further matches the\nperformance of state-of-the-art metrics while being robust after fine-tuning.\nMoreover, our perceptual metric achieves strong performance on related tasks\nsuch as robust image-to-image retrieval, which becomes especially relevant when\napplied to \"Not Safe for Work\" (NSFW) content detection and dataset filtering.\nWhile standard perceptual metrics can be easily attacked by a small\nperturbation completely degrading NSFW detection, our robust perceptual metric\nmaintains high accuracy under an attack while having similar performance for\nunperturbed images. Finally, perceptual metrics induced by robust CLIP models\nhave higher interpretability: feature inversion can show which images are\nconsidered similar, while text inversion can find what images are associated to\na given prompt. This also allows us to visualize the very rich visual concepts\nlearned by a CLIP model, including memorized persons, paintings and complex\nqueries.\n", "link": "http://arxiv.org/abs/2502.11725v1", "date": "2025-02-17", "relevancy": 2.6691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarially%20Robust%20CLIP%20Models%20Can%20Induce%20Better%20%28Robust%29%20Perceptual%0A%20%20Metrics&body=Title%3A%20Adversarially%20Robust%20CLIP%20Models%20Can%20Induce%20Better%20%28Robust%29%20Perceptual%0A%20%20Metrics%0AAuthor%3A%20Francesco%20Croce%20and%20Christian%20Schlarmann%20and%20Naman%20Deep%20Singh%20and%20Matthias%20Hein%0AAbstract%3A%20%20%20Measuring%20perceptual%20similarity%20is%20a%20key%20tool%20in%20computer%20vision.%20In%20recent%0Ayears%20perceptual%20metrics%20based%20on%20features%20extracted%20from%20neural%20networks%20with%0Alarge%20and%20diverse%20training%20sets%2C%20e.g.%20CLIP%2C%20have%20become%20popular.%20At%20the%20same%0Atime%2C%20the%20metrics%20extracted%20from%20features%20of%20neural%20networks%20are%20not%0Aadversarially%20robust.%20In%20this%20paper%20we%20show%20that%20adversarially%20robust%20CLIP%0Amodels%2C%20called%20R-CLIP%24_%5Ctextrm%7BF%7D%24%2C%20obtained%20by%20unsupervised%20adversarial%0Afine-tuning%20induce%20a%20better%20and%20adversarially%20robust%20perceptual%20metric%20that%0Aoutperforms%20existing%20metrics%20in%20a%20zero-shot%20setting%2C%20and%20further%20matches%20the%0Aperformance%20of%20state-of-the-art%20metrics%20while%20being%20robust%20after%20fine-tuning.%0AMoreover%2C%20our%20perceptual%20metric%20achieves%20strong%20performance%20on%20related%20tasks%0Asuch%20as%20robust%20image-to-image%20retrieval%2C%20which%20becomes%20especially%20relevant%20when%0Aapplied%20to%20%22Not%20Safe%20for%20Work%22%20%28NSFW%29%20content%20detection%20and%20dataset%20filtering.%0AWhile%20standard%20perceptual%20metrics%20can%20be%20easily%20attacked%20by%20a%20small%0Aperturbation%20completely%20degrading%20NSFW%20detection%2C%20our%20robust%20perceptual%20metric%0Amaintains%20high%20accuracy%20under%20an%20attack%20while%20having%20similar%20performance%20for%0Aunperturbed%20images.%20Finally%2C%20perceptual%20metrics%20induced%20by%20robust%20CLIP%20models%0Ahave%20higher%20interpretability%3A%20feature%20inversion%20can%20show%20which%20images%20are%0Aconsidered%20similar%2C%20while%20text%20inversion%20can%20find%20what%20images%20are%20associated%20to%0Aa%20given%20prompt.%20This%20also%20allows%20us%20to%20visualize%20the%20very%20rich%20visual%20concepts%0Alearned%20by%20a%20CLIP%20model%2C%20including%20memorized%20persons%2C%20paintings%20and%20complex%0Aqueries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarially%2520Robust%2520CLIP%2520Models%2520Can%2520Induce%2520Better%2520%2528Robust%2529%2520Perceptual%250A%2520%2520Metrics%26entry.906535625%3DFrancesco%2520Croce%2520and%2520Christian%2520Schlarmann%2520and%2520Naman%2520Deep%2520Singh%2520and%2520Matthias%2520Hein%26entry.1292438233%3D%2520%2520Measuring%2520perceptual%2520similarity%2520is%2520a%2520key%2520tool%2520in%2520computer%2520vision.%2520In%2520recent%250Ayears%2520perceptual%2520metrics%2520based%2520on%2520features%2520extracted%2520from%2520neural%2520networks%2520with%250Alarge%2520and%2520diverse%2520training%2520sets%252C%2520e.g.%2520CLIP%252C%2520have%2520become%2520popular.%2520At%2520the%2520same%250Atime%252C%2520the%2520metrics%2520extracted%2520from%2520features%2520of%2520neural%2520networks%2520are%2520not%250Aadversarially%2520robust.%2520In%2520this%2520paper%2520we%2520show%2520that%2520adversarially%2520robust%2520CLIP%250Amodels%252C%2520called%2520R-CLIP%2524_%255Ctextrm%257BF%257D%2524%252C%2520obtained%2520by%2520unsupervised%2520adversarial%250Afine-tuning%2520induce%2520a%2520better%2520and%2520adversarially%2520robust%2520perceptual%2520metric%2520that%250Aoutperforms%2520existing%2520metrics%2520in%2520a%2520zero-shot%2520setting%252C%2520and%2520further%2520matches%2520the%250Aperformance%2520of%2520state-of-the-art%2520metrics%2520while%2520being%2520robust%2520after%2520fine-tuning.%250AMoreover%252C%2520our%2520perceptual%2520metric%2520achieves%2520strong%2520performance%2520on%2520related%2520tasks%250Asuch%2520as%2520robust%2520image-to-image%2520retrieval%252C%2520which%2520becomes%2520especially%2520relevant%2520when%250Aapplied%2520to%2520%2522Not%2520Safe%2520for%2520Work%2522%2520%2528NSFW%2529%2520content%2520detection%2520and%2520dataset%2520filtering.%250AWhile%2520standard%2520perceptual%2520metrics%2520can%2520be%2520easily%2520attacked%2520by%2520a%2520small%250Aperturbation%2520completely%2520degrading%2520NSFW%2520detection%252C%2520our%2520robust%2520perceptual%2520metric%250Amaintains%2520high%2520accuracy%2520under%2520an%2520attack%2520while%2520having%2520similar%2520performance%2520for%250Aunperturbed%2520images.%2520Finally%252C%2520perceptual%2520metrics%2520induced%2520by%2520robust%2520CLIP%2520models%250Ahave%2520higher%2520interpretability%253A%2520feature%2520inversion%2520can%2520show%2520which%2520images%2520are%250Aconsidered%2520similar%252C%2520while%2520text%2520inversion%2520can%2520find%2520what%2520images%2520are%2520associated%2520to%250Aa%2520given%2520prompt.%2520This%2520also%2520allows%2520us%2520to%2520visualize%2520the%2520very%2520rich%2520visual%2520concepts%250Alearned%2520by%2520a%2520CLIP%2520model%252C%2520including%2520memorized%2520persons%252C%2520paintings%2520and%2520complex%250Aqueries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarially%20Robust%20CLIP%20Models%20Can%20Induce%20Better%20%28Robust%29%20Perceptual%0A%20%20Metrics&entry.906535625=Francesco%20Croce%20and%20Christian%20Schlarmann%20and%20Naman%20Deep%20Singh%20and%20Matthias%20Hein&entry.1292438233=%20%20Measuring%20perceptual%20similarity%20is%20a%20key%20tool%20in%20computer%20vision.%20In%20recent%0Ayears%20perceptual%20metrics%20based%20on%20features%20extracted%20from%20neural%20networks%20with%0Alarge%20and%20diverse%20training%20sets%2C%20e.g.%20CLIP%2C%20have%20become%20popular.%20At%20the%20same%0Atime%2C%20the%20metrics%20extracted%20from%20features%20of%20neural%20networks%20are%20not%0Aadversarially%20robust.%20In%20this%20paper%20we%20show%20that%20adversarially%20robust%20CLIP%0Amodels%2C%20called%20R-CLIP%24_%5Ctextrm%7BF%7D%24%2C%20obtained%20by%20unsupervised%20adversarial%0Afine-tuning%20induce%20a%20better%20and%20adversarially%20robust%20perceptual%20metric%20that%0Aoutperforms%20existing%20metrics%20in%20a%20zero-shot%20setting%2C%20and%20further%20matches%20the%0Aperformance%20of%20state-of-the-art%20metrics%20while%20being%20robust%20after%20fine-tuning.%0AMoreover%2C%20our%20perceptual%20metric%20achieves%20strong%20performance%20on%20related%20tasks%0Asuch%20as%20robust%20image-to-image%20retrieval%2C%20which%20becomes%20especially%20relevant%20when%0Aapplied%20to%20%22Not%20Safe%20for%20Work%22%20%28NSFW%29%20content%20detection%20and%20dataset%20filtering.%0AWhile%20standard%20perceptual%20metrics%20can%20be%20easily%20attacked%20by%20a%20small%0Aperturbation%20completely%20degrading%20NSFW%20detection%2C%20our%20robust%20perceptual%20metric%0Amaintains%20high%20accuracy%20under%20an%20attack%20while%20having%20similar%20performance%20for%0Aunperturbed%20images.%20Finally%2C%20perceptual%20metrics%20induced%20by%20robust%20CLIP%20models%0Ahave%20higher%20interpretability%3A%20feature%20inversion%20can%20show%20which%20images%20are%0Aconsidered%20similar%2C%20while%20text%20inversion%20can%20find%20what%20images%20are%20associated%20to%0Aa%20given%20prompt.%20This%20also%20allows%20us%20to%20visualize%20the%20very%20rich%20visual%20concepts%0Alearned%20by%20a%20CLIP%20model%2C%20including%20memorized%20persons%2C%20paintings%20and%20complex%0Aqueries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11725v1&entry.124074799=Read"},
{"title": "Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SCICAP Challenge 2023", "author": "Ting-Yao E. Hsu and Yi-Li Hsu and Shaurya Rohatgi and Chieh-Yang Huang and Ho Yin Sam Ng and Ryan Rossi and Sungchul Kim and Tong Yu and Lun-Wei Ku and C. Lee Giles and Ting-Hao K. Huang", "abstract": "  Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n", "link": "http://arxiv.org/abs/2501.19353v2", "date": "2025-02-17", "relevancy": 2.6689, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SCICAP%20Challenge%202023&body=Title%3A%20Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SCICAP%20Challenge%202023%0AAuthor%3A%20Ting-Yao%20E.%20Hsu%20and%20Yi-Li%20Hsu%20and%20Shaurya%20Rohatgi%20and%20Chieh-Yang%20Huang%20and%20Ho%20Yin%20Sam%20Ng%20and%20Ryan%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20Lun-Wei%20Ku%20and%20C.%20Lee%20Giles%20and%20Ting-Hao%20K.%20Huang%0AAbstract%3A%20%20%20Since%20the%20SCICAP%20datasets%20launch%20in%202021%2C%20the%20research%20community%20has%20made%0Asignificant%20progress%20in%20generating%20captions%20for%20scientific%20figures%20in%20scholarly%0Aarticles.%20In%202023%2C%20the%20first%20SCICAP%20Challenge%20took%20place%2C%20inviting%20global%20teams%0Ato%20use%20an%20expanded%20SCICAP%20dataset%20to%20develop%20models%20for%20captioning%20diverse%0Afigure%20types%20across%20various%20academic%20fields.%20At%20the%20same%20time%2C%20text%20generation%0Amodels%20advanced%20quickly%2C%20with%20many%20powerful%20pre-trained%20large%20multimodal%20models%0A%28LMMs%29%20emerging%20that%20showed%20impressive%20capabilities%20in%20various%0Avision-and-language%20tasks.%20This%20paper%20presents%20an%20overview%20of%20the%20first%20SCICAP%0AChallenge%20and%20details%20the%20performance%20of%20various%20models%20on%20its%20data%2C%20capturing%0Aa%20snapshot%20of%20the%20fields%20state.%20We%20found%20that%20professional%20editors%0Aoverwhelmingly%20preferred%20figure%20captions%20generated%20by%20GPT-4V%20over%20those%20from%0Aall%20other%20models%20and%20even%20the%20original%20captions%20written%20by%20authors.%20Following%0Athis%20key%20finding%2C%20we%20conducted%20detailed%20analyses%20to%20answer%20this%20question%3A%20Have%0Aadvanced%20LMMs%20solved%20the%20task%20of%20generating%20captions%20for%20scientific%20figures%3F%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19353v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Multimodal%2520Models%2520Solve%2520Caption%2520Generation%2520for%2520Scientific%250A%2520%2520Figures%253F%2520Lessons%2520Learned%2520from%2520SCICAP%2520Challenge%25202023%26entry.906535625%3DTing-Yao%2520E.%2520Hsu%2520and%2520Yi-Li%2520Hsu%2520and%2520Shaurya%2520Rohatgi%2520and%2520Chieh-Yang%2520Huang%2520and%2520Ho%2520Yin%2520Sam%2520Ng%2520and%2520Ryan%2520Rossi%2520and%2520Sungchul%2520Kim%2520and%2520Tong%2520Yu%2520and%2520Lun-Wei%2520Ku%2520and%2520C.%2520Lee%2520Giles%2520and%2520Ting-Hao%2520K.%2520Huang%26entry.1292438233%3D%2520%2520Since%2520the%2520SCICAP%2520datasets%2520launch%2520in%25202021%252C%2520the%2520research%2520community%2520has%2520made%250Asignificant%2520progress%2520in%2520generating%2520captions%2520for%2520scientific%2520figures%2520in%2520scholarly%250Aarticles.%2520In%25202023%252C%2520the%2520first%2520SCICAP%2520Challenge%2520took%2520place%252C%2520inviting%2520global%2520teams%250Ato%2520use%2520an%2520expanded%2520SCICAP%2520dataset%2520to%2520develop%2520models%2520for%2520captioning%2520diverse%250Afigure%2520types%2520across%2520various%2520academic%2520fields.%2520At%2520the%2520same%2520time%252C%2520text%2520generation%250Amodels%2520advanced%2520quickly%252C%2520with%2520many%2520powerful%2520pre-trained%2520large%2520multimodal%2520models%250A%2528LMMs%2529%2520emerging%2520that%2520showed%2520impressive%2520capabilities%2520in%2520various%250Avision-and-language%2520tasks.%2520This%2520paper%2520presents%2520an%2520overview%2520of%2520the%2520first%2520SCICAP%250AChallenge%2520and%2520details%2520the%2520performance%2520of%2520various%2520models%2520on%2520its%2520data%252C%2520capturing%250Aa%2520snapshot%2520of%2520the%2520fields%2520state.%2520We%2520found%2520that%2520professional%2520editors%250Aoverwhelmingly%2520preferred%2520figure%2520captions%2520generated%2520by%2520GPT-4V%2520over%2520those%2520from%250Aall%2520other%2520models%2520and%2520even%2520the%2520original%2520captions%2520written%2520by%2520authors.%2520Following%250Athis%2520key%2520finding%252C%2520we%2520conducted%2520detailed%2520analyses%2520to%2520answer%2520this%2520question%253A%2520Have%250Aadvanced%2520LMMs%2520solved%2520the%2520task%2520of%2520generating%2520captions%2520for%2520scientific%2520figures%253F%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19353v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SCICAP%20Challenge%202023&entry.906535625=Ting-Yao%20E.%20Hsu%20and%20Yi-Li%20Hsu%20and%20Shaurya%20Rohatgi%20and%20Chieh-Yang%20Huang%20and%20Ho%20Yin%20Sam%20Ng%20and%20Ryan%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20Lun-Wei%20Ku%20and%20C.%20Lee%20Giles%20and%20Ting-Hao%20K.%20Huang&entry.1292438233=%20%20Since%20the%20SCICAP%20datasets%20launch%20in%202021%2C%20the%20research%20community%20has%20made%0Asignificant%20progress%20in%20generating%20captions%20for%20scientific%20figures%20in%20scholarly%0Aarticles.%20In%202023%2C%20the%20first%20SCICAP%20Challenge%20took%20place%2C%20inviting%20global%20teams%0Ato%20use%20an%20expanded%20SCICAP%20dataset%20to%20develop%20models%20for%20captioning%20diverse%0Afigure%20types%20across%20various%20academic%20fields.%20At%20the%20same%20time%2C%20text%20generation%0Amodels%20advanced%20quickly%2C%20with%20many%20powerful%20pre-trained%20large%20multimodal%20models%0A%28LMMs%29%20emerging%20that%20showed%20impressive%20capabilities%20in%20various%0Avision-and-language%20tasks.%20This%20paper%20presents%20an%20overview%20of%20the%20first%20SCICAP%0AChallenge%20and%20details%20the%20performance%20of%20various%20models%20on%20its%20data%2C%20capturing%0Aa%20snapshot%20of%20the%20fields%20state.%20We%20found%20that%20professional%20editors%0Aoverwhelmingly%20preferred%20figure%20captions%20generated%20by%20GPT-4V%20over%20those%20from%0Aall%20other%20models%20and%20even%20the%20original%20captions%20written%20by%20authors.%20Following%0Athis%20key%20finding%2C%20we%20conducted%20detailed%20analyses%20to%20answer%20this%20question%3A%20Have%0Aadvanced%20LMMs%20solved%20the%20task%20of%20generating%20captions%20for%20scientific%20figures%3F%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19353v2&entry.124074799=Read"},
{"title": "Object-Attribute-Relation Representation Based Video Semantic\n  Communication", "author": "Qiyuan Du and Yiping Duan and Qianqian Yang and Xiaoming Tao and M\u00e9rouane Debbah", "abstract": "  With the rapid growth of multimedia data volume, there is an increasing need\nfor efficient video transmission in applications such as virtual reality and\nfuture video streaming services. Semantic communication is emerging as a vital\ntechnique for ensuring efficient and reliable transmission in low-bandwidth,\nhigh-noise settings. However, most current approaches focus on joint\nsource-channel coding (JSCC) that depends on end-to-end training. These methods\noften lack an interpretable semantic representation and struggle with\nadaptability to various downstream tasks. In this paper, we introduce the use\nof object-attribute-relation (OAR) as a semantic framework for videos to\nfacilitate low bit-rate coding and enhance the JSCC process for more effective\nvideo transmission. We utilize OAR sequences for both low bit-rate\nrepresentation and generative video reconstruction. Additionally, we\nincorporate OAR into the image JSCC model to prioritize communication resources\nfor areas more critical to downstream tasks. Our experiments on traffic\nsurveillance video datasets assess the effectiveness of our approach in terms\nof video transmission performance. The empirical findings demonstrate that our\nOAR-based video coding method not only outperforms H.265 coding at lower\nbit-rates but also synergizes with JSCC to deliver robust and efficient video\ntransmission.\n", "link": "http://arxiv.org/abs/2406.10469v2", "date": "2025-02-17", "relevancy": 2.6581, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Attribute-Relation%20Representation%20Based%20Video%20Semantic%0A%20%20Communication&body=Title%3A%20Object-Attribute-Relation%20Representation%20Based%20Video%20Semantic%0A%20%20Communication%0AAuthor%3A%20Qiyuan%20Du%20and%20Yiping%20Duan%20and%20Qianqian%20Yang%20and%20Xiaoming%20Tao%20and%20M%C3%A9rouane%20Debbah%0AAbstract%3A%20%20%20With%20the%20rapid%20growth%20of%20multimedia%20data%20volume%2C%20there%20is%20an%20increasing%20need%0Afor%20efficient%20video%20transmission%20in%20applications%20such%20as%20virtual%20reality%20and%0Afuture%20video%20streaming%20services.%20Semantic%20communication%20is%20emerging%20as%20a%20vital%0Atechnique%20for%20ensuring%20efficient%20and%20reliable%20transmission%20in%20low-bandwidth%2C%0Ahigh-noise%20settings.%20However%2C%20most%20current%20approaches%20focus%20on%20joint%0Asource-channel%20coding%20%28JSCC%29%20that%20depends%20on%20end-to-end%20training.%20These%20methods%0Aoften%20lack%20an%20interpretable%20semantic%20representation%20and%20struggle%20with%0Aadaptability%20to%20various%20downstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20the%20use%0Aof%20object-attribute-relation%20%28OAR%29%20as%20a%20semantic%20framework%20for%20videos%20to%0Afacilitate%20low%20bit-rate%20coding%20and%20enhance%20the%20JSCC%20process%20for%20more%20effective%0Avideo%20transmission.%20We%20utilize%20OAR%20sequences%20for%20both%20low%20bit-rate%0Arepresentation%20and%20generative%20video%20reconstruction.%20Additionally%2C%20we%0Aincorporate%20OAR%20into%20the%20image%20JSCC%20model%20to%20prioritize%20communication%20resources%0Afor%20areas%20more%20critical%20to%20downstream%20tasks.%20Our%20experiments%20on%20traffic%0Asurveillance%20video%20datasets%20assess%20the%20effectiveness%20of%20our%20approach%20in%20terms%0Aof%20video%20transmission%20performance.%20The%20empirical%20findings%20demonstrate%20that%20our%0AOAR-based%20video%20coding%20method%20not%20only%20outperforms%20H.265%20coding%20at%20lower%0Abit-rates%20but%20also%20synergizes%20with%20JSCC%20to%20deliver%20robust%20and%20efficient%20video%0Atransmission.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Attribute-Relation%2520Representation%2520Based%2520Video%2520Semantic%250A%2520%2520Communication%26entry.906535625%3DQiyuan%2520Du%2520and%2520Yiping%2520Duan%2520and%2520Qianqian%2520Yang%2520and%2520Xiaoming%2520Tao%2520and%2520M%25C3%25A9rouane%2520Debbah%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520growth%2520of%2520multimedia%2520data%2520volume%252C%2520there%2520is%2520an%2520increasing%2520need%250Afor%2520efficient%2520video%2520transmission%2520in%2520applications%2520such%2520as%2520virtual%2520reality%2520and%250Afuture%2520video%2520streaming%2520services.%2520Semantic%2520communication%2520is%2520emerging%2520as%2520a%2520vital%250Atechnique%2520for%2520ensuring%2520efficient%2520and%2520reliable%2520transmission%2520in%2520low-bandwidth%252C%250Ahigh-noise%2520settings.%2520However%252C%2520most%2520current%2520approaches%2520focus%2520on%2520joint%250Asource-channel%2520coding%2520%2528JSCC%2529%2520that%2520depends%2520on%2520end-to-end%2520training.%2520These%2520methods%250Aoften%2520lack%2520an%2520interpretable%2520semantic%2520representation%2520and%2520struggle%2520with%250Aadaptability%2520to%2520various%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520use%250Aof%2520object-attribute-relation%2520%2528OAR%2529%2520as%2520a%2520semantic%2520framework%2520for%2520videos%2520to%250Afacilitate%2520low%2520bit-rate%2520coding%2520and%2520enhance%2520the%2520JSCC%2520process%2520for%2520more%2520effective%250Avideo%2520transmission.%2520We%2520utilize%2520OAR%2520sequences%2520for%2520both%2520low%2520bit-rate%250Arepresentation%2520and%2520generative%2520video%2520reconstruction.%2520Additionally%252C%2520we%250Aincorporate%2520OAR%2520into%2520the%2520image%2520JSCC%2520model%2520to%2520prioritize%2520communication%2520resources%250Afor%2520areas%2520more%2520critical%2520to%2520downstream%2520tasks.%2520Our%2520experiments%2520on%2520traffic%250Asurveillance%2520video%2520datasets%2520assess%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520terms%250Aof%2520video%2520transmission%2520performance.%2520The%2520empirical%2520findings%2520demonstrate%2520that%2520our%250AOAR-based%2520video%2520coding%2520method%2520not%2520only%2520outperforms%2520H.265%2520coding%2520at%2520lower%250Abit-rates%2520but%2520also%2520synergizes%2520with%2520JSCC%2520to%2520deliver%2520robust%2520and%2520efficient%2520video%250Atransmission.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Attribute-Relation%20Representation%20Based%20Video%20Semantic%0A%20%20Communication&entry.906535625=Qiyuan%20Du%20and%20Yiping%20Duan%20and%20Qianqian%20Yang%20and%20Xiaoming%20Tao%20and%20M%C3%A9rouane%20Debbah&entry.1292438233=%20%20With%20the%20rapid%20growth%20of%20multimedia%20data%20volume%2C%20there%20is%20an%20increasing%20need%0Afor%20efficient%20video%20transmission%20in%20applications%20such%20as%20virtual%20reality%20and%0Afuture%20video%20streaming%20services.%20Semantic%20communication%20is%20emerging%20as%20a%20vital%0Atechnique%20for%20ensuring%20efficient%20and%20reliable%20transmission%20in%20low-bandwidth%2C%0Ahigh-noise%20settings.%20However%2C%20most%20current%20approaches%20focus%20on%20joint%0Asource-channel%20coding%20%28JSCC%29%20that%20depends%20on%20end-to-end%20training.%20These%20methods%0Aoften%20lack%20an%20interpretable%20semantic%20representation%20and%20struggle%20with%0Aadaptability%20to%20various%20downstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20the%20use%0Aof%20object-attribute-relation%20%28OAR%29%20as%20a%20semantic%20framework%20for%20videos%20to%0Afacilitate%20low%20bit-rate%20coding%20and%20enhance%20the%20JSCC%20process%20for%20more%20effective%0Avideo%20transmission.%20We%20utilize%20OAR%20sequences%20for%20both%20low%20bit-rate%0Arepresentation%20and%20generative%20video%20reconstruction.%20Additionally%2C%20we%0Aincorporate%20OAR%20into%20the%20image%20JSCC%20model%20to%20prioritize%20communication%20resources%0Afor%20areas%20more%20critical%20to%20downstream%20tasks.%20Our%20experiments%20on%20traffic%0Asurveillance%20video%20datasets%20assess%20the%20effectiveness%20of%20our%20approach%20in%20terms%0Aof%20video%20transmission%20performance.%20The%20empirical%20findings%20demonstrate%20that%20our%0AOAR-based%20video%20coding%20method%20not%20only%20outperforms%20H.265%20coding%20at%20lower%0Abit-rates%20but%20also%20synergizes%20with%20JSCC%20to%20deliver%20robust%20and%20efficient%20video%0Atransmission.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10469v2&entry.124074799=Read"},
{"title": "No-reference geometry quality assessment for colorless point clouds via\n  list-wise rank learning", "author": "Zheng Li and Bingxu Xie and Chao Chu and Weiqing Li and Zhiyong Su", "abstract": "  Geometry quality assessment (GQA) of colorless point clouds is crucial for\nevaluating the performance of emerging point cloud-based solutions (e.g.,\nwatermarking, compression, and 3-Dimensional (3D) reconstruction).\nUnfortunately, existing objective GQA approaches are traditional full-reference\nmetrics, whereas state-of-the-art learning-based point cloud quality assessment\n(PCQA) methods target both color and geometry distortions, neither of which are\nqualified for the no-reference GQA task. In addition, the lack of large-scale\nGQA datasets with subjective scores, which are always imprecise, biased, and\ninconsistent, also hinders the development of learning-based GQA metrics.\nDriven by these limitations, this paper proposes a no-reference geometry-only\nquality assessment approach based on list-wise rank learning, termed LRL-GQA,\nwhich comprises of a geometry quality assessment network (GQANet) and a\nlist-wise rank learning network (LRLNet). The proposed LRL-GQA formulates the\nno-reference GQA as a list-wise rank problem, with the objective of directly\noptimizing the entire quality ordering. Specifically, a large dataset\ncontaining a variety of geometry-only distortions is constructed first, named\nLRL dataset, in which each sample is label-free but coupled with quality\nranking information. Then, the GQANet is designed to capture intrinsic\nmulti-scale patch-wise geometric features in order to predict a quality index\nfor each point cloud. After that, the LRLNet leverages the LRL dataset and a\nlikelihood loss to train the GQANet and ranks the input list of degraded point\nclouds according to their distortion levels. In addition, the pre-trained\nGQANet can be fine-tuned further to obtain absolute quality scores.\nExperimental results demonstrate the superior performance of the proposed\nno-reference LRL-GQA method compared with existing full-reference GQA metrics.\n", "link": "http://arxiv.org/abs/2502.11726v1", "date": "2025-02-17", "relevancy": 2.6561, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.554}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5246}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No-reference%20geometry%20quality%20assessment%20for%20colorless%20point%20clouds%20via%0A%20%20list-wise%20rank%20learning&body=Title%3A%20No-reference%20geometry%20quality%20assessment%20for%20colorless%20point%20clouds%20via%0A%20%20list-wise%20rank%20learning%0AAuthor%3A%20Zheng%20Li%20and%20Bingxu%20Xie%20and%20Chao%20Chu%20and%20Weiqing%20Li%20and%20Zhiyong%20Su%0AAbstract%3A%20%20%20Geometry%20quality%20assessment%20%28GQA%29%20of%20colorless%20point%20clouds%20is%20crucial%20for%0Aevaluating%20the%20performance%20of%20emerging%20point%20cloud-based%20solutions%20%28e.g.%2C%0Awatermarking%2C%20compression%2C%20and%203-Dimensional%20%283D%29%20reconstruction%29.%0AUnfortunately%2C%20existing%20objective%20GQA%20approaches%20are%20traditional%20full-reference%0Ametrics%2C%20whereas%20state-of-the-art%20learning-based%20point%20cloud%20quality%20assessment%0A%28PCQA%29%20methods%20target%20both%20color%20and%20geometry%20distortions%2C%20neither%20of%20which%20are%0Aqualified%20for%20the%20no-reference%20GQA%20task.%20In%20addition%2C%20the%20lack%20of%20large-scale%0AGQA%20datasets%20with%20subjective%20scores%2C%20which%20are%20always%20imprecise%2C%20biased%2C%20and%0Ainconsistent%2C%20also%20hinders%20the%20development%20of%20learning-based%20GQA%20metrics.%0ADriven%20by%20these%20limitations%2C%20this%20paper%20proposes%20a%20no-reference%20geometry-only%0Aquality%20assessment%20approach%20based%20on%20list-wise%20rank%20learning%2C%20termed%20LRL-GQA%2C%0Awhich%20comprises%20of%20a%20geometry%20quality%20assessment%20network%20%28GQANet%29%20and%20a%0Alist-wise%20rank%20learning%20network%20%28LRLNet%29.%20The%20proposed%20LRL-GQA%20formulates%20the%0Ano-reference%20GQA%20as%20a%20list-wise%20rank%20problem%2C%20with%20the%20objective%20of%20directly%0Aoptimizing%20the%20entire%20quality%20ordering.%20Specifically%2C%20a%20large%20dataset%0Acontaining%20a%20variety%20of%20geometry-only%20distortions%20is%20constructed%20first%2C%20named%0ALRL%20dataset%2C%20in%20which%20each%20sample%20is%20label-free%20but%20coupled%20with%20quality%0Aranking%20information.%20Then%2C%20the%20GQANet%20is%20designed%20to%20capture%20intrinsic%0Amulti-scale%20patch-wise%20geometric%20features%20in%20order%20to%20predict%20a%20quality%20index%0Afor%20each%20point%20cloud.%20After%20that%2C%20the%20LRLNet%20leverages%20the%20LRL%20dataset%20and%20a%0Alikelihood%20loss%20to%20train%20the%20GQANet%20and%20ranks%20the%20input%20list%20of%20degraded%20point%0Aclouds%20according%20to%20their%20distortion%20levels.%20In%20addition%2C%20the%20pre-trained%0AGQANet%20can%20be%20fine-tuned%20further%20to%20obtain%20absolute%20quality%20scores.%0AExperimental%20results%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%0Ano-reference%20LRL-GQA%20method%20compared%20with%20existing%20full-reference%20GQA%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo-reference%2520geometry%2520quality%2520assessment%2520for%2520colorless%2520point%2520clouds%2520via%250A%2520%2520list-wise%2520rank%2520learning%26entry.906535625%3DZheng%2520Li%2520and%2520Bingxu%2520Xie%2520and%2520Chao%2520Chu%2520and%2520Weiqing%2520Li%2520and%2520Zhiyong%2520Su%26entry.1292438233%3D%2520%2520Geometry%2520quality%2520assessment%2520%2528GQA%2529%2520of%2520colorless%2520point%2520clouds%2520is%2520crucial%2520for%250Aevaluating%2520the%2520performance%2520of%2520emerging%2520point%2520cloud-based%2520solutions%2520%2528e.g.%252C%250Awatermarking%252C%2520compression%252C%2520and%25203-Dimensional%2520%25283D%2529%2520reconstruction%2529.%250AUnfortunately%252C%2520existing%2520objective%2520GQA%2520approaches%2520are%2520traditional%2520full-reference%250Ametrics%252C%2520whereas%2520state-of-the-art%2520learning-based%2520point%2520cloud%2520quality%2520assessment%250A%2528PCQA%2529%2520methods%2520target%2520both%2520color%2520and%2520geometry%2520distortions%252C%2520neither%2520of%2520which%2520are%250Aqualified%2520for%2520the%2520no-reference%2520GQA%2520task.%2520In%2520addition%252C%2520the%2520lack%2520of%2520large-scale%250AGQA%2520datasets%2520with%2520subjective%2520scores%252C%2520which%2520are%2520always%2520imprecise%252C%2520biased%252C%2520and%250Ainconsistent%252C%2520also%2520hinders%2520the%2520development%2520of%2520learning-based%2520GQA%2520metrics.%250ADriven%2520by%2520these%2520limitations%252C%2520this%2520paper%2520proposes%2520a%2520no-reference%2520geometry-only%250Aquality%2520assessment%2520approach%2520based%2520on%2520list-wise%2520rank%2520learning%252C%2520termed%2520LRL-GQA%252C%250Awhich%2520comprises%2520of%2520a%2520geometry%2520quality%2520assessment%2520network%2520%2528GQANet%2529%2520and%2520a%250Alist-wise%2520rank%2520learning%2520network%2520%2528LRLNet%2529.%2520The%2520proposed%2520LRL-GQA%2520formulates%2520the%250Ano-reference%2520GQA%2520as%2520a%2520list-wise%2520rank%2520problem%252C%2520with%2520the%2520objective%2520of%2520directly%250Aoptimizing%2520the%2520entire%2520quality%2520ordering.%2520Specifically%252C%2520a%2520large%2520dataset%250Acontaining%2520a%2520variety%2520of%2520geometry-only%2520distortions%2520is%2520constructed%2520first%252C%2520named%250ALRL%2520dataset%252C%2520in%2520which%2520each%2520sample%2520is%2520label-free%2520but%2520coupled%2520with%2520quality%250Aranking%2520information.%2520Then%252C%2520the%2520GQANet%2520is%2520designed%2520to%2520capture%2520intrinsic%250Amulti-scale%2520patch-wise%2520geometric%2520features%2520in%2520order%2520to%2520predict%2520a%2520quality%2520index%250Afor%2520each%2520point%2520cloud.%2520After%2520that%252C%2520the%2520LRLNet%2520leverages%2520the%2520LRL%2520dataset%2520and%2520a%250Alikelihood%2520loss%2520to%2520train%2520the%2520GQANet%2520and%2520ranks%2520the%2520input%2520list%2520of%2520degraded%2520point%250Aclouds%2520according%2520to%2520their%2520distortion%2520levels.%2520In%2520addition%252C%2520the%2520pre-trained%250AGQANet%2520can%2520be%2520fine-tuned%2520further%2520to%2520obtain%2520absolute%2520quality%2520scores.%250AExperimental%2520results%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%250Ano-reference%2520LRL-GQA%2520method%2520compared%2520with%2520existing%2520full-reference%2520GQA%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No-reference%20geometry%20quality%20assessment%20for%20colorless%20point%20clouds%20via%0A%20%20list-wise%20rank%20learning&entry.906535625=Zheng%20Li%20and%20Bingxu%20Xie%20and%20Chao%20Chu%20and%20Weiqing%20Li%20and%20Zhiyong%20Su&entry.1292438233=%20%20Geometry%20quality%20assessment%20%28GQA%29%20of%20colorless%20point%20clouds%20is%20crucial%20for%0Aevaluating%20the%20performance%20of%20emerging%20point%20cloud-based%20solutions%20%28e.g.%2C%0Awatermarking%2C%20compression%2C%20and%203-Dimensional%20%283D%29%20reconstruction%29.%0AUnfortunately%2C%20existing%20objective%20GQA%20approaches%20are%20traditional%20full-reference%0Ametrics%2C%20whereas%20state-of-the-art%20learning-based%20point%20cloud%20quality%20assessment%0A%28PCQA%29%20methods%20target%20both%20color%20and%20geometry%20distortions%2C%20neither%20of%20which%20are%0Aqualified%20for%20the%20no-reference%20GQA%20task.%20In%20addition%2C%20the%20lack%20of%20large-scale%0AGQA%20datasets%20with%20subjective%20scores%2C%20which%20are%20always%20imprecise%2C%20biased%2C%20and%0Ainconsistent%2C%20also%20hinders%20the%20development%20of%20learning-based%20GQA%20metrics.%0ADriven%20by%20these%20limitations%2C%20this%20paper%20proposes%20a%20no-reference%20geometry-only%0Aquality%20assessment%20approach%20based%20on%20list-wise%20rank%20learning%2C%20termed%20LRL-GQA%2C%0Awhich%20comprises%20of%20a%20geometry%20quality%20assessment%20network%20%28GQANet%29%20and%20a%0Alist-wise%20rank%20learning%20network%20%28LRLNet%29.%20The%20proposed%20LRL-GQA%20formulates%20the%0Ano-reference%20GQA%20as%20a%20list-wise%20rank%20problem%2C%20with%20the%20objective%20of%20directly%0Aoptimizing%20the%20entire%20quality%20ordering.%20Specifically%2C%20a%20large%20dataset%0Acontaining%20a%20variety%20of%20geometry-only%20distortions%20is%20constructed%20first%2C%20named%0ALRL%20dataset%2C%20in%20which%20each%20sample%20is%20label-free%20but%20coupled%20with%20quality%0Aranking%20information.%20Then%2C%20the%20GQANet%20is%20designed%20to%20capture%20intrinsic%0Amulti-scale%20patch-wise%20geometric%20features%20in%20order%20to%20predict%20a%20quality%20index%0Afor%20each%20point%20cloud.%20After%20that%2C%20the%20LRLNet%20leverages%20the%20LRL%20dataset%20and%20a%0Alikelihood%20loss%20to%20train%20the%20GQANet%20and%20ranks%20the%20input%20list%20of%20degraded%20point%0Aclouds%20according%20to%20their%20distortion%20levels.%20In%20addition%2C%20the%20pre-trained%0AGQANet%20can%20be%20fine-tuned%20further%20to%20obtain%20absolute%20quality%20scores.%0AExperimental%20results%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%0Ano-reference%20LRL-GQA%20method%20compared%20with%20existing%20full-reference%20GQA%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11726v1&entry.124074799=Read"},
{"title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection", "author": "Jinhe Bi and Yifan Wang and Danqi Yan and Xun Xiao and Artur Hecker and Volker Tresp and Yunpu Ma", "abstract": "  Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.\n", "link": "http://arxiv.org/abs/2502.12119v1", "date": "2025-02-17", "relevancy": 2.6543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRISM%3A%20Self-Pruning%20Intrinsic%20Selection%20Method%20for%20Training-Free%0A%20%20Multimodal%20Data%20Selection&body=Title%3A%20PRISM%3A%20Self-Pruning%20Intrinsic%20Selection%20Method%20for%20Training-Free%0A%20%20Multimodal%20Data%20Selection%0AAuthor%3A%20Jinhe%20Bi%20and%20Yifan%20Wang%20and%20Danqi%20Yan%20and%20Xun%20Xiao%20and%20Artur%20Hecker%20and%20Volker%20Tresp%20and%20Yunpu%20Ma%0AAbstract%3A%20%20%20Visual%20instruction%20tuning%20refines%20pre-trained%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20enhance%20their%20real-world%20task%20performance.%20However%2C%20the%20rapid%0Aexpansion%20of%20visual%20instruction%20datasets%20introduces%20significant%20data%0Aredundancy%2C%20leading%20to%20excessive%20computational%20costs.%20Existing%20data%20selection%0Amethods%20predominantly%20rely%20on%20proxy%20models%20or%20loss-based%20metrics%2C%20both%20of%20which%0Aimpose%20substantial%20computational%20overheads%20due%20to%20the%20necessity%20of%20model%0Ainference%20and%20backpropagation.%20To%20address%20this%20challenge%2C%20we%20propose%20PRISM%2C%20a%0Anovel%20training-free%20approach%20for%20efficient%20multimodal%20data%20selection.%20Unlike%0Aexisting%20methods%2C%20PRISM%20eliminates%20the%20reliance%20on%20proxy%20models%2C%20warm-up%0Apretraining%2C%20and%20gradient-based%20optimization.%20Instead%2C%20it%20leverages%20Pearson%0Acorrelation%20analysis%20to%20quantify%20the%20intrinsic%20visual%20encoding%20properties%20of%0AMLLMs%2C%20computing%20a%20task-specific%20correlation%20score%20to%20identify%20high-value%0Ainstances.%20This%20not%20only%20enbles%20data-efficient%20selection%2Cbut%20maintains%20the%0Aoriginal%20performance.%20Empirical%20evaluations%20across%20multiple%20MLLMs%20demonstrate%0Athat%20PRISM%20reduces%20the%20overall%20time%20required%20for%20visual%20instruction%20tuning%20and%0Adata%20selection%20to%20just%2030%25%20of%20conventional%20methods%2C%20while%20surpassing%20fully%0Afine-tuned%20models%20across%20eight%20multimodal%20and%20three%20language%20understanding%0Abenchmarks%2C%20achieving%20a%20101.7%25%20relative%20improvement%20in%20final%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRISM%253A%2520Self-Pruning%2520Intrinsic%2520Selection%2520Method%2520for%2520Training-Free%250A%2520%2520Multimodal%2520Data%2520Selection%26entry.906535625%3DJinhe%2520Bi%2520and%2520Yifan%2520Wang%2520and%2520Danqi%2520Yan%2520and%2520Xun%2520Xiao%2520and%2520Artur%2520Hecker%2520and%2520Volker%2520Tresp%2520and%2520Yunpu%2520Ma%26entry.1292438233%3D%2520%2520Visual%2520instruction%2520tuning%2520refines%2520pre-trained%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520to%2520enhance%2520their%2520real-world%2520task%2520performance.%2520However%252C%2520the%2520rapid%250Aexpansion%2520of%2520visual%2520instruction%2520datasets%2520introduces%2520significant%2520data%250Aredundancy%252C%2520leading%2520to%2520excessive%2520computational%2520costs.%2520Existing%2520data%2520selection%250Amethods%2520predominantly%2520rely%2520on%2520proxy%2520models%2520or%2520loss-based%2520metrics%252C%2520both%2520of%2520which%250Aimpose%2520substantial%2520computational%2520overheads%2520due%2520to%2520the%2520necessity%2520of%2520model%250Ainference%2520and%2520backpropagation.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520PRISM%252C%2520a%250Anovel%2520training-free%2520approach%2520for%2520efficient%2520multimodal%2520data%2520selection.%2520Unlike%250Aexisting%2520methods%252C%2520PRISM%2520eliminates%2520the%2520reliance%2520on%2520proxy%2520models%252C%2520warm-up%250Apretraining%252C%2520and%2520gradient-based%2520optimization.%2520Instead%252C%2520it%2520leverages%2520Pearson%250Acorrelation%2520analysis%2520to%2520quantify%2520the%2520intrinsic%2520visual%2520encoding%2520properties%2520of%250AMLLMs%252C%2520computing%2520a%2520task-specific%2520correlation%2520score%2520to%2520identify%2520high-value%250Ainstances.%2520This%2520not%2520only%2520enbles%2520data-efficient%2520selection%252Cbut%2520maintains%2520the%250Aoriginal%2520performance.%2520Empirical%2520evaluations%2520across%2520multiple%2520MLLMs%2520demonstrate%250Athat%2520PRISM%2520reduces%2520the%2520overall%2520time%2520required%2520for%2520visual%2520instruction%2520tuning%2520and%250Adata%2520selection%2520to%2520just%252030%2525%2520of%2520conventional%2520methods%252C%2520while%2520surpassing%2520fully%250Afine-tuned%2520models%2520across%2520eight%2520multimodal%2520and%2520three%2520language%2520understanding%250Abenchmarks%252C%2520achieving%2520a%2520101.7%2525%2520relative%2520improvement%2520in%2520final%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISM%3A%20Self-Pruning%20Intrinsic%20Selection%20Method%20for%20Training-Free%0A%20%20Multimodal%20Data%20Selection&entry.906535625=Jinhe%20Bi%20and%20Yifan%20Wang%20and%20Danqi%20Yan%20and%20Xun%20Xiao%20and%20Artur%20Hecker%20and%20Volker%20Tresp%20and%20Yunpu%20Ma&entry.1292438233=%20%20Visual%20instruction%20tuning%20refines%20pre-trained%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20enhance%20their%20real-world%20task%20performance.%20However%2C%20the%20rapid%0Aexpansion%20of%20visual%20instruction%20datasets%20introduces%20significant%20data%0Aredundancy%2C%20leading%20to%20excessive%20computational%20costs.%20Existing%20data%20selection%0Amethods%20predominantly%20rely%20on%20proxy%20models%20or%20loss-based%20metrics%2C%20both%20of%20which%0Aimpose%20substantial%20computational%20overheads%20due%20to%20the%20necessity%20of%20model%0Ainference%20and%20backpropagation.%20To%20address%20this%20challenge%2C%20we%20propose%20PRISM%2C%20a%0Anovel%20training-free%20approach%20for%20efficient%20multimodal%20data%20selection.%20Unlike%0Aexisting%20methods%2C%20PRISM%20eliminates%20the%20reliance%20on%20proxy%20models%2C%20warm-up%0Apretraining%2C%20and%20gradient-based%20optimization.%20Instead%2C%20it%20leverages%20Pearson%0Acorrelation%20analysis%20to%20quantify%20the%20intrinsic%20visual%20encoding%20properties%20of%0AMLLMs%2C%20computing%20a%20task-specific%20correlation%20score%20to%20identify%20high-value%0Ainstances.%20This%20not%20only%20enbles%20data-efficient%20selection%2Cbut%20maintains%20the%0Aoriginal%20performance.%20Empirical%20evaluations%20across%20multiple%20MLLMs%20demonstrate%0Athat%20PRISM%20reduces%20the%20overall%20time%20required%20for%20visual%20instruction%20tuning%20and%0Adata%20selection%20to%20just%2030%25%20of%20conventional%20methods%2C%20while%20surpassing%20fully%0Afine-tuned%20models%20across%20eight%20multimodal%20and%20three%20language%20understanding%0Abenchmarks%2C%20achieving%20a%20101.7%25%20relative%20improvement%20in%20final%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12119v1&entry.124074799=Read"},
{"title": "Generation and Detection of Sign Language Deepfakes - A Linguistic and\n  Visual Analysis", "author": "Shahzeb Naeem and Muhammad Riyyan Khan and Usman Tariq and Abhinav Dhall and Carlos Ivan Colon and Hasan Al-Nashash", "abstract": "  This research explores the positive application of deepfake technology for\nupper body generation, specifically sign language for the Deaf and Hard of\nHearing (DHoH) community. Given the complexity of sign language and the\nscarcity of experts, the generated videos are vetted by a sign language expert\nfor accuracy. We construct a reliable deepfake dataset, evaluating its\ntechnical and visual credibility using computer vision and natural language\nprocessing models. The dataset, consisting of over 1200 videos featuring both\nseen and unseen individuals, is also used to detect deepfake videos targeting\nvulnerable individuals. Expert annotations confirm that the generated videos\nare comparable to real sign language content. Linguistic analysis, using\ntextual similarity scores and interpreter evaluations, shows that the\ninterpretation of generated videos is at least 90% similar to authentic sign\nlanguage. Visual analysis demonstrates that convincingly realistic deepfakes\ncan be produced, even for new subjects. Using a pose/style transfer model, we\npay close attention to detail, ensuring hand movements are accurate and align\nwith the driving video. We also apply machine learning algorithms to establish\na baseline for deepfake detection on this dataset, contributing to the\ndetection of fraudulent sign language videos.\n", "link": "http://arxiv.org/abs/2404.01438v2", "date": "2025-02-17", "relevancy": 2.6535, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5433}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5248}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20and%20Detection%20of%20Sign%20Language%20Deepfakes%20-%20A%20Linguistic%20and%0A%20%20Visual%20Analysis&body=Title%3A%20Generation%20and%20Detection%20of%20Sign%20Language%20Deepfakes%20-%20A%20Linguistic%20and%0A%20%20Visual%20Analysis%0AAuthor%3A%20Shahzeb%20Naeem%20and%20Muhammad%20Riyyan%20Khan%20and%20Usman%20Tariq%20and%20Abhinav%20Dhall%20and%20Carlos%20Ivan%20Colon%20and%20Hasan%20Al-Nashash%0AAbstract%3A%20%20%20This%20research%20explores%20the%20positive%20application%20of%20deepfake%20technology%20for%0Aupper%20body%20generation%2C%20specifically%20sign%20language%20for%20the%20Deaf%20and%20Hard%20of%0AHearing%20%28DHoH%29%20community.%20Given%20the%20complexity%20of%20sign%20language%20and%20the%0Ascarcity%20of%20experts%2C%20the%20generated%20videos%20are%20vetted%20by%20a%20sign%20language%20expert%0Afor%20accuracy.%20We%20construct%20a%20reliable%20deepfake%20dataset%2C%20evaluating%20its%0Atechnical%20and%20visual%20credibility%20using%20computer%20vision%20and%20natural%20language%0Aprocessing%20models.%20The%20dataset%2C%20consisting%20of%20over%201200%20videos%20featuring%20both%0Aseen%20and%20unseen%20individuals%2C%20is%20also%20used%20to%20detect%20deepfake%20videos%20targeting%0Avulnerable%20individuals.%20Expert%20annotations%20confirm%20that%20the%20generated%20videos%0Aare%20comparable%20to%20real%20sign%20language%20content.%20Linguistic%20analysis%2C%20using%0Atextual%20similarity%20scores%20and%20interpreter%20evaluations%2C%20shows%20that%20the%0Ainterpretation%20of%20generated%20videos%20is%20at%20least%2090%25%20similar%20to%20authentic%20sign%0Alanguage.%20Visual%20analysis%20demonstrates%20that%20convincingly%20realistic%20deepfakes%0Acan%20be%20produced%2C%20even%20for%20new%20subjects.%20Using%20a%20pose/style%20transfer%20model%2C%20we%0Apay%20close%20attention%20to%20detail%2C%20ensuring%20hand%20movements%20are%20accurate%20and%20align%0Awith%20the%20driving%20video.%20We%20also%20apply%20machine%20learning%20algorithms%20to%20establish%0Aa%20baseline%20for%20deepfake%20detection%20on%20this%20dataset%2C%20contributing%20to%20the%0Adetection%20of%20fraudulent%20sign%20language%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520and%2520Detection%2520of%2520Sign%2520Language%2520Deepfakes%2520-%2520A%2520Linguistic%2520and%250A%2520%2520Visual%2520Analysis%26entry.906535625%3DShahzeb%2520Naeem%2520and%2520Muhammad%2520Riyyan%2520Khan%2520and%2520Usman%2520Tariq%2520and%2520Abhinav%2520Dhall%2520and%2520Carlos%2520Ivan%2520Colon%2520and%2520Hasan%2520Al-Nashash%26entry.1292438233%3D%2520%2520This%2520research%2520explores%2520the%2520positive%2520application%2520of%2520deepfake%2520technology%2520for%250Aupper%2520body%2520generation%252C%2520specifically%2520sign%2520language%2520for%2520the%2520Deaf%2520and%2520Hard%2520of%250AHearing%2520%2528DHoH%2529%2520community.%2520Given%2520the%2520complexity%2520of%2520sign%2520language%2520and%2520the%250Ascarcity%2520of%2520experts%252C%2520the%2520generated%2520videos%2520are%2520vetted%2520by%2520a%2520sign%2520language%2520expert%250Afor%2520accuracy.%2520We%2520construct%2520a%2520reliable%2520deepfake%2520dataset%252C%2520evaluating%2520its%250Atechnical%2520and%2520visual%2520credibility%2520using%2520computer%2520vision%2520and%2520natural%2520language%250Aprocessing%2520models.%2520The%2520dataset%252C%2520consisting%2520of%2520over%25201200%2520videos%2520featuring%2520both%250Aseen%2520and%2520unseen%2520individuals%252C%2520is%2520also%2520used%2520to%2520detect%2520deepfake%2520videos%2520targeting%250Avulnerable%2520individuals.%2520Expert%2520annotations%2520confirm%2520that%2520the%2520generated%2520videos%250Aare%2520comparable%2520to%2520real%2520sign%2520language%2520content.%2520Linguistic%2520analysis%252C%2520using%250Atextual%2520similarity%2520scores%2520and%2520interpreter%2520evaluations%252C%2520shows%2520that%2520the%250Ainterpretation%2520of%2520generated%2520videos%2520is%2520at%2520least%252090%2525%2520similar%2520to%2520authentic%2520sign%250Alanguage.%2520Visual%2520analysis%2520demonstrates%2520that%2520convincingly%2520realistic%2520deepfakes%250Acan%2520be%2520produced%252C%2520even%2520for%2520new%2520subjects.%2520Using%2520a%2520pose/style%2520transfer%2520model%252C%2520we%250Apay%2520close%2520attention%2520to%2520detail%252C%2520ensuring%2520hand%2520movements%2520are%2520accurate%2520and%2520align%250Awith%2520the%2520driving%2520video.%2520We%2520also%2520apply%2520machine%2520learning%2520algorithms%2520to%2520establish%250Aa%2520baseline%2520for%2520deepfake%2520detection%2520on%2520this%2520dataset%252C%2520contributing%2520to%2520the%250Adetection%2520of%2520fraudulent%2520sign%2520language%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20and%20Detection%20of%20Sign%20Language%20Deepfakes%20-%20A%20Linguistic%20and%0A%20%20Visual%20Analysis&entry.906535625=Shahzeb%20Naeem%20and%20Muhammad%20Riyyan%20Khan%20and%20Usman%20Tariq%20and%20Abhinav%20Dhall%20and%20Carlos%20Ivan%20Colon%20and%20Hasan%20Al-Nashash&entry.1292438233=%20%20This%20research%20explores%20the%20positive%20application%20of%20deepfake%20technology%20for%0Aupper%20body%20generation%2C%20specifically%20sign%20language%20for%20the%20Deaf%20and%20Hard%20of%0AHearing%20%28DHoH%29%20community.%20Given%20the%20complexity%20of%20sign%20language%20and%20the%0Ascarcity%20of%20experts%2C%20the%20generated%20videos%20are%20vetted%20by%20a%20sign%20language%20expert%0Afor%20accuracy.%20We%20construct%20a%20reliable%20deepfake%20dataset%2C%20evaluating%20its%0Atechnical%20and%20visual%20credibility%20using%20computer%20vision%20and%20natural%20language%0Aprocessing%20models.%20The%20dataset%2C%20consisting%20of%20over%201200%20videos%20featuring%20both%0Aseen%20and%20unseen%20individuals%2C%20is%20also%20used%20to%20detect%20deepfake%20videos%20targeting%0Avulnerable%20individuals.%20Expert%20annotations%20confirm%20that%20the%20generated%20videos%0Aare%20comparable%20to%20real%20sign%20language%20content.%20Linguistic%20analysis%2C%20using%0Atextual%20similarity%20scores%20and%20interpreter%20evaluations%2C%20shows%20that%20the%0Ainterpretation%20of%20generated%20videos%20is%20at%20least%2090%25%20similar%20to%20authentic%20sign%0Alanguage.%20Visual%20analysis%20demonstrates%20that%20convincingly%20realistic%20deepfakes%0Acan%20be%20produced%2C%20even%20for%20new%20subjects.%20Using%20a%20pose/style%20transfer%20model%2C%20we%0Apay%20close%20attention%20to%20detail%2C%20ensuring%20hand%20movements%20are%20accurate%20and%20align%0Awith%20the%20driving%20video.%20We%20also%20apply%20machine%20learning%20algorithms%20to%20establish%0Aa%20baseline%20for%20deepfake%20detection%20on%20this%20dataset%2C%20contributing%20to%20the%0Adetection%20of%20fraudulent%20sign%20language%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01438v2&entry.124074799=Read"},
{"title": "Parametric PerceptNet: A bio-inspired deep-net trained for Image Quality\n  Assessment", "author": "Jorge Vila-Tom\u00e1s and Pablo Hern\u00e1ndez-C\u00e1mara and Valero Laparra and Jes\u00fas Malo", "abstract": "  Human vision models are at the core of image processing. For instance,\nclassical approaches to the problem of image quality are based on models that\ninclude knowledge about human vision. However, nowadays, deep learning\napproaches have obtained competitive results by simply approaching this problem\nas regression of human decisions, and training an standard network on\nhuman-rated datasets. These approaches have the advantages of being easily\nadaptable to a particular problem and they fit very efficiently when data is\navailable. However, mainly due to the excess of parameters, they have the\nproblems of lack of interpretability, and over-fitting. Here we propose a\nvision model that combines the best of both worlds by using a parametric neural\nnetwork architecture. We parameterize the layers to have bioplausible\nfunctionality, and provide a set of bioplausible parameters. We analyzed\ndifferent versions of the model and compared it with the non-parametric\nversion. The parametric models achieve a three orders of magnitude reduction in\nthe number of parameters without suffering in regression performance.\nFurthermore, we show that the parametric models behave better during training\nand are easier to interpret as vision models. Interestingly, we find that, even\ninitialized with bioplausible trained for regression using human rated\ndatasets, which we call the feature-spreading problem. This suggests that the\ndeep learning approach is inherently flawed, and emphasizes the need to\nevaluate and train models beyond regression.\n", "link": "http://arxiv.org/abs/2412.03210v2", "date": "2025-02-17", "relevancy": 2.6518, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parametric%20PerceptNet%3A%20A%20bio-inspired%20deep-net%20trained%20for%20Image%20Quality%0A%20%20Assessment&body=Title%3A%20Parametric%20PerceptNet%3A%20A%20bio-inspired%20deep-net%20trained%20for%20Image%20Quality%0A%20%20Assessment%0AAuthor%3A%20Jorge%20Vila-Tom%C3%A1s%20and%20Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Valero%20Laparra%20and%20Jes%C3%BAs%20Malo%0AAbstract%3A%20%20%20Human%20vision%20models%20are%20at%20the%20core%20of%20image%20processing.%20For%20instance%2C%0Aclassical%20approaches%20to%20the%20problem%20of%20image%20quality%20are%20based%20on%20models%20that%0Ainclude%20knowledge%20about%20human%20vision.%20However%2C%20nowadays%2C%20deep%20learning%0Aapproaches%20have%20obtained%20competitive%20results%20by%20simply%20approaching%20this%20problem%0Aas%20regression%20of%20human%20decisions%2C%20and%20training%20an%20standard%20network%20on%0Ahuman-rated%20datasets.%20These%20approaches%20have%20the%20advantages%20of%20being%20easily%0Aadaptable%20to%20a%20particular%20problem%20and%20they%20fit%20very%20efficiently%20when%20data%20is%0Aavailable.%20However%2C%20mainly%20due%20to%20the%20excess%20of%20parameters%2C%20they%20have%20the%0Aproblems%20of%20lack%20of%20interpretability%2C%20and%20over-fitting.%20Here%20we%20propose%20a%0Avision%20model%20that%20combines%20the%20best%20of%20both%20worlds%20by%20using%20a%20parametric%20neural%0Anetwork%20architecture.%20We%20parameterize%20the%20layers%20to%20have%20bioplausible%0Afunctionality%2C%20and%20provide%20a%20set%20of%20bioplausible%20parameters.%20We%20analyzed%0Adifferent%20versions%20of%20the%20model%20and%20compared%20it%20with%20the%20non-parametric%0Aversion.%20The%20parametric%20models%20achieve%20a%20three%20orders%20of%20magnitude%20reduction%20in%0Athe%20number%20of%20parameters%20without%20suffering%20in%20regression%20performance.%0AFurthermore%2C%20we%20show%20that%20the%20parametric%20models%20behave%20better%20during%20training%0Aand%20are%20easier%20to%20interpret%20as%20vision%20models.%20Interestingly%2C%20we%20find%20that%2C%20even%0Ainitialized%20with%20bioplausible%20trained%20for%20regression%20using%20human%20rated%0Adatasets%2C%20which%20we%20call%20the%20feature-spreading%20problem.%20This%20suggests%20that%20the%0Adeep%20learning%20approach%20is%20inherently%20flawed%2C%20and%20emphasizes%20the%20need%20to%0Aevaluate%20and%20train%20models%20beyond%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParametric%2520PerceptNet%253A%2520A%2520bio-inspired%2520deep-net%2520trained%2520for%2520Image%2520Quality%250A%2520%2520Assessment%26entry.906535625%3DJorge%2520Vila-Tom%25C3%25A1s%2520and%2520Pablo%2520Hern%25C3%25A1ndez-C%25C3%25A1mara%2520and%2520Valero%2520Laparra%2520and%2520Jes%25C3%25BAs%2520Malo%26entry.1292438233%3D%2520%2520Human%2520vision%2520models%2520are%2520at%2520the%2520core%2520of%2520image%2520processing.%2520For%2520instance%252C%250Aclassical%2520approaches%2520to%2520the%2520problem%2520of%2520image%2520quality%2520are%2520based%2520on%2520models%2520that%250Ainclude%2520knowledge%2520about%2520human%2520vision.%2520However%252C%2520nowadays%252C%2520deep%2520learning%250Aapproaches%2520have%2520obtained%2520competitive%2520results%2520by%2520simply%2520approaching%2520this%2520problem%250Aas%2520regression%2520of%2520human%2520decisions%252C%2520and%2520training%2520an%2520standard%2520network%2520on%250Ahuman-rated%2520datasets.%2520These%2520approaches%2520have%2520the%2520advantages%2520of%2520being%2520easily%250Aadaptable%2520to%2520a%2520particular%2520problem%2520and%2520they%2520fit%2520very%2520efficiently%2520when%2520data%2520is%250Aavailable.%2520However%252C%2520mainly%2520due%2520to%2520the%2520excess%2520of%2520parameters%252C%2520they%2520have%2520the%250Aproblems%2520of%2520lack%2520of%2520interpretability%252C%2520and%2520over-fitting.%2520Here%2520we%2520propose%2520a%250Avision%2520model%2520that%2520combines%2520the%2520best%2520of%2520both%2520worlds%2520by%2520using%2520a%2520parametric%2520neural%250Anetwork%2520architecture.%2520We%2520parameterize%2520the%2520layers%2520to%2520have%2520bioplausible%250Afunctionality%252C%2520and%2520provide%2520a%2520set%2520of%2520bioplausible%2520parameters.%2520We%2520analyzed%250Adifferent%2520versions%2520of%2520the%2520model%2520and%2520compared%2520it%2520with%2520the%2520non-parametric%250Aversion.%2520The%2520parametric%2520models%2520achieve%2520a%2520three%2520orders%2520of%2520magnitude%2520reduction%2520in%250Athe%2520number%2520of%2520parameters%2520without%2520suffering%2520in%2520regression%2520performance.%250AFurthermore%252C%2520we%2520show%2520that%2520the%2520parametric%2520models%2520behave%2520better%2520during%2520training%250Aand%2520are%2520easier%2520to%2520interpret%2520as%2520vision%2520models.%2520Interestingly%252C%2520we%2520find%2520that%252C%2520even%250Ainitialized%2520with%2520bioplausible%2520trained%2520for%2520regression%2520using%2520human%2520rated%250Adatasets%252C%2520which%2520we%2520call%2520the%2520feature-spreading%2520problem.%2520This%2520suggests%2520that%2520the%250Adeep%2520learning%2520approach%2520is%2520inherently%2520flawed%252C%2520and%2520emphasizes%2520the%2520need%2520to%250Aevaluate%2520and%2520train%2520models%2520beyond%2520regression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parametric%20PerceptNet%3A%20A%20bio-inspired%20deep-net%20trained%20for%20Image%20Quality%0A%20%20Assessment&entry.906535625=Jorge%20Vila-Tom%C3%A1s%20and%20Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Valero%20Laparra%20and%20Jes%C3%BAs%20Malo&entry.1292438233=%20%20Human%20vision%20models%20are%20at%20the%20core%20of%20image%20processing.%20For%20instance%2C%0Aclassical%20approaches%20to%20the%20problem%20of%20image%20quality%20are%20based%20on%20models%20that%0Ainclude%20knowledge%20about%20human%20vision.%20However%2C%20nowadays%2C%20deep%20learning%0Aapproaches%20have%20obtained%20competitive%20results%20by%20simply%20approaching%20this%20problem%0Aas%20regression%20of%20human%20decisions%2C%20and%20training%20an%20standard%20network%20on%0Ahuman-rated%20datasets.%20These%20approaches%20have%20the%20advantages%20of%20being%20easily%0Aadaptable%20to%20a%20particular%20problem%20and%20they%20fit%20very%20efficiently%20when%20data%20is%0Aavailable.%20However%2C%20mainly%20due%20to%20the%20excess%20of%20parameters%2C%20they%20have%20the%0Aproblems%20of%20lack%20of%20interpretability%2C%20and%20over-fitting.%20Here%20we%20propose%20a%0Avision%20model%20that%20combines%20the%20best%20of%20both%20worlds%20by%20using%20a%20parametric%20neural%0Anetwork%20architecture.%20We%20parameterize%20the%20layers%20to%20have%20bioplausible%0Afunctionality%2C%20and%20provide%20a%20set%20of%20bioplausible%20parameters.%20We%20analyzed%0Adifferent%20versions%20of%20the%20model%20and%20compared%20it%20with%20the%20non-parametric%0Aversion.%20The%20parametric%20models%20achieve%20a%20three%20orders%20of%20magnitude%20reduction%20in%0Athe%20number%20of%20parameters%20without%20suffering%20in%20regression%20performance.%0AFurthermore%2C%20we%20show%20that%20the%20parametric%20models%20behave%20better%20during%20training%0Aand%20are%20easier%20to%20interpret%20as%20vision%20models.%20Interestingly%2C%20we%20find%20that%2C%20even%0Ainitialized%20with%20bioplausible%20trained%20for%20regression%20using%20human%20rated%0Adatasets%2C%20which%20we%20call%20the%20feature-spreading%20problem.%20This%20suggests%20that%20the%0Adeep%20learning%20approach%20is%20inherently%20flawed%2C%20and%20emphasizes%20the%20need%20to%0Aevaluate%20and%20train%20models%20beyond%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03210v2&entry.124074799=Read"},
{"title": "PrototypeFormer: Learning to Explore Prototype Relationships for\n  Few-shot Image Classification", "author": "Meijuan Su and Feihong He and Fanzhang Li", "abstract": "  Few-shot image classification has received considerable attention for\novercoming the challenge of limited classification performance with limited\nsamples in novel classes. Most existing works employ sophisticated learning\nstrategies and feature learning modules to alleviate this challenge. In this\npaper, we propose a novel method called PrototypeFormer, exploring the\nrelationships among category prototypes in the few-shot scenario. Specifically,\nwe utilize a transformer architecture to build a prototype extraction module,\naiming to extract class representations that are more discriminative for\nfew-shot classification. Besides, during the model training process, we propose\na contrastive learning-based optimization approach to optimize prototype\nfeatures in few-shot learning scenarios. Despite its simplicity, our method\nperforms remarkably well, with no bells and whistles. We have experimented with\nour approach on several popular few-shot image classification benchmark\ndatasets, which shows that our method outperforms all current state-of-the-art\nmethods. In particular, our method achieves 97.07\\% and 90.88\\% on 5-way 5-shot\nand 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art\nresults with accuracy of 0.57\\% and 6.84\\%, respectively. The code will be\nreleased later.\n", "link": "http://arxiv.org/abs/2310.03517v2", "date": "2025-02-17", "relevancy": 2.6267, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.546}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrototypeFormer%3A%20Learning%20to%20Explore%20Prototype%20Relationships%20for%0A%20%20Few-shot%20Image%20Classification&body=Title%3A%20PrototypeFormer%3A%20Learning%20to%20Explore%20Prototype%20Relationships%20for%0A%20%20Few-shot%20Image%20Classification%0AAuthor%3A%20Meijuan%20Su%20and%20Feihong%20He%20and%20Fanzhang%20Li%0AAbstract%3A%20%20%20Few-shot%20image%20classification%20has%20received%20considerable%20attention%20for%0Aovercoming%20the%20challenge%20of%20limited%20classification%20performance%20with%20limited%0Asamples%20in%20novel%20classes.%20Most%20existing%20works%20employ%20sophisticated%20learning%0Astrategies%20and%20feature%20learning%20modules%20to%20alleviate%20this%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20method%20called%20PrototypeFormer%2C%20exploring%20the%0Arelationships%20among%20category%20prototypes%20in%20the%20few-shot%20scenario.%20Specifically%2C%0Awe%20utilize%20a%20transformer%20architecture%20to%20build%20a%20prototype%20extraction%20module%2C%0Aaiming%20to%20extract%20class%20representations%20that%20are%20more%20discriminative%20for%0Afew-shot%20classification.%20Besides%2C%20during%20the%20model%20training%20process%2C%20we%20propose%0Aa%20contrastive%20learning-based%20optimization%20approach%20to%20optimize%20prototype%0Afeatures%20in%20few-shot%20learning%20scenarios.%20Despite%20its%20simplicity%2C%20our%20method%0Aperforms%20remarkably%20well%2C%20with%20no%20bells%20and%20whistles.%20We%20have%20experimented%20with%0Aour%20approach%20on%20several%20popular%20few-shot%20image%20classification%20benchmark%0Adatasets%2C%20which%20shows%20that%20our%20method%20outperforms%20all%20current%20state-of-the-art%0Amethods.%20In%20particular%2C%20our%20method%20achieves%2097.07%5C%25%20and%2090.88%5C%25%20on%205-way%205-shot%0Aand%205-way%201-shot%20tasks%20of%20miniImageNet%2C%20which%20surpasses%20the%20state-of-the-art%0Aresults%20with%20accuracy%20of%200.57%5C%25%20and%206.84%5C%25%2C%20respectively.%20The%20code%20will%20be%0Areleased%20later.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03517v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototypeFormer%253A%2520Learning%2520to%2520Explore%2520Prototype%2520Relationships%2520for%250A%2520%2520Few-shot%2520Image%2520Classification%26entry.906535625%3DMeijuan%2520Su%2520and%2520Feihong%2520He%2520and%2520Fanzhang%2520Li%26entry.1292438233%3D%2520%2520Few-shot%2520image%2520classification%2520has%2520received%2520considerable%2520attention%2520for%250Aovercoming%2520the%2520challenge%2520of%2520limited%2520classification%2520performance%2520with%2520limited%250Asamples%2520in%2520novel%2520classes.%2520Most%2520existing%2520works%2520employ%2520sophisticated%2520learning%250Astrategies%2520and%2520feature%2520learning%2520modules%2520to%2520alleviate%2520this%2520challenge.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520method%2520called%2520PrototypeFormer%252C%2520exploring%2520the%250Arelationships%2520among%2520category%2520prototypes%2520in%2520the%2520few-shot%2520scenario.%2520Specifically%252C%250Awe%2520utilize%2520a%2520transformer%2520architecture%2520to%2520build%2520a%2520prototype%2520extraction%2520module%252C%250Aaiming%2520to%2520extract%2520class%2520representations%2520that%2520are%2520more%2520discriminative%2520for%250Afew-shot%2520classification.%2520Besides%252C%2520during%2520the%2520model%2520training%2520process%252C%2520we%2520propose%250Aa%2520contrastive%2520learning-based%2520optimization%2520approach%2520to%2520optimize%2520prototype%250Afeatures%2520in%2520few-shot%2520learning%2520scenarios.%2520Despite%2520its%2520simplicity%252C%2520our%2520method%250Aperforms%2520remarkably%2520well%252C%2520with%2520no%2520bells%2520and%2520whistles.%2520We%2520have%2520experimented%2520with%250Aour%2520approach%2520on%2520several%2520popular%2520few-shot%2520image%2520classification%2520benchmark%250Adatasets%252C%2520which%2520shows%2520that%2520our%2520method%2520outperforms%2520all%2520current%2520state-of-the-art%250Amethods.%2520In%2520particular%252C%2520our%2520method%2520achieves%252097.07%255C%2525%2520and%252090.88%255C%2525%2520on%25205-way%25205-shot%250Aand%25205-way%25201-shot%2520tasks%2520of%2520miniImageNet%252C%2520which%2520surpasses%2520the%2520state-of-the-art%250Aresults%2520with%2520accuracy%2520of%25200.57%255C%2525%2520and%25206.84%255C%2525%252C%2520respectively.%2520The%2520code%2520will%2520be%250Areleased%2520later.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03517v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrototypeFormer%3A%20Learning%20to%20Explore%20Prototype%20Relationships%20for%0A%20%20Few-shot%20Image%20Classification&entry.906535625=Meijuan%20Su%20and%20Feihong%20He%20and%20Fanzhang%20Li&entry.1292438233=%20%20Few-shot%20image%20classification%20has%20received%20considerable%20attention%20for%0Aovercoming%20the%20challenge%20of%20limited%20classification%20performance%20with%20limited%0Asamples%20in%20novel%20classes.%20Most%20existing%20works%20employ%20sophisticated%20learning%0Astrategies%20and%20feature%20learning%20modules%20to%20alleviate%20this%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20method%20called%20PrototypeFormer%2C%20exploring%20the%0Arelationships%20among%20category%20prototypes%20in%20the%20few-shot%20scenario.%20Specifically%2C%0Awe%20utilize%20a%20transformer%20architecture%20to%20build%20a%20prototype%20extraction%20module%2C%0Aaiming%20to%20extract%20class%20representations%20that%20are%20more%20discriminative%20for%0Afew-shot%20classification.%20Besides%2C%20during%20the%20model%20training%20process%2C%20we%20propose%0Aa%20contrastive%20learning-based%20optimization%20approach%20to%20optimize%20prototype%0Afeatures%20in%20few-shot%20learning%20scenarios.%20Despite%20its%20simplicity%2C%20our%20method%0Aperforms%20remarkably%20well%2C%20with%20no%20bells%20and%20whistles.%20We%20have%20experimented%20with%0Aour%20approach%20on%20several%20popular%20few-shot%20image%20classification%20benchmark%0Adatasets%2C%20which%20shows%20that%20our%20method%20outperforms%20all%20current%20state-of-the-art%0Amethods.%20In%20particular%2C%20our%20method%20achieves%2097.07%5C%25%20and%2090.88%5C%25%20on%205-way%205-shot%0Aand%205-way%201-shot%20tasks%20of%20miniImageNet%2C%20which%20surpasses%20the%20state-of-the-art%0Aresults%20with%20accuracy%20of%200.57%5C%25%20and%206.84%5C%25%2C%20respectively.%20The%20code%20will%20be%0Areleased%20later.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03517v2&entry.124074799=Read"},
{"title": "Step-Audio: Unified Understanding and Generation in Intelligent Speech\n  Interaction", "author": "Ailin Huang and Boyong Wu and Bruce Wang and Chao Yan and Chen Hu and Chengli Feng and Fei Tian and Feiyu Shen and Jingbei Li and Mingrui Chen and Peng Liu and Ruihang Miao and Wang You and Xi Chen and Xuerui Yang and Yechang Huang and Yuxiang Zhang and Zheng Gong and Zixin Zhang and Brian Li and Changyi Wan and Hanpeng Hu and Ranchen Ming and Song Yuan and Xuelin Zhang and Yu Zhou and Bingxin Li and Buyun Ma and Kang An and Wei Ji and Wen Li and Xuan Wen and Yuankai Ma and Yuanwei Liang and Yun Mou and Bahtiyar Ahmidi and Bin Wang and Bo Li and Changxin Miao and Chen Xu and Chengting Feng and Chenrun Wang and Dapeng Shi and Deshan Sun and Dingyuan Hu and Dula Sai and Enle Liu and Guanzhe Huang and Gulin Yan and Heng Wang and Haonan Jia and Haoyang Zhang and Jiahao Gong and Jianchang Wu and Jiahong Liu and Jianjian Sun and Jiangjie Zhen and Jie Feng and Jie Wu and Jiaoren Wu and Jie Yang and Jinguo Wang and Jingyang Zhang and Junzhe Lin and Kaixiang Li and Lei Xia and Li Zhou and Longlong Gu and Mei Chen and Menglin Wu and Ming Li and Mingxiao Li and Mingyao Liang and Na Wang and Nie Hao and Qiling Wu and Qinyuan Tan and Shaoliang Pang and Shiliang Yang and Shuli Gao and Siqi Liu and Sitong Liu and Tiancheng Cao and Tianyu Wang and Wenjin Deng and Wenqing He and Wen Sun and Xin Han and Xiaomin Deng and Xiaojia Liu and Xu Zhao and Yanan Wei and Yanbo Yu and Yang Cao and Yangguang Li and Yangzhen Ma and Yanming Xu and Yaqiang Shi and Yilei Wang and Yinmin Zhong and Yu Luo and Yuanwei Lu and Yuhe Yin and Yuting Yan and Yuxiang Yang and Zhe Xie and Zheng Ge and Zheng Sun and Zhewei Huang and Zhichao Chang and Zidong Yang and Zili Zhang and Binxing Jiao and Daxin Jiang and Heung-Yeung Shum and Jiansheng Chen and Jing Li and Shuchang Zhou and Xiangyu Zhang and Xinhao Zhang and Yibo Zhu", "abstract": "  Real-time speech interaction, serving as a fundamental interface for\nhuman-machine collaboration, holds immense potential. However, current\nopen-source models face limitations such as high costs in voice data\ncollection, weakness in dynamic control, and limited intelligence. To address\nthese challenges, this paper introduces Step-Audio, the first production-ready\nopen-source solution. Key contributions include: 1) a 130B-parameter unified\nspeech-text multi-modal model that achieves unified understanding and\ngeneration, with the Step-Audio-Chat version open-sourced; 2) a generative\nspeech data engine that establishes an affordable voice cloning framework and\nproduces the open-sourced lightweight Step-Audio-TTS-3B model through\ndistillation; 3) an instruction-driven fine control system enabling dynamic\nadjustments across dialects, emotions, singing, and RAP; 4) an enhanced\ncognitive architecture augmented with tool calling and role-playing abilities\nto manage complex tasks effectively. Based on our new StepEval-Audio-360\nevaluation benchmark, Step-Audio achieves state-of-the-art performance in human\nevaluations, especially in terms of instruction following. On open-source\nbenchmarks like LLaMA Question, shows 9.3% average performance improvement,\ndemonstrating our commitment to advancing the development of open-source\nmulti-modal language technologies. Our code and models are available at\nhttps://github.com/stepfun-ai/Step-Audio.\n", "link": "http://arxiv.org/abs/2502.11946v1", "date": "2025-02-17", "relevancy": 2.6211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-Audio%3A%20Unified%20Understanding%20and%20Generation%20in%20Intelligent%20Speech%0A%20%20Interaction&body=Title%3A%20Step-Audio%3A%20Unified%20Understanding%20and%20Generation%20in%20Intelligent%20Speech%0A%20%20Interaction%0AAuthor%3A%20Ailin%20Huang%20and%20Boyong%20Wu%20and%20Bruce%20Wang%20and%20Chao%20Yan%20and%20Chen%20Hu%20and%20Chengli%20Feng%20and%20Fei%20Tian%20and%20Feiyu%20Shen%20and%20Jingbei%20Li%20and%20Mingrui%20Chen%20and%20Peng%20Liu%20and%20Ruihang%20Miao%20and%20Wang%20You%20and%20Xi%20Chen%20and%20Xuerui%20Yang%20and%20Yechang%20Huang%20and%20Yuxiang%20Zhang%20and%20Zheng%20Gong%20and%20Zixin%20Zhang%20and%20Brian%20Li%20and%20Changyi%20Wan%20and%20Hanpeng%20Hu%20and%20Ranchen%20Ming%20and%20Song%20Yuan%20and%20Xuelin%20Zhang%20and%20Yu%20Zhou%20and%20Bingxin%20Li%20and%20Buyun%20Ma%20and%20Kang%20An%20and%20Wei%20Ji%20and%20Wen%20Li%20and%20Xuan%20Wen%20and%20Yuankai%20Ma%20and%20Yuanwei%20Liang%20and%20Yun%20Mou%20and%20Bahtiyar%20Ahmidi%20and%20Bin%20Wang%20and%20Bo%20Li%20and%20Changxin%20Miao%20and%20Chen%20Xu%20and%20Chengting%20Feng%20and%20Chenrun%20Wang%20and%20Dapeng%20Shi%20and%20Deshan%20Sun%20and%20Dingyuan%20Hu%20and%20Dula%20Sai%20and%20Enle%20Liu%20and%20Guanzhe%20Huang%20and%20Gulin%20Yan%20and%20Heng%20Wang%20and%20Haonan%20Jia%20and%20Haoyang%20Zhang%20and%20Jiahao%20Gong%20and%20Jianchang%20Wu%20and%20Jiahong%20Liu%20and%20Jianjian%20Sun%20and%20Jiangjie%20Zhen%20and%20Jie%20Feng%20and%20Jie%20Wu%20and%20Jiaoren%20Wu%20and%20Jie%20Yang%20and%20Jinguo%20Wang%20and%20Jingyang%20Zhang%20and%20Junzhe%20Lin%20and%20Kaixiang%20Li%20and%20Lei%20Xia%20and%20Li%20Zhou%20and%20Longlong%20Gu%20and%20Mei%20Chen%20and%20Menglin%20Wu%20and%20Ming%20Li%20and%20Mingxiao%20Li%20and%20Mingyao%20Liang%20and%20Na%20Wang%20and%20Nie%20Hao%20and%20Qiling%20Wu%20and%20Qinyuan%20Tan%20and%20Shaoliang%20Pang%20and%20Shiliang%20Yang%20and%20Shuli%20Gao%20and%20Siqi%20Liu%20and%20Sitong%20Liu%20and%20Tiancheng%20Cao%20and%20Tianyu%20Wang%20and%20Wenjin%20Deng%20and%20Wenqing%20He%20and%20Wen%20Sun%20and%20Xin%20Han%20and%20Xiaomin%20Deng%20and%20Xiaojia%20Liu%20and%20Xu%20Zhao%20and%20Yanan%20Wei%20and%20Yanbo%20Yu%20and%20Yang%20Cao%20and%20Yangguang%20Li%20and%20Yangzhen%20Ma%20and%20Yanming%20Xu%20and%20Yaqiang%20Shi%20and%20Yilei%20Wang%20and%20Yinmin%20Zhong%20and%20Yu%20Luo%20and%20Yuanwei%20Lu%20and%20Yuhe%20Yin%20and%20Yuting%20Yan%20and%20Yuxiang%20Yang%20and%20Zhe%20Xie%20and%20Zheng%20Ge%20and%20Zheng%20Sun%20and%20Zhewei%20Huang%20and%20Zhichao%20Chang%20and%20Zidong%20Yang%20and%20Zili%20Zhang%20and%20Binxing%20Jiao%20and%20Daxin%20Jiang%20and%20Heung-Yeung%20Shum%20and%20Jiansheng%20Chen%20and%20Jing%20Li%20and%20Shuchang%20Zhou%20and%20Xiangyu%20Zhang%20and%20Xinhao%20Zhang%20and%20Yibo%20Zhu%0AAbstract%3A%20%20%20Real-time%20speech%20interaction%2C%20serving%20as%20a%20fundamental%20interface%20for%0Ahuman-machine%20collaboration%2C%20holds%20immense%20potential.%20However%2C%20current%0Aopen-source%20models%20face%20limitations%20such%20as%20high%20costs%20in%20voice%20data%0Acollection%2C%20weakness%20in%20dynamic%20control%2C%20and%20limited%20intelligence.%20To%20address%0Athese%20challenges%2C%20this%20paper%20introduces%20Step-Audio%2C%20the%20first%20production-ready%0Aopen-source%20solution.%20Key%20contributions%20include%3A%201%29%20a%20130B-parameter%20unified%0Aspeech-text%20multi-modal%20model%20that%20achieves%20unified%20understanding%20and%0Ageneration%2C%20with%20the%20Step-Audio-Chat%20version%20open-sourced%3B%202%29%20a%20generative%0Aspeech%20data%20engine%20that%20establishes%20an%20affordable%20voice%20cloning%20framework%20and%0Aproduces%20the%20open-sourced%20lightweight%20Step-Audio-TTS-3B%20model%20through%0Adistillation%3B%203%29%20an%20instruction-driven%20fine%20control%20system%20enabling%20dynamic%0Aadjustments%20across%20dialects%2C%20emotions%2C%20singing%2C%20and%20RAP%3B%204%29%20an%20enhanced%0Acognitive%20architecture%20augmented%20with%20tool%20calling%20and%20role-playing%20abilities%0Ato%20manage%20complex%20tasks%20effectively.%20Based%20on%20our%20new%20StepEval-Audio-360%0Aevaluation%20benchmark%2C%20Step-Audio%20achieves%20state-of-the-art%20performance%20in%20human%0Aevaluations%2C%20especially%20in%20terms%20of%20instruction%20following.%20On%20open-source%0Abenchmarks%20like%20LLaMA%20Question%2C%20shows%209.3%25%20average%20performance%20improvement%2C%0Ademonstrating%20our%20commitment%20to%20advancing%20the%20development%20of%20open-source%0Amulti-modal%20language%20technologies.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/stepfun-ai/Step-Audio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-Audio%253A%2520Unified%2520Understanding%2520and%2520Generation%2520in%2520Intelligent%2520Speech%250A%2520%2520Interaction%26entry.906535625%3DAilin%2520Huang%2520and%2520Boyong%2520Wu%2520and%2520Bruce%2520Wang%2520and%2520Chao%2520Yan%2520and%2520Chen%2520Hu%2520and%2520Chengli%2520Feng%2520and%2520Fei%2520Tian%2520and%2520Feiyu%2520Shen%2520and%2520Jingbei%2520Li%2520and%2520Mingrui%2520Chen%2520and%2520Peng%2520Liu%2520and%2520Ruihang%2520Miao%2520and%2520Wang%2520You%2520and%2520Xi%2520Chen%2520and%2520Xuerui%2520Yang%2520and%2520Yechang%2520Huang%2520and%2520Yuxiang%2520Zhang%2520and%2520Zheng%2520Gong%2520and%2520Zixin%2520Zhang%2520and%2520Brian%2520Li%2520and%2520Changyi%2520Wan%2520and%2520Hanpeng%2520Hu%2520and%2520Ranchen%2520Ming%2520and%2520Song%2520Yuan%2520and%2520Xuelin%2520Zhang%2520and%2520Yu%2520Zhou%2520and%2520Bingxin%2520Li%2520and%2520Buyun%2520Ma%2520and%2520Kang%2520An%2520and%2520Wei%2520Ji%2520and%2520Wen%2520Li%2520and%2520Xuan%2520Wen%2520and%2520Yuankai%2520Ma%2520and%2520Yuanwei%2520Liang%2520and%2520Yun%2520Mou%2520and%2520Bahtiyar%2520Ahmidi%2520and%2520Bin%2520Wang%2520and%2520Bo%2520Li%2520and%2520Changxin%2520Miao%2520and%2520Chen%2520Xu%2520and%2520Chengting%2520Feng%2520and%2520Chenrun%2520Wang%2520and%2520Dapeng%2520Shi%2520and%2520Deshan%2520Sun%2520and%2520Dingyuan%2520Hu%2520and%2520Dula%2520Sai%2520and%2520Enle%2520Liu%2520and%2520Guanzhe%2520Huang%2520and%2520Gulin%2520Yan%2520and%2520Heng%2520Wang%2520and%2520Haonan%2520Jia%2520and%2520Haoyang%2520Zhang%2520and%2520Jiahao%2520Gong%2520and%2520Jianchang%2520Wu%2520and%2520Jiahong%2520Liu%2520and%2520Jianjian%2520Sun%2520and%2520Jiangjie%2520Zhen%2520and%2520Jie%2520Feng%2520and%2520Jie%2520Wu%2520and%2520Jiaoren%2520Wu%2520and%2520Jie%2520Yang%2520and%2520Jinguo%2520Wang%2520and%2520Jingyang%2520Zhang%2520and%2520Junzhe%2520Lin%2520and%2520Kaixiang%2520Li%2520and%2520Lei%2520Xia%2520and%2520Li%2520Zhou%2520and%2520Longlong%2520Gu%2520and%2520Mei%2520Chen%2520and%2520Menglin%2520Wu%2520and%2520Ming%2520Li%2520and%2520Mingxiao%2520Li%2520and%2520Mingyao%2520Liang%2520and%2520Na%2520Wang%2520and%2520Nie%2520Hao%2520and%2520Qiling%2520Wu%2520and%2520Qinyuan%2520Tan%2520and%2520Shaoliang%2520Pang%2520and%2520Shiliang%2520Yang%2520and%2520Shuli%2520Gao%2520and%2520Siqi%2520Liu%2520and%2520Sitong%2520Liu%2520and%2520Tiancheng%2520Cao%2520and%2520Tianyu%2520Wang%2520and%2520Wenjin%2520Deng%2520and%2520Wenqing%2520He%2520and%2520Wen%2520Sun%2520and%2520Xin%2520Han%2520and%2520Xiaomin%2520Deng%2520and%2520Xiaojia%2520Liu%2520and%2520Xu%2520Zhao%2520and%2520Yanan%2520Wei%2520and%2520Yanbo%2520Yu%2520and%2520Yang%2520Cao%2520and%2520Yangguang%2520Li%2520and%2520Yangzhen%2520Ma%2520and%2520Yanming%2520Xu%2520and%2520Yaqiang%2520Shi%2520and%2520Yilei%2520Wang%2520and%2520Yinmin%2520Zhong%2520and%2520Yu%2520Luo%2520and%2520Yuanwei%2520Lu%2520and%2520Yuhe%2520Yin%2520and%2520Yuting%2520Yan%2520and%2520Yuxiang%2520Yang%2520and%2520Zhe%2520Xie%2520and%2520Zheng%2520Ge%2520and%2520Zheng%2520Sun%2520and%2520Zhewei%2520Huang%2520and%2520Zhichao%2520Chang%2520and%2520Zidong%2520Yang%2520and%2520Zili%2520Zhang%2520and%2520Binxing%2520Jiao%2520and%2520Daxin%2520Jiang%2520and%2520Heung-Yeung%2520Shum%2520and%2520Jiansheng%2520Chen%2520and%2520Jing%2520Li%2520and%2520Shuchang%2520Zhou%2520and%2520Xiangyu%2520Zhang%2520and%2520Xinhao%2520Zhang%2520and%2520Yibo%2520Zhu%26entry.1292438233%3D%2520%2520Real-time%2520speech%2520interaction%252C%2520serving%2520as%2520a%2520fundamental%2520interface%2520for%250Ahuman-machine%2520collaboration%252C%2520holds%2520immense%2520potential.%2520However%252C%2520current%250Aopen-source%2520models%2520face%2520limitations%2520such%2520as%2520high%2520costs%2520in%2520voice%2520data%250Acollection%252C%2520weakness%2520in%2520dynamic%2520control%252C%2520and%2520limited%2520intelligence.%2520To%2520address%250Athese%2520challenges%252C%2520this%2520paper%2520introduces%2520Step-Audio%252C%2520the%2520first%2520production-ready%250Aopen-source%2520solution.%2520Key%2520contributions%2520include%253A%25201%2529%2520a%2520130B-parameter%2520unified%250Aspeech-text%2520multi-modal%2520model%2520that%2520achieves%2520unified%2520understanding%2520and%250Ageneration%252C%2520with%2520the%2520Step-Audio-Chat%2520version%2520open-sourced%253B%25202%2529%2520a%2520generative%250Aspeech%2520data%2520engine%2520that%2520establishes%2520an%2520affordable%2520voice%2520cloning%2520framework%2520and%250Aproduces%2520the%2520open-sourced%2520lightweight%2520Step-Audio-TTS-3B%2520model%2520through%250Adistillation%253B%25203%2529%2520an%2520instruction-driven%2520fine%2520control%2520system%2520enabling%2520dynamic%250Aadjustments%2520across%2520dialects%252C%2520emotions%252C%2520singing%252C%2520and%2520RAP%253B%25204%2529%2520an%2520enhanced%250Acognitive%2520architecture%2520augmented%2520with%2520tool%2520calling%2520and%2520role-playing%2520abilities%250Ato%2520manage%2520complex%2520tasks%2520effectively.%2520Based%2520on%2520our%2520new%2520StepEval-Audio-360%250Aevaluation%2520benchmark%252C%2520Step-Audio%2520achieves%2520state-of-the-art%2520performance%2520in%2520human%250Aevaluations%252C%2520especially%2520in%2520terms%2520of%2520instruction%2520following.%2520On%2520open-source%250Abenchmarks%2520like%2520LLaMA%2520Question%252C%2520shows%25209.3%2525%2520average%2520performance%2520improvement%252C%250Ademonstrating%2520our%2520commitment%2520to%2520advancing%2520the%2520development%2520of%2520open-source%250Amulti-modal%2520language%2520technologies.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/stepfun-ai/Step-Audio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-Audio%3A%20Unified%20Understanding%20and%20Generation%20in%20Intelligent%20Speech%0A%20%20Interaction&entry.906535625=Ailin%20Huang%20and%20Boyong%20Wu%20and%20Bruce%20Wang%20and%20Chao%20Yan%20and%20Chen%20Hu%20and%20Chengli%20Feng%20and%20Fei%20Tian%20and%20Feiyu%20Shen%20and%20Jingbei%20Li%20and%20Mingrui%20Chen%20and%20Peng%20Liu%20and%20Ruihang%20Miao%20and%20Wang%20You%20and%20Xi%20Chen%20and%20Xuerui%20Yang%20and%20Yechang%20Huang%20and%20Yuxiang%20Zhang%20and%20Zheng%20Gong%20and%20Zixin%20Zhang%20and%20Brian%20Li%20and%20Changyi%20Wan%20and%20Hanpeng%20Hu%20and%20Ranchen%20Ming%20and%20Song%20Yuan%20and%20Xuelin%20Zhang%20and%20Yu%20Zhou%20and%20Bingxin%20Li%20and%20Buyun%20Ma%20and%20Kang%20An%20and%20Wei%20Ji%20and%20Wen%20Li%20and%20Xuan%20Wen%20and%20Yuankai%20Ma%20and%20Yuanwei%20Liang%20and%20Yun%20Mou%20and%20Bahtiyar%20Ahmidi%20and%20Bin%20Wang%20and%20Bo%20Li%20and%20Changxin%20Miao%20and%20Chen%20Xu%20and%20Chengting%20Feng%20and%20Chenrun%20Wang%20and%20Dapeng%20Shi%20and%20Deshan%20Sun%20and%20Dingyuan%20Hu%20and%20Dula%20Sai%20and%20Enle%20Liu%20and%20Guanzhe%20Huang%20and%20Gulin%20Yan%20and%20Heng%20Wang%20and%20Haonan%20Jia%20and%20Haoyang%20Zhang%20and%20Jiahao%20Gong%20and%20Jianchang%20Wu%20and%20Jiahong%20Liu%20and%20Jianjian%20Sun%20and%20Jiangjie%20Zhen%20and%20Jie%20Feng%20and%20Jie%20Wu%20and%20Jiaoren%20Wu%20and%20Jie%20Yang%20and%20Jinguo%20Wang%20and%20Jingyang%20Zhang%20and%20Junzhe%20Lin%20and%20Kaixiang%20Li%20and%20Lei%20Xia%20and%20Li%20Zhou%20and%20Longlong%20Gu%20and%20Mei%20Chen%20and%20Menglin%20Wu%20and%20Ming%20Li%20and%20Mingxiao%20Li%20and%20Mingyao%20Liang%20and%20Na%20Wang%20and%20Nie%20Hao%20and%20Qiling%20Wu%20and%20Qinyuan%20Tan%20and%20Shaoliang%20Pang%20and%20Shiliang%20Yang%20and%20Shuli%20Gao%20and%20Siqi%20Liu%20and%20Sitong%20Liu%20and%20Tiancheng%20Cao%20and%20Tianyu%20Wang%20and%20Wenjin%20Deng%20and%20Wenqing%20He%20and%20Wen%20Sun%20and%20Xin%20Han%20and%20Xiaomin%20Deng%20and%20Xiaojia%20Liu%20and%20Xu%20Zhao%20and%20Yanan%20Wei%20and%20Yanbo%20Yu%20and%20Yang%20Cao%20and%20Yangguang%20Li%20and%20Yangzhen%20Ma%20and%20Yanming%20Xu%20and%20Yaqiang%20Shi%20and%20Yilei%20Wang%20and%20Yinmin%20Zhong%20and%20Yu%20Luo%20and%20Yuanwei%20Lu%20and%20Yuhe%20Yin%20and%20Yuting%20Yan%20and%20Yuxiang%20Yang%20and%20Zhe%20Xie%20and%20Zheng%20Ge%20and%20Zheng%20Sun%20and%20Zhewei%20Huang%20and%20Zhichao%20Chang%20and%20Zidong%20Yang%20and%20Zili%20Zhang%20and%20Binxing%20Jiao%20and%20Daxin%20Jiang%20and%20Heung-Yeung%20Shum%20and%20Jiansheng%20Chen%20and%20Jing%20Li%20and%20Shuchang%20Zhou%20and%20Xiangyu%20Zhang%20and%20Xinhao%20Zhang%20and%20Yibo%20Zhu&entry.1292438233=%20%20Real-time%20speech%20interaction%2C%20serving%20as%20a%20fundamental%20interface%20for%0Ahuman-machine%20collaboration%2C%20holds%20immense%20potential.%20However%2C%20current%0Aopen-source%20models%20face%20limitations%20such%20as%20high%20costs%20in%20voice%20data%0Acollection%2C%20weakness%20in%20dynamic%20control%2C%20and%20limited%20intelligence.%20To%20address%0Athese%20challenges%2C%20this%20paper%20introduces%20Step-Audio%2C%20the%20first%20production-ready%0Aopen-source%20solution.%20Key%20contributions%20include%3A%201%29%20a%20130B-parameter%20unified%0Aspeech-text%20multi-modal%20model%20that%20achieves%20unified%20understanding%20and%0Ageneration%2C%20with%20the%20Step-Audio-Chat%20version%20open-sourced%3B%202%29%20a%20generative%0Aspeech%20data%20engine%20that%20establishes%20an%20affordable%20voice%20cloning%20framework%20and%0Aproduces%20the%20open-sourced%20lightweight%20Step-Audio-TTS-3B%20model%20through%0Adistillation%3B%203%29%20an%20instruction-driven%20fine%20control%20system%20enabling%20dynamic%0Aadjustments%20across%20dialects%2C%20emotions%2C%20singing%2C%20and%20RAP%3B%204%29%20an%20enhanced%0Acognitive%20architecture%20augmented%20with%20tool%20calling%20and%20role-playing%20abilities%0Ato%20manage%20complex%20tasks%20effectively.%20Based%20on%20our%20new%20StepEval-Audio-360%0Aevaluation%20benchmark%2C%20Step-Audio%20achieves%20state-of-the-art%20performance%20in%20human%0Aevaluations%2C%20especially%20in%20terms%20of%20instruction%20following.%20On%20open-source%0Abenchmarks%20like%20LLaMA%20Question%2C%20shows%209.3%25%20average%20performance%20improvement%2C%0Ademonstrating%20our%20commitment%20to%20advancing%20the%20development%20of%20open-source%0Amulti-modal%20language%20technologies.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/stepfun-ai/Step-Audio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11946v1&entry.124074799=Read"},
{"title": "Robotic CBCT Meets Robotic Ultrasound", "author": "Feng Li and Yuan Bi and Dianye Huang and Zhongliang Jiang and Nassir Navab", "abstract": "  The multi-modality imaging system offers optimal fused images for safe and\nprecise interventions in modern clinical practices, such as computed tomography\n- ultrasound (CT-US) guidance for needle insertion. However, the limited\ndexterity and mobility of current imaging devices hinder their integration into\nstandardized workflows and the advancement toward fully autonomous intervention\nsystems. In this paper, we present a novel clinical setup where robotic cone\nbeam computed tomography (CBCT) and robotic US are pre-calibrated and\ndynamically co-registered, enabling new clinical applications. This setup\nallows registration-free rigid registration, facilitating multi-modal guided\nprocedures in the absence of tissue deformation. First, a one-time\npre-calibration is performed between the systems. To ensure a safe insertion\npath by highlighting critical vasculature on the 3D CBCT, SAM2 segments vessels\nfrom B-mode images, using the Doppler signal as an autonomously generated\nprompt. Based on the registration, the Doppler image or segmented vessel masks\nare then mapped onto the CBCT, creating an optimally fused image with\ncomprehensive detail. To validate the system, we used a specially designed\nphantom, featuring lesions covered by ribs and multiple vessels with simulated\nmoving flow. The mapping error between US and CBCT resulted in an average\ndeviation of 1.72+-0.62 mm. A user study demonstrated the effectiveness of\nCBCT-US fusion for needle insertion guidance, showing significant improvements\nin time efficiency, accuracy, and success rate. Needle intervention performance\nimproved by approximately 50% compared to the conventional US-guided workflow.\nWe present the first robotic dual-modality imaging system designed to guide\nclinical applications. The results show significant performance improvements\ncompared to traditional manual interventions.\n", "link": "http://arxiv.org/abs/2502.12019v1", "date": "2025-02-17", "relevancy": 2.6141, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5361}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5192}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20CBCT%20Meets%20Robotic%20Ultrasound&body=Title%3A%20Robotic%20CBCT%20Meets%20Robotic%20Ultrasound%0AAuthor%3A%20Feng%20Li%20and%20Yuan%20Bi%20and%20Dianye%20Huang%20and%20Zhongliang%20Jiang%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20The%20multi-modality%20imaging%20system%20offers%20optimal%20fused%20images%20for%20safe%20and%0Aprecise%20interventions%20in%20modern%20clinical%20practices%2C%20such%20as%20computed%20tomography%0A-%20ultrasound%20%28CT-US%29%20guidance%20for%20needle%20insertion.%20However%2C%20the%20limited%0Adexterity%20and%20mobility%20of%20current%20imaging%20devices%20hinder%20their%20integration%20into%0Astandardized%20workflows%20and%20the%20advancement%20toward%20fully%20autonomous%20intervention%0Asystems.%20In%20this%20paper%2C%20we%20present%20a%20novel%20clinical%20setup%20where%20robotic%20cone%0Abeam%20computed%20tomography%20%28CBCT%29%20and%20robotic%20US%20are%20pre-calibrated%20and%0Adynamically%20co-registered%2C%20enabling%20new%20clinical%20applications.%20This%20setup%0Aallows%20registration-free%20rigid%20registration%2C%20facilitating%20multi-modal%20guided%0Aprocedures%20in%20the%20absence%20of%20tissue%20deformation.%20First%2C%20a%20one-time%0Apre-calibration%20is%20performed%20between%20the%20systems.%20To%20ensure%20a%20safe%20insertion%0Apath%20by%20highlighting%20critical%20vasculature%20on%20the%203D%20CBCT%2C%20SAM2%20segments%20vessels%0Afrom%20B-mode%20images%2C%20using%20the%20Doppler%20signal%20as%20an%20autonomously%20generated%0Aprompt.%20Based%20on%20the%20registration%2C%20the%20Doppler%20image%20or%20segmented%20vessel%20masks%0Aare%20then%20mapped%20onto%20the%20CBCT%2C%20creating%20an%20optimally%20fused%20image%20with%0Acomprehensive%20detail.%20To%20validate%20the%20system%2C%20we%20used%20a%20specially%20designed%0Aphantom%2C%20featuring%20lesions%20covered%20by%20ribs%20and%20multiple%20vessels%20with%20simulated%0Amoving%20flow.%20The%20mapping%20error%20between%20US%20and%20CBCT%20resulted%20in%20an%20average%0Adeviation%20of%201.72%2B-0.62%20mm.%20A%20user%20study%20demonstrated%20the%20effectiveness%20of%0ACBCT-US%20fusion%20for%20needle%20insertion%20guidance%2C%20showing%20significant%20improvements%0Ain%20time%20efficiency%2C%20accuracy%2C%20and%20success%20rate.%20Needle%20intervention%20performance%0Aimproved%20by%20approximately%2050%25%20compared%20to%20the%20conventional%20US-guided%20workflow.%0AWe%20present%20the%20first%20robotic%20dual-modality%20imaging%20system%20designed%20to%20guide%0Aclinical%20applications.%20The%20results%20show%20significant%20performance%20improvements%0Acompared%20to%20traditional%20manual%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520CBCT%2520Meets%2520Robotic%2520Ultrasound%26entry.906535625%3DFeng%2520Li%2520and%2520Yuan%2520Bi%2520and%2520Dianye%2520Huang%2520and%2520Zhongliang%2520Jiang%2520and%2520Nassir%2520Navab%26entry.1292438233%3D%2520%2520The%2520multi-modality%2520imaging%2520system%2520offers%2520optimal%2520fused%2520images%2520for%2520safe%2520and%250Aprecise%2520interventions%2520in%2520modern%2520clinical%2520practices%252C%2520such%2520as%2520computed%2520tomography%250A-%2520ultrasound%2520%2528CT-US%2529%2520guidance%2520for%2520needle%2520insertion.%2520However%252C%2520the%2520limited%250Adexterity%2520and%2520mobility%2520of%2520current%2520imaging%2520devices%2520hinder%2520their%2520integration%2520into%250Astandardized%2520workflows%2520and%2520the%2520advancement%2520toward%2520fully%2520autonomous%2520intervention%250Asystems.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520clinical%2520setup%2520where%2520robotic%2520cone%250Abeam%2520computed%2520tomography%2520%2528CBCT%2529%2520and%2520robotic%2520US%2520are%2520pre-calibrated%2520and%250Adynamically%2520co-registered%252C%2520enabling%2520new%2520clinical%2520applications.%2520This%2520setup%250Aallows%2520registration-free%2520rigid%2520registration%252C%2520facilitating%2520multi-modal%2520guided%250Aprocedures%2520in%2520the%2520absence%2520of%2520tissue%2520deformation.%2520First%252C%2520a%2520one-time%250Apre-calibration%2520is%2520performed%2520between%2520the%2520systems.%2520To%2520ensure%2520a%2520safe%2520insertion%250Apath%2520by%2520highlighting%2520critical%2520vasculature%2520on%2520the%25203D%2520CBCT%252C%2520SAM2%2520segments%2520vessels%250Afrom%2520B-mode%2520images%252C%2520using%2520the%2520Doppler%2520signal%2520as%2520an%2520autonomously%2520generated%250Aprompt.%2520Based%2520on%2520the%2520registration%252C%2520the%2520Doppler%2520image%2520or%2520segmented%2520vessel%2520masks%250Aare%2520then%2520mapped%2520onto%2520the%2520CBCT%252C%2520creating%2520an%2520optimally%2520fused%2520image%2520with%250Acomprehensive%2520detail.%2520To%2520validate%2520the%2520system%252C%2520we%2520used%2520a%2520specially%2520designed%250Aphantom%252C%2520featuring%2520lesions%2520covered%2520by%2520ribs%2520and%2520multiple%2520vessels%2520with%2520simulated%250Amoving%2520flow.%2520The%2520mapping%2520error%2520between%2520US%2520and%2520CBCT%2520resulted%2520in%2520an%2520average%250Adeviation%2520of%25201.72%252B-0.62%2520mm.%2520A%2520user%2520study%2520demonstrated%2520the%2520effectiveness%2520of%250ACBCT-US%2520fusion%2520for%2520needle%2520insertion%2520guidance%252C%2520showing%2520significant%2520improvements%250Ain%2520time%2520efficiency%252C%2520accuracy%252C%2520and%2520success%2520rate.%2520Needle%2520intervention%2520performance%250Aimproved%2520by%2520approximately%252050%2525%2520compared%2520to%2520the%2520conventional%2520US-guided%2520workflow.%250AWe%2520present%2520the%2520first%2520robotic%2520dual-modality%2520imaging%2520system%2520designed%2520to%2520guide%250Aclinical%2520applications.%2520The%2520results%2520show%2520significant%2520performance%2520improvements%250Acompared%2520to%2520traditional%2520manual%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20CBCT%20Meets%20Robotic%20Ultrasound&entry.906535625=Feng%20Li%20and%20Yuan%20Bi%20and%20Dianye%20Huang%20and%20Zhongliang%20Jiang%20and%20Nassir%20Navab&entry.1292438233=%20%20The%20multi-modality%20imaging%20system%20offers%20optimal%20fused%20images%20for%20safe%20and%0Aprecise%20interventions%20in%20modern%20clinical%20practices%2C%20such%20as%20computed%20tomography%0A-%20ultrasound%20%28CT-US%29%20guidance%20for%20needle%20insertion.%20However%2C%20the%20limited%0Adexterity%20and%20mobility%20of%20current%20imaging%20devices%20hinder%20their%20integration%20into%0Astandardized%20workflows%20and%20the%20advancement%20toward%20fully%20autonomous%20intervention%0Asystems.%20In%20this%20paper%2C%20we%20present%20a%20novel%20clinical%20setup%20where%20robotic%20cone%0Abeam%20computed%20tomography%20%28CBCT%29%20and%20robotic%20US%20are%20pre-calibrated%20and%0Adynamically%20co-registered%2C%20enabling%20new%20clinical%20applications.%20This%20setup%0Aallows%20registration-free%20rigid%20registration%2C%20facilitating%20multi-modal%20guided%0Aprocedures%20in%20the%20absence%20of%20tissue%20deformation.%20First%2C%20a%20one-time%0Apre-calibration%20is%20performed%20between%20the%20systems.%20To%20ensure%20a%20safe%20insertion%0Apath%20by%20highlighting%20critical%20vasculature%20on%20the%203D%20CBCT%2C%20SAM2%20segments%20vessels%0Afrom%20B-mode%20images%2C%20using%20the%20Doppler%20signal%20as%20an%20autonomously%20generated%0Aprompt.%20Based%20on%20the%20registration%2C%20the%20Doppler%20image%20or%20segmented%20vessel%20masks%0Aare%20then%20mapped%20onto%20the%20CBCT%2C%20creating%20an%20optimally%20fused%20image%20with%0Acomprehensive%20detail.%20To%20validate%20the%20system%2C%20we%20used%20a%20specially%20designed%0Aphantom%2C%20featuring%20lesions%20covered%20by%20ribs%20and%20multiple%20vessels%20with%20simulated%0Amoving%20flow.%20The%20mapping%20error%20between%20US%20and%20CBCT%20resulted%20in%20an%20average%0Adeviation%20of%201.72%2B-0.62%20mm.%20A%20user%20study%20demonstrated%20the%20effectiveness%20of%0ACBCT-US%20fusion%20for%20needle%20insertion%20guidance%2C%20showing%20significant%20improvements%0Ain%20time%20efficiency%2C%20accuracy%2C%20and%20success%20rate.%20Needle%20intervention%20performance%0Aimproved%20by%20approximately%2050%25%20compared%20to%20the%20conventional%20US-guided%20workflow.%0AWe%20present%20the%20first%20robotic%20dual-modality%20imaging%20system%20designed%20to%20guide%0Aclinical%20applications.%20The%20results%20show%20significant%20performance%20improvements%0Acompared%20to%20traditional%20manual%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12019v1&entry.124074799=Read"},
{"title": "Evaluation of End-to-End Continuous Spanish Lipreading in Different Data\n  Conditions", "author": "David Gimeno-G\u00f3mez and Carlos-D. Mart\u00ednez-Hinarejos", "abstract": "  Visual speech recognition remains an open research problem where different\nchallenges must be considered by dispensing with the auditory sense, such as\nvisual ambiguities, the inter-personal variability among speakers, and the\ncomplex modeling of silence. Nonetheless, recent remarkable results have been\nachieved in the field thanks to the availability of large-scale databases and\nthe use of powerful attention mechanisms. Besides, multiple languages apart\nfrom English are nowadays a focus of interest. This paper presents noticeable\nadvances in automatic continuous lipreading for Spanish. First, an end-to-end\nsystem based on the hybrid CTC/Attention architecture is presented. Experiments\nare conducted on two corpora of disparate nature, reaching state-of-the-art\nresults that significantly improve the best performance obtained to date for\nboth databases. In addition, a thorough ablation study is carried out, where it\nis studied how the different components that form the architecture influence\nthe quality of speech recognition. Then, a rigorous error analysis is carried\nout to investigate the different factors that could affect the learning of the\nautomatic system. Finally, a new Spanish lipreading benchmark is consolidated.\nCode and trained models are available at\nhttps://github.com/david-gimeno/evaluating-end2end-spanish-lipreading.\n", "link": "http://arxiv.org/abs/2502.00464v2", "date": "2025-02-17", "relevancy": 2.6096, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20End-to-End%20Continuous%20Spanish%20Lipreading%20in%20Different%20Data%0A%20%20Conditions&body=Title%3A%20Evaluation%20of%20End-to-End%20Continuous%20Spanish%20Lipreading%20in%20Different%20Data%0A%20%20Conditions%0AAuthor%3A%20David%20Gimeno-G%C3%B3mez%20and%20Carlos-D.%20Mart%C3%ADnez-Hinarejos%0AAbstract%3A%20%20%20Visual%20speech%20recognition%20remains%20an%20open%20research%20problem%20where%20different%0Achallenges%20must%20be%20considered%20by%20dispensing%20with%20the%20auditory%20sense%2C%20such%20as%0Avisual%20ambiguities%2C%20the%20inter-personal%20variability%20among%20speakers%2C%20and%20the%0Acomplex%20modeling%20of%20silence.%20Nonetheless%2C%20recent%20remarkable%20results%20have%20been%0Aachieved%20in%20the%20field%20thanks%20to%20the%20availability%20of%20large-scale%20databases%20and%0Athe%20use%20of%20powerful%20attention%20mechanisms.%20Besides%2C%20multiple%20languages%20apart%0Afrom%20English%20are%20nowadays%20a%20focus%20of%20interest.%20This%20paper%20presents%20noticeable%0Aadvances%20in%20automatic%20continuous%20lipreading%20for%20Spanish.%20First%2C%20an%20end-to-end%0Asystem%20based%20on%20the%20hybrid%20CTC/Attention%20architecture%20is%20presented.%20Experiments%0Aare%20conducted%20on%20two%20corpora%20of%20disparate%20nature%2C%20reaching%20state-of-the-art%0Aresults%20that%20significantly%20improve%20the%20best%20performance%20obtained%20to%20date%20for%0Aboth%20databases.%20In%20addition%2C%20a%20thorough%20ablation%20study%20is%20carried%20out%2C%20where%20it%0Ais%20studied%20how%20the%20different%20components%20that%20form%20the%20architecture%20influence%0Athe%20quality%20of%20speech%20recognition.%20Then%2C%20a%20rigorous%20error%20analysis%20is%20carried%0Aout%20to%20investigate%20the%20different%20factors%20that%20could%20affect%20the%20learning%20of%20the%0Aautomatic%20system.%20Finally%2C%20a%20new%20Spanish%20lipreading%20benchmark%20is%20consolidated.%0ACode%20and%20trained%20models%20are%20available%20at%0Ahttps%3A//github.com/david-gimeno/evaluating-end2end-spanish-lipreading.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00464v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520End-to-End%2520Continuous%2520Spanish%2520Lipreading%2520in%2520Different%2520Data%250A%2520%2520Conditions%26entry.906535625%3DDavid%2520Gimeno-G%25C3%25B3mez%2520and%2520Carlos-D.%2520Mart%25C3%25ADnez-Hinarejos%26entry.1292438233%3D%2520%2520Visual%2520speech%2520recognition%2520remains%2520an%2520open%2520research%2520problem%2520where%2520different%250Achallenges%2520must%2520be%2520considered%2520by%2520dispensing%2520with%2520the%2520auditory%2520sense%252C%2520such%2520as%250Avisual%2520ambiguities%252C%2520the%2520inter-personal%2520variability%2520among%2520speakers%252C%2520and%2520the%250Acomplex%2520modeling%2520of%2520silence.%2520Nonetheless%252C%2520recent%2520remarkable%2520results%2520have%2520been%250Aachieved%2520in%2520the%2520field%2520thanks%2520to%2520the%2520availability%2520of%2520large-scale%2520databases%2520and%250Athe%2520use%2520of%2520powerful%2520attention%2520mechanisms.%2520Besides%252C%2520multiple%2520languages%2520apart%250Afrom%2520English%2520are%2520nowadays%2520a%2520focus%2520of%2520interest.%2520This%2520paper%2520presents%2520noticeable%250Aadvances%2520in%2520automatic%2520continuous%2520lipreading%2520for%2520Spanish.%2520First%252C%2520an%2520end-to-end%250Asystem%2520based%2520on%2520the%2520hybrid%2520CTC/Attention%2520architecture%2520is%2520presented.%2520Experiments%250Aare%2520conducted%2520on%2520two%2520corpora%2520of%2520disparate%2520nature%252C%2520reaching%2520state-of-the-art%250Aresults%2520that%2520significantly%2520improve%2520the%2520best%2520performance%2520obtained%2520to%2520date%2520for%250Aboth%2520databases.%2520In%2520addition%252C%2520a%2520thorough%2520ablation%2520study%2520is%2520carried%2520out%252C%2520where%2520it%250Ais%2520studied%2520how%2520the%2520different%2520components%2520that%2520form%2520the%2520architecture%2520influence%250Athe%2520quality%2520of%2520speech%2520recognition.%2520Then%252C%2520a%2520rigorous%2520error%2520analysis%2520is%2520carried%250Aout%2520to%2520investigate%2520the%2520different%2520factors%2520that%2520could%2520affect%2520the%2520learning%2520of%2520the%250Aautomatic%2520system.%2520Finally%252C%2520a%2520new%2520Spanish%2520lipreading%2520benchmark%2520is%2520consolidated.%250ACode%2520and%2520trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/david-gimeno/evaluating-end2end-spanish-lipreading.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00464v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20End-to-End%20Continuous%20Spanish%20Lipreading%20in%20Different%20Data%0A%20%20Conditions&entry.906535625=David%20Gimeno-G%C3%B3mez%20and%20Carlos-D.%20Mart%C3%ADnez-Hinarejos&entry.1292438233=%20%20Visual%20speech%20recognition%20remains%20an%20open%20research%20problem%20where%20different%0Achallenges%20must%20be%20considered%20by%20dispensing%20with%20the%20auditory%20sense%2C%20such%20as%0Avisual%20ambiguities%2C%20the%20inter-personal%20variability%20among%20speakers%2C%20and%20the%0Acomplex%20modeling%20of%20silence.%20Nonetheless%2C%20recent%20remarkable%20results%20have%20been%0Aachieved%20in%20the%20field%20thanks%20to%20the%20availability%20of%20large-scale%20databases%20and%0Athe%20use%20of%20powerful%20attention%20mechanisms.%20Besides%2C%20multiple%20languages%20apart%0Afrom%20English%20are%20nowadays%20a%20focus%20of%20interest.%20This%20paper%20presents%20noticeable%0Aadvances%20in%20automatic%20continuous%20lipreading%20for%20Spanish.%20First%2C%20an%20end-to-end%0Asystem%20based%20on%20the%20hybrid%20CTC/Attention%20architecture%20is%20presented.%20Experiments%0Aare%20conducted%20on%20two%20corpora%20of%20disparate%20nature%2C%20reaching%20state-of-the-art%0Aresults%20that%20significantly%20improve%20the%20best%20performance%20obtained%20to%20date%20for%0Aboth%20databases.%20In%20addition%2C%20a%20thorough%20ablation%20study%20is%20carried%20out%2C%20where%20it%0Ais%20studied%20how%20the%20different%20components%20that%20form%20the%20architecture%20influence%0Athe%20quality%20of%20speech%20recognition.%20Then%2C%20a%20rigorous%20error%20analysis%20is%20carried%0Aout%20to%20investigate%20the%20different%20factors%20that%20could%20affect%20the%20learning%20of%20the%0Aautomatic%20system.%20Finally%2C%20a%20new%20Spanish%20lipreading%20benchmark%20is%20consolidated.%0ACode%20and%20trained%20models%20are%20available%20at%0Ahttps%3A//github.com/david-gimeno/evaluating-end2end-spanish-lipreading.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00464v2&entry.124074799=Read"},
{"title": "3D Whole-body Grasp Synthesis with Directional Controllability", "author": "Georgios Paschalidis and Romana Wilschut and Dimitrije Anti\u0107 and Omid Taheri and Dimitrios Tzionas", "abstract": "  Synthesizing 3D whole bodies that realistically grasp objects is useful for\nanimation, mixed reality, and robotics. This is challenging, because the hands\nand body need to look natural w.r.t. each other, the grasped object, as well as\nthe local scene (i.e., a receptacle supporting the object). Moreover, training\ndata for this task is really scarce, while capturing new data is expensive.\nRecent work goes beyond finite datasets via a divide-and-conquer approach; it\nfirst generates a \"guiding\" right-hand grasp, and then searches for bodies that\nmatch this. However, the guiding-hand synthesis lacks controllability and\nreceptacle awareness, so it likely has an implausible direction (i.e., a body\ncan't match this without penetrating the receptacle) and needs corrections\nthrough major post-processing. Moreover, the body search needs exhaustive\nsampling and is expensive. These are strong limitations. We tackle these with a\nnovel method called CWGrasp. Our key idea is that performing geometry-based\nreasoning \"early on,\" instead of \"too late,\" provides rich \"control\" signals\nfor inference. To this end, CWGrasp first samples a plausible\nreaching-direction vector (used later for both the arm and hand) from a\nprobabilistic model built via ray-casting from the object and collision\nchecking. Moreover, CWGrasp uniquely tackles both right and left-hand grasps.\nWe evaluate on the GRAB and ReplicaGrasp datasets. CWGrasp outperforms\nbaselines, at lower runtime and budget, while all components help performance.\nCode and models are available at https://gpaschalidis.github.io/cwgrasp.\n", "link": "http://arxiv.org/abs/2408.16770v2", "date": "2025-02-17", "relevancy": 2.6061, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6725}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.658}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Whole-body%20Grasp%20Synthesis%20with%20Directional%20Controllability&body=Title%3A%203D%20Whole-body%20Grasp%20Synthesis%20with%20Directional%20Controllability%0AAuthor%3A%20Georgios%20Paschalidis%20and%20Romana%20Wilschut%20and%20Dimitrije%20Anti%C4%87%20and%20Omid%20Taheri%20and%20Dimitrios%20Tzionas%0AAbstract%3A%20%20%20Synthesizing%203D%20whole%20bodies%20that%20realistically%20grasp%20objects%20is%20useful%20for%0Aanimation%2C%20mixed%20reality%2C%20and%20robotics.%20This%20is%20challenging%2C%20because%20the%20hands%0Aand%20body%20need%20to%20look%20natural%20w.r.t.%20each%20other%2C%20the%20grasped%20object%2C%20as%20well%20as%0Athe%20local%20scene%20%28i.e.%2C%20a%20receptacle%20supporting%20the%20object%29.%20Moreover%2C%20training%0Adata%20for%20this%20task%20is%20really%20scarce%2C%20while%20capturing%20new%20data%20is%20expensive.%0ARecent%20work%20goes%20beyond%20finite%20datasets%20via%20a%20divide-and-conquer%20approach%3B%20it%0Afirst%20generates%20a%20%22guiding%22%20right-hand%20grasp%2C%20and%20then%20searches%20for%20bodies%20that%0Amatch%20this.%20However%2C%20the%20guiding-hand%20synthesis%20lacks%20controllability%20and%0Areceptacle%20awareness%2C%20so%20it%20likely%20has%20an%20implausible%20direction%20%28i.e.%2C%20a%20body%0Acan%27t%20match%20this%20without%20penetrating%20the%20receptacle%29%20and%20needs%20corrections%0Athrough%20major%20post-processing.%20Moreover%2C%20the%20body%20search%20needs%20exhaustive%0Asampling%20and%20is%20expensive.%20These%20are%20strong%20limitations.%20We%20tackle%20these%20with%20a%0Anovel%20method%20called%20CWGrasp.%20Our%20key%20idea%20is%20that%20performing%20geometry-based%0Areasoning%20%22early%20on%2C%22%20instead%20of%20%22too%20late%2C%22%20provides%20rich%20%22control%22%20signals%0Afor%20inference.%20To%20this%20end%2C%20CWGrasp%20first%20samples%20a%20plausible%0Areaching-direction%20vector%20%28used%20later%20for%20both%20the%20arm%20and%20hand%29%20from%20a%0Aprobabilistic%20model%20built%20via%20ray-casting%20from%20the%20object%20and%20collision%0Achecking.%20Moreover%2C%20CWGrasp%20uniquely%20tackles%20both%20right%20and%20left-hand%20grasps.%0AWe%20evaluate%20on%20the%20GRAB%20and%20ReplicaGrasp%20datasets.%20CWGrasp%20outperforms%0Abaselines%2C%20at%20lower%20runtime%20and%20budget%2C%20while%20all%20components%20help%20performance.%0ACode%20and%20models%20are%20available%20at%20https%3A//gpaschalidis.github.io/cwgrasp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Whole-body%2520Grasp%2520Synthesis%2520with%2520Directional%2520Controllability%26entry.906535625%3DGeorgios%2520Paschalidis%2520and%2520Romana%2520Wilschut%2520and%2520Dimitrije%2520Anti%25C4%2587%2520and%2520Omid%2520Taheri%2520and%2520Dimitrios%2520Tzionas%26entry.1292438233%3D%2520%2520Synthesizing%25203D%2520whole%2520bodies%2520that%2520realistically%2520grasp%2520objects%2520is%2520useful%2520for%250Aanimation%252C%2520mixed%2520reality%252C%2520and%2520robotics.%2520This%2520is%2520challenging%252C%2520because%2520the%2520hands%250Aand%2520body%2520need%2520to%2520look%2520natural%2520w.r.t.%2520each%2520other%252C%2520the%2520grasped%2520object%252C%2520as%2520well%2520as%250Athe%2520local%2520scene%2520%2528i.e.%252C%2520a%2520receptacle%2520supporting%2520the%2520object%2529.%2520Moreover%252C%2520training%250Adata%2520for%2520this%2520task%2520is%2520really%2520scarce%252C%2520while%2520capturing%2520new%2520data%2520is%2520expensive.%250ARecent%2520work%2520goes%2520beyond%2520finite%2520datasets%2520via%2520a%2520divide-and-conquer%2520approach%253B%2520it%250Afirst%2520generates%2520a%2520%2522guiding%2522%2520right-hand%2520grasp%252C%2520and%2520then%2520searches%2520for%2520bodies%2520that%250Amatch%2520this.%2520However%252C%2520the%2520guiding-hand%2520synthesis%2520lacks%2520controllability%2520and%250Areceptacle%2520awareness%252C%2520so%2520it%2520likely%2520has%2520an%2520implausible%2520direction%2520%2528i.e.%252C%2520a%2520body%250Acan%2527t%2520match%2520this%2520without%2520penetrating%2520the%2520receptacle%2529%2520and%2520needs%2520corrections%250Athrough%2520major%2520post-processing.%2520Moreover%252C%2520the%2520body%2520search%2520needs%2520exhaustive%250Asampling%2520and%2520is%2520expensive.%2520These%2520are%2520strong%2520limitations.%2520We%2520tackle%2520these%2520with%2520a%250Anovel%2520method%2520called%2520CWGrasp.%2520Our%2520key%2520idea%2520is%2520that%2520performing%2520geometry-based%250Areasoning%2520%2522early%2520on%252C%2522%2520instead%2520of%2520%2522too%2520late%252C%2522%2520provides%2520rich%2520%2522control%2522%2520signals%250Afor%2520inference.%2520To%2520this%2520end%252C%2520CWGrasp%2520first%2520samples%2520a%2520plausible%250Areaching-direction%2520vector%2520%2528used%2520later%2520for%2520both%2520the%2520arm%2520and%2520hand%2529%2520from%2520a%250Aprobabilistic%2520model%2520built%2520via%2520ray-casting%2520from%2520the%2520object%2520and%2520collision%250Achecking.%2520Moreover%252C%2520CWGrasp%2520uniquely%2520tackles%2520both%2520right%2520and%2520left-hand%2520grasps.%250AWe%2520evaluate%2520on%2520the%2520GRAB%2520and%2520ReplicaGrasp%2520datasets.%2520CWGrasp%2520outperforms%250Abaselines%252C%2520at%2520lower%2520runtime%2520and%2520budget%252C%2520while%2520all%2520components%2520help%2520performance.%250ACode%2520and%2520models%2520are%2520available%2520at%2520https%253A//gpaschalidis.github.io/cwgrasp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Whole-body%20Grasp%20Synthesis%20with%20Directional%20Controllability&entry.906535625=Georgios%20Paschalidis%20and%20Romana%20Wilschut%20and%20Dimitrije%20Anti%C4%87%20and%20Omid%20Taheri%20and%20Dimitrios%20Tzionas&entry.1292438233=%20%20Synthesizing%203D%20whole%20bodies%20that%20realistically%20grasp%20objects%20is%20useful%20for%0Aanimation%2C%20mixed%20reality%2C%20and%20robotics.%20This%20is%20challenging%2C%20because%20the%20hands%0Aand%20body%20need%20to%20look%20natural%20w.r.t.%20each%20other%2C%20the%20grasped%20object%2C%20as%20well%20as%0Athe%20local%20scene%20%28i.e.%2C%20a%20receptacle%20supporting%20the%20object%29.%20Moreover%2C%20training%0Adata%20for%20this%20task%20is%20really%20scarce%2C%20while%20capturing%20new%20data%20is%20expensive.%0ARecent%20work%20goes%20beyond%20finite%20datasets%20via%20a%20divide-and-conquer%20approach%3B%20it%0Afirst%20generates%20a%20%22guiding%22%20right-hand%20grasp%2C%20and%20then%20searches%20for%20bodies%20that%0Amatch%20this.%20However%2C%20the%20guiding-hand%20synthesis%20lacks%20controllability%20and%0Areceptacle%20awareness%2C%20so%20it%20likely%20has%20an%20implausible%20direction%20%28i.e.%2C%20a%20body%0Acan%27t%20match%20this%20without%20penetrating%20the%20receptacle%29%20and%20needs%20corrections%0Athrough%20major%20post-processing.%20Moreover%2C%20the%20body%20search%20needs%20exhaustive%0Asampling%20and%20is%20expensive.%20These%20are%20strong%20limitations.%20We%20tackle%20these%20with%20a%0Anovel%20method%20called%20CWGrasp.%20Our%20key%20idea%20is%20that%20performing%20geometry-based%0Areasoning%20%22early%20on%2C%22%20instead%20of%20%22too%20late%2C%22%20provides%20rich%20%22control%22%20signals%0Afor%20inference.%20To%20this%20end%2C%20CWGrasp%20first%20samples%20a%20plausible%0Areaching-direction%20vector%20%28used%20later%20for%20both%20the%20arm%20and%20hand%29%20from%20a%0Aprobabilistic%20model%20built%20via%20ray-casting%20from%20the%20object%20and%20collision%0Achecking.%20Moreover%2C%20CWGrasp%20uniquely%20tackles%20both%20right%20and%20left-hand%20grasps.%0AWe%20evaluate%20on%20the%20GRAB%20and%20ReplicaGrasp%20datasets.%20CWGrasp%20outperforms%0Abaselines%2C%20at%20lower%20runtime%20and%20budget%2C%20while%20all%20components%20help%20performance.%0ACode%20and%20models%20are%20available%20at%20https%3A//gpaschalidis.github.io/cwgrasp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16770v2&entry.124074799=Read"},
{"title": "JotlasNet: Joint Tensor Low-Rank and Attention-based Sparse Unrolling\n  Network for Accelerating Dynamic MRI", "author": "Yinghao Zhang and Haiyan Gui and Ningdi Yang and Yue Hu", "abstract": "  Joint low-rank and sparse unrolling networks have shown superior performance\nin dynamic MRI reconstruction. However, existing works mainly utilized matrix\nlow-rank priors, neglecting the tensor characteristics of dynamic MRI images,\nand only a global threshold is applied for the sparse constraint to the\nmulti-channel data, limiting the flexibility of the network. Additionally, most\nof them have inherently complex network structure, with intricate interactions\namong variables. In this paper, we propose a novel deep unrolling network,\nJotlasNet, for dynamic MRI reconstruction by jointly utilizing tensor low-rank\nand attention-based sparse priors. Specifically, we utilize tensor low-rank\nprior to exploit the structural correlations in high-dimensional data.\nConvolutional neural networks are used to adaptively learn the low-rank and\nsparse transform domains. A novel attention-based soft thresholding operator is\nproposed to assign a unique learnable threshold to each channel of the data in\nthe CNN-learned sparse domain. The network is unrolled from the elaborately\ndesigned composite splitting algorithm and thus features a simple yet efficient\nparallel structure. Extensive experiments on two datasets (OCMR, CMRxRecon)\ndemonstrate the superior performance of JotlasNet in dynamic MRI\nreconstruction.\n", "link": "http://arxiv.org/abs/2502.11749v1", "date": "2025-02-17", "relevancy": 2.5854, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5248}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.519}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JotlasNet%3A%20Joint%20Tensor%20Low-Rank%20and%20Attention-based%20Sparse%20Unrolling%0A%20%20Network%20for%20Accelerating%20Dynamic%20MRI&body=Title%3A%20JotlasNet%3A%20Joint%20Tensor%20Low-Rank%20and%20Attention-based%20Sparse%20Unrolling%0A%20%20Network%20for%20Accelerating%20Dynamic%20MRI%0AAuthor%3A%20Yinghao%20Zhang%20and%20Haiyan%20Gui%20and%20Ningdi%20Yang%20and%20Yue%20Hu%0AAbstract%3A%20%20%20Joint%20low-rank%20and%20sparse%20unrolling%20networks%20have%20shown%20superior%20performance%0Ain%20dynamic%20MRI%20reconstruction.%20However%2C%20existing%20works%20mainly%20utilized%20matrix%0Alow-rank%20priors%2C%20neglecting%20the%20tensor%20characteristics%20of%20dynamic%20MRI%20images%2C%0Aand%20only%20a%20global%20threshold%20is%20applied%20for%20the%20sparse%20constraint%20to%20the%0Amulti-channel%20data%2C%20limiting%20the%20flexibility%20of%20the%20network.%20Additionally%2C%20most%0Aof%20them%20have%20inherently%20complex%20network%20structure%2C%20with%20intricate%20interactions%0Aamong%20variables.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20deep%20unrolling%20network%2C%0AJotlasNet%2C%20for%20dynamic%20MRI%20reconstruction%20by%20jointly%20utilizing%20tensor%20low-rank%0Aand%20attention-based%20sparse%20priors.%20Specifically%2C%20we%20utilize%20tensor%20low-rank%0Aprior%20to%20exploit%20the%20structural%20correlations%20in%20high-dimensional%20data.%0AConvolutional%20neural%20networks%20are%20used%20to%20adaptively%20learn%20the%20low-rank%20and%0Asparse%20transform%20domains.%20A%20novel%20attention-based%20soft%20thresholding%20operator%20is%0Aproposed%20to%20assign%20a%20unique%20learnable%20threshold%20to%20each%20channel%20of%20the%20data%20in%0Athe%20CNN-learned%20sparse%20domain.%20The%20network%20is%20unrolled%20from%20the%20elaborately%0Adesigned%20composite%20splitting%20algorithm%20and%20thus%20features%20a%20simple%20yet%20efficient%0Aparallel%20structure.%20Extensive%20experiments%20on%20two%20datasets%20%28OCMR%2C%20CMRxRecon%29%0Ademonstrate%20the%20superior%20performance%20of%20JotlasNet%20in%20dynamic%20MRI%0Areconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJotlasNet%253A%2520Joint%2520Tensor%2520Low-Rank%2520and%2520Attention-based%2520Sparse%2520Unrolling%250A%2520%2520Network%2520for%2520Accelerating%2520Dynamic%2520MRI%26entry.906535625%3DYinghao%2520Zhang%2520and%2520Haiyan%2520Gui%2520and%2520Ningdi%2520Yang%2520and%2520Yue%2520Hu%26entry.1292438233%3D%2520%2520Joint%2520low-rank%2520and%2520sparse%2520unrolling%2520networks%2520have%2520shown%2520superior%2520performance%250Ain%2520dynamic%2520MRI%2520reconstruction.%2520However%252C%2520existing%2520works%2520mainly%2520utilized%2520matrix%250Alow-rank%2520priors%252C%2520neglecting%2520the%2520tensor%2520characteristics%2520of%2520dynamic%2520MRI%2520images%252C%250Aand%2520only%2520a%2520global%2520threshold%2520is%2520applied%2520for%2520the%2520sparse%2520constraint%2520to%2520the%250Amulti-channel%2520data%252C%2520limiting%2520the%2520flexibility%2520of%2520the%2520network.%2520Additionally%252C%2520most%250Aof%2520them%2520have%2520inherently%2520complex%2520network%2520structure%252C%2520with%2520intricate%2520interactions%250Aamong%2520variables.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520deep%2520unrolling%2520network%252C%250AJotlasNet%252C%2520for%2520dynamic%2520MRI%2520reconstruction%2520by%2520jointly%2520utilizing%2520tensor%2520low-rank%250Aand%2520attention-based%2520sparse%2520priors.%2520Specifically%252C%2520we%2520utilize%2520tensor%2520low-rank%250Aprior%2520to%2520exploit%2520the%2520structural%2520correlations%2520in%2520high-dimensional%2520data.%250AConvolutional%2520neural%2520networks%2520are%2520used%2520to%2520adaptively%2520learn%2520the%2520low-rank%2520and%250Asparse%2520transform%2520domains.%2520A%2520novel%2520attention-based%2520soft%2520thresholding%2520operator%2520is%250Aproposed%2520to%2520assign%2520a%2520unique%2520learnable%2520threshold%2520to%2520each%2520channel%2520of%2520the%2520data%2520in%250Athe%2520CNN-learned%2520sparse%2520domain.%2520The%2520network%2520is%2520unrolled%2520from%2520the%2520elaborately%250Adesigned%2520composite%2520splitting%2520algorithm%2520and%2520thus%2520features%2520a%2520simple%2520yet%2520efficient%250Aparallel%2520structure.%2520Extensive%2520experiments%2520on%2520two%2520datasets%2520%2528OCMR%252C%2520CMRxRecon%2529%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520JotlasNet%2520in%2520dynamic%2520MRI%250Areconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JotlasNet%3A%20Joint%20Tensor%20Low-Rank%20and%20Attention-based%20Sparse%20Unrolling%0A%20%20Network%20for%20Accelerating%20Dynamic%20MRI&entry.906535625=Yinghao%20Zhang%20and%20Haiyan%20Gui%20and%20Ningdi%20Yang%20and%20Yue%20Hu&entry.1292438233=%20%20Joint%20low-rank%20and%20sparse%20unrolling%20networks%20have%20shown%20superior%20performance%0Ain%20dynamic%20MRI%20reconstruction.%20However%2C%20existing%20works%20mainly%20utilized%20matrix%0Alow-rank%20priors%2C%20neglecting%20the%20tensor%20characteristics%20of%20dynamic%20MRI%20images%2C%0Aand%20only%20a%20global%20threshold%20is%20applied%20for%20the%20sparse%20constraint%20to%20the%0Amulti-channel%20data%2C%20limiting%20the%20flexibility%20of%20the%20network.%20Additionally%2C%20most%0Aof%20them%20have%20inherently%20complex%20network%20structure%2C%20with%20intricate%20interactions%0Aamong%20variables.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20deep%20unrolling%20network%2C%0AJotlasNet%2C%20for%20dynamic%20MRI%20reconstruction%20by%20jointly%20utilizing%20tensor%20low-rank%0Aand%20attention-based%20sparse%20priors.%20Specifically%2C%20we%20utilize%20tensor%20low-rank%0Aprior%20to%20exploit%20the%20structural%20correlations%20in%20high-dimensional%20data.%0AConvolutional%20neural%20networks%20are%20used%20to%20adaptively%20learn%20the%20low-rank%20and%0Asparse%20transform%20domains.%20A%20novel%20attention-based%20soft%20thresholding%20operator%20is%0Aproposed%20to%20assign%20a%20unique%20learnable%20threshold%20to%20each%20channel%20of%20the%20data%20in%0Athe%20CNN-learned%20sparse%20domain.%20The%20network%20is%20unrolled%20from%20the%20elaborately%0Adesigned%20composite%20splitting%20algorithm%20and%20thus%20features%20a%20simple%20yet%20efficient%0Aparallel%20structure.%20Extensive%20experiments%20on%20two%20datasets%20%28OCMR%2C%20CMRxRecon%29%0Ademonstrate%20the%20superior%20performance%20of%20JotlasNet%20in%20dynamic%20MRI%0Areconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11749v1&entry.124074799=Read"},
{"title": "Rethinking Benign Overfitting in Two-Layer Neural Networks", "author": "Ruichen Xu and Kexin Chen", "abstract": "  Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) have revealed\na sharp phase transition from benign to harmful overfitting when the\nnoise-to-feature ratio exceeds a threshold-a situation common in long-tailed\ndata distributions where atypical data is prevalent. However, harmful\noverfitting rarely happens in overparameterized neural networks. Further\nexperimental results suggested that memorization is necessary for achieving\nnear-optimal generalization error in long-tailed data distributions (Feldman &\nZhang, 2020). We argue that this discrepancy between theoretical predictions\nand empirical observations arises because previous feature-noise data models\noverlook the heterogeneous nature of noise across different data classes. In\nthis paper, we refine the feature-noise data model by incorporating\nclass-dependent heterogeneous noise and re-examine the overfitting phenomenon\nin neural networks. Through a comprehensive analysis of the training dynamics,\nwe establish test loss bounds for the refined model. Our findings reveal that\nneural networks can leverage \"data noise\", previously deemed harmful, to learn\nimplicit features that improve the classification accuracy for long-tailed\ndata. Experimental validation on both synthetic and real-world datasets\nsupports our theoretical results.\n", "link": "http://arxiv.org/abs/2502.11893v1", "date": "2025-02-17", "relevancy": 2.5783, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5366}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5214}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Benign%20Overfitting%20in%20Two-Layer%20Neural%20Networks&body=Title%3A%20Rethinking%20Benign%20Overfitting%20in%20Two-Layer%20Neural%20Networks%0AAuthor%3A%20Ruichen%20Xu%20and%20Kexin%20Chen%0AAbstract%3A%20%20%20Recent%20theoretical%20studies%20%28Kou%20et%20al.%2C%202023%3B%20Cao%20et%20al.%2C%202022%29%20have%20revealed%0Aa%20sharp%20phase%20transition%20from%20benign%20to%20harmful%20overfitting%20when%20the%0Anoise-to-feature%20ratio%20exceeds%20a%20threshold-a%20situation%20common%20in%20long-tailed%0Adata%20distributions%20where%20atypical%20data%20is%20prevalent.%20However%2C%20harmful%0Aoverfitting%20rarely%20happens%20in%20overparameterized%20neural%20networks.%20Further%0Aexperimental%20results%20suggested%20that%20memorization%20is%20necessary%20for%20achieving%0Anear-optimal%20generalization%20error%20in%20long-tailed%20data%20distributions%20%28Feldman%20%26%0AZhang%2C%202020%29.%20We%20argue%20that%20this%20discrepancy%20between%20theoretical%20predictions%0Aand%20empirical%20observations%20arises%20because%20previous%20feature-noise%20data%20models%0Aoverlook%20the%20heterogeneous%20nature%20of%20noise%20across%20different%20data%20classes.%20In%0Athis%20paper%2C%20we%20refine%20the%20feature-noise%20data%20model%20by%20incorporating%0Aclass-dependent%20heterogeneous%20noise%20and%20re-examine%20the%20overfitting%20phenomenon%0Ain%20neural%20networks.%20Through%20a%20comprehensive%20analysis%20of%20the%20training%20dynamics%2C%0Awe%20establish%20test%20loss%20bounds%20for%20the%20refined%20model.%20Our%20findings%20reveal%20that%0Aneural%20networks%20can%20leverage%20%22data%20noise%22%2C%20previously%20deemed%20harmful%2C%20to%20learn%0Aimplicit%20features%20that%20improve%20the%20classification%20accuracy%20for%20long-tailed%0Adata.%20Experimental%20validation%20on%20both%20synthetic%20and%20real-world%20datasets%0Asupports%20our%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Benign%2520Overfitting%2520in%2520Two-Layer%2520Neural%2520Networks%26entry.906535625%3DRuichen%2520Xu%2520and%2520Kexin%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520theoretical%2520studies%2520%2528Kou%2520et%2520al.%252C%25202023%253B%2520Cao%2520et%2520al.%252C%25202022%2529%2520have%2520revealed%250Aa%2520sharp%2520phase%2520transition%2520from%2520benign%2520to%2520harmful%2520overfitting%2520when%2520the%250Anoise-to-feature%2520ratio%2520exceeds%2520a%2520threshold-a%2520situation%2520common%2520in%2520long-tailed%250Adata%2520distributions%2520where%2520atypical%2520data%2520is%2520prevalent.%2520However%252C%2520harmful%250Aoverfitting%2520rarely%2520happens%2520in%2520overparameterized%2520neural%2520networks.%2520Further%250Aexperimental%2520results%2520suggested%2520that%2520memorization%2520is%2520necessary%2520for%2520achieving%250Anear-optimal%2520generalization%2520error%2520in%2520long-tailed%2520data%2520distributions%2520%2528Feldman%2520%2526%250AZhang%252C%25202020%2529.%2520We%2520argue%2520that%2520this%2520discrepancy%2520between%2520theoretical%2520predictions%250Aand%2520empirical%2520observations%2520arises%2520because%2520previous%2520feature-noise%2520data%2520models%250Aoverlook%2520the%2520heterogeneous%2520nature%2520of%2520noise%2520across%2520different%2520data%2520classes.%2520In%250Athis%2520paper%252C%2520we%2520refine%2520the%2520feature-noise%2520data%2520model%2520by%2520incorporating%250Aclass-dependent%2520heterogeneous%2520noise%2520and%2520re-examine%2520the%2520overfitting%2520phenomenon%250Ain%2520neural%2520networks.%2520Through%2520a%2520comprehensive%2520analysis%2520of%2520the%2520training%2520dynamics%252C%250Awe%2520establish%2520test%2520loss%2520bounds%2520for%2520the%2520refined%2520model.%2520Our%2520findings%2520reveal%2520that%250Aneural%2520networks%2520can%2520leverage%2520%2522data%2520noise%2522%252C%2520previously%2520deemed%2520harmful%252C%2520to%2520learn%250Aimplicit%2520features%2520that%2520improve%2520the%2520classification%2520accuracy%2520for%2520long-tailed%250Adata.%2520Experimental%2520validation%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%250Asupports%2520our%2520theoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Benign%20Overfitting%20in%20Two-Layer%20Neural%20Networks&entry.906535625=Ruichen%20Xu%20and%20Kexin%20Chen&entry.1292438233=%20%20Recent%20theoretical%20studies%20%28Kou%20et%20al.%2C%202023%3B%20Cao%20et%20al.%2C%202022%29%20have%20revealed%0Aa%20sharp%20phase%20transition%20from%20benign%20to%20harmful%20overfitting%20when%20the%0Anoise-to-feature%20ratio%20exceeds%20a%20threshold-a%20situation%20common%20in%20long-tailed%0Adata%20distributions%20where%20atypical%20data%20is%20prevalent.%20However%2C%20harmful%0Aoverfitting%20rarely%20happens%20in%20overparameterized%20neural%20networks.%20Further%0Aexperimental%20results%20suggested%20that%20memorization%20is%20necessary%20for%20achieving%0Anear-optimal%20generalization%20error%20in%20long-tailed%20data%20distributions%20%28Feldman%20%26%0AZhang%2C%202020%29.%20We%20argue%20that%20this%20discrepancy%20between%20theoretical%20predictions%0Aand%20empirical%20observations%20arises%20because%20previous%20feature-noise%20data%20models%0Aoverlook%20the%20heterogeneous%20nature%20of%20noise%20across%20different%20data%20classes.%20In%0Athis%20paper%2C%20we%20refine%20the%20feature-noise%20data%20model%20by%20incorporating%0Aclass-dependent%20heterogeneous%20noise%20and%20re-examine%20the%20overfitting%20phenomenon%0Ain%20neural%20networks.%20Through%20a%20comprehensive%20analysis%20of%20the%20training%20dynamics%2C%0Awe%20establish%20test%20loss%20bounds%20for%20the%20refined%20model.%20Our%20findings%20reveal%20that%0Aneural%20networks%20can%20leverage%20%22data%20noise%22%2C%20previously%20deemed%20harmful%2C%20to%20learn%0Aimplicit%20features%20that%20improve%20the%20classification%20accuracy%20for%20long-tailed%0Adata.%20Experimental%20validation%20on%20both%20synthetic%20and%20real-world%20datasets%0Asupports%20our%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11893v1&entry.124074799=Read"},
{"title": "ILIAS: Instance-Level Image retrieval At Scale", "author": "Giorgos Kordopatis-Zilos and Vladan Stojni\u0107 and Anna Manko and Pavel \u0160uma and Nikolaos-Antonios Ypsilantis and Nikos Efthymiadis and Zakaria Laskar and Ji\u0159\u00ed Matas and Ond\u0159ej Chum and Giorgos Tolias", "abstract": "  This work introduces ILIAS, a new test dataset for Instance-Level Image\nretrieval At Scale. It is designed to evaluate the ability of current and\nfuture foundation models and retrieval techniques to recognize particular\nobjects. The key benefits over existing datasets include large scale, domain\ndiversity, accurate ground truth, and a performance that is far from saturated.\nILIAS includes query and positive images for 1,000 object instances, manually\ncollected to capture challenging conditions and diverse domains. Large-scale\nretrieval is conducted against 100 million distractor images from YFCC100M. To\navoid false negatives without extra annotation effort, we include only query\nobjects confirmed to have emerged after 2014, i.e. the compilation date of\nYFCC100M. An extensive benchmarking is performed with the following\nobservations: i) models fine-tuned on specific domains, such as landmarks or\nproducts, excel in that domain but fail on ILIAS ii) learning a linear\nadaptation layer using multi-domain class supervision results in performance\nimprovements, especially for vision-language models iii) local descriptors in\nretrieval re-ranking are still a key ingredient, especially in the presence of\nsevere background clutter iv) the text-to-image performance of the\nvision-language foundation models is surprisingly close to the corresponding\nimage-to-image case. website: https://vrg.fel.cvut.cz/ilias/\n", "link": "http://arxiv.org/abs/2502.11748v1", "date": "2025-02-17", "relevancy": 2.5702, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ILIAS%3A%20Instance-Level%20Image%20retrieval%20At%20Scale&body=Title%3A%20ILIAS%3A%20Instance-Level%20Image%20retrieval%20At%20Scale%0AAuthor%3A%20Giorgos%20Kordopatis-Zilos%20and%20Vladan%20Stojni%C4%87%20and%20Anna%20Manko%20and%20Pavel%20%C5%A0uma%20and%20Nikolaos-Antonios%20Ypsilantis%20and%20Nikos%20Efthymiadis%20and%20Zakaria%20Laskar%20and%20Ji%C5%99%C3%AD%20Matas%20and%20Ond%C5%99ej%20Chum%20and%20Giorgos%20Tolias%0AAbstract%3A%20%20%20This%20work%20introduces%20ILIAS%2C%20a%20new%20test%20dataset%20for%20Instance-Level%20Image%0Aretrieval%20At%20Scale.%20It%20is%20designed%20to%20evaluate%20the%20ability%20of%20current%20and%0Afuture%20foundation%20models%20and%20retrieval%20techniques%20to%20recognize%20particular%0Aobjects.%20The%20key%20benefits%20over%20existing%20datasets%20include%20large%20scale%2C%20domain%0Adiversity%2C%20accurate%20ground%20truth%2C%20and%20a%20performance%20that%20is%20far%20from%20saturated.%0AILIAS%20includes%20query%20and%20positive%20images%20for%201%2C000%20object%20instances%2C%20manually%0Acollected%20to%20capture%20challenging%20conditions%20and%20diverse%20domains.%20Large-scale%0Aretrieval%20is%20conducted%20against%20100%20million%20distractor%20images%20from%20YFCC100M.%20To%0Aavoid%20false%20negatives%20without%20extra%20annotation%20effort%2C%20we%20include%20only%20query%0Aobjects%20confirmed%20to%20have%20emerged%20after%202014%2C%20i.e.%20the%20compilation%20date%20of%0AYFCC100M.%20An%20extensive%20benchmarking%20is%20performed%20with%20the%20following%0Aobservations%3A%20i%29%20models%20fine-tuned%20on%20specific%20domains%2C%20such%20as%20landmarks%20or%0Aproducts%2C%20excel%20in%20that%20domain%20but%20fail%20on%20ILIAS%20ii%29%20learning%20a%20linear%0Aadaptation%20layer%20using%20multi-domain%20class%20supervision%20results%20in%20performance%0Aimprovements%2C%20especially%20for%20vision-language%20models%20iii%29%20local%20descriptors%20in%0Aretrieval%20re-ranking%20are%20still%20a%20key%20ingredient%2C%20especially%20in%20the%20presence%20of%0Asevere%20background%20clutter%20iv%29%20the%20text-to-image%20performance%20of%20the%0Avision-language%20foundation%20models%20is%20surprisingly%20close%20to%20the%20corresponding%0Aimage-to-image%20case.%20website%3A%20https%3A//vrg.fel.cvut.cz/ilias/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DILIAS%253A%2520Instance-Level%2520Image%2520retrieval%2520At%2520Scale%26entry.906535625%3DGiorgos%2520Kordopatis-Zilos%2520and%2520Vladan%2520Stojni%25C4%2587%2520and%2520Anna%2520Manko%2520and%2520Pavel%2520%25C5%25A0uma%2520and%2520Nikolaos-Antonios%2520Ypsilantis%2520and%2520Nikos%2520Efthymiadis%2520and%2520Zakaria%2520Laskar%2520and%2520Ji%25C5%2599%25C3%25AD%2520Matas%2520and%2520Ond%25C5%2599ej%2520Chum%2520and%2520Giorgos%2520Tolias%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520ILIAS%252C%2520a%2520new%2520test%2520dataset%2520for%2520Instance-Level%2520Image%250Aretrieval%2520At%2520Scale.%2520It%2520is%2520designed%2520to%2520evaluate%2520the%2520ability%2520of%2520current%2520and%250Afuture%2520foundation%2520models%2520and%2520retrieval%2520techniques%2520to%2520recognize%2520particular%250Aobjects.%2520The%2520key%2520benefits%2520over%2520existing%2520datasets%2520include%2520large%2520scale%252C%2520domain%250Adiversity%252C%2520accurate%2520ground%2520truth%252C%2520and%2520a%2520performance%2520that%2520is%2520far%2520from%2520saturated.%250AILIAS%2520includes%2520query%2520and%2520positive%2520images%2520for%25201%252C000%2520object%2520instances%252C%2520manually%250Acollected%2520to%2520capture%2520challenging%2520conditions%2520and%2520diverse%2520domains.%2520Large-scale%250Aretrieval%2520is%2520conducted%2520against%2520100%2520million%2520distractor%2520images%2520from%2520YFCC100M.%2520To%250Aavoid%2520false%2520negatives%2520without%2520extra%2520annotation%2520effort%252C%2520we%2520include%2520only%2520query%250Aobjects%2520confirmed%2520to%2520have%2520emerged%2520after%25202014%252C%2520i.e.%2520the%2520compilation%2520date%2520of%250AYFCC100M.%2520An%2520extensive%2520benchmarking%2520is%2520performed%2520with%2520the%2520following%250Aobservations%253A%2520i%2529%2520models%2520fine-tuned%2520on%2520specific%2520domains%252C%2520such%2520as%2520landmarks%2520or%250Aproducts%252C%2520excel%2520in%2520that%2520domain%2520but%2520fail%2520on%2520ILIAS%2520ii%2529%2520learning%2520a%2520linear%250Aadaptation%2520layer%2520using%2520multi-domain%2520class%2520supervision%2520results%2520in%2520performance%250Aimprovements%252C%2520especially%2520for%2520vision-language%2520models%2520iii%2529%2520local%2520descriptors%2520in%250Aretrieval%2520re-ranking%2520are%2520still%2520a%2520key%2520ingredient%252C%2520especially%2520in%2520the%2520presence%2520of%250Asevere%2520background%2520clutter%2520iv%2529%2520the%2520text-to-image%2520performance%2520of%2520the%250Avision-language%2520foundation%2520models%2520is%2520surprisingly%2520close%2520to%2520the%2520corresponding%250Aimage-to-image%2520case.%2520website%253A%2520https%253A//vrg.fel.cvut.cz/ilias/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ILIAS%3A%20Instance-Level%20Image%20retrieval%20At%20Scale&entry.906535625=Giorgos%20Kordopatis-Zilos%20and%20Vladan%20Stojni%C4%87%20and%20Anna%20Manko%20and%20Pavel%20%C5%A0uma%20and%20Nikolaos-Antonios%20Ypsilantis%20and%20Nikos%20Efthymiadis%20and%20Zakaria%20Laskar%20and%20Ji%C5%99%C3%AD%20Matas%20and%20Ond%C5%99ej%20Chum%20and%20Giorgos%20Tolias&entry.1292438233=%20%20This%20work%20introduces%20ILIAS%2C%20a%20new%20test%20dataset%20for%20Instance-Level%20Image%0Aretrieval%20At%20Scale.%20It%20is%20designed%20to%20evaluate%20the%20ability%20of%20current%20and%0Afuture%20foundation%20models%20and%20retrieval%20techniques%20to%20recognize%20particular%0Aobjects.%20The%20key%20benefits%20over%20existing%20datasets%20include%20large%20scale%2C%20domain%0Adiversity%2C%20accurate%20ground%20truth%2C%20and%20a%20performance%20that%20is%20far%20from%20saturated.%0AILIAS%20includes%20query%20and%20positive%20images%20for%201%2C000%20object%20instances%2C%20manually%0Acollected%20to%20capture%20challenging%20conditions%20and%20diverse%20domains.%20Large-scale%0Aretrieval%20is%20conducted%20against%20100%20million%20distractor%20images%20from%20YFCC100M.%20To%0Aavoid%20false%20negatives%20without%20extra%20annotation%20effort%2C%20we%20include%20only%20query%0Aobjects%20confirmed%20to%20have%20emerged%20after%202014%2C%20i.e.%20the%20compilation%20date%20of%0AYFCC100M.%20An%20extensive%20benchmarking%20is%20performed%20with%20the%20following%0Aobservations%3A%20i%29%20models%20fine-tuned%20on%20specific%20domains%2C%20such%20as%20landmarks%20or%0Aproducts%2C%20excel%20in%20that%20domain%20but%20fail%20on%20ILIAS%20ii%29%20learning%20a%20linear%0Aadaptation%20layer%20using%20multi-domain%20class%20supervision%20results%20in%20performance%0Aimprovements%2C%20especially%20for%20vision-language%20models%20iii%29%20local%20descriptors%20in%0Aretrieval%20re-ranking%20are%20still%20a%20key%20ingredient%2C%20especially%20in%20the%20presence%20of%0Asevere%20background%20clutter%20iv%29%20the%20text-to-image%20performance%20of%20the%0Avision-language%20foundation%20models%20is%20surprisingly%20close%20to%20the%20corresponding%0Aimage-to-image%20case.%20website%3A%20https%3A//vrg.fel.cvut.cz/ilias/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11748v1&entry.124074799=Read"},
{"title": "Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning", "author": "Ishika Agarwal and Dilek Hakkani-T\u00fcr", "abstract": "  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n", "link": "http://arxiv.org/abs/2502.09969v2", "date": "2025-02-17", "relevancy": 2.5627, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5594}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4896}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Valuation%20using%20Neural%20Networks%20for%20Efficient%20Instruction%0A%20%20Fine-Tuning&body=Title%3A%20Data%20Valuation%20using%20Neural%20Networks%20for%20Efficient%20Instruction%0A%20%20Fine-Tuning%0AAuthor%3A%20Ishika%20Agarwal%20and%20Dilek%20Hakkani-T%C3%BCr%0AAbstract%3A%20%20%20Influence%20functions%20provide%20crucial%20insights%20into%20model%20training%2C%20but%0Aexisting%20methods%20suffer%20from%20large%20computational%20costs%20and%20limited%0Ageneralization.%20Particularly%2C%20recent%20works%20have%20proposed%20various%20metrics%20and%0Aalgorithms%20to%20calculate%20the%20influence%20of%20data%20using%20language%20models%2C%20which%20do%0Anot%20scale%20well%20with%20large%20models%20and%20datasets.%20This%20is%20because%20of%20the%20expensive%0Aforward%20and%20backward%20passes%20required%20for%20computation%2C%20substantial%20memory%0Arequirements%20to%20store%20large%20models%2C%20and%20poor%20generalization%20of%20influence%0Aestimates%20to%20new%20data.%20In%20this%20paper%2C%20we%20explore%20the%20use%20of%20small%20neural%0Anetworks%20--%20which%20we%20refer%20to%20as%20the%20InfluenceNetwork%20--%20to%20estimate%20influence%0Avalues%2C%20achieving%20up%20to%2099%25%20cost%20reduction.%20Our%20evaluation%20demonstrates%20that%0Ainfluence%20values%20can%20be%20estimated%20with%20models%20just%200.0027%25%20the%20size%20of%20full%0Alanguage%20models%20%28we%20use%207B%20and%208B%20versions%29.%20We%20apply%20our%20algorithm%20of%0Aestimating%20influence%20values%20%28called%20NN-CIFT%3A%20Neural%20Networks%20for%20effiCient%0AInstruction%20Fine-Tuning%29%20to%20the%20downstream%20task%20of%20subset%20selection%20for%20general%0Ainstruction%20fine-tuning.%20In%20our%20study%2C%20we%20include%20four%20state-of-the-art%0Ainfluence%20functions%20and%20show%20no%20compromise%20in%20performance%2C%20despite%20large%0Aspeedups%2C%20between%20NN-CIFT%20and%20the%20original%20influence%20functions.%20We%20provide%20an%0Ain-depth%20hyperparameter%20analyses%20of%20NN-CIFT.%20The%20code%20for%20our%20method%20can%20be%0Afound%20here%3A%20https%3A//github.com/agarwalishika/NN-CIFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Valuation%2520using%2520Neural%2520Networks%2520for%2520Efficient%2520Instruction%250A%2520%2520Fine-Tuning%26entry.906535625%3DIshika%2520Agarwal%2520and%2520Dilek%2520Hakkani-T%25C3%25BCr%26entry.1292438233%3D%2520%2520Influence%2520functions%2520provide%2520crucial%2520insights%2520into%2520model%2520training%252C%2520but%250Aexisting%2520methods%2520suffer%2520from%2520large%2520computational%2520costs%2520and%2520limited%250Ageneralization.%2520Particularly%252C%2520recent%2520works%2520have%2520proposed%2520various%2520metrics%2520and%250Aalgorithms%2520to%2520calculate%2520the%2520influence%2520of%2520data%2520using%2520language%2520models%252C%2520which%2520do%250Anot%2520scale%2520well%2520with%2520large%2520models%2520and%2520datasets.%2520This%2520is%2520because%2520of%2520the%2520expensive%250Aforward%2520and%2520backward%2520passes%2520required%2520for%2520computation%252C%2520substantial%2520memory%250Arequirements%2520to%2520store%2520large%2520models%252C%2520and%2520poor%2520generalization%2520of%2520influence%250Aestimates%2520to%2520new%2520data.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520use%2520of%2520small%2520neural%250Anetworks%2520--%2520which%2520we%2520refer%2520to%2520as%2520the%2520InfluenceNetwork%2520--%2520to%2520estimate%2520influence%250Avalues%252C%2520achieving%2520up%2520to%252099%2525%2520cost%2520reduction.%2520Our%2520evaluation%2520demonstrates%2520that%250Ainfluence%2520values%2520can%2520be%2520estimated%2520with%2520models%2520just%25200.0027%2525%2520the%2520size%2520of%2520full%250Alanguage%2520models%2520%2528we%2520use%25207B%2520and%25208B%2520versions%2529.%2520We%2520apply%2520our%2520algorithm%2520of%250Aestimating%2520influence%2520values%2520%2528called%2520NN-CIFT%253A%2520Neural%2520Networks%2520for%2520effiCient%250AInstruction%2520Fine-Tuning%2529%2520to%2520the%2520downstream%2520task%2520of%2520subset%2520selection%2520for%2520general%250Ainstruction%2520fine-tuning.%2520In%2520our%2520study%252C%2520we%2520include%2520four%2520state-of-the-art%250Ainfluence%2520functions%2520and%2520show%2520no%2520compromise%2520in%2520performance%252C%2520despite%2520large%250Aspeedups%252C%2520between%2520NN-CIFT%2520and%2520the%2520original%2520influence%2520functions.%2520We%2520provide%2520an%250Ain-depth%2520hyperparameter%2520analyses%2520of%2520NN-CIFT.%2520The%2520code%2520for%2520our%2520method%2520can%2520be%250Afound%2520here%253A%2520https%253A//github.com/agarwalishika/NN-CIFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Valuation%20using%20Neural%20Networks%20for%20Efficient%20Instruction%0A%20%20Fine-Tuning&entry.906535625=Ishika%20Agarwal%20and%20Dilek%20Hakkani-T%C3%BCr&entry.1292438233=%20%20Influence%20functions%20provide%20crucial%20insights%20into%20model%20training%2C%20but%0Aexisting%20methods%20suffer%20from%20large%20computational%20costs%20and%20limited%0Ageneralization.%20Particularly%2C%20recent%20works%20have%20proposed%20various%20metrics%20and%0Aalgorithms%20to%20calculate%20the%20influence%20of%20data%20using%20language%20models%2C%20which%20do%0Anot%20scale%20well%20with%20large%20models%20and%20datasets.%20This%20is%20because%20of%20the%20expensive%0Aforward%20and%20backward%20passes%20required%20for%20computation%2C%20substantial%20memory%0Arequirements%20to%20store%20large%20models%2C%20and%20poor%20generalization%20of%20influence%0Aestimates%20to%20new%20data.%20In%20this%20paper%2C%20we%20explore%20the%20use%20of%20small%20neural%0Anetworks%20--%20which%20we%20refer%20to%20as%20the%20InfluenceNetwork%20--%20to%20estimate%20influence%0Avalues%2C%20achieving%20up%20to%2099%25%20cost%20reduction.%20Our%20evaluation%20demonstrates%20that%0Ainfluence%20values%20can%20be%20estimated%20with%20models%20just%200.0027%25%20the%20size%20of%20full%0Alanguage%20models%20%28we%20use%207B%20and%208B%20versions%29.%20We%20apply%20our%20algorithm%20of%0Aestimating%20influence%20values%20%28called%20NN-CIFT%3A%20Neural%20Networks%20for%20effiCient%0AInstruction%20Fine-Tuning%29%20to%20the%20downstream%20task%20of%20subset%20selection%20for%20general%0Ainstruction%20fine-tuning.%20In%20our%20study%2C%20we%20include%20four%20state-of-the-art%0Ainfluence%20functions%20and%20show%20no%20compromise%20in%20performance%2C%20despite%20large%0Aspeedups%2C%20between%20NN-CIFT%20and%20the%20original%20influence%20functions.%20We%20provide%20an%0Ain-depth%20hyperparameter%20analyses%20of%20NN-CIFT.%20The%20code%20for%20our%20method%20can%20be%0Afound%20here%3A%20https%3A//github.com/agarwalishika/NN-CIFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09969v2&entry.124074799=Read"},
{"title": "Learning Generalizable Prompt for CLIP with Class Similarity Knowledge", "author": "Sehun Jung and Hyang-won Lee", "abstract": "  In vision-language models (VLMs), prompt tuning has shown its effectiveness\nin adapting models to downstream tasks. However, learned prompts struggle to\ngeneralize to unseen classes, as they tend to overfit to the classes that are\ntargeted during prompt tuning. Examining failure cases, we observed that\nlearned prompts disrupt the semantics of unseen classes, generating text\nembeddings with incorrect semantic relationships among classes. To address\nthis, we propose Similarity Alignment Regularization (SAR), which regularizes\nlearnable prompts to preserve the semantic relationships among classes captured\nby hand-crafted prompts. Specifically, we first obtain novel classes related to\nbase classes using ChatGPT-4o and utilize them as potential unseen classes\nduring prompt tuning. Then, by targeting both base and novel classes, SAR\naligns the similarity relationships among text embeddings generated by\nlearnable prompts with the similarity relationships from hand-crafted prompts.\nExtensive experiments applying SAR to existing prompt tuning methods\ndemonstrate its effectiveness in improving generalization to unseen classes.\n", "link": "http://arxiv.org/abs/2502.11969v1", "date": "2025-02-17", "relevancy": 2.5474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Generalizable%20Prompt%20for%20CLIP%20with%20Class%20Similarity%20Knowledge&body=Title%3A%20Learning%20Generalizable%20Prompt%20for%20CLIP%20with%20Class%20Similarity%20Knowledge%0AAuthor%3A%20Sehun%20Jung%20and%20Hyang-won%20Lee%0AAbstract%3A%20%20%20In%20vision-language%20models%20%28VLMs%29%2C%20prompt%20tuning%20has%20shown%20its%20effectiveness%0Ain%20adapting%20models%20to%20downstream%20tasks.%20However%2C%20learned%20prompts%20struggle%20to%0Ageneralize%20to%20unseen%20classes%2C%20as%20they%20tend%20to%20overfit%20to%20the%20classes%20that%20are%0Atargeted%20during%20prompt%20tuning.%20Examining%20failure%20cases%2C%20we%20observed%20that%0Alearned%20prompts%20disrupt%20the%20semantics%20of%20unseen%20classes%2C%20generating%20text%0Aembeddings%20with%20incorrect%20semantic%20relationships%20among%20classes.%20To%20address%0Athis%2C%20we%20propose%20Similarity%20Alignment%20Regularization%20%28SAR%29%2C%20which%20regularizes%0Alearnable%20prompts%20to%20preserve%20the%20semantic%20relationships%20among%20classes%20captured%0Aby%20hand-crafted%20prompts.%20Specifically%2C%20we%20first%20obtain%20novel%20classes%20related%20to%0Abase%20classes%20using%20ChatGPT-4o%20and%20utilize%20them%20as%20potential%20unseen%20classes%0Aduring%20prompt%20tuning.%20Then%2C%20by%20targeting%20both%20base%20and%20novel%20classes%2C%20SAR%0Aaligns%20the%20similarity%20relationships%20among%20text%20embeddings%20generated%20by%0Alearnable%20prompts%20with%20the%20similarity%20relationships%20from%20hand-crafted%20prompts.%0AExtensive%20experiments%20applying%20SAR%20to%20existing%20prompt%20tuning%20methods%0Ademonstrate%20its%20effectiveness%20in%20improving%20generalization%20to%20unseen%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Generalizable%2520Prompt%2520for%2520CLIP%2520with%2520Class%2520Similarity%2520Knowledge%26entry.906535625%3DSehun%2520Jung%2520and%2520Hyang-won%2520Lee%26entry.1292438233%3D%2520%2520In%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520prompt%2520tuning%2520has%2520shown%2520its%2520effectiveness%250Ain%2520adapting%2520models%2520to%2520downstream%2520tasks.%2520However%252C%2520learned%2520prompts%2520struggle%2520to%250Ageneralize%2520to%2520unseen%2520classes%252C%2520as%2520they%2520tend%2520to%2520overfit%2520to%2520the%2520classes%2520that%2520are%250Atargeted%2520during%2520prompt%2520tuning.%2520Examining%2520failure%2520cases%252C%2520we%2520observed%2520that%250Alearned%2520prompts%2520disrupt%2520the%2520semantics%2520of%2520unseen%2520classes%252C%2520generating%2520text%250Aembeddings%2520with%2520incorrect%2520semantic%2520relationships%2520among%2520classes.%2520To%2520address%250Athis%252C%2520we%2520propose%2520Similarity%2520Alignment%2520Regularization%2520%2528SAR%2529%252C%2520which%2520regularizes%250Alearnable%2520prompts%2520to%2520preserve%2520the%2520semantic%2520relationships%2520among%2520classes%2520captured%250Aby%2520hand-crafted%2520prompts.%2520Specifically%252C%2520we%2520first%2520obtain%2520novel%2520classes%2520related%2520to%250Abase%2520classes%2520using%2520ChatGPT-4o%2520and%2520utilize%2520them%2520as%2520potential%2520unseen%2520classes%250Aduring%2520prompt%2520tuning.%2520Then%252C%2520by%2520targeting%2520both%2520base%2520and%2520novel%2520classes%252C%2520SAR%250Aaligns%2520the%2520similarity%2520relationships%2520among%2520text%2520embeddings%2520generated%2520by%250Alearnable%2520prompts%2520with%2520the%2520similarity%2520relationships%2520from%2520hand-crafted%2520prompts.%250AExtensive%2520experiments%2520applying%2520SAR%2520to%2520existing%2520prompt%2520tuning%2520methods%250Ademonstrate%2520its%2520effectiveness%2520in%2520improving%2520generalization%2520to%2520unseen%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Generalizable%20Prompt%20for%20CLIP%20with%20Class%20Similarity%20Knowledge&entry.906535625=Sehun%20Jung%20and%20Hyang-won%20Lee&entry.1292438233=%20%20In%20vision-language%20models%20%28VLMs%29%2C%20prompt%20tuning%20has%20shown%20its%20effectiveness%0Ain%20adapting%20models%20to%20downstream%20tasks.%20However%2C%20learned%20prompts%20struggle%20to%0Ageneralize%20to%20unseen%20classes%2C%20as%20they%20tend%20to%20overfit%20to%20the%20classes%20that%20are%0Atargeted%20during%20prompt%20tuning.%20Examining%20failure%20cases%2C%20we%20observed%20that%0Alearned%20prompts%20disrupt%20the%20semantics%20of%20unseen%20classes%2C%20generating%20text%0Aembeddings%20with%20incorrect%20semantic%20relationships%20among%20classes.%20To%20address%0Athis%2C%20we%20propose%20Similarity%20Alignment%20Regularization%20%28SAR%29%2C%20which%20regularizes%0Alearnable%20prompts%20to%20preserve%20the%20semantic%20relationships%20among%20classes%20captured%0Aby%20hand-crafted%20prompts.%20Specifically%2C%20we%20first%20obtain%20novel%20classes%20related%20to%0Abase%20classes%20using%20ChatGPT-4o%20and%20utilize%20them%20as%20potential%20unseen%20classes%0Aduring%20prompt%20tuning.%20Then%2C%20by%20targeting%20both%20base%20and%20novel%20classes%2C%20SAR%0Aaligns%20the%20similarity%20relationships%20among%20text%20embeddings%20generated%20by%0Alearnable%20prompts%20with%20the%20similarity%20relationships%20from%20hand-crafted%20prompts.%0AExtensive%20experiments%20applying%20SAR%20to%20existing%20prompt%20tuning%20methods%0Ademonstrate%20its%20effectiveness%20in%20improving%20generalization%20to%20unseen%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11969v1&entry.124074799=Read"},
{"title": "Low-Rank Thinning", "author": "Annabelle Michael Carrell and Albert Gong and Abhishek Shetty and Raaz Dwivedi and Lester Mackey", "abstract": "  The goal in thinning is to summarize a dataset using a small set of\nrepresentative points. Remarkably, sub-Gaussian thinning algorithms like Kernel\nHalving and Compress can match the quality of uniform subsampling while\nsubstantially reducing the number of summary points. However, existing\nguarantees cover only a restricted range of distributions and kernel-based\nquality measures and suffer from pessimistic dimension dependence. To address\nthese deficiencies, we introduce a new low-rank analysis of sub-Gaussian\nthinning that applies to any distribution and any kernel, guaranteeing\nhigh-quality compression whenever the kernel or data matrix is approximately\nlow-rank. To demonstrate the broad applicability of the techniques, we design\npractical sub-Gaussian thinning approaches that improve upon the best known\nguarantees for approximating attention in transformers, accelerating stochastic\ngradient training through reordering, and distinguishing distributions in\nnear-linear time.\n", "link": "http://arxiv.org/abs/2502.12063v1", "date": "2025-02-17", "relevancy": 2.5444, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5332}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5002}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Thinning&body=Title%3A%20Low-Rank%20Thinning%0AAuthor%3A%20Annabelle%20Michael%20Carrell%20and%20Albert%20Gong%20and%20Abhishek%20Shetty%20and%20Raaz%20Dwivedi%20and%20Lester%20Mackey%0AAbstract%3A%20%20%20The%20goal%20in%20thinning%20is%20to%20summarize%20a%20dataset%20using%20a%20small%20set%20of%0Arepresentative%20points.%20Remarkably%2C%20sub-Gaussian%20thinning%20algorithms%20like%20Kernel%0AHalving%20and%20Compress%20can%20match%20the%20quality%20of%20uniform%20subsampling%20while%0Asubstantially%20reducing%20the%20number%20of%20summary%20points.%20However%2C%20existing%0Aguarantees%20cover%20only%20a%20restricted%20range%20of%20distributions%20and%20kernel-based%0Aquality%20measures%20and%20suffer%20from%20pessimistic%20dimension%20dependence.%20To%20address%0Athese%20deficiencies%2C%20we%20introduce%20a%20new%20low-rank%20analysis%20of%20sub-Gaussian%0Athinning%20that%20applies%20to%20any%20distribution%20and%20any%20kernel%2C%20guaranteeing%0Ahigh-quality%20compression%20whenever%20the%20kernel%20or%20data%20matrix%20is%20approximately%0Alow-rank.%20To%20demonstrate%20the%20broad%20applicability%20of%20the%20techniques%2C%20we%20design%0Apractical%20sub-Gaussian%20thinning%20approaches%20that%20improve%20upon%20the%20best%20known%0Aguarantees%20for%20approximating%20attention%20in%20transformers%2C%20accelerating%20stochastic%0Agradient%20training%20through%20reordering%2C%20and%20distinguishing%20distributions%20in%0Anear-linear%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Thinning%26entry.906535625%3DAnnabelle%2520Michael%2520Carrell%2520and%2520Albert%2520Gong%2520and%2520Abhishek%2520Shetty%2520and%2520Raaz%2520Dwivedi%2520and%2520Lester%2520Mackey%26entry.1292438233%3D%2520%2520The%2520goal%2520in%2520thinning%2520is%2520to%2520summarize%2520a%2520dataset%2520using%2520a%2520small%2520set%2520of%250Arepresentative%2520points.%2520Remarkably%252C%2520sub-Gaussian%2520thinning%2520algorithms%2520like%2520Kernel%250AHalving%2520and%2520Compress%2520can%2520match%2520the%2520quality%2520of%2520uniform%2520subsampling%2520while%250Asubstantially%2520reducing%2520the%2520number%2520of%2520summary%2520points.%2520However%252C%2520existing%250Aguarantees%2520cover%2520only%2520a%2520restricted%2520range%2520of%2520distributions%2520and%2520kernel-based%250Aquality%2520measures%2520and%2520suffer%2520from%2520pessimistic%2520dimension%2520dependence.%2520To%2520address%250Athese%2520deficiencies%252C%2520we%2520introduce%2520a%2520new%2520low-rank%2520analysis%2520of%2520sub-Gaussian%250Athinning%2520that%2520applies%2520to%2520any%2520distribution%2520and%2520any%2520kernel%252C%2520guaranteeing%250Ahigh-quality%2520compression%2520whenever%2520the%2520kernel%2520or%2520data%2520matrix%2520is%2520approximately%250Alow-rank.%2520To%2520demonstrate%2520the%2520broad%2520applicability%2520of%2520the%2520techniques%252C%2520we%2520design%250Apractical%2520sub-Gaussian%2520thinning%2520approaches%2520that%2520improve%2520upon%2520the%2520best%2520known%250Aguarantees%2520for%2520approximating%2520attention%2520in%2520transformers%252C%2520accelerating%2520stochastic%250Agradient%2520training%2520through%2520reordering%252C%2520and%2520distinguishing%2520distributions%2520in%250Anear-linear%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Thinning&entry.906535625=Annabelle%20Michael%20Carrell%20and%20Albert%20Gong%20and%20Abhishek%20Shetty%20and%20Raaz%20Dwivedi%20and%20Lester%20Mackey&entry.1292438233=%20%20The%20goal%20in%20thinning%20is%20to%20summarize%20a%20dataset%20using%20a%20small%20set%20of%0Arepresentative%20points.%20Remarkably%2C%20sub-Gaussian%20thinning%20algorithms%20like%20Kernel%0AHalving%20and%20Compress%20can%20match%20the%20quality%20of%20uniform%20subsampling%20while%0Asubstantially%20reducing%20the%20number%20of%20summary%20points.%20However%2C%20existing%0Aguarantees%20cover%20only%20a%20restricted%20range%20of%20distributions%20and%20kernel-based%0Aquality%20measures%20and%20suffer%20from%20pessimistic%20dimension%20dependence.%20To%20address%0Athese%20deficiencies%2C%20we%20introduce%20a%20new%20low-rank%20analysis%20of%20sub-Gaussian%0Athinning%20that%20applies%20to%20any%20distribution%20and%20any%20kernel%2C%20guaranteeing%0Ahigh-quality%20compression%20whenever%20the%20kernel%20or%20data%20matrix%20is%20approximately%0Alow-rank.%20To%20demonstrate%20the%20broad%20applicability%20of%20the%20techniques%2C%20we%20design%0Apractical%20sub-Gaussian%20thinning%20approaches%20that%20improve%20upon%20the%20best%20known%0Aguarantees%20for%20approximating%20attention%20in%20transformers%2C%20accelerating%20stochastic%0Agradient%20training%20through%20reordering%2C%20and%20distinguishing%20distributions%20in%0Anear-linear%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12063v1&entry.124074799=Read"},
{"title": "Model Generalization on Text Attribute Graphs: Principles with Large\n  Language Models", "author": "Haoyu Wang and Shikun Liu and Rongzhe Wei and Pan Li", "abstract": "  Large language models (LLMs) have recently been introduced to graph learning,\naiming to extend their zero-shot generalization success to tasks where labeled\ngraph data is scarce. Among these applications, inference over text-attributed\ngraphs (TAGs) presents unique challenges: existing methods struggle with LLMs'\nlimited context length for processing large node neighborhoods and the\nmisalignment between node embeddings and the LLM token space. To address these\nissues, we establish two key principles for ensuring generalization and derive\nthe framework LLM-BP accordingly: (1) Unifying the attribute space with\ntask-adaptive embeddings, where we leverage LLM-based encoders and task-aware\nprompting to enhance generalization of the text attribute embeddings; (2)\nDeveloping a generalizable graph information aggregation mechanism, for which\nwe adopt belief propagation with LLM-estimated parameters that adapt across\ngraphs. Evaluations on 11 real-world TAG benchmarks demonstrate that LLM-BP\nsignificantly outperforms existing approaches, achieving 8.10% improvement with\ntask-conditional embeddings and an additional 1.71% gain from adaptive\naggregation.\n", "link": "http://arxiv.org/abs/2502.11836v1", "date": "2025-02-17", "relevancy": 2.5283, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Generalization%20on%20Text%20Attribute%20Graphs%3A%20Principles%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Model%20Generalization%20on%20Text%20Attribute%20Graphs%3A%20Principles%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Haoyu%20Wang%20and%20Shikun%20Liu%20and%20Rongzhe%20Wei%20and%20Pan%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20been%20introduced%20to%20graph%20learning%2C%0Aaiming%20to%20extend%20their%20zero-shot%20generalization%20success%20to%20tasks%20where%20labeled%0Agraph%20data%20is%20scarce.%20Among%20these%20applications%2C%20inference%20over%20text-attributed%0Agraphs%20%28TAGs%29%20presents%20unique%20challenges%3A%20existing%20methods%20struggle%20with%20LLMs%27%0Alimited%20context%20length%20for%20processing%20large%20node%20neighborhoods%20and%20the%0Amisalignment%20between%20node%20embeddings%20and%20the%20LLM%20token%20space.%20To%20address%20these%0Aissues%2C%20we%20establish%20two%20key%20principles%20for%20ensuring%20generalization%20and%20derive%0Athe%20framework%20LLM-BP%20accordingly%3A%20%281%29%20Unifying%20the%20attribute%20space%20with%0Atask-adaptive%20embeddings%2C%20where%20we%20leverage%20LLM-based%20encoders%20and%20task-aware%0Aprompting%20to%20enhance%20generalization%20of%20the%20text%20attribute%20embeddings%3B%20%282%29%0ADeveloping%20a%20generalizable%20graph%20information%20aggregation%20mechanism%2C%20for%20which%0Awe%20adopt%20belief%20propagation%20with%20LLM-estimated%20parameters%20that%20adapt%20across%0Agraphs.%20Evaluations%20on%2011%20real-world%20TAG%20benchmarks%20demonstrate%20that%20LLM-BP%0Asignificantly%20outperforms%20existing%20approaches%2C%20achieving%208.10%25%20improvement%20with%0Atask-conditional%20embeddings%20and%20an%20additional%201.71%25%20gain%20from%20adaptive%0Aaggregation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Generalization%2520on%2520Text%2520Attribute%2520Graphs%253A%2520Principles%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DHaoyu%2520Wang%2520and%2520Shikun%2520Liu%2520and%2520Rongzhe%2520Wei%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520been%2520introduced%2520to%2520graph%2520learning%252C%250Aaiming%2520to%2520extend%2520their%2520zero-shot%2520generalization%2520success%2520to%2520tasks%2520where%2520labeled%250Agraph%2520data%2520is%2520scarce.%2520Among%2520these%2520applications%252C%2520inference%2520over%2520text-attributed%250Agraphs%2520%2528TAGs%2529%2520presents%2520unique%2520challenges%253A%2520existing%2520methods%2520struggle%2520with%2520LLMs%2527%250Alimited%2520context%2520length%2520for%2520processing%2520large%2520node%2520neighborhoods%2520and%2520the%250Amisalignment%2520between%2520node%2520embeddings%2520and%2520the%2520LLM%2520token%2520space.%2520To%2520address%2520these%250Aissues%252C%2520we%2520establish%2520two%2520key%2520principles%2520for%2520ensuring%2520generalization%2520and%2520derive%250Athe%2520framework%2520LLM-BP%2520accordingly%253A%2520%25281%2529%2520Unifying%2520the%2520attribute%2520space%2520with%250Atask-adaptive%2520embeddings%252C%2520where%2520we%2520leverage%2520LLM-based%2520encoders%2520and%2520task-aware%250Aprompting%2520to%2520enhance%2520generalization%2520of%2520the%2520text%2520attribute%2520embeddings%253B%2520%25282%2529%250ADeveloping%2520a%2520generalizable%2520graph%2520information%2520aggregation%2520mechanism%252C%2520for%2520which%250Awe%2520adopt%2520belief%2520propagation%2520with%2520LLM-estimated%2520parameters%2520that%2520adapt%2520across%250Agraphs.%2520Evaluations%2520on%252011%2520real-world%2520TAG%2520benchmarks%2520demonstrate%2520that%2520LLM-BP%250Asignificantly%2520outperforms%2520existing%2520approaches%252C%2520achieving%25208.10%2525%2520improvement%2520with%250Atask-conditional%2520embeddings%2520and%2520an%2520additional%25201.71%2525%2520gain%2520from%2520adaptive%250Aaggregation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Generalization%20on%20Text%20Attribute%20Graphs%3A%20Principles%20with%20Large%0A%20%20Language%20Models&entry.906535625=Haoyu%20Wang%20and%20Shikun%20Liu%20and%20Rongzhe%20Wei%20and%20Pan%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20been%20introduced%20to%20graph%20learning%2C%0Aaiming%20to%20extend%20their%20zero-shot%20generalization%20success%20to%20tasks%20where%20labeled%0Agraph%20data%20is%20scarce.%20Among%20these%20applications%2C%20inference%20over%20text-attributed%0Agraphs%20%28TAGs%29%20presents%20unique%20challenges%3A%20existing%20methods%20struggle%20with%20LLMs%27%0Alimited%20context%20length%20for%20processing%20large%20node%20neighborhoods%20and%20the%0Amisalignment%20between%20node%20embeddings%20and%20the%20LLM%20token%20space.%20To%20address%20these%0Aissues%2C%20we%20establish%20two%20key%20principles%20for%20ensuring%20generalization%20and%20derive%0Athe%20framework%20LLM-BP%20accordingly%3A%20%281%29%20Unifying%20the%20attribute%20space%20with%0Atask-adaptive%20embeddings%2C%20where%20we%20leverage%20LLM-based%20encoders%20and%20task-aware%0Aprompting%20to%20enhance%20generalization%20of%20the%20text%20attribute%20embeddings%3B%20%282%29%0ADeveloping%20a%20generalizable%20graph%20information%20aggregation%20mechanism%2C%20for%20which%0Awe%20adopt%20belief%20propagation%20with%20LLM-estimated%20parameters%20that%20adapt%20across%0Agraphs.%20Evaluations%20on%2011%20real-world%20TAG%20benchmarks%20demonstrate%20that%20LLM-BP%0Asignificantly%20outperforms%20existing%20approaches%2C%20achieving%208.10%25%20improvement%20with%0Atask-conditional%20embeddings%20and%20an%20additional%201.71%25%20gain%20from%20adaptive%0Aaggregation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11836v1&entry.124074799=Read"},
{"title": "SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through\n  Hierarchical Evaluation", "author": "Wenyu Zhang and Wei En Ng and Lixin Ma and Yuwen Wang and Jungqi Zhao and Allison Koenecke and Boyang Li and Lu Wang", "abstract": "  Current vision-language models may grasp basic spatial cues and simple\ndirections (e.g. left, right, front, back), but struggle with the\nmulti-dimensional spatial reasoning necessary for human-like understanding and\nreal-world applications. To address this gap, we develop SPHERE (Spatial\nPerception and Hierarchical Evaluation of REasoning), a hierarchical evaluation\nframework supported by a new human-annotated dataset. SPHERE systematically\nprobes models across increasing levels of complexity, from fundamental skills\nto multi-skill integration and high-level reasoning that combines spatial,\nvisual, and logical understanding. Benchmark evaluation of state-of-the-art\nmodels reveals significant deficiencies, especially in reasoning about distance\nand proximity, understanding both egocentric and allocentric perspectives, and\napplying spatial logic in physical contexts. These findings expose critical\nblind spots in existing models and underscore the need for more advanced\nspatial reasoning techniques, driving the development of vision-language models\nthat align more closely with human spatial cognition. The dataset will be\nopen-sourced upon publication.\n", "link": "http://arxiv.org/abs/2412.12693v2", "date": "2025-02-17", "relevancy": 2.5201, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6459}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPHERE%3A%20Unveiling%20Spatial%20Blind%20Spots%20in%20Vision-Language%20Models%20Through%0A%20%20Hierarchical%20Evaluation&body=Title%3A%20SPHERE%3A%20Unveiling%20Spatial%20Blind%20Spots%20in%20Vision-Language%20Models%20Through%0A%20%20Hierarchical%20Evaluation%0AAuthor%3A%20Wenyu%20Zhang%20and%20Wei%20En%20Ng%20and%20Lixin%20Ma%20and%20Yuwen%20Wang%20and%20Jungqi%20Zhao%20and%20Allison%20Koenecke%20and%20Boyang%20Li%20and%20Lu%20Wang%0AAbstract%3A%20%20%20Current%20vision-language%20models%20may%20grasp%20basic%20spatial%20cues%20and%20simple%0Adirections%20%28e.g.%20left%2C%20right%2C%20front%2C%20back%29%2C%20but%20struggle%20with%20the%0Amulti-dimensional%20spatial%20reasoning%20necessary%20for%20human-like%20understanding%20and%0Areal-world%20applications.%20To%20address%20this%20gap%2C%20we%20develop%20SPHERE%20%28Spatial%0APerception%20and%20Hierarchical%20Evaluation%20of%20REasoning%29%2C%20a%20hierarchical%20evaluation%0Aframework%20supported%20by%20a%20new%20human-annotated%20dataset.%20SPHERE%20systematically%0Aprobes%20models%20across%20increasing%20levels%20of%20complexity%2C%20from%20fundamental%20skills%0Ato%20multi-skill%20integration%20and%20high-level%20reasoning%20that%20combines%20spatial%2C%0Avisual%2C%20and%20logical%20understanding.%20Benchmark%20evaluation%20of%20state-of-the-art%0Amodels%20reveals%20significant%20deficiencies%2C%20especially%20in%20reasoning%20about%20distance%0Aand%20proximity%2C%20understanding%20both%20egocentric%20and%20allocentric%20perspectives%2C%20and%0Aapplying%20spatial%20logic%20in%20physical%20contexts.%20These%20findings%20expose%20critical%0Ablind%20spots%20in%20existing%20models%20and%20underscore%20the%20need%20for%20more%20advanced%0Aspatial%20reasoning%20techniques%2C%20driving%20the%20development%20of%20vision-language%20models%0Athat%20align%20more%20closely%20with%20human%20spatial%20cognition.%20The%20dataset%20will%20be%0Aopen-sourced%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPHERE%253A%2520Unveiling%2520Spatial%2520Blind%2520Spots%2520in%2520Vision-Language%2520Models%2520Through%250A%2520%2520Hierarchical%2520Evaluation%26entry.906535625%3DWenyu%2520Zhang%2520and%2520Wei%2520En%2520Ng%2520and%2520Lixin%2520Ma%2520and%2520Yuwen%2520Wang%2520and%2520Jungqi%2520Zhao%2520and%2520Allison%2520Koenecke%2520and%2520Boyang%2520Li%2520and%2520Lu%2520Wang%26entry.1292438233%3D%2520%2520Current%2520vision-language%2520models%2520may%2520grasp%2520basic%2520spatial%2520cues%2520and%2520simple%250Adirections%2520%2528e.g.%2520left%252C%2520right%252C%2520front%252C%2520back%2529%252C%2520but%2520struggle%2520with%2520the%250Amulti-dimensional%2520spatial%2520reasoning%2520necessary%2520for%2520human-like%2520understanding%2520and%250Areal-world%2520applications.%2520To%2520address%2520this%2520gap%252C%2520we%2520develop%2520SPHERE%2520%2528Spatial%250APerception%2520and%2520Hierarchical%2520Evaluation%2520of%2520REasoning%2529%252C%2520a%2520hierarchical%2520evaluation%250Aframework%2520supported%2520by%2520a%2520new%2520human-annotated%2520dataset.%2520SPHERE%2520systematically%250Aprobes%2520models%2520across%2520increasing%2520levels%2520of%2520complexity%252C%2520from%2520fundamental%2520skills%250Ato%2520multi-skill%2520integration%2520and%2520high-level%2520reasoning%2520that%2520combines%2520spatial%252C%250Avisual%252C%2520and%2520logical%2520understanding.%2520Benchmark%2520evaluation%2520of%2520state-of-the-art%250Amodels%2520reveals%2520significant%2520deficiencies%252C%2520especially%2520in%2520reasoning%2520about%2520distance%250Aand%2520proximity%252C%2520understanding%2520both%2520egocentric%2520and%2520allocentric%2520perspectives%252C%2520and%250Aapplying%2520spatial%2520logic%2520in%2520physical%2520contexts.%2520These%2520findings%2520expose%2520critical%250Ablind%2520spots%2520in%2520existing%2520models%2520and%2520underscore%2520the%2520need%2520for%2520more%2520advanced%250Aspatial%2520reasoning%2520techniques%252C%2520driving%2520the%2520development%2520of%2520vision-language%2520models%250Athat%2520align%2520more%2520closely%2520with%2520human%2520spatial%2520cognition.%2520The%2520dataset%2520will%2520be%250Aopen-sourced%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPHERE%3A%20Unveiling%20Spatial%20Blind%20Spots%20in%20Vision-Language%20Models%20Through%0A%20%20Hierarchical%20Evaluation&entry.906535625=Wenyu%20Zhang%20and%20Wei%20En%20Ng%20and%20Lixin%20Ma%20and%20Yuwen%20Wang%20and%20Jungqi%20Zhao%20and%20Allison%20Koenecke%20and%20Boyang%20Li%20and%20Lu%20Wang&entry.1292438233=%20%20Current%20vision-language%20models%20may%20grasp%20basic%20spatial%20cues%20and%20simple%0Adirections%20%28e.g.%20left%2C%20right%2C%20front%2C%20back%29%2C%20but%20struggle%20with%20the%0Amulti-dimensional%20spatial%20reasoning%20necessary%20for%20human-like%20understanding%20and%0Areal-world%20applications.%20To%20address%20this%20gap%2C%20we%20develop%20SPHERE%20%28Spatial%0APerception%20and%20Hierarchical%20Evaluation%20of%20REasoning%29%2C%20a%20hierarchical%20evaluation%0Aframework%20supported%20by%20a%20new%20human-annotated%20dataset.%20SPHERE%20systematically%0Aprobes%20models%20across%20increasing%20levels%20of%20complexity%2C%20from%20fundamental%20skills%0Ato%20multi-skill%20integration%20and%20high-level%20reasoning%20that%20combines%20spatial%2C%0Avisual%2C%20and%20logical%20understanding.%20Benchmark%20evaluation%20of%20state-of-the-art%0Amodels%20reveals%20significant%20deficiencies%2C%20especially%20in%20reasoning%20about%20distance%0Aand%20proximity%2C%20understanding%20both%20egocentric%20and%20allocentric%20perspectives%2C%20and%0Aapplying%20spatial%20logic%20in%20physical%20contexts.%20These%20findings%20expose%20critical%0Ablind%20spots%20in%20existing%20models%20and%20underscore%20the%20need%20for%20more%20advanced%0Aspatial%20reasoning%20techniques%2C%20driving%20the%20development%20of%20vision-language%20models%0Athat%20align%20more%20closely%20with%20human%20spatial%20cognition.%20The%20dataset%20will%20be%0Aopen-sourced%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12693v2&entry.124074799=Read"},
{"title": "ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition", "author": "Muhammad Waseem Akram and Stefano Dettori and Valentina Colla and Giorgio Carlo Buttazzo", "abstract": "  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n", "link": "http://arxiv.org/abs/2502.11840v1", "date": "2025-02-17", "relevancy": 2.5194, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChordFormer%3A%20A%20Conformer-Based%20Architecture%20for%20Large-Vocabulary%20Audio%0A%20%20Chord%20Recognition&body=Title%3A%20ChordFormer%3A%20A%20Conformer-Based%20Architecture%20for%20Large-Vocabulary%20Audio%0A%20%20Chord%20Recognition%0AAuthor%3A%20Muhammad%20Waseem%20Akram%20and%20Stefano%20Dettori%20and%20Valentina%20Colla%20and%20Giorgio%20Carlo%20Buttazzo%0AAbstract%3A%20%20%20Chord%20recognition%20serves%20as%20a%20critical%20task%20in%20music%20information%20retrieval%0Adue%20to%20the%20abstract%20and%20descriptive%20nature%20of%20chords%20in%20music%20analysis.%20While%0Aaudio%20chord%20recognition%20systems%20have%20achieved%20significant%20accuracy%20for%20small%0Avocabularies%20%28e.g.%2C%20major/minor%20chords%29%2C%20large-vocabulary%20chord%20recognition%0Aremains%20a%20challenging%20problem.%20This%20complexity%20also%20arises%20from%20the%20inherent%0Along-tail%20distribution%20of%20chords%2C%20where%20rare%20chord%20types%20are%20underrepresented%0Ain%20most%20datasets%2C%20leading%20to%20insufficient%20training%20samples.%20Effective%20chord%0Arecognition%20requires%20leveraging%20contextual%20information%20from%20audio%20sequences%2C%0Ayet%20existing%20models%2C%20such%20as%20combinations%20of%20convolutional%20neural%20networks%2C%0Abidirectional%20long%20short-term%20memory%20networks%2C%20and%20bidirectional%20transformers%2C%0Aface%20limitations%20in%20capturing%20long-term%20dependencies%20and%20exhibit%20suboptimal%0Aperformance%20on%20large-vocabulary%20chord%20recognition%20tasks.%20This%20work%20proposes%0AChordFormer%2C%20a%20novel%20conformer-based%20architecture%20designed%20to%20tackle%20structural%0Achord%20recognition%20%28e.g.%2C%20triads%2C%20bass%2C%20sevenths%29%20for%20large%20vocabularies.%0AChordFormer%20leverages%20conformer%20blocks%20that%20integrate%20convolutional%20neural%0Anetworks%20with%20transformers%2C%20thus%20enabling%20the%20model%20to%20capture%20both%20local%0Apatterns%20and%20global%20dependencies%20effectively.%20By%20addressing%20challenges%20such%20as%0Aclass%20imbalance%20through%20a%20reweighted%20loss%20function%20and%20structured%20chord%0Arepresentations%2C%20ChordFormer%20outperforms%20state-of-the-art%20models%2C%20achieving%20a%0A2%25%20improvement%20in%20frame-wise%20accuracy%20and%20a%206%25%20increase%20in%20class-wise%20accuracy%0Aon%20large-vocabulary%20chord%20datasets.%20Furthermore%2C%20ChordFormer%20excels%20in%20handling%0Aclass%20imbalance%2C%20providing%20robust%20and%20balanced%20recognition%20across%20chord%20types.%0AThis%20approach%20bridges%20the%20gap%20between%20theoretical%20music%20knowledge%20and%20practical%0Aapplications%2C%20advancing%20the%20field%20of%20large-vocabulary%20chord%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChordFormer%253A%2520A%2520Conformer-Based%2520Architecture%2520for%2520Large-Vocabulary%2520Audio%250A%2520%2520Chord%2520Recognition%26entry.906535625%3DMuhammad%2520Waseem%2520Akram%2520and%2520Stefano%2520Dettori%2520and%2520Valentina%2520Colla%2520and%2520Giorgio%2520Carlo%2520Buttazzo%26entry.1292438233%3D%2520%2520Chord%2520recognition%2520serves%2520as%2520a%2520critical%2520task%2520in%2520music%2520information%2520retrieval%250Adue%2520to%2520the%2520abstract%2520and%2520descriptive%2520nature%2520of%2520chords%2520in%2520music%2520analysis.%2520While%250Aaudio%2520chord%2520recognition%2520systems%2520have%2520achieved%2520significant%2520accuracy%2520for%2520small%250Avocabularies%2520%2528e.g.%252C%2520major/minor%2520chords%2529%252C%2520large-vocabulary%2520chord%2520recognition%250Aremains%2520a%2520challenging%2520problem.%2520This%2520complexity%2520also%2520arises%2520from%2520the%2520inherent%250Along-tail%2520distribution%2520of%2520chords%252C%2520where%2520rare%2520chord%2520types%2520are%2520underrepresented%250Ain%2520most%2520datasets%252C%2520leading%2520to%2520insufficient%2520training%2520samples.%2520Effective%2520chord%250Arecognition%2520requires%2520leveraging%2520contextual%2520information%2520from%2520audio%2520sequences%252C%250Ayet%2520existing%2520models%252C%2520such%2520as%2520combinations%2520of%2520convolutional%2520neural%2520networks%252C%250Abidirectional%2520long%2520short-term%2520memory%2520networks%252C%2520and%2520bidirectional%2520transformers%252C%250Aface%2520limitations%2520in%2520capturing%2520long-term%2520dependencies%2520and%2520exhibit%2520suboptimal%250Aperformance%2520on%2520large-vocabulary%2520chord%2520recognition%2520tasks.%2520This%2520work%2520proposes%250AChordFormer%252C%2520a%2520novel%2520conformer-based%2520architecture%2520designed%2520to%2520tackle%2520structural%250Achord%2520recognition%2520%2528e.g.%252C%2520triads%252C%2520bass%252C%2520sevenths%2529%2520for%2520large%2520vocabularies.%250AChordFormer%2520leverages%2520conformer%2520blocks%2520that%2520integrate%2520convolutional%2520neural%250Anetworks%2520with%2520transformers%252C%2520thus%2520enabling%2520the%2520model%2520to%2520capture%2520both%2520local%250Apatterns%2520and%2520global%2520dependencies%2520effectively.%2520By%2520addressing%2520challenges%2520such%2520as%250Aclass%2520imbalance%2520through%2520a%2520reweighted%2520loss%2520function%2520and%2520structured%2520chord%250Arepresentations%252C%2520ChordFormer%2520outperforms%2520state-of-the-art%2520models%252C%2520achieving%2520a%250A2%2525%2520improvement%2520in%2520frame-wise%2520accuracy%2520and%2520a%25206%2525%2520increase%2520in%2520class-wise%2520accuracy%250Aon%2520large-vocabulary%2520chord%2520datasets.%2520Furthermore%252C%2520ChordFormer%2520excels%2520in%2520handling%250Aclass%2520imbalance%252C%2520providing%2520robust%2520and%2520balanced%2520recognition%2520across%2520chord%2520types.%250AThis%2520approach%2520bridges%2520the%2520gap%2520between%2520theoretical%2520music%2520knowledge%2520and%2520practical%250Aapplications%252C%2520advancing%2520the%2520field%2520of%2520large-vocabulary%2520chord%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChordFormer%3A%20A%20Conformer-Based%20Architecture%20for%20Large-Vocabulary%20Audio%0A%20%20Chord%20Recognition&entry.906535625=Muhammad%20Waseem%20Akram%20and%20Stefano%20Dettori%20and%20Valentina%20Colla%20and%20Giorgio%20Carlo%20Buttazzo&entry.1292438233=%20%20Chord%20recognition%20serves%20as%20a%20critical%20task%20in%20music%20information%20retrieval%0Adue%20to%20the%20abstract%20and%20descriptive%20nature%20of%20chords%20in%20music%20analysis.%20While%0Aaudio%20chord%20recognition%20systems%20have%20achieved%20significant%20accuracy%20for%20small%0Avocabularies%20%28e.g.%2C%20major/minor%20chords%29%2C%20large-vocabulary%20chord%20recognition%0Aremains%20a%20challenging%20problem.%20This%20complexity%20also%20arises%20from%20the%20inherent%0Along-tail%20distribution%20of%20chords%2C%20where%20rare%20chord%20types%20are%20underrepresented%0Ain%20most%20datasets%2C%20leading%20to%20insufficient%20training%20samples.%20Effective%20chord%0Arecognition%20requires%20leveraging%20contextual%20information%20from%20audio%20sequences%2C%0Ayet%20existing%20models%2C%20such%20as%20combinations%20of%20convolutional%20neural%20networks%2C%0Abidirectional%20long%20short-term%20memory%20networks%2C%20and%20bidirectional%20transformers%2C%0Aface%20limitations%20in%20capturing%20long-term%20dependencies%20and%20exhibit%20suboptimal%0Aperformance%20on%20large-vocabulary%20chord%20recognition%20tasks.%20This%20work%20proposes%0AChordFormer%2C%20a%20novel%20conformer-based%20architecture%20designed%20to%20tackle%20structural%0Achord%20recognition%20%28e.g.%2C%20triads%2C%20bass%2C%20sevenths%29%20for%20large%20vocabularies.%0AChordFormer%20leverages%20conformer%20blocks%20that%20integrate%20convolutional%20neural%0Anetworks%20with%20transformers%2C%20thus%20enabling%20the%20model%20to%20capture%20both%20local%0Apatterns%20and%20global%20dependencies%20effectively.%20By%20addressing%20challenges%20such%20as%0Aclass%20imbalance%20through%20a%20reweighted%20loss%20function%20and%20structured%20chord%0Arepresentations%2C%20ChordFormer%20outperforms%20state-of-the-art%20models%2C%20achieving%20a%0A2%25%20improvement%20in%20frame-wise%20accuracy%20and%20a%206%25%20increase%20in%20class-wise%20accuracy%0Aon%20large-vocabulary%20chord%20datasets.%20Furthermore%2C%20ChordFormer%20excels%20in%20handling%0Aclass%20imbalance%2C%20providing%20robust%20and%20balanced%20recognition%20across%20chord%20types.%0AThis%20approach%20bridges%20the%20gap%20between%20theoretical%20music%20knowledge%20and%20practical%0Aapplications%2C%20advancing%20the%20field%20of%20large-vocabulary%20chord%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11840v1&entry.124074799=Read"},
{"title": "Adapting Language-Specific LLMs to a Reasoning Model in One Day via\n  Model Merging - An Open Recipe", "author": "Kunat Pipatanakul and Pittawat Taveekitworachai and Potsawee Manakul and Kasima Tharnpipitchai", "abstract": "  This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.\n", "link": "http://arxiv.org/abs/2502.09056v2", "date": "2025-02-17", "relevancy": 2.5143, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Language-Specific%20LLMs%20to%20a%20Reasoning%20Model%20in%20One%20Day%20via%0A%20%20Model%20Merging%20-%20An%20Open%20Recipe&body=Title%3A%20Adapting%20Language-Specific%20LLMs%20to%20a%20Reasoning%20Model%20in%20One%20Day%20via%0A%20%20Model%20Merging%20-%20An%20Open%20Recipe%0AAuthor%3A%20Kunat%20Pipatanakul%20and%20Pittawat%20Taveekitworachai%20and%20Potsawee%20Manakul%20and%20Kasima%20Tharnpipitchai%0AAbstract%3A%20%20%20This%20paper%20investigates%20data%20selection%20and%20model%20merging%20methodologies%20aimed%0Aat%20incorporating%20advanced%20reasoning%20capabilities%20such%20as%20those%20of%20DeepSeek%20R1%0Ainto%20language-specific%20large%20language%20models%20%28LLMs%29%2C%20with%20a%20particular%20focus%20on%0Athe%20Thai%20LLM.%20Our%20goal%20is%20to%20enhance%20the%20reasoning%20capabilities%20of%0Alanguage-specific%20LLMs%20while%20maintaining%20their%20target%20language%20abilities.%0ADeepSeek%20R1%20excels%20in%20reasoning%20but%20primarily%20benefits%20high-resource%20languages%0Asuch%20as%20English%20and%20Chinese.%20However%2C%20low-resource%20languages%20remain%20underserved%0Adue%20to%20the%20dominance%20of%20English-centric%20training%20data%20and%20model%20optimizations%2C%0Awhich%20limit%20performance%20in%20these%20languages.%20This%20limitation%20results%20in%0Aunreliable%20code-switching%20and%20diminished%20effectiveness%20on%20tasks%20in%20low-resource%0Alanguages.%20Meanwhile%2C%20local%20and%20regional%20LLM%20initiatives%20have%20attempted%20to%0Abridge%20this%20gap%20by%20developing%20language-specific%20LLMs%20that%20focus%20on%20improving%0Alocal%20linguistic%20fidelity.%20We%20demonstrate%20that%2C%20with%20only%20publicly%20available%0Adatasets%20and%20a%20computational%20budget%20of%20%24120%2C%20it%20is%20possible%20to%20enhance%20the%0Areasoning%20capabilities%20of%20language-specific%20LLMs%20to%20match%20the%20level%20of%20DeepSeek%0AR1%2C%20without%20compromising%20their%20performance%20on%20target%20language%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09056v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Language-Specific%2520LLMs%2520to%2520a%2520Reasoning%2520Model%2520in%2520One%2520Day%2520via%250A%2520%2520Model%2520Merging%2520-%2520An%2520Open%2520Recipe%26entry.906535625%3DKunat%2520Pipatanakul%2520and%2520Pittawat%2520Taveekitworachai%2520and%2520Potsawee%2520Manakul%2520and%2520Kasima%2520Tharnpipitchai%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520data%2520selection%2520and%2520model%2520merging%2520methodologies%2520aimed%250Aat%2520incorporating%2520advanced%2520reasoning%2520capabilities%2520such%2520as%2520those%2520of%2520DeepSeek%2520R1%250Ainto%2520language-specific%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%2520a%2520particular%2520focus%2520on%250Athe%2520Thai%2520LLM.%2520Our%2520goal%2520is%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%250Alanguage-specific%2520LLMs%2520while%2520maintaining%2520their%2520target%2520language%2520abilities.%250ADeepSeek%2520R1%2520excels%2520in%2520reasoning%2520but%2520primarily%2520benefits%2520high-resource%2520languages%250Asuch%2520as%2520English%2520and%2520Chinese.%2520However%252C%2520low-resource%2520languages%2520remain%2520underserved%250Adue%2520to%2520the%2520dominance%2520of%2520English-centric%2520training%2520data%2520and%2520model%2520optimizations%252C%250Awhich%2520limit%2520performance%2520in%2520these%2520languages.%2520This%2520limitation%2520results%2520in%250Aunreliable%2520code-switching%2520and%2520diminished%2520effectiveness%2520on%2520tasks%2520in%2520low-resource%250Alanguages.%2520Meanwhile%252C%2520local%2520and%2520regional%2520LLM%2520initiatives%2520have%2520attempted%2520to%250Abridge%2520this%2520gap%2520by%2520developing%2520language-specific%2520LLMs%2520that%2520focus%2520on%2520improving%250Alocal%2520linguistic%2520fidelity.%2520We%2520demonstrate%2520that%252C%2520with%2520only%2520publicly%2520available%250Adatasets%2520and%2520a%2520computational%2520budget%2520of%2520%2524120%252C%2520it%2520is%2520possible%2520to%2520enhance%2520the%250Areasoning%2520capabilities%2520of%2520language-specific%2520LLMs%2520to%2520match%2520the%2520level%2520of%2520DeepSeek%250AR1%252C%2520without%2520compromising%2520their%2520performance%2520on%2520target%2520language%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09056v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Language-Specific%20LLMs%20to%20a%20Reasoning%20Model%20in%20One%20Day%20via%0A%20%20Model%20Merging%20-%20An%20Open%20Recipe&entry.906535625=Kunat%20Pipatanakul%20and%20Pittawat%20Taveekitworachai%20and%20Potsawee%20Manakul%20and%20Kasima%20Tharnpipitchai&entry.1292438233=%20%20This%20paper%20investigates%20data%20selection%20and%20model%20merging%20methodologies%20aimed%0Aat%20incorporating%20advanced%20reasoning%20capabilities%20such%20as%20those%20of%20DeepSeek%20R1%0Ainto%20language-specific%20large%20language%20models%20%28LLMs%29%2C%20with%20a%20particular%20focus%20on%0Athe%20Thai%20LLM.%20Our%20goal%20is%20to%20enhance%20the%20reasoning%20capabilities%20of%0Alanguage-specific%20LLMs%20while%20maintaining%20their%20target%20language%20abilities.%0ADeepSeek%20R1%20excels%20in%20reasoning%20but%20primarily%20benefits%20high-resource%20languages%0Asuch%20as%20English%20and%20Chinese.%20However%2C%20low-resource%20languages%20remain%20underserved%0Adue%20to%20the%20dominance%20of%20English-centric%20training%20data%20and%20model%20optimizations%2C%0Awhich%20limit%20performance%20in%20these%20languages.%20This%20limitation%20results%20in%0Aunreliable%20code-switching%20and%20diminished%20effectiveness%20on%20tasks%20in%20low-resource%0Alanguages.%20Meanwhile%2C%20local%20and%20regional%20LLM%20initiatives%20have%20attempted%20to%0Abridge%20this%20gap%20by%20developing%20language-specific%20LLMs%20that%20focus%20on%20improving%0Alocal%20linguistic%20fidelity.%20We%20demonstrate%20that%2C%20with%20only%20publicly%20available%0Adatasets%20and%20a%20computational%20budget%20of%20%24120%2C%20it%20is%20possible%20to%20enhance%20the%0Areasoning%20capabilities%20of%20language-specific%20LLMs%20to%20match%20the%20level%20of%20DeepSeek%0AR1%2C%20without%20compromising%20their%20performance%20on%20target%20language%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09056v2&entry.124074799=Read"},
{"title": "Merging Language and Domain Specific Models: The Impact on Technical\n  Vocabulary Acquisition", "author": "Thibault Rousset and Taisei Kakibuchi and Yusuke Sasaki and Yoshihide Nomura", "abstract": "  This paper investigates the integration of technical vocabulary in merged\nlanguage models. We explore the knowledge transfer mechanisms involved when\ncombining a general-purpose language-specific model with a domain-specific\nmodel, focusing on the resulting model's comprehension of technical jargon. Our\nexperiments analyze the impact of this merging process on the target model's\nproficiency in handling specialized terminology. We present a quantitative\nevaluation of the performance of the merged model, comparing it with that of\nthe individual constituent models. The findings offer insights into the\neffectiveness of different model merging methods for enhancing domain-specific\nknowledge and highlight potential challenges and future directions in\nleveraging these methods for cross-lingual knowledge transfer in Natural\nLanguage Processing.\n", "link": "http://arxiv.org/abs/2502.12001v1", "date": "2025-02-17", "relevancy": 2.5107, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Merging%20Language%20and%20Domain%20Specific%20Models%3A%20The%20Impact%20on%20Technical%0A%20%20Vocabulary%20Acquisition&body=Title%3A%20Merging%20Language%20and%20Domain%20Specific%20Models%3A%20The%20Impact%20on%20Technical%0A%20%20Vocabulary%20Acquisition%0AAuthor%3A%20Thibault%20Rousset%20and%20Taisei%20Kakibuchi%20and%20Yusuke%20Sasaki%20and%20Yoshihide%20Nomura%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20integration%20of%20technical%20vocabulary%20in%20merged%0Alanguage%20models.%20We%20explore%20the%20knowledge%20transfer%20mechanisms%20involved%20when%0Acombining%20a%20general-purpose%20language-specific%20model%20with%20a%20domain-specific%0Amodel%2C%20focusing%20on%20the%20resulting%20model%27s%20comprehension%20of%20technical%20jargon.%20Our%0Aexperiments%20analyze%20the%20impact%20of%20this%20merging%20process%20on%20the%20target%20model%27s%0Aproficiency%20in%20handling%20specialized%20terminology.%20We%20present%20a%20quantitative%0Aevaluation%20of%20the%20performance%20of%20the%20merged%20model%2C%20comparing%20it%20with%20that%20of%0Athe%20individual%20constituent%20models.%20The%20findings%20offer%20insights%20into%20the%0Aeffectiveness%20of%20different%20model%20merging%20methods%20for%20enhancing%20domain-specific%0Aknowledge%20and%20highlight%20potential%20challenges%20and%20future%20directions%20in%0Aleveraging%20these%20methods%20for%20cross-lingual%20knowledge%20transfer%20in%20Natural%0ALanguage%20Processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMerging%2520Language%2520and%2520Domain%2520Specific%2520Models%253A%2520The%2520Impact%2520on%2520Technical%250A%2520%2520Vocabulary%2520Acquisition%26entry.906535625%3DThibault%2520Rousset%2520and%2520Taisei%2520Kakibuchi%2520and%2520Yusuke%2520Sasaki%2520and%2520Yoshihide%2520Nomura%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520integration%2520of%2520technical%2520vocabulary%2520in%2520merged%250Alanguage%2520models.%2520We%2520explore%2520the%2520knowledge%2520transfer%2520mechanisms%2520involved%2520when%250Acombining%2520a%2520general-purpose%2520language-specific%2520model%2520with%2520a%2520domain-specific%250Amodel%252C%2520focusing%2520on%2520the%2520resulting%2520model%2527s%2520comprehension%2520of%2520technical%2520jargon.%2520Our%250Aexperiments%2520analyze%2520the%2520impact%2520of%2520this%2520merging%2520process%2520on%2520the%2520target%2520model%2527s%250Aproficiency%2520in%2520handling%2520specialized%2520terminology.%2520We%2520present%2520a%2520quantitative%250Aevaluation%2520of%2520the%2520performance%2520of%2520the%2520merged%2520model%252C%2520comparing%2520it%2520with%2520that%2520of%250Athe%2520individual%2520constituent%2520models.%2520The%2520findings%2520offer%2520insights%2520into%2520the%250Aeffectiveness%2520of%2520different%2520model%2520merging%2520methods%2520for%2520enhancing%2520domain-specific%250Aknowledge%2520and%2520highlight%2520potential%2520challenges%2520and%2520future%2520directions%2520in%250Aleveraging%2520these%2520methods%2520for%2520cross-lingual%2520knowledge%2520transfer%2520in%2520Natural%250ALanguage%2520Processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Merging%20Language%20and%20Domain%20Specific%20Models%3A%20The%20Impact%20on%20Technical%0A%20%20Vocabulary%20Acquisition&entry.906535625=Thibault%20Rousset%20and%20Taisei%20Kakibuchi%20and%20Yusuke%20Sasaki%20and%20Yoshihide%20Nomura&entry.1292438233=%20%20This%20paper%20investigates%20the%20integration%20of%20technical%20vocabulary%20in%20merged%0Alanguage%20models.%20We%20explore%20the%20knowledge%20transfer%20mechanisms%20involved%20when%0Acombining%20a%20general-purpose%20language-specific%20model%20with%20a%20domain-specific%0Amodel%2C%20focusing%20on%20the%20resulting%20model%27s%20comprehension%20of%20technical%20jargon.%20Our%0Aexperiments%20analyze%20the%20impact%20of%20this%20merging%20process%20on%20the%20target%20model%27s%0Aproficiency%20in%20handling%20specialized%20terminology.%20We%20present%20a%20quantitative%0Aevaluation%20of%20the%20performance%20of%20the%20merged%20model%2C%20comparing%20it%20with%20that%20of%0Athe%20individual%20constituent%20models.%20The%20findings%20offer%20insights%20into%20the%0Aeffectiveness%20of%20different%20model%20merging%20methods%20for%20enhancing%20domain-specific%0Aknowledge%20and%20highlight%20potential%20challenges%20and%20future%20directions%20in%0Aleveraging%20these%20methods%20for%20cross-lingual%20knowledge%20transfer%20in%20Natural%0ALanguage%20Processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12001v1&entry.124074799=Read"},
{"title": "Demystifying Catastrophic Forgetting in Two-Stage Incremental Object\n  Detector", "author": "Qirui Wu and Shizhou Zhang and De Cheng and Yinghui Xing and Di Xu and Peng Wang and Yanning Zhang", "abstract": "  Catastrophic forgetting is a critical chanllenge for incremental object\ndetection (IOD). Most existing methods treat the detector monolithically,\nrelying on instance replay or knowledge distillation without analyzing\ncomponent-specific forgetting. Through dissection of Faster R-CNN, we reveal a\nkey insight: Catastrophic forgetting is predominantly localized to the RoI Head\nclassifier, while regressors retain robustness across incremental stages. This\nfinding challenges conventional assumptions, motivating us to develop a\nframework termed NSGP-RePRE. Regional Prototype Replay (RePRE) mitigates\nclassifier forgetting via replay of two types of prototypes: coarse prototypes\nrepresent class-wise semantic centers of RoI features, while fine-grained\nprototypes model intra-class variations. Null Space Gradient Projection (NSGP)\nis further introduced to eliminate prototype-feature misalignment by updating\nthe feature extractor in directions orthogonal to subspace of old inputs via\ngradient projection, aligning RePRE with incremental learning dynamics. Our\nsimple yet effective design allows NSGP-RePRE to achieve state-of-the-art\nperformance on the Pascal VOC and MS COCO datasets under various settings. Our\nwork not only advances IOD methodology but also provide pivotal insights for\ncatastrophic forgetting mitigation in IOD. Code will be available soon.\n", "link": "http://arxiv.org/abs/2502.05540v2", "date": "2025-02-17", "relevancy": 2.4884, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5037}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5001}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20Catastrophic%20Forgetting%20in%20Two-Stage%20Incremental%20Object%0A%20%20Detector&body=Title%3A%20Demystifying%20Catastrophic%20Forgetting%20in%20Two-Stage%20Incremental%20Object%0A%20%20Detector%0AAuthor%3A%20Qirui%20Wu%20and%20Shizhou%20Zhang%20and%20De%20Cheng%20and%20Yinghui%20Xing%20and%20Di%20Xu%20and%20Peng%20Wang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20is%20a%20critical%20chanllenge%20for%20incremental%20object%0Adetection%20%28IOD%29.%20Most%20existing%20methods%20treat%20the%20detector%20monolithically%2C%0Arelying%20on%20instance%20replay%20or%20knowledge%20distillation%20without%20analyzing%0Acomponent-specific%20forgetting.%20Through%20dissection%20of%20Faster%20R-CNN%2C%20we%20reveal%20a%0Akey%20insight%3A%20Catastrophic%20forgetting%20is%20predominantly%20localized%20to%20the%20RoI%20Head%0Aclassifier%2C%20while%20regressors%20retain%20robustness%20across%20incremental%20stages.%20This%0Afinding%20challenges%20conventional%20assumptions%2C%20motivating%20us%20to%20develop%20a%0Aframework%20termed%20NSGP-RePRE.%20Regional%20Prototype%20Replay%20%28RePRE%29%20mitigates%0Aclassifier%20forgetting%20via%20replay%20of%20two%20types%20of%20prototypes%3A%20coarse%20prototypes%0Arepresent%20class-wise%20semantic%20centers%20of%20RoI%20features%2C%20while%20fine-grained%0Aprototypes%20model%20intra-class%20variations.%20Null%20Space%20Gradient%20Projection%20%28NSGP%29%0Ais%20further%20introduced%20to%20eliminate%20prototype-feature%20misalignment%20by%20updating%0Athe%20feature%20extractor%20in%20directions%20orthogonal%20to%20subspace%20of%20old%20inputs%20via%0Agradient%20projection%2C%20aligning%20RePRE%20with%20incremental%20learning%20dynamics.%20Our%0Asimple%20yet%20effective%20design%20allows%20NSGP-RePRE%20to%20achieve%20state-of-the-art%0Aperformance%20on%20the%20Pascal%20VOC%20and%20MS%20COCO%20datasets%20under%20various%20settings.%20Our%0Awork%20not%20only%20advances%20IOD%20methodology%20but%20also%20provide%20pivotal%20insights%20for%0Acatastrophic%20forgetting%20mitigation%20in%20IOD.%20Code%20will%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520Catastrophic%2520Forgetting%2520in%2520Two-Stage%2520Incremental%2520Object%250A%2520%2520Detector%26entry.906535625%3DQirui%2520Wu%2520and%2520Shizhou%2520Zhang%2520and%2520De%2520Cheng%2520and%2520Yinghui%2520Xing%2520and%2520Di%2520Xu%2520and%2520Peng%2520Wang%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520is%2520a%2520critical%2520chanllenge%2520for%2520incremental%2520object%250Adetection%2520%2528IOD%2529.%2520Most%2520existing%2520methods%2520treat%2520the%2520detector%2520monolithically%252C%250Arelying%2520on%2520instance%2520replay%2520or%2520knowledge%2520distillation%2520without%2520analyzing%250Acomponent-specific%2520forgetting.%2520Through%2520dissection%2520of%2520Faster%2520R-CNN%252C%2520we%2520reveal%2520a%250Akey%2520insight%253A%2520Catastrophic%2520forgetting%2520is%2520predominantly%2520localized%2520to%2520the%2520RoI%2520Head%250Aclassifier%252C%2520while%2520regressors%2520retain%2520robustness%2520across%2520incremental%2520stages.%2520This%250Afinding%2520challenges%2520conventional%2520assumptions%252C%2520motivating%2520us%2520to%2520develop%2520a%250Aframework%2520termed%2520NSGP-RePRE.%2520Regional%2520Prototype%2520Replay%2520%2528RePRE%2529%2520mitigates%250Aclassifier%2520forgetting%2520via%2520replay%2520of%2520two%2520types%2520of%2520prototypes%253A%2520coarse%2520prototypes%250Arepresent%2520class-wise%2520semantic%2520centers%2520of%2520RoI%2520features%252C%2520while%2520fine-grained%250Aprototypes%2520model%2520intra-class%2520variations.%2520Null%2520Space%2520Gradient%2520Projection%2520%2528NSGP%2529%250Ais%2520further%2520introduced%2520to%2520eliminate%2520prototype-feature%2520misalignment%2520by%2520updating%250Athe%2520feature%2520extractor%2520in%2520directions%2520orthogonal%2520to%2520subspace%2520of%2520old%2520inputs%2520via%250Agradient%2520projection%252C%2520aligning%2520RePRE%2520with%2520incremental%2520learning%2520dynamics.%2520Our%250Asimple%2520yet%2520effective%2520design%2520allows%2520NSGP-RePRE%2520to%2520achieve%2520state-of-the-art%250Aperformance%2520on%2520the%2520Pascal%2520VOC%2520and%2520MS%2520COCO%2520datasets%2520under%2520various%2520settings.%2520Our%250Awork%2520not%2520only%2520advances%2520IOD%2520methodology%2520but%2520also%2520provide%2520pivotal%2520insights%2520for%250Acatastrophic%2520forgetting%2520mitigation%2520in%2520IOD.%2520Code%2520will%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20Catastrophic%20Forgetting%20in%20Two-Stage%20Incremental%20Object%0A%20%20Detector&entry.906535625=Qirui%20Wu%20and%20Shizhou%20Zhang%20and%20De%20Cheng%20and%20Yinghui%20Xing%20and%20Di%20Xu%20and%20Peng%20Wang%20and%20Yanning%20Zhang&entry.1292438233=%20%20Catastrophic%20forgetting%20is%20a%20critical%20chanllenge%20for%20incremental%20object%0Adetection%20%28IOD%29.%20Most%20existing%20methods%20treat%20the%20detector%20monolithically%2C%0Arelying%20on%20instance%20replay%20or%20knowledge%20distillation%20without%20analyzing%0Acomponent-specific%20forgetting.%20Through%20dissection%20of%20Faster%20R-CNN%2C%20we%20reveal%20a%0Akey%20insight%3A%20Catastrophic%20forgetting%20is%20predominantly%20localized%20to%20the%20RoI%20Head%0Aclassifier%2C%20while%20regressors%20retain%20robustness%20across%20incremental%20stages.%20This%0Afinding%20challenges%20conventional%20assumptions%2C%20motivating%20us%20to%20develop%20a%0Aframework%20termed%20NSGP-RePRE.%20Regional%20Prototype%20Replay%20%28RePRE%29%20mitigates%0Aclassifier%20forgetting%20via%20replay%20of%20two%20types%20of%20prototypes%3A%20coarse%20prototypes%0Arepresent%20class-wise%20semantic%20centers%20of%20RoI%20features%2C%20while%20fine-grained%0Aprototypes%20model%20intra-class%20variations.%20Null%20Space%20Gradient%20Projection%20%28NSGP%29%0Ais%20further%20introduced%20to%20eliminate%20prototype-feature%20misalignment%20by%20updating%0Athe%20feature%20extractor%20in%20directions%20orthogonal%20to%20subspace%20of%20old%20inputs%20via%0Agradient%20projection%2C%20aligning%20RePRE%20with%20incremental%20learning%20dynamics.%20Our%0Asimple%20yet%20effective%20design%20allows%20NSGP-RePRE%20to%20achieve%20state-of-the-art%0Aperformance%20on%20the%20Pascal%20VOC%20and%20MS%20COCO%20datasets%20under%20various%20settings.%20Our%0Awork%20not%20only%20advances%20IOD%20methodology%20but%20also%20provide%20pivotal%20insights%20for%0Acatastrophic%20forgetting%20mitigation%20in%20IOD.%20Code%20will%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05540v2&entry.124074799=Read"},
{"title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs", "author": "Yuhang Zhou and Giannis Karamanolakis and Victor Soto and Anna Rumshisky and Mayank Kulkarni and Furong Huang and Wei Ai and Jianhua Lu", "abstract": "  The recent success of specialized Large Language Models (LLMs) in domains\nsuch as mathematical reasoning and coding has led to growing interest in\nmethods for merging these expert LLMs into a unified Mixture-of-Experts (MoE)\nmodel, with the goal of enhancing performance in each domain while retaining\neffectiveness on general tasks. However, the effective merging of expert models\nremains an open challenge, especially for models with highly divergent weight\nparameters or different architectures. State-of-the-art MoE merging methods\nonly work with homogeneous model architectures and rely on simple unweighted\naveraging to merge expert layers, which does not address parameter interference\nand requires extensive fine-tuning of the merged MoE to restore performance. To\naddress these limitations, this paper introduces new MoE merging techniques,\nincluding strategies to mitigate parameter interference, routing heuristics to\nreduce the need for MoE fine-tuning, and a novel method for merging experts\nwith different architectures. Extensive experiments across multiple domains\ndemonstrate the effectiveness of our proposed methods, reducing fine-tuning\ncosts, improving performance over state-of-the-art methods, and expanding the\napplicability of MoE merging.\n", "link": "http://arxiv.org/abs/2502.00997v3", "date": "2025-02-17", "relevancy": 2.484, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MergeME%3A%20Model%20Merging%20Techniques%20for%20Homogeneous%20and%20Heterogeneous%20MoEs&body=Title%3A%20MergeME%3A%20Model%20Merging%20Techniques%20for%20Homogeneous%20and%20Heterogeneous%20MoEs%0AAuthor%3A%20Yuhang%20Zhou%20and%20Giannis%20Karamanolakis%20and%20Victor%20Soto%20and%20Anna%20Rumshisky%20and%20Mayank%20Kulkarni%20and%20Furong%20Huang%20and%20Wei%20Ai%20and%20Jianhua%20Lu%0AAbstract%3A%20%20%20The%20recent%20success%20of%20specialized%20Large%20Language%20Models%20%28LLMs%29%20in%20domains%0Asuch%20as%20mathematical%20reasoning%20and%20coding%20has%20led%20to%20growing%20interest%20in%0Amethods%20for%20merging%20these%20expert%20LLMs%20into%20a%20unified%20Mixture-of-Experts%20%28MoE%29%0Amodel%2C%20with%20the%20goal%20of%20enhancing%20performance%20in%20each%20domain%20while%20retaining%0Aeffectiveness%20on%20general%20tasks.%20However%2C%20the%20effective%20merging%20of%20expert%20models%0Aremains%20an%20open%20challenge%2C%20especially%20for%20models%20with%20highly%20divergent%20weight%0Aparameters%20or%20different%20architectures.%20State-of-the-art%20MoE%20merging%20methods%0Aonly%20work%20with%20homogeneous%20model%20architectures%20and%20rely%20on%20simple%20unweighted%0Aaveraging%20to%20merge%20expert%20layers%2C%20which%20does%20not%20address%20parameter%20interference%0Aand%20requires%20extensive%20fine-tuning%20of%20the%20merged%20MoE%20to%20restore%20performance.%20To%0Aaddress%20these%20limitations%2C%20this%20paper%20introduces%20new%20MoE%20merging%20techniques%2C%0Aincluding%20strategies%20to%20mitigate%20parameter%20interference%2C%20routing%20heuristics%20to%0Areduce%20the%20need%20for%20MoE%20fine-tuning%2C%20and%20a%20novel%20method%20for%20merging%20experts%0Awith%20different%20architectures.%20Extensive%20experiments%20across%20multiple%20domains%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20methods%2C%20reducing%20fine-tuning%0Acosts%2C%20improving%20performance%20over%20state-of-the-art%20methods%2C%20and%20expanding%20the%0Aapplicability%20of%20MoE%20merging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00997v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMergeME%253A%2520Model%2520Merging%2520Techniques%2520for%2520Homogeneous%2520and%2520Heterogeneous%2520MoEs%26entry.906535625%3DYuhang%2520Zhou%2520and%2520Giannis%2520Karamanolakis%2520and%2520Victor%2520Soto%2520and%2520Anna%2520Rumshisky%2520and%2520Mayank%2520Kulkarni%2520and%2520Furong%2520Huang%2520and%2520Wei%2520Ai%2520and%2520Jianhua%2520Lu%26entry.1292438233%3D%2520%2520The%2520recent%2520success%2520of%2520specialized%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520domains%250Asuch%2520as%2520mathematical%2520reasoning%2520and%2520coding%2520has%2520led%2520to%2520growing%2520interest%2520in%250Amethods%2520for%2520merging%2520these%2520expert%2520LLMs%2520into%2520a%2520unified%2520Mixture-of-Experts%2520%2528MoE%2529%250Amodel%252C%2520with%2520the%2520goal%2520of%2520enhancing%2520performance%2520in%2520each%2520domain%2520while%2520retaining%250Aeffectiveness%2520on%2520general%2520tasks.%2520However%252C%2520the%2520effective%2520merging%2520of%2520expert%2520models%250Aremains%2520an%2520open%2520challenge%252C%2520especially%2520for%2520models%2520with%2520highly%2520divergent%2520weight%250Aparameters%2520or%2520different%2520architectures.%2520State-of-the-art%2520MoE%2520merging%2520methods%250Aonly%2520work%2520with%2520homogeneous%2520model%2520architectures%2520and%2520rely%2520on%2520simple%2520unweighted%250Aaveraging%2520to%2520merge%2520expert%2520layers%252C%2520which%2520does%2520not%2520address%2520parameter%2520interference%250Aand%2520requires%2520extensive%2520fine-tuning%2520of%2520the%2520merged%2520MoE%2520to%2520restore%2520performance.%2520To%250Aaddress%2520these%2520limitations%252C%2520this%2520paper%2520introduces%2520new%2520MoE%2520merging%2520techniques%252C%250Aincluding%2520strategies%2520to%2520mitigate%2520parameter%2520interference%252C%2520routing%2520heuristics%2520to%250Areduce%2520the%2520need%2520for%2520MoE%2520fine-tuning%252C%2520and%2520a%2520novel%2520method%2520for%2520merging%2520experts%250Awith%2520different%2520architectures.%2520Extensive%2520experiments%2520across%2520multiple%2520domains%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520methods%252C%2520reducing%2520fine-tuning%250Acosts%252C%2520improving%2520performance%2520over%2520state-of-the-art%2520methods%252C%2520and%2520expanding%2520the%250Aapplicability%2520of%2520MoE%2520merging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00997v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MergeME%3A%20Model%20Merging%20Techniques%20for%20Homogeneous%20and%20Heterogeneous%20MoEs&entry.906535625=Yuhang%20Zhou%20and%20Giannis%20Karamanolakis%20and%20Victor%20Soto%20and%20Anna%20Rumshisky%20and%20Mayank%20Kulkarni%20and%20Furong%20Huang%20and%20Wei%20Ai%20and%20Jianhua%20Lu&entry.1292438233=%20%20The%20recent%20success%20of%20specialized%20Large%20Language%20Models%20%28LLMs%29%20in%20domains%0Asuch%20as%20mathematical%20reasoning%20and%20coding%20has%20led%20to%20growing%20interest%20in%0Amethods%20for%20merging%20these%20expert%20LLMs%20into%20a%20unified%20Mixture-of-Experts%20%28MoE%29%0Amodel%2C%20with%20the%20goal%20of%20enhancing%20performance%20in%20each%20domain%20while%20retaining%0Aeffectiveness%20on%20general%20tasks.%20However%2C%20the%20effective%20merging%20of%20expert%20models%0Aremains%20an%20open%20challenge%2C%20especially%20for%20models%20with%20highly%20divergent%20weight%0Aparameters%20or%20different%20architectures.%20State-of-the-art%20MoE%20merging%20methods%0Aonly%20work%20with%20homogeneous%20model%20architectures%20and%20rely%20on%20simple%20unweighted%0Aaveraging%20to%20merge%20expert%20layers%2C%20which%20does%20not%20address%20parameter%20interference%0Aand%20requires%20extensive%20fine-tuning%20of%20the%20merged%20MoE%20to%20restore%20performance.%20To%0Aaddress%20these%20limitations%2C%20this%20paper%20introduces%20new%20MoE%20merging%20techniques%2C%0Aincluding%20strategies%20to%20mitigate%20parameter%20interference%2C%20routing%20heuristics%20to%0Areduce%20the%20need%20for%20MoE%20fine-tuning%2C%20and%20a%20novel%20method%20for%20merging%20experts%0Awith%20different%20architectures.%20Extensive%20experiments%20across%20multiple%20domains%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20methods%2C%20reducing%20fine-tuning%0Acosts%2C%20improving%20performance%20over%20state-of-the-art%20methods%2C%20and%20expanding%20the%0Aapplicability%20of%20MoE%20merging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00997v3&entry.124074799=Read"},
{"title": "Object-Centric Image to Video Generation with Language Guidance", "author": "Angel Villar-Corrales and Gjergj Plepi and Sven Behnke", "abstract": "  Accurate and flexible world models are crucial for autonomous systems to\nunderstand their environment and predict future events. Object-centric models,\nwith structured latent spaces, have shown promise in modeling object dynamics\nand interactions, but often face challenges in scaling to complex datasets and\nincorporating external guidance, limiting their applicability in robotics. To\naddress these limitations, we propose TextOCVP, an object-centric model for\nimage-to-video generation guided by textual descriptions. TextOCVP parses an\nobserved scene into object representations, called slots, and utilizes a\ntext-conditioned transformer predictor to forecast future object states and\nvideo frames. Our approach jointly models object dynamics and interactions\nwhile incorporating textual guidance, thus leading to accurate and controllable\npredictions. Our method's structured latent space offers enhanced control over\nthe prediction process, outperforming several image-to-video generative\nbaselines. Additionally, we demonstrate that structured object-centric\nrepresentations provide superior controllability and interpretability,\nfacilitating the modeling of object dynamics and enabling more precise and\nunderstandable predictions. Videos and code are available at\nhttps://play-slot.github.io/TextOCVP/.\n", "link": "http://arxiv.org/abs/2502.11655v1", "date": "2025-02-17", "relevancy": 2.4812, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6248}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6216}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Centric%20Image%20to%20Video%20Generation%20with%20Language%20Guidance&body=Title%3A%20Object-Centric%20Image%20to%20Video%20Generation%20with%20Language%20Guidance%0AAuthor%3A%20Angel%20Villar-Corrales%20and%20Gjergj%20Plepi%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20Accurate%20and%20flexible%20world%20models%20are%20crucial%20for%20autonomous%20systems%20to%0Aunderstand%20their%20environment%20and%20predict%20future%20events.%20Object-centric%20models%2C%0Awith%20structured%20latent%20spaces%2C%20have%20shown%20promise%20in%20modeling%20object%20dynamics%0Aand%20interactions%2C%20but%20often%20face%20challenges%20in%20scaling%20to%20complex%20datasets%20and%0Aincorporating%20external%20guidance%2C%20limiting%20their%20applicability%20in%20robotics.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20TextOCVP%2C%20an%20object-centric%20model%20for%0Aimage-to-video%20generation%20guided%20by%20textual%20descriptions.%20TextOCVP%20parses%20an%0Aobserved%20scene%20into%20object%20representations%2C%20called%20slots%2C%20and%20utilizes%20a%0Atext-conditioned%20transformer%20predictor%20to%20forecast%20future%20object%20states%20and%0Avideo%20frames.%20Our%20approach%20jointly%20models%20object%20dynamics%20and%20interactions%0Awhile%20incorporating%20textual%20guidance%2C%20thus%20leading%20to%20accurate%20and%20controllable%0Apredictions.%20Our%20method%27s%20structured%20latent%20space%20offers%20enhanced%20control%20over%0Athe%20prediction%20process%2C%20outperforming%20several%20image-to-video%20generative%0Abaselines.%20Additionally%2C%20we%20demonstrate%20that%20structured%20object-centric%0Arepresentations%20provide%20superior%20controllability%20and%20interpretability%2C%0Afacilitating%20the%20modeling%20of%20object%20dynamics%20and%20enabling%20more%20precise%20and%0Aunderstandable%20predictions.%20Videos%20and%20code%20are%20available%20at%0Ahttps%3A//play-slot.github.io/TextOCVP/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Centric%2520Image%2520to%2520Video%2520Generation%2520with%2520Language%2520Guidance%26entry.906535625%3DAngel%2520Villar-Corrales%2520and%2520Gjergj%2520Plepi%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520Accurate%2520and%2520flexible%2520world%2520models%2520are%2520crucial%2520for%2520autonomous%2520systems%2520to%250Aunderstand%2520their%2520environment%2520and%2520predict%2520future%2520events.%2520Object-centric%2520models%252C%250Awith%2520structured%2520latent%2520spaces%252C%2520have%2520shown%2520promise%2520in%2520modeling%2520object%2520dynamics%250Aand%2520interactions%252C%2520but%2520often%2520face%2520challenges%2520in%2520scaling%2520to%2520complex%2520datasets%2520and%250Aincorporating%2520external%2520guidance%252C%2520limiting%2520their%2520applicability%2520in%2520robotics.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520TextOCVP%252C%2520an%2520object-centric%2520model%2520for%250Aimage-to-video%2520generation%2520guided%2520by%2520textual%2520descriptions.%2520TextOCVP%2520parses%2520an%250Aobserved%2520scene%2520into%2520object%2520representations%252C%2520called%2520slots%252C%2520and%2520utilizes%2520a%250Atext-conditioned%2520transformer%2520predictor%2520to%2520forecast%2520future%2520object%2520states%2520and%250Avideo%2520frames.%2520Our%2520approach%2520jointly%2520models%2520object%2520dynamics%2520and%2520interactions%250Awhile%2520incorporating%2520textual%2520guidance%252C%2520thus%2520leading%2520to%2520accurate%2520and%2520controllable%250Apredictions.%2520Our%2520method%2527s%2520structured%2520latent%2520space%2520offers%2520enhanced%2520control%2520over%250Athe%2520prediction%2520process%252C%2520outperforming%2520several%2520image-to-video%2520generative%250Abaselines.%2520Additionally%252C%2520we%2520demonstrate%2520that%2520structured%2520object-centric%250Arepresentations%2520provide%2520superior%2520controllability%2520and%2520interpretability%252C%250Afacilitating%2520the%2520modeling%2520of%2520object%2520dynamics%2520and%2520enabling%2520more%2520precise%2520and%250Aunderstandable%2520predictions.%2520Videos%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//play-slot.github.io/TextOCVP/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Centric%20Image%20to%20Video%20Generation%20with%20Language%20Guidance&entry.906535625=Angel%20Villar-Corrales%20and%20Gjergj%20Plepi%20and%20Sven%20Behnke&entry.1292438233=%20%20Accurate%20and%20flexible%20world%20models%20are%20crucial%20for%20autonomous%20systems%20to%0Aunderstand%20their%20environment%20and%20predict%20future%20events.%20Object-centric%20models%2C%0Awith%20structured%20latent%20spaces%2C%20have%20shown%20promise%20in%20modeling%20object%20dynamics%0Aand%20interactions%2C%20but%20often%20face%20challenges%20in%20scaling%20to%20complex%20datasets%20and%0Aincorporating%20external%20guidance%2C%20limiting%20their%20applicability%20in%20robotics.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20TextOCVP%2C%20an%20object-centric%20model%20for%0Aimage-to-video%20generation%20guided%20by%20textual%20descriptions.%20TextOCVP%20parses%20an%0Aobserved%20scene%20into%20object%20representations%2C%20called%20slots%2C%20and%20utilizes%20a%0Atext-conditioned%20transformer%20predictor%20to%20forecast%20future%20object%20states%20and%0Avideo%20frames.%20Our%20approach%20jointly%20models%20object%20dynamics%20and%20interactions%0Awhile%20incorporating%20textual%20guidance%2C%20thus%20leading%20to%20accurate%20and%20controllable%0Apredictions.%20Our%20method%27s%20structured%20latent%20space%20offers%20enhanced%20control%20over%0Athe%20prediction%20process%2C%20outperforming%20several%20image-to-video%20generative%0Abaselines.%20Additionally%2C%20we%20demonstrate%20that%20structured%20object-centric%0Arepresentations%20provide%20superior%20controllability%20and%20interpretability%2C%0Afacilitating%20the%20modeling%20of%20object%20dynamics%20and%20enabling%20more%20precise%20and%0Aunderstandable%20predictions.%20Videos%20and%20code%20are%20available%20at%0Ahttps%3A//play-slot.github.io/TextOCVP/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11655v1&entry.124074799=Read"},
{"title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs", "author": "Heming Xia and Yongqi Li and Chak Tou Leong and Wenjie Wang and Wenjie Li", "abstract": "  Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.\n", "link": "http://arxiv.org/abs/2502.12067v1", "date": "2025-02-17", "relevancy": 2.4712, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenSkip%3A%20Controllable%20Chain-of-Thought%20Compression%20in%20LLMs&body=Title%3A%20TokenSkip%3A%20Controllable%20Chain-of-Thought%20Compression%20in%20LLMs%0AAuthor%3A%20Heming%20Xia%20and%20Yongqi%20Li%20and%20Chak%20Tou%20Leong%20and%20Wenjie%20Wang%20and%20Wenjie%20Li%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20has%20been%20proven%20effective%20in%20enhancing%20the%20reasoning%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20Recent%20advancements%2C%20such%20as%0AOpenAI%27s%20o1%20and%20DeepSeek-R1%2C%20suggest%20that%20scaling%20up%20the%20length%20of%20CoT%0Asequences%20during%20inference%20could%20further%20boost%20LLM%20reasoning%20performance.%0AHowever%2C%20due%20to%20the%20autoregressive%20nature%20of%20LLM%20decoding%2C%20longer%20CoT%20outputs%0Alead%20to%20a%20linear%20increase%20in%20inference%20latency%2C%20adversely%20affecting%20user%0Aexperience%2C%20particularly%20when%20the%20CoT%20exceeds%2010%2C000%20tokens.%20To%20address%20this%0Alimitation%2C%20we%20analyze%20the%20semantic%20importance%20of%20tokens%20within%20CoT%20outputs%20and%0Areveal%20that%20their%20contributions%20to%20reasoning%20vary.%20Building%20on%20this%20insight%2C%20we%0Apropose%20TokenSkip%2C%20a%20simple%20yet%20effective%20approach%20that%20enables%20LLMs%20to%0Aselectively%20skip%20less%20important%20tokens%2C%20allowing%20for%20controllable%20CoT%0Acompression.%20Extensive%20experiments%20across%20various%20models%20and%20tasks%20demonstrate%0Athe%20effectiveness%20of%20TokenSkip%20in%20reducing%20CoT%20token%20usage%20while%20preserving%0Astrong%20reasoning%20performance.%20Notably%2C%20when%20applied%20to%20Qwen2.5-14B-Instruct%2C%0ATokenSkip%20reduces%20reasoning%20tokens%20by%2040%25%20%28from%20313%20to%20181%29%20on%20GSM8K%2C%20with%20less%0Athan%20a%200.4%25%20performance%20drop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenSkip%253A%2520Controllable%2520Chain-of-Thought%2520Compression%2520in%2520LLMs%26entry.906535625%3DHeming%2520Xia%2520and%2520Yongqi%2520Li%2520and%2520Chak%2520Tou%2520Leong%2520and%2520Wenjie%2520Wang%2520and%2520Wenjie%2520Li%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520has%2520been%2520proven%2520effective%2520in%2520enhancing%2520the%2520reasoning%250Acapabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Recent%2520advancements%252C%2520such%2520as%250AOpenAI%2527s%2520o1%2520and%2520DeepSeek-R1%252C%2520suggest%2520that%2520scaling%2520up%2520the%2520length%2520of%2520CoT%250Asequences%2520during%2520inference%2520could%2520further%2520boost%2520LLM%2520reasoning%2520performance.%250AHowever%252C%2520due%2520to%2520the%2520autoregressive%2520nature%2520of%2520LLM%2520decoding%252C%2520longer%2520CoT%2520outputs%250Alead%2520to%2520a%2520linear%2520increase%2520in%2520inference%2520latency%252C%2520adversely%2520affecting%2520user%250Aexperience%252C%2520particularly%2520when%2520the%2520CoT%2520exceeds%252010%252C000%2520tokens.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520analyze%2520the%2520semantic%2520importance%2520of%2520tokens%2520within%2520CoT%2520outputs%2520and%250Areveal%2520that%2520their%2520contributions%2520to%2520reasoning%2520vary.%2520Building%2520on%2520this%2520insight%252C%2520we%250Apropose%2520TokenSkip%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520that%2520enables%2520LLMs%2520to%250Aselectively%2520skip%2520less%2520important%2520tokens%252C%2520allowing%2520for%2520controllable%2520CoT%250Acompression.%2520Extensive%2520experiments%2520across%2520various%2520models%2520and%2520tasks%2520demonstrate%250Athe%2520effectiveness%2520of%2520TokenSkip%2520in%2520reducing%2520CoT%2520token%2520usage%2520while%2520preserving%250Astrong%2520reasoning%2520performance.%2520Notably%252C%2520when%2520applied%2520to%2520Qwen2.5-14B-Instruct%252C%250ATokenSkip%2520reduces%2520reasoning%2520tokens%2520by%252040%2525%2520%2528from%2520313%2520to%2520181%2529%2520on%2520GSM8K%252C%2520with%2520less%250Athan%2520a%25200.4%2525%2520performance%2520drop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenSkip%3A%20Controllable%20Chain-of-Thought%20Compression%20in%20LLMs&entry.906535625=Heming%20Xia%20and%20Yongqi%20Li%20and%20Chak%20Tou%20Leong%20and%20Wenjie%20Wang%20and%20Wenjie%20Li&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20has%20been%20proven%20effective%20in%20enhancing%20the%20reasoning%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20Recent%20advancements%2C%20such%20as%0AOpenAI%27s%20o1%20and%20DeepSeek-R1%2C%20suggest%20that%20scaling%20up%20the%20length%20of%20CoT%0Asequences%20during%20inference%20could%20further%20boost%20LLM%20reasoning%20performance.%0AHowever%2C%20due%20to%20the%20autoregressive%20nature%20of%20LLM%20decoding%2C%20longer%20CoT%20outputs%0Alead%20to%20a%20linear%20increase%20in%20inference%20latency%2C%20adversely%20affecting%20user%0Aexperience%2C%20particularly%20when%20the%20CoT%20exceeds%2010%2C000%20tokens.%20To%20address%20this%0Alimitation%2C%20we%20analyze%20the%20semantic%20importance%20of%20tokens%20within%20CoT%20outputs%20and%0Areveal%20that%20their%20contributions%20to%20reasoning%20vary.%20Building%20on%20this%20insight%2C%20we%0Apropose%20TokenSkip%2C%20a%20simple%20yet%20effective%20approach%20that%20enables%20LLMs%20to%0Aselectively%20skip%20less%20important%20tokens%2C%20allowing%20for%20controllable%20CoT%0Acompression.%20Extensive%20experiments%20across%20various%20models%20and%20tasks%20demonstrate%0Athe%20effectiveness%20of%20TokenSkip%20in%20reducing%20CoT%20token%20usage%20while%20preserving%0Astrong%20reasoning%20performance.%20Notably%2C%20when%20applied%20to%20Qwen2.5-14B-Instruct%2C%0ATokenSkip%20reduces%20reasoning%20tokens%20by%2040%25%20%28from%20313%20to%20181%29%20on%20GSM8K%2C%20with%20less%0Athan%20a%200.4%25%20performance%20drop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12067v1&entry.124074799=Read"},
{"title": "Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the\n  Lens of Class Hierarchy", "author": "Roman Malashin and Valeria Yachnaya and Alexander Mullin", "abstract": "  We investigate the training dynamics of deep classifiers by examining how\nhierarchical relationships between classes evolve during training. Through\nextensive experiments, we argue that the learning process in classification\nproblems can be understood through the lens of label clustering. Specifically,\nwe observe that networks tend to distinguish higher-level (hypernym) categories\nin the early stages of training, and learn more specific (hyponym) categories\nlater. We introduce a novel framework to track the evolution of the feature\nmanifold during training, revealing how the hierarchy of class relations\nemerges and refines across the network layers. Our analysis demonstrates that\nthe learned representations closely align with the semantic structure of the\ndataset, providing a quantitative description of the clustering process.\nNotably, we show that in the hypernym label space, certain properties of neural\ncollapse appear earlier than in the hyponym label space, helping to bridge the\ngap between the initial and terminal phases of learning. We believe our\nfindings offer new insights into the mechanisms driving hierarchical learning\nin deep networks, paving the way for future advancements in understanding deep\nlearning dynamics.\n", "link": "http://arxiv.org/abs/2502.12125v1", "date": "2025-02-17", "relevancy": 2.4643, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5008}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4899}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypernym%20Bias%3A%20Unraveling%20Deep%20Classifier%20Training%20Dynamics%20through%20the%0A%20%20Lens%20of%20Class%20Hierarchy&body=Title%3A%20Hypernym%20Bias%3A%20Unraveling%20Deep%20Classifier%20Training%20Dynamics%20through%20the%0A%20%20Lens%20of%20Class%20Hierarchy%0AAuthor%3A%20Roman%20Malashin%20and%20Valeria%20Yachnaya%20and%20Alexander%20Mullin%0AAbstract%3A%20%20%20We%20investigate%20the%20training%20dynamics%20of%20deep%20classifiers%20by%20examining%20how%0Ahierarchical%20relationships%20between%20classes%20evolve%20during%20training.%20Through%0Aextensive%20experiments%2C%20we%20argue%20that%20the%20learning%20process%20in%20classification%0Aproblems%20can%20be%20understood%20through%20the%20lens%20of%20label%20clustering.%20Specifically%2C%0Awe%20observe%20that%20networks%20tend%20to%20distinguish%20higher-level%20%28hypernym%29%20categories%0Ain%20the%20early%20stages%20of%20training%2C%20and%20learn%20more%20specific%20%28hyponym%29%20categories%0Alater.%20We%20introduce%20a%20novel%20framework%20to%20track%20the%20evolution%20of%20the%20feature%0Amanifold%20during%20training%2C%20revealing%20how%20the%20hierarchy%20of%20class%20relations%0Aemerges%20and%20refines%20across%20the%20network%20layers.%20Our%20analysis%20demonstrates%20that%0Athe%20learned%20representations%20closely%20align%20with%20the%20semantic%20structure%20of%20the%0Adataset%2C%20providing%20a%20quantitative%20description%20of%20the%20clustering%20process.%0ANotably%2C%20we%20show%20that%20in%20the%20hypernym%20label%20space%2C%20certain%20properties%20of%20neural%0Acollapse%20appear%20earlier%20than%20in%20the%20hyponym%20label%20space%2C%20helping%20to%20bridge%20the%0Agap%20between%20the%20initial%20and%20terminal%20phases%20of%20learning.%20We%20believe%20our%0Afindings%20offer%20new%20insights%20into%20the%20mechanisms%20driving%20hierarchical%20learning%0Ain%20deep%20networks%2C%20paving%20the%20way%20for%20future%20advancements%20in%20understanding%20deep%0Alearning%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypernym%2520Bias%253A%2520Unraveling%2520Deep%2520Classifier%2520Training%2520Dynamics%2520through%2520the%250A%2520%2520Lens%2520of%2520Class%2520Hierarchy%26entry.906535625%3DRoman%2520Malashin%2520and%2520Valeria%2520Yachnaya%2520and%2520Alexander%2520Mullin%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520training%2520dynamics%2520of%2520deep%2520classifiers%2520by%2520examining%2520how%250Ahierarchical%2520relationships%2520between%2520classes%2520evolve%2520during%2520training.%2520Through%250Aextensive%2520experiments%252C%2520we%2520argue%2520that%2520the%2520learning%2520process%2520in%2520classification%250Aproblems%2520can%2520be%2520understood%2520through%2520the%2520lens%2520of%2520label%2520clustering.%2520Specifically%252C%250Awe%2520observe%2520that%2520networks%2520tend%2520to%2520distinguish%2520higher-level%2520%2528hypernym%2529%2520categories%250Ain%2520the%2520early%2520stages%2520of%2520training%252C%2520and%2520learn%2520more%2520specific%2520%2528hyponym%2529%2520categories%250Alater.%2520We%2520introduce%2520a%2520novel%2520framework%2520to%2520track%2520the%2520evolution%2520of%2520the%2520feature%250Amanifold%2520during%2520training%252C%2520revealing%2520how%2520the%2520hierarchy%2520of%2520class%2520relations%250Aemerges%2520and%2520refines%2520across%2520the%2520network%2520layers.%2520Our%2520analysis%2520demonstrates%2520that%250Athe%2520learned%2520representations%2520closely%2520align%2520with%2520the%2520semantic%2520structure%2520of%2520the%250Adataset%252C%2520providing%2520a%2520quantitative%2520description%2520of%2520the%2520clustering%2520process.%250ANotably%252C%2520we%2520show%2520that%2520in%2520the%2520hypernym%2520label%2520space%252C%2520certain%2520properties%2520of%2520neural%250Acollapse%2520appear%2520earlier%2520than%2520in%2520the%2520hyponym%2520label%2520space%252C%2520helping%2520to%2520bridge%2520the%250Agap%2520between%2520the%2520initial%2520and%2520terminal%2520phases%2520of%2520learning.%2520We%2520believe%2520our%250Afindings%2520offer%2520new%2520insights%2520into%2520the%2520mechanisms%2520driving%2520hierarchical%2520learning%250Ain%2520deep%2520networks%252C%2520paving%2520the%2520way%2520for%2520future%2520advancements%2520in%2520understanding%2520deep%250Alearning%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypernym%20Bias%3A%20Unraveling%20Deep%20Classifier%20Training%20Dynamics%20through%20the%0A%20%20Lens%20of%20Class%20Hierarchy&entry.906535625=Roman%20Malashin%20and%20Valeria%20Yachnaya%20and%20Alexander%20Mullin&entry.1292438233=%20%20We%20investigate%20the%20training%20dynamics%20of%20deep%20classifiers%20by%20examining%20how%0Ahierarchical%20relationships%20between%20classes%20evolve%20during%20training.%20Through%0Aextensive%20experiments%2C%20we%20argue%20that%20the%20learning%20process%20in%20classification%0Aproblems%20can%20be%20understood%20through%20the%20lens%20of%20label%20clustering.%20Specifically%2C%0Awe%20observe%20that%20networks%20tend%20to%20distinguish%20higher-level%20%28hypernym%29%20categories%0Ain%20the%20early%20stages%20of%20training%2C%20and%20learn%20more%20specific%20%28hyponym%29%20categories%0Alater.%20We%20introduce%20a%20novel%20framework%20to%20track%20the%20evolution%20of%20the%20feature%0Amanifold%20during%20training%2C%20revealing%20how%20the%20hierarchy%20of%20class%20relations%0Aemerges%20and%20refines%20across%20the%20network%20layers.%20Our%20analysis%20demonstrates%20that%0Athe%20learned%20representations%20closely%20align%20with%20the%20semantic%20structure%20of%20the%0Adataset%2C%20providing%20a%20quantitative%20description%20of%20the%20clustering%20process.%0ANotably%2C%20we%20show%20that%20in%20the%20hypernym%20label%20space%2C%20certain%20properties%20of%20neural%0Acollapse%20appear%20earlier%20than%20in%20the%20hyponym%20label%20space%2C%20helping%20to%20bridge%20the%0Agap%20between%20the%20initial%20and%20terminal%20phases%20of%20learning.%20We%20believe%20our%0Afindings%20offer%20new%20insights%20into%20the%20mechanisms%20driving%20hierarchical%20learning%0Ain%20deep%20networks%2C%20paving%20the%20way%20for%20future%20advancements%20in%20understanding%20deep%0Alearning%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12125v1&entry.124074799=Read"},
{"title": "Selective Task Group Updates for Multi-Task Optimization", "author": "Wooseong Jeong and Kuk-Jin Yoon", "abstract": "  Multi-task learning enables the acquisition of task-generic knowledge by\ntraining multiple tasks within a unified architecture. However, training all\ntasks together in a single architecture can lead to performance degradation,\nknown as negative transfer, which is a main concern in multi-task learning.\nPrevious works have addressed this issue by optimizing the multi-task network\nthrough gradient manipulation or weighted loss adjustments. However, their\noptimization strategy focuses on addressing task imbalance in shared\nparameters, neglecting the learning of task-specific parameters. As a result,\nthey show limitations in mitigating negative transfer, since the learning of\nshared space and task-specific information influences each other during\noptimization. To address this, we propose a different approach to enhance\nmulti-task performance by selectively grouping tasks and updating them for each\nbatch during optimization. We introduce an algorithm that adaptively determines\nhow to effectively group tasks and update them during the learning process. To\ntrack inter-task relations and optimize multi-task networks simultaneously, we\npropose proximal inter-task affinity, which can be measured during the\noptimization process. We provide a theoretical analysis on how dividing tasks\ninto multiple groups and updating them sequentially significantly affects\nmulti-task performance by enhancing the learning of task-specific parameters.\nOur methods substantially outperform previous multi-task optimization\napproaches and are scalable to different architectures and various numbers of\ntasks.\n", "link": "http://arxiv.org/abs/2502.11986v1", "date": "2025-02-17", "relevancy": 2.4612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4896}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Task%20Group%20Updates%20for%20Multi-Task%20Optimization&body=Title%3A%20Selective%20Task%20Group%20Updates%20for%20Multi-Task%20Optimization%0AAuthor%3A%20Wooseong%20Jeong%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%20Multi-task%20learning%20enables%20the%20acquisition%20of%20task-generic%20knowledge%20by%0Atraining%20multiple%20tasks%20within%20a%20unified%20architecture.%20However%2C%20training%20all%0Atasks%20together%20in%20a%20single%20architecture%20can%20lead%20to%20performance%20degradation%2C%0Aknown%20as%20negative%20transfer%2C%20which%20is%20a%20main%20concern%20in%20multi-task%20learning.%0APrevious%20works%20have%20addressed%20this%20issue%20by%20optimizing%20the%20multi-task%20network%0Athrough%20gradient%20manipulation%20or%20weighted%20loss%20adjustments.%20However%2C%20their%0Aoptimization%20strategy%20focuses%20on%20addressing%20task%20imbalance%20in%20shared%0Aparameters%2C%20neglecting%20the%20learning%20of%20task-specific%20parameters.%20As%20a%20result%2C%0Athey%20show%20limitations%20in%20mitigating%20negative%20transfer%2C%20since%20the%20learning%20of%0Ashared%20space%20and%20task-specific%20information%20influences%20each%20other%20during%0Aoptimization.%20To%20address%20this%2C%20we%20propose%20a%20different%20approach%20to%20enhance%0Amulti-task%20performance%20by%20selectively%20grouping%20tasks%20and%20updating%20them%20for%20each%0Abatch%20during%20optimization.%20We%20introduce%20an%20algorithm%20that%20adaptively%20determines%0Ahow%20to%20effectively%20group%20tasks%20and%20update%20them%20during%20the%20learning%20process.%20To%0Atrack%20inter-task%20relations%20and%20optimize%20multi-task%20networks%20simultaneously%2C%20we%0Apropose%20proximal%20inter-task%20affinity%2C%20which%20can%20be%20measured%20during%20the%0Aoptimization%20process.%20We%20provide%20a%20theoretical%20analysis%20on%20how%20dividing%20tasks%0Ainto%20multiple%20groups%20and%20updating%20them%20sequentially%20significantly%20affects%0Amulti-task%20performance%20by%20enhancing%20the%20learning%20of%20task-specific%20parameters.%0AOur%20methods%20substantially%20outperform%20previous%20multi-task%20optimization%0Aapproaches%20and%20are%20scalable%20to%20different%20architectures%20and%20various%20numbers%20of%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Task%2520Group%2520Updates%2520for%2520Multi-Task%2520Optimization%26entry.906535625%3DWooseong%2520Jeong%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%2520Multi-task%2520learning%2520enables%2520the%2520acquisition%2520of%2520task-generic%2520knowledge%2520by%250Atraining%2520multiple%2520tasks%2520within%2520a%2520unified%2520architecture.%2520However%252C%2520training%2520all%250Atasks%2520together%2520in%2520a%2520single%2520architecture%2520can%2520lead%2520to%2520performance%2520degradation%252C%250Aknown%2520as%2520negative%2520transfer%252C%2520which%2520is%2520a%2520main%2520concern%2520in%2520multi-task%2520learning.%250APrevious%2520works%2520have%2520addressed%2520this%2520issue%2520by%2520optimizing%2520the%2520multi-task%2520network%250Athrough%2520gradient%2520manipulation%2520or%2520weighted%2520loss%2520adjustments.%2520However%252C%2520their%250Aoptimization%2520strategy%2520focuses%2520on%2520addressing%2520task%2520imbalance%2520in%2520shared%250Aparameters%252C%2520neglecting%2520the%2520learning%2520of%2520task-specific%2520parameters.%2520As%2520a%2520result%252C%250Athey%2520show%2520limitations%2520in%2520mitigating%2520negative%2520transfer%252C%2520since%2520the%2520learning%2520of%250Ashared%2520space%2520and%2520task-specific%2520information%2520influences%2520each%2520other%2520during%250Aoptimization.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520different%2520approach%2520to%2520enhance%250Amulti-task%2520performance%2520by%2520selectively%2520grouping%2520tasks%2520and%2520updating%2520them%2520for%2520each%250Abatch%2520during%2520optimization.%2520We%2520introduce%2520an%2520algorithm%2520that%2520adaptively%2520determines%250Ahow%2520to%2520effectively%2520group%2520tasks%2520and%2520update%2520them%2520during%2520the%2520learning%2520process.%2520To%250Atrack%2520inter-task%2520relations%2520and%2520optimize%2520multi-task%2520networks%2520simultaneously%252C%2520we%250Apropose%2520proximal%2520inter-task%2520affinity%252C%2520which%2520can%2520be%2520measured%2520during%2520the%250Aoptimization%2520process.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520on%2520how%2520dividing%2520tasks%250Ainto%2520multiple%2520groups%2520and%2520updating%2520them%2520sequentially%2520significantly%2520affects%250Amulti-task%2520performance%2520by%2520enhancing%2520the%2520learning%2520of%2520task-specific%2520parameters.%250AOur%2520methods%2520substantially%2520outperform%2520previous%2520multi-task%2520optimization%250Aapproaches%2520and%2520are%2520scalable%2520to%2520different%2520architectures%2520and%2520various%2520numbers%2520of%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Task%20Group%20Updates%20for%20Multi-Task%20Optimization&entry.906535625=Wooseong%20Jeong%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%20Multi-task%20learning%20enables%20the%20acquisition%20of%20task-generic%20knowledge%20by%0Atraining%20multiple%20tasks%20within%20a%20unified%20architecture.%20However%2C%20training%20all%0Atasks%20together%20in%20a%20single%20architecture%20can%20lead%20to%20performance%20degradation%2C%0Aknown%20as%20negative%20transfer%2C%20which%20is%20a%20main%20concern%20in%20multi-task%20learning.%0APrevious%20works%20have%20addressed%20this%20issue%20by%20optimizing%20the%20multi-task%20network%0Athrough%20gradient%20manipulation%20or%20weighted%20loss%20adjustments.%20However%2C%20their%0Aoptimization%20strategy%20focuses%20on%20addressing%20task%20imbalance%20in%20shared%0Aparameters%2C%20neglecting%20the%20learning%20of%20task-specific%20parameters.%20As%20a%20result%2C%0Athey%20show%20limitations%20in%20mitigating%20negative%20transfer%2C%20since%20the%20learning%20of%0Ashared%20space%20and%20task-specific%20information%20influences%20each%20other%20during%0Aoptimization.%20To%20address%20this%2C%20we%20propose%20a%20different%20approach%20to%20enhance%0Amulti-task%20performance%20by%20selectively%20grouping%20tasks%20and%20updating%20them%20for%20each%0Abatch%20during%20optimization.%20We%20introduce%20an%20algorithm%20that%20adaptively%20determines%0Ahow%20to%20effectively%20group%20tasks%20and%20update%20them%20during%20the%20learning%20process.%20To%0Atrack%20inter-task%20relations%20and%20optimize%20multi-task%20networks%20simultaneously%2C%20we%0Apropose%20proximal%20inter-task%20affinity%2C%20which%20can%20be%20measured%20during%20the%0Aoptimization%20process.%20We%20provide%20a%20theoretical%20analysis%20on%20how%20dividing%20tasks%0Ainto%20multiple%20groups%20and%20updating%20them%20sequentially%20significantly%20affects%0Amulti-task%20performance%20by%20enhancing%20the%20learning%20of%20task-specific%20parameters.%0AOur%20methods%20substantially%20outperform%20previous%20multi-task%20optimization%0Aapproaches%20and%20are%20scalable%20to%20different%20architectures%20and%20various%20numbers%20of%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11986v1&entry.124074799=Read"},
{"title": "Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms:\n  Exploring Tuning Strategies", "author": "Boshko Koloski and Bla\u017e \u0160krlj and Marko Robnik-\u0160ikonja and Senja Pollak", "abstract": "  The cross-lingual transfer is a promising technique to solve tasks in\nless-resourced languages. In this empirical study, we compare two fine-tuning\napproaches combined with zero-shot and full-shot learning approaches for large\nlanguage models in a cross-lingual setting. As fine-tuning strategies, we\ncompare parameter-efficient adapter methods with fine-tuning of all parameters.\nAs cross-lingual transfer strategies, we compare the intermediate-training\n(\\textit{IT}) that uses each language sequentially and cross-lingual validation\n(\\textit{CLV}) that uses a target language already in the validation phase of\nfine-tuning. We assess the success of transfer and the extent of catastrophic\nforgetting in a source language due to cross-lingual transfer, i.e., how much\npreviously acquired knowledge is lost when we learn new information in a\ndifferent language. The results on two different classification problems, hate\nspeech detection and product reviews, each containing datasets in several\nlanguages, show that the \\textit{IT} cross-lingual strategy outperforms\n\\textit{CLV} for the target language. Our findings indicate that, in the\nmajority of cases, the \\textit{CLV} strategy demonstrates superior retention of\nknowledge in the base language (English) compared to the \\textit{IT} strategy,\nwhen evaluating catastrophic forgetting in multiple cross-lingual transfers.\n", "link": "http://arxiv.org/abs/2309.06089v3", "date": "2025-02-17", "relevancy": 2.4606, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5247}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Catastrophic%20Forgetting%20in%20Cross-Lingual%20Transfer%20Paradigms%3A%0A%20%20Exploring%20Tuning%20Strategies&body=Title%3A%20Measuring%20Catastrophic%20Forgetting%20in%20Cross-Lingual%20Transfer%20Paradigms%3A%0A%20%20Exploring%20Tuning%20Strategies%0AAuthor%3A%20Boshko%20Koloski%20and%20Bla%C5%BE%20%C5%A0krlj%20and%20Marko%20Robnik-%C5%A0ikonja%20and%20Senja%20Pollak%0AAbstract%3A%20%20%20The%20cross-lingual%20transfer%20is%20a%20promising%20technique%20to%20solve%20tasks%20in%0Aless-resourced%20languages.%20In%20this%20empirical%20study%2C%20we%20compare%20two%20fine-tuning%0Aapproaches%20combined%20with%20zero-shot%20and%20full-shot%20learning%20approaches%20for%20large%0Alanguage%20models%20in%20a%20cross-lingual%20setting.%20As%20fine-tuning%20strategies%2C%20we%0Acompare%20parameter-efficient%20adapter%20methods%20with%20fine-tuning%20of%20all%20parameters.%0AAs%20cross-lingual%20transfer%20strategies%2C%20we%20compare%20the%20intermediate-training%0A%28%5Ctextit%7BIT%7D%29%20that%20uses%20each%20language%20sequentially%20and%20cross-lingual%20validation%0A%28%5Ctextit%7BCLV%7D%29%20that%20uses%20a%20target%20language%20already%20in%20the%20validation%20phase%20of%0Afine-tuning.%20We%20assess%20the%20success%20of%20transfer%20and%20the%20extent%20of%20catastrophic%0Aforgetting%20in%20a%20source%20language%20due%20to%20cross-lingual%20transfer%2C%20i.e.%2C%20how%20much%0Apreviously%20acquired%20knowledge%20is%20lost%20when%20we%20learn%20new%20information%20in%20a%0Adifferent%20language.%20The%20results%20on%20two%20different%20classification%20problems%2C%20hate%0Aspeech%20detection%20and%20product%20reviews%2C%20each%20containing%20datasets%20in%20several%0Alanguages%2C%20show%20that%20the%20%5Ctextit%7BIT%7D%20cross-lingual%20strategy%20outperforms%0A%5Ctextit%7BCLV%7D%20for%20the%20target%20language.%20Our%20findings%20indicate%20that%2C%20in%20the%0Amajority%20of%20cases%2C%20the%20%5Ctextit%7BCLV%7D%20strategy%20demonstrates%20superior%20retention%20of%0Aknowledge%20in%20the%20base%20language%20%28English%29%20compared%20to%20the%20%5Ctextit%7BIT%7D%20strategy%2C%0Awhen%20evaluating%20catastrophic%20forgetting%20in%20multiple%20cross-lingual%20transfers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06089v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Catastrophic%2520Forgetting%2520in%2520Cross-Lingual%2520Transfer%2520Paradigms%253A%250A%2520%2520Exploring%2520Tuning%2520Strategies%26entry.906535625%3DBoshko%2520Koloski%2520and%2520Bla%25C5%25BE%2520%25C5%25A0krlj%2520and%2520Marko%2520Robnik-%25C5%25A0ikonja%2520and%2520Senja%2520Pollak%26entry.1292438233%3D%2520%2520The%2520cross-lingual%2520transfer%2520is%2520a%2520promising%2520technique%2520to%2520solve%2520tasks%2520in%250Aless-resourced%2520languages.%2520In%2520this%2520empirical%2520study%252C%2520we%2520compare%2520two%2520fine-tuning%250Aapproaches%2520combined%2520with%2520zero-shot%2520and%2520full-shot%2520learning%2520approaches%2520for%2520large%250Alanguage%2520models%2520in%2520a%2520cross-lingual%2520setting.%2520As%2520fine-tuning%2520strategies%252C%2520we%250Acompare%2520parameter-efficient%2520adapter%2520methods%2520with%2520fine-tuning%2520of%2520all%2520parameters.%250AAs%2520cross-lingual%2520transfer%2520strategies%252C%2520we%2520compare%2520the%2520intermediate-training%250A%2528%255Ctextit%257BIT%257D%2529%2520that%2520uses%2520each%2520language%2520sequentially%2520and%2520cross-lingual%2520validation%250A%2528%255Ctextit%257BCLV%257D%2529%2520that%2520uses%2520a%2520target%2520language%2520already%2520in%2520the%2520validation%2520phase%2520of%250Afine-tuning.%2520We%2520assess%2520the%2520success%2520of%2520transfer%2520and%2520the%2520extent%2520of%2520catastrophic%250Aforgetting%2520in%2520a%2520source%2520language%2520due%2520to%2520cross-lingual%2520transfer%252C%2520i.e.%252C%2520how%2520much%250Apreviously%2520acquired%2520knowledge%2520is%2520lost%2520when%2520we%2520learn%2520new%2520information%2520in%2520a%250Adifferent%2520language.%2520The%2520results%2520on%2520two%2520different%2520classification%2520problems%252C%2520hate%250Aspeech%2520detection%2520and%2520product%2520reviews%252C%2520each%2520containing%2520datasets%2520in%2520several%250Alanguages%252C%2520show%2520that%2520the%2520%255Ctextit%257BIT%257D%2520cross-lingual%2520strategy%2520outperforms%250A%255Ctextit%257BCLV%257D%2520for%2520the%2520target%2520language.%2520Our%2520findings%2520indicate%2520that%252C%2520in%2520the%250Amajority%2520of%2520cases%252C%2520the%2520%255Ctextit%257BCLV%257D%2520strategy%2520demonstrates%2520superior%2520retention%2520of%250Aknowledge%2520in%2520the%2520base%2520language%2520%2528English%2529%2520compared%2520to%2520the%2520%255Ctextit%257BIT%257D%2520strategy%252C%250Awhen%2520evaluating%2520catastrophic%2520forgetting%2520in%2520multiple%2520cross-lingual%2520transfers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06089v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Catastrophic%20Forgetting%20in%20Cross-Lingual%20Transfer%20Paradigms%3A%0A%20%20Exploring%20Tuning%20Strategies&entry.906535625=Boshko%20Koloski%20and%20Bla%C5%BE%20%C5%A0krlj%20and%20Marko%20Robnik-%C5%A0ikonja%20and%20Senja%20Pollak&entry.1292438233=%20%20The%20cross-lingual%20transfer%20is%20a%20promising%20technique%20to%20solve%20tasks%20in%0Aless-resourced%20languages.%20In%20this%20empirical%20study%2C%20we%20compare%20two%20fine-tuning%0Aapproaches%20combined%20with%20zero-shot%20and%20full-shot%20learning%20approaches%20for%20large%0Alanguage%20models%20in%20a%20cross-lingual%20setting.%20As%20fine-tuning%20strategies%2C%20we%0Acompare%20parameter-efficient%20adapter%20methods%20with%20fine-tuning%20of%20all%20parameters.%0AAs%20cross-lingual%20transfer%20strategies%2C%20we%20compare%20the%20intermediate-training%0A%28%5Ctextit%7BIT%7D%29%20that%20uses%20each%20language%20sequentially%20and%20cross-lingual%20validation%0A%28%5Ctextit%7BCLV%7D%29%20that%20uses%20a%20target%20language%20already%20in%20the%20validation%20phase%20of%0Afine-tuning.%20We%20assess%20the%20success%20of%20transfer%20and%20the%20extent%20of%20catastrophic%0Aforgetting%20in%20a%20source%20language%20due%20to%20cross-lingual%20transfer%2C%20i.e.%2C%20how%20much%0Apreviously%20acquired%20knowledge%20is%20lost%20when%20we%20learn%20new%20information%20in%20a%0Adifferent%20language.%20The%20results%20on%20two%20different%20classification%20problems%2C%20hate%0Aspeech%20detection%20and%20product%20reviews%2C%20each%20containing%20datasets%20in%20several%0Alanguages%2C%20show%20that%20the%20%5Ctextit%7BIT%7D%20cross-lingual%20strategy%20outperforms%0A%5Ctextit%7BCLV%7D%20for%20the%20target%20language.%20Our%20findings%20indicate%20that%2C%20in%20the%0Amajority%20of%20cases%2C%20the%20%5Ctextit%7BCLV%7D%20strategy%20demonstrates%20superior%20retention%20of%0Aknowledge%20in%20the%20base%20language%20%28English%29%20compared%20to%20the%20%5Ctextit%7BIT%7D%20strategy%2C%0Awhen%20evaluating%20catastrophic%20forgetting%20in%20multiple%20cross-lingual%20transfers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06089v3&entry.124074799=Read"},
{"title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning", "author": "Zhen Li and Yupeng Su and Runming Yang and Congkai Xie and Zheng Wang and Zhongwei Xie and Ngai Wong and Hongxia Yang", "abstract": "  Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.\n", "link": "http://arxiv.org/abs/2501.03035v2", "date": "2025-02-17", "relevancy": 2.4566, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning&body=Title%3A%20Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning%0AAuthor%3A%20Zhen%20Li%20and%20Yupeng%20Su%20and%20Runming%20Yang%20and%20Congkai%20Xie%20and%20Zheng%20Wang%20and%20Zhongwei%20Xie%20and%20Ngai%20Wong%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20achieved%20significant%20advancements%20in%20complex%0Amathematical%20reasoning%20benchmarks%2C%20such%20as%20MATH.%20However%2C%20their%20substantial%0Acomputational%20requirements%20present%20challenges%20for%20practical%20deployment.%20Model%0Aquantization%20has%20emerged%20as%20an%20effective%20strategy%20to%20reduce%20memory%20usage%20and%0Acomputational%20costs%20by%20employing%20lower%20precision%20and%20bit-width%20representations.%0AIn%20this%20study%2C%20we%20systematically%20evaluate%20the%20impact%20of%20quantization%20on%0Amathematical%20reasoning%20tasks.%20Our%20results%20demonstrate%20that%20aggressive%0Aquantization%20methods%20like%20AWQ%20and%20GPTQ%20introduce%20up%20to%2032.39%25%20accuracy%0Adegradation%20%28average%2011.31%25%29%20on%20Llama-3%20models%2C%20particularly%20in%20numerical%0Acomputation%20and%20reasoning%20planning.%20To%20address%20this%2C%20we%20introduce%20a%0Amultidimensional%20evaluation%20framework%20combining%20qualitative%20capability%20analysis%0Aand%20quantitative%20error%20assessment.%20We%20further%20develop%20targeted%20recovery%0Astrategies%2C%20showing%20that%20fine-tuning%20quantized%20models%20on%20only%20545%20task-specific%0Aexamples%20for%203%20minutes%20on%204%20GPUs%20effectively%20restores%20reasoning%20capabilities%20to%0Anear%20full-precision%20levels.%20Additionally%2C%20our%20error%20assessment%20pipeline%0Aachieves%2098.9%25%20accuracy%20in%20diagnosing%20and%20localizing%20errors%20across%203%2C366%0Afailure%20cases%2C%20providing%20actionable%20insights%20for%20mitigating%0Aquantization-induced%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization%2520Meets%2520Reasoning%253A%2520Exploring%2520LLM%2520Low-Bit%2520Quantization%250A%2520%2520Degradation%2520for%2520Mathematical%2520Reasoning%26entry.906535625%3DZhen%2520Li%2520and%2520Yupeng%2520Su%2520and%2520Runming%2520Yang%2520and%2520Congkai%2520Xie%2520and%2520Zheng%2520Wang%2520and%2520Zhongwei%2520Xie%2520and%2520Ngai%2520Wong%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520achieved%2520significant%2520advancements%2520in%2520complex%250Amathematical%2520reasoning%2520benchmarks%252C%2520such%2520as%2520MATH.%2520However%252C%2520their%2520substantial%250Acomputational%2520requirements%2520present%2520challenges%2520for%2520practical%2520deployment.%2520Model%250Aquantization%2520has%2520emerged%2520as%2520an%2520effective%2520strategy%2520to%2520reduce%2520memory%2520usage%2520and%250Acomputational%2520costs%2520by%2520employing%2520lower%2520precision%2520and%2520bit-width%2520representations.%250AIn%2520this%2520study%252C%2520we%2520systematically%2520evaluate%2520the%2520impact%2520of%2520quantization%2520on%250Amathematical%2520reasoning%2520tasks.%2520Our%2520results%2520demonstrate%2520that%2520aggressive%250Aquantization%2520methods%2520like%2520AWQ%2520and%2520GPTQ%2520introduce%2520up%2520to%252032.39%2525%2520accuracy%250Adegradation%2520%2528average%252011.31%2525%2529%2520on%2520Llama-3%2520models%252C%2520particularly%2520in%2520numerical%250Acomputation%2520and%2520reasoning%2520planning.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%250Amultidimensional%2520evaluation%2520framework%2520combining%2520qualitative%2520capability%2520analysis%250Aand%2520quantitative%2520error%2520assessment.%2520We%2520further%2520develop%2520targeted%2520recovery%250Astrategies%252C%2520showing%2520that%2520fine-tuning%2520quantized%2520models%2520on%2520only%2520545%2520task-specific%250Aexamples%2520for%25203%2520minutes%2520on%25204%2520GPUs%2520effectively%2520restores%2520reasoning%2520capabilities%2520to%250Anear%2520full-precision%2520levels.%2520Additionally%252C%2520our%2520error%2520assessment%2520pipeline%250Aachieves%252098.9%2525%2520accuracy%2520in%2520diagnosing%2520and%2520localizing%2520errors%2520across%25203%252C366%250Afailure%2520cases%252C%2520providing%2520actionable%2520insights%2520for%2520mitigating%250Aquantization-induced%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning&entry.906535625=Zhen%20Li%20and%20Yupeng%20Su%20and%20Runming%20Yang%20and%20Congkai%20Xie%20and%20Zheng%20Wang%20and%20Zhongwei%20Xie%20and%20Ngai%20Wong%20and%20Hongxia%20Yang&entry.1292438233=%20%20Large%20language%20models%20have%20achieved%20significant%20advancements%20in%20complex%0Amathematical%20reasoning%20benchmarks%2C%20such%20as%20MATH.%20However%2C%20their%20substantial%0Acomputational%20requirements%20present%20challenges%20for%20practical%20deployment.%20Model%0Aquantization%20has%20emerged%20as%20an%20effective%20strategy%20to%20reduce%20memory%20usage%20and%0Acomputational%20costs%20by%20employing%20lower%20precision%20and%20bit-width%20representations.%0AIn%20this%20study%2C%20we%20systematically%20evaluate%20the%20impact%20of%20quantization%20on%0Amathematical%20reasoning%20tasks.%20Our%20results%20demonstrate%20that%20aggressive%0Aquantization%20methods%20like%20AWQ%20and%20GPTQ%20introduce%20up%20to%2032.39%25%20accuracy%0Adegradation%20%28average%2011.31%25%29%20on%20Llama-3%20models%2C%20particularly%20in%20numerical%0Acomputation%20and%20reasoning%20planning.%20To%20address%20this%2C%20we%20introduce%20a%0Amultidimensional%20evaluation%20framework%20combining%20qualitative%20capability%20analysis%0Aand%20quantitative%20error%20assessment.%20We%20further%20develop%20targeted%20recovery%0Astrategies%2C%20showing%20that%20fine-tuning%20quantized%20models%20on%20only%20545%20task-specific%0Aexamples%20for%203%20minutes%20on%204%20GPUs%20effectively%20restores%20reasoning%20capabilities%20to%0Anear%20full-precision%20levels.%20Additionally%2C%20our%20error%20assessment%20pipeline%0Aachieves%2098.9%25%20accuracy%20in%20diagnosing%20and%20localizing%20errors%20across%203%2C366%0Afailure%20cases%2C%20providing%20actionable%20insights%20for%20mitigating%0Aquantization-induced%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03035v2&entry.124074799=Read"},
{"title": "DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party\n  Dialogue Understanding of Conversation Systems", "author": "Jiho Kim and Woosog Chay and Hyeonji Hwang and Daeun Kyung and Hyunseung Chung and Eunbyeol Cho and Yohan Jo and Edward Choi", "abstract": "  Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversation systems, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nsystems often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, a conversation system is assigned the role of a\ncharacter from popular TV shows, requiring it to respond to spontaneous\nquestions using past dialogue information and to distinguish between known and\nunknown information. Key features of DialSim include assessing the system's\nability to respond within a reasonable time limit, handling long-term\nmulti-party dialogues, and evaluating performance under randomized questioning\nwith LongDialQA, a novel, high-quality question-answering dataset. Our\nexperiments using DialSim reveal the strengths and weaknesses of the latest\nconversation systems, offering valuable insights for future advancements in\nconversational AI. DialSim is available at https://dialsim.github.io/.\n", "link": "http://arxiv.org/abs/2406.13144v5", "date": "2025-02-17", "relevancy": 2.4483, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DialSim%3A%20A%20Real-Time%20Simulator%20for%20Evaluating%20Long-Term%20Multi-Party%0A%20%20Dialogue%20Understanding%20of%20Conversation%20Systems&body=Title%3A%20DialSim%3A%20A%20Real-Time%20Simulator%20for%20Evaluating%20Long-Term%20Multi-Party%0A%20%20Dialogue%20Understanding%20of%20Conversation%20Systems%0AAuthor%3A%20Jiho%20Kim%20and%20Woosog%20Chay%20and%20Hyeonji%20Hwang%20and%20Daeun%20Kyung%20and%20Hyunseung%20Chung%20and%20Eunbyeol%20Cho%20and%20Yohan%20Jo%20and%20Edward%20Choi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%0Aenhanced%20the%20capabilities%20of%20conversation%20systems%2C%20making%20them%20applicable%20to%0Avarious%20fields%20%28e.g.%2C%20education%29.%20Despite%20their%20progress%2C%20the%20evaluation%20of%20the%0Asystems%20often%20overlooks%20the%20complexities%20of%20real-world%20conversations%2C%20such%20as%0Areal-time%20interactions%2C%20multi-party%20dialogues%2C%20and%20extended%20contextual%0Adependencies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DialSim%2C%20a%20real-time%20dialogue%0Asimulator.%20In%20this%20simulator%2C%20a%20conversation%20system%20is%20assigned%20the%20role%20of%20a%0Acharacter%20from%20popular%20TV%20shows%2C%20requiring%20it%20to%20respond%20to%20spontaneous%0Aquestions%20using%20past%20dialogue%20information%20and%20to%20distinguish%20between%20known%20and%0Aunknown%20information.%20Key%20features%20of%20DialSim%20include%20assessing%20the%20system%27s%0Aability%20to%20respond%20within%20a%20reasonable%20time%20limit%2C%20handling%20long-term%0Amulti-party%20dialogues%2C%20and%20evaluating%20performance%20under%20randomized%20questioning%0Awith%20LongDialQA%2C%20a%20novel%2C%20high-quality%20question-answering%20dataset.%20Our%0Aexperiments%20using%20DialSim%20reveal%20the%20strengths%20and%20weaknesses%20of%20the%20latest%0Aconversation%20systems%2C%20offering%20valuable%20insights%20for%20future%20advancements%20in%0Aconversational%20AI.%20DialSim%20is%20available%20at%20https%3A//dialsim.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13144v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDialSim%253A%2520A%2520Real-Time%2520Simulator%2520for%2520Evaluating%2520Long-Term%2520Multi-Party%250A%2520%2520Dialogue%2520Understanding%2520of%2520Conversation%2520Systems%26entry.906535625%3DJiho%2520Kim%2520and%2520Woosog%2520Chay%2520and%2520Hyeonji%2520Hwang%2520and%2520Daeun%2520Kyung%2520and%2520Hyunseung%2520Chung%2520and%2520Eunbyeol%2520Cho%2520and%2520Yohan%2520Jo%2520and%2520Edward%2520Choi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520significantly%250Aenhanced%2520the%2520capabilities%2520of%2520conversation%2520systems%252C%2520making%2520them%2520applicable%2520to%250Avarious%2520fields%2520%2528e.g.%252C%2520education%2529.%2520Despite%2520their%2520progress%252C%2520the%2520evaluation%2520of%2520the%250Asystems%2520often%2520overlooks%2520the%2520complexities%2520of%2520real-world%2520conversations%252C%2520such%2520as%250Areal-time%2520interactions%252C%2520multi-party%2520dialogues%252C%2520and%2520extended%2520contextual%250Adependencies.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520DialSim%252C%2520a%2520real-time%2520dialogue%250Asimulator.%2520In%2520this%2520simulator%252C%2520a%2520conversation%2520system%2520is%2520assigned%2520the%2520role%2520of%2520a%250Acharacter%2520from%2520popular%2520TV%2520shows%252C%2520requiring%2520it%2520to%2520respond%2520to%2520spontaneous%250Aquestions%2520using%2520past%2520dialogue%2520information%2520and%2520to%2520distinguish%2520between%2520known%2520and%250Aunknown%2520information.%2520Key%2520features%2520of%2520DialSim%2520include%2520assessing%2520the%2520system%2527s%250Aability%2520to%2520respond%2520within%2520a%2520reasonable%2520time%2520limit%252C%2520handling%2520long-term%250Amulti-party%2520dialogues%252C%2520and%2520evaluating%2520performance%2520under%2520randomized%2520questioning%250Awith%2520LongDialQA%252C%2520a%2520novel%252C%2520high-quality%2520question-answering%2520dataset.%2520Our%250Aexperiments%2520using%2520DialSim%2520reveal%2520the%2520strengths%2520and%2520weaknesses%2520of%2520the%2520latest%250Aconversation%2520systems%252C%2520offering%2520valuable%2520insights%2520for%2520future%2520advancements%2520in%250Aconversational%2520AI.%2520DialSim%2520is%2520available%2520at%2520https%253A//dialsim.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13144v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DialSim%3A%20A%20Real-Time%20Simulator%20for%20Evaluating%20Long-Term%20Multi-Party%0A%20%20Dialogue%20Understanding%20of%20Conversation%20Systems&entry.906535625=Jiho%20Kim%20and%20Woosog%20Chay%20and%20Hyeonji%20Hwang%20and%20Daeun%20Kyung%20and%20Hyunseung%20Chung%20and%20Eunbyeol%20Cho%20and%20Yohan%20Jo%20and%20Edward%20Choi&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%0Aenhanced%20the%20capabilities%20of%20conversation%20systems%2C%20making%20them%20applicable%20to%0Avarious%20fields%20%28e.g.%2C%20education%29.%20Despite%20their%20progress%2C%20the%20evaluation%20of%20the%0Asystems%20often%20overlooks%20the%20complexities%20of%20real-world%20conversations%2C%20such%20as%0Areal-time%20interactions%2C%20multi-party%20dialogues%2C%20and%20extended%20contextual%0Adependencies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DialSim%2C%20a%20real-time%20dialogue%0Asimulator.%20In%20this%20simulator%2C%20a%20conversation%20system%20is%20assigned%20the%20role%20of%20a%0Acharacter%20from%20popular%20TV%20shows%2C%20requiring%20it%20to%20respond%20to%20spontaneous%0Aquestions%20using%20past%20dialogue%20information%20and%20to%20distinguish%20between%20known%20and%0Aunknown%20information.%20Key%20features%20of%20DialSim%20include%20assessing%20the%20system%27s%0Aability%20to%20respond%20within%20a%20reasonable%20time%20limit%2C%20handling%20long-term%0Amulti-party%20dialogues%2C%20and%20evaluating%20performance%20under%20randomized%20questioning%0Awith%20LongDialQA%2C%20a%20novel%2C%20high-quality%20question-answering%20dataset.%20Our%0Aexperiments%20using%20DialSim%20reveal%20the%20strengths%20and%20weaknesses%20of%20the%20latest%0Aconversation%20systems%2C%20offering%20valuable%20insights%20for%20future%20advancements%20in%0Aconversational%20AI.%20DialSim%20is%20available%20at%20https%3A//dialsim.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13144v5&entry.124074799=Read"},
{"title": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis", "author": "Xu Wang and Yan Hu and Wenyu Du and Reynold Cheng and Benyou Wang and Difan Zou", "abstract": "  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat focus on tasks where pre-trained models already perform well, we develop a\nset of mathematical tasks where fine-tuning yields substantial performance\ngains, which are closer to the practical setting. In our experiments, we\nidentify circuits at various checkpoints during fine-tuning and examine the\ninterplay between circuit analysis, fine-tuning methods, and task complexities.\nFirst, we find that while circuits maintain high node similarity before and\nafter fine-tuning, their edges undergo significant changes, which is in\ncontrast to the previous work\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat show circuits only add some additional components after fine-tuning. Based\non these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)\nmethod, which assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46\\% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n", "link": "http://arxiv.org/abs/2502.11812v1", "date": "2025-02-17", "relevancy": 2.4332, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4901}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20Fine-Tuning%20Mechanisms%20of%20LLMs%20via%20Circuit%0A%20%20Analysis&body=Title%3A%20Towards%20Understanding%20Fine-Tuning%20Mechanisms%20of%20LLMs%20via%20Circuit%0A%20%20Analysis%0AAuthor%3A%20Xu%20Wang%20and%20Yan%20Hu%20and%20Wenyu%20Du%20and%20Reynold%20Cheng%20and%20Benyou%20Wang%20and%20Difan%20Zou%0AAbstract%3A%20%20%20Fine-tuning%20significantly%20improves%20the%20performance%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20yet%20its%20underlying%20mechanisms%20remain%20poorly%20understood.%20This%20paper%20aims%0Ato%20provide%20an%20in-depth%20interpretation%20of%20the%20fine-tuning%20process%20through%0Acircuit%20analysis%2C%20a%20popular%20tool%20in%20Mechanistic%20Interpretability%20%28MI%29.%20Unlike%0Aprevious%20studies%0A%5Ccite%7Bprakash2024finetuningenhancesexistingmechanisms%2Cchhabra2024neuroplasticity%7D%0Athat%20focus%20on%20tasks%20where%20pre-trained%20models%20already%20perform%20well%2C%20we%20develop%20a%0Aset%20of%20mathematical%20tasks%20where%20fine-tuning%20yields%20substantial%20performance%0Agains%2C%20which%20are%20closer%20to%20the%20practical%20setting.%20In%20our%20experiments%2C%20we%0Aidentify%20circuits%20at%20various%20checkpoints%20during%20fine-tuning%20and%20examine%20the%0Ainterplay%20between%20circuit%20analysis%2C%20fine-tuning%20methods%2C%20and%20task%20complexities.%0AFirst%2C%20we%20find%20that%20while%20circuits%20maintain%20high%20node%20similarity%20before%20and%0Aafter%20fine-tuning%2C%20their%20edges%20undergo%20significant%20changes%2C%20which%20is%20in%0Acontrast%20to%20the%20previous%20work%0A%5Ccite%7Bprakash2024finetuningenhancesexistingmechanisms%2Cchhabra2024neuroplasticity%7D%0Athat%20show%20circuits%20only%20add%20some%20additional%20components%20after%20fine-tuning.%20Based%0Aon%20these%20observations%2C%20we%20develop%20a%20circuit-aware%20Low-Rank%20Adaptation%20%28LoRA%29%0Amethod%2C%20which%20assigns%20ranks%20to%20layers%20based%20on%20edge%20changes%20in%20the%20circuits.%0AExperimental%20results%20demonstrate%20that%20our%20circuit-based%20LoRA%20algorithm%20achieves%0Aan%20average%20performance%20improvement%20of%202.46%5C%25%20over%20standard%20LoRA%20with%20similar%0Aparameter%20sizes.%20Furthermore%2C%20we%20explore%20how%20combining%20circuits%20from%20subtasks%0Acan%20enhance%20fine-tuning%20in%20compositional%20tasks%2C%20providing%20new%20insights%20into%20the%0Adesign%20of%20such%20tasks%20and%20deepening%20the%20understanding%20of%20circuit%20dynamics%20and%0Afine-tuning%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520Fine-Tuning%2520Mechanisms%2520of%2520LLMs%2520via%2520Circuit%250A%2520%2520Analysis%26entry.906535625%3DXu%2520Wang%2520and%2520Yan%2520Hu%2520and%2520Wenyu%2520Du%2520and%2520Reynold%2520Cheng%2520and%2520Benyou%2520Wang%2520and%2520Difan%2520Zou%26entry.1292438233%3D%2520%2520Fine-tuning%2520significantly%2520improves%2520the%2520performance%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520yet%2520its%2520underlying%2520mechanisms%2520remain%2520poorly%2520understood.%2520This%2520paper%2520aims%250Ato%2520provide%2520an%2520in-depth%2520interpretation%2520of%2520the%2520fine-tuning%2520process%2520through%250Acircuit%2520analysis%252C%2520a%2520popular%2520tool%2520in%2520Mechanistic%2520Interpretability%2520%2528MI%2529.%2520Unlike%250Aprevious%2520studies%250A%255Ccite%257Bprakash2024finetuningenhancesexistingmechanisms%252Cchhabra2024neuroplasticity%257D%250Athat%2520focus%2520on%2520tasks%2520where%2520pre-trained%2520models%2520already%2520perform%2520well%252C%2520we%2520develop%2520a%250Aset%2520of%2520mathematical%2520tasks%2520where%2520fine-tuning%2520yields%2520substantial%2520performance%250Agains%252C%2520which%2520are%2520closer%2520to%2520the%2520practical%2520setting.%2520In%2520our%2520experiments%252C%2520we%250Aidentify%2520circuits%2520at%2520various%2520checkpoints%2520during%2520fine-tuning%2520and%2520examine%2520the%250Ainterplay%2520between%2520circuit%2520analysis%252C%2520fine-tuning%2520methods%252C%2520and%2520task%2520complexities.%250AFirst%252C%2520we%2520find%2520that%2520while%2520circuits%2520maintain%2520high%2520node%2520similarity%2520before%2520and%250Aafter%2520fine-tuning%252C%2520their%2520edges%2520undergo%2520significant%2520changes%252C%2520which%2520is%2520in%250Acontrast%2520to%2520the%2520previous%2520work%250A%255Ccite%257Bprakash2024finetuningenhancesexistingmechanisms%252Cchhabra2024neuroplasticity%257D%250Athat%2520show%2520circuits%2520only%2520add%2520some%2520additional%2520components%2520after%2520fine-tuning.%2520Based%250Aon%2520these%2520observations%252C%2520we%2520develop%2520a%2520circuit-aware%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%250Amethod%252C%2520which%2520assigns%2520ranks%2520to%2520layers%2520based%2520on%2520edge%2520changes%2520in%2520the%2520circuits.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520circuit-based%2520LoRA%2520algorithm%2520achieves%250Aan%2520average%2520performance%2520improvement%2520of%25202.46%255C%2525%2520over%2520standard%2520LoRA%2520with%2520similar%250Aparameter%2520sizes.%2520Furthermore%252C%2520we%2520explore%2520how%2520combining%2520circuits%2520from%2520subtasks%250Acan%2520enhance%2520fine-tuning%2520in%2520compositional%2520tasks%252C%2520providing%2520new%2520insights%2520into%2520the%250Adesign%2520of%2520such%2520tasks%2520and%2520deepening%2520the%2520understanding%2520of%2520circuit%2520dynamics%2520and%250Afine-tuning%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20Fine-Tuning%20Mechanisms%20of%20LLMs%20via%20Circuit%0A%20%20Analysis&entry.906535625=Xu%20Wang%20and%20Yan%20Hu%20and%20Wenyu%20Du%20and%20Reynold%20Cheng%20and%20Benyou%20Wang%20and%20Difan%20Zou&entry.1292438233=%20%20Fine-tuning%20significantly%20improves%20the%20performance%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20yet%20its%20underlying%20mechanisms%20remain%20poorly%20understood.%20This%20paper%20aims%0Ato%20provide%20an%20in-depth%20interpretation%20of%20the%20fine-tuning%20process%20through%0Acircuit%20analysis%2C%20a%20popular%20tool%20in%20Mechanistic%20Interpretability%20%28MI%29.%20Unlike%0Aprevious%20studies%0A%5Ccite%7Bprakash2024finetuningenhancesexistingmechanisms%2Cchhabra2024neuroplasticity%7D%0Athat%20focus%20on%20tasks%20where%20pre-trained%20models%20already%20perform%20well%2C%20we%20develop%20a%0Aset%20of%20mathematical%20tasks%20where%20fine-tuning%20yields%20substantial%20performance%0Agains%2C%20which%20are%20closer%20to%20the%20practical%20setting.%20In%20our%20experiments%2C%20we%0Aidentify%20circuits%20at%20various%20checkpoints%20during%20fine-tuning%20and%20examine%20the%0Ainterplay%20between%20circuit%20analysis%2C%20fine-tuning%20methods%2C%20and%20task%20complexities.%0AFirst%2C%20we%20find%20that%20while%20circuits%20maintain%20high%20node%20similarity%20before%20and%0Aafter%20fine-tuning%2C%20their%20edges%20undergo%20significant%20changes%2C%20which%20is%20in%0Acontrast%20to%20the%20previous%20work%0A%5Ccite%7Bprakash2024finetuningenhancesexistingmechanisms%2Cchhabra2024neuroplasticity%7D%0Athat%20show%20circuits%20only%20add%20some%20additional%20components%20after%20fine-tuning.%20Based%0Aon%20these%20observations%2C%20we%20develop%20a%20circuit-aware%20Low-Rank%20Adaptation%20%28LoRA%29%0Amethod%2C%20which%20assigns%20ranks%20to%20layers%20based%20on%20edge%20changes%20in%20the%20circuits.%0AExperimental%20results%20demonstrate%20that%20our%20circuit-based%20LoRA%20algorithm%20achieves%0Aan%20average%20performance%20improvement%20of%202.46%5C%25%20over%20standard%20LoRA%20with%20similar%0Aparameter%20sizes.%20Furthermore%2C%20we%20explore%20how%20combining%20circuits%20from%20subtasks%0Acan%20enhance%20fine-tuning%20in%20compositional%20tasks%2C%20providing%20new%20insights%20into%20the%0Adesign%20of%20such%20tasks%20and%20deepening%20the%20understanding%20of%20circuit%20dynamics%20and%0Afine-tuning%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11812v1&entry.124074799=Read"},
{"title": "GraphMorph: Tubular Structure Extraction by Morphing Predicted Graphs", "author": "Zhao Zhang and Ziwei Zhao and Dong Wang and Liwei Wang", "abstract": "  Accurately restoring topology is both challenging and crucial in tubular\nstructure extraction tasks, such as blood vessel segmentation and road network\nextraction. Diverging from traditional approaches based on pixel-level\nclassification, our proposed method, named GraphMorph, focuses on branch-level\nfeatures of tubular structures to achieve more topologically accurate\npredictions. GraphMorph comprises two main components: a Graph Decoder and a\nMorph Module. Utilizing multi-scale features extracted from an image patch by\nthe segmentation network, the Graph Decoder facilitates the learning of\nbranch-level features and generates a graph that accurately represents the\ntubular structure in this patch. The Morph Module processes two primary inputs:\nthe graph and the centerline probability map, provided by the Graph Decoder and\nthe segmentation network, respectively. Employing a novel SkeletonDijkstra\nalgorithm, the Morph Module produces a centerline mask that aligns with the\npredicted graph. Furthermore, we observe that employing centerline masks\npredicted by GraphMorph significantly reduces false positives in the\nsegmentation task, which is achieved by a simple yet effective post-processing\nstrategy. The efficacy of our method in the centerline extraction and\nsegmentation tasks has been substantiated through experimental evaluations\nacross various datasets. Source code will be released soon.\n", "link": "http://arxiv.org/abs/2502.11731v1", "date": "2025-02-17", "relevancy": 2.4322, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4873}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphMorph%3A%20Tubular%20Structure%20Extraction%20by%20Morphing%20Predicted%20Graphs&body=Title%3A%20GraphMorph%3A%20Tubular%20Structure%20Extraction%20by%20Morphing%20Predicted%20Graphs%0AAuthor%3A%20Zhao%20Zhang%20and%20Ziwei%20Zhao%20and%20Dong%20Wang%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Accurately%20restoring%20topology%20is%20both%20challenging%20and%20crucial%20in%20tubular%0Astructure%20extraction%20tasks%2C%20such%20as%20blood%20vessel%20segmentation%20and%20road%20network%0Aextraction.%20Diverging%20from%20traditional%20approaches%20based%20on%20pixel-level%0Aclassification%2C%20our%20proposed%20method%2C%20named%20GraphMorph%2C%20focuses%20on%20branch-level%0Afeatures%20of%20tubular%20structures%20to%20achieve%20more%20topologically%20accurate%0Apredictions.%20GraphMorph%20comprises%20two%20main%20components%3A%20a%20Graph%20Decoder%20and%20a%0AMorph%20Module.%20Utilizing%20multi-scale%20features%20extracted%20from%20an%20image%20patch%20by%0Athe%20segmentation%20network%2C%20the%20Graph%20Decoder%20facilitates%20the%20learning%20of%0Abranch-level%20features%20and%20generates%20a%20graph%20that%20accurately%20represents%20the%0Atubular%20structure%20in%20this%20patch.%20The%20Morph%20Module%20processes%20two%20primary%20inputs%3A%0Athe%20graph%20and%20the%20centerline%20probability%20map%2C%20provided%20by%20the%20Graph%20Decoder%20and%0Athe%20segmentation%20network%2C%20respectively.%20Employing%20a%20novel%20SkeletonDijkstra%0Aalgorithm%2C%20the%20Morph%20Module%20produces%20a%20centerline%20mask%20that%20aligns%20with%20the%0Apredicted%20graph.%20Furthermore%2C%20we%20observe%20that%20employing%20centerline%20masks%0Apredicted%20by%20GraphMorph%20significantly%20reduces%20false%20positives%20in%20the%0Asegmentation%20task%2C%20which%20is%20achieved%20by%20a%20simple%20yet%20effective%20post-processing%0Astrategy.%20The%20efficacy%20of%20our%20method%20in%20the%20centerline%20extraction%20and%0Asegmentation%20tasks%20has%20been%20substantiated%20through%20experimental%20evaluations%0Aacross%20various%20datasets.%20Source%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphMorph%253A%2520Tubular%2520Structure%2520Extraction%2520by%2520Morphing%2520Predicted%2520Graphs%26entry.906535625%3DZhao%2520Zhang%2520and%2520Ziwei%2520Zhao%2520and%2520Dong%2520Wang%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Accurately%2520restoring%2520topology%2520is%2520both%2520challenging%2520and%2520crucial%2520in%2520tubular%250Astructure%2520extraction%2520tasks%252C%2520such%2520as%2520blood%2520vessel%2520segmentation%2520and%2520road%2520network%250Aextraction.%2520Diverging%2520from%2520traditional%2520approaches%2520based%2520on%2520pixel-level%250Aclassification%252C%2520our%2520proposed%2520method%252C%2520named%2520GraphMorph%252C%2520focuses%2520on%2520branch-level%250Afeatures%2520of%2520tubular%2520structures%2520to%2520achieve%2520more%2520topologically%2520accurate%250Apredictions.%2520GraphMorph%2520comprises%2520two%2520main%2520components%253A%2520a%2520Graph%2520Decoder%2520and%2520a%250AMorph%2520Module.%2520Utilizing%2520multi-scale%2520features%2520extracted%2520from%2520an%2520image%2520patch%2520by%250Athe%2520segmentation%2520network%252C%2520the%2520Graph%2520Decoder%2520facilitates%2520the%2520learning%2520of%250Abranch-level%2520features%2520and%2520generates%2520a%2520graph%2520that%2520accurately%2520represents%2520the%250Atubular%2520structure%2520in%2520this%2520patch.%2520The%2520Morph%2520Module%2520processes%2520two%2520primary%2520inputs%253A%250Athe%2520graph%2520and%2520the%2520centerline%2520probability%2520map%252C%2520provided%2520by%2520the%2520Graph%2520Decoder%2520and%250Athe%2520segmentation%2520network%252C%2520respectively.%2520Employing%2520a%2520novel%2520SkeletonDijkstra%250Aalgorithm%252C%2520the%2520Morph%2520Module%2520produces%2520a%2520centerline%2520mask%2520that%2520aligns%2520with%2520the%250Apredicted%2520graph.%2520Furthermore%252C%2520we%2520observe%2520that%2520employing%2520centerline%2520masks%250Apredicted%2520by%2520GraphMorph%2520significantly%2520reduces%2520false%2520positives%2520in%2520the%250Asegmentation%2520task%252C%2520which%2520is%2520achieved%2520by%2520a%2520simple%2520yet%2520effective%2520post-processing%250Astrategy.%2520The%2520efficacy%2520of%2520our%2520method%2520in%2520the%2520centerline%2520extraction%2520and%250Asegmentation%2520tasks%2520has%2520been%2520substantiated%2520through%2520experimental%2520evaluations%250Aacross%2520various%2520datasets.%2520Source%2520code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphMorph%3A%20Tubular%20Structure%20Extraction%20by%20Morphing%20Predicted%20Graphs&entry.906535625=Zhao%20Zhang%20and%20Ziwei%20Zhao%20and%20Dong%20Wang%20and%20Liwei%20Wang&entry.1292438233=%20%20Accurately%20restoring%20topology%20is%20both%20challenging%20and%20crucial%20in%20tubular%0Astructure%20extraction%20tasks%2C%20such%20as%20blood%20vessel%20segmentation%20and%20road%20network%0Aextraction.%20Diverging%20from%20traditional%20approaches%20based%20on%20pixel-level%0Aclassification%2C%20our%20proposed%20method%2C%20named%20GraphMorph%2C%20focuses%20on%20branch-level%0Afeatures%20of%20tubular%20structures%20to%20achieve%20more%20topologically%20accurate%0Apredictions.%20GraphMorph%20comprises%20two%20main%20components%3A%20a%20Graph%20Decoder%20and%20a%0AMorph%20Module.%20Utilizing%20multi-scale%20features%20extracted%20from%20an%20image%20patch%20by%0Athe%20segmentation%20network%2C%20the%20Graph%20Decoder%20facilitates%20the%20learning%20of%0Abranch-level%20features%20and%20generates%20a%20graph%20that%20accurately%20represents%20the%0Atubular%20structure%20in%20this%20patch.%20The%20Morph%20Module%20processes%20two%20primary%20inputs%3A%0Athe%20graph%20and%20the%20centerline%20probability%20map%2C%20provided%20by%20the%20Graph%20Decoder%20and%0Athe%20segmentation%20network%2C%20respectively.%20Employing%20a%20novel%20SkeletonDijkstra%0Aalgorithm%2C%20the%20Morph%20Module%20produces%20a%20centerline%20mask%20that%20aligns%20with%20the%0Apredicted%20graph.%20Furthermore%2C%20we%20observe%20that%20employing%20centerline%20masks%0Apredicted%20by%20GraphMorph%20significantly%20reduces%20false%20positives%20in%20the%0Asegmentation%20task%2C%20which%20is%20achieved%20by%20a%20simple%20yet%20effective%20post-processing%0Astrategy.%20The%20efficacy%20of%20our%20method%20in%20the%20centerline%20extraction%20and%0Asegmentation%20tasks%20has%20been%20substantiated%20through%20experimental%20evaluations%0Aacross%20various%20datasets.%20Source%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11731v1&entry.124074799=Read"},
{"title": "Characterizing Photorealism and Artifacts in Diffusion Model-Generated\n  Images", "author": "Negar Kamali and Karyn Nakamura and Aakriti Kumar and Angelos Chatzimparmpas and Jessica Hullman and Matthew Groh", "abstract": "  Diffusion model-generated images can appear indistinguishable from authentic\nphotographs, but these images often contain artifacts and implausibilities that\nreveal their AI-generated provenance. Given the challenge to public trust in\nmedia posed by photorealistic AI-generated images, we conducted a large-scale\nexperiment measuring human detection accuracy on 450 diffusion-model generated\nimages and 149 real images. Based on collecting 749,828 observations and 34,675\ncomments from 50,444 participants, we find that scene complexity of an image,\nartifact types within an image, display time of an image, and human curation of\nAI-generated images all play significant roles in how accurately people\ndistinguish real from AI-generated images. Additionally, we propose a taxonomy\ncharacterizing artifacts often appearing in images generated by diffusion\nmodels. Our empirical observations and taxonomy offer nuanced insights into the\ncapabilities and limitations of diffusion models to generate photorealistic\nimages in 2024.\n", "link": "http://arxiv.org/abs/2502.11989v1", "date": "2025-02-17", "relevancy": 2.4202, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6173}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6024}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Photorealism%20and%20Artifacts%20in%20Diffusion%20Model-Generated%0A%20%20Images&body=Title%3A%20Characterizing%20Photorealism%20and%20Artifacts%20in%20Diffusion%20Model-Generated%0A%20%20Images%0AAuthor%3A%20Negar%20Kamali%20and%20Karyn%20Nakamura%20and%20Aakriti%20Kumar%20and%20Angelos%20Chatzimparmpas%20and%20Jessica%20Hullman%20and%20Matthew%20Groh%0AAbstract%3A%20%20%20Diffusion%20model-generated%20images%20can%20appear%20indistinguishable%20from%20authentic%0Aphotographs%2C%20but%20these%20images%20often%20contain%20artifacts%20and%20implausibilities%20that%0Areveal%20their%20AI-generated%20provenance.%20Given%20the%20challenge%20to%20public%20trust%20in%0Amedia%20posed%20by%20photorealistic%20AI-generated%20images%2C%20we%20conducted%20a%20large-scale%0Aexperiment%20measuring%20human%20detection%20accuracy%20on%20450%20diffusion-model%20generated%0Aimages%20and%20149%20real%20images.%20Based%20on%20collecting%20749%2C828%20observations%20and%2034%2C675%0Acomments%20from%2050%2C444%20participants%2C%20we%20find%20that%20scene%20complexity%20of%20an%20image%2C%0Aartifact%20types%20within%20an%20image%2C%20display%20time%20of%20an%20image%2C%20and%20human%20curation%20of%0AAI-generated%20images%20all%20play%20significant%20roles%20in%20how%20accurately%20people%0Adistinguish%20real%20from%20AI-generated%20images.%20Additionally%2C%20we%20propose%20a%20taxonomy%0Acharacterizing%20artifacts%20often%20appearing%20in%20images%20generated%20by%20diffusion%0Amodels.%20Our%20empirical%20observations%20and%20taxonomy%20offer%20nuanced%20insights%20into%20the%0Acapabilities%20and%20limitations%20of%20diffusion%20models%20to%20generate%20photorealistic%0Aimages%20in%202024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Photorealism%2520and%2520Artifacts%2520in%2520Diffusion%2520Model-Generated%250A%2520%2520Images%26entry.906535625%3DNegar%2520Kamali%2520and%2520Karyn%2520Nakamura%2520and%2520Aakriti%2520Kumar%2520and%2520Angelos%2520Chatzimparmpas%2520and%2520Jessica%2520Hullman%2520and%2520Matthew%2520Groh%26entry.1292438233%3D%2520%2520Diffusion%2520model-generated%2520images%2520can%2520appear%2520indistinguishable%2520from%2520authentic%250Aphotographs%252C%2520but%2520these%2520images%2520often%2520contain%2520artifacts%2520and%2520implausibilities%2520that%250Areveal%2520their%2520AI-generated%2520provenance.%2520Given%2520the%2520challenge%2520to%2520public%2520trust%2520in%250Amedia%2520posed%2520by%2520photorealistic%2520AI-generated%2520images%252C%2520we%2520conducted%2520a%2520large-scale%250Aexperiment%2520measuring%2520human%2520detection%2520accuracy%2520on%2520450%2520diffusion-model%2520generated%250Aimages%2520and%2520149%2520real%2520images.%2520Based%2520on%2520collecting%2520749%252C828%2520observations%2520and%252034%252C675%250Acomments%2520from%252050%252C444%2520participants%252C%2520we%2520find%2520that%2520scene%2520complexity%2520of%2520an%2520image%252C%250Aartifact%2520types%2520within%2520an%2520image%252C%2520display%2520time%2520of%2520an%2520image%252C%2520and%2520human%2520curation%2520of%250AAI-generated%2520images%2520all%2520play%2520significant%2520roles%2520in%2520how%2520accurately%2520people%250Adistinguish%2520real%2520from%2520AI-generated%2520images.%2520Additionally%252C%2520we%2520propose%2520a%2520taxonomy%250Acharacterizing%2520artifacts%2520often%2520appearing%2520in%2520images%2520generated%2520by%2520diffusion%250Amodels.%2520Our%2520empirical%2520observations%2520and%2520taxonomy%2520offer%2520nuanced%2520insights%2520into%2520the%250Acapabilities%2520and%2520limitations%2520of%2520diffusion%2520models%2520to%2520generate%2520photorealistic%250Aimages%2520in%25202024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Photorealism%20and%20Artifacts%20in%20Diffusion%20Model-Generated%0A%20%20Images&entry.906535625=Negar%20Kamali%20and%20Karyn%20Nakamura%20and%20Aakriti%20Kumar%20and%20Angelos%20Chatzimparmpas%20and%20Jessica%20Hullman%20and%20Matthew%20Groh&entry.1292438233=%20%20Diffusion%20model-generated%20images%20can%20appear%20indistinguishable%20from%20authentic%0Aphotographs%2C%20but%20these%20images%20often%20contain%20artifacts%20and%20implausibilities%20that%0Areveal%20their%20AI-generated%20provenance.%20Given%20the%20challenge%20to%20public%20trust%20in%0Amedia%20posed%20by%20photorealistic%20AI-generated%20images%2C%20we%20conducted%20a%20large-scale%0Aexperiment%20measuring%20human%20detection%20accuracy%20on%20450%20diffusion-model%20generated%0Aimages%20and%20149%20real%20images.%20Based%20on%20collecting%20749%2C828%20observations%20and%2034%2C675%0Acomments%20from%2050%2C444%20participants%2C%20we%20find%20that%20scene%20complexity%20of%20an%20image%2C%0Aartifact%20types%20within%20an%20image%2C%20display%20time%20of%20an%20image%2C%20and%20human%20curation%20of%0AAI-generated%20images%20all%20play%20significant%20roles%20in%20how%20accurately%20people%0Adistinguish%20real%20from%20AI-generated%20images.%20Additionally%2C%20we%20propose%20a%20taxonomy%0Acharacterizing%20artifacts%20often%20appearing%20in%20images%20generated%20by%20diffusion%0Amodels.%20Our%20empirical%20observations%20and%20taxonomy%20offer%20nuanced%20insights%20into%20the%0Acapabilities%20and%20limitations%20of%20diffusion%20models%20to%20generate%20photorealistic%0Aimages%20in%202024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11989v1&entry.124074799=Read"},
{"title": "Metalearning Continual Learning Algorithms", "author": "Kazuki Irie and R\u00f3bert Csord\u00e1s and J\u00fcrgen Schmidhuber", "abstract": "  General-purpose learning systems should improve themselves in open-ended\nfashion in ever-changing environments. Conventional learning algorithms for\nneural networks, however, suffer from catastrophic forgetting (CF), i.e.,\npreviously acquired skills are forgotten when a new task is learned. Instead of\nhand-crafting new algorithms for avoiding CF, we propose Automated Continual\nLearning (ACL) to train self-referential neural networks to metalearn their own\nin-context continual (meta)learning algorithms. ACL encodes continual learning\n(CL) desiderata -- good performance on both old and new tasks -- into its\nmetalearning objectives. Our experiments demonstrate that ACL effectively\nresolves \"in-context catastrophic forgetting,\" a problem that naive in-context\nlearning algorithms suffer from; ACL-learned algorithms outperform both\nhand-crafted learning algorithms and popular meta-continual learning methods on\nthe Split-MNIST benchmark in the replay-free setting, and enables continual\nlearning of diverse tasks consisting of multiple standard image classification\ndatasets. We also discuss the current limitations of in-context CL by comparing\nACL with state-of-the-art CL methods that leverage pre-trained models. Overall,\nwe bring several novel perspectives into the long-standing problem of CL.\n", "link": "http://arxiv.org/abs/2312.00276v3", "date": "2025-02-17", "relevancy": 2.4183, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4872}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metalearning%20Continual%20Learning%20Algorithms&body=Title%3A%20Metalearning%20Continual%20Learning%20Algorithms%0AAuthor%3A%20Kazuki%20Irie%20and%20R%C3%B3bert%20Csord%C3%A1s%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20General-purpose%20learning%20systems%20should%20improve%20themselves%20in%20open-ended%0Afashion%20in%20ever-changing%20environments.%20Conventional%20learning%20algorithms%20for%0Aneural%20networks%2C%20however%2C%20suffer%20from%20catastrophic%20forgetting%20%28CF%29%2C%20i.e.%2C%0Apreviously%20acquired%20skills%20are%20forgotten%20when%20a%20new%20task%20is%20learned.%20Instead%20of%0Ahand-crafting%20new%20algorithms%20for%20avoiding%20CF%2C%20we%20propose%20Automated%20Continual%0ALearning%20%28ACL%29%20to%20train%20self-referential%20neural%20networks%20to%20metalearn%20their%20own%0Ain-context%20continual%20%28meta%29learning%20algorithms.%20ACL%20encodes%20continual%20learning%0A%28CL%29%20desiderata%20--%20good%20performance%20on%20both%20old%20and%20new%20tasks%20--%20into%20its%0Ametalearning%20objectives.%20Our%20experiments%20demonstrate%20that%20ACL%20effectively%0Aresolves%20%22in-context%20catastrophic%20forgetting%2C%22%20a%20problem%20that%20naive%20in-context%0Alearning%20algorithms%20suffer%20from%3B%20ACL-learned%20algorithms%20outperform%20both%0Ahand-crafted%20learning%20algorithms%20and%20popular%20meta-continual%20learning%20methods%20on%0Athe%20Split-MNIST%20benchmark%20in%20the%20replay-free%20setting%2C%20and%20enables%20continual%0Alearning%20of%20diverse%20tasks%20consisting%20of%20multiple%20standard%20image%20classification%0Adatasets.%20We%20also%20discuss%20the%20current%20limitations%20of%20in-context%20CL%20by%20comparing%0AACL%20with%20state-of-the-art%20CL%20methods%20that%20leverage%20pre-trained%20models.%20Overall%2C%0Awe%20bring%20several%20novel%20perspectives%20into%20the%20long-standing%20problem%20of%20CL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00276v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetalearning%2520Continual%2520Learning%2520Algorithms%26entry.906535625%3DKazuki%2520Irie%2520and%2520R%25C3%25B3bert%2520Csord%25C3%25A1s%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520General-purpose%2520learning%2520systems%2520should%2520improve%2520themselves%2520in%2520open-ended%250Afashion%2520in%2520ever-changing%2520environments.%2520Conventional%2520learning%2520algorithms%2520for%250Aneural%2520networks%252C%2520however%252C%2520suffer%2520from%2520catastrophic%2520forgetting%2520%2528CF%2529%252C%2520i.e.%252C%250Apreviously%2520acquired%2520skills%2520are%2520forgotten%2520when%2520a%2520new%2520task%2520is%2520learned.%2520Instead%2520of%250Ahand-crafting%2520new%2520algorithms%2520for%2520avoiding%2520CF%252C%2520we%2520propose%2520Automated%2520Continual%250ALearning%2520%2528ACL%2529%2520to%2520train%2520self-referential%2520neural%2520networks%2520to%2520metalearn%2520their%2520own%250Ain-context%2520continual%2520%2528meta%2529learning%2520algorithms.%2520ACL%2520encodes%2520continual%2520learning%250A%2528CL%2529%2520desiderata%2520--%2520good%2520performance%2520on%2520both%2520old%2520and%2520new%2520tasks%2520--%2520into%2520its%250Ametalearning%2520objectives.%2520Our%2520experiments%2520demonstrate%2520that%2520ACL%2520effectively%250Aresolves%2520%2522in-context%2520catastrophic%2520forgetting%252C%2522%2520a%2520problem%2520that%2520naive%2520in-context%250Alearning%2520algorithms%2520suffer%2520from%253B%2520ACL-learned%2520algorithms%2520outperform%2520both%250Ahand-crafted%2520learning%2520algorithms%2520and%2520popular%2520meta-continual%2520learning%2520methods%2520on%250Athe%2520Split-MNIST%2520benchmark%2520in%2520the%2520replay-free%2520setting%252C%2520and%2520enables%2520continual%250Alearning%2520of%2520diverse%2520tasks%2520consisting%2520of%2520multiple%2520standard%2520image%2520classification%250Adatasets.%2520We%2520also%2520discuss%2520the%2520current%2520limitations%2520of%2520in-context%2520CL%2520by%2520comparing%250AACL%2520with%2520state-of-the-art%2520CL%2520methods%2520that%2520leverage%2520pre-trained%2520models.%2520Overall%252C%250Awe%2520bring%2520several%2520novel%2520perspectives%2520into%2520the%2520long-standing%2520problem%2520of%2520CL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00276v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metalearning%20Continual%20Learning%20Algorithms&entry.906535625=Kazuki%20Irie%20and%20R%C3%B3bert%20Csord%C3%A1s%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20General-purpose%20learning%20systems%20should%20improve%20themselves%20in%20open-ended%0Afashion%20in%20ever-changing%20environments.%20Conventional%20learning%20algorithms%20for%0Aneural%20networks%2C%20however%2C%20suffer%20from%20catastrophic%20forgetting%20%28CF%29%2C%20i.e.%2C%0Apreviously%20acquired%20skills%20are%20forgotten%20when%20a%20new%20task%20is%20learned.%20Instead%20of%0Ahand-crafting%20new%20algorithms%20for%20avoiding%20CF%2C%20we%20propose%20Automated%20Continual%0ALearning%20%28ACL%29%20to%20train%20self-referential%20neural%20networks%20to%20metalearn%20their%20own%0Ain-context%20continual%20%28meta%29learning%20algorithms.%20ACL%20encodes%20continual%20learning%0A%28CL%29%20desiderata%20--%20good%20performance%20on%20both%20old%20and%20new%20tasks%20--%20into%20its%0Ametalearning%20objectives.%20Our%20experiments%20demonstrate%20that%20ACL%20effectively%0Aresolves%20%22in-context%20catastrophic%20forgetting%2C%22%20a%20problem%20that%20naive%20in-context%0Alearning%20algorithms%20suffer%20from%3B%20ACL-learned%20algorithms%20outperform%20both%0Ahand-crafted%20learning%20algorithms%20and%20popular%20meta-continual%20learning%20methods%20on%0Athe%20Split-MNIST%20benchmark%20in%20the%20replay-free%20setting%2C%20and%20enables%20continual%0Alearning%20of%20diverse%20tasks%20consisting%20of%20multiple%20standard%20image%20classification%0Adatasets.%20We%20also%20discuss%20the%20current%20limitations%20of%20in-context%20CL%20by%20comparing%0AACL%20with%20state-of-the-art%20CL%20methods%20that%20leverage%20pre-trained%20models.%20Overall%2C%0Awe%20bring%20several%20novel%20perspectives%20into%20the%20long-standing%20problem%20of%20CL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00276v3&entry.124074799=Read"},
{"title": "Revisiting the Equivalence of Bayesian Neural Networks and Gaussian\n  Processes: On the Importance of Learning Activations", "author": "Marcin Sendera and Amin Sorkhei and Tomasz Ku\u015bmierczyk", "abstract": "  Gaussian Processes (GPs) provide a convenient framework for specifying\nfunction-space priors, making them a natural choice for modeling uncertainty.\nIn contrast, Bayesian Neural Networks (BNNs) offer greater scalability and\nextendability but lack the advantageous properties of GPs. This motivates the\ndevelopment of BNNs capable of replicating GP-like behavior. However, existing\nsolutions are either limited to specific GP kernels or rely on heuristics.\n  We demonstrate that trainable activations are crucial for effective mapping\nof GP priors to wide BNNs. Specifically, we leverage the closed-form\n2-Wasserstein distance for efficient gradient-based optimization of\nreparameterized priors and activations. Beyond learned activations, we also\nintroduce trainable periodic activations that ensure global stationarity by\ndesign, and functional priors conditioned on GP hyperparameters to allow\nefficient model selection.\n  Empirically, our method consistently outperforms existing approaches or\nmatches performance of the heuristic methods, while offering stronger\ntheoretical foundations.\n", "link": "http://arxiv.org/abs/2410.15777v2", "date": "2025-02-17", "relevancy": 2.4157, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5046}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4756}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20the%20Equivalence%20of%20Bayesian%20Neural%20Networks%20and%20Gaussian%0A%20%20Processes%3A%20On%20the%20Importance%20of%20Learning%20Activations&body=Title%3A%20Revisiting%20the%20Equivalence%20of%20Bayesian%20Neural%20Networks%20and%20Gaussian%0A%20%20Processes%3A%20On%20the%20Importance%20of%20Learning%20Activations%0AAuthor%3A%20Marcin%20Sendera%20and%20Amin%20Sorkhei%20and%20Tomasz%20Ku%C5%9Bmierczyk%0AAbstract%3A%20%20%20Gaussian%20Processes%20%28GPs%29%20provide%20a%20convenient%20framework%20for%20specifying%0Afunction-space%20priors%2C%20making%20them%20a%20natural%20choice%20for%20modeling%20uncertainty.%0AIn%20contrast%2C%20Bayesian%20Neural%20Networks%20%28BNNs%29%20offer%20greater%20scalability%20and%0Aextendability%20but%20lack%20the%20advantageous%20properties%20of%20GPs.%20This%20motivates%20the%0Adevelopment%20of%20BNNs%20capable%20of%20replicating%20GP-like%20behavior.%20However%2C%20existing%0Asolutions%20are%20either%20limited%20to%20specific%20GP%20kernels%20or%20rely%20on%20heuristics.%0A%20%20We%20demonstrate%20that%20trainable%20activations%20are%20crucial%20for%20effective%20mapping%0Aof%20GP%20priors%20to%20wide%20BNNs.%20Specifically%2C%20we%20leverage%20the%20closed-form%0A2-Wasserstein%20distance%20for%20efficient%20gradient-based%20optimization%20of%0Areparameterized%20priors%20and%20activations.%20Beyond%20learned%20activations%2C%20we%20also%0Aintroduce%20trainable%20periodic%20activations%20that%20ensure%20global%20stationarity%20by%0Adesign%2C%20and%20functional%20priors%20conditioned%20on%20GP%20hyperparameters%20to%20allow%0Aefficient%20model%20selection.%0A%20%20Empirically%2C%20our%20method%20consistently%20outperforms%20existing%20approaches%20or%0Amatches%20performance%20of%20the%20heuristic%20methods%2C%20while%20offering%20stronger%0Atheoretical%20foundations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520the%2520Equivalence%2520of%2520Bayesian%2520Neural%2520Networks%2520and%2520Gaussian%250A%2520%2520Processes%253A%2520On%2520the%2520Importance%2520of%2520Learning%2520Activations%26entry.906535625%3DMarcin%2520Sendera%2520and%2520Amin%2520Sorkhei%2520and%2520Tomasz%2520Ku%25C5%259Bmierczyk%26entry.1292438233%3D%2520%2520Gaussian%2520Processes%2520%2528GPs%2529%2520provide%2520a%2520convenient%2520framework%2520for%2520specifying%250Afunction-space%2520priors%252C%2520making%2520them%2520a%2520natural%2520choice%2520for%2520modeling%2520uncertainty.%250AIn%2520contrast%252C%2520Bayesian%2520Neural%2520Networks%2520%2528BNNs%2529%2520offer%2520greater%2520scalability%2520and%250Aextendability%2520but%2520lack%2520the%2520advantageous%2520properties%2520of%2520GPs.%2520This%2520motivates%2520the%250Adevelopment%2520of%2520BNNs%2520capable%2520of%2520replicating%2520GP-like%2520behavior.%2520However%252C%2520existing%250Asolutions%2520are%2520either%2520limited%2520to%2520specific%2520GP%2520kernels%2520or%2520rely%2520on%2520heuristics.%250A%2520%2520We%2520demonstrate%2520that%2520trainable%2520activations%2520are%2520crucial%2520for%2520effective%2520mapping%250Aof%2520GP%2520priors%2520to%2520wide%2520BNNs.%2520Specifically%252C%2520we%2520leverage%2520the%2520closed-form%250A2-Wasserstein%2520distance%2520for%2520efficient%2520gradient-based%2520optimization%2520of%250Areparameterized%2520priors%2520and%2520activations.%2520Beyond%2520learned%2520activations%252C%2520we%2520also%250Aintroduce%2520trainable%2520periodic%2520activations%2520that%2520ensure%2520global%2520stationarity%2520by%250Adesign%252C%2520and%2520functional%2520priors%2520conditioned%2520on%2520GP%2520hyperparameters%2520to%2520allow%250Aefficient%2520model%2520selection.%250A%2520%2520Empirically%252C%2520our%2520method%2520consistently%2520outperforms%2520existing%2520approaches%2520or%250Amatches%2520performance%2520of%2520the%2520heuristic%2520methods%252C%2520while%2520offering%2520stronger%250Atheoretical%2520foundations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20the%20Equivalence%20of%20Bayesian%20Neural%20Networks%20and%20Gaussian%0A%20%20Processes%3A%20On%20the%20Importance%20of%20Learning%20Activations&entry.906535625=Marcin%20Sendera%20and%20Amin%20Sorkhei%20and%20Tomasz%20Ku%C5%9Bmierczyk&entry.1292438233=%20%20Gaussian%20Processes%20%28GPs%29%20provide%20a%20convenient%20framework%20for%20specifying%0Afunction-space%20priors%2C%20making%20them%20a%20natural%20choice%20for%20modeling%20uncertainty.%0AIn%20contrast%2C%20Bayesian%20Neural%20Networks%20%28BNNs%29%20offer%20greater%20scalability%20and%0Aextendability%20but%20lack%20the%20advantageous%20properties%20of%20GPs.%20This%20motivates%20the%0Adevelopment%20of%20BNNs%20capable%20of%20replicating%20GP-like%20behavior.%20However%2C%20existing%0Asolutions%20are%20either%20limited%20to%20specific%20GP%20kernels%20or%20rely%20on%20heuristics.%0A%20%20We%20demonstrate%20that%20trainable%20activations%20are%20crucial%20for%20effective%20mapping%0Aof%20GP%20priors%20to%20wide%20BNNs.%20Specifically%2C%20we%20leverage%20the%20closed-form%0A2-Wasserstein%20distance%20for%20efficient%20gradient-based%20optimization%20of%0Areparameterized%20priors%20and%20activations.%20Beyond%20learned%20activations%2C%20we%20also%0Aintroduce%20trainable%20periodic%20activations%20that%20ensure%20global%20stationarity%20by%0Adesign%2C%20and%20functional%20priors%20conditioned%20on%20GP%20hyperparameters%20to%20allow%0Aefficient%20model%20selection.%0A%20%20Empirically%2C%20our%20method%20consistently%20outperforms%20existing%20approaches%20or%0Amatches%20performance%20of%20the%20heuristic%20methods%2C%20while%20offering%20stronger%0Atheoretical%20foundations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15777v2&entry.124074799=Read"},
{"title": "On the Universality of Self-Supervised Representation Learning", "author": "Wenwen Qiang and Jingyao Wang and Lingyu Si and Chuxiong Sun and Fuchun Sun and Hui Xiong", "abstract": "  In this paper, we investigate the characteristics that define a good\nrepresentation or model. We propose that such a representation or model should\npossess universality, characterized by: (i) discriminability: performing well\non training samples; (ii) generalization: performing well on unseen datasets;\nand (iii) transferability: performing well on unseen tasks with distribution\nshifts. Despite its importance, current self-supervised learning (SSL) methods\nlack explicit modeling of universality, and theoretical analysis remains\nunderexplored. To address these issues, we aim to explore and incorporate\nuniversality into SSL. Specifically, we first revisit SSL from a task\nperspective and find that each mini-batch can be viewed as a multi-class\nclassification task. We then propose that a universal SSL model should achieve:\n(i) learning universality by minimizing loss across all training samples, and\n(ii) evaluation universality by learning causally invariant representations\nthat generalize well to unseen tasks. To quantify this, we introduce a\n$\\sigma$-measurement that assesses the gap between the performance of SSL model\nand optimal task-specific models. Furthermore, to model universality, we\npropose the GeSSL framework. It first learns task-specific models by minimizing\nSSL loss, then incorporates future updates to enhance discriminability, and\nfinally integrates these models to learn from multiple tasks. Theoretical and\nempirical evidence supports the effectiveness of GeSSL.\n", "link": "http://arxiv.org/abs/2405.01053v4", "date": "2025-02-17", "relevancy": 2.4139, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5147}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4763}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Universality%20of%20Self-Supervised%20Representation%20Learning&body=Title%3A%20On%20the%20Universality%20of%20Self-Supervised%20Representation%20Learning%0AAuthor%3A%20Wenwen%20Qiang%20and%20Jingyao%20Wang%20and%20Lingyu%20Si%20and%20Chuxiong%20Sun%20and%20Fuchun%20Sun%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20characteristics%20that%20define%20a%20good%0Arepresentation%20or%20model.%20We%20propose%20that%20such%20a%20representation%20or%20model%20should%0Apossess%20universality%2C%20characterized%20by%3A%20%28i%29%20discriminability%3A%20performing%20well%0Aon%20training%20samples%3B%20%28ii%29%20generalization%3A%20performing%20well%20on%20unseen%20datasets%3B%0Aand%20%28iii%29%20transferability%3A%20performing%20well%20on%20unseen%20tasks%20with%20distribution%0Ashifts.%20Despite%20its%20importance%2C%20current%20self-supervised%20learning%20%28SSL%29%20methods%0Alack%20explicit%20modeling%20of%20universality%2C%20and%20theoretical%20analysis%20remains%0Aunderexplored.%20To%20address%20these%20issues%2C%20we%20aim%20to%20explore%20and%20incorporate%0Auniversality%20into%20SSL.%20Specifically%2C%20we%20first%20revisit%20SSL%20from%20a%20task%0Aperspective%20and%20find%20that%20each%20mini-batch%20can%20be%20viewed%20as%20a%20multi-class%0Aclassification%20task.%20We%20then%20propose%20that%20a%20universal%20SSL%20model%20should%20achieve%3A%0A%28i%29%20learning%20universality%20by%20minimizing%20loss%20across%20all%20training%20samples%2C%20and%0A%28ii%29%20evaluation%20universality%20by%20learning%20causally%20invariant%20representations%0Athat%20generalize%20well%20to%20unseen%20tasks.%20To%20quantify%20this%2C%20we%20introduce%20a%0A%24%5Csigma%24-measurement%20that%20assesses%20the%20gap%20between%20the%20performance%20of%20SSL%20model%0Aand%20optimal%20task-specific%20models.%20Furthermore%2C%20to%20model%20universality%2C%20we%0Apropose%20the%20GeSSL%20framework.%20It%20first%20learns%20task-specific%20models%20by%20minimizing%0ASSL%20loss%2C%20then%20incorporates%20future%20updates%20to%20enhance%20discriminability%2C%20and%0Afinally%20integrates%20these%20models%20to%20learn%20from%20multiple%20tasks.%20Theoretical%20and%0Aempirical%20evidence%20supports%20the%20effectiveness%20of%20GeSSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01053v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Universality%2520of%2520Self-Supervised%2520Representation%2520Learning%26entry.906535625%3DWenwen%2520Qiang%2520and%2520Jingyao%2520Wang%2520and%2520Lingyu%2520Si%2520and%2520Chuxiong%2520Sun%2520and%2520Fuchun%2520Sun%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520characteristics%2520that%2520define%2520a%2520good%250Arepresentation%2520or%2520model.%2520We%2520propose%2520that%2520such%2520a%2520representation%2520or%2520model%2520should%250Apossess%2520universality%252C%2520characterized%2520by%253A%2520%2528i%2529%2520discriminability%253A%2520performing%2520well%250Aon%2520training%2520samples%253B%2520%2528ii%2529%2520generalization%253A%2520performing%2520well%2520on%2520unseen%2520datasets%253B%250Aand%2520%2528iii%2529%2520transferability%253A%2520performing%2520well%2520on%2520unseen%2520tasks%2520with%2520distribution%250Ashifts.%2520Despite%2520its%2520importance%252C%2520current%2520self-supervised%2520learning%2520%2528SSL%2529%2520methods%250Alack%2520explicit%2520modeling%2520of%2520universality%252C%2520and%2520theoretical%2520analysis%2520remains%250Aunderexplored.%2520To%2520address%2520these%2520issues%252C%2520we%2520aim%2520to%2520explore%2520and%2520incorporate%250Auniversality%2520into%2520SSL.%2520Specifically%252C%2520we%2520first%2520revisit%2520SSL%2520from%2520a%2520task%250Aperspective%2520and%2520find%2520that%2520each%2520mini-batch%2520can%2520be%2520viewed%2520as%2520a%2520multi-class%250Aclassification%2520task.%2520We%2520then%2520propose%2520that%2520a%2520universal%2520SSL%2520model%2520should%2520achieve%253A%250A%2528i%2529%2520learning%2520universality%2520by%2520minimizing%2520loss%2520across%2520all%2520training%2520samples%252C%2520and%250A%2528ii%2529%2520evaluation%2520universality%2520by%2520learning%2520causally%2520invariant%2520representations%250Athat%2520generalize%2520well%2520to%2520unseen%2520tasks.%2520To%2520quantify%2520this%252C%2520we%2520introduce%2520a%250A%2524%255Csigma%2524-measurement%2520that%2520assesses%2520the%2520gap%2520between%2520the%2520performance%2520of%2520SSL%2520model%250Aand%2520optimal%2520task-specific%2520models.%2520Furthermore%252C%2520to%2520model%2520universality%252C%2520we%250Apropose%2520the%2520GeSSL%2520framework.%2520It%2520first%2520learns%2520task-specific%2520models%2520by%2520minimizing%250ASSL%2520loss%252C%2520then%2520incorporates%2520future%2520updates%2520to%2520enhance%2520discriminability%252C%2520and%250Afinally%2520integrates%2520these%2520models%2520to%2520learn%2520from%2520multiple%2520tasks.%2520Theoretical%2520and%250Aempirical%2520evidence%2520supports%2520the%2520effectiveness%2520of%2520GeSSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01053v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Universality%20of%20Self-Supervised%20Representation%20Learning&entry.906535625=Wenwen%20Qiang%20and%20Jingyao%20Wang%20and%20Lingyu%20Si%20and%20Chuxiong%20Sun%20and%20Fuchun%20Sun%20and%20Hui%20Xiong&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20characteristics%20that%20define%20a%20good%0Arepresentation%20or%20model.%20We%20propose%20that%20such%20a%20representation%20or%20model%20should%0Apossess%20universality%2C%20characterized%20by%3A%20%28i%29%20discriminability%3A%20performing%20well%0Aon%20training%20samples%3B%20%28ii%29%20generalization%3A%20performing%20well%20on%20unseen%20datasets%3B%0Aand%20%28iii%29%20transferability%3A%20performing%20well%20on%20unseen%20tasks%20with%20distribution%0Ashifts.%20Despite%20its%20importance%2C%20current%20self-supervised%20learning%20%28SSL%29%20methods%0Alack%20explicit%20modeling%20of%20universality%2C%20and%20theoretical%20analysis%20remains%0Aunderexplored.%20To%20address%20these%20issues%2C%20we%20aim%20to%20explore%20and%20incorporate%0Auniversality%20into%20SSL.%20Specifically%2C%20we%20first%20revisit%20SSL%20from%20a%20task%0Aperspective%20and%20find%20that%20each%20mini-batch%20can%20be%20viewed%20as%20a%20multi-class%0Aclassification%20task.%20We%20then%20propose%20that%20a%20universal%20SSL%20model%20should%20achieve%3A%0A%28i%29%20learning%20universality%20by%20minimizing%20loss%20across%20all%20training%20samples%2C%20and%0A%28ii%29%20evaluation%20universality%20by%20learning%20causally%20invariant%20representations%0Athat%20generalize%20well%20to%20unseen%20tasks.%20To%20quantify%20this%2C%20we%20introduce%20a%0A%24%5Csigma%24-measurement%20that%20assesses%20the%20gap%20between%20the%20performance%20of%20SSL%20model%0Aand%20optimal%20task-specific%20models.%20Furthermore%2C%20to%20model%20universality%2C%20we%0Apropose%20the%20GeSSL%20framework.%20It%20first%20learns%20task-specific%20models%20by%20minimizing%0ASSL%20loss%2C%20then%20incorporates%20future%20updates%20to%20enhance%20discriminability%2C%20and%0Afinally%20integrates%20these%20models%20to%20learn%20from%20multiple%20tasks.%20Theoretical%20and%0Aempirical%20evidence%20supports%20the%20effectiveness%20of%20GeSSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01053v4&entry.124074799=Read"},
{"title": "Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code\n  Generation Capabilities", "author": "Hanbin Wang and Xiaoxuan Zhou and Zhipeng Xu and Keyuan Cheng and Yuxin Zuo and Kai Tian and Jingwei Song and Junting Lu and Wenhui Hu and Xueyang Liu", "abstract": "  This paper introduces Code-Vision, a benchmark designed to evaluate the\nlogical understanding and code generation capabilities of Multimodal Large\nLanguage Models (MLLMs). It challenges MLLMs to generate a correct program that\nfulfills specific functionality requirements based on a given flowchart, which\nvisually represents the desired algorithm or process. Code-Vision comprises\nthree subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding\nabilities across basic programming, algorithmic, and mathematical\nproblem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.\nExperimental results demonstrate that there is a large performance difference\nbetween proprietary and open-source models. On Hard problems, GPT-4o can\nachieve 79.3% pass@1, but the best open-source model only achieves 15%. Further\nexperiments reveal that Code-Vision can pose unique challenges compared to\nother multimodal reasoning benchmarks MMCode and MathVista. We also explore the\nreason for the poor performance of the open-source models. All data and codes\nare available at https://github.com/wanghanbinpanda/CodeVision.\n", "link": "http://arxiv.org/abs/2502.11829v1", "date": "2025-02-17", "relevancy": 2.4048, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Code-Vision%3A%20Evaluating%20Multimodal%20LLMs%20Logic%20Understanding%20and%20Code%0A%20%20Generation%20Capabilities&body=Title%3A%20Code-Vision%3A%20Evaluating%20Multimodal%20LLMs%20Logic%20Understanding%20and%20Code%0A%20%20Generation%20Capabilities%0AAuthor%3A%20Hanbin%20Wang%20and%20Xiaoxuan%20Zhou%20and%20Zhipeng%20Xu%20and%20Keyuan%20Cheng%20and%20Yuxin%20Zuo%20and%20Kai%20Tian%20and%20Jingwei%20Song%20and%20Junting%20Lu%20and%20Wenhui%20Hu%20and%20Xueyang%20Liu%0AAbstract%3A%20%20%20This%20paper%20introduces%20Code-Vision%2C%20a%20benchmark%20designed%20to%20evaluate%20the%0Alogical%20understanding%20and%20code%20generation%20capabilities%20of%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29.%20It%20challenges%20MLLMs%20to%20generate%20a%20correct%20program%20that%0Afulfills%20specific%20functionality%20requirements%20based%20on%20a%20given%20flowchart%2C%20which%0Avisually%20represents%20the%20desired%20algorithm%20or%20process.%20Code-Vision%20comprises%0Athree%20subsets%3A%20HumanEval-V%2C%20Algorithm%2C%20and%20MATH%2C%20which%20evaluate%20MLLMs%27%20coding%0Aabilities%20across%20basic%20programming%2C%20algorithmic%2C%20and%20mathematical%0Aproblem-solving%20domains.%20Our%20experiments%20evaluate%2012%20MLLMs%20on%20Code-Vision.%0AExperimental%20results%20demonstrate%20that%20there%20is%20a%20large%20performance%20difference%0Abetween%20proprietary%20and%20open-source%20models.%20On%20Hard%20problems%2C%20GPT-4o%20can%0Aachieve%2079.3%25%20pass%401%2C%20but%20the%20best%20open-source%20model%20only%20achieves%2015%25.%20Further%0Aexperiments%20reveal%20that%20Code-Vision%20can%20pose%20unique%20challenges%20compared%20to%0Aother%20multimodal%20reasoning%20benchmarks%20MMCode%20and%20MathVista.%20We%20also%20explore%20the%0Areason%20for%20the%20poor%20performance%20of%20the%20open-source%20models.%20All%20data%20and%20codes%0Aare%20available%20at%20https%3A//github.com/wanghanbinpanda/CodeVision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCode-Vision%253A%2520Evaluating%2520Multimodal%2520LLMs%2520Logic%2520Understanding%2520and%2520Code%250A%2520%2520Generation%2520Capabilities%26entry.906535625%3DHanbin%2520Wang%2520and%2520Xiaoxuan%2520Zhou%2520and%2520Zhipeng%2520Xu%2520and%2520Keyuan%2520Cheng%2520and%2520Yuxin%2520Zuo%2520and%2520Kai%2520Tian%2520and%2520Jingwei%2520Song%2520and%2520Junting%2520Lu%2520and%2520Wenhui%2520Hu%2520and%2520Xueyang%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Code-Vision%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520the%250Alogical%2520understanding%2520and%2520code%2520generation%2520capabilities%2520of%2520Multimodal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529.%2520It%2520challenges%2520MLLMs%2520to%2520generate%2520a%2520correct%2520program%2520that%250Afulfills%2520specific%2520functionality%2520requirements%2520based%2520on%2520a%2520given%2520flowchart%252C%2520which%250Avisually%2520represents%2520the%2520desired%2520algorithm%2520or%2520process.%2520Code-Vision%2520comprises%250Athree%2520subsets%253A%2520HumanEval-V%252C%2520Algorithm%252C%2520and%2520MATH%252C%2520which%2520evaluate%2520MLLMs%2527%2520coding%250Aabilities%2520across%2520basic%2520programming%252C%2520algorithmic%252C%2520and%2520mathematical%250Aproblem-solving%2520domains.%2520Our%2520experiments%2520evaluate%252012%2520MLLMs%2520on%2520Code-Vision.%250AExperimental%2520results%2520demonstrate%2520that%2520there%2520is%2520a%2520large%2520performance%2520difference%250Abetween%2520proprietary%2520and%2520open-source%2520models.%2520On%2520Hard%2520problems%252C%2520GPT-4o%2520can%250Aachieve%252079.3%2525%2520pass%25401%252C%2520but%2520the%2520best%2520open-source%2520model%2520only%2520achieves%252015%2525.%2520Further%250Aexperiments%2520reveal%2520that%2520Code-Vision%2520can%2520pose%2520unique%2520challenges%2520compared%2520to%250Aother%2520multimodal%2520reasoning%2520benchmarks%2520MMCode%2520and%2520MathVista.%2520We%2520also%2520explore%2520the%250Areason%2520for%2520the%2520poor%2520performance%2520of%2520the%2520open-source%2520models.%2520All%2520data%2520and%2520codes%250Aare%2520available%2520at%2520https%253A//github.com/wanghanbinpanda/CodeVision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Code-Vision%3A%20Evaluating%20Multimodal%20LLMs%20Logic%20Understanding%20and%20Code%0A%20%20Generation%20Capabilities&entry.906535625=Hanbin%20Wang%20and%20Xiaoxuan%20Zhou%20and%20Zhipeng%20Xu%20and%20Keyuan%20Cheng%20and%20Yuxin%20Zuo%20and%20Kai%20Tian%20and%20Jingwei%20Song%20and%20Junting%20Lu%20and%20Wenhui%20Hu%20and%20Xueyang%20Liu&entry.1292438233=%20%20This%20paper%20introduces%20Code-Vision%2C%20a%20benchmark%20designed%20to%20evaluate%20the%0Alogical%20understanding%20and%20code%20generation%20capabilities%20of%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29.%20It%20challenges%20MLLMs%20to%20generate%20a%20correct%20program%20that%0Afulfills%20specific%20functionality%20requirements%20based%20on%20a%20given%20flowchart%2C%20which%0Avisually%20represents%20the%20desired%20algorithm%20or%20process.%20Code-Vision%20comprises%0Athree%20subsets%3A%20HumanEval-V%2C%20Algorithm%2C%20and%20MATH%2C%20which%20evaluate%20MLLMs%27%20coding%0Aabilities%20across%20basic%20programming%2C%20algorithmic%2C%20and%20mathematical%0Aproblem-solving%20domains.%20Our%20experiments%20evaluate%2012%20MLLMs%20on%20Code-Vision.%0AExperimental%20results%20demonstrate%20that%20there%20is%20a%20large%20performance%20difference%0Abetween%20proprietary%20and%20open-source%20models.%20On%20Hard%20problems%2C%20GPT-4o%20can%0Aachieve%2079.3%25%20pass%401%2C%20but%20the%20best%20open-source%20model%20only%20achieves%2015%25.%20Further%0Aexperiments%20reveal%20that%20Code-Vision%20can%20pose%20unique%20challenges%20compared%20to%0Aother%20multimodal%20reasoning%20benchmarks%20MMCode%20and%20MathVista.%20We%20also%20explore%20the%0Areason%20for%20the%20poor%20performance%20of%20the%20open-source%20models.%20All%20data%20and%20codes%0Aare%20available%20at%20https%3A//github.com/wanghanbinpanda/CodeVision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11829v1&entry.124074799=Read"},
{"title": "Small Models Struggle to Learn from Strong Reasoners", "author": "Yuetai Li and Xiang Yue and Zhangchen Xu and Fengqing Jiang and Luyao Niu and Bill Yuchen Lin and Bhaskar Ramasubramanian and Radha Poovendran", "abstract": "  Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.\n", "link": "http://arxiv.org/abs/2502.12143v1", "date": "2025-02-17", "relevancy": 2.4032, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small%20Models%20Struggle%20to%20Learn%20from%20Strong%20Reasoners&body=Title%3A%20Small%20Models%20Struggle%20to%20Learn%20from%20Strong%20Reasoners%0AAuthor%3A%20Yuetai%20Li%20and%20Xiang%20Yue%20and%20Zhangchen%20Xu%20and%20Fengqing%20Jiang%20and%20Luyao%20Niu%20and%20Bill%20Yuchen%20Lin%20and%20Bhaskar%20Ramasubramanian%20and%20Radha%20Poovendran%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20complex%20reasoning%20tasks%2C%20and%20distilling%0Atheir%20reasoning%20capabilities%20into%20smaller%20models%20has%20shown%20promise.%20However%2C%20we%0Auncover%20an%20interesting%20phenomenon%2C%20which%20we%20term%20the%20Small%20Model%20Learnability%0AGap%3A%20small%20models%20%28%24%5Cleq%243B%20parameters%29%20do%20not%20consistently%20benefit%20from%20long%0Achain-of-thought%20%28CoT%29%20reasoning%20or%20distillation%20from%20larger%20models.%20Instead%2C%0Athey%20perform%20better%20when%20fine-tuned%20on%20shorter%2C%20simpler%20reasoning%20chains%20that%0Abetter%20align%20with%20their%20intrinsic%20learning%20capacity.%20To%20address%20this%2C%20we%0Apropose%20Mix%20Distillation%2C%20a%20simple%20yet%20effective%20strategy%20that%20balances%0Areasoning%20complexity%20by%20combining%20long%20and%20short%20CoT%20examples%20or%20reasoning%20from%0Aboth%20larger%20and%20smaller%20models.%20Our%20experiments%20demonstrate%20that%20Mix%0ADistillation%20significantly%20improves%20small%20model%20reasoning%20performance%20compared%0Ato%20training%20on%20either%20data%20alone.%20These%20findings%20highlight%20the%20limitations%20of%0Adirect%20strong%20model%20distillation%20and%20underscore%20the%20importance%20of%20adapting%0Areasoning%20complexity%20for%20effective%20reasoning%20capability%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall%2520Models%2520Struggle%2520to%2520Learn%2520from%2520Strong%2520Reasoners%26entry.906535625%3DYuetai%2520Li%2520and%2520Xiang%2520Yue%2520and%2520Zhangchen%2520Xu%2520and%2520Fengqing%2520Jiang%2520and%2520Luyao%2520Niu%2520and%2520Bill%2520Yuchen%2520Lin%2520and%2520Bhaskar%2520Ramasubramanian%2520and%2520Radha%2520Poovendran%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520in%2520complex%2520reasoning%2520tasks%252C%2520and%2520distilling%250Atheir%2520reasoning%2520capabilities%2520into%2520smaller%2520models%2520has%2520shown%2520promise.%2520However%252C%2520we%250Auncover%2520an%2520interesting%2520phenomenon%252C%2520which%2520we%2520term%2520the%2520Small%2520Model%2520Learnability%250AGap%253A%2520small%2520models%2520%2528%2524%255Cleq%25243B%2520parameters%2529%2520do%2520not%2520consistently%2520benefit%2520from%2520long%250Achain-of-thought%2520%2528CoT%2529%2520reasoning%2520or%2520distillation%2520from%2520larger%2520models.%2520Instead%252C%250Athey%2520perform%2520better%2520when%2520fine-tuned%2520on%2520shorter%252C%2520simpler%2520reasoning%2520chains%2520that%250Abetter%2520align%2520with%2520their%2520intrinsic%2520learning%2520capacity.%2520To%2520address%2520this%252C%2520we%250Apropose%2520Mix%2520Distillation%252C%2520a%2520simple%2520yet%2520effective%2520strategy%2520that%2520balances%250Areasoning%2520complexity%2520by%2520combining%2520long%2520and%2520short%2520CoT%2520examples%2520or%2520reasoning%2520from%250Aboth%2520larger%2520and%2520smaller%2520models.%2520Our%2520experiments%2520demonstrate%2520that%2520Mix%250ADistillation%2520significantly%2520improves%2520small%2520model%2520reasoning%2520performance%2520compared%250Ato%2520training%2520on%2520either%2520data%2520alone.%2520These%2520findings%2520highlight%2520the%2520limitations%2520of%250Adirect%2520strong%2520model%2520distillation%2520and%2520underscore%2520the%2520importance%2520of%2520adapting%250Areasoning%2520complexity%2520for%2520effective%2520reasoning%2520capability%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20Models%20Struggle%20to%20Learn%20from%20Strong%20Reasoners&entry.906535625=Yuetai%20Li%20and%20Xiang%20Yue%20and%20Zhangchen%20Xu%20and%20Fengqing%20Jiang%20and%20Luyao%20Niu%20and%20Bill%20Yuchen%20Lin%20and%20Bhaskar%20Ramasubramanian%20and%20Radha%20Poovendran&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20complex%20reasoning%20tasks%2C%20and%20distilling%0Atheir%20reasoning%20capabilities%20into%20smaller%20models%20has%20shown%20promise.%20However%2C%20we%0Auncover%20an%20interesting%20phenomenon%2C%20which%20we%20term%20the%20Small%20Model%20Learnability%0AGap%3A%20small%20models%20%28%24%5Cleq%243B%20parameters%29%20do%20not%20consistently%20benefit%20from%20long%0Achain-of-thought%20%28CoT%29%20reasoning%20or%20distillation%20from%20larger%20models.%20Instead%2C%0Athey%20perform%20better%20when%20fine-tuned%20on%20shorter%2C%20simpler%20reasoning%20chains%20that%0Abetter%20align%20with%20their%20intrinsic%20learning%20capacity.%20To%20address%20this%2C%20we%0Apropose%20Mix%20Distillation%2C%20a%20simple%20yet%20effective%20strategy%20that%20balances%0Areasoning%20complexity%20by%20combining%20long%20and%20short%20CoT%20examples%20or%20reasoning%20from%0Aboth%20larger%20and%20smaller%20models.%20Our%20experiments%20demonstrate%20that%20Mix%0ADistillation%20significantly%20improves%20small%20model%20reasoning%20performance%20compared%0Ato%20training%20on%20either%20data%20alone.%20These%20findings%20highlight%20the%20limitations%20of%0Adirect%20strong%20model%20distillation%20and%20underscore%20the%20importance%20of%20adapting%0Areasoning%20complexity%20for%20effective%20reasoning%20capability%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12143v1&entry.124074799=Read"},
{"title": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views", "author": "Shangzhan Zhang and Jianyuan Wang and Yinghao Xu and Nan Xue and Christian Rupprecht and Xiaowei Zhou and Yujun Shen and Gordon Wetzstein", "abstract": "  We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https://zhanghe3z.github.io/FLARE/\n", "link": "http://arxiv.org/abs/2502.12138v1", "date": "2025-02-17", "relevancy": 2.3988, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6327}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6022}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLARE%3A%20Feed-forward%20Geometry%2C%20Appearance%20and%20Camera%20Estimation%20from%0A%20%20Uncalibrated%20Sparse%20Views&body=Title%3A%20FLARE%3A%20Feed-forward%20Geometry%2C%20Appearance%20and%20Camera%20Estimation%20from%0A%20%20Uncalibrated%20Sparse%20Views%0AAuthor%3A%20Shangzhan%20Zhang%20and%20Jianyuan%20Wang%20and%20Yinghao%20Xu%20and%20Nan%20Xue%20and%20Christian%20Rupprecht%20and%20Xiaowei%20Zhou%20and%20Yujun%20Shen%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20We%20present%20FLARE%2C%20a%20feed-forward%20model%20designed%20to%20infer%20high-quality%20camera%0Aposes%20and%203D%20geometry%20from%20uncalibrated%20sparse-view%20images%20%28i.e.%2C%20as%20few%20as%202-8%0Ainputs%29%2C%20which%20is%20a%20challenging%20yet%20practical%20setting%20in%20real-world%0Aapplications.%20Our%20solution%20features%20a%20cascaded%20learning%20paradigm%20with%20camera%0Apose%20serving%20as%20the%20critical%20bridge%2C%20recognizing%20its%20essential%20role%20in%20mapping%0A3D%20structures%20onto%202D%20image%20planes.%20Concretely%2C%20FLARE%20starts%20with%20camera%20pose%0Aestimation%2C%20whose%20results%20condition%20the%20subsequent%20learning%20of%20geometric%0Astructure%20and%20appearance%2C%20optimized%20through%20the%20objectives%20of%20geometry%0Areconstruction%20and%20novel-view%20synthesis.%20Utilizing%20large-scale%20public%20datasets%0Afor%20training%2C%20our%20method%20delivers%20state-of-the-art%20performance%20in%20the%20tasks%20of%0Apose%20estimation%2C%20geometry%20reconstruction%2C%20and%20novel%20view%20synthesis%2C%20while%0Amaintaining%20the%20inference%20efficiency%20%28i.e.%2C%20less%20than%200.5%20seconds%29.%20The%20project%0Apage%20and%20code%20can%20be%20found%20at%3A%20https%3A//zhanghe3z.github.io/FLARE/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLARE%253A%2520Feed-forward%2520Geometry%252C%2520Appearance%2520and%2520Camera%2520Estimation%2520from%250A%2520%2520Uncalibrated%2520Sparse%2520Views%26entry.906535625%3DShangzhan%2520Zhang%2520and%2520Jianyuan%2520Wang%2520and%2520Yinghao%2520Xu%2520and%2520Nan%2520Xue%2520and%2520Christian%2520Rupprecht%2520and%2520Xiaowei%2520Zhou%2520and%2520Yujun%2520Shen%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3D%2520%2520We%2520present%2520FLARE%252C%2520a%2520feed-forward%2520model%2520designed%2520to%2520infer%2520high-quality%2520camera%250Aposes%2520and%25203D%2520geometry%2520from%2520uncalibrated%2520sparse-view%2520images%2520%2528i.e.%252C%2520as%2520few%2520as%25202-8%250Ainputs%2529%252C%2520which%2520is%2520a%2520challenging%2520yet%2520practical%2520setting%2520in%2520real-world%250Aapplications.%2520Our%2520solution%2520features%2520a%2520cascaded%2520learning%2520paradigm%2520with%2520camera%250Apose%2520serving%2520as%2520the%2520critical%2520bridge%252C%2520recognizing%2520its%2520essential%2520role%2520in%2520mapping%250A3D%2520structures%2520onto%25202D%2520image%2520planes.%2520Concretely%252C%2520FLARE%2520starts%2520with%2520camera%2520pose%250Aestimation%252C%2520whose%2520results%2520condition%2520the%2520subsequent%2520learning%2520of%2520geometric%250Astructure%2520and%2520appearance%252C%2520optimized%2520through%2520the%2520objectives%2520of%2520geometry%250Areconstruction%2520and%2520novel-view%2520synthesis.%2520Utilizing%2520large-scale%2520public%2520datasets%250Afor%2520training%252C%2520our%2520method%2520delivers%2520state-of-the-art%2520performance%2520in%2520the%2520tasks%2520of%250Apose%2520estimation%252C%2520geometry%2520reconstruction%252C%2520and%2520novel%2520view%2520synthesis%252C%2520while%250Amaintaining%2520the%2520inference%2520efficiency%2520%2528i.e.%252C%2520less%2520than%25200.5%2520seconds%2529.%2520The%2520project%250Apage%2520and%2520code%2520can%2520be%2520found%2520at%253A%2520https%253A//zhanghe3z.github.io/FLARE/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLARE%3A%20Feed-forward%20Geometry%2C%20Appearance%20and%20Camera%20Estimation%20from%0A%20%20Uncalibrated%20Sparse%20Views&entry.906535625=Shangzhan%20Zhang%20and%20Jianyuan%20Wang%20and%20Yinghao%20Xu%20and%20Nan%20Xue%20and%20Christian%20Rupprecht%20and%20Xiaowei%20Zhou%20and%20Yujun%20Shen%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20We%20present%20FLARE%2C%20a%20feed-forward%20model%20designed%20to%20infer%20high-quality%20camera%0Aposes%20and%203D%20geometry%20from%20uncalibrated%20sparse-view%20images%20%28i.e.%2C%20as%20few%20as%202-8%0Ainputs%29%2C%20which%20is%20a%20challenging%20yet%20practical%20setting%20in%20real-world%0Aapplications.%20Our%20solution%20features%20a%20cascaded%20learning%20paradigm%20with%20camera%0Apose%20serving%20as%20the%20critical%20bridge%2C%20recognizing%20its%20essential%20role%20in%20mapping%0A3D%20structures%20onto%202D%20image%20planes.%20Concretely%2C%20FLARE%20starts%20with%20camera%20pose%0Aestimation%2C%20whose%20results%20condition%20the%20subsequent%20learning%20of%20geometric%0Astructure%20and%20appearance%2C%20optimized%20through%20the%20objectives%20of%20geometry%0Areconstruction%20and%20novel-view%20synthesis.%20Utilizing%20large-scale%20public%20datasets%0Afor%20training%2C%20our%20method%20delivers%20state-of-the-art%20performance%20in%20the%20tasks%20of%0Apose%20estimation%2C%20geometry%20reconstruction%2C%20and%20novel%20view%20synthesis%2C%20while%0Amaintaining%20the%20inference%20efficiency%20%28i.e.%2C%20less%20than%200.5%20seconds%29.%20The%20project%0Apage%20and%20code%20can%20be%20found%20at%3A%20https%3A//zhanghe3z.github.io/FLARE/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12138v1&entry.124074799=Read"},
{"title": "BoxMAC -- A Boxing Dataset for Multi-label Action Classification", "author": "Shashikanta Sahoo", "abstract": "  In competitive combat sports like boxing, analyzing a boxers's performance\nstatics is crucial for evaluating the quantity and variety of punches delivered\nduring bouts. These statistics provide valuable data and feedback, which are\nroutinely used for coaching and performance enhancement. We introduce BoxMAC, a\nreal-world boxing dataset featuring 15 professional boxers and encompassing 13\ndistinct action labels. Comprising over 60,000 frames, our dataset has been\nmeticulously annotated for multiple actions per frame with inputs from a boxing\ncoach. Since two boxers can execute different punches within a single\ntimestamp, this problem falls under the domain of multi-label action\nclassification. We propose a novel architecture for jointly recognizing\nmultiple actions in both individual images and videos. We investigate baselines\nusing deep neural network architectures to address both tasks. We believe that\nBoxMAC will enable researchers and practitioners to develop and evaluate more\nefficient models for performance analysis. With its realistic and diverse\nnature, BoxMAC can serve as a valuable resource for the advancement of boxing\nas a sport\n", "link": "http://arxiv.org/abs/2412.18204v2", "date": "2025-02-17", "relevancy": 2.3931, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4904}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4744}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoxMAC%20--%20A%20Boxing%20Dataset%20for%20Multi-label%20Action%20Classification&body=Title%3A%20BoxMAC%20--%20A%20Boxing%20Dataset%20for%20Multi-label%20Action%20Classification%0AAuthor%3A%20Shashikanta%20Sahoo%0AAbstract%3A%20%20%20In%20competitive%20combat%20sports%20like%20boxing%2C%20analyzing%20a%20boxers%27s%20performance%0Astatics%20is%20crucial%20for%20evaluating%20the%20quantity%20and%20variety%20of%20punches%20delivered%0Aduring%20bouts.%20These%20statistics%20provide%20valuable%20data%20and%20feedback%2C%20which%20are%0Aroutinely%20used%20for%20coaching%20and%20performance%20enhancement.%20We%20introduce%20BoxMAC%2C%20a%0Areal-world%20boxing%20dataset%20featuring%2015%20professional%20boxers%20and%20encompassing%2013%0Adistinct%20action%20labels.%20Comprising%20over%2060%2C000%20frames%2C%20our%20dataset%20has%20been%0Ameticulously%20annotated%20for%20multiple%20actions%20per%20frame%20with%20inputs%20from%20a%20boxing%0Acoach.%20Since%20two%20boxers%20can%20execute%20different%20punches%20within%20a%20single%0Atimestamp%2C%20this%20problem%20falls%20under%20the%20domain%20of%20multi-label%20action%0Aclassification.%20We%20propose%20a%20novel%20architecture%20for%20jointly%20recognizing%0Amultiple%20actions%20in%20both%20individual%20images%20and%20videos.%20We%20investigate%20baselines%0Ausing%20deep%20neural%20network%20architectures%20to%20address%20both%20tasks.%20We%20believe%20that%0ABoxMAC%20will%20enable%20researchers%20and%20practitioners%20to%20develop%20and%20evaluate%20more%0Aefficient%20models%20for%20performance%20analysis.%20With%20its%20realistic%20and%20diverse%0Anature%2C%20BoxMAC%20can%20serve%20as%20a%20valuable%20resource%20for%20the%20advancement%20of%20boxing%0Aas%20a%20sport%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoxMAC%2520--%2520A%2520Boxing%2520Dataset%2520for%2520Multi-label%2520Action%2520Classification%26entry.906535625%3DShashikanta%2520Sahoo%26entry.1292438233%3D%2520%2520In%2520competitive%2520combat%2520sports%2520like%2520boxing%252C%2520analyzing%2520a%2520boxers%2527s%2520performance%250Astatics%2520is%2520crucial%2520for%2520evaluating%2520the%2520quantity%2520and%2520variety%2520of%2520punches%2520delivered%250Aduring%2520bouts.%2520These%2520statistics%2520provide%2520valuable%2520data%2520and%2520feedback%252C%2520which%2520are%250Aroutinely%2520used%2520for%2520coaching%2520and%2520performance%2520enhancement.%2520We%2520introduce%2520BoxMAC%252C%2520a%250Areal-world%2520boxing%2520dataset%2520featuring%252015%2520professional%2520boxers%2520and%2520encompassing%252013%250Adistinct%2520action%2520labels.%2520Comprising%2520over%252060%252C000%2520frames%252C%2520our%2520dataset%2520has%2520been%250Ameticulously%2520annotated%2520for%2520multiple%2520actions%2520per%2520frame%2520with%2520inputs%2520from%2520a%2520boxing%250Acoach.%2520Since%2520two%2520boxers%2520can%2520execute%2520different%2520punches%2520within%2520a%2520single%250Atimestamp%252C%2520this%2520problem%2520falls%2520under%2520the%2520domain%2520of%2520multi-label%2520action%250Aclassification.%2520We%2520propose%2520a%2520novel%2520architecture%2520for%2520jointly%2520recognizing%250Amultiple%2520actions%2520in%2520both%2520individual%2520images%2520and%2520videos.%2520We%2520investigate%2520baselines%250Ausing%2520deep%2520neural%2520network%2520architectures%2520to%2520address%2520both%2520tasks.%2520We%2520believe%2520that%250ABoxMAC%2520will%2520enable%2520researchers%2520and%2520practitioners%2520to%2520develop%2520and%2520evaluate%2520more%250Aefficient%2520models%2520for%2520performance%2520analysis.%2520With%2520its%2520realistic%2520and%2520diverse%250Anature%252C%2520BoxMAC%2520can%2520serve%2520as%2520a%2520valuable%2520resource%2520for%2520the%2520advancement%2520of%2520boxing%250Aas%2520a%2520sport%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoxMAC%20--%20A%20Boxing%20Dataset%20for%20Multi-label%20Action%20Classification&entry.906535625=Shashikanta%20Sahoo&entry.1292438233=%20%20In%20competitive%20combat%20sports%20like%20boxing%2C%20analyzing%20a%20boxers%27s%20performance%0Astatics%20is%20crucial%20for%20evaluating%20the%20quantity%20and%20variety%20of%20punches%20delivered%0Aduring%20bouts.%20These%20statistics%20provide%20valuable%20data%20and%20feedback%2C%20which%20are%0Aroutinely%20used%20for%20coaching%20and%20performance%20enhancement.%20We%20introduce%20BoxMAC%2C%20a%0Areal-world%20boxing%20dataset%20featuring%2015%20professional%20boxers%20and%20encompassing%2013%0Adistinct%20action%20labels.%20Comprising%20over%2060%2C000%20frames%2C%20our%20dataset%20has%20been%0Ameticulously%20annotated%20for%20multiple%20actions%20per%20frame%20with%20inputs%20from%20a%20boxing%0Acoach.%20Since%20two%20boxers%20can%20execute%20different%20punches%20within%20a%20single%0Atimestamp%2C%20this%20problem%20falls%20under%20the%20domain%20of%20multi-label%20action%0Aclassification.%20We%20propose%20a%20novel%20architecture%20for%20jointly%20recognizing%0Amultiple%20actions%20in%20both%20individual%20images%20and%20videos.%20We%20investigate%20baselines%0Ausing%20deep%20neural%20network%20architectures%20to%20address%20both%20tasks.%20We%20believe%20that%0ABoxMAC%20will%20enable%20researchers%20and%20practitioners%20to%20develop%20and%20evaluate%20more%0Aefficient%20models%20for%20performance%20analysis.%20With%20its%20realistic%20and%20diverse%0Anature%2C%20BoxMAC%20can%20serve%20as%20a%20valuable%20resource%20for%20the%20advancement%20of%20boxing%0Aas%20a%20sport%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18204v2&entry.124074799=Read"},
{"title": "Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models", "author": "Hao Dong and Moru Liu and Kaiyang Zhou and Eleni Chatzi and Juho Kannala and Cyrill Stachniss and Olga Fink", "abstract": "  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n", "link": "http://arxiv.org/abs/2501.18592v3", "date": "2025-02-17", "relevancy": 2.3892, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6271}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models&body=Title%3A%20Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models%0AAuthor%3A%20Hao%20Dong%20and%20Moru%20Liu%20and%20Kaiyang%20Zhou%20and%20Eleni%20Chatzi%20and%20Juho%20Kannala%20and%20Cyrill%20Stachniss%20and%20Olga%20Fink%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20achieving%20domain%20adaptation%20and%20generalization%20poses%0Asignificant%20challenges%2C%20as%20models%20must%20adapt%20to%20or%20generalize%20across%20unknown%0Atarget%20distributions.%20Extending%20these%20capabilities%20to%20unseen%20multimodal%0Adistributions%2C%20i.e.%2C%20multimodal%20domain%20adaptation%20and%20generalization%2C%20is%20even%0Amore%20challenging%20due%20to%20the%20distinct%20characteristics%20of%20different%20modalities.%0ASignificant%20progress%20has%20been%20made%20over%20the%20years%2C%20with%20applications%20ranging%0Afrom%20action%20recognition%20to%20semantic%20segmentation.%20Besides%2C%20the%20recent%20advent%20of%0Alarge-scale%20pre-trained%20multimodal%20foundation%20models%2C%20such%20as%20CLIP%2C%20has%0Ainspired%20works%20leveraging%20these%20models%20to%20enhance%20adaptation%20and%20generalization%0Aperformances%20or%20adapting%20them%20to%20downstream%20tasks.%20This%20survey%20provides%20the%0Afirst%20comprehensive%20review%20of%20recent%20advances%20from%20traditional%20approaches%20to%0Afoundation%20models%2C%20covering%3A%20%281%29%20Multimodal%20domain%20adaptation%3B%20%282%29%20Multimodal%0Atest-time%20adaptation%3B%20%283%29%20Multimodal%20domain%20generalization%3B%20%284%29%20Domain%0Aadaptation%20and%20generalization%20with%20the%20help%20of%20multimodal%20foundation%20models%3B%0Aand%20%285%29%20Adaptation%20of%20multimodal%20foundation%20models.%20For%20each%20topic%2C%20we%20formally%0Adefine%20the%20problem%20and%20thoroughly%20review%20existing%20methods.%20Additionally%2C%20we%0Aanalyze%20relevant%20datasets%20and%20applications%2C%20highlighting%20open%20challenges%20and%0Apotential%20future%20research%20directions.%20We%20maintain%20an%20active%20repository%20that%0Acontains%20up-to-date%20literature%20at%0Ahttps%3A//github.com/donghao51/Awesome-Multimodal-Adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18592v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Multimodal%2520Adaptation%2520and%2520Generalization%253A%2520From%2520Traditional%250A%2520%2520Approaches%2520to%2520Foundation%2520Models%26entry.906535625%3DHao%2520Dong%2520and%2520Moru%2520Liu%2520and%2520Kaiyang%2520Zhou%2520and%2520Eleni%2520Chatzi%2520and%2520Juho%2520Kannala%2520and%2520Cyrill%2520Stachniss%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520achieving%2520domain%2520adaptation%2520and%2520generalization%2520poses%250Asignificant%2520challenges%252C%2520as%2520models%2520must%2520adapt%2520to%2520or%2520generalize%2520across%2520unknown%250Atarget%2520distributions.%2520Extending%2520these%2520capabilities%2520to%2520unseen%2520multimodal%250Adistributions%252C%2520i.e.%252C%2520multimodal%2520domain%2520adaptation%2520and%2520generalization%252C%2520is%2520even%250Amore%2520challenging%2520due%2520to%2520the%2520distinct%2520characteristics%2520of%2520different%2520modalities.%250ASignificant%2520progress%2520has%2520been%2520made%2520over%2520the%2520years%252C%2520with%2520applications%2520ranging%250Afrom%2520action%2520recognition%2520to%2520semantic%2520segmentation.%2520Besides%252C%2520the%2520recent%2520advent%2520of%250Alarge-scale%2520pre-trained%2520multimodal%2520foundation%2520models%252C%2520such%2520as%2520CLIP%252C%2520has%250Ainspired%2520works%2520leveraging%2520these%2520models%2520to%2520enhance%2520adaptation%2520and%2520generalization%250Aperformances%2520or%2520adapting%2520them%2520to%2520downstream%2520tasks.%2520This%2520survey%2520provides%2520the%250Afirst%2520comprehensive%2520review%2520of%2520recent%2520advances%2520from%2520traditional%2520approaches%2520to%250Afoundation%2520models%252C%2520covering%253A%2520%25281%2529%2520Multimodal%2520domain%2520adaptation%253B%2520%25282%2529%2520Multimodal%250Atest-time%2520adaptation%253B%2520%25283%2529%2520Multimodal%2520domain%2520generalization%253B%2520%25284%2529%2520Domain%250Aadaptation%2520and%2520generalization%2520with%2520the%2520help%2520of%2520multimodal%2520foundation%2520models%253B%250Aand%2520%25285%2529%2520Adaptation%2520of%2520multimodal%2520foundation%2520models.%2520For%2520each%2520topic%252C%2520we%2520formally%250Adefine%2520the%2520problem%2520and%2520thoroughly%2520review%2520existing%2520methods.%2520Additionally%252C%2520we%250Aanalyze%2520relevant%2520datasets%2520and%2520applications%252C%2520highlighting%2520open%2520challenges%2520and%250Apotential%2520future%2520research%2520directions.%2520We%2520maintain%2520an%2520active%2520repository%2520that%250Acontains%2520up-to-date%2520literature%2520at%250Ahttps%253A//github.com/donghao51/Awesome-Multimodal-Adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18592v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Multimodal%20Adaptation%20and%20Generalization%3A%20From%20Traditional%0A%20%20Approaches%20to%20Foundation%20Models&entry.906535625=Hao%20Dong%20and%20Moru%20Liu%20and%20Kaiyang%20Zhou%20and%20Eleni%20Chatzi%20and%20Juho%20Kannala%20and%20Cyrill%20Stachniss%20and%20Olga%20Fink&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20achieving%20domain%20adaptation%20and%20generalization%20poses%0Asignificant%20challenges%2C%20as%20models%20must%20adapt%20to%20or%20generalize%20across%20unknown%0Atarget%20distributions.%20Extending%20these%20capabilities%20to%20unseen%20multimodal%0Adistributions%2C%20i.e.%2C%20multimodal%20domain%20adaptation%20and%20generalization%2C%20is%20even%0Amore%20challenging%20due%20to%20the%20distinct%20characteristics%20of%20different%20modalities.%0ASignificant%20progress%20has%20been%20made%20over%20the%20years%2C%20with%20applications%20ranging%0Afrom%20action%20recognition%20to%20semantic%20segmentation.%20Besides%2C%20the%20recent%20advent%20of%0Alarge-scale%20pre-trained%20multimodal%20foundation%20models%2C%20such%20as%20CLIP%2C%20has%0Ainspired%20works%20leveraging%20these%20models%20to%20enhance%20adaptation%20and%20generalization%0Aperformances%20or%20adapting%20them%20to%20downstream%20tasks.%20This%20survey%20provides%20the%0Afirst%20comprehensive%20review%20of%20recent%20advances%20from%20traditional%20approaches%20to%0Afoundation%20models%2C%20covering%3A%20%281%29%20Multimodal%20domain%20adaptation%3B%20%282%29%20Multimodal%0Atest-time%20adaptation%3B%20%283%29%20Multimodal%20domain%20generalization%3B%20%284%29%20Domain%0Aadaptation%20and%20generalization%20with%20the%20help%20of%20multimodal%20foundation%20models%3B%0Aand%20%285%29%20Adaptation%20of%20multimodal%20foundation%20models.%20For%20each%20topic%2C%20we%20formally%0Adefine%20the%20problem%20and%20thoroughly%20review%20existing%20methods.%20Additionally%2C%20we%0Aanalyze%20relevant%20datasets%20and%20applications%2C%20highlighting%20open%20challenges%20and%0Apotential%20future%20research%20directions.%20We%20maintain%20an%20active%20repository%20that%0Acontains%20up-to-date%20literature%20at%0Ahttps%3A//github.com/donghao51/Awesome-Multimodal-Adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18592v3&entry.124074799=Read"},
{"title": "Manifold Learning with Sparse Regularised Optimal Transport", "author": "Stephen Zhang and Gilles Mordant and Tetsuya Matsumoto and Geoffrey Schiebinger", "abstract": "  Manifold learning is a central task in modern statistics and data science.\nMany datasets (cells, documents, images, molecules) can be represented as point\nclouds embedded in a high dimensional ambient space, however the degrees of\nfreedom intrinsic to the data are usually far fewer than the number of ambient\ndimensions. The task of detecting a latent manifold along which the data are\nembedded is a prerequisite for a wide family of downstream analyses. Real-world\ndatasets are subject to noisy observations and sampling, so that distilling\ninformation about the underlying manifold is a major challenge. We propose a\nmethod for manifold learning that utilises a symmetric version of optimal\ntransport with a quadratic regularisation that constructs a sparse and adaptive\naffinity matrix, that can be interpreted as a generalisation of the\nbistochastic kernel normalisation.\n  We prove that the resulting kernel is consistent with a Laplace-type operator\nin the continuous limit, establish robustness to heteroskedastic noise and\nexhibit these results in numerical experiments. We identify a highly efficient\ncomputational scheme for computing this optimal transport for discrete data and\ndemonstrate that it outperforms competing methods in a set of examples.\n", "link": "http://arxiv.org/abs/2307.09816v2", "date": "2025-02-17", "relevancy": 2.389, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4842}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4783}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manifold%20Learning%20with%20Sparse%20Regularised%20Optimal%20Transport&body=Title%3A%20Manifold%20Learning%20with%20Sparse%20Regularised%20Optimal%20Transport%0AAuthor%3A%20Stephen%20Zhang%20and%20Gilles%20Mordant%20and%20Tetsuya%20Matsumoto%20and%20Geoffrey%20Schiebinger%0AAbstract%3A%20%20%20Manifold%20learning%20is%20a%20central%20task%20in%20modern%20statistics%20and%20data%20science.%0AMany%20datasets%20%28cells%2C%20documents%2C%20images%2C%20molecules%29%20can%20be%20represented%20as%20point%0Aclouds%20embedded%20in%20a%20high%20dimensional%20ambient%20space%2C%20however%20the%20degrees%20of%0Afreedom%20intrinsic%20to%20the%20data%20are%20usually%20far%20fewer%20than%20the%20number%20of%20ambient%0Adimensions.%20The%20task%20of%20detecting%20a%20latent%20manifold%20along%20which%20the%20data%20are%0Aembedded%20is%20a%20prerequisite%20for%20a%20wide%20family%20of%20downstream%20analyses.%20Real-world%0Adatasets%20are%20subject%20to%20noisy%20observations%20and%20sampling%2C%20so%20that%20distilling%0Ainformation%20about%20the%20underlying%20manifold%20is%20a%20major%20challenge.%20We%20propose%20a%0Amethod%20for%20manifold%20learning%20that%20utilises%20a%20symmetric%20version%20of%20optimal%0Atransport%20with%20a%20quadratic%20regularisation%20that%20constructs%20a%20sparse%20and%20adaptive%0Aaffinity%20matrix%2C%20that%20can%20be%20interpreted%20as%20a%20generalisation%20of%20the%0Abistochastic%20kernel%20normalisation.%0A%20%20We%20prove%20that%20the%20resulting%20kernel%20is%20consistent%20with%20a%20Laplace-type%20operator%0Ain%20the%20continuous%20limit%2C%20establish%20robustness%20to%20heteroskedastic%20noise%20and%0Aexhibit%20these%20results%20in%20numerical%20experiments.%20We%20identify%20a%20highly%20efficient%0Acomputational%20scheme%20for%20computing%20this%20optimal%20transport%20for%20discrete%20data%20and%0Ademonstrate%20that%20it%20outperforms%20competing%20methods%20in%20a%20set%20of%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManifold%2520Learning%2520with%2520Sparse%2520Regularised%2520Optimal%2520Transport%26entry.906535625%3DStephen%2520Zhang%2520and%2520Gilles%2520Mordant%2520and%2520Tetsuya%2520Matsumoto%2520and%2520Geoffrey%2520Schiebinger%26entry.1292438233%3D%2520%2520Manifold%2520learning%2520is%2520a%2520central%2520task%2520in%2520modern%2520statistics%2520and%2520data%2520science.%250AMany%2520datasets%2520%2528cells%252C%2520documents%252C%2520images%252C%2520molecules%2529%2520can%2520be%2520represented%2520as%2520point%250Aclouds%2520embedded%2520in%2520a%2520high%2520dimensional%2520ambient%2520space%252C%2520however%2520the%2520degrees%2520of%250Afreedom%2520intrinsic%2520to%2520the%2520data%2520are%2520usually%2520far%2520fewer%2520than%2520the%2520number%2520of%2520ambient%250Adimensions.%2520The%2520task%2520of%2520detecting%2520a%2520latent%2520manifold%2520along%2520which%2520the%2520data%2520are%250Aembedded%2520is%2520a%2520prerequisite%2520for%2520a%2520wide%2520family%2520of%2520downstream%2520analyses.%2520Real-world%250Adatasets%2520are%2520subject%2520to%2520noisy%2520observations%2520and%2520sampling%252C%2520so%2520that%2520distilling%250Ainformation%2520about%2520the%2520underlying%2520manifold%2520is%2520a%2520major%2520challenge.%2520We%2520propose%2520a%250Amethod%2520for%2520manifold%2520learning%2520that%2520utilises%2520a%2520symmetric%2520version%2520of%2520optimal%250Atransport%2520with%2520a%2520quadratic%2520regularisation%2520that%2520constructs%2520a%2520sparse%2520and%2520adaptive%250Aaffinity%2520matrix%252C%2520that%2520can%2520be%2520interpreted%2520as%2520a%2520generalisation%2520of%2520the%250Abistochastic%2520kernel%2520normalisation.%250A%2520%2520We%2520prove%2520that%2520the%2520resulting%2520kernel%2520is%2520consistent%2520with%2520a%2520Laplace-type%2520operator%250Ain%2520the%2520continuous%2520limit%252C%2520establish%2520robustness%2520to%2520heteroskedastic%2520noise%2520and%250Aexhibit%2520these%2520results%2520in%2520numerical%2520experiments.%2520We%2520identify%2520a%2520highly%2520efficient%250Acomputational%2520scheme%2520for%2520computing%2520this%2520optimal%2520transport%2520for%2520discrete%2520data%2520and%250Ademonstrate%2520that%2520it%2520outperforms%2520competing%2520methods%2520in%2520a%2520set%2520of%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold%20Learning%20with%20Sparse%20Regularised%20Optimal%20Transport&entry.906535625=Stephen%20Zhang%20and%20Gilles%20Mordant%20and%20Tetsuya%20Matsumoto%20and%20Geoffrey%20Schiebinger&entry.1292438233=%20%20Manifold%20learning%20is%20a%20central%20task%20in%20modern%20statistics%20and%20data%20science.%0AMany%20datasets%20%28cells%2C%20documents%2C%20images%2C%20molecules%29%20can%20be%20represented%20as%20point%0Aclouds%20embedded%20in%20a%20high%20dimensional%20ambient%20space%2C%20however%20the%20degrees%20of%0Afreedom%20intrinsic%20to%20the%20data%20are%20usually%20far%20fewer%20than%20the%20number%20of%20ambient%0Adimensions.%20The%20task%20of%20detecting%20a%20latent%20manifold%20along%20which%20the%20data%20are%0Aembedded%20is%20a%20prerequisite%20for%20a%20wide%20family%20of%20downstream%20analyses.%20Real-world%0Adatasets%20are%20subject%20to%20noisy%20observations%20and%20sampling%2C%20so%20that%20distilling%0Ainformation%20about%20the%20underlying%20manifold%20is%20a%20major%20challenge.%20We%20propose%20a%0Amethod%20for%20manifold%20learning%20that%20utilises%20a%20symmetric%20version%20of%20optimal%0Atransport%20with%20a%20quadratic%20regularisation%20that%20constructs%20a%20sparse%20and%20adaptive%0Aaffinity%20matrix%2C%20that%20can%20be%20interpreted%20as%20a%20generalisation%20of%20the%0Abistochastic%20kernel%20normalisation.%0A%20%20We%20prove%20that%20the%20resulting%20kernel%20is%20consistent%20with%20a%20Laplace-type%20operator%0Ain%20the%20continuous%20limit%2C%20establish%20robustness%20to%20heteroskedastic%20noise%20and%0Aexhibit%20these%20results%20in%20numerical%20experiments.%20We%20identify%20a%20highly%20efficient%0Acomputational%20scheme%20for%20computing%20this%20optimal%20transport%20for%20discrete%20data%20and%0Ademonstrate%20that%20it%20outperforms%20competing%20methods%20in%20a%20set%20of%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09816v2&entry.124074799=Read"},
{"title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising\n  Trajectory Sharpening", "author": "Ye Tian and Ling Yang and Xinchen Zhang and Yunhai Tong and Mengdi Wang and Bin Cui", "abstract": "  We propose Diffusion-Sharpening, a fine-tuning approach that enhances\ndownstream alignment by optimizing sampling trajectories. Existing RL-based\nfine-tuning methods focus on single training timesteps and neglect\ntrajectory-level alignment, while recent sampling trajectory optimization\nmethods incur significant inference NFE costs. Diffusion-Sharpening overcomes\nthis by using a path integral framework to select optimal trajectories during\ntraining, leveraging reward feedback, and amortizing inference costs. Our\nmethod demonstrates superior training efficiency with faster convergence, and\nbest inference efficiency without requiring additional NFEs. Extensive\nexperiments show that Diffusion-Sharpening outperforms RL-based fine-tuning\nmethods (e.g., Diffusion-DPO) and sampling trajectory optimization methods\n(e.g., Inference Scaling) across diverse metrics including text alignment,\ncompositional capabilities, and human preferences, offering a scalable and\nefficient solution for future diffusion model fine-tuning. Code:\nhttps://github.com/Gen-Verse/Diffusion-Sharpening\n", "link": "http://arxiv.org/abs/2502.12146v1", "date": "2025-02-17", "relevancy": 2.3565, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5941}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5896}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Sharpening%3A%20Fine-tuning%20Diffusion%20Models%20with%20Denoising%0A%20%20Trajectory%20Sharpening&body=Title%3A%20Diffusion-Sharpening%3A%20Fine-tuning%20Diffusion%20Models%20with%20Denoising%0A%20%20Trajectory%20Sharpening%0AAuthor%3A%20Ye%20Tian%20and%20Ling%20Yang%20and%20Xinchen%20Zhang%20and%20Yunhai%20Tong%20and%20Mengdi%20Wang%20and%20Bin%20Cui%0AAbstract%3A%20%20%20We%20propose%20Diffusion-Sharpening%2C%20a%20fine-tuning%20approach%20that%20enhances%0Adownstream%20alignment%20by%20optimizing%20sampling%20trajectories.%20Existing%20RL-based%0Afine-tuning%20methods%20focus%20on%20single%20training%20timesteps%20and%20neglect%0Atrajectory-level%20alignment%2C%20while%20recent%20sampling%20trajectory%20optimization%0Amethods%20incur%20significant%20inference%20NFE%20costs.%20Diffusion-Sharpening%20overcomes%0Athis%20by%20using%20a%20path%20integral%20framework%20to%20select%20optimal%20trajectories%20during%0Atraining%2C%20leveraging%20reward%20feedback%2C%20and%20amortizing%20inference%20costs.%20Our%0Amethod%20demonstrates%20superior%20training%20efficiency%20with%20faster%20convergence%2C%20and%0Abest%20inference%20efficiency%20without%20requiring%20additional%20NFEs.%20Extensive%0Aexperiments%20show%20that%20Diffusion-Sharpening%20outperforms%20RL-based%20fine-tuning%0Amethods%20%28e.g.%2C%20Diffusion-DPO%29%20and%20sampling%20trajectory%20optimization%20methods%0A%28e.g.%2C%20Inference%20Scaling%29%20across%20diverse%20metrics%20including%20text%20alignment%2C%0Acompositional%20capabilities%2C%20and%20human%20preferences%2C%20offering%20a%20scalable%20and%0Aefficient%20solution%20for%20future%20diffusion%20model%20fine-tuning.%20Code%3A%0Ahttps%3A//github.com/Gen-Verse/Diffusion-Sharpening%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Sharpening%253A%2520Fine-tuning%2520Diffusion%2520Models%2520with%2520Denoising%250A%2520%2520Trajectory%2520Sharpening%26entry.906535625%3DYe%2520Tian%2520and%2520Ling%2520Yang%2520and%2520Xinchen%2520Zhang%2520and%2520Yunhai%2520Tong%2520and%2520Mengdi%2520Wang%2520and%2520Bin%2520Cui%26entry.1292438233%3D%2520%2520We%2520propose%2520Diffusion-Sharpening%252C%2520a%2520fine-tuning%2520approach%2520that%2520enhances%250Adownstream%2520alignment%2520by%2520optimizing%2520sampling%2520trajectories.%2520Existing%2520RL-based%250Afine-tuning%2520methods%2520focus%2520on%2520single%2520training%2520timesteps%2520and%2520neglect%250Atrajectory-level%2520alignment%252C%2520while%2520recent%2520sampling%2520trajectory%2520optimization%250Amethods%2520incur%2520significant%2520inference%2520NFE%2520costs.%2520Diffusion-Sharpening%2520overcomes%250Athis%2520by%2520using%2520a%2520path%2520integral%2520framework%2520to%2520select%2520optimal%2520trajectories%2520during%250Atraining%252C%2520leveraging%2520reward%2520feedback%252C%2520and%2520amortizing%2520inference%2520costs.%2520Our%250Amethod%2520demonstrates%2520superior%2520training%2520efficiency%2520with%2520faster%2520convergence%252C%2520and%250Abest%2520inference%2520efficiency%2520without%2520requiring%2520additional%2520NFEs.%2520Extensive%250Aexperiments%2520show%2520that%2520Diffusion-Sharpening%2520outperforms%2520RL-based%2520fine-tuning%250Amethods%2520%2528e.g.%252C%2520Diffusion-DPO%2529%2520and%2520sampling%2520trajectory%2520optimization%2520methods%250A%2528e.g.%252C%2520Inference%2520Scaling%2529%2520across%2520diverse%2520metrics%2520including%2520text%2520alignment%252C%250Acompositional%2520capabilities%252C%2520and%2520human%2520preferences%252C%2520offering%2520a%2520scalable%2520and%250Aefficient%2520solution%2520for%2520future%2520diffusion%2520model%2520fine-tuning.%2520Code%253A%250Ahttps%253A//github.com/Gen-Verse/Diffusion-Sharpening%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Sharpening%3A%20Fine-tuning%20Diffusion%20Models%20with%20Denoising%0A%20%20Trajectory%20Sharpening&entry.906535625=Ye%20Tian%20and%20Ling%20Yang%20and%20Xinchen%20Zhang%20and%20Yunhai%20Tong%20and%20Mengdi%20Wang%20and%20Bin%20Cui&entry.1292438233=%20%20We%20propose%20Diffusion-Sharpening%2C%20a%20fine-tuning%20approach%20that%20enhances%0Adownstream%20alignment%20by%20optimizing%20sampling%20trajectories.%20Existing%20RL-based%0Afine-tuning%20methods%20focus%20on%20single%20training%20timesteps%20and%20neglect%0Atrajectory-level%20alignment%2C%20while%20recent%20sampling%20trajectory%20optimization%0Amethods%20incur%20significant%20inference%20NFE%20costs.%20Diffusion-Sharpening%20overcomes%0Athis%20by%20using%20a%20path%20integral%20framework%20to%20select%20optimal%20trajectories%20during%0Atraining%2C%20leveraging%20reward%20feedback%2C%20and%20amortizing%20inference%20costs.%20Our%0Amethod%20demonstrates%20superior%20training%20efficiency%20with%20faster%20convergence%2C%20and%0Abest%20inference%20efficiency%20without%20requiring%20additional%20NFEs.%20Extensive%0Aexperiments%20show%20that%20Diffusion-Sharpening%20outperforms%20RL-based%20fine-tuning%0Amethods%20%28e.g.%2C%20Diffusion-DPO%29%20and%20sampling%20trajectory%20optimization%20methods%0A%28e.g.%2C%20Inference%20Scaling%29%20across%20diverse%20metrics%20including%20text%20alignment%2C%0Acompositional%20capabilities%2C%20and%20human%20preferences%2C%20offering%20a%20scalable%20and%0Aefficient%20solution%20for%20future%20diffusion%20model%20fine-tuning.%20Code%3A%0Ahttps%3A//github.com/Gen-Verse/Diffusion-Sharpening%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12146v1&entry.124074799=Read"},
{"title": "SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion\n  Models with Self-Augmented Training", "author": "Lu Zhang and Liang Zeng", "abstract": "  The rapid proliferation of AI-generated images necessitates effective\nwatermarking techniques to protect intellectual property and detect fraudulent\ncontent. While existing training-based watermarking methods show promise, they\noften struggle with generalizing across diverse prompts and tend to introduce\nvisible artifacts. To this end, we propose a novel, provably generalizable\nimage watermarking approach for Latent Diffusion Models, termed Self-Augmented\nTraining (SAT-LDM). Our method aligns the training and testing phases through a\nfree generation distribution, thereby enhancing the watermarking module's\ngeneralization capabilities. We theoretically consolidate SAT-LDM by proving\nthat the free generation distribution contributes to its tight generalization\nbound, without the need for additional data collection. Extensive experiments\nshow that SAT-LDM not only achieves robust watermarking but also significantly\nimproves the quality of watermarked images across a wide range of prompts.\nMoreover, our experimental analyses confirm the strong generalization abilities\nof SAT-LDM. We hope that our method provides a practical and efficient solution\nfor securing high-fidelity AI-generated content.\n", "link": "http://arxiv.org/abs/2501.00463v2", "date": "2025-02-17", "relevancy": 2.3539, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6212}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5854}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAT-LDM%3A%20Provably%20Generalizable%20Image%20Watermarking%20for%20Latent%20Diffusion%0A%20%20Models%20with%20Self-Augmented%20Training&body=Title%3A%20SAT-LDM%3A%20Provably%20Generalizable%20Image%20Watermarking%20for%20Latent%20Diffusion%0A%20%20Models%20with%20Self-Augmented%20Training%0AAuthor%3A%20Lu%20Zhang%20and%20Liang%20Zeng%0AAbstract%3A%20%20%20The%20rapid%20proliferation%20of%20AI-generated%20images%20necessitates%20effective%0Awatermarking%20techniques%20to%20protect%20intellectual%20property%20and%20detect%20fraudulent%0Acontent.%20While%20existing%20training-based%20watermarking%20methods%20show%20promise%2C%20they%0Aoften%20struggle%20with%20generalizing%20across%20diverse%20prompts%20and%20tend%20to%20introduce%0Avisible%20artifacts.%20To%20this%20end%2C%20we%20propose%20a%20novel%2C%20provably%20generalizable%0Aimage%20watermarking%20approach%20for%20Latent%20Diffusion%20Models%2C%20termed%20Self-Augmented%0ATraining%20%28SAT-LDM%29.%20Our%20method%20aligns%20the%20training%20and%20testing%20phases%20through%20a%0Afree%20generation%20distribution%2C%20thereby%20enhancing%20the%20watermarking%20module%27s%0Ageneralization%20capabilities.%20We%20theoretically%20consolidate%20SAT-LDM%20by%20proving%0Athat%20the%20free%20generation%20distribution%20contributes%20to%20its%20tight%20generalization%0Abound%2C%20without%20the%20need%20for%20additional%20data%20collection.%20Extensive%20experiments%0Ashow%20that%20SAT-LDM%20not%20only%20achieves%20robust%20watermarking%20but%20also%20significantly%0Aimproves%20the%20quality%20of%20watermarked%20images%20across%20a%20wide%20range%20of%20prompts.%0AMoreover%2C%20our%20experimental%20analyses%20confirm%20the%20strong%20generalization%20abilities%0Aof%20SAT-LDM.%20We%20hope%20that%20our%20method%20provides%20a%20practical%20and%20efficient%20solution%0Afor%20securing%20high-fidelity%20AI-generated%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAT-LDM%253A%2520Provably%2520Generalizable%2520Image%2520Watermarking%2520for%2520Latent%2520Diffusion%250A%2520%2520Models%2520with%2520Self-Augmented%2520Training%26entry.906535625%3DLu%2520Zhang%2520and%2520Liang%2520Zeng%26entry.1292438233%3D%2520%2520The%2520rapid%2520proliferation%2520of%2520AI-generated%2520images%2520necessitates%2520effective%250Awatermarking%2520techniques%2520to%2520protect%2520intellectual%2520property%2520and%2520detect%2520fraudulent%250Acontent.%2520While%2520existing%2520training-based%2520watermarking%2520methods%2520show%2520promise%252C%2520they%250Aoften%2520struggle%2520with%2520generalizing%2520across%2520diverse%2520prompts%2520and%2520tend%2520to%2520introduce%250Avisible%2520artifacts.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%252C%2520provably%2520generalizable%250Aimage%2520watermarking%2520approach%2520for%2520Latent%2520Diffusion%2520Models%252C%2520termed%2520Self-Augmented%250ATraining%2520%2528SAT-LDM%2529.%2520Our%2520method%2520aligns%2520the%2520training%2520and%2520testing%2520phases%2520through%2520a%250Afree%2520generation%2520distribution%252C%2520thereby%2520enhancing%2520the%2520watermarking%2520module%2527s%250Ageneralization%2520capabilities.%2520We%2520theoretically%2520consolidate%2520SAT-LDM%2520by%2520proving%250Athat%2520the%2520free%2520generation%2520distribution%2520contributes%2520to%2520its%2520tight%2520generalization%250Abound%252C%2520without%2520the%2520need%2520for%2520additional%2520data%2520collection.%2520Extensive%2520experiments%250Ashow%2520that%2520SAT-LDM%2520not%2520only%2520achieves%2520robust%2520watermarking%2520but%2520also%2520significantly%250Aimproves%2520the%2520quality%2520of%2520watermarked%2520images%2520across%2520a%2520wide%2520range%2520of%2520prompts.%250AMoreover%252C%2520our%2520experimental%2520analyses%2520confirm%2520the%2520strong%2520generalization%2520abilities%250Aof%2520SAT-LDM.%2520We%2520hope%2520that%2520our%2520method%2520provides%2520a%2520practical%2520and%2520efficient%2520solution%250Afor%2520securing%2520high-fidelity%2520AI-generated%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAT-LDM%3A%20Provably%20Generalizable%20Image%20Watermarking%20for%20Latent%20Diffusion%0A%20%20Models%20with%20Self-Augmented%20Training&entry.906535625=Lu%20Zhang%20and%20Liang%20Zeng&entry.1292438233=%20%20The%20rapid%20proliferation%20of%20AI-generated%20images%20necessitates%20effective%0Awatermarking%20techniques%20to%20protect%20intellectual%20property%20and%20detect%20fraudulent%0Acontent.%20While%20existing%20training-based%20watermarking%20methods%20show%20promise%2C%20they%0Aoften%20struggle%20with%20generalizing%20across%20diverse%20prompts%20and%20tend%20to%20introduce%0Avisible%20artifacts.%20To%20this%20end%2C%20we%20propose%20a%20novel%2C%20provably%20generalizable%0Aimage%20watermarking%20approach%20for%20Latent%20Diffusion%20Models%2C%20termed%20Self-Augmented%0ATraining%20%28SAT-LDM%29.%20Our%20method%20aligns%20the%20training%20and%20testing%20phases%20through%20a%0Afree%20generation%20distribution%2C%20thereby%20enhancing%20the%20watermarking%20module%27s%0Ageneralization%20capabilities.%20We%20theoretically%20consolidate%20SAT-LDM%20by%20proving%0Athat%20the%20free%20generation%20distribution%20contributes%20to%20its%20tight%20generalization%0Abound%2C%20without%20the%20need%20for%20additional%20data%20collection.%20Extensive%20experiments%0Ashow%20that%20SAT-LDM%20not%20only%20achieves%20robust%20watermarking%20but%20also%20significantly%0Aimproves%20the%20quality%20of%20watermarked%20images%20across%20a%20wide%20range%20of%20prompts.%0AMoreover%2C%20our%20experimental%20analyses%20confirm%20the%20strong%20generalization%20abilities%0Aof%20SAT-LDM.%20We%20hope%20that%20our%20method%20provides%20a%20practical%20and%20efficient%20solution%0Afor%20securing%20high-fidelity%20AI-generated%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00463v2&entry.124074799=Read"},
{"title": "MaskGWM: A Generalizable Driving World Model with Video Mask\n  Reconstruction", "author": "Jingcheng Ni and Yuxin Guo and Yichen Liu and Rui Chen and Lewei Lu and Zehuan Wu", "abstract": "  World models that forecast environmental changes from actions are vital for\nautonomous driving models with strong generalization. The prevailing driving\nworld model mainly build on video prediction model. Although these models can\nproduce high-fidelity video sequences with advanced diffusion-based generator,\nthey are constrained by their predictive duration and overall generalization\ncapabilities. In this paper, we explore to solve this problem by combining\ngeneration loss with MAE-style feature-level context learning. In particular,\nwe instantiate this target with three key design: (1) A more scalable Diffusion\nTransformer (DiT) structure trained with extra mask construction task. (2) we\ndevise diffusion-related mask tokens to deal with the fuzzy relations between\nmask reconstruction and generative diffusion process. (3) we extend mask\nconstruction task to spatial-temporal domain by utilizing row-wise mask for\nshifted self-attention rather than masked self-attention in MAE. Then, we adopt\na row-wise cross-view module to align with this mask design. Based on above\nimprovement, we propose MaskGWM: a Generalizable driving World Model embodied\nwith Video Mask reconstruction. Our model contains two variants: MaskGWM-long,\nfocusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view\ngeneration. Comprehensive experiments on standard benchmarks validate the\neffectiveness of the proposed method, which contain normal validation of\nNuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot\nvalidation of Waymo dataset. Quantitative metrics on these datasets show our\nmethod notably improving state-of-the-art driving world model.\n", "link": "http://arxiv.org/abs/2502.11663v1", "date": "2025-02-17", "relevancy": 2.349, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5936}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5867}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaskGWM%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20Video%20Mask%0A%20%20Reconstruction&body=Title%3A%20MaskGWM%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20Video%20Mask%0A%20%20Reconstruction%0AAuthor%3A%20Jingcheng%20Ni%20and%20Yuxin%20Guo%20and%20Yichen%20Liu%20and%20Rui%20Chen%20and%20Lewei%20Lu%20and%20Zehuan%20Wu%0AAbstract%3A%20%20%20World%20models%20that%20forecast%20environmental%20changes%20from%20actions%20are%20vital%20for%0Aautonomous%20driving%20models%20with%20strong%20generalization.%20The%20prevailing%20driving%0Aworld%20model%20mainly%20build%20on%20video%20prediction%20model.%20Although%20these%20models%20can%0Aproduce%20high-fidelity%20video%20sequences%20with%20advanced%20diffusion-based%20generator%2C%0Athey%20are%20constrained%20by%20their%20predictive%20duration%20and%20overall%20generalization%0Acapabilities.%20In%20this%20paper%2C%20we%20explore%20to%20solve%20this%20problem%20by%20combining%0Ageneration%20loss%20with%20MAE-style%20feature-level%20context%20learning.%20In%20particular%2C%0Awe%20instantiate%20this%20target%20with%20three%20key%20design%3A%20%281%29%20A%20more%20scalable%20Diffusion%0ATransformer%20%28DiT%29%20structure%20trained%20with%20extra%20mask%20construction%20task.%20%282%29%20we%0Adevise%20diffusion-related%20mask%20tokens%20to%20deal%20with%20the%20fuzzy%20relations%20between%0Amask%20reconstruction%20and%20generative%20diffusion%20process.%20%283%29%20we%20extend%20mask%0Aconstruction%20task%20to%20spatial-temporal%20domain%20by%20utilizing%20row-wise%20mask%20for%0Ashifted%20self-attention%20rather%20than%20masked%20self-attention%20in%20MAE.%20Then%2C%20we%20adopt%0Aa%20row-wise%20cross-view%20module%20to%20align%20with%20this%20mask%20design.%20Based%20on%20above%0Aimprovement%2C%20we%20propose%20MaskGWM%3A%20a%20Generalizable%20driving%20World%20Model%20embodied%0Awith%20Video%20Mask%20reconstruction.%20Our%20model%20contains%20two%20variants%3A%20MaskGWM-long%2C%0Afocusing%20on%20long-horizon%20prediction%2C%20and%20MaskGWM-mview%2C%20dedicated%20to%20multi-view%0Ageneration.%20Comprehensive%20experiments%20on%20standard%20benchmarks%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20method%2C%20which%20contain%20normal%20validation%20of%0ANuscene%20dataset%2C%20long-horizon%20rollout%20of%20OpenDV-2K%20dataset%20and%20zero-shot%0Avalidation%20of%20Waymo%20dataset.%20Quantitative%20metrics%20on%20these%20datasets%20show%20our%0Amethod%20notably%20improving%20state-of-the-art%20driving%20world%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaskGWM%253A%2520A%2520Generalizable%2520Driving%2520World%2520Model%2520with%2520Video%2520Mask%250A%2520%2520Reconstruction%26entry.906535625%3DJingcheng%2520Ni%2520and%2520Yuxin%2520Guo%2520and%2520Yichen%2520Liu%2520and%2520Rui%2520Chen%2520and%2520Lewei%2520Lu%2520and%2520Zehuan%2520Wu%26entry.1292438233%3D%2520%2520World%2520models%2520that%2520forecast%2520environmental%2520changes%2520from%2520actions%2520are%2520vital%2520for%250Aautonomous%2520driving%2520models%2520with%2520strong%2520generalization.%2520The%2520prevailing%2520driving%250Aworld%2520model%2520mainly%2520build%2520on%2520video%2520prediction%2520model.%2520Although%2520these%2520models%2520can%250Aproduce%2520high-fidelity%2520video%2520sequences%2520with%2520advanced%2520diffusion-based%2520generator%252C%250Athey%2520are%2520constrained%2520by%2520their%2520predictive%2520duration%2520and%2520overall%2520generalization%250Acapabilities.%2520In%2520this%2520paper%252C%2520we%2520explore%2520to%2520solve%2520this%2520problem%2520by%2520combining%250Ageneration%2520loss%2520with%2520MAE-style%2520feature-level%2520context%2520learning.%2520In%2520particular%252C%250Awe%2520instantiate%2520this%2520target%2520with%2520three%2520key%2520design%253A%2520%25281%2529%2520A%2520more%2520scalable%2520Diffusion%250ATransformer%2520%2528DiT%2529%2520structure%2520trained%2520with%2520extra%2520mask%2520construction%2520task.%2520%25282%2529%2520we%250Adevise%2520diffusion-related%2520mask%2520tokens%2520to%2520deal%2520with%2520the%2520fuzzy%2520relations%2520between%250Amask%2520reconstruction%2520and%2520generative%2520diffusion%2520process.%2520%25283%2529%2520we%2520extend%2520mask%250Aconstruction%2520task%2520to%2520spatial-temporal%2520domain%2520by%2520utilizing%2520row-wise%2520mask%2520for%250Ashifted%2520self-attention%2520rather%2520than%2520masked%2520self-attention%2520in%2520MAE.%2520Then%252C%2520we%2520adopt%250Aa%2520row-wise%2520cross-view%2520module%2520to%2520align%2520with%2520this%2520mask%2520design.%2520Based%2520on%2520above%250Aimprovement%252C%2520we%2520propose%2520MaskGWM%253A%2520a%2520Generalizable%2520driving%2520World%2520Model%2520embodied%250Awith%2520Video%2520Mask%2520reconstruction.%2520Our%2520model%2520contains%2520two%2520variants%253A%2520MaskGWM-long%252C%250Afocusing%2520on%2520long-horizon%2520prediction%252C%2520and%2520MaskGWM-mview%252C%2520dedicated%2520to%2520multi-view%250Ageneration.%2520Comprehensive%2520experiments%2520on%2520standard%2520benchmarks%2520validate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method%252C%2520which%2520contain%2520normal%2520validation%2520of%250ANuscene%2520dataset%252C%2520long-horizon%2520rollout%2520of%2520OpenDV-2K%2520dataset%2520and%2520zero-shot%250Avalidation%2520of%2520Waymo%2520dataset.%2520Quantitative%2520metrics%2520on%2520these%2520datasets%2520show%2520our%250Amethod%2520notably%2520improving%2520state-of-the-art%2520driving%2520world%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskGWM%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20Video%20Mask%0A%20%20Reconstruction&entry.906535625=Jingcheng%20Ni%20and%20Yuxin%20Guo%20and%20Yichen%20Liu%20and%20Rui%20Chen%20and%20Lewei%20Lu%20and%20Zehuan%20Wu&entry.1292438233=%20%20World%20models%20that%20forecast%20environmental%20changes%20from%20actions%20are%20vital%20for%0Aautonomous%20driving%20models%20with%20strong%20generalization.%20The%20prevailing%20driving%0Aworld%20model%20mainly%20build%20on%20video%20prediction%20model.%20Although%20these%20models%20can%0Aproduce%20high-fidelity%20video%20sequences%20with%20advanced%20diffusion-based%20generator%2C%0Athey%20are%20constrained%20by%20their%20predictive%20duration%20and%20overall%20generalization%0Acapabilities.%20In%20this%20paper%2C%20we%20explore%20to%20solve%20this%20problem%20by%20combining%0Ageneration%20loss%20with%20MAE-style%20feature-level%20context%20learning.%20In%20particular%2C%0Awe%20instantiate%20this%20target%20with%20three%20key%20design%3A%20%281%29%20A%20more%20scalable%20Diffusion%0ATransformer%20%28DiT%29%20structure%20trained%20with%20extra%20mask%20construction%20task.%20%282%29%20we%0Adevise%20diffusion-related%20mask%20tokens%20to%20deal%20with%20the%20fuzzy%20relations%20between%0Amask%20reconstruction%20and%20generative%20diffusion%20process.%20%283%29%20we%20extend%20mask%0Aconstruction%20task%20to%20spatial-temporal%20domain%20by%20utilizing%20row-wise%20mask%20for%0Ashifted%20self-attention%20rather%20than%20masked%20self-attention%20in%20MAE.%20Then%2C%20we%20adopt%0Aa%20row-wise%20cross-view%20module%20to%20align%20with%20this%20mask%20design.%20Based%20on%20above%0Aimprovement%2C%20we%20propose%20MaskGWM%3A%20a%20Generalizable%20driving%20World%20Model%20embodied%0Awith%20Video%20Mask%20reconstruction.%20Our%20model%20contains%20two%20variants%3A%20MaskGWM-long%2C%0Afocusing%20on%20long-horizon%20prediction%2C%20and%20MaskGWM-mview%2C%20dedicated%20to%20multi-view%0Ageneration.%20Comprehensive%20experiments%20on%20standard%20benchmarks%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20method%2C%20which%20contain%20normal%20validation%20of%0ANuscene%20dataset%2C%20long-horizon%20rollout%20of%20OpenDV-2K%20dataset%20and%20zero-shot%0Avalidation%20of%20Waymo%20dataset.%20Quantitative%20metrics%20on%20these%20datasets%20show%20our%0Amethod%20notably%20improving%20state-of-the-art%20driving%20world%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11663v1&entry.124074799=Read"},
{"title": "Omnidirectional Sensor Placement: A Large-Scale Computational Study and\n  Novel Hybrid Accelerated-Refinement Heuristics", "author": "Jan Mikula and Miroslav Kulich", "abstract": "  This paper studies the omnidirectional sensor-placement problem (OSPP), which\ninvolves placing static sensors in a continuous 2D environment to achieve a\nuser-defined coverage requirement while minimizing sensor count. The problem is\nmotivated by applications in mobile robotics, particularly for optimizing\nvisibility-based route planning tasks such as environment inspection, target\nsearch, and region patrolling. We focus on omnidirectional visibility models,\nwhich eliminate sensor orientation constraints while remaining relevant to\nreal-world sensing technologies like LiDAR, 360-degree cameras, and\nmulti-sensor arrays. Three key models are considered: unlimited visibility,\nlimited-range visibility to reflect physical or application-specific\nconstraints, and localization-uncertainty visibility to account for sensor\nplacement uncertainty in robotics. Our first contribution is a large-scale\ncomputational study comparing classical convex-partitioning and sampling-based\nheuristics for the OSPP, analyzing their trade-off between runtime efficiency\nand solution quality. Our second contribution is a new class of hybrid\naccelerated-refinement (HAR) heuristics, which combine and refine outputs from\nmultiple sensor-placement methods while incorporating preprocessing techniques\nto accelerate refinement. Results demonstrate that HAR heuristics significantly\noutperform traditional methods, achieving the lowest sensor counts and\nimproving the runtime of sampling-based approaches. Additionally, we adapt a\nspecific HAR heuristic to the localization-uncertainty visibility model,\nshowing that it achieves the required coverage for small to moderate\nlocalization uncertainty. Future work may apply HAR to visibility-based route\nplanning tasks or explore novel sensor-placement approaches to achieve formal\ncoverage guarantees under uncertainty.\n", "link": "http://arxiv.org/abs/2410.08784v2", "date": "2025-02-17", "relevancy": 2.3376, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6174}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5755}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omnidirectional%20Sensor%20Placement%3A%20A%20Large-Scale%20Computational%20Study%20and%0A%20%20Novel%20Hybrid%20Accelerated-Refinement%20Heuristics&body=Title%3A%20Omnidirectional%20Sensor%20Placement%3A%20A%20Large-Scale%20Computational%20Study%20and%0A%20%20Novel%20Hybrid%20Accelerated-Refinement%20Heuristics%0AAuthor%3A%20Jan%20Mikula%20and%20Miroslav%20Kulich%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20omnidirectional%20sensor-placement%20problem%20%28OSPP%29%2C%20which%0Ainvolves%20placing%20static%20sensors%20in%20a%20continuous%202D%20environment%20to%20achieve%20a%0Auser-defined%20coverage%20requirement%20while%20minimizing%20sensor%20count.%20The%20problem%20is%0Amotivated%20by%20applications%20in%20mobile%20robotics%2C%20particularly%20for%20optimizing%0Avisibility-based%20route%20planning%20tasks%20such%20as%20environment%20inspection%2C%20target%0Asearch%2C%20and%20region%20patrolling.%20We%20focus%20on%20omnidirectional%20visibility%20models%2C%0Awhich%20eliminate%20sensor%20orientation%20constraints%20while%20remaining%20relevant%20to%0Areal-world%20sensing%20technologies%20like%20LiDAR%2C%20360-degree%20cameras%2C%20and%0Amulti-sensor%20arrays.%20Three%20key%20models%20are%20considered%3A%20unlimited%20visibility%2C%0Alimited-range%20visibility%20to%20reflect%20physical%20or%20application-specific%0Aconstraints%2C%20and%20localization-uncertainty%20visibility%20to%20account%20for%20sensor%0Aplacement%20uncertainty%20in%20robotics.%20Our%20first%20contribution%20is%20a%20large-scale%0Acomputational%20study%20comparing%20classical%20convex-partitioning%20and%20sampling-based%0Aheuristics%20for%20the%20OSPP%2C%20analyzing%20their%20trade-off%20between%20runtime%20efficiency%0Aand%20solution%20quality.%20Our%20second%20contribution%20is%20a%20new%20class%20of%20hybrid%0Aaccelerated-refinement%20%28HAR%29%20heuristics%2C%20which%20combine%20and%20refine%20outputs%20from%0Amultiple%20sensor-placement%20methods%20while%20incorporating%20preprocessing%20techniques%0Ato%20accelerate%20refinement.%20Results%20demonstrate%20that%20HAR%20heuristics%20significantly%0Aoutperform%20traditional%20methods%2C%20achieving%20the%20lowest%20sensor%20counts%20and%0Aimproving%20the%20runtime%20of%20sampling-based%20approaches.%20Additionally%2C%20we%20adapt%20a%0Aspecific%20HAR%20heuristic%20to%20the%20localization-uncertainty%20visibility%20model%2C%0Ashowing%20that%20it%20achieves%20the%20required%20coverage%20for%20small%20to%20moderate%0Alocalization%20uncertainty.%20Future%20work%20may%20apply%20HAR%20to%20visibility-based%20route%0Aplanning%20tasks%20or%20explore%20novel%20sensor-placement%20approaches%20to%20achieve%20formal%0Acoverage%20guarantees%20under%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmnidirectional%2520Sensor%2520Placement%253A%2520A%2520Large-Scale%2520Computational%2520Study%2520and%250A%2520%2520Novel%2520Hybrid%2520Accelerated-Refinement%2520Heuristics%26entry.906535625%3DJan%2520Mikula%2520and%2520Miroslav%2520Kulich%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520omnidirectional%2520sensor-placement%2520problem%2520%2528OSPP%2529%252C%2520which%250Ainvolves%2520placing%2520static%2520sensors%2520in%2520a%2520continuous%25202D%2520environment%2520to%2520achieve%2520a%250Auser-defined%2520coverage%2520requirement%2520while%2520minimizing%2520sensor%2520count.%2520The%2520problem%2520is%250Amotivated%2520by%2520applications%2520in%2520mobile%2520robotics%252C%2520particularly%2520for%2520optimizing%250Avisibility-based%2520route%2520planning%2520tasks%2520such%2520as%2520environment%2520inspection%252C%2520target%250Asearch%252C%2520and%2520region%2520patrolling.%2520We%2520focus%2520on%2520omnidirectional%2520visibility%2520models%252C%250Awhich%2520eliminate%2520sensor%2520orientation%2520constraints%2520while%2520remaining%2520relevant%2520to%250Areal-world%2520sensing%2520technologies%2520like%2520LiDAR%252C%2520360-degree%2520cameras%252C%2520and%250Amulti-sensor%2520arrays.%2520Three%2520key%2520models%2520are%2520considered%253A%2520unlimited%2520visibility%252C%250Alimited-range%2520visibility%2520to%2520reflect%2520physical%2520or%2520application-specific%250Aconstraints%252C%2520and%2520localization-uncertainty%2520visibility%2520to%2520account%2520for%2520sensor%250Aplacement%2520uncertainty%2520in%2520robotics.%2520Our%2520first%2520contribution%2520is%2520a%2520large-scale%250Acomputational%2520study%2520comparing%2520classical%2520convex-partitioning%2520and%2520sampling-based%250Aheuristics%2520for%2520the%2520OSPP%252C%2520analyzing%2520their%2520trade-off%2520between%2520runtime%2520efficiency%250Aand%2520solution%2520quality.%2520Our%2520second%2520contribution%2520is%2520a%2520new%2520class%2520of%2520hybrid%250Aaccelerated-refinement%2520%2528HAR%2529%2520heuristics%252C%2520which%2520combine%2520and%2520refine%2520outputs%2520from%250Amultiple%2520sensor-placement%2520methods%2520while%2520incorporating%2520preprocessing%2520techniques%250Ato%2520accelerate%2520refinement.%2520Results%2520demonstrate%2520that%2520HAR%2520heuristics%2520significantly%250Aoutperform%2520traditional%2520methods%252C%2520achieving%2520the%2520lowest%2520sensor%2520counts%2520and%250Aimproving%2520the%2520runtime%2520of%2520sampling-based%2520approaches.%2520Additionally%252C%2520we%2520adapt%2520a%250Aspecific%2520HAR%2520heuristic%2520to%2520the%2520localization-uncertainty%2520visibility%2520model%252C%250Ashowing%2520that%2520it%2520achieves%2520the%2520required%2520coverage%2520for%2520small%2520to%2520moderate%250Alocalization%2520uncertainty.%2520Future%2520work%2520may%2520apply%2520HAR%2520to%2520visibility-based%2520route%250Aplanning%2520tasks%2520or%2520explore%2520novel%2520sensor-placement%2520approaches%2520to%2520achieve%2520formal%250Acoverage%2520guarantees%2520under%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omnidirectional%20Sensor%20Placement%3A%20A%20Large-Scale%20Computational%20Study%20and%0A%20%20Novel%20Hybrid%20Accelerated-Refinement%20Heuristics&entry.906535625=Jan%20Mikula%20and%20Miroslav%20Kulich&entry.1292438233=%20%20This%20paper%20studies%20the%20omnidirectional%20sensor-placement%20problem%20%28OSPP%29%2C%20which%0Ainvolves%20placing%20static%20sensors%20in%20a%20continuous%202D%20environment%20to%20achieve%20a%0Auser-defined%20coverage%20requirement%20while%20minimizing%20sensor%20count.%20The%20problem%20is%0Amotivated%20by%20applications%20in%20mobile%20robotics%2C%20particularly%20for%20optimizing%0Avisibility-based%20route%20planning%20tasks%20such%20as%20environment%20inspection%2C%20target%0Asearch%2C%20and%20region%20patrolling.%20We%20focus%20on%20omnidirectional%20visibility%20models%2C%0Awhich%20eliminate%20sensor%20orientation%20constraints%20while%20remaining%20relevant%20to%0Areal-world%20sensing%20technologies%20like%20LiDAR%2C%20360-degree%20cameras%2C%20and%0Amulti-sensor%20arrays.%20Three%20key%20models%20are%20considered%3A%20unlimited%20visibility%2C%0Alimited-range%20visibility%20to%20reflect%20physical%20or%20application-specific%0Aconstraints%2C%20and%20localization-uncertainty%20visibility%20to%20account%20for%20sensor%0Aplacement%20uncertainty%20in%20robotics.%20Our%20first%20contribution%20is%20a%20large-scale%0Acomputational%20study%20comparing%20classical%20convex-partitioning%20and%20sampling-based%0Aheuristics%20for%20the%20OSPP%2C%20analyzing%20their%20trade-off%20between%20runtime%20efficiency%0Aand%20solution%20quality.%20Our%20second%20contribution%20is%20a%20new%20class%20of%20hybrid%0Aaccelerated-refinement%20%28HAR%29%20heuristics%2C%20which%20combine%20and%20refine%20outputs%20from%0Amultiple%20sensor-placement%20methods%20while%20incorporating%20preprocessing%20techniques%0Ato%20accelerate%20refinement.%20Results%20demonstrate%20that%20HAR%20heuristics%20significantly%0Aoutperform%20traditional%20methods%2C%20achieving%20the%20lowest%20sensor%20counts%20and%0Aimproving%20the%20runtime%20of%20sampling-based%20approaches.%20Additionally%2C%20we%20adapt%20a%0Aspecific%20HAR%20heuristic%20to%20the%20localization-uncertainty%20visibility%20model%2C%0Ashowing%20that%20it%20achieves%20the%20required%20coverage%20for%20small%20to%20moderate%0Alocalization%20uncertainty.%20Future%20work%20may%20apply%20HAR%20to%20visibility-based%20route%0Aplanning%20tasks%20or%20explore%20novel%20sensor-placement%20approaches%20to%20achieve%20formal%0Acoverage%20guarantees%20under%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08784v2&entry.124074799=Read"},
{"title": "Descriminative-Generative Custom Tokens for Vision-Language Models", "author": "Pramuditha Perera and Matthew Trager and Luca Zancato and Alessandro Achille and Stefano Soatto", "abstract": "  This paper explores the possibility of learning custom tokens for\nrepresenting new concepts in Vision-Language Models (VLMs). Our aim is to learn\ntokens that can be effective for both discriminative and generative tasks while\ncomposing well with words to form new input queries. The targeted concept is\nspecified in terms of a small set of images and a parent concept described\nusing text. We operate on CLIP text features and propose to use a combination\nof a textual inversion loss and a classification loss to ensure that text\nfeatures of the learned token are aligned with image features of the concept in\nthe CLIP embedding space. We restrict the learned token to a low-dimensional\nsubspace spanned by tokens for attributes that are appropriate for the given\nsuper-class. These modifications improve the quality of compositions of the\nlearned token with natural language for generating new scenes. Further, we show\nthat learned custom tokens can be used to form queries for text-to-image\nretrieval task, and also have the important benefit that composite queries can\nbe visualized to ensure that the desired concept is faithfully encoded. Based\non this, we introduce the method of Generation Aided Image Retrieval, where the\nquery is modified at inference time to better suit the search intent. On the\nDeepFashion2 dataset, our method improves Mean Reciprocal Retrieval (MRR) over\nrelevant baselines by 7%.\n", "link": "http://arxiv.org/abs/2502.12095v1", "date": "2025-02-17", "relevancy": 2.3346, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6044}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5704}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Descriminative-Generative%20Custom%20Tokens%20for%20Vision-Language%20Models&body=Title%3A%20Descriminative-Generative%20Custom%20Tokens%20for%20Vision-Language%20Models%0AAuthor%3A%20Pramuditha%20Perera%20and%20Matthew%20Trager%20and%20Luca%20Zancato%20and%20Alessandro%20Achille%20and%20Stefano%20Soatto%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20possibility%20of%20learning%20custom%20tokens%20for%0Arepresenting%20new%20concepts%20in%20Vision-Language%20Models%20%28VLMs%29.%20Our%20aim%20is%20to%20learn%0Atokens%20that%20can%20be%20effective%20for%20both%20discriminative%20and%20generative%20tasks%20while%0Acomposing%20well%20with%20words%20to%20form%20new%20input%20queries.%20The%20targeted%20concept%20is%0Aspecified%20in%20terms%20of%20a%20small%20set%20of%20images%20and%20a%20parent%20concept%20described%0Ausing%20text.%20We%20operate%20on%20CLIP%20text%20features%20and%20propose%20to%20use%20a%20combination%0Aof%20a%20textual%20inversion%20loss%20and%20a%20classification%20loss%20to%20ensure%20that%20text%0Afeatures%20of%20the%20learned%20token%20are%20aligned%20with%20image%20features%20of%20the%20concept%20in%0Athe%20CLIP%20embedding%20space.%20We%20restrict%20the%20learned%20token%20to%20a%20low-dimensional%0Asubspace%20spanned%20by%20tokens%20for%20attributes%20that%20are%20appropriate%20for%20the%20given%0Asuper-class.%20These%20modifications%20improve%20the%20quality%20of%20compositions%20of%20the%0Alearned%20token%20with%20natural%20language%20for%20generating%20new%20scenes.%20Further%2C%20we%20show%0Athat%20learned%20custom%20tokens%20can%20be%20used%20to%20form%20queries%20for%20text-to-image%0Aretrieval%20task%2C%20and%20also%20have%20the%20important%20benefit%20that%20composite%20queries%20can%0Abe%20visualized%20to%20ensure%20that%20the%20desired%20concept%20is%20faithfully%20encoded.%20Based%0Aon%20this%2C%20we%20introduce%20the%20method%20of%20Generation%20Aided%20Image%20Retrieval%2C%20where%20the%0Aquery%20is%20modified%20at%20inference%20time%20to%20better%20suit%20the%20search%20intent.%20On%20the%0ADeepFashion2%20dataset%2C%20our%20method%20improves%20Mean%20Reciprocal%20Retrieval%20%28MRR%29%20over%0Arelevant%20baselines%20by%207%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDescriminative-Generative%2520Custom%2520Tokens%2520for%2520Vision-Language%2520Models%26entry.906535625%3DPramuditha%2520Perera%2520and%2520Matthew%2520Trager%2520and%2520Luca%2520Zancato%2520and%2520Alessandro%2520Achille%2520and%2520Stefano%2520Soatto%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520possibility%2520of%2520learning%2520custom%2520tokens%2520for%250Arepresenting%2520new%2520concepts%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520Our%2520aim%2520is%2520to%2520learn%250Atokens%2520that%2520can%2520be%2520effective%2520for%2520both%2520discriminative%2520and%2520generative%2520tasks%2520while%250Acomposing%2520well%2520with%2520words%2520to%2520form%2520new%2520input%2520queries.%2520The%2520targeted%2520concept%2520is%250Aspecified%2520in%2520terms%2520of%2520a%2520small%2520set%2520of%2520images%2520and%2520a%2520parent%2520concept%2520described%250Ausing%2520text.%2520We%2520operate%2520on%2520CLIP%2520text%2520features%2520and%2520propose%2520to%2520use%2520a%2520combination%250Aof%2520a%2520textual%2520inversion%2520loss%2520and%2520a%2520classification%2520loss%2520to%2520ensure%2520that%2520text%250Afeatures%2520of%2520the%2520learned%2520token%2520are%2520aligned%2520with%2520image%2520features%2520of%2520the%2520concept%2520in%250Athe%2520CLIP%2520embedding%2520space.%2520We%2520restrict%2520the%2520learned%2520token%2520to%2520a%2520low-dimensional%250Asubspace%2520spanned%2520by%2520tokens%2520for%2520attributes%2520that%2520are%2520appropriate%2520for%2520the%2520given%250Asuper-class.%2520These%2520modifications%2520improve%2520the%2520quality%2520of%2520compositions%2520of%2520the%250Alearned%2520token%2520with%2520natural%2520language%2520for%2520generating%2520new%2520scenes.%2520Further%252C%2520we%2520show%250Athat%2520learned%2520custom%2520tokens%2520can%2520be%2520used%2520to%2520form%2520queries%2520for%2520text-to-image%250Aretrieval%2520task%252C%2520and%2520also%2520have%2520the%2520important%2520benefit%2520that%2520composite%2520queries%2520can%250Abe%2520visualized%2520to%2520ensure%2520that%2520the%2520desired%2520concept%2520is%2520faithfully%2520encoded.%2520Based%250Aon%2520this%252C%2520we%2520introduce%2520the%2520method%2520of%2520Generation%2520Aided%2520Image%2520Retrieval%252C%2520where%2520the%250Aquery%2520is%2520modified%2520at%2520inference%2520time%2520to%2520better%2520suit%2520the%2520search%2520intent.%2520On%2520the%250ADeepFashion2%2520dataset%252C%2520our%2520method%2520improves%2520Mean%2520Reciprocal%2520Retrieval%2520%2528MRR%2529%2520over%250Arelevant%2520baselines%2520by%25207%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Descriminative-Generative%20Custom%20Tokens%20for%20Vision-Language%20Models&entry.906535625=Pramuditha%20Perera%20and%20Matthew%20Trager%20and%20Luca%20Zancato%20and%20Alessandro%20Achille%20and%20Stefano%20Soatto&entry.1292438233=%20%20This%20paper%20explores%20the%20possibility%20of%20learning%20custom%20tokens%20for%0Arepresenting%20new%20concepts%20in%20Vision-Language%20Models%20%28VLMs%29.%20Our%20aim%20is%20to%20learn%0Atokens%20that%20can%20be%20effective%20for%20both%20discriminative%20and%20generative%20tasks%20while%0Acomposing%20well%20with%20words%20to%20form%20new%20input%20queries.%20The%20targeted%20concept%20is%0Aspecified%20in%20terms%20of%20a%20small%20set%20of%20images%20and%20a%20parent%20concept%20described%0Ausing%20text.%20We%20operate%20on%20CLIP%20text%20features%20and%20propose%20to%20use%20a%20combination%0Aof%20a%20textual%20inversion%20loss%20and%20a%20classification%20loss%20to%20ensure%20that%20text%0Afeatures%20of%20the%20learned%20token%20are%20aligned%20with%20image%20features%20of%20the%20concept%20in%0Athe%20CLIP%20embedding%20space.%20We%20restrict%20the%20learned%20token%20to%20a%20low-dimensional%0Asubspace%20spanned%20by%20tokens%20for%20attributes%20that%20are%20appropriate%20for%20the%20given%0Asuper-class.%20These%20modifications%20improve%20the%20quality%20of%20compositions%20of%20the%0Alearned%20token%20with%20natural%20language%20for%20generating%20new%20scenes.%20Further%2C%20we%20show%0Athat%20learned%20custom%20tokens%20can%20be%20used%20to%20form%20queries%20for%20text-to-image%0Aretrieval%20task%2C%20and%20also%20have%20the%20important%20benefit%20that%20composite%20queries%20can%0Abe%20visualized%20to%20ensure%20that%20the%20desired%20concept%20is%20faithfully%20encoded.%20Based%0Aon%20this%2C%20we%20introduce%20the%20method%20of%20Generation%20Aided%20Image%20Retrieval%2C%20where%20the%0Aquery%20is%20modified%20at%20inference%20time%20to%20better%20suit%20the%20search%20intent.%20On%20the%0ADeepFashion2%20dataset%2C%20our%20method%20improves%20Mean%20Reciprocal%20Retrieval%20%28MRR%29%20over%0Arelevant%20baselines%20by%207%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12095v1&entry.124074799=Read"},
{"title": "Knowledge Swapping via Learning and Unlearning", "author": "Mingyu Xing and Lechao Cheng and Shengeng Tang and Yaxiong Wang and Zhun Zhong and Meng Wang", "abstract": "  We introduce \\textbf{Knowledge Swapping}, a novel task designed to\nselectively regulate knowledge of a pretrained model by enabling the forgetting\nof user\\-specified information, retaining essential knowledge, and acquiring\nnew knowledge simultaneously. By delving into the analysis of knock-on feature\nhierarchy, we find that incremental learning typically progresses from\nlow\\-level representations to higher\\-level semantics, whereas forgetting tends\nto occur in the opposite direction\\-starting from high-level semantics and\nmoving down to low-level features. Building upon this, we propose to benchmark\nthe knowledge swapping task with the strategy of \\textit{Learning Before\nForgetting}. Comprehensive experiments on various tasks like image\nclassification, object detection, and semantic segmentation validate the\neffectiveness of the proposed strategy. The source code is available at\n\\href{https://github.com/xingmingyu123456/KnowledgeSwapping}{https://github.com/xingmingyu123456/KnowledgeSwapping}.\n", "link": "http://arxiv.org/abs/2502.08075v2", "date": "2025-02-17", "relevancy": 2.3342, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4771}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4639}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Swapping%20via%20Learning%20and%20Unlearning&body=Title%3A%20Knowledge%20Swapping%20via%20Learning%20and%20Unlearning%0AAuthor%3A%20Mingyu%20Xing%20and%20Lechao%20Cheng%20and%20Shengeng%20Tang%20and%20Yaxiong%20Wang%20and%20Zhun%20Zhong%20and%20Meng%20Wang%0AAbstract%3A%20%20%20We%20introduce%20%5Ctextbf%7BKnowledge%20Swapping%7D%2C%20a%20novel%20task%20designed%20to%0Aselectively%20regulate%20knowledge%20of%20a%20pretrained%20model%20by%20enabling%20the%20forgetting%0Aof%20user%5C-specified%20information%2C%20retaining%20essential%20knowledge%2C%20and%20acquiring%0Anew%20knowledge%20simultaneously.%20By%20delving%20into%20the%20analysis%20of%20knock-on%20feature%0Ahierarchy%2C%20we%20find%20that%20incremental%20learning%20typically%20progresses%20from%0Alow%5C-level%20representations%20to%20higher%5C-level%20semantics%2C%20whereas%20forgetting%20tends%0Ato%20occur%20in%20the%20opposite%20direction%5C-starting%20from%20high-level%20semantics%20and%0Amoving%20down%20to%20low-level%20features.%20Building%20upon%20this%2C%20we%20propose%20to%20benchmark%0Athe%20knowledge%20swapping%20task%20with%20the%20strategy%20of%20%5Ctextit%7BLearning%20Before%0AForgetting%7D.%20Comprehensive%20experiments%20on%20various%20tasks%20like%20image%0Aclassification%2C%20object%20detection%2C%20and%20semantic%20segmentation%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20strategy.%20The%20source%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/xingmingyu123456/KnowledgeSwapping%7D%7Bhttps%3A//github.com/xingmingyu123456/KnowledgeSwapping%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Swapping%2520via%2520Learning%2520and%2520Unlearning%26entry.906535625%3DMingyu%2520Xing%2520and%2520Lechao%2520Cheng%2520and%2520Shengeng%2520Tang%2520and%2520Yaxiong%2520Wang%2520and%2520Zhun%2520Zhong%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520%255Ctextbf%257BKnowledge%2520Swapping%257D%252C%2520a%2520novel%2520task%2520designed%2520to%250Aselectively%2520regulate%2520knowledge%2520of%2520a%2520pretrained%2520model%2520by%2520enabling%2520the%2520forgetting%250Aof%2520user%255C-specified%2520information%252C%2520retaining%2520essential%2520knowledge%252C%2520and%2520acquiring%250Anew%2520knowledge%2520simultaneously.%2520By%2520delving%2520into%2520the%2520analysis%2520of%2520knock-on%2520feature%250Ahierarchy%252C%2520we%2520find%2520that%2520incremental%2520learning%2520typically%2520progresses%2520from%250Alow%255C-level%2520representations%2520to%2520higher%255C-level%2520semantics%252C%2520whereas%2520forgetting%2520tends%250Ato%2520occur%2520in%2520the%2520opposite%2520direction%255C-starting%2520from%2520high-level%2520semantics%2520and%250Amoving%2520down%2520to%2520low-level%2520features.%2520Building%2520upon%2520this%252C%2520we%2520propose%2520to%2520benchmark%250Athe%2520knowledge%2520swapping%2520task%2520with%2520the%2520strategy%2520of%2520%255Ctextit%257BLearning%2520Before%250AForgetting%257D.%2520Comprehensive%2520experiments%2520on%2520various%2520tasks%2520like%2520image%250Aclassification%252C%2520object%2520detection%252C%2520and%2520semantic%2520segmentation%2520validate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520strategy.%2520The%2520source%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/xingmingyu123456/KnowledgeSwapping%257D%257Bhttps%253A//github.com/xingmingyu123456/KnowledgeSwapping%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Swapping%20via%20Learning%20and%20Unlearning&entry.906535625=Mingyu%20Xing%20and%20Lechao%20Cheng%20and%20Shengeng%20Tang%20and%20Yaxiong%20Wang%20and%20Zhun%20Zhong%20and%20Meng%20Wang&entry.1292438233=%20%20We%20introduce%20%5Ctextbf%7BKnowledge%20Swapping%7D%2C%20a%20novel%20task%20designed%20to%0Aselectively%20regulate%20knowledge%20of%20a%20pretrained%20model%20by%20enabling%20the%20forgetting%0Aof%20user%5C-specified%20information%2C%20retaining%20essential%20knowledge%2C%20and%20acquiring%0Anew%20knowledge%20simultaneously.%20By%20delving%20into%20the%20analysis%20of%20knock-on%20feature%0Ahierarchy%2C%20we%20find%20that%20incremental%20learning%20typically%20progresses%20from%0Alow%5C-level%20representations%20to%20higher%5C-level%20semantics%2C%20whereas%20forgetting%20tends%0Ato%20occur%20in%20the%20opposite%20direction%5C-starting%20from%20high-level%20semantics%20and%0Amoving%20down%20to%20low-level%20features.%20Building%20upon%20this%2C%20we%20propose%20to%20benchmark%0Athe%20knowledge%20swapping%20task%20with%20the%20strategy%20of%20%5Ctextit%7BLearning%20Before%0AForgetting%7D.%20Comprehensive%20experiments%20on%20various%20tasks%20like%20image%0Aclassification%2C%20object%20detection%2C%20and%20semantic%20segmentation%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20strategy.%20The%20source%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/xingmingyu123456/KnowledgeSwapping%7D%7Bhttps%3A//github.com/xingmingyu123456/KnowledgeSwapping%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08075v2&entry.124074799=Read"},
{"title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models", "author": "Shehel Yoosuf and Temoor Ali and Ahmed Lekssays and Mashael AlSabah and Issa Khalil", "abstract": "  In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g. SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing\n\\textit{content transformations}, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware, and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection.\n", "link": "http://arxiv.org/abs/2502.11853v1", "date": "2025-02-17", "relevancy": 2.3265, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models&body=Title%3A%20StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Shehel%20Yoosuf%20and%20Temoor%20Ali%20and%20Ahmed%20Lekssays%20and%20Mashael%20AlSabah%20and%20Issa%20Khalil%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20series%20of%20structure%20transformation%20attacks%20on%20LLM%0Aalignment%2C%20where%20we%20encode%20natural%20language%20intent%20using%20diverse%20syntax%20spaces%2C%0Aranging%20from%20simple%20structure%20formats%20and%20basic%20query%20languages%20%28e.g.%20SQL%29%20to%0Anew%20novel%20spaces%20and%20syntaxes%20created%20entirely%20by%20LLMs.%20Our%20extensive%0Aevaluation%20shows%20that%20our%20simplest%20attacks%20can%20achieve%20close%20to%2090%25%20success%0Arate%2C%20even%20on%20strict%20LLMs%20%28such%20as%20Claude%203.5%20Sonnet%29%20using%20SOTA%20alignment%0Amechanisms.%20We%20improve%20the%20attack%20performance%20further%20by%20using%20an%20adaptive%0Ascheme%20that%20combines%20structure%20transformations%20along%20with%20existing%0A%5Ctextit%7Bcontent%20transformations%7D%2C%20resulting%20in%20over%2096%25%20ASR%20with%200%25%20refusals.%0A%20%20To%20generalize%20our%20attacks%2C%20we%20explore%20numerous%20structure%20formats%2C%20including%0Asyntaxes%20purely%20generated%20by%20LLMs.%20Our%20results%20indicate%20that%20such%20novel%0Asyntaxes%20are%20easy%20to%20generate%20and%20result%20in%20a%20high%20ASR%2C%20suggesting%20that%0Adefending%20against%20our%20attacks%20is%20not%20a%20straightforward%20process.%20Finally%2C%20we%0Adevelop%20a%20benchmark%20and%20evaluate%20existing%20safety-alignment%20defenses%20against%20it%2C%0Ashowing%20that%20most%20of%20them%20fail%20with%20100%25%20ASR.%20Our%20results%20show%20that%20existing%0Asafety%20alignment%20mostly%20relies%20on%20token-level%20patterns%20without%20recognizing%0Aharmful%20concepts%2C%20highlighting%20and%20motivating%20the%20need%20for%20serious%20research%0Aefforts%20in%20this%20direction.%20As%20a%20case%20study%2C%20we%20demonstrate%20how%20attackers%20can%0Ause%20our%20attack%20to%20easily%20generate%20a%20sample%20malware%2C%20and%20a%20corpus%20of%20fraudulent%0ASMS%20messages%2C%20which%20perform%20well%20in%20bypassing%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructTransform%253A%2520A%2520Scalable%2520Attack%2520Surface%2520for%2520Safety-Aligned%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DShehel%2520Yoosuf%2520and%2520Temoor%2520Ali%2520and%2520Ahmed%2520Lekssays%2520and%2520Mashael%2520AlSabah%2520and%2520Issa%2520Khalil%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520series%2520of%2520structure%2520transformation%2520attacks%2520on%2520LLM%250Aalignment%252C%2520where%2520we%2520encode%2520natural%2520language%2520intent%2520using%2520diverse%2520syntax%2520spaces%252C%250Aranging%2520from%2520simple%2520structure%2520formats%2520and%2520basic%2520query%2520languages%2520%2528e.g.%2520SQL%2529%2520to%250Anew%2520novel%2520spaces%2520and%2520syntaxes%2520created%2520entirely%2520by%2520LLMs.%2520Our%2520extensive%250Aevaluation%2520shows%2520that%2520our%2520simplest%2520attacks%2520can%2520achieve%2520close%2520to%252090%2525%2520success%250Arate%252C%2520even%2520on%2520strict%2520LLMs%2520%2528such%2520as%2520Claude%25203.5%2520Sonnet%2529%2520using%2520SOTA%2520alignment%250Amechanisms.%2520We%2520improve%2520the%2520attack%2520performance%2520further%2520by%2520using%2520an%2520adaptive%250Ascheme%2520that%2520combines%2520structure%2520transformations%2520along%2520with%2520existing%250A%255Ctextit%257Bcontent%2520transformations%257D%252C%2520resulting%2520in%2520over%252096%2525%2520ASR%2520with%25200%2525%2520refusals.%250A%2520%2520To%2520generalize%2520our%2520attacks%252C%2520we%2520explore%2520numerous%2520structure%2520formats%252C%2520including%250Asyntaxes%2520purely%2520generated%2520by%2520LLMs.%2520Our%2520results%2520indicate%2520that%2520such%2520novel%250Asyntaxes%2520are%2520easy%2520to%2520generate%2520and%2520result%2520in%2520a%2520high%2520ASR%252C%2520suggesting%2520that%250Adefending%2520against%2520our%2520attacks%2520is%2520not%2520a%2520straightforward%2520process.%2520Finally%252C%2520we%250Adevelop%2520a%2520benchmark%2520and%2520evaluate%2520existing%2520safety-alignment%2520defenses%2520against%2520it%252C%250Ashowing%2520that%2520most%2520of%2520them%2520fail%2520with%2520100%2525%2520ASR.%2520Our%2520results%2520show%2520that%2520existing%250Asafety%2520alignment%2520mostly%2520relies%2520on%2520token-level%2520patterns%2520without%2520recognizing%250Aharmful%2520concepts%252C%2520highlighting%2520and%2520motivating%2520the%2520need%2520for%2520serious%2520research%250Aefforts%2520in%2520this%2520direction.%2520As%2520a%2520case%2520study%252C%2520we%2520demonstrate%2520how%2520attackers%2520can%250Ause%2520our%2520attack%2520to%2520easily%2520generate%2520a%2520sample%2520malware%252C%2520and%2520a%2520corpus%2520of%2520fraudulent%250ASMS%2520messages%252C%2520which%2520perform%2520well%2520in%2520bypassing%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructTransform%3A%20A%20Scalable%20Attack%20Surface%20for%20Safety-Aligned%20Large%0A%20%20Language%20Models&entry.906535625=Shehel%20Yoosuf%20and%20Temoor%20Ali%20and%20Ahmed%20Lekssays%20and%20Mashael%20AlSabah%20and%20Issa%20Khalil&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20series%20of%20structure%20transformation%20attacks%20on%20LLM%0Aalignment%2C%20where%20we%20encode%20natural%20language%20intent%20using%20diverse%20syntax%20spaces%2C%0Aranging%20from%20simple%20structure%20formats%20and%20basic%20query%20languages%20%28e.g.%20SQL%29%20to%0Anew%20novel%20spaces%20and%20syntaxes%20created%20entirely%20by%20LLMs.%20Our%20extensive%0Aevaluation%20shows%20that%20our%20simplest%20attacks%20can%20achieve%20close%20to%2090%25%20success%0Arate%2C%20even%20on%20strict%20LLMs%20%28such%20as%20Claude%203.5%20Sonnet%29%20using%20SOTA%20alignment%0Amechanisms.%20We%20improve%20the%20attack%20performance%20further%20by%20using%20an%20adaptive%0Ascheme%20that%20combines%20structure%20transformations%20along%20with%20existing%0A%5Ctextit%7Bcontent%20transformations%7D%2C%20resulting%20in%20over%2096%25%20ASR%20with%200%25%20refusals.%0A%20%20To%20generalize%20our%20attacks%2C%20we%20explore%20numerous%20structure%20formats%2C%20including%0Asyntaxes%20purely%20generated%20by%20LLMs.%20Our%20results%20indicate%20that%20such%20novel%0Asyntaxes%20are%20easy%20to%20generate%20and%20result%20in%20a%20high%20ASR%2C%20suggesting%20that%0Adefending%20against%20our%20attacks%20is%20not%20a%20straightforward%20process.%20Finally%2C%20we%0Adevelop%20a%20benchmark%20and%20evaluate%20existing%20safety-alignment%20defenses%20against%20it%2C%0Ashowing%20that%20most%20of%20them%20fail%20with%20100%25%20ASR.%20Our%20results%20show%20that%20existing%0Asafety%20alignment%20mostly%20relies%20on%20token-level%20patterns%20without%20recognizing%0Aharmful%20concepts%2C%20highlighting%20and%20motivating%20the%20need%20for%20serious%20research%0Aefforts%20in%20this%20direction.%20As%20a%20case%20study%2C%20we%20demonstrate%20how%20attackers%20can%0Ause%20our%20attack%20to%20easily%20generate%20a%20sample%20malware%2C%20and%20a%20corpus%20of%20fraudulent%0ASMS%20messages%2C%20which%20perform%20well%20in%20bypassing%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11853v1&entry.124074799=Read"},
{"title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model", "author": "Guangzhi Sun and Yudong Yang and Jimin Zhuang and Changli Tang and Yixuan Li and Wei Li and Zejun MA and Chao Zhang", "abstract": "  While recent advancements in reasoning optimization have significantly\nenhanced the capabilities of large language models (LLMs), existing efforts to\nimprove reasoning have been limited to solving mathematical problems and\nfocusing on visual graphical inputs, neglecting broader applications in general\nvideo understanding.This paper proposes video-SALMONN-o1, the first open-source\nreasoning-enhanced audio-visual LLM designed for general video understanding\ntasks. To enhance its reasoning abilities, we develop a reasoning-intensive\ndataset featuring challenging audio-visual questions with step-by-step\nsolutions. We also propose process direct preference optimization (pDPO), which\nleverages contrastive step selection to achieve efficient step-level reward\nmodelling tailored for multimodal inputs. Additionally, we introduce RivaBench,\nthe first reasoning-intensive video understanding benchmark, featuring over\n4,000 high-quality, expert-curated question-answer pairs across scenarios such\nas standup comedy, academic presentations, and synthetic video detection.\nvideo-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision\nbaseline across different video reasoning benchmarks. Besides, pDPO achieves\n6-8% improvements compared to the supervised fine-tuning model on RivaBench.\nEnhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection\ncapabilities.\n", "link": "http://arxiv.org/abs/2502.11775v1", "date": "2025-02-17", "relevancy": 2.3213, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5941}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5941}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20video-SALMONN-o1%3A%20Reasoning-enhanced%20Audio-visual%20Large%20Language%20Model&body=Title%3A%20video-SALMONN-o1%3A%20Reasoning-enhanced%20Audio-visual%20Large%20Language%20Model%0AAuthor%3A%20Guangzhi%20Sun%20and%20Yudong%20Yang%20and%20Jimin%20Zhuang%20and%20Changli%20Tang%20and%20Yixuan%20Li%20and%20Wei%20Li%20and%20Zejun%20MA%20and%20Chao%20Zhang%0AAbstract%3A%20%20%20While%20recent%20advancements%20in%20reasoning%20optimization%20have%20significantly%0Aenhanced%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%2C%20existing%20efforts%20to%0Aimprove%20reasoning%20have%20been%20limited%20to%20solving%20mathematical%20problems%20and%0Afocusing%20on%20visual%20graphical%20inputs%2C%20neglecting%20broader%20applications%20in%20general%0Avideo%20understanding.This%20paper%20proposes%20video-SALMONN-o1%2C%20the%20first%20open-source%0Areasoning-enhanced%20audio-visual%20LLM%20designed%20for%20general%20video%20understanding%0Atasks.%20To%20enhance%20its%20reasoning%20abilities%2C%20we%20develop%20a%20reasoning-intensive%0Adataset%20featuring%20challenging%20audio-visual%20questions%20with%20step-by-step%0Asolutions.%20We%20also%20propose%20process%20direct%20preference%20optimization%20%28pDPO%29%2C%20which%0Aleverages%20contrastive%20step%20selection%20to%20achieve%20efficient%20step-level%20reward%0Amodelling%20tailored%20for%20multimodal%20inputs.%20Additionally%2C%20we%20introduce%20RivaBench%2C%0Athe%20first%20reasoning-intensive%20video%20understanding%20benchmark%2C%20featuring%20over%0A4%2C000%20high-quality%2C%20expert-curated%20question-answer%20pairs%20across%20scenarios%20such%0Aas%20standup%20comedy%2C%20academic%20presentations%2C%20and%20synthetic%20video%20detection.%0Avideo-SALMONN-o1%20achieves%203-8%25%20accuracy%20improvements%20over%20the%20LLaVA-OneVision%0Abaseline%20across%20different%20video%20reasoning%20benchmarks.%20Besides%2C%20pDPO%20achieves%0A6-8%25%20improvements%20compared%20to%20the%20supervised%20fine-tuning%20model%20on%20RivaBench.%0AEnhanced%20reasoning%20enables%20video-SALMONN-o1%20zero-shot%20synthetic%20video%20detection%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dvideo-SALMONN-o1%253A%2520Reasoning-enhanced%2520Audio-visual%2520Large%2520Language%2520Model%26entry.906535625%3DGuangzhi%2520Sun%2520and%2520Yudong%2520Yang%2520and%2520Jimin%2520Zhuang%2520and%2520Changli%2520Tang%2520and%2520Yixuan%2520Li%2520and%2520Wei%2520Li%2520and%2520Zejun%2520MA%2520and%2520Chao%2520Zhang%26entry.1292438233%3D%2520%2520While%2520recent%2520advancements%2520in%2520reasoning%2520optimization%2520have%2520significantly%250Aenhanced%2520the%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520existing%2520efforts%2520to%250Aimprove%2520reasoning%2520have%2520been%2520limited%2520to%2520solving%2520mathematical%2520problems%2520and%250Afocusing%2520on%2520visual%2520graphical%2520inputs%252C%2520neglecting%2520broader%2520applications%2520in%2520general%250Avideo%2520understanding.This%2520paper%2520proposes%2520video-SALMONN-o1%252C%2520the%2520first%2520open-source%250Areasoning-enhanced%2520audio-visual%2520LLM%2520designed%2520for%2520general%2520video%2520understanding%250Atasks.%2520To%2520enhance%2520its%2520reasoning%2520abilities%252C%2520we%2520develop%2520a%2520reasoning-intensive%250Adataset%2520featuring%2520challenging%2520audio-visual%2520questions%2520with%2520step-by-step%250Asolutions.%2520We%2520also%2520propose%2520process%2520direct%2520preference%2520optimization%2520%2528pDPO%2529%252C%2520which%250Aleverages%2520contrastive%2520step%2520selection%2520to%2520achieve%2520efficient%2520step-level%2520reward%250Amodelling%2520tailored%2520for%2520multimodal%2520inputs.%2520Additionally%252C%2520we%2520introduce%2520RivaBench%252C%250Athe%2520first%2520reasoning-intensive%2520video%2520understanding%2520benchmark%252C%2520featuring%2520over%250A4%252C000%2520high-quality%252C%2520expert-curated%2520question-answer%2520pairs%2520across%2520scenarios%2520such%250Aas%2520standup%2520comedy%252C%2520academic%2520presentations%252C%2520and%2520synthetic%2520video%2520detection.%250Avideo-SALMONN-o1%2520achieves%25203-8%2525%2520accuracy%2520improvements%2520over%2520the%2520LLaVA-OneVision%250Abaseline%2520across%2520different%2520video%2520reasoning%2520benchmarks.%2520Besides%252C%2520pDPO%2520achieves%250A6-8%2525%2520improvements%2520compared%2520to%2520the%2520supervised%2520fine-tuning%2520model%2520on%2520RivaBench.%250AEnhanced%2520reasoning%2520enables%2520video-SALMONN-o1%2520zero-shot%2520synthetic%2520video%2520detection%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=video-SALMONN-o1%3A%20Reasoning-enhanced%20Audio-visual%20Large%20Language%20Model&entry.906535625=Guangzhi%20Sun%20and%20Yudong%20Yang%20and%20Jimin%20Zhuang%20and%20Changli%20Tang%20and%20Yixuan%20Li%20and%20Wei%20Li%20and%20Zejun%20MA%20and%20Chao%20Zhang&entry.1292438233=%20%20While%20recent%20advancements%20in%20reasoning%20optimization%20have%20significantly%0Aenhanced%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%2C%20existing%20efforts%20to%0Aimprove%20reasoning%20have%20been%20limited%20to%20solving%20mathematical%20problems%20and%0Afocusing%20on%20visual%20graphical%20inputs%2C%20neglecting%20broader%20applications%20in%20general%0Avideo%20understanding.This%20paper%20proposes%20video-SALMONN-o1%2C%20the%20first%20open-source%0Areasoning-enhanced%20audio-visual%20LLM%20designed%20for%20general%20video%20understanding%0Atasks.%20To%20enhance%20its%20reasoning%20abilities%2C%20we%20develop%20a%20reasoning-intensive%0Adataset%20featuring%20challenging%20audio-visual%20questions%20with%20step-by-step%0Asolutions.%20We%20also%20propose%20process%20direct%20preference%20optimization%20%28pDPO%29%2C%20which%0Aleverages%20contrastive%20step%20selection%20to%20achieve%20efficient%20step-level%20reward%0Amodelling%20tailored%20for%20multimodal%20inputs.%20Additionally%2C%20we%20introduce%20RivaBench%2C%0Athe%20first%20reasoning-intensive%20video%20understanding%20benchmark%2C%20featuring%20over%0A4%2C000%20high-quality%2C%20expert-curated%20question-answer%20pairs%20across%20scenarios%20such%0Aas%20standup%20comedy%2C%20academic%20presentations%2C%20and%20synthetic%20video%20detection.%0Avideo-SALMONN-o1%20achieves%203-8%25%20accuracy%20improvements%20over%20the%20LLaVA-OneVision%0Abaseline%20across%20different%20video%20reasoning%20benchmarks.%20Besides%2C%20pDPO%20achieves%0A6-8%25%20improvements%20compared%20to%20the%20supervised%20fine-tuning%20model%20on%20RivaBench.%0AEnhanced%20reasoning%20enables%20video-SALMONN-o1%20zero-shot%20synthetic%20video%20detection%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11775v1&entry.124074799=Read"},
{"title": "Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat", "author": "Roland Daynauth and Christopher Clarke and Krisztian Flautner and Lingjia Tang and Jason Mars", "abstract": "  Deciding which large language model (LLM) to use is a complex challenge.\nPairwise ranking has emerged as a new method for evaluating human preferences\nfor LLMs. This approach entails humans evaluating pairs of model outputs based\non a predefined criterion. By collecting these comparisons, a ranking can be\nconstructed using methods such as Elo. However, applying these algorithms as\nconstructed in the context of LLM evaluation introduces several challenges. In\nthis paper, we explore the effectiveness of ranking systems for head-to-head\ncomparisons of LLMs. We formally define a set of fundamental principles for\neffective ranking and conduct a series of extensive evaluations on the\nrobustness of several ranking algorithms in the context of LLMs. Our analysis\nuncovers key insights into the factors that affect ranking accuracy and\nefficiency, offering guidelines for selecting the most appropriate methods\nbased on specific evaluation contexts and resource constraints.\n", "link": "http://arxiv.org/abs/2411.14483v2", "date": "2025-02-17", "relevancy": 2.3201, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ranking%20Unraveled%3A%20Recipes%20for%20LLM%20Rankings%20in%20Head-to-Head%20AI%20Combat&body=Title%3A%20Ranking%20Unraveled%3A%20Recipes%20for%20LLM%20Rankings%20in%20Head-to-Head%20AI%20Combat%0AAuthor%3A%20Roland%20Daynauth%20and%20Christopher%20Clarke%20and%20Krisztian%20Flautner%20and%20Lingjia%20Tang%20and%20Jason%20Mars%0AAbstract%3A%20%20%20Deciding%20which%20large%20language%20model%20%28LLM%29%20to%20use%20is%20a%20complex%20challenge.%0APairwise%20ranking%20has%20emerged%20as%20a%20new%20method%20for%20evaluating%20human%20preferences%0Afor%20LLMs.%20This%20approach%20entails%20humans%20evaluating%20pairs%20of%20model%20outputs%20based%0Aon%20a%20predefined%20criterion.%20By%20collecting%20these%20comparisons%2C%20a%20ranking%20can%20be%0Aconstructed%20using%20methods%20such%20as%20Elo.%20However%2C%20applying%20these%20algorithms%20as%0Aconstructed%20in%20the%20context%20of%20LLM%20evaluation%20introduces%20several%20challenges.%20In%0Athis%20paper%2C%20we%20explore%20the%20effectiveness%20of%20ranking%20systems%20for%20head-to-head%0Acomparisons%20of%20LLMs.%20We%20formally%20define%20a%20set%20of%20fundamental%20principles%20for%0Aeffective%20ranking%20and%20conduct%20a%20series%20of%20extensive%20evaluations%20on%20the%0Arobustness%20of%20several%20ranking%20algorithms%20in%20the%20context%20of%20LLMs.%20Our%20analysis%0Auncovers%20key%20insights%20into%20the%20factors%20that%20affect%20ranking%20accuracy%20and%0Aefficiency%2C%20offering%20guidelines%20for%20selecting%20the%20most%20appropriate%20methods%0Abased%20on%20specific%20evaluation%20contexts%20and%20resource%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRanking%2520Unraveled%253A%2520Recipes%2520for%2520LLM%2520Rankings%2520in%2520Head-to-Head%2520AI%2520Combat%26entry.906535625%3DRoland%2520Daynauth%2520and%2520Christopher%2520Clarke%2520and%2520Krisztian%2520Flautner%2520and%2520Lingjia%2520Tang%2520and%2520Jason%2520Mars%26entry.1292438233%3D%2520%2520Deciding%2520which%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520use%2520is%2520a%2520complex%2520challenge.%250APairwise%2520ranking%2520has%2520emerged%2520as%2520a%2520new%2520method%2520for%2520evaluating%2520human%2520preferences%250Afor%2520LLMs.%2520This%2520approach%2520entails%2520humans%2520evaluating%2520pairs%2520of%2520model%2520outputs%2520based%250Aon%2520a%2520predefined%2520criterion.%2520By%2520collecting%2520these%2520comparisons%252C%2520a%2520ranking%2520can%2520be%250Aconstructed%2520using%2520methods%2520such%2520as%2520Elo.%2520However%252C%2520applying%2520these%2520algorithms%2520as%250Aconstructed%2520in%2520the%2520context%2520of%2520LLM%2520evaluation%2520introduces%2520several%2520challenges.%2520In%250Athis%2520paper%252C%2520we%2520explore%2520the%2520effectiveness%2520of%2520ranking%2520systems%2520for%2520head-to-head%250Acomparisons%2520of%2520LLMs.%2520We%2520formally%2520define%2520a%2520set%2520of%2520fundamental%2520principles%2520for%250Aeffective%2520ranking%2520and%2520conduct%2520a%2520series%2520of%2520extensive%2520evaluations%2520on%2520the%250Arobustness%2520of%2520several%2520ranking%2520algorithms%2520in%2520the%2520context%2520of%2520LLMs.%2520Our%2520analysis%250Auncovers%2520key%2520insights%2520into%2520the%2520factors%2520that%2520affect%2520ranking%2520accuracy%2520and%250Aefficiency%252C%2520offering%2520guidelines%2520for%2520selecting%2520the%2520most%2520appropriate%2520methods%250Abased%2520on%2520specific%2520evaluation%2520contexts%2520and%2520resource%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ranking%20Unraveled%3A%20Recipes%20for%20LLM%20Rankings%20in%20Head-to-Head%20AI%20Combat&entry.906535625=Roland%20Daynauth%20and%20Christopher%20Clarke%20and%20Krisztian%20Flautner%20and%20Lingjia%20Tang%20and%20Jason%20Mars&entry.1292438233=%20%20Deciding%20which%20large%20language%20model%20%28LLM%29%20to%20use%20is%20a%20complex%20challenge.%0APairwise%20ranking%20has%20emerged%20as%20a%20new%20method%20for%20evaluating%20human%20preferences%0Afor%20LLMs.%20This%20approach%20entails%20humans%20evaluating%20pairs%20of%20model%20outputs%20based%0Aon%20a%20predefined%20criterion.%20By%20collecting%20these%20comparisons%2C%20a%20ranking%20can%20be%0Aconstructed%20using%20methods%20such%20as%20Elo.%20However%2C%20applying%20these%20algorithms%20as%0Aconstructed%20in%20the%20context%20of%20LLM%20evaluation%20introduces%20several%20challenges.%20In%0Athis%20paper%2C%20we%20explore%20the%20effectiveness%20of%20ranking%20systems%20for%20head-to-head%0Acomparisons%20of%20LLMs.%20We%20formally%20define%20a%20set%20of%20fundamental%20principles%20for%0Aeffective%20ranking%20and%20conduct%20a%20series%20of%20extensive%20evaluations%20on%20the%0Arobustness%20of%20several%20ranking%20algorithms%20in%20the%20context%20of%20LLMs.%20Our%20analysis%0Auncovers%20key%20insights%20into%20the%20factors%20that%20affect%20ranking%20accuracy%20and%0Aefficiency%2C%20offering%20guidelines%20for%20selecting%20the%20most%20appropriate%20methods%0Abased%20on%20specific%20evaluation%20contexts%20and%20resource%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14483v2&entry.124074799=Read"},
{"title": "I-CTRL: Imitation to Control Humanoid Robots Through Constrained\n  Reinforcement Learning", "author": "Yashuai Yan and Esteve Valls Mascaro and Tobias Egle and Dongheui Lee", "abstract": "  Humanoid robots have the potential to mimic human motions with high visual\nfidelity, yet translating these motions into practical, physical execution\nremains a significant challenge. Existing techniques in the graphics community\noften prioritize visual fidelity over physics-based feasibility, posing a\nsignificant challenge for deploying bipedal systems in practical applications.\nThis paper addresses these issues through bounded residual reinforcement\nlearning to produce physics-based high-quality motion imitation onto legged\nhumanoid robots that enhance motion resemblance while successfully following\nthe reference human trajectory. Our framework, Imitation to Control Humanoid\nRobots Through Bounded Residual Reinforcement Learning (I-CTRL), reformulates\nmotion imitation as a constrained refinement over non-physics-based retargeted\nmotions. I-CTRL excels in motion imitation with simple and unique rewards that\ngeneralize across five robots. Moreover, our framework introduces an automatic\npriority scheduler to manage large-scale motion datasets when efficiently\ntraining a unified RL policy across diverse motions. The proposed approach\nsignifies a crucial step forward in advancing the control of bipedal robots,\nemphasizing the importance of aligning visual and physical realism for\nsuccessful motion imitation.\n", "link": "http://arxiv.org/abs/2405.08726v2", "date": "2025-02-17", "relevancy": 2.3194, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5816}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I-CTRL%3A%20Imitation%20to%20Control%20Humanoid%20Robots%20Through%20Constrained%0A%20%20Reinforcement%20Learning&body=Title%3A%20I-CTRL%3A%20Imitation%20to%20Control%20Humanoid%20Robots%20Through%20Constrained%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Yashuai%20Yan%20and%20Esteve%20Valls%20Mascaro%20and%20Tobias%20Egle%20and%20Dongheui%20Lee%0AAbstract%3A%20%20%20Humanoid%20robots%20have%20the%20potential%20to%20mimic%20human%20motions%20with%20high%20visual%0Afidelity%2C%20yet%20translating%20these%20motions%20into%20practical%2C%20physical%20execution%0Aremains%20a%20significant%20challenge.%20Existing%20techniques%20in%20the%20graphics%20community%0Aoften%20prioritize%20visual%20fidelity%20over%20physics-based%20feasibility%2C%20posing%20a%0Asignificant%20challenge%20for%20deploying%20bipedal%20systems%20in%20practical%20applications.%0AThis%20paper%20addresses%20these%20issues%20through%20bounded%20residual%20reinforcement%0Alearning%20to%20produce%20physics-based%20high-quality%20motion%20imitation%20onto%20legged%0Ahumanoid%20robots%20that%20enhance%20motion%20resemblance%20while%20successfully%20following%0Athe%20reference%20human%20trajectory.%20Our%20framework%2C%20Imitation%20to%20Control%20Humanoid%0ARobots%20Through%20Bounded%20Residual%20Reinforcement%20Learning%20%28I-CTRL%29%2C%20reformulates%0Amotion%20imitation%20as%20a%20constrained%20refinement%20over%20non-physics-based%20retargeted%0Amotions.%20I-CTRL%20excels%20in%20motion%20imitation%20with%20simple%20and%20unique%20rewards%20that%0Ageneralize%20across%20five%20robots.%20Moreover%2C%20our%20framework%20introduces%20an%20automatic%0Apriority%20scheduler%20to%20manage%20large-scale%20motion%20datasets%20when%20efficiently%0Atraining%20a%20unified%20RL%20policy%20across%20diverse%20motions.%20The%20proposed%20approach%0Asignifies%20a%20crucial%20step%20forward%20in%20advancing%20the%20control%20of%20bipedal%20robots%2C%0Aemphasizing%20the%20importance%20of%20aligning%20visual%20and%20physical%20realism%20for%0Asuccessful%20motion%20imitation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08726v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI-CTRL%253A%2520Imitation%2520to%2520Control%2520Humanoid%2520Robots%2520Through%2520Constrained%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DYashuai%2520Yan%2520and%2520Esteve%2520Valls%2520Mascaro%2520and%2520Tobias%2520Egle%2520and%2520Dongheui%2520Lee%26entry.1292438233%3D%2520%2520Humanoid%2520robots%2520have%2520the%2520potential%2520to%2520mimic%2520human%2520motions%2520with%2520high%2520visual%250Afidelity%252C%2520yet%2520translating%2520these%2520motions%2520into%2520practical%252C%2520physical%2520execution%250Aremains%2520a%2520significant%2520challenge.%2520Existing%2520techniques%2520in%2520the%2520graphics%2520community%250Aoften%2520prioritize%2520visual%2520fidelity%2520over%2520physics-based%2520feasibility%252C%2520posing%2520a%250Asignificant%2520challenge%2520for%2520deploying%2520bipedal%2520systems%2520in%2520practical%2520applications.%250AThis%2520paper%2520addresses%2520these%2520issues%2520through%2520bounded%2520residual%2520reinforcement%250Alearning%2520to%2520produce%2520physics-based%2520high-quality%2520motion%2520imitation%2520onto%2520legged%250Ahumanoid%2520robots%2520that%2520enhance%2520motion%2520resemblance%2520while%2520successfully%2520following%250Athe%2520reference%2520human%2520trajectory.%2520Our%2520framework%252C%2520Imitation%2520to%2520Control%2520Humanoid%250ARobots%2520Through%2520Bounded%2520Residual%2520Reinforcement%2520Learning%2520%2528I-CTRL%2529%252C%2520reformulates%250Amotion%2520imitation%2520as%2520a%2520constrained%2520refinement%2520over%2520non-physics-based%2520retargeted%250Amotions.%2520I-CTRL%2520excels%2520in%2520motion%2520imitation%2520with%2520simple%2520and%2520unique%2520rewards%2520that%250Ageneralize%2520across%2520five%2520robots.%2520Moreover%252C%2520our%2520framework%2520introduces%2520an%2520automatic%250Apriority%2520scheduler%2520to%2520manage%2520large-scale%2520motion%2520datasets%2520when%2520efficiently%250Atraining%2520a%2520unified%2520RL%2520policy%2520across%2520diverse%2520motions.%2520The%2520proposed%2520approach%250Asignifies%2520a%2520crucial%2520step%2520forward%2520in%2520advancing%2520the%2520control%2520of%2520bipedal%2520robots%252C%250Aemphasizing%2520the%2520importance%2520of%2520aligning%2520visual%2520and%2520physical%2520realism%2520for%250Asuccessful%2520motion%2520imitation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08726v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I-CTRL%3A%20Imitation%20to%20Control%20Humanoid%20Robots%20Through%20Constrained%0A%20%20Reinforcement%20Learning&entry.906535625=Yashuai%20Yan%20and%20Esteve%20Valls%20Mascaro%20and%20Tobias%20Egle%20and%20Dongheui%20Lee&entry.1292438233=%20%20Humanoid%20robots%20have%20the%20potential%20to%20mimic%20human%20motions%20with%20high%20visual%0Afidelity%2C%20yet%20translating%20these%20motions%20into%20practical%2C%20physical%20execution%0Aremains%20a%20significant%20challenge.%20Existing%20techniques%20in%20the%20graphics%20community%0Aoften%20prioritize%20visual%20fidelity%20over%20physics-based%20feasibility%2C%20posing%20a%0Asignificant%20challenge%20for%20deploying%20bipedal%20systems%20in%20practical%20applications.%0AThis%20paper%20addresses%20these%20issues%20through%20bounded%20residual%20reinforcement%0Alearning%20to%20produce%20physics-based%20high-quality%20motion%20imitation%20onto%20legged%0Ahumanoid%20robots%20that%20enhance%20motion%20resemblance%20while%20successfully%20following%0Athe%20reference%20human%20trajectory.%20Our%20framework%2C%20Imitation%20to%20Control%20Humanoid%0ARobots%20Through%20Bounded%20Residual%20Reinforcement%20Learning%20%28I-CTRL%29%2C%20reformulates%0Amotion%20imitation%20as%20a%20constrained%20refinement%20over%20non-physics-based%20retargeted%0Amotions.%20I-CTRL%20excels%20in%20motion%20imitation%20with%20simple%20and%20unique%20rewards%20that%0Ageneralize%20across%20five%20robots.%20Moreover%2C%20our%20framework%20introduces%20an%20automatic%0Apriority%20scheduler%20to%20manage%20large-scale%20motion%20datasets%20when%20efficiently%0Atraining%20a%20unified%20RL%20policy%20across%20diverse%20motions.%20The%20proposed%20approach%0Asignifies%20a%20crucial%20step%20forward%20in%20advancing%20the%20control%20of%20bipedal%20robots%2C%0Aemphasizing%20the%20importance%20of%20aligning%20visual%20and%20physical%20realism%20for%0Asuccessful%20motion%20imitation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08726v2&entry.124074799=Read"},
{"title": "Bridging Compressed Image Latents and Multimodal Large Language Models", "author": "Chia-Hao Kao and Cheng Chien and Yu-Jen Tseng and Yi-Hsin Chen and Alessandro Gnutti and Shao-Yuan Lo and Wen-Hsiao Peng and Riccardo Leonardi", "abstract": "  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. Given the huge scale of MLLMs, our\nframework excludes the entire downstream MLLM except part of its visual encoder\nfrom training our system. This stands out from most existing coding for machine\napproaches that involve downstream networks in training and thus could be\nimpractical when the networks are MLLMs. The proposed framework is general in\nthat it is applicable to various MLLMs, neural image codecs, and multiple\napplication scenarios, where the neural image codec can be (1) pre-trained for\nhuman perception without updating, (2) fully updated for joint human and\nmachine perception, or (3) fully updated for only machine perception. Extensive\nexperiments on different neural image codecs and various MLLMs show that our\nmethod achieves great rate-accuracy performance with much less complexity.\n", "link": "http://arxiv.org/abs/2407.19651v2", "date": "2025-02-17", "relevancy": 2.3178, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Compressed%20Image%20Latents%20and%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Bridging%20Compressed%20Image%20Latents%20and%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Chia-Hao%20Kao%20and%20Cheng%20Chien%20and%20Yu-Jen%20Tseng%20and%20Yi-Hsin%20Chen%20and%20Alessandro%20Gnutti%20and%20Shao-Yuan%20Lo%20and%20Wen-Hsiao%20Peng%20and%20Riccardo%20Leonardi%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20first-ever%20study%20of%20adapting%20compressed%20image%20latents%0Ato%20suit%20the%20needs%20of%20downstream%20vision%20tasks%20that%20adopt%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29.%20MLLMs%20have%20extended%20the%20success%20of%20large%20language%0Amodels%20to%20modalities%20%28e.g.%20images%29%20beyond%20text%2C%20but%20their%20billion%20scale%20hinders%0Adeployment%20on%20resource-constrained%20end%20devices.%20While%20cloud-hosted%20MLLMs%20could%0Abe%20available%2C%20transmitting%20raw%2C%20uncompressed%20images%20captured%20by%20end%20devices%20to%0Athe%20cloud%20requires%20an%20efficient%20image%20compression%20system.%20To%20address%20this%2C%20we%0Afocus%20on%20emerging%20neural%20image%20compression%20and%20propose%20a%20novel%20framework%20with%20a%0Alightweight%20transform-neck%20and%20a%20surrogate%20loss%20to%20adapt%20compressed%20image%0Alatents%20for%20MLLM-based%20vision%20tasks.%20Given%20the%20huge%20scale%20of%20MLLMs%2C%20our%0Aframework%20excludes%20the%20entire%20downstream%20MLLM%20except%20part%20of%20its%20visual%20encoder%0Afrom%20training%20our%20system.%20This%20stands%20out%20from%20most%20existing%20coding%20for%20machine%0Aapproaches%20that%20involve%20downstream%20networks%20in%20training%20and%20thus%20could%20be%0Aimpractical%20when%20the%20networks%20are%20MLLMs.%20The%20proposed%20framework%20is%20general%20in%0Athat%20it%20is%20applicable%20to%20various%20MLLMs%2C%20neural%20image%20codecs%2C%20and%20multiple%0Aapplication%20scenarios%2C%20where%20the%20neural%20image%20codec%20can%20be%20%281%29%20pre-trained%20for%0Ahuman%20perception%20without%20updating%2C%20%282%29%20fully%20updated%20for%20joint%20human%20and%0Amachine%20perception%2C%20or%20%283%29%20fully%20updated%20for%20only%20machine%20perception.%20Extensive%0Aexperiments%20on%20different%20neural%20image%20codecs%20and%20various%20MLLMs%20show%20that%20our%0Amethod%20achieves%20great%20rate-accuracy%20performance%20with%20much%20less%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19651v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Compressed%2520Image%2520Latents%2520and%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DChia-Hao%2520Kao%2520and%2520Cheng%2520Chien%2520and%2520Yu-Jen%2520Tseng%2520and%2520Yi-Hsin%2520Chen%2520and%2520Alessandro%2520Gnutti%2520and%2520Shao-Yuan%2520Lo%2520and%2520Wen-Hsiao%2520Peng%2520and%2520Riccardo%2520Leonardi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520first-ever%2520study%2520of%2520adapting%2520compressed%2520image%2520latents%250Ato%2520suit%2520the%2520needs%2520of%2520downstream%2520vision%2520tasks%2520that%2520adopt%2520Multimodal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529.%2520MLLMs%2520have%2520extended%2520the%2520success%2520of%2520large%2520language%250Amodels%2520to%2520modalities%2520%2528e.g.%2520images%2529%2520beyond%2520text%252C%2520but%2520their%2520billion%2520scale%2520hinders%250Adeployment%2520on%2520resource-constrained%2520end%2520devices.%2520While%2520cloud-hosted%2520MLLMs%2520could%250Abe%2520available%252C%2520transmitting%2520raw%252C%2520uncompressed%2520images%2520captured%2520by%2520end%2520devices%2520to%250Athe%2520cloud%2520requires%2520an%2520efficient%2520image%2520compression%2520system.%2520To%2520address%2520this%252C%2520we%250Afocus%2520on%2520emerging%2520neural%2520image%2520compression%2520and%2520propose%2520a%2520novel%2520framework%2520with%2520a%250Alightweight%2520transform-neck%2520and%2520a%2520surrogate%2520loss%2520to%2520adapt%2520compressed%2520image%250Alatents%2520for%2520MLLM-based%2520vision%2520tasks.%2520Given%2520the%2520huge%2520scale%2520of%2520MLLMs%252C%2520our%250Aframework%2520excludes%2520the%2520entire%2520downstream%2520MLLM%2520except%2520part%2520of%2520its%2520visual%2520encoder%250Afrom%2520training%2520our%2520system.%2520This%2520stands%2520out%2520from%2520most%2520existing%2520coding%2520for%2520machine%250Aapproaches%2520that%2520involve%2520downstream%2520networks%2520in%2520training%2520and%2520thus%2520could%2520be%250Aimpractical%2520when%2520the%2520networks%2520are%2520MLLMs.%2520The%2520proposed%2520framework%2520is%2520general%2520in%250Athat%2520it%2520is%2520applicable%2520to%2520various%2520MLLMs%252C%2520neural%2520image%2520codecs%252C%2520and%2520multiple%250Aapplication%2520scenarios%252C%2520where%2520the%2520neural%2520image%2520codec%2520can%2520be%2520%25281%2529%2520pre-trained%2520for%250Ahuman%2520perception%2520without%2520updating%252C%2520%25282%2529%2520fully%2520updated%2520for%2520joint%2520human%2520and%250Amachine%2520perception%252C%2520or%2520%25283%2529%2520fully%2520updated%2520for%2520only%2520machine%2520perception.%2520Extensive%250Aexperiments%2520on%2520different%2520neural%2520image%2520codecs%2520and%2520various%2520MLLMs%2520show%2520that%2520our%250Amethod%2520achieves%2520great%2520rate-accuracy%2520performance%2520with%2520much%2520less%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19651v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Compressed%20Image%20Latents%20and%20Multimodal%20Large%20Language%20Models&entry.906535625=Chia-Hao%20Kao%20and%20Cheng%20Chien%20and%20Yu-Jen%20Tseng%20and%20Yi-Hsin%20Chen%20and%20Alessandro%20Gnutti%20and%20Shao-Yuan%20Lo%20and%20Wen-Hsiao%20Peng%20and%20Riccardo%20Leonardi&entry.1292438233=%20%20This%20paper%20presents%20the%20first-ever%20study%20of%20adapting%20compressed%20image%20latents%0Ato%20suit%20the%20needs%20of%20downstream%20vision%20tasks%20that%20adopt%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29.%20MLLMs%20have%20extended%20the%20success%20of%20large%20language%0Amodels%20to%20modalities%20%28e.g.%20images%29%20beyond%20text%2C%20but%20their%20billion%20scale%20hinders%0Adeployment%20on%20resource-constrained%20end%20devices.%20While%20cloud-hosted%20MLLMs%20could%0Abe%20available%2C%20transmitting%20raw%2C%20uncompressed%20images%20captured%20by%20end%20devices%20to%0Athe%20cloud%20requires%20an%20efficient%20image%20compression%20system.%20To%20address%20this%2C%20we%0Afocus%20on%20emerging%20neural%20image%20compression%20and%20propose%20a%20novel%20framework%20with%20a%0Alightweight%20transform-neck%20and%20a%20surrogate%20loss%20to%20adapt%20compressed%20image%0Alatents%20for%20MLLM-based%20vision%20tasks.%20Given%20the%20huge%20scale%20of%20MLLMs%2C%20our%0Aframework%20excludes%20the%20entire%20downstream%20MLLM%20except%20part%20of%20its%20visual%20encoder%0Afrom%20training%20our%20system.%20This%20stands%20out%20from%20most%20existing%20coding%20for%20machine%0Aapproaches%20that%20involve%20downstream%20networks%20in%20training%20and%20thus%20could%20be%0Aimpractical%20when%20the%20networks%20are%20MLLMs.%20The%20proposed%20framework%20is%20general%20in%0Athat%20it%20is%20applicable%20to%20various%20MLLMs%2C%20neural%20image%20codecs%2C%20and%20multiple%0Aapplication%20scenarios%2C%20where%20the%20neural%20image%20codec%20can%20be%20%281%29%20pre-trained%20for%0Ahuman%20perception%20without%20updating%2C%20%282%29%20fully%20updated%20for%20joint%20human%20and%0Amachine%20perception%2C%20or%20%283%29%20fully%20updated%20for%20only%20machine%20perception.%20Extensive%0Aexperiments%20on%20different%20neural%20image%20codecs%20and%20various%20MLLMs%20show%20that%20our%0Amethod%20achieves%20great%20rate-accuracy%20performance%20with%20much%20less%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19651v2&entry.124074799=Read"},
{"title": "T2VEval: T2V-generated Videos Benchmark Dataset and Objective Evaluation\n  Method", "author": "Zelu Qi and Ping Shi and Shuqi Wang and Zhaoyang Zhang and Fei Zhao and Zefeng Ying and Da Pan", "abstract": "  Recent advances in text-to-video (T2V) technology, as demonstrated by models\nsuch as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the\napplicability and popularity of the technology. This progress has created a\ngrowing demand for accurate quality assessment metrics to evaluate the\nperceptual quality of T2V-generated videos and optimize video generation\nmodels. However, assessing the quality of text-to-video outputs remain\nchallenging due to the presence of highly complex distortions, such as\nunnatural actions and phenomena that defy human cognition. To address these\nchallenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset\nfor text-to-video quality evaluation, comprising 148 textual prompts and 1,783\nvideos generated by 13 T2V models. To ensure a comprehensive evaluation, we\nscored each video on four dimensions in the subjective experiment, which are\noverall impression, text-video consistency, realness, and technical quality.\nBased on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for\nT2V quality evaluation. T2VEval assesses videos across three branches:\ntext-video consistency, realness, and technical quality. Using an\nattention-based fusion module, T2VEval effectively integrates features from\neach branch and predicts scores with the aid of a large language model.\nAdditionally, we implemented a progressive training strategy, enabling each\nbranch to learn targeted knowledge while maintaining synergy with the others.\nExperimental results demonstrate that T2VEval achieves state-of-the-art\nperformance across multiple metrics.\n", "link": "http://arxiv.org/abs/2501.08545v3", "date": "2025-02-17", "relevancy": 2.3144, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6142}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5962}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2VEval%3A%20T2V-generated%20Videos%20Benchmark%20Dataset%20and%20Objective%20Evaluation%0A%20%20Method&body=Title%3A%20T2VEval%3A%20T2V-generated%20Videos%20Benchmark%20Dataset%20and%20Objective%20Evaluation%0A%20%20Method%0AAuthor%3A%20Zelu%20Qi%20and%20Ping%20Shi%20and%20Shuqi%20Wang%20and%20Zhaoyang%20Zhang%20and%20Fei%20Zhao%20and%20Zefeng%20Ying%20and%20Da%20Pan%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20technology%2C%20as%20demonstrated%20by%20models%0Asuch%20as%20Runway%20Gen-3%2C%20Pika%2C%20Sora%2C%20and%20Kling%2C%20have%20significantly%20broadened%20the%0Aapplicability%20and%20popularity%20of%20the%20technology.%20This%20progress%20has%20created%20a%0Agrowing%20demand%20for%20accurate%20quality%20assessment%20metrics%20to%20evaluate%20the%0Aperceptual%20quality%20of%20T2V-generated%20videos%20and%20optimize%20video%20generation%0Amodels.%20However%2C%20assessing%20the%20quality%20of%20text-to-video%20outputs%20remain%0Achallenging%20due%20to%20the%20presence%20of%20highly%20complex%20distortions%2C%20such%20as%0Aunnatural%20actions%20and%20phenomena%20that%20defy%20human%20cognition.%20To%20address%20these%0Achallenges%2C%20we%20constructed%20T2VEval-Bench%2C%20a%20multi-dimensional%20benchmark%20dataset%0Afor%20text-to-video%20quality%20evaluation%2C%20comprising%20148%20textual%20prompts%20and%201%2C783%0Avideos%20generated%20by%2013%20T2V%20models.%20To%20ensure%20a%20comprehensive%20evaluation%2C%20we%0Ascored%20each%20video%20on%20four%20dimensions%20in%20the%20subjective%20experiment%2C%20which%20are%0Aoverall%20impression%2C%20text-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%0ABased%20on%20T2VEval-Bench%2C%20we%20developed%20T2VEval%2C%20a%20multi-branch%20fusion%20scheme%20for%0AT2V%20quality%20evaluation.%20T2VEval%20assesses%20videos%20across%20three%20branches%3A%0Atext-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%20Using%20an%0Aattention-based%20fusion%20module%2C%20T2VEval%20effectively%20integrates%20features%20from%0Aeach%20branch%20and%20predicts%20scores%20with%20the%20aid%20of%20a%20large%20language%20model.%0AAdditionally%2C%20we%20implemented%20a%20progressive%20training%20strategy%2C%20enabling%20each%0Abranch%20to%20learn%20targeted%20knowledge%20while%20maintaining%20synergy%20with%20the%20others.%0AExperimental%20results%20demonstrate%20that%20T2VEval%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08545v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2VEval%253A%2520T2V-generated%2520Videos%2520Benchmark%2520Dataset%2520and%2520Objective%2520Evaluation%250A%2520%2520Method%26entry.906535625%3DZelu%2520Qi%2520and%2520Ping%2520Shi%2520and%2520Shuqi%2520Wang%2520and%2520Zhaoyang%2520Zhang%2520and%2520Fei%2520Zhao%2520and%2520Zefeng%2520Ying%2520and%2520Da%2520Pan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-video%2520%2528T2V%2529%2520technology%252C%2520as%2520demonstrated%2520by%2520models%250Asuch%2520as%2520Runway%2520Gen-3%252C%2520Pika%252C%2520Sora%252C%2520and%2520Kling%252C%2520have%2520significantly%2520broadened%2520the%250Aapplicability%2520and%2520popularity%2520of%2520the%2520technology.%2520This%2520progress%2520has%2520created%2520a%250Agrowing%2520demand%2520for%2520accurate%2520quality%2520assessment%2520metrics%2520to%2520evaluate%2520the%250Aperceptual%2520quality%2520of%2520T2V-generated%2520videos%2520and%2520optimize%2520video%2520generation%250Amodels.%2520However%252C%2520assessing%2520the%2520quality%2520of%2520text-to-video%2520outputs%2520remain%250Achallenging%2520due%2520to%2520the%2520presence%2520of%2520highly%2520complex%2520distortions%252C%2520such%2520as%250Aunnatural%2520actions%2520and%2520phenomena%2520that%2520defy%2520human%2520cognition.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520constructed%2520T2VEval-Bench%252C%2520a%2520multi-dimensional%2520benchmark%2520dataset%250Afor%2520text-to-video%2520quality%2520evaluation%252C%2520comprising%2520148%2520textual%2520prompts%2520and%25201%252C783%250Avideos%2520generated%2520by%252013%2520T2V%2520models.%2520To%2520ensure%2520a%2520comprehensive%2520evaluation%252C%2520we%250Ascored%2520each%2520video%2520on%2520four%2520dimensions%2520in%2520the%2520subjective%2520experiment%252C%2520which%2520are%250Aoverall%2520impression%252C%2520text-video%2520consistency%252C%2520realness%252C%2520and%2520technical%2520quality.%250ABased%2520on%2520T2VEval-Bench%252C%2520we%2520developed%2520T2VEval%252C%2520a%2520multi-branch%2520fusion%2520scheme%2520for%250AT2V%2520quality%2520evaluation.%2520T2VEval%2520assesses%2520videos%2520across%2520three%2520branches%253A%250Atext-video%2520consistency%252C%2520realness%252C%2520and%2520technical%2520quality.%2520Using%2520an%250Aattention-based%2520fusion%2520module%252C%2520T2VEval%2520effectively%2520integrates%2520features%2520from%250Aeach%2520branch%2520and%2520predicts%2520scores%2520with%2520the%2520aid%2520of%2520a%2520large%2520language%2520model.%250AAdditionally%252C%2520we%2520implemented%2520a%2520progressive%2520training%2520strategy%252C%2520enabling%2520each%250Abranch%2520to%2520learn%2520targeted%2520knowledge%2520while%2520maintaining%2520synergy%2520with%2520the%2520others.%250AExperimental%2520results%2520demonstrate%2520that%2520T2VEval%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520multiple%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08545v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2VEval%3A%20T2V-generated%20Videos%20Benchmark%20Dataset%20and%20Objective%20Evaluation%0A%20%20Method&entry.906535625=Zelu%20Qi%20and%20Ping%20Shi%20and%20Shuqi%20Wang%20and%20Zhaoyang%20Zhang%20and%20Fei%20Zhao%20and%20Zefeng%20Ying%20and%20Da%20Pan&entry.1292438233=%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20technology%2C%20as%20demonstrated%20by%20models%0Asuch%20as%20Runway%20Gen-3%2C%20Pika%2C%20Sora%2C%20and%20Kling%2C%20have%20significantly%20broadened%20the%0Aapplicability%20and%20popularity%20of%20the%20technology.%20This%20progress%20has%20created%20a%0Agrowing%20demand%20for%20accurate%20quality%20assessment%20metrics%20to%20evaluate%20the%0Aperceptual%20quality%20of%20T2V-generated%20videos%20and%20optimize%20video%20generation%0Amodels.%20However%2C%20assessing%20the%20quality%20of%20text-to-video%20outputs%20remain%0Achallenging%20due%20to%20the%20presence%20of%20highly%20complex%20distortions%2C%20such%20as%0Aunnatural%20actions%20and%20phenomena%20that%20defy%20human%20cognition.%20To%20address%20these%0Achallenges%2C%20we%20constructed%20T2VEval-Bench%2C%20a%20multi-dimensional%20benchmark%20dataset%0Afor%20text-to-video%20quality%20evaluation%2C%20comprising%20148%20textual%20prompts%20and%201%2C783%0Avideos%20generated%20by%2013%20T2V%20models.%20To%20ensure%20a%20comprehensive%20evaluation%2C%20we%0Ascored%20each%20video%20on%20four%20dimensions%20in%20the%20subjective%20experiment%2C%20which%20are%0Aoverall%20impression%2C%20text-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%0ABased%20on%20T2VEval-Bench%2C%20we%20developed%20T2VEval%2C%20a%20multi-branch%20fusion%20scheme%20for%0AT2V%20quality%20evaluation.%20T2VEval%20assesses%20videos%20across%20three%20branches%3A%0Atext-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%20Using%20an%0Aattention-based%20fusion%20module%2C%20T2VEval%20effectively%20integrates%20features%20from%0Aeach%20branch%20and%20predicts%20scores%20with%20the%20aid%20of%20a%20large%20language%20model.%0AAdditionally%2C%20we%20implemented%20a%20progressive%20training%20strategy%2C%20enabling%20each%0Abranch%20to%20learn%20targeted%20knowledge%20while%20maintaining%20synergy%20with%20the%20others.%0AExperimental%20results%20demonstrate%20that%20T2VEval%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08545v3&entry.124074799=Read"},
{"title": "Robust 6DoF Pose Tracking Considering Contour and Interior\n  Correspondence Uncertainty for AR Assembly Guidance", "author": "Jixiang Chen and Jing Chen and Kai Liu and Haochen Chang and Shanfeng Fu and Jian Yang", "abstract": "  Augmented reality assembly guidance is essential for intelligent\nmanufacturing and medical applications, requiring continuous measurement of the\n6DoF poses of manipulated objects. Although current tracking methods have made\nsignificant advancements in accuracy and efficiency, they still face challenges\nin robustness when dealing with cluttered backgrounds, rotationally symmetric\nobjects, and noisy sequences. In this paper, we first propose a robust\ncontour-based pose tracking method that addresses error-prone contour\ncorrespondences and improves noise tolerance. It utilizes a fan-shaped search\nstrategy to refine correspondences and models local contour shape and noise\nuncertainty as mixed probability distribution, resulting in a highly robust\ncontour energy function. Secondly, we introduce a CPU-only strategy to better\ntrack rotationally symmetric objects and assist the contour-based method in\novercoming local minima by exploring sparse interior correspondences. This is\nachieved by pre-sampling interior points from sparse viewpoint templates\noffline and using the DIS optical flow algorithm to compute their\ncorrespondences during tracking. Finally, we formulate a unified energy\nfunction to fuse contour and interior information, which is solvable using a\nre-weighted least squares algorithm. Experiments on public datasets and real\nscenarios demonstrate that our method significantly outperforms\nstate-of-the-art monocular tracking methods and can achieve more than 100 FPS\nusing only a CPU.\n", "link": "http://arxiv.org/abs/2502.11971v1", "date": "2025-02-17", "relevancy": 2.3004, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.58}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5765}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%206DoF%20Pose%20Tracking%20Considering%20Contour%20and%20Interior%0A%20%20Correspondence%20Uncertainty%20for%20AR%20Assembly%20Guidance&body=Title%3A%20Robust%206DoF%20Pose%20Tracking%20Considering%20Contour%20and%20Interior%0A%20%20Correspondence%20Uncertainty%20for%20AR%20Assembly%20Guidance%0AAuthor%3A%20Jixiang%20Chen%20and%20Jing%20Chen%20and%20Kai%20Liu%20and%20Haochen%20Chang%20and%20Shanfeng%20Fu%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Augmented%20reality%20assembly%20guidance%20is%20essential%20for%20intelligent%0Amanufacturing%20and%20medical%20applications%2C%20requiring%20continuous%20measurement%20of%20the%0A6DoF%20poses%20of%20manipulated%20objects.%20Although%20current%20tracking%20methods%20have%20made%0Asignificant%20advancements%20in%20accuracy%20and%20efficiency%2C%20they%20still%20face%20challenges%0Ain%20robustness%20when%20dealing%20with%20cluttered%20backgrounds%2C%20rotationally%20symmetric%0Aobjects%2C%20and%20noisy%20sequences.%20In%20this%20paper%2C%20we%20first%20propose%20a%20robust%0Acontour-based%20pose%20tracking%20method%20that%20addresses%20error-prone%20contour%0Acorrespondences%20and%20improves%20noise%20tolerance.%20It%20utilizes%20a%20fan-shaped%20search%0Astrategy%20to%20refine%20correspondences%20and%20models%20local%20contour%20shape%20and%20noise%0Auncertainty%20as%20mixed%20probability%20distribution%2C%20resulting%20in%20a%20highly%20robust%0Acontour%20energy%20function.%20Secondly%2C%20we%20introduce%20a%20CPU-only%20strategy%20to%20better%0Atrack%20rotationally%20symmetric%20objects%20and%20assist%20the%20contour-based%20method%20in%0Aovercoming%20local%20minima%20by%20exploring%20sparse%20interior%20correspondences.%20This%20is%0Aachieved%20by%20pre-sampling%20interior%20points%20from%20sparse%20viewpoint%20templates%0Aoffline%20and%20using%20the%20DIS%20optical%20flow%20algorithm%20to%20compute%20their%0Acorrespondences%20during%20tracking.%20Finally%2C%20we%20formulate%20a%20unified%20energy%0Afunction%20to%20fuse%20contour%20and%20interior%20information%2C%20which%20is%20solvable%20using%20a%0Are-weighted%20least%20squares%20algorithm.%20Experiments%20on%20public%20datasets%20and%20real%0Ascenarios%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Astate-of-the-art%20monocular%20tracking%20methods%20and%20can%20achieve%20more%20than%20100%20FPS%0Ausing%20only%20a%20CPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%25206DoF%2520Pose%2520Tracking%2520Considering%2520Contour%2520and%2520Interior%250A%2520%2520Correspondence%2520Uncertainty%2520for%2520AR%2520Assembly%2520Guidance%26entry.906535625%3DJixiang%2520Chen%2520and%2520Jing%2520Chen%2520and%2520Kai%2520Liu%2520and%2520Haochen%2520Chang%2520and%2520Shanfeng%2520Fu%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Augmented%2520reality%2520assembly%2520guidance%2520is%2520essential%2520for%2520intelligent%250Amanufacturing%2520and%2520medical%2520applications%252C%2520requiring%2520continuous%2520measurement%2520of%2520the%250A6DoF%2520poses%2520of%2520manipulated%2520objects.%2520Although%2520current%2520tracking%2520methods%2520have%2520made%250Asignificant%2520advancements%2520in%2520accuracy%2520and%2520efficiency%252C%2520they%2520still%2520face%2520challenges%250Ain%2520robustness%2520when%2520dealing%2520with%2520cluttered%2520backgrounds%252C%2520rotationally%2520symmetric%250Aobjects%252C%2520and%2520noisy%2520sequences.%2520In%2520this%2520paper%252C%2520we%2520first%2520propose%2520a%2520robust%250Acontour-based%2520pose%2520tracking%2520method%2520that%2520addresses%2520error-prone%2520contour%250Acorrespondences%2520and%2520improves%2520noise%2520tolerance.%2520It%2520utilizes%2520a%2520fan-shaped%2520search%250Astrategy%2520to%2520refine%2520correspondences%2520and%2520models%2520local%2520contour%2520shape%2520and%2520noise%250Auncertainty%2520as%2520mixed%2520probability%2520distribution%252C%2520resulting%2520in%2520a%2520highly%2520robust%250Acontour%2520energy%2520function.%2520Secondly%252C%2520we%2520introduce%2520a%2520CPU-only%2520strategy%2520to%2520better%250Atrack%2520rotationally%2520symmetric%2520objects%2520and%2520assist%2520the%2520contour-based%2520method%2520in%250Aovercoming%2520local%2520minima%2520by%2520exploring%2520sparse%2520interior%2520correspondences.%2520This%2520is%250Aachieved%2520by%2520pre-sampling%2520interior%2520points%2520from%2520sparse%2520viewpoint%2520templates%250Aoffline%2520and%2520using%2520the%2520DIS%2520optical%2520flow%2520algorithm%2520to%2520compute%2520their%250Acorrespondences%2520during%2520tracking.%2520Finally%252C%2520we%2520formulate%2520a%2520unified%2520energy%250Afunction%2520to%2520fuse%2520contour%2520and%2520interior%2520information%252C%2520which%2520is%2520solvable%2520using%2520a%250Are-weighted%2520least%2520squares%2520algorithm.%2520Experiments%2520on%2520public%2520datasets%2520and%2520real%250Ascenarios%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%250Astate-of-the-art%2520monocular%2520tracking%2520methods%2520and%2520can%2520achieve%2520more%2520than%2520100%2520FPS%250Ausing%2520only%2520a%2520CPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%206DoF%20Pose%20Tracking%20Considering%20Contour%20and%20Interior%0A%20%20Correspondence%20Uncertainty%20for%20AR%20Assembly%20Guidance&entry.906535625=Jixiang%20Chen%20and%20Jing%20Chen%20and%20Kai%20Liu%20and%20Haochen%20Chang%20and%20Shanfeng%20Fu%20and%20Jian%20Yang&entry.1292438233=%20%20Augmented%20reality%20assembly%20guidance%20is%20essential%20for%20intelligent%0Amanufacturing%20and%20medical%20applications%2C%20requiring%20continuous%20measurement%20of%20the%0A6DoF%20poses%20of%20manipulated%20objects.%20Although%20current%20tracking%20methods%20have%20made%0Asignificant%20advancements%20in%20accuracy%20and%20efficiency%2C%20they%20still%20face%20challenges%0Ain%20robustness%20when%20dealing%20with%20cluttered%20backgrounds%2C%20rotationally%20symmetric%0Aobjects%2C%20and%20noisy%20sequences.%20In%20this%20paper%2C%20we%20first%20propose%20a%20robust%0Acontour-based%20pose%20tracking%20method%20that%20addresses%20error-prone%20contour%0Acorrespondences%20and%20improves%20noise%20tolerance.%20It%20utilizes%20a%20fan-shaped%20search%0Astrategy%20to%20refine%20correspondences%20and%20models%20local%20contour%20shape%20and%20noise%0Auncertainty%20as%20mixed%20probability%20distribution%2C%20resulting%20in%20a%20highly%20robust%0Acontour%20energy%20function.%20Secondly%2C%20we%20introduce%20a%20CPU-only%20strategy%20to%20better%0Atrack%20rotationally%20symmetric%20objects%20and%20assist%20the%20contour-based%20method%20in%0Aovercoming%20local%20minima%20by%20exploring%20sparse%20interior%20correspondences.%20This%20is%0Aachieved%20by%20pre-sampling%20interior%20points%20from%20sparse%20viewpoint%20templates%0Aoffline%20and%20using%20the%20DIS%20optical%20flow%20algorithm%20to%20compute%20their%0Acorrespondences%20during%20tracking.%20Finally%2C%20we%20formulate%20a%20unified%20energy%0Afunction%20to%20fuse%20contour%20and%20interior%20information%2C%20which%20is%20solvable%20using%20a%0Are-weighted%20least%20squares%20algorithm.%20Experiments%20on%20public%20datasets%20and%20real%0Ascenarios%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Astate-of-the-art%20monocular%20tracking%20methods%20and%20can%20achieve%20more%20than%20100%20FPS%0Ausing%20only%20a%20CPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11971v1&entry.124074799=Read"},
{"title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation", "author": "Zhihang Yuan and Siyuan Wang and Rui Xie and Hanling Zhang and Tongcheng Fang and Yuzhang Shang and Shengen Yan and Guohao Dai and Yu Wang", "abstract": "  In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a\ntraining-free paradigm that can make use of adaptive temporal compression in\nlatent space. While existing video generative models apply fixed compression\nrates via pretrained VAE, we observe that real-world video content exhibits\nsubstantial temporal non-uniformity, with high-motion segments containing more\ninformation than static scenes. Based on this insight, DLFR-VAE dynamically\nadjusts the latent frame rate according to the content complexity.\nSpecifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent\nFrame Rate Scheduler that partitions videos into temporal chunks and adaptively\ndetermines optimal frame rates based on information-theoretic content\ncomplexity, and (2) A training-free adaptation mechanism that transforms\npretrained VAE architectures into a dynamic VAE that can process features with\nvariable frame rates. Our simple but effective DLFR-VAE can function as a\nplug-and-play module, seamlessly integrating with existing video generation\nmodels and accelerating the video generation process.\n", "link": "http://arxiv.org/abs/2502.11897v1", "date": "2025-02-17", "relevancy": 2.2892, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.578}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5726}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DLFR-VAE%3A%20Dynamic%20Latent%20Frame%20Rate%20VAE%20for%20Video%20Generation&body=Title%3A%20DLFR-VAE%3A%20Dynamic%20Latent%20Frame%20Rate%20VAE%20for%20Video%20Generation%0AAuthor%3A%20Zhihang%20Yuan%20and%20Siyuan%20Wang%20and%20Rui%20Xie%20and%20Hanling%20Zhang%20and%20Tongcheng%20Fang%20and%20Yuzhang%20Shang%20and%20Shengen%20Yan%20and%20Guohao%20Dai%20and%20Yu%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20the%20Dynamic%20Latent%20Frame%20Rate%20VAE%20%28DLFR-VAE%29%2C%20a%0Atraining-free%20paradigm%20that%20can%20make%20use%20of%20adaptive%20temporal%20compression%20in%0Alatent%20space.%20While%20existing%20video%20generative%20models%20apply%20fixed%20compression%0Arates%20via%20pretrained%20VAE%2C%20we%20observe%20that%20real-world%20video%20content%20exhibits%0Asubstantial%20temporal%20non-uniformity%2C%20with%20high-motion%20segments%20containing%20more%0Ainformation%20than%20static%20scenes.%20Based%20on%20this%20insight%2C%20DLFR-VAE%20dynamically%0Aadjusts%20the%20latent%20frame%20rate%20according%20to%20the%20content%20complexity.%0ASpecifically%2C%20DLFR-VAE%20comprises%20two%20core%20innovations%3A%20%281%29%20A%20Dynamic%20Latent%0AFrame%20Rate%20Scheduler%20that%20partitions%20videos%20into%20temporal%20chunks%20and%20adaptively%0Adetermines%20optimal%20frame%20rates%20based%20on%20information-theoretic%20content%0Acomplexity%2C%20and%20%282%29%20A%20training-free%20adaptation%20mechanism%20that%20transforms%0Apretrained%20VAE%20architectures%20into%20a%20dynamic%20VAE%20that%20can%20process%20features%20with%0Avariable%20frame%20rates.%20Our%20simple%20but%20effective%20DLFR-VAE%20can%20function%20as%20a%0Aplug-and-play%20module%2C%20seamlessly%20integrating%20with%20existing%20video%20generation%0Amodels%20and%20accelerating%20the%20video%20generation%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDLFR-VAE%253A%2520Dynamic%2520Latent%2520Frame%2520Rate%2520VAE%2520for%2520Video%2520Generation%26entry.906535625%3DZhihang%2520Yuan%2520and%2520Siyuan%2520Wang%2520and%2520Rui%2520Xie%2520and%2520Hanling%2520Zhang%2520and%2520Tongcheng%2520Fang%2520and%2520Yuzhang%2520Shang%2520and%2520Shengen%2520Yan%2520and%2520Guohao%2520Dai%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Dynamic%2520Latent%2520Frame%2520Rate%2520VAE%2520%2528DLFR-VAE%2529%252C%2520a%250Atraining-free%2520paradigm%2520that%2520can%2520make%2520use%2520of%2520adaptive%2520temporal%2520compression%2520in%250Alatent%2520space.%2520While%2520existing%2520video%2520generative%2520models%2520apply%2520fixed%2520compression%250Arates%2520via%2520pretrained%2520VAE%252C%2520we%2520observe%2520that%2520real-world%2520video%2520content%2520exhibits%250Asubstantial%2520temporal%2520non-uniformity%252C%2520with%2520high-motion%2520segments%2520containing%2520more%250Ainformation%2520than%2520static%2520scenes.%2520Based%2520on%2520this%2520insight%252C%2520DLFR-VAE%2520dynamically%250Aadjusts%2520the%2520latent%2520frame%2520rate%2520according%2520to%2520the%2520content%2520complexity.%250ASpecifically%252C%2520DLFR-VAE%2520comprises%2520two%2520core%2520innovations%253A%2520%25281%2529%2520A%2520Dynamic%2520Latent%250AFrame%2520Rate%2520Scheduler%2520that%2520partitions%2520videos%2520into%2520temporal%2520chunks%2520and%2520adaptively%250Adetermines%2520optimal%2520frame%2520rates%2520based%2520on%2520information-theoretic%2520content%250Acomplexity%252C%2520and%2520%25282%2529%2520A%2520training-free%2520adaptation%2520mechanism%2520that%2520transforms%250Apretrained%2520VAE%2520architectures%2520into%2520a%2520dynamic%2520VAE%2520that%2520can%2520process%2520features%2520with%250Avariable%2520frame%2520rates.%2520Our%2520simple%2520but%2520effective%2520DLFR-VAE%2520can%2520function%2520as%2520a%250Aplug-and-play%2520module%252C%2520seamlessly%2520integrating%2520with%2520existing%2520video%2520generation%250Amodels%2520and%2520accelerating%2520the%2520video%2520generation%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DLFR-VAE%3A%20Dynamic%20Latent%20Frame%20Rate%20VAE%20for%20Video%20Generation&entry.906535625=Zhihang%20Yuan%20and%20Siyuan%20Wang%20and%20Rui%20Xie%20and%20Hanling%20Zhang%20and%20Tongcheng%20Fang%20and%20Yuzhang%20Shang%20and%20Shengen%20Yan%20and%20Guohao%20Dai%20and%20Yu%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20the%20Dynamic%20Latent%20Frame%20Rate%20VAE%20%28DLFR-VAE%29%2C%20a%0Atraining-free%20paradigm%20that%20can%20make%20use%20of%20adaptive%20temporal%20compression%20in%0Alatent%20space.%20While%20existing%20video%20generative%20models%20apply%20fixed%20compression%0Arates%20via%20pretrained%20VAE%2C%20we%20observe%20that%20real-world%20video%20content%20exhibits%0Asubstantial%20temporal%20non-uniformity%2C%20with%20high-motion%20segments%20containing%20more%0Ainformation%20than%20static%20scenes.%20Based%20on%20this%20insight%2C%20DLFR-VAE%20dynamically%0Aadjusts%20the%20latent%20frame%20rate%20according%20to%20the%20content%20complexity.%0ASpecifically%2C%20DLFR-VAE%20comprises%20two%20core%20innovations%3A%20%281%29%20A%20Dynamic%20Latent%0AFrame%20Rate%20Scheduler%20that%20partitions%20videos%20into%20temporal%20chunks%20and%20adaptively%0Adetermines%20optimal%20frame%20rates%20based%20on%20information-theoretic%20content%0Acomplexity%2C%20and%20%282%29%20A%20training-free%20adaptation%20mechanism%20that%20transforms%0Apretrained%20VAE%20architectures%20into%20a%20dynamic%20VAE%20that%20can%20process%20features%20with%0Avariable%20frame%20rates.%20Our%20simple%20but%20effective%20DLFR-VAE%20can%20function%20as%20a%0Aplug-and-play%20module%2C%20seamlessly%20integrating%20with%20existing%20video%20generation%0Amodels%20and%20accelerating%20the%20video%20generation%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11897v1&entry.124074799=Read"},
{"title": "Intuitive physics understanding emerges from self-supervised pretraining\n  on natural videos", "author": "Quentin Garrido and Nicolas Ballas and Mahmoud Assran and Adrien Bardes and Laurent Najman and Michael Rabbat and Emmanuel Dupoux and Yann LeCun", "abstract": "  We investigate the emergence of intuitive physics understanding in\ngeneral-purpose deep neural network models trained to predict masked regions in\nnatural videos. Leveraging the violation-of-expectation framework, we find that\nvideo prediction models trained to predict outcomes in a learned representation\nspace demonstrate an understanding of various intuitive physics properties,\nsuch as object permanence and shape consistency. In contrast, video prediction\nin pixel space and multimodal large language models, which reason through text,\nachieve performance closer to chance. Our comparisons of these architectures\nreveal that jointly learning an abstract representation space while predicting\nmissing parts of sensory input, akin to predictive coding, is sufficient to\nacquire an understanding of intuitive physics, and that even models trained on\none week of unique video achieve above chance performance. This challenges the\nidea that core knowledge -- a set of innate systems to help understand the\nworld -- needs to be hardwired to develop an understanding of intuitive\nphysics.\n", "link": "http://arxiv.org/abs/2502.11831v1", "date": "2025-02-17", "relevancy": 2.2754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6007}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intuitive%20physics%20understanding%20emerges%20from%20self-supervised%20pretraining%0A%20%20on%20natural%20videos&body=Title%3A%20Intuitive%20physics%20understanding%20emerges%20from%20self-supervised%20pretraining%0A%20%20on%20natural%20videos%0AAuthor%3A%20Quentin%20Garrido%20and%20Nicolas%20Ballas%20and%20Mahmoud%20Assran%20and%20Adrien%20Bardes%20and%20Laurent%20Najman%20and%20Michael%20Rabbat%20and%20Emmanuel%20Dupoux%20and%20Yann%20LeCun%0AAbstract%3A%20%20%20We%20investigate%20the%20emergence%20of%20intuitive%20physics%20understanding%20in%0Ageneral-purpose%20deep%20neural%20network%20models%20trained%20to%20predict%20masked%20regions%20in%0Anatural%20videos.%20Leveraging%20the%20violation-of-expectation%20framework%2C%20we%20find%20that%0Avideo%20prediction%20models%20trained%20to%20predict%20outcomes%20in%20a%20learned%20representation%0Aspace%20demonstrate%20an%20understanding%20of%20various%20intuitive%20physics%20properties%2C%0Asuch%20as%20object%20permanence%20and%20shape%20consistency.%20In%20contrast%2C%20video%20prediction%0Ain%20pixel%20space%20and%20multimodal%20large%20language%20models%2C%20which%20reason%20through%20text%2C%0Aachieve%20performance%20closer%20to%20chance.%20Our%20comparisons%20of%20these%20architectures%0Areveal%20that%20jointly%20learning%20an%20abstract%20representation%20space%20while%20predicting%0Amissing%20parts%20of%20sensory%20input%2C%20akin%20to%20predictive%20coding%2C%20is%20sufficient%20to%0Aacquire%20an%20understanding%20of%20intuitive%20physics%2C%20and%20that%20even%20models%20trained%20on%0Aone%20week%20of%20unique%20video%20achieve%20above%20chance%20performance.%20This%20challenges%20the%0Aidea%20that%20core%20knowledge%20--%20a%20set%20of%20innate%20systems%20to%20help%20understand%20the%0Aworld%20--%20needs%20to%20be%20hardwired%20to%20develop%20an%20understanding%20of%20intuitive%0Aphysics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntuitive%2520physics%2520understanding%2520emerges%2520from%2520self-supervised%2520pretraining%250A%2520%2520on%2520natural%2520videos%26entry.906535625%3DQuentin%2520Garrido%2520and%2520Nicolas%2520Ballas%2520and%2520Mahmoud%2520Assran%2520and%2520Adrien%2520Bardes%2520and%2520Laurent%2520Najman%2520and%2520Michael%2520Rabbat%2520and%2520Emmanuel%2520Dupoux%2520and%2520Yann%2520LeCun%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520emergence%2520of%2520intuitive%2520physics%2520understanding%2520in%250Ageneral-purpose%2520deep%2520neural%2520network%2520models%2520trained%2520to%2520predict%2520masked%2520regions%2520in%250Anatural%2520videos.%2520Leveraging%2520the%2520violation-of-expectation%2520framework%252C%2520we%2520find%2520that%250Avideo%2520prediction%2520models%2520trained%2520to%2520predict%2520outcomes%2520in%2520a%2520learned%2520representation%250Aspace%2520demonstrate%2520an%2520understanding%2520of%2520various%2520intuitive%2520physics%2520properties%252C%250Asuch%2520as%2520object%2520permanence%2520and%2520shape%2520consistency.%2520In%2520contrast%252C%2520video%2520prediction%250Ain%2520pixel%2520space%2520and%2520multimodal%2520large%2520language%2520models%252C%2520which%2520reason%2520through%2520text%252C%250Aachieve%2520performance%2520closer%2520to%2520chance.%2520Our%2520comparisons%2520of%2520these%2520architectures%250Areveal%2520that%2520jointly%2520learning%2520an%2520abstract%2520representation%2520space%2520while%2520predicting%250Amissing%2520parts%2520of%2520sensory%2520input%252C%2520akin%2520to%2520predictive%2520coding%252C%2520is%2520sufficient%2520to%250Aacquire%2520an%2520understanding%2520of%2520intuitive%2520physics%252C%2520and%2520that%2520even%2520models%2520trained%2520on%250Aone%2520week%2520of%2520unique%2520video%2520achieve%2520above%2520chance%2520performance.%2520This%2520challenges%2520the%250Aidea%2520that%2520core%2520knowledge%2520--%2520a%2520set%2520of%2520innate%2520systems%2520to%2520help%2520understand%2520the%250Aworld%2520--%2520needs%2520to%2520be%2520hardwired%2520to%2520develop%2520an%2520understanding%2520of%2520intuitive%250Aphysics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intuitive%20physics%20understanding%20emerges%20from%20self-supervised%20pretraining%0A%20%20on%20natural%20videos&entry.906535625=Quentin%20Garrido%20and%20Nicolas%20Ballas%20and%20Mahmoud%20Assran%20and%20Adrien%20Bardes%20and%20Laurent%20Najman%20and%20Michael%20Rabbat%20and%20Emmanuel%20Dupoux%20and%20Yann%20LeCun&entry.1292438233=%20%20We%20investigate%20the%20emergence%20of%20intuitive%20physics%20understanding%20in%0Ageneral-purpose%20deep%20neural%20network%20models%20trained%20to%20predict%20masked%20regions%20in%0Anatural%20videos.%20Leveraging%20the%20violation-of-expectation%20framework%2C%20we%20find%20that%0Avideo%20prediction%20models%20trained%20to%20predict%20outcomes%20in%20a%20learned%20representation%0Aspace%20demonstrate%20an%20understanding%20of%20various%20intuitive%20physics%20properties%2C%0Asuch%20as%20object%20permanence%20and%20shape%20consistency.%20In%20contrast%2C%20video%20prediction%0Ain%20pixel%20space%20and%20multimodal%20large%20language%20models%2C%20which%20reason%20through%20text%2C%0Aachieve%20performance%20closer%20to%20chance.%20Our%20comparisons%20of%20these%20architectures%0Areveal%20that%20jointly%20learning%20an%20abstract%20representation%20space%20while%20predicting%0Amissing%20parts%20of%20sensory%20input%2C%20akin%20to%20predictive%20coding%2C%20is%20sufficient%20to%0Aacquire%20an%20understanding%20of%20intuitive%20physics%2C%20and%20that%20even%20models%20trained%20on%0Aone%20week%20of%20unique%20video%20achieve%20above%20chance%20performance.%20This%20challenges%20the%0Aidea%20that%20core%20knowledge%20--%20a%20set%20of%20innate%20systems%20to%20help%20understand%20the%0Aworld%20--%20needs%20to%20be%20hardwired%20to%20develop%20an%20understanding%20of%20intuitive%0Aphysics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11831v1&entry.124074799=Read"},
{"title": "Kernel-Based Distributed Q-Learning: A Scalable Reinforcement Learning\n  Approach for Dynamic Treatment Regimes", "author": "Di Wang and Yao Wang and Shao-Bo Lin", "abstract": "  In recent years, large amounts of electronic health records (EHRs) concerning\nchronic diseases have been collected to facilitate medical diagnosis. Modeling\nthe dynamic properties of EHRs related to chronic diseases can be efficiently\ndone using dynamic treatment regimes (DTRs). While reinforcement learning (RL)\nis a widely used method for creating DTRs, there is ongoing research in\ndeveloping RL algorithms that can effectively handle large amounts of data. In\nthis paper, we present a scalable kernel-based distributed Q-learning algorithm\nfor generating DTRs. We perform both theoretical assessments and numerical\nanalysis for the proposed approach. The results demonstrate that our algorithm\nsignificantly reduces the computational complexity associated with the\nstate-of-the-art deep reinforcement learning methods, while maintaining\ncomparable generalization performance in terms of accumulated rewards across\nstages, such as survival time or cumulative survival probability.\n", "link": "http://arxiv.org/abs/2302.10434v2", "date": "2025-02-17", "relevancy": 2.2734, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4671}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4497}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel-Based%20Distributed%20Q-Learning%3A%20A%20Scalable%20Reinforcement%20Learning%0A%20%20Approach%20for%20Dynamic%20Treatment%20Regimes&body=Title%3A%20Kernel-Based%20Distributed%20Q-Learning%3A%20A%20Scalable%20Reinforcement%20Learning%0A%20%20Approach%20for%20Dynamic%20Treatment%20Regimes%0AAuthor%3A%20Di%20Wang%20and%20Yao%20Wang%20and%20Shao-Bo%20Lin%0AAbstract%3A%20%20%20In%20recent%20years%2C%20large%20amounts%20of%20electronic%20health%20records%20%28EHRs%29%20concerning%0Achronic%20diseases%20have%20been%20collected%20to%20facilitate%20medical%20diagnosis.%20Modeling%0Athe%20dynamic%20properties%20of%20EHRs%20related%20to%20chronic%20diseases%20can%20be%20efficiently%0Adone%20using%20dynamic%20treatment%20regimes%20%28DTRs%29.%20While%20reinforcement%20learning%20%28RL%29%0Ais%20a%20widely%20used%20method%20for%20creating%20DTRs%2C%20there%20is%20ongoing%20research%20in%0Adeveloping%20RL%20algorithms%20that%20can%20effectively%20handle%20large%20amounts%20of%20data.%20In%0Athis%20paper%2C%20we%20present%20a%20scalable%20kernel-based%20distributed%20Q-learning%20algorithm%0Afor%20generating%20DTRs.%20We%20perform%20both%20theoretical%20assessments%20and%20numerical%0Aanalysis%20for%20the%20proposed%20approach.%20The%20results%20demonstrate%20that%20our%20algorithm%0Asignificantly%20reduces%20the%20computational%20complexity%20associated%20with%20the%0Astate-of-the-art%20deep%20reinforcement%20learning%20methods%2C%20while%20maintaining%0Acomparable%20generalization%20performance%20in%20terms%20of%20accumulated%20rewards%20across%0Astages%2C%20such%20as%20survival%20time%20or%20cumulative%20survival%20probability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.10434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel-Based%2520Distributed%2520Q-Learning%253A%2520A%2520Scalable%2520Reinforcement%2520Learning%250A%2520%2520Approach%2520for%2520Dynamic%2520Treatment%2520Regimes%26entry.906535625%3DDi%2520Wang%2520and%2520Yao%2520Wang%2520and%2520Shao-Bo%2520Lin%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520large%2520amounts%2520of%2520electronic%2520health%2520records%2520%2528EHRs%2529%2520concerning%250Achronic%2520diseases%2520have%2520been%2520collected%2520to%2520facilitate%2520medical%2520diagnosis.%2520Modeling%250Athe%2520dynamic%2520properties%2520of%2520EHRs%2520related%2520to%2520chronic%2520diseases%2520can%2520be%2520efficiently%250Adone%2520using%2520dynamic%2520treatment%2520regimes%2520%2528DTRs%2529.%2520While%2520reinforcement%2520learning%2520%2528RL%2529%250Ais%2520a%2520widely%2520used%2520method%2520for%2520creating%2520DTRs%252C%2520there%2520is%2520ongoing%2520research%2520in%250Adeveloping%2520RL%2520algorithms%2520that%2520can%2520effectively%2520handle%2520large%2520amounts%2520of%2520data.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520scalable%2520kernel-based%2520distributed%2520Q-learning%2520algorithm%250Afor%2520generating%2520DTRs.%2520We%2520perform%2520both%2520theoretical%2520assessments%2520and%2520numerical%250Aanalysis%2520for%2520the%2520proposed%2520approach.%2520The%2520results%2520demonstrate%2520that%2520our%2520algorithm%250Asignificantly%2520reduces%2520the%2520computational%2520complexity%2520associated%2520with%2520the%250Astate-of-the-art%2520deep%2520reinforcement%2520learning%2520methods%252C%2520while%2520maintaining%250Acomparable%2520generalization%2520performance%2520in%2520terms%2520of%2520accumulated%2520rewards%2520across%250Astages%252C%2520such%2520as%2520survival%2520time%2520or%2520cumulative%2520survival%2520probability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.10434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel-Based%20Distributed%20Q-Learning%3A%20A%20Scalable%20Reinforcement%20Learning%0A%20%20Approach%20for%20Dynamic%20Treatment%20Regimes&entry.906535625=Di%20Wang%20and%20Yao%20Wang%20and%20Shao-Bo%20Lin&entry.1292438233=%20%20In%20recent%20years%2C%20large%20amounts%20of%20electronic%20health%20records%20%28EHRs%29%20concerning%0Achronic%20diseases%20have%20been%20collected%20to%20facilitate%20medical%20diagnosis.%20Modeling%0Athe%20dynamic%20properties%20of%20EHRs%20related%20to%20chronic%20diseases%20can%20be%20efficiently%0Adone%20using%20dynamic%20treatment%20regimes%20%28DTRs%29.%20While%20reinforcement%20learning%20%28RL%29%0Ais%20a%20widely%20used%20method%20for%20creating%20DTRs%2C%20there%20is%20ongoing%20research%20in%0Adeveloping%20RL%20algorithms%20that%20can%20effectively%20handle%20large%20amounts%20of%20data.%20In%0Athis%20paper%2C%20we%20present%20a%20scalable%20kernel-based%20distributed%20Q-learning%20algorithm%0Afor%20generating%20DTRs.%20We%20perform%20both%20theoretical%20assessments%20and%20numerical%0Aanalysis%20for%20the%20proposed%20approach.%20The%20results%20demonstrate%20that%20our%20algorithm%0Asignificantly%20reduces%20the%20computational%20complexity%20associated%20with%20the%0Astate-of-the-art%20deep%20reinforcement%20learning%20methods%2C%20while%20maintaining%0Acomparable%20generalization%20performance%20in%20terms%20of%20accumulated%20rewards%20across%0Astages%2C%20such%20as%20survival%20time%20or%20cumulative%20survival%20probability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.10434v2&entry.124074799=Read"},
{"title": "Language Models Struggle to Achieve a Consistent Temporal Representation\n  of Facts", "author": "Hichem Ammar Khodja and Fr\u00e9d\u00e9ric B\u00e9chet and Quentin Brabant and Alexis Nasr and Gw\u00e9nol\u00e9 Lecorv\u00e9", "abstract": "  Language Models (LMs) have shown substantial improvements in handling factual\nknowledge, yet their capability to consistently represent temporal facts, which\nare valid only within specific timeframes, remains underexplored. To\ninvestigate this, we introduce TimeStress, a novel dataset comprising 521K\nstatements on 2003 of the most popular temporal facts in Wikidata. Each\nstatement contextualizes a fact with correct and incorrect dates across three\nprecisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to\ndiscern between correct and incorrect temporal statements based on their\nprobability of being generated. We assess 18 LMs across various architectures\nusing two metrics: the win rate, indicating how often correct dates outperform\nincorrect ones, and robustness, reflecting consistent performance across all\ndates. Our findings reveal that while some LMs achieve a win rate exceeding\n80\\%, robustness remains low, with the best model achieving only 6\\%.\nFurthermore, robust knowledge at one date precision does not reliably transfer\nto others, highlighting a significant generalization gap. These results\nunderscore the struggle of LMs to maintain a consistent temporal\nrepresentation, supporting their limitations as reliable sources of temporal\nknowledge. We provide all data and code for further research.\n", "link": "http://arxiv.org/abs/2502.01220v2", "date": "2025-02-17", "relevancy": 2.2729, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4637}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20Struggle%20to%20Achieve%20a%20Consistent%20Temporal%20Representation%0A%20%20of%20Facts&body=Title%3A%20Language%20Models%20Struggle%20to%20Achieve%20a%20Consistent%20Temporal%20Representation%0A%20%20of%20Facts%0AAuthor%3A%20Hichem%20Ammar%20Khodja%20and%20Fr%C3%A9d%C3%A9ric%20B%C3%A9chet%20and%20Quentin%20Brabant%20and%20Alexis%20Nasr%20and%20Gw%C3%A9nol%C3%A9%20Lecorv%C3%A9%0AAbstract%3A%20%20%20Language%20Models%20%28LMs%29%20have%20shown%20substantial%20improvements%20in%20handling%20factual%0Aknowledge%2C%20yet%20their%20capability%20to%20consistently%20represent%20temporal%20facts%2C%20which%0Aare%20valid%20only%20within%20specific%20timeframes%2C%20remains%20underexplored.%20To%0Ainvestigate%20this%2C%20we%20introduce%20TimeStress%2C%20a%20novel%20dataset%20comprising%20521K%0Astatements%20on%202003%20of%20the%20most%20popular%20temporal%20facts%20in%20Wikidata.%20Each%0Astatement%20contextualizes%20a%20fact%20with%20correct%20and%20incorrect%20dates%20across%20three%0Aprecisions%20%28Day%2C%20Month%2C%20Year%29.%20This%20setup%20allows%20us%20to%20evaluate%20LMs%27%20ability%20to%0Adiscern%20between%20correct%20and%20incorrect%20temporal%20statements%20based%20on%20their%0Aprobability%20of%20being%20generated.%20We%20assess%2018%20LMs%20across%20various%20architectures%0Ausing%20two%20metrics%3A%20the%20win%20rate%2C%20indicating%20how%20often%20correct%20dates%20outperform%0Aincorrect%20ones%2C%20and%20robustness%2C%20reflecting%20consistent%20performance%20across%20all%0Adates.%20Our%20findings%20reveal%20that%20while%20some%20LMs%20achieve%20a%20win%20rate%20exceeding%0A80%5C%25%2C%20robustness%20remains%20low%2C%20with%20the%20best%20model%20achieving%20only%206%5C%25.%0AFurthermore%2C%20robust%20knowledge%20at%20one%20date%20precision%20does%20not%20reliably%20transfer%0Ato%20others%2C%20highlighting%20a%20significant%20generalization%20gap.%20These%20results%0Aunderscore%20the%20struggle%20of%20LMs%20to%20maintain%20a%20consistent%20temporal%0Arepresentation%2C%20supporting%20their%20limitations%20as%20reliable%20sources%20of%20temporal%0Aknowledge.%20We%20provide%20all%20data%20and%20code%20for%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520Struggle%2520to%2520Achieve%2520a%2520Consistent%2520Temporal%2520Representation%250A%2520%2520of%2520Facts%26entry.906535625%3DHichem%2520Ammar%2520Khodja%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520B%25C3%25A9chet%2520and%2520Quentin%2520Brabant%2520and%2520Alexis%2520Nasr%2520and%2520Gw%25C3%25A9nol%25C3%25A9%2520Lecorv%25C3%25A9%26entry.1292438233%3D%2520%2520Language%2520Models%2520%2528LMs%2529%2520have%2520shown%2520substantial%2520improvements%2520in%2520handling%2520factual%250Aknowledge%252C%2520yet%2520their%2520capability%2520to%2520consistently%2520represent%2520temporal%2520facts%252C%2520which%250Aare%2520valid%2520only%2520within%2520specific%2520timeframes%252C%2520remains%2520underexplored.%2520To%250Ainvestigate%2520this%252C%2520we%2520introduce%2520TimeStress%252C%2520a%2520novel%2520dataset%2520comprising%2520521K%250Astatements%2520on%25202003%2520of%2520the%2520most%2520popular%2520temporal%2520facts%2520in%2520Wikidata.%2520Each%250Astatement%2520contextualizes%2520a%2520fact%2520with%2520correct%2520and%2520incorrect%2520dates%2520across%2520three%250Aprecisions%2520%2528Day%252C%2520Month%252C%2520Year%2529.%2520This%2520setup%2520allows%2520us%2520to%2520evaluate%2520LMs%2527%2520ability%2520to%250Adiscern%2520between%2520correct%2520and%2520incorrect%2520temporal%2520statements%2520based%2520on%2520their%250Aprobability%2520of%2520being%2520generated.%2520We%2520assess%252018%2520LMs%2520across%2520various%2520architectures%250Ausing%2520two%2520metrics%253A%2520the%2520win%2520rate%252C%2520indicating%2520how%2520often%2520correct%2520dates%2520outperform%250Aincorrect%2520ones%252C%2520and%2520robustness%252C%2520reflecting%2520consistent%2520performance%2520across%2520all%250Adates.%2520Our%2520findings%2520reveal%2520that%2520while%2520some%2520LMs%2520achieve%2520a%2520win%2520rate%2520exceeding%250A80%255C%2525%252C%2520robustness%2520remains%2520low%252C%2520with%2520the%2520best%2520model%2520achieving%2520only%25206%255C%2525.%250AFurthermore%252C%2520robust%2520knowledge%2520at%2520one%2520date%2520precision%2520does%2520not%2520reliably%2520transfer%250Ato%2520others%252C%2520highlighting%2520a%2520significant%2520generalization%2520gap.%2520These%2520results%250Aunderscore%2520the%2520struggle%2520of%2520LMs%2520to%2520maintain%2520a%2520consistent%2520temporal%250Arepresentation%252C%2520supporting%2520their%2520limitations%2520as%2520reliable%2520sources%2520of%2520temporal%250Aknowledge.%2520We%2520provide%2520all%2520data%2520and%2520code%2520for%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Struggle%20to%20Achieve%20a%20Consistent%20Temporal%20Representation%0A%20%20of%20Facts&entry.906535625=Hichem%20Ammar%20Khodja%20and%20Fr%C3%A9d%C3%A9ric%20B%C3%A9chet%20and%20Quentin%20Brabant%20and%20Alexis%20Nasr%20and%20Gw%C3%A9nol%C3%A9%20Lecorv%C3%A9&entry.1292438233=%20%20Language%20Models%20%28LMs%29%20have%20shown%20substantial%20improvements%20in%20handling%20factual%0Aknowledge%2C%20yet%20their%20capability%20to%20consistently%20represent%20temporal%20facts%2C%20which%0Aare%20valid%20only%20within%20specific%20timeframes%2C%20remains%20underexplored.%20To%0Ainvestigate%20this%2C%20we%20introduce%20TimeStress%2C%20a%20novel%20dataset%20comprising%20521K%0Astatements%20on%202003%20of%20the%20most%20popular%20temporal%20facts%20in%20Wikidata.%20Each%0Astatement%20contextualizes%20a%20fact%20with%20correct%20and%20incorrect%20dates%20across%20three%0Aprecisions%20%28Day%2C%20Month%2C%20Year%29.%20This%20setup%20allows%20us%20to%20evaluate%20LMs%27%20ability%20to%0Adiscern%20between%20correct%20and%20incorrect%20temporal%20statements%20based%20on%20their%0Aprobability%20of%20being%20generated.%20We%20assess%2018%20LMs%20across%20various%20architectures%0Ausing%20two%20metrics%3A%20the%20win%20rate%2C%20indicating%20how%20often%20correct%20dates%20outperform%0Aincorrect%20ones%2C%20and%20robustness%2C%20reflecting%20consistent%20performance%20across%20all%0Adates.%20Our%20findings%20reveal%20that%20while%20some%20LMs%20achieve%20a%20win%20rate%20exceeding%0A80%5C%25%2C%20robustness%20remains%20low%2C%20with%20the%20best%20model%20achieving%20only%206%5C%25.%0AFurthermore%2C%20robust%20knowledge%20at%20one%20date%20precision%20does%20not%20reliably%20transfer%0Ato%20others%2C%20highlighting%20a%20significant%20generalization%20gap.%20These%20results%0Aunderscore%20the%20struggle%20of%20LMs%20to%20maintain%20a%20consistent%20temporal%0Arepresentation%2C%20supporting%20their%20limitations%20as%20reliable%20sources%20of%20temporal%0Aknowledge.%20We%20provide%20all%20data%20and%20code%20for%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01220v2&entry.124074799=Read"},
{"title": "On the Query Complexity of Verifier-Assisted Language Generation", "author": "Edoardo Botta and Yuchen Li and Aashay Mehta and Jordan T. Ash and Cyril Zhang and Andrej Risteski", "abstract": "  Recently, a plethora of works have proposed inference-time algorithms (e.g.\nbest-of-n), which incorporate verifiers to assist the generation process. Their\nquality-efficiency trade-offs have been empirically benchmarked on a variety of\nconstrained generation tasks, but the algorithmic design landscape is still\nlargely poorly understood. In this paper, we develop a mathematical framework\nfor reasoning about constrained generation using a pre-trained language model\ngenerator oracle and a process verifier--which can decide whether a prefix can\nbe extended to a string which satisfies the constraints of choice. We show that\neven in very simple settings, access to a verifier can render an intractable\nproblem (information-theoretically or computationally) to a tractable one. In\nfact, we show even simple algorithms, like tokenwise rejection sampling, can\nenjoy significant benefits from access to a verifier. Empirically, we show that\na natural modification of tokenwise rejection sampling, in which the sampler is\nallowed to \"backtrack\" (i.e., erase the final few generated tokens) has robust\nand substantive benefits over natural baselines (e.g. (blockwise) rejection\nsampling, nucleus sampling)--both in terms of computational efficiency,\naccuracy and diversity.\n", "link": "http://arxiv.org/abs/2502.12123v1", "date": "2025-02-17", "relevancy": 2.2708, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4666}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Query%20Complexity%20of%20Verifier-Assisted%20Language%20Generation&body=Title%3A%20On%20the%20Query%20Complexity%20of%20Verifier-Assisted%20Language%20Generation%0AAuthor%3A%20Edoardo%20Botta%20and%20Yuchen%20Li%20and%20Aashay%20Mehta%20and%20Jordan%20T.%20Ash%20and%20Cyril%20Zhang%20and%20Andrej%20Risteski%0AAbstract%3A%20%20%20Recently%2C%20a%20plethora%20of%20works%20have%20proposed%20inference-time%20algorithms%20%28e.g.%0Abest-of-n%29%2C%20which%20incorporate%20verifiers%20to%20assist%20the%20generation%20process.%20Their%0Aquality-efficiency%20trade-offs%20have%20been%20empirically%20benchmarked%20on%20a%20variety%20of%0Aconstrained%20generation%20tasks%2C%20but%20the%20algorithmic%20design%20landscape%20is%20still%0Alargely%20poorly%20understood.%20In%20this%20paper%2C%20we%20develop%20a%20mathematical%20framework%0Afor%20reasoning%20about%20constrained%20generation%20using%20a%20pre-trained%20language%20model%0Agenerator%20oracle%20and%20a%20process%20verifier--which%20can%20decide%20whether%20a%20prefix%20can%0Abe%20extended%20to%20a%20string%20which%20satisfies%20the%20constraints%20of%20choice.%20We%20show%20that%0Aeven%20in%20very%20simple%20settings%2C%20access%20to%20a%20verifier%20can%20render%20an%20intractable%0Aproblem%20%28information-theoretically%20or%20computationally%29%20to%20a%20tractable%20one.%20In%0Afact%2C%20we%20show%20even%20simple%20algorithms%2C%20like%20tokenwise%20rejection%20sampling%2C%20can%0Aenjoy%20significant%20benefits%20from%20access%20to%20a%20verifier.%20Empirically%2C%20we%20show%20that%0Aa%20natural%20modification%20of%20tokenwise%20rejection%20sampling%2C%20in%20which%20the%20sampler%20is%0Aallowed%20to%20%22backtrack%22%20%28i.e.%2C%20erase%20the%20final%20few%20generated%20tokens%29%20has%20robust%0Aand%20substantive%20benefits%20over%20natural%20baselines%20%28e.g.%20%28blockwise%29%20rejection%0Asampling%2C%20nucleus%20sampling%29--both%20in%20terms%20of%20computational%20efficiency%2C%0Aaccuracy%20and%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Query%2520Complexity%2520of%2520Verifier-Assisted%2520Language%2520Generation%26entry.906535625%3DEdoardo%2520Botta%2520and%2520Yuchen%2520Li%2520and%2520Aashay%2520Mehta%2520and%2520Jordan%2520T.%2520Ash%2520and%2520Cyril%2520Zhang%2520and%2520Andrej%2520Risteski%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520plethora%2520of%2520works%2520have%2520proposed%2520inference-time%2520algorithms%2520%2528e.g.%250Abest-of-n%2529%252C%2520which%2520incorporate%2520verifiers%2520to%2520assist%2520the%2520generation%2520process.%2520Their%250Aquality-efficiency%2520trade-offs%2520have%2520been%2520empirically%2520benchmarked%2520on%2520a%2520variety%2520of%250Aconstrained%2520generation%2520tasks%252C%2520but%2520the%2520algorithmic%2520design%2520landscape%2520is%2520still%250Alargely%2520poorly%2520understood.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520mathematical%2520framework%250Afor%2520reasoning%2520about%2520constrained%2520generation%2520using%2520a%2520pre-trained%2520language%2520model%250Agenerator%2520oracle%2520and%2520a%2520process%2520verifier--which%2520can%2520decide%2520whether%2520a%2520prefix%2520can%250Abe%2520extended%2520to%2520a%2520string%2520which%2520satisfies%2520the%2520constraints%2520of%2520choice.%2520We%2520show%2520that%250Aeven%2520in%2520very%2520simple%2520settings%252C%2520access%2520to%2520a%2520verifier%2520can%2520render%2520an%2520intractable%250Aproblem%2520%2528information-theoretically%2520or%2520computationally%2529%2520to%2520a%2520tractable%2520one.%2520In%250Afact%252C%2520we%2520show%2520even%2520simple%2520algorithms%252C%2520like%2520tokenwise%2520rejection%2520sampling%252C%2520can%250Aenjoy%2520significant%2520benefits%2520from%2520access%2520to%2520a%2520verifier.%2520Empirically%252C%2520we%2520show%2520that%250Aa%2520natural%2520modification%2520of%2520tokenwise%2520rejection%2520sampling%252C%2520in%2520which%2520the%2520sampler%2520is%250Aallowed%2520to%2520%2522backtrack%2522%2520%2528i.e.%252C%2520erase%2520the%2520final%2520few%2520generated%2520tokens%2529%2520has%2520robust%250Aand%2520substantive%2520benefits%2520over%2520natural%2520baselines%2520%2528e.g.%2520%2528blockwise%2529%2520rejection%250Asampling%252C%2520nucleus%2520sampling%2529--both%2520in%2520terms%2520of%2520computational%2520efficiency%252C%250Aaccuracy%2520and%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Query%20Complexity%20of%20Verifier-Assisted%20Language%20Generation&entry.906535625=Edoardo%20Botta%20and%20Yuchen%20Li%20and%20Aashay%20Mehta%20and%20Jordan%20T.%20Ash%20and%20Cyril%20Zhang%20and%20Andrej%20Risteski&entry.1292438233=%20%20Recently%2C%20a%20plethora%20of%20works%20have%20proposed%20inference-time%20algorithms%20%28e.g.%0Abest-of-n%29%2C%20which%20incorporate%20verifiers%20to%20assist%20the%20generation%20process.%20Their%0Aquality-efficiency%20trade-offs%20have%20been%20empirically%20benchmarked%20on%20a%20variety%20of%0Aconstrained%20generation%20tasks%2C%20but%20the%20algorithmic%20design%20landscape%20is%20still%0Alargely%20poorly%20understood.%20In%20this%20paper%2C%20we%20develop%20a%20mathematical%20framework%0Afor%20reasoning%20about%20constrained%20generation%20using%20a%20pre-trained%20language%20model%0Agenerator%20oracle%20and%20a%20process%20verifier--which%20can%20decide%20whether%20a%20prefix%20can%0Abe%20extended%20to%20a%20string%20which%20satisfies%20the%20constraints%20of%20choice.%20We%20show%20that%0Aeven%20in%20very%20simple%20settings%2C%20access%20to%20a%20verifier%20can%20render%20an%20intractable%0Aproblem%20%28information-theoretically%20or%20computationally%29%20to%20a%20tractable%20one.%20In%0Afact%2C%20we%20show%20even%20simple%20algorithms%2C%20like%20tokenwise%20rejection%20sampling%2C%20can%0Aenjoy%20significant%20benefits%20from%20access%20to%20a%20verifier.%20Empirically%2C%20we%20show%20that%0Aa%20natural%20modification%20of%20tokenwise%20rejection%20sampling%2C%20in%20which%20the%20sampler%20is%0Aallowed%20to%20%22backtrack%22%20%28i.e.%2C%20erase%20the%20final%20few%20generated%20tokens%29%20has%20robust%0Aand%20substantive%20benefits%20over%20natural%20baselines%20%28e.g.%20%28blockwise%29%20rejection%0Asampling%2C%20nucleus%20sampling%29--both%20in%20terms%20of%20computational%20efficiency%2C%0Aaccuracy%20and%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12123v1&entry.124074799=Read"},
{"title": "TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for\n  Traversability Estimation", "author": "Antonio Santo and Juan J. Cabrera and David Valiente and Carlos Viegas and Arturo Gil", "abstract": "  This paper presents TE-NeXt, a novel and efficient architecture for\nTraversability Estimation (TE) from sparse LiDAR point clouds based on a\nresidual convolution block. TE-NeXt block fuses notions of current trends such\nas attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstrate\nhigh capacity for generalisation in a variety of urban and natural\nenvironments, using well-known and accessible datasets such as SemanticKITTI,\nRellis-3D and SemanticUSL. Thus, the designed architecture ouperforms\nstate-of-the-art methods in the problem of semantic segmentation, demonstrating\nbetter results in unstructured environments and maintaining high reliability\nand robustness in urbans environments, which leads to better abstraction.\nImplementation is available in a open repository to the scientific community\nwith the aim of ensuring the reproducibility of results.\n", "link": "http://arxiv.org/abs/2406.01395v4", "date": "2025-02-17", "relevancy": 2.2655, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5712}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5668}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TE-NeXt%3A%20A%20LiDAR-Based%203D%20Sparse%20Convolutional%20Network%20for%0A%20%20Traversability%20Estimation&body=Title%3A%20TE-NeXt%3A%20A%20LiDAR-Based%203D%20Sparse%20Convolutional%20Network%20for%0A%20%20Traversability%20Estimation%0AAuthor%3A%20Antonio%20Santo%20and%20Juan%20J.%20Cabrera%20and%20David%20Valiente%20and%20Carlos%20Viegas%20and%20Arturo%20Gil%0AAbstract%3A%20%20%20This%20paper%20presents%20TE-NeXt%2C%20a%20novel%20and%20efficient%20architecture%20for%0ATraversability%20Estimation%20%28TE%29%20from%20sparse%20LiDAR%20point%20clouds%20based%20on%20a%0Aresidual%20convolution%20block.%20TE-NeXt%20block%20fuses%20notions%20of%20current%20trends%20such%0Aas%20attention%20mechanisms%20and%203D%20sparse%20convolutions.%20TE-NeXt%20aims%20to%20demonstrate%0Ahigh%20capacity%20for%20generalisation%20in%20a%20variety%20of%20urban%20and%20natural%0Aenvironments%2C%20using%20well-known%20and%20accessible%20datasets%20such%20as%20SemanticKITTI%2C%0ARellis-3D%20and%20SemanticUSL.%20Thus%2C%20the%20designed%20architecture%20ouperforms%0Astate-of-the-art%20methods%20in%20the%20problem%20of%20semantic%20segmentation%2C%20demonstrating%0Abetter%20results%20in%20unstructured%20environments%20and%20maintaining%20high%20reliability%0Aand%20robustness%20in%20urbans%20environments%2C%20which%20leads%20to%20better%20abstraction.%0AImplementation%20is%20available%20in%20a%20open%20repository%20to%20the%20scientific%20community%0Awith%20the%20aim%20of%20ensuring%20the%20reproducibility%20of%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01395v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTE-NeXt%253A%2520A%2520LiDAR-Based%25203D%2520Sparse%2520Convolutional%2520Network%2520for%250A%2520%2520Traversability%2520Estimation%26entry.906535625%3DAntonio%2520Santo%2520and%2520Juan%2520J.%2520Cabrera%2520and%2520David%2520Valiente%2520and%2520Carlos%2520Viegas%2520and%2520Arturo%2520Gil%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520TE-NeXt%252C%2520a%2520novel%2520and%2520efficient%2520architecture%2520for%250ATraversability%2520Estimation%2520%2528TE%2529%2520from%2520sparse%2520LiDAR%2520point%2520clouds%2520based%2520on%2520a%250Aresidual%2520convolution%2520block.%2520TE-NeXt%2520block%2520fuses%2520notions%2520of%2520current%2520trends%2520such%250Aas%2520attention%2520mechanisms%2520and%25203D%2520sparse%2520convolutions.%2520TE-NeXt%2520aims%2520to%2520demonstrate%250Ahigh%2520capacity%2520for%2520generalisation%2520in%2520a%2520variety%2520of%2520urban%2520and%2520natural%250Aenvironments%252C%2520using%2520well-known%2520and%2520accessible%2520datasets%2520such%2520as%2520SemanticKITTI%252C%250ARellis-3D%2520and%2520SemanticUSL.%2520Thus%252C%2520the%2520designed%2520architecture%2520ouperforms%250Astate-of-the-art%2520methods%2520in%2520the%2520problem%2520of%2520semantic%2520segmentation%252C%2520demonstrating%250Abetter%2520results%2520in%2520unstructured%2520environments%2520and%2520maintaining%2520high%2520reliability%250Aand%2520robustness%2520in%2520urbans%2520environments%252C%2520which%2520leads%2520to%2520better%2520abstraction.%250AImplementation%2520is%2520available%2520in%2520a%2520open%2520repository%2520to%2520the%2520scientific%2520community%250Awith%2520the%2520aim%2520of%2520ensuring%2520the%2520reproducibility%2520of%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01395v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TE-NeXt%3A%20A%20LiDAR-Based%203D%20Sparse%20Convolutional%20Network%20for%0A%20%20Traversability%20Estimation&entry.906535625=Antonio%20Santo%20and%20Juan%20J.%20Cabrera%20and%20David%20Valiente%20and%20Carlos%20Viegas%20and%20Arturo%20Gil&entry.1292438233=%20%20This%20paper%20presents%20TE-NeXt%2C%20a%20novel%20and%20efficient%20architecture%20for%0ATraversability%20Estimation%20%28TE%29%20from%20sparse%20LiDAR%20point%20clouds%20based%20on%20a%0Aresidual%20convolution%20block.%20TE-NeXt%20block%20fuses%20notions%20of%20current%20trends%20such%0Aas%20attention%20mechanisms%20and%203D%20sparse%20convolutions.%20TE-NeXt%20aims%20to%20demonstrate%0Ahigh%20capacity%20for%20generalisation%20in%20a%20variety%20of%20urban%20and%20natural%0Aenvironments%2C%20using%20well-known%20and%20accessible%20datasets%20such%20as%20SemanticKITTI%2C%0ARellis-3D%20and%20SemanticUSL.%20Thus%2C%20the%20designed%20architecture%20ouperforms%0Astate-of-the-art%20methods%20in%20the%20problem%20of%20semantic%20segmentation%2C%20demonstrating%0Abetter%20results%20in%20unstructured%20environments%20and%20maintaining%20high%20reliability%0Aand%20robustness%20in%20urbans%20environments%2C%20which%20leads%20to%20better%20abstraction.%0AImplementation%20is%20available%20in%20a%20open%20repository%20to%20the%20scientific%20community%0Awith%20the%20aim%20of%20ensuring%20the%20reproducibility%20of%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01395v4&entry.124074799=Read"},
{"title": "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification", "author": "Haisong Gong and Jing Li and Junfei Wu and Qiang Liu and Shu Wu and Liang Wang", "abstract": "  Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness.\n", "link": "http://arxiv.org/abs/2502.11959v1", "date": "2025-02-17", "relevancy": 2.2581, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4584}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STRIVE%3A%20Structured%20Reasoning%20for%20Self-Improvement%20in%20Claim%20Verification&body=Title%3A%20STRIVE%3A%20Structured%20Reasoning%20for%20Self-Improvement%20in%20Claim%20Verification%0AAuthor%3A%20Haisong%20Gong%20and%20Jing%20Li%20and%20Junfei%20Wu%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Claim%20verification%20is%20the%20task%20of%20determining%20whether%20a%20claim%20is%20supported%20or%0Arefuted%20by%20evidence.%20Self-improvement%20methods%2C%20where%20reasoning%20chains%20are%0Agenerated%20and%20those%20leading%20to%20correct%20results%20are%20selected%20for%20training%2C%20have%0Asucceeded%20in%20tasks%20like%20mathematical%20problem%20solving.%20However%2C%20in%20claim%0Averification%2C%20this%20approach%20struggles.%20Low-quality%20reasoning%20chains%20may%20falsely%0Amatch%20binary%20truth%20labels%2C%20introducing%20faulty%20reasoning%20into%20the%0Aself-improvement%20process%20and%20ultimately%20degrading%20performance.%20To%20address%20this%2C%0Awe%20propose%20STRIVE%3A%20Structured%20Reasoning%20for%20Self-Improved%20Verification.%20Our%0Amethod%20introduces%20a%20structured%20reasoning%20design%20with%20Claim%20Decomposition%2C%0AEntity%20Analysis%2C%20and%20Evidence%20Grounding%20Verification.%20These%20components%20improve%0Areasoning%20quality%2C%20reduce%20errors%2C%20and%20provide%20additional%20supervision%20signals%0Afor%20self-improvement.%20STRIVE%20begins%20with%20a%20warm-up%20phase%2C%20where%20the%20base%20model%0Ais%20fine-tuned%20on%20a%20small%20number%20of%20annotated%20examples%20to%20learn%20the%20structured%0Areasoning%20design.%20It%20is%20then%20applied%20to%20generate%20reasoning%20chains%20for%20all%0Atraining%20examples%2C%20selecting%20only%20those%20that%20are%20correct%20and%20structurally%20sound%0Afor%20subsequent%20self-improvement%20training.%20We%20demonstrate%20that%20STRIVE%20achieves%0Asignificant%20improvements%20over%20baseline%20models%2C%20with%20a%2031.4%25%20performance%20gain%0Aover%20the%20base%20model%20and%2020.7%25%20over%20Chain%20of%20Thought%20on%20the%20HOVER%20datasets%2C%0Ahighlighting%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTRIVE%253A%2520Structured%2520Reasoning%2520for%2520Self-Improvement%2520in%2520Claim%2520Verification%26entry.906535625%3DHaisong%2520Gong%2520and%2520Jing%2520Li%2520and%2520Junfei%2520Wu%2520and%2520Qiang%2520Liu%2520and%2520Shu%2520Wu%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Claim%2520verification%2520is%2520the%2520task%2520of%2520determining%2520whether%2520a%2520claim%2520is%2520supported%2520or%250Arefuted%2520by%2520evidence.%2520Self-improvement%2520methods%252C%2520where%2520reasoning%2520chains%2520are%250Agenerated%2520and%2520those%2520leading%2520to%2520correct%2520results%2520are%2520selected%2520for%2520training%252C%2520have%250Asucceeded%2520in%2520tasks%2520like%2520mathematical%2520problem%2520solving.%2520However%252C%2520in%2520claim%250Averification%252C%2520this%2520approach%2520struggles.%2520Low-quality%2520reasoning%2520chains%2520may%2520falsely%250Amatch%2520binary%2520truth%2520labels%252C%2520introducing%2520faulty%2520reasoning%2520into%2520the%250Aself-improvement%2520process%2520and%2520ultimately%2520degrading%2520performance.%2520To%2520address%2520this%252C%250Awe%2520propose%2520STRIVE%253A%2520Structured%2520Reasoning%2520for%2520Self-Improved%2520Verification.%2520Our%250Amethod%2520introduces%2520a%2520structured%2520reasoning%2520design%2520with%2520Claim%2520Decomposition%252C%250AEntity%2520Analysis%252C%2520and%2520Evidence%2520Grounding%2520Verification.%2520These%2520components%2520improve%250Areasoning%2520quality%252C%2520reduce%2520errors%252C%2520and%2520provide%2520additional%2520supervision%2520signals%250Afor%2520self-improvement.%2520STRIVE%2520begins%2520with%2520a%2520warm-up%2520phase%252C%2520where%2520the%2520base%2520model%250Ais%2520fine-tuned%2520on%2520a%2520small%2520number%2520of%2520annotated%2520examples%2520to%2520learn%2520the%2520structured%250Areasoning%2520design.%2520It%2520is%2520then%2520applied%2520to%2520generate%2520reasoning%2520chains%2520for%2520all%250Atraining%2520examples%252C%2520selecting%2520only%2520those%2520that%2520are%2520correct%2520and%2520structurally%2520sound%250Afor%2520subsequent%2520self-improvement%2520training.%2520We%2520demonstrate%2520that%2520STRIVE%2520achieves%250Asignificant%2520improvements%2520over%2520baseline%2520models%252C%2520with%2520a%252031.4%2525%2520performance%2520gain%250Aover%2520the%2520base%2520model%2520and%252020.7%2525%2520over%2520Chain%2520of%2520Thought%2520on%2520the%2520HOVER%2520datasets%252C%250Ahighlighting%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STRIVE%3A%20Structured%20Reasoning%20for%20Self-Improvement%20in%20Claim%20Verification&entry.906535625=Haisong%20Gong%20and%20Jing%20Li%20and%20Junfei%20Wu%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang&entry.1292438233=%20%20Claim%20verification%20is%20the%20task%20of%20determining%20whether%20a%20claim%20is%20supported%20or%0Arefuted%20by%20evidence.%20Self-improvement%20methods%2C%20where%20reasoning%20chains%20are%0Agenerated%20and%20those%20leading%20to%20correct%20results%20are%20selected%20for%20training%2C%20have%0Asucceeded%20in%20tasks%20like%20mathematical%20problem%20solving.%20However%2C%20in%20claim%0Averification%2C%20this%20approach%20struggles.%20Low-quality%20reasoning%20chains%20may%20falsely%0Amatch%20binary%20truth%20labels%2C%20introducing%20faulty%20reasoning%20into%20the%0Aself-improvement%20process%20and%20ultimately%20degrading%20performance.%20To%20address%20this%2C%0Awe%20propose%20STRIVE%3A%20Structured%20Reasoning%20for%20Self-Improved%20Verification.%20Our%0Amethod%20introduces%20a%20structured%20reasoning%20design%20with%20Claim%20Decomposition%2C%0AEntity%20Analysis%2C%20and%20Evidence%20Grounding%20Verification.%20These%20components%20improve%0Areasoning%20quality%2C%20reduce%20errors%2C%20and%20provide%20additional%20supervision%20signals%0Afor%20self-improvement.%20STRIVE%20begins%20with%20a%20warm-up%20phase%2C%20where%20the%20base%20model%0Ais%20fine-tuned%20on%20a%20small%20number%20of%20annotated%20examples%20to%20learn%20the%20structured%0Areasoning%20design.%20It%20is%20then%20applied%20to%20generate%20reasoning%20chains%20for%20all%0Atraining%20examples%2C%20selecting%20only%20those%20that%20are%20correct%20and%20structurally%20sound%0Afor%20subsequent%20self-improvement%20training.%20We%20demonstrate%20that%20STRIVE%20achieves%0Asignificant%20improvements%20over%20baseline%20models%2C%20with%20a%2031.4%25%20performance%20gain%0Aover%20the%20base%20model%20and%2020.7%25%20over%20Chain%20of%20Thought%20on%20the%20HOVER%20datasets%2C%0Ahighlighting%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11959v1&entry.124074799=Read"},
{"title": "AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land\n  Simulator", "author": "William Yang and Karthikeya Kona and Yashveer Jain and Tomer Atzili and Abhinav Bhamidipati and Xiaomin Lin and Yantian Zha", "abstract": "  Current mobile manipulators and high-fidelity simulators lack the ability to\nseamlessly operate and simulate across integrated environments spanning sea,\nair, and land. To address this gap, we introduce Aerial-Aquatic Manipulators\n(AAMs) in SEa, Air, and Land Simulator (SEALS), a comprehensive and\nphotorealistic simulator designed for AAMs to operate and learn in these\ndiverse environments. The development of AAM-SEALS tackles several significant\nchallenges, including the creation of integrated controllers for flying,\nswimming, and manipulation, and the high-fidelity simulation of aerial dynamics\nand hydrodynamics leveraging particle-based hydrodynamics. Our evaluation\ndemonstrates smooth operation and photorealistic transitions across air, water,\nand their interfaces. We quantitatively validate the fidelity of particle-based\nhydrodynamics by comparing position-tracking errors across real-world and\nsimulated systems. AAM-SEALS benefits a broad range of robotics communities,\nincluding robot learning, aerial robotics, underwater robotics, mobile\nmanipulation, and robotic simulators. We will open-source our code and data to\nfoster the advancement of research in these fields. The overview video is\navailable at https://youtu.be/MbqIIrYvR78. Visit our project website at\nhttps://aam-seals.umd.edu for more details.\n", "link": "http://arxiv.org/abs/2412.19744v4", "date": "2025-02-17", "relevancy": 1.5538, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5298}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AAM-SEALS%3A%20Developing%20Aerial-Aquatic%20Manipulators%20in%20SEa%2C%20Air%2C%20and%20Land%0A%20%20Simulator&body=Title%3A%20AAM-SEALS%3A%20Developing%20Aerial-Aquatic%20Manipulators%20in%20SEa%2C%20Air%2C%20and%20Land%0A%20%20Simulator%0AAuthor%3A%20William%20Yang%20and%20Karthikeya%20Kona%20and%20Yashveer%20Jain%20and%20Tomer%20Atzili%20and%20Abhinav%20Bhamidipati%20and%20Xiaomin%20Lin%20and%20Yantian%20Zha%0AAbstract%3A%20%20%20Current%20mobile%20manipulators%20and%20high-fidelity%20simulators%20lack%20the%20ability%20to%0Aseamlessly%20operate%20and%20simulate%20across%20integrated%20environments%20spanning%20sea%2C%0Aair%2C%20and%20land.%20To%20address%20this%20gap%2C%20we%20introduce%20Aerial-Aquatic%20Manipulators%0A%28AAMs%29%20in%20SEa%2C%20Air%2C%20and%20Land%20Simulator%20%28SEALS%29%2C%20a%20comprehensive%20and%0Aphotorealistic%20simulator%20designed%20for%20AAMs%20to%20operate%20and%20learn%20in%20these%0Adiverse%20environments.%20The%20development%20of%20AAM-SEALS%20tackles%20several%20significant%0Achallenges%2C%20including%20the%20creation%20of%20integrated%20controllers%20for%20flying%2C%0Aswimming%2C%20and%20manipulation%2C%20and%20the%20high-fidelity%20simulation%20of%20aerial%20dynamics%0Aand%20hydrodynamics%20leveraging%20particle-based%20hydrodynamics.%20Our%20evaluation%0Ademonstrates%20smooth%20operation%20and%20photorealistic%20transitions%20across%20air%2C%20water%2C%0Aand%20their%20interfaces.%20We%20quantitatively%20validate%20the%20fidelity%20of%20particle-based%0Ahydrodynamics%20by%20comparing%20position-tracking%20errors%20across%20real-world%20and%0Asimulated%20systems.%20AAM-SEALS%20benefits%20a%20broad%20range%20of%20robotics%20communities%2C%0Aincluding%20robot%20learning%2C%20aerial%20robotics%2C%20underwater%20robotics%2C%20mobile%0Amanipulation%2C%20and%20robotic%20simulators.%20We%20will%20open-source%20our%20code%20and%20data%20to%0Afoster%20the%20advancement%20of%20research%20in%20these%20fields.%20The%20overview%20video%20is%0Aavailable%20at%20https%3A//youtu.be/MbqIIrYvR78.%20Visit%20our%20project%20website%20at%0Ahttps%3A//aam-seals.umd.edu%20for%20more%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19744v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAAM-SEALS%253A%2520Developing%2520Aerial-Aquatic%2520Manipulators%2520in%2520SEa%252C%2520Air%252C%2520and%2520Land%250A%2520%2520Simulator%26entry.906535625%3DWilliam%2520Yang%2520and%2520Karthikeya%2520Kona%2520and%2520Yashveer%2520Jain%2520and%2520Tomer%2520Atzili%2520and%2520Abhinav%2520Bhamidipati%2520and%2520Xiaomin%2520Lin%2520and%2520Yantian%2520Zha%26entry.1292438233%3D%2520%2520Current%2520mobile%2520manipulators%2520and%2520high-fidelity%2520simulators%2520lack%2520the%2520ability%2520to%250Aseamlessly%2520operate%2520and%2520simulate%2520across%2520integrated%2520environments%2520spanning%2520sea%252C%250Aair%252C%2520and%2520land.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Aerial-Aquatic%2520Manipulators%250A%2528AAMs%2529%2520in%2520SEa%252C%2520Air%252C%2520and%2520Land%2520Simulator%2520%2528SEALS%2529%252C%2520a%2520comprehensive%2520and%250Aphotorealistic%2520simulator%2520designed%2520for%2520AAMs%2520to%2520operate%2520and%2520learn%2520in%2520these%250Adiverse%2520environments.%2520The%2520development%2520of%2520AAM-SEALS%2520tackles%2520several%2520significant%250Achallenges%252C%2520including%2520the%2520creation%2520of%2520integrated%2520controllers%2520for%2520flying%252C%250Aswimming%252C%2520and%2520manipulation%252C%2520and%2520the%2520high-fidelity%2520simulation%2520of%2520aerial%2520dynamics%250Aand%2520hydrodynamics%2520leveraging%2520particle-based%2520hydrodynamics.%2520Our%2520evaluation%250Ademonstrates%2520smooth%2520operation%2520and%2520photorealistic%2520transitions%2520across%2520air%252C%2520water%252C%250Aand%2520their%2520interfaces.%2520We%2520quantitatively%2520validate%2520the%2520fidelity%2520of%2520particle-based%250Ahydrodynamics%2520by%2520comparing%2520position-tracking%2520errors%2520across%2520real-world%2520and%250Asimulated%2520systems.%2520AAM-SEALS%2520benefits%2520a%2520broad%2520range%2520of%2520robotics%2520communities%252C%250Aincluding%2520robot%2520learning%252C%2520aerial%2520robotics%252C%2520underwater%2520robotics%252C%2520mobile%250Amanipulation%252C%2520and%2520robotic%2520simulators.%2520We%2520will%2520open-source%2520our%2520code%2520and%2520data%2520to%250Afoster%2520the%2520advancement%2520of%2520research%2520in%2520these%2520fields.%2520The%2520overview%2520video%2520is%250Aavailable%2520at%2520https%253A//youtu.be/MbqIIrYvR78.%2520Visit%2520our%2520project%2520website%2520at%250Ahttps%253A//aam-seals.umd.edu%2520for%2520more%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19744v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AAM-SEALS%3A%20Developing%20Aerial-Aquatic%20Manipulators%20in%20SEa%2C%20Air%2C%20and%20Land%0A%20%20Simulator&entry.906535625=William%20Yang%20and%20Karthikeya%20Kona%20and%20Yashveer%20Jain%20and%20Tomer%20Atzili%20and%20Abhinav%20Bhamidipati%20and%20Xiaomin%20Lin%20and%20Yantian%20Zha&entry.1292438233=%20%20Current%20mobile%20manipulators%20and%20high-fidelity%20simulators%20lack%20the%20ability%20to%0Aseamlessly%20operate%20and%20simulate%20across%20integrated%20environments%20spanning%20sea%2C%0Aair%2C%20and%20land.%20To%20address%20this%20gap%2C%20we%20introduce%20Aerial-Aquatic%20Manipulators%0A%28AAMs%29%20in%20SEa%2C%20Air%2C%20and%20Land%20Simulator%20%28SEALS%29%2C%20a%20comprehensive%20and%0Aphotorealistic%20simulator%20designed%20for%20AAMs%20to%20operate%20and%20learn%20in%20these%0Adiverse%20environments.%20The%20development%20of%20AAM-SEALS%20tackles%20several%20significant%0Achallenges%2C%20including%20the%20creation%20of%20integrated%20controllers%20for%20flying%2C%0Aswimming%2C%20and%20manipulation%2C%20and%20the%20high-fidelity%20simulation%20of%20aerial%20dynamics%0Aand%20hydrodynamics%20leveraging%20particle-based%20hydrodynamics.%20Our%20evaluation%0Ademonstrates%20smooth%20operation%20and%20photorealistic%20transitions%20across%20air%2C%20water%2C%0Aand%20their%20interfaces.%20We%20quantitatively%20validate%20the%20fidelity%20of%20particle-based%0Ahydrodynamics%20by%20comparing%20position-tracking%20errors%20across%20real-world%20and%0Asimulated%20systems.%20AAM-SEALS%20benefits%20a%20broad%20range%20of%20robotics%20communities%2C%0Aincluding%20robot%20learning%2C%20aerial%20robotics%2C%20underwater%20robotics%2C%20mobile%0Amanipulation%2C%20and%20robotic%20simulators.%20We%20will%20open-source%20our%20code%20and%20data%20to%0Afoster%20the%20advancement%20of%20research%20in%20these%20fields.%20The%20overview%20video%20is%0Aavailable%20at%20https%3A//youtu.be/MbqIIrYvR78.%20Visit%20our%20project%20website%20at%0Ahttps%3A//aam-seals.umd.edu%20for%20more%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19744v4&entry.124074799=Read"},
{"title": "Incomplete Modality Disentangled Representation for Ophthalmic Disease\n  Grading and Diagnosis", "author": "Chengzhi Liu and Zile Huang and Zhe Chen and Feilong Tang and Yu Tian and Zhongxing Xu and Zihong Luo and Yalin Zheng and Yanda Meng", "abstract": "  Ophthalmologists typically require multimodal data sources to improve\ndiagnostic accuracy in clinical decisions. However, due to medical device\nshortages, low-quality data and data privacy concerns, missing data modalities\nare common in real-world scenarios. Existing deep learning methods tend to\naddress it by learning an implicit latent subspace representation for different\nmodality combinations. We identify two significant limitations of these\nmethods: (1) implicit representation constraints that hinder the model's\nability to capture modality-specific information and (2) modality\nheterogeneity, causing distribution gaps and redundancy in feature\nrepresentations. To address these, we propose an Incomplete Modality\nDisentangled Representation (IMDR) strategy, which disentangles features into\nexplicit independent modal-common and modal-specific features by guidance of\nmutual information, distilling informative knowledge and enabling it to\nreconstruct valuable missing semantics and produce robust multimodal\nrepresentations. Furthermore, we introduce a joint proxy learning module that\nassists IMDR in eliminating intra-modality redundancy by exploiting the\nextracted proxies from each class. Experiments on four ophthalmology multimodal\ndatasets demonstrate that the proposed IMDR outperforms the state-of-the-art\nmethods significantly.\n", "link": "http://arxiv.org/abs/2502.11724v1", "date": "2025-02-17", "relevancy": 1.6781, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5648}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incomplete%20Modality%20Disentangled%20Representation%20for%20Ophthalmic%20Disease%0A%20%20Grading%20and%20Diagnosis&body=Title%3A%20Incomplete%20Modality%20Disentangled%20Representation%20for%20Ophthalmic%20Disease%0A%20%20Grading%20and%20Diagnosis%0AAuthor%3A%20Chengzhi%20Liu%20and%20Zile%20Huang%20and%20Zhe%20Chen%20and%20Feilong%20Tang%20and%20Yu%20Tian%20and%20Zhongxing%20Xu%20and%20Zihong%20Luo%20and%20Yalin%20Zheng%20and%20Yanda%20Meng%0AAbstract%3A%20%20%20Ophthalmologists%20typically%20require%20multimodal%20data%20sources%20to%20improve%0Adiagnostic%20accuracy%20in%20clinical%20decisions.%20However%2C%20due%20to%20medical%20device%0Ashortages%2C%20low-quality%20data%20and%20data%20privacy%20concerns%2C%20missing%20data%20modalities%0Aare%20common%20in%20real-world%20scenarios.%20Existing%20deep%20learning%20methods%20tend%20to%0Aaddress%20it%20by%20learning%20an%20implicit%20latent%20subspace%20representation%20for%20different%0Amodality%20combinations.%20We%20identify%20two%20significant%20limitations%20of%20these%0Amethods%3A%20%281%29%20implicit%20representation%20constraints%20that%20hinder%20the%20model%27s%0Aability%20to%20capture%20modality-specific%20information%20and%20%282%29%20modality%0Aheterogeneity%2C%20causing%20distribution%20gaps%20and%20redundancy%20in%20feature%0Arepresentations.%20To%20address%20these%2C%20we%20propose%20an%20Incomplete%20Modality%0ADisentangled%20Representation%20%28IMDR%29%20strategy%2C%20which%20disentangles%20features%20into%0Aexplicit%20independent%20modal-common%20and%20modal-specific%20features%20by%20guidance%20of%0Amutual%20information%2C%20distilling%20informative%20knowledge%20and%20enabling%20it%20to%0Areconstruct%20valuable%20missing%20semantics%20and%20produce%20robust%20multimodal%0Arepresentations.%20Furthermore%2C%20we%20introduce%20a%20joint%20proxy%20learning%20module%20that%0Aassists%20IMDR%20in%20eliminating%20intra-modality%20redundancy%20by%20exploiting%20the%0Aextracted%20proxies%20from%20each%20class.%20Experiments%20on%20four%20ophthalmology%20multimodal%0Adatasets%20demonstrate%20that%20the%20proposed%20IMDR%20outperforms%20the%20state-of-the-art%0Amethods%20significantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncomplete%2520Modality%2520Disentangled%2520Representation%2520for%2520Ophthalmic%2520Disease%250A%2520%2520Grading%2520and%2520Diagnosis%26entry.906535625%3DChengzhi%2520Liu%2520and%2520Zile%2520Huang%2520and%2520Zhe%2520Chen%2520and%2520Feilong%2520Tang%2520and%2520Yu%2520Tian%2520and%2520Zhongxing%2520Xu%2520and%2520Zihong%2520Luo%2520and%2520Yalin%2520Zheng%2520and%2520Yanda%2520Meng%26entry.1292438233%3D%2520%2520Ophthalmologists%2520typically%2520require%2520multimodal%2520data%2520sources%2520to%2520improve%250Adiagnostic%2520accuracy%2520in%2520clinical%2520decisions.%2520However%252C%2520due%2520to%2520medical%2520device%250Ashortages%252C%2520low-quality%2520data%2520and%2520data%2520privacy%2520concerns%252C%2520missing%2520data%2520modalities%250Aare%2520common%2520in%2520real-world%2520scenarios.%2520Existing%2520deep%2520learning%2520methods%2520tend%2520to%250Aaddress%2520it%2520by%2520learning%2520an%2520implicit%2520latent%2520subspace%2520representation%2520for%2520different%250Amodality%2520combinations.%2520We%2520identify%2520two%2520significant%2520limitations%2520of%2520these%250Amethods%253A%2520%25281%2529%2520implicit%2520representation%2520constraints%2520that%2520hinder%2520the%2520model%2527s%250Aability%2520to%2520capture%2520modality-specific%2520information%2520and%2520%25282%2529%2520modality%250Aheterogeneity%252C%2520causing%2520distribution%2520gaps%2520and%2520redundancy%2520in%2520feature%250Arepresentations.%2520To%2520address%2520these%252C%2520we%2520propose%2520an%2520Incomplete%2520Modality%250ADisentangled%2520Representation%2520%2528IMDR%2529%2520strategy%252C%2520which%2520disentangles%2520features%2520into%250Aexplicit%2520independent%2520modal-common%2520and%2520modal-specific%2520features%2520by%2520guidance%2520of%250Amutual%2520information%252C%2520distilling%2520informative%2520knowledge%2520and%2520enabling%2520it%2520to%250Areconstruct%2520valuable%2520missing%2520semantics%2520and%2520produce%2520robust%2520multimodal%250Arepresentations.%2520Furthermore%252C%2520we%2520introduce%2520a%2520joint%2520proxy%2520learning%2520module%2520that%250Aassists%2520IMDR%2520in%2520eliminating%2520intra-modality%2520redundancy%2520by%2520exploiting%2520the%250Aextracted%2520proxies%2520from%2520each%2520class.%2520Experiments%2520on%2520four%2520ophthalmology%2520multimodal%250Adatasets%2520demonstrate%2520that%2520the%2520proposed%2520IMDR%2520outperforms%2520the%2520state-of-the-art%250Amethods%2520significantly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incomplete%20Modality%20Disentangled%20Representation%20for%20Ophthalmic%20Disease%0A%20%20Grading%20and%20Diagnosis&entry.906535625=Chengzhi%20Liu%20and%20Zile%20Huang%20and%20Zhe%20Chen%20and%20Feilong%20Tang%20and%20Yu%20Tian%20and%20Zhongxing%20Xu%20and%20Zihong%20Luo%20and%20Yalin%20Zheng%20and%20Yanda%20Meng&entry.1292438233=%20%20Ophthalmologists%20typically%20require%20multimodal%20data%20sources%20to%20improve%0Adiagnostic%20accuracy%20in%20clinical%20decisions.%20However%2C%20due%20to%20medical%20device%0Ashortages%2C%20low-quality%20data%20and%20data%20privacy%20concerns%2C%20missing%20data%20modalities%0Aare%20common%20in%20real-world%20scenarios.%20Existing%20deep%20learning%20methods%20tend%20to%0Aaddress%20it%20by%20learning%20an%20implicit%20latent%20subspace%20representation%20for%20different%0Amodality%20combinations.%20We%20identify%20two%20significant%20limitations%20of%20these%0Amethods%3A%20%281%29%20implicit%20representation%20constraints%20that%20hinder%20the%20model%27s%0Aability%20to%20capture%20modality-specific%20information%20and%20%282%29%20modality%0Aheterogeneity%2C%20causing%20distribution%20gaps%20and%20redundancy%20in%20feature%0Arepresentations.%20To%20address%20these%2C%20we%20propose%20an%20Incomplete%20Modality%0ADisentangled%20Representation%20%28IMDR%29%20strategy%2C%20which%20disentangles%20features%20into%0Aexplicit%20independent%20modal-common%20and%20modal-specific%20features%20by%20guidance%20of%0Amutual%20information%2C%20distilling%20informative%20knowledge%20and%20enabling%20it%20to%0Areconstruct%20valuable%20missing%20semantics%20and%20produce%20robust%20multimodal%0Arepresentations.%20Furthermore%2C%20we%20introduce%20a%20joint%20proxy%20learning%20module%20that%0Aassists%20IMDR%20in%20eliminating%20intra-modality%20redundancy%20by%20exploiting%20the%0Aextracted%20proxies%20from%20each%20class.%20Experiments%20on%20four%20ophthalmology%20multimodal%0Adatasets%20demonstrate%20that%20the%20proposed%20IMDR%20outperforms%20the%20state-of-the-art%0Amethods%20significantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11724v1&entry.124074799=Read"},
{"title": "FedEAT: A Robustness Optimization Framework for Federated LLMs", "author": "Yahao Pang and Xingyuan Wu and Xiaojin Zhang and Wei Chen and Hai Jin", "abstract": "  Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss.\n", "link": "http://arxiv.org/abs/2502.11863v1", "date": "2025-02-17", "relevancy": 2.0518, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5307}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5305}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedEAT%3A%20A%20Robustness%20Optimization%20Framework%20for%20Federated%20LLMs&body=Title%3A%20FedEAT%3A%20A%20Robustness%20Optimization%20Framework%20for%20Federated%20LLMs%0AAuthor%3A%20Yahao%20Pang%20and%20Xingyuan%20Wu%20and%20Xiaojin%20Zhang%20and%20Wei%20Chen%20and%20Hai%20Jin%0AAbstract%3A%20%20%20Significant%20advancements%20have%20been%20made%20by%20Large%20Language%20Models%20%28LLMs%29%20in%0Athe%20domains%20of%20natural%20language%20understanding%20and%20automated%20content%20creation.%0AHowever%2C%20they%20still%20face%20persistent%20problems%2C%20including%20substantial%0Acomputational%20costs%20and%20inadequate%20availability%20of%20training%20data.%20The%0Acombination%20of%20Federated%20Learning%20%28FL%29%20and%20LLMs%20%28federated%20LLMs%29%20offers%20a%0Asolution%20by%20leveraging%20distributed%20data%20while%20protecting%20privacy%2C%20which%0Apositions%20it%20as%20an%20ideal%20choice%20for%20sensitive%20domains.%20However%2C%20Federated%20LLMs%0Astill%20suffer%20from%20robustness%20challenges%2C%20including%20data%20heterogeneity%2C%0Amalicious%20clients%2C%20and%20adversarial%20attacks%2C%20which%20greatly%20hinder%20their%0Aapplications.%20We%20first%20introduce%20the%20robustness%20problems%20in%20federated%20LLMs%2C%20to%0Aaddress%20these%20challenges%2C%20we%20propose%20FedEAT%20%28Federated%20Embedding%20space%0AAdversarial%20Training%29%2C%20a%20novel%20framework%20that%20applies%20adversarial%20training%20in%0Athe%20embedding%20space%20of%20client%20LLM%20and%20employs%20a%20robust%20aggregation%20approach%2C%0Aspecifically%20geometric%20median%20aggregation%2C%20to%20enhance%20the%20robustness%20of%0AFederated%20LLMs.%20Our%20experiments%20demonstrate%20that%20FedEAT%20effectively%20improves%0Athe%20robustness%20of%20Federated%20LLMs%20with%20minimal%20performance%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedEAT%253A%2520A%2520Robustness%2520Optimization%2520Framework%2520for%2520Federated%2520LLMs%26entry.906535625%3DYahao%2520Pang%2520and%2520Xingyuan%2520Wu%2520and%2520Xiaojin%2520Zhang%2520and%2520Wei%2520Chen%2520and%2520Hai%2520Jin%26entry.1292438233%3D%2520%2520Significant%2520advancements%2520have%2520been%2520made%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%250Athe%2520domains%2520of%2520natural%2520language%2520understanding%2520and%2520automated%2520content%2520creation.%250AHowever%252C%2520they%2520still%2520face%2520persistent%2520problems%252C%2520including%2520substantial%250Acomputational%2520costs%2520and%2520inadequate%2520availability%2520of%2520training%2520data.%2520The%250Acombination%2520of%2520Federated%2520Learning%2520%2528FL%2529%2520and%2520LLMs%2520%2528federated%2520LLMs%2529%2520offers%2520a%250Asolution%2520by%2520leveraging%2520distributed%2520data%2520while%2520protecting%2520privacy%252C%2520which%250Apositions%2520it%2520as%2520an%2520ideal%2520choice%2520for%2520sensitive%2520domains.%2520However%252C%2520Federated%2520LLMs%250Astill%2520suffer%2520from%2520robustness%2520challenges%252C%2520including%2520data%2520heterogeneity%252C%250Amalicious%2520clients%252C%2520and%2520adversarial%2520attacks%252C%2520which%2520greatly%2520hinder%2520their%250Aapplications.%2520We%2520first%2520introduce%2520the%2520robustness%2520problems%2520in%2520federated%2520LLMs%252C%2520to%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520FedEAT%2520%2528Federated%2520Embedding%2520space%250AAdversarial%2520Training%2529%252C%2520a%2520novel%2520framework%2520that%2520applies%2520adversarial%2520training%2520in%250Athe%2520embedding%2520space%2520of%2520client%2520LLM%2520and%2520employs%2520a%2520robust%2520aggregation%2520approach%252C%250Aspecifically%2520geometric%2520median%2520aggregation%252C%2520to%2520enhance%2520the%2520robustness%2520of%250AFederated%2520LLMs.%2520Our%2520experiments%2520demonstrate%2520that%2520FedEAT%2520effectively%2520improves%250Athe%2520robustness%2520of%2520Federated%2520LLMs%2520with%2520minimal%2520performance%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedEAT%3A%20A%20Robustness%20Optimization%20Framework%20for%20Federated%20LLMs&entry.906535625=Yahao%20Pang%20and%20Xingyuan%20Wu%20and%20Xiaojin%20Zhang%20and%20Wei%20Chen%20and%20Hai%20Jin&entry.1292438233=%20%20Significant%20advancements%20have%20been%20made%20by%20Large%20Language%20Models%20%28LLMs%29%20in%0Athe%20domains%20of%20natural%20language%20understanding%20and%20automated%20content%20creation.%0AHowever%2C%20they%20still%20face%20persistent%20problems%2C%20including%20substantial%0Acomputational%20costs%20and%20inadequate%20availability%20of%20training%20data.%20The%0Acombination%20of%20Federated%20Learning%20%28FL%29%20and%20LLMs%20%28federated%20LLMs%29%20offers%20a%0Asolution%20by%20leveraging%20distributed%20data%20while%20protecting%20privacy%2C%20which%0Apositions%20it%20as%20an%20ideal%20choice%20for%20sensitive%20domains.%20However%2C%20Federated%20LLMs%0Astill%20suffer%20from%20robustness%20challenges%2C%20including%20data%20heterogeneity%2C%0Amalicious%20clients%2C%20and%20adversarial%20attacks%2C%20which%20greatly%20hinder%20their%0Aapplications.%20We%20first%20introduce%20the%20robustness%20problems%20in%20federated%20LLMs%2C%20to%0Aaddress%20these%20challenges%2C%20we%20propose%20FedEAT%20%28Federated%20Embedding%20space%0AAdversarial%20Training%29%2C%20a%20novel%20framework%20that%20applies%20adversarial%20training%20in%0Athe%20embedding%20space%20of%20client%20LLM%20and%20employs%20a%20robust%20aggregation%20approach%2C%0Aspecifically%20geometric%20median%20aggregation%2C%20to%20enhance%20the%20robustness%20of%0AFederated%20LLMs.%20Our%20experiments%20demonstrate%20that%20FedEAT%20effectively%20improves%0Athe%20robustness%20of%20Federated%20LLMs%20with%20minimal%20performance%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11863v1&entry.124074799=Read"},
{"title": "DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without\n  Domain-Specific Factors", "author": "Keon Lee and Dong Won Kim and Jaehyeon Kim and Seungjun Chung and Jaewoong Cho", "abstract": "  Large-scale latent diffusion models (LDMs) excel in content generation across\nvarious modalities, but their reliance on phonemes and durations in\ntext-to-speech (TTS) limits scalability and access from other fields. While\nrecent studies show potential in removing these domain-specific factors,\nperformance remains suboptimal. In this work, we introduce DiTTo-TTS, a\nDiffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based\nTTS can achieve state-of-the-art performance without domain-specific factors.\nThrough rigorous analysis and empirical exploration, we find that (1) DiT with\nminimal modifications outperforms U-Net, (2) variable-length modeling with a\nspeech length predictor significantly improves results over fixed-length\napproaches, and (3) conditions like semantic alignment in speech latent\nrepresentations are key to further enhancement. By scaling our training data to\n82K hours and the model size to 790M parameters, we achieve superior or\ncomparable zero-shot performance to state-of-the-art TTS models in naturalness,\nintelligibility, and speaker similarity, all without relying on domain-specific\nfactors. Speech samples are available at https://ditto-tts.github.io.\n", "link": "http://arxiv.org/abs/2406.11427v2", "date": "2025-02-17", "relevancy": 1.7399, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.632}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5867}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiTTo-TTS%3A%20Diffusion%20Transformers%20for%20Scalable%20Text-to-Speech%20without%0A%20%20Domain-Specific%20Factors&body=Title%3A%20DiTTo-TTS%3A%20Diffusion%20Transformers%20for%20Scalable%20Text-to-Speech%20without%0A%20%20Domain-Specific%20Factors%0AAuthor%3A%20Keon%20Lee%20and%20Dong%20Won%20Kim%20and%20Jaehyeon%20Kim%20and%20Seungjun%20Chung%20and%20Jaewoong%20Cho%0AAbstract%3A%20%20%20Large-scale%20latent%20diffusion%20models%20%28LDMs%29%20excel%20in%20content%20generation%20across%0Avarious%20modalities%2C%20but%20their%20reliance%20on%20phonemes%20and%20durations%20in%0Atext-to-speech%20%28TTS%29%20limits%20scalability%20and%20access%20from%20other%20fields.%20While%0Arecent%20studies%20show%20potential%20in%20removing%20these%20domain-specific%20factors%2C%0Aperformance%20remains%20suboptimal.%20In%20this%20work%2C%20we%20introduce%20DiTTo-TTS%2C%20a%0ADiffusion%20Transformer%20%28DiT%29-based%20TTS%20model%2C%20to%20investigate%20whether%20LDM-based%0ATTS%20can%20achieve%20state-of-the-art%20performance%20without%20domain-specific%20factors.%0AThrough%20rigorous%20analysis%20and%20empirical%20exploration%2C%20we%20find%20that%20%281%29%20DiT%20with%0Aminimal%20modifications%20outperforms%20U-Net%2C%20%282%29%20variable-length%20modeling%20with%20a%0Aspeech%20length%20predictor%20significantly%20improves%20results%20over%20fixed-length%0Aapproaches%2C%20and%20%283%29%20conditions%20like%20semantic%20alignment%20in%20speech%20latent%0Arepresentations%20are%20key%20to%20further%20enhancement.%20By%20scaling%20our%20training%20data%20to%0A82K%20hours%20and%20the%20model%20size%20to%20790M%20parameters%2C%20we%20achieve%20superior%20or%0Acomparable%20zero-shot%20performance%20to%20state-of-the-art%20TTS%20models%20in%20naturalness%2C%0Aintelligibility%2C%20and%20speaker%20similarity%2C%20all%20without%20relying%20on%20domain-specific%0Afactors.%20Speech%20samples%20are%20available%20at%20https%3A//ditto-tts.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiTTo-TTS%253A%2520Diffusion%2520Transformers%2520for%2520Scalable%2520Text-to-Speech%2520without%250A%2520%2520Domain-Specific%2520Factors%26entry.906535625%3DKeon%2520Lee%2520and%2520Dong%2520Won%2520Kim%2520and%2520Jaehyeon%2520Kim%2520and%2520Seungjun%2520Chung%2520and%2520Jaewoong%2520Cho%26entry.1292438233%3D%2520%2520Large-scale%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520excel%2520in%2520content%2520generation%2520across%250Avarious%2520modalities%252C%2520but%2520their%2520reliance%2520on%2520phonemes%2520and%2520durations%2520in%250Atext-to-speech%2520%2528TTS%2529%2520limits%2520scalability%2520and%2520access%2520from%2520other%2520fields.%2520While%250Arecent%2520studies%2520show%2520potential%2520in%2520removing%2520these%2520domain-specific%2520factors%252C%250Aperformance%2520remains%2520suboptimal.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DiTTo-TTS%252C%2520a%250ADiffusion%2520Transformer%2520%2528DiT%2529-based%2520TTS%2520model%252C%2520to%2520investigate%2520whether%2520LDM-based%250ATTS%2520can%2520achieve%2520state-of-the-art%2520performance%2520without%2520domain-specific%2520factors.%250AThrough%2520rigorous%2520analysis%2520and%2520empirical%2520exploration%252C%2520we%2520find%2520that%2520%25281%2529%2520DiT%2520with%250Aminimal%2520modifications%2520outperforms%2520U-Net%252C%2520%25282%2529%2520variable-length%2520modeling%2520with%2520a%250Aspeech%2520length%2520predictor%2520significantly%2520improves%2520results%2520over%2520fixed-length%250Aapproaches%252C%2520and%2520%25283%2529%2520conditions%2520like%2520semantic%2520alignment%2520in%2520speech%2520latent%250Arepresentations%2520are%2520key%2520to%2520further%2520enhancement.%2520By%2520scaling%2520our%2520training%2520data%2520to%250A82K%2520hours%2520and%2520the%2520model%2520size%2520to%2520790M%2520parameters%252C%2520we%2520achieve%2520superior%2520or%250Acomparable%2520zero-shot%2520performance%2520to%2520state-of-the-art%2520TTS%2520models%2520in%2520naturalness%252C%250Aintelligibility%252C%2520and%2520speaker%2520similarity%252C%2520all%2520without%2520relying%2520on%2520domain-specific%250Afactors.%2520Speech%2520samples%2520are%2520available%2520at%2520https%253A//ditto-tts.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiTTo-TTS%3A%20Diffusion%20Transformers%20for%20Scalable%20Text-to-Speech%20without%0A%20%20Domain-Specific%20Factors&entry.906535625=Keon%20Lee%20and%20Dong%20Won%20Kim%20and%20Jaehyeon%20Kim%20and%20Seungjun%20Chung%20and%20Jaewoong%20Cho&entry.1292438233=%20%20Large-scale%20latent%20diffusion%20models%20%28LDMs%29%20excel%20in%20content%20generation%20across%0Avarious%20modalities%2C%20but%20their%20reliance%20on%20phonemes%20and%20durations%20in%0Atext-to-speech%20%28TTS%29%20limits%20scalability%20and%20access%20from%20other%20fields.%20While%0Arecent%20studies%20show%20potential%20in%20removing%20these%20domain-specific%20factors%2C%0Aperformance%20remains%20suboptimal.%20In%20this%20work%2C%20we%20introduce%20DiTTo-TTS%2C%20a%0ADiffusion%20Transformer%20%28DiT%29-based%20TTS%20model%2C%20to%20investigate%20whether%20LDM-based%0ATTS%20can%20achieve%20state-of-the-art%20performance%20without%20domain-specific%20factors.%0AThrough%20rigorous%20analysis%20and%20empirical%20exploration%2C%20we%20find%20that%20%281%29%20DiT%20with%0Aminimal%20modifications%20outperforms%20U-Net%2C%20%282%29%20variable-length%20modeling%20with%20a%0Aspeech%20length%20predictor%20significantly%20improves%20results%20over%20fixed-length%0Aapproaches%2C%20and%20%283%29%20conditions%20like%20semantic%20alignment%20in%20speech%20latent%0Arepresentations%20are%20key%20to%20further%20enhancement.%20By%20scaling%20our%20training%20data%20to%0A82K%20hours%20and%20the%20model%20size%20to%20790M%20parameters%2C%20we%20achieve%20superior%20or%0Acomparable%20zero-shot%20performance%20to%20state-of-the-art%20TTS%20models%20in%20naturalness%2C%0Aintelligibility%2C%20and%20speaker%20similarity%2C%20all%20without%20relying%20on%20domain-specific%0Afactors.%20Speech%20samples%20are%20available%20at%20https%3A//ditto-tts.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11427v2&entry.124074799=Read"},
{"title": "Unsupervised Structural-Counterfactual Generation under Domain Shift", "author": "Krishn Vishwas Kher and Lokesh Venkata Siva Maruthi Badisa and Kusampudi Venkata Datta Sri Harsha and Chitneedi Geetha Sowmya and SakethaNath Jagarlapudi", "abstract": "  Motivated by the burgeoning interest in cross-domain learning, we present a\nnovel generative modeling challenge: generating counterfactual samples in a\ntarget domain based on factual observations from a source domain. Our approach\noperates within an unsupervised paradigm devoid of parallel or joint datasets,\nrelying exclusively on distinct observational samples and causal graphs for\neach domain. This setting presents challenges that surpass those of\nconventional counterfactual generation. Central to our methodology is the\ndisambiguation of exogenous causes into effect-intrinsic and domain-intrinsic\ncategories. This differentiation facilitates the integration of domain-specific\ncausal graphs into a unified joint causal graph via shared effect-intrinsic\nexogenous variables. We propose leveraging Neural Causal models within this\njoint framework to enable accurate counterfactual generation under standard\nidentifiability assumptions. Furthermore, we introduce a novel loss function\nthat effectively segregates effect-intrinsic from domain-intrinsic variables\nduring model training. Given a factual observation, our framework combines the\nposterior distribution of effect-intrinsic variables from the source domain\nwith the prior distribution of domain-intrinsic variables from the target\ndomain to synthesize the desired counterfactuals, adhering to Pearl's causal\nhierarchy. Intriguingly, when domain shifts are restricted to alterations in\ncausal mechanisms without accompanying covariate shifts, our training regimen\nparallels the resolution of a conditional optimal transport problem. Empirical\nevaluations on a synthetic dataset show that our framework generates\ncounterfactuals in the target domain that very closely resemble the ground\ntruth.\n", "link": "http://arxiv.org/abs/2502.12013v1", "date": "2025-02-17", "relevancy": 2.1221, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5742}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5403}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Structural-Counterfactual%20Generation%20under%20Domain%20Shift&body=Title%3A%20Unsupervised%20Structural-Counterfactual%20Generation%20under%20Domain%20Shift%0AAuthor%3A%20Krishn%20Vishwas%20Kher%20and%20Lokesh%20Venkata%20Siva%20Maruthi%20Badisa%20and%20Kusampudi%20Venkata%20Datta%20Sri%20Harsha%20and%20Chitneedi%20Geetha%20Sowmya%20and%20SakethaNath%20Jagarlapudi%0AAbstract%3A%20%20%20Motivated%20by%20the%20burgeoning%20interest%20in%20cross-domain%20learning%2C%20we%20present%20a%0Anovel%20generative%20modeling%20challenge%3A%20generating%20counterfactual%20samples%20in%20a%0Atarget%20domain%20based%20on%20factual%20observations%20from%20a%20source%20domain.%20Our%20approach%0Aoperates%20within%20an%20unsupervised%20paradigm%20devoid%20of%20parallel%20or%20joint%20datasets%2C%0Arelying%20exclusively%20on%20distinct%20observational%20samples%20and%20causal%20graphs%20for%0Aeach%20domain.%20This%20setting%20presents%20challenges%20that%20surpass%20those%20of%0Aconventional%20counterfactual%20generation.%20Central%20to%20our%20methodology%20is%20the%0Adisambiguation%20of%20exogenous%20causes%20into%20effect-intrinsic%20and%20domain-intrinsic%0Acategories.%20This%20differentiation%20facilitates%20the%20integration%20of%20domain-specific%0Acausal%20graphs%20into%20a%20unified%20joint%20causal%20graph%20via%20shared%20effect-intrinsic%0Aexogenous%20variables.%20We%20propose%20leveraging%20Neural%20Causal%20models%20within%20this%0Ajoint%20framework%20to%20enable%20accurate%20counterfactual%20generation%20under%20standard%0Aidentifiability%20assumptions.%20Furthermore%2C%20we%20introduce%20a%20novel%20loss%20function%0Athat%20effectively%20segregates%20effect-intrinsic%20from%20domain-intrinsic%20variables%0Aduring%20model%20training.%20Given%20a%20factual%20observation%2C%20our%20framework%20combines%20the%0Aposterior%20distribution%20of%20effect-intrinsic%20variables%20from%20the%20source%20domain%0Awith%20the%20prior%20distribution%20of%20domain-intrinsic%20variables%20from%20the%20target%0Adomain%20to%20synthesize%20the%20desired%20counterfactuals%2C%20adhering%20to%20Pearl%27s%20causal%0Ahierarchy.%20Intriguingly%2C%20when%20domain%20shifts%20are%20restricted%20to%20alterations%20in%0Acausal%20mechanisms%20without%20accompanying%20covariate%20shifts%2C%20our%20training%20regimen%0Aparallels%20the%20resolution%20of%20a%20conditional%20optimal%20transport%20problem.%20Empirical%0Aevaluations%20on%20a%20synthetic%20dataset%20show%20that%20our%20framework%20generates%0Acounterfactuals%20in%20the%20target%20domain%20that%20very%20closely%20resemble%20the%20ground%0Atruth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Structural-Counterfactual%2520Generation%2520under%2520Domain%2520Shift%26entry.906535625%3DKrishn%2520Vishwas%2520Kher%2520and%2520Lokesh%2520Venkata%2520Siva%2520Maruthi%2520Badisa%2520and%2520Kusampudi%2520Venkata%2520Datta%2520Sri%2520Harsha%2520and%2520Chitneedi%2520Geetha%2520Sowmya%2520and%2520SakethaNath%2520Jagarlapudi%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520burgeoning%2520interest%2520in%2520cross-domain%2520learning%252C%2520we%2520present%2520a%250Anovel%2520generative%2520modeling%2520challenge%253A%2520generating%2520counterfactual%2520samples%2520in%2520a%250Atarget%2520domain%2520based%2520on%2520factual%2520observations%2520from%2520a%2520source%2520domain.%2520Our%2520approach%250Aoperates%2520within%2520an%2520unsupervised%2520paradigm%2520devoid%2520of%2520parallel%2520or%2520joint%2520datasets%252C%250Arelying%2520exclusively%2520on%2520distinct%2520observational%2520samples%2520and%2520causal%2520graphs%2520for%250Aeach%2520domain.%2520This%2520setting%2520presents%2520challenges%2520that%2520surpass%2520those%2520of%250Aconventional%2520counterfactual%2520generation.%2520Central%2520to%2520our%2520methodology%2520is%2520the%250Adisambiguation%2520of%2520exogenous%2520causes%2520into%2520effect-intrinsic%2520and%2520domain-intrinsic%250Acategories.%2520This%2520differentiation%2520facilitates%2520the%2520integration%2520of%2520domain-specific%250Acausal%2520graphs%2520into%2520a%2520unified%2520joint%2520causal%2520graph%2520via%2520shared%2520effect-intrinsic%250Aexogenous%2520variables.%2520We%2520propose%2520leveraging%2520Neural%2520Causal%2520models%2520within%2520this%250Ajoint%2520framework%2520to%2520enable%2520accurate%2520counterfactual%2520generation%2520under%2520standard%250Aidentifiability%2520assumptions.%2520Furthermore%252C%2520we%2520introduce%2520a%2520novel%2520loss%2520function%250Athat%2520effectively%2520segregates%2520effect-intrinsic%2520from%2520domain-intrinsic%2520variables%250Aduring%2520model%2520training.%2520Given%2520a%2520factual%2520observation%252C%2520our%2520framework%2520combines%2520the%250Aposterior%2520distribution%2520of%2520effect-intrinsic%2520variables%2520from%2520the%2520source%2520domain%250Awith%2520the%2520prior%2520distribution%2520of%2520domain-intrinsic%2520variables%2520from%2520the%2520target%250Adomain%2520to%2520synthesize%2520the%2520desired%2520counterfactuals%252C%2520adhering%2520to%2520Pearl%2527s%2520causal%250Ahierarchy.%2520Intriguingly%252C%2520when%2520domain%2520shifts%2520are%2520restricted%2520to%2520alterations%2520in%250Acausal%2520mechanisms%2520without%2520accompanying%2520covariate%2520shifts%252C%2520our%2520training%2520regimen%250Aparallels%2520the%2520resolution%2520of%2520a%2520conditional%2520optimal%2520transport%2520problem.%2520Empirical%250Aevaluations%2520on%2520a%2520synthetic%2520dataset%2520show%2520that%2520our%2520framework%2520generates%250Acounterfactuals%2520in%2520the%2520target%2520domain%2520that%2520very%2520closely%2520resemble%2520the%2520ground%250Atruth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Structural-Counterfactual%20Generation%20under%20Domain%20Shift&entry.906535625=Krishn%20Vishwas%20Kher%20and%20Lokesh%20Venkata%20Siva%20Maruthi%20Badisa%20and%20Kusampudi%20Venkata%20Datta%20Sri%20Harsha%20and%20Chitneedi%20Geetha%20Sowmya%20and%20SakethaNath%20Jagarlapudi&entry.1292438233=%20%20Motivated%20by%20the%20burgeoning%20interest%20in%20cross-domain%20learning%2C%20we%20present%20a%0Anovel%20generative%20modeling%20challenge%3A%20generating%20counterfactual%20samples%20in%20a%0Atarget%20domain%20based%20on%20factual%20observations%20from%20a%20source%20domain.%20Our%20approach%0Aoperates%20within%20an%20unsupervised%20paradigm%20devoid%20of%20parallel%20or%20joint%20datasets%2C%0Arelying%20exclusively%20on%20distinct%20observational%20samples%20and%20causal%20graphs%20for%0Aeach%20domain.%20This%20setting%20presents%20challenges%20that%20surpass%20those%20of%0Aconventional%20counterfactual%20generation.%20Central%20to%20our%20methodology%20is%20the%0Adisambiguation%20of%20exogenous%20causes%20into%20effect-intrinsic%20and%20domain-intrinsic%0Acategories.%20This%20differentiation%20facilitates%20the%20integration%20of%20domain-specific%0Acausal%20graphs%20into%20a%20unified%20joint%20causal%20graph%20via%20shared%20effect-intrinsic%0Aexogenous%20variables.%20We%20propose%20leveraging%20Neural%20Causal%20models%20within%20this%0Ajoint%20framework%20to%20enable%20accurate%20counterfactual%20generation%20under%20standard%0Aidentifiability%20assumptions.%20Furthermore%2C%20we%20introduce%20a%20novel%20loss%20function%0Athat%20effectively%20segregates%20effect-intrinsic%20from%20domain-intrinsic%20variables%0Aduring%20model%20training.%20Given%20a%20factual%20observation%2C%20our%20framework%20combines%20the%0Aposterior%20distribution%20of%20effect-intrinsic%20variables%20from%20the%20source%20domain%0Awith%20the%20prior%20distribution%20of%20domain-intrinsic%20variables%20from%20the%20target%0Adomain%20to%20synthesize%20the%20desired%20counterfactuals%2C%20adhering%20to%20Pearl%27s%20causal%0Ahierarchy.%20Intriguingly%2C%20when%20domain%20shifts%20are%20restricted%20to%20alterations%20in%0Acausal%20mechanisms%20without%20accompanying%20covariate%20shifts%2C%20our%20training%20regimen%0Aparallels%20the%20resolution%20of%20a%20conditional%20optimal%20transport%20problem.%20Empirical%0Aevaluations%20on%20a%20synthetic%20dataset%20show%20that%20our%20framework%20generates%0Acounterfactuals%20in%20the%20target%20domain%20that%20very%20closely%20resemble%20the%20ground%0Atruth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12013v1&entry.124074799=Read"},
{"title": "Qubit-Based Framework for Quantum Machine Learning: Bridging Classical\n  Data and Quantum Algorithms", "author": "Bhavna Bose and Saurav Verma", "abstract": "  This paper dives into the exciting and rapidly growing field of quantum\ncomputing, explaining its core ideas, current progress, and how it could\nrevolutionize the way we solve complex problems. It starts by breaking down the\nbasics, like qubits, quantum circuits, and how principles like superposition\nand entanglement make quantum computers fundamentally different-and far more\npowerful for certain tasks-than the classical computers we use today. We also\nexplore how quantum computing deals with complex problems and why it is\nuniquely suited for challenges classical systems struggle to handle. A big part\nof this paper focuses on Quantum Machine Learning (QML), where the strengths of\nquantum computing meet the world of artificial intelligence. By processing\nmassive datasets and optimizing intricate algorithms, quantum systems offer new\npossibilities for machine learning. We highlight different approaches to\ncombining quantum and classical computing, showing how they can work together\nto produce faster and more accurate results. Additionally, we explore the tools\nand platforms available-like TensorFlow Quantum, Qiskit and PennyLane-that are\nhelping researchers and developers bring these theories to life. Of course,\nquantum computing has its hurdles. Challenges like scaling up hardware,\ncorrecting errors, and keeping qubits stable are significant roadblocks. Yet,\nwith rapid advancements in cloud-based platforms and innovative technologies,\nthe potential of quantum computing feels closer than ever. This paper aims to\noffer readers a clear and comprehensive introduction to quantum computing, its\nrole in machine learning, and the immense possibilities it holds for the future\nof technology.\n", "link": "http://arxiv.org/abs/2502.11951v1", "date": "2025-02-17", "relevancy": 1.6616, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qubit-Based%20Framework%20for%20Quantum%20Machine%20Learning%3A%20Bridging%20Classical%0A%20%20Data%20and%20Quantum%20Algorithms&body=Title%3A%20Qubit-Based%20Framework%20for%20Quantum%20Machine%20Learning%3A%20Bridging%20Classical%0A%20%20Data%20and%20Quantum%20Algorithms%0AAuthor%3A%20Bhavna%20Bose%20and%20Saurav%20Verma%0AAbstract%3A%20%20%20This%20paper%20dives%20into%20the%20exciting%20and%20rapidly%20growing%20field%20of%20quantum%0Acomputing%2C%20explaining%20its%20core%20ideas%2C%20current%20progress%2C%20and%20how%20it%20could%0Arevolutionize%20the%20way%20we%20solve%20complex%20problems.%20It%20starts%20by%20breaking%20down%20the%0Abasics%2C%20like%20qubits%2C%20quantum%20circuits%2C%20and%20how%20principles%20like%20superposition%0Aand%20entanglement%20make%20quantum%20computers%20fundamentally%20different-and%20far%20more%0Apowerful%20for%20certain%20tasks-than%20the%20classical%20computers%20we%20use%20today.%20We%20also%0Aexplore%20how%20quantum%20computing%20deals%20with%20complex%20problems%20and%20why%20it%20is%0Auniquely%20suited%20for%20challenges%20classical%20systems%20struggle%20to%20handle.%20A%20big%20part%0Aof%20this%20paper%20focuses%20on%20Quantum%20Machine%20Learning%20%28QML%29%2C%20where%20the%20strengths%20of%0Aquantum%20computing%20meet%20the%20world%20of%20artificial%20intelligence.%20By%20processing%0Amassive%20datasets%20and%20optimizing%20intricate%20algorithms%2C%20quantum%20systems%20offer%20new%0Apossibilities%20for%20machine%20learning.%20We%20highlight%20different%20approaches%20to%0Acombining%20quantum%20and%20classical%20computing%2C%20showing%20how%20they%20can%20work%20together%0Ato%20produce%20faster%20and%20more%20accurate%20results.%20Additionally%2C%20we%20explore%20the%20tools%0Aand%20platforms%20available-like%20TensorFlow%20Quantum%2C%20Qiskit%20and%20PennyLane-that%20are%0Ahelping%20researchers%20and%20developers%20bring%20these%20theories%20to%20life.%20Of%20course%2C%0Aquantum%20computing%20has%20its%20hurdles.%20Challenges%20like%20scaling%20up%20hardware%2C%0Acorrecting%20errors%2C%20and%20keeping%20qubits%20stable%20are%20significant%20roadblocks.%20Yet%2C%0Awith%20rapid%20advancements%20in%20cloud-based%20platforms%20and%20innovative%20technologies%2C%0Athe%20potential%20of%20quantum%20computing%20feels%20closer%20than%20ever.%20This%20paper%20aims%20to%0Aoffer%20readers%20a%20clear%20and%20comprehensive%20introduction%20to%20quantum%20computing%2C%20its%0Arole%20in%20machine%20learning%2C%20and%20the%20immense%20possibilities%20it%20holds%20for%20the%20future%0Aof%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQubit-Based%2520Framework%2520for%2520Quantum%2520Machine%2520Learning%253A%2520Bridging%2520Classical%250A%2520%2520Data%2520and%2520Quantum%2520Algorithms%26entry.906535625%3DBhavna%2520Bose%2520and%2520Saurav%2520Verma%26entry.1292438233%3D%2520%2520This%2520paper%2520dives%2520into%2520the%2520exciting%2520and%2520rapidly%2520growing%2520field%2520of%2520quantum%250Acomputing%252C%2520explaining%2520its%2520core%2520ideas%252C%2520current%2520progress%252C%2520and%2520how%2520it%2520could%250Arevolutionize%2520the%2520way%2520we%2520solve%2520complex%2520problems.%2520It%2520starts%2520by%2520breaking%2520down%2520the%250Abasics%252C%2520like%2520qubits%252C%2520quantum%2520circuits%252C%2520and%2520how%2520principles%2520like%2520superposition%250Aand%2520entanglement%2520make%2520quantum%2520computers%2520fundamentally%2520different-and%2520far%2520more%250Apowerful%2520for%2520certain%2520tasks-than%2520the%2520classical%2520computers%2520we%2520use%2520today.%2520We%2520also%250Aexplore%2520how%2520quantum%2520computing%2520deals%2520with%2520complex%2520problems%2520and%2520why%2520it%2520is%250Auniquely%2520suited%2520for%2520challenges%2520classical%2520systems%2520struggle%2520to%2520handle.%2520A%2520big%2520part%250Aof%2520this%2520paper%2520focuses%2520on%2520Quantum%2520Machine%2520Learning%2520%2528QML%2529%252C%2520where%2520the%2520strengths%2520of%250Aquantum%2520computing%2520meet%2520the%2520world%2520of%2520artificial%2520intelligence.%2520By%2520processing%250Amassive%2520datasets%2520and%2520optimizing%2520intricate%2520algorithms%252C%2520quantum%2520systems%2520offer%2520new%250Apossibilities%2520for%2520machine%2520learning.%2520We%2520highlight%2520different%2520approaches%2520to%250Acombining%2520quantum%2520and%2520classical%2520computing%252C%2520showing%2520how%2520they%2520can%2520work%2520together%250Ato%2520produce%2520faster%2520and%2520more%2520accurate%2520results.%2520Additionally%252C%2520we%2520explore%2520the%2520tools%250Aand%2520platforms%2520available-like%2520TensorFlow%2520Quantum%252C%2520Qiskit%2520and%2520PennyLane-that%2520are%250Ahelping%2520researchers%2520and%2520developers%2520bring%2520these%2520theories%2520to%2520life.%2520Of%2520course%252C%250Aquantum%2520computing%2520has%2520its%2520hurdles.%2520Challenges%2520like%2520scaling%2520up%2520hardware%252C%250Acorrecting%2520errors%252C%2520and%2520keeping%2520qubits%2520stable%2520are%2520significant%2520roadblocks.%2520Yet%252C%250Awith%2520rapid%2520advancements%2520in%2520cloud-based%2520platforms%2520and%2520innovative%2520technologies%252C%250Athe%2520potential%2520of%2520quantum%2520computing%2520feels%2520closer%2520than%2520ever.%2520This%2520paper%2520aims%2520to%250Aoffer%2520readers%2520a%2520clear%2520and%2520comprehensive%2520introduction%2520to%2520quantum%2520computing%252C%2520its%250Arole%2520in%2520machine%2520learning%252C%2520and%2520the%2520immense%2520possibilities%2520it%2520holds%2520for%2520the%2520future%250Aof%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qubit-Based%20Framework%20for%20Quantum%20Machine%20Learning%3A%20Bridging%20Classical%0A%20%20Data%20and%20Quantum%20Algorithms&entry.906535625=Bhavna%20Bose%20and%20Saurav%20Verma&entry.1292438233=%20%20This%20paper%20dives%20into%20the%20exciting%20and%20rapidly%20growing%20field%20of%20quantum%0Acomputing%2C%20explaining%20its%20core%20ideas%2C%20current%20progress%2C%20and%20how%20it%20could%0Arevolutionize%20the%20way%20we%20solve%20complex%20problems.%20It%20starts%20by%20breaking%20down%20the%0Abasics%2C%20like%20qubits%2C%20quantum%20circuits%2C%20and%20how%20principles%20like%20superposition%0Aand%20entanglement%20make%20quantum%20computers%20fundamentally%20different-and%20far%20more%0Apowerful%20for%20certain%20tasks-than%20the%20classical%20computers%20we%20use%20today.%20We%20also%0Aexplore%20how%20quantum%20computing%20deals%20with%20complex%20problems%20and%20why%20it%20is%0Auniquely%20suited%20for%20challenges%20classical%20systems%20struggle%20to%20handle.%20A%20big%20part%0Aof%20this%20paper%20focuses%20on%20Quantum%20Machine%20Learning%20%28QML%29%2C%20where%20the%20strengths%20of%0Aquantum%20computing%20meet%20the%20world%20of%20artificial%20intelligence.%20By%20processing%0Amassive%20datasets%20and%20optimizing%20intricate%20algorithms%2C%20quantum%20systems%20offer%20new%0Apossibilities%20for%20machine%20learning.%20We%20highlight%20different%20approaches%20to%0Acombining%20quantum%20and%20classical%20computing%2C%20showing%20how%20they%20can%20work%20together%0Ato%20produce%20faster%20and%20more%20accurate%20results.%20Additionally%2C%20we%20explore%20the%20tools%0Aand%20platforms%20available-like%20TensorFlow%20Quantum%2C%20Qiskit%20and%20PennyLane-that%20are%0Ahelping%20researchers%20and%20developers%20bring%20these%20theories%20to%20life.%20Of%20course%2C%0Aquantum%20computing%20has%20its%20hurdles.%20Challenges%20like%20scaling%20up%20hardware%2C%0Acorrecting%20errors%2C%20and%20keeping%20qubits%20stable%20are%20significant%20roadblocks.%20Yet%2C%0Awith%20rapid%20advancements%20in%20cloud-based%20platforms%20and%20innovative%20technologies%2C%0Athe%20potential%20of%20quantum%20computing%20feels%20closer%20than%20ever.%20This%20paper%20aims%20to%0Aoffer%20readers%20a%20clear%20and%20comprehensive%20introduction%20to%20quantum%20computing%2C%20its%0Arole%20in%20machine%20learning%2C%20and%20the%20immense%20possibilities%20it%20holds%20for%20the%20future%0Aof%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11951v1&entry.124074799=Read"},
{"title": "Bandwidth-Adaptive Spatiotemporal Correspondence Identification for\n  Collaborative Perception", "author": "Peng Gao and Williard Joshua Jose and Hao Zhang", "abstract": "  Correspondence identification (CoID) is an essential capability in\nmulti-robot collaborative perception, which enables a group of robots to\nconsistently refer to the same objects within their respective fields of view.\nIn real-world applications, such as connected autonomous driving, vehicles face\nchallenges in directly sharing raw observations due to limited communication\nbandwidth. In order to address this challenge, we propose a novel approach for\nbandwidth-adaptive spatiotemporal CoID in collaborative perception. This\napproach allows robots to progressively select partial spatiotemporal\nobservations and share with others, while adapting to communication constraints\nthat dynamically change over time. We evaluate our approach across various\nscenarios in connected autonomous driving simulations. Experimental results\nvalidate that our approach enables CoID and adapts to dynamic communication\nbandwidth changes. In addition, our approach achieves 8%-56% overall\nimprovements in terms of covisible object retrieval for CoID and data sharing\nefficiency, which outperforms previous techniques and achieves the\nstate-of-the-art performance. More information is available at:\nhttps://gaopeng5.github.io/acoid.\n", "link": "http://arxiv.org/abs/2502.12098v1", "date": "2025-02-17", "relevancy": 2.0796, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5501}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5297}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bandwidth-Adaptive%20Spatiotemporal%20Correspondence%20Identification%20for%0A%20%20Collaborative%20Perception&body=Title%3A%20Bandwidth-Adaptive%20Spatiotemporal%20Correspondence%20Identification%20for%0A%20%20Collaborative%20Perception%0AAuthor%3A%20Peng%20Gao%20and%20Williard%20Joshua%20Jose%20and%20Hao%20Zhang%0AAbstract%3A%20%20%20Correspondence%20identification%20%28CoID%29%20is%20an%20essential%20capability%20in%0Amulti-robot%20collaborative%20perception%2C%20which%20enables%20a%20group%20of%20robots%20to%0Aconsistently%20refer%20to%20the%20same%20objects%20within%20their%20respective%20fields%20of%20view.%0AIn%20real-world%20applications%2C%20such%20as%20connected%20autonomous%20driving%2C%20vehicles%20face%0Achallenges%20in%20directly%20sharing%20raw%20observations%20due%20to%20limited%20communication%0Abandwidth.%20In%20order%20to%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20approach%20for%0Abandwidth-adaptive%20spatiotemporal%20CoID%20in%20collaborative%20perception.%20This%0Aapproach%20allows%20robots%20to%20progressively%20select%20partial%20spatiotemporal%0Aobservations%20and%20share%20with%20others%2C%20while%20adapting%20to%20communication%20constraints%0Athat%20dynamically%20change%20over%20time.%20We%20evaluate%20our%20approach%20across%20various%0Ascenarios%20in%20connected%20autonomous%20driving%20simulations.%20Experimental%20results%0Avalidate%20that%20our%20approach%20enables%20CoID%20and%20adapts%20to%20dynamic%20communication%0Abandwidth%20changes.%20In%20addition%2C%20our%20approach%20achieves%208%25-56%25%20overall%0Aimprovements%20in%20terms%20of%20covisible%20object%20retrieval%20for%20CoID%20and%20data%20sharing%0Aefficiency%2C%20which%20outperforms%20previous%20techniques%20and%20achieves%20the%0Astate-of-the-art%20performance.%20More%20information%20is%20available%20at%3A%0Ahttps%3A//gaopeng5.github.io/acoid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBandwidth-Adaptive%2520Spatiotemporal%2520Correspondence%2520Identification%2520for%250A%2520%2520Collaborative%2520Perception%26entry.906535625%3DPeng%2520Gao%2520and%2520Williard%2520Joshua%2520Jose%2520and%2520Hao%2520Zhang%26entry.1292438233%3D%2520%2520Correspondence%2520identification%2520%2528CoID%2529%2520is%2520an%2520essential%2520capability%2520in%250Amulti-robot%2520collaborative%2520perception%252C%2520which%2520enables%2520a%2520group%2520of%2520robots%2520to%250Aconsistently%2520refer%2520to%2520the%2520same%2520objects%2520within%2520their%2520respective%2520fields%2520of%2520view.%250AIn%2520real-world%2520applications%252C%2520such%2520as%2520connected%2520autonomous%2520driving%252C%2520vehicles%2520face%250Achallenges%2520in%2520directly%2520sharing%2520raw%2520observations%2520due%2520to%2520limited%2520communication%250Abandwidth.%2520In%2520order%2520to%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%250Abandwidth-adaptive%2520spatiotemporal%2520CoID%2520in%2520collaborative%2520perception.%2520This%250Aapproach%2520allows%2520robots%2520to%2520progressively%2520select%2520partial%2520spatiotemporal%250Aobservations%2520and%2520share%2520with%2520others%252C%2520while%2520adapting%2520to%2520communication%2520constraints%250Athat%2520dynamically%2520change%2520over%2520time.%2520We%2520evaluate%2520our%2520approach%2520across%2520various%250Ascenarios%2520in%2520connected%2520autonomous%2520driving%2520simulations.%2520Experimental%2520results%250Avalidate%2520that%2520our%2520approach%2520enables%2520CoID%2520and%2520adapts%2520to%2520dynamic%2520communication%250Abandwidth%2520changes.%2520In%2520addition%252C%2520our%2520approach%2520achieves%25208%2525-56%2525%2520overall%250Aimprovements%2520in%2520terms%2520of%2520covisible%2520object%2520retrieval%2520for%2520CoID%2520and%2520data%2520sharing%250Aefficiency%252C%2520which%2520outperforms%2520previous%2520techniques%2520and%2520achieves%2520the%250Astate-of-the-art%2520performance.%2520More%2520information%2520is%2520available%2520at%253A%250Ahttps%253A//gaopeng5.github.io/acoid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bandwidth-Adaptive%20Spatiotemporal%20Correspondence%20Identification%20for%0A%20%20Collaborative%20Perception&entry.906535625=Peng%20Gao%20and%20Williard%20Joshua%20Jose%20and%20Hao%20Zhang&entry.1292438233=%20%20Correspondence%20identification%20%28CoID%29%20is%20an%20essential%20capability%20in%0Amulti-robot%20collaborative%20perception%2C%20which%20enables%20a%20group%20of%20robots%20to%0Aconsistently%20refer%20to%20the%20same%20objects%20within%20their%20respective%20fields%20of%20view.%0AIn%20real-world%20applications%2C%20such%20as%20connected%20autonomous%20driving%2C%20vehicles%20face%0Achallenges%20in%20directly%20sharing%20raw%20observations%20due%20to%20limited%20communication%0Abandwidth.%20In%20order%20to%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20approach%20for%0Abandwidth-adaptive%20spatiotemporal%20CoID%20in%20collaborative%20perception.%20This%0Aapproach%20allows%20robots%20to%20progressively%20select%20partial%20spatiotemporal%0Aobservations%20and%20share%20with%20others%2C%20while%20adapting%20to%20communication%20constraints%0Athat%20dynamically%20change%20over%20time.%20We%20evaluate%20our%20approach%20across%20various%0Ascenarios%20in%20connected%20autonomous%20driving%20simulations.%20Experimental%20results%0Avalidate%20that%20our%20approach%20enables%20CoID%20and%20adapts%20to%20dynamic%20communication%0Abandwidth%20changes.%20In%20addition%2C%20our%20approach%20achieves%208%25-56%25%20overall%0Aimprovements%20in%20terms%20of%20covisible%20object%20retrieval%20for%20CoID%20and%20data%20sharing%0Aefficiency%2C%20which%20outperforms%20previous%20techniques%20and%20achieves%20the%0Astate-of-the-art%20performance.%20More%20information%20is%20available%20at%3A%0Ahttps%3A//gaopeng5.github.io/acoid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12098v1&entry.124074799=Read"},
{"title": "Cluster and Predict Latent Patches for Improved Masked Image Modeling", "author": "Timoth\u00e9e Darcet and Federico Baldassarre and Maxime Oquab and Julien Mairal and Piotr Bojanowski", "abstract": "  Masked Image Modeling (MIM) offers a promising approach to self-supervised\nrepresentation learning, however existing MIM models still lag behind the\nstate-of-the-art. In this paper, we systematically analyze target\nrepresentations, loss functions, and architectures, to introduce CAPI - a novel\npure-MIM framework that relies on the prediction of latent clusterings. Our\napproach leverages a clustering-based loss, which is stable to train, and\nexhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%\naccuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,\nsubstantially outperforming previous MIM methods and approaching the\nperformance of the current state-of-the-art, DINOv2. We release all our code\nand models.\n", "link": "http://arxiv.org/abs/2502.08769v2", "date": "2025-02-17", "relevancy": 2.2102, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5859}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5338}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster%20and%20Predict%20Latent%20Patches%20for%20Improved%20Masked%20Image%20Modeling&body=Title%3A%20Cluster%20and%20Predict%20Latent%20Patches%20for%20Improved%20Masked%20Image%20Modeling%0AAuthor%3A%20Timoth%C3%A9e%20Darcet%20and%20Federico%20Baldassarre%20and%20Maxime%20Oquab%20and%20Julien%20Mairal%20and%20Piotr%20Bojanowski%0AAbstract%3A%20%20%20Masked%20Image%20Modeling%20%28MIM%29%20offers%20a%20promising%20approach%20to%20self-supervised%0Arepresentation%20learning%2C%20however%20existing%20MIM%20models%20still%20lag%20behind%20the%0Astate-of-the-art.%20In%20this%20paper%2C%20we%20systematically%20analyze%20target%0Arepresentations%2C%20loss%20functions%2C%20and%20architectures%2C%20to%20introduce%20CAPI%20-%20a%20novel%0Apure-MIM%20framework%20that%20relies%20on%20the%20prediction%20of%20latent%20clusterings.%20Our%0Aapproach%20leverages%20a%20clustering-based%20loss%2C%20which%20is%20stable%20to%20train%2C%20and%0Aexhibits%20promising%20scaling%20properties.%20Our%20ViT-L%20backbone%2C%20CAPI%2C%20achieves%2083.8%25%0Aaccuracy%20on%20ImageNet%20and%2032.1%25%20mIoU%20on%20ADE20K%20with%20simple%20linear%20probes%2C%0Asubstantially%20outperforming%20previous%20MIM%20methods%20and%20approaching%20the%0Aperformance%20of%20the%20current%20state-of-the-art%2C%20DINOv2.%20We%20release%20all%20our%20code%0Aand%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster%2520and%2520Predict%2520Latent%2520Patches%2520for%2520Improved%2520Masked%2520Image%2520Modeling%26entry.906535625%3DTimoth%25C3%25A9e%2520Darcet%2520and%2520Federico%2520Baldassarre%2520and%2520Maxime%2520Oquab%2520and%2520Julien%2520Mairal%2520and%2520Piotr%2520Bojanowski%26entry.1292438233%3D%2520%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%2520offers%2520a%2520promising%2520approach%2520to%2520self-supervised%250Arepresentation%2520learning%252C%2520however%2520existing%2520MIM%2520models%2520still%2520lag%2520behind%2520the%250Astate-of-the-art.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520analyze%2520target%250Arepresentations%252C%2520loss%2520functions%252C%2520and%2520architectures%252C%2520to%2520introduce%2520CAPI%2520-%2520a%2520novel%250Apure-MIM%2520framework%2520that%2520relies%2520on%2520the%2520prediction%2520of%2520latent%2520clusterings.%2520Our%250Aapproach%2520leverages%2520a%2520clustering-based%2520loss%252C%2520which%2520is%2520stable%2520to%2520train%252C%2520and%250Aexhibits%2520promising%2520scaling%2520properties.%2520Our%2520ViT-L%2520backbone%252C%2520CAPI%252C%2520achieves%252083.8%2525%250Aaccuracy%2520on%2520ImageNet%2520and%252032.1%2525%2520mIoU%2520on%2520ADE20K%2520with%2520simple%2520linear%2520probes%252C%250Asubstantially%2520outperforming%2520previous%2520MIM%2520methods%2520and%2520approaching%2520the%250Aperformance%2520of%2520the%2520current%2520state-of-the-art%252C%2520DINOv2.%2520We%2520release%2520all%2520our%2520code%250Aand%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster%20and%20Predict%20Latent%20Patches%20for%20Improved%20Masked%20Image%20Modeling&entry.906535625=Timoth%C3%A9e%20Darcet%20and%20Federico%20Baldassarre%20and%20Maxime%20Oquab%20and%20Julien%20Mairal%20and%20Piotr%20Bojanowski&entry.1292438233=%20%20Masked%20Image%20Modeling%20%28MIM%29%20offers%20a%20promising%20approach%20to%20self-supervised%0Arepresentation%20learning%2C%20however%20existing%20MIM%20models%20still%20lag%20behind%20the%0Astate-of-the-art.%20In%20this%20paper%2C%20we%20systematically%20analyze%20target%0Arepresentations%2C%20loss%20functions%2C%20and%20architectures%2C%20to%20introduce%20CAPI%20-%20a%20novel%0Apure-MIM%20framework%20that%20relies%20on%20the%20prediction%20of%20latent%20clusterings.%20Our%0Aapproach%20leverages%20a%20clustering-based%20loss%2C%20which%20is%20stable%20to%20train%2C%20and%0Aexhibits%20promising%20scaling%20properties.%20Our%20ViT-L%20backbone%2C%20CAPI%2C%20achieves%2083.8%25%0Aaccuracy%20on%20ImageNet%20and%2032.1%25%20mIoU%20on%20ADE20K%20with%20simple%20linear%20probes%2C%0Asubstantially%20outperforming%20previous%20MIM%20methods%20and%20approaching%20the%0Aperformance%20of%20the%20current%20state-of-the-art%2C%20DINOv2.%20We%20release%20all%20our%20code%0Aand%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08769v2&entry.124074799=Read"},
{"title": "Identifying Gender Stereotypes and Biases in Automated Translation from\n  English to Italian using Similarity Networks", "author": "Fatemeh Mohammadi and Marta Annamaria Tamborini and Paolo Ceravolo and Costanza Nardocci and Samira Maghool", "abstract": "  This paper is a collaborative effort between Linguistics, Law, and Computer\nScience to evaluate stereotypes and biases in automated translation systems. We\nadvocate gender-neutral translation as a means to promote gender inclusion and\nimprove the objectivity of machine translation. Our approach focuses on\nidentifying gender bias in English-to-Italian translations. First, we define\ngender bias following human rights law and linguistics literature. Then we\nproceed by identifying gender-specific terms such as she/lei and he/lui as key\nelements. We then evaluate the cosine similarity between these target terms and\nothers in the dataset to reveal the model's perception of semantic relations.\nUsing numerical features, we effectively evaluate the intensity and direction\nof the bias. Our findings provide tangible insights for developing and training\ngender-neutral translation algorithms.\n", "link": "http://arxiv.org/abs/2502.11611v1", "date": "2025-02-17", "relevancy": 1.2148, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4166}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3961}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Gender%20Stereotypes%20and%20Biases%20in%20Automated%20Translation%20from%0A%20%20English%20to%20Italian%20using%20Similarity%20Networks&body=Title%3A%20Identifying%20Gender%20Stereotypes%20and%20Biases%20in%20Automated%20Translation%20from%0A%20%20English%20to%20Italian%20using%20Similarity%20Networks%0AAuthor%3A%20Fatemeh%20Mohammadi%20and%20Marta%20Annamaria%20Tamborini%20and%20Paolo%20Ceravolo%20and%20Costanza%20Nardocci%20and%20Samira%20Maghool%0AAbstract%3A%20%20%20This%20paper%20is%20a%20collaborative%20effort%20between%20Linguistics%2C%20Law%2C%20and%20Computer%0AScience%20to%20evaluate%20stereotypes%20and%20biases%20in%20automated%20translation%20systems.%20We%0Aadvocate%20gender-neutral%20translation%20as%20a%20means%20to%20promote%20gender%20inclusion%20and%0Aimprove%20the%20objectivity%20of%20machine%20translation.%20Our%20approach%20focuses%20on%0Aidentifying%20gender%20bias%20in%20English-to-Italian%20translations.%20First%2C%20we%20define%0Agender%20bias%20following%20human%20rights%20law%20and%20linguistics%20literature.%20Then%20we%0Aproceed%20by%20identifying%20gender-specific%20terms%20such%20as%20she/lei%20and%20he/lui%20as%20key%0Aelements.%20We%20then%20evaluate%20the%20cosine%20similarity%20between%20these%20target%20terms%20and%0Aothers%20in%20the%20dataset%20to%20reveal%20the%20model%27s%20perception%20of%20semantic%20relations.%0AUsing%20numerical%20features%2C%20we%20effectively%20evaluate%20the%20intensity%20and%20direction%0Aof%20the%20bias.%20Our%20findings%20provide%20tangible%20insights%20for%20developing%20and%20training%0Agender-neutral%20translation%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Gender%2520Stereotypes%2520and%2520Biases%2520in%2520Automated%2520Translation%2520from%250A%2520%2520English%2520to%2520Italian%2520using%2520Similarity%2520Networks%26entry.906535625%3DFatemeh%2520Mohammadi%2520and%2520Marta%2520Annamaria%2520Tamborini%2520and%2520Paolo%2520Ceravolo%2520and%2520Costanza%2520Nardocci%2520and%2520Samira%2520Maghool%26entry.1292438233%3D%2520%2520This%2520paper%2520is%2520a%2520collaborative%2520effort%2520between%2520Linguistics%252C%2520Law%252C%2520and%2520Computer%250AScience%2520to%2520evaluate%2520stereotypes%2520and%2520biases%2520in%2520automated%2520translation%2520systems.%2520We%250Aadvocate%2520gender-neutral%2520translation%2520as%2520a%2520means%2520to%2520promote%2520gender%2520inclusion%2520and%250Aimprove%2520the%2520objectivity%2520of%2520machine%2520translation.%2520Our%2520approach%2520focuses%2520on%250Aidentifying%2520gender%2520bias%2520in%2520English-to-Italian%2520translations.%2520First%252C%2520we%2520define%250Agender%2520bias%2520following%2520human%2520rights%2520law%2520and%2520linguistics%2520literature.%2520Then%2520we%250Aproceed%2520by%2520identifying%2520gender-specific%2520terms%2520such%2520as%2520she/lei%2520and%2520he/lui%2520as%2520key%250Aelements.%2520We%2520then%2520evaluate%2520the%2520cosine%2520similarity%2520between%2520these%2520target%2520terms%2520and%250Aothers%2520in%2520the%2520dataset%2520to%2520reveal%2520the%2520model%2527s%2520perception%2520of%2520semantic%2520relations.%250AUsing%2520numerical%2520features%252C%2520we%2520effectively%2520evaluate%2520the%2520intensity%2520and%2520direction%250Aof%2520the%2520bias.%2520Our%2520findings%2520provide%2520tangible%2520insights%2520for%2520developing%2520and%2520training%250Agender-neutral%2520translation%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Gender%20Stereotypes%20and%20Biases%20in%20Automated%20Translation%20from%0A%20%20English%20to%20Italian%20using%20Similarity%20Networks&entry.906535625=Fatemeh%20Mohammadi%20and%20Marta%20Annamaria%20Tamborini%20and%20Paolo%20Ceravolo%20and%20Costanza%20Nardocci%20and%20Samira%20Maghool&entry.1292438233=%20%20This%20paper%20is%20a%20collaborative%20effort%20between%20Linguistics%2C%20Law%2C%20and%20Computer%0AScience%20to%20evaluate%20stereotypes%20and%20biases%20in%20automated%20translation%20systems.%20We%0Aadvocate%20gender-neutral%20translation%20as%20a%20means%20to%20promote%20gender%20inclusion%20and%0Aimprove%20the%20objectivity%20of%20machine%20translation.%20Our%20approach%20focuses%20on%0Aidentifying%20gender%20bias%20in%20English-to-Italian%20translations.%20First%2C%20we%20define%0Agender%20bias%20following%20human%20rights%20law%20and%20linguistics%20literature.%20Then%20we%0Aproceed%20by%20identifying%20gender-specific%20terms%20such%20as%20she/lei%20and%20he/lui%20as%20key%0Aelements.%20We%20then%20evaluate%20the%20cosine%20similarity%20between%20these%20target%20terms%20and%0Aothers%20in%20the%20dataset%20to%20reveal%20the%20model%27s%20perception%20of%20semantic%20relations.%0AUsing%20numerical%20features%2C%20we%20effectively%20evaluate%20the%20intensity%20and%20direction%0Aof%20the%20bias.%20Our%20findings%20provide%20tangible%20insights%20for%20developing%20and%20training%0Agender-neutral%20translation%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11611v1&entry.124074799=Read"},
{"title": "GraphThought: Graph Combinatorial Optimization with Thought Generation", "author": "Zixiao Huang and Lifeng Guo and Junjie Sheng and Haosheng Chen and Wenhao Li and Bo Jin and Changhong Lu and Xiangfeng Wang", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, especially in text processing and generative tasks. Recent\nadvancements in the reasoning capabilities of state-of-the-art LLMs, such as\nOpenAI-o1, have significantly broadened their applicability, particularly in\ncomplex problem-solving and logical inference. However, most existing LLMs\nstruggle with notable limitations in handling graph combinatorial optimization\n(GCO) problems. To bridge this gap, we formally define the Optimal Thoughts\nDesign (OTD) problem, including its state and action thought space. We then\nintroduce a novel framework, GraphThought, designed to generate high-quality\nthought datasets for GCO problems. Leveraging these datasets, we fine-tune the\nLlama-3-8B-Instruct model to develop Llama-GT. Notably, despite its compact\n8B-parameter architecture, Llama-GT matches the performance of state-of-the-art\nLLMs on the GraphArena benchmark. Experimental results show that our approach\noutperforms both proprietary and open-source models, even rivaling specialized\nmodels like o1-mini. This work sets a new state-of-the-art benchmark while\nchallenging the prevailing notion that model scale is the primary driver of\nreasoning capability.\n", "link": "http://arxiv.org/abs/2502.11607v1", "date": "2025-02-17", "relevancy": 1.5434, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5544}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.514}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphThought%3A%20Graph%20Combinatorial%20Optimization%20with%20Thought%20Generation&body=Title%3A%20GraphThought%3A%20Graph%20Combinatorial%20Optimization%20with%20Thought%20Generation%0AAuthor%3A%20Zixiao%20Huang%20and%20Lifeng%20Guo%20and%20Junjie%20Sheng%20and%20Haosheng%20Chen%20and%20Wenhao%20Li%20and%20Bo%20Jin%20and%20Changhong%20Lu%20and%20Xiangfeng%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20domains%2C%20especially%20in%20text%20processing%20and%20generative%20tasks.%20Recent%0Aadvancements%20in%20the%20reasoning%20capabilities%20of%20state-of-the-art%20LLMs%2C%20such%20as%0AOpenAI-o1%2C%20have%20significantly%20broadened%20their%20applicability%2C%20particularly%20in%0Acomplex%20problem-solving%20and%20logical%20inference.%20However%2C%20most%20existing%20LLMs%0Astruggle%20with%20notable%20limitations%20in%20handling%20graph%20combinatorial%20optimization%0A%28GCO%29%20problems.%20To%20bridge%20this%20gap%2C%20we%20formally%20define%20the%20Optimal%20Thoughts%0ADesign%20%28OTD%29%20problem%2C%20including%20its%20state%20and%20action%20thought%20space.%20We%20then%0Aintroduce%20a%20novel%20framework%2C%20GraphThought%2C%20designed%20to%20generate%20high-quality%0Athought%20datasets%20for%20GCO%20problems.%20Leveraging%20these%20datasets%2C%20we%20fine-tune%20the%0ALlama-3-8B-Instruct%20model%20to%20develop%20Llama-GT.%20Notably%2C%20despite%20its%20compact%0A8B-parameter%20architecture%2C%20Llama-GT%20matches%20the%20performance%20of%20state-of-the-art%0ALLMs%20on%20the%20GraphArena%20benchmark.%20Experimental%20results%20show%20that%20our%20approach%0Aoutperforms%20both%20proprietary%20and%20open-source%20models%2C%20even%20rivaling%20specialized%0Amodels%20like%20o1-mini.%20This%20work%20sets%20a%20new%20state-of-the-art%20benchmark%20while%0Achallenging%20the%20prevailing%20notion%20that%20model%20scale%20is%20the%20primary%20driver%20of%0Areasoning%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphThought%253A%2520Graph%2520Combinatorial%2520Optimization%2520with%2520Thought%2520Generation%26entry.906535625%3DZixiao%2520Huang%2520and%2520Lifeng%2520Guo%2520and%2520Junjie%2520Sheng%2520and%2520Haosheng%2520Chen%2520and%2520Wenhao%2520Li%2520and%2520Bo%2520Jin%2520and%2520Changhong%2520Lu%2520and%2520Xiangfeng%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Avarious%2520domains%252C%2520especially%2520in%2520text%2520processing%2520and%2520generative%2520tasks.%2520Recent%250Aadvancements%2520in%2520the%2520reasoning%2520capabilities%2520of%2520state-of-the-art%2520LLMs%252C%2520such%2520as%250AOpenAI-o1%252C%2520have%2520significantly%2520broadened%2520their%2520applicability%252C%2520particularly%2520in%250Acomplex%2520problem-solving%2520and%2520logical%2520inference.%2520However%252C%2520most%2520existing%2520LLMs%250Astruggle%2520with%2520notable%2520limitations%2520in%2520handling%2520graph%2520combinatorial%2520optimization%250A%2528GCO%2529%2520problems.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520formally%2520define%2520the%2520Optimal%2520Thoughts%250ADesign%2520%2528OTD%2529%2520problem%252C%2520including%2520its%2520state%2520and%2520action%2520thought%2520space.%2520We%2520then%250Aintroduce%2520a%2520novel%2520framework%252C%2520GraphThought%252C%2520designed%2520to%2520generate%2520high-quality%250Athought%2520datasets%2520for%2520GCO%2520problems.%2520Leveraging%2520these%2520datasets%252C%2520we%2520fine-tune%2520the%250ALlama-3-8B-Instruct%2520model%2520to%2520develop%2520Llama-GT.%2520Notably%252C%2520despite%2520its%2520compact%250A8B-parameter%2520architecture%252C%2520Llama-GT%2520matches%2520the%2520performance%2520of%2520state-of-the-art%250ALLMs%2520on%2520the%2520GraphArena%2520benchmark.%2520Experimental%2520results%2520show%2520that%2520our%2520approach%250Aoutperforms%2520both%2520proprietary%2520and%2520open-source%2520models%252C%2520even%2520rivaling%2520specialized%250Amodels%2520like%2520o1-mini.%2520This%2520work%2520sets%2520a%2520new%2520state-of-the-art%2520benchmark%2520while%250Achallenging%2520the%2520prevailing%2520notion%2520that%2520model%2520scale%2520is%2520the%2520primary%2520driver%2520of%250Areasoning%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphThought%3A%20Graph%20Combinatorial%20Optimization%20with%20Thought%20Generation&entry.906535625=Zixiao%20Huang%20and%20Lifeng%20Guo%20and%20Junjie%20Sheng%20and%20Haosheng%20Chen%20and%20Wenhao%20Li%20and%20Bo%20Jin%20and%20Changhong%20Lu%20and%20Xiangfeng%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20domains%2C%20especially%20in%20text%20processing%20and%20generative%20tasks.%20Recent%0Aadvancements%20in%20the%20reasoning%20capabilities%20of%20state-of-the-art%20LLMs%2C%20such%20as%0AOpenAI-o1%2C%20have%20significantly%20broadened%20their%20applicability%2C%20particularly%20in%0Acomplex%20problem-solving%20and%20logical%20inference.%20However%2C%20most%20existing%20LLMs%0Astruggle%20with%20notable%20limitations%20in%20handling%20graph%20combinatorial%20optimization%0A%28GCO%29%20problems.%20To%20bridge%20this%20gap%2C%20we%20formally%20define%20the%20Optimal%20Thoughts%0ADesign%20%28OTD%29%20problem%2C%20including%20its%20state%20and%20action%20thought%20space.%20We%20then%0Aintroduce%20a%20novel%20framework%2C%20GraphThought%2C%20designed%20to%20generate%20high-quality%0Athought%20datasets%20for%20GCO%20problems.%20Leveraging%20these%20datasets%2C%20we%20fine-tune%20the%0ALlama-3-8B-Instruct%20model%20to%20develop%20Llama-GT.%20Notably%2C%20despite%20its%20compact%0A8B-parameter%20architecture%2C%20Llama-GT%20matches%20the%20performance%20of%20state-of-the-art%0ALLMs%20on%20the%20GraphArena%20benchmark.%20Experimental%20results%20show%20that%20our%20approach%0Aoutperforms%20both%20proprietary%20and%20open-source%20models%2C%20even%20rivaling%20specialized%0Amodels%20like%20o1-mini.%20This%20work%20sets%20a%20new%20state-of-the-art%20benchmark%20while%0Achallenging%20the%20prevailing%20notion%20that%20model%20scale%20is%20the%20primary%20driver%20of%0Areasoning%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11607v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


