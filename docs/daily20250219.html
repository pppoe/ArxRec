<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250218.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM", "author": "Mingrui Li and Weijian Chen and Na Cheng and Jingyuan Xu and Dong Li and Hongyu Wang", "abstract": "  The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread\nattention due to its excellent performance in real-time high-fidelity\nrendering. However, in real-world environments with dynamic objects, existing\n3DGS-based SLAM systems often face mapping errors and tracking drift issues. To\naddress these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM\nsystem tailored for dynamic scenes. In terms of tracking, unlike traditional\nmethods, we directly perform dynamic segmentation on Gaussians and map them\nback to the front-end to obtain dynamic point labels through a Gaussian pyramid\nnetwork, achieving precise dynamic removal and robust tracking. For mapping, we\nimpose rendering penalties on dynamically labeled Gaussians, which are updated\nthrough the network, to avoid irreversible erroneous removal caused by simple\npruning. Our results on real-world datasets demonstrate that our method is\ncompetitive in tracking compared to baseline methods, generating fewer\nartifacts and higher-quality reconstructions in rendering.\n", "link": "http://arxiv.org/abs/2502.03228v2", "date": "2025-02-18", "relevancy": 3.5496, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8033}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6675}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GARAD-SLAM%3A%203D%20GAussian%20splatting%20for%20Real-time%20Anti%20Dynamic%20SLAM&body=Title%3A%20GARAD-SLAM%3A%203D%20GAussian%20splatting%20for%20Real-time%20Anti%20Dynamic%20SLAM%0AAuthor%3A%20Mingrui%20Li%20and%20Weijian%20Chen%20and%20Na%20Cheng%20and%20Jingyuan%20Xu%20and%20Dong%20Li%20and%20Hongyu%20Wang%0AAbstract%3A%20%20%20The%203D%20Gaussian%20Splatting%20%283DGS%29-based%20SLAM%20system%20has%20garnered%20widespread%0Aattention%20due%20to%20its%20excellent%20performance%20in%20real-time%20high-fidelity%0Arendering.%20However%2C%20in%20real-world%20environments%20with%20dynamic%20objects%2C%20existing%0A3DGS-based%20SLAM%20systems%20often%20face%20mapping%20errors%20and%20tracking%20drift%20issues.%20To%0Aaddress%20these%20problems%2C%20we%20propose%20GARAD-SLAM%2C%20a%20real-time%203DGS-based%20SLAM%0Asystem%20tailored%20for%20dynamic%20scenes.%20In%20terms%20of%20tracking%2C%20unlike%20traditional%0Amethods%2C%20we%20directly%20perform%20dynamic%20segmentation%20on%20Gaussians%20and%20map%20them%0Aback%20to%20the%20front-end%20to%20obtain%20dynamic%20point%20labels%20through%20a%20Gaussian%20pyramid%0Anetwork%2C%20achieving%20precise%20dynamic%20removal%20and%20robust%20tracking.%20For%20mapping%2C%20we%0Aimpose%20rendering%20penalties%20on%20dynamically%20labeled%20Gaussians%2C%20which%20are%20updated%0Athrough%20the%20network%2C%20to%20avoid%20irreversible%20erroneous%20removal%20caused%20by%20simple%0Apruning.%20Our%20results%20on%20real-world%20datasets%20demonstrate%20that%20our%20method%20is%0Acompetitive%20in%20tracking%20compared%20to%20baseline%20methods%2C%20generating%20fewer%0Aartifacts%20and%20higher-quality%20reconstructions%20in%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGARAD-SLAM%253A%25203D%2520GAussian%2520splatting%2520for%2520Real-time%2520Anti%2520Dynamic%2520SLAM%26entry.906535625%3DMingrui%2520Li%2520and%2520Weijian%2520Chen%2520and%2520Na%2520Cheng%2520and%2520Jingyuan%2520Xu%2520and%2520Dong%2520Li%2520and%2520Hongyu%2520Wang%26entry.1292438233%3D%2520%2520The%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529-based%2520SLAM%2520system%2520has%2520garnered%2520widespread%250Aattention%2520due%2520to%2520its%2520excellent%2520performance%2520in%2520real-time%2520high-fidelity%250Arendering.%2520However%252C%2520in%2520real-world%2520environments%2520with%2520dynamic%2520objects%252C%2520existing%250A3DGS-based%2520SLAM%2520systems%2520often%2520face%2520mapping%2520errors%2520and%2520tracking%2520drift%2520issues.%2520To%250Aaddress%2520these%2520problems%252C%2520we%2520propose%2520GARAD-SLAM%252C%2520a%2520real-time%25203DGS-based%2520SLAM%250Asystem%2520tailored%2520for%2520dynamic%2520scenes.%2520In%2520terms%2520of%2520tracking%252C%2520unlike%2520traditional%250Amethods%252C%2520we%2520directly%2520perform%2520dynamic%2520segmentation%2520on%2520Gaussians%2520and%2520map%2520them%250Aback%2520to%2520the%2520front-end%2520to%2520obtain%2520dynamic%2520point%2520labels%2520through%2520a%2520Gaussian%2520pyramid%250Anetwork%252C%2520achieving%2520precise%2520dynamic%2520removal%2520and%2520robust%2520tracking.%2520For%2520mapping%252C%2520we%250Aimpose%2520rendering%2520penalties%2520on%2520dynamically%2520labeled%2520Gaussians%252C%2520which%2520are%2520updated%250Athrough%2520the%2520network%252C%2520to%2520avoid%2520irreversible%2520erroneous%2520removal%2520caused%2520by%2520simple%250Apruning.%2520Our%2520results%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520method%2520is%250Acompetitive%2520in%2520tracking%2520compared%2520to%2520baseline%2520methods%252C%2520generating%2520fewer%250Aartifacts%2520and%2520higher-quality%2520reconstructions%2520in%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GARAD-SLAM%3A%203D%20GAussian%20splatting%20for%20Real-time%20Anti%20Dynamic%20SLAM&entry.906535625=Mingrui%20Li%20and%20Weijian%20Chen%20and%20Na%20Cheng%20and%20Jingyuan%20Xu%20and%20Dong%20Li%20and%20Hongyu%20Wang&entry.1292438233=%20%20The%203D%20Gaussian%20Splatting%20%283DGS%29-based%20SLAM%20system%20has%20garnered%20widespread%0Aattention%20due%20to%20its%20excellent%20performance%20in%20real-time%20high-fidelity%0Arendering.%20However%2C%20in%20real-world%20environments%20with%20dynamic%20objects%2C%20existing%0A3DGS-based%20SLAM%20systems%20often%20face%20mapping%20errors%20and%20tracking%20drift%20issues.%20To%0Aaddress%20these%20problems%2C%20we%20propose%20GARAD-SLAM%2C%20a%20real-time%203DGS-based%20SLAM%0Asystem%20tailored%20for%20dynamic%20scenes.%20In%20terms%20of%20tracking%2C%20unlike%20traditional%0Amethods%2C%20we%20directly%20perform%20dynamic%20segmentation%20on%20Gaussians%20and%20map%20them%0Aback%20to%20the%20front-end%20to%20obtain%20dynamic%20point%20labels%20through%20a%20Gaussian%20pyramid%0Anetwork%2C%20achieving%20precise%20dynamic%20removal%20and%20robust%20tracking.%20For%20mapping%2C%20we%0Aimpose%20rendering%20penalties%20on%20dynamically%20labeled%20Gaussians%2C%20which%20are%20updated%0Athrough%20the%20network%2C%20to%20avoid%20irreversible%20erroneous%20removal%20caused%20by%20simple%0Apruning.%20Our%20results%20on%20real-world%20datasets%20demonstrate%20that%20our%20method%20is%0Acompetitive%20in%20tracking%20compared%20to%20baseline%20methods%2C%20generating%20fewer%0Aartifacts%20and%20higher-quality%20reconstructions%20in%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03228v2&entry.124074799=Read"},
{"title": "CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image", "author": "Kaixin Yao and Longwen Zhang and Xinhao Yan and Yan Zeng and Qixuan Zhang and Lan Xu and Wei Yang and Jiayuan Gu and Jingyi Yu", "abstract": "  Recovering high-quality 3D scenes from a single RGB image is a challenging\ntask in computer graphics. Current methods often struggle with domain-specific\nlimitations or low-quality object generation. To address these, we propose CAST\n(Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel\nmethod for 3D scene reconstruction and recovery. CAST starts by extracting\nobject-level 2D segmentation and relative depth information from the input\nimage, followed by using a GPT-based model to analyze inter-object spatial\nrelationships. This enables the understanding of how objects relate to each\nother within the scene, ensuring more coherent reconstruction. CAST then\nemploys an occlusion-aware large-scale 3D generation model to independently\ngenerate each object's full geometry, using MAE and point cloud conditioning to\nmitigate the effects of occlusions and partial object information, ensuring\naccurate alignment with the source image's geometry and texture. To align each\nobject with the scene, the alignment generation model computes the necessary\ntransformations, allowing the generated meshes to be accurately placed and\nintegrated into the scene's point cloud. Finally, CAST incorporates a\nphysics-aware correction step that leverages a fine-grained relation graph to\ngenerate a constraint graph. This graph guides the optimization of object\nposes, ensuring physical consistency and spatial coherence. By utilizing Signed\nDistance Fields (SDF), the model effectively addresses issues such as\nocclusions, object penetration, and floating objects, ensuring that the\ngenerated scene accurately reflects real-world physical interactions. CAST can\nbe leveraged in robotics, enabling efficient real-to-simulation workflows and\nproviding realistic, scalable simulation environments for robotic systems.\n", "link": "http://arxiv.org/abs/2502.12894v1", "date": "2025-02-18", "relevancy": 3.3018, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6667}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6667}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAST%3A%20Component-Aligned%203D%20Scene%20Reconstruction%20from%20an%20RGB%20Image&body=Title%3A%20CAST%3A%20Component-Aligned%203D%20Scene%20Reconstruction%20from%20an%20RGB%20Image%0AAuthor%3A%20Kaixin%20Yao%20and%20Longwen%20Zhang%20and%20Xinhao%20Yan%20and%20Yan%20Zeng%20and%20Qixuan%20Zhang%20and%20Lan%20Xu%20and%20Wei%20Yang%20and%20Jiayuan%20Gu%20and%20Jingyi%20Yu%0AAbstract%3A%20%20%20Recovering%20high-quality%203D%20scenes%20from%20a%20single%20RGB%20image%20is%20a%20challenging%0Atask%20in%20computer%20graphics.%20Current%20methods%20often%20struggle%20with%20domain-specific%0Alimitations%20or%20low-quality%20object%20generation.%20To%20address%20these%2C%20we%20propose%20CAST%0A%28Component-Aligned%203D%20Scene%20Reconstruction%20from%20a%20Single%20RGB%20Image%29%2C%20a%20novel%0Amethod%20for%203D%20scene%20reconstruction%20and%20recovery.%20CAST%20starts%20by%20extracting%0Aobject-level%202D%20segmentation%20and%20relative%20depth%20information%20from%20the%20input%0Aimage%2C%20followed%20by%20using%20a%20GPT-based%20model%20to%20analyze%20inter-object%20spatial%0Arelationships.%20This%20enables%20the%20understanding%20of%20how%20objects%20relate%20to%20each%0Aother%20within%20the%20scene%2C%20ensuring%20more%20coherent%20reconstruction.%20CAST%20then%0Aemploys%20an%20occlusion-aware%20large-scale%203D%20generation%20model%20to%20independently%0Agenerate%20each%20object%27s%20full%20geometry%2C%20using%20MAE%20and%20point%20cloud%20conditioning%20to%0Amitigate%20the%20effects%20of%20occlusions%20and%20partial%20object%20information%2C%20ensuring%0Aaccurate%20alignment%20with%20the%20source%20image%27s%20geometry%20and%20texture.%20To%20align%20each%0Aobject%20with%20the%20scene%2C%20the%20alignment%20generation%20model%20computes%20the%20necessary%0Atransformations%2C%20allowing%20the%20generated%20meshes%20to%20be%20accurately%20placed%20and%0Aintegrated%20into%20the%20scene%27s%20point%20cloud.%20Finally%2C%20CAST%20incorporates%20a%0Aphysics-aware%20correction%20step%20that%20leverages%20a%20fine-grained%20relation%20graph%20to%0Agenerate%20a%20constraint%20graph.%20This%20graph%20guides%20the%20optimization%20of%20object%0Aposes%2C%20ensuring%20physical%20consistency%20and%20spatial%20coherence.%20By%20utilizing%20Signed%0ADistance%20Fields%20%28SDF%29%2C%20the%20model%20effectively%20addresses%20issues%20such%20as%0Aocclusions%2C%20object%20penetration%2C%20and%20floating%20objects%2C%20ensuring%20that%20the%0Agenerated%20scene%20accurately%20reflects%20real-world%20physical%20interactions.%20CAST%20can%0Abe%20leveraged%20in%20robotics%2C%20enabling%20efficient%20real-to-simulation%20workflows%20and%0Aproviding%20realistic%2C%20scalable%20simulation%20environments%20for%20robotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAST%253A%2520Component-Aligned%25203D%2520Scene%2520Reconstruction%2520from%2520an%2520RGB%2520Image%26entry.906535625%3DKaixin%2520Yao%2520and%2520Longwen%2520Zhang%2520and%2520Xinhao%2520Yan%2520and%2520Yan%2520Zeng%2520and%2520Qixuan%2520Zhang%2520and%2520Lan%2520Xu%2520and%2520Wei%2520Yang%2520and%2520Jiayuan%2520Gu%2520and%2520Jingyi%2520Yu%26entry.1292438233%3D%2520%2520Recovering%2520high-quality%25203D%2520scenes%2520from%2520a%2520single%2520RGB%2520image%2520is%2520a%2520challenging%250Atask%2520in%2520computer%2520graphics.%2520Current%2520methods%2520often%2520struggle%2520with%2520domain-specific%250Alimitations%2520or%2520low-quality%2520object%2520generation.%2520To%2520address%2520these%252C%2520we%2520propose%2520CAST%250A%2528Component-Aligned%25203D%2520Scene%2520Reconstruction%2520from%2520a%2520Single%2520RGB%2520Image%2529%252C%2520a%2520novel%250Amethod%2520for%25203D%2520scene%2520reconstruction%2520and%2520recovery.%2520CAST%2520starts%2520by%2520extracting%250Aobject-level%25202D%2520segmentation%2520and%2520relative%2520depth%2520information%2520from%2520the%2520input%250Aimage%252C%2520followed%2520by%2520using%2520a%2520GPT-based%2520model%2520to%2520analyze%2520inter-object%2520spatial%250Arelationships.%2520This%2520enables%2520the%2520understanding%2520of%2520how%2520objects%2520relate%2520to%2520each%250Aother%2520within%2520the%2520scene%252C%2520ensuring%2520more%2520coherent%2520reconstruction.%2520CAST%2520then%250Aemploys%2520an%2520occlusion-aware%2520large-scale%25203D%2520generation%2520model%2520to%2520independently%250Agenerate%2520each%2520object%2527s%2520full%2520geometry%252C%2520using%2520MAE%2520and%2520point%2520cloud%2520conditioning%2520to%250Amitigate%2520the%2520effects%2520of%2520occlusions%2520and%2520partial%2520object%2520information%252C%2520ensuring%250Aaccurate%2520alignment%2520with%2520the%2520source%2520image%2527s%2520geometry%2520and%2520texture.%2520To%2520align%2520each%250Aobject%2520with%2520the%2520scene%252C%2520the%2520alignment%2520generation%2520model%2520computes%2520the%2520necessary%250Atransformations%252C%2520allowing%2520the%2520generated%2520meshes%2520to%2520be%2520accurately%2520placed%2520and%250Aintegrated%2520into%2520the%2520scene%2527s%2520point%2520cloud.%2520Finally%252C%2520CAST%2520incorporates%2520a%250Aphysics-aware%2520correction%2520step%2520that%2520leverages%2520a%2520fine-grained%2520relation%2520graph%2520to%250Agenerate%2520a%2520constraint%2520graph.%2520This%2520graph%2520guides%2520the%2520optimization%2520of%2520object%250Aposes%252C%2520ensuring%2520physical%2520consistency%2520and%2520spatial%2520coherence.%2520By%2520utilizing%2520Signed%250ADistance%2520Fields%2520%2528SDF%2529%252C%2520the%2520model%2520effectively%2520addresses%2520issues%2520such%2520as%250Aocclusions%252C%2520object%2520penetration%252C%2520and%2520floating%2520objects%252C%2520ensuring%2520that%2520the%250Agenerated%2520scene%2520accurately%2520reflects%2520real-world%2520physical%2520interactions.%2520CAST%2520can%250Abe%2520leveraged%2520in%2520robotics%252C%2520enabling%2520efficient%2520real-to-simulation%2520workflows%2520and%250Aproviding%2520realistic%252C%2520scalable%2520simulation%2520environments%2520for%2520robotic%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAST%3A%20Component-Aligned%203D%20Scene%20Reconstruction%20from%20an%20RGB%20Image&entry.906535625=Kaixin%20Yao%20and%20Longwen%20Zhang%20and%20Xinhao%20Yan%20and%20Yan%20Zeng%20and%20Qixuan%20Zhang%20and%20Lan%20Xu%20and%20Wei%20Yang%20and%20Jiayuan%20Gu%20and%20Jingyi%20Yu&entry.1292438233=%20%20Recovering%20high-quality%203D%20scenes%20from%20a%20single%20RGB%20image%20is%20a%20challenging%0Atask%20in%20computer%20graphics.%20Current%20methods%20often%20struggle%20with%20domain-specific%0Alimitations%20or%20low-quality%20object%20generation.%20To%20address%20these%2C%20we%20propose%20CAST%0A%28Component-Aligned%203D%20Scene%20Reconstruction%20from%20a%20Single%20RGB%20Image%29%2C%20a%20novel%0Amethod%20for%203D%20scene%20reconstruction%20and%20recovery.%20CAST%20starts%20by%20extracting%0Aobject-level%202D%20segmentation%20and%20relative%20depth%20information%20from%20the%20input%0Aimage%2C%20followed%20by%20using%20a%20GPT-based%20model%20to%20analyze%20inter-object%20spatial%0Arelationships.%20This%20enables%20the%20understanding%20of%20how%20objects%20relate%20to%20each%0Aother%20within%20the%20scene%2C%20ensuring%20more%20coherent%20reconstruction.%20CAST%20then%0Aemploys%20an%20occlusion-aware%20large-scale%203D%20generation%20model%20to%20independently%0Agenerate%20each%20object%27s%20full%20geometry%2C%20using%20MAE%20and%20point%20cloud%20conditioning%20to%0Amitigate%20the%20effects%20of%20occlusions%20and%20partial%20object%20information%2C%20ensuring%0Aaccurate%20alignment%20with%20the%20source%20image%27s%20geometry%20and%20texture.%20To%20align%20each%0Aobject%20with%20the%20scene%2C%20the%20alignment%20generation%20model%20computes%20the%20necessary%0Atransformations%2C%20allowing%20the%20generated%20meshes%20to%20be%20accurately%20placed%20and%0Aintegrated%20into%20the%20scene%27s%20point%20cloud.%20Finally%2C%20CAST%20incorporates%20a%0Aphysics-aware%20correction%20step%20that%20leverages%20a%20fine-grained%20relation%20graph%20to%0Agenerate%20a%20constraint%20graph.%20This%20graph%20guides%20the%20optimization%20of%20object%0Aposes%2C%20ensuring%20physical%20consistency%20and%20spatial%20coherence.%20By%20utilizing%20Signed%0ADistance%20Fields%20%28SDF%29%2C%20the%20model%20effectively%20addresses%20issues%20such%20as%0Aocclusions%2C%20object%20penetration%2C%20and%20floating%20objects%2C%20ensuring%20that%20the%0Agenerated%20scene%20accurately%20reflects%20real-world%20physical%20interactions.%20CAST%20can%0Abe%20leveraged%20in%20robotics%2C%20enabling%20efficient%20real-to-simulation%20workflows%20and%0Aproviding%20realistic%2C%20scalable%20simulation%20environments%20for%20robotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12894v1&entry.124074799=Read"},
{"title": "TIPS: Text-Induced Pose Synthesis", "author": "Prasun Roy and Subhankar Ghosh and Saumik Bhattacharya and Umapada Pal and Michael Blumenstein", "abstract": "  In computer vision, human pose synthesis and transfer deal with probabilistic\nimage generation of a person in a previously unseen pose from an already\navailable observation of that person. Though researchers have recently proposed\nseveral methods to achieve this task, most of these techniques derive the\ntarget pose directly from the desired target image on a specific dataset,\nmaking the underlying process challenging to apply in real-world scenarios as\nthe generation of the target image is the actual aim. In this paper, we first\npresent the shortcomings of current pose transfer algorithms and then propose a\nnovel text-based pose transfer technique to address those issues. We divide the\nproblem into three independent stages: (a) text to pose representation, (b)\npose refinement, and (c) pose rendering. To the best of our knowledge, this is\none of the first attempts to develop a text-based pose transfer framework where\nwe also introduce a new dataset DF-PASS, by adding descriptive pose annotations\nfor the images of the DeepFashion dataset. The proposed method generates\npromising results with significant qualitative and quantitative scores in our\nexperiments.\n", "link": "http://arxiv.org/abs/2207.11718v2", "date": "2025-02-18", "relevancy": 3.1788, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6725}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6215}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIPS%3A%20Text-Induced%20Pose%20Synthesis&body=Title%3A%20TIPS%3A%20Text-Induced%20Pose%20Synthesis%0AAuthor%3A%20Prasun%20Roy%20and%20Subhankar%20Ghosh%20and%20Saumik%20Bhattacharya%20and%20Umapada%20Pal%20and%20Michael%20Blumenstein%0AAbstract%3A%20%20%20In%20computer%20vision%2C%20human%20pose%20synthesis%20and%20transfer%20deal%20with%20probabilistic%0Aimage%20generation%20of%20a%20person%20in%20a%20previously%20unseen%20pose%20from%20an%20already%0Aavailable%20observation%20of%20that%20person.%20Though%20researchers%20have%20recently%20proposed%0Aseveral%20methods%20to%20achieve%20this%20task%2C%20most%20of%20these%20techniques%20derive%20the%0Atarget%20pose%20directly%20from%20the%20desired%20target%20image%20on%20a%20specific%20dataset%2C%0Amaking%20the%20underlying%20process%20challenging%20to%20apply%20in%20real-world%20scenarios%20as%0Athe%20generation%20of%20the%20target%20image%20is%20the%20actual%20aim.%20In%20this%20paper%2C%20we%20first%0Apresent%20the%20shortcomings%20of%20current%20pose%20transfer%20algorithms%20and%20then%20propose%20a%0Anovel%20text-based%20pose%20transfer%20technique%20to%20address%20those%20issues.%20We%20divide%20the%0Aproblem%20into%20three%20independent%20stages%3A%20%28a%29%20text%20to%20pose%20representation%2C%20%28b%29%0Apose%20refinement%2C%20and%20%28c%29%20pose%20rendering.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Aone%20of%20the%20first%20attempts%20to%20develop%20a%20text-based%20pose%20transfer%20framework%20where%0Awe%20also%20introduce%20a%20new%20dataset%20DF-PASS%2C%20by%20adding%20descriptive%20pose%20annotations%0Afor%20the%20images%20of%20the%20DeepFashion%20dataset.%20The%20proposed%20method%20generates%0Apromising%20results%20with%20significant%20qualitative%20and%20quantitative%20scores%20in%20our%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.11718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIPS%253A%2520Text-Induced%2520Pose%2520Synthesis%26entry.906535625%3DPrasun%2520Roy%2520and%2520Subhankar%2520Ghosh%2520and%2520Saumik%2520Bhattacharya%2520and%2520Umapada%2520Pal%2520and%2520Michael%2520Blumenstein%26entry.1292438233%3D%2520%2520In%2520computer%2520vision%252C%2520human%2520pose%2520synthesis%2520and%2520transfer%2520deal%2520with%2520probabilistic%250Aimage%2520generation%2520of%2520a%2520person%2520in%2520a%2520previously%2520unseen%2520pose%2520from%2520an%2520already%250Aavailable%2520observation%2520of%2520that%2520person.%2520Though%2520researchers%2520have%2520recently%2520proposed%250Aseveral%2520methods%2520to%2520achieve%2520this%2520task%252C%2520most%2520of%2520these%2520techniques%2520derive%2520the%250Atarget%2520pose%2520directly%2520from%2520the%2520desired%2520target%2520image%2520on%2520a%2520specific%2520dataset%252C%250Amaking%2520the%2520underlying%2520process%2520challenging%2520to%2520apply%2520in%2520real-world%2520scenarios%2520as%250Athe%2520generation%2520of%2520the%2520target%2520image%2520is%2520the%2520actual%2520aim.%2520In%2520this%2520paper%252C%2520we%2520first%250Apresent%2520the%2520shortcomings%2520of%2520current%2520pose%2520transfer%2520algorithms%2520and%2520then%2520propose%2520a%250Anovel%2520text-based%2520pose%2520transfer%2520technique%2520to%2520address%2520those%2520issues.%2520We%2520divide%2520the%250Aproblem%2520into%2520three%2520independent%2520stages%253A%2520%2528a%2529%2520text%2520to%2520pose%2520representation%252C%2520%2528b%2529%250Apose%2520refinement%252C%2520and%2520%2528c%2529%2520pose%2520rendering.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%250Aone%2520of%2520the%2520first%2520attempts%2520to%2520develop%2520a%2520text-based%2520pose%2520transfer%2520framework%2520where%250Awe%2520also%2520introduce%2520a%2520new%2520dataset%2520DF-PASS%252C%2520by%2520adding%2520descriptive%2520pose%2520annotations%250Afor%2520the%2520images%2520of%2520the%2520DeepFashion%2520dataset.%2520The%2520proposed%2520method%2520generates%250Apromising%2520results%2520with%2520significant%2520qualitative%2520and%2520quantitative%2520scores%2520in%2520our%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.11718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIPS%3A%20Text-Induced%20Pose%20Synthesis&entry.906535625=Prasun%20Roy%20and%20Subhankar%20Ghosh%20and%20Saumik%20Bhattacharya%20and%20Umapada%20Pal%20and%20Michael%20Blumenstein&entry.1292438233=%20%20In%20computer%20vision%2C%20human%20pose%20synthesis%20and%20transfer%20deal%20with%20probabilistic%0Aimage%20generation%20of%20a%20person%20in%20a%20previously%20unseen%20pose%20from%20an%20already%0Aavailable%20observation%20of%20that%20person.%20Though%20researchers%20have%20recently%20proposed%0Aseveral%20methods%20to%20achieve%20this%20task%2C%20most%20of%20these%20techniques%20derive%20the%0Atarget%20pose%20directly%20from%20the%20desired%20target%20image%20on%20a%20specific%20dataset%2C%0Amaking%20the%20underlying%20process%20challenging%20to%20apply%20in%20real-world%20scenarios%20as%0Athe%20generation%20of%20the%20target%20image%20is%20the%20actual%20aim.%20In%20this%20paper%2C%20we%20first%0Apresent%20the%20shortcomings%20of%20current%20pose%20transfer%20algorithms%20and%20then%20propose%20a%0Anovel%20text-based%20pose%20transfer%20technique%20to%20address%20those%20issues.%20We%20divide%20the%0Aproblem%20into%20three%20independent%20stages%3A%20%28a%29%20text%20to%20pose%20representation%2C%20%28b%29%0Apose%20refinement%2C%20and%20%28c%29%20pose%20rendering.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Aone%20of%20the%20first%20attempts%20to%20develop%20a%20text-based%20pose%20transfer%20framework%20where%0Awe%20also%20introduce%20a%20new%20dataset%20DF-PASS%2C%20by%20adding%20descriptive%20pose%20annotations%0Afor%20the%20images%20of%20the%20DeepFashion%20dataset.%20The%20proposed%20method%20generates%0Apromising%20results%20with%20significant%20qualitative%20and%20quantitative%20scores%20in%20our%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.11718v2&entry.124074799=Read"},
{"title": "High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion", "author": "Xiang Zhang and Yang Zhang and Lukas Mehl and Markus Gross and Christopher Schroers", "abstract": "  Despite recent advances in Novel View Synthesis (NVS), generating\nhigh-fidelity views from single or sparse observations remains a significant\nchallenge. Existing splatting-based approaches often produce distorted geometry\ndue to splatting errors. While diffusion-based methods leverage rich 3D priors\nto achieve improved geometry, they often suffer from texture hallucination. In\nthis paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion\nmodel designed to synthesize high-fidelity novel views from a single image.\nSpecifically, we propose an aligned synthesis strategy for precise control of\ntarget viewpoints and geometry-consistent view synthesis. To mitigate texture\nhallucination, we design a texture bridge module that enables high-fidelity\ntexture generation through adaptive feature fusion. In this manner, SplatDiff\nleverages the strengths of splatting and diffusion to generate novel views with\nconsistent geometry and high-fidelity details. Extensive experiments verify the\nstate-of-the-art performance of SplatDiff in single-view NVS. Additionally,\nwithout extra training, SplatDiff shows remarkable zero-shot performance across\ndiverse tasks, including sparse-view NVS and stereo video conversion.\n", "link": "http://arxiv.org/abs/2502.12752v1", "date": "2025-02-18", "relevancy": 3.1328, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6299}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6249}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Fidelity%20Novel%20View%20Synthesis%20via%20Splatting-Guided%20Diffusion&body=Title%3A%20High-Fidelity%20Novel%20View%20Synthesis%20via%20Splatting-Guided%20Diffusion%0AAuthor%3A%20Xiang%20Zhang%20and%20Yang%20Zhang%20and%20Lukas%20Mehl%20and%20Markus%20Gross%20and%20Christopher%20Schroers%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20Novel%20View%20Synthesis%20%28NVS%29%2C%20generating%0Ahigh-fidelity%20views%20from%20single%20or%20sparse%20observations%20remains%20a%20significant%0Achallenge.%20Existing%20splatting-based%20approaches%20often%20produce%20distorted%20geometry%0Adue%20to%20splatting%20errors.%20While%20diffusion-based%20methods%20leverage%20rich%203D%20priors%0Ato%20achieve%20improved%20geometry%2C%20they%20often%20suffer%20from%20texture%20hallucination.%20In%0Athis%20paper%2C%20we%20introduce%20SplatDiff%2C%20a%20pixel-splatting-guided%20video%20diffusion%0Amodel%20designed%20to%20synthesize%20high-fidelity%20novel%20views%20from%20a%20single%20image.%0ASpecifically%2C%20we%20propose%20an%20aligned%20synthesis%20strategy%20for%20precise%20control%20of%0Atarget%20viewpoints%20and%20geometry-consistent%20view%20synthesis.%20To%20mitigate%20texture%0Ahallucination%2C%20we%20design%20a%20texture%20bridge%20module%20that%20enables%20high-fidelity%0Atexture%20generation%20through%20adaptive%20feature%20fusion.%20In%20this%20manner%2C%20SplatDiff%0Aleverages%20the%20strengths%20of%20splatting%20and%20diffusion%20to%20generate%20novel%20views%20with%0Aconsistent%20geometry%20and%20high-fidelity%20details.%20Extensive%20experiments%20verify%20the%0Astate-of-the-art%20performance%20of%20SplatDiff%20in%20single-view%20NVS.%20Additionally%2C%0Awithout%20extra%20training%2C%20SplatDiff%20shows%20remarkable%20zero-shot%20performance%20across%0Adiverse%20tasks%2C%20including%20sparse-view%20NVS%20and%20stereo%20video%20conversion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Fidelity%2520Novel%2520View%2520Synthesis%2520via%2520Splatting-Guided%2520Diffusion%26entry.906535625%3DXiang%2520Zhang%2520and%2520Yang%2520Zhang%2520and%2520Lukas%2520Mehl%2520and%2520Markus%2520Gross%2520and%2520Christopher%2520Schroers%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520Novel%2520View%2520Synthesis%2520%2528NVS%2529%252C%2520generating%250Ahigh-fidelity%2520views%2520from%2520single%2520or%2520sparse%2520observations%2520remains%2520a%2520significant%250Achallenge.%2520Existing%2520splatting-based%2520approaches%2520often%2520produce%2520distorted%2520geometry%250Adue%2520to%2520splatting%2520errors.%2520While%2520diffusion-based%2520methods%2520leverage%2520rich%25203D%2520priors%250Ato%2520achieve%2520improved%2520geometry%252C%2520they%2520often%2520suffer%2520from%2520texture%2520hallucination.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520SplatDiff%252C%2520a%2520pixel-splatting-guided%2520video%2520diffusion%250Amodel%2520designed%2520to%2520synthesize%2520high-fidelity%2520novel%2520views%2520from%2520a%2520single%2520image.%250ASpecifically%252C%2520we%2520propose%2520an%2520aligned%2520synthesis%2520strategy%2520for%2520precise%2520control%2520of%250Atarget%2520viewpoints%2520and%2520geometry-consistent%2520view%2520synthesis.%2520To%2520mitigate%2520texture%250Ahallucination%252C%2520we%2520design%2520a%2520texture%2520bridge%2520module%2520that%2520enables%2520high-fidelity%250Atexture%2520generation%2520through%2520adaptive%2520feature%2520fusion.%2520In%2520this%2520manner%252C%2520SplatDiff%250Aleverages%2520the%2520strengths%2520of%2520splatting%2520and%2520diffusion%2520to%2520generate%2520novel%2520views%2520with%250Aconsistent%2520geometry%2520and%2520high-fidelity%2520details.%2520Extensive%2520experiments%2520verify%2520the%250Astate-of-the-art%2520performance%2520of%2520SplatDiff%2520in%2520single-view%2520NVS.%2520Additionally%252C%250Awithout%2520extra%2520training%252C%2520SplatDiff%2520shows%2520remarkable%2520zero-shot%2520performance%2520across%250Adiverse%2520tasks%252C%2520including%2520sparse-view%2520NVS%2520and%2520stereo%2520video%2520conversion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Fidelity%20Novel%20View%20Synthesis%20via%20Splatting-Guided%20Diffusion&entry.906535625=Xiang%20Zhang%20and%20Yang%20Zhang%20and%20Lukas%20Mehl%20and%20Markus%20Gross%20and%20Christopher%20Schroers&entry.1292438233=%20%20Despite%20recent%20advances%20in%20Novel%20View%20Synthesis%20%28NVS%29%2C%20generating%0Ahigh-fidelity%20views%20from%20single%20or%20sparse%20observations%20remains%20a%20significant%0Achallenge.%20Existing%20splatting-based%20approaches%20often%20produce%20distorted%20geometry%0Adue%20to%20splatting%20errors.%20While%20diffusion-based%20methods%20leverage%20rich%203D%20priors%0Ato%20achieve%20improved%20geometry%2C%20they%20often%20suffer%20from%20texture%20hallucination.%20In%0Athis%20paper%2C%20we%20introduce%20SplatDiff%2C%20a%20pixel-splatting-guided%20video%20diffusion%0Amodel%20designed%20to%20synthesize%20high-fidelity%20novel%20views%20from%20a%20single%20image.%0ASpecifically%2C%20we%20propose%20an%20aligned%20synthesis%20strategy%20for%20precise%20control%20of%0Atarget%20viewpoints%20and%20geometry-consistent%20view%20synthesis.%20To%20mitigate%20texture%0Ahallucination%2C%20we%20design%20a%20texture%20bridge%20module%20that%20enables%20high-fidelity%0Atexture%20generation%20through%20adaptive%20feature%20fusion.%20In%20this%20manner%2C%20SplatDiff%0Aleverages%20the%20strengths%20of%20splatting%20and%20diffusion%20to%20generate%20novel%20views%20with%0Aconsistent%20geometry%20and%20high-fidelity%20details.%20Extensive%20experiments%20verify%20the%0Astate-of-the-art%20performance%20of%20SplatDiff%20in%20single-view%20NVS.%20Additionally%2C%0Awithout%20extra%20training%2C%20SplatDiff%20shows%20remarkable%20zero-shot%20performance%20across%0Adiverse%20tasks%2C%20including%20sparse-view%20NVS%20and%20stereo%20video%20conversion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12752v1&entry.124074799=Read"},
{"title": "L4P: Low-Level 4D Vision Perception Unified", "author": "Abhishek Badki and Hang Su and Bowen Wen and Orazio Gallo", "abstract": "  The spatio-temporal relationship between the pixels of a video carries\ncritical information for low-level 4D perception. A single model that reasons\nabout it should be able to solve several such tasks well. Yet, most\nstate-of-the-art methods rely on architectures specialized for the task at\nhand. We present L4P (pronounced \"LAP\"), a feedforward, general-purpose\narchitecture that solves low-level 4D perception tasks in a unified framework.\nL4P combines a ViT-based backbone with per-task heads that are lightweight and\ntherefore do not require extensive training. Despite its general and\nfeedforward formulation, our method matches or surpasses the performance of\nexisting specialized methods on both dense tasks, such as depth or optical flow\nestimation, and sparse tasks, such as 2D/3D tracking. Moreover, it solves all\nthose tasks at once in a time comparable to that of individual single-task\nmethods.\n", "link": "http://arxiv.org/abs/2502.13078v1", "date": "2025-02-18", "relevancy": 3.1216, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6346}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L4P%3A%20Low-Level%204D%20Vision%20Perception%20Unified&body=Title%3A%20L4P%3A%20Low-Level%204D%20Vision%20Perception%20Unified%0AAuthor%3A%20Abhishek%20Badki%20and%20Hang%20Su%20and%20Bowen%20Wen%20and%20Orazio%20Gallo%0AAbstract%3A%20%20%20The%20spatio-temporal%20relationship%20between%20the%20pixels%20of%20a%20video%20carries%0Acritical%20information%20for%20low-level%204D%20perception.%20A%20single%20model%20that%20reasons%0Aabout%20it%20should%20be%20able%20to%20solve%20several%20such%20tasks%20well.%20Yet%2C%20most%0Astate-of-the-art%20methods%20rely%20on%20architectures%20specialized%20for%20the%20task%20at%0Ahand.%20We%20present%20L4P%20%28pronounced%20%22LAP%22%29%2C%20a%20feedforward%2C%20general-purpose%0Aarchitecture%20that%20solves%20low-level%204D%20perception%20tasks%20in%20a%20unified%20framework.%0AL4P%20combines%20a%20ViT-based%20backbone%20with%20per-task%20heads%20that%20are%20lightweight%20and%0Atherefore%20do%20not%20require%20extensive%20training.%20Despite%20its%20general%20and%0Afeedforward%20formulation%2C%20our%20method%20matches%20or%20surpasses%20the%20performance%20of%0Aexisting%20specialized%20methods%20on%20both%20dense%20tasks%2C%20such%20as%20depth%20or%20optical%20flow%0Aestimation%2C%20and%20sparse%20tasks%2C%20such%20as%202D/3D%20tracking.%20Moreover%2C%20it%20solves%20all%0Athose%20tasks%20at%20once%20in%20a%20time%20comparable%20to%20that%20of%20individual%20single-task%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL4P%253A%2520Low-Level%25204D%2520Vision%2520Perception%2520Unified%26entry.906535625%3DAbhishek%2520Badki%2520and%2520Hang%2520Su%2520and%2520Bowen%2520Wen%2520and%2520Orazio%2520Gallo%26entry.1292438233%3D%2520%2520The%2520spatio-temporal%2520relationship%2520between%2520the%2520pixels%2520of%2520a%2520video%2520carries%250Acritical%2520information%2520for%2520low-level%25204D%2520perception.%2520A%2520single%2520model%2520that%2520reasons%250Aabout%2520it%2520should%2520be%2520able%2520to%2520solve%2520several%2520such%2520tasks%2520well.%2520Yet%252C%2520most%250Astate-of-the-art%2520methods%2520rely%2520on%2520architectures%2520specialized%2520for%2520the%2520task%2520at%250Ahand.%2520We%2520present%2520L4P%2520%2528pronounced%2520%2522LAP%2522%2529%252C%2520a%2520feedforward%252C%2520general-purpose%250Aarchitecture%2520that%2520solves%2520low-level%25204D%2520perception%2520tasks%2520in%2520a%2520unified%2520framework.%250AL4P%2520combines%2520a%2520ViT-based%2520backbone%2520with%2520per-task%2520heads%2520that%2520are%2520lightweight%2520and%250Atherefore%2520do%2520not%2520require%2520extensive%2520training.%2520Despite%2520its%2520general%2520and%250Afeedforward%2520formulation%252C%2520our%2520method%2520matches%2520or%2520surpasses%2520the%2520performance%2520of%250Aexisting%2520specialized%2520methods%2520on%2520both%2520dense%2520tasks%252C%2520such%2520as%2520depth%2520or%2520optical%2520flow%250Aestimation%252C%2520and%2520sparse%2520tasks%252C%2520such%2520as%25202D/3D%2520tracking.%2520Moreover%252C%2520it%2520solves%2520all%250Athose%2520tasks%2520at%2520once%2520in%2520a%2520time%2520comparable%2520to%2520that%2520of%2520individual%2520single-task%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L4P%3A%20Low-Level%204D%20Vision%20Perception%20Unified&entry.906535625=Abhishek%20Badki%20and%20Hang%20Su%20and%20Bowen%20Wen%20and%20Orazio%20Gallo&entry.1292438233=%20%20The%20spatio-temporal%20relationship%20between%20the%20pixels%20of%20a%20video%20carries%0Acritical%20information%20for%20low-level%204D%20perception.%20A%20single%20model%20that%20reasons%0Aabout%20it%20should%20be%20able%20to%20solve%20several%20such%20tasks%20well.%20Yet%2C%20most%0Astate-of-the-art%20methods%20rely%20on%20architectures%20specialized%20for%20the%20task%20at%0Ahand.%20We%20present%20L4P%20%28pronounced%20%22LAP%22%29%2C%20a%20feedforward%2C%20general-purpose%0Aarchitecture%20that%20solves%20low-level%204D%20perception%20tasks%20in%20a%20unified%20framework.%0AL4P%20combines%20a%20ViT-based%20backbone%20with%20per-task%20heads%20that%20are%20lightweight%20and%0Atherefore%20do%20not%20require%20extensive%20training.%20Despite%20its%20general%20and%0Afeedforward%20formulation%2C%20our%20method%20matches%20or%20surpasses%20the%20performance%20of%0Aexisting%20specialized%20methods%20on%20both%20dense%20tasks%2C%20such%20as%20depth%20or%20optical%20flow%0Aestimation%2C%20and%20sparse%20tasks%2C%20such%20as%202D/3D%20tracking.%20Moreover%2C%20it%20solves%20all%0Athose%20tasks%20at%20once%20in%20a%20time%20comparable%20to%20that%20of%20individual%20single-task%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13078v1&entry.124074799=Read"},
{"title": "Spherical Dense Text-to-Image Synthesis", "author": "Timon Winter and Stanislav Frolov and Brian Bernhard Moser and Andreas Dengel", "abstract": "  Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF.\n", "link": "http://arxiv.org/abs/2502.12691v1", "date": "2025-02-18", "relevancy": 3.0884, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6246}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6246}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spherical%20Dense%20Text-to-Image%20Synthesis&body=Title%3A%20Spherical%20Dense%20Text-to-Image%20Synthesis%0AAuthor%3A%20Timon%20Winter%20and%20Stanislav%20Frolov%20and%20Brian%20Bernhard%20Moser%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-image%20%28T2I%29%20have%20improved%20synthesis%20results%2C%0Abut%20challenges%20remain%20in%20layout%20control%20and%20generating%20omnidirectional%0Apanoramic%20images.%20Dense%20T2I%20%28DT2I%29%20and%20spherical%20T2I%20%28ST2I%29%20models%20address%0Athese%20issues%2C%20but%20so%20far%20no%20unified%20approach%20exists.%20Trivial%20approaches%2C%20like%0Aprompting%20a%20DT2I%20model%20to%20generate%20panoramas%20can%20not%20generate%20proper%20spherical%0Adistortions%20and%20seamless%20transitions%20at%20the%20borders.%20Our%20work%20shows%20that%0Aspherical%20dense%20text-to-image%20%28SDT2I%29%20can%20be%20achieved%20by%20integrating%0Atraining-free%20DT2I%20approaches%20into%20finetuned%20panorama%20models.%20Specifically%2C%20we%0Apropose%20MultiStitchDiffusion%20%28MSTD%29%20and%20MultiPanFusion%20%28MPF%29%20by%20integrating%0AMultiDiffusion%20into%20StitchDiffusion%20and%20PanFusion%2C%20respectively.%20Since%20no%0Abenchmark%20for%20SDT2I%20exists%2C%20we%20further%20construct%20Dense-Synthetic-View%0A%28DSynView%29%2C%20a%20new%20synthetic%20dataset%20containing%20spherical%20layouts%20to%20evaluate%0Aour%20models.%20Our%20results%20show%20that%20MSTD%20outperforms%20MPF%20across%20image%20quality%20as%0Awell%20as%20prompt-%20and%20layout%20adherence.%20MultiPanFusion%20generates%20more%20diverse%0Aimages%20but%20struggles%20to%20synthesize%20flawless%20foreground%20objects.%20We%20propose%0Abootstrap-coupling%20and%20turning%20off%20equirectangular%20perspective-projection%0Aattention%20in%20the%20foreground%20as%20an%20improvement%20of%20MPF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpherical%2520Dense%2520Text-to-Image%2520Synthesis%26entry.906535625%3DTimon%2520Winter%2520and%2520Stanislav%2520Frolov%2520and%2520Brian%2520Bernhard%2520Moser%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-image%2520%2528T2I%2529%2520have%2520improved%2520synthesis%2520results%252C%250Abut%2520challenges%2520remain%2520in%2520layout%2520control%2520and%2520generating%2520omnidirectional%250Apanoramic%2520images.%2520Dense%2520T2I%2520%2528DT2I%2529%2520and%2520spherical%2520T2I%2520%2528ST2I%2529%2520models%2520address%250Athese%2520issues%252C%2520but%2520so%2520far%2520no%2520unified%2520approach%2520exists.%2520Trivial%2520approaches%252C%2520like%250Aprompting%2520a%2520DT2I%2520model%2520to%2520generate%2520panoramas%2520can%2520not%2520generate%2520proper%2520spherical%250Adistortions%2520and%2520seamless%2520transitions%2520at%2520the%2520borders.%2520Our%2520work%2520shows%2520that%250Aspherical%2520dense%2520text-to-image%2520%2528SDT2I%2529%2520can%2520be%2520achieved%2520by%2520integrating%250Atraining-free%2520DT2I%2520approaches%2520into%2520finetuned%2520panorama%2520models.%2520Specifically%252C%2520we%250Apropose%2520MultiStitchDiffusion%2520%2528MSTD%2529%2520and%2520MultiPanFusion%2520%2528MPF%2529%2520by%2520integrating%250AMultiDiffusion%2520into%2520StitchDiffusion%2520and%2520PanFusion%252C%2520respectively.%2520Since%2520no%250Abenchmark%2520for%2520SDT2I%2520exists%252C%2520we%2520further%2520construct%2520Dense-Synthetic-View%250A%2528DSynView%2529%252C%2520a%2520new%2520synthetic%2520dataset%2520containing%2520spherical%2520layouts%2520to%2520evaluate%250Aour%2520models.%2520Our%2520results%2520show%2520that%2520MSTD%2520outperforms%2520MPF%2520across%2520image%2520quality%2520as%250Awell%2520as%2520prompt-%2520and%2520layout%2520adherence.%2520MultiPanFusion%2520generates%2520more%2520diverse%250Aimages%2520but%2520struggles%2520to%2520synthesize%2520flawless%2520foreground%2520objects.%2520We%2520propose%250Abootstrap-coupling%2520and%2520turning%2520off%2520equirectangular%2520perspective-projection%250Aattention%2520in%2520the%2520foreground%2520as%2520an%2520improvement%2520of%2520MPF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spherical%20Dense%20Text-to-Image%20Synthesis&entry.906535625=Timon%20Winter%20and%20Stanislav%20Frolov%20and%20Brian%20Bernhard%20Moser%20and%20Andreas%20Dengel&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-image%20%28T2I%29%20have%20improved%20synthesis%20results%2C%0Abut%20challenges%20remain%20in%20layout%20control%20and%20generating%20omnidirectional%0Apanoramic%20images.%20Dense%20T2I%20%28DT2I%29%20and%20spherical%20T2I%20%28ST2I%29%20models%20address%0Athese%20issues%2C%20but%20so%20far%20no%20unified%20approach%20exists.%20Trivial%20approaches%2C%20like%0Aprompting%20a%20DT2I%20model%20to%20generate%20panoramas%20can%20not%20generate%20proper%20spherical%0Adistortions%20and%20seamless%20transitions%20at%20the%20borders.%20Our%20work%20shows%20that%0Aspherical%20dense%20text-to-image%20%28SDT2I%29%20can%20be%20achieved%20by%20integrating%0Atraining-free%20DT2I%20approaches%20into%20finetuned%20panorama%20models.%20Specifically%2C%20we%0Apropose%20MultiStitchDiffusion%20%28MSTD%29%20and%20MultiPanFusion%20%28MPF%29%20by%20integrating%0AMultiDiffusion%20into%20StitchDiffusion%20and%20PanFusion%2C%20respectively.%20Since%20no%0Abenchmark%20for%20SDT2I%20exists%2C%20we%20further%20construct%20Dense-Synthetic-View%0A%28DSynView%29%2C%20a%20new%20synthetic%20dataset%20containing%20spherical%20layouts%20to%20evaluate%0Aour%20models.%20Our%20results%20show%20that%20MSTD%20outperforms%20MPF%20across%20image%20quality%20as%0Awell%20as%20prompt-%20and%20layout%20adherence.%20MultiPanFusion%20generates%20more%20diverse%0Aimages%20but%20struggles%20to%20synthesize%20flawless%20foreground%20objects.%20We%20propose%0Abootstrap-coupling%20and%20turning%20off%20equirectangular%20perspective-projection%0Aattention%20in%20the%20foreground%20as%20an%20improvement%20of%20MPF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12691v1&entry.124074799=Read"},
{"title": "3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from\n  Cortical Surfaces", "author": "Fabian Bongratz and Yitong Li and Sama Elbaroudy and Christian Wachinger", "abstract": "  Despite recent advances in medical image generation, existing methods\nstruggle to produce anatomically plausible 3D structures. In synthetic brain\nmagnetic resonance images (MRIs), characteristic fissures are often missing,\nand reconstructed cortical surfaces appear scattered rather than densely\nconvoluted. To address this issue, we introduce Cor2Vox, the first diffusion\nmodel-based method that translates continuous cortical shape priors to\nsynthetic brain MRIs. To achieve this, we leverage a Brownian bridge process\nwhich allows for direct structured mapping between shape contours and medical\nimages. Specifically, we adapt the concept of the Brownian bridge diffusion\nmodel to 3D and extend it to embrace various complementary shape\nrepresentations. Our experiments demonstrate significant improvements in the\ngeometric accuracy of reconstructed structures compared to previous voxel-based\napproaches. Moreover, Cor2Vox excels in image quality and diversity, yielding\nhigh variation in non-target structures like the skull. Finally, we highlight\nthe capability of our approach to simulate cortical atrophy at the sub-voxel\nlevel. Our code is available at https://github.com/ai-med/Cor2Vox.\n", "link": "http://arxiv.org/abs/2502.12742v1", "date": "2025-02-18", "relevancy": 3.0502, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6188}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6188}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Shape-to-Image%20Brownian%20Bridge%20Diffusion%20for%20Brain%20MRI%20Synthesis%20from%0A%20%20Cortical%20Surfaces&body=Title%3A%203D%20Shape-to-Image%20Brownian%20Bridge%20Diffusion%20for%20Brain%20MRI%20Synthesis%20from%0A%20%20Cortical%20Surfaces%0AAuthor%3A%20Fabian%20Bongratz%20and%20Yitong%20Li%20and%20Sama%20Elbaroudy%20and%20Christian%20Wachinger%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20medical%20image%20generation%2C%20existing%20methods%0Astruggle%20to%20produce%20anatomically%20plausible%203D%20structures.%20In%20synthetic%20brain%0Amagnetic%20resonance%20images%20%28MRIs%29%2C%20characteristic%20fissures%20are%20often%20missing%2C%0Aand%20reconstructed%20cortical%20surfaces%20appear%20scattered%20rather%20than%20densely%0Aconvoluted.%20To%20address%20this%20issue%2C%20we%20introduce%20Cor2Vox%2C%20the%20first%20diffusion%0Amodel-based%20method%20that%20translates%20continuous%20cortical%20shape%20priors%20to%0Asynthetic%20brain%20MRIs.%20To%20achieve%20this%2C%20we%20leverage%20a%20Brownian%20bridge%20process%0Awhich%20allows%20for%20direct%20structured%20mapping%20between%20shape%20contours%20and%20medical%0Aimages.%20Specifically%2C%20we%20adapt%20the%20concept%20of%20the%20Brownian%20bridge%20diffusion%0Amodel%20to%203D%20and%20extend%20it%20to%20embrace%20various%20complementary%20shape%0Arepresentations.%20Our%20experiments%20demonstrate%20significant%20improvements%20in%20the%0Ageometric%20accuracy%20of%20reconstructed%20structures%20compared%20to%20previous%20voxel-based%0Aapproaches.%20Moreover%2C%20Cor2Vox%20excels%20in%20image%20quality%20and%20diversity%2C%20yielding%0Ahigh%20variation%20in%20non-target%20structures%20like%20the%20skull.%20Finally%2C%20we%20highlight%0Athe%20capability%20of%20our%20approach%20to%20simulate%20cortical%20atrophy%20at%20the%20sub-voxel%0Alevel.%20Our%20code%20is%20available%20at%20https%3A//github.com/ai-med/Cor2Vox.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Shape-to-Image%2520Brownian%2520Bridge%2520Diffusion%2520for%2520Brain%2520MRI%2520Synthesis%2520from%250A%2520%2520Cortical%2520Surfaces%26entry.906535625%3DFabian%2520Bongratz%2520and%2520Yitong%2520Li%2520and%2520Sama%2520Elbaroudy%2520and%2520Christian%2520Wachinger%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520medical%2520image%2520generation%252C%2520existing%2520methods%250Astruggle%2520to%2520produce%2520anatomically%2520plausible%25203D%2520structures.%2520In%2520synthetic%2520brain%250Amagnetic%2520resonance%2520images%2520%2528MRIs%2529%252C%2520characteristic%2520fissures%2520are%2520often%2520missing%252C%250Aand%2520reconstructed%2520cortical%2520surfaces%2520appear%2520scattered%2520rather%2520than%2520densely%250Aconvoluted.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Cor2Vox%252C%2520the%2520first%2520diffusion%250Amodel-based%2520method%2520that%2520translates%2520continuous%2520cortical%2520shape%2520priors%2520to%250Asynthetic%2520brain%2520MRIs.%2520To%2520achieve%2520this%252C%2520we%2520leverage%2520a%2520Brownian%2520bridge%2520process%250Awhich%2520allows%2520for%2520direct%2520structured%2520mapping%2520between%2520shape%2520contours%2520and%2520medical%250Aimages.%2520Specifically%252C%2520we%2520adapt%2520the%2520concept%2520of%2520the%2520Brownian%2520bridge%2520diffusion%250Amodel%2520to%25203D%2520and%2520extend%2520it%2520to%2520embrace%2520various%2520complementary%2520shape%250Arepresentations.%2520Our%2520experiments%2520demonstrate%2520significant%2520improvements%2520in%2520the%250Ageometric%2520accuracy%2520of%2520reconstructed%2520structures%2520compared%2520to%2520previous%2520voxel-based%250Aapproaches.%2520Moreover%252C%2520Cor2Vox%2520excels%2520in%2520image%2520quality%2520and%2520diversity%252C%2520yielding%250Ahigh%2520variation%2520in%2520non-target%2520structures%2520like%2520the%2520skull.%2520Finally%252C%2520we%2520highlight%250Athe%2520capability%2520of%2520our%2520approach%2520to%2520simulate%2520cortical%2520atrophy%2520at%2520the%2520sub-voxel%250Alevel.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ai-med/Cor2Vox.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Shape-to-Image%20Brownian%20Bridge%20Diffusion%20for%20Brain%20MRI%20Synthesis%20from%0A%20%20Cortical%20Surfaces&entry.906535625=Fabian%20Bongratz%20and%20Yitong%20Li%20and%20Sama%20Elbaroudy%20and%20Christian%20Wachinger&entry.1292438233=%20%20Despite%20recent%20advances%20in%20medical%20image%20generation%2C%20existing%20methods%0Astruggle%20to%20produce%20anatomically%20plausible%203D%20structures.%20In%20synthetic%20brain%0Amagnetic%20resonance%20images%20%28MRIs%29%2C%20characteristic%20fissures%20are%20often%20missing%2C%0Aand%20reconstructed%20cortical%20surfaces%20appear%20scattered%20rather%20than%20densely%0Aconvoluted.%20To%20address%20this%20issue%2C%20we%20introduce%20Cor2Vox%2C%20the%20first%20diffusion%0Amodel-based%20method%20that%20translates%20continuous%20cortical%20shape%20priors%20to%0Asynthetic%20brain%20MRIs.%20To%20achieve%20this%2C%20we%20leverage%20a%20Brownian%20bridge%20process%0Awhich%20allows%20for%20direct%20structured%20mapping%20between%20shape%20contours%20and%20medical%0Aimages.%20Specifically%2C%20we%20adapt%20the%20concept%20of%20the%20Brownian%20bridge%20diffusion%0Amodel%20to%203D%20and%20extend%20it%20to%20embrace%20various%20complementary%20shape%0Arepresentations.%20Our%20experiments%20demonstrate%20significant%20improvements%20in%20the%0Ageometric%20accuracy%20of%20reconstructed%20structures%20compared%20to%20previous%20voxel-based%0Aapproaches.%20Moreover%2C%20Cor2Vox%20excels%20in%20image%20quality%20and%20diversity%2C%20yielding%0Ahigh%20variation%20in%20non-target%20structures%20like%20the%20skull.%20Finally%2C%20we%20highlight%0Athe%20capability%20of%20our%20approach%20to%20simulate%20cortical%20atrophy%20at%20the%20sub-voxel%0Alevel.%20Our%20code%20is%20available%20at%20https%3A//github.com/ai-med/Cor2Vox.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12742v1&entry.124074799=Read"},
{"title": "Multi-scale Attention Guided Pose Transfer", "author": "Prasun Roy and Saumik Bhattacharya and Subhankar Ghosh and Umapada Pal", "abstract": "  Pose transfer refers to the probabilistic image generation of a person with a\npreviously unseen novel pose from another image of that person having a\ndifferent pose. Due to potential academic and commercial applications, this\nproblem is extensively studied in recent years. Among the various approaches to\nthe problem, attention guided progressive generation is shown to produce\nstate-of-the-art results in most cases. In this paper, we present an improved\nnetwork architecture for pose transfer by introducing attention links at every\nresolution level of the encoder and decoder. By utilizing such dense\nmulti-scale attention guided approach, we are able to achieve significant\nimprovement over the existing methods both visually and analytically. We\nconclude our findings with extensive qualitative and quantitative comparisons\nagainst several existing methods on the DeepFashion dataset.\n", "link": "http://arxiv.org/abs/2202.06777v2", "date": "2025-02-18", "relevancy": 2.9269, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.595}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5944}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20Attention%20Guided%20Pose%20Transfer&body=Title%3A%20Multi-scale%20Attention%20Guided%20Pose%20Transfer%0AAuthor%3A%20Prasun%20Roy%20and%20Saumik%20Bhattacharya%20and%20Subhankar%20Ghosh%20and%20Umapada%20Pal%0AAbstract%3A%20%20%20Pose%20transfer%20refers%20to%20the%20probabilistic%20image%20generation%20of%20a%20person%20with%20a%0Apreviously%20unseen%20novel%20pose%20from%20another%20image%20of%20that%20person%20having%20a%0Adifferent%20pose.%20Due%20to%20potential%20academic%20and%20commercial%20applications%2C%20this%0Aproblem%20is%20extensively%20studied%20in%20recent%20years.%20Among%20the%20various%20approaches%20to%0Athe%20problem%2C%20attention%20guided%20progressive%20generation%20is%20shown%20to%20produce%0Astate-of-the-art%20results%20in%20most%20cases.%20In%20this%20paper%2C%20we%20present%20an%20improved%0Anetwork%20architecture%20for%20pose%20transfer%20by%20introducing%20attention%20links%20at%20every%0Aresolution%20level%20of%20the%20encoder%20and%20decoder.%20By%20utilizing%20such%20dense%0Amulti-scale%20attention%20guided%20approach%2C%20we%20are%20able%20to%20achieve%20significant%0Aimprovement%20over%20the%20existing%20methods%20both%20visually%20and%20analytically.%20We%0Aconclude%20our%20findings%20with%20extensive%20qualitative%20and%20quantitative%20comparisons%0Aagainst%20several%20existing%20methods%20on%20the%20DeepFashion%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.06777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-scale%2520Attention%2520Guided%2520Pose%2520Transfer%26entry.906535625%3DPrasun%2520Roy%2520and%2520Saumik%2520Bhattacharya%2520and%2520Subhankar%2520Ghosh%2520and%2520Umapada%2520Pal%26entry.1292438233%3D%2520%2520Pose%2520transfer%2520refers%2520to%2520the%2520probabilistic%2520image%2520generation%2520of%2520a%2520person%2520with%2520a%250Apreviously%2520unseen%2520novel%2520pose%2520from%2520another%2520image%2520of%2520that%2520person%2520having%2520a%250Adifferent%2520pose.%2520Due%2520to%2520potential%2520academic%2520and%2520commercial%2520applications%252C%2520this%250Aproblem%2520is%2520extensively%2520studied%2520in%2520recent%2520years.%2520Among%2520the%2520various%2520approaches%2520to%250Athe%2520problem%252C%2520attention%2520guided%2520progressive%2520generation%2520is%2520shown%2520to%2520produce%250Astate-of-the-art%2520results%2520in%2520most%2520cases.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520improved%250Anetwork%2520architecture%2520for%2520pose%2520transfer%2520by%2520introducing%2520attention%2520links%2520at%2520every%250Aresolution%2520level%2520of%2520the%2520encoder%2520and%2520decoder.%2520By%2520utilizing%2520such%2520dense%250Amulti-scale%2520attention%2520guided%2520approach%252C%2520we%2520are%2520able%2520to%2520achieve%2520significant%250Aimprovement%2520over%2520the%2520existing%2520methods%2520both%2520visually%2520and%2520analytically.%2520We%250Aconclude%2520our%2520findings%2520with%2520extensive%2520qualitative%2520and%2520quantitative%2520comparisons%250Aagainst%2520several%2520existing%2520methods%2520on%2520the%2520DeepFashion%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.06777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20Attention%20Guided%20Pose%20Transfer&entry.906535625=Prasun%20Roy%20and%20Saumik%20Bhattacharya%20and%20Subhankar%20Ghosh%20and%20Umapada%20Pal&entry.1292438233=%20%20Pose%20transfer%20refers%20to%20the%20probabilistic%20image%20generation%20of%20a%20person%20with%20a%0Apreviously%20unseen%20novel%20pose%20from%20another%20image%20of%20that%20person%20having%20a%0Adifferent%20pose.%20Due%20to%20potential%20academic%20and%20commercial%20applications%2C%20this%0Aproblem%20is%20extensively%20studied%20in%20recent%20years.%20Among%20the%20various%20approaches%20to%0Athe%20problem%2C%20attention%20guided%20progressive%20generation%20is%20shown%20to%20produce%0Astate-of-the-art%20results%20in%20most%20cases.%20In%20this%20paper%2C%20we%20present%20an%20improved%0Anetwork%20architecture%20for%20pose%20transfer%20by%20introducing%20attention%20links%20at%20every%0Aresolution%20level%20of%20the%20encoder%20and%20decoder.%20By%20utilizing%20such%20dense%0Amulti-scale%20attention%20guided%20approach%2C%20we%20are%20able%20to%20achieve%20significant%0Aimprovement%20over%20the%20existing%20methods%20both%20visually%20and%20analytically.%20We%0Aconclude%20our%20findings%20with%20extensive%20qualitative%20and%20quantitative%20comparisons%0Aagainst%20several%20existing%20methods%20on%20the%20DeepFashion%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.06777v2&entry.124074799=Read"},
{"title": "R3L: Relative Representations for Reinforcement Learning", "author": "Antonio Pio Ricciardi and Valentino Maiorca and Luca Moschella and Riccardo Marin and Emanuele Rodol\u00e0", "abstract": "  Visual Reinforcement Learning is a popular and powerful framework that takes\nfull advantage of the Deep Learning breakthrough. It is known that variations\nin input domains (e.g., different panorama colors due to seasonal changes) or\ntask domains (e.g., altering the target speed of a car) can disrupt agent\nperformance, necessitating new training for each variation. Recent advancements\nin the field of representation learning have demonstrated the possibility of\ncombining components from different neural networks to create new models in a\nzero-shot fashion. In this paper, we build upon relative representations, a\nframework that maps encoder embeddings to a universal space. We adapt this\nframework to the Visual Reinforcement Learning setting, allowing to combine\nagents components to create new agents capable of effectively handling novel\nvisual-task pairs not encountered during training. Our findings highlight the\npotential for model reuse, significantly reducing the need for retraining and,\nconsequently, the time and computational resources required.\n", "link": "http://arxiv.org/abs/2404.12917v3", "date": "2025-02-18", "relevancy": 2.9266, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R3L%3A%20Relative%20Representations%20for%20Reinforcement%20Learning&body=Title%3A%20R3L%3A%20Relative%20Representations%20for%20Reinforcement%20Learning%0AAuthor%3A%20Antonio%20Pio%20Ricciardi%20and%20Valentino%20Maiorca%20and%20Luca%20Moschella%20and%20Riccardo%20Marin%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Visual%20Reinforcement%20Learning%20is%20a%20popular%20and%20powerful%20framework%20that%20takes%0Afull%20advantage%20of%20the%20Deep%20Learning%20breakthrough.%20It%20is%20known%20that%20variations%0Ain%20input%20domains%20%28e.g.%2C%20different%20panorama%20colors%20due%20to%20seasonal%20changes%29%20or%0Atask%20domains%20%28e.g.%2C%20altering%20the%20target%20speed%20of%20a%20car%29%20can%20disrupt%20agent%0Aperformance%2C%20necessitating%20new%20training%20for%20each%20variation.%20Recent%20advancements%0Ain%20the%20field%20of%20representation%20learning%20have%20demonstrated%20the%20possibility%20of%0Acombining%20components%20from%20different%20neural%20networks%20to%20create%20new%20models%20in%20a%0Azero-shot%20fashion.%20In%20this%20paper%2C%20we%20build%20upon%20relative%20representations%2C%20a%0Aframework%20that%20maps%20encoder%20embeddings%20to%20a%20universal%20space.%20We%20adapt%20this%0Aframework%20to%20the%20Visual%20Reinforcement%20Learning%20setting%2C%20allowing%20to%20combine%0Aagents%20components%20to%20create%20new%20agents%20capable%20of%20effectively%20handling%20novel%0Avisual-task%20pairs%20not%20encountered%20during%20training.%20Our%20findings%20highlight%20the%0Apotential%20for%20model%20reuse%2C%20significantly%20reducing%20the%20need%20for%20retraining%20and%2C%0Aconsequently%2C%20the%20time%20and%20computational%20resources%20required.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12917v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR3L%253A%2520Relative%2520Representations%2520for%2520Reinforcement%2520Learning%26entry.906535625%3DAntonio%2520Pio%2520Ricciardi%2520and%2520Valentino%2520Maiorca%2520and%2520Luca%2520Moschella%2520and%2520Riccardo%2520Marin%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520Visual%2520Reinforcement%2520Learning%2520is%2520a%2520popular%2520and%2520powerful%2520framework%2520that%2520takes%250Afull%2520advantage%2520of%2520the%2520Deep%2520Learning%2520breakthrough.%2520It%2520is%2520known%2520that%2520variations%250Ain%2520input%2520domains%2520%2528e.g.%252C%2520different%2520panorama%2520colors%2520due%2520to%2520seasonal%2520changes%2529%2520or%250Atask%2520domains%2520%2528e.g.%252C%2520altering%2520the%2520target%2520speed%2520of%2520a%2520car%2529%2520can%2520disrupt%2520agent%250Aperformance%252C%2520necessitating%2520new%2520training%2520for%2520each%2520variation.%2520Recent%2520advancements%250Ain%2520the%2520field%2520of%2520representation%2520learning%2520have%2520demonstrated%2520the%2520possibility%2520of%250Acombining%2520components%2520from%2520different%2520neural%2520networks%2520to%2520create%2520new%2520models%2520in%2520a%250Azero-shot%2520fashion.%2520In%2520this%2520paper%252C%2520we%2520build%2520upon%2520relative%2520representations%252C%2520a%250Aframework%2520that%2520maps%2520encoder%2520embeddings%2520to%2520a%2520universal%2520space.%2520We%2520adapt%2520this%250Aframework%2520to%2520the%2520Visual%2520Reinforcement%2520Learning%2520setting%252C%2520allowing%2520to%2520combine%250Aagents%2520components%2520to%2520create%2520new%2520agents%2520capable%2520of%2520effectively%2520handling%2520novel%250Avisual-task%2520pairs%2520not%2520encountered%2520during%2520training.%2520Our%2520findings%2520highlight%2520the%250Apotential%2520for%2520model%2520reuse%252C%2520significantly%2520reducing%2520the%2520need%2520for%2520retraining%2520and%252C%250Aconsequently%252C%2520the%2520time%2520and%2520computational%2520resources%2520required.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12917v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R3L%3A%20Relative%20Representations%20for%20Reinforcement%20Learning&entry.906535625=Antonio%20Pio%20Ricciardi%20and%20Valentino%20Maiorca%20and%20Luca%20Moschella%20and%20Riccardo%20Marin%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Visual%20Reinforcement%20Learning%20is%20a%20popular%20and%20powerful%20framework%20that%20takes%0Afull%20advantage%20of%20the%20Deep%20Learning%20breakthrough.%20It%20is%20known%20that%20variations%0Ain%20input%20domains%20%28e.g.%2C%20different%20panorama%20colors%20due%20to%20seasonal%20changes%29%20or%0Atask%20domains%20%28e.g.%2C%20altering%20the%20target%20speed%20of%20a%20car%29%20can%20disrupt%20agent%0Aperformance%2C%20necessitating%20new%20training%20for%20each%20variation.%20Recent%20advancements%0Ain%20the%20field%20of%20representation%20learning%20have%20demonstrated%20the%20possibility%20of%0Acombining%20components%20from%20different%20neural%20networks%20to%20create%20new%20models%20in%20a%0Azero-shot%20fashion.%20In%20this%20paper%2C%20we%20build%20upon%20relative%20representations%2C%20a%0Aframework%20that%20maps%20encoder%20embeddings%20to%20a%20universal%20space.%20We%20adapt%20this%0Aframework%20to%20the%20Visual%20Reinforcement%20Learning%20setting%2C%20allowing%20to%20combine%0Aagents%20components%20to%20create%20new%20agents%20capable%20of%20effectively%20handling%20novel%0Avisual-task%20pairs%20not%20encountered%20during%20training.%20Our%20findings%20highlight%20the%0Apotential%20for%20model%20reuse%2C%20significantly%20reducing%20the%20need%20for%20retraining%20and%2C%0Aconsequently%2C%20the%20time%20and%20computational%20resources%20required.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12917v3&entry.124074799=Read"},
{"title": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization", "author": "Shuo Xing and Yuping Wang and Peiran Li and Ruizheng Bai and Yueqi Wang and Chengxuan Qian and Huaxiu Yao and Zhengzhong Tu", "abstract": "  The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https://github.com/taco-group/Re-Align.\n", "link": "http://arxiv.org/abs/2502.13146v1", "date": "2025-02-18", "relevancy": 2.9211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-Align%3A%20Aligning%20Vision%20Language%20Models%20via%20Retrieval-Augmented%20Direct%0A%20%20Preference%20Optimization&body=Title%3A%20Re-Align%3A%20Aligning%20Vision%20Language%20Models%20via%20Retrieval-Augmented%20Direct%0A%20%20Preference%20Optimization%0AAuthor%3A%20Shuo%20Xing%20and%20Yuping%20Wang%20and%20Peiran%20Li%20and%20Ruizheng%20Bai%20and%20Yueqi%20Wang%20and%20Chengxuan%20Qian%20and%20Huaxiu%20Yao%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20Vision%20Language%20Models%20%28VLMs%29%20has%20broadened%20the%20scope%0Aand%20capabilities%20of%20single-modal%20Large%20Language%20Models%20%28LLMs%29%20by%20integrating%0Avisual%20modalities%2C%20thereby%20unlocking%20transformative%20cross-modal%20applications%20in%0Aa%20variety%20of%20real-world%20scenarios.%20Despite%20their%20impressive%20performance%2C%20VLMs%0Aare%20prone%20to%20significant%20hallucinations%2C%20particularly%20in%20the%20form%20of%0Across-modal%20inconsistencies.%20Building%20on%20the%20success%20of%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20in%20aligning%20LLMs%2C%20recent%20advancements%20have%20focused%0Aon%20applying%20direct%20preference%20optimization%20%28DPO%29%20on%20carefully%20curated%20datasets%0Ato%20mitigate%20these%20issues.%20Yet%2C%20such%20approaches%20typically%20introduce%20preference%0Asignals%20in%20a%20brute-force%20manner%2C%20neglecting%20the%20crucial%20role%20of%20visual%0Ainformation%20in%20the%20alignment%20process.%20In%20this%20paper%2C%20we%20introduce%20Re-Align%2C%20a%0Anovel%20alignment%20framework%20that%20leverages%20image%20retrieval%20to%20construct%20a%0Adual-preference%20dataset%2C%20effectively%20incorporating%20both%20textual%20and%20visual%0Apreference%20signals.%20We%20further%20introduce%20rDPO%2C%20an%20extension%20of%20the%20standard%0Adirect%20preference%20optimization%20that%20incorporates%20an%20additional%20visual%0Apreference%20objective%20during%20fine-tuning.%20Our%20experimental%20results%20demonstrate%0Athat%20Re-Align%20not%20only%20mitigates%20hallucinations%20more%20effectively%20than%20previous%0Amethods%20but%20also%20yields%20significant%20performance%20gains%20in%20general%20visual%0Aquestion-answering%20%28VQA%29%20tasks.%20Moreover%2C%20we%20show%20that%20Re-Align%20maintains%0Arobustness%20and%20scalability%20across%20a%20wide%20range%20of%20VLM%20sizes%20and%20architectures.%0AThis%20work%20represents%20a%20significant%20step%20forward%20in%20aligning%20multimodal%20LLMs%2C%0Apaving%20the%20way%20for%20more%20reliable%20and%20effective%20cross-modal%20applications.%20We%0Arelease%20all%20the%20code%20in%20https%3A//github.com/taco-group/Re-Align.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-Align%253A%2520Aligning%2520Vision%2520Language%2520Models%2520via%2520Retrieval-Augmented%2520Direct%250A%2520%2520Preference%2520Optimization%26entry.906535625%3DShuo%2520Xing%2520and%2520Yuping%2520Wang%2520and%2520Peiran%2520Li%2520and%2520Ruizheng%2520Bai%2520and%2520Yueqi%2520Wang%2520and%2520Chengxuan%2520Qian%2520and%2520Huaxiu%2520Yao%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520has%2520broadened%2520the%2520scope%250Aand%2520capabilities%2520of%2520single-modal%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520by%2520integrating%250Avisual%2520modalities%252C%2520thereby%2520unlocking%2520transformative%2520cross-modal%2520applications%2520in%250Aa%2520variety%2520of%2520real-world%2520scenarios.%2520Despite%2520their%2520impressive%2520performance%252C%2520VLMs%250Aare%2520prone%2520to%2520significant%2520hallucinations%252C%2520particularly%2520in%2520the%2520form%2520of%250Across-modal%2520inconsistencies.%2520Building%2520on%2520the%2520success%2520of%2520Reinforcement%2520Learning%250Afrom%2520Human%2520Feedback%2520%2528RLHF%2529%2520in%2520aligning%2520LLMs%252C%2520recent%2520advancements%2520have%2520focused%250Aon%2520applying%2520direct%2520preference%2520optimization%2520%2528DPO%2529%2520on%2520carefully%2520curated%2520datasets%250Ato%2520mitigate%2520these%2520issues.%2520Yet%252C%2520such%2520approaches%2520typically%2520introduce%2520preference%250Asignals%2520in%2520a%2520brute-force%2520manner%252C%2520neglecting%2520the%2520crucial%2520role%2520of%2520visual%250Ainformation%2520in%2520the%2520alignment%2520process.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Re-Align%252C%2520a%250Anovel%2520alignment%2520framework%2520that%2520leverages%2520image%2520retrieval%2520to%2520construct%2520a%250Adual-preference%2520dataset%252C%2520effectively%2520incorporating%2520both%2520textual%2520and%2520visual%250Apreference%2520signals.%2520We%2520further%2520introduce%2520rDPO%252C%2520an%2520extension%2520of%2520the%2520standard%250Adirect%2520preference%2520optimization%2520that%2520incorporates%2520an%2520additional%2520visual%250Apreference%2520objective%2520during%2520fine-tuning.%2520Our%2520experimental%2520results%2520demonstrate%250Athat%2520Re-Align%2520not%2520only%2520mitigates%2520hallucinations%2520more%2520effectively%2520than%2520previous%250Amethods%2520but%2520also%2520yields%2520significant%2520performance%2520gains%2520in%2520general%2520visual%250Aquestion-answering%2520%2528VQA%2529%2520tasks.%2520Moreover%252C%2520we%2520show%2520that%2520Re-Align%2520maintains%250Arobustness%2520and%2520scalability%2520across%2520a%2520wide%2520range%2520of%2520VLM%2520sizes%2520and%2520architectures.%250AThis%2520work%2520represents%2520a%2520significant%2520step%2520forward%2520in%2520aligning%2520multimodal%2520LLMs%252C%250Apaving%2520the%2520way%2520for%2520more%2520reliable%2520and%2520effective%2520cross-modal%2520applications.%2520We%250Arelease%2520all%2520the%2520code%2520in%2520https%253A//github.com/taco-group/Re-Align.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-Align%3A%20Aligning%20Vision%20Language%20Models%20via%20Retrieval-Augmented%20Direct%0A%20%20Preference%20Optimization&entry.906535625=Shuo%20Xing%20and%20Yuping%20Wang%20and%20Peiran%20Li%20and%20Ruizheng%20Bai%20and%20Yueqi%20Wang%20and%20Chengxuan%20Qian%20and%20Huaxiu%20Yao%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20The%20emergence%20of%20large%20Vision%20Language%20Models%20%28VLMs%29%20has%20broadened%20the%20scope%0Aand%20capabilities%20of%20single-modal%20Large%20Language%20Models%20%28LLMs%29%20by%20integrating%0Avisual%20modalities%2C%20thereby%20unlocking%20transformative%20cross-modal%20applications%20in%0Aa%20variety%20of%20real-world%20scenarios.%20Despite%20their%20impressive%20performance%2C%20VLMs%0Aare%20prone%20to%20significant%20hallucinations%2C%20particularly%20in%20the%20form%20of%0Across-modal%20inconsistencies.%20Building%20on%20the%20success%20of%20Reinforcement%20Learning%0Afrom%20Human%20Feedback%20%28RLHF%29%20in%20aligning%20LLMs%2C%20recent%20advancements%20have%20focused%0Aon%20applying%20direct%20preference%20optimization%20%28DPO%29%20on%20carefully%20curated%20datasets%0Ato%20mitigate%20these%20issues.%20Yet%2C%20such%20approaches%20typically%20introduce%20preference%0Asignals%20in%20a%20brute-force%20manner%2C%20neglecting%20the%20crucial%20role%20of%20visual%0Ainformation%20in%20the%20alignment%20process.%20In%20this%20paper%2C%20we%20introduce%20Re-Align%2C%20a%0Anovel%20alignment%20framework%20that%20leverages%20image%20retrieval%20to%20construct%20a%0Adual-preference%20dataset%2C%20effectively%20incorporating%20both%20textual%20and%20visual%0Apreference%20signals.%20We%20further%20introduce%20rDPO%2C%20an%20extension%20of%20the%20standard%0Adirect%20preference%20optimization%20that%20incorporates%20an%20additional%20visual%0Apreference%20objective%20during%20fine-tuning.%20Our%20experimental%20results%20demonstrate%0Athat%20Re-Align%20not%20only%20mitigates%20hallucinations%20more%20effectively%20than%20previous%0Amethods%20but%20also%20yields%20significant%20performance%20gains%20in%20general%20visual%0Aquestion-answering%20%28VQA%29%20tasks.%20Moreover%2C%20we%20show%20that%20Re-Align%20maintains%0Arobustness%20and%20scalability%20across%20a%20wide%20range%20of%20VLM%20sizes%20and%20architectures.%0AThis%20work%20represents%20a%20significant%20step%20forward%20in%20aligning%20multimodal%20LLMs%2C%0Apaving%20the%20way%20for%20more%20reliable%20and%20effective%20cross-modal%20applications.%20We%0Arelease%20all%20the%20code%20in%20https%3A//github.com/taco-group/Re-Align.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13146v1&entry.124074799=Read"},
{"title": "Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept\n  Extraction in Large Vision Models", "author": "Thomas Fel and Ekdeep Singh Lubana and Jacob S. Prince and Matthew Kowal and Victor Boutin and Isabel Papadimitriou and Binxu Wang and Martin Wattenberg and Demba Ba and Talia Konkle", "abstract": "  Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine\nlearning interpretability, enabling the unsupervised decomposition of model\nrepresentations into a dictionary of abstract, human-interpretable concepts.\nHowever, we reveal a fundamental limitation: existing SAEs exhibit severe\ninstability, as identical models trained on similar datasets can produce\nsharply different dictionaries, undermining their reliability as an\ninterpretability tool. To address this issue, we draw inspiration from the\nArchetypal Analysis framework introduced by Cutler & Breiman (1994) and present\nArchetypal SAEs (A-SAE), wherein dictionary atoms are constrained to the convex\nhull of data. This geometric anchoring significantly enhances the stability of\ninferred dictionaries, and their mildly relaxed variants RA-SAEs further match\nstate-of-the-art reconstruction abilities. To rigorously assess dictionary\nquality learned by SAEs, we introduce two new benchmarks that test (i)\nplausibility, if dictionaries recover \"true\" classification directions and (ii)\nidentifiability, if dictionaries disentangle synthetic concept mixtures. Across\nall evaluations, RA-SAEs consistently yield more structured representations\nwhile uncovering novel, semantically meaningful concepts in large-scale vision\nmodels.\n", "link": "http://arxiv.org/abs/2502.12892v1", "date": "2025-02-18", "relevancy": 2.8949, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.592}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Archetypal%20SAE%3A%20Adaptive%20and%20Stable%20Dictionary%20Learning%20for%20Concept%0A%20%20Extraction%20in%20Large%20Vision%20Models&body=Title%3A%20Archetypal%20SAE%3A%20Adaptive%20and%20Stable%20Dictionary%20Learning%20for%20Concept%0A%20%20Extraction%20in%20Large%20Vision%20Models%0AAuthor%3A%20Thomas%20Fel%20and%20Ekdeep%20Singh%20Lubana%20and%20Jacob%20S.%20Prince%20and%20Matthew%20Kowal%20and%20Victor%20Boutin%20and%20Isabel%20Papadimitriou%20and%20Binxu%20Wang%20and%20Martin%20Wattenberg%20and%20Demba%20Ba%20and%20Talia%20Konkle%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20powerful%20framework%20for%20machine%0Alearning%20interpretability%2C%20enabling%20the%20unsupervised%20decomposition%20of%20model%0Arepresentations%20into%20a%20dictionary%20of%20abstract%2C%20human-interpretable%20concepts.%0AHowever%2C%20we%20reveal%20a%20fundamental%20limitation%3A%20existing%20SAEs%20exhibit%20severe%0Ainstability%2C%20as%20identical%20models%20trained%20on%20similar%20datasets%20can%20produce%0Asharply%20different%20dictionaries%2C%20undermining%20their%20reliability%20as%20an%0Ainterpretability%20tool.%20To%20address%20this%20issue%2C%20we%20draw%20inspiration%20from%20the%0AArchetypal%20Analysis%20framework%20introduced%20by%20Cutler%20%26%20Breiman%20%281994%29%20and%20present%0AArchetypal%20SAEs%20%28A-SAE%29%2C%20wherein%20dictionary%20atoms%20are%20constrained%20to%20the%20convex%0Ahull%20of%20data.%20This%20geometric%20anchoring%20significantly%20enhances%20the%20stability%20of%0Ainferred%20dictionaries%2C%20and%20their%20mildly%20relaxed%20variants%20RA-SAEs%20further%20match%0Astate-of-the-art%20reconstruction%20abilities.%20To%20rigorously%20assess%20dictionary%0Aquality%20learned%20by%20SAEs%2C%20we%20introduce%20two%20new%20benchmarks%20that%20test%20%28i%29%0Aplausibility%2C%20if%20dictionaries%20recover%20%22true%22%20classification%20directions%20and%20%28ii%29%0Aidentifiability%2C%20if%20dictionaries%20disentangle%20synthetic%20concept%20mixtures.%20Across%0Aall%20evaluations%2C%20RA-SAEs%20consistently%20yield%20more%20structured%20representations%0Awhile%20uncovering%20novel%2C%20semantically%20meaningful%20concepts%20in%20large-scale%20vision%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchetypal%2520SAE%253A%2520Adaptive%2520and%2520Stable%2520Dictionary%2520Learning%2520for%2520Concept%250A%2520%2520Extraction%2520in%2520Large%2520Vision%2520Models%26entry.906535625%3DThomas%2520Fel%2520and%2520Ekdeep%2520Singh%2520Lubana%2520and%2520Jacob%2520S.%2520Prince%2520and%2520Matthew%2520Kowal%2520and%2520Victor%2520Boutin%2520and%2520Isabel%2520Papadimitriou%2520and%2520Binxu%2520Wang%2520and%2520Martin%2520Wattenberg%2520and%2520Demba%2520Ba%2520and%2520Talia%2520Konkle%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520framework%2520for%2520machine%250Alearning%2520interpretability%252C%2520enabling%2520the%2520unsupervised%2520decomposition%2520of%2520model%250Arepresentations%2520into%2520a%2520dictionary%2520of%2520abstract%252C%2520human-interpretable%2520concepts.%250AHowever%252C%2520we%2520reveal%2520a%2520fundamental%2520limitation%253A%2520existing%2520SAEs%2520exhibit%2520severe%250Ainstability%252C%2520as%2520identical%2520models%2520trained%2520on%2520similar%2520datasets%2520can%2520produce%250Asharply%2520different%2520dictionaries%252C%2520undermining%2520their%2520reliability%2520as%2520an%250Ainterpretability%2520tool.%2520To%2520address%2520this%2520issue%252C%2520we%2520draw%2520inspiration%2520from%2520the%250AArchetypal%2520Analysis%2520framework%2520introduced%2520by%2520Cutler%2520%2526%2520Breiman%2520%25281994%2529%2520and%2520present%250AArchetypal%2520SAEs%2520%2528A-SAE%2529%252C%2520wherein%2520dictionary%2520atoms%2520are%2520constrained%2520to%2520the%2520convex%250Ahull%2520of%2520data.%2520This%2520geometric%2520anchoring%2520significantly%2520enhances%2520the%2520stability%2520of%250Ainferred%2520dictionaries%252C%2520and%2520their%2520mildly%2520relaxed%2520variants%2520RA-SAEs%2520further%2520match%250Astate-of-the-art%2520reconstruction%2520abilities.%2520To%2520rigorously%2520assess%2520dictionary%250Aquality%2520learned%2520by%2520SAEs%252C%2520we%2520introduce%2520two%2520new%2520benchmarks%2520that%2520test%2520%2528i%2529%250Aplausibility%252C%2520if%2520dictionaries%2520recover%2520%2522true%2522%2520classification%2520directions%2520and%2520%2528ii%2529%250Aidentifiability%252C%2520if%2520dictionaries%2520disentangle%2520synthetic%2520concept%2520mixtures.%2520Across%250Aall%2520evaluations%252C%2520RA-SAEs%2520consistently%2520yield%2520more%2520structured%2520representations%250Awhile%2520uncovering%2520novel%252C%2520semantically%2520meaningful%2520concepts%2520in%2520large-scale%2520vision%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Archetypal%20SAE%3A%20Adaptive%20and%20Stable%20Dictionary%20Learning%20for%20Concept%0A%20%20Extraction%20in%20Large%20Vision%20Models&entry.906535625=Thomas%20Fel%20and%20Ekdeep%20Singh%20Lubana%20and%20Jacob%20S.%20Prince%20and%20Matthew%20Kowal%20and%20Victor%20Boutin%20and%20Isabel%20Papadimitriou%20and%20Binxu%20Wang%20and%20Martin%20Wattenberg%20and%20Demba%20Ba%20and%20Talia%20Konkle&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20powerful%20framework%20for%20machine%0Alearning%20interpretability%2C%20enabling%20the%20unsupervised%20decomposition%20of%20model%0Arepresentations%20into%20a%20dictionary%20of%20abstract%2C%20human-interpretable%20concepts.%0AHowever%2C%20we%20reveal%20a%20fundamental%20limitation%3A%20existing%20SAEs%20exhibit%20severe%0Ainstability%2C%20as%20identical%20models%20trained%20on%20similar%20datasets%20can%20produce%0Asharply%20different%20dictionaries%2C%20undermining%20their%20reliability%20as%20an%0Ainterpretability%20tool.%20To%20address%20this%20issue%2C%20we%20draw%20inspiration%20from%20the%0AArchetypal%20Analysis%20framework%20introduced%20by%20Cutler%20%26%20Breiman%20%281994%29%20and%20present%0AArchetypal%20SAEs%20%28A-SAE%29%2C%20wherein%20dictionary%20atoms%20are%20constrained%20to%20the%20convex%0Ahull%20of%20data.%20This%20geometric%20anchoring%20significantly%20enhances%20the%20stability%20of%0Ainferred%20dictionaries%2C%20and%20their%20mildly%20relaxed%20variants%20RA-SAEs%20further%20match%0Astate-of-the-art%20reconstruction%20abilities.%20To%20rigorously%20assess%20dictionary%0Aquality%20learned%20by%20SAEs%2C%20we%20introduce%20two%20new%20benchmarks%20that%20test%20%28i%29%0Aplausibility%2C%20if%20dictionaries%20recover%20%22true%22%20classification%20directions%20and%20%28ii%29%0Aidentifiability%2C%20if%20dictionaries%20disentangle%20synthetic%20concept%20mixtures.%20Across%0Aall%20evaluations%2C%20RA-SAEs%20consistently%20yield%20more%20structured%20representations%0Awhile%20uncovering%20novel%2C%20semantically%20meaningful%20concepts%20in%20large-scale%20vision%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12892v1&entry.124074799=Read"},
{"title": "Natural Language Generation from Visual Sequences: Challenges and Future\n  Directions", "author": "Aditya K Surikuchi and Raquel Fern\u00e1ndez and Sandro Pezzelle", "abstract": "  The ability to use natural language to talk about visual content is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. Various studies have focused on generating text for single images. In\ncontrast, comparatively little attention has been paid to exhaustively\nanalyzing and advancing work on multiple-image vision-to-text settings. In this\nposition paper, we claim that any task dealing with temporally ordered\nsequences of multiple images or frames is an instance of a broader, more\ngeneral problem involving the understanding of intricate relationships between\nthe visual content and the corresponding text. We comprehensively analyze five\ntasks that are instances of this problem and argue that they pose a common set\nof challenges and share similarities in terms of modeling and evaluation\napproaches. Based on the insights from these various aspects and stages of\nmulti-image-to-text generation, we highlight several open questions and suggest\nfuture research directions. We believe that these directions can advance the\nunderstanding of complex phenomena in this domain and the development of better\nmodels.\n", "link": "http://arxiv.org/abs/2502.13034v1", "date": "2025-02-18", "relevancy": 2.8861, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5829}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20Generation%20from%20Visual%20Sequences%3A%20Challenges%20and%20Future%0A%20%20Directions&body=Title%3A%20Natural%20Language%20Generation%20from%20Visual%20Sequences%3A%20Challenges%20and%20Future%0A%20%20Directions%0AAuthor%3A%20Aditya%20K%20Surikuchi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle%0AAbstract%3A%20%20%20The%20ability%20to%20use%20natural%20language%20to%20talk%20about%20visual%20content%20is%20at%20the%0Acore%20of%20human%20intelligence%20and%20a%20crucial%20feature%20of%20any%20artificial%20intelligence%0Asystem.%20Various%20studies%20have%20focused%20on%20generating%20text%20for%20single%20images.%20In%0Acontrast%2C%20comparatively%20little%20attention%20has%20been%20paid%20to%20exhaustively%0Aanalyzing%20and%20advancing%20work%20on%20multiple-image%20vision-to-text%20settings.%20In%20this%0Aposition%20paper%2C%20we%20claim%20that%20any%20task%20dealing%20with%20temporally%20ordered%0Asequences%20of%20multiple%20images%20or%20frames%20is%20an%20instance%20of%20a%20broader%2C%20more%0Ageneral%20problem%20involving%20the%20understanding%20of%20intricate%20relationships%20between%0Athe%20visual%20content%20and%20the%20corresponding%20text.%20We%20comprehensively%20analyze%20five%0Atasks%20that%20are%20instances%20of%20this%20problem%20and%20argue%20that%20they%20pose%20a%20common%20set%0Aof%20challenges%20and%20share%20similarities%20in%20terms%20of%20modeling%20and%20evaluation%0Aapproaches.%20Based%20on%20the%20insights%20from%20these%20various%20aspects%20and%20stages%20of%0Amulti-image-to-text%20generation%2C%20we%20highlight%20several%20open%20questions%20and%20suggest%0Afuture%20research%20directions.%20We%20believe%20that%20these%20directions%20can%20advance%20the%0Aunderstanding%20of%20complex%20phenomena%20in%20this%20domain%20and%20the%20development%20of%20better%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520Generation%2520from%2520Visual%2520Sequences%253A%2520Challenges%2520and%2520Future%250A%2520%2520Directions%26entry.906535625%3DAditya%2520K%2520Surikuchi%2520and%2520Raquel%2520Fern%25C3%25A1ndez%2520and%2520Sandro%2520Pezzelle%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520use%2520natural%2520language%2520to%2520talk%2520about%2520visual%2520content%2520is%2520at%2520the%250Acore%2520of%2520human%2520intelligence%2520and%2520a%2520crucial%2520feature%2520of%2520any%2520artificial%2520intelligence%250Asystem.%2520Various%2520studies%2520have%2520focused%2520on%2520generating%2520text%2520for%2520single%2520images.%2520In%250Acontrast%252C%2520comparatively%2520little%2520attention%2520has%2520been%2520paid%2520to%2520exhaustively%250Aanalyzing%2520and%2520advancing%2520work%2520on%2520multiple-image%2520vision-to-text%2520settings.%2520In%2520this%250Aposition%2520paper%252C%2520we%2520claim%2520that%2520any%2520task%2520dealing%2520with%2520temporally%2520ordered%250Asequences%2520of%2520multiple%2520images%2520or%2520frames%2520is%2520an%2520instance%2520of%2520a%2520broader%252C%2520more%250Ageneral%2520problem%2520involving%2520the%2520understanding%2520of%2520intricate%2520relationships%2520between%250Athe%2520visual%2520content%2520and%2520the%2520corresponding%2520text.%2520We%2520comprehensively%2520analyze%2520five%250Atasks%2520that%2520are%2520instances%2520of%2520this%2520problem%2520and%2520argue%2520that%2520they%2520pose%2520a%2520common%2520set%250Aof%2520challenges%2520and%2520share%2520similarities%2520in%2520terms%2520of%2520modeling%2520and%2520evaluation%250Aapproaches.%2520Based%2520on%2520the%2520insights%2520from%2520these%2520various%2520aspects%2520and%2520stages%2520of%250Amulti-image-to-text%2520generation%252C%2520we%2520highlight%2520several%2520open%2520questions%2520and%2520suggest%250Afuture%2520research%2520directions.%2520We%2520believe%2520that%2520these%2520directions%2520can%2520advance%2520the%250Aunderstanding%2520of%2520complex%2520phenomena%2520in%2520this%2520domain%2520and%2520the%2520development%2520of%2520better%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20Generation%20from%20Visual%20Sequences%3A%20Challenges%20and%20Future%0A%20%20Directions&entry.906535625=Aditya%20K%20Surikuchi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle&entry.1292438233=%20%20The%20ability%20to%20use%20natural%20language%20to%20talk%20about%20visual%20content%20is%20at%20the%0Acore%20of%20human%20intelligence%20and%20a%20crucial%20feature%20of%20any%20artificial%20intelligence%0Asystem.%20Various%20studies%20have%20focused%20on%20generating%20text%20for%20single%20images.%20In%0Acontrast%2C%20comparatively%20little%20attention%20has%20been%20paid%20to%20exhaustively%0Aanalyzing%20and%20advancing%20work%20on%20multiple-image%20vision-to-text%20settings.%20In%20this%0Aposition%20paper%2C%20we%20claim%20that%20any%20task%20dealing%20with%20temporally%20ordered%0Asequences%20of%20multiple%20images%20or%20frames%20is%20an%20instance%20of%20a%20broader%2C%20more%0Ageneral%20problem%20involving%20the%20understanding%20of%20intricate%20relationships%20between%0Athe%20visual%20content%20and%20the%20corresponding%20text.%20We%20comprehensively%20analyze%20five%0Atasks%20that%20are%20instances%20of%20this%20problem%20and%20argue%20that%20they%20pose%20a%20common%20set%0Aof%20challenges%20and%20share%20similarities%20in%20terms%20of%20modeling%20and%20evaluation%0Aapproaches.%20Based%20on%20the%20insights%20from%20these%20various%20aspects%20and%20stages%20of%0Amulti-image-to-text%20generation%2C%20we%20highlight%20several%20open%20questions%20and%20suggest%0Afuture%20research%20directions.%20We%20believe%20that%20these%20directions%20can%20advance%20the%0Aunderstanding%20of%20complex%20phenomena%20in%20this%20domain%20and%20the%20development%20of%20better%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13034v1&entry.124074799=Read"},
{"title": "Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?", "author": "Antonia W\u00fcst and Tim Tobiasch and Lukas Helff and Inga Ibs and Wolfgang Stammer and Devendra S. Dhami and Constantin A. Rothkopf and Kristian Kersting", "abstract": "  Recently, newly developed Vision-Language Models (VLMs), such as OpenAI's o1,\nhave emerged, seemingly demonstrating advanced reasoning capabilities across\ntext and image modalities. However, the depth of these advances in\nlanguage-guided perception and abstract reasoning remains underexplored, and it\nis unclear whether these models can truly live up to their ambitious promises.\nTo assess the progress and identify shortcomings, we enter the wonderland of\nBongard problems, a set of classic visual reasoning puzzles that require\nhuman-like abilities of pattern recognition and abstract reasoning. With our\nextensive evaluation setup, we show that while VLMs occasionally succeed in\nidentifying discriminative concepts and solving some of the problems, they\nfrequently falter. Surprisingly, even elementary concepts that may seem trivial\nto humans, such as simple spirals, pose significant challenges. Moreover, when\nexplicitly asked to recognize ground truth concepts, they continue to falter,\nsuggesting not only a lack of understanding of these elementary visual concepts\nbut also an inability to generalize to unseen concepts. We compare the results\nof VLMs to human performance and observe that a significant gap remains between\nhuman visual reasoning capabilities and machine cognition.\n", "link": "http://arxiv.org/abs/2410.19546v2", "date": "2025-02-18", "relevancy": 2.8751, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bongard%20in%20Wonderland%3A%20Visual%20Puzzles%20that%20Still%20Make%20AI%20Go%20Mad%3F&body=Title%3A%20Bongard%20in%20Wonderland%3A%20Visual%20Puzzles%20that%20Still%20Make%20AI%20Go%20Mad%3F%0AAuthor%3A%20Antonia%20W%C3%BCst%20and%20Tim%20Tobiasch%20and%20Lukas%20Helff%20and%20Inga%20Ibs%20and%20Wolfgang%20Stammer%20and%20Devendra%20S.%20Dhami%20and%20Constantin%20A.%20Rothkopf%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Recently%2C%20newly%20developed%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20OpenAI%27s%20o1%2C%0Ahave%20emerged%2C%20seemingly%20demonstrating%20advanced%20reasoning%20capabilities%20across%0Atext%20and%20image%20modalities.%20However%2C%20the%20depth%20of%20these%20advances%20in%0Alanguage-guided%20perception%20and%20abstract%20reasoning%20remains%20underexplored%2C%20and%20it%0Ais%20unclear%20whether%20these%20models%20can%20truly%20live%20up%20to%20their%20ambitious%20promises.%0ATo%20assess%20the%20progress%20and%20identify%20shortcomings%2C%20we%20enter%20the%20wonderland%20of%0ABongard%20problems%2C%20a%20set%20of%20classic%20visual%20reasoning%20puzzles%20that%20require%0Ahuman-like%20abilities%20of%20pattern%20recognition%20and%20abstract%20reasoning.%20With%20our%0Aextensive%20evaluation%20setup%2C%20we%20show%20that%20while%20VLMs%20occasionally%20succeed%20in%0Aidentifying%20discriminative%20concepts%20and%20solving%20some%20of%20the%20problems%2C%20they%0Afrequently%20falter.%20Surprisingly%2C%20even%20elementary%20concepts%20that%20may%20seem%20trivial%0Ato%20humans%2C%20such%20as%20simple%20spirals%2C%20pose%20significant%20challenges.%20Moreover%2C%20when%0Aexplicitly%20asked%20to%20recognize%20ground%20truth%20concepts%2C%20they%20continue%20to%20falter%2C%0Asuggesting%20not%20only%20a%20lack%20of%20understanding%20of%20these%20elementary%20visual%20concepts%0Abut%20also%20an%20inability%20to%20generalize%20to%20unseen%20concepts.%20We%20compare%20the%20results%0Aof%20VLMs%20to%20human%20performance%20and%20observe%20that%20a%20significant%20gap%20remains%20between%0Ahuman%20visual%20reasoning%20capabilities%20and%20machine%20cognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBongard%2520in%2520Wonderland%253A%2520Visual%2520Puzzles%2520that%2520Still%2520Make%2520AI%2520Go%2520Mad%253F%26entry.906535625%3DAntonia%2520W%25C3%25BCst%2520and%2520Tim%2520Tobiasch%2520and%2520Lukas%2520Helff%2520and%2520Inga%2520Ibs%2520and%2520Wolfgang%2520Stammer%2520and%2520Devendra%2520S.%2520Dhami%2520and%2520Constantin%2520A.%2520Rothkopf%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Recently%252C%2520newly%2520developed%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520such%2520as%2520OpenAI%2527s%2520o1%252C%250Ahave%2520emerged%252C%2520seemingly%2520demonstrating%2520advanced%2520reasoning%2520capabilities%2520across%250Atext%2520and%2520image%2520modalities.%2520However%252C%2520the%2520depth%2520of%2520these%2520advances%2520in%250Alanguage-guided%2520perception%2520and%2520abstract%2520reasoning%2520remains%2520underexplored%252C%2520and%2520it%250Ais%2520unclear%2520whether%2520these%2520models%2520can%2520truly%2520live%2520up%2520to%2520their%2520ambitious%2520promises.%250ATo%2520assess%2520the%2520progress%2520and%2520identify%2520shortcomings%252C%2520we%2520enter%2520the%2520wonderland%2520of%250ABongard%2520problems%252C%2520a%2520set%2520of%2520classic%2520visual%2520reasoning%2520puzzles%2520that%2520require%250Ahuman-like%2520abilities%2520of%2520pattern%2520recognition%2520and%2520abstract%2520reasoning.%2520With%2520our%250Aextensive%2520evaluation%2520setup%252C%2520we%2520show%2520that%2520while%2520VLMs%2520occasionally%2520succeed%2520in%250Aidentifying%2520discriminative%2520concepts%2520and%2520solving%2520some%2520of%2520the%2520problems%252C%2520they%250Afrequently%2520falter.%2520Surprisingly%252C%2520even%2520elementary%2520concepts%2520that%2520may%2520seem%2520trivial%250Ato%2520humans%252C%2520such%2520as%2520simple%2520spirals%252C%2520pose%2520significant%2520challenges.%2520Moreover%252C%2520when%250Aexplicitly%2520asked%2520to%2520recognize%2520ground%2520truth%2520concepts%252C%2520they%2520continue%2520to%2520falter%252C%250Asuggesting%2520not%2520only%2520a%2520lack%2520of%2520understanding%2520of%2520these%2520elementary%2520visual%2520concepts%250Abut%2520also%2520an%2520inability%2520to%2520generalize%2520to%2520unseen%2520concepts.%2520We%2520compare%2520the%2520results%250Aof%2520VLMs%2520to%2520human%2520performance%2520and%2520observe%2520that%2520a%2520significant%2520gap%2520remains%2520between%250Ahuman%2520visual%2520reasoning%2520capabilities%2520and%2520machine%2520cognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bongard%20in%20Wonderland%3A%20Visual%20Puzzles%20that%20Still%20Make%20AI%20Go%20Mad%3F&entry.906535625=Antonia%20W%C3%BCst%20and%20Tim%20Tobiasch%20and%20Lukas%20Helff%20and%20Inga%20Ibs%20and%20Wolfgang%20Stammer%20and%20Devendra%20S.%20Dhami%20and%20Constantin%20A.%20Rothkopf%20and%20Kristian%20Kersting&entry.1292438233=%20%20Recently%2C%20newly%20developed%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20OpenAI%27s%20o1%2C%0Ahave%20emerged%2C%20seemingly%20demonstrating%20advanced%20reasoning%20capabilities%20across%0Atext%20and%20image%20modalities.%20However%2C%20the%20depth%20of%20these%20advances%20in%0Alanguage-guided%20perception%20and%20abstract%20reasoning%20remains%20underexplored%2C%20and%20it%0Ais%20unclear%20whether%20these%20models%20can%20truly%20live%20up%20to%20their%20ambitious%20promises.%0ATo%20assess%20the%20progress%20and%20identify%20shortcomings%2C%20we%20enter%20the%20wonderland%20of%0ABongard%20problems%2C%20a%20set%20of%20classic%20visual%20reasoning%20puzzles%20that%20require%0Ahuman-like%20abilities%20of%20pattern%20recognition%20and%20abstract%20reasoning.%20With%20our%0Aextensive%20evaluation%20setup%2C%20we%20show%20that%20while%20VLMs%20occasionally%20succeed%20in%0Aidentifying%20discriminative%20concepts%20and%20solving%20some%20of%20the%20problems%2C%20they%0Afrequently%20falter.%20Surprisingly%2C%20even%20elementary%20concepts%20that%20may%20seem%20trivial%0Ato%20humans%2C%20such%20as%20simple%20spirals%2C%20pose%20significant%20challenges.%20Moreover%2C%20when%0Aexplicitly%20asked%20to%20recognize%20ground%20truth%20concepts%2C%20they%20continue%20to%20falter%2C%0Asuggesting%20not%20only%20a%20lack%20of%20understanding%20of%20these%20elementary%20visual%20concepts%0Abut%20also%20an%20inability%20to%20generalize%20to%20unseen%20concepts.%20We%20compare%20the%20results%0Aof%20VLMs%20to%20human%20performance%20and%20observe%20that%20a%20significant%20gap%20remains%20between%0Ahuman%20visual%20reasoning%20capabilities%20and%20machine%20cognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19546v2&entry.124074799=Read"},
{"title": "PartSDF: Part-Based Implicit Neural Representation for Composite 3D\n  Shape Parametrization and Optimization", "author": "Nicolas Talabot and Olivier Clerc and Arda Cinar Demirtas and Doruk Oner and Pascal Fua", "abstract": "  Accurate 3D shape representation is essential in engineering applications\nsuch as design, optimization, and simulation. In practice, engineering\nworkflows require structured, part-aware representations, as objects are\ninherently designed as assemblies of distinct components. However, most\nexisting methods either model shapes holistically or decompose them without\npredefined part structures, limiting their applicability in real-world design\ntasks. We propose PartSDF, a supervised implicit representation framework that\nexplicitly models composite shapes with independent, controllable parts while\nmaintaining shape consistency. Despite its simple single-decoder architecture,\nPartSDF outperforms both supervised and unsupervised baselines in\nreconstruction and generation tasks. We further demonstrate its effectiveness\nas a structured shape prior for engineering applications, enabling precise\ncontrol over individual components while preserving overall coherence. Code\navailable at https://github.com/cvlab-epfl/PartSDF.\n", "link": "http://arxiv.org/abs/2502.12985v1", "date": "2025-02-18", "relevancy": 2.8194, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5809}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5603}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartSDF%3A%20Part-Based%20Implicit%20Neural%20Representation%20for%20Composite%203D%0A%20%20Shape%20Parametrization%20and%20Optimization&body=Title%3A%20PartSDF%3A%20Part-Based%20Implicit%20Neural%20Representation%20for%20Composite%203D%0A%20%20Shape%20Parametrization%20and%20Optimization%0AAuthor%3A%20Nicolas%20Talabot%20and%20Olivier%20Clerc%20and%20Arda%20Cinar%20Demirtas%20and%20Doruk%20Oner%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Accurate%203D%20shape%20representation%20is%20essential%20in%20engineering%20applications%0Asuch%20as%20design%2C%20optimization%2C%20and%20simulation.%20In%20practice%2C%20engineering%0Aworkflows%20require%20structured%2C%20part-aware%20representations%2C%20as%20objects%20are%0Ainherently%20designed%20as%20assemblies%20of%20distinct%20components.%20However%2C%20most%0Aexisting%20methods%20either%20model%20shapes%20holistically%20or%20decompose%20them%20without%0Apredefined%20part%20structures%2C%20limiting%20their%20applicability%20in%20real-world%20design%0Atasks.%20We%20propose%20PartSDF%2C%20a%20supervised%20implicit%20representation%20framework%20that%0Aexplicitly%20models%20composite%20shapes%20with%20independent%2C%20controllable%20parts%20while%0Amaintaining%20shape%20consistency.%20Despite%20its%20simple%20single-decoder%20architecture%2C%0APartSDF%20outperforms%20both%20supervised%20and%20unsupervised%20baselines%20in%0Areconstruction%20and%20generation%20tasks.%20We%20further%20demonstrate%20its%20effectiveness%0Aas%20a%20structured%20shape%20prior%20for%20engineering%20applications%2C%20enabling%20precise%0Acontrol%20over%20individual%20components%20while%20preserving%20overall%20coherence.%20Code%0Aavailable%20at%20https%3A//github.com/cvlab-epfl/PartSDF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartSDF%253A%2520Part-Based%2520Implicit%2520Neural%2520Representation%2520for%2520Composite%25203D%250A%2520%2520Shape%2520Parametrization%2520and%2520Optimization%26entry.906535625%3DNicolas%2520Talabot%2520and%2520Olivier%2520Clerc%2520and%2520Arda%2520Cinar%2520Demirtas%2520and%2520Doruk%2520Oner%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520Accurate%25203D%2520shape%2520representation%2520is%2520essential%2520in%2520engineering%2520applications%250Asuch%2520as%2520design%252C%2520optimization%252C%2520and%2520simulation.%2520In%2520practice%252C%2520engineering%250Aworkflows%2520require%2520structured%252C%2520part-aware%2520representations%252C%2520as%2520objects%2520are%250Ainherently%2520designed%2520as%2520assemblies%2520of%2520distinct%2520components.%2520However%252C%2520most%250Aexisting%2520methods%2520either%2520model%2520shapes%2520holistically%2520or%2520decompose%2520them%2520without%250Apredefined%2520part%2520structures%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%2520design%250Atasks.%2520We%2520propose%2520PartSDF%252C%2520a%2520supervised%2520implicit%2520representation%2520framework%2520that%250Aexplicitly%2520models%2520composite%2520shapes%2520with%2520independent%252C%2520controllable%2520parts%2520while%250Amaintaining%2520shape%2520consistency.%2520Despite%2520its%2520simple%2520single-decoder%2520architecture%252C%250APartSDF%2520outperforms%2520both%2520supervised%2520and%2520unsupervised%2520baselines%2520in%250Areconstruction%2520and%2520generation%2520tasks.%2520We%2520further%2520demonstrate%2520its%2520effectiveness%250Aas%2520a%2520structured%2520shape%2520prior%2520for%2520engineering%2520applications%252C%2520enabling%2520precise%250Acontrol%2520over%2520individual%2520components%2520while%2520preserving%2520overall%2520coherence.%2520Code%250Aavailable%2520at%2520https%253A//github.com/cvlab-epfl/PartSDF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartSDF%3A%20Part-Based%20Implicit%20Neural%20Representation%20for%20Composite%203D%0A%20%20Shape%20Parametrization%20and%20Optimization&entry.906535625=Nicolas%20Talabot%20and%20Olivier%20Clerc%20and%20Arda%20Cinar%20Demirtas%20and%20Doruk%20Oner%20and%20Pascal%20Fua&entry.1292438233=%20%20Accurate%203D%20shape%20representation%20is%20essential%20in%20engineering%20applications%0Asuch%20as%20design%2C%20optimization%2C%20and%20simulation.%20In%20practice%2C%20engineering%0Aworkflows%20require%20structured%2C%20part-aware%20representations%2C%20as%20objects%20are%0Ainherently%20designed%20as%20assemblies%20of%20distinct%20components.%20However%2C%20most%0Aexisting%20methods%20either%20model%20shapes%20holistically%20or%20decompose%20them%20without%0Apredefined%20part%20structures%2C%20limiting%20their%20applicability%20in%20real-world%20design%0Atasks.%20We%20propose%20PartSDF%2C%20a%20supervised%20implicit%20representation%20framework%20that%0Aexplicitly%20models%20composite%20shapes%20with%20independent%2C%20controllable%20parts%20while%0Amaintaining%20shape%20consistency.%20Despite%20its%20simple%20single-decoder%20architecture%2C%0APartSDF%20outperforms%20both%20supervised%20and%20unsupervised%20baselines%20in%0Areconstruction%20and%20generation%20tasks.%20We%20further%20demonstrate%20its%20effectiveness%0Aas%20a%20structured%20shape%20prior%20for%20engineering%20applications%2C%20enabling%20precise%0Acontrol%20over%20individual%20components%20while%20preserving%20overall%20coherence.%20Code%0Aavailable%20at%20https%3A//github.com/cvlab-epfl/PartSDF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12985v1&entry.124074799=Read"},
{"title": "RedundancyLens: Revealing and Exploiting Visual Token Processing\n  Redundancy for Efficient Decoder-Only MLLMs", "author": "Hongliang Li and Jiaxin Zhang and Wenhui Liao and Dezhi Peng and Kai Ding and Lianwen Jin", "abstract": "  Current Multimodal Large Language Model (MLLM) architectures face a critical\ntradeoff between performance and efficiency: decoder-only architectures achieve\nhigher performance but lower efficiency, while cross-attention-based\narchitectures offer greater efficiency but lower performance. The key\ndistinction lies in how visual tokens are processed. Decoder-only architectures\napply self-attention and FFN operations on visual tokens, while cross-attention\narchitectures skip these computations. To investigate whether redundancy exists\nin this computationally expensive process, we propose a training-free framework\nfor analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and\nHollow Attention, which enable adjustable reductions in computations for visual\ntokens, as well as a Layer Ranking Algorithm that prioritizes layers for these\nreductions. Extensive experiments demonstrate substantial, structured, and\nclustered redundancy unique to decoder-only MLLMs, offering valuable insights\nfor future MLLM architecture design. Furthermore, by leveraging our reduction\nframework as a training-free inference acceleration approach, we achieve\nperformance comparable to or better than state-of-the-art methods while\nremaining compatible with them. Code will be publicly available at\nhttps://github.com/L-Hugh/RedundancyLens.\n", "link": "http://arxiv.org/abs/2501.19036v2", "date": "2025-02-18", "relevancy": 2.8129, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RedundancyLens%3A%20Revealing%20and%20Exploiting%20Visual%20Token%20Processing%0A%20%20Redundancy%20for%20Efficient%20Decoder-Only%20MLLMs&body=Title%3A%20RedundancyLens%3A%20Revealing%20and%20Exploiting%20Visual%20Token%20Processing%0A%20%20Redundancy%20for%20Efficient%20Decoder-Only%20MLLMs%0AAuthor%3A%20Hongliang%20Li%20and%20Jiaxin%20Zhang%20and%20Wenhui%20Liao%20and%20Dezhi%20Peng%20and%20Kai%20Ding%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Current%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20architectures%20face%20a%20critical%0Atradeoff%20between%20performance%20and%20efficiency%3A%20decoder-only%20architectures%20achieve%0Ahigher%20performance%20but%20lower%20efficiency%2C%20while%20cross-attention-based%0Aarchitectures%20offer%20greater%20efficiency%20but%20lower%20performance.%20The%20key%0Adistinction%20lies%20in%20how%20visual%20tokens%20are%20processed.%20Decoder-only%20architectures%0Aapply%20self-attention%20and%20FFN%20operations%20on%20visual%20tokens%2C%20while%20cross-attention%0Aarchitectures%20skip%20these%20computations.%20To%20investigate%20whether%20redundancy%20exists%0Ain%20this%20computationally%20expensive%20process%2C%20we%20propose%20a%20training-free%20framework%0Afor%20analyzing%20trained%20MLLMs.%20It%20consists%20of%20Probe-Activated%20Dynamic%20FFN%20and%0AHollow%20Attention%2C%20which%20enable%20adjustable%20reductions%20in%20computations%20for%20visual%0Atokens%2C%20as%20well%20as%20a%20Layer%20Ranking%20Algorithm%20that%20prioritizes%20layers%20for%20these%0Areductions.%20Extensive%20experiments%20demonstrate%20substantial%2C%20structured%2C%20and%0Aclustered%20redundancy%20unique%20to%20decoder-only%20MLLMs%2C%20offering%20valuable%20insights%0Afor%20future%20MLLM%20architecture%20design.%20Furthermore%2C%20by%20leveraging%20our%20reduction%0Aframework%20as%20a%20training-free%20inference%20acceleration%20approach%2C%20we%20achieve%0Aperformance%20comparable%20to%20or%20better%20than%20state-of-the-art%20methods%20while%0Aremaining%20compatible%20with%20them.%20Code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/L-Hugh/RedundancyLens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19036v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedundancyLens%253A%2520Revealing%2520and%2520Exploiting%2520Visual%2520Token%2520Processing%250A%2520%2520Redundancy%2520for%2520Efficient%2520Decoder-Only%2520MLLMs%26entry.906535625%3DHongliang%2520Li%2520and%2520Jiaxin%2520Zhang%2520and%2520Wenhui%2520Liao%2520and%2520Dezhi%2520Peng%2520and%2520Kai%2520Ding%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Current%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520architectures%2520face%2520a%2520critical%250Atradeoff%2520between%2520performance%2520and%2520efficiency%253A%2520decoder-only%2520architectures%2520achieve%250Ahigher%2520performance%2520but%2520lower%2520efficiency%252C%2520while%2520cross-attention-based%250Aarchitectures%2520offer%2520greater%2520efficiency%2520but%2520lower%2520performance.%2520The%2520key%250Adistinction%2520lies%2520in%2520how%2520visual%2520tokens%2520are%2520processed.%2520Decoder-only%2520architectures%250Aapply%2520self-attention%2520and%2520FFN%2520operations%2520on%2520visual%2520tokens%252C%2520while%2520cross-attention%250Aarchitectures%2520skip%2520these%2520computations.%2520To%2520investigate%2520whether%2520redundancy%2520exists%250Ain%2520this%2520computationally%2520expensive%2520process%252C%2520we%2520propose%2520a%2520training-free%2520framework%250Afor%2520analyzing%2520trained%2520MLLMs.%2520It%2520consists%2520of%2520Probe-Activated%2520Dynamic%2520FFN%2520and%250AHollow%2520Attention%252C%2520which%2520enable%2520adjustable%2520reductions%2520in%2520computations%2520for%2520visual%250Atokens%252C%2520as%2520well%2520as%2520a%2520Layer%2520Ranking%2520Algorithm%2520that%2520prioritizes%2520layers%2520for%2520these%250Areductions.%2520Extensive%2520experiments%2520demonstrate%2520substantial%252C%2520structured%252C%2520and%250Aclustered%2520redundancy%2520unique%2520to%2520decoder-only%2520MLLMs%252C%2520offering%2520valuable%2520insights%250Afor%2520future%2520MLLM%2520architecture%2520design.%2520Furthermore%252C%2520by%2520leveraging%2520our%2520reduction%250Aframework%2520as%2520a%2520training-free%2520inference%2520acceleration%2520approach%252C%2520we%2520achieve%250Aperformance%2520comparable%2520to%2520or%2520better%2520than%2520state-of-the-art%2520methods%2520while%250Aremaining%2520compatible%2520with%2520them.%2520Code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/L-Hugh/RedundancyLens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19036v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RedundancyLens%3A%20Revealing%20and%20Exploiting%20Visual%20Token%20Processing%0A%20%20Redundancy%20for%20Efficient%20Decoder-Only%20MLLMs&entry.906535625=Hongliang%20Li%20and%20Jiaxin%20Zhang%20and%20Wenhui%20Liao%20and%20Dezhi%20Peng%20and%20Kai%20Ding%20and%20Lianwen%20Jin&entry.1292438233=%20%20Current%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20architectures%20face%20a%20critical%0Atradeoff%20between%20performance%20and%20efficiency%3A%20decoder-only%20architectures%20achieve%0Ahigher%20performance%20but%20lower%20efficiency%2C%20while%20cross-attention-based%0Aarchitectures%20offer%20greater%20efficiency%20but%20lower%20performance.%20The%20key%0Adistinction%20lies%20in%20how%20visual%20tokens%20are%20processed.%20Decoder-only%20architectures%0Aapply%20self-attention%20and%20FFN%20operations%20on%20visual%20tokens%2C%20while%20cross-attention%0Aarchitectures%20skip%20these%20computations.%20To%20investigate%20whether%20redundancy%20exists%0Ain%20this%20computationally%20expensive%20process%2C%20we%20propose%20a%20training-free%20framework%0Afor%20analyzing%20trained%20MLLMs.%20It%20consists%20of%20Probe-Activated%20Dynamic%20FFN%20and%0AHollow%20Attention%2C%20which%20enable%20adjustable%20reductions%20in%20computations%20for%20visual%0Atokens%2C%20as%20well%20as%20a%20Layer%20Ranking%20Algorithm%20that%20prioritizes%20layers%20for%20these%0Areductions.%20Extensive%20experiments%20demonstrate%20substantial%2C%20structured%2C%20and%0Aclustered%20redundancy%20unique%20to%20decoder-only%20MLLMs%2C%20offering%20valuable%20insights%0Afor%20future%20MLLM%20architecture%20design.%20Furthermore%2C%20by%20leveraging%20our%20reduction%0Aframework%20as%20a%20training-free%20inference%20acceleration%20approach%2C%20we%20achieve%0Aperformance%20comparable%20to%20or%20better%20than%20state-of-the-art%20methods%20while%0Aremaining%20compatible%20with%20them.%20Code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/L-Hugh/RedundancyLens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19036v2&entry.124074799=Read"},
{"title": "Contrast-Unity for Partially-Supervised Temporal Sentence Grounding", "author": "Haicheng Wang and Chen Ju and Weixiong Lin and Chaofan Ma and Shuai Xiao and Ya Zhang and Yanfeng Wang", "abstract": "  Temporal sentence grounding aims to detect event timestamps described by the\nnatural language query from given untrimmed videos. The existing\nfully-supervised setting achieves great results but requires expensive\nannotation costs; while the weakly-supervised setting adopts cheap labels but\nperforms poorly. To pursue high performance with less annotation costs, this\npaper introduces an intermediate partially-supervised setting, i.e., only\nshort-clip is available during training. To make full use of partial labels, we\nspecially design one contrast-unity framework, with the two-stage goal of\nimplicit-explicit progressive grounding. In the implicit stage, we align\nevent-query representations at fine granularity using comprehensive quadruple\ncontrastive learning: event-query gather, event-background separation,\nintra-cluster compactness and inter-cluster separability. Then, high-quality\nrepresentations bring acceptable grounding pseudo-labels. In the explicit\nstage, to explicitly optimize grounding objectives, we train one\nfully-supervised model using obtained pseudo-labels for grounding refinement\nand denoising. Extensive experiments and thoroughly ablations on Charades-STA\nand ActivityNet Captions demonstrate the significance of partial supervision,\nas well as our superior performance.\n", "link": "http://arxiv.org/abs/2502.12917v1", "date": "2025-02-18", "relevancy": 2.7853, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrast-Unity%20for%20Partially-Supervised%20Temporal%20Sentence%20Grounding&body=Title%3A%20Contrast-Unity%20for%20Partially-Supervised%20Temporal%20Sentence%20Grounding%0AAuthor%3A%20Haicheng%20Wang%20and%20Chen%20Ju%20and%20Weixiong%20Lin%20and%20Chaofan%20Ma%20and%20Shuai%20Xiao%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Temporal%20sentence%20grounding%20aims%20to%20detect%20event%20timestamps%20described%20by%20the%0Anatural%20language%20query%20from%20given%20untrimmed%20videos.%20The%20existing%0Afully-supervised%20setting%20achieves%20great%20results%20but%20requires%20expensive%0Aannotation%20costs%3B%20while%20the%20weakly-supervised%20setting%20adopts%20cheap%20labels%20but%0Aperforms%20poorly.%20To%20pursue%20high%20performance%20with%20less%20annotation%20costs%2C%20this%0Apaper%20introduces%20an%20intermediate%20partially-supervised%20setting%2C%20i.e.%2C%20only%0Ashort-clip%20is%20available%20during%20training.%20To%20make%20full%20use%20of%20partial%20labels%2C%20we%0Aspecially%20design%20one%20contrast-unity%20framework%2C%20with%20the%20two-stage%20goal%20of%0Aimplicit-explicit%20progressive%20grounding.%20In%20the%20implicit%20stage%2C%20we%20align%0Aevent-query%20representations%20at%20fine%20granularity%20using%20comprehensive%20quadruple%0Acontrastive%20learning%3A%20event-query%20gather%2C%20event-background%20separation%2C%0Aintra-cluster%20compactness%20and%20inter-cluster%20separability.%20Then%2C%20high-quality%0Arepresentations%20bring%20acceptable%20grounding%20pseudo-labels.%20In%20the%20explicit%0Astage%2C%20to%20explicitly%20optimize%20grounding%20objectives%2C%20we%20train%20one%0Afully-supervised%20model%20using%20obtained%20pseudo-labels%20for%20grounding%20refinement%0Aand%20denoising.%20Extensive%20experiments%20and%20thoroughly%20ablations%20on%20Charades-STA%0Aand%20ActivityNet%20Captions%20demonstrate%20the%20significance%20of%20partial%20supervision%2C%0Aas%20well%20as%20our%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrast-Unity%2520for%2520Partially-Supervised%2520Temporal%2520Sentence%2520Grounding%26entry.906535625%3DHaicheng%2520Wang%2520and%2520Chen%2520Ju%2520and%2520Weixiong%2520Lin%2520and%2520Chaofan%2520Ma%2520and%2520Shuai%2520Xiao%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Temporal%2520sentence%2520grounding%2520aims%2520to%2520detect%2520event%2520timestamps%2520described%2520by%2520the%250Anatural%2520language%2520query%2520from%2520given%2520untrimmed%2520videos.%2520The%2520existing%250Afully-supervised%2520setting%2520achieves%2520great%2520results%2520but%2520requires%2520expensive%250Aannotation%2520costs%253B%2520while%2520the%2520weakly-supervised%2520setting%2520adopts%2520cheap%2520labels%2520but%250Aperforms%2520poorly.%2520To%2520pursue%2520high%2520performance%2520with%2520less%2520annotation%2520costs%252C%2520this%250Apaper%2520introduces%2520an%2520intermediate%2520partially-supervised%2520setting%252C%2520i.e.%252C%2520only%250Ashort-clip%2520is%2520available%2520during%2520training.%2520To%2520make%2520full%2520use%2520of%2520partial%2520labels%252C%2520we%250Aspecially%2520design%2520one%2520contrast-unity%2520framework%252C%2520with%2520the%2520two-stage%2520goal%2520of%250Aimplicit-explicit%2520progressive%2520grounding.%2520In%2520the%2520implicit%2520stage%252C%2520we%2520align%250Aevent-query%2520representations%2520at%2520fine%2520granularity%2520using%2520comprehensive%2520quadruple%250Acontrastive%2520learning%253A%2520event-query%2520gather%252C%2520event-background%2520separation%252C%250Aintra-cluster%2520compactness%2520and%2520inter-cluster%2520separability.%2520Then%252C%2520high-quality%250Arepresentations%2520bring%2520acceptable%2520grounding%2520pseudo-labels.%2520In%2520the%2520explicit%250Astage%252C%2520to%2520explicitly%2520optimize%2520grounding%2520objectives%252C%2520we%2520train%2520one%250Afully-supervised%2520model%2520using%2520obtained%2520pseudo-labels%2520for%2520grounding%2520refinement%250Aand%2520denoising.%2520Extensive%2520experiments%2520and%2520thoroughly%2520ablations%2520on%2520Charades-STA%250Aand%2520ActivityNet%2520Captions%2520demonstrate%2520the%2520significance%2520of%2520partial%2520supervision%252C%250Aas%2520well%2520as%2520our%2520superior%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrast-Unity%20for%20Partially-Supervised%20Temporal%20Sentence%20Grounding&entry.906535625=Haicheng%20Wang%20and%20Chen%20Ju%20and%20Weixiong%20Lin%20and%20Chaofan%20Ma%20and%20Shuai%20Xiao%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Temporal%20sentence%20grounding%20aims%20to%20detect%20event%20timestamps%20described%20by%20the%0Anatural%20language%20query%20from%20given%20untrimmed%20videos.%20The%20existing%0Afully-supervised%20setting%20achieves%20great%20results%20but%20requires%20expensive%0Aannotation%20costs%3B%20while%20the%20weakly-supervised%20setting%20adopts%20cheap%20labels%20but%0Aperforms%20poorly.%20To%20pursue%20high%20performance%20with%20less%20annotation%20costs%2C%20this%0Apaper%20introduces%20an%20intermediate%20partially-supervised%20setting%2C%20i.e.%2C%20only%0Ashort-clip%20is%20available%20during%20training.%20To%20make%20full%20use%20of%20partial%20labels%2C%20we%0Aspecially%20design%20one%20contrast-unity%20framework%2C%20with%20the%20two-stage%20goal%20of%0Aimplicit-explicit%20progressive%20grounding.%20In%20the%20implicit%20stage%2C%20we%20align%0Aevent-query%20representations%20at%20fine%20granularity%20using%20comprehensive%20quadruple%0Acontrastive%20learning%3A%20event-query%20gather%2C%20event-background%20separation%2C%0Aintra-cluster%20compactness%20and%20inter-cluster%20separability.%20Then%2C%20high-quality%0Arepresentations%20bring%20acceptable%20grounding%20pseudo-labels.%20In%20the%20explicit%0Astage%2C%20to%20explicitly%20optimize%20grounding%20objectives%2C%20we%20train%20one%0Afully-supervised%20model%20using%20obtained%20pseudo-labels%20for%20grounding%20refinement%0Aand%20denoising.%20Extensive%20experiments%20and%20thoroughly%20ablations%20on%20Charades-STA%0Aand%20ActivityNet%20Captions%20demonstrate%20the%20significance%20of%20partial%20supervision%2C%0Aas%20well%20as%20our%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12917v1&entry.124074799=Read"},
{"title": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation", "author": "Mengkang Hu and Tianxing Chen and Yude Zou and Yuheng Lei and Qiguang Chen and Ming Li and Hongyuan Zhang and Wenqi Shao and Ping Luo", "abstract": "  Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.\n", "link": "http://arxiv.org/abs/2502.13092v1", "date": "2025-02-18", "relevancy": 2.7526, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2World%3A%20Benchmarking%20Large%20Language%20Models%20for%20Symbolic%20World%20Model%0A%20%20Generation&body=Title%3A%20Text2World%3A%20Benchmarking%20Large%20Language%20Models%20for%20Symbolic%20World%20Model%0A%20%20Generation%0AAuthor%3A%20Mengkang%20Hu%20and%20Tianxing%20Chen%20and%20Yude%20Zou%20and%20Yuheng%20Lei%20and%20Qiguang%20Chen%20and%20Ming%20Li%20and%20Hongyuan%20Zhang%20and%20Wenqi%20Shao%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20leveraging%20large%20language%20models%0A%28LLMs%29%20to%20generate%20symbolic%20world%20models%20from%20textual%20descriptions.%20Although%0ALLMs%20have%20been%20extensively%20explored%20in%20the%20context%20of%20world%20modeling%2C%20prior%0Astudies%20encountered%20several%20challenges%2C%20including%20evaluation%20randomness%2C%0Adependence%20on%20indirect%20metrics%2C%20and%20a%20limited%20domain%20scope.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20novel%20benchmark%2C%20Text2World%2C%20based%20on%20planning%0Adomain%20definition%20language%20%28PDDL%29%2C%20featuring%20hundreds%20of%20diverse%20domains%20and%0Aemploying%20multi-criteria%2C%20execution-based%20metrics%20for%20a%20more%20robust%20evaluation.%0AWe%20benchmark%20current%20LLMs%20using%20Text2World%20and%20find%20that%20reasoning%20models%0Atrained%20with%20large-scale%20reinforcement%20learning%20outperform%20others.%20However%2C%0Aeven%20the%20best-performing%20model%20still%20demonstrates%20limited%20capabilities%20in%20world%0Amodeling.%20Building%20on%20these%20insights%2C%20we%20examine%20several%20promising%20strategies%0Ato%20enhance%20the%20world%20modeling%20capabilities%20of%20LLMs%2C%20including%20test-time%0Ascaling%2C%20agent%20training%2C%20and%20more.%20We%20hope%20that%20Text2World%20can%20serve%20as%20a%0Acrucial%20resource%2C%20laying%20the%20groundwork%20for%20future%20research%20in%20leveraging%20LLMs%0Aas%20world%20models.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//text-to-world.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2World%253A%2520Benchmarking%2520Large%2520Language%2520Models%2520for%2520Symbolic%2520World%2520Model%250A%2520%2520Generation%26entry.906535625%3DMengkang%2520Hu%2520and%2520Tianxing%2520Chen%2520and%2520Yude%2520Zou%2520and%2520Yuheng%2520Lei%2520and%2520Qiguang%2520Chen%2520and%2520Ming%2520Li%2520and%2520Hongyuan%2520Zhang%2520and%2520Wenqi%2520Shao%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520growing%2520interest%2520in%2520leveraging%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520generate%2520symbolic%2520world%2520models%2520from%2520textual%2520descriptions.%2520Although%250ALLMs%2520have%2520been%2520extensively%2520explored%2520in%2520the%2520context%2520of%2520world%2520modeling%252C%2520prior%250Astudies%2520encountered%2520several%2520challenges%252C%2520including%2520evaluation%2520randomness%252C%250Adependence%2520on%2520indirect%2520metrics%252C%2520and%2520a%2520limited%2520domain%2520scope.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%252C%2520Text2World%252C%2520based%2520on%2520planning%250Adomain%2520definition%2520language%2520%2528PDDL%2529%252C%2520featuring%2520hundreds%2520of%2520diverse%2520domains%2520and%250Aemploying%2520multi-criteria%252C%2520execution-based%2520metrics%2520for%2520a%2520more%2520robust%2520evaluation.%250AWe%2520benchmark%2520current%2520LLMs%2520using%2520Text2World%2520and%2520find%2520that%2520reasoning%2520models%250Atrained%2520with%2520large-scale%2520reinforcement%2520learning%2520outperform%2520others.%2520However%252C%250Aeven%2520the%2520best-performing%2520model%2520still%2520demonstrates%2520limited%2520capabilities%2520in%2520world%250Amodeling.%2520Building%2520on%2520these%2520insights%252C%2520we%2520examine%2520several%2520promising%2520strategies%250Ato%2520enhance%2520the%2520world%2520modeling%2520capabilities%2520of%2520LLMs%252C%2520including%2520test-time%250Ascaling%252C%2520agent%2520training%252C%2520and%2520more.%2520We%2520hope%2520that%2520Text2World%2520can%2520serve%2520as%2520a%250Acrucial%2520resource%252C%2520laying%2520the%2520groundwork%2520for%2520future%2520research%2520in%2520leveraging%2520LLMs%250Aas%2520world%2520models.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//text-to-world.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2World%3A%20Benchmarking%20Large%20Language%20Models%20for%20Symbolic%20World%20Model%0A%20%20Generation&entry.906535625=Mengkang%20Hu%20and%20Tianxing%20Chen%20and%20Yude%20Zou%20and%20Yuheng%20Lei%20and%20Qiguang%20Chen%20and%20Ming%20Li%20and%20Hongyuan%20Zhang%20and%20Wenqi%20Shao%20and%20Ping%20Luo&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20leveraging%20large%20language%20models%0A%28LLMs%29%20to%20generate%20symbolic%20world%20models%20from%20textual%20descriptions.%20Although%0ALLMs%20have%20been%20extensively%20explored%20in%20the%20context%20of%20world%20modeling%2C%20prior%0Astudies%20encountered%20several%20challenges%2C%20including%20evaluation%20randomness%2C%0Adependence%20on%20indirect%20metrics%2C%20and%20a%20limited%20domain%20scope.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20novel%20benchmark%2C%20Text2World%2C%20based%20on%20planning%0Adomain%20definition%20language%20%28PDDL%29%2C%20featuring%20hundreds%20of%20diverse%20domains%20and%0Aemploying%20multi-criteria%2C%20execution-based%20metrics%20for%20a%20more%20robust%20evaluation.%0AWe%20benchmark%20current%20LLMs%20using%20Text2World%20and%20find%20that%20reasoning%20models%0Atrained%20with%20large-scale%20reinforcement%20learning%20outperform%20others.%20However%2C%0Aeven%20the%20best-performing%20model%20still%20demonstrates%20limited%20capabilities%20in%20world%0Amodeling.%20Building%20on%20these%20insights%2C%20we%20examine%20several%20promising%20strategies%0Ato%20enhance%20the%20world%20modeling%20capabilities%20of%20LLMs%2C%20including%20test-time%0Ascaling%2C%20agent%20training%2C%20and%20more.%20We%20hope%20that%20Text2World%20can%20serve%20as%20a%0Acrucial%20resource%2C%20laying%20the%20groundwork%20for%20future%20research%20in%20leveraging%20LLMs%0Aas%20world%20models.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//text-to-world.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13092v1&entry.124074799=Read"},
{"title": "Where Do We Stand with Implicit Neural Representations? A Technical and\n  Performance Survey", "author": "Amer Essakine and Yanqi Cheng and Chun-Wun Cheng and Lipei Zhang and Zhongying Deng and Lei Zhu and Carola-Bibiane Sch\u00f6nlieb and Angelica I Aviles-Rivero", "abstract": "  Implicit Neural Representations (INRs) have emerged as a paradigm in\nknowledge representation, offering exceptional flexibility and performance\nacross a diverse range of applications. INRs leverage multilayer perceptrons\n(MLPs) to model data as continuous implicit functions, providing critical\nadvantages such as resolution independence, memory efficiency, and\ngeneralisation beyond discretised data structures. Their ability to solve\ncomplex inverse problems makes them particularly effective for tasks including\naudio reconstruction, image representation, 3D object reconstruction, and\nhigh-dimensional data synthesis. This survey provides a comprehensive review of\nstate-of-the-art INR methods, introducing a clear taxonomy that categorises\nthem into four key areas: activation functions, position encoding, combined\nstrategies, and network structure optimisation. We rigorously analyse their\ncritical properties, such as full differentiability, smoothness, compactness,\nand adaptability to varying resolutions while also examining their strengths\nand limitations in addressing locality biases and capturing fine details. Our\nexperimental comparison offers new insights into the trade-offs between\ndifferent approaches, showcasing the capabilities and challenges of the latest\nINR techniques across various tasks. In addition to identifying areas where\ncurrent methods excel, we highlight key limitations and potential avenues for\nimprovement, such as developing more expressive activation functions, enhancing\npositional encoding mechanisms, and improving scalability for complex,\nhigh-dimensional data. This survey serves as a roadmap for researchers,\noffering practical guidance for future exploration in the field of INRs. We aim\nto foster new methodologies by outlining promising research directions for INRs\nand applications.\n", "link": "http://arxiv.org/abs/2411.03688v2", "date": "2025-02-18", "relevancy": 2.744, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5559}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20Do%20We%20Stand%20with%20Implicit%20Neural%20Representations%3F%20A%20Technical%20and%0A%20%20Performance%20Survey&body=Title%3A%20Where%20Do%20We%20Stand%20with%20Implicit%20Neural%20Representations%3F%20A%20Technical%20and%0A%20%20Performance%20Survey%0AAuthor%3A%20Amer%20Essakine%20and%20Yanqi%20Cheng%20and%20Chun-Wun%20Cheng%20and%20Lipei%20Zhang%20and%20Zhongying%20Deng%20and%20Lei%20Zhu%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Angelica%20I%20Aviles-Rivero%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20%28INRs%29%20have%20emerged%20as%20a%20paradigm%20in%0Aknowledge%20representation%2C%20offering%20exceptional%20flexibility%20and%20performance%0Aacross%20a%20diverse%20range%20of%20applications.%20INRs%20leverage%20multilayer%20perceptrons%0A%28MLPs%29%20to%20model%20data%20as%20continuous%20implicit%20functions%2C%20providing%20critical%0Aadvantages%20such%20as%20resolution%20independence%2C%20memory%20efficiency%2C%20and%0Ageneralisation%20beyond%20discretised%20data%20structures.%20Their%20ability%20to%20solve%0Acomplex%20inverse%20problems%20makes%20them%20particularly%20effective%20for%20tasks%20including%0Aaudio%20reconstruction%2C%20image%20representation%2C%203D%20object%20reconstruction%2C%20and%0Ahigh-dimensional%20data%20synthesis.%20This%20survey%20provides%20a%20comprehensive%20review%20of%0Astate-of-the-art%20INR%20methods%2C%20introducing%20a%20clear%20taxonomy%20that%20categorises%0Athem%20into%20four%20key%20areas%3A%20activation%20functions%2C%20position%20encoding%2C%20combined%0Astrategies%2C%20and%20network%20structure%20optimisation.%20We%20rigorously%20analyse%20their%0Acritical%20properties%2C%20such%20as%20full%20differentiability%2C%20smoothness%2C%20compactness%2C%0Aand%20adaptability%20to%20varying%20resolutions%20while%20also%20examining%20their%20strengths%0Aand%20limitations%20in%20addressing%20locality%20biases%20and%20capturing%20fine%20details.%20Our%0Aexperimental%20comparison%20offers%20new%20insights%20into%20the%20trade-offs%20between%0Adifferent%20approaches%2C%20showcasing%20the%20capabilities%20and%20challenges%20of%20the%20latest%0AINR%20techniques%20across%20various%20tasks.%20In%20addition%20to%20identifying%20areas%20where%0Acurrent%20methods%20excel%2C%20we%20highlight%20key%20limitations%20and%20potential%20avenues%20for%0Aimprovement%2C%20such%20as%20developing%20more%20expressive%20activation%20functions%2C%20enhancing%0Apositional%20encoding%20mechanisms%2C%20and%20improving%20scalability%20for%20complex%2C%0Ahigh-dimensional%20data.%20This%20survey%20serves%20as%20a%20roadmap%20for%20researchers%2C%0Aoffering%20practical%20guidance%20for%20future%20exploration%20in%20the%20field%20of%20INRs.%20We%20aim%0Ato%20foster%20new%20methodologies%20by%20outlining%20promising%20research%20directions%20for%20INRs%0Aand%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520Do%2520We%2520Stand%2520with%2520Implicit%2520Neural%2520Representations%253F%2520A%2520Technical%2520and%250A%2520%2520Performance%2520Survey%26entry.906535625%3DAmer%2520Essakine%2520and%2520Yanqi%2520Cheng%2520and%2520Chun-Wun%2520Cheng%2520and%2520Lipei%2520Zhang%2520and%2520Zhongying%2520Deng%2520and%2520Lei%2520Zhu%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Angelica%2520I%2520Aviles-Rivero%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520have%2520emerged%2520as%2520a%2520paradigm%2520in%250Aknowledge%2520representation%252C%2520offering%2520exceptional%2520flexibility%2520and%2520performance%250Aacross%2520a%2520diverse%2520range%2520of%2520applications.%2520INRs%2520leverage%2520multilayer%2520perceptrons%250A%2528MLPs%2529%2520to%2520model%2520data%2520as%2520continuous%2520implicit%2520functions%252C%2520providing%2520critical%250Aadvantages%2520such%2520as%2520resolution%2520independence%252C%2520memory%2520efficiency%252C%2520and%250Ageneralisation%2520beyond%2520discretised%2520data%2520structures.%2520Their%2520ability%2520to%2520solve%250Acomplex%2520inverse%2520problems%2520makes%2520them%2520particularly%2520effective%2520for%2520tasks%2520including%250Aaudio%2520reconstruction%252C%2520image%2520representation%252C%25203D%2520object%2520reconstruction%252C%2520and%250Ahigh-dimensional%2520data%2520synthesis.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520review%2520of%250Astate-of-the-art%2520INR%2520methods%252C%2520introducing%2520a%2520clear%2520taxonomy%2520that%2520categorises%250Athem%2520into%2520four%2520key%2520areas%253A%2520activation%2520functions%252C%2520position%2520encoding%252C%2520combined%250Astrategies%252C%2520and%2520network%2520structure%2520optimisation.%2520We%2520rigorously%2520analyse%2520their%250Acritical%2520properties%252C%2520such%2520as%2520full%2520differentiability%252C%2520smoothness%252C%2520compactness%252C%250Aand%2520adaptability%2520to%2520varying%2520resolutions%2520while%2520also%2520examining%2520their%2520strengths%250Aand%2520limitations%2520in%2520addressing%2520locality%2520biases%2520and%2520capturing%2520fine%2520details.%2520Our%250Aexperimental%2520comparison%2520offers%2520new%2520insights%2520into%2520the%2520trade-offs%2520between%250Adifferent%2520approaches%252C%2520showcasing%2520the%2520capabilities%2520and%2520challenges%2520of%2520the%2520latest%250AINR%2520techniques%2520across%2520various%2520tasks.%2520In%2520addition%2520to%2520identifying%2520areas%2520where%250Acurrent%2520methods%2520excel%252C%2520we%2520highlight%2520key%2520limitations%2520and%2520potential%2520avenues%2520for%250Aimprovement%252C%2520such%2520as%2520developing%2520more%2520expressive%2520activation%2520functions%252C%2520enhancing%250Apositional%2520encoding%2520mechanisms%252C%2520and%2520improving%2520scalability%2520for%2520complex%252C%250Ahigh-dimensional%2520data.%2520This%2520survey%2520serves%2520as%2520a%2520roadmap%2520for%2520researchers%252C%250Aoffering%2520practical%2520guidance%2520for%2520future%2520exploration%2520in%2520the%2520field%2520of%2520INRs.%2520We%2520aim%250Ato%2520foster%2520new%2520methodologies%2520by%2520outlining%2520promising%2520research%2520directions%2520for%2520INRs%250Aand%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20Do%20We%20Stand%20with%20Implicit%20Neural%20Representations%3F%20A%20Technical%20and%0A%20%20Performance%20Survey&entry.906535625=Amer%20Essakine%20and%20Yanqi%20Cheng%20and%20Chun-Wun%20Cheng%20and%20Lipei%20Zhang%20and%20Zhongying%20Deng%20and%20Lei%20Zhu%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Angelica%20I%20Aviles-Rivero&entry.1292438233=%20%20Implicit%20Neural%20Representations%20%28INRs%29%20have%20emerged%20as%20a%20paradigm%20in%0Aknowledge%20representation%2C%20offering%20exceptional%20flexibility%20and%20performance%0Aacross%20a%20diverse%20range%20of%20applications.%20INRs%20leverage%20multilayer%20perceptrons%0A%28MLPs%29%20to%20model%20data%20as%20continuous%20implicit%20functions%2C%20providing%20critical%0Aadvantages%20such%20as%20resolution%20independence%2C%20memory%20efficiency%2C%20and%0Ageneralisation%20beyond%20discretised%20data%20structures.%20Their%20ability%20to%20solve%0Acomplex%20inverse%20problems%20makes%20them%20particularly%20effective%20for%20tasks%20including%0Aaudio%20reconstruction%2C%20image%20representation%2C%203D%20object%20reconstruction%2C%20and%0Ahigh-dimensional%20data%20synthesis.%20This%20survey%20provides%20a%20comprehensive%20review%20of%0Astate-of-the-art%20INR%20methods%2C%20introducing%20a%20clear%20taxonomy%20that%20categorises%0Athem%20into%20four%20key%20areas%3A%20activation%20functions%2C%20position%20encoding%2C%20combined%0Astrategies%2C%20and%20network%20structure%20optimisation.%20We%20rigorously%20analyse%20their%0Acritical%20properties%2C%20such%20as%20full%20differentiability%2C%20smoothness%2C%20compactness%2C%0Aand%20adaptability%20to%20varying%20resolutions%20while%20also%20examining%20their%20strengths%0Aand%20limitations%20in%20addressing%20locality%20biases%20and%20capturing%20fine%20details.%20Our%0Aexperimental%20comparison%20offers%20new%20insights%20into%20the%20trade-offs%20between%0Adifferent%20approaches%2C%20showcasing%20the%20capabilities%20and%20challenges%20of%20the%20latest%0AINR%20techniques%20across%20various%20tasks.%20In%20addition%20to%20identifying%20areas%20where%0Acurrent%20methods%20excel%2C%20we%20highlight%20key%20limitations%20and%20potential%20avenues%20for%0Aimprovement%2C%20such%20as%20developing%20more%20expressive%20activation%20functions%2C%20enhancing%0Apositional%20encoding%20mechanisms%2C%20and%20improving%20scalability%20for%20complex%2C%0Ahigh-dimensional%20data.%20This%20survey%20serves%20as%20a%20roadmap%20for%20researchers%2C%0Aoffering%20practical%20guidance%20for%20future%20exploration%20in%20the%20field%20of%20INRs.%20We%20aim%0Ato%20foster%20new%20methodologies%20by%20outlining%20promising%20research%20directions%20for%20INRs%0Aand%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03688v2&entry.124074799=Read"},
{"title": "Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The\n  answer is No!", "author": "Mohamed Fazli Imam and Chenyang Lyu and Alham Fikri Aji", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved significant\nadvancements in tasks like Visual Question Answering (VQA) by leveraging\nfoundational Large Language Models (LLMs). However, their abilities in specific\nareas such as visual temporal understanding, which is crucial for comprehending\nreal-world dynamics, remain underexplored. To address this, we propose a\nchallenging evaluation benchmark named TemporalVQA, consisting of two parts: 1)\nTemporal Order Understanding and 2) Time-lapse Estimation. The first part\nrequires MLLMs to determine the sequence of events by analyzing temporally\nconsecutive video frames. The second part presents image pairs with varying\ntime differences, framed as multiple-choice questions, asking MLLMs to estimate\nthe time-lapse between images with options ranging from seconds to years. Our\nevaluations of advanced MLLMs, including models like GPT-4o and Gemini-1.5-Pro,\nreveal significant challenges: GPT-4o achieved only 49.1% average consistent\naccuracy in temporal order task and 70% in time-lapse estimation, with\nopen-source models performing even poorly. These findings underscore the\nlimitations of current MLLMs in visual temporal understanding and reasoning,\nhighlighting the need for further improvements for their temporal capability.\nOur dataset can be found at\nhttps://huggingface.co/datasets/fazliimam/temporal-vqa.\n", "link": "http://arxiv.org/abs/2501.10674v2", "date": "2025-02-18", "relevancy": 2.7223, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Multimodal%20LLMs%20do%20Visual%20Temporal%20Understanding%20and%20Reasoning%3F%20The%0A%20%20answer%20is%20No%21&body=Title%3A%20Can%20Multimodal%20LLMs%20do%20Visual%20Temporal%20Understanding%20and%20Reasoning%3F%20The%0A%20%20answer%20is%20No%21%0AAuthor%3A%20Mohamed%20Fazli%20Imam%20and%20Chenyang%20Lyu%20and%20Alham%20Fikri%20Aji%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20significant%0Aadvancements%20in%20tasks%20like%20Visual%20Question%20Answering%20%28VQA%29%20by%20leveraging%0Afoundational%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20their%20abilities%20in%20specific%0Aareas%20such%20as%20visual%20temporal%20understanding%2C%20which%20is%20crucial%20for%20comprehending%0Areal-world%20dynamics%2C%20remain%20underexplored.%20To%20address%20this%2C%20we%20propose%20a%0Achallenging%20evaluation%20benchmark%20named%20TemporalVQA%2C%20consisting%20of%20two%20parts%3A%201%29%0ATemporal%20Order%20Understanding%20and%202%29%20Time-lapse%20Estimation.%20The%20first%20part%0Arequires%20MLLMs%20to%20determine%20the%20sequence%20of%20events%20by%20analyzing%20temporally%0Aconsecutive%20video%20frames.%20The%20second%20part%20presents%20image%20pairs%20with%20varying%0Atime%20differences%2C%20framed%20as%20multiple-choice%20questions%2C%20asking%20MLLMs%20to%20estimate%0Athe%20time-lapse%20between%20images%20with%20options%20ranging%20from%20seconds%20to%20years.%20Our%0Aevaluations%20of%20advanced%20MLLMs%2C%20including%20models%20like%20GPT-4o%20and%20Gemini-1.5-Pro%2C%0Areveal%20significant%20challenges%3A%20GPT-4o%20achieved%20only%2049.1%25%20average%20consistent%0Aaccuracy%20in%20temporal%20order%20task%20and%2070%25%20in%20time-lapse%20estimation%2C%20with%0Aopen-source%20models%20performing%20even%20poorly.%20These%20findings%20underscore%20the%0Alimitations%20of%20current%20MLLMs%20in%20visual%20temporal%20understanding%20and%20reasoning%2C%0Ahighlighting%20the%20need%20for%20further%20improvements%20for%20their%20temporal%20capability.%0AOur%20dataset%20can%20be%20found%20at%0Ahttps%3A//huggingface.co/datasets/fazliimam/temporal-vqa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10674v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Multimodal%2520LLMs%2520do%2520Visual%2520Temporal%2520Understanding%2520and%2520Reasoning%253F%2520The%250A%2520%2520answer%2520is%2520No%2521%26entry.906535625%3DMohamed%2520Fazli%2520Imam%2520and%2520Chenyang%2520Lyu%2520and%2520Alham%2520Fikri%2520Aji%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520significant%250Aadvancements%2520in%2520tasks%2520like%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520by%2520leveraging%250Afoundational%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%2520their%2520abilities%2520in%2520specific%250Aareas%2520such%2520as%2520visual%2520temporal%2520understanding%252C%2520which%2520is%2520crucial%2520for%2520comprehending%250Areal-world%2520dynamics%252C%2520remain%2520underexplored.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Achallenging%2520evaluation%2520benchmark%2520named%2520TemporalVQA%252C%2520consisting%2520of%2520two%2520parts%253A%25201%2529%250ATemporal%2520Order%2520Understanding%2520and%25202%2529%2520Time-lapse%2520Estimation.%2520The%2520first%2520part%250Arequires%2520MLLMs%2520to%2520determine%2520the%2520sequence%2520of%2520events%2520by%2520analyzing%2520temporally%250Aconsecutive%2520video%2520frames.%2520The%2520second%2520part%2520presents%2520image%2520pairs%2520with%2520varying%250Atime%2520differences%252C%2520framed%2520as%2520multiple-choice%2520questions%252C%2520asking%2520MLLMs%2520to%2520estimate%250Athe%2520time-lapse%2520between%2520images%2520with%2520options%2520ranging%2520from%2520seconds%2520to%2520years.%2520Our%250Aevaluations%2520of%2520advanced%2520MLLMs%252C%2520including%2520models%2520like%2520GPT-4o%2520and%2520Gemini-1.5-Pro%252C%250Areveal%2520significant%2520challenges%253A%2520GPT-4o%2520achieved%2520only%252049.1%2525%2520average%2520consistent%250Aaccuracy%2520in%2520temporal%2520order%2520task%2520and%252070%2525%2520in%2520time-lapse%2520estimation%252C%2520with%250Aopen-source%2520models%2520performing%2520even%2520poorly.%2520These%2520findings%2520underscore%2520the%250Alimitations%2520of%2520current%2520MLLMs%2520in%2520visual%2520temporal%2520understanding%2520and%2520reasoning%252C%250Ahighlighting%2520the%2520need%2520for%2520further%2520improvements%2520for%2520their%2520temporal%2520capability.%250AOur%2520dataset%2520can%2520be%2520found%2520at%250Ahttps%253A//huggingface.co/datasets/fazliimam/temporal-vqa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10674v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Multimodal%20LLMs%20do%20Visual%20Temporal%20Understanding%20and%20Reasoning%3F%20The%0A%20%20answer%20is%20No%21&entry.906535625=Mohamed%20Fazli%20Imam%20and%20Chenyang%20Lyu%20and%20Alham%20Fikri%20Aji&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20significant%0Aadvancements%20in%20tasks%20like%20Visual%20Question%20Answering%20%28VQA%29%20by%20leveraging%0Afoundational%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20their%20abilities%20in%20specific%0Aareas%20such%20as%20visual%20temporal%20understanding%2C%20which%20is%20crucial%20for%20comprehending%0Areal-world%20dynamics%2C%20remain%20underexplored.%20To%20address%20this%2C%20we%20propose%20a%0Achallenging%20evaluation%20benchmark%20named%20TemporalVQA%2C%20consisting%20of%20two%20parts%3A%201%29%0ATemporal%20Order%20Understanding%20and%202%29%20Time-lapse%20Estimation.%20The%20first%20part%0Arequires%20MLLMs%20to%20determine%20the%20sequence%20of%20events%20by%20analyzing%20temporally%0Aconsecutive%20video%20frames.%20The%20second%20part%20presents%20image%20pairs%20with%20varying%0Atime%20differences%2C%20framed%20as%20multiple-choice%20questions%2C%20asking%20MLLMs%20to%20estimate%0Athe%20time-lapse%20between%20images%20with%20options%20ranging%20from%20seconds%20to%20years.%20Our%0Aevaluations%20of%20advanced%20MLLMs%2C%20including%20models%20like%20GPT-4o%20and%20Gemini-1.5-Pro%2C%0Areveal%20significant%20challenges%3A%20GPT-4o%20achieved%20only%2049.1%25%20average%20consistent%0Aaccuracy%20in%20temporal%20order%20task%20and%2070%25%20in%20time-lapse%20estimation%2C%20with%0Aopen-source%20models%20performing%20even%20poorly.%20These%20findings%20underscore%20the%0Alimitations%20of%20current%20MLLMs%20in%20visual%20temporal%20understanding%20and%20reasoning%2C%0Ahighlighting%20the%20need%20for%20further%20improvements%20for%20their%20temporal%20capability.%0AOur%20dataset%20can%20be%20found%20at%0Ahttps%3A//huggingface.co/datasets/fazliimam/temporal-vqa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10674v2&entry.124074799=Read"},
{"title": "Locality-aware Cross-modal Correspondence Learning for Dense\n  Audio-Visual Events Localization", "author": "Ling Xing and Hongyu Qu and Rui Yan and Xiangbo Shu and Jinhui Tang", "abstract": "  Dense-localization Audio-Visual Events (DAVE) aims to identify time\nboundaries and corresponding categories for events that can be heard and seen\nconcurrently in an untrimmed video. Existing DAVE solutions extract audio and\nvisual features through modality-specific encoders and fuse them via dense\ncross-attention. The independent processing of each modality neglects their\ncomplementarity, resulting in modality-specific noise, while dense attention\nfails to account for local temporal continuity of events, causing irrelevant\nsignal distractions. In this paper, we present LoCo, a Locality-aware\ncross-modal Correspondence learning framework for DAVE. The core idea is to\nexplore local temporal continuity nature of audio-visual events, which serves\nas informative yet free supervision signals to guide the filtering of\nirrelevant information and inspire the extraction of complementary multimodal\ninformation during both unimodal and cross-modal learning stages. i)\nSpecifically, LoCo applies Locality-aware Correspondence Correction (LCC) to\nunimodal features via leveraging cross-modal local-correlated properties\nwithout any extra annotations. This enforces unimodal encoders to highlight\nsimilar semantics shared by audio and visual features. ii) To better aggregate\nsuch audio and visual features, we further customize Cross-modal Dynamic\nPerception layer (CDP) in cross-modal feature pyramid to understand local\ntemporal patterns of audio-visual events by imposing local consistency within\nmultimodal features in a data-driven manner. By incorporating LCC and CDP, LoCo\nprovides solid performance gains and outperforms existing DAVE methods.\n", "link": "http://arxiv.org/abs/2409.07967v2", "date": "2025-02-18", "relevancy": 2.7175, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization&body=Title%3A%20Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization%0AAuthor%3A%20Ling%20Xing%20and%20Hongyu%20Qu%20and%20Rui%20Yan%20and%20Xiangbo%20Shu%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Dense-localization%20Audio-Visual%20Events%20%28DAVE%29%20aims%20to%20identify%20time%0Aboundaries%20and%20corresponding%20categories%20for%20events%20that%20can%20be%20heard%20and%20seen%0Aconcurrently%20in%20an%20untrimmed%20video.%20Existing%20DAVE%20solutions%20extract%20audio%20and%0Avisual%20features%20through%20modality-specific%20encoders%20and%20fuse%20them%20via%20dense%0Across-attention.%20The%20independent%20processing%20of%20each%20modality%20neglects%20their%0Acomplementarity%2C%20resulting%20in%20modality-specific%20noise%2C%20while%20dense%20attention%0Afails%20to%20account%20for%20local%20temporal%20continuity%20of%20events%2C%20causing%20irrelevant%0Asignal%20distractions.%20In%20this%20paper%2C%20we%20present%20LoCo%2C%20a%20Locality-aware%0Across-modal%20Correspondence%20learning%20framework%20for%20DAVE.%20The%20core%20idea%20is%20to%0Aexplore%20local%20temporal%20continuity%20nature%20of%20audio-visual%20events%2C%20which%20serves%0Aas%20informative%20yet%20free%20supervision%20signals%20to%20guide%20the%20filtering%20of%0Airrelevant%20information%20and%20inspire%20the%20extraction%20of%20complementary%20multimodal%0Ainformation%20during%20both%20unimodal%20and%20cross-modal%20learning%20stages.%20i%29%0ASpecifically%2C%20LoCo%20applies%20Locality-aware%20Correspondence%20Correction%20%28LCC%29%20to%0Aunimodal%20features%20via%20leveraging%20cross-modal%20local-correlated%20properties%0Awithout%20any%20extra%20annotations.%20This%20enforces%20unimodal%20encoders%20to%20highlight%0Asimilar%20semantics%20shared%20by%20audio%20and%20visual%20features.%20ii%29%20To%20better%20aggregate%0Asuch%20audio%20and%20visual%20features%2C%20we%20further%20customize%20Cross-modal%20Dynamic%0APerception%20layer%20%28CDP%29%20in%20cross-modal%20feature%20pyramid%20to%20understand%20local%0Atemporal%20patterns%20of%20audio-visual%20events%20by%20imposing%20local%20consistency%20within%0Amultimodal%20features%20in%20a%20data-driven%20manner.%20By%20incorporating%20LCC%20and%20CDP%2C%20LoCo%0Aprovides%20solid%20performance%20gains%20and%20outperforms%20existing%20DAVE%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07967v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocality-aware%2520Cross-modal%2520Correspondence%2520Learning%2520for%2520Dense%250A%2520%2520Audio-Visual%2520Events%2520Localization%26entry.906535625%3DLing%2520Xing%2520and%2520Hongyu%2520Qu%2520and%2520Rui%2520Yan%2520and%2520Xiangbo%2520Shu%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Dense-localization%2520Audio-Visual%2520Events%2520%2528DAVE%2529%2520aims%2520to%2520identify%2520time%250Aboundaries%2520and%2520corresponding%2520categories%2520for%2520events%2520that%2520can%2520be%2520heard%2520and%2520seen%250Aconcurrently%2520in%2520an%2520untrimmed%2520video.%2520Existing%2520DAVE%2520solutions%2520extract%2520audio%2520and%250Avisual%2520features%2520through%2520modality-specific%2520encoders%2520and%2520fuse%2520them%2520via%2520dense%250Across-attention.%2520The%2520independent%2520processing%2520of%2520each%2520modality%2520neglects%2520their%250Acomplementarity%252C%2520resulting%2520in%2520modality-specific%2520noise%252C%2520while%2520dense%2520attention%250Afails%2520to%2520account%2520for%2520local%2520temporal%2520continuity%2520of%2520events%252C%2520causing%2520irrelevant%250Asignal%2520distractions.%2520In%2520this%2520paper%252C%2520we%2520present%2520LoCo%252C%2520a%2520Locality-aware%250Across-modal%2520Correspondence%2520learning%2520framework%2520for%2520DAVE.%2520The%2520core%2520idea%2520is%2520to%250Aexplore%2520local%2520temporal%2520continuity%2520nature%2520of%2520audio-visual%2520events%252C%2520which%2520serves%250Aas%2520informative%2520yet%2520free%2520supervision%2520signals%2520to%2520guide%2520the%2520filtering%2520of%250Airrelevant%2520information%2520and%2520inspire%2520the%2520extraction%2520of%2520complementary%2520multimodal%250Ainformation%2520during%2520both%2520unimodal%2520and%2520cross-modal%2520learning%2520stages.%2520i%2529%250ASpecifically%252C%2520LoCo%2520applies%2520Locality-aware%2520Correspondence%2520Correction%2520%2528LCC%2529%2520to%250Aunimodal%2520features%2520via%2520leveraging%2520cross-modal%2520local-correlated%2520properties%250Awithout%2520any%2520extra%2520annotations.%2520This%2520enforces%2520unimodal%2520encoders%2520to%2520highlight%250Asimilar%2520semantics%2520shared%2520by%2520audio%2520and%2520visual%2520features.%2520ii%2529%2520To%2520better%2520aggregate%250Asuch%2520audio%2520and%2520visual%2520features%252C%2520we%2520further%2520customize%2520Cross-modal%2520Dynamic%250APerception%2520layer%2520%2528CDP%2529%2520in%2520cross-modal%2520feature%2520pyramid%2520to%2520understand%2520local%250Atemporal%2520patterns%2520of%2520audio-visual%2520events%2520by%2520imposing%2520local%2520consistency%2520within%250Amultimodal%2520features%2520in%2520a%2520data-driven%2520manner.%2520By%2520incorporating%2520LCC%2520and%2520CDP%252C%2520LoCo%250Aprovides%2520solid%2520performance%2520gains%2520and%2520outperforms%2520existing%2520DAVE%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07967v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locality-aware%20Cross-modal%20Correspondence%20Learning%20for%20Dense%0A%20%20Audio-Visual%20Events%20Localization&entry.906535625=Ling%20Xing%20and%20Hongyu%20Qu%20and%20Rui%20Yan%20and%20Xiangbo%20Shu%20and%20Jinhui%20Tang&entry.1292438233=%20%20Dense-localization%20Audio-Visual%20Events%20%28DAVE%29%20aims%20to%20identify%20time%0Aboundaries%20and%20corresponding%20categories%20for%20events%20that%20can%20be%20heard%20and%20seen%0Aconcurrently%20in%20an%20untrimmed%20video.%20Existing%20DAVE%20solutions%20extract%20audio%20and%0Avisual%20features%20through%20modality-specific%20encoders%20and%20fuse%20them%20via%20dense%0Across-attention.%20The%20independent%20processing%20of%20each%20modality%20neglects%20their%0Acomplementarity%2C%20resulting%20in%20modality-specific%20noise%2C%20while%20dense%20attention%0Afails%20to%20account%20for%20local%20temporal%20continuity%20of%20events%2C%20causing%20irrelevant%0Asignal%20distractions.%20In%20this%20paper%2C%20we%20present%20LoCo%2C%20a%20Locality-aware%0Across-modal%20Correspondence%20learning%20framework%20for%20DAVE.%20The%20core%20idea%20is%20to%0Aexplore%20local%20temporal%20continuity%20nature%20of%20audio-visual%20events%2C%20which%20serves%0Aas%20informative%20yet%20free%20supervision%20signals%20to%20guide%20the%20filtering%20of%0Airrelevant%20information%20and%20inspire%20the%20extraction%20of%20complementary%20multimodal%0Ainformation%20during%20both%20unimodal%20and%20cross-modal%20learning%20stages.%20i%29%0ASpecifically%2C%20LoCo%20applies%20Locality-aware%20Correspondence%20Correction%20%28LCC%29%20to%0Aunimodal%20features%20via%20leveraging%20cross-modal%20local-correlated%20properties%0Awithout%20any%20extra%20annotations.%20This%20enforces%20unimodal%20encoders%20to%20highlight%0Asimilar%20semantics%20shared%20by%20audio%20and%20visual%20features.%20ii%29%20To%20better%20aggregate%0Asuch%20audio%20and%20visual%20features%2C%20we%20further%20customize%20Cross-modal%20Dynamic%0APerception%20layer%20%28CDP%29%20in%20cross-modal%20feature%20pyramid%20to%20understand%20local%0Atemporal%20patterns%20of%20audio-visual%20events%20by%20imposing%20local%20consistency%20within%0Amultimodal%20features%20in%20a%20data-driven%20manner.%20By%20incorporating%20LCC%20and%20CDP%2C%20LoCo%0Aprovides%20solid%20performance%20gains%20and%20outperforms%20existing%20DAVE%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07967v2&entry.124074799=Read"},
{"title": "Understanding and Rectifying Safety Perception Distortion in VLMs", "author": "Xiaohan Zou and Jian Kang and George Kesidis and Lu Lin", "abstract": "  Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.\n", "link": "http://arxiv.org/abs/2502.13095v1", "date": "2025-02-18", "relevancy": 2.7118, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Rectifying%20Safety%20Perception%20Distortion%20in%20VLMs&body=Title%3A%20Understanding%20and%20Rectifying%20Safety%20Perception%20Distortion%20in%20VLMs%0AAuthor%3A%20Xiaohan%20Zou%20and%20Jian%20Kang%20and%20George%20Kesidis%20and%20Lu%20Lin%0AAbstract%3A%20%20%20Recent%20studies%20reveal%20that%20vision-language%20models%20%28VLMs%29%20become%20more%0Asusceptible%20to%20harmful%20requests%20and%20jailbreak%20attacks%20after%20integrating%20the%0Avision%20modality%2C%20exhibiting%20greater%20vulnerability%20than%20their%20text-only%20LLM%0Abackbones.%20To%20uncover%20the%20root%20cause%20of%20this%20phenomenon%2C%20we%20conduct%20an%20in-depth%0Aanalysis%20and%20identify%20a%20key%20issue%3A%20multimodal%20inputs%20introduce%20an%0Amodality-induced%20activation%20shift%20toward%20a%20%22safer%22%20direction%20compared%20to%20their%0Atext-only%20counterparts%2C%20leading%20VLMs%20to%20systematically%20overestimate%20the%20safety%0Aof%20harmful%20inputs.%20We%20refer%20to%20this%20issue%20as%20safety%20perception%20distortion.%20To%0Amitigate%20such%20distortion%2C%20we%20propose%20Activation%20Shift%20Disentanglement%20and%0ACalibration%20%28ShiftDC%29%2C%20a%20training-free%20method%20that%20decomposes%20and%20calibrates%0Athe%20modality-induced%20activation%20shift%20to%20reduce%20the%20impact%20of%20modality%20on%0Asafety.%20By%20isolating%20and%20removing%20the%20safety-relevant%20component%2C%20ShiftDC%0Arestores%20the%20inherent%20safety%20alignment%20of%20the%20LLM%20backbone%20while%20preserving%20the%0Avision-language%20capabilities%20of%20VLMs.%20Empirical%20results%20demonstrate%20that%0AShiftDC%20significantly%20enhances%20alignment%20performance%20on%20safety%20benchmarks%0Awithout%20impairing%20model%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Rectifying%2520Safety%2520Perception%2520Distortion%2520in%2520VLMs%26entry.906535625%3DXiaohan%2520Zou%2520and%2520Jian%2520Kang%2520and%2520George%2520Kesidis%2520and%2520Lu%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520studies%2520reveal%2520that%2520vision-language%2520models%2520%2528VLMs%2529%2520become%2520more%250Asusceptible%2520to%2520harmful%2520requests%2520and%2520jailbreak%2520attacks%2520after%2520integrating%2520the%250Avision%2520modality%252C%2520exhibiting%2520greater%2520vulnerability%2520than%2520their%2520text-only%2520LLM%250Abackbones.%2520To%2520uncover%2520the%2520root%2520cause%2520of%2520this%2520phenomenon%252C%2520we%2520conduct%2520an%2520in-depth%250Aanalysis%2520and%2520identify%2520a%2520key%2520issue%253A%2520multimodal%2520inputs%2520introduce%2520an%250Amodality-induced%2520activation%2520shift%2520toward%2520a%2520%2522safer%2522%2520direction%2520compared%2520to%2520their%250Atext-only%2520counterparts%252C%2520leading%2520VLMs%2520to%2520systematically%2520overestimate%2520the%2520safety%250Aof%2520harmful%2520inputs.%2520We%2520refer%2520to%2520this%2520issue%2520as%2520safety%2520perception%2520distortion.%2520To%250Amitigate%2520such%2520distortion%252C%2520we%2520propose%2520Activation%2520Shift%2520Disentanglement%2520and%250ACalibration%2520%2528ShiftDC%2529%252C%2520a%2520training-free%2520method%2520that%2520decomposes%2520and%2520calibrates%250Athe%2520modality-induced%2520activation%2520shift%2520to%2520reduce%2520the%2520impact%2520of%2520modality%2520on%250Asafety.%2520By%2520isolating%2520and%2520removing%2520the%2520safety-relevant%2520component%252C%2520ShiftDC%250Arestores%2520the%2520inherent%2520safety%2520alignment%2520of%2520the%2520LLM%2520backbone%2520while%2520preserving%2520the%250Avision-language%2520capabilities%2520of%2520VLMs.%2520Empirical%2520results%2520demonstrate%2520that%250AShiftDC%2520significantly%2520enhances%2520alignment%2520performance%2520on%2520safety%2520benchmarks%250Awithout%2520impairing%2520model%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Rectifying%20Safety%20Perception%20Distortion%20in%20VLMs&entry.906535625=Xiaohan%20Zou%20and%20Jian%20Kang%20and%20George%20Kesidis%20and%20Lu%20Lin&entry.1292438233=%20%20Recent%20studies%20reveal%20that%20vision-language%20models%20%28VLMs%29%20become%20more%0Asusceptible%20to%20harmful%20requests%20and%20jailbreak%20attacks%20after%20integrating%20the%0Avision%20modality%2C%20exhibiting%20greater%20vulnerability%20than%20their%20text-only%20LLM%0Abackbones.%20To%20uncover%20the%20root%20cause%20of%20this%20phenomenon%2C%20we%20conduct%20an%20in-depth%0Aanalysis%20and%20identify%20a%20key%20issue%3A%20multimodal%20inputs%20introduce%20an%0Amodality-induced%20activation%20shift%20toward%20a%20%22safer%22%20direction%20compared%20to%20their%0Atext-only%20counterparts%2C%20leading%20VLMs%20to%20systematically%20overestimate%20the%20safety%0Aof%20harmful%20inputs.%20We%20refer%20to%20this%20issue%20as%20safety%20perception%20distortion.%20To%0Amitigate%20such%20distortion%2C%20we%20propose%20Activation%20Shift%20Disentanglement%20and%0ACalibration%20%28ShiftDC%29%2C%20a%20training-free%20method%20that%20decomposes%20and%20calibrates%0Athe%20modality-induced%20activation%20shift%20to%20reduce%20the%20impact%20of%20modality%20on%0Asafety.%20By%20isolating%20and%20removing%20the%20safety-relevant%20component%2C%20ShiftDC%0Arestores%20the%20inherent%20safety%20alignment%20of%20the%20LLM%20backbone%20while%20preserving%20the%0Avision-language%20capabilities%20of%20VLMs.%20Empirical%20results%20demonstrate%20that%0AShiftDC%20significantly%20enhances%20alignment%20performance%20on%20safety%20benchmarks%0Awithout%20impairing%20model%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13095v1&entry.124074799=Read"},
{"title": "An Experimental Study of SOTA LiDAR Segmentation Models", "author": "Bike Chen and Antti Tikanm\u00e4ki and Juha R\u00f6ning", "abstract": "  Point cloud segmentation (PCS) is to classify each point in point clouds. The\ntask enables robots to parse their 3D surroundings and run autonomously.\nAccording to different point cloud representations, existing PCS models can be\nroughly divided into point-, voxel-, and range image-based models. However, no\nwork has been found to report comprehensive comparisons among the\nstate-of-the-art point-, voxel-, and range image-based models from an\napplication perspective, bringing difficulty in utilizing these models for\nreal-world scenarios. In this paper, we provide thorough comparisons among the\nmodels by considering the LiDAR data motion compensation and the metrics of\nmodel parameters, max GPU memory allocated during testing, inference latency,\nframes per second, intersection-over-union (IoU) and mean IoU (mIoU) scores.\nThe experimental results benefit engineers when choosing a reasonable PCS model\nfor an application and inspire researchers in the PCS field to design more\npractical models for a real-world scenario.\n", "link": "http://arxiv.org/abs/2502.12860v1", "date": "2025-02-18", "relevancy": 2.7048, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Experimental%20Study%20of%20SOTA%20LiDAR%20Segmentation%20Models&body=Title%3A%20An%20Experimental%20Study%20of%20SOTA%20LiDAR%20Segmentation%20Models%0AAuthor%3A%20Bike%20Chen%20and%20Antti%20Tikanm%C3%A4ki%20and%20Juha%20R%C3%B6ning%0AAbstract%3A%20%20%20Point%20cloud%20segmentation%20%28PCS%29%20is%20to%20classify%20each%20point%20in%20point%20clouds.%20The%0Atask%20enables%20robots%20to%20parse%20their%203D%20surroundings%20and%20run%20autonomously.%0AAccording%20to%20different%20point%20cloud%20representations%2C%20existing%20PCS%20models%20can%20be%0Aroughly%20divided%20into%20point-%2C%20voxel-%2C%20and%20range%20image-based%20models.%20However%2C%20no%0Awork%20has%20been%20found%20to%20report%20comprehensive%20comparisons%20among%20the%0Astate-of-the-art%20point-%2C%20voxel-%2C%20and%20range%20image-based%20models%20from%20an%0Aapplication%20perspective%2C%20bringing%20difficulty%20in%20utilizing%20these%20models%20for%0Areal-world%20scenarios.%20In%20this%20paper%2C%20we%20provide%20thorough%20comparisons%20among%20the%0Amodels%20by%20considering%20the%20LiDAR%20data%20motion%20compensation%20and%20the%20metrics%20of%0Amodel%20parameters%2C%20max%20GPU%20memory%20allocated%20during%20testing%2C%20inference%20latency%2C%0Aframes%20per%20second%2C%20intersection-over-union%20%28IoU%29%20and%20mean%20IoU%20%28mIoU%29%20scores.%0AThe%20experimental%20results%20benefit%20engineers%20when%20choosing%20a%20reasonable%20PCS%20model%0Afor%20an%20application%20and%20inspire%20researchers%20in%20the%20PCS%20field%20to%20design%20more%0Apractical%20models%20for%20a%20real-world%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Experimental%2520Study%2520of%2520SOTA%2520LiDAR%2520Segmentation%2520Models%26entry.906535625%3DBike%2520Chen%2520and%2520Antti%2520Tikanm%25C3%25A4ki%2520and%2520Juha%2520R%25C3%25B6ning%26entry.1292438233%3D%2520%2520Point%2520cloud%2520segmentation%2520%2528PCS%2529%2520is%2520to%2520classify%2520each%2520point%2520in%2520point%2520clouds.%2520The%250Atask%2520enables%2520robots%2520to%2520parse%2520their%25203D%2520surroundings%2520and%2520run%2520autonomously.%250AAccording%2520to%2520different%2520point%2520cloud%2520representations%252C%2520existing%2520PCS%2520models%2520can%2520be%250Aroughly%2520divided%2520into%2520point-%252C%2520voxel-%252C%2520and%2520range%2520image-based%2520models.%2520However%252C%2520no%250Awork%2520has%2520been%2520found%2520to%2520report%2520comprehensive%2520comparisons%2520among%2520the%250Astate-of-the-art%2520point-%252C%2520voxel-%252C%2520and%2520range%2520image-based%2520models%2520from%2520an%250Aapplication%2520perspective%252C%2520bringing%2520difficulty%2520in%2520utilizing%2520these%2520models%2520for%250Areal-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520provide%2520thorough%2520comparisons%2520among%2520the%250Amodels%2520by%2520considering%2520the%2520LiDAR%2520data%2520motion%2520compensation%2520and%2520the%2520metrics%2520of%250Amodel%2520parameters%252C%2520max%2520GPU%2520memory%2520allocated%2520during%2520testing%252C%2520inference%2520latency%252C%250Aframes%2520per%2520second%252C%2520intersection-over-union%2520%2528IoU%2529%2520and%2520mean%2520IoU%2520%2528mIoU%2529%2520scores.%250AThe%2520experimental%2520results%2520benefit%2520engineers%2520when%2520choosing%2520a%2520reasonable%2520PCS%2520model%250Afor%2520an%2520application%2520and%2520inspire%2520researchers%2520in%2520the%2520PCS%2520field%2520to%2520design%2520more%250Apractical%2520models%2520for%2520a%2520real-world%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Experimental%20Study%20of%20SOTA%20LiDAR%20Segmentation%20Models&entry.906535625=Bike%20Chen%20and%20Antti%20Tikanm%C3%A4ki%20and%20Juha%20R%C3%B6ning&entry.1292438233=%20%20Point%20cloud%20segmentation%20%28PCS%29%20is%20to%20classify%20each%20point%20in%20point%20clouds.%20The%0Atask%20enables%20robots%20to%20parse%20their%203D%20surroundings%20and%20run%20autonomously.%0AAccording%20to%20different%20point%20cloud%20representations%2C%20existing%20PCS%20models%20can%20be%0Aroughly%20divided%20into%20point-%2C%20voxel-%2C%20and%20range%20image-based%20models.%20However%2C%20no%0Awork%20has%20been%20found%20to%20report%20comprehensive%20comparisons%20among%20the%0Astate-of-the-art%20point-%2C%20voxel-%2C%20and%20range%20image-based%20models%20from%20an%0Aapplication%20perspective%2C%20bringing%20difficulty%20in%20utilizing%20these%20models%20for%0Areal-world%20scenarios.%20In%20this%20paper%2C%20we%20provide%20thorough%20comparisons%20among%20the%0Amodels%20by%20considering%20the%20LiDAR%20data%20motion%20compensation%20and%20the%20metrics%20of%0Amodel%20parameters%2C%20max%20GPU%20memory%20allocated%20during%20testing%2C%20inference%20latency%2C%0Aframes%20per%20second%2C%20intersection-over-union%20%28IoU%29%20and%20mean%20IoU%20%28mIoU%29%20scores.%0AThe%20experimental%20results%20benefit%20engineers%20when%20choosing%20a%20reasonable%20PCS%20model%0Afor%20an%20application%20and%20inspire%20researchers%20in%20the%20PCS%20field%20to%20design%20more%0Apractical%20models%20for%20a%20real-world%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12860v1&entry.124074799=Read"},
{"title": "Keep what you need : extracting efficient subnetworks from large audio\n  representation models", "author": "David Genova and Philippe Esling and Tom Hurlin", "abstract": "  Recently, research on audio foundation models has witnessed notable advances,\nas illustrated by the ever improving results on complex downstream tasks.\nSubsequently, those pretrained networks have quickly been used for various\naudio applications. These improvements have however resulted in a considerable\nincrease both in size and complexity of these models. Along the environmental\nconcerns this issue raises, this prevents the deployment of such networks on\nconsumer-level devices, and precludes their use for real-time applications.\nMoreover, this appears contradictory with the specificity of the tasks for\nwhich these models are used, which are often simpler compared to extracting a\nrich, multi-purpose representation from any type of audio data. In this paper,\nwe address this issue with a simple, yet effective method to extract\nlightweight specialist subnetworks from large foundation models. Specifically,\nwe introduce learnable binary masks in-between the layers of a pretrained\nrepresentation model. When training the end-to-end model on a downstream task,\nwe add a sparsity-inducing loss to the overall objective, hence learning a\ncompact subnetwork specialized on a single task. Importantly, the weights of\nthe foundation model are kept frozen, resulting into low additional training\ncosts. Once trained, the masked computational units can then be removed from\nthe network, implying significant performance gains. We assess our method on\nthree widespread audio foundation models, each based on a different backbone\narchitecture, and illustrate its effectiveness on common audio representation\nevaluation tasks, as well as its versatility on both speech, music, and general\naudio. Code for reproducing the results and supporting webpage are available at\nhttps://github.com/gnvIRCAM/Audio-representation-trimming\n", "link": "http://arxiv.org/abs/2502.12925v1", "date": "2025-02-18", "relevancy": 2.6758, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keep%20what%20you%20need%20%3A%20extracting%20efficient%20subnetworks%20from%20large%20audio%0A%20%20representation%20models&body=Title%3A%20Keep%20what%20you%20need%20%3A%20extracting%20efficient%20subnetworks%20from%20large%20audio%0A%20%20representation%20models%0AAuthor%3A%20David%20Genova%20and%20Philippe%20Esling%20and%20Tom%20Hurlin%0AAbstract%3A%20%20%20Recently%2C%20research%20on%20audio%20foundation%20models%20has%20witnessed%20notable%20advances%2C%0Aas%20illustrated%20by%20the%20ever%20improving%20results%20on%20complex%20downstream%20tasks.%0ASubsequently%2C%20those%20pretrained%20networks%20have%20quickly%20been%20used%20for%20various%0Aaudio%20applications.%20These%20improvements%20have%20however%20resulted%20in%20a%20considerable%0Aincrease%20both%20in%20size%20and%20complexity%20of%20these%20models.%20Along%20the%20environmental%0Aconcerns%20this%20issue%20raises%2C%20this%20prevents%20the%20deployment%20of%20such%20networks%20on%0Aconsumer-level%20devices%2C%20and%20precludes%20their%20use%20for%20real-time%20applications.%0AMoreover%2C%20this%20appears%20contradictory%20with%20the%20specificity%20of%20the%20tasks%20for%0Awhich%20these%20models%20are%20used%2C%20which%20are%20often%20simpler%20compared%20to%20extracting%20a%0Arich%2C%20multi-purpose%20representation%20from%20any%20type%20of%20audio%20data.%20In%20this%20paper%2C%0Awe%20address%20this%20issue%20with%20a%20simple%2C%20yet%20effective%20method%20to%20extract%0Alightweight%20specialist%20subnetworks%20from%20large%20foundation%20models.%20Specifically%2C%0Awe%20introduce%20learnable%20binary%20masks%20in-between%20the%20layers%20of%20a%20pretrained%0Arepresentation%20model.%20When%20training%20the%20end-to-end%20model%20on%20a%20downstream%20task%2C%0Awe%20add%20a%20sparsity-inducing%20loss%20to%20the%20overall%20objective%2C%20hence%20learning%20a%0Acompact%20subnetwork%20specialized%20on%20a%20single%20task.%20Importantly%2C%20the%20weights%20of%0Athe%20foundation%20model%20are%20kept%20frozen%2C%20resulting%20into%20low%20additional%20training%0Acosts.%20Once%20trained%2C%20the%20masked%20computational%20units%20can%20then%20be%20removed%20from%0Athe%20network%2C%20implying%20significant%20performance%20gains.%20We%20assess%20our%20method%20on%0Athree%20widespread%20audio%20foundation%20models%2C%20each%20based%20on%20a%20different%20backbone%0Aarchitecture%2C%20and%20illustrate%20its%20effectiveness%20on%20common%20audio%20representation%0Aevaluation%20tasks%2C%20as%20well%20as%20its%20versatility%20on%20both%20speech%2C%20music%2C%20and%20general%0Aaudio.%20Code%20for%20reproducing%20the%20results%20and%20supporting%20webpage%20are%20available%20at%0Ahttps%3A//github.com/gnvIRCAM/Audio-representation-trimming%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeep%2520what%2520you%2520need%2520%253A%2520extracting%2520efficient%2520subnetworks%2520from%2520large%2520audio%250A%2520%2520representation%2520models%26entry.906535625%3DDavid%2520Genova%2520and%2520Philippe%2520Esling%2520and%2520Tom%2520Hurlin%26entry.1292438233%3D%2520%2520Recently%252C%2520research%2520on%2520audio%2520foundation%2520models%2520has%2520witnessed%2520notable%2520advances%252C%250Aas%2520illustrated%2520by%2520the%2520ever%2520improving%2520results%2520on%2520complex%2520downstream%2520tasks.%250ASubsequently%252C%2520those%2520pretrained%2520networks%2520have%2520quickly%2520been%2520used%2520for%2520various%250Aaudio%2520applications.%2520These%2520improvements%2520have%2520however%2520resulted%2520in%2520a%2520considerable%250Aincrease%2520both%2520in%2520size%2520and%2520complexity%2520of%2520these%2520models.%2520Along%2520the%2520environmental%250Aconcerns%2520this%2520issue%2520raises%252C%2520this%2520prevents%2520the%2520deployment%2520of%2520such%2520networks%2520on%250Aconsumer-level%2520devices%252C%2520and%2520precludes%2520their%2520use%2520for%2520real-time%2520applications.%250AMoreover%252C%2520this%2520appears%2520contradictory%2520with%2520the%2520specificity%2520of%2520the%2520tasks%2520for%250Awhich%2520these%2520models%2520are%2520used%252C%2520which%2520are%2520often%2520simpler%2520compared%2520to%2520extracting%2520a%250Arich%252C%2520multi-purpose%2520representation%2520from%2520any%2520type%2520of%2520audio%2520data.%2520In%2520this%2520paper%252C%250Awe%2520address%2520this%2520issue%2520with%2520a%2520simple%252C%2520yet%2520effective%2520method%2520to%2520extract%250Alightweight%2520specialist%2520subnetworks%2520from%2520large%2520foundation%2520models.%2520Specifically%252C%250Awe%2520introduce%2520learnable%2520binary%2520masks%2520in-between%2520the%2520layers%2520of%2520a%2520pretrained%250Arepresentation%2520model.%2520When%2520training%2520the%2520end-to-end%2520model%2520on%2520a%2520downstream%2520task%252C%250Awe%2520add%2520a%2520sparsity-inducing%2520loss%2520to%2520the%2520overall%2520objective%252C%2520hence%2520learning%2520a%250Acompact%2520subnetwork%2520specialized%2520on%2520a%2520single%2520task.%2520Importantly%252C%2520the%2520weights%2520of%250Athe%2520foundation%2520model%2520are%2520kept%2520frozen%252C%2520resulting%2520into%2520low%2520additional%2520training%250Acosts.%2520Once%2520trained%252C%2520the%2520masked%2520computational%2520units%2520can%2520then%2520be%2520removed%2520from%250Athe%2520network%252C%2520implying%2520significant%2520performance%2520gains.%2520We%2520assess%2520our%2520method%2520on%250Athree%2520widespread%2520audio%2520foundation%2520models%252C%2520each%2520based%2520on%2520a%2520different%2520backbone%250Aarchitecture%252C%2520and%2520illustrate%2520its%2520effectiveness%2520on%2520common%2520audio%2520representation%250Aevaluation%2520tasks%252C%2520as%2520well%2520as%2520its%2520versatility%2520on%2520both%2520speech%252C%2520music%252C%2520and%2520general%250Aaudio.%2520Code%2520for%2520reproducing%2520the%2520results%2520and%2520supporting%2520webpage%2520are%2520available%2520at%250Ahttps%253A//github.com/gnvIRCAM/Audio-representation-trimming%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keep%20what%20you%20need%20%3A%20extracting%20efficient%20subnetworks%20from%20large%20audio%0A%20%20representation%20models&entry.906535625=David%20Genova%20and%20Philippe%20Esling%20and%20Tom%20Hurlin&entry.1292438233=%20%20Recently%2C%20research%20on%20audio%20foundation%20models%20has%20witnessed%20notable%20advances%2C%0Aas%20illustrated%20by%20the%20ever%20improving%20results%20on%20complex%20downstream%20tasks.%0ASubsequently%2C%20those%20pretrained%20networks%20have%20quickly%20been%20used%20for%20various%0Aaudio%20applications.%20These%20improvements%20have%20however%20resulted%20in%20a%20considerable%0Aincrease%20both%20in%20size%20and%20complexity%20of%20these%20models.%20Along%20the%20environmental%0Aconcerns%20this%20issue%20raises%2C%20this%20prevents%20the%20deployment%20of%20such%20networks%20on%0Aconsumer-level%20devices%2C%20and%20precludes%20their%20use%20for%20real-time%20applications.%0AMoreover%2C%20this%20appears%20contradictory%20with%20the%20specificity%20of%20the%20tasks%20for%0Awhich%20these%20models%20are%20used%2C%20which%20are%20often%20simpler%20compared%20to%20extracting%20a%0Arich%2C%20multi-purpose%20representation%20from%20any%20type%20of%20audio%20data.%20In%20this%20paper%2C%0Awe%20address%20this%20issue%20with%20a%20simple%2C%20yet%20effective%20method%20to%20extract%0Alightweight%20specialist%20subnetworks%20from%20large%20foundation%20models.%20Specifically%2C%0Awe%20introduce%20learnable%20binary%20masks%20in-between%20the%20layers%20of%20a%20pretrained%0Arepresentation%20model.%20When%20training%20the%20end-to-end%20model%20on%20a%20downstream%20task%2C%0Awe%20add%20a%20sparsity-inducing%20loss%20to%20the%20overall%20objective%2C%20hence%20learning%20a%0Acompact%20subnetwork%20specialized%20on%20a%20single%20task.%20Importantly%2C%20the%20weights%20of%0Athe%20foundation%20model%20are%20kept%20frozen%2C%20resulting%20into%20low%20additional%20training%0Acosts.%20Once%20trained%2C%20the%20masked%20computational%20units%20can%20then%20be%20removed%20from%0Athe%20network%2C%20implying%20significant%20performance%20gains.%20We%20assess%20our%20method%20on%0Athree%20widespread%20audio%20foundation%20models%2C%20each%20based%20on%20a%20different%20backbone%0Aarchitecture%2C%20and%20illustrate%20its%20effectiveness%20on%20common%20audio%20representation%0Aevaluation%20tasks%2C%20as%20well%20as%20its%20versatility%20on%20both%20speech%2C%20music%2C%20and%20general%0Aaudio.%20Code%20for%20reproducing%20the%20results%20and%20supporting%20webpage%20are%20available%20at%0Ahttps%3A//github.com/gnvIRCAM/Audio-representation-trimming%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12925v1&entry.124074799=Read"},
{"title": "LieRE: Generalizing Rotary Position Encodings", "author": "Sophie Ostmeier and Brian Axelrod and Michael E. Moseley and Akshay Chaudhari and Curtis Langlotz", "abstract": "  Transformer architectures rely on position encodings to capture token\ndependencies. Rotary Position Encoding (RoPE) has emerged as a popular choice\nin language models due to its efficient encoding of relative position\ninformation through key-query rotations. However, RoPE faces significant\nlimitations beyond language processing: it is constrained to one-dimensional\nsequence data and, even with learnable phases, offers limited representational\ncapacity. We address these challenges with Lie Relative Encodings (LieRE),\nwhich replaces RoPE's block-2D rotation matrix with a learned, dense,\nhigh-dimensional rotation matrix of variable sparsity. Through extensive\nevaluation on three image datasets across 2D and 3D classification tasks, LieRE\nachieves 2\\% relative improvement over state-of-the-art baselines on 2D tasks\nand 1.5\\% on 3D tasks, while demonstrating superior generalization to higher\nresolutions. Our implementation is computationally efficient, with results\nreproducible on 4 A100 GPUs in 30 minutes on CIFAR100, and we release our code\nto facilitate further research.\n", "link": "http://arxiv.org/abs/2406.10322v3", "date": "2025-02-18", "relevancy": 2.6401, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LieRE%3A%20Generalizing%20Rotary%20Position%20Encodings&body=Title%3A%20LieRE%3A%20Generalizing%20Rotary%20Position%20Encodings%0AAuthor%3A%20Sophie%20Ostmeier%20and%20Brian%20Axelrod%20and%20Michael%20E.%20Moseley%20and%20Akshay%20Chaudhari%20and%20Curtis%20Langlotz%0AAbstract%3A%20%20%20Transformer%20architectures%20rely%20on%20position%20encodings%20to%20capture%20token%0Adependencies.%20Rotary%20Position%20Encoding%20%28RoPE%29%20has%20emerged%20as%20a%20popular%20choice%0Ain%20language%20models%20due%20to%20its%20efficient%20encoding%20of%20relative%20position%0Ainformation%20through%20key-query%20rotations.%20However%2C%20RoPE%20faces%20significant%0Alimitations%20beyond%20language%20processing%3A%20it%20is%20constrained%20to%20one-dimensional%0Asequence%20data%20and%2C%20even%20with%20learnable%20phases%2C%20offers%20limited%20representational%0Acapacity.%20We%20address%20these%20challenges%20with%20Lie%20Relative%20Encodings%20%28LieRE%29%2C%0Awhich%20replaces%20RoPE%27s%20block-2D%20rotation%20matrix%20with%20a%20learned%2C%20dense%2C%0Ahigh-dimensional%20rotation%20matrix%20of%20variable%20sparsity.%20Through%20extensive%0Aevaluation%20on%20three%20image%20datasets%20across%202D%20and%203D%20classification%20tasks%2C%20LieRE%0Aachieves%202%5C%25%20relative%20improvement%20over%20state-of-the-art%20baselines%20on%202D%20tasks%0Aand%201.5%5C%25%20on%203D%20tasks%2C%20while%20demonstrating%20superior%20generalization%20to%20higher%0Aresolutions.%20Our%20implementation%20is%20computationally%20efficient%2C%20with%20results%0Areproducible%20on%204%20A100%20GPUs%20in%2030%20minutes%20on%20CIFAR100%2C%20and%20we%20release%20our%20code%0Ato%20facilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10322v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLieRE%253A%2520Generalizing%2520Rotary%2520Position%2520Encodings%26entry.906535625%3DSophie%2520Ostmeier%2520and%2520Brian%2520Axelrod%2520and%2520Michael%2520E.%2520Moseley%2520and%2520Akshay%2520Chaudhari%2520and%2520Curtis%2520Langlotz%26entry.1292438233%3D%2520%2520Transformer%2520architectures%2520rely%2520on%2520position%2520encodings%2520to%2520capture%2520token%250Adependencies.%2520Rotary%2520Position%2520Encoding%2520%2528RoPE%2529%2520has%2520emerged%2520as%2520a%2520popular%2520choice%250Ain%2520language%2520models%2520due%2520to%2520its%2520efficient%2520encoding%2520of%2520relative%2520position%250Ainformation%2520through%2520key-query%2520rotations.%2520However%252C%2520RoPE%2520faces%2520significant%250Alimitations%2520beyond%2520language%2520processing%253A%2520it%2520is%2520constrained%2520to%2520one-dimensional%250Asequence%2520data%2520and%252C%2520even%2520with%2520learnable%2520phases%252C%2520offers%2520limited%2520representational%250Acapacity.%2520We%2520address%2520these%2520challenges%2520with%2520Lie%2520Relative%2520Encodings%2520%2528LieRE%2529%252C%250Awhich%2520replaces%2520RoPE%2527s%2520block-2D%2520rotation%2520matrix%2520with%2520a%2520learned%252C%2520dense%252C%250Ahigh-dimensional%2520rotation%2520matrix%2520of%2520variable%2520sparsity.%2520Through%2520extensive%250Aevaluation%2520on%2520three%2520image%2520datasets%2520across%25202D%2520and%25203D%2520classification%2520tasks%252C%2520LieRE%250Aachieves%25202%255C%2525%2520relative%2520improvement%2520over%2520state-of-the-art%2520baselines%2520on%25202D%2520tasks%250Aand%25201.5%255C%2525%2520on%25203D%2520tasks%252C%2520while%2520demonstrating%2520superior%2520generalization%2520to%2520higher%250Aresolutions.%2520Our%2520implementation%2520is%2520computationally%2520efficient%252C%2520with%2520results%250Areproducible%2520on%25204%2520A100%2520GPUs%2520in%252030%2520minutes%2520on%2520CIFAR100%252C%2520and%2520we%2520release%2520our%2520code%250Ato%2520facilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10322v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LieRE%3A%20Generalizing%20Rotary%20Position%20Encodings&entry.906535625=Sophie%20Ostmeier%20and%20Brian%20Axelrod%20and%20Michael%20E.%20Moseley%20and%20Akshay%20Chaudhari%20and%20Curtis%20Langlotz&entry.1292438233=%20%20Transformer%20architectures%20rely%20on%20position%20encodings%20to%20capture%20token%0Adependencies.%20Rotary%20Position%20Encoding%20%28RoPE%29%20has%20emerged%20as%20a%20popular%20choice%0Ain%20language%20models%20due%20to%20its%20efficient%20encoding%20of%20relative%20position%0Ainformation%20through%20key-query%20rotations.%20However%2C%20RoPE%20faces%20significant%0Alimitations%20beyond%20language%20processing%3A%20it%20is%20constrained%20to%20one-dimensional%0Asequence%20data%20and%2C%20even%20with%20learnable%20phases%2C%20offers%20limited%20representational%0Acapacity.%20We%20address%20these%20challenges%20with%20Lie%20Relative%20Encodings%20%28LieRE%29%2C%0Awhich%20replaces%20RoPE%27s%20block-2D%20rotation%20matrix%20with%20a%20learned%2C%20dense%2C%0Ahigh-dimensional%20rotation%20matrix%20of%20variable%20sparsity.%20Through%20extensive%0Aevaluation%20on%20three%20image%20datasets%20across%202D%20and%203D%20classification%20tasks%2C%20LieRE%0Aachieves%202%5C%25%20relative%20improvement%20over%20state-of-the-art%20baselines%20on%202D%20tasks%0Aand%201.5%5C%25%20on%203D%20tasks%2C%20while%20demonstrating%20superior%20generalization%20to%20higher%0Aresolutions.%20Our%20implementation%20is%20computationally%20efficient%2C%20with%20results%0Areproducible%20on%204%20A100%20GPUs%20in%2030%20minutes%20on%20CIFAR100%2C%20and%20we%20release%20our%20code%0Ato%20facilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10322v3&entry.124074799=Read"},
{"title": "MLPs at the EOC: Dynamics of Feature Learning", "author": "D\u00e1vid Terj\u00e9k", "abstract": "  Since infinitely wide neural networks in the kernel regime are random feature\nmodels, the success of contemporary deep learning lies in the rich regime,\nwhere a satisfying theory should explain not only the convergence of gradient\ndescent but the learning of features along the way. Such a theory should also\ncover phenomena observed by practicioners including the Edge of Stability (EOS)\nand the catapult mechanism. For a practically relevant theory in the limit,\nneural network parameterizations have to efficiently reproduce limiting\nbehavior as width and depth are scaled up. While widthwise scaling is mostly\nsettled, depthwise scaling is solved only at initialization by the Edge of\nChaos (EOC). During training, scaling up depth is either done by inversely\nscaling the learning rate or adding residual connections. We propose $(1)$ the\nNormalized Update Parameterization ($\\nu$P) to solve this issue by growing\nhidden layer sizes depthwise inducing the regularized evolution of\npreactivations, $(2)$ a hypothetical explanation for feature learning via the\ncosine of new and cumulative parameter updates and $(3)$ a geometry-aware\nlearning rate schedule that is able to prolong the catapult phase indefinitely.\nWe support our hypotheses and demonstrate the usefulness of $\\nu$P and the\nlearning rate schedule by empirical evidence.\n", "link": "http://arxiv.org/abs/2502.13110v1", "date": "2025-02-18", "relevancy": 2.6264, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLPs%20at%20the%20EOC%3A%20Dynamics%20of%20Feature%20Learning&body=Title%3A%20MLPs%20at%20the%20EOC%3A%20Dynamics%20of%20Feature%20Learning%0AAuthor%3A%20D%C3%A1vid%20Terj%C3%A9k%0AAbstract%3A%20%20%20Since%20infinitely%20wide%20neural%20networks%20in%20the%20kernel%20regime%20are%20random%20feature%0Amodels%2C%20the%20success%20of%20contemporary%20deep%20learning%20lies%20in%20the%20rich%20regime%2C%0Awhere%20a%20satisfying%20theory%20should%20explain%20not%20only%20the%20convergence%20of%20gradient%0Adescent%20but%20the%20learning%20of%20features%20along%20the%20way.%20Such%20a%20theory%20should%20also%0Acover%20phenomena%20observed%20by%20practicioners%20including%20the%20Edge%20of%20Stability%20%28EOS%29%0Aand%20the%20catapult%20mechanism.%20For%20a%20practically%20relevant%20theory%20in%20the%20limit%2C%0Aneural%20network%20parameterizations%20have%20to%20efficiently%20reproduce%20limiting%0Abehavior%20as%20width%20and%20depth%20are%20scaled%20up.%20While%20widthwise%20scaling%20is%20mostly%0Asettled%2C%20depthwise%20scaling%20is%20solved%20only%20at%20initialization%20by%20the%20Edge%20of%0AChaos%20%28EOC%29.%20During%20training%2C%20scaling%20up%20depth%20is%20either%20done%20by%20inversely%0Ascaling%20the%20learning%20rate%20or%20adding%20residual%20connections.%20We%20propose%20%24%281%29%24%20the%0ANormalized%20Update%20Parameterization%20%28%24%5Cnu%24P%29%20to%20solve%20this%20issue%20by%20growing%0Ahidden%20layer%20sizes%20depthwise%20inducing%20the%20regularized%20evolution%20of%0Apreactivations%2C%20%24%282%29%24%20a%20hypothetical%20explanation%20for%20feature%20learning%20via%20the%0Acosine%20of%20new%20and%20cumulative%20parameter%20updates%20and%20%24%283%29%24%20a%20geometry-aware%0Alearning%20rate%20schedule%20that%20is%20able%20to%20prolong%20the%20catapult%20phase%20indefinitely.%0AWe%20support%20our%20hypotheses%20and%20demonstrate%20the%20usefulness%20of%20%24%5Cnu%24P%20and%20the%0Alearning%20rate%20schedule%20by%20empirical%20evidence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLPs%2520at%2520the%2520EOC%253A%2520Dynamics%2520of%2520Feature%2520Learning%26entry.906535625%3DD%25C3%25A1vid%2520Terj%25C3%25A9k%26entry.1292438233%3D%2520%2520Since%2520infinitely%2520wide%2520neural%2520networks%2520in%2520the%2520kernel%2520regime%2520are%2520random%2520feature%250Amodels%252C%2520the%2520success%2520of%2520contemporary%2520deep%2520learning%2520lies%2520in%2520the%2520rich%2520regime%252C%250Awhere%2520a%2520satisfying%2520theory%2520should%2520explain%2520not%2520only%2520the%2520convergence%2520of%2520gradient%250Adescent%2520but%2520the%2520learning%2520of%2520features%2520along%2520the%2520way.%2520Such%2520a%2520theory%2520should%2520also%250Acover%2520phenomena%2520observed%2520by%2520practicioners%2520including%2520the%2520Edge%2520of%2520Stability%2520%2528EOS%2529%250Aand%2520the%2520catapult%2520mechanism.%2520For%2520a%2520practically%2520relevant%2520theory%2520in%2520the%2520limit%252C%250Aneural%2520network%2520parameterizations%2520have%2520to%2520efficiently%2520reproduce%2520limiting%250Abehavior%2520as%2520width%2520and%2520depth%2520are%2520scaled%2520up.%2520While%2520widthwise%2520scaling%2520is%2520mostly%250Asettled%252C%2520depthwise%2520scaling%2520is%2520solved%2520only%2520at%2520initialization%2520by%2520the%2520Edge%2520of%250AChaos%2520%2528EOC%2529.%2520During%2520training%252C%2520scaling%2520up%2520depth%2520is%2520either%2520done%2520by%2520inversely%250Ascaling%2520the%2520learning%2520rate%2520or%2520adding%2520residual%2520connections.%2520We%2520propose%2520%2524%25281%2529%2524%2520the%250ANormalized%2520Update%2520Parameterization%2520%2528%2524%255Cnu%2524P%2529%2520to%2520solve%2520this%2520issue%2520by%2520growing%250Ahidden%2520layer%2520sizes%2520depthwise%2520inducing%2520the%2520regularized%2520evolution%2520of%250Apreactivations%252C%2520%2524%25282%2529%2524%2520a%2520hypothetical%2520explanation%2520for%2520feature%2520learning%2520via%2520the%250Acosine%2520of%2520new%2520and%2520cumulative%2520parameter%2520updates%2520and%2520%2524%25283%2529%2524%2520a%2520geometry-aware%250Alearning%2520rate%2520schedule%2520that%2520is%2520able%2520to%2520prolong%2520the%2520catapult%2520phase%2520indefinitely.%250AWe%2520support%2520our%2520hypotheses%2520and%2520demonstrate%2520the%2520usefulness%2520of%2520%2524%255Cnu%2524P%2520and%2520the%250Alearning%2520rate%2520schedule%2520by%2520empirical%2520evidence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLPs%20at%20the%20EOC%3A%20Dynamics%20of%20Feature%20Learning&entry.906535625=D%C3%A1vid%20Terj%C3%A9k&entry.1292438233=%20%20Since%20infinitely%20wide%20neural%20networks%20in%20the%20kernel%20regime%20are%20random%20feature%0Amodels%2C%20the%20success%20of%20contemporary%20deep%20learning%20lies%20in%20the%20rich%20regime%2C%0Awhere%20a%20satisfying%20theory%20should%20explain%20not%20only%20the%20convergence%20of%20gradient%0Adescent%20but%20the%20learning%20of%20features%20along%20the%20way.%20Such%20a%20theory%20should%20also%0Acover%20phenomena%20observed%20by%20practicioners%20including%20the%20Edge%20of%20Stability%20%28EOS%29%0Aand%20the%20catapult%20mechanism.%20For%20a%20practically%20relevant%20theory%20in%20the%20limit%2C%0Aneural%20network%20parameterizations%20have%20to%20efficiently%20reproduce%20limiting%0Abehavior%20as%20width%20and%20depth%20are%20scaled%20up.%20While%20widthwise%20scaling%20is%20mostly%0Asettled%2C%20depthwise%20scaling%20is%20solved%20only%20at%20initialization%20by%20the%20Edge%20of%0AChaos%20%28EOC%29.%20During%20training%2C%20scaling%20up%20depth%20is%20either%20done%20by%20inversely%0Ascaling%20the%20learning%20rate%20or%20adding%20residual%20connections.%20We%20propose%20%24%281%29%24%20the%0ANormalized%20Update%20Parameterization%20%28%24%5Cnu%24P%29%20to%20solve%20this%20issue%20by%20growing%0Ahidden%20layer%20sizes%20depthwise%20inducing%20the%20regularized%20evolution%20of%0Apreactivations%2C%20%24%282%29%24%20a%20hypothetical%20explanation%20for%20feature%20learning%20via%20the%0Acosine%20of%20new%20and%20cumulative%20parameter%20updates%20and%20%24%283%29%24%20a%20geometry-aware%0Alearning%20rate%20schedule%20that%20is%20able%20to%20prolong%20the%20catapult%20phase%20indefinitely.%0AWe%20support%20our%20hypotheses%20and%20demonstrate%20the%20usefulness%20of%20%24%5Cnu%24P%20and%20the%0Alearning%20rate%20schedule%20by%20empirical%20evidence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13110v1&entry.124074799=Read"},
{"title": "Task-Informed Anti-Curriculum by Masking Improves Downstream Performance\n  on Text", "author": "Andrei Jarca and Florinel Alin Croitoru and Radu Tudor Ionescu", "abstract": "  Masked language modeling has become a widely adopted unsupervised technique\nto pre-train language models. However, the process of selecting tokens for\nmasking is random, and the percentage of masked tokens is typically fixed for\nthe entire training process. In this paper, we propose to adjust the masking\nratio and to decide which tokens to mask based on a novel task-informed\nanti-curriculum learning scheme. First, we harness task-specific knowledge\nabout useful and harmful tokens in order to determine which tokens to mask.\nSecond, we propose a cyclic decaying masking ratio, which corresponds to an\nanti-curriculum schedule (from hard to easy). We exemplify our novel\ntask-informed anti-curriculum by masking (TIACBM) approach across three diverse\ndownstream tasks: sentiment analysis, text classification by topic, and\nauthorship attribution. Our findings suggest that TIACBM enhances the ability\nof the model to focus on key task-relevant features, contributing to\nstatistically significant performance gains across tasks. We release our code\nat https://github.com/JarcaAndrei/TIACBM.\n", "link": "http://arxiv.org/abs/2502.12953v1", "date": "2025-02-18", "relevancy": 2.623, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5251}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Informed%20Anti-Curriculum%20by%20Masking%20Improves%20Downstream%20Performance%0A%20%20on%20Text&body=Title%3A%20Task-Informed%20Anti-Curriculum%20by%20Masking%20Improves%20Downstream%20Performance%0A%20%20on%20Text%0AAuthor%3A%20Andrei%20Jarca%20and%20Florinel%20Alin%20Croitoru%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20%20%20Masked%20language%20modeling%20has%20become%20a%20widely%20adopted%20unsupervised%20technique%0Ato%20pre-train%20language%20models.%20However%2C%20the%20process%20of%20selecting%20tokens%20for%0Amasking%20is%20random%2C%20and%20the%20percentage%20of%20masked%20tokens%20is%20typically%20fixed%20for%0Athe%20entire%20training%20process.%20In%20this%20paper%2C%20we%20propose%20to%20adjust%20the%20masking%0Aratio%20and%20to%20decide%20which%20tokens%20to%20mask%20based%20on%20a%20novel%20task-informed%0Aanti-curriculum%20learning%20scheme.%20First%2C%20we%20harness%20task-specific%20knowledge%0Aabout%20useful%20and%20harmful%20tokens%20in%20order%20to%20determine%20which%20tokens%20to%20mask.%0ASecond%2C%20we%20propose%20a%20cyclic%20decaying%20masking%20ratio%2C%20which%20corresponds%20to%20an%0Aanti-curriculum%20schedule%20%28from%20hard%20to%20easy%29.%20We%20exemplify%20our%20novel%0Atask-informed%20anti-curriculum%20by%20masking%20%28TIACBM%29%20approach%20across%20three%20diverse%0Adownstream%20tasks%3A%20sentiment%20analysis%2C%20text%20classification%20by%20topic%2C%20and%0Aauthorship%20attribution.%20Our%20findings%20suggest%20that%20TIACBM%20enhances%20the%20ability%0Aof%20the%20model%20to%20focus%20on%20key%20task-relevant%20features%2C%20contributing%20to%0Astatistically%20significant%20performance%20gains%20across%20tasks.%20We%20release%20our%20code%0Aat%20https%3A//github.com/JarcaAndrei/TIACBM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Informed%2520Anti-Curriculum%2520by%2520Masking%2520Improves%2520Downstream%2520Performance%250A%2520%2520on%2520Text%26entry.906535625%3DAndrei%2520Jarca%2520and%2520Florinel%2520Alin%2520Croitoru%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3D%2520%2520Masked%2520language%2520modeling%2520has%2520become%2520a%2520widely%2520adopted%2520unsupervised%2520technique%250Ato%2520pre-train%2520language%2520models.%2520However%252C%2520the%2520process%2520of%2520selecting%2520tokens%2520for%250Amasking%2520is%2520random%252C%2520and%2520the%2520percentage%2520of%2520masked%2520tokens%2520is%2520typically%2520fixed%2520for%250Athe%2520entire%2520training%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520adjust%2520the%2520masking%250Aratio%2520and%2520to%2520decide%2520which%2520tokens%2520to%2520mask%2520based%2520on%2520a%2520novel%2520task-informed%250Aanti-curriculum%2520learning%2520scheme.%2520First%252C%2520we%2520harness%2520task-specific%2520knowledge%250Aabout%2520useful%2520and%2520harmful%2520tokens%2520in%2520order%2520to%2520determine%2520which%2520tokens%2520to%2520mask.%250ASecond%252C%2520we%2520propose%2520a%2520cyclic%2520decaying%2520masking%2520ratio%252C%2520which%2520corresponds%2520to%2520an%250Aanti-curriculum%2520schedule%2520%2528from%2520hard%2520to%2520easy%2529.%2520We%2520exemplify%2520our%2520novel%250Atask-informed%2520anti-curriculum%2520by%2520masking%2520%2528TIACBM%2529%2520approach%2520across%2520three%2520diverse%250Adownstream%2520tasks%253A%2520sentiment%2520analysis%252C%2520text%2520classification%2520by%2520topic%252C%2520and%250Aauthorship%2520attribution.%2520Our%2520findings%2520suggest%2520that%2520TIACBM%2520enhances%2520the%2520ability%250Aof%2520the%2520model%2520to%2520focus%2520on%2520key%2520task-relevant%2520features%252C%2520contributing%2520to%250Astatistically%2520significant%2520performance%2520gains%2520across%2520tasks.%2520We%2520release%2520our%2520code%250Aat%2520https%253A//github.com/JarcaAndrei/TIACBM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Informed%20Anti-Curriculum%20by%20Masking%20Improves%20Downstream%20Performance%0A%20%20on%20Text&entry.906535625=Andrei%20Jarca%20and%20Florinel%20Alin%20Croitoru%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=%20%20Masked%20language%20modeling%20has%20become%20a%20widely%20adopted%20unsupervised%20technique%0Ato%20pre-train%20language%20models.%20However%2C%20the%20process%20of%20selecting%20tokens%20for%0Amasking%20is%20random%2C%20and%20the%20percentage%20of%20masked%20tokens%20is%20typically%20fixed%20for%0Athe%20entire%20training%20process.%20In%20this%20paper%2C%20we%20propose%20to%20adjust%20the%20masking%0Aratio%20and%20to%20decide%20which%20tokens%20to%20mask%20based%20on%20a%20novel%20task-informed%0Aanti-curriculum%20learning%20scheme.%20First%2C%20we%20harness%20task-specific%20knowledge%0Aabout%20useful%20and%20harmful%20tokens%20in%20order%20to%20determine%20which%20tokens%20to%20mask.%0ASecond%2C%20we%20propose%20a%20cyclic%20decaying%20masking%20ratio%2C%20which%20corresponds%20to%20an%0Aanti-curriculum%20schedule%20%28from%20hard%20to%20easy%29.%20We%20exemplify%20our%20novel%0Atask-informed%20anti-curriculum%20by%20masking%20%28TIACBM%29%20approach%20across%20three%20diverse%0Adownstream%20tasks%3A%20sentiment%20analysis%2C%20text%20classification%20by%20topic%2C%20and%0Aauthorship%20attribution.%20Our%20findings%20suggest%20that%20TIACBM%20enhances%20the%20ability%0Aof%20the%20model%20to%20focus%20on%20key%20task-relevant%20features%2C%20contributing%20to%0Astatistically%20significant%20performance%20gains%20across%20tasks.%20We%20release%20our%20code%0Aat%20https%3A//github.com/JarcaAndrei/TIACBM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12953v1&entry.124074799=Read"},
{"title": "Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words", "author": "Gouki Minegishi and Hiroki Furuta and Yusuke Iwasawa and Yutaka Matsuo", "abstract": "  Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs.\n", "link": "http://arxiv.org/abs/2501.06254v2", "date": "2025-02-18", "relevancy": 2.5961, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Evaluation%20of%20Sparse%20Autoencoders%20through%20the%20Representation%0A%20%20of%20Polysemous%20Words&body=Title%3A%20Rethinking%20Evaluation%20of%20Sparse%20Autoencoders%20through%20the%20Representation%0A%20%20of%20Polysemous%20Words%0AAuthor%3A%20Gouki%20Minegishi%20and%20Hiroki%20Furuta%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20gained%20a%20lot%20of%20attention%20as%20a%20promising%20tool%0Ato%20improve%20the%20interpretability%20of%20large%20language%20models%20%28LLMs%29%20by%20mapping%20the%0Acomplex%20superposition%20of%20polysemantic%20neurons%20into%20monosemantic%20features%20and%0Acomposing%20a%20sparse%20dictionary%20of%20words.%20However%2C%20traditional%20performance%0Ametrics%20like%20Mean%20Squared%20Error%20and%20L0%20sparsity%20ignore%20the%20evaluation%20of%20the%0Asemantic%20representational%20power%20of%20SAEs%20--%20whether%20they%20can%20acquire%0Ainterpretable%20monosemantic%20features%20while%20preserving%20the%20semantic%20relationship%0Aof%20words.%20For%20instance%2C%20it%20is%20not%20obvious%20whether%20a%20learned%20sparse%20feature%0Acould%20distinguish%20different%20meanings%20in%20one%20word.%20In%20this%20paper%2C%20we%20propose%20a%0Asuite%20of%20evaluations%20for%20SAEs%20to%20analyze%20the%20quality%20of%20monosemantic%20features%0Aby%20focusing%20on%20polysemous%20words.%20Our%20findings%20reveal%20that%20SAEs%20developed%20to%0Aimprove%20the%20MSE-L0%20Pareto%20frontier%20may%20confuse%20interpretability%2C%20which%20does%20not%0Anecessarily%20enhance%20the%20extraction%20of%20monosemantic%20features.%20The%20analysis%20of%0ASAEs%20with%20polysemous%20words%20can%20also%20figure%20out%20the%20internal%20mechanism%20of%20LLMs%3B%0Adeeper%20layers%20and%20the%20Attention%20module%20contribute%20to%20distinguishing%20polysemy%20in%0Aa%20word.%20Our%20semantics%20focused%20evaluation%20offers%20new%20insights%20into%20the%20polysemy%0Aand%20the%20existing%20SAE%20objective%20and%20contributes%20to%20the%20development%20of%20more%0Apractical%20SAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Evaluation%2520of%2520Sparse%2520Autoencoders%2520through%2520the%2520Representation%250A%2520%2520of%2520Polysemous%2520Words%26entry.906535625%3DGouki%2520Minegishi%2520and%2520Hiroki%2520Furuta%2520and%2520Yusuke%2520Iwasawa%2520and%2520Yutaka%2520Matsuo%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520gained%2520a%2520lot%2520of%2520attention%2520as%2520a%2520promising%2520tool%250Ato%2520improve%2520the%2520interpretability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520mapping%2520the%250Acomplex%2520superposition%2520of%2520polysemantic%2520neurons%2520into%2520monosemantic%2520features%2520and%250Acomposing%2520a%2520sparse%2520dictionary%2520of%2520words.%2520However%252C%2520traditional%2520performance%250Ametrics%2520like%2520Mean%2520Squared%2520Error%2520and%2520L0%2520sparsity%2520ignore%2520the%2520evaluation%2520of%2520the%250Asemantic%2520representational%2520power%2520of%2520SAEs%2520--%2520whether%2520they%2520can%2520acquire%250Ainterpretable%2520monosemantic%2520features%2520while%2520preserving%2520the%2520semantic%2520relationship%250Aof%2520words.%2520For%2520instance%252C%2520it%2520is%2520not%2520obvious%2520whether%2520a%2520learned%2520sparse%2520feature%250Acould%2520distinguish%2520different%2520meanings%2520in%2520one%2520word.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Asuite%2520of%2520evaluations%2520for%2520SAEs%2520to%2520analyze%2520the%2520quality%2520of%2520monosemantic%2520features%250Aby%2520focusing%2520on%2520polysemous%2520words.%2520Our%2520findings%2520reveal%2520that%2520SAEs%2520developed%2520to%250Aimprove%2520the%2520MSE-L0%2520Pareto%2520frontier%2520may%2520confuse%2520interpretability%252C%2520which%2520does%2520not%250Anecessarily%2520enhance%2520the%2520extraction%2520of%2520monosemantic%2520features.%2520The%2520analysis%2520of%250ASAEs%2520with%2520polysemous%2520words%2520can%2520also%2520figure%2520out%2520the%2520internal%2520mechanism%2520of%2520LLMs%253B%250Adeeper%2520layers%2520and%2520the%2520Attention%2520module%2520contribute%2520to%2520distinguishing%2520polysemy%2520in%250Aa%2520word.%2520Our%2520semantics%2520focused%2520evaluation%2520offers%2520new%2520insights%2520into%2520the%2520polysemy%250Aand%2520the%2520existing%2520SAE%2520objective%2520and%2520contributes%2520to%2520the%2520development%2520of%2520more%250Apractical%2520SAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Evaluation%20of%20Sparse%20Autoencoders%20through%20the%20Representation%0A%20%20of%20Polysemous%20Words&entry.906535625=Gouki%20Minegishi%20and%20Hiroki%20Furuta%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20gained%20a%20lot%20of%20attention%20as%20a%20promising%20tool%0Ato%20improve%20the%20interpretability%20of%20large%20language%20models%20%28LLMs%29%20by%20mapping%20the%0Acomplex%20superposition%20of%20polysemantic%20neurons%20into%20monosemantic%20features%20and%0Acomposing%20a%20sparse%20dictionary%20of%20words.%20However%2C%20traditional%20performance%0Ametrics%20like%20Mean%20Squared%20Error%20and%20L0%20sparsity%20ignore%20the%20evaluation%20of%20the%0Asemantic%20representational%20power%20of%20SAEs%20--%20whether%20they%20can%20acquire%0Ainterpretable%20monosemantic%20features%20while%20preserving%20the%20semantic%20relationship%0Aof%20words.%20For%20instance%2C%20it%20is%20not%20obvious%20whether%20a%20learned%20sparse%20feature%0Acould%20distinguish%20different%20meanings%20in%20one%20word.%20In%20this%20paper%2C%20we%20propose%20a%0Asuite%20of%20evaluations%20for%20SAEs%20to%20analyze%20the%20quality%20of%20monosemantic%20features%0Aby%20focusing%20on%20polysemous%20words.%20Our%20findings%20reveal%20that%20SAEs%20developed%20to%0Aimprove%20the%20MSE-L0%20Pareto%20frontier%20may%20confuse%20interpretability%2C%20which%20does%20not%0Anecessarily%20enhance%20the%20extraction%20of%20monosemantic%20features.%20The%20analysis%20of%0ASAEs%20with%20polysemous%20words%20can%20also%20figure%20out%20the%20internal%20mechanism%20of%20LLMs%3B%0Adeeper%20layers%20and%20the%20Attention%20module%20contribute%20to%20distinguishing%20polysemy%20in%0Aa%20word.%20Our%20semantics%20focused%20evaluation%20offers%20new%20insights%20into%20the%20polysemy%0Aand%20the%20existing%20SAE%20objective%20and%20contributes%20to%20the%20development%20of%20more%0Apractical%20SAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06254v2&entry.124074799=Read"},
{"title": "HyGEN: Regularizing Negative Hyperedge Generation for Accurate Hyperedge\n  Prediction", "author": "Song Kyung Yu and Da Eun Lee and Yunyong Ko and Sang-Wook Kim", "abstract": "  Hyperedge prediction is a fundamental task to predict future high-order\nrelations based on the observed network structure. Existing hyperedge\nprediction methods, however, suffer from the data sparsity problem. To\nalleviate this problem, negative sampling methods can be used, which leverage\nnon-existing hyperedges as contrastive information for model training. However,\nthe following important challenges have been rarely studied: (C1) lack of\nguidance for generating negatives and (C2) possibility of producing false\nnegatives. To address them, we propose a novel hyperedge prediction method,\nHyGEN, that employs (1) a negative hyperedge generator that employs positive\nhyperedges as a guidance to generate more realistic ones and (2) a\nregularization term that prevents the generated hyperedges from being false\nnegatives. Extensive experiments on six real-world hypergraphs reveal that\nHyGEN consistently outperforms four state-of-the-art hyperedge prediction\nmethods.\n", "link": "http://arxiv.org/abs/2502.05827v2", "date": "2025-02-18", "relevancy": 2.595, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5258}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5234}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyGEN%3A%20Regularizing%20Negative%20Hyperedge%20Generation%20for%20Accurate%20Hyperedge%0A%20%20Prediction&body=Title%3A%20HyGEN%3A%20Regularizing%20Negative%20Hyperedge%20Generation%20for%20Accurate%20Hyperedge%0A%20%20Prediction%0AAuthor%3A%20Song%20Kyung%20Yu%20and%20Da%20Eun%20Lee%20and%20Yunyong%20Ko%20and%20Sang-Wook%20Kim%0AAbstract%3A%20%20%20Hyperedge%20prediction%20is%20a%20fundamental%20task%20to%20predict%20future%20high-order%0Arelations%20based%20on%20the%20observed%20network%20structure.%20Existing%20hyperedge%0Aprediction%20methods%2C%20however%2C%20suffer%20from%20the%20data%20sparsity%20problem.%20To%0Aalleviate%20this%20problem%2C%20negative%20sampling%20methods%20can%20be%20used%2C%20which%20leverage%0Anon-existing%20hyperedges%20as%20contrastive%20information%20for%20model%20training.%20However%2C%0Athe%20following%20important%20challenges%20have%20been%20rarely%20studied%3A%20%28C1%29%20lack%20of%0Aguidance%20for%20generating%20negatives%20and%20%28C2%29%20possibility%20of%20producing%20false%0Anegatives.%20To%20address%20them%2C%20we%20propose%20a%20novel%20hyperedge%20prediction%20method%2C%0AHyGEN%2C%20that%20employs%20%281%29%20a%20negative%20hyperedge%20generator%20that%20employs%20positive%0Ahyperedges%20as%20a%20guidance%20to%20generate%20more%20realistic%20ones%20and%20%282%29%20a%0Aregularization%20term%20that%20prevents%20the%20generated%20hyperedges%20from%20being%20false%0Anegatives.%20Extensive%20experiments%20on%20six%20real-world%20hypergraphs%20reveal%20that%0AHyGEN%20consistently%20outperforms%20four%20state-of-the-art%20hyperedge%20prediction%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05827v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyGEN%253A%2520Regularizing%2520Negative%2520Hyperedge%2520Generation%2520for%2520Accurate%2520Hyperedge%250A%2520%2520Prediction%26entry.906535625%3DSong%2520Kyung%2520Yu%2520and%2520Da%2520Eun%2520Lee%2520and%2520Yunyong%2520Ko%2520and%2520Sang-Wook%2520Kim%26entry.1292438233%3D%2520%2520Hyperedge%2520prediction%2520is%2520a%2520fundamental%2520task%2520to%2520predict%2520future%2520high-order%250Arelations%2520based%2520on%2520the%2520observed%2520network%2520structure.%2520Existing%2520hyperedge%250Aprediction%2520methods%252C%2520however%252C%2520suffer%2520from%2520the%2520data%2520sparsity%2520problem.%2520To%250Aalleviate%2520this%2520problem%252C%2520negative%2520sampling%2520methods%2520can%2520be%2520used%252C%2520which%2520leverage%250Anon-existing%2520hyperedges%2520as%2520contrastive%2520information%2520for%2520model%2520training.%2520However%252C%250Athe%2520following%2520important%2520challenges%2520have%2520been%2520rarely%2520studied%253A%2520%2528C1%2529%2520lack%2520of%250Aguidance%2520for%2520generating%2520negatives%2520and%2520%2528C2%2529%2520possibility%2520of%2520producing%2520false%250Anegatives.%2520To%2520address%2520them%252C%2520we%2520propose%2520a%2520novel%2520hyperedge%2520prediction%2520method%252C%250AHyGEN%252C%2520that%2520employs%2520%25281%2529%2520a%2520negative%2520hyperedge%2520generator%2520that%2520employs%2520positive%250Ahyperedges%2520as%2520a%2520guidance%2520to%2520generate%2520more%2520realistic%2520ones%2520and%2520%25282%2529%2520a%250Aregularization%2520term%2520that%2520prevents%2520the%2520generated%2520hyperedges%2520from%2520being%2520false%250Anegatives.%2520Extensive%2520experiments%2520on%2520six%2520real-world%2520hypergraphs%2520reveal%2520that%250AHyGEN%2520consistently%2520outperforms%2520four%2520state-of-the-art%2520hyperedge%2520prediction%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05827v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyGEN%3A%20Regularizing%20Negative%20Hyperedge%20Generation%20for%20Accurate%20Hyperedge%0A%20%20Prediction&entry.906535625=Song%20Kyung%20Yu%20and%20Da%20Eun%20Lee%20and%20Yunyong%20Ko%20and%20Sang-Wook%20Kim&entry.1292438233=%20%20Hyperedge%20prediction%20is%20a%20fundamental%20task%20to%20predict%20future%20high-order%0Arelations%20based%20on%20the%20observed%20network%20structure.%20Existing%20hyperedge%0Aprediction%20methods%2C%20however%2C%20suffer%20from%20the%20data%20sparsity%20problem.%20To%0Aalleviate%20this%20problem%2C%20negative%20sampling%20methods%20can%20be%20used%2C%20which%20leverage%0Anon-existing%20hyperedges%20as%20contrastive%20information%20for%20model%20training.%20However%2C%0Athe%20following%20important%20challenges%20have%20been%20rarely%20studied%3A%20%28C1%29%20lack%20of%0Aguidance%20for%20generating%20negatives%20and%20%28C2%29%20possibility%20of%20producing%20false%0Anegatives.%20To%20address%20them%2C%20we%20propose%20a%20novel%20hyperedge%20prediction%20method%2C%0AHyGEN%2C%20that%20employs%20%281%29%20a%20negative%20hyperedge%20generator%20that%20employs%20positive%0Ahyperedges%20as%20a%20guidance%20to%20generate%20more%20realistic%20ones%20and%20%282%29%20a%0Aregularization%20term%20that%20prevents%20the%20generated%20hyperedges%20from%20being%20false%0Anegatives.%20Extensive%20experiments%20on%20six%20real-world%20hypergraphs%20reveal%20that%0AHyGEN%20consistently%20outperforms%20four%20state-of-the-art%20hyperedge%20prediction%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05827v2&entry.124074799=Read"},
{"title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity", "author": "Yuri Kuratov and Mikhail Arkhipov and Aydar Bulatov and Mikhail Burtsev", "abstract": "  A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.\n", "link": "http://arxiv.org/abs/2502.13063v1", "date": "2025-02-18", "relevancy": 2.5849, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cramming%201568%20Tokens%20into%20a%20Single%20Vector%20and%20Back%20Again%3A%20Exploring%20the%0A%20%20Limits%20of%20Embedding%20Space%20Capacity&body=Title%3A%20Cramming%201568%20Tokens%20into%20a%20Single%20Vector%20and%20Back%20Again%3A%20Exploring%20the%0A%20%20Limits%20of%20Embedding%20Space%20Capacity%0AAuthor%3A%20Yuri%20Kuratov%20and%20Mikhail%20Arkhipov%20and%20Aydar%20Bulatov%20and%20Mikhail%20Burtsev%0AAbstract%3A%20%20%20A%20range%20of%20recent%20works%20addresses%20the%20problem%20of%20compression%20of%20sequence%20of%0Atokens%20into%20a%20shorter%20sequence%20of%20real-valued%20vectors%20to%20be%20used%20as%20inputs%0Ainstead%20of%20token%20embeddings%20or%20key-value%20cache.%20These%20approaches%20allow%20to%0Areduce%20the%20amount%20of%20compute%20in%20existing%20language%20models.%20Despite%20relying%20on%0Apowerful%20models%20as%20encoders%2C%20the%20maximum%20attainable%20lossless%20compression%20ratio%0Ais%20typically%20not%20higher%20than%20x10.%20This%20fact%20is%20highly%20intriguing%20because%2C%20in%0Atheory%2C%20the%20maximum%20information%20capacity%20of%20large%20real-valued%20vectors%20is%20far%0Abeyond%20the%20presented%20rates%20even%20for%2016-bit%20precision%20and%20a%20modest%20vector%20size.%0AIn%20this%20work%2C%20we%20explore%20the%20limits%20of%20compression%20by%20replacing%20the%20encoder%0Awith%20a%20per-sample%20optimization%20procedure.%20We%20show%20that%20vectors%20with%20compression%0Aratios%20up%20to%20x1500%20exist%2C%20which%20highlights%20two%20orders%20of%20magnitude%20gap%20between%0Aexisting%20and%20practically%20attainable%20solutions.%20Furthermore%2C%20we%20empirically%20show%0Athat%20the%20compression%20limits%20are%20determined%20not%20by%20the%20length%20of%20the%20input%20but%0Aby%20the%20amount%20of%20uncertainty%20to%20be%20reduced%2C%20namely%2C%20the%20cross-entropy%20loss%20on%0Athis%20sequence%20without%20any%20conditioning.%20The%20obtained%20limits%20highlight%20the%0Asubstantial%20gap%20between%20the%20theoretical%20capacity%20of%20input%20embeddings%20and%20their%0Apractical%20utilization%2C%20suggesting%20significant%20room%20for%20optimization%20in%20model%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCramming%25201568%2520Tokens%2520into%2520a%2520Single%2520Vector%2520and%2520Back%2520Again%253A%2520Exploring%2520the%250A%2520%2520Limits%2520of%2520Embedding%2520Space%2520Capacity%26entry.906535625%3DYuri%2520Kuratov%2520and%2520Mikhail%2520Arkhipov%2520and%2520Aydar%2520Bulatov%2520and%2520Mikhail%2520Burtsev%26entry.1292438233%3D%2520%2520A%2520range%2520of%2520recent%2520works%2520addresses%2520the%2520problem%2520of%2520compression%2520of%2520sequence%2520of%250Atokens%2520into%2520a%2520shorter%2520sequence%2520of%2520real-valued%2520vectors%2520to%2520be%2520used%2520as%2520inputs%250Ainstead%2520of%2520token%2520embeddings%2520or%2520key-value%2520cache.%2520These%2520approaches%2520allow%2520to%250Areduce%2520the%2520amount%2520of%2520compute%2520in%2520existing%2520language%2520models.%2520Despite%2520relying%2520on%250Apowerful%2520models%2520as%2520encoders%252C%2520the%2520maximum%2520attainable%2520lossless%2520compression%2520ratio%250Ais%2520typically%2520not%2520higher%2520than%2520x10.%2520This%2520fact%2520is%2520highly%2520intriguing%2520because%252C%2520in%250Atheory%252C%2520the%2520maximum%2520information%2520capacity%2520of%2520large%2520real-valued%2520vectors%2520is%2520far%250Abeyond%2520the%2520presented%2520rates%2520even%2520for%252016-bit%2520precision%2520and%2520a%2520modest%2520vector%2520size.%250AIn%2520this%2520work%252C%2520we%2520explore%2520the%2520limits%2520of%2520compression%2520by%2520replacing%2520the%2520encoder%250Awith%2520a%2520per-sample%2520optimization%2520procedure.%2520We%2520show%2520that%2520vectors%2520with%2520compression%250Aratios%2520up%2520to%2520x1500%2520exist%252C%2520which%2520highlights%2520two%2520orders%2520of%2520magnitude%2520gap%2520between%250Aexisting%2520and%2520practically%2520attainable%2520solutions.%2520Furthermore%252C%2520we%2520empirically%2520show%250Athat%2520the%2520compression%2520limits%2520are%2520determined%2520not%2520by%2520the%2520length%2520of%2520the%2520input%2520but%250Aby%2520the%2520amount%2520of%2520uncertainty%2520to%2520be%2520reduced%252C%2520namely%252C%2520the%2520cross-entropy%2520loss%2520on%250Athis%2520sequence%2520without%2520any%2520conditioning.%2520The%2520obtained%2520limits%2520highlight%2520the%250Asubstantial%2520gap%2520between%2520the%2520theoretical%2520capacity%2520of%2520input%2520embeddings%2520and%2520their%250Apractical%2520utilization%252C%2520suggesting%2520significant%2520room%2520for%2520optimization%2520in%2520model%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cramming%201568%20Tokens%20into%20a%20Single%20Vector%20and%20Back%20Again%3A%20Exploring%20the%0A%20%20Limits%20of%20Embedding%20Space%20Capacity&entry.906535625=Yuri%20Kuratov%20and%20Mikhail%20Arkhipov%20and%20Aydar%20Bulatov%20and%20Mikhail%20Burtsev&entry.1292438233=%20%20A%20range%20of%20recent%20works%20addresses%20the%20problem%20of%20compression%20of%20sequence%20of%0Atokens%20into%20a%20shorter%20sequence%20of%20real-valued%20vectors%20to%20be%20used%20as%20inputs%0Ainstead%20of%20token%20embeddings%20or%20key-value%20cache.%20These%20approaches%20allow%20to%0Areduce%20the%20amount%20of%20compute%20in%20existing%20language%20models.%20Despite%20relying%20on%0Apowerful%20models%20as%20encoders%2C%20the%20maximum%20attainable%20lossless%20compression%20ratio%0Ais%20typically%20not%20higher%20than%20x10.%20This%20fact%20is%20highly%20intriguing%20because%2C%20in%0Atheory%2C%20the%20maximum%20information%20capacity%20of%20large%20real-valued%20vectors%20is%20far%0Abeyond%20the%20presented%20rates%20even%20for%2016-bit%20precision%20and%20a%20modest%20vector%20size.%0AIn%20this%20work%2C%20we%20explore%20the%20limits%20of%20compression%20by%20replacing%20the%20encoder%0Awith%20a%20per-sample%20optimization%20procedure.%20We%20show%20that%20vectors%20with%20compression%0Aratios%20up%20to%20x1500%20exist%2C%20which%20highlights%20two%20orders%20of%20magnitude%20gap%20between%0Aexisting%20and%20practically%20attainable%20solutions.%20Furthermore%2C%20we%20empirically%20show%0Athat%20the%20compression%20limits%20are%20determined%20not%20by%20the%20length%20of%20the%20input%20but%0Aby%20the%20amount%20of%20uncertainty%20to%20be%20reduced%2C%20namely%2C%20the%20cross-entropy%20loss%20on%0Athis%20sequence%20without%20any%20conditioning.%20The%20obtained%20limits%20highlight%20the%0Asubstantial%20gap%20between%20the%20theoretical%20capacity%20of%20input%20embeddings%20and%20their%0Apractical%20utilization%2C%20suggesting%20significant%20room%20for%20optimization%20in%20model%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13063v1&entry.124074799=Read"},
{"title": "Towards Homogeneous Lexical Tone Decoding from Heterogeneous\n  Intracranial Recordings", "author": "Di Wu and Siyuan Li and Chen Feng and Lu Cao and Yue Zhang and Jie Yang and Mohamad Sawan", "abstract": "  Recent advancements in brain-computer interfaces (BCIs) have enabled the\ndecoding of lexical tones from intracranial recordings, offering the potential\nto restore the communication abilities of speech-impaired tonal language\nspeakers. However, data heterogeneity induced by both physiological and\ninstrumental factors poses a significant challenge for unified invasive brain\ntone decoding. Traditional subject-specific models, which operate under a\nheterogeneous decoding paradigm, fail to capture generalized neural\nrepresentations and cannot effectively leverage data across subjects. To\naddress these limitations, we introduce Homogeneity-Heterogeneity Disentangled\nLearning for neural Representations (H2DiLR), a novel framework that\ndisentangles and learns both the homogeneity and heterogeneity from\nintracranial recordings across multiple subjects. To evaluate H2DiLR, we\ncollected stereoelectroencephalography (sEEG) data from multiple participants\nreading Mandarin materials comprising 407 syllables, representing nearly all\nMandarin characters. Extensive experiments demonstrate that H2DiLR, as a\nunified decoding paradigm, significantly outperforms the conventional\nheterogeneous decoding approach. Furthermore, we empirically confirm that\nH2DiLR effectively captures both homogeneity and heterogeneity during neural\nrepresentation learning.\n", "link": "http://arxiv.org/abs/2410.12866v2", "date": "2025-02-18", "relevancy": 2.558, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Homogeneous%20Lexical%20Tone%20Decoding%20from%20Heterogeneous%0A%20%20Intracranial%20Recordings&body=Title%3A%20Towards%20Homogeneous%20Lexical%20Tone%20Decoding%20from%20Heterogeneous%0A%20%20Intracranial%20Recordings%0AAuthor%3A%20Di%20Wu%20and%20Siyuan%20Li%20and%20Chen%20Feng%20and%20Lu%20Cao%20and%20Yue%20Zhang%20and%20Jie%20Yang%20and%20Mohamad%20Sawan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20brain-computer%20interfaces%20%28BCIs%29%20have%20enabled%20the%0Adecoding%20of%20lexical%20tones%20from%20intracranial%20recordings%2C%20offering%20the%20potential%0Ato%20restore%20the%20communication%20abilities%20of%20speech-impaired%20tonal%20language%0Aspeakers.%20However%2C%20data%20heterogeneity%20induced%20by%20both%20physiological%20and%0Ainstrumental%20factors%20poses%20a%20significant%20challenge%20for%20unified%20invasive%20brain%0Atone%20decoding.%20Traditional%20subject-specific%20models%2C%20which%20operate%20under%20a%0Aheterogeneous%20decoding%20paradigm%2C%20fail%20to%20capture%20generalized%20neural%0Arepresentations%20and%20cannot%20effectively%20leverage%20data%20across%20subjects.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20Homogeneity-Heterogeneity%20Disentangled%0ALearning%20for%20neural%20Representations%20%28H2DiLR%29%2C%20a%20novel%20framework%20that%0Adisentangles%20and%20learns%20both%20the%20homogeneity%20and%20heterogeneity%20from%0Aintracranial%20recordings%20across%20multiple%20subjects.%20To%20evaluate%20H2DiLR%2C%20we%0Acollected%20stereoelectroencephalography%20%28sEEG%29%20data%20from%20multiple%20participants%0Areading%20Mandarin%20materials%20comprising%20407%20syllables%2C%20representing%20nearly%20all%0AMandarin%20characters.%20Extensive%20experiments%20demonstrate%20that%20H2DiLR%2C%20as%20a%0Aunified%20decoding%20paradigm%2C%20significantly%20outperforms%20the%20conventional%0Aheterogeneous%20decoding%20approach.%20Furthermore%2C%20we%20empirically%20confirm%20that%0AH2DiLR%20effectively%20captures%20both%20homogeneity%20and%20heterogeneity%20during%20neural%0Arepresentation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12866v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Homogeneous%2520Lexical%2520Tone%2520Decoding%2520from%2520Heterogeneous%250A%2520%2520Intracranial%2520Recordings%26entry.906535625%3DDi%2520Wu%2520and%2520Siyuan%2520Li%2520and%2520Chen%2520Feng%2520and%2520Lu%2520Cao%2520and%2520Yue%2520Zhang%2520and%2520Jie%2520Yang%2520and%2520Mohamad%2520Sawan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520brain-computer%2520interfaces%2520%2528BCIs%2529%2520have%2520enabled%2520the%250Adecoding%2520of%2520lexical%2520tones%2520from%2520intracranial%2520recordings%252C%2520offering%2520the%2520potential%250Ato%2520restore%2520the%2520communication%2520abilities%2520of%2520speech-impaired%2520tonal%2520language%250Aspeakers.%2520However%252C%2520data%2520heterogeneity%2520induced%2520by%2520both%2520physiological%2520and%250Ainstrumental%2520factors%2520poses%2520a%2520significant%2520challenge%2520for%2520unified%2520invasive%2520brain%250Atone%2520decoding.%2520Traditional%2520subject-specific%2520models%252C%2520which%2520operate%2520under%2520a%250Aheterogeneous%2520decoding%2520paradigm%252C%2520fail%2520to%2520capture%2520generalized%2520neural%250Arepresentations%2520and%2520cannot%2520effectively%2520leverage%2520data%2520across%2520subjects.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520Homogeneity-Heterogeneity%2520Disentangled%250ALearning%2520for%2520neural%2520Representations%2520%2528H2DiLR%2529%252C%2520a%2520novel%2520framework%2520that%250Adisentangles%2520and%2520learns%2520both%2520the%2520homogeneity%2520and%2520heterogeneity%2520from%250Aintracranial%2520recordings%2520across%2520multiple%2520subjects.%2520To%2520evaluate%2520H2DiLR%252C%2520we%250Acollected%2520stereoelectroencephalography%2520%2528sEEG%2529%2520data%2520from%2520multiple%2520participants%250Areading%2520Mandarin%2520materials%2520comprising%2520407%2520syllables%252C%2520representing%2520nearly%2520all%250AMandarin%2520characters.%2520Extensive%2520experiments%2520demonstrate%2520that%2520H2DiLR%252C%2520as%2520a%250Aunified%2520decoding%2520paradigm%252C%2520significantly%2520outperforms%2520the%2520conventional%250Aheterogeneous%2520decoding%2520approach.%2520Furthermore%252C%2520we%2520empirically%2520confirm%2520that%250AH2DiLR%2520effectively%2520captures%2520both%2520homogeneity%2520and%2520heterogeneity%2520during%2520neural%250Arepresentation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12866v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Homogeneous%20Lexical%20Tone%20Decoding%20from%20Heterogeneous%0A%20%20Intracranial%20Recordings&entry.906535625=Di%20Wu%20and%20Siyuan%20Li%20and%20Chen%20Feng%20and%20Lu%20Cao%20and%20Yue%20Zhang%20and%20Jie%20Yang%20and%20Mohamad%20Sawan&entry.1292438233=%20%20Recent%20advancements%20in%20brain-computer%20interfaces%20%28BCIs%29%20have%20enabled%20the%0Adecoding%20of%20lexical%20tones%20from%20intracranial%20recordings%2C%20offering%20the%20potential%0Ato%20restore%20the%20communication%20abilities%20of%20speech-impaired%20tonal%20language%0Aspeakers.%20However%2C%20data%20heterogeneity%20induced%20by%20both%20physiological%20and%0Ainstrumental%20factors%20poses%20a%20significant%20challenge%20for%20unified%20invasive%20brain%0Atone%20decoding.%20Traditional%20subject-specific%20models%2C%20which%20operate%20under%20a%0Aheterogeneous%20decoding%20paradigm%2C%20fail%20to%20capture%20generalized%20neural%0Arepresentations%20and%20cannot%20effectively%20leverage%20data%20across%20subjects.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20Homogeneity-Heterogeneity%20Disentangled%0ALearning%20for%20neural%20Representations%20%28H2DiLR%29%2C%20a%20novel%20framework%20that%0Adisentangles%20and%20learns%20both%20the%20homogeneity%20and%20heterogeneity%20from%0Aintracranial%20recordings%20across%20multiple%20subjects.%20To%20evaluate%20H2DiLR%2C%20we%0Acollected%20stereoelectroencephalography%20%28sEEG%29%20data%20from%20multiple%20participants%0Areading%20Mandarin%20materials%20comprising%20407%20syllables%2C%20representing%20nearly%20all%0AMandarin%20characters.%20Extensive%20experiments%20demonstrate%20that%20H2DiLR%2C%20as%20a%0Aunified%20decoding%20paradigm%2C%20significantly%20outperforms%20the%20conventional%0Aheterogeneous%20decoding%20approach.%20Furthermore%2C%20we%20empirically%20confirm%20that%0AH2DiLR%20effectively%20captures%20both%20homogeneity%20and%20heterogeneity%20during%20neural%0Arepresentation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12866v2&entry.124074799=Read"},
{"title": "BenthicNet: A global compilation of seafloor images for deep learning\n  applications", "author": "Scott C. Lowe and Benjamin Misiuk and Isaac Xu and Shakhboz Abdulazizov and Amit R. Baroi and Alex C. Bastos and Merlin Best and Vicki Ferrini and Ariell Friedman and Deborah Hart and Ove Hoegh-Guldberg and Daniel Ierodiaconou and Julia Mackin-McLaughlin and Kathryn Markey and Pedro S. Menandro and Jacquomo Monk and Shreya Nemani and John O'Brien and Elizabeth Oh and Luba Y. Reshitnyk and Katleen Robert and Chris M. Roelfsema and Jessica A. Sameoto and Alexandre C. G. Schimel and Jordan A. Thomson and Brittany R. Wilson and Melisa C. Wong and Craig J. Brown and Thomas Trappenberg", "abstract": "  Advances in underwater imaging enable collection of extensive seafloor image\ndatasets necessary for monitoring important benthic ecosystems. The ability to\ncollect seafloor imagery has outpaced our capacity to analyze it, hindering\nmobilization of this crucial environmental information. Machine learning\napproaches provide opportunities to increase the efficiency with which seafloor\nimagery is analyzed, yet large and consistent datasets to support development\nof such approaches are scarce. Here we present BenthicNet: a global compilation\nof seafloor imagery designed to support the training and evaluation of\nlarge-scale image recognition models. An initial set of over 11.4 million\nimages was collected and curated to represent a diversity of seafloor\nenvironments using a representative subset of 1.3 million images. These are\naccompanied by 3.1 million annotations translated to the CATAMI scheme, which\nspan 190,000 of the images. A large deep learning model was trained on this\ncompilation and preliminary results suggest it has utility for automating large\nand small-scale image analysis tasks. The compilation and model are made openly\navailable for reuse at https://doi.org/10.20383/103.0614.\n", "link": "http://arxiv.org/abs/2405.05241v3", "date": "2025-02-18", "relevancy": 2.5477, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5042}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications&body=Title%3A%20BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications%0AAuthor%3A%20Scott%20C.%20Lowe%20and%20Benjamin%20Misiuk%20and%20Isaac%20Xu%20and%20Shakhboz%20Abdulazizov%20and%20Amit%20R.%20Baroi%20and%20Alex%20C.%20Bastos%20and%20Merlin%20Best%20and%20Vicki%20Ferrini%20and%20Ariell%20Friedman%20and%20Deborah%20Hart%20and%20Ove%20Hoegh-Guldberg%20and%20Daniel%20Ierodiaconou%20and%20Julia%20Mackin-McLaughlin%20and%20Kathryn%20Markey%20and%20Pedro%20S.%20Menandro%20and%20Jacquomo%20Monk%20and%20Shreya%20Nemani%20and%20John%20O%27Brien%20and%20Elizabeth%20Oh%20and%20Luba%20Y.%20Reshitnyk%20and%20Katleen%20Robert%20and%20Chris%20M.%20Roelfsema%20and%20Jessica%20A.%20Sameoto%20and%20Alexandre%20C.%20G.%20Schimel%20and%20Jordan%20A.%20Thomson%20and%20Brittany%20R.%20Wilson%20and%20Melisa%20C.%20Wong%20and%20Craig%20J.%20Brown%20and%20Thomas%20Trappenberg%0AAbstract%3A%20%20%20Advances%20in%20underwater%20imaging%20enable%20collection%20of%20extensive%20seafloor%20image%0Adatasets%20necessary%20for%20monitoring%20important%20benthic%20ecosystems.%20The%20ability%20to%0Acollect%20seafloor%20imagery%20has%20outpaced%20our%20capacity%20to%20analyze%20it%2C%20hindering%0Amobilization%20of%20this%20crucial%20environmental%20information.%20Machine%20learning%0Aapproaches%20provide%20opportunities%20to%20increase%20the%20efficiency%20with%20which%20seafloor%0Aimagery%20is%20analyzed%2C%20yet%20large%20and%20consistent%20datasets%20to%20support%20development%0Aof%20such%20approaches%20are%20scarce.%20Here%20we%20present%20BenthicNet%3A%20a%20global%20compilation%0Aof%20seafloor%20imagery%20designed%20to%20support%20the%20training%20and%20evaluation%20of%0Alarge-scale%20image%20recognition%20models.%20An%20initial%20set%20of%20over%2011.4%20million%0Aimages%20was%20collected%20and%20curated%20to%20represent%20a%20diversity%20of%20seafloor%0Aenvironments%20using%20a%20representative%20subset%20of%201.3%20million%20images.%20These%20are%0Aaccompanied%20by%203.1%20million%20annotations%20translated%20to%20the%20CATAMI%20scheme%2C%20which%0Aspan%20190%2C000%20of%20the%20images.%20A%20large%20deep%20learning%20model%20was%20trained%20on%20this%0Acompilation%20and%20preliminary%20results%20suggest%20it%20has%20utility%20for%20automating%20large%0Aand%20small-scale%20image%20analysis%20tasks.%20The%20compilation%20and%20model%20are%20made%20openly%0Aavailable%20for%20reuse%20at%20https%3A//doi.org/10.20383/103.0614.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05241v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenthicNet%253A%2520A%2520global%2520compilation%2520of%2520seafloor%2520images%2520for%2520deep%2520learning%250A%2520%2520applications%26entry.906535625%3DScott%2520C.%2520Lowe%2520and%2520Benjamin%2520Misiuk%2520and%2520Isaac%2520Xu%2520and%2520Shakhboz%2520Abdulazizov%2520and%2520Amit%2520R.%2520Baroi%2520and%2520Alex%2520C.%2520Bastos%2520and%2520Merlin%2520Best%2520and%2520Vicki%2520Ferrini%2520and%2520Ariell%2520Friedman%2520and%2520Deborah%2520Hart%2520and%2520Ove%2520Hoegh-Guldberg%2520and%2520Daniel%2520Ierodiaconou%2520and%2520Julia%2520Mackin-McLaughlin%2520and%2520Kathryn%2520Markey%2520and%2520Pedro%2520S.%2520Menandro%2520and%2520Jacquomo%2520Monk%2520and%2520Shreya%2520Nemani%2520and%2520John%2520O%2527Brien%2520and%2520Elizabeth%2520Oh%2520and%2520Luba%2520Y.%2520Reshitnyk%2520and%2520Katleen%2520Robert%2520and%2520Chris%2520M.%2520Roelfsema%2520and%2520Jessica%2520A.%2520Sameoto%2520and%2520Alexandre%2520C.%2520G.%2520Schimel%2520and%2520Jordan%2520A.%2520Thomson%2520and%2520Brittany%2520R.%2520Wilson%2520and%2520Melisa%2520C.%2520Wong%2520and%2520Craig%2520J.%2520Brown%2520and%2520Thomas%2520Trappenberg%26entry.1292438233%3D%2520%2520Advances%2520in%2520underwater%2520imaging%2520enable%2520collection%2520of%2520extensive%2520seafloor%2520image%250Adatasets%2520necessary%2520for%2520monitoring%2520important%2520benthic%2520ecosystems.%2520The%2520ability%2520to%250Acollect%2520seafloor%2520imagery%2520has%2520outpaced%2520our%2520capacity%2520to%2520analyze%2520it%252C%2520hindering%250Amobilization%2520of%2520this%2520crucial%2520environmental%2520information.%2520Machine%2520learning%250Aapproaches%2520provide%2520opportunities%2520to%2520increase%2520the%2520efficiency%2520with%2520which%2520seafloor%250Aimagery%2520is%2520analyzed%252C%2520yet%2520large%2520and%2520consistent%2520datasets%2520to%2520support%2520development%250Aof%2520such%2520approaches%2520are%2520scarce.%2520Here%2520we%2520present%2520BenthicNet%253A%2520a%2520global%2520compilation%250Aof%2520seafloor%2520imagery%2520designed%2520to%2520support%2520the%2520training%2520and%2520evaluation%2520of%250Alarge-scale%2520image%2520recognition%2520models.%2520An%2520initial%2520set%2520of%2520over%252011.4%2520million%250Aimages%2520was%2520collected%2520and%2520curated%2520to%2520represent%2520a%2520diversity%2520of%2520seafloor%250Aenvironments%2520using%2520a%2520representative%2520subset%2520of%25201.3%2520million%2520images.%2520These%2520are%250Aaccompanied%2520by%25203.1%2520million%2520annotations%2520translated%2520to%2520the%2520CATAMI%2520scheme%252C%2520which%250Aspan%2520190%252C000%2520of%2520the%2520images.%2520A%2520large%2520deep%2520learning%2520model%2520was%2520trained%2520on%2520this%250Acompilation%2520and%2520preliminary%2520results%2520suggest%2520it%2520has%2520utility%2520for%2520automating%2520large%250Aand%2520small-scale%2520image%2520analysis%2520tasks.%2520The%2520compilation%2520and%2520model%2520are%2520made%2520openly%250Aavailable%2520for%2520reuse%2520at%2520https%253A//doi.org/10.20383/103.0614.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05241v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BenthicNet%3A%20A%20global%20compilation%20of%20seafloor%20images%20for%20deep%20learning%0A%20%20applications&entry.906535625=Scott%20C.%20Lowe%20and%20Benjamin%20Misiuk%20and%20Isaac%20Xu%20and%20Shakhboz%20Abdulazizov%20and%20Amit%20R.%20Baroi%20and%20Alex%20C.%20Bastos%20and%20Merlin%20Best%20and%20Vicki%20Ferrini%20and%20Ariell%20Friedman%20and%20Deborah%20Hart%20and%20Ove%20Hoegh-Guldberg%20and%20Daniel%20Ierodiaconou%20and%20Julia%20Mackin-McLaughlin%20and%20Kathryn%20Markey%20and%20Pedro%20S.%20Menandro%20and%20Jacquomo%20Monk%20and%20Shreya%20Nemani%20and%20John%20O%27Brien%20and%20Elizabeth%20Oh%20and%20Luba%20Y.%20Reshitnyk%20and%20Katleen%20Robert%20and%20Chris%20M.%20Roelfsema%20and%20Jessica%20A.%20Sameoto%20and%20Alexandre%20C.%20G.%20Schimel%20and%20Jordan%20A.%20Thomson%20and%20Brittany%20R.%20Wilson%20and%20Melisa%20C.%20Wong%20and%20Craig%20J.%20Brown%20and%20Thomas%20Trappenberg&entry.1292438233=%20%20Advances%20in%20underwater%20imaging%20enable%20collection%20of%20extensive%20seafloor%20image%0Adatasets%20necessary%20for%20monitoring%20important%20benthic%20ecosystems.%20The%20ability%20to%0Acollect%20seafloor%20imagery%20has%20outpaced%20our%20capacity%20to%20analyze%20it%2C%20hindering%0Amobilization%20of%20this%20crucial%20environmental%20information.%20Machine%20learning%0Aapproaches%20provide%20opportunities%20to%20increase%20the%20efficiency%20with%20which%20seafloor%0Aimagery%20is%20analyzed%2C%20yet%20large%20and%20consistent%20datasets%20to%20support%20development%0Aof%20such%20approaches%20are%20scarce.%20Here%20we%20present%20BenthicNet%3A%20a%20global%20compilation%0Aof%20seafloor%20imagery%20designed%20to%20support%20the%20training%20and%20evaluation%20of%0Alarge-scale%20image%20recognition%20models.%20An%20initial%20set%20of%20over%2011.4%20million%0Aimages%20was%20collected%20and%20curated%20to%20represent%20a%20diversity%20of%20seafloor%0Aenvironments%20using%20a%20representative%20subset%20of%201.3%20million%20images.%20These%20are%0Aaccompanied%20by%203.1%20million%20annotations%20translated%20to%20the%20CATAMI%20scheme%2C%20which%0Aspan%20190%2C000%20of%20the%20images.%20A%20large%20deep%20learning%20model%20was%20trained%20on%20this%0Acompilation%20and%20preliminary%20results%20suggest%20it%20has%20utility%20for%20automating%20large%0Aand%20small-scale%20image%20analysis%20tasks.%20The%20compilation%20and%20model%20are%20made%20openly%0Aavailable%20for%20reuse%20at%20https%3A//doi.org/10.20383/103.0614.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05241v3&entry.124074799=Read"},
{"title": "CogSteer: Cognition-Inspired Selective Layer Intervention for\n  Efficiently Steering Large Language Models", "author": "Xintong Wang and Jingheng Pan and Liang Ding and Longyue Wang and Longqin Jiang and Xingshan Li and Chris Biemann", "abstract": "  Large Language Models (LLMs) achieve remarkable performance through\npretraining on extensive data. This enables efficient adaptation to diverse\ndownstream tasks. However, the lack of interpretability in their underlying\nmechanisms limits the ability to effectively steer LLMs for specific\napplications. In this work, we investigate the intrinsic mechanisms of LLMs\nfrom a cognitive perspective using eye movement measures. Specifically, we\nanalyze the layer-wise correlation between human cognitive indicators and LLM\nrepresentations. Building on these insights, we propose a heuristic approach\nfor selecting the optimal steering layer to modulate LLM semantics. To this\nend, we introduce an efficient selective layer intervention based on prominent\nparameter-efficient fine-tuning methods, which conventionally adjust either all\nlayers or only the final layer. Additionally, we present an implicit layer\ncontrastive intervention during inference to steer LLMs away from toxic\noutputs. Extensive experiments on natural language understanding, reasoning,\nand generation tasks, conducted on GPT-2, LLaMa2-7B, and Mixtral-7B,\ndemonstrate the effectiveness and efficiency of our approach. As a\nmodel-agnostic framework, it enhances the interpretability of LLMs while\nimproving efficiency for safe deployment.\n", "link": "http://arxiv.org/abs/2410.17714v2", "date": "2025-02-18", "relevancy": 2.5425, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogSteer%3A%20Cognition-Inspired%20Selective%20Layer%20Intervention%20for%0A%20%20Efficiently%20Steering%20Large%20Language%20Models&body=Title%3A%20CogSteer%3A%20Cognition-Inspired%20Selective%20Layer%20Intervention%20for%0A%20%20Efficiently%20Steering%20Large%20Language%20Models%0AAuthor%3A%20Xintong%20Wang%20and%20Jingheng%20Pan%20and%20Liang%20Ding%20and%20Longyue%20Wang%20and%20Longqin%20Jiang%20and%20Xingshan%20Li%20and%20Chris%20Biemann%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20achieve%20remarkable%20performance%20through%0Apretraining%20on%20extensive%20data.%20This%20enables%20efficient%20adaptation%20to%20diverse%0Adownstream%20tasks.%20However%2C%20the%20lack%20of%20interpretability%20in%20their%20underlying%0Amechanisms%20limits%20the%20ability%20to%20effectively%20steer%20LLMs%20for%20specific%0Aapplications.%20In%20this%20work%2C%20we%20investigate%20the%20intrinsic%20mechanisms%20of%20LLMs%0Afrom%20a%20cognitive%20perspective%20using%20eye%20movement%20measures.%20Specifically%2C%20we%0Aanalyze%20the%20layer-wise%20correlation%20between%20human%20cognitive%20indicators%20and%20LLM%0Arepresentations.%20Building%20on%20these%20insights%2C%20we%20propose%20a%20heuristic%20approach%0Afor%20selecting%20the%20optimal%20steering%20layer%20to%20modulate%20LLM%20semantics.%20To%20this%0Aend%2C%20we%20introduce%20an%20efficient%20selective%20layer%20intervention%20based%20on%20prominent%0Aparameter-efficient%20fine-tuning%20methods%2C%20which%20conventionally%20adjust%20either%20all%0Alayers%20or%20only%20the%20final%20layer.%20Additionally%2C%20we%20present%20an%20implicit%20layer%0Acontrastive%20intervention%20during%20inference%20to%20steer%20LLMs%20away%20from%20toxic%0Aoutputs.%20Extensive%20experiments%20on%20natural%20language%20understanding%2C%20reasoning%2C%0Aand%20generation%20tasks%2C%20conducted%20on%20GPT-2%2C%20LLaMa2-7B%2C%20and%20Mixtral-7B%2C%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20approach.%20As%20a%0Amodel-agnostic%20framework%2C%20it%20enhances%20the%20interpretability%20of%20LLMs%20while%0Aimproving%20efficiency%20for%20safe%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogSteer%253A%2520Cognition-Inspired%2520Selective%2520Layer%2520Intervention%2520for%250A%2520%2520Efficiently%2520Steering%2520Large%2520Language%2520Models%26entry.906535625%3DXintong%2520Wang%2520and%2520Jingheng%2520Pan%2520and%2520Liang%2520Ding%2520and%2520Longyue%2520Wang%2520and%2520Longqin%2520Jiang%2520and%2520Xingshan%2520Li%2520and%2520Chris%2520Biemann%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520achieve%2520remarkable%2520performance%2520through%250Apretraining%2520on%2520extensive%2520data.%2520This%2520enables%2520efficient%2520adaptation%2520to%2520diverse%250Adownstream%2520tasks.%2520However%252C%2520the%2520lack%2520of%2520interpretability%2520in%2520their%2520underlying%250Amechanisms%2520limits%2520the%2520ability%2520to%2520effectively%2520steer%2520LLMs%2520for%2520specific%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520intrinsic%2520mechanisms%2520of%2520LLMs%250Afrom%2520a%2520cognitive%2520perspective%2520using%2520eye%2520movement%2520measures.%2520Specifically%252C%2520we%250Aanalyze%2520the%2520layer-wise%2520correlation%2520between%2520human%2520cognitive%2520indicators%2520and%2520LLM%250Arepresentations.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%2520a%2520heuristic%2520approach%250Afor%2520selecting%2520the%2520optimal%2520steering%2520layer%2520to%2520modulate%2520LLM%2520semantics.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520an%2520efficient%2520selective%2520layer%2520intervention%2520based%2520on%2520prominent%250Aparameter-efficient%2520fine-tuning%2520methods%252C%2520which%2520conventionally%2520adjust%2520either%2520all%250Alayers%2520or%2520only%2520the%2520final%2520layer.%2520Additionally%252C%2520we%2520present%2520an%2520implicit%2520layer%250Acontrastive%2520intervention%2520during%2520inference%2520to%2520steer%2520LLMs%2520away%2520from%2520toxic%250Aoutputs.%2520Extensive%2520experiments%2520on%2520natural%2520language%2520understanding%252C%2520reasoning%252C%250Aand%2520generation%2520tasks%252C%2520conducted%2520on%2520GPT-2%252C%2520LLaMa2-7B%252C%2520and%2520Mixtral-7B%252C%250Ademonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520approach.%2520As%2520a%250Amodel-agnostic%2520framework%252C%2520it%2520enhances%2520the%2520interpretability%2520of%2520LLMs%2520while%250Aimproving%2520efficiency%2520for%2520safe%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogSteer%3A%20Cognition-Inspired%20Selective%20Layer%20Intervention%20for%0A%20%20Efficiently%20Steering%20Large%20Language%20Models&entry.906535625=Xintong%20Wang%20and%20Jingheng%20Pan%20and%20Liang%20Ding%20and%20Longyue%20Wang%20and%20Longqin%20Jiang%20and%20Xingshan%20Li%20and%20Chris%20Biemann&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20achieve%20remarkable%20performance%20through%0Apretraining%20on%20extensive%20data.%20This%20enables%20efficient%20adaptation%20to%20diverse%0Adownstream%20tasks.%20However%2C%20the%20lack%20of%20interpretability%20in%20their%20underlying%0Amechanisms%20limits%20the%20ability%20to%20effectively%20steer%20LLMs%20for%20specific%0Aapplications.%20In%20this%20work%2C%20we%20investigate%20the%20intrinsic%20mechanisms%20of%20LLMs%0Afrom%20a%20cognitive%20perspective%20using%20eye%20movement%20measures.%20Specifically%2C%20we%0Aanalyze%20the%20layer-wise%20correlation%20between%20human%20cognitive%20indicators%20and%20LLM%0Arepresentations.%20Building%20on%20these%20insights%2C%20we%20propose%20a%20heuristic%20approach%0Afor%20selecting%20the%20optimal%20steering%20layer%20to%20modulate%20LLM%20semantics.%20To%20this%0Aend%2C%20we%20introduce%20an%20efficient%20selective%20layer%20intervention%20based%20on%20prominent%0Aparameter-efficient%20fine-tuning%20methods%2C%20which%20conventionally%20adjust%20either%20all%0Alayers%20or%20only%20the%20final%20layer.%20Additionally%2C%20we%20present%20an%20implicit%20layer%0Acontrastive%20intervention%20during%20inference%20to%20steer%20LLMs%20away%20from%20toxic%0Aoutputs.%20Extensive%20experiments%20on%20natural%20language%20understanding%2C%20reasoning%2C%0Aand%20generation%20tasks%2C%20conducted%20on%20GPT-2%2C%20LLaMa2-7B%2C%20and%20Mixtral-7B%2C%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20approach.%20As%20a%0Amodel-agnostic%20framework%2C%20it%20enhances%20the%20interpretability%20of%20LLMs%20while%0Aimproving%20efficiency%20for%20safe%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17714v2&entry.124074799=Read"},
{"title": "Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided\n  Mobile Manipulation", "author": "Zhijie Yan and Shufei Li and Zuoxu Wang and Lixiu Wu and Han Wang and Jun Zhu and Lijiang Chen and Jihong Liu", "abstract": "  Enabling mobile robots to perform long-term tasks in dynamic real-world\nenvironments is a formidable challenge, especially when the environment changes\nfrequently due to human-robot interactions or the robot's own actions.\nTraditional methods typically assume static scenes, which limits their\napplicability in the continuously changing real world. To overcome these\nlimitations, we present DovSG, a novel mobile manipulation framework that\nleverages dynamic open-vocabulary 3D scene graphs and a language-guided task\nplanning module for long-term task execution. DovSG takes RGB-D sequences as\ninput and utilizes vision-language models (VLMs) for object detection to obtain\nhigh-level object semantic features. Based on the segmented objects, a\nstructured 3D scene graph is generated for low-level spatial relationships.\nFurthermore, an efficient mechanism for locally updating the scene graph,\nallows the robot to adjust parts of the graph dynamically during interactions\nwithout the need for full scene reconstruction. This mechanism is particularly\nvaluable in dynamic environments, enabling the robot to continually adapt to\nscene changes and effectively support the execution of long-term tasks. We\nvalidated our system in real-world environments with varying degrees of manual\nmodifications, demonstrating its effectiveness and superior performance in\nlong-term tasks. Our project page is available at:\nhttps://bjhyzj.github.io/dovsg-web.\n", "link": "http://arxiv.org/abs/2410.11989v5", "date": "2025-02-18", "relevancy": 2.4942, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.7046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Long-term%20Language-Guided%0A%20%20Mobile%20Manipulation&body=Title%3A%20Dynamic%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Long-term%20Language-Guided%0A%20%20Mobile%20Manipulation%0AAuthor%3A%20Zhijie%20Yan%20and%20Shufei%20Li%20and%20Zuoxu%20Wang%20and%20Lixiu%20Wu%20and%20Han%20Wang%20and%20Jun%20Zhu%20and%20Lijiang%20Chen%20and%20Jihong%20Liu%0AAbstract%3A%20%20%20Enabling%20mobile%20robots%20to%20perform%20long-term%20tasks%20in%20dynamic%20real-world%0Aenvironments%20is%20a%20formidable%20challenge%2C%20especially%20when%20the%20environment%20changes%0Afrequently%20due%20to%20human-robot%20interactions%20or%20the%20robot%27s%20own%20actions.%0ATraditional%20methods%20typically%20assume%20static%20scenes%2C%20which%20limits%20their%0Aapplicability%20in%20the%20continuously%20changing%20real%20world.%20To%20overcome%20these%0Alimitations%2C%20we%20present%20DovSG%2C%20a%20novel%20mobile%20manipulation%20framework%20that%0Aleverages%20dynamic%20open-vocabulary%203D%20scene%20graphs%20and%20a%20language-guided%20task%0Aplanning%20module%20for%20long-term%20task%20execution.%20DovSG%20takes%20RGB-D%20sequences%20as%0Ainput%20and%20utilizes%20vision-language%20models%20%28VLMs%29%20for%20object%20detection%20to%20obtain%0Ahigh-level%20object%20semantic%20features.%20Based%20on%20the%20segmented%20objects%2C%20a%0Astructured%203D%20scene%20graph%20is%20generated%20for%20low-level%20spatial%20relationships.%0AFurthermore%2C%20an%20efficient%20mechanism%20for%20locally%20updating%20the%20scene%20graph%2C%0Aallows%20the%20robot%20to%20adjust%20parts%20of%20the%20graph%20dynamically%20during%20interactions%0Awithout%20the%20need%20for%20full%20scene%20reconstruction.%20This%20mechanism%20is%20particularly%0Avaluable%20in%20dynamic%20environments%2C%20enabling%20the%20robot%20to%20continually%20adapt%20to%0Ascene%20changes%20and%20effectively%20support%20the%20execution%20of%20long-term%20tasks.%20We%0Avalidated%20our%20system%20in%20real-world%20environments%20with%20varying%20degrees%20of%20manual%0Amodifications%2C%20demonstrating%20its%20effectiveness%20and%20superior%20performance%20in%0Along-term%20tasks.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//bjhyzj.github.io/dovsg-web.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11989v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Open-Vocabulary%25203D%2520Scene%2520Graphs%2520for%2520Long-term%2520Language-Guided%250A%2520%2520Mobile%2520Manipulation%26entry.906535625%3DZhijie%2520Yan%2520and%2520Shufei%2520Li%2520and%2520Zuoxu%2520Wang%2520and%2520Lixiu%2520Wu%2520and%2520Han%2520Wang%2520and%2520Jun%2520Zhu%2520and%2520Lijiang%2520Chen%2520and%2520Jihong%2520Liu%26entry.1292438233%3D%2520%2520Enabling%2520mobile%2520robots%2520to%2520perform%2520long-term%2520tasks%2520in%2520dynamic%2520real-world%250Aenvironments%2520is%2520a%2520formidable%2520challenge%252C%2520especially%2520when%2520the%2520environment%2520changes%250Afrequently%2520due%2520to%2520human-robot%2520interactions%2520or%2520the%2520robot%2527s%2520own%2520actions.%250ATraditional%2520methods%2520typically%2520assume%2520static%2520scenes%252C%2520which%2520limits%2520their%250Aapplicability%2520in%2520the%2520continuously%2520changing%2520real%2520world.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520present%2520DovSG%252C%2520a%2520novel%2520mobile%2520manipulation%2520framework%2520that%250Aleverages%2520dynamic%2520open-vocabulary%25203D%2520scene%2520graphs%2520and%2520a%2520language-guided%2520task%250Aplanning%2520module%2520for%2520long-term%2520task%2520execution.%2520DovSG%2520takes%2520RGB-D%2520sequences%2520as%250Ainput%2520and%2520utilizes%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520object%2520detection%2520to%2520obtain%250Ahigh-level%2520object%2520semantic%2520features.%2520Based%2520on%2520the%2520segmented%2520objects%252C%2520a%250Astructured%25203D%2520scene%2520graph%2520is%2520generated%2520for%2520low-level%2520spatial%2520relationships.%250AFurthermore%252C%2520an%2520efficient%2520mechanism%2520for%2520locally%2520updating%2520the%2520scene%2520graph%252C%250Aallows%2520the%2520robot%2520to%2520adjust%2520parts%2520of%2520the%2520graph%2520dynamically%2520during%2520interactions%250Awithout%2520the%2520need%2520for%2520full%2520scene%2520reconstruction.%2520This%2520mechanism%2520is%2520particularly%250Avaluable%2520in%2520dynamic%2520environments%252C%2520enabling%2520the%2520robot%2520to%2520continually%2520adapt%2520to%250Ascene%2520changes%2520and%2520effectively%2520support%2520the%2520execution%2520of%2520long-term%2520tasks.%2520We%250Avalidated%2520our%2520system%2520in%2520real-world%2520environments%2520with%2520varying%2520degrees%2520of%2520manual%250Amodifications%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520superior%2520performance%2520in%250Along-term%2520tasks.%2520Our%2520project%2520page%2520is%2520available%2520at%253A%250Ahttps%253A//bjhyzj.github.io/dovsg-web.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11989v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Long-term%20Language-Guided%0A%20%20Mobile%20Manipulation&entry.906535625=Zhijie%20Yan%20and%20Shufei%20Li%20and%20Zuoxu%20Wang%20and%20Lixiu%20Wu%20and%20Han%20Wang%20and%20Jun%20Zhu%20and%20Lijiang%20Chen%20and%20Jihong%20Liu&entry.1292438233=%20%20Enabling%20mobile%20robots%20to%20perform%20long-term%20tasks%20in%20dynamic%20real-world%0Aenvironments%20is%20a%20formidable%20challenge%2C%20especially%20when%20the%20environment%20changes%0Afrequently%20due%20to%20human-robot%20interactions%20or%20the%20robot%27s%20own%20actions.%0ATraditional%20methods%20typically%20assume%20static%20scenes%2C%20which%20limits%20their%0Aapplicability%20in%20the%20continuously%20changing%20real%20world.%20To%20overcome%20these%0Alimitations%2C%20we%20present%20DovSG%2C%20a%20novel%20mobile%20manipulation%20framework%20that%0Aleverages%20dynamic%20open-vocabulary%203D%20scene%20graphs%20and%20a%20language-guided%20task%0Aplanning%20module%20for%20long-term%20task%20execution.%20DovSG%20takes%20RGB-D%20sequences%20as%0Ainput%20and%20utilizes%20vision-language%20models%20%28VLMs%29%20for%20object%20detection%20to%20obtain%0Ahigh-level%20object%20semantic%20features.%20Based%20on%20the%20segmented%20objects%2C%20a%0Astructured%203D%20scene%20graph%20is%20generated%20for%20low-level%20spatial%20relationships.%0AFurthermore%2C%20an%20efficient%20mechanism%20for%20locally%20updating%20the%20scene%20graph%2C%0Aallows%20the%20robot%20to%20adjust%20parts%20of%20the%20graph%20dynamically%20during%20interactions%0Awithout%20the%20need%20for%20full%20scene%20reconstruction.%20This%20mechanism%20is%20particularly%0Avaluable%20in%20dynamic%20environments%2C%20enabling%20the%20robot%20to%20continually%20adapt%20to%0Ascene%20changes%20and%20effectively%20support%20the%20execution%20of%20long-term%20tasks.%20We%0Avalidated%20our%20system%20in%20real-world%20environments%20with%20varying%20degrees%20of%20manual%0Amodifications%2C%20demonstrating%20its%20effectiveness%20and%20superior%20performance%20in%0Along-term%20tasks.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//bjhyzj.github.io/dovsg-web.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11989v5&entry.124074799=Read"},
{"title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement\n  Learning", "author": "Jonas Gehring and Kunhao Zheng and Jade Copet and Vegard Mella and Quentin Carbonneaux and Taco Cohen and Gabriel Synnaeve", "abstract": "  Large language models (LLMs) deployed as agents solve user-specified tasks\nover multiple steps while keeping the required manual engagement to a minimum.\nCrucially, such LLMs need to ground their generations in any feedback obtained\nto reliably achieve the desired outcomes. We propose an end-to-end\nreinforcement learning method for teaching models to leverage execution\nfeedback in the realm of code synthesis, where state-of-the-art LLMs struggle\nto improve code iteratively compared to independent sampling. We benchmark on\ncompetitive programming tasks, where we achieve new state-of-the art results\nwith both small (8B parameters) and large (70B) models while reducing the\namount of samples required by an order of magnitude. Our analysis of\ninference-time behavior demonstrates that our method produces LLMs that\neffectively leverage automatic feedback over multiple steps.\n", "link": "http://arxiv.org/abs/2410.02089v2", "date": "2025-02-18", "relevancy": 2.4868, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLEF%3A%20Grounding%20Code%20LLMs%20in%20Execution%20Feedback%20with%20Reinforcement%0A%20%20Learning&body=Title%3A%20RLEF%3A%20Grounding%20Code%20LLMs%20in%20Execution%20Feedback%20with%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Jonas%20Gehring%20and%20Kunhao%20Zheng%20and%20Jade%20Copet%20and%20Vegard%20Mella%20and%20Quentin%20Carbonneaux%20and%20Taco%20Cohen%20and%20Gabriel%20Synnaeve%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20deployed%20as%20agents%20solve%20user-specified%20tasks%0Aover%20multiple%20steps%20while%20keeping%20the%20required%20manual%20engagement%20to%20a%20minimum.%0ACrucially%2C%20such%20LLMs%20need%20to%20ground%20their%20generations%20in%20any%20feedback%20obtained%0Ato%20reliably%20achieve%20the%20desired%20outcomes.%20We%20propose%20an%20end-to-end%0Areinforcement%20learning%20method%20for%20teaching%20models%20to%20leverage%20execution%0Afeedback%20in%20the%20realm%20of%20code%20synthesis%2C%20where%20state-of-the-art%20LLMs%20struggle%0Ato%20improve%20code%20iteratively%20compared%20to%20independent%20sampling.%20We%20benchmark%20on%0Acompetitive%20programming%20tasks%2C%20where%20we%20achieve%20new%20state-of-the%20art%20results%0Awith%20both%20small%20%288B%20parameters%29%20and%20large%20%2870B%29%20models%20while%20reducing%20the%0Aamount%20of%20samples%20required%20by%20an%20order%20of%20magnitude.%20Our%20analysis%20of%0Ainference-time%20behavior%20demonstrates%20that%20our%20method%20produces%20LLMs%20that%0Aeffectively%20leverage%20automatic%20feedback%20over%20multiple%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLEF%253A%2520Grounding%2520Code%2520LLMs%2520in%2520Execution%2520Feedback%2520with%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DJonas%2520Gehring%2520and%2520Kunhao%2520Zheng%2520and%2520Jade%2520Copet%2520and%2520Vegard%2520Mella%2520and%2520Quentin%2520Carbonneaux%2520and%2520Taco%2520Cohen%2520and%2520Gabriel%2520Synnaeve%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520deployed%2520as%2520agents%2520solve%2520user-specified%2520tasks%250Aover%2520multiple%2520steps%2520while%2520keeping%2520the%2520required%2520manual%2520engagement%2520to%2520a%2520minimum.%250ACrucially%252C%2520such%2520LLMs%2520need%2520to%2520ground%2520their%2520generations%2520in%2520any%2520feedback%2520obtained%250Ato%2520reliably%2520achieve%2520the%2520desired%2520outcomes.%2520We%2520propose%2520an%2520end-to-end%250Areinforcement%2520learning%2520method%2520for%2520teaching%2520models%2520to%2520leverage%2520execution%250Afeedback%2520in%2520the%2520realm%2520of%2520code%2520synthesis%252C%2520where%2520state-of-the-art%2520LLMs%2520struggle%250Ato%2520improve%2520code%2520iteratively%2520compared%2520to%2520independent%2520sampling.%2520We%2520benchmark%2520on%250Acompetitive%2520programming%2520tasks%252C%2520where%2520we%2520achieve%2520new%2520state-of-the%2520art%2520results%250Awith%2520both%2520small%2520%25288B%2520parameters%2529%2520and%2520large%2520%252870B%2529%2520models%2520while%2520reducing%2520the%250Aamount%2520of%2520samples%2520required%2520by%2520an%2520order%2520of%2520magnitude.%2520Our%2520analysis%2520of%250Ainference-time%2520behavior%2520demonstrates%2520that%2520our%2520method%2520produces%2520LLMs%2520that%250Aeffectively%2520leverage%2520automatic%2520feedback%2520over%2520multiple%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLEF%3A%20Grounding%20Code%20LLMs%20in%20Execution%20Feedback%20with%20Reinforcement%0A%20%20Learning&entry.906535625=Jonas%20Gehring%20and%20Kunhao%20Zheng%20and%20Jade%20Copet%20and%20Vegard%20Mella%20and%20Quentin%20Carbonneaux%20and%20Taco%20Cohen%20and%20Gabriel%20Synnaeve&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20deployed%20as%20agents%20solve%20user-specified%20tasks%0Aover%20multiple%20steps%20while%20keeping%20the%20required%20manual%20engagement%20to%20a%20minimum.%0ACrucially%2C%20such%20LLMs%20need%20to%20ground%20their%20generations%20in%20any%20feedback%20obtained%0Ato%20reliably%20achieve%20the%20desired%20outcomes.%20We%20propose%20an%20end-to-end%0Areinforcement%20learning%20method%20for%20teaching%20models%20to%20leverage%20execution%0Afeedback%20in%20the%20realm%20of%20code%20synthesis%2C%20where%20state-of-the-art%20LLMs%20struggle%0Ato%20improve%20code%20iteratively%20compared%20to%20independent%20sampling.%20We%20benchmark%20on%0Acompetitive%20programming%20tasks%2C%20where%20we%20achieve%20new%20state-of-the%20art%20results%0Awith%20both%20small%20%288B%20parameters%29%20and%20large%20%2870B%29%20models%20while%20reducing%20the%0Aamount%20of%20samples%20required%20by%20an%20order%20of%20magnitude.%20Our%20analysis%20of%0Ainference-time%20behavior%20demonstrates%20that%20our%20method%20produces%20LLMs%20that%0Aeffectively%20leverage%20automatic%20feedback%20over%20multiple%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02089v2&entry.124074799=Read"},
{"title": "Universal Embedding Function for Traffic Classification via QUIC Domain\n  Recognition Pretraining: A Transfer Learning Success", "author": "Jan Luxemburk and Karel Hynek and Richard Pln\u00fd and Tom\u00e1\u0161 \u010cejka", "abstract": "  Encrypted traffic classification (TC) methods must adapt to new protocols and\nextensions as well as to advancements in other machine learning fields. In this\npaper, we follow a transfer learning setup best known from computer vision. We\nfirst pretrain an embedding model on a complex task with a large number of\nclasses and then transfer it to five well-known TC datasets. The pretraining\ntask is recognition of SNI domains in encrypted QUIC traffic, which in itself\nis a problem for network monitoring due to the growing adoption of TLS\nEncrypted Client Hello. Our training pipeline -- featuring a disjoint class\nsetup, ArcFace loss function, and a modern deep learning architecture -- aims\nto produce universal embeddings applicable across tasks. The proposed solution,\nbased on nearest neighbors search in the embedding space, surpasses SOTA\nperformance on four of the five TC datasets. A comparison with a baseline\nmethod utilizing raw packet sequences revealed unexpected findings with\npotential implications for the broader TC field. We published the model\narchitecture, trained weights, and transfer learning experiments.\n", "link": "http://arxiv.org/abs/2502.12930v1", "date": "2025-02-18", "relevancy": 2.4865, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Embedding%20Function%20for%20Traffic%20Classification%20via%20QUIC%20Domain%0A%20%20Recognition%20Pretraining%3A%20A%20Transfer%20Learning%20Success&body=Title%3A%20Universal%20Embedding%20Function%20for%20Traffic%20Classification%20via%20QUIC%20Domain%0A%20%20Recognition%20Pretraining%3A%20A%20Transfer%20Learning%20Success%0AAuthor%3A%20Jan%20Luxemburk%20and%20Karel%20Hynek%20and%20Richard%20Pln%C3%BD%20and%20Tom%C3%A1%C5%A1%20%C4%8Cejka%0AAbstract%3A%20%20%20Encrypted%20traffic%20classification%20%28TC%29%20methods%20must%20adapt%20to%20new%20protocols%20and%0Aextensions%20as%20well%20as%20to%20advancements%20in%20other%20machine%20learning%20fields.%20In%20this%0Apaper%2C%20we%20follow%20a%20transfer%20learning%20setup%20best%20known%20from%20computer%20vision.%20We%0Afirst%20pretrain%20an%20embedding%20model%20on%20a%20complex%20task%20with%20a%20large%20number%20of%0Aclasses%20and%20then%20transfer%20it%20to%20five%20well-known%20TC%20datasets.%20The%20pretraining%0Atask%20is%20recognition%20of%20SNI%20domains%20in%20encrypted%20QUIC%20traffic%2C%20which%20in%20itself%0Ais%20a%20problem%20for%20network%20monitoring%20due%20to%20the%20growing%20adoption%20of%20TLS%0AEncrypted%20Client%20Hello.%20Our%20training%20pipeline%20--%20featuring%20a%20disjoint%20class%0Asetup%2C%20ArcFace%20loss%20function%2C%20and%20a%20modern%20deep%20learning%20architecture%20--%20aims%0Ato%20produce%20universal%20embeddings%20applicable%20across%20tasks.%20The%20proposed%20solution%2C%0Abased%20on%20nearest%20neighbors%20search%20in%20the%20embedding%20space%2C%20surpasses%20SOTA%0Aperformance%20on%20four%20of%20the%20five%20TC%20datasets.%20A%20comparison%20with%20a%20baseline%0Amethod%20utilizing%20raw%20packet%20sequences%20revealed%20unexpected%20findings%20with%0Apotential%20implications%20for%20the%20broader%20TC%20field.%20We%20published%20the%20model%0Aarchitecture%2C%20trained%20weights%2C%20and%20transfer%20learning%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Embedding%2520Function%2520for%2520Traffic%2520Classification%2520via%2520QUIC%2520Domain%250A%2520%2520Recognition%2520Pretraining%253A%2520A%2520Transfer%2520Learning%2520Success%26entry.906535625%3DJan%2520Luxemburk%2520and%2520Karel%2520Hynek%2520and%2520Richard%2520Pln%25C3%25BD%2520and%2520Tom%25C3%25A1%25C5%25A1%2520%25C4%258Cejka%26entry.1292438233%3D%2520%2520Encrypted%2520traffic%2520classification%2520%2528TC%2529%2520methods%2520must%2520adapt%2520to%2520new%2520protocols%2520and%250Aextensions%2520as%2520well%2520as%2520to%2520advancements%2520in%2520other%2520machine%2520learning%2520fields.%2520In%2520this%250Apaper%252C%2520we%2520follow%2520a%2520transfer%2520learning%2520setup%2520best%2520known%2520from%2520computer%2520vision.%2520We%250Afirst%2520pretrain%2520an%2520embedding%2520model%2520on%2520a%2520complex%2520task%2520with%2520a%2520large%2520number%2520of%250Aclasses%2520and%2520then%2520transfer%2520it%2520to%2520five%2520well-known%2520TC%2520datasets.%2520The%2520pretraining%250Atask%2520is%2520recognition%2520of%2520SNI%2520domains%2520in%2520encrypted%2520QUIC%2520traffic%252C%2520which%2520in%2520itself%250Ais%2520a%2520problem%2520for%2520network%2520monitoring%2520due%2520to%2520the%2520growing%2520adoption%2520of%2520TLS%250AEncrypted%2520Client%2520Hello.%2520Our%2520training%2520pipeline%2520--%2520featuring%2520a%2520disjoint%2520class%250Asetup%252C%2520ArcFace%2520loss%2520function%252C%2520and%2520a%2520modern%2520deep%2520learning%2520architecture%2520--%2520aims%250Ato%2520produce%2520universal%2520embeddings%2520applicable%2520across%2520tasks.%2520The%2520proposed%2520solution%252C%250Abased%2520on%2520nearest%2520neighbors%2520search%2520in%2520the%2520embedding%2520space%252C%2520surpasses%2520SOTA%250Aperformance%2520on%2520four%2520of%2520the%2520five%2520TC%2520datasets.%2520A%2520comparison%2520with%2520a%2520baseline%250Amethod%2520utilizing%2520raw%2520packet%2520sequences%2520revealed%2520unexpected%2520findings%2520with%250Apotential%2520implications%2520for%2520the%2520broader%2520TC%2520field.%2520We%2520published%2520the%2520model%250Aarchitecture%252C%2520trained%2520weights%252C%2520and%2520transfer%2520learning%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Embedding%20Function%20for%20Traffic%20Classification%20via%20QUIC%20Domain%0A%20%20Recognition%20Pretraining%3A%20A%20Transfer%20Learning%20Success&entry.906535625=Jan%20Luxemburk%20and%20Karel%20Hynek%20and%20Richard%20Pln%C3%BD%20and%20Tom%C3%A1%C5%A1%20%C4%8Cejka&entry.1292438233=%20%20Encrypted%20traffic%20classification%20%28TC%29%20methods%20must%20adapt%20to%20new%20protocols%20and%0Aextensions%20as%20well%20as%20to%20advancements%20in%20other%20machine%20learning%20fields.%20In%20this%0Apaper%2C%20we%20follow%20a%20transfer%20learning%20setup%20best%20known%20from%20computer%20vision.%20We%0Afirst%20pretrain%20an%20embedding%20model%20on%20a%20complex%20task%20with%20a%20large%20number%20of%0Aclasses%20and%20then%20transfer%20it%20to%20five%20well-known%20TC%20datasets.%20The%20pretraining%0Atask%20is%20recognition%20of%20SNI%20domains%20in%20encrypted%20QUIC%20traffic%2C%20which%20in%20itself%0Ais%20a%20problem%20for%20network%20monitoring%20due%20to%20the%20growing%20adoption%20of%20TLS%0AEncrypted%20Client%20Hello.%20Our%20training%20pipeline%20--%20featuring%20a%20disjoint%20class%0Asetup%2C%20ArcFace%20loss%20function%2C%20and%20a%20modern%20deep%20learning%20architecture%20--%20aims%0Ato%20produce%20universal%20embeddings%20applicable%20across%20tasks.%20The%20proposed%20solution%2C%0Abased%20on%20nearest%20neighbors%20search%20in%20the%20embedding%20space%2C%20surpasses%20SOTA%0Aperformance%20on%20four%20of%20the%20five%20TC%20datasets.%20A%20comparison%20with%20a%20baseline%0Amethod%20utilizing%20raw%20packet%20sequences%20revealed%20unexpected%20findings%20with%0Apotential%20implications%20for%20the%20broader%20TC%20field.%20We%20published%20the%20model%0Aarchitecture%2C%20trained%20weights%2C%20and%20transfer%20learning%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12930v1&entry.124074799=Read"},
{"title": "The Responsible Development of Automated Student Feedback with\n  Generative AI", "author": "Euan D Lindsay and Mike Zhang and Aditya Johri and Johannes Bjerva", "abstract": "  Providing rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]\n", "link": "http://arxiv.org/abs/2308.15334v3", "date": "2025-02-18", "relevancy": 2.4653, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5298}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4843}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Responsible%20Development%20of%20Automated%20Student%20Feedback%20with%0A%20%20Generative%20AI&body=Title%3A%20The%20Responsible%20Development%20of%20Automated%20Student%20Feedback%20with%0A%20%20Generative%20AI%0AAuthor%3A%20Euan%20D%20Lindsay%20and%20Mike%20Zhang%20and%20Aditya%20Johri%20and%20Johannes%20Bjerva%0AAbstract%3A%20%20%20Providing%20rich%2C%20constructive%20feedback%20to%20students%20is%20essential%20for%20supporting%0Aand%20enhancing%20their%20learning.%20Recent%20advancements%20in%20Generative%20Artificial%0AIntelligence%20%28AI%29%2C%20particularly%20with%20large%20language%20models%20%28LLMs%29%2C%20present%20new%0Aopportunities%20to%20deliver%20scalable%2C%20repeatable%2C%20and%20instant%20feedback%2C%0Aeffectively%20making%20abundant%20a%20resource%20that%20has%20historically%20been%20scarce%20and%0Acostly.%20From%20a%20technical%20perspective%2C%20this%20approach%20is%20now%20feasible%20due%20to%0Abreakthroughs%20in%20AI%20and%20Natural%20Language%20Processing%20%28NLP%29.%20While%20the%20potential%0Aeducational%20benefits%20are%20compelling%2C%20implementing%20these%20technologies%20also%0Aintroduces%20a%20host%20of%20ethical%20considerations%20that%20must%20be%20thoughtfully%0Aaddressed.%20One%20of%20the%20core%20advantages%20of%20AI%20systems%20is%20their%20ability%20to%0Aautomate%20routine%20and%20mundane%20tasks%2C%20potentially%20freeing%20up%20human%20educators%20for%0Amore%20nuanced%20work.%20However%2C%20the%20ease%20of%20automation%20risks%20a%20%60%60tyranny%20of%20the%0Amajority%27%27%2C%20where%20the%20diverse%20needs%20of%20minority%20or%20unique%20learners%20are%0Aoverlooked%2C%20as%20they%20may%20be%20harder%20to%20systematize%20and%20less%20straightforward%20to%0Aaccommodate.%20Ensuring%20inclusivity%20and%20equity%20in%20AI-generated%20feedback%2C%0Atherefore%2C%20becomes%20a%20critical%20aspect%20of%20responsible%20AI%20implementation%20in%0Aeducation.%20The%20process%20of%20developing%20machine%20learning%20models%20that%20produce%0Avaluable%2C%20personalized%2C%20and%20authentic%20feedback%20also%20requires%20significant%20input%0Afrom%20human%20domain%20experts.%20Decisions%20around%20whose%20expertise%20is%20incorporated%2C%0Ahow%20it%20is%20captured%2C%20and%20when%20it%20is%20applied%20have%20profound%20implications%20for%20the%0Arelevance%20and%20quality%20of%20the%20resulting%20feedback.%20Additionally%2C%20the%20maintenance%0Aand%20continuous%20refinement%20of%20these%20models%20are%20necessary%20to%20adapt%20feedback%20to%0Aevolving%20contextual%2C%20theoretical%2C%20and%20student-related%20factors.%20Without%20ongoing%0Aadaptation%2C%20feedback%20risks%20becoming%20obsolete%20or%20mismatched%20with%20the%20current%0Aneeds%20of%20diverse%20student%20populations%20%5B...%5D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.15334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Responsible%2520Development%2520of%2520Automated%2520Student%2520Feedback%2520with%250A%2520%2520Generative%2520AI%26entry.906535625%3DEuan%2520D%2520Lindsay%2520and%2520Mike%2520Zhang%2520and%2520Aditya%2520Johri%2520and%2520Johannes%2520Bjerva%26entry.1292438233%3D%2520%2520Providing%2520rich%252C%2520constructive%2520feedback%2520to%2520students%2520is%2520essential%2520for%2520supporting%250Aand%2520enhancing%2520their%2520learning.%2520Recent%2520advancements%2520in%2520Generative%2520Artificial%250AIntelligence%2520%2528AI%2529%252C%2520particularly%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520present%2520new%250Aopportunities%2520to%2520deliver%2520scalable%252C%2520repeatable%252C%2520and%2520instant%2520feedback%252C%250Aeffectively%2520making%2520abundant%2520a%2520resource%2520that%2520has%2520historically%2520been%2520scarce%2520and%250Acostly.%2520From%2520a%2520technical%2520perspective%252C%2520this%2520approach%2520is%2520now%2520feasible%2520due%2520to%250Abreakthroughs%2520in%2520AI%2520and%2520Natural%2520Language%2520Processing%2520%2528NLP%2529.%2520While%2520the%2520potential%250Aeducational%2520benefits%2520are%2520compelling%252C%2520implementing%2520these%2520technologies%2520also%250Aintroduces%2520a%2520host%2520of%2520ethical%2520considerations%2520that%2520must%2520be%2520thoughtfully%250Aaddressed.%2520One%2520of%2520the%2520core%2520advantages%2520of%2520AI%2520systems%2520is%2520their%2520ability%2520to%250Aautomate%2520routine%2520and%2520mundane%2520tasks%252C%2520potentially%2520freeing%2520up%2520human%2520educators%2520for%250Amore%2520nuanced%2520work.%2520However%252C%2520the%2520ease%2520of%2520automation%2520risks%2520a%2520%2560%2560tyranny%2520of%2520the%250Amajority%2527%2527%252C%2520where%2520the%2520diverse%2520needs%2520of%2520minority%2520or%2520unique%2520learners%2520are%250Aoverlooked%252C%2520as%2520they%2520may%2520be%2520harder%2520to%2520systematize%2520and%2520less%2520straightforward%2520to%250Aaccommodate.%2520Ensuring%2520inclusivity%2520and%2520equity%2520in%2520AI-generated%2520feedback%252C%250Atherefore%252C%2520becomes%2520a%2520critical%2520aspect%2520of%2520responsible%2520AI%2520implementation%2520in%250Aeducation.%2520The%2520process%2520of%2520developing%2520machine%2520learning%2520models%2520that%2520produce%250Avaluable%252C%2520personalized%252C%2520and%2520authentic%2520feedback%2520also%2520requires%2520significant%2520input%250Afrom%2520human%2520domain%2520experts.%2520Decisions%2520around%2520whose%2520expertise%2520is%2520incorporated%252C%250Ahow%2520it%2520is%2520captured%252C%2520and%2520when%2520it%2520is%2520applied%2520have%2520profound%2520implications%2520for%2520the%250Arelevance%2520and%2520quality%2520of%2520the%2520resulting%2520feedback.%2520Additionally%252C%2520the%2520maintenance%250Aand%2520continuous%2520refinement%2520of%2520these%2520models%2520are%2520necessary%2520to%2520adapt%2520feedback%2520to%250Aevolving%2520contextual%252C%2520theoretical%252C%2520and%2520student-related%2520factors.%2520Without%2520ongoing%250Aadaptation%252C%2520feedback%2520risks%2520becoming%2520obsolete%2520or%2520mismatched%2520with%2520the%2520current%250Aneeds%2520of%2520diverse%2520student%2520populations%2520%255B...%255D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.15334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Responsible%20Development%20of%20Automated%20Student%20Feedback%20with%0A%20%20Generative%20AI&entry.906535625=Euan%20D%20Lindsay%20and%20Mike%20Zhang%20and%20Aditya%20Johri%20and%20Johannes%20Bjerva&entry.1292438233=%20%20Providing%20rich%2C%20constructive%20feedback%20to%20students%20is%20essential%20for%20supporting%0Aand%20enhancing%20their%20learning.%20Recent%20advancements%20in%20Generative%20Artificial%0AIntelligence%20%28AI%29%2C%20particularly%20with%20large%20language%20models%20%28LLMs%29%2C%20present%20new%0Aopportunities%20to%20deliver%20scalable%2C%20repeatable%2C%20and%20instant%20feedback%2C%0Aeffectively%20making%20abundant%20a%20resource%20that%20has%20historically%20been%20scarce%20and%0Acostly.%20From%20a%20technical%20perspective%2C%20this%20approach%20is%20now%20feasible%20due%20to%0Abreakthroughs%20in%20AI%20and%20Natural%20Language%20Processing%20%28NLP%29.%20While%20the%20potential%0Aeducational%20benefits%20are%20compelling%2C%20implementing%20these%20technologies%20also%0Aintroduces%20a%20host%20of%20ethical%20considerations%20that%20must%20be%20thoughtfully%0Aaddressed.%20One%20of%20the%20core%20advantages%20of%20AI%20systems%20is%20their%20ability%20to%0Aautomate%20routine%20and%20mundane%20tasks%2C%20potentially%20freeing%20up%20human%20educators%20for%0Amore%20nuanced%20work.%20However%2C%20the%20ease%20of%20automation%20risks%20a%20%60%60tyranny%20of%20the%0Amajority%27%27%2C%20where%20the%20diverse%20needs%20of%20minority%20or%20unique%20learners%20are%0Aoverlooked%2C%20as%20they%20may%20be%20harder%20to%20systematize%20and%20less%20straightforward%20to%0Aaccommodate.%20Ensuring%20inclusivity%20and%20equity%20in%20AI-generated%20feedback%2C%0Atherefore%2C%20becomes%20a%20critical%20aspect%20of%20responsible%20AI%20implementation%20in%0Aeducation.%20The%20process%20of%20developing%20machine%20learning%20models%20that%20produce%0Avaluable%2C%20personalized%2C%20and%20authentic%20feedback%20also%20requires%20significant%20input%0Afrom%20human%20domain%20experts.%20Decisions%20around%20whose%20expertise%20is%20incorporated%2C%0Ahow%20it%20is%20captured%2C%20and%20when%20it%20is%20applied%20have%20profound%20implications%20for%20the%0Arelevance%20and%20quality%20of%20the%20resulting%20feedback.%20Additionally%2C%20the%20maintenance%0Aand%20continuous%20refinement%20of%20these%20models%20are%20necessary%20to%20adapt%20feedback%20to%0Aevolving%20contextual%2C%20theoretical%2C%20and%20student-related%20factors.%20Without%20ongoing%0Aadaptation%2C%20feedback%20risks%20becoming%20obsolete%20or%20mismatched%20with%20the%20current%0Aneeds%20of%20diverse%20student%20populations%20%5B...%5D%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15334v3&entry.124074799=Read"},
{"title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CoALM: A Unified Conversational Agentic Language Model", "author": "Emre Can Acikgoz and Jeremiah Greer and Akul Datta and Ze Yang and William Zeng and Oussama Elachqar and Emmanouil Koukoumidis and Dilek Hakkani-T\u00fcr and Gokhan Tur", "abstract": "  Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CoALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CoALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,\nand CoALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.This demonstrates the feasibility of a single model\napproach for both TOD and LA, setting a new standard for conversational agents.\n", "link": "http://arxiv.org/abs/2502.08820v2", "date": "2025-02-18", "relevancy": 2.4629, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20a%20Single%20Model%20Master%20Both%20Multi-turn%20Conversations%20and%20Tool%20Use%3F%0A%20%20CoALM%3A%20A%20Unified%20Conversational%20Agentic%20Language%20Model&body=Title%3A%20Can%20a%20Single%20Model%20Master%20Both%20Multi-turn%20Conversations%20and%20Tool%20Use%3F%0A%20%20CoALM%3A%20A%20Unified%20Conversational%20Agentic%20Language%20Model%0AAuthor%3A%20Emre%20Can%20Acikgoz%20and%20Jeremiah%20Greer%20and%20Akul%20Datta%20and%20Ze%20Yang%20and%20William%20Zeng%20and%20Oussama%20Elachqar%20and%20Emmanouil%20Koukoumidis%20and%20Dilek%20Hakkani-T%C3%BCr%20and%20Gokhan%20Tur%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20with%20API-calling%20capabilities%20enabled%20building%0Aeffective%20Language%20Agents%20%28LA%29%2C%20while%20also%20revolutionizing%20the%20conventional%0Atask-oriented%20dialogue%20%28TOD%29%20paradigm.%20However%2C%20current%20approaches%20face%20a%0Acritical%20dilemma%3A%20TOD%20systems%20are%20often%20trained%20on%20a%20limited%20set%20of%20target%0AAPIs%2C%20requiring%20new%20data%20to%20maintain%20their%20quality%20when%20interfacing%20with%20new%0Aservices%2C%20while%20LAs%20are%20not%20trained%20to%20maintain%20user%20intent%20over%20multi-turn%0Aconversations.%20Because%20both%20robust%20multi-turn%20management%20and%20advanced%20function%0Acalling%20are%20crucial%20for%20effective%20conversational%20agents%2C%20we%20evaluate%20these%0Askills%20on%20three%20popular%20benchmarks%3A%20MultiWOZ%202.4%20%28TOD%29%2C%20BFCL%20V3%20%28LA%29%2C%20and%0AAPI-Bank%20%28LA%29%2C%20and%20our%20analyses%20reveal%20that%20specialized%20approaches%20excel%20in%20one%0Adomain%20but%20underperform%20in%20the%20other.%20To%20bridge%20this%20chasm%2C%20we%20introduce%20CoALM%0A%28Conversational%20Agentic%20Language%20Model%29%2C%20a%20unified%20approach%20that%20integrates%0Aboth%20conversational%20and%20agentic%20capabilities.%20We%20created%20CoALM-IT%2C%20a%20carefully%0Aconstructed%20multi-task%20dataset%20that%20interleave%20multi-turn%20ReAct%20reasoning%20with%0Acomplex%20API%20usage.%20Using%20CoALM-IT%2C%20we%20train%20three%20models%20CoALM%208B%2C%20CoALM%2070B%2C%0Aand%20CoALM%20405B%2C%20which%20outperform%20top%20domain-specific%20models%2C%20including%20GPT-4o%2C%0Aacross%20all%20three%20benchmarks.This%20demonstrates%20the%20feasibility%20of%20a%20single%20model%0Aapproach%20for%20both%20TOD%20and%20LA%2C%20setting%20a%20new%20standard%20for%20conversational%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520a%2520Single%2520Model%2520Master%2520Both%2520Multi-turn%2520Conversations%2520and%2520Tool%2520Use%253F%250A%2520%2520CoALM%253A%2520A%2520Unified%2520Conversational%2520Agentic%2520Language%2520Model%26entry.906535625%3DEmre%2520Can%2520Acikgoz%2520and%2520Jeremiah%2520Greer%2520and%2520Akul%2520Datta%2520and%2520Ze%2520Yang%2520and%2520William%2520Zeng%2520and%2520Oussama%2520Elachqar%2520and%2520Emmanouil%2520Koukoumidis%2520and%2520Dilek%2520Hakkani-T%25C3%25BCr%2520and%2520Gokhan%2520Tur%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520API-calling%2520capabilities%2520enabled%2520building%250Aeffective%2520Language%2520Agents%2520%2528LA%2529%252C%2520while%2520also%2520revolutionizing%2520the%2520conventional%250Atask-oriented%2520dialogue%2520%2528TOD%2529%2520paradigm.%2520However%252C%2520current%2520approaches%2520face%2520a%250Acritical%2520dilemma%253A%2520TOD%2520systems%2520are%2520often%2520trained%2520on%2520a%2520limited%2520set%2520of%2520target%250AAPIs%252C%2520requiring%2520new%2520data%2520to%2520maintain%2520their%2520quality%2520when%2520interfacing%2520with%2520new%250Aservices%252C%2520while%2520LAs%2520are%2520not%2520trained%2520to%2520maintain%2520user%2520intent%2520over%2520multi-turn%250Aconversations.%2520Because%2520both%2520robust%2520multi-turn%2520management%2520and%2520advanced%2520function%250Acalling%2520are%2520crucial%2520for%2520effective%2520conversational%2520agents%252C%2520we%2520evaluate%2520these%250Askills%2520on%2520three%2520popular%2520benchmarks%253A%2520MultiWOZ%25202.4%2520%2528TOD%2529%252C%2520BFCL%2520V3%2520%2528LA%2529%252C%2520and%250AAPI-Bank%2520%2528LA%2529%252C%2520and%2520our%2520analyses%2520reveal%2520that%2520specialized%2520approaches%2520excel%2520in%2520one%250Adomain%2520but%2520underperform%2520in%2520the%2520other.%2520To%2520bridge%2520this%2520chasm%252C%2520we%2520introduce%2520CoALM%250A%2528Conversational%2520Agentic%2520Language%2520Model%2529%252C%2520a%2520unified%2520approach%2520that%2520integrates%250Aboth%2520conversational%2520and%2520agentic%2520capabilities.%2520We%2520created%2520CoALM-IT%252C%2520a%2520carefully%250Aconstructed%2520multi-task%2520dataset%2520that%2520interleave%2520multi-turn%2520ReAct%2520reasoning%2520with%250Acomplex%2520API%2520usage.%2520Using%2520CoALM-IT%252C%2520we%2520train%2520three%2520models%2520CoALM%25208B%252C%2520CoALM%252070B%252C%250Aand%2520CoALM%2520405B%252C%2520which%2520outperform%2520top%2520domain-specific%2520models%252C%2520including%2520GPT-4o%252C%250Aacross%2520all%2520three%2520benchmarks.This%2520demonstrates%2520the%2520feasibility%2520of%2520a%2520single%2520model%250Aapproach%2520for%2520both%2520TOD%2520and%2520LA%252C%2520setting%2520a%2520new%2520standard%2520for%2520conversational%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20a%20Single%20Model%20Master%20Both%20Multi-turn%20Conversations%20and%20Tool%20Use%3F%0A%20%20CoALM%3A%20A%20Unified%20Conversational%20Agentic%20Language%20Model&entry.906535625=Emre%20Can%20Acikgoz%20and%20Jeremiah%20Greer%20and%20Akul%20Datta%20and%20Ze%20Yang%20and%20William%20Zeng%20and%20Oussama%20Elachqar%20and%20Emmanouil%20Koukoumidis%20and%20Dilek%20Hakkani-T%C3%BCr%20and%20Gokhan%20Tur&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20with%20API-calling%20capabilities%20enabled%20building%0Aeffective%20Language%20Agents%20%28LA%29%2C%20while%20also%20revolutionizing%20the%20conventional%0Atask-oriented%20dialogue%20%28TOD%29%20paradigm.%20However%2C%20current%20approaches%20face%20a%0Acritical%20dilemma%3A%20TOD%20systems%20are%20often%20trained%20on%20a%20limited%20set%20of%20target%0AAPIs%2C%20requiring%20new%20data%20to%20maintain%20their%20quality%20when%20interfacing%20with%20new%0Aservices%2C%20while%20LAs%20are%20not%20trained%20to%20maintain%20user%20intent%20over%20multi-turn%0Aconversations.%20Because%20both%20robust%20multi-turn%20management%20and%20advanced%20function%0Acalling%20are%20crucial%20for%20effective%20conversational%20agents%2C%20we%20evaluate%20these%0Askills%20on%20three%20popular%20benchmarks%3A%20MultiWOZ%202.4%20%28TOD%29%2C%20BFCL%20V3%20%28LA%29%2C%20and%0AAPI-Bank%20%28LA%29%2C%20and%20our%20analyses%20reveal%20that%20specialized%20approaches%20excel%20in%20one%0Adomain%20but%20underperform%20in%20the%20other.%20To%20bridge%20this%20chasm%2C%20we%20introduce%20CoALM%0A%28Conversational%20Agentic%20Language%20Model%29%2C%20a%20unified%20approach%20that%20integrates%0Aboth%20conversational%20and%20agentic%20capabilities.%20We%20created%20CoALM-IT%2C%20a%20carefully%0Aconstructed%20multi-task%20dataset%20that%20interleave%20multi-turn%20ReAct%20reasoning%20with%0Acomplex%20API%20usage.%20Using%20CoALM-IT%2C%20we%20train%20three%20models%20CoALM%208B%2C%20CoALM%2070B%2C%0Aand%20CoALM%20405B%2C%20which%20outperform%20top%20domain-specific%20models%2C%20including%20GPT-4o%2C%0Aacross%20all%20three%20benchmarks.This%20demonstrates%20the%20feasibility%20of%20a%20single%20model%0Aapproach%20for%20both%20TOD%20and%20LA%2C%20setting%20a%20new%20standard%20for%20conversational%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08820v2&entry.124074799=Read"},
{"title": "Mean of Means: Human Localization with Calibration-free and\n  Unconstrained Camera Settings (extended version)", "author": "Tianyi Zhang and Wengyu Zhang and Xulu Zhang and Jiaxin Wu and Xiao-Yong Wei and Jiannong Cao and Qing Li", "abstract": "  Accurate human localization is crucial for various applications, especially\nin the Metaverse era. Existing high precision solutions rely on expensive,\ntag-dependent hardware, while vision-based methods offer a cheaper, tag-free\nalternative. However, current vision solutions based on stereo vision face\nlimitations due to rigid perspective transformation principles and error\npropagation in multi-stage SVD solvers. These solutions also require multiple\nhigh-resolution cameras with strict setup constraints.To address these\nlimitations, we propose a probabilistic approach that considers all points on\nthe human body as observations generated by a distribution centered around the\nbody's geometric center. This enables us to improve sampling significantly,\nincreasing the number of samples for each point of interest from hundreds to\nbillions. By modeling the relation between the means of the distributions of\nworld coordinates and pixel coordinates, leveraging the Central Limit Theorem,\nwe ensure normality and facilitate the learning process. Experimental results\ndemonstrate human localization accuracy of 96\\% within a 0.3$m$ range and\nnearly 100\\% accuracy within a 0.5$m$ range, achieved at a low cost of only 10\nUSD using two web cameras with a resolution of 640$\\times$480 pixels.\n", "link": "http://arxiv.org/abs/2502.13017v1", "date": "2025-02-18", "relevancy": 2.4575, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6253}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6116}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mean%20of%20Means%3A%20Human%20Localization%20with%20Calibration-free%20and%0A%20%20Unconstrained%20Camera%20Settings%20%28extended%20version%29&body=Title%3A%20Mean%20of%20Means%3A%20Human%20Localization%20with%20Calibration-free%20and%0A%20%20Unconstrained%20Camera%20Settings%20%28extended%20version%29%0AAuthor%3A%20Tianyi%20Zhang%20and%20Wengyu%20Zhang%20and%20Xulu%20Zhang%20and%20Jiaxin%20Wu%20and%20Xiao-Yong%20Wei%20and%20Jiannong%20Cao%20and%20Qing%20Li%0AAbstract%3A%20%20%20Accurate%20human%20localization%20is%20crucial%20for%20various%20applications%2C%20especially%0Ain%20the%20Metaverse%20era.%20Existing%20high%20precision%20solutions%20rely%20on%20expensive%2C%0Atag-dependent%20hardware%2C%20while%20vision-based%20methods%20offer%20a%20cheaper%2C%20tag-free%0Aalternative.%20However%2C%20current%20vision%20solutions%20based%20on%20stereo%20vision%20face%0Alimitations%20due%20to%20rigid%20perspective%20transformation%20principles%20and%20error%0Apropagation%20in%20multi-stage%20SVD%20solvers.%20These%20solutions%20also%20require%20multiple%0Ahigh-resolution%20cameras%20with%20strict%20setup%20constraints.To%20address%20these%0Alimitations%2C%20we%20propose%20a%20probabilistic%20approach%20that%20considers%20all%20points%20on%0Athe%20human%20body%20as%20observations%20generated%20by%20a%20distribution%20centered%20around%20the%0Abody%27s%20geometric%20center.%20This%20enables%20us%20to%20improve%20sampling%20significantly%2C%0Aincreasing%20the%20number%20of%20samples%20for%20each%20point%20of%20interest%20from%20hundreds%20to%0Abillions.%20By%20modeling%20the%20relation%20between%20the%20means%20of%20the%20distributions%20of%0Aworld%20coordinates%20and%20pixel%20coordinates%2C%20leveraging%20the%20Central%20Limit%20Theorem%2C%0Awe%20ensure%20normality%20and%20facilitate%20the%20learning%20process.%20Experimental%20results%0Ademonstrate%20human%20localization%20accuracy%20of%2096%5C%25%20within%20a%200.3%24m%24%20range%20and%0Anearly%20100%5C%25%20accuracy%20within%20a%200.5%24m%24%20range%2C%20achieved%20at%20a%20low%20cost%20of%20only%2010%0AUSD%20using%20two%20web%20cameras%20with%20a%20resolution%20of%20640%24%5Ctimes%24480%20pixels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMean%2520of%2520Means%253A%2520Human%2520Localization%2520with%2520Calibration-free%2520and%250A%2520%2520Unconstrained%2520Camera%2520Settings%2520%2528extended%2520version%2529%26entry.906535625%3DTianyi%2520Zhang%2520and%2520Wengyu%2520Zhang%2520and%2520Xulu%2520Zhang%2520and%2520Jiaxin%2520Wu%2520and%2520Xiao-Yong%2520Wei%2520and%2520Jiannong%2520Cao%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520Accurate%2520human%2520localization%2520is%2520crucial%2520for%2520various%2520applications%252C%2520especially%250Ain%2520the%2520Metaverse%2520era.%2520Existing%2520high%2520precision%2520solutions%2520rely%2520on%2520expensive%252C%250Atag-dependent%2520hardware%252C%2520while%2520vision-based%2520methods%2520offer%2520a%2520cheaper%252C%2520tag-free%250Aalternative.%2520However%252C%2520current%2520vision%2520solutions%2520based%2520on%2520stereo%2520vision%2520face%250Alimitations%2520due%2520to%2520rigid%2520perspective%2520transformation%2520principles%2520and%2520error%250Apropagation%2520in%2520multi-stage%2520SVD%2520solvers.%2520These%2520solutions%2520also%2520require%2520multiple%250Ahigh-resolution%2520cameras%2520with%2520strict%2520setup%2520constraints.To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520probabilistic%2520approach%2520that%2520considers%2520all%2520points%2520on%250Athe%2520human%2520body%2520as%2520observations%2520generated%2520by%2520a%2520distribution%2520centered%2520around%2520the%250Abody%2527s%2520geometric%2520center.%2520This%2520enables%2520us%2520to%2520improve%2520sampling%2520significantly%252C%250Aincreasing%2520the%2520number%2520of%2520samples%2520for%2520each%2520point%2520of%2520interest%2520from%2520hundreds%2520to%250Abillions.%2520By%2520modeling%2520the%2520relation%2520between%2520the%2520means%2520of%2520the%2520distributions%2520of%250Aworld%2520coordinates%2520and%2520pixel%2520coordinates%252C%2520leveraging%2520the%2520Central%2520Limit%2520Theorem%252C%250Awe%2520ensure%2520normality%2520and%2520facilitate%2520the%2520learning%2520process.%2520Experimental%2520results%250Ademonstrate%2520human%2520localization%2520accuracy%2520of%252096%255C%2525%2520within%2520a%25200.3%2524m%2524%2520range%2520and%250Anearly%2520100%255C%2525%2520accuracy%2520within%2520a%25200.5%2524m%2524%2520range%252C%2520achieved%2520at%2520a%2520low%2520cost%2520of%2520only%252010%250AUSD%2520using%2520two%2520web%2520cameras%2520with%2520a%2520resolution%2520of%2520640%2524%255Ctimes%2524480%2520pixels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mean%20of%20Means%3A%20Human%20Localization%20with%20Calibration-free%20and%0A%20%20Unconstrained%20Camera%20Settings%20%28extended%20version%29&entry.906535625=Tianyi%20Zhang%20and%20Wengyu%20Zhang%20and%20Xulu%20Zhang%20and%20Jiaxin%20Wu%20and%20Xiao-Yong%20Wei%20and%20Jiannong%20Cao%20and%20Qing%20Li&entry.1292438233=%20%20Accurate%20human%20localization%20is%20crucial%20for%20various%20applications%2C%20especially%0Ain%20the%20Metaverse%20era.%20Existing%20high%20precision%20solutions%20rely%20on%20expensive%2C%0Atag-dependent%20hardware%2C%20while%20vision-based%20methods%20offer%20a%20cheaper%2C%20tag-free%0Aalternative.%20However%2C%20current%20vision%20solutions%20based%20on%20stereo%20vision%20face%0Alimitations%20due%20to%20rigid%20perspective%20transformation%20principles%20and%20error%0Apropagation%20in%20multi-stage%20SVD%20solvers.%20These%20solutions%20also%20require%20multiple%0Ahigh-resolution%20cameras%20with%20strict%20setup%20constraints.To%20address%20these%0Alimitations%2C%20we%20propose%20a%20probabilistic%20approach%20that%20considers%20all%20points%20on%0Athe%20human%20body%20as%20observations%20generated%20by%20a%20distribution%20centered%20around%20the%0Abody%27s%20geometric%20center.%20This%20enables%20us%20to%20improve%20sampling%20significantly%2C%0Aincreasing%20the%20number%20of%20samples%20for%20each%20point%20of%20interest%20from%20hundreds%20to%0Abillions.%20By%20modeling%20the%20relation%20between%20the%20means%20of%20the%20distributions%20of%0Aworld%20coordinates%20and%20pixel%20coordinates%2C%20leveraging%20the%20Central%20Limit%20Theorem%2C%0Awe%20ensure%20normality%20and%20facilitate%20the%20learning%20process.%20Experimental%20results%0Ademonstrate%20human%20localization%20accuracy%20of%2096%5C%25%20within%20a%200.3%24m%24%20range%20and%0Anearly%20100%5C%25%20accuracy%20within%20a%200.5%24m%24%20range%2C%20achieved%20at%20a%20low%20cost%20of%20only%2010%0AUSD%20using%20two%20web%20cameras%20with%20a%20resolution%20of%20640%24%5Ctimes%24480%20pixels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13017v1&entry.124074799=Read"},
{"title": "AV-Flow: Transforming Text to Audio-Visual Human-like Interactions", "author": "Aggelina Chatziagapi and Louis-Philippe Morency and Hongyu Gong and Michael Zollhoefer and Dimitris Samaras and Alexander Richard", "abstract": "  We introduce AV-Flow, an audio-visual generative model that animates\nphoto-realistic 4D talking avatars given only text input. In contrast to prior\nwork that assumes an existing speech signal, we synthesize speech and vision\njointly. We demonstrate human-like speech synthesis, synchronized lip motion,\nlively facial expressions and head pose; all generated from just text\ncharacters. The core premise of our approach lies in the architecture of our\ntwo parallel diffusion transformers. Intermediate highway connections ensure\ncommunication between the audio and visual modalities, and thus, synchronized\nspeech intonation and facial dynamics (e.g., eyebrow motion). Our model is\ntrained with flow matching, leading to expressive results and fast inference.\nIn case of dyadic conversations, AV-Flow produces an always-on avatar, that\nactively listens and reacts to the audio-visual input of a user. Through\nextensive experiments, we show that our method outperforms prior work,\nsynthesizing natural-looking 4D talking avatars. Project page:\nhttps://aggelinacha.github.io/AV-Flow/\n", "link": "http://arxiv.org/abs/2502.13133v1", "date": "2025-02-18", "relevancy": 2.4349, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.621}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6166}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AV-Flow%3A%20Transforming%20Text%20to%20Audio-Visual%20Human-like%20Interactions&body=Title%3A%20AV-Flow%3A%20Transforming%20Text%20to%20Audio-Visual%20Human-like%20Interactions%0AAuthor%3A%20Aggelina%20Chatziagapi%20and%20Louis-Philippe%20Morency%20and%20Hongyu%20Gong%20and%20Michael%20Zollhoefer%20and%20Dimitris%20Samaras%20and%20Alexander%20Richard%0AAbstract%3A%20%20%20We%20introduce%20AV-Flow%2C%20an%20audio-visual%20generative%20model%20that%20animates%0Aphoto-realistic%204D%20talking%20avatars%20given%20only%20text%20input.%20In%20contrast%20to%20prior%0Awork%20that%20assumes%20an%20existing%20speech%20signal%2C%20we%20synthesize%20speech%20and%20vision%0Ajointly.%20We%20demonstrate%20human-like%20speech%20synthesis%2C%20synchronized%20lip%20motion%2C%0Alively%20facial%20expressions%20and%20head%20pose%3B%20all%20generated%20from%20just%20text%0Acharacters.%20The%20core%20premise%20of%20our%20approach%20lies%20in%20the%20architecture%20of%20our%0Atwo%20parallel%20diffusion%20transformers.%20Intermediate%20highway%20connections%20ensure%0Acommunication%20between%20the%20audio%20and%20visual%20modalities%2C%20and%20thus%2C%20synchronized%0Aspeech%20intonation%20and%20facial%20dynamics%20%28e.g.%2C%20eyebrow%20motion%29.%20Our%20model%20is%0Atrained%20with%20flow%20matching%2C%20leading%20to%20expressive%20results%20and%20fast%20inference.%0AIn%20case%20of%20dyadic%20conversations%2C%20AV-Flow%20produces%20an%20always-on%20avatar%2C%20that%0Aactively%20listens%20and%20reacts%20to%20the%20audio-visual%20input%20of%20a%20user.%20Through%0Aextensive%20experiments%2C%20we%20show%20that%20our%20method%20outperforms%20prior%20work%2C%0Asynthesizing%20natural-looking%204D%20talking%20avatars.%20Project%20page%3A%0Ahttps%3A//aggelinacha.github.io/AV-Flow/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAV-Flow%253A%2520Transforming%2520Text%2520to%2520Audio-Visual%2520Human-like%2520Interactions%26entry.906535625%3DAggelina%2520Chatziagapi%2520and%2520Louis-Philippe%2520Morency%2520and%2520Hongyu%2520Gong%2520and%2520Michael%2520Zollhoefer%2520and%2520Dimitris%2520Samaras%2520and%2520Alexander%2520Richard%26entry.1292438233%3D%2520%2520We%2520introduce%2520AV-Flow%252C%2520an%2520audio-visual%2520generative%2520model%2520that%2520animates%250Aphoto-realistic%25204D%2520talking%2520avatars%2520given%2520only%2520text%2520input.%2520In%2520contrast%2520to%2520prior%250Awork%2520that%2520assumes%2520an%2520existing%2520speech%2520signal%252C%2520we%2520synthesize%2520speech%2520and%2520vision%250Ajointly.%2520We%2520demonstrate%2520human-like%2520speech%2520synthesis%252C%2520synchronized%2520lip%2520motion%252C%250Alively%2520facial%2520expressions%2520and%2520head%2520pose%253B%2520all%2520generated%2520from%2520just%2520text%250Acharacters.%2520The%2520core%2520premise%2520of%2520our%2520approach%2520lies%2520in%2520the%2520architecture%2520of%2520our%250Atwo%2520parallel%2520diffusion%2520transformers.%2520Intermediate%2520highway%2520connections%2520ensure%250Acommunication%2520between%2520the%2520audio%2520and%2520visual%2520modalities%252C%2520and%2520thus%252C%2520synchronized%250Aspeech%2520intonation%2520and%2520facial%2520dynamics%2520%2528e.g.%252C%2520eyebrow%2520motion%2529.%2520Our%2520model%2520is%250Atrained%2520with%2520flow%2520matching%252C%2520leading%2520to%2520expressive%2520results%2520and%2520fast%2520inference.%250AIn%2520case%2520of%2520dyadic%2520conversations%252C%2520AV-Flow%2520produces%2520an%2520always-on%2520avatar%252C%2520that%250Aactively%2520listens%2520and%2520reacts%2520to%2520the%2520audio-visual%2520input%2520of%2520a%2520user.%2520Through%250Aextensive%2520experiments%252C%2520we%2520show%2520that%2520our%2520method%2520outperforms%2520prior%2520work%252C%250Asynthesizing%2520natural-looking%25204D%2520talking%2520avatars.%2520Project%2520page%253A%250Ahttps%253A//aggelinacha.github.io/AV-Flow/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AV-Flow%3A%20Transforming%20Text%20to%20Audio-Visual%20Human-like%20Interactions&entry.906535625=Aggelina%20Chatziagapi%20and%20Louis-Philippe%20Morency%20and%20Hongyu%20Gong%20and%20Michael%20Zollhoefer%20and%20Dimitris%20Samaras%20and%20Alexander%20Richard&entry.1292438233=%20%20We%20introduce%20AV-Flow%2C%20an%20audio-visual%20generative%20model%20that%20animates%0Aphoto-realistic%204D%20talking%20avatars%20given%20only%20text%20input.%20In%20contrast%20to%20prior%0Awork%20that%20assumes%20an%20existing%20speech%20signal%2C%20we%20synthesize%20speech%20and%20vision%0Ajointly.%20We%20demonstrate%20human-like%20speech%20synthesis%2C%20synchronized%20lip%20motion%2C%0Alively%20facial%20expressions%20and%20head%20pose%3B%20all%20generated%20from%20just%20text%0Acharacters.%20The%20core%20premise%20of%20our%20approach%20lies%20in%20the%20architecture%20of%20our%0Atwo%20parallel%20diffusion%20transformers.%20Intermediate%20highway%20connections%20ensure%0Acommunication%20between%20the%20audio%20and%20visual%20modalities%2C%20and%20thus%2C%20synchronized%0Aspeech%20intonation%20and%20facial%20dynamics%20%28e.g.%2C%20eyebrow%20motion%29.%20Our%20model%20is%0Atrained%20with%20flow%20matching%2C%20leading%20to%20expressive%20results%20and%20fast%20inference.%0AIn%20case%20of%20dyadic%20conversations%2C%20AV-Flow%20produces%20an%20always-on%20avatar%2C%20that%0Aactively%20listens%20and%20reacts%20to%20the%20audio-visual%20input%20of%20a%20user.%20Through%0Aextensive%20experiments%2C%20we%20show%20that%20our%20method%20outperforms%20prior%20work%2C%0Asynthesizing%20natural-looking%204D%20talking%20avatars.%20Project%20page%3A%0Ahttps%3A//aggelinacha.github.io/AV-Flow/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13133v1&entry.124074799=Read"},
{"title": "Soundwave: Less is More for Speech-Text Alignment in LLMs", "author": "Yuhao Zhang and Zhiheng Liu and Fan Bu and Ruiyu Zhang and Benyou Wang and Haizhou Li", "abstract": "  Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave.\n", "link": "http://arxiv.org/abs/2502.12900v1", "date": "2025-02-18", "relevancy": 2.4223, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soundwave%3A%20Less%20is%20More%20for%20Speech-Text%20Alignment%20in%20LLMs&body=Title%3A%20Soundwave%3A%20Less%20is%20More%20for%20Speech-Text%20Alignment%20in%20LLMs%0AAuthor%3A%20Yuhao%20Zhang%20and%20Zhiheng%20Liu%20and%20Fan%20Bu%20and%20Ruiyu%20Zhang%20and%20Benyou%20Wang%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20Existing%20end-to-end%20speech%20large%20language%20models%20%28LLMs%29%20usually%20rely%20on%0Alarge-scale%20annotated%20data%20for%20training%2C%20while%20data-efficient%20training%20has%20not%0Abeen%20discussed%20in%20depth.%20We%20focus%20on%20two%20fundamental%20problems%20between%20speech%0Aand%20text%3A%20the%20representation%20space%20gap%20and%20sequence%20length%20inconsistency.%20We%0Apropose%20Soundwave%2C%20which%20utilizes%20an%20efficient%20training%20strategy%20and%20a%20novel%0Aarchitecture%20to%20address%20these%20issues.%20Results%20show%20that%20Soundwave%20outperforms%0Athe%20advanced%20Qwen2-Audio%20in%20speech%20translation%20and%20AIR-Bench%20speech%20tasks%2C%0Ausing%20only%20one-fiftieth%20of%20the%20training%20data.%20Further%20analysis%20shows%20that%0ASoundwave%20still%20retains%20its%20intelligence%20during%20conversation.%20The%20project%20is%0Aavailable%20at%20https%3A//github.com/FreedomIntelligence/Soundwave.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoundwave%253A%2520Less%2520is%2520More%2520for%2520Speech-Text%2520Alignment%2520in%2520LLMs%26entry.906535625%3DYuhao%2520Zhang%2520and%2520Zhiheng%2520Liu%2520and%2520Fan%2520Bu%2520and%2520Ruiyu%2520Zhang%2520and%2520Benyou%2520Wang%2520and%2520Haizhou%2520Li%26entry.1292438233%3D%2520%2520Existing%2520end-to-end%2520speech%2520large%2520language%2520models%2520%2528LLMs%2529%2520usually%2520rely%2520on%250Alarge-scale%2520annotated%2520data%2520for%2520training%252C%2520while%2520data-efficient%2520training%2520has%2520not%250Abeen%2520discussed%2520in%2520depth.%2520We%2520focus%2520on%2520two%2520fundamental%2520problems%2520between%2520speech%250Aand%2520text%253A%2520the%2520representation%2520space%2520gap%2520and%2520sequence%2520length%2520inconsistency.%2520We%250Apropose%2520Soundwave%252C%2520which%2520utilizes%2520an%2520efficient%2520training%2520strategy%2520and%2520a%2520novel%250Aarchitecture%2520to%2520address%2520these%2520issues.%2520Results%2520show%2520that%2520Soundwave%2520outperforms%250Athe%2520advanced%2520Qwen2-Audio%2520in%2520speech%2520translation%2520and%2520AIR-Bench%2520speech%2520tasks%252C%250Ausing%2520only%2520one-fiftieth%2520of%2520the%2520training%2520data.%2520Further%2520analysis%2520shows%2520that%250ASoundwave%2520still%2520retains%2520its%2520intelligence%2520during%2520conversation.%2520The%2520project%2520is%250Aavailable%2520at%2520https%253A//github.com/FreedomIntelligence/Soundwave.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soundwave%3A%20Less%20is%20More%20for%20Speech-Text%20Alignment%20in%20LLMs&entry.906535625=Yuhao%20Zhang%20and%20Zhiheng%20Liu%20and%20Fan%20Bu%20and%20Ruiyu%20Zhang%20and%20Benyou%20Wang%20and%20Haizhou%20Li&entry.1292438233=%20%20Existing%20end-to-end%20speech%20large%20language%20models%20%28LLMs%29%20usually%20rely%20on%0Alarge-scale%20annotated%20data%20for%20training%2C%20while%20data-efficient%20training%20has%20not%0Abeen%20discussed%20in%20depth.%20We%20focus%20on%20two%20fundamental%20problems%20between%20speech%0Aand%20text%3A%20the%20representation%20space%20gap%20and%20sequence%20length%20inconsistency.%20We%0Apropose%20Soundwave%2C%20which%20utilizes%20an%20efficient%20training%20strategy%20and%20a%20novel%0Aarchitecture%20to%20address%20these%20issues.%20Results%20show%20that%20Soundwave%20outperforms%0Athe%20advanced%20Qwen2-Audio%20in%20speech%20translation%20and%20AIR-Bench%20speech%20tasks%2C%0Ausing%20only%20one-fiftieth%20of%20the%20training%20data.%20Further%20analysis%20shows%20that%0ASoundwave%20still%20retains%20its%20intelligence%20during%20conversation.%20The%20project%20is%0Aavailable%20at%20https%3A//github.com/FreedomIntelligence/Soundwave.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12900v1&entry.124074799=Read"},
{"title": "MOLLM: Multi-Objective Large Language Model for Molecular Design --\n  Optimizing with Experts", "author": "Nian Ran and Yue Wang and Richard Allmendinger", "abstract": "  Molecular design plays a critical role in advancing fields such as drug\ndiscovery, materials science, and chemical engineering. This work introduces\nthe Multi-Objective Large Language Model for Molecular Design (MOLLM), a novel\nframework that combines domain-specific knowledge with the adaptability of\nLarge Language Models to optimize molecular properties across multiple\nobjectives. Leveraging in-context learning and multi-objective optimization,\nMOLLM achieves superior efficiency, innovation, and performance, significantly\nsurpassing state-of-the-art (SOTA) methods. Recognizing the substantial impact\nof initial populations on evolutionary algorithms, we categorize them into\nthree types: best initial, worst initial, and random initial, to ensure the\ninitial molecules are the same for each method across experiments. Our results\ndemonstrate that MOLLM consistently outperforms SOTA models in all of our\nexperiments. We also provide extensive ablation studies to evaluate the\nsuperiority of our components.\n", "link": "http://arxiv.org/abs/2502.12845v1", "date": "2025-02-18", "relevancy": 2.4192, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOLLM%3A%20Multi-Objective%20Large%20Language%20Model%20for%20Molecular%20Design%20--%0A%20%20Optimizing%20with%20Experts&body=Title%3A%20MOLLM%3A%20Multi-Objective%20Large%20Language%20Model%20for%20Molecular%20Design%20--%0A%20%20Optimizing%20with%20Experts%0AAuthor%3A%20Nian%20Ran%20and%20Yue%20Wang%20and%20Richard%20Allmendinger%0AAbstract%3A%20%20%20Molecular%20design%20plays%20a%20critical%20role%20in%20advancing%20fields%20such%20as%20drug%0Adiscovery%2C%20materials%20science%2C%20and%20chemical%20engineering.%20This%20work%20introduces%0Athe%20Multi-Objective%20Large%20Language%20Model%20for%20Molecular%20Design%20%28MOLLM%29%2C%20a%20novel%0Aframework%20that%20combines%20domain-specific%20knowledge%20with%20the%20adaptability%20of%0ALarge%20Language%20Models%20to%20optimize%20molecular%20properties%20across%20multiple%0Aobjectives.%20Leveraging%20in-context%20learning%20and%20multi-objective%20optimization%2C%0AMOLLM%20achieves%20superior%20efficiency%2C%20innovation%2C%20and%20performance%2C%20significantly%0Asurpassing%20state-of-the-art%20%28SOTA%29%20methods.%20Recognizing%20the%20substantial%20impact%0Aof%20initial%20populations%20on%20evolutionary%20algorithms%2C%20we%20categorize%20them%20into%0Athree%20types%3A%20best%20initial%2C%20worst%20initial%2C%20and%20random%20initial%2C%20to%20ensure%20the%0Ainitial%20molecules%20are%20the%20same%20for%20each%20method%20across%20experiments.%20Our%20results%0Ademonstrate%20that%20MOLLM%20consistently%20outperforms%20SOTA%20models%20in%20all%20of%20our%0Aexperiments.%20We%20also%20provide%20extensive%20ablation%20studies%20to%20evaluate%20the%0Asuperiority%20of%20our%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOLLM%253A%2520Multi-Objective%2520Large%2520Language%2520Model%2520for%2520Molecular%2520Design%2520--%250A%2520%2520Optimizing%2520with%2520Experts%26entry.906535625%3DNian%2520Ran%2520and%2520Yue%2520Wang%2520and%2520Richard%2520Allmendinger%26entry.1292438233%3D%2520%2520Molecular%2520design%2520plays%2520a%2520critical%2520role%2520in%2520advancing%2520fields%2520such%2520as%2520drug%250Adiscovery%252C%2520materials%2520science%252C%2520and%2520chemical%2520engineering.%2520This%2520work%2520introduces%250Athe%2520Multi-Objective%2520Large%2520Language%2520Model%2520for%2520Molecular%2520Design%2520%2528MOLLM%2529%252C%2520a%2520novel%250Aframework%2520that%2520combines%2520domain-specific%2520knowledge%2520with%2520the%2520adaptability%2520of%250ALarge%2520Language%2520Models%2520to%2520optimize%2520molecular%2520properties%2520across%2520multiple%250Aobjectives.%2520Leveraging%2520in-context%2520learning%2520and%2520multi-objective%2520optimization%252C%250AMOLLM%2520achieves%2520superior%2520efficiency%252C%2520innovation%252C%2520and%2520performance%252C%2520significantly%250Asurpassing%2520state-of-the-art%2520%2528SOTA%2529%2520methods.%2520Recognizing%2520the%2520substantial%2520impact%250Aof%2520initial%2520populations%2520on%2520evolutionary%2520algorithms%252C%2520we%2520categorize%2520them%2520into%250Athree%2520types%253A%2520best%2520initial%252C%2520worst%2520initial%252C%2520and%2520random%2520initial%252C%2520to%2520ensure%2520the%250Ainitial%2520molecules%2520are%2520the%2520same%2520for%2520each%2520method%2520across%2520experiments.%2520Our%2520results%250Ademonstrate%2520that%2520MOLLM%2520consistently%2520outperforms%2520SOTA%2520models%2520in%2520all%2520of%2520our%250Aexperiments.%2520We%2520also%2520provide%2520extensive%2520ablation%2520studies%2520to%2520evaluate%2520the%250Asuperiority%2520of%2520our%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOLLM%3A%20Multi-Objective%20Large%20Language%20Model%20for%20Molecular%20Design%20--%0A%20%20Optimizing%20with%20Experts&entry.906535625=Nian%20Ran%20and%20Yue%20Wang%20and%20Richard%20Allmendinger&entry.1292438233=%20%20Molecular%20design%20plays%20a%20critical%20role%20in%20advancing%20fields%20such%20as%20drug%0Adiscovery%2C%20materials%20science%2C%20and%20chemical%20engineering.%20This%20work%20introduces%0Athe%20Multi-Objective%20Large%20Language%20Model%20for%20Molecular%20Design%20%28MOLLM%29%2C%20a%20novel%0Aframework%20that%20combines%20domain-specific%20knowledge%20with%20the%20adaptability%20of%0ALarge%20Language%20Models%20to%20optimize%20molecular%20properties%20across%20multiple%0Aobjectives.%20Leveraging%20in-context%20learning%20and%20multi-objective%20optimization%2C%0AMOLLM%20achieves%20superior%20efficiency%2C%20innovation%2C%20and%20performance%2C%20significantly%0Asurpassing%20state-of-the-art%20%28SOTA%29%20methods.%20Recognizing%20the%20substantial%20impact%0Aof%20initial%20populations%20on%20evolutionary%20algorithms%2C%20we%20categorize%20them%20into%0Athree%20types%3A%20best%20initial%2C%20worst%20initial%2C%20and%20random%20initial%2C%20to%20ensure%20the%0Ainitial%20molecules%20are%20the%20same%20for%20each%20method%20across%20experiments.%20Our%20results%0Ademonstrate%20that%20MOLLM%20consistently%20outperforms%20SOTA%20models%20in%20all%20of%20our%0Aexperiments.%20We%20also%20provide%20extensive%20ablation%20studies%20to%20evaluate%20the%0Asuperiority%20of%20our%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12845v1&entry.124074799=Read"},
{"title": "Evaluating link prediction: New perspectives and recommendations", "author": "Bhargavi Kalyani I and A Rama Prasad Mathi and Niladri Sett", "abstract": "  Link prediction (LP) is an important problem in network science and machine\nlearning research. The state-of-the-art LP methods are usually evaluated in a\nuniform setup, ignoring several factors associated with the data and\napplication specific needs. We identify a number of such factors, such as,\nnetwork-type, problem-type, geodesic distance between the end nodes and its\ndistribution over the classes, nature and applicability of LP methods, class\nimbalance and its impact on early retrieval, evaluation metric, etc., and\npresent an experimental setup which allows us to evaluate LP methods in a\nrigorous and controlled manner. We perform extensive experiments with a variety\nof LP methods over real network datasets in this controlled setup, and gather\nvaluable insights on the interactions of these factors with the performance of\nLP through an array of carefully designed hypotheses. Following the insights,\nwe provide recommendations to be followed as best practice for evaluating LP\nmethods.\n", "link": "http://arxiv.org/abs/2502.12777v1", "date": "2025-02-18", "relevancy": 2.4176, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20link%20prediction%3A%20New%20perspectives%20and%20recommendations&body=Title%3A%20Evaluating%20link%20prediction%3A%20New%20perspectives%20and%20recommendations%0AAuthor%3A%20Bhargavi%20Kalyani%20I%20and%20A%20Rama%20Prasad%20Mathi%20and%20Niladri%20Sett%0AAbstract%3A%20%20%20Link%20prediction%20%28LP%29%20is%20an%20important%20problem%20in%20network%20science%20and%20machine%0Alearning%20research.%20The%20state-of-the-art%20LP%20methods%20are%20usually%20evaluated%20in%20a%0Auniform%20setup%2C%20ignoring%20several%20factors%20associated%20with%20the%20data%20and%0Aapplication%20specific%20needs.%20We%20identify%20a%20number%20of%20such%20factors%2C%20such%20as%2C%0Anetwork-type%2C%20problem-type%2C%20geodesic%20distance%20between%20the%20end%20nodes%20and%20its%0Adistribution%20over%20the%20classes%2C%20nature%20and%20applicability%20of%20LP%20methods%2C%20class%0Aimbalance%20and%20its%20impact%20on%20early%20retrieval%2C%20evaluation%20metric%2C%20etc.%2C%20and%0Apresent%20an%20experimental%20setup%20which%20allows%20us%20to%20evaluate%20LP%20methods%20in%20a%0Arigorous%20and%20controlled%20manner.%20We%20perform%20extensive%20experiments%20with%20a%20variety%0Aof%20LP%20methods%20over%20real%20network%20datasets%20in%20this%20controlled%20setup%2C%20and%20gather%0Avaluable%20insights%20on%20the%20interactions%20of%20these%20factors%20with%20the%20performance%20of%0ALP%20through%20an%20array%20of%20carefully%20designed%20hypotheses.%20Following%20the%20insights%2C%0Awe%20provide%20recommendations%20to%20be%20followed%20as%20best%20practice%20for%20evaluating%20LP%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520link%2520prediction%253A%2520New%2520perspectives%2520and%2520recommendations%26entry.906535625%3DBhargavi%2520Kalyani%2520I%2520and%2520A%2520Rama%2520Prasad%2520Mathi%2520and%2520Niladri%2520Sett%26entry.1292438233%3D%2520%2520Link%2520prediction%2520%2528LP%2529%2520is%2520an%2520important%2520problem%2520in%2520network%2520science%2520and%2520machine%250Alearning%2520research.%2520The%2520state-of-the-art%2520LP%2520methods%2520are%2520usually%2520evaluated%2520in%2520a%250Auniform%2520setup%252C%2520ignoring%2520several%2520factors%2520associated%2520with%2520the%2520data%2520and%250Aapplication%2520specific%2520needs.%2520We%2520identify%2520a%2520number%2520of%2520such%2520factors%252C%2520such%2520as%252C%250Anetwork-type%252C%2520problem-type%252C%2520geodesic%2520distance%2520between%2520the%2520end%2520nodes%2520and%2520its%250Adistribution%2520over%2520the%2520classes%252C%2520nature%2520and%2520applicability%2520of%2520LP%2520methods%252C%2520class%250Aimbalance%2520and%2520its%2520impact%2520on%2520early%2520retrieval%252C%2520evaluation%2520metric%252C%2520etc.%252C%2520and%250Apresent%2520an%2520experimental%2520setup%2520which%2520allows%2520us%2520to%2520evaluate%2520LP%2520methods%2520in%2520a%250Arigorous%2520and%2520controlled%2520manner.%2520We%2520perform%2520extensive%2520experiments%2520with%2520a%2520variety%250Aof%2520LP%2520methods%2520over%2520real%2520network%2520datasets%2520in%2520this%2520controlled%2520setup%252C%2520and%2520gather%250Avaluable%2520insights%2520on%2520the%2520interactions%2520of%2520these%2520factors%2520with%2520the%2520performance%2520of%250ALP%2520through%2520an%2520array%2520of%2520carefully%2520designed%2520hypotheses.%2520Following%2520the%2520insights%252C%250Awe%2520provide%2520recommendations%2520to%2520be%2520followed%2520as%2520best%2520practice%2520for%2520evaluating%2520LP%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20link%20prediction%3A%20New%20perspectives%20and%20recommendations&entry.906535625=Bhargavi%20Kalyani%20I%20and%20A%20Rama%20Prasad%20Mathi%20and%20Niladri%20Sett&entry.1292438233=%20%20Link%20prediction%20%28LP%29%20is%20an%20important%20problem%20in%20network%20science%20and%20machine%0Alearning%20research.%20The%20state-of-the-art%20LP%20methods%20are%20usually%20evaluated%20in%20a%0Auniform%20setup%2C%20ignoring%20several%20factors%20associated%20with%20the%20data%20and%0Aapplication%20specific%20needs.%20We%20identify%20a%20number%20of%20such%20factors%2C%20such%20as%2C%0Anetwork-type%2C%20problem-type%2C%20geodesic%20distance%20between%20the%20end%20nodes%20and%20its%0Adistribution%20over%20the%20classes%2C%20nature%20and%20applicability%20of%20LP%20methods%2C%20class%0Aimbalance%20and%20its%20impact%20on%20early%20retrieval%2C%20evaluation%20metric%2C%20etc.%2C%20and%0Apresent%20an%20experimental%20setup%20which%20allows%20us%20to%20evaluate%20LP%20methods%20in%20a%0Arigorous%20and%20controlled%20manner.%20We%20perform%20extensive%20experiments%20with%20a%20variety%0Aof%20LP%20methods%20over%20real%20network%20datasets%20in%20this%20controlled%20setup%2C%20and%20gather%0Avaluable%20insights%20on%20the%20interactions%20of%20these%20factors%20with%20the%20performance%20of%0ALP%20through%20an%20array%20of%20carefully%20designed%20hypotheses.%20Following%20the%20insights%2C%0Awe%20provide%20recommendations%20to%20be%20followed%20as%20best%20practice%20for%20evaluating%20LP%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12777v1&entry.124074799=Read"},
{"title": "Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh\n  Representations", "author": "Hinrich Rahlfs and Markus H\u00fcllebrand and Sebastian Schmitter and Christoph Strecker and Andreas Harloff and Anja Hennemuth", "abstract": "  Purpose: Enabling a comprehensive and robust assessment of carotid artery\nplaques in 3D through extraction and visualization of quantitative plaque\nparameters. These parameters have potential applications in stroke risk\nanalysis, evaluation of therapy effectiveness, and plaque progression\nprediction. Methods: We propose a novel method for extracting a plaque mesh\nfrom 3D vessel wall segmentation using distance encoding on the inner and outer\nwall mesh for precise plaque structure analysis. A case-specific threshold,\nderived from the normal vessel wall thickness, was applied to extract plaques\nfrom a dataset of 202 T1-weighted black-blood MRI scans of subjects with up to\n50% stenosis. Applied to baseline and one-year follow-up data, the method\nsupports detailed plaque morphology analysis over time, including plaque volume\nquantification, aided by improved visualization via mesh unfolding. Results: We\nsuccessfully extracted plaque meshes from 341 carotid arteries, capturing a\nwide range of plaque shapes with volumes ranging from 2.69{\\mu}l to\n847.7{\\mu}l. The use of a case-specific threshold effectively eliminated false\npositives in young, healthy subjects. Conclusion: The proposed method enables\nprecise extraction of plaque meshes from 3D vessel wall segmentation masks\nenabling a correspondence between baseline and one-year follow-up examinations.\nUnfolding the plaque meshes enhances visualization, while the mesh-based\nanalysis allows quantification of plaque parameters independent of voxel\nresolution.\n", "link": "http://arxiv.org/abs/2502.12819v1", "date": "2025-02-18", "relevancy": 2.4095, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.485}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.485}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Carotid%20Artery%20Plaque%20Analysis%20in%203D%20Based%20on%20Distance%20Encoding%20in%20Mesh%0A%20%20Representations&body=Title%3A%20Carotid%20Artery%20Plaque%20Analysis%20in%203D%20Based%20on%20Distance%20Encoding%20in%20Mesh%0A%20%20Representations%0AAuthor%3A%20Hinrich%20Rahlfs%20and%20Markus%20H%C3%BCllebrand%20and%20Sebastian%20Schmitter%20and%20Christoph%20Strecker%20and%20Andreas%20Harloff%20and%20Anja%20Hennemuth%0AAbstract%3A%20%20%20Purpose%3A%20Enabling%20a%20comprehensive%20and%20robust%20assessment%20of%20carotid%20artery%0Aplaques%20in%203D%20through%20extraction%20and%20visualization%20of%20quantitative%20plaque%0Aparameters.%20These%20parameters%20have%20potential%20applications%20in%20stroke%20risk%0Aanalysis%2C%20evaluation%20of%20therapy%20effectiveness%2C%20and%20plaque%20progression%0Aprediction.%20Methods%3A%20We%20propose%20a%20novel%20method%20for%20extracting%20a%20plaque%20mesh%0Afrom%203D%20vessel%20wall%20segmentation%20using%20distance%20encoding%20on%20the%20inner%20and%20outer%0Awall%20mesh%20for%20precise%20plaque%20structure%20analysis.%20A%20case-specific%20threshold%2C%0Aderived%20from%20the%20normal%20vessel%20wall%20thickness%2C%20was%20applied%20to%20extract%20plaques%0Afrom%20a%20dataset%20of%20202%20T1-weighted%20black-blood%20MRI%20scans%20of%20subjects%20with%20up%20to%0A50%25%20stenosis.%20Applied%20to%20baseline%20and%20one-year%20follow-up%20data%2C%20the%20method%0Asupports%20detailed%20plaque%20morphology%20analysis%20over%20time%2C%20including%20plaque%20volume%0Aquantification%2C%20aided%20by%20improved%20visualization%20via%20mesh%20unfolding.%20Results%3A%20We%0Asuccessfully%20extracted%20plaque%20meshes%20from%20341%20carotid%20arteries%2C%20capturing%20a%0Awide%20range%20of%20plaque%20shapes%20with%20volumes%20ranging%20from%202.69%7B%5Cmu%7Dl%20to%0A847.7%7B%5Cmu%7Dl.%20The%20use%20of%20a%20case-specific%20threshold%20effectively%20eliminated%20false%0Apositives%20in%20young%2C%20healthy%20subjects.%20Conclusion%3A%20The%20proposed%20method%20enables%0Aprecise%20extraction%20of%20plaque%20meshes%20from%203D%20vessel%20wall%20segmentation%20masks%0Aenabling%20a%20correspondence%20between%20baseline%20and%20one-year%20follow-up%20examinations.%0AUnfolding%20the%20plaque%20meshes%20enhances%20visualization%2C%20while%20the%20mesh-based%0Aanalysis%20allows%20quantification%20of%20plaque%20parameters%20independent%20of%20voxel%0Aresolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarotid%2520Artery%2520Plaque%2520Analysis%2520in%25203D%2520Based%2520on%2520Distance%2520Encoding%2520in%2520Mesh%250A%2520%2520Representations%26entry.906535625%3DHinrich%2520Rahlfs%2520and%2520Markus%2520H%25C3%25BCllebrand%2520and%2520Sebastian%2520Schmitter%2520and%2520Christoph%2520Strecker%2520and%2520Andreas%2520Harloff%2520and%2520Anja%2520Hennemuth%26entry.1292438233%3D%2520%2520Purpose%253A%2520Enabling%2520a%2520comprehensive%2520and%2520robust%2520assessment%2520of%2520carotid%2520artery%250Aplaques%2520in%25203D%2520through%2520extraction%2520and%2520visualization%2520of%2520quantitative%2520plaque%250Aparameters.%2520These%2520parameters%2520have%2520potential%2520applications%2520in%2520stroke%2520risk%250Aanalysis%252C%2520evaluation%2520of%2520therapy%2520effectiveness%252C%2520and%2520plaque%2520progression%250Aprediction.%2520Methods%253A%2520We%2520propose%2520a%2520novel%2520method%2520for%2520extracting%2520a%2520plaque%2520mesh%250Afrom%25203D%2520vessel%2520wall%2520segmentation%2520using%2520distance%2520encoding%2520on%2520the%2520inner%2520and%2520outer%250Awall%2520mesh%2520for%2520precise%2520plaque%2520structure%2520analysis.%2520A%2520case-specific%2520threshold%252C%250Aderived%2520from%2520the%2520normal%2520vessel%2520wall%2520thickness%252C%2520was%2520applied%2520to%2520extract%2520plaques%250Afrom%2520a%2520dataset%2520of%2520202%2520T1-weighted%2520black-blood%2520MRI%2520scans%2520of%2520subjects%2520with%2520up%2520to%250A50%2525%2520stenosis.%2520Applied%2520to%2520baseline%2520and%2520one-year%2520follow-up%2520data%252C%2520the%2520method%250Asupports%2520detailed%2520plaque%2520morphology%2520analysis%2520over%2520time%252C%2520including%2520plaque%2520volume%250Aquantification%252C%2520aided%2520by%2520improved%2520visualization%2520via%2520mesh%2520unfolding.%2520Results%253A%2520We%250Asuccessfully%2520extracted%2520plaque%2520meshes%2520from%2520341%2520carotid%2520arteries%252C%2520capturing%2520a%250Awide%2520range%2520of%2520plaque%2520shapes%2520with%2520volumes%2520ranging%2520from%25202.69%257B%255Cmu%257Dl%2520to%250A847.7%257B%255Cmu%257Dl.%2520The%2520use%2520of%2520a%2520case-specific%2520threshold%2520effectively%2520eliminated%2520false%250Apositives%2520in%2520young%252C%2520healthy%2520subjects.%2520Conclusion%253A%2520The%2520proposed%2520method%2520enables%250Aprecise%2520extraction%2520of%2520plaque%2520meshes%2520from%25203D%2520vessel%2520wall%2520segmentation%2520masks%250Aenabling%2520a%2520correspondence%2520between%2520baseline%2520and%2520one-year%2520follow-up%2520examinations.%250AUnfolding%2520the%2520plaque%2520meshes%2520enhances%2520visualization%252C%2520while%2520the%2520mesh-based%250Aanalysis%2520allows%2520quantification%2520of%2520plaque%2520parameters%2520independent%2520of%2520voxel%250Aresolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Carotid%20Artery%20Plaque%20Analysis%20in%203D%20Based%20on%20Distance%20Encoding%20in%20Mesh%0A%20%20Representations&entry.906535625=Hinrich%20Rahlfs%20and%20Markus%20H%C3%BCllebrand%20and%20Sebastian%20Schmitter%20and%20Christoph%20Strecker%20and%20Andreas%20Harloff%20and%20Anja%20Hennemuth&entry.1292438233=%20%20Purpose%3A%20Enabling%20a%20comprehensive%20and%20robust%20assessment%20of%20carotid%20artery%0Aplaques%20in%203D%20through%20extraction%20and%20visualization%20of%20quantitative%20plaque%0Aparameters.%20These%20parameters%20have%20potential%20applications%20in%20stroke%20risk%0Aanalysis%2C%20evaluation%20of%20therapy%20effectiveness%2C%20and%20plaque%20progression%0Aprediction.%20Methods%3A%20We%20propose%20a%20novel%20method%20for%20extracting%20a%20plaque%20mesh%0Afrom%203D%20vessel%20wall%20segmentation%20using%20distance%20encoding%20on%20the%20inner%20and%20outer%0Awall%20mesh%20for%20precise%20plaque%20structure%20analysis.%20A%20case-specific%20threshold%2C%0Aderived%20from%20the%20normal%20vessel%20wall%20thickness%2C%20was%20applied%20to%20extract%20plaques%0Afrom%20a%20dataset%20of%20202%20T1-weighted%20black-blood%20MRI%20scans%20of%20subjects%20with%20up%20to%0A50%25%20stenosis.%20Applied%20to%20baseline%20and%20one-year%20follow-up%20data%2C%20the%20method%0Asupports%20detailed%20plaque%20morphology%20analysis%20over%20time%2C%20including%20plaque%20volume%0Aquantification%2C%20aided%20by%20improved%20visualization%20via%20mesh%20unfolding.%20Results%3A%20We%0Asuccessfully%20extracted%20plaque%20meshes%20from%20341%20carotid%20arteries%2C%20capturing%20a%0Awide%20range%20of%20plaque%20shapes%20with%20volumes%20ranging%20from%202.69%7B%5Cmu%7Dl%20to%0A847.7%7B%5Cmu%7Dl.%20The%20use%20of%20a%20case-specific%20threshold%20effectively%20eliminated%20false%0Apositives%20in%20young%2C%20healthy%20subjects.%20Conclusion%3A%20The%20proposed%20method%20enables%0Aprecise%20extraction%20of%20plaque%20meshes%20from%203D%20vessel%20wall%20segmentation%20masks%0Aenabling%20a%20correspondence%20between%20baseline%20and%20one-year%20follow-up%20examinations.%0AUnfolding%20the%20plaque%20meshes%20enhances%20visualization%2C%20while%20the%20mesh-based%0Aanalysis%20allows%20quantification%20of%20plaque%20parameters%20independent%20of%20voxel%0Aresolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12819v1&entry.124074799=Read"},
{"title": "ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment\n  and Generation", "author": "Yiyi Chen and Qiongkai Xu and Johannes Bjerva", "abstract": "  With the growing popularity of Large Language Models (LLMs) and vector\ndatabases, private textual data is increasingly processed and stored as\nnumerical embeddings. However, recent studies have proven that such embeddings\nare vulnerable to inversion attacks, where original text is reconstructed to\nreveal sensitive information. Previous research has largely assumed access to\nmillions of sentences to train attack models, e.g., through data leakage or\nnearly unrestricted API access. With our method, a single data point is\nsufficient for a partially successful inversion attack. With as little as 1k\ndata samples, performance reaches an optimum across a range of black-box\nencoders, without training on leaked data. We present a Few-shot Textual\nEmbedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning\nvictim embeddings to the attack space and using a generative model to\nreconstruct text. We find that ALGEN attacks can be effectively transferred\nacross domains and languages, revealing key information. We further examine a\nvariety of defense mechanisms against ALGEN, and find that none are effective,\nhighlighting the vulnerabilities posed by inversion attacks. By significantly\nlowering the cost of inversion and proving that embedding spaces can be aligned\nthrough one-step optimization, we establish a new textual embedding inversion\nparadigm with broader applications for embedding alignment in NLP.\n", "link": "http://arxiv.org/abs/2502.11308v2", "date": "2025-02-18", "relevancy": 2.4059, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4842}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4812}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALGEN%3A%20Few-shot%20Inversion%20Attacks%20on%20Textual%20Embeddings%20using%20Alignment%0A%20%20and%20Generation&body=Title%3A%20ALGEN%3A%20Few-shot%20Inversion%20Attacks%20on%20Textual%20Embeddings%20using%20Alignment%0A%20%20and%20Generation%0AAuthor%3A%20Yiyi%20Chen%20and%20Qiongkai%20Xu%20and%20Johannes%20Bjerva%0AAbstract%3A%20%20%20With%20the%20growing%20popularity%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20vector%0Adatabases%2C%20private%20textual%20data%20is%20increasingly%20processed%20and%20stored%20as%0Anumerical%20embeddings.%20However%2C%20recent%20studies%20have%20proven%20that%20such%20embeddings%0Aare%20vulnerable%20to%20inversion%20attacks%2C%20where%20original%20text%20is%20reconstructed%20to%0Areveal%20sensitive%20information.%20Previous%20research%20has%20largely%20assumed%20access%20to%0Amillions%20of%20sentences%20to%20train%20attack%20models%2C%20e.g.%2C%20through%20data%20leakage%20or%0Anearly%20unrestricted%20API%20access.%20With%20our%20method%2C%20a%20single%20data%20point%20is%0Asufficient%20for%20a%20partially%20successful%20inversion%20attack.%20With%20as%20little%20as%201k%0Adata%20samples%2C%20performance%20reaches%20an%20optimum%20across%20a%20range%20of%20black-box%0Aencoders%2C%20without%20training%20on%20leaked%20data.%20We%20present%20a%20Few-shot%20Textual%0AEmbedding%20Inversion%20Attack%20using%20ALignment%20and%20GENeration%20%28ALGEN%29%2C%20by%20aligning%0Avictim%20embeddings%20to%20the%20attack%20space%20and%20using%20a%20generative%20model%20to%0Areconstruct%20text.%20We%20find%20that%20ALGEN%20attacks%20can%20be%20effectively%20transferred%0Aacross%20domains%20and%20languages%2C%20revealing%20key%20information.%20We%20further%20examine%20a%0Avariety%20of%20defense%20mechanisms%20against%20ALGEN%2C%20and%20find%20that%20none%20are%20effective%2C%0Ahighlighting%20the%20vulnerabilities%20posed%20by%20inversion%20attacks.%20By%20significantly%0Alowering%20the%20cost%20of%20inversion%20and%20proving%20that%20embedding%20spaces%20can%20be%20aligned%0Athrough%20one-step%20optimization%2C%20we%20establish%20a%20new%20textual%20embedding%20inversion%0Aparadigm%20with%20broader%20applications%20for%20embedding%20alignment%20in%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALGEN%253A%2520Few-shot%2520Inversion%2520Attacks%2520on%2520Textual%2520Embeddings%2520using%2520Alignment%250A%2520%2520and%2520Generation%26entry.906535625%3DYiyi%2520Chen%2520and%2520Qiongkai%2520Xu%2520and%2520Johannes%2520Bjerva%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520popularity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520vector%250Adatabases%252C%2520private%2520textual%2520data%2520is%2520increasingly%2520processed%2520and%2520stored%2520as%250Anumerical%2520embeddings.%2520However%252C%2520recent%2520studies%2520have%2520proven%2520that%2520such%2520embeddings%250Aare%2520vulnerable%2520to%2520inversion%2520attacks%252C%2520where%2520original%2520text%2520is%2520reconstructed%2520to%250Areveal%2520sensitive%2520information.%2520Previous%2520research%2520has%2520largely%2520assumed%2520access%2520to%250Amillions%2520of%2520sentences%2520to%2520train%2520attack%2520models%252C%2520e.g.%252C%2520through%2520data%2520leakage%2520or%250Anearly%2520unrestricted%2520API%2520access.%2520With%2520our%2520method%252C%2520a%2520single%2520data%2520point%2520is%250Asufficient%2520for%2520a%2520partially%2520successful%2520inversion%2520attack.%2520With%2520as%2520little%2520as%25201k%250Adata%2520samples%252C%2520performance%2520reaches%2520an%2520optimum%2520across%2520a%2520range%2520of%2520black-box%250Aencoders%252C%2520without%2520training%2520on%2520leaked%2520data.%2520We%2520present%2520a%2520Few-shot%2520Textual%250AEmbedding%2520Inversion%2520Attack%2520using%2520ALignment%2520and%2520GENeration%2520%2528ALGEN%2529%252C%2520by%2520aligning%250Avictim%2520embeddings%2520to%2520the%2520attack%2520space%2520and%2520using%2520a%2520generative%2520model%2520to%250Areconstruct%2520text.%2520We%2520find%2520that%2520ALGEN%2520attacks%2520can%2520be%2520effectively%2520transferred%250Aacross%2520domains%2520and%2520languages%252C%2520revealing%2520key%2520information.%2520We%2520further%2520examine%2520a%250Avariety%2520of%2520defense%2520mechanisms%2520against%2520ALGEN%252C%2520and%2520find%2520that%2520none%2520are%2520effective%252C%250Ahighlighting%2520the%2520vulnerabilities%2520posed%2520by%2520inversion%2520attacks.%2520By%2520significantly%250Alowering%2520the%2520cost%2520of%2520inversion%2520and%2520proving%2520that%2520embedding%2520spaces%2520can%2520be%2520aligned%250Athrough%2520one-step%2520optimization%252C%2520we%2520establish%2520a%2520new%2520textual%2520embedding%2520inversion%250Aparadigm%2520with%2520broader%2520applications%2520for%2520embedding%2520alignment%2520in%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALGEN%3A%20Few-shot%20Inversion%20Attacks%20on%20Textual%20Embeddings%20using%20Alignment%0A%20%20and%20Generation&entry.906535625=Yiyi%20Chen%20and%20Qiongkai%20Xu%20and%20Johannes%20Bjerva&entry.1292438233=%20%20With%20the%20growing%20popularity%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20vector%0Adatabases%2C%20private%20textual%20data%20is%20increasingly%20processed%20and%20stored%20as%0Anumerical%20embeddings.%20However%2C%20recent%20studies%20have%20proven%20that%20such%20embeddings%0Aare%20vulnerable%20to%20inversion%20attacks%2C%20where%20original%20text%20is%20reconstructed%20to%0Areveal%20sensitive%20information.%20Previous%20research%20has%20largely%20assumed%20access%20to%0Amillions%20of%20sentences%20to%20train%20attack%20models%2C%20e.g.%2C%20through%20data%20leakage%20or%0Anearly%20unrestricted%20API%20access.%20With%20our%20method%2C%20a%20single%20data%20point%20is%0Asufficient%20for%20a%20partially%20successful%20inversion%20attack.%20With%20as%20little%20as%201k%0Adata%20samples%2C%20performance%20reaches%20an%20optimum%20across%20a%20range%20of%20black-box%0Aencoders%2C%20without%20training%20on%20leaked%20data.%20We%20present%20a%20Few-shot%20Textual%0AEmbedding%20Inversion%20Attack%20using%20ALignment%20and%20GENeration%20%28ALGEN%29%2C%20by%20aligning%0Avictim%20embeddings%20to%20the%20attack%20space%20and%20using%20a%20generative%20model%20to%0Areconstruct%20text.%20We%20find%20that%20ALGEN%20attacks%20can%20be%20effectively%20transferred%0Aacross%20domains%20and%20languages%2C%20revealing%20key%20information.%20We%20further%20examine%20a%0Avariety%20of%20defense%20mechanisms%20against%20ALGEN%2C%20and%20find%20that%20none%20are%20effective%2C%0Ahighlighting%20the%20vulnerabilities%20posed%20by%20inversion%20attacks.%20By%20significantly%0Alowering%20the%20cost%20of%20inversion%20and%20proving%20that%20embedding%20spaces%20can%20be%20aligned%0Athrough%20one-step%20optimization%2C%20we%20establish%20a%20new%20textual%20embedding%20inversion%0Aparadigm%20with%20broader%20applications%20for%20embedding%20alignment%20in%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11308v2&entry.124074799=Read"},
{"title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations", "author": "Frederic Kirstein and Muneeb Khan and Jan Philip Wahle and Terry Ruas and Bela Gipp", "abstract": "  Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.\n", "link": "http://arxiv.org/abs/2502.13001v1", "date": "2025-02-18", "relevancy": 2.403, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20need%20to%20MIMIC%20to%20get%20FAME%3A%20Solving%20Meeting%20Transcript%20Scarcity%20with%0A%20%20a%20Multi-Agent%20Conversations&body=Title%3A%20You%20need%20to%20MIMIC%20to%20get%20FAME%3A%20Solving%20Meeting%20Transcript%20Scarcity%20with%0A%20%20a%20Multi-Agent%20Conversations%0AAuthor%3A%20Frederic%20Kirstein%20and%20Muneeb%20Khan%20and%20Jan%20Philip%20Wahle%20and%20Terry%20Ruas%20and%20Bela%20Gipp%0AAbstract%3A%20%20%20Meeting%20summarization%20suffers%20from%20limited%20high-quality%20data%2C%20mainly%20due%20to%0Aprivacy%20restrictions%20and%20expensive%20collection%20processes.%20We%20address%20this%20gap%0Awith%20FAME%2C%20a%20dataset%20of%20500%20meetings%20in%20English%20and%20300%20in%20German%20produced%20by%0AMIMIC%2C%20our%20new%20multi-agent%20meeting%20synthesis%20framework%20that%20generates%20meeting%0Atranscripts%20on%20a%20given%20knowledge%20source%20by%20defining%20psychologically%20grounded%0Aparticipant%20profiles%2C%20outlining%20the%20conversation%2C%20and%20orchestrating%20a%20large%0Alanguage%20model%20%28LLM%29%20debate.%20A%20modular%20post-processing%20step%20refines%20these%0Aoutputs%2C%20mitigating%20potential%20repetitiveness%20and%20overly%20formal%20tones%2C%20ensuring%0Acoherent%2C%20credible%20dialogues%20at%20scale.%20We%20also%20propose%20a%20psychologically%0Agrounded%20evaluation%20framework%20assessing%20naturalness%2C%20social%20behavior%0Aauthenticity%2C%20and%20transcript%20difficulties.%20Human%20assessments%20show%20that%20FAME%0Aapproximates%20real-meeting%20spontaneity%20%284.5/5%20in%20naturalness%29%2C%20preserves%0Aspeaker-centric%20challenges%20%283/5%20in%20spoken%20language%29%2C%20and%20introduces%20richer%0Ainformation-oriented%20difficulty%20%284/5%20in%20difficulty%29.%20These%20findings%20highlight%0Athat%20FAME%20is%20a%20good%20and%20scalable%20proxy%20for%20real-world%20meeting%20conditions.%20It%0Aenables%20new%20test%20scenarios%20for%20meeting%20summarization%20research%20and%20other%0Aconversation-centric%20applications%20in%20tasks%20requiring%20conversation%20data%20or%0Asimulating%20social%20scenarios%20under%20behavioral%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520need%2520to%2520MIMIC%2520to%2520get%2520FAME%253A%2520Solving%2520Meeting%2520Transcript%2520Scarcity%2520with%250A%2520%2520a%2520Multi-Agent%2520Conversations%26entry.906535625%3DFrederic%2520Kirstein%2520and%2520Muneeb%2520Khan%2520and%2520Jan%2520Philip%2520Wahle%2520and%2520Terry%2520Ruas%2520and%2520Bela%2520Gipp%26entry.1292438233%3D%2520%2520Meeting%2520summarization%2520suffers%2520from%2520limited%2520high-quality%2520data%252C%2520mainly%2520due%2520to%250Aprivacy%2520restrictions%2520and%2520expensive%2520collection%2520processes.%2520We%2520address%2520this%2520gap%250Awith%2520FAME%252C%2520a%2520dataset%2520of%2520500%2520meetings%2520in%2520English%2520and%2520300%2520in%2520German%2520produced%2520by%250AMIMIC%252C%2520our%2520new%2520multi-agent%2520meeting%2520synthesis%2520framework%2520that%2520generates%2520meeting%250Atranscripts%2520on%2520a%2520given%2520knowledge%2520source%2520by%2520defining%2520psychologically%2520grounded%250Aparticipant%2520profiles%252C%2520outlining%2520the%2520conversation%252C%2520and%2520orchestrating%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520debate.%2520A%2520modular%2520post-processing%2520step%2520refines%2520these%250Aoutputs%252C%2520mitigating%2520potential%2520repetitiveness%2520and%2520overly%2520formal%2520tones%252C%2520ensuring%250Acoherent%252C%2520credible%2520dialogues%2520at%2520scale.%2520We%2520also%2520propose%2520a%2520psychologically%250Agrounded%2520evaluation%2520framework%2520assessing%2520naturalness%252C%2520social%2520behavior%250Aauthenticity%252C%2520and%2520transcript%2520difficulties.%2520Human%2520assessments%2520show%2520that%2520FAME%250Aapproximates%2520real-meeting%2520spontaneity%2520%25284.5/5%2520in%2520naturalness%2529%252C%2520preserves%250Aspeaker-centric%2520challenges%2520%25283/5%2520in%2520spoken%2520language%2529%252C%2520and%2520introduces%2520richer%250Ainformation-oriented%2520difficulty%2520%25284/5%2520in%2520difficulty%2529.%2520These%2520findings%2520highlight%250Athat%2520FAME%2520is%2520a%2520good%2520and%2520scalable%2520proxy%2520for%2520real-world%2520meeting%2520conditions.%2520It%250Aenables%2520new%2520test%2520scenarios%2520for%2520meeting%2520summarization%2520research%2520and%2520other%250Aconversation-centric%2520applications%2520in%2520tasks%2520requiring%2520conversation%2520data%2520or%250Asimulating%2520social%2520scenarios%2520under%2520behavioral%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20need%20to%20MIMIC%20to%20get%20FAME%3A%20Solving%20Meeting%20Transcript%20Scarcity%20with%0A%20%20a%20Multi-Agent%20Conversations&entry.906535625=Frederic%20Kirstein%20and%20Muneeb%20Khan%20and%20Jan%20Philip%20Wahle%20and%20Terry%20Ruas%20and%20Bela%20Gipp&entry.1292438233=%20%20Meeting%20summarization%20suffers%20from%20limited%20high-quality%20data%2C%20mainly%20due%20to%0Aprivacy%20restrictions%20and%20expensive%20collection%20processes.%20We%20address%20this%20gap%0Awith%20FAME%2C%20a%20dataset%20of%20500%20meetings%20in%20English%20and%20300%20in%20German%20produced%20by%0AMIMIC%2C%20our%20new%20multi-agent%20meeting%20synthesis%20framework%20that%20generates%20meeting%0Atranscripts%20on%20a%20given%20knowledge%20source%20by%20defining%20psychologically%20grounded%0Aparticipant%20profiles%2C%20outlining%20the%20conversation%2C%20and%20orchestrating%20a%20large%0Alanguage%20model%20%28LLM%29%20debate.%20A%20modular%20post-processing%20step%20refines%20these%0Aoutputs%2C%20mitigating%20potential%20repetitiveness%20and%20overly%20formal%20tones%2C%20ensuring%0Acoherent%2C%20credible%20dialogues%20at%20scale.%20We%20also%20propose%20a%20psychologically%0Agrounded%20evaluation%20framework%20assessing%20naturalness%2C%20social%20behavior%0Aauthenticity%2C%20and%20transcript%20difficulties.%20Human%20assessments%20show%20that%20FAME%0Aapproximates%20real-meeting%20spontaneity%20%284.5/5%20in%20naturalness%29%2C%20preserves%0Aspeaker-centric%20challenges%20%283/5%20in%20spoken%20language%29%2C%20and%20introduces%20richer%0Ainformation-oriented%20difficulty%20%284/5%20in%20difficulty%29.%20These%20findings%20highlight%0Athat%20FAME%20is%20a%20good%20and%20scalable%20proxy%20for%20real-world%20meeting%20conditions.%20It%0Aenables%20new%20test%20scenarios%20for%20meeting%20summarization%20research%20and%20other%0Aconversation-centric%20applications%20in%20tasks%20requiring%20conversation%20data%20or%0Asimulating%20social%20scenarios%20under%20behavioral%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13001v1&entry.124074799=Read"},
{"title": "G-Mapper: Learning a Cover in the Mapper Construction", "author": "Enrique Alvarado and Robin Belton and Emily Fischer and Kang-Ju Lee and Sourabh Palande and Sarah Percival and Emilie Purvine", "abstract": "  The Mapper algorithm is a visualization technique in topological data\nanalysis (TDA) that outputs a graph reflecting the structure of a given\ndataset. However, the Mapper algorithm requires tuning several parameters in\norder to generate a ``nice\" Mapper graph. This paper focuses on selecting the\ncover parameter. We present an algorithm that optimizes the cover of a Mapper\ngraph by splitting a cover repeatedly according to a statistical test for\nnormality. Our algorithm is based on G-means clustering which searches for the\noptimal number of clusters in $k$-means by iteratively applying the\nAnderson-Darling test. Our splitting procedure employs a Gaussian mixture model\nto carefully choose the cover according to the distribution of the given data.\nExperiments for synthetic and real-world datasets demonstrate that our\nalgorithm generates covers so that the Mapper graphs retain the essence of the\ndatasets, while also running significantly faster than a previous iterative\nmethod.\n", "link": "http://arxiv.org/abs/2309.06634v4", "date": "2025-02-18", "relevancy": 2.4027, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4872}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4789}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-Mapper%3A%20Learning%20a%20Cover%20in%20the%20Mapper%20Construction&body=Title%3A%20G-Mapper%3A%20Learning%20a%20Cover%20in%20the%20Mapper%20Construction%0AAuthor%3A%20Enrique%20Alvarado%20and%20Robin%20Belton%20and%20Emily%20Fischer%20and%20Kang-Ju%20Lee%20and%20Sourabh%20Palande%20and%20Sarah%20Percival%20and%20Emilie%20Purvine%0AAbstract%3A%20%20%20The%20Mapper%20algorithm%20is%20a%20visualization%20technique%20in%20topological%20data%0Aanalysis%20%28TDA%29%20that%20outputs%20a%20graph%20reflecting%20the%20structure%20of%20a%20given%0Adataset.%20However%2C%20the%20Mapper%20algorithm%20requires%20tuning%20several%20parameters%20in%0Aorder%20to%20generate%20a%20%60%60nice%22%20Mapper%20graph.%20This%20paper%20focuses%20on%20selecting%20the%0Acover%20parameter.%20We%20present%20an%20algorithm%20that%20optimizes%20the%20cover%20of%20a%20Mapper%0Agraph%20by%20splitting%20a%20cover%20repeatedly%20according%20to%20a%20statistical%20test%20for%0Anormality.%20Our%20algorithm%20is%20based%20on%20G-means%20clustering%20which%20searches%20for%20the%0Aoptimal%20number%20of%20clusters%20in%20%24k%24-means%20by%20iteratively%20applying%20the%0AAnderson-Darling%20test.%20Our%20splitting%20procedure%20employs%20a%20Gaussian%20mixture%20model%0Ato%20carefully%20choose%20the%20cover%20according%20to%20the%20distribution%20of%20the%20given%20data.%0AExperiments%20for%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%0Aalgorithm%20generates%20covers%20so%20that%20the%20Mapper%20graphs%20retain%20the%20essence%20of%20the%0Adatasets%2C%20while%20also%20running%20significantly%20faster%20than%20a%20previous%20iterative%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06634v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-Mapper%253A%2520Learning%2520a%2520Cover%2520in%2520the%2520Mapper%2520Construction%26entry.906535625%3DEnrique%2520Alvarado%2520and%2520Robin%2520Belton%2520and%2520Emily%2520Fischer%2520and%2520Kang-Ju%2520Lee%2520and%2520Sourabh%2520Palande%2520and%2520Sarah%2520Percival%2520and%2520Emilie%2520Purvine%26entry.1292438233%3D%2520%2520The%2520Mapper%2520algorithm%2520is%2520a%2520visualization%2520technique%2520in%2520topological%2520data%250Aanalysis%2520%2528TDA%2529%2520that%2520outputs%2520a%2520graph%2520reflecting%2520the%2520structure%2520of%2520a%2520given%250Adataset.%2520However%252C%2520the%2520Mapper%2520algorithm%2520requires%2520tuning%2520several%2520parameters%2520in%250Aorder%2520to%2520generate%2520a%2520%2560%2560nice%2522%2520Mapper%2520graph.%2520This%2520paper%2520focuses%2520on%2520selecting%2520the%250Acover%2520parameter.%2520We%2520present%2520an%2520algorithm%2520that%2520optimizes%2520the%2520cover%2520of%2520a%2520Mapper%250Agraph%2520by%2520splitting%2520a%2520cover%2520repeatedly%2520according%2520to%2520a%2520statistical%2520test%2520for%250Anormality.%2520Our%2520algorithm%2520is%2520based%2520on%2520G-means%2520clustering%2520which%2520searches%2520for%2520the%250Aoptimal%2520number%2520of%2520clusters%2520in%2520%2524k%2524-means%2520by%2520iteratively%2520applying%2520the%250AAnderson-Darling%2520test.%2520Our%2520splitting%2520procedure%2520employs%2520a%2520Gaussian%2520mixture%2520model%250Ato%2520carefully%2520choose%2520the%2520cover%2520according%2520to%2520the%2520distribution%2520of%2520the%2520given%2520data.%250AExperiments%2520for%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%250Aalgorithm%2520generates%2520covers%2520so%2520that%2520the%2520Mapper%2520graphs%2520retain%2520the%2520essence%2520of%2520the%250Adatasets%252C%2520while%2520also%2520running%2520significantly%2520faster%2520than%2520a%2520previous%2520iterative%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06634v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-Mapper%3A%20Learning%20a%20Cover%20in%20the%20Mapper%20Construction&entry.906535625=Enrique%20Alvarado%20and%20Robin%20Belton%20and%20Emily%20Fischer%20and%20Kang-Ju%20Lee%20and%20Sourabh%20Palande%20and%20Sarah%20Percival%20and%20Emilie%20Purvine&entry.1292438233=%20%20The%20Mapper%20algorithm%20is%20a%20visualization%20technique%20in%20topological%20data%0Aanalysis%20%28TDA%29%20that%20outputs%20a%20graph%20reflecting%20the%20structure%20of%20a%20given%0Adataset.%20However%2C%20the%20Mapper%20algorithm%20requires%20tuning%20several%20parameters%20in%0Aorder%20to%20generate%20a%20%60%60nice%22%20Mapper%20graph.%20This%20paper%20focuses%20on%20selecting%20the%0Acover%20parameter.%20We%20present%20an%20algorithm%20that%20optimizes%20the%20cover%20of%20a%20Mapper%0Agraph%20by%20splitting%20a%20cover%20repeatedly%20according%20to%20a%20statistical%20test%20for%0Anormality.%20Our%20algorithm%20is%20based%20on%20G-means%20clustering%20which%20searches%20for%20the%0Aoptimal%20number%20of%20clusters%20in%20%24k%24-means%20by%20iteratively%20applying%20the%0AAnderson-Darling%20test.%20Our%20splitting%20procedure%20employs%20a%20Gaussian%20mixture%20model%0Ato%20carefully%20choose%20the%20cover%20according%20to%20the%20distribution%20of%20the%20given%20data.%0AExperiments%20for%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%0Aalgorithm%20generates%20covers%20so%20that%20the%20Mapper%20graphs%20retain%20the%20essence%20of%20the%0Adatasets%2C%20while%20also%20running%20significantly%20faster%20than%20a%20previous%20iterative%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06634v4&entry.124074799=Read"},
{"title": "AlignFreeze: Navigating the Impact of Realignment on the Layers of\n  Multilingual Models Across Diverse Languages", "author": "Steve Bakos and F\u00e9lix Gaschi and David Guzm\u00e1n and Riddhi More and Kelly Chutong Li and En-Shiun Annie Lee", "abstract": "  Realignment techniques are often employed to enhance cross-lingual transfer\nin multilingual language models, still, they can sometimes degrade performance\nin languages that differ significantly from the fine-tuned source language.\nThis paper introduces AlignFreeze, a method that freezes either the layers'\nlower half or upper half during realignment. Through controlled experiments on\n4 tasks, 3 models, and in 35 languages, we find that realignment affects all\nthe layers but can be the most detrimental to the lower ones. Freezing the\nlower layers can prevent performance degradation. Particularly, AlignFreeze\nimproves Part-of-Speech (PoS) tagging performances in languages where full\nrealignment fails: with XLM-R, it provides improvements of more than one\nstandard deviation in accuracy in seven more languages than full realignment.\n", "link": "http://arxiv.org/abs/2502.12959v1", "date": "2025-02-18", "relevancy": 2.3942, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignFreeze%3A%20Navigating%20the%20Impact%20of%20Realignment%20on%20the%20Layers%20of%0A%20%20Multilingual%20Models%20Across%20Diverse%20Languages&body=Title%3A%20AlignFreeze%3A%20Navigating%20the%20Impact%20of%20Realignment%20on%20the%20Layers%20of%0A%20%20Multilingual%20Models%20Across%20Diverse%20Languages%0AAuthor%3A%20Steve%20Bakos%20and%20F%C3%A9lix%20Gaschi%20and%20David%20Guzm%C3%A1n%20and%20Riddhi%20More%20and%20Kelly%20Chutong%20Li%20and%20En-Shiun%20Annie%20Lee%0AAbstract%3A%20%20%20Realignment%20techniques%20are%20often%20employed%20to%20enhance%20cross-lingual%20transfer%0Ain%20multilingual%20language%20models%2C%20still%2C%20they%20can%20sometimes%20degrade%20performance%0Ain%20languages%20that%20differ%20significantly%20from%20the%20fine-tuned%20source%20language.%0AThis%20paper%20introduces%20AlignFreeze%2C%20a%20method%20that%20freezes%20either%20the%20layers%27%0Alower%20half%20or%20upper%20half%20during%20realignment.%20Through%20controlled%20experiments%20on%0A4%20tasks%2C%203%20models%2C%20and%20in%2035%20languages%2C%20we%20find%20that%20realignment%20affects%20all%0Athe%20layers%20but%20can%20be%20the%20most%20detrimental%20to%20the%20lower%20ones.%20Freezing%20the%0Alower%20layers%20can%20prevent%20performance%20degradation.%20Particularly%2C%20AlignFreeze%0Aimproves%20Part-of-Speech%20%28PoS%29%20tagging%20performances%20in%20languages%20where%20full%0Arealignment%20fails%3A%20with%20XLM-R%2C%20it%20provides%20improvements%20of%20more%20than%20one%0Astandard%20deviation%20in%20accuracy%20in%20seven%20more%20languages%20than%20full%20realignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignFreeze%253A%2520Navigating%2520the%2520Impact%2520of%2520Realignment%2520on%2520the%2520Layers%2520of%250A%2520%2520Multilingual%2520Models%2520Across%2520Diverse%2520Languages%26entry.906535625%3DSteve%2520Bakos%2520and%2520F%25C3%25A9lix%2520Gaschi%2520and%2520David%2520Guzm%25C3%25A1n%2520and%2520Riddhi%2520More%2520and%2520Kelly%2520Chutong%2520Li%2520and%2520En-Shiun%2520Annie%2520Lee%26entry.1292438233%3D%2520%2520Realignment%2520techniques%2520are%2520often%2520employed%2520to%2520enhance%2520cross-lingual%2520transfer%250Ain%2520multilingual%2520language%2520models%252C%2520still%252C%2520they%2520can%2520sometimes%2520degrade%2520performance%250Ain%2520languages%2520that%2520differ%2520significantly%2520from%2520the%2520fine-tuned%2520source%2520language.%250AThis%2520paper%2520introduces%2520AlignFreeze%252C%2520a%2520method%2520that%2520freezes%2520either%2520the%2520layers%2527%250Alower%2520half%2520or%2520upper%2520half%2520during%2520realignment.%2520Through%2520controlled%2520experiments%2520on%250A4%2520tasks%252C%25203%2520models%252C%2520and%2520in%252035%2520languages%252C%2520we%2520find%2520that%2520realignment%2520affects%2520all%250Athe%2520layers%2520but%2520can%2520be%2520the%2520most%2520detrimental%2520to%2520the%2520lower%2520ones.%2520Freezing%2520the%250Alower%2520layers%2520can%2520prevent%2520performance%2520degradation.%2520Particularly%252C%2520AlignFreeze%250Aimproves%2520Part-of-Speech%2520%2528PoS%2529%2520tagging%2520performances%2520in%2520languages%2520where%2520full%250Arealignment%2520fails%253A%2520with%2520XLM-R%252C%2520it%2520provides%2520improvements%2520of%2520more%2520than%2520one%250Astandard%2520deviation%2520in%2520accuracy%2520in%2520seven%2520more%2520languages%2520than%2520full%2520realignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignFreeze%3A%20Navigating%20the%20Impact%20of%20Realignment%20on%20the%20Layers%20of%0A%20%20Multilingual%20Models%20Across%20Diverse%20Languages&entry.906535625=Steve%20Bakos%20and%20F%C3%A9lix%20Gaschi%20and%20David%20Guzm%C3%A1n%20and%20Riddhi%20More%20and%20Kelly%20Chutong%20Li%20and%20En-Shiun%20Annie%20Lee&entry.1292438233=%20%20Realignment%20techniques%20are%20often%20employed%20to%20enhance%20cross-lingual%20transfer%0Ain%20multilingual%20language%20models%2C%20still%2C%20they%20can%20sometimes%20degrade%20performance%0Ain%20languages%20that%20differ%20significantly%20from%20the%20fine-tuned%20source%20language.%0AThis%20paper%20introduces%20AlignFreeze%2C%20a%20method%20that%20freezes%20either%20the%20layers%27%0Alower%20half%20or%20upper%20half%20during%20realignment.%20Through%20controlled%20experiments%20on%0A4%20tasks%2C%203%20models%2C%20and%20in%2035%20languages%2C%20we%20find%20that%20realignment%20affects%20all%0Athe%20layers%20but%20can%20be%20the%20most%20detrimental%20to%20the%20lower%20ones.%20Freezing%20the%0Alower%20layers%20can%20prevent%20performance%20degradation.%20Particularly%2C%20AlignFreeze%0Aimproves%20Part-of-Speech%20%28PoS%29%20tagging%20performances%20in%20languages%20where%20full%0Arealignment%20fails%3A%20with%20XLM-R%2C%20it%20provides%20improvements%20of%20more%20than%20one%0Astandard%20deviation%20in%20accuracy%20in%20seven%20more%20languages%20than%20full%20realignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12959v1&entry.124074799=Read"},
{"title": "Temporal reasoning for timeline summarisation in social media", "author": "Jiayu Song and Mahmud Akhter and Dana Atzil Slonim and Maria Liakata", "abstract": "  This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation.\n", "link": "http://arxiv.org/abs/2501.00152v2", "date": "2025-02-18", "relevancy": 2.3854, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5137}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20reasoning%20for%20timeline%20summarisation%20in%20social%20media&body=Title%3A%20Temporal%20reasoning%20for%20timeline%20summarisation%20in%20social%20media%0AAuthor%3A%20Jiayu%20Song%20and%20Mahmud%20Akhter%20and%20Dana%20Atzil%20Slonim%20and%20Maria%20Liakata%0AAbstract%3A%20%20%20This%20paper%20explores%20whether%20enhancing%20temporal%20reasoning%20capabilities%20in%0ALarge%20Language%20Models%20%28LLMs%29%20can%20improve%20the%20quality%20of%20timeline%20summarisation%2C%0Athe%20task%20of%20summarising%20long%20texts%20containing%20sequences%20of%20events%2C%20such%20as%0Asocial%20media%20threads.%20We%20first%20introduce%20NarrativeReason%2C%20a%20novel%20dataset%0Afocused%20on%20temporal%20relationships%20among%20sequential%20events%20within%20narratives%2C%0Adistinguishing%20it%20from%20existing%20temporal%20reasoning%20datasets%20that%20primarily%0Aaddress%20pair-wise%20event%20relationships.%20Our%20approach%20then%20combines%20temporal%0Areasoning%20with%20timeline%20summarisation%20through%20a%20knowledge%20distillation%0Aframework%2C%20where%20we%20first%20fine-tune%20a%20teacher%20model%20on%20temporal%20reasoning%20tasks%0Aand%20then%20distill%20this%20knowledge%20into%20a%20student%20model%20while%20simultaneously%0Atraining%20it%20for%20the%20task%20of%20timeline%20summarisation.%20Experimental%20results%0Ademonstrate%20that%20our%20model%20achieves%20superior%20performance%20on%20out-of-domain%0Amental%20health-related%20timeline%20summarisation%20tasks%2C%20which%20involve%20long%20social%0Amedia%20threads%20with%20repetitions%20of%20events%20and%20a%20mix%20of%20emotions%2C%20highlighting%0Athe%20importance%20and%20generalisability%20of%20leveraging%20temporal%20reasoning%20to%20improve%0Atimeline%20summarisation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00152v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520reasoning%2520for%2520timeline%2520summarisation%2520in%2520social%2520media%26entry.906535625%3DJiayu%2520Song%2520and%2520Mahmud%2520Akhter%2520and%2520Dana%2520Atzil%2520Slonim%2520and%2520Maria%2520Liakata%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520whether%2520enhancing%2520temporal%2520reasoning%2520capabilities%2520in%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520improve%2520the%2520quality%2520of%2520timeline%2520summarisation%252C%250Athe%2520task%2520of%2520summarising%2520long%2520texts%2520containing%2520sequences%2520of%2520events%252C%2520such%2520as%250Asocial%2520media%2520threads.%2520We%2520first%2520introduce%2520NarrativeReason%252C%2520a%2520novel%2520dataset%250Afocused%2520on%2520temporal%2520relationships%2520among%2520sequential%2520events%2520within%2520narratives%252C%250Adistinguishing%2520it%2520from%2520existing%2520temporal%2520reasoning%2520datasets%2520that%2520primarily%250Aaddress%2520pair-wise%2520event%2520relationships.%2520Our%2520approach%2520then%2520combines%2520temporal%250Areasoning%2520with%2520timeline%2520summarisation%2520through%2520a%2520knowledge%2520distillation%250Aframework%252C%2520where%2520we%2520first%2520fine-tune%2520a%2520teacher%2520model%2520on%2520temporal%2520reasoning%2520tasks%250Aand%2520then%2520distill%2520this%2520knowledge%2520into%2520a%2520student%2520model%2520while%2520simultaneously%250Atraining%2520it%2520for%2520the%2520task%2520of%2520timeline%2520summarisation.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520model%2520achieves%2520superior%2520performance%2520on%2520out-of-domain%250Amental%2520health-related%2520timeline%2520summarisation%2520tasks%252C%2520which%2520involve%2520long%2520social%250Amedia%2520threads%2520with%2520repetitions%2520of%2520events%2520and%2520a%2520mix%2520of%2520emotions%252C%2520highlighting%250Athe%2520importance%2520and%2520generalisability%2520of%2520leveraging%2520temporal%2520reasoning%2520to%2520improve%250Atimeline%2520summarisation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00152v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20reasoning%20for%20timeline%20summarisation%20in%20social%20media&entry.906535625=Jiayu%20Song%20and%20Mahmud%20Akhter%20and%20Dana%20Atzil%20Slonim%20and%20Maria%20Liakata&entry.1292438233=%20%20This%20paper%20explores%20whether%20enhancing%20temporal%20reasoning%20capabilities%20in%0ALarge%20Language%20Models%20%28LLMs%29%20can%20improve%20the%20quality%20of%20timeline%20summarisation%2C%0Athe%20task%20of%20summarising%20long%20texts%20containing%20sequences%20of%20events%2C%20such%20as%0Asocial%20media%20threads.%20We%20first%20introduce%20NarrativeReason%2C%20a%20novel%20dataset%0Afocused%20on%20temporal%20relationships%20among%20sequential%20events%20within%20narratives%2C%0Adistinguishing%20it%20from%20existing%20temporal%20reasoning%20datasets%20that%20primarily%0Aaddress%20pair-wise%20event%20relationships.%20Our%20approach%20then%20combines%20temporal%0Areasoning%20with%20timeline%20summarisation%20through%20a%20knowledge%20distillation%0Aframework%2C%20where%20we%20first%20fine-tune%20a%20teacher%20model%20on%20temporal%20reasoning%20tasks%0Aand%20then%20distill%20this%20knowledge%20into%20a%20student%20model%20while%20simultaneously%0Atraining%20it%20for%20the%20task%20of%20timeline%20summarisation.%20Experimental%20results%0Ademonstrate%20that%20our%20model%20achieves%20superior%20performance%20on%20out-of-domain%0Amental%20health-related%20timeline%20summarisation%20tasks%2C%20which%20involve%20long%20social%0Amedia%20threads%20with%20repetitions%20of%20events%20and%20a%20mix%20of%20emotions%2C%20highlighting%0Athe%20importance%20and%20generalisability%20of%20leveraging%20temporal%20reasoning%20to%20improve%0Atimeline%20summarisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00152v2&entry.124074799=Read"},
{"title": "Personalized Image Generation with Deep Generative Models: A Decade\n  Survey", "author": "Yuxiang Wei and Yiheng Zheng and Yabo Zhang and Ming Liu and Zhilong Ji and Lei Zhang and Wangmeng Zuo", "abstract": "  Recent advancements in generative models have significantly facilitated the\ndevelopment of personalized content creation. Given a small set of images with\nuser-specific concept, personalized image generation allows to create images\nthat incorporate the specified concept and adhere to provided text\ndescriptions. Due to its wide applications in content creation, significant\neffort has been devoted to this field in recent years. Nonetheless, the\ntechnologies used for personalization have evolved alongside the development of\ngenerative models, with their distinct and interrelated components. In this\nsurvey, we present a comprehensive review of generalized personalized image\ngeneration across various generative models, including traditional GANs,\ncontemporary text-to-image diffusion models, and emerging multi-model\nautoregressive models. We first define a unified framework that standardizes\nthe personalization process across different generative models, encompassing\nthree key components, i.e., inversion spaces, inversion methods, and\npersonalization schemes. This unified framework offers a structured approach to\ndissecting and comparing personalization techniques across different generative\narchitectures. Building upon this unified framework, we further provide an\nin-depth analysis of personalization techniques within each generative model,\nhighlighting their unique contributions and innovations. Through comparative\nanalysis, this survey elucidates the current landscape of personalized image\ngeneration, identifying commonalities and distinguishing features among\nexisting methods. Finally, we discuss the open challenges in the field and\npropose potential directions for future research. We keep tracing related works\nat https://github.com/csyxwei/Awesome-Personalized-Image-Generation.\n", "link": "http://arxiv.org/abs/2502.13081v1", "date": "2025-02-18", "relevancy": 2.3327, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.604}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5701}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Image%20Generation%20with%20Deep%20Generative%20Models%3A%20A%20Decade%0A%20%20Survey&body=Title%3A%20Personalized%20Image%20Generation%20with%20Deep%20Generative%20Models%3A%20A%20Decade%0A%20%20Survey%0AAuthor%3A%20Yuxiang%20Wei%20and%20Yiheng%20Zheng%20and%20Yabo%20Zhang%20and%20Ming%20Liu%20and%20Zhilong%20Ji%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generative%20models%20have%20significantly%20facilitated%20the%0Adevelopment%20of%20personalized%20content%20creation.%20Given%20a%20small%20set%20of%20images%20with%0Auser-specific%20concept%2C%20personalized%20image%20generation%20allows%20to%20create%20images%0Athat%20incorporate%20the%20specified%20concept%20and%20adhere%20to%20provided%20text%0Adescriptions.%20Due%20to%20its%20wide%20applications%20in%20content%20creation%2C%20significant%0Aeffort%20has%20been%20devoted%20to%20this%20field%20in%20recent%20years.%20Nonetheless%2C%20the%0Atechnologies%20used%20for%20personalization%20have%20evolved%20alongside%20the%20development%20of%0Agenerative%20models%2C%20with%20their%20distinct%20and%20interrelated%20components.%20In%20this%0Asurvey%2C%20we%20present%20a%20comprehensive%20review%20of%20generalized%20personalized%20image%0Ageneration%20across%20various%20generative%20models%2C%20including%20traditional%20GANs%2C%0Acontemporary%20text-to-image%20diffusion%20models%2C%20and%20emerging%20multi-model%0Aautoregressive%20models.%20We%20first%20define%20a%20unified%20framework%20that%20standardizes%0Athe%20personalization%20process%20across%20different%20generative%20models%2C%20encompassing%0Athree%20key%20components%2C%20i.e.%2C%20inversion%20spaces%2C%20inversion%20methods%2C%20and%0Apersonalization%20schemes.%20This%20unified%20framework%20offers%20a%20structured%20approach%20to%0Adissecting%20and%20comparing%20personalization%20techniques%20across%20different%20generative%0Aarchitectures.%20Building%20upon%20this%20unified%20framework%2C%20we%20further%20provide%20an%0Ain-depth%20analysis%20of%20personalization%20techniques%20within%20each%20generative%20model%2C%0Ahighlighting%20their%20unique%20contributions%20and%20innovations.%20Through%20comparative%0Aanalysis%2C%20this%20survey%20elucidates%20the%20current%20landscape%20of%20personalized%20image%0Ageneration%2C%20identifying%20commonalities%20and%20distinguishing%20features%20among%0Aexisting%20methods.%20Finally%2C%20we%20discuss%20the%20open%20challenges%20in%20the%20field%20and%0Apropose%20potential%20directions%20for%20future%20research.%20We%20keep%20tracing%20related%20works%0Aat%20https%3A//github.com/csyxwei/Awesome-Personalized-Image-Generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Image%2520Generation%2520with%2520Deep%2520Generative%2520Models%253A%2520A%2520Decade%250A%2520%2520Survey%26entry.906535625%3DYuxiang%2520Wei%2520and%2520Yiheng%2520Zheng%2520and%2520Yabo%2520Zhang%2520and%2520Ming%2520Liu%2520and%2520Zhilong%2520Ji%2520and%2520Lei%2520Zhang%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generative%2520models%2520have%2520significantly%2520facilitated%2520the%250Adevelopment%2520of%2520personalized%2520content%2520creation.%2520Given%2520a%2520small%2520set%2520of%2520images%2520with%250Auser-specific%2520concept%252C%2520personalized%2520image%2520generation%2520allows%2520to%2520create%2520images%250Athat%2520incorporate%2520the%2520specified%2520concept%2520and%2520adhere%2520to%2520provided%2520text%250Adescriptions.%2520Due%2520to%2520its%2520wide%2520applications%2520in%2520content%2520creation%252C%2520significant%250Aeffort%2520has%2520been%2520devoted%2520to%2520this%2520field%2520in%2520recent%2520years.%2520Nonetheless%252C%2520the%250Atechnologies%2520used%2520for%2520personalization%2520have%2520evolved%2520alongside%2520the%2520development%2520of%250Agenerative%2520models%252C%2520with%2520their%2520distinct%2520and%2520interrelated%2520components.%2520In%2520this%250Asurvey%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520of%2520generalized%2520personalized%2520image%250Ageneration%2520across%2520various%2520generative%2520models%252C%2520including%2520traditional%2520GANs%252C%250Acontemporary%2520text-to-image%2520diffusion%2520models%252C%2520and%2520emerging%2520multi-model%250Aautoregressive%2520models.%2520We%2520first%2520define%2520a%2520unified%2520framework%2520that%2520standardizes%250Athe%2520personalization%2520process%2520across%2520different%2520generative%2520models%252C%2520encompassing%250Athree%2520key%2520components%252C%2520i.e.%252C%2520inversion%2520spaces%252C%2520inversion%2520methods%252C%2520and%250Apersonalization%2520schemes.%2520This%2520unified%2520framework%2520offers%2520a%2520structured%2520approach%2520to%250Adissecting%2520and%2520comparing%2520personalization%2520techniques%2520across%2520different%2520generative%250Aarchitectures.%2520Building%2520upon%2520this%2520unified%2520framework%252C%2520we%2520further%2520provide%2520an%250Ain-depth%2520analysis%2520of%2520personalization%2520techniques%2520within%2520each%2520generative%2520model%252C%250Ahighlighting%2520their%2520unique%2520contributions%2520and%2520innovations.%2520Through%2520comparative%250Aanalysis%252C%2520this%2520survey%2520elucidates%2520the%2520current%2520landscape%2520of%2520personalized%2520image%250Ageneration%252C%2520identifying%2520commonalities%2520and%2520distinguishing%2520features%2520among%250Aexisting%2520methods.%2520Finally%252C%2520we%2520discuss%2520the%2520open%2520challenges%2520in%2520the%2520field%2520and%250Apropose%2520potential%2520directions%2520for%2520future%2520research.%2520We%2520keep%2520tracing%2520related%2520works%250Aat%2520https%253A//github.com/csyxwei/Awesome-Personalized-Image-Generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Image%20Generation%20with%20Deep%20Generative%20Models%3A%20A%20Decade%0A%20%20Survey&entry.906535625=Yuxiang%20Wei%20and%20Yiheng%20Zheng%20and%20Yabo%20Zhang%20and%20Ming%20Liu%20and%20Zhilong%20Ji%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Recent%20advancements%20in%20generative%20models%20have%20significantly%20facilitated%20the%0Adevelopment%20of%20personalized%20content%20creation.%20Given%20a%20small%20set%20of%20images%20with%0Auser-specific%20concept%2C%20personalized%20image%20generation%20allows%20to%20create%20images%0Athat%20incorporate%20the%20specified%20concept%20and%20adhere%20to%20provided%20text%0Adescriptions.%20Due%20to%20its%20wide%20applications%20in%20content%20creation%2C%20significant%0Aeffort%20has%20been%20devoted%20to%20this%20field%20in%20recent%20years.%20Nonetheless%2C%20the%0Atechnologies%20used%20for%20personalization%20have%20evolved%20alongside%20the%20development%20of%0Agenerative%20models%2C%20with%20their%20distinct%20and%20interrelated%20components.%20In%20this%0Asurvey%2C%20we%20present%20a%20comprehensive%20review%20of%20generalized%20personalized%20image%0Ageneration%20across%20various%20generative%20models%2C%20including%20traditional%20GANs%2C%0Acontemporary%20text-to-image%20diffusion%20models%2C%20and%20emerging%20multi-model%0Aautoregressive%20models.%20We%20first%20define%20a%20unified%20framework%20that%20standardizes%0Athe%20personalization%20process%20across%20different%20generative%20models%2C%20encompassing%0Athree%20key%20components%2C%20i.e.%2C%20inversion%20spaces%2C%20inversion%20methods%2C%20and%0Apersonalization%20schemes.%20This%20unified%20framework%20offers%20a%20structured%20approach%20to%0Adissecting%20and%20comparing%20personalization%20techniques%20across%20different%20generative%0Aarchitectures.%20Building%20upon%20this%20unified%20framework%2C%20we%20further%20provide%20an%0Ain-depth%20analysis%20of%20personalization%20techniques%20within%20each%20generative%20model%2C%0Ahighlighting%20their%20unique%20contributions%20and%20innovations.%20Through%20comparative%0Aanalysis%2C%20this%20survey%20elucidates%20the%20current%20landscape%20of%20personalized%20image%0Ageneration%2C%20identifying%20commonalities%20and%20distinguishing%20features%20among%0Aexisting%20methods.%20Finally%2C%20we%20discuss%20the%20open%20challenges%20in%20the%20field%20and%0Apropose%20potential%20directions%20for%20future%20research.%20We%20keep%20tracing%20related%20works%0Aat%20https%3A//github.com/csyxwei/Awesome-Personalized-Image-Generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13081v1&entry.124074799=Read"},
{"title": "TS40K: a 3D Point Cloud Dataset of Rural Terrain and Electrical\n  Transmission System", "author": "Diogo Lavado and Cl\u00e1udia Soares and Alessandra Micheletti and Ricardo Santos and Andr\u00e9 Coelho and Jo\u00e3o Santos", "abstract": "  Research on supervised learning algorithms in 3D scene understanding has\nrisen in prominence and witness great increases in performance across several\ndatasets. The leading force of this research is the problem of autonomous\ndriving followed by indoor scene segmentation. However, openly available 3D\ndata on these tasks mainly focuses on urban scenarios. In this paper, we\npropose TS40K, a 3D point cloud dataset that encompasses more than 40,000 Km on\nelectrical transmission systems situated in European rural terrain. This is not\nonly a novel problem for the research community that can aid in the high-risk\nmission of power-grid inspection, but it also offers 3D point clouds with\ndistinct characteristics from those in self-driving and indoor 3D data, such as\nhigh point-density and no occlusion. In our dataset, each 3D point is labeled\nwith 1 out of 22 annotated classes. We evaluate the performance of\nstate-of-the-art methods on our dataset concerning 3D semantic segmentation and\n3D object detection. Finally, we provide a comprehensive analysis of the\nresults along with key challenges such as using labels that were not originally\nintended for learning tasks.\n", "link": "http://arxiv.org/abs/2405.13989v2", "date": "2025-02-18", "relevancy": 2.3324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TS40K%3A%20a%203D%20Point%20Cloud%20Dataset%20of%20Rural%20Terrain%20and%20Electrical%0A%20%20Transmission%20System&body=Title%3A%20TS40K%3A%20a%203D%20Point%20Cloud%20Dataset%20of%20Rural%20Terrain%20and%20Electrical%0A%20%20Transmission%20System%0AAuthor%3A%20Diogo%20Lavado%20and%20Cl%C3%A1udia%20Soares%20and%20Alessandra%20Micheletti%20and%20Ricardo%20Santos%20and%20Andr%C3%A9%20Coelho%20and%20Jo%C3%A3o%20Santos%0AAbstract%3A%20%20%20Research%20on%20supervised%20learning%20algorithms%20in%203D%20scene%20understanding%20has%0Arisen%20in%20prominence%20and%20witness%20great%20increases%20in%20performance%20across%20several%0Adatasets.%20The%20leading%20force%20of%20this%20research%20is%20the%20problem%20of%20autonomous%0Adriving%20followed%20by%20indoor%20scene%20segmentation.%20However%2C%20openly%20available%203D%0Adata%20on%20these%20tasks%20mainly%20focuses%20on%20urban%20scenarios.%20In%20this%20paper%2C%20we%0Apropose%20TS40K%2C%20a%203D%20point%20cloud%20dataset%20that%20encompasses%20more%20than%2040%2C000%20Km%20on%0Aelectrical%20transmission%20systems%20situated%20in%20European%20rural%20terrain.%20This%20is%20not%0Aonly%20a%20novel%20problem%20for%20the%20research%20community%20that%20can%20aid%20in%20the%20high-risk%0Amission%20of%20power-grid%20inspection%2C%20but%20it%20also%20offers%203D%20point%20clouds%20with%0Adistinct%20characteristics%20from%20those%20in%20self-driving%20and%20indoor%203D%20data%2C%20such%20as%0Ahigh%20point-density%20and%20no%20occlusion.%20In%20our%20dataset%2C%20each%203D%20point%20is%20labeled%0Awith%201%20out%20of%2022%20annotated%20classes.%20We%20evaluate%20the%20performance%20of%0Astate-of-the-art%20methods%20on%20our%20dataset%20concerning%203D%20semantic%20segmentation%20and%0A3D%20object%20detection.%20Finally%2C%20we%20provide%20a%20comprehensive%20analysis%20of%20the%0Aresults%20along%20with%20key%20challenges%20such%20as%20using%20labels%20that%20were%20not%20originally%0Aintended%20for%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13989v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTS40K%253A%2520a%25203D%2520Point%2520Cloud%2520Dataset%2520of%2520Rural%2520Terrain%2520and%2520Electrical%250A%2520%2520Transmission%2520System%26entry.906535625%3DDiogo%2520Lavado%2520and%2520Cl%25C3%25A1udia%2520Soares%2520and%2520Alessandra%2520Micheletti%2520and%2520Ricardo%2520Santos%2520and%2520Andr%25C3%25A9%2520Coelho%2520and%2520Jo%25C3%25A3o%2520Santos%26entry.1292438233%3D%2520%2520Research%2520on%2520supervised%2520learning%2520algorithms%2520in%25203D%2520scene%2520understanding%2520has%250Arisen%2520in%2520prominence%2520and%2520witness%2520great%2520increases%2520in%2520performance%2520across%2520several%250Adatasets.%2520The%2520leading%2520force%2520of%2520this%2520research%2520is%2520the%2520problem%2520of%2520autonomous%250Adriving%2520followed%2520by%2520indoor%2520scene%2520segmentation.%2520However%252C%2520openly%2520available%25203D%250Adata%2520on%2520these%2520tasks%2520mainly%2520focuses%2520on%2520urban%2520scenarios.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520TS40K%252C%2520a%25203D%2520point%2520cloud%2520dataset%2520that%2520encompasses%2520more%2520than%252040%252C000%2520Km%2520on%250Aelectrical%2520transmission%2520systems%2520situated%2520in%2520European%2520rural%2520terrain.%2520This%2520is%2520not%250Aonly%2520a%2520novel%2520problem%2520for%2520the%2520research%2520community%2520that%2520can%2520aid%2520in%2520the%2520high-risk%250Amission%2520of%2520power-grid%2520inspection%252C%2520but%2520it%2520also%2520offers%25203D%2520point%2520clouds%2520with%250Adistinct%2520characteristics%2520from%2520those%2520in%2520self-driving%2520and%2520indoor%25203D%2520data%252C%2520such%2520as%250Ahigh%2520point-density%2520and%2520no%2520occlusion.%2520In%2520our%2520dataset%252C%2520each%25203D%2520point%2520is%2520labeled%250Awith%25201%2520out%2520of%252022%2520annotated%2520classes.%2520We%2520evaluate%2520the%2520performance%2520of%250Astate-of-the-art%2520methods%2520on%2520our%2520dataset%2520concerning%25203D%2520semantic%2520segmentation%2520and%250A3D%2520object%2520detection.%2520Finally%252C%2520we%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520the%250Aresults%2520along%2520with%2520key%2520challenges%2520such%2520as%2520using%2520labels%2520that%2520were%2520not%2520originally%250Aintended%2520for%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13989v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TS40K%3A%20a%203D%20Point%20Cloud%20Dataset%20of%20Rural%20Terrain%20and%20Electrical%0A%20%20Transmission%20System&entry.906535625=Diogo%20Lavado%20and%20Cl%C3%A1udia%20Soares%20and%20Alessandra%20Micheletti%20and%20Ricardo%20Santos%20and%20Andr%C3%A9%20Coelho%20and%20Jo%C3%A3o%20Santos&entry.1292438233=%20%20Research%20on%20supervised%20learning%20algorithms%20in%203D%20scene%20understanding%20has%0Arisen%20in%20prominence%20and%20witness%20great%20increases%20in%20performance%20across%20several%0Adatasets.%20The%20leading%20force%20of%20this%20research%20is%20the%20problem%20of%20autonomous%0Adriving%20followed%20by%20indoor%20scene%20segmentation.%20However%2C%20openly%20available%203D%0Adata%20on%20these%20tasks%20mainly%20focuses%20on%20urban%20scenarios.%20In%20this%20paper%2C%20we%0Apropose%20TS40K%2C%20a%203D%20point%20cloud%20dataset%20that%20encompasses%20more%20than%2040%2C000%20Km%20on%0Aelectrical%20transmission%20systems%20situated%20in%20European%20rural%20terrain.%20This%20is%20not%0Aonly%20a%20novel%20problem%20for%20the%20research%20community%20that%20can%20aid%20in%20the%20high-risk%0Amission%20of%20power-grid%20inspection%2C%20but%20it%20also%20offers%203D%20point%20clouds%20with%0Adistinct%20characteristics%20from%20those%20in%20self-driving%20and%20indoor%203D%20data%2C%20such%20as%0Ahigh%20point-density%20and%20no%20occlusion.%20In%20our%20dataset%2C%20each%203D%20point%20is%20labeled%0Awith%201%20out%20of%2022%20annotated%20classes.%20We%20evaluate%20the%20performance%20of%0Astate-of-the-art%20methods%20on%20our%20dataset%20concerning%203D%20semantic%20segmentation%20and%0A3D%20object%20detection.%20Finally%2C%20we%20provide%20a%20comprehensive%20analysis%20of%20the%0Aresults%20along%20with%20key%20challenges%20such%20as%20using%20labels%20that%20were%20not%20originally%0Aintended%20for%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13989v2&entry.124074799=Read"},
{"title": "T2VEval: T2V-generated Videos Benchmark Dataset and Objective Evaluation\n  Method", "author": "Zelu Qi and Ping Shi and Shuqi Wang and Zhaoyang Zhang and Fei Zhao and Zefeng Ying and Da Pan", "abstract": "  Recent advances in text-to-video (T2V) technology, as demonstrated by models\nsuch as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the\napplicability and popularity of the technology. This progress has created a\ngrowing demand for accurate quality assessment metrics to evaluate the\nperceptual quality of T2V-generated videos and optimize video generation\nmodels. However, assessing the quality of text-to-video outputs remain\nchallenging due to the presence of highly complex distortions, such as\nunnatural actions and phenomena that defy human cognition. To address these\nchallenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset\nfor text-to-video quality evaluation, which contains 148 textual prompts and\n1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation,\nwe scored each video on four dimensions in the subjective experiment, which are\noverall impression, text-video consistency, realness, and technical quality.\nBased on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for\nT2V quality evaluation. T2VEval assesses videos across three branches:\ntext-video consistency, realness, and technical quality. Using an\nattention-based fusion module, T2VEval effectively integrates features from\neach branch and predicts scores with the aid of a large language model.\nAdditionally, we implemented a divide-and-conquer training strategy, enabling\neach branch to learn targeted knowledge while maintaining synergy with the\nothers. Experimental results demonstrate that T2VEval achieves state-of-the-art\nperformance across multiple metrics.\n", "link": "http://arxiv.org/abs/2501.08545v4", "date": "2025-02-18", "relevancy": 2.3212, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6148}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5985}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2VEval%3A%20T2V-generated%20Videos%20Benchmark%20Dataset%20and%20Objective%20Evaluation%0A%20%20Method&body=Title%3A%20T2VEval%3A%20T2V-generated%20Videos%20Benchmark%20Dataset%20and%20Objective%20Evaluation%0A%20%20Method%0AAuthor%3A%20Zelu%20Qi%20and%20Ping%20Shi%20and%20Shuqi%20Wang%20and%20Zhaoyang%20Zhang%20and%20Fei%20Zhao%20and%20Zefeng%20Ying%20and%20Da%20Pan%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20technology%2C%20as%20demonstrated%20by%20models%0Asuch%20as%20Runway%20Gen-3%2C%20Pika%2C%20Sora%2C%20and%20Kling%2C%20have%20significantly%20broadened%20the%0Aapplicability%20and%20popularity%20of%20the%20technology.%20This%20progress%20has%20created%20a%0Agrowing%20demand%20for%20accurate%20quality%20assessment%20metrics%20to%20evaluate%20the%0Aperceptual%20quality%20of%20T2V-generated%20videos%20and%20optimize%20video%20generation%0Amodels.%20However%2C%20assessing%20the%20quality%20of%20text-to-video%20outputs%20remain%0Achallenging%20due%20to%20the%20presence%20of%20highly%20complex%20distortions%2C%20such%20as%0Aunnatural%20actions%20and%20phenomena%20that%20defy%20human%20cognition.%20To%20address%20these%0Achallenges%2C%20we%20constructed%20T2VEval-Bench%2C%20a%20multi-dimensional%20benchmark%20dataset%0Afor%20text-to-video%20quality%20evaluation%2C%20which%20contains%20148%20textual%20prompts%20and%0A1%2C783%20videos%20generated%20by%2013%20T2V%20models.%20To%20ensure%20a%20comprehensive%20evaluation%2C%0Awe%20scored%20each%20video%20on%20four%20dimensions%20in%20the%20subjective%20experiment%2C%20which%20are%0Aoverall%20impression%2C%20text-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%0ABased%20on%20T2VEval-Bench%2C%20we%20developed%20T2VEval%2C%20a%20multi-branch%20fusion%20scheme%20for%0AT2V%20quality%20evaluation.%20T2VEval%20assesses%20videos%20across%20three%20branches%3A%0Atext-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%20Using%20an%0Aattention-based%20fusion%20module%2C%20T2VEval%20effectively%20integrates%20features%20from%0Aeach%20branch%20and%20predicts%20scores%20with%20the%20aid%20of%20a%20large%20language%20model.%0AAdditionally%2C%20we%20implemented%20a%20divide-and-conquer%20training%20strategy%2C%20enabling%0Aeach%20branch%20to%20learn%20targeted%20knowledge%20while%20maintaining%20synergy%20with%20the%0Aothers.%20Experimental%20results%20demonstrate%20that%20T2VEval%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08545v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2VEval%253A%2520T2V-generated%2520Videos%2520Benchmark%2520Dataset%2520and%2520Objective%2520Evaluation%250A%2520%2520Method%26entry.906535625%3DZelu%2520Qi%2520and%2520Ping%2520Shi%2520and%2520Shuqi%2520Wang%2520and%2520Zhaoyang%2520Zhang%2520and%2520Fei%2520Zhao%2520and%2520Zefeng%2520Ying%2520and%2520Da%2520Pan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-video%2520%2528T2V%2529%2520technology%252C%2520as%2520demonstrated%2520by%2520models%250Asuch%2520as%2520Runway%2520Gen-3%252C%2520Pika%252C%2520Sora%252C%2520and%2520Kling%252C%2520have%2520significantly%2520broadened%2520the%250Aapplicability%2520and%2520popularity%2520of%2520the%2520technology.%2520This%2520progress%2520has%2520created%2520a%250Agrowing%2520demand%2520for%2520accurate%2520quality%2520assessment%2520metrics%2520to%2520evaluate%2520the%250Aperceptual%2520quality%2520of%2520T2V-generated%2520videos%2520and%2520optimize%2520video%2520generation%250Amodels.%2520However%252C%2520assessing%2520the%2520quality%2520of%2520text-to-video%2520outputs%2520remain%250Achallenging%2520due%2520to%2520the%2520presence%2520of%2520highly%2520complex%2520distortions%252C%2520such%2520as%250Aunnatural%2520actions%2520and%2520phenomena%2520that%2520defy%2520human%2520cognition.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520constructed%2520T2VEval-Bench%252C%2520a%2520multi-dimensional%2520benchmark%2520dataset%250Afor%2520text-to-video%2520quality%2520evaluation%252C%2520which%2520contains%2520148%2520textual%2520prompts%2520and%250A1%252C783%2520videos%2520generated%2520by%252013%2520T2V%2520models.%2520To%2520ensure%2520a%2520comprehensive%2520evaluation%252C%250Awe%2520scored%2520each%2520video%2520on%2520four%2520dimensions%2520in%2520the%2520subjective%2520experiment%252C%2520which%2520are%250Aoverall%2520impression%252C%2520text-video%2520consistency%252C%2520realness%252C%2520and%2520technical%2520quality.%250ABased%2520on%2520T2VEval-Bench%252C%2520we%2520developed%2520T2VEval%252C%2520a%2520multi-branch%2520fusion%2520scheme%2520for%250AT2V%2520quality%2520evaluation.%2520T2VEval%2520assesses%2520videos%2520across%2520three%2520branches%253A%250Atext-video%2520consistency%252C%2520realness%252C%2520and%2520technical%2520quality.%2520Using%2520an%250Aattention-based%2520fusion%2520module%252C%2520T2VEval%2520effectively%2520integrates%2520features%2520from%250Aeach%2520branch%2520and%2520predicts%2520scores%2520with%2520the%2520aid%2520of%2520a%2520large%2520language%2520model.%250AAdditionally%252C%2520we%2520implemented%2520a%2520divide-and-conquer%2520training%2520strategy%252C%2520enabling%250Aeach%2520branch%2520to%2520learn%2520targeted%2520knowledge%2520while%2520maintaining%2520synergy%2520with%2520the%250Aothers.%2520Experimental%2520results%2520demonstrate%2520that%2520T2VEval%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520multiple%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08545v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2VEval%3A%20T2V-generated%20Videos%20Benchmark%20Dataset%20and%20Objective%20Evaluation%0A%20%20Method&entry.906535625=Zelu%20Qi%20and%20Ping%20Shi%20and%20Shuqi%20Wang%20and%20Zhaoyang%20Zhang%20and%20Fei%20Zhao%20and%20Zefeng%20Ying%20and%20Da%20Pan&entry.1292438233=%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20technology%2C%20as%20demonstrated%20by%20models%0Asuch%20as%20Runway%20Gen-3%2C%20Pika%2C%20Sora%2C%20and%20Kling%2C%20have%20significantly%20broadened%20the%0Aapplicability%20and%20popularity%20of%20the%20technology.%20This%20progress%20has%20created%20a%0Agrowing%20demand%20for%20accurate%20quality%20assessment%20metrics%20to%20evaluate%20the%0Aperceptual%20quality%20of%20T2V-generated%20videos%20and%20optimize%20video%20generation%0Amodels.%20However%2C%20assessing%20the%20quality%20of%20text-to-video%20outputs%20remain%0Achallenging%20due%20to%20the%20presence%20of%20highly%20complex%20distortions%2C%20such%20as%0Aunnatural%20actions%20and%20phenomena%20that%20defy%20human%20cognition.%20To%20address%20these%0Achallenges%2C%20we%20constructed%20T2VEval-Bench%2C%20a%20multi-dimensional%20benchmark%20dataset%0Afor%20text-to-video%20quality%20evaluation%2C%20which%20contains%20148%20textual%20prompts%20and%0A1%2C783%20videos%20generated%20by%2013%20T2V%20models.%20To%20ensure%20a%20comprehensive%20evaluation%2C%0Awe%20scored%20each%20video%20on%20four%20dimensions%20in%20the%20subjective%20experiment%2C%20which%20are%0Aoverall%20impression%2C%20text-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%0ABased%20on%20T2VEval-Bench%2C%20we%20developed%20T2VEval%2C%20a%20multi-branch%20fusion%20scheme%20for%0AT2V%20quality%20evaluation.%20T2VEval%20assesses%20videos%20across%20three%20branches%3A%0Atext-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%20Using%20an%0Aattention-based%20fusion%20module%2C%20T2VEval%20effectively%20integrates%20features%20from%0Aeach%20branch%20and%20predicts%20scores%20with%20the%20aid%20of%20a%20large%20language%20model.%0AAdditionally%2C%20we%20implemented%20a%20divide-and-conquer%20training%20strategy%2C%20enabling%0Aeach%20branch%20to%20learn%20targeted%20knowledge%20while%20maintaining%20synergy%20with%20the%0Aothers.%20Experimental%20results%20demonstrate%20that%20T2VEval%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08545v4&entry.124074799=Read"},
{"title": "Continual Learning from Simulated Interactions via Multitask Prospective\n  Rehearsal for Bionic Limb Behavior Modeling", "author": "Sharmita Dey and Benjamin Paassen and Sarath Ravindran Nair and Sabri Boughorbel and Arndt F. Schilling", "abstract": "  Lower limb amputations and neuromuscular impairments severely restrict\nmobility, necessitating advancements beyond conventional prosthetics. While\nmotorized bionic limbs show promise, their effectiveness depends on replicating\nthe dynamic coordination of human movement across diverse environments. In this\npaper, we introduce a model for human behavior in the context of bionic\nprosthesis control. Our approach leverages human locomotion demonstrations to\nlearn the synergistic coupling of the lower limbs, enabling the prediction of\nthe kinematic behavior of a missing limb during tasks such as walking, climbing\ninclines, and stairs. We propose a multitasking, continually adaptive model\nthat anticipates and refines movements over time. At the core of our method is\na technique called multitask prospective rehearsal, that anticipates and\nsynthesizes future movements based on the previous prediction and employs a\ncorrective mechanism for subsequent predictions. Our evolving architecture\nmerges lightweight, task-specific modules on a shared backbone, ensuring both\nspecificity and scalability. We validate our model through experiments on\nreal-world human gait datasets, including transtibial amputees, across a wide\nrange of locomotion tasks. Results demonstrate that our approach consistently\noutperforms baseline models, particularly in scenarios with distributional\nshifts, adversarial perturbations, and noise.\n", "link": "http://arxiv.org/abs/2405.01114v3", "date": "2025-02-18", "relevancy": 2.3197, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6485}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5676}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20from%20Simulated%20Interactions%20via%20Multitask%20Prospective%0A%20%20Rehearsal%20for%20Bionic%20Limb%20Behavior%20Modeling&body=Title%3A%20Continual%20Learning%20from%20Simulated%20Interactions%20via%20Multitask%20Prospective%0A%20%20Rehearsal%20for%20Bionic%20Limb%20Behavior%20Modeling%0AAuthor%3A%20Sharmita%20Dey%20and%20Benjamin%20Paassen%20and%20Sarath%20Ravindran%20Nair%20and%20Sabri%20Boughorbel%20and%20Arndt%20F.%20Schilling%0AAbstract%3A%20%20%20Lower%20limb%20amputations%20and%20neuromuscular%20impairments%20severely%20restrict%0Amobility%2C%20necessitating%20advancements%20beyond%20conventional%20prosthetics.%20While%0Amotorized%20bionic%20limbs%20show%20promise%2C%20their%20effectiveness%20depends%20on%20replicating%0Athe%20dynamic%20coordination%20of%20human%20movement%20across%20diverse%20environments.%20In%20this%0Apaper%2C%20we%20introduce%20a%20model%20for%20human%20behavior%20in%20the%20context%20of%20bionic%0Aprosthesis%20control.%20Our%20approach%20leverages%20human%20locomotion%20demonstrations%20to%0Alearn%20the%20synergistic%20coupling%20of%20the%20lower%20limbs%2C%20enabling%20the%20prediction%20of%0Athe%20kinematic%20behavior%20of%20a%20missing%20limb%20during%20tasks%20such%20as%20walking%2C%20climbing%0Ainclines%2C%20and%20stairs.%20We%20propose%20a%20multitasking%2C%20continually%20adaptive%20model%0Athat%20anticipates%20and%20refines%20movements%20over%20time.%20At%20the%20core%20of%20our%20method%20is%0Aa%20technique%20called%20multitask%20prospective%20rehearsal%2C%20that%20anticipates%20and%0Asynthesizes%20future%20movements%20based%20on%20the%20previous%20prediction%20and%20employs%20a%0Acorrective%20mechanism%20for%20subsequent%20predictions.%20Our%20evolving%20architecture%0Amerges%20lightweight%2C%20task-specific%20modules%20on%20a%20shared%20backbone%2C%20ensuring%20both%0Aspecificity%20and%20scalability.%20We%20validate%20our%20model%20through%20experiments%20on%0Areal-world%20human%20gait%20datasets%2C%20including%20transtibial%20amputees%2C%20across%20a%20wide%0Arange%20of%20locomotion%20tasks.%20Results%20demonstrate%20that%20our%20approach%20consistently%0Aoutperforms%20baseline%20models%2C%20particularly%20in%20scenarios%20with%20distributional%0Ashifts%2C%20adversarial%20perturbations%2C%20and%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01114v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520from%2520Simulated%2520Interactions%2520via%2520Multitask%2520Prospective%250A%2520%2520Rehearsal%2520for%2520Bionic%2520Limb%2520Behavior%2520Modeling%26entry.906535625%3DSharmita%2520Dey%2520and%2520Benjamin%2520Paassen%2520and%2520Sarath%2520Ravindran%2520Nair%2520and%2520Sabri%2520Boughorbel%2520and%2520Arndt%2520F.%2520Schilling%26entry.1292438233%3D%2520%2520Lower%2520limb%2520amputations%2520and%2520neuromuscular%2520impairments%2520severely%2520restrict%250Amobility%252C%2520necessitating%2520advancements%2520beyond%2520conventional%2520prosthetics.%2520While%250Amotorized%2520bionic%2520limbs%2520show%2520promise%252C%2520their%2520effectiveness%2520depends%2520on%2520replicating%250Athe%2520dynamic%2520coordination%2520of%2520human%2520movement%2520across%2520diverse%2520environments.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520model%2520for%2520human%2520behavior%2520in%2520the%2520context%2520of%2520bionic%250Aprosthesis%2520control.%2520Our%2520approach%2520leverages%2520human%2520locomotion%2520demonstrations%2520to%250Alearn%2520the%2520synergistic%2520coupling%2520of%2520the%2520lower%2520limbs%252C%2520enabling%2520the%2520prediction%2520of%250Athe%2520kinematic%2520behavior%2520of%2520a%2520missing%2520limb%2520during%2520tasks%2520such%2520as%2520walking%252C%2520climbing%250Ainclines%252C%2520and%2520stairs.%2520We%2520propose%2520a%2520multitasking%252C%2520continually%2520adaptive%2520model%250Athat%2520anticipates%2520and%2520refines%2520movements%2520over%2520time.%2520At%2520the%2520core%2520of%2520our%2520method%2520is%250Aa%2520technique%2520called%2520multitask%2520prospective%2520rehearsal%252C%2520that%2520anticipates%2520and%250Asynthesizes%2520future%2520movements%2520based%2520on%2520the%2520previous%2520prediction%2520and%2520employs%2520a%250Acorrective%2520mechanism%2520for%2520subsequent%2520predictions.%2520Our%2520evolving%2520architecture%250Amerges%2520lightweight%252C%2520task-specific%2520modules%2520on%2520a%2520shared%2520backbone%252C%2520ensuring%2520both%250Aspecificity%2520and%2520scalability.%2520We%2520validate%2520our%2520model%2520through%2520experiments%2520on%250Areal-world%2520human%2520gait%2520datasets%252C%2520including%2520transtibial%2520amputees%252C%2520across%2520a%2520wide%250Arange%2520of%2520locomotion%2520tasks.%2520Results%2520demonstrate%2520that%2520our%2520approach%2520consistently%250Aoutperforms%2520baseline%2520models%252C%2520particularly%2520in%2520scenarios%2520with%2520distributional%250Ashifts%252C%2520adversarial%2520perturbations%252C%2520and%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01114v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20from%20Simulated%20Interactions%20via%20Multitask%20Prospective%0A%20%20Rehearsal%20for%20Bionic%20Limb%20Behavior%20Modeling&entry.906535625=Sharmita%20Dey%20and%20Benjamin%20Paassen%20and%20Sarath%20Ravindran%20Nair%20and%20Sabri%20Boughorbel%20and%20Arndt%20F.%20Schilling&entry.1292438233=%20%20Lower%20limb%20amputations%20and%20neuromuscular%20impairments%20severely%20restrict%0Amobility%2C%20necessitating%20advancements%20beyond%20conventional%20prosthetics.%20While%0Amotorized%20bionic%20limbs%20show%20promise%2C%20their%20effectiveness%20depends%20on%20replicating%0Athe%20dynamic%20coordination%20of%20human%20movement%20across%20diverse%20environments.%20In%20this%0Apaper%2C%20we%20introduce%20a%20model%20for%20human%20behavior%20in%20the%20context%20of%20bionic%0Aprosthesis%20control.%20Our%20approach%20leverages%20human%20locomotion%20demonstrations%20to%0Alearn%20the%20synergistic%20coupling%20of%20the%20lower%20limbs%2C%20enabling%20the%20prediction%20of%0Athe%20kinematic%20behavior%20of%20a%20missing%20limb%20during%20tasks%20such%20as%20walking%2C%20climbing%0Ainclines%2C%20and%20stairs.%20We%20propose%20a%20multitasking%2C%20continually%20adaptive%20model%0Athat%20anticipates%20and%20refines%20movements%20over%20time.%20At%20the%20core%20of%20our%20method%20is%0Aa%20technique%20called%20multitask%20prospective%20rehearsal%2C%20that%20anticipates%20and%0Asynthesizes%20future%20movements%20based%20on%20the%20previous%20prediction%20and%20employs%20a%0Acorrective%20mechanism%20for%20subsequent%20predictions.%20Our%20evolving%20architecture%0Amerges%20lightweight%2C%20task-specific%20modules%20on%20a%20shared%20backbone%2C%20ensuring%20both%0Aspecificity%20and%20scalability.%20We%20validate%20our%20model%20through%20experiments%20on%0Areal-world%20human%20gait%20datasets%2C%20including%20transtibial%20amputees%2C%20across%20a%20wide%0Arange%20of%20locomotion%20tasks.%20Results%20demonstrate%20that%20our%20approach%20consistently%0Aoutperforms%20baseline%20models%2C%20particularly%20in%20scenarios%20with%20distributional%0Ashifts%2C%20adversarial%20perturbations%2C%20and%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01114v3&entry.124074799=Read"},
{"title": "VLMaterial: Procedural Material Generation with Large Vision-Language\n  Models", "author": "Beichen Li and Rundi Wu and Armando Solar-Lezama and Changxi Zheng and Liang Shi and Bernd Bickel and Wojciech Matusik", "abstract": "  Procedural materials, represented as functional node graphs, are ubiquitous\nin computer graphics for photorealistic material appearance design. They allow\nusers to perform intuitive and precise editing to achieve desired visual\nappearances. However, creating a procedural material given an input image\nrequires professional knowledge and significant effort. In this work, we\nleverage the ability to convert procedural materials into standard Python\nprograms and fine-tune a large pre-trained vision-language model (VLM) to\ngenerate such programs from input images. To enable effective fine-tuning, we\nalso contribute an open-source procedural material dataset and propose to\nperform program-level augmentation by prompting another pre-trained large\nlanguage model (LLM). Through extensive evaluation, we show that our method\noutperforms previous methods on both synthetic and real-world examples.\n", "link": "http://arxiv.org/abs/2501.18623v2", "date": "2025-02-18", "relevancy": 2.315, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLMaterial%3A%20Procedural%20Material%20Generation%20with%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20VLMaterial%3A%20Procedural%20Material%20Generation%20with%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Beichen%20Li%20and%20Rundi%20Wu%20and%20Armando%20Solar-Lezama%20and%20Changxi%20Zheng%20and%20Liang%20Shi%20and%20Bernd%20Bickel%20and%20Wojciech%20Matusik%0AAbstract%3A%20%20%20Procedural%20materials%2C%20represented%20as%20functional%20node%20graphs%2C%20are%20ubiquitous%0Ain%20computer%20graphics%20for%20photorealistic%20material%20appearance%20design.%20They%20allow%0Ausers%20to%20perform%20intuitive%20and%20precise%20editing%20to%20achieve%20desired%20visual%0Aappearances.%20However%2C%20creating%20a%20procedural%20material%20given%20an%20input%20image%0Arequires%20professional%20knowledge%20and%20significant%20effort.%20In%20this%20work%2C%20we%0Aleverage%20the%20ability%20to%20convert%20procedural%20materials%20into%20standard%20Python%0Aprograms%20and%20fine-tune%20a%20large%20pre-trained%20vision-language%20model%20%28VLM%29%20to%0Agenerate%20such%20programs%20from%20input%20images.%20To%20enable%20effective%20fine-tuning%2C%20we%0Aalso%20contribute%20an%20open-source%20procedural%20material%20dataset%20and%20propose%20to%0Aperform%20program-level%20augmentation%20by%20prompting%20another%20pre-trained%20large%0Alanguage%20model%20%28LLM%29.%20Through%20extensive%20evaluation%2C%20we%20show%20that%20our%20method%0Aoutperforms%20previous%20methods%20on%20both%20synthetic%20and%20real-world%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18623v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLMaterial%253A%2520Procedural%2520Material%2520Generation%2520with%2520Large%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DBeichen%2520Li%2520and%2520Rundi%2520Wu%2520and%2520Armando%2520Solar-Lezama%2520and%2520Changxi%2520Zheng%2520and%2520Liang%2520Shi%2520and%2520Bernd%2520Bickel%2520and%2520Wojciech%2520Matusik%26entry.1292438233%3D%2520%2520Procedural%2520materials%252C%2520represented%2520as%2520functional%2520node%2520graphs%252C%2520are%2520ubiquitous%250Ain%2520computer%2520graphics%2520for%2520photorealistic%2520material%2520appearance%2520design.%2520They%2520allow%250Ausers%2520to%2520perform%2520intuitive%2520and%2520precise%2520editing%2520to%2520achieve%2520desired%2520visual%250Aappearances.%2520However%252C%2520creating%2520a%2520procedural%2520material%2520given%2520an%2520input%2520image%250Arequires%2520professional%2520knowledge%2520and%2520significant%2520effort.%2520In%2520this%2520work%252C%2520we%250Aleverage%2520the%2520ability%2520to%2520convert%2520procedural%2520materials%2520into%2520standard%2520Python%250Aprograms%2520and%2520fine-tune%2520a%2520large%2520pre-trained%2520vision-language%2520model%2520%2528VLM%2529%2520to%250Agenerate%2520such%2520programs%2520from%2520input%2520images.%2520To%2520enable%2520effective%2520fine-tuning%252C%2520we%250Aalso%2520contribute%2520an%2520open-source%2520procedural%2520material%2520dataset%2520and%2520propose%2520to%250Aperform%2520program-level%2520augmentation%2520by%2520prompting%2520another%2520pre-trained%2520large%250Alanguage%2520model%2520%2528LLM%2529.%2520Through%2520extensive%2520evaluation%252C%2520we%2520show%2520that%2520our%2520method%250Aoutperforms%2520previous%2520methods%2520on%2520both%2520synthetic%2520and%2520real-world%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18623v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLMaterial%3A%20Procedural%20Material%20Generation%20with%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Beichen%20Li%20and%20Rundi%20Wu%20and%20Armando%20Solar-Lezama%20and%20Changxi%20Zheng%20and%20Liang%20Shi%20and%20Bernd%20Bickel%20and%20Wojciech%20Matusik&entry.1292438233=%20%20Procedural%20materials%2C%20represented%20as%20functional%20node%20graphs%2C%20are%20ubiquitous%0Ain%20computer%20graphics%20for%20photorealistic%20material%20appearance%20design.%20They%20allow%0Ausers%20to%20perform%20intuitive%20and%20precise%20editing%20to%20achieve%20desired%20visual%0Aappearances.%20However%2C%20creating%20a%20procedural%20material%20given%20an%20input%20image%0Arequires%20professional%20knowledge%20and%20significant%20effort.%20In%20this%20work%2C%20we%0Aleverage%20the%20ability%20to%20convert%20procedural%20materials%20into%20standard%20Python%0Aprograms%20and%20fine-tune%20a%20large%20pre-trained%20vision-language%20model%20%28VLM%29%20to%0Agenerate%20such%20programs%20from%20input%20images.%20To%20enable%20effective%20fine-tuning%2C%20we%0Aalso%20contribute%20an%20open-source%20procedural%20material%20dataset%20and%20propose%20to%0Aperform%20program-level%20augmentation%20by%20prompting%20another%20pre-trained%20large%0Alanguage%20model%20%28LLM%29.%20Through%20extensive%20evaluation%2C%20we%20show%20that%20our%20method%0Aoutperforms%20previous%20methods%20on%20both%20synthetic%20and%20real-world%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18623v2&entry.124074799=Read"},
{"title": "RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye\n  View for 3D Object Detection", "author": "Jingtong Yue and Zhiwei Lin and Xin Lin and Xiaoyu Zhou and Xiangtai Li and Lu Qi and Yongtao Wang and Ming-Hsuan Yang", "abstract": "  While recent low-cost radar-camera approaches have shown promising results in\nmulti-modal 3D object detection, both sensors face challenges from\nenvironmental and intrinsic disturbances. Poor lighting or adverse weather\nconditions degrade camera performance, while radar suffers from noise and\npositional ambiguity. Achieving robust radar-camera 3D object detection\nrequires consistent performance across varying conditions, a topic that has not\nyet been fully explored. In this work, we first conduct a systematic analysis\nof robustness in radar-camera detection on five kinds of noises and propose\nRobuRCDet, a robust object detection model in BEV. Specifically, we design a 3D\nGaussian Expansion (3DGE) module to mitigate inaccuracies in radar points,\nincluding position, Radar Cross-Section (RCS), and velocity. The 3DGE uses RCS\nand velocity priors to generate a deformable kernel map and variance for kernel\nsize adjustment and value distribution. Additionally, we introduce a\nweather-adaptive fusion module, which adaptively fuses radar and camera\nfeatures based on camera signal confidence. Extensive experiments on the\npopular benchmark, nuScenes, show that our model achieves competitive results\nin regular and noisy conditions.\n", "link": "http://arxiv.org/abs/2502.13071v1", "date": "2025-02-18", "relevancy": 2.2997, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5762}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.576}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobuRCDet%3A%20Enhancing%20Robustness%20of%20Radar-Camera%20Fusion%20in%20Bird%27s%20Eye%0A%20%20View%20for%203D%20Object%20Detection&body=Title%3A%20RobuRCDet%3A%20Enhancing%20Robustness%20of%20Radar-Camera%20Fusion%20in%20Bird%27s%20Eye%0A%20%20View%20for%203D%20Object%20Detection%0AAuthor%3A%20Jingtong%20Yue%20and%20Zhiwei%20Lin%20and%20Xin%20Lin%20and%20Xiaoyu%20Zhou%20and%20Xiangtai%20Li%20and%20Lu%20Qi%20and%20Yongtao%20Wang%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20While%20recent%20low-cost%20radar-camera%20approaches%20have%20shown%20promising%20results%20in%0Amulti-modal%203D%20object%20detection%2C%20both%20sensors%20face%20challenges%20from%0Aenvironmental%20and%20intrinsic%20disturbances.%20Poor%20lighting%20or%20adverse%20weather%0Aconditions%20degrade%20camera%20performance%2C%20while%20radar%20suffers%20from%20noise%20and%0Apositional%20ambiguity.%20Achieving%20robust%20radar-camera%203D%20object%20detection%0Arequires%20consistent%20performance%20across%20varying%20conditions%2C%20a%20topic%20that%20has%20not%0Ayet%20been%20fully%20explored.%20In%20this%20work%2C%20we%20first%20conduct%20a%20systematic%20analysis%0Aof%20robustness%20in%20radar-camera%20detection%20on%20five%20kinds%20of%20noises%20and%20propose%0ARobuRCDet%2C%20a%20robust%20object%20detection%20model%20in%20BEV.%20Specifically%2C%20we%20design%20a%203D%0AGaussian%20Expansion%20%283DGE%29%20module%20to%20mitigate%20inaccuracies%20in%20radar%20points%2C%0Aincluding%20position%2C%20Radar%20Cross-Section%20%28RCS%29%2C%20and%20velocity.%20The%203DGE%20uses%20RCS%0Aand%20velocity%20priors%20to%20generate%20a%20deformable%20kernel%20map%20and%20variance%20for%20kernel%0Asize%20adjustment%20and%20value%20distribution.%20Additionally%2C%20we%20introduce%20a%0Aweather-adaptive%20fusion%20module%2C%20which%20adaptively%20fuses%20radar%20and%20camera%0Afeatures%20based%20on%20camera%20signal%20confidence.%20Extensive%20experiments%20on%20the%0Apopular%20benchmark%2C%20nuScenes%2C%20show%20that%20our%20model%20achieves%20competitive%20results%0Ain%20regular%20and%20noisy%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobuRCDet%253A%2520Enhancing%2520Robustness%2520of%2520Radar-Camera%2520Fusion%2520in%2520Bird%2527s%2520Eye%250A%2520%2520View%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DJingtong%2520Yue%2520and%2520Zhiwei%2520Lin%2520and%2520Xin%2520Lin%2520and%2520Xiaoyu%2520Zhou%2520and%2520Xiangtai%2520Li%2520and%2520Lu%2520Qi%2520and%2520Yongtao%2520Wang%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520While%2520recent%2520low-cost%2520radar-camera%2520approaches%2520have%2520shown%2520promising%2520results%2520in%250Amulti-modal%25203D%2520object%2520detection%252C%2520both%2520sensors%2520face%2520challenges%2520from%250Aenvironmental%2520and%2520intrinsic%2520disturbances.%2520Poor%2520lighting%2520or%2520adverse%2520weather%250Aconditions%2520degrade%2520camera%2520performance%252C%2520while%2520radar%2520suffers%2520from%2520noise%2520and%250Apositional%2520ambiguity.%2520Achieving%2520robust%2520radar-camera%25203D%2520object%2520detection%250Arequires%2520consistent%2520performance%2520across%2520varying%2520conditions%252C%2520a%2520topic%2520that%2520has%2520not%250Ayet%2520been%2520fully%2520explored.%2520In%2520this%2520work%252C%2520we%2520first%2520conduct%2520a%2520systematic%2520analysis%250Aof%2520robustness%2520in%2520radar-camera%2520detection%2520on%2520five%2520kinds%2520of%2520noises%2520and%2520propose%250ARobuRCDet%252C%2520a%2520robust%2520object%2520detection%2520model%2520in%2520BEV.%2520Specifically%252C%2520we%2520design%2520a%25203D%250AGaussian%2520Expansion%2520%25283DGE%2529%2520module%2520to%2520mitigate%2520inaccuracies%2520in%2520radar%2520points%252C%250Aincluding%2520position%252C%2520Radar%2520Cross-Section%2520%2528RCS%2529%252C%2520and%2520velocity.%2520The%25203DGE%2520uses%2520RCS%250Aand%2520velocity%2520priors%2520to%2520generate%2520a%2520deformable%2520kernel%2520map%2520and%2520variance%2520for%2520kernel%250Asize%2520adjustment%2520and%2520value%2520distribution.%2520Additionally%252C%2520we%2520introduce%2520a%250Aweather-adaptive%2520fusion%2520module%252C%2520which%2520adaptively%2520fuses%2520radar%2520and%2520camera%250Afeatures%2520based%2520on%2520camera%2520signal%2520confidence.%2520Extensive%2520experiments%2520on%2520the%250Apopular%2520benchmark%252C%2520nuScenes%252C%2520show%2520that%2520our%2520model%2520achieves%2520competitive%2520results%250Ain%2520regular%2520and%2520noisy%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobuRCDet%3A%20Enhancing%20Robustness%20of%20Radar-Camera%20Fusion%20in%20Bird%27s%20Eye%0A%20%20View%20for%203D%20Object%20Detection&entry.906535625=Jingtong%20Yue%20and%20Zhiwei%20Lin%20and%20Xin%20Lin%20and%20Xiaoyu%20Zhou%20and%20Xiangtai%20Li%20and%20Lu%20Qi%20and%20Yongtao%20Wang%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20While%20recent%20low-cost%20radar-camera%20approaches%20have%20shown%20promising%20results%20in%0Amulti-modal%203D%20object%20detection%2C%20both%20sensors%20face%20challenges%20from%0Aenvironmental%20and%20intrinsic%20disturbances.%20Poor%20lighting%20or%20adverse%20weather%0Aconditions%20degrade%20camera%20performance%2C%20while%20radar%20suffers%20from%20noise%20and%0Apositional%20ambiguity.%20Achieving%20robust%20radar-camera%203D%20object%20detection%0Arequires%20consistent%20performance%20across%20varying%20conditions%2C%20a%20topic%20that%20has%20not%0Ayet%20been%20fully%20explored.%20In%20this%20work%2C%20we%20first%20conduct%20a%20systematic%20analysis%0Aof%20robustness%20in%20radar-camera%20detection%20on%20five%20kinds%20of%20noises%20and%20propose%0ARobuRCDet%2C%20a%20robust%20object%20detection%20model%20in%20BEV.%20Specifically%2C%20we%20design%20a%203D%0AGaussian%20Expansion%20%283DGE%29%20module%20to%20mitigate%20inaccuracies%20in%20radar%20points%2C%0Aincluding%20position%2C%20Radar%20Cross-Section%20%28RCS%29%2C%20and%20velocity.%20The%203DGE%20uses%20RCS%0Aand%20velocity%20priors%20to%20generate%20a%20deformable%20kernel%20map%20and%20variance%20for%20kernel%0Asize%20adjustment%20and%20value%20distribution.%20Additionally%2C%20we%20introduce%20a%0Aweather-adaptive%20fusion%20module%2C%20which%20adaptively%20fuses%20radar%20and%20camera%0Afeatures%20based%20on%20camera%20signal%20confidence.%20Extensive%20experiments%20on%20the%0Apopular%20benchmark%2C%20nuScenes%2C%20show%20that%20our%20model%20achieves%20competitive%20results%0Ain%20regular%20and%20noisy%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13071v1&entry.124074799=Read"},
{"title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song\n  Generation", "author": "Zihan Liu and Shuangrui Ding and Zhixiong Zhang and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Dahua Lin and Jiaqi Wang", "abstract": "  Text-to-song generation, the task of creating vocals and accompaniment from\ntextual inputs, poses significant challenges due to domain complexity and data\nscarcity. Existing approaches often employ multi-stage generation procedures,\nresulting in cumbersome training and inference pipelines. In this paper, we\npropose SongGen, a fully open-source, single-stage auto-regressive transformer\ndesigned for controllable song generation. The proposed model facilitates\nfine-grained control over diverse musical attributes, including lyrics and\ntextual descriptions of instrumentation, genre, mood, and timbre, while also\noffering an optional three-second reference clip for voice cloning. Within a\nunified auto-regressive framework, SongGen supports two output modes: mixed\nmode, which generates a mixture of vocals and accompaniment directly, and\ndual-track mode, which synthesizes them separately for greater flexibility in\ndownstream applications. We explore diverse token pattern strategies for each\nmode, leading to notable improvements and valuable insights. Furthermore, we\ndesign an automated data preprocessing pipeline with effective quality control.\nTo foster community engagement and future research, we will release our model\nweights, training code, annotated data, and preprocessing pipeline. The\ngenerated samples are showcased on our project page at\nhttps://liuzh-19.github.io/SongGen/ , and the code will be available at\nhttps://github.com/LiuZH-19/SongGen .\n", "link": "http://arxiv.org/abs/2502.13128v1", "date": "2025-02-18", "relevancy": 2.2934, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5909}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5642}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SongGen%3A%20A%20Single%20Stage%20Auto-regressive%20Transformer%20for%20Text-to-Song%0A%20%20Generation&body=Title%3A%20SongGen%3A%20A%20Single%20Stage%20Auto-regressive%20Transformer%20for%20Text-to-Song%0A%20%20Generation%0AAuthor%3A%20Zihan%20Liu%20and%20Shuangrui%20Ding%20and%20Zhixiong%20Zhang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Text-to-song%20generation%2C%20the%20task%20of%20creating%20vocals%20and%20accompaniment%20from%0Atextual%20inputs%2C%20poses%20significant%20challenges%20due%20to%20domain%20complexity%20and%20data%0Ascarcity.%20Existing%20approaches%20often%20employ%20multi-stage%20generation%20procedures%2C%0Aresulting%20in%20cumbersome%20training%20and%20inference%20pipelines.%20In%20this%20paper%2C%20we%0Apropose%20SongGen%2C%20a%20fully%20open-source%2C%20single-stage%20auto-regressive%20transformer%0Adesigned%20for%20controllable%20song%20generation.%20The%20proposed%20model%20facilitates%0Afine-grained%20control%20over%20diverse%20musical%20attributes%2C%20including%20lyrics%20and%0Atextual%20descriptions%20of%20instrumentation%2C%20genre%2C%20mood%2C%20and%20timbre%2C%20while%20also%0Aoffering%20an%20optional%20three-second%20reference%20clip%20for%20voice%20cloning.%20Within%20a%0Aunified%20auto-regressive%20framework%2C%20SongGen%20supports%20two%20output%20modes%3A%20mixed%0Amode%2C%20which%20generates%20a%20mixture%20of%20vocals%20and%20accompaniment%20directly%2C%20and%0Adual-track%20mode%2C%20which%20synthesizes%20them%20separately%20for%20greater%20flexibility%20in%0Adownstream%20applications.%20We%20explore%20diverse%20token%20pattern%20strategies%20for%20each%0Amode%2C%20leading%20to%20notable%20improvements%20and%20valuable%20insights.%20Furthermore%2C%20we%0Adesign%20an%20automated%20data%20preprocessing%20pipeline%20with%20effective%20quality%20control.%0ATo%20foster%20community%20engagement%20and%20future%20research%2C%20we%20will%20release%20our%20model%0Aweights%2C%20training%20code%2C%20annotated%20data%2C%20and%20preprocessing%20pipeline.%20The%0Agenerated%20samples%20are%20showcased%20on%20our%20project%20page%20at%0Ahttps%3A//liuzh-19.github.io/SongGen/%20%2C%20and%20the%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/LiuZH-19/SongGen%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSongGen%253A%2520A%2520Single%2520Stage%2520Auto-regressive%2520Transformer%2520for%2520Text-to-Song%250A%2520%2520Generation%26entry.906535625%3DZihan%2520Liu%2520and%2520Shuangrui%2520Ding%2520and%2520Zhixiong%2520Zhang%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Text-to-song%2520generation%252C%2520the%2520task%2520of%2520creating%2520vocals%2520and%2520accompaniment%2520from%250Atextual%2520inputs%252C%2520poses%2520significant%2520challenges%2520due%2520to%2520domain%2520complexity%2520and%2520data%250Ascarcity.%2520Existing%2520approaches%2520often%2520employ%2520multi-stage%2520generation%2520procedures%252C%250Aresulting%2520in%2520cumbersome%2520training%2520and%2520inference%2520pipelines.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520SongGen%252C%2520a%2520fully%2520open-source%252C%2520single-stage%2520auto-regressive%2520transformer%250Adesigned%2520for%2520controllable%2520song%2520generation.%2520The%2520proposed%2520model%2520facilitates%250Afine-grained%2520control%2520over%2520diverse%2520musical%2520attributes%252C%2520including%2520lyrics%2520and%250Atextual%2520descriptions%2520of%2520instrumentation%252C%2520genre%252C%2520mood%252C%2520and%2520timbre%252C%2520while%2520also%250Aoffering%2520an%2520optional%2520three-second%2520reference%2520clip%2520for%2520voice%2520cloning.%2520Within%2520a%250Aunified%2520auto-regressive%2520framework%252C%2520SongGen%2520supports%2520two%2520output%2520modes%253A%2520mixed%250Amode%252C%2520which%2520generates%2520a%2520mixture%2520of%2520vocals%2520and%2520accompaniment%2520directly%252C%2520and%250Adual-track%2520mode%252C%2520which%2520synthesizes%2520them%2520separately%2520for%2520greater%2520flexibility%2520in%250Adownstream%2520applications.%2520We%2520explore%2520diverse%2520token%2520pattern%2520strategies%2520for%2520each%250Amode%252C%2520leading%2520to%2520notable%2520improvements%2520and%2520valuable%2520insights.%2520Furthermore%252C%2520we%250Adesign%2520an%2520automated%2520data%2520preprocessing%2520pipeline%2520with%2520effective%2520quality%2520control.%250ATo%2520foster%2520community%2520engagement%2520and%2520future%2520research%252C%2520we%2520will%2520release%2520our%2520model%250Aweights%252C%2520training%2520code%252C%2520annotated%2520data%252C%2520and%2520preprocessing%2520pipeline.%2520The%250Agenerated%2520samples%2520are%2520showcased%2520on%2520our%2520project%2520page%2520at%250Ahttps%253A//liuzh-19.github.io/SongGen/%2520%252C%2520and%2520the%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/LiuZH-19/SongGen%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SongGen%3A%20A%20Single%20Stage%20Auto-regressive%20Transformer%20for%20Text-to-Song%0A%20%20Generation&entry.906535625=Zihan%20Liu%20and%20Shuangrui%20Ding%20and%20Zhixiong%20Zhang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Text-to-song%20generation%2C%20the%20task%20of%20creating%20vocals%20and%20accompaniment%20from%0Atextual%20inputs%2C%20poses%20significant%20challenges%20due%20to%20domain%20complexity%20and%20data%0Ascarcity.%20Existing%20approaches%20often%20employ%20multi-stage%20generation%20procedures%2C%0Aresulting%20in%20cumbersome%20training%20and%20inference%20pipelines.%20In%20this%20paper%2C%20we%0Apropose%20SongGen%2C%20a%20fully%20open-source%2C%20single-stage%20auto-regressive%20transformer%0Adesigned%20for%20controllable%20song%20generation.%20The%20proposed%20model%20facilitates%0Afine-grained%20control%20over%20diverse%20musical%20attributes%2C%20including%20lyrics%20and%0Atextual%20descriptions%20of%20instrumentation%2C%20genre%2C%20mood%2C%20and%20timbre%2C%20while%20also%0Aoffering%20an%20optional%20three-second%20reference%20clip%20for%20voice%20cloning.%20Within%20a%0Aunified%20auto-regressive%20framework%2C%20SongGen%20supports%20two%20output%20modes%3A%20mixed%0Amode%2C%20which%20generates%20a%20mixture%20of%20vocals%20and%20accompaniment%20directly%2C%20and%0Adual-track%20mode%2C%20which%20synthesizes%20them%20separately%20for%20greater%20flexibility%20in%0Adownstream%20applications.%20We%20explore%20diverse%20token%20pattern%20strategies%20for%20each%0Amode%2C%20leading%20to%20notable%20improvements%20and%20valuable%20insights.%20Furthermore%2C%20we%0Adesign%20an%20automated%20data%20preprocessing%20pipeline%20with%20effective%20quality%20control.%0ATo%20foster%20community%20engagement%20and%20future%20research%2C%20we%20will%20release%20our%20model%0Aweights%2C%20training%20code%2C%20annotated%20data%2C%20and%20preprocessing%20pipeline.%20The%0Agenerated%20samples%20are%20showcased%20on%20our%20project%20page%20at%0Ahttps%3A//liuzh-19.github.io/SongGen/%20%2C%20and%20the%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/LiuZH-19/SongGen%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13128v1&entry.124074799=Read"},
{"title": "Performance Evaluation of Large Language Models in Statistical\n  Programming", "author": "Xinyi Song and Kexin Xie and Lina Lee and Ruizhe Chen and Jared M. Clark and Hao He and Haoran He and Jie Min and Xinlei Zhang and Simin Zheng and Zhiyang Zhang and Xinwei Deng and Yili Hong", "abstract": "  The programming capabilities of large language models (LLMs) have\nrevolutionized automatic code generation and opened new avenues for automatic\nstatistical analysis. However, the validity and quality of these generated\ncodes need to be systematically evaluated before they can be widely adopted.\nDespite their growing prominence, a comprehensive evaluation of statistical\ncode generated by LLMs remains scarce in the literature. In this paper, we\nassess the performance of LLMs, including two versions of ChatGPT and one\nversion of Llama, in the domain of SAS programming for statistical analysis.\nOur study utilizes a set of statistical analysis tasks encompassing diverse\nstatistical topics and datasets. Each task includes a problem description,\ndataset information, and human-verified SAS code. We conduct a comprehensive\nassessment of the quality of SAS code generated by LLMs through human expert\nevaluation based on correctness, effectiveness, readability, executability, and\nthe accuracy of output results. The analysis of rating scores reveals that\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\nthey struggle with tasks requiring deep domain understanding and may produce\nredundant or incorrect results. This study offers valuable insights into the\ncapabilities and limitations of LLMs in statistical programming, providing\nguidance for future advancements in AI-assisted coding systems for statistical\nanalysis.\n", "link": "http://arxiv.org/abs/2502.13117v1", "date": "2025-02-18", "relevancy": 2.2892, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20Evaluation%20of%20Large%20Language%20Models%20in%20Statistical%0A%20%20Programming&body=Title%3A%20Performance%20Evaluation%20of%20Large%20Language%20Models%20in%20Statistical%0A%20%20Programming%0AAuthor%3A%20Xinyi%20Song%20and%20Kexin%20Xie%20and%20Lina%20Lee%20and%20Ruizhe%20Chen%20and%20Jared%20M.%20Clark%20and%20Hao%20He%20and%20Haoran%20He%20and%20Jie%20Min%20and%20Xinlei%20Zhang%20and%20Simin%20Zheng%20and%20Zhiyang%20Zhang%20and%20Xinwei%20Deng%20and%20Yili%20Hong%0AAbstract%3A%20%20%20The%20programming%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%0Arevolutionized%20automatic%20code%20generation%20and%20opened%20new%20avenues%20for%20automatic%0Astatistical%20analysis.%20However%2C%20the%20validity%20and%20quality%20of%20these%20generated%0Acodes%20need%20to%20be%20systematically%20evaluated%20before%20they%20can%20be%20widely%20adopted.%0ADespite%20their%20growing%20prominence%2C%20a%20comprehensive%20evaluation%20of%20statistical%0Acode%20generated%20by%20LLMs%20remains%20scarce%20in%20the%20literature.%20In%20this%20paper%2C%20we%0Aassess%20the%20performance%20of%20LLMs%2C%20including%20two%20versions%20of%20ChatGPT%20and%20one%0Aversion%20of%20Llama%2C%20in%20the%20domain%20of%20SAS%20programming%20for%20statistical%20analysis.%0AOur%20study%20utilizes%20a%20set%20of%20statistical%20analysis%20tasks%20encompassing%20diverse%0Astatistical%20topics%20and%20datasets.%20Each%20task%20includes%20a%20problem%20description%2C%0Adataset%20information%2C%20and%20human-verified%20SAS%20code.%20We%20conduct%20a%20comprehensive%0Aassessment%20of%20the%20quality%20of%20SAS%20code%20generated%20by%20LLMs%20through%20human%20expert%0Aevaluation%20based%20on%20correctness%2C%20effectiveness%2C%20readability%2C%20executability%2C%20and%0Athe%20accuracy%20of%20output%20results.%20The%20analysis%20of%20rating%20scores%20reveals%20that%0Awhile%20LLMs%20demonstrate%20usefulness%20in%20generating%20syntactically%20correct%20code%2C%0Athey%20struggle%20with%20tasks%20requiring%20deep%20domain%20understanding%20and%20may%20produce%0Aredundant%20or%20incorrect%20results.%20This%20study%20offers%20valuable%20insights%20into%20the%0Acapabilities%20and%20limitations%20of%20LLMs%20in%20statistical%20programming%2C%20providing%0Aguidance%20for%20future%20advancements%20in%20AI-assisted%20coding%20systems%20for%20statistical%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520in%2520Statistical%250A%2520%2520Programming%26entry.906535625%3DXinyi%2520Song%2520and%2520Kexin%2520Xie%2520and%2520Lina%2520Lee%2520and%2520Ruizhe%2520Chen%2520and%2520Jared%2520M.%2520Clark%2520and%2520Hao%2520He%2520and%2520Haoran%2520He%2520and%2520Jie%2520Min%2520and%2520Xinlei%2520Zhang%2520and%2520Simin%2520Zheng%2520and%2520Zhiyang%2520Zhang%2520and%2520Xinwei%2520Deng%2520and%2520Yili%2520Hong%26entry.1292438233%3D%2520%2520The%2520programming%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Arevolutionized%2520automatic%2520code%2520generation%2520and%2520opened%2520new%2520avenues%2520for%2520automatic%250Astatistical%2520analysis.%2520However%252C%2520the%2520validity%2520and%2520quality%2520of%2520these%2520generated%250Acodes%2520need%2520to%2520be%2520systematically%2520evaluated%2520before%2520they%2520can%2520be%2520widely%2520adopted.%250ADespite%2520their%2520growing%2520prominence%252C%2520a%2520comprehensive%2520evaluation%2520of%2520statistical%250Acode%2520generated%2520by%2520LLMs%2520remains%2520scarce%2520in%2520the%2520literature.%2520In%2520this%2520paper%252C%2520we%250Aassess%2520the%2520performance%2520of%2520LLMs%252C%2520including%2520two%2520versions%2520of%2520ChatGPT%2520and%2520one%250Aversion%2520of%2520Llama%252C%2520in%2520the%2520domain%2520of%2520SAS%2520programming%2520for%2520statistical%2520analysis.%250AOur%2520study%2520utilizes%2520a%2520set%2520of%2520statistical%2520analysis%2520tasks%2520encompassing%2520diverse%250Astatistical%2520topics%2520and%2520datasets.%2520Each%2520task%2520includes%2520a%2520problem%2520description%252C%250Adataset%2520information%252C%2520and%2520human-verified%2520SAS%2520code.%2520We%2520conduct%2520a%2520comprehensive%250Aassessment%2520of%2520the%2520quality%2520of%2520SAS%2520code%2520generated%2520by%2520LLMs%2520through%2520human%2520expert%250Aevaluation%2520based%2520on%2520correctness%252C%2520effectiveness%252C%2520readability%252C%2520executability%252C%2520and%250Athe%2520accuracy%2520of%2520output%2520results.%2520The%2520analysis%2520of%2520rating%2520scores%2520reveals%2520that%250Awhile%2520LLMs%2520demonstrate%2520usefulness%2520in%2520generating%2520syntactically%2520correct%2520code%252C%250Athey%2520struggle%2520with%2520tasks%2520requiring%2520deep%2520domain%2520understanding%2520and%2520may%2520produce%250Aredundant%2520or%2520incorrect%2520results.%2520This%2520study%2520offers%2520valuable%2520insights%2520into%2520the%250Acapabilities%2520and%2520limitations%2520of%2520LLMs%2520in%2520statistical%2520programming%252C%2520providing%250Aguidance%2520for%2520future%2520advancements%2520in%2520AI-assisted%2520coding%2520systems%2520for%2520statistical%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20Evaluation%20of%20Large%20Language%20Models%20in%20Statistical%0A%20%20Programming&entry.906535625=Xinyi%20Song%20and%20Kexin%20Xie%20and%20Lina%20Lee%20and%20Ruizhe%20Chen%20and%20Jared%20M.%20Clark%20and%20Hao%20He%20and%20Haoran%20He%20and%20Jie%20Min%20and%20Xinlei%20Zhang%20and%20Simin%20Zheng%20and%20Zhiyang%20Zhang%20and%20Xinwei%20Deng%20and%20Yili%20Hong&entry.1292438233=%20%20The%20programming%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%0Arevolutionized%20automatic%20code%20generation%20and%20opened%20new%20avenues%20for%20automatic%0Astatistical%20analysis.%20However%2C%20the%20validity%20and%20quality%20of%20these%20generated%0Acodes%20need%20to%20be%20systematically%20evaluated%20before%20they%20can%20be%20widely%20adopted.%0ADespite%20their%20growing%20prominence%2C%20a%20comprehensive%20evaluation%20of%20statistical%0Acode%20generated%20by%20LLMs%20remains%20scarce%20in%20the%20literature.%20In%20this%20paper%2C%20we%0Aassess%20the%20performance%20of%20LLMs%2C%20including%20two%20versions%20of%20ChatGPT%20and%20one%0Aversion%20of%20Llama%2C%20in%20the%20domain%20of%20SAS%20programming%20for%20statistical%20analysis.%0AOur%20study%20utilizes%20a%20set%20of%20statistical%20analysis%20tasks%20encompassing%20diverse%0Astatistical%20topics%20and%20datasets.%20Each%20task%20includes%20a%20problem%20description%2C%0Adataset%20information%2C%20and%20human-verified%20SAS%20code.%20We%20conduct%20a%20comprehensive%0Aassessment%20of%20the%20quality%20of%20SAS%20code%20generated%20by%20LLMs%20through%20human%20expert%0Aevaluation%20based%20on%20correctness%2C%20effectiveness%2C%20readability%2C%20executability%2C%20and%0Athe%20accuracy%20of%20output%20results.%20The%20analysis%20of%20rating%20scores%20reveals%20that%0Awhile%20LLMs%20demonstrate%20usefulness%20in%20generating%20syntactically%20correct%20code%2C%0Athey%20struggle%20with%20tasks%20requiring%20deep%20domain%20understanding%20and%20may%20produce%0Aredundant%20or%20incorrect%20results.%20This%20study%20offers%20valuable%20insights%20into%20the%0Acapabilities%20and%20limitations%20of%20LLMs%20in%20statistical%20programming%2C%20providing%0Aguidance%20for%20future%20advancements%20in%20AI-assisted%20coding%20systems%20for%20statistical%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13117v1&entry.124074799=Read"},
{"title": "Statistically Significant $k$NNAD by Selective Inference", "author": "Mizuki Niihori and Teruyuki Katsuoka and Tomohiro Shiraishi and Shuichi Nishino and Ichiro Takeuchi", "abstract": "  In this paper, we investigate the problem of unsupervised anomaly detection\nusing the k-Nearest Neighbor method. The k-Nearest Neighbor Anomaly Detection\n(kNNAD) is a simple yet effective approach for identifying anomalies across\nvarious domains and fields. A critical challenge in anomaly detection,\nincluding kNNAD, is appropriately quantifying the reliability of detected\nanomalies. To address this, we formulate kNNAD as a statistical hypothesis test\nand quantify the probability of false detection using $p$-values. The main\ntechnical challenge lies in performing both anomaly detection and statistical\ntesting on the same data, which hinders correct $p$-value calculation within\nthe conventional statistical testing framework. To resolve this issue, we\nintroduce a statistical hypothesis testing framework called Selective Inference\n(SI) and propose a method named Statistically Significant NNAD (Stat-kNNAD). By\nleveraging SI, the Stat-kNNAD method ensures that detected anomalies are\nstatistically significant with theoretical guarantees. The proposed Stat-kNNAD\nmethod is applicable to anomaly detection in both the original feature space\nand latent feature spaces derived from deep learning models. Through numerical\nexperiments on synthetic data and applications to industrial product anomaly\ndetection, we demonstrate the validity and effectiveness of the Stat-kNNAD\nmethod.\n", "link": "http://arxiv.org/abs/2502.12978v1", "date": "2025-02-18", "relevancy": 2.2823, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4865}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4477}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistically%20Significant%20%24k%24NNAD%20by%20Selective%20Inference&body=Title%3A%20Statistically%20Significant%20%24k%24NNAD%20by%20Selective%20Inference%0AAuthor%3A%20Mizuki%20Niihori%20and%20Teruyuki%20Katsuoka%20and%20Tomohiro%20Shiraishi%20and%20Shuichi%20Nishino%20and%20Ichiro%20Takeuchi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20problem%20of%20unsupervised%20anomaly%20detection%0Ausing%20the%20k-Nearest%20Neighbor%20method.%20The%20k-Nearest%20Neighbor%20Anomaly%20Detection%0A%28kNNAD%29%20is%20a%20simple%20yet%20effective%20approach%20for%20identifying%20anomalies%20across%0Avarious%20domains%20and%20fields.%20A%20critical%20challenge%20in%20anomaly%20detection%2C%0Aincluding%20kNNAD%2C%20is%20appropriately%20quantifying%20the%20reliability%20of%20detected%0Aanomalies.%20To%20address%20this%2C%20we%20formulate%20kNNAD%20as%20a%20statistical%20hypothesis%20test%0Aand%20quantify%20the%20probability%20of%20false%20detection%20using%20%24p%24-values.%20The%20main%0Atechnical%20challenge%20lies%20in%20performing%20both%20anomaly%20detection%20and%20statistical%0Atesting%20on%20the%20same%20data%2C%20which%20hinders%20correct%20%24p%24-value%20calculation%20within%0Athe%20conventional%20statistical%20testing%20framework.%20To%20resolve%20this%20issue%2C%20we%0Aintroduce%20a%20statistical%20hypothesis%20testing%20framework%20called%20Selective%20Inference%0A%28SI%29%20and%20propose%20a%20method%20named%20Statistically%20Significant%20NNAD%20%28Stat-kNNAD%29.%20By%0Aleveraging%20SI%2C%20the%20Stat-kNNAD%20method%20ensures%20that%20detected%20anomalies%20are%0Astatistically%20significant%20with%20theoretical%20guarantees.%20The%20proposed%20Stat-kNNAD%0Amethod%20is%20applicable%20to%20anomaly%20detection%20in%20both%20the%20original%20feature%20space%0Aand%20latent%20feature%20spaces%20derived%20from%20deep%20learning%20models.%20Through%20numerical%0Aexperiments%20on%20synthetic%20data%20and%20applications%20to%20industrial%20product%20anomaly%0Adetection%2C%20we%20demonstrate%20the%20validity%20and%20effectiveness%20of%20the%20Stat-kNNAD%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistically%2520Significant%2520%2524k%2524NNAD%2520by%2520Selective%2520Inference%26entry.906535625%3DMizuki%2520Niihori%2520and%2520Teruyuki%2520Katsuoka%2520and%2520Tomohiro%2520Shiraishi%2520and%2520Shuichi%2520Nishino%2520and%2520Ichiro%2520Takeuchi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520problem%2520of%2520unsupervised%2520anomaly%2520detection%250Ausing%2520the%2520k-Nearest%2520Neighbor%2520method.%2520The%2520k-Nearest%2520Neighbor%2520Anomaly%2520Detection%250A%2528kNNAD%2529%2520is%2520a%2520simple%2520yet%2520effective%2520approach%2520for%2520identifying%2520anomalies%2520across%250Avarious%2520domains%2520and%2520fields.%2520A%2520critical%2520challenge%2520in%2520anomaly%2520detection%252C%250Aincluding%2520kNNAD%252C%2520is%2520appropriately%2520quantifying%2520the%2520reliability%2520of%2520detected%250Aanomalies.%2520To%2520address%2520this%252C%2520we%2520formulate%2520kNNAD%2520as%2520a%2520statistical%2520hypothesis%2520test%250Aand%2520quantify%2520the%2520probability%2520of%2520false%2520detection%2520using%2520%2524p%2524-values.%2520The%2520main%250Atechnical%2520challenge%2520lies%2520in%2520performing%2520both%2520anomaly%2520detection%2520and%2520statistical%250Atesting%2520on%2520the%2520same%2520data%252C%2520which%2520hinders%2520correct%2520%2524p%2524-value%2520calculation%2520within%250Athe%2520conventional%2520statistical%2520testing%2520framework.%2520To%2520resolve%2520this%2520issue%252C%2520we%250Aintroduce%2520a%2520statistical%2520hypothesis%2520testing%2520framework%2520called%2520Selective%2520Inference%250A%2528SI%2529%2520and%2520propose%2520a%2520method%2520named%2520Statistically%2520Significant%2520NNAD%2520%2528Stat-kNNAD%2529.%2520By%250Aleveraging%2520SI%252C%2520the%2520Stat-kNNAD%2520method%2520ensures%2520that%2520detected%2520anomalies%2520are%250Astatistically%2520significant%2520with%2520theoretical%2520guarantees.%2520The%2520proposed%2520Stat-kNNAD%250Amethod%2520is%2520applicable%2520to%2520anomaly%2520detection%2520in%2520both%2520the%2520original%2520feature%2520space%250Aand%2520latent%2520feature%2520spaces%2520derived%2520from%2520deep%2520learning%2520models.%2520Through%2520numerical%250Aexperiments%2520on%2520synthetic%2520data%2520and%2520applications%2520to%2520industrial%2520product%2520anomaly%250Adetection%252C%2520we%2520demonstrate%2520the%2520validity%2520and%2520effectiveness%2520of%2520the%2520Stat-kNNAD%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistically%20Significant%20%24k%24NNAD%20by%20Selective%20Inference&entry.906535625=Mizuki%20Niihori%20and%20Teruyuki%20Katsuoka%20and%20Tomohiro%20Shiraishi%20and%20Shuichi%20Nishino%20and%20Ichiro%20Takeuchi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20problem%20of%20unsupervised%20anomaly%20detection%0Ausing%20the%20k-Nearest%20Neighbor%20method.%20The%20k-Nearest%20Neighbor%20Anomaly%20Detection%0A%28kNNAD%29%20is%20a%20simple%20yet%20effective%20approach%20for%20identifying%20anomalies%20across%0Avarious%20domains%20and%20fields.%20A%20critical%20challenge%20in%20anomaly%20detection%2C%0Aincluding%20kNNAD%2C%20is%20appropriately%20quantifying%20the%20reliability%20of%20detected%0Aanomalies.%20To%20address%20this%2C%20we%20formulate%20kNNAD%20as%20a%20statistical%20hypothesis%20test%0Aand%20quantify%20the%20probability%20of%20false%20detection%20using%20%24p%24-values.%20The%20main%0Atechnical%20challenge%20lies%20in%20performing%20both%20anomaly%20detection%20and%20statistical%0Atesting%20on%20the%20same%20data%2C%20which%20hinders%20correct%20%24p%24-value%20calculation%20within%0Athe%20conventional%20statistical%20testing%20framework.%20To%20resolve%20this%20issue%2C%20we%0Aintroduce%20a%20statistical%20hypothesis%20testing%20framework%20called%20Selective%20Inference%0A%28SI%29%20and%20propose%20a%20method%20named%20Statistically%20Significant%20NNAD%20%28Stat-kNNAD%29.%20By%0Aleveraging%20SI%2C%20the%20Stat-kNNAD%20method%20ensures%20that%20detected%20anomalies%20are%0Astatistically%20significant%20with%20theoretical%20guarantees.%20The%20proposed%20Stat-kNNAD%0Amethod%20is%20applicable%20to%20anomaly%20detection%20in%20both%20the%20original%20feature%20space%0Aand%20latent%20feature%20spaces%20derived%20from%20deep%20learning%20models.%20Through%20numerical%0Aexperiments%20on%20synthetic%20data%20and%20applications%20to%20industrial%20product%20anomaly%0Adetection%2C%20we%20demonstrate%20the%20validity%20and%20effectiveness%20of%20the%20Stat-kNNAD%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12978v1&entry.124074799=Read"},
{"title": "Towards Quantum Tensor Decomposition in Biomedical Applications", "author": "Myson Burch and Jiasen Zhang and Gideon Idumah and Hakan Doga and Richard Lartey and Lamis Yehia and Mingrui Yang and Murat Yildirim and Mihriban Karaayvaz and Omar Shehab and Weihong Guo and Ying Ni and Laxmi Parida and Xiaojuan Li and Aritra Bose", "abstract": "  Tensor decomposition has emerged as a powerful framework for feature\nextraction in multi-modal biomedical data. In this review, we present a\ncomprehensive analysis of tensor decomposition methods such as Tucker,\nCANDECOMP/PARAFAC, spiked tensor decomposition, etc. and their diverse\napplications across biomedical domains such as imaging, multi-omics, and\nspatial transcriptomics. To systematically investigate the literature, we\napplied a topic modeling-based approach that identifies and groups distinct\nthematic sub-areas in biomedicine where tensor decomposition has been used,\nthereby revealing key trends and research directions. We evaluated challenges\nrelated to the scalability of latent spaces along with obtaining the optimal\nrank of the tensor, which often hinder the extraction of meaningful features\nfrom increasingly large and complex datasets. Additionally, we discuss recent\nadvances in quantum algorithms for tensor decomposition, exploring how quantum\ncomputing can be leveraged to address these challenges. Our study includes a\npreliminary resource estimation analysis for quantum computing platforms and\nexamines the feasibility of implementing quantum-enhanced tensor decomposition\nmethods on near-term quantum devices. Collectively, this review not only\nsynthesizes current applications and challenges of tensor decomposition in\nbiomedical analyses but also outlines promising quantum computing strategies to\nenhance its impact on deriving actionable insights from complex biomedical\ndata.\n", "link": "http://arxiv.org/abs/2502.13140v1", "date": "2025-02-18", "relevancy": 2.2638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Quantum%20Tensor%20Decomposition%20in%20Biomedical%20Applications&body=Title%3A%20Towards%20Quantum%20Tensor%20Decomposition%20in%20Biomedical%20Applications%0AAuthor%3A%20Myson%20Burch%20and%20Jiasen%20Zhang%20and%20Gideon%20Idumah%20and%20Hakan%20Doga%20and%20Richard%20Lartey%20and%20Lamis%20Yehia%20and%20Mingrui%20Yang%20and%20Murat%20Yildirim%20and%20Mihriban%20Karaayvaz%20and%20Omar%20Shehab%20and%20Weihong%20Guo%20and%20Ying%20Ni%20and%20Laxmi%20Parida%20and%20Xiaojuan%20Li%20and%20Aritra%20Bose%0AAbstract%3A%20%20%20Tensor%20decomposition%20has%20emerged%20as%20a%20powerful%20framework%20for%20feature%0Aextraction%20in%20multi-modal%20biomedical%20data.%20In%20this%20review%2C%20we%20present%20a%0Acomprehensive%20analysis%20of%20tensor%20decomposition%20methods%20such%20as%20Tucker%2C%0ACANDECOMP/PARAFAC%2C%20spiked%20tensor%20decomposition%2C%20etc.%20and%20their%20diverse%0Aapplications%20across%20biomedical%20domains%20such%20as%20imaging%2C%20multi-omics%2C%20and%0Aspatial%20transcriptomics.%20To%20systematically%20investigate%20the%20literature%2C%20we%0Aapplied%20a%20topic%20modeling-based%20approach%20that%20identifies%20and%20groups%20distinct%0Athematic%20sub-areas%20in%20biomedicine%20where%20tensor%20decomposition%20has%20been%20used%2C%0Athereby%20revealing%20key%20trends%20and%20research%20directions.%20We%20evaluated%20challenges%0Arelated%20to%20the%20scalability%20of%20latent%20spaces%20along%20with%20obtaining%20the%20optimal%0Arank%20of%20the%20tensor%2C%20which%20often%20hinder%20the%20extraction%20of%20meaningful%20features%0Afrom%20increasingly%20large%20and%20complex%20datasets.%20Additionally%2C%20we%20discuss%20recent%0Aadvances%20in%20quantum%20algorithms%20for%20tensor%20decomposition%2C%20exploring%20how%20quantum%0Acomputing%20can%20be%20leveraged%20to%20address%20these%20challenges.%20Our%20study%20includes%20a%0Apreliminary%20resource%20estimation%20analysis%20for%20quantum%20computing%20platforms%20and%0Aexamines%20the%20feasibility%20of%20implementing%20quantum-enhanced%20tensor%20decomposition%0Amethods%20on%20near-term%20quantum%20devices.%20Collectively%2C%20this%20review%20not%20only%0Asynthesizes%20current%20applications%20and%20challenges%20of%20tensor%20decomposition%20in%0Abiomedical%20analyses%20but%20also%20outlines%20promising%20quantum%20computing%20strategies%20to%0Aenhance%20its%20impact%20on%20deriving%20actionable%20insights%20from%20complex%20biomedical%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Quantum%2520Tensor%2520Decomposition%2520in%2520Biomedical%2520Applications%26entry.906535625%3DMyson%2520Burch%2520and%2520Jiasen%2520Zhang%2520and%2520Gideon%2520Idumah%2520and%2520Hakan%2520Doga%2520and%2520Richard%2520Lartey%2520and%2520Lamis%2520Yehia%2520and%2520Mingrui%2520Yang%2520and%2520Murat%2520Yildirim%2520and%2520Mihriban%2520Karaayvaz%2520and%2520Omar%2520Shehab%2520and%2520Weihong%2520Guo%2520and%2520Ying%2520Ni%2520and%2520Laxmi%2520Parida%2520and%2520Xiaojuan%2520Li%2520and%2520Aritra%2520Bose%26entry.1292438233%3D%2520%2520Tensor%2520decomposition%2520has%2520emerged%2520as%2520a%2520powerful%2520framework%2520for%2520feature%250Aextraction%2520in%2520multi-modal%2520biomedical%2520data.%2520In%2520this%2520review%252C%2520we%2520present%2520a%250Acomprehensive%2520analysis%2520of%2520tensor%2520decomposition%2520methods%2520such%2520as%2520Tucker%252C%250ACANDECOMP/PARAFAC%252C%2520spiked%2520tensor%2520decomposition%252C%2520etc.%2520and%2520their%2520diverse%250Aapplications%2520across%2520biomedical%2520domains%2520such%2520as%2520imaging%252C%2520multi-omics%252C%2520and%250Aspatial%2520transcriptomics.%2520To%2520systematically%2520investigate%2520the%2520literature%252C%2520we%250Aapplied%2520a%2520topic%2520modeling-based%2520approach%2520that%2520identifies%2520and%2520groups%2520distinct%250Athematic%2520sub-areas%2520in%2520biomedicine%2520where%2520tensor%2520decomposition%2520has%2520been%2520used%252C%250Athereby%2520revealing%2520key%2520trends%2520and%2520research%2520directions.%2520We%2520evaluated%2520challenges%250Arelated%2520to%2520the%2520scalability%2520of%2520latent%2520spaces%2520along%2520with%2520obtaining%2520the%2520optimal%250Arank%2520of%2520the%2520tensor%252C%2520which%2520often%2520hinder%2520the%2520extraction%2520of%2520meaningful%2520features%250Afrom%2520increasingly%2520large%2520and%2520complex%2520datasets.%2520Additionally%252C%2520we%2520discuss%2520recent%250Aadvances%2520in%2520quantum%2520algorithms%2520for%2520tensor%2520decomposition%252C%2520exploring%2520how%2520quantum%250Acomputing%2520can%2520be%2520leveraged%2520to%2520address%2520these%2520challenges.%2520Our%2520study%2520includes%2520a%250Apreliminary%2520resource%2520estimation%2520analysis%2520for%2520quantum%2520computing%2520platforms%2520and%250Aexamines%2520the%2520feasibility%2520of%2520implementing%2520quantum-enhanced%2520tensor%2520decomposition%250Amethods%2520on%2520near-term%2520quantum%2520devices.%2520Collectively%252C%2520this%2520review%2520not%2520only%250Asynthesizes%2520current%2520applications%2520and%2520challenges%2520of%2520tensor%2520decomposition%2520in%250Abiomedical%2520analyses%2520but%2520also%2520outlines%2520promising%2520quantum%2520computing%2520strategies%2520to%250Aenhance%2520its%2520impact%2520on%2520deriving%2520actionable%2520insights%2520from%2520complex%2520biomedical%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Quantum%20Tensor%20Decomposition%20in%20Biomedical%20Applications&entry.906535625=Myson%20Burch%20and%20Jiasen%20Zhang%20and%20Gideon%20Idumah%20and%20Hakan%20Doga%20and%20Richard%20Lartey%20and%20Lamis%20Yehia%20and%20Mingrui%20Yang%20and%20Murat%20Yildirim%20and%20Mihriban%20Karaayvaz%20and%20Omar%20Shehab%20and%20Weihong%20Guo%20and%20Ying%20Ni%20and%20Laxmi%20Parida%20and%20Xiaojuan%20Li%20and%20Aritra%20Bose&entry.1292438233=%20%20Tensor%20decomposition%20has%20emerged%20as%20a%20powerful%20framework%20for%20feature%0Aextraction%20in%20multi-modal%20biomedical%20data.%20In%20this%20review%2C%20we%20present%20a%0Acomprehensive%20analysis%20of%20tensor%20decomposition%20methods%20such%20as%20Tucker%2C%0ACANDECOMP/PARAFAC%2C%20spiked%20tensor%20decomposition%2C%20etc.%20and%20their%20diverse%0Aapplications%20across%20biomedical%20domains%20such%20as%20imaging%2C%20multi-omics%2C%20and%0Aspatial%20transcriptomics.%20To%20systematically%20investigate%20the%20literature%2C%20we%0Aapplied%20a%20topic%20modeling-based%20approach%20that%20identifies%20and%20groups%20distinct%0Athematic%20sub-areas%20in%20biomedicine%20where%20tensor%20decomposition%20has%20been%20used%2C%0Athereby%20revealing%20key%20trends%20and%20research%20directions.%20We%20evaluated%20challenges%0Arelated%20to%20the%20scalability%20of%20latent%20spaces%20along%20with%20obtaining%20the%20optimal%0Arank%20of%20the%20tensor%2C%20which%20often%20hinder%20the%20extraction%20of%20meaningful%20features%0Afrom%20increasingly%20large%20and%20complex%20datasets.%20Additionally%2C%20we%20discuss%20recent%0Aadvances%20in%20quantum%20algorithms%20for%20tensor%20decomposition%2C%20exploring%20how%20quantum%0Acomputing%20can%20be%20leveraged%20to%20address%20these%20challenges.%20Our%20study%20includes%20a%0Apreliminary%20resource%20estimation%20analysis%20for%20quantum%20computing%20platforms%20and%0Aexamines%20the%20feasibility%20of%20implementing%20quantum-enhanced%20tensor%20decomposition%0Amethods%20on%20near-term%20quantum%20devices.%20Collectively%2C%20this%20review%20not%20only%0Asynthesizes%20current%20applications%20and%20challenges%20of%20tensor%20decomposition%20in%0Abiomedical%20analyses%20but%20also%20outlines%20promising%20quantum%20computing%20strategies%20to%0Aenhance%20its%20impact%20on%20deriving%20actionable%20insights%20from%20complex%20biomedical%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13140v1&entry.124074799=Read"},
{"title": "VidCapBench: A Comprehensive Benchmark of Video Captioning for\n  Controllable Text-to-Video Generation", "author": "Xinlong Chen and Yuanxing Zhang and Chongling Rao and Yushuo Guan and Jiaheng Liu and Fuzheng Zhang and Chengru Song and Qiang Liu and Di Zhang and Tieniu Tan", "abstract": "  The training of controllable text-to-video (T2V) models relies heavily on the\nalignment between videos and captions, yet little existing research connects\nvideo caption evaluation with T2V generation assessment. This paper introduces\nVidCapBench, a video caption evaluation scheme specifically designed for T2V\ngeneration, agnostic to any particular caption format. VidCapBench employs a\ndata annotation pipeline, combining expert model labeling and human refinement,\nto associate each collected video with key information spanning video\naesthetics, content, motion, and physical laws. VidCapBench then partitions\nthese key information attributes into automatically assessable and manually\nassessable subsets, catering to both the rapid evaluation needs of agile\ndevelopment and the accuracy requirements of thorough validation. By evaluating\nnumerous state-of-the-art captioning models, we demonstrate the superior\nstability and comprehensiveness of VidCapBench compared to existing video\ncaptioning evaluation approaches. Verification with off-the-shelf T2V models\nreveals a significant positive correlation between scores on VidCapBench and\nthe T2V quality evaluation metrics, indicating that VidCapBench can provide\nvaluable guidance for training T2V models. The project is available at\nhttps://github.com/VidCapBench/VidCapBench.\n", "link": "http://arxiv.org/abs/2502.12782v1", "date": "2025-02-18", "relevancy": 2.2565, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5883}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5821}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidCapBench%3A%20A%20Comprehensive%20Benchmark%20of%20Video%20Captioning%20for%0A%20%20Controllable%20Text-to-Video%20Generation&body=Title%3A%20VidCapBench%3A%20A%20Comprehensive%20Benchmark%20of%20Video%20Captioning%20for%0A%20%20Controllable%20Text-to-Video%20Generation%0AAuthor%3A%20Xinlong%20Chen%20and%20Yuanxing%20Zhang%20and%20Chongling%20Rao%20and%20Yushuo%20Guan%20and%20Jiaheng%20Liu%20and%20Fuzheng%20Zhang%20and%20Chengru%20Song%20and%20Qiang%20Liu%20and%20Di%20Zhang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20The%20training%20of%20controllable%20text-to-video%20%28T2V%29%20models%20relies%20heavily%20on%20the%0Aalignment%20between%20videos%20and%20captions%2C%20yet%20little%20existing%20research%20connects%0Avideo%20caption%20evaluation%20with%20T2V%20generation%20assessment.%20This%20paper%20introduces%0AVidCapBench%2C%20a%20video%20caption%20evaluation%20scheme%20specifically%20designed%20for%20T2V%0Ageneration%2C%20agnostic%20to%20any%20particular%20caption%20format.%20VidCapBench%20employs%20a%0Adata%20annotation%20pipeline%2C%20combining%20expert%20model%20labeling%20and%20human%20refinement%2C%0Ato%20associate%20each%20collected%20video%20with%20key%20information%20spanning%20video%0Aaesthetics%2C%20content%2C%20motion%2C%20and%20physical%20laws.%20VidCapBench%20then%20partitions%0Athese%20key%20information%20attributes%20into%20automatically%20assessable%20and%20manually%0Aassessable%20subsets%2C%20catering%20to%20both%20the%20rapid%20evaluation%20needs%20of%20agile%0Adevelopment%20and%20the%20accuracy%20requirements%20of%20thorough%20validation.%20By%20evaluating%0Anumerous%20state-of-the-art%20captioning%20models%2C%20we%20demonstrate%20the%20superior%0Astability%20and%20comprehensiveness%20of%20VidCapBench%20compared%20to%20existing%20video%0Acaptioning%20evaluation%20approaches.%20Verification%20with%20off-the-shelf%20T2V%20models%0Areveals%20a%20significant%20positive%20correlation%20between%20scores%20on%20VidCapBench%20and%0Athe%20T2V%20quality%20evaluation%20metrics%2C%20indicating%20that%20VidCapBench%20can%20provide%0Avaluable%20guidance%20for%20training%20T2V%20models.%20The%20project%20is%20available%20at%0Ahttps%3A//github.com/VidCapBench/VidCapBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidCapBench%253A%2520A%2520Comprehensive%2520Benchmark%2520of%2520Video%2520Captioning%2520for%250A%2520%2520Controllable%2520Text-to-Video%2520Generation%26entry.906535625%3DXinlong%2520Chen%2520and%2520Yuanxing%2520Zhang%2520and%2520Chongling%2520Rao%2520and%2520Yushuo%2520Guan%2520and%2520Jiaheng%2520Liu%2520and%2520Fuzheng%2520Zhang%2520and%2520Chengru%2520Song%2520and%2520Qiang%2520Liu%2520and%2520Di%2520Zhang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520The%2520training%2520of%2520controllable%2520text-to-video%2520%2528T2V%2529%2520models%2520relies%2520heavily%2520on%2520the%250Aalignment%2520between%2520videos%2520and%2520captions%252C%2520yet%2520little%2520existing%2520research%2520connects%250Avideo%2520caption%2520evaluation%2520with%2520T2V%2520generation%2520assessment.%2520This%2520paper%2520introduces%250AVidCapBench%252C%2520a%2520video%2520caption%2520evaluation%2520scheme%2520specifically%2520designed%2520for%2520T2V%250Ageneration%252C%2520agnostic%2520to%2520any%2520particular%2520caption%2520format.%2520VidCapBench%2520employs%2520a%250Adata%2520annotation%2520pipeline%252C%2520combining%2520expert%2520model%2520labeling%2520and%2520human%2520refinement%252C%250Ato%2520associate%2520each%2520collected%2520video%2520with%2520key%2520information%2520spanning%2520video%250Aaesthetics%252C%2520content%252C%2520motion%252C%2520and%2520physical%2520laws.%2520VidCapBench%2520then%2520partitions%250Athese%2520key%2520information%2520attributes%2520into%2520automatically%2520assessable%2520and%2520manually%250Aassessable%2520subsets%252C%2520catering%2520to%2520both%2520the%2520rapid%2520evaluation%2520needs%2520of%2520agile%250Adevelopment%2520and%2520the%2520accuracy%2520requirements%2520of%2520thorough%2520validation.%2520By%2520evaluating%250Anumerous%2520state-of-the-art%2520captioning%2520models%252C%2520we%2520demonstrate%2520the%2520superior%250Astability%2520and%2520comprehensiveness%2520of%2520VidCapBench%2520compared%2520to%2520existing%2520video%250Acaptioning%2520evaluation%2520approaches.%2520Verification%2520with%2520off-the-shelf%2520T2V%2520models%250Areveals%2520a%2520significant%2520positive%2520correlation%2520between%2520scores%2520on%2520VidCapBench%2520and%250Athe%2520T2V%2520quality%2520evaluation%2520metrics%252C%2520indicating%2520that%2520VidCapBench%2520can%2520provide%250Avaluable%2520guidance%2520for%2520training%2520T2V%2520models.%2520The%2520project%2520is%2520available%2520at%250Ahttps%253A//github.com/VidCapBench/VidCapBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidCapBench%3A%20A%20Comprehensive%20Benchmark%20of%20Video%20Captioning%20for%0A%20%20Controllable%20Text-to-Video%20Generation&entry.906535625=Xinlong%20Chen%20and%20Yuanxing%20Zhang%20and%20Chongling%20Rao%20and%20Yushuo%20Guan%20and%20Jiaheng%20Liu%20and%20Fuzheng%20Zhang%20and%20Chengru%20Song%20and%20Qiang%20Liu%20and%20Di%20Zhang%20and%20Tieniu%20Tan&entry.1292438233=%20%20The%20training%20of%20controllable%20text-to-video%20%28T2V%29%20models%20relies%20heavily%20on%20the%0Aalignment%20between%20videos%20and%20captions%2C%20yet%20little%20existing%20research%20connects%0Avideo%20caption%20evaluation%20with%20T2V%20generation%20assessment.%20This%20paper%20introduces%0AVidCapBench%2C%20a%20video%20caption%20evaluation%20scheme%20specifically%20designed%20for%20T2V%0Ageneration%2C%20agnostic%20to%20any%20particular%20caption%20format.%20VidCapBench%20employs%20a%0Adata%20annotation%20pipeline%2C%20combining%20expert%20model%20labeling%20and%20human%20refinement%2C%0Ato%20associate%20each%20collected%20video%20with%20key%20information%20spanning%20video%0Aaesthetics%2C%20content%2C%20motion%2C%20and%20physical%20laws.%20VidCapBench%20then%20partitions%0Athese%20key%20information%20attributes%20into%20automatically%20assessable%20and%20manually%0Aassessable%20subsets%2C%20catering%20to%20both%20the%20rapid%20evaluation%20needs%20of%20agile%0Adevelopment%20and%20the%20accuracy%20requirements%20of%20thorough%20validation.%20By%20evaluating%0Anumerous%20state-of-the-art%20captioning%20models%2C%20we%20demonstrate%20the%20superior%0Astability%20and%20comprehensiveness%20of%20VidCapBench%20compared%20to%20existing%20video%0Acaptioning%20evaluation%20approaches.%20Verification%20with%20off-the-shelf%20T2V%20models%0Areveals%20a%20significant%20positive%20correlation%20between%20scores%20on%20VidCapBench%20and%0Athe%20T2V%20quality%20evaluation%20metrics%2C%20indicating%20that%20VidCapBench%20can%20provide%0Avaluable%20guidance%20for%20training%20T2V%20models.%20The%20project%20is%20available%20at%0Ahttps%3A//github.com/VidCapBench/VidCapBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12782v1&entry.124074799=Read"},
{"title": "HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit", "author": "Qingwei Ben and Feiyu Jia and Jia Zeng and Junting Dong and Dahua Lin and Jiangmiao Pang", "abstract": "  Current humanoid teleoperation systems either lack reliable low-level control\npolicies, or struggle to acquire accurate whole-body control commands, making\nit difficult to teleoperate humanoids for loco-manipulation tasks. To solve\nthese issues, we propose HOMIE, a novel humanoid teleoperation cockpit\nintegrates a humanoid loco-manipulation policy and a low-cost exoskeleton-based\nhardware system. The policy enables humanoid robots to walk and squat to\nspecific heights while accommodating arbitrary upper-body poses. This is\nachieved through our novel reinforcement learning-based training framework that\nincorporates upper-body pose curriculum, height-tracking reward, and symmetry\nutilization, without relying on any motion priors. Complementing the policy,\nthe hardware system integrates isomorphic exoskeleton arms, a pair of\nmotion-sensing gloves, and a pedal, allowing a single operator to achieve full\ncontrol of the humanoid robot. Our experiments show our cockpit facilitates\nmore stable, rapid, and precise humanoid loco-manipulation teleoperation,\naccelerating task completion and eliminating retargeting errors compared to\ninverse kinematics-based methods. We also validate the effectiveness of the\ndata collected by our cockpit for imitation learning. Our project is fully\nopen-sourced, demos and code can be found in https://homietele.github.io/.\n", "link": "http://arxiv.org/abs/2502.13013v1", "date": "2025-02-18", "relevancy": 2.2495, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5868}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5454}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOMIE%3A%20Humanoid%20Loco-Manipulation%20with%20Isomorphic%20Exoskeleton%20Cockpit&body=Title%3A%20HOMIE%3A%20Humanoid%20Loco-Manipulation%20with%20Isomorphic%20Exoskeleton%20Cockpit%0AAuthor%3A%20Qingwei%20Ben%20and%20Feiyu%20Jia%20and%20Jia%20Zeng%20and%20Junting%20Dong%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Current%20humanoid%20teleoperation%20systems%20either%20lack%20reliable%20low-level%20control%0Apolicies%2C%20or%20struggle%20to%20acquire%20accurate%20whole-body%20control%20commands%2C%20making%0Ait%20difficult%20to%20teleoperate%20humanoids%20for%20loco-manipulation%20tasks.%20To%20solve%0Athese%20issues%2C%20we%20propose%20HOMIE%2C%20a%20novel%20humanoid%20teleoperation%20cockpit%0Aintegrates%20a%20humanoid%20loco-manipulation%20policy%20and%20a%20low-cost%20exoskeleton-based%0Ahardware%20system.%20The%20policy%20enables%20humanoid%20robots%20to%20walk%20and%20squat%20to%0Aspecific%20heights%20while%20accommodating%20arbitrary%20upper-body%20poses.%20This%20is%0Aachieved%20through%20our%20novel%20reinforcement%20learning-based%20training%20framework%20that%0Aincorporates%20upper-body%20pose%20curriculum%2C%20height-tracking%20reward%2C%20and%20symmetry%0Autilization%2C%20without%20relying%20on%20any%20motion%20priors.%20Complementing%20the%20policy%2C%0Athe%20hardware%20system%20integrates%20isomorphic%20exoskeleton%20arms%2C%20a%20pair%20of%0Amotion-sensing%20gloves%2C%20and%20a%20pedal%2C%20allowing%20a%20single%20operator%20to%20achieve%20full%0Acontrol%20of%20the%20humanoid%20robot.%20Our%20experiments%20show%20our%20cockpit%20facilitates%0Amore%20stable%2C%20rapid%2C%20and%20precise%20humanoid%20loco-manipulation%20teleoperation%2C%0Aaccelerating%20task%20completion%20and%20eliminating%20retargeting%20errors%20compared%20to%0Ainverse%20kinematics-based%20methods.%20We%20also%20validate%20the%20effectiveness%20of%20the%0Adata%20collected%20by%20our%20cockpit%20for%20imitation%20learning.%20Our%20project%20is%20fully%0Aopen-sourced%2C%20demos%20and%20code%20can%20be%20found%20in%20https%3A//homietele.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOMIE%253A%2520Humanoid%2520Loco-Manipulation%2520with%2520Isomorphic%2520Exoskeleton%2520Cockpit%26entry.906535625%3DQingwei%2520Ben%2520and%2520Feiyu%2520Jia%2520and%2520Jia%2520Zeng%2520and%2520Junting%2520Dong%2520and%2520Dahua%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Current%2520humanoid%2520teleoperation%2520systems%2520either%2520lack%2520reliable%2520low-level%2520control%250Apolicies%252C%2520or%2520struggle%2520to%2520acquire%2520accurate%2520whole-body%2520control%2520commands%252C%2520making%250Ait%2520difficult%2520to%2520teleoperate%2520humanoids%2520for%2520loco-manipulation%2520tasks.%2520To%2520solve%250Athese%2520issues%252C%2520we%2520propose%2520HOMIE%252C%2520a%2520novel%2520humanoid%2520teleoperation%2520cockpit%250Aintegrates%2520a%2520humanoid%2520loco-manipulation%2520policy%2520and%2520a%2520low-cost%2520exoskeleton-based%250Ahardware%2520system.%2520The%2520policy%2520enables%2520humanoid%2520robots%2520to%2520walk%2520and%2520squat%2520to%250Aspecific%2520heights%2520while%2520accommodating%2520arbitrary%2520upper-body%2520poses.%2520This%2520is%250Aachieved%2520through%2520our%2520novel%2520reinforcement%2520learning-based%2520training%2520framework%2520that%250Aincorporates%2520upper-body%2520pose%2520curriculum%252C%2520height-tracking%2520reward%252C%2520and%2520symmetry%250Autilization%252C%2520without%2520relying%2520on%2520any%2520motion%2520priors.%2520Complementing%2520the%2520policy%252C%250Athe%2520hardware%2520system%2520integrates%2520isomorphic%2520exoskeleton%2520arms%252C%2520a%2520pair%2520of%250Amotion-sensing%2520gloves%252C%2520and%2520a%2520pedal%252C%2520allowing%2520a%2520single%2520operator%2520to%2520achieve%2520full%250Acontrol%2520of%2520the%2520humanoid%2520robot.%2520Our%2520experiments%2520show%2520our%2520cockpit%2520facilitates%250Amore%2520stable%252C%2520rapid%252C%2520and%2520precise%2520humanoid%2520loco-manipulation%2520teleoperation%252C%250Aaccelerating%2520task%2520completion%2520and%2520eliminating%2520retargeting%2520errors%2520compared%2520to%250Ainverse%2520kinematics-based%2520methods.%2520We%2520also%2520validate%2520the%2520effectiveness%2520of%2520the%250Adata%2520collected%2520by%2520our%2520cockpit%2520for%2520imitation%2520learning.%2520Our%2520project%2520is%2520fully%250Aopen-sourced%252C%2520demos%2520and%2520code%2520can%2520be%2520found%2520in%2520https%253A//homietele.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOMIE%3A%20Humanoid%20Loco-Manipulation%20with%20Isomorphic%20Exoskeleton%20Cockpit&entry.906535625=Qingwei%20Ben%20and%20Feiyu%20Jia%20and%20Jia%20Zeng%20and%20Junting%20Dong%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Current%20humanoid%20teleoperation%20systems%20either%20lack%20reliable%20low-level%20control%0Apolicies%2C%20or%20struggle%20to%20acquire%20accurate%20whole-body%20control%20commands%2C%20making%0Ait%20difficult%20to%20teleoperate%20humanoids%20for%20loco-manipulation%20tasks.%20To%20solve%0Athese%20issues%2C%20we%20propose%20HOMIE%2C%20a%20novel%20humanoid%20teleoperation%20cockpit%0Aintegrates%20a%20humanoid%20loco-manipulation%20policy%20and%20a%20low-cost%20exoskeleton-based%0Ahardware%20system.%20The%20policy%20enables%20humanoid%20robots%20to%20walk%20and%20squat%20to%0Aspecific%20heights%20while%20accommodating%20arbitrary%20upper-body%20poses.%20This%20is%0Aachieved%20through%20our%20novel%20reinforcement%20learning-based%20training%20framework%20that%0Aincorporates%20upper-body%20pose%20curriculum%2C%20height-tracking%20reward%2C%20and%20symmetry%0Autilization%2C%20without%20relying%20on%20any%20motion%20priors.%20Complementing%20the%20policy%2C%0Athe%20hardware%20system%20integrates%20isomorphic%20exoskeleton%20arms%2C%20a%20pair%20of%0Amotion-sensing%20gloves%2C%20and%20a%20pedal%2C%20allowing%20a%20single%20operator%20to%20achieve%20full%0Acontrol%20of%20the%20humanoid%20robot.%20Our%20experiments%20show%20our%20cockpit%20facilitates%0Amore%20stable%2C%20rapid%2C%20and%20precise%20humanoid%20loco-manipulation%20teleoperation%2C%0Aaccelerating%20task%20completion%20and%20eliminating%20retargeting%20errors%20compared%20to%0Ainverse%20kinematics-based%20methods.%20We%20also%20validate%20the%20effectiveness%20of%20the%0Adata%20collected%20by%20our%20cockpit%20for%20imitation%20learning.%20Our%20project%20is%20fully%0Aopen-sourced%2C%20demos%20and%20code%20can%20be%20found%20in%20https%3A//homietele.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13013v1&entry.124074799=Read"},
{"title": "Magma: A Foundation Model for Multimodal AI Agents", "author": "Jianwei Yang and Reuben Tan and Qianhui Wu and Ruijie Zheng and Baolin Peng and Yongyuan Liang and Yu Gu and Mu Cai and Seonghyeon Ye and Joel Jang and Yuquan Deng and Lars Liden and Jianfeng Gao", "abstract": "  We present Magma, a foundation model that serves multimodal AI agentic tasks\nin both the digital and physical worlds. Magma is a significant extension of\nvision-language (VL) models in that it not only retains the VL understanding\nability (verbal intelligence) of the latter, but is also equipped with the\nability to plan and act in the visual-spatial world (spatial-temporal\nintelligence) and complete agentic tasks ranging from UI navigation to robot\nmanipulation. To endow the agentic capabilities, Magma is pretrained on large\namounts of heterogeneous datasets spanning from images, videos to robotics\ndata, where the actionable visual objects (e.g., clickable buttons in GUI) in\nimages are labeled by Set-of-Mark (SoM) for action grounding, and the object\nmovements (e.g., the trace of human hands or robotic arms) in videos are\nlabeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show\nthat SoM and ToM reach great synergy and facilitate the acquisition of\nspatial-temporal intelligence for our Magma model, which is fundamental to a\nwide range of tasks as shown in Fig.1. In particular, Magma creates new\nstate-of-the-art results on UI navigation and robotic manipulation tasks,\noutperforming previous models that are specifically tailored to these tasks. On\nimage and video-related multimodal tasks, Magma also compares favorably to\npopular large multimodal models that are trained on much larger datasets. We\nmake our model and code public for reproducibility at\nhttps://microsoft.github.io/Magma.\n", "link": "http://arxiv.org/abs/2502.13130v1", "date": "2025-02-18", "relevancy": 2.2434, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5698}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Magma%3A%20A%20Foundation%20Model%20for%20Multimodal%20AI%20Agents&body=Title%3A%20Magma%3A%20A%20Foundation%20Model%20for%20Multimodal%20AI%20Agents%0AAuthor%3A%20Jianwei%20Yang%20and%20Reuben%20Tan%20and%20Qianhui%20Wu%20and%20Ruijie%20Zheng%20and%20Baolin%20Peng%20and%20Yongyuan%20Liang%20and%20Yu%20Gu%20and%20Mu%20Cai%20and%20Seonghyeon%20Ye%20and%20Joel%20Jang%20and%20Yuquan%20Deng%20and%20Lars%20Liden%20and%20Jianfeng%20Gao%0AAbstract%3A%20%20%20We%20present%20Magma%2C%20a%20foundation%20model%20that%20serves%20multimodal%20AI%20agentic%20tasks%0Ain%20both%20the%20digital%20and%20physical%20worlds.%20Magma%20is%20a%20significant%20extension%20of%0Avision-language%20%28VL%29%20models%20in%20that%20it%20not%20only%20retains%20the%20VL%20understanding%0Aability%20%28verbal%20intelligence%29%20of%20the%20latter%2C%20but%20is%20also%20equipped%20with%20the%0Aability%20to%20plan%20and%20act%20in%20the%20visual-spatial%20world%20%28spatial-temporal%0Aintelligence%29%20and%20complete%20agentic%20tasks%20ranging%20from%20UI%20navigation%20to%20robot%0Amanipulation.%20To%20endow%20the%20agentic%20capabilities%2C%20Magma%20is%20pretrained%20on%20large%0Aamounts%20of%20heterogeneous%20datasets%20spanning%20from%20images%2C%20videos%20to%20robotics%0Adata%2C%20where%20the%20actionable%20visual%20objects%20%28e.g.%2C%20clickable%20buttons%20in%20GUI%29%20in%0Aimages%20are%20labeled%20by%20Set-of-Mark%20%28SoM%29%20for%20action%20grounding%2C%20and%20the%20object%0Amovements%20%28e.g.%2C%20the%20trace%20of%20human%20hands%20or%20robotic%20arms%29%20in%20videos%20are%0Alabeled%20by%20Trace-of-Mark%20%28ToM%29%20for%20action%20planning.%20Extensive%20experiments%20show%0Athat%20SoM%20and%20ToM%20reach%20great%20synergy%20and%20facilitate%20the%20acquisition%20of%0Aspatial-temporal%20intelligence%20for%20our%20Magma%20model%2C%20which%20is%20fundamental%20to%20a%0Awide%20range%20of%20tasks%20as%20shown%20in%20Fig.1.%20In%20particular%2C%20Magma%20creates%20new%0Astate-of-the-art%20results%20on%20UI%20navigation%20and%20robotic%20manipulation%20tasks%2C%0Aoutperforming%20previous%20models%20that%20are%20specifically%20tailored%20to%20these%20tasks.%20On%0Aimage%20and%20video-related%20multimodal%20tasks%2C%20Magma%20also%20compares%20favorably%20to%0Apopular%20large%20multimodal%20models%20that%20are%20trained%20on%20much%20larger%20datasets.%20We%0Amake%20our%20model%20and%20code%20public%20for%20reproducibility%20at%0Ahttps%3A//microsoft.github.io/Magma.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagma%253A%2520A%2520Foundation%2520Model%2520for%2520Multimodal%2520AI%2520Agents%26entry.906535625%3DJianwei%2520Yang%2520and%2520Reuben%2520Tan%2520and%2520Qianhui%2520Wu%2520and%2520Ruijie%2520Zheng%2520and%2520Baolin%2520Peng%2520and%2520Yongyuan%2520Liang%2520and%2520Yu%2520Gu%2520and%2520Mu%2520Cai%2520and%2520Seonghyeon%2520Ye%2520and%2520Joel%2520Jang%2520and%2520Yuquan%2520Deng%2520and%2520Lars%2520Liden%2520and%2520Jianfeng%2520Gao%26entry.1292438233%3D%2520%2520We%2520present%2520Magma%252C%2520a%2520foundation%2520model%2520that%2520serves%2520multimodal%2520AI%2520agentic%2520tasks%250Ain%2520both%2520the%2520digital%2520and%2520physical%2520worlds.%2520Magma%2520is%2520a%2520significant%2520extension%2520of%250Avision-language%2520%2528VL%2529%2520models%2520in%2520that%2520it%2520not%2520only%2520retains%2520the%2520VL%2520understanding%250Aability%2520%2528verbal%2520intelligence%2529%2520of%2520the%2520latter%252C%2520but%2520is%2520also%2520equipped%2520with%2520the%250Aability%2520to%2520plan%2520and%2520act%2520in%2520the%2520visual-spatial%2520world%2520%2528spatial-temporal%250Aintelligence%2529%2520and%2520complete%2520agentic%2520tasks%2520ranging%2520from%2520UI%2520navigation%2520to%2520robot%250Amanipulation.%2520To%2520endow%2520the%2520agentic%2520capabilities%252C%2520Magma%2520is%2520pretrained%2520on%2520large%250Aamounts%2520of%2520heterogeneous%2520datasets%2520spanning%2520from%2520images%252C%2520videos%2520to%2520robotics%250Adata%252C%2520where%2520the%2520actionable%2520visual%2520objects%2520%2528e.g.%252C%2520clickable%2520buttons%2520in%2520GUI%2529%2520in%250Aimages%2520are%2520labeled%2520by%2520Set-of-Mark%2520%2528SoM%2529%2520for%2520action%2520grounding%252C%2520and%2520the%2520object%250Amovements%2520%2528e.g.%252C%2520the%2520trace%2520of%2520human%2520hands%2520or%2520robotic%2520arms%2529%2520in%2520videos%2520are%250Alabeled%2520by%2520Trace-of-Mark%2520%2528ToM%2529%2520for%2520action%2520planning.%2520Extensive%2520experiments%2520show%250Athat%2520SoM%2520and%2520ToM%2520reach%2520great%2520synergy%2520and%2520facilitate%2520the%2520acquisition%2520of%250Aspatial-temporal%2520intelligence%2520for%2520our%2520Magma%2520model%252C%2520which%2520is%2520fundamental%2520to%2520a%250Awide%2520range%2520of%2520tasks%2520as%2520shown%2520in%2520Fig.1.%2520In%2520particular%252C%2520Magma%2520creates%2520new%250Astate-of-the-art%2520results%2520on%2520UI%2520navigation%2520and%2520robotic%2520manipulation%2520tasks%252C%250Aoutperforming%2520previous%2520models%2520that%2520are%2520specifically%2520tailored%2520to%2520these%2520tasks.%2520On%250Aimage%2520and%2520video-related%2520multimodal%2520tasks%252C%2520Magma%2520also%2520compares%2520favorably%2520to%250Apopular%2520large%2520multimodal%2520models%2520that%2520are%2520trained%2520on%2520much%2520larger%2520datasets.%2520We%250Amake%2520our%2520model%2520and%2520code%2520public%2520for%2520reproducibility%2520at%250Ahttps%253A//microsoft.github.io/Magma.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Magma%3A%20A%20Foundation%20Model%20for%20Multimodal%20AI%20Agents&entry.906535625=Jianwei%20Yang%20and%20Reuben%20Tan%20and%20Qianhui%20Wu%20and%20Ruijie%20Zheng%20and%20Baolin%20Peng%20and%20Yongyuan%20Liang%20and%20Yu%20Gu%20and%20Mu%20Cai%20and%20Seonghyeon%20Ye%20and%20Joel%20Jang%20and%20Yuquan%20Deng%20and%20Lars%20Liden%20and%20Jianfeng%20Gao&entry.1292438233=%20%20We%20present%20Magma%2C%20a%20foundation%20model%20that%20serves%20multimodal%20AI%20agentic%20tasks%0Ain%20both%20the%20digital%20and%20physical%20worlds.%20Magma%20is%20a%20significant%20extension%20of%0Avision-language%20%28VL%29%20models%20in%20that%20it%20not%20only%20retains%20the%20VL%20understanding%0Aability%20%28verbal%20intelligence%29%20of%20the%20latter%2C%20but%20is%20also%20equipped%20with%20the%0Aability%20to%20plan%20and%20act%20in%20the%20visual-spatial%20world%20%28spatial-temporal%0Aintelligence%29%20and%20complete%20agentic%20tasks%20ranging%20from%20UI%20navigation%20to%20robot%0Amanipulation.%20To%20endow%20the%20agentic%20capabilities%2C%20Magma%20is%20pretrained%20on%20large%0Aamounts%20of%20heterogeneous%20datasets%20spanning%20from%20images%2C%20videos%20to%20robotics%0Adata%2C%20where%20the%20actionable%20visual%20objects%20%28e.g.%2C%20clickable%20buttons%20in%20GUI%29%20in%0Aimages%20are%20labeled%20by%20Set-of-Mark%20%28SoM%29%20for%20action%20grounding%2C%20and%20the%20object%0Amovements%20%28e.g.%2C%20the%20trace%20of%20human%20hands%20or%20robotic%20arms%29%20in%20videos%20are%0Alabeled%20by%20Trace-of-Mark%20%28ToM%29%20for%20action%20planning.%20Extensive%20experiments%20show%0Athat%20SoM%20and%20ToM%20reach%20great%20synergy%20and%20facilitate%20the%20acquisition%20of%0Aspatial-temporal%20intelligence%20for%20our%20Magma%20model%2C%20which%20is%20fundamental%20to%20a%0Awide%20range%20of%20tasks%20as%20shown%20in%20Fig.1.%20In%20particular%2C%20Magma%20creates%20new%0Astate-of-the-art%20results%20on%20UI%20navigation%20and%20robotic%20manipulation%20tasks%2C%0Aoutperforming%20previous%20models%20that%20are%20specifically%20tailored%20to%20these%20tasks.%20On%0Aimage%20and%20video-related%20multimodal%20tasks%2C%20Magma%20also%20compares%20favorably%20to%0Apopular%20large%20multimodal%20models%20that%20are%20trained%20on%20much%20larger%20datasets.%20We%0Amake%20our%20model%20and%20code%20public%20for%20reproducibility%20at%0Ahttps%3A//microsoft.github.io/Magma.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13130v1&entry.124074799=Read"},
{"title": "DeepSeek-V3 Technical Report", "author": " DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan", "abstract": "  We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3.\n", "link": "http://arxiv.org/abs/2412.19437v2", "date": "2025-02-18", "relevancy": 2.2219, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepSeek-V3%20Technical%20Report&body=Title%3A%20DeepSeek-V3%20Technical%20Report%0AAuthor%3A%20%20DeepSeek-AI%20and%20Aixin%20Liu%20and%20Bei%20Feng%20and%20Bing%20Xue%20and%20Bingxuan%20Wang%20and%20Bochao%20Wu%20and%20Chengda%20Lu%20and%20Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Chenyu%20Zhang%20and%20Chong%20Ruan%20and%20Damai%20Dai%20and%20Daya%20Guo%20and%20Dejian%20Yang%20and%20Deli%20Chen%20and%20Dongjie%20Ji%20and%20Erhang%20Li%20and%20Fangyun%20Lin%20and%20Fucong%20Dai%20and%20Fuli%20Luo%20and%20Guangbo%20Hao%20and%20Guanting%20Chen%20and%20Guowei%20Li%20and%20H.%20Zhang%20and%20Han%20Bao%20and%20Hanwei%20Xu%20and%20Haocheng%20Wang%20and%20Haowei%20Zhang%20and%20Honghui%20Ding%20and%20Huajian%20Xin%20and%20Huazuo%20Gao%20and%20Hui%20Li%20and%20Hui%20Qu%20and%20J.%20L.%20Cai%20and%20Jian%20Liang%20and%20Jianzhong%20Guo%20and%20Jiaqi%20Ni%20and%20Jiashi%20Li%20and%20Jiawei%20Wang%20and%20Jin%20Chen%20and%20Jingchang%20Chen%20and%20Jingyang%20Yuan%20and%20Junjie%20Qiu%20and%20Junlong%20Li%20and%20Junxiao%20Song%20and%20Kai%20Dong%20and%20Kai%20Hu%20and%20Kaige%20Gao%20and%20Kang%20Guan%20and%20Kexin%20Huang%20and%20Kuai%20Yu%20and%20Lean%20Wang%20and%20Lecong%20Zhang%20and%20Lei%20Xu%20and%20Leyi%20Xia%20and%20Liang%20Zhao%20and%20Litong%20Wang%20and%20Liyue%20Zhang%20and%20Meng%20Li%20and%20Miaojun%20Wang%20and%20Mingchuan%20Zhang%20and%20Minghua%20Zhang%20and%20Minghui%20Tang%20and%20Mingming%20Li%20and%20Ning%20Tian%20and%20Panpan%20Huang%20and%20Peiyi%20Wang%20and%20Peng%20Zhang%20and%20Qiancheng%20Wang%20and%20Qihao%20Zhu%20and%20Qinyu%20Chen%20and%20Qiushi%20Du%20and%20R.%20J.%20Chen%20and%20R.%20L.%20Jin%20and%20Ruiqi%20Ge%20and%20Ruisong%20Zhang%20and%20Ruizhe%20Pan%20and%20Runji%20Wang%20and%20Runxin%20Xu%20and%20Ruoyu%20Zhang%20and%20Ruyi%20Chen%20and%20S.%20S.%20Li%20and%20Shanghao%20Lu%20and%20Shangyan%20Zhou%20and%20Shanhuang%20Chen%20and%20Shaoqing%20Wu%20and%20Shengfeng%20Ye%20and%20Shengfeng%20Ye%20and%20Shirong%20Ma%20and%20Shiyu%20Wang%20and%20Shuang%20Zhou%20and%20Shuiping%20Yu%20and%20Shunfeng%20Zhou%20and%20Shuting%20Pan%20and%20T.%20Wang%20and%20Tao%20Yun%20and%20Tian%20Pei%20and%20Tianyu%20Sun%20and%20W.%20L.%20Xiao%20and%20Wangding%20Zeng%20and%20Wanjia%20Zhao%20and%20Wei%20An%20and%20Wen%20Liu%20and%20Wenfeng%20Liang%20and%20Wenjun%20Gao%20and%20Wenqin%20Yu%20and%20Wentao%20Zhang%20and%20X.%20Q.%20Li%20and%20Xiangyue%20Jin%20and%20Xianzu%20Wang%20and%20Xiao%20Bi%20and%20Xiaodong%20Liu%20and%20Xiaohan%20Wang%20and%20Xiaojin%20Shen%20and%20Xiaokang%20Chen%20and%20Xiaokang%20Zhang%20and%20Xiaosha%20Chen%20and%20Xiaotao%20Nie%20and%20Xiaowen%20Sun%20and%20Xiaoxiang%20Wang%20and%20Xin%20Cheng%20and%20Xin%20Liu%20and%20Xin%20Xie%20and%20Xingchao%20Liu%20and%20Xingkai%20Yu%20and%20Xinnan%20Song%20and%20Xinxia%20Shan%20and%20Xinyi%20Zhou%20and%20Xinyu%20Yang%20and%20Xinyuan%20Li%20and%20Xuecheng%20Su%20and%20Xuheng%20Lin%20and%20Y.%20K.%20Li%20and%20Y.%20Q.%20Wang%20and%20Y.%20X.%20Wei%20and%20Y.%20X.%20Zhu%20and%20Yang%20Zhang%20and%20Yanhong%20Xu%20and%20Yanhong%20Xu%20and%20Yanping%20Huang%20and%20Yao%20Li%20and%20Yao%20Zhao%20and%20Yaofeng%20Sun%20and%20Yaohui%20Li%20and%20Yaohui%20Wang%20and%20Yi%20Yu%20and%20Yi%20Zheng%20and%20Yichao%20Zhang%20and%20Yifan%20Shi%20and%20Yiliang%20Xiong%20and%20Ying%20He%20and%20Ying%20Tang%20and%20Yishi%20Piao%20and%20Yisong%20Wang%20and%20Yixuan%20Tan%20and%20Yiyang%20Ma%20and%20Yiyuan%20Liu%20and%20Yongqiang%20Guo%20and%20Yu%20Wu%20and%20Yuan%20Ou%20and%20Yuchen%20Zhu%20and%20Yuduan%20Wang%20and%20Yue%20Gong%20and%20Yuheng%20Zou%20and%20Yujia%20He%20and%20Yukun%20Zha%20and%20Yunfan%20Xiong%20and%20Yunxian%20Ma%20and%20Yuting%20Yan%20and%20Yuxiang%20Luo%20and%20Yuxiang%20You%20and%20Yuxuan%20Liu%20and%20Yuyang%20Zhou%20and%20Z.%20F.%20Wu%20and%20Z.%20Z.%20Ren%20and%20Zehui%20Ren%20and%20Zhangli%20Sha%20and%20Zhe%20Fu%20and%20Zhean%20Xu%20and%20Zhen%20Huang%20and%20Zhen%20Zhang%20and%20Zhenda%20Xie%20and%20Zhengyan%20Zhang%20and%20Zhewen%20Hao%20and%20Zhibin%20Gou%20and%20Zhicheng%20Ma%20and%20Zhigang%20Yan%20and%20Zhihong%20Shao%20and%20Zhipeng%20Xu%20and%20Zhiyu%20Wu%20and%20Zhongyu%20Zhang%20and%20Zhuoshu%20Li%20and%20Zihui%20Gu%20and%20Zijia%20Zhu%20and%20Zijun%20Liu%20and%20Zilin%20Li%20and%20Ziwei%20Xie%20and%20Ziyang%20Song%20and%20Ziyi%20Gao%20and%20Zizheng%20Pan%0AAbstract%3A%20%20%20We%20present%20DeepSeek-V3%2C%20a%20strong%20Mixture-of-Experts%20%28MoE%29%20language%20model%20with%0A671B%20total%20parameters%20with%2037B%20activated%20for%20each%20token.%20To%20achieve%20efficient%0Ainference%20and%20cost-effective%20training%2C%20DeepSeek-V3%20adopts%20Multi-head%20Latent%0AAttention%20%28MLA%29%20and%20DeepSeekMoE%20architectures%2C%20which%20were%20thoroughly%20validated%0Ain%20DeepSeek-V2.%20Furthermore%2C%20DeepSeek-V3%20pioneers%20an%20auxiliary-loss-free%0Astrategy%20for%20load%20balancing%20and%20sets%20a%20multi-token%20prediction%20training%0Aobjective%20for%20stronger%20performance.%20We%20pre-train%20DeepSeek-V3%20on%2014.8%20trillion%0Adiverse%20and%20high-quality%20tokens%2C%20followed%20by%20Supervised%20Fine-Tuning%20and%0AReinforcement%20Learning%20stages%20to%20fully%20harness%20its%20capabilities.%20Comprehensive%0Aevaluations%20reveal%20that%20DeepSeek-V3%20outperforms%20other%20open-source%20models%20and%0Aachieves%20performance%20comparable%20to%20leading%20closed-source%20models.%20Despite%20its%0Aexcellent%20performance%2C%20DeepSeek-V3%20requires%20only%202.788M%20H800%20GPU%20hours%20for%20its%0Afull%20training.%20In%20addition%2C%20its%20training%20process%20is%20remarkably%20stable.%0AThroughout%20the%20entire%20training%20process%2C%20we%20did%20not%20experience%20any%20irrecoverable%0Aloss%20spikes%20or%20perform%20any%20rollbacks.%20The%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/deepseek-ai/DeepSeek-V3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19437v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepSeek-V3%2520Technical%2520Report%26entry.906535625%3D%2520DeepSeek-AI%2520and%2520Aixin%2520Liu%2520and%2520Bei%2520Feng%2520and%2520Bing%2520Xue%2520and%2520Bingxuan%2520Wang%2520and%2520Bochao%2520Wu%2520and%2520Chengda%2520Lu%2520and%2520Chenggang%2520Zhao%2520and%2520Chengqi%2520Deng%2520and%2520Chenyu%2520Zhang%2520and%2520Chong%2520Ruan%2520and%2520Damai%2520Dai%2520and%2520Daya%2520Guo%2520and%2520Dejian%2520Yang%2520and%2520Deli%2520Chen%2520and%2520Dongjie%2520Ji%2520and%2520Erhang%2520Li%2520and%2520Fangyun%2520Lin%2520and%2520Fucong%2520Dai%2520and%2520Fuli%2520Luo%2520and%2520Guangbo%2520Hao%2520and%2520Guanting%2520Chen%2520and%2520Guowei%2520Li%2520and%2520H.%2520Zhang%2520and%2520Han%2520Bao%2520and%2520Hanwei%2520Xu%2520and%2520Haocheng%2520Wang%2520and%2520Haowei%2520Zhang%2520and%2520Honghui%2520Ding%2520and%2520Huajian%2520Xin%2520and%2520Huazuo%2520Gao%2520and%2520Hui%2520Li%2520and%2520Hui%2520Qu%2520and%2520J.%2520L.%2520Cai%2520and%2520Jian%2520Liang%2520and%2520Jianzhong%2520Guo%2520and%2520Jiaqi%2520Ni%2520and%2520Jiashi%2520Li%2520and%2520Jiawei%2520Wang%2520and%2520Jin%2520Chen%2520and%2520Jingchang%2520Chen%2520and%2520Jingyang%2520Yuan%2520and%2520Junjie%2520Qiu%2520and%2520Junlong%2520Li%2520and%2520Junxiao%2520Song%2520and%2520Kai%2520Dong%2520and%2520Kai%2520Hu%2520and%2520Kaige%2520Gao%2520and%2520Kang%2520Guan%2520and%2520Kexin%2520Huang%2520and%2520Kuai%2520Yu%2520and%2520Lean%2520Wang%2520and%2520Lecong%2520Zhang%2520and%2520Lei%2520Xu%2520and%2520Leyi%2520Xia%2520and%2520Liang%2520Zhao%2520and%2520Litong%2520Wang%2520and%2520Liyue%2520Zhang%2520and%2520Meng%2520Li%2520and%2520Miaojun%2520Wang%2520and%2520Mingchuan%2520Zhang%2520and%2520Minghua%2520Zhang%2520and%2520Minghui%2520Tang%2520and%2520Mingming%2520Li%2520and%2520Ning%2520Tian%2520and%2520Panpan%2520Huang%2520and%2520Peiyi%2520Wang%2520and%2520Peng%2520Zhang%2520and%2520Qiancheng%2520Wang%2520and%2520Qihao%2520Zhu%2520and%2520Qinyu%2520Chen%2520and%2520Qiushi%2520Du%2520and%2520R.%2520J.%2520Chen%2520and%2520R.%2520L.%2520Jin%2520and%2520Ruiqi%2520Ge%2520and%2520Ruisong%2520Zhang%2520and%2520Ruizhe%2520Pan%2520and%2520Runji%2520Wang%2520and%2520Runxin%2520Xu%2520and%2520Ruoyu%2520Zhang%2520and%2520Ruyi%2520Chen%2520and%2520S.%2520S.%2520Li%2520and%2520Shanghao%2520Lu%2520and%2520Shangyan%2520Zhou%2520and%2520Shanhuang%2520Chen%2520and%2520Shaoqing%2520Wu%2520and%2520Shengfeng%2520Ye%2520and%2520Shengfeng%2520Ye%2520and%2520Shirong%2520Ma%2520and%2520Shiyu%2520Wang%2520and%2520Shuang%2520Zhou%2520and%2520Shuiping%2520Yu%2520and%2520Shunfeng%2520Zhou%2520and%2520Shuting%2520Pan%2520and%2520T.%2520Wang%2520and%2520Tao%2520Yun%2520and%2520Tian%2520Pei%2520and%2520Tianyu%2520Sun%2520and%2520W.%2520L.%2520Xiao%2520and%2520Wangding%2520Zeng%2520and%2520Wanjia%2520Zhao%2520and%2520Wei%2520An%2520and%2520Wen%2520Liu%2520and%2520Wenfeng%2520Liang%2520and%2520Wenjun%2520Gao%2520and%2520Wenqin%2520Yu%2520and%2520Wentao%2520Zhang%2520and%2520X.%2520Q.%2520Li%2520and%2520Xiangyue%2520Jin%2520and%2520Xianzu%2520Wang%2520and%2520Xiao%2520Bi%2520and%2520Xiaodong%2520Liu%2520and%2520Xiaohan%2520Wang%2520and%2520Xiaojin%2520Shen%2520and%2520Xiaokang%2520Chen%2520and%2520Xiaokang%2520Zhang%2520and%2520Xiaosha%2520Chen%2520and%2520Xiaotao%2520Nie%2520and%2520Xiaowen%2520Sun%2520and%2520Xiaoxiang%2520Wang%2520and%2520Xin%2520Cheng%2520and%2520Xin%2520Liu%2520and%2520Xin%2520Xie%2520and%2520Xingchao%2520Liu%2520and%2520Xingkai%2520Yu%2520and%2520Xinnan%2520Song%2520and%2520Xinxia%2520Shan%2520and%2520Xinyi%2520Zhou%2520and%2520Xinyu%2520Yang%2520and%2520Xinyuan%2520Li%2520and%2520Xuecheng%2520Su%2520and%2520Xuheng%2520Lin%2520and%2520Y.%2520K.%2520Li%2520and%2520Y.%2520Q.%2520Wang%2520and%2520Y.%2520X.%2520Wei%2520and%2520Y.%2520X.%2520Zhu%2520and%2520Yang%2520Zhang%2520and%2520Yanhong%2520Xu%2520and%2520Yanhong%2520Xu%2520and%2520Yanping%2520Huang%2520and%2520Yao%2520Li%2520and%2520Yao%2520Zhao%2520and%2520Yaofeng%2520Sun%2520and%2520Yaohui%2520Li%2520and%2520Yaohui%2520Wang%2520and%2520Yi%2520Yu%2520and%2520Yi%2520Zheng%2520and%2520Yichao%2520Zhang%2520and%2520Yifan%2520Shi%2520and%2520Yiliang%2520Xiong%2520and%2520Ying%2520He%2520and%2520Ying%2520Tang%2520and%2520Yishi%2520Piao%2520and%2520Yisong%2520Wang%2520and%2520Yixuan%2520Tan%2520and%2520Yiyang%2520Ma%2520and%2520Yiyuan%2520Liu%2520and%2520Yongqiang%2520Guo%2520and%2520Yu%2520Wu%2520and%2520Yuan%2520Ou%2520and%2520Yuchen%2520Zhu%2520and%2520Yuduan%2520Wang%2520and%2520Yue%2520Gong%2520and%2520Yuheng%2520Zou%2520and%2520Yujia%2520He%2520and%2520Yukun%2520Zha%2520and%2520Yunfan%2520Xiong%2520and%2520Yunxian%2520Ma%2520and%2520Yuting%2520Yan%2520and%2520Yuxiang%2520Luo%2520and%2520Yuxiang%2520You%2520and%2520Yuxuan%2520Liu%2520and%2520Yuyang%2520Zhou%2520and%2520Z.%2520F.%2520Wu%2520and%2520Z.%2520Z.%2520Ren%2520and%2520Zehui%2520Ren%2520and%2520Zhangli%2520Sha%2520and%2520Zhe%2520Fu%2520and%2520Zhean%2520Xu%2520and%2520Zhen%2520Huang%2520and%2520Zhen%2520Zhang%2520and%2520Zhenda%2520Xie%2520and%2520Zhengyan%2520Zhang%2520and%2520Zhewen%2520Hao%2520and%2520Zhibin%2520Gou%2520and%2520Zhicheng%2520Ma%2520and%2520Zhigang%2520Yan%2520and%2520Zhihong%2520Shao%2520and%2520Zhipeng%2520Xu%2520and%2520Zhiyu%2520Wu%2520and%2520Zhongyu%2520Zhang%2520and%2520Zhuoshu%2520Li%2520and%2520Zihui%2520Gu%2520and%2520Zijia%2520Zhu%2520and%2520Zijun%2520Liu%2520and%2520Zilin%2520Li%2520and%2520Ziwei%2520Xie%2520and%2520Ziyang%2520Song%2520and%2520Ziyi%2520Gao%2520and%2520Zizheng%2520Pan%26entry.1292438233%3D%2520%2520We%2520present%2520DeepSeek-V3%252C%2520a%2520strong%2520Mixture-of-Experts%2520%2528MoE%2529%2520language%2520model%2520with%250A671B%2520total%2520parameters%2520with%252037B%2520activated%2520for%2520each%2520token.%2520To%2520achieve%2520efficient%250Ainference%2520and%2520cost-effective%2520training%252C%2520DeepSeek-V3%2520adopts%2520Multi-head%2520Latent%250AAttention%2520%2528MLA%2529%2520and%2520DeepSeekMoE%2520architectures%252C%2520which%2520were%2520thoroughly%2520validated%250Ain%2520DeepSeek-V2.%2520Furthermore%252C%2520DeepSeek-V3%2520pioneers%2520an%2520auxiliary-loss-free%250Astrategy%2520for%2520load%2520balancing%2520and%2520sets%2520a%2520multi-token%2520prediction%2520training%250Aobjective%2520for%2520stronger%2520performance.%2520We%2520pre-train%2520DeepSeek-V3%2520on%252014.8%2520trillion%250Adiverse%2520and%2520high-quality%2520tokens%252C%2520followed%2520by%2520Supervised%2520Fine-Tuning%2520and%250AReinforcement%2520Learning%2520stages%2520to%2520fully%2520harness%2520its%2520capabilities.%2520Comprehensive%250Aevaluations%2520reveal%2520that%2520DeepSeek-V3%2520outperforms%2520other%2520open-source%2520models%2520and%250Aachieves%2520performance%2520comparable%2520to%2520leading%2520closed-source%2520models.%2520Despite%2520its%250Aexcellent%2520performance%252C%2520DeepSeek-V3%2520requires%2520only%25202.788M%2520H800%2520GPU%2520hours%2520for%2520its%250Afull%2520training.%2520In%2520addition%252C%2520its%2520training%2520process%2520is%2520remarkably%2520stable.%250AThroughout%2520the%2520entire%2520training%2520process%252C%2520we%2520did%2520not%2520experience%2520any%2520irrecoverable%250Aloss%2520spikes%2520or%2520perform%2520any%2520rollbacks.%2520The%2520model%2520checkpoints%2520are%2520available%2520at%250Ahttps%253A//github.com/deepseek-ai/DeepSeek-V3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19437v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSeek-V3%20Technical%20Report&entry.906535625=%20DeepSeek-AI%20and%20Aixin%20Liu%20and%20Bei%20Feng%20and%20Bing%20Xue%20and%20Bingxuan%20Wang%20and%20Bochao%20Wu%20and%20Chengda%20Lu%20and%20Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Chenyu%20Zhang%20and%20Chong%20Ruan%20and%20Damai%20Dai%20and%20Daya%20Guo%20and%20Dejian%20Yang%20and%20Deli%20Chen%20and%20Dongjie%20Ji%20and%20Erhang%20Li%20and%20Fangyun%20Lin%20and%20Fucong%20Dai%20and%20Fuli%20Luo%20and%20Guangbo%20Hao%20and%20Guanting%20Chen%20and%20Guowei%20Li%20and%20H.%20Zhang%20and%20Han%20Bao%20and%20Hanwei%20Xu%20and%20Haocheng%20Wang%20and%20Haowei%20Zhang%20and%20Honghui%20Ding%20and%20Huajian%20Xin%20and%20Huazuo%20Gao%20and%20Hui%20Li%20and%20Hui%20Qu%20and%20J.%20L.%20Cai%20and%20Jian%20Liang%20and%20Jianzhong%20Guo%20and%20Jiaqi%20Ni%20and%20Jiashi%20Li%20and%20Jiawei%20Wang%20and%20Jin%20Chen%20and%20Jingchang%20Chen%20and%20Jingyang%20Yuan%20and%20Junjie%20Qiu%20and%20Junlong%20Li%20and%20Junxiao%20Song%20and%20Kai%20Dong%20and%20Kai%20Hu%20and%20Kaige%20Gao%20and%20Kang%20Guan%20and%20Kexin%20Huang%20and%20Kuai%20Yu%20and%20Lean%20Wang%20and%20Lecong%20Zhang%20and%20Lei%20Xu%20and%20Leyi%20Xia%20and%20Liang%20Zhao%20and%20Litong%20Wang%20and%20Liyue%20Zhang%20and%20Meng%20Li%20and%20Miaojun%20Wang%20and%20Mingchuan%20Zhang%20and%20Minghua%20Zhang%20and%20Minghui%20Tang%20and%20Mingming%20Li%20and%20Ning%20Tian%20and%20Panpan%20Huang%20and%20Peiyi%20Wang%20and%20Peng%20Zhang%20and%20Qiancheng%20Wang%20and%20Qihao%20Zhu%20and%20Qinyu%20Chen%20and%20Qiushi%20Du%20and%20R.%20J.%20Chen%20and%20R.%20L.%20Jin%20and%20Ruiqi%20Ge%20and%20Ruisong%20Zhang%20and%20Ruizhe%20Pan%20and%20Runji%20Wang%20and%20Runxin%20Xu%20and%20Ruoyu%20Zhang%20and%20Ruyi%20Chen%20and%20S.%20S.%20Li%20and%20Shanghao%20Lu%20and%20Shangyan%20Zhou%20and%20Shanhuang%20Chen%20and%20Shaoqing%20Wu%20and%20Shengfeng%20Ye%20and%20Shengfeng%20Ye%20and%20Shirong%20Ma%20and%20Shiyu%20Wang%20and%20Shuang%20Zhou%20and%20Shuiping%20Yu%20and%20Shunfeng%20Zhou%20and%20Shuting%20Pan%20and%20T.%20Wang%20and%20Tao%20Yun%20and%20Tian%20Pei%20and%20Tianyu%20Sun%20and%20W.%20L.%20Xiao%20and%20Wangding%20Zeng%20and%20Wanjia%20Zhao%20and%20Wei%20An%20and%20Wen%20Liu%20and%20Wenfeng%20Liang%20and%20Wenjun%20Gao%20and%20Wenqin%20Yu%20and%20Wentao%20Zhang%20and%20X.%20Q.%20Li%20and%20Xiangyue%20Jin%20and%20Xianzu%20Wang%20and%20Xiao%20Bi%20and%20Xiaodong%20Liu%20and%20Xiaohan%20Wang%20and%20Xiaojin%20Shen%20and%20Xiaokang%20Chen%20and%20Xiaokang%20Zhang%20and%20Xiaosha%20Chen%20and%20Xiaotao%20Nie%20and%20Xiaowen%20Sun%20and%20Xiaoxiang%20Wang%20and%20Xin%20Cheng%20and%20Xin%20Liu%20and%20Xin%20Xie%20and%20Xingchao%20Liu%20and%20Xingkai%20Yu%20and%20Xinnan%20Song%20and%20Xinxia%20Shan%20and%20Xinyi%20Zhou%20and%20Xinyu%20Yang%20and%20Xinyuan%20Li%20and%20Xuecheng%20Su%20and%20Xuheng%20Lin%20and%20Y.%20K.%20Li%20and%20Y.%20Q.%20Wang%20and%20Y.%20X.%20Wei%20and%20Y.%20X.%20Zhu%20and%20Yang%20Zhang%20and%20Yanhong%20Xu%20and%20Yanhong%20Xu%20and%20Yanping%20Huang%20and%20Yao%20Li%20and%20Yao%20Zhao%20and%20Yaofeng%20Sun%20and%20Yaohui%20Li%20and%20Yaohui%20Wang%20and%20Yi%20Yu%20and%20Yi%20Zheng%20and%20Yichao%20Zhang%20and%20Yifan%20Shi%20and%20Yiliang%20Xiong%20and%20Ying%20He%20and%20Ying%20Tang%20and%20Yishi%20Piao%20and%20Yisong%20Wang%20and%20Yixuan%20Tan%20and%20Yiyang%20Ma%20and%20Yiyuan%20Liu%20and%20Yongqiang%20Guo%20and%20Yu%20Wu%20and%20Yuan%20Ou%20and%20Yuchen%20Zhu%20and%20Yuduan%20Wang%20and%20Yue%20Gong%20and%20Yuheng%20Zou%20and%20Yujia%20He%20and%20Yukun%20Zha%20and%20Yunfan%20Xiong%20and%20Yunxian%20Ma%20and%20Yuting%20Yan%20and%20Yuxiang%20Luo%20and%20Yuxiang%20You%20and%20Yuxuan%20Liu%20and%20Yuyang%20Zhou%20and%20Z.%20F.%20Wu%20and%20Z.%20Z.%20Ren%20and%20Zehui%20Ren%20and%20Zhangli%20Sha%20and%20Zhe%20Fu%20and%20Zhean%20Xu%20and%20Zhen%20Huang%20and%20Zhen%20Zhang%20and%20Zhenda%20Xie%20and%20Zhengyan%20Zhang%20and%20Zhewen%20Hao%20and%20Zhibin%20Gou%20and%20Zhicheng%20Ma%20and%20Zhigang%20Yan%20and%20Zhihong%20Shao%20and%20Zhipeng%20Xu%20and%20Zhiyu%20Wu%20and%20Zhongyu%20Zhang%20and%20Zhuoshu%20Li%20and%20Zihui%20Gu%20and%20Zijia%20Zhu%20and%20Zijun%20Liu%20and%20Zilin%20Li%20and%20Ziwei%20Xie%20and%20Ziyang%20Song%20and%20Ziyi%20Gao%20and%20Zizheng%20Pan&entry.1292438233=%20%20We%20present%20DeepSeek-V3%2C%20a%20strong%20Mixture-of-Experts%20%28MoE%29%20language%20model%20with%0A671B%20total%20parameters%20with%2037B%20activated%20for%20each%20token.%20To%20achieve%20efficient%0Ainference%20and%20cost-effective%20training%2C%20DeepSeek-V3%20adopts%20Multi-head%20Latent%0AAttention%20%28MLA%29%20and%20DeepSeekMoE%20architectures%2C%20which%20were%20thoroughly%20validated%0Ain%20DeepSeek-V2.%20Furthermore%2C%20DeepSeek-V3%20pioneers%20an%20auxiliary-loss-free%0Astrategy%20for%20load%20balancing%20and%20sets%20a%20multi-token%20prediction%20training%0Aobjective%20for%20stronger%20performance.%20We%20pre-train%20DeepSeek-V3%20on%2014.8%20trillion%0Adiverse%20and%20high-quality%20tokens%2C%20followed%20by%20Supervised%20Fine-Tuning%20and%0AReinforcement%20Learning%20stages%20to%20fully%20harness%20its%20capabilities.%20Comprehensive%0Aevaluations%20reveal%20that%20DeepSeek-V3%20outperforms%20other%20open-source%20models%20and%0Aachieves%20performance%20comparable%20to%20leading%20closed-source%20models.%20Despite%20its%0Aexcellent%20performance%2C%20DeepSeek-V3%20requires%20only%202.788M%20H800%20GPU%20hours%20for%20its%0Afull%20training.%20In%20addition%2C%20its%20training%20process%20is%20remarkably%20stable.%0AThroughout%20the%20entire%20training%20process%2C%20we%20did%20not%20experience%20any%20irrecoverable%0Aloss%20spikes%20or%20perform%20any%20rollbacks.%20The%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/deepseek-ai/DeepSeek-V3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19437v2&entry.124074799=Read"},
{"title": "RAPID: Retrieval Augmented Training of Differentially Private Diffusion\n  Models", "author": "Tanqiu Jiang and Changjiang Li and Fenglong Ma and Ting Wang", "abstract": "  Differentially private diffusion models (DPDMs) harness the remarkable\ngenerative capabilities of diffusion models while enforcing differential\nprivacy (DP) for sensitive data. However, existing DPDM training approaches\noften suffer from significant utility loss, large memory footprint, and\nexpensive inference cost, impeding their practical uses. To overcome such\nlimitations, we present RAPID: Retrieval Augmented PrIvate Diffusion model, a\nnovel approach that integrates retrieval augmented generation (RAG) into DPDM\ntraining. Specifically, RAPID leverages available public data to build a\nknowledge base of sample trajectories; when training the diffusion model on\nprivate data, RAPID computes the early sampling steps as queries, retrieves\nsimilar trajectories from the knowledge base as surrogates, and focuses on\ntraining the later sampling steps in a differentially private manner. Extensive\nevaluation using benchmark datasets and models demonstrates that, with the same\nprivacy guarantee, RAPID significantly outperforms state-of-the-art approaches\nby large margins in generative quality, memory footprint, and inference cost,\nsuggesting that retrieval-augmented DP training represents a promising\ndirection for developing future privacy-preserving generative models. The code\nis available at: https://github.com/TanqiuJiang/RAPID\n", "link": "http://arxiv.org/abs/2502.12794v1", "date": "2025-02-18", "relevancy": 2.2115, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6022}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5444}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAPID%3A%20Retrieval%20Augmented%20Training%20of%20Differentially%20Private%20Diffusion%0A%20%20Models&body=Title%3A%20RAPID%3A%20Retrieval%20Augmented%20Training%20of%20Differentially%20Private%20Diffusion%0A%20%20Models%0AAuthor%3A%20Tanqiu%20Jiang%20and%20Changjiang%20Li%20and%20Fenglong%20Ma%20and%20Ting%20Wang%0AAbstract%3A%20%20%20Differentially%20private%20diffusion%20models%20%28DPDMs%29%20harness%20the%20remarkable%0Agenerative%20capabilities%20of%20diffusion%20models%20while%20enforcing%20differential%0Aprivacy%20%28DP%29%20for%20sensitive%20data.%20However%2C%20existing%20DPDM%20training%20approaches%0Aoften%20suffer%20from%20significant%20utility%20loss%2C%20large%20memory%20footprint%2C%20and%0Aexpensive%20inference%20cost%2C%20impeding%20their%20practical%20uses.%20To%20overcome%20such%0Alimitations%2C%20we%20present%20RAPID%3A%20Retrieval%20Augmented%20PrIvate%20Diffusion%20model%2C%20a%0Anovel%20approach%20that%20integrates%20retrieval%20augmented%20generation%20%28RAG%29%20into%20DPDM%0Atraining.%20Specifically%2C%20RAPID%20leverages%20available%20public%20data%20to%20build%20a%0Aknowledge%20base%20of%20sample%20trajectories%3B%20when%20training%20the%20diffusion%20model%20on%0Aprivate%20data%2C%20RAPID%20computes%20the%20early%20sampling%20steps%20as%20queries%2C%20retrieves%0Asimilar%20trajectories%20from%20the%20knowledge%20base%20as%20surrogates%2C%20and%20focuses%20on%0Atraining%20the%20later%20sampling%20steps%20in%20a%20differentially%20private%20manner.%20Extensive%0Aevaluation%20using%20benchmark%20datasets%20and%20models%20demonstrates%20that%2C%20with%20the%20same%0Aprivacy%20guarantee%2C%20RAPID%20significantly%20outperforms%20state-of-the-art%20approaches%0Aby%20large%20margins%20in%20generative%20quality%2C%20memory%20footprint%2C%20and%20inference%20cost%2C%0Asuggesting%20that%20retrieval-augmented%20DP%20training%20represents%20a%20promising%0Adirection%20for%20developing%20future%20privacy-preserving%20generative%20models.%20The%20code%0Ais%20available%20at%3A%20https%3A//github.com/TanqiuJiang/RAPID%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAPID%253A%2520Retrieval%2520Augmented%2520Training%2520of%2520Differentially%2520Private%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DTanqiu%2520Jiang%2520and%2520Changjiang%2520Li%2520and%2520Fenglong%2520Ma%2520and%2520Ting%2520Wang%26entry.1292438233%3D%2520%2520Differentially%2520private%2520diffusion%2520models%2520%2528DPDMs%2529%2520harness%2520the%2520remarkable%250Agenerative%2520capabilities%2520of%2520diffusion%2520models%2520while%2520enforcing%2520differential%250Aprivacy%2520%2528DP%2529%2520for%2520sensitive%2520data.%2520However%252C%2520existing%2520DPDM%2520training%2520approaches%250Aoften%2520suffer%2520from%2520significant%2520utility%2520loss%252C%2520large%2520memory%2520footprint%252C%2520and%250Aexpensive%2520inference%2520cost%252C%2520impeding%2520their%2520practical%2520uses.%2520To%2520overcome%2520such%250Alimitations%252C%2520we%2520present%2520RAPID%253A%2520Retrieval%2520Augmented%2520PrIvate%2520Diffusion%2520model%252C%2520a%250Anovel%2520approach%2520that%2520integrates%2520retrieval%2520augmented%2520generation%2520%2528RAG%2529%2520into%2520DPDM%250Atraining.%2520Specifically%252C%2520RAPID%2520leverages%2520available%2520public%2520data%2520to%2520build%2520a%250Aknowledge%2520base%2520of%2520sample%2520trajectories%253B%2520when%2520training%2520the%2520diffusion%2520model%2520on%250Aprivate%2520data%252C%2520RAPID%2520computes%2520the%2520early%2520sampling%2520steps%2520as%2520queries%252C%2520retrieves%250Asimilar%2520trajectories%2520from%2520the%2520knowledge%2520base%2520as%2520surrogates%252C%2520and%2520focuses%2520on%250Atraining%2520the%2520later%2520sampling%2520steps%2520in%2520a%2520differentially%2520private%2520manner.%2520Extensive%250Aevaluation%2520using%2520benchmark%2520datasets%2520and%2520models%2520demonstrates%2520that%252C%2520with%2520the%2520same%250Aprivacy%2520guarantee%252C%2520RAPID%2520significantly%2520outperforms%2520state-of-the-art%2520approaches%250Aby%2520large%2520margins%2520in%2520generative%2520quality%252C%2520memory%2520footprint%252C%2520and%2520inference%2520cost%252C%250Asuggesting%2520that%2520retrieval-augmented%2520DP%2520training%2520represents%2520a%2520promising%250Adirection%2520for%2520developing%2520future%2520privacy-preserving%2520generative%2520models.%2520The%2520code%250Ais%2520available%2520at%253A%2520https%253A//github.com/TanqiuJiang/RAPID%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAPID%3A%20Retrieval%20Augmented%20Training%20of%20Differentially%20Private%20Diffusion%0A%20%20Models&entry.906535625=Tanqiu%20Jiang%20and%20Changjiang%20Li%20and%20Fenglong%20Ma%20and%20Ting%20Wang&entry.1292438233=%20%20Differentially%20private%20diffusion%20models%20%28DPDMs%29%20harness%20the%20remarkable%0Agenerative%20capabilities%20of%20diffusion%20models%20while%20enforcing%20differential%0Aprivacy%20%28DP%29%20for%20sensitive%20data.%20However%2C%20existing%20DPDM%20training%20approaches%0Aoften%20suffer%20from%20significant%20utility%20loss%2C%20large%20memory%20footprint%2C%20and%0Aexpensive%20inference%20cost%2C%20impeding%20their%20practical%20uses.%20To%20overcome%20such%0Alimitations%2C%20we%20present%20RAPID%3A%20Retrieval%20Augmented%20PrIvate%20Diffusion%20model%2C%20a%0Anovel%20approach%20that%20integrates%20retrieval%20augmented%20generation%20%28RAG%29%20into%20DPDM%0Atraining.%20Specifically%2C%20RAPID%20leverages%20available%20public%20data%20to%20build%20a%0Aknowledge%20base%20of%20sample%20trajectories%3B%20when%20training%20the%20diffusion%20model%20on%0Aprivate%20data%2C%20RAPID%20computes%20the%20early%20sampling%20steps%20as%20queries%2C%20retrieves%0Asimilar%20trajectories%20from%20the%20knowledge%20base%20as%20surrogates%2C%20and%20focuses%20on%0Atraining%20the%20later%20sampling%20steps%20in%20a%20differentially%20private%20manner.%20Extensive%0Aevaluation%20using%20benchmark%20datasets%20and%20models%20demonstrates%20that%2C%20with%20the%20same%0Aprivacy%20guarantee%2C%20RAPID%20significantly%20outperforms%20state-of-the-art%20approaches%0Aby%20large%20margins%20in%20generative%20quality%2C%20memory%20footprint%2C%20and%20inference%20cost%2C%0Asuggesting%20that%20retrieval-augmented%20DP%20training%20represents%20a%20promising%0Adirection%20for%20developing%20future%20privacy-preserving%20generative%20models.%20The%20code%0Ais%20available%20at%3A%20https%3A//github.com/TanqiuJiang/RAPID%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12794v1&entry.124074799=Read"},
{"title": "Learning Tree Pattern Transformations", "author": "Daniel Neider and Leif Sabellek and Johannes Schmidt and Fabian Vehlken and Thomas Zeume", "abstract": "  Explaining why and how a tree $t$ structurally differs from another tree\n$t^\\star$ is a question that is encountered throughout computer science,\nincluding in understanding tree-structured data such as XML or JSON data. In\nthis article, we explore how to learn explanations for structural differences\nbetween pairs of trees from sample data: suppose we are given a set $\\{(t_1,\nt_1^\\star),\\dots, (t_n, t_n^\\star)\\}$ of pairs of labelled, ordered trees; is\nthere a small set of rules that explains the structural differences between all\npairs $(t_i, t_i^\\star)$? This raises two research questions: (i) what is a\ngood notion of \"rule\" in this context?; and (ii) how can sets of rules\nexplaining a data set be learned algorithmically?\n  We explore these questions from the perspective of database theory by (1)\nintroducing a pattern-based specification language for tree transformations;\n(2) exploring the computational complexity of variants of the above algorithmic\nproblem, e.g. showing NP-hardness for very restricted variants; and (3)\ndiscussing how to solve the problem for data from CS education research using\nSAT solvers.\n", "link": "http://arxiv.org/abs/2410.07708v2", "date": "2025-02-18", "relevancy": 2.2071, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.431}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Tree%20Pattern%20Transformations&body=Title%3A%20Learning%20Tree%20Pattern%20Transformations%0AAuthor%3A%20Daniel%20Neider%20and%20Leif%20Sabellek%20and%20Johannes%20Schmidt%20and%20Fabian%20Vehlken%20and%20Thomas%20Zeume%0AAbstract%3A%20%20%20Explaining%20why%20and%20how%20a%20tree%20%24t%24%20structurally%20differs%20from%20another%20tree%0A%24t%5E%5Cstar%24%20is%20a%20question%20that%20is%20encountered%20throughout%20computer%20science%2C%0Aincluding%20in%20understanding%20tree-structured%20data%20such%20as%20XML%20or%20JSON%20data.%20In%0Athis%20article%2C%20we%20explore%20how%20to%20learn%20explanations%20for%20structural%20differences%0Abetween%20pairs%20of%20trees%20from%20sample%20data%3A%20suppose%20we%20are%20given%20a%20set%20%24%5C%7B%28t_1%2C%0At_1%5E%5Cstar%29%2C%5Cdots%2C%20%28t_n%2C%20t_n%5E%5Cstar%29%5C%7D%24%20of%20pairs%20of%20labelled%2C%20ordered%20trees%3B%20is%0Athere%20a%20small%20set%20of%20rules%20that%20explains%20the%20structural%20differences%20between%20all%0Apairs%20%24%28t_i%2C%20t_i%5E%5Cstar%29%24%3F%20This%20raises%20two%20research%20questions%3A%20%28i%29%20what%20is%20a%0Agood%20notion%20of%20%22rule%22%20in%20this%20context%3F%3B%20and%20%28ii%29%20how%20can%20sets%20of%20rules%0Aexplaining%20a%20data%20set%20be%20learned%20algorithmically%3F%0A%20%20We%20explore%20these%20questions%20from%20the%20perspective%20of%20database%20theory%20by%20%281%29%0Aintroducing%20a%20pattern-based%20specification%20language%20for%20tree%20transformations%3B%0A%282%29%20exploring%20the%20computational%20complexity%20of%20variants%20of%20the%20above%20algorithmic%0Aproblem%2C%20e.g.%20showing%20NP-hardness%20for%20very%20restricted%20variants%3B%20and%20%283%29%0Adiscussing%20how%20to%20solve%20the%20problem%20for%20data%20from%20CS%20education%20research%20using%0ASAT%20solvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Tree%2520Pattern%2520Transformations%26entry.906535625%3DDaniel%2520Neider%2520and%2520Leif%2520Sabellek%2520and%2520Johannes%2520Schmidt%2520and%2520Fabian%2520Vehlken%2520and%2520Thomas%2520Zeume%26entry.1292438233%3D%2520%2520Explaining%2520why%2520and%2520how%2520a%2520tree%2520%2524t%2524%2520structurally%2520differs%2520from%2520another%2520tree%250A%2524t%255E%255Cstar%2524%2520is%2520a%2520question%2520that%2520is%2520encountered%2520throughout%2520computer%2520science%252C%250Aincluding%2520in%2520understanding%2520tree-structured%2520data%2520such%2520as%2520XML%2520or%2520JSON%2520data.%2520In%250Athis%2520article%252C%2520we%2520explore%2520how%2520to%2520learn%2520explanations%2520for%2520structural%2520differences%250Abetween%2520pairs%2520of%2520trees%2520from%2520sample%2520data%253A%2520suppose%2520we%2520are%2520given%2520a%2520set%2520%2524%255C%257B%2528t_1%252C%250At_1%255E%255Cstar%2529%252C%255Cdots%252C%2520%2528t_n%252C%2520t_n%255E%255Cstar%2529%255C%257D%2524%2520of%2520pairs%2520of%2520labelled%252C%2520ordered%2520trees%253B%2520is%250Athere%2520a%2520small%2520set%2520of%2520rules%2520that%2520explains%2520the%2520structural%2520differences%2520between%2520all%250Apairs%2520%2524%2528t_i%252C%2520t_i%255E%255Cstar%2529%2524%253F%2520This%2520raises%2520two%2520research%2520questions%253A%2520%2528i%2529%2520what%2520is%2520a%250Agood%2520notion%2520of%2520%2522rule%2522%2520in%2520this%2520context%253F%253B%2520and%2520%2528ii%2529%2520how%2520can%2520sets%2520of%2520rules%250Aexplaining%2520a%2520data%2520set%2520be%2520learned%2520algorithmically%253F%250A%2520%2520We%2520explore%2520these%2520questions%2520from%2520the%2520perspective%2520of%2520database%2520theory%2520by%2520%25281%2529%250Aintroducing%2520a%2520pattern-based%2520specification%2520language%2520for%2520tree%2520transformations%253B%250A%25282%2529%2520exploring%2520the%2520computational%2520complexity%2520of%2520variants%2520of%2520the%2520above%2520algorithmic%250Aproblem%252C%2520e.g.%2520showing%2520NP-hardness%2520for%2520very%2520restricted%2520variants%253B%2520and%2520%25283%2529%250Adiscussing%2520how%2520to%2520solve%2520the%2520problem%2520for%2520data%2520from%2520CS%2520education%2520research%2520using%250ASAT%2520solvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Tree%20Pattern%20Transformations&entry.906535625=Daniel%20Neider%20and%20Leif%20Sabellek%20and%20Johannes%20Schmidt%20and%20Fabian%20Vehlken%20and%20Thomas%20Zeume&entry.1292438233=%20%20Explaining%20why%20and%20how%20a%20tree%20%24t%24%20structurally%20differs%20from%20another%20tree%0A%24t%5E%5Cstar%24%20is%20a%20question%20that%20is%20encountered%20throughout%20computer%20science%2C%0Aincluding%20in%20understanding%20tree-structured%20data%20such%20as%20XML%20or%20JSON%20data.%20In%0Athis%20article%2C%20we%20explore%20how%20to%20learn%20explanations%20for%20structural%20differences%0Abetween%20pairs%20of%20trees%20from%20sample%20data%3A%20suppose%20we%20are%20given%20a%20set%20%24%5C%7B%28t_1%2C%0At_1%5E%5Cstar%29%2C%5Cdots%2C%20%28t_n%2C%20t_n%5E%5Cstar%29%5C%7D%24%20of%20pairs%20of%20labelled%2C%20ordered%20trees%3B%20is%0Athere%20a%20small%20set%20of%20rules%20that%20explains%20the%20structural%20differences%20between%20all%0Apairs%20%24%28t_i%2C%20t_i%5E%5Cstar%29%24%3F%20This%20raises%20two%20research%20questions%3A%20%28i%29%20what%20is%20a%0Agood%20notion%20of%20%22rule%22%20in%20this%20context%3F%3B%20and%20%28ii%29%20how%20can%20sets%20of%20rules%0Aexplaining%20a%20data%20set%20be%20learned%20algorithmically%3F%0A%20%20We%20explore%20these%20questions%20from%20the%20perspective%20of%20database%20theory%20by%20%281%29%0Aintroducing%20a%20pattern-based%20specification%20language%20for%20tree%20transformations%3B%0A%282%29%20exploring%20the%20computational%20complexity%20of%20variants%20of%20the%20above%20algorithmic%0Aproblem%2C%20e.g.%20showing%20NP-hardness%20for%20very%20restricted%20variants%3B%20and%20%283%29%0Adiscussing%20how%20to%20solve%20the%20problem%20for%20data%20from%20CS%20education%20research%20using%0ASAT%20solvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07708v2&entry.124074799=Read"},
{"title": "HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for\n  Multi-session Radar SLAM", "author": "Hanjun Kim and Minwoo Jung and Chiyun Noh and Sangwoo Jung and Hyunho Song and Wooseong Yang and Hyesu Jang and Ayoung Kim", "abstract": "  Recently, radars have been widely featured in robotics for their robustness\nin challenging weather conditions. Two commonly used radar types are spinning\nradars and phased-array radars, each offering distinct sensor characteristics.\nExisting datasets typically feature only a single type of radar, leading to the\ndevelopment of algorithms limited to that specific kind. In this work, we\nhighlight that combining different radar types offers complementary advantages,\nwhich can be leveraged through a heterogeneous radar dataset. Moreover, this\nnew dataset fosters research in multi-session and multi-robot scenarios where\nrobots are equipped with different types of radars. In this context, we\nintroduce the HeRCULES dataset, a comprehensive, multi-modal dataset with\nheterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first\ndataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering\nunparalleled localization, mapping, and place recognition capabilities. The\ndataset covers diverse weather and lighting conditions and a range of urban\ntraffic scenarios, enabling a comprehensive analysis across various\nenvironments. The sequence paths with multiple revisits and ground truth pose\nfor each sensor enhance its suitability for place recognition research. We\nexpect the HeRCULES dataset to facilitate odometry, mapping, place recognition,\nand sensor fusion research. The dataset and development tools are available at\nhttps://sites.google.com/view/herculesdataset.\n", "link": "http://arxiv.org/abs/2502.01946v2", "date": "2025-02-18", "relevancy": 2.1933, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5508}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeRCULES%3A%20Heterogeneous%20Radar%20Dataset%20in%20Complex%20Urban%20Environment%20for%0A%20%20Multi-session%20Radar%20SLAM&body=Title%3A%20HeRCULES%3A%20Heterogeneous%20Radar%20Dataset%20in%20Complex%20Urban%20Environment%20for%0A%20%20Multi-session%20Radar%20SLAM%0AAuthor%3A%20Hanjun%20Kim%20and%20Minwoo%20Jung%20and%20Chiyun%20Noh%20and%20Sangwoo%20Jung%20and%20Hyunho%20Song%20and%20Wooseong%20Yang%20and%20Hyesu%20Jang%20and%20Ayoung%20Kim%0AAbstract%3A%20%20%20Recently%2C%20radars%20have%20been%20widely%20featured%20in%20robotics%20for%20their%20robustness%0Ain%20challenging%20weather%20conditions.%20Two%20commonly%20used%20radar%20types%20are%20spinning%0Aradars%20and%20phased-array%20radars%2C%20each%20offering%20distinct%20sensor%20characteristics.%0AExisting%20datasets%20typically%20feature%20only%20a%20single%20type%20of%20radar%2C%20leading%20to%20the%0Adevelopment%20of%20algorithms%20limited%20to%20that%20specific%20kind.%20In%20this%20work%2C%20we%0Ahighlight%20that%20combining%20different%20radar%20types%20offers%20complementary%20advantages%2C%0Awhich%20can%20be%20leveraged%20through%20a%20heterogeneous%20radar%20dataset.%20Moreover%2C%20this%0Anew%20dataset%20fosters%20research%20in%20multi-session%20and%20multi-robot%20scenarios%20where%0Arobots%20are%20equipped%20with%20different%20types%20of%20radars.%20In%20this%20context%2C%20we%0Aintroduce%20the%20HeRCULES%20dataset%2C%20a%20comprehensive%2C%20multi-modal%20dataset%20with%0Aheterogeneous%20radars%2C%20FMCW%20LiDAR%2C%20IMU%2C%20GPS%2C%20and%20cameras.%20This%20is%20the%20first%0Adataset%20to%20integrate%204D%20radar%20and%20spinning%20radar%20alongside%20FMCW%20LiDAR%2C%20offering%0Aunparalleled%20localization%2C%20mapping%2C%20and%20place%20recognition%20capabilities.%20The%0Adataset%20covers%20diverse%20weather%20and%20lighting%20conditions%20and%20a%20range%20of%20urban%0Atraffic%20scenarios%2C%20enabling%20a%20comprehensive%20analysis%20across%20various%0Aenvironments.%20The%20sequence%20paths%20with%20multiple%20revisits%20and%20ground%20truth%20pose%0Afor%20each%20sensor%20enhance%20its%20suitability%20for%20place%20recognition%20research.%20We%0Aexpect%20the%20HeRCULES%20dataset%20to%20facilitate%20odometry%2C%20mapping%2C%20place%20recognition%2C%0Aand%20sensor%20fusion%20research.%20The%20dataset%20and%20development%20tools%20are%20available%20at%0Ahttps%3A//sites.google.com/view/herculesdataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01946v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeRCULES%253A%2520Heterogeneous%2520Radar%2520Dataset%2520in%2520Complex%2520Urban%2520Environment%2520for%250A%2520%2520Multi-session%2520Radar%2520SLAM%26entry.906535625%3DHanjun%2520Kim%2520and%2520Minwoo%2520Jung%2520and%2520Chiyun%2520Noh%2520and%2520Sangwoo%2520Jung%2520and%2520Hyunho%2520Song%2520and%2520Wooseong%2520Yang%2520and%2520Hyesu%2520Jang%2520and%2520Ayoung%2520Kim%26entry.1292438233%3D%2520%2520Recently%252C%2520radars%2520have%2520been%2520widely%2520featured%2520in%2520robotics%2520for%2520their%2520robustness%250Ain%2520challenging%2520weather%2520conditions.%2520Two%2520commonly%2520used%2520radar%2520types%2520are%2520spinning%250Aradars%2520and%2520phased-array%2520radars%252C%2520each%2520offering%2520distinct%2520sensor%2520characteristics.%250AExisting%2520datasets%2520typically%2520feature%2520only%2520a%2520single%2520type%2520of%2520radar%252C%2520leading%2520to%2520the%250Adevelopment%2520of%2520algorithms%2520limited%2520to%2520that%2520specific%2520kind.%2520In%2520this%2520work%252C%2520we%250Ahighlight%2520that%2520combining%2520different%2520radar%2520types%2520offers%2520complementary%2520advantages%252C%250Awhich%2520can%2520be%2520leveraged%2520through%2520a%2520heterogeneous%2520radar%2520dataset.%2520Moreover%252C%2520this%250Anew%2520dataset%2520fosters%2520research%2520in%2520multi-session%2520and%2520multi-robot%2520scenarios%2520where%250Arobots%2520are%2520equipped%2520with%2520different%2520types%2520of%2520radars.%2520In%2520this%2520context%252C%2520we%250Aintroduce%2520the%2520HeRCULES%2520dataset%252C%2520a%2520comprehensive%252C%2520multi-modal%2520dataset%2520with%250Aheterogeneous%2520radars%252C%2520FMCW%2520LiDAR%252C%2520IMU%252C%2520GPS%252C%2520and%2520cameras.%2520This%2520is%2520the%2520first%250Adataset%2520to%2520integrate%25204D%2520radar%2520and%2520spinning%2520radar%2520alongside%2520FMCW%2520LiDAR%252C%2520offering%250Aunparalleled%2520localization%252C%2520mapping%252C%2520and%2520place%2520recognition%2520capabilities.%2520The%250Adataset%2520covers%2520diverse%2520weather%2520and%2520lighting%2520conditions%2520and%2520a%2520range%2520of%2520urban%250Atraffic%2520scenarios%252C%2520enabling%2520a%2520comprehensive%2520analysis%2520across%2520various%250Aenvironments.%2520The%2520sequence%2520paths%2520with%2520multiple%2520revisits%2520and%2520ground%2520truth%2520pose%250Afor%2520each%2520sensor%2520enhance%2520its%2520suitability%2520for%2520place%2520recognition%2520research.%2520We%250Aexpect%2520the%2520HeRCULES%2520dataset%2520to%2520facilitate%2520odometry%252C%2520mapping%252C%2520place%2520recognition%252C%250Aand%2520sensor%2520fusion%2520research.%2520The%2520dataset%2520and%2520development%2520tools%2520are%2520available%2520at%250Ahttps%253A//sites.google.com/view/herculesdataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01946v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeRCULES%3A%20Heterogeneous%20Radar%20Dataset%20in%20Complex%20Urban%20Environment%20for%0A%20%20Multi-session%20Radar%20SLAM&entry.906535625=Hanjun%20Kim%20and%20Minwoo%20Jung%20and%20Chiyun%20Noh%20and%20Sangwoo%20Jung%20and%20Hyunho%20Song%20and%20Wooseong%20Yang%20and%20Hyesu%20Jang%20and%20Ayoung%20Kim&entry.1292438233=%20%20Recently%2C%20radars%20have%20been%20widely%20featured%20in%20robotics%20for%20their%20robustness%0Ain%20challenging%20weather%20conditions.%20Two%20commonly%20used%20radar%20types%20are%20spinning%0Aradars%20and%20phased-array%20radars%2C%20each%20offering%20distinct%20sensor%20characteristics.%0AExisting%20datasets%20typically%20feature%20only%20a%20single%20type%20of%20radar%2C%20leading%20to%20the%0Adevelopment%20of%20algorithms%20limited%20to%20that%20specific%20kind.%20In%20this%20work%2C%20we%0Ahighlight%20that%20combining%20different%20radar%20types%20offers%20complementary%20advantages%2C%0Awhich%20can%20be%20leveraged%20through%20a%20heterogeneous%20radar%20dataset.%20Moreover%2C%20this%0Anew%20dataset%20fosters%20research%20in%20multi-session%20and%20multi-robot%20scenarios%20where%0Arobots%20are%20equipped%20with%20different%20types%20of%20radars.%20In%20this%20context%2C%20we%0Aintroduce%20the%20HeRCULES%20dataset%2C%20a%20comprehensive%2C%20multi-modal%20dataset%20with%0Aheterogeneous%20radars%2C%20FMCW%20LiDAR%2C%20IMU%2C%20GPS%2C%20and%20cameras.%20This%20is%20the%20first%0Adataset%20to%20integrate%204D%20radar%20and%20spinning%20radar%20alongside%20FMCW%20LiDAR%2C%20offering%0Aunparalleled%20localization%2C%20mapping%2C%20and%20place%20recognition%20capabilities.%20The%0Adataset%20covers%20diverse%20weather%20and%20lighting%20conditions%20and%20a%20range%20of%20urban%0Atraffic%20scenarios%2C%20enabling%20a%20comprehensive%20analysis%20across%20various%0Aenvironments.%20The%20sequence%20paths%20with%20multiple%20revisits%20and%20ground%20truth%20pose%0Afor%20each%20sensor%20enhance%20its%20suitability%20for%20place%20recognition%20research.%20We%0Aexpect%20the%20HeRCULES%20dataset%20to%20facilitate%20odometry%2C%20mapping%2C%20place%20recognition%2C%0Aand%20sensor%20fusion%20research.%20The%20dataset%20and%20development%20tools%20are%20available%20at%0Ahttps%3A//sites.google.com/view/herculesdataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01946v2&entry.124074799=Read"},
{"title": "Exploring the Impact of Dataset Statistical Effect Size on Model\n  Performance and Data Sample Size Sufficiency", "author": "Arya Hatamian and Lionel Levine and Haniyeh Ehsani Oskouie and Majid Sarrafzadeh", "abstract": "  Having a sufficient quantity of quality data is a critical enabler of\ntraining effective machine learning models. Being able to effectively determine\nthe adequacy of a dataset prior to training and evaluating a model's\nperformance would be an essential tool for anyone engaged in experimental\ndesign or data collection. However, despite the need for it, the ability to\nprospectively assess data sufficiency remains an elusive capability. We report\nhere on two experiments undertaken in an attempt to better ascertain whether or\nnot basic descriptive statistical measures can be indicative of how effective a\ndataset will be at training a resulting model. Leveraging the effect size of\nour features, this work first explores whether or not a correlation exists\nbetween effect size, and resulting model performance (theorizing that the\nmagnitude of the distinction between classes could correlate to a classifier's\nresulting success). We then explore whether or not the magnitude of the effect\nsize will impact the rate of convergence of our learning rate, (theorizing\nagain that a greater effect size may indicate that the model will converge more\nrapidly, and with a smaller sample size needed). Our results appear to indicate\nthat this is not an effective heuristic for determining adequate sample size or\nprojecting model performance, and therefore that additional work is still\nneeded to better prospectively assess adequacy of data.\n", "link": "http://arxiv.org/abs/2501.02673v2", "date": "2025-02-18", "relevancy": 2.1835, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4378}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Impact%20of%20Dataset%20Statistical%20Effect%20Size%20on%20Model%0A%20%20Performance%20and%20Data%20Sample%20Size%20Sufficiency&body=Title%3A%20Exploring%20the%20Impact%20of%20Dataset%20Statistical%20Effect%20Size%20on%20Model%0A%20%20Performance%20and%20Data%20Sample%20Size%20Sufficiency%0AAuthor%3A%20Arya%20Hatamian%20and%20Lionel%20Levine%20and%20Haniyeh%20Ehsani%20Oskouie%20and%20Majid%20Sarrafzadeh%0AAbstract%3A%20%20%20Having%20a%20sufficient%20quantity%20of%20quality%20data%20is%20a%20critical%20enabler%20of%0Atraining%20effective%20machine%20learning%20models.%20Being%20able%20to%20effectively%20determine%0Athe%20adequacy%20of%20a%20dataset%20prior%20to%20training%20and%20evaluating%20a%20model%27s%0Aperformance%20would%20be%20an%20essential%20tool%20for%20anyone%20engaged%20in%20experimental%0Adesign%20or%20data%20collection.%20However%2C%20despite%20the%20need%20for%20it%2C%20the%20ability%20to%0Aprospectively%20assess%20data%20sufficiency%20remains%20an%20elusive%20capability.%20We%20report%0Ahere%20on%20two%20experiments%20undertaken%20in%20an%20attempt%20to%20better%20ascertain%20whether%20or%0Anot%20basic%20descriptive%20statistical%20measures%20can%20be%20indicative%20of%20how%20effective%20a%0Adataset%20will%20be%20at%20training%20a%20resulting%20model.%20Leveraging%20the%20effect%20size%20of%0Aour%20features%2C%20this%20work%20first%20explores%20whether%20or%20not%20a%20correlation%20exists%0Abetween%20effect%20size%2C%20and%20resulting%20model%20performance%20%28theorizing%20that%20the%0Amagnitude%20of%20the%20distinction%20between%20classes%20could%20correlate%20to%20a%20classifier%27s%0Aresulting%20success%29.%20We%20then%20explore%20whether%20or%20not%20the%20magnitude%20of%20the%20effect%0Asize%20will%20impact%20the%20rate%20of%20convergence%20of%20our%20learning%20rate%2C%20%28theorizing%0Aagain%20that%20a%20greater%20effect%20size%20may%20indicate%20that%20the%20model%20will%20converge%20more%0Arapidly%2C%20and%20with%20a%20smaller%20sample%20size%20needed%29.%20Our%20results%20appear%20to%20indicate%0Athat%20this%20is%20not%20an%20effective%20heuristic%20for%20determining%20adequate%20sample%20size%20or%0Aprojecting%20model%20performance%2C%20and%20therefore%20that%20additional%20work%20is%20still%0Aneeded%20to%20better%20prospectively%20assess%20adequacy%20of%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Impact%2520of%2520Dataset%2520Statistical%2520Effect%2520Size%2520on%2520Model%250A%2520%2520Performance%2520and%2520Data%2520Sample%2520Size%2520Sufficiency%26entry.906535625%3DArya%2520Hatamian%2520and%2520Lionel%2520Levine%2520and%2520Haniyeh%2520Ehsani%2520Oskouie%2520and%2520Majid%2520Sarrafzadeh%26entry.1292438233%3D%2520%2520Having%2520a%2520sufficient%2520quantity%2520of%2520quality%2520data%2520is%2520a%2520critical%2520enabler%2520of%250Atraining%2520effective%2520machine%2520learning%2520models.%2520Being%2520able%2520to%2520effectively%2520determine%250Athe%2520adequacy%2520of%2520a%2520dataset%2520prior%2520to%2520training%2520and%2520evaluating%2520a%2520model%2527s%250Aperformance%2520would%2520be%2520an%2520essential%2520tool%2520for%2520anyone%2520engaged%2520in%2520experimental%250Adesign%2520or%2520data%2520collection.%2520However%252C%2520despite%2520the%2520need%2520for%2520it%252C%2520the%2520ability%2520to%250Aprospectively%2520assess%2520data%2520sufficiency%2520remains%2520an%2520elusive%2520capability.%2520We%2520report%250Ahere%2520on%2520two%2520experiments%2520undertaken%2520in%2520an%2520attempt%2520to%2520better%2520ascertain%2520whether%2520or%250Anot%2520basic%2520descriptive%2520statistical%2520measures%2520can%2520be%2520indicative%2520of%2520how%2520effective%2520a%250Adataset%2520will%2520be%2520at%2520training%2520a%2520resulting%2520model.%2520Leveraging%2520the%2520effect%2520size%2520of%250Aour%2520features%252C%2520this%2520work%2520first%2520explores%2520whether%2520or%2520not%2520a%2520correlation%2520exists%250Abetween%2520effect%2520size%252C%2520and%2520resulting%2520model%2520performance%2520%2528theorizing%2520that%2520the%250Amagnitude%2520of%2520the%2520distinction%2520between%2520classes%2520could%2520correlate%2520to%2520a%2520classifier%2527s%250Aresulting%2520success%2529.%2520We%2520then%2520explore%2520whether%2520or%2520not%2520the%2520magnitude%2520of%2520the%2520effect%250Asize%2520will%2520impact%2520the%2520rate%2520of%2520convergence%2520of%2520our%2520learning%2520rate%252C%2520%2528theorizing%250Aagain%2520that%2520a%2520greater%2520effect%2520size%2520may%2520indicate%2520that%2520the%2520model%2520will%2520converge%2520more%250Arapidly%252C%2520and%2520with%2520a%2520smaller%2520sample%2520size%2520needed%2529.%2520Our%2520results%2520appear%2520to%2520indicate%250Athat%2520this%2520is%2520not%2520an%2520effective%2520heuristic%2520for%2520determining%2520adequate%2520sample%2520size%2520or%250Aprojecting%2520model%2520performance%252C%2520and%2520therefore%2520that%2520additional%2520work%2520is%2520still%250Aneeded%2520to%2520better%2520prospectively%2520assess%2520adequacy%2520of%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Impact%20of%20Dataset%20Statistical%20Effect%20Size%20on%20Model%0A%20%20Performance%20and%20Data%20Sample%20Size%20Sufficiency&entry.906535625=Arya%20Hatamian%20and%20Lionel%20Levine%20and%20Haniyeh%20Ehsani%20Oskouie%20and%20Majid%20Sarrafzadeh&entry.1292438233=%20%20Having%20a%20sufficient%20quantity%20of%20quality%20data%20is%20a%20critical%20enabler%20of%0Atraining%20effective%20machine%20learning%20models.%20Being%20able%20to%20effectively%20determine%0Athe%20adequacy%20of%20a%20dataset%20prior%20to%20training%20and%20evaluating%20a%20model%27s%0Aperformance%20would%20be%20an%20essential%20tool%20for%20anyone%20engaged%20in%20experimental%0Adesign%20or%20data%20collection.%20However%2C%20despite%20the%20need%20for%20it%2C%20the%20ability%20to%0Aprospectively%20assess%20data%20sufficiency%20remains%20an%20elusive%20capability.%20We%20report%0Ahere%20on%20two%20experiments%20undertaken%20in%20an%20attempt%20to%20better%20ascertain%20whether%20or%0Anot%20basic%20descriptive%20statistical%20measures%20can%20be%20indicative%20of%20how%20effective%20a%0Adataset%20will%20be%20at%20training%20a%20resulting%20model.%20Leveraging%20the%20effect%20size%20of%0Aour%20features%2C%20this%20work%20first%20explores%20whether%20or%20not%20a%20correlation%20exists%0Abetween%20effect%20size%2C%20and%20resulting%20model%20performance%20%28theorizing%20that%20the%0Amagnitude%20of%20the%20distinction%20between%20classes%20could%20correlate%20to%20a%20classifier%27s%0Aresulting%20success%29.%20We%20then%20explore%20whether%20or%20not%20the%20magnitude%20of%20the%20effect%0Asize%20will%20impact%20the%20rate%20of%20convergence%20of%20our%20learning%20rate%2C%20%28theorizing%0Aagain%20that%20a%20greater%20effect%20size%20may%20indicate%20that%20the%20model%20will%20converge%20more%0Arapidly%2C%20and%20with%20a%20smaller%20sample%20size%20needed%29.%20Our%20results%20appear%20to%20indicate%0Athat%20this%20is%20not%20an%20effective%20heuristic%20for%20determining%20adequate%20sample%20size%20or%0Aprojecting%20model%20performance%2C%20and%20therefore%20that%20additional%20work%20is%20still%0Aneeded%20to%20better%20prospectively%20assess%20adequacy%20of%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02673v2&entry.124074799=Read"},
{"title": "SHADeS: Self-supervised Monocular Depth Estimation Through\n  Non-Lambertian Image Decomposition", "author": "Rema Daher and Francisco Vasconcelos and Danail Stoyanov", "abstract": "  Purpose: Visual 3D scene reconstruction can support colonoscopy navigation.\nIt can help in recognising which portions of the colon have been visualised and\ncharacterising the size and shape of polyps. This is still a very challenging\nproblem due to complex illumination variations, including abundant specular\nreflections. We investigate how to effectively decouple light and depth in this\nproblem.\n  Methods: We introduce a self-supervised model that simultaneously\ncharacterises the shape and lighting of the visualised colonoscopy scene. Our\nmodel estimates shading, albedo, depth, and specularities (SHADeS) from single\nimages. Unlike previous approaches (IID), we use a non-Lambertian model that\ntreats specular reflections as a separate light component. The implementation\nof our method is available at https://github.com/RemaDaher/SHADeS.\n  Results: We demonstrate on real colonoscopy images (Hyper Kvasir) that\nprevious models for light decomposition (IID) and depth estimation (MonoVIT,\nModoDepth2) are negatively affected by specularities. In contrast, SHADeS can\nsimultaneously produce light decomposition and depth maps that are robust to\nspecular regions. We also perform a quantitative comparison on phantom data\n(C3VD) where we further demonstrate the robustness of our model.\n  Conclusion: Modelling specular reflections improves depth estimation in\ncolonoscopy. We propose an effective self-supervised approach that uses this\ninsight to jointly estimate light decomposition and depth. Light decomposition\nhas the potential to help with other problems, such as place recognition within\nthe colon.\n", "link": "http://arxiv.org/abs/2502.12994v1", "date": "2025-02-18", "relevancy": 2.1826, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHADeS%3A%20Self-supervised%20Monocular%20Depth%20Estimation%20Through%0A%20%20Non-Lambertian%20Image%20Decomposition&body=Title%3A%20SHADeS%3A%20Self-supervised%20Monocular%20Depth%20Estimation%20Through%0A%20%20Non-Lambertian%20Image%20Decomposition%0AAuthor%3A%20Rema%20Daher%20and%20Francisco%20Vasconcelos%20and%20Danail%20Stoyanov%0AAbstract%3A%20%20%20Purpose%3A%20Visual%203D%20scene%20reconstruction%20can%20support%20colonoscopy%20navigation.%0AIt%20can%20help%20in%20recognising%20which%20portions%20of%20the%20colon%20have%20been%20visualised%20and%0Acharacterising%20the%20size%20and%20shape%20of%20polyps.%20This%20is%20still%20a%20very%20challenging%0Aproblem%20due%20to%20complex%20illumination%20variations%2C%20including%20abundant%20specular%0Areflections.%20We%20investigate%20how%20to%20effectively%20decouple%20light%20and%20depth%20in%20this%0Aproblem.%0A%20%20Methods%3A%20We%20introduce%20a%20self-supervised%20model%20that%20simultaneously%0Acharacterises%20the%20shape%20and%20lighting%20of%20the%20visualised%20colonoscopy%20scene.%20Our%0Amodel%20estimates%20shading%2C%20albedo%2C%20depth%2C%20and%20specularities%20%28SHADeS%29%20from%20single%0Aimages.%20Unlike%20previous%20approaches%20%28IID%29%2C%20we%20use%20a%20non-Lambertian%20model%20that%0Atreats%20specular%20reflections%20as%20a%20separate%20light%20component.%20The%20implementation%0Aof%20our%20method%20is%20available%20at%20https%3A//github.com/RemaDaher/SHADeS.%0A%20%20Results%3A%20We%20demonstrate%20on%20real%20colonoscopy%20images%20%28Hyper%20Kvasir%29%20that%0Aprevious%20models%20for%20light%20decomposition%20%28IID%29%20and%20depth%20estimation%20%28MonoVIT%2C%0AModoDepth2%29%20are%20negatively%20affected%20by%20specularities.%20In%20contrast%2C%20SHADeS%20can%0Asimultaneously%20produce%20light%20decomposition%20and%20depth%20maps%20that%20are%20robust%20to%0Aspecular%20regions.%20We%20also%20perform%20a%20quantitative%20comparison%20on%20phantom%20data%0A%28C3VD%29%20where%20we%20further%20demonstrate%20the%20robustness%20of%20our%20model.%0A%20%20Conclusion%3A%20Modelling%20specular%20reflections%20improves%20depth%20estimation%20in%0Acolonoscopy.%20We%20propose%20an%20effective%20self-supervised%20approach%20that%20uses%20this%0Ainsight%20to%20jointly%20estimate%20light%20decomposition%20and%20depth.%20Light%20decomposition%0Ahas%20the%20potential%20to%20help%20with%20other%20problems%2C%20such%20as%20place%20recognition%20within%0Athe%20colon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHADeS%253A%2520Self-supervised%2520Monocular%2520Depth%2520Estimation%2520Through%250A%2520%2520Non-Lambertian%2520Image%2520Decomposition%26entry.906535625%3DRema%2520Daher%2520and%2520Francisco%2520Vasconcelos%2520and%2520Danail%2520Stoyanov%26entry.1292438233%3D%2520%2520Purpose%253A%2520Visual%25203D%2520scene%2520reconstruction%2520can%2520support%2520colonoscopy%2520navigation.%250AIt%2520can%2520help%2520in%2520recognising%2520which%2520portions%2520of%2520the%2520colon%2520have%2520been%2520visualised%2520and%250Acharacterising%2520the%2520size%2520and%2520shape%2520of%2520polyps.%2520This%2520is%2520still%2520a%2520very%2520challenging%250Aproblem%2520due%2520to%2520complex%2520illumination%2520variations%252C%2520including%2520abundant%2520specular%250Areflections.%2520We%2520investigate%2520how%2520to%2520effectively%2520decouple%2520light%2520and%2520depth%2520in%2520this%250Aproblem.%250A%2520%2520Methods%253A%2520We%2520introduce%2520a%2520self-supervised%2520model%2520that%2520simultaneously%250Acharacterises%2520the%2520shape%2520and%2520lighting%2520of%2520the%2520visualised%2520colonoscopy%2520scene.%2520Our%250Amodel%2520estimates%2520shading%252C%2520albedo%252C%2520depth%252C%2520and%2520specularities%2520%2528SHADeS%2529%2520from%2520single%250Aimages.%2520Unlike%2520previous%2520approaches%2520%2528IID%2529%252C%2520we%2520use%2520a%2520non-Lambertian%2520model%2520that%250Atreats%2520specular%2520reflections%2520as%2520a%2520separate%2520light%2520component.%2520The%2520implementation%250Aof%2520our%2520method%2520is%2520available%2520at%2520https%253A//github.com/RemaDaher/SHADeS.%250A%2520%2520Results%253A%2520We%2520demonstrate%2520on%2520real%2520colonoscopy%2520images%2520%2528Hyper%2520Kvasir%2529%2520that%250Aprevious%2520models%2520for%2520light%2520decomposition%2520%2528IID%2529%2520and%2520depth%2520estimation%2520%2528MonoVIT%252C%250AModoDepth2%2529%2520are%2520negatively%2520affected%2520by%2520specularities.%2520In%2520contrast%252C%2520SHADeS%2520can%250Asimultaneously%2520produce%2520light%2520decomposition%2520and%2520depth%2520maps%2520that%2520are%2520robust%2520to%250Aspecular%2520regions.%2520We%2520also%2520perform%2520a%2520quantitative%2520comparison%2520on%2520phantom%2520data%250A%2528C3VD%2529%2520where%2520we%2520further%2520demonstrate%2520the%2520robustness%2520of%2520our%2520model.%250A%2520%2520Conclusion%253A%2520Modelling%2520specular%2520reflections%2520improves%2520depth%2520estimation%2520in%250Acolonoscopy.%2520We%2520propose%2520an%2520effective%2520self-supervised%2520approach%2520that%2520uses%2520this%250Ainsight%2520to%2520jointly%2520estimate%2520light%2520decomposition%2520and%2520depth.%2520Light%2520decomposition%250Ahas%2520the%2520potential%2520to%2520help%2520with%2520other%2520problems%252C%2520such%2520as%2520place%2520recognition%2520within%250Athe%2520colon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHADeS%3A%20Self-supervised%20Monocular%20Depth%20Estimation%20Through%0A%20%20Non-Lambertian%20Image%20Decomposition&entry.906535625=Rema%20Daher%20and%20Francisco%20Vasconcelos%20and%20Danail%20Stoyanov&entry.1292438233=%20%20Purpose%3A%20Visual%203D%20scene%20reconstruction%20can%20support%20colonoscopy%20navigation.%0AIt%20can%20help%20in%20recognising%20which%20portions%20of%20the%20colon%20have%20been%20visualised%20and%0Acharacterising%20the%20size%20and%20shape%20of%20polyps.%20This%20is%20still%20a%20very%20challenging%0Aproblem%20due%20to%20complex%20illumination%20variations%2C%20including%20abundant%20specular%0Areflections.%20We%20investigate%20how%20to%20effectively%20decouple%20light%20and%20depth%20in%20this%0Aproblem.%0A%20%20Methods%3A%20We%20introduce%20a%20self-supervised%20model%20that%20simultaneously%0Acharacterises%20the%20shape%20and%20lighting%20of%20the%20visualised%20colonoscopy%20scene.%20Our%0Amodel%20estimates%20shading%2C%20albedo%2C%20depth%2C%20and%20specularities%20%28SHADeS%29%20from%20single%0Aimages.%20Unlike%20previous%20approaches%20%28IID%29%2C%20we%20use%20a%20non-Lambertian%20model%20that%0Atreats%20specular%20reflections%20as%20a%20separate%20light%20component.%20The%20implementation%0Aof%20our%20method%20is%20available%20at%20https%3A//github.com/RemaDaher/SHADeS.%0A%20%20Results%3A%20We%20demonstrate%20on%20real%20colonoscopy%20images%20%28Hyper%20Kvasir%29%20that%0Aprevious%20models%20for%20light%20decomposition%20%28IID%29%20and%20depth%20estimation%20%28MonoVIT%2C%0AModoDepth2%29%20are%20negatively%20affected%20by%20specularities.%20In%20contrast%2C%20SHADeS%20can%0Asimultaneously%20produce%20light%20decomposition%20and%20depth%20maps%20that%20are%20robust%20to%0Aspecular%20regions.%20We%20also%20perform%20a%20quantitative%20comparison%20on%20phantom%20data%0A%28C3VD%29%20where%20we%20further%20demonstrate%20the%20robustness%20of%20our%20model.%0A%20%20Conclusion%3A%20Modelling%20specular%20reflections%20improves%20depth%20estimation%20in%0Acolonoscopy.%20We%20propose%20an%20effective%20self-supervised%20approach%20that%20uses%20this%0Ainsight%20to%20jointly%20estimate%20light%20decomposition%20and%20depth.%20Light%20decomposition%0Ahas%20the%20potential%20to%20help%20with%20other%20problems%2C%20such%20as%20place%20recognition%20within%0Athe%20colon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12994v1&entry.124074799=Read"},
{"title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based\n  Reinforcement Learning", "author": "Hao Gao and Shaoyu Chen and Bo Jiang and Bencheng Liao and Yiang Shi and Xiaoyang Guo and Yuechuan Pu and Haoran Yin and Xiangyu Li and Xinbang Zhang and Ying Zhang and Wenyu Liu and Qian Zhang and Xinggang Wang", "abstract": "  Existing end-to-end autonomous driving (AD) algorithms typically follow the\nImitation Learning (IL) paradigm, which faces challenges such as causal\nconfusion and the open-loop gap. In this work, we establish a 3DGS-based\nclosed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS\ntechniques, we construct a photorealistic digital replica of the real physical\nworld, enabling the AD policy to extensively explore the state space and learn\nto handle out-of-distribution scenarios through large-scale trial and error. To\nenhance safety, we design specialized rewards that guide the policy to\neffectively respond to safety-critical events and understand real-world causal\nrelationships. For better alignment with human driving behavior, IL is\nincorporated into RL training as a regularization term. We introduce a\nclosed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS\nenvironments. Compared to IL-based methods, RAD achieves stronger performance\nin most closed-loop metrics, especially 3x lower collision rate. Abundant\nclosed-loop results are presented at https://hgao-cv.github.io/RAD.\n", "link": "http://arxiv.org/abs/2502.13144v1", "date": "2025-02-18", "relevancy": 2.1797, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5425}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAD%3A%20Training%20an%20End-to-End%20Driving%20Policy%20via%20Large-Scale%203DGS-based%0A%20%20Reinforcement%20Learning&body=Title%3A%20RAD%3A%20Training%20an%20End-to-End%20Driving%20Policy%20via%20Large-Scale%203DGS-based%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Hao%20Gao%20and%20Shaoyu%20Chen%20and%20Bo%20Jiang%20and%20Bencheng%20Liao%20and%20Yiang%20Shi%20and%20Xiaoyang%20Guo%20and%20Yuechuan%20Pu%20and%20Haoran%20Yin%20and%20Xiangyu%20Li%20and%20Xinbang%20Zhang%20and%20Ying%20Zhang%20and%20Wenyu%20Liu%20and%20Qian%20Zhang%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Existing%20end-to-end%20autonomous%20driving%20%28AD%29%20algorithms%20typically%20follow%20the%0AImitation%20Learning%20%28IL%29%20paradigm%2C%20which%20faces%20challenges%20such%20as%20causal%0Aconfusion%20and%20the%20open-loop%20gap.%20In%20this%20work%2C%20we%20establish%20a%203DGS-based%0Aclosed-loop%20Reinforcement%20Learning%20%28RL%29%20training%20paradigm.%20By%20leveraging%203DGS%0Atechniques%2C%20we%20construct%20a%20photorealistic%20digital%20replica%20of%20the%20real%20physical%0Aworld%2C%20enabling%20the%20AD%20policy%20to%20extensively%20explore%20the%20state%20space%20and%20learn%0Ato%20handle%20out-of-distribution%20scenarios%20through%20large-scale%20trial%20and%20error.%20To%0Aenhance%20safety%2C%20we%20design%20specialized%20rewards%20that%20guide%20the%20policy%20to%0Aeffectively%20respond%20to%20safety-critical%20events%20and%20understand%20real-world%20causal%0Arelationships.%20For%20better%20alignment%20with%20human%20driving%20behavior%2C%20IL%20is%0Aincorporated%20into%20RL%20training%20as%20a%20regularization%20term.%20We%20introduce%20a%0Aclosed-loop%20evaluation%20benchmark%20consisting%20of%20diverse%2C%20previously%20unseen%203DGS%0Aenvironments.%20Compared%20to%20IL-based%20methods%2C%20RAD%20achieves%20stronger%20performance%0Ain%20most%20closed-loop%20metrics%2C%20especially%203x%20lower%20collision%20rate.%20Abundant%0Aclosed-loop%20results%20are%20presented%20at%20https%3A//hgao-cv.github.io/RAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAD%253A%2520Training%2520an%2520End-to-End%2520Driving%2520Policy%2520via%2520Large-Scale%25203DGS-based%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DHao%2520Gao%2520and%2520Shaoyu%2520Chen%2520and%2520Bo%2520Jiang%2520and%2520Bencheng%2520Liao%2520and%2520Yiang%2520Shi%2520and%2520Xiaoyang%2520Guo%2520and%2520Yuechuan%2520Pu%2520and%2520Haoran%2520Yin%2520and%2520Xiangyu%2520Li%2520and%2520Xinbang%2520Zhang%2520and%2520Ying%2520Zhang%2520and%2520Wenyu%2520Liu%2520and%2520Qian%2520Zhang%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520end-to-end%2520autonomous%2520driving%2520%2528AD%2529%2520algorithms%2520typically%2520follow%2520the%250AImitation%2520Learning%2520%2528IL%2529%2520paradigm%252C%2520which%2520faces%2520challenges%2520such%2520as%2520causal%250Aconfusion%2520and%2520the%2520open-loop%2520gap.%2520In%2520this%2520work%252C%2520we%2520establish%2520a%25203DGS-based%250Aclosed-loop%2520Reinforcement%2520Learning%2520%2528RL%2529%2520training%2520paradigm.%2520By%2520leveraging%25203DGS%250Atechniques%252C%2520we%2520construct%2520a%2520photorealistic%2520digital%2520replica%2520of%2520the%2520real%2520physical%250Aworld%252C%2520enabling%2520the%2520AD%2520policy%2520to%2520extensively%2520explore%2520the%2520state%2520space%2520and%2520learn%250Ato%2520handle%2520out-of-distribution%2520scenarios%2520through%2520large-scale%2520trial%2520and%2520error.%2520To%250Aenhance%2520safety%252C%2520we%2520design%2520specialized%2520rewards%2520that%2520guide%2520the%2520policy%2520to%250Aeffectively%2520respond%2520to%2520safety-critical%2520events%2520and%2520understand%2520real-world%2520causal%250Arelationships.%2520For%2520better%2520alignment%2520with%2520human%2520driving%2520behavior%252C%2520IL%2520is%250Aincorporated%2520into%2520RL%2520training%2520as%2520a%2520regularization%2520term.%2520We%2520introduce%2520a%250Aclosed-loop%2520evaluation%2520benchmark%2520consisting%2520of%2520diverse%252C%2520previously%2520unseen%25203DGS%250Aenvironments.%2520Compared%2520to%2520IL-based%2520methods%252C%2520RAD%2520achieves%2520stronger%2520performance%250Ain%2520most%2520closed-loop%2520metrics%252C%2520especially%25203x%2520lower%2520collision%2520rate.%2520Abundant%250Aclosed-loop%2520results%2520are%2520presented%2520at%2520https%253A//hgao-cv.github.io/RAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAD%3A%20Training%20an%20End-to-End%20Driving%20Policy%20via%20Large-Scale%203DGS-based%0A%20%20Reinforcement%20Learning&entry.906535625=Hao%20Gao%20and%20Shaoyu%20Chen%20and%20Bo%20Jiang%20and%20Bencheng%20Liao%20and%20Yiang%20Shi%20and%20Xiaoyang%20Guo%20and%20Yuechuan%20Pu%20and%20Haoran%20Yin%20and%20Xiangyu%20Li%20and%20Xinbang%20Zhang%20and%20Ying%20Zhang%20and%20Wenyu%20Liu%20and%20Qian%20Zhang%20and%20Xinggang%20Wang&entry.1292438233=%20%20Existing%20end-to-end%20autonomous%20driving%20%28AD%29%20algorithms%20typically%20follow%20the%0AImitation%20Learning%20%28IL%29%20paradigm%2C%20which%20faces%20challenges%20such%20as%20causal%0Aconfusion%20and%20the%20open-loop%20gap.%20In%20this%20work%2C%20we%20establish%20a%203DGS-based%0Aclosed-loop%20Reinforcement%20Learning%20%28RL%29%20training%20paradigm.%20By%20leveraging%203DGS%0Atechniques%2C%20we%20construct%20a%20photorealistic%20digital%20replica%20of%20the%20real%20physical%0Aworld%2C%20enabling%20the%20AD%20policy%20to%20extensively%20explore%20the%20state%20space%20and%20learn%0Ato%20handle%20out-of-distribution%20scenarios%20through%20large-scale%20trial%20and%20error.%20To%0Aenhance%20safety%2C%20we%20design%20specialized%20rewards%20that%20guide%20the%20policy%20to%0Aeffectively%20respond%20to%20safety-critical%20events%20and%20understand%20real-world%20causal%0Arelationships.%20For%20better%20alignment%20with%20human%20driving%20behavior%2C%20IL%20is%0Aincorporated%20into%20RL%20training%20as%20a%20regularization%20term.%20We%20introduce%20a%0Aclosed-loop%20evaluation%20benchmark%20consisting%20of%20diverse%2C%20previously%20unseen%203DGS%0Aenvironments.%20Compared%20to%20IL-based%20methods%2C%20RAD%20achieves%20stronger%20performance%0Ain%20most%20closed-loop%20metrics%2C%20especially%203x%20lower%20collision%20rate.%20Abundant%0Aclosed-loop%20results%20are%20presented%20at%20https%3A//hgao-cv.github.io/RAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13144v1&entry.124074799=Read"},
{"title": "LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks", "author": "Andreas Happe and Aaron Kaplan and Juergen Cito", "abstract": "  Penetration testing, an essential component of software security testing,\nallows organizations to identify and remediate vulnerabilities in their\nsystems, thus bolstering their defense mechanisms against cyberattacks. One\nrecent advancement in the realm of penetration testing is the utilization of\nLanguage Models (LLMs). We explore the intersection of LLMs and penetration\ntesting to gain insight into their capabilities and challenges in the context\nof privilege escalation. We introduce a fully automated privilege-escalation\ntool designed for evaluating the efficacy of LLMs for (ethical) hacking,\nexecuting benchmarks using multiple LLMs, and investigating their respective\nresults.\n  Our results show that GPT-4-turbo is well suited to exploit vulnerabilities\n(33-83% of vulnerabilities). GPT-3.5-turbo can abuse 16-50% of vulnerabilities,\nwhile local models, such as Llama3, can only exploit between 0 and 33% of the\nvulnerabilities.\n  We analyze the impact of different context sizes, in-context learning,\noptional high-level guidance mechanisms, and memory management techniques. We\ndiscuss challenging areas for LLMs, including maintaining focus during testing,\ncoping with errors, and finally comparing LLMs with human hackers.\n  The current version of the LLM-guided privilege-escalation prototype can be\nfound at https://github.com/ipa-labs/hackingBuddyGPT.\n", "link": "http://arxiv.org/abs/2310.11409v5", "date": "2025-02-18", "relevancy": 2.1726, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20Hackers%3A%20Autonomous%20Linux%20Privilege%20Escalation%20Attacks&body=Title%3A%20LLMs%20as%20Hackers%3A%20Autonomous%20Linux%20Privilege%20Escalation%20Attacks%0AAuthor%3A%20Andreas%20Happe%20and%20Aaron%20Kaplan%20and%20Juergen%20Cito%0AAbstract%3A%20%20%20Penetration%20testing%2C%20an%20essential%20component%20of%20software%20security%20testing%2C%0Aallows%20organizations%20to%20identify%20and%20remediate%20vulnerabilities%20in%20their%0Asystems%2C%20thus%20bolstering%20their%20defense%20mechanisms%20against%20cyberattacks.%20One%0Arecent%20advancement%20in%20the%20realm%20of%20penetration%20testing%20is%20the%20utilization%20of%0ALanguage%20Models%20%28LLMs%29.%20We%20explore%20the%20intersection%20of%20LLMs%20and%20penetration%0Atesting%20to%20gain%20insight%20into%20their%20capabilities%20and%20challenges%20in%20the%20context%0Aof%20privilege%20escalation.%20We%20introduce%20a%20fully%20automated%20privilege-escalation%0Atool%20designed%20for%20evaluating%20the%20efficacy%20of%20LLMs%20for%20%28ethical%29%20hacking%2C%0Aexecuting%20benchmarks%20using%20multiple%20LLMs%2C%20and%20investigating%20their%20respective%0Aresults.%0A%20%20Our%20results%20show%20that%20GPT-4-turbo%20is%20well%20suited%20to%20exploit%20vulnerabilities%0A%2833-83%25%20of%20vulnerabilities%29.%20GPT-3.5-turbo%20can%20abuse%2016-50%25%20of%20vulnerabilities%2C%0Awhile%20local%20models%2C%20such%20as%20Llama3%2C%20can%20only%20exploit%20between%200%20and%2033%25%20of%20the%0Avulnerabilities.%0A%20%20We%20analyze%20the%20impact%20of%20different%20context%20sizes%2C%20in-context%20learning%2C%0Aoptional%20high-level%20guidance%20mechanisms%2C%20and%20memory%20management%20techniques.%20We%0Adiscuss%20challenging%20areas%20for%20LLMs%2C%20including%20maintaining%20focus%20during%20testing%2C%0Acoping%20with%20errors%2C%20and%20finally%20comparing%20LLMs%20with%20human%20hackers.%0A%20%20The%20current%20version%20of%20the%20LLM-guided%20privilege-escalation%20prototype%20can%20be%0Afound%20at%20https%3A//github.com/ipa-labs/hackingBuddyGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11409v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520as%2520Hackers%253A%2520Autonomous%2520Linux%2520Privilege%2520Escalation%2520Attacks%26entry.906535625%3DAndreas%2520Happe%2520and%2520Aaron%2520Kaplan%2520and%2520Juergen%2520Cito%26entry.1292438233%3D%2520%2520Penetration%2520testing%252C%2520an%2520essential%2520component%2520of%2520software%2520security%2520testing%252C%250Aallows%2520organizations%2520to%2520identify%2520and%2520remediate%2520vulnerabilities%2520in%2520their%250Asystems%252C%2520thus%2520bolstering%2520their%2520defense%2520mechanisms%2520against%2520cyberattacks.%2520One%250Arecent%2520advancement%2520in%2520the%2520realm%2520of%2520penetration%2520testing%2520is%2520the%2520utilization%2520of%250ALanguage%2520Models%2520%2528LLMs%2529.%2520We%2520explore%2520the%2520intersection%2520of%2520LLMs%2520and%2520penetration%250Atesting%2520to%2520gain%2520insight%2520into%2520their%2520capabilities%2520and%2520challenges%2520in%2520the%2520context%250Aof%2520privilege%2520escalation.%2520We%2520introduce%2520a%2520fully%2520automated%2520privilege-escalation%250Atool%2520designed%2520for%2520evaluating%2520the%2520efficacy%2520of%2520LLMs%2520for%2520%2528ethical%2529%2520hacking%252C%250Aexecuting%2520benchmarks%2520using%2520multiple%2520LLMs%252C%2520and%2520investigating%2520their%2520respective%250Aresults.%250A%2520%2520Our%2520results%2520show%2520that%2520GPT-4-turbo%2520is%2520well%2520suited%2520to%2520exploit%2520vulnerabilities%250A%252833-83%2525%2520of%2520vulnerabilities%2529.%2520GPT-3.5-turbo%2520can%2520abuse%252016-50%2525%2520of%2520vulnerabilities%252C%250Awhile%2520local%2520models%252C%2520such%2520as%2520Llama3%252C%2520can%2520only%2520exploit%2520between%25200%2520and%252033%2525%2520of%2520the%250Avulnerabilities.%250A%2520%2520We%2520analyze%2520the%2520impact%2520of%2520different%2520context%2520sizes%252C%2520in-context%2520learning%252C%250Aoptional%2520high-level%2520guidance%2520mechanisms%252C%2520and%2520memory%2520management%2520techniques.%2520We%250Adiscuss%2520challenging%2520areas%2520for%2520LLMs%252C%2520including%2520maintaining%2520focus%2520during%2520testing%252C%250Acoping%2520with%2520errors%252C%2520and%2520finally%2520comparing%2520LLMs%2520with%2520human%2520hackers.%250A%2520%2520The%2520current%2520version%2520of%2520the%2520LLM-guided%2520privilege-escalation%2520prototype%2520can%2520be%250Afound%2520at%2520https%253A//github.com/ipa-labs/hackingBuddyGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11409v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20Hackers%3A%20Autonomous%20Linux%20Privilege%20Escalation%20Attacks&entry.906535625=Andreas%20Happe%20and%20Aaron%20Kaplan%20and%20Juergen%20Cito&entry.1292438233=%20%20Penetration%20testing%2C%20an%20essential%20component%20of%20software%20security%20testing%2C%0Aallows%20organizations%20to%20identify%20and%20remediate%20vulnerabilities%20in%20their%0Asystems%2C%20thus%20bolstering%20their%20defense%20mechanisms%20against%20cyberattacks.%20One%0Arecent%20advancement%20in%20the%20realm%20of%20penetration%20testing%20is%20the%20utilization%20of%0ALanguage%20Models%20%28LLMs%29.%20We%20explore%20the%20intersection%20of%20LLMs%20and%20penetration%0Atesting%20to%20gain%20insight%20into%20their%20capabilities%20and%20challenges%20in%20the%20context%0Aof%20privilege%20escalation.%20We%20introduce%20a%20fully%20automated%20privilege-escalation%0Atool%20designed%20for%20evaluating%20the%20efficacy%20of%20LLMs%20for%20%28ethical%29%20hacking%2C%0Aexecuting%20benchmarks%20using%20multiple%20LLMs%2C%20and%20investigating%20their%20respective%0Aresults.%0A%20%20Our%20results%20show%20that%20GPT-4-turbo%20is%20well%20suited%20to%20exploit%20vulnerabilities%0A%2833-83%25%20of%20vulnerabilities%29.%20GPT-3.5-turbo%20can%20abuse%2016-50%25%20of%20vulnerabilities%2C%0Awhile%20local%20models%2C%20such%20as%20Llama3%2C%20can%20only%20exploit%20between%200%20and%2033%25%20of%20the%0Avulnerabilities.%0A%20%20We%20analyze%20the%20impact%20of%20different%20context%20sizes%2C%20in-context%20learning%2C%0Aoptional%20high-level%20guidance%20mechanisms%2C%20and%20memory%20management%20techniques.%20We%0Adiscuss%20challenging%20areas%20for%20LLMs%2C%20including%20maintaining%20focus%20during%20testing%2C%0Acoping%20with%20errors%2C%20and%20finally%20comparing%20LLMs%20with%20human%20hackers.%0A%20%20The%20current%20version%20of%20the%20LLM-guided%20privilege-escalation%20prototype%20can%20be%0Afound%20at%20https%3A//github.com/ipa-labs/hackingBuddyGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11409v5&entry.124074799=Read"},
{"title": "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection", "author": "Gianluca Guglielmo and Marc Masana", "abstract": "  In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.\n", "link": "http://arxiv.org/abs/2502.12849v1", "date": "2025-02-18", "relevancy": 2.1706, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5621}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5403}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Intermediate%20Representations%20for%20Better%20Out-of-Distribution%0A%20%20Detection&body=Title%3A%20Leveraging%20Intermediate%20Representations%20for%20Better%20Out-of-Distribution%0A%20%20Detection%0AAuthor%3A%20Gianluca%20Guglielmo%20and%20Marc%20Masana%0AAbstract%3A%20%20%20In%20real-world%20applications%2C%20machine%20learning%20models%20must%20reliably%20detect%0AOut-of-Distribution%20%28OoD%29%20samples%20to%20prevent%20unsafe%20decisions.%20Current%20OoD%0Adetection%20methods%20often%20rely%20on%20analyzing%20the%20logits%20or%20the%20embeddings%20of%20the%0Apenultimate%20layer%20of%20a%20neural%20network.%20However%2C%20little%20work%20has%20been%20conducted%0Aon%20the%20exploitation%20of%20the%20rich%20information%20encoded%20in%20intermediate%20layers.%20To%0Aaddress%20this%2C%20we%20analyze%20the%20discriminative%20power%20of%20intermediate%20layers%20and%0Ashow%20that%20they%20can%20positively%20be%20used%20for%20OoD%20detection.%20Therefore%2C%20we%20propose%0Ato%20regularize%20intermediate%20layers%20with%20an%20energy-based%20contrastive%20loss%2C%20and%20by%0Agrouping%20multiple%20layers%20in%20a%20single%20aggregated%20response.%20We%20demonstrate%20that%0Aintermediate%20layer%20activations%20improves%20OoD%20detection%20performance%20by%20running%20a%0Acomprehensive%20evaluation%20across%20multiple%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Intermediate%2520Representations%2520for%2520Better%2520Out-of-Distribution%250A%2520%2520Detection%26entry.906535625%3DGianluca%2520Guglielmo%2520and%2520Marc%2520Masana%26entry.1292438233%3D%2520%2520In%2520real-world%2520applications%252C%2520machine%2520learning%2520models%2520must%2520reliably%2520detect%250AOut-of-Distribution%2520%2528OoD%2529%2520samples%2520to%2520prevent%2520unsafe%2520decisions.%2520Current%2520OoD%250Adetection%2520methods%2520often%2520rely%2520on%2520analyzing%2520the%2520logits%2520or%2520the%2520embeddings%2520of%2520the%250Apenultimate%2520layer%2520of%2520a%2520neural%2520network.%2520However%252C%2520little%2520work%2520has%2520been%2520conducted%250Aon%2520the%2520exploitation%2520of%2520the%2520rich%2520information%2520encoded%2520in%2520intermediate%2520layers.%2520To%250Aaddress%2520this%252C%2520we%2520analyze%2520the%2520discriminative%2520power%2520of%2520intermediate%2520layers%2520and%250Ashow%2520that%2520they%2520can%2520positively%2520be%2520used%2520for%2520OoD%2520detection.%2520Therefore%252C%2520we%2520propose%250Ato%2520regularize%2520intermediate%2520layers%2520with%2520an%2520energy-based%2520contrastive%2520loss%252C%2520and%2520by%250Agrouping%2520multiple%2520layers%2520in%2520a%2520single%2520aggregated%2520response.%2520We%2520demonstrate%2520that%250Aintermediate%2520layer%2520activations%2520improves%2520OoD%2520detection%2520performance%2520by%2520running%2520a%250Acomprehensive%2520evaluation%2520across%2520multiple%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Intermediate%20Representations%20for%20Better%20Out-of-Distribution%0A%20%20Detection&entry.906535625=Gianluca%20Guglielmo%20and%20Marc%20Masana&entry.1292438233=%20%20In%20real-world%20applications%2C%20machine%20learning%20models%20must%20reliably%20detect%0AOut-of-Distribution%20%28OoD%29%20samples%20to%20prevent%20unsafe%20decisions.%20Current%20OoD%0Adetection%20methods%20often%20rely%20on%20analyzing%20the%20logits%20or%20the%20embeddings%20of%20the%0Apenultimate%20layer%20of%20a%20neural%20network.%20However%2C%20little%20work%20has%20been%20conducted%0Aon%20the%20exploitation%20of%20the%20rich%20information%20encoded%20in%20intermediate%20layers.%20To%0Aaddress%20this%2C%20we%20analyze%20the%20discriminative%20power%20of%20intermediate%20layers%20and%0Ashow%20that%20they%20can%20positively%20be%20used%20for%20OoD%20detection.%20Therefore%2C%20we%20propose%0Ato%20regularize%20intermediate%20layers%20with%20an%20energy-based%20contrastive%20loss%2C%20and%20by%0Agrouping%20multiple%20layers%20in%20a%20single%20aggregated%20response.%20We%20demonstrate%20that%0Aintermediate%20layer%20activations%20improves%20OoD%20detection%20performance%20by%20running%20a%0Acomprehensive%20evaluation%20across%20multiple%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12849v1&entry.124074799=Read"},
{"title": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs", "author": "Longxu Dou and Qian Liu and Fan Zhou and Changyu Chen and Zili Wang and Ziqi Jin and Zichen Liu and Tongyao Zhu and Cunxiao Du and Penghui Yang and Haonan Wang and Jiaheng Liu and Yongchi Zhao and Xiachong Feng and Xin Mao and Man Tsung Yeung and Kunat Pipatanakul and Fajri Koto and Min Si Thu and Hynek Kydl\u00ed\u010dek and Zeyi Liu and Qunshu Lin and Sittipong Sripaisarnmongkol and Kridtaphad Sae-Khow and Nirattisai Thongchim and Taechawat Konkaew and Narong Borijindargoon and Anh Dao and Matichon Maneegard and Phakphum Artkaew and Zheng-Xin Yong and Quan Nguyen and Wannaphong Phatthiyaphaibun and Hoang H. Tran and Mike Zhang and Shiqi Chen and Tianyu Pang and Chao Du and Xinyi Wan and Wei Lu and Min Lin", "abstract": "  Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages.\n", "link": "http://arxiv.org/abs/2502.12982v1", "date": "2025-02-18", "relevancy": 2.1667, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4201}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sailor2%3A%20Sailing%20in%20South-East%20Asia%20with%20Inclusive%20Multilingual%20LLMs&body=Title%3A%20Sailor2%3A%20Sailing%20in%20South-East%20Asia%20with%20Inclusive%20Multilingual%20LLMs%0AAuthor%3A%20Longxu%20Dou%20and%20Qian%20Liu%20and%20Fan%20Zhou%20and%20Changyu%20Chen%20and%20Zili%20Wang%20and%20Ziqi%20Jin%20and%20Zichen%20Liu%20and%20Tongyao%20Zhu%20and%20Cunxiao%20Du%20and%20Penghui%20Yang%20and%20Haonan%20Wang%20and%20Jiaheng%20Liu%20and%20Yongchi%20Zhao%20and%20Xiachong%20Feng%20and%20Xin%20Mao%20and%20Man%20Tsung%20Yeung%20and%20Kunat%20Pipatanakul%20and%20Fajri%20Koto%20and%20Min%20Si%20Thu%20and%20Hynek%20Kydl%C3%AD%C4%8Dek%20and%20Zeyi%20Liu%20and%20Qunshu%20Lin%20and%20Sittipong%20Sripaisarnmongkol%20and%20Kridtaphad%20Sae-Khow%20and%20Nirattisai%20Thongchim%20and%20Taechawat%20Konkaew%20and%20Narong%20Borijindargoon%20and%20Anh%20Dao%20and%20Matichon%20Maneegard%20and%20Phakphum%20Artkaew%20and%20Zheng-Xin%20Yong%20and%20Quan%20Nguyen%20and%20Wannaphong%20Phatthiyaphaibun%20and%20Hoang%20H.%20Tran%20and%20Mike%20Zhang%20and%20Shiqi%20Chen%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Xinyi%20Wan%20and%20Wei%20Lu%20and%20Min%20Lin%0AAbstract%3A%20%20%20Sailor2%20is%20a%20family%20of%20cutting-edge%20multilingual%20language%20models%20for%0ASouth-East%20Asian%20%28SEA%29%20languages%2C%20available%20in%201B%2C%208B%2C%20and%2020B%20sizes%20to%20suit%0Adiverse%20applications.%20Building%20on%20Qwen2.5%2C%20Sailor2%20undergoes%20continuous%0Apre-training%20on%20500B%20tokens%20%28400B%20SEA-specific%20and%20100B%20replay%20tokens%29%20to%0Asupport%2013%20SEA%20languages%20while%20retaining%20proficiency%20in%20Chinese%20and%20English.%0ASailor2-20B%20model%20achieves%20a%2050-50%20win%20rate%20against%20GPT-4o%20across%20SEA%0Alanguages.%20We%20also%20deliver%20a%20comprehensive%20cookbook%20on%20how%20to%20develop%20the%0Amultilingual%20model%20in%20an%20efficient%20manner%2C%20including%20five%20key%20aspects%3A%20data%0Acuration%2C%20pre-training%2C%20post-training%2C%20model%20customization%20and%20evaluation.%20We%0Ahope%20that%20Sailor2%20model%20%28Apache%202.0%20license%29%20will%20drive%20language%20development%20in%0Athe%20SEA%20region%2C%20and%20Sailor2%20cookbook%20will%20inspire%20researchers%20to%20build%20more%0Ainclusive%20LLMs%20for%20other%20under-served%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSailor2%253A%2520Sailing%2520in%2520South-East%2520Asia%2520with%2520Inclusive%2520Multilingual%2520LLMs%26entry.906535625%3DLongxu%2520Dou%2520and%2520Qian%2520Liu%2520and%2520Fan%2520Zhou%2520and%2520Changyu%2520Chen%2520and%2520Zili%2520Wang%2520and%2520Ziqi%2520Jin%2520and%2520Zichen%2520Liu%2520and%2520Tongyao%2520Zhu%2520and%2520Cunxiao%2520Du%2520and%2520Penghui%2520Yang%2520and%2520Haonan%2520Wang%2520and%2520Jiaheng%2520Liu%2520and%2520Yongchi%2520Zhao%2520and%2520Xiachong%2520Feng%2520and%2520Xin%2520Mao%2520and%2520Man%2520Tsung%2520Yeung%2520and%2520Kunat%2520Pipatanakul%2520and%2520Fajri%2520Koto%2520and%2520Min%2520Si%2520Thu%2520and%2520Hynek%2520Kydl%25C3%25AD%25C4%258Dek%2520and%2520Zeyi%2520Liu%2520and%2520Qunshu%2520Lin%2520and%2520Sittipong%2520Sripaisarnmongkol%2520and%2520Kridtaphad%2520Sae-Khow%2520and%2520Nirattisai%2520Thongchim%2520and%2520Taechawat%2520Konkaew%2520and%2520Narong%2520Borijindargoon%2520and%2520Anh%2520Dao%2520and%2520Matichon%2520Maneegard%2520and%2520Phakphum%2520Artkaew%2520and%2520Zheng-Xin%2520Yong%2520and%2520Quan%2520Nguyen%2520and%2520Wannaphong%2520Phatthiyaphaibun%2520and%2520Hoang%2520H.%2520Tran%2520and%2520Mike%2520Zhang%2520and%2520Shiqi%2520Chen%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Xinyi%2520Wan%2520and%2520Wei%2520Lu%2520and%2520Min%2520Lin%26entry.1292438233%3D%2520%2520Sailor2%2520is%2520a%2520family%2520of%2520cutting-edge%2520multilingual%2520language%2520models%2520for%250ASouth-East%2520Asian%2520%2528SEA%2529%2520languages%252C%2520available%2520in%25201B%252C%25208B%252C%2520and%252020B%2520sizes%2520to%2520suit%250Adiverse%2520applications.%2520Building%2520on%2520Qwen2.5%252C%2520Sailor2%2520undergoes%2520continuous%250Apre-training%2520on%2520500B%2520tokens%2520%2528400B%2520SEA-specific%2520and%2520100B%2520replay%2520tokens%2529%2520to%250Asupport%252013%2520SEA%2520languages%2520while%2520retaining%2520proficiency%2520in%2520Chinese%2520and%2520English.%250ASailor2-20B%2520model%2520achieves%2520a%252050-50%2520win%2520rate%2520against%2520GPT-4o%2520across%2520SEA%250Alanguages.%2520We%2520also%2520deliver%2520a%2520comprehensive%2520cookbook%2520on%2520how%2520to%2520develop%2520the%250Amultilingual%2520model%2520in%2520an%2520efficient%2520manner%252C%2520including%2520five%2520key%2520aspects%253A%2520data%250Acuration%252C%2520pre-training%252C%2520post-training%252C%2520model%2520customization%2520and%2520evaluation.%2520We%250Ahope%2520that%2520Sailor2%2520model%2520%2528Apache%25202.0%2520license%2529%2520will%2520drive%2520language%2520development%2520in%250Athe%2520SEA%2520region%252C%2520and%2520Sailor2%2520cookbook%2520will%2520inspire%2520researchers%2520to%2520build%2520more%250Ainclusive%2520LLMs%2520for%2520other%2520under-served%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sailor2%3A%20Sailing%20in%20South-East%20Asia%20with%20Inclusive%20Multilingual%20LLMs&entry.906535625=Longxu%20Dou%20and%20Qian%20Liu%20and%20Fan%20Zhou%20and%20Changyu%20Chen%20and%20Zili%20Wang%20and%20Ziqi%20Jin%20and%20Zichen%20Liu%20and%20Tongyao%20Zhu%20and%20Cunxiao%20Du%20and%20Penghui%20Yang%20and%20Haonan%20Wang%20and%20Jiaheng%20Liu%20and%20Yongchi%20Zhao%20and%20Xiachong%20Feng%20and%20Xin%20Mao%20and%20Man%20Tsung%20Yeung%20and%20Kunat%20Pipatanakul%20and%20Fajri%20Koto%20and%20Min%20Si%20Thu%20and%20Hynek%20Kydl%C3%AD%C4%8Dek%20and%20Zeyi%20Liu%20and%20Qunshu%20Lin%20and%20Sittipong%20Sripaisarnmongkol%20and%20Kridtaphad%20Sae-Khow%20and%20Nirattisai%20Thongchim%20and%20Taechawat%20Konkaew%20and%20Narong%20Borijindargoon%20and%20Anh%20Dao%20and%20Matichon%20Maneegard%20and%20Phakphum%20Artkaew%20and%20Zheng-Xin%20Yong%20and%20Quan%20Nguyen%20and%20Wannaphong%20Phatthiyaphaibun%20and%20Hoang%20H.%20Tran%20and%20Mike%20Zhang%20and%20Shiqi%20Chen%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Xinyi%20Wan%20and%20Wei%20Lu%20and%20Min%20Lin&entry.1292438233=%20%20Sailor2%20is%20a%20family%20of%20cutting-edge%20multilingual%20language%20models%20for%0ASouth-East%20Asian%20%28SEA%29%20languages%2C%20available%20in%201B%2C%208B%2C%20and%2020B%20sizes%20to%20suit%0Adiverse%20applications.%20Building%20on%20Qwen2.5%2C%20Sailor2%20undergoes%20continuous%0Apre-training%20on%20500B%20tokens%20%28400B%20SEA-specific%20and%20100B%20replay%20tokens%29%20to%0Asupport%2013%20SEA%20languages%20while%20retaining%20proficiency%20in%20Chinese%20and%20English.%0ASailor2-20B%20model%20achieves%20a%2050-50%20win%20rate%20against%20GPT-4o%20across%20SEA%0Alanguages.%20We%20also%20deliver%20a%20comprehensive%20cookbook%20on%20how%20to%20develop%20the%0Amultilingual%20model%20in%20an%20efficient%20manner%2C%20including%20five%20key%20aspects%3A%20data%0Acuration%2C%20pre-training%2C%20post-training%2C%20model%20customization%20and%20evaluation.%20We%0Ahope%20that%20Sailor2%20model%20%28Apache%202.0%20license%29%20will%20drive%20language%20development%20in%0Athe%20SEA%20region%2C%20and%20Sailor2%20cookbook%20will%20inspire%20researchers%20to%20build%20more%0Ainclusive%20LLMs%20for%20other%20under-served%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12982v1&entry.124074799=Read"},
{"title": "Towards Text-Image Interleaved Retrieval", "author": "Xin Zhang and Ziqi Dai and Yongqi Li and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang and Jun Yu and Wenjie Li and Min Zhang", "abstract": "  Current multimodal information retrieval studies mainly focus on single-image\ninputs, which limits real-world applications involving multiple images and\ntext-image interleaved content. In this work, we introduce the text-image\ninterleaved retrieval (TIIR) task, where the query and document are interleaved\ntext-image sequences, and the model is required to understand the semantics\nfrom the interleaved context for effective retrieval. We construct a TIIR\nbenchmark based on naturally interleaved wikiHow tutorials, where a specific\npipeline is designed to generate interleaved queries. To explore the task, we\nadapt several off-the-shelf retrievers and build a dense baseline by\ninterleaved multimodal large language model (MLLM). We then propose a novel\nMatryoshka Multimodal Embedder (MME), which compresses the number of visual\ntokens at different granularity, to address the challenge of excessive visual\ntokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption\nof existing models does not consistently yield effective results. Our MME\nachieves significant improvements over the baseline by substantially fewer\nvisual tokens. We provide extensive analysis and will release the dataset and\ncode to facilitate future research.\n", "link": "http://arxiv.org/abs/2502.12799v1", "date": "2025-02-18", "relevancy": 2.1629, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5876}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5089}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Text-Image%20Interleaved%20Retrieval&body=Title%3A%20Towards%20Text-Image%20Interleaved%20Retrieval%0AAuthor%3A%20Xin%20Zhang%20and%20Ziqi%20Dai%20and%20Yongqi%20Li%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Pengjun%20Xie%20and%20Meishan%20Zhang%20and%20Jun%20Yu%20and%20Wenjie%20Li%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Current%20multimodal%20information%20retrieval%20studies%20mainly%20focus%20on%20single-image%0Ainputs%2C%20which%20limits%20real-world%20applications%20involving%20multiple%20images%20and%0Atext-image%20interleaved%20content.%20In%20this%20work%2C%20we%20introduce%20the%20text-image%0Ainterleaved%20retrieval%20%28TIIR%29%20task%2C%20where%20the%20query%20and%20document%20are%20interleaved%0Atext-image%20sequences%2C%20and%20the%20model%20is%20required%20to%20understand%20the%20semantics%0Afrom%20the%20interleaved%20context%20for%20effective%20retrieval.%20We%20construct%20a%20TIIR%0Abenchmark%20based%20on%20naturally%20interleaved%20wikiHow%20tutorials%2C%20where%20a%20specific%0Apipeline%20is%20designed%20to%20generate%20interleaved%20queries.%20To%20explore%20the%20task%2C%20we%0Aadapt%20several%20off-the-shelf%20retrievers%20and%20build%20a%20dense%20baseline%20by%0Ainterleaved%20multimodal%20large%20language%20model%20%28MLLM%29.%20We%20then%20propose%20a%20novel%0AMatryoshka%20Multimodal%20Embedder%20%28MME%29%2C%20which%20compresses%20the%20number%20of%20visual%0Atokens%20at%20different%20granularity%2C%20to%20address%20the%20challenge%20of%20excessive%20visual%0Atokens%20in%20MLLM-based%20TIIR%20models.%20Experiments%20demonstrate%20that%20simple%20adaption%0Aof%20existing%20models%20does%20not%20consistently%20yield%20effective%20results.%20Our%20MME%0Aachieves%20significant%20improvements%20over%20the%20baseline%20by%20substantially%20fewer%0Avisual%20tokens.%20We%20provide%20extensive%20analysis%20and%20will%20release%20the%20dataset%20and%0Acode%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Text-Image%2520Interleaved%2520Retrieval%26entry.906535625%3DXin%2520Zhang%2520and%2520Ziqi%2520Dai%2520and%2520Yongqi%2520Li%2520and%2520Yanzhao%2520Zhang%2520and%2520Dingkun%2520Long%2520and%2520Pengjun%2520Xie%2520and%2520Meishan%2520Zhang%2520and%2520Jun%2520Yu%2520and%2520Wenjie%2520Li%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520multimodal%2520information%2520retrieval%2520studies%2520mainly%2520focus%2520on%2520single-image%250Ainputs%252C%2520which%2520limits%2520real-world%2520applications%2520involving%2520multiple%2520images%2520and%250Atext-image%2520interleaved%2520content.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520text-image%250Ainterleaved%2520retrieval%2520%2528TIIR%2529%2520task%252C%2520where%2520the%2520query%2520and%2520document%2520are%2520interleaved%250Atext-image%2520sequences%252C%2520and%2520the%2520model%2520is%2520required%2520to%2520understand%2520the%2520semantics%250Afrom%2520the%2520interleaved%2520context%2520for%2520effective%2520retrieval.%2520We%2520construct%2520a%2520TIIR%250Abenchmark%2520based%2520on%2520naturally%2520interleaved%2520wikiHow%2520tutorials%252C%2520where%2520a%2520specific%250Apipeline%2520is%2520designed%2520to%2520generate%2520interleaved%2520queries.%2520To%2520explore%2520the%2520task%252C%2520we%250Aadapt%2520several%2520off-the-shelf%2520retrievers%2520and%2520build%2520a%2520dense%2520baseline%2520by%250Ainterleaved%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529.%2520We%2520then%2520propose%2520a%2520novel%250AMatryoshka%2520Multimodal%2520Embedder%2520%2528MME%2529%252C%2520which%2520compresses%2520the%2520number%2520of%2520visual%250Atokens%2520at%2520different%2520granularity%252C%2520to%2520address%2520the%2520challenge%2520of%2520excessive%2520visual%250Atokens%2520in%2520MLLM-based%2520TIIR%2520models.%2520Experiments%2520demonstrate%2520that%2520simple%2520adaption%250Aof%2520existing%2520models%2520does%2520not%2520consistently%2520yield%2520effective%2520results.%2520Our%2520MME%250Aachieves%2520significant%2520improvements%2520over%2520the%2520baseline%2520by%2520substantially%2520fewer%250Avisual%2520tokens.%2520We%2520provide%2520extensive%2520analysis%2520and%2520will%2520release%2520the%2520dataset%2520and%250Acode%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Text-Image%20Interleaved%20Retrieval&entry.906535625=Xin%20Zhang%20and%20Ziqi%20Dai%20and%20Yongqi%20Li%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Pengjun%20Xie%20and%20Meishan%20Zhang%20and%20Jun%20Yu%20and%20Wenjie%20Li%20and%20Min%20Zhang&entry.1292438233=%20%20Current%20multimodal%20information%20retrieval%20studies%20mainly%20focus%20on%20single-image%0Ainputs%2C%20which%20limits%20real-world%20applications%20involving%20multiple%20images%20and%0Atext-image%20interleaved%20content.%20In%20this%20work%2C%20we%20introduce%20the%20text-image%0Ainterleaved%20retrieval%20%28TIIR%29%20task%2C%20where%20the%20query%20and%20document%20are%20interleaved%0Atext-image%20sequences%2C%20and%20the%20model%20is%20required%20to%20understand%20the%20semantics%0Afrom%20the%20interleaved%20context%20for%20effective%20retrieval.%20We%20construct%20a%20TIIR%0Abenchmark%20based%20on%20naturally%20interleaved%20wikiHow%20tutorials%2C%20where%20a%20specific%0Apipeline%20is%20designed%20to%20generate%20interleaved%20queries.%20To%20explore%20the%20task%2C%20we%0Aadapt%20several%20off-the-shelf%20retrievers%20and%20build%20a%20dense%20baseline%20by%0Ainterleaved%20multimodal%20large%20language%20model%20%28MLLM%29.%20We%20then%20propose%20a%20novel%0AMatryoshka%20Multimodal%20Embedder%20%28MME%29%2C%20which%20compresses%20the%20number%20of%20visual%0Atokens%20at%20different%20granularity%2C%20to%20address%20the%20challenge%20of%20excessive%20visual%0Atokens%20in%20MLLM-based%20TIIR%20models.%20Experiments%20demonstrate%20that%20simple%20adaption%0Aof%20existing%20models%20does%20not%20consistently%20yield%20effective%20results.%20Our%20MME%0Aachieves%20significant%20improvements%20over%20the%20baseline%20by%20substantially%20fewer%0Avisual%20tokens.%20We%20provide%20extensive%20analysis%20and%20will%20release%20the%20dataset%20and%0Acode%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12799v1&entry.124074799=Read"},
{"title": "Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SciCap Challenge 2023", "author": "Ting-Yao E. Hsu and Yi-Li Hsu and Shaurya Rohatgi and Chieh-Yang Huang and Ho Yin Sam Ng and Ryan Rossi and Sungchul Kim and Tong Yu and Lun-Wei Ku and C. Lee Giles and Ting-Hao K. Huang", "abstract": "  Since the SciCap datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SciCap Challenge took place, inviting global teams\nto use an expanded SciCap dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SciCap\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n", "link": "http://arxiv.org/abs/2501.19353v3", "date": "2025-02-18", "relevancy": 2.1368, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5387}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SciCap%20Challenge%202023&body=Title%3A%20Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SciCap%20Challenge%202023%0AAuthor%3A%20Ting-Yao%20E.%20Hsu%20and%20Yi-Li%20Hsu%20and%20Shaurya%20Rohatgi%20and%20Chieh-Yang%20Huang%20and%20Ho%20Yin%20Sam%20Ng%20and%20Ryan%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20Lun-Wei%20Ku%20and%20C.%20Lee%20Giles%20and%20Ting-Hao%20K.%20Huang%0AAbstract%3A%20%20%20Since%20the%20SciCap%20datasets%20launch%20in%202021%2C%20the%20research%20community%20has%20made%0Asignificant%20progress%20in%20generating%20captions%20for%20scientific%20figures%20in%20scholarly%0Aarticles.%20In%202023%2C%20the%20first%20SciCap%20Challenge%20took%20place%2C%20inviting%20global%20teams%0Ato%20use%20an%20expanded%20SciCap%20dataset%20to%20develop%20models%20for%20captioning%20diverse%0Afigure%20types%20across%20various%20academic%20fields.%20At%20the%20same%20time%2C%20text%20generation%0Amodels%20advanced%20quickly%2C%20with%20many%20powerful%20pre-trained%20large%20multimodal%20models%0A%28LMMs%29%20emerging%20that%20showed%20impressive%20capabilities%20in%20various%0Avision-and-language%20tasks.%20This%20paper%20presents%20an%20overview%20of%20the%20first%20SciCap%0AChallenge%20and%20details%20the%20performance%20of%20various%20models%20on%20its%20data%2C%20capturing%0Aa%20snapshot%20of%20the%20fields%20state.%20We%20found%20that%20professional%20editors%0Aoverwhelmingly%20preferred%20figure%20captions%20generated%20by%20GPT-4V%20over%20those%20from%0Aall%20other%20models%20and%20even%20the%20original%20captions%20written%20by%20authors.%20Following%0Athis%20key%20finding%2C%20we%20conducted%20detailed%20analyses%20to%20answer%20this%20question%3A%20Have%0Aadvanced%20LMMs%20solved%20the%20task%20of%20generating%20captions%20for%20scientific%20figures%3F%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19353v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Multimodal%2520Models%2520Solve%2520Caption%2520Generation%2520for%2520Scientific%250A%2520%2520Figures%253F%2520Lessons%2520Learned%2520from%2520SciCap%2520Challenge%25202023%26entry.906535625%3DTing-Yao%2520E.%2520Hsu%2520and%2520Yi-Li%2520Hsu%2520and%2520Shaurya%2520Rohatgi%2520and%2520Chieh-Yang%2520Huang%2520and%2520Ho%2520Yin%2520Sam%2520Ng%2520and%2520Ryan%2520Rossi%2520and%2520Sungchul%2520Kim%2520and%2520Tong%2520Yu%2520and%2520Lun-Wei%2520Ku%2520and%2520C.%2520Lee%2520Giles%2520and%2520Ting-Hao%2520K.%2520Huang%26entry.1292438233%3D%2520%2520Since%2520the%2520SciCap%2520datasets%2520launch%2520in%25202021%252C%2520the%2520research%2520community%2520has%2520made%250Asignificant%2520progress%2520in%2520generating%2520captions%2520for%2520scientific%2520figures%2520in%2520scholarly%250Aarticles.%2520In%25202023%252C%2520the%2520first%2520SciCap%2520Challenge%2520took%2520place%252C%2520inviting%2520global%2520teams%250Ato%2520use%2520an%2520expanded%2520SciCap%2520dataset%2520to%2520develop%2520models%2520for%2520captioning%2520diverse%250Afigure%2520types%2520across%2520various%2520academic%2520fields.%2520At%2520the%2520same%2520time%252C%2520text%2520generation%250Amodels%2520advanced%2520quickly%252C%2520with%2520many%2520powerful%2520pre-trained%2520large%2520multimodal%2520models%250A%2528LMMs%2529%2520emerging%2520that%2520showed%2520impressive%2520capabilities%2520in%2520various%250Avision-and-language%2520tasks.%2520This%2520paper%2520presents%2520an%2520overview%2520of%2520the%2520first%2520SciCap%250AChallenge%2520and%2520details%2520the%2520performance%2520of%2520various%2520models%2520on%2520its%2520data%252C%2520capturing%250Aa%2520snapshot%2520of%2520the%2520fields%2520state.%2520We%2520found%2520that%2520professional%2520editors%250Aoverwhelmingly%2520preferred%2520figure%2520captions%2520generated%2520by%2520GPT-4V%2520over%2520those%2520from%250Aall%2520other%2520models%2520and%2520even%2520the%2520original%2520captions%2520written%2520by%2520authors.%2520Following%250Athis%2520key%2520finding%252C%2520we%2520conducted%2520detailed%2520analyses%2520to%2520answer%2520this%2520question%253A%2520Have%250Aadvanced%2520LMMs%2520solved%2520the%2520task%2520of%2520generating%2520captions%2520for%2520scientific%2520figures%253F%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19353v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Multimodal%20Models%20Solve%20Caption%20Generation%20for%20Scientific%0A%20%20Figures%3F%20Lessons%20Learned%20from%20SciCap%20Challenge%202023&entry.906535625=Ting-Yao%20E.%20Hsu%20and%20Yi-Li%20Hsu%20and%20Shaurya%20Rohatgi%20and%20Chieh-Yang%20Huang%20and%20Ho%20Yin%20Sam%20Ng%20and%20Ryan%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20Lun-Wei%20Ku%20and%20C.%20Lee%20Giles%20and%20Ting-Hao%20K.%20Huang&entry.1292438233=%20%20Since%20the%20SciCap%20datasets%20launch%20in%202021%2C%20the%20research%20community%20has%20made%0Asignificant%20progress%20in%20generating%20captions%20for%20scientific%20figures%20in%20scholarly%0Aarticles.%20In%202023%2C%20the%20first%20SciCap%20Challenge%20took%20place%2C%20inviting%20global%20teams%0Ato%20use%20an%20expanded%20SciCap%20dataset%20to%20develop%20models%20for%20captioning%20diverse%0Afigure%20types%20across%20various%20academic%20fields.%20At%20the%20same%20time%2C%20text%20generation%0Amodels%20advanced%20quickly%2C%20with%20many%20powerful%20pre-trained%20large%20multimodal%20models%0A%28LMMs%29%20emerging%20that%20showed%20impressive%20capabilities%20in%20various%0Avision-and-language%20tasks.%20This%20paper%20presents%20an%20overview%20of%20the%20first%20SciCap%0AChallenge%20and%20details%20the%20performance%20of%20various%20models%20on%20its%20data%2C%20capturing%0Aa%20snapshot%20of%20the%20fields%20state.%20We%20found%20that%20professional%20editors%0Aoverwhelmingly%20preferred%20figure%20captions%20generated%20by%20GPT-4V%20over%20those%20from%0Aall%20other%20models%20and%20even%20the%20original%20captions%20written%20by%20authors.%20Following%0Athis%20key%20finding%2C%20we%20conducted%20detailed%20analyses%20to%20answer%20this%20question%3A%20Have%0Aadvanced%20LMMs%20solved%20the%20task%20of%20generating%20captions%20for%20scientific%20figures%3F%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19353v3&entry.124074799=Read"},
{"title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models", "author": "Rubing Lu and Jo\u00e3o Sedoc and Arun Sundararajan", "abstract": "  When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.\n", "link": "http://arxiv.org/abs/2502.12825v1", "date": "2025-02-18", "relevancy": 2.1362, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20and%20the%20Trusting%20Behavior%20of%20DeepSeek%20and%20GPT%3A%20An%20Experiment%0A%20%20Revealing%20Hidden%20Fault%20Lines%20in%20Large%20Language%20Models&body=Title%3A%20Reasoning%20and%20the%20Trusting%20Behavior%20of%20DeepSeek%20and%20GPT%3A%20An%20Experiment%0A%20%20Revealing%20Hidden%20Fault%20Lines%20in%20Large%20Language%20Models%0AAuthor%3A%20Rubing%20Lu%20and%20Jo%C3%A3o%20Sedoc%20and%20Arun%20Sundararajan%0AAbstract%3A%20%20%20When%20encountering%20increasingly%20frequent%20performance%20improvements%20or%20cost%0Areductions%20from%20a%20new%20large%20language%20model%20%28LLM%29%2C%20developers%20of%20applications%0Aleveraging%20LLMs%20must%20decide%20whether%20to%20take%20advantage%20of%20these%20improvements%20or%0Astay%20with%20older%20tried-and-tested%20models.%20Low%20perceived%20switching%20frictions%20can%0Alead%20to%20choices%20that%20do%20not%20consider%20more%20subtle%20behavior%20changes%20that%20the%0Atransition%20may%20induce.%20Our%20experiments%20use%20a%20popular%20game-theoretic%20behavioral%0Aeconomics%20model%20of%20trust%20to%20show%20stark%20differences%20in%20the%20trusting%20behavior%20of%0AOpenAI%27s%20and%20DeepSeek%27s%20models.%20We%20highlight%20a%20collapse%20in%20the%20economic%20trust%0Abehavior%20of%20the%20o1-mini%20and%20o3-mini%20models%20as%20they%20reconcile%20profit-maximizing%0Aand%20risk-seeking%20with%20future%20returns%20from%20trust%2C%20and%20contrast%20it%20with%0ADeepSeek%27s%20more%20sophisticated%20and%20profitable%20trusting%20behavior%20that%20stems%20from%0Aan%20ability%20to%20incorporate%20deeper%20concepts%20like%20forward%20planning%20and%0Atheory-of-mind.%20As%20LLMs%20form%20the%20basis%20for%20high-stakes%20commercial%20systems%2C%20our%0Aresults%20highlight%20the%20perils%20of%20relying%20on%20LLM%20performance%20benchmarks%20that%20are%0Atoo%20narrowly%20defined%20and%20suggest%20that%20careful%20analysis%20of%20their%20hidden%20fault%0Alines%20should%20be%20part%20of%20any%20organization%27s%20AI%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520and%2520the%2520Trusting%2520Behavior%2520of%2520DeepSeek%2520and%2520GPT%253A%2520An%2520Experiment%250A%2520%2520Revealing%2520Hidden%2520Fault%2520Lines%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DRubing%2520Lu%2520and%2520Jo%25C3%25A3o%2520Sedoc%2520and%2520Arun%2520Sundararajan%26entry.1292438233%3D%2520%2520When%2520encountering%2520increasingly%2520frequent%2520performance%2520improvements%2520or%2520cost%250Areductions%2520from%2520a%2520new%2520large%2520language%2520model%2520%2528LLM%2529%252C%2520developers%2520of%2520applications%250Aleveraging%2520LLMs%2520must%2520decide%2520whether%2520to%2520take%2520advantage%2520of%2520these%2520improvements%2520or%250Astay%2520with%2520older%2520tried-and-tested%2520models.%2520Low%2520perceived%2520switching%2520frictions%2520can%250Alead%2520to%2520choices%2520that%2520do%2520not%2520consider%2520more%2520subtle%2520behavior%2520changes%2520that%2520the%250Atransition%2520may%2520induce.%2520Our%2520experiments%2520use%2520a%2520popular%2520game-theoretic%2520behavioral%250Aeconomics%2520model%2520of%2520trust%2520to%2520show%2520stark%2520differences%2520in%2520the%2520trusting%2520behavior%2520of%250AOpenAI%2527s%2520and%2520DeepSeek%2527s%2520models.%2520We%2520highlight%2520a%2520collapse%2520in%2520the%2520economic%2520trust%250Abehavior%2520of%2520the%2520o1-mini%2520and%2520o3-mini%2520models%2520as%2520they%2520reconcile%2520profit-maximizing%250Aand%2520risk-seeking%2520with%2520future%2520returns%2520from%2520trust%252C%2520and%2520contrast%2520it%2520with%250ADeepSeek%2527s%2520more%2520sophisticated%2520and%2520profitable%2520trusting%2520behavior%2520that%2520stems%2520from%250Aan%2520ability%2520to%2520incorporate%2520deeper%2520concepts%2520like%2520forward%2520planning%2520and%250Atheory-of-mind.%2520As%2520LLMs%2520form%2520the%2520basis%2520for%2520high-stakes%2520commercial%2520systems%252C%2520our%250Aresults%2520highlight%2520the%2520perils%2520of%2520relying%2520on%2520LLM%2520performance%2520benchmarks%2520that%2520are%250Atoo%2520narrowly%2520defined%2520and%2520suggest%2520that%2520careful%2520analysis%2520of%2520their%2520hidden%2520fault%250Alines%2520should%2520be%2520part%2520of%2520any%2520organization%2527s%2520AI%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20and%20the%20Trusting%20Behavior%20of%20DeepSeek%20and%20GPT%3A%20An%20Experiment%0A%20%20Revealing%20Hidden%20Fault%20Lines%20in%20Large%20Language%20Models&entry.906535625=Rubing%20Lu%20and%20Jo%C3%A3o%20Sedoc%20and%20Arun%20Sundararajan&entry.1292438233=%20%20When%20encountering%20increasingly%20frequent%20performance%20improvements%20or%20cost%0Areductions%20from%20a%20new%20large%20language%20model%20%28LLM%29%2C%20developers%20of%20applications%0Aleveraging%20LLMs%20must%20decide%20whether%20to%20take%20advantage%20of%20these%20improvements%20or%0Astay%20with%20older%20tried-and-tested%20models.%20Low%20perceived%20switching%20frictions%20can%0Alead%20to%20choices%20that%20do%20not%20consider%20more%20subtle%20behavior%20changes%20that%20the%0Atransition%20may%20induce.%20Our%20experiments%20use%20a%20popular%20game-theoretic%20behavioral%0Aeconomics%20model%20of%20trust%20to%20show%20stark%20differences%20in%20the%20trusting%20behavior%20of%0AOpenAI%27s%20and%20DeepSeek%27s%20models.%20We%20highlight%20a%20collapse%20in%20the%20economic%20trust%0Abehavior%20of%20the%20o1-mini%20and%20o3-mini%20models%20as%20they%20reconcile%20profit-maximizing%0Aand%20risk-seeking%20with%20future%20returns%20from%20trust%2C%20and%20contrast%20it%20with%0ADeepSeek%27s%20more%20sophisticated%20and%20profitable%20trusting%20behavior%20that%20stems%20from%0Aan%20ability%20to%20incorporate%20deeper%20concepts%20like%20forward%20planning%20and%0Atheory-of-mind.%20As%20LLMs%20form%20the%20basis%20for%20high-stakes%20commercial%20systems%2C%20our%0Aresults%20highlight%20the%20perils%20of%20relying%20on%20LLM%20performance%20benchmarks%20that%20are%0Atoo%20narrowly%20defined%20and%20suggest%20that%20careful%20analysis%20of%20their%20hidden%20fault%0Alines%20should%20be%20part%20of%20any%20organization%27s%20AI%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12825v1&entry.124074799=Read"},
{"title": "Approximate Tree Completion and Learning-Augmented Algorithms for Metric\n  Minimum Spanning Trees", "author": "Nate Veldt and Thomas Stanley and Benjamin W. Priest and Trevor Steil and Keita Iwabuchi and T. S. Jayram and Geoffrey Sanders", "abstract": "  Finding a minimum spanning tree (MST) for $n$ points in an arbitrary metric\nspace is a fundamental primitive for hierarchical clustering and many other ML\ntasks, but this takes $\\Omega(n^2)$ time to even approximate. We introduce a\nframework for metric MSTs that first (1) finds a forest of disconnected\ncomponents using practical heuristics, and then (2) finds a small weight set of\nedges to connect disjoint components of the forest into a spanning tree. We\nprove that optimally solving the second step still takes $\\Omega(n^2)$ time,\nbut we provide a subquadratic 2.62-approximation algorithm. In the spirit of\nlearning-augmented algorithms, we then show that if the forest found in step\n(1) overlaps with an optimal MST, we can approximate the original MST problem\nin subquadratic time, where the approximation factor depends on a measure of\noverlap. In practice, we find nearly optimal spanning trees for a wide range of\nmetrics, while being orders of magnitude faster than exact algorithms.\n", "link": "http://arxiv.org/abs/2502.12993v1", "date": "2025-02-18", "relevancy": 2.1276, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4441}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4188}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximate%20Tree%20Completion%20and%20Learning-Augmented%20Algorithms%20for%20Metric%0A%20%20Minimum%20Spanning%20Trees&body=Title%3A%20Approximate%20Tree%20Completion%20and%20Learning-Augmented%20Algorithms%20for%20Metric%0A%20%20Minimum%20Spanning%20Trees%0AAuthor%3A%20Nate%20Veldt%20and%20Thomas%20Stanley%20and%20Benjamin%20W.%20Priest%20and%20Trevor%20Steil%20and%20Keita%20Iwabuchi%20and%20T.%20S.%20Jayram%20and%20Geoffrey%20Sanders%0AAbstract%3A%20%20%20Finding%20a%20minimum%20spanning%20tree%20%28MST%29%20for%20%24n%24%20points%20in%20an%20arbitrary%20metric%0Aspace%20is%20a%20fundamental%20primitive%20for%20hierarchical%20clustering%20and%20many%20other%20ML%0Atasks%2C%20but%20this%20takes%20%24%5COmega%28n%5E2%29%24%20time%20to%20even%20approximate.%20We%20introduce%20a%0Aframework%20for%20metric%20MSTs%20that%20first%20%281%29%20finds%20a%20forest%20of%20disconnected%0Acomponents%20using%20practical%20heuristics%2C%20and%20then%20%282%29%20finds%20a%20small%20weight%20set%20of%0Aedges%20to%20connect%20disjoint%20components%20of%20the%20forest%20into%20a%20spanning%20tree.%20We%0Aprove%20that%20optimally%20solving%20the%20second%20step%20still%20takes%20%24%5COmega%28n%5E2%29%24%20time%2C%0Abut%20we%20provide%20a%20subquadratic%202.62-approximation%20algorithm.%20In%20the%20spirit%20of%0Alearning-augmented%20algorithms%2C%20we%20then%20show%20that%20if%20the%20forest%20found%20in%20step%0A%281%29%20overlaps%20with%20an%20optimal%20MST%2C%20we%20can%20approximate%20the%20original%20MST%20problem%0Ain%20subquadratic%20time%2C%20where%20the%20approximation%20factor%20depends%20on%20a%20measure%20of%0Aoverlap.%20In%20practice%2C%20we%20find%20nearly%20optimal%20spanning%20trees%20for%20a%20wide%20range%20of%0Ametrics%2C%20while%20being%20orders%20of%20magnitude%20faster%20than%20exact%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximate%2520Tree%2520Completion%2520and%2520Learning-Augmented%2520Algorithms%2520for%2520Metric%250A%2520%2520Minimum%2520Spanning%2520Trees%26entry.906535625%3DNate%2520Veldt%2520and%2520Thomas%2520Stanley%2520and%2520Benjamin%2520W.%2520Priest%2520and%2520Trevor%2520Steil%2520and%2520Keita%2520Iwabuchi%2520and%2520T.%2520S.%2520Jayram%2520and%2520Geoffrey%2520Sanders%26entry.1292438233%3D%2520%2520Finding%2520a%2520minimum%2520spanning%2520tree%2520%2528MST%2529%2520for%2520%2524n%2524%2520points%2520in%2520an%2520arbitrary%2520metric%250Aspace%2520is%2520a%2520fundamental%2520primitive%2520for%2520hierarchical%2520clustering%2520and%2520many%2520other%2520ML%250Atasks%252C%2520but%2520this%2520takes%2520%2524%255COmega%2528n%255E2%2529%2524%2520time%2520to%2520even%2520approximate.%2520We%2520introduce%2520a%250Aframework%2520for%2520metric%2520MSTs%2520that%2520first%2520%25281%2529%2520finds%2520a%2520forest%2520of%2520disconnected%250Acomponents%2520using%2520practical%2520heuristics%252C%2520and%2520then%2520%25282%2529%2520finds%2520a%2520small%2520weight%2520set%2520of%250Aedges%2520to%2520connect%2520disjoint%2520components%2520of%2520the%2520forest%2520into%2520a%2520spanning%2520tree.%2520We%250Aprove%2520that%2520optimally%2520solving%2520the%2520second%2520step%2520still%2520takes%2520%2524%255COmega%2528n%255E2%2529%2524%2520time%252C%250Abut%2520we%2520provide%2520a%2520subquadratic%25202.62-approximation%2520algorithm.%2520In%2520the%2520spirit%2520of%250Alearning-augmented%2520algorithms%252C%2520we%2520then%2520show%2520that%2520if%2520the%2520forest%2520found%2520in%2520step%250A%25281%2529%2520overlaps%2520with%2520an%2520optimal%2520MST%252C%2520we%2520can%2520approximate%2520the%2520original%2520MST%2520problem%250Ain%2520subquadratic%2520time%252C%2520where%2520the%2520approximation%2520factor%2520depends%2520on%2520a%2520measure%2520of%250Aoverlap.%2520In%2520practice%252C%2520we%2520find%2520nearly%2520optimal%2520spanning%2520trees%2520for%2520a%2520wide%2520range%2520of%250Ametrics%252C%2520while%2520being%2520orders%2520of%2520magnitude%2520faster%2520than%2520exact%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximate%20Tree%20Completion%20and%20Learning-Augmented%20Algorithms%20for%20Metric%0A%20%20Minimum%20Spanning%20Trees&entry.906535625=Nate%20Veldt%20and%20Thomas%20Stanley%20and%20Benjamin%20W.%20Priest%20and%20Trevor%20Steil%20and%20Keita%20Iwabuchi%20and%20T.%20S.%20Jayram%20and%20Geoffrey%20Sanders&entry.1292438233=%20%20Finding%20a%20minimum%20spanning%20tree%20%28MST%29%20for%20%24n%24%20points%20in%20an%20arbitrary%20metric%0Aspace%20is%20a%20fundamental%20primitive%20for%20hierarchical%20clustering%20and%20many%20other%20ML%0Atasks%2C%20but%20this%20takes%20%24%5COmega%28n%5E2%29%24%20time%20to%20even%20approximate.%20We%20introduce%20a%0Aframework%20for%20metric%20MSTs%20that%20first%20%281%29%20finds%20a%20forest%20of%20disconnected%0Acomponents%20using%20practical%20heuristics%2C%20and%20then%20%282%29%20finds%20a%20small%20weight%20set%20of%0Aedges%20to%20connect%20disjoint%20components%20of%20the%20forest%20into%20a%20spanning%20tree.%20We%0Aprove%20that%20optimally%20solving%20the%20second%20step%20still%20takes%20%24%5COmega%28n%5E2%29%24%20time%2C%0Abut%20we%20provide%20a%20subquadratic%202.62-approximation%20algorithm.%20In%20the%20spirit%20of%0Alearning-augmented%20algorithms%2C%20we%20then%20show%20that%20if%20the%20forest%20found%20in%20step%0A%281%29%20overlaps%20with%20an%20optimal%20MST%2C%20we%20can%20approximate%20the%20original%20MST%20problem%0Ain%20subquadratic%20time%2C%20where%20the%20approximation%20factor%20depends%20on%20a%20measure%20of%0Aoverlap.%20In%20practice%2C%20we%20find%20nearly%20optimal%20spanning%20trees%20for%20a%20wide%20range%20of%0Ametrics%2C%20while%20being%20orders%20of%20magnitude%20faster%20than%20exact%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12993v1&entry.124074799=Read"},
{"title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular\n  Micro-video Generation", "author": "Junchen Fu and Xuri Ge and Kaiwen Zheng and Ioannis Arapakis and Xin Xin and Joemon M. Jose", "abstract": "  Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold\nsignificant commercial value. The rise of high-quality AI-generated content has\nspurred interest in AI-driven micro-video creation. However, despite the\nadvanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek\nin text generation and reasoning, their potential to assist the creation of\npopular micro-videos remains largely unexplored.\n  In this paper, we conduct an empirical study on LLM-assisted popular\nmicro-video generation (LLMPopcorn). Specifically, we investigate the following\nresearch questions: (i) How can LLMs be effectively utilized to assist popular\nmicro-video generation? (ii) To what extent can prompt-based enhancements\noptimize the LLM-generated content for higher popularity? (iii) How well do\nvarious LLMs and video generators perform in the popular micro-video generation\ntask? By exploring these questions, we show that advanced LLMs like DeepSeek-V3\nenable micro-video generation to achieve popularity comparable to human-created\ncontent. Prompt enhancements further boost popularity, and benchmarking\nhighlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and\nHunyuanVideo lead in video generation. This pioneering work advances\nAI-assisted micro-video creation, uncovering new research opportunities. We\nwill release the code and datasets to support future studies.\n", "link": "http://arxiv.org/abs/2502.12945v1", "date": "2025-02-18", "relevancy": 2.1254, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5623}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5192}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMPopcorn%3A%20An%20Empirical%20Study%20of%20LLMs%20as%20Assistants%20for%20Popular%0A%20%20Micro-video%20Generation&body=Title%3A%20LLMPopcorn%3A%20An%20Empirical%20Study%20of%20LLMs%20as%20Assistants%20for%20Popular%0A%20%20Micro-video%20Generation%0AAuthor%3A%20Junchen%20Fu%20and%20Xuri%20Ge%20and%20Kaiwen%20Zheng%20and%20Ioannis%20Arapakis%20and%20Xin%20Xin%20and%20Joemon%20M.%20Jose%0AAbstract%3A%20%20%20Popular%20Micro-videos%2C%20dominant%20on%20platforms%20like%20TikTok%20and%20YouTube%2C%20hold%0Asignificant%20commercial%20value.%20The%20rise%20of%20high-quality%20AI-generated%20content%20has%0Aspurred%20interest%20in%20AI-driven%20micro-video%20creation.%20However%2C%20despite%20the%0Aadvanced%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20and%20DeepSeek%0Ain%20text%20generation%20and%20reasoning%2C%20their%20potential%20to%20assist%20the%20creation%20of%0Apopular%20micro-videos%20remains%20largely%20unexplored.%0A%20%20In%20this%20paper%2C%20we%20conduct%20an%20empirical%20study%20on%20LLM-assisted%20popular%0Amicro-video%20generation%20%28LLMPopcorn%29.%20Specifically%2C%20we%20investigate%20the%20following%0Aresearch%20questions%3A%20%28i%29%20How%20can%20LLMs%20be%20effectively%20utilized%20to%20assist%20popular%0Amicro-video%20generation%3F%20%28ii%29%20To%20what%20extent%20can%20prompt-based%20enhancements%0Aoptimize%20the%20LLM-generated%20content%20for%20higher%20popularity%3F%20%28iii%29%20How%20well%20do%0Avarious%20LLMs%20and%20video%20generators%20perform%20in%20the%20popular%20micro-video%20generation%0Atask%3F%20By%20exploring%20these%20questions%2C%20we%20show%20that%20advanced%20LLMs%20like%20DeepSeek-V3%0Aenable%20micro-video%20generation%20to%20achieve%20popularity%20comparable%20to%20human-created%0Acontent.%20Prompt%20enhancements%20further%20boost%20popularity%2C%20and%20benchmarking%0Ahighlights%20DeepSeek-V3%20and%20DeepSeek-R1%20among%20LLMs%2C%20while%20LTX-Video%20and%0AHunyuanVideo%20lead%20in%20video%20generation.%20This%20pioneering%20work%20advances%0AAI-assisted%20micro-video%20creation%2C%20uncovering%20new%20research%20opportunities.%20We%0Awill%20release%20the%20code%20and%20datasets%20to%20support%20future%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMPopcorn%253A%2520An%2520Empirical%2520Study%2520of%2520LLMs%2520as%2520Assistants%2520for%2520Popular%250A%2520%2520Micro-video%2520Generation%26entry.906535625%3DJunchen%2520Fu%2520and%2520Xuri%2520Ge%2520and%2520Kaiwen%2520Zheng%2520and%2520Ioannis%2520Arapakis%2520and%2520Xin%2520Xin%2520and%2520Joemon%2520M.%2520Jose%26entry.1292438233%3D%2520%2520Popular%2520Micro-videos%252C%2520dominant%2520on%2520platforms%2520like%2520TikTok%2520and%2520YouTube%252C%2520hold%250Asignificant%2520commercial%2520value.%2520The%2520rise%2520of%2520high-quality%2520AI-generated%2520content%2520has%250Aspurred%2520interest%2520in%2520AI-driven%2520micro-video%2520creation.%2520However%252C%2520despite%2520the%250Aadvanced%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520like%2520ChatGPT%2520and%2520DeepSeek%250Ain%2520text%2520generation%2520and%2520reasoning%252C%2520their%2520potential%2520to%2520assist%2520the%2520creation%2520of%250Apopular%2520micro-videos%2520remains%2520largely%2520unexplored.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520conduct%2520an%2520empirical%2520study%2520on%2520LLM-assisted%2520popular%250Amicro-video%2520generation%2520%2528LLMPopcorn%2529.%2520Specifically%252C%2520we%2520investigate%2520the%2520following%250Aresearch%2520questions%253A%2520%2528i%2529%2520How%2520can%2520LLMs%2520be%2520effectively%2520utilized%2520to%2520assist%2520popular%250Amicro-video%2520generation%253F%2520%2528ii%2529%2520To%2520what%2520extent%2520can%2520prompt-based%2520enhancements%250Aoptimize%2520the%2520LLM-generated%2520content%2520for%2520higher%2520popularity%253F%2520%2528iii%2529%2520How%2520well%2520do%250Avarious%2520LLMs%2520and%2520video%2520generators%2520perform%2520in%2520the%2520popular%2520micro-video%2520generation%250Atask%253F%2520By%2520exploring%2520these%2520questions%252C%2520we%2520show%2520that%2520advanced%2520LLMs%2520like%2520DeepSeek-V3%250Aenable%2520micro-video%2520generation%2520to%2520achieve%2520popularity%2520comparable%2520to%2520human-created%250Acontent.%2520Prompt%2520enhancements%2520further%2520boost%2520popularity%252C%2520and%2520benchmarking%250Ahighlights%2520DeepSeek-V3%2520and%2520DeepSeek-R1%2520among%2520LLMs%252C%2520while%2520LTX-Video%2520and%250AHunyuanVideo%2520lead%2520in%2520video%2520generation.%2520This%2520pioneering%2520work%2520advances%250AAI-assisted%2520micro-video%2520creation%252C%2520uncovering%2520new%2520research%2520opportunities.%2520We%250Awill%2520release%2520the%2520code%2520and%2520datasets%2520to%2520support%2520future%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMPopcorn%3A%20An%20Empirical%20Study%20of%20LLMs%20as%20Assistants%20for%20Popular%0A%20%20Micro-video%20Generation&entry.906535625=Junchen%20Fu%20and%20Xuri%20Ge%20and%20Kaiwen%20Zheng%20and%20Ioannis%20Arapakis%20and%20Xin%20Xin%20and%20Joemon%20M.%20Jose&entry.1292438233=%20%20Popular%20Micro-videos%2C%20dominant%20on%20platforms%20like%20TikTok%20and%20YouTube%2C%20hold%0Asignificant%20commercial%20value.%20The%20rise%20of%20high-quality%20AI-generated%20content%20has%0Aspurred%20interest%20in%20AI-driven%20micro-video%20creation.%20However%2C%20despite%20the%0Aadvanced%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20and%20DeepSeek%0Ain%20text%20generation%20and%20reasoning%2C%20their%20potential%20to%20assist%20the%20creation%20of%0Apopular%20micro-videos%20remains%20largely%20unexplored.%0A%20%20In%20this%20paper%2C%20we%20conduct%20an%20empirical%20study%20on%20LLM-assisted%20popular%0Amicro-video%20generation%20%28LLMPopcorn%29.%20Specifically%2C%20we%20investigate%20the%20following%0Aresearch%20questions%3A%20%28i%29%20How%20can%20LLMs%20be%20effectively%20utilized%20to%20assist%20popular%0Amicro-video%20generation%3F%20%28ii%29%20To%20what%20extent%20can%20prompt-based%20enhancements%0Aoptimize%20the%20LLM-generated%20content%20for%20higher%20popularity%3F%20%28iii%29%20How%20well%20do%0Avarious%20LLMs%20and%20video%20generators%20perform%20in%20the%20popular%20micro-video%20generation%0Atask%3F%20By%20exploring%20these%20questions%2C%20we%20show%20that%20advanced%20LLMs%20like%20DeepSeek-V3%0Aenable%20micro-video%20generation%20to%20achieve%20popularity%20comparable%20to%20human-created%0Acontent.%20Prompt%20enhancements%20further%20boost%20popularity%2C%20and%20benchmarking%0Ahighlights%20DeepSeek-V3%20and%20DeepSeek-R1%20among%20LLMs%2C%20while%20LTX-Video%20and%0AHunyuanVideo%20lead%20in%20video%20generation.%20This%20pioneering%20work%20advances%0AAI-assisted%20micro-video%20creation%2C%20uncovering%20new%20research%20opportunities.%20We%0Awill%20release%20the%20code%20and%20datasets%20to%20support%20future%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12945v1&entry.124074799=Read"},
{"title": "PTQ4RIS: Post-Training Quantization for Referring Image Segmentation", "author": "Xiaoyan Jiang and Hang Yang and Kaiying Zhu and Xihe Qiu and Shibo Zhao and Sifan Zhou", "abstract": "  Referring Image Segmentation (RIS), aims to segment the object referred by a\ngiven sentence in an image by understanding both visual and linguistic\ninformation. However, existing RIS methods tend to explore top-performance\nmodels, disregarding considerations for practical applications on\nresources-limited edge devices. This oversight poses a significant challenge\nfor on-device RIS inference. To this end, we propose an effective and efficient\npost-training quantization framework termed PTQ4RIS. Specifically, we first\nconduct an in-depth analysis of the root causes of performance degradation in\nRIS model quantization and propose dual-region quantization (DRQ) and\nreorder-based outlier-retained quantization (RORQ) to address the quantization\ndifficulties in visual and text encoders. Extensive experiments on three\nbenchmarks with different bits settings (from 8 to 4 bits) demonstrates its\nsuperior performance. Importantly, we are the first PTQ method specifically\ndesigned for the RIS task, highlighting the feasibility of PTQ in RIS\napplications. Code and video are available at\n{https://github.com/gugu511yy/PTQ4RIS}.\n", "link": "http://arxiv.org/abs/2409.17020v2", "date": "2025-02-18", "relevancy": 2.1138, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5462}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5175}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PTQ4RIS%3A%20Post-Training%20Quantization%20for%20Referring%20Image%20Segmentation&body=Title%3A%20PTQ4RIS%3A%20Post-Training%20Quantization%20for%20Referring%20Image%20Segmentation%0AAuthor%3A%20Xiaoyan%20Jiang%20and%20Hang%20Yang%20and%20Kaiying%20Zhu%20and%20Xihe%20Qiu%20and%20Shibo%20Zhao%20and%20Sifan%20Zhou%0AAbstract%3A%20%20%20Referring%20Image%20Segmentation%20%28RIS%29%2C%20aims%20to%20segment%20the%20object%20referred%20by%20a%0Agiven%20sentence%20in%20an%20image%20by%20understanding%20both%20visual%20and%20linguistic%0Ainformation.%20However%2C%20existing%20RIS%20methods%20tend%20to%20explore%20top-performance%0Amodels%2C%20disregarding%20considerations%20for%20practical%20applications%20on%0Aresources-limited%20edge%20devices.%20This%20oversight%20poses%20a%20significant%20challenge%0Afor%20on-device%20RIS%20inference.%20To%20this%20end%2C%20we%20propose%20an%20effective%20and%20efficient%0Apost-training%20quantization%20framework%20termed%20PTQ4RIS.%20Specifically%2C%20we%20first%0Aconduct%20an%20in-depth%20analysis%20of%20the%20root%20causes%20of%20performance%20degradation%20in%0ARIS%20model%20quantization%20and%20propose%20dual-region%20quantization%20%28DRQ%29%20and%0Areorder-based%20outlier-retained%20quantization%20%28RORQ%29%20to%20address%20the%20quantization%0Adifficulties%20in%20visual%20and%20text%20encoders.%20Extensive%20experiments%20on%20three%0Abenchmarks%20with%20different%20bits%20settings%20%28from%208%20to%204%20bits%29%20demonstrates%20its%0Asuperior%20performance.%20Importantly%2C%20we%20are%20the%20first%20PTQ%20method%20specifically%0Adesigned%20for%20the%20RIS%20task%2C%20highlighting%20the%20feasibility%20of%20PTQ%20in%20RIS%0Aapplications.%20Code%20and%20video%20are%20available%20at%0A%7Bhttps%3A//github.com/gugu511yy/PTQ4RIS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPTQ4RIS%253A%2520Post-Training%2520Quantization%2520for%2520Referring%2520Image%2520Segmentation%26entry.906535625%3DXiaoyan%2520Jiang%2520and%2520Hang%2520Yang%2520and%2520Kaiying%2520Zhu%2520and%2520Xihe%2520Qiu%2520and%2520Shibo%2520Zhao%2520and%2520Sifan%2520Zhou%26entry.1292438233%3D%2520%2520Referring%2520Image%2520Segmentation%2520%2528RIS%2529%252C%2520aims%2520to%2520segment%2520the%2520object%2520referred%2520by%2520a%250Agiven%2520sentence%2520in%2520an%2520image%2520by%2520understanding%2520both%2520visual%2520and%2520linguistic%250Ainformation.%2520However%252C%2520existing%2520RIS%2520methods%2520tend%2520to%2520explore%2520top-performance%250Amodels%252C%2520disregarding%2520considerations%2520for%2520practical%2520applications%2520on%250Aresources-limited%2520edge%2520devices.%2520This%2520oversight%2520poses%2520a%2520significant%2520challenge%250Afor%2520on-device%2520RIS%2520inference.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520effective%2520and%2520efficient%250Apost-training%2520quantization%2520framework%2520termed%2520PTQ4RIS.%2520Specifically%252C%2520we%2520first%250Aconduct%2520an%2520in-depth%2520analysis%2520of%2520the%2520root%2520causes%2520of%2520performance%2520degradation%2520in%250ARIS%2520model%2520quantization%2520and%2520propose%2520dual-region%2520quantization%2520%2528DRQ%2529%2520and%250Areorder-based%2520outlier-retained%2520quantization%2520%2528RORQ%2529%2520to%2520address%2520the%2520quantization%250Adifficulties%2520in%2520visual%2520and%2520text%2520encoders.%2520Extensive%2520experiments%2520on%2520three%250Abenchmarks%2520with%2520different%2520bits%2520settings%2520%2528from%25208%2520to%25204%2520bits%2529%2520demonstrates%2520its%250Asuperior%2520performance.%2520Importantly%252C%2520we%2520are%2520the%2520first%2520PTQ%2520method%2520specifically%250Adesigned%2520for%2520the%2520RIS%2520task%252C%2520highlighting%2520the%2520feasibility%2520of%2520PTQ%2520in%2520RIS%250Aapplications.%2520Code%2520and%2520video%2520are%2520available%2520at%250A%257Bhttps%253A//github.com/gugu511yy/PTQ4RIS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PTQ4RIS%3A%20Post-Training%20Quantization%20for%20Referring%20Image%20Segmentation&entry.906535625=Xiaoyan%20Jiang%20and%20Hang%20Yang%20and%20Kaiying%20Zhu%20and%20Xihe%20Qiu%20and%20Shibo%20Zhao%20and%20Sifan%20Zhou&entry.1292438233=%20%20Referring%20Image%20Segmentation%20%28RIS%29%2C%20aims%20to%20segment%20the%20object%20referred%20by%20a%0Agiven%20sentence%20in%20an%20image%20by%20understanding%20both%20visual%20and%20linguistic%0Ainformation.%20However%2C%20existing%20RIS%20methods%20tend%20to%20explore%20top-performance%0Amodels%2C%20disregarding%20considerations%20for%20practical%20applications%20on%0Aresources-limited%20edge%20devices.%20This%20oversight%20poses%20a%20significant%20challenge%0Afor%20on-device%20RIS%20inference.%20To%20this%20end%2C%20we%20propose%20an%20effective%20and%20efficient%0Apost-training%20quantization%20framework%20termed%20PTQ4RIS.%20Specifically%2C%20we%20first%0Aconduct%20an%20in-depth%20analysis%20of%20the%20root%20causes%20of%20performance%20degradation%20in%0ARIS%20model%20quantization%20and%20propose%20dual-region%20quantization%20%28DRQ%29%20and%0Areorder-based%20outlier-retained%20quantization%20%28RORQ%29%20to%20address%20the%20quantization%0Adifficulties%20in%20visual%20and%20text%20encoders.%20Extensive%20experiments%20on%20three%0Abenchmarks%20with%20different%20bits%20settings%20%28from%208%20to%204%20bits%29%20demonstrates%20its%0Asuperior%20performance.%20Importantly%2C%20we%20are%20the%20first%20PTQ%20method%20specifically%0Adesigned%20for%20the%20RIS%20task%2C%20highlighting%20the%20feasibility%20of%20PTQ%20in%20RIS%0Aapplications.%20Code%20and%20video%20are%20available%20at%0A%7Bhttps%3A//github.com/gugu511yy/PTQ4RIS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17020v2&entry.124074799=Read"},
{"title": "Enhanced uncertainty quantification variational autoencoders for the\n  solution of Bayesian inverse problems", "author": "Andrea Tonini and Luca Dede'", "abstract": "  Among other uses, neural networks are a powerful tool for solving\ndeterministic and Bayesian inverse problems in real-time. In the Bayesian\nframework, variational autoencoders, a specialized type of neural network,\nenable the estimation of model parameters and their distribution based on\nobservational data allowing to perform real-time inverse uncertainty\nquantification. In this work, we build upon existing research [Goh, H. et al.,\nProceedings of Machine Learning Research, 2022] by proposing a novel loss\nfunction to train variational autoencoders for Bayesian inverse problems. When\nthe forward map is affine, we provide a theoretical proof of the convergence of\nthe latent states of variational autoencoders to the posterior distribution of\nthe model parameters. We validate this theoretical result through numerical\ntests and we compare the proposed variational autoencoder with the existing one\nin the literature. Finally, we test the proposed variational autoencoder on the\nLaplace equation.\n", "link": "http://arxiv.org/abs/2502.13105v1", "date": "2025-02-18", "relevancy": 2.1062, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5349}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.534}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20uncertainty%20quantification%20variational%20autoencoders%20for%20the%0A%20%20solution%20of%20Bayesian%20inverse%20problems&body=Title%3A%20Enhanced%20uncertainty%20quantification%20variational%20autoencoders%20for%20the%0A%20%20solution%20of%20Bayesian%20inverse%20problems%0AAuthor%3A%20Andrea%20Tonini%20and%20Luca%20Dede%27%0AAbstract%3A%20%20%20Among%20other%20uses%2C%20neural%20networks%20are%20a%20powerful%20tool%20for%20solving%0Adeterministic%20and%20Bayesian%20inverse%20problems%20in%20real-time.%20In%20the%20Bayesian%0Aframework%2C%20variational%20autoencoders%2C%20a%20specialized%20type%20of%20neural%20network%2C%0Aenable%20the%20estimation%20of%20model%20parameters%20and%20their%20distribution%20based%20on%0Aobservational%20data%20allowing%20to%20perform%20real-time%20inverse%20uncertainty%0Aquantification.%20In%20this%20work%2C%20we%20build%20upon%20existing%20research%20%5BGoh%2C%20H.%20et%20al.%2C%0AProceedings%20of%20Machine%20Learning%20Research%2C%202022%5D%20by%20proposing%20a%20novel%20loss%0Afunction%20to%20train%20variational%20autoencoders%20for%20Bayesian%20inverse%20problems.%20When%0Athe%20forward%20map%20is%20affine%2C%20we%20provide%20a%20theoretical%20proof%20of%20the%20convergence%20of%0Athe%20latent%20states%20of%20variational%20autoencoders%20to%20the%20posterior%20distribution%20of%0Athe%20model%20parameters.%20We%20validate%20this%20theoretical%20result%20through%20numerical%0Atests%20and%20we%20compare%20the%20proposed%20variational%20autoencoder%20with%20the%20existing%20one%0Ain%20the%20literature.%20Finally%2C%20we%20test%20the%20proposed%20variational%20autoencoder%20on%20the%0ALaplace%20equation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520uncertainty%2520quantification%2520variational%2520autoencoders%2520for%2520the%250A%2520%2520solution%2520of%2520Bayesian%2520inverse%2520problems%26entry.906535625%3DAndrea%2520Tonini%2520and%2520Luca%2520Dede%2527%26entry.1292438233%3D%2520%2520Among%2520other%2520uses%252C%2520neural%2520networks%2520are%2520a%2520powerful%2520tool%2520for%2520solving%250Adeterministic%2520and%2520Bayesian%2520inverse%2520problems%2520in%2520real-time.%2520In%2520the%2520Bayesian%250Aframework%252C%2520variational%2520autoencoders%252C%2520a%2520specialized%2520type%2520of%2520neural%2520network%252C%250Aenable%2520the%2520estimation%2520of%2520model%2520parameters%2520and%2520their%2520distribution%2520based%2520on%250Aobservational%2520data%2520allowing%2520to%2520perform%2520real-time%2520inverse%2520uncertainty%250Aquantification.%2520In%2520this%2520work%252C%2520we%2520build%2520upon%2520existing%2520research%2520%255BGoh%252C%2520H.%2520et%2520al.%252C%250AProceedings%2520of%2520Machine%2520Learning%2520Research%252C%25202022%255D%2520by%2520proposing%2520a%2520novel%2520loss%250Afunction%2520to%2520train%2520variational%2520autoencoders%2520for%2520Bayesian%2520inverse%2520problems.%2520When%250Athe%2520forward%2520map%2520is%2520affine%252C%2520we%2520provide%2520a%2520theoretical%2520proof%2520of%2520the%2520convergence%2520of%250Athe%2520latent%2520states%2520of%2520variational%2520autoencoders%2520to%2520the%2520posterior%2520distribution%2520of%250Athe%2520model%2520parameters.%2520We%2520validate%2520this%2520theoretical%2520result%2520through%2520numerical%250Atests%2520and%2520we%2520compare%2520the%2520proposed%2520variational%2520autoencoder%2520with%2520the%2520existing%2520one%250Ain%2520the%2520literature.%2520Finally%252C%2520we%2520test%2520the%2520proposed%2520variational%2520autoencoder%2520on%2520the%250ALaplace%2520equation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20uncertainty%20quantification%20variational%20autoencoders%20for%20the%0A%20%20solution%20of%20Bayesian%20inverse%20problems&entry.906535625=Andrea%20Tonini%20and%20Luca%20Dede%27&entry.1292438233=%20%20Among%20other%20uses%2C%20neural%20networks%20are%20a%20powerful%20tool%20for%20solving%0Adeterministic%20and%20Bayesian%20inverse%20problems%20in%20real-time.%20In%20the%20Bayesian%0Aframework%2C%20variational%20autoencoders%2C%20a%20specialized%20type%20of%20neural%20network%2C%0Aenable%20the%20estimation%20of%20model%20parameters%20and%20their%20distribution%20based%20on%0Aobservational%20data%20allowing%20to%20perform%20real-time%20inverse%20uncertainty%0Aquantification.%20In%20this%20work%2C%20we%20build%20upon%20existing%20research%20%5BGoh%2C%20H.%20et%20al.%2C%0AProceedings%20of%20Machine%20Learning%20Research%2C%202022%5D%20by%20proposing%20a%20novel%20loss%0Afunction%20to%20train%20variational%20autoencoders%20for%20Bayesian%20inverse%20problems.%20When%0Athe%20forward%20map%20is%20affine%2C%20we%20provide%20a%20theoretical%20proof%20of%20the%20convergence%20of%0Athe%20latent%20states%20of%20variational%20autoencoders%20to%20the%20posterior%20distribution%20of%0Athe%20model%20parameters.%20We%20validate%20this%20theoretical%20result%20through%20numerical%0Atests%20and%20we%20compare%20the%20proposed%20variational%20autoencoder%20with%20the%20existing%20one%0Ain%20the%20literature.%20Finally%2C%20we%20test%20the%20proposed%20variational%20autoencoder%20on%20the%0ALaplace%20equation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13105v1&entry.124074799=Read"},
{"title": "GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS", "author": "Saman Kazemkhani and Aarav Pandya and Daphne Cornelisse and Brennan Shacklett and Eugene Vinitsky", "abstract": "  Multi-agent learning algorithms have been successful at generating superhuman\nplanning in various games but have had limited impact on the design of deployed\nmulti-agent planners. A key bottleneck in applying these techniques to\nmulti-agent planning is that they require billions of steps of experience. To\nenable the study of multi-agent planning at scale, we present GPUDrive.\nGPUDrive is a GPU-accelerated, multi-agent simulator built on top of the\nMadrona Game Engine capable of generating over a million simulation steps per\nsecond. Observation, reward, and dynamics functions are written directly in\nC++, allowing users to define complex, heterogeneous agent behaviors that are\nlowered to high-performance CUDA. Despite these low-level optimizations,\nGPUDrive is fully accessible through Python, offering a seamless and efficient\nworkflow for multi-agent, closed-loop simulation. Using GPUDrive, we train\nreinforcement learning agents on the Waymo Open Motion Dataset, achieving\nefficient goal-reaching in minutes and scaling to thousands of scenarios in\nhours. We open-source the code and pre-trained agents at\nhttps://github.com/Emerge-Lab/gpudrive.\n", "link": "http://arxiv.org/abs/2408.01584v3", "date": "2025-02-18", "relevancy": 2.1008, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5392}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5225}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPUDrive%3A%20Data-driven%2C%20multi-agent%20driving%20simulation%20at%201%20million%20FPS&body=Title%3A%20GPUDrive%3A%20Data-driven%2C%20multi-agent%20driving%20simulation%20at%201%20million%20FPS%0AAuthor%3A%20Saman%20Kazemkhani%20and%20Aarav%20Pandya%20and%20Daphne%20Cornelisse%20and%20Brennan%20Shacklett%20and%20Eugene%20Vinitsky%0AAbstract%3A%20%20%20Multi-agent%20learning%20algorithms%20have%20been%20successful%20at%20generating%20superhuman%0Aplanning%20in%20various%20games%20but%20have%20had%20limited%20impact%20on%20the%20design%20of%20deployed%0Amulti-agent%20planners.%20A%20key%20bottleneck%20in%20applying%20these%20techniques%20to%0Amulti-agent%20planning%20is%20that%20they%20require%20billions%20of%20steps%20of%20experience.%20To%0Aenable%20the%20study%20of%20multi-agent%20planning%20at%20scale%2C%20we%20present%20GPUDrive.%0AGPUDrive%20is%20a%20GPU-accelerated%2C%20multi-agent%20simulator%20built%20on%20top%20of%20the%0AMadrona%20Game%20Engine%20capable%20of%20generating%20over%20a%20million%20simulation%20steps%20per%0Asecond.%20Observation%2C%20reward%2C%20and%20dynamics%20functions%20are%20written%20directly%20in%0AC%2B%2B%2C%20allowing%20users%20to%20define%20complex%2C%20heterogeneous%20agent%20behaviors%20that%20are%0Alowered%20to%20high-performance%20CUDA.%20Despite%20these%20low-level%20optimizations%2C%0AGPUDrive%20is%20fully%20accessible%20through%20Python%2C%20offering%20a%20seamless%20and%20efficient%0Aworkflow%20for%20multi-agent%2C%20closed-loop%20simulation.%20Using%20GPUDrive%2C%20we%20train%0Areinforcement%20learning%20agents%20on%20the%20Waymo%20Open%20Motion%20Dataset%2C%20achieving%0Aefficient%20goal-reaching%20in%20minutes%20and%20scaling%20to%20thousands%20of%20scenarios%20in%0Ahours.%20We%20open-source%20the%20code%20and%20pre-trained%20agents%20at%0Ahttps%3A//github.com/Emerge-Lab/gpudrive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01584v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPUDrive%253A%2520Data-driven%252C%2520multi-agent%2520driving%2520simulation%2520at%25201%2520million%2520FPS%26entry.906535625%3DSaman%2520Kazemkhani%2520and%2520Aarav%2520Pandya%2520and%2520Daphne%2520Cornelisse%2520and%2520Brennan%2520Shacklett%2520and%2520Eugene%2520Vinitsky%26entry.1292438233%3D%2520%2520Multi-agent%2520learning%2520algorithms%2520have%2520been%2520successful%2520at%2520generating%2520superhuman%250Aplanning%2520in%2520various%2520games%2520but%2520have%2520had%2520limited%2520impact%2520on%2520the%2520design%2520of%2520deployed%250Amulti-agent%2520planners.%2520A%2520key%2520bottleneck%2520in%2520applying%2520these%2520techniques%2520to%250Amulti-agent%2520planning%2520is%2520that%2520they%2520require%2520billions%2520of%2520steps%2520of%2520experience.%2520To%250Aenable%2520the%2520study%2520of%2520multi-agent%2520planning%2520at%2520scale%252C%2520we%2520present%2520GPUDrive.%250AGPUDrive%2520is%2520a%2520GPU-accelerated%252C%2520multi-agent%2520simulator%2520built%2520on%2520top%2520of%2520the%250AMadrona%2520Game%2520Engine%2520capable%2520of%2520generating%2520over%2520a%2520million%2520simulation%2520steps%2520per%250Asecond.%2520Observation%252C%2520reward%252C%2520and%2520dynamics%2520functions%2520are%2520written%2520directly%2520in%250AC%252B%252B%252C%2520allowing%2520users%2520to%2520define%2520complex%252C%2520heterogeneous%2520agent%2520behaviors%2520that%2520are%250Alowered%2520to%2520high-performance%2520CUDA.%2520Despite%2520these%2520low-level%2520optimizations%252C%250AGPUDrive%2520is%2520fully%2520accessible%2520through%2520Python%252C%2520offering%2520a%2520seamless%2520and%2520efficient%250Aworkflow%2520for%2520multi-agent%252C%2520closed-loop%2520simulation.%2520Using%2520GPUDrive%252C%2520we%2520train%250Areinforcement%2520learning%2520agents%2520on%2520the%2520Waymo%2520Open%2520Motion%2520Dataset%252C%2520achieving%250Aefficient%2520goal-reaching%2520in%2520minutes%2520and%2520scaling%2520to%2520thousands%2520of%2520scenarios%2520in%250Ahours.%2520We%2520open-source%2520the%2520code%2520and%2520pre-trained%2520agents%2520at%250Ahttps%253A//github.com/Emerge-Lab/gpudrive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01584v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPUDrive%3A%20Data-driven%2C%20multi-agent%20driving%20simulation%20at%201%20million%20FPS&entry.906535625=Saman%20Kazemkhani%20and%20Aarav%20Pandya%20and%20Daphne%20Cornelisse%20and%20Brennan%20Shacklett%20and%20Eugene%20Vinitsky&entry.1292438233=%20%20Multi-agent%20learning%20algorithms%20have%20been%20successful%20at%20generating%20superhuman%0Aplanning%20in%20various%20games%20but%20have%20had%20limited%20impact%20on%20the%20design%20of%20deployed%0Amulti-agent%20planners.%20A%20key%20bottleneck%20in%20applying%20these%20techniques%20to%0Amulti-agent%20planning%20is%20that%20they%20require%20billions%20of%20steps%20of%20experience.%20To%0Aenable%20the%20study%20of%20multi-agent%20planning%20at%20scale%2C%20we%20present%20GPUDrive.%0AGPUDrive%20is%20a%20GPU-accelerated%2C%20multi-agent%20simulator%20built%20on%20top%20of%20the%0AMadrona%20Game%20Engine%20capable%20of%20generating%20over%20a%20million%20simulation%20steps%20per%0Asecond.%20Observation%2C%20reward%2C%20and%20dynamics%20functions%20are%20written%20directly%20in%0AC%2B%2B%2C%20allowing%20users%20to%20define%20complex%2C%20heterogeneous%20agent%20behaviors%20that%20are%0Alowered%20to%20high-performance%20CUDA.%20Despite%20these%20low-level%20optimizations%2C%0AGPUDrive%20is%20fully%20accessible%20through%20Python%2C%20offering%20a%20seamless%20and%20efficient%0Aworkflow%20for%20multi-agent%2C%20closed-loop%20simulation.%20Using%20GPUDrive%2C%20we%20train%0Areinforcement%20learning%20agents%20on%20the%20Waymo%20Open%20Motion%20Dataset%2C%20achieving%0Aefficient%20goal-reaching%20in%20minutes%20and%20scaling%20to%20thousands%20of%20scenarios%20in%0Ahours.%20We%20open-source%20the%20code%20and%20pre-trained%20agents%20at%0Ahttps%3A//github.com/Emerge-Lab/gpudrive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01584v3&entry.124074799=Read"},
{"title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System", "author": "Weize Chen and Jiarui Yuan and Chen Qian and Cheng Yang and Zhiyuan Liu and Maosong Sun", "abstract": "  Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).\n", "link": "http://arxiv.org/abs/2410.08115v2", "date": "2025-02-18", "relevancy": 2.0987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optima%3A%20Optimizing%20Effectiveness%20and%20Efficiency%20for%20LLM-Based%0A%20%20Multi-Agent%20System&body=Title%3A%20Optima%3A%20Optimizing%20Effectiveness%20and%20Efficiency%20for%20LLM-Based%0A%20%20Multi-Agent%20System%0AAuthor%3A%20Weize%20Chen%20and%20Jiarui%20Yuan%20and%20Chen%20Qian%20and%20Cheng%20Yang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20based%20multi-agent%20systems%20%28MAS%29%20show%20remarkable%0Apotential%20in%20collaborative%20problem-solving%2C%20yet%20they%20still%20face%20critical%0Achallenges%3A%20low%20communication%20efficiency%2C%20poor%20scalability%2C%20and%20a%20lack%20of%0Aeffective%20parameter-updating%20optimization%20methods.%20We%20present%20Optima%2C%20a%20novel%0Aframework%20that%20addresses%20these%20issues%20by%20significantly%20enhancing%20both%0Acommunication%20efficiency%20and%20task%20effectiveness%20in%20LLM-based%20MAS%20through%20LLM%0Atraining.%20Optima%20employs%20an%20iterative%20generate%2C%20rank%2C%20select%2C%20and%20train%0Aparadigm%20with%20a%20reward%20function%20balancing%20task%20performance%2C%20token%20efficiency%2C%0Aand%20communication%20readability.%20We%20explore%20various%20RL%20algorithms%2C%20including%0ASupervised%20Fine-Tuning%2C%20Direct%20Preference%20Optimization%2C%20and%20their%20hybrid%0Aapproaches%2C%20providing%20insights%20into%20their%20effectiveness-efficiency%20trade-offs.%0AWe%20integrate%20Monte%20Carlo%20Tree%20Search-inspired%20techniques%20for%20DPO%20data%0Ageneration%2C%20treating%20conversation%20turns%20as%20tree%20nodes%20to%20explore%20diverse%0Ainteraction%20paths.%20Evaluated%20on%20common%20multi-agent%20tasks%2C%20including%0Ainformation-asymmetric%20question%20answering%20and%20complex%20reasoning%2C%20Optima%20shows%0Aconsistent%20and%20substantial%20improvements%20over%20single-agent%20baselines%20and%20vanilla%0AMAS%20based%20on%20Llama%203%208B%2C%20achieving%20up%20to%202.8x%20performance%20gain%20with%20less%20than%0A10%5C%25%20tokens%20on%20tasks%20requiring%20heavy%20information%20exchange.%20Moreover%2C%20Optima%27s%0Aefficiency%20gains%20open%20new%20possibilities%20for%20leveraging%20inference-compute%20more%0Aeffectively%2C%20leading%20to%20improved%20inference-time%20scaling%20laws.%20By%20addressing%0Afundamental%20challenges%20in%20LLM-based%20MAS%2C%20Optima%20shows%20the%20potential%20towards%0Ascalable%2C%20efficient%2C%20and%20effective%20MAS%0A%28https%3A//chenweize1998.github.io/optima-project-page%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptima%253A%2520Optimizing%2520Effectiveness%2520and%2520Efficiency%2520for%2520LLM-Based%250A%2520%2520Multi-Agent%2520System%26entry.906535625%3DWeize%2520Chen%2520and%2520Jiarui%2520Yuan%2520and%2520Chen%2520Qian%2520and%2520Cheng%2520Yang%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520multi-agent%2520systems%2520%2528MAS%2529%2520show%2520remarkable%250Apotential%2520in%2520collaborative%2520problem-solving%252C%2520yet%2520they%2520still%2520face%2520critical%250Achallenges%253A%2520low%2520communication%2520efficiency%252C%2520poor%2520scalability%252C%2520and%2520a%2520lack%2520of%250Aeffective%2520parameter-updating%2520optimization%2520methods.%2520We%2520present%2520Optima%252C%2520a%2520novel%250Aframework%2520that%2520addresses%2520these%2520issues%2520by%2520significantly%2520enhancing%2520both%250Acommunication%2520efficiency%2520and%2520task%2520effectiveness%2520in%2520LLM-based%2520MAS%2520through%2520LLM%250Atraining.%2520Optima%2520employs%2520an%2520iterative%2520generate%252C%2520rank%252C%2520select%252C%2520and%2520train%250Aparadigm%2520with%2520a%2520reward%2520function%2520balancing%2520task%2520performance%252C%2520token%2520efficiency%252C%250Aand%2520communication%2520readability.%2520We%2520explore%2520various%2520RL%2520algorithms%252C%2520including%250ASupervised%2520Fine-Tuning%252C%2520Direct%2520Preference%2520Optimization%252C%2520and%2520their%2520hybrid%250Aapproaches%252C%2520providing%2520insights%2520into%2520their%2520effectiveness-efficiency%2520trade-offs.%250AWe%2520integrate%2520Monte%2520Carlo%2520Tree%2520Search-inspired%2520techniques%2520for%2520DPO%2520data%250Ageneration%252C%2520treating%2520conversation%2520turns%2520as%2520tree%2520nodes%2520to%2520explore%2520diverse%250Ainteraction%2520paths.%2520Evaluated%2520on%2520common%2520multi-agent%2520tasks%252C%2520including%250Ainformation-asymmetric%2520question%2520answering%2520and%2520complex%2520reasoning%252C%2520Optima%2520shows%250Aconsistent%2520and%2520substantial%2520improvements%2520over%2520single-agent%2520baselines%2520and%2520vanilla%250AMAS%2520based%2520on%2520Llama%25203%25208B%252C%2520achieving%2520up%2520to%25202.8x%2520performance%2520gain%2520with%2520less%2520than%250A10%255C%2525%2520tokens%2520on%2520tasks%2520requiring%2520heavy%2520information%2520exchange.%2520Moreover%252C%2520Optima%2527s%250Aefficiency%2520gains%2520open%2520new%2520possibilities%2520for%2520leveraging%2520inference-compute%2520more%250Aeffectively%252C%2520leading%2520to%2520improved%2520inference-time%2520scaling%2520laws.%2520By%2520addressing%250Afundamental%2520challenges%2520in%2520LLM-based%2520MAS%252C%2520Optima%2520shows%2520the%2520potential%2520towards%250Ascalable%252C%2520efficient%252C%2520and%2520effective%2520MAS%250A%2528https%253A//chenweize1998.github.io/optima-project-page%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optima%3A%20Optimizing%20Effectiveness%20and%20Efficiency%20for%20LLM-Based%0A%20%20Multi-Agent%20System&entry.906535625=Weize%20Chen%20and%20Jiarui%20Yuan%20and%20Chen%20Qian%20and%20Cheng%20Yang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20based%20multi-agent%20systems%20%28MAS%29%20show%20remarkable%0Apotential%20in%20collaborative%20problem-solving%2C%20yet%20they%20still%20face%20critical%0Achallenges%3A%20low%20communication%20efficiency%2C%20poor%20scalability%2C%20and%20a%20lack%20of%0Aeffective%20parameter-updating%20optimization%20methods.%20We%20present%20Optima%2C%20a%20novel%0Aframework%20that%20addresses%20these%20issues%20by%20significantly%20enhancing%20both%0Acommunication%20efficiency%20and%20task%20effectiveness%20in%20LLM-based%20MAS%20through%20LLM%0Atraining.%20Optima%20employs%20an%20iterative%20generate%2C%20rank%2C%20select%2C%20and%20train%0Aparadigm%20with%20a%20reward%20function%20balancing%20task%20performance%2C%20token%20efficiency%2C%0Aand%20communication%20readability.%20We%20explore%20various%20RL%20algorithms%2C%20including%0ASupervised%20Fine-Tuning%2C%20Direct%20Preference%20Optimization%2C%20and%20their%20hybrid%0Aapproaches%2C%20providing%20insights%20into%20their%20effectiveness-efficiency%20trade-offs.%0AWe%20integrate%20Monte%20Carlo%20Tree%20Search-inspired%20techniques%20for%20DPO%20data%0Ageneration%2C%20treating%20conversation%20turns%20as%20tree%20nodes%20to%20explore%20diverse%0Ainteraction%20paths.%20Evaluated%20on%20common%20multi-agent%20tasks%2C%20including%0Ainformation-asymmetric%20question%20answering%20and%20complex%20reasoning%2C%20Optima%20shows%0Aconsistent%20and%20substantial%20improvements%20over%20single-agent%20baselines%20and%20vanilla%0AMAS%20based%20on%20Llama%203%208B%2C%20achieving%20up%20to%202.8x%20performance%20gain%20with%20less%20than%0A10%5C%25%20tokens%20on%20tasks%20requiring%20heavy%20information%20exchange.%20Moreover%2C%20Optima%27s%0Aefficiency%20gains%20open%20new%20possibilities%20for%20leveraging%20inference-compute%20more%0Aeffectively%2C%20leading%20to%20improved%20inference-time%20scaling%20laws.%20By%20addressing%0Afundamental%20challenges%20in%20LLM-based%20MAS%2C%20Optima%20shows%20the%20potential%20towards%0Ascalable%2C%20efficient%2C%20and%20effective%20MAS%0A%28https%3A//chenweize1998.github.io/optima-project-page%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08115v2&entry.124074799=Read"},
{"title": "Towards Variational Flow Matching on General Geometries", "author": "Olga Zaghen and Floor Eijkelboom and Alison Pouplin and Erik J. Bekkers", "abstract": "  We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an\nextension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian\ndistributions for generative modeling on structured manifolds. We derive a\nvariational objective for probability flows on manifolds with closed-form\ngeodesics, making RG-VFM comparable - though fundamentally different to\nRiemannian Flow Matching (RFM) in this geometric setting. Experiments on a\ncheckerboard dataset wrapped on the sphere demonstrate that RG-VFM captures\ngeometric structure more effectively than Euclidean VFM and baseline methods,\nestablishing it as a robust framework for manifold-aware generative modeling.\n", "link": "http://arxiv.org/abs/2502.12981v1", "date": "2025-02-18", "relevancy": 2.0972, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5472}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5259}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Variational%20Flow%20Matching%20on%20General%20Geometries&body=Title%3A%20Towards%20Variational%20Flow%20Matching%20on%20General%20Geometries%0AAuthor%3A%20Olga%20Zaghen%20and%20Floor%20Eijkelboom%20and%20Alison%20Pouplin%20and%20Erik%20J.%20Bekkers%0AAbstract%3A%20%20%20We%20introduce%20Riemannian%20Gaussian%20Variational%20Flow%20Matching%20%28RG-VFM%29%2C%20an%0Aextension%20of%20Variational%20Flow%20Matching%20%28VFM%29%20that%20leverages%20Riemannian%20Gaussian%0Adistributions%20for%20generative%20modeling%20on%20structured%20manifolds.%20We%20derive%20a%0Avariational%20objective%20for%20probability%20flows%20on%20manifolds%20with%20closed-form%0Ageodesics%2C%20making%20RG-VFM%20comparable%20-%20though%20fundamentally%20different%20to%0ARiemannian%20Flow%20Matching%20%28RFM%29%20in%20this%20geometric%20setting.%20Experiments%20on%20a%0Acheckerboard%20dataset%20wrapped%20on%20the%20sphere%20demonstrate%20that%20RG-VFM%20captures%0Ageometric%20structure%20more%20effectively%20than%20Euclidean%20VFM%20and%20baseline%20methods%2C%0Aestablishing%20it%20as%20a%20robust%20framework%20for%20manifold-aware%20generative%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Variational%2520Flow%2520Matching%2520on%2520General%2520Geometries%26entry.906535625%3DOlga%2520Zaghen%2520and%2520Floor%2520Eijkelboom%2520and%2520Alison%2520Pouplin%2520and%2520Erik%2520J.%2520Bekkers%26entry.1292438233%3D%2520%2520We%2520introduce%2520Riemannian%2520Gaussian%2520Variational%2520Flow%2520Matching%2520%2528RG-VFM%2529%252C%2520an%250Aextension%2520of%2520Variational%2520Flow%2520Matching%2520%2528VFM%2529%2520that%2520leverages%2520Riemannian%2520Gaussian%250Adistributions%2520for%2520generative%2520modeling%2520on%2520structured%2520manifolds.%2520We%2520derive%2520a%250Avariational%2520objective%2520for%2520probability%2520flows%2520on%2520manifolds%2520with%2520closed-form%250Ageodesics%252C%2520making%2520RG-VFM%2520comparable%2520-%2520though%2520fundamentally%2520different%2520to%250ARiemannian%2520Flow%2520Matching%2520%2528RFM%2529%2520in%2520this%2520geometric%2520setting.%2520Experiments%2520on%2520a%250Acheckerboard%2520dataset%2520wrapped%2520on%2520the%2520sphere%2520demonstrate%2520that%2520RG-VFM%2520captures%250Ageometric%2520structure%2520more%2520effectively%2520than%2520Euclidean%2520VFM%2520and%2520baseline%2520methods%252C%250Aestablishing%2520it%2520as%2520a%2520robust%2520framework%2520for%2520manifold-aware%2520generative%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Variational%20Flow%20Matching%20on%20General%20Geometries&entry.906535625=Olga%20Zaghen%20and%20Floor%20Eijkelboom%20and%20Alison%20Pouplin%20and%20Erik%20J.%20Bekkers&entry.1292438233=%20%20We%20introduce%20Riemannian%20Gaussian%20Variational%20Flow%20Matching%20%28RG-VFM%29%2C%20an%0Aextension%20of%20Variational%20Flow%20Matching%20%28VFM%29%20that%20leverages%20Riemannian%20Gaussian%0Adistributions%20for%20generative%20modeling%20on%20structured%20manifolds.%20We%20derive%20a%0Avariational%20objective%20for%20probability%20flows%20on%20manifolds%20with%20closed-form%0Ageodesics%2C%20making%20RG-VFM%20comparable%20-%20though%20fundamentally%20different%20to%0ARiemannian%20Flow%20Matching%20%28RFM%29%20in%20this%20geometric%20setting.%20Experiments%20on%20a%0Acheckerboard%20dataset%20wrapped%20on%20the%20sphere%20demonstrate%20that%20RG-VFM%20captures%0Ageometric%20structure%20more%20effectively%20than%20Euclidean%20VFM%20and%20baseline%20methods%2C%0Aestablishing%20it%20as%20a%20robust%20framework%20for%20manifold-aware%20generative%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12981v1&entry.124074799=Read"},
{"title": "Enhancing Power Grid Inspections with Machine Learning", "author": "Diogo Lavado and Ricardo Santos and Andre Coelho and Joao Santos and Alessandra Micheletti and Claudia Soares", "abstract": "  Ensuring the safety and reliability of power grids is critical as global\nenergy demands continue to rise. Traditional inspection methods, such as manual\nobservations or helicopter surveys, are resource-intensive and lack\nscalability. This paper explores the use of 3D computer vision to automate\npower grid inspections, utilizing the TS40K dataset -- a high-density,\nannotated collection of 3D LiDAR point clouds. By concentrating on 3D semantic\nsegmentation, our approach addresses challenges like class imbalance and noisy\ndata to enhance the detection of critical grid components such as power lines\nand towers. The benchmark results indicate significant performance\nimprovements, with IoU scores reaching 95.53% for the detection of power lines\nusing transformer-based models. Our findings illustrate the potential for\nintegrating ML into grid maintenance workflows, increasing efficiency and\nenabling proactive risk management strategies.\n", "link": "http://arxiv.org/abs/2502.13037v1", "date": "2025-02-18", "relevancy": 2.0953, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5308}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Power%20Grid%20Inspections%20with%20Machine%20Learning&body=Title%3A%20Enhancing%20Power%20Grid%20Inspections%20with%20Machine%20Learning%0AAuthor%3A%20Diogo%20Lavado%20and%20Ricardo%20Santos%20and%20Andre%20Coelho%20and%20Joao%20Santos%20and%20Alessandra%20Micheletti%20and%20Claudia%20Soares%0AAbstract%3A%20%20%20Ensuring%20the%20safety%20and%20reliability%20of%20power%20grids%20is%20critical%20as%20global%0Aenergy%20demands%20continue%20to%20rise.%20Traditional%20inspection%20methods%2C%20such%20as%20manual%0Aobservations%20or%20helicopter%20surveys%2C%20are%20resource-intensive%20and%20lack%0Ascalability.%20This%20paper%20explores%20the%20use%20of%203D%20computer%20vision%20to%20automate%0Apower%20grid%20inspections%2C%20utilizing%20the%20TS40K%20dataset%20--%20a%20high-density%2C%0Aannotated%20collection%20of%203D%20LiDAR%20point%20clouds.%20By%20concentrating%20on%203D%20semantic%0Asegmentation%2C%20our%20approach%20addresses%20challenges%20like%20class%20imbalance%20and%20noisy%0Adata%20to%20enhance%20the%20detection%20of%20critical%20grid%20components%20such%20as%20power%20lines%0Aand%20towers.%20The%20benchmark%20results%20indicate%20significant%20performance%0Aimprovements%2C%20with%20IoU%20scores%20reaching%2095.53%25%20for%20the%20detection%20of%20power%20lines%0Ausing%20transformer-based%20models.%20Our%20findings%20illustrate%20the%20potential%20for%0Aintegrating%20ML%20into%20grid%20maintenance%20workflows%2C%20increasing%20efficiency%20and%0Aenabling%20proactive%20risk%20management%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Power%2520Grid%2520Inspections%2520with%2520Machine%2520Learning%26entry.906535625%3DDiogo%2520Lavado%2520and%2520Ricardo%2520Santos%2520and%2520Andre%2520Coelho%2520and%2520Joao%2520Santos%2520and%2520Alessandra%2520Micheletti%2520and%2520Claudia%2520Soares%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520safety%2520and%2520reliability%2520of%2520power%2520grids%2520is%2520critical%2520as%2520global%250Aenergy%2520demands%2520continue%2520to%2520rise.%2520Traditional%2520inspection%2520methods%252C%2520such%2520as%2520manual%250Aobservations%2520or%2520helicopter%2520surveys%252C%2520are%2520resource-intensive%2520and%2520lack%250Ascalability.%2520This%2520paper%2520explores%2520the%2520use%2520of%25203D%2520computer%2520vision%2520to%2520automate%250Apower%2520grid%2520inspections%252C%2520utilizing%2520the%2520TS40K%2520dataset%2520--%2520a%2520high-density%252C%250Aannotated%2520collection%2520of%25203D%2520LiDAR%2520point%2520clouds.%2520By%2520concentrating%2520on%25203D%2520semantic%250Asegmentation%252C%2520our%2520approach%2520addresses%2520challenges%2520like%2520class%2520imbalance%2520and%2520noisy%250Adata%2520to%2520enhance%2520the%2520detection%2520of%2520critical%2520grid%2520components%2520such%2520as%2520power%2520lines%250Aand%2520towers.%2520The%2520benchmark%2520results%2520indicate%2520significant%2520performance%250Aimprovements%252C%2520with%2520IoU%2520scores%2520reaching%252095.53%2525%2520for%2520the%2520detection%2520of%2520power%2520lines%250Ausing%2520transformer-based%2520models.%2520Our%2520findings%2520illustrate%2520the%2520potential%2520for%250Aintegrating%2520ML%2520into%2520grid%2520maintenance%2520workflows%252C%2520increasing%2520efficiency%2520and%250Aenabling%2520proactive%2520risk%2520management%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Power%20Grid%20Inspections%20with%20Machine%20Learning&entry.906535625=Diogo%20Lavado%20and%20Ricardo%20Santos%20and%20Andre%20Coelho%20and%20Joao%20Santos%20and%20Alessandra%20Micheletti%20and%20Claudia%20Soares&entry.1292438233=%20%20Ensuring%20the%20safety%20and%20reliability%20of%20power%20grids%20is%20critical%20as%20global%0Aenergy%20demands%20continue%20to%20rise.%20Traditional%20inspection%20methods%2C%20such%20as%20manual%0Aobservations%20or%20helicopter%20surveys%2C%20are%20resource-intensive%20and%20lack%0Ascalability.%20This%20paper%20explores%20the%20use%20of%203D%20computer%20vision%20to%20automate%0Apower%20grid%20inspections%2C%20utilizing%20the%20TS40K%20dataset%20--%20a%20high-density%2C%0Aannotated%20collection%20of%203D%20LiDAR%20point%20clouds.%20By%20concentrating%20on%203D%20semantic%0Asegmentation%2C%20our%20approach%20addresses%20challenges%20like%20class%20imbalance%20and%20noisy%0Adata%20to%20enhance%20the%20detection%20of%20critical%20grid%20components%20such%20as%20power%20lines%0Aand%20towers.%20The%20benchmark%20results%20indicate%20significant%20performance%0Aimprovements%2C%20with%20IoU%20scores%20reaching%2095.53%25%20for%20the%20detection%20of%20power%20lines%0Ausing%20transformer-based%20models.%20Our%20findings%20illustrate%20the%20potential%20for%0Aintegrating%20ML%20into%20grid%20maintenance%20workflows%2C%20increasing%20efficiency%20and%0Aenabling%20proactive%20risk%20management%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13037v1&entry.124074799=Read"},
{"title": "Graph Neural Networks for Databases: A Survey", "author": "Ziming Li and Youhuan Li and Yuyu Luo and Guoliang Li and Chuxu Zhang", "abstract": "  Graph neural networks (GNNs) are powerful deep learning models for\ngraph-structured data, demonstrating remarkable success across diverse domains.\nRecently, the database (DB) community has increasingly recognized the\npotentiality of GNNs, prompting a surge of researches focusing on improving\ndatabase systems through GNN-based approaches. However, despite notable\nadvances, There is a lack of a comprehensive review and understanding of how\nGNNs could improve DB systems. Therefore, this survey aims to bridge this gap\nby providing a structured and in-depth overview of GNNs for DB systems.\nSpecifically, we propose a new taxonomy that classifies existing methods into\ntwo key categories: (1) Relational Databases, which includes tasks like\nperformance prediction, query optimization, and text-to-SQL, and (2) Graph\nDatabases, addressing challenges like efficient graph query processing and\ngraph similarity computation. We systematically review key methods in each\ncategory, highlighting their contributions and practical implications. Finally,\nwe suggest promising avenues for integrating GNNs into Database systems.\n", "link": "http://arxiv.org/abs/2502.12908v1", "date": "2025-02-18", "relevancy": 2.0822, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4402}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4073}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20for%20Databases%3A%20A%20Survey&body=Title%3A%20Graph%20Neural%20Networks%20for%20Databases%3A%20A%20Survey%0AAuthor%3A%20Ziming%20Li%20and%20Youhuan%20Li%20and%20Yuyu%20Luo%20and%20Guoliang%20Li%20and%20Chuxu%20Zhang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20powerful%20deep%20learning%20models%20for%0Agraph-structured%20data%2C%20demonstrating%20remarkable%20success%20across%20diverse%20domains.%0ARecently%2C%20the%20database%20%28DB%29%20community%20has%20increasingly%20recognized%20the%0Apotentiality%20of%20GNNs%2C%20prompting%20a%20surge%20of%20researches%20focusing%20on%20improving%0Adatabase%20systems%20through%20GNN-based%20approaches.%20However%2C%20despite%20notable%0Aadvances%2C%20There%20is%20a%20lack%20of%20a%20comprehensive%20review%20and%20understanding%20of%20how%0AGNNs%20could%20improve%20DB%20systems.%20Therefore%2C%20this%20survey%20aims%20to%20bridge%20this%20gap%0Aby%20providing%20a%20structured%20and%20in-depth%20overview%20of%20GNNs%20for%20DB%20systems.%0ASpecifically%2C%20we%20propose%20a%20new%20taxonomy%20that%20classifies%20existing%20methods%20into%0Atwo%20key%20categories%3A%20%281%29%20Relational%20Databases%2C%20which%20includes%20tasks%20like%0Aperformance%20prediction%2C%20query%20optimization%2C%20and%20text-to-SQL%2C%20and%20%282%29%20Graph%0ADatabases%2C%20addressing%20challenges%20like%20efficient%20graph%20query%20processing%20and%0Agraph%20similarity%20computation.%20We%20systematically%20review%20key%20methods%20in%20each%0Acategory%2C%20highlighting%20their%20contributions%20and%20practical%20implications.%20Finally%2C%0Awe%20suggest%20promising%20avenues%20for%20integrating%20GNNs%20into%20Database%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520for%2520Databases%253A%2520A%2520Survey%26entry.906535625%3DZiming%2520Li%2520and%2520Youhuan%2520Li%2520and%2520Yuyu%2520Luo%2520and%2520Guoliang%2520Li%2520and%2520Chuxu%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520powerful%2520deep%2520learning%2520models%2520for%250Agraph-structured%2520data%252C%2520demonstrating%2520remarkable%2520success%2520across%2520diverse%2520domains.%250ARecently%252C%2520the%2520database%2520%2528DB%2529%2520community%2520has%2520increasingly%2520recognized%2520the%250Apotentiality%2520of%2520GNNs%252C%2520prompting%2520a%2520surge%2520of%2520researches%2520focusing%2520on%2520improving%250Adatabase%2520systems%2520through%2520GNN-based%2520approaches.%2520However%252C%2520despite%2520notable%250Aadvances%252C%2520There%2520is%2520a%2520lack%2520of%2520a%2520comprehensive%2520review%2520and%2520understanding%2520of%2520how%250AGNNs%2520could%2520improve%2520DB%2520systems.%2520Therefore%252C%2520this%2520survey%2520aims%2520to%2520bridge%2520this%2520gap%250Aby%2520providing%2520a%2520structured%2520and%2520in-depth%2520overview%2520of%2520GNNs%2520for%2520DB%2520systems.%250ASpecifically%252C%2520we%2520propose%2520a%2520new%2520taxonomy%2520that%2520classifies%2520existing%2520methods%2520into%250Atwo%2520key%2520categories%253A%2520%25281%2529%2520Relational%2520Databases%252C%2520which%2520includes%2520tasks%2520like%250Aperformance%2520prediction%252C%2520query%2520optimization%252C%2520and%2520text-to-SQL%252C%2520and%2520%25282%2529%2520Graph%250ADatabases%252C%2520addressing%2520challenges%2520like%2520efficient%2520graph%2520query%2520processing%2520and%250Agraph%2520similarity%2520computation.%2520We%2520systematically%2520review%2520key%2520methods%2520in%2520each%250Acategory%252C%2520highlighting%2520their%2520contributions%2520and%2520practical%2520implications.%2520Finally%252C%250Awe%2520suggest%2520promising%2520avenues%2520for%2520integrating%2520GNNs%2520into%2520Database%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20for%20Databases%3A%20A%20Survey&entry.906535625=Ziming%20Li%20and%20Youhuan%20Li%20and%20Yuyu%20Luo%20and%20Guoliang%20Li%20and%20Chuxu%20Zhang&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20powerful%20deep%20learning%20models%20for%0Agraph-structured%20data%2C%20demonstrating%20remarkable%20success%20across%20diverse%20domains.%0ARecently%2C%20the%20database%20%28DB%29%20community%20has%20increasingly%20recognized%20the%0Apotentiality%20of%20GNNs%2C%20prompting%20a%20surge%20of%20researches%20focusing%20on%20improving%0Adatabase%20systems%20through%20GNN-based%20approaches.%20However%2C%20despite%20notable%0Aadvances%2C%20There%20is%20a%20lack%20of%20a%20comprehensive%20review%20and%20understanding%20of%20how%0AGNNs%20could%20improve%20DB%20systems.%20Therefore%2C%20this%20survey%20aims%20to%20bridge%20this%20gap%0Aby%20providing%20a%20structured%20and%20in-depth%20overview%20of%20GNNs%20for%20DB%20systems.%0ASpecifically%2C%20we%20propose%20a%20new%20taxonomy%20that%20classifies%20existing%20methods%20into%0Atwo%20key%20categories%3A%20%281%29%20Relational%20Databases%2C%20which%20includes%20tasks%20like%0Aperformance%20prediction%2C%20query%20optimization%2C%20and%20text-to-SQL%2C%20and%20%282%29%20Graph%0ADatabases%2C%20addressing%20challenges%20like%20efficient%20graph%20query%20processing%20and%0Agraph%20similarity%20computation.%20We%20systematically%20review%20key%20methods%20in%20each%0Acategory%2C%20highlighting%20their%20contributions%20and%20practical%20implications.%20Finally%2C%0Awe%20suggest%20promising%20avenues%20for%20integrating%20GNNs%20into%20Database%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12908v1&entry.124074799=Read"},
{"title": "Detection and Geographic Localization of Natural Objects in the Wild: A\n  Case Study on Palms", "author": "Kangning Cui and Rongkun Zhu and Manqi Wang and Wei Tang and Gregory D. Larsen and Victor P. Pauca and Sarra Alqahtani and Fan Yang and David Segurado and David Lutz and Jean-Michel Morel and Miles R. Silman", "abstract": "  Palms are ecologically and economically indicators of tropical forest health,\nbiodiversity, and human impact that support local economies and global forest\nproduct supply chains. While palm detection in plantations is well-studied,\nefforts to map naturally occurring palms in dense forests remain limited by\noverlapping crowns, uneven shading, and heterogeneous landscapes. We develop\nPRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline\nfor detecting and localizing palms in dense tropical forests using large\northomosaic images. Orthomosaics are created from thousands of aerial images\nand spanning several to hundreds of gigabytes. Our contributions are threefold.\nFirst, we construct a large UAV-derived orthomosaic dataset collected across 21\necologically diverse sites in western Ecuador, annotated with 8,830 bounding\nboxes and 5,026 palm center points. Second, we evaluate multiple\nstate-of-the-art object detectors based on efficiency and performance,\nintegrating zero-shot SAM 2 as the segmentation backbone, and refining the\nresults for precise geographic mapping. Third, we apply calibration methods to\nalign confidence scores with IoU and explore saliency maps for feature\nexplainability. Though optimized for palms, PRISM is adaptable for identifying\nother natural objects, such as eastern white pines. Future work will explore\ntransfer learning for lower-resolution datasets (0.5 to 1m).\n", "link": "http://arxiv.org/abs/2502.13023v1", "date": "2025-02-18", "relevancy": 2.081, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5878}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20and%20Geographic%20Localization%20of%20Natural%20Objects%20in%20the%20Wild%3A%20A%0A%20%20Case%20Study%20on%20Palms&body=Title%3A%20Detection%20and%20Geographic%20Localization%20of%20Natural%20Objects%20in%20the%20Wild%3A%20A%0A%20%20Case%20Study%20on%20Palms%0AAuthor%3A%20Kangning%20Cui%20and%20Rongkun%20Zhu%20and%20Manqi%20Wang%20and%20Wei%20Tang%20and%20Gregory%20D.%20Larsen%20and%20Victor%20P.%20Pauca%20and%20Sarra%20Alqahtani%20and%20Fan%20Yang%20and%20David%20Segurado%20and%20David%20Lutz%20and%20Jean-Michel%20Morel%20and%20Miles%20R.%20Silman%0AAbstract%3A%20%20%20Palms%20are%20ecologically%20and%20economically%20indicators%20of%20tropical%20forest%20health%2C%0Abiodiversity%2C%20and%20human%20impact%20that%20support%20local%20economies%20and%20global%20forest%0Aproduct%20supply%20chains.%20While%20palm%20detection%20in%20plantations%20is%20well-studied%2C%0Aefforts%20to%20map%20naturally%20occurring%20palms%20in%20dense%20forests%20remain%20limited%20by%0Aoverlapping%20crowns%2C%20uneven%20shading%2C%20and%20heterogeneous%20landscapes.%20We%20develop%0APRISM%20%28Processing%2C%20Inference%2C%20Segmentation%2C%20and%20Mapping%29%2C%20a%20flexible%20pipeline%0Afor%20detecting%20and%20localizing%20palms%20in%20dense%20tropical%20forests%20using%20large%0Aorthomosaic%20images.%20Orthomosaics%20are%20created%20from%20thousands%20of%20aerial%20images%0Aand%20spanning%20several%20to%20hundreds%20of%20gigabytes.%20Our%20contributions%20are%20threefold.%0AFirst%2C%20we%20construct%20a%20large%20UAV-derived%20orthomosaic%20dataset%20collected%20across%2021%0Aecologically%20diverse%20sites%20in%20western%20Ecuador%2C%20annotated%20with%208%2C830%20bounding%0Aboxes%20and%205%2C026%20palm%20center%20points.%20Second%2C%20we%20evaluate%20multiple%0Astate-of-the-art%20object%20detectors%20based%20on%20efficiency%20and%20performance%2C%0Aintegrating%20zero-shot%20SAM%202%20as%20the%20segmentation%20backbone%2C%20and%20refining%20the%0Aresults%20for%20precise%20geographic%20mapping.%20Third%2C%20we%20apply%20calibration%20methods%20to%0Aalign%20confidence%20scores%20with%20IoU%20and%20explore%20saliency%20maps%20for%20feature%0Aexplainability.%20Though%20optimized%20for%20palms%2C%20PRISM%20is%20adaptable%20for%20identifying%0Aother%20natural%20objects%2C%20such%20as%20eastern%20white%20pines.%20Future%20work%20will%20explore%0Atransfer%20learning%20for%20lower-resolution%20datasets%20%280.5%20to%201m%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520and%2520Geographic%2520Localization%2520of%2520Natural%2520Objects%2520in%2520the%2520Wild%253A%2520A%250A%2520%2520Case%2520Study%2520on%2520Palms%26entry.906535625%3DKangning%2520Cui%2520and%2520Rongkun%2520Zhu%2520and%2520Manqi%2520Wang%2520and%2520Wei%2520Tang%2520and%2520Gregory%2520D.%2520Larsen%2520and%2520Victor%2520P.%2520Pauca%2520and%2520Sarra%2520Alqahtani%2520and%2520Fan%2520Yang%2520and%2520David%2520Segurado%2520and%2520David%2520Lutz%2520and%2520Jean-Michel%2520Morel%2520and%2520Miles%2520R.%2520Silman%26entry.1292438233%3D%2520%2520Palms%2520are%2520ecologically%2520and%2520economically%2520indicators%2520of%2520tropical%2520forest%2520health%252C%250Abiodiversity%252C%2520and%2520human%2520impact%2520that%2520support%2520local%2520economies%2520and%2520global%2520forest%250Aproduct%2520supply%2520chains.%2520While%2520palm%2520detection%2520in%2520plantations%2520is%2520well-studied%252C%250Aefforts%2520to%2520map%2520naturally%2520occurring%2520palms%2520in%2520dense%2520forests%2520remain%2520limited%2520by%250Aoverlapping%2520crowns%252C%2520uneven%2520shading%252C%2520and%2520heterogeneous%2520landscapes.%2520We%2520develop%250APRISM%2520%2528Processing%252C%2520Inference%252C%2520Segmentation%252C%2520and%2520Mapping%2529%252C%2520a%2520flexible%2520pipeline%250Afor%2520detecting%2520and%2520localizing%2520palms%2520in%2520dense%2520tropical%2520forests%2520using%2520large%250Aorthomosaic%2520images.%2520Orthomosaics%2520are%2520created%2520from%2520thousands%2520of%2520aerial%2520images%250Aand%2520spanning%2520several%2520to%2520hundreds%2520of%2520gigabytes.%2520Our%2520contributions%2520are%2520threefold.%250AFirst%252C%2520we%2520construct%2520a%2520large%2520UAV-derived%2520orthomosaic%2520dataset%2520collected%2520across%252021%250Aecologically%2520diverse%2520sites%2520in%2520western%2520Ecuador%252C%2520annotated%2520with%25208%252C830%2520bounding%250Aboxes%2520and%25205%252C026%2520palm%2520center%2520points.%2520Second%252C%2520we%2520evaluate%2520multiple%250Astate-of-the-art%2520object%2520detectors%2520based%2520on%2520efficiency%2520and%2520performance%252C%250Aintegrating%2520zero-shot%2520SAM%25202%2520as%2520the%2520segmentation%2520backbone%252C%2520and%2520refining%2520the%250Aresults%2520for%2520precise%2520geographic%2520mapping.%2520Third%252C%2520we%2520apply%2520calibration%2520methods%2520to%250Aalign%2520confidence%2520scores%2520with%2520IoU%2520and%2520explore%2520saliency%2520maps%2520for%2520feature%250Aexplainability.%2520Though%2520optimized%2520for%2520palms%252C%2520PRISM%2520is%2520adaptable%2520for%2520identifying%250Aother%2520natural%2520objects%252C%2520such%2520as%2520eastern%2520white%2520pines.%2520Future%2520work%2520will%2520explore%250Atransfer%2520learning%2520for%2520lower-resolution%2520datasets%2520%25280.5%2520to%25201m%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20and%20Geographic%20Localization%20of%20Natural%20Objects%20in%20the%20Wild%3A%20A%0A%20%20Case%20Study%20on%20Palms&entry.906535625=Kangning%20Cui%20and%20Rongkun%20Zhu%20and%20Manqi%20Wang%20and%20Wei%20Tang%20and%20Gregory%20D.%20Larsen%20and%20Victor%20P.%20Pauca%20and%20Sarra%20Alqahtani%20and%20Fan%20Yang%20and%20David%20Segurado%20and%20David%20Lutz%20and%20Jean-Michel%20Morel%20and%20Miles%20R.%20Silman&entry.1292438233=%20%20Palms%20are%20ecologically%20and%20economically%20indicators%20of%20tropical%20forest%20health%2C%0Abiodiversity%2C%20and%20human%20impact%20that%20support%20local%20economies%20and%20global%20forest%0Aproduct%20supply%20chains.%20While%20palm%20detection%20in%20plantations%20is%20well-studied%2C%0Aefforts%20to%20map%20naturally%20occurring%20palms%20in%20dense%20forests%20remain%20limited%20by%0Aoverlapping%20crowns%2C%20uneven%20shading%2C%20and%20heterogeneous%20landscapes.%20We%20develop%0APRISM%20%28Processing%2C%20Inference%2C%20Segmentation%2C%20and%20Mapping%29%2C%20a%20flexible%20pipeline%0Afor%20detecting%20and%20localizing%20palms%20in%20dense%20tropical%20forests%20using%20large%0Aorthomosaic%20images.%20Orthomosaics%20are%20created%20from%20thousands%20of%20aerial%20images%0Aand%20spanning%20several%20to%20hundreds%20of%20gigabytes.%20Our%20contributions%20are%20threefold.%0AFirst%2C%20we%20construct%20a%20large%20UAV-derived%20orthomosaic%20dataset%20collected%20across%2021%0Aecologically%20diverse%20sites%20in%20western%20Ecuador%2C%20annotated%20with%208%2C830%20bounding%0Aboxes%20and%205%2C026%20palm%20center%20points.%20Second%2C%20we%20evaluate%20multiple%0Astate-of-the-art%20object%20detectors%20based%20on%20efficiency%20and%20performance%2C%0Aintegrating%20zero-shot%20SAM%202%20as%20the%20segmentation%20backbone%2C%20and%20refining%20the%0Aresults%20for%20precise%20geographic%20mapping.%20Third%2C%20we%20apply%20calibration%20methods%20to%0Aalign%20confidence%20scores%20with%20IoU%20and%20explore%20saliency%20maps%20for%20feature%0Aexplainability.%20Though%20optimized%20for%20palms%2C%20PRISM%20is%20adaptable%20for%20identifying%0Aother%20natural%20objects%2C%20such%20as%20eastern%20white%20pines.%20Future%20work%20will%20explore%0Atransfer%20learning%20for%20lower-resolution%20datasets%20%280.5%20to%201m%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13023v1&entry.124074799=Read"},
{"title": "Free Argumentative Exchanges for Explaining Image Classifiers", "author": "Avinash Kori and Antonio Rago and Francesca Toni", "abstract": "  Deep learning models are powerful image classifiers but their opacity hinders\ntheir trustworthiness. Explanation methods for capturing the reasoning process\nwithin these classifiers faithfully and in a clear manner are scarce, due to\ntheir sheer complexity and size. We provide a solution for this problem by\ndefining a novel method for explaining the outputs of image classifiers with\ndebates between two agents, each arguing for a particular class. We obtain\nthese debates as concrete instances of Free Argumentative eXchanges (FAXs), a\nnovel argumentation-based multi-agent framework allowing agents to internalise\nopinions by other agents differently than originally stated. We define two\nmetrics (consensus and persuasion rate) to assess the usefulness of FAXs as\nargumentative explanations for image classifiers. We then conduct a number of\nempirical experiments showing that FAXs perform well along these metrics as\nwell as being more faithful to the image classifiers than conventional,\nnon-argumentative explanation methods. All our implementations can be found at\nhttps://github.com/koriavinash1/FAX.\n", "link": "http://arxiv.org/abs/2502.12995v1", "date": "2025-02-18", "relevancy": 2.0739, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5255}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free%20Argumentative%20Exchanges%20for%20Explaining%20Image%20Classifiers&body=Title%3A%20Free%20Argumentative%20Exchanges%20for%20Explaining%20Image%20Classifiers%0AAuthor%3A%20Avinash%20Kori%20and%20Antonio%20Rago%20and%20Francesca%20Toni%0AAbstract%3A%20%20%20Deep%20learning%20models%20are%20powerful%20image%20classifiers%20but%20their%20opacity%20hinders%0Atheir%20trustworthiness.%20Explanation%20methods%20for%20capturing%20the%20reasoning%20process%0Awithin%20these%20classifiers%20faithfully%20and%20in%20a%20clear%20manner%20are%20scarce%2C%20due%20to%0Atheir%20sheer%20complexity%20and%20size.%20We%20provide%20a%20solution%20for%20this%20problem%20by%0Adefining%20a%20novel%20method%20for%20explaining%20the%20outputs%20of%20image%20classifiers%20with%0Adebates%20between%20two%20agents%2C%20each%20arguing%20for%20a%20particular%20class.%20We%20obtain%0Athese%20debates%20as%20concrete%20instances%20of%20Free%20Argumentative%20eXchanges%20%28FAXs%29%2C%20a%0Anovel%20argumentation-based%20multi-agent%20framework%20allowing%20agents%20to%20internalise%0Aopinions%20by%20other%20agents%20differently%20than%20originally%20stated.%20We%20define%20two%0Ametrics%20%28consensus%20and%20persuasion%20rate%29%20to%20assess%20the%20usefulness%20of%20FAXs%20as%0Aargumentative%20explanations%20for%20image%20classifiers.%20We%20then%20conduct%20a%20number%20of%0Aempirical%20experiments%20showing%20that%20FAXs%20perform%20well%20along%20these%20metrics%20as%0Awell%20as%20being%20more%20faithful%20to%20the%20image%20classifiers%20than%20conventional%2C%0Anon-argumentative%20explanation%20methods.%20All%20our%20implementations%20can%20be%20found%20at%0Ahttps%3A//github.com/koriavinash1/FAX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree%2520Argumentative%2520Exchanges%2520for%2520Explaining%2520Image%2520Classifiers%26entry.906535625%3DAvinash%2520Kori%2520and%2520Antonio%2520Rago%2520and%2520Francesca%2520Toni%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520are%2520powerful%2520image%2520classifiers%2520but%2520their%2520opacity%2520hinders%250Atheir%2520trustworthiness.%2520Explanation%2520methods%2520for%2520capturing%2520the%2520reasoning%2520process%250Awithin%2520these%2520classifiers%2520faithfully%2520and%2520in%2520a%2520clear%2520manner%2520are%2520scarce%252C%2520due%2520to%250Atheir%2520sheer%2520complexity%2520and%2520size.%2520We%2520provide%2520a%2520solution%2520for%2520this%2520problem%2520by%250Adefining%2520a%2520novel%2520method%2520for%2520explaining%2520the%2520outputs%2520of%2520image%2520classifiers%2520with%250Adebates%2520between%2520two%2520agents%252C%2520each%2520arguing%2520for%2520a%2520particular%2520class.%2520We%2520obtain%250Athese%2520debates%2520as%2520concrete%2520instances%2520of%2520Free%2520Argumentative%2520eXchanges%2520%2528FAXs%2529%252C%2520a%250Anovel%2520argumentation-based%2520multi-agent%2520framework%2520allowing%2520agents%2520to%2520internalise%250Aopinions%2520by%2520other%2520agents%2520differently%2520than%2520originally%2520stated.%2520We%2520define%2520two%250Ametrics%2520%2528consensus%2520and%2520persuasion%2520rate%2529%2520to%2520assess%2520the%2520usefulness%2520of%2520FAXs%2520as%250Aargumentative%2520explanations%2520for%2520image%2520classifiers.%2520We%2520then%2520conduct%2520a%2520number%2520of%250Aempirical%2520experiments%2520showing%2520that%2520FAXs%2520perform%2520well%2520along%2520these%2520metrics%2520as%250Awell%2520as%2520being%2520more%2520faithful%2520to%2520the%2520image%2520classifiers%2520than%2520conventional%252C%250Anon-argumentative%2520explanation%2520methods.%2520All%2520our%2520implementations%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/koriavinash1/FAX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free%20Argumentative%20Exchanges%20for%20Explaining%20Image%20Classifiers&entry.906535625=Avinash%20Kori%20and%20Antonio%20Rago%20and%20Francesca%20Toni&entry.1292438233=%20%20Deep%20learning%20models%20are%20powerful%20image%20classifiers%20but%20their%20opacity%20hinders%0Atheir%20trustworthiness.%20Explanation%20methods%20for%20capturing%20the%20reasoning%20process%0Awithin%20these%20classifiers%20faithfully%20and%20in%20a%20clear%20manner%20are%20scarce%2C%20due%20to%0Atheir%20sheer%20complexity%20and%20size.%20We%20provide%20a%20solution%20for%20this%20problem%20by%0Adefining%20a%20novel%20method%20for%20explaining%20the%20outputs%20of%20image%20classifiers%20with%0Adebates%20between%20two%20agents%2C%20each%20arguing%20for%20a%20particular%20class.%20We%20obtain%0Athese%20debates%20as%20concrete%20instances%20of%20Free%20Argumentative%20eXchanges%20%28FAXs%29%2C%20a%0Anovel%20argumentation-based%20multi-agent%20framework%20allowing%20agents%20to%20internalise%0Aopinions%20by%20other%20agents%20differently%20than%20originally%20stated.%20We%20define%20two%0Ametrics%20%28consensus%20and%20persuasion%20rate%29%20to%20assess%20the%20usefulness%20of%20FAXs%20as%0Aargumentative%20explanations%20for%20image%20classifiers.%20We%20then%20conduct%20a%20number%20of%0Aempirical%20experiments%20showing%20that%20FAXs%20perform%20well%20along%20these%20metrics%20as%0Awell%20as%20being%20more%20faithful%20to%20the%20image%20classifiers%20than%20conventional%2C%0Anon-argumentative%20explanation%20methods.%20All%20our%20implementations%20can%20be%20found%20at%0Ahttps%3A//github.com/koriavinash1/FAX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12995v1&entry.124074799=Read"},
{"title": "Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations", "author": "Hinrich Rahlfs and Markus H\u00fcllebrand and Sebastian Schmitter and Christoph Strecker and Andreas Harloff and Anja Hennemuth", "abstract": "  We propose a novel approach that uses sparse annotations from clinical\nstudies to train a 3D segmentation of the carotid artery wall. We use a\ncenterline annotation to sample perpendicular cross-sections of the carotid\nartery and use an adversarial 2D network to segment them. These annotations are\nthen transformed into 3D pseudo-labels for training of a 3D convolutional\nneural network, circumventing the creation of manual 3D masks. For pseudo-label\ncreation in the bifurcation area we propose the use of cross-sections\nperpendicular to the bifurcation axis and show that this enhances segmentation\nperformance. Different sampling distances had a lesser impact. The proposed\nmethod allows for efficient training of 3D segmentation, offering potential\nimprovements in the assessment of carotid artery stenosis and allowing the\nextraction of 3D biomarkers such as plaque volume.\n", "link": "http://arxiv.org/abs/2502.12801v1", "date": "2025-02-18", "relevancy": 2.0732, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5234}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Wall%20Segmentation%20in%203D%20Vessel%20Trees%20using%20Sparse%20Annotations&body=Title%3A%20Learning%20Wall%20Segmentation%20in%203D%20Vessel%20Trees%20using%20Sparse%20Annotations%0AAuthor%3A%20Hinrich%20Rahlfs%20and%20Markus%20H%C3%BCllebrand%20and%20Sebastian%20Schmitter%20and%20Christoph%20Strecker%20and%20Andreas%20Harloff%20and%20Anja%20Hennemuth%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20that%20uses%20sparse%20annotations%20from%20clinical%0Astudies%20to%20train%20a%203D%20segmentation%20of%20the%20carotid%20artery%20wall.%20We%20use%20a%0Acenterline%20annotation%20to%20sample%20perpendicular%20cross-sections%20of%20the%20carotid%0Aartery%20and%20use%20an%20adversarial%202D%20network%20to%20segment%20them.%20These%20annotations%20are%0Athen%20transformed%20into%203D%20pseudo-labels%20for%20training%20of%20a%203D%20convolutional%0Aneural%20network%2C%20circumventing%20the%20creation%20of%20manual%203D%20masks.%20For%20pseudo-label%0Acreation%20in%20the%20bifurcation%20area%20we%20propose%20the%20use%20of%20cross-sections%0Aperpendicular%20to%20the%20bifurcation%20axis%20and%20show%20that%20this%20enhances%20segmentation%0Aperformance.%20Different%20sampling%20distances%20had%20a%20lesser%20impact.%20The%20proposed%0Amethod%20allows%20for%20efficient%20training%20of%203D%20segmentation%2C%20offering%20potential%0Aimprovements%20in%20the%20assessment%20of%20carotid%20artery%20stenosis%20and%20allowing%20the%0Aextraction%20of%203D%20biomarkers%20such%20as%20plaque%20volume.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Wall%2520Segmentation%2520in%25203D%2520Vessel%2520Trees%2520using%2520Sparse%2520Annotations%26entry.906535625%3DHinrich%2520Rahlfs%2520and%2520Markus%2520H%25C3%25BCllebrand%2520and%2520Sebastian%2520Schmitter%2520and%2520Christoph%2520Strecker%2520and%2520Andreas%2520Harloff%2520and%2520Anja%2520Hennemuth%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520that%2520uses%2520sparse%2520annotations%2520from%2520clinical%250Astudies%2520to%2520train%2520a%25203D%2520segmentation%2520of%2520the%2520carotid%2520artery%2520wall.%2520We%2520use%2520a%250Acenterline%2520annotation%2520to%2520sample%2520perpendicular%2520cross-sections%2520of%2520the%2520carotid%250Aartery%2520and%2520use%2520an%2520adversarial%25202D%2520network%2520to%2520segment%2520them.%2520These%2520annotations%2520are%250Athen%2520transformed%2520into%25203D%2520pseudo-labels%2520for%2520training%2520of%2520a%25203D%2520convolutional%250Aneural%2520network%252C%2520circumventing%2520the%2520creation%2520of%2520manual%25203D%2520masks.%2520For%2520pseudo-label%250Acreation%2520in%2520the%2520bifurcation%2520area%2520we%2520propose%2520the%2520use%2520of%2520cross-sections%250Aperpendicular%2520to%2520the%2520bifurcation%2520axis%2520and%2520show%2520that%2520this%2520enhances%2520segmentation%250Aperformance.%2520Different%2520sampling%2520distances%2520had%2520a%2520lesser%2520impact.%2520The%2520proposed%250Amethod%2520allows%2520for%2520efficient%2520training%2520of%25203D%2520segmentation%252C%2520offering%2520potential%250Aimprovements%2520in%2520the%2520assessment%2520of%2520carotid%2520artery%2520stenosis%2520and%2520allowing%2520the%250Aextraction%2520of%25203D%2520biomarkers%2520such%2520as%2520plaque%2520volume.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Wall%20Segmentation%20in%203D%20Vessel%20Trees%20using%20Sparse%20Annotations&entry.906535625=Hinrich%20Rahlfs%20and%20Markus%20H%C3%BCllebrand%20and%20Sebastian%20Schmitter%20and%20Christoph%20Strecker%20and%20Andreas%20Harloff%20and%20Anja%20Hennemuth&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20that%20uses%20sparse%20annotations%20from%20clinical%0Astudies%20to%20train%20a%203D%20segmentation%20of%20the%20carotid%20artery%20wall.%20We%20use%20a%0Acenterline%20annotation%20to%20sample%20perpendicular%20cross-sections%20of%20the%20carotid%0Aartery%20and%20use%20an%20adversarial%202D%20network%20to%20segment%20them.%20These%20annotations%20are%0Athen%20transformed%20into%203D%20pseudo-labels%20for%20training%20of%20a%203D%20convolutional%0Aneural%20network%2C%20circumventing%20the%20creation%20of%20manual%203D%20masks.%20For%20pseudo-label%0Acreation%20in%20the%20bifurcation%20area%20we%20propose%20the%20use%20of%20cross-sections%0Aperpendicular%20to%20the%20bifurcation%20axis%20and%20show%20that%20this%20enhances%20segmentation%0Aperformance.%20Different%20sampling%20distances%20had%20a%20lesser%20impact.%20The%20proposed%0Amethod%20allows%20for%20efficient%20training%20of%203D%20segmentation%2C%20offering%20potential%0Aimprovements%20in%20the%20assessment%20of%20carotid%20artery%20stenosis%20and%20allowing%20the%0Aextraction%20of%203D%20biomarkers%20such%20as%20plaque%20volume.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12801v1&entry.124074799=Read"},
{"title": "MatterChat: A Multi-Modal LLM for Material Science", "author": "Yingheng Tang and Wenbin Xu and Jie Cao and Jianzhu Ma and Weilu Gao and Steve Farrell and Benjamin Erichson and Michael W. Mahoney and Andy Nonaka and Zhi Yao", "abstract": "  Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis.\n", "link": "http://arxiv.org/abs/2502.13107v1", "date": "2025-02-18", "relevancy": 2.0705, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatterChat%3A%20A%20Multi-Modal%20LLM%20for%20Material%20Science&body=Title%3A%20MatterChat%3A%20A%20Multi-Modal%20LLM%20for%20Material%20Science%0AAuthor%3A%20Yingheng%20Tang%20and%20Wenbin%20Xu%20and%20Jie%20Cao%20and%20Jianzhu%20Ma%20and%20Weilu%20Gao%20and%20Steve%20Farrell%20and%20Benjamin%20Erichson%20and%20Michael%20W.%20Mahoney%20and%20Andy%20Nonaka%20and%20Zhi%20Yao%0AAbstract%3A%20%20%20Understanding%20and%20predicting%20the%20properties%20of%20inorganic%20materials%20is%20crucial%0Afor%20accelerating%20advancements%20in%20materials%20science%20and%20driving%20applications%20in%0Aenergy%2C%20electronics%2C%20and%20beyond.%20Integrating%20material%20structure%20data%20with%0Alanguage-based%20information%20through%20multi-modal%20large%20language%20models%20%28LLMs%29%0Aoffers%20great%20potential%20to%20support%20these%20efforts%20by%20enhancing%20human-AI%0Ainteraction.%20However%2C%20a%20key%20challenge%20lies%20in%20integrating%20atomic%20structures%20at%0Afull%20resolution%20into%20LLMs.%20In%20this%20work%2C%20we%20introduce%20MatterChat%2C%20a%20versatile%0Astructure-aware%20multi-modal%20LLM%20that%20unifies%20material%20structural%20data%20and%0Atextual%20inputs%20into%20a%20single%20cohesive%20model.%20MatterChat%20employs%20a%20bridging%0Amodule%20to%20effectively%20align%20a%20pretrained%20machine%20learning%20interatomic%20potential%0Awith%20a%20pretrained%20LLM%2C%20reducing%20training%20costs%20and%20enhancing%20flexibility.%20Our%0Aresults%20demonstrate%20that%20MatterChat%20significantly%20improves%20performance%20in%0Amaterial%20property%20prediction%20and%20human-AI%20interaction%2C%20surpassing%0Ageneral-purpose%20LLMs%20such%20as%20GPT-4.%20We%20also%20demonstrate%20its%20usefulness%20in%0Aapplications%20such%20as%20more%20advanced%20scientific%20reasoning%20and%20step-by-step%0Amaterial%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatterChat%253A%2520A%2520Multi-Modal%2520LLM%2520for%2520Material%2520Science%26entry.906535625%3DYingheng%2520Tang%2520and%2520Wenbin%2520Xu%2520and%2520Jie%2520Cao%2520and%2520Jianzhu%2520Ma%2520and%2520Weilu%2520Gao%2520and%2520Steve%2520Farrell%2520and%2520Benjamin%2520Erichson%2520and%2520Michael%2520W.%2520Mahoney%2520and%2520Andy%2520Nonaka%2520and%2520Zhi%2520Yao%26entry.1292438233%3D%2520%2520Understanding%2520and%2520predicting%2520the%2520properties%2520of%2520inorganic%2520materials%2520is%2520crucial%250Afor%2520accelerating%2520advancements%2520in%2520materials%2520science%2520and%2520driving%2520applications%2520in%250Aenergy%252C%2520electronics%252C%2520and%2520beyond.%2520Integrating%2520material%2520structure%2520data%2520with%250Alanguage-based%2520information%2520through%2520multi-modal%2520large%2520language%2520models%2520%2528LLMs%2529%250Aoffers%2520great%2520potential%2520to%2520support%2520these%2520efforts%2520by%2520enhancing%2520human-AI%250Ainteraction.%2520However%252C%2520a%2520key%2520challenge%2520lies%2520in%2520integrating%2520atomic%2520structures%2520at%250Afull%2520resolution%2520into%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MatterChat%252C%2520a%2520versatile%250Astructure-aware%2520multi-modal%2520LLM%2520that%2520unifies%2520material%2520structural%2520data%2520and%250Atextual%2520inputs%2520into%2520a%2520single%2520cohesive%2520model.%2520MatterChat%2520employs%2520a%2520bridging%250Amodule%2520to%2520effectively%2520align%2520a%2520pretrained%2520machine%2520learning%2520interatomic%2520potential%250Awith%2520a%2520pretrained%2520LLM%252C%2520reducing%2520training%2520costs%2520and%2520enhancing%2520flexibility.%2520Our%250Aresults%2520demonstrate%2520that%2520MatterChat%2520significantly%2520improves%2520performance%2520in%250Amaterial%2520property%2520prediction%2520and%2520human-AI%2520interaction%252C%2520surpassing%250Ageneral-purpose%2520LLMs%2520such%2520as%2520GPT-4.%2520We%2520also%2520demonstrate%2520its%2520usefulness%2520in%250Aapplications%2520such%2520as%2520more%2520advanced%2520scientific%2520reasoning%2520and%2520step-by-step%250Amaterial%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatterChat%3A%20A%20Multi-Modal%20LLM%20for%20Material%20Science&entry.906535625=Yingheng%20Tang%20and%20Wenbin%20Xu%20and%20Jie%20Cao%20and%20Jianzhu%20Ma%20and%20Weilu%20Gao%20and%20Steve%20Farrell%20and%20Benjamin%20Erichson%20and%20Michael%20W.%20Mahoney%20and%20Andy%20Nonaka%20and%20Zhi%20Yao&entry.1292438233=%20%20Understanding%20and%20predicting%20the%20properties%20of%20inorganic%20materials%20is%20crucial%0Afor%20accelerating%20advancements%20in%20materials%20science%20and%20driving%20applications%20in%0Aenergy%2C%20electronics%2C%20and%20beyond.%20Integrating%20material%20structure%20data%20with%0Alanguage-based%20information%20through%20multi-modal%20large%20language%20models%20%28LLMs%29%0Aoffers%20great%20potential%20to%20support%20these%20efforts%20by%20enhancing%20human-AI%0Ainteraction.%20However%2C%20a%20key%20challenge%20lies%20in%20integrating%20atomic%20structures%20at%0Afull%20resolution%20into%20LLMs.%20In%20this%20work%2C%20we%20introduce%20MatterChat%2C%20a%20versatile%0Astructure-aware%20multi-modal%20LLM%20that%20unifies%20material%20structural%20data%20and%0Atextual%20inputs%20into%20a%20single%20cohesive%20model.%20MatterChat%20employs%20a%20bridging%0Amodule%20to%20effectively%20align%20a%20pretrained%20machine%20learning%20interatomic%20potential%0Awith%20a%20pretrained%20LLM%2C%20reducing%20training%20costs%20and%20enhancing%20flexibility.%20Our%0Aresults%20demonstrate%20that%20MatterChat%20significantly%20improves%20performance%20in%0Amaterial%20property%20prediction%20and%20human-AI%20interaction%2C%20surpassing%0Ageneral-purpose%20LLMs%20such%20as%20GPT-4.%20We%20also%20demonstrate%20its%20usefulness%20in%0Aapplications%20such%20as%20more%20advanced%20scientific%20reasoning%20and%20step-by-step%0Amaterial%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13107v1&entry.124074799=Read"},
{"title": "LADDER: Language Driven Slice Discovery and Error Rectification", "author": "Shantanu Ghosh and Rayan Syed and Chenyu Wang and Clare B. Poynton and Shyam Visweswaran and Kayhan Batmanghelich", "abstract": "  Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases.\n", "link": "http://arxiv.org/abs/2408.07832v9", "date": "2025-02-18", "relevancy": 2.0687, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LADDER%3A%20Language%20Driven%20Slice%20Discovery%20and%20Error%20Rectification&body=Title%3A%20LADDER%3A%20Language%20Driven%20Slice%20Discovery%20and%20Error%20Rectification%0AAuthor%3A%20Shantanu%20Ghosh%20and%20Rayan%20Syed%20and%20Chenyu%20Wang%20and%20Clare%20B.%20Poynton%20and%20Shyam%20Visweswaran%20and%20Kayhan%20Batmanghelich%0AAbstract%3A%20%20%20Error%20slice%20discovery%20is%20crucial%20to%20diagnose%20and%20mitigate%20model%20errors.%0ACurrent%20clustering%20or%20discrete%20attribute-based%20slice%20discovery%20methods%20face%20key%0Alimitations%3A%201%29%20clustering%20results%20in%20incoherent%20slices%2C%20while%20assigning%0Adiscrete%20attributes%20to%20slices%20leads%20to%20incomplete%20coverage%20of%20error%20patterns%0Adue%20to%20missing%20or%20insufficient%20attributes%3B%202%29%20these%20methods%20lack%20complex%0Areasoning%2C%20preventing%20them%20from%20fully%20explaining%20model%20biases%3B%203%29%20they%20fail%20to%0Aintegrate%20%5Ctextit%7Bdomain%20knowledge%7D%2C%20limiting%20their%20usage%20in%20specialized%20fields%0A%5Ceg%20radiology.%20We%20propose%5Cladder%20%28%5Cunderline%7BLa%7Dnguage-%5Cunderline%7BD%7Driven%0A%5Cunderline%7BD%7Discovery%20and%20%5Cunderline%7BE%7Drror%20%5Cunderline%7BR%7Dectification%29%2C%20to%0Aaddress%20the%20limitations%20by%3A%20%281%29%20leveraging%20the%20flexibility%20of%20natural%20language%0Ato%20address%20incompleteness%2C%20%282%29%20employing%20LLM%27s%20latent%20%5Ctextit%7Bdomain%20knowledge%7D%0Aand%20advanced%20reasoning%20to%20analyze%20sentences%20and%20derive%20testable%20hypotheses%0Adirectly%2C%20identifying%20biased%20attributes%2C%20and%20form%20coherent%20error%20slices%20without%0Aclustering.%20Existing%20mitigation%20methods%20typically%20address%20only%20the%0Aworst-performing%20group%2C%20often%20amplifying%20errors%20in%20other%20subgroups.%20In%0Acontrast%2C%5Cladder%20generates%20pseudo%20attributes%20from%20the%20discovered%20hypotheses%20to%0Amitigate%20errors%20across%20all%20biases%20without%20explicit%20attribute%20annotations%20or%0Aprior%20knowledge%20of%20bias.%20Rigorous%20evaluations%20on%206%20datasets%20spanning%20natural%0Aand%20medical%20images%20--%20comparing%20200%2B%20classifiers%20with%20diverse%20architectures%2C%0Apretraining%20strategies%2C%20and%20LLMs%20--%20show%20that%5Cladder%20consistently%20outperforms%0Aexisting%20baselines%20in%20discovering%20and%20mitigating%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07832v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLADDER%253A%2520Language%2520Driven%2520Slice%2520Discovery%2520and%2520Error%2520Rectification%26entry.906535625%3DShantanu%2520Ghosh%2520and%2520Rayan%2520Syed%2520and%2520Chenyu%2520Wang%2520and%2520Clare%2520B.%2520Poynton%2520and%2520Shyam%2520Visweswaran%2520and%2520Kayhan%2520Batmanghelich%26entry.1292438233%3D%2520%2520Error%2520slice%2520discovery%2520is%2520crucial%2520to%2520diagnose%2520and%2520mitigate%2520model%2520errors.%250ACurrent%2520clustering%2520or%2520discrete%2520attribute-based%2520slice%2520discovery%2520methods%2520face%2520key%250Alimitations%253A%25201%2529%2520clustering%2520results%2520in%2520incoherent%2520slices%252C%2520while%2520assigning%250Adiscrete%2520attributes%2520to%2520slices%2520leads%2520to%2520incomplete%2520coverage%2520of%2520error%2520patterns%250Adue%2520to%2520missing%2520or%2520insufficient%2520attributes%253B%25202%2529%2520these%2520methods%2520lack%2520complex%250Areasoning%252C%2520preventing%2520them%2520from%2520fully%2520explaining%2520model%2520biases%253B%25203%2529%2520they%2520fail%2520to%250Aintegrate%2520%255Ctextit%257Bdomain%2520knowledge%257D%252C%2520limiting%2520their%2520usage%2520in%2520specialized%2520fields%250A%255Ceg%2520radiology.%2520We%2520propose%255Cladder%2520%2528%255Cunderline%257BLa%257Dnguage-%255Cunderline%257BD%257Driven%250A%255Cunderline%257BD%257Discovery%2520and%2520%255Cunderline%257BE%257Drror%2520%255Cunderline%257BR%257Dectification%2529%252C%2520to%250Aaddress%2520the%2520limitations%2520by%253A%2520%25281%2529%2520leveraging%2520the%2520flexibility%2520of%2520natural%2520language%250Ato%2520address%2520incompleteness%252C%2520%25282%2529%2520employing%2520LLM%2527s%2520latent%2520%255Ctextit%257Bdomain%2520knowledge%257D%250Aand%2520advanced%2520reasoning%2520to%2520analyze%2520sentences%2520and%2520derive%2520testable%2520hypotheses%250Adirectly%252C%2520identifying%2520biased%2520attributes%252C%2520and%2520form%2520coherent%2520error%2520slices%2520without%250Aclustering.%2520Existing%2520mitigation%2520methods%2520typically%2520address%2520only%2520the%250Aworst-performing%2520group%252C%2520often%2520amplifying%2520errors%2520in%2520other%2520subgroups.%2520In%250Acontrast%252C%255Cladder%2520generates%2520pseudo%2520attributes%2520from%2520the%2520discovered%2520hypotheses%2520to%250Amitigate%2520errors%2520across%2520all%2520biases%2520without%2520explicit%2520attribute%2520annotations%2520or%250Aprior%2520knowledge%2520of%2520bias.%2520Rigorous%2520evaluations%2520on%25206%2520datasets%2520spanning%2520natural%250Aand%2520medical%2520images%2520--%2520comparing%2520200%252B%2520classifiers%2520with%2520diverse%2520architectures%252C%250Apretraining%2520strategies%252C%2520and%2520LLMs%2520--%2520show%2520that%255Cladder%2520consistently%2520outperforms%250Aexisting%2520baselines%2520in%2520discovering%2520and%2520mitigating%2520biases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07832v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LADDER%3A%20Language%20Driven%20Slice%20Discovery%20and%20Error%20Rectification&entry.906535625=Shantanu%20Ghosh%20and%20Rayan%20Syed%20and%20Chenyu%20Wang%20and%20Clare%20B.%20Poynton%20and%20Shyam%20Visweswaran%20and%20Kayhan%20Batmanghelich&entry.1292438233=%20%20Error%20slice%20discovery%20is%20crucial%20to%20diagnose%20and%20mitigate%20model%20errors.%0ACurrent%20clustering%20or%20discrete%20attribute-based%20slice%20discovery%20methods%20face%20key%0Alimitations%3A%201%29%20clustering%20results%20in%20incoherent%20slices%2C%20while%20assigning%0Adiscrete%20attributes%20to%20slices%20leads%20to%20incomplete%20coverage%20of%20error%20patterns%0Adue%20to%20missing%20or%20insufficient%20attributes%3B%202%29%20these%20methods%20lack%20complex%0Areasoning%2C%20preventing%20them%20from%20fully%20explaining%20model%20biases%3B%203%29%20they%20fail%20to%0Aintegrate%20%5Ctextit%7Bdomain%20knowledge%7D%2C%20limiting%20their%20usage%20in%20specialized%20fields%0A%5Ceg%20radiology.%20We%20propose%5Cladder%20%28%5Cunderline%7BLa%7Dnguage-%5Cunderline%7BD%7Driven%0A%5Cunderline%7BD%7Discovery%20and%20%5Cunderline%7BE%7Drror%20%5Cunderline%7BR%7Dectification%29%2C%20to%0Aaddress%20the%20limitations%20by%3A%20%281%29%20leveraging%20the%20flexibility%20of%20natural%20language%0Ato%20address%20incompleteness%2C%20%282%29%20employing%20LLM%27s%20latent%20%5Ctextit%7Bdomain%20knowledge%7D%0Aand%20advanced%20reasoning%20to%20analyze%20sentences%20and%20derive%20testable%20hypotheses%0Adirectly%2C%20identifying%20biased%20attributes%2C%20and%20form%20coherent%20error%20slices%20without%0Aclustering.%20Existing%20mitigation%20methods%20typically%20address%20only%20the%0Aworst-performing%20group%2C%20often%20amplifying%20errors%20in%20other%20subgroups.%20In%0Acontrast%2C%5Cladder%20generates%20pseudo%20attributes%20from%20the%20discovered%20hypotheses%20to%0Amitigate%20errors%20across%20all%20biases%20without%20explicit%20attribute%20annotations%20or%0Aprior%20knowledge%20of%20bias.%20Rigorous%20evaluations%20on%206%20datasets%20spanning%20natural%0Aand%20medical%20images%20--%20comparing%20200%2B%20classifiers%20with%20diverse%20architectures%2C%0Apretraining%20strategies%2C%20and%20LLMs%20--%20show%20that%5Cladder%20consistently%20outperforms%0Aexisting%20baselines%20in%20discovering%20and%20mitigating%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07832v9&entry.124074799=Read"},
{"title": "High-Fidelity Music Vocoder using Neural Audio Codecs", "author": "Luca A. Lanzend\u00f6rfer and Florian Gr\u00f6tschla and Michael Ungersb\u00f6ck and Roger Wattenhofer", "abstract": "  While neural vocoders have made significant progress in high-fidelity speech\nsynthesis, their application on polyphonic music has remained underexplored. In\nthis work, we propose DisCoder, a neural vocoder that leverages a generative\nadversarial encoder-decoder architecture informed by a neural audio codec to\nreconstruct high-fidelity 44.1 kHz audio from mel spectrograms. Our approach\nfirst transforms the mel spectrogram into a lower-dimensional representation\naligned with the Descript Audio Codec (DAC) latent space before reconstructing\nit to an audio signal using a fine-tuned DAC decoder. DisCoder achieves\nstate-of-the-art performance in music synthesis on several objective metrics\nand in a MUSHRA listening study. Our approach also shows competitive\nperformance in speech synthesis, highlighting its potential as a universal\nvocoder.\n", "link": "http://arxiv.org/abs/2502.12759v1", "date": "2025-02-18", "relevancy": 2.0679, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5413}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5077}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Fidelity%20Music%20Vocoder%20using%20Neural%20Audio%20Codecs&body=Title%3A%20High-Fidelity%20Music%20Vocoder%20using%20Neural%20Audio%20Codecs%0AAuthor%3A%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Florian%20Gr%C3%B6tschla%20and%20Michael%20Ungersb%C3%B6ck%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20While%20neural%20vocoders%20have%20made%20significant%20progress%20in%20high-fidelity%20speech%0Asynthesis%2C%20their%20application%20on%20polyphonic%20music%20has%20remained%20underexplored.%20In%0Athis%20work%2C%20we%20propose%20DisCoder%2C%20a%20neural%20vocoder%20that%20leverages%20a%20generative%0Aadversarial%20encoder-decoder%20architecture%20informed%20by%20a%20neural%20audio%20codec%20to%0Areconstruct%20high-fidelity%2044.1%20kHz%20audio%20from%20mel%20spectrograms.%20Our%20approach%0Afirst%20transforms%20the%20mel%20spectrogram%20into%20a%20lower-dimensional%20representation%0Aaligned%20with%20the%20Descript%20Audio%20Codec%20%28DAC%29%20latent%20space%20before%20reconstructing%0Ait%20to%20an%20audio%20signal%20using%20a%20fine-tuned%20DAC%20decoder.%20DisCoder%20achieves%0Astate-of-the-art%20performance%20in%20music%20synthesis%20on%20several%20objective%20metrics%0Aand%20in%20a%20MUSHRA%20listening%20study.%20Our%20approach%20also%20shows%20competitive%0Aperformance%20in%20speech%20synthesis%2C%20highlighting%20its%20potential%20as%20a%20universal%0Avocoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Fidelity%2520Music%2520Vocoder%2520using%2520Neural%2520Audio%2520Codecs%26entry.906535625%3DLuca%2520A.%2520Lanzend%25C3%25B6rfer%2520and%2520Florian%2520Gr%25C3%25B6tschla%2520and%2520Michael%2520Ungersb%25C3%25B6ck%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520While%2520neural%2520vocoders%2520have%2520made%2520significant%2520progress%2520in%2520high-fidelity%2520speech%250Asynthesis%252C%2520their%2520application%2520on%2520polyphonic%2520music%2520has%2520remained%2520underexplored.%2520In%250Athis%2520work%252C%2520we%2520propose%2520DisCoder%252C%2520a%2520neural%2520vocoder%2520that%2520leverages%2520a%2520generative%250Aadversarial%2520encoder-decoder%2520architecture%2520informed%2520by%2520a%2520neural%2520audio%2520codec%2520to%250Areconstruct%2520high-fidelity%252044.1%2520kHz%2520audio%2520from%2520mel%2520spectrograms.%2520Our%2520approach%250Afirst%2520transforms%2520the%2520mel%2520spectrogram%2520into%2520a%2520lower-dimensional%2520representation%250Aaligned%2520with%2520the%2520Descript%2520Audio%2520Codec%2520%2528DAC%2529%2520latent%2520space%2520before%2520reconstructing%250Ait%2520to%2520an%2520audio%2520signal%2520using%2520a%2520fine-tuned%2520DAC%2520decoder.%2520DisCoder%2520achieves%250Astate-of-the-art%2520performance%2520in%2520music%2520synthesis%2520on%2520several%2520objective%2520metrics%250Aand%2520in%2520a%2520MUSHRA%2520listening%2520study.%2520Our%2520approach%2520also%2520shows%2520competitive%250Aperformance%2520in%2520speech%2520synthesis%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520universal%250Avocoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Fidelity%20Music%20Vocoder%20using%20Neural%20Audio%20Codecs&entry.906535625=Luca%20A.%20Lanzend%C3%B6rfer%20and%20Florian%20Gr%C3%B6tschla%20and%20Michael%20Ungersb%C3%B6ck%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20While%20neural%20vocoders%20have%20made%20significant%20progress%20in%20high-fidelity%20speech%0Asynthesis%2C%20their%20application%20on%20polyphonic%20music%20has%20remained%20underexplored.%20In%0Athis%20work%2C%20we%20propose%20DisCoder%2C%20a%20neural%20vocoder%20that%20leverages%20a%20generative%0Aadversarial%20encoder-decoder%20architecture%20informed%20by%20a%20neural%20audio%20codec%20to%0Areconstruct%20high-fidelity%2044.1%20kHz%20audio%20from%20mel%20spectrograms.%20Our%20approach%0Afirst%20transforms%20the%20mel%20spectrogram%20into%20a%20lower-dimensional%20representation%0Aaligned%20with%20the%20Descript%20Audio%20Codec%20%28DAC%29%20latent%20space%20before%20reconstructing%0Ait%20to%20an%20audio%20signal%20using%20a%20fine-tuned%20DAC%20decoder.%20DisCoder%20achieves%0Astate-of-the-art%20performance%20in%20music%20synthesis%20on%20several%20objective%20metrics%0Aand%20in%20a%20MUSHRA%20listening%20study.%20Our%20approach%20also%20shows%20competitive%0Aperformance%20in%20speech%20synthesis%2C%20highlighting%20its%20potential%20as%20a%20universal%0Avocoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12759v1&entry.124074799=Read"},
{"title": "AIDE: AI-Driven Exploration in the Space of Code", "author": "Zhengyao Jiang and Dominik Schmidt and Dhruv Srikanth and Dixing Xu and Ian Kaplan and Deniss Jacenko and Yuxiang Wu", "abstract": "  Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.\n", "link": "http://arxiv.org/abs/2502.13138v1", "date": "2025-02-18", "relevancy": 2.0658, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5681}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5095}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIDE%3A%20AI-Driven%20Exploration%20in%20the%20Space%20of%20Code&body=Title%3A%20AIDE%3A%20AI-Driven%20Exploration%20in%20the%20Space%20of%20Code%0AAuthor%3A%20Zhengyao%20Jiang%20and%20Dominik%20Schmidt%20and%20Dhruv%20Srikanth%20and%20Dixing%20Xu%20and%20Ian%20Kaplan%20and%20Deniss%20Jacenko%20and%20Yuxiang%20Wu%0AAbstract%3A%20%20%20Machine%20learning%2C%20the%20foundation%20of%20modern%20artificial%20intelligence%2C%20has%0Adriven%20innovations%20that%20have%20fundamentally%20transformed%20the%20world.%20Yet%2C%20behind%0Aadvancements%20lies%20a%20complex%20and%20often%20tedious%20process%20requiring%20labor%20and%0Acompute%20intensive%20iteration%20and%20experimentation.%20Engineers%20and%20scientists%0Adeveloping%20machine%20learning%20models%20spend%20much%20of%20their%20time%20on%20trial-and-error%0Atasks%20instead%20of%20conceptualizing%20innovative%20solutions%20or%20research%20hypotheses.%0ATo%20address%20this%20challenge%2C%20we%20introduce%20AI-Driven%20Exploration%20%28AIDE%29%2C%20a%20machine%0Alearning%20engineering%20agent%20powered%20by%20large%20language%20models%20%28LLMs%29.%20AIDE%20frames%0Amachine%20learning%20engineering%20as%20a%20code%20optimization%20problem%2C%20and%20formulates%0Atrial-and-error%20as%20a%20tree%20search%20in%20the%20space%20of%20potential%20solutions.%20By%0Astrategically%20reusing%20and%20refining%20promising%20solutions%2C%20AIDE%20effectively%20trades%0Acomputational%20resources%20for%20enhanced%20performance%2C%20achieving%20state-of-the-art%0Aresults%20on%20multiple%20machine%20learning%20engineering%20benchmarks%2C%20including%20our%0AKaggle%20evaluations%2C%20OpenAI%20MLE-Bench%20and%20METRs%20RE-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIDE%253A%2520AI-Driven%2520Exploration%2520in%2520the%2520Space%2520of%2520Code%26entry.906535625%3DZhengyao%2520Jiang%2520and%2520Dominik%2520Schmidt%2520and%2520Dhruv%2520Srikanth%2520and%2520Dixing%2520Xu%2520and%2520Ian%2520Kaplan%2520and%2520Deniss%2520Jacenko%2520and%2520Yuxiang%2520Wu%26entry.1292438233%3D%2520%2520Machine%2520learning%252C%2520the%2520foundation%2520of%2520modern%2520artificial%2520intelligence%252C%2520has%250Adriven%2520innovations%2520that%2520have%2520fundamentally%2520transformed%2520the%2520world.%2520Yet%252C%2520behind%250Aadvancements%2520lies%2520a%2520complex%2520and%2520often%2520tedious%2520process%2520requiring%2520labor%2520and%250Acompute%2520intensive%2520iteration%2520and%2520experimentation.%2520Engineers%2520and%2520scientists%250Adeveloping%2520machine%2520learning%2520models%2520spend%2520much%2520of%2520their%2520time%2520on%2520trial-and-error%250Atasks%2520instead%2520of%2520conceptualizing%2520innovative%2520solutions%2520or%2520research%2520hypotheses.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520AI-Driven%2520Exploration%2520%2528AIDE%2529%252C%2520a%2520machine%250Alearning%2520engineering%2520agent%2520powered%2520by%2520large%2520language%2520models%2520%2528LLMs%2529.%2520AIDE%2520frames%250Amachine%2520learning%2520engineering%2520as%2520a%2520code%2520optimization%2520problem%252C%2520and%2520formulates%250Atrial-and-error%2520as%2520a%2520tree%2520search%2520in%2520the%2520space%2520of%2520potential%2520solutions.%2520By%250Astrategically%2520reusing%2520and%2520refining%2520promising%2520solutions%252C%2520AIDE%2520effectively%2520trades%250Acomputational%2520resources%2520for%2520enhanced%2520performance%252C%2520achieving%2520state-of-the-art%250Aresults%2520on%2520multiple%2520machine%2520learning%2520engineering%2520benchmarks%252C%2520including%2520our%250AKaggle%2520evaluations%252C%2520OpenAI%2520MLE-Bench%2520and%2520METRs%2520RE-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIDE%3A%20AI-Driven%20Exploration%20in%20the%20Space%20of%20Code&entry.906535625=Zhengyao%20Jiang%20and%20Dominik%20Schmidt%20and%20Dhruv%20Srikanth%20and%20Dixing%20Xu%20and%20Ian%20Kaplan%20and%20Deniss%20Jacenko%20and%20Yuxiang%20Wu&entry.1292438233=%20%20Machine%20learning%2C%20the%20foundation%20of%20modern%20artificial%20intelligence%2C%20has%0Adriven%20innovations%20that%20have%20fundamentally%20transformed%20the%20world.%20Yet%2C%20behind%0Aadvancements%20lies%20a%20complex%20and%20often%20tedious%20process%20requiring%20labor%20and%0Acompute%20intensive%20iteration%20and%20experimentation.%20Engineers%20and%20scientists%0Adeveloping%20machine%20learning%20models%20spend%20much%20of%20their%20time%20on%20trial-and-error%0Atasks%20instead%20of%20conceptualizing%20innovative%20solutions%20or%20research%20hypotheses.%0ATo%20address%20this%20challenge%2C%20we%20introduce%20AI-Driven%20Exploration%20%28AIDE%29%2C%20a%20machine%0Alearning%20engineering%20agent%20powered%20by%20large%20language%20models%20%28LLMs%29.%20AIDE%20frames%0Amachine%20learning%20engineering%20as%20a%20code%20optimization%20problem%2C%20and%20formulates%0Atrial-and-error%20as%20a%20tree%20search%20in%20the%20space%20of%20potential%20solutions.%20By%0Astrategically%20reusing%20and%20refining%20promising%20solutions%2C%20AIDE%20effectively%20trades%0Acomputational%20resources%20for%20enhanced%20performance%2C%20achieving%20state-of-the-art%0Aresults%20on%20multiple%20machine%20learning%20engineering%20benchmarks%2C%20including%20our%0AKaggle%20evaluations%2C%20OpenAI%20MLE-Bench%20and%20METRs%20RE-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13138v1&entry.124074799=Read"},
{"title": "Every Expert Matters: Towards Effective Knowledge Distillation for\n  Mixture-of-Experts Language Models", "author": "Gyeongman Kim and Gyouk Chu and Eunho Yang", "abstract": "  With the emergence of Mixture-of-Experts (MoE), the efficient scaling of\nmodel size has accelerated the development of large language models in recent\nyears. However, their high memory requirements prevent their use in\nresource-constrained environments. While knowledge distillation (KD) has been a\nproven method for model compression, its application to MoE teacher models\nremains underexplored. Through our investigation, we discover that\nnon-activated experts in MoE models possess valuable knowledge that benefits\nstudent models. We further demonstrate that existing KD methods are not optimal\nfor compressing MoE models, as they fail to leverage this knowledge\neffectively. To address this, we propose two intuitive MoE-specific KD methods\nfor the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR),\nboth designed to effectively extract knowledge from all experts. Specifically,\nKA augments knowledge by sampling experts multiple times, while SAR uses all\nexperts and adjusts the expert weights through router training to provide\noptimal knowledge. Extensive experiments show that our methods outperform\nconventional KD methods, demonstrating their effectiveness for MoE teacher\nmodels.\n", "link": "http://arxiv.org/abs/2502.12947v1", "date": "2025-02-18", "relevancy": 1.9334, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Every%20Expert%20Matters%3A%20Towards%20Effective%20Knowledge%20Distillation%20for%0A%20%20Mixture-of-Experts%20Language%20Models&body=Title%3A%20Every%20Expert%20Matters%3A%20Towards%20Effective%20Knowledge%20Distillation%20for%0A%20%20Mixture-of-Experts%20Language%20Models%0AAuthor%3A%20Gyeongman%20Kim%20and%20Gyouk%20Chu%20and%20Eunho%20Yang%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20Mixture-of-Experts%20%28MoE%29%2C%20the%20efficient%20scaling%20of%0Amodel%20size%20has%20accelerated%20the%20development%20of%20large%20language%20models%20in%20recent%0Ayears.%20However%2C%20their%20high%20memory%20requirements%20prevent%20their%20use%20in%0Aresource-constrained%20environments.%20While%20knowledge%20distillation%20%28KD%29%20has%20been%20a%0Aproven%20method%20for%20model%20compression%2C%20its%20application%20to%20MoE%20teacher%20models%0Aremains%20underexplored.%20Through%20our%20investigation%2C%20we%20discover%20that%0Anon-activated%20experts%20in%20MoE%20models%20possess%20valuable%20knowledge%20that%20benefits%0Astudent%20models.%20We%20further%20demonstrate%20that%20existing%20KD%20methods%20are%20not%20optimal%0Afor%20compressing%20MoE%20models%2C%20as%20they%20fail%20to%20leverage%20this%20knowledge%0Aeffectively.%20To%20address%20this%2C%20we%20propose%20two%20intuitive%20MoE-specific%20KD%20methods%0Afor%20the%20first%20time%3A%20Knowledge%20Augmentation%20%28KA%29%20and%20Student-Aware%20Router%20%28SAR%29%2C%0Aboth%20designed%20to%20effectively%20extract%20knowledge%20from%20all%20experts.%20Specifically%2C%0AKA%20augments%20knowledge%20by%20sampling%20experts%20multiple%20times%2C%20while%20SAR%20uses%20all%0Aexperts%20and%20adjusts%20the%20expert%20weights%20through%20router%20training%20to%20provide%0Aoptimal%20knowledge.%20Extensive%20experiments%20show%20that%20our%20methods%20outperform%0Aconventional%20KD%20methods%2C%20demonstrating%20their%20effectiveness%20for%20MoE%20teacher%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvery%2520Expert%2520Matters%253A%2520Towards%2520Effective%2520Knowledge%2520Distillation%2520for%250A%2520%2520Mixture-of-Experts%2520Language%2520Models%26entry.906535625%3DGyeongman%2520Kim%2520and%2520Gyouk%2520Chu%2520and%2520Eunho%2520Yang%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520Mixture-of-Experts%2520%2528MoE%2529%252C%2520the%2520efficient%2520scaling%2520of%250Amodel%2520size%2520has%2520accelerated%2520the%2520development%2520of%2520large%2520language%2520models%2520in%2520recent%250Ayears.%2520However%252C%2520their%2520high%2520memory%2520requirements%2520prevent%2520their%2520use%2520in%250Aresource-constrained%2520environments.%2520While%2520knowledge%2520distillation%2520%2528KD%2529%2520has%2520been%2520a%250Aproven%2520method%2520for%2520model%2520compression%252C%2520its%2520application%2520to%2520MoE%2520teacher%2520models%250Aremains%2520underexplored.%2520Through%2520our%2520investigation%252C%2520we%2520discover%2520that%250Anon-activated%2520experts%2520in%2520MoE%2520models%2520possess%2520valuable%2520knowledge%2520that%2520benefits%250Astudent%2520models.%2520We%2520further%2520demonstrate%2520that%2520existing%2520KD%2520methods%2520are%2520not%2520optimal%250Afor%2520compressing%2520MoE%2520models%252C%2520as%2520they%2520fail%2520to%2520leverage%2520this%2520knowledge%250Aeffectively.%2520To%2520address%2520this%252C%2520we%2520propose%2520two%2520intuitive%2520MoE-specific%2520KD%2520methods%250Afor%2520the%2520first%2520time%253A%2520Knowledge%2520Augmentation%2520%2528KA%2529%2520and%2520Student-Aware%2520Router%2520%2528SAR%2529%252C%250Aboth%2520designed%2520to%2520effectively%2520extract%2520knowledge%2520from%2520all%2520experts.%2520Specifically%252C%250AKA%2520augments%2520knowledge%2520by%2520sampling%2520experts%2520multiple%2520times%252C%2520while%2520SAR%2520uses%2520all%250Aexperts%2520and%2520adjusts%2520the%2520expert%2520weights%2520through%2520router%2520training%2520to%2520provide%250Aoptimal%2520knowledge.%2520Extensive%2520experiments%2520show%2520that%2520our%2520methods%2520outperform%250Aconventional%2520KD%2520methods%252C%2520demonstrating%2520their%2520effectiveness%2520for%2520MoE%2520teacher%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Every%20Expert%20Matters%3A%20Towards%20Effective%20Knowledge%20Distillation%20for%0A%20%20Mixture-of-Experts%20Language%20Models&entry.906535625=Gyeongman%20Kim%20and%20Gyouk%20Chu%20and%20Eunho%20Yang&entry.1292438233=%20%20With%20the%20emergence%20of%20Mixture-of-Experts%20%28MoE%29%2C%20the%20efficient%20scaling%20of%0Amodel%20size%20has%20accelerated%20the%20development%20of%20large%20language%20models%20in%20recent%0Ayears.%20However%2C%20their%20high%20memory%20requirements%20prevent%20their%20use%20in%0Aresource-constrained%20environments.%20While%20knowledge%20distillation%20%28KD%29%20has%20been%20a%0Aproven%20method%20for%20model%20compression%2C%20its%20application%20to%20MoE%20teacher%20models%0Aremains%20underexplored.%20Through%20our%20investigation%2C%20we%20discover%20that%0Anon-activated%20experts%20in%20MoE%20models%20possess%20valuable%20knowledge%20that%20benefits%0Astudent%20models.%20We%20further%20demonstrate%20that%20existing%20KD%20methods%20are%20not%20optimal%0Afor%20compressing%20MoE%20models%2C%20as%20they%20fail%20to%20leverage%20this%20knowledge%0Aeffectively.%20To%20address%20this%2C%20we%20propose%20two%20intuitive%20MoE-specific%20KD%20methods%0Afor%20the%20first%20time%3A%20Knowledge%20Augmentation%20%28KA%29%20and%20Student-Aware%20Router%20%28SAR%29%2C%0Aboth%20designed%20to%20effectively%20extract%20knowledge%20from%20all%20experts.%20Specifically%2C%0AKA%20augments%20knowledge%20by%20sampling%20experts%20multiple%20times%2C%20while%20SAR%20uses%20all%0Aexperts%20and%20adjusts%20the%20expert%20weights%20through%20router%20training%20to%20provide%0Aoptimal%20knowledge.%20Extensive%20experiments%20show%20that%20our%20methods%20outperform%0Aconventional%20KD%20methods%2C%20demonstrating%20their%20effectiveness%20for%20MoE%20teacher%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12947v1&entry.124074799=Read"},
{"title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement\n  Learning", "author": "Ruotian Ma and Peisong Wang and Cheng Liu and Xingyan Liu and Jiaqi Chen and Bang Zhang and Xin Zhou and Nan Du and Jia Li", "abstract": "  Recent studies have demonstrated the effectiveness of LLM test-time scaling.\nHowever, existing approaches to incentivize LLMs' deep thinking abilities\ngenerally require large-scale data or significant training efforts. Meanwhile,\nit remains unclear how to improve the thinking abilities of less powerful base\nmodels. In this work, we introduce S$^2$R, an efficient framework that enhances\nLLM reasoning by teaching models to self-verify and self-correct during\ninference. Specifically, we first initialize LLMs with iterative\nself-verification and self-correction behaviors through supervised fine-tuning\non carefully curated data. The self-verification and self-correction skills are\nthen further strengthened by both outcome-level and process-level reinforcement\nlearning, with minimized resource requirements, enabling the model to\nadaptively refine its reasoning process during inference. Our results\ndemonstrate that, with only 3.1k self-verifying and self-correcting behavior\ninitialization samples, Qwen2.5-math-7B achieves an accuracy improvement from\n51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of\nlong-CoT distilled data. Extensive experiments and analysis based on three base\nmodels across both in-domain and out-of-domain benchmarks validate the\neffectiveness of S$^2$R. Our code and data are available at\nhttps://github.com/NineAbyss/S2R.\n", "link": "http://arxiv.org/abs/2502.12853v1", "date": "2025-02-18", "relevancy": 1.9078, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S%24%5E2%24R%3A%20Teaching%20LLMs%20to%20Self-verify%20and%20Self-correct%20via%20Reinforcement%0A%20%20Learning&body=Title%3A%20S%24%5E2%24R%3A%20Teaching%20LLMs%20to%20Self-verify%20and%20Self-correct%20via%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Ruotian%20Ma%20and%20Peisong%20Wang%20and%20Cheng%20Liu%20and%20Xingyan%20Liu%20and%20Jiaqi%20Chen%20and%20Bang%20Zhang%20and%20Xin%20Zhou%20and%20Nan%20Du%20and%20Jia%20Li%0AAbstract%3A%20%20%20Recent%20studies%20have%20demonstrated%20the%20effectiveness%20of%20LLM%20test-time%20scaling.%0AHowever%2C%20existing%20approaches%20to%20incentivize%20LLMs%27%20deep%20thinking%20abilities%0Agenerally%20require%20large-scale%20data%20or%20significant%20training%20efforts.%20Meanwhile%2C%0Ait%20remains%20unclear%20how%20to%20improve%20the%20thinking%20abilities%20of%20less%20powerful%20base%0Amodels.%20In%20this%20work%2C%20we%20introduce%20S%24%5E2%24R%2C%20an%20efficient%20framework%20that%20enhances%0ALLM%20reasoning%20by%20teaching%20models%20to%20self-verify%20and%20self-correct%20during%0Ainference.%20Specifically%2C%20we%20first%20initialize%20LLMs%20with%20iterative%0Aself-verification%20and%20self-correction%20behaviors%20through%20supervised%20fine-tuning%0Aon%20carefully%20curated%20data.%20The%20self-verification%20and%20self-correction%20skills%20are%0Athen%20further%20strengthened%20by%20both%20outcome-level%20and%20process-level%20reinforcement%0Alearning%2C%20with%20minimized%20resource%20requirements%2C%20enabling%20the%20model%20to%0Aadaptively%20refine%20its%20reasoning%20process%20during%20inference.%20Our%20results%0Ademonstrate%20that%2C%20with%20only%203.1k%20self-verifying%20and%20self-correcting%20behavior%0Ainitialization%20samples%2C%20Qwen2.5-math-7B%20achieves%20an%20accuracy%20improvement%20from%0A51.0%5C%25%20to%2081.6%5C%25%2C%20outperforming%20models%20trained%20on%20an%20equivalent%20amount%20of%0Along-CoT%20distilled%20data.%20Extensive%20experiments%20and%20analysis%20based%20on%20three%20base%0Amodels%20across%20both%20in-domain%20and%20out-of-domain%20benchmarks%20validate%20the%0Aeffectiveness%20of%20S%24%5E2%24R.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/NineAbyss/S2R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS%2524%255E2%2524R%253A%2520Teaching%2520LLMs%2520to%2520Self-verify%2520and%2520Self-correct%2520via%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DRuotian%2520Ma%2520and%2520Peisong%2520Wang%2520and%2520Cheng%2520Liu%2520and%2520Xingyan%2520Liu%2520and%2520Jiaqi%2520Chen%2520and%2520Bang%2520Zhang%2520and%2520Xin%2520Zhou%2520and%2520Nan%2520Du%2520and%2520Jia%2520Li%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520LLM%2520test-time%2520scaling.%250AHowever%252C%2520existing%2520approaches%2520to%2520incentivize%2520LLMs%2527%2520deep%2520thinking%2520abilities%250Agenerally%2520require%2520large-scale%2520data%2520or%2520significant%2520training%2520efforts.%2520Meanwhile%252C%250Ait%2520remains%2520unclear%2520how%2520to%2520improve%2520the%2520thinking%2520abilities%2520of%2520less%2520powerful%2520base%250Amodels.%2520In%2520this%2520work%252C%2520we%2520introduce%2520S%2524%255E2%2524R%252C%2520an%2520efficient%2520framework%2520that%2520enhances%250ALLM%2520reasoning%2520by%2520teaching%2520models%2520to%2520self-verify%2520and%2520self-correct%2520during%250Ainference.%2520Specifically%252C%2520we%2520first%2520initialize%2520LLMs%2520with%2520iterative%250Aself-verification%2520and%2520self-correction%2520behaviors%2520through%2520supervised%2520fine-tuning%250Aon%2520carefully%2520curated%2520data.%2520The%2520self-verification%2520and%2520self-correction%2520skills%2520are%250Athen%2520further%2520strengthened%2520by%2520both%2520outcome-level%2520and%2520process-level%2520reinforcement%250Alearning%252C%2520with%2520minimized%2520resource%2520requirements%252C%2520enabling%2520the%2520model%2520to%250Aadaptively%2520refine%2520its%2520reasoning%2520process%2520during%2520inference.%2520Our%2520results%250Ademonstrate%2520that%252C%2520with%2520only%25203.1k%2520self-verifying%2520and%2520self-correcting%2520behavior%250Ainitialization%2520samples%252C%2520Qwen2.5-math-7B%2520achieves%2520an%2520accuracy%2520improvement%2520from%250A51.0%255C%2525%2520to%252081.6%255C%2525%252C%2520outperforming%2520models%2520trained%2520on%2520an%2520equivalent%2520amount%2520of%250Along-CoT%2520distilled%2520data.%2520Extensive%2520experiments%2520and%2520analysis%2520based%2520on%2520three%2520base%250Amodels%2520across%2520both%2520in-domain%2520and%2520out-of-domain%2520benchmarks%2520validate%2520the%250Aeffectiveness%2520of%2520S%2524%255E2%2524R.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/NineAbyss/S2R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S%24%5E2%24R%3A%20Teaching%20LLMs%20to%20Self-verify%20and%20Self-correct%20via%20Reinforcement%0A%20%20Learning&entry.906535625=Ruotian%20Ma%20and%20Peisong%20Wang%20and%20Cheng%20Liu%20and%20Xingyan%20Liu%20and%20Jiaqi%20Chen%20and%20Bang%20Zhang%20and%20Xin%20Zhou%20and%20Nan%20Du%20and%20Jia%20Li&entry.1292438233=%20%20Recent%20studies%20have%20demonstrated%20the%20effectiveness%20of%20LLM%20test-time%20scaling.%0AHowever%2C%20existing%20approaches%20to%20incentivize%20LLMs%27%20deep%20thinking%20abilities%0Agenerally%20require%20large-scale%20data%20or%20significant%20training%20efforts.%20Meanwhile%2C%0Ait%20remains%20unclear%20how%20to%20improve%20the%20thinking%20abilities%20of%20less%20powerful%20base%0Amodels.%20In%20this%20work%2C%20we%20introduce%20S%24%5E2%24R%2C%20an%20efficient%20framework%20that%20enhances%0ALLM%20reasoning%20by%20teaching%20models%20to%20self-verify%20and%20self-correct%20during%0Ainference.%20Specifically%2C%20we%20first%20initialize%20LLMs%20with%20iterative%0Aself-verification%20and%20self-correction%20behaviors%20through%20supervised%20fine-tuning%0Aon%20carefully%20curated%20data.%20The%20self-verification%20and%20self-correction%20skills%20are%0Athen%20further%20strengthened%20by%20both%20outcome-level%20and%20process-level%20reinforcement%0Alearning%2C%20with%20minimized%20resource%20requirements%2C%20enabling%20the%20model%20to%0Aadaptively%20refine%20its%20reasoning%20process%20during%20inference.%20Our%20results%0Ademonstrate%20that%2C%20with%20only%203.1k%20self-verifying%20and%20self-correcting%20behavior%0Ainitialization%20samples%2C%20Qwen2.5-math-7B%20achieves%20an%20accuracy%20improvement%20from%0A51.0%5C%25%20to%2081.6%5C%25%2C%20outperforming%20models%20trained%20on%20an%20equivalent%20amount%20of%0Along-CoT%20distilled%20data.%20Extensive%20experiments%20and%20analysis%20based%20on%20three%20base%0Amodels%20across%20both%20in-domain%20and%20out-of-domain%20benchmarks%20validate%20the%0Aeffectiveness%20of%20S%24%5E2%24R.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/NineAbyss/S2R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12853v1&entry.124074799=Read"},
{"title": "Soft Arm-Motor Thrust Characterization for a Pneumatically Actuated Soft\n  Morphing Quadrotor", "author": "Vidya Sumathy and Jakub Haluska and George Nikolokopoulos", "abstract": "  In this work, an experimental characterization of the configuration space of\na soft, pneumatically actuated morphing quadrotor is presented, with a focus on\nprecise thrust characterization of its flexible arms, considering the effect of\ndownwash. Unlike traditional quadrotors, the soft drone has pneumatically\nactuated arms, introducing complex, nonlinear interactions between motor thrust\nand arm deformation, which make precise control challenging. The silicone arms\nare actuated using differential pressure to achieve flexibility and thus have a\nvariable workspace compared to their fixed counter-parts. The deflection of the\nsoft arms during compression and expansion is controlled throughout the flight.\nHowever, in real time, the downwash from the motor attached at the tip of the\nsoft arm generates a significant and random disturbance on the arm. This\ndisturbance affects both the desired deflection of the arm and the overall\nstability of the system. To address this factor, an experimental\ncharacterization of the effect of downwash on the deflection angle of the arm\nis conducted.\n", "link": "http://arxiv.org/abs/2502.12716v1", "date": "2025-02-18", "relevancy": 1.7453, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4682}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4336}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Arm-Motor%20Thrust%20Characterization%20for%20a%20Pneumatically%20Actuated%20Soft%0A%20%20Morphing%20Quadrotor&body=Title%3A%20Soft%20Arm-Motor%20Thrust%20Characterization%20for%20a%20Pneumatically%20Actuated%20Soft%0A%20%20Morphing%20Quadrotor%0AAuthor%3A%20Vidya%20Sumathy%20and%20Jakub%20Haluska%20and%20George%20Nikolokopoulos%0AAbstract%3A%20%20%20In%20this%20work%2C%20an%20experimental%20characterization%20of%20the%20configuration%20space%20of%0Aa%20soft%2C%20pneumatically%20actuated%20morphing%20quadrotor%20is%20presented%2C%20with%20a%20focus%20on%0Aprecise%20thrust%20characterization%20of%20its%20flexible%20arms%2C%20considering%20the%20effect%20of%0Adownwash.%20Unlike%20traditional%20quadrotors%2C%20the%20soft%20drone%20has%20pneumatically%0Aactuated%20arms%2C%20introducing%20complex%2C%20nonlinear%20interactions%20between%20motor%20thrust%0Aand%20arm%20deformation%2C%20which%20make%20precise%20control%20challenging.%20The%20silicone%20arms%0Aare%20actuated%20using%20differential%20pressure%20to%20achieve%20flexibility%20and%20thus%20have%20a%0Avariable%20workspace%20compared%20to%20their%20fixed%20counter-parts.%20The%20deflection%20of%20the%0Asoft%20arms%20during%20compression%20and%20expansion%20is%20controlled%20throughout%20the%20flight.%0AHowever%2C%20in%20real%20time%2C%20the%20downwash%20from%20the%20motor%20attached%20at%20the%20tip%20of%20the%0Asoft%20arm%20generates%20a%20significant%20and%20random%20disturbance%20on%20the%20arm.%20This%0Adisturbance%20affects%20both%20the%20desired%20deflection%20of%20the%20arm%20and%20the%20overall%0Astability%20of%20the%20system.%20To%20address%20this%20factor%2C%20an%20experimental%0Acharacterization%20of%20the%20effect%20of%20downwash%20on%20the%20deflection%20angle%20of%20the%20arm%0Ais%20conducted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Arm-Motor%2520Thrust%2520Characterization%2520for%2520a%2520Pneumatically%2520Actuated%2520Soft%250A%2520%2520Morphing%2520Quadrotor%26entry.906535625%3DVidya%2520Sumathy%2520and%2520Jakub%2520Haluska%2520and%2520George%2520Nikolokopoulos%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520an%2520experimental%2520characterization%2520of%2520the%2520configuration%2520space%2520of%250Aa%2520soft%252C%2520pneumatically%2520actuated%2520morphing%2520quadrotor%2520is%2520presented%252C%2520with%2520a%2520focus%2520on%250Aprecise%2520thrust%2520characterization%2520of%2520its%2520flexible%2520arms%252C%2520considering%2520the%2520effect%2520of%250Adownwash.%2520Unlike%2520traditional%2520quadrotors%252C%2520the%2520soft%2520drone%2520has%2520pneumatically%250Aactuated%2520arms%252C%2520introducing%2520complex%252C%2520nonlinear%2520interactions%2520between%2520motor%2520thrust%250Aand%2520arm%2520deformation%252C%2520which%2520make%2520precise%2520control%2520challenging.%2520The%2520silicone%2520arms%250Aare%2520actuated%2520using%2520differential%2520pressure%2520to%2520achieve%2520flexibility%2520and%2520thus%2520have%2520a%250Avariable%2520workspace%2520compared%2520to%2520their%2520fixed%2520counter-parts.%2520The%2520deflection%2520of%2520the%250Asoft%2520arms%2520during%2520compression%2520and%2520expansion%2520is%2520controlled%2520throughout%2520the%2520flight.%250AHowever%252C%2520in%2520real%2520time%252C%2520the%2520downwash%2520from%2520the%2520motor%2520attached%2520at%2520the%2520tip%2520of%2520the%250Asoft%2520arm%2520generates%2520a%2520significant%2520and%2520random%2520disturbance%2520on%2520the%2520arm.%2520This%250Adisturbance%2520affects%2520both%2520the%2520desired%2520deflection%2520of%2520the%2520arm%2520and%2520the%2520overall%250Astability%2520of%2520the%2520system.%2520To%2520address%2520this%2520factor%252C%2520an%2520experimental%250Acharacterization%2520of%2520the%2520effect%2520of%2520downwash%2520on%2520the%2520deflection%2520angle%2520of%2520the%2520arm%250Ais%2520conducted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Arm-Motor%20Thrust%20Characterization%20for%20a%20Pneumatically%20Actuated%20Soft%0A%20%20Morphing%20Quadrotor&entry.906535625=Vidya%20Sumathy%20and%20Jakub%20Haluska%20and%20George%20Nikolokopoulos&entry.1292438233=%20%20In%20this%20work%2C%20an%20experimental%20characterization%20of%20the%20configuration%20space%20of%0Aa%20soft%2C%20pneumatically%20actuated%20morphing%20quadrotor%20is%20presented%2C%20with%20a%20focus%20on%0Aprecise%20thrust%20characterization%20of%20its%20flexible%20arms%2C%20considering%20the%20effect%20of%0Adownwash.%20Unlike%20traditional%20quadrotors%2C%20the%20soft%20drone%20has%20pneumatically%0Aactuated%20arms%2C%20introducing%20complex%2C%20nonlinear%20interactions%20between%20motor%20thrust%0Aand%20arm%20deformation%2C%20which%20make%20precise%20control%20challenging.%20The%20silicone%20arms%0Aare%20actuated%20using%20differential%20pressure%20to%20achieve%20flexibility%20and%20thus%20have%20a%0Avariable%20workspace%20compared%20to%20their%20fixed%20counter-parts.%20The%20deflection%20of%20the%0Asoft%20arms%20during%20compression%20and%20expansion%20is%20controlled%20throughout%20the%20flight.%0AHowever%2C%20in%20real%20time%2C%20the%20downwash%20from%20the%20motor%20attached%20at%20the%20tip%20of%20the%0Asoft%20arm%20generates%20a%20significant%20and%20random%20disturbance%20on%20the%20arm.%20This%0Adisturbance%20affects%20both%20the%20desired%20deflection%20of%20the%20arm%20and%20the%20overall%0Astability%20of%20the%20system.%20To%20address%20this%20factor%2C%20an%20experimental%0Acharacterization%20of%20the%20effect%20of%20downwash%20on%20the%20deflection%20angle%20of%20the%20arm%0Ais%20conducted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12716v1&entry.124074799=Read"},
{"title": "Malware Detection based on API calls", "author": "Christofer Fellicious and Manuel Bischof and Kevin Mayer and Dorian Eikenberg and Stefan Hausotte and Hans P. Reiser and Michael Granitzer", "abstract": "  Malware attacks pose a significant threat in today's interconnected digital\nlandscape, causing billions of dollars in damages. Detecting and identifying\nfamilies as early as possible provides an edge in protecting against such\nmalware. We explore a lightweight, order-invariant approach to detecting and\nmitigating malware threats: analyzing API calls without regard to their\nsequence. We publish a public dataset of over three hundred thousand samples\nand their function call parameters for this task, annotated with labels\nindicating benign or malicious activity. The complete dataset is above 550GB\nuncompressed in size. We leverage machine learning algorithms, such as random\nforests, and conduct behavioral analysis by examining patterns and anomalies in\nAPI call sequences. By investigating how the function calls occur regardless of\ntheir order, we can identify discriminating features that can help us identify\nmalware early on. The models we've developed are not only effective but also\nefficient. They are lightweight and can run on any machine with minimal\nperformance overhead, while still achieving an impressive F1-Score of over\n85\\%. We also empirically show that we only need a subset of the function call\nsequence, specifically calls to the ntdll.dll library, to identify malware. Our\nresearch demonstrates the efficacy of this approach through empirical\nevaluations, underscoring its accuracy and scalability. The code is open source\nand available at Github along with the dataset on Zenodo.\n", "link": "http://arxiv.org/abs/2502.12863v1", "date": "2025-02-18", "relevancy": 1.2469, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4331}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4114}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Malware%20Detection%20based%20on%20API%20calls&body=Title%3A%20Malware%20Detection%20based%20on%20API%20calls%0AAuthor%3A%20Christofer%20Fellicious%20and%20Manuel%20Bischof%20and%20Kevin%20Mayer%20and%20Dorian%20Eikenberg%20and%20Stefan%20Hausotte%20and%20Hans%20P.%20Reiser%20and%20Michael%20Granitzer%0AAbstract%3A%20%20%20Malware%20attacks%20pose%20a%20significant%20threat%20in%20today%27s%20interconnected%20digital%0Alandscape%2C%20causing%20billions%20of%20dollars%20in%20damages.%20Detecting%20and%20identifying%0Afamilies%20as%20early%20as%20possible%20provides%20an%20edge%20in%20protecting%20against%20such%0Amalware.%20We%20explore%20a%20lightweight%2C%20order-invariant%20approach%20to%20detecting%20and%0Amitigating%20malware%20threats%3A%20analyzing%20API%20calls%20without%20regard%20to%20their%0Asequence.%20We%20publish%20a%20public%20dataset%20of%20over%20three%20hundred%20thousand%20samples%0Aand%20their%20function%20call%20parameters%20for%20this%20task%2C%20annotated%20with%20labels%0Aindicating%20benign%20or%20malicious%20activity.%20The%20complete%20dataset%20is%20above%20550GB%0Auncompressed%20in%20size.%20We%20leverage%20machine%20learning%20algorithms%2C%20such%20as%20random%0Aforests%2C%20and%20conduct%20behavioral%20analysis%20by%20examining%20patterns%20and%20anomalies%20in%0AAPI%20call%20sequences.%20By%20investigating%20how%20the%20function%20calls%20occur%20regardless%20of%0Atheir%20order%2C%20we%20can%20identify%20discriminating%20features%20that%20can%20help%20us%20identify%0Amalware%20early%20on.%20The%20models%20we%27ve%20developed%20are%20not%20only%20effective%20but%20also%0Aefficient.%20They%20are%20lightweight%20and%20can%20run%20on%20any%20machine%20with%20minimal%0Aperformance%20overhead%2C%20while%20still%20achieving%20an%20impressive%20F1-Score%20of%20over%0A85%5C%25.%20We%20also%20empirically%20show%20that%20we%20only%20need%20a%20subset%20of%20the%20function%20call%0Asequence%2C%20specifically%20calls%20to%20the%20ntdll.dll%20library%2C%20to%20identify%20malware.%20Our%0Aresearch%20demonstrates%20the%20efficacy%20of%20this%20approach%20through%20empirical%0Aevaluations%2C%20underscoring%20its%20accuracy%20and%20scalability.%20The%20code%20is%20open%20source%0Aand%20available%20at%20Github%20along%20with%20the%20dataset%20on%20Zenodo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMalware%2520Detection%2520based%2520on%2520API%2520calls%26entry.906535625%3DChristofer%2520Fellicious%2520and%2520Manuel%2520Bischof%2520and%2520Kevin%2520Mayer%2520and%2520Dorian%2520Eikenberg%2520and%2520Stefan%2520Hausotte%2520and%2520Hans%2520P.%2520Reiser%2520and%2520Michael%2520Granitzer%26entry.1292438233%3D%2520%2520Malware%2520attacks%2520pose%2520a%2520significant%2520threat%2520in%2520today%2527s%2520interconnected%2520digital%250Alandscape%252C%2520causing%2520billions%2520of%2520dollars%2520in%2520damages.%2520Detecting%2520and%2520identifying%250Afamilies%2520as%2520early%2520as%2520possible%2520provides%2520an%2520edge%2520in%2520protecting%2520against%2520such%250Amalware.%2520We%2520explore%2520a%2520lightweight%252C%2520order-invariant%2520approach%2520to%2520detecting%2520and%250Amitigating%2520malware%2520threats%253A%2520analyzing%2520API%2520calls%2520without%2520regard%2520to%2520their%250Asequence.%2520We%2520publish%2520a%2520public%2520dataset%2520of%2520over%2520three%2520hundred%2520thousand%2520samples%250Aand%2520their%2520function%2520call%2520parameters%2520for%2520this%2520task%252C%2520annotated%2520with%2520labels%250Aindicating%2520benign%2520or%2520malicious%2520activity.%2520The%2520complete%2520dataset%2520is%2520above%2520550GB%250Auncompressed%2520in%2520size.%2520We%2520leverage%2520machine%2520learning%2520algorithms%252C%2520such%2520as%2520random%250Aforests%252C%2520and%2520conduct%2520behavioral%2520analysis%2520by%2520examining%2520patterns%2520and%2520anomalies%2520in%250AAPI%2520call%2520sequences.%2520By%2520investigating%2520how%2520the%2520function%2520calls%2520occur%2520regardless%2520of%250Atheir%2520order%252C%2520we%2520can%2520identify%2520discriminating%2520features%2520that%2520can%2520help%2520us%2520identify%250Amalware%2520early%2520on.%2520The%2520models%2520we%2527ve%2520developed%2520are%2520not%2520only%2520effective%2520but%2520also%250Aefficient.%2520They%2520are%2520lightweight%2520and%2520can%2520run%2520on%2520any%2520machine%2520with%2520minimal%250Aperformance%2520overhead%252C%2520while%2520still%2520achieving%2520an%2520impressive%2520F1-Score%2520of%2520over%250A85%255C%2525.%2520We%2520also%2520empirically%2520show%2520that%2520we%2520only%2520need%2520a%2520subset%2520of%2520the%2520function%2520call%250Asequence%252C%2520specifically%2520calls%2520to%2520the%2520ntdll.dll%2520library%252C%2520to%2520identify%2520malware.%2520Our%250Aresearch%2520demonstrates%2520the%2520efficacy%2520of%2520this%2520approach%2520through%2520empirical%250Aevaluations%252C%2520underscoring%2520its%2520accuracy%2520and%2520scalability.%2520The%2520code%2520is%2520open%2520source%250Aand%2520available%2520at%2520Github%2520along%2520with%2520the%2520dataset%2520on%2520Zenodo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Malware%20Detection%20based%20on%20API%20calls&entry.906535625=Christofer%20Fellicious%20and%20Manuel%20Bischof%20and%20Kevin%20Mayer%20and%20Dorian%20Eikenberg%20and%20Stefan%20Hausotte%20and%20Hans%20P.%20Reiser%20and%20Michael%20Granitzer&entry.1292438233=%20%20Malware%20attacks%20pose%20a%20significant%20threat%20in%20today%27s%20interconnected%20digital%0Alandscape%2C%20causing%20billions%20of%20dollars%20in%20damages.%20Detecting%20and%20identifying%0Afamilies%20as%20early%20as%20possible%20provides%20an%20edge%20in%20protecting%20against%20such%0Amalware.%20We%20explore%20a%20lightweight%2C%20order-invariant%20approach%20to%20detecting%20and%0Amitigating%20malware%20threats%3A%20analyzing%20API%20calls%20without%20regard%20to%20their%0Asequence.%20We%20publish%20a%20public%20dataset%20of%20over%20three%20hundred%20thousand%20samples%0Aand%20their%20function%20call%20parameters%20for%20this%20task%2C%20annotated%20with%20labels%0Aindicating%20benign%20or%20malicious%20activity.%20The%20complete%20dataset%20is%20above%20550GB%0Auncompressed%20in%20size.%20We%20leverage%20machine%20learning%20algorithms%2C%20such%20as%20random%0Aforests%2C%20and%20conduct%20behavioral%20analysis%20by%20examining%20patterns%20and%20anomalies%20in%0AAPI%20call%20sequences.%20By%20investigating%20how%20the%20function%20calls%20occur%20regardless%20of%0Atheir%20order%2C%20we%20can%20identify%20discriminating%20features%20that%20can%20help%20us%20identify%0Amalware%20early%20on.%20The%20models%20we%27ve%20developed%20are%20not%20only%20effective%20but%20also%0Aefficient.%20They%20are%20lightweight%20and%20can%20run%20on%20any%20machine%20with%20minimal%0Aperformance%20overhead%2C%20while%20still%20achieving%20an%20impressive%20F1-Score%20of%20over%0A85%5C%25.%20We%20also%20empirically%20show%20that%20we%20only%20need%20a%20subset%20of%20the%20function%20call%0Asequence%2C%20specifically%20calls%20to%20the%20ntdll.dll%20library%2C%20to%20identify%20malware.%20Our%0Aresearch%20demonstrates%20the%20efficacy%20of%20this%20approach%20through%20empirical%0Aevaluations%2C%20underscoring%20its%20accuracy%20and%20scalability.%20The%20code%20is%20open%20source%0Aand%20available%20at%20Github%20along%20with%20the%20dataset%20on%20Zenodo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12863v1&entry.124074799=Read"},
{"title": "A Real-to-Sim-to-Real Approach to Robotic Manipulation with\n  VLM-Generated Iterative Keypoint Rewards", "author": "Shivansh Patel and Xinchen Yin and Wenlong Huang and Shubham Garg and Hooshang Nayyeri and Li Fei-Fei and Svetlana Lazebnik and Yunzhu Li", "abstract": "  Task specification for robotic manipulation in open-world environments is\nchallenging, requiring flexible and adaptive objectives that align with human\nintentions and can evolve through iterative feedback. We introduce Iterative\nKeypoint Reward (IKER), a visually grounded, Python-based reward function that\nserves as a dynamic task specification. Our framework leverages VLMs to\ngenerate and refine these reward functions for multi-step manipulation tasks.\nGiven RGB-D observations and free-form language instructions, we sample\nkeypoints in the scene and generate a reward function conditioned on these\nkeypoints. IKER operates on the spatial relationships between keypoints,\nleveraging commonsense priors about the desired behaviors, and enabling precise\nSE(3) control. We reconstruct real-world scenes in simulation and use the\ngenerated rewards to train reinforcement learning (RL) policies, which are then\ndeployed into the real world-forming a real-to-sim-to-real loop. Our approach\ndemonstrates notable capabilities across diverse scenarios, including both\nprehensile and non-prehensile tasks, showcasing multi-step task execution,\nspontaneous error recovery, and on-the-fly strategy adjustments. The results\nhighlight IKER's effectiveness in enabling robots to perform multi-step tasks\nin dynamic environments through iterative reward shaping.\n", "link": "http://arxiv.org/abs/2502.08643v2", "date": "2025-02-18", "relevancy": 1.7454, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.602}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Real-to-Sim-to-Real%20Approach%20to%20Robotic%20Manipulation%20with%0A%20%20VLM-Generated%20Iterative%20Keypoint%20Rewards&body=Title%3A%20A%20Real-to-Sim-to-Real%20Approach%20to%20Robotic%20Manipulation%20with%0A%20%20VLM-Generated%20Iterative%20Keypoint%20Rewards%0AAuthor%3A%20Shivansh%20Patel%20and%20Xinchen%20Yin%20and%20Wenlong%20Huang%20and%20Shubham%20Garg%20and%20Hooshang%20Nayyeri%20and%20Li%20Fei-Fei%20and%20Svetlana%20Lazebnik%20and%20Yunzhu%20Li%0AAbstract%3A%20%20%20Task%20specification%20for%20robotic%20manipulation%20in%20open-world%20environments%20is%0Achallenging%2C%20requiring%20flexible%20and%20adaptive%20objectives%20that%20align%20with%20human%0Aintentions%20and%20can%20evolve%20through%20iterative%20feedback.%20We%20introduce%20Iterative%0AKeypoint%20Reward%20%28IKER%29%2C%20a%20visually%20grounded%2C%20Python-based%20reward%20function%20that%0Aserves%20as%20a%20dynamic%20task%20specification.%20Our%20framework%20leverages%20VLMs%20to%0Agenerate%20and%20refine%20these%20reward%20functions%20for%20multi-step%20manipulation%20tasks.%0AGiven%20RGB-D%20observations%20and%20free-form%20language%20instructions%2C%20we%20sample%0Akeypoints%20in%20the%20scene%20and%20generate%20a%20reward%20function%20conditioned%20on%20these%0Akeypoints.%20IKER%20operates%20on%20the%20spatial%20relationships%20between%20keypoints%2C%0Aleveraging%20commonsense%20priors%20about%20the%20desired%20behaviors%2C%20and%20enabling%20precise%0ASE%283%29%20control.%20We%20reconstruct%20real-world%20scenes%20in%20simulation%20and%20use%20the%0Agenerated%20rewards%20to%20train%20reinforcement%20learning%20%28RL%29%20policies%2C%20which%20are%20then%0Adeployed%20into%20the%20real%20world-forming%20a%20real-to-sim-to-real%20loop.%20Our%20approach%0Ademonstrates%20notable%20capabilities%20across%20diverse%20scenarios%2C%20including%20both%0Aprehensile%20and%20non-prehensile%20tasks%2C%20showcasing%20multi-step%20task%20execution%2C%0Aspontaneous%20error%20recovery%2C%20and%20on-the-fly%20strategy%20adjustments.%20The%20results%0Ahighlight%20IKER%27s%20effectiveness%20in%20enabling%20robots%20to%20perform%20multi-step%20tasks%0Ain%20dynamic%20environments%20through%20iterative%20reward%20shaping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Real-to-Sim-to-Real%2520Approach%2520to%2520Robotic%2520Manipulation%2520with%250A%2520%2520VLM-Generated%2520Iterative%2520Keypoint%2520Rewards%26entry.906535625%3DShivansh%2520Patel%2520and%2520Xinchen%2520Yin%2520and%2520Wenlong%2520Huang%2520and%2520Shubham%2520Garg%2520and%2520Hooshang%2520Nayyeri%2520and%2520Li%2520Fei-Fei%2520and%2520Svetlana%2520Lazebnik%2520and%2520Yunzhu%2520Li%26entry.1292438233%3D%2520%2520Task%2520specification%2520for%2520robotic%2520manipulation%2520in%2520open-world%2520environments%2520is%250Achallenging%252C%2520requiring%2520flexible%2520and%2520adaptive%2520objectives%2520that%2520align%2520with%2520human%250Aintentions%2520and%2520can%2520evolve%2520through%2520iterative%2520feedback.%2520We%2520introduce%2520Iterative%250AKeypoint%2520Reward%2520%2528IKER%2529%252C%2520a%2520visually%2520grounded%252C%2520Python-based%2520reward%2520function%2520that%250Aserves%2520as%2520a%2520dynamic%2520task%2520specification.%2520Our%2520framework%2520leverages%2520VLMs%2520to%250Agenerate%2520and%2520refine%2520these%2520reward%2520functions%2520for%2520multi-step%2520manipulation%2520tasks.%250AGiven%2520RGB-D%2520observations%2520and%2520free-form%2520language%2520instructions%252C%2520we%2520sample%250Akeypoints%2520in%2520the%2520scene%2520and%2520generate%2520a%2520reward%2520function%2520conditioned%2520on%2520these%250Akeypoints.%2520IKER%2520operates%2520on%2520the%2520spatial%2520relationships%2520between%2520keypoints%252C%250Aleveraging%2520commonsense%2520priors%2520about%2520the%2520desired%2520behaviors%252C%2520and%2520enabling%2520precise%250ASE%25283%2529%2520control.%2520We%2520reconstruct%2520real-world%2520scenes%2520in%2520simulation%2520and%2520use%2520the%250Agenerated%2520rewards%2520to%2520train%2520reinforcement%2520learning%2520%2528RL%2529%2520policies%252C%2520which%2520are%2520then%250Adeployed%2520into%2520the%2520real%2520world-forming%2520a%2520real-to-sim-to-real%2520loop.%2520Our%2520approach%250Ademonstrates%2520notable%2520capabilities%2520across%2520diverse%2520scenarios%252C%2520including%2520both%250Aprehensile%2520and%2520non-prehensile%2520tasks%252C%2520showcasing%2520multi-step%2520task%2520execution%252C%250Aspontaneous%2520error%2520recovery%252C%2520and%2520on-the-fly%2520strategy%2520adjustments.%2520The%2520results%250Ahighlight%2520IKER%2527s%2520effectiveness%2520in%2520enabling%2520robots%2520to%2520perform%2520multi-step%2520tasks%250Ain%2520dynamic%2520environments%2520through%2520iterative%2520reward%2520shaping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Real-to-Sim-to-Real%20Approach%20to%20Robotic%20Manipulation%20with%0A%20%20VLM-Generated%20Iterative%20Keypoint%20Rewards&entry.906535625=Shivansh%20Patel%20and%20Xinchen%20Yin%20and%20Wenlong%20Huang%20and%20Shubham%20Garg%20and%20Hooshang%20Nayyeri%20and%20Li%20Fei-Fei%20and%20Svetlana%20Lazebnik%20and%20Yunzhu%20Li&entry.1292438233=%20%20Task%20specification%20for%20robotic%20manipulation%20in%20open-world%20environments%20is%0Achallenging%2C%20requiring%20flexible%20and%20adaptive%20objectives%20that%20align%20with%20human%0Aintentions%20and%20can%20evolve%20through%20iterative%20feedback.%20We%20introduce%20Iterative%0AKeypoint%20Reward%20%28IKER%29%2C%20a%20visually%20grounded%2C%20Python-based%20reward%20function%20that%0Aserves%20as%20a%20dynamic%20task%20specification.%20Our%20framework%20leverages%20VLMs%20to%0Agenerate%20and%20refine%20these%20reward%20functions%20for%20multi-step%20manipulation%20tasks.%0AGiven%20RGB-D%20observations%20and%20free-form%20language%20instructions%2C%20we%20sample%0Akeypoints%20in%20the%20scene%20and%20generate%20a%20reward%20function%20conditioned%20on%20these%0Akeypoints.%20IKER%20operates%20on%20the%20spatial%20relationships%20between%20keypoints%2C%0Aleveraging%20commonsense%20priors%20about%20the%20desired%20behaviors%2C%20and%20enabling%20precise%0ASE%283%29%20control.%20We%20reconstruct%20real-world%20scenes%20in%20simulation%20and%20use%20the%0Agenerated%20rewards%20to%20train%20reinforcement%20learning%20%28RL%29%20policies%2C%20which%20are%20then%0Adeployed%20into%20the%20real%20world-forming%20a%20real-to-sim-to-real%20loop.%20Our%20approach%0Ademonstrates%20notable%20capabilities%20across%20diverse%20scenarios%2C%20including%20both%0Aprehensile%20and%20non-prehensile%20tasks%2C%20showcasing%20multi-step%20task%20execution%2C%0Aspontaneous%20error%20recovery%2C%20and%20on-the-fly%20strategy%20adjustments.%20The%20results%0Ahighlight%20IKER%27s%20effectiveness%20in%20enabling%20robots%20to%20perform%20multi-step%20tasks%0Ain%20dynamic%20environments%20through%20iterative%20reward%20shaping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08643v2&entry.124074799=Read"},
{"title": "RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human\n  Demonstrations", "author": "Jingxiao Chen and Xinyao Li and Jiahang Cao and Zhengbang Zhu and Wentao Dong and Minghuan Liu and Ying Wen and Yong Yu and Liqing Zhang and Weinan Zhang", "abstract": "  Humanoid robots have shown success in locomotion and manipulation. Despite\nthese basic abilities, humanoids are still required to quickly understand human\ninstructions and react based on human interaction signals to become valuable\nassistants in human daily life. Unfortunately, most existing works only focus\non multi-stage interactions, treating each task separately, and neglecting\nreal-time feedback. In this work, we aim to empower humanoid robots with\nreal-time reaction abilities to achieve various tasks, allowing human to\ninterrupt robots at any time, and making robots respond to humans immediately.\nTo support such abilities, we propose a general humanoid-human-object\ninteraction framework, named RHINO, i.e., Real-time Humanoid-human Interaction\nand Object manipulation. RHINO provides a unified view of reactive motion,\ninstruction-based manipulation, and safety concerns, over multiple human signal\nmodalities, such as languages, images, and motions. RHINO is a hierarchical\nlearning framework, enabling humanoids to learn reaction skills from\nhuman-human-object demonstrations and teleoperation data. In particular, it\ndecouples the interaction process into two levels: 1) a high-level planner\ninferring human intentions from real-time human behaviors; and 2) a low-level\ncontroller achieving reactive motion behaviors and object manipulation skills\nbased on the predicted intentions. We evaluate the proposed framework on a real\nhumanoid robot and demonstrate its effectiveness, flexibility, and safety in\nvarious scenarios.\n", "link": "http://arxiv.org/abs/2502.13134v1", "date": "2025-02-18", "relevancy": 1.7271, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5999}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5733}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RHINO%3A%20Learning%20Real-Time%20Humanoid-Human-Object%20Interaction%20from%20Human%0A%20%20Demonstrations&body=Title%3A%20RHINO%3A%20Learning%20Real-Time%20Humanoid-Human-Object%20Interaction%20from%20Human%0A%20%20Demonstrations%0AAuthor%3A%20Jingxiao%20Chen%20and%20Xinyao%20Li%20and%20Jiahang%20Cao%20and%20Zhengbang%20Zhu%20and%20Wentao%20Dong%20and%20Minghuan%20Liu%20and%20Ying%20Wen%20and%20Yong%20Yu%20and%20Liqing%20Zhang%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20Humanoid%20robots%20have%20shown%20success%20in%20locomotion%20and%20manipulation.%20Despite%0Athese%20basic%20abilities%2C%20humanoids%20are%20still%20required%20to%20quickly%20understand%20human%0Ainstructions%20and%20react%20based%20on%20human%20interaction%20signals%20to%20become%20valuable%0Aassistants%20in%20human%20daily%20life.%20Unfortunately%2C%20most%20existing%20works%20only%20focus%0Aon%20multi-stage%20interactions%2C%20treating%20each%20task%20separately%2C%20and%20neglecting%0Areal-time%20feedback.%20In%20this%20work%2C%20we%20aim%20to%20empower%20humanoid%20robots%20with%0Areal-time%20reaction%20abilities%20to%20achieve%20various%20tasks%2C%20allowing%20human%20to%0Ainterrupt%20robots%20at%20any%20time%2C%20and%20making%20robots%20respond%20to%20humans%20immediately.%0ATo%20support%20such%20abilities%2C%20we%20propose%20a%20general%20humanoid-human-object%0Ainteraction%20framework%2C%20named%20RHINO%2C%20i.e.%2C%20Real-time%20Humanoid-human%20Interaction%0Aand%20Object%20manipulation.%20RHINO%20provides%20a%20unified%20view%20of%20reactive%20motion%2C%0Ainstruction-based%20manipulation%2C%20and%20safety%20concerns%2C%20over%20multiple%20human%20signal%0Amodalities%2C%20such%20as%20languages%2C%20images%2C%20and%20motions.%20RHINO%20is%20a%20hierarchical%0Alearning%20framework%2C%20enabling%20humanoids%20to%20learn%20reaction%20skills%20from%0Ahuman-human-object%20demonstrations%20and%20teleoperation%20data.%20In%20particular%2C%20it%0Adecouples%20the%20interaction%20process%20into%20two%20levels%3A%201%29%20a%20high-level%20planner%0Ainferring%20human%20intentions%20from%20real-time%20human%20behaviors%3B%20and%202%29%20a%20low-level%0Acontroller%20achieving%20reactive%20motion%20behaviors%20and%20object%20manipulation%20skills%0Abased%20on%20the%20predicted%20intentions.%20We%20evaluate%20the%20proposed%20framework%20on%20a%20real%0Ahumanoid%20robot%20and%20demonstrate%20its%20effectiveness%2C%20flexibility%2C%20and%20safety%20in%0Avarious%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRHINO%253A%2520Learning%2520Real-Time%2520Humanoid-Human-Object%2520Interaction%2520from%2520Human%250A%2520%2520Demonstrations%26entry.906535625%3DJingxiao%2520Chen%2520and%2520Xinyao%2520Li%2520and%2520Jiahang%2520Cao%2520and%2520Zhengbang%2520Zhu%2520and%2520Wentao%2520Dong%2520and%2520Minghuan%2520Liu%2520and%2520Ying%2520Wen%2520and%2520Yong%2520Yu%2520and%2520Liqing%2520Zhang%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520Humanoid%2520robots%2520have%2520shown%2520success%2520in%2520locomotion%2520and%2520manipulation.%2520Despite%250Athese%2520basic%2520abilities%252C%2520humanoids%2520are%2520still%2520required%2520to%2520quickly%2520understand%2520human%250Ainstructions%2520and%2520react%2520based%2520on%2520human%2520interaction%2520signals%2520to%2520become%2520valuable%250Aassistants%2520in%2520human%2520daily%2520life.%2520Unfortunately%252C%2520most%2520existing%2520works%2520only%2520focus%250Aon%2520multi-stage%2520interactions%252C%2520treating%2520each%2520task%2520separately%252C%2520and%2520neglecting%250Areal-time%2520feedback.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520empower%2520humanoid%2520robots%2520with%250Areal-time%2520reaction%2520abilities%2520to%2520achieve%2520various%2520tasks%252C%2520allowing%2520human%2520to%250Ainterrupt%2520robots%2520at%2520any%2520time%252C%2520and%2520making%2520robots%2520respond%2520to%2520humans%2520immediately.%250ATo%2520support%2520such%2520abilities%252C%2520we%2520propose%2520a%2520general%2520humanoid-human-object%250Ainteraction%2520framework%252C%2520named%2520RHINO%252C%2520i.e.%252C%2520Real-time%2520Humanoid-human%2520Interaction%250Aand%2520Object%2520manipulation.%2520RHINO%2520provides%2520a%2520unified%2520view%2520of%2520reactive%2520motion%252C%250Ainstruction-based%2520manipulation%252C%2520and%2520safety%2520concerns%252C%2520over%2520multiple%2520human%2520signal%250Amodalities%252C%2520such%2520as%2520languages%252C%2520images%252C%2520and%2520motions.%2520RHINO%2520is%2520a%2520hierarchical%250Alearning%2520framework%252C%2520enabling%2520humanoids%2520to%2520learn%2520reaction%2520skills%2520from%250Ahuman-human-object%2520demonstrations%2520and%2520teleoperation%2520data.%2520In%2520particular%252C%2520it%250Adecouples%2520the%2520interaction%2520process%2520into%2520two%2520levels%253A%25201%2529%2520a%2520high-level%2520planner%250Ainferring%2520human%2520intentions%2520from%2520real-time%2520human%2520behaviors%253B%2520and%25202%2529%2520a%2520low-level%250Acontroller%2520achieving%2520reactive%2520motion%2520behaviors%2520and%2520object%2520manipulation%2520skills%250Abased%2520on%2520the%2520predicted%2520intentions.%2520We%2520evaluate%2520the%2520proposed%2520framework%2520on%2520a%2520real%250Ahumanoid%2520robot%2520and%2520demonstrate%2520its%2520effectiveness%252C%2520flexibility%252C%2520and%2520safety%2520in%250Avarious%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RHINO%3A%20Learning%20Real-Time%20Humanoid-Human-Object%20Interaction%20from%20Human%0A%20%20Demonstrations&entry.906535625=Jingxiao%20Chen%20and%20Xinyao%20Li%20and%20Jiahang%20Cao%20and%20Zhengbang%20Zhu%20and%20Wentao%20Dong%20and%20Minghuan%20Liu%20and%20Ying%20Wen%20and%20Yong%20Yu%20and%20Liqing%20Zhang%20and%20Weinan%20Zhang&entry.1292438233=%20%20Humanoid%20robots%20have%20shown%20success%20in%20locomotion%20and%20manipulation.%20Despite%0Athese%20basic%20abilities%2C%20humanoids%20are%20still%20required%20to%20quickly%20understand%20human%0Ainstructions%20and%20react%20based%20on%20human%20interaction%20signals%20to%20become%20valuable%0Aassistants%20in%20human%20daily%20life.%20Unfortunately%2C%20most%20existing%20works%20only%20focus%0Aon%20multi-stage%20interactions%2C%20treating%20each%20task%20separately%2C%20and%20neglecting%0Areal-time%20feedback.%20In%20this%20work%2C%20we%20aim%20to%20empower%20humanoid%20robots%20with%0Areal-time%20reaction%20abilities%20to%20achieve%20various%20tasks%2C%20allowing%20human%20to%0Ainterrupt%20robots%20at%20any%20time%2C%20and%20making%20robots%20respond%20to%20humans%20immediately.%0ATo%20support%20such%20abilities%2C%20we%20propose%20a%20general%20humanoid-human-object%0Ainteraction%20framework%2C%20named%20RHINO%2C%20i.e.%2C%20Real-time%20Humanoid-human%20Interaction%0Aand%20Object%20manipulation.%20RHINO%20provides%20a%20unified%20view%20of%20reactive%20motion%2C%0Ainstruction-based%20manipulation%2C%20and%20safety%20concerns%2C%20over%20multiple%20human%20signal%0Amodalities%2C%20such%20as%20languages%2C%20images%2C%20and%20motions.%20RHINO%20is%20a%20hierarchical%0Alearning%20framework%2C%20enabling%20humanoids%20to%20learn%20reaction%20skills%20from%0Ahuman-human-object%20demonstrations%20and%20teleoperation%20data.%20In%20particular%2C%20it%0Adecouples%20the%20interaction%20process%20into%20two%20levels%3A%201%29%20a%20high-level%20planner%0Ainferring%20human%20intentions%20from%20real-time%20human%20behaviors%3B%20and%202%29%20a%20low-level%0Acontroller%20achieving%20reactive%20motion%20behaviors%20and%20object%20manipulation%20skills%0Abased%20on%20the%20predicted%20intentions.%20We%20evaluate%20the%20proposed%20framework%20on%20a%20real%0Ahumanoid%20robot%20and%20demonstrate%20its%20effectiveness%2C%20flexibility%2C%20and%20safety%20in%0Avarious%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13134v1&entry.124074799=Read"},
{"title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation", "author": "Bencheng Liao and Hongyuan Tao and Qian Zhang and Tianheng Cheng and Yingyue Li and Haoran Yin and Wenyu Liu and Xinggang Wang", "abstract": "  Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba\n", "link": "http://arxiv.org/abs/2502.13145v1", "date": "2025-02-18", "relevancy": 1.6116, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Mamba%3A%20Decoder-only%20Multimodal%20State%20Space%20Model%20via%0A%20%20Quadratic%20to%20Linear%20Distillation&body=Title%3A%20Multimodal%20Mamba%3A%20Decoder-only%20Multimodal%20State%20Space%20Model%20via%0A%20%20Quadratic%20to%20Linear%20Distillation%0AAuthor%3A%20Bencheng%20Liao%20and%20Hongyuan%20Tao%20and%20Qian%20Zhang%20and%20Tianheng%20Cheng%20and%20Yingyue%20Li%20and%20Haoran%20Yin%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%0Aperformance%20but%20face%20deployment%20challenges%20due%20to%20their%20quadratic%20computational%0Acomplexity%2C%20growing%20Key-Value%20cache%20requirements%2C%20and%20reliance%20on%20separate%0Avision%20encoders.%20We%20propose%20mmMamba%2C%20a%20framework%20for%20developing%0Alinear-complexity%20native%20multimodal%20state%20space%20models%20through%20progressive%0Adistillation%20from%20existing%20MLLMs%20using%20moderate%20academic%20computational%0Aresources.%20Our%20approach%20enables%20the%20direct%20conversion%20of%20trained%20decoder-only%0AMLLMs%20to%20linear-complexity%20architectures%20without%20requiring%20pre-trained%0ARNN-based%20LLM%20or%20vision%20encoders.%20We%20propose%20an%20seeding%20strategy%20to%20carve%20Mamba%0Afrom%20trained%20Transformer%20and%20a%20three-stage%20distillation%20recipe%2C%20which%20can%0Aeffectively%20transfer%20the%20knowledge%20from%20Transformer%20to%20Mamba%20while%20preserving%0Amultimodal%20capabilities.%20Our%20method%20also%20supports%20flexible%20hybrid%20architectures%0Athat%20combine%20Transformer%20and%20Mamba%20layers%20for%20customizable%0Aefficiency-performance%20trade-offs.%20Distilled%20from%20the%20Transformer-based%0Adecoder-only%20HoVLE%2C%20mmMamba-linear%20achieves%20competitive%20performance%20against%0Aexisting%20linear%20and%20quadratic-complexity%20VLMs%2C%20while%20mmMamba-hybrid%20further%0Aimproves%20performance%20significantly%2C%20approaching%20HoVLE%27s%20capabilities.%20At%20103K%0Atokens%2C%20mmMamba-linear%20demonstrates%2020.6%24%5Ctimes%24%20speedup%20and%2075.8%25%20GPU%20memory%0Areduction%20compared%20to%20HoVLE%2C%20while%20mmMamba-hybrid%20achieves%2013.5%24%5Ctimes%24%20speedup%0Aand%2060.2%25%20memory%20savings.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/hustvl/mmMamba%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Mamba%253A%2520Decoder-only%2520Multimodal%2520State%2520Space%2520Model%2520via%250A%2520%2520Quadratic%2520to%2520Linear%2520Distillation%26entry.906535625%3DBencheng%2520Liao%2520and%2520Hongyuan%2520Tao%2520and%2520Qian%2520Zhang%2520and%2520Tianheng%2520Cheng%2520and%2520Yingyue%2520Li%2520and%2520Haoran%2520Yin%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%250Aperformance%2520but%2520face%2520deployment%2520challenges%2520due%2520to%2520their%2520quadratic%2520computational%250Acomplexity%252C%2520growing%2520Key-Value%2520cache%2520requirements%252C%2520and%2520reliance%2520on%2520separate%250Avision%2520encoders.%2520We%2520propose%2520mmMamba%252C%2520a%2520framework%2520for%2520developing%250Alinear-complexity%2520native%2520multimodal%2520state%2520space%2520models%2520through%2520progressive%250Adistillation%2520from%2520existing%2520MLLMs%2520using%2520moderate%2520academic%2520computational%250Aresources.%2520Our%2520approach%2520enables%2520the%2520direct%2520conversion%2520of%2520trained%2520decoder-only%250AMLLMs%2520to%2520linear-complexity%2520architectures%2520without%2520requiring%2520pre-trained%250ARNN-based%2520LLM%2520or%2520vision%2520encoders.%2520We%2520propose%2520an%2520seeding%2520strategy%2520to%2520carve%2520Mamba%250Afrom%2520trained%2520Transformer%2520and%2520a%2520three-stage%2520distillation%2520recipe%252C%2520which%2520can%250Aeffectively%2520transfer%2520the%2520knowledge%2520from%2520Transformer%2520to%2520Mamba%2520while%2520preserving%250Amultimodal%2520capabilities.%2520Our%2520method%2520also%2520supports%2520flexible%2520hybrid%2520architectures%250Athat%2520combine%2520Transformer%2520and%2520Mamba%2520layers%2520for%2520customizable%250Aefficiency-performance%2520trade-offs.%2520Distilled%2520from%2520the%2520Transformer-based%250Adecoder-only%2520HoVLE%252C%2520mmMamba-linear%2520achieves%2520competitive%2520performance%2520against%250Aexisting%2520linear%2520and%2520quadratic-complexity%2520VLMs%252C%2520while%2520mmMamba-hybrid%2520further%250Aimproves%2520performance%2520significantly%252C%2520approaching%2520HoVLE%2527s%2520capabilities.%2520At%2520103K%250Atokens%252C%2520mmMamba-linear%2520demonstrates%252020.6%2524%255Ctimes%2524%2520speedup%2520and%252075.8%2525%2520GPU%2520memory%250Areduction%2520compared%2520to%2520HoVLE%252C%2520while%2520mmMamba-hybrid%2520achieves%252013.5%2524%255Ctimes%2524%2520speedup%250Aand%252060.2%2525%2520memory%2520savings.%2520Code%2520and%2520models%2520are%2520released%2520at%250Ahttps%253A//github.com/hustvl/mmMamba%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Mamba%3A%20Decoder-only%20Multimodal%20State%20Space%20Model%20via%0A%20%20Quadratic%20to%20Linear%20Distillation&entry.906535625=Bencheng%20Liao%20and%20Hongyuan%20Tao%20and%20Qian%20Zhang%20and%20Tianheng%20Cheng%20and%20Yingyue%20Li%20and%20Haoran%20Yin%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%0Aperformance%20but%20face%20deployment%20challenges%20due%20to%20their%20quadratic%20computational%0Acomplexity%2C%20growing%20Key-Value%20cache%20requirements%2C%20and%20reliance%20on%20separate%0Avision%20encoders.%20We%20propose%20mmMamba%2C%20a%20framework%20for%20developing%0Alinear-complexity%20native%20multimodal%20state%20space%20models%20through%20progressive%0Adistillation%20from%20existing%20MLLMs%20using%20moderate%20academic%20computational%0Aresources.%20Our%20approach%20enables%20the%20direct%20conversion%20of%20trained%20decoder-only%0AMLLMs%20to%20linear-complexity%20architectures%20without%20requiring%20pre-trained%0ARNN-based%20LLM%20or%20vision%20encoders.%20We%20propose%20an%20seeding%20strategy%20to%20carve%20Mamba%0Afrom%20trained%20Transformer%20and%20a%20three-stage%20distillation%20recipe%2C%20which%20can%0Aeffectively%20transfer%20the%20knowledge%20from%20Transformer%20to%20Mamba%20while%20preserving%0Amultimodal%20capabilities.%20Our%20method%20also%20supports%20flexible%20hybrid%20architectures%0Athat%20combine%20Transformer%20and%20Mamba%20layers%20for%20customizable%0Aefficiency-performance%20trade-offs.%20Distilled%20from%20the%20Transformer-based%0Adecoder-only%20HoVLE%2C%20mmMamba-linear%20achieves%20competitive%20performance%20against%0Aexisting%20linear%20and%20quadratic-complexity%20VLMs%2C%20while%20mmMamba-hybrid%20further%0Aimproves%20performance%20significantly%2C%20approaching%20HoVLE%27s%20capabilities.%20At%20103K%0Atokens%2C%20mmMamba-linear%20demonstrates%2020.6%24%5Ctimes%24%20speedup%20and%2075.8%25%20GPU%20memory%0Areduction%20compared%20to%20HoVLE%2C%20while%20mmMamba-hybrid%20achieves%2013.5%24%5Ctimes%24%20speedup%0Aand%2060.2%25%20memory%20savings.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/hustvl/mmMamba%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13145v1&entry.124074799=Read"},
{"title": "TabM: Advancing Tabular Deep Learning with Parameter-Efficient\n  Ensembling", "author": "Yury Gorishniy and Akim Kotelnikov and Artem Babenko", "abstract": "  Deep learning architectures for supervised learning on tabular data range\nfrom simple multilayer perceptrons (MLP) to sophisticated Transformers and\nretrieval-augmented methods. This study highlights a major, yet so far\noverlooked opportunity for designing substantially better MLP-based tabular\narchitectures. Namely, our new model TabM relies on efficient ensembling, where\none TabM efficiently imitates an ensemble of MLPs and produces multiple\npredictions per object. Compared to a traditional deep ensemble, in TabM, the\nunderlying implicit MLPs are trained simultaneously, and (by default) share\nmost of their parameters, which results in significantly better performance and\nefficiency. Using TabM as a new baseline, we perform a large-scale evaluation\nof tabular DL architectures on public benchmarks in terms of both task\nperformance and efficiency, which renders the landscape of tabular DL in a new\nlight. Generally, we show that MLPs, including TabM, form a line of stronger\nand more practical models compared to attention- and retrieval-based\narchitectures. In particular, we find that TabM demonstrates the best\nperformance among tabular DL models. Then, we conduct an empirical analysis on\nthe ensemble-like nature of TabM. We observe that the multiple predictions of\nTabM are weak individually, but powerful collectively. Overall, our work brings\nan impactful technique to tabular DL and advances the performance-efficiency\ntrade-off with TabM -- a simple and powerful baseline for researchers and\npractitioners.\n", "link": "http://arxiv.org/abs/2410.24210v3", "date": "2025-02-18", "relevancy": 1.961, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.513}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabM%3A%20Advancing%20Tabular%20Deep%20Learning%20with%20Parameter-Efficient%0A%20%20Ensembling&body=Title%3A%20TabM%3A%20Advancing%20Tabular%20Deep%20Learning%20with%20Parameter-Efficient%0A%20%20Ensembling%0AAuthor%3A%20Yury%20Gorishniy%20and%20Akim%20Kotelnikov%20and%20Artem%20Babenko%0AAbstract%3A%20%20%20Deep%20learning%20architectures%20for%20supervised%20learning%20on%20tabular%20data%20range%0Afrom%20simple%20multilayer%20perceptrons%20%28MLP%29%20to%20sophisticated%20Transformers%20and%0Aretrieval-augmented%20methods.%20This%20study%20highlights%20a%20major%2C%20yet%20so%20far%0Aoverlooked%20opportunity%20for%20designing%20substantially%20better%20MLP-based%20tabular%0Aarchitectures.%20Namely%2C%20our%20new%20model%20TabM%20relies%20on%20efficient%20ensembling%2C%20where%0Aone%20TabM%20efficiently%20imitates%20an%20ensemble%20of%20MLPs%20and%20produces%20multiple%0Apredictions%20per%20object.%20Compared%20to%20a%20traditional%20deep%20ensemble%2C%20in%20TabM%2C%20the%0Aunderlying%20implicit%20MLPs%20are%20trained%20simultaneously%2C%20and%20%28by%20default%29%20share%0Amost%20of%20their%20parameters%2C%20which%20results%20in%20significantly%20better%20performance%20and%0Aefficiency.%20Using%20TabM%20as%20a%20new%20baseline%2C%20we%20perform%20a%20large-scale%20evaluation%0Aof%20tabular%20DL%20architectures%20on%20public%20benchmarks%20in%20terms%20of%20both%20task%0Aperformance%20and%20efficiency%2C%20which%20renders%20the%20landscape%20of%20tabular%20DL%20in%20a%20new%0Alight.%20Generally%2C%20we%20show%20that%20MLPs%2C%20including%20TabM%2C%20form%20a%20line%20of%20stronger%0Aand%20more%20practical%20models%20compared%20to%20attention-%20and%20retrieval-based%0Aarchitectures.%20In%20particular%2C%20we%20find%20that%20TabM%20demonstrates%20the%20best%0Aperformance%20among%20tabular%20DL%20models.%20Then%2C%20we%20conduct%20an%20empirical%20analysis%20on%0Athe%20ensemble-like%20nature%20of%20TabM.%20We%20observe%20that%20the%20multiple%20predictions%20of%0ATabM%20are%20weak%20individually%2C%20but%20powerful%20collectively.%20Overall%2C%20our%20work%20brings%0Aan%20impactful%20technique%20to%20tabular%20DL%20and%20advances%20the%20performance-efficiency%0Atrade-off%20with%20TabM%20--%20a%20simple%20and%20powerful%20baseline%20for%20researchers%20and%0Apractitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24210v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabM%253A%2520Advancing%2520Tabular%2520Deep%2520Learning%2520with%2520Parameter-Efficient%250A%2520%2520Ensembling%26entry.906535625%3DYury%2520Gorishniy%2520and%2520Akim%2520Kotelnikov%2520and%2520Artem%2520Babenko%26entry.1292438233%3D%2520%2520Deep%2520learning%2520architectures%2520for%2520supervised%2520learning%2520on%2520tabular%2520data%2520range%250Afrom%2520simple%2520multilayer%2520perceptrons%2520%2528MLP%2529%2520to%2520sophisticated%2520Transformers%2520and%250Aretrieval-augmented%2520methods.%2520This%2520study%2520highlights%2520a%2520major%252C%2520yet%2520so%2520far%250Aoverlooked%2520opportunity%2520for%2520designing%2520substantially%2520better%2520MLP-based%2520tabular%250Aarchitectures.%2520Namely%252C%2520our%2520new%2520model%2520TabM%2520relies%2520on%2520efficient%2520ensembling%252C%2520where%250Aone%2520TabM%2520efficiently%2520imitates%2520an%2520ensemble%2520of%2520MLPs%2520and%2520produces%2520multiple%250Apredictions%2520per%2520object.%2520Compared%2520to%2520a%2520traditional%2520deep%2520ensemble%252C%2520in%2520TabM%252C%2520the%250Aunderlying%2520implicit%2520MLPs%2520are%2520trained%2520simultaneously%252C%2520and%2520%2528by%2520default%2529%2520share%250Amost%2520of%2520their%2520parameters%252C%2520which%2520results%2520in%2520significantly%2520better%2520performance%2520and%250Aefficiency.%2520Using%2520TabM%2520as%2520a%2520new%2520baseline%252C%2520we%2520perform%2520a%2520large-scale%2520evaluation%250Aof%2520tabular%2520DL%2520architectures%2520on%2520public%2520benchmarks%2520in%2520terms%2520of%2520both%2520task%250Aperformance%2520and%2520efficiency%252C%2520which%2520renders%2520the%2520landscape%2520of%2520tabular%2520DL%2520in%2520a%2520new%250Alight.%2520Generally%252C%2520we%2520show%2520that%2520MLPs%252C%2520including%2520TabM%252C%2520form%2520a%2520line%2520of%2520stronger%250Aand%2520more%2520practical%2520models%2520compared%2520to%2520attention-%2520and%2520retrieval-based%250Aarchitectures.%2520In%2520particular%252C%2520we%2520find%2520that%2520TabM%2520demonstrates%2520the%2520best%250Aperformance%2520among%2520tabular%2520DL%2520models.%2520Then%252C%2520we%2520conduct%2520an%2520empirical%2520analysis%2520on%250Athe%2520ensemble-like%2520nature%2520of%2520TabM.%2520We%2520observe%2520that%2520the%2520multiple%2520predictions%2520of%250ATabM%2520are%2520weak%2520individually%252C%2520but%2520powerful%2520collectively.%2520Overall%252C%2520our%2520work%2520brings%250Aan%2520impactful%2520technique%2520to%2520tabular%2520DL%2520and%2520advances%2520the%2520performance-efficiency%250Atrade-off%2520with%2520TabM%2520--%2520a%2520simple%2520and%2520powerful%2520baseline%2520for%2520researchers%2520and%250Apractitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24210v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabM%3A%20Advancing%20Tabular%20Deep%20Learning%20with%20Parameter-Efficient%0A%20%20Ensembling&entry.906535625=Yury%20Gorishniy%20and%20Akim%20Kotelnikov%20and%20Artem%20Babenko&entry.1292438233=%20%20Deep%20learning%20architectures%20for%20supervised%20learning%20on%20tabular%20data%20range%0Afrom%20simple%20multilayer%20perceptrons%20%28MLP%29%20to%20sophisticated%20Transformers%20and%0Aretrieval-augmented%20methods.%20This%20study%20highlights%20a%20major%2C%20yet%20so%20far%0Aoverlooked%20opportunity%20for%20designing%20substantially%20better%20MLP-based%20tabular%0Aarchitectures.%20Namely%2C%20our%20new%20model%20TabM%20relies%20on%20efficient%20ensembling%2C%20where%0Aone%20TabM%20efficiently%20imitates%20an%20ensemble%20of%20MLPs%20and%20produces%20multiple%0Apredictions%20per%20object.%20Compared%20to%20a%20traditional%20deep%20ensemble%2C%20in%20TabM%2C%20the%0Aunderlying%20implicit%20MLPs%20are%20trained%20simultaneously%2C%20and%20%28by%20default%29%20share%0Amost%20of%20their%20parameters%2C%20which%20results%20in%20significantly%20better%20performance%20and%0Aefficiency.%20Using%20TabM%20as%20a%20new%20baseline%2C%20we%20perform%20a%20large-scale%20evaluation%0Aof%20tabular%20DL%20architectures%20on%20public%20benchmarks%20in%20terms%20of%20both%20task%0Aperformance%20and%20efficiency%2C%20which%20renders%20the%20landscape%20of%20tabular%20DL%20in%20a%20new%0Alight.%20Generally%2C%20we%20show%20that%20MLPs%2C%20including%20TabM%2C%20form%20a%20line%20of%20stronger%0Aand%20more%20practical%20models%20compared%20to%20attention-%20and%20retrieval-based%0Aarchitectures.%20In%20particular%2C%20we%20find%20that%20TabM%20demonstrates%20the%20best%0Aperformance%20among%20tabular%20DL%20models.%20Then%2C%20we%20conduct%20an%20empirical%20analysis%20on%0Athe%20ensemble-like%20nature%20of%20TabM.%20We%20observe%20that%20the%20multiple%20predictions%20of%0ATabM%20are%20weak%20individually%2C%20but%20powerful%20collectively.%20Overall%2C%20our%20work%20brings%0Aan%20impactful%20technique%20to%20tabular%20DL%20and%20advances%20the%20performance-efficiency%0Atrade-off%20with%20TabM%20--%20a%20simple%20and%20powerful%20baseline%20for%20researchers%20and%0Apractitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24210v3&entry.124074799=Read"},
{"title": "MediaMind: Revolutionizing Media Monitoring using Agentification", "author": "Ahmet Gunduz and Kamer Ali Yuksel and Hassan Sawaf", "abstract": "  In an era of rapid technological advancements, agentification of software\ntools has emerged as a critical innovation, enabling systems to function\nautonomously and adaptively. This paper introduces MediaMind as a case study to\ndemonstrate the agentification process, highlighting how existing software can\nbe transformed into intelligent agents capable of independent decision-making\nand dynamic interaction. Developed by aiXplain, MediaMind leverages agent-based\narchitecture to autonomously monitor, analyze, and provide insights from\nmultilingual media content in real time. The focus of this paper is on the\ntechnical methodologies and design principles behind agentifying MediaMind,\nshowcasing how agentification enhances adaptability, efficiency, and\nresponsiveness. Through detailed case studies and practical examples, we\nillustrate how the agentification of MediaMind empowers organizations to\nstreamline workflows, optimize decision-making, and respond to evolving trends.\nThis work underscores the broader potential of agentification to revolutionize\nsoftware tools across various domains.\n", "link": "http://arxiv.org/abs/2502.12745v1", "date": "2025-02-18", "relevancy": 1.8794, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4738}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4729}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MediaMind%3A%20Revolutionizing%20Media%20Monitoring%20using%20Agentification&body=Title%3A%20MediaMind%3A%20Revolutionizing%20Media%20Monitoring%20using%20Agentification%0AAuthor%3A%20Ahmet%20Gunduz%20and%20Kamer%20Ali%20Yuksel%20and%20Hassan%20Sawaf%0AAbstract%3A%20%20%20In%20an%20era%20of%20rapid%20technological%20advancements%2C%20agentification%20of%20software%0Atools%20has%20emerged%20as%20a%20critical%20innovation%2C%20enabling%20systems%20to%20function%0Aautonomously%20and%20adaptively.%20This%20paper%20introduces%20MediaMind%20as%20a%20case%20study%20to%0Ademonstrate%20the%20agentification%20process%2C%20highlighting%20how%20existing%20software%20can%0Abe%20transformed%20into%20intelligent%20agents%20capable%20of%20independent%20decision-making%0Aand%20dynamic%20interaction.%20Developed%20by%20aiXplain%2C%20MediaMind%20leverages%20agent-based%0Aarchitecture%20to%20autonomously%20monitor%2C%20analyze%2C%20and%20provide%20insights%20from%0Amultilingual%20media%20content%20in%20real%20time.%20The%20focus%20of%20this%20paper%20is%20on%20the%0Atechnical%20methodologies%20and%20design%20principles%20behind%20agentifying%20MediaMind%2C%0Ashowcasing%20how%20agentification%20enhances%20adaptability%2C%20efficiency%2C%20and%0Aresponsiveness.%20Through%20detailed%20case%20studies%20and%20practical%20examples%2C%20we%0Aillustrate%20how%20the%20agentification%20of%20MediaMind%20empowers%20organizations%20to%0Astreamline%20workflows%2C%20optimize%20decision-making%2C%20and%20respond%20to%20evolving%20trends.%0AThis%20work%20underscores%20the%20broader%20potential%20of%20agentification%20to%20revolutionize%0Asoftware%20tools%20across%20various%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMediaMind%253A%2520Revolutionizing%2520Media%2520Monitoring%2520using%2520Agentification%26entry.906535625%3DAhmet%2520Gunduz%2520and%2520Kamer%2520Ali%2520Yuksel%2520and%2520Hassan%2520Sawaf%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520of%2520rapid%2520technological%2520advancements%252C%2520agentification%2520of%2520software%250Atools%2520has%2520emerged%2520as%2520a%2520critical%2520innovation%252C%2520enabling%2520systems%2520to%2520function%250Aautonomously%2520and%2520adaptively.%2520This%2520paper%2520introduces%2520MediaMind%2520as%2520a%2520case%2520study%2520to%250Ademonstrate%2520the%2520agentification%2520process%252C%2520highlighting%2520how%2520existing%2520software%2520can%250Abe%2520transformed%2520into%2520intelligent%2520agents%2520capable%2520of%2520independent%2520decision-making%250Aand%2520dynamic%2520interaction.%2520Developed%2520by%2520aiXplain%252C%2520MediaMind%2520leverages%2520agent-based%250Aarchitecture%2520to%2520autonomously%2520monitor%252C%2520analyze%252C%2520and%2520provide%2520insights%2520from%250Amultilingual%2520media%2520content%2520in%2520real%2520time.%2520The%2520focus%2520of%2520this%2520paper%2520is%2520on%2520the%250Atechnical%2520methodologies%2520and%2520design%2520principles%2520behind%2520agentifying%2520MediaMind%252C%250Ashowcasing%2520how%2520agentification%2520enhances%2520adaptability%252C%2520efficiency%252C%2520and%250Aresponsiveness.%2520Through%2520detailed%2520case%2520studies%2520and%2520practical%2520examples%252C%2520we%250Aillustrate%2520how%2520the%2520agentification%2520of%2520MediaMind%2520empowers%2520organizations%2520to%250Astreamline%2520workflows%252C%2520optimize%2520decision-making%252C%2520and%2520respond%2520to%2520evolving%2520trends.%250AThis%2520work%2520underscores%2520the%2520broader%2520potential%2520of%2520agentification%2520to%2520revolutionize%250Asoftware%2520tools%2520across%2520various%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MediaMind%3A%20Revolutionizing%20Media%20Monitoring%20using%20Agentification&entry.906535625=Ahmet%20Gunduz%20and%20Kamer%20Ali%20Yuksel%20and%20Hassan%20Sawaf&entry.1292438233=%20%20In%20an%20era%20of%20rapid%20technological%20advancements%2C%20agentification%20of%20software%0Atools%20has%20emerged%20as%20a%20critical%20innovation%2C%20enabling%20systems%20to%20function%0Aautonomously%20and%20adaptively.%20This%20paper%20introduces%20MediaMind%20as%20a%20case%20study%20to%0Ademonstrate%20the%20agentification%20process%2C%20highlighting%20how%20existing%20software%20can%0Abe%20transformed%20into%20intelligent%20agents%20capable%20of%20independent%20decision-making%0Aand%20dynamic%20interaction.%20Developed%20by%20aiXplain%2C%20MediaMind%20leverages%20agent-based%0Aarchitecture%20to%20autonomously%20monitor%2C%20analyze%2C%20and%20provide%20insights%20from%0Amultilingual%20media%20content%20in%20real%20time.%20The%20focus%20of%20this%20paper%20is%20on%20the%0Atechnical%20methodologies%20and%20design%20principles%20behind%20agentifying%20MediaMind%2C%0Ashowcasing%20how%20agentification%20enhances%20adaptability%2C%20efficiency%2C%20and%0Aresponsiveness.%20Through%20detailed%20case%20studies%20and%20practical%20examples%2C%20we%0Aillustrate%20how%20the%20agentification%20of%20MediaMind%20empowers%20organizations%20to%0Astreamline%20workflows%2C%20optimize%20decision-making%2C%20and%20respond%20to%20evolving%20trends.%0AThis%20work%20underscores%20the%20broader%20potential%20of%20agentification%20to%20revolutionize%0Asoftware%20tools%20across%20various%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12745v1&entry.124074799=Read"},
{"title": "Scaling Test-Time Compute Without Verification or RL is Suboptimal", "author": "Amrith Setlur and Nived Rajaraman and Sergey Levine and Aviral Kumar", "abstract": "  Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.\n", "link": "http://arxiv.org/abs/2502.12118v2", "date": "2025-02-18", "relevancy": 1.8884, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5017}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Test-Time%20Compute%20Without%20Verification%20or%20RL%20is%20Suboptimal&body=Title%3A%20Scaling%20Test-Time%20Compute%20Without%20Verification%20or%20RL%20is%20Suboptimal%0AAuthor%3A%20Amrith%20Setlur%20and%20Nived%20Rajaraman%20and%20Sergey%20Levine%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20Despite%20substantial%20advances%20in%20scaling%20test-time%20compute%2C%20an%20ongoing%20debate%0Ain%20the%20community%20is%20how%20it%20should%20be%20scaled%20up%20to%20enable%20continued%20and%0Aefficient%20improvements%20with%20scaling.%20There%20are%20largely%20two%20approaches%3A%20first%2C%0Adistilling%20successful%20search%20or%20thinking%20traces%3B%20and%20second%2C%20using%20verification%0A%28e.g.%2C%200/1%20outcome%20rewards%2C%20reward%20models%2C%20or%20verifiers%29%20to%20guide%20reinforcement%0Alearning%20%28RL%29%20and%20search%20algorithms.%20In%20this%20paper%2C%20we%20prove%20that%20finetuning%0ALLMs%20with%20verifier-based%20%28VB%29%20methods%20based%20on%20RL%20or%20search%20is%20far%20superior%20to%0Averifier-free%20%28VF%29%20approaches%20based%20on%20distilling%20or%20cloning%20search%20traces%2C%0Agiven%20a%20fixed%20amount%20of%20compute/data%20budget.%20Further%2C%20we%20show%20that%20as%20we%20scale%0Atest-time%20compute%20%28measured%20as%20the%20output%20token%20length%29%20and%20training%20data%2C%0Asuboptimality%20of%20VF%20methods%20scales%20poorly%20compared%20to%20VB%20when%20the%20base%0Apre-trained%20LLM%20presents%20a%20heterogeneous%20distribution%20over%20correct%20solution%0Atraces%20%28e.g.%2C%20different%20lengths%2C%20styles%2C%20etc.%29%20and%20admits%20a%20non-sharp%0Adistribution%20over%20rewards%20on%20traces%20sampled%20from%20it.%20We%20formalize%20this%0Acondition%20using%20anti-concentration%20%5BErd%5CH%7Bo%7Ds%2C%201945%5D.%20This%20implies%20a%20stronger%0Aresult%20that%20VB%20methods%20scale%20better%20asymptotically%2C%20with%20the%20performance%20gap%0Abetween%20VB%20and%20VF%20methods%20widening%20as%20test-time%20budget%20grows.%20We%20corroborate%0Aour%20theory%20empirically%20on%20both%20didactic%20and%20math%20reasoning%20problems%20with%0A3/8/32B-sized%20pre-trained%20LLMs%2C%20where%20we%20find%20verification%20is%20crucial%20for%0Ascaling%20test-time%20compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Test-Time%2520Compute%2520Without%2520Verification%2520or%2520RL%2520is%2520Suboptimal%26entry.906535625%3DAmrith%2520Setlur%2520and%2520Nived%2520Rajaraman%2520and%2520Sergey%2520Levine%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520Despite%2520substantial%2520advances%2520in%2520scaling%2520test-time%2520compute%252C%2520an%2520ongoing%2520debate%250Ain%2520the%2520community%2520is%2520how%2520it%2520should%2520be%2520scaled%2520up%2520to%2520enable%2520continued%2520and%250Aefficient%2520improvements%2520with%2520scaling.%2520There%2520are%2520largely%2520two%2520approaches%253A%2520first%252C%250Adistilling%2520successful%2520search%2520or%2520thinking%2520traces%253B%2520and%2520second%252C%2520using%2520verification%250A%2528e.g.%252C%25200/1%2520outcome%2520rewards%252C%2520reward%2520models%252C%2520or%2520verifiers%2529%2520to%2520guide%2520reinforcement%250Alearning%2520%2528RL%2529%2520and%2520search%2520algorithms.%2520In%2520this%2520paper%252C%2520we%2520prove%2520that%2520finetuning%250ALLMs%2520with%2520verifier-based%2520%2528VB%2529%2520methods%2520based%2520on%2520RL%2520or%2520search%2520is%2520far%2520superior%2520to%250Averifier-free%2520%2528VF%2529%2520approaches%2520based%2520on%2520distilling%2520or%2520cloning%2520search%2520traces%252C%250Agiven%2520a%2520fixed%2520amount%2520of%2520compute/data%2520budget.%2520Further%252C%2520we%2520show%2520that%2520as%2520we%2520scale%250Atest-time%2520compute%2520%2528measured%2520as%2520the%2520output%2520token%2520length%2529%2520and%2520training%2520data%252C%250Asuboptimality%2520of%2520VF%2520methods%2520scales%2520poorly%2520compared%2520to%2520VB%2520when%2520the%2520base%250Apre-trained%2520LLM%2520presents%2520a%2520heterogeneous%2520distribution%2520over%2520correct%2520solution%250Atraces%2520%2528e.g.%252C%2520different%2520lengths%252C%2520styles%252C%2520etc.%2529%2520and%2520admits%2520a%2520non-sharp%250Adistribution%2520over%2520rewards%2520on%2520traces%2520sampled%2520from%2520it.%2520We%2520formalize%2520this%250Acondition%2520using%2520anti-concentration%2520%255BErd%255CH%257Bo%257Ds%252C%25201945%255D.%2520This%2520implies%2520a%2520stronger%250Aresult%2520that%2520VB%2520methods%2520scale%2520better%2520asymptotically%252C%2520with%2520the%2520performance%2520gap%250Abetween%2520VB%2520and%2520VF%2520methods%2520widening%2520as%2520test-time%2520budget%2520grows.%2520We%2520corroborate%250Aour%2520theory%2520empirically%2520on%2520both%2520didactic%2520and%2520math%2520reasoning%2520problems%2520with%250A3/8/32B-sized%2520pre-trained%2520LLMs%252C%2520where%2520we%2520find%2520verification%2520is%2520crucial%2520for%250Ascaling%2520test-time%2520compute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Test-Time%20Compute%20Without%20Verification%20or%20RL%20is%20Suboptimal&entry.906535625=Amrith%20Setlur%20and%20Nived%20Rajaraman%20and%20Sergey%20Levine%20and%20Aviral%20Kumar&entry.1292438233=%20%20Despite%20substantial%20advances%20in%20scaling%20test-time%20compute%2C%20an%20ongoing%20debate%0Ain%20the%20community%20is%20how%20it%20should%20be%20scaled%20up%20to%20enable%20continued%20and%0Aefficient%20improvements%20with%20scaling.%20There%20are%20largely%20two%20approaches%3A%20first%2C%0Adistilling%20successful%20search%20or%20thinking%20traces%3B%20and%20second%2C%20using%20verification%0A%28e.g.%2C%200/1%20outcome%20rewards%2C%20reward%20models%2C%20or%20verifiers%29%20to%20guide%20reinforcement%0Alearning%20%28RL%29%20and%20search%20algorithms.%20In%20this%20paper%2C%20we%20prove%20that%20finetuning%0ALLMs%20with%20verifier-based%20%28VB%29%20methods%20based%20on%20RL%20or%20search%20is%20far%20superior%20to%0Averifier-free%20%28VF%29%20approaches%20based%20on%20distilling%20or%20cloning%20search%20traces%2C%0Agiven%20a%20fixed%20amount%20of%20compute/data%20budget.%20Further%2C%20we%20show%20that%20as%20we%20scale%0Atest-time%20compute%20%28measured%20as%20the%20output%20token%20length%29%20and%20training%20data%2C%0Asuboptimality%20of%20VF%20methods%20scales%20poorly%20compared%20to%20VB%20when%20the%20base%0Apre-trained%20LLM%20presents%20a%20heterogeneous%20distribution%20over%20correct%20solution%0Atraces%20%28e.g.%2C%20different%20lengths%2C%20styles%2C%20etc.%29%20and%20admits%20a%20non-sharp%0Adistribution%20over%20rewards%20on%20traces%20sampled%20from%20it.%20We%20formalize%20this%0Acondition%20using%20anti-concentration%20%5BErd%5CH%7Bo%7Ds%2C%201945%5D.%20This%20implies%20a%20stronger%0Aresult%20that%20VB%20methods%20scale%20better%20asymptotically%2C%20with%20the%20performance%20gap%0Abetween%20VB%20and%20VF%20methods%20widening%20as%20test-time%20budget%20grows.%20We%20corroborate%0Aour%20theory%20empirically%20on%20both%20didactic%20and%20math%20reasoning%20problems%20with%0A3/8/32B-sized%20pre-trained%20LLMs%2C%20where%20we%20find%20verification%20is%20crucial%20for%0Ascaling%20test-time%20compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12118v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


