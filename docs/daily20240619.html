<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240618.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SRGS: Super-Resolution 3D Gaussian Splatting", "author": "Xiang Feng and Yongbo He and Yubo Wang and Yan Yang and Wen Li and Yifei Chen and Zhenzhong Kuang and Jiajun ding and Jianping Fan and Yu Jun", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel\nexplicit 3D representation. This approach relies on the representation power of\nGaussian primitives to provide a high-quality rendering. However, primitives\noptimized at low resolution inevitably exhibit sparsity and texture deficiency,\nposing a challenge for achieving high-resolution novel view synthesis (HRNVS).\nTo address this problem, we propose Super-Resolution 3D Gaussian Splatting\n(SRGS) to perform the optimization in a high-resolution (HR) space. The\nsub-pixel constraint is introduced for the increased viewpoints in HR space,\nexploiting the sub-pixel cross-view information of the multiple low-resolution\n(LR) views. The gradient accumulated from more viewpoints will facilitate the\ndensification of primitives. Furthermore, a pre-trained 2D super-resolution\nmodel is integrated with the sub-pixel constraint, enabling these dense\nprimitives to learn faithful texture features. In general, our method focuses\non densification and texture learning to effectively enhance the representation\nability of primitives. Experimentally, our method achieves high rendering\nquality on HRNVS only with LR inputs, outperforming state-of-the-art methods on\nchallenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes\nwill be released upon acceptance.\n", "link": "http://arxiv.org/abs/2404.10318v2", "date": "2024-06-18", "relevancy": 3.1679, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7587}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6053}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRGS%3A%20Super-Resolution%203D%20Gaussian%20Splatting&body=Title%3A%20SRGS%3A%20Super-Resolution%203D%20Gaussian%20Splatting%0AAuthor%3A%20Xiang%20Feng%20and%20Yongbo%20He%20and%20Yubo%20Wang%20and%20Yan%20Yang%20and%20Wen%20Li%20and%20Yifei%20Chen%20and%20Zhenzhong%20Kuang%20and%20Jiajun%20ding%20and%20Jianping%20Fan%20and%20Yu%20Jun%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20gained%20popularity%20as%20a%20novel%0Aexplicit%203D%20representation.%20This%20approach%20relies%20on%20the%20representation%20power%20of%0AGaussian%20primitives%20to%20provide%20a%20high-quality%20rendering.%20However%2C%20primitives%0Aoptimized%20at%20low%20resolution%20inevitably%20exhibit%20sparsity%20and%20texture%20deficiency%2C%0Aposing%20a%20challenge%20for%20achieving%20high-resolution%20novel%20view%20synthesis%20%28HRNVS%29.%0ATo%20address%20this%20problem%2C%20we%20propose%20Super-Resolution%203D%20Gaussian%20Splatting%0A%28SRGS%29%20to%20perform%20the%20optimization%20in%20a%20high-resolution%20%28HR%29%20space.%20The%0Asub-pixel%20constraint%20is%20introduced%20for%20the%20increased%20viewpoints%20in%20HR%20space%2C%0Aexploiting%20the%20sub-pixel%20cross-view%20information%20of%20the%20multiple%20low-resolution%0A%28LR%29%20views.%20The%20gradient%20accumulated%20from%20more%20viewpoints%20will%20facilitate%20the%0Adensification%20of%20primitives.%20Furthermore%2C%20a%20pre-trained%202D%20super-resolution%0Amodel%20is%20integrated%20with%20the%20sub-pixel%20constraint%2C%20enabling%20these%20dense%0Aprimitives%20to%20learn%20faithful%20texture%20features.%20In%20general%2C%20our%20method%20focuses%0Aon%20densification%20and%20texture%20learning%20to%20effectively%20enhance%20the%20representation%0Aability%20of%20primitives.%20Experimentally%2C%20our%20method%20achieves%20high%20rendering%0Aquality%20on%20HRNVS%20only%20with%20LR%20inputs%2C%20outperforming%20state-of-the-art%20methods%20on%0Achallenging%20datasets%20such%20as%20Mip-NeRF%20360%20and%20Tanks%20%26%20Temples.%20Related%20codes%0Awill%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRGS%253A%2520Super-Resolution%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DXiang%2520Feng%2520and%2520Yongbo%2520He%2520and%2520Yubo%2520Wang%2520and%2520Yan%2520Yang%2520and%2520Wen%2520Li%2520and%2520Yifei%2520Chen%2520and%2520Zhenzhong%2520Kuang%2520and%2520Jiajun%2520ding%2520and%2520Jianping%2520Fan%2520and%2520Yu%2520Jun%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520gained%2520popularity%2520as%2520a%2520novel%250Aexplicit%25203D%2520representation.%2520This%2520approach%2520relies%2520on%2520the%2520representation%2520power%2520of%250AGaussian%2520primitives%2520to%2520provide%2520a%2520high-quality%2520rendering.%2520However%252C%2520primitives%250Aoptimized%2520at%2520low%2520resolution%2520inevitably%2520exhibit%2520sparsity%2520and%2520texture%2520deficiency%252C%250Aposing%2520a%2520challenge%2520for%2520achieving%2520high-resolution%2520novel%2520view%2520synthesis%2520%2528HRNVS%2529.%250ATo%2520address%2520this%2520problem%252C%2520we%2520propose%2520Super-Resolution%25203D%2520Gaussian%2520Splatting%250A%2528SRGS%2529%2520to%2520perform%2520the%2520optimization%2520in%2520a%2520high-resolution%2520%2528HR%2529%2520space.%2520The%250Asub-pixel%2520constraint%2520is%2520introduced%2520for%2520the%2520increased%2520viewpoints%2520in%2520HR%2520space%252C%250Aexploiting%2520the%2520sub-pixel%2520cross-view%2520information%2520of%2520the%2520multiple%2520low-resolution%250A%2528LR%2529%2520views.%2520The%2520gradient%2520accumulated%2520from%2520more%2520viewpoints%2520will%2520facilitate%2520the%250Adensification%2520of%2520primitives.%2520Furthermore%252C%2520a%2520pre-trained%25202D%2520super-resolution%250Amodel%2520is%2520integrated%2520with%2520the%2520sub-pixel%2520constraint%252C%2520enabling%2520these%2520dense%250Aprimitives%2520to%2520learn%2520faithful%2520texture%2520features.%2520In%2520general%252C%2520our%2520method%2520focuses%250Aon%2520densification%2520and%2520texture%2520learning%2520to%2520effectively%2520enhance%2520the%2520representation%250Aability%2520of%2520primitives.%2520Experimentally%252C%2520our%2520method%2520achieves%2520high%2520rendering%250Aquality%2520on%2520HRNVS%2520only%2520with%2520LR%2520inputs%252C%2520outperforming%2520state-of-the-art%2520methods%2520on%250Achallenging%2520datasets%2520such%2520as%2520Mip-NeRF%2520360%2520and%2520Tanks%2520%2526%2520Temples.%2520Related%2520codes%250Awill%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRGS%3A%20Super-Resolution%203D%20Gaussian%20Splatting&entry.906535625=Xiang%20Feng%20and%20Yongbo%20He%20and%20Yubo%20Wang%20and%20Yan%20Yang%20and%20Wen%20Li%20and%20Yifei%20Chen%20and%20Zhenzhong%20Kuang%20and%20Jiajun%20ding%20and%20Jianping%20Fan%20and%20Yu%20Jun&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20gained%20popularity%20as%20a%20novel%0Aexplicit%203D%20representation.%20This%20approach%20relies%20on%20the%20representation%20power%20of%0AGaussian%20primitives%20to%20provide%20a%20high-quality%20rendering.%20However%2C%20primitives%0Aoptimized%20at%20low%20resolution%20inevitably%20exhibit%20sparsity%20and%20texture%20deficiency%2C%0Aposing%20a%20challenge%20for%20achieving%20high-resolution%20novel%20view%20synthesis%20%28HRNVS%29.%0ATo%20address%20this%20problem%2C%20we%20propose%20Super-Resolution%203D%20Gaussian%20Splatting%0A%28SRGS%29%20to%20perform%20the%20optimization%20in%20a%20high-resolution%20%28HR%29%20space.%20The%0Asub-pixel%20constraint%20is%20introduced%20for%20the%20increased%20viewpoints%20in%20HR%20space%2C%0Aexploiting%20the%20sub-pixel%20cross-view%20information%20of%20the%20multiple%20low-resolution%0A%28LR%29%20views.%20The%20gradient%20accumulated%20from%20more%20viewpoints%20will%20facilitate%20the%0Adensification%20of%20primitives.%20Furthermore%2C%20a%20pre-trained%202D%20super-resolution%0Amodel%20is%20integrated%20with%20the%20sub-pixel%20constraint%2C%20enabling%20these%20dense%0Aprimitives%20to%20learn%20faithful%20texture%20features.%20In%20general%2C%20our%20method%20focuses%0Aon%20densification%20and%20texture%20learning%20to%20effectively%20enhance%20the%20representation%0Aability%20of%20primitives.%20Experimentally%2C%20our%20method%20achieves%20high%20rendering%0Aquality%20on%20HRNVS%20only%20with%20LR%20inputs%2C%20outperforming%20state-of-the-art%20methods%20on%0Achallenging%20datasets%20such%20as%20Mip-NeRF%20360%20and%20Tanks%20%26%20Temples.%20Related%20codes%0Awill%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10318v2&entry.124074799=Read"},
{"title": "Efficient and Long-Tailed Generalization for Pre-trained Vision-Language\n  Model", "author": "Jiang-Xin Shi and Chi Zhang and Tong Wei and Yu-Feng Li", "abstract": "  Pre-trained vision-language models like CLIP have shown powerful zero-shot\ninference ability via image-text matching and prove to be strong few-shot\nlearners in various downstream tasks. However, in real-world scenarios,\nadapting CLIP to downstream tasks may encounter the following challenges: 1)\ndata may exhibit long-tailed data distributions and might not have abundant\nsamples for all the classes; 2) There might be emerging tasks with new classes\nthat contain no samples at all. To overcome them, we propose a novel framework\nto achieve efficient and long-tailed generalization, which can be termed as\nCandle. During the training process, we propose compensating logit-adjusted\nloss to encourage large margins of prototypes and alleviate imbalance both\nwithin the base classes and between the base and new classes. For efficient\nadaptation, we treat the CLIP model as a black box and leverage the extracted\nfeatures to obtain visual and textual prototypes for prediction. To make full\nuse of multi-modal information, we also propose cross-modal attention to enrich\nthe features from both modalities. For effective generalization, we introduce\nvirtual prototypes for new classes to make up for their lack of training\nimages. Candle achieves state-of-the-art performance over extensive experiments\non 11 diverse datasets while substantially reducing the training time,\ndemonstrating the superiority of our approach. The source code is available at\nhttps://github.com/shijxcs/Candle.\n", "link": "http://arxiv.org/abs/2406.12638v1", "date": "2024-06-18", "relevancy": 2.9418, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6518}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5634}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Long-Tailed%20Generalization%20for%20Pre-trained%20Vision-Language%0A%20%20Model&body=Title%3A%20Efficient%20and%20Long-Tailed%20Generalization%20for%20Pre-trained%20Vision-Language%0A%20%20Model%0AAuthor%3A%20Jiang-Xin%20Shi%20and%20Chi%20Zhang%20and%20Tong%20Wei%20and%20Yu-Feng%20Li%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20like%20CLIP%20have%20shown%20powerful%20zero-shot%0Ainference%20ability%20via%20image-text%20matching%20and%20prove%20to%20be%20strong%20few-shot%0Alearners%20in%20various%20downstream%20tasks.%20However%2C%20in%20real-world%20scenarios%2C%0Aadapting%20CLIP%20to%20downstream%20tasks%20may%20encounter%20the%20following%20challenges%3A%201%29%0Adata%20may%20exhibit%20long-tailed%20data%20distributions%20and%20might%20not%20have%20abundant%0Asamples%20for%20all%20the%20classes%3B%202%29%20There%20might%20be%20emerging%20tasks%20with%20new%20classes%0Athat%20contain%20no%20samples%20at%20all.%20To%20overcome%20them%2C%20we%20propose%20a%20novel%20framework%0Ato%20achieve%20efficient%20and%20long-tailed%20generalization%2C%20which%20can%20be%20termed%20as%0ACandle.%20During%20the%20training%20process%2C%20we%20propose%20compensating%20logit-adjusted%0Aloss%20to%20encourage%20large%20margins%20of%20prototypes%20and%20alleviate%20imbalance%20both%0Awithin%20the%20base%20classes%20and%20between%20the%20base%20and%20new%20classes.%20For%20efficient%0Aadaptation%2C%20we%20treat%20the%20CLIP%20model%20as%20a%20black%20box%20and%20leverage%20the%20extracted%0Afeatures%20to%20obtain%20visual%20and%20textual%20prototypes%20for%20prediction.%20To%20make%20full%0Ause%20of%20multi-modal%20information%2C%20we%20also%20propose%20cross-modal%20attention%20to%20enrich%0Athe%20features%20from%20both%20modalities.%20For%20effective%20generalization%2C%20we%20introduce%0Avirtual%20prototypes%20for%20new%20classes%20to%20make%20up%20for%20their%20lack%20of%20training%0Aimages.%20Candle%20achieves%20state-of-the-art%20performance%20over%20extensive%20experiments%0Aon%2011%20diverse%20datasets%20while%20substantially%20reducing%20the%20training%20time%2C%0Ademonstrating%20the%20superiority%20of%20our%20approach.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/shijxcs/Candle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Long-Tailed%2520Generalization%2520for%2520Pre-trained%2520Vision-Language%250A%2520%2520Model%26entry.906535625%3DJiang-Xin%2520Shi%2520and%2520Chi%2520Zhang%2520and%2520Tong%2520Wei%2520and%2520Yu-Feng%2520Li%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520like%2520CLIP%2520have%2520shown%2520powerful%2520zero-shot%250Ainference%2520ability%2520via%2520image-text%2520matching%2520and%2520prove%2520to%2520be%2520strong%2520few-shot%250Alearners%2520in%2520various%2520downstream%2520tasks.%2520However%252C%2520in%2520real-world%2520scenarios%252C%250Aadapting%2520CLIP%2520to%2520downstream%2520tasks%2520may%2520encounter%2520the%2520following%2520challenges%253A%25201%2529%250Adata%2520may%2520exhibit%2520long-tailed%2520data%2520distributions%2520and%2520might%2520not%2520have%2520abundant%250Asamples%2520for%2520all%2520the%2520classes%253B%25202%2529%2520There%2520might%2520be%2520emerging%2520tasks%2520with%2520new%2520classes%250Athat%2520contain%2520no%2520samples%2520at%2520all.%2520To%2520overcome%2520them%252C%2520we%2520propose%2520a%2520novel%2520framework%250Ato%2520achieve%2520efficient%2520and%2520long-tailed%2520generalization%252C%2520which%2520can%2520be%2520termed%2520as%250ACandle.%2520During%2520the%2520training%2520process%252C%2520we%2520propose%2520compensating%2520logit-adjusted%250Aloss%2520to%2520encourage%2520large%2520margins%2520of%2520prototypes%2520and%2520alleviate%2520imbalance%2520both%250Awithin%2520the%2520base%2520classes%2520and%2520between%2520the%2520base%2520and%2520new%2520classes.%2520For%2520efficient%250Aadaptation%252C%2520we%2520treat%2520the%2520CLIP%2520model%2520as%2520a%2520black%2520box%2520and%2520leverage%2520the%2520extracted%250Afeatures%2520to%2520obtain%2520visual%2520and%2520textual%2520prototypes%2520for%2520prediction.%2520To%2520make%2520full%250Ause%2520of%2520multi-modal%2520information%252C%2520we%2520also%2520propose%2520cross-modal%2520attention%2520to%2520enrich%250Athe%2520features%2520from%2520both%2520modalities.%2520For%2520effective%2520generalization%252C%2520we%2520introduce%250Avirtual%2520prototypes%2520for%2520new%2520classes%2520to%2520make%2520up%2520for%2520their%2520lack%2520of%2520training%250Aimages.%2520Candle%2520achieves%2520state-of-the-art%2520performance%2520over%2520extensive%2520experiments%250Aon%252011%2520diverse%2520datasets%2520while%2520substantially%2520reducing%2520the%2520training%2520time%252C%250Ademonstrating%2520the%2520superiority%2520of%2520our%2520approach.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shijxcs/Candle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Long-Tailed%20Generalization%20for%20Pre-trained%20Vision-Language%0A%20%20Model&entry.906535625=Jiang-Xin%20Shi%20and%20Chi%20Zhang%20and%20Tong%20Wei%20and%20Yu-Feng%20Li&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20like%20CLIP%20have%20shown%20powerful%20zero-shot%0Ainference%20ability%20via%20image-text%20matching%20and%20prove%20to%20be%20strong%20few-shot%0Alearners%20in%20various%20downstream%20tasks.%20However%2C%20in%20real-world%20scenarios%2C%0Aadapting%20CLIP%20to%20downstream%20tasks%20may%20encounter%20the%20following%20challenges%3A%201%29%0Adata%20may%20exhibit%20long-tailed%20data%20distributions%20and%20might%20not%20have%20abundant%0Asamples%20for%20all%20the%20classes%3B%202%29%20There%20might%20be%20emerging%20tasks%20with%20new%20classes%0Athat%20contain%20no%20samples%20at%20all.%20To%20overcome%20them%2C%20we%20propose%20a%20novel%20framework%0Ato%20achieve%20efficient%20and%20long-tailed%20generalization%2C%20which%20can%20be%20termed%20as%0ACandle.%20During%20the%20training%20process%2C%20we%20propose%20compensating%20logit-adjusted%0Aloss%20to%20encourage%20large%20margins%20of%20prototypes%20and%20alleviate%20imbalance%20both%0Awithin%20the%20base%20classes%20and%20between%20the%20base%20and%20new%20classes.%20For%20efficient%0Aadaptation%2C%20we%20treat%20the%20CLIP%20model%20as%20a%20black%20box%20and%20leverage%20the%20extracted%0Afeatures%20to%20obtain%20visual%20and%20textual%20prototypes%20for%20prediction.%20To%20make%20full%0Ause%20of%20multi-modal%20information%2C%20we%20also%20propose%20cross-modal%20attention%20to%20enrich%0Athe%20features%20from%20both%20modalities.%20For%20effective%20generalization%2C%20we%20introduce%0Avirtual%20prototypes%20for%20new%20classes%20to%20make%20up%20for%20their%20lack%20of%20training%0Aimages.%20Candle%20achieves%20state-of-the-art%20performance%20over%20extensive%20experiments%0Aon%2011%20diverse%20datasets%20while%20substantially%20reducing%20the%20training%20time%2C%0Ademonstrating%20the%20superiority%20of%20our%20approach.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/shijxcs/Candle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12638v1&entry.124074799=Read"},
{"title": "AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation", "author": "Xinyu Hou and Xiaoming Li and Chen Change Loy", "abstract": "  Despite the high-quality results of text-to-image generation, stereotypical\nbiases have been spotted in their generated contents, compromising the fairness\nof generative models. In this work, we propose to learn adaptive inclusive\ntokens to shift the attribute distribution of the final generative outputs.\nUnlike existing de-biasing approaches, our method requires neither explicit\nattribute specification nor prior knowledge of the bias distribution.\nSpecifically, the core of our method is a lightweight adaptive mapping network,\nwhich can customize the inclusive tokens for the concepts to be de-biased,\nmaking the tokens generalizable to unseen concepts regardless of their original\nbias distributions. This is achieved by tuning the adaptive mapping network\nwith a handful of balanced and inclusive samples using an anchor loss.\nExperimental results demonstrate that our method outperforms previous bias\nmitigation methods without attribute specification while preserving the\nalignment between generative results and text descriptions. Moreover, our\nmethod achieves comparable performance to models that require specific\nattributes or editing directions for generation. Extensive experiments showcase\nthe effectiveness of our adaptive inclusive tokens in mitigating stereotypical\nbias in text-to-image generation. The code will be available at\nhttps://github.com/itsmag11/AITTI.\n", "link": "http://arxiv.org/abs/2406.12805v1", "date": "2024-06-18", "relevancy": 2.9351, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6045}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.593}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AITTI%3A%20Learning%20Adaptive%20Inclusive%20Token%20for%20Text-to-Image%20Generation&body=Title%3A%20AITTI%3A%20Learning%20Adaptive%20Inclusive%20Token%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Xinyu%20Hou%20and%20Xiaoming%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20Despite%20the%20high-quality%20results%20of%20text-to-image%20generation%2C%20stereotypical%0Abiases%20have%20been%20spotted%20in%20their%20generated%20contents%2C%20compromising%20the%20fairness%0Aof%20generative%20models.%20In%20this%20work%2C%20we%20propose%20to%20learn%20adaptive%20inclusive%0Atokens%20to%20shift%20the%20attribute%20distribution%20of%20the%20final%20generative%20outputs.%0AUnlike%20existing%20de-biasing%20approaches%2C%20our%20method%20requires%20neither%20explicit%0Aattribute%20specification%20nor%20prior%20knowledge%20of%20the%20bias%20distribution.%0ASpecifically%2C%20the%20core%20of%20our%20method%20is%20a%20lightweight%20adaptive%20mapping%20network%2C%0Awhich%20can%20customize%20the%20inclusive%20tokens%20for%20the%20concepts%20to%20be%20de-biased%2C%0Amaking%20the%20tokens%20generalizable%20to%20unseen%20concepts%20regardless%20of%20their%20original%0Abias%20distributions.%20This%20is%20achieved%20by%20tuning%20the%20adaptive%20mapping%20network%0Awith%20a%20handful%20of%20balanced%20and%20inclusive%20samples%20using%20an%20anchor%20loss.%0AExperimental%20results%20demonstrate%20that%20our%20method%20outperforms%20previous%20bias%0Amitigation%20methods%20without%20attribute%20specification%20while%20preserving%20the%0Aalignment%20between%20generative%20results%20and%20text%20descriptions.%20Moreover%2C%20our%0Amethod%20achieves%20comparable%20performance%20to%20models%20that%20require%20specific%0Aattributes%20or%20editing%20directions%20for%20generation.%20Extensive%20experiments%20showcase%0Athe%20effectiveness%20of%20our%20adaptive%20inclusive%20tokens%20in%20mitigating%20stereotypical%0Abias%20in%20text-to-image%20generation.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/itsmag11/AITTI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAITTI%253A%2520Learning%2520Adaptive%2520Inclusive%2520Token%2520for%2520Text-to-Image%2520Generation%26entry.906535625%3DXinyu%2520Hou%2520and%2520Xiaoming%2520Li%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520Despite%2520the%2520high-quality%2520results%2520of%2520text-to-image%2520generation%252C%2520stereotypical%250Abiases%2520have%2520been%2520spotted%2520in%2520their%2520generated%2520contents%252C%2520compromising%2520the%2520fairness%250Aof%2520generative%2520models.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520learn%2520adaptive%2520inclusive%250Atokens%2520to%2520shift%2520the%2520attribute%2520distribution%2520of%2520the%2520final%2520generative%2520outputs.%250AUnlike%2520existing%2520de-biasing%2520approaches%252C%2520our%2520method%2520requires%2520neither%2520explicit%250Aattribute%2520specification%2520nor%2520prior%2520knowledge%2520of%2520the%2520bias%2520distribution.%250ASpecifically%252C%2520the%2520core%2520of%2520our%2520method%2520is%2520a%2520lightweight%2520adaptive%2520mapping%2520network%252C%250Awhich%2520can%2520customize%2520the%2520inclusive%2520tokens%2520for%2520the%2520concepts%2520to%2520be%2520de-biased%252C%250Amaking%2520the%2520tokens%2520generalizable%2520to%2520unseen%2520concepts%2520regardless%2520of%2520their%2520original%250Abias%2520distributions.%2520This%2520is%2520achieved%2520by%2520tuning%2520the%2520adaptive%2520mapping%2520network%250Awith%2520a%2520handful%2520of%2520balanced%2520and%2520inclusive%2520samples%2520using%2520an%2520anchor%2520loss.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520previous%2520bias%250Amitigation%2520methods%2520without%2520attribute%2520specification%2520while%2520preserving%2520the%250Aalignment%2520between%2520generative%2520results%2520and%2520text%2520descriptions.%2520Moreover%252C%2520our%250Amethod%2520achieves%2520comparable%2520performance%2520to%2520models%2520that%2520require%2520specific%250Aattributes%2520or%2520editing%2520directions%2520for%2520generation.%2520Extensive%2520experiments%2520showcase%250Athe%2520effectiveness%2520of%2520our%2520adaptive%2520inclusive%2520tokens%2520in%2520mitigating%2520stereotypical%250Abias%2520in%2520text-to-image%2520generation.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/itsmag11/AITTI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AITTI%3A%20Learning%20Adaptive%20Inclusive%20Token%20for%20Text-to-Image%20Generation&entry.906535625=Xinyu%20Hou%20and%20Xiaoming%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20Despite%20the%20high-quality%20results%20of%20text-to-image%20generation%2C%20stereotypical%0Abiases%20have%20been%20spotted%20in%20their%20generated%20contents%2C%20compromising%20the%20fairness%0Aof%20generative%20models.%20In%20this%20work%2C%20we%20propose%20to%20learn%20adaptive%20inclusive%0Atokens%20to%20shift%20the%20attribute%20distribution%20of%20the%20final%20generative%20outputs.%0AUnlike%20existing%20de-biasing%20approaches%2C%20our%20method%20requires%20neither%20explicit%0Aattribute%20specification%20nor%20prior%20knowledge%20of%20the%20bias%20distribution.%0ASpecifically%2C%20the%20core%20of%20our%20method%20is%20a%20lightweight%20adaptive%20mapping%20network%2C%0Awhich%20can%20customize%20the%20inclusive%20tokens%20for%20the%20concepts%20to%20be%20de-biased%2C%0Amaking%20the%20tokens%20generalizable%20to%20unseen%20concepts%20regardless%20of%20their%20original%0Abias%20distributions.%20This%20is%20achieved%20by%20tuning%20the%20adaptive%20mapping%20network%0Awith%20a%20handful%20of%20balanced%20and%20inclusive%20samples%20using%20an%20anchor%20loss.%0AExperimental%20results%20demonstrate%20that%20our%20method%20outperforms%20previous%20bias%0Amitigation%20methods%20without%20attribute%20specification%20while%20preserving%20the%0Aalignment%20between%20generative%20results%20and%20text%20descriptions.%20Moreover%2C%20our%0Amethod%20achieves%20comparable%20performance%20to%20models%20that%20require%20specific%0Aattributes%20or%20editing%20directions%20for%20generation.%20Extensive%20experiments%20showcase%0Athe%20effectiveness%20of%20our%20adaptive%20inclusive%20tokens%20in%20mitigating%20stereotypical%0Abias%20in%20text-to-image%20generation.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/itsmag11/AITTI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12805v1&entry.124074799=Read"},
{"title": "A real-time, hardware agnostic framework for close-up branch\n  reconstruction using RGB data", "author": "Alexander You and Aarushi Mehta and Luke Strohbehn and Jochen Hemming and Cindy Grimm and Joseph R. Davidson", "abstract": "  Creating accurate 3D models of tree topology is an important task for tree\npruning. The 3D model is used to decide which branches to prune and then to\nexecute the pruning cuts. Previous methods for creating 3D tree models have\ntypically relied on point clouds, which are often computationally expensive to\nprocess and can suffer from data defects, especially with thin branches. In\nthis paper, we propose a method for actively scanning along a primary tree\nbranch, detecting secondary branches to be pruned, and reconstructing their 3D\ngeometry using just an RGB camera mounted on a robot arm. We experimentally\nvalidate that our setup is able to produce primary branch models with 4-5 mm\naccuracy and secondary branch models with 15 degrees orientation accuracy with\nrespect to the ground truth model. Our framework is real-time and can run up to\n10 cm/s with no loss in model accuracy or ability to detect secondary branches.\n", "link": "http://arxiv.org/abs/2309.11580v2", "date": "2024-06-18", "relevancy": 2.856, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6296}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20real-time%2C%20hardware%20agnostic%20framework%20for%20close-up%20branch%0A%20%20reconstruction%20using%20RGB%20data&body=Title%3A%20A%20real-time%2C%20hardware%20agnostic%20framework%20for%20close-up%20branch%0A%20%20reconstruction%20using%20RGB%20data%0AAuthor%3A%20Alexander%20You%20and%20Aarushi%20Mehta%20and%20Luke%20Strohbehn%20and%20Jochen%20Hemming%20and%20Cindy%20Grimm%20and%20Joseph%20R.%20Davidson%0AAbstract%3A%20%20%20Creating%20accurate%203D%20models%20of%20tree%20topology%20is%20an%20important%20task%20for%20tree%0Apruning.%20The%203D%20model%20is%20used%20to%20decide%20which%20branches%20to%20prune%20and%20then%20to%0Aexecute%20the%20pruning%20cuts.%20Previous%20methods%20for%20creating%203D%20tree%20models%20have%0Atypically%20relied%20on%20point%20clouds%2C%20which%20are%20often%20computationally%20expensive%20to%0Aprocess%20and%20can%20suffer%20from%20data%20defects%2C%20especially%20with%20thin%20branches.%20In%0Athis%20paper%2C%20we%20propose%20a%20method%20for%20actively%20scanning%20along%20a%20primary%20tree%0Abranch%2C%20detecting%20secondary%20branches%20to%20be%20pruned%2C%20and%20reconstructing%20their%203D%0Ageometry%20using%20just%20an%20RGB%20camera%20mounted%20on%20a%20robot%20arm.%20We%20experimentally%0Avalidate%20that%20our%20setup%20is%20able%20to%20produce%20primary%20branch%20models%20with%204-5%20mm%0Aaccuracy%20and%20secondary%20branch%20models%20with%2015%20degrees%20orientation%20accuracy%20with%0Arespect%20to%20the%20ground%20truth%20model.%20Our%20framework%20is%20real-time%20and%20can%20run%20up%20to%0A10%20cm/s%20with%20no%20loss%20in%20model%20accuracy%20or%20ability%20to%20detect%20secondary%20branches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11580v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520real-time%252C%2520hardware%2520agnostic%2520framework%2520for%2520close-up%2520branch%250A%2520%2520reconstruction%2520using%2520RGB%2520data%26entry.906535625%3DAlexander%2520You%2520and%2520Aarushi%2520Mehta%2520and%2520Luke%2520Strohbehn%2520and%2520Jochen%2520Hemming%2520and%2520Cindy%2520Grimm%2520and%2520Joseph%2520R.%2520Davidson%26entry.1292438233%3D%2520%2520Creating%2520accurate%25203D%2520models%2520of%2520tree%2520topology%2520is%2520an%2520important%2520task%2520for%2520tree%250Apruning.%2520The%25203D%2520model%2520is%2520used%2520to%2520decide%2520which%2520branches%2520to%2520prune%2520and%2520then%2520to%250Aexecute%2520the%2520pruning%2520cuts.%2520Previous%2520methods%2520for%2520creating%25203D%2520tree%2520models%2520have%250Atypically%2520relied%2520on%2520point%2520clouds%252C%2520which%2520are%2520often%2520computationally%2520expensive%2520to%250Aprocess%2520and%2520can%2520suffer%2520from%2520data%2520defects%252C%2520especially%2520with%2520thin%2520branches.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520method%2520for%2520actively%2520scanning%2520along%2520a%2520primary%2520tree%250Abranch%252C%2520detecting%2520secondary%2520branches%2520to%2520be%2520pruned%252C%2520and%2520reconstructing%2520their%25203D%250Ageometry%2520using%2520just%2520an%2520RGB%2520camera%2520mounted%2520on%2520a%2520robot%2520arm.%2520We%2520experimentally%250Avalidate%2520that%2520our%2520setup%2520is%2520able%2520to%2520produce%2520primary%2520branch%2520models%2520with%25204-5%2520mm%250Aaccuracy%2520and%2520secondary%2520branch%2520models%2520with%252015%2520degrees%2520orientation%2520accuracy%2520with%250Arespect%2520to%2520the%2520ground%2520truth%2520model.%2520Our%2520framework%2520is%2520real-time%2520and%2520can%2520run%2520up%2520to%250A10%2520cm/s%2520with%2520no%2520loss%2520in%2520model%2520accuracy%2520or%2520ability%2520to%2520detect%2520secondary%2520branches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11580v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20real-time%2C%20hardware%20agnostic%20framework%20for%20close-up%20branch%0A%20%20reconstruction%20using%20RGB%20data&entry.906535625=Alexander%20You%20and%20Aarushi%20Mehta%20and%20Luke%20Strohbehn%20and%20Jochen%20Hemming%20and%20Cindy%20Grimm%20and%20Joseph%20R.%20Davidson&entry.1292438233=%20%20Creating%20accurate%203D%20models%20of%20tree%20topology%20is%20an%20important%20task%20for%20tree%0Apruning.%20The%203D%20model%20is%20used%20to%20decide%20which%20branches%20to%20prune%20and%20then%20to%0Aexecute%20the%20pruning%20cuts.%20Previous%20methods%20for%20creating%203D%20tree%20models%20have%0Atypically%20relied%20on%20point%20clouds%2C%20which%20are%20often%20computationally%20expensive%20to%0Aprocess%20and%20can%20suffer%20from%20data%20defects%2C%20especially%20with%20thin%20branches.%20In%0Athis%20paper%2C%20we%20propose%20a%20method%20for%20actively%20scanning%20along%20a%20primary%20tree%0Abranch%2C%20detecting%20secondary%20branches%20to%20be%20pruned%2C%20and%20reconstructing%20their%203D%0Ageometry%20using%20just%20an%20RGB%20camera%20mounted%20on%20a%20robot%20arm.%20We%20experimentally%0Avalidate%20that%20our%20setup%20is%20able%20to%20produce%20primary%20branch%20models%20with%204-5%20mm%0Aaccuracy%20and%20secondary%20branch%20models%20with%2015%20degrees%20orientation%20accuracy%20with%0Arespect%20to%20the%20ground%20truth%20model.%20Our%20framework%20is%20real-time%20and%20can%20run%20up%20to%0A10%20cm/s%20with%20no%20loss%20in%20model%20accuracy%20or%20ability%20to%20detect%20secondary%20branches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11580v2&entry.124074799=Read"},
{"title": "MeshXL: Neural Coordinate Field for Generative 3D Foundation Models", "author": "Sijin Chen and Xin Chen and Anqi Pang and Xianfang Zeng and Wei Cheng and Yijun Fu and Fukun Yin and Yanru Wang and Zhibin Wang and Chi Zhang and Jingyi Yu and Gang Yu and Bin Fu and Tao Chen", "abstract": "  The polygon mesh representation of 3D data exhibits great flexibility, fast\nrendering speed, and storage efficiency, which is widely preferred in various\napplications. However, given its unstructured graph representation, the direct\ngeneration of high-fidelity 3D meshes is challenging. Fortunately, with a\npre-defined ordering strategy, 3D meshes can be represented as sequences, and\nthe generation process can be seamlessly treated as an auto-regressive problem.\nIn this paper, we validate the Neural Coordinate Field (NeurCF), an explicit\ncoordinate representation with implicit neural embeddings, is a\nsimple-yet-effective representation for large-scale sequential mesh modeling.\nAfter that, we present MeshXL, a family of generative pre-trained\nauto-regressive models, which addresses the process of 3D mesh generation with\nmodern large language model approaches. Extensive experiments show that MeshXL\nis able to generate high-quality 3D meshes, and can also serve as foundation\nmodels for various down-stream applications.\n", "link": "http://arxiv.org/abs/2405.20853v2", "date": "2024-06-18", "relevancy": 2.8412, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6519}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5312}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshXL%3A%20Neural%20Coordinate%20Field%20for%20Generative%203D%20Foundation%20Models&body=Title%3A%20MeshXL%3A%20Neural%20Coordinate%20Field%20for%20Generative%203D%20Foundation%20Models%0AAuthor%3A%20Sijin%20Chen%20and%20Xin%20Chen%20and%20Anqi%20Pang%20and%20Xianfang%20Zeng%20and%20Wei%20Cheng%20and%20Yijun%20Fu%20and%20Fukun%20Yin%20and%20Yanru%20Wang%20and%20Zhibin%20Wang%20and%20Chi%20Zhang%20and%20Jingyi%20Yu%20and%20Gang%20Yu%20and%20Bin%20Fu%20and%20Tao%20Chen%0AAbstract%3A%20%20%20The%20polygon%20mesh%20representation%20of%203D%20data%20exhibits%20great%20flexibility%2C%20fast%0Arendering%20speed%2C%20and%20storage%20efficiency%2C%20which%20is%20widely%20preferred%20in%20various%0Aapplications.%20However%2C%20given%20its%20unstructured%20graph%20representation%2C%20the%20direct%0Ageneration%20of%20high-fidelity%203D%20meshes%20is%20challenging.%20Fortunately%2C%20with%20a%0Apre-defined%20ordering%20strategy%2C%203D%20meshes%20can%20be%20represented%20as%20sequences%2C%20and%0Athe%20generation%20process%20can%20be%20seamlessly%20treated%20as%20an%20auto-regressive%20problem.%0AIn%20this%20paper%2C%20we%20validate%20the%20Neural%20Coordinate%20Field%20%28NeurCF%29%2C%20an%20explicit%0Acoordinate%20representation%20with%20implicit%20neural%20embeddings%2C%20is%20a%0Asimple-yet-effective%20representation%20for%20large-scale%20sequential%20mesh%20modeling.%0AAfter%20that%2C%20we%20present%20MeshXL%2C%20a%20family%20of%20generative%20pre-trained%0Aauto-regressive%20models%2C%20which%20addresses%20the%20process%20of%203D%20mesh%20generation%20with%0Amodern%20large%20language%20model%20approaches.%20Extensive%20experiments%20show%20that%20MeshXL%0Ais%20able%20to%20generate%20high-quality%203D%20meshes%2C%20and%20can%20also%20serve%20as%20foundation%0Amodels%20for%20various%20down-stream%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshXL%253A%2520Neural%2520Coordinate%2520Field%2520for%2520Generative%25203D%2520Foundation%2520Models%26entry.906535625%3DSijin%2520Chen%2520and%2520Xin%2520Chen%2520and%2520Anqi%2520Pang%2520and%2520Xianfang%2520Zeng%2520and%2520Wei%2520Cheng%2520and%2520Yijun%2520Fu%2520and%2520Fukun%2520Yin%2520and%2520Yanru%2520Wang%2520and%2520Zhibin%2520Wang%2520and%2520Chi%2520Zhang%2520and%2520Jingyi%2520Yu%2520and%2520Gang%2520Yu%2520and%2520Bin%2520Fu%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520The%2520polygon%2520mesh%2520representation%2520of%25203D%2520data%2520exhibits%2520great%2520flexibility%252C%2520fast%250Arendering%2520speed%252C%2520and%2520storage%2520efficiency%252C%2520which%2520is%2520widely%2520preferred%2520in%2520various%250Aapplications.%2520However%252C%2520given%2520its%2520unstructured%2520graph%2520representation%252C%2520the%2520direct%250Ageneration%2520of%2520high-fidelity%25203D%2520meshes%2520is%2520challenging.%2520Fortunately%252C%2520with%2520a%250Apre-defined%2520ordering%2520strategy%252C%25203D%2520meshes%2520can%2520be%2520represented%2520as%2520sequences%252C%2520and%250Athe%2520generation%2520process%2520can%2520be%2520seamlessly%2520treated%2520as%2520an%2520auto-regressive%2520problem.%250AIn%2520this%2520paper%252C%2520we%2520validate%2520the%2520Neural%2520Coordinate%2520Field%2520%2528NeurCF%2529%252C%2520an%2520explicit%250Acoordinate%2520representation%2520with%2520implicit%2520neural%2520embeddings%252C%2520is%2520a%250Asimple-yet-effective%2520representation%2520for%2520large-scale%2520sequential%2520mesh%2520modeling.%250AAfter%2520that%252C%2520we%2520present%2520MeshXL%252C%2520a%2520family%2520of%2520generative%2520pre-trained%250Aauto-regressive%2520models%252C%2520which%2520addresses%2520the%2520process%2520of%25203D%2520mesh%2520generation%2520with%250Amodern%2520large%2520language%2520model%2520approaches.%2520Extensive%2520experiments%2520show%2520that%2520MeshXL%250Ais%2520able%2520to%2520generate%2520high-quality%25203D%2520meshes%252C%2520and%2520can%2520also%2520serve%2520as%2520foundation%250Amodels%2520for%2520various%2520down-stream%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshXL%3A%20Neural%20Coordinate%20Field%20for%20Generative%203D%20Foundation%20Models&entry.906535625=Sijin%20Chen%20and%20Xin%20Chen%20and%20Anqi%20Pang%20and%20Xianfang%20Zeng%20and%20Wei%20Cheng%20and%20Yijun%20Fu%20and%20Fukun%20Yin%20and%20Yanru%20Wang%20and%20Zhibin%20Wang%20and%20Chi%20Zhang%20and%20Jingyi%20Yu%20and%20Gang%20Yu%20and%20Bin%20Fu%20and%20Tao%20Chen&entry.1292438233=%20%20The%20polygon%20mesh%20representation%20of%203D%20data%20exhibits%20great%20flexibility%2C%20fast%0Arendering%20speed%2C%20and%20storage%20efficiency%2C%20which%20is%20widely%20preferred%20in%20various%0Aapplications.%20However%2C%20given%20its%20unstructured%20graph%20representation%2C%20the%20direct%0Ageneration%20of%20high-fidelity%203D%20meshes%20is%20challenging.%20Fortunately%2C%20with%20a%0Apre-defined%20ordering%20strategy%2C%203D%20meshes%20can%20be%20represented%20as%20sequences%2C%20and%0Athe%20generation%20process%20can%20be%20seamlessly%20treated%20as%20an%20auto-regressive%20problem.%0AIn%20this%20paper%2C%20we%20validate%20the%20Neural%20Coordinate%20Field%20%28NeurCF%29%2C%20an%20explicit%0Acoordinate%20representation%20with%20implicit%20neural%20embeddings%2C%20is%20a%0Asimple-yet-effective%20representation%20for%20large-scale%20sequential%20mesh%20modeling.%0AAfter%20that%2C%20we%20present%20MeshXL%2C%20a%20family%20of%20generative%20pre-trained%0Aauto-regressive%20models%2C%20which%20addresses%20the%20process%20of%203D%20mesh%20generation%20with%0Amodern%20large%20language%20model%20approaches.%20Extensive%20experiments%20show%20that%20MeshXL%0Ais%20able%20to%20generate%20high-quality%203D%20meshes%2C%20and%20can%20also%20serve%20as%20foundation%0Amodels%20for%20various%20down-stream%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20853v2&entry.124074799=Read"},
{"title": "Advances in 3D Neural Stylization: A Survey", "author": "Yingshu Chen and Guocheng Shao and Ka Chun Shum and Binh-Son Hua and Sai-Kit Yeung", "abstract": "  Modern artificial intelligence offers a novel and transformative approach to\ncreating digital art across diverse styles and modalities like images, videos\nand 3D data, unleashing the power of creativity and revolutionizing the way\nthat we perceive and interact with visual content. This paper reports on recent\nadvances in stylized 3D asset creation and manipulation with the expressive\npower of neural networks. We establish a taxonomy for neural stylization,\nconsidering crucial design choices such as scene representation, guidance data,\noptimization strategies, and output styles. Building on such taxonomy, our\nsurvey first revisits the background of neural stylization on 2D images, and\nthen presents in-depth discussions on recent neural stylization methods for 3D\ndata, accompanied by a mini-benchmark evaluating selected neural field\nstylization methods. Based on the insights gained from the survey, we highlight\nthe practical significance, open challenges, future research, and potential\nimpacts of neural stylization, which facilitates researchers and practitioners\nto navigate the rapidly evolving landscape of 3D content creation using modern\nartificial intelligence.\n", "link": "http://arxiv.org/abs/2311.18328v2", "date": "2024-06-18", "relevancy": 2.778, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5679}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5679}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%203D%20Neural%20Stylization%3A%20A%20Survey&body=Title%3A%20Advances%20in%203D%20Neural%20Stylization%3A%20A%20Survey%0AAuthor%3A%20Yingshu%20Chen%20and%20Guocheng%20Shao%20and%20Ka%20Chun%20Shum%20and%20Binh-Son%20Hua%20and%20Sai-Kit%20Yeung%0AAbstract%3A%20%20%20Modern%20artificial%20intelligence%20offers%20a%20novel%20and%20transformative%20approach%20to%0Acreating%20digital%20art%20across%20diverse%20styles%20and%20modalities%20like%20images%2C%20videos%0Aand%203D%20data%2C%20unleashing%20the%20power%20of%20creativity%20and%20revolutionizing%20the%20way%0Athat%20we%20perceive%20and%20interact%20with%20visual%20content.%20This%20paper%20reports%20on%20recent%0Aadvances%20in%20stylized%203D%20asset%20creation%20and%20manipulation%20with%20the%20expressive%0Apower%20of%20neural%20networks.%20We%20establish%20a%20taxonomy%20for%20neural%20stylization%2C%0Aconsidering%20crucial%20design%20choices%20such%20as%20scene%20representation%2C%20guidance%20data%2C%0Aoptimization%20strategies%2C%20and%20output%20styles.%20Building%20on%20such%20taxonomy%2C%20our%0Asurvey%20first%20revisits%20the%20background%20of%20neural%20stylization%20on%202D%20images%2C%20and%0Athen%20presents%20in-depth%20discussions%20on%20recent%20neural%20stylization%20methods%20for%203D%0Adata%2C%20accompanied%20by%20a%20mini-benchmark%20evaluating%20selected%20neural%20field%0Astylization%20methods.%20Based%20on%20the%20insights%20gained%20from%20the%20survey%2C%20we%20highlight%0Athe%20practical%20significance%2C%20open%20challenges%2C%20future%20research%2C%20and%20potential%0Aimpacts%20of%20neural%20stylization%2C%20which%20facilitates%20researchers%20and%20practitioners%0Ato%20navigate%20the%20rapidly%20evolving%20landscape%20of%203D%20content%20creation%20using%20modern%0Aartificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%25203D%2520Neural%2520Stylization%253A%2520A%2520Survey%26entry.906535625%3DYingshu%2520Chen%2520and%2520Guocheng%2520Shao%2520and%2520Ka%2520Chun%2520Shum%2520and%2520Binh-Son%2520Hua%2520and%2520Sai-Kit%2520Yeung%26entry.1292438233%3D%2520%2520Modern%2520artificial%2520intelligence%2520offers%2520a%2520novel%2520and%2520transformative%2520approach%2520to%250Acreating%2520digital%2520art%2520across%2520diverse%2520styles%2520and%2520modalities%2520like%2520images%252C%2520videos%250Aand%25203D%2520data%252C%2520unleashing%2520the%2520power%2520of%2520creativity%2520and%2520revolutionizing%2520the%2520way%250Athat%2520we%2520perceive%2520and%2520interact%2520with%2520visual%2520content.%2520This%2520paper%2520reports%2520on%2520recent%250Aadvances%2520in%2520stylized%25203D%2520asset%2520creation%2520and%2520manipulation%2520with%2520the%2520expressive%250Apower%2520of%2520neural%2520networks.%2520We%2520establish%2520a%2520taxonomy%2520for%2520neural%2520stylization%252C%250Aconsidering%2520crucial%2520design%2520choices%2520such%2520as%2520scene%2520representation%252C%2520guidance%2520data%252C%250Aoptimization%2520strategies%252C%2520and%2520output%2520styles.%2520Building%2520on%2520such%2520taxonomy%252C%2520our%250Asurvey%2520first%2520revisits%2520the%2520background%2520of%2520neural%2520stylization%2520on%25202D%2520images%252C%2520and%250Athen%2520presents%2520in-depth%2520discussions%2520on%2520recent%2520neural%2520stylization%2520methods%2520for%25203D%250Adata%252C%2520accompanied%2520by%2520a%2520mini-benchmark%2520evaluating%2520selected%2520neural%2520field%250Astylization%2520methods.%2520Based%2520on%2520the%2520insights%2520gained%2520from%2520the%2520survey%252C%2520we%2520highlight%250Athe%2520practical%2520significance%252C%2520open%2520challenges%252C%2520future%2520research%252C%2520and%2520potential%250Aimpacts%2520of%2520neural%2520stylization%252C%2520which%2520facilitates%2520researchers%2520and%2520practitioners%250Ato%2520navigate%2520the%2520rapidly%2520evolving%2520landscape%2520of%25203D%2520content%2520creation%2520using%2520modern%250Aartificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%203D%20Neural%20Stylization%3A%20A%20Survey&entry.906535625=Yingshu%20Chen%20and%20Guocheng%20Shao%20and%20Ka%20Chun%20Shum%20and%20Binh-Son%20Hua%20and%20Sai-Kit%20Yeung&entry.1292438233=%20%20Modern%20artificial%20intelligence%20offers%20a%20novel%20and%20transformative%20approach%20to%0Acreating%20digital%20art%20across%20diverse%20styles%20and%20modalities%20like%20images%2C%20videos%0Aand%203D%20data%2C%20unleashing%20the%20power%20of%20creativity%20and%20revolutionizing%20the%20way%0Athat%20we%20perceive%20and%20interact%20with%20visual%20content.%20This%20paper%20reports%20on%20recent%0Aadvances%20in%20stylized%203D%20asset%20creation%20and%20manipulation%20with%20the%20expressive%0Apower%20of%20neural%20networks.%20We%20establish%20a%20taxonomy%20for%20neural%20stylization%2C%0Aconsidering%20crucial%20design%20choices%20such%20as%20scene%20representation%2C%20guidance%20data%2C%0Aoptimization%20strategies%2C%20and%20output%20styles.%20Building%20on%20such%20taxonomy%2C%20our%0Asurvey%20first%20revisits%20the%20background%20of%20neural%20stylization%20on%202D%20images%2C%20and%0Athen%20presents%20in-depth%20discussions%20on%20recent%20neural%20stylization%20methods%20for%203D%0Adata%2C%20accompanied%20by%20a%20mini-benchmark%20evaluating%20selected%20neural%20field%0Astylization%20methods.%20Based%20on%20the%20insights%20gained%20from%20the%20survey%2C%20we%20highlight%0Athe%20practical%20significance%2C%20open%20challenges%2C%20future%20research%2C%20and%20potential%0Aimpacts%20of%20neural%20stylization%2C%20which%20facilitates%20researchers%20and%20practitioners%0Ato%20navigate%20the%20rapidly%20evolving%20landscape%20of%203D%20content%20creation%20using%20modern%0Aartificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18328v2&entry.124074799=Read"},
{"title": "MAC: A Benchmark for Multiple Attributes Compositional Zero-Shot\n  Learning", "author": "Shuo Xu and Sai Wang and Xinyue Hu and Yutian Lin and Bo Du and Yu Wu", "abstract": "  Compositional Zero-Shot Learning (CZSL) aims to learn semantic primitives\n(attributes and objects) from seen compositions and recognize unseen\nattribute-object compositions. Existing CZSL datasets focus on single\nattributes, neglecting the fact that objects naturally exhibit multiple\ninterrelated attributes. Real-world objects often possess multiple interrelated\nattributes, and current datasets' narrow attribute scope and single attribute\nlabeling introduce annotation biases, undermining model performance and\nevaluation. To address these limitations, we introduce the Multi-Attribute\nComposition (MAC) dataset, encompassing 18,217 images and 11,067 compositions\nwith comprehensive, representative, and diverse attribute annotations. MAC\nincludes an average of 30.2 attributes per object and 65.4 objects per\nattribute, facilitating better multi-attribute composition predictions. Our\ndataset supports deeper semantic understanding and higher-order attribute\nassociations, providing a more realistic and challenging benchmark for the CZSL\ntask. We also develop solutions for multi-attribute compositional learning and\npropose the MM-encoder to disentangling the attributes and objects.\n", "link": "http://arxiv.org/abs/2406.12757v1", "date": "2024-06-18", "relevancy": 2.599, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5215}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAC%3A%20A%20Benchmark%20for%20Multiple%20Attributes%20Compositional%20Zero-Shot%0A%20%20Learning&body=Title%3A%20MAC%3A%20A%20Benchmark%20for%20Multiple%20Attributes%20Compositional%20Zero-Shot%0A%20%20Learning%0AAuthor%3A%20Shuo%20Xu%20and%20Sai%20Wang%20and%20Xinyue%20Hu%20and%20Yutian%20Lin%20and%20Bo%20Du%20and%20Yu%20Wu%0AAbstract%3A%20%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20learn%20semantic%20primitives%0A%28attributes%20and%20objects%29%20from%20seen%20compositions%20and%20recognize%20unseen%0Aattribute-object%20compositions.%20Existing%20CZSL%20datasets%20focus%20on%20single%0Aattributes%2C%20neglecting%20the%20fact%20that%20objects%20naturally%20exhibit%20multiple%0Ainterrelated%20attributes.%20Real-world%20objects%20often%20possess%20multiple%20interrelated%0Aattributes%2C%20and%20current%20datasets%27%20narrow%20attribute%20scope%20and%20single%20attribute%0Alabeling%20introduce%20annotation%20biases%2C%20undermining%20model%20performance%20and%0Aevaluation.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20Multi-Attribute%0AComposition%20%28MAC%29%20dataset%2C%20encompassing%2018%2C217%20images%20and%2011%2C067%20compositions%0Awith%20comprehensive%2C%20representative%2C%20and%20diverse%20attribute%20annotations.%20MAC%0Aincludes%20an%20average%20of%2030.2%20attributes%20per%20object%20and%2065.4%20objects%20per%0Aattribute%2C%20facilitating%20better%20multi-attribute%20composition%20predictions.%20Our%0Adataset%20supports%20deeper%20semantic%20understanding%20and%20higher-order%20attribute%0Aassociations%2C%20providing%20a%20more%20realistic%20and%20challenging%20benchmark%20for%20the%20CZSL%0Atask.%20We%20also%20develop%20solutions%20for%20multi-attribute%20compositional%20learning%20and%0Apropose%20the%20MM-encoder%20to%20disentangling%20the%20attributes%20and%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAC%253A%2520A%2520Benchmark%2520for%2520Multiple%2520Attributes%2520Compositional%2520Zero-Shot%250A%2520%2520Learning%26entry.906535625%3DShuo%2520Xu%2520and%2520Sai%2520Wang%2520and%2520Xinyue%2520Hu%2520and%2520Yutian%2520Lin%2520and%2520Bo%2520Du%2520and%2520Yu%2520Wu%26entry.1292438233%3D%2520%2520Compositional%2520Zero-Shot%2520Learning%2520%2528CZSL%2529%2520aims%2520to%2520learn%2520semantic%2520primitives%250A%2528attributes%2520and%2520objects%2529%2520from%2520seen%2520compositions%2520and%2520recognize%2520unseen%250Aattribute-object%2520compositions.%2520Existing%2520CZSL%2520datasets%2520focus%2520on%2520single%250Aattributes%252C%2520neglecting%2520the%2520fact%2520that%2520objects%2520naturally%2520exhibit%2520multiple%250Ainterrelated%2520attributes.%2520Real-world%2520objects%2520often%2520possess%2520multiple%2520interrelated%250Aattributes%252C%2520and%2520current%2520datasets%2527%2520narrow%2520attribute%2520scope%2520and%2520single%2520attribute%250Alabeling%2520introduce%2520annotation%2520biases%252C%2520undermining%2520model%2520performance%2520and%250Aevaluation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520Multi-Attribute%250AComposition%2520%2528MAC%2529%2520dataset%252C%2520encompassing%252018%252C217%2520images%2520and%252011%252C067%2520compositions%250Awith%2520comprehensive%252C%2520representative%252C%2520and%2520diverse%2520attribute%2520annotations.%2520MAC%250Aincludes%2520an%2520average%2520of%252030.2%2520attributes%2520per%2520object%2520and%252065.4%2520objects%2520per%250Aattribute%252C%2520facilitating%2520better%2520multi-attribute%2520composition%2520predictions.%2520Our%250Adataset%2520supports%2520deeper%2520semantic%2520understanding%2520and%2520higher-order%2520attribute%250Aassociations%252C%2520providing%2520a%2520more%2520realistic%2520and%2520challenging%2520benchmark%2520for%2520the%2520CZSL%250Atask.%2520We%2520also%2520develop%2520solutions%2520for%2520multi-attribute%2520compositional%2520learning%2520and%250Apropose%2520the%2520MM-encoder%2520to%2520disentangling%2520the%2520attributes%2520and%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAC%3A%20A%20Benchmark%20for%20Multiple%20Attributes%20Compositional%20Zero-Shot%0A%20%20Learning&entry.906535625=Shuo%20Xu%20and%20Sai%20Wang%20and%20Xinyue%20Hu%20and%20Yutian%20Lin%20and%20Bo%20Du%20and%20Yu%20Wu&entry.1292438233=%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20learn%20semantic%20primitives%0A%28attributes%20and%20objects%29%20from%20seen%20compositions%20and%20recognize%20unseen%0Aattribute-object%20compositions.%20Existing%20CZSL%20datasets%20focus%20on%20single%0Aattributes%2C%20neglecting%20the%20fact%20that%20objects%20naturally%20exhibit%20multiple%0Ainterrelated%20attributes.%20Real-world%20objects%20often%20possess%20multiple%20interrelated%0Aattributes%2C%20and%20current%20datasets%27%20narrow%20attribute%20scope%20and%20single%20attribute%0Alabeling%20introduce%20annotation%20biases%2C%20undermining%20model%20performance%20and%0Aevaluation.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20Multi-Attribute%0AComposition%20%28MAC%29%20dataset%2C%20encompassing%2018%2C217%20images%20and%2011%2C067%20compositions%0Awith%20comprehensive%2C%20representative%2C%20and%20diverse%20attribute%20annotations.%20MAC%0Aincludes%20an%20average%20of%2030.2%20attributes%20per%20object%20and%2065.4%20objects%20per%0Aattribute%2C%20facilitating%20better%20multi-attribute%20composition%20predictions.%20Our%0Adataset%20supports%20deeper%20semantic%20understanding%20and%20higher-order%20attribute%0Aassociations%2C%20providing%20a%20more%20realistic%20and%20challenging%20benchmark%20for%20the%20CZSL%0Atask.%20We%20also%20develop%20solutions%20for%20multi-attribute%20compositional%20learning%20and%0Apropose%20the%20MM-encoder%20to%20disentangling%20the%20attributes%20and%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12757v1&entry.124074799=Read"},
{"title": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation", "author": "Yixia Li and Boya Xiong and Guanhua Chen and Yun Chen", "abstract": "  Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the false positive rate by up to 18.95% and 36.80%\ncompared to zero-shot and fine-tuning baselines. Ablation studies further\nvalidate our approach's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area.\n", "link": "http://arxiv.org/abs/2406.12629v1", "date": "2024-06-18", "relevancy": 2.546, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5233}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeTAR%3A%20Out-of-Distribution%20Detection%20with%20Selective%20Low-Rank%0A%20%20Approximation&body=Title%3A%20SeTAR%3A%20Out-of-Distribution%20Detection%20with%20Selective%20Low-Rank%0A%20%20Approximation%0AAuthor%3A%20Yixia%20Li%20and%20Boya%20Xiong%20and%20Guanhua%20Chen%20and%20Yun%20Chen%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20crucial%20for%20the%20safe%20deployment%20of%0Aneural%20networks.%20Existing%20CLIP-based%20approaches%20perform%20OOD%20detection%20by%0Adevising%20novel%20scoring%20functions%20or%20sophisticated%20fine-tuning%20methods.%20In%20this%0Awork%2C%20we%20propose%20SeTAR%2C%20a%20novel%2C%20training-free%20OOD%20detection%20method%20that%0Aleverages%20selective%20low-rank%20approximation%20of%20weight%20matrices%20in%0Avision-language%20and%20vision-only%20models.%20SeTAR%20enhances%20OOD%20detection%20via%0Apost-hoc%20modification%20of%20the%20model%27s%20weight%20matrices%20using%20a%20simple%20greedy%0Asearch%20algorithm.%20Based%20on%20SeTAR%2C%20we%20further%20propose%20SeTAR%2BFT%2C%20a%20fine-tuning%0Aextension%20optimizing%20model%20performance%20for%20OOD%20detection%20tasks.%20Extensive%0Aevaluations%20on%20ImageNet1K%20and%20Pascal-VOC%20benchmarks%20show%20SeTAR%27s%20superior%0Aperformance%2C%20reducing%20the%20false%20positive%20rate%20by%20up%20to%2018.95%25%20and%2036.80%25%0Acompared%20to%20zero-shot%20and%20fine-tuning%20baselines.%20Ablation%20studies%20further%0Avalidate%20our%20approach%27s%20effectiveness%2C%20robustness%2C%20and%20generalizability%20across%0Adifferent%20model%20backbones.%20Our%20work%20offers%20a%20scalable%2C%20efficient%20solution%20for%0AOOD%20detection%2C%20setting%20a%20new%20state-of-the-art%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeTAR%253A%2520Out-of-Distribution%2520Detection%2520with%2520Selective%2520Low-Rank%250A%2520%2520Approximation%26entry.906535625%3DYixia%2520Li%2520and%2520Boya%2520Xiong%2520and%2520Guanhua%2520Chen%2520and%2520Yun%2520Chen%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520is%2520crucial%2520for%2520the%2520safe%2520deployment%2520of%250Aneural%2520networks.%2520Existing%2520CLIP-based%2520approaches%2520perform%2520OOD%2520detection%2520by%250Adevising%2520novel%2520scoring%2520functions%2520or%2520sophisticated%2520fine-tuning%2520methods.%2520In%2520this%250Awork%252C%2520we%2520propose%2520SeTAR%252C%2520a%2520novel%252C%2520training-free%2520OOD%2520detection%2520method%2520that%250Aleverages%2520selective%2520low-rank%2520approximation%2520of%2520weight%2520matrices%2520in%250Avision-language%2520and%2520vision-only%2520models.%2520SeTAR%2520enhances%2520OOD%2520detection%2520via%250Apost-hoc%2520modification%2520of%2520the%2520model%2527s%2520weight%2520matrices%2520using%2520a%2520simple%2520greedy%250Asearch%2520algorithm.%2520Based%2520on%2520SeTAR%252C%2520we%2520further%2520propose%2520SeTAR%252BFT%252C%2520a%2520fine-tuning%250Aextension%2520optimizing%2520model%2520performance%2520for%2520OOD%2520detection%2520tasks.%2520Extensive%250Aevaluations%2520on%2520ImageNet1K%2520and%2520Pascal-VOC%2520benchmarks%2520show%2520SeTAR%2527s%2520superior%250Aperformance%252C%2520reducing%2520the%2520false%2520positive%2520rate%2520by%2520up%2520to%252018.95%2525%2520and%252036.80%2525%250Acompared%2520to%2520zero-shot%2520and%2520fine-tuning%2520baselines.%2520Ablation%2520studies%2520further%250Avalidate%2520our%2520approach%2527s%2520effectiveness%252C%2520robustness%252C%2520and%2520generalizability%2520across%250Adifferent%2520model%2520backbones.%2520Our%2520work%2520offers%2520a%2520scalable%252C%2520efficient%2520solution%2520for%250AOOD%2520detection%252C%2520setting%2520a%2520new%2520state-of-the-art%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeTAR%3A%20Out-of-Distribution%20Detection%20with%20Selective%20Low-Rank%0A%20%20Approximation&entry.906535625=Yixia%20Li%20and%20Boya%20Xiong%20and%20Guanhua%20Chen%20and%20Yun%20Chen&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20crucial%20for%20the%20safe%20deployment%20of%0Aneural%20networks.%20Existing%20CLIP-based%20approaches%20perform%20OOD%20detection%20by%0Adevising%20novel%20scoring%20functions%20or%20sophisticated%20fine-tuning%20methods.%20In%20this%0Awork%2C%20we%20propose%20SeTAR%2C%20a%20novel%2C%20training-free%20OOD%20detection%20method%20that%0Aleverages%20selective%20low-rank%20approximation%20of%20weight%20matrices%20in%0Avision-language%20and%20vision-only%20models.%20SeTAR%20enhances%20OOD%20detection%20via%0Apost-hoc%20modification%20of%20the%20model%27s%20weight%20matrices%20using%20a%20simple%20greedy%0Asearch%20algorithm.%20Based%20on%20SeTAR%2C%20we%20further%20propose%20SeTAR%2BFT%2C%20a%20fine-tuning%0Aextension%20optimizing%20model%20performance%20for%20OOD%20detection%20tasks.%20Extensive%0Aevaluations%20on%20ImageNet1K%20and%20Pascal-VOC%20benchmarks%20show%20SeTAR%27s%20superior%0Aperformance%2C%20reducing%20the%20false%20positive%20rate%20by%20up%20to%2018.95%25%20and%2036.80%25%0Acompared%20to%20zero-shot%20and%20fine-tuning%20baselines.%20Ablation%20studies%20further%0Avalidate%20our%20approach%27s%20effectiveness%2C%20robustness%2C%20and%20generalizability%20across%0Adifferent%20model%20backbones.%20Our%20work%20offers%20a%20scalable%2C%20efficient%20solution%20for%0AOOD%20detection%2C%20setting%20a%20new%20state-of-the-art%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12629v1&entry.124074799=Read"},
{"title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification", "author": "Jintao Rong and Hao Chen and Tianxiao Chen and Linlin Ou and Xinyi Yu and Yifan Liu", "abstract": "  Prompt learning has become a popular approach for adapting large\nvision-language models, such as CLIP, to downstream tasks. Typically, prompt\nlearning relies on a fixed prompt token or an input-conditional token to fit a\nsmall amount of data under full supervision. While this paradigm can generalize\nto a certain range of unseen classes, it may struggle when domain gap\nincreases, such as in fine-grained classification and satellite image\nsegmentation. To address this limitation, we propose Retrieval-enhanced Prompt\nlearning (RePrompt), which introduces retrieval mechanisms to cache the\nknowledge representations from downstream tasks. we first construct a retrieval\ndatabase from training examples, or from external examples when available. We\nthen integrate this retrieval-enhanced mechanism into various stages of a\nsimple prompt learning baseline. By referencing similar samples in the training\nset, the enhanced model is better able to adapt to new tasks with few samples.\nOur extensive experiments over 15 vision datasets, including 11 downstream\ntasks with few-shot setting and 4 domain generalization benchmarks, demonstrate\nthat RePrompt achieves considerably improved performance. Our proposed approach\nprovides a promising solution to the challenges faced by prompt learning when\ndomain gap increases. The code and models will be available.\n", "link": "http://arxiv.org/abs/2306.02243v2", "date": "2024-06-18", "relevancy": 2.5409, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-Enhanced%20Visual%20Prompt%20Learning%20for%20Few-shot%20Classification&body=Title%3A%20Retrieval-Enhanced%20Visual%20Prompt%20Learning%20for%20Few-shot%20Classification%0AAuthor%3A%20Jintao%20Rong%20and%20Hao%20Chen%20and%20Tianxiao%20Chen%20and%20Linlin%20Ou%20and%20Xinyi%20Yu%20and%20Yifan%20Liu%0AAbstract%3A%20%20%20Prompt%20learning%20has%20become%20a%20popular%20approach%20for%20adapting%20large%0Avision-language%20models%2C%20such%20as%20CLIP%2C%20to%20downstream%20tasks.%20Typically%2C%20prompt%0Alearning%20relies%20on%20a%20fixed%20prompt%20token%20or%20an%20input-conditional%20token%20to%20fit%20a%0Asmall%20amount%20of%20data%20under%20full%20supervision.%20While%20this%20paradigm%20can%20generalize%0Ato%20a%20certain%20range%20of%20unseen%20classes%2C%20it%20may%20struggle%20when%20domain%20gap%0Aincreases%2C%20such%20as%20in%20fine-grained%20classification%20and%20satellite%20image%0Asegmentation.%20To%20address%20this%20limitation%2C%20we%20propose%20Retrieval-enhanced%20Prompt%0Alearning%20%28RePrompt%29%2C%20which%20introduces%20retrieval%20mechanisms%20to%20cache%20the%0Aknowledge%20representations%20from%20downstream%20tasks.%20we%20first%20construct%20a%20retrieval%0Adatabase%20from%20training%20examples%2C%20or%20from%20external%20examples%20when%20available.%20We%0Athen%20integrate%20this%20retrieval-enhanced%20mechanism%20into%20various%20stages%20of%20a%0Asimple%20prompt%20learning%20baseline.%20By%20referencing%20similar%20samples%20in%20the%20training%0Aset%2C%20the%20enhanced%20model%20is%20better%20able%20to%20adapt%20to%20new%20tasks%20with%20few%20samples.%0AOur%20extensive%20experiments%20over%2015%20vision%20datasets%2C%20including%2011%20downstream%0Atasks%20with%20few-shot%20setting%20and%204%20domain%20generalization%20benchmarks%2C%20demonstrate%0Athat%20RePrompt%20achieves%20considerably%20improved%20performance.%20Our%20proposed%20approach%0Aprovides%20a%20promising%20solution%20to%20the%20challenges%20faced%20by%20prompt%20learning%20when%0Adomain%20gap%20increases.%20The%20code%20and%20models%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02243v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-Enhanced%2520Visual%2520Prompt%2520Learning%2520for%2520Few-shot%2520Classification%26entry.906535625%3DJintao%2520Rong%2520and%2520Hao%2520Chen%2520and%2520Tianxiao%2520Chen%2520and%2520Linlin%2520Ou%2520and%2520Xinyi%2520Yu%2520and%2520Yifan%2520Liu%26entry.1292438233%3D%2520%2520Prompt%2520learning%2520has%2520become%2520a%2520popular%2520approach%2520for%2520adapting%2520large%250Avision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520to%2520downstream%2520tasks.%2520Typically%252C%2520prompt%250Alearning%2520relies%2520on%2520a%2520fixed%2520prompt%2520token%2520or%2520an%2520input-conditional%2520token%2520to%2520fit%2520a%250Asmall%2520amount%2520of%2520data%2520under%2520full%2520supervision.%2520While%2520this%2520paradigm%2520can%2520generalize%250Ato%2520a%2520certain%2520range%2520of%2520unseen%2520classes%252C%2520it%2520may%2520struggle%2520when%2520domain%2520gap%250Aincreases%252C%2520such%2520as%2520in%2520fine-grained%2520classification%2520and%2520satellite%2520image%250Asegmentation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Retrieval-enhanced%2520Prompt%250Alearning%2520%2528RePrompt%2529%252C%2520which%2520introduces%2520retrieval%2520mechanisms%2520to%2520cache%2520the%250Aknowledge%2520representations%2520from%2520downstream%2520tasks.%2520we%2520first%2520construct%2520a%2520retrieval%250Adatabase%2520from%2520training%2520examples%252C%2520or%2520from%2520external%2520examples%2520when%2520available.%2520We%250Athen%2520integrate%2520this%2520retrieval-enhanced%2520mechanism%2520into%2520various%2520stages%2520of%2520a%250Asimple%2520prompt%2520learning%2520baseline.%2520By%2520referencing%2520similar%2520samples%2520in%2520the%2520training%250Aset%252C%2520the%2520enhanced%2520model%2520is%2520better%2520able%2520to%2520adapt%2520to%2520new%2520tasks%2520with%2520few%2520samples.%250AOur%2520extensive%2520experiments%2520over%252015%2520vision%2520datasets%252C%2520including%252011%2520downstream%250Atasks%2520with%2520few-shot%2520setting%2520and%25204%2520domain%2520generalization%2520benchmarks%252C%2520demonstrate%250Athat%2520RePrompt%2520achieves%2520considerably%2520improved%2520performance.%2520Our%2520proposed%2520approach%250Aprovides%2520a%2520promising%2520solution%2520to%2520the%2520challenges%2520faced%2520by%2520prompt%2520learning%2520when%250Adomain%2520gap%2520increases.%2520The%2520code%2520and%2520models%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02243v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Enhanced%20Visual%20Prompt%20Learning%20for%20Few-shot%20Classification&entry.906535625=Jintao%20Rong%20and%20Hao%20Chen%20and%20Tianxiao%20Chen%20and%20Linlin%20Ou%20and%20Xinyi%20Yu%20and%20Yifan%20Liu&entry.1292438233=%20%20Prompt%20learning%20has%20become%20a%20popular%20approach%20for%20adapting%20large%0Avision-language%20models%2C%20such%20as%20CLIP%2C%20to%20downstream%20tasks.%20Typically%2C%20prompt%0Alearning%20relies%20on%20a%20fixed%20prompt%20token%20or%20an%20input-conditional%20token%20to%20fit%20a%0Asmall%20amount%20of%20data%20under%20full%20supervision.%20While%20this%20paradigm%20can%20generalize%0Ato%20a%20certain%20range%20of%20unseen%20classes%2C%20it%20may%20struggle%20when%20domain%20gap%0Aincreases%2C%20such%20as%20in%20fine-grained%20classification%20and%20satellite%20image%0Asegmentation.%20To%20address%20this%20limitation%2C%20we%20propose%20Retrieval-enhanced%20Prompt%0Alearning%20%28RePrompt%29%2C%20which%20introduces%20retrieval%20mechanisms%20to%20cache%20the%0Aknowledge%20representations%20from%20downstream%20tasks.%20we%20first%20construct%20a%20retrieval%0Adatabase%20from%20training%20examples%2C%20or%20from%20external%20examples%20when%20available.%20We%0Athen%20integrate%20this%20retrieval-enhanced%20mechanism%20into%20various%20stages%20of%20a%0Asimple%20prompt%20learning%20baseline.%20By%20referencing%20similar%20samples%20in%20the%20training%0Aset%2C%20the%20enhanced%20model%20is%20better%20able%20to%20adapt%20to%20new%20tasks%20with%20few%20samples.%0AOur%20extensive%20experiments%20over%2015%20vision%20datasets%2C%20including%2011%20downstream%0Atasks%20with%20few-shot%20setting%20and%204%20domain%20generalization%20benchmarks%2C%20demonstrate%0Athat%20RePrompt%20achieves%20considerably%20improved%20performance.%20Our%20proposed%20approach%0Aprovides%20a%20promising%20solution%20to%20the%20challenges%20faced%20by%20prompt%20learning%20when%0Adomain%20gap%20increases.%20The%20code%20and%20models%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02243v2&entry.124074799=Read"},
{"title": "Cephalometric Landmark Detection across Ages with Prototypical Network", "author": "Han Wu and Chong Wang and Lanzhuju Mei and Tong Yang and Min Zhu and Dingggang Shen and Zhiming Cui", "abstract": "  Automated cephalometric landmark detection is crucial in real-world\northodontic diagnosis. Current studies mainly focus on only adult subjects,\nneglecting the clinically crucial scenario presented by adolescents whose\nlandmarks often exhibit significantly different appearances compared to adults.\nHence, an open question arises about how to develop a unified and effective\ndetection algorithm across various age groups, including adolescents and\nadults. In this paper, we propose CeLDA, the first work for Cephalometric\nLandmark Detection across Ages. Our method leverages a prototypical network for\nlandmark detection by comparing image features with landmark prototypes. To\ntackle the appearance discrepancy of landmarks between age groups, we design\nnew strategies for CeLDA to improve prototype alignment and obtain a holistic\nestimation of landmark prototypes from a large set of training images.\nMoreover, a novel prototype relation mining paradigm is introduced to exploit\nthe anatomical relations between the landmark prototypes. Extensive experiments\nvalidate the superiority of CeLDA in detecting cephalometric landmarks on both\nadult and adolescent subjects. To our knowledge, this is the first effort\ntoward developing a unified solution and dataset for cephalometric landmark\ndetection across age groups. Our code and dataset will be made public on\nhttps://github.com/ShanghaiTech-IMPACT/Cephalometric-Landmark-Detection-across-Ages-with-Prototypical-Network\n", "link": "http://arxiv.org/abs/2406.12577v1", "date": "2024-06-18", "relevancy": 2.5374, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5268}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5039}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cephalometric%20Landmark%20Detection%20across%20Ages%20with%20Prototypical%20Network&body=Title%3A%20Cephalometric%20Landmark%20Detection%20across%20Ages%20with%20Prototypical%20Network%0AAuthor%3A%20Han%20Wu%20and%20Chong%20Wang%20and%20Lanzhuju%20Mei%20and%20Tong%20Yang%20and%20Min%20Zhu%20and%20Dingggang%20Shen%20and%20Zhiming%20Cui%0AAbstract%3A%20%20%20Automated%20cephalometric%20landmark%20detection%20is%20crucial%20in%20real-world%0Aorthodontic%20diagnosis.%20Current%20studies%20mainly%20focus%20on%20only%20adult%20subjects%2C%0Aneglecting%20the%20clinically%20crucial%20scenario%20presented%20by%20adolescents%20whose%0Alandmarks%20often%20exhibit%20significantly%20different%20appearances%20compared%20to%20adults.%0AHence%2C%20an%20open%20question%20arises%20about%20how%20to%20develop%20a%20unified%20and%20effective%0Adetection%20algorithm%20across%20various%20age%20groups%2C%20including%20adolescents%20and%0Aadults.%20In%20this%20paper%2C%20we%20propose%20CeLDA%2C%20the%20first%20work%20for%20Cephalometric%0ALandmark%20Detection%20across%20Ages.%20Our%20method%20leverages%20a%20prototypical%20network%20for%0Alandmark%20detection%20by%20comparing%20image%20features%20with%20landmark%20prototypes.%20To%0Atackle%20the%20appearance%20discrepancy%20of%20landmarks%20between%20age%20groups%2C%20we%20design%0Anew%20strategies%20for%20CeLDA%20to%20improve%20prototype%20alignment%20and%20obtain%20a%20holistic%0Aestimation%20of%20landmark%20prototypes%20from%20a%20large%20set%20of%20training%20images.%0AMoreover%2C%20a%20novel%20prototype%20relation%20mining%20paradigm%20is%20introduced%20to%20exploit%0Athe%20anatomical%20relations%20between%20the%20landmark%20prototypes.%20Extensive%20experiments%0Avalidate%20the%20superiority%20of%20CeLDA%20in%20detecting%20cephalometric%20landmarks%20on%20both%0Aadult%20and%20adolescent%20subjects.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20effort%0Atoward%20developing%20a%20unified%20solution%20and%20dataset%20for%20cephalometric%20landmark%0Adetection%20across%20age%20groups.%20Our%20code%20and%20dataset%20will%20be%20made%20public%20on%0Ahttps%3A//github.com/ShanghaiTech-IMPACT/Cephalometric-Landmark-Detection-across-Ages-with-Prototypical-Network%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCephalometric%2520Landmark%2520Detection%2520across%2520Ages%2520with%2520Prototypical%2520Network%26entry.906535625%3DHan%2520Wu%2520and%2520Chong%2520Wang%2520and%2520Lanzhuju%2520Mei%2520and%2520Tong%2520Yang%2520and%2520Min%2520Zhu%2520and%2520Dingggang%2520Shen%2520and%2520Zhiming%2520Cui%26entry.1292438233%3D%2520%2520Automated%2520cephalometric%2520landmark%2520detection%2520is%2520crucial%2520in%2520real-world%250Aorthodontic%2520diagnosis.%2520Current%2520studies%2520mainly%2520focus%2520on%2520only%2520adult%2520subjects%252C%250Aneglecting%2520the%2520clinically%2520crucial%2520scenario%2520presented%2520by%2520adolescents%2520whose%250Alandmarks%2520often%2520exhibit%2520significantly%2520different%2520appearances%2520compared%2520to%2520adults.%250AHence%252C%2520an%2520open%2520question%2520arises%2520about%2520how%2520to%2520develop%2520a%2520unified%2520and%2520effective%250Adetection%2520algorithm%2520across%2520various%2520age%2520groups%252C%2520including%2520adolescents%2520and%250Aadults.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CeLDA%252C%2520the%2520first%2520work%2520for%2520Cephalometric%250ALandmark%2520Detection%2520across%2520Ages.%2520Our%2520method%2520leverages%2520a%2520prototypical%2520network%2520for%250Alandmark%2520detection%2520by%2520comparing%2520image%2520features%2520with%2520landmark%2520prototypes.%2520To%250Atackle%2520the%2520appearance%2520discrepancy%2520of%2520landmarks%2520between%2520age%2520groups%252C%2520we%2520design%250Anew%2520strategies%2520for%2520CeLDA%2520to%2520improve%2520prototype%2520alignment%2520and%2520obtain%2520a%2520holistic%250Aestimation%2520of%2520landmark%2520prototypes%2520from%2520a%2520large%2520set%2520of%2520training%2520images.%250AMoreover%252C%2520a%2520novel%2520prototype%2520relation%2520mining%2520paradigm%2520is%2520introduced%2520to%2520exploit%250Athe%2520anatomical%2520relations%2520between%2520the%2520landmark%2520prototypes.%2520Extensive%2520experiments%250Avalidate%2520the%2520superiority%2520of%2520CeLDA%2520in%2520detecting%2520cephalometric%2520landmarks%2520on%2520both%250Aadult%2520and%2520adolescent%2520subjects.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520effort%250Atoward%2520developing%2520a%2520unified%2520solution%2520and%2520dataset%2520for%2520cephalometric%2520landmark%250Adetection%2520across%2520age%2520groups.%2520Our%2520code%2520and%2520dataset%2520will%2520be%2520made%2520public%2520on%250Ahttps%253A//github.com/ShanghaiTech-IMPACT/Cephalometric-Landmark-Detection-across-Ages-with-Prototypical-Network%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cephalometric%20Landmark%20Detection%20across%20Ages%20with%20Prototypical%20Network&entry.906535625=Han%20Wu%20and%20Chong%20Wang%20and%20Lanzhuju%20Mei%20and%20Tong%20Yang%20and%20Min%20Zhu%20and%20Dingggang%20Shen%20and%20Zhiming%20Cui&entry.1292438233=%20%20Automated%20cephalometric%20landmark%20detection%20is%20crucial%20in%20real-world%0Aorthodontic%20diagnosis.%20Current%20studies%20mainly%20focus%20on%20only%20adult%20subjects%2C%0Aneglecting%20the%20clinically%20crucial%20scenario%20presented%20by%20adolescents%20whose%0Alandmarks%20often%20exhibit%20significantly%20different%20appearances%20compared%20to%20adults.%0AHence%2C%20an%20open%20question%20arises%20about%20how%20to%20develop%20a%20unified%20and%20effective%0Adetection%20algorithm%20across%20various%20age%20groups%2C%20including%20adolescents%20and%0Aadults.%20In%20this%20paper%2C%20we%20propose%20CeLDA%2C%20the%20first%20work%20for%20Cephalometric%0ALandmark%20Detection%20across%20Ages.%20Our%20method%20leverages%20a%20prototypical%20network%20for%0Alandmark%20detection%20by%20comparing%20image%20features%20with%20landmark%20prototypes.%20To%0Atackle%20the%20appearance%20discrepancy%20of%20landmarks%20between%20age%20groups%2C%20we%20design%0Anew%20strategies%20for%20CeLDA%20to%20improve%20prototype%20alignment%20and%20obtain%20a%20holistic%0Aestimation%20of%20landmark%20prototypes%20from%20a%20large%20set%20of%20training%20images.%0AMoreover%2C%20a%20novel%20prototype%20relation%20mining%20paradigm%20is%20introduced%20to%20exploit%0Athe%20anatomical%20relations%20between%20the%20landmark%20prototypes.%20Extensive%20experiments%0Avalidate%20the%20superiority%20of%20CeLDA%20in%20detecting%20cephalometric%20landmarks%20on%20both%0Aadult%20and%20adolescent%20subjects.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20effort%0Atoward%20developing%20a%20unified%20solution%20and%20dataset%20for%20cephalometric%20landmark%0Adetection%20across%20age%20groups.%20Our%20code%20and%20dataset%20will%20be%20made%20public%20on%0Ahttps%3A//github.com/ShanghaiTech-IMPACT/Cephalometric-Landmark-Detection-across-Ages-with-Prototypical-Network%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12577v1&entry.124074799=Read"},
{"title": "The Heterophilic Snowflake Hypothesis: Training and Empowering GNNs for\n  Heterophilic Graphs", "author": "Kun Wang and Guibin Zhang and Xinnan Zhang and Junfeng Fang and Xun Wu and Guohao Li and Shirui Pan and Wei Huang and Yuxuan Liang", "abstract": "  Graph Neural Networks (GNNs) have become pivotal tools for a range of\ngraph-based learning tasks. Notably, most current GNN architectures operate\nunder the assumption of homophily, whether explicitly or implicitly. While this\nunderlying assumption is frequently adopted, it is not universally applicable,\nwhich can result in potential shortcomings in learning effectiveness. In this\npaper, \\textbf{for the first time}, we transfer the prevailing concept of ``one\nnode one receptive field\" to the heterophilic graph. By constructing a proxy\nlabel predictor, we enable each node to possess a latent prediction\ndistribution, which assists connected nodes in determining whether they should\naggregate their associated neighbors. Ultimately, every node can have its own\nunique aggregation hop and pattern, much like each snowflake is unique and\npossesses its own characteristics. Based on observations, we innovatively\nintroduce the Heterophily Snowflake Hypothesis and provide an effective\nsolution to guide and facilitate research on heterophilic graphs and beyond. We\nconduct comprehensive experiments including (1) main results on 10 graphs with\nvarying heterophily ratios across 10 backbones; (2) scalability on various deep\nGNN backbones (SGC, JKNet, etc.) across various large number of layers\n(2,4,6,8,16,32 layers); (3) comparison with conventional snowflake hypothesis;\n(4) efficiency comparison with existing graph pruning algorithms. Our\nobservations show that our framework acts as a versatile operator for diverse\ntasks. It can be integrated into various GNN frameworks, boosting performance\nin-depth and offering an explainable approach to choosing the optimal network\ndepth. The source code is available at\n\\url{https://github.com/bingreeky/HeteroSnoH}.\n", "link": "http://arxiv.org/abs/2406.12539v1", "date": "2024-06-18", "relevancy": 2.5312, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5384}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4935}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Heterophilic%20Snowflake%20Hypothesis%3A%20Training%20and%20Empowering%20GNNs%20for%0A%20%20Heterophilic%20Graphs&body=Title%3A%20The%20Heterophilic%20Snowflake%20Hypothesis%3A%20Training%20and%20Empowering%20GNNs%20for%0A%20%20Heterophilic%20Graphs%0AAuthor%3A%20Kun%20Wang%20and%20Guibin%20Zhang%20and%20Xinnan%20Zhang%20and%20Junfeng%20Fang%20and%20Xun%20Wu%20and%20Guohao%20Li%20and%20Shirui%20Pan%20and%20Wei%20Huang%20and%20Yuxuan%20Liang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20pivotal%20tools%20for%20a%20range%20of%0Agraph-based%20learning%20tasks.%20Notably%2C%20most%20current%20GNN%20architectures%20operate%0Aunder%20the%20assumption%20of%20homophily%2C%20whether%20explicitly%20or%20implicitly.%20While%20this%0Aunderlying%20assumption%20is%20frequently%20adopted%2C%20it%20is%20not%20universally%20applicable%2C%0Awhich%20can%20result%20in%20potential%20shortcomings%20in%20learning%20effectiveness.%20In%20this%0Apaper%2C%20%5Ctextbf%7Bfor%20the%20first%20time%7D%2C%20we%20transfer%20the%20prevailing%20concept%20of%20%60%60one%0Anode%20one%20receptive%20field%22%20to%20the%20heterophilic%20graph.%20By%20constructing%20a%20proxy%0Alabel%20predictor%2C%20we%20enable%20each%20node%20to%20possess%20a%20latent%20prediction%0Adistribution%2C%20which%20assists%20connected%20nodes%20in%20determining%20whether%20they%20should%0Aaggregate%20their%20associated%20neighbors.%20Ultimately%2C%20every%20node%20can%20have%20its%20own%0Aunique%20aggregation%20hop%20and%20pattern%2C%20much%20like%20each%20snowflake%20is%20unique%20and%0Apossesses%20its%20own%20characteristics.%20Based%20on%20observations%2C%20we%20innovatively%0Aintroduce%20the%20Heterophily%20Snowflake%20Hypothesis%20and%20provide%20an%20effective%0Asolution%20to%20guide%20and%20facilitate%20research%20on%20heterophilic%20graphs%20and%20beyond.%20We%0Aconduct%20comprehensive%20experiments%20including%20%281%29%20main%20results%20on%2010%20graphs%20with%0Avarying%20heterophily%20ratios%20across%2010%20backbones%3B%20%282%29%20scalability%20on%20various%20deep%0AGNN%20backbones%20%28SGC%2C%20JKNet%2C%20etc.%29%20across%20various%20large%20number%20of%20layers%0A%282%2C4%2C6%2C8%2C16%2C32%20layers%29%3B%20%283%29%20comparison%20with%20conventional%20snowflake%20hypothesis%3B%0A%284%29%20efficiency%20comparison%20with%20existing%20graph%20pruning%20algorithms.%20Our%0Aobservations%20show%20that%20our%20framework%20acts%20as%20a%20versatile%20operator%20for%20diverse%0Atasks.%20It%20can%20be%20integrated%20into%20various%20GNN%20frameworks%2C%20boosting%20performance%0Ain-depth%20and%20offering%20an%20explainable%20approach%20to%20choosing%20the%20optimal%20network%0Adepth.%20The%20source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/bingreeky/HeteroSnoH%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Heterophilic%2520Snowflake%2520Hypothesis%253A%2520Training%2520and%2520Empowering%2520GNNs%2520for%250A%2520%2520Heterophilic%2520Graphs%26entry.906535625%3DKun%2520Wang%2520and%2520Guibin%2520Zhang%2520and%2520Xinnan%2520Zhang%2520and%2520Junfeng%2520Fang%2520and%2520Xun%2520Wu%2520and%2520Guohao%2520Li%2520and%2520Shirui%2520Pan%2520and%2520Wei%2520Huang%2520and%2520Yuxuan%2520Liang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520pivotal%2520tools%2520for%2520a%2520range%2520of%250Agraph-based%2520learning%2520tasks.%2520Notably%252C%2520most%2520current%2520GNN%2520architectures%2520operate%250Aunder%2520the%2520assumption%2520of%2520homophily%252C%2520whether%2520explicitly%2520or%2520implicitly.%2520While%2520this%250Aunderlying%2520assumption%2520is%2520frequently%2520adopted%252C%2520it%2520is%2520not%2520universally%2520applicable%252C%250Awhich%2520can%2520result%2520in%2520potential%2520shortcomings%2520in%2520learning%2520effectiveness.%2520In%2520this%250Apaper%252C%2520%255Ctextbf%257Bfor%2520the%2520first%2520time%257D%252C%2520we%2520transfer%2520the%2520prevailing%2520concept%2520of%2520%2560%2560one%250Anode%2520one%2520receptive%2520field%2522%2520to%2520the%2520heterophilic%2520graph.%2520By%2520constructing%2520a%2520proxy%250Alabel%2520predictor%252C%2520we%2520enable%2520each%2520node%2520to%2520possess%2520a%2520latent%2520prediction%250Adistribution%252C%2520which%2520assists%2520connected%2520nodes%2520in%2520determining%2520whether%2520they%2520should%250Aaggregate%2520their%2520associated%2520neighbors.%2520Ultimately%252C%2520every%2520node%2520can%2520have%2520its%2520own%250Aunique%2520aggregation%2520hop%2520and%2520pattern%252C%2520much%2520like%2520each%2520snowflake%2520is%2520unique%2520and%250Apossesses%2520its%2520own%2520characteristics.%2520Based%2520on%2520observations%252C%2520we%2520innovatively%250Aintroduce%2520the%2520Heterophily%2520Snowflake%2520Hypothesis%2520and%2520provide%2520an%2520effective%250Asolution%2520to%2520guide%2520and%2520facilitate%2520research%2520on%2520heterophilic%2520graphs%2520and%2520beyond.%2520We%250Aconduct%2520comprehensive%2520experiments%2520including%2520%25281%2529%2520main%2520results%2520on%252010%2520graphs%2520with%250Avarying%2520heterophily%2520ratios%2520across%252010%2520backbones%253B%2520%25282%2529%2520scalability%2520on%2520various%2520deep%250AGNN%2520backbones%2520%2528SGC%252C%2520JKNet%252C%2520etc.%2529%2520across%2520various%2520large%2520number%2520of%2520layers%250A%25282%252C4%252C6%252C8%252C16%252C32%2520layers%2529%253B%2520%25283%2529%2520comparison%2520with%2520conventional%2520snowflake%2520hypothesis%253B%250A%25284%2529%2520efficiency%2520comparison%2520with%2520existing%2520graph%2520pruning%2520algorithms.%2520Our%250Aobservations%2520show%2520that%2520our%2520framework%2520acts%2520as%2520a%2520versatile%2520operator%2520for%2520diverse%250Atasks.%2520It%2520can%2520be%2520integrated%2520into%2520various%2520GNN%2520frameworks%252C%2520boosting%2520performance%250Ain-depth%2520and%2520offering%2520an%2520explainable%2520approach%2520to%2520choosing%2520the%2520optimal%2520network%250Adepth.%2520The%2520source%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/bingreeky/HeteroSnoH%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Heterophilic%20Snowflake%20Hypothesis%3A%20Training%20and%20Empowering%20GNNs%20for%0A%20%20Heterophilic%20Graphs&entry.906535625=Kun%20Wang%20and%20Guibin%20Zhang%20and%20Xinnan%20Zhang%20and%20Junfeng%20Fang%20and%20Xun%20Wu%20and%20Guohao%20Li%20and%20Shirui%20Pan%20and%20Wei%20Huang%20and%20Yuxuan%20Liang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20pivotal%20tools%20for%20a%20range%20of%0Agraph-based%20learning%20tasks.%20Notably%2C%20most%20current%20GNN%20architectures%20operate%0Aunder%20the%20assumption%20of%20homophily%2C%20whether%20explicitly%20or%20implicitly.%20While%20this%0Aunderlying%20assumption%20is%20frequently%20adopted%2C%20it%20is%20not%20universally%20applicable%2C%0Awhich%20can%20result%20in%20potential%20shortcomings%20in%20learning%20effectiveness.%20In%20this%0Apaper%2C%20%5Ctextbf%7Bfor%20the%20first%20time%7D%2C%20we%20transfer%20the%20prevailing%20concept%20of%20%60%60one%0Anode%20one%20receptive%20field%22%20to%20the%20heterophilic%20graph.%20By%20constructing%20a%20proxy%0Alabel%20predictor%2C%20we%20enable%20each%20node%20to%20possess%20a%20latent%20prediction%0Adistribution%2C%20which%20assists%20connected%20nodes%20in%20determining%20whether%20they%20should%0Aaggregate%20their%20associated%20neighbors.%20Ultimately%2C%20every%20node%20can%20have%20its%20own%0Aunique%20aggregation%20hop%20and%20pattern%2C%20much%20like%20each%20snowflake%20is%20unique%20and%0Apossesses%20its%20own%20characteristics.%20Based%20on%20observations%2C%20we%20innovatively%0Aintroduce%20the%20Heterophily%20Snowflake%20Hypothesis%20and%20provide%20an%20effective%0Asolution%20to%20guide%20and%20facilitate%20research%20on%20heterophilic%20graphs%20and%20beyond.%20We%0Aconduct%20comprehensive%20experiments%20including%20%281%29%20main%20results%20on%2010%20graphs%20with%0Avarying%20heterophily%20ratios%20across%2010%20backbones%3B%20%282%29%20scalability%20on%20various%20deep%0AGNN%20backbones%20%28SGC%2C%20JKNet%2C%20etc.%29%20across%20various%20large%20number%20of%20layers%0A%282%2C4%2C6%2C8%2C16%2C32%20layers%29%3B%20%283%29%20comparison%20with%20conventional%20snowflake%20hypothesis%3B%0A%284%29%20efficiency%20comparison%20with%20existing%20graph%20pruning%20algorithms.%20Our%0Aobservations%20show%20that%20our%20framework%20acts%20as%20a%20versatile%20operator%20for%20diverse%0Atasks.%20It%20can%20be%20integrated%20into%20various%20GNN%20frameworks%2C%20boosting%20performance%0Ain-depth%20and%20offering%20an%20explainable%20approach%20to%20choosing%20the%20optimal%20network%0Adepth.%20The%20source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/bingreeky/HeteroSnoH%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12539v1&entry.124074799=Read"},
{"title": "Online Anchor-based Training for Image Classification Tasks", "author": "Maria Tzelepi and Vasileios Mezaris", "abstract": "  In this paper, we aim to improve the performance of a deep learning model\ntowards image classification tasks, proposing a novel anchor-based training\nmethodology, named \\textit{Online Anchor-based Training} (OAT). The OAT method,\nguided by the insights provided in the anchor-based object detection\nmethodologies, instead of learning directly the class labels, proposes to train\na model to learn percentage changes of the class labels with respect to defined\nanchors. We define as anchors the batch centers at the output of the model.\nThen, during the test phase, the predictions are converted back to the original\nclass label space, and the performance is evaluated. The effectiveness of the\nOAT method is validated on four datasets.\n", "link": "http://arxiv.org/abs/2406.12662v1", "date": "2024-06-18", "relevancy": 2.5297, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5293}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4985}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Anchor-based%20Training%20for%20Image%20Classification%20Tasks&body=Title%3A%20Online%20Anchor-based%20Training%20for%20Image%20Classification%20Tasks%0AAuthor%3A%20Maria%20Tzelepi%20and%20Vasileios%20Mezaris%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20improve%20the%20performance%20of%20a%20deep%20learning%20model%0Atowards%20image%20classification%20tasks%2C%20proposing%20a%20novel%20anchor-based%20training%0Amethodology%2C%20named%20%5Ctextit%7BOnline%20Anchor-based%20Training%7D%20%28OAT%29.%20The%20OAT%20method%2C%0Aguided%20by%20the%20insights%20provided%20in%20the%20anchor-based%20object%20detection%0Amethodologies%2C%20instead%20of%20learning%20directly%20the%20class%20labels%2C%20proposes%20to%20train%0Aa%20model%20to%20learn%20percentage%20changes%20of%20the%20class%20labels%20with%20respect%20to%20defined%0Aanchors.%20We%20define%20as%20anchors%20the%20batch%20centers%20at%20the%20output%20of%20the%20model.%0AThen%2C%20during%20the%20test%20phase%2C%20the%20predictions%20are%20converted%20back%20to%20the%20original%0Aclass%20label%20space%2C%20and%20the%20performance%20is%20evaluated.%20The%20effectiveness%20of%20the%0AOAT%20method%20is%20validated%20on%20four%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Anchor-based%2520Training%2520for%2520Image%2520Classification%2520Tasks%26entry.906535625%3DMaria%2520Tzelepi%2520and%2520Vasileios%2520Mezaris%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520improve%2520the%2520performance%2520of%2520a%2520deep%2520learning%2520model%250Atowards%2520image%2520classification%2520tasks%252C%2520proposing%2520a%2520novel%2520anchor-based%2520training%250Amethodology%252C%2520named%2520%255Ctextit%257BOnline%2520Anchor-based%2520Training%257D%2520%2528OAT%2529.%2520The%2520OAT%2520method%252C%250Aguided%2520by%2520the%2520insights%2520provided%2520in%2520the%2520anchor-based%2520object%2520detection%250Amethodologies%252C%2520instead%2520of%2520learning%2520directly%2520the%2520class%2520labels%252C%2520proposes%2520to%2520train%250Aa%2520model%2520to%2520learn%2520percentage%2520changes%2520of%2520the%2520class%2520labels%2520with%2520respect%2520to%2520defined%250Aanchors.%2520We%2520define%2520as%2520anchors%2520the%2520batch%2520centers%2520at%2520the%2520output%2520of%2520the%2520model.%250AThen%252C%2520during%2520the%2520test%2520phase%252C%2520the%2520predictions%2520are%2520converted%2520back%2520to%2520the%2520original%250Aclass%2520label%2520space%252C%2520and%2520the%2520performance%2520is%2520evaluated.%2520The%2520effectiveness%2520of%2520the%250AOAT%2520method%2520is%2520validated%2520on%2520four%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Anchor-based%20Training%20for%20Image%20Classification%20Tasks&entry.906535625=Maria%20Tzelepi%20and%20Vasileios%20Mezaris&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20improve%20the%20performance%20of%20a%20deep%20learning%20model%0Atowards%20image%20classification%20tasks%2C%20proposing%20a%20novel%20anchor-based%20training%0Amethodology%2C%20named%20%5Ctextit%7BOnline%20Anchor-based%20Training%7D%20%28OAT%29.%20The%20OAT%20method%2C%0Aguided%20by%20the%20insights%20provided%20in%20the%20anchor-based%20object%20detection%0Amethodologies%2C%20instead%20of%20learning%20directly%20the%20class%20labels%2C%20proposes%20to%20train%0Aa%20model%20to%20learn%20percentage%20changes%20of%20the%20class%20labels%20with%20respect%20to%20defined%0Aanchors.%20We%20define%20as%20anchors%20the%20batch%20centers%20at%20the%20output%20of%20the%20model.%0AThen%2C%20during%20the%20test%20phase%2C%20the%20predictions%20are%20converted%20back%20to%20the%20original%0Aclass%20label%20space%2C%20and%20the%20performance%20is%20evaluated.%20The%20effectiveness%20of%20the%0AOAT%20method%20is%20validated%20on%20four%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12662v1&entry.124074799=Read"},
{"title": "Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective\n  Distillation and Unlabeled Data Augmentation", "author": "Ning-Hsu Wang and Yu-Lun Liu", "abstract": "  Accurately estimating depth in 360-degree imagery is crucial for virtual\nreality, autonomous navigation, and immersive media applications. Existing\ndepth estimation methods designed for perspective-view imagery fail when\napplied to 360-degree images due to different camera projections and\ndistortions, whereas 360-degree methods perform inferior due to the lack of\nlabeled data pairs. We propose a new depth estimation framework that utilizes\nunlabeled 360-degree data effectively. Our approach uses state-of-the-art\nperspective depth estimation models as teacher models to generate pseudo labels\nthrough a six-face cube projection technique, enabling efficient labeling of\ndepth in 360-degree images. This method leverages the increasing availability\nof large datasets. Our approach includes two main stages: offline mask\ngeneration for invalid regions and an online semi-supervised joint training\nregime. We tested our approach on benchmark datasets such as Matterport3D and\nStanford2D3D, showing significant improvements in depth estimation accuracy,\nparticularly in zero-shot scenarios. Our proposed training pipeline can enhance\nany 360 monocular depth estimator and demonstrates effective knowledge transfer\nacross different camera projections and data types. See our project page for\nresults: https://albert100121.github.io/Depth-Anywhere/\n", "link": "http://arxiv.org/abs/2406.12849v1", "date": "2024-06-18", "relevancy": 2.4715, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6349}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6145}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Anywhere%3A%20Enhancing%20360%20Monocular%20Depth%20Estimation%20via%20Perspective%0A%20%20Distillation%20and%20Unlabeled%20Data%20Augmentation&body=Title%3A%20Depth%20Anywhere%3A%20Enhancing%20360%20Monocular%20Depth%20Estimation%20via%20Perspective%0A%20%20Distillation%20and%20Unlabeled%20Data%20Augmentation%0AAuthor%3A%20Ning-Hsu%20Wang%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20Accurately%20estimating%20depth%20in%20360-degree%20imagery%20is%20crucial%20for%20virtual%0Areality%2C%20autonomous%20navigation%2C%20and%20immersive%20media%20applications.%20Existing%0Adepth%20estimation%20methods%20designed%20for%20perspective-view%20imagery%20fail%20when%0Aapplied%20to%20360-degree%20images%20due%20to%20different%20camera%20projections%20and%0Adistortions%2C%20whereas%20360-degree%20methods%20perform%20inferior%20due%20to%20the%20lack%20of%0Alabeled%20data%20pairs.%20We%20propose%20a%20new%20depth%20estimation%20framework%20that%20utilizes%0Aunlabeled%20360-degree%20data%20effectively.%20Our%20approach%20uses%20state-of-the-art%0Aperspective%20depth%20estimation%20models%20as%20teacher%20models%20to%20generate%20pseudo%20labels%0Athrough%20a%20six-face%20cube%20projection%20technique%2C%20enabling%20efficient%20labeling%20of%0Adepth%20in%20360-degree%20images.%20This%20method%20leverages%20the%20increasing%20availability%0Aof%20large%20datasets.%20Our%20approach%20includes%20two%20main%20stages%3A%20offline%20mask%0Ageneration%20for%20invalid%20regions%20and%20an%20online%20semi-supervised%20joint%20training%0Aregime.%20We%20tested%20our%20approach%20on%20benchmark%20datasets%20such%20as%20Matterport3D%20and%0AStanford2D3D%2C%20showing%20significant%20improvements%20in%20depth%20estimation%20accuracy%2C%0Aparticularly%20in%20zero-shot%20scenarios.%20Our%20proposed%20training%20pipeline%20can%20enhance%0Aany%20360%20monocular%20depth%20estimator%20and%20demonstrates%20effective%20knowledge%20transfer%0Aacross%20different%20camera%20projections%20and%20data%20types.%20See%20our%20project%20page%20for%0Aresults%3A%20https%3A//albert100121.github.io/Depth-Anywhere/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Anywhere%253A%2520Enhancing%2520360%2520Monocular%2520Depth%2520Estimation%2520via%2520Perspective%250A%2520%2520Distillation%2520and%2520Unlabeled%2520Data%2520Augmentation%26entry.906535625%3DNing-Hsu%2520Wang%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520Accurately%2520estimating%2520depth%2520in%2520360-degree%2520imagery%2520is%2520crucial%2520for%2520virtual%250Areality%252C%2520autonomous%2520navigation%252C%2520and%2520immersive%2520media%2520applications.%2520Existing%250Adepth%2520estimation%2520methods%2520designed%2520for%2520perspective-view%2520imagery%2520fail%2520when%250Aapplied%2520to%2520360-degree%2520images%2520due%2520to%2520different%2520camera%2520projections%2520and%250Adistortions%252C%2520whereas%2520360-degree%2520methods%2520perform%2520inferior%2520due%2520to%2520the%2520lack%2520of%250Alabeled%2520data%2520pairs.%2520We%2520propose%2520a%2520new%2520depth%2520estimation%2520framework%2520that%2520utilizes%250Aunlabeled%2520360-degree%2520data%2520effectively.%2520Our%2520approach%2520uses%2520state-of-the-art%250Aperspective%2520depth%2520estimation%2520models%2520as%2520teacher%2520models%2520to%2520generate%2520pseudo%2520labels%250Athrough%2520a%2520six-face%2520cube%2520projection%2520technique%252C%2520enabling%2520efficient%2520labeling%2520of%250Adepth%2520in%2520360-degree%2520images.%2520This%2520method%2520leverages%2520the%2520increasing%2520availability%250Aof%2520large%2520datasets.%2520Our%2520approach%2520includes%2520two%2520main%2520stages%253A%2520offline%2520mask%250Ageneration%2520for%2520invalid%2520regions%2520and%2520an%2520online%2520semi-supervised%2520joint%2520training%250Aregime.%2520We%2520tested%2520our%2520approach%2520on%2520benchmark%2520datasets%2520such%2520as%2520Matterport3D%2520and%250AStanford2D3D%252C%2520showing%2520significant%2520improvements%2520in%2520depth%2520estimation%2520accuracy%252C%250Aparticularly%2520in%2520zero-shot%2520scenarios.%2520Our%2520proposed%2520training%2520pipeline%2520can%2520enhance%250Aany%2520360%2520monocular%2520depth%2520estimator%2520and%2520demonstrates%2520effective%2520knowledge%2520transfer%250Aacross%2520different%2520camera%2520projections%2520and%2520data%2520types.%2520See%2520our%2520project%2520page%2520for%250Aresults%253A%2520https%253A//albert100121.github.io/Depth-Anywhere/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Anywhere%3A%20Enhancing%20360%20Monocular%20Depth%20Estimation%20via%20Perspective%0A%20%20Distillation%20and%20Unlabeled%20Data%20Augmentation&entry.906535625=Ning-Hsu%20Wang%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20Accurately%20estimating%20depth%20in%20360-degree%20imagery%20is%20crucial%20for%20virtual%0Areality%2C%20autonomous%20navigation%2C%20and%20immersive%20media%20applications.%20Existing%0Adepth%20estimation%20methods%20designed%20for%20perspective-view%20imagery%20fail%20when%0Aapplied%20to%20360-degree%20images%20due%20to%20different%20camera%20projections%20and%0Adistortions%2C%20whereas%20360-degree%20methods%20perform%20inferior%20due%20to%20the%20lack%20of%0Alabeled%20data%20pairs.%20We%20propose%20a%20new%20depth%20estimation%20framework%20that%20utilizes%0Aunlabeled%20360-degree%20data%20effectively.%20Our%20approach%20uses%20state-of-the-art%0Aperspective%20depth%20estimation%20models%20as%20teacher%20models%20to%20generate%20pseudo%20labels%0Athrough%20a%20six-face%20cube%20projection%20technique%2C%20enabling%20efficient%20labeling%20of%0Adepth%20in%20360-degree%20images.%20This%20method%20leverages%20the%20increasing%20availability%0Aof%20large%20datasets.%20Our%20approach%20includes%20two%20main%20stages%3A%20offline%20mask%0Ageneration%20for%20invalid%20regions%20and%20an%20online%20semi-supervised%20joint%20training%0Aregime.%20We%20tested%20our%20approach%20on%20benchmark%20datasets%20such%20as%20Matterport3D%20and%0AStanford2D3D%2C%20showing%20significant%20improvements%20in%20depth%20estimation%20accuracy%2C%0Aparticularly%20in%20zero-shot%20scenarios.%20Our%20proposed%20training%20pipeline%20can%20enhance%0Aany%20360%20monocular%20depth%20estimator%20and%20demonstrates%20effective%20knowledge%20transfer%0Aacross%20different%20camera%20projections%20and%20data%20types.%20See%20our%20project%20page%20for%0Aresults%3A%20https%3A//albert100121.github.io/Depth-Anywhere/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12849v1&entry.124074799=Read"},
{"title": "Scalable Rule Lists Learning with Sampling", "author": "Leonardo Pellegrina and Fabio Vandin", "abstract": "  Learning interpretable models has become a major focus of machine learning\nresearch, given the increasing prominence of machine learning in socially\nimportant decision-making. Among interpretable models, rule lists are among the\nbest-known and easily interpretable ones. However, finding optimal rule lists\nis computationally challenging, and current approaches are impractical for\nlarge datasets.\n  We present a novel and scalable approach to learn nearly optimal rule lists\nfrom large datasets. Our algorithm uses sampling to efficiently obtain an\napproximation of the optimal rule list with rigorous guarantees on the quality\nof the approximation. In particular, our algorithm guarantees to find a rule\nlist with accuracy very close to the optimal rule list when a rule list with\nhigh accuracy exists. Our algorithm builds on the VC-dimension of rule lists,\nfor which we prove novel upper and lower bounds. Our experimental evaluation on\nlarge datasets shows that our algorithm identifies nearly optimal rule lists\nwith a speed-up up to two orders of magnitude over state-of-the-art exact\napproaches. Moreover, our algorithm is as fast as, and sometimes faster than,\nrecent heuristic approaches, while reporting higher quality rule lists. In\naddition, the rules reported by our algorithm are more similar to the rules in\nthe optimal rule list than the rules from heuristic approaches.\n", "link": "http://arxiv.org/abs/2406.12803v1", "date": "2024-06-18", "relevancy": 2.4689, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5247}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Rule%20Lists%20Learning%20with%20Sampling&body=Title%3A%20Scalable%20Rule%20Lists%20Learning%20with%20Sampling%0AAuthor%3A%20Leonardo%20Pellegrina%20and%20Fabio%20Vandin%0AAbstract%3A%20%20%20Learning%20interpretable%20models%20has%20become%20a%20major%20focus%20of%20machine%20learning%0Aresearch%2C%20given%20the%20increasing%20prominence%20of%20machine%20learning%20in%20socially%0Aimportant%20decision-making.%20Among%20interpretable%20models%2C%20rule%20lists%20are%20among%20the%0Abest-known%20and%20easily%20interpretable%20ones.%20However%2C%20finding%20optimal%20rule%20lists%0Ais%20computationally%20challenging%2C%20and%20current%20approaches%20are%20impractical%20for%0Alarge%20datasets.%0A%20%20We%20present%20a%20novel%20and%20scalable%20approach%20to%20learn%20nearly%20optimal%20rule%20lists%0Afrom%20large%20datasets.%20Our%20algorithm%20uses%20sampling%20to%20efficiently%20obtain%20an%0Aapproximation%20of%20the%20optimal%20rule%20list%20with%20rigorous%20guarantees%20on%20the%20quality%0Aof%20the%20approximation.%20In%20particular%2C%20our%20algorithm%20guarantees%20to%20find%20a%20rule%0Alist%20with%20accuracy%20very%20close%20to%20the%20optimal%20rule%20list%20when%20a%20rule%20list%20with%0Ahigh%20accuracy%20exists.%20Our%20algorithm%20builds%20on%20the%20VC-dimension%20of%20rule%20lists%2C%0Afor%20which%20we%20prove%20novel%20upper%20and%20lower%20bounds.%20Our%20experimental%20evaluation%20on%0Alarge%20datasets%20shows%20that%20our%20algorithm%20identifies%20nearly%20optimal%20rule%20lists%0Awith%20a%20speed-up%20up%20to%20two%20orders%20of%20magnitude%20over%20state-of-the-art%20exact%0Aapproaches.%20Moreover%2C%20our%20algorithm%20is%20as%20fast%20as%2C%20and%20sometimes%20faster%20than%2C%0Arecent%20heuristic%20approaches%2C%20while%20reporting%20higher%20quality%20rule%20lists.%20In%0Aaddition%2C%20the%20rules%20reported%20by%20our%20algorithm%20are%20more%20similar%20to%20the%20rules%20in%0Athe%20optimal%20rule%20list%20than%20the%20rules%20from%20heuristic%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Rule%2520Lists%2520Learning%2520with%2520Sampling%26entry.906535625%3DLeonardo%2520Pellegrina%2520and%2520Fabio%2520Vandin%26entry.1292438233%3D%2520%2520Learning%2520interpretable%2520models%2520has%2520become%2520a%2520major%2520focus%2520of%2520machine%2520learning%250Aresearch%252C%2520given%2520the%2520increasing%2520prominence%2520of%2520machine%2520learning%2520in%2520socially%250Aimportant%2520decision-making.%2520Among%2520interpretable%2520models%252C%2520rule%2520lists%2520are%2520among%2520the%250Abest-known%2520and%2520easily%2520interpretable%2520ones.%2520However%252C%2520finding%2520optimal%2520rule%2520lists%250Ais%2520computationally%2520challenging%252C%2520and%2520current%2520approaches%2520are%2520impractical%2520for%250Alarge%2520datasets.%250A%2520%2520We%2520present%2520a%2520novel%2520and%2520scalable%2520approach%2520to%2520learn%2520nearly%2520optimal%2520rule%2520lists%250Afrom%2520large%2520datasets.%2520Our%2520algorithm%2520uses%2520sampling%2520to%2520efficiently%2520obtain%2520an%250Aapproximation%2520of%2520the%2520optimal%2520rule%2520list%2520with%2520rigorous%2520guarantees%2520on%2520the%2520quality%250Aof%2520the%2520approximation.%2520In%2520particular%252C%2520our%2520algorithm%2520guarantees%2520to%2520find%2520a%2520rule%250Alist%2520with%2520accuracy%2520very%2520close%2520to%2520the%2520optimal%2520rule%2520list%2520when%2520a%2520rule%2520list%2520with%250Ahigh%2520accuracy%2520exists.%2520Our%2520algorithm%2520builds%2520on%2520the%2520VC-dimension%2520of%2520rule%2520lists%252C%250Afor%2520which%2520we%2520prove%2520novel%2520upper%2520and%2520lower%2520bounds.%2520Our%2520experimental%2520evaluation%2520on%250Alarge%2520datasets%2520shows%2520that%2520our%2520algorithm%2520identifies%2520nearly%2520optimal%2520rule%2520lists%250Awith%2520a%2520speed-up%2520up%2520to%2520two%2520orders%2520of%2520magnitude%2520over%2520state-of-the-art%2520exact%250Aapproaches.%2520Moreover%252C%2520our%2520algorithm%2520is%2520as%2520fast%2520as%252C%2520and%2520sometimes%2520faster%2520than%252C%250Arecent%2520heuristic%2520approaches%252C%2520while%2520reporting%2520higher%2520quality%2520rule%2520lists.%2520In%250Aaddition%252C%2520the%2520rules%2520reported%2520by%2520our%2520algorithm%2520are%2520more%2520similar%2520to%2520the%2520rules%2520in%250Athe%2520optimal%2520rule%2520list%2520than%2520the%2520rules%2520from%2520heuristic%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Rule%20Lists%20Learning%20with%20Sampling&entry.906535625=Leonardo%20Pellegrina%20and%20Fabio%20Vandin&entry.1292438233=%20%20Learning%20interpretable%20models%20has%20become%20a%20major%20focus%20of%20machine%20learning%0Aresearch%2C%20given%20the%20increasing%20prominence%20of%20machine%20learning%20in%20socially%0Aimportant%20decision-making.%20Among%20interpretable%20models%2C%20rule%20lists%20are%20among%20the%0Abest-known%20and%20easily%20interpretable%20ones.%20However%2C%20finding%20optimal%20rule%20lists%0Ais%20computationally%20challenging%2C%20and%20current%20approaches%20are%20impractical%20for%0Alarge%20datasets.%0A%20%20We%20present%20a%20novel%20and%20scalable%20approach%20to%20learn%20nearly%20optimal%20rule%20lists%0Afrom%20large%20datasets.%20Our%20algorithm%20uses%20sampling%20to%20efficiently%20obtain%20an%0Aapproximation%20of%20the%20optimal%20rule%20list%20with%20rigorous%20guarantees%20on%20the%20quality%0Aof%20the%20approximation.%20In%20particular%2C%20our%20algorithm%20guarantees%20to%20find%20a%20rule%0Alist%20with%20accuracy%20very%20close%20to%20the%20optimal%20rule%20list%20when%20a%20rule%20list%20with%0Ahigh%20accuracy%20exists.%20Our%20algorithm%20builds%20on%20the%20VC-dimension%20of%20rule%20lists%2C%0Afor%20which%20we%20prove%20novel%20upper%20and%20lower%20bounds.%20Our%20experimental%20evaluation%20on%0Alarge%20datasets%20shows%20that%20our%20algorithm%20identifies%20nearly%20optimal%20rule%20lists%0Awith%20a%20speed-up%20up%20to%20two%20orders%20of%20magnitude%20over%20state-of-the-art%20exact%0Aapproaches.%20Moreover%2C%20our%20algorithm%20is%20as%20fast%20as%2C%20and%20sometimes%20faster%20than%2C%0Arecent%20heuristic%20approaches%2C%20while%20reporting%20higher%20quality%20rule%20lists.%20In%0Aaddition%2C%20the%20rules%20reported%20by%20our%20algorithm%20are%20more%20similar%20to%20the%20rules%20in%0Athe%20optimal%20rule%20list%20than%20the%20rules%20from%20heuristic%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12803v1&entry.124074799=Read"},
{"title": "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL", "author": "Arian Askari and Christian Poelitz and Xinye Tang", "abstract": "  Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhance the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. We make all agent interactions publicly available to\nthe research community, to foster further research in this area, offering a\nsynthetic dataset for future explorations into automatic self-correction\nguideline generation.\n", "link": "http://arxiv.org/abs/2406.12692v1", "date": "2024-06-18", "relevancy": 2.4529, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.508}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4936}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGIC%3A%20Generating%20Self-Correction%20Guideline%20for%20In-Context%20Text-to-SQL&body=Title%3A%20MAGIC%3A%20Generating%20Self-Correction%20Guideline%20for%20In-Context%20Text-to-SQL%0AAuthor%3A%20Arian%20Askari%20and%20Christian%20Poelitz%20and%20Xinye%20Tang%0AAbstract%3A%20%20%20Self-correction%20in%20text-to-SQL%20is%20the%20process%20of%20prompting%20large%20language%0Amodel%20%28LLM%29%20to%20revise%20its%20previously%20incorrectly%20generated%20SQL%2C%20and%20commonly%0Arelies%20on%20manually%20crafted%20self-correction%20guidelines%20by%20human%20experts%20that%20are%0Anot%20only%20labor-intensive%20to%20produce%20but%20also%20limited%20by%20the%20human%20ability%20in%0Aidentifying%20all%20potential%20error%20patterns%20in%20LLM%20responses.%20We%20introduce%20MAGIC%2C%0Aa%20novel%20multi-agent%20method%20that%20automates%20the%20creation%20of%20the%20self-correction%0Aguideline.%20MAGIC%20uses%20three%20specialized%20agents%3A%20a%20manager%2C%20a%20correction%2C%20and%20a%0Afeedback%20agent.%20These%20agents%20collaborate%20on%20the%20failures%20of%20an%20LLM-based%20method%0Aon%20the%20training%20set%20to%20iteratively%20generate%20and%20refine%20a%20self-correction%0Aguideline%20tailored%20to%20LLM%20mistakes%2C%20mirroring%20human%20processes%20but%20without%20human%0Ainvolvement.%20Our%20extensive%20experiments%20show%20that%20MAGIC%27s%20guideline%20outperforms%0Aexpert%20human%27s%20created%20ones.%20We%20empirically%20find%20out%20that%20the%20guideline%0Aproduced%20by%20MAGIC%20enhance%20the%20interpretability%20of%20the%20corrections%20made%2C%0Aproviding%20insights%20in%20analyzing%20the%20reason%20behind%20the%20failures%20and%20successes%20of%0ALLMs%20in%20self-correction.%20We%20make%20all%20agent%20interactions%20publicly%20available%20to%0Athe%20research%20community%2C%20to%20foster%20further%20research%20in%20this%20area%2C%20offering%20a%0Asynthetic%20dataset%20for%20future%20explorations%20into%20automatic%20self-correction%0Aguideline%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGIC%253A%2520Generating%2520Self-Correction%2520Guideline%2520for%2520In-Context%2520Text-to-SQL%26entry.906535625%3DArian%2520Askari%2520and%2520Christian%2520Poelitz%2520and%2520Xinye%2520Tang%26entry.1292438233%3D%2520%2520Self-correction%2520in%2520text-to-SQL%2520is%2520the%2520process%2520of%2520prompting%2520large%2520language%250Amodel%2520%2528LLM%2529%2520to%2520revise%2520its%2520previously%2520incorrectly%2520generated%2520SQL%252C%2520and%2520commonly%250Arelies%2520on%2520manually%2520crafted%2520self-correction%2520guidelines%2520by%2520human%2520experts%2520that%2520are%250Anot%2520only%2520labor-intensive%2520to%2520produce%2520but%2520also%2520limited%2520by%2520the%2520human%2520ability%2520in%250Aidentifying%2520all%2520potential%2520error%2520patterns%2520in%2520LLM%2520responses.%2520We%2520introduce%2520MAGIC%252C%250Aa%2520novel%2520multi-agent%2520method%2520that%2520automates%2520the%2520creation%2520of%2520the%2520self-correction%250Aguideline.%2520MAGIC%2520uses%2520three%2520specialized%2520agents%253A%2520a%2520manager%252C%2520a%2520correction%252C%2520and%2520a%250Afeedback%2520agent.%2520These%2520agents%2520collaborate%2520on%2520the%2520failures%2520of%2520an%2520LLM-based%2520method%250Aon%2520the%2520training%2520set%2520to%2520iteratively%2520generate%2520and%2520refine%2520a%2520self-correction%250Aguideline%2520tailored%2520to%2520LLM%2520mistakes%252C%2520mirroring%2520human%2520processes%2520but%2520without%2520human%250Ainvolvement.%2520Our%2520extensive%2520experiments%2520show%2520that%2520MAGIC%2527s%2520guideline%2520outperforms%250Aexpert%2520human%2527s%2520created%2520ones.%2520We%2520empirically%2520find%2520out%2520that%2520the%2520guideline%250Aproduced%2520by%2520MAGIC%2520enhance%2520the%2520interpretability%2520of%2520the%2520corrections%2520made%252C%250Aproviding%2520insights%2520in%2520analyzing%2520the%2520reason%2520behind%2520the%2520failures%2520and%2520successes%2520of%250ALLMs%2520in%2520self-correction.%2520We%2520make%2520all%2520agent%2520interactions%2520publicly%2520available%2520to%250Athe%2520research%2520community%252C%2520to%2520foster%2520further%2520research%2520in%2520this%2520area%252C%2520offering%2520a%250Asynthetic%2520dataset%2520for%2520future%2520explorations%2520into%2520automatic%2520self-correction%250Aguideline%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGIC%3A%20Generating%20Self-Correction%20Guideline%20for%20In-Context%20Text-to-SQL&entry.906535625=Arian%20Askari%20and%20Christian%20Poelitz%20and%20Xinye%20Tang&entry.1292438233=%20%20Self-correction%20in%20text-to-SQL%20is%20the%20process%20of%20prompting%20large%20language%0Amodel%20%28LLM%29%20to%20revise%20its%20previously%20incorrectly%20generated%20SQL%2C%20and%20commonly%0Arelies%20on%20manually%20crafted%20self-correction%20guidelines%20by%20human%20experts%20that%20are%0Anot%20only%20labor-intensive%20to%20produce%20but%20also%20limited%20by%20the%20human%20ability%20in%0Aidentifying%20all%20potential%20error%20patterns%20in%20LLM%20responses.%20We%20introduce%20MAGIC%2C%0Aa%20novel%20multi-agent%20method%20that%20automates%20the%20creation%20of%20the%20self-correction%0Aguideline.%20MAGIC%20uses%20three%20specialized%20agents%3A%20a%20manager%2C%20a%20correction%2C%20and%20a%0Afeedback%20agent.%20These%20agents%20collaborate%20on%20the%20failures%20of%20an%20LLM-based%20method%0Aon%20the%20training%20set%20to%20iteratively%20generate%20and%20refine%20a%20self-correction%0Aguideline%20tailored%20to%20LLM%20mistakes%2C%20mirroring%20human%20processes%20but%20without%20human%0Ainvolvement.%20Our%20extensive%20experiments%20show%20that%20MAGIC%27s%20guideline%20outperforms%0Aexpert%20human%27s%20created%20ones.%20We%20empirically%20find%20out%20that%20the%20guideline%0Aproduced%20by%20MAGIC%20enhance%20the%20interpretability%20of%20the%20corrections%20made%2C%0Aproviding%20insights%20in%20analyzing%20the%20reason%20behind%20the%20failures%20and%20successes%20of%0ALLMs%20in%20self-correction.%20We%20make%20all%20agent%20interactions%20publicly%20available%20to%0Athe%20research%20community%2C%20to%20foster%20further%20research%20in%20this%20area%2C%20offering%20a%0Asynthetic%20dataset%20for%20future%20explorations%20into%20automatic%20self-correction%0Aguideline%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12692v1&entry.124074799=Read"},
{"title": "Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix\n  Controller", "author": "Min Cai and Yuchen Zhang and Shichang Zhang and Fan Yin and Difan Zou and Yisong Yue and Ziniu Hu", "abstract": "  We propose Self-Control, a novel method utilizing suffix gradients to control\nthe behavior of large language models (LLMs) without explicit human\nannotations. Given a guideline expressed in suffix string and the model's\nself-assessment of adherence, Self-Control computes the gradient of this\nself-judgment concerning the model's hidden states, directly influencing the\nauto-regressive generation process towards desired behaviors. To enhance\nefficiency, we introduce Self-Control_{prefix}, a compact module that\nencapsulates the learned representations from suffix gradients into a Prefix\nController, facilitating inference-time control for various LLM behaviors. Our\nexperiments demonstrate Self-Control's efficacy across multiple domains,\nincluding emotional modulation, ensuring harmlessness, and enhancing complex\nreasoning. Especially, Self-Control_{prefix} enables a plug-and-play control\nand jointly controls multiple attributes, improving model outputs without\naltering model parameters or increasing inference-time costs.\n", "link": "http://arxiv.org/abs/2406.02721v2", "date": "2024-06-18", "relevancy": 2.434, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.494}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4853}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Control%20of%20LLM%20Behaviors%20by%20Compressing%20Suffix%20Gradient%20into%20Prefix%0A%20%20Controller&body=Title%3A%20Self-Control%20of%20LLM%20Behaviors%20by%20Compressing%20Suffix%20Gradient%20into%20Prefix%0A%20%20Controller%0AAuthor%3A%20Min%20Cai%20and%20Yuchen%20Zhang%20and%20Shichang%20Zhang%20and%20Fan%20Yin%20and%20Difan%20Zou%20and%20Yisong%20Yue%20and%20Ziniu%20Hu%0AAbstract%3A%20%20%20We%20propose%20Self-Control%2C%20a%20novel%20method%20utilizing%20suffix%20gradients%20to%20control%0Athe%20behavior%20of%20large%20language%20models%20%28LLMs%29%20without%20explicit%20human%0Aannotations.%20Given%20a%20guideline%20expressed%20in%20suffix%20string%20and%20the%20model%27s%0Aself-assessment%20of%20adherence%2C%20Self-Control%20computes%20the%20gradient%20of%20this%0Aself-judgment%20concerning%20the%20model%27s%20hidden%20states%2C%20directly%20influencing%20the%0Aauto-regressive%20generation%20process%20towards%20desired%20behaviors.%20To%20enhance%0Aefficiency%2C%20we%20introduce%20Self-Control_%7Bprefix%7D%2C%20a%20compact%20module%20that%0Aencapsulates%20the%20learned%20representations%20from%20suffix%20gradients%20into%20a%20Prefix%0AController%2C%20facilitating%20inference-time%20control%20for%20various%20LLM%20behaviors.%20Our%0Aexperiments%20demonstrate%20Self-Control%27s%20efficacy%20across%20multiple%20domains%2C%0Aincluding%20emotional%20modulation%2C%20ensuring%20harmlessness%2C%20and%20enhancing%20complex%0Areasoning.%20Especially%2C%20Self-Control_%7Bprefix%7D%20enables%20a%20plug-and-play%20control%0Aand%20jointly%20controls%20multiple%20attributes%2C%20improving%20model%20outputs%20without%0Aaltering%20model%20parameters%20or%20increasing%20inference-time%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02721v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Control%2520of%2520LLM%2520Behaviors%2520by%2520Compressing%2520Suffix%2520Gradient%2520into%2520Prefix%250A%2520%2520Controller%26entry.906535625%3DMin%2520Cai%2520and%2520Yuchen%2520Zhang%2520and%2520Shichang%2520Zhang%2520and%2520Fan%2520Yin%2520and%2520Difan%2520Zou%2520and%2520Yisong%2520Yue%2520and%2520Ziniu%2520Hu%26entry.1292438233%3D%2520%2520We%2520propose%2520Self-Control%252C%2520a%2520novel%2520method%2520utilizing%2520suffix%2520gradients%2520to%2520control%250Athe%2520behavior%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520without%2520explicit%2520human%250Aannotations.%2520Given%2520a%2520guideline%2520expressed%2520in%2520suffix%2520string%2520and%2520the%2520model%2527s%250Aself-assessment%2520of%2520adherence%252C%2520Self-Control%2520computes%2520the%2520gradient%2520of%2520this%250Aself-judgment%2520concerning%2520the%2520model%2527s%2520hidden%2520states%252C%2520directly%2520influencing%2520the%250Aauto-regressive%2520generation%2520process%2520towards%2520desired%2520behaviors.%2520To%2520enhance%250Aefficiency%252C%2520we%2520introduce%2520Self-Control_%257Bprefix%257D%252C%2520a%2520compact%2520module%2520that%250Aencapsulates%2520the%2520learned%2520representations%2520from%2520suffix%2520gradients%2520into%2520a%2520Prefix%250AController%252C%2520facilitating%2520inference-time%2520control%2520for%2520various%2520LLM%2520behaviors.%2520Our%250Aexperiments%2520demonstrate%2520Self-Control%2527s%2520efficacy%2520across%2520multiple%2520domains%252C%250Aincluding%2520emotional%2520modulation%252C%2520ensuring%2520harmlessness%252C%2520and%2520enhancing%2520complex%250Areasoning.%2520Especially%252C%2520Self-Control_%257Bprefix%257D%2520enables%2520a%2520plug-and-play%2520control%250Aand%2520jointly%2520controls%2520multiple%2520attributes%252C%2520improving%2520model%2520outputs%2520without%250Aaltering%2520model%2520parameters%2520or%2520increasing%2520inference-time%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02721v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Control%20of%20LLM%20Behaviors%20by%20Compressing%20Suffix%20Gradient%20into%20Prefix%0A%20%20Controller&entry.906535625=Min%20Cai%20and%20Yuchen%20Zhang%20and%20Shichang%20Zhang%20and%20Fan%20Yin%20and%20Difan%20Zou%20and%20Yisong%20Yue%20and%20Ziniu%20Hu&entry.1292438233=%20%20We%20propose%20Self-Control%2C%20a%20novel%20method%20utilizing%20suffix%20gradients%20to%20control%0Athe%20behavior%20of%20large%20language%20models%20%28LLMs%29%20without%20explicit%20human%0Aannotations.%20Given%20a%20guideline%20expressed%20in%20suffix%20string%20and%20the%20model%27s%0Aself-assessment%20of%20adherence%2C%20Self-Control%20computes%20the%20gradient%20of%20this%0Aself-judgment%20concerning%20the%20model%27s%20hidden%20states%2C%20directly%20influencing%20the%0Aauto-regressive%20generation%20process%20towards%20desired%20behaviors.%20To%20enhance%0Aefficiency%2C%20we%20introduce%20Self-Control_%7Bprefix%7D%2C%20a%20compact%20module%20that%0Aencapsulates%20the%20learned%20representations%20from%20suffix%20gradients%20into%20a%20Prefix%0AController%2C%20facilitating%20inference-time%20control%20for%20various%20LLM%20behaviors.%20Our%0Aexperiments%20demonstrate%20Self-Control%27s%20efficacy%20across%20multiple%20domains%2C%0Aincluding%20emotional%20modulation%2C%20ensuring%20harmlessness%2C%20and%20enhancing%20complex%0Areasoning.%20Especially%2C%20Self-Control_%7Bprefix%7D%20enables%20a%20plug-and-play%20control%0Aand%20jointly%20controls%20multiple%20attributes%2C%20improving%20model%20outputs%20without%0Aaltering%20model%20parameters%20or%20increasing%20inference-time%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02721v2&entry.124074799=Read"},
{"title": "Local Recovery of Two-layer Neural Networks at Overparameterization", "author": "Leyang Zhang and Yaoyu Zhang and Tao Luo", "abstract": "  Under mild assumptions, we investigate the structure of loss landscape of\ntwo-layer neural networks near global minima, determine the set of parameters\nwhich recovers the target function, and characterize the gradient flows around\nit. With novel techniques, our work uncovers some simple aspects of the\ncomplicated loss landscape and reveals how model, target function, samples and\ninitialization affect the training dynamics differently. These results\nconcludes that two-layer neural networks can be recovered locally at\noverparameterization.\n", "link": "http://arxiv.org/abs/2309.00508v2", "date": "2024-06-18", "relevancy": 2.4191, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4928}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4834}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Recovery%20of%20Two-layer%20Neural%20Networks%20at%20Overparameterization&body=Title%3A%20Local%20Recovery%20of%20Two-layer%20Neural%20Networks%20at%20Overparameterization%0AAuthor%3A%20Leyang%20Zhang%20and%20Yaoyu%20Zhang%20and%20Tao%20Luo%0AAbstract%3A%20%20%20Under%20mild%20assumptions%2C%20we%20investigate%20the%20structure%20of%20loss%20landscape%20of%0Atwo-layer%20neural%20networks%20near%20global%20minima%2C%20determine%20the%20set%20of%20parameters%0Awhich%20recovers%20the%20target%20function%2C%20and%20characterize%20the%20gradient%20flows%20around%0Ait.%20With%20novel%20techniques%2C%20our%20work%20uncovers%20some%20simple%20aspects%20of%20the%0Acomplicated%20loss%20landscape%20and%20reveals%20how%20model%2C%20target%20function%2C%20samples%20and%0Ainitialization%20affect%20the%20training%20dynamics%20differently.%20These%20results%0Aconcludes%20that%20two-layer%20neural%20networks%20can%20be%20recovered%20locally%20at%0Aoverparameterization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Recovery%2520of%2520Two-layer%2520Neural%2520Networks%2520at%2520Overparameterization%26entry.906535625%3DLeyang%2520Zhang%2520and%2520Yaoyu%2520Zhang%2520and%2520Tao%2520Luo%26entry.1292438233%3D%2520%2520Under%2520mild%2520assumptions%252C%2520we%2520investigate%2520the%2520structure%2520of%2520loss%2520landscape%2520of%250Atwo-layer%2520neural%2520networks%2520near%2520global%2520minima%252C%2520determine%2520the%2520set%2520of%2520parameters%250Awhich%2520recovers%2520the%2520target%2520function%252C%2520and%2520characterize%2520the%2520gradient%2520flows%2520around%250Ait.%2520With%2520novel%2520techniques%252C%2520our%2520work%2520uncovers%2520some%2520simple%2520aspects%2520of%2520the%250Acomplicated%2520loss%2520landscape%2520and%2520reveals%2520how%2520model%252C%2520target%2520function%252C%2520samples%2520and%250Ainitialization%2520affect%2520the%2520training%2520dynamics%2520differently.%2520These%2520results%250Aconcludes%2520that%2520two-layer%2520neural%2520networks%2520can%2520be%2520recovered%2520locally%2520at%250Aoverparameterization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Recovery%20of%20Two-layer%20Neural%20Networks%20at%20Overparameterization&entry.906535625=Leyang%20Zhang%20and%20Yaoyu%20Zhang%20and%20Tao%20Luo&entry.1292438233=%20%20Under%20mild%20assumptions%2C%20we%20investigate%20the%20structure%20of%20loss%20landscape%20of%0Atwo-layer%20neural%20networks%20near%20global%20minima%2C%20determine%20the%20set%20of%20parameters%0Awhich%20recovers%20the%20target%20function%2C%20and%20characterize%20the%20gradient%20flows%20around%0Ait.%20With%20novel%20techniques%2C%20our%20work%20uncovers%20some%20simple%20aspects%20of%20the%0Acomplicated%20loss%20landscape%20and%20reveals%20how%20model%2C%20target%20function%2C%20samples%20and%0Ainitialization%20affect%20the%20training%20dynamics%20differently.%20These%20results%0Aconcludes%20that%20two-layer%20neural%20networks%20can%20be%20recovered%20locally%20at%0Aoverparameterization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00508v2&entry.124074799=Read"},
{"title": "Memorization in Self-Supervised Learning Improves Downstream\n  Generalization", "author": "Wenhao Wang and Muhammad Ahmad Kaleem and Adam Dziedzic and Michael Backes and Nicolas Papernot and Franziska Boenisch", "abstract": "  Self-supervised learning (SSL) has recently received significant attention\ndue to its ability to train high-performance encoders purely on unlabeled\ndata-often scraped from the internet. This data can still be sensitive and\nempirical evidence suggests that SSL encoders memorize private information of\ntheir training data and can disclose them at inference time. Since existing\ntheoretical definitions of memorization from supervised learning rely on\nlabels, they do not transfer to SSL. To address this gap, we propose SSLMem, a\nframework for defining memorization within SSL. Our definition compares the\ndifference in alignment of representations for data points and their augmented\nviews returned by both encoders that were trained on these data points and\nencoders that were not. Through comprehensive empirical analysis on diverse\nencoder architectures and datasets we highlight that even though SSL relies on\nlarge datasets and strong augmentations-both known in supervised learning as\nregularization techniques that reduce overfitting-still significant fractions\nof training data points experience high memorization. Through our empirical\nresults, we show that this memorization is essential for encoders to achieve\nhigher generalization performance on different downstream tasks.\n", "link": "http://arxiv.org/abs/2401.12233v3", "date": "2024-06-18", "relevancy": 2.4102, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5155}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4661}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorization%20in%20Self-Supervised%20Learning%20Improves%20Downstream%0A%20%20Generalization&body=Title%3A%20Memorization%20in%20Self-Supervised%20Learning%20Improves%20Downstream%0A%20%20Generalization%0AAuthor%3A%20Wenhao%20Wang%20and%20Muhammad%20Ahmad%20Kaleem%20and%20Adam%20Dziedzic%20and%20Michael%20Backes%20and%20Nicolas%20Papernot%20and%20Franziska%20Boenisch%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20received%20significant%20attention%0Adue%20to%20its%20ability%20to%20train%20high-performance%20encoders%20purely%20on%20unlabeled%0Adata-often%20scraped%20from%20the%20internet.%20This%20data%20can%20still%20be%20sensitive%20and%0Aempirical%20evidence%20suggests%20that%20SSL%20encoders%20memorize%20private%20information%20of%0Atheir%20training%20data%20and%20can%20disclose%20them%20at%20inference%20time.%20Since%20existing%0Atheoretical%20definitions%20of%20memorization%20from%20supervised%20learning%20rely%20on%0Alabels%2C%20they%20do%20not%20transfer%20to%20SSL.%20To%20address%20this%20gap%2C%20we%20propose%20SSLMem%2C%20a%0Aframework%20for%20defining%20memorization%20within%20SSL.%20Our%20definition%20compares%20the%0Adifference%20in%20alignment%20of%20representations%20for%20data%20points%20and%20their%20augmented%0Aviews%20returned%20by%20both%20encoders%20that%20were%20trained%20on%20these%20data%20points%20and%0Aencoders%20that%20were%20not.%20Through%20comprehensive%20empirical%20analysis%20on%20diverse%0Aencoder%20architectures%20and%20datasets%20we%20highlight%20that%20even%20though%20SSL%20relies%20on%0Alarge%20datasets%20and%20strong%20augmentations-both%20known%20in%20supervised%20learning%20as%0Aregularization%20techniques%20that%20reduce%20overfitting-still%20significant%20fractions%0Aof%20training%20data%20points%20experience%20high%20memorization.%20Through%20our%20empirical%0Aresults%2C%20we%20show%20that%20this%20memorization%20is%20essential%20for%20encoders%20to%20achieve%0Ahigher%20generalization%20performance%20on%20different%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12233v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorization%2520in%2520Self-Supervised%2520Learning%2520Improves%2520Downstream%250A%2520%2520Generalization%26entry.906535625%3DWenhao%2520Wang%2520and%2520Muhammad%2520Ahmad%2520Kaleem%2520and%2520Adam%2520Dziedzic%2520and%2520Michael%2520Backes%2520and%2520Nicolas%2520Papernot%2520and%2520Franziska%2520Boenisch%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520recently%2520received%2520significant%2520attention%250Adue%2520to%2520its%2520ability%2520to%2520train%2520high-performance%2520encoders%2520purely%2520on%2520unlabeled%250Adata-often%2520scraped%2520from%2520the%2520internet.%2520This%2520data%2520can%2520still%2520be%2520sensitive%2520and%250Aempirical%2520evidence%2520suggests%2520that%2520SSL%2520encoders%2520memorize%2520private%2520information%2520of%250Atheir%2520training%2520data%2520and%2520can%2520disclose%2520them%2520at%2520inference%2520time.%2520Since%2520existing%250Atheoretical%2520definitions%2520of%2520memorization%2520from%2520supervised%2520learning%2520rely%2520on%250Alabels%252C%2520they%2520do%2520not%2520transfer%2520to%2520SSL.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520SSLMem%252C%2520a%250Aframework%2520for%2520defining%2520memorization%2520within%2520SSL.%2520Our%2520definition%2520compares%2520the%250Adifference%2520in%2520alignment%2520of%2520representations%2520for%2520data%2520points%2520and%2520their%2520augmented%250Aviews%2520returned%2520by%2520both%2520encoders%2520that%2520were%2520trained%2520on%2520these%2520data%2520points%2520and%250Aencoders%2520that%2520were%2520not.%2520Through%2520comprehensive%2520empirical%2520analysis%2520on%2520diverse%250Aencoder%2520architectures%2520and%2520datasets%2520we%2520highlight%2520that%2520even%2520though%2520SSL%2520relies%2520on%250Alarge%2520datasets%2520and%2520strong%2520augmentations-both%2520known%2520in%2520supervised%2520learning%2520as%250Aregularization%2520techniques%2520that%2520reduce%2520overfitting-still%2520significant%2520fractions%250Aof%2520training%2520data%2520points%2520experience%2520high%2520memorization.%2520Through%2520our%2520empirical%250Aresults%252C%2520we%2520show%2520that%2520this%2520memorization%2520is%2520essential%2520for%2520encoders%2520to%2520achieve%250Ahigher%2520generalization%2520performance%2520on%2520different%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12233v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorization%20in%20Self-Supervised%20Learning%20Improves%20Downstream%0A%20%20Generalization&entry.906535625=Wenhao%20Wang%20and%20Muhammad%20Ahmad%20Kaleem%20and%20Adam%20Dziedzic%20and%20Michael%20Backes%20and%20Nicolas%20Papernot%20and%20Franziska%20Boenisch&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20received%20significant%20attention%0Adue%20to%20its%20ability%20to%20train%20high-performance%20encoders%20purely%20on%20unlabeled%0Adata-often%20scraped%20from%20the%20internet.%20This%20data%20can%20still%20be%20sensitive%20and%0Aempirical%20evidence%20suggests%20that%20SSL%20encoders%20memorize%20private%20information%20of%0Atheir%20training%20data%20and%20can%20disclose%20them%20at%20inference%20time.%20Since%20existing%0Atheoretical%20definitions%20of%20memorization%20from%20supervised%20learning%20rely%20on%0Alabels%2C%20they%20do%20not%20transfer%20to%20SSL.%20To%20address%20this%20gap%2C%20we%20propose%20SSLMem%2C%20a%0Aframework%20for%20defining%20memorization%20within%20SSL.%20Our%20definition%20compares%20the%0Adifference%20in%20alignment%20of%20representations%20for%20data%20points%20and%20their%20augmented%0Aviews%20returned%20by%20both%20encoders%20that%20were%20trained%20on%20these%20data%20points%20and%0Aencoders%20that%20were%20not.%20Through%20comprehensive%20empirical%20analysis%20on%20diverse%0Aencoder%20architectures%20and%20datasets%20we%20highlight%20that%20even%20though%20SSL%20relies%20on%0Alarge%20datasets%20and%20strong%20augmentations-both%20known%20in%20supervised%20learning%20as%0Aregularization%20techniques%20that%20reduce%20overfitting-still%20significant%20fractions%0Aof%20training%20data%20points%20experience%20high%20memorization.%20Through%20our%20empirical%0Aresults%2C%20we%20show%20that%20this%20memorization%20is%20essential%20for%20encoders%20to%20achieve%0Ahigher%20generalization%20performance%20on%20different%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12233v3&entry.124074799=Read"},
{"title": "Breaking the Ceiling of the LLM Community by Treating Token Generation\n  as a Classification for Ensembling", "author": "Yao-Ching Yu and Chun-Chih Kuo and Ziqi Ye and Yu-Cheng Chang and Yueh-Se Li", "abstract": "  Ensembling multiple models has always been an effective approach to push the\nlimits of existing performance and is widely used in classification tasks by\nsimply averaging the classification probability vectors from multiple\nclassifiers to achieve better accuracy. However, in the thriving open-source\nLarge Language Model (LLM) community, ensembling methods are rare and typically\nlimited to ensembling the full-text outputs of LLMs, such as selecting the best\noutput using a ranker, which leads to underutilization of token-level\nprobability information. In this paper, we treat the Generation of each token\nby LLMs as a Classification (GaC) for ensembling. This approach fully exploits\nthe probability information at each generation step and better prevents LLMs\nfrom producing early incorrect tokens that lead to snowballing errors. In\nexperiments, we ensemble state-of-the-art LLMs on several benchmarks, including\nexams, mathematics and reasoning, and observe that our method breaks the\nexisting community performance ceiling. Furthermore, we observed that most of\nthe tokens in the answer are simple and do not affect the correctness of the\nfinal answer. Therefore, we also experimented with ensembling only key tokens,\nand the results showed better performance with lower latency across benchmarks.\n", "link": "http://arxiv.org/abs/2406.12585v1", "date": "2024-06-18", "relevancy": 2.4049, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4941}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4911}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Ceiling%20of%20the%20LLM%20Community%20by%20Treating%20Token%20Generation%0A%20%20as%20a%20Classification%20for%20Ensembling&body=Title%3A%20Breaking%20the%20Ceiling%20of%20the%20LLM%20Community%20by%20Treating%20Token%20Generation%0A%20%20as%20a%20Classification%20for%20Ensembling%0AAuthor%3A%20Yao-Ching%20Yu%20and%20Chun-Chih%20Kuo%20and%20Ziqi%20Ye%20and%20Yu-Cheng%20Chang%20and%20Yueh-Se%20Li%0AAbstract%3A%20%20%20Ensembling%20multiple%20models%20has%20always%20been%20an%20effective%20approach%20to%20push%20the%0Alimits%20of%20existing%20performance%20and%20is%20widely%20used%20in%20classification%20tasks%20by%0Asimply%20averaging%20the%20classification%20probability%20vectors%20from%20multiple%0Aclassifiers%20to%20achieve%20better%20accuracy.%20However%2C%20in%20the%20thriving%20open-source%0ALarge%20Language%20Model%20%28LLM%29%20community%2C%20ensembling%20methods%20are%20rare%20and%20typically%0Alimited%20to%20ensembling%20the%20full-text%20outputs%20of%20LLMs%2C%20such%20as%20selecting%20the%20best%0Aoutput%20using%20a%20ranker%2C%20which%20leads%20to%20underutilization%20of%20token-level%0Aprobability%20information.%20In%20this%20paper%2C%20we%20treat%20the%20Generation%20of%20each%20token%0Aby%20LLMs%20as%20a%20Classification%20%28GaC%29%20for%20ensembling.%20This%20approach%20fully%20exploits%0Athe%20probability%20information%20at%20each%20generation%20step%20and%20better%20prevents%20LLMs%0Afrom%20producing%20early%20incorrect%20tokens%20that%20lead%20to%20snowballing%20errors.%20In%0Aexperiments%2C%20we%20ensemble%20state-of-the-art%20LLMs%20on%20several%20benchmarks%2C%20including%0Aexams%2C%20mathematics%20and%20reasoning%2C%20and%20observe%20that%20our%20method%20breaks%20the%0Aexisting%20community%20performance%20ceiling.%20Furthermore%2C%20we%20observed%20that%20most%20of%0Athe%20tokens%20in%20the%20answer%20are%20simple%20and%20do%20not%20affect%20the%20correctness%20of%20the%0Afinal%20answer.%20Therefore%2C%20we%20also%20experimented%20with%20ensembling%20only%20key%20tokens%2C%0Aand%20the%20results%20showed%20better%20performance%20with%20lower%20latency%20across%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Ceiling%2520of%2520the%2520LLM%2520Community%2520by%2520Treating%2520Token%2520Generation%250A%2520%2520as%2520a%2520Classification%2520for%2520Ensembling%26entry.906535625%3DYao-Ching%2520Yu%2520and%2520Chun-Chih%2520Kuo%2520and%2520Ziqi%2520Ye%2520and%2520Yu-Cheng%2520Chang%2520and%2520Yueh-Se%2520Li%26entry.1292438233%3D%2520%2520Ensembling%2520multiple%2520models%2520has%2520always%2520been%2520an%2520effective%2520approach%2520to%2520push%2520the%250Alimits%2520of%2520existing%2520performance%2520and%2520is%2520widely%2520used%2520in%2520classification%2520tasks%2520by%250Asimply%2520averaging%2520the%2520classification%2520probability%2520vectors%2520from%2520multiple%250Aclassifiers%2520to%2520achieve%2520better%2520accuracy.%2520However%252C%2520in%2520the%2520thriving%2520open-source%250ALarge%2520Language%2520Model%2520%2528LLM%2529%2520community%252C%2520ensembling%2520methods%2520are%2520rare%2520and%2520typically%250Alimited%2520to%2520ensembling%2520the%2520full-text%2520outputs%2520of%2520LLMs%252C%2520such%2520as%2520selecting%2520the%2520best%250Aoutput%2520using%2520a%2520ranker%252C%2520which%2520leads%2520to%2520underutilization%2520of%2520token-level%250Aprobability%2520information.%2520In%2520this%2520paper%252C%2520we%2520treat%2520the%2520Generation%2520of%2520each%2520token%250Aby%2520LLMs%2520as%2520a%2520Classification%2520%2528GaC%2529%2520for%2520ensembling.%2520This%2520approach%2520fully%2520exploits%250Athe%2520probability%2520information%2520at%2520each%2520generation%2520step%2520and%2520better%2520prevents%2520LLMs%250Afrom%2520producing%2520early%2520incorrect%2520tokens%2520that%2520lead%2520to%2520snowballing%2520errors.%2520In%250Aexperiments%252C%2520we%2520ensemble%2520state-of-the-art%2520LLMs%2520on%2520several%2520benchmarks%252C%2520including%250Aexams%252C%2520mathematics%2520and%2520reasoning%252C%2520and%2520observe%2520that%2520our%2520method%2520breaks%2520the%250Aexisting%2520community%2520performance%2520ceiling.%2520Furthermore%252C%2520we%2520observed%2520that%2520most%2520of%250Athe%2520tokens%2520in%2520the%2520answer%2520are%2520simple%2520and%2520do%2520not%2520affect%2520the%2520correctness%2520of%2520the%250Afinal%2520answer.%2520Therefore%252C%2520we%2520also%2520experimented%2520with%2520ensembling%2520only%2520key%2520tokens%252C%250Aand%2520the%2520results%2520showed%2520better%2520performance%2520with%2520lower%2520latency%2520across%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Ceiling%20of%20the%20LLM%20Community%20by%20Treating%20Token%20Generation%0A%20%20as%20a%20Classification%20for%20Ensembling&entry.906535625=Yao-Ching%20Yu%20and%20Chun-Chih%20Kuo%20and%20Ziqi%20Ye%20and%20Yu-Cheng%20Chang%20and%20Yueh-Se%20Li&entry.1292438233=%20%20Ensembling%20multiple%20models%20has%20always%20been%20an%20effective%20approach%20to%20push%20the%0Alimits%20of%20existing%20performance%20and%20is%20widely%20used%20in%20classification%20tasks%20by%0Asimply%20averaging%20the%20classification%20probability%20vectors%20from%20multiple%0Aclassifiers%20to%20achieve%20better%20accuracy.%20However%2C%20in%20the%20thriving%20open-source%0ALarge%20Language%20Model%20%28LLM%29%20community%2C%20ensembling%20methods%20are%20rare%20and%20typically%0Alimited%20to%20ensembling%20the%20full-text%20outputs%20of%20LLMs%2C%20such%20as%20selecting%20the%20best%0Aoutput%20using%20a%20ranker%2C%20which%20leads%20to%20underutilization%20of%20token-level%0Aprobability%20information.%20In%20this%20paper%2C%20we%20treat%20the%20Generation%20of%20each%20token%0Aby%20LLMs%20as%20a%20Classification%20%28GaC%29%20for%20ensembling.%20This%20approach%20fully%20exploits%0Athe%20probability%20information%20at%20each%20generation%20step%20and%20better%20prevents%20LLMs%0Afrom%20producing%20early%20incorrect%20tokens%20that%20lead%20to%20snowballing%20errors.%20In%0Aexperiments%2C%20we%20ensemble%20state-of-the-art%20LLMs%20on%20several%20benchmarks%2C%20including%0Aexams%2C%20mathematics%20and%20reasoning%2C%20and%20observe%20that%20our%20method%20breaks%20the%0Aexisting%20community%20performance%20ceiling.%20Furthermore%2C%20we%20observed%20that%20most%20of%0Athe%20tokens%20in%20the%20answer%20are%20simple%20and%20do%20not%20affect%20the%20correctness%20of%20the%0Afinal%20answer.%20Therefore%2C%20we%20also%20experimented%20with%20ensembling%20only%20key%20tokens%2C%0Aand%20the%20results%20showed%20better%20performance%20with%20lower%20latency%20across%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12585v1&entry.124074799=Read"},
{"title": "A generalizable framework for low-rank tensor completion with numerical\n  priors", "author": "Shiran Yuan and Kaizhu Huang", "abstract": "  Low-Rank Tensor Completion, a method which exploits the inherent structure of\ntensors, has been studied extensively as an effective approach to tensor\ncompletion. Whilst such methods attained great success, none have\nsystematically considered exploiting the numerical priors of tensor elements.\nIgnoring numerical priors causes loss of important information regarding the\ndata, and therefore prevents the algorithms from reaching optimal accuracy.\nDespite the existence of some individual works which consider ad hoc numerical\npriors for specific tasks, no generalizable frameworks for incorporating\nnumerical priors have appeared. We present the Generalized CP Decomposition\nTensor Completion (GCDTC) framework, the first generalizable framework for\nlow-rank tensor completion that takes numerical priors of the data into\naccount. We test GCDTC by further proposing the Smooth Poisson Tensor\nCompletion (SPTC) algorithm, an instantiation of the GCDTC framework, whose\nperformance exceeds current state-of-the-arts by considerable margins in the\ntask of non-negative tensor completion, exemplifying GCDTC's effectiveness. Our\ncode is open-source.\n", "link": "http://arxiv.org/abs/2302.05881v5", "date": "2024-06-18", "relevancy": 2.4025, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4957}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4764}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20generalizable%20framework%20for%20low-rank%20tensor%20completion%20with%20numerical%0A%20%20priors&body=Title%3A%20A%20generalizable%20framework%20for%20low-rank%20tensor%20completion%20with%20numerical%0A%20%20priors%0AAuthor%3A%20Shiran%20Yuan%20and%20Kaizhu%20Huang%0AAbstract%3A%20%20%20Low-Rank%20Tensor%20Completion%2C%20a%20method%20which%20exploits%20the%20inherent%20structure%20of%0Atensors%2C%20has%20been%20studied%20extensively%20as%20an%20effective%20approach%20to%20tensor%0Acompletion.%20Whilst%20such%20methods%20attained%20great%20success%2C%20none%20have%0Asystematically%20considered%20exploiting%20the%20numerical%20priors%20of%20tensor%20elements.%0AIgnoring%20numerical%20priors%20causes%20loss%20of%20important%20information%20regarding%20the%0Adata%2C%20and%20therefore%20prevents%20the%20algorithms%20from%20reaching%20optimal%20accuracy.%0ADespite%20the%20existence%20of%20some%20individual%20works%20which%20consider%20ad%20hoc%20numerical%0Apriors%20for%20specific%20tasks%2C%20no%20generalizable%20frameworks%20for%20incorporating%0Anumerical%20priors%20have%20appeared.%20We%20present%20the%20Generalized%20CP%20Decomposition%0ATensor%20Completion%20%28GCDTC%29%20framework%2C%20the%20first%20generalizable%20framework%20for%0Alow-rank%20tensor%20completion%20that%20takes%20numerical%20priors%20of%20the%20data%20into%0Aaccount.%20We%20test%20GCDTC%20by%20further%20proposing%20the%20Smooth%20Poisson%20Tensor%0ACompletion%20%28SPTC%29%20algorithm%2C%20an%20instantiation%20of%20the%20GCDTC%20framework%2C%20whose%0Aperformance%20exceeds%20current%20state-of-the-arts%20by%20considerable%20margins%20in%20the%0Atask%20of%20non-negative%20tensor%20completion%2C%20exemplifying%20GCDTC%27s%20effectiveness.%20Our%0Acode%20is%20open-source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05881v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520generalizable%2520framework%2520for%2520low-rank%2520tensor%2520completion%2520with%2520numerical%250A%2520%2520priors%26entry.906535625%3DShiran%2520Yuan%2520and%2520Kaizhu%2520Huang%26entry.1292438233%3D%2520%2520Low-Rank%2520Tensor%2520Completion%252C%2520a%2520method%2520which%2520exploits%2520the%2520inherent%2520structure%2520of%250Atensors%252C%2520has%2520been%2520studied%2520extensively%2520as%2520an%2520effective%2520approach%2520to%2520tensor%250Acompletion.%2520Whilst%2520such%2520methods%2520attained%2520great%2520success%252C%2520none%2520have%250Asystematically%2520considered%2520exploiting%2520the%2520numerical%2520priors%2520of%2520tensor%2520elements.%250AIgnoring%2520numerical%2520priors%2520causes%2520loss%2520of%2520important%2520information%2520regarding%2520the%250Adata%252C%2520and%2520therefore%2520prevents%2520the%2520algorithms%2520from%2520reaching%2520optimal%2520accuracy.%250ADespite%2520the%2520existence%2520of%2520some%2520individual%2520works%2520which%2520consider%2520ad%2520hoc%2520numerical%250Apriors%2520for%2520specific%2520tasks%252C%2520no%2520generalizable%2520frameworks%2520for%2520incorporating%250Anumerical%2520priors%2520have%2520appeared.%2520We%2520present%2520the%2520Generalized%2520CP%2520Decomposition%250ATensor%2520Completion%2520%2528GCDTC%2529%2520framework%252C%2520the%2520first%2520generalizable%2520framework%2520for%250Alow-rank%2520tensor%2520completion%2520that%2520takes%2520numerical%2520priors%2520of%2520the%2520data%2520into%250Aaccount.%2520We%2520test%2520GCDTC%2520by%2520further%2520proposing%2520the%2520Smooth%2520Poisson%2520Tensor%250ACompletion%2520%2528SPTC%2529%2520algorithm%252C%2520an%2520instantiation%2520of%2520the%2520GCDTC%2520framework%252C%2520whose%250Aperformance%2520exceeds%2520current%2520state-of-the-arts%2520by%2520considerable%2520margins%2520in%2520the%250Atask%2520of%2520non-negative%2520tensor%2520completion%252C%2520exemplifying%2520GCDTC%2527s%2520effectiveness.%2520Our%250Acode%2520is%2520open-source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.05881v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20generalizable%20framework%20for%20low-rank%20tensor%20completion%20with%20numerical%0A%20%20priors&entry.906535625=Shiran%20Yuan%20and%20Kaizhu%20Huang&entry.1292438233=%20%20Low-Rank%20Tensor%20Completion%2C%20a%20method%20which%20exploits%20the%20inherent%20structure%20of%0Atensors%2C%20has%20been%20studied%20extensively%20as%20an%20effective%20approach%20to%20tensor%0Acompletion.%20Whilst%20such%20methods%20attained%20great%20success%2C%20none%20have%0Asystematically%20considered%20exploiting%20the%20numerical%20priors%20of%20tensor%20elements.%0AIgnoring%20numerical%20priors%20causes%20loss%20of%20important%20information%20regarding%20the%0Adata%2C%20and%20therefore%20prevents%20the%20algorithms%20from%20reaching%20optimal%20accuracy.%0ADespite%20the%20existence%20of%20some%20individual%20works%20which%20consider%20ad%20hoc%20numerical%0Apriors%20for%20specific%20tasks%2C%20no%20generalizable%20frameworks%20for%20incorporating%0Anumerical%20priors%20have%20appeared.%20We%20present%20the%20Generalized%20CP%20Decomposition%0ATensor%20Completion%20%28GCDTC%29%20framework%2C%20the%20first%20generalizable%20framework%20for%0Alow-rank%20tensor%20completion%20that%20takes%20numerical%20priors%20of%20the%20data%20into%0Aaccount.%20We%20test%20GCDTC%20by%20further%20proposing%20the%20Smooth%20Poisson%20Tensor%0ACompletion%20%28SPTC%29%20algorithm%2C%20an%20instantiation%20of%20the%20GCDTC%20framework%2C%20whose%0Aperformance%20exceeds%20current%20state-of-the-arts%20by%20considerable%20margins%20in%20the%0Atask%20of%20non-negative%20tensor%20completion%2C%20exemplifying%20GCDTC%27s%20effectiveness.%20Our%0Acode%20is%20open-source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05881v5&entry.124074799=Read"},
{"title": "Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning:\n  Lessons Learned", "author": "Du Yin and Jinliang Deng and Shuang Ao and Zechen Li and Hao Xue and Arian Prabowo and Renhe Jiang and Xuan Song and Flora Salim", "abstract": "  Training models on spatio-temporal (ST) data poses an open problem due to the\ncomplicated and diverse nature of the data itself, and it is challenging to\nensure the model's performance directly trained on the original ST data. While\nlimiting the variety of training data can make training easier, it can also\nlead to a lack of knowledge and information for the model, resulting in a\ndecrease in performance. To address this challenge, we presented an innovative\nparadigm that incorporates three separate forms of curriculum learning\nspecifically targeting from spatial, temporal, and quantile perspectives.\nFurthermore, our framework incorporates a stacking fusion module to combine\ndiverse information from three types of curriculum learning, resulting in a\nstrong and thorough learning process. We demonstrated the effectiveness of this\nframework with extensive empirical evaluations, highlighting its better\nperformance in addressing complex ST challenges. We provided thorough ablation\nstudies to investigate the effectiveness of our curriculum and to explain how\nit contributes to the improvement of learning efficiency on ST data.\n", "link": "http://arxiv.org/abs/2406.12709v1", "date": "2024-06-18", "relevancy": 2.4013, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4872}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4836}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Spatio-temporal%20Quantile%20Forecasting%20with%20Curriculum%20Learning%3A%0A%20%20Lessons%20Learned&body=Title%3A%20Enhancing%20Spatio-temporal%20Quantile%20Forecasting%20with%20Curriculum%20Learning%3A%0A%20%20Lessons%20Learned%0AAuthor%3A%20Du%20Yin%20and%20Jinliang%20Deng%20and%20Shuang%20Ao%20and%20Zechen%20Li%20and%20Hao%20Xue%20and%20Arian%20Prabowo%20and%20Renhe%20Jiang%20and%20Xuan%20Song%20and%20Flora%20Salim%0AAbstract%3A%20%20%20Training%20models%20on%20spatio-temporal%20%28ST%29%20data%20poses%20an%20open%20problem%20due%20to%20the%0Acomplicated%20and%20diverse%20nature%20of%20the%20data%20itself%2C%20and%20it%20is%20challenging%20to%0Aensure%20the%20model%27s%20performance%20directly%20trained%20on%20the%20original%20ST%20data.%20While%0Alimiting%20the%20variety%20of%20training%20data%20can%20make%20training%20easier%2C%20it%20can%20also%0Alead%20to%20a%20lack%20of%20knowledge%20and%20information%20for%20the%20model%2C%20resulting%20in%20a%0Adecrease%20in%20performance.%20To%20address%20this%20challenge%2C%20we%20presented%20an%20innovative%0Aparadigm%20that%20incorporates%20three%20separate%20forms%20of%20curriculum%20learning%0Aspecifically%20targeting%20from%20spatial%2C%20temporal%2C%20and%20quantile%20perspectives.%0AFurthermore%2C%20our%20framework%20incorporates%20a%20stacking%20fusion%20module%20to%20combine%0Adiverse%20information%20from%20three%20types%20of%20curriculum%20learning%2C%20resulting%20in%20a%0Astrong%20and%20thorough%20learning%20process.%20We%20demonstrated%20the%20effectiveness%20of%20this%0Aframework%20with%20extensive%20empirical%20evaluations%2C%20highlighting%20its%20better%0Aperformance%20in%20addressing%20complex%20ST%20challenges.%20We%20provided%20thorough%20ablation%0Astudies%20to%20investigate%20the%20effectiveness%20of%20our%20curriculum%20and%20to%20explain%20how%0Ait%20contributes%20to%20the%20improvement%20of%20learning%20efficiency%20on%20ST%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Spatio-temporal%2520Quantile%2520Forecasting%2520with%2520Curriculum%2520Learning%253A%250A%2520%2520Lessons%2520Learned%26entry.906535625%3DDu%2520Yin%2520and%2520Jinliang%2520Deng%2520and%2520Shuang%2520Ao%2520and%2520Zechen%2520Li%2520and%2520Hao%2520Xue%2520and%2520Arian%2520Prabowo%2520and%2520Renhe%2520Jiang%2520and%2520Xuan%2520Song%2520and%2520Flora%2520Salim%26entry.1292438233%3D%2520%2520Training%2520models%2520on%2520spatio-temporal%2520%2528ST%2529%2520data%2520poses%2520an%2520open%2520problem%2520due%2520to%2520the%250Acomplicated%2520and%2520diverse%2520nature%2520of%2520the%2520data%2520itself%252C%2520and%2520it%2520is%2520challenging%2520to%250Aensure%2520the%2520model%2527s%2520performance%2520directly%2520trained%2520on%2520the%2520original%2520ST%2520data.%2520While%250Alimiting%2520the%2520variety%2520of%2520training%2520data%2520can%2520make%2520training%2520easier%252C%2520it%2520can%2520also%250Alead%2520to%2520a%2520lack%2520of%2520knowledge%2520and%2520information%2520for%2520the%2520model%252C%2520resulting%2520in%2520a%250Adecrease%2520in%2520performance.%2520To%2520address%2520this%2520challenge%252C%2520we%2520presented%2520an%2520innovative%250Aparadigm%2520that%2520incorporates%2520three%2520separate%2520forms%2520of%2520curriculum%2520learning%250Aspecifically%2520targeting%2520from%2520spatial%252C%2520temporal%252C%2520and%2520quantile%2520perspectives.%250AFurthermore%252C%2520our%2520framework%2520incorporates%2520a%2520stacking%2520fusion%2520module%2520to%2520combine%250Adiverse%2520information%2520from%2520three%2520types%2520of%2520curriculum%2520learning%252C%2520resulting%2520in%2520a%250Astrong%2520and%2520thorough%2520learning%2520process.%2520We%2520demonstrated%2520the%2520effectiveness%2520of%2520this%250Aframework%2520with%2520extensive%2520empirical%2520evaluations%252C%2520highlighting%2520its%2520better%250Aperformance%2520in%2520addressing%2520complex%2520ST%2520challenges.%2520We%2520provided%2520thorough%2520ablation%250Astudies%2520to%2520investigate%2520the%2520effectiveness%2520of%2520our%2520curriculum%2520and%2520to%2520explain%2520how%250Ait%2520contributes%2520to%2520the%2520improvement%2520of%2520learning%2520efficiency%2520on%2520ST%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Spatio-temporal%20Quantile%20Forecasting%20with%20Curriculum%20Learning%3A%0A%20%20Lessons%20Learned&entry.906535625=Du%20Yin%20and%20Jinliang%20Deng%20and%20Shuang%20Ao%20and%20Zechen%20Li%20and%20Hao%20Xue%20and%20Arian%20Prabowo%20and%20Renhe%20Jiang%20and%20Xuan%20Song%20and%20Flora%20Salim&entry.1292438233=%20%20Training%20models%20on%20spatio-temporal%20%28ST%29%20data%20poses%20an%20open%20problem%20due%20to%20the%0Acomplicated%20and%20diverse%20nature%20of%20the%20data%20itself%2C%20and%20it%20is%20challenging%20to%0Aensure%20the%20model%27s%20performance%20directly%20trained%20on%20the%20original%20ST%20data.%20While%0Alimiting%20the%20variety%20of%20training%20data%20can%20make%20training%20easier%2C%20it%20can%20also%0Alead%20to%20a%20lack%20of%20knowledge%20and%20information%20for%20the%20model%2C%20resulting%20in%20a%0Adecrease%20in%20performance.%20To%20address%20this%20challenge%2C%20we%20presented%20an%20innovative%0Aparadigm%20that%20incorporates%20three%20separate%20forms%20of%20curriculum%20learning%0Aspecifically%20targeting%20from%20spatial%2C%20temporal%2C%20and%20quantile%20perspectives.%0AFurthermore%2C%20our%20framework%20incorporates%20a%20stacking%20fusion%20module%20to%20combine%0Adiverse%20information%20from%20three%20types%20of%20curriculum%20learning%2C%20resulting%20in%20a%0Astrong%20and%20thorough%20learning%20process.%20We%20demonstrated%20the%20effectiveness%20of%20this%0Aframework%20with%20extensive%20empirical%20evaluations%2C%20highlighting%20its%20better%0Aperformance%20in%20addressing%20complex%20ST%20challenges.%20We%20provided%20thorough%20ablation%0Astudies%20to%20investigate%20the%20effectiveness%20of%20our%20curriculum%20and%20to%20explain%20how%0Ait%20contributes%20to%20the%20improvement%20of%20learning%20efficiency%20on%20ST%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12709v1&entry.124074799=Read"},
{"title": "Demystifying Higher-Order Graph Neural Networks", "author": "Maciej Besta and Florian Scheidl and Lukas Gianinazzi and Shachar Klaiman and J\u00fcrgen M\u00fcller and Torsten Hoefler", "abstract": "  Higher-order graph neural networks (HOGNNs) are an important class of GNN\nmodels that harness polyadic relations between vertices beyond plain edges.\nThey have been used to eliminate issues such as over-smoothing or\nover-squashing, to significantly enhance the accuracy of GNN predictions, to\nimprove the expressiveness of GNN architectures, and for numerous other goals.\nA plethora of HOGNN models have been introduced, and they come with diverse\nneural architectures, and even with different notions of what the\n\"higher-order\" means. This richness makes it very challenging to appropriately\nanalyze and compare HOGNN models, and to decide in what scenario to use\nspecific ones. To alleviate this, we first design an in-depth taxonomy and a\nblueprint for HOGNNs. This facilitates designing models that maximize\nperformance. Then, we use our taxonomy to analyze and compare the available\nHOGNN models. The outcomes of our analysis are synthesized in a set of insights\nthat help to select the most beneficial GNN model in a given scenario, and a\ncomprehensive list of challenges and opportunities for further research into\nmore powerful HOGNNs.\n", "link": "http://arxiv.org/abs/2406.12841v1", "date": "2024-06-18", "relevancy": 2.3923, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4846}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4807}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20Higher-Order%20Graph%20Neural%20Networks&body=Title%3A%20Demystifying%20Higher-Order%20Graph%20Neural%20Networks%0AAuthor%3A%20Maciej%20Besta%20and%20Florian%20Scheidl%20and%20Lukas%20Gianinazzi%20and%20Shachar%20Klaiman%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20Torsten%20Hoefler%0AAbstract%3A%20%20%20Higher-order%20graph%20neural%20networks%20%28HOGNNs%29%20are%20an%20important%20class%20of%20GNN%0Amodels%20that%20harness%20polyadic%20relations%20between%20vertices%20beyond%20plain%20edges.%0AThey%20have%20been%20used%20to%20eliminate%20issues%20such%20as%20over-smoothing%20or%0Aover-squashing%2C%20to%20significantly%20enhance%20the%20accuracy%20of%20GNN%20predictions%2C%20to%0Aimprove%20the%20expressiveness%20of%20GNN%20architectures%2C%20and%20for%20numerous%20other%20goals.%0AA%20plethora%20of%20HOGNN%20models%20have%20been%20introduced%2C%20and%20they%20come%20with%20diverse%0Aneural%20architectures%2C%20and%20even%20with%20different%20notions%20of%20what%20the%0A%22higher-order%22%20means.%20This%20richness%20makes%20it%20very%20challenging%20to%20appropriately%0Aanalyze%20and%20compare%20HOGNN%20models%2C%20and%20to%20decide%20in%20what%20scenario%20to%20use%0Aspecific%20ones.%20To%20alleviate%20this%2C%20we%20first%20design%20an%20in-depth%20taxonomy%20and%20a%0Ablueprint%20for%20HOGNNs.%20This%20facilitates%20designing%20models%20that%20maximize%0Aperformance.%20Then%2C%20we%20use%20our%20taxonomy%20to%20analyze%20and%20compare%20the%20available%0AHOGNN%20models.%20The%20outcomes%20of%20our%20analysis%20are%20synthesized%20in%20a%20set%20of%20insights%0Athat%20help%20to%20select%20the%20most%20beneficial%20GNN%20model%20in%20a%20given%20scenario%2C%20and%20a%0Acomprehensive%20list%20of%20challenges%20and%20opportunities%20for%20further%20research%20into%0Amore%20powerful%20HOGNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520Higher-Order%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMaciej%2520Besta%2520and%2520Florian%2520Scheidl%2520and%2520Lukas%2520Gianinazzi%2520and%2520Shachar%2520Klaiman%2520and%2520J%25C3%25BCrgen%2520M%25C3%25BCller%2520and%2520Torsten%2520Hoefler%26entry.1292438233%3D%2520%2520Higher-order%2520graph%2520neural%2520networks%2520%2528HOGNNs%2529%2520are%2520an%2520important%2520class%2520of%2520GNN%250Amodels%2520that%2520harness%2520polyadic%2520relations%2520between%2520vertices%2520beyond%2520plain%2520edges.%250AThey%2520have%2520been%2520used%2520to%2520eliminate%2520issues%2520such%2520as%2520over-smoothing%2520or%250Aover-squashing%252C%2520to%2520significantly%2520enhance%2520the%2520accuracy%2520of%2520GNN%2520predictions%252C%2520to%250Aimprove%2520the%2520expressiveness%2520of%2520GNN%2520architectures%252C%2520and%2520for%2520numerous%2520other%2520goals.%250AA%2520plethora%2520of%2520HOGNN%2520models%2520have%2520been%2520introduced%252C%2520and%2520they%2520come%2520with%2520diverse%250Aneural%2520architectures%252C%2520and%2520even%2520with%2520different%2520notions%2520of%2520what%2520the%250A%2522higher-order%2522%2520means.%2520This%2520richness%2520makes%2520it%2520very%2520challenging%2520to%2520appropriately%250Aanalyze%2520and%2520compare%2520HOGNN%2520models%252C%2520and%2520to%2520decide%2520in%2520what%2520scenario%2520to%2520use%250Aspecific%2520ones.%2520To%2520alleviate%2520this%252C%2520we%2520first%2520design%2520an%2520in-depth%2520taxonomy%2520and%2520a%250Ablueprint%2520for%2520HOGNNs.%2520This%2520facilitates%2520designing%2520models%2520that%2520maximize%250Aperformance.%2520Then%252C%2520we%2520use%2520our%2520taxonomy%2520to%2520analyze%2520and%2520compare%2520the%2520available%250AHOGNN%2520models.%2520The%2520outcomes%2520of%2520our%2520analysis%2520are%2520synthesized%2520in%2520a%2520set%2520of%2520insights%250Athat%2520help%2520to%2520select%2520the%2520most%2520beneficial%2520GNN%2520model%2520in%2520a%2520given%2520scenario%252C%2520and%2520a%250Acomprehensive%2520list%2520of%2520challenges%2520and%2520opportunities%2520for%2520further%2520research%2520into%250Amore%2520powerful%2520HOGNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20Higher-Order%20Graph%20Neural%20Networks&entry.906535625=Maciej%20Besta%20and%20Florian%20Scheidl%20and%20Lukas%20Gianinazzi%20and%20Shachar%20Klaiman%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20Torsten%20Hoefler&entry.1292438233=%20%20Higher-order%20graph%20neural%20networks%20%28HOGNNs%29%20are%20an%20important%20class%20of%20GNN%0Amodels%20that%20harness%20polyadic%20relations%20between%20vertices%20beyond%20plain%20edges.%0AThey%20have%20been%20used%20to%20eliminate%20issues%20such%20as%20over-smoothing%20or%0Aover-squashing%2C%20to%20significantly%20enhance%20the%20accuracy%20of%20GNN%20predictions%2C%20to%0Aimprove%20the%20expressiveness%20of%20GNN%20architectures%2C%20and%20for%20numerous%20other%20goals.%0AA%20plethora%20of%20HOGNN%20models%20have%20been%20introduced%2C%20and%20they%20come%20with%20diverse%0Aneural%20architectures%2C%20and%20even%20with%20different%20notions%20of%20what%20the%0A%22higher-order%22%20means.%20This%20richness%20makes%20it%20very%20challenging%20to%20appropriately%0Aanalyze%20and%20compare%20HOGNN%20models%2C%20and%20to%20decide%20in%20what%20scenario%20to%20use%0Aspecific%20ones.%20To%20alleviate%20this%2C%20we%20first%20design%20an%20in-depth%20taxonomy%20and%20a%0Ablueprint%20for%20HOGNNs.%20This%20facilitates%20designing%20models%20that%20maximize%0Aperformance.%20Then%2C%20we%20use%20our%20taxonomy%20to%20analyze%20and%20compare%20the%20available%0AHOGNN%20models.%20The%20outcomes%20of%20our%20analysis%20are%20synthesized%20in%20a%20set%20of%20insights%0Athat%20help%20to%20select%20the%20most%20beneficial%20GNN%20model%20in%20a%20given%20scenario%2C%20and%20a%0Acomprehensive%20list%20of%20challenges%20and%20opportunities%20for%20further%20research%20into%0Amore%20powerful%20HOGNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12841v1&entry.124074799=Read"},
{"title": "News Without Borders: Domain Adaptation of Multilingual Sentence\n  Embeddings for Cross-lingual News Recommendation", "author": "Andreea Iana and Fabian David Schmidt and Goran Glava\u0161 and Heiko Paulheim", "abstract": "  Rapidly growing numbers of multilingual news consumers pose an increasing\nchallenge to news recommender systems in terms of providing customized\nrecommendations. First, existing neural news recommenders, even when powered by\nmultilingual language models (LMs), suffer substantial performance losses in\nzero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of\nfine-tuning the backbone LM of a neural recommender on task-specific data is\ncomputationally expensive and infeasible in few-shot recommendation and\ncold-start setups, where data is scarce or completely unavailable. In this\nwork, we propose a news-adapted sentence encoder (NaSE), domain-specialized\nfrom a pretrained massively multilingual sentence encoder (SE). To this end, we\nconstruct and leverage PolyNews and PolyNewsParallel, two multilingual\nnews-specific corpora. With the news-adapted multilingual SE in place, we test\nthe effectiveness of (i.e., question the need for) supervised fine-tuning for\nnews recommendation, and propose a simple and strong baseline based on (i)\nfrozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE\nachieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot\nnews recommendation.\n", "link": "http://arxiv.org/abs/2406.12634v1", "date": "2024-06-18", "relevancy": 2.3715, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4787}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4782}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20News%20Without%20Borders%3A%20Domain%20Adaptation%20of%20Multilingual%20Sentence%0A%20%20Embeddings%20for%20Cross-lingual%20News%20Recommendation&body=Title%3A%20News%20Without%20Borders%3A%20Domain%20Adaptation%20of%20Multilingual%20Sentence%0A%20%20Embeddings%20for%20Cross-lingual%20News%20Recommendation%0AAuthor%3A%20Andreea%20Iana%20and%20Fabian%20David%20Schmidt%20and%20Goran%20Glava%C5%A1%20and%20Heiko%20Paulheim%0AAbstract%3A%20%20%20Rapidly%20growing%20numbers%20of%20multilingual%20news%20consumers%20pose%20an%20increasing%0Achallenge%20to%20news%20recommender%20systems%20in%20terms%20of%20providing%20customized%0Arecommendations.%20First%2C%20existing%20neural%20news%20recommenders%2C%20even%20when%20powered%20by%0Amultilingual%20language%20models%20%28LMs%29%2C%20suffer%20substantial%20performance%20losses%20in%0Azero-shot%20cross-lingual%20transfer%20%28ZS-XLT%29.%20Second%2C%20the%20current%20paradigm%20of%0Afine-tuning%20the%20backbone%20LM%20of%20a%20neural%20recommender%20on%20task-specific%20data%20is%0Acomputationally%20expensive%20and%20infeasible%20in%20few-shot%20recommendation%20and%0Acold-start%20setups%2C%20where%20data%20is%20scarce%20or%20completely%20unavailable.%20In%20this%0Awork%2C%20we%20propose%20a%20news-adapted%20sentence%20encoder%20%28NaSE%29%2C%20domain-specialized%0Afrom%20a%20pretrained%20massively%20multilingual%20sentence%20encoder%20%28SE%29.%20To%20this%20end%2C%20we%0Aconstruct%20and%20leverage%20PolyNews%20and%20PolyNewsParallel%2C%20two%20multilingual%0Anews-specific%20corpora.%20With%20the%20news-adapted%20multilingual%20SE%20in%20place%2C%20we%20test%0Athe%20effectiveness%20of%20%28i.e.%2C%20question%20the%20need%20for%29%20supervised%20fine-tuning%20for%0Anews%20recommendation%2C%20and%20propose%20a%20simple%20and%20strong%20baseline%20based%20on%20%28i%29%0Afrozen%20NaSE%20embeddings%20and%20%28ii%29%20late%20click-behavior%20fusion.%20We%20show%20that%20NaSE%0Aachieves%20state-of-the-art%20performance%20in%20ZS-XLT%20in%20true%20cold-start%20and%20few-shot%0Anews%20recommendation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNews%2520Without%2520Borders%253A%2520Domain%2520Adaptation%2520of%2520Multilingual%2520Sentence%250A%2520%2520Embeddings%2520for%2520Cross-lingual%2520News%2520Recommendation%26entry.906535625%3DAndreea%2520Iana%2520and%2520Fabian%2520David%2520Schmidt%2520and%2520Goran%2520Glava%25C5%25A1%2520and%2520Heiko%2520Paulheim%26entry.1292438233%3D%2520%2520Rapidly%2520growing%2520numbers%2520of%2520multilingual%2520news%2520consumers%2520pose%2520an%2520increasing%250Achallenge%2520to%2520news%2520recommender%2520systems%2520in%2520terms%2520of%2520providing%2520customized%250Arecommendations.%2520First%252C%2520existing%2520neural%2520news%2520recommenders%252C%2520even%2520when%2520powered%2520by%250Amultilingual%2520language%2520models%2520%2528LMs%2529%252C%2520suffer%2520substantial%2520performance%2520losses%2520in%250Azero-shot%2520cross-lingual%2520transfer%2520%2528ZS-XLT%2529.%2520Second%252C%2520the%2520current%2520paradigm%2520of%250Afine-tuning%2520the%2520backbone%2520LM%2520of%2520a%2520neural%2520recommender%2520on%2520task-specific%2520data%2520is%250Acomputationally%2520expensive%2520and%2520infeasible%2520in%2520few-shot%2520recommendation%2520and%250Acold-start%2520setups%252C%2520where%2520data%2520is%2520scarce%2520or%2520completely%2520unavailable.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520news-adapted%2520sentence%2520encoder%2520%2528NaSE%2529%252C%2520domain-specialized%250Afrom%2520a%2520pretrained%2520massively%2520multilingual%2520sentence%2520encoder%2520%2528SE%2529.%2520To%2520this%2520end%252C%2520we%250Aconstruct%2520and%2520leverage%2520PolyNews%2520and%2520PolyNewsParallel%252C%2520two%2520multilingual%250Anews-specific%2520corpora.%2520With%2520the%2520news-adapted%2520multilingual%2520SE%2520in%2520place%252C%2520we%2520test%250Athe%2520effectiveness%2520of%2520%2528i.e.%252C%2520question%2520the%2520need%2520for%2529%2520supervised%2520fine-tuning%2520for%250Anews%2520recommendation%252C%2520and%2520propose%2520a%2520simple%2520and%2520strong%2520baseline%2520based%2520on%2520%2528i%2529%250Afrozen%2520NaSE%2520embeddings%2520and%2520%2528ii%2529%2520late%2520click-behavior%2520fusion.%2520We%2520show%2520that%2520NaSE%250Aachieves%2520state-of-the-art%2520performance%2520in%2520ZS-XLT%2520in%2520true%2520cold-start%2520and%2520few-shot%250Anews%2520recommendation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=News%20Without%20Borders%3A%20Domain%20Adaptation%20of%20Multilingual%20Sentence%0A%20%20Embeddings%20for%20Cross-lingual%20News%20Recommendation&entry.906535625=Andreea%20Iana%20and%20Fabian%20David%20Schmidt%20and%20Goran%20Glava%C5%A1%20and%20Heiko%20Paulheim&entry.1292438233=%20%20Rapidly%20growing%20numbers%20of%20multilingual%20news%20consumers%20pose%20an%20increasing%0Achallenge%20to%20news%20recommender%20systems%20in%20terms%20of%20providing%20customized%0Arecommendations.%20First%2C%20existing%20neural%20news%20recommenders%2C%20even%20when%20powered%20by%0Amultilingual%20language%20models%20%28LMs%29%2C%20suffer%20substantial%20performance%20losses%20in%0Azero-shot%20cross-lingual%20transfer%20%28ZS-XLT%29.%20Second%2C%20the%20current%20paradigm%20of%0Afine-tuning%20the%20backbone%20LM%20of%20a%20neural%20recommender%20on%20task-specific%20data%20is%0Acomputationally%20expensive%20and%20infeasible%20in%20few-shot%20recommendation%20and%0Acold-start%20setups%2C%20where%20data%20is%20scarce%20or%20completely%20unavailable.%20In%20this%0Awork%2C%20we%20propose%20a%20news-adapted%20sentence%20encoder%20%28NaSE%29%2C%20domain-specialized%0Afrom%20a%20pretrained%20massively%20multilingual%20sentence%20encoder%20%28SE%29.%20To%20this%20end%2C%20we%0Aconstruct%20and%20leverage%20PolyNews%20and%20PolyNewsParallel%2C%20two%20multilingual%0Anews-specific%20corpora.%20With%20the%20news-adapted%20multilingual%20SE%20in%20place%2C%20we%20test%0Athe%20effectiveness%20of%20%28i.e.%2C%20question%20the%20need%20for%29%20supervised%20fine-tuning%20for%0Anews%20recommendation%2C%20and%20propose%20a%20simple%20and%20strong%20baseline%20based%20on%20%28i%29%0Afrozen%20NaSE%20embeddings%20and%20%28ii%29%20late%20click-behavior%20fusion.%20We%20show%20that%20NaSE%0Aachieves%20state-of-the-art%20performance%20in%20ZS-XLT%20in%20true%20cold-start%20and%20few-shot%0Anews%20recommendation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12634v1&entry.124074799=Read"},
{"title": "User Centric Evaluation of Code Generation Tools", "author": "Tanha Miah and Hong Zhu", "abstract": "  With the rapid advance of machine learning (ML) technology, large language\nmodels (LLMs) are increasingly explored as an intelligent tool to generate\nprogram code from natural language specifications. However, existing\nevaluations of LLMs have focused on their capabilities in comparison with\nhumans. It is desirable to evaluate their usability when deciding on whether to\nuse a LLM in software production. This paper proposes a user centric method for\nthis purpose. It includes metadata in the test cases of a benchmark to describe\ntheir usages, conducts testing in a multi-attempt process that mimics the uses\nof LLMs, measures LLM generated solutions on a set of quality attributes that\nreflect usability, and evaluates the performance based on user experiences in\nthe uses of LLMs as a tool.\n  The paper also reports a case study with the method in the evaluation of\nChatGPT's usability as a code generation tool for the R programming language.\nOur experiments demonstrated that ChatGPT is highly useful for generating R\nprogram code although it may fail on hard programming tasks. The user\nexperiences are good with overall average number of attempts being 1.61 and the\naverage time of completion being 47.02 seconds. Our experiments also found that\nthe weakest aspect of usability is conciseness, which has a score of 3.80 out\nof 5.\n", "link": "http://arxiv.org/abs/2402.03130v3", "date": "2024-06-18", "relevancy": 2.3442, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4948}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4686}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20User%20Centric%20Evaluation%20of%20Code%20Generation%20Tools&body=Title%3A%20User%20Centric%20Evaluation%20of%20Code%20Generation%20Tools%0AAuthor%3A%20Tanha%20Miah%20and%20Hong%20Zhu%0AAbstract%3A%20%20%20With%20the%20rapid%20advance%20of%20machine%20learning%20%28ML%29%20technology%2C%20large%20language%0Amodels%20%28LLMs%29%20are%20increasingly%20explored%20as%20an%20intelligent%20tool%20to%20generate%0Aprogram%20code%20from%20natural%20language%20specifications.%20However%2C%20existing%0Aevaluations%20of%20LLMs%20have%20focused%20on%20their%20capabilities%20in%20comparison%20with%0Ahumans.%20It%20is%20desirable%20to%20evaluate%20their%20usability%20when%20deciding%20on%20whether%20to%0Ause%20a%20LLM%20in%20software%20production.%20This%20paper%20proposes%20a%20user%20centric%20method%20for%0Athis%20purpose.%20It%20includes%20metadata%20in%20the%20test%20cases%20of%20a%20benchmark%20to%20describe%0Atheir%20usages%2C%20conducts%20testing%20in%20a%20multi-attempt%20process%20that%20mimics%20the%20uses%0Aof%20LLMs%2C%20measures%20LLM%20generated%20solutions%20on%20a%20set%20of%20quality%20attributes%20that%0Areflect%20usability%2C%20and%20evaluates%20the%20performance%20based%20on%20user%20experiences%20in%0Athe%20uses%20of%20LLMs%20as%20a%20tool.%0A%20%20The%20paper%20also%20reports%20a%20case%20study%20with%20the%20method%20in%20the%20evaluation%20of%0AChatGPT%27s%20usability%20as%20a%20code%20generation%20tool%20for%20the%20R%20programming%20language.%0AOur%20experiments%20demonstrated%20that%20ChatGPT%20is%20highly%20useful%20for%20generating%20R%0Aprogram%20code%20although%20it%20may%20fail%20on%20hard%20programming%20tasks.%20The%20user%0Aexperiences%20are%20good%20with%20overall%20average%20number%20of%20attempts%20being%201.61%20and%20the%0Aaverage%20time%20of%20completion%20being%2047.02%20seconds.%20Our%20experiments%20also%20found%20that%0Athe%20weakest%20aspect%20of%20usability%20is%20conciseness%2C%20which%20has%20a%20score%20of%203.80%20out%0Aof%205.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03130v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUser%2520Centric%2520Evaluation%2520of%2520Code%2520Generation%2520Tools%26entry.906535625%3DTanha%2520Miah%2520and%2520Hong%2520Zhu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advance%2520of%2520machine%2520learning%2520%2528ML%2529%2520technology%252C%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520are%2520increasingly%2520explored%2520as%2520an%2520intelligent%2520tool%2520to%2520generate%250Aprogram%2520code%2520from%2520natural%2520language%2520specifications.%2520However%252C%2520existing%250Aevaluations%2520of%2520LLMs%2520have%2520focused%2520on%2520their%2520capabilities%2520in%2520comparison%2520with%250Ahumans.%2520It%2520is%2520desirable%2520to%2520evaluate%2520their%2520usability%2520when%2520deciding%2520on%2520whether%2520to%250Ause%2520a%2520LLM%2520in%2520software%2520production.%2520This%2520paper%2520proposes%2520a%2520user%2520centric%2520method%2520for%250Athis%2520purpose.%2520It%2520includes%2520metadata%2520in%2520the%2520test%2520cases%2520of%2520a%2520benchmark%2520to%2520describe%250Atheir%2520usages%252C%2520conducts%2520testing%2520in%2520a%2520multi-attempt%2520process%2520that%2520mimics%2520the%2520uses%250Aof%2520LLMs%252C%2520measures%2520LLM%2520generated%2520solutions%2520on%2520a%2520set%2520of%2520quality%2520attributes%2520that%250Areflect%2520usability%252C%2520and%2520evaluates%2520the%2520performance%2520based%2520on%2520user%2520experiences%2520in%250Athe%2520uses%2520of%2520LLMs%2520as%2520a%2520tool.%250A%2520%2520The%2520paper%2520also%2520reports%2520a%2520case%2520study%2520with%2520the%2520method%2520in%2520the%2520evaluation%2520of%250AChatGPT%2527s%2520usability%2520as%2520a%2520code%2520generation%2520tool%2520for%2520the%2520R%2520programming%2520language.%250AOur%2520experiments%2520demonstrated%2520that%2520ChatGPT%2520is%2520highly%2520useful%2520for%2520generating%2520R%250Aprogram%2520code%2520although%2520it%2520may%2520fail%2520on%2520hard%2520programming%2520tasks.%2520The%2520user%250Aexperiences%2520are%2520good%2520with%2520overall%2520average%2520number%2520of%2520attempts%2520being%25201.61%2520and%2520the%250Aaverage%2520time%2520of%2520completion%2520being%252047.02%2520seconds.%2520Our%2520experiments%2520also%2520found%2520that%250Athe%2520weakest%2520aspect%2520of%2520usability%2520is%2520conciseness%252C%2520which%2520has%2520a%2520score%2520of%25203.80%2520out%250Aof%25205.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03130v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=User%20Centric%20Evaluation%20of%20Code%20Generation%20Tools&entry.906535625=Tanha%20Miah%20and%20Hong%20Zhu&entry.1292438233=%20%20With%20the%20rapid%20advance%20of%20machine%20learning%20%28ML%29%20technology%2C%20large%20language%0Amodels%20%28LLMs%29%20are%20increasingly%20explored%20as%20an%20intelligent%20tool%20to%20generate%0Aprogram%20code%20from%20natural%20language%20specifications.%20However%2C%20existing%0Aevaluations%20of%20LLMs%20have%20focused%20on%20their%20capabilities%20in%20comparison%20with%0Ahumans.%20It%20is%20desirable%20to%20evaluate%20their%20usability%20when%20deciding%20on%20whether%20to%0Ause%20a%20LLM%20in%20software%20production.%20This%20paper%20proposes%20a%20user%20centric%20method%20for%0Athis%20purpose.%20It%20includes%20metadata%20in%20the%20test%20cases%20of%20a%20benchmark%20to%20describe%0Atheir%20usages%2C%20conducts%20testing%20in%20a%20multi-attempt%20process%20that%20mimics%20the%20uses%0Aof%20LLMs%2C%20measures%20LLM%20generated%20solutions%20on%20a%20set%20of%20quality%20attributes%20that%0Areflect%20usability%2C%20and%20evaluates%20the%20performance%20based%20on%20user%20experiences%20in%0Athe%20uses%20of%20LLMs%20as%20a%20tool.%0A%20%20The%20paper%20also%20reports%20a%20case%20study%20with%20the%20method%20in%20the%20evaluation%20of%0AChatGPT%27s%20usability%20as%20a%20code%20generation%20tool%20for%20the%20R%20programming%20language.%0AOur%20experiments%20demonstrated%20that%20ChatGPT%20is%20highly%20useful%20for%20generating%20R%0Aprogram%20code%20although%20it%20may%20fail%20on%20hard%20programming%20tasks.%20The%20user%0Aexperiences%20are%20good%20with%20overall%20average%20number%20of%20attempts%20being%201.61%20and%20the%0Aaverage%20time%20of%20completion%20being%2047.02%20seconds.%20Our%20experiments%20also%20found%20that%0Athe%20weakest%20aspect%20of%20usability%20is%20conciseness%2C%20which%20has%20a%20score%20of%203.80%20out%0Aof%205.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03130v3&entry.124074799=Read"},
{"title": "VIA: A Spatiotemporal Video Adaptation Framework for Global and Local\n  Video Editing", "author": "Jing Gu and Yuwei Fang and Ivan Skorokhodov and Peter Wonka and Xinya Du and Sergey Tulyakov and Xin Eric Wang", "abstract": "  Video editing stands as a cornerstone of digital media, from entertainment\nand education to professional communication. However, previous methods often\noverlook the necessity of comprehensively understanding both global and local\ncontexts, leading to inaccurate and inconsistency edits in the spatiotemporal\ndimension, especially for long videos. In this paper, we introduce VIA, a\nunified spatiotemporal VIdeo Adaptation framework for global and local video\nediting, pushing the limits of consistently editing minute-long videos. First,\nto ensure local consistency within individual frames, the foundation of VIA is\na novel test-time editing adaptation method, which adapts a pre-trained image\nediting model for improving consistency between potential editing directions\nand the text instruction, and adapts masked latent variables for precise local\ncontrol. Furthermore, to maintain global consistency over the video sequence,\nwe introduce spatiotemporal adaptation that adapts consistent attention\nvariables in key frames and strategically applies them across the whole\nsequence to realize the editing effects. Extensive experiments demonstrate\nthat, compared to baseline methods, our VIA approach produces edits that are\nmore faithful to the source videos, more coherent in the spatiotemporal\ncontext, and more precise in local control. More importantly, we show that VIA\ncan achieve consistent long video editing in minutes, unlocking the potentials\nfor advanced video editing tasks over long video sequences.\n", "link": "http://arxiv.org/abs/2406.12831v1", "date": "2024-06-18", "relevancy": 2.3412, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6425}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5919}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIA%3A%20A%20Spatiotemporal%20Video%20Adaptation%20Framework%20for%20Global%20and%20Local%0A%20%20Video%20Editing&body=Title%3A%20VIA%3A%20A%20Spatiotemporal%20Video%20Adaptation%20Framework%20for%20Global%20and%20Local%0A%20%20Video%20Editing%0AAuthor%3A%20Jing%20Gu%20and%20Yuwei%20Fang%20and%20Ivan%20Skorokhodov%20and%20Peter%20Wonka%20and%20Xinya%20Du%20and%20Sergey%20Tulyakov%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20%20%20Video%20editing%20stands%20as%20a%20cornerstone%20of%20digital%20media%2C%20from%20entertainment%0Aand%20education%20to%20professional%20communication.%20However%2C%20previous%20methods%20often%0Aoverlook%20the%20necessity%20of%20comprehensively%20understanding%20both%20global%20and%20local%0Acontexts%2C%20leading%20to%20inaccurate%20and%20inconsistency%20edits%20in%20the%20spatiotemporal%0Adimension%2C%20especially%20for%20long%20videos.%20In%20this%20paper%2C%20we%20introduce%20VIA%2C%20a%0Aunified%20spatiotemporal%20VIdeo%20Adaptation%20framework%20for%20global%20and%20local%20video%0Aediting%2C%20pushing%20the%20limits%20of%20consistently%20editing%20minute-long%20videos.%20First%2C%0Ato%20ensure%20local%20consistency%20within%20individual%20frames%2C%20the%20foundation%20of%20VIA%20is%0Aa%20novel%20test-time%20editing%20adaptation%20method%2C%20which%20adapts%20a%20pre-trained%20image%0Aediting%20model%20for%20improving%20consistency%20between%20potential%20editing%20directions%0Aand%20the%20text%20instruction%2C%20and%20adapts%20masked%20latent%20variables%20for%20precise%20local%0Acontrol.%20Furthermore%2C%20to%20maintain%20global%20consistency%20over%20the%20video%20sequence%2C%0Awe%20introduce%20spatiotemporal%20adaptation%20that%20adapts%20consistent%20attention%0Avariables%20in%20key%20frames%20and%20strategically%20applies%20them%20across%20the%20whole%0Asequence%20to%20realize%20the%20editing%20effects.%20Extensive%20experiments%20demonstrate%0Athat%2C%20compared%20to%20baseline%20methods%2C%20our%20VIA%20approach%20produces%20edits%20that%20are%0Amore%20faithful%20to%20the%20source%20videos%2C%20more%20coherent%20in%20the%20spatiotemporal%0Acontext%2C%20and%20more%20precise%20in%20local%20control.%20More%20importantly%2C%20we%20show%20that%20VIA%0Acan%20achieve%20consistent%20long%20video%20editing%20in%20minutes%2C%20unlocking%20the%20potentials%0Afor%20advanced%20video%20editing%20tasks%20over%20long%20video%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIA%253A%2520A%2520Spatiotemporal%2520Video%2520Adaptation%2520Framework%2520for%2520Global%2520and%2520Local%250A%2520%2520Video%2520Editing%26entry.906535625%3DJing%2520Gu%2520and%2520Yuwei%2520Fang%2520and%2520Ivan%2520Skorokhodov%2520and%2520Peter%2520Wonka%2520and%2520Xinya%2520Du%2520and%2520Sergey%2520Tulyakov%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3D%2520%2520Video%2520editing%2520stands%2520as%2520a%2520cornerstone%2520of%2520digital%2520media%252C%2520from%2520entertainment%250Aand%2520education%2520to%2520professional%2520communication.%2520However%252C%2520previous%2520methods%2520often%250Aoverlook%2520the%2520necessity%2520of%2520comprehensively%2520understanding%2520both%2520global%2520and%2520local%250Acontexts%252C%2520leading%2520to%2520inaccurate%2520and%2520inconsistency%2520edits%2520in%2520the%2520spatiotemporal%250Adimension%252C%2520especially%2520for%2520long%2520videos.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VIA%252C%2520a%250Aunified%2520spatiotemporal%2520VIdeo%2520Adaptation%2520framework%2520for%2520global%2520and%2520local%2520video%250Aediting%252C%2520pushing%2520the%2520limits%2520of%2520consistently%2520editing%2520minute-long%2520videos.%2520First%252C%250Ato%2520ensure%2520local%2520consistency%2520within%2520individual%2520frames%252C%2520the%2520foundation%2520of%2520VIA%2520is%250Aa%2520novel%2520test-time%2520editing%2520adaptation%2520method%252C%2520which%2520adapts%2520a%2520pre-trained%2520image%250Aediting%2520model%2520for%2520improving%2520consistency%2520between%2520potential%2520editing%2520directions%250Aand%2520the%2520text%2520instruction%252C%2520and%2520adapts%2520masked%2520latent%2520variables%2520for%2520precise%2520local%250Acontrol.%2520Furthermore%252C%2520to%2520maintain%2520global%2520consistency%2520over%2520the%2520video%2520sequence%252C%250Awe%2520introduce%2520spatiotemporal%2520adaptation%2520that%2520adapts%2520consistent%2520attention%250Avariables%2520in%2520key%2520frames%2520and%2520strategically%2520applies%2520them%2520across%2520the%2520whole%250Asequence%2520to%2520realize%2520the%2520editing%2520effects.%2520Extensive%2520experiments%2520demonstrate%250Athat%252C%2520compared%2520to%2520baseline%2520methods%252C%2520our%2520VIA%2520approach%2520produces%2520edits%2520that%2520are%250Amore%2520faithful%2520to%2520the%2520source%2520videos%252C%2520more%2520coherent%2520in%2520the%2520spatiotemporal%250Acontext%252C%2520and%2520more%2520precise%2520in%2520local%2520control.%2520More%2520importantly%252C%2520we%2520show%2520that%2520VIA%250Acan%2520achieve%2520consistent%2520long%2520video%2520editing%2520in%2520minutes%252C%2520unlocking%2520the%2520potentials%250Afor%2520advanced%2520video%2520editing%2520tasks%2520over%2520long%2520video%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIA%3A%20A%20Spatiotemporal%20Video%20Adaptation%20Framework%20for%20Global%20and%20Local%0A%20%20Video%20Editing&entry.906535625=Jing%20Gu%20and%20Yuwei%20Fang%20and%20Ivan%20Skorokhodov%20and%20Peter%20Wonka%20and%20Xinya%20Du%20and%20Sergey%20Tulyakov%20and%20Xin%20Eric%20Wang&entry.1292438233=%20%20Video%20editing%20stands%20as%20a%20cornerstone%20of%20digital%20media%2C%20from%20entertainment%0Aand%20education%20to%20professional%20communication.%20However%2C%20previous%20methods%20often%0Aoverlook%20the%20necessity%20of%20comprehensively%20understanding%20both%20global%20and%20local%0Acontexts%2C%20leading%20to%20inaccurate%20and%20inconsistency%20edits%20in%20the%20spatiotemporal%0Adimension%2C%20especially%20for%20long%20videos.%20In%20this%20paper%2C%20we%20introduce%20VIA%2C%20a%0Aunified%20spatiotemporal%20VIdeo%20Adaptation%20framework%20for%20global%20and%20local%20video%0Aediting%2C%20pushing%20the%20limits%20of%20consistently%20editing%20minute-long%20videos.%20First%2C%0Ato%20ensure%20local%20consistency%20within%20individual%20frames%2C%20the%20foundation%20of%20VIA%20is%0Aa%20novel%20test-time%20editing%20adaptation%20method%2C%20which%20adapts%20a%20pre-trained%20image%0Aediting%20model%20for%20improving%20consistency%20between%20potential%20editing%20directions%0Aand%20the%20text%20instruction%2C%20and%20adapts%20masked%20latent%20variables%20for%20precise%20local%0Acontrol.%20Furthermore%2C%20to%20maintain%20global%20consistency%20over%20the%20video%20sequence%2C%0Awe%20introduce%20spatiotemporal%20adaptation%20that%20adapts%20consistent%20attention%0Avariables%20in%20key%20frames%20and%20strategically%20applies%20them%20across%20the%20whole%0Asequence%20to%20realize%20the%20editing%20effects.%20Extensive%20experiments%20demonstrate%0Athat%2C%20compared%20to%20baseline%20methods%2C%20our%20VIA%20approach%20produces%20edits%20that%20are%0Amore%20faithful%20to%20the%20source%20videos%2C%20more%20coherent%20in%20the%20spatiotemporal%0Acontext%2C%20and%20more%20precise%20in%20local%20control.%20More%20importantly%2C%20we%20show%20that%20VIA%0Acan%20achieve%20consistent%20long%20video%20editing%20in%20minutes%2C%20unlocking%20the%20potentials%0Afor%20advanced%20video%20editing%20tasks%20over%20long%20video%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12831v1&entry.124074799=Read"},
{"title": "HARE: HumAn pRiors, a key to small language model Efficiency", "author": "Lingyun Zhang and Bin jin and Gaojian Ge and Lunhui Liu and Xuewen Shen and Mingyong Wu and Houqian Zhang and Yongneng Jiang and Shiqi Chen and Shi Pu", "abstract": "  Human priors play a crucial role in efficiently utilizing data in deep\nlearning. However, with the development of large language models (LLMs), there\nis an increasing emphasis on scaling both model size and data volume, which\noften diminishes the importance of human priors in data construction.\nInfluenced by these trends, existing Small Language Models (SLMs) mainly rely\non web-scraped large-scale training data, neglecting the proper incorporation\nof human priors. This oversight limits the training efficiency of language\nmodels in resource-constrained settings. In this paper, we propose a principle\nto leverage human priors for data construction. This principle emphasizes\nachieving high-performance SLMs by training on a concise dataset that\naccommodates both semantic diversity and data quality consistency, while\navoiding benchmark data leakage. Following this principle, we train an SLM\nnamed HARE-1.1B. Extensive experiments on large-scale benchmark datasets\ndemonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs,\nvalidating the effectiveness of the proposed principle. Additionally, this\nprovides new insights into efficient language model training in\nresource-constrained environments from the view of human priors.\n", "link": "http://arxiv.org/abs/2406.11410v2", "date": "2024-06-18", "relevancy": 2.3392, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4592}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HARE%3A%20HumAn%20pRiors%2C%20a%20key%20to%20small%20language%20model%20Efficiency&body=Title%3A%20HARE%3A%20HumAn%20pRiors%2C%20a%20key%20to%20small%20language%20model%20Efficiency%0AAuthor%3A%20Lingyun%20Zhang%20and%20Bin%20jin%20and%20Gaojian%20Ge%20and%20Lunhui%20Liu%20and%20Xuewen%20Shen%20and%20Mingyong%20Wu%20and%20Houqian%20Zhang%20and%20Yongneng%20Jiang%20and%20Shiqi%20Chen%20and%20Shi%20Pu%0AAbstract%3A%20%20%20Human%20priors%20play%20a%20crucial%20role%20in%20efficiently%20utilizing%20data%20in%20deep%0Alearning.%20However%2C%20with%20the%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20there%0Ais%20an%20increasing%20emphasis%20on%20scaling%20both%20model%20size%20and%20data%20volume%2C%20which%0Aoften%20diminishes%20the%20importance%20of%20human%20priors%20in%20data%20construction.%0AInfluenced%20by%20these%20trends%2C%20existing%20Small%20Language%20Models%20%28SLMs%29%20mainly%20rely%0Aon%20web-scraped%20large-scale%20training%20data%2C%20neglecting%20the%20proper%20incorporation%0Aof%20human%20priors.%20This%20oversight%20limits%20the%20training%20efficiency%20of%20language%0Amodels%20in%20resource-constrained%20settings.%20In%20this%20paper%2C%20we%20propose%20a%20principle%0Ato%20leverage%20human%20priors%20for%20data%20construction.%20This%20principle%20emphasizes%0Aachieving%20high-performance%20SLMs%20by%20training%20on%20a%20concise%20dataset%20that%0Aaccommodates%20both%20semantic%20diversity%20and%20data%20quality%20consistency%2C%20while%0Aavoiding%20benchmark%20data%20leakage.%20Following%20this%20principle%2C%20we%20train%20an%20SLM%0Anamed%20HARE-1.1B.%20Extensive%20experiments%20on%20large-scale%20benchmark%20datasets%0Ademonstrate%20that%20HARE-1.1B%20performs%20favorably%20against%20state-of-the-art%20SLMs%2C%0Avalidating%20the%20effectiveness%20of%20the%20proposed%20principle.%20Additionally%2C%20this%0Aprovides%20new%20insights%20into%20efficient%20language%20model%20training%20in%0Aresource-constrained%20environments%20from%20the%20view%20of%20human%20priors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11410v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHARE%253A%2520HumAn%2520pRiors%252C%2520a%2520key%2520to%2520small%2520language%2520model%2520Efficiency%26entry.906535625%3DLingyun%2520Zhang%2520and%2520Bin%2520jin%2520and%2520Gaojian%2520Ge%2520and%2520Lunhui%2520Liu%2520and%2520Xuewen%2520Shen%2520and%2520Mingyong%2520Wu%2520and%2520Houqian%2520Zhang%2520and%2520Yongneng%2520Jiang%2520and%2520Shiqi%2520Chen%2520and%2520Shi%2520Pu%26entry.1292438233%3D%2520%2520Human%2520priors%2520play%2520a%2520crucial%2520role%2520in%2520efficiently%2520utilizing%2520data%2520in%2520deep%250Alearning.%2520However%252C%2520with%2520the%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520there%250Ais%2520an%2520increasing%2520emphasis%2520on%2520scaling%2520both%2520model%2520size%2520and%2520data%2520volume%252C%2520which%250Aoften%2520diminishes%2520the%2520importance%2520of%2520human%2520priors%2520in%2520data%2520construction.%250AInfluenced%2520by%2520these%2520trends%252C%2520existing%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520mainly%2520rely%250Aon%2520web-scraped%2520large-scale%2520training%2520data%252C%2520neglecting%2520the%2520proper%2520incorporation%250Aof%2520human%2520priors.%2520This%2520oversight%2520limits%2520the%2520training%2520efficiency%2520of%2520language%250Amodels%2520in%2520resource-constrained%2520settings.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520principle%250Ato%2520leverage%2520human%2520priors%2520for%2520data%2520construction.%2520This%2520principle%2520emphasizes%250Aachieving%2520high-performance%2520SLMs%2520by%2520training%2520on%2520a%2520concise%2520dataset%2520that%250Aaccommodates%2520both%2520semantic%2520diversity%2520and%2520data%2520quality%2520consistency%252C%2520while%250Aavoiding%2520benchmark%2520data%2520leakage.%2520Following%2520this%2520principle%252C%2520we%2520train%2520an%2520SLM%250Anamed%2520HARE-1.1B.%2520Extensive%2520experiments%2520on%2520large-scale%2520benchmark%2520datasets%250Ademonstrate%2520that%2520HARE-1.1B%2520performs%2520favorably%2520against%2520state-of-the-art%2520SLMs%252C%250Avalidating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520principle.%2520Additionally%252C%2520this%250Aprovides%2520new%2520insights%2520into%2520efficient%2520language%2520model%2520training%2520in%250Aresource-constrained%2520environments%2520from%2520the%2520view%2520of%2520human%2520priors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11410v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HARE%3A%20HumAn%20pRiors%2C%20a%20key%20to%20small%20language%20model%20Efficiency&entry.906535625=Lingyun%20Zhang%20and%20Bin%20jin%20and%20Gaojian%20Ge%20and%20Lunhui%20Liu%20and%20Xuewen%20Shen%20and%20Mingyong%20Wu%20and%20Houqian%20Zhang%20and%20Yongneng%20Jiang%20and%20Shiqi%20Chen%20and%20Shi%20Pu&entry.1292438233=%20%20Human%20priors%20play%20a%20crucial%20role%20in%20efficiently%20utilizing%20data%20in%20deep%0Alearning.%20However%2C%20with%20the%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20there%0Ais%20an%20increasing%20emphasis%20on%20scaling%20both%20model%20size%20and%20data%20volume%2C%20which%0Aoften%20diminishes%20the%20importance%20of%20human%20priors%20in%20data%20construction.%0AInfluenced%20by%20these%20trends%2C%20existing%20Small%20Language%20Models%20%28SLMs%29%20mainly%20rely%0Aon%20web-scraped%20large-scale%20training%20data%2C%20neglecting%20the%20proper%20incorporation%0Aof%20human%20priors.%20This%20oversight%20limits%20the%20training%20efficiency%20of%20language%0Amodels%20in%20resource-constrained%20settings.%20In%20this%20paper%2C%20we%20propose%20a%20principle%0Ato%20leverage%20human%20priors%20for%20data%20construction.%20This%20principle%20emphasizes%0Aachieving%20high-performance%20SLMs%20by%20training%20on%20a%20concise%20dataset%20that%0Aaccommodates%20both%20semantic%20diversity%20and%20data%20quality%20consistency%2C%20while%0Aavoiding%20benchmark%20data%20leakage.%20Following%20this%20principle%2C%20we%20train%20an%20SLM%0Anamed%20HARE-1.1B.%20Extensive%20experiments%20on%20large-scale%20benchmark%20datasets%0Ademonstrate%20that%20HARE-1.1B%20performs%20favorably%20against%20state-of-the-art%20SLMs%2C%0Avalidating%20the%20effectiveness%20of%20the%20proposed%20principle.%20Additionally%2C%20this%0Aprovides%20new%20insights%20into%20efficient%20language%20model%20training%20in%0Aresource-constrained%20environments%20from%20the%20view%20of%20human%20priors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11410v2&entry.124074799=Read"},
{"title": "Graph Neural Networks in Histopathology: Emerging Trends and Future\n  Directions", "author": "Siemen Brussee and Giorgio Buzzanca and Anne M. R. Schrader and Jesper Kers", "abstract": "  Histopathological analysis of Whole Slide Images (WSIs) has seen a surge in\nthe utilization of deep learning methods, particularly Convolutional Neural\nNetworks (CNNs). However, CNNs often fall short in capturing the intricate\nspatial dependencies inherent in WSIs. Graph Neural Networks (GNNs) present a\npromising alternative, adept at directly modeling pairwise interactions and\neffectively discerning the topological tissue and cellular structures within\nWSIs. Recognizing the pressing need for deep learning techniques that harness\nthe topological structure of WSIs, the application of GNNs in histopathology\nhas experienced rapid growth. In this comprehensive review, we survey GNNs in\nhistopathology, discuss their applications, and exploring emerging trends that\npave the way for future advancements in the field. We begin by elucidating the\nfundamentals of GNNs and their potential applications in histopathology.\nLeveraging quantitative literature analysis, we identify four emerging trends:\nHierarchical GNNs, Adaptive Graph Structure Learning, Multimodal GNNs, and\nHigher-order GNNs. Through an in-depth exploration of these trends, we offer\ninsights into the evolving landscape of GNNs in histopathological analysis.\nBased on our findings, we propose future directions to propel the field\nforward. Our analysis serves to guide researchers and practitioners towards\ninnovative approaches and methodologies, fostering advancements in\nhistopathological analysis through the lens of graph neural networks.\n", "link": "http://arxiv.org/abs/2406.12808v1", "date": "2024-06-18", "relevancy": 2.3235, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4871}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4653}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20in%20Histopathology%3A%20Emerging%20Trends%20and%20Future%0A%20%20Directions&body=Title%3A%20Graph%20Neural%20Networks%20in%20Histopathology%3A%20Emerging%20Trends%20and%20Future%0A%20%20Directions%0AAuthor%3A%20Siemen%20Brussee%20and%20Giorgio%20Buzzanca%20and%20Anne%20M.%20R.%20Schrader%20and%20Jesper%20Kers%0AAbstract%3A%20%20%20Histopathological%20analysis%20of%20Whole%20Slide%20Images%20%28WSIs%29%20has%20seen%20a%20surge%20in%0Athe%20utilization%20of%20deep%20learning%20methods%2C%20particularly%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20However%2C%20CNNs%20often%20fall%20short%20in%20capturing%20the%20intricate%0Aspatial%20dependencies%20inherent%20in%20WSIs.%20Graph%20Neural%20Networks%20%28GNNs%29%20present%20a%0Apromising%20alternative%2C%20adept%20at%20directly%20modeling%20pairwise%20interactions%20and%0Aeffectively%20discerning%20the%20topological%20tissue%20and%20cellular%20structures%20within%0AWSIs.%20Recognizing%20the%20pressing%20need%20for%20deep%20learning%20techniques%20that%20harness%0Athe%20topological%20structure%20of%20WSIs%2C%20the%20application%20of%20GNNs%20in%20histopathology%0Ahas%20experienced%20rapid%20growth.%20In%20this%20comprehensive%20review%2C%20we%20survey%20GNNs%20in%0Ahistopathology%2C%20discuss%20their%20applications%2C%20and%20exploring%20emerging%20trends%20that%0Apave%20the%20way%20for%20future%20advancements%20in%20the%20field.%20We%20begin%20by%20elucidating%20the%0Afundamentals%20of%20GNNs%20and%20their%20potential%20applications%20in%20histopathology.%0ALeveraging%20quantitative%20literature%20analysis%2C%20we%20identify%20four%20emerging%20trends%3A%0AHierarchical%20GNNs%2C%20Adaptive%20Graph%20Structure%20Learning%2C%20Multimodal%20GNNs%2C%20and%0AHigher-order%20GNNs.%20Through%20an%20in-depth%20exploration%20of%20these%20trends%2C%20we%20offer%0Ainsights%20into%20the%20evolving%20landscape%20of%20GNNs%20in%20histopathological%20analysis.%0ABased%20on%20our%20findings%2C%20we%20propose%20future%20directions%20to%20propel%20the%20field%0Aforward.%20Our%20analysis%20serves%20to%20guide%20researchers%20and%20practitioners%20towards%0Ainnovative%20approaches%20and%20methodologies%2C%20fostering%20advancements%20in%0Ahistopathological%20analysis%20through%20the%20lens%20of%20graph%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520in%2520Histopathology%253A%2520Emerging%2520Trends%2520and%2520Future%250A%2520%2520Directions%26entry.906535625%3DSiemen%2520Brussee%2520and%2520Giorgio%2520Buzzanca%2520and%2520Anne%2520M.%2520R.%2520Schrader%2520and%2520Jesper%2520Kers%26entry.1292438233%3D%2520%2520Histopathological%2520analysis%2520of%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520has%2520seen%2520a%2520surge%2520in%250Athe%2520utilization%2520of%2520deep%2520learning%2520methods%252C%2520particularly%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529.%2520However%252C%2520CNNs%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520intricate%250Aspatial%2520dependencies%2520inherent%2520in%2520WSIs.%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520present%2520a%250Apromising%2520alternative%252C%2520adept%2520at%2520directly%2520modeling%2520pairwise%2520interactions%2520and%250Aeffectively%2520discerning%2520the%2520topological%2520tissue%2520and%2520cellular%2520structures%2520within%250AWSIs.%2520Recognizing%2520the%2520pressing%2520need%2520for%2520deep%2520learning%2520techniques%2520that%2520harness%250Athe%2520topological%2520structure%2520of%2520WSIs%252C%2520the%2520application%2520of%2520GNNs%2520in%2520histopathology%250Ahas%2520experienced%2520rapid%2520growth.%2520In%2520this%2520comprehensive%2520review%252C%2520we%2520survey%2520GNNs%2520in%250Ahistopathology%252C%2520discuss%2520their%2520applications%252C%2520and%2520exploring%2520emerging%2520trends%2520that%250Apave%2520the%2520way%2520for%2520future%2520advancements%2520in%2520the%2520field.%2520We%2520begin%2520by%2520elucidating%2520the%250Afundamentals%2520of%2520GNNs%2520and%2520their%2520potential%2520applications%2520in%2520histopathology.%250ALeveraging%2520quantitative%2520literature%2520analysis%252C%2520we%2520identify%2520four%2520emerging%2520trends%253A%250AHierarchical%2520GNNs%252C%2520Adaptive%2520Graph%2520Structure%2520Learning%252C%2520Multimodal%2520GNNs%252C%2520and%250AHigher-order%2520GNNs.%2520Through%2520an%2520in-depth%2520exploration%2520of%2520these%2520trends%252C%2520we%2520offer%250Ainsights%2520into%2520the%2520evolving%2520landscape%2520of%2520GNNs%2520in%2520histopathological%2520analysis.%250ABased%2520on%2520our%2520findings%252C%2520we%2520propose%2520future%2520directions%2520to%2520propel%2520the%2520field%250Aforward.%2520Our%2520analysis%2520serves%2520to%2520guide%2520researchers%2520and%2520practitioners%2520towards%250Ainnovative%2520approaches%2520and%2520methodologies%252C%2520fostering%2520advancements%2520in%250Ahistopathological%2520analysis%2520through%2520the%2520lens%2520of%2520graph%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20in%20Histopathology%3A%20Emerging%20Trends%20and%20Future%0A%20%20Directions&entry.906535625=Siemen%20Brussee%20and%20Giorgio%20Buzzanca%20and%20Anne%20M.%20R.%20Schrader%20and%20Jesper%20Kers&entry.1292438233=%20%20Histopathological%20analysis%20of%20Whole%20Slide%20Images%20%28WSIs%29%20has%20seen%20a%20surge%20in%0Athe%20utilization%20of%20deep%20learning%20methods%2C%20particularly%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20However%2C%20CNNs%20often%20fall%20short%20in%20capturing%20the%20intricate%0Aspatial%20dependencies%20inherent%20in%20WSIs.%20Graph%20Neural%20Networks%20%28GNNs%29%20present%20a%0Apromising%20alternative%2C%20adept%20at%20directly%20modeling%20pairwise%20interactions%20and%0Aeffectively%20discerning%20the%20topological%20tissue%20and%20cellular%20structures%20within%0AWSIs.%20Recognizing%20the%20pressing%20need%20for%20deep%20learning%20techniques%20that%20harness%0Athe%20topological%20structure%20of%20WSIs%2C%20the%20application%20of%20GNNs%20in%20histopathology%0Ahas%20experienced%20rapid%20growth.%20In%20this%20comprehensive%20review%2C%20we%20survey%20GNNs%20in%0Ahistopathology%2C%20discuss%20their%20applications%2C%20and%20exploring%20emerging%20trends%20that%0Apave%20the%20way%20for%20future%20advancements%20in%20the%20field.%20We%20begin%20by%20elucidating%20the%0Afundamentals%20of%20GNNs%20and%20their%20potential%20applications%20in%20histopathology.%0ALeveraging%20quantitative%20literature%20analysis%2C%20we%20identify%20four%20emerging%20trends%3A%0AHierarchical%20GNNs%2C%20Adaptive%20Graph%20Structure%20Learning%2C%20Multimodal%20GNNs%2C%20and%0AHigher-order%20GNNs.%20Through%20an%20in-depth%20exploration%20of%20these%20trends%2C%20we%20offer%0Ainsights%20into%20the%20evolving%20landscape%20of%20GNNs%20in%20histopathological%20analysis.%0ABased%20on%20our%20findings%2C%20we%20propose%20future%20directions%20to%20propel%20the%20field%0Aforward.%20Our%20analysis%20serves%20to%20guide%20researchers%20and%20practitioners%20towards%0Ainnovative%20approaches%20and%20methodologies%2C%20fostering%20advancements%20in%0Ahistopathological%20analysis%20through%20the%20lens%20of%20graph%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12808v1&entry.124074799=Read"},
{"title": "Self-Localized Collaborative Perception", "author": "Zhenyang Ni and Zixing Lei and Yifan Lu and Dingju Wang and Chen Feng and Yanfeng Wang and Siheng Chen", "abstract": "  Collaborative perception has garnered considerable attention due to its\ncapacity to address several inherent challenges in single-agent perception,\nincluding occlusion and out-of-range issues. However, existing collaborative\nperception systems heavily rely on precise localization systems to establish a\nconsistent spatial coordinate system between agents. This reliance makes them\nsusceptible to large pose errors or malicious attacks, resulting in substantial\nreductions in perception performance. To address this, we\npropose~$\\mathtt{CoBEVGlue}$, a novel self-localized collaborative perception\nsystem, which achieves more holistic and robust collaboration without using an\nexternal localization system. The core of~$\\mathtt{CoBEVGlue}$ is a novel\nspatial alignment module, which provides the relative poses between agents by\neffectively matching co-visible objects across agents. We validate our method\non both real-world and simulated datasets. The results show that i)\n$\\mathtt{CoBEVGlue}$ achieves state-of-the-art detection performance under\narbitrary localization noises and attacks; and ii) the spatial alignment module\ncan seamlessly integrate with a majority of previous methods, enhancing their\nperformance by an average of $57.7\\%$. Code is available at\nhttps://github.com/VincentNi0107/CoBEVGlue\n", "link": "http://arxiv.org/abs/2406.12712v1", "date": "2024-06-18", "relevancy": 2.3072, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6077}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5652}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Localized%20Collaborative%20Perception&body=Title%3A%20Self-Localized%20Collaborative%20Perception%0AAuthor%3A%20Zhenyang%20Ni%20and%20Zixing%20Lei%20and%20Yifan%20Lu%20and%20Dingju%20Wang%20and%20Chen%20Feng%20and%20Yanfeng%20Wang%20and%20Siheng%20Chen%0AAbstract%3A%20%20%20Collaborative%20perception%20has%20garnered%20considerable%20attention%20due%20to%20its%0Acapacity%20to%20address%20several%20inherent%20challenges%20in%20single-agent%20perception%2C%0Aincluding%20occlusion%20and%20out-of-range%20issues.%20However%2C%20existing%20collaborative%0Aperception%20systems%20heavily%20rely%20on%20precise%20localization%20systems%20to%20establish%20a%0Aconsistent%20spatial%20coordinate%20system%20between%20agents.%20This%20reliance%20makes%20them%0Asusceptible%20to%20large%20pose%20errors%20or%20malicious%20attacks%2C%20resulting%20in%20substantial%0Areductions%20in%20perception%20performance.%20To%20address%20this%2C%20we%0Apropose~%24%5Cmathtt%7BCoBEVGlue%7D%24%2C%20a%20novel%20self-localized%20collaborative%20perception%0Asystem%2C%20which%20achieves%20more%20holistic%20and%20robust%20collaboration%20without%20using%20an%0Aexternal%20localization%20system.%20The%20core%20of~%24%5Cmathtt%7BCoBEVGlue%7D%24%20is%20a%20novel%0Aspatial%20alignment%20module%2C%20which%20provides%20the%20relative%20poses%20between%20agents%20by%0Aeffectively%20matching%20co-visible%20objects%20across%20agents.%20We%20validate%20our%20method%0Aon%20both%20real-world%20and%20simulated%20datasets.%20The%20results%20show%20that%20i%29%0A%24%5Cmathtt%7BCoBEVGlue%7D%24%20achieves%20state-of-the-art%20detection%20performance%20under%0Aarbitrary%20localization%20noises%20and%20attacks%3B%20and%20ii%29%20the%20spatial%20alignment%20module%0Acan%20seamlessly%20integrate%20with%20a%20majority%20of%20previous%20methods%2C%20enhancing%20their%0Aperformance%20by%20an%20average%20of%20%2457.7%5C%25%24.%20Code%20is%20available%20at%0Ahttps%3A//github.com/VincentNi0107/CoBEVGlue%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Localized%2520Collaborative%2520Perception%26entry.906535625%3DZhenyang%2520Ni%2520and%2520Zixing%2520Lei%2520and%2520Yifan%2520Lu%2520and%2520Dingju%2520Wang%2520and%2520Chen%2520Feng%2520and%2520Yanfeng%2520Wang%2520and%2520Siheng%2520Chen%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520has%2520garnered%2520considerable%2520attention%2520due%2520to%2520its%250Acapacity%2520to%2520address%2520several%2520inherent%2520challenges%2520in%2520single-agent%2520perception%252C%250Aincluding%2520occlusion%2520and%2520out-of-range%2520issues.%2520However%252C%2520existing%2520collaborative%250Aperception%2520systems%2520heavily%2520rely%2520on%2520precise%2520localization%2520systems%2520to%2520establish%2520a%250Aconsistent%2520spatial%2520coordinate%2520system%2520between%2520agents.%2520This%2520reliance%2520makes%2520them%250Asusceptible%2520to%2520large%2520pose%2520errors%2520or%2520malicious%2520attacks%252C%2520resulting%2520in%2520substantial%250Areductions%2520in%2520perception%2520performance.%2520To%2520address%2520this%252C%2520we%250Apropose~%2524%255Cmathtt%257BCoBEVGlue%257D%2524%252C%2520a%2520novel%2520self-localized%2520collaborative%2520perception%250Asystem%252C%2520which%2520achieves%2520more%2520holistic%2520and%2520robust%2520collaboration%2520without%2520using%2520an%250Aexternal%2520localization%2520system.%2520The%2520core%2520of~%2524%255Cmathtt%257BCoBEVGlue%257D%2524%2520is%2520a%2520novel%250Aspatial%2520alignment%2520module%252C%2520which%2520provides%2520the%2520relative%2520poses%2520between%2520agents%2520by%250Aeffectively%2520matching%2520co-visible%2520objects%2520across%2520agents.%2520We%2520validate%2520our%2520method%250Aon%2520both%2520real-world%2520and%2520simulated%2520datasets.%2520The%2520results%2520show%2520that%2520i%2529%250A%2524%255Cmathtt%257BCoBEVGlue%257D%2524%2520achieves%2520state-of-the-art%2520detection%2520performance%2520under%250Aarbitrary%2520localization%2520noises%2520and%2520attacks%253B%2520and%2520ii%2529%2520the%2520spatial%2520alignment%2520module%250Acan%2520seamlessly%2520integrate%2520with%2520a%2520majority%2520of%2520previous%2520methods%252C%2520enhancing%2520their%250Aperformance%2520by%2520an%2520average%2520of%2520%252457.7%255C%2525%2524.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/VincentNi0107/CoBEVGlue%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Localized%20Collaborative%20Perception&entry.906535625=Zhenyang%20Ni%20and%20Zixing%20Lei%20and%20Yifan%20Lu%20and%20Dingju%20Wang%20and%20Chen%20Feng%20and%20Yanfeng%20Wang%20and%20Siheng%20Chen&entry.1292438233=%20%20Collaborative%20perception%20has%20garnered%20considerable%20attention%20due%20to%20its%0Acapacity%20to%20address%20several%20inherent%20challenges%20in%20single-agent%20perception%2C%0Aincluding%20occlusion%20and%20out-of-range%20issues.%20However%2C%20existing%20collaborative%0Aperception%20systems%20heavily%20rely%20on%20precise%20localization%20systems%20to%20establish%20a%0Aconsistent%20spatial%20coordinate%20system%20between%20agents.%20This%20reliance%20makes%20them%0Asusceptible%20to%20large%20pose%20errors%20or%20malicious%20attacks%2C%20resulting%20in%20substantial%0Areductions%20in%20perception%20performance.%20To%20address%20this%2C%20we%0Apropose~%24%5Cmathtt%7BCoBEVGlue%7D%24%2C%20a%20novel%20self-localized%20collaborative%20perception%0Asystem%2C%20which%20achieves%20more%20holistic%20and%20robust%20collaboration%20without%20using%20an%0Aexternal%20localization%20system.%20The%20core%20of~%24%5Cmathtt%7BCoBEVGlue%7D%24%20is%20a%20novel%0Aspatial%20alignment%20module%2C%20which%20provides%20the%20relative%20poses%20between%20agents%20by%0Aeffectively%20matching%20co-visible%20objects%20across%20agents.%20We%20validate%20our%20method%0Aon%20both%20real-world%20and%20simulated%20datasets.%20The%20results%20show%20that%20i%29%0A%24%5Cmathtt%7BCoBEVGlue%7D%24%20achieves%20state-of-the-art%20detection%20performance%20under%0Aarbitrary%20localization%20noises%20and%20attacks%3B%20and%20ii%29%20the%20spatial%20alignment%20module%0Acan%20seamlessly%20integrate%20with%20a%20majority%20of%20previous%20methods%2C%20enhancing%20their%0Aperformance%20by%20an%20average%20of%20%2457.7%5C%25%24.%20Code%20is%20available%20at%0Ahttps%3A//github.com/VincentNi0107/CoBEVGlue%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12712v1&entry.124074799=Read"},
{"title": "Decentralized Multi-Robot Line-of-Sight Connectivity Maintenance under\n  Uncertainty", "author": "Yupeng Yang and Yiwei Lyu and Yanze Zhang and Sha Yi and Wenhao Luo", "abstract": "  In this paper, we propose a novel decentralized control method to maintain\nLine-of-Sight connectivity for multi-robot networks in the presence of\nGuassian-distributed localization uncertainty. In contrast to most existing\nwork that assumes perfect positional information about robots or enforces\noverly restrictive rigid formation against uncertainty, our method enables\nrobots to preserve Line-of-Sight connectivity with high probability under\nunbounded Gaussian-like positional noises while remaining minimally intrusive\nto the original robots' tasks. This is achieved by a motion coordination\nframework that jointly optimizes the set of existing Line-of-Sight edges to\npreserve and control revisions to the nominal task-related controllers, subject\nto the safety constraints and the corresponding composition of\nuncertainty-aware Line-of-Sight control constraints. Such compositional control\nconstraints, expressed by our novel notion of probabilistic Line-of-Sight\nconnectivity barrier certificates (PrLOS-CBC) for pairwise robots using control\nbarrier functions, explicitly characterize the deterministic admissible control\nspace for the two robots. The resulting motion ensures Line-of-Sight\nconnectedness for the robot team with high probability. Furthermore, we propose\na fully decentralized algorithm that decomposes the motion coordination\nframework by interleaving the composite constraint specification and solving\nfor the resulting optimization-based controllers. The optimality of our\napproach is justified by the theoretical proofs. Simulation and real-world\nexperiments results are given to demonstrate the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2406.12802v1", "date": "2024-06-18", "relevancy": 2.2963, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6153}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5735}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Multi-Robot%20Line-of-Sight%20Connectivity%20Maintenance%20under%0A%20%20Uncertainty&body=Title%3A%20Decentralized%20Multi-Robot%20Line-of-Sight%20Connectivity%20Maintenance%20under%0A%20%20Uncertainty%0AAuthor%3A%20Yupeng%20Yang%20and%20Yiwei%20Lyu%20and%20Yanze%20Zhang%20and%20Sha%20Yi%20and%20Wenhao%20Luo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20decentralized%20control%20method%20to%20maintain%0ALine-of-Sight%20connectivity%20for%20multi-robot%20networks%20in%20the%20presence%20of%0AGuassian-distributed%20localization%20uncertainty.%20In%20contrast%20to%20most%20existing%0Awork%20that%20assumes%20perfect%20positional%20information%20about%20robots%20or%20enforces%0Aoverly%20restrictive%20rigid%20formation%20against%20uncertainty%2C%20our%20method%20enables%0Arobots%20to%20preserve%20Line-of-Sight%20connectivity%20with%20high%20probability%20under%0Aunbounded%20Gaussian-like%20positional%20noises%20while%20remaining%20minimally%20intrusive%0Ato%20the%20original%20robots%27%20tasks.%20This%20is%20achieved%20by%20a%20motion%20coordination%0Aframework%20that%20jointly%20optimizes%20the%20set%20of%20existing%20Line-of-Sight%20edges%20to%0Apreserve%20and%20control%20revisions%20to%20the%20nominal%20task-related%20controllers%2C%20subject%0Ato%20the%20safety%20constraints%20and%20the%20corresponding%20composition%20of%0Auncertainty-aware%20Line-of-Sight%20control%20constraints.%20Such%20compositional%20control%0Aconstraints%2C%20expressed%20by%20our%20novel%20notion%20of%20probabilistic%20Line-of-Sight%0Aconnectivity%20barrier%20certificates%20%28PrLOS-CBC%29%20for%20pairwise%20robots%20using%20control%0Abarrier%20functions%2C%20explicitly%20characterize%20the%20deterministic%20admissible%20control%0Aspace%20for%20the%20two%20robots.%20The%20resulting%20motion%20ensures%20Line-of-Sight%0Aconnectedness%20for%20the%20robot%20team%20with%20high%20probability.%20Furthermore%2C%20we%20propose%0Aa%20fully%20decentralized%20algorithm%20that%20decomposes%20the%20motion%20coordination%0Aframework%20by%20interleaving%20the%20composite%20constraint%20specification%20and%20solving%0Afor%20the%20resulting%20optimization-based%20controllers.%20The%20optimality%20of%20our%0Aapproach%20is%20justified%20by%20the%20theoretical%20proofs.%20Simulation%20and%20real-world%0Aexperiments%20results%20are%20given%20to%20demonstrate%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Multi-Robot%2520Line-of-Sight%2520Connectivity%2520Maintenance%2520under%250A%2520%2520Uncertainty%26entry.906535625%3DYupeng%2520Yang%2520and%2520Yiwei%2520Lyu%2520and%2520Yanze%2520Zhang%2520and%2520Sha%2520Yi%2520and%2520Wenhao%2520Luo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520decentralized%2520control%2520method%2520to%2520maintain%250ALine-of-Sight%2520connectivity%2520for%2520multi-robot%2520networks%2520in%2520the%2520presence%2520of%250AGuassian-distributed%2520localization%2520uncertainty.%2520In%2520contrast%2520to%2520most%2520existing%250Awork%2520that%2520assumes%2520perfect%2520positional%2520information%2520about%2520robots%2520or%2520enforces%250Aoverly%2520restrictive%2520rigid%2520formation%2520against%2520uncertainty%252C%2520our%2520method%2520enables%250Arobots%2520to%2520preserve%2520Line-of-Sight%2520connectivity%2520with%2520high%2520probability%2520under%250Aunbounded%2520Gaussian-like%2520positional%2520noises%2520while%2520remaining%2520minimally%2520intrusive%250Ato%2520the%2520original%2520robots%2527%2520tasks.%2520This%2520is%2520achieved%2520by%2520a%2520motion%2520coordination%250Aframework%2520that%2520jointly%2520optimizes%2520the%2520set%2520of%2520existing%2520Line-of-Sight%2520edges%2520to%250Apreserve%2520and%2520control%2520revisions%2520to%2520the%2520nominal%2520task-related%2520controllers%252C%2520subject%250Ato%2520the%2520safety%2520constraints%2520and%2520the%2520corresponding%2520composition%2520of%250Auncertainty-aware%2520Line-of-Sight%2520control%2520constraints.%2520Such%2520compositional%2520control%250Aconstraints%252C%2520expressed%2520by%2520our%2520novel%2520notion%2520of%2520probabilistic%2520Line-of-Sight%250Aconnectivity%2520barrier%2520certificates%2520%2528PrLOS-CBC%2529%2520for%2520pairwise%2520robots%2520using%2520control%250Abarrier%2520functions%252C%2520explicitly%2520characterize%2520the%2520deterministic%2520admissible%2520control%250Aspace%2520for%2520the%2520two%2520robots.%2520The%2520resulting%2520motion%2520ensures%2520Line-of-Sight%250Aconnectedness%2520for%2520the%2520robot%2520team%2520with%2520high%2520probability.%2520Furthermore%252C%2520we%2520propose%250Aa%2520fully%2520decentralized%2520algorithm%2520that%2520decomposes%2520the%2520motion%2520coordination%250Aframework%2520by%2520interleaving%2520the%2520composite%2520constraint%2520specification%2520and%2520solving%250Afor%2520the%2520resulting%2520optimization-based%2520controllers.%2520The%2520optimality%2520of%2520our%250Aapproach%2520is%2520justified%2520by%2520the%2520theoretical%2520proofs.%2520Simulation%2520and%2520real-world%250Aexperiments%2520results%2520are%2520given%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Multi-Robot%20Line-of-Sight%20Connectivity%20Maintenance%20under%0A%20%20Uncertainty&entry.906535625=Yupeng%20Yang%20and%20Yiwei%20Lyu%20and%20Yanze%20Zhang%20and%20Sha%20Yi%20and%20Wenhao%20Luo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20decentralized%20control%20method%20to%20maintain%0ALine-of-Sight%20connectivity%20for%20multi-robot%20networks%20in%20the%20presence%20of%0AGuassian-distributed%20localization%20uncertainty.%20In%20contrast%20to%20most%20existing%0Awork%20that%20assumes%20perfect%20positional%20information%20about%20robots%20or%20enforces%0Aoverly%20restrictive%20rigid%20formation%20against%20uncertainty%2C%20our%20method%20enables%0Arobots%20to%20preserve%20Line-of-Sight%20connectivity%20with%20high%20probability%20under%0Aunbounded%20Gaussian-like%20positional%20noises%20while%20remaining%20minimally%20intrusive%0Ato%20the%20original%20robots%27%20tasks.%20This%20is%20achieved%20by%20a%20motion%20coordination%0Aframework%20that%20jointly%20optimizes%20the%20set%20of%20existing%20Line-of-Sight%20edges%20to%0Apreserve%20and%20control%20revisions%20to%20the%20nominal%20task-related%20controllers%2C%20subject%0Ato%20the%20safety%20constraints%20and%20the%20corresponding%20composition%20of%0Auncertainty-aware%20Line-of-Sight%20control%20constraints.%20Such%20compositional%20control%0Aconstraints%2C%20expressed%20by%20our%20novel%20notion%20of%20probabilistic%20Line-of-Sight%0Aconnectivity%20barrier%20certificates%20%28PrLOS-CBC%29%20for%20pairwise%20robots%20using%20control%0Abarrier%20functions%2C%20explicitly%20characterize%20the%20deterministic%20admissible%20control%0Aspace%20for%20the%20two%20robots.%20The%20resulting%20motion%20ensures%20Line-of-Sight%0Aconnectedness%20for%20the%20robot%20team%20with%20high%20probability.%20Furthermore%2C%20we%20propose%0Aa%20fully%20decentralized%20algorithm%20that%20decomposes%20the%20motion%20coordination%0Aframework%20by%20interleaving%20the%20composite%20constraint%20specification%20and%20solving%0Afor%20the%20resulting%20optimization-based%20controllers.%20The%20optimality%20of%20our%0Aapproach%20is%20justified%20by%20the%20theoretical%20proofs.%20Simulation%20and%20real-world%0Aexperiments%20results%20are%20given%20to%20demonstrate%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12802v1&entry.124074799=Read"},
{"title": "What If We Recaption Billions of Web Images with LLaMA-3?", "author": "Xianhang Li and Haoqin Tu and Mude Hui and Zeyu Wang and Bingchen Zhao and Junfei Xiao and Sucheng Ren and Jieru Mei and Qing Liu and Huangjie Zheng and Yuyin Zhou and Cihang Xie", "abstract": "  Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate\nthat semantically aligning and enriching textual descriptions of these pairs\ncan significantly enhance model training across various vision-language tasks,\nparticularly text-to-image generation. However, large-scale investigations in\nthis area remain predominantly closed-source. Our paper aims to bridge this\ncommunity effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a\nGPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a\nLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images\nfrom the DataComp-1B dataset. Our empirical results confirm that this enhanced\ndataset, Recap-DataComp-1B, offers substantial benefits in training advanced\nvision-language models. For discriminative models like CLIP, we observe\nenhanced zero-shot performance in cross-modal retrieval tasks. For generative\nmodels like text-to-image Diffusion Transformers, the generated images exhibit\na significant improvement in alignment with users' text instructions,\nespecially in following complex queries. Our project page is\nhttps://www.haqtu.me/Recap-Datacomp-1B/\n", "link": "http://arxiv.org/abs/2406.08478v2", "date": "2024-06-18", "relevancy": 2.2869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6091}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5461}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20If%20We%20Recaption%20Billions%20of%20Web%20Images%20with%20LLaMA-3%3F&body=Title%3A%20What%20If%20We%20Recaption%20Billions%20of%20Web%20Images%20with%20LLaMA-3%3F%0AAuthor%3A%20Xianhang%20Li%20and%20Haoqin%20Tu%20and%20Mude%20Hui%20and%20Zeyu%20Wang%20and%20Bingchen%20Zhao%20and%20Junfei%20Xiao%20and%20Sucheng%20Ren%20and%20Jieru%20Mei%20and%20Qing%20Liu%20and%20Huangjie%20Zheng%20and%20Yuyin%20Zhou%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20Web-crawled%20image-text%20pairs%20are%20inherently%20noisy.%20Prior%20studies%20demonstrate%0Athat%20semantically%20aligning%20and%20enriching%20textual%20descriptions%20of%20these%20pairs%0Acan%20significantly%20enhance%20model%20training%20across%20various%20vision-language%20tasks%2C%0Aparticularly%20text-to-image%20generation.%20However%2C%20large-scale%20investigations%20in%0Athis%20area%20remain%20predominantly%20closed-source.%20Our%20paper%20aims%20to%20bridge%20this%0Acommunity%20effort%2C%20leveraging%20the%20powerful%20and%20%5Ctextit%7Bopen-sourced%7D%20LLaMA-3%2C%20a%0AGPT-4%20level%20LLM.%20Our%20recaptioning%20pipeline%20is%20simple%3A%20first%2C%20we%20fine-tune%20a%0ALLaMA-3-8B%20powered%20LLaVA-1.5%20and%20then%20employ%20it%20to%20recaption%201.3%20billion%20images%0Afrom%20the%20DataComp-1B%20dataset.%20Our%20empirical%20results%20confirm%20that%20this%20enhanced%0Adataset%2C%20Recap-DataComp-1B%2C%20offers%20substantial%20benefits%20in%20training%20advanced%0Avision-language%20models.%20For%20discriminative%20models%20like%20CLIP%2C%20we%20observe%0Aenhanced%20zero-shot%20performance%20in%20cross-modal%20retrieval%20tasks.%20For%20generative%0Amodels%20like%20text-to-image%20Diffusion%20Transformers%2C%20the%20generated%20images%20exhibit%0Aa%20significant%20improvement%20in%20alignment%20with%20users%27%20text%20instructions%2C%0Aespecially%20in%20following%20complex%20queries.%20Our%20project%20page%20is%0Ahttps%3A//www.haqtu.me/Recap-Datacomp-1B/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520If%2520We%2520Recaption%2520Billions%2520of%2520Web%2520Images%2520with%2520LLaMA-3%253F%26entry.906535625%3DXianhang%2520Li%2520and%2520Haoqin%2520Tu%2520and%2520Mude%2520Hui%2520and%2520Zeyu%2520Wang%2520and%2520Bingchen%2520Zhao%2520and%2520Junfei%2520Xiao%2520and%2520Sucheng%2520Ren%2520and%2520Jieru%2520Mei%2520and%2520Qing%2520Liu%2520and%2520Huangjie%2520Zheng%2520and%2520Yuyin%2520Zhou%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520Web-crawled%2520image-text%2520pairs%2520are%2520inherently%2520noisy.%2520Prior%2520studies%2520demonstrate%250Athat%2520semantically%2520aligning%2520and%2520enriching%2520textual%2520descriptions%2520of%2520these%2520pairs%250Acan%2520significantly%2520enhance%2520model%2520training%2520across%2520various%2520vision-language%2520tasks%252C%250Aparticularly%2520text-to-image%2520generation.%2520However%252C%2520large-scale%2520investigations%2520in%250Athis%2520area%2520remain%2520predominantly%2520closed-source.%2520Our%2520paper%2520aims%2520to%2520bridge%2520this%250Acommunity%2520effort%252C%2520leveraging%2520the%2520powerful%2520and%2520%255Ctextit%257Bopen-sourced%257D%2520LLaMA-3%252C%2520a%250AGPT-4%2520level%2520LLM.%2520Our%2520recaptioning%2520pipeline%2520is%2520simple%253A%2520first%252C%2520we%2520fine-tune%2520a%250ALLaMA-3-8B%2520powered%2520LLaVA-1.5%2520and%2520then%2520employ%2520it%2520to%2520recaption%25201.3%2520billion%2520images%250Afrom%2520the%2520DataComp-1B%2520dataset.%2520Our%2520empirical%2520results%2520confirm%2520that%2520this%2520enhanced%250Adataset%252C%2520Recap-DataComp-1B%252C%2520offers%2520substantial%2520benefits%2520in%2520training%2520advanced%250Avision-language%2520models.%2520For%2520discriminative%2520models%2520like%2520CLIP%252C%2520we%2520observe%250Aenhanced%2520zero-shot%2520performance%2520in%2520cross-modal%2520retrieval%2520tasks.%2520For%2520generative%250Amodels%2520like%2520text-to-image%2520Diffusion%2520Transformers%252C%2520the%2520generated%2520images%2520exhibit%250Aa%2520significant%2520improvement%2520in%2520alignment%2520with%2520users%2527%2520text%2520instructions%252C%250Aespecially%2520in%2520following%2520complex%2520queries.%2520Our%2520project%2520page%2520is%250Ahttps%253A//www.haqtu.me/Recap-Datacomp-1B/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20If%20We%20Recaption%20Billions%20of%20Web%20Images%20with%20LLaMA-3%3F&entry.906535625=Xianhang%20Li%20and%20Haoqin%20Tu%20and%20Mude%20Hui%20and%20Zeyu%20Wang%20and%20Bingchen%20Zhao%20and%20Junfei%20Xiao%20and%20Sucheng%20Ren%20and%20Jieru%20Mei%20and%20Qing%20Liu%20and%20Huangjie%20Zheng%20and%20Yuyin%20Zhou%20and%20Cihang%20Xie&entry.1292438233=%20%20Web-crawled%20image-text%20pairs%20are%20inherently%20noisy.%20Prior%20studies%20demonstrate%0Athat%20semantically%20aligning%20and%20enriching%20textual%20descriptions%20of%20these%20pairs%0Acan%20significantly%20enhance%20model%20training%20across%20various%20vision-language%20tasks%2C%0Aparticularly%20text-to-image%20generation.%20However%2C%20large-scale%20investigations%20in%0Athis%20area%20remain%20predominantly%20closed-source.%20Our%20paper%20aims%20to%20bridge%20this%0Acommunity%20effort%2C%20leveraging%20the%20powerful%20and%20%5Ctextit%7Bopen-sourced%7D%20LLaMA-3%2C%20a%0AGPT-4%20level%20LLM.%20Our%20recaptioning%20pipeline%20is%20simple%3A%20first%2C%20we%20fine-tune%20a%0ALLaMA-3-8B%20powered%20LLaVA-1.5%20and%20then%20employ%20it%20to%20recaption%201.3%20billion%20images%0Afrom%20the%20DataComp-1B%20dataset.%20Our%20empirical%20results%20confirm%20that%20this%20enhanced%0Adataset%2C%20Recap-DataComp-1B%2C%20offers%20substantial%20benefits%20in%20training%20advanced%0Avision-language%20models.%20For%20discriminative%20models%20like%20CLIP%2C%20we%20observe%0Aenhanced%20zero-shot%20performance%20in%20cross-modal%20retrieval%20tasks.%20For%20generative%0Amodels%20like%20text-to-image%20Diffusion%20Transformers%2C%20the%20generated%20images%20exhibit%0Aa%20significant%20improvement%20in%20alignment%20with%20users%27%20text%20instructions%2C%0Aespecially%20in%20following%20complex%20queries.%20Our%20project%20page%20is%0Ahttps%3A//www.haqtu.me/Recap-Datacomp-1B/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08478v2&entry.124074799=Read"},
{"title": "SUPER: Selfie Undistortion and Head Pose Editing with Identity\n  Preservation", "author": "Polina Karpikova and Andrei Spiridonov and Anna Vorontsova and Anastasia Yaschenko and Ekaterina Radionova and Igor Medvedev and Alexander Limonov", "abstract": "  Self-portraits captured from a short distance might look unnatural or even\nunattractive due to heavy distortions making facial features malformed, and\nill-placed head poses. In this paper, we propose SUPER, a novel method of\neliminating distortions and adjusting head pose in a close-up face crop. We\nperform 3D GAN inversion for a facial image by optimizing camera parameters and\nface latent code, which gives a generated image. Besides, we estimate depth\nfrom the obtained latent code, create a depth-induced 3D mesh, and render it\nwith updated camera parameters to obtain a warped portrait. Finally, we apply\nthe visibility-based blending so that visible regions are reprojected, and\noccluded parts are restored with a generative model. Experiments on face\nundistortion benchmarks and on our self-collected Head Rotation dataset (HeRo),\nshow that SUPER outperforms previous approaches both qualitatively and\nquantitatively, opening new possibilities for photorealistic selfie editing.\n", "link": "http://arxiv.org/abs/2406.12700v1", "date": "2024-06-18", "relevancy": 2.2768, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.7106}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.55}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUPER%3A%20Selfie%20Undistortion%20and%20Head%20Pose%20Editing%20with%20Identity%0A%20%20Preservation&body=Title%3A%20SUPER%3A%20Selfie%20Undistortion%20and%20Head%20Pose%20Editing%20with%20Identity%0A%20%20Preservation%0AAuthor%3A%20Polina%20Karpikova%20and%20Andrei%20Spiridonov%20and%20Anna%20Vorontsova%20and%20Anastasia%20Yaschenko%20and%20Ekaterina%20Radionova%20and%20Igor%20Medvedev%20and%20Alexander%20Limonov%0AAbstract%3A%20%20%20Self-portraits%20captured%20from%20a%20short%20distance%20might%20look%20unnatural%20or%20even%0Aunattractive%20due%20to%20heavy%20distortions%20making%20facial%20features%20malformed%2C%20and%0Aill-placed%20head%20poses.%20In%20this%20paper%2C%20we%20propose%20SUPER%2C%20a%20novel%20method%20of%0Aeliminating%20distortions%20and%20adjusting%20head%20pose%20in%20a%20close-up%20face%20crop.%20We%0Aperform%203D%20GAN%20inversion%20for%20a%20facial%20image%20by%20optimizing%20camera%20parameters%20and%0Aface%20latent%20code%2C%20which%20gives%20a%20generated%20image.%20Besides%2C%20we%20estimate%20depth%0Afrom%20the%20obtained%20latent%20code%2C%20create%20a%20depth-induced%203D%20mesh%2C%20and%20render%20it%0Awith%20updated%20camera%20parameters%20to%20obtain%20a%20warped%20portrait.%20Finally%2C%20we%20apply%0Athe%20visibility-based%20blending%20so%20that%20visible%20regions%20are%20reprojected%2C%20and%0Aoccluded%20parts%20are%20restored%20with%20a%20generative%20model.%20Experiments%20on%20face%0Aundistortion%20benchmarks%20and%20on%20our%20self-collected%20Head%20Rotation%20dataset%20%28HeRo%29%2C%0Ashow%20that%20SUPER%20outperforms%20previous%20approaches%20both%20qualitatively%20and%0Aquantitatively%2C%20opening%20new%20possibilities%20for%20photorealistic%20selfie%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUPER%253A%2520Selfie%2520Undistortion%2520and%2520Head%2520Pose%2520Editing%2520with%2520Identity%250A%2520%2520Preservation%26entry.906535625%3DPolina%2520Karpikova%2520and%2520Andrei%2520Spiridonov%2520and%2520Anna%2520Vorontsova%2520and%2520Anastasia%2520Yaschenko%2520and%2520Ekaterina%2520Radionova%2520and%2520Igor%2520Medvedev%2520and%2520Alexander%2520Limonov%26entry.1292438233%3D%2520%2520Self-portraits%2520captured%2520from%2520a%2520short%2520distance%2520might%2520look%2520unnatural%2520or%2520even%250Aunattractive%2520due%2520to%2520heavy%2520distortions%2520making%2520facial%2520features%2520malformed%252C%2520and%250Aill-placed%2520head%2520poses.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SUPER%252C%2520a%2520novel%2520method%2520of%250Aeliminating%2520distortions%2520and%2520adjusting%2520head%2520pose%2520in%2520a%2520close-up%2520face%2520crop.%2520We%250Aperform%25203D%2520GAN%2520inversion%2520for%2520a%2520facial%2520image%2520by%2520optimizing%2520camera%2520parameters%2520and%250Aface%2520latent%2520code%252C%2520which%2520gives%2520a%2520generated%2520image.%2520Besides%252C%2520we%2520estimate%2520depth%250Afrom%2520the%2520obtained%2520latent%2520code%252C%2520create%2520a%2520depth-induced%25203D%2520mesh%252C%2520and%2520render%2520it%250Awith%2520updated%2520camera%2520parameters%2520to%2520obtain%2520a%2520warped%2520portrait.%2520Finally%252C%2520we%2520apply%250Athe%2520visibility-based%2520blending%2520so%2520that%2520visible%2520regions%2520are%2520reprojected%252C%2520and%250Aoccluded%2520parts%2520are%2520restored%2520with%2520a%2520generative%2520model.%2520Experiments%2520on%2520face%250Aundistortion%2520benchmarks%2520and%2520on%2520our%2520self-collected%2520Head%2520Rotation%2520dataset%2520%2528HeRo%2529%252C%250Ashow%2520that%2520SUPER%2520outperforms%2520previous%2520approaches%2520both%2520qualitatively%2520and%250Aquantitatively%252C%2520opening%2520new%2520possibilities%2520for%2520photorealistic%2520selfie%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUPER%3A%20Selfie%20Undistortion%20and%20Head%20Pose%20Editing%20with%20Identity%0A%20%20Preservation&entry.906535625=Polina%20Karpikova%20and%20Andrei%20Spiridonov%20and%20Anna%20Vorontsova%20and%20Anastasia%20Yaschenko%20and%20Ekaterina%20Radionova%20and%20Igor%20Medvedev%20and%20Alexander%20Limonov&entry.1292438233=%20%20Self-portraits%20captured%20from%20a%20short%20distance%20might%20look%20unnatural%20or%20even%0Aunattractive%20due%20to%20heavy%20distortions%20making%20facial%20features%20malformed%2C%20and%0Aill-placed%20head%20poses.%20In%20this%20paper%2C%20we%20propose%20SUPER%2C%20a%20novel%20method%20of%0Aeliminating%20distortions%20and%20adjusting%20head%20pose%20in%20a%20close-up%20face%20crop.%20We%0Aperform%203D%20GAN%20inversion%20for%20a%20facial%20image%20by%20optimizing%20camera%20parameters%20and%0Aface%20latent%20code%2C%20which%20gives%20a%20generated%20image.%20Besides%2C%20we%20estimate%20depth%0Afrom%20the%20obtained%20latent%20code%2C%20create%20a%20depth-induced%203D%20mesh%2C%20and%20render%20it%0Awith%20updated%20camera%20parameters%20to%20obtain%20a%20warped%20portrait.%20Finally%2C%20we%20apply%0Athe%20visibility-based%20blending%20so%20that%20visible%20regions%20are%20reprojected%2C%20and%0Aoccluded%20parts%20are%20restored%20with%20a%20generative%20model.%20Experiments%20on%20face%0Aundistortion%20benchmarks%20and%20on%20our%20self-collected%20Head%20Rotation%20dataset%20%28HeRo%29%2C%0Ashow%20that%20SUPER%20outperforms%20previous%20approaches%20both%20qualitatively%20and%0Aquantitatively%2C%20opening%20new%20possibilities%20for%20photorealistic%20selfie%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12700v1&entry.124074799=Read"},
{"title": "Privacy Preserving Federated Learning in Medical Imaging with\n  Uncertainty Estimation", "author": "Nikolas Koutsoubis and Yasin Yilmaz and Ravi P. Ramachandran and Matthew Schabath and Ghulam Rasool", "abstract": "  Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable\nadvancements, particularly in healthcare. Within medical imaging, ML models\nhold the promise of improving disease diagnoses, treatment planning, and\npost-treatment monitoring. Various computer vision tasks like image\nclassification, object detection, and image segmentation are poised to become\nroutine in clinical analysis. However, privacy concerns surrounding patient\ndata hinder the assembly of large training datasets needed for developing and\ntraining accurate, robust, and generalizable models. Federated Learning (FL)\nemerges as a compelling solution, enabling organizations to collaborate on ML\nmodel training by sharing model training information (gradients) rather than\ndata (e.g., medical images). FL's distributed learning framework facilitates\ninter-institutional collaboration while preserving patient privacy. However,\nFL, while robust in privacy preservation, faces several challenges. Sensitive\ninformation can still be gleaned from shared gradients that are passed on\nbetween organizations during model training. Additionally, in medical imaging,\nquantifying model confidence\\uncertainty accurately is crucial due to the noise\nand artifacts present in the data. Uncertainty estimation in FL encounters\nunique hurdles due to data heterogeneity across organizations. This paper\noffers a comprehensive review of FL, privacy preservation, and uncertainty\nestimation, with a focus on medical imaging. Alongside a survey of current\nresearch, we identify gaps in the field and suggest future directions for FL\nresearch to enhance privacy and address noisy medical imaging data challenges.\n", "link": "http://arxiv.org/abs/2406.12815v1", "date": "2024-06-18", "relevancy": 2.2707, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6197}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5915}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy%20Preserving%20Federated%20Learning%20in%20Medical%20Imaging%20with%0A%20%20Uncertainty%20Estimation&body=Title%3A%20Privacy%20Preserving%20Federated%20Learning%20in%20Medical%20Imaging%20with%0A%20%20Uncertainty%20Estimation%0AAuthor%3A%20Nikolas%20Koutsoubis%20and%20Yasin%20Yilmaz%20and%20Ravi%20P.%20Ramachandran%20and%20Matthew%20Schabath%20and%20Ghulam%20Rasool%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20and%20Artificial%20Intelligence%20%28AI%29%20have%20fueled%20remarkable%0Aadvancements%2C%20particularly%20in%20healthcare.%20Within%20medical%20imaging%2C%20ML%20models%0Ahold%20the%20promise%20of%20improving%20disease%20diagnoses%2C%20treatment%20planning%2C%20and%0Apost-treatment%20monitoring.%20Various%20computer%20vision%20tasks%20like%20image%0Aclassification%2C%20object%20detection%2C%20and%20image%20segmentation%20are%20poised%20to%20become%0Aroutine%20in%20clinical%20analysis.%20However%2C%20privacy%20concerns%20surrounding%20patient%0Adata%20hinder%20the%20assembly%20of%20large%20training%20datasets%20needed%20for%20developing%20and%0Atraining%20accurate%2C%20robust%2C%20and%20generalizable%20models.%20Federated%20Learning%20%28FL%29%0Aemerges%20as%20a%20compelling%20solution%2C%20enabling%20organizations%20to%20collaborate%20on%20ML%0Amodel%20training%20by%20sharing%20model%20training%20information%20%28gradients%29%20rather%20than%0Adata%20%28e.g.%2C%20medical%20images%29.%20FL%27s%20distributed%20learning%20framework%20facilitates%0Ainter-institutional%20collaboration%20while%20preserving%20patient%20privacy.%20However%2C%0AFL%2C%20while%20robust%20in%20privacy%20preservation%2C%20faces%20several%20challenges.%20Sensitive%0Ainformation%20can%20still%20be%20gleaned%20from%20shared%20gradients%20that%20are%20passed%20on%0Abetween%20organizations%20during%20model%20training.%20Additionally%2C%20in%20medical%20imaging%2C%0Aquantifying%20model%20confidence%5Cuncertainty%20accurately%20is%20crucial%20due%20to%20the%20noise%0Aand%20artifacts%20present%20in%20the%20data.%20Uncertainty%20estimation%20in%20FL%20encounters%0Aunique%20hurdles%20due%20to%20data%20heterogeneity%20across%20organizations.%20This%20paper%0Aoffers%20a%20comprehensive%20review%20of%20FL%2C%20privacy%20preservation%2C%20and%20uncertainty%0Aestimation%2C%20with%20a%20focus%20on%20medical%20imaging.%20Alongside%20a%20survey%20of%20current%0Aresearch%2C%20we%20identify%20gaps%20in%20the%20field%20and%20suggest%20future%20directions%20for%20FL%0Aresearch%20to%20enhance%20privacy%20and%20address%20noisy%20medical%20imaging%20data%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy%2520Preserving%2520Federated%2520Learning%2520in%2520Medical%2520Imaging%2520with%250A%2520%2520Uncertainty%2520Estimation%26entry.906535625%3DNikolas%2520Koutsoubis%2520and%2520Yasin%2520Yilmaz%2520and%2520Ravi%2520P.%2520Ramachandran%2520and%2520Matthew%2520Schabath%2520and%2520Ghulam%2520Rasool%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520and%2520Artificial%2520Intelligence%2520%2528AI%2529%2520have%2520fueled%2520remarkable%250Aadvancements%252C%2520particularly%2520in%2520healthcare.%2520Within%2520medical%2520imaging%252C%2520ML%2520models%250Ahold%2520the%2520promise%2520of%2520improving%2520disease%2520diagnoses%252C%2520treatment%2520planning%252C%2520and%250Apost-treatment%2520monitoring.%2520Various%2520computer%2520vision%2520tasks%2520like%2520image%250Aclassification%252C%2520object%2520detection%252C%2520and%2520image%2520segmentation%2520are%2520poised%2520to%2520become%250Aroutine%2520in%2520clinical%2520analysis.%2520However%252C%2520privacy%2520concerns%2520surrounding%2520patient%250Adata%2520hinder%2520the%2520assembly%2520of%2520large%2520training%2520datasets%2520needed%2520for%2520developing%2520and%250Atraining%2520accurate%252C%2520robust%252C%2520and%2520generalizable%2520models.%2520Federated%2520Learning%2520%2528FL%2529%250Aemerges%2520as%2520a%2520compelling%2520solution%252C%2520enabling%2520organizations%2520to%2520collaborate%2520on%2520ML%250Amodel%2520training%2520by%2520sharing%2520model%2520training%2520information%2520%2528gradients%2529%2520rather%2520than%250Adata%2520%2528e.g.%252C%2520medical%2520images%2529.%2520FL%2527s%2520distributed%2520learning%2520framework%2520facilitates%250Ainter-institutional%2520collaboration%2520while%2520preserving%2520patient%2520privacy.%2520However%252C%250AFL%252C%2520while%2520robust%2520in%2520privacy%2520preservation%252C%2520faces%2520several%2520challenges.%2520Sensitive%250Ainformation%2520can%2520still%2520be%2520gleaned%2520from%2520shared%2520gradients%2520that%2520are%2520passed%2520on%250Abetween%2520organizations%2520during%2520model%2520training.%2520Additionally%252C%2520in%2520medical%2520imaging%252C%250Aquantifying%2520model%2520confidence%255Cuncertainty%2520accurately%2520is%2520crucial%2520due%2520to%2520the%2520noise%250Aand%2520artifacts%2520present%2520in%2520the%2520data.%2520Uncertainty%2520estimation%2520in%2520FL%2520encounters%250Aunique%2520hurdles%2520due%2520to%2520data%2520heterogeneity%2520across%2520organizations.%2520This%2520paper%250Aoffers%2520a%2520comprehensive%2520review%2520of%2520FL%252C%2520privacy%2520preservation%252C%2520and%2520uncertainty%250Aestimation%252C%2520with%2520a%2520focus%2520on%2520medical%2520imaging.%2520Alongside%2520a%2520survey%2520of%2520current%250Aresearch%252C%2520we%2520identify%2520gaps%2520in%2520the%2520field%2520and%2520suggest%2520future%2520directions%2520for%2520FL%250Aresearch%2520to%2520enhance%2520privacy%2520and%2520address%2520noisy%2520medical%2520imaging%2520data%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy%20Preserving%20Federated%20Learning%20in%20Medical%20Imaging%20with%0A%20%20Uncertainty%20Estimation&entry.906535625=Nikolas%20Koutsoubis%20and%20Yasin%20Yilmaz%20and%20Ravi%20P.%20Ramachandran%20and%20Matthew%20Schabath%20and%20Ghulam%20Rasool&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20and%20Artificial%20Intelligence%20%28AI%29%20have%20fueled%20remarkable%0Aadvancements%2C%20particularly%20in%20healthcare.%20Within%20medical%20imaging%2C%20ML%20models%0Ahold%20the%20promise%20of%20improving%20disease%20diagnoses%2C%20treatment%20planning%2C%20and%0Apost-treatment%20monitoring.%20Various%20computer%20vision%20tasks%20like%20image%0Aclassification%2C%20object%20detection%2C%20and%20image%20segmentation%20are%20poised%20to%20become%0Aroutine%20in%20clinical%20analysis.%20However%2C%20privacy%20concerns%20surrounding%20patient%0Adata%20hinder%20the%20assembly%20of%20large%20training%20datasets%20needed%20for%20developing%20and%0Atraining%20accurate%2C%20robust%2C%20and%20generalizable%20models.%20Federated%20Learning%20%28FL%29%0Aemerges%20as%20a%20compelling%20solution%2C%20enabling%20organizations%20to%20collaborate%20on%20ML%0Amodel%20training%20by%20sharing%20model%20training%20information%20%28gradients%29%20rather%20than%0Adata%20%28e.g.%2C%20medical%20images%29.%20FL%27s%20distributed%20learning%20framework%20facilitates%0Ainter-institutional%20collaboration%20while%20preserving%20patient%20privacy.%20However%2C%0AFL%2C%20while%20robust%20in%20privacy%20preservation%2C%20faces%20several%20challenges.%20Sensitive%0Ainformation%20can%20still%20be%20gleaned%20from%20shared%20gradients%20that%20are%20passed%20on%0Abetween%20organizations%20during%20model%20training.%20Additionally%2C%20in%20medical%20imaging%2C%0Aquantifying%20model%20confidence%5Cuncertainty%20accurately%20is%20crucial%20due%20to%20the%20noise%0Aand%20artifacts%20present%20in%20the%20data.%20Uncertainty%20estimation%20in%20FL%20encounters%0Aunique%20hurdles%20due%20to%20data%20heterogeneity%20across%20organizations.%20This%20paper%0Aoffers%20a%20comprehensive%20review%20of%20FL%2C%20privacy%20preservation%2C%20and%20uncertainty%0Aestimation%2C%20with%20a%20focus%20on%20medical%20imaging.%20Alongside%20a%20survey%20of%20current%0Aresearch%2C%20we%20identify%20gaps%20in%20the%20field%20and%20suggest%20future%20directions%20for%20FL%0Aresearch%20to%20enhance%20privacy%20and%20address%20noisy%20medical%20imaging%20data%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12815v1&entry.124074799=Read"},
{"title": "GeoBench: Benchmarking and Analyzing Monocular Geometry Estimation\n  Models", "author": "Yongtao Ge and Guangkai Xu and Zhiyue Zhao and Libo Sun and Zheng Huang and Yanlong Sun and Hao Chen and Chunhua Shen", "abstract": "  Recent advances in discriminative and generative pretraining have yielded\ngeometry estimation models with strong generalization capabilities. While\ndiscriminative monocular geometry estimation methods rely on large-scale\nfine-tuning data to achieve zero-shot generalization, several generative-based\nparadigms show the potential of achieving impressive generalization performance\non unseen scenes by leveraging pre-trained diffusion models and fine-tuning on\neven a small scale of synthetic training data. Frustratingly, these models are\ntrained with different recipes on different datasets, making it hard to find\nout the critical factors that determine the evaluation performance. Besides,\ncurrent geometry evaluation benchmarks have two main drawbacks that may prevent\nthe development of the field, i.e., limited scene diversity and unfavorable\nlabel quality. To resolve the above issues, (1) we build fair and strong\nbaselines in a unified codebase for evaluating and analyzing the geometry\nestimation models; (2) we evaluate monocular geometry estimators on more\nchallenging benchmarks for geometry estimation task with diverse scenes and\nhigh-quality annotations. Our results reveal that pre-trained using large data,\ndiscriminative models such as DINOv2, can outperform generative counterparts\nwith a small amount of high-quality synthetic data under the same training\nconfiguration, which suggests that fine-tuning data quality is a more important\nfactor than the data scale and model architecture. Our observation also raises\na question: if simply fine-tuning a general vision model such as DINOv2 using a\nsmall amount of synthetic depth data produces SOTA results, do we really need\ncomplex generative models for depth estimation? We believe this work can propel\nadvancements in geometry estimation tasks as well as a wide range of downstream\napplications.\n", "link": "http://arxiv.org/abs/2406.12671v1", "date": "2024-06-18", "relevancy": 2.2476, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5899}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.554}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoBench%3A%20Benchmarking%20and%20Analyzing%20Monocular%20Geometry%20Estimation%0A%20%20Models&body=Title%3A%20GeoBench%3A%20Benchmarking%20and%20Analyzing%20Monocular%20Geometry%20Estimation%0A%20%20Models%0AAuthor%3A%20Yongtao%20Ge%20and%20Guangkai%20Xu%20and%20Zhiyue%20Zhao%20and%20Libo%20Sun%20and%20Zheng%20Huang%20and%20Yanlong%20Sun%20and%20Hao%20Chen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Recent%20advances%20in%20discriminative%20and%20generative%20pretraining%20have%20yielded%0Ageometry%20estimation%20models%20with%20strong%20generalization%20capabilities.%20While%0Adiscriminative%20monocular%20geometry%20estimation%20methods%20rely%20on%20large-scale%0Afine-tuning%20data%20to%20achieve%20zero-shot%20generalization%2C%20several%20generative-based%0Aparadigms%20show%20the%20potential%20of%20achieving%20impressive%20generalization%20performance%0Aon%20unseen%20scenes%20by%20leveraging%20pre-trained%20diffusion%20models%20and%20fine-tuning%20on%0Aeven%20a%20small%20scale%20of%20synthetic%20training%20data.%20Frustratingly%2C%20these%20models%20are%0Atrained%20with%20different%20recipes%20on%20different%20datasets%2C%20making%20it%20hard%20to%20find%0Aout%20the%20critical%20factors%20that%20determine%20the%20evaluation%20performance.%20Besides%2C%0Acurrent%20geometry%20evaluation%20benchmarks%20have%20two%20main%20drawbacks%20that%20may%20prevent%0Athe%20development%20of%20the%20field%2C%20i.e.%2C%20limited%20scene%20diversity%20and%20unfavorable%0Alabel%20quality.%20To%20resolve%20the%20above%20issues%2C%20%281%29%20we%20build%20fair%20and%20strong%0Abaselines%20in%20a%20unified%20codebase%20for%20evaluating%20and%20analyzing%20the%20geometry%0Aestimation%20models%3B%20%282%29%20we%20evaluate%20monocular%20geometry%20estimators%20on%20more%0Achallenging%20benchmarks%20for%20geometry%20estimation%20task%20with%20diverse%20scenes%20and%0Ahigh-quality%20annotations.%20Our%20results%20reveal%20that%20pre-trained%20using%20large%20data%2C%0Adiscriminative%20models%20such%20as%20DINOv2%2C%20can%20outperform%20generative%20counterparts%0Awith%20a%20small%20amount%20of%20high-quality%20synthetic%20data%20under%20the%20same%20training%0Aconfiguration%2C%20which%20suggests%20that%20fine-tuning%20data%20quality%20is%20a%20more%20important%0Afactor%20than%20the%20data%20scale%20and%20model%20architecture.%20Our%20observation%20also%20raises%0Aa%20question%3A%20if%20simply%20fine-tuning%20a%20general%20vision%20model%20such%20as%20DINOv2%20using%20a%0Asmall%20amount%20of%20synthetic%20depth%20data%20produces%20SOTA%20results%2C%20do%20we%20really%20need%0Acomplex%20generative%20models%20for%20depth%20estimation%3F%20We%20believe%20this%20work%20can%20propel%0Aadvancements%20in%20geometry%20estimation%20tasks%20as%20well%20as%20a%20wide%20range%20of%20downstream%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoBench%253A%2520Benchmarking%2520and%2520Analyzing%2520Monocular%2520Geometry%2520Estimation%250A%2520%2520Models%26entry.906535625%3DYongtao%2520Ge%2520and%2520Guangkai%2520Xu%2520and%2520Zhiyue%2520Zhao%2520and%2520Libo%2520Sun%2520and%2520Zheng%2520Huang%2520and%2520Yanlong%2520Sun%2520and%2520Hao%2520Chen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520discriminative%2520and%2520generative%2520pretraining%2520have%2520yielded%250Ageometry%2520estimation%2520models%2520with%2520strong%2520generalization%2520capabilities.%2520While%250Adiscriminative%2520monocular%2520geometry%2520estimation%2520methods%2520rely%2520on%2520large-scale%250Afine-tuning%2520data%2520to%2520achieve%2520zero-shot%2520generalization%252C%2520several%2520generative-based%250Aparadigms%2520show%2520the%2520potential%2520of%2520achieving%2520impressive%2520generalization%2520performance%250Aon%2520unseen%2520scenes%2520by%2520leveraging%2520pre-trained%2520diffusion%2520models%2520and%2520fine-tuning%2520on%250Aeven%2520a%2520small%2520scale%2520of%2520synthetic%2520training%2520data.%2520Frustratingly%252C%2520these%2520models%2520are%250Atrained%2520with%2520different%2520recipes%2520on%2520different%2520datasets%252C%2520making%2520it%2520hard%2520to%2520find%250Aout%2520the%2520critical%2520factors%2520that%2520determine%2520the%2520evaluation%2520performance.%2520Besides%252C%250Acurrent%2520geometry%2520evaluation%2520benchmarks%2520have%2520two%2520main%2520drawbacks%2520that%2520may%2520prevent%250Athe%2520development%2520of%2520the%2520field%252C%2520i.e.%252C%2520limited%2520scene%2520diversity%2520and%2520unfavorable%250Alabel%2520quality.%2520To%2520resolve%2520the%2520above%2520issues%252C%2520%25281%2529%2520we%2520build%2520fair%2520and%2520strong%250Abaselines%2520in%2520a%2520unified%2520codebase%2520for%2520evaluating%2520and%2520analyzing%2520the%2520geometry%250Aestimation%2520models%253B%2520%25282%2529%2520we%2520evaluate%2520monocular%2520geometry%2520estimators%2520on%2520more%250Achallenging%2520benchmarks%2520for%2520geometry%2520estimation%2520task%2520with%2520diverse%2520scenes%2520and%250Ahigh-quality%2520annotations.%2520Our%2520results%2520reveal%2520that%2520pre-trained%2520using%2520large%2520data%252C%250Adiscriminative%2520models%2520such%2520as%2520DINOv2%252C%2520can%2520outperform%2520generative%2520counterparts%250Awith%2520a%2520small%2520amount%2520of%2520high-quality%2520synthetic%2520data%2520under%2520the%2520same%2520training%250Aconfiguration%252C%2520which%2520suggests%2520that%2520fine-tuning%2520data%2520quality%2520is%2520a%2520more%2520important%250Afactor%2520than%2520the%2520data%2520scale%2520and%2520model%2520architecture.%2520Our%2520observation%2520also%2520raises%250Aa%2520question%253A%2520if%2520simply%2520fine-tuning%2520a%2520general%2520vision%2520model%2520such%2520as%2520DINOv2%2520using%2520a%250Asmall%2520amount%2520of%2520synthetic%2520depth%2520data%2520produces%2520SOTA%2520results%252C%2520do%2520we%2520really%2520need%250Acomplex%2520generative%2520models%2520for%2520depth%2520estimation%253F%2520We%2520believe%2520this%2520work%2520can%2520propel%250Aadvancements%2520in%2520geometry%2520estimation%2520tasks%2520as%2520well%2520as%2520a%2520wide%2520range%2520of%2520downstream%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoBench%3A%20Benchmarking%20and%20Analyzing%20Monocular%20Geometry%20Estimation%0A%20%20Models&entry.906535625=Yongtao%20Ge%20and%20Guangkai%20Xu%20and%20Zhiyue%20Zhao%20and%20Libo%20Sun%20and%20Zheng%20Huang%20and%20Yanlong%20Sun%20and%20Hao%20Chen%20and%20Chunhua%20Shen&entry.1292438233=%20%20Recent%20advances%20in%20discriminative%20and%20generative%20pretraining%20have%20yielded%0Ageometry%20estimation%20models%20with%20strong%20generalization%20capabilities.%20While%0Adiscriminative%20monocular%20geometry%20estimation%20methods%20rely%20on%20large-scale%0Afine-tuning%20data%20to%20achieve%20zero-shot%20generalization%2C%20several%20generative-based%0Aparadigms%20show%20the%20potential%20of%20achieving%20impressive%20generalization%20performance%0Aon%20unseen%20scenes%20by%20leveraging%20pre-trained%20diffusion%20models%20and%20fine-tuning%20on%0Aeven%20a%20small%20scale%20of%20synthetic%20training%20data.%20Frustratingly%2C%20these%20models%20are%0Atrained%20with%20different%20recipes%20on%20different%20datasets%2C%20making%20it%20hard%20to%20find%0Aout%20the%20critical%20factors%20that%20determine%20the%20evaluation%20performance.%20Besides%2C%0Acurrent%20geometry%20evaluation%20benchmarks%20have%20two%20main%20drawbacks%20that%20may%20prevent%0Athe%20development%20of%20the%20field%2C%20i.e.%2C%20limited%20scene%20diversity%20and%20unfavorable%0Alabel%20quality.%20To%20resolve%20the%20above%20issues%2C%20%281%29%20we%20build%20fair%20and%20strong%0Abaselines%20in%20a%20unified%20codebase%20for%20evaluating%20and%20analyzing%20the%20geometry%0Aestimation%20models%3B%20%282%29%20we%20evaluate%20monocular%20geometry%20estimators%20on%20more%0Achallenging%20benchmarks%20for%20geometry%20estimation%20task%20with%20diverse%20scenes%20and%0Ahigh-quality%20annotations.%20Our%20results%20reveal%20that%20pre-trained%20using%20large%20data%2C%0Adiscriminative%20models%20such%20as%20DINOv2%2C%20can%20outperform%20generative%20counterparts%0Awith%20a%20small%20amount%20of%20high-quality%20synthetic%20data%20under%20the%20same%20training%0Aconfiguration%2C%20which%20suggests%20that%20fine-tuning%20data%20quality%20is%20a%20more%20important%0Afactor%20than%20the%20data%20scale%20and%20model%20architecture.%20Our%20observation%20also%20raises%0Aa%20question%3A%20if%20simply%20fine-tuning%20a%20general%20vision%20model%20such%20as%20DINOv2%20using%20a%0Asmall%20amount%20of%20synthetic%20depth%20data%20produces%20SOTA%20results%2C%20do%20we%20really%20need%0Acomplex%20generative%20models%20for%20depth%20estimation%3F%20We%20believe%20this%20work%20can%20propel%0Aadvancements%20in%20geometry%20estimation%20tasks%20as%20well%20as%20a%20wide%20range%20of%20downstream%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12671v1&entry.124074799=Read"},
{"title": "How structured are the representations in transformer-based vision\n  encoders? An analysis of multi-object representations in vision-language\n  models", "author": "Tarun Khajuria and Braian Olmiro Dias and Jaan Aru", "abstract": "  Forming and using symbol-like structured representations for reasoning has\nbeen considered essential for generalising over novel inputs. The primary tool\nthat allows generalisation outside training data distribution is the ability to\nabstract away irrelevant information into a compact form relevant to the task.\nAn extreme form of such abstract representations is symbols. Humans make use of\nsymbols to bind information while abstracting away irrelevant parts to utilise\nthe information consistently and meaningfully. This work estimates the state of\nsuch structured representations in vision encoders. Specifically, we evaluate\nimage encoders in large vision-language pre-trained models to address the\nquestion of which desirable properties their representations lack by applying\nthe criteria of symbolic structured reasoning described for LLMs to the image\nmodels. We test the representation space of image encoders like VIT, BLIP,\nCLIP, and FLAVA to characterise the distribution of the object representations\nin these models. In particular, we create decoding tasks using multi-object\nscenes from the COCO dataset, relating the token space to its input content for\nvarious objects in the scene. We use these tasks to characterise the network's\ntoken and layer-wise information modelling. Our analysis highlights that the\nCLS token, used for the downstream task, only focuses on a few objects\nnecessary for the trained downstream task. Still, other individual objects are\nwell-modelled separately by the tokens in the network originating from those\nobjects. We further observed a widespread distribution of scene information.\nThis demonstrates that information is far more entangled in tokens than optimal\nfor representing objects similar to symbols. Given these symbolic properties,\nwe show the network dynamics that cause failure modes of these models on basic\ndownstream tasks in a multi-object scene.\n", "link": "http://arxiv.org/abs/2406.09067v2", "date": "2024-06-18", "relevancy": 2.2467, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5938}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5437}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20structured%20are%20the%20representations%20in%20transformer-based%20vision%0A%20%20encoders%3F%20An%20analysis%20of%20multi-object%20representations%20in%20vision-language%0A%20%20models&body=Title%3A%20How%20structured%20are%20the%20representations%20in%20transformer-based%20vision%0A%20%20encoders%3F%20An%20analysis%20of%20multi-object%20representations%20in%20vision-language%0A%20%20models%0AAuthor%3A%20Tarun%20Khajuria%20and%20Braian%20Olmiro%20Dias%20and%20Jaan%20Aru%0AAbstract%3A%20%20%20Forming%20and%20using%20symbol-like%20structured%20representations%20for%20reasoning%20has%0Abeen%20considered%20essential%20for%20generalising%20over%20novel%20inputs.%20The%20primary%20tool%0Athat%20allows%20generalisation%20outside%20training%20data%20distribution%20is%20the%20ability%20to%0Aabstract%20away%20irrelevant%20information%20into%20a%20compact%20form%20relevant%20to%20the%20task.%0AAn%20extreme%20form%20of%20such%20abstract%20representations%20is%20symbols.%20Humans%20make%20use%20of%0Asymbols%20to%20bind%20information%20while%20abstracting%20away%20irrelevant%20parts%20to%20utilise%0Athe%20information%20consistently%20and%20meaningfully.%20This%20work%20estimates%20the%20state%20of%0Asuch%20structured%20representations%20in%20vision%20encoders.%20Specifically%2C%20we%20evaluate%0Aimage%20encoders%20in%20large%20vision-language%20pre-trained%20models%20to%20address%20the%0Aquestion%20of%20which%20desirable%20properties%20their%20representations%20lack%20by%20applying%0Athe%20criteria%20of%20symbolic%20structured%20reasoning%20described%20for%20LLMs%20to%20the%20image%0Amodels.%20We%20test%20the%20representation%20space%20of%20image%20encoders%20like%20VIT%2C%20BLIP%2C%0ACLIP%2C%20and%20FLAVA%20to%20characterise%20the%20distribution%20of%20the%20object%20representations%0Ain%20these%20models.%20In%20particular%2C%20we%20create%20decoding%20tasks%20using%20multi-object%0Ascenes%20from%20the%20COCO%20dataset%2C%20relating%20the%20token%20space%20to%20its%20input%20content%20for%0Avarious%20objects%20in%20the%20scene.%20We%20use%20these%20tasks%20to%20characterise%20the%20network%27s%0Atoken%20and%20layer-wise%20information%20modelling.%20Our%20analysis%20highlights%20that%20the%0ACLS%20token%2C%20used%20for%20the%20downstream%20task%2C%20only%20focuses%20on%20a%20few%20objects%0Anecessary%20for%20the%20trained%20downstream%20task.%20Still%2C%20other%20individual%20objects%20are%0Awell-modelled%20separately%20by%20the%20tokens%20in%20the%20network%20originating%20from%20those%0Aobjects.%20We%20further%20observed%20a%20widespread%20distribution%20of%20scene%20information.%0AThis%20demonstrates%20that%20information%20is%20far%20more%20entangled%20in%20tokens%20than%20optimal%0Afor%20representing%20objects%20similar%20to%20symbols.%20Given%20these%20symbolic%20properties%2C%0Awe%20show%20the%20network%20dynamics%20that%20cause%20failure%20modes%20of%20these%20models%20on%20basic%0Adownstream%20tasks%20in%20a%20multi-object%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520structured%2520are%2520the%2520representations%2520in%2520transformer-based%2520vision%250A%2520%2520encoders%253F%2520An%2520analysis%2520of%2520multi-object%2520representations%2520in%2520vision-language%250A%2520%2520models%26entry.906535625%3DTarun%2520Khajuria%2520and%2520Braian%2520Olmiro%2520Dias%2520and%2520Jaan%2520Aru%26entry.1292438233%3D%2520%2520Forming%2520and%2520using%2520symbol-like%2520structured%2520representations%2520for%2520reasoning%2520has%250Abeen%2520considered%2520essential%2520for%2520generalising%2520over%2520novel%2520inputs.%2520The%2520primary%2520tool%250Athat%2520allows%2520generalisation%2520outside%2520training%2520data%2520distribution%2520is%2520the%2520ability%2520to%250Aabstract%2520away%2520irrelevant%2520information%2520into%2520a%2520compact%2520form%2520relevant%2520to%2520the%2520task.%250AAn%2520extreme%2520form%2520of%2520such%2520abstract%2520representations%2520is%2520symbols.%2520Humans%2520make%2520use%2520of%250Asymbols%2520to%2520bind%2520information%2520while%2520abstracting%2520away%2520irrelevant%2520parts%2520to%2520utilise%250Athe%2520information%2520consistently%2520and%2520meaningfully.%2520This%2520work%2520estimates%2520the%2520state%2520of%250Asuch%2520structured%2520representations%2520in%2520vision%2520encoders.%2520Specifically%252C%2520we%2520evaluate%250Aimage%2520encoders%2520in%2520large%2520vision-language%2520pre-trained%2520models%2520to%2520address%2520the%250Aquestion%2520of%2520which%2520desirable%2520properties%2520their%2520representations%2520lack%2520by%2520applying%250Athe%2520criteria%2520of%2520symbolic%2520structured%2520reasoning%2520described%2520for%2520LLMs%2520to%2520the%2520image%250Amodels.%2520We%2520test%2520the%2520representation%2520space%2520of%2520image%2520encoders%2520like%2520VIT%252C%2520BLIP%252C%250ACLIP%252C%2520and%2520FLAVA%2520to%2520characterise%2520the%2520distribution%2520of%2520the%2520object%2520representations%250Ain%2520these%2520models.%2520In%2520particular%252C%2520we%2520create%2520decoding%2520tasks%2520using%2520multi-object%250Ascenes%2520from%2520the%2520COCO%2520dataset%252C%2520relating%2520the%2520token%2520space%2520to%2520its%2520input%2520content%2520for%250Avarious%2520objects%2520in%2520the%2520scene.%2520We%2520use%2520these%2520tasks%2520to%2520characterise%2520the%2520network%2527s%250Atoken%2520and%2520layer-wise%2520information%2520modelling.%2520Our%2520analysis%2520highlights%2520that%2520the%250ACLS%2520token%252C%2520used%2520for%2520the%2520downstream%2520task%252C%2520only%2520focuses%2520on%2520a%2520few%2520objects%250Anecessary%2520for%2520the%2520trained%2520downstream%2520task.%2520Still%252C%2520other%2520individual%2520objects%2520are%250Awell-modelled%2520separately%2520by%2520the%2520tokens%2520in%2520the%2520network%2520originating%2520from%2520those%250Aobjects.%2520We%2520further%2520observed%2520a%2520widespread%2520distribution%2520of%2520scene%2520information.%250AThis%2520demonstrates%2520that%2520information%2520is%2520far%2520more%2520entangled%2520in%2520tokens%2520than%2520optimal%250Afor%2520representing%2520objects%2520similar%2520to%2520symbols.%2520Given%2520these%2520symbolic%2520properties%252C%250Awe%2520show%2520the%2520network%2520dynamics%2520that%2520cause%2520failure%2520modes%2520of%2520these%2520models%2520on%2520basic%250Adownstream%2520tasks%2520in%2520a%2520multi-object%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20structured%20are%20the%20representations%20in%20transformer-based%20vision%0A%20%20encoders%3F%20An%20analysis%20of%20multi-object%20representations%20in%20vision-language%0A%20%20models&entry.906535625=Tarun%20Khajuria%20and%20Braian%20Olmiro%20Dias%20and%20Jaan%20Aru&entry.1292438233=%20%20Forming%20and%20using%20symbol-like%20structured%20representations%20for%20reasoning%20has%0Abeen%20considered%20essential%20for%20generalising%20over%20novel%20inputs.%20The%20primary%20tool%0Athat%20allows%20generalisation%20outside%20training%20data%20distribution%20is%20the%20ability%20to%0Aabstract%20away%20irrelevant%20information%20into%20a%20compact%20form%20relevant%20to%20the%20task.%0AAn%20extreme%20form%20of%20such%20abstract%20representations%20is%20symbols.%20Humans%20make%20use%20of%0Asymbols%20to%20bind%20information%20while%20abstracting%20away%20irrelevant%20parts%20to%20utilise%0Athe%20information%20consistently%20and%20meaningfully.%20This%20work%20estimates%20the%20state%20of%0Asuch%20structured%20representations%20in%20vision%20encoders.%20Specifically%2C%20we%20evaluate%0Aimage%20encoders%20in%20large%20vision-language%20pre-trained%20models%20to%20address%20the%0Aquestion%20of%20which%20desirable%20properties%20their%20representations%20lack%20by%20applying%0Athe%20criteria%20of%20symbolic%20structured%20reasoning%20described%20for%20LLMs%20to%20the%20image%0Amodels.%20We%20test%20the%20representation%20space%20of%20image%20encoders%20like%20VIT%2C%20BLIP%2C%0ACLIP%2C%20and%20FLAVA%20to%20characterise%20the%20distribution%20of%20the%20object%20representations%0Ain%20these%20models.%20In%20particular%2C%20we%20create%20decoding%20tasks%20using%20multi-object%0Ascenes%20from%20the%20COCO%20dataset%2C%20relating%20the%20token%20space%20to%20its%20input%20content%20for%0Avarious%20objects%20in%20the%20scene.%20We%20use%20these%20tasks%20to%20characterise%20the%20network%27s%0Atoken%20and%20layer-wise%20information%20modelling.%20Our%20analysis%20highlights%20that%20the%0ACLS%20token%2C%20used%20for%20the%20downstream%20task%2C%20only%20focuses%20on%20a%20few%20objects%0Anecessary%20for%20the%20trained%20downstream%20task.%20Still%2C%20other%20individual%20objects%20are%0Awell-modelled%20separately%20by%20the%20tokens%20in%20the%20network%20originating%20from%20those%0Aobjects.%20We%20further%20observed%20a%20widespread%20distribution%20of%20scene%20information.%0AThis%20demonstrates%20that%20information%20is%20far%20more%20entangled%20in%20tokens%20than%20optimal%0Afor%20representing%20objects%20similar%20to%20symbols.%20Given%20these%20symbolic%20properties%2C%0Awe%20show%20the%20network%20dynamics%20that%20cause%20failure%20modes%20of%20these%20models%20on%20basic%0Adownstream%20tasks%20in%20a%20multi-object%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09067v2&entry.124074799=Read"},
{"title": "Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Image Synthesis: T1 MRI\n  to Tau-PET", "author": "Symac Kim and Junho Moon and Haejun Chung and Ikbeom Jang", "abstract": "  Alzheimer's Disease (AD) is the most common form of dementia, characterised\nby cognitive decline and biomarkers such as tau-proteins. Tau-positron emission\ntomography (tau-PET), which employs a radiotracer to selectively bind, detect,\nand visualise tau protein aggregates within the brain, is valuable for early AD\ndiagnosis but is less accessible due to high costs, limited availability, and\nits invasive nature. Image synthesis with neural networks enables the\ngeneration of tau-PET images from more accessible T1-weighted magnetic\nresonance imaging (MRI) images. To ensure high-quality image synthesis, we\npropose a cyclic 2.5D perceptual loss combined with mean squared error and\nstructural similarity index measure (SSIM) losses. The cyclic 2.5D perceptual\nloss sequentially calculates the axial 2D average perceptual loss for a\nspecified number of epochs, followed by the coronal and sagittal planes for the\nsame number of epochs. This sequence is cyclically performed, with intervals\nreducing as the cycles repeat. We conduct supervised synthesis of tau-PET\nimages from T1w MRI images using 516 paired T1w MRI and tau-PET 3D images from\nthe ADNI database. For the collected data, we perform preprocessing, including\nintensity standardisation for tau-PET images from each manufacturer. The\nproposed loss, applied to generative 3D U-Net and its variants, outperformed\nthose with 2.5D and 3D perceptual losses in SSIM and peak signal-to-noise ratio\n(PSNR). In addition, including the cyclic 2.5D perceptual loss to the original\nlosses of GAN-based image synthesis models such as CycleGAN and Pix2Pix\nimproves SSIM and PSNR by at least 2% and 3%. Furthermore, by-manufacturer PET\nstandardisation helps the models in synthesising high-quality images than\nmin-max PET normalisation.\n", "link": "http://arxiv.org/abs/2406.12632v1", "date": "2024-06-18", "relevancy": 2.2273, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5601}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5562}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cyclic%202.5D%20Perceptual%20Loss%20for%20Cross-Modal%203D%20Image%20Synthesis%3A%20T1%20MRI%0A%20%20to%20Tau-PET&body=Title%3A%20Cyclic%202.5D%20Perceptual%20Loss%20for%20Cross-Modal%203D%20Image%20Synthesis%3A%20T1%20MRI%0A%20%20to%20Tau-PET%0AAuthor%3A%20Symac%20Kim%20and%20Junho%20Moon%20and%20Haejun%20Chung%20and%20Ikbeom%20Jang%0AAbstract%3A%20%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20the%20most%20common%20form%20of%20dementia%2C%20characterised%0Aby%20cognitive%20decline%20and%20biomarkers%20such%20as%20tau-proteins.%20Tau-positron%20emission%0Atomography%20%28tau-PET%29%2C%20which%20employs%20a%20radiotracer%20to%20selectively%20bind%2C%20detect%2C%0Aand%20visualise%20tau%20protein%20aggregates%20within%20the%20brain%2C%20is%20valuable%20for%20early%20AD%0Adiagnosis%20but%20is%20less%20accessible%20due%20to%20high%20costs%2C%20limited%20availability%2C%20and%0Aits%20invasive%20nature.%20Image%20synthesis%20with%20neural%20networks%20enables%20the%0Ageneration%20of%20tau-PET%20images%20from%20more%20accessible%20T1-weighted%20magnetic%0Aresonance%20imaging%20%28MRI%29%20images.%20To%20ensure%20high-quality%20image%20synthesis%2C%20we%0Apropose%20a%20cyclic%202.5D%20perceptual%20loss%20combined%20with%20mean%20squared%20error%20and%0Astructural%20similarity%20index%20measure%20%28SSIM%29%20losses.%20The%20cyclic%202.5D%20perceptual%0Aloss%20sequentially%20calculates%20the%20axial%202D%20average%20perceptual%20loss%20for%20a%0Aspecified%20number%20of%20epochs%2C%20followed%20by%20the%20coronal%20and%20sagittal%20planes%20for%20the%0Asame%20number%20of%20epochs.%20This%20sequence%20is%20cyclically%20performed%2C%20with%20intervals%0Areducing%20as%20the%20cycles%20repeat.%20We%20conduct%20supervised%20synthesis%20of%20tau-PET%0Aimages%20from%20T1w%20MRI%20images%20using%20516%20paired%20T1w%20MRI%20and%20tau-PET%203D%20images%20from%0Athe%20ADNI%20database.%20For%20the%20collected%20data%2C%20we%20perform%20preprocessing%2C%20including%0Aintensity%20standardisation%20for%20tau-PET%20images%20from%20each%20manufacturer.%20The%0Aproposed%20loss%2C%20applied%20to%20generative%203D%20U-Net%20and%20its%20variants%2C%20outperformed%0Athose%20with%202.5D%20and%203D%20perceptual%20losses%20in%20SSIM%20and%20peak%20signal-to-noise%20ratio%0A%28PSNR%29.%20In%20addition%2C%20including%20the%20cyclic%202.5D%20perceptual%20loss%20to%20the%20original%0Alosses%20of%20GAN-based%20image%20synthesis%20models%20such%20as%20CycleGAN%20and%20Pix2Pix%0Aimproves%20SSIM%20and%20PSNR%20by%20at%20least%202%25%20and%203%25.%20Furthermore%2C%20by-manufacturer%20PET%0Astandardisation%20helps%20the%20models%20in%20synthesising%20high-quality%20images%20than%0Amin-max%20PET%20normalisation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyclic%25202.5D%2520Perceptual%2520Loss%2520for%2520Cross-Modal%25203D%2520Image%2520Synthesis%253A%2520T1%2520MRI%250A%2520%2520to%2520Tau-PET%26entry.906535625%3DSymac%2520Kim%2520and%2520Junho%2520Moon%2520and%2520Haejun%2520Chung%2520and%2520Ikbeom%2520Jang%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520is%2520the%2520most%2520common%2520form%2520of%2520dementia%252C%2520characterised%250Aby%2520cognitive%2520decline%2520and%2520biomarkers%2520such%2520as%2520tau-proteins.%2520Tau-positron%2520emission%250Atomography%2520%2528tau-PET%2529%252C%2520which%2520employs%2520a%2520radiotracer%2520to%2520selectively%2520bind%252C%2520detect%252C%250Aand%2520visualise%2520tau%2520protein%2520aggregates%2520within%2520the%2520brain%252C%2520is%2520valuable%2520for%2520early%2520AD%250Adiagnosis%2520but%2520is%2520less%2520accessible%2520due%2520to%2520high%2520costs%252C%2520limited%2520availability%252C%2520and%250Aits%2520invasive%2520nature.%2520Image%2520synthesis%2520with%2520neural%2520networks%2520enables%2520the%250Ageneration%2520of%2520tau-PET%2520images%2520from%2520more%2520accessible%2520T1-weighted%2520magnetic%250Aresonance%2520imaging%2520%2528MRI%2529%2520images.%2520To%2520ensure%2520high-quality%2520image%2520synthesis%252C%2520we%250Apropose%2520a%2520cyclic%25202.5D%2520perceptual%2520loss%2520combined%2520with%2520mean%2520squared%2520error%2520and%250Astructural%2520similarity%2520index%2520measure%2520%2528SSIM%2529%2520losses.%2520The%2520cyclic%25202.5D%2520perceptual%250Aloss%2520sequentially%2520calculates%2520the%2520axial%25202D%2520average%2520perceptual%2520loss%2520for%2520a%250Aspecified%2520number%2520of%2520epochs%252C%2520followed%2520by%2520the%2520coronal%2520and%2520sagittal%2520planes%2520for%2520the%250Asame%2520number%2520of%2520epochs.%2520This%2520sequence%2520is%2520cyclically%2520performed%252C%2520with%2520intervals%250Areducing%2520as%2520the%2520cycles%2520repeat.%2520We%2520conduct%2520supervised%2520synthesis%2520of%2520tau-PET%250Aimages%2520from%2520T1w%2520MRI%2520images%2520using%2520516%2520paired%2520T1w%2520MRI%2520and%2520tau-PET%25203D%2520images%2520from%250Athe%2520ADNI%2520database.%2520For%2520the%2520collected%2520data%252C%2520we%2520perform%2520preprocessing%252C%2520including%250Aintensity%2520standardisation%2520for%2520tau-PET%2520images%2520from%2520each%2520manufacturer.%2520The%250Aproposed%2520loss%252C%2520applied%2520to%2520generative%25203D%2520U-Net%2520and%2520its%2520variants%252C%2520outperformed%250Athose%2520with%25202.5D%2520and%25203D%2520perceptual%2520losses%2520in%2520SSIM%2520and%2520peak%2520signal-to-noise%2520ratio%250A%2528PSNR%2529.%2520In%2520addition%252C%2520including%2520the%2520cyclic%25202.5D%2520perceptual%2520loss%2520to%2520the%2520original%250Alosses%2520of%2520GAN-based%2520image%2520synthesis%2520models%2520such%2520as%2520CycleGAN%2520and%2520Pix2Pix%250Aimproves%2520SSIM%2520and%2520PSNR%2520by%2520at%2520least%25202%2525%2520and%25203%2525.%2520Furthermore%252C%2520by-manufacturer%2520PET%250Astandardisation%2520helps%2520the%2520models%2520in%2520synthesising%2520high-quality%2520images%2520than%250Amin-max%2520PET%2520normalisation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cyclic%202.5D%20Perceptual%20Loss%20for%20Cross-Modal%203D%20Image%20Synthesis%3A%20T1%20MRI%0A%20%20to%20Tau-PET&entry.906535625=Symac%20Kim%20and%20Junho%20Moon%20and%20Haejun%20Chung%20and%20Ikbeom%20Jang&entry.1292438233=%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20the%20most%20common%20form%20of%20dementia%2C%20characterised%0Aby%20cognitive%20decline%20and%20biomarkers%20such%20as%20tau-proteins.%20Tau-positron%20emission%0Atomography%20%28tau-PET%29%2C%20which%20employs%20a%20radiotracer%20to%20selectively%20bind%2C%20detect%2C%0Aand%20visualise%20tau%20protein%20aggregates%20within%20the%20brain%2C%20is%20valuable%20for%20early%20AD%0Adiagnosis%20but%20is%20less%20accessible%20due%20to%20high%20costs%2C%20limited%20availability%2C%20and%0Aits%20invasive%20nature.%20Image%20synthesis%20with%20neural%20networks%20enables%20the%0Ageneration%20of%20tau-PET%20images%20from%20more%20accessible%20T1-weighted%20magnetic%0Aresonance%20imaging%20%28MRI%29%20images.%20To%20ensure%20high-quality%20image%20synthesis%2C%20we%0Apropose%20a%20cyclic%202.5D%20perceptual%20loss%20combined%20with%20mean%20squared%20error%20and%0Astructural%20similarity%20index%20measure%20%28SSIM%29%20losses.%20The%20cyclic%202.5D%20perceptual%0Aloss%20sequentially%20calculates%20the%20axial%202D%20average%20perceptual%20loss%20for%20a%0Aspecified%20number%20of%20epochs%2C%20followed%20by%20the%20coronal%20and%20sagittal%20planes%20for%20the%0Asame%20number%20of%20epochs.%20This%20sequence%20is%20cyclically%20performed%2C%20with%20intervals%0Areducing%20as%20the%20cycles%20repeat.%20We%20conduct%20supervised%20synthesis%20of%20tau-PET%0Aimages%20from%20T1w%20MRI%20images%20using%20516%20paired%20T1w%20MRI%20and%20tau-PET%203D%20images%20from%0Athe%20ADNI%20database.%20For%20the%20collected%20data%2C%20we%20perform%20preprocessing%2C%20including%0Aintensity%20standardisation%20for%20tau-PET%20images%20from%20each%20manufacturer.%20The%0Aproposed%20loss%2C%20applied%20to%20generative%203D%20U-Net%20and%20its%20variants%2C%20outperformed%0Athose%20with%202.5D%20and%203D%20perceptual%20losses%20in%20SSIM%20and%20peak%20signal-to-noise%20ratio%0A%28PSNR%29.%20In%20addition%2C%20including%20the%20cyclic%202.5D%20perceptual%20loss%20to%20the%20original%0Alosses%20of%20GAN-based%20image%20synthesis%20models%20such%20as%20CycleGAN%20and%20Pix2Pix%0Aimproves%20SSIM%20and%20PSNR%20by%20at%20least%202%25%20and%203%25.%20Furthermore%2C%20by-manufacturer%20PET%0Astandardisation%20helps%20the%20models%20in%20synthesising%20high-quality%20images%20than%0Amin-max%20PET%20normalisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12632v1&entry.124074799=Read"},
{"title": "Discovering Minimal Reinforcement Learning Environments", "author": "Jarek Liesen and Chris Lu and Andrei Lupu and Jakob N. Foerster and Henning Sprekeler and Robert T. Lange", "abstract": "  Reinforcement learning (RL) agents are commonly trained and evaluated in the\nsame environment. In contrast, humans often train in a specialized environment\nbefore being evaluated, such as studying a book before taking an exam. The\npotential of such specialized training environments is still vastly\nunderexplored, despite their capacity to dramatically speed up training.\n  The framework of synthetic environments takes a first step in this direction\nby meta-learning neural network-based Markov decision processes (MDPs). The\ninitial approach was limited to toy problems and produced environments that did\nnot transfer to unseen RL algorithms. We extend this approach in three ways:\nFirstly, we modify the meta-learning algorithm to discover environments\ninvariant towards hyperparameter configurations and learning algorithms.\nSecondly, by leveraging hardware parallelism and introducing a curriculum on an\nagent's evaluation episode horizon, we can achieve competitive results on\nseveral challenging continuous control problems. Thirdly, we surprisingly find\nthat contextual bandits enable training RL agents that transfer well to their\nevaluation environment, even if it is a complex MDP. Hence, we set up our\nexperiments to train synthetic contextual bandits, which perform on par with\nsynthetic MDPs, yield additional insights into the evaluation environment, and\ncan speed up downstream applications.\n", "link": "http://arxiv.org/abs/2406.12589v1", "date": "2024-06-18", "relevancy": 2.2263, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5834}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5661}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Minimal%20Reinforcement%20Learning%20Environments&body=Title%3A%20Discovering%20Minimal%20Reinforcement%20Learning%20Environments%0AAuthor%3A%20Jarek%20Liesen%20and%20Chris%20Lu%20and%20Andrei%20Lupu%20and%20Jakob%20N.%20Foerster%20and%20Henning%20Sprekeler%20and%20Robert%20T.%20Lange%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20agents%20are%20commonly%20trained%20and%20evaluated%20in%20the%0Asame%20environment.%20In%20contrast%2C%20humans%20often%20train%20in%20a%20specialized%20environment%0Abefore%20being%20evaluated%2C%20such%20as%20studying%20a%20book%20before%20taking%20an%20exam.%20The%0Apotential%20of%20such%20specialized%20training%20environments%20is%20still%20vastly%0Aunderexplored%2C%20despite%20their%20capacity%20to%20dramatically%20speed%20up%20training.%0A%20%20The%20framework%20of%20synthetic%20environments%20takes%20a%20first%20step%20in%20this%20direction%0Aby%20meta-learning%20neural%20network-based%20Markov%20decision%20processes%20%28MDPs%29.%20The%0Ainitial%20approach%20was%20limited%20to%20toy%20problems%20and%20produced%20environments%20that%20did%0Anot%20transfer%20to%20unseen%20RL%20algorithms.%20We%20extend%20this%20approach%20in%20three%20ways%3A%0AFirstly%2C%20we%20modify%20the%20meta-learning%20algorithm%20to%20discover%20environments%0Ainvariant%20towards%20hyperparameter%20configurations%20and%20learning%20algorithms.%0ASecondly%2C%20by%20leveraging%20hardware%20parallelism%20and%20introducing%20a%20curriculum%20on%20an%0Aagent%27s%20evaluation%20episode%20horizon%2C%20we%20can%20achieve%20competitive%20results%20on%0Aseveral%20challenging%20continuous%20control%20problems.%20Thirdly%2C%20we%20surprisingly%20find%0Athat%20contextual%20bandits%20enable%20training%20RL%20agents%20that%20transfer%20well%20to%20their%0Aevaluation%20environment%2C%20even%20if%20it%20is%20a%20complex%20MDP.%20Hence%2C%20we%20set%20up%20our%0Aexperiments%20to%20train%20synthetic%20contextual%20bandits%2C%20which%20perform%20on%20par%20with%0Asynthetic%20MDPs%2C%20yield%20additional%20insights%20into%20the%20evaluation%20environment%2C%20and%0Acan%20speed%20up%20downstream%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Minimal%2520Reinforcement%2520Learning%2520Environments%26entry.906535625%3DJarek%2520Liesen%2520and%2520Chris%2520Lu%2520and%2520Andrei%2520Lupu%2520and%2520Jakob%2520N.%2520Foerster%2520and%2520Henning%2520Sprekeler%2520and%2520Robert%2520T.%2520Lange%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520agents%2520are%2520commonly%2520trained%2520and%2520evaluated%2520in%2520the%250Asame%2520environment.%2520In%2520contrast%252C%2520humans%2520often%2520train%2520in%2520a%2520specialized%2520environment%250Abefore%2520being%2520evaluated%252C%2520such%2520as%2520studying%2520a%2520book%2520before%2520taking%2520an%2520exam.%2520The%250Apotential%2520of%2520such%2520specialized%2520training%2520environments%2520is%2520still%2520vastly%250Aunderexplored%252C%2520despite%2520their%2520capacity%2520to%2520dramatically%2520speed%2520up%2520training.%250A%2520%2520The%2520framework%2520of%2520synthetic%2520environments%2520takes%2520a%2520first%2520step%2520in%2520this%2520direction%250Aby%2520meta-learning%2520neural%2520network-based%2520Markov%2520decision%2520processes%2520%2528MDPs%2529.%2520The%250Ainitial%2520approach%2520was%2520limited%2520to%2520toy%2520problems%2520and%2520produced%2520environments%2520that%2520did%250Anot%2520transfer%2520to%2520unseen%2520RL%2520algorithms.%2520We%2520extend%2520this%2520approach%2520in%2520three%2520ways%253A%250AFirstly%252C%2520we%2520modify%2520the%2520meta-learning%2520algorithm%2520to%2520discover%2520environments%250Ainvariant%2520towards%2520hyperparameter%2520configurations%2520and%2520learning%2520algorithms.%250ASecondly%252C%2520by%2520leveraging%2520hardware%2520parallelism%2520and%2520introducing%2520a%2520curriculum%2520on%2520an%250Aagent%2527s%2520evaluation%2520episode%2520horizon%252C%2520we%2520can%2520achieve%2520competitive%2520results%2520on%250Aseveral%2520challenging%2520continuous%2520control%2520problems.%2520Thirdly%252C%2520we%2520surprisingly%2520find%250Athat%2520contextual%2520bandits%2520enable%2520training%2520RL%2520agents%2520that%2520transfer%2520well%2520to%2520their%250Aevaluation%2520environment%252C%2520even%2520if%2520it%2520is%2520a%2520complex%2520MDP.%2520Hence%252C%2520we%2520set%2520up%2520our%250Aexperiments%2520to%2520train%2520synthetic%2520contextual%2520bandits%252C%2520which%2520perform%2520on%2520par%2520with%250Asynthetic%2520MDPs%252C%2520yield%2520additional%2520insights%2520into%2520the%2520evaluation%2520environment%252C%2520and%250Acan%2520speed%2520up%2520downstream%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Minimal%20Reinforcement%20Learning%20Environments&entry.906535625=Jarek%20Liesen%20and%20Chris%20Lu%20and%20Andrei%20Lupu%20and%20Jakob%20N.%20Foerster%20and%20Henning%20Sprekeler%20and%20Robert%20T.%20Lange&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20agents%20are%20commonly%20trained%20and%20evaluated%20in%20the%0Asame%20environment.%20In%20contrast%2C%20humans%20often%20train%20in%20a%20specialized%20environment%0Abefore%20being%20evaluated%2C%20such%20as%20studying%20a%20book%20before%20taking%20an%20exam.%20The%0Apotential%20of%20such%20specialized%20training%20environments%20is%20still%20vastly%0Aunderexplored%2C%20despite%20their%20capacity%20to%20dramatically%20speed%20up%20training.%0A%20%20The%20framework%20of%20synthetic%20environments%20takes%20a%20first%20step%20in%20this%20direction%0Aby%20meta-learning%20neural%20network-based%20Markov%20decision%20processes%20%28MDPs%29.%20The%0Ainitial%20approach%20was%20limited%20to%20toy%20problems%20and%20produced%20environments%20that%20did%0Anot%20transfer%20to%20unseen%20RL%20algorithms.%20We%20extend%20this%20approach%20in%20three%20ways%3A%0AFirstly%2C%20we%20modify%20the%20meta-learning%20algorithm%20to%20discover%20environments%0Ainvariant%20towards%20hyperparameter%20configurations%20and%20learning%20algorithms.%0ASecondly%2C%20by%20leveraging%20hardware%20parallelism%20and%20introducing%20a%20curriculum%20on%20an%0Aagent%27s%20evaluation%20episode%20horizon%2C%20we%20can%20achieve%20competitive%20results%20on%0Aseveral%20challenging%20continuous%20control%20problems.%20Thirdly%2C%20we%20surprisingly%20find%0Athat%20contextual%20bandits%20enable%20training%20RL%20agents%20that%20transfer%20well%20to%20their%0Aevaluation%20environment%2C%20even%20if%20it%20is%20a%20complex%20MDP.%20Hence%2C%20we%20set%20up%20our%0Aexperiments%20to%20train%20synthetic%20contextual%20bandits%2C%20which%20perform%20on%20par%20with%0Asynthetic%20MDPs%2C%20yield%20additional%20insights%20into%20the%20evaluation%20environment%2C%20and%0Acan%20speed%20up%20downstream%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12589v1&entry.124074799=Read"},
{"title": "ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient\n  Object Detection", "author": "Junhao Lin and Lei Zhu and Jiaxing Shen and Huazhu Fu and Qing Zhang and Liansheng Wang", "abstract": "  With the rapid development of depth sensor, more and more RGB-D videos could\nbe obtained. Identifying the foreground in RGB-D videos is a fundamental and\nimportant task. However, the existing salient object detection (SOD) works only\nfocus on either static RGB-D images or RGB videos, ignoring the collaborating\nof RGB-D and video information. In this paper, we first collect a new annotated\nRGB-D video SOD (ViDSOD-100) dataset, which contains 100 videos within a total\nof 9,362 frames, acquired from diverse natural scenes. All the frames in each\nvideo are manually annotated to a high-quality saliency annotation. Moreover,\nwe propose a new baseline model, named attentive triple-fusion network\n(ATF-Net), for RGB-D video salient object detection. Our method aggregates the\nappearance information from an input RGB image, spatio-temporal information\nfrom an estimated motion map, and the geometry information from the depth map\nby devising three modality-specific branches and a multi-modality integration\nbranch. The modality-specific branches extract the representation of different\ninputs, while the multi-modality integration branch combines the multi-level\nmodality-specific features by introducing the encoder feature aggregation (MEA)\nmodules and decoder feature aggregation (MDA) modules. The experimental\nfindings conducted on both our newly introduced ViDSOD-100 dataset and the\nwell-established DAVSOD dataset highlight the superior performance of the\nproposed ATF-Net. This performance enhancement is demonstrated both\nquantitatively and qualitatively, surpassing the capabilities of current\nstate-of-the-art techniques across various domains, including RGB-D saliency\ndetection, video saliency detection, and video object segmentation. Our data\nand our code are available at github.com/jhl-Det/RGBD_Video_SOD.\n", "link": "http://arxiv.org/abs/2406.12536v1", "date": "2024-06-18", "relevancy": 2.2065, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5564}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5497}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViDSOD-100%3A%20A%20New%20Dataset%20and%20a%20Baseline%20Model%20for%20RGB-D%20Video%20Salient%0A%20%20Object%20Detection&body=Title%3A%20ViDSOD-100%3A%20A%20New%20Dataset%20and%20a%20Baseline%20Model%20for%20RGB-D%20Video%20Salient%0A%20%20Object%20Detection%0AAuthor%3A%20Junhao%20Lin%20and%20Lei%20Zhu%20and%20Jiaxing%20Shen%20and%20Huazhu%20Fu%20and%20Qing%20Zhang%20and%20Liansheng%20Wang%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20depth%20sensor%2C%20more%20and%20more%20RGB-D%20videos%20could%0Abe%20obtained.%20Identifying%20the%20foreground%20in%20RGB-D%20videos%20is%20a%20fundamental%20and%0Aimportant%20task.%20However%2C%20the%20existing%20salient%20object%20detection%20%28SOD%29%20works%20only%0Afocus%20on%20either%20static%20RGB-D%20images%20or%20RGB%20videos%2C%20ignoring%20the%20collaborating%0Aof%20RGB-D%20and%20video%20information.%20In%20this%20paper%2C%20we%20first%20collect%20a%20new%20annotated%0ARGB-D%20video%20SOD%20%28ViDSOD-100%29%20dataset%2C%20which%20contains%20100%20videos%20within%20a%20total%0Aof%209%2C362%20frames%2C%20acquired%20from%20diverse%20natural%20scenes.%20All%20the%20frames%20in%20each%0Avideo%20are%20manually%20annotated%20to%20a%20high-quality%20saliency%20annotation.%20Moreover%2C%0Awe%20propose%20a%20new%20baseline%20model%2C%20named%20attentive%20triple-fusion%20network%0A%28ATF-Net%29%2C%20for%20RGB-D%20video%20salient%20object%20detection.%20Our%20method%20aggregates%20the%0Aappearance%20information%20from%20an%20input%20RGB%20image%2C%20spatio-temporal%20information%0Afrom%20an%20estimated%20motion%20map%2C%20and%20the%20geometry%20information%20from%20the%20depth%20map%0Aby%20devising%20three%20modality-specific%20branches%20and%20a%20multi-modality%20integration%0Abranch.%20The%20modality-specific%20branches%20extract%20the%20representation%20of%20different%0Ainputs%2C%20while%20the%20multi-modality%20integration%20branch%20combines%20the%20multi-level%0Amodality-specific%20features%20by%20introducing%20the%20encoder%20feature%20aggregation%20%28MEA%29%0Amodules%20and%20decoder%20feature%20aggregation%20%28MDA%29%20modules.%20The%20experimental%0Afindings%20conducted%20on%20both%20our%20newly%20introduced%20ViDSOD-100%20dataset%20and%20the%0Awell-established%20DAVSOD%20dataset%20highlight%20the%20superior%20performance%20of%20the%0Aproposed%20ATF-Net.%20This%20performance%20enhancement%20is%20demonstrated%20both%0Aquantitatively%20and%20qualitatively%2C%20surpassing%20the%20capabilities%20of%20current%0Astate-of-the-art%20techniques%20across%20various%20domains%2C%20including%20RGB-D%20saliency%0Adetection%2C%20video%20saliency%20detection%2C%20and%20video%20object%20segmentation.%20Our%20data%0Aand%20our%20code%20are%20available%20at%20github.com/jhl-Det/RGBD_Video_SOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViDSOD-100%253A%2520A%2520New%2520Dataset%2520and%2520a%2520Baseline%2520Model%2520for%2520RGB-D%2520Video%2520Salient%250A%2520%2520Object%2520Detection%26entry.906535625%3DJunhao%2520Lin%2520and%2520Lei%2520Zhu%2520and%2520Jiaxing%2520Shen%2520and%2520Huazhu%2520Fu%2520and%2520Qing%2520Zhang%2520and%2520Liansheng%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520depth%2520sensor%252C%2520more%2520and%2520more%2520RGB-D%2520videos%2520could%250Abe%2520obtained.%2520Identifying%2520the%2520foreground%2520in%2520RGB-D%2520videos%2520is%2520a%2520fundamental%2520and%250Aimportant%2520task.%2520However%252C%2520the%2520existing%2520salient%2520object%2520detection%2520%2528SOD%2529%2520works%2520only%250Afocus%2520on%2520either%2520static%2520RGB-D%2520images%2520or%2520RGB%2520videos%252C%2520ignoring%2520the%2520collaborating%250Aof%2520RGB-D%2520and%2520video%2520information.%2520In%2520this%2520paper%252C%2520we%2520first%2520collect%2520a%2520new%2520annotated%250ARGB-D%2520video%2520SOD%2520%2528ViDSOD-100%2529%2520dataset%252C%2520which%2520contains%2520100%2520videos%2520within%2520a%2520total%250Aof%25209%252C362%2520frames%252C%2520acquired%2520from%2520diverse%2520natural%2520scenes.%2520All%2520the%2520frames%2520in%2520each%250Avideo%2520are%2520manually%2520annotated%2520to%2520a%2520high-quality%2520saliency%2520annotation.%2520Moreover%252C%250Awe%2520propose%2520a%2520new%2520baseline%2520model%252C%2520named%2520attentive%2520triple-fusion%2520network%250A%2528ATF-Net%2529%252C%2520for%2520RGB-D%2520video%2520salient%2520object%2520detection.%2520Our%2520method%2520aggregates%2520the%250Aappearance%2520information%2520from%2520an%2520input%2520RGB%2520image%252C%2520spatio-temporal%2520information%250Afrom%2520an%2520estimated%2520motion%2520map%252C%2520and%2520the%2520geometry%2520information%2520from%2520the%2520depth%2520map%250Aby%2520devising%2520three%2520modality-specific%2520branches%2520and%2520a%2520multi-modality%2520integration%250Abranch.%2520The%2520modality-specific%2520branches%2520extract%2520the%2520representation%2520of%2520different%250Ainputs%252C%2520while%2520the%2520multi-modality%2520integration%2520branch%2520combines%2520the%2520multi-level%250Amodality-specific%2520features%2520by%2520introducing%2520the%2520encoder%2520feature%2520aggregation%2520%2528MEA%2529%250Amodules%2520and%2520decoder%2520feature%2520aggregation%2520%2528MDA%2529%2520modules.%2520The%2520experimental%250Afindings%2520conducted%2520on%2520both%2520our%2520newly%2520introduced%2520ViDSOD-100%2520dataset%2520and%2520the%250Awell-established%2520DAVSOD%2520dataset%2520highlight%2520the%2520superior%2520performance%2520of%2520the%250Aproposed%2520ATF-Net.%2520This%2520performance%2520enhancement%2520is%2520demonstrated%2520both%250Aquantitatively%2520and%2520qualitatively%252C%2520surpassing%2520the%2520capabilities%2520of%2520current%250Astate-of-the-art%2520techniques%2520across%2520various%2520domains%252C%2520including%2520RGB-D%2520saliency%250Adetection%252C%2520video%2520saliency%2520detection%252C%2520and%2520video%2520object%2520segmentation.%2520Our%2520data%250Aand%2520our%2520code%2520are%2520available%2520at%2520github.com/jhl-Det/RGBD_Video_SOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViDSOD-100%3A%20A%20New%20Dataset%20and%20a%20Baseline%20Model%20for%20RGB-D%20Video%20Salient%0A%20%20Object%20Detection&entry.906535625=Junhao%20Lin%20and%20Lei%20Zhu%20and%20Jiaxing%20Shen%20and%20Huazhu%20Fu%20and%20Qing%20Zhang%20and%20Liansheng%20Wang&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20depth%20sensor%2C%20more%20and%20more%20RGB-D%20videos%20could%0Abe%20obtained.%20Identifying%20the%20foreground%20in%20RGB-D%20videos%20is%20a%20fundamental%20and%0Aimportant%20task.%20However%2C%20the%20existing%20salient%20object%20detection%20%28SOD%29%20works%20only%0Afocus%20on%20either%20static%20RGB-D%20images%20or%20RGB%20videos%2C%20ignoring%20the%20collaborating%0Aof%20RGB-D%20and%20video%20information.%20In%20this%20paper%2C%20we%20first%20collect%20a%20new%20annotated%0ARGB-D%20video%20SOD%20%28ViDSOD-100%29%20dataset%2C%20which%20contains%20100%20videos%20within%20a%20total%0Aof%209%2C362%20frames%2C%20acquired%20from%20diverse%20natural%20scenes.%20All%20the%20frames%20in%20each%0Avideo%20are%20manually%20annotated%20to%20a%20high-quality%20saliency%20annotation.%20Moreover%2C%0Awe%20propose%20a%20new%20baseline%20model%2C%20named%20attentive%20triple-fusion%20network%0A%28ATF-Net%29%2C%20for%20RGB-D%20video%20salient%20object%20detection.%20Our%20method%20aggregates%20the%0Aappearance%20information%20from%20an%20input%20RGB%20image%2C%20spatio-temporal%20information%0Afrom%20an%20estimated%20motion%20map%2C%20and%20the%20geometry%20information%20from%20the%20depth%20map%0Aby%20devising%20three%20modality-specific%20branches%20and%20a%20multi-modality%20integration%0Abranch.%20The%20modality-specific%20branches%20extract%20the%20representation%20of%20different%0Ainputs%2C%20while%20the%20multi-modality%20integration%20branch%20combines%20the%20multi-level%0Amodality-specific%20features%20by%20introducing%20the%20encoder%20feature%20aggregation%20%28MEA%29%0Amodules%20and%20decoder%20feature%20aggregation%20%28MDA%29%20modules.%20The%20experimental%0Afindings%20conducted%20on%20both%20our%20newly%20introduced%20ViDSOD-100%20dataset%20and%20the%0Awell-established%20DAVSOD%20dataset%20highlight%20the%20superior%20performance%20of%20the%0Aproposed%20ATF-Net.%20This%20performance%20enhancement%20is%20demonstrated%20both%0Aquantitatively%20and%20qualitatively%2C%20surpassing%20the%20capabilities%20of%20current%0Astate-of-the-art%20techniques%20across%20various%20domains%2C%20including%20RGB-D%20saliency%0Adetection%2C%20video%20saliency%20detection%2C%20and%20video%20object%20segmentation.%20Our%20data%0Aand%20our%20code%20are%20available%20at%20github.com/jhl-Det/RGBD_Video_SOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12536v1&entry.124074799=Read"},
{"title": "Voxel Mamba: Group-Free State Space Models for Point Cloud based 3D\n  Object Detection", "author": "Guowen Zhang and Lue Fan and Chenhang He and Zhen Lei and Zhaoxiang Zhang and Lei Zhang", "abstract": "  Serialization-based methods, which serialize the 3D voxels and group them\ninto multiple sequences before inputting to Transformers, have demonstrated\ntheir effectiveness in 3D object detection. However, serializing 3D voxels into\n1D sequences will inevitably sacrifice the voxel spatial proximity. Such an\nissue is hard to be addressed by enlarging the group size with existing\nserialization-based methods due to the quadratic complexity of Transformers\nwith feature sizes. Inspired by the recent advances of state space models\n(SSMs), we present a Voxel SSM, termed as Voxel Mamba, which employs a\ngroup-free strategy to serialize the whole space of voxels into a single\nsequence. The linear complexity of SSMs encourages our group-free design,\nalleviating the loss of spatial proximity of voxels. To further enhance the\nspatial proximity, we propose a Dual-scale SSM Block to establish a\nhierarchical structure, enabling a larger receptive field in the 1D\nserialization curve, as well as more complete local regions in 3D space.\nMoreover, we implicitly apply window partition under the group-free framework\nby positional encoding, which further enhances spatial proximity by encoding\nvoxel positional information. Our experiments on Waymo Open Dataset and\nnuScenes dataset show that Voxel Mamba not only achieves higher accuracy than\nstate-of-the-art methods, but also demonstrates significant advantages in\ncomputational efficiency.\n", "link": "http://arxiv.org/abs/2406.10700v2", "date": "2024-06-18", "relevancy": 2.1799, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5537}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5534}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voxel%20Mamba%3A%20Group-Free%20State%20Space%20Models%20for%20Point%20Cloud%20based%203D%0A%20%20Object%20Detection&body=Title%3A%20Voxel%20Mamba%3A%20Group-Free%20State%20Space%20Models%20for%20Point%20Cloud%20based%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Guowen%20Zhang%20and%20Lue%20Fan%20and%20Chenhang%20He%20and%20Zhen%20Lei%20and%20Zhaoxiang%20Zhang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Serialization-based%20methods%2C%20which%20serialize%20the%203D%20voxels%20and%20group%20them%0Ainto%20multiple%20sequences%20before%20inputting%20to%20Transformers%2C%20have%20demonstrated%0Atheir%20effectiveness%20in%203D%20object%20detection.%20However%2C%20serializing%203D%20voxels%20into%0A1D%20sequences%20will%20inevitably%20sacrifice%20the%20voxel%20spatial%20proximity.%20Such%20an%0Aissue%20is%20hard%20to%20be%20addressed%20by%20enlarging%20the%20group%20size%20with%20existing%0Aserialization-based%20methods%20due%20to%20the%20quadratic%20complexity%20of%20Transformers%0Awith%20feature%20sizes.%20Inspired%20by%20the%20recent%20advances%20of%20state%20space%20models%0A%28SSMs%29%2C%20we%20present%20a%20Voxel%20SSM%2C%20termed%20as%20Voxel%20Mamba%2C%20which%20employs%20a%0Agroup-free%20strategy%20to%20serialize%20the%20whole%20space%20of%20voxels%20into%20a%20single%0Asequence.%20The%20linear%20complexity%20of%20SSMs%20encourages%20our%20group-free%20design%2C%0Aalleviating%20the%20loss%20of%20spatial%20proximity%20of%20voxels.%20To%20further%20enhance%20the%0Aspatial%20proximity%2C%20we%20propose%20a%20Dual-scale%20SSM%20Block%20to%20establish%20a%0Ahierarchical%20structure%2C%20enabling%20a%20larger%20receptive%20field%20in%20the%201D%0Aserialization%20curve%2C%20as%20well%20as%20more%20complete%20local%20regions%20in%203D%20space.%0AMoreover%2C%20we%20implicitly%20apply%20window%20partition%20under%20the%20group-free%20framework%0Aby%20positional%20encoding%2C%20which%20further%20enhances%20spatial%20proximity%20by%20encoding%0Avoxel%20positional%20information.%20Our%20experiments%20on%20Waymo%20Open%20Dataset%20and%0AnuScenes%20dataset%20show%20that%20Voxel%20Mamba%20not%20only%20achieves%20higher%20accuracy%20than%0Astate-of-the-art%20methods%2C%20but%20also%20demonstrates%20significant%20advantages%20in%0Acomputational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxel%2520Mamba%253A%2520Group-Free%2520State%2520Space%2520Models%2520for%2520Point%2520Cloud%2520based%25203D%250A%2520%2520Object%2520Detection%26entry.906535625%3DGuowen%2520Zhang%2520and%2520Lue%2520Fan%2520and%2520Chenhang%2520He%2520and%2520Zhen%2520Lei%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Serialization-based%2520methods%252C%2520which%2520serialize%2520the%25203D%2520voxels%2520and%2520group%2520them%250Ainto%2520multiple%2520sequences%2520before%2520inputting%2520to%2520Transformers%252C%2520have%2520demonstrated%250Atheir%2520effectiveness%2520in%25203D%2520object%2520detection.%2520However%252C%2520serializing%25203D%2520voxels%2520into%250A1D%2520sequences%2520will%2520inevitably%2520sacrifice%2520the%2520voxel%2520spatial%2520proximity.%2520Such%2520an%250Aissue%2520is%2520hard%2520to%2520be%2520addressed%2520by%2520enlarging%2520the%2520group%2520size%2520with%2520existing%250Aserialization-based%2520methods%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520Transformers%250Awith%2520feature%2520sizes.%2520Inspired%2520by%2520the%2520recent%2520advances%2520of%2520state%2520space%2520models%250A%2528SSMs%2529%252C%2520we%2520present%2520a%2520Voxel%2520SSM%252C%2520termed%2520as%2520Voxel%2520Mamba%252C%2520which%2520employs%2520a%250Agroup-free%2520strategy%2520to%2520serialize%2520the%2520whole%2520space%2520of%2520voxels%2520into%2520a%2520single%250Asequence.%2520The%2520linear%2520complexity%2520of%2520SSMs%2520encourages%2520our%2520group-free%2520design%252C%250Aalleviating%2520the%2520loss%2520of%2520spatial%2520proximity%2520of%2520voxels.%2520To%2520further%2520enhance%2520the%250Aspatial%2520proximity%252C%2520we%2520propose%2520a%2520Dual-scale%2520SSM%2520Block%2520to%2520establish%2520a%250Ahierarchical%2520structure%252C%2520enabling%2520a%2520larger%2520receptive%2520field%2520in%2520the%25201D%250Aserialization%2520curve%252C%2520as%2520well%2520as%2520more%2520complete%2520local%2520regions%2520in%25203D%2520space.%250AMoreover%252C%2520we%2520implicitly%2520apply%2520window%2520partition%2520under%2520the%2520group-free%2520framework%250Aby%2520positional%2520encoding%252C%2520which%2520further%2520enhances%2520spatial%2520proximity%2520by%2520encoding%250Avoxel%2520positional%2520information.%2520Our%2520experiments%2520on%2520Waymo%2520Open%2520Dataset%2520and%250AnuScenes%2520dataset%2520show%2520that%2520Voxel%2520Mamba%2520not%2520only%2520achieves%2520higher%2520accuracy%2520than%250Astate-of-the-art%2520methods%252C%2520but%2520also%2520demonstrates%2520significant%2520advantages%2520in%250Acomputational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voxel%20Mamba%3A%20Group-Free%20State%20Space%20Models%20for%20Point%20Cloud%20based%203D%0A%20%20Object%20Detection&entry.906535625=Guowen%20Zhang%20and%20Lue%20Fan%20and%20Chenhang%20He%20and%20Zhen%20Lei%20and%20Zhaoxiang%20Zhang%20and%20Lei%20Zhang&entry.1292438233=%20%20Serialization-based%20methods%2C%20which%20serialize%20the%203D%20voxels%20and%20group%20them%0Ainto%20multiple%20sequences%20before%20inputting%20to%20Transformers%2C%20have%20demonstrated%0Atheir%20effectiveness%20in%203D%20object%20detection.%20However%2C%20serializing%203D%20voxels%20into%0A1D%20sequences%20will%20inevitably%20sacrifice%20the%20voxel%20spatial%20proximity.%20Such%20an%0Aissue%20is%20hard%20to%20be%20addressed%20by%20enlarging%20the%20group%20size%20with%20existing%0Aserialization-based%20methods%20due%20to%20the%20quadratic%20complexity%20of%20Transformers%0Awith%20feature%20sizes.%20Inspired%20by%20the%20recent%20advances%20of%20state%20space%20models%0A%28SSMs%29%2C%20we%20present%20a%20Voxel%20SSM%2C%20termed%20as%20Voxel%20Mamba%2C%20which%20employs%20a%0Agroup-free%20strategy%20to%20serialize%20the%20whole%20space%20of%20voxels%20into%20a%20single%0Asequence.%20The%20linear%20complexity%20of%20SSMs%20encourages%20our%20group-free%20design%2C%0Aalleviating%20the%20loss%20of%20spatial%20proximity%20of%20voxels.%20To%20further%20enhance%20the%0Aspatial%20proximity%2C%20we%20propose%20a%20Dual-scale%20SSM%20Block%20to%20establish%20a%0Ahierarchical%20structure%2C%20enabling%20a%20larger%20receptive%20field%20in%20the%201D%0Aserialization%20curve%2C%20as%20well%20as%20more%20complete%20local%20regions%20in%203D%20space.%0AMoreover%2C%20we%20implicitly%20apply%20window%20partition%20under%20the%20group-free%20framework%0Aby%20positional%20encoding%2C%20which%20further%20enhances%20spatial%20proximity%20by%20encoding%0Avoxel%20positional%20information.%20Our%20experiments%20on%20Waymo%20Open%20Dataset%20and%0AnuScenes%20dataset%20show%20that%20Voxel%20Mamba%20not%20only%20achieves%20higher%20accuracy%20than%0Astate-of-the-art%20methods%2C%20but%20also%20demonstrates%20significant%20advantages%20in%0Acomputational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10700v2&entry.124074799=Read"},
{"title": "Demonstrating Agile Flight from Pixels without State Estimation", "author": "Ismail Geles and Leonard Bauersfeld and Angel Romero and Jiaxu Xing and Davide Scaramuzza", "abstract": "  Quadrotors are among the most agile flying robots. Despite recent advances in\nlearning-based control and computer vision, autonomous drones still rely on\nexplicit state estimation. On the other hand, human pilots only rely on a\nfirst-person-view video stream from the drone onboard camera to push the\nplatform to its limits and fly robustly in unseen environments. To the best of\nour knowledge, we present the first vision-based quadrotor system that\nautonomously navigates through a sequence of gates at high speeds while\ndirectly mapping pixels to control commands. Like professional drone-racing\npilots, our system does not use explicit state estimation and leverages the\nsame control commands humans use (collective thrust and body rates). We\ndemonstrate agile flight at speeds up to 40km/h with accelerations up to 2g.\nThis is achieved by training vision-based policies with reinforcement learning\n(RL). The training is facilitated using an asymmetric actor-critic with access\nto privileged information. To overcome the computational complexity during\nimage-based RL training, we use the inner edges of the gates as a sensor\nabstraction. This simple yet robust, task-relevant representation can be\nsimulated during training without rendering images. During deployment, a\nSwin-transformer-based gate detector is used. Our approach enables autonomous\nagile flight with standard, off-the-shelf hardware. Although our demonstration\nfocuses on drone racing, we believe that our method has an impact beyond drone\nracing and can serve as a foundation for future research into real-world\napplications in structured environments.\n", "link": "http://arxiv.org/abs/2406.12505v1", "date": "2024-06-18", "relevancy": 2.1796, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5486}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demonstrating%20Agile%20Flight%20from%20Pixels%20without%20State%20Estimation&body=Title%3A%20Demonstrating%20Agile%20Flight%20from%20Pixels%20without%20State%20Estimation%0AAuthor%3A%20Ismail%20Geles%20and%20Leonard%20Bauersfeld%20and%20Angel%20Romero%20and%20Jiaxu%20Xing%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Quadrotors%20are%20among%20the%20most%20agile%20flying%20robots.%20Despite%20recent%20advances%20in%0Alearning-based%20control%20and%20computer%20vision%2C%20autonomous%20drones%20still%20rely%20on%0Aexplicit%20state%20estimation.%20On%20the%20other%20hand%2C%20human%20pilots%20only%20rely%20on%20a%0Afirst-person-view%20video%20stream%20from%20the%20drone%20onboard%20camera%20to%20push%20the%0Aplatform%20to%20its%20limits%20and%20fly%20robustly%20in%20unseen%20environments.%20To%20the%20best%20of%0Aour%20knowledge%2C%20we%20present%20the%20first%20vision-based%20quadrotor%20system%20that%0Aautonomously%20navigates%20through%20a%20sequence%20of%20gates%20at%20high%20speeds%20while%0Adirectly%20mapping%20pixels%20to%20control%20commands.%20Like%20professional%20drone-racing%0Apilots%2C%20our%20system%20does%20not%20use%20explicit%20state%20estimation%20and%20leverages%20the%0Asame%20control%20commands%20humans%20use%20%28collective%20thrust%20and%20body%20rates%29.%20We%0Ademonstrate%20agile%20flight%20at%20speeds%20up%20to%2040km/h%20with%20accelerations%20up%20to%202g.%0AThis%20is%20achieved%20by%20training%20vision-based%20policies%20with%20reinforcement%20learning%0A%28RL%29.%20The%20training%20is%20facilitated%20using%20an%20asymmetric%20actor-critic%20with%20access%0Ato%20privileged%20information.%20To%20overcome%20the%20computational%20complexity%20during%0Aimage-based%20RL%20training%2C%20we%20use%20the%20inner%20edges%20of%20the%20gates%20as%20a%20sensor%0Aabstraction.%20This%20simple%20yet%20robust%2C%20task-relevant%20representation%20can%20be%0Asimulated%20during%20training%20without%20rendering%20images.%20During%20deployment%2C%20a%0ASwin-transformer-based%20gate%20detector%20is%20used.%20Our%20approach%20enables%20autonomous%0Aagile%20flight%20with%20standard%2C%20off-the-shelf%20hardware.%20Although%20our%20demonstration%0Afocuses%20on%20drone%20racing%2C%20we%20believe%20that%20our%20method%20has%20an%20impact%20beyond%20drone%0Aracing%20and%20can%20serve%20as%20a%20foundation%20for%20future%20research%20into%20real-world%0Aapplications%20in%20structured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemonstrating%2520Agile%2520Flight%2520from%2520Pixels%2520without%2520State%2520Estimation%26entry.906535625%3DIsmail%2520Geles%2520and%2520Leonard%2520Bauersfeld%2520and%2520Angel%2520Romero%2520and%2520Jiaxu%2520Xing%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Quadrotors%2520are%2520among%2520the%2520most%2520agile%2520flying%2520robots.%2520Despite%2520recent%2520advances%2520in%250Alearning-based%2520control%2520and%2520computer%2520vision%252C%2520autonomous%2520drones%2520still%2520rely%2520on%250Aexplicit%2520state%2520estimation.%2520On%2520the%2520other%2520hand%252C%2520human%2520pilots%2520only%2520rely%2520on%2520a%250Afirst-person-view%2520video%2520stream%2520from%2520the%2520drone%2520onboard%2520camera%2520to%2520push%2520the%250Aplatform%2520to%2520its%2520limits%2520and%2520fly%2520robustly%2520in%2520unseen%2520environments.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520we%2520present%2520the%2520first%2520vision-based%2520quadrotor%2520system%2520that%250Aautonomously%2520navigates%2520through%2520a%2520sequence%2520of%2520gates%2520at%2520high%2520speeds%2520while%250Adirectly%2520mapping%2520pixels%2520to%2520control%2520commands.%2520Like%2520professional%2520drone-racing%250Apilots%252C%2520our%2520system%2520does%2520not%2520use%2520explicit%2520state%2520estimation%2520and%2520leverages%2520the%250Asame%2520control%2520commands%2520humans%2520use%2520%2528collective%2520thrust%2520and%2520body%2520rates%2529.%2520We%250Ademonstrate%2520agile%2520flight%2520at%2520speeds%2520up%2520to%252040km/h%2520with%2520accelerations%2520up%2520to%25202g.%250AThis%2520is%2520achieved%2520by%2520training%2520vision-based%2520policies%2520with%2520reinforcement%2520learning%250A%2528RL%2529.%2520The%2520training%2520is%2520facilitated%2520using%2520an%2520asymmetric%2520actor-critic%2520with%2520access%250Ato%2520privileged%2520information.%2520To%2520overcome%2520the%2520computational%2520complexity%2520during%250Aimage-based%2520RL%2520training%252C%2520we%2520use%2520the%2520inner%2520edges%2520of%2520the%2520gates%2520as%2520a%2520sensor%250Aabstraction.%2520This%2520simple%2520yet%2520robust%252C%2520task-relevant%2520representation%2520can%2520be%250Asimulated%2520during%2520training%2520without%2520rendering%2520images.%2520During%2520deployment%252C%2520a%250ASwin-transformer-based%2520gate%2520detector%2520is%2520used.%2520Our%2520approach%2520enables%2520autonomous%250Aagile%2520flight%2520with%2520standard%252C%2520off-the-shelf%2520hardware.%2520Although%2520our%2520demonstration%250Afocuses%2520on%2520drone%2520racing%252C%2520we%2520believe%2520that%2520our%2520method%2520has%2520an%2520impact%2520beyond%2520drone%250Aracing%2520and%2520can%2520serve%2520as%2520a%2520foundation%2520for%2520future%2520research%2520into%2520real-world%250Aapplications%2520in%2520structured%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demonstrating%20Agile%20Flight%20from%20Pixels%20without%20State%20Estimation&entry.906535625=Ismail%20Geles%20and%20Leonard%20Bauersfeld%20and%20Angel%20Romero%20and%20Jiaxu%20Xing%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Quadrotors%20are%20among%20the%20most%20agile%20flying%20robots.%20Despite%20recent%20advances%20in%0Alearning-based%20control%20and%20computer%20vision%2C%20autonomous%20drones%20still%20rely%20on%0Aexplicit%20state%20estimation.%20On%20the%20other%20hand%2C%20human%20pilots%20only%20rely%20on%20a%0Afirst-person-view%20video%20stream%20from%20the%20drone%20onboard%20camera%20to%20push%20the%0Aplatform%20to%20its%20limits%20and%20fly%20robustly%20in%20unseen%20environments.%20To%20the%20best%20of%0Aour%20knowledge%2C%20we%20present%20the%20first%20vision-based%20quadrotor%20system%20that%0Aautonomously%20navigates%20through%20a%20sequence%20of%20gates%20at%20high%20speeds%20while%0Adirectly%20mapping%20pixels%20to%20control%20commands.%20Like%20professional%20drone-racing%0Apilots%2C%20our%20system%20does%20not%20use%20explicit%20state%20estimation%20and%20leverages%20the%0Asame%20control%20commands%20humans%20use%20%28collective%20thrust%20and%20body%20rates%29.%20We%0Ademonstrate%20agile%20flight%20at%20speeds%20up%20to%2040km/h%20with%20accelerations%20up%20to%202g.%0AThis%20is%20achieved%20by%20training%20vision-based%20policies%20with%20reinforcement%20learning%0A%28RL%29.%20The%20training%20is%20facilitated%20using%20an%20asymmetric%20actor-critic%20with%20access%0Ato%20privileged%20information.%20To%20overcome%20the%20computational%20complexity%20during%0Aimage-based%20RL%20training%2C%20we%20use%20the%20inner%20edges%20of%20the%20gates%20as%20a%20sensor%0Aabstraction.%20This%20simple%20yet%20robust%2C%20task-relevant%20representation%20can%20be%0Asimulated%20during%20training%20without%20rendering%20images.%20During%20deployment%2C%20a%0ASwin-transformer-based%20gate%20detector%20is%20used.%20Our%20approach%20enables%20autonomous%0Aagile%20flight%20with%20standard%2C%20off-the-shelf%20hardware.%20Although%20our%20demonstration%0Afocuses%20on%20drone%20racing%2C%20we%20believe%20that%20our%20method%20has%20an%20impact%20beyond%20drone%0Aracing%20and%20can%20serve%20as%20a%20foundation%20for%20future%20research%20into%20real-world%0Aapplications%20in%20structured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12505v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning-based Quadcopter Controller: A Practical\n  Approach and Experiments", "author": "Truong-Dong Do and Nguyen Xuan Mung and Sung Kyung Hong", "abstract": "  Quadcopters have been studied for decades thanks to their maneuverability and\ncapability of operating in a variety of circumstances. However, quadcopters\nsuffer from dynamical nonlinearity, actuator saturation, as well as sensor\nnoise that make it challenging and time consuming to obtain accurate dynamic\nmodels and achieve satisfactory control performance. Fortunately, deep\nreinforcement learning came and has shown significant potential in system\nmodelling and control of autonomous multirotor aerial vehicles, with recent\nadvancements in deployment, performance enhancement, and generalization. In\nthis paper, an end-to-end deep reinforcement learning-based controller for\nquadcopters is proposed that is secure for real-world implementation,\ndata-efficient, and free of human gain adjustments. First, a novel\nactor-critic-based architecture is designed to map the robot states directly to\nthe motor outputs. Then, a quadcopter dynamics-based simulator was devised to\nfacilitate the training of the controller policy. Finally, the trained policy\nis deployed on a real Crazyflie nano quadrotor platform, without any additional\nfine-tuning process. Experimental results show that the quadcopter exhibits\nsatisfactory performance as it tracks a given complicated trajectory, which\ndemonstrates the effectiveness and feasibility of the proposed method and\nsignifies its capability in filling the simulation-to-reality gap.\n", "link": "http://arxiv.org/abs/2406.08815v2", "date": "2024-06-18", "relevancy": 2.1704, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5572}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5355}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning-based%20Quadcopter%20Controller%3A%20A%20Practical%0A%20%20Approach%20and%20Experiments&body=Title%3A%20Deep%20Reinforcement%20Learning-based%20Quadcopter%20Controller%3A%20A%20Practical%0A%20%20Approach%20and%20Experiments%0AAuthor%3A%20Truong-Dong%20Do%20and%20Nguyen%20Xuan%20Mung%20and%20Sung%20Kyung%20Hong%0AAbstract%3A%20%20%20Quadcopters%20have%20been%20studied%20for%20decades%20thanks%20to%20their%20maneuverability%20and%0Acapability%20of%20operating%20in%20a%20variety%20of%20circumstances.%20However%2C%20quadcopters%0Asuffer%20from%20dynamical%20nonlinearity%2C%20actuator%20saturation%2C%20as%20well%20as%20sensor%0Anoise%20that%20make%20it%20challenging%20and%20time%20consuming%20to%20obtain%20accurate%20dynamic%0Amodels%20and%20achieve%20satisfactory%20control%20performance.%20Fortunately%2C%20deep%0Areinforcement%20learning%20came%20and%20has%20shown%20significant%20potential%20in%20system%0Amodelling%20and%20control%20of%20autonomous%20multirotor%20aerial%20vehicles%2C%20with%20recent%0Aadvancements%20in%20deployment%2C%20performance%20enhancement%2C%20and%20generalization.%20In%0Athis%20paper%2C%20an%20end-to-end%20deep%20reinforcement%20learning-based%20controller%20for%0Aquadcopters%20is%20proposed%20that%20is%20secure%20for%20real-world%20implementation%2C%0Adata-efficient%2C%20and%20free%20of%20human%20gain%20adjustments.%20First%2C%20a%20novel%0Aactor-critic-based%20architecture%20is%20designed%20to%20map%20the%20robot%20states%20directly%20to%0Athe%20motor%20outputs.%20Then%2C%20a%20quadcopter%20dynamics-based%20simulator%20was%20devised%20to%0Afacilitate%20the%20training%20of%20the%20controller%20policy.%20Finally%2C%20the%20trained%20policy%0Ais%20deployed%20on%20a%20real%20Crazyflie%20nano%20quadrotor%20platform%2C%20without%20any%20additional%0Afine-tuning%20process.%20Experimental%20results%20show%20that%20the%20quadcopter%20exhibits%0Asatisfactory%20performance%20as%20it%20tracks%20a%20given%20complicated%20trajectory%2C%20which%0Ademonstrates%20the%20effectiveness%20and%20feasibility%20of%20the%20proposed%20method%20and%0Asignifies%20its%20capability%20in%20filling%20the%20simulation-to-reality%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning-based%2520Quadcopter%2520Controller%253A%2520A%2520Practical%250A%2520%2520Approach%2520and%2520Experiments%26entry.906535625%3DTruong-Dong%2520Do%2520and%2520Nguyen%2520Xuan%2520Mung%2520and%2520Sung%2520Kyung%2520Hong%26entry.1292438233%3D%2520%2520Quadcopters%2520have%2520been%2520studied%2520for%2520decades%2520thanks%2520to%2520their%2520maneuverability%2520and%250Acapability%2520of%2520operating%2520in%2520a%2520variety%2520of%2520circumstances.%2520However%252C%2520quadcopters%250Asuffer%2520from%2520dynamical%2520nonlinearity%252C%2520actuator%2520saturation%252C%2520as%2520well%2520as%2520sensor%250Anoise%2520that%2520make%2520it%2520challenging%2520and%2520time%2520consuming%2520to%2520obtain%2520accurate%2520dynamic%250Amodels%2520and%2520achieve%2520satisfactory%2520control%2520performance.%2520Fortunately%252C%2520deep%250Areinforcement%2520learning%2520came%2520and%2520has%2520shown%2520significant%2520potential%2520in%2520system%250Amodelling%2520and%2520control%2520of%2520autonomous%2520multirotor%2520aerial%2520vehicles%252C%2520with%2520recent%250Aadvancements%2520in%2520deployment%252C%2520performance%2520enhancement%252C%2520and%2520generalization.%2520In%250Athis%2520paper%252C%2520an%2520end-to-end%2520deep%2520reinforcement%2520learning-based%2520controller%2520for%250Aquadcopters%2520is%2520proposed%2520that%2520is%2520secure%2520for%2520real-world%2520implementation%252C%250Adata-efficient%252C%2520and%2520free%2520of%2520human%2520gain%2520adjustments.%2520First%252C%2520a%2520novel%250Aactor-critic-based%2520architecture%2520is%2520designed%2520to%2520map%2520the%2520robot%2520states%2520directly%2520to%250Athe%2520motor%2520outputs.%2520Then%252C%2520a%2520quadcopter%2520dynamics-based%2520simulator%2520was%2520devised%2520to%250Afacilitate%2520the%2520training%2520of%2520the%2520controller%2520policy.%2520Finally%252C%2520the%2520trained%2520policy%250Ais%2520deployed%2520on%2520a%2520real%2520Crazyflie%2520nano%2520quadrotor%2520platform%252C%2520without%2520any%2520additional%250Afine-tuning%2520process.%2520Experimental%2520results%2520show%2520that%2520the%2520quadcopter%2520exhibits%250Asatisfactory%2520performance%2520as%2520it%2520tracks%2520a%2520given%2520complicated%2520trajectory%252C%2520which%250Ademonstrates%2520the%2520effectiveness%2520and%2520feasibility%2520of%2520the%2520proposed%2520method%2520and%250Asignifies%2520its%2520capability%2520in%2520filling%2520the%2520simulation-to-reality%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning-based%20Quadcopter%20Controller%3A%20A%20Practical%0A%20%20Approach%20and%20Experiments&entry.906535625=Truong-Dong%20Do%20and%20Nguyen%20Xuan%20Mung%20and%20Sung%20Kyung%20Hong&entry.1292438233=%20%20Quadcopters%20have%20been%20studied%20for%20decades%20thanks%20to%20their%20maneuverability%20and%0Acapability%20of%20operating%20in%20a%20variety%20of%20circumstances.%20However%2C%20quadcopters%0Asuffer%20from%20dynamical%20nonlinearity%2C%20actuator%20saturation%2C%20as%20well%20as%20sensor%0Anoise%20that%20make%20it%20challenging%20and%20time%20consuming%20to%20obtain%20accurate%20dynamic%0Amodels%20and%20achieve%20satisfactory%20control%20performance.%20Fortunately%2C%20deep%0Areinforcement%20learning%20came%20and%20has%20shown%20significant%20potential%20in%20system%0Amodelling%20and%20control%20of%20autonomous%20multirotor%20aerial%20vehicles%2C%20with%20recent%0Aadvancements%20in%20deployment%2C%20performance%20enhancement%2C%20and%20generalization.%20In%0Athis%20paper%2C%20an%20end-to-end%20deep%20reinforcement%20learning-based%20controller%20for%0Aquadcopters%20is%20proposed%20that%20is%20secure%20for%20real-world%20implementation%2C%0Adata-efficient%2C%20and%20free%20of%20human%20gain%20adjustments.%20First%2C%20a%20novel%0Aactor-critic-based%20architecture%20is%20designed%20to%20map%20the%20robot%20states%20directly%20to%0Athe%20motor%20outputs.%20Then%2C%20a%20quadcopter%20dynamics-based%20simulator%20was%20devised%20to%0Afacilitate%20the%20training%20of%20the%20controller%20policy.%20Finally%2C%20the%20trained%20policy%0Ais%20deployed%20on%20a%20real%20Crazyflie%20nano%20quadrotor%20platform%2C%20without%20any%20additional%0Afine-tuning%20process.%20Experimental%20results%20show%20that%20the%20quadcopter%20exhibits%0Asatisfactory%20performance%20as%20it%20tracks%20a%20given%20complicated%20trajectory%2C%20which%0Ademonstrates%20the%20effectiveness%20and%20feasibility%20of%20the%20proposed%20method%20and%0Asignifies%20its%20capability%20in%20filling%20the%20simulation-to-reality%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08815v2&entry.124074799=Read"},
{"title": "A Super-human Vision-based Reinforcement Learning Agent for Autonomous\n  Racing in Gran Turismo", "author": "Miguel Vasco and Takuma Seno and Kenta Kawamoto and Kaushik Subramanian and Peter R. Wurman and Peter Stone", "abstract": "  Racing autonomous cars faster than the best human drivers has been a\nlongstanding grand challenge for the fields of Artificial Intelligence and\nrobotics. Recently, an end-to-end deep reinforcement learning agent met this\nchallenge in a high-fidelity racing simulator, Gran Turismo. However, this\nagent relied on global features that require instrumentation external to the\ncar. This paper introduces, to the best of our knowledge, the first super-human\ncar racing agent whose sensor input is purely local to the car, namely pixels\nfrom an ego-centric camera view and quantities that can be sensed from on-board\nthe car, such as the car's velocity. By leveraging global features only at\ntraining time, the learned agent is able to outperform the best human drivers\nin time trial (one car on the track at a time) races using only local input\nfeatures. The resulting agent is evaluated in Gran Turismo 7 on multiple tracks\nand cars. Detailed ablation experiments demonstrate the agent's strong reliance\non visual inputs, making it the first vision-based super-human car racing\nagent.\n", "link": "http://arxiv.org/abs/2406.12563v1", "date": "2024-06-18", "relevancy": 2.1678, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Super-human%20Vision-based%20Reinforcement%20Learning%20Agent%20for%20Autonomous%0A%20%20Racing%20in%20Gran%20Turismo&body=Title%3A%20A%20Super-human%20Vision-based%20Reinforcement%20Learning%20Agent%20for%20Autonomous%0A%20%20Racing%20in%20Gran%20Turismo%0AAuthor%3A%20Miguel%20Vasco%20and%20Takuma%20Seno%20and%20Kenta%20Kawamoto%20and%20Kaushik%20Subramanian%20and%20Peter%20R.%20Wurman%20and%20Peter%20Stone%0AAbstract%3A%20%20%20Racing%20autonomous%20cars%20faster%20than%20the%20best%20human%20drivers%20has%20been%20a%0Alongstanding%20grand%20challenge%20for%20the%20fields%20of%20Artificial%20Intelligence%20and%0Arobotics.%20Recently%2C%20an%20end-to-end%20deep%20reinforcement%20learning%20agent%20met%20this%0Achallenge%20in%20a%20high-fidelity%20racing%20simulator%2C%20Gran%20Turismo.%20However%2C%20this%0Aagent%20relied%20on%20global%20features%20that%20require%20instrumentation%20external%20to%20the%0Acar.%20This%20paper%20introduces%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20super-human%0Acar%20racing%20agent%20whose%20sensor%20input%20is%20purely%20local%20to%20the%20car%2C%20namely%20pixels%0Afrom%20an%20ego-centric%20camera%20view%20and%20quantities%20that%20can%20be%20sensed%20from%20on-board%0Athe%20car%2C%20such%20as%20the%20car%27s%20velocity.%20By%20leveraging%20global%20features%20only%20at%0Atraining%20time%2C%20the%20learned%20agent%20is%20able%20to%20outperform%20the%20best%20human%20drivers%0Ain%20time%20trial%20%28one%20car%20on%20the%20track%20at%20a%20time%29%20races%20using%20only%20local%20input%0Afeatures.%20The%20resulting%20agent%20is%20evaluated%20in%20Gran%20Turismo%207%20on%20multiple%20tracks%0Aand%20cars.%20Detailed%20ablation%20experiments%20demonstrate%20the%20agent%27s%20strong%20reliance%0Aon%20visual%20inputs%2C%20making%20it%20the%20first%20vision-based%20super-human%20car%20racing%0Aagent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Super-human%2520Vision-based%2520Reinforcement%2520Learning%2520Agent%2520for%2520Autonomous%250A%2520%2520Racing%2520in%2520Gran%2520Turismo%26entry.906535625%3DMiguel%2520Vasco%2520and%2520Takuma%2520Seno%2520and%2520Kenta%2520Kawamoto%2520and%2520Kaushik%2520Subramanian%2520and%2520Peter%2520R.%2520Wurman%2520and%2520Peter%2520Stone%26entry.1292438233%3D%2520%2520Racing%2520autonomous%2520cars%2520faster%2520than%2520the%2520best%2520human%2520drivers%2520has%2520been%2520a%250Alongstanding%2520grand%2520challenge%2520for%2520the%2520fields%2520of%2520Artificial%2520Intelligence%2520and%250Arobotics.%2520Recently%252C%2520an%2520end-to-end%2520deep%2520reinforcement%2520learning%2520agent%2520met%2520this%250Achallenge%2520in%2520a%2520high-fidelity%2520racing%2520simulator%252C%2520Gran%2520Turismo.%2520However%252C%2520this%250Aagent%2520relied%2520on%2520global%2520features%2520that%2520require%2520instrumentation%2520external%2520to%2520the%250Acar.%2520This%2520paper%2520introduces%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520super-human%250Acar%2520racing%2520agent%2520whose%2520sensor%2520input%2520is%2520purely%2520local%2520to%2520the%2520car%252C%2520namely%2520pixels%250Afrom%2520an%2520ego-centric%2520camera%2520view%2520and%2520quantities%2520that%2520can%2520be%2520sensed%2520from%2520on-board%250Athe%2520car%252C%2520such%2520as%2520the%2520car%2527s%2520velocity.%2520By%2520leveraging%2520global%2520features%2520only%2520at%250Atraining%2520time%252C%2520the%2520learned%2520agent%2520is%2520able%2520to%2520outperform%2520the%2520best%2520human%2520drivers%250Ain%2520time%2520trial%2520%2528one%2520car%2520on%2520the%2520track%2520at%2520a%2520time%2529%2520races%2520using%2520only%2520local%2520input%250Afeatures.%2520The%2520resulting%2520agent%2520is%2520evaluated%2520in%2520Gran%2520Turismo%25207%2520on%2520multiple%2520tracks%250Aand%2520cars.%2520Detailed%2520ablation%2520experiments%2520demonstrate%2520the%2520agent%2527s%2520strong%2520reliance%250Aon%2520visual%2520inputs%252C%2520making%2520it%2520the%2520first%2520vision-based%2520super-human%2520car%2520racing%250Aagent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Super-human%20Vision-based%20Reinforcement%20Learning%20Agent%20for%20Autonomous%0A%20%20Racing%20in%20Gran%20Turismo&entry.906535625=Miguel%20Vasco%20and%20Takuma%20Seno%20and%20Kenta%20Kawamoto%20and%20Kaushik%20Subramanian%20and%20Peter%20R.%20Wurman%20and%20Peter%20Stone&entry.1292438233=%20%20Racing%20autonomous%20cars%20faster%20than%20the%20best%20human%20drivers%20has%20been%20a%0Alongstanding%20grand%20challenge%20for%20the%20fields%20of%20Artificial%20Intelligence%20and%0Arobotics.%20Recently%2C%20an%20end-to-end%20deep%20reinforcement%20learning%20agent%20met%20this%0Achallenge%20in%20a%20high-fidelity%20racing%20simulator%2C%20Gran%20Turismo.%20However%2C%20this%0Aagent%20relied%20on%20global%20features%20that%20require%20instrumentation%20external%20to%20the%0Acar.%20This%20paper%20introduces%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20super-human%0Acar%20racing%20agent%20whose%20sensor%20input%20is%20purely%20local%20to%20the%20car%2C%20namely%20pixels%0Afrom%20an%20ego-centric%20camera%20view%20and%20quantities%20that%20can%20be%20sensed%20from%20on-board%0Athe%20car%2C%20such%20as%20the%20car%27s%20velocity.%20By%20leveraging%20global%20features%20only%20at%0Atraining%20time%2C%20the%20learned%20agent%20is%20able%20to%20outperform%20the%20best%20human%20drivers%0Ain%20time%20trial%20%28one%20car%20on%20the%20track%20at%20a%20time%29%20races%20using%20only%20local%20input%0Afeatures.%20The%20resulting%20agent%20is%20evaluated%20in%20Gran%20Turismo%207%20on%20multiple%20tracks%0Aand%20cars.%20Detailed%20ablation%20experiments%20demonstrate%20the%20agent%27s%20strong%20reliance%0Aon%20visual%20inputs%2C%20making%20it%20the%20first%20vision-based%20super-human%20car%20racing%0Aagent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12563v1&entry.124074799=Read"},
{"title": "Restorer: Solving Multiple Image Restoration Tasks with One Set of\n  Parameters", "author": "Jiawei Mao and Xuesong Yin and Yuanqi Chang", "abstract": "  Although there are many excellent solutions in image restoration, the fact\nthat they are specifically designed for a single image restoration task may\nprevent them from being state-of-the-art (SOTA) in other types of image\nrestoration tasks. While some approaches require considering multiple image\nrestoration tasks, they are still not sufficient for the requirements of the\nreal world and may suffer from the task confusion issue. In this work, we focus\non designing a unified and effective solution for multiple image restoration\ntasks including deraining, desnowing, defogging, deblurring, denoising, and\nlow-light enhancement. Based on the above purpose, we propose a Transformer\nnetwork Restorer with U-Net architecture. In order to effectively deal with\ndegraded information in multiple image restoration tasks, we need a more\ncomprehensive attention mechanism. Thus, we design all-axis attention (AAA)\nthrough stereo embedding and 3D convolution, which can simultaneously model the\nlong-range dependencies in both spatial and channel dimensions, capturing\npotential correlations among all axis. Moreover, we propose a Restorer based on\ntextual prompts. Compared to previous methods that employ learnable queries,\ntextual prompts bring explicit task priors to solve the task confusion issue\narising from learnable queries and introduce interactivity. Based on these\ndesigns, Restorer demonstrates SOTA or comparable performance in multiple image\nrestoration tasks compared to universal image restoration frameworks and\nmethods specifically designed for these individual tasks. Meanwhile, Restorer\nis faster during inference. The above results along with the real-world test\nresults show that Restorer has the potential to serve as a backbone for\nmultiple real-world image restoration tasks.\n", "link": "http://arxiv.org/abs/2406.12587v1", "date": "2024-06-18", "relevancy": 2.1472, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5551}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5411}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restorer%3A%20Solving%20Multiple%20Image%20Restoration%20Tasks%20with%20One%20Set%20of%0A%20%20Parameters&body=Title%3A%20Restorer%3A%20Solving%20Multiple%20Image%20Restoration%20Tasks%20with%20One%20Set%20of%0A%20%20Parameters%0AAuthor%3A%20Jiawei%20Mao%20and%20Xuesong%20Yin%20and%20Yuanqi%20Chang%0AAbstract%3A%20%20%20Although%20there%20are%20many%20excellent%20solutions%20in%20image%20restoration%2C%20the%20fact%0Athat%20they%20are%20specifically%20designed%20for%20a%20single%20image%20restoration%20task%20may%0Aprevent%20them%20from%20being%20state-of-the-art%20%28SOTA%29%20in%20other%20types%20of%20image%0Arestoration%20tasks.%20While%20some%20approaches%20require%20considering%20multiple%20image%0Arestoration%20tasks%2C%20they%20are%20still%20not%20sufficient%20for%20the%20requirements%20of%20the%0Areal%20world%20and%20may%20suffer%20from%20the%20task%20confusion%20issue.%20In%20this%20work%2C%20we%20focus%0Aon%20designing%20a%20unified%20and%20effective%20solution%20for%20multiple%20image%20restoration%0Atasks%20including%20deraining%2C%20desnowing%2C%20defogging%2C%20deblurring%2C%20denoising%2C%20and%0Alow-light%20enhancement.%20Based%20on%20the%20above%20purpose%2C%20we%20propose%20a%20Transformer%0Anetwork%20Restorer%20with%20U-Net%20architecture.%20In%20order%20to%20effectively%20deal%20with%0Adegraded%20information%20in%20multiple%20image%20restoration%20tasks%2C%20we%20need%20a%20more%0Acomprehensive%20attention%20mechanism.%20Thus%2C%20we%20design%20all-axis%20attention%20%28AAA%29%0Athrough%20stereo%20embedding%20and%203D%20convolution%2C%20which%20can%20simultaneously%20model%20the%0Along-range%20dependencies%20in%20both%20spatial%20and%20channel%20dimensions%2C%20capturing%0Apotential%20correlations%20among%20all%20axis.%20Moreover%2C%20we%20propose%20a%20Restorer%20based%20on%0Atextual%20prompts.%20Compared%20to%20previous%20methods%20that%20employ%20learnable%20queries%2C%0Atextual%20prompts%20bring%20explicit%20task%20priors%20to%20solve%20the%20task%20confusion%20issue%0Aarising%20from%20learnable%20queries%20and%20introduce%20interactivity.%20Based%20on%20these%0Adesigns%2C%20Restorer%20demonstrates%20SOTA%20or%20comparable%20performance%20in%20multiple%20image%0Arestoration%20tasks%20compared%20to%20universal%20image%20restoration%20frameworks%20and%0Amethods%20specifically%20designed%20for%20these%20individual%20tasks.%20Meanwhile%2C%20Restorer%0Ais%20faster%20during%20inference.%20The%20above%20results%20along%20with%20the%20real-world%20test%0Aresults%20show%20that%20Restorer%20has%20the%20potential%20to%20serve%20as%20a%20backbone%20for%0Amultiple%20real-world%20image%20restoration%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestorer%253A%2520Solving%2520Multiple%2520Image%2520Restoration%2520Tasks%2520with%2520One%2520Set%2520of%250A%2520%2520Parameters%26entry.906535625%3DJiawei%2520Mao%2520and%2520Xuesong%2520Yin%2520and%2520Yuanqi%2520Chang%26entry.1292438233%3D%2520%2520Although%2520there%2520are%2520many%2520excellent%2520solutions%2520in%2520image%2520restoration%252C%2520the%2520fact%250Athat%2520they%2520are%2520specifically%2520designed%2520for%2520a%2520single%2520image%2520restoration%2520task%2520may%250Aprevent%2520them%2520from%2520being%2520state-of-the-art%2520%2528SOTA%2529%2520in%2520other%2520types%2520of%2520image%250Arestoration%2520tasks.%2520While%2520some%2520approaches%2520require%2520considering%2520multiple%2520image%250Arestoration%2520tasks%252C%2520they%2520are%2520still%2520not%2520sufficient%2520for%2520the%2520requirements%2520of%2520the%250Areal%2520world%2520and%2520may%2520suffer%2520from%2520the%2520task%2520confusion%2520issue.%2520In%2520this%2520work%252C%2520we%2520focus%250Aon%2520designing%2520a%2520unified%2520and%2520effective%2520solution%2520for%2520multiple%2520image%2520restoration%250Atasks%2520including%2520deraining%252C%2520desnowing%252C%2520defogging%252C%2520deblurring%252C%2520denoising%252C%2520and%250Alow-light%2520enhancement.%2520Based%2520on%2520the%2520above%2520purpose%252C%2520we%2520propose%2520a%2520Transformer%250Anetwork%2520Restorer%2520with%2520U-Net%2520architecture.%2520In%2520order%2520to%2520effectively%2520deal%2520with%250Adegraded%2520information%2520in%2520multiple%2520image%2520restoration%2520tasks%252C%2520we%2520need%2520a%2520more%250Acomprehensive%2520attention%2520mechanism.%2520Thus%252C%2520we%2520design%2520all-axis%2520attention%2520%2528AAA%2529%250Athrough%2520stereo%2520embedding%2520and%25203D%2520convolution%252C%2520which%2520can%2520simultaneously%2520model%2520the%250Along-range%2520dependencies%2520in%2520both%2520spatial%2520and%2520channel%2520dimensions%252C%2520capturing%250Apotential%2520correlations%2520among%2520all%2520axis.%2520Moreover%252C%2520we%2520propose%2520a%2520Restorer%2520based%2520on%250Atextual%2520prompts.%2520Compared%2520to%2520previous%2520methods%2520that%2520employ%2520learnable%2520queries%252C%250Atextual%2520prompts%2520bring%2520explicit%2520task%2520priors%2520to%2520solve%2520the%2520task%2520confusion%2520issue%250Aarising%2520from%2520learnable%2520queries%2520and%2520introduce%2520interactivity.%2520Based%2520on%2520these%250Adesigns%252C%2520Restorer%2520demonstrates%2520SOTA%2520or%2520comparable%2520performance%2520in%2520multiple%2520image%250Arestoration%2520tasks%2520compared%2520to%2520universal%2520image%2520restoration%2520frameworks%2520and%250Amethods%2520specifically%2520designed%2520for%2520these%2520individual%2520tasks.%2520Meanwhile%252C%2520Restorer%250Ais%2520faster%2520during%2520inference.%2520The%2520above%2520results%2520along%2520with%2520the%2520real-world%2520test%250Aresults%2520show%2520that%2520Restorer%2520has%2520the%2520potential%2520to%2520serve%2520as%2520a%2520backbone%2520for%250Amultiple%2520real-world%2520image%2520restoration%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restorer%3A%20Solving%20Multiple%20Image%20Restoration%20Tasks%20with%20One%20Set%20of%0A%20%20Parameters&entry.906535625=Jiawei%20Mao%20and%20Xuesong%20Yin%20and%20Yuanqi%20Chang&entry.1292438233=%20%20Although%20there%20are%20many%20excellent%20solutions%20in%20image%20restoration%2C%20the%20fact%0Athat%20they%20are%20specifically%20designed%20for%20a%20single%20image%20restoration%20task%20may%0Aprevent%20them%20from%20being%20state-of-the-art%20%28SOTA%29%20in%20other%20types%20of%20image%0Arestoration%20tasks.%20While%20some%20approaches%20require%20considering%20multiple%20image%0Arestoration%20tasks%2C%20they%20are%20still%20not%20sufficient%20for%20the%20requirements%20of%20the%0Areal%20world%20and%20may%20suffer%20from%20the%20task%20confusion%20issue.%20In%20this%20work%2C%20we%20focus%0Aon%20designing%20a%20unified%20and%20effective%20solution%20for%20multiple%20image%20restoration%0Atasks%20including%20deraining%2C%20desnowing%2C%20defogging%2C%20deblurring%2C%20denoising%2C%20and%0Alow-light%20enhancement.%20Based%20on%20the%20above%20purpose%2C%20we%20propose%20a%20Transformer%0Anetwork%20Restorer%20with%20U-Net%20architecture.%20In%20order%20to%20effectively%20deal%20with%0Adegraded%20information%20in%20multiple%20image%20restoration%20tasks%2C%20we%20need%20a%20more%0Acomprehensive%20attention%20mechanism.%20Thus%2C%20we%20design%20all-axis%20attention%20%28AAA%29%0Athrough%20stereo%20embedding%20and%203D%20convolution%2C%20which%20can%20simultaneously%20model%20the%0Along-range%20dependencies%20in%20both%20spatial%20and%20channel%20dimensions%2C%20capturing%0Apotential%20correlations%20among%20all%20axis.%20Moreover%2C%20we%20propose%20a%20Restorer%20based%20on%0Atextual%20prompts.%20Compared%20to%20previous%20methods%20that%20employ%20learnable%20queries%2C%0Atextual%20prompts%20bring%20explicit%20task%20priors%20to%20solve%20the%20task%20confusion%20issue%0Aarising%20from%20learnable%20queries%20and%20introduce%20interactivity.%20Based%20on%20these%0Adesigns%2C%20Restorer%20demonstrates%20SOTA%20or%20comparable%20performance%20in%20multiple%20image%0Arestoration%20tasks%20compared%20to%20universal%20image%20restoration%20frameworks%20and%0Amethods%20specifically%20designed%20for%20these%20individual%20tasks.%20Meanwhile%2C%20Restorer%0Ais%20faster%20during%20inference.%20The%20above%20results%20along%20with%20the%20real-world%20test%0Aresults%20show%20that%20Restorer%20has%20the%20potential%20to%20serve%20as%20a%20backbone%20for%0Amultiple%20real-world%20image%20restoration%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12587v1&entry.124074799=Read"},
{"title": "Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations\n  for Vision Foundation Models", "author": "Hengyi Wang and Shiwei Tan and Hao Wang", "abstract": "  Vision transformers (ViTs) have emerged as a significant area of focus,\nparticularly for their capacity to be jointly trained with large language\nmodels and to serve as robust vision foundation models. Yet, the development of\ntrustworthy explanation methods for ViTs has lagged, particularly in the\ncontext of post-hoc interpretations of ViT predictions. Existing sub-image\nselection approaches, such as feature-attribution and conceptual models, fall\nshort in this regard. This paper proposes five desiderata for explaining ViTs\n-- faithfulness, stability, sparsity, multi-level structure, and parsimony --\nand demonstrates the inadequacy of current methods in meeting these criteria\ncomprehensively. We introduce a variational Bayesian explanation framework,\ndubbed ProbAbilistic Concept Explainers (PACE), which models the distributions\nof patch embeddings to provide trustworthy post-hoc conceptual explanations.\nOur qualitative analysis reveals the distributions of patch-level concepts,\nelucidating the effectiveness of ViTs by modeling the joint distribution of\npatch embeddings and ViT's predictions. Moreover, these patch-level\nexplanations bridge the gap between image-level and dataset-level explanations,\nthus completing the multi-level structure of PACE. Through extensive\nexperiments on both synthetic and real-world datasets, we demonstrate that PACE\nsurpasses state-of-the-art methods in terms of the defined desiderata.\n", "link": "http://arxiv.org/abs/2406.12649v1", "date": "2024-06-18", "relevancy": 2.1414, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5543}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5361}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Conceptual%20Explainers%3A%20Trustworthy%20Conceptual%20Explanations%0A%20%20for%20Vision%20Foundation%20Models&body=Title%3A%20Probabilistic%20Conceptual%20Explainers%3A%20Trustworthy%20Conceptual%20Explanations%0A%20%20for%20Vision%20Foundation%20Models%0AAuthor%3A%20Hengyi%20Wang%20and%20Shiwei%20Tan%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Vision%20transformers%20%28ViTs%29%20have%20emerged%20as%20a%20significant%20area%20of%20focus%2C%0Aparticularly%20for%20their%20capacity%20to%20be%20jointly%20trained%20with%20large%20language%0Amodels%20and%20to%20serve%20as%20robust%20vision%20foundation%20models.%20Yet%2C%20the%20development%20of%0Atrustworthy%20explanation%20methods%20for%20ViTs%20has%20lagged%2C%20particularly%20in%20the%0Acontext%20of%20post-hoc%20interpretations%20of%20ViT%20predictions.%20Existing%20sub-image%0Aselection%20approaches%2C%20such%20as%20feature-attribution%20and%20conceptual%20models%2C%20fall%0Ashort%20in%20this%20regard.%20This%20paper%20proposes%20five%20desiderata%20for%20explaining%20ViTs%0A--%20faithfulness%2C%20stability%2C%20sparsity%2C%20multi-level%20structure%2C%20and%20parsimony%20--%0Aand%20demonstrates%20the%20inadequacy%20of%20current%20methods%20in%20meeting%20these%20criteria%0Acomprehensively.%20We%20introduce%20a%20variational%20Bayesian%20explanation%20framework%2C%0Adubbed%20ProbAbilistic%20Concept%20Explainers%20%28PACE%29%2C%20which%20models%20the%20distributions%0Aof%20patch%20embeddings%20to%20provide%20trustworthy%20post-hoc%20conceptual%20explanations.%0AOur%20qualitative%20analysis%20reveals%20the%20distributions%20of%20patch-level%20concepts%2C%0Aelucidating%20the%20effectiveness%20of%20ViTs%20by%20modeling%20the%20joint%20distribution%20of%0Apatch%20embeddings%20and%20ViT%27s%20predictions.%20Moreover%2C%20these%20patch-level%0Aexplanations%20bridge%20the%20gap%20between%20image-level%20and%20dataset-level%20explanations%2C%0Athus%20completing%20the%20multi-level%20structure%20of%20PACE.%20Through%20extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20we%20demonstrate%20that%20PACE%0Asurpasses%20state-of-the-art%20methods%20in%20terms%20of%20the%20defined%20desiderata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Conceptual%2520Explainers%253A%2520Trustworthy%2520Conceptual%2520Explanations%250A%2520%2520for%2520Vision%2520Foundation%2520Models%26entry.906535625%3DHengyi%2520Wang%2520and%2520Shiwei%2520Tan%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520%2528ViTs%2529%2520have%2520emerged%2520as%2520a%2520significant%2520area%2520of%2520focus%252C%250Aparticularly%2520for%2520their%2520capacity%2520to%2520be%2520jointly%2520trained%2520with%2520large%2520language%250Amodels%2520and%2520to%2520serve%2520as%2520robust%2520vision%2520foundation%2520models.%2520Yet%252C%2520the%2520development%2520of%250Atrustworthy%2520explanation%2520methods%2520for%2520ViTs%2520has%2520lagged%252C%2520particularly%2520in%2520the%250Acontext%2520of%2520post-hoc%2520interpretations%2520of%2520ViT%2520predictions.%2520Existing%2520sub-image%250Aselection%2520approaches%252C%2520such%2520as%2520feature-attribution%2520and%2520conceptual%2520models%252C%2520fall%250Ashort%2520in%2520this%2520regard.%2520This%2520paper%2520proposes%2520five%2520desiderata%2520for%2520explaining%2520ViTs%250A--%2520faithfulness%252C%2520stability%252C%2520sparsity%252C%2520multi-level%2520structure%252C%2520and%2520parsimony%2520--%250Aand%2520demonstrates%2520the%2520inadequacy%2520of%2520current%2520methods%2520in%2520meeting%2520these%2520criteria%250Acomprehensively.%2520We%2520introduce%2520a%2520variational%2520Bayesian%2520explanation%2520framework%252C%250Adubbed%2520ProbAbilistic%2520Concept%2520Explainers%2520%2528PACE%2529%252C%2520which%2520models%2520the%2520distributions%250Aof%2520patch%2520embeddings%2520to%2520provide%2520trustworthy%2520post-hoc%2520conceptual%2520explanations.%250AOur%2520qualitative%2520analysis%2520reveals%2520the%2520distributions%2520of%2520patch-level%2520concepts%252C%250Aelucidating%2520the%2520effectiveness%2520of%2520ViTs%2520by%2520modeling%2520the%2520joint%2520distribution%2520of%250Apatch%2520embeddings%2520and%2520ViT%2527s%2520predictions.%2520Moreover%252C%2520these%2520patch-level%250Aexplanations%2520bridge%2520the%2520gap%2520between%2520image-level%2520and%2520dataset-level%2520explanations%252C%250Athus%2520completing%2520the%2520multi-level%2520structure%2520of%2520PACE.%2520Through%2520extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%2520we%2520demonstrate%2520that%2520PACE%250Asurpasses%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520the%2520defined%2520desiderata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Conceptual%20Explainers%3A%20Trustworthy%20Conceptual%20Explanations%0A%20%20for%20Vision%20Foundation%20Models&entry.906535625=Hengyi%20Wang%20and%20Shiwei%20Tan%20and%20Hao%20Wang&entry.1292438233=%20%20Vision%20transformers%20%28ViTs%29%20have%20emerged%20as%20a%20significant%20area%20of%20focus%2C%0Aparticularly%20for%20their%20capacity%20to%20be%20jointly%20trained%20with%20large%20language%0Amodels%20and%20to%20serve%20as%20robust%20vision%20foundation%20models.%20Yet%2C%20the%20development%20of%0Atrustworthy%20explanation%20methods%20for%20ViTs%20has%20lagged%2C%20particularly%20in%20the%0Acontext%20of%20post-hoc%20interpretations%20of%20ViT%20predictions.%20Existing%20sub-image%0Aselection%20approaches%2C%20such%20as%20feature-attribution%20and%20conceptual%20models%2C%20fall%0Ashort%20in%20this%20regard.%20This%20paper%20proposes%20five%20desiderata%20for%20explaining%20ViTs%0A--%20faithfulness%2C%20stability%2C%20sparsity%2C%20multi-level%20structure%2C%20and%20parsimony%20--%0Aand%20demonstrates%20the%20inadequacy%20of%20current%20methods%20in%20meeting%20these%20criteria%0Acomprehensively.%20We%20introduce%20a%20variational%20Bayesian%20explanation%20framework%2C%0Adubbed%20ProbAbilistic%20Concept%20Explainers%20%28PACE%29%2C%20which%20models%20the%20distributions%0Aof%20patch%20embeddings%20to%20provide%20trustworthy%20post-hoc%20conceptual%20explanations.%0AOur%20qualitative%20analysis%20reveals%20the%20distributions%20of%20patch-level%20concepts%2C%0Aelucidating%20the%20effectiveness%20of%20ViTs%20by%20modeling%20the%20joint%20distribution%20of%0Apatch%20embeddings%20and%20ViT%27s%20predictions.%20Moreover%2C%20these%20patch-level%0Aexplanations%20bridge%20the%20gap%20between%20image-level%20and%20dataset-level%20explanations%2C%0Athus%20completing%20the%20multi-level%20structure%20of%20PACE.%20Through%20extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20we%20demonstrate%20that%20PACE%0Asurpasses%20state-of-the-art%20methods%20in%20terms%20of%20the%20defined%20desiderata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12649v1&entry.124074799=Read"},
{"title": "High-Performance Hybrid Algorithm for Minimum Sum-of-Squares Clustering\n  of Infinitely Tall Data", "author": "Ravil Mussabayev and Rustam Mussabayev", "abstract": "  This paper introduces a novel formulation of the clustering problem, namely\nthe Minimum Sum-of-Squares Clustering of Infinitely Tall Data (MSSC-ITD), and\npresents HPClust, an innovative set of hybrid parallel approaches for its\neffective solution. By utilizing modern high-performance computing techniques,\nHPClust enhances key clustering metrics: effectiveness, computational\nefficiency, and scalability. In contrast to vanilla data parallelism, which\nonly accelerates processing time through the MapReduce framework, our approach\nunlocks superior performance by leveraging the multi-strategy\ncompetitive-cooperative parallelism and intricate properties of the objective\nfunction landscape. Unlike other available algorithms that struggle to scale,\nour algorithm is inherently parallel in nature, improving solution quality\nthrough increased scalability and parallelism, and outperforming even advanced\nalgorithms designed for small and medium-sized datasets. Our evaluation of\nHPClust, featuring four parallel strategies, demonstrates its superiority over\ntraditional and cutting-edge methods by offering better performance in the key\nmetrics. These results also show that parallel processing not only enhances the\nclustering efficiency, but the accuracy as well. Additionally, we explore the\nbalance between computational efficiency and clustering quality, providing\ninsights into optimal parallel strategies based on dataset specifics and\nresource availability. This research advances our understanding of parallelism\nin clustering algorithms, demonstrating that a judicious hybridization of\nadvanced parallel approaches yields optimal results for MSSC-ITD. Experiments\non synthetic data further confirm HPClust's exceptional scalability and\nrobustness to noise.\n", "link": "http://arxiv.org/abs/2311.04517v4", "date": "2024-06-18", "relevancy": 2.1398, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4296}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4284}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data&body=Title%3A%20High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data%0AAuthor%3A%20Ravil%20Mussabayev%20and%20Rustam%20Mussabayev%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20formulation%20of%20the%20clustering%20problem%2C%20namely%0Athe%20Minimum%20Sum-of-Squares%20Clustering%20of%20Infinitely%20Tall%20Data%20%28MSSC-ITD%29%2C%20and%0Apresents%20HPClust%2C%20an%20innovative%20set%20of%20hybrid%20parallel%20approaches%20for%20its%0Aeffective%20solution.%20By%20utilizing%20modern%20high-performance%20computing%20techniques%2C%0AHPClust%20enhances%20key%20clustering%20metrics%3A%20effectiveness%2C%20computational%0Aefficiency%2C%20and%20scalability.%20In%20contrast%20to%20vanilla%20data%20parallelism%2C%20which%0Aonly%20accelerates%20processing%20time%20through%20the%20MapReduce%20framework%2C%20our%20approach%0Aunlocks%20superior%20performance%20by%20leveraging%20the%20multi-strategy%0Acompetitive-cooperative%20parallelism%20and%20intricate%20properties%20of%20the%20objective%0Afunction%20landscape.%20Unlike%20other%20available%20algorithms%20that%20struggle%20to%20scale%2C%0Aour%20algorithm%20is%20inherently%20parallel%20in%20nature%2C%20improving%20solution%20quality%0Athrough%20increased%20scalability%20and%20parallelism%2C%20and%20outperforming%20even%20advanced%0Aalgorithms%20designed%20for%20small%20and%20medium-sized%20datasets.%20Our%20evaluation%20of%0AHPClust%2C%20featuring%20four%20parallel%20strategies%2C%20demonstrates%20its%20superiority%20over%0Atraditional%20and%20cutting-edge%20methods%20by%20offering%20better%20performance%20in%20the%20key%0Ametrics.%20These%20results%20also%20show%20that%20parallel%20processing%20not%20only%20enhances%20the%0Aclustering%20efficiency%2C%20but%20the%20accuracy%20as%20well.%20Additionally%2C%20we%20explore%20the%0Abalance%20between%20computational%20efficiency%20and%20clustering%20quality%2C%20providing%0Ainsights%20into%20optimal%20parallel%20strategies%20based%20on%20dataset%20specifics%20and%0Aresource%20availability.%20This%20research%20advances%20our%20understanding%20of%20parallelism%0Ain%20clustering%20algorithms%2C%20demonstrating%20that%20a%20judicious%20hybridization%20of%0Aadvanced%20parallel%20approaches%20yields%20optimal%20results%20for%20MSSC-ITD.%20Experiments%0Aon%20synthetic%20data%20further%20confirm%20HPClust%27s%20exceptional%20scalability%20and%0Arobustness%20to%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04517v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Performance%2520Hybrid%2520Algorithm%2520for%2520Minimum%2520Sum-of-Squares%2520Clustering%250A%2520%2520of%2520Infinitely%2520Tall%2520Data%26entry.906535625%3DRavil%2520Mussabayev%2520and%2520Rustam%2520Mussabayev%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520formulation%2520of%2520the%2520clustering%2520problem%252C%2520namely%250Athe%2520Minimum%2520Sum-of-Squares%2520Clustering%2520of%2520Infinitely%2520Tall%2520Data%2520%2528MSSC-ITD%2529%252C%2520and%250Apresents%2520HPClust%252C%2520an%2520innovative%2520set%2520of%2520hybrid%2520parallel%2520approaches%2520for%2520its%250Aeffective%2520solution.%2520By%2520utilizing%2520modern%2520high-performance%2520computing%2520techniques%252C%250AHPClust%2520enhances%2520key%2520clustering%2520metrics%253A%2520effectiveness%252C%2520computational%250Aefficiency%252C%2520and%2520scalability.%2520In%2520contrast%2520to%2520vanilla%2520data%2520parallelism%252C%2520which%250Aonly%2520accelerates%2520processing%2520time%2520through%2520the%2520MapReduce%2520framework%252C%2520our%2520approach%250Aunlocks%2520superior%2520performance%2520by%2520leveraging%2520the%2520multi-strategy%250Acompetitive-cooperative%2520parallelism%2520and%2520intricate%2520properties%2520of%2520the%2520objective%250Afunction%2520landscape.%2520Unlike%2520other%2520available%2520algorithms%2520that%2520struggle%2520to%2520scale%252C%250Aour%2520algorithm%2520is%2520inherently%2520parallel%2520in%2520nature%252C%2520improving%2520solution%2520quality%250Athrough%2520increased%2520scalability%2520and%2520parallelism%252C%2520and%2520outperforming%2520even%2520advanced%250Aalgorithms%2520designed%2520for%2520small%2520and%2520medium-sized%2520datasets.%2520Our%2520evaluation%2520of%250AHPClust%252C%2520featuring%2520four%2520parallel%2520strategies%252C%2520demonstrates%2520its%2520superiority%2520over%250Atraditional%2520and%2520cutting-edge%2520methods%2520by%2520offering%2520better%2520performance%2520in%2520the%2520key%250Ametrics.%2520These%2520results%2520also%2520show%2520that%2520parallel%2520processing%2520not%2520only%2520enhances%2520the%250Aclustering%2520efficiency%252C%2520but%2520the%2520accuracy%2520as%2520well.%2520Additionally%252C%2520we%2520explore%2520the%250Abalance%2520between%2520computational%2520efficiency%2520and%2520clustering%2520quality%252C%2520providing%250Ainsights%2520into%2520optimal%2520parallel%2520strategies%2520based%2520on%2520dataset%2520specifics%2520and%250Aresource%2520availability.%2520This%2520research%2520advances%2520our%2520understanding%2520of%2520parallelism%250Ain%2520clustering%2520algorithms%252C%2520demonstrating%2520that%2520a%2520judicious%2520hybridization%2520of%250Aadvanced%2520parallel%2520approaches%2520yields%2520optimal%2520results%2520for%2520MSSC-ITD.%2520Experiments%250Aon%2520synthetic%2520data%2520further%2520confirm%2520HPClust%2527s%2520exceptional%2520scalability%2520and%250Arobustness%2520to%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04517v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data&entry.906535625=Ravil%20Mussabayev%20and%20Rustam%20Mussabayev&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20formulation%20of%20the%20clustering%20problem%2C%20namely%0Athe%20Minimum%20Sum-of-Squares%20Clustering%20of%20Infinitely%20Tall%20Data%20%28MSSC-ITD%29%2C%20and%0Apresents%20HPClust%2C%20an%20innovative%20set%20of%20hybrid%20parallel%20approaches%20for%20its%0Aeffective%20solution.%20By%20utilizing%20modern%20high-performance%20computing%20techniques%2C%0AHPClust%20enhances%20key%20clustering%20metrics%3A%20effectiveness%2C%20computational%0Aefficiency%2C%20and%20scalability.%20In%20contrast%20to%20vanilla%20data%20parallelism%2C%20which%0Aonly%20accelerates%20processing%20time%20through%20the%20MapReduce%20framework%2C%20our%20approach%0Aunlocks%20superior%20performance%20by%20leveraging%20the%20multi-strategy%0Acompetitive-cooperative%20parallelism%20and%20intricate%20properties%20of%20the%20objective%0Afunction%20landscape.%20Unlike%20other%20available%20algorithms%20that%20struggle%20to%20scale%2C%0Aour%20algorithm%20is%20inherently%20parallel%20in%20nature%2C%20improving%20solution%20quality%0Athrough%20increased%20scalability%20and%20parallelism%2C%20and%20outperforming%20even%20advanced%0Aalgorithms%20designed%20for%20small%20and%20medium-sized%20datasets.%20Our%20evaluation%20of%0AHPClust%2C%20featuring%20four%20parallel%20strategies%2C%20demonstrates%20its%20superiority%20over%0Atraditional%20and%20cutting-edge%20methods%20by%20offering%20better%20performance%20in%20the%20key%0Ametrics.%20These%20results%20also%20show%20that%20parallel%20processing%20not%20only%20enhances%20the%0Aclustering%20efficiency%2C%20but%20the%20accuracy%20as%20well.%20Additionally%2C%20we%20explore%20the%0Abalance%20between%20computational%20efficiency%20and%20clustering%20quality%2C%20providing%0Ainsights%20into%20optimal%20parallel%20strategies%20based%20on%20dataset%20specifics%20and%0Aresource%20availability.%20This%20research%20advances%20our%20understanding%20of%20parallelism%0Ain%20clustering%20algorithms%2C%20demonstrating%20that%20a%20judicious%20hybridization%20of%0Aadvanced%20parallel%20approaches%20yields%20optimal%20results%20for%20MSSC-ITD.%20Experiments%0Aon%20synthetic%20data%20further%20confirm%20HPClust%27s%20exceptional%20scalability%20and%0Arobustness%20to%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04517v4&entry.124074799=Read"},
{"title": "Adversarial Attacks on Multimodal Agents", "author": "Chen Henry Wu and Jing Yu Koh and Ruslan Salakhutdinov and Daniel Fried and Aditi Raghunathan", "abstract": "  Vision-enabled language models (VLMs) are now used to build autonomous\nmultimodal agents capable of taking actions in real environments. In this\npaper, we show that multimodal agents raise new safety risks, even though\nattacking agents is more challenging than prior attacks due to limited access\nto and knowledge about the environment. Our attacks use adversarial text\nstrings to guide gradient-based perturbation over one trigger image in the\nenvironment: (1) our captioner attack attacks white-box captioners if they are\nused to process images into captions as additional inputs to the VLM; (2) our\nCLIP attack attacks a set of CLIP models jointly, which can transfer to\nproprietary VLMs. To evaluate the attacks, we curated VisualWebArena-Adv, a set\nof adversarial tasks based on VisualWebArena, an environment for web-based\nmultimodal agent tasks. Within an L-infinity norm of $16/256$ on a single\nimage, the captioner attack can make a captioner-augmented GPT-4V agent execute\nthe adversarial goals with a 75% success rate. When we remove the captioner or\nuse GPT-4V to generate its own captions, the CLIP attack can achieve success\nrates of 21% and 43%, respectively. Experiments on agents based on other VLMs,\nsuch as Gemini-1.5, Claude-3, and GPT-4o, show interesting differences in their\nrobustness. Further analysis reveals several key factors contributing to the\nattack's success, and we also discuss the implications for defenses as well.\nProject page: https://chenwu.io/attack-agent Code and data:\nhttps://github.com/ChenWu98/agent-attack\n", "link": "http://arxiv.org/abs/2406.12814v1", "date": "2024-06-18", "relevancy": 2.1253, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5626}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attacks%20on%20Multimodal%20Agents&body=Title%3A%20Adversarial%20Attacks%20on%20Multimodal%20Agents%0AAuthor%3A%20Chen%20Henry%20Wu%20and%20Jing%20Yu%20Koh%20and%20Ruslan%20Salakhutdinov%20and%20Daniel%20Fried%20and%20Aditi%20Raghunathan%0AAbstract%3A%20%20%20Vision-enabled%20language%20models%20%28VLMs%29%20are%20now%20used%20to%20build%20autonomous%0Amultimodal%20agents%20capable%20of%20taking%20actions%20in%20real%20environments.%20In%20this%0Apaper%2C%20we%20show%20that%20multimodal%20agents%20raise%20new%20safety%20risks%2C%20even%20though%0Aattacking%20agents%20is%20more%20challenging%20than%20prior%20attacks%20due%20to%20limited%20access%0Ato%20and%20knowledge%20about%20the%20environment.%20Our%20attacks%20use%20adversarial%20text%0Astrings%20to%20guide%20gradient-based%20perturbation%20over%20one%20trigger%20image%20in%20the%0Aenvironment%3A%20%281%29%20our%20captioner%20attack%20attacks%20white-box%20captioners%20if%20they%20are%0Aused%20to%20process%20images%20into%20captions%20as%20additional%20inputs%20to%20the%20VLM%3B%20%282%29%20our%0ACLIP%20attack%20attacks%20a%20set%20of%20CLIP%20models%20jointly%2C%20which%20can%20transfer%20to%0Aproprietary%20VLMs.%20To%20evaluate%20the%20attacks%2C%20we%20curated%20VisualWebArena-Adv%2C%20a%20set%0Aof%20adversarial%20tasks%20based%20on%20VisualWebArena%2C%20an%20environment%20for%20web-based%0Amultimodal%20agent%20tasks.%20Within%20an%20L-infinity%20norm%20of%20%2416/256%24%20on%20a%20single%0Aimage%2C%20the%20captioner%20attack%20can%20make%20a%20captioner-augmented%20GPT-4V%20agent%20execute%0Athe%20adversarial%20goals%20with%20a%2075%25%20success%20rate.%20When%20we%20remove%20the%20captioner%20or%0Ause%20GPT-4V%20to%20generate%20its%20own%20captions%2C%20the%20CLIP%20attack%20can%20achieve%20success%0Arates%20of%2021%25%20and%2043%25%2C%20respectively.%20Experiments%20on%20agents%20based%20on%20other%20VLMs%2C%0Asuch%20as%20Gemini-1.5%2C%20Claude-3%2C%20and%20GPT-4o%2C%20show%20interesting%20differences%20in%20their%0Arobustness.%20Further%20analysis%20reveals%20several%20key%20factors%20contributing%20to%20the%0Aattack%27s%20success%2C%20and%20we%20also%20discuss%20the%20implications%20for%20defenses%20as%20well.%0AProject%20page%3A%20https%3A//chenwu.io/attack-agent%20Code%20and%20data%3A%0Ahttps%3A//github.com/ChenWu98/agent-attack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attacks%2520on%2520Multimodal%2520Agents%26entry.906535625%3DChen%2520Henry%2520Wu%2520and%2520Jing%2520Yu%2520Koh%2520and%2520Ruslan%2520Salakhutdinov%2520and%2520Daniel%2520Fried%2520and%2520Aditi%2520Raghunathan%26entry.1292438233%3D%2520%2520Vision-enabled%2520language%2520models%2520%2528VLMs%2529%2520are%2520now%2520used%2520to%2520build%2520autonomous%250Amultimodal%2520agents%2520capable%2520of%2520taking%2520actions%2520in%2520real%2520environments.%2520In%2520this%250Apaper%252C%2520we%2520show%2520that%2520multimodal%2520agents%2520raise%2520new%2520safety%2520risks%252C%2520even%2520though%250Aattacking%2520agents%2520is%2520more%2520challenging%2520than%2520prior%2520attacks%2520due%2520to%2520limited%2520access%250Ato%2520and%2520knowledge%2520about%2520the%2520environment.%2520Our%2520attacks%2520use%2520adversarial%2520text%250Astrings%2520to%2520guide%2520gradient-based%2520perturbation%2520over%2520one%2520trigger%2520image%2520in%2520the%250Aenvironment%253A%2520%25281%2529%2520our%2520captioner%2520attack%2520attacks%2520white-box%2520captioners%2520if%2520they%2520are%250Aused%2520to%2520process%2520images%2520into%2520captions%2520as%2520additional%2520inputs%2520to%2520the%2520VLM%253B%2520%25282%2529%2520our%250ACLIP%2520attack%2520attacks%2520a%2520set%2520of%2520CLIP%2520models%2520jointly%252C%2520which%2520can%2520transfer%2520to%250Aproprietary%2520VLMs.%2520To%2520evaluate%2520the%2520attacks%252C%2520we%2520curated%2520VisualWebArena-Adv%252C%2520a%2520set%250Aof%2520adversarial%2520tasks%2520based%2520on%2520VisualWebArena%252C%2520an%2520environment%2520for%2520web-based%250Amultimodal%2520agent%2520tasks.%2520Within%2520an%2520L-infinity%2520norm%2520of%2520%252416/256%2524%2520on%2520a%2520single%250Aimage%252C%2520the%2520captioner%2520attack%2520can%2520make%2520a%2520captioner-augmented%2520GPT-4V%2520agent%2520execute%250Athe%2520adversarial%2520goals%2520with%2520a%252075%2525%2520success%2520rate.%2520When%2520we%2520remove%2520the%2520captioner%2520or%250Ause%2520GPT-4V%2520to%2520generate%2520its%2520own%2520captions%252C%2520the%2520CLIP%2520attack%2520can%2520achieve%2520success%250Arates%2520of%252021%2525%2520and%252043%2525%252C%2520respectively.%2520Experiments%2520on%2520agents%2520based%2520on%2520other%2520VLMs%252C%250Asuch%2520as%2520Gemini-1.5%252C%2520Claude-3%252C%2520and%2520GPT-4o%252C%2520show%2520interesting%2520differences%2520in%2520their%250Arobustness.%2520Further%2520analysis%2520reveals%2520several%2520key%2520factors%2520contributing%2520to%2520the%250Aattack%2527s%2520success%252C%2520and%2520we%2520also%2520discuss%2520the%2520implications%2520for%2520defenses%2520as%2520well.%250AProject%2520page%253A%2520https%253A//chenwu.io/attack-agent%2520Code%2520and%2520data%253A%250Ahttps%253A//github.com/ChenWu98/agent-attack%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attacks%20on%20Multimodal%20Agents&entry.906535625=Chen%20Henry%20Wu%20and%20Jing%20Yu%20Koh%20and%20Ruslan%20Salakhutdinov%20and%20Daniel%20Fried%20and%20Aditi%20Raghunathan&entry.1292438233=%20%20Vision-enabled%20language%20models%20%28VLMs%29%20are%20now%20used%20to%20build%20autonomous%0Amultimodal%20agents%20capable%20of%20taking%20actions%20in%20real%20environments.%20In%20this%0Apaper%2C%20we%20show%20that%20multimodal%20agents%20raise%20new%20safety%20risks%2C%20even%20though%0Aattacking%20agents%20is%20more%20challenging%20than%20prior%20attacks%20due%20to%20limited%20access%0Ato%20and%20knowledge%20about%20the%20environment.%20Our%20attacks%20use%20adversarial%20text%0Astrings%20to%20guide%20gradient-based%20perturbation%20over%20one%20trigger%20image%20in%20the%0Aenvironment%3A%20%281%29%20our%20captioner%20attack%20attacks%20white-box%20captioners%20if%20they%20are%0Aused%20to%20process%20images%20into%20captions%20as%20additional%20inputs%20to%20the%20VLM%3B%20%282%29%20our%0ACLIP%20attack%20attacks%20a%20set%20of%20CLIP%20models%20jointly%2C%20which%20can%20transfer%20to%0Aproprietary%20VLMs.%20To%20evaluate%20the%20attacks%2C%20we%20curated%20VisualWebArena-Adv%2C%20a%20set%0Aof%20adversarial%20tasks%20based%20on%20VisualWebArena%2C%20an%20environment%20for%20web-based%0Amultimodal%20agent%20tasks.%20Within%20an%20L-infinity%20norm%20of%20%2416/256%24%20on%20a%20single%0Aimage%2C%20the%20captioner%20attack%20can%20make%20a%20captioner-augmented%20GPT-4V%20agent%20execute%0Athe%20adversarial%20goals%20with%20a%2075%25%20success%20rate.%20When%20we%20remove%20the%20captioner%20or%0Ause%20GPT-4V%20to%20generate%20its%20own%20captions%2C%20the%20CLIP%20attack%20can%20achieve%20success%0Arates%20of%2021%25%20and%2043%25%2C%20respectively.%20Experiments%20on%20agents%20based%20on%20other%20VLMs%2C%0Asuch%20as%20Gemini-1.5%2C%20Claude-3%2C%20and%20GPT-4o%2C%20show%20interesting%20differences%20in%20their%0Arobustness.%20Further%20analysis%20reveals%20several%20key%20factors%20contributing%20to%20the%0Aattack%27s%20success%2C%20and%20we%20also%20discuss%20the%20implications%20for%20defenses%20as%20well.%0AProject%20page%3A%20https%3A//chenwu.io/attack-agent%20Code%20and%20data%3A%0Ahttps%3A//github.com/ChenWu98/agent-attack%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12814v1&entry.124074799=Read"},
{"title": "Offline Imitation Learning with Model-based Reverse Augmentation", "author": "Jie-Jing Shao and Hao-Sen Shi and Lan-Zhe Guo and Yu-Feng Li", "abstract": "  In offline Imitation Learning (IL), one of the main challenges is the\n\\textit{covariate shift} between the expert observations and the actual\ndistribution encountered by the agent, because it is difficult to determine\nwhat action an agent should take when outside the state distribution of the\nexpert demonstrations. Recently, the model-free solutions introduce the\nsupplementary data and identify the latent expert-similar samples to augment\nthe reliable samples during learning. Model-based solutions build forward\ndynamic models with conservatism quantification and then generate additional\ntrajectories in the neighborhood of expert demonstrations. However, without\nreward supervision, these methods are often over-conservative in the\nout-of-expert-support regions, because only in states close to expert-observed\nstates can there be a preferred action enabling policy optimization. To\nencourage more exploration on expert-unobserved states, we propose a novel\nmodel-based framework, called offline Imitation Learning with Self-paced\nReverse Augmentation (SRA). Specifically, we build a reverse dynamic model from\nthe offline demonstrations, which can efficiently generate trajectories leading\nto the expert-observed states in a self-paced style. Then, we use the\nsubsequent reinforcement learning method to learn from the augmented\ntrajectories and transit from expert-unobserved states to expert-observed\nstates. This framework not only explores the expert-unobserved states but also\nguides maximizing long-term returns on these states, ultimately enabling\ngeneralization beyond the expert data. Empirical results show that our proposal\ncould effectively mitigate the covariate shift and achieve the state-of-the-art\nperformance on the offline imitation learning benchmarks. Project website:\n\\url{https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/}.\n", "link": "http://arxiv.org/abs/2406.12550v1", "date": "2024-06-18", "relevancy": 2.1184, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5335}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5295}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Offline%20Imitation%20Learning%20with%20Model-based%20Reverse%20Augmentation&body=Title%3A%20Offline%20Imitation%20Learning%20with%20Model-based%20Reverse%20Augmentation%0AAuthor%3A%20Jie-Jing%20Shao%20and%20Hao-Sen%20Shi%20and%20Lan-Zhe%20Guo%20and%20Yu-Feng%20Li%0AAbstract%3A%20%20%20In%20offline%20Imitation%20Learning%20%28IL%29%2C%20one%20of%20the%20main%20challenges%20is%20the%0A%5Ctextit%7Bcovariate%20shift%7D%20between%20the%20expert%20observations%20and%20the%20actual%0Adistribution%20encountered%20by%20the%20agent%2C%20because%20it%20is%20difficult%20to%20determine%0Awhat%20action%20an%20agent%20should%20take%20when%20outside%20the%20state%20distribution%20of%20the%0Aexpert%20demonstrations.%20Recently%2C%20the%20model-free%20solutions%20introduce%20the%0Asupplementary%20data%20and%20identify%20the%20latent%20expert-similar%20samples%20to%20augment%0Athe%20reliable%20samples%20during%20learning.%20Model-based%20solutions%20build%20forward%0Adynamic%20models%20with%20conservatism%20quantification%20and%20then%20generate%20additional%0Atrajectories%20in%20the%20neighborhood%20of%20expert%20demonstrations.%20However%2C%20without%0Areward%20supervision%2C%20these%20methods%20are%20often%20over-conservative%20in%20the%0Aout-of-expert-support%20regions%2C%20because%20only%20in%20states%20close%20to%20expert-observed%0Astates%20can%20there%20be%20a%20preferred%20action%20enabling%20policy%20optimization.%20To%0Aencourage%20more%20exploration%20on%20expert-unobserved%20states%2C%20we%20propose%20a%20novel%0Amodel-based%20framework%2C%20called%20offline%20Imitation%20Learning%20with%20Self-paced%0AReverse%20Augmentation%20%28SRA%29.%20Specifically%2C%20we%20build%20a%20reverse%20dynamic%20model%20from%0Athe%20offline%20demonstrations%2C%20which%20can%20efficiently%20generate%20trajectories%20leading%0Ato%20the%20expert-observed%20states%20in%20a%20self-paced%20style.%20Then%2C%20we%20use%20the%0Asubsequent%20reinforcement%20learning%20method%20to%20learn%20from%20the%20augmented%0Atrajectories%20and%20transit%20from%20expert-unobserved%20states%20to%20expert-observed%0Astates.%20This%20framework%20not%20only%20explores%20the%20expert-unobserved%20states%20but%20also%0Aguides%20maximizing%20long-term%20returns%20on%20these%20states%2C%20ultimately%20enabling%0Ageneralization%20beyond%20the%20expert%20data.%20Empirical%20results%20show%20that%20our%20proposal%0Acould%20effectively%20mitigate%20the%20covariate%20shift%20and%20achieve%20the%20state-of-the-art%0Aperformance%20on%20the%20offline%20imitation%20learning%20benchmarks.%20Project%20website%3A%0A%5Curl%7Bhttps%3A//www.lamda.nju.edu.cn/shaojj/KDD24_SRA/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOffline%2520Imitation%2520Learning%2520with%2520Model-based%2520Reverse%2520Augmentation%26entry.906535625%3DJie-Jing%2520Shao%2520and%2520Hao-Sen%2520Shi%2520and%2520Lan-Zhe%2520Guo%2520and%2520Yu-Feng%2520Li%26entry.1292438233%3D%2520%2520In%2520offline%2520Imitation%2520Learning%2520%2528IL%2529%252C%2520one%2520of%2520the%2520main%2520challenges%2520is%2520the%250A%255Ctextit%257Bcovariate%2520shift%257D%2520between%2520the%2520expert%2520observations%2520and%2520the%2520actual%250Adistribution%2520encountered%2520by%2520the%2520agent%252C%2520because%2520it%2520is%2520difficult%2520to%2520determine%250Awhat%2520action%2520an%2520agent%2520should%2520take%2520when%2520outside%2520the%2520state%2520distribution%2520of%2520the%250Aexpert%2520demonstrations.%2520Recently%252C%2520the%2520model-free%2520solutions%2520introduce%2520the%250Asupplementary%2520data%2520and%2520identify%2520the%2520latent%2520expert-similar%2520samples%2520to%2520augment%250Athe%2520reliable%2520samples%2520during%2520learning.%2520Model-based%2520solutions%2520build%2520forward%250Adynamic%2520models%2520with%2520conservatism%2520quantification%2520and%2520then%2520generate%2520additional%250Atrajectories%2520in%2520the%2520neighborhood%2520of%2520expert%2520demonstrations.%2520However%252C%2520without%250Areward%2520supervision%252C%2520these%2520methods%2520are%2520often%2520over-conservative%2520in%2520the%250Aout-of-expert-support%2520regions%252C%2520because%2520only%2520in%2520states%2520close%2520to%2520expert-observed%250Astates%2520can%2520there%2520be%2520a%2520preferred%2520action%2520enabling%2520policy%2520optimization.%2520To%250Aencourage%2520more%2520exploration%2520on%2520expert-unobserved%2520states%252C%2520we%2520propose%2520a%2520novel%250Amodel-based%2520framework%252C%2520called%2520offline%2520Imitation%2520Learning%2520with%2520Self-paced%250AReverse%2520Augmentation%2520%2528SRA%2529.%2520Specifically%252C%2520we%2520build%2520a%2520reverse%2520dynamic%2520model%2520from%250Athe%2520offline%2520demonstrations%252C%2520which%2520can%2520efficiently%2520generate%2520trajectories%2520leading%250Ato%2520the%2520expert-observed%2520states%2520in%2520a%2520self-paced%2520style.%2520Then%252C%2520we%2520use%2520the%250Asubsequent%2520reinforcement%2520learning%2520method%2520to%2520learn%2520from%2520the%2520augmented%250Atrajectories%2520and%2520transit%2520from%2520expert-unobserved%2520states%2520to%2520expert-observed%250Astates.%2520This%2520framework%2520not%2520only%2520explores%2520the%2520expert-unobserved%2520states%2520but%2520also%250Aguides%2520maximizing%2520long-term%2520returns%2520on%2520these%2520states%252C%2520ultimately%2520enabling%250Ageneralization%2520beyond%2520the%2520expert%2520data.%2520Empirical%2520results%2520show%2520that%2520our%2520proposal%250Acould%2520effectively%2520mitigate%2520the%2520covariate%2520shift%2520and%2520achieve%2520the%2520state-of-the-art%250Aperformance%2520on%2520the%2520offline%2520imitation%2520learning%2520benchmarks.%2520Project%2520website%253A%250A%255Curl%257Bhttps%253A//www.lamda.nju.edu.cn/shaojj/KDD24_SRA/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Offline%20Imitation%20Learning%20with%20Model-based%20Reverse%20Augmentation&entry.906535625=Jie-Jing%20Shao%20and%20Hao-Sen%20Shi%20and%20Lan-Zhe%20Guo%20and%20Yu-Feng%20Li&entry.1292438233=%20%20In%20offline%20Imitation%20Learning%20%28IL%29%2C%20one%20of%20the%20main%20challenges%20is%20the%0A%5Ctextit%7Bcovariate%20shift%7D%20between%20the%20expert%20observations%20and%20the%20actual%0Adistribution%20encountered%20by%20the%20agent%2C%20because%20it%20is%20difficult%20to%20determine%0Awhat%20action%20an%20agent%20should%20take%20when%20outside%20the%20state%20distribution%20of%20the%0Aexpert%20demonstrations.%20Recently%2C%20the%20model-free%20solutions%20introduce%20the%0Asupplementary%20data%20and%20identify%20the%20latent%20expert-similar%20samples%20to%20augment%0Athe%20reliable%20samples%20during%20learning.%20Model-based%20solutions%20build%20forward%0Adynamic%20models%20with%20conservatism%20quantification%20and%20then%20generate%20additional%0Atrajectories%20in%20the%20neighborhood%20of%20expert%20demonstrations.%20However%2C%20without%0Areward%20supervision%2C%20these%20methods%20are%20often%20over-conservative%20in%20the%0Aout-of-expert-support%20regions%2C%20because%20only%20in%20states%20close%20to%20expert-observed%0Astates%20can%20there%20be%20a%20preferred%20action%20enabling%20policy%20optimization.%20To%0Aencourage%20more%20exploration%20on%20expert-unobserved%20states%2C%20we%20propose%20a%20novel%0Amodel-based%20framework%2C%20called%20offline%20Imitation%20Learning%20with%20Self-paced%0AReverse%20Augmentation%20%28SRA%29.%20Specifically%2C%20we%20build%20a%20reverse%20dynamic%20model%20from%0Athe%20offline%20demonstrations%2C%20which%20can%20efficiently%20generate%20trajectories%20leading%0Ato%20the%20expert-observed%20states%20in%20a%20self-paced%20style.%20Then%2C%20we%20use%20the%0Asubsequent%20reinforcement%20learning%20method%20to%20learn%20from%20the%20augmented%0Atrajectories%20and%20transit%20from%20expert-unobserved%20states%20to%20expert-observed%0Astates.%20This%20framework%20not%20only%20explores%20the%20expert-unobserved%20states%20but%20also%0Aguides%20maximizing%20long-term%20returns%20on%20these%20states%2C%20ultimately%20enabling%0Ageneralization%20beyond%20the%20expert%20data.%20Empirical%20results%20show%20that%20our%20proposal%0Acould%20effectively%20mitigate%20the%20covariate%20shift%20and%20achieve%20the%20state-of-the-art%0Aperformance%20on%20the%20offline%20imitation%20learning%20benchmarks.%20Project%20website%3A%0A%5Curl%7Bhttps%3A//www.lamda.nju.edu.cn/shaojj/KDD24_SRA/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12550v1&entry.124074799=Read"},
{"title": "Theoretical Understanding of In-Context Learning in Shallow Transformers\n  with Unstructured Data", "author": "Yue Xing and Xiaofeng Lin and Chenheng Xu and Namjoon Suh and Qifan Song and Guang Cheng", "abstract": "  Large language models (LLMs) are powerful models that can learn concepts at\nthe inference stage via in-context learning (ICL). While theoretical studies,\ne.g., \\cite{zhang2023trained}, attempt to explain the mechanism of ICL, they\nassume the input $x_i$ and the output $y_i$ of each demonstration example are\nin the same token (i.e., structured data). However, in real practice, the\nexamples are usually text input, and all words, regardless of their logic\nrelationship, are stored in different tokens (i.e., unstructured data\n\\cite{wibisono2023role}). To understand how LLMs learn from the unstructured\ndata in ICL, this paper studies the role of each component in the transformer\narchitecture and provides a theoretical understanding to explain the success of\nthe architecture. In particular, we consider a simple transformer with one/two\nattention layers and linear regression tasks for the ICL prediction. We observe\nthat (1) a transformer with two layers of (self-)attentions with a look-ahead\nattention mask can learn from the prompt in the unstructured data, and (2)\npositional encoding can match the $x_i$ and $y_i$ tokens to achieve a better\nICL performance.\n", "link": "http://arxiv.org/abs/2402.00743v2", "date": "2024-06-18", "relevancy": 2.1178, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5484}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5382}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Understanding%20of%20In-Context%20Learning%20in%20Shallow%20Transformers%0A%20%20with%20Unstructured%20Data&body=Title%3A%20Theoretical%20Understanding%20of%20In-Context%20Learning%20in%20Shallow%20Transformers%0A%20%20with%20Unstructured%20Data%0AAuthor%3A%20Yue%20Xing%20and%20Xiaofeng%20Lin%20and%20Chenheng%20Xu%20and%20Namjoon%20Suh%20and%20Qifan%20Song%20and%20Guang%20Cheng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20powerful%20models%20that%20can%20learn%20concepts%20at%0Athe%20inference%20stage%20via%20in-context%20learning%20%28ICL%29.%20While%20theoretical%20studies%2C%0Ae.g.%2C%20%5Ccite%7Bzhang2023trained%7D%2C%20attempt%20to%20explain%20the%20mechanism%20of%20ICL%2C%20they%0Aassume%20the%20input%20%24x_i%24%20and%20the%20output%20%24y_i%24%20of%20each%20demonstration%20example%20are%0Ain%20the%20same%20token%20%28i.e.%2C%20structured%20data%29.%20However%2C%20in%20real%20practice%2C%20the%0Aexamples%20are%20usually%20text%20input%2C%20and%20all%20words%2C%20regardless%20of%20their%20logic%0Arelationship%2C%20are%20stored%20in%20different%20tokens%20%28i.e.%2C%20unstructured%20data%0A%5Ccite%7Bwibisono2023role%7D%29.%20To%20understand%20how%20LLMs%20learn%20from%20the%20unstructured%0Adata%20in%20ICL%2C%20this%20paper%20studies%20the%20role%20of%20each%20component%20in%20the%20transformer%0Aarchitecture%20and%20provides%20a%20theoretical%20understanding%20to%20explain%20the%20success%20of%0Athe%20architecture.%20In%20particular%2C%20we%20consider%20a%20simple%20transformer%20with%20one/two%0Aattention%20layers%20and%20linear%20regression%20tasks%20for%20the%20ICL%20prediction.%20We%20observe%0Athat%20%281%29%20a%20transformer%20with%20two%20layers%20of%20%28self-%29attentions%20with%20a%20look-ahead%0Aattention%20mask%20can%20learn%20from%20the%20prompt%20in%20the%20unstructured%20data%2C%20and%20%282%29%0Apositional%20encoding%20can%20match%20the%20%24x_i%24%20and%20%24y_i%24%20tokens%20to%20achieve%20a%20better%0AICL%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00743v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoretical%2520Understanding%2520of%2520In-Context%2520Learning%2520in%2520Shallow%2520Transformers%250A%2520%2520with%2520Unstructured%2520Data%26entry.906535625%3DYue%2520Xing%2520and%2520Xiaofeng%2520Lin%2520and%2520Chenheng%2520Xu%2520and%2520Namjoon%2520Suh%2520and%2520Qifan%2520Song%2520and%2520Guang%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520powerful%2520models%2520that%2520can%2520learn%2520concepts%2520at%250Athe%2520inference%2520stage%2520via%2520in-context%2520learning%2520%2528ICL%2529.%2520While%2520theoretical%2520studies%252C%250Ae.g.%252C%2520%255Ccite%257Bzhang2023trained%257D%252C%2520attempt%2520to%2520explain%2520the%2520mechanism%2520of%2520ICL%252C%2520they%250Aassume%2520the%2520input%2520%2524x_i%2524%2520and%2520the%2520output%2520%2524y_i%2524%2520of%2520each%2520demonstration%2520example%2520are%250Ain%2520the%2520same%2520token%2520%2528i.e.%252C%2520structured%2520data%2529.%2520However%252C%2520in%2520real%2520practice%252C%2520the%250Aexamples%2520are%2520usually%2520text%2520input%252C%2520and%2520all%2520words%252C%2520regardless%2520of%2520their%2520logic%250Arelationship%252C%2520are%2520stored%2520in%2520different%2520tokens%2520%2528i.e.%252C%2520unstructured%2520data%250A%255Ccite%257Bwibisono2023role%257D%2529.%2520To%2520understand%2520how%2520LLMs%2520learn%2520from%2520the%2520unstructured%250Adata%2520in%2520ICL%252C%2520this%2520paper%2520studies%2520the%2520role%2520of%2520each%2520component%2520in%2520the%2520transformer%250Aarchitecture%2520and%2520provides%2520a%2520theoretical%2520understanding%2520to%2520explain%2520the%2520success%2520of%250Athe%2520architecture.%2520In%2520particular%252C%2520we%2520consider%2520a%2520simple%2520transformer%2520with%2520one/two%250Aattention%2520layers%2520and%2520linear%2520regression%2520tasks%2520for%2520the%2520ICL%2520prediction.%2520We%2520observe%250Athat%2520%25281%2529%2520a%2520transformer%2520with%2520two%2520layers%2520of%2520%2528self-%2529attentions%2520with%2520a%2520look-ahead%250Aattention%2520mask%2520can%2520learn%2520from%2520the%2520prompt%2520in%2520the%2520unstructured%2520data%252C%2520and%2520%25282%2529%250Apositional%2520encoding%2520can%2520match%2520the%2520%2524x_i%2524%2520and%2520%2524y_i%2524%2520tokens%2520to%2520achieve%2520a%2520better%250AICL%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00743v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Understanding%20of%20In-Context%20Learning%20in%20Shallow%20Transformers%0A%20%20with%20Unstructured%20Data&entry.906535625=Yue%20Xing%20and%20Xiaofeng%20Lin%20and%20Chenheng%20Xu%20and%20Namjoon%20Suh%20and%20Qifan%20Song%20and%20Guang%20Cheng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20powerful%20models%20that%20can%20learn%20concepts%20at%0Athe%20inference%20stage%20via%20in-context%20learning%20%28ICL%29.%20While%20theoretical%20studies%2C%0Ae.g.%2C%20%5Ccite%7Bzhang2023trained%7D%2C%20attempt%20to%20explain%20the%20mechanism%20of%20ICL%2C%20they%0Aassume%20the%20input%20%24x_i%24%20and%20the%20output%20%24y_i%24%20of%20each%20demonstration%20example%20are%0Ain%20the%20same%20token%20%28i.e.%2C%20structured%20data%29.%20However%2C%20in%20real%20practice%2C%20the%0Aexamples%20are%20usually%20text%20input%2C%20and%20all%20words%2C%20regardless%20of%20their%20logic%0Arelationship%2C%20are%20stored%20in%20different%20tokens%20%28i.e.%2C%20unstructured%20data%0A%5Ccite%7Bwibisono2023role%7D%29.%20To%20understand%20how%20LLMs%20learn%20from%20the%20unstructured%0Adata%20in%20ICL%2C%20this%20paper%20studies%20the%20role%20of%20each%20component%20in%20the%20transformer%0Aarchitecture%20and%20provides%20a%20theoretical%20understanding%20to%20explain%20the%20success%20of%0Athe%20architecture.%20In%20particular%2C%20we%20consider%20a%20simple%20transformer%20with%20one/two%0Aattention%20layers%20and%20linear%20regression%20tasks%20for%20the%20ICL%20prediction.%20We%20observe%0Athat%20%281%29%20a%20transformer%20with%20two%20layers%20of%20%28self-%29attentions%20with%20a%20look-ahead%0Aattention%20mask%20can%20learn%20from%20the%20prompt%20in%20the%20unstructured%20data%2C%20and%20%282%29%0Apositional%20encoding%20can%20match%20the%20%24x_i%24%20and%20%24y_i%24%20tokens%20to%20achieve%20a%20better%0AICL%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00743v2&entry.124074799=Read"},
{"title": "AGLA: Mitigating Object Hallucinations in Large Vision-Language Models\n  with Assembly of Global and Local Attention", "author": "Wenbin An and Feng Tian and Sicong Leng and Jiahao Nie and Haonan Lin and QianYing Wang and Guang Dai and Ping Chen and Shijian Lu", "abstract": "  Despite their great success across various multimodal tasks, Large\nVision-Language Models (LVLMs) are facing a prevalent problem with object\nhallucinations, where the generated textual responses are inconsistent with\nground-truth objects in the given image. This paper investigates various LVLMs\nand pinpoints attention deficiency toward discriminative local image features\nas one root cause of object hallucinations. Specifically, LVLMs predominantly\nattend to prompt-independent global image features, while failing to capture\nprompt-relevant local features, consequently undermining the visual grounding\ncapacity of LVLMs and leading to hallucinations. To this end, we propose\nAssembly of Global and Local Attention (AGLA), a training-free and\nplug-and-play approach that mitigates object hallucinations by exploring an\nensemble of global features for response generation and local features for\nvisual discrimination simultaneously. Our approach exhibits an image-prompt\nmatching scheme that captures prompt-relevant local features from images,\nleading to an augmented view of the input image where prompt-relevant content\nis reserved while irrelevant distractions are masked. With the augmented view,\na calibrated decoding distribution can be derived by integrating generative\nglobal features from the original image and discriminative local features from\nthe augmented image. Extensive experiments show that AGLA consistently\nmitigates object hallucinations and enhances general perception capability for\nLVLMs across various discriminative and generative benchmarks. Our code will be\nreleased at https://github.com/Lackel/AGLA.\n", "link": "http://arxiv.org/abs/2406.12718v1", "date": "2024-06-18", "relevancy": 2.1048, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5283}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGLA%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20with%20Assembly%20of%20Global%20and%20Local%20Attention&body=Title%3A%20AGLA%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20with%20Assembly%20of%20Global%20and%20Local%20Attention%0AAuthor%3A%20Wenbin%20An%20and%20Feng%20Tian%20and%20Sicong%20Leng%20and%20Jiahao%20Nie%20and%20Haonan%20Lin%20and%20QianYing%20Wang%20and%20Guang%20Dai%20and%20Ping%20Chen%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20Despite%20their%20great%20success%20across%20various%20multimodal%20tasks%2C%20Large%0AVision-Language%20Models%20%28LVLMs%29%20are%20facing%20a%20prevalent%20problem%20with%20object%0Ahallucinations%2C%20where%20the%20generated%20textual%20responses%20are%20inconsistent%20with%0Aground-truth%20objects%20in%20the%20given%20image.%20This%20paper%20investigates%20various%20LVLMs%0Aand%20pinpoints%20attention%20deficiency%20toward%20discriminative%20local%20image%20features%0Aas%20one%20root%20cause%20of%20object%20hallucinations.%20Specifically%2C%20LVLMs%20predominantly%0Aattend%20to%20prompt-independent%20global%20image%20features%2C%20while%20failing%20to%20capture%0Aprompt-relevant%20local%20features%2C%20consequently%20undermining%20the%20visual%20grounding%0Acapacity%20of%20LVLMs%20and%20leading%20to%20hallucinations.%20To%20this%20end%2C%20we%20propose%0AAssembly%20of%20Global%20and%20Local%20Attention%20%28AGLA%29%2C%20a%20training-free%20and%0Aplug-and-play%20approach%20that%20mitigates%20object%20hallucinations%20by%20exploring%20an%0Aensemble%20of%20global%20features%20for%20response%20generation%20and%20local%20features%20for%0Avisual%20discrimination%20simultaneously.%20Our%20approach%20exhibits%20an%20image-prompt%0Amatching%20scheme%20that%20captures%20prompt-relevant%20local%20features%20from%20images%2C%0Aleading%20to%20an%20augmented%20view%20of%20the%20input%20image%20where%20prompt-relevant%20content%0Ais%20reserved%20while%20irrelevant%20distractions%20are%20masked.%20With%20the%20augmented%20view%2C%0Aa%20calibrated%20decoding%20distribution%20can%20be%20derived%20by%20integrating%20generative%0Aglobal%20features%20from%20the%20original%20image%20and%20discriminative%20local%20features%20from%0Athe%20augmented%20image.%20Extensive%20experiments%20show%20that%20AGLA%20consistently%0Amitigates%20object%20hallucinations%20and%20enhances%20general%20perception%20capability%20for%0ALVLMs%20across%20various%20discriminative%20and%20generative%20benchmarks.%20Our%20code%20will%20be%0Areleased%20at%20https%3A//github.com/Lackel/AGLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGLA%253A%2520Mitigating%2520Object%2520Hallucinations%2520in%2520Large%2520Vision-Language%2520Models%250A%2520%2520with%2520Assembly%2520of%2520Global%2520and%2520Local%2520Attention%26entry.906535625%3DWenbin%2520An%2520and%2520Feng%2520Tian%2520and%2520Sicong%2520Leng%2520and%2520Jiahao%2520Nie%2520and%2520Haonan%2520Lin%2520and%2520QianYing%2520Wang%2520and%2520Guang%2520Dai%2520and%2520Ping%2520Chen%2520and%2520Shijian%2520Lu%26entry.1292438233%3D%2520%2520Despite%2520their%2520great%2520success%2520across%2520various%2520multimodal%2520tasks%252C%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520are%2520facing%2520a%2520prevalent%2520problem%2520with%2520object%250Ahallucinations%252C%2520where%2520the%2520generated%2520textual%2520responses%2520are%2520inconsistent%2520with%250Aground-truth%2520objects%2520in%2520the%2520given%2520image.%2520This%2520paper%2520investigates%2520various%2520LVLMs%250Aand%2520pinpoints%2520attention%2520deficiency%2520toward%2520discriminative%2520local%2520image%2520features%250Aas%2520one%2520root%2520cause%2520of%2520object%2520hallucinations.%2520Specifically%252C%2520LVLMs%2520predominantly%250Aattend%2520to%2520prompt-independent%2520global%2520image%2520features%252C%2520while%2520failing%2520to%2520capture%250Aprompt-relevant%2520local%2520features%252C%2520consequently%2520undermining%2520the%2520visual%2520grounding%250Acapacity%2520of%2520LVLMs%2520and%2520leading%2520to%2520hallucinations.%2520To%2520this%2520end%252C%2520we%2520propose%250AAssembly%2520of%2520Global%2520and%2520Local%2520Attention%2520%2528AGLA%2529%252C%2520a%2520training-free%2520and%250Aplug-and-play%2520approach%2520that%2520mitigates%2520object%2520hallucinations%2520by%2520exploring%2520an%250Aensemble%2520of%2520global%2520features%2520for%2520response%2520generation%2520and%2520local%2520features%2520for%250Avisual%2520discrimination%2520simultaneously.%2520Our%2520approach%2520exhibits%2520an%2520image-prompt%250Amatching%2520scheme%2520that%2520captures%2520prompt-relevant%2520local%2520features%2520from%2520images%252C%250Aleading%2520to%2520an%2520augmented%2520view%2520of%2520the%2520input%2520image%2520where%2520prompt-relevant%2520content%250Ais%2520reserved%2520while%2520irrelevant%2520distractions%2520are%2520masked.%2520With%2520the%2520augmented%2520view%252C%250Aa%2520calibrated%2520decoding%2520distribution%2520can%2520be%2520derived%2520by%2520integrating%2520generative%250Aglobal%2520features%2520from%2520the%2520original%2520image%2520and%2520discriminative%2520local%2520features%2520from%250Athe%2520augmented%2520image.%2520Extensive%2520experiments%2520show%2520that%2520AGLA%2520consistently%250Amitigates%2520object%2520hallucinations%2520and%2520enhances%2520general%2520perception%2520capability%2520for%250ALVLMs%2520across%2520various%2520discriminative%2520and%2520generative%2520benchmarks.%2520Our%2520code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/Lackel/AGLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGLA%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20with%20Assembly%20of%20Global%20and%20Local%20Attention&entry.906535625=Wenbin%20An%20and%20Feng%20Tian%20and%20Sicong%20Leng%20and%20Jiahao%20Nie%20and%20Haonan%20Lin%20and%20QianYing%20Wang%20and%20Guang%20Dai%20and%20Ping%20Chen%20and%20Shijian%20Lu&entry.1292438233=%20%20Despite%20their%20great%20success%20across%20various%20multimodal%20tasks%2C%20Large%0AVision-Language%20Models%20%28LVLMs%29%20are%20facing%20a%20prevalent%20problem%20with%20object%0Ahallucinations%2C%20where%20the%20generated%20textual%20responses%20are%20inconsistent%20with%0Aground-truth%20objects%20in%20the%20given%20image.%20This%20paper%20investigates%20various%20LVLMs%0Aand%20pinpoints%20attention%20deficiency%20toward%20discriminative%20local%20image%20features%0Aas%20one%20root%20cause%20of%20object%20hallucinations.%20Specifically%2C%20LVLMs%20predominantly%0Aattend%20to%20prompt-independent%20global%20image%20features%2C%20while%20failing%20to%20capture%0Aprompt-relevant%20local%20features%2C%20consequently%20undermining%20the%20visual%20grounding%0Acapacity%20of%20LVLMs%20and%20leading%20to%20hallucinations.%20To%20this%20end%2C%20we%20propose%0AAssembly%20of%20Global%20and%20Local%20Attention%20%28AGLA%29%2C%20a%20training-free%20and%0Aplug-and-play%20approach%20that%20mitigates%20object%20hallucinations%20by%20exploring%20an%0Aensemble%20of%20global%20features%20for%20response%20generation%20and%20local%20features%20for%0Avisual%20discrimination%20simultaneously.%20Our%20approach%20exhibits%20an%20image-prompt%0Amatching%20scheme%20that%20captures%20prompt-relevant%20local%20features%20from%20images%2C%0Aleading%20to%20an%20augmented%20view%20of%20the%20input%20image%20where%20prompt-relevant%20content%0Ais%20reserved%20while%20irrelevant%20distractions%20are%20masked.%20With%20the%20augmented%20view%2C%0Aa%20calibrated%20decoding%20distribution%20can%20be%20derived%20by%20integrating%20generative%0Aglobal%20features%20from%20the%20original%20image%20and%20discriminative%20local%20features%20from%0Athe%20augmented%20image.%20Extensive%20experiments%20show%20that%20AGLA%20consistently%0Amitigates%20object%20hallucinations%20and%20enhances%20general%20perception%20capability%20for%0ALVLMs%20across%20various%20discriminative%20and%20generative%20benchmarks.%20Our%20code%20will%20be%0Areleased%20at%20https%3A//github.com/Lackel/AGLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12718v1&entry.124074799=Read"},
{"title": "ChangeViT: Unleashing Plain Vision Transformers for Change Detection", "author": "Duowang Zhu and Xiaohu Huang and Haiyan Huang and Zhenfeng Shao and Qimin Cheng", "abstract": "  Change detection in remote sensing images is essential for tracking\nenvironmental changes on the Earth's surface. Despite the success of vision\ntransformers (ViTs) as backbones in numerous computer vision applications, they\nremain underutilized in change detection, where convolutional neural networks\n(CNNs) continue to dominate due to their powerful feature extraction\ncapabilities. In this paper, our study uncovers ViTs' unique advantage in\ndiscerning large-scale changes, a capability where CNNs fall short.\nCapitalizing on this insight, we introduce ChangeViT, a framework that adopts a\nplain ViT backbone to enhance the performance of large-scale changes. This\nframework is supplemented by a detail-capture module that generates detailed\nspatial features and a feature injector that efficiently integrates\nfine-grained spatial information into high-level semantic learning. The feature\nintegration ensures that ChangeViT excels in both detecting large-scale changes\nand capturing fine-grained details, providing comprehensive change detection\nacross diverse scales. Without bells and whistles, ChangeViT achieves\nstate-of-the-art performance on three popular high-resolution datasets (i.e.,\nLEVIR-CD, WHU-CD, and CLCD) and one low-resolution dataset (i.e., OSCD), which\nunderscores the unleashed potential of plain ViTs for change detection.\nFurthermore, thorough quantitative and qualitative analyses validate the\nefficacy of the introduced modules, solidifying the effectiveness of our\napproach. The source code is available at\nhttps://github.com/zhuduowang/ChangeViT.\n", "link": "http://arxiv.org/abs/2406.12847v1", "date": "2024-06-18", "relevancy": 2.0983, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5297}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5268}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChangeViT%3A%20Unleashing%20Plain%20Vision%20Transformers%20for%20Change%20Detection&body=Title%3A%20ChangeViT%3A%20Unleashing%20Plain%20Vision%20Transformers%20for%20Change%20Detection%0AAuthor%3A%20Duowang%20Zhu%20and%20Xiaohu%20Huang%20and%20Haiyan%20Huang%20and%20Zhenfeng%20Shao%20and%20Qimin%20Cheng%0AAbstract%3A%20%20%20Change%20detection%20in%20remote%20sensing%20images%20is%20essential%20for%20tracking%0Aenvironmental%20changes%20on%20the%20Earth%27s%20surface.%20Despite%20the%20success%20of%20vision%0Atransformers%20%28ViTs%29%20as%20backbones%20in%20numerous%20computer%20vision%20applications%2C%20they%0Aremain%20underutilized%20in%20change%20detection%2C%20where%20convolutional%20neural%20networks%0A%28CNNs%29%20continue%20to%20dominate%20due%20to%20their%20powerful%20feature%20extraction%0Acapabilities.%20In%20this%20paper%2C%20our%20study%20uncovers%20ViTs%27%20unique%20advantage%20in%0Adiscerning%20large-scale%20changes%2C%20a%20capability%20where%20CNNs%20fall%20short.%0ACapitalizing%20on%20this%20insight%2C%20we%20introduce%20ChangeViT%2C%20a%20framework%20that%20adopts%20a%0Aplain%20ViT%20backbone%20to%20enhance%20the%20performance%20of%20large-scale%20changes.%20This%0Aframework%20is%20supplemented%20by%20a%20detail-capture%20module%20that%20generates%20detailed%0Aspatial%20features%20and%20a%20feature%20injector%20that%20efficiently%20integrates%0Afine-grained%20spatial%20information%20into%20high-level%20semantic%20learning.%20The%20feature%0Aintegration%20ensures%20that%20ChangeViT%20excels%20in%20both%20detecting%20large-scale%20changes%0Aand%20capturing%20fine-grained%20details%2C%20providing%20comprehensive%20change%20detection%0Aacross%20diverse%20scales.%20Without%20bells%20and%20whistles%2C%20ChangeViT%20achieves%0Astate-of-the-art%20performance%20on%20three%20popular%20high-resolution%20datasets%20%28i.e.%2C%0ALEVIR-CD%2C%20WHU-CD%2C%20and%20CLCD%29%20and%20one%20low-resolution%20dataset%20%28i.e.%2C%20OSCD%29%2C%20which%0Aunderscores%20the%20unleashed%20potential%20of%20plain%20ViTs%20for%20change%20detection.%0AFurthermore%2C%20thorough%20quantitative%20and%20qualitative%20analyses%20validate%20the%0Aefficacy%20of%20the%20introduced%20modules%2C%20solidifying%20the%20effectiveness%20of%20our%0Aapproach.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/zhuduowang/ChangeViT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChangeViT%253A%2520Unleashing%2520Plain%2520Vision%2520Transformers%2520for%2520Change%2520Detection%26entry.906535625%3DDuowang%2520Zhu%2520and%2520Xiaohu%2520Huang%2520and%2520Haiyan%2520Huang%2520and%2520Zhenfeng%2520Shao%2520and%2520Qimin%2520Cheng%26entry.1292438233%3D%2520%2520Change%2520detection%2520in%2520remote%2520sensing%2520images%2520is%2520essential%2520for%2520tracking%250Aenvironmental%2520changes%2520on%2520the%2520Earth%2527s%2520surface.%2520Despite%2520the%2520success%2520of%2520vision%250Atransformers%2520%2528ViTs%2529%2520as%2520backbones%2520in%2520numerous%2520computer%2520vision%2520applications%252C%2520they%250Aremain%2520underutilized%2520in%2520change%2520detection%252C%2520where%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520continue%2520to%2520dominate%2520due%2520to%2520their%2520powerful%2520feature%2520extraction%250Acapabilities.%2520In%2520this%2520paper%252C%2520our%2520study%2520uncovers%2520ViTs%2527%2520unique%2520advantage%2520in%250Adiscerning%2520large-scale%2520changes%252C%2520a%2520capability%2520where%2520CNNs%2520fall%2520short.%250ACapitalizing%2520on%2520this%2520insight%252C%2520we%2520introduce%2520ChangeViT%252C%2520a%2520framework%2520that%2520adopts%2520a%250Aplain%2520ViT%2520backbone%2520to%2520enhance%2520the%2520performance%2520of%2520large-scale%2520changes.%2520This%250Aframework%2520is%2520supplemented%2520by%2520a%2520detail-capture%2520module%2520that%2520generates%2520detailed%250Aspatial%2520features%2520and%2520a%2520feature%2520injector%2520that%2520efficiently%2520integrates%250Afine-grained%2520spatial%2520information%2520into%2520high-level%2520semantic%2520learning.%2520The%2520feature%250Aintegration%2520ensures%2520that%2520ChangeViT%2520excels%2520in%2520both%2520detecting%2520large-scale%2520changes%250Aand%2520capturing%2520fine-grained%2520details%252C%2520providing%2520comprehensive%2520change%2520detection%250Aacross%2520diverse%2520scales.%2520Without%2520bells%2520and%2520whistles%252C%2520ChangeViT%2520achieves%250Astate-of-the-art%2520performance%2520on%2520three%2520popular%2520high-resolution%2520datasets%2520%2528i.e.%252C%250ALEVIR-CD%252C%2520WHU-CD%252C%2520and%2520CLCD%2529%2520and%2520one%2520low-resolution%2520dataset%2520%2528i.e.%252C%2520OSCD%2529%252C%2520which%250Aunderscores%2520the%2520unleashed%2520potential%2520of%2520plain%2520ViTs%2520for%2520change%2520detection.%250AFurthermore%252C%2520thorough%2520quantitative%2520and%2520qualitative%2520analyses%2520validate%2520the%250Aefficacy%2520of%2520the%2520introduced%2520modules%252C%2520solidifying%2520the%2520effectiveness%2520of%2520our%250Aapproach.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zhuduowang/ChangeViT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChangeViT%3A%20Unleashing%20Plain%20Vision%20Transformers%20for%20Change%20Detection&entry.906535625=Duowang%20Zhu%20and%20Xiaohu%20Huang%20and%20Haiyan%20Huang%20and%20Zhenfeng%20Shao%20and%20Qimin%20Cheng&entry.1292438233=%20%20Change%20detection%20in%20remote%20sensing%20images%20is%20essential%20for%20tracking%0Aenvironmental%20changes%20on%20the%20Earth%27s%20surface.%20Despite%20the%20success%20of%20vision%0Atransformers%20%28ViTs%29%20as%20backbones%20in%20numerous%20computer%20vision%20applications%2C%20they%0Aremain%20underutilized%20in%20change%20detection%2C%20where%20convolutional%20neural%20networks%0A%28CNNs%29%20continue%20to%20dominate%20due%20to%20their%20powerful%20feature%20extraction%0Acapabilities.%20In%20this%20paper%2C%20our%20study%20uncovers%20ViTs%27%20unique%20advantage%20in%0Adiscerning%20large-scale%20changes%2C%20a%20capability%20where%20CNNs%20fall%20short.%0ACapitalizing%20on%20this%20insight%2C%20we%20introduce%20ChangeViT%2C%20a%20framework%20that%20adopts%20a%0Aplain%20ViT%20backbone%20to%20enhance%20the%20performance%20of%20large-scale%20changes.%20This%0Aframework%20is%20supplemented%20by%20a%20detail-capture%20module%20that%20generates%20detailed%0Aspatial%20features%20and%20a%20feature%20injector%20that%20efficiently%20integrates%0Afine-grained%20spatial%20information%20into%20high-level%20semantic%20learning.%20The%20feature%0Aintegration%20ensures%20that%20ChangeViT%20excels%20in%20both%20detecting%20large-scale%20changes%0Aand%20capturing%20fine-grained%20details%2C%20providing%20comprehensive%20change%20detection%0Aacross%20diverse%20scales.%20Without%20bells%20and%20whistles%2C%20ChangeViT%20achieves%0Astate-of-the-art%20performance%20on%20three%20popular%20high-resolution%20datasets%20%28i.e.%2C%0ALEVIR-CD%2C%20WHU-CD%2C%20and%20CLCD%29%20and%20one%20low-resolution%20dataset%20%28i.e.%2C%20OSCD%29%2C%20which%0Aunderscores%20the%20unleashed%20potential%20of%20plain%20ViTs%20for%20change%20detection.%0AFurthermore%2C%20thorough%20quantitative%20and%20qualitative%20analyses%20validate%20the%0Aefficacy%20of%20the%20introduced%20modules%2C%20solidifying%20the%20effectiveness%20of%20our%0Aapproach.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/zhuduowang/ChangeViT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12847v1&entry.124074799=Read"},
{"title": "Coarse-Fine Spectral-Aware Deformable Convolution For Hyperspectral\n  Image Reconstruction", "author": "Jincheng Yang and Lishun Wang and Miao Cao and Huan Wang and Yinping Zhao and Xin Yuan", "abstract": "  We study the inverse problem of Coded Aperture Snapshot Spectral Imaging\n(CASSI), which captures a spatial-spectral data cube using snapshot 2D\nmeasurements and uses algorithms to reconstruct 3D hyperspectral images (HSI).\nHowever, current methods based on Convolutional Neural Networks (CNNs) struggle\nto capture long-range dependencies and non-local similarities. The recently\npopular Transformer-based methods are poorly deployed on downstream tasks due\nto the high computational cost caused by self-attention. In this paper, we\npropose Coarse-Fine Spectral-Aware Deformable Convolution Network (CFSDCN),\napplying deformable convolutional networks (DCN) to this task for the first\ntime. Considering the sparsity of HSI, we design a deformable convolution\nmodule that exploits its deformability to capture long-range dependencies and\nnon-local similarities. In addition, we propose a new spectral information\ninteraction module that considers both coarse-grained and fine-grained spectral\nsimilarities. Extensive experiments demonstrate that our CFSDCN significantly\noutperforms previous state-of-the-art (SOTA) methods on both simulated and real\nHSI datasets.\n", "link": "http://arxiv.org/abs/2406.12703v1", "date": "2024-06-18", "relevancy": 2.0946, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5352}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5297}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse-Fine%20Spectral-Aware%20Deformable%20Convolution%20For%20Hyperspectral%0A%20%20Image%20Reconstruction&body=Title%3A%20Coarse-Fine%20Spectral-Aware%20Deformable%20Convolution%20For%20Hyperspectral%0A%20%20Image%20Reconstruction%0AAuthor%3A%20Jincheng%20Yang%20and%20Lishun%20Wang%20and%20Miao%20Cao%20and%20Huan%20Wang%20and%20Yinping%20Zhao%20and%20Xin%20Yuan%0AAbstract%3A%20%20%20We%20study%20the%20inverse%20problem%20of%20Coded%20Aperture%20Snapshot%20Spectral%20Imaging%0A%28CASSI%29%2C%20which%20captures%20a%20spatial-spectral%20data%20cube%20using%20snapshot%202D%0Ameasurements%20and%20uses%20algorithms%20to%20reconstruct%203D%20hyperspectral%20images%20%28HSI%29.%0AHowever%2C%20current%20methods%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20struggle%0Ato%20capture%20long-range%20dependencies%20and%20non-local%20similarities.%20The%20recently%0Apopular%20Transformer-based%20methods%20are%20poorly%20deployed%20on%20downstream%20tasks%20due%0Ato%20the%20high%20computational%20cost%20caused%20by%20self-attention.%20In%20this%20paper%2C%20we%0Apropose%20Coarse-Fine%20Spectral-Aware%20Deformable%20Convolution%20Network%20%28CFSDCN%29%2C%0Aapplying%20deformable%20convolutional%20networks%20%28DCN%29%20to%20this%20task%20for%20the%20first%0Atime.%20Considering%20the%20sparsity%20of%20HSI%2C%20we%20design%20a%20deformable%20convolution%0Amodule%20that%20exploits%20its%20deformability%20to%20capture%20long-range%20dependencies%20and%0Anon-local%20similarities.%20In%20addition%2C%20we%20propose%20a%20new%20spectral%20information%0Ainteraction%20module%20that%20considers%20both%20coarse-grained%20and%20fine-grained%20spectral%0Asimilarities.%20Extensive%20experiments%20demonstrate%20that%20our%20CFSDCN%20significantly%0Aoutperforms%20previous%20state-of-the-art%20%28SOTA%29%20methods%20on%20both%20simulated%20and%20real%0AHSI%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse-Fine%2520Spectral-Aware%2520Deformable%2520Convolution%2520For%2520Hyperspectral%250A%2520%2520Image%2520Reconstruction%26entry.906535625%3DJincheng%2520Yang%2520and%2520Lishun%2520Wang%2520and%2520Miao%2520Cao%2520and%2520Huan%2520Wang%2520and%2520Yinping%2520Zhao%2520and%2520Xin%2520Yuan%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520inverse%2520problem%2520of%2520Coded%2520Aperture%2520Snapshot%2520Spectral%2520Imaging%250A%2528CASSI%2529%252C%2520which%2520captures%2520a%2520spatial-spectral%2520data%2520cube%2520using%2520snapshot%25202D%250Ameasurements%2520and%2520uses%2520algorithms%2520to%2520reconstruct%25203D%2520hyperspectral%2520images%2520%2528HSI%2529.%250AHowever%252C%2520current%2520methods%2520based%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520struggle%250Ato%2520capture%2520long-range%2520dependencies%2520and%2520non-local%2520similarities.%2520The%2520recently%250Apopular%2520Transformer-based%2520methods%2520are%2520poorly%2520deployed%2520on%2520downstream%2520tasks%2520due%250Ato%2520the%2520high%2520computational%2520cost%2520caused%2520by%2520self-attention.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Coarse-Fine%2520Spectral-Aware%2520Deformable%2520Convolution%2520Network%2520%2528CFSDCN%2529%252C%250Aapplying%2520deformable%2520convolutional%2520networks%2520%2528DCN%2529%2520to%2520this%2520task%2520for%2520the%2520first%250Atime.%2520Considering%2520the%2520sparsity%2520of%2520HSI%252C%2520we%2520design%2520a%2520deformable%2520convolution%250Amodule%2520that%2520exploits%2520its%2520deformability%2520to%2520capture%2520long-range%2520dependencies%2520and%250Anon-local%2520similarities.%2520In%2520addition%252C%2520we%2520propose%2520a%2520new%2520spectral%2520information%250Ainteraction%2520module%2520that%2520considers%2520both%2520coarse-grained%2520and%2520fine-grained%2520spectral%250Asimilarities.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520CFSDCN%2520significantly%250Aoutperforms%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520on%2520both%2520simulated%2520and%2520real%250AHSI%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse-Fine%20Spectral-Aware%20Deformable%20Convolution%20For%20Hyperspectral%0A%20%20Image%20Reconstruction&entry.906535625=Jincheng%20Yang%20and%20Lishun%20Wang%20and%20Miao%20Cao%20and%20Huan%20Wang%20and%20Yinping%20Zhao%20and%20Xin%20Yuan&entry.1292438233=%20%20We%20study%20the%20inverse%20problem%20of%20Coded%20Aperture%20Snapshot%20Spectral%20Imaging%0A%28CASSI%29%2C%20which%20captures%20a%20spatial-spectral%20data%20cube%20using%20snapshot%202D%0Ameasurements%20and%20uses%20algorithms%20to%20reconstruct%203D%20hyperspectral%20images%20%28HSI%29.%0AHowever%2C%20current%20methods%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20struggle%0Ato%20capture%20long-range%20dependencies%20and%20non-local%20similarities.%20The%20recently%0Apopular%20Transformer-based%20methods%20are%20poorly%20deployed%20on%20downstream%20tasks%20due%0Ato%20the%20high%20computational%20cost%20caused%20by%20self-attention.%20In%20this%20paper%2C%20we%0Apropose%20Coarse-Fine%20Spectral-Aware%20Deformable%20Convolution%20Network%20%28CFSDCN%29%2C%0Aapplying%20deformable%20convolutional%20networks%20%28DCN%29%20to%20this%20task%20for%20the%20first%0Atime.%20Considering%20the%20sparsity%20of%20HSI%2C%20we%20design%20a%20deformable%20convolution%0Amodule%20that%20exploits%20its%20deformability%20to%20capture%20long-range%20dependencies%20and%0Anon-local%20similarities.%20In%20addition%2C%20we%20propose%20a%20new%20spectral%20information%0Ainteraction%20module%20that%20considers%20both%20coarse-grained%20and%20fine-grained%20spectral%0Asimilarities.%20Extensive%20experiments%20demonstrate%20that%20our%20CFSDCN%20significantly%0Aoutperforms%20previous%20state-of-the-art%20%28SOTA%29%20methods%20on%20both%20simulated%20and%20real%0AHSI%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12703v1&entry.124074799=Read"},
{"title": "Improving global awareness of linkset predictions using Cross-Attentive\n  Modulation tokens", "author": "F\u00e9lix Marcoccia and C\u00e9dric Adjih and Paul M\u00fchlethaler", "abstract": "  Most of multiple link prediction or graph generation techniques rely on the\nattention mechanism or on Graph Neural Networks (GNNs), which consist in\nleveraging node-level information exchanges in order to form proper link\npredictions. Such node-level interactions do not process nodes as an ordered\nsequence, which would imply some kind of natural ordering of the nodes: they\nare said to be permutation invariant mechanisms. They are well suited for graph\nproblems, but struggle at providing a global orchestration of the predicted\nlinks, which can result in a loss of performance. Some typical issues can be\nthe difficulty to ensure high-level properties such as global connectedness,\nfixed diameter or to avoid information bottleneck effects such as oversmoothing\nand oversquashing, which respectively consist in abundant smoothing in dense\nareas leading to a loss of information and a tendency to exclude isolated nodes\nfrom the message passing scheme, and often result in irrelevant, unbalanced\nlink predictions. To tackle this problem, we hereby present Cross-Attentive\nModulation (CAM) tokens, which introduce cross-attentive units used to\ncondition node and edge-level modulations in order to enable context-aware\ncomputations that improve the global consistency of the prediction links. We\nwill implement it on a few permutation invariant architectures, and showcase\nbenchmarks that prove the merits of our work.\n", "link": "http://arxiv.org/abs/2405.19375v2", "date": "2024-06-18", "relevancy": 2.0783, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5348}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20global%20awareness%20of%20linkset%20predictions%20using%20Cross-Attentive%0A%20%20Modulation%20tokens&body=Title%3A%20Improving%20global%20awareness%20of%20linkset%20predictions%20using%20Cross-Attentive%0A%20%20Modulation%20tokens%0AAuthor%3A%20F%C3%A9lix%20Marcoccia%20and%20C%C3%A9dric%20Adjih%20and%20Paul%20M%C3%BChlethaler%0AAbstract%3A%20%20%20Most%20of%20multiple%20link%20prediction%20or%20graph%20generation%20techniques%20rely%20on%20the%0Aattention%20mechanism%20or%20on%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20consist%20in%0Aleveraging%20node-level%20information%20exchanges%20in%20order%20to%20form%20proper%20link%0Apredictions.%20Such%20node-level%20interactions%20do%20not%20process%20nodes%20as%20an%20ordered%0Asequence%2C%20which%20would%20imply%20some%20kind%20of%20natural%20ordering%20of%20the%20nodes%3A%20they%0Aare%20said%20to%20be%20permutation%20invariant%20mechanisms.%20They%20are%20well%20suited%20for%20graph%0Aproblems%2C%20but%20struggle%20at%20providing%20a%20global%20orchestration%20of%20the%20predicted%0Alinks%2C%20which%20can%20result%20in%20a%20loss%20of%20performance.%20Some%20typical%20issues%20can%20be%0Athe%20difficulty%20to%20ensure%20high-level%20properties%20such%20as%20global%20connectedness%2C%0Afixed%20diameter%20or%20to%20avoid%20information%20bottleneck%20effects%20such%20as%20oversmoothing%0Aand%20oversquashing%2C%20which%20respectively%20consist%20in%20abundant%20smoothing%20in%20dense%0Aareas%20leading%20to%20a%20loss%20of%20information%20and%20a%20tendency%20to%20exclude%20isolated%20nodes%0Afrom%20the%20message%20passing%20scheme%2C%20and%20often%20result%20in%20irrelevant%2C%20unbalanced%0Alink%20predictions.%20To%20tackle%20this%20problem%2C%20we%20hereby%20present%20Cross-Attentive%0AModulation%20%28CAM%29%20tokens%2C%20which%20introduce%20cross-attentive%20units%20used%20to%0Acondition%20node%20and%20edge-level%20modulations%20in%20order%20to%20enable%20context-aware%0Acomputations%20that%20improve%20the%20global%20consistency%20of%20the%20prediction%20links.%20We%0Awill%20implement%20it%20on%20a%20few%20permutation%20invariant%20architectures%2C%20and%20showcase%0Abenchmarks%20that%20prove%20the%20merits%20of%20our%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19375v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520global%2520awareness%2520of%2520linkset%2520predictions%2520using%2520Cross-Attentive%250A%2520%2520Modulation%2520tokens%26entry.906535625%3DF%25C3%25A9lix%2520Marcoccia%2520and%2520C%25C3%25A9dric%2520Adjih%2520and%2520Paul%2520M%25C3%25BChlethaler%26entry.1292438233%3D%2520%2520Most%2520of%2520multiple%2520link%2520prediction%2520or%2520graph%2520generation%2520techniques%2520rely%2520on%2520the%250Aattention%2520mechanism%2520or%2520on%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520which%2520consist%2520in%250Aleveraging%2520node-level%2520information%2520exchanges%2520in%2520order%2520to%2520form%2520proper%2520link%250Apredictions.%2520Such%2520node-level%2520interactions%2520do%2520not%2520process%2520nodes%2520as%2520an%2520ordered%250Asequence%252C%2520which%2520would%2520imply%2520some%2520kind%2520of%2520natural%2520ordering%2520of%2520the%2520nodes%253A%2520they%250Aare%2520said%2520to%2520be%2520permutation%2520invariant%2520mechanisms.%2520They%2520are%2520well%2520suited%2520for%2520graph%250Aproblems%252C%2520but%2520struggle%2520at%2520providing%2520a%2520global%2520orchestration%2520of%2520the%2520predicted%250Alinks%252C%2520which%2520can%2520result%2520in%2520a%2520loss%2520of%2520performance.%2520Some%2520typical%2520issues%2520can%2520be%250Athe%2520difficulty%2520to%2520ensure%2520high-level%2520properties%2520such%2520as%2520global%2520connectedness%252C%250Afixed%2520diameter%2520or%2520to%2520avoid%2520information%2520bottleneck%2520effects%2520such%2520as%2520oversmoothing%250Aand%2520oversquashing%252C%2520which%2520respectively%2520consist%2520in%2520abundant%2520smoothing%2520in%2520dense%250Aareas%2520leading%2520to%2520a%2520loss%2520of%2520information%2520and%2520a%2520tendency%2520to%2520exclude%2520isolated%2520nodes%250Afrom%2520the%2520message%2520passing%2520scheme%252C%2520and%2520often%2520result%2520in%2520irrelevant%252C%2520unbalanced%250Alink%2520predictions.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520hereby%2520present%2520Cross-Attentive%250AModulation%2520%2528CAM%2529%2520tokens%252C%2520which%2520introduce%2520cross-attentive%2520units%2520used%2520to%250Acondition%2520node%2520and%2520edge-level%2520modulations%2520in%2520order%2520to%2520enable%2520context-aware%250Acomputations%2520that%2520improve%2520the%2520global%2520consistency%2520of%2520the%2520prediction%2520links.%2520We%250Awill%2520implement%2520it%2520on%2520a%2520few%2520permutation%2520invariant%2520architectures%252C%2520and%2520showcase%250Abenchmarks%2520that%2520prove%2520the%2520merits%2520of%2520our%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19375v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20global%20awareness%20of%20linkset%20predictions%20using%20Cross-Attentive%0A%20%20Modulation%20tokens&entry.906535625=F%C3%A9lix%20Marcoccia%20and%20C%C3%A9dric%20Adjih%20and%20Paul%20M%C3%BChlethaler&entry.1292438233=%20%20Most%20of%20multiple%20link%20prediction%20or%20graph%20generation%20techniques%20rely%20on%20the%0Aattention%20mechanism%20or%20on%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20consist%20in%0Aleveraging%20node-level%20information%20exchanges%20in%20order%20to%20form%20proper%20link%0Apredictions.%20Such%20node-level%20interactions%20do%20not%20process%20nodes%20as%20an%20ordered%0Asequence%2C%20which%20would%20imply%20some%20kind%20of%20natural%20ordering%20of%20the%20nodes%3A%20they%0Aare%20said%20to%20be%20permutation%20invariant%20mechanisms.%20They%20are%20well%20suited%20for%20graph%0Aproblems%2C%20but%20struggle%20at%20providing%20a%20global%20orchestration%20of%20the%20predicted%0Alinks%2C%20which%20can%20result%20in%20a%20loss%20of%20performance.%20Some%20typical%20issues%20can%20be%0Athe%20difficulty%20to%20ensure%20high-level%20properties%20such%20as%20global%20connectedness%2C%0Afixed%20diameter%20or%20to%20avoid%20information%20bottleneck%20effects%20such%20as%20oversmoothing%0Aand%20oversquashing%2C%20which%20respectively%20consist%20in%20abundant%20smoothing%20in%20dense%0Aareas%20leading%20to%20a%20loss%20of%20information%20and%20a%20tendency%20to%20exclude%20isolated%20nodes%0Afrom%20the%20message%20passing%20scheme%2C%20and%20often%20result%20in%20irrelevant%2C%20unbalanced%0Alink%20predictions.%20To%20tackle%20this%20problem%2C%20we%20hereby%20present%20Cross-Attentive%0AModulation%20%28CAM%29%20tokens%2C%20which%20introduce%20cross-attentive%20units%20used%20to%0Acondition%20node%20and%20edge-level%20modulations%20in%20order%20to%20enable%20context-aware%0Acomputations%20that%20improve%20the%20global%20consistency%20of%20the%20prediction%20links.%20We%0Awill%20implement%20it%20on%20a%20few%20permutation%20invariant%20architectures%2C%20and%20showcase%0Abenchmarks%20that%20prove%20the%20merits%20of%20our%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19375v2&entry.124074799=Read"},
{"title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional\n  Adaptation", "author": "Seyedarmin Azizi and Souvik Kundu and Massoud Pedram", "abstract": "  Low-rank adaptation (LoRA) has become the default approach to fine-tune large\nlanguage models (LLMs) due to its significant reduction in trainable\nparameters. However, trainable parameter demand for LoRA increases with\nincreasing model embedding dimensions, leading to high compute costs.\nAdditionally, its backward updates require storing high-dimensional\nintermediate activations and optimizer states, demanding high peak GPU memory.\nIn this paper, we introduce large model fine-tuning via spectrally decomposed\nlow-dimensional adaptation (LaMDA), a novel approach to fine-tuning large\nlanguage models, which leverages low-dimensional adaptation to achieve\nsignificant reductions in trainable parameters and peak GPU memory footprint.\nLaMDA freezes a first projection matrix (PMA) in the adaptation path while\nintroducing a low-dimensional trainable square matrix, resulting in substantial\nreductions in trainable parameters and peak GPU memory usage. LaMDA gradually\nfreezes a second projection matrix (PMB) during the early fine-tuning stages,\nreducing the compute cost associated with weight updates to enhance parameter\nefficiency further. We also present an enhancement, LaMDA++, incorporating a\n``lite-weight\" adaptive rank allocation for the LoRA path via normalized\nspectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++\nacross various tasks, including natural language understanding with the GLUE\nbenchmark, text summarization, natural language generation, and complex\nreasoning on different LLMs. Results show that LaMDA matches or surpasses the\nperformance of existing alternatives while requiring up to 17.7x fewer\nparameter updates and up to 1.32x lower peak GPU memory usage during\nfine-tuning. Code will be publicly available.\n", "link": "http://arxiv.org/abs/2406.12832v1", "date": "2024-06-18", "relevancy": 2.0773, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5176}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaMDA%3A%20Large%20Model%20Fine-Tuning%20via%20Spectrally%20Decomposed%20Low-Dimensional%0A%20%20Adaptation&body=Title%3A%20LaMDA%3A%20Large%20Model%20Fine-Tuning%20via%20Spectrally%20Decomposed%20Low-Dimensional%0A%20%20Adaptation%0AAuthor%3A%20Seyedarmin%20Azizi%20and%20Souvik%20Kundu%20and%20Massoud%20Pedram%0AAbstract%3A%20%20%20Low-rank%20adaptation%20%28LoRA%29%20has%20become%20the%20default%20approach%20to%20fine-tune%20large%0Alanguage%20models%20%28LLMs%29%20due%20to%20its%20significant%20reduction%20in%20trainable%0Aparameters.%20However%2C%20trainable%20parameter%20demand%20for%20LoRA%20increases%20with%0Aincreasing%20model%20embedding%20dimensions%2C%20leading%20to%20high%20compute%20costs.%0AAdditionally%2C%20its%20backward%20updates%20require%20storing%20high-dimensional%0Aintermediate%20activations%20and%20optimizer%20states%2C%20demanding%20high%20peak%20GPU%20memory.%0AIn%20this%20paper%2C%20we%20introduce%20large%20model%20fine-tuning%20via%20spectrally%20decomposed%0Alow-dimensional%20adaptation%20%28LaMDA%29%2C%20a%20novel%20approach%20to%20fine-tuning%20large%0Alanguage%20models%2C%20which%20leverages%20low-dimensional%20adaptation%20to%20achieve%0Asignificant%20reductions%20in%20trainable%20parameters%20and%20peak%20GPU%20memory%20footprint.%0ALaMDA%20freezes%20a%20first%20projection%20matrix%20%28PMA%29%20in%20the%20adaptation%20path%20while%0Aintroducing%20a%20low-dimensional%20trainable%20square%20matrix%2C%20resulting%20in%20substantial%0Areductions%20in%20trainable%20parameters%20and%20peak%20GPU%20memory%20usage.%20LaMDA%20gradually%0Afreezes%20a%20second%20projection%20matrix%20%28PMB%29%20during%20the%20early%20fine-tuning%20stages%2C%0Areducing%20the%20compute%20cost%20associated%20with%20weight%20updates%20to%20enhance%20parameter%0Aefficiency%20further.%20We%20also%20present%20an%20enhancement%2C%20LaMDA%2B%2B%2C%20incorporating%20a%0A%60%60lite-weight%22%20adaptive%20rank%20allocation%20for%20the%20LoRA%20path%20via%20normalized%0Aspectrum%20analysis%20of%20pre-trained%20model%20weights.%20We%20evaluate%20LaMDA/LaMDA%2B%2B%0Aacross%20various%20tasks%2C%20including%20natural%20language%20understanding%20with%20the%20GLUE%0Abenchmark%2C%20text%20summarization%2C%20natural%20language%20generation%2C%20and%20complex%0Areasoning%20on%20different%20LLMs.%20Results%20show%20that%20LaMDA%20matches%20or%20surpasses%20the%0Aperformance%20of%20existing%20alternatives%20while%20requiring%20up%20to%2017.7x%20fewer%0Aparameter%20updates%20and%20up%20to%201.32x%20lower%20peak%20GPU%20memory%20usage%20during%0Afine-tuning.%20Code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaMDA%253A%2520Large%2520Model%2520Fine-Tuning%2520via%2520Spectrally%2520Decomposed%2520Low-Dimensional%250A%2520%2520Adaptation%26entry.906535625%3DSeyedarmin%2520Azizi%2520and%2520Souvik%2520Kundu%2520and%2520Massoud%2520Pedram%26entry.1292438233%3D%2520%2520Low-rank%2520adaptation%2520%2528LoRA%2529%2520has%2520become%2520the%2520default%2520approach%2520to%2520fine-tune%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520due%2520to%2520its%2520significant%2520reduction%2520in%2520trainable%250Aparameters.%2520However%252C%2520trainable%2520parameter%2520demand%2520for%2520LoRA%2520increases%2520with%250Aincreasing%2520model%2520embedding%2520dimensions%252C%2520leading%2520to%2520high%2520compute%2520costs.%250AAdditionally%252C%2520its%2520backward%2520updates%2520require%2520storing%2520high-dimensional%250Aintermediate%2520activations%2520and%2520optimizer%2520states%252C%2520demanding%2520high%2520peak%2520GPU%2520memory.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520large%2520model%2520fine-tuning%2520via%2520spectrally%2520decomposed%250Alow-dimensional%2520adaptation%2520%2528LaMDA%2529%252C%2520a%2520novel%2520approach%2520to%2520fine-tuning%2520large%250Alanguage%2520models%252C%2520which%2520leverages%2520low-dimensional%2520adaptation%2520to%2520achieve%250Asignificant%2520reductions%2520in%2520trainable%2520parameters%2520and%2520peak%2520GPU%2520memory%2520footprint.%250ALaMDA%2520freezes%2520a%2520first%2520projection%2520matrix%2520%2528PMA%2529%2520in%2520the%2520adaptation%2520path%2520while%250Aintroducing%2520a%2520low-dimensional%2520trainable%2520square%2520matrix%252C%2520resulting%2520in%2520substantial%250Areductions%2520in%2520trainable%2520parameters%2520and%2520peak%2520GPU%2520memory%2520usage.%2520LaMDA%2520gradually%250Afreezes%2520a%2520second%2520projection%2520matrix%2520%2528PMB%2529%2520during%2520the%2520early%2520fine-tuning%2520stages%252C%250Areducing%2520the%2520compute%2520cost%2520associated%2520with%2520weight%2520updates%2520to%2520enhance%2520parameter%250Aefficiency%2520further.%2520We%2520also%2520present%2520an%2520enhancement%252C%2520LaMDA%252B%252B%252C%2520incorporating%2520a%250A%2560%2560lite-weight%2522%2520adaptive%2520rank%2520allocation%2520for%2520the%2520LoRA%2520path%2520via%2520normalized%250Aspectrum%2520analysis%2520of%2520pre-trained%2520model%2520weights.%2520We%2520evaluate%2520LaMDA/LaMDA%252B%252B%250Aacross%2520various%2520tasks%252C%2520including%2520natural%2520language%2520understanding%2520with%2520the%2520GLUE%250Abenchmark%252C%2520text%2520summarization%252C%2520natural%2520language%2520generation%252C%2520and%2520complex%250Areasoning%2520on%2520different%2520LLMs.%2520Results%2520show%2520that%2520LaMDA%2520matches%2520or%2520surpasses%2520the%250Aperformance%2520of%2520existing%2520alternatives%2520while%2520requiring%2520up%2520to%252017.7x%2520fewer%250Aparameter%2520updates%2520and%2520up%2520to%25201.32x%2520lower%2520peak%2520GPU%2520memory%2520usage%2520during%250Afine-tuning.%2520Code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaMDA%3A%20Large%20Model%20Fine-Tuning%20via%20Spectrally%20Decomposed%20Low-Dimensional%0A%20%20Adaptation&entry.906535625=Seyedarmin%20Azizi%20and%20Souvik%20Kundu%20and%20Massoud%20Pedram&entry.1292438233=%20%20Low-rank%20adaptation%20%28LoRA%29%20has%20become%20the%20default%20approach%20to%20fine-tune%20large%0Alanguage%20models%20%28LLMs%29%20due%20to%20its%20significant%20reduction%20in%20trainable%0Aparameters.%20However%2C%20trainable%20parameter%20demand%20for%20LoRA%20increases%20with%0Aincreasing%20model%20embedding%20dimensions%2C%20leading%20to%20high%20compute%20costs.%0AAdditionally%2C%20its%20backward%20updates%20require%20storing%20high-dimensional%0Aintermediate%20activations%20and%20optimizer%20states%2C%20demanding%20high%20peak%20GPU%20memory.%0AIn%20this%20paper%2C%20we%20introduce%20large%20model%20fine-tuning%20via%20spectrally%20decomposed%0Alow-dimensional%20adaptation%20%28LaMDA%29%2C%20a%20novel%20approach%20to%20fine-tuning%20large%0Alanguage%20models%2C%20which%20leverages%20low-dimensional%20adaptation%20to%20achieve%0Asignificant%20reductions%20in%20trainable%20parameters%20and%20peak%20GPU%20memory%20footprint.%0ALaMDA%20freezes%20a%20first%20projection%20matrix%20%28PMA%29%20in%20the%20adaptation%20path%20while%0Aintroducing%20a%20low-dimensional%20trainable%20square%20matrix%2C%20resulting%20in%20substantial%0Areductions%20in%20trainable%20parameters%20and%20peak%20GPU%20memory%20usage.%20LaMDA%20gradually%0Afreezes%20a%20second%20projection%20matrix%20%28PMB%29%20during%20the%20early%20fine-tuning%20stages%2C%0Areducing%20the%20compute%20cost%20associated%20with%20weight%20updates%20to%20enhance%20parameter%0Aefficiency%20further.%20We%20also%20present%20an%20enhancement%2C%20LaMDA%2B%2B%2C%20incorporating%20a%0A%60%60lite-weight%22%20adaptive%20rank%20allocation%20for%20the%20LoRA%20path%20via%20normalized%0Aspectrum%20analysis%20of%20pre-trained%20model%20weights.%20We%20evaluate%20LaMDA/LaMDA%2B%2B%0Aacross%20various%20tasks%2C%20including%20natural%20language%20understanding%20with%20the%20GLUE%0Abenchmark%2C%20text%20summarization%2C%20natural%20language%20generation%2C%20and%20complex%0Areasoning%20on%20different%20LLMs.%20Results%20show%20that%20LaMDA%20matches%20or%20surpasses%20the%0Aperformance%20of%20existing%20alternatives%20while%20requiring%20up%20to%2017.7x%20fewer%0Aparameter%20updates%20and%20up%20to%201.32x%20lower%20peak%20GPU%20memory%20usage%20during%0Afine-tuning.%20Code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12832v1&entry.124074799=Read"},
{"title": "Tactile SoftHand-A: 3D-Printed, Tactile, Highly-underactuated,\n  Anthropomorphic Robot Hand with an Antagonistic Tendon Mechanism", "author": "Haoran Li and Christopher J. Ford and Chenghua Lu and Yijiong Lin and Matteo Bianchi and Manuel G. Catalano and Efi Psomopoulou and Nathan F. Lepora", "abstract": "  For tendon-driven multi-fingered robotic hands, ensuring grasp adaptability\nwhile minimizing the number of actuators needed to provide human-like\nfunctionality is a challenging problem. Inspired by the Pisa/IIT SoftHand, this\npaper introduces a 3D-printed, highly-underactuated, five-finger robotic hand\nnamed the Tactile SoftHand-A, which features only two actuators. The\ndual-tendon design allows for the active control of specific (distal or\nproximal interphalangeal) joints to adjust the hand's grasp gesture. We have\nalso developed a new design of fully 3D-printed tactile sensor that requires no\nhand assembly and is printed directly as part of the robotic finger. This\nsensor is integrated into the fingertips and combined with the antagonistic\ntendon mechanism to develop a human-hand-guided tactile feedback grasping\nsystem. The system can actively mirror human hand gestures, adaptively\nstabilize grasp gestures upon contact, and adjust grasp gestures to prevent\nobject movement after detecting slippage. Finally, we designed four different\nexperiments to evaluate the novel fingers coupled with the antagonistic\nmechanism for controlling the robotic hand's gestures, adaptive grasping\nability, and human-hand-guided tactile feedback grasping capability. The\nexperimental results demonstrate that the Tactile SoftHand-A can adaptively\ngrasp objects of a wide range of shapes and automatically adjust its gripping\ngestures upon detecting contact and slippage. Overall, this study points the\nway towards a class of low-cost, accessible, 3D-printable, underactuated\nhuman-like robotic hands, and we openly release the designs to facilitate\nothers to build upon this work. This work is Open-sourced at\ngithub.com/SoutheastWind/Tactile_SoftHand_A\n", "link": "http://arxiv.org/abs/2406.12731v1", "date": "2024-06-18", "relevancy": 2.0709, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5425}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5068}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tactile%20SoftHand-A%3A%203D-Printed%2C%20Tactile%2C%20Highly-underactuated%2C%0A%20%20Anthropomorphic%20Robot%20Hand%20with%20an%20Antagonistic%20Tendon%20Mechanism&body=Title%3A%20Tactile%20SoftHand-A%3A%203D-Printed%2C%20Tactile%2C%20Highly-underactuated%2C%0A%20%20Anthropomorphic%20Robot%20Hand%20with%20an%20Antagonistic%20Tendon%20Mechanism%0AAuthor%3A%20Haoran%20Li%20and%20Christopher%20J.%20Ford%20and%20Chenghua%20Lu%20and%20Yijiong%20Lin%20and%20Matteo%20Bianchi%20and%20Manuel%20G.%20Catalano%20and%20Efi%20Psomopoulou%20and%20Nathan%20F.%20Lepora%0AAbstract%3A%20%20%20For%20tendon-driven%20multi-fingered%20robotic%20hands%2C%20ensuring%20grasp%20adaptability%0Awhile%20minimizing%20the%20number%20of%20actuators%20needed%20to%20provide%20human-like%0Afunctionality%20is%20a%20challenging%20problem.%20Inspired%20by%20the%20Pisa/IIT%20SoftHand%2C%20this%0Apaper%20introduces%20a%203D-printed%2C%20highly-underactuated%2C%20five-finger%20robotic%20hand%0Anamed%20the%20Tactile%20SoftHand-A%2C%20which%20features%20only%20two%20actuators.%20The%0Adual-tendon%20design%20allows%20for%20the%20active%20control%20of%20specific%20%28distal%20or%0Aproximal%20interphalangeal%29%20joints%20to%20adjust%20the%20hand%27s%20grasp%20gesture.%20We%20have%0Aalso%20developed%20a%20new%20design%20of%20fully%203D-printed%20tactile%20sensor%20that%20requires%20no%0Ahand%20assembly%20and%20is%20printed%20directly%20as%20part%20of%20the%20robotic%20finger.%20This%0Asensor%20is%20integrated%20into%20the%20fingertips%20and%20combined%20with%20the%20antagonistic%0Atendon%20mechanism%20to%20develop%20a%20human-hand-guided%20tactile%20feedback%20grasping%0Asystem.%20The%20system%20can%20actively%20mirror%20human%20hand%20gestures%2C%20adaptively%0Astabilize%20grasp%20gestures%20upon%20contact%2C%20and%20adjust%20grasp%20gestures%20to%20prevent%0Aobject%20movement%20after%20detecting%20slippage.%20Finally%2C%20we%20designed%20four%20different%0Aexperiments%20to%20evaluate%20the%20novel%20fingers%20coupled%20with%20the%20antagonistic%0Amechanism%20for%20controlling%20the%20robotic%20hand%27s%20gestures%2C%20adaptive%20grasping%0Aability%2C%20and%20human-hand-guided%20tactile%20feedback%20grasping%20capability.%20The%0Aexperimental%20results%20demonstrate%20that%20the%20Tactile%20SoftHand-A%20can%20adaptively%0Agrasp%20objects%20of%20a%20wide%20range%20of%20shapes%20and%20automatically%20adjust%20its%20gripping%0Agestures%20upon%20detecting%20contact%20and%20slippage.%20Overall%2C%20this%20study%20points%20the%0Away%20towards%20a%20class%20of%20low-cost%2C%20accessible%2C%203D-printable%2C%20underactuated%0Ahuman-like%20robotic%20hands%2C%20and%20we%20openly%20release%20the%20designs%20to%20facilitate%0Aothers%20to%20build%20upon%20this%20work.%20This%20work%20is%20Open-sourced%20at%0Agithub.com/SoutheastWind/Tactile_SoftHand_A%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTactile%2520SoftHand-A%253A%25203D-Printed%252C%2520Tactile%252C%2520Highly-underactuated%252C%250A%2520%2520Anthropomorphic%2520Robot%2520Hand%2520with%2520an%2520Antagonistic%2520Tendon%2520Mechanism%26entry.906535625%3DHaoran%2520Li%2520and%2520Christopher%2520J.%2520Ford%2520and%2520Chenghua%2520Lu%2520and%2520Yijiong%2520Lin%2520and%2520Matteo%2520Bianchi%2520and%2520Manuel%2520G.%2520Catalano%2520and%2520Efi%2520Psomopoulou%2520and%2520Nathan%2520F.%2520Lepora%26entry.1292438233%3D%2520%2520For%2520tendon-driven%2520multi-fingered%2520robotic%2520hands%252C%2520ensuring%2520grasp%2520adaptability%250Awhile%2520minimizing%2520the%2520number%2520of%2520actuators%2520needed%2520to%2520provide%2520human-like%250Afunctionality%2520is%2520a%2520challenging%2520problem.%2520Inspired%2520by%2520the%2520Pisa/IIT%2520SoftHand%252C%2520this%250Apaper%2520introduces%2520a%25203D-printed%252C%2520highly-underactuated%252C%2520five-finger%2520robotic%2520hand%250Anamed%2520the%2520Tactile%2520SoftHand-A%252C%2520which%2520features%2520only%2520two%2520actuators.%2520The%250Adual-tendon%2520design%2520allows%2520for%2520the%2520active%2520control%2520of%2520specific%2520%2528distal%2520or%250Aproximal%2520interphalangeal%2529%2520joints%2520to%2520adjust%2520the%2520hand%2527s%2520grasp%2520gesture.%2520We%2520have%250Aalso%2520developed%2520a%2520new%2520design%2520of%2520fully%25203D-printed%2520tactile%2520sensor%2520that%2520requires%2520no%250Ahand%2520assembly%2520and%2520is%2520printed%2520directly%2520as%2520part%2520of%2520the%2520robotic%2520finger.%2520This%250Asensor%2520is%2520integrated%2520into%2520the%2520fingertips%2520and%2520combined%2520with%2520the%2520antagonistic%250Atendon%2520mechanism%2520to%2520develop%2520a%2520human-hand-guided%2520tactile%2520feedback%2520grasping%250Asystem.%2520The%2520system%2520can%2520actively%2520mirror%2520human%2520hand%2520gestures%252C%2520adaptively%250Astabilize%2520grasp%2520gestures%2520upon%2520contact%252C%2520and%2520adjust%2520grasp%2520gestures%2520to%2520prevent%250Aobject%2520movement%2520after%2520detecting%2520slippage.%2520Finally%252C%2520we%2520designed%2520four%2520different%250Aexperiments%2520to%2520evaluate%2520the%2520novel%2520fingers%2520coupled%2520with%2520the%2520antagonistic%250Amechanism%2520for%2520controlling%2520the%2520robotic%2520hand%2527s%2520gestures%252C%2520adaptive%2520grasping%250Aability%252C%2520and%2520human-hand-guided%2520tactile%2520feedback%2520grasping%2520capability.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520Tactile%2520SoftHand-A%2520can%2520adaptively%250Agrasp%2520objects%2520of%2520a%2520wide%2520range%2520of%2520shapes%2520and%2520automatically%2520adjust%2520its%2520gripping%250Agestures%2520upon%2520detecting%2520contact%2520and%2520slippage.%2520Overall%252C%2520this%2520study%2520points%2520the%250Away%2520towards%2520a%2520class%2520of%2520low-cost%252C%2520accessible%252C%25203D-printable%252C%2520underactuated%250Ahuman-like%2520robotic%2520hands%252C%2520and%2520we%2520openly%2520release%2520the%2520designs%2520to%2520facilitate%250Aothers%2520to%2520build%2520upon%2520this%2520work.%2520This%2520work%2520is%2520Open-sourced%2520at%250Agithub.com/SoutheastWind/Tactile_SoftHand_A%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tactile%20SoftHand-A%3A%203D-Printed%2C%20Tactile%2C%20Highly-underactuated%2C%0A%20%20Anthropomorphic%20Robot%20Hand%20with%20an%20Antagonistic%20Tendon%20Mechanism&entry.906535625=Haoran%20Li%20and%20Christopher%20J.%20Ford%20and%20Chenghua%20Lu%20and%20Yijiong%20Lin%20and%20Matteo%20Bianchi%20and%20Manuel%20G.%20Catalano%20and%20Efi%20Psomopoulou%20and%20Nathan%20F.%20Lepora&entry.1292438233=%20%20For%20tendon-driven%20multi-fingered%20robotic%20hands%2C%20ensuring%20grasp%20adaptability%0Awhile%20minimizing%20the%20number%20of%20actuators%20needed%20to%20provide%20human-like%0Afunctionality%20is%20a%20challenging%20problem.%20Inspired%20by%20the%20Pisa/IIT%20SoftHand%2C%20this%0Apaper%20introduces%20a%203D-printed%2C%20highly-underactuated%2C%20five-finger%20robotic%20hand%0Anamed%20the%20Tactile%20SoftHand-A%2C%20which%20features%20only%20two%20actuators.%20The%0Adual-tendon%20design%20allows%20for%20the%20active%20control%20of%20specific%20%28distal%20or%0Aproximal%20interphalangeal%29%20joints%20to%20adjust%20the%20hand%27s%20grasp%20gesture.%20We%20have%0Aalso%20developed%20a%20new%20design%20of%20fully%203D-printed%20tactile%20sensor%20that%20requires%20no%0Ahand%20assembly%20and%20is%20printed%20directly%20as%20part%20of%20the%20robotic%20finger.%20This%0Asensor%20is%20integrated%20into%20the%20fingertips%20and%20combined%20with%20the%20antagonistic%0Atendon%20mechanism%20to%20develop%20a%20human-hand-guided%20tactile%20feedback%20grasping%0Asystem.%20The%20system%20can%20actively%20mirror%20human%20hand%20gestures%2C%20adaptively%0Astabilize%20grasp%20gestures%20upon%20contact%2C%20and%20adjust%20grasp%20gestures%20to%20prevent%0Aobject%20movement%20after%20detecting%20slippage.%20Finally%2C%20we%20designed%20four%20different%0Aexperiments%20to%20evaluate%20the%20novel%20fingers%20coupled%20with%20the%20antagonistic%0Amechanism%20for%20controlling%20the%20robotic%20hand%27s%20gestures%2C%20adaptive%20grasping%0Aability%2C%20and%20human-hand-guided%20tactile%20feedback%20grasping%20capability.%20The%0Aexperimental%20results%20demonstrate%20that%20the%20Tactile%20SoftHand-A%20can%20adaptively%0Agrasp%20objects%20of%20a%20wide%20range%20of%20shapes%20and%20automatically%20adjust%20its%20gripping%0Agestures%20upon%20detecting%20contact%20and%20slippage.%20Overall%2C%20this%20study%20points%20the%0Away%20towards%20a%20class%20of%20low-cost%2C%20accessible%2C%203D-printable%2C%20underactuated%0Ahuman-like%20robotic%20hands%2C%20and%20we%20openly%20release%20the%20designs%20to%20facilitate%0Aothers%20to%20build%20upon%20this%20work.%20This%20work%20is%20Open-sourced%20at%0Agithub.com/SoutheastWind/Tactile_SoftHand_A%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12731v1&entry.124074799=Read"},
{"title": "BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity", "author": "Zahra Gharaee and Scott C. Lowe and ZeMing Gong and Pablo Millan Arias and Nicholas Pellegrino and Austin T. Wang and Joakim Bruslund Haurum and Iuliia Zarubiieva and Lila Kari and Dirk Steinke and Graham W. Taylor and Paul Fieguth and Angel X. Chang", "abstract": "  As part of an ongoing worldwide effort to comprehend and monitor insect\nbiodiversity, this paper presents the BIOSCAN-5M Insect dataset to the machine\nlearning community and establish several benchmark tasks. BIOSCAN-5M is a\ncomprehensive dataset containing multi-modal information for over 5 million\ninsect specimens, and it significantly expands existing image-based biological\ndatasets by including taxonomic labels, raw nucleotide barcode sequences,\nassigned barcode index numbers, and geographical information. We propose three\nbenchmark experiments to demonstrate the impact of the multi-modal data types\non the classification and clustering accuracy. First, we pretrain a masked\nlanguage model on the DNA barcode sequences of the \\mbox{BIOSCAN-5M} dataset,\nand demonstrate the impact of using this large reference library on species-\nand genus-level classification performance. Second, we propose a zero-shot\ntransfer learning task applied to images and DNA barcodes to cluster feature\nembeddings obtained from self-supervised learning, to investigate whether\nmeaningful clusters can be derived from these representation embeddings. Third,\nwe benchmark multi-modality by performing contrastive learning on DNA barcodes,\nimage data, and taxonomic information. This yields a general shared embedding\nspace enabling taxonomic classification using multiple types of information and\nmodalities. The code repository of the BIOSCAN-5M Insect dataset is available\nat {\\url{https://github.com/zahrag/BIOSCAN-5M}}\n", "link": "http://arxiv.org/abs/2406.12723v1", "date": "2024-06-18", "relevancy": 2.0553, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIOSCAN-5M%3A%20A%20Multimodal%20Dataset%20for%20Insect%20Biodiversity&body=Title%3A%20BIOSCAN-5M%3A%20A%20Multimodal%20Dataset%20for%20Insect%20Biodiversity%0AAuthor%3A%20Zahra%20Gharaee%20and%20Scott%20C.%20Lowe%20and%20ZeMing%20Gong%20and%20Pablo%20Millan%20Arias%20and%20Nicholas%20Pellegrino%20and%20Austin%20T.%20Wang%20and%20Joakim%20Bruslund%20Haurum%20and%20Iuliia%20Zarubiieva%20and%20Lila%20Kari%20and%20Dirk%20Steinke%20and%20Graham%20W.%20Taylor%20and%20Paul%20Fieguth%20and%20Angel%20X.%20Chang%0AAbstract%3A%20%20%20As%20part%20of%20an%20ongoing%20worldwide%20effort%20to%20comprehend%20and%20monitor%20insect%0Abiodiversity%2C%20this%20paper%20presents%20the%20BIOSCAN-5M%20Insect%20dataset%20to%20the%20machine%0Alearning%20community%20and%20establish%20several%20benchmark%20tasks.%20BIOSCAN-5M%20is%20a%0Acomprehensive%20dataset%20containing%20multi-modal%20information%20for%20over%205%20million%0Ainsect%20specimens%2C%20and%20it%20significantly%20expands%20existing%20image-based%20biological%0Adatasets%20by%20including%20taxonomic%20labels%2C%20raw%20nucleotide%20barcode%20sequences%2C%0Aassigned%20barcode%20index%20numbers%2C%20and%20geographical%20information.%20We%20propose%20three%0Abenchmark%20experiments%20to%20demonstrate%20the%20impact%20of%20the%20multi-modal%20data%20types%0Aon%20the%20classification%20and%20clustering%20accuracy.%20First%2C%20we%20pretrain%20a%20masked%0Alanguage%20model%20on%20the%20DNA%20barcode%20sequences%20of%20the%20%5Cmbox%7BBIOSCAN-5M%7D%20dataset%2C%0Aand%20demonstrate%20the%20impact%20of%20using%20this%20large%20reference%20library%20on%20species-%0Aand%20genus-level%20classification%20performance.%20Second%2C%20we%20propose%20a%20zero-shot%0Atransfer%20learning%20task%20applied%20to%20images%20and%20DNA%20barcodes%20to%20cluster%20feature%0Aembeddings%20obtained%20from%20self-supervised%20learning%2C%20to%20investigate%20whether%0Ameaningful%20clusters%20can%20be%20derived%20from%20these%20representation%20embeddings.%20Third%2C%0Awe%20benchmark%20multi-modality%20by%20performing%20contrastive%20learning%20on%20DNA%20barcodes%2C%0Aimage%20data%2C%20and%20taxonomic%20information.%20This%20yields%20a%20general%20shared%20embedding%0Aspace%20enabling%20taxonomic%20classification%20using%20multiple%20types%20of%20information%20and%0Amodalities.%20The%20code%20repository%20of%20the%20BIOSCAN-5M%20Insect%20dataset%20is%20available%0Aat%20%7B%5Curl%7Bhttps%3A//github.com/zahrag/BIOSCAN-5M%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIOSCAN-5M%253A%2520A%2520Multimodal%2520Dataset%2520for%2520Insect%2520Biodiversity%26entry.906535625%3DZahra%2520Gharaee%2520and%2520Scott%2520C.%2520Lowe%2520and%2520ZeMing%2520Gong%2520and%2520Pablo%2520Millan%2520Arias%2520and%2520Nicholas%2520Pellegrino%2520and%2520Austin%2520T.%2520Wang%2520and%2520Joakim%2520Bruslund%2520Haurum%2520and%2520Iuliia%2520Zarubiieva%2520and%2520Lila%2520Kari%2520and%2520Dirk%2520Steinke%2520and%2520Graham%2520W.%2520Taylor%2520and%2520Paul%2520Fieguth%2520and%2520Angel%2520X.%2520Chang%26entry.1292438233%3D%2520%2520As%2520part%2520of%2520an%2520ongoing%2520worldwide%2520effort%2520to%2520comprehend%2520and%2520monitor%2520insect%250Abiodiversity%252C%2520this%2520paper%2520presents%2520the%2520BIOSCAN-5M%2520Insect%2520dataset%2520to%2520the%2520machine%250Alearning%2520community%2520and%2520establish%2520several%2520benchmark%2520tasks.%2520BIOSCAN-5M%2520is%2520a%250Acomprehensive%2520dataset%2520containing%2520multi-modal%2520information%2520for%2520over%25205%2520million%250Ainsect%2520specimens%252C%2520and%2520it%2520significantly%2520expands%2520existing%2520image-based%2520biological%250Adatasets%2520by%2520including%2520taxonomic%2520labels%252C%2520raw%2520nucleotide%2520barcode%2520sequences%252C%250Aassigned%2520barcode%2520index%2520numbers%252C%2520and%2520geographical%2520information.%2520We%2520propose%2520three%250Abenchmark%2520experiments%2520to%2520demonstrate%2520the%2520impact%2520of%2520the%2520multi-modal%2520data%2520types%250Aon%2520the%2520classification%2520and%2520clustering%2520accuracy.%2520First%252C%2520we%2520pretrain%2520a%2520masked%250Alanguage%2520model%2520on%2520the%2520DNA%2520barcode%2520sequences%2520of%2520the%2520%255Cmbox%257BBIOSCAN-5M%257D%2520dataset%252C%250Aand%2520demonstrate%2520the%2520impact%2520of%2520using%2520this%2520large%2520reference%2520library%2520on%2520species-%250Aand%2520genus-level%2520classification%2520performance.%2520Second%252C%2520we%2520propose%2520a%2520zero-shot%250Atransfer%2520learning%2520task%2520applied%2520to%2520images%2520and%2520DNA%2520barcodes%2520to%2520cluster%2520feature%250Aembeddings%2520obtained%2520from%2520self-supervised%2520learning%252C%2520to%2520investigate%2520whether%250Ameaningful%2520clusters%2520can%2520be%2520derived%2520from%2520these%2520representation%2520embeddings.%2520Third%252C%250Awe%2520benchmark%2520multi-modality%2520by%2520performing%2520contrastive%2520learning%2520on%2520DNA%2520barcodes%252C%250Aimage%2520data%252C%2520and%2520taxonomic%2520information.%2520This%2520yields%2520a%2520general%2520shared%2520embedding%250Aspace%2520enabling%2520taxonomic%2520classification%2520using%2520multiple%2520types%2520of%2520information%2520and%250Amodalities.%2520The%2520code%2520repository%2520of%2520the%2520BIOSCAN-5M%2520Insect%2520dataset%2520is%2520available%250Aat%2520%257B%255Curl%257Bhttps%253A//github.com/zahrag/BIOSCAN-5M%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIOSCAN-5M%3A%20A%20Multimodal%20Dataset%20for%20Insect%20Biodiversity&entry.906535625=Zahra%20Gharaee%20and%20Scott%20C.%20Lowe%20and%20ZeMing%20Gong%20and%20Pablo%20Millan%20Arias%20and%20Nicholas%20Pellegrino%20and%20Austin%20T.%20Wang%20and%20Joakim%20Bruslund%20Haurum%20and%20Iuliia%20Zarubiieva%20and%20Lila%20Kari%20and%20Dirk%20Steinke%20and%20Graham%20W.%20Taylor%20and%20Paul%20Fieguth%20and%20Angel%20X.%20Chang&entry.1292438233=%20%20As%20part%20of%20an%20ongoing%20worldwide%20effort%20to%20comprehend%20and%20monitor%20insect%0Abiodiversity%2C%20this%20paper%20presents%20the%20BIOSCAN-5M%20Insect%20dataset%20to%20the%20machine%0Alearning%20community%20and%20establish%20several%20benchmark%20tasks.%20BIOSCAN-5M%20is%20a%0Acomprehensive%20dataset%20containing%20multi-modal%20information%20for%20over%205%20million%0Ainsect%20specimens%2C%20and%20it%20significantly%20expands%20existing%20image-based%20biological%0Adatasets%20by%20including%20taxonomic%20labels%2C%20raw%20nucleotide%20barcode%20sequences%2C%0Aassigned%20barcode%20index%20numbers%2C%20and%20geographical%20information.%20We%20propose%20three%0Abenchmark%20experiments%20to%20demonstrate%20the%20impact%20of%20the%20multi-modal%20data%20types%0Aon%20the%20classification%20and%20clustering%20accuracy.%20First%2C%20we%20pretrain%20a%20masked%0Alanguage%20model%20on%20the%20DNA%20barcode%20sequences%20of%20the%20%5Cmbox%7BBIOSCAN-5M%7D%20dataset%2C%0Aand%20demonstrate%20the%20impact%20of%20using%20this%20large%20reference%20library%20on%20species-%0Aand%20genus-level%20classification%20performance.%20Second%2C%20we%20propose%20a%20zero-shot%0Atransfer%20learning%20task%20applied%20to%20images%20and%20DNA%20barcodes%20to%20cluster%20feature%0Aembeddings%20obtained%20from%20self-supervised%20learning%2C%20to%20investigate%20whether%0Ameaningful%20clusters%20can%20be%20derived%20from%20these%20representation%20embeddings.%20Third%2C%0Awe%20benchmark%20multi-modality%20by%20performing%20contrastive%20learning%20on%20DNA%20barcodes%2C%0Aimage%20data%2C%20and%20taxonomic%20information.%20This%20yields%20a%20general%20shared%20embedding%0Aspace%20enabling%20taxonomic%20classification%20using%20multiple%20types%20of%20information%20and%0Amodalities.%20The%20code%20repository%20of%20the%20BIOSCAN-5M%20Insect%20dataset%20is%20available%0Aat%20%7B%5Curl%7Bhttps%3A//github.com/zahrag/BIOSCAN-5M%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12723v1&entry.124074799=Read"},
{"title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language\n  Models", "author": "Yibin Wang and Haizhou Shi and Ligong Han and Dimitris Metaxas and Hao Wang", "abstract": "  Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data.\n", "link": "http://arxiv.org/abs/2406.11675v2", "date": "2024-06-18", "relevancy": 2.0544, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yibin%20Wang%20and%20Haizhou%20Shi%20and%20Ligong%20Han%20and%20Dimitris%20Metaxas%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20suffer%20from%20overconfidence%20during%0Ainference%2C%20particularly%20when%20adapted%20to%20downstream%20domain-specific%20tasks%20with%0Alimited%20data.%20Previous%20work%20addresses%20this%20issue%20by%20employing%20approximate%0ABayesian%20estimation%20after%20the%20LLMs%20are%20trained%2C%20enabling%20them%20to%20quantify%0Auncertainty.%20However%2C%20such%20post-training%20approaches%27%20performance%20is%20severely%0Alimited%20by%20the%20parameters%20learned%20during%20training.%20In%20this%20paper%2C%20we%20go%20beyond%0Apost-training%20Bayesianization%20and%20propose%20Bayesian%20Low-Rank%20Adaptation%20by%0ABackpropagation%20%28BLoB%29%2C%20an%20algorithm%20that%20continuously%20and%20jointly%20adjusts%20both%0Athe%20mean%20and%20covariance%20of%20LLM%20parameters%20throughout%20the%20whole%20fine-tuning%0Aprocess.%20Our%20empirical%20results%20verify%20the%20effectiveness%20of%20BLoB%20in%20terms%20of%0Ageneralization%20and%20uncertainty%20estimation%2C%20when%20evaluated%20on%20both%0Ain-distribution%20and%20out-of-distribution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11675v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLoB%253A%2520Bayesian%2520Low-Rank%2520Adaptation%2520by%2520Backpropagation%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYibin%2520Wang%2520and%2520Haizhou%2520Shi%2520and%2520Ligong%2520Han%2520and%2520Dimitris%2520Metaxas%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520suffer%2520from%2520overconfidence%2520during%250Ainference%252C%2520particularly%2520when%2520adapted%2520to%2520downstream%2520domain-specific%2520tasks%2520with%250Alimited%2520data.%2520Previous%2520work%2520addresses%2520this%2520issue%2520by%2520employing%2520approximate%250ABayesian%2520estimation%2520after%2520the%2520LLMs%2520are%2520trained%252C%2520enabling%2520them%2520to%2520quantify%250Auncertainty.%2520However%252C%2520such%2520post-training%2520approaches%2527%2520performance%2520is%2520severely%250Alimited%2520by%2520the%2520parameters%2520learned%2520during%2520training.%2520In%2520this%2520paper%252C%2520we%2520go%2520beyond%250Apost-training%2520Bayesianization%2520and%2520propose%2520Bayesian%2520Low-Rank%2520Adaptation%2520by%250ABackpropagation%2520%2528BLoB%2529%252C%2520an%2520algorithm%2520that%2520continuously%2520and%2520jointly%2520adjusts%2520both%250Athe%2520mean%2520and%2520covariance%2520of%2520LLM%2520parameters%2520throughout%2520the%2520whole%2520fine-tuning%250Aprocess.%2520Our%2520empirical%2520results%2520verify%2520the%2520effectiveness%2520of%2520BLoB%2520in%2520terms%2520of%250Ageneralization%2520and%2520uncertainty%2520estimation%252C%2520when%2520evaluated%2520on%2520both%250Ain-distribution%2520and%2520out-of-distribution%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11675v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models&entry.906535625=Yibin%20Wang%20and%20Haizhou%20Shi%20and%20Ligong%20Han%20and%20Dimitris%20Metaxas%20and%20Hao%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20suffer%20from%20overconfidence%20during%0Ainference%2C%20particularly%20when%20adapted%20to%20downstream%20domain-specific%20tasks%20with%0Alimited%20data.%20Previous%20work%20addresses%20this%20issue%20by%20employing%20approximate%0ABayesian%20estimation%20after%20the%20LLMs%20are%20trained%2C%20enabling%20them%20to%20quantify%0Auncertainty.%20However%2C%20such%20post-training%20approaches%27%20performance%20is%20severely%0Alimited%20by%20the%20parameters%20learned%20during%20training.%20In%20this%20paper%2C%20we%20go%20beyond%0Apost-training%20Bayesianization%20and%20propose%20Bayesian%20Low-Rank%20Adaptation%20by%0ABackpropagation%20%28BLoB%29%2C%20an%20algorithm%20that%20continuously%20and%20jointly%20adjusts%20both%0Athe%20mean%20and%20covariance%20of%20LLM%20parameters%20throughout%20the%20whole%20fine-tuning%0Aprocess.%20Our%20empirical%20results%20verify%20the%20effectiveness%20of%20BLoB%20in%20terms%20of%0Ageneralization%20and%20uncertainty%20estimation%2C%20when%20evaluated%20on%20both%0Ain-distribution%20and%20out-of-distribution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11675v2&entry.124074799=Read"},
{"title": "Contraction rates for conjugate gradient and Lanczos approximate\n  posteriors in Gaussian process regression", "author": "Bernhard Stankewitz and Botond Szabo", "abstract": "  Due to their flexibility and theoretical tractability Gaussian process (GP)\nregression models have become a central topic in modern statistics and machine\nlearning. While the true posterior in these models is given explicitly,\nnumerical evaluations depend on the inversion of the augmented kernel matrix $\nK + \\sigma^2 I $, which requires up to $ O(n^3) $ operations. For large sample\nsizes n, which are typically given in modern applications, this is\ncomputationally infeasible and necessitates the use of an approximate version\nof the posterior. Although such methods are widely used in practice, they\ntypically have very limtied theoretical underpinning.\n  In this context, we analyze a class of recently proposed approximation\nalgorithms from the field of Probabilistic numerics. They can be interpreted in\nterms of Lanczos approximate eigenvectors of the kernel matrix or a conjugate\ngradient approximation of the posterior mean, which are particularly\nadvantageous in truly large scale applications, as they are fundamentally only\nbased on matrix vector multiplications amenable to the GPU acceleration of\nmodern software frameworks. We combine result from the numerical analysis\nliterature with state of the art concentration results for spectra of kernel\nmatrices to obtain minimax contraction rates. Our theoretical findings are\nillustrated by numerical experiments.\n", "link": "http://arxiv.org/abs/2406.12678v1", "date": "2024-06-18", "relevancy": 2.0465, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5274}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5149}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contraction%20rates%20for%20conjugate%20gradient%20and%20Lanczos%20approximate%0A%20%20posteriors%20in%20Gaussian%20process%20regression&body=Title%3A%20Contraction%20rates%20for%20conjugate%20gradient%20and%20Lanczos%20approximate%0A%20%20posteriors%20in%20Gaussian%20process%20regression%0AAuthor%3A%20Bernhard%20Stankewitz%20and%20Botond%20Szabo%0AAbstract%3A%20%20%20Due%20to%20their%20flexibility%20and%20theoretical%20tractability%20Gaussian%20process%20%28GP%29%0Aregression%20models%20have%20become%20a%20central%20topic%20in%20modern%20statistics%20and%20machine%0Alearning.%20While%20the%20true%20posterior%20in%20these%20models%20is%20given%20explicitly%2C%0Anumerical%20evaluations%20depend%20on%20the%20inversion%20of%20the%20augmented%20kernel%20matrix%20%24%0AK%20%2B%20%5Csigma%5E2%20I%20%24%2C%20which%20requires%20up%20to%20%24%20O%28n%5E3%29%20%24%20operations.%20For%20large%20sample%0Asizes%20n%2C%20which%20are%20typically%20given%20in%20modern%20applications%2C%20this%20is%0Acomputationally%20infeasible%20and%20necessitates%20the%20use%20of%20an%20approximate%20version%0Aof%20the%20posterior.%20Although%20such%20methods%20are%20widely%20used%20in%20practice%2C%20they%0Atypically%20have%20very%20limtied%20theoretical%20underpinning.%0A%20%20In%20this%20context%2C%20we%20analyze%20a%20class%20of%20recently%20proposed%20approximation%0Aalgorithms%20from%20the%20field%20of%20Probabilistic%20numerics.%20They%20can%20be%20interpreted%20in%0Aterms%20of%20Lanczos%20approximate%20eigenvectors%20of%20the%20kernel%20matrix%20or%20a%20conjugate%0Agradient%20approximation%20of%20the%20posterior%20mean%2C%20which%20are%20particularly%0Aadvantageous%20in%20truly%20large%20scale%20applications%2C%20as%20they%20are%20fundamentally%20only%0Abased%20on%20matrix%20vector%20multiplications%20amenable%20to%20the%20GPU%20acceleration%20of%0Amodern%20software%20frameworks.%20We%20combine%20result%20from%20the%20numerical%20analysis%0Aliterature%20with%20state%20of%20the%20art%20concentration%20results%20for%20spectra%20of%20kernel%0Amatrices%20to%20obtain%20minimax%20contraction%20rates.%20Our%20theoretical%20findings%20are%0Aillustrated%20by%20numerical%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContraction%2520rates%2520for%2520conjugate%2520gradient%2520and%2520Lanczos%2520approximate%250A%2520%2520posteriors%2520in%2520Gaussian%2520process%2520regression%26entry.906535625%3DBernhard%2520Stankewitz%2520and%2520Botond%2520Szabo%26entry.1292438233%3D%2520%2520Due%2520to%2520their%2520flexibility%2520and%2520theoretical%2520tractability%2520Gaussian%2520process%2520%2528GP%2529%250Aregression%2520models%2520have%2520become%2520a%2520central%2520topic%2520in%2520modern%2520statistics%2520and%2520machine%250Alearning.%2520While%2520the%2520true%2520posterior%2520in%2520these%2520models%2520is%2520given%2520explicitly%252C%250Anumerical%2520evaluations%2520depend%2520on%2520the%2520inversion%2520of%2520the%2520augmented%2520kernel%2520matrix%2520%2524%250AK%2520%252B%2520%255Csigma%255E2%2520I%2520%2524%252C%2520which%2520requires%2520up%2520to%2520%2524%2520O%2528n%255E3%2529%2520%2524%2520operations.%2520For%2520large%2520sample%250Asizes%2520n%252C%2520which%2520are%2520typically%2520given%2520in%2520modern%2520applications%252C%2520this%2520is%250Acomputationally%2520infeasible%2520and%2520necessitates%2520the%2520use%2520of%2520an%2520approximate%2520version%250Aof%2520the%2520posterior.%2520Although%2520such%2520methods%2520are%2520widely%2520used%2520in%2520practice%252C%2520they%250Atypically%2520have%2520very%2520limtied%2520theoretical%2520underpinning.%250A%2520%2520In%2520this%2520context%252C%2520we%2520analyze%2520a%2520class%2520of%2520recently%2520proposed%2520approximation%250Aalgorithms%2520from%2520the%2520field%2520of%2520Probabilistic%2520numerics.%2520They%2520can%2520be%2520interpreted%2520in%250Aterms%2520of%2520Lanczos%2520approximate%2520eigenvectors%2520of%2520the%2520kernel%2520matrix%2520or%2520a%2520conjugate%250Agradient%2520approximation%2520of%2520the%2520posterior%2520mean%252C%2520which%2520are%2520particularly%250Aadvantageous%2520in%2520truly%2520large%2520scale%2520applications%252C%2520as%2520they%2520are%2520fundamentally%2520only%250Abased%2520on%2520matrix%2520vector%2520multiplications%2520amenable%2520to%2520the%2520GPU%2520acceleration%2520of%250Amodern%2520software%2520frameworks.%2520We%2520combine%2520result%2520from%2520the%2520numerical%2520analysis%250Aliterature%2520with%2520state%2520of%2520the%2520art%2520concentration%2520results%2520for%2520spectra%2520of%2520kernel%250Amatrices%2520to%2520obtain%2520minimax%2520contraction%2520rates.%2520Our%2520theoretical%2520findings%2520are%250Aillustrated%2520by%2520numerical%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contraction%20rates%20for%20conjugate%20gradient%20and%20Lanczos%20approximate%0A%20%20posteriors%20in%20Gaussian%20process%20regression&entry.906535625=Bernhard%20Stankewitz%20and%20Botond%20Szabo&entry.1292438233=%20%20Due%20to%20their%20flexibility%20and%20theoretical%20tractability%20Gaussian%20process%20%28GP%29%0Aregression%20models%20have%20become%20a%20central%20topic%20in%20modern%20statistics%20and%20machine%0Alearning.%20While%20the%20true%20posterior%20in%20these%20models%20is%20given%20explicitly%2C%0Anumerical%20evaluations%20depend%20on%20the%20inversion%20of%20the%20augmented%20kernel%20matrix%20%24%0AK%20%2B%20%5Csigma%5E2%20I%20%24%2C%20which%20requires%20up%20to%20%24%20O%28n%5E3%29%20%24%20operations.%20For%20large%20sample%0Asizes%20n%2C%20which%20are%20typically%20given%20in%20modern%20applications%2C%20this%20is%0Acomputationally%20infeasible%20and%20necessitates%20the%20use%20of%20an%20approximate%20version%0Aof%20the%20posterior.%20Although%20such%20methods%20are%20widely%20used%20in%20practice%2C%20they%0Atypically%20have%20very%20limtied%20theoretical%20underpinning.%0A%20%20In%20this%20context%2C%20we%20analyze%20a%20class%20of%20recently%20proposed%20approximation%0Aalgorithms%20from%20the%20field%20of%20Probabilistic%20numerics.%20They%20can%20be%20interpreted%20in%0Aterms%20of%20Lanczos%20approximate%20eigenvectors%20of%20the%20kernel%20matrix%20or%20a%20conjugate%0Agradient%20approximation%20of%20the%20posterior%20mean%2C%20which%20are%20particularly%0Aadvantageous%20in%20truly%20large%20scale%20applications%2C%20as%20they%20are%20fundamentally%20only%0Abased%20on%20matrix%20vector%20multiplications%20amenable%20to%20the%20GPU%20acceleration%20of%0Amodern%20software%20frameworks.%20We%20combine%20result%20from%20the%20numerical%20analysis%0Aliterature%20with%20state%20of%20the%20art%20concentration%20results%20for%20spectra%20of%20kernel%0Amatrices%20to%20obtain%20minimax%20contraction%20rates.%20Our%20theoretical%20findings%20are%0Aillustrated%20by%20numerical%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12678v1&entry.124074799=Read"},
{"title": "Research and Implementation of Data Enhancement Techniques for Graph\n  Neural Networks", "author": "Jingzhao Gu and Haoyang Huang", "abstract": "  Data, algorithms, and arithmetic power are the three foundational conditions\nfor deep learning to be effective in the application domain. Data is the focus\nfor developing deep learning algorithms. In practical engineering applications,\nsome data are affected by the conditions under which more data cannot be\nobtained or the cost of obtaining data is too high, resulting in smaller data\nsets (generally several hundred to several thousand) and data sizes that are\nfar smaller than the size of large data sets (tens of thousands). The above two\nmethods are based on the original dataset to generate, in the case of\ninsufficient data volume of the original data may not reflect all the real\nenvironment, such as the real environment of the light, silhouette and other\ninformation, if the amount of data is not enough, it is difficult to use a\nsimple transformation or neural network generative model to generate the\nrequired data. The research in this paper firstly analyses the key points of\nthe data enhancement technology of graph neural network, and at the same time\nintroduces the composition foundation of graph neural network in depth, on the\nbasis of which the data enhancement technology of graph neural network is\noptimized and analysed.\n", "link": "http://arxiv.org/abs/2406.12640v1", "date": "2024-06-18", "relevancy": 2.0418, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5148}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5086}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20and%20Implementation%20of%20Data%20Enhancement%20Techniques%20for%20Graph%0A%20%20Neural%20Networks&body=Title%3A%20Research%20and%20Implementation%20of%20Data%20Enhancement%20Techniques%20for%20Graph%0A%20%20Neural%20Networks%0AAuthor%3A%20Jingzhao%20Gu%20and%20Haoyang%20Huang%0AAbstract%3A%20%20%20Data%2C%20algorithms%2C%20and%20arithmetic%20power%20are%20the%20three%20foundational%20conditions%0Afor%20deep%20learning%20to%20be%20effective%20in%20the%20application%20domain.%20Data%20is%20the%20focus%0Afor%20developing%20deep%20learning%20algorithms.%20In%20practical%20engineering%20applications%2C%0Asome%20data%20are%20affected%20by%20the%20conditions%20under%20which%20more%20data%20cannot%20be%0Aobtained%20or%20the%20cost%20of%20obtaining%20data%20is%20too%20high%2C%20resulting%20in%20smaller%20data%0Asets%20%28generally%20several%20hundred%20to%20several%20thousand%29%20and%20data%20sizes%20that%20are%0Afar%20smaller%20than%20the%20size%20of%20large%20data%20sets%20%28tens%20of%20thousands%29.%20The%20above%20two%0Amethods%20are%20based%20on%20the%20original%20dataset%20to%20generate%2C%20in%20the%20case%20of%0Ainsufficient%20data%20volume%20of%20the%20original%20data%20may%20not%20reflect%20all%20the%20real%0Aenvironment%2C%20such%20as%20the%20real%20environment%20of%20the%20light%2C%20silhouette%20and%20other%0Ainformation%2C%20if%20the%20amount%20of%20data%20is%20not%20enough%2C%20it%20is%20difficult%20to%20use%20a%0Asimple%20transformation%20or%20neural%20network%20generative%20model%20to%20generate%20the%0Arequired%20data.%20The%20research%20in%20this%20paper%20firstly%20analyses%20the%20key%20points%20of%0Athe%20data%20enhancement%20technology%20of%20graph%20neural%20network%2C%20and%20at%20the%20same%20time%0Aintroduces%20the%20composition%20foundation%20of%20graph%20neural%20network%20in%20depth%2C%20on%20the%0Abasis%20of%20which%20the%20data%20enhancement%20technology%20of%20graph%20neural%20network%20is%0Aoptimized%20and%20analysed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520and%2520Implementation%2520of%2520Data%2520Enhancement%2520Techniques%2520for%2520Graph%250A%2520%2520Neural%2520Networks%26entry.906535625%3DJingzhao%2520Gu%2520and%2520Haoyang%2520Huang%26entry.1292438233%3D%2520%2520Data%252C%2520algorithms%252C%2520and%2520arithmetic%2520power%2520are%2520the%2520three%2520foundational%2520conditions%250Afor%2520deep%2520learning%2520to%2520be%2520effective%2520in%2520the%2520application%2520domain.%2520Data%2520is%2520the%2520focus%250Afor%2520developing%2520deep%2520learning%2520algorithms.%2520In%2520practical%2520engineering%2520applications%252C%250Asome%2520data%2520are%2520affected%2520by%2520the%2520conditions%2520under%2520which%2520more%2520data%2520cannot%2520be%250Aobtained%2520or%2520the%2520cost%2520of%2520obtaining%2520data%2520is%2520too%2520high%252C%2520resulting%2520in%2520smaller%2520data%250Asets%2520%2528generally%2520several%2520hundred%2520to%2520several%2520thousand%2529%2520and%2520data%2520sizes%2520that%2520are%250Afar%2520smaller%2520than%2520the%2520size%2520of%2520large%2520data%2520sets%2520%2528tens%2520of%2520thousands%2529.%2520The%2520above%2520two%250Amethods%2520are%2520based%2520on%2520the%2520original%2520dataset%2520to%2520generate%252C%2520in%2520the%2520case%2520of%250Ainsufficient%2520data%2520volume%2520of%2520the%2520original%2520data%2520may%2520not%2520reflect%2520all%2520the%2520real%250Aenvironment%252C%2520such%2520as%2520the%2520real%2520environment%2520of%2520the%2520light%252C%2520silhouette%2520and%2520other%250Ainformation%252C%2520if%2520the%2520amount%2520of%2520data%2520is%2520not%2520enough%252C%2520it%2520is%2520difficult%2520to%2520use%2520a%250Asimple%2520transformation%2520or%2520neural%2520network%2520generative%2520model%2520to%2520generate%2520the%250Arequired%2520data.%2520The%2520research%2520in%2520this%2520paper%2520firstly%2520analyses%2520the%2520key%2520points%2520of%250Athe%2520data%2520enhancement%2520technology%2520of%2520graph%2520neural%2520network%252C%2520and%2520at%2520the%2520same%2520time%250Aintroduces%2520the%2520composition%2520foundation%2520of%2520graph%2520neural%2520network%2520in%2520depth%252C%2520on%2520the%250Abasis%2520of%2520which%2520the%2520data%2520enhancement%2520technology%2520of%2520graph%2520neural%2520network%2520is%250Aoptimized%2520and%2520analysed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20and%20Implementation%20of%20Data%20Enhancement%20Techniques%20for%20Graph%0A%20%20Neural%20Networks&entry.906535625=Jingzhao%20Gu%20and%20Haoyang%20Huang&entry.1292438233=%20%20Data%2C%20algorithms%2C%20and%20arithmetic%20power%20are%20the%20three%20foundational%20conditions%0Afor%20deep%20learning%20to%20be%20effective%20in%20the%20application%20domain.%20Data%20is%20the%20focus%0Afor%20developing%20deep%20learning%20algorithms.%20In%20practical%20engineering%20applications%2C%0Asome%20data%20are%20affected%20by%20the%20conditions%20under%20which%20more%20data%20cannot%20be%0Aobtained%20or%20the%20cost%20of%20obtaining%20data%20is%20too%20high%2C%20resulting%20in%20smaller%20data%0Asets%20%28generally%20several%20hundred%20to%20several%20thousand%29%20and%20data%20sizes%20that%20are%0Afar%20smaller%20than%20the%20size%20of%20large%20data%20sets%20%28tens%20of%20thousands%29.%20The%20above%20two%0Amethods%20are%20based%20on%20the%20original%20dataset%20to%20generate%2C%20in%20the%20case%20of%0Ainsufficient%20data%20volume%20of%20the%20original%20data%20may%20not%20reflect%20all%20the%20real%0Aenvironment%2C%20such%20as%20the%20real%20environment%20of%20the%20light%2C%20silhouette%20and%20other%0Ainformation%2C%20if%20the%20amount%20of%20data%20is%20not%20enough%2C%20it%20is%20difficult%20to%20use%20a%0Asimple%20transformation%20or%20neural%20network%20generative%20model%20to%20generate%20the%0Arequired%20data.%20The%20research%20in%20this%20paper%20firstly%20analyses%20the%20key%20points%20of%0Athe%20data%20enhancement%20technology%20of%20graph%20neural%20network%2C%20and%20at%20the%20same%20time%0Aintroduces%20the%20composition%20foundation%20of%20graph%20neural%20network%20in%20depth%2C%20on%20the%0Abasis%20of%20which%20the%20data%20enhancement%20technology%20of%20graph%20neural%20network%20is%0Aoptimized%20and%20analysed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12640v1&entry.124074799=Read"},
{"title": "Federated Learning with a Single Shared Image", "author": "Sunny Soni and Aaqib Saeed and Yuki M. Asano", "abstract": "  Federated Learning (FL) enables multiple machines to collaboratively train a\nmachine learning model without sharing of private training data. Yet,\nespecially for heterogeneous models, a key bottleneck remains the transfer of\nknowledge gained from each client model with the server. One popular method,\nFedDF, uses distillation to tackle this task with the use of a common, shared\ndataset on which predictions are exchanged. However, in many contexts such a\ndataset might be difficult to acquire due to privacy and the clients might not\nallow for storage of a large shared dataset. To this end, in this paper, we\nintroduce a new method that improves this knowledge distillation method to only\nrely on a single shared image between clients and server. In particular, we\npropose a novel adaptive dataset pruning algorithm that selects the most\ninformative crops generated from only a single image. With this, we show that\nfederated learning with distillation under a limited shared dataset budget\nworks better by using a single image compared to multiple individual ones.\nFinally, we extend our approach to allow for training heterogeneous client\narchitectures by incorporating a non-uniform distillation schedule and\nclient-model mirroring on the server side.\n", "link": "http://arxiv.org/abs/2406.12658v1", "date": "2024-06-18", "relevancy": 2.0346, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20with%20a%20Single%20Shared%20Image&body=Title%3A%20Federated%20Learning%20with%20a%20Single%20Shared%20Image%0AAuthor%3A%20Sunny%20Soni%20and%20Aaqib%20Saeed%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20multiple%20machines%20to%20collaboratively%20train%20a%0Amachine%20learning%20model%20without%20sharing%20of%20private%20training%20data.%20Yet%2C%0Aespecially%20for%20heterogeneous%20models%2C%20a%20key%20bottleneck%20remains%20the%20transfer%20of%0Aknowledge%20gained%20from%20each%20client%20model%20with%20the%20server.%20One%20popular%20method%2C%0AFedDF%2C%20uses%20distillation%20to%20tackle%20this%20task%20with%20the%20use%20of%20a%20common%2C%20shared%0Adataset%20on%20which%20predictions%20are%20exchanged.%20However%2C%20in%20many%20contexts%20such%20a%0Adataset%20might%20be%20difficult%20to%20acquire%20due%20to%20privacy%20and%20the%20clients%20might%20not%0Aallow%20for%20storage%20of%20a%20large%20shared%20dataset.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%0Aintroduce%20a%20new%20method%20that%20improves%20this%20knowledge%20distillation%20method%20to%20only%0Arely%20on%20a%20single%20shared%20image%20between%20clients%20and%20server.%20In%20particular%2C%20we%0Apropose%20a%20novel%20adaptive%20dataset%20pruning%20algorithm%20that%20selects%20the%20most%0Ainformative%20crops%20generated%20from%20only%20a%20single%20image.%20With%20this%2C%20we%20show%20that%0Afederated%20learning%20with%20distillation%20under%20a%20limited%20shared%20dataset%20budget%0Aworks%20better%20by%20using%20a%20single%20image%20compared%20to%20multiple%20individual%20ones.%0AFinally%2C%20we%20extend%20our%20approach%20to%20allow%20for%20training%20heterogeneous%20client%0Aarchitectures%20by%20incorporating%20a%20non-uniform%20distillation%20schedule%20and%0Aclient-model%20mirroring%20on%20the%20server%20side.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520with%2520a%2520Single%2520Shared%2520Image%26entry.906535625%3DSunny%2520Soni%2520and%2520Aaqib%2520Saeed%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520multiple%2520machines%2520to%2520collaboratively%2520train%2520a%250Amachine%2520learning%2520model%2520without%2520sharing%2520of%2520private%2520training%2520data.%2520Yet%252C%250Aespecially%2520for%2520heterogeneous%2520models%252C%2520a%2520key%2520bottleneck%2520remains%2520the%2520transfer%2520of%250Aknowledge%2520gained%2520from%2520each%2520client%2520model%2520with%2520the%2520server.%2520One%2520popular%2520method%252C%250AFedDF%252C%2520uses%2520distillation%2520to%2520tackle%2520this%2520task%2520with%2520the%2520use%2520of%2520a%2520common%252C%2520shared%250Adataset%2520on%2520which%2520predictions%2520are%2520exchanged.%2520However%252C%2520in%2520many%2520contexts%2520such%2520a%250Adataset%2520might%2520be%2520difficult%2520to%2520acquire%2520due%2520to%2520privacy%2520and%2520the%2520clients%2520might%2520not%250Aallow%2520for%2520storage%2520of%2520a%2520large%2520shared%2520dataset.%2520To%2520this%2520end%252C%2520in%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520new%2520method%2520that%2520improves%2520this%2520knowledge%2520distillation%2520method%2520to%2520only%250Arely%2520on%2520a%2520single%2520shared%2520image%2520between%2520clients%2520and%2520server.%2520In%2520particular%252C%2520we%250Apropose%2520a%2520novel%2520adaptive%2520dataset%2520pruning%2520algorithm%2520that%2520selects%2520the%2520most%250Ainformative%2520crops%2520generated%2520from%2520only%2520a%2520single%2520image.%2520With%2520this%252C%2520we%2520show%2520that%250Afederated%2520learning%2520with%2520distillation%2520under%2520a%2520limited%2520shared%2520dataset%2520budget%250Aworks%2520better%2520by%2520using%2520a%2520single%2520image%2520compared%2520to%2520multiple%2520individual%2520ones.%250AFinally%252C%2520we%2520extend%2520our%2520approach%2520to%2520allow%2520for%2520training%2520heterogeneous%2520client%250Aarchitectures%2520by%2520incorporating%2520a%2520non-uniform%2520distillation%2520schedule%2520and%250Aclient-model%2520mirroring%2520on%2520the%2520server%2520side.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20with%20a%20Single%20Shared%20Image&entry.906535625=Sunny%20Soni%20and%20Aaqib%20Saeed%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20multiple%20machines%20to%20collaboratively%20train%20a%0Amachine%20learning%20model%20without%20sharing%20of%20private%20training%20data.%20Yet%2C%0Aespecially%20for%20heterogeneous%20models%2C%20a%20key%20bottleneck%20remains%20the%20transfer%20of%0Aknowledge%20gained%20from%20each%20client%20model%20with%20the%20server.%20One%20popular%20method%2C%0AFedDF%2C%20uses%20distillation%20to%20tackle%20this%20task%20with%20the%20use%20of%20a%20common%2C%20shared%0Adataset%20on%20which%20predictions%20are%20exchanged.%20However%2C%20in%20many%20contexts%20such%20a%0Adataset%20might%20be%20difficult%20to%20acquire%20due%20to%20privacy%20and%20the%20clients%20might%20not%0Aallow%20for%20storage%20of%20a%20large%20shared%20dataset.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%0Aintroduce%20a%20new%20method%20that%20improves%20this%20knowledge%20distillation%20method%20to%20only%0Arely%20on%20a%20single%20shared%20image%20between%20clients%20and%20server.%20In%20particular%2C%20we%0Apropose%20a%20novel%20adaptive%20dataset%20pruning%20algorithm%20that%20selects%20the%20most%0Ainformative%20crops%20generated%20from%20only%20a%20single%20image.%20With%20this%2C%20we%20show%20that%0Afederated%20learning%20with%20distillation%20under%20a%20limited%20shared%20dataset%20budget%0Aworks%20better%20by%20using%20a%20single%20image%20compared%20to%20multiple%20individual%20ones.%0AFinally%2C%20we%20extend%20our%20approach%20to%20allow%20for%20training%20heterogeneous%20client%0Aarchitectures%20by%20incorporating%20a%20non-uniform%20distillation%20schedule%20and%0Aclient-model%20mirroring%20on%20the%20server%20side.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12658v1&entry.124074799=Read"},
{"title": "Benchmarking Label Noise in Instance Segmentation: Spatial Noise Matters", "author": "Eden Grad and Moshe Kimhi and Lion Halika and Chaim Baskin", "abstract": "  Obtaining accurate labels for instance segmentation is particularly\nchallenging due to the complex nature of the task. Each image necessitates\nmultiple annotations, encompassing not only the object's class but also its\nprecise spatial boundaries. These requirements elevate the likelihood of errors\nand inconsistencies in both manual and automated annotation processes. By\nsimulating different noise conditions, we provide a realistic scenario for\nassessing the robustness and generalization capabilities of instance\nsegmentation models in different segmentation tasks, introducing COCO-N and\nCityscapes-N. We also propose a benchmark for weakly annotation noise, dubbed\nCOCO-WAN, which utilizes foundation models and weak annotations to simulate\nsemi-automated annotation tools and their noisy labels. This study sheds light\non the quality of segmentation masks produced by various models and challenges\nthe efficacy of popular methods designed to address learning with label noise.\n", "link": "http://arxiv.org/abs/2406.10891v2", "date": "2024-06-18", "relevancy": 2.0313, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5167}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.515}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Label%20Noise%20in%20Instance%20Segmentation%3A%20Spatial%20Noise%20Matters&body=Title%3A%20Benchmarking%20Label%20Noise%20in%20Instance%20Segmentation%3A%20Spatial%20Noise%20Matters%0AAuthor%3A%20Eden%20Grad%20and%20Moshe%20Kimhi%20and%20Lion%20Halika%20and%20Chaim%20Baskin%0AAbstract%3A%20%20%20Obtaining%20accurate%20labels%20for%20instance%20segmentation%20is%20particularly%0Achallenging%20due%20to%20the%20complex%20nature%20of%20the%20task.%20Each%20image%20necessitates%0Amultiple%20annotations%2C%20encompassing%20not%20only%20the%20object%27s%20class%20but%20also%20its%0Aprecise%20spatial%20boundaries.%20These%20requirements%20elevate%20the%20likelihood%20of%20errors%0Aand%20inconsistencies%20in%20both%20manual%20and%20automated%20annotation%20processes.%20By%0Asimulating%20different%20noise%20conditions%2C%20we%20provide%20a%20realistic%20scenario%20for%0Aassessing%20the%20robustness%20and%20generalization%20capabilities%20of%20instance%0Asegmentation%20models%20in%20different%20segmentation%20tasks%2C%20introducing%20COCO-N%20and%0ACityscapes-N.%20We%20also%20propose%20a%20benchmark%20for%20weakly%20annotation%20noise%2C%20dubbed%0ACOCO-WAN%2C%20which%20utilizes%20foundation%20models%20and%20weak%20annotations%20to%20simulate%0Asemi-automated%20annotation%20tools%20and%20their%20noisy%20labels.%20This%20study%20sheds%20light%0Aon%20the%20quality%20of%20segmentation%20masks%20produced%20by%20various%20models%20and%20challenges%0Athe%20efficacy%20of%20popular%20methods%20designed%20to%20address%20learning%20with%20label%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Label%2520Noise%2520in%2520Instance%2520Segmentation%253A%2520Spatial%2520Noise%2520Matters%26entry.906535625%3DEden%2520Grad%2520and%2520Moshe%2520Kimhi%2520and%2520Lion%2520Halika%2520and%2520Chaim%2520Baskin%26entry.1292438233%3D%2520%2520Obtaining%2520accurate%2520labels%2520for%2520instance%2520segmentation%2520is%2520particularly%250Achallenging%2520due%2520to%2520the%2520complex%2520nature%2520of%2520the%2520task.%2520Each%2520image%2520necessitates%250Amultiple%2520annotations%252C%2520encompassing%2520not%2520only%2520the%2520object%2527s%2520class%2520but%2520also%2520its%250Aprecise%2520spatial%2520boundaries.%2520These%2520requirements%2520elevate%2520the%2520likelihood%2520of%2520errors%250Aand%2520inconsistencies%2520in%2520both%2520manual%2520and%2520automated%2520annotation%2520processes.%2520By%250Asimulating%2520different%2520noise%2520conditions%252C%2520we%2520provide%2520a%2520realistic%2520scenario%2520for%250Aassessing%2520the%2520robustness%2520and%2520generalization%2520capabilities%2520of%2520instance%250Asegmentation%2520models%2520in%2520different%2520segmentation%2520tasks%252C%2520introducing%2520COCO-N%2520and%250ACityscapes-N.%2520We%2520also%2520propose%2520a%2520benchmark%2520for%2520weakly%2520annotation%2520noise%252C%2520dubbed%250ACOCO-WAN%252C%2520which%2520utilizes%2520foundation%2520models%2520and%2520weak%2520annotations%2520to%2520simulate%250Asemi-automated%2520annotation%2520tools%2520and%2520their%2520noisy%2520labels.%2520This%2520study%2520sheds%2520light%250Aon%2520the%2520quality%2520of%2520segmentation%2520masks%2520produced%2520by%2520various%2520models%2520and%2520challenges%250Athe%2520efficacy%2520of%2520popular%2520methods%2520designed%2520to%2520address%2520learning%2520with%2520label%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Label%20Noise%20in%20Instance%20Segmentation%3A%20Spatial%20Noise%20Matters&entry.906535625=Eden%20Grad%20and%20Moshe%20Kimhi%20and%20Lion%20Halika%20and%20Chaim%20Baskin&entry.1292438233=%20%20Obtaining%20accurate%20labels%20for%20instance%20segmentation%20is%20particularly%0Achallenging%20due%20to%20the%20complex%20nature%20of%20the%20task.%20Each%20image%20necessitates%0Amultiple%20annotations%2C%20encompassing%20not%20only%20the%20object%27s%20class%20but%20also%20its%0Aprecise%20spatial%20boundaries.%20These%20requirements%20elevate%20the%20likelihood%20of%20errors%0Aand%20inconsistencies%20in%20both%20manual%20and%20automated%20annotation%20processes.%20By%0Asimulating%20different%20noise%20conditions%2C%20we%20provide%20a%20realistic%20scenario%20for%0Aassessing%20the%20robustness%20and%20generalization%20capabilities%20of%20instance%0Asegmentation%20models%20in%20different%20segmentation%20tasks%2C%20introducing%20COCO-N%20and%0ACityscapes-N.%20We%20also%20propose%20a%20benchmark%20for%20weakly%20annotation%20noise%2C%20dubbed%0ACOCO-WAN%2C%20which%20utilizes%20foundation%20models%20and%20weak%20annotations%20to%20simulate%0Asemi-automated%20annotation%20tools%20and%20their%20noisy%20labels.%20This%20study%20sheds%20light%0Aon%20the%20quality%20of%20segmentation%20masks%20produced%20by%20various%20models%20and%20challenges%0Athe%20efficacy%20of%20popular%20methods%20designed%20to%20address%20learning%20with%20label%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10891v2&entry.124074799=Read"},
{"title": "Navigating Complexity: Toward Lossless Graph Condensation via Expanding\n  Window Matching", "author": "Yuchen Zhang and Tianle Zhang and Kai Wang and Ziyao Guo and Yuxuan Liang and Xavier Bresson and Wei Jin and Yang You", "abstract": "  Graph condensation aims to reduce the size of a large-scale graph dataset by\nsynthesizing a compact counterpart without sacrificing the performance of Graph\nNeural Networks (GNNs) trained on it, which has shed light on reducing the\ncomputational cost for training GNNs. Nevertheless, existing methods often fall\nshort of accurately replicating the original graph for certain datasets,\nthereby failing to achieve the objective of lossless condensation. To\nunderstand this phenomenon, we investigate the potential reasons and reveal\nthat the previous state-of-the-art trajectory matching method provides biased\nand restricted supervision signals from the original graph when optimizing the\ncondensed one. This significantly limits both the scale and efficacy of the\ncondensed graph. In this paper, we make the first attempt toward\n\\textit{lossless graph condensation} by bridging the previously neglected\nsupervision signals. Specifically, we employ a curriculum learning strategy to\ntrain expert trajectories with more diverse supervision signals from the\noriginal graph, and then effectively transfer the information into the\ncondensed graph with expanding window matching. Moreover, we design a loss\nfunction to further extract knowledge from the expert trajectories. Theoretical\nanalysis justifies the design of our method and extensive experiments verify\nits superiority across different datasets. Code is released at\nhttps://github.com/NUS-HPC-AI-Lab/GEOM.\n", "link": "http://arxiv.org/abs/2402.05011v3", "date": "2024-06-18", "relevancy": 2.0144, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.522}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4932}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20Complexity%3A%20Toward%20Lossless%20Graph%20Condensation%20via%20Expanding%0A%20%20Window%20Matching&body=Title%3A%20Navigating%20Complexity%3A%20Toward%20Lossless%20Graph%20Condensation%20via%20Expanding%0A%20%20Window%20Matching%0AAuthor%3A%20Yuchen%20Zhang%20and%20Tianle%20Zhang%20and%20Kai%20Wang%20and%20Ziyao%20Guo%20and%20Yuxuan%20Liang%20and%20Xavier%20Bresson%20and%20Wei%20Jin%20and%20Yang%20You%0AAbstract%3A%20%20%20Graph%20condensation%20aims%20to%20reduce%20the%20size%20of%20a%20large-scale%20graph%20dataset%20by%0Asynthesizing%20a%20compact%20counterpart%20without%20sacrificing%20the%20performance%20of%20Graph%0ANeural%20Networks%20%28GNNs%29%20trained%20on%20it%2C%20which%20has%20shed%20light%20on%20reducing%20the%0Acomputational%20cost%20for%20training%20GNNs.%20Nevertheless%2C%20existing%20methods%20often%20fall%0Ashort%20of%20accurately%20replicating%20the%20original%20graph%20for%20certain%20datasets%2C%0Athereby%20failing%20to%20achieve%20the%20objective%20of%20lossless%20condensation.%20To%0Aunderstand%20this%20phenomenon%2C%20we%20investigate%20the%20potential%20reasons%20and%20reveal%0Athat%20the%20previous%20state-of-the-art%20trajectory%20matching%20method%20provides%20biased%0Aand%20restricted%20supervision%20signals%20from%20the%20original%20graph%20when%20optimizing%20the%0Acondensed%20one.%20This%20significantly%20limits%20both%20the%20scale%20and%20efficacy%20of%20the%0Acondensed%20graph.%20In%20this%20paper%2C%20we%20make%20the%20first%20attempt%20toward%0A%5Ctextit%7Blossless%20graph%20condensation%7D%20by%20bridging%20the%20previously%20neglected%0Asupervision%20signals.%20Specifically%2C%20we%20employ%20a%20curriculum%20learning%20strategy%20to%0Atrain%20expert%20trajectories%20with%20more%20diverse%20supervision%20signals%20from%20the%0Aoriginal%20graph%2C%20and%20then%20effectively%20transfer%20the%20information%20into%20the%0Acondensed%20graph%20with%20expanding%20window%20matching.%20Moreover%2C%20we%20design%20a%20loss%0Afunction%20to%20further%20extract%20knowledge%20from%20the%20expert%20trajectories.%20Theoretical%0Aanalysis%20justifies%20the%20design%20of%20our%20method%20and%20extensive%20experiments%20verify%0Aits%20superiority%20across%20different%20datasets.%20Code%20is%20released%20at%0Ahttps%3A//github.com/NUS-HPC-AI-Lab/GEOM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05011v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520Complexity%253A%2520Toward%2520Lossless%2520Graph%2520Condensation%2520via%2520Expanding%250A%2520%2520Window%2520Matching%26entry.906535625%3DYuchen%2520Zhang%2520and%2520Tianle%2520Zhang%2520and%2520Kai%2520Wang%2520and%2520Ziyao%2520Guo%2520and%2520Yuxuan%2520Liang%2520and%2520Xavier%2520Bresson%2520and%2520Wei%2520Jin%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Graph%2520condensation%2520aims%2520to%2520reduce%2520the%2520size%2520of%2520a%2520large-scale%2520graph%2520dataset%2520by%250Asynthesizing%2520a%2520compact%2520counterpart%2520without%2520sacrificing%2520the%2520performance%2520of%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%2520trained%2520on%2520it%252C%2520which%2520has%2520shed%2520light%2520on%2520reducing%2520the%250Acomputational%2520cost%2520for%2520training%2520GNNs.%2520Nevertheless%252C%2520existing%2520methods%2520often%2520fall%250Ashort%2520of%2520accurately%2520replicating%2520the%2520original%2520graph%2520for%2520certain%2520datasets%252C%250Athereby%2520failing%2520to%2520achieve%2520the%2520objective%2520of%2520lossless%2520condensation.%2520To%250Aunderstand%2520this%2520phenomenon%252C%2520we%2520investigate%2520the%2520potential%2520reasons%2520and%2520reveal%250Athat%2520the%2520previous%2520state-of-the-art%2520trajectory%2520matching%2520method%2520provides%2520biased%250Aand%2520restricted%2520supervision%2520signals%2520from%2520the%2520original%2520graph%2520when%2520optimizing%2520the%250Acondensed%2520one.%2520This%2520significantly%2520limits%2520both%2520the%2520scale%2520and%2520efficacy%2520of%2520the%250Acondensed%2520graph.%2520In%2520this%2520paper%252C%2520we%2520make%2520the%2520first%2520attempt%2520toward%250A%255Ctextit%257Blossless%2520graph%2520condensation%257D%2520by%2520bridging%2520the%2520previously%2520neglected%250Asupervision%2520signals.%2520Specifically%252C%2520we%2520employ%2520a%2520curriculum%2520learning%2520strategy%2520to%250Atrain%2520expert%2520trajectories%2520with%2520more%2520diverse%2520supervision%2520signals%2520from%2520the%250Aoriginal%2520graph%252C%2520and%2520then%2520effectively%2520transfer%2520the%2520information%2520into%2520the%250Acondensed%2520graph%2520with%2520expanding%2520window%2520matching.%2520Moreover%252C%2520we%2520design%2520a%2520loss%250Afunction%2520to%2520further%2520extract%2520knowledge%2520from%2520the%2520expert%2520trajectories.%2520Theoretical%250Aanalysis%2520justifies%2520the%2520design%2520of%2520our%2520method%2520and%2520extensive%2520experiments%2520verify%250Aits%2520superiority%2520across%2520different%2520datasets.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/NUS-HPC-AI-Lab/GEOM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05011v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20Complexity%3A%20Toward%20Lossless%20Graph%20Condensation%20via%20Expanding%0A%20%20Window%20Matching&entry.906535625=Yuchen%20Zhang%20and%20Tianle%20Zhang%20and%20Kai%20Wang%20and%20Ziyao%20Guo%20and%20Yuxuan%20Liang%20and%20Xavier%20Bresson%20and%20Wei%20Jin%20and%20Yang%20You&entry.1292438233=%20%20Graph%20condensation%20aims%20to%20reduce%20the%20size%20of%20a%20large-scale%20graph%20dataset%20by%0Asynthesizing%20a%20compact%20counterpart%20without%20sacrificing%20the%20performance%20of%20Graph%0ANeural%20Networks%20%28GNNs%29%20trained%20on%20it%2C%20which%20has%20shed%20light%20on%20reducing%20the%0Acomputational%20cost%20for%20training%20GNNs.%20Nevertheless%2C%20existing%20methods%20often%20fall%0Ashort%20of%20accurately%20replicating%20the%20original%20graph%20for%20certain%20datasets%2C%0Athereby%20failing%20to%20achieve%20the%20objective%20of%20lossless%20condensation.%20To%0Aunderstand%20this%20phenomenon%2C%20we%20investigate%20the%20potential%20reasons%20and%20reveal%0Athat%20the%20previous%20state-of-the-art%20trajectory%20matching%20method%20provides%20biased%0Aand%20restricted%20supervision%20signals%20from%20the%20original%20graph%20when%20optimizing%20the%0Acondensed%20one.%20This%20significantly%20limits%20both%20the%20scale%20and%20efficacy%20of%20the%0Acondensed%20graph.%20In%20this%20paper%2C%20we%20make%20the%20first%20attempt%20toward%0A%5Ctextit%7Blossless%20graph%20condensation%7D%20by%20bridging%20the%20previously%20neglected%0Asupervision%20signals.%20Specifically%2C%20we%20employ%20a%20curriculum%20learning%20strategy%20to%0Atrain%20expert%20trajectories%20with%20more%20diverse%20supervision%20signals%20from%20the%0Aoriginal%20graph%2C%20and%20then%20effectively%20transfer%20the%20information%20into%20the%0Acondensed%20graph%20with%20expanding%20window%20matching.%20Moreover%2C%20we%20design%20a%20loss%0Afunction%20to%20further%20extract%20knowledge%20from%20the%20expert%20trajectories.%20Theoretical%0Aanalysis%20justifies%20the%20design%20of%20our%20method%20and%20extensive%20experiments%20verify%0Aits%20superiority%20across%20different%20datasets.%20Code%20is%20released%20at%0Ahttps%3A//github.com/NUS-HPC-AI-Lab/GEOM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05011v3&entry.124074799=Read"},
{"title": "Erase to Enhance: Data-Efficient Machine Unlearning in MRI\n  Reconstruction", "author": "Yuyang Xue and Jingshuai Liu and Steven McDonagh and Sotirios A. Tsaftaris", "abstract": "  Machine unlearning is a promising paradigm for removing unwanted data samples\nfrom a trained model, towards ensuring compliance with privacy regulations and\nlimiting harmful biases. Although unlearning has been shown in, e.g.,\nclassification and recommendation systems, its potential in medical\nimage-to-image translation, specifically in image recon-struction, has not been\nthoroughly investigated. This paper shows that machine unlearning is possible\nin MRI tasks and has the potential to benefit for bias removal. We set up a\nprotocol to study how much shared knowledge exists between datasets of\ndifferent organs, allowing us to effectively quantify the effect of unlearning.\nOur study reveals that combining training data can lead to hallucinations and\nreduced image quality in the reconstructed data. We use unlearning to remove\nhallucinations as a proxy exemplar of undesired data removal. Indeed, we show\nthat machine unlearning is possible without full retraining. Furthermore, our\nobservations indicate that maintaining high performance is feasible even when\nusing only a subset of retain data. We have made our code publicly accessible.\n", "link": "http://arxiv.org/abs/2405.15517v2", "date": "2024-06-18", "relevancy": 2.0142, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5667}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4973}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Erase%20to%20Enhance%3A%20Data-Efficient%20Machine%20Unlearning%20in%20MRI%0A%20%20Reconstruction&body=Title%3A%20Erase%20to%20Enhance%3A%20Data-Efficient%20Machine%20Unlearning%20in%20MRI%0A%20%20Reconstruction%0AAuthor%3A%20Yuyang%20Xue%20and%20Jingshuai%20Liu%20and%20Steven%20McDonagh%20and%20Sotirios%20A.%20Tsaftaris%0AAbstract%3A%20%20%20Machine%20unlearning%20is%20a%20promising%20paradigm%20for%20removing%20unwanted%20data%20samples%0Afrom%20a%20trained%20model%2C%20towards%20ensuring%20compliance%20with%20privacy%20regulations%20and%0Alimiting%20harmful%20biases.%20Although%20unlearning%20has%20been%20shown%20in%2C%20e.g.%2C%0Aclassification%20and%20recommendation%20systems%2C%20its%20potential%20in%20medical%0Aimage-to-image%20translation%2C%20specifically%20in%20image%20recon-struction%2C%20has%20not%20been%0Athoroughly%20investigated.%20This%20paper%20shows%20that%20machine%20unlearning%20is%20possible%0Ain%20MRI%20tasks%20and%20has%20the%20potential%20to%20benefit%20for%20bias%20removal.%20We%20set%20up%20a%0Aprotocol%20to%20study%20how%20much%20shared%20knowledge%20exists%20between%20datasets%20of%0Adifferent%20organs%2C%20allowing%20us%20to%20effectively%20quantify%20the%20effect%20of%20unlearning.%0AOur%20study%20reveals%20that%20combining%20training%20data%20can%20lead%20to%20hallucinations%20and%0Areduced%20image%20quality%20in%20the%20reconstructed%20data.%20We%20use%20unlearning%20to%20remove%0Ahallucinations%20as%20a%20proxy%20exemplar%20of%20undesired%20data%20removal.%20Indeed%2C%20we%20show%0Athat%20machine%20unlearning%20is%20possible%20without%20full%20retraining.%20Furthermore%2C%20our%0Aobservations%20indicate%20that%20maintaining%20high%20performance%20is%20feasible%20even%20when%0Ausing%20only%20a%20subset%20of%20retain%20data.%20We%20have%20made%20our%20code%20publicly%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15517v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DErase%2520to%2520Enhance%253A%2520Data-Efficient%2520Machine%2520Unlearning%2520in%2520MRI%250A%2520%2520Reconstruction%26entry.906535625%3DYuyang%2520Xue%2520and%2520Jingshuai%2520Liu%2520and%2520Steven%2520McDonagh%2520and%2520Sotirios%2520A.%2520Tsaftaris%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520is%2520a%2520promising%2520paradigm%2520for%2520removing%2520unwanted%2520data%2520samples%250Afrom%2520a%2520trained%2520model%252C%2520towards%2520ensuring%2520compliance%2520with%2520privacy%2520regulations%2520and%250Alimiting%2520harmful%2520biases.%2520Although%2520unlearning%2520has%2520been%2520shown%2520in%252C%2520e.g.%252C%250Aclassification%2520and%2520recommendation%2520systems%252C%2520its%2520potential%2520in%2520medical%250Aimage-to-image%2520translation%252C%2520specifically%2520in%2520image%2520recon-struction%252C%2520has%2520not%2520been%250Athoroughly%2520investigated.%2520This%2520paper%2520shows%2520that%2520machine%2520unlearning%2520is%2520possible%250Ain%2520MRI%2520tasks%2520and%2520has%2520the%2520potential%2520to%2520benefit%2520for%2520bias%2520removal.%2520We%2520set%2520up%2520a%250Aprotocol%2520to%2520study%2520how%2520much%2520shared%2520knowledge%2520exists%2520between%2520datasets%2520of%250Adifferent%2520organs%252C%2520allowing%2520us%2520to%2520effectively%2520quantify%2520the%2520effect%2520of%2520unlearning.%250AOur%2520study%2520reveals%2520that%2520combining%2520training%2520data%2520can%2520lead%2520to%2520hallucinations%2520and%250Areduced%2520image%2520quality%2520in%2520the%2520reconstructed%2520data.%2520We%2520use%2520unlearning%2520to%2520remove%250Ahallucinations%2520as%2520a%2520proxy%2520exemplar%2520of%2520undesired%2520data%2520removal.%2520Indeed%252C%2520we%2520show%250Athat%2520machine%2520unlearning%2520is%2520possible%2520without%2520full%2520retraining.%2520Furthermore%252C%2520our%250Aobservations%2520indicate%2520that%2520maintaining%2520high%2520performance%2520is%2520feasible%2520even%2520when%250Ausing%2520only%2520a%2520subset%2520of%2520retain%2520data.%2520We%2520have%2520made%2520our%2520code%2520publicly%2520accessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15517v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Erase%20to%20Enhance%3A%20Data-Efficient%20Machine%20Unlearning%20in%20MRI%0A%20%20Reconstruction&entry.906535625=Yuyang%20Xue%20and%20Jingshuai%20Liu%20and%20Steven%20McDonagh%20and%20Sotirios%20A.%20Tsaftaris&entry.1292438233=%20%20Machine%20unlearning%20is%20a%20promising%20paradigm%20for%20removing%20unwanted%20data%20samples%0Afrom%20a%20trained%20model%2C%20towards%20ensuring%20compliance%20with%20privacy%20regulations%20and%0Alimiting%20harmful%20biases.%20Although%20unlearning%20has%20been%20shown%20in%2C%20e.g.%2C%0Aclassification%20and%20recommendation%20systems%2C%20its%20potential%20in%20medical%0Aimage-to-image%20translation%2C%20specifically%20in%20image%20recon-struction%2C%20has%20not%20been%0Athoroughly%20investigated.%20This%20paper%20shows%20that%20machine%20unlearning%20is%20possible%0Ain%20MRI%20tasks%20and%20has%20the%20potential%20to%20benefit%20for%20bias%20removal.%20We%20set%20up%20a%0Aprotocol%20to%20study%20how%20much%20shared%20knowledge%20exists%20between%20datasets%20of%0Adifferent%20organs%2C%20allowing%20us%20to%20effectively%20quantify%20the%20effect%20of%20unlearning.%0AOur%20study%20reveals%20that%20combining%20training%20data%20can%20lead%20to%20hallucinations%20and%0Areduced%20image%20quality%20in%20the%20reconstructed%20data.%20We%20use%20unlearning%20to%20remove%0Ahallucinations%20as%20a%20proxy%20exemplar%20of%20undesired%20data%20removal.%20Indeed%2C%20we%20show%0Athat%20machine%20unlearning%20is%20possible%20without%20full%20retraining.%20Furthermore%2C%20our%0Aobservations%20indicate%20that%20maintaining%20high%20performance%20is%20feasible%20even%20when%0Ausing%20only%20a%20subset%20of%20retain%20data.%20We%20have%20made%20our%20code%20publicly%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15517v2&entry.124074799=Read"},
{"title": "Inference via Interpolation: Contrastive Representations Provably Enable\n  Planning and Inference", "author": "Benjamin Eysenbach and Vivek Myers and Ruslan Salakhutdinov and Sergey Levine", "abstract": "  Given time series data, how can we answer questions like \"what will happen in\nthe future?\" and \"how did we get here?\" These sorts of probabilistic inference\nquestions are challenging when observations are high-dimensional. In this\npaper, we show how these questions can have compact, closed form solutions in\nterms of learned representations. The key idea is to apply a variant of\ncontrastive learning to time series data. Prior work already shows that the\nrepresentations learned by contrastive learning encode a probability ratio. By\nextending prior work to show that the marginal distribution over\nrepresentations is Gaussian, we can then prove that joint distribution of\nrepresentations is also Gaussian. Taken together, these results show that\nrepresentations learned via temporal contrastive learning follow a Gauss-Markov\nchain, a graphical model where inference (e.g., prediction, planning) over\nrepresentations corresponds to inverting a low-dimensional matrix. In one\nspecial case, inferring intermediate representations will be equivalent to\ninterpolating between the learned representations. We validate our theory using\nnumerical simulations on tasks up to 46-dimensions.\n", "link": "http://arxiv.org/abs/2403.04082v2", "date": "2024-06-18", "relevancy": 2.0075, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.545}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4962}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference%20via%20Interpolation%3A%20Contrastive%20Representations%20Provably%20Enable%0A%20%20Planning%20and%20Inference&body=Title%3A%20Inference%20via%20Interpolation%3A%20Contrastive%20Representations%20Provably%20Enable%0A%20%20Planning%20and%20Inference%0AAuthor%3A%20Benjamin%20Eysenbach%20and%20Vivek%20Myers%20and%20Ruslan%20Salakhutdinov%20and%20Sergey%20Levine%0AAbstract%3A%20%20%20Given%20time%20series%20data%2C%20how%20can%20we%20answer%20questions%20like%20%22what%20will%20happen%20in%0Athe%20future%3F%22%20and%20%22how%20did%20we%20get%20here%3F%22%20These%20sorts%20of%20probabilistic%20inference%0Aquestions%20are%20challenging%20when%20observations%20are%20high-dimensional.%20In%20this%0Apaper%2C%20we%20show%20how%20these%20questions%20can%20have%20compact%2C%20closed%20form%20solutions%20in%0Aterms%20of%20learned%20representations.%20The%20key%20idea%20is%20to%20apply%20a%20variant%20of%0Acontrastive%20learning%20to%20time%20series%20data.%20Prior%20work%20already%20shows%20that%20the%0Arepresentations%20learned%20by%20contrastive%20learning%20encode%20a%20probability%20ratio.%20By%0Aextending%20prior%20work%20to%20show%20that%20the%20marginal%20distribution%20over%0Arepresentations%20is%20Gaussian%2C%20we%20can%20then%20prove%20that%20joint%20distribution%20of%0Arepresentations%20is%20also%20Gaussian.%20Taken%20together%2C%20these%20results%20show%20that%0Arepresentations%20learned%20via%20temporal%20contrastive%20learning%20follow%20a%20Gauss-Markov%0Achain%2C%20a%20graphical%20model%20where%20inference%20%28e.g.%2C%20prediction%2C%20planning%29%20over%0Arepresentations%20corresponds%20to%20inverting%20a%20low-dimensional%20matrix.%20In%20one%0Aspecial%20case%2C%20inferring%20intermediate%20representations%20will%20be%20equivalent%20to%0Ainterpolating%20between%20the%20learned%20representations.%20We%20validate%20our%20theory%20using%0Anumerical%20simulations%20on%20tasks%20up%20to%2046-dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference%2520via%2520Interpolation%253A%2520Contrastive%2520Representations%2520Provably%2520Enable%250A%2520%2520Planning%2520and%2520Inference%26entry.906535625%3DBenjamin%2520Eysenbach%2520and%2520Vivek%2520Myers%2520and%2520Ruslan%2520Salakhutdinov%2520and%2520Sergey%2520Levine%26entry.1292438233%3D%2520%2520Given%2520time%2520series%2520data%252C%2520how%2520can%2520we%2520answer%2520questions%2520like%2520%2522what%2520will%2520happen%2520in%250Athe%2520future%253F%2522%2520and%2520%2522how%2520did%2520we%2520get%2520here%253F%2522%2520These%2520sorts%2520of%2520probabilistic%2520inference%250Aquestions%2520are%2520challenging%2520when%2520observations%2520are%2520high-dimensional.%2520In%2520this%250Apaper%252C%2520we%2520show%2520how%2520these%2520questions%2520can%2520have%2520compact%252C%2520closed%2520form%2520solutions%2520in%250Aterms%2520of%2520learned%2520representations.%2520The%2520key%2520idea%2520is%2520to%2520apply%2520a%2520variant%2520of%250Acontrastive%2520learning%2520to%2520time%2520series%2520data.%2520Prior%2520work%2520already%2520shows%2520that%2520the%250Arepresentations%2520learned%2520by%2520contrastive%2520learning%2520encode%2520a%2520probability%2520ratio.%2520By%250Aextending%2520prior%2520work%2520to%2520show%2520that%2520the%2520marginal%2520distribution%2520over%250Arepresentations%2520is%2520Gaussian%252C%2520we%2520can%2520then%2520prove%2520that%2520joint%2520distribution%2520of%250Arepresentations%2520is%2520also%2520Gaussian.%2520Taken%2520together%252C%2520these%2520results%2520show%2520that%250Arepresentations%2520learned%2520via%2520temporal%2520contrastive%2520learning%2520follow%2520a%2520Gauss-Markov%250Achain%252C%2520a%2520graphical%2520model%2520where%2520inference%2520%2528e.g.%252C%2520prediction%252C%2520planning%2529%2520over%250Arepresentations%2520corresponds%2520to%2520inverting%2520a%2520low-dimensional%2520matrix.%2520In%2520one%250Aspecial%2520case%252C%2520inferring%2520intermediate%2520representations%2520will%2520be%2520equivalent%2520to%250Ainterpolating%2520between%2520the%2520learned%2520representations.%2520We%2520validate%2520our%2520theory%2520using%250Anumerical%2520simulations%2520on%2520tasks%2520up%2520to%252046-dimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference%20via%20Interpolation%3A%20Contrastive%20Representations%20Provably%20Enable%0A%20%20Planning%20and%20Inference&entry.906535625=Benjamin%20Eysenbach%20and%20Vivek%20Myers%20and%20Ruslan%20Salakhutdinov%20and%20Sergey%20Levine&entry.1292438233=%20%20Given%20time%20series%20data%2C%20how%20can%20we%20answer%20questions%20like%20%22what%20will%20happen%20in%0Athe%20future%3F%22%20and%20%22how%20did%20we%20get%20here%3F%22%20These%20sorts%20of%20probabilistic%20inference%0Aquestions%20are%20challenging%20when%20observations%20are%20high-dimensional.%20In%20this%0Apaper%2C%20we%20show%20how%20these%20questions%20can%20have%20compact%2C%20closed%20form%20solutions%20in%0Aterms%20of%20learned%20representations.%20The%20key%20idea%20is%20to%20apply%20a%20variant%20of%0Acontrastive%20learning%20to%20time%20series%20data.%20Prior%20work%20already%20shows%20that%20the%0Arepresentations%20learned%20by%20contrastive%20learning%20encode%20a%20probability%20ratio.%20By%0Aextending%20prior%20work%20to%20show%20that%20the%20marginal%20distribution%20over%0Arepresentations%20is%20Gaussian%2C%20we%20can%20then%20prove%20that%20joint%20distribution%20of%0Arepresentations%20is%20also%20Gaussian.%20Taken%20together%2C%20these%20results%20show%20that%0Arepresentations%20learned%20via%20temporal%20contrastive%20learning%20follow%20a%20Gauss-Markov%0Achain%2C%20a%20graphical%20model%20where%20inference%20%28e.g.%2C%20prediction%2C%20planning%29%20over%0Arepresentations%20corresponds%20to%20inverting%20a%20low-dimensional%20matrix.%20In%20one%0Aspecial%20case%2C%20inferring%20intermediate%20representations%20will%20be%20equivalent%20to%0Ainterpolating%20between%20the%20learned%20representations.%20We%20validate%20our%20theory%20using%0Anumerical%20simulations%20on%20tasks%20up%20to%2046-dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04082v2&entry.124074799=Read"},
{"title": "ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation", "author": "Debalina Ghosh Paul and Hong Zhu and Ian Bayley", "abstract": "  In the scenario-based evaluation of machine learning models, a key problem is\nhow to construct test datasets that represent various scenarios. The\nmethodology proposed in this paper is to construct a benchmark and attach\nmetadata to each test case. Then a test system can be constructed with test\nmorphisms that filter the test cases based on metadata to form a dataset.\n  The paper demonstrates this methodology with large language models for code\ngeneration. A benchmark called ScenEval is constructed from problems in\ntextbooks, an online tutorial website and Stack Overflow. Filtering by scenario\nis demonstrated and the test sets are used to evaluate ChatGPT for Java code\ngeneration.\n  Our experiments found that the performance of ChatGPT decreases with the\ncomplexity of the coding task. It is weakest for advanced topics like\nmulti-threading, data structure algorithms and recursive methods. The Java code\ngenerated by ChatGPT tends to be much shorter than reference solution in terms\nof number of lines, while it is more likely to be more complex in both\ncyclomatic and cognitive complexity metrics, if the generated code is correct.\nHowever, the generated code is more likely to be less complex than the\nreference solution if the code is incorrect.\n", "link": "http://arxiv.org/abs/2406.12635v1", "date": "2024-06-18", "relevancy": 2.0068, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5103}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4976}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScenEval%3A%20A%20Benchmark%20for%20Scenario-Based%20Evaluation%20of%20Code%20Generation&body=Title%3A%20ScenEval%3A%20A%20Benchmark%20for%20Scenario-Based%20Evaluation%20of%20Code%20Generation%0AAuthor%3A%20Debalina%20Ghosh%20Paul%20and%20Hong%20Zhu%20and%20Ian%20Bayley%0AAbstract%3A%20%20%20In%20the%20scenario-based%20evaluation%20of%20machine%20learning%20models%2C%20a%20key%20problem%20is%0Ahow%20to%20construct%20test%20datasets%20that%20represent%20various%20scenarios.%20The%0Amethodology%20proposed%20in%20this%20paper%20is%20to%20construct%20a%20benchmark%20and%20attach%0Ametadata%20to%20each%20test%20case.%20Then%20a%20test%20system%20can%20be%20constructed%20with%20test%0Amorphisms%20that%20filter%20the%20test%20cases%20based%20on%20metadata%20to%20form%20a%20dataset.%0A%20%20The%20paper%20demonstrates%20this%20methodology%20with%20large%20language%20models%20for%20code%0Ageneration.%20A%20benchmark%20called%20ScenEval%20is%20constructed%20from%20problems%20in%0Atextbooks%2C%20an%20online%20tutorial%20website%20and%20Stack%20Overflow.%20Filtering%20by%20scenario%0Ais%20demonstrated%20and%20the%20test%20sets%20are%20used%20to%20evaluate%20ChatGPT%20for%20Java%20code%0Ageneration.%0A%20%20Our%20experiments%20found%20that%20the%20performance%20of%20ChatGPT%20decreases%20with%20the%0Acomplexity%20of%20the%20coding%20task.%20It%20is%20weakest%20for%20advanced%20topics%20like%0Amulti-threading%2C%20data%20structure%20algorithms%20and%20recursive%20methods.%20The%20Java%20code%0Agenerated%20by%20ChatGPT%20tends%20to%20be%20much%20shorter%20than%20reference%20solution%20in%20terms%0Aof%20number%20of%20lines%2C%20while%20it%20is%20more%20likely%20to%20be%20more%20complex%20in%20both%0Acyclomatic%20and%20cognitive%20complexity%20metrics%2C%20if%20the%20generated%20code%20is%20correct.%0AHowever%2C%20the%20generated%20code%20is%20more%20likely%20to%20be%20less%20complex%20than%20the%0Areference%20solution%20if%20the%20code%20is%20incorrect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScenEval%253A%2520A%2520Benchmark%2520for%2520Scenario-Based%2520Evaluation%2520of%2520Code%2520Generation%26entry.906535625%3DDebalina%2520Ghosh%2520Paul%2520and%2520Hong%2520Zhu%2520and%2520Ian%2520Bayley%26entry.1292438233%3D%2520%2520In%2520the%2520scenario-based%2520evaluation%2520of%2520machine%2520learning%2520models%252C%2520a%2520key%2520problem%2520is%250Ahow%2520to%2520construct%2520test%2520datasets%2520that%2520represent%2520various%2520scenarios.%2520The%250Amethodology%2520proposed%2520in%2520this%2520paper%2520is%2520to%2520construct%2520a%2520benchmark%2520and%2520attach%250Ametadata%2520to%2520each%2520test%2520case.%2520Then%2520a%2520test%2520system%2520can%2520be%2520constructed%2520with%2520test%250Amorphisms%2520that%2520filter%2520the%2520test%2520cases%2520based%2520on%2520metadata%2520to%2520form%2520a%2520dataset.%250A%2520%2520The%2520paper%2520demonstrates%2520this%2520methodology%2520with%2520large%2520language%2520models%2520for%2520code%250Ageneration.%2520A%2520benchmark%2520called%2520ScenEval%2520is%2520constructed%2520from%2520problems%2520in%250Atextbooks%252C%2520an%2520online%2520tutorial%2520website%2520and%2520Stack%2520Overflow.%2520Filtering%2520by%2520scenario%250Ais%2520demonstrated%2520and%2520the%2520test%2520sets%2520are%2520used%2520to%2520evaluate%2520ChatGPT%2520for%2520Java%2520code%250Ageneration.%250A%2520%2520Our%2520experiments%2520found%2520that%2520the%2520performance%2520of%2520ChatGPT%2520decreases%2520with%2520the%250Acomplexity%2520of%2520the%2520coding%2520task.%2520It%2520is%2520weakest%2520for%2520advanced%2520topics%2520like%250Amulti-threading%252C%2520data%2520structure%2520algorithms%2520and%2520recursive%2520methods.%2520The%2520Java%2520code%250Agenerated%2520by%2520ChatGPT%2520tends%2520to%2520be%2520much%2520shorter%2520than%2520reference%2520solution%2520in%2520terms%250Aof%2520number%2520of%2520lines%252C%2520while%2520it%2520is%2520more%2520likely%2520to%2520be%2520more%2520complex%2520in%2520both%250Acyclomatic%2520and%2520cognitive%2520complexity%2520metrics%252C%2520if%2520the%2520generated%2520code%2520is%2520correct.%250AHowever%252C%2520the%2520generated%2520code%2520is%2520more%2520likely%2520to%2520be%2520less%2520complex%2520than%2520the%250Areference%2520solution%2520if%2520the%2520code%2520is%2520incorrect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScenEval%3A%20A%20Benchmark%20for%20Scenario-Based%20Evaluation%20of%20Code%20Generation&entry.906535625=Debalina%20Ghosh%20Paul%20and%20Hong%20Zhu%20and%20Ian%20Bayley&entry.1292438233=%20%20In%20the%20scenario-based%20evaluation%20of%20machine%20learning%20models%2C%20a%20key%20problem%20is%0Ahow%20to%20construct%20test%20datasets%20that%20represent%20various%20scenarios.%20The%0Amethodology%20proposed%20in%20this%20paper%20is%20to%20construct%20a%20benchmark%20and%20attach%0Ametadata%20to%20each%20test%20case.%20Then%20a%20test%20system%20can%20be%20constructed%20with%20test%0Amorphisms%20that%20filter%20the%20test%20cases%20based%20on%20metadata%20to%20form%20a%20dataset.%0A%20%20The%20paper%20demonstrates%20this%20methodology%20with%20large%20language%20models%20for%20code%0Ageneration.%20A%20benchmark%20called%20ScenEval%20is%20constructed%20from%20problems%20in%0Atextbooks%2C%20an%20online%20tutorial%20website%20and%20Stack%20Overflow.%20Filtering%20by%20scenario%0Ais%20demonstrated%20and%20the%20test%20sets%20are%20used%20to%20evaluate%20ChatGPT%20for%20Java%20code%0Ageneration.%0A%20%20Our%20experiments%20found%20that%20the%20performance%20of%20ChatGPT%20decreases%20with%20the%0Acomplexity%20of%20the%20coding%20task.%20It%20is%20weakest%20for%20advanced%20topics%20like%0Amulti-threading%2C%20data%20structure%20algorithms%20and%20recursive%20methods.%20The%20Java%20code%0Agenerated%20by%20ChatGPT%20tends%20to%20be%20much%20shorter%20than%20reference%20solution%20in%20terms%0Aof%20number%20of%20lines%2C%20while%20it%20is%20more%20likely%20to%20be%20more%20complex%20in%20both%0Acyclomatic%20and%20cognitive%20complexity%20metrics%2C%20if%20the%20generated%20code%20is%20correct.%0AHowever%2C%20the%20generated%20code%20is%20more%20likely%20to%20be%20less%20complex%20than%20the%0Areference%20solution%20if%20the%20code%20is%20incorrect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12635v1&entry.124074799=Read"},
{"title": "A Single Graph Convolution Is All You Need: Efficient Grayscale Image\n  Classification", "author": "Jacob Fein-Ashley and Tian Ye and Sachini Wickramasinghe and Bingyi Zhang and Rajgopal Kannan and Viktor Prasanna", "abstract": "  Image classifiers often rely on convolutional neural networks (CNN) for their\ntasks, which are inherently more heavyweight than multilayer perceptrons\n(MLPs), which can be problematic in real-time applications. Additionally, many\nimage classification models work on both RGB and grayscale datasets.\nClassifiers that operate solely on grayscale images are much less common.\nGrayscale image classification has diverse applications, including but not\nlimited to medical image classification and synthetic aperture radar (SAR)\nautomatic target recognition (ATR). Thus, we present a novel grayscale (single\nchannel) image classification approach using a vectorized view of images. We\nexploit the lightweightness of MLPs by viewing images as a vector and reducing\nour problem setting to the grayscale image classification setting. We find that\nusing a single graph convolutional layer batch-wise increases accuracy and\nreduces variance in the performance of our model. Moreover, we develop a\ncustomized accelerator on FPGA for the proposed model with several\noptimizations to improve its performance. Our experimental results on benchmark\ngrayscale image datasets demonstrate the effectiveness of the proposed model,\nachieving vastly lower latency (up to 16$\\times$ less) and competitive or\nleading performance compared to other state-of-the-art image classification\nmodels on various domain-specific grayscale image classification datasets.\n", "link": "http://arxiv.org/abs/2402.00564v4", "date": "2024-06-18", "relevancy": 2.0042, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5226}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5066}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Single%20Graph%20Convolution%20Is%20All%20You%20Need%3A%20Efficient%20Grayscale%20Image%0A%20%20Classification&body=Title%3A%20A%20Single%20Graph%20Convolution%20Is%20All%20You%20Need%3A%20Efficient%20Grayscale%20Image%0A%20%20Classification%0AAuthor%3A%20Jacob%20Fein-Ashley%20and%20Tian%20Ye%20and%20Sachini%20Wickramasinghe%20and%20Bingyi%20Zhang%20and%20Rajgopal%20Kannan%20and%20Viktor%20Prasanna%0AAbstract%3A%20%20%20Image%20classifiers%20often%20rely%20on%20convolutional%20neural%20networks%20%28CNN%29%20for%20their%0Atasks%2C%20which%20are%20inherently%20more%20heavyweight%20than%20multilayer%20perceptrons%0A%28MLPs%29%2C%20which%20can%20be%20problematic%20in%20real-time%20applications.%20Additionally%2C%20many%0Aimage%20classification%20models%20work%20on%20both%20RGB%20and%20grayscale%20datasets.%0AClassifiers%20that%20operate%20solely%20on%20grayscale%20images%20are%20much%20less%20common.%0AGrayscale%20image%20classification%20has%20diverse%20applications%2C%20including%20but%20not%0Alimited%20to%20medical%20image%20classification%20and%20synthetic%20aperture%20radar%20%28SAR%29%0Aautomatic%20target%20recognition%20%28ATR%29.%20Thus%2C%20we%20present%20a%20novel%20grayscale%20%28single%0Achannel%29%20image%20classification%20approach%20using%20a%20vectorized%20view%20of%20images.%20We%0Aexploit%20the%20lightweightness%20of%20MLPs%20by%20viewing%20images%20as%20a%20vector%20and%20reducing%0Aour%20problem%20setting%20to%20the%20grayscale%20image%20classification%20setting.%20We%20find%20that%0Ausing%20a%20single%20graph%20convolutional%20layer%20batch-wise%20increases%20accuracy%20and%0Areduces%20variance%20in%20the%20performance%20of%20our%20model.%20Moreover%2C%20we%20develop%20a%0Acustomized%20accelerator%20on%20FPGA%20for%20the%20proposed%20model%20with%20several%0Aoptimizations%20to%20improve%20its%20performance.%20Our%20experimental%20results%20on%20benchmark%0Agrayscale%20image%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20model%2C%0Aachieving%20vastly%20lower%20latency%20%28up%20to%2016%24%5Ctimes%24%20less%29%20and%20competitive%20or%0Aleading%20performance%20compared%20to%20other%20state-of-the-art%20image%20classification%0Amodels%20on%20various%20domain-specific%20grayscale%20image%20classification%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00564v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Single%2520Graph%2520Convolution%2520Is%2520All%2520You%2520Need%253A%2520Efficient%2520Grayscale%2520Image%250A%2520%2520Classification%26entry.906535625%3DJacob%2520Fein-Ashley%2520and%2520Tian%2520Ye%2520and%2520Sachini%2520Wickramasinghe%2520and%2520Bingyi%2520Zhang%2520and%2520Rajgopal%2520Kannan%2520and%2520Viktor%2520Prasanna%26entry.1292438233%3D%2520%2520Image%2520classifiers%2520often%2520rely%2520on%2520convolutional%2520neural%2520networks%2520%2528CNN%2529%2520for%2520their%250Atasks%252C%2520which%2520are%2520inherently%2520more%2520heavyweight%2520than%2520multilayer%2520perceptrons%250A%2528MLPs%2529%252C%2520which%2520can%2520be%2520problematic%2520in%2520real-time%2520applications.%2520Additionally%252C%2520many%250Aimage%2520classification%2520models%2520work%2520on%2520both%2520RGB%2520and%2520grayscale%2520datasets.%250AClassifiers%2520that%2520operate%2520solely%2520on%2520grayscale%2520images%2520are%2520much%2520less%2520common.%250AGrayscale%2520image%2520classification%2520has%2520diverse%2520applications%252C%2520including%2520but%2520not%250Alimited%2520to%2520medical%2520image%2520classification%2520and%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%250Aautomatic%2520target%2520recognition%2520%2528ATR%2529.%2520Thus%252C%2520we%2520present%2520a%2520novel%2520grayscale%2520%2528single%250Achannel%2529%2520image%2520classification%2520approach%2520using%2520a%2520vectorized%2520view%2520of%2520images.%2520We%250Aexploit%2520the%2520lightweightness%2520of%2520MLPs%2520by%2520viewing%2520images%2520as%2520a%2520vector%2520and%2520reducing%250Aour%2520problem%2520setting%2520to%2520the%2520grayscale%2520image%2520classification%2520setting.%2520We%2520find%2520that%250Ausing%2520a%2520single%2520graph%2520convolutional%2520layer%2520batch-wise%2520increases%2520accuracy%2520and%250Areduces%2520variance%2520in%2520the%2520performance%2520of%2520our%2520model.%2520Moreover%252C%2520we%2520develop%2520a%250Acustomized%2520accelerator%2520on%2520FPGA%2520for%2520the%2520proposed%2520model%2520with%2520several%250Aoptimizations%2520to%2520improve%2520its%2520performance.%2520Our%2520experimental%2520results%2520on%2520benchmark%250Agrayscale%2520image%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520model%252C%250Aachieving%2520vastly%2520lower%2520latency%2520%2528up%2520to%252016%2524%255Ctimes%2524%2520less%2529%2520and%2520competitive%2520or%250Aleading%2520performance%2520compared%2520to%2520other%2520state-of-the-art%2520image%2520classification%250Amodels%2520on%2520various%2520domain-specific%2520grayscale%2520image%2520classification%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00564v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Single%20Graph%20Convolution%20Is%20All%20You%20Need%3A%20Efficient%20Grayscale%20Image%0A%20%20Classification&entry.906535625=Jacob%20Fein-Ashley%20and%20Tian%20Ye%20and%20Sachini%20Wickramasinghe%20and%20Bingyi%20Zhang%20and%20Rajgopal%20Kannan%20and%20Viktor%20Prasanna&entry.1292438233=%20%20Image%20classifiers%20often%20rely%20on%20convolutional%20neural%20networks%20%28CNN%29%20for%20their%0Atasks%2C%20which%20are%20inherently%20more%20heavyweight%20than%20multilayer%20perceptrons%0A%28MLPs%29%2C%20which%20can%20be%20problematic%20in%20real-time%20applications.%20Additionally%2C%20many%0Aimage%20classification%20models%20work%20on%20both%20RGB%20and%20grayscale%20datasets.%0AClassifiers%20that%20operate%20solely%20on%20grayscale%20images%20are%20much%20less%20common.%0AGrayscale%20image%20classification%20has%20diverse%20applications%2C%20including%20but%20not%0Alimited%20to%20medical%20image%20classification%20and%20synthetic%20aperture%20radar%20%28SAR%29%0Aautomatic%20target%20recognition%20%28ATR%29.%20Thus%2C%20we%20present%20a%20novel%20grayscale%20%28single%0Achannel%29%20image%20classification%20approach%20using%20a%20vectorized%20view%20of%20images.%20We%0Aexploit%20the%20lightweightness%20of%20MLPs%20by%20viewing%20images%20as%20a%20vector%20and%20reducing%0Aour%20problem%20setting%20to%20the%20grayscale%20image%20classification%20setting.%20We%20find%20that%0Ausing%20a%20single%20graph%20convolutional%20layer%20batch-wise%20increases%20accuracy%20and%0Areduces%20variance%20in%20the%20performance%20of%20our%20model.%20Moreover%2C%20we%20develop%20a%0Acustomized%20accelerator%20on%20FPGA%20for%20the%20proposed%20model%20with%20several%0Aoptimizations%20to%20improve%20its%20performance.%20Our%20experimental%20results%20on%20benchmark%0Agrayscale%20image%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20model%2C%0Aachieving%20vastly%20lower%20latency%20%28up%20to%2016%24%5Ctimes%24%20less%29%20and%20competitive%20or%0Aleading%20performance%20compared%20to%20other%20state-of-the-art%20image%20classification%0Amodels%20on%20various%20domain-specific%20grayscale%20image%20classification%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00564v4&entry.124074799=Read"},
{"title": "Beyond Visual Appearances: Privacy-sensitive Objects Identification via\n  Hybrid Graph Reasoning", "author": "Zhuohang Jiang and Bingkui Tong and Xia Du and Ahmed Alhammadi and Jizhe Zhou", "abstract": "  The Privacy-sensitive Object Identification (POI) task allocates bounding\nboxes for privacy-sensitive objects in a scene. The key to POI is settling an\nobject's privacy class (privacy-sensitive or non-privacy-sensitive). In\ncontrast to conventional object classes which are determined by the visual\nappearance of an object, one object's privacy class is derived from the scene\ncontexts and is subject to various implicit factors beyond its visual\nappearance. That is, visually similar objects may be totally opposite in their\nprivacy classes. To explicitly derive the objects' privacy class from the scene\ncontexts, in this paper, we interpret the POI task as a visual reasoning task\naimed at the privacy of each object in the scene. Following this\ninterpretation, we propose the PrivacyGuard framework for POI. PrivacyGuard\ncontains three stages. i) Structuring: an unstructured image is first converted\ninto a structured, heterogeneous scene graph that embeds rich scene contexts.\nii) Data Augmentation: a contextual perturbation oversampling strategy is\nproposed to create slightly perturbed privacy-sensitive objects in a scene\ngraph, thereby balancing the skewed distribution of privacy classes. iii)\nHybrid Graph Generation & Reasoning: the balanced, heterogeneous scene graph is\nthen transformed into a hybrid graph by endowing it with extra \"node-node\" and\n\"edge-edge\" homogeneous paths. These homogeneous paths allow direct message\npassing between nodes or edges, thereby accelerating reasoning and facilitating\nthe capturing of subtle context changes. Based on this hybrid graph... **For\nthe full abstract, see the original paper.**\n", "link": "http://arxiv.org/abs/2406.12736v1", "date": "2024-06-18", "relevancy": 1.9989, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5119}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4958}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Visual%20Appearances%3A%20Privacy-sensitive%20Objects%20Identification%20via%0A%20%20Hybrid%20Graph%20Reasoning&body=Title%3A%20Beyond%20Visual%20Appearances%3A%20Privacy-sensitive%20Objects%20Identification%20via%0A%20%20Hybrid%20Graph%20Reasoning%0AAuthor%3A%20Zhuohang%20Jiang%20and%20Bingkui%20Tong%20and%20Xia%20Du%20and%20Ahmed%20Alhammadi%20and%20Jizhe%20Zhou%0AAbstract%3A%20%20%20The%20Privacy-sensitive%20Object%20Identification%20%28POI%29%20task%20allocates%20bounding%0Aboxes%20for%20privacy-sensitive%20objects%20in%20a%20scene.%20The%20key%20to%20POI%20is%20settling%20an%0Aobject%27s%20privacy%20class%20%28privacy-sensitive%20or%20non-privacy-sensitive%29.%20In%0Acontrast%20to%20conventional%20object%20classes%20which%20are%20determined%20by%20the%20visual%0Aappearance%20of%20an%20object%2C%20one%20object%27s%20privacy%20class%20is%20derived%20from%20the%20scene%0Acontexts%20and%20is%20subject%20to%20various%20implicit%20factors%20beyond%20its%20visual%0Aappearance.%20That%20is%2C%20visually%20similar%20objects%20may%20be%20totally%20opposite%20in%20their%0Aprivacy%20classes.%20To%20explicitly%20derive%20the%20objects%27%20privacy%20class%20from%20the%20scene%0Acontexts%2C%20in%20this%20paper%2C%20we%20interpret%20the%20POI%20task%20as%20a%20visual%20reasoning%20task%0Aaimed%20at%20the%20privacy%20of%20each%20object%20in%20the%20scene.%20Following%20this%0Ainterpretation%2C%20we%20propose%20the%20PrivacyGuard%20framework%20for%20POI.%20PrivacyGuard%0Acontains%20three%20stages.%20i%29%20Structuring%3A%20an%20unstructured%20image%20is%20first%20converted%0Ainto%20a%20structured%2C%20heterogeneous%20scene%20graph%20that%20embeds%20rich%20scene%20contexts.%0Aii%29%20Data%20Augmentation%3A%20a%20contextual%20perturbation%20oversampling%20strategy%20is%0Aproposed%20to%20create%20slightly%20perturbed%20privacy-sensitive%20objects%20in%20a%20scene%0Agraph%2C%20thereby%20balancing%20the%20skewed%20distribution%20of%20privacy%20classes.%20iii%29%0AHybrid%20Graph%20Generation%20%26%20Reasoning%3A%20the%20balanced%2C%20heterogeneous%20scene%20graph%20is%0Athen%20transformed%20into%20a%20hybrid%20graph%20by%20endowing%20it%20with%20extra%20%22node-node%22%20and%0A%22edge-edge%22%20homogeneous%20paths.%20These%20homogeneous%20paths%20allow%20direct%20message%0Apassing%20between%20nodes%20or%20edges%2C%20thereby%20accelerating%20reasoning%20and%20facilitating%0Athe%20capturing%20of%20subtle%20context%20changes.%20Based%20on%20this%20hybrid%20graph...%20%2A%2AFor%0Athe%20full%20abstract%2C%20see%20the%20original%20paper.%2A%2A%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Visual%2520Appearances%253A%2520Privacy-sensitive%2520Objects%2520Identification%2520via%250A%2520%2520Hybrid%2520Graph%2520Reasoning%26entry.906535625%3DZhuohang%2520Jiang%2520and%2520Bingkui%2520Tong%2520and%2520Xia%2520Du%2520and%2520Ahmed%2520Alhammadi%2520and%2520Jizhe%2520Zhou%26entry.1292438233%3D%2520%2520The%2520Privacy-sensitive%2520Object%2520Identification%2520%2528POI%2529%2520task%2520allocates%2520bounding%250Aboxes%2520for%2520privacy-sensitive%2520objects%2520in%2520a%2520scene.%2520The%2520key%2520to%2520POI%2520is%2520settling%2520an%250Aobject%2527s%2520privacy%2520class%2520%2528privacy-sensitive%2520or%2520non-privacy-sensitive%2529.%2520In%250Acontrast%2520to%2520conventional%2520object%2520classes%2520which%2520are%2520determined%2520by%2520the%2520visual%250Aappearance%2520of%2520an%2520object%252C%2520one%2520object%2527s%2520privacy%2520class%2520is%2520derived%2520from%2520the%2520scene%250Acontexts%2520and%2520is%2520subject%2520to%2520various%2520implicit%2520factors%2520beyond%2520its%2520visual%250Aappearance.%2520That%2520is%252C%2520visually%2520similar%2520objects%2520may%2520be%2520totally%2520opposite%2520in%2520their%250Aprivacy%2520classes.%2520To%2520explicitly%2520derive%2520the%2520objects%2527%2520privacy%2520class%2520from%2520the%2520scene%250Acontexts%252C%2520in%2520this%2520paper%252C%2520we%2520interpret%2520the%2520POI%2520task%2520as%2520a%2520visual%2520reasoning%2520task%250Aaimed%2520at%2520the%2520privacy%2520of%2520each%2520object%2520in%2520the%2520scene.%2520Following%2520this%250Ainterpretation%252C%2520we%2520propose%2520the%2520PrivacyGuard%2520framework%2520for%2520POI.%2520PrivacyGuard%250Acontains%2520three%2520stages.%2520i%2529%2520Structuring%253A%2520an%2520unstructured%2520image%2520is%2520first%2520converted%250Ainto%2520a%2520structured%252C%2520heterogeneous%2520scene%2520graph%2520that%2520embeds%2520rich%2520scene%2520contexts.%250Aii%2529%2520Data%2520Augmentation%253A%2520a%2520contextual%2520perturbation%2520oversampling%2520strategy%2520is%250Aproposed%2520to%2520create%2520slightly%2520perturbed%2520privacy-sensitive%2520objects%2520in%2520a%2520scene%250Agraph%252C%2520thereby%2520balancing%2520the%2520skewed%2520distribution%2520of%2520privacy%2520classes.%2520iii%2529%250AHybrid%2520Graph%2520Generation%2520%2526%2520Reasoning%253A%2520the%2520balanced%252C%2520heterogeneous%2520scene%2520graph%2520is%250Athen%2520transformed%2520into%2520a%2520hybrid%2520graph%2520by%2520endowing%2520it%2520with%2520extra%2520%2522node-node%2522%2520and%250A%2522edge-edge%2522%2520homogeneous%2520paths.%2520These%2520homogeneous%2520paths%2520allow%2520direct%2520message%250Apassing%2520between%2520nodes%2520or%2520edges%252C%2520thereby%2520accelerating%2520reasoning%2520and%2520facilitating%250Athe%2520capturing%2520of%2520subtle%2520context%2520changes.%2520Based%2520on%2520this%2520hybrid%2520graph...%2520%252A%252AFor%250Athe%2520full%2520abstract%252C%2520see%2520the%2520original%2520paper.%252A%252A%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Visual%20Appearances%3A%20Privacy-sensitive%20Objects%20Identification%20via%0A%20%20Hybrid%20Graph%20Reasoning&entry.906535625=Zhuohang%20Jiang%20and%20Bingkui%20Tong%20and%20Xia%20Du%20and%20Ahmed%20Alhammadi%20and%20Jizhe%20Zhou&entry.1292438233=%20%20The%20Privacy-sensitive%20Object%20Identification%20%28POI%29%20task%20allocates%20bounding%0Aboxes%20for%20privacy-sensitive%20objects%20in%20a%20scene.%20The%20key%20to%20POI%20is%20settling%20an%0Aobject%27s%20privacy%20class%20%28privacy-sensitive%20or%20non-privacy-sensitive%29.%20In%0Acontrast%20to%20conventional%20object%20classes%20which%20are%20determined%20by%20the%20visual%0Aappearance%20of%20an%20object%2C%20one%20object%27s%20privacy%20class%20is%20derived%20from%20the%20scene%0Acontexts%20and%20is%20subject%20to%20various%20implicit%20factors%20beyond%20its%20visual%0Aappearance.%20That%20is%2C%20visually%20similar%20objects%20may%20be%20totally%20opposite%20in%20their%0Aprivacy%20classes.%20To%20explicitly%20derive%20the%20objects%27%20privacy%20class%20from%20the%20scene%0Acontexts%2C%20in%20this%20paper%2C%20we%20interpret%20the%20POI%20task%20as%20a%20visual%20reasoning%20task%0Aaimed%20at%20the%20privacy%20of%20each%20object%20in%20the%20scene.%20Following%20this%0Ainterpretation%2C%20we%20propose%20the%20PrivacyGuard%20framework%20for%20POI.%20PrivacyGuard%0Acontains%20three%20stages.%20i%29%20Structuring%3A%20an%20unstructured%20image%20is%20first%20converted%0Ainto%20a%20structured%2C%20heterogeneous%20scene%20graph%20that%20embeds%20rich%20scene%20contexts.%0Aii%29%20Data%20Augmentation%3A%20a%20contextual%20perturbation%20oversampling%20strategy%20is%0Aproposed%20to%20create%20slightly%20perturbed%20privacy-sensitive%20objects%20in%20a%20scene%0Agraph%2C%20thereby%20balancing%20the%20skewed%20distribution%20of%20privacy%20classes.%20iii%29%0AHybrid%20Graph%20Generation%20%26%20Reasoning%3A%20the%20balanced%2C%20heterogeneous%20scene%20graph%20is%0Athen%20transformed%20into%20a%20hybrid%20graph%20by%20endowing%20it%20with%20extra%20%22node-node%22%20and%0A%22edge-edge%22%20homogeneous%20paths.%20These%20homogeneous%20paths%20allow%20direct%20message%0Apassing%20between%20nodes%20or%20edges%2C%20thereby%20accelerating%20reasoning%20and%20facilitating%0Athe%20capturing%20of%20subtle%20context%20changes.%20Based%20on%20this%20hybrid%20graph...%20%2A%2AFor%0Athe%20full%20abstract%2C%20see%20the%20original%20paper.%2A%2A%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12736v1&entry.124074799=Read"},
{"title": "Leveraging Generative Models for Covert Messaging: Challenges and\n  Tradeoffs for \"Dead-Drop\" Deployments", "author": "Luke A. Bauer and James K. Howes IV and Sam A. Markelon and Vincent Bindschaedler and Thomas Shrimpton", "abstract": "  State of the art generative models of human-produced content are the focus of\nmany recent papers that explore their use for steganographic communication. In\nparticular, generative models of natural language text. Loosely, these works\n(invertibly) encode message-carrying bits into a sequence of samples from the\nmodel, ultimately yielding a plausible natural language covertext. By focusing\non this narrow steganographic piece, prior work has largely ignored the\nsignificant algorithmic challenges, and performance-security tradeoffs, that\narise when one actually tries to build a messaging pipeline around it. We make\nthese challenges concrete, by considering the natural application of such a\npipeline: namely, \"dead-drop\" covert messaging over large, public internet\nplatforms (e.g. social media sites). We explicate the challenges and describe\napproaches to overcome them, surfacing in the process important performance and\nsecurity tradeoffs that must be carefully tuned. We implement a system around\nthis model-based format-transforming encryption pipeline, and give an empirical\nanalysis of its performance and (heuristic) security.\n", "link": "http://arxiv.org/abs/2110.07009v3", "date": "2024-06-18", "relevancy": 1.9978, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5113}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5074}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Generative%20Models%20for%20Covert%20Messaging%3A%20Challenges%20and%0A%20%20Tradeoffs%20for%20%22Dead-Drop%22%20Deployments&body=Title%3A%20Leveraging%20Generative%20Models%20for%20Covert%20Messaging%3A%20Challenges%20and%0A%20%20Tradeoffs%20for%20%22Dead-Drop%22%20Deployments%0AAuthor%3A%20Luke%20A.%20Bauer%20and%20James%20K.%20Howes%20IV%20and%20Sam%20A.%20Markelon%20and%20Vincent%20Bindschaedler%20and%20Thomas%20Shrimpton%0AAbstract%3A%20%20%20State%20of%20the%20art%20generative%20models%20of%20human-produced%20content%20are%20the%20focus%20of%0Amany%20recent%20papers%20that%20explore%20their%20use%20for%20steganographic%20communication.%20In%0Aparticular%2C%20generative%20models%20of%20natural%20language%20text.%20Loosely%2C%20these%20works%0A%28invertibly%29%20encode%20message-carrying%20bits%20into%20a%20sequence%20of%20samples%20from%20the%0Amodel%2C%20ultimately%20yielding%20a%20plausible%20natural%20language%20covertext.%20By%20focusing%0Aon%20this%20narrow%20steganographic%20piece%2C%20prior%20work%20has%20largely%20ignored%20the%0Asignificant%20algorithmic%20challenges%2C%20and%20performance-security%20tradeoffs%2C%20that%0Aarise%20when%20one%20actually%20tries%20to%20build%20a%20messaging%20pipeline%20around%20it.%20We%20make%0Athese%20challenges%20concrete%2C%20by%20considering%20the%20natural%20application%20of%20such%20a%0Apipeline%3A%20namely%2C%20%22dead-drop%22%20covert%20messaging%20over%20large%2C%20public%20internet%0Aplatforms%20%28e.g.%20social%20media%20sites%29.%20We%20explicate%20the%20challenges%20and%20describe%0Aapproaches%20to%20overcome%20them%2C%20surfacing%20in%20the%20process%20important%20performance%20and%0Asecurity%20tradeoffs%20that%20must%20be%20carefully%20tuned.%20We%20implement%20a%20system%20around%0Athis%20model-based%20format-transforming%20encryption%20pipeline%2C%20and%20give%20an%20empirical%0Aanalysis%20of%20its%20performance%20and%20%28heuristic%29%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2110.07009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Generative%2520Models%2520for%2520Covert%2520Messaging%253A%2520Challenges%2520and%250A%2520%2520Tradeoffs%2520for%2520%2522Dead-Drop%2522%2520Deployments%26entry.906535625%3DLuke%2520A.%2520Bauer%2520and%2520James%2520K.%2520Howes%2520IV%2520and%2520Sam%2520A.%2520Markelon%2520and%2520Vincent%2520Bindschaedler%2520and%2520Thomas%2520Shrimpton%26entry.1292438233%3D%2520%2520State%2520of%2520the%2520art%2520generative%2520models%2520of%2520human-produced%2520content%2520are%2520the%2520focus%2520of%250Amany%2520recent%2520papers%2520that%2520explore%2520their%2520use%2520for%2520steganographic%2520communication.%2520In%250Aparticular%252C%2520generative%2520models%2520of%2520natural%2520language%2520text.%2520Loosely%252C%2520these%2520works%250A%2528invertibly%2529%2520encode%2520message-carrying%2520bits%2520into%2520a%2520sequence%2520of%2520samples%2520from%2520the%250Amodel%252C%2520ultimately%2520yielding%2520a%2520plausible%2520natural%2520language%2520covertext.%2520By%2520focusing%250Aon%2520this%2520narrow%2520steganographic%2520piece%252C%2520prior%2520work%2520has%2520largely%2520ignored%2520the%250Asignificant%2520algorithmic%2520challenges%252C%2520and%2520performance-security%2520tradeoffs%252C%2520that%250Aarise%2520when%2520one%2520actually%2520tries%2520to%2520build%2520a%2520messaging%2520pipeline%2520around%2520it.%2520We%2520make%250Athese%2520challenges%2520concrete%252C%2520by%2520considering%2520the%2520natural%2520application%2520of%2520such%2520a%250Apipeline%253A%2520namely%252C%2520%2522dead-drop%2522%2520covert%2520messaging%2520over%2520large%252C%2520public%2520internet%250Aplatforms%2520%2528e.g.%2520social%2520media%2520sites%2529.%2520We%2520explicate%2520the%2520challenges%2520and%2520describe%250Aapproaches%2520to%2520overcome%2520them%252C%2520surfacing%2520in%2520the%2520process%2520important%2520performance%2520and%250Asecurity%2520tradeoffs%2520that%2520must%2520be%2520carefully%2520tuned.%2520We%2520implement%2520a%2520system%2520around%250Athis%2520model-based%2520format-transforming%2520encryption%2520pipeline%252C%2520and%2520give%2520an%2520empirical%250Aanalysis%2520of%2520its%2520performance%2520and%2520%2528heuristic%2529%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2110.07009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Generative%20Models%20for%20Covert%20Messaging%3A%20Challenges%20and%0A%20%20Tradeoffs%20for%20%22Dead-Drop%22%20Deployments&entry.906535625=Luke%20A.%20Bauer%20and%20James%20K.%20Howes%20IV%20and%20Sam%20A.%20Markelon%20and%20Vincent%20Bindschaedler%20and%20Thomas%20Shrimpton&entry.1292438233=%20%20State%20of%20the%20art%20generative%20models%20of%20human-produced%20content%20are%20the%20focus%20of%0Amany%20recent%20papers%20that%20explore%20their%20use%20for%20steganographic%20communication.%20In%0Aparticular%2C%20generative%20models%20of%20natural%20language%20text.%20Loosely%2C%20these%20works%0A%28invertibly%29%20encode%20message-carrying%20bits%20into%20a%20sequence%20of%20samples%20from%20the%0Amodel%2C%20ultimately%20yielding%20a%20plausible%20natural%20language%20covertext.%20By%20focusing%0Aon%20this%20narrow%20steganographic%20piece%2C%20prior%20work%20has%20largely%20ignored%20the%0Asignificant%20algorithmic%20challenges%2C%20and%20performance-security%20tradeoffs%2C%20that%0Aarise%20when%20one%20actually%20tries%20to%20build%20a%20messaging%20pipeline%20around%20it.%20We%20make%0Athese%20challenges%20concrete%2C%20by%20considering%20the%20natural%20application%20of%20such%20a%0Apipeline%3A%20namely%2C%20%22dead-drop%22%20covert%20messaging%20over%20large%2C%20public%20internet%0Aplatforms%20%28e.g.%20social%20media%20sites%29.%20We%20explicate%20the%20challenges%20and%20describe%0Aapproaches%20to%20overcome%20them%2C%20surfacing%20in%20the%20process%20important%20performance%20and%0Asecurity%20tradeoffs%20that%20must%20be%20carefully%20tuned.%20We%20implement%20a%20system%20around%0Athis%20model-based%20format-transforming%20encryption%20pipeline%2C%20and%20give%20an%20empirical%0Aanalysis%20of%20its%20performance%20and%20%28heuristic%29%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2110.07009v3&entry.124074799=Read"},
{"title": "Bridging Local Details and Global Context in Text-Attributed Graphs", "author": "Yaoke Wang and Yun Zhu and Wenqiao Zhang and Yueting Zhuang and Yunfei Li and Siliang Tang", "abstract": "  Representation learning on text-attributed graphs (TAGs) is vital for\nreal-world applications, as they combine semantic textual and contextual\nstructural information. Research in this field generally consist of two main\nperspectives: local-level encoding and global-level aggregating, respectively\nrefer to textual node information unification (e.g., using Language Models) and\nstructure-augmented modeling (e.g., using Graph Neural Networks). Most existing\nworks focus on combining different information levels but overlook the\ninterconnections, i.e., the contextual textual information among nodes, which\nprovides semantic insights to bridge local and global levels. In this paper, we\npropose GraphBridge, a multi-granularity integration framework that bridges\nlocal and global perspectives by leveraging contextual textual information,\nenhancing fine-grained understanding of TAGs. Besides, to tackle scalability\nand efficiency challenges, we introduce a graphaware token reduction module.\nExtensive experiments across various models and datasets show that our method\nachieves state-of-theart performance, while our graph-aware token reduction\nmodule significantly enhances efficiency and solves scalability issues.\n", "link": "http://arxiv.org/abs/2406.12608v1", "date": "2024-06-18", "relevancy": 1.9943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5135}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.488}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Local%20Details%20and%20Global%20Context%20in%20Text-Attributed%20Graphs&body=Title%3A%20Bridging%20Local%20Details%20and%20Global%20Context%20in%20Text-Attributed%20Graphs%0AAuthor%3A%20Yaoke%20Wang%20and%20Yun%20Zhu%20and%20Wenqiao%20Zhang%20and%20Yueting%20Zhuang%20and%20Yunfei%20Li%20and%20Siliang%20Tang%0AAbstract%3A%20%20%20Representation%20learning%20on%20text-attributed%20graphs%20%28TAGs%29%20is%20vital%20for%0Areal-world%20applications%2C%20as%20they%20combine%20semantic%20textual%20and%20contextual%0Astructural%20information.%20Research%20in%20this%20field%20generally%20consist%20of%20two%20main%0Aperspectives%3A%20local-level%20encoding%20and%20global-level%20aggregating%2C%20respectively%0Arefer%20to%20textual%20node%20information%20unification%20%28e.g.%2C%20using%20Language%20Models%29%20and%0Astructure-augmented%20modeling%20%28e.g.%2C%20using%20Graph%20Neural%20Networks%29.%20Most%20existing%0Aworks%20focus%20on%20combining%20different%20information%20levels%20but%20overlook%20the%0Ainterconnections%2C%20i.e.%2C%20the%20contextual%20textual%20information%20among%20nodes%2C%20which%0Aprovides%20semantic%20insights%20to%20bridge%20local%20and%20global%20levels.%20In%20this%20paper%2C%20we%0Apropose%20GraphBridge%2C%20a%20multi-granularity%20integration%20framework%20that%20bridges%0Alocal%20and%20global%20perspectives%20by%20leveraging%20contextual%20textual%20information%2C%0Aenhancing%20fine-grained%20understanding%20of%20TAGs.%20Besides%2C%20to%20tackle%20scalability%0Aand%20efficiency%20challenges%2C%20we%20introduce%20a%20graphaware%20token%20reduction%20module.%0AExtensive%20experiments%20across%20various%20models%20and%20datasets%20show%20that%20our%20method%0Aachieves%20state-of-theart%20performance%2C%20while%20our%20graph-aware%20token%20reduction%0Amodule%20significantly%20enhances%20efficiency%20and%20solves%20scalability%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Local%2520Details%2520and%2520Global%2520Context%2520in%2520Text-Attributed%2520Graphs%26entry.906535625%3DYaoke%2520Wang%2520and%2520Yun%2520Zhu%2520and%2520Wenqiao%2520Zhang%2520and%2520Yueting%2520Zhuang%2520and%2520Yunfei%2520Li%2520and%2520Siliang%2520Tang%26entry.1292438233%3D%2520%2520Representation%2520learning%2520on%2520text-attributed%2520graphs%2520%2528TAGs%2529%2520is%2520vital%2520for%250Areal-world%2520applications%252C%2520as%2520they%2520combine%2520semantic%2520textual%2520and%2520contextual%250Astructural%2520information.%2520Research%2520in%2520this%2520field%2520generally%2520consist%2520of%2520two%2520main%250Aperspectives%253A%2520local-level%2520encoding%2520and%2520global-level%2520aggregating%252C%2520respectively%250Arefer%2520to%2520textual%2520node%2520information%2520unification%2520%2528e.g.%252C%2520using%2520Language%2520Models%2529%2520and%250Astructure-augmented%2520modeling%2520%2528e.g.%252C%2520using%2520Graph%2520Neural%2520Networks%2529.%2520Most%2520existing%250Aworks%2520focus%2520on%2520combining%2520different%2520information%2520levels%2520but%2520overlook%2520the%250Ainterconnections%252C%2520i.e.%252C%2520the%2520contextual%2520textual%2520information%2520among%2520nodes%252C%2520which%250Aprovides%2520semantic%2520insights%2520to%2520bridge%2520local%2520and%2520global%2520levels.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520GraphBridge%252C%2520a%2520multi-granularity%2520integration%2520framework%2520that%2520bridges%250Alocal%2520and%2520global%2520perspectives%2520by%2520leveraging%2520contextual%2520textual%2520information%252C%250Aenhancing%2520fine-grained%2520understanding%2520of%2520TAGs.%2520Besides%252C%2520to%2520tackle%2520scalability%250Aand%2520efficiency%2520challenges%252C%2520we%2520introduce%2520a%2520graphaware%2520token%2520reduction%2520module.%250AExtensive%2520experiments%2520across%2520various%2520models%2520and%2520datasets%2520show%2520that%2520our%2520method%250Aachieves%2520state-of-theart%2520performance%252C%2520while%2520our%2520graph-aware%2520token%2520reduction%250Amodule%2520significantly%2520enhances%2520efficiency%2520and%2520solves%2520scalability%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Local%20Details%20and%20Global%20Context%20in%20Text-Attributed%20Graphs&entry.906535625=Yaoke%20Wang%20and%20Yun%20Zhu%20and%20Wenqiao%20Zhang%20and%20Yueting%20Zhuang%20and%20Yunfei%20Li%20and%20Siliang%20Tang&entry.1292438233=%20%20Representation%20learning%20on%20text-attributed%20graphs%20%28TAGs%29%20is%20vital%20for%0Areal-world%20applications%2C%20as%20they%20combine%20semantic%20textual%20and%20contextual%0Astructural%20information.%20Research%20in%20this%20field%20generally%20consist%20of%20two%20main%0Aperspectives%3A%20local-level%20encoding%20and%20global-level%20aggregating%2C%20respectively%0Arefer%20to%20textual%20node%20information%20unification%20%28e.g.%2C%20using%20Language%20Models%29%20and%0Astructure-augmented%20modeling%20%28e.g.%2C%20using%20Graph%20Neural%20Networks%29.%20Most%20existing%0Aworks%20focus%20on%20combining%20different%20information%20levels%20but%20overlook%20the%0Ainterconnections%2C%20i.e.%2C%20the%20contextual%20textual%20information%20among%20nodes%2C%20which%0Aprovides%20semantic%20insights%20to%20bridge%20local%20and%20global%20levels.%20In%20this%20paper%2C%20we%0Apropose%20GraphBridge%2C%20a%20multi-granularity%20integration%20framework%20that%20bridges%0Alocal%20and%20global%20perspectives%20by%20leveraging%20contextual%20textual%20information%2C%0Aenhancing%20fine-grained%20understanding%20of%20TAGs.%20Besides%2C%20to%20tackle%20scalability%0Aand%20efficiency%20challenges%2C%20we%20introduce%20a%20graphaware%20token%20reduction%20module.%0AExtensive%20experiments%20across%20various%20models%20and%20datasets%20show%20that%20our%20method%0Aachieves%20state-of-theart%20performance%2C%20while%20our%20graph-aware%20token%20reduction%0Amodule%20significantly%20enhances%20efficiency%20and%20solves%20scalability%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12608v1&entry.124074799=Read"},
{"title": "DataComp-LM: In search of the next generation of training sets for\n  language models", "author": "Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar", "abstract": "  We introduce DataComp for Language Models (DCLM), a testbed for controlled\ndataset experiments with the goal of improving language models. As part of\nDCLM, we provide a standardized corpus of 240T tokens extracted from Common\nCrawl, effective pretraining recipes based on the OpenLM framework, and a broad\nsuite of 53 downstream evaluations. Participants in the DCLM benchmark can\nexperiment with data curation strategies such as deduplication, filtering, and\ndata mixing at model scales ranging from 412M to 7B parameters. As a baseline\nfor DCLM, we conduct extensive experiments and find that model-based filtering\nis key to assembling a high-quality training set. The resulting dataset,\nDCLM-Baseline enables training a 7B parameter language model from scratch to\n64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the\nprevious state-of-the-art in open-data language models, DCLM-Baseline\nrepresents a 6.6 percentage point improvement on MMLU while being trained with\n40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and\nLlama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53\nnatural language understanding tasks while being trained with 6.6x less compute\nthan Llama 3 8B. Our results highlight the importance of dataset design for\ntraining language models and offer a starting point for further research on\ndata curation.\n", "link": "http://arxiv.org/abs/2406.11794v2", "date": "2024-06-18", "relevancy": 1.9928, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5259}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4795}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataComp-LM%3A%20In%20search%20of%20the%20next%20generation%20of%20training%20sets%20for%0A%20%20language%20models&body=Title%3A%20DataComp-LM%3A%20In%20search%20of%20the%20next%20generation%20of%20training%20sets%20for%0A%20%20language%20models%0AAuthor%3A%20Jeffrey%20Li%20and%20Alex%20Fang%20and%20Georgios%20Smyrnis%20and%20Maor%20Ivgi%20and%20Matt%20Jordan%20and%20Samir%20Gadre%20and%20Hritik%20Bansal%20and%20Etash%20Guha%20and%20Sedrick%20Keh%20and%20Kushal%20Arora%20and%20Saurabh%20Garg%20and%20Rui%20Xin%20and%20Niklas%20Muennighoff%20and%20Reinhard%20Heckel%20and%20Jean%20Mercat%20and%20Mayee%20Chen%20and%20Suchin%20Gururangan%20and%20Mitchell%20Wortsman%20and%20Alon%20Albalak%20and%20Yonatan%20Bitton%20and%20Marianna%20Nezhurina%20and%20Amro%20Abbas%20and%20Cheng-Yu%20Hsieh%20and%20Dhruba%20Ghosh%20and%20Josh%20Gardner%20and%20Maciej%20Kilian%20and%20Hanlin%20Zhang%20and%20Rulin%20Shao%20and%20Sarah%20Pratt%20and%20Sunny%20Sanyal%20and%20Gabriel%20Ilharco%20and%20Giannis%20Daras%20and%20Kalyani%20Marathe%20and%20Aaron%20Gokaslan%20and%20Jieyu%20Zhang%20and%20Khyathi%20Chandu%20and%20Thao%20Nguyen%20and%20Igor%20Vasiljevic%20and%20Sham%20Kakade%20and%20Shuran%20Song%20and%20Sujay%20Sanghavi%20and%20Fartash%20Faghri%20and%20Sewoong%20Oh%20and%20Luke%20Zettlemoyer%20and%20Kyle%20Lo%20and%20Alaaeldin%20El-Nouby%20and%20Hadi%20Pouransari%20and%20Alexander%20Toshev%20and%20Stephanie%20Wang%20and%20Dirk%20Groeneveld%20and%20Luca%20Soldaini%20and%20Pang%20Wei%20Koh%20and%20Jenia%20Jitsev%20and%20Thomas%20Kollar%20and%20Alexandros%20G.%20Dimakis%20and%20Yair%20Carmon%20and%20Achal%20Dave%20and%20Ludwig%20Schmidt%20and%20Vaishaal%20Shankar%0AAbstract%3A%20%20%20We%20introduce%20DataComp%20for%20Language%20Models%20%28DCLM%29%2C%20a%20testbed%20for%20controlled%0Adataset%20experiments%20with%20the%20goal%20of%20improving%20language%20models.%20As%20part%20of%0ADCLM%2C%20we%20provide%20a%20standardized%20corpus%20of%20240T%20tokens%20extracted%20from%20Common%0ACrawl%2C%20effective%20pretraining%20recipes%20based%20on%20the%20OpenLM%20framework%2C%20and%20a%20broad%0Asuite%20of%2053%20downstream%20evaluations.%20Participants%20in%20the%20DCLM%20benchmark%20can%0Aexperiment%20with%20data%20curation%20strategies%20such%20as%20deduplication%2C%20filtering%2C%20and%0Adata%20mixing%20at%20model%20scales%20ranging%20from%20412M%20to%207B%20parameters.%20As%20a%20baseline%0Afor%20DCLM%2C%20we%20conduct%20extensive%20experiments%20and%20find%20that%20model-based%20filtering%0Ais%20key%20to%20assembling%20a%20high-quality%20training%20set.%20The%20resulting%20dataset%2C%0ADCLM-Baseline%20enables%20training%20a%207B%20parameter%20language%20model%20from%20scratch%20to%0A64%25%205-shot%20accuracy%20on%20MMLU%20with%202.6T%20training%20tokens.%20Compared%20to%20MAP-Neo%2C%20the%0Aprevious%20state-of-the-art%20in%20open-data%20language%20models%2C%20DCLM-Baseline%0Arepresents%20a%206.6%20percentage%20point%20improvement%20on%20MMLU%20while%20being%20trained%20with%0A40%25%20less%20compute.%20Our%20baseline%20model%20is%20also%20comparable%20to%20Mistral-7B-v0.3%20and%0ALlama%203%208B%20on%20MMLU%20%2863%25%20%26%2066%25%29%2C%20and%20performs%20similarly%20on%20an%20average%20of%2053%0Anatural%20language%20understanding%20tasks%20while%20being%20trained%20with%206.6x%20less%20compute%0Athan%20Llama%203%208B.%20Our%20results%20highlight%20the%20importance%20of%20dataset%20design%20for%0Atraining%20language%20models%20and%20offer%20a%20starting%20point%20for%20further%20research%20on%0Adata%20curation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataComp-LM%253A%2520In%2520search%2520of%2520the%2520next%2520generation%2520of%2520training%2520sets%2520for%250A%2520%2520language%2520models%26entry.906535625%3DJeffrey%2520Li%2520and%2520Alex%2520Fang%2520and%2520Georgios%2520Smyrnis%2520and%2520Maor%2520Ivgi%2520and%2520Matt%2520Jordan%2520and%2520Samir%2520Gadre%2520and%2520Hritik%2520Bansal%2520and%2520Etash%2520Guha%2520and%2520Sedrick%2520Keh%2520and%2520Kushal%2520Arora%2520and%2520Saurabh%2520Garg%2520and%2520Rui%2520Xin%2520and%2520Niklas%2520Muennighoff%2520and%2520Reinhard%2520Heckel%2520and%2520Jean%2520Mercat%2520and%2520Mayee%2520Chen%2520and%2520Suchin%2520Gururangan%2520and%2520Mitchell%2520Wortsman%2520and%2520Alon%2520Albalak%2520and%2520Yonatan%2520Bitton%2520and%2520Marianna%2520Nezhurina%2520and%2520Amro%2520Abbas%2520and%2520Cheng-Yu%2520Hsieh%2520and%2520Dhruba%2520Ghosh%2520and%2520Josh%2520Gardner%2520and%2520Maciej%2520Kilian%2520and%2520Hanlin%2520Zhang%2520and%2520Rulin%2520Shao%2520and%2520Sarah%2520Pratt%2520and%2520Sunny%2520Sanyal%2520and%2520Gabriel%2520Ilharco%2520and%2520Giannis%2520Daras%2520and%2520Kalyani%2520Marathe%2520and%2520Aaron%2520Gokaslan%2520and%2520Jieyu%2520Zhang%2520and%2520Khyathi%2520Chandu%2520and%2520Thao%2520Nguyen%2520and%2520Igor%2520Vasiljevic%2520and%2520Sham%2520Kakade%2520and%2520Shuran%2520Song%2520and%2520Sujay%2520Sanghavi%2520and%2520Fartash%2520Faghri%2520and%2520Sewoong%2520Oh%2520and%2520Luke%2520Zettlemoyer%2520and%2520Kyle%2520Lo%2520and%2520Alaaeldin%2520El-Nouby%2520and%2520Hadi%2520Pouransari%2520and%2520Alexander%2520Toshev%2520and%2520Stephanie%2520Wang%2520and%2520Dirk%2520Groeneveld%2520and%2520Luca%2520Soldaini%2520and%2520Pang%2520Wei%2520Koh%2520and%2520Jenia%2520Jitsev%2520and%2520Thomas%2520Kollar%2520and%2520Alexandros%2520G.%2520Dimakis%2520and%2520Yair%2520Carmon%2520and%2520Achal%2520Dave%2520and%2520Ludwig%2520Schmidt%2520and%2520Vaishaal%2520Shankar%26entry.1292438233%3D%2520%2520We%2520introduce%2520DataComp%2520for%2520Language%2520Models%2520%2528DCLM%2529%252C%2520a%2520testbed%2520for%2520controlled%250Adataset%2520experiments%2520with%2520the%2520goal%2520of%2520improving%2520language%2520models.%2520As%2520part%2520of%250ADCLM%252C%2520we%2520provide%2520a%2520standardized%2520corpus%2520of%2520240T%2520tokens%2520extracted%2520from%2520Common%250ACrawl%252C%2520effective%2520pretraining%2520recipes%2520based%2520on%2520the%2520OpenLM%2520framework%252C%2520and%2520a%2520broad%250Asuite%2520of%252053%2520downstream%2520evaluations.%2520Participants%2520in%2520the%2520DCLM%2520benchmark%2520can%250Aexperiment%2520with%2520data%2520curation%2520strategies%2520such%2520as%2520deduplication%252C%2520filtering%252C%2520and%250Adata%2520mixing%2520at%2520model%2520scales%2520ranging%2520from%2520412M%2520to%25207B%2520parameters.%2520As%2520a%2520baseline%250Afor%2520DCLM%252C%2520we%2520conduct%2520extensive%2520experiments%2520and%2520find%2520that%2520model-based%2520filtering%250Ais%2520key%2520to%2520assembling%2520a%2520high-quality%2520training%2520set.%2520The%2520resulting%2520dataset%252C%250ADCLM-Baseline%2520enables%2520training%2520a%25207B%2520parameter%2520language%2520model%2520from%2520scratch%2520to%250A64%2525%25205-shot%2520accuracy%2520on%2520MMLU%2520with%25202.6T%2520training%2520tokens.%2520Compared%2520to%2520MAP-Neo%252C%2520the%250Aprevious%2520state-of-the-art%2520in%2520open-data%2520language%2520models%252C%2520DCLM-Baseline%250Arepresents%2520a%25206.6%2520percentage%2520point%2520improvement%2520on%2520MMLU%2520while%2520being%2520trained%2520with%250A40%2525%2520less%2520compute.%2520Our%2520baseline%2520model%2520is%2520also%2520comparable%2520to%2520Mistral-7B-v0.3%2520and%250ALlama%25203%25208B%2520on%2520MMLU%2520%252863%2525%2520%2526%252066%2525%2529%252C%2520and%2520performs%2520similarly%2520on%2520an%2520average%2520of%252053%250Anatural%2520language%2520understanding%2520tasks%2520while%2520being%2520trained%2520with%25206.6x%2520less%2520compute%250Athan%2520Llama%25203%25208B.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520dataset%2520design%2520for%250Atraining%2520language%2520models%2520and%2520offer%2520a%2520starting%2520point%2520for%2520further%2520research%2520on%250Adata%2520curation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataComp-LM%3A%20In%20search%20of%20the%20next%20generation%20of%20training%20sets%20for%0A%20%20language%20models&entry.906535625=Jeffrey%20Li%20and%20Alex%20Fang%20and%20Georgios%20Smyrnis%20and%20Maor%20Ivgi%20and%20Matt%20Jordan%20and%20Samir%20Gadre%20and%20Hritik%20Bansal%20and%20Etash%20Guha%20and%20Sedrick%20Keh%20and%20Kushal%20Arora%20and%20Saurabh%20Garg%20and%20Rui%20Xin%20and%20Niklas%20Muennighoff%20and%20Reinhard%20Heckel%20and%20Jean%20Mercat%20and%20Mayee%20Chen%20and%20Suchin%20Gururangan%20and%20Mitchell%20Wortsman%20and%20Alon%20Albalak%20and%20Yonatan%20Bitton%20and%20Marianna%20Nezhurina%20and%20Amro%20Abbas%20and%20Cheng-Yu%20Hsieh%20and%20Dhruba%20Ghosh%20and%20Josh%20Gardner%20and%20Maciej%20Kilian%20and%20Hanlin%20Zhang%20and%20Rulin%20Shao%20and%20Sarah%20Pratt%20and%20Sunny%20Sanyal%20and%20Gabriel%20Ilharco%20and%20Giannis%20Daras%20and%20Kalyani%20Marathe%20and%20Aaron%20Gokaslan%20and%20Jieyu%20Zhang%20and%20Khyathi%20Chandu%20and%20Thao%20Nguyen%20and%20Igor%20Vasiljevic%20and%20Sham%20Kakade%20and%20Shuran%20Song%20and%20Sujay%20Sanghavi%20and%20Fartash%20Faghri%20and%20Sewoong%20Oh%20and%20Luke%20Zettlemoyer%20and%20Kyle%20Lo%20and%20Alaaeldin%20El-Nouby%20and%20Hadi%20Pouransari%20and%20Alexander%20Toshev%20and%20Stephanie%20Wang%20and%20Dirk%20Groeneveld%20and%20Luca%20Soldaini%20and%20Pang%20Wei%20Koh%20and%20Jenia%20Jitsev%20and%20Thomas%20Kollar%20and%20Alexandros%20G.%20Dimakis%20and%20Yair%20Carmon%20and%20Achal%20Dave%20and%20Ludwig%20Schmidt%20and%20Vaishaal%20Shankar&entry.1292438233=%20%20We%20introduce%20DataComp%20for%20Language%20Models%20%28DCLM%29%2C%20a%20testbed%20for%20controlled%0Adataset%20experiments%20with%20the%20goal%20of%20improving%20language%20models.%20As%20part%20of%0ADCLM%2C%20we%20provide%20a%20standardized%20corpus%20of%20240T%20tokens%20extracted%20from%20Common%0ACrawl%2C%20effective%20pretraining%20recipes%20based%20on%20the%20OpenLM%20framework%2C%20and%20a%20broad%0Asuite%20of%2053%20downstream%20evaluations.%20Participants%20in%20the%20DCLM%20benchmark%20can%0Aexperiment%20with%20data%20curation%20strategies%20such%20as%20deduplication%2C%20filtering%2C%20and%0Adata%20mixing%20at%20model%20scales%20ranging%20from%20412M%20to%207B%20parameters.%20As%20a%20baseline%0Afor%20DCLM%2C%20we%20conduct%20extensive%20experiments%20and%20find%20that%20model-based%20filtering%0Ais%20key%20to%20assembling%20a%20high-quality%20training%20set.%20The%20resulting%20dataset%2C%0ADCLM-Baseline%20enables%20training%20a%207B%20parameter%20language%20model%20from%20scratch%20to%0A64%25%205-shot%20accuracy%20on%20MMLU%20with%202.6T%20training%20tokens.%20Compared%20to%20MAP-Neo%2C%20the%0Aprevious%20state-of-the-art%20in%20open-data%20language%20models%2C%20DCLM-Baseline%0Arepresents%20a%206.6%20percentage%20point%20improvement%20on%20MMLU%20while%20being%20trained%20with%0A40%25%20less%20compute.%20Our%20baseline%20model%20is%20also%20comparable%20to%20Mistral-7B-v0.3%20and%0ALlama%203%208B%20on%20MMLU%20%2863%25%20%26%2066%25%29%2C%20and%20performs%20similarly%20on%20an%20average%20of%2053%0Anatural%20language%20understanding%20tasks%20while%20being%20trained%20with%206.6x%20less%20compute%0Athan%20Llama%203%208B.%20Our%20results%20highlight%20the%20importance%20of%20dataset%20design%20for%0Atraining%20language%20models%20and%20offer%20a%20starting%20point%20for%20further%20research%20on%0Adata%20curation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11794v2&entry.124074799=Read"},
{"title": "LayerMerge: Neural Network Depth Compression through Layer Pruning and\n  Merging", "author": "Jinuk Kim and Marwa El Halabi and Mingi Ji and Hyun Oh Song", "abstract": "  Recent works show that reducing the number of layers in a convolutional\nneural network can enhance efficiency while maintaining the performance of the\nnetwork. Existing depth compression methods remove redundant non-linear\nactivation functions and merge the consecutive convolution layers into a single\nlayer. However, these methods suffer from a critical drawback; the kernel size\nof the merged layers becomes larger, significantly undermining the latency\nreduction gained from reducing the depth of the network. We show that this\nproblem can be addressed by jointly pruning convolution layers and activation\nfunctions. To this end, we propose LayerMerge, a novel depth compression method\nthat selects which activation layers and convolution layers to remove, to\nachieve a desired inference speed-up while minimizing performance loss. Since\nthe corresponding selection problem involves an exponential search space, we\nformulate a novel surrogate optimization problem and efficiently solve it via\ndynamic programming. Empirical results demonstrate that our method consistently\noutperforms existing depth compression and layer pruning methods on various\nnetwork architectures, both on image classification and generation tasks. We\nrelease the code at https://github.com/snu-mllab/LayerMerge.\n", "link": "http://arxiv.org/abs/2406.12837v1", "date": "2024-06-18", "relevancy": 1.9567, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5001}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerMerge%3A%20Neural%20Network%20Depth%20Compression%20through%20Layer%20Pruning%20and%0A%20%20Merging&body=Title%3A%20LayerMerge%3A%20Neural%20Network%20Depth%20Compression%20through%20Layer%20Pruning%20and%0A%20%20Merging%0AAuthor%3A%20Jinuk%20Kim%20and%20Marwa%20El%20Halabi%20and%20Mingi%20Ji%20and%20Hyun%20Oh%20Song%0AAbstract%3A%20%20%20Recent%20works%20show%20that%20reducing%20the%20number%20of%20layers%20in%20a%20convolutional%0Aneural%20network%20can%20enhance%20efficiency%20while%20maintaining%20the%20performance%20of%20the%0Anetwork.%20Existing%20depth%20compression%20methods%20remove%20redundant%20non-linear%0Aactivation%20functions%20and%20merge%20the%20consecutive%20convolution%20layers%20into%20a%20single%0Alayer.%20However%2C%20these%20methods%20suffer%20from%20a%20critical%20drawback%3B%20the%20kernel%20size%0Aof%20the%20merged%20layers%20becomes%20larger%2C%20significantly%20undermining%20the%20latency%0Areduction%20gained%20from%20reducing%20the%20depth%20of%20the%20network.%20We%20show%20that%20this%0Aproblem%20can%20be%20addressed%20by%20jointly%20pruning%20convolution%20layers%20and%20activation%0Afunctions.%20To%20this%20end%2C%20we%20propose%20LayerMerge%2C%20a%20novel%20depth%20compression%20method%0Athat%20selects%20which%20activation%20layers%20and%20convolution%20layers%20to%20remove%2C%20to%0Aachieve%20a%20desired%20inference%20speed-up%20while%20minimizing%20performance%20loss.%20Since%0Athe%20corresponding%20selection%20problem%20involves%20an%20exponential%20search%20space%2C%20we%0Aformulate%20a%20novel%20surrogate%20optimization%20problem%20and%20efficiently%20solve%20it%20via%0Adynamic%20programming.%20Empirical%20results%20demonstrate%20that%20our%20method%20consistently%0Aoutperforms%20existing%20depth%20compression%20and%20layer%20pruning%20methods%20on%20various%0Anetwork%20architectures%2C%20both%20on%20image%20classification%20and%20generation%20tasks.%20We%0Arelease%20the%20code%20at%20https%3A//github.com/snu-mllab/LayerMerge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerMerge%253A%2520Neural%2520Network%2520Depth%2520Compression%2520through%2520Layer%2520Pruning%2520and%250A%2520%2520Merging%26entry.906535625%3DJinuk%2520Kim%2520and%2520Marwa%2520El%2520Halabi%2520and%2520Mingi%2520Ji%2520and%2520Hyun%2520Oh%2520Song%26entry.1292438233%3D%2520%2520Recent%2520works%2520show%2520that%2520reducing%2520the%2520number%2520of%2520layers%2520in%2520a%2520convolutional%250Aneural%2520network%2520can%2520enhance%2520efficiency%2520while%2520maintaining%2520the%2520performance%2520of%2520the%250Anetwork.%2520Existing%2520depth%2520compression%2520methods%2520remove%2520redundant%2520non-linear%250Aactivation%2520functions%2520and%2520merge%2520the%2520consecutive%2520convolution%2520layers%2520into%2520a%2520single%250Alayer.%2520However%252C%2520these%2520methods%2520suffer%2520from%2520a%2520critical%2520drawback%253B%2520the%2520kernel%2520size%250Aof%2520the%2520merged%2520layers%2520becomes%2520larger%252C%2520significantly%2520undermining%2520the%2520latency%250Areduction%2520gained%2520from%2520reducing%2520the%2520depth%2520of%2520the%2520network.%2520We%2520show%2520that%2520this%250Aproblem%2520can%2520be%2520addressed%2520by%2520jointly%2520pruning%2520convolution%2520layers%2520and%2520activation%250Afunctions.%2520To%2520this%2520end%252C%2520we%2520propose%2520LayerMerge%252C%2520a%2520novel%2520depth%2520compression%2520method%250Athat%2520selects%2520which%2520activation%2520layers%2520and%2520convolution%2520layers%2520to%2520remove%252C%2520to%250Aachieve%2520a%2520desired%2520inference%2520speed-up%2520while%2520minimizing%2520performance%2520loss.%2520Since%250Athe%2520corresponding%2520selection%2520problem%2520involves%2520an%2520exponential%2520search%2520space%252C%2520we%250Aformulate%2520a%2520novel%2520surrogate%2520optimization%2520problem%2520and%2520efficiently%2520solve%2520it%2520via%250Adynamic%2520programming.%2520Empirical%2520results%2520demonstrate%2520that%2520our%2520method%2520consistently%250Aoutperforms%2520existing%2520depth%2520compression%2520and%2520layer%2520pruning%2520methods%2520on%2520various%250Anetwork%2520architectures%252C%2520both%2520on%2520image%2520classification%2520and%2520generation%2520tasks.%2520We%250Arelease%2520the%2520code%2520at%2520https%253A//github.com/snu-mllab/LayerMerge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerMerge%3A%20Neural%20Network%20Depth%20Compression%20through%20Layer%20Pruning%20and%0A%20%20Merging&entry.906535625=Jinuk%20Kim%20and%20Marwa%20El%20Halabi%20and%20Mingi%20Ji%20and%20Hyun%20Oh%20Song&entry.1292438233=%20%20Recent%20works%20show%20that%20reducing%20the%20number%20of%20layers%20in%20a%20convolutional%0Aneural%20network%20can%20enhance%20efficiency%20while%20maintaining%20the%20performance%20of%20the%0Anetwork.%20Existing%20depth%20compression%20methods%20remove%20redundant%20non-linear%0Aactivation%20functions%20and%20merge%20the%20consecutive%20convolution%20layers%20into%20a%20single%0Alayer.%20However%2C%20these%20methods%20suffer%20from%20a%20critical%20drawback%3B%20the%20kernel%20size%0Aof%20the%20merged%20layers%20becomes%20larger%2C%20significantly%20undermining%20the%20latency%0Areduction%20gained%20from%20reducing%20the%20depth%20of%20the%20network.%20We%20show%20that%20this%0Aproblem%20can%20be%20addressed%20by%20jointly%20pruning%20convolution%20layers%20and%20activation%0Afunctions.%20To%20this%20end%2C%20we%20propose%20LayerMerge%2C%20a%20novel%20depth%20compression%20method%0Athat%20selects%20which%20activation%20layers%20and%20convolution%20layers%20to%20remove%2C%20to%0Aachieve%20a%20desired%20inference%20speed-up%20while%20minimizing%20performance%20loss.%20Since%0Athe%20corresponding%20selection%20problem%20involves%20an%20exponential%20search%20space%2C%20we%0Aformulate%20a%20novel%20surrogate%20optimization%20problem%20and%20efficiently%20solve%20it%20via%0Adynamic%20programming.%20Empirical%20results%20demonstrate%20that%20our%20method%20consistently%0Aoutperforms%20existing%20depth%20compression%20and%20layer%20pruning%20methods%20on%20various%0Anetwork%20architectures%2C%20both%20on%20image%20classification%20and%20generation%20tasks.%20We%0Arelease%20the%20code%20at%20https%3A//github.com/snu-mllab/LayerMerge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12837v1&entry.124074799=Read"},
{"title": "STG4Traffic: A Survey and Benchmark of Spatial-Temporal Graph Neural\n  Networks for Traffic Prediction", "author": "Xunlian Luo and Chunjiang Zhu and Detian Zhang and Qing Li", "abstract": "  Traffic prediction has been an active research topic in the domain of\nspatial-temporal data mining. Accurate real-time traffic prediction is\nessential to improve the safety, stability, and versatility of smart city\nsystems, i.e., traffic control and optimal routing. The complex and highly\ndynamic spatial-temporal dependencies make effective predictions still face\nmany challenges. Recent studies have shown that spatial-temporal graph neural\nnetworks exhibit great potential applied to traffic prediction, which combines\nsequential models with graph convolutional networks to jointly model temporal\nand spatial correlations. However, a survey study of graph learning,\nspatial-temporal graph models for traffic, as well as a fair comparison of\nbaseline models are pending and unavoidable issues. In this paper, we first\nprovide a systematic review of graph learning strategies and commonly used\ngraph convolution algorithms. Then we conduct a comprehensive analysis of the\nstrengths and weaknesses of recently proposed spatial-temporal graph network\nmodels. Furthermore, we build a study called STG4Traffic using the deep\nlearning framework PyTorch to establish a standardized and scalable benchmark\non two types of traffic datasets. We can evaluate their performance by\npersonalizing the model settings with uniform metrics. Finally, we point out\nsome problems in the current study and discuss future directions. Source codes\nare available at https://github.com/trainingl/STG4Traffic.\n", "link": "http://arxiv.org/abs/2307.00495v2", "date": "2024-06-18", "relevancy": 1.9558, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5055}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4864}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STG4Traffic%3A%20A%20Survey%20and%20Benchmark%20of%20Spatial-Temporal%20Graph%20Neural%0A%20%20Networks%20for%20Traffic%20Prediction&body=Title%3A%20STG4Traffic%3A%20A%20Survey%20and%20Benchmark%20of%20Spatial-Temporal%20Graph%20Neural%0A%20%20Networks%20for%20Traffic%20Prediction%0AAuthor%3A%20Xunlian%20Luo%20and%20Chunjiang%20Zhu%20and%20Detian%20Zhang%20and%20Qing%20Li%0AAbstract%3A%20%20%20Traffic%20prediction%20has%20been%20an%20active%20research%20topic%20in%20the%20domain%20of%0Aspatial-temporal%20data%20mining.%20Accurate%20real-time%20traffic%20prediction%20is%0Aessential%20to%20improve%20the%20safety%2C%20stability%2C%20and%20versatility%20of%20smart%20city%0Asystems%2C%20i.e.%2C%20traffic%20control%20and%20optimal%20routing.%20The%20complex%20and%20highly%0Adynamic%20spatial-temporal%20dependencies%20make%20effective%20predictions%20still%20face%0Amany%20challenges.%20Recent%20studies%20have%20shown%20that%20spatial-temporal%20graph%20neural%0Anetworks%20exhibit%20great%20potential%20applied%20to%20traffic%20prediction%2C%20which%20combines%0Asequential%20models%20with%20graph%20convolutional%20networks%20to%20jointly%20model%20temporal%0Aand%20spatial%20correlations.%20However%2C%20a%20survey%20study%20of%20graph%20learning%2C%0Aspatial-temporal%20graph%20models%20for%20traffic%2C%20as%20well%20as%20a%20fair%20comparison%20of%0Abaseline%20models%20are%20pending%20and%20unavoidable%20issues.%20In%20this%20paper%2C%20we%20first%0Aprovide%20a%20systematic%20review%20of%20graph%20learning%20strategies%20and%20commonly%20used%0Agraph%20convolution%20algorithms.%20Then%20we%20conduct%20a%20comprehensive%20analysis%20of%20the%0Astrengths%20and%20weaknesses%20of%20recently%20proposed%20spatial-temporal%20graph%20network%0Amodels.%20Furthermore%2C%20we%20build%20a%20study%20called%20STG4Traffic%20using%20the%20deep%0Alearning%20framework%20PyTorch%20to%20establish%20a%20standardized%20and%20scalable%20benchmark%0Aon%20two%20types%20of%20traffic%20datasets.%20We%20can%20evaluate%20their%20performance%20by%0Apersonalizing%20the%20model%20settings%20with%20uniform%20metrics.%20Finally%2C%20we%20point%20out%0Asome%20problems%20in%20the%20current%20study%20and%20discuss%20future%20directions.%20Source%20codes%0Aare%20available%20at%20https%3A//github.com/trainingl/STG4Traffic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.00495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTG4Traffic%253A%2520A%2520Survey%2520and%2520Benchmark%2520of%2520Spatial-Temporal%2520Graph%2520Neural%250A%2520%2520Networks%2520for%2520Traffic%2520Prediction%26entry.906535625%3DXunlian%2520Luo%2520and%2520Chunjiang%2520Zhu%2520and%2520Detian%2520Zhang%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520Traffic%2520prediction%2520has%2520been%2520an%2520active%2520research%2520topic%2520in%2520the%2520domain%2520of%250Aspatial-temporal%2520data%2520mining.%2520Accurate%2520real-time%2520traffic%2520prediction%2520is%250Aessential%2520to%2520improve%2520the%2520safety%252C%2520stability%252C%2520and%2520versatility%2520of%2520smart%2520city%250Asystems%252C%2520i.e.%252C%2520traffic%2520control%2520and%2520optimal%2520routing.%2520The%2520complex%2520and%2520highly%250Adynamic%2520spatial-temporal%2520dependencies%2520make%2520effective%2520predictions%2520still%2520face%250Amany%2520challenges.%2520Recent%2520studies%2520have%2520shown%2520that%2520spatial-temporal%2520graph%2520neural%250Anetworks%2520exhibit%2520great%2520potential%2520applied%2520to%2520traffic%2520prediction%252C%2520which%2520combines%250Asequential%2520models%2520with%2520graph%2520convolutional%2520networks%2520to%2520jointly%2520model%2520temporal%250Aand%2520spatial%2520correlations.%2520However%252C%2520a%2520survey%2520study%2520of%2520graph%2520learning%252C%250Aspatial-temporal%2520graph%2520models%2520for%2520traffic%252C%2520as%2520well%2520as%2520a%2520fair%2520comparison%2520of%250Abaseline%2520models%2520are%2520pending%2520and%2520unavoidable%2520issues.%2520In%2520this%2520paper%252C%2520we%2520first%250Aprovide%2520a%2520systematic%2520review%2520of%2520graph%2520learning%2520strategies%2520and%2520commonly%2520used%250Agraph%2520convolution%2520algorithms.%2520Then%2520we%2520conduct%2520a%2520comprehensive%2520analysis%2520of%2520the%250Astrengths%2520and%2520weaknesses%2520of%2520recently%2520proposed%2520spatial-temporal%2520graph%2520network%250Amodels.%2520Furthermore%252C%2520we%2520build%2520a%2520study%2520called%2520STG4Traffic%2520using%2520the%2520deep%250Alearning%2520framework%2520PyTorch%2520to%2520establish%2520a%2520standardized%2520and%2520scalable%2520benchmark%250Aon%2520two%2520types%2520of%2520traffic%2520datasets.%2520We%2520can%2520evaluate%2520their%2520performance%2520by%250Apersonalizing%2520the%2520model%2520settings%2520with%2520uniform%2520metrics.%2520Finally%252C%2520we%2520point%2520out%250Asome%2520problems%2520in%2520the%2520current%2520study%2520and%2520discuss%2520future%2520directions.%2520Source%2520codes%250Aare%2520available%2520at%2520https%253A//github.com/trainingl/STG4Traffic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.00495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STG4Traffic%3A%20A%20Survey%20and%20Benchmark%20of%20Spatial-Temporal%20Graph%20Neural%0A%20%20Networks%20for%20Traffic%20Prediction&entry.906535625=Xunlian%20Luo%20and%20Chunjiang%20Zhu%20and%20Detian%20Zhang%20and%20Qing%20Li&entry.1292438233=%20%20Traffic%20prediction%20has%20been%20an%20active%20research%20topic%20in%20the%20domain%20of%0Aspatial-temporal%20data%20mining.%20Accurate%20real-time%20traffic%20prediction%20is%0Aessential%20to%20improve%20the%20safety%2C%20stability%2C%20and%20versatility%20of%20smart%20city%0Asystems%2C%20i.e.%2C%20traffic%20control%20and%20optimal%20routing.%20The%20complex%20and%20highly%0Adynamic%20spatial-temporal%20dependencies%20make%20effective%20predictions%20still%20face%0Amany%20challenges.%20Recent%20studies%20have%20shown%20that%20spatial-temporal%20graph%20neural%0Anetworks%20exhibit%20great%20potential%20applied%20to%20traffic%20prediction%2C%20which%20combines%0Asequential%20models%20with%20graph%20convolutional%20networks%20to%20jointly%20model%20temporal%0Aand%20spatial%20correlations.%20However%2C%20a%20survey%20study%20of%20graph%20learning%2C%0Aspatial-temporal%20graph%20models%20for%20traffic%2C%20as%20well%20as%20a%20fair%20comparison%20of%0Abaseline%20models%20are%20pending%20and%20unavoidable%20issues.%20In%20this%20paper%2C%20we%20first%0Aprovide%20a%20systematic%20review%20of%20graph%20learning%20strategies%20and%20commonly%20used%0Agraph%20convolution%20algorithms.%20Then%20we%20conduct%20a%20comprehensive%20analysis%20of%20the%0Astrengths%20and%20weaknesses%20of%20recently%20proposed%20spatial-temporal%20graph%20network%0Amodels.%20Furthermore%2C%20we%20build%20a%20study%20called%20STG4Traffic%20using%20the%20deep%0Alearning%20framework%20PyTorch%20to%20establish%20a%20standardized%20and%20scalable%20benchmark%0Aon%20two%20types%20of%20traffic%20datasets.%20We%20can%20evaluate%20their%20performance%20by%0Apersonalizing%20the%20model%20settings%20with%20uniform%20metrics.%20Finally%2C%20we%20point%20out%0Asome%20problems%20in%20the%20current%20study%20and%20discuss%20future%20directions.%20Source%20codes%0Aare%20available%20at%20https%3A//github.com/trainingl/STG4Traffic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.00495v2&entry.124074799=Read"},
{"title": "DeformTime: Capturing Variable Dependencies with Deformable Attention\n  for Time Series Forecasting", "author": "Yuxuan Shu and Vasileios Lampos", "abstract": "  In multivariate time series (MTS) forecasting, existing state-of-the-art deep\nlearning approaches tend to focus on autoregressive formulations and overlook\nthe information within exogenous indicators. To address this limitation, we\npresent DeformTime, a neural network architecture that attempts to capture\ncorrelated temporal patterns from the input space, and hence, improve\nforecasting accuracy. It deploys two core operations performed by deformable\nattention blocks (DABs): learning dependencies across variables from different\ntime steps (variable DAB), and preserving temporal dependencies in data from\nprevious time steps (temporal DAB). Input data transformation is explicitly\ndesigned to enhance learning from the deformed series of information while\npassing through a DAB. We conduct extensive experiments on 6 MTS data sets,\nusing previously established benchmarks as well as challenging infectious\ndisease modelling tasks with more exogenous variables. The results demonstrate\nthat DeformTime improves accuracy against previous competitive methods across\nthe vast majority of MTS forecasting tasks, reducing the mean absolute error by\n10% on average. Notably, performance gains remain consistent across longer\nforecasting horizons.\n", "link": "http://arxiv.org/abs/2406.07438v2", "date": "2024-06-18", "relevancy": 1.9551, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4967}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4958}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeformTime%3A%20Capturing%20Variable%20Dependencies%20with%20Deformable%20Attention%0A%20%20for%20Time%20Series%20Forecasting&body=Title%3A%20DeformTime%3A%20Capturing%20Variable%20Dependencies%20with%20Deformable%20Attention%0A%20%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Yuxuan%20Shu%20and%20Vasileios%20Lampos%0AAbstract%3A%20%20%20In%20multivariate%20time%20series%20%28MTS%29%20forecasting%2C%20existing%20state-of-the-art%20deep%0Alearning%20approaches%20tend%20to%20focus%20on%20autoregressive%20formulations%20and%20overlook%0Athe%20information%20within%20exogenous%20indicators.%20To%20address%20this%20limitation%2C%20we%0Apresent%20DeformTime%2C%20a%20neural%20network%20architecture%20that%20attempts%20to%20capture%0Acorrelated%20temporal%20patterns%20from%20the%20input%20space%2C%20and%20hence%2C%20improve%0Aforecasting%20accuracy.%20It%20deploys%20two%20core%20operations%20performed%20by%20deformable%0Aattention%20blocks%20%28DABs%29%3A%20learning%20dependencies%20across%20variables%20from%20different%0Atime%20steps%20%28variable%20DAB%29%2C%20and%20preserving%20temporal%20dependencies%20in%20data%20from%0Aprevious%20time%20steps%20%28temporal%20DAB%29.%20Input%20data%20transformation%20is%20explicitly%0Adesigned%20to%20enhance%20learning%20from%20the%20deformed%20series%20of%20information%20while%0Apassing%20through%20a%20DAB.%20We%20conduct%20extensive%20experiments%20on%206%20MTS%20data%20sets%2C%0Ausing%20previously%20established%20benchmarks%20as%20well%20as%20challenging%20infectious%0Adisease%20modelling%20tasks%20with%20more%20exogenous%20variables.%20The%20results%20demonstrate%0Athat%20DeformTime%20improves%20accuracy%20against%20previous%20competitive%20methods%20across%0Athe%20vast%20majority%20of%20MTS%20forecasting%20tasks%2C%20reducing%20the%20mean%20absolute%20error%20by%0A10%25%20on%20average.%20Notably%2C%20performance%20gains%20remain%20consistent%20across%20longer%0Aforecasting%20horizons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeformTime%253A%2520Capturing%2520Variable%2520Dependencies%2520with%2520Deformable%2520Attention%250A%2520%2520for%2520Time%2520Series%2520Forecasting%26entry.906535625%3DYuxuan%2520Shu%2520and%2520Vasileios%2520Lampos%26entry.1292438233%3D%2520%2520In%2520multivariate%2520time%2520series%2520%2528MTS%2529%2520forecasting%252C%2520existing%2520state-of-the-art%2520deep%250Alearning%2520approaches%2520tend%2520to%2520focus%2520on%2520autoregressive%2520formulations%2520and%2520overlook%250Athe%2520information%2520within%2520exogenous%2520indicators.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apresent%2520DeformTime%252C%2520a%2520neural%2520network%2520architecture%2520that%2520attempts%2520to%2520capture%250Acorrelated%2520temporal%2520patterns%2520from%2520the%2520input%2520space%252C%2520and%2520hence%252C%2520improve%250Aforecasting%2520accuracy.%2520It%2520deploys%2520two%2520core%2520operations%2520performed%2520by%2520deformable%250Aattention%2520blocks%2520%2528DABs%2529%253A%2520learning%2520dependencies%2520across%2520variables%2520from%2520different%250Atime%2520steps%2520%2528variable%2520DAB%2529%252C%2520and%2520preserving%2520temporal%2520dependencies%2520in%2520data%2520from%250Aprevious%2520time%2520steps%2520%2528temporal%2520DAB%2529.%2520Input%2520data%2520transformation%2520is%2520explicitly%250Adesigned%2520to%2520enhance%2520learning%2520from%2520the%2520deformed%2520series%2520of%2520information%2520while%250Apassing%2520through%2520a%2520DAB.%2520We%2520conduct%2520extensive%2520experiments%2520on%25206%2520MTS%2520data%2520sets%252C%250Ausing%2520previously%2520established%2520benchmarks%2520as%2520well%2520as%2520challenging%2520infectious%250Adisease%2520modelling%2520tasks%2520with%2520more%2520exogenous%2520variables.%2520The%2520results%2520demonstrate%250Athat%2520DeformTime%2520improves%2520accuracy%2520against%2520previous%2520competitive%2520methods%2520across%250Athe%2520vast%2520majority%2520of%2520MTS%2520forecasting%2520tasks%252C%2520reducing%2520the%2520mean%2520absolute%2520error%2520by%250A10%2525%2520on%2520average.%2520Notably%252C%2520performance%2520gains%2520remain%2520consistent%2520across%2520longer%250Aforecasting%2520horizons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeformTime%3A%20Capturing%20Variable%20Dependencies%20with%20Deformable%20Attention%0A%20%20for%20Time%20Series%20Forecasting&entry.906535625=Yuxuan%20Shu%20and%20Vasileios%20Lampos&entry.1292438233=%20%20In%20multivariate%20time%20series%20%28MTS%29%20forecasting%2C%20existing%20state-of-the-art%20deep%0Alearning%20approaches%20tend%20to%20focus%20on%20autoregressive%20formulations%20and%20overlook%0Athe%20information%20within%20exogenous%20indicators.%20To%20address%20this%20limitation%2C%20we%0Apresent%20DeformTime%2C%20a%20neural%20network%20architecture%20that%20attempts%20to%20capture%0Acorrelated%20temporal%20patterns%20from%20the%20input%20space%2C%20and%20hence%2C%20improve%0Aforecasting%20accuracy.%20It%20deploys%20two%20core%20operations%20performed%20by%20deformable%0Aattention%20blocks%20%28DABs%29%3A%20learning%20dependencies%20across%20variables%20from%20different%0Atime%20steps%20%28variable%20DAB%29%2C%20and%20preserving%20temporal%20dependencies%20in%20data%20from%0Aprevious%20time%20steps%20%28temporal%20DAB%29.%20Input%20data%20transformation%20is%20explicitly%0Adesigned%20to%20enhance%20learning%20from%20the%20deformed%20series%20of%20information%20while%0Apassing%20through%20a%20DAB.%20We%20conduct%20extensive%20experiments%20on%206%20MTS%20data%20sets%2C%0Ausing%20previously%20established%20benchmarks%20as%20well%20as%20challenging%20infectious%0Adisease%20modelling%20tasks%20with%20more%20exogenous%20variables.%20The%20results%20demonstrate%0Athat%20DeformTime%20improves%20accuracy%20against%20previous%20competitive%20methods%20across%0Athe%20vast%20majority%20of%20MTS%20forecasting%20tasks%2C%20reducing%20the%20mean%20absolute%20error%20by%0A10%25%20on%20average.%20Notably%2C%20performance%20gains%20remain%20consistent%20across%20longer%0Aforecasting%20horizons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07438v2&entry.124074799=Read"},
{"title": "Learned Image Compression for HE-stained Histopathological Images via\n  Stain Deconvolution", "author": "Maximilian Fischer and Peter Neher and Tassilo Wald and Silvia Dias Almeida and Shuhan Xiao and Peter Sch\u00fcffler and Rickmer Braren and Michael G\u00f6tz and Alexander Muckenhuber and Jens Kleesiek and Marco Nolden and Klaus Maier-Hein", "abstract": "  Processing histopathological Whole Slide Images (WSI) leads to massive\nstorage requirements for clinics worldwide. Even after lossy image compression\nduring image acquisition, additional lossy compression is frequently possible\nwithout substantially affecting the performance of deep learning-based (DL)\ndownstream tasks. In this paper, we show that the commonly used JPEG algorithm\nis not best suited for further compression and we propose Stain Quantized\nLatent Compression (SQLC ), a novel DL based histopathology data compression\napproach. SQLC compresses staining and RGB channels before passing it through a\ncompression autoencoder (CAE ) in order to obtain quantized latent\nrepresentations for maximizing the compression. We show that our approach\nyields superior performance in a classification downstream task, compared to\ntraditional approaches like JPEG, while image quality metrics like the\nMulti-Scale Structural Similarity Index (MS-SSIM) is largely preserved. Our\nmethod is online available.\n", "link": "http://arxiv.org/abs/2406.12623v1", "date": "2024-06-18", "relevancy": 1.9444, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5318}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4782}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Image%20Compression%20for%20HE-stained%20Histopathological%20Images%20via%0A%20%20Stain%20Deconvolution&body=Title%3A%20Learned%20Image%20Compression%20for%20HE-stained%20Histopathological%20Images%20via%0A%20%20Stain%20Deconvolution%0AAuthor%3A%20Maximilian%20Fischer%20and%20Peter%20Neher%20and%20Tassilo%20Wald%20and%20Silvia%20Dias%20Almeida%20and%20Shuhan%20Xiao%20and%20Peter%20Sch%C3%BCffler%20and%20Rickmer%20Braren%20and%20Michael%20G%C3%B6tz%20and%20Alexander%20Muckenhuber%20and%20Jens%20Kleesiek%20and%20Marco%20Nolden%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Processing%20histopathological%20Whole%20Slide%20Images%20%28WSI%29%20leads%20to%20massive%0Astorage%20requirements%20for%20clinics%20worldwide.%20Even%20after%20lossy%20image%20compression%0Aduring%20image%20acquisition%2C%20additional%20lossy%20compression%20is%20frequently%20possible%0Awithout%20substantially%20affecting%20the%20performance%20of%20deep%20learning-based%20%28DL%29%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20show%20that%20the%20commonly%20used%20JPEG%20algorithm%0Ais%20not%20best%20suited%20for%20further%20compression%20and%20we%20propose%20Stain%20Quantized%0ALatent%20Compression%20%28SQLC%20%29%2C%20a%20novel%20DL%20based%20histopathology%20data%20compression%0Aapproach.%20SQLC%20compresses%20staining%20and%20RGB%20channels%20before%20passing%20it%20through%20a%0Acompression%20autoencoder%20%28CAE%20%29%20in%20order%20to%20obtain%20quantized%20latent%0Arepresentations%20for%20maximizing%20the%20compression.%20We%20show%20that%20our%20approach%0Ayields%20superior%20performance%20in%20a%20classification%20downstream%20task%2C%20compared%20to%0Atraditional%20approaches%20like%20JPEG%2C%20while%20image%20quality%20metrics%20like%20the%0AMulti-Scale%20Structural%20Similarity%20Index%20%28MS-SSIM%29%20is%20largely%20preserved.%20Our%0Amethod%20is%20online%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Image%2520Compression%2520for%2520HE-stained%2520Histopathological%2520Images%2520via%250A%2520%2520Stain%2520Deconvolution%26entry.906535625%3DMaximilian%2520Fischer%2520and%2520Peter%2520Neher%2520and%2520Tassilo%2520Wald%2520and%2520Silvia%2520Dias%2520Almeida%2520and%2520Shuhan%2520Xiao%2520and%2520Peter%2520Sch%25C3%25BCffler%2520and%2520Rickmer%2520Braren%2520and%2520Michael%2520G%25C3%25B6tz%2520and%2520Alexander%2520Muckenhuber%2520and%2520Jens%2520Kleesiek%2520and%2520Marco%2520Nolden%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520Processing%2520histopathological%2520Whole%2520Slide%2520Images%2520%2528WSI%2529%2520leads%2520to%2520massive%250Astorage%2520requirements%2520for%2520clinics%2520worldwide.%2520Even%2520after%2520lossy%2520image%2520compression%250Aduring%2520image%2520acquisition%252C%2520additional%2520lossy%2520compression%2520is%2520frequently%2520possible%250Awithout%2520substantially%2520affecting%2520the%2520performance%2520of%2520deep%2520learning-based%2520%2528DL%2529%250Adownstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520the%2520commonly%2520used%2520JPEG%2520algorithm%250Ais%2520not%2520best%2520suited%2520for%2520further%2520compression%2520and%2520we%2520propose%2520Stain%2520Quantized%250ALatent%2520Compression%2520%2528SQLC%2520%2529%252C%2520a%2520novel%2520DL%2520based%2520histopathology%2520data%2520compression%250Aapproach.%2520SQLC%2520compresses%2520staining%2520and%2520RGB%2520channels%2520before%2520passing%2520it%2520through%2520a%250Acompression%2520autoencoder%2520%2528CAE%2520%2529%2520in%2520order%2520to%2520obtain%2520quantized%2520latent%250Arepresentations%2520for%2520maximizing%2520the%2520compression.%2520We%2520show%2520that%2520our%2520approach%250Ayields%2520superior%2520performance%2520in%2520a%2520classification%2520downstream%2520task%252C%2520compared%2520to%250Atraditional%2520approaches%2520like%2520JPEG%252C%2520while%2520image%2520quality%2520metrics%2520like%2520the%250AMulti-Scale%2520Structural%2520Similarity%2520Index%2520%2528MS-SSIM%2529%2520is%2520largely%2520preserved.%2520Our%250Amethod%2520is%2520online%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Image%20Compression%20for%20HE-stained%20Histopathological%20Images%20via%0A%20%20Stain%20Deconvolution&entry.906535625=Maximilian%20Fischer%20and%20Peter%20Neher%20and%20Tassilo%20Wald%20and%20Silvia%20Dias%20Almeida%20and%20Shuhan%20Xiao%20and%20Peter%20Sch%C3%BCffler%20and%20Rickmer%20Braren%20and%20Michael%20G%C3%B6tz%20and%20Alexander%20Muckenhuber%20and%20Jens%20Kleesiek%20and%20Marco%20Nolden%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Processing%20histopathological%20Whole%20Slide%20Images%20%28WSI%29%20leads%20to%20massive%0Astorage%20requirements%20for%20clinics%20worldwide.%20Even%20after%20lossy%20image%20compression%0Aduring%20image%20acquisition%2C%20additional%20lossy%20compression%20is%20frequently%20possible%0Awithout%20substantially%20affecting%20the%20performance%20of%20deep%20learning-based%20%28DL%29%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20show%20that%20the%20commonly%20used%20JPEG%20algorithm%0Ais%20not%20best%20suited%20for%20further%20compression%20and%20we%20propose%20Stain%20Quantized%0ALatent%20Compression%20%28SQLC%20%29%2C%20a%20novel%20DL%20based%20histopathology%20data%20compression%0Aapproach.%20SQLC%20compresses%20staining%20and%20RGB%20channels%20before%20passing%20it%20through%20a%0Acompression%20autoencoder%20%28CAE%20%29%20in%20order%20to%20obtain%20quantized%20latent%0Arepresentations%20for%20maximizing%20the%20compression.%20We%20show%20that%20our%20approach%0Ayields%20superior%20performance%20in%20a%20classification%20downstream%20task%2C%20compared%20to%0Atraditional%20approaches%20like%20JPEG%2C%20while%20image%20quality%20metrics%20like%20the%0AMulti-Scale%20Structural%20Similarity%20Index%20%28MS-SSIM%29%20is%20largely%20preserved.%20Our%0Amethod%20is%20online%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12623v1&entry.124074799=Read"},
{"title": "UIFV: Data Reconstruction Attack in Vertical Federated Learning", "author": "Jirui Yang and Peng Chen and Zhihui Lu and Qiang Duan and Yubing Bao", "abstract": "  Vertical Federated Learning (VFL) facilitates collaborative machine learning\nwithout the need for participants to share raw private data. However, recent\nstudies have revealed privacy risks where adversaries might reconstruct\nsensitive features through data leakage during the learning process. Although\ndata reconstruction methods based on gradient or model information are somewhat\neffective, they reveal limitations in VFL application scenarios. This is\nbecause these traditional methods heavily rely on specific model structures\nand/or have strict limitations on application scenarios. To address this, our\nstudy introduces the Unified InverNet Framework into VFL, which yields a novel\nand flexible approach (dubbed UIFV) that leverages intermediate feature data to\nreconstruct original data, instead of relying on gradients or model details.\nThe intermediate feature data is the feature exchanged by different\nparticipants during the inference phase of VFL. Experiments on four datasets\ndemonstrate that our methods significantly outperform state-of-the-art\ntechniques in attack precision. Our work exposes severe privacy vulnerabilities\nwithin VFL systems that pose real threats to practical VFL applications and\nthus confirms the necessity of further enhancing privacy protection in the VFL\narchitecture.\n", "link": "http://arxiv.org/abs/2406.12588v1", "date": "2024-06-18", "relevancy": 1.9362, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4873}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4845}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UIFV%3A%20Data%20Reconstruction%20Attack%20in%20Vertical%20Federated%20Learning&body=Title%3A%20UIFV%3A%20Data%20Reconstruction%20Attack%20in%20Vertical%20Federated%20Learning%0AAuthor%3A%20Jirui%20Yang%20and%20Peng%20Chen%20and%20Zhihui%20Lu%20and%20Qiang%20Duan%20and%20Yubing%20Bao%0AAbstract%3A%20%20%20Vertical%20Federated%20Learning%20%28VFL%29%20facilitates%20collaborative%20machine%20learning%0Awithout%20the%20need%20for%20participants%20to%20share%20raw%20private%20data.%20However%2C%20recent%0Astudies%20have%20revealed%20privacy%20risks%20where%20adversaries%20might%20reconstruct%0Asensitive%20features%20through%20data%20leakage%20during%20the%20learning%20process.%20Although%0Adata%20reconstruction%20methods%20based%20on%20gradient%20or%20model%20information%20are%20somewhat%0Aeffective%2C%20they%20reveal%20limitations%20in%20VFL%20application%20scenarios.%20This%20is%0Abecause%20these%20traditional%20methods%20heavily%20rely%20on%20specific%20model%20structures%0Aand/or%20have%20strict%20limitations%20on%20application%20scenarios.%20To%20address%20this%2C%20our%0Astudy%20introduces%20the%20Unified%20InverNet%20Framework%20into%20VFL%2C%20which%20yields%20a%20novel%0Aand%20flexible%20approach%20%28dubbed%20UIFV%29%20that%20leverages%20intermediate%20feature%20data%20to%0Areconstruct%20original%20data%2C%20instead%20of%20relying%20on%20gradients%20or%20model%20details.%0AThe%20intermediate%20feature%20data%20is%20the%20feature%20exchanged%20by%20different%0Aparticipants%20during%20the%20inference%20phase%20of%20VFL.%20Experiments%20on%20four%20datasets%0Ademonstrate%20that%20our%20methods%20significantly%20outperform%20state-of-the-art%0Atechniques%20in%20attack%20precision.%20Our%20work%20exposes%20severe%20privacy%20vulnerabilities%0Awithin%20VFL%20systems%20that%20pose%20real%20threats%20to%20practical%20VFL%20applications%20and%0Athus%20confirms%20the%20necessity%20of%20further%20enhancing%20privacy%20protection%20in%20the%20VFL%0Aarchitecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUIFV%253A%2520Data%2520Reconstruction%2520Attack%2520in%2520Vertical%2520Federated%2520Learning%26entry.906535625%3DJirui%2520Yang%2520and%2520Peng%2520Chen%2520and%2520Zhihui%2520Lu%2520and%2520Qiang%2520Duan%2520and%2520Yubing%2520Bao%26entry.1292438233%3D%2520%2520Vertical%2520Federated%2520Learning%2520%2528VFL%2529%2520facilitates%2520collaborative%2520machine%2520learning%250Awithout%2520the%2520need%2520for%2520participants%2520to%2520share%2520raw%2520private%2520data.%2520However%252C%2520recent%250Astudies%2520have%2520revealed%2520privacy%2520risks%2520where%2520adversaries%2520might%2520reconstruct%250Asensitive%2520features%2520through%2520data%2520leakage%2520during%2520the%2520learning%2520process.%2520Although%250Adata%2520reconstruction%2520methods%2520based%2520on%2520gradient%2520or%2520model%2520information%2520are%2520somewhat%250Aeffective%252C%2520they%2520reveal%2520limitations%2520in%2520VFL%2520application%2520scenarios.%2520This%2520is%250Abecause%2520these%2520traditional%2520methods%2520heavily%2520rely%2520on%2520specific%2520model%2520structures%250Aand/or%2520have%2520strict%2520limitations%2520on%2520application%2520scenarios.%2520To%2520address%2520this%252C%2520our%250Astudy%2520introduces%2520the%2520Unified%2520InverNet%2520Framework%2520into%2520VFL%252C%2520which%2520yields%2520a%2520novel%250Aand%2520flexible%2520approach%2520%2528dubbed%2520UIFV%2529%2520that%2520leverages%2520intermediate%2520feature%2520data%2520to%250Areconstruct%2520original%2520data%252C%2520instead%2520of%2520relying%2520on%2520gradients%2520or%2520model%2520details.%250AThe%2520intermediate%2520feature%2520data%2520is%2520the%2520feature%2520exchanged%2520by%2520different%250Aparticipants%2520during%2520the%2520inference%2520phase%2520of%2520VFL.%2520Experiments%2520on%2520four%2520datasets%250Ademonstrate%2520that%2520our%2520methods%2520significantly%2520outperform%2520state-of-the-art%250Atechniques%2520in%2520attack%2520precision.%2520Our%2520work%2520exposes%2520severe%2520privacy%2520vulnerabilities%250Awithin%2520VFL%2520systems%2520that%2520pose%2520real%2520threats%2520to%2520practical%2520VFL%2520applications%2520and%250Athus%2520confirms%2520the%2520necessity%2520of%2520further%2520enhancing%2520privacy%2520protection%2520in%2520the%2520VFL%250Aarchitecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UIFV%3A%20Data%20Reconstruction%20Attack%20in%20Vertical%20Federated%20Learning&entry.906535625=Jirui%20Yang%20and%20Peng%20Chen%20and%20Zhihui%20Lu%20and%20Qiang%20Duan%20and%20Yubing%20Bao&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20facilitates%20collaborative%20machine%20learning%0Awithout%20the%20need%20for%20participants%20to%20share%20raw%20private%20data.%20However%2C%20recent%0Astudies%20have%20revealed%20privacy%20risks%20where%20adversaries%20might%20reconstruct%0Asensitive%20features%20through%20data%20leakage%20during%20the%20learning%20process.%20Although%0Adata%20reconstruction%20methods%20based%20on%20gradient%20or%20model%20information%20are%20somewhat%0Aeffective%2C%20they%20reveal%20limitations%20in%20VFL%20application%20scenarios.%20This%20is%0Abecause%20these%20traditional%20methods%20heavily%20rely%20on%20specific%20model%20structures%0Aand/or%20have%20strict%20limitations%20on%20application%20scenarios.%20To%20address%20this%2C%20our%0Astudy%20introduces%20the%20Unified%20InverNet%20Framework%20into%20VFL%2C%20which%20yields%20a%20novel%0Aand%20flexible%20approach%20%28dubbed%20UIFV%29%20that%20leverages%20intermediate%20feature%20data%20to%0Areconstruct%20original%20data%2C%20instead%20of%20relying%20on%20gradients%20or%20model%20details.%0AThe%20intermediate%20feature%20data%20is%20the%20feature%20exchanged%20by%20different%0Aparticipants%20during%20the%20inference%20phase%20of%20VFL.%20Experiments%20on%20four%20datasets%0Ademonstrate%20that%20our%20methods%20significantly%20outperform%20state-of-the-art%0Atechniques%20in%20attack%20precision.%20Our%20work%20exposes%20severe%20privacy%20vulnerabilities%0Awithin%20VFL%20systems%20that%20pose%20real%20threats%20to%20practical%20VFL%20applications%20and%0Athus%20confirms%20the%20necessity%20of%20further%20enhancing%20privacy%20protection%20in%20the%20VFL%0Aarchitecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12588v1&entry.124074799=Read"},
{"title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges", "author": "Aman Singh Thakur and Kartik Choudhary and Venkat Srinik Ramayapally and Sankaran Vaidyanathan and Dieuwke Hupkes", "abstract": "  Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges. We leverage TriviaQA\nas a benchmark for assessing objective knowledge reasoning of LLMs and evaluate\nthem alongside human annotations which we found to have a high inter-annotator\nagreement. Our study includes 9 judge models and 9 exam taker models -- both\nbase and instruction-tuned. We assess the judge model's alignment across\ndifferent model sizes, families, and judge prompts. Among other results, our\nresearch rediscovers the importance of using Cohen's kappa as a metric of\nalignment as opposed to simple percent agreement, showing that judges with high\npercent agreement can still assign vastly different scores. We find that both\nLlama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but in\nterms of ranking exam taker models, they are outperformed by both JudgeLM-7B\nand the lexical judge Contains, which have up to 34 points lower human\nalignment. Through error analysis and various other studies, including the\neffects of instruction length and leniency bias, we hope to provide valuable\nlessons for using LLMs as judges in the future.\n", "link": "http://arxiv.org/abs/2406.12624v1", "date": "2024-06-18", "relevancy": 1.934, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4964}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Judging%20the%20Judges%3A%20Evaluating%20Alignment%20and%20Vulnerabilities%20in%0A%20%20LLMs-as-Judges&body=Title%3A%20Judging%20the%20Judges%3A%20Evaluating%20Alignment%20and%20Vulnerabilities%20in%0A%20%20LLMs-as-Judges%0AAuthor%3A%20Aman%20Singh%20Thakur%20and%20Kartik%20Choudhary%20and%20Venkat%20Srinik%20Ramayapally%20and%20Sankaran%20Vaidyanathan%20and%20Dieuwke%20Hupkes%0AAbstract%3A%20%20%20Offering%20a%20promising%20solution%20to%20the%20scalability%20challenges%20associated%20with%0Ahuman%20evaluation%2C%20the%20LLM-as-a-judge%20paradigm%20is%20rapidly%20gaining%20traction%20as%20an%0Aapproach%20to%20evaluating%20large%20language%20models%20%28LLMs%29.%20However%2C%20there%20are%20still%0Amany%20open%20questions%20about%20the%20strengths%20and%20weaknesses%20of%20this%20paradigm%2C%20and%0Awhat%20potential%20biases%20it%20may%20hold.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%0Astudy%20of%20the%20performance%20of%20various%20LLMs%20acting%20as%20judges.%20We%20leverage%20TriviaQA%0Aas%20a%20benchmark%20for%20assessing%20objective%20knowledge%20reasoning%20of%20LLMs%20and%20evaluate%0Athem%20alongside%20human%20annotations%20which%20we%20found%20to%20have%20a%20high%20inter-annotator%0Aagreement.%20Our%20study%20includes%209%20judge%20models%20and%209%20exam%20taker%20models%20--%20both%0Abase%20and%20instruction-tuned.%20We%20assess%20the%20judge%20model%27s%20alignment%20across%0Adifferent%20model%20sizes%2C%20families%2C%20and%20judge%20prompts.%20Among%20other%20results%2C%20our%0Aresearch%20rediscovers%20the%20importance%20of%20using%20Cohen%27s%20kappa%20as%20a%20metric%20of%0Aalignment%20as%20opposed%20to%20simple%20percent%20agreement%2C%20showing%20that%20judges%20with%20high%0Apercent%20agreement%20can%20still%20assign%20vastly%20different%20scores.%20We%20find%20that%20both%0ALlama-3%2070B%20and%20GPT-4%20Turbo%20have%20an%20excellent%20alignment%20with%20humans%2C%20but%20in%0Aterms%20of%20ranking%20exam%20taker%20models%2C%20they%20are%20outperformed%20by%20both%20JudgeLM-7B%0Aand%20the%20lexical%20judge%20Contains%2C%20which%20have%20up%20to%2034%20points%20lower%20human%0Aalignment.%20Through%20error%20analysis%20and%20various%20other%20studies%2C%20including%20the%0Aeffects%20of%20instruction%20length%20and%20leniency%20bias%2C%20we%20hope%20to%20provide%20valuable%0Alessons%20for%20using%20LLMs%20as%20judges%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJudging%2520the%2520Judges%253A%2520Evaluating%2520Alignment%2520and%2520Vulnerabilities%2520in%250A%2520%2520LLMs-as-Judges%26entry.906535625%3DAman%2520Singh%2520Thakur%2520and%2520Kartik%2520Choudhary%2520and%2520Venkat%2520Srinik%2520Ramayapally%2520and%2520Sankaran%2520Vaidyanathan%2520and%2520Dieuwke%2520Hupkes%26entry.1292438233%3D%2520%2520Offering%2520a%2520promising%2520solution%2520to%2520the%2520scalability%2520challenges%2520associated%2520with%250Ahuman%2520evaluation%252C%2520the%2520LLM-as-a-judge%2520paradigm%2520is%2520rapidly%2520gaining%2520traction%2520as%2520an%250Aapproach%2520to%2520evaluating%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520there%2520are%2520still%250Amany%2520open%2520questions%2520about%2520the%2520strengths%2520and%2520weaknesses%2520of%2520this%2520paradigm%252C%2520and%250Awhat%2520potential%2520biases%2520it%2520may%2520hold.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%250Astudy%2520of%2520the%2520performance%2520of%2520various%2520LLMs%2520acting%2520as%2520judges.%2520We%2520leverage%2520TriviaQA%250Aas%2520a%2520benchmark%2520for%2520assessing%2520objective%2520knowledge%2520reasoning%2520of%2520LLMs%2520and%2520evaluate%250Athem%2520alongside%2520human%2520annotations%2520which%2520we%2520found%2520to%2520have%2520a%2520high%2520inter-annotator%250Aagreement.%2520Our%2520study%2520includes%25209%2520judge%2520models%2520and%25209%2520exam%2520taker%2520models%2520--%2520both%250Abase%2520and%2520instruction-tuned.%2520We%2520assess%2520the%2520judge%2520model%2527s%2520alignment%2520across%250Adifferent%2520model%2520sizes%252C%2520families%252C%2520and%2520judge%2520prompts.%2520Among%2520other%2520results%252C%2520our%250Aresearch%2520rediscovers%2520the%2520importance%2520of%2520using%2520Cohen%2527s%2520kappa%2520as%2520a%2520metric%2520of%250Aalignment%2520as%2520opposed%2520to%2520simple%2520percent%2520agreement%252C%2520showing%2520that%2520judges%2520with%2520high%250Apercent%2520agreement%2520can%2520still%2520assign%2520vastly%2520different%2520scores.%2520We%2520find%2520that%2520both%250ALlama-3%252070B%2520and%2520GPT-4%2520Turbo%2520have%2520an%2520excellent%2520alignment%2520with%2520humans%252C%2520but%2520in%250Aterms%2520of%2520ranking%2520exam%2520taker%2520models%252C%2520they%2520are%2520outperformed%2520by%2520both%2520JudgeLM-7B%250Aand%2520the%2520lexical%2520judge%2520Contains%252C%2520which%2520have%2520up%2520to%252034%2520points%2520lower%2520human%250Aalignment.%2520Through%2520error%2520analysis%2520and%2520various%2520other%2520studies%252C%2520including%2520the%250Aeffects%2520of%2520instruction%2520length%2520and%2520leniency%2520bias%252C%2520we%2520hope%2520to%2520provide%2520valuable%250Alessons%2520for%2520using%2520LLMs%2520as%2520judges%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Judging%20the%20Judges%3A%20Evaluating%20Alignment%20and%20Vulnerabilities%20in%0A%20%20LLMs-as-Judges&entry.906535625=Aman%20Singh%20Thakur%20and%20Kartik%20Choudhary%20and%20Venkat%20Srinik%20Ramayapally%20and%20Sankaran%20Vaidyanathan%20and%20Dieuwke%20Hupkes&entry.1292438233=%20%20Offering%20a%20promising%20solution%20to%20the%20scalability%20challenges%20associated%20with%0Ahuman%20evaluation%2C%20the%20LLM-as-a-judge%20paradigm%20is%20rapidly%20gaining%20traction%20as%20an%0Aapproach%20to%20evaluating%20large%20language%20models%20%28LLMs%29.%20However%2C%20there%20are%20still%0Amany%20open%20questions%20about%20the%20strengths%20and%20weaknesses%20of%20this%20paradigm%2C%20and%0Awhat%20potential%20biases%20it%20may%20hold.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%0Astudy%20of%20the%20performance%20of%20various%20LLMs%20acting%20as%20judges.%20We%20leverage%20TriviaQA%0Aas%20a%20benchmark%20for%20assessing%20objective%20knowledge%20reasoning%20of%20LLMs%20and%20evaluate%0Athem%20alongside%20human%20annotations%20which%20we%20found%20to%20have%20a%20high%20inter-annotator%0Aagreement.%20Our%20study%20includes%209%20judge%20models%20and%209%20exam%20taker%20models%20--%20both%0Abase%20and%20instruction-tuned.%20We%20assess%20the%20judge%20model%27s%20alignment%20across%0Adifferent%20model%20sizes%2C%20families%2C%20and%20judge%20prompts.%20Among%20other%20results%2C%20our%0Aresearch%20rediscovers%20the%20importance%20of%20using%20Cohen%27s%20kappa%20as%20a%20metric%20of%0Aalignment%20as%20opposed%20to%20simple%20percent%20agreement%2C%20showing%20that%20judges%20with%20high%0Apercent%20agreement%20can%20still%20assign%20vastly%20different%20scores.%20We%20find%20that%20both%0ALlama-3%2070B%20and%20GPT-4%20Turbo%20have%20an%20excellent%20alignment%20with%20humans%2C%20but%20in%0Aterms%20of%20ranking%20exam%20taker%20models%2C%20they%20are%20outperformed%20by%20both%20JudgeLM-7B%0Aand%20the%20lexical%20judge%20Contains%2C%20which%20have%20up%20to%2034%20points%20lower%20human%0Aalignment.%20Through%20error%20analysis%20and%20various%20other%20studies%2C%20including%20the%0Aeffects%20of%20instruction%20length%20and%20leniency%20bias%2C%20we%20hope%20to%20provide%20valuable%0Alessons%20for%20using%20LLMs%20as%20judges%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12624v1&entry.124074799=Read"},
{"title": "In-Context Learning of Energy Functions", "author": "Rylan Schaeffer and Mikail Khona and Sanmi Koyejo", "abstract": "  In-context learning is a powerful capability of certain machine learning\nmodels that arguably underpins the success of today's frontier AI models.\nHowever, in-context learning is critically limited to settings where the\nin-context distribution of interest $p_{\\theta}^{ICL}( x|\\mathcal{D})$ can be\nstraightforwardly expressed and/or parameterized by the model; for instance,\nlanguage modeling relies on expressing the next-token distribution as a\ncategorical distribution parameterized by the network's output logits. In this\nwork, we present a more general form of in-context learning without such a\nlimitation that we call \\textit{in-context learning of energy functions}. The\nidea is to instead learn the unconstrained and arbitrary in-context energy\nfunction $E_{\\theta}^{ICL}(x|\\mathcal{D})$ corresponding to the in-context\ndistribution $p_{\\theta}^{ICL}(x|\\mathcal{D})$. To do this, we use classic\nideas from energy-based modeling. We provide preliminary evidence that our\nmethod empirically works on synthetic data. Interestingly, our work contributes\n(to the best of our knowledge) the first example of in-context learning where\nthe input space and output space differ from one another, suggesting that\nin-context learning is a more-general capability than previously realized.\n", "link": "http://arxiv.org/abs/2406.12785v1", "date": "2024-06-18", "relevancy": 1.9317, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.465}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Learning%20of%20Energy%20Functions&body=Title%3A%20In-Context%20Learning%20of%20Energy%20Functions%0AAuthor%3A%20Rylan%20Schaeffer%20and%20Mikail%20Khona%20and%20Sanmi%20Koyejo%0AAbstract%3A%20%20%20In-context%20learning%20is%20a%20powerful%20capability%20of%20certain%20machine%20learning%0Amodels%20that%20arguably%20underpins%20the%20success%20of%20today%27s%20frontier%20AI%20models.%0AHowever%2C%20in-context%20learning%20is%20critically%20limited%20to%20settings%20where%20the%0Ain-context%20distribution%20of%20interest%20%24p_%7B%5Ctheta%7D%5E%7BICL%7D%28%20x%7C%5Cmathcal%7BD%7D%29%24%20can%20be%0Astraightforwardly%20expressed%20and/or%20parameterized%20by%20the%20model%3B%20for%20instance%2C%0Alanguage%20modeling%20relies%20on%20expressing%20the%20next-token%20distribution%20as%20a%0Acategorical%20distribution%20parameterized%20by%20the%20network%27s%20output%20logits.%20In%20this%0Awork%2C%20we%20present%20a%20more%20general%20form%20of%20in-context%20learning%20without%20such%20a%0Alimitation%20that%20we%20call%20%5Ctextit%7Bin-context%20learning%20of%20energy%20functions%7D.%20The%0Aidea%20is%20to%20instead%20learn%20the%20unconstrained%20and%20arbitrary%20in-context%20energy%0Afunction%20%24E_%7B%5Ctheta%7D%5E%7BICL%7D%28x%7C%5Cmathcal%7BD%7D%29%24%20corresponding%20to%20the%20in-context%0Adistribution%20%24p_%7B%5Ctheta%7D%5E%7BICL%7D%28x%7C%5Cmathcal%7BD%7D%29%24.%20To%20do%20this%2C%20we%20use%20classic%0Aideas%20from%20energy-based%20modeling.%20We%20provide%20preliminary%20evidence%20that%20our%0Amethod%20empirically%20works%20on%20synthetic%20data.%20Interestingly%2C%20our%20work%20contributes%0A%28to%20the%20best%20of%20our%20knowledge%29%20the%20first%20example%20of%20in-context%20learning%20where%0Athe%20input%20space%20and%20output%20space%20differ%20from%20one%20another%2C%20suggesting%20that%0Ain-context%20learning%20is%20a%20more-general%20capability%20than%20previously%20realized.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Learning%2520of%2520Energy%2520Functions%26entry.906535625%3DRylan%2520Schaeffer%2520and%2520Mikail%2520Khona%2520and%2520Sanmi%2520Koyejo%26entry.1292438233%3D%2520%2520In-context%2520learning%2520is%2520a%2520powerful%2520capability%2520of%2520certain%2520machine%2520learning%250Amodels%2520that%2520arguably%2520underpins%2520the%2520success%2520of%2520today%2527s%2520frontier%2520AI%2520models.%250AHowever%252C%2520in-context%2520learning%2520is%2520critically%2520limited%2520to%2520settings%2520where%2520the%250Ain-context%2520distribution%2520of%2520interest%2520%2524p_%257B%255Ctheta%257D%255E%257BICL%257D%2528%2520x%257C%255Cmathcal%257BD%257D%2529%2524%2520can%2520be%250Astraightforwardly%2520expressed%2520and/or%2520parameterized%2520by%2520the%2520model%253B%2520for%2520instance%252C%250Alanguage%2520modeling%2520relies%2520on%2520expressing%2520the%2520next-token%2520distribution%2520as%2520a%250Acategorical%2520distribution%2520parameterized%2520by%2520the%2520network%2527s%2520output%2520logits.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520more%2520general%2520form%2520of%2520in-context%2520learning%2520without%2520such%2520a%250Alimitation%2520that%2520we%2520call%2520%255Ctextit%257Bin-context%2520learning%2520of%2520energy%2520functions%257D.%2520The%250Aidea%2520is%2520to%2520instead%2520learn%2520the%2520unconstrained%2520and%2520arbitrary%2520in-context%2520energy%250Afunction%2520%2524E_%257B%255Ctheta%257D%255E%257BICL%257D%2528x%257C%255Cmathcal%257BD%257D%2529%2524%2520corresponding%2520to%2520the%2520in-context%250Adistribution%2520%2524p_%257B%255Ctheta%257D%255E%257BICL%257D%2528x%257C%255Cmathcal%257BD%257D%2529%2524.%2520To%2520do%2520this%252C%2520we%2520use%2520classic%250Aideas%2520from%2520energy-based%2520modeling.%2520We%2520provide%2520preliminary%2520evidence%2520that%2520our%250Amethod%2520empirically%2520works%2520on%2520synthetic%2520data.%2520Interestingly%252C%2520our%2520work%2520contributes%250A%2528to%2520the%2520best%2520of%2520our%2520knowledge%2529%2520the%2520first%2520example%2520of%2520in-context%2520learning%2520where%250Athe%2520input%2520space%2520and%2520output%2520space%2520differ%2520from%2520one%2520another%252C%2520suggesting%2520that%250Ain-context%2520learning%2520is%2520a%2520more-general%2520capability%2520than%2520previously%2520realized.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Learning%20of%20Energy%20Functions&entry.906535625=Rylan%20Schaeffer%20and%20Mikail%20Khona%20and%20Sanmi%20Koyejo&entry.1292438233=%20%20In-context%20learning%20is%20a%20powerful%20capability%20of%20certain%20machine%20learning%0Amodels%20that%20arguably%20underpins%20the%20success%20of%20today%27s%20frontier%20AI%20models.%0AHowever%2C%20in-context%20learning%20is%20critically%20limited%20to%20settings%20where%20the%0Ain-context%20distribution%20of%20interest%20%24p_%7B%5Ctheta%7D%5E%7BICL%7D%28%20x%7C%5Cmathcal%7BD%7D%29%24%20can%20be%0Astraightforwardly%20expressed%20and/or%20parameterized%20by%20the%20model%3B%20for%20instance%2C%0Alanguage%20modeling%20relies%20on%20expressing%20the%20next-token%20distribution%20as%20a%0Acategorical%20distribution%20parameterized%20by%20the%20network%27s%20output%20logits.%20In%20this%0Awork%2C%20we%20present%20a%20more%20general%20form%20of%20in-context%20learning%20without%20such%20a%0Alimitation%20that%20we%20call%20%5Ctextit%7Bin-context%20learning%20of%20energy%20functions%7D.%20The%0Aidea%20is%20to%20instead%20learn%20the%20unconstrained%20and%20arbitrary%20in-context%20energy%0Afunction%20%24E_%7B%5Ctheta%7D%5E%7BICL%7D%28x%7C%5Cmathcal%7BD%7D%29%24%20corresponding%20to%20the%20in-context%0Adistribution%20%24p_%7B%5Ctheta%7D%5E%7BICL%7D%28x%7C%5Cmathcal%7BD%7D%29%24.%20To%20do%20this%2C%20we%20use%20classic%0Aideas%20from%20energy-based%20modeling.%20We%20provide%20preliminary%20evidence%20that%20our%0Amethod%20empirically%20works%20on%20synthetic%20data.%20Interestingly%2C%20our%20work%20contributes%0A%28to%20the%20best%20of%20our%20knowledge%29%20the%20first%20example%20of%20in-context%20learning%20where%0Athe%20input%20space%20and%20output%20space%20differ%20from%20one%20another%2C%20suggesting%20that%0Ain-context%20learning%20is%20a%20more-general%20capability%20than%20previously%20realized.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12785v1&entry.124074799=Read"},
{"title": "Spatial Sequence Attention Network for Schizophrenia Classification from\n  Structural Brain MR Images", "author": "Nagur Shareef Shaik and Teja Krishna Cherukuri and Vince Calhoun and Dong Hye Ye", "abstract": "  Schizophrenia is a debilitating, chronic mental disorder that significantly\nimpacts an individual's cognitive abilities, behavior, and social interactions.\nIt is characterized by subtle morphological changes in the brain, particularly\nin the gray matter. These changes are often imperceptible through manual\nobservation, demanding an automated approach to diagnosis. This study\nintroduces a deep learning methodology for the classification of individuals\nwith Schizophrenia. We achieve this by implementing a diversified attention\nmechanism known as Spatial Sequence Attention (SSA) which is designed to\nextract and emphasize significant feature representations from structural MRI\n(sMRI). Initially, we employ the transfer learning paradigm by leveraging\npre-trained DenseNet to extract initial feature maps from the final\nconvolutional block which contains morphological alterations associated with\nSchizophrenia. These features are further processed by the proposed SSA to\ncapture and emphasize intricate spatial interactions and relationships across\nvolumes within the brain. Our experimental studies conducted on a clinical\ndataset have revealed that the proposed attention mechanism outperforms the\nexisting Squeeze & Excitation Network for Schizophrenia classification.\n", "link": "http://arxiv.org/abs/2406.12683v1", "date": "2024-06-18", "relevancy": 1.9312, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4965}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.482}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Sequence%20Attention%20Network%20for%20Schizophrenia%20Classification%20from%0A%20%20Structural%20Brain%20MR%20Images&body=Title%3A%20Spatial%20Sequence%20Attention%20Network%20for%20Schizophrenia%20Classification%20from%0A%20%20Structural%20Brain%20MR%20Images%0AAuthor%3A%20Nagur%20Shareef%20Shaik%20and%20Teja%20Krishna%20Cherukuri%20and%20Vince%20Calhoun%20and%20Dong%20Hye%20Ye%0AAbstract%3A%20%20%20Schizophrenia%20is%20a%20debilitating%2C%20chronic%20mental%20disorder%20that%20significantly%0Aimpacts%20an%20individual%27s%20cognitive%20abilities%2C%20behavior%2C%20and%20social%20interactions.%0AIt%20is%20characterized%20by%20subtle%20morphological%20changes%20in%20the%20brain%2C%20particularly%0Ain%20the%20gray%20matter.%20These%20changes%20are%20often%20imperceptible%20through%20manual%0Aobservation%2C%20demanding%20an%20automated%20approach%20to%20diagnosis.%20This%20study%0Aintroduces%20a%20deep%20learning%20methodology%20for%20the%20classification%20of%20individuals%0Awith%20Schizophrenia.%20We%20achieve%20this%20by%20implementing%20a%20diversified%20attention%0Amechanism%20known%20as%20Spatial%20Sequence%20Attention%20%28SSA%29%20which%20is%20designed%20to%0Aextract%20and%20emphasize%20significant%20feature%20representations%20from%20structural%20MRI%0A%28sMRI%29.%20Initially%2C%20we%20employ%20the%20transfer%20learning%20paradigm%20by%20leveraging%0Apre-trained%20DenseNet%20to%20extract%20initial%20feature%20maps%20from%20the%20final%0Aconvolutional%20block%20which%20contains%20morphological%20alterations%20associated%20with%0ASchizophrenia.%20These%20features%20are%20further%20processed%20by%20the%20proposed%20SSA%20to%0Acapture%20and%20emphasize%20intricate%20spatial%20interactions%20and%20relationships%20across%0Avolumes%20within%20the%20brain.%20Our%20experimental%20studies%20conducted%20on%20a%20clinical%0Adataset%20have%20revealed%20that%20the%20proposed%20attention%20mechanism%20outperforms%20the%0Aexisting%20Squeeze%20%26%20Excitation%20Network%20for%20Schizophrenia%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Sequence%2520Attention%2520Network%2520for%2520Schizophrenia%2520Classification%2520from%250A%2520%2520Structural%2520Brain%2520MR%2520Images%26entry.906535625%3DNagur%2520Shareef%2520Shaik%2520and%2520Teja%2520Krishna%2520Cherukuri%2520and%2520Vince%2520Calhoun%2520and%2520Dong%2520Hye%2520Ye%26entry.1292438233%3D%2520%2520Schizophrenia%2520is%2520a%2520debilitating%252C%2520chronic%2520mental%2520disorder%2520that%2520significantly%250Aimpacts%2520an%2520individual%2527s%2520cognitive%2520abilities%252C%2520behavior%252C%2520and%2520social%2520interactions.%250AIt%2520is%2520characterized%2520by%2520subtle%2520morphological%2520changes%2520in%2520the%2520brain%252C%2520particularly%250Ain%2520the%2520gray%2520matter.%2520These%2520changes%2520are%2520often%2520imperceptible%2520through%2520manual%250Aobservation%252C%2520demanding%2520an%2520automated%2520approach%2520to%2520diagnosis.%2520This%2520study%250Aintroduces%2520a%2520deep%2520learning%2520methodology%2520for%2520the%2520classification%2520of%2520individuals%250Awith%2520Schizophrenia.%2520We%2520achieve%2520this%2520by%2520implementing%2520a%2520diversified%2520attention%250Amechanism%2520known%2520as%2520Spatial%2520Sequence%2520Attention%2520%2528SSA%2529%2520which%2520is%2520designed%2520to%250Aextract%2520and%2520emphasize%2520significant%2520feature%2520representations%2520from%2520structural%2520MRI%250A%2528sMRI%2529.%2520Initially%252C%2520we%2520employ%2520the%2520transfer%2520learning%2520paradigm%2520by%2520leveraging%250Apre-trained%2520DenseNet%2520to%2520extract%2520initial%2520feature%2520maps%2520from%2520the%2520final%250Aconvolutional%2520block%2520which%2520contains%2520morphological%2520alterations%2520associated%2520with%250ASchizophrenia.%2520These%2520features%2520are%2520further%2520processed%2520by%2520the%2520proposed%2520SSA%2520to%250Acapture%2520and%2520emphasize%2520intricate%2520spatial%2520interactions%2520and%2520relationships%2520across%250Avolumes%2520within%2520the%2520brain.%2520Our%2520experimental%2520studies%2520conducted%2520on%2520a%2520clinical%250Adataset%2520have%2520revealed%2520that%2520the%2520proposed%2520attention%2520mechanism%2520outperforms%2520the%250Aexisting%2520Squeeze%2520%2526%2520Excitation%2520Network%2520for%2520Schizophrenia%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Sequence%20Attention%20Network%20for%20Schizophrenia%20Classification%20from%0A%20%20Structural%20Brain%20MR%20Images&entry.906535625=Nagur%20Shareef%20Shaik%20and%20Teja%20Krishna%20Cherukuri%20and%20Vince%20Calhoun%20and%20Dong%20Hye%20Ye&entry.1292438233=%20%20Schizophrenia%20is%20a%20debilitating%2C%20chronic%20mental%20disorder%20that%20significantly%0Aimpacts%20an%20individual%27s%20cognitive%20abilities%2C%20behavior%2C%20and%20social%20interactions.%0AIt%20is%20characterized%20by%20subtle%20morphological%20changes%20in%20the%20brain%2C%20particularly%0Ain%20the%20gray%20matter.%20These%20changes%20are%20often%20imperceptible%20through%20manual%0Aobservation%2C%20demanding%20an%20automated%20approach%20to%20diagnosis.%20This%20study%0Aintroduces%20a%20deep%20learning%20methodology%20for%20the%20classification%20of%20individuals%0Awith%20Schizophrenia.%20We%20achieve%20this%20by%20implementing%20a%20diversified%20attention%0Amechanism%20known%20as%20Spatial%20Sequence%20Attention%20%28SSA%29%20which%20is%20designed%20to%0Aextract%20and%20emphasize%20significant%20feature%20representations%20from%20structural%20MRI%0A%28sMRI%29.%20Initially%2C%20we%20employ%20the%20transfer%20learning%20paradigm%20by%20leveraging%0Apre-trained%20DenseNet%20to%20extract%20initial%20feature%20maps%20from%20the%20final%0Aconvolutional%20block%20which%20contains%20morphological%20alterations%20associated%20with%0ASchizophrenia.%20These%20features%20are%20further%20processed%20by%20the%20proposed%20SSA%20to%0Acapture%20and%20emphasize%20intricate%20spatial%20interactions%20and%20relationships%20across%0Avolumes%20within%20the%20brain.%20Our%20experimental%20studies%20conducted%20on%20a%20clinical%0Adataset%20have%20revealed%20that%20the%20proposed%20attention%20mechanism%20outperforms%20the%0Aexisting%20Squeeze%20%26%20Excitation%20Network%20for%20Schizophrenia%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12683v1&entry.124074799=Read"},
{"title": "TREE: Tree Regularization for Efficient Execution", "author": "Lena Schmid and Daniel Biebert and Christian Hakert and Kuan-Hsun Chen and Michel Lang and Markus Pauly and Jian-Jia Chen", "abstract": "  The rise of machine learning methods on heavily resource constrained devices\nrequires not only the choice of a suitable model architecture for the target\nplatform, but also the optimization of the chosen model with regard to\nexecution time consumption for inference in order to optimally utilize the\navailable resources. Random forests and decision trees are shown to be a\nsuitable model for such a scenario, since they are not only heavily tunable\ntowards the total model size, but also offer a high potential for optimizing\ntheir executions according to the underlying memory architecture.\n  In addition to the straightforward strategy of enforcing shorter paths\nthrough decision trees and hence reducing the execution time for inference,\nhardware-aware implementations can optimize the execution time in an orthogonal\nmanner. One particular hardware-aware optimization is to layout the memory of\ndecision trees in such a way, that higher probably paths are less likely to be\nevicted from system caches. This works particularly well when splits within\ntree nodes are uneven and have a high probability to visit one of the child\nnodes.\n  In this paper, we present a method to reduce path lengths by rewarding uneven\nprobability distributions during the training of decision trees at the cost of\na minimal accuracy degradation. Specifically, we regularize the impurity\ncomputation of the CART algorithm in order to favor not only low impurity, but\nalso highly asymmetric distributions for the evaluation of split criteria and\nhence offer a high optimization potential for a memory architecture-aware\nimplementation.\n  We show that especially for binary classification data sets and data sets\nwith many samples, this form of regularization can lead to an reduction of up\nto approximately four times in the execution time with a minimal accuracy\ndegradation.\n", "link": "http://arxiv.org/abs/2406.12531v1", "date": "2024-06-18", "relevancy": 1.9287, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4733}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TREE%3A%20Tree%20Regularization%20for%20Efficient%20Execution&body=Title%3A%20TREE%3A%20Tree%20Regularization%20for%20Efficient%20Execution%0AAuthor%3A%20Lena%20Schmid%20and%20Daniel%20Biebert%20and%20Christian%20Hakert%20and%20Kuan-Hsun%20Chen%20and%20Michel%20Lang%20and%20Markus%20Pauly%20and%20Jian-Jia%20Chen%0AAbstract%3A%20%20%20The%20rise%20of%20machine%20learning%20methods%20on%20heavily%20resource%20constrained%20devices%0Arequires%20not%20only%20the%20choice%20of%20a%20suitable%20model%20architecture%20for%20the%20target%0Aplatform%2C%20but%20also%20the%20optimization%20of%20the%20chosen%20model%20with%20regard%20to%0Aexecution%20time%20consumption%20for%20inference%20in%20order%20to%20optimally%20utilize%20the%0Aavailable%20resources.%20Random%20forests%20and%20decision%20trees%20are%20shown%20to%20be%20a%0Asuitable%20model%20for%20such%20a%20scenario%2C%20since%20they%20are%20not%20only%20heavily%20tunable%0Atowards%20the%20total%20model%20size%2C%20but%20also%20offer%20a%20high%20potential%20for%20optimizing%0Atheir%20executions%20according%20to%20the%20underlying%20memory%20architecture.%0A%20%20In%20addition%20to%20the%20straightforward%20strategy%20of%20enforcing%20shorter%20paths%0Athrough%20decision%20trees%20and%20hence%20reducing%20the%20execution%20time%20for%20inference%2C%0Ahardware-aware%20implementations%20can%20optimize%20the%20execution%20time%20in%20an%20orthogonal%0Amanner.%20One%20particular%20hardware-aware%20optimization%20is%20to%20layout%20the%20memory%20of%0Adecision%20trees%20in%20such%20a%20way%2C%20that%20higher%20probably%20paths%20are%20less%20likely%20to%20be%0Aevicted%20from%20system%20caches.%20This%20works%20particularly%20well%20when%20splits%20within%0Atree%20nodes%20are%20uneven%20and%20have%20a%20high%20probability%20to%20visit%20one%20of%20the%20child%0Anodes.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20method%20to%20reduce%20path%20lengths%20by%20rewarding%20uneven%0Aprobability%20distributions%20during%20the%20training%20of%20decision%20trees%20at%20the%20cost%20of%0Aa%20minimal%20accuracy%20degradation.%20Specifically%2C%20we%20regularize%20the%20impurity%0Acomputation%20of%20the%20CART%20algorithm%20in%20order%20to%20favor%20not%20only%20low%20impurity%2C%20but%0Aalso%20highly%20asymmetric%20distributions%20for%20the%20evaluation%20of%20split%20criteria%20and%0Ahence%20offer%20a%20high%20optimization%20potential%20for%20a%20memory%20architecture-aware%0Aimplementation.%0A%20%20We%20show%20that%20especially%20for%20binary%20classification%20data%20sets%20and%20data%20sets%0Awith%20many%20samples%2C%20this%20form%20of%20regularization%20can%20lead%20to%20an%20reduction%20of%20up%0Ato%20approximately%20four%20times%20in%20the%20execution%20time%20with%20a%20minimal%20accuracy%0Adegradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTREE%253A%2520Tree%2520Regularization%2520for%2520Efficient%2520Execution%26entry.906535625%3DLena%2520Schmid%2520and%2520Daniel%2520Biebert%2520and%2520Christian%2520Hakert%2520and%2520Kuan-Hsun%2520Chen%2520and%2520Michel%2520Lang%2520and%2520Markus%2520Pauly%2520and%2520Jian-Jia%2520Chen%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520machine%2520learning%2520methods%2520on%2520heavily%2520resource%2520constrained%2520devices%250Arequires%2520not%2520only%2520the%2520choice%2520of%2520a%2520suitable%2520model%2520architecture%2520for%2520the%2520target%250Aplatform%252C%2520but%2520also%2520the%2520optimization%2520of%2520the%2520chosen%2520model%2520with%2520regard%2520to%250Aexecution%2520time%2520consumption%2520for%2520inference%2520in%2520order%2520to%2520optimally%2520utilize%2520the%250Aavailable%2520resources.%2520Random%2520forests%2520and%2520decision%2520trees%2520are%2520shown%2520to%2520be%2520a%250Asuitable%2520model%2520for%2520such%2520a%2520scenario%252C%2520since%2520they%2520are%2520not%2520only%2520heavily%2520tunable%250Atowards%2520the%2520total%2520model%2520size%252C%2520but%2520also%2520offer%2520a%2520high%2520potential%2520for%2520optimizing%250Atheir%2520executions%2520according%2520to%2520the%2520underlying%2520memory%2520architecture.%250A%2520%2520In%2520addition%2520to%2520the%2520straightforward%2520strategy%2520of%2520enforcing%2520shorter%2520paths%250Athrough%2520decision%2520trees%2520and%2520hence%2520reducing%2520the%2520execution%2520time%2520for%2520inference%252C%250Ahardware-aware%2520implementations%2520can%2520optimize%2520the%2520execution%2520time%2520in%2520an%2520orthogonal%250Amanner.%2520One%2520particular%2520hardware-aware%2520optimization%2520is%2520to%2520layout%2520the%2520memory%2520of%250Adecision%2520trees%2520in%2520such%2520a%2520way%252C%2520that%2520higher%2520probably%2520paths%2520are%2520less%2520likely%2520to%2520be%250Aevicted%2520from%2520system%2520caches.%2520This%2520works%2520particularly%2520well%2520when%2520splits%2520within%250Atree%2520nodes%2520are%2520uneven%2520and%2520have%2520a%2520high%2520probability%2520to%2520visit%2520one%2520of%2520the%2520child%250Anodes.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520method%2520to%2520reduce%2520path%2520lengths%2520by%2520rewarding%2520uneven%250Aprobability%2520distributions%2520during%2520the%2520training%2520of%2520decision%2520trees%2520at%2520the%2520cost%2520of%250Aa%2520minimal%2520accuracy%2520degradation.%2520Specifically%252C%2520we%2520regularize%2520the%2520impurity%250Acomputation%2520of%2520the%2520CART%2520algorithm%2520in%2520order%2520to%2520favor%2520not%2520only%2520low%2520impurity%252C%2520but%250Aalso%2520highly%2520asymmetric%2520distributions%2520for%2520the%2520evaluation%2520of%2520split%2520criteria%2520and%250Ahence%2520offer%2520a%2520high%2520optimization%2520potential%2520for%2520a%2520memory%2520architecture-aware%250Aimplementation.%250A%2520%2520We%2520show%2520that%2520especially%2520for%2520binary%2520classification%2520data%2520sets%2520and%2520data%2520sets%250Awith%2520many%2520samples%252C%2520this%2520form%2520of%2520regularization%2520can%2520lead%2520to%2520an%2520reduction%2520of%2520up%250Ato%2520approximately%2520four%2520times%2520in%2520the%2520execution%2520time%2520with%2520a%2520minimal%2520accuracy%250Adegradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TREE%3A%20Tree%20Regularization%20for%20Efficient%20Execution&entry.906535625=Lena%20Schmid%20and%20Daniel%20Biebert%20and%20Christian%20Hakert%20and%20Kuan-Hsun%20Chen%20and%20Michel%20Lang%20and%20Markus%20Pauly%20and%20Jian-Jia%20Chen&entry.1292438233=%20%20The%20rise%20of%20machine%20learning%20methods%20on%20heavily%20resource%20constrained%20devices%0Arequires%20not%20only%20the%20choice%20of%20a%20suitable%20model%20architecture%20for%20the%20target%0Aplatform%2C%20but%20also%20the%20optimization%20of%20the%20chosen%20model%20with%20regard%20to%0Aexecution%20time%20consumption%20for%20inference%20in%20order%20to%20optimally%20utilize%20the%0Aavailable%20resources.%20Random%20forests%20and%20decision%20trees%20are%20shown%20to%20be%20a%0Asuitable%20model%20for%20such%20a%20scenario%2C%20since%20they%20are%20not%20only%20heavily%20tunable%0Atowards%20the%20total%20model%20size%2C%20but%20also%20offer%20a%20high%20potential%20for%20optimizing%0Atheir%20executions%20according%20to%20the%20underlying%20memory%20architecture.%0A%20%20In%20addition%20to%20the%20straightforward%20strategy%20of%20enforcing%20shorter%20paths%0Athrough%20decision%20trees%20and%20hence%20reducing%20the%20execution%20time%20for%20inference%2C%0Ahardware-aware%20implementations%20can%20optimize%20the%20execution%20time%20in%20an%20orthogonal%0Amanner.%20One%20particular%20hardware-aware%20optimization%20is%20to%20layout%20the%20memory%20of%0Adecision%20trees%20in%20such%20a%20way%2C%20that%20higher%20probably%20paths%20are%20less%20likely%20to%20be%0Aevicted%20from%20system%20caches.%20This%20works%20particularly%20well%20when%20splits%20within%0Atree%20nodes%20are%20uneven%20and%20have%20a%20high%20probability%20to%20visit%20one%20of%20the%20child%0Anodes.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20method%20to%20reduce%20path%20lengths%20by%20rewarding%20uneven%0Aprobability%20distributions%20during%20the%20training%20of%20decision%20trees%20at%20the%20cost%20of%0Aa%20minimal%20accuracy%20degradation.%20Specifically%2C%20we%20regularize%20the%20impurity%0Acomputation%20of%20the%20CART%20algorithm%20in%20order%20to%20favor%20not%20only%20low%20impurity%2C%20but%0Aalso%20highly%20asymmetric%20distributions%20for%20the%20evaluation%20of%20split%20criteria%20and%0Ahence%20offer%20a%20high%20optimization%20potential%20for%20a%20memory%20architecture-aware%0Aimplementation.%0A%20%20We%20show%20that%20especially%20for%20binary%20classification%20data%20sets%20and%20data%20sets%0Awith%20many%20samples%2C%20this%20form%20of%20regularization%20can%20lead%20to%20an%20reduction%20of%20up%0Ato%20approximately%20four%20times%20in%20the%20execution%20time%20with%20a%20minimal%20accuracy%0Adegradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12531v1&entry.124074799=Read"},
{"title": "Synergizing Foundation Models and Federated Learning: A Survey", "author": "Shenghui Li and Fanghua Ye and Meng Fang and Jiaxu Zhao and Yun-Hin Chan and Edith C. -H. Ngai and Thiemo Voigt", "abstract": "  The recent development of Foundation Models (FMs), represented by large\nlanguage models, vision transformers, and multimodal models, has been making a\nsignificant impact on both academia and industry. Compared with small-scale\nmodels, FMs have a much stronger demand for high-volume data during the\npre-training phase. Although general FMs can be pre-trained on data collected\nfrom open sources such as the Internet, domain-specific FMs need proprietary\ndata, posing a practical challenge regarding the amount of data available due\nto privacy concerns. Federated Learning (FL) is a collaborative learning\nparadigm that breaks the barrier of data availability from different\nparticipants. Therefore, it provides a promising solution to customize and\nadapt FMs to a wide range of domain-specific tasks using distributed datasets\nwhilst preserving privacy. This survey paper discusses the potentials and\nchallenges of synergizing FL and FMs and summarizes core techniques, future\ndirections, and applications. A periodically updated paper collection on FM-FL\nis available at https://github.com/lishenghui/awesome-fm-fl.\n", "link": "http://arxiv.org/abs/2406.12844v1", "date": "2024-06-18", "relevancy": 1.9096, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synergizing%20Foundation%20Models%20and%20Federated%20Learning%3A%20A%20Survey&body=Title%3A%20Synergizing%20Foundation%20Models%20and%20Federated%20Learning%3A%20A%20Survey%0AAuthor%3A%20Shenghui%20Li%20and%20Fanghua%20Ye%20and%20Meng%20Fang%20and%20Jiaxu%20Zhao%20and%20Yun-Hin%20Chan%20and%20Edith%20C.%20-H.%20Ngai%20and%20Thiemo%20Voigt%0AAbstract%3A%20%20%20The%20recent%20development%20of%20Foundation%20Models%20%28FMs%29%2C%20represented%20by%20large%0Alanguage%20models%2C%20vision%20transformers%2C%20and%20multimodal%20models%2C%20has%20been%20making%20a%0Asignificant%20impact%20on%20both%20academia%20and%20industry.%20Compared%20with%20small-scale%0Amodels%2C%20FMs%20have%20a%20much%20stronger%20demand%20for%20high-volume%20data%20during%20the%0Apre-training%20phase.%20Although%20general%20FMs%20can%20be%20pre-trained%20on%20data%20collected%0Afrom%20open%20sources%20such%20as%20the%20Internet%2C%20domain-specific%20FMs%20need%20proprietary%0Adata%2C%20posing%20a%20practical%20challenge%20regarding%20the%20amount%20of%20data%20available%20due%0Ato%20privacy%20concerns.%20Federated%20Learning%20%28FL%29%20is%20a%20collaborative%20learning%0Aparadigm%20that%20breaks%20the%20barrier%20of%20data%20availability%20from%20different%0Aparticipants.%20Therefore%2C%20it%20provides%20a%20promising%20solution%20to%20customize%20and%0Aadapt%20FMs%20to%20a%20wide%20range%20of%20domain-specific%20tasks%20using%20distributed%20datasets%0Awhilst%20preserving%20privacy.%20This%20survey%20paper%20discusses%20the%20potentials%20and%0Achallenges%20of%20synergizing%20FL%20and%20FMs%20and%20summarizes%20core%20techniques%2C%20future%0Adirections%2C%20and%20applications.%20A%20periodically%20updated%20paper%20collection%20on%20FM-FL%0Ais%20available%20at%20https%3A//github.com/lishenghui/awesome-fm-fl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynergizing%2520Foundation%2520Models%2520and%2520Federated%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DShenghui%2520Li%2520and%2520Fanghua%2520Ye%2520and%2520Meng%2520Fang%2520and%2520Jiaxu%2520Zhao%2520and%2520Yun-Hin%2520Chan%2520and%2520Edith%2520C.%2520-H.%2520Ngai%2520and%2520Thiemo%2520Voigt%26entry.1292438233%3D%2520%2520The%2520recent%2520development%2520of%2520Foundation%2520Models%2520%2528FMs%2529%252C%2520represented%2520by%2520large%250Alanguage%2520models%252C%2520vision%2520transformers%252C%2520and%2520multimodal%2520models%252C%2520has%2520been%2520making%2520a%250Asignificant%2520impact%2520on%2520both%2520academia%2520and%2520industry.%2520Compared%2520with%2520small-scale%250Amodels%252C%2520FMs%2520have%2520a%2520much%2520stronger%2520demand%2520for%2520high-volume%2520data%2520during%2520the%250Apre-training%2520phase.%2520Although%2520general%2520FMs%2520can%2520be%2520pre-trained%2520on%2520data%2520collected%250Afrom%2520open%2520sources%2520such%2520as%2520the%2520Internet%252C%2520domain-specific%2520FMs%2520need%2520proprietary%250Adata%252C%2520posing%2520a%2520practical%2520challenge%2520regarding%2520the%2520amount%2520of%2520data%2520available%2520due%250Ato%2520privacy%2520concerns.%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520collaborative%2520learning%250Aparadigm%2520that%2520breaks%2520the%2520barrier%2520of%2520data%2520availability%2520from%2520different%250Aparticipants.%2520Therefore%252C%2520it%2520provides%2520a%2520promising%2520solution%2520to%2520customize%2520and%250Aadapt%2520FMs%2520to%2520a%2520wide%2520range%2520of%2520domain-specific%2520tasks%2520using%2520distributed%2520datasets%250Awhilst%2520preserving%2520privacy.%2520This%2520survey%2520paper%2520discusses%2520the%2520potentials%2520and%250Achallenges%2520of%2520synergizing%2520FL%2520and%2520FMs%2520and%2520summarizes%2520core%2520techniques%252C%2520future%250Adirections%252C%2520and%2520applications.%2520A%2520periodically%2520updated%2520paper%2520collection%2520on%2520FM-FL%250Ais%2520available%2520at%2520https%253A//github.com/lishenghui/awesome-fm-fl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergizing%20Foundation%20Models%20and%20Federated%20Learning%3A%20A%20Survey&entry.906535625=Shenghui%20Li%20and%20Fanghua%20Ye%20and%20Meng%20Fang%20and%20Jiaxu%20Zhao%20and%20Yun-Hin%20Chan%20and%20Edith%20C.%20-H.%20Ngai%20and%20Thiemo%20Voigt&entry.1292438233=%20%20The%20recent%20development%20of%20Foundation%20Models%20%28FMs%29%2C%20represented%20by%20large%0Alanguage%20models%2C%20vision%20transformers%2C%20and%20multimodal%20models%2C%20has%20been%20making%20a%0Asignificant%20impact%20on%20both%20academia%20and%20industry.%20Compared%20with%20small-scale%0Amodels%2C%20FMs%20have%20a%20much%20stronger%20demand%20for%20high-volume%20data%20during%20the%0Apre-training%20phase.%20Although%20general%20FMs%20can%20be%20pre-trained%20on%20data%20collected%0Afrom%20open%20sources%20such%20as%20the%20Internet%2C%20domain-specific%20FMs%20need%20proprietary%0Adata%2C%20posing%20a%20practical%20challenge%20regarding%20the%20amount%20of%20data%20available%20due%0Ato%20privacy%20concerns.%20Federated%20Learning%20%28FL%29%20is%20a%20collaborative%20learning%0Aparadigm%20that%20breaks%20the%20barrier%20of%20data%20availability%20from%20different%0Aparticipants.%20Therefore%2C%20it%20provides%20a%20promising%20solution%20to%20customize%20and%0Aadapt%20FMs%20to%20a%20wide%20range%20of%20domain-specific%20tasks%20using%20distributed%20datasets%0Awhilst%20preserving%20privacy.%20This%20survey%20paper%20discusses%20the%20potentials%20and%0Achallenges%20of%20synergizing%20FL%20and%20FMs%20and%20summarizes%20core%20techniques%2C%20future%0Adirections%2C%20and%20applications.%20A%20periodically%20updated%20paper%20collection%20on%20FM-FL%0Ais%20available%20at%20https%3A//github.com/lishenghui/awesome-fm-fl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12844v1&entry.124074799=Read"},
{"title": "Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding\n  Dataset from Ruo Zhi Ba", "author": "Ruiqi He and Yushu He and Longju Bai and Jiarui Liu and Zhenjie Sun and Zenghao Tang and He Wang and Hanchen Xia and Naihao Deng", "abstract": "  Existing humor datasets and evaluations predominantly focus on English,\nlacking resources for culturally nuanced humor in non-English languages like\nChinese. To address this gap, we construct Chumor, a dataset sourced from Ruo\nZhi Ba (RZB), a Chinese Reddit-like platform dedicated to sharing\nintellectually challenging and culturally specific jokes. We annotate\nexplanations for each joke and evaluate human explanations against two\nstate-of-the-art LLMs, GPT-4o and ERNIE Bot, through A/B testing by native\nChinese speakers. Our evaluation shows that Chumor is challenging even for SOTA\nLLMs, and the human explanations for Chumor jokes are significantly better than\nexplanations generated by the LLMs.\n", "link": "http://arxiv.org/abs/2406.12754v1", "date": "2024-06-18", "relevancy": 1.8938, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3834}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3818}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chumor%201.0%3A%20A%20Truly%20Funny%20and%20Challenging%20Chinese%20Humor%20Understanding%0A%20%20Dataset%20from%20Ruo%20Zhi%20Ba&body=Title%3A%20Chumor%201.0%3A%20A%20Truly%20Funny%20and%20Challenging%20Chinese%20Humor%20Understanding%0A%20%20Dataset%20from%20Ruo%20Zhi%20Ba%0AAuthor%3A%20Ruiqi%20He%20and%20Yushu%20He%20and%20Longju%20Bai%20and%20Jiarui%20Liu%20and%20Zhenjie%20Sun%20and%20Zenghao%20Tang%20and%20He%20Wang%20and%20Hanchen%20Xia%20and%20Naihao%20Deng%0AAbstract%3A%20%20%20Existing%20humor%20datasets%20and%20evaluations%20predominantly%20focus%20on%20English%2C%0Alacking%20resources%20for%20culturally%20nuanced%20humor%20in%20non-English%20languages%20like%0AChinese.%20To%20address%20this%20gap%2C%20we%20construct%20Chumor%2C%20a%20dataset%20sourced%20from%20Ruo%0AZhi%20Ba%20%28RZB%29%2C%20a%20Chinese%20Reddit-like%20platform%20dedicated%20to%20sharing%0Aintellectually%20challenging%20and%20culturally%20specific%20jokes.%20We%20annotate%0Aexplanations%20for%20each%20joke%20and%20evaluate%20human%20explanations%20against%20two%0Astate-of-the-art%20LLMs%2C%20GPT-4o%20and%20ERNIE%20Bot%2C%20through%20A/B%20testing%20by%20native%0AChinese%20speakers.%20Our%20evaluation%20shows%20that%20Chumor%20is%20challenging%20even%20for%20SOTA%0ALLMs%2C%20and%20the%20human%20explanations%20for%20Chumor%20jokes%20are%20significantly%20better%20than%0Aexplanations%20generated%20by%20the%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChumor%25201.0%253A%2520A%2520Truly%2520Funny%2520and%2520Challenging%2520Chinese%2520Humor%2520Understanding%250A%2520%2520Dataset%2520from%2520Ruo%2520Zhi%2520Ba%26entry.906535625%3DRuiqi%2520He%2520and%2520Yushu%2520He%2520and%2520Longju%2520Bai%2520and%2520Jiarui%2520Liu%2520and%2520Zhenjie%2520Sun%2520and%2520Zenghao%2520Tang%2520and%2520He%2520Wang%2520and%2520Hanchen%2520Xia%2520and%2520Naihao%2520Deng%26entry.1292438233%3D%2520%2520Existing%2520humor%2520datasets%2520and%2520evaluations%2520predominantly%2520focus%2520on%2520English%252C%250Alacking%2520resources%2520for%2520culturally%2520nuanced%2520humor%2520in%2520non-English%2520languages%2520like%250AChinese.%2520To%2520address%2520this%2520gap%252C%2520we%2520construct%2520Chumor%252C%2520a%2520dataset%2520sourced%2520from%2520Ruo%250AZhi%2520Ba%2520%2528RZB%2529%252C%2520a%2520Chinese%2520Reddit-like%2520platform%2520dedicated%2520to%2520sharing%250Aintellectually%2520challenging%2520and%2520culturally%2520specific%2520jokes.%2520We%2520annotate%250Aexplanations%2520for%2520each%2520joke%2520and%2520evaluate%2520human%2520explanations%2520against%2520two%250Astate-of-the-art%2520LLMs%252C%2520GPT-4o%2520and%2520ERNIE%2520Bot%252C%2520through%2520A/B%2520testing%2520by%2520native%250AChinese%2520speakers.%2520Our%2520evaluation%2520shows%2520that%2520Chumor%2520is%2520challenging%2520even%2520for%2520SOTA%250ALLMs%252C%2520and%2520the%2520human%2520explanations%2520for%2520Chumor%2520jokes%2520are%2520significantly%2520better%2520than%250Aexplanations%2520generated%2520by%2520the%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chumor%201.0%3A%20A%20Truly%20Funny%20and%20Challenging%20Chinese%20Humor%20Understanding%0A%20%20Dataset%20from%20Ruo%20Zhi%20Ba&entry.906535625=Ruiqi%20He%20and%20Yushu%20He%20and%20Longju%20Bai%20and%20Jiarui%20Liu%20and%20Zhenjie%20Sun%20and%20Zenghao%20Tang%20and%20He%20Wang%20and%20Hanchen%20Xia%20and%20Naihao%20Deng&entry.1292438233=%20%20Existing%20humor%20datasets%20and%20evaluations%20predominantly%20focus%20on%20English%2C%0Alacking%20resources%20for%20culturally%20nuanced%20humor%20in%20non-English%20languages%20like%0AChinese.%20To%20address%20this%20gap%2C%20we%20construct%20Chumor%2C%20a%20dataset%20sourced%20from%20Ruo%0AZhi%20Ba%20%28RZB%29%2C%20a%20Chinese%20Reddit-like%20platform%20dedicated%20to%20sharing%0Aintellectually%20challenging%20and%20culturally%20specific%20jokes.%20We%20annotate%0Aexplanations%20for%20each%20joke%20and%20evaluate%20human%20explanations%20against%20two%0Astate-of-the-art%20LLMs%2C%20GPT-4o%20and%20ERNIE%20Bot%2C%20through%20A/B%20testing%20by%20native%0AChinese%20speakers.%20Our%20evaluation%20shows%20that%20Chumor%20is%20challenging%20even%20for%20SOTA%0ALLMs%2C%20and%20the%20human%20explanations%20for%20Chumor%20jokes%20are%20significantly%20better%20than%0Aexplanations%20generated%20by%20the%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12754v1&entry.124074799=Read"},
{"title": "Attack and Defense of Deep Learning Models in the Field of Web Attack\n  Detection", "author": "Lijia Shi and Shihao Dong", "abstract": "  The challenge of WAD (web attack detection) is growing as hackers\ncontinuously refine their methods to evade traditional detection. Deep learning\nmodels excel in handling complex unknown attacks due to their strong\ngeneralization and adaptability. However, they are vulnerable to backdoor\nattacks, where contextually irrelevant fragments are inserted into requests,\ncompromising model stability. While backdoor attacks are well studied in image\nrecognition, they are largely unexplored in WAD. This paper introduces backdoor\nattacks in WAD, proposing five methods and corresponding defenses. Testing on\ntextCNN, biLSTM, and tinybert models shows an attack success rate over 87%,\nreducible through fine-tuning. Future research should focus on backdoor\ndefenses in WAD. All the code and data of this paper can be obtained at\nhttps://anonymous.4open.science/r/attackDefenceinDL-7E05\n", "link": "http://arxiv.org/abs/2406.12605v1", "date": "2024-06-18", "relevancy": 1.8938, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.475}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4744}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attack%20and%20Defense%20of%20Deep%20Learning%20Models%20in%20the%20Field%20of%20Web%20Attack%0A%20%20Detection&body=Title%3A%20Attack%20and%20Defense%20of%20Deep%20Learning%20Models%20in%20the%20Field%20of%20Web%20Attack%0A%20%20Detection%0AAuthor%3A%20Lijia%20Shi%20and%20Shihao%20Dong%0AAbstract%3A%20%20%20The%20challenge%20of%20WAD%20%28web%20attack%20detection%29%20is%20growing%20as%20hackers%0Acontinuously%20refine%20their%20methods%20to%20evade%20traditional%20detection.%20Deep%20learning%0Amodels%20excel%20in%20handling%20complex%20unknown%20attacks%20due%20to%20their%20strong%0Ageneralization%20and%20adaptability.%20However%2C%20they%20are%20vulnerable%20to%20backdoor%0Aattacks%2C%20where%20contextually%20irrelevant%20fragments%20are%20inserted%20into%20requests%2C%0Acompromising%20model%20stability.%20While%20backdoor%20attacks%20are%20well%20studied%20in%20image%0Arecognition%2C%20they%20are%20largely%20unexplored%20in%20WAD.%20This%20paper%20introduces%20backdoor%0Aattacks%20in%20WAD%2C%20proposing%20five%20methods%20and%20corresponding%20defenses.%20Testing%20on%0AtextCNN%2C%20biLSTM%2C%20and%20tinybert%20models%20shows%20an%20attack%20success%20rate%20over%2087%25%2C%0Areducible%20through%20fine-tuning.%20Future%20research%20should%20focus%20on%20backdoor%0Adefenses%20in%20WAD.%20All%20the%20code%20and%20data%20of%20this%20paper%20can%20be%20obtained%20at%0Ahttps%3A//anonymous.4open.science/r/attackDefenceinDL-7E05%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttack%2520and%2520Defense%2520of%2520Deep%2520Learning%2520Models%2520in%2520the%2520Field%2520of%2520Web%2520Attack%250A%2520%2520Detection%26entry.906535625%3DLijia%2520Shi%2520and%2520Shihao%2520Dong%26entry.1292438233%3D%2520%2520The%2520challenge%2520of%2520WAD%2520%2528web%2520attack%2520detection%2529%2520is%2520growing%2520as%2520hackers%250Acontinuously%2520refine%2520their%2520methods%2520to%2520evade%2520traditional%2520detection.%2520Deep%2520learning%250Amodels%2520excel%2520in%2520handling%2520complex%2520unknown%2520attacks%2520due%2520to%2520their%2520strong%250Ageneralization%2520and%2520adaptability.%2520However%252C%2520they%2520are%2520vulnerable%2520to%2520backdoor%250Aattacks%252C%2520where%2520contextually%2520irrelevant%2520fragments%2520are%2520inserted%2520into%2520requests%252C%250Acompromising%2520model%2520stability.%2520While%2520backdoor%2520attacks%2520are%2520well%2520studied%2520in%2520image%250Arecognition%252C%2520they%2520are%2520largely%2520unexplored%2520in%2520WAD.%2520This%2520paper%2520introduces%2520backdoor%250Aattacks%2520in%2520WAD%252C%2520proposing%2520five%2520methods%2520and%2520corresponding%2520defenses.%2520Testing%2520on%250AtextCNN%252C%2520biLSTM%252C%2520and%2520tinybert%2520models%2520shows%2520an%2520attack%2520success%2520rate%2520over%252087%2525%252C%250Areducible%2520through%2520fine-tuning.%2520Future%2520research%2520should%2520focus%2520on%2520backdoor%250Adefenses%2520in%2520WAD.%2520All%2520the%2520code%2520and%2520data%2520of%2520this%2520paper%2520can%2520be%2520obtained%2520at%250Ahttps%253A//anonymous.4open.science/r/attackDefenceinDL-7E05%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attack%20and%20Defense%20of%20Deep%20Learning%20Models%20in%20the%20Field%20of%20Web%20Attack%0A%20%20Detection&entry.906535625=Lijia%20Shi%20and%20Shihao%20Dong&entry.1292438233=%20%20The%20challenge%20of%20WAD%20%28web%20attack%20detection%29%20is%20growing%20as%20hackers%0Acontinuously%20refine%20their%20methods%20to%20evade%20traditional%20detection.%20Deep%20learning%0Amodels%20excel%20in%20handling%20complex%20unknown%20attacks%20due%20to%20their%20strong%0Ageneralization%20and%20adaptability.%20However%2C%20they%20are%20vulnerable%20to%20backdoor%0Aattacks%2C%20where%20contextually%20irrelevant%20fragments%20are%20inserted%20into%20requests%2C%0Acompromising%20model%20stability.%20While%20backdoor%20attacks%20are%20well%20studied%20in%20image%0Arecognition%2C%20they%20are%20largely%20unexplored%20in%20WAD.%20This%20paper%20introduces%20backdoor%0Aattacks%20in%20WAD%2C%20proposing%20five%20methods%20and%20corresponding%20defenses.%20Testing%20on%0AtextCNN%2C%20biLSTM%2C%20and%20tinybert%20models%20shows%20an%20attack%20success%20rate%20over%2087%25%2C%0Areducible%20through%20fine-tuning.%20Future%20research%20should%20focus%20on%20backdoor%0Adefenses%20in%20WAD.%20All%20the%20code%20and%20data%20of%20this%20paper%20can%20be%20obtained%20at%0Ahttps%3A//anonymous.4open.science/r/attackDefenceinDL-7E05%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12605v1&entry.124074799=Read"},
{"title": "ROCOv2: Radiology Objects in COntext Version 2, an Updated Multimodal\n  Image Dataset", "author": "Johannes R\u00fcckert and Louise Bloch and Raphael Br\u00fcngel and Ahmad Idrissi-Yaghir and Henning Sch\u00e4fer and Cynthia S. Schmidt and Sven Koitka and Obioma Pelka and Asma Ben Abacha and Alba G. Seco de Herrera and Henning M\u00fcller and Peter A. Horn and Felix Nensa and Christoph M. Friedrich", "abstract": "  Automated medical image analysis systems often require large amounts of\ntraining data with high quality labels, which are difficult and time consuming\nto generate. This paper introduces Radiology Object in COntext version 2\n(ROCOv2), a multimodal dataset consisting of radiological images and associated\nmedical concepts and captions extracted from the PMC Open Access subset. It is\nan updated version of the ROCO dataset published in 2018, and adds 35,705 new\nimages added to PMC since 2018. It further provides manually curated concepts\nfor imaging modalities with additional anatomical and directional concepts for\nX-rays. The dataset consists of 79,789 images and has been used, with minor\nmodifications, in the concept detection and caption prediction tasks of\nImageCLEFmedical Caption 2023. The dataset is suitable for training image\nannotation models based on image-caption pairs, or for multi-label image\nclassification using Unified Medical Language System (UMLS) concepts provided\nwith each image. In addition, it can serve for pre-training of medical domain\nmodels, and evaluation of deep learning models for multi-task learning.\n", "link": "http://arxiv.org/abs/2405.10004v2", "date": "2024-06-18", "relevancy": 1.8883, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4676}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROCOv2%3A%20Radiology%20Objects%20in%20COntext%20Version%202%2C%20an%20Updated%20Multimodal%0A%20%20Image%20Dataset&body=Title%3A%20ROCOv2%3A%20Radiology%20Objects%20in%20COntext%20Version%202%2C%20an%20Updated%20Multimodal%0A%20%20Image%20Dataset%0AAuthor%3A%20Johannes%20R%C3%BCckert%20and%20Louise%20Bloch%20and%20Raphael%20Br%C3%BCngel%20and%20Ahmad%20Idrissi-Yaghir%20and%20Henning%20Sch%C3%A4fer%20and%20Cynthia%20S.%20Schmidt%20and%20Sven%20Koitka%20and%20Obioma%20Pelka%20and%20Asma%20Ben%20Abacha%20and%20Alba%20G.%20Seco%20de%20Herrera%20and%20Henning%20M%C3%BCller%20and%20Peter%20A.%20Horn%20and%20Felix%20Nensa%20and%20Christoph%20M.%20Friedrich%0AAbstract%3A%20%20%20Automated%20medical%20image%20analysis%20systems%20often%20require%20large%20amounts%20of%0Atraining%20data%20with%20high%20quality%20labels%2C%20which%20are%20difficult%20and%20time%20consuming%0Ato%20generate.%20This%20paper%20introduces%20Radiology%20Object%20in%20COntext%20version%202%0A%28ROCOv2%29%2C%20a%20multimodal%20dataset%20consisting%20of%20radiological%20images%20and%20associated%0Amedical%20concepts%20and%20captions%20extracted%20from%20the%20PMC%20Open%20Access%20subset.%20It%20is%0Aan%20updated%20version%20of%20the%20ROCO%20dataset%20published%20in%202018%2C%20and%20adds%2035%2C705%20new%0Aimages%20added%20to%20PMC%20since%202018.%20It%20further%20provides%20manually%20curated%20concepts%0Afor%20imaging%20modalities%20with%20additional%20anatomical%20and%20directional%20concepts%20for%0AX-rays.%20The%20dataset%20consists%20of%2079%2C789%20images%20and%20has%20been%20used%2C%20with%20minor%0Amodifications%2C%20in%20the%20concept%20detection%20and%20caption%20prediction%20tasks%20of%0AImageCLEFmedical%20Caption%202023.%20The%20dataset%20is%20suitable%20for%20training%20image%0Aannotation%20models%20based%20on%20image-caption%20pairs%2C%20or%20for%20multi-label%20image%0Aclassification%20using%20Unified%20Medical%20Language%20System%20%28UMLS%29%20concepts%20provided%0Awith%20each%20image.%20In%20addition%2C%20it%20can%20serve%20for%20pre-training%20of%20medical%20domain%0Amodels%2C%20and%20evaluation%20of%20deep%20learning%20models%20for%20multi-task%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROCOv2%253A%2520Radiology%2520Objects%2520in%2520COntext%2520Version%25202%252C%2520an%2520Updated%2520Multimodal%250A%2520%2520Image%2520Dataset%26entry.906535625%3DJohannes%2520R%25C3%25BCckert%2520and%2520Louise%2520Bloch%2520and%2520Raphael%2520Br%25C3%25BCngel%2520and%2520Ahmad%2520Idrissi-Yaghir%2520and%2520Henning%2520Sch%25C3%25A4fer%2520and%2520Cynthia%2520S.%2520Schmidt%2520and%2520Sven%2520Koitka%2520and%2520Obioma%2520Pelka%2520and%2520Asma%2520Ben%2520Abacha%2520and%2520Alba%2520G.%2520Seco%2520de%2520Herrera%2520and%2520Henning%2520M%25C3%25BCller%2520and%2520Peter%2520A.%2520Horn%2520and%2520Felix%2520Nensa%2520and%2520Christoph%2520M.%2520Friedrich%26entry.1292438233%3D%2520%2520Automated%2520medical%2520image%2520analysis%2520systems%2520often%2520require%2520large%2520amounts%2520of%250Atraining%2520data%2520with%2520high%2520quality%2520labels%252C%2520which%2520are%2520difficult%2520and%2520time%2520consuming%250Ato%2520generate.%2520This%2520paper%2520introduces%2520Radiology%2520Object%2520in%2520COntext%2520version%25202%250A%2528ROCOv2%2529%252C%2520a%2520multimodal%2520dataset%2520consisting%2520of%2520radiological%2520images%2520and%2520associated%250Amedical%2520concepts%2520and%2520captions%2520extracted%2520from%2520the%2520PMC%2520Open%2520Access%2520subset.%2520It%2520is%250Aan%2520updated%2520version%2520of%2520the%2520ROCO%2520dataset%2520published%2520in%25202018%252C%2520and%2520adds%252035%252C705%2520new%250Aimages%2520added%2520to%2520PMC%2520since%25202018.%2520It%2520further%2520provides%2520manually%2520curated%2520concepts%250Afor%2520imaging%2520modalities%2520with%2520additional%2520anatomical%2520and%2520directional%2520concepts%2520for%250AX-rays.%2520The%2520dataset%2520consists%2520of%252079%252C789%2520images%2520and%2520has%2520been%2520used%252C%2520with%2520minor%250Amodifications%252C%2520in%2520the%2520concept%2520detection%2520and%2520caption%2520prediction%2520tasks%2520of%250AImageCLEFmedical%2520Caption%25202023.%2520The%2520dataset%2520is%2520suitable%2520for%2520training%2520image%250Aannotation%2520models%2520based%2520on%2520image-caption%2520pairs%252C%2520or%2520for%2520multi-label%2520image%250Aclassification%2520using%2520Unified%2520Medical%2520Language%2520System%2520%2528UMLS%2529%2520concepts%2520provided%250Awith%2520each%2520image.%2520In%2520addition%252C%2520it%2520can%2520serve%2520for%2520pre-training%2520of%2520medical%2520domain%250Amodels%252C%2520and%2520evaluation%2520of%2520deep%2520learning%2520models%2520for%2520multi-task%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROCOv2%3A%20Radiology%20Objects%20in%20COntext%20Version%202%2C%20an%20Updated%20Multimodal%0A%20%20Image%20Dataset&entry.906535625=Johannes%20R%C3%BCckert%20and%20Louise%20Bloch%20and%20Raphael%20Br%C3%BCngel%20and%20Ahmad%20Idrissi-Yaghir%20and%20Henning%20Sch%C3%A4fer%20and%20Cynthia%20S.%20Schmidt%20and%20Sven%20Koitka%20and%20Obioma%20Pelka%20and%20Asma%20Ben%20Abacha%20and%20Alba%20G.%20Seco%20de%20Herrera%20and%20Henning%20M%C3%BCller%20and%20Peter%20A.%20Horn%20and%20Felix%20Nensa%20and%20Christoph%20M.%20Friedrich&entry.1292438233=%20%20Automated%20medical%20image%20analysis%20systems%20often%20require%20large%20amounts%20of%0Atraining%20data%20with%20high%20quality%20labels%2C%20which%20are%20difficult%20and%20time%20consuming%0Ato%20generate.%20This%20paper%20introduces%20Radiology%20Object%20in%20COntext%20version%202%0A%28ROCOv2%29%2C%20a%20multimodal%20dataset%20consisting%20of%20radiological%20images%20and%20associated%0Amedical%20concepts%20and%20captions%20extracted%20from%20the%20PMC%20Open%20Access%20subset.%20It%20is%0Aan%20updated%20version%20of%20the%20ROCO%20dataset%20published%20in%202018%2C%20and%20adds%2035%2C705%20new%0Aimages%20added%20to%20PMC%20since%202018.%20It%20further%20provides%20manually%20curated%20concepts%0Afor%20imaging%20modalities%20with%20additional%20anatomical%20and%20directional%20concepts%20for%0AX-rays.%20The%20dataset%20consists%20of%2079%2C789%20images%20and%20has%20been%20used%2C%20with%20minor%0Amodifications%2C%20in%20the%20concept%20detection%20and%20caption%20prediction%20tasks%20of%0AImageCLEFmedical%20Caption%202023.%20The%20dataset%20is%20suitable%20for%20training%20image%0Aannotation%20models%20based%20on%20image-caption%20pairs%2C%20or%20for%20multi-label%20image%0Aclassification%20using%20Unified%20Medical%20Language%20System%20%28UMLS%29%20concepts%20provided%0Awith%20each%20image.%20In%20addition%2C%20it%20can%20serve%20for%20pre-training%20of%20medical%20domain%0Amodels%2C%20and%20evaluation%20of%20deep%20learning%20models%20for%20multi-task%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10004v2&entry.124074799=Read"},
{"title": "Low-Resource Machine Translation through the Lens of Personalized\n  Federated Learning", "author": "Viktor Moskvoretskii and Nazarii Tupitsa and Chris Biemann and Samuel Horv\u00e1th and Eduard Gorbunov and Irina Nikishina", "abstract": "  We present a new approach based on the Personalized Federated Learning\nalgorithm MeritFed that can be applied to Natural Language Tasks with\nheterogeneous data. We evaluate it on the Low-Resource Machine Translation\ntask, using the dataset from the Large-Scale Multilingual Machine Translation\nShared Task (Small Track #2) and the subset of Sami languages from the\nmultilingual benchmark for Finno-Ugric languages. In addition to its\neffectiveness, MeritFed is also highly interpretable, as it can be applied to\ntrack the impact of each language used for training. Our analysis reveals that\ntarget dataset size affects weight distribution across auxiliary languages,\nthat unrelated languages do not interfere with the training, and auxiliary\noptimizer parameters have minimal impact. Our approach is easy to apply with a\nfew lines of code, and we provide scripts for reproducing the experiments at\nhttps://github.com/VityaVitalich/MeritFed\n", "link": "http://arxiv.org/abs/2406.12564v1", "date": "2024-06-18", "relevancy": 1.8783, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4969}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4652}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Resource%20Machine%20Translation%20through%20the%20Lens%20of%20Personalized%0A%20%20Federated%20Learning&body=Title%3A%20Low-Resource%20Machine%20Translation%20through%20the%20Lens%20of%20Personalized%0A%20%20Federated%20Learning%0AAuthor%3A%20Viktor%20Moskvoretskii%20and%20Nazarii%20Tupitsa%20and%20Chris%20Biemann%20and%20Samuel%20Horv%C3%A1th%20and%20Eduard%20Gorbunov%20and%20Irina%20Nikishina%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20based%20on%20the%20Personalized%20Federated%20Learning%0Aalgorithm%20MeritFed%20that%20can%20be%20applied%20to%20Natural%20Language%20Tasks%20with%0Aheterogeneous%20data.%20We%20evaluate%20it%20on%20the%20Low-Resource%20Machine%20Translation%0Atask%2C%20using%20the%20dataset%20from%20the%20Large-Scale%20Multilingual%20Machine%20Translation%0AShared%20Task%20%28Small%20Track%20%232%29%20and%20the%20subset%20of%20Sami%20languages%20from%20the%0Amultilingual%20benchmark%20for%20Finno-Ugric%20languages.%20In%20addition%20to%20its%0Aeffectiveness%2C%20MeritFed%20is%20also%20highly%20interpretable%2C%20as%20it%20can%20be%20applied%20to%0Atrack%20the%20impact%20of%20each%20language%20used%20for%20training.%20Our%20analysis%20reveals%20that%0Atarget%20dataset%20size%20affects%20weight%20distribution%20across%20auxiliary%20languages%2C%0Athat%20unrelated%20languages%20do%20not%20interfere%20with%20the%20training%2C%20and%20auxiliary%0Aoptimizer%20parameters%20have%20minimal%20impact.%20Our%20approach%20is%20easy%20to%20apply%20with%20a%0Afew%20lines%20of%20code%2C%20and%20we%20provide%20scripts%20for%20reproducing%20the%20experiments%20at%0Ahttps%3A//github.com/VityaVitalich/MeritFed%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Resource%2520Machine%2520Translation%2520through%2520the%2520Lens%2520of%2520Personalized%250A%2520%2520Federated%2520Learning%26entry.906535625%3DViktor%2520Moskvoretskii%2520and%2520Nazarii%2520Tupitsa%2520and%2520Chris%2520Biemann%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Eduard%2520Gorbunov%2520and%2520Irina%2520Nikishina%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520approach%2520based%2520on%2520the%2520Personalized%2520Federated%2520Learning%250Aalgorithm%2520MeritFed%2520that%2520can%2520be%2520applied%2520to%2520Natural%2520Language%2520Tasks%2520with%250Aheterogeneous%2520data.%2520We%2520evaluate%2520it%2520on%2520the%2520Low-Resource%2520Machine%2520Translation%250Atask%252C%2520using%2520the%2520dataset%2520from%2520the%2520Large-Scale%2520Multilingual%2520Machine%2520Translation%250AShared%2520Task%2520%2528Small%2520Track%2520%25232%2529%2520and%2520the%2520subset%2520of%2520Sami%2520languages%2520from%2520the%250Amultilingual%2520benchmark%2520for%2520Finno-Ugric%2520languages.%2520In%2520addition%2520to%2520its%250Aeffectiveness%252C%2520MeritFed%2520is%2520also%2520highly%2520interpretable%252C%2520as%2520it%2520can%2520be%2520applied%2520to%250Atrack%2520the%2520impact%2520of%2520each%2520language%2520used%2520for%2520training.%2520Our%2520analysis%2520reveals%2520that%250Atarget%2520dataset%2520size%2520affects%2520weight%2520distribution%2520across%2520auxiliary%2520languages%252C%250Athat%2520unrelated%2520languages%2520do%2520not%2520interfere%2520with%2520the%2520training%252C%2520and%2520auxiliary%250Aoptimizer%2520parameters%2520have%2520minimal%2520impact.%2520Our%2520approach%2520is%2520easy%2520to%2520apply%2520with%2520a%250Afew%2520lines%2520of%2520code%252C%2520and%2520we%2520provide%2520scripts%2520for%2520reproducing%2520the%2520experiments%2520at%250Ahttps%253A//github.com/VityaVitalich/MeritFed%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Resource%20Machine%20Translation%20through%20the%20Lens%20of%20Personalized%0A%20%20Federated%20Learning&entry.906535625=Viktor%20Moskvoretskii%20and%20Nazarii%20Tupitsa%20and%20Chris%20Biemann%20and%20Samuel%20Horv%C3%A1th%20and%20Eduard%20Gorbunov%20and%20Irina%20Nikishina&entry.1292438233=%20%20We%20present%20a%20new%20approach%20based%20on%20the%20Personalized%20Federated%20Learning%0Aalgorithm%20MeritFed%20that%20can%20be%20applied%20to%20Natural%20Language%20Tasks%20with%0Aheterogeneous%20data.%20We%20evaluate%20it%20on%20the%20Low-Resource%20Machine%20Translation%0Atask%2C%20using%20the%20dataset%20from%20the%20Large-Scale%20Multilingual%20Machine%20Translation%0AShared%20Task%20%28Small%20Track%20%232%29%20and%20the%20subset%20of%20Sami%20languages%20from%20the%0Amultilingual%20benchmark%20for%20Finno-Ugric%20languages.%20In%20addition%20to%20its%0Aeffectiveness%2C%20MeritFed%20is%20also%20highly%20interpretable%2C%20as%20it%20can%20be%20applied%20to%0Atrack%20the%20impact%20of%20each%20language%20used%20for%20training.%20Our%20analysis%20reveals%20that%0Atarget%20dataset%20size%20affects%20weight%20distribution%20across%20auxiliary%20languages%2C%0Athat%20unrelated%20languages%20do%20not%20interfere%20with%20the%20training%2C%20and%20auxiliary%0Aoptimizer%20parameters%20have%20minimal%20impact.%20Our%20approach%20is%20easy%20to%20apply%20with%20a%0Afew%20lines%20of%20code%2C%20and%20we%20provide%20scripts%20for%20reproducing%20the%20experiments%20at%0Ahttps%3A//github.com/VityaVitalich/MeritFed%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12564v1&entry.124074799=Read"},
{"title": "Estimating class separability of text embeddings with persistent\n  homology", "author": "Kostis Gourgoulias and Najah Ghalyan and Maxime Labonne and Yash Satsangi and Sean Moran and Joseph Sabelja", "abstract": "  This paper introduces an unsupervised method to estimate the class\nseparability of text datasets from a topological point of view. Using\npersistent homology, we demonstrate how tracking the evolution of embedding\nmanifolds during training can inform about class separability. More\nspecifically, we show how this technique can be applied to detect when the\ntraining process stops improving the separability of the embeddings. Our\nresults, validated across binary and multi-class text classification tasks,\nshow that the proposed method's estimates of class separability align with\nthose obtained from supervised methods. This approach offers a novel\nperspective on monitoring and improving the fine-tuning of sentence\ntransformers for classification tasks, particularly in scenarios where labeled\ndata is scarce. We also discuss how tracking these quantities can provide\nadditional insights into the properties of the trained classifier.\n", "link": "http://arxiv.org/abs/2305.15016v4", "date": "2024-06-18", "relevancy": 1.8651, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4676}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4662}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20class%20separability%20of%20text%20embeddings%20with%20persistent%0A%20%20homology&body=Title%3A%20Estimating%20class%20separability%20of%20text%20embeddings%20with%20persistent%0A%20%20homology%0AAuthor%3A%20Kostis%20Gourgoulias%20and%20Najah%20Ghalyan%20and%20Maxime%20Labonne%20and%20Yash%20Satsangi%20and%20Sean%20Moran%20and%20Joseph%20Sabelja%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20unsupervised%20method%20to%20estimate%20the%20class%0Aseparability%20of%20text%20datasets%20from%20a%20topological%20point%20of%20view.%20Using%0Apersistent%20homology%2C%20we%20demonstrate%20how%20tracking%20the%20evolution%20of%20embedding%0Amanifolds%20during%20training%20can%20inform%20about%20class%20separability.%20More%0Aspecifically%2C%20we%20show%20how%20this%20technique%20can%20be%20applied%20to%20detect%20when%20the%0Atraining%20process%20stops%20improving%20the%20separability%20of%20the%20embeddings.%20Our%0Aresults%2C%20validated%20across%20binary%20and%20multi-class%20text%20classification%20tasks%2C%0Ashow%20that%20the%20proposed%20method%27s%20estimates%20of%20class%20separability%20align%20with%0Athose%20obtained%20from%20supervised%20methods.%20This%20approach%20offers%20a%20novel%0Aperspective%20on%20monitoring%20and%20improving%20the%20fine-tuning%20of%20sentence%0Atransformers%20for%20classification%20tasks%2C%20particularly%20in%20scenarios%20where%20labeled%0Adata%20is%20scarce.%20We%20also%20discuss%20how%20tracking%20these%20quantities%20can%20provide%0Aadditional%20insights%20into%20the%20properties%20of%20the%20trained%20classifier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15016v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520class%2520separability%2520of%2520text%2520embeddings%2520with%2520persistent%250A%2520%2520homology%26entry.906535625%3DKostis%2520Gourgoulias%2520and%2520Najah%2520Ghalyan%2520and%2520Maxime%2520Labonne%2520and%2520Yash%2520Satsangi%2520and%2520Sean%2520Moran%2520and%2520Joseph%2520Sabelja%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520unsupervised%2520method%2520to%2520estimate%2520the%2520class%250Aseparability%2520of%2520text%2520datasets%2520from%2520a%2520topological%2520point%2520of%2520view.%2520Using%250Apersistent%2520homology%252C%2520we%2520demonstrate%2520how%2520tracking%2520the%2520evolution%2520of%2520embedding%250Amanifolds%2520during%2520training%2520can%2520inform%2520about%2520class%2520separability.%2520More%250Aspecifically%252C%2520we%2520show%2520how%2520this%2520technique%2520can%2520be%2520applied%2520to%2520detect%2520when%2520the%250Atraining%2520process%2520stops%2520improving%2520the%2520separability%2520of%2520the%2520embeddings.%2520Our%250Aresults%252C%2520validated%2520across%2520binary%2520and%2520multi-class%2520text%2520classification%2520tasks%252C%250Ashow%2520that%2520the%2520proposed%2520method%2527s%2520estimates%2520of%2520class%2520separability%2520align%2520with%250Athose%2520obtained%2520from%2520supervised%2520methods.%2520This%2520approach%2520offers%2520a%2520novel%250Aperspective%2520on%2520monitoring%2520and%2520improving%2520the%2520fine-tuning%2520of%2520sentence%250Atransformers%2520for%2520classification%2520tasks%252C%2520particularly%2520in%2520scenarios%2520where%2520labeled%250Adata%2520is%2520scarce.%2520We%2520also%2520discuss%2520how%2520tracking%2520these%2520quantities%2520can%2520provide%250Aadditional%2520insights%2520into%2520the%2520properties%2520of%2520the%2520trained%2520classifier.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15016v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20class%20separability%20of%20text%20embeddings%20with%20persistent%0A%20%20homology&entry.906535625=Kostis%20Gourgoulias%20and%20Najah%20Ghalyan%20and%20Maxime%20Labonne%20and%20Yash%20Satsangi%20and%20Sean%20Moran%20and%20Joseph%20Sabelja&entry.1292438233=%20%20This%20paper%20introduces%20an%20unsupervised%20method%20to%20estimate%20the%20class%0Aseparability%20of%20text%20datasets%20from%20a%20topological%20point%20of%20view.%20Using%0Apersistent%20homology%2C%20we%20demonstrate%20how%20tracking%20the%20evolution%20of%20embedding%0Amanifolds%20during%20training%20can%20inform%20about%20class%20separability.%20More%0Aspecifically%2C%20we%20show%20how%20this%20technique%20can%20be%20applied%20to%20detect%20when%20the%0Atraining%20process%20stops%20improving%20the%20separability%20of%20the%20embeddings.%20Our%0Aresults%2C%20validated%20across%20binary%20and%20multi-class%20text%20classification%20tasks%2C%0Ashow%20that%20the%20proposed%20method%27s%20estimates%20of%20class%20separability%20align%20with%0Athose%20obtained%20from%20supervised%20methods.%20This%20approach%20offers%20a%20novel%0Aperspective%20on%20monitoring%20and%20improving%20the%20fine-tuning%20of%20sentence%0Atransformers%20for%20classification%20tasks%2C%20particularly%20in%20scenarios%20where%20labeled%0Adata%20is%20scarce.%20We%20also%20discuss%20how%20tracking%20these%20quantities%20can%20provide%0Aadditional%20insights%20into%20the%20properties%20of%20the%20trained%20classifier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15016v4&entry.124074799=Read"},
{"title": "ROOT-SGD: Sharp Nonasymptotics and Near-Optimal Asymptotics in a Single\n  Algorithm", "author": "Chris Junchi Li", "abstract": "  We study the problem of solving strongly convex and smooth unconstrained\noptimization problems using stochastic first-order algorithms. We devise a\nnovel algorithm, referred to as \\emph{Recursive One-Over-T SGD}\n(\\textsf{ROOT-SGD}), based on an easily implementable, recursive averaging of\npast stochastic gradients. We prove that it simultaneously achieves\nstate-of-the-art performance in both a finite-sample, nonasymptotic sense and\nan asymptotic sense. On the nonasymptotic side, we prove risk bounds on the\nlast iterate of \\textsf{ROOT-SGD} with leading-order terms that match the\noptimal statistical risk with a unity pre-factor, along with a higher-order\nterm that scales at the sharp rate of $O(n^{-3/2})$ under the Lipschitz\ncondition on the Hessian matrix. On the asymptotic side, we show that when a\nmild, one-point Hessian continuity condition is imposed, the rescaled last\niterate of (multi-epoch) \\textsf{ROOT-SGD} converges asymptotically to a\nGaussian limit with the Cram\\'{e}r-Rao optimal asymptotic covariance, for a\nbroad range of step-size choices.\n", "link": "http://arxiv.org/abs/2008.12690v3", "date": "2024-06-18", "relevancy": 1.8648, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4771}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4614}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROOT-SGD%3A%20Sharp%20Nonasymptotics%20and%20Near-Optimal%20Asymptotics%20in%20a%20Single%0A%20%20Algorithm&body=Title%3A%20ROOT-SGD%3A%20Sharp%20Nonasymptotics%20and%20Near-Optimal%20Asymptotics%20in%20a%20Single%0A%20%20Algorithm%0AAuthor%3A%20Chris%20Junchi%20Li%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20solving%20strongly%20convex%20and%20smooth%20unconstrained%0Aoptimization%20problems%20using%20stochastic%20first-order%20algorithms.%20We%20devise%20a%0Anovel%20algorithm%2C%20referred%20to%20as%20%5Cemph%7BRecursive%20One-Over-T%20SGD%7D%0A%28%5Ctextsf%7BROOT-SGD%7D%29%2C%20based%20on%20an%20easily%20implementable%2C%20recursive%20averaging%20of%0Apast%20stochastic%20gradients.%20We%20prove%20that%20it%20simultaneously%20achieves%0Astate-of-the-art%20performance%20in%20both%20a%20finite-sample%2C%20nonasymptotic%20sense%20and%0Aan%20asymptotic%20sense.%20On%20the%20nonasymptotic%20side%2C%20we%20prove%20risk%20bounds%20on%20the%0Alast%20iterate%20of%20%5Ctextsf%7BROOT-SGD%7D%20with%20leading-order%20terms%20that%20match%20the%0Aoptimal%20statistical%20risk%20with%20a%20unity%20pre-factor%2C%20along%20with%20a%20higher-order%0Aterm%20that%20scales%20at%20the%20sharp%20rate%20of%20%24O%28n%5E%7B-3/2%7D%29%24%20under%20the%20Lipschitz%0Acondition%20on%20the%20Hessian%20matrix.%20On%20the%20asymptotic%20side%2C%20we%20show%20that%20when%20a%0Amild%2C%20one-point%20Hessian%20continuity%20condition%20is%20imposed%2C%20the%20rescaled%20last%0Aiterate%20of%20%28multi-epoch%29%20%5Ctextsf%7BROOT-SGD%7D%20converges%20asymptotically%20to%20a%0AGaussian%20limit%20with%20the%20Cram%5C%27%7Be%7Dr-Rao%20optimal%20asymptotic%20covariance%2C%20for%20a%0Abroad%20range%20of%20step-size%20choices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2008.12690v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROOT-SGD%253A%2520Sharp%2520Nonasymptotics%2520and%2520Near-Optimal%2520Asymptotics%2520in%2520a%2520Single%250A%2520%2520Algorithm%26entry.906535625%3DChris%2520Junchi%2520Li%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520solving%2520strongly%2520convex%2520and%2520smooth%2520unconstrained%250Aoptimization%2520problems%2520using%2520stochastic%2520first-order%2520algorithms.%2520We%2520devise%2520a%250Anovel%2520algorithm%252C%2520referred%2520to%2520as%2520%255Cemph%257BRecursive%2520One-Over-T%2520SGD%257D%250A%2528%255Ctextsf%257BROOT-SGD%257D%2529%252C%2520based%2520on%2520an%2520easily%2520implementable%252C%2520recursive%2520averaging%2520of%250Apast%2520stochastic%2520gradients.%2520We%2520prove%2520that%2520it%2520simultaneously%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520a%2520finite-sample%252C%2520nonasymptotic%2520sense%2520and%250Aan%2520asymptotic%2520sense.%2520On%2520the%2520nonasymptotic%2520side%252C%2520we%2520prove%2520risk%2520bounds%2520on%2520the%250Alast%2520iterate%2520of%2520%255Ctextsf%257BROOT-SGD%257D%2520with%2520leading-order%2520terms%2520that%2520match%2520the%250Aoptimal%2520statistical%2520risk%2520with%2520a%2520unity%2520pre-factor%252C%2520along%2520with%2520a%2520higher-order%250Aterm%2520that%2520scales%2520at%2520the%2520sharp%2520rate%2520of%2520%2524O%2528n%255E%257B-3/2%257D%2529%2524%2520under%2520the%2520Lipschitz%250Acondition%2520on%2520the%2520Hessian%2520matrix.%2520On%2520the%2520asymptotic%2520side%252C%2520we%2520show%2520that%2520when%2520a%250Amild%252C%2520one-point%2520Hessian%2520continuity%2520condition%2520is%2520imposed%252C%2520the%2520rescaled%2520last%250Aiterate%2520of%2520%2528multi-epoch%2529%2520%255Ctextsf%257BROOT-SGD%257D%2520converges%2520asymptotically%2520to%2520a%250AGaussian%2520limit%2520with%2520the%2520Cram%255C%2527%257Be%257Dr-Rao%2520optimal%2520asymptotic%2520covariance%252C%2520for%2520a%250Abroad%2520range%2520of%2520step-size%2520choices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2008.12690v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROOT-SGD%3A%20Sharp%20Nonasymptotics%20and%20Near-Optimal%20Asymptotics%20in%20a%20Single%0A%20%20Algorithm&entry.906535625=Chris%20Junchi%20Li&entry.1292438233=%20%20We%20study%20the%20problem%20of%20solving%20strongly%20convex%20and%20smooth%20unconstrained%0Aoptimization%20problems%20using%20stochastic%20first-order%20algorithms.%20We%20devise%20a%0Anovel%20algorithm%2C%20referred%20to%20as%20%5Cemph%7BRecursive%20One-Over-T%20SGD%7D%0A%28%5Ctextsf%7BROOT-SGD%7D%29%2C%20based%20on%20an%20easily%20implementable%2C%20recursive%20averaging%20of%0Apast%20stochastic%20gradients.%20We%20prove%20that%20it%20simultaneously%20achieves%0Astate-of-the-art%20performance%20in%20both%20a%20finite-sample%2C%20nonasymptotic%20sense%20and%0Aan%20asymptotic%20sense.%20On%20the%20nonasymptotic%20side%2C%20we%20prove%20risk%20bounds%20on%20the%0Alast%20iterate%20of%20%5Ctextsf%7BROOT-SGD%7D%20with%20leading-order%20terms%20that%20match%20the%0Aoptimal%20statistical%20risk%20with%20a%20unity%20pre-factor%2C%20along%20with%20a%20higher-order%0Aterm%20that%20scales%20at%20the%20sharp%20rate%20of%20%24O%28n%5E%7B-3/2%7D%29%24%20under%20the%20Lipschitz%0Acondition%20on%20the%20Hessian%20matrix.%20On%20the%20asymptotic%20side%2C%20we%20show%20that%20when%20a%0Amild%2C%20one-point%20Hessian%20continuity%20condition%20is%20imposed%2C%20the%20rescaled%20last%0Aiterate%20of%20%28multi-epoch%29%20%5Ctextsf%7BROOT-SGD%7D%20converges%20asymptotically%20to%20a%0AGaussian%20limit%20with%20the%20Cram%5C%27%7Be%7Dr-Rao%20optimal%20asymptotic%20covariance%2C%20for%20a%0Abroad%20range%20of%20step-size%20choices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2008.12690v3&entry.124074799=Read"},
{"title": "Bayesian Data Selection", "author": "Julian Rodemann", "abstract": "  A wide range of machine learning algorithms iteratively add data to the\ntraining sample. Examples include semi-supervised learning, active learning,\nmulti-armed bandits, and Bayesian optimization. We embed this kind of data\naddition into decision theory by framing data selection as a decision problem.\nThis paves the way for finding Bayes-optimal selections of data. For the\nillustrative case of self-training in semi-supervised learning, we derive the\nrespective Bayes criterion. We further show that deploying this criterion\nmitigates the issue of confirmation bias by empirically assessing our method\nfor generalized linear models, semi-parametric generalized additive models, and\nBayesian neural networks on simulated and real-world data.\n", "link": "http://arxiv.org/abs/2406.12560v1", "date": "2024-06-18", "relevancy": 1.8518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4814}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4653}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Data%20Selection&body=Title%3A%20Bayesian%20Data%20Selection%0AAuthor%3A%20Julian%20Rodemann%0AAbstract%3A%20%20%20A%20wide%20range%20of%20machine%20learning%20algorithms%20iteratively%20add%20data%20to%20the%0Atraining%20sample.%20Examples%20include%20semi-supervised%20learning%2C%20active%20learning%2C%0Amulti-armed%20bandits%2C%20and%20Bayesian%20optimization.%20We%20embed%20this%20kind%20of%20data%0Aaddition%20into%20decision%20theory%20by%20framing%20data%20selection%20as%20a%20decision%20problem.%0AThis%20paves%20the%20way%20for%20finding%20Bayes-optimal%20selections%20of%20data.%20For%20the%0Aillustrative%20case%20of%20self-training%20in%20semi-supervised%20learning%2C%20we%20derive%20the%0Arespective%20Bayes%20criterion.%20We%20further%20show%20that%20deploying%20this%20criterion%0Amitigates%20the%20issue%20of%20confirmation%20bias%20by%20empirically%20assessing%20our%20method%0Afor%20generalized%20linear%20models%2C%20semi-parametric%20generalized%20additive%20models%2C%20and%0ABayesian%20neural%20networks%20on%20simulated%20and%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Data%2520Selection%26entry.906535625%3DJulian%2520Rodemann%26entry.1292438233%3D%2520%2520A%2520wide%2520range%2520of%2520machine%2520learning%2520algorithms%2520iteratively%2520add%2520data%2520to%2520the%250Atraining%2520sample.%2520Examples%2520include%2520semi-supervised%2520learning%252C%2520active%2520learning%252C%250Amulti-armed%2520bandits%252C%2520and%2520Bayesian%2520optimization.%2520We%2520embed%2520this%2520kind%2520of%2520data%250Aaddition%2520into%2520decision%2520theory%2520by%2520framing%2520data%2520selection%2520as%2520a%2520decision%2520problem.%250AThis%2520paves%2520the%2520way%2520for%2520finding%2520Bayes-optimal%2520selections%2520of%2520data.%2520For%2520the%250Aillustrative%2520case%2520of%2520self-training%2520in%2520semi-supervised%2520learning%252C%2520we%2520derive%2520the%250Arespective%2520Bayes%2520criterion.%2520We%2520further%2520show%2520that%2520deploying%2520this%2520criterion%250Amitigates%2520the%2520issue%2520of%2520confirmation%2520bias%2520by%2520empirically%2520assessing%2520our%2520method%250Afor%2520generalized%2520linear%2520models%252C%2520semi-parametric%2520generalized%2520additive%2520models%252C%2520and%250ABayesian%2520neural%2520networks%2520on%2520simulated%2520and%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Data%20Selection&entry.906535625=Julian%20Rodemann&entry.1292438233=%20%20A%20wide%20range%20of%20machine%20learning%20algorithms%20iteratively%20add%20data%20to%20the%0Atraining%20sample.%20Examples%20include%20semi-supervised%20learning%2C%20active%20learning%2C%0Amulti-armed%20bandits%2C%20and%20Bayesian%20optimization.%20We%20embed%20this%20kind%20of%20data%0Aaddition%20into%20decision%20theory%20by%20framing%20data%20selection%20as%20a%20decision%20problem.%0AThis%20paves%20the%20way%20for%20finding%20Bayes-optimal%20selections%20of%20data.%20For%20the%0Aillustrative%20case%20of%20self-training%20in%20semi-supervised%20learning%2C%20we%20derive%20the%0Arespective%20Bayes%20criterion.%20We%20further%20show%20that%20deploying%20this%20criterion%0Amitigates%20the%20issue%20of%20confirmation%20bias%20by%20empirically%20assessing%20our%20method%0Afor%20generalized%20linear%20models%2C%20semi-parametric%20generalized%20additive%20models%2C%20and%0ABayesian%20neural%20networks%20on%20simulated%20and%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12560v1&entry.124074799=Read"},
{"title": "Large Language Models Are Zero-Shot Time Series Forecasters", "author": "Nate Gruver and Marc Finzi and Shikai Qiu and Andrew Gordon Wilson", "abstract": "  By encoding time series as a string of numerical digits, we can frame time\nseries forecasting as next-token prediction in text. Developing this approach,\nwe find that large language models (LLMs) such as GPT-3 and LLaMA-2 can\nsurprisingly zero-shot extrapolate time series at a level comparable to or\nexceeding the performance of purpose-built time series models trained on the\ndownstream tasks. To facilitate this performance, we propose procedures for\neffectively tokenizing time series data and converting discrete distributions\nover tokens into highly flexible densities over continuous values. We argue the\nsuccess of LLMs for time series stems from their ability to naturally represent\nmultimodal distributions, in conjunction with biases for simplicity, and\nrepetition, which align with the salient features in many time series, such as\nrepeated seasonal trends. We also show how LLMs can naturally handle missing\ndata without imputation through non-numerical text, accommodate textual side\ninformation, and answer questions to help explain predictions. While we find\nthat increasing model size generally improves performance on time series, we\nshow GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,\nand poor uncertainty calibration, which is likely the result of alignment\ninterventions such as RLHF.\n", "link": "http://arxiv.org/abs/2310.07820v2", "date": "2024-06-18", "relevancy": 1.8517, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4821}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Are%20Zero-Shot%20Time%20Series%20Forecasters&body=Title%3A%20Large%20Language%20Models%20Are%20Zero-Shot%20Time%20Series%20Forecasters%0AAuthor%3A%20Nate%20Gruver%20and%20Marc%20Finzi%20and%20Shikai%20Qiu%20and%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20By%20encoding%20time%20series%20as%20a%20string%20of%20numerical%20digits%2C%20we%20can%20frame%20time%0Aseries%20forecasting%20as%20next-token%20prediction%20in%20text.%20Developing%20this%20approach%2C%0Awe%20find%20that%20large%20language%20models%20%28LLMs%29%20such%20as%20GPT-3%20and%20LLaMA-2%20can%0Asurprisingly%20zero-shot%20extrapolate%20time%20series%20at%20a%20level%20comparable%20to%20or%0Aexceeding%20the%20performance%20of%20purpose-built%20time%20series%20models%20trained%20on%20the%0Adownstream%20tasks.%20To%20facilitate%20this%20performance%2C%20we%20propose%20procedures%20for%0Aeffectively%20tokenizing%20time%20series%20data%20and%20converting%20discrete%20distributions%0Aover%20tokens%20into%20highly%20flexible%20densities%20over%20continuous%20values.%20We%20argue%20the%0Asuccess%20of%20LLMs%20for%20time%20series%20stems%20from%20their%20ability%20to%20naturally%20represent%0Amultimodal%20distributions%2C%20in%20conjunction%20with%20biases%20for%20simplicity%2C%20and%0Arepetition%2C%20which%20align%20with%20the%20salient%20features%20in%20many%20time%20series%2C%20such%20as%0Arepeated%20seasonal%20trends.%20We%20also%20show%20how%20LLMs%20can%20naturally%20handle%20missing%0Adata%20without%20imputation%20through%20non-numerical%20text%2C%20accommodate%20textual%20side%0Ainformation%2C%20and%20answer%20questions%20to%20help%20explain%20predictions.%20While%20we%20find%0Athat%20increasing%20model%20size%20generally%20improves%20performance%20on%20time%20series%2C%20we%0Ashow%20GPT-4%20can%20perform%20worse%20than%20GPT-3%20because%20of%20how%20it%20tokenizes%20numbers%2C%0Aand%20poor%20uncertainty%20calibration%2C%20which%20is%20likely%20the%20result%20of%20alignment%0Ainterventions%20such%20as%20RLHF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Are%2520Zero-Shot%2520Time%2520Series%2520Forecasters%26entry.906535625%3DNate%2520Gruver%2520and%2520Marc%2520Finzi%2520and%2520Shikai%2520Qiu%2520and%2520Andrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520By%2520encoding%2520time%2520series%2520as%2520a%2520string%2520of%2520numerical%2520digits%252C%2520we%2520can%2520frame%2520time%250Aseries%2520forecasting%2520as%2520next-token%2520prediction%2520in%2520text.%2520Developing%2520this%2520approach%252C%250Awe%2520find%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520such%2520as%2520GPT-3%2520and%2520LLaMA-2%2520can%250Asurprisingly%2520zero-shot%2520extrapolate%2520time%2520series%2520at%2520a%2520level%2520comparable%2520to%2520or%250Aexceeding%2520the%2520performance%2520of%2520purpose-built%2520time%2520series%2520models%2520trained%2520on%2520the%250Adownstream%2520tasks.%2520To%2520facilitate%2520this%2520performance%252C%2520we%2520propose%2520procedures%2520for%250Aeffectively%2520tokenizing%2520time%2520series%2520data%2520and%2520converting%2520discrete%2520distributions%250Aover%2520tokens%2520into%2520highly%2520flexible%2520densities%2520over%2520continuous%2520values.%2520We%2520argue%2520the%250Asuccess%2520of%2520LLMs%2520for%2520time%2520series%2520stems%2520from%2520their%2520ability%2520to%2520naturally%2520represent%250Amultimodal%2520distributions%252C%2520in%2520conjunction%2520with%2520biases%2520for%2520simplicity%252C%2520and%250Arepetition%252C%2520which%2520align%2520with%2520the%2520salient%2520features%2520in%2520many%2520time%2520series%252C%2520such%2520as%250Arepeated%2520seasonal%2520trends.%2520We%2520also%2520show%2520how%2520LLMs%2520can%2520naturally%2520handle%2520missing%250Adata%2520without%2520imputation%2520through%2520non-numerical%2520text%252C%2520accommodate%2520textual%2520side%250Ainformation%252C%2520and%2520answer%2520questions%2520to%2520help%2520explain%2520predictions.%2520While%2520we%2520find%250Athat%2520increasing%2520model%2520size%2520generally%2520improves%2520performance%2520on%2520time%2520series%252C%2520we%250Ashow%2520GPT-4%2520can%2520perform%2520worse%2520than%2520GPT-3%2520because%2520of%2520how%2520it%2520tokenizes%2520numbers%252C%250Aand%2520poor%2520uncertainty%2520calibration%252C%2520which%2520is%2520likely%2520the%2520result%2520of%2520alignment%250Ainterventions%2520such%2520as%2520RLHF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Are%20Zero-Shot%20Time%20Series%20Forecasters&entry.906535625=Nate%20Gruver%20and%20Marc%20Finzi%20and%20Shikai%20Qiu%20and%20Andrew%20Gordon%20Wilson&entry.1292438233=%20%20By%20encoding%20time%20series%20as%20a%20string%20of%20numerical%20digits%2C%20we%20can%20frame%20time%0Aseries%20forecasting%20as%20next-token%20prediction%20in%20text.%20Developing%20this%20approach%2C%0Awe%20find%20that%20large%20language%20models%20%28LLMs%29%20such%20as%20GPT-3%20and%20LLaMA-2%20can%0Asurprisingly%20zero-shot%20extrapolate%20time%20series%20at%20a%20level%20comparable%20to%20or%0Aexceeding%20the%20performance%20of%20purpose-built%20time%20series%20models%20trained%20on%20the%0Adownstream%20tasks.%20To%20facilitate%20this%20performance%2C%20we%20propose%20procedures%20for%0Aeffectively%20tokenizing%20time%20series%20data%20and%20converting%20discrete%20distributions%0Aover%20tokens%20into%20highly%20flexible%20densities%20over%20continuous%20values.%20We%20argue%20the%0Asuccess%20of%20LLMs%20for%20time%20series%20stems%20from%20their%20ability%20to%20naturally%20represent%0Amultimodal%20distributions%2C%20in%20conjunction%20with%20biases%20for%20simplicity%2C%20and%0Arepetition%2C%20which%20align%20with%20the%20salient%20features%20in%20many%20time%20series%2C%20such%20as%0Arepeated%20seasonal%20trends.%20We%20also%20show%20how%20LLMs%20can%20naturally%20handle%20missing%0Adata%20without%20imputation%20through%20non-numerical%20text%2C%20accommodate%20textual%20side%0Ainformation%2C%20and%20answer%20questions%20to%20help%20explain%20predictions.%20While%20we%20find%0Athat%20increasing%20model%20size%20generally%20improves%20performance%20on%20time%20series%2C%20we%0Ashow%20GPT-4%20can%20perform%20worse%20than%20GPT-3%20because%20of%20how%20it%20tokenizes%20numbers%2C%0Aand%20poor%20uncertainty%20calibration%2C%20which%20is%20likely%20the%20result%20of%20alignment%0Ainterventions%20such%20as%20RLHF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07820v2&entry.124074799=Read"},
{"title": "Zero-Shot Neural Architecture Search: Challenges, Solutions, and\n  Opportunities", "author": "Guihong Li and Duc Hoang and Kartikeya Bhardwaj and Ming Lin and Zhangyang Wang and Radu Marculescu", "abstract": "  Recently, zero-shot (or training-free) Neural Architecture Search (NAS)\napproaches have been proposed to liberate NAS from the expensive training\nprocess. The key idea behind zero-shot NAS approaches is to design proxies that\ncan predict the accuracy of some given networks without training the network\nparameters. The proxies proposed so far are usually inspired by recent progress\nin theoretical understanding of deep learning and have shown great potential on\nseveral datasets and NAS benchmarks. This paper aims to comprehensively review\nand compare the state-of-the-art (SOTA) zero-shot NAS approaches, with an\nemphasis on their hardware awareness. To this end, we first review the\nmainstream zero-shot proxies and discuss their theoretical underpinnings. We\nthen compare these zero-shot proxies through large-scale experiments and\ndemonstrate their effectiveness in both hardware-aware and hardware-oblivious\nNAS scenarios. Finally, we point out several promising ideas to design better\nproxies. Our source code and the list of related papers are available on\nhttps://github.com/SLDGroup/survey-zero-shot-nas.\n", "link": "http://arxiv.org/abs/2307.01998v3", "date": "2024-06-18", "relevancy": 1.846, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4638}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4634}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Neural%20Architecture%20Search%3A%20Challenges%2C%20Solutions%2C%20and%0A%20%20Opportunities&body=Title%3A%20Zero-Shot%20Neural%20Architecture%20Search%3A%20Challenges%2C%20Solutions%2C%20and%0A%20%20Opportunities%0AAuthor%3A%20Guihong%20Li%20and%20Duc%20Hoang%20and%20Kartikeya%20Bhardwaj%20and%20Ming%20Lin%20and%20Zhangyang%20Wang%20and%20Radu%20Marculescu%0AAbstract%3A%20%20%20Recently%2C%20zero-shot%20%28or%20training-free%29%20Neural%20Architecture%20Search%20%28NAS%29%0Aapproaches%20have%20been%20proposed%20to%20liberate%20NAS%20from%20the%20expensive%20training%0Aprocess.%20The%20key%20idea%20behind%20zero-shot%20NAS%20approaches%20is%20to%20design%20proxies%20that%0Acan%20predict%20the%20accuracy%20of%20some%20given%20networks%20without%20training%20the%20network%0Aparameters.%20The%20proxies%20proposed%20so%20far%20are%20usually%20inspired%20by%20recent%20progress%0Ain%20theoretical%20understanding%20of%20deep%20learning%20and%20have%20shown%20great%20potential%20on%0Aseveral%20datasets%20and%20NAS%20benchmarks.%20This%20paper%20aims%20to%20comprehensively%20review%0Aand%20compare%20the%20state-of-the-art%20%28SOTA%29%20zero-shot%20NAS%20approaches%2C%20with%20an%0Aemphasis%20on%20their%20hardware%20awareness.%20To%20this%20end%2C%20we%20first%20review%20the%0Amainstream%20zero-shot%20proxies%20and%20discuss%20their%20theoretical%20underpinnings.%20We%0Athen%20compare%20these%20zero-shot%20proxies%20through%20large-scale%20experiments%20and%0Ademonstrate%20their%20effectiveness%20in%20both%20hardware-aware%20and%20hardware-oblivious%0ANAS%20scenarios.%20Finally%2C%20we%20point%20out%20several%20promising%20ideas%20to%20design%20better%0Aproxies.%20Our%20source%20code%20and%20the%20list%20of%20related%20papers%20are%20available%20on%0Ahttps%3A//github.com/SLDGroup/survey-zero-shot-nas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01998v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Neural%2520Architecture%2520Search%253A%2520Challenges%252C%2520Solutions%252C%2520and%250A%2520%2520Opportunities%26entry.906535625%3DGuihong%2520Li%2520and%2520Duc%2520Hoang%2520and%2520Kartikeya%2520Bhardwaj%2520and%2520Ming%2520Lin%2520and%2520Zhangyang%2520Wang%2520and%2520Radu%2520Marculescu%26entry.1292438233%3D%2520%2520Recently%252C%2520zero-shot%2520%2528or%2520training-free%2529%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%250Aapproaches%2520have%2520been%2520proposed%2520to%2520liberate%2520NAS%2520from%2520the%2520expensive%2520training%250Aprocess.%2520The%2520key%2520idea%2520behind%2520zero-shot%2520NAS%2520approaches%2520is%2520to%2520design%2520proxies%2520that%250Acan%2520predict%2520the%2520accuracy%2520of%2520some%2520given%2520networks%2520without%2520training%2520the%2520network%250Aparameters.%2520The%2520proxies%2520proposed%2520so%2520far%2520are%2520usually%2520inspired%2520by%2520recent%2520progress%250Ain%2520theoretical%2520understanding%2520of%2520deep%2520learning%2520and%2520have%2520shown%2520great%2520potential%2520on%250Aseveral%2520datasets%2520and%2520NAS%2520benchmarks.%2520This%2520paper%2520aims%2520to%2520comprehensively%2520review%250Aand%2520compare%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520zero-shot%2520NAS%2520approaches%252C%2520with%2520an%250Aemphasis%2520on%2520their%2520hardware%2520awareness.%2520To%2520this%2520end%252C%2520we%2520first%2520review%2520the%250Amainstream%2520zero-shot%2520proxies%2520and%2520discuss%2520their%2520theoretical%2520underpinnings.%2520We%250Athen%2520compare%2520these%2520zero-shot%2520proxies%2520through%2520large-scale%2520experiments%2520and%250Ademonstrate%2520their%2520effectiveness%2520in%2520both%2520hardware-aware%2520and%2520hardware-oblivious%250ANAS%2520scenarios.%2520Finally%252C%2520we%2520point%2520out%2520several%2520promising%2520ideas%2520to%2520design%2520better%250Aproxies.%2520Our%2520source%2520code%2520and%2520the%2520list%2520of%2520related%2520papers%2520are%2520available%2520on%250Ahttps%253A//github.com/SLDGroup/survey-zero-shot-nas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.01998v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Neural%20Architecture%20Search%3A%20Challenges%2C%20Solutions%2C%20and%0A%20%20Opportunities&entry.906535625=Guihong%20Li%20and%20Duc%20Hoang%20and%20Kartikeya%20Bhardwaj%20and%20Ming%20Lin%20and%20Zhangyang%20Wang%20and%20Radu%20Marculescu&entry.1292438233=%20%20Recently%2C%20zero-shot%20%28or%20training-free%29%20Neural%20Architecture%20Search%20%28NAS%29%0Aapproaches%20have%20been%20proposed%20to%20liberate%20NAS%20from%20the%20expensive%20training%0Aprocess.%20The%20key%20idea%20behind%20zero-shot%20NAS%20approaches%20is%20to%20design%20proxies%20that%0Acan%20predict%20the%20accuracy%20of%20some%20given%20networks%20without%20training%20the%20network%0Aparameters.%20The%20proxies%20proposed%20so%20far%20are%20usually%20inspired%20by%20recent%20progress%0Ain%20theoretical%20understanding%20of%20deep%20learning%20and%20have%20shown%20great%20potential%20on%0Aseveral%20datasets%20and%20NAS%20benchmarks.%20This%20paper%20aims%20to%20comprehensively%20review%0Aand%20compare%20the%20state-of-the-art%20%28SOTA%29%20zero-shot%20NAS%20approaches%2C%20with%20an%0Aemphasis%20on%20their%20hardware%20awareness.%20To%20this%20end%2C%20we%20first%20review%20the%0Amainstream%20zero-shot%20proxies%20and%20discuss%20their%20theoretical%20underpinnings.%20We%0Athen%20compare%20these%20zero-shot%20proxies%20through%20large-scale%20experiments%20and%0Ademonstrate%20their%20effectiveness%20in%20both%20hardware-aware%20and%20hardware-oblivious%0ANAS%20scenarios.%20Finally%2C%20we%20point%20out%20several%20promising%20ideas%20to%20design%20better%0Aproxies.%20Our%20source%20code%20and%20the%20list%20of%20related%20papers%20are%20available%20on%0Ahttps%3A//github.com/SLDGroup/survey-zero-shot-nas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01998v3&entry.124074799=Read"},
{"title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries", "author": "Jonathan F\u00fcrst and Catherine Kosten and Farhard Nooralahzadeh and Yi Zhang and Kurt Stockinger", "abstract": "  Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.\n", "link": "http://arxiv.org/abs/2402.08349v2", "date": "2024-06-18", "relevancy": 1.8392, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5025}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4546}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Data%20Model%20Robustness%20of%20Text-to-SQL%20Systems%20Based%20on%0A%20%20Real%20User%20Queries&body=Title%3A%20Evaluating%20the%20Data%20Model%20Robustness%20of%20Text-to-SQL%20Systems%20Based%20on%0A%20%20Real%20User%20Queries%0AAuthor%3A%20Jonathan%20F%C3%BCrst%20and%20Catherine%20Kosten%20and%20Farhard%20Nooralahzadeh%20and%20Yi%20Zhang%20and%20Kurt%20Stockinger%0AAbstract%3A%20%20%20Text-to-SQL%20systems%20%28also%20known%20as%20NL-to-SQL%20systems%29%20have%20become%20an%0Aincreasingly%20popular%20solution%20for%20bridging%20the%20gap%20between%20user%20capabilities%0Aand%20SQL-based%20data%20access.%20These%20systems%20translate%20user%20requests%20in%20natural%0Alanguage%20to%20valid%20SQL%20statements%20for%20a%20specific%20database.%20Recent%20Text-to-SQL%0Asystems%20have%20benefited%20from%20the%20rapid%20improvement%20of%20transformer-based%20language%0Amodels.%20However%2C%20while%20Text-to-SQL%20systems%20that%20incorporate%20such%20models%0Acontinuously%20reach%20new%20high%20scores%20on%20--%20often%20synthetic%20--%20benchmark%20datasets%2C%0Aa%20systematic%20exploration%20of%20their%20robustness%20towards%20different%20data%20models%20in%20a%0Areal-world%2C%20realistic%20scenario%20is%20notably%20missing.%20This%20paper%20provides%20the%0Afirst%20in-depth%20evaluation%20of%20the%20data%20model%20robustness%20of%20Text-to-SQL%20systems%0Ain%20practice%20based%20on%20a%20multi-year%20international%20project%20focused%20on%20Text-to-SQL%0Ainterfaces.%20Our%20evaluation%20is%20based%20on%20a%20real-world%20deployment%20of%20FootballDB%2C%20a%0Asystem%20that%20was%20deployed%20over%20a%209%20month%20period%20in%20the%20context%20of%20the%20FIFA%20World%0ACup%202022%2C%20during%20which%20about%206K%20natural%20language%20questions%20were%20asked%20and%0Aexecuted.%20All%20of%20our%20data%20is%20based%20on%20real%20user%20questions%20that%20were%20asked%20live%0Ato%20the%20system.%20We%20manually%20labeled%20and%20translated%20a%20subset%20of%20these%20questions%0Afor%20three%20different%20data%20models.%20For%20each%20data%20model%2C%20we%20explore%20the%0Aperformance%20of%20representative%20Text-to-SQL%20systems%20and%20language%20models.%20We%0Afurther%20quantify%20the%20impact%20of%20training%20data%20size%2C%20pre-%2C%20and%20post-processing%0Asteps%20as%20well%20as%20language%20model%20inference%20time.%20Our%20comprehensive%20evaluation%0Asheds%20light%20on%20the%20design%20choices%20of%20real-world%20Text-to-SQL%20systems%20and%20their%0Aimpact%20on%20moving%20from%20research%20prototypes%20to%20real%20deployments.%20Last%2C%20we%20provide%0Aa%20new%20benchmark%20dataset%20to%20the%20community%2C%20which%20is%20the%20first%20to%20enable%20the%0Aevaluation%20of%20different%20data%20models%20for%20the%20same%20dataset%20and%20is%20substantially%0Amore%20challenging%20than%20most%20previous%20datasets%20in%20terms%20of%20query%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Data%2520Model%2520Robustness%2520of%2520Text-to-SQL%2520Systems%2520Based%2520on%250A%2520%2520Real%2520User%2520Queries%26entry.906535625%3DJonathan%2520F%25C3%25BCrst%2520and%2520Catherine%2520Kosten%2520and%2520Farhard%2520Nooralahzadeh%2520and%2520Yi%2520Zhang%2520and%2520Kurt%2520Stockinger%26entry.1292438233%3D%2520%2520Text-to-SQL%2520systems%2520%2528also%2520known%2520as%2520NL-to-SQL%2520systems%2529%2520have%2520become%2520an%250Aincreasingly%2520popular%2520solution%2520for%2520bridging%2520the%2520gap%2520between%2520user%2520capabilities%250Aand%2520SQL-based%2520data%2520access.%2520These%2520systems%2520translate%2520user%2520requests%2520in%2520natural%250Alanguage%2520to%2520valid%2520SQL%2520statements%2520for%2520a%2520specific%2520database.%2520Recent%2520Text-to-SQL%250Asystems%2520have%2520benefited%2520from%2520the%2520rapid%2520improvement%2520of%2520transformer-based%2520language%250Amodels.%2520However%252C%2520while%2520Text-to-SQL%2520systems%2520that%2520incorporate%2520such%2520models%250Acontinuously%2520reach%2520new%2520high%2520scores%2520on%2520--%2520often%2520synthetic%2520--%2520benchmark%2520datasets%252C%250Aa%2520systematic%2520exploration%2520of%2520their%2520robustness%2520towards%2520different%2520data%2520models%2520in%2520a%250Areal-world%252C%2520realistic%2520scenario%2520is%2520notably%2520missing.%2520This%2520paper%2520provides%2520the%250Afirst%2520in-depth%2520evaluation%2520of%2520the%2520data%2520model%2520robustness%2520of%2520Text-to-SQL%2520systems%250Ain%2520practice%2520based%2520on%2520a%2520multi-year%2520international%2520project%2520focused%2520on%2520Text-to-SQL%250Ainterfaces.%2520Our%2520evaluation%2520is%2520based%2520on%2520a%2520real-world%2520deployment%2520of%2520FootballDB%252C%2520a%250Asystem%2520that%2520was%2520deployed%2520over%2520a%25209%2520month%2520period%2520in%2520the%2520context%2520of%2520the%2520FIFA%2520World%250ACup%25202022%252C%2520during%2520which%2520about%25206K%2520natural%2520language%2520questions%2520were%2520asked%2520and%250Aexecuted.%2520All%2520of%2520our%2520data%2520is%2520based%2520on%2520real%2520user%2520questions%2520that%2520were%2520asked%2520live%250Ato%2520the%2520system.%2520We%2520manually%2520labeled%2520and%2520translated%2520a%2520subset%2520of%2520these%2520questions%250Afor%2520three%2520different%2520data%2520models.%2520For%2520each%2520data%2520model%252C%2520we%2520explore%2520the%250Aperformance%2520of%2520representative%2520Text-to-SQL%2520systems%2520and%2520language%2520models.%2520We%250Afurther%2520quantify%2520the%2520impact%2520of%2520training%2520data%2520size%252C%2520pre-%252C%2520and%2520post-processing%250Asteps%2520as%2520well%2520as%2520language%2520model%2520inference%2520time.%2520Our%2520comprehensive%2520evaluation%250Asheds%2520light%2520on%2520the%2520design%2520choices%2520of%2520real-world%2520Text-to-SQL%2520systems%2520and%2520their%250Aimpact%2520on%2520moving%2520from%2520research%2520prototypes%2520to%2520real%2520deployments.%2520Last%252C%2520we%2520provide%250Aa%2520new%2520benchmark%2520dataset%2520to%2520the%2520community%252C%2520which%2520is%2520the%2520first%2520to%2520enable%2520the%250Aevaluation%2520of%2520different%2520data%2520models%2520for%2520the%2520same%2520dataset%2520and%2520is%2520substantially%250Amore%2520challenging%2520than%2520most%2520previous%2520datasets%2520in%2520terms%2520of%2520query%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Data%20Model%20Robustness%20of%20Text-to-SQL%20Systems%20Based%20on%0A%20%20Real%20User%20Queries&entry.906535625=Jonathan%20F%C3%BCrst%20and%20Catherine%20Kosten%20and%20Farhard%20Nooralahzadeh%20and%20Yi%20Zhang%20and%20Kurt%20Stockinger&entry.1292438233=%20%20Text-to-SQL%20systems%20%28also%20known%20as%20NL-to-SQL%20systems%29%20have%20become%20an%0Aincreasingly%20popular%20solution%20for%20bridging%20the%20gap%20between%20user%20capabilities%0Aand%20SQL-based%20data%20access.%20These%20systems%20translate%20user%20requests%20in%20natural%0Alanguage%20to%20valid%20SQL%20statements%20for%20a%20specific%20database.%20Recent%20Text-to-SQL%0Asystems%20have%20benefited%20from%20the%20rapid%20improvement%20of%20transformer-based%20language%0Amodels.%20However%2C%20while%20Text-to-SQL%20systems%20that%20incorporate%20such%20models%0Acontinuously%20reach%20new%20high%20scores%20on%20--%20often%20synthetic%20--%20benchmark%20datasets%2C%0Aa%20systematic%20exploration%20of%20their%20robustness%20towards%20different%20data%20models%20in%20a%0Areal-world%2C%20realistic%20scenario%20is%20notably%20missing.%20This%20paper%20provides%20the%0Afirst%20in-depth%20evaluation%20of%20the%20data%20model%20robustness%20of%20Text-to-SQL%20systems%0Ain%20practice%20based%20on%20a%20multi-year%20international%20project%20focused%20on%20Text-to-SQL%0Ainterfaces.%20Our%20evaluation%20is%20based%20on%20a%20real-world%20deployment%20of%20FootballDB%2C%20a%0Asystem%20that%20was%20deployed%20over%20a%209%20month%20period%20in%20the%20context%20of%20the%20FIFA%20World%0ACup%202022%2C%20during%20which%20about%206K%20natural%20language%20questions%20were%20asked%20and%0Aexecuted.%20All%20of%20our%20data%20is%20based%20on%20real%20user%20questions%20that%20were%20asked%20live%0Ato%20the%20system.%20We%20manually%20labeled%20and%20translated%20a%20subset%20of%20these%20questions%0Afor%20three%20different%20data%20models.%20For%20each%20data%20model%2C%20we%20explore%20the%0Aperformance%20of%20representative%20Text-to-SQL%20systems%20and%20language%20models.%20We%0Afurther%20quantify%20the%20impact%20of%20training%20data%20size%2C%20pre-%2C%20and%20post-processing%0Asteps%20as%20well%20as%20language%20model%20inference%20time.%20Our%20comprehensive%20evaluation%0Asheds%20light%20on%20the%20design%20choices%20of%20real-world%20Text-to-SQL%20systems%20and%20their%0Aimpact%20on%20moving%20from%20research%20prototypes%20to%20real%20deployments.%20Last%2C%20we%20provide%0Aa%20new%20benchmark%20dataset%20to%20the%20community%2C%20which%20is%20the%20first%20to%20enable%20the%0Aevaluation%20of%20different%20data%20models%20for%20the%20same%20dataset%20and%20is%20substantially%0Amore%20challenging%20than%20most%20previous%20datasets%20in%20terms%20of%20query%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08349v2&entry.124074799=Read"},
{"title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval", "author": "Tuan-Luc Huynh and Thuy-Trang Vu and Weiqing Wang and Yinwei Wei and Trung Le and Dragan Gasevic and Yuan-Fang Li and Thanh-Toan Do", "abstract": "  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSIs need full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nrehearsal-free, prompt-based approach for instance-wise incremental learning in\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that PromptDSI matches IncDSI in managing\nforgetting while significantly enhancing recall by over 4% on new corpora.\n", "link": "http://arxiv.org/abs/2406.12593v1", "date": "2024-06-18", "relevancy": 1.8387, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4753}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4625}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PromptDSI%3A%20Prompt-based%20Rehearsal-free%20Instance-wise%20Incremental%0A%20%20Learning%20for%20Document%20Retrieval&body=Title%3A%20PromptDSI%3A%20Prompt-based%20Rehearsal-free%20Instance-wise%20Incremental%0A%20%20Learning%20for%20Document%20Retrieval%0AAuthor%3A%20Tuan-Luc%20Huynh%20and%20Thuy-Trang%20Vu%20and%20Weiqing%20Wang%20and%20Yinwei%20Wei%20and%20Trung%20Le%20and%20Dragan%20Gasevic%20and%20Yuan-Fang%20Li%20and%20Thanh-Toan%20Do%0AAbstract%3A%20%20%20Differentiable%20Search%20Index%20%28DSI%29%20utilizes%20Pre-trained%20Language%20Models%20%28PLMs%29%0Afor%20efficient%20document%20retrieval%20without%20relying%20on%20external%20indexes.%20However%2C%0ADSIs%20need%20full%20re-training%20to%20handle%20updates%20in%20dynamic%20corpora%2C%20causing%0Asignificant%20computational%20inefficiencies.%20We%20introduce%20PromptDSI%2C%20a%0Arehearsal-free%2C%20prompt-based%20approach%20for%20instance-wise%20incremental%20learning%20in%0Adocument%20retrieval.%20PromptDSI%20attaches%20prompts%20to%20the%20frozen%20PLM%27s%20encoder%20of%0ADSI%2C%20leveraging%20its%20powerful%20representation%20to%20efficiently%20index%20new%20corpora%0Awhile%20maintaining%20a%20balance%20between%20stability%20and%20plasticity.%20We%20eliminate%20the%0Ainitial%20forward%20pass%20of%20prompt-based%20continual%20learning%20methods%20that%20doubles%0Atraining%20and%20inference%20time.%20Moreover%2C%20we%20propose%20a%20topic-aware%20prompt%20pool%0Athat%20employs%20neural%20topic%20embeddings%20as%20fixed%20keys.%20This%20strategy%20ensures%0Adiverse%20and%20effective%20prompt%20usage%2C%20addressing%20the%20challenge%20of%20parameter%0Aunderutilization%20caused%20by%20the%20collapse%20of%20the%20query-key%20matching%20mechanism.%0AOur%20empirical%20evaluations%20demonstrate%20that%20PromptDSI%20matches%20IncDSI%20in%20managing%0Aforgetting%20while%20significantly%20enhancing%20recall%20by%20over%204%25%20on%20new%20corpora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptDSI%253A%2520Prompt-based%2520Rehearsal-free%2520Instance-wise%2520Incremental%250A%2520%2520Learning%2520for%2520Document%2520Retrieval%26entry.906535625%3DTuan-Luc%2520Huynh%2520and%2520Thuy-Trang%2520Vu%2520and%2520Weiqing%2520Wang%2520and%2520Yinwei%2520Wei%2520and%2520Trung%2520Le%2520and%2520Dragan%2520Gasevic%2520and%2520Yuan-Fang%2520Li%2520and%2520Thanh-Toan%2520Do%26entry.1292438233%3D%2520%2520Differentiable%2520Search%2520Index%2520%2528DSI%2529%2520utilizes%2520Pre-trained%2520Language%2520Models%2520%2528PLMs%2529%250Afor%2520efficient%2520document%2520retrieval%2520without%2520relying%2520on%2520external%2520indexes.%2520However%252C%250ADSIs%2520need%2520full%2520re-training%2520to%2520handle%2520updates%2520in%2520dynamic%2520corpora%252C%2520causing%250Asignificant%2520computational%2520inefficiencies.%2520We%2520introduce%2520PromptDSI%252C%2520a%250Arehearsal-free%252C%2520prompt-based%2520approach%2520for%2520instance-wise%2520incremental%2520learning%2520in%250Adocument%2520retrieval.%2520PromptDSI%2520attaches%2520prompts%2520to%2520the%2520frozen%2520PLM%2527s%2520encoder%2520of%250ADSI%252C%2520leveraging%2520its%2520powerful%2520representation%2520to%2520efficiently%2520index%2520new%2520corpora%250Awhile%2520maintaining%2520a%2520balance%2520between%2520stability%2520and%2520plasticity.%2520We%2520eliminate%2520the%250Ainitial%2520forward%2520pass%2520of%2520prompt-based%2520continual%2520learning%2520methods%2520that%2520doubles%250Atraining%2520and%2520inference%2520time.%2520Moreover%252C%2520we%2520propose%2520a%2520topic-aware%2520prompt%2520pool%250Athat%2520employs%2520neural%2520topic%2520embeddings%2520as%2520fixed%2520keys.%2520This%2520strategy%2520ensures%250Adiverse%2520and%2520effective%2520prompt%2520usage%252C%2520addressing%2520the%2520challenge%2520of%2520parameter%250Aunderutilization%2520caused%2520by%2520the%2520collapse%2520of%2520the%2520query-key%2520matching%2520mechanism.%250AOur%2520empirical%2520evaluations%2520demonstrate%2520that%2520PromptDSI%2520matches%2520IncDSI%2520in%2520managing%250Aforgetting%2520while%2520significantly%2520enhancing%2520recall%2520by%2520over%25204%2525%2520on%2520new%2520corpora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromptDSI%3A%20Prompt-based%20Rehearsal-free%20Instance-wise%20Incremental%0A%20%20Learning%20for%20Document%20Retrieval&entry.906535625=Tuan-Luc%20Huynh%20and%20Thuy-Trang%20Vu%20and%20Weiqing%20Wang%20and%20Yinwei%20Wei%20and%20Trung%20Le%20and%20Dragan%20Gasevic%20and%20Yuan-Fang%20Li%20and%20Thanh-Toan%20Do&entry.1292438233=%20%20Differentiable%20Search%20Index%20%28DSI%29%20utilizes%20Pre-trained%20Language%20Models%20%28PLMs%29%0Afor%20efficient%20document%20retrieval%20without%20relying%20on%20external%20indexes.%20However%2C%0ADSIs%20need%20full%20re-training%20to%20handle%20updates%20in%20dynamic%20corpora%2C%20causing%0Asignificant%20computational%20inefficiencies.%20We%20introduce%20PromptDSI%2C%20a%0Arehearsal-free%2C%20prompt-based%20approach%20for%20instance-wise%20incremental%20learning%20in%0Adocument%20retrieval.%20PromptDSI%20attaches%20prompts%20to%20the%20frozen%20PLM%27s%20encoder%20of%0ADSI%2C%20leveraging%20its%20powerful%20representation%20to%20efficiently%20index%20new%20corpora%0Awhile%20maintaining%20a%20balance%20between%20stability%20and%20plasticity.%20We%20eliminate%20the%0Ainitial%20forward%20pass%20of%20prompt-based%20continual%20learning%20methods%20that%20doubles%0Atraining%20and%20inference%20time.%20Moreover%2C%20we%20propose%20a%20topic-aware%20prompt%20pool%0Athat%20employs%20neural%20topic%20embeddings%20as%20fixed%20keys.%20This%20strategy%20ensures%0Adiverse%20and%20effective%20prompt%20usage%2C%20addressing%20the%20challenge%20of%20parameter%0Aunderutilization%20caused%20by%20the%20collapse%20of%20the%20query-key%20matching%20mechanism.%0AOur%20empirical%20evaluations%20demonstrate%20that%20PromptDSI%20matches%20IncDSI%20in%20managing%0Aforgetting%20while%20significantly%20enhancing%20recall%20by%20over%204%25%20on%20new%20corpora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12593v1&entry.124074799=Read"},
{"title": "Influence Maximization via Graph Neural Bandits", "author": "Yuting Feng and Vincent Y. F. Tan and Bogdan Cautis", "abstract": "  We consider a ubiquitous scenario in the study of Influence Maximization\n(IM), in which there is limited knowledge about the topology of the diffusion\nnetwork. We set the IM problem in a multi-round diffusion campaign, aiming to\nmaximize the number of distinct users that are influenced. Leveraging the\ncapability of bandit algorithms to effectively balance the objectives of\nexploration and exploitation, as well as the expressivity of neural networks,\nour study explores the application of neural bandit algorithms to the IM\nproblem. We propose the framework IM-GNB (Influence Maximization with Graph\nNeural Bandits), where we provide an estimate of the users' probabilities of\nbeing influenced by influencers (also known as diffusion seeds). This initial\nestimate forms the basis for constructing both an exploitation graph and an\nexploration one. Subsequently, IM-GNB handles the exploration-exploitation\ntradeoff, by selecting seed nodes in real-time using Graph Convolutional\nNetworks (GCN), in which the pre-estimated graphs are employed to refine the\ninfluencers' estimated rewards in each contextual setting. Through extensive\nexperiments on two large real-world datasets, we demonstrate the effectiveness\nof IM-GNB compared with other baseline methods, significantly improving the\nspread outcome of such diffusion campaigns, when the underlying network is\nunknown.\n", "link": "http://arxiv.org/abs/2406.12835v1", "date": "2024-06-18", "relevancy": 1.8362, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4854}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4425}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Influence%20Maximization%20via%20Graph%20Neural%20Bandits&body=Title%3A%20Influence%20Maximization%20via%20Graph%20Neural%20Bandits%0AAuthor%3A%20Yuting%20Feng%20and%20Vincent%20Y.%20F.%20Tan%20and%20Bogdan%20Cautis%0AAbstract%3A%20%20%20We%20consider%20a%20ubiquitous%20scenario%20in%20the%20study%20of%20Influence%20Maximization%0A%28IM%29%2C%20in%20which%20there%20is%20limited%20knowledge%20about%20the%20topology%20of%20the%20diffusion%0Anetwork.%20We%20set%20the%20IM%20problem%20in%20a%20multi-round%20diffusion%20campaign%2C%20aiming%20to%0Amaximize%20the%20number%20of%20distinct%20users%20that%20are%20influenced.%20Leveraging%20the%0Acapability%20of%20bandit%20algorithms%20to%20effectively%20balance%20the%20objectives%20of%0Aexploration%20and%20exploitation%2C%20as%20well%20as%20the%20expressivity%20of%20neural%20networks%2C%0Aour%20study%20explores%20the%20application%20of%20neural%20bandit%20algorithms%20to%20the%20IM%0Aproblem.%20We%20propose%20the%20framework%20IM-GNB%20%28Influence%20Maximization%20with%20Graph%0ANeural%20Bandits%29%2C%20where%20we%20provide%20an%20estimate%20of%20the%20users%27%20probabilities%20of%0Abeing%20influenced%20by%20influencers%20%28also%20known%20as%20diffusion%20seeds%29.%20This%20initial%0Aestimate%20forms%20the%20basis%20for%20constructing%20both%20an%20exploitation%20graph%20and%20an%0Aexploration%20one.%20Subsequently%2C%20IM-GNB%20handles%20the%20exploration-exploitation%0Atradeoff%2C%20by%20selecting%20seed%20nodes%20in%20real-time%20using%20Graph%20Convolutional%0ANetworks%20%28GCN%29%2C%20in%20which%20the%20pre-estimated%20graphs%20are%20employed%20to%20refine%20the%0Ainfluencers%27%20estimated%20rewards%20in%20each%20contextual%20setting.%20Through%20extensive%0Aexperiments%20on%20two%20large%20real-world%20datasets%2C%20we%20demonstrate%20the%20effectiveness%0Aof%20IM-GNB%20compared%20with%20other%20baseline%20methods%2C%20significantly%20improving%20the%0Aspread%20outcome%20of%20such%20diffusion%20campaigns%2C%20when%20the%20underlying%20network%20is%0Aunknown.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfluence%2520Maximization%2520via%2520Graph%2520Neural%2520Bandits%26entry.906535625%3DYuting%2520Feng%2520and%2520Vincent%2520Y.%2520F.%2520Tan%2520and%2520Bogdan%2520Cautis%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520ubiquitous%2520scenario%2520in%2520the%2520study%2520of%2520Influence%2520Maximization%250A%2528IM%2529%252C%2520in%2520which%2520there%2520is%2520limited%2520knowledge%2520about%2520the%2520topology%2520of%2520the%2520diffusion%250Anetwork.%2520We%2520set%2520the%2520IM%2520problem%2520in%2520a%2520multi-round%2520diffusion%2520campaign%252C%2520aiming%2520to%250Amaximize%2520the%2520number%2520of%2520distinct%2520users%2520that%2520are%2520influenced.%2520Leveraging%2520the%250Acapability%2520of%2520bandit%2520algorithms%2520to%2520effectively%2520balance%2520the%2520objectives%2520of%250Aexploration%2520and%2520exploitation%252C%2520as%2520well%2520as%2520the%2520expressivity%2520of%2520neural%2520networks%252C%250Aour%2520study%2520explores%2520the%2520application%2520of%2520neural%2520bandit%2520algorithms%2520to%2520the%2520IM%250Aproblem.%2520We%2520propose%2520the%2520framework%2520IM-GNB%2520%2528Influence%2520Maximization%2520with%2520Graph%250ANeural%2520Bandits%2529%252C%2520where%2520we%2520provide%2520an%2520estimate%2520of%2520the%2520users%2527%2520probabilities%2520of%250Abeing%2520influenced%2520by%2520influencers%2520%2528also%2520known%2520as%2520diffusion%2520seeds%2529.%2520This%2520initial%250Aestimate%2520forms%2520the%2520basis%2520for%2520constructing%2520both%2520an%2520exploitation%2520graph%2520and%2520an%250Aexploration%2520one.%2520Subsequently%252C%2520IM-GNB%2520handles%2520the%2520exploration-exploitation%250Atradeoff%252C%2520by%2520selecting%2520seed%2520nodes%2520in%2520real-time%2520using%2520Graph%2520Convolutional%250ANetworks%2520%2528GCN%2529%252C%2520in%2520which%2520the%2520pre-estimated%2520graphs%2520are%2520employed%2520to%2520refine%2520the%250Ainfluencers%2527%2520estimated%2520rewards%2520in%2520each%2520contextual%2520setting.%2520Through%2520extensive%250Aexperiments%2520on%2520two%2520large%2520real-world%2520datasets%252C%2520we%2520demonstrate%2520the%2520effectiveness%250Aof%2520IM-GNB%2520compared%2520with%2520other%2520baseline%2520methods%252C%2520significantly%2520improving%2520the%250Aspread%2520outcome%2520of%2520such%2520diffusion%2520campaigns%252C%2520when%2520the%2520underlying%2520network%2520is%250Aunknown.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influence%20Maximization%20via%20Graph%20Neural%20Bandits&entry.906535625=Yuting%20Feng%20and%20Vincent%20Y.%20F.%20Tan%20and%20Bogdan%20Cautis&entry.1292438233=%20%20We%20consider%20a%20ubiquitous%20scenario%20in%20the%20study%20of%20Influence%20Maximization%0A%28IM%29%2C%20in%20which%20there%20is%20limited%20knowledge%20about%20the%20topology%20of%20the%20diffusion%0Anetwork.%20We%20set%20the%20IM%20problem%20in%20a%20multi-round%20diffusion%20campaign%2C%20aiming%20to%0Amaximize%20the%20number%20of%20distinct%20users%20that%20are%20influenced.%20Leveraging%20the%0Acapability%20of%20bandit%20algorithms%20to%20effectively%20balance%20the%20objectives%20of%0Aexploration%20and%20exploitation%2C%20as%20well%20as%20the%20expressivity%20of%20neural%20networks%2C%0Aour%20study%20explores%20the%20application%20of%20neural%20bandit%20algorithms%20to%20the%20IM%0Aproblem.%20We%20propose%20the%20framework%20IM-GNB%20%28Influence%20Maximization%20with%20Graph%0ANeural%20Bandits%29%2C%20where%20we%20provide%20an%20estimate%20of%20the%20users%27%20probabilities%20of%0Abeing%20influenced%20by%20influencers%20%28also%20known%20as%20diffusion%20seeds%29.%20This%20initial%0Aestimate%20forms%20the%20basis%20for%20constructing%20both%20an%20exploitation%20graph%20and%20an%0Aexploration%20one.%20Subsequently%2C%20IM-GNB%20handles%20the%20exploration-exploitation%0Atradeoff%2C%20by%20selecting%20seed%20nodes%20in%20real-time%20using%20Graph%20Convolutional%0ANetworks%20%28GCN%29%2C%20in%20which%20the%20pre-estimated%20graphs%20are%20employed%20to%20refine%20the%0Ainfluencers%27%20estimated%20rewards%20in%20each%20contextual%20setting.%20Through%20extensive%0Aexperiments%20on%20two%20large%20real-world%20datasets%2C%20we%20demonstrate%20the%20effectiveness%0Aof%20IM-GNB%20compared%20with%20other%20baseline%20methods%2C%20significantly%20improving%20the%0Aspread%20outcome%20of%20such%20diffusion%20campaigns%2C%20when%20the%20underlying%20network%20is%0Aunknown.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12835v1&entry.124074799=Read"},
{"title": "EUvsDisinfo: a Dataset for Multilingual Detection of Pro-Kremlin\n  Disinformation in News Articles", "author": "Jo\u00e3o A. Leite and Olesya Razuvayevskaya and Kalina Bontcheva and Carolina Scarton", "abstract": "  This work introduces EUvsDisinfo, a multilingual dataset of trustworthy and\ndisinformation articles related to pro-Kremlin themes. It is sourced directly\nfrom the debunk articles written by experts leading the EUvsDisinfo project.\nOur dataset is the largest to-date resource in terms of the overall number of\narticles and distinct languages. It also provides the largest topical and\ntemporal coverage. Using this dataset, we investigate the dissemination of\npro-Kremlin disinformation across different languages, uncovering\nlanguage-specific patterns targeting specific disinformation topics. We further\nanalyse the evolution of topic distribution over an eight-year period, noting a\nsignificant surge in disinformation content before the full-scale invasion of\nUkraine in 2022. Lastly, we demonstrate the dataset's applicability in training\nmodels to effectively distinguish between disinformation and trustworthy\ncontent in multilingual settings.\n", "link": "http://arxiv.org/abs/2406.12614v1", "date": "2024-06-18", "relevancy": 1.4568, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4238}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3685}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EUvsDisinfo%3A%20a%20Dataset%20for%20Multilingual%20Detection%20of%20Pro-Kremlin%0A%20%20Disinformation%20in%20News%20Articles&body=Title%3A%20EUvsDisinfo%3A%20a%20Dataset%20for%20Multilingual%20Detection%20of%20Pro-Kremlin%0A%20%20Disinformation%20in%20News%20Articles%0AAuthor%3A%20Jo%C3%A3o%20A.%20Leite%20and%20Olesya%20Razuvayevskaya%20and%20Kalina%20Bontcheva%20and%20Carolina%20Scarton%0AAbstract%3A%20%20%20This%20work%20introduces%20EUvsDisinfo%2C%20a%20multilingual%20dataset%20of%20trustworthy%20and%0Adisinformation%20articles%20related%20to%20pro-Kremlin%20themes.%20It%20is%20sourced%20directly%0Afrom%20the%20debunk%20articles%20written%20by%20experts%20leading%20the%20EUvsDisinfo%20project.%0AOur%20dataset%20is%20the%20largest%20to-date%20resource%20in%20terms%20of%20the%20overall%20number%20of%0Aarticles%20and%20distinct%20languages.%20It%20also%20provides%20the%20largest%20topical%20and%0Atemporal%20coverage.%20Using%20this%20dataset%2C%20we%20investigate%20the%20dissemination%20of%0Apro-Kremlin%20disinformation%20across%20different%20languages%2C%20uncovering%0Alanguage-specific%20patterns%20targeting%20specific%20disinformation%20topics.%20We%20further%0Aanalyse%20the%20evolution%20of%20topic%20distribution%20over%20an%20eight-year%20period%2C%20noting%20a%0Asignificant%20surge%20in%20disinformation%20content%20before%20the%20full-scale%20invasion%20of%0AUkraine%20in%202022.%20Lastly%2C%20we%20demonstrate%20the%20dataset%27s%20applicability%20in%20training%0Amodels%20to%20effectively%20distinguish%20between%20disinformation%20and%20trustworthy%0Acontent%20in%20multilingual%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEUvsDisinfo%253A%2520a%2520Dataset%2520for%2520Multilingual%2520Detection%2520of%2520Pro-Kremlin%250A%2520%2520Disinformation%2520in%2520News%2520Articles%26entry.906535625%3DJo%25C3%25A3o%2520A.%2520Leite%2520and%2520Olesya%2520Razuvayevskaya%2520and%2520Kalina%2520Bontcheva%2520and%2520Carolina%2520Scarton%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520EUvsDisinfo%252C%2520a%2520multilingual%2520dataset%2520of%2520trustworthy%2520and%250Adisinformation%2520articles%2520related%2520to%2520pro-Kremlin%2520themes.%2520It%2520is%2520sourced%2520directly%250Afrom%2520the%2520debunk%2520articles%2520written%2520by%2520experts%2520leading%2520the%2520EUvsDisinfo%2520project.%250AOur%2520dataset%2520is%2520the%2520largest%2520to-date%2520resource%2520in%2520terms%2520of%2520the%2520overall%2520number%2520of%250Aarticles%2520and%2520distinct%2520languages.%2520It%2520also%2520provides%2520the%2520largest%2520topical%2520and%250Atemporal%2520coverage.%2520Using%2520this%2520dataset%252C%2520we%2520investigate%2520the%2520dissemination%2520of%250Apro-Kremlin%2520disinformation%2520across%2520different%2520languages%252C%2520uncovering%250Alanguage-specific%2520patterns%2520targeting%2520specific%2520disinformation%2520topics.%2520We%2520further%250Aanalyse%2520the%2520evolution%2520of%2520topic%2520distribution%2520over%2520an%2520eight-year%2520period%252C%2520noting%2520a%250Asignificant%2520surge%2520in%2520disinformation%2520content%2520before%2520the%2520full-scale%2520invasion%2520of%250AUkraine%2520in%25202022.%2520Lastly%252C%2520we%2520demonstrate%2520the%2520dataset%2527s%2520applicability%2520in%2520training%250Amodels%2520to%2520effectively%2520distinguish%2520between%2520disinformation%2520and%2520trustworthy%250Acontent%2520in%2520multilingual%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EUvsDisinfo%3A%20a%20Dataset%20for%20Multilingual%20Detection%20of%20Pro-Kremlin%0A%20%20Disinformation%20in%20News%20Articles&entry.906535625=Jo%C3%A3o%20A.%20Leite%20and%20Olesya%20Razuvayevskaya%20and%20Kalina%20Bontcheva%20and%20Carolina%20Scarton&entry.1292438233=%20%20This%20work%20introduces%20EUvsDisinfo%2C%20a%20multilingual%20dataset%20of%20trustworthy%20and%0Adisinformation%20articles%20related%20to%20pro-Kremlin%20themes.%20It%20is%20sourced%20directly%0Afrom%20the%20debunk%20articles%20written%20by%20experts%20leading%20the%20EUvsDisinfo%20project.%0AOur%20dataset%20is%20the%20largest%20to-date%20resource%20in%20terms%20of%20the%20overall%20number%20of%0Aarticles%20and%20distinct%20languages.%20It%20also%20provides%20the%20largest%20topical%20and%0Atemporal%20coverage.%20Using%20this%20dataset%2C%20we%20investigate%20the%20dissemination%20of%0Apro-Kremlin%20disinformation%20across%20different%20languages%2C%20uncovering%0Alanguage-specific%20patterns%20targeting%20specific%20disinformation%20topics.%20We%20further%0Aanalyse%20the%20evolution%20of%20topic%20distribution%20over%20an%20eight-year%20period%2C%20noting%20a%0Asignificant%20surge%20in%20disinformation%20content%20before%20the%20full-scale%20invasion%20of%0AUkraine%20in%202022.%20Lastly%2C%20we%20demonstrate%20the%20dataset%27s%20applicability%20in%20training%0Amodels%20to%20effectively%20distinguish%20between%20disinformation%20and%20trustworthy%0Acontent%20in%20multilingual%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12614v1&entry.124074799=Read"},
{"title": "On Efficiently Representing Regular Languages as RNNs", "author": "Anej Svete and Robin Shing Moon Chan and Ryan Cotterell", "abstract": "  Recent work by Hewitt et al. (2020) provides an interpretation of the\nempirical success of recurrent neural networks (RNNs) as language models (LMs).\nIt shows that RNNs can efficiently represent bounded hierarchical structures\nthat are prevalent in human language. This suggests that RNNs' success might be\nlinked to their ability to model hierarchy. However, a closer inspection of\nHewitt et al.'s (2020) construction shows that it is not inherently limited to\nhierarchical structures. This poses a natural question: What other classes of\nLMs can RNNs efficiently represent? To this end, we generalize Hewitt et al.'s\n(2020) construction and show that RNNs can efficiently represent a larger class\nof LMs than previously claimed -- specifically, those that can be represented\nby a pushdown automaton with a bounded stack and a specific stack update\nfunction. Altogether, the efficiency of representing this diverse class of LMs\nwith RNN LMs suggests novel interpretations of their inductive bias.\n", "link": "http://arxiv.org/abs/2402.15814v2", "date": "2024-06-18", "relevancy": 1.7321, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.44}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4386}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Efficiently%20Representing%20Regular%20Languages%20as%20RNNs&body=Title%3A%20On%20Efficiently%20Representing%20Regular%20Languages%20as%20RNNs%0AAuthor%3A%20Anej%20Svete%20and%20Robin%20Shing%20Moon%20Chan%20and%20Ryan%20Cotterell%0AAbstract%3A%20%20%20Recent%20work%20by%20Hewitt%20et%20al.%20%282020%29%20provides%20an%20interpretation%20of%20the%0Aempirical%20success%20of%20recurrent%20neural%20networks%20%28RNNs%29%20as%20language%20models%20%28LMs%29.%0AIt%20shows%20that%20RNNs%20can%20efficiently%20represent%20bounded%20hierarchical%20structures%0Athat%20are%20prevalent%20in%20human%20language.%20This%20suggests%20that%20RNNs%27%20success%20might%20be%0Alinked%20to%20their%20ability%20to%20model%20hierarchy.%20However%2C%20a%20closer%20inspection%20of%0AHewitt%20et%20al.%27s%20%282020%29%20construction%20shows%20that%20it%20is%20not%20inherently%20limited%20to%0Ahierarchical%20structures.%20This%20poses%20a%20natural%20question%3A%20What%20other%20classes%20of%0ALMs%20can%20RNNs%20efficiently%20represent%3F%20To%20this%20end%2C%20we%20generalize%20Hewitt%20et%20al.%27s%0A%282020%29%20construction%20and%20show%20that%20RNNs%20can%20efficiently%20represent%20a%20larger%20class%0Aof%20LMs%20than%20previously%20claimed%20--%20specifically%2C%20those%20that%20can%20be%20represented%0Aby%20a%20pushdown%20automaton%20with%20a%20bounded%20stack%20and%20a%20specific%20stack%20update%0Afunction.%20Altogether%2C%20the%20efficiency%20of%20representing%20this%20diverse%20class%20of%20LMs%0Awith%20RNN%20LMs%20suggests%20novel%20interpretations%20of%20their%20inductive%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Efficiently%2520Representing%2520Regular%2520Languages%2520as%2520RNNs%26entry.906535625%3DAnej%2520Svete%2520and%2520Robin%2520Shing%2520Moon%2520Chan%2520and%2520Ryan%2520Cotterell%26entry.1292438233%3D%2520%2520Recent%2520work%2520by%2520Hewitt%2520et%2520al.%2520%25282020%2529%2520provides%2520an%2520interpretation%2520of%2520the%250Aempirical%2520success%2520of%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520as%2520language%2520models%2520%2528LMs%2529.%250AIt%2520shows%2520that%2520RNNs%2520can%2520efficiently%2520represent%2520bounded%2520hierarchical%2520structures%250Athat%2520are%2520prevalent%2520in%2520human%2520language.%2520This%2520suggests%2520that%2520RNNs%2527%2520success%2520might%2520be%250Alinked%2520to%2520their%2520ability%2520to%2520model%2520hierarchy.%2520However%252C%2520a%2520closer%2520inspection%2520of%250AHewitt%2520et%2520al.%2527s%2520%25282020%2529%2520construction%2520shows%2520that%2520it%2520is%2520not%2520inherently%2520limited%2520to%250Ahierarchical%2520structures.%2520This%2520poses%2520a%2520natural%2520question%253A%2520What%2520other%2520classes%2520of%250ALMs%2520can%2520RNNs%2520efficiently%2520represent%253F%2520To%2520this%2520end%252C%2520we%2520generalize%2520Hewitt%2520et%2520al.%2527s%250A%25282020%2529%2520construction%2520and%2520show%2520that%2520RNNs%2520can%2520efficiently%2520represent%2520a%2520larger%2520class%250Aof%2520LMs%2520than%2520previously%2520claimed%2520--%2520specifically%252C%2520those%2520that%2520can%2520be%2520represented%250Aby%2520a%2520pushdown%2520automaton%2520with%2520a%2520bounded%2520stack%2520and%2520a%2520specific%2520stack%2520update%250Afunction.%2520Altogether%252C%2520the%2520efficiency%2520of%2520representing%2520this%2520diverse%2520class%2520of%2520LMs%250Awith%2520RNN%2520LMs%2520suggests%2520novel%2520interpretations%2520of%2520their%2520inductive%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Efficiently%20Representing%20Regular%20Languages%20as%20RNNs&entry.906535625=Anej%20Svete%20and%20Robin%20Shing%20Moon%20Chan%20and%20Ryan%20Cotterell&entry.1292438233=%20%20Recent%20work%20by%20Hewitt%20et%20al.%20%282020%29%20provides%20an%20interpretation%20of%20the%0Aempirical%20success%20of%20recurrent%20neural%20networks%20%28RNNs%29%20as%20language%20models%20%28LMs%29.%0AIt%20shows%20that%20RNNs%20can%20efficiently%20represent%20bounded%20hierarchical%20structures%0Athat%20are%20prevalent%20in%20human%20language.%20This%20suggests%20that%20RNNs%27%20success%20might%20be%0Alinked%20to%20their%20ability%20to%20model%20hierarchy.%20However%2C%20a%20closer%20inspection%20of%0AHewitt%20et%20al.%27s%20%282020%29%20construction%20shows%20that%20it%20is%20not%20inherently%20limited%20to%0Ahierarchical%20structures.%20This%20poses%20a%20natural%20question%3A%20What%20other%20classes%20of%0ALMs%20can%20RNNs%20efficiently%20represent%3F%20To%20this%20end%2C%20we%20generalize%20Hewitt%20et%20al.%27s%0A%282020%29%20construction%20and%20show%20that%20RNNs%20can%20efficiently%20represent%20a%20larger%20class%0Aof%20LMs%20than%20previously%20claimed%20--%20specifically%2C%20those%20that%20can%20be%20represented%0Aby%20a%20pushdown%20automaton%20with%20a%20bounded%20stack%20and%20a%20specific%20stack%20update%0Afunction.%20Altogether%2C%20the%20efficiency%20of%20representing%20this%20diverse%20class%20of%20LMs%0Awith%20RNN%20LMs%20suggests%20novel%20interpretations%20of%20their%20inductive%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15814v2&entry.124074799=Read"},
{"title": "Segmentation and Characterization of Macerated Fibers and Vessels Using\n  Deep Learning", "author": "Saqib Qamar and Abu Imran Baba and St\u00e9phane Verger and Magnus Andersson", "abstract": "  Wood comprises different cell types, such as fibers, tracheids and vessels,\ndefining its properties. Studying cells' shape, size, and arrangement in\nmicroscopy images is crucial for understanding wood characteristics. Typically,\nthis involves macerating (soaking) samples in a solution to separate cells,\nthen spreading them on slides for imaging with a microscope that covers a wide\narea, capturing thousands of cells. However, these cells often cluster and\noverlap in images, making the segmentation difficult and time-consuming using\nstandard image-processing methods. In this work, we developed an automatic deep\nlearning segmentation approach that utilizes the one-stage YOLOv8 model for\nfast and accurate segmentation and characterization of macerated fiber and\nvessel form aspen trees in microscopy images. The model can analyze 32,640 x\n25,920 pixels images and demonstrate effective cell detection and segmentation,\nachieving a mAP_{0.5-0.95} of 78 %. To assess the model's robustness, we\nexamined fibers from a genetically modified tree line known for longer fibers.\nThe outcomes were comparable to previous manual measurements. Additionally, we\ncreated a user-friendly web application for image analysis and provided the\ncode for use on Google Colab. By leveraging YOLOv8's advances, this work\nprovides a deep learning solution to enable efficient quantification and\nanalysis of wood cells suitable for practical applications.\n", "link": "http://arxiv.org/abs/2401.16937v2", "date": "2024-06-18", "relevancy": 1.8339, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4666}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4546}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation%20and%20Characterization%20of%20Macerated%20Fibers%20and%20Vessels%20Using%0A%20%20Deep%20Learning&body=Title%3A%20Segmentation%20and%20Characterization%20of%20Macerated%20Fibers%20and%20Vessels%20Using%0A%20%20Deep%20Learning%0AAuthor%3A%20Saqib%20Qamar%20and%20Abu%20Imran%20Baba%20and%20St%C3%A9phane%20Verger%20and%20Magnus%20Andersson%0AAbstract%3A%20%20%20Wood%20comprises%20different%20cell%20types%2C%20such%20as%20fibers%2C%20tracheids%20and%20vessels%2C%0Adefining%20its%20properties.%20Studying%20cells%27%20shape%2C%20size%2C%20and%20arrangement%20in%0Amicroscopy%20images%20is%20crucial%20for%20understanding%20wood%20characteristics.%20Typically%2C%0Athis%20involves%20macerating%20%28soaking%29%20samples%20in%20a%20solution%20to%20separate%20cells%2C%0Athen%20spreading%20them%20on%20slides%20for%20imaging%20with%20a%20microscope%20that%20covers%20a%20wide%0Aarea%2C%20capturing%20thousands%20of%20cells.%20However%2C%20these%20cells%20often%20cluster%20and%0Aoverlap%20in%20images%2C%20making%20the%20segmentation%20difficult%20and%20time-consuming%20using%0Astandard%20image-processing%20methods.%20In%20this%20work%2C%20we%20developed%20an%20automatic%20deep%0Alearning%20segmentation%20approach%20that%20utilizes%20the%20one-stage%20YOLOv8%20model%20for%0Afast%20and%20accurate%20segmentation%20and%20characterization%20of%20macerated%20fiber%20and%0Avessel%20form%20aspen%20trees%20in%20microscopy%20images.%20The%20model%20can%20analyze%2032%2C640%20x%0A25%2C920%20pixels%20images%20and%20demonstrate%20effective%20cell%20detection%20and%20segmentation%2C%0Aachieving%20a%20mAP_%7B0.5-0.95%7D%20of%2078%20%25.%20To%20assess%20the%20model%27s%20robustness%2C%20we%0Aexamined%20fibers%20from%20a%20genetically%20modified%20tree%20line%20known%20for%20longer%20fibers.%0AThe%20outcomes%20were%20comparable%20to%20previous%20manual%20measurements.%20Additionally%2C%20we%0Acreated%20a%20user-friendly%20web%20application%20for%20image%20analysis%20and%20provided%20the%0Acode%20for%20use%20on%20Google%20Colab.%20By%20leveraging%20YOLOv8%27s%20advances%2C%20this%20work%0Aprovides%20a%20deep%20learning%20solution%20to%20enable%20efficient%20quantification%20and%0Aanalysis%20of%20wood%20cells%20suitable%20for%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation%2520and%2520Characterization%2520of%2520Macerated%2520Fibers%2520and%2520Vessels%2520Using%250A%2520%2520Deep%2520Learning%26entry.906535625%3DSaqib%2520Qamar%2520and%2520Abu%2520Imran%2520Baba%2520and%2520St%25C3%25A9phane%2520Verger%2520and%2520Magnus%2520Andersson%26entry.1292438233%3D%2520%2520Wood%2520comprises%2520different%2520cell%2520types%252C%2520such%2520as%2520fibers%252C%2520tracheids%2520and%2520vessels%252C%250Adefining%2520its%2520properties.%2520Studying%2520cells%2527%2520shape%252C%2520size%252C%2520and%2520arrangement%2520in%250Amicroscopy%2520images%2520is%2520crucial%2520for%2520understanding%2520wood%2520characteristics.%2520Typically%252C%250Athis%2520involves%2520macerating%2520%2528soaking%2529%2520samples%2520in%2520a%2520solution%2520to%2520separate%2520cells%252C%250Athen%2520spreading%2520them%2520on%2520slides%2520for%2520imaging%2520with%2520a%2520microscope%2520that%2520covers%2520a%2520wide%250Aarea%252C%2520capturing%2520thousands%2520of%2520cells.%2520However%252C%2520these%2520cells%2520often%2520cluster%2520and%250Aoverlap%2520in%2520images%252C%2520making%2520the%2520segmentation%2520difficult%2520and%2520time-consuming%2520using%250Astandard%2520image-processing%2520methods.%2520In%2520this%2520work%252C%2520we%2520developed%2520an%2520automatic%2520deep%250Alearning%2520segmentation%2520approach%2520that%2520utilizes%2520the%2520one-stage%2520YOLOv8%2520model%2520for%250Afast%2520and%2520accurate%2520segmentation%2520and%2520characterization%2520of%2520macerated%2520fiber%2520and%250Avessel%2520form%2520aspen%2520trees%2520in%2520microscopy%2520images.%2520The%2520model%2520can%2520analyze%252032%252C640%2520x%250A25%252C920%2520pixels%2520images%2520and%2520demonstrate%2520effective%2520cell%2520detection%2520and%2520segmentation%252C%250Aachieving%2520a%2520mAP_%257B0.5-0.95%257D%2520of%252078%2520%2525.%2520To%2520assess%2520the%2520model%2527s%2520robustness%252C%2520we%250Aexamined%2520fibers%2520from%2520a%2520genetically%2520modified%2520tree%2520line%2520known%2520for%2520longer%2520fibers.%250AThe%2520outcomes%2520were%2520comparable%2520to%2520previous%2520manual%2520measurements.%2520Additionally%252C%2520we%250Acreated%2520a%2520user-friendly%2520web%2520application%2520for%2520image%2520analysis%2520and%2520provided%2520the%250Acode%2520for%2520use%2520on%2520Google%2520Colab.%2520By%2520leveraging%2520YOLOv8%2527s%2520advances%252C%2520this%2520work%250Aprovides%2520a%2520deep%2520learning%2520solution%2520to%2520enable%2520efficient%2520quantification%2520and%250Aanalysis%2520of%2520wood%2520cells%2520suitable%2520for%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20and%20Characterization%20of%20Macerated%20Fibers%20and%20Vessels%20Using%0A%20%20Deep%20Learning&entry.906535625=Saqib%20Qamar%20and%20Abu%20Imran%20Baba%20and%20St%C3%A9phane%20Verger%20and%20Magnus%20Andersson&entry.1292438233=%20%20Wood%20comprises%20different%20cell%20types%2C%20such%20as%20fibers%2C%20tracheids%20and%20vessels%2C%0Adefining%20its%20properties.%20Studying%20cells%27%20shape%2C%20size%2C%20and%20arrangement%20in%0Amicroscopy%20images%20is%20crucial%20for%20understanding%20wood%20characteristics.%20Typically%2C%0Athis%20involves%20macerating%20%28soaking%29%20samples%20in%20a%20solution%20to%20separate%20cells%2C%0Athen%20spreading%20them%20on%20slides%20for%20imaging%20with%20a%20microscope%20that%20covers%20a%20wide%0Aarea%2C%20capturing%20thousands%20of%20cells.%20However%2C%20these%20cells%20often%20cluster%20and%0Aoverlap%20in%20images%2C%20making%20the%20segmentation%20difficult%20and%20time-consuming%20using%0Astandard%20image-processing%20methods.%20In%20this%20work%2C%20we%20developed%20an%20automatic%20deep%0Alearning%20segmentation%20approach%20that%20utilizes%20the%20one-stage%20YOLOv8%20model%20for%0Afast%20and%20accurate%20segmentation%20and%20characterization%20of%20macerated%20fiber%20and%0Avessel%20form%20aspen%20trees%20in%20microscopy%20images.%20The%20model%20can%20analyze%2032%2C640%20x%0A25%2C920%20pixels%20images%20and%20demonstrate%20effective%20cell%20detection%20and%20segmentation%2C%0Aachieving%20a%20mAP_%7B0.5-0.95%7D%20of%2078%20%25.%20To%20assess%20the%20model%27s%20robustness%2C%20we%0Aexamined%20fibers%20from%20a%20genetically%20modified%20tree%20line%20known%20for%20longer%20fibers.%0AThe%20outcomes%20were%20comparable%20to%20previous%20manual%20measurements.%20Additionally%2C%20we%0Acreated%20a%20user-friendly%20web%20application%20for%20image%20analysis%20and%20provided%20the%0Acode%20for%20use%20on%20Google%20Colab.%20By%20leveraging%20YOLOv8%27s%20advances%2C%20this%20work%0Aprovides%20a%20deep%20learning%20solution%20to%20enable%20efficient%20quantification%20and%0Aanalysis%20of%20wood%20cells%20suitable%20for%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16937v2&entry.124074799=Read"},
{"title": "Scalable and Flexible Causal Discovery with an Efficient Test for\n  Adjacency", "author": "Alan Nawzad Amin and Andrew Gordon Wilson", "abstract": "  To make accurate predictions, understand mechanisms, and design interventions\nin systems of many variables, we wish to learn causal graphs from large scale\ndata. Unfortunately the space of all possible causal graphs is enormous so\nscalably and accurately searching for the best fit to the data is a challenge.\nIn principle we could substantially decrease the search space, or learn the\ngraph entirely, by testing the conditional independence of variables. However,\ndeciding if two variables are adjacent in a causal graph may require an\nexponential number of tests. Here we build a scalable and flexible method to\nevaluate if two variables are adjacent in a causal graph, the Differentiable\nAdjacency Test (DAT). DAT replaces an exponential number of tests with a\nprovably equivalent relaxed problem. It then solves this problem by training\ntwo neural networks. We build a graph learning method based on DAT, DAT-Graph,\nthat can also learn from data with interventions. DAT-Graph can learn graphs of\n1000 variables with state of the art accuracy. Using the graph learned by\nDAT-Graph, we also build models that make much more accurate predictions of the\neffects of interventions on large scale RNA sequencing data.\n", "link": "http://arxiv.org/abs/2406.09177v2", "date": "2024-06-18", "relevancy": 1.4003, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4755}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4569}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20and%20Flexible%20Causal%20Discovery%20with%20an%20Efficient%20Test%20for%0A%20%20Adjacency&body=Title%3A%20Scalable%20and%20Flexible%20Causal%20Discovery%20with%20an%20Efficient%20Test%20for%0A%20%20Adjacency%0AAuthor%3A%20Alan%20Nawzad%20Amin%20and%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20To%20make%20accurate%20predictions%2C%20understand%20mechanisms%2C%20and%20design%20interventions%0Ain%20systems%20of%20many%20variables%2C%20we%20wish%20to%20learn%20causal%20graphs%20from%20large%20scale%0Adata.%20Unfortunately%20the%20space%20of%20all%20possible%20causal%20graphs%20is%20enormous%20so%0Ascalably%20and%20accurately%20searching%20for%20the%20best%20fit%20to%20the%20data%20is%20a%20challenge.%0AIn%20principle%20we%20could%20substantially%20decrease%20the%20search%20space%2C%20or%20learn%20the%0Agraph%20entirely%2C%20by%20testing%20the%20conditional%20independence%20of%20variables.%20However%2C%0Adeciding%20if%20two%20variables%20are%20adjacent%20in%20a%20causal%20graph%20may%20require%20an%0Aexponential%20number%20of%20tests.%20Here%20we%20build%20a%20scalable%20and%20flexible%20method%20to%0Aevaluate%20if%20two%20variables%20are%20adjacent%20in%20a%20causal%20graph%2C%20the%20Differentiable%0AAdjacency%20Test%20%28DAT%29.%20DAT%20replaces%20an%20exponential%20number%20of%20tests%20with%20a%0Aprovably%20equivalent%20relaxed%20problem.%20It%20then%20solves%20this%20problem%20by%20training%0Atwo%20neural%20networks.%20We%20build%20a%20graph%20learning%20method%20based%20on%20DAT%2C%20DAT-Graph%2C%0Athat%20can%20also%20learn%20from%20data%20with%20interventions.%20DAT-Graph%20can%20learn%20graphs%20of%0A1000%20variables%20with%20state%20of%20the%20art%20accuracy.%20Using%20the%20graph%20learned%20by%0ADAT-Graph%2C%20we%20also%20build%20models%20that%20make%20much%20more%20accurate%20predictions%20of%20the%0Aeffects%20of%20interventions%20on%20large%20scale%20RNA%20sequencing%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520and%2520Flexible%2520Causal%2520Discovery%2520with%2520an%2520Efficient%2520Test%2520for%250A%2520%2520Adjacency%26entry.906535625%3DAlan%2520Nawzad%2520Amin%2520and%2520Andrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520To%2520make%2520accurate%2520predictions%252C%2520understand%2520mechanisms%252C%2520and%2520design%2520interventions%250Ain%2520systems%2520of%2520many%2520variables%252C%2520we%2520wish%2520to%2520learn%2520causal%2520graphs%2520from%2520large%2520scale%250Adata.%2520Unfortunately%2520the%2520space%2520of%2520all%2520possible%2520causal%2520graphs%2520is%2520enormous%2520so%250Ascalably%2520and%2520accurately%2520searching%2520for%2520the%2520best%2520fit%2520to%2520the%2520data%2520is%2520a%2520challenge.%250AIn%2520principle%2520we%2520could%2520substantially%2520decrease%2520the%2520search%2520space%252C%2520or%2520learn%2520the%250Agraph%2520entirely%252C%2520by%2520testing%2520the%2520conditional%2520independence%2520of%2520variables.%2520However%252C%250Adeciding%2520if%2520two%2520variables%2520are%2520adjacent%2520in%2520a%2520causal%2520graph%2520may%2520require%2520an%250Aexponential%2520number%2520of%2520tests.%2520Here%2520we%2520build%2520a%2520scalable%2520and%2520flexible%2520method%2520to%250Aevaluate%2520if%2520two%2520variables%2520are%2520adjacent%2520in%2520a%2520causal%2520graph%252C%2520the%2520Differentiable%250AAdjacency%2520Test%2520%2528DAT%2529.%2520DAT%2520replaces%2520an%2520exponential%2520number%2520of%2520tests%2520with%2520a%250Aprovably%2520equivalent%2520relaxed%2520problem.%2520It%2520then%2520solves%2520this%2520problem%2520by%2520training%250Atwo%2520neural%2520networks.%2520We%2520build%2520a%2520graph%2520learning%2520method%2520based%2520on%2520DAT%252C%2520DAT-Graph%252C%250Athat%2520can%2520also%2520learn%2520from%2520data%2520with%2520interventions.%2520DAT-Graph%2520can%2520learn%2520graphs%2520of%250A1000%2520variables%2520with%2520state%2520of%2520the%2520art%2520accuracy.%2520Using%2520the%2520graph%2520learned%2520by%250ADAT-Graph%252C%2520we%2520also%2520build%2520models%2520that%2520make%2520much%2520more%2520accurate%2520predictions%2520of%2520the%250Aeffects%2520of%2520interventions%2520on%2520large%2520scale%2520RNA%2520sequencing%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20and%20Flexible%20Causal%20Discovery%20with%20an%20Efficient%20Test%20for%0A%20%20Adjacency&entry.906535625=Alan%20Nawzad%20Amin%20and%20Andrew%20Gordon%20Wilson&entry.1292438233=%20%20To%20make%20accurate%20predictions%2C%20understand%20mechanisms%2C%20and%20design%20interventions%0Ain%20systems%20of%20many%20variables%2C%20we%20wish%20to%20learn%20causal%20graphs%20from%20large%20scale%0Adata.%20Unfortunately%20the%20space%20of%20all%20possible%20causal%20graphs%20is%20enormous%20so%0Ascalably%20and%20accurately%20searching%20for%20the%20best%20fit%20to%20the%20data%20is%20a%20challenge.%0AIn%20principle%20we%20could%20substantially%20decrease%20the%20search%20space%2C%20or%20learn%20the%0Agraph%20entirely%2C%20by%20testing%20the%20conditional%20independence%20of%20variables.%20However%2C%0Adeciding%20if%20two%20variables%20are%20adjacent%20in%20a%20causal%20graph%20may%20require%20an%0Aexponential%20number%20of%20tests.%20Here%20we%20build%20a%20scalable%20and%20flexible%20method%20to%0Aevaluate%20if%20two%20variables%20are%20adjacent%20in%20a%20causal%20graph%2C%20the%20Differentiable%0AAdjacency%20Test%20%28DAT%29.%20DAT%20replaces%20an%20exponential%20number%20of%20tests%20with%20a%0Aprovably%20equivalent%20relaxed%20problem.%20It%20then%20solves%20this%20problem%20by%20training%0Atwo%20neural%20networks.%20We%20build%20a%20graph%20learning%20method%20based%20on%20DAT%2C%20DAT-Graph%2C%0Athat%20can%20also%20learn%20from%20data%20with%20interventions.%20DAT-Graph%20can%20learn%20graphs%20of%0A1000%20variables%20with%20state%20of%20the%20art%20accuracy.%20Using%20the%20graph%20learned%20by%0ADAT-Graph%2C%20we%20also%20build%20models%20that%20make%20much%20more%20accurate%20predictions%20of%20the%0Aeffects%20of%20interventions%20on%20large%20scale%20RNA%20sequencing%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09177v2&entry.124074799=Read"},
{"title": "Evaluating the design space of diffusion-based generative models", "author": "Yuqing Wang and Ye He and Molei Tao", "abstract": "  Most existing theoretical investigations of the accuracy of diffusion models,\nalbeit significant, assume the score function has been approximated to a\ncertain accuracy, and then use this a priori bound to control the error of\ngeneration. This article instead provides a first quantitative understanding of\nthe whole generation process, i.e., both training and sampling. More precisely,\nit conducts a non-asymptotic convergence analysis of denoising score matching\nunder gradient descent. In addition, a refined sampling error analysis for\nvariance exploding models is also provided. The combination of these two\nresults yields a full error analysis, which elucidates (again, but this time\ntheoretically) how to design the training and sampling processes for effective\ngeneration. For instance, our theory implies a preference toward noise\ndistribution and loss weighting that qualitatively agree with the ones used in\n[Karras et al. 2022]. It also provides some perspectives on why the time and\nvariance schedule used in [Karras et al. 2022] could be better tuned than the\npioneering version in [Song et al. 2020].\n", "link": "http://arxiv.org/abs/2406.12839v1", "date": "2024-06-18", "relevancy": 1.7144, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5957}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5661}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20design%20space%20of%20diffusion-based%20generative%20models&body=Title%3A%20Evaluating%20the%20design%20space%20of%20diffusion-based%20generative%20models%0AAuthor%3A%20Yuqing%20Wang%20and%20Ye%20He%20and%20Molei%20Tao%0AAbstract%3A%20%20%20Most%20existing%20theoretical%20investigations%20of%20the%20accuracy%20of%20diffusion%20models%2C%0Aalbeit%20significant%2C%20assume%20the%20score%20function%20has%20been%20approximated%20to%20a%0Acertain%20accuracy%2C%20and%20then%20use%20this%20a%20priori%20bound%20to%20control%20the%20error%20of%0Ageneration.%20This%20article%20instead%20provides%20a%20first%20quantitative%20understanding%20of%0Athe%20whole%20generation%20process%2C%20i.e.%2C%20both%20training%20and%20sampling.%20More%20precisely%2C%0Ait%20conducts%20a%20non-asymptotic%20convergence%20analysis%20of%20denoising%20score%20matching%0Aunder%20gradient%20descent.%20In%20addition%2C%20a%20refined%20sampling%20error%20analysis%20for%0Avariance%20exploding%20models%20is%20also%20provided.%20The%20combination%20of%20these%20two%0Aresults%20yields%20a%20full%20error%20analysis%2C%20which%20elucidates%20%28again%2C%20but%20this%20time%0Atheoretically%29%20how%20to%20design%20the%20training%20and%20sampling%20processes%20for%20effective%0Ageneration.%20For%20instance%2C%20our%20theory%20implies%20a%20preference%20toward%20noise%0Adistribution%20and%20loss%20weighting%20that%20qualitatively%20agree%20with%20the%20ones%20used%20in%0A%5BKarras%20et%20al.%202022%5D.%20It%20also%20provides%20some%20perspectives%20on%20why%20the%20time%20and%0Avariance%20schedule%20used%20in%20%5BKarras%20et%20al.%202022%5D%20could%20be%20better%20tuned%20than%20the%0Apioneering%20version%20in%20%5BSong%20et%20al.%202020%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520design%2520space%2520of%2520diffusion-based%2520generative%2520models%26entry.906535625%3DYuqing%2520Wang%2520and%2520Ye%2520He%2520and%2520Molei%2520Tao%26entry.1292438233%3D%2520%2520Most%2520existing%2520theoretical%2520investigations%2520of%2520the%2520accuracy%2520of%2520diffusion%2520models%252C%250Aalbeit%2520significant%252C%2520assume%2520the%2520score%2520function%2520has%2520been%2520approximated%2520to%2520a%250Acertain%2520accuracy%252C%2520and%2520then%2520use%2520this%2520a%2520priori%2520bound%2520to%2520control%2520the%2520error%2520of%250Ageneration.%2520This%2520article%2520instead%2520provides%2520a%2520first%2520quantitative%2520understanding%2520of%250Athe%2520whole%2520generation%2520process%252C%2520i.e.%252C%2520both%2520training%2520and%2520sampling.%2520More%2520precisely%252C%250Ait%2520conducts%2520a%2520non-asymptotic%2520convergence%2520analysis%2520of%2520denoising%2520score%2520matching%250Aunder%2520gradient%2520descent.%2520In%2520addition%252C%2520a%2520refined%2520sampling%2520error%2520analysis%2520for%250Avariance%2520exploding%2520models%2520is%2520also%2520provided.%2520The%2520combination%2520of%2520these%2520two%250Aresults%2520yields%2520a%2520full%2520error%2520analysis%252C%2520which%2520elucidates%2520%2528again%252C%2520but%2520this%2520time%250Atheoretically%2529%2520how%2520to%2520design%2520the%2520training%2520and%2520sampling%2520processes%2520for%2520effective%250Ageneration.%2520For%2520instance%252C%2520our%2520theory%2520implies%2520a%2520preference%2520toward%2520noise%250Adistribution%2520and%2520loss%2520weighting%2520that%2520qualitatively%2520agree%2520with%2520the%2520ones%2520used%2520in%250A%255BKarras%2520et%2520al.%25202022%255D.%2520It%2520also%2520provides%2520some%2520perspectives%2520on%2520why%2520the%2520time%2520and%250Avariance%2520schedule%2520used%2520in%2520%255BKarras%2520et%2520al.%25202022%255D%2520could%2520be%2520better%2520tuned%2520than%2520the%250Apioneering%2520version%2520in%2520%255BSong%2520et%2520al.%25202020%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20design%20space%20of%20diffusion-based%20generative%20models&entry.906535625=Yuqing%20Wang%20and%20Ye%20He%20and%20Molei%20Tao&entry.1292438233=%20%20Most%20existing%20theoretical%20investigations%20of%20the%20accuracy%20of%20diffusion%20models%2C%0Aalbeit%20significant%2C%20assume%20the%20score%20function%20has%20been%20approximated%20to%20a%0Acertain%20accuracy%2C%20and%20then%20use%20this%20a%20priori%20bound%20to%20control%20the%20error%20of%0Ageneration.%20This%20article%20instead%20provides%20a%20first%20quantitative%20understanding%20of%0Athe%20whole%20generation%20process%2C%20i.e.%2C%20both%20training%20and%20sampling.%20More%20precisely%2C%0Ait%20conducts%20a%20non-asymptotic%20convergence%20analysis%20of%20denoising%20score%20matching%0Aunder%20gradient%20descent.%20In%20addition%2C%20a%20refined%20sampling%20error%20analysis%20for%0Avariance%20exploding%20models%20is%20also%20provided.%20The%20combination%20of%20these%20two%0Aresults%20yields%20a%20full%20error%20analysis%2C%20which%20elucidates%20%28again%2C%20but%20this%20time%0Atheoretically%29%20how%20to%20design%20the%20training%20and%20sampling%20processes%20for%20effective%0Ageneration.%20For%20instance%2C%20our%20theory%20implies%20a%20preference%20toward%20noise%0Adistribution%20and%20loss%20weighting%20that%20qualitatively%20agree%20with%20the%20ones%20used%20in%0A%5BKarras%20et%20al.%202022%5D.%20It%20also%20provides%20some%20perspectives%20on%20why%20the%20time%20and%0Avariance%20schedule%20used%20in%20%5BKarras%20et%20al.%202022%5D%20could%20be%20better%20tuned%20than%20the%0Apioneering%20version%20in%20%5BSong%20et%20al.%202020%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12839v1&entry.124074799=Read"},
{"title": "Assistive Large Language Model Agents for Socially-Aware Negotiation\n  Dialogues", "author": "Yuncheng Hua and Lizhen Qu and Gholamreza Haffari", "abstract": "  We develop assistive agents based on Large Language Models (LLMs) that aid\ninterlocutors in business negotiations. Specifically, we simulate business\nnegotiations by letting two LLM-based agents engage in role play. A third LLM\nacts as a remediator agent to rewrite utterances violating norms for improving\nnegotiation outcomes. We introduce a simple tuning-free and label-free\nIn-Context Learning (ICL) method to identify high-quality ICL exemplars for the\nremediator, where we propose a novel select criteria, called value impact, to\nmeasure the quality of the negotiation outcomes. We provide rich empirical\nevidence to demonstrate its effectiveness in negotiations across three\ndifferent negotiation topics. The source code and the generated dataset will be\npublicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2402.01737v2", "date": "2024-06-18", "relevancy": 1.4035, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4943}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4709}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assistive%20Large%20Language%20Model%20Agents%20for%20Socially-Aware%20Negotiation%0A%20%20Dialogues&body=Title%3A%20Assistive%20Large%20Language%20Model%20Agents%20for%20Socially-Aware%20Negotiation%0A%20%20Dialogues%0AAuthor%3A%20Yuncheng%20Hua%20and%20Lizhen%20Qu%20and%20Gholamreza%20Haffari%0AAbstract%3A%20%20%20We%20develop%20assistive%20agents%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20that%20aid%0Ainterlocutors%20in%20business%20negotiations.%20Specifically%2C%20we%20simulate%20business%0Anegotiations%20by%20letting%20two%20LLM-based%20agents%20engage%20in%20role%20play.%20A%20third%20LLM%0Aacts%20as%20a%20remediator%20agent%20to%20rewrite%20utterances%20violating%20norms%20for%20improving%0Anegotiation%20outcomes.%20We%20introduce%20a%20simple%20tuning-free%20and%20label-free%0AIn-Context%20Learning%20%28ICL%29%20method%20to%20identify%20high-quality%20ICL%20exemplars%20for%20the%0Aremediator%2C%20where%20we%20propose%20a%20novel%20select%20criteria%2C%20called%20value%20impact%2C%20to%0Ameasure%20the%20quality%20of%20the%20negotiation%20outcomes.%20We%20provide%20rich%20empirical%0Aevidence%20to%20demonstrate%20its%20effectiveness%20in%20negotiations%20across%20three%0Adifferent%20negotiation%20topics.%20The%20source%20code%20and%20the%20generated%20dataset%20will%20be%0Apublicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01737v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssistive%2520Large%2520Language%2520Model%2520Agents%2520for%2520Socially-Aware%2520Negotiation%250A%2520%2520Dialogues%26entry.906535625%3DYuncheng%2520Hua%2520and%2520Lizhen%2520Qu%2520and%2520Gholamreza%2520Haffari%26entry.1292438233%3D%2520%2520We%2520develop%2520assistive%2520agents%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520that%2520aid%250Ainterlocutors%2520in%2520business%2520negotiations.%2520Specifically%252C%2520we%2520simulate%2520business%250Anegotiations%2520by%2520letting%2520two%2520LLM-based%2520agents%2520engage%2520in%2520role%2520play.%2520A%2520third%2520LLM%250Aacts%2520as%2520a%2520remediator%2520agent%2520to%2520rewrite%2520utterances%2520violating%2520norms%2520for%2520improving%250Anegotiation%2520outcomes.%2520We%2520introduce%2520a%2520simple%2520tuning-free%2520and%2520label-free%250AIn-Context%2520Learning%2520%2528ICL%2529%2520method%2520to%2520identify%2520high-quality%2520ICL%2520exemplars%2520for%2520the%250Aremediator%252C%2520where%2520we%2520propose%2520a%2520novel%2520select%2520criteria%252C%2520called%2520value%2520impact%252C%2520to%250Ameasure%2520the%2520quality%2520of%2520the%2520negotiation%2520outcomes.%2520We%2520provide%2520rich%2520empirical%250Aevidence%2520to%2520demonstrate%2520its%2520effectiveness%2520in%2520negotiations%2520across%2520three%250Adifferent%2520negotiation%2520topics.%2520The%2520source%2520code%2520and%2520the%2520generated%2520dataset%2520will%2520be%250Apublicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01737v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assistive%20Large%20Language%20Model%20Agents%20for%20Socially-Aware%20Negotiation%0A%20%20Dialogues&entry.906535625=Yuncheng%20Hua%20and%20Lizhen%20Qu%20and%20Gholamreza%20Haffari&entry.1292438233=%20%20We%20develop%20assistive%20agents%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20that%20aid%0Ainterlocutors%20in%20business%20negotiations.%20Specifically%2C%20we%20simulate%20business%0Anegotiations%20by%20letting%20two%20LLM-based%20agents%20engage%20in%20role%20play.%20A%20third%20LLM%0Aacts%20as%20a%20remediator%20agent%20to%20rewrite%20utterances%20violating%20norms%20for%20improving%0Anegotiation%20outcomes.%20We%20introduce%20a%20simple%20tuning-free%20and%20label-free%0AIn-Context%20Learning%20%28ICL%29%20method%20to%20identify%20high-quality%20ICL%20exemplars%20for%20the%0Aremediator%2C%20where%20we%20propose%20a%20novel%20select%20criteria%2C%20called%20value%20impact%2C%20to%0Ameasure%20the%20quality%20of%20the%20negotiation%20outcomes.%20We%20provide%20rich%20empirical%0Aevidence%20to%20demonstrate%20its%20effectiveness%20in%20negotiations%20across%20three%0Adifferent%20negotiation%20topics.%20The%20source%20code%20and%20the%20generated%20dataset%20will%20be%0Apublicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01737v2&entry.124074799=Read"},
{"title": "Learning Useful Representations of Recurrent Neural Network Weight\n  Matrices", "author": "Vincent Herrmann and Francesco Faccio and J\u00fcrgen Schmidhuber", "abstract": "  Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential\ncomputers. The program of an RNN is its weight matrix. How to learn useful\nrepresentations of RNN weights that facilitate RNN analysis as well as\ndownstream tasks? While the mechanistic approach directly looks at some RNN's\nweights to predict its behavior, the functionalist approach analyzes its\noverall functionality-specifically, its input-output mapping. We consider\nseveral mechanistic approaches for RNN weights and adapt the permutation\nequivariant Deep Weight Space layer for RNNs. Our two novel functionalist\napproaches extract information from RNN weights by 'interrogating' the RNN\nthrough probing inputs. We develop a theoretical framework that demonstrates\nconditions under which the functionalist approach can generate rich\nrepresentations that help determine RNN behavior. We release the first two\n'model zoo' datasets for RNN weight representation learning. One consists of\ngenerative models of a class of formal languages, and the other one of\nclassifiers of sequentially processed MNIST digits.With the help of an\nemulation-based self-supervised learning technique we compare and evaluate the\ndifferent RNN weight encoding techniques on multiple downstream applications.\nOn the most challenging one, namely predicting which exact task the RNN was\ntrained on, functionalist approaches show clear superiority.\n", "link": "http://arxiv.org/abs/2403.11998v2", "date": "2024-06-18", "relevancy": 1.5401, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5181}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Useful%20Representations%20of%20Recurrent%20Neural%20Network%20Weight%0A%20%20Matrices&body=Title%3A%20Learning%20Useful%20Representations%20of%20Recurrent%20Neural%20Network%20Weight%0A%20%20Matrices%0AAuthor%3A%20Vincent%20Herrmann%20and%20Francesco%20Faccio%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Recurrent%20Neural%20Networks%20%28RNNs%29%20are%20general-purpose%20parallel-sequential%0Acomputers.%20The%20program%20of%20an%20RNN%20is%20its%20weight%20matrix.%20How%20to%20learn%20useful%0Arepresentations%20of%20RNN%20weights%20that%20facilitate%20RNN%20analysis%20as%20well%20as%0Adownstream%20tasks%3F%20While%20the%20mechanistic%20approach%20directly%20looks%20at%20some%20RNN%27s%0Aweights%20to%20predict%20its%20behavior%2C%20the%20functionalist%20approach%20analyzes%20its%0Aoverall%20functionality-specifically%2C%20its%20input-output%20mapping.%20We%20consider%0Aseveral%20mechanistic%20approaches%20for%20RNN%20weights%20and%20adapt%20the%20permutation%0Aequivariant%20Deep%20Weight%20Space%20layer%20for%20RNNs.%20Our%20two%20novel%20functionalist%0Aapproaches%20extract%20information%20from%20RNN%20weights%20by%20%27interrogating%27%20the%20RNN%0Athrough%20probing%20inputs.%20We%20develop%20a%20theoretical%20framework%20that%20demonstrates%0Aconditions%20under%20which%20the%20functionalist%20approach%20can%20generate%20rich%0Arepresentations%20that%20help%20determine%20RNN%20behavior.%20We%20release%20the%20first%20two%0A%27model%20zoo%27%20datasets%20for%20RNN%20weight%20representation%20learning.%20One%20consists%20of%0Agenerative%20models%20of%20a%20class%20of%20formal%20languages%2C%20and%20the%20other%20one%20of%0Aclassifiers%20of%20sequentially%20processed%20MNIST%20digits.With%20the%20help%20of%20an%0Aemulation-based%20self-supervised%20learning%20technique%20we%20compare%20and%20evaluate%20the%0Adifferent%20RNN%20weight%20encoding%20techniques%20on%20multiple%20downstream%20applications.%0AOn%20the%20most%20challenging%20one%2C%20namely%20predicting%20which%20exact%20task%20the%20RNN%20was%0Atrained%20on%2C%20functionalist%20approaches%20show%20clear%20superiority.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Useful%2520Representations%2520of%2520Recurrent%2520Neural%2520Network%2520Weight%250A%2520%2520Matrices%26entry.906535625%3DVincent%2520Herrmann%2520and%2520Francesco%2520Faccio%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520are%2520general-purpose%2520parallel-sequential%250Acomputers.%2520The%2520program%2520of%2520an%2520RNN%2520is%2520its%2520weight%2520matrix.%2520How%2520to%2520learn%2520useful%250Arepresentations%2520of%2520RNN%2520weights%2520that%2520facilitate%2520RNN%2520analysis%2520as%2520well%2520as%250Adownstream%2520tasks%253F%2520While%2520the%2520mechanistic%2520approach%2520directly%2520looks%2520at%2520some%2520RNN%2527s%250Aweights%2520to%2520predict%2520its%2520behavior%252C%2520the%2520functionalist%2520approach%2520analyzes%2520its%250Aoverall%2520functionality-specifically%252C%2520its%2520input-output%2520mapping.%2520We%2520consider%250Aseveral%2520mechanistic%2520approaches%2520for%2520RNN%2520weights%2520and%2520adapt%2520the%2520permutation%250Aequivariant%2520Deep%2520Weight%2520Space%2520layer%2520for%2520RNNs.%2520Our%2520two%2520novel%2520functionalist%250Aapproaches%2520extract%2520information%2520from%2520RNN%2520weights%2520by%2520%2527interrogating%2527%2520the%2520RNN%250Athrough%2520probing%2520inputs.%2520We%2520develop%2520a%2520theoretical%2520framework%2520that%2520demonstrates%250Aconditions%2520under%2520which%2520the%2520functionalist%2520approach%2520can%2520generate%2520rich%250Arepresentations%2520that%2520help%2520determine%2520RNN%2520behavior.%2520We%2520release%2520the%2520first%2520two%250A%2527model%2520zoo%2527%2520datasets%2520for%2520RNN%2520weight%2520representation%2520learning.%2520One%2520consists%2520of%250Agenerative%2520models%2520of%2520a%2520class%2520of%2520formal%2520languages%252C%2520and%2520the%2520other%2520one%2520of%250Aclassifiers%2520of%2520sequentially%2520processed%2520MNIST%2520digits.With%2520the%2520help%2520of%2520an%250Aemulation-based%2520self-supervised%2520learning%2520technique%2520we%2520compare%2520and%2520evaluate%2520the%250Adifferent%2520RNN%2520weight%2520encoding%2520techniques%2520on%2520multiple%2520downstream%2520applications.%250AOn%2520the%2520most%2520challenging%2520one%252C%2520namely%2520predicting%2520which%2520exact%2520task%2520the%2520RNN%2520was%250Atrained%2520on%252C%2520functionalist%2520approaches%2520show%2520clear%2520superiority.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Useful%20Representations%20of%20Recurrent%20Neural%20Network%20Weight%0A%20%20Matrices&entry.906535625=Vincent%20Herrmann%20and%20Francesco%20Faccio%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Recurrent%20Neural%20Networks%20%28RNNs%29%20are%20general-purpose%20parallel-sequential%0Acomputers.%20The%20program%20of%20an%20RNN%20is%20its%20weight%20matrix.%20How%20to%20learn%20useful%0Arepresentations%20of%20RNN%20weights%20that%20facilitate%20RNN%20analysis%20as%20well%20as%0Adownstream%20tasks%3F%20While%20the%20mechanistic%20approach%20directly%20looks%20at%20some%20RNN%27s%0Aweights%20to%20predict%20its%20behavior%2C%20the%20functionalist%20approach%20analyzes%20its%0Aoverall%20functionality-specifically%2C%20its%20input-output%20mapping.%20We%20consider%0Aseveral%20mechanistic%20approaches%20for%20RNN%20weights%20and%20adapt%20the%20permutation%0Aequivariant%20Deep%20Weight%20Space%20layer%20for%20RNNs.%20Our%20two%20novel%20functionalist%0Aapproaches%20extract%20information%20from%20RNN%20weights%20by%20%27interrogating%27%20the%20RNN%0Athrough%20probing%20inputs.%20We%20develop%20a%20theoretical%20framework%20that%20demonstrates%0Aconditions%20under%20which%20the%20functionalist%20approach%20can%20generate%20rich%0Arepresentations%20that%20help%20determine%20RNN%20behavior.%20We%20release%20the%20first%20two%0A%27model%20zoo%27%20datasets%20for%20RNN%20weight%20representation%20learning.%20One%20consists%20of%0Agenerative%20models%20of%20a%20class%20of%20formal%20languages%2C%20and%20the%20other%20one%20of%0Aclassifiers%20of%20sequentially%20processed%20MNIST%20digits.With%20the%20help%20of%20an%0Aemulation-based%20self-supervised%20learning%20technique%20we%20compare%20and%20evaluate%20the%0Adifferent%20RNN%20weight%20encoding%20techniques%20on%20multiple%20downstream%20applications.%0AOn%20the%20most%20challenging%20one%2C%20namely%20predicting%20which%20exact%20task%20the%20RNN%20was%0Atrained%20on%2C%20functionalist%20approaches%20show%20clear%20superiority.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11998v2&entry.124074799=Read"},
{"title": "A Rate-Distortion View of Uncertainty Quantification", "author": "Ifigeneia Apostolopoulou and Benjamin Eysenbach and Frank Nielsen and Artur Dubrawski", "abstract": "  In supervised learning, understanding an input's proximity to the training\ndata can help a model decide whether it has sufficient evidence for reaching a\nreliable prediction. While powerful probabilistic models such as Gaussian\nProcesses naturally have this property, deep neural networks often lack it. In\nthis paper, we introduce Distance Aware Bottleneck (DAB), i.e., a new method\nfor enriching deep neural networks with this property. Building on prior\ninformation bottleneck approaches, our method learns a codebook that stores a\ncompressed representation of all inputs seen during training. The distance of a\nnew example from this codebook can serve as an uncertainty estimate for the\nexample. The resulting model is simple to train and provides deterministic\nuncertainty estimates by a single forward pass. Finally, our method achieves\nbetter out-of-distribution (OOD) detection and misclassification prediction\nthan prior methods, including expensive ensemble methods, deep kernel Gaussian\nProcesses, and approaches based on the standard information bottleneck.\n", "link": "http://arxiv.org/abs/2406.10775v2", "date": "2024-06-18", "relevancy": 1.6313, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.58}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Rate-Distortion%20View%20of%20Uncertainty%20Quantification&body=Title%3A%20A%20Rate-Distortion%20View%20of%20Uncertainty%20Quantification%0AAuthor%3A%20Ifigeneia%20Apostolopoulou%20and%20Benjamin%20Eysenbach%20and%20Frank%20Nielsen%20and%20Artur%20Dubrawski%0AAbstract%3A%20%20%20In%20supervised%20learning%2C%20understanding%20an%20input%27s%20proximity%20to%20the%20training%0Adata%20can%20help%20a%20model%20decide%20whether%20it%20has%20sufficient%20evidence%20for%20reaching%20a%0Areliable%20prediction.%20While%20powerful%20probabilistic%20models%20such%20as%20Gaussian%0AProcesses%20naturally%20have%20this%20property%2C%20deep%20neural%20networks%20often%20lack%20it.%20In%0Athis%20paper%2C%20we%20introduce%20Distance%20Aware%20Bottleneck%20%28DAB%29%2C%20i.e.%2C%20a%20new%20method%0Afor%20enriching%20deep%20neural%20networks%20with%20this%20property.%20Building%20on%20prior%0Ainformation%20bottleneck%20approaches%2C%20our%20method%20learns%20a%20codebook%20that%20stores%20a%0Acompressed%20representation%20of%20all%20inputs%20seen%20during%20training.%20The%20distance%20of%20a%0Anew%20example%20from%20this%20codebook%20can%20serve%20as%20an%20uncertainty%20estimate%20for%20the%0Aexample.%20The%20resulting%20model%20is%20simple%20to%20train%20and%20provides%20deterministic%0Auncertainty%20estimates%20by%20a%20single%20forward%20pass.%20Finally%2C%20our%20method%20achieves%0Abetter%20out-of-distribution%20%28OOD%29%20detection%20and%20misclassification%20prediction%0Athan%20prior%20methods%2C%20including%20expensive%20ensemble%20methods%2C%20deep%20kernel%20Gaussian%0AProcesses%2C%20and%20approaches%20based%20on%20the%20standard%20information%20bottleneck.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Rate-Distortion%2520View%2520of%2520Uncertainty%2520Quantification%26entry.906535625%3DIfigeneia%2520Apostolopoulou%2520and%2520Benjamin%2520Eysenbach%2520and%2520Frank%2520Nielsen%2520and%2520Artur%2520Dubrawski%26entry.1292438233%3D%2520%2520In%2520supervised%2520learning%252C%2520understanding%2520an%2520input%2527s%2520proximity%2520to%2520the%2520training%250Adata%2520can%2520help%2520a%2520model%2520decide%2520whether%2520it%2520has%2520sufficient%2520evidence%2520for%2520reaching%2520a%250Areliable%2520prediction.%2520While%2520powerful%2520probabilistic%2520models%2520such%2520as%2520Gaussian%250AProcesses%2520naturally%2520have%2520this%2520property%252C%2520deep%2520neural%2520networks%2520often%2520lack%2520it.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520Distance%2520Aware%2520Bottleneck%2520%2528DAB%2529%252C%2520i.e.%252C%2520a%2520new%2520method%250Afor%2520enriching%2520deep%2520neural%2520networks%2520with%2520this%2520property.%2520Building%2520on%2520prior%250Ainformation%2520bottleneck%2520approaches%252C%2520our%2520method%2520learns%2520a%2520codebook%2520that%2520stores%2520a%250Acompressed%2520representation%2520of%2520all%2520inputs%2520seen%2520during%2520training.%2520The%2520distance%2520of%2520a%250Anew%2520example%2520from%2520this%2520codebook%2520can%2520serve%2520as%2520an%2520uncertainty%2520estimate%2520for%2520the%250Aexample.%2520The%2520resulting%2520model%2520is%2520simple%2520to%2520train%2520and%2520provides%2520deterministic%250Auncertainty%2520estimates%2520by%2520a%2520single%2520forward%2520pass.%2520Finally%252C%2520our%2520method%2520achieves%250Abetter%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520and%2520misclassification%2520prediction%250Athan%2520prior%2520methods%252C%2520including%2520expensive%2520ensemble%2520methods%252C%2520deep%2520kernel%2520Gaussian%250AProcesses%252C%2520and%2520approaches%2520based%2520on%2520the%2520standard%2520information%2520bottleneck.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Rate-Distortion%20View%20of%20Uncertainty%20Quantification&entry.906535625=Ifigeneia%20Apostolopoulou%20and%20Benjamin%20Eysenbach%20and%20Frank%20Nielsen%20and%20Artur%20Dubrawski&entry.1292438233=%20%20In%20supervised%20learning%2C%20understanding%20an%20input%27s%20proximity%20to%20the%20training%0Adata%20can%20help%20a%20model%20decide%20whether%20it%20has%20sufficient%20evidence%20for%20reaching%20a%0Areliable%20prediction.%20While%20powerful%20probabilistic%20models%20such%20as%20Gaussian%0AProcesses%20naturally%20have%20this%20property%2C%20deep%20neural%20networks%20often%20lack%20it.%20In%0Athis%20paper%2C%20we%20introduce%20Distance%20Aware%20Bottleneck%20%28DAB%29%2C%20i.e.%2C%20a%20new%20method%0Afor%20enriching%20deep%20neural%20networks%20with%20this%20property.%20Building%20on%20prior%0Ainformation%20bottleneck%20approaches%2C%20our%20method%20learns%20a%20codebook%20that%20stores%20a%0Acompressed%20representation%20of%20all%20inputs%20seen%20during%20training.%20The%20distance%20of%20a%0Anew%20example%20from%20this%20codebook%20can%20serve%20as%20an%20uncertainty%20estimate%20for%20the%0Aexample.%20The%20resulting%20model%20is%20simple%20to%20train%20and%20provides%20deterministic%0Auncertainty%20estimates%20by%20a%20single%20forward%20pass.%20Finally%2C%20our%20method%20achieves%0Abetter%20out-of-distribution%20%28OOD%29%20detection%20and%20misclassification%20prediction%0Athan%20prior%20methods%2C%20including%20expensive%20ensemble%20methods%2C%20deep%20kernel%20Gaussian%0AProcesses%2C%20and%20approaches%20based%20on%20the%20standard%20information%20bottleneck.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10775v2&entry.124074799=Read"},
{"title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step", "author": "Owen Dugan and Donato Manuel Jimenez Beneto and Charlotte Loh and Zhuo Chen and Rumen Dangovski and Marin Solja\u010di\u0107", "abstract": "  Despite significant advancements in text generation and reasoning, Large\nLanguage Models (LLMs) still face challenges in accurately performing complex\narithmetic operations. To achieve accurate calculations, language model systems\noften enable LLMs to generate code for arithmetic operations. However, this\napproach compromises speed and security and, if finetuning is involved, risks\nthe language model losing prior capabilities. We propose a framework that\nenables exact arithmetic in \\textit{a single autoregressive step}, providing\nfaster, more secure, and more interpretable LLM systems with arithmetic\ncapabilities. We use the hidden states of an LLM to control a symbolic\narchitecture which performs arithmetic. Our implementation using Llama 3 8B\nInstruct with OccamNet as a symbolic model (OccamLlama) achieves 100\\% accuracy\non single arithmetic operations\n($+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$), outperforming GPT 4o\nand on par with GPT 4o using a code interpreter. OccamLlama also outperforms\nGPT 4o both with and without a code interpreter on mathematical problem solving\nbenchmarks involving challenging arithmetic, thus enabling small LLMs to match\nthe arithmetic performance of even much larger models. We will make our code\npublic shortly.\n", "link": "http://arxiv.org/abs/2406.06576v2", "date": "2024-06-18", "relevancy": 1.2221, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4046}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OccamLLM%3A%20Fast%20and%20Exact%20Language%20Model%20Arithmetic%20in%20a%20Single%20Step&body=Title%3A%20OccamLLM%3A%20Fast%20and%20Exact%20Language%20Model%20Arithmetic%20in%20a%20Single%20Step%0AAuthor%3A%20Owen%20Dugan%20and%20Donato%20Manuel%20Jimenez%20Beneto%20and%20Charlotte%20Loh%20and%20Zhuo%20Chen%20and%20Rumen%20Dangovski%20and%20Marin%20Solja%C4%8Di%C4%87%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20text%20generation%20and%20reasoning%2C%20Large%0ALanguage%20Models%20%28LLMs%29%20still%20face%20challenges%20in%20accurately%20performing%20complex%0Aarithmetic%20operations.%20To%20achieve%20accurate%20calculations%2C%20language%20model%20systems%0Aoften%20enable%20LLMs%20to%20generate%20code%20for%20arithmetic%20operations.%20However%2C%20this%0Aapproach%20compromises%20speed%20and%20security%20and%2C%20if%20finetuning%20is%20involved%2C%20risks%0Athe%20language%20model%20losing%20prior%20capabilities.%20We%20propose%20a%20framework%20that%0Aenables%20exact%20arithmetic%20in%20%5Ctextit%7Ba%20single%20autoregressive%20step%7D%2C%20providing%0Afaster%2C%20more%20secure%2C%20and%20more%20interpretable%20LLM%20systems%20with%20arithmetic%0Acapabilities.%20We%20use%20the%20hidden%20states%20of%20an%20LLM%20to%20control%20a%20symbolic%0Aarchitecture%20which%20performs%20arithmetic.%20Our%20implementation%20using%20Llama%203%208B%0AInstruct%20with%20OccamNet%20as%20a%20symbolic%20model%20%28OccamLlama%29%20achieves%20100%5C%25%20accuracy%0Aon%20single%20arithmetic%20operations%0A%28%24%2B%2C-%2C%5Ctimes%2C%5Cdiv%2C%5Csin%7B%7D%2C%5Ccos%7B%7D%2C%5Clog%7B%7D%2C%5Cexp%7B%7D%2C%5Csqrt%7B%7D%24%29%2C%20outperforming%20GPT%204o%0Aand%20on%20par%20with%20GPT%204o%20using%20a%20code%20interpreter.%20OccamLlama%20also%20outperforms%0AGPT%204o%20both%20with%20and%20without%20a%20code%20interpreter%20on%20mathematical%20problem%20solving%0Abenchmarks%20involving%20challenging%20arithmetic%2C%20thus%20enabling%20small%20LLMs%20to%20match%0Athe%20arithmetic%20performance%20of%20even%20much%20larger%20models.%20We%20will%20make%20our%20code%0Apublic%20shortly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06576v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccamLLM%253A%2520Fast%2520and%2520Exact%2520Language%2520Model%2520Arithmetic%2520in%2520a%2520Single%2520Step%26entry.906535625%3DOwen%2520Dugan%2520and%2520Donato%2520Manuel%2520Jimenez%2520Beneto%2520and%2520Charlotte%2520Loh%2520and%2520Zhuo%2520Chen%2520and%2520Rumen%2520Dangovski%2520and%2520Marin%2520Solja%25C4%258Di%25C4%2587%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520text%2520generation%2520and%2520reasoning%252C%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520still%2520face%2520challenges%2520in%2520accurately%2520performing%2520complex%250Aarithmetic%2520operations.%2520To%2520achieve%2520accurate%2520calculations%252C%2520language%2520model%2520systems%250Aoften%2520enable%2520LLMs%2520to%2520generate%2520code%2520for%2520arithmetic%2520operations.%2520However%252C%2520this%250Aapproach%2520compromises%2520speed%2520and%2520security%2520and%252C%2520if%2520finetuning%2520is%2520involved%252C%2520risks%250Athe%2520language%2520model%2520losing%2520prior%2520capabilities.%2520We%2520propose%2520a%2520framework%2520that%250Aenables%2520exact%2520arithmetic%2520in%2520%255Ctextit%257Ba%2520single%2520autoregressive%2520step%257D%252C%2520providing%250Afaster%252C%2520more%2520secure%252C%2520and%2520more%2520interpretable%2520LLM%2520systems%2520with%2520arithmetic%250Acapabilities.%2520We%2520use%2520the%2520hidden%2520states%2520of%2520an%2520LLM%2520to%2520control%2520a%2520symbolic%250Aarchitecture%2520which%2520performs%2520arithmetic.%2520Our%2520implementation%2520using%2520Llama%25203%25208B%250AInstruct%2520with%2520OccamNet%2520as%2520a%2520symbolic%2520model%2520%2528OccamLlama%2529%2520achieves%2520100%255C%2525%2520accuracy%250Aon%2520single%2520arithmetic%2520operations%250A%2528%2524%252B%252C-%252C%255Ctimes%252C%255Cdiv%252C%255Csin%257B%257D%252C%255Ccos%257B%257D%252C%255Clog%257B%257D%252C%255Cexp%257B%257D%252C%255Csqrt%257B%257D%2524%2529%252C%2520outperforming%2520GPT%25204o%250Aand%2520on%2520par%2520with%2520GPT%25204o%2520using%2520a%2520code%2520interpreter.%2520OccamLlama%2520also%2520outperforms%250AGPT%25204o%2520both%2520with%2520and%2520without%2520a%2520code%2520interpreter%2520on%2520mathematical%2520problem%2520solving%250Abenchmarks%2520involving%2520challenging%2520arithmetic%252C%2520thus%2520enabling%2520small%2520LLMs%2520to%2520match%250Athe%2520arithmetic%2520performance%2520of%2520even%2520much%2520larger%2520models.%2520We%2520will%2520make%2520our%2520code%250Apublic%2520shortly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06576v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccamLLM%3A%20Fast%20and%20Exact%20Language%20Model%20Arithmetic%20in%20a%20Single%20Step&entry.906535625=Owen%20Dugan%20and%20Donato%20Manuel%20Jimenez%20Beneto%20and%20Charlotte%20Loh%20and%20Zhuo%20Chen%20and%20Rumen%20Dangovski%20and%20Marin%20Solja%C4%8Di%C4%87&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20text%20generation%20and%20reasoning%2C%20Large%0ALanguage%20Models%20%28LLMs%29%20still%20face%20challenges%20in%20accurately%20performing%20complex%0Aarithmetic%20operations.%20To%20achieve%20accurate%20calculations%2C%20language%20model%20systems%0Aoften%20enable%20LLMs%20to%20generate%20code%20for%20arithmetic%20operations.%20However%2C%20this%0Aapproach%20compromises%20speed%20and%20security%20and%2C%20if%20finetuning%20is%20involved%2C%20risks%0Athe%20language%20model%20losing%20prior%20capabilities.%20We%20propose%20a%20framework%20that%0Aenables%20exact%20arithmetic%20in%20%5Ctextit%7Ba%20single%20autoregressive%20step%7D%2C%20providing%0Afaster%2C%20more%20secure%2C%20and%20more%20interpretable%20LLM%20systems%20with%20arithmetic%0Acapabilities.%20We%20use%20the%20hidden%20states%20of%20an%20LLM%20to%20control%20a%20symbolic%0Aarchitecture%20which%20performs%20arithmetic.%20Our%20implementation%20using%20Llama%203%208B%0AInstruct%20with%20OccamNet%20as%20a%20symbolic%20model%20%28OccamLlama%29%20achieves%20100%5C%25%20accuracy%0Aon%20single%20arithmetic%20operations%0A%28%24%2B%2C-%2C%5Ctimes%2C%5Cdiv%2C%5Csin%7B%7D%2C%5Ccos%7B%7D%2C%5Clog%7B%7D%2C%5Cexp%7B%7D%2C%5Csqrt%7B%7D%24%29%2C%20outperforming%20GPT%204o%0Aand%20on%20par%20with%20GPT%204o%20using%20a%20code%20interpreter.%20OccamLlama%20also%20outperforms%0AGPT%204o%20both%20with%20and%20without%20a%20code%20interpreter%20on%20mathematical%20problem%20solving%0Abenchmarks%20involving%20challenging%20arithmetic%2C%20thus%20enabling%20small%20LLMs%20to%20match%0Athe%20arithmetic%20performance%20of%20even%20much%20larger%20models.%20We%20will%20make%20our%20code%0Apublic%20shortly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06576v2&entry.124074799=Read"},
{"title": "Sparsifying dimensionality reduction of PDE solution data with Bregman\n  learning", "author": "Tjeerd Jan Heeringa and Christoph Brune and Mengwu Guo", "abstract": "  Classical model reduction techniques project the governing equations onto a\nlinear subspace of the original state space. More recent data-driven techniques\nuse neural networks to enable nonlinear projections. Whilst those often enable\nstronger compression, they may have redundant parameters and lead to suboptimal\nlatent dimensionality. To overcome these, we propose a multistep algorithm that\ninduces sparsity in the encoder-decoder networks for effective reduction in the\nnumber of parameters and additional compression of the latent space. This\nalgorithm starts with sparsely initialized a network and training it using\nlinearized Bregman iterations. These iterations have been very successful in\ncomputer vision and compressed sensing tasks, but have not yet been used for\nreduced-order modelling. After the training, we further compress the latent\nspace dimensionality by using a form of proper orthogonal decomposition. Last,\nwe use a bias propagation technique to change the induced sparsity into an\neffective reduction of parameters. We apply this algorithm to three\nrepresentative PDE models: 1D diffusion, 1D advection, and 2D\nreaction-diffusion. Compared to conventional training methods like Adam, the\nproposed method achieves similar accuracy with 30% less parameters and a\nsignificantly smaller latent space.\n", "link": "http://arxiv.org/abs/2406.12672v1", "date": "2024-06-18", "relevancy": 1.0762, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5596}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5452}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsifying%20dimensionality%20reduction%20of%20PDE%20solution%20data%20with%20Bregman%0A%20%20learning&body=Title%3A%20Sparsifying%20dimensionality%20reduction%20of%20PDE%20solution%20data%20with%20Bregman%0A%20%20learning%0AAuthor%3A%20Tjeerd%20Jan%20Heeringa%20and%20Christoph%20Brune%20and%20Mengwu%20Guo%0AAbstract%3A%20%20%20Classical%20model%20reduction%20techniques%20project%20the%20governing%20equations%20onto%20a%0Alinear%20subspace%20of%20the%20original%20state%20space.%20More%20recent%20data-driven%20techniques%0Ause%20neural%20networks%20to%20enable%20nonlinear%20projections.%20Whilst%20those%20often%20enable%0Astronger%20compression%2C%20they%20may%20have%20redundant%20parameters%20and%20lead%20to%20suboptimal%0Alatent%20dimensionality.%20To%20overcome%20these%2C%20we%20propose%20a%20multistep%20algorithm%20that%0Ainduces%20sparsity%20in%20the%20encoder-decoder%20networks%20for%20effective%20reduction%20in%20the%0Anumber%20of%20parameters%20and%20additional%20compression%20of%20the%20latent%20space.%20This%0Aalgorithm%20starts%20with%20sparsely%20initialized%20a%20network%20and%20training%20it%20using%0Alinearized%20Bregman%20iterations.%20These%20iterations%20have%20been%20very%20successful%20in%0Acomputer%20vision%20and%20compressed%20sensing%20tasks%2C%20but%20have%20not%20yet%20been%20used%20for%0Areduced-order%20modelling.%20After%20the%20training%2C%20we%20further%20compress%20the%20latent%0Aspace%20dimensionality%20by%20using%20a%20form%20of%20proper%20orthogonal%20decomposition.%20Last%2C%0Awe%20use%20a%20bias%20propagation%20technique%20to%20change%20the%20induced%20sparsity%20into%20an%0Aeffective%20reduction%20of%20parameters.%20We%20apply%20this%20algorithm%20to%20three%0Arepresentative%20PDE%20models%3A%201D%20diffusion%2C%201D%20advection%2C%20and%202D%0Areaction-diffusion.%20Compared%20to%20conventional%20training%20methods%20like%20Adam%2C%20the%0Aproposed%20method%20achieves%20similar%20accuracy%20with%2030%25%20less%20parameters%20and%20a%0Asignificantly%20smaller%20latent%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsifying%2520dimensionality%2520reduction%2520of%2520PDE%2520solution%2520data%2520with%2520Bregman%250A%2520%2520learning%26entry.906535625%3DTjeerd%2520Jan%2520Heeringa%2520and%2520Christoph%2520Brune%2520and%2520Mengwu%2520Guo%26entry.1292438233%3D%2520%2520Classical%2520model%2520reduction%2520techniques%2520project%2520the%2520governing%2520equations%2520onto%2520a%250Alinear%2520subspace%2520of%2520the%2520original%2520state%2520space.%2520More%2520recent%2520data-driven%2520techniques%250Ause%2520neural%2520networks%2520to%2520enable%2520nonlinear%2520projections.%2520Whilst%2520those%2520often%2520enable%250Astronger%2520compression%252C%2520they%2520may%2520have%2520redundant%2520parameters%2520and%2520lead%2520to%2520suboptimal%250Alatent%2520dimensionality.%2520To%2520overcome%2520these%252C%2520we%2520propose%2520a%2520multistep%2520algorithm%2520that%250Ainduces%2520sparsity%2520in%2520the%2520encoder-decoder%2520networks%2520for%2520effective%2520reduction%2520in%2520the%250Anumber%2520of%2520parameters%2520and%2520additional%2520compression%2520of%2520the%2520latent%2520space.%2520This%250Aalgorithm%2520starts%2520with%2520sparsely%2520initialized%2520a%2520network%2520and%2520training%2520it%2520using%250Alinearized%2520Bregman%2520iterations.%2520These%2520iterations%2520have%2520been%2520very%2520successful%2520in%250Acomputer%2520vision%2520and%2520compressed%2520sensing%2520tasks%252C%2520but%2520have%2520not%2520yet%2520been%2520used%2520for%250Areduced-order%2520modelling.%2520After%2520the%2520training%252C%2520we%2520further%2520compress%2520the%2520latent%250Aspace%2520dimensionality%2520by%2520using%2520a%2520form%2520of%2520proper%2520orthogonal%2520decomposition.%2520Last%252C%250Awe%2520use%2520a%2520bias%2520propagation%2520technique%2520to%2520change%2520the%2520induced%2520sparsity%2520into%2520an%250Aeffective%2520reduction%2520of%2520parameters.%2520We%2520apply%2520this%2520algorithm%2520to%2520three%250Arepresentative%2520PDE%2520models%253A%25201D%2520diffusion%252C%25201D%2520advection%252C%2520and%25202D%250Areaction-diffusion.%2520Compared%2520to%2520conventional%2520training%2520methods%2520like%2520Adam%252C%2520the%250Aproposed%2520method%2520achieves%2520similar%2520accuracy%2520with%252030%2525%2520less%2520parameters%2520and%2520a%250Asignificantly%2520smaller%2520latent%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsifying%20dimensionality%20reduction%20of%20PDE%20solution%20data%20with%20Bregman%0A%20%20learning&entry.906535625=Tjeerd%20Jan%20Heeringa%20and%20Christoph%20Brune%20and%20Mengwu%20Guo&entry.1292438233=%20%20Classical%20model%20reduction%20techniques%20project%20the%20governing%20equations%20onto%20a%0Alinear%20subspace%20of%20the%20original%20state%20space.%20More%20recent%20data-driven%20techniques%0Ause%20neural%20networks%20to%20enable%20nonlinear%20projections.%20Whilst%20those%20often%20enable%0Astronger%20compression%2C%20they%20may%20have%20redundant%20parameters%20and%20lead%20to%20suboptimal%0Alatent%20dimensionality.%20To%20overcome%20these%2C%20we%20propose%20a%20multistep%20algorithm%20that%0Ainduces%20sparsity%20in%20the%20encoder-decoder%20networks%20for%20effective%20reduction%20in%20the%0Anumber%20of%20parameters%20and%20additional%20compression%20of%20the%20latent%20space.%20This%0Aalgorithm%20starts%20with%20sparsely%20initialized%20a%20network%20and%20training%20it%20using%0Alinearized%20Bregman%20iterations.%20These%20iterations%20have%20been%20very%20successful%20in%0Acomputer%20vision%20and%20compressed%20sensing%20tasks%2C%20but%20have%20not%20yet%20been%20used%20for%0Areduced-order%20modelling.%20After%20the%20training%2C%20we%20further%20compress%20the%20latent%0Aspace%20dimensionality%20by%20using%20a%20form%20of%20proper%20orthogonal%20decomposition.%20Last%2C%0Awe%20use%20a%20bias%20propagation%20technique%20to%20change%20the%20induced%20sparsity%20into%20an%0Aeffective%20reduction%20of%20parameters.%20We%20apply%20this%20algorithm%20to%20three%0Arepresentative%20PDE%20models%3A%201D%20diffusion%2C%201D%20advection%2C%20and%202D%0Areaction-diffusion.%20Compared%20to%20conventional%20training%20methods%20like%20Adam%2C%20the%0Aproposed%20method%20achieves%20similar%20accuracy%20with%2030%25%20less%20parameters%20and%20a%0Asignificantly%20smaller%20latent%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12672v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


