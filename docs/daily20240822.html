<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240821.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Robust 3D Gaussian Splatting for Novel View Synthesis in Presence of\n  Distractors", "author": "Paul Ungermann and Armin Ettenhofer and Matthias Nie\u00dfner and Barbara Roessle", "abstract": "  3D Gaussian Splatting has shown impressive novel view synthesis results;\nnonetheless, it is vulnerable to dynamic objects polluting the input data of an\notherwise static scene, so called distractors. Distractors have severe impact\non the rendering quality as they get represented as view-dependent effects or\nresult in floating artifacts. Our goal is to identify and ignore such\ndistractors during the 3D Gaussian optimization to obtain a clean\nreconstruction. To this end, we take a self-supervised approach that looks at\nthe image residuals during the optimization to determine areas that have likely\nbeen falsified by a distractor. In addition, we leverage a pretrained\nsegmentation network to provide object awareness, enabling more accurate\nexclusion of distractors. This way, we obtain segmentation masks of distractors\nto effectively ignore them in the loss formulation. We demonstrate that our\napproach is robust to various distractors and strongly improves rendering\nquality on distractor-polluted scenes, improving PSNR by 1.86dB compared to 3D\nGaussian Splatting.\n", "link": "http://arxiv.org/abs/2408.11697v1", "date": "2024-08-21", "relevancy": 3.1101, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6811}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6327}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%203D%20Gaussian%20Splatting%20for%20Novel%20View%20Synthesis%20in%20Presence%20of%0A%20%20Distractors&body=Title%3A%20Robust%203D%20Gaussian%20Splatting%20for%20Novel%20View%20Synthesis%20in%20Presence%20of%0A%20%20Distractors%0AAuthor%3A%20Paul%20Ungermann%20and%20Armin%20Ettenhofer%20and%20Matthias%20Nie%C3%9Fner%20and%20Barbara%20Roessle%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20shown%20impressive%20novel%20view%20synthesis%20results%3B%0Anonetheless%2C%20it%20is%20vulnerable%20to%20dynamic%20objects%20polluting%20the%20input%20data%20of%20an%0Aotherwise%20static%20scene%2C%20so%20called%20distractors.%20Distractors%20have%20severe%20impact%0Aon%20the%20rendering%20quality%20as%20they%20get%20represented%20as%20view-dependent%20effects%20or%0Aresult%20in%20floating%20artifacts.%20Our%20goal%20is%20to%20identify%20and%20ignore%20such%0Adistractors%20during%20the%203D%20Gaussian%20optimization%20to%20obtain%20a%20clean%0Areconstruction.%20To%20this%20end%2C%20we%20take%20a%20self-supervised%20approach%20that%20looks%20at%0Athe%20image%20residuals%20during%20the%20optimization%20to%20determine%20areas%20that%20have%20likely%0Abeen%20falsified%20by%20a%20distractor.%20In%20addition%2C%20we%20leverage%20a%20pretrained%0Asegmentation%20network%20to%20provide%20object%20awareness%2C%20enabling%20more%20accurate%0Aexclusion%20of%20distractors.%20This%20way%2C%20we%20obtain%20segmentation%20masks%20of%20distractors%0Ato%20effectively%20ignore%20them%20in%20the%20loss%20formulation.%20We%20demonstrate%20that%20our%0Aapproach%20is%20robust%20to%20various%20distractors%20and%20strongly%20improves%20rendering%0Aquality%20on%20distractor-polluted%20scenes%2C%20improving%20PSNR%20by%201.86dB%20compared%20to%203D%0AGaussian%20Splatting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%25203D%2520Gaussian%2520Splatting%2520for%2520Novel%2520View%2520Synthesis%2520in%2520Presence%2520of%250A%2520%2520Distractors%26entry.906535625%3DPaul%2520Ungermann%2520and%2520Armin%2520Ettenhofer%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Barbara%2520Roessle%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520shown%2520impressive%2520novel%2520view%2520synthesis%2520results%253B%250Anonetheless%252C%2520it%2520is%2520vulnerable%2520to%2520dynamic%2520objects%2520polluting%2520the%2520input%2520data%2520of%2520an%250Aotherwise%2520static%2520scene%252C%2520so%2520called%2520distractors.%2520Distractors%2520have%2520severe%2520impact%250Aon%2520the%2520rendering%2520quality%2520as%2520they%2520get%2520represented%2520as%2520view-dependent%2520effects%2520or%250Aresult%2520in%2520floating%2520artifacts.%2520Our%2520goal%2520is%2520to%2520identify%2520and%2520ignore%2520such%250Adistractors%2520during%2520the%25203D%2520Gaussian%2520optimization%2520to%2520obtain%2520a%2520clean%250Areconstruction.%2520To%2520this%2520end%252C%2520we%2520take%2520a%2520self-supervised%2520approach%2520that%2520looks%2520at%250Athe%2520image%2520residuals%2520during%2520the%2520optimization%2520to%2520determine%2520areas%2520that%2520have%2520likely%250Abeen%2520falsified%2520by%2520a%2520distractor.%2520In%2520addition%252C%2520we%2520leverage%2520a%2520pretrained%250Asegmentation%2520network%2520to%2520provide%2520object%2520awareness%252C%2520enabling%2520more%2520accurate%250Aexclusion%2520of%2520distractors.%2520This%2520way%252C%2520we%2520obtain%2520segmentation%2520masks%2520of%2520distractors%250Ato%2520effectively%2520ignore%2520them%2520in%2520the%2520loss%2520formulation.%2520We%2520demonstrate%2520that%2520our%250Aapproach%2520is%2520robust%2520to%2520various%2520distractors%2520and%2520strongly%2520improves%2520rendering%250Aquality%2520on%2520distractor-polluted%2520scenes%252C%2520improving%2520PSNR%2520by%25201.86dB%2520compared%2520to%25203D%250AGaussian%2520Splatting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%203D%20Gaussian%20Splatting%20for%20Novel%20View%20Synthesis%20in%20Presence%20of%0A%20%20Distractors&entry.906535625=Paul%20Ungermann%20and%20Armin%20Ettenhofer%20and%20Matthias%20Nie%C3%9Fner%20and%20Barbara%20Roessle&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20shown%20impressive%20novel%20view%20synthesis%20results%3B%0Anonetheless%2C%20it%20is%20vulnerable%20to%20dynamic%20objects%20polluting%20the%20input%20data%20of%20an%0Aotherwise%20static%20scene%2C%20so%20called%20distractors.%20Distractors%20have%20severe%20impact%0Aon%20the%20rendering%20quality%20as%20they%20get%20represented%20as%20view-dependent%20effects%20or%0Aresult%20in%20floating%20artifacts.%20Our%20goal%20is%20to%20identify%20and%20ignore%20such%0Adistractors%20during%20the%203D%20Gaussian%20optimization%20to%20obtain%20a%20clean%0Areconstruction.%20To%20this%20end%2C%20we%20take%20a%20self-supervised%20approach%20that%20looks%20at%0Athe%20image%20residuals%20during%20the%20optimization%20to%20determine%20areas%20that%20have%20likely%0Abeen%20falsified%20by%20a%20distractor.%20In%20addition%2C%20we%20leverage%20a%20pretrained%0Asegmentation%20network%20to%20provide%20object%20awareness%2C%20enabling%20more%20accurate%0Aexclusion%20of%20distractors.%20This%20way%2C%20we%20obtain%20segmentation%20masks%20of%20distractors%0Ato%20effectively%20ignore%20them%20in%20the%20loss%20formulation.%20We%20demonstrate%20that%20our%0Aapproach%20is%20robust%20to%20various%20distractors%20and%20strongly%20improves%20rendering%0Aquality%20on%20distractor-polluted%20scenes%2C%20improving%20PSNR%20by%201.86dB%20compared%20to%203D%0AGaussian%20Splatting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11697v1&entry.124074799=Read"},
{"title": "EmbodiedSAM: Online Segment Any 3D Thing in Real Time", "author": "Xiuwei Xu and Huangxing Chen and Linqing Zhao and Ziwei Wang and Jie Zhou and Jiwen Lu", "abstract": "  Embodied tasks require the agent to fully understand 3D scenes simultaneously\nwith its exploration, so an online, real-time, fine-grained and\nhighly-generalized 3D perception model is desperately needed. Since\nhigh-quality 3D data is limited, directly training such a model in 3D is almost\ninfeasible. Meanwhile, vision foundation models (VFM) has revolutionized the\nfield of 2D computer vision with superior performance, which makes the use of\nVFM to assist embodied 3D perception a promising direction. However, most\nexisting VFM-assisted 3D perception methods are either offline or too slow that\ncannot be applied in practical embodied tasks. In this paper, we aim to\nleverage Segment Anything Model (SAM) for real-time 3D instance segmentation in\nan online setting. This is a challenging problem since future frames are not\navailable in the input streaming RGB-D video, and an instance may be observed\nin several frames so object matching between frames is required. To address\nthese challenges, we first propose a geometric-aware query lifting module to\nrepresent the 2D masks generated by SAM by 3D-aware queries, which is then\niteratively refined by a dual-level query decoder. In this way, the 2D masks\nare transferred to fine-grained shapes on 3D point clouds. Benefit from the\nquery representation for 3D masks, we can compute the similarity matrix between\nthe 3D masks from different views by efficient matrix operation, which enables\nreal-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan\nshow our method achieves leading performance even compared with offline\nmethods. Our method also demonstrates great generalization ability in several\nzero-shot dataset transferring experiments and show great potential in\nopen-vocabulary and data-efficient setting. Code and demo are available at\nhttps://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for\ntraining and evaluation.\n", "link": "http://arxiv.org/abs/2408.11811v1", "date": "2024-08-21", "relevancy": 3.0626, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6306}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6083}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmbodiedSAM%3A%20Online%20Segment%20Any%203D%20Thing%20in%20Real%20Time&body=Title%3A%20EmbodiedSAM%3A%20Online%20Segment%20Any%203D%20Thing%20in%20Real%20Time%0AAuthor%3A%20Xiuwei%20Xu%20and%20Huangxing%20Chen%20and%20Linqing%20Zhao%20and%20Ziwei%20Wang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Embodied%20tasks%20require%20the%20agent%20to%20fully%20understand%203D%20scenes%20simultaneously%0Awith%20its%20exploration%2C%20so%20an%20online%2C%20real-time%2C%20fine-grained%20and%0Ahighly-generalized%203D%20perception%20model%20is%20desperately%20needed.%20Since%0Ahigh-quality%203D%20data%20is%20limited%2C%20directly%20training%20such%20a%20model%20in%203D%20is%20almost%0Ainfeasible.%20Meanwhile%2C%20vision%20foundation%20models%20%28VFM%29%20has%20revolutionized%20the%0Afield%20of%202D%20computer%20vision%20with%20superior%20performance%2C%20which%20makes%20the%20use%20of%0AVFM%20to%20assist%20embodied%203D%20perception%20a%20promising%20direction.%20However%2C%20most%0Aexisting%20VFM-assisted%203D%20perception%20methods%20are%20either%20offline%20or%20too%20slow%20that%0Acannot%20be%20applied%20in%20practical%20embodied%20tasks.%20In%20this%20paper%2C%20we%20aim%20to%0Aleverage%20Segment%20Anything%20Model%20%28SAM%29%20for%20real-time%203D%20instance%20segmentation%20in%0Aan%20online%20setting.%20This%20is%20a%20challenging%20problem%20since%20future%20frames%20are%20not%0Aavailable%20in%20the%20input%20streaming%20RGB-D%20video%2C%20and%20an%20instance%20may%20be%20observed%0Ain%20several%20frames%20so%20object%20matching%20between%20frames%20is%20required.%20To%20address%0Athese%20challenges%2C%20we%20first%20propose%20a%20geometric-aware%20query%20lifting%20module%20to%0Arepresent%20the%202D%20masks%20generated%20by%20SAM%20by%203D-aware%20queries%2C%20which%20is%20then%0Aiteratively%20refined%20by%20a%20dual-level%20query%20decoder.%20In%20this%20way%2C%20the%202D%20masks%0Aare%20transferred%20to%20fine-grained%20shapes%20on%203D%20point%20clouds.%20Benefit%20from%20the%0Aquery%20representation%20for%203D%20masks%2C%20we%20can%20compute%20the%20similarity%20matrix%20between%0Athe%203D%20masks%20from%20different%20views%20by%20efficient%20matrix%20operation%2C%20which%20enables%0Areal-time%20inference.%20Experiments%20on%20ScanNet%2C%20ScanNet200%2C%20SceneNN%20and%203RScan%0Ashow%20our%20method%20achieves%20leading%20performance%20even%20compared%20with%20offline%0Amethods.%20Our%20method%20also%20demonstrates%20great%20generalization%20ability%20in%20several%0Azero-shot%20dataset%20transferring%20experiments%20and%20show%20great%20potential%20in%0Aopen-vocabulary%20and%20data-efficient%20setting.%20Code%20and%20demo%20are%20available%20at%0Ahttps%3A//xuxw98.github.io/ESAM/%2C%20with%20only%20one%20RTX%203090%20GPU%20required%20for%0Atraining%20and%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodiedSAM%253A%2520Online%2520Segment%2520Any%25203D%2520Thing%2520in%2520Real%2520Time%26entry.906535625%3DXiuwei%2520Xu%2520and%2520Huangxing%2520Chen%2520and%2520Linqing%2520Zhao%2520and%2520Ziwei%2520Wang%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Embodied%2520tasks%2520require%2520the%2520agent%2520to%2520fully%2520understand%25203D%2520scenes%2520simultaneously%250Awith%2520its%2520exploration%252C%2520so%2520an%2520online%252C%2520real-time%252C%2520fine-grained%2520and%250Ahighly-generalized%25203D%2520perception%2520model%2520is%2520desperately%2520needed.%2520Since%250Ahigh-quality%25203D%2520data%2520is%2520limited%252C%2520directly%2520training%2520such%2520a%2520model%2520in%25203D%2520is%2520almost%250Ainfeasible.%2520Meanwhile%252C%2520vision%2520foundation%2520models%2520%2528VFM%2529%2520has%2520revolutionized%2520the%250Afield%2520of%25202D%2520computer%2520vision%2520with%2520superior%2520performance%252C%2520which%2520makes%2520the%2520use%2520of%250AVFM%2520to%2520assist%2520embodied%25203D%2520perception%2520a%2520promising%2520direction.%2520However%252C%2520most%250Aexisting%2520VFM-assisted%25203D%2520perception%2520methods%2520are%2520either%2520offline%2520or%2520too%2520slow%2520that%250Acannot%2520be%2520applied%2520in%2520practical%2520embodied%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%250Aleverage%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520for%2520real-time%25203D%2520instance%2520segmentation%2520in%250Aan%2520online%2520setting.%2520This%2520is%2520a%2520challenging%2520problem%2520since%2520future%2520frames%2520are%2520not%250Aavailable%2520in%2520the%2520input%2520streaming%2520RGB-D%2520video%252C%2520and%2520an%2520instance%2520may%2520be%2520observed%250Ain%2520several%2520frames%2520so%2520object%2520matching%2520between%2520frames%2520is%2520required.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520first%2520propose%2520a%2520geometric-aware%2520query%2520lifting%2520module%2520to%250Arepresent%2520the%25202D%2520masks%2520generated%2520by%2520SAM%2520by%25203D-aware%2520queries%252C%2520which%2520is%2520then%250Aiteratively%2520refined%2520by%2520a%2520dual-level%2520query%2520decoder.%2520In%2520this%2520way%252C%2520the%25202D%2520masks%250Aare%2520transferred%2520to%2520fine-grained%2520shapes%2520on%25203D%2520point%2520clouds.%2520Benefit%2520from%2520the%250Aquery%2520representation%2520for%25203D%2520masks%252C%2520we%2520can%2520compute%2520the%2520similarity%2520matrix%2520between%250Athe%25203D%2520masks%2520from%2520different%2520views%2520by%2520efficient%2520matrix%2520operation%252C%2520which%2520enables%250Areal-time%2520inference.%2520Experiments%2520on%2520ScanNet%252C%2520ScanNet200%252C%2520SceneNN%2520and%25203RScan%250Ashow%2520our%2520method%2520achieves%2520leading%2520performance%2520even%2520compared%2520with%2520offline%250Amethods.%2520Our%2520method%2520also%2520demonstrates%2520great%2520generalization%2520ability%2520in%2520several%250Azero-shot%2520dataset%2520transferring%2520experiments%2520and%2520show%2520great%2520potential%2520in%250Aopen-vocabulary%2520and%2520data-efficient%2520setting.%2520Code%2520and%2520demo%2520are%2520available%2520at%250Ahttps%253A//xuxw98.github.io/ESAM/%252C%2520with%2520only%2520one%2520RTX%25203090%2520GPU%2520required%2520for%250Atraining%2520and%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmbodiedSAM%3A%20Online%20Segment%20Any%203D%20Thing%20in%20Real%20Time&entry.906535625=Xiuwei%20Xu%20and%20Huangxing%20Chen%20and%20Linqing%20Zhao%20and%20Ziwei%20Wang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Embodied%20tasks%20require%20the%20agent%20to%20fully%20understand%203D%20scenes%20simultaneously%0Awith%20its%20exploration%2C%20so%20an%20online%2C%20real-time%2C%20fine-grained%20and%0Ahighly-generalized%203D%20perception%20model%20is%20desperately%20needed.%20Since%0Ahigh-quality%203D%20data%20is%20limited%2C%20directly%20training%20such%20a%20model%20in%203D%20is%20almost%0Ainfeasible.%20Meanwhile%2C%20vision%20foundation%20models%20%28VFM%29%20has%20revolutionized%20the%0Afield%20of%202D%20computer%20vision%20with%20superior%20performance%2C%20which%20makes%20the%20use%20of%0AVFM%20to%20assist%20embodied%203D%20perception%20a%20promising%20direction.%20However%2C%20most%0Aexisting%20VFM-assisted%203D%20perception%20methods%20are%20either%20offline%20or%20too%20slow%20that%0Acannot%20be%20applied%20in%20practical%20embodied%20tasks.%20In%20this%20paper%2C%20we%20aim%20to%0Aleverage%20Segment%20Anything%20Model%20%28SAM%29%20for%20real-time%203D%20instance%20segmentation%20in%0Aan%20online%20setting.%20This%20is%20a%20challenging%20problem%20since%20future%20frames%20are%20not%0Aavailable%20in%20the%20input%20streaming%20RGB-D%20video%2C%20and%20an%20instance%20may%20be%20observed%0Ain%20several%20frames%20so%20object%20matching%20between%20frames%20is%20required.%20To%20address%0Athese%20challenges%2C%20we%20first%20propose%20a%20geometric-aware%20query%20lifting%20module%20to%0Arepresent%20the%202D%20masks%20generated%20by%20SAM%20by%203D-aware%20queries%2C%20which%20is%20then%0Aiteratively%20refined%20by%20a%20dual-level%20query%20decoder.%20In%20this%20way%2C%20the%202D%20masks%0Aare%20transferred%20to%20fine-grained%20shapes%20on%203D%20point%20clouds.%20Benefit%20from%20the%0Aquery%20representation%20for%203D%20masks%2C%20we%20can%20compute%20the%20similarity%20matrix%20between%0Athe%203D%20masks%20from%20different%20views%20by%20efficient%20matrix%20operation%2C%20which%20enables%0Areal-time%20inference.%20Experiments%20on%20ScanNet%2C%20ScanNet200%2C%20SceneNN%20and%203RScan%0Ashow%20our%20method%20achieves%20leading%20performance%20even%20compared%20with%20offline%0Amethods.%20Our%20method%20also%20demonstrates%20great%20generalization%20ability%20in%20several%0Azero-shot%20dataset%20transferring%20experiments%20and%20show%20great%20potential%20in%0Aopen-vocabulary%20and%20data-efficient%20setting.%20Code%20and%20demo%20are%20available%20at%0Ahttps%3A//xuxw98.github.io/ESAM/%2C%20with%20only%20one%20RTX%203090%20GPU%20required%20for%0Atraining%20and%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11811v1&entry.124074799=Read"},
{"title": "DeRainGS: Gaussian Splatting for Enhanced Scene Reconstruction in Rainy", "author": "Shuhong Liu and Xiang Chen and Hongming Chen and Quanfeng Xu and Mingrui Li", "abstract": "  Reconstruction under adverse rainy conditions poses significant challenges\ndue to reduced visibility and the distortion of visual perception. These\nconditions can severely impair the quality of geometric maps, which is\nessential for applications ranging from autonomous planning to environmental\nmonitoring. In response to these challenges, this study introduces the novel\ntask of 3D Reconstruction in Rainy Environments (3DRRE), specifically designed\nto address the complexities of reconstructing 3D scenes under rainy conditions.\nTo benchmark this task, we construct the HydroViews dataset that comprises a\ndiverse collection of both synthesized and real-world scene images\ncharacterized by various intensities of rain streaks and raindrops.\nFurthermore, we propose DeRainGS, the first 3DGS method tailored for\nreconstruction in adverse rainy environments. Extensive experiments across a\nwide range of rain scenarios demonstrate that our method delivers\nstate-of-the-art performance, remarkably outperforming existing occlusion-free\nmethods by a large margin.\n", "link": "http://arxiv.org/abs/2408.11540v1", "date": "2024-08-21", "relevancy": 3.0428, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6753}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5985}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeRainGS%3A%20Gaussian%20Splatting%20for%20Enhanced%20Scene%20Reconstruction%20in%20Rainy&body=Title%3A%20DeRainGS%3A%20Gaussian%20Splatting%20for%20Enhanced%20Scene%20Reconstruction%20in%20Rainy%0AAuthor%3A%20Shuhong%20Liu%20and%20Xiang%20Chen%20and%20Hongming%20Chen%20and%20Quanfeng%20Xu%20and%20Mingrui%20Li%0AAbstract%3A%20%20%20Reconstruction%20under%20adverse%20rainy%20conditions%20poses%20significant%20challenges%0Adue%20to%20reduced%20visibility%20and%20the%20distortion%20of%20visual%20perception.%20These%0Aconditions%20can%20severely%20impair%20the%20quality%20of%20geometric%20maps%2C%20which%20is%0Aessential%20for%20applications%20ranging%20from%20autonomous%20planning%20to%20environmental%0Amonitoring.%20In%20response%20to%20these%20challenges%2C%20this%20study%20introduces%20the%20novel%0Atask%20of%203D%20Reconstruction%20in%20Rainy%20Environments%20%283DRRE%29%2C%20specifically%20designed%0Ato%20address%20the%20complexities%20of%20reconstructing%203D%20scenes%20under%20rainy%20conditions.%0ATo%20benchmark%20this%20task%2C%20we%20construct%20the%20HydroViews%20dataset%20that%20comprises%20a%0Adiverse%20collection%20of%20both%20synthesized%20and%20real-world%20scene%20images%0Acharacterized%20by%20various%20intensities%20of%20rain%20streaks%20and%20raindrops.%0AFurthermore%2C%20we%20propose%20DeRainGS%2C%20the%20first%203DGS%20method%20tailored%20for%0Areconstruction%20in%20adverse%20rainy%20environments.%20Extensive%20experiments%20across%20a%0Awide%20range%20of%20rain%20scenarios%20demonstrate%20that%20our%20method%20delivers%0Astate-of-the-art%20performance%2C%20remarkably%20outperforming%20existing%20occlusion-free%0Amethods%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeRainGS%253A%2520Gaussian%2520Splatting%2520for%2520Enhanced%2520Scene%2520Reconstruction%2520in%2520Rainy%26entry.906535625%3DShuhong%2520Liu%2520and%2520Xiang%2520Chen%2520and%2520Hongming%2520Chen%2520and%2520Quanfeng%2520Xu%2520and%2520Mingrui%2520Li%26entry.1292438233%3D%2520%2520Reconstruction%2520under%2520adverse%2520rainy%2520conditions%2520poses%2520significant%2520challenges%250Adue%2520to%2520reduced%2520visibility%2520and%2520the%2520distortion%2520of%2520visual%2520perception.%2520These%250Aconditions%2520can%2520severely%2520impair%2520the%2520quality%2520of%2520geometric%2520maps%252C%2520which%2520is%250Aessential%2520for%2520applications%2520ranging%2520from%2520autonomous%2520planning%2520to%2520environmental%250Amonitoring.%2520In%2520response%2520to%2520these%2520challenges%252C%2520this%2520study%2520introduces%2520the%2520novel%250Atask%2520of%25203D%2520Reconstruction%2520in%2520Rainy%2520Environments%2520%25283DRRE%2529%252C%2520specifically%2520designed%250Ato%2520address%2520the%2520complexities%2520of%2520reconstructing%25203D%2520scenes%2520under%2520rainy%2520conditions.%250ATo%2520benchmark%2520this%2520task%252C%2520we%2520construct%2520the%2520HydroViews%2520dataset%2520that%2520comprises%2520a%250Adiverse%2520collection%2520of%2520both%2520synthesized%2520and%2520real-world%2520scene%2520images%250Acharacterized%2520by%2520various%2520intensities%2520of%2520rain%2520streaks%2520and%2520raindrops.%250AFurthermore%252C%2520we%2520propose%2520DeRainGS%252C%2520the%2520first%25203DGS%2520method%2520tailored%2520for%250Areconstruction%2520in%2520adverse%2520rainy%2520environments.%2520Extensive%2520experiments%2520across%2520a%250Awide%2520range%2520of%2520rain%2520scenarios%2520demonstrate%2520that%2520our%2520method%2520delivers%250Astate-of-the-art%2520performance%252C%2520remarkably%2520outperforming%2520existing%2520occlusion-free%250Amethods%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeRainGS%3A%20Gaussian%20Splatting%20for%20Enhanced%20Scene%20Reconstruction%20in%20Rainy&entry.906535625=Shuhong%20Liu%20and%20Xiang%20Chen%20and%20Hongming%20Chen%20and%20Quanfeng%20Xu%20and%20Mingrui%20Li&entry.1292438233=%20%20Reconstruction%20under%20adverse%20rainy%20conditions%20poses%20significant%20challenges%0Adue%20to%20reduced%20visibility%20and%20the%20distortion%20of%20visual%20perception.%20These%0Aconditions%20can%20severely%20impair%20the%20quality%20of%20geometric%20maps%2C%20which%20is%0Aessential%20for%20applications%20ranging%20from%20autonomous%20planning%20to%20environmental%0Amonitoring.%20In%20response%20to%20these%20challenges%2C%20this%20study%20introduces%20the%20novel%0Atask%20of%203D%20Reconstruction%20in%20Rainy%20Environments%20%283DRRE%29%2C%20specifically%20designed%0Ato%20address%20the%20complexities%20of%20reconstructing%203D%20scenes%20under%20rainy%20conditions.%0ATo%20benchmark%20this%20task%2C%20we%20construct%20the%20HydroViews%20dataset%20that%20comprises%20a%0Adiverse%20collection%20of%20both%20synthesized%20and%20real-world%20scene%20images%0Acharacterized%20by%20various%20intensities%20of%20rain%20streaks%20and%20raindrops.%0AFurthermore%2C%20we%20propose%20DeRainGS%2C%20the%20first%203DGS%20method%20tailored%20for%0Areconstruction%20in%20adverse%20rainy%20environments.%20Extensive%20experiments%20across%20a%0Awide%20range%20of%20rain%20scenarios%20demonstrate%20that%20our%20method%20delivers%0Astate-of-the-art%20performance%2C%20remarkably%20outperforming%20existing%20occlusion-free%0Amethods%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11540v1&entry.124074799=Read"},
{"title": "The NeRFect Match: Exploring NeRF Features for Visual Localization", "author": "Qunjie Zhou and Maxim Maximov and Or Litany and Laura Leal-Taix\u00e9", "abstract": "  In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene\nrepresentation for visual localization. Recently, NeRF has been employed to\nenhance pose regression and scene coordinate regression models by augmenting\nthe training database, providing auxiliary supervision through rendered images,\nor serving as an iterative refinement module. We extend its recognized\nadvantages -- its ability to provide a compact scene representation with\nrealistic appearances and accurate geometry -- by exploring the potential of\nNeRF's internal features in establishing precise 2D-3D matches for\nlocalization. To this end, we conduct a comprehensive examination of NeRF's\nimplicit knowledge, acquired through view synthesis, for matching under various\nconditions. This includes exploring different matching network architectures,\nextracting encoder features at multiple layers, and varying training\nconfigurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D\nmatching function that capitalizes on the internal knowledge of NeRF learned\nvia view synthesis. Our evaluation of NeRFMatch on standard localization\nbenchmarks, within a structure-based pipeline, sets a new state-of-the-art for\nlocalization performance on Cambridge Landmarks.\n", "link": "http://arxiv.org/abs/2403.09577v2", "date": "2024-08-21", "relevancy": 2.9681, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6634}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5779}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization&body=Title%3A%20The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization%0AAuthor%3A%20Qunjie%20Zhou%20and%20Maxim%20Maximov%20and%20Or%20Litany%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20the%20use%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20as%20a%20scene%0Arepresentation%20for%20visual%20localization.%20Recently%2C%20NeRF%20has%20been%20employed%20to%0Aenhance%20pose%20regression%20and%20scene%20coordinate%20regression%20models%20by%20augmenting%0Athe%20training%20database%2C%20providing%20auxiliary%20supervision%20through%20rendered%20images%2C%0Aor%20serving%20as%20an%20iterative%20refinement%20module.%20We%20extend%20its%20recognized%0Aadvantages%20--%20its%20ability%20to%20provide%20a%20compact%20scene%20representation%20with%0Arealistic%20appearances%20and%20accurate%20geometry%20--%20by%20exploring%20the%20potential%20of%0ANeRF%27s%20internal%20features%20in%20establishing%20precise%202D-3D%20matches%20for%0Alocalization.%20To%20this%20end%2C%20we%20conduct%20a%20comprehensive%20examination%20of%20NeRF%27s%0Aimplicit%20knowledge%2C%20acquired%20through%20view%20synthesis%2C%20for%20matching%20under%20various%0Aconditions.%20This%20includes%20exploring%20different%20matching%20network%20architectures%2C%0Aextracting%20encoder%20features%20at%20multiple%20layers%2C%20and%20varying%20training%0Aconfigurations.%20Significantly%2C%20we%20introduce%20NeRFMatch%2C%20an%20advanced%202D-3D%0Amatching%20function%20that%20capitalizes%20on%20the%20internal%20knowledge%20of%20NeRF%20learned%0Avia%20view%20synthesis.%20Our%20evaluation%20of%20NeRFMatch%20on%20standard%20localization%0Abenchmarks%2C%20within%20a%20structure-based%20pipeline%2C%20sets%20a%20new%20state-of-the-art%20for%0Alocalization%20performance%20on%20Cambridge%20Landmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520NeRFect%2520Match%253A%2520Exploring%2520NeRF%2520Features%2520for%2520Visual%2520Localization%26entry.906535625%3DQunjie%2520Zhou%2520and%2520Maxim%2520Maximov%2520and%2520Or%2520Litany%2520and%2520Laura%2520Leal-Taix%25C3%25A9%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520use%2520of%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520as%2520a%2520scene%250Arepresentation%2520for%2520visual%2520localization.%2520Recently%252C%2520NeRF%2520has%2520been%2520employed%2520to%250Aenhance%2520pose%2520regression%2520and%2520scene%2520coordinate%2520regression%2520models%2520by%2520augmenting%250Athe%2520training%2520database%252C%2520providing%2520auxiliary%2520supervision%2520through%2520rendered%2520images%252C%250Aor%2520serving%2520as%2520an%2520iterative%2520refinement%2520module.%2520We%2520extend%2520its%2520recognized%250Aadvantages%2520--%2520its%2520ability%2520to%2520provide%2520a%2520compact%2520scene%2520representation%2520with%250Arealistic%2520appearances%2520and%2520accurate%2520geometry%2520--%2520by%2520exploring%2520the%2520potential%2520of%250ANeRF%2527s%2520internal%2520features%2520in%2520establishing%2520precise%25202D-3D%2520matches%2520for%250Alocalization.%2520To%2520this%2520end%252C%2520we%2520conduct%2520a%2520comprehensive%2520examination%2520of%2520NeRF%2527s%250Aimplicit%2520knowledge%252C%2520acquired%2520through%2520view%2520synthesis%252C%2520for%2520matching%2520under%2520various%250Aconditions.%2520This%2520includes%2520exploring%2520different%2520matching%2520network%2520architectures%252C%250Aextracting%2520encoder%2520features%2520at%2520multiple%2520layers%252C%2520and%2520varying%2520training%250Aconfigurations.%2520Significantly%252C%2520we%2520introduce%2520NeRFMatch%252C%2520an%2520advanced%25202D-3D%250Amatching%2520function%2520that%2520capitalizes%2520on%2520the%2520internal%2520knowledge%2520of%2520NeRF%2520learned%250Avia%2520view%2520synthesis.%2520Our%2520evaluation%2520of%2520NeRFMatch%2520on%2520standard%2520localization%250Abenchmarks%252C%2520within%2520a%2520structure-based%2520pipeline%252C%2520sets%2520a%2520new%2520state-of-the-art%2520for%250Alocalization%2520performance%2520on%2520Cambridge%2520Landmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization&entry.906535625=Qunjie%20Zhou%20and%20Maxim%20Maximov%20and%20Or%20Litany%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20the%20use%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20as%20a%20scene%0Arepresentation%20for%20visual%20localization.%20Recently%2C%20NeRF%20has%20been%20employed%20to%0Aenhance%20pose%20regression%20and%20scene%20coordinate%20regression%20models%20by%20augmenting%0Athe%20training%20database%2C%20providing%20auxiliary%20supervision%20through%20rendered%20images%2C%0Aor%20serving%20as%20an%20iterative%20refinement%20module.%20We%20extend%20its%20recognized%0Aadvantages%20--%20its%20ability%20to%20provide%20a%20compact%20scene%20representation%20with%0Arealistic%20appearances%20and%20accurate%20geometry%20--%20by%20exploring%20the%20potential%20of%0ANeRF%27s%20internal%20features%20in%20establishing%20precise%202D-3D%20matches%20for%0Alocalization.%20To%20this%20end%2C%20we%20conduct%20a%20comprehensive%20examination%20of%20NeRF%27s%0Aimplicit%20knowledge%2C%20acquired%20through%20view%20synthesis%2C%20for%20matching%20under%20various%0Aconditions.%20This%20includes%20exploring%20different%20matching%20network%20architectures%2C%0Aextracting%20encoder%20features%20at%20multiple%20layers%2C%20and%20varying%20training%0Aconfigurations.%20Significantly%2C%20we%20introduce%20NeRFMatch%2C%20an%20advanced%202D-3D%0Amatching%20function%20that%20capitalizes%20on%20the%20internal%20knowledge%20of%20NeRF%20learned%0Avia%20view%20synthesis.%20Our%20evaluation%20of%20NeRFMatch%20on%20standard%20localization%0Abenchmarks%2C%20within%20a%20structure-based%20pipeline%2C%20sets%20a%20new%20state-of-the-art%20for%0Alocalization%20performance%20on%20Cambridge%20Landmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09577v2&entry.124074799=Read"},
{"title": "EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face with\n  Mesh Attention", "author": "Yihong Lin and Liang Peng and Jianqiao Hu and Xiandong Li and Wenxiong Kang and Songju Lei and Xianjia Wu and Huang Xu", "abstract": "  The creation of increasingly vivid 3D virtual digital humans has become a hot\ntopic in recent years. Currently, most speech-driven work focuses on training\nmodels to learn the relationship between phonemes and visemes to achieve more\nrealistic lips. However, they fail to capture the correlations between emotions\nand facial expressions effectively. To solve this problem, we propose a new\nmodel, termed EmoFace. EmoFace employs a novel Mesh Attention mechanism, which\nhelps to learn potential feature dependencies between mesh vertices in time and\nspace. We also adopt, for the first time to our knowledge, an effective\nself-growing training scheme that combines teacher-forcing and scheduled\nsampling in a 3D face animation task. Additionally, since EmoFace is an\nautoregressive model, there is no requirement that the first frame of the\ntraining data must be a silent frame, which greatly reduces the data\nlimitations and contributes to solve the current dilemma of insufficient\ndatasets. Comprehensive quantitative and qualitative evaluations on our\nproposed high-quality reconstructed 3D emotional facial animation dataset,\n3D-RAVDESS ($5.0343\\times 10^{-5}$mm for LVE and $1.0196\\times 10^{-5}$mm for\nEVE), and publicly available dataset VOCASET ($2.8669\\times 10^{-5}$mm for LVE\nand $0.4664\\times 10^{-5}$mm for EVE), demonstrate that our algorithm achieves\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2408.11518v1", "date": "2024-08-21", "relevancy": 2.9453, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6023}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6023}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoFace%3A%20Emotion-Content%20Disentangled%20Speech-Driven%203D%20Talking%20Face%20with%0A%20%20Mesh%20Attention&body=Title%3A%20EmoFace%3A%20Emotion-Content%20Disentangled%20Speech-Driven%203D%20Talking%20Face%20with%0A%20%20Mesh%20Attention%0AAuthor%3A%20Yihong%20Lin%20and%20Liang%20Peng%20and%20Jianqiao%20Hu%20and%20Xiandong%20Li%20and%20Wenxiong%20Kang%20and%20Songju%20Lei%20and%20Xianjia%20Wu%20and%20Huang%20Xu%0AAbstract%3A%20%20%20The%20creation%20of%20increasingly%20vivid%203D%20virtual%20digital%20humans%20has%20become%20a%20hot%0Atopic%20in%20recent%20years.%20Currently%2C%20most%20speech-driven%20work%20focuses%20on%20training%0Amodels%20to%20learn%20the%20relationship%20between%20phonemes%20and%20visemes%20to%20achieve%20more%0Arealistic%20lips.%20However%2C%20they%20fail%20to%20capture%20the%20correlations%20between%20emotions%0Aand%20facial%20expressions%20effectively.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20new%0Amodel%2C%20termed%20EmoFace.%20EmoFace%20employs%20a%20novel%20Mesh%20Attention%20mechanism%2C%20which%0Ahelps%20to%20learn%20potential%20feature%20dependencies%20between%20mesh%20vertices%20in%20time%20and%0Aspace.%20We%20also%20adopt%2C%20for%20the%20first%20time%20to%20our%20knowledge%2C%20an%20effective%0Aself-growing%20training%20scheme%20that%20combines%20teacher-forcing%20and%20scheduled%0Asampling%20in%20a%203D%20face%20animation%20task.%20Additionally%2C%20since%20EmoFace%20is%20an%0Aautoregressive%20model%2C%20there%20is%20no%20requirement%20that%20the%20first%20frame%20of%20the%0Atraining%20data%20must%20be%20a%20silent%20frame%2C%20which%20greatly%20reduces%20the%20data%0Alimitations%20and%20contributes%20to%20solve%20the%20current%20dilemma%20of%20insufficient%0Adatasets.%20Comprehensive%20quantitative%20and%20qualitative%20evaluations%20on%20our%0Aproposed%20high-quality%20reconstructed%203D%20emotional%20facial%20animation%20dataset%2C%0A3D-RAVDESS%20%28%245.0343%5Ctimes%2010%5E%7B-5%7D%24mm%20for%20LVE%20and%20%241.0196%5Ctimes%2010%5E%7B-5%7D%24mm%20for%0AEVE%29%2C%20and%20publicly%20available%20dataset%20VOCASET%20%28%242.8669%5Ctimes%2010%5E%7B-5%7D%24mm%20for%20LVE%0Aand%20%240.4664%5Ctimes%2010%5E%7B-5%7D%24mm%20for%20EVE%29%2C%20demonstrate%20that%20our%20algorithm%20achieves%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoFace%253A%2520Emotion-Content%2520Disentangled%2520Speech-Driven%25203D%2520Talking%2520Face%2520with%250A%2520%2520Mesh%2520Attention%26entry.906535625%3DYihong%2520Lin%2520and%2520Liang%2520Peng%2520and%2520Jianqiao%2520Hu%2520and%2520Xiandong%2520Li%2520and%2520Wenxiong%2520Kang%2520and%2520Songju%2520Lei%2520and%2520Xianjia%2520Wu%2520and%2520Huang%2520Xu%26entry.1292438233%3D%2520%2520The%2520creation%2520of%2520increasingly%2520vivid%25203D%2520virtual%2520digital%2520humans%2520has%2520become%2520a%2520hot%250Atopic%2520in%2520recent%2520years.%2520Currently%252C%2520most%2520speech-driven%2520work%2520focuses%2520on%2520training%250Amodels%2520to%2520learn%2520the%2520relationship%2520between%2520phonemes%2520and%2520visemes%2520to%2520achieve%2520more%250Arealistic%2520lips.%2520However%252C%2520they%2520fail%2520to%2520capture%2520the%2520correlations%2520between%2520emotions%250Aand%2520facial%2520expressions%2520effectively.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520a%2520new%250Amodel%252C%2520termed%2520EmoFace.%2520EmoFace%2520employs%2520a%2520novel%2520Mesh%2520Attention%2520mechanism%252C%2520which%250Ahelps%2520to%2520learn%2520potential%2520feature%2520dependencies%2520between%2520mesh%2520vertices%2520in%2520time%2520and%250Aspace.%2520We%2520also%2520adopt%252C%2520for%2520the%2520first%2520time%2520to%2520our%2520knowledge%252C%2520an%2520effective%250Aself-growing%2520training%2520scheme%2520that%2520combines%2520teacher-forcing%2520and%2520scheduled%250Asampling%2520in%2520a%25203D%2520face%2520animation%2520task.%2520Additionally%252C%2520since%2520EmoFace%2520is%2520an%250Aautoregressive%2520model%252C%2520there%2520is%2520no%2520requirement%2520that%2520the%2520first%2520frame%2520of%2520the%250Atraining%2520data%2520must%2520be%2520a%2520silent%2520frame%252C%2520which%2520greatly%2520reduces%2520the%2520data%250Alimitations%2520and%2520contributes%2520to%2520solve%2520the%2520current%2520dilemma%2520of%2520insufficient%250Adatasets.%2520Comprehensive%2520quantitative%2520and%2520qualitative%2520evaluations%2520on%2520our%250Aproposed%2520high-quality%2520reconstructed%25203D%2520emotional%2520facial%2520animation%2520dataset%252C%250A3D-RAVDESS%2520%2528%25245.0343%255Ctimes%252010%255E%257B-5%257D%2524mm%2520for%2520LVE%2520and%2520%25241.0196%255Ctimes%252010%255E%257B-5%257D%2524mm%2520for%250AEVE%2529%252C%2520and%2520publicly%2520available%2520dataset%2520VOCASET%2520%2528%25242.8669%255Ctimes%252010%255E%257B-5%257D%2524mm%2520for%2520LVE%250Aand%2520%25240.4664%255Ctimes%252010%255E%257B-5%257D%2524mm%2520for%2520EVE%2529%252C%2520demonstrate%2520that%2520our%2520algorithm%2520achieves%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoFace%3A%20Emotion-Content%20Disentangled%20Speech-Driven%203D%20Talking%20Face%20with%0A%20%20Mesh%20Attention&entry.906535625=Yihong%20Lin%20and%20Liang%20Peng%20and%20Jianqiao%20Hu%20and%20Xiandong%20Li%20and%20Wenxiong%20Kang%20and%20Songju%20Lei%20and%20Xianjia%20Wu%20and%20Huang%20Xu&entry.1292438233=%20%20The%20creation%20of%20increasingly%20vivid%203D%20virtual%20digital%20humans%20has%20become%20a%20hot%0Atopic%20in%20recent%20years.%20Currently%2C%20most%20speech-driven%20work%20focuses%20on%20training%0Amodels%20to%20learn%20the%20relationship%20between%20phonemes%20and%20visemes%20to%20achieve%20more%0Arealistic%20lips.%20However%2C%20they%20fail%20to%20capture%20the%20correlations%20between%20emotions%0Aand%20facial%20expressions%20effectively.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20new%0Amodel%2C%20termed%20EmoFace.%20EmoFace%20employs%20a%20novel%20Mesh%20Attention%20mechanism%2C%20which%0Ahelps%20to%20learn%20potential%20feature%20dependencies%20between%20mesh%20vertices%20in%20time%20and%0Aspace.%20We%20also%20adopt%2C%20for%20the%20first%20time%20to%20our%20knowledge%2C%20an%20effective%0Aself-growing%20training%20scheme%20that%20combines%20teacher-forcing%20and%20scheduled%0Asampling%20in%20a%203D%20face%20animation%20task.%20Additionally%2C%20since%20EmoFace%20is%20an%0Aautoregressive%20model%2C%20there%20is%20no%20requirement%20that%20the%20first%20frame%20of%20the%0Atraining%20data%20must%20be%20a%20silent%20frame%2C%20which%20greatly%20reduces%20the%20data%0Alimitations%20and%20contributes%20to%20solve%20the%20current%20dilemma%20of%20insufficient%0Adatasets.%20Comprehensive%20quantitative%20and%20qualitative%20evaluations%20on%20our%0Aproposed%20high-quality%20reconstructed%203D%20emotional%20facial%20animation%20dataset%2C%0A3D-RAVDESS%20%28%245.0343%5Ctimes%2010%5E%7B-5%7D%24mm%20for%20LVE%20and%20%241.0196%5Ctimes%2010%5E%7B-5%7D%24mm%20for%0AEVE%29%2C%20and%20publicly%20available%20dataset%20VOCASET%20%28%242.8669%5Ctimes%2010%5E%7B-5%7D%24mm%20for%20LVE%0Aand%20%240.4664%5Ctimes%2010%5E%7B-5%7D%24mm%20for%20EVE%29%2C%20demonstrate%20that%20our%20algorithm%20achieves%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11518v1&entry.124074799=Read"},
{"title": "GSTran: Joint Geometric and Semantic Coherence for Point Cloud\n  Segmentation", "author": "Abiao Li and Chenlei Lv and Guofeng Mei and Yifan Zuo and Jian Zhang and Yuming Fang", "abstract": "  Learning meaningful local and global information remains a challenge in point\ncloud segmentation tasks. When utilizing local information, prior studies\nindiscriminately aggregates neighbor information from different classes to\nupdate query points, potentially compromising the distinctive feature of query\npoints. In parallel, inaccurate modeling of long-distance contextual\ndependencies when utilizing global information can also impact model\nperformance. To address these issues, we propose GSTran, a novel transformer\nnetwork tailored for the segmentation task. The proposed network mainly\nconsists of two principal components: a local geometric transformer and a\nglobal semantic transformer. In the local geometric transformer module, we\nexplicitly calculate the geometric disparity within the local region. This\nenables amplifying the affinity with geometrically similar neighbor points\nwhile suppressing the association with other neighbors. In the global semantic\ntransformer module, we design a multi-head voting strategy. This strategy\nevaluates semantic similarity across the entire spatial range, facilitating the\nprecise capture of contextual dependencies. Experiments on ShapeNetPart and\nS3DIS benchmarks demonstrate the effectiveness of the proposed method, showing\nits superiority over other algorithms. The code is available at\nhttps://github.com/LAB123-tech/GSTran.\n", "link": "http://arxiv.org/abs/2408.11558v1", "date": "2024-08-21", "relevancy": 2.8224, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5868}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5594}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSTran%3A%20Joint%20Geometric%20and%20Semantic%20Coherence%20for%20Point%20Cloud%0A%20%20Segmentation&body=Title%3A%20GSTran%3A%20Joint%20Geometric%20and%20Semantic%20Coherence%20for%20Point%20Cloud%0A%20%20Segmentation%0AAuthor%3A%20Abiao%20Li%20and%20Chenlei%20Lv%20and%20Guofeng%20Mei%20and%20Yifan%20Zuo%20and%20Jian%20Zhang%20and%20Yuming%20Fang%0AAbstract%3A%20%20%20Learning%20meaningful%20local%20and%20global%20information%20remains%20a%20challenge%20in%20point%0Acloud%20segmentation%20tasks.%20When%20utilizing%20local%20information%2C%20prior%20studies%0Aindiscriminately%20aggregates%20neighbor%20information%20from%20different%20classes%20to%0Aupdate%20query%20points%2C%20potentially%20compromising%20the%20distinctive%20feature%20of%20query%0Apoints.%20In%20parallel%2C%20inaccurate%20modeling%20of%20long-distance%20contextual%0Adependencies%20when%20utilizing%20global%20information%20can%20also%20impact%20model%0Aperformance.%20To%20address%20these%20issues%2C%20we%20propose%20GSTran%2C%20a%20novel%20transformer%0Anetwork%20tailored%20for%20the%20segmentation%20task.%20The%20proposed%20network%20mainly%0Aconsists%20of%20two%20principal%20components%3A%20a%20local%20geometric%20transformer%20and%20a%0Aglobal%20semantic%20transformer.%20In%20the%20local%20geometric%20transformer%20module%2C%20we%0Aexplicitly%20calculate%20the%20geometric%20disparity%20within%20the%20local%20region.%20This%0Aenables%20amplifying%20the%20affinity%20with%20geometrically%20similar%20neighbor%20points%0Awhile%20suppressing%20the%20association%20with%20other%20neighbors.%20In%20the%20global%20semantic%0Atransformer%20module%2C%20we%20design%20a%20multi-head%20voting%20strategy.%20This%20strategy%0Aevaluates%20semantic%20similarity%20across%20the%20entire%20spatial%20range%2C%20facilitating%20the%0Aprecise%20capture%20of%20contextual%20dependencies.%20Experiments%20on%20ShapeNetPart%20and%0AS3DIS%20benchmarks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20showing%0Aits%20superiority%20over%20other%20algorithms.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LAB123-tech/GSTran.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSTran%253A%2520Joint%2520Geometric%2520and%2520Semantic%2520Coherence%2520for%2520Point%2520Cloud%250A%2520%2520Segmentation%26entry.906535625%3DAbiao%2520Li%2520and%2520Chenlei%2520Lv%2520and%2520Guofeng%2520Mei%2520and%2520Yifan%2520Zuo%2520and%2520Jian%2520Zhang%2520and%2520Yuming%2520Fang%26entry.1292438233%3D%2520%2520Learning%2520meaningful%2520local%2520and%2520global%2520information%2520remains%2520a%2520challenge%2520in%2520point%250Acloud%2520segmentation%2520tasks.%2520When%2520utilizing%2520local%2520information%252C%2520prior%2520studies%250Aindiscriminately%2520aggregates%2520neighbor%2520information%2520from%2520different%2520classes%2520to%250Aupdate%2520query%2520points%252C%2520potentially%2520compromising%2520the%2520distinctive%2520feature%2520of%2520query%250Apoints.%2520In%2520parallel%252C%2520inaccurate%2520modeling%2520of%2520long-distance%2520contextual%250Adependencies%2520when%2520utilizing%2520global%2520information%2520can%2520also%2520impact%2520model%250Aperformance.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520GSTran%252C%2520a%2520novel%2520transformer%250Anetwork%2520tailored%2520for%2520the%2520segmentation%2520task.%2520The%2520proposed%2520network%2520mainly%250Aconsists%2520of%2520two%2520principal%2520components%253A%2520a%2520local%2520geometric%2520transformer%2520and%2520a%250Aglobal%2520semantic%2520transformer.%2520In%2520the%2520local%2520geometric%2520transformer%2520module%252C%2520we%250Aexplicitly%2520calculate%2520the%2520geometric%2520disparity%2520within%2520the%2520local%2520region.%2520This%250Aenables%2520amplifying%2520the%2520affinity%2520with%2520geometrically%2520similar%2520neighbor%2520points%250Awhile%2520suppressing%2520the%2520association%2520with%2520other%2520neighbors.%2520In%2520the%2520global%2520semantic%250Atransformer%2520module%252C%2520we%2520design%2520a%2520multi-head%2520voting%2520strategy.%2520This%2520strategy%250Aevaluates%2520semantic%2520similarity%2520across%2520the%2520entire%2520spatial%2520range%252C%2520facilitating%2520the%250Aprecise%2520capture%2520of%2520contextual%2520dependencies.%2520Experiments%2520on%2520ShapeNetPart%2520and%250AS3DIS%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520showing%250Aits%2520superiority%2520over%2520other%2520algorithms.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LAB123-tech/GSTran.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSTran%3A%20Joint%20Geometric%20and%20Semantic%20Coherence%20for%20Point%20Cloud%0A%20%20Segmentation&entry.906535625=Abiao%20Li%20and%20Chenlei%20Lv%20and%20Guofeng%20Mei%20and%20Yifan%20Zuo%20and%20Jian%20Zhang%20and%20Yuming%20Fang&entry.1292438233=%20%20Learning%20meaningful%20local%20and%20global%20information%20remains%20a%20challenge%20in%20point%0Acloud%20segmentation%20tasks.%20When%20utilizing%20local%20information%2C%20prior%20studies%0Aindiscriminately%20aggregates%20neighbor%20information%20from%20different%20classes%20to%0Aupdate%20query%20points%2C%20potentially%20compromising%20the%20distinctive%20feature%20of%20query%0Apoints.%20In%20parallel%2C%20inaccurate%20modeling%20of%20long-distance%20contextual%0Adependencies%20when%20utilizing%20global%20information%20can%20also%20impact%20model%0Aperformance.%20To%20address%20these%20issues%2C%20we%20propose%20GSTran%2C%20a%20novel%20transformer%0Anetwork%20tailored%20for%20the%20segmentation%20task.%20The%20proposed%20network%20mainly%0Aconsists%20of%20two%20principal%20components%3A%20a%20local%20geometric%20transformer%20and%20a%0Aglobal%20semantic%20transformer.%20In%20the%20local%20geometric%20transformer%20module%2C%20we%0Aexplicitly%20calculate%20the%20geometric%20disparity%20within%20the%20local%20region.%20This%0Aenables%20amplifying%20the%20affinity%20with%20geometrically%20similar%20neighbor%20points%0Awhile%20suppressing%20the%20association%20with%20other%20neighbors.%20In%20the%20global%20semantic%0Atransformer%20module%2C%20we%20design%20a%20multi-head%20voting%20strategy.%20This%20strategy%0Aevaluates%20semantic%20similarity%20across%20the%20entire%20spatial%20range%2C%20facilitating%20the%0Aprecise%20capture%20of%20contextual%20dependencies.%20Experiments%20on%20ShapeNetPart%20and%0AS3DIS%20benchmarks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20showing%0Aits%20superiority%20over%20other%20algorithms.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LAB123-tech/GSTran.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11558v1&entry.124074799=Read"},
{"title": "Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware\n  Point Cloud Registration in the Wild", "author": "Turcan Tuna and Julian Nubert and Patrick Pfreundschuh and Cesar Cadena and Shehryar Khattak and Marco Hutter", "abstract": "  The ICP registration algorithm has been a preferred method for LiDAR-based\nrobot localization for nearly a decade. However, even in modern SLAM solutions,\nICP can degrade and become unreliable in geometrically ill-conditioned\nenvironments. Current solutions primarily focus on utilizing additional sources\nof information, such as external odometry, to either replace the degenerate\ndirections of the optimization solution or add additional constraints in a\nsensor-fusion setup afterward. In response, this work investigates and compares\nnew and existing degeneracy mitigation methods for robust LiDAR-based\nlocalization and analyzes the efficacy of these approaches in degenerate\nenvironments for the first time in the literature at this scale. Specifically,\nthis work proposes and investigates i) the incorporation of different types of\nconstraints into the ICP algorithm, ii) the effect of using active or passive\ndegeneracy mitigation techniques, and iii) the choice of utilizing global point\ncloud registration methods on the ill-conditioned ICP problem in LiDAR\ndegenerate environments. The study results are validated through multiple\nreal-world field and simulated experiments. The analysis shows that active\noptimization degeneracy mitigation is necessary and advantageous in the absence\nof reliable external estimate assistance for LiDAR-SLAM. Furthermore,\nintroducing degeneracy-aware hard constraints in the optimization before or\nduring the optimization is shown to perform better in the wild than by\nincluding the constraints after. Moreover, with heuristic fine-tuned\nparameters, soft constraints can provide equal or better results in complex\nill-conditioned scenarios. The implementations used in the analysis of this\nwork are made publicly available to the community.\n", "link": "http://arxiv.org/abs/2408.11809v1", "date": "2024-08-21", "relevancy": 2.8016, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5911}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.545}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild&body=Title%3A%20Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild%0AAuthor%3A%20Turcan%20Tuna%20and%20Julian%20Nubert%20and%20Patrick%20Pfreundschuh%20and%20Cesar%20Cadena%20and%20Shehryar%20Khattak%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20The%20ICP%20registration%20algorithm%20has%20been%20a%20preferred%20method%20for%20LiDAR-based%0Arobot%20localization%20for%20nearly%20a%20decade.%20However%2C%20even%20in%20modern%20SLAM%20solutions%2C%0AICP%20can%20degrade%20and%20become%20unreliable%20in%20geometrically%20ill-conditioned%0Aenvironments.%20Current%20solutions%20primarily%20focus%20on%20utilizing%20additional%20sources%0Aof%20information%2C%20such%20as%20external%20odometry%2C%20to%20either%20replace%20the%20degenerate%0Adirections%20of%20the%20optimization%20solution%20or%20add%20additional%20constraints%20in%20a%0Asensor-fusion%20setup%20afterward.%20In%20response%2C%20this%20work%20investigates%20and%20compares%0Anew%20and%20existing%20degeneracy%20mitigation%20methods%20for%20robust%20LiDAR-based%0Alocalization%20and%20analyzes%20the%20efficacy%20of%20these%20approaches%20in%20degenerate%0Aenvironments%20for%20the%20first%20time%20in%20the%20literature%20at%20this%20scale.%20Specifically%2C%0Athis%20work%20proposes%20and%20investigates%20i%29%20the%20incorporation%20of%20different%20types%20of%0Aconstraints%20into%20the%20ICP%20algorithm%2C%20ii%29%20the%20effect%20of%20using%20active%20or%20passive%0Adegeneracy%20mitigation%20techniques%2C%20and%20iii%29%20the%20choice%20of%20utilizing%20global%20point%0Acloud%20registration%20methods%20on%20the%20ill-conditioned%20ICP%20problem%20in%20LiDAR%0Adegenerate%20environments.%20The%20study%20results%20are%20validated%20through%20multiple%0Areal-world%20field%20and%20simulated%20experiments.%20The%20analysis%20shows%20that%20active%0Aoptimization%20degeneracy%20mitigation%20is%20necessary%20and%20advantageous%20in%20the%20absence%0Aof%20reliable%20external%20estimate%20assistance%20for%20LiDAR-SLAM.%20Furthermore%2C%0Aintroducing%20degeneracy-aware%20hard%20constraints%20in%20the%20optimization%20before%20or%0Aduring%20the%20optimization%20is%20shown%20to%20perform%20better%20in%20the%20wild%20than%20by%0Aincluding%20the%20constraints%20after.%20Moreover%2C%20with%20heuristic%20fine-tuned%0Aparameters%2C%20soft%20constraints%20can%20provide%20equal%20or%20better%20results%20in%20complex%0Aill-conditioned%20scenarios.%20The%20implementations%20used%20in%20the%20analysis%20of%20this%0Awork%20are%20made%20publicly%20available%20to%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformed%252C%2520Constrained%252C%2520Aligned%253A%2520A%2520Field%2520Analysis%2520on%2520Degeneracy-aware%250A%2520%2520Point%2520Cloud%2520Registration%2520in%2520the%2520Wild%26entry.906535625%3DTurcan%2520Tuna%2520and%2520Julian%2520Nubert%2520and%2520Patrick%2520Pfreundschuh%2520and%2520Cesar%2520Cadena%2520and%2520Shehryar%2520Khattak%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520The%2520ICP%2520registration%2520algorithm%2520has%2520been%2520a%2520preferred%2520method%2520for%2520LiDAR-based%250Arobot%2520localization%2520for%2520nearly%2520a%2520decade.%2520However%252C%2520even%2520in%2520modern%2520SLAM%2520solutions%252C%250AICP%2520can%2520degrade%2520and%2520become%2520unreliable%2520in%2520geometrically%2520ill-conditioned%250Aenvironments.%2520Current%2520solutions%2520primarily%2520focus%2520on%2520utilizing%2520additional%2520sources%250Aof%2520information%252C%2520such%2520as%2520external%2520odometry%252C%2520to%2520either%2520replace%2520the%2520degenerate%250Adirections%2520of%2520the%2520optimization%2520solution%2520or%2520add%2520additional%2520constraints%2520in%2520a%250Asensor-fusion%2520setup%2520afterward.%2520In%2520response%252C%2520this%2520work%2520investigates%2520and%2520compares%250Anew%2520and%2520existing%2520degeneracy%2520mitigation%2520methods%2520for%2520robust%2520LiDAR-based%250Alocalization%2520and%2520analyzes%2520the%2520efficacy%2520of%2520these%2520approaches%2520in%2520degenerate%250Aenvironments%2520for%2520the%2520first%2520time%2520in%2520the%2520literature%2520at%2520this%2520scale.%2520Specifically%252C%250Athis%2520work%2520proposes%2520and%2520investigates%2520i%2529%2520the%2520incorporation%2520of%2520different%2520types%2520of%250Aconstraints%2520into%2520the%2520ICP%2520algorithm%252C%2520ii%2529%2520the%2520effect%2520of%2520using%2520active%2520or%2520passive%250Adegeneracy%2520mitigation%2520techniques%252C%2520and%2520iii%2529%2520the%2520choice%2520of%2520utilizing%2520global%2520point%250Acloud%2520registration%2520methods%2520on%2520the%2520ill-conditioned%2520ICP%2520problem%2520in%2520LiDAR%250Adegenerate%2520environments.%2520The%2520study%2520results%2520are%2520validated%2520through%2520multiple%250Areal-world%2520field%2520and%2520simulated%2520experiments.%2520The%2520analysis%2520shows%2520that%2520active%250Aoptimization%2520degeneracy%2520mitigation%2520is%2520necessary%2520and%2520advantageous%2520in%2520the%2520absence%250Aof%2520reliable%2520external%2520estimate%2520assistance%2520for%2520LiDAR-SLAM.%2520Furthermore%252C%250Aintroducing%2520degeneracy-aware%2520hard%2520constraints%2520in%2520the%2520optimization%2520before%2520or%250Aduring%2520the%2520optimization%2520is%2520shown%2520to%2520perform%2520better%2520in%2520the%2520wild%2520than%2520by%250Aincluding%2520the%2520constraints%2520after.%2520Moreover%252C%2520with%2520heuristic%2520fine-tuned%250Aparameters%252C%2520soft%2520constraints%2520can%2520provide%2520equal%2520or%2520better%2520results%2520in%2520complex%250Aill-conditioned%2520scenarios.%2520The%2520implementations%2520used%2520in%2520the%2520analysis%2520of%2520this%250Awork%2520are%2520made%2520publicly%2520available%2520to%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Informed%2C%20Constrained%2C%20Aligned%3A%20A%20Field%20Analysis%20on%20Degeneracy-aware%0A%20%20Point%20Cloud%20Registration%20in%20the%20Wild&entry.906535625=Turcan%20Tuna%20and%20Julian%20Nubert%20and%20Patrick%20Pfreundschuh%20and%20Cesar%20Cadena%20and%20Shehryar%20Khattak%20and%20Marco%20Hutter&entry.1292438233=%20%20The%20ICP%20registration%20algorithm%20has%20been%20a%20preferred%20method%20for%20LiDAR-based%0Arobot%20localization%20for%20nearly%20a%20decade.%20However%2C%20even%20in%20modern%20SLAM%20solutions%2C%0AICP%20can%20degrade%20and%20become%20unreliable%20in%20geometrically%20ill-conditioned%0Aenvironments.%20Current%20solutions%20primarily%20focus%20on%20utilizing%20additional%20sources%0Aof%20information%2C%20such%20as%20external%20odometry%2C%20to%20either%20replace%20the%20degenerate%0Adirections%20of%20the%20optimization%20solution%20or%20add%20additional%20constraints%20in%20a%0Asensor-fusion%20setup%20afterward.%20In%20response%2C%20this%20work%20investigates%20and%20compares%0Anew%20and%20existing%20degeneracy%20mitigation%20methods%20for%20robust%20LiDAR-based%0Alocalization%20and%20analyzes%20the%20efficacy%20of%20these%20approaches%20in%20degenerate%0Aenvironments%20for%20the%20first%20time%20in%20the%20literature%20at%20this%20scale.%20Specifically%2C%0Athis%20work%20proposes%20and%20investigates%20i%29%20the%20incorporation%20of%20different%20types%20of%0Aconstraints%20into%20the%20ICP%20algorithm%2C%20ii%29%20the%20effect%20of%20using%20active%20or%20passive%0Adegeneracy%20mitigation%20techniques%2C%20and%20iii%29%20the%20choice%20of%20utilizing%20global%20point%0Acloud%20registration%20methods%20on%20the%20ill-conditioned%20ICP%20problem%20in%20LiDAR%0Adegenerate%20environments.%20The%20study%20results%20are%20validated%20through%20multiple%0Areal-world%20field%20and%20simulated%20experiments.%20The%20analysis%20shows%20that%20active%0Aoptimization%20degeneracy%20mitigation%20is%20necessary%20and%20advantageous%20in%20the%20absence%0Aof%20reliable%20external%20estimate%20assistance%20for%20LiDAR-SLAM.%20Furthermore%2C%0Aintroducing%20degeneracy-aware%20hard%20constraints%20in%20the%20optimization%20before%20or%0Aduring%20the%20optimization%20is%20shown%20to%20perform%20better%20in%20the%20wild%20than%20by%0Aincluding%20the%20constraints%20after.%20Moreover%2C%20with%20heuristic%20fine-tuned%0Aparameters%2C%20soft%20constraints%20can%20provide%20equal%20or%20better%20results%20in%20complex%0Aill-conditioned%20scenarios.%20The%20implementations%20used%20in%20the%20analysis%20of%20this%0Awork%20are%20made%20publicly%20available%20to%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11809v1&entry.124074799=Read"},
{"title": "LiFCal: Online Light Field Camera Calibration via Bundle Adjustment", "author": "Aymeric Fleith and Doaa Ahmed and Daniel Cremers and Niclas Zeller", "abstract": "  We propose LiFCal, a novel geometric online calibration pipeline for\nMLA-based light field cameras. LiFCal accurately determines model parameters\nfrom a moving camera sequence without precise calibration targets, integrating\narbitrary metric scaling constraints. It optimizes intrinsic parameters of the\nlight field camera model, the 3D coordinates of a sparse set of scene points\nand camera poses in a single bundle adjustment defined directly on micro image\npoints.\n  We show that LiFCal can reliably and repeatably calibrate a focused plenoptic\ncamera using different input sequences, providing intrinsic camera parameters\nextremely close to state-of-the-art methods, while offering two main\nadvantages: it can be applied in a target-free scene, and it is implemented\nonline in a complete and continuous pipeline.\n  Furthermore, we demonstrate the quality of the obtained camera parameters in\ndownstream tasks like depth estimation and SLAM.\n  Webpage: https://lifcal.github.io/\n", "link": "http://arxiv.org/abs/2408.11682v1", "date": "2024-08-21", "relevancy": 2.8014, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5828}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5656}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiFCal%3A%20Online%20Light%20Field%20Camera%20Calibration%20via%20Bundle%20Adjustment&body=Title%3A%20LiFCal%3A%20Online%20Light%20Field%20Camera%20Calibration%20via%20Bundle%20Adjustment%0AAuthor%3A%20Aymeric%20Fleith%20and%20Doaa%20Ahmed%20and%20Daniel%20Cremers%20and%20Niclas%20Zeller%0AAbstract%3A%20%20%20We%20propose%20LiFCal%2C%20a%20novel%20geometric%20online%20calibration%20pipeline%20for%0AMLA-based%20light%20field%20cameras.%20LiFCal%20accurately%20determines%20model%20parameters%0Afrom%20a%20moving%20camera%20sequence%20without%20precise%20calibration%20targets%2C%20integrating%0Aarbitrary%20metric%20scaling%20constraints.%20It%20optimizes%20intrinsic%20parameters%20of%20the%0Alight%20field%20camera%20model%2C%20the%203D%20coordinates%20of%20a%20sparse%20set%20of%20scene%20points%0Aand%20camera%20poses%20in%20a%20single%20bundle%20adjustment%20defined%20directly%20on%20micro%20image%0Apoints.%0A%20%20We%20show%20that%20LiFCal%20can%20reliably%20and%20repeatably%20calibrate%20a%20focused%20plenoptic%0Acamera%20using%20different%20input%20sequences%2C%20providing%20intrinsic%20camera%20parameters%0Aextremely%20close%20to%20state-of-the-art%20methods%2C%20while%20offering%20two%20main%0Aadvantages%3A%20it%20can%20be%20applied%20in%20a%20target-free%20scene%2C%20and%20it%20is%20implemented%0Aonline%20in%20a%20complete%20and%20continuous%20pipeline.%0A%20%20Furthermore%2C%20we%20demonstrate%20the%20quality%20of%20the%20obtained%20camera%20parameters%20in%0Adownstream%20tasks%20like%20depth%20estimation%20and%20SLAM.%0A%20%20Webpage%3A%20https%3A//lifcal.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiFCal%253A%2520Online%2520Light%2520Field%2520Camera%2520Calibration%2520via%2520Bundle%2520Adjustment%26entry.906535625%3DAymeric%2520Fleith%2520and%2520Doaa%2520Ahmed%2520and%2520Daniel%2520Cremers%2520and%2520Niclas%2520Zeller%26entry.1292438233%3D%2520%2520We%2520propose%2520LiFCal%252C%2520a%2520novel%2520geometric%2520online%2520calibration%2520pipeline%2520for%250AMLA-based%2520light%2520field%2520cameras.%2520LiFCal%2520accurately%2520determines%2520model%2520parameters%250Afrom%2520a%2520moving%2520camera%2520sequence%2520without%2520precise%2520calibration%2520targets%252C%2520integrating%250Aarbitrary%2520metric%2520scaling%2520constraints.%2520It%2520optimizes%2520intrinsic%2520parameters%2520of%2520the%250Alight%2520field%2520camera%2520model%252C%2520the%25203D%2520coordinates%2520of%2520a%2520sparse%2520set%2520of%2520scene%2520points%250Aand%2520camera%2520poses%2520in%2520a%2520single%2520bundle%2520adjustment%2520defined%2520directly%2520on%2520micro%2520image%250Apoints.%250A%2520%2520We%2520show%2520that%2520LiFCal%2520can%2520reliably%2520and%2520repeatably%2520calibrate%2520a%2520focused%2520plenoptic%250Acamera%2520using%2520different%2520input%2520sequences%252C%2520providing%2520intrinsic%2520camera%2520parameters%250Aextremely%2520close%2520to%2520state-of-the-art%2520methods%252C%2520while%2520offering%2520two%2520main%250Aadvantages%253A%2520it%2520can%2520be%2520applied%2520in%2520a%2520target-free%2520scene%252C%2520and%2520it%2520is%2520implemented%250Aonline%2520in%2520a%2520complete%2520and%2520continuous%2520pipeline.%250A%2520%2520Furthermore%252C%2520we%2520demonstrate%2520the%2520quality%2520of%2520the%2520obtained%2520camera%2520parameters%2520in%250Adownstream%2520tasks%2520like%2520depth%2520estimation%2520and%2520SLAM.%250A%2520%2520Webpage%253A%2520https%253A//lifcal.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiFCal%3A%20Online%20Light%20Field%20Camera%20Calibration%20via%20Bundle%20Adjustment&entry.906535625=Aymeric%20Fleith%20and%20Doaa%20Ahmed%20and%20Daniel%20Cremers%20and%20Niclas%20Zeller&entry.1292438233=%20%20We%20propose%20LiFCal%2C%20a%20novel%20geometric%20online%20calibration%20pipeline%20for%0AMLA-based%20light%20field%20cameras.%20LiFCal%20accurately%20determines%20model%20parameters%0Afrom%20a%20moving%20camera%20sequence%20without%20precise%20calibration%20targets%2C%20integrating%0Aarbitrary%20metric%20scaling%20constraints.%20It%20optimizes%20intrinsic%20parameters%20of%20the%0Alight%20field%20camera%20model%2C%20the%203D%20coordinates%20of%20a%20sparse%20set%20of%20scene%20points%0Aand%20camera%20poses%20in%20a%20single%20bundle%20adjustment%20defined%20directly%20on%20micro%20image%0Apoints.%0A%20%20We%20show%20that%20LiFCal%20can%20reliably%20and%20repeatably%20calibrate%20a%20focused%20plenoptic%0Acamera%20using%20different%20input%20sequences%2C%20providing%20intrinsic%20camera%20parameters%0Aextremely%20close%20to%20state-of-the-art%20methods%2C%20while%20offering%20two%20main%0Aadvantages%3A%20it%20can%20be%20applied%20in%20a%20target-free%20scene%2C%20and%20it%20is%20implemented%0Aonline%20in%20a%20complete%20and%20continuous%20pipeline.%0A%20%20Furthermore%2C%20we%20demonstrate%20the%20quality%20of%20the%20obtained%20camera%20parameters%20in%0Adownstream%20tasks%20like%20depth%20estimation%20and%20SLAM.%0A%20%20Webpage%3A%20https%3A//lifcal.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11682v1&entry.124074799=Read"},
{"title": "Vessel-Promoted OCT to OCTA Image Translation by Heuristic Contextual\n  Constraints", "author": "Shuhan Li and Dong Zhang and Xiaomeng Li and Chubin Ou and Lin An and Yanwu Xu and Kwang-Ting Cheng", "abstract": "  Optical Coherence Tomography Angiography (OCTA) is a crucial tool in the\nclinical screening of retinal diseases, allowing for accurate 3D imaging of\nblood vessels through non-invasive scanning. However, the hardware-based\napproach for acquiring OCTA images presents challenges due to the need for\nspecialized sensors and expensive devices. In this paper, we introduce a novel\nmethod called TransPro, which can translate the readily available 3D Optical\nCoherence Tomography (OCT) images into 3D OCTA images without requiring any\nadditional hardware modifications. Our TransPro method is primarily driven by\ntwo novel ideas that have been overlooked by prior work. The first idea is\nderived from a critical observation that the OCTA projection map is generated\nby averaging pixel values from its corresponding B-scans along the Z-axis.\nHence, we introduce a hybrid architecture incorporating a 3D adversarial\ngenerative network and a novel Heuristic Contextual Guidance (HCG) module,\nwhich effectively maintains the consistency of the generated OCTA images\nbetween 3D volumes and projection maps. The second idea is to improve the\nvessel quality in the translated OCTA projection maps. As a result, we propose\na novel Vessel Promoted Guidance (VPG) module to enhance the attention of\nnetwork on retinal vessels. Experimental results on two datasets demonstrate\nthat our TransPro outperforms state-of-the-art approaches, with relative\nimprovements around 11.4% in MAE, 2.7% in PSNR, 2% in SSIM, 40% in VDE, and\n9.1% in VDC compared to the baseline method. The code is available at:\nhttps://github.com/ustlsh/TransPro.\n", "link": "http://arxiv.org/abs/2303.06807v2", "date": "2024-08-21", "relevancy": 2.7837, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5578}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vessel-Promoted%20OCT%20to%20OCTA%20Image%20Translation%20by%20Heuristic%20Contextual%0A%20%20Constraints&body=Title%3A%20Vessel-Promoted%20OCT%20to%20OCTA%20Image%20Translation%20by%20Heuristic%20Contextual%0A%20%20Constraints%0AAuthor%3A%20Shuhan%20Li%20and%20Dong%20Zhang%20and%20Xiaomeng%20Li%20and%20Chubin%20Ou%20and%20Lin%20An%20and%20Yanwu%20Xu%20and%20Kwang-Ting%20Cheng%0AAbstract%3A%20%20%20Optical%20Coherence%20Tomography%20Angiography%20%28OCTA%29%20is%20a%20crucial%20tool%20in%20the%0Aclinical%20screening%20of%20retinal%20diseases%2C%20allowing%20for%20accurate%203D%20imaging%20of%0Ablood%20vessels%20through%20non-invasive%20scanning.%20However%2C%20the%20hardware-based%0Aapproach%20for%20acquiring%20OCTA%20images%20presents%20challenges%20due%20to%20the%20need%20for%0Aspecialized%20sensors%20and%20expensive%20devices.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Amethod%20called%20TransPro%2C%20which%20can%20translate%20the%20readily%20available%203D%20Optical%0ACoherence%20Tomography%20%28OCT%29%20images%20into%203D%20OCTA%20images%20without%20requiring%20any%0Aadditional%20hardware%20modifications.%20Our%20TransPro%20method%20is%20primarily%20driven%20by%0Atwo%20novel%20ideas%20that%20have%20been%20overlooked%20by%20prior%20work.%20The%20first%20idea%20is%0Aderived%20from%20a%20critical%20observation%20that%20the%20OCTA%20projection%20map%20is%20generated%0Aby%20averaging%20pixel%20values%20from%20its%20corresponding%20B-scans%20along%20the%20Z-axis.%0AHence%2C%20we%20introduce%20a%20hybrid%20architecture%20incorporating%20a%203D%20adversarial%0Agenerative%20network%20and%20a%20novel%20Heuristic%20Contextual%20Guidance%20%28HCG%29%20module%2C%0Awhich%20effectively%20maintains%20the%20consistency%20of%20the%20generated%20OCTA%20images%0Abetween%203D%20volumes%20and%20projection%20maps.%20The%20second%20idea%20is%20to%20improve%20the%0Avessel%20quality%20in%20the%20translated%20OCTA%20projection%20maps.%20As%20a%20result%2C%20we%20propose%0Aa%20novel%20Vessel%20Promoted%20Guidance%20%28VPG%29%20module%20to%20enhance%20the%20attention%20of%0Anetwork%20on%20retinal%20vessels.%20Experimental%20results%20on%20two%20datasets%20demonstrate%0Athat%20our%20TransPro%20outperforms%20state-of-the-art%20approaches%2C%20with%20relative%0Aimprovements%20around%2011.4%25%20in%20MAE%2C%202.7%25%20in%20PSNR%2C%202%25%20in%20SSIM%2C%2040%25%20in%20VDE%2C%20and%0A9.1%25%20in%20VDC%20compared%20to%20the%20baseline%20method.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ustlsh/TransPro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVessel-Promoted%2520OCT%2520to%2520OCTA%2520Image%2520Translation%2520by%2520Heuristic%2520Contextual%250A%2520%2520Constraints%26entry.906535625%3DShuhan%2520Li%2520and%2520Dong%2520Zhang%2520and%2520Xiaomeng%2520Li%2520and%2520Chubin%2520Ou%2520and%2520Lin%2520An%2520and%2520Yanwu%2520Xu%2520and%2520Kwang-Ting%2520Cheng%26entry.1292438233%3D%2520%2520Optical%2520Coherence%2520Tomography%2520Angiography%2520%2528OCTA%2529%2520is%2520a%2520crucial%2520tool%2520in%2520the%250Aclinical%2520screening%2520of%2520retinal%2520diseases%252C%2520allowing%2520for%2520accurate%25203D%2520imaging%2520of%250Ablood%2520vessels%2520through%2520non-invasive%2520scanning.%2520However%252C%2520the%2520hardware-based%250Aapproach%2520for%2520acquiring%2520OCTA%2520images%2520presents%2520challenges%2520due%2520to%2520the%2520need%2520for%250Aspecialized%2520sensors%2520and%2520expensive%2520devices.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Amethod%2520called%2520TransPro%252C%2520which%2520can%2520translate%2520the%2520readily%2520available%25203D%2520Optical%250ACoherence%2520Tomography%2520%2528OCT%2529%2520images%2520into%25203D%2520OCTA%2520images%2520without%2520requiring%2520any%250Aadditional%2520hardware%2520modifications.%2520Our%2520TransPro%2520method%2520is%2520primarily%2520driven%2520by%250Atwo%2520novel%2520ideas%2520that%2520have%2520been%2520overlooked%2520by%2520prior%2520work.%2520The%2520first%2520idea%2520is%250Aderived%2520from%2520a%2520critical%2520observation%2520that%2520the%2520OCTA%2520projection%2520map%2520is%2520generated%250Aby%2520averaging%2520pixel%2520values%2520from%2520its%2520corresponding%2520B-scans%2520along%2520the%2520Z-axis.%250AHence%252C%2520we%2520introduce%2520a%2520hybrid%2520architecture%2520incorporating%2520a%25203D%2520adversarial%250Agenerative%2520network%2520and%2520a%2520novel%2520Heuristic%2520Contextual%2520Guidance%2520%2528HCG%2529%2520module%252C%250Awhich%2520effectively%2520maintains%2520the%2520consistency%2520of%2520the%2520generated%2520OCTA%2520images%250Abetween%25203D%2520volumes%2520and%2520projection%2520maps.%2520The%2520second%2520idea%2520is%2520to%2520improve%2520the%250Avessel%2520quality%2520in%2520the%2520translated%2520OCTA%2520projection%2520maps.%2520As%2520a%2520result%252C%2520we%2520propose%250Aa%2520novel%2520Vessel%2520Promoted%2520Guidance%2520%2528VPG%2529%2520module%2520to%2520enhance%2520the%2520attention%2520of%250Anetwork%2520on%2520retinal%2520vessels.%2520Experimental%2520results%2520on%2520two%2520datasets%2520demonstrate%250Athat%2520our%2520TransPro%2520outperforms%2520state-of-the-art%2520approaches%252C%2520with%2520relative%250Aimprovements%2520around%252011.4%2525%2520in%2520MAE%252C%25202.7%2525%2520in%2520PSNR%252C%25202%2525%2520in%2520SSIM%252C%252040%2525%2520in%2520VDE%252C%2520and%250A9.1%2525%2520in%2520VDC%2520compared%2520to%2520the%2520baseline%2520method.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ustlsh/TransPro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.06807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vessel-Promoted%20OCT%20to%20OCTA%20Image%20Translation%20by%20Heuristic%20Contextual%0A%20%20Constraints&entry.906535625=Shuhan%20Li%20and%20Dong%20Zhang%20and%20Xiaomeng%20Li%20and%20Chubin%20Ou%20and%20Lin%20An%20and%20Yanwu%20Xu%20and%20Kwang-Ting%20Cheng&entry.1292438233=%20%20Optical%20Coherence%20Tomography%20Angiography%20%28OCTA%29%20is%20a%20crucial%20tool%20in%20the%0Aclinical%20screening%20of%20retinal%20diseases%2C%20allowing%20for%20accurate%203D%20imaging%20of%0Ablood%20vessels%20through%20non-invasive%20scanning.%20However%2C%20the%20hardware-based%0Aapproach%20for%20acquiring%20OCTA%20images%20presents%20challenges%20due%20to%20the%20need%20for%0Aspecialized%20sensors%20and%20expensive%20devices.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Amethod%20called%20TransPro%2C%20which%20can%20translate%20the%20readily%20available%203D%20Optical%0ACoherence%20Tomography%20%28OCT%29%20images%20into%203D%20OCTA%20images%20without%20requiring%20any%0Aadditional%20hardware%20modifications.%20Our%20TransPro%20method%20is%20primarily%20driven%20by%0Atwo%20novel%20ideas%20that%20have%20been%20overlooked%20by%20prior%20work.%20The%20first%20idea%20is%0Aderived%20from%20a%20critical%20observation%20that%20the%20OCTA%20projection%20map%20is%20generated%0Aby%20averaging%20pixel%20values%20from%20its%20corresponding%20B-scans%20along%20the%20Z-axis.%0AHence%2C%20we%20introduce%20a%20hybrid%20architecture%20incorporating%20a%203D%20adversarial%0Agenerative%20network%20and%20a%20novel%20Heuristic%20Contextual%20Guidance%20%28HCG%29%20module%2C%0Awhich%20effectively%20maintains%20the%20consistency%20of%20the%20generated%20OCTA%20images%0Abetween%203D%20volumes%20and%20projection%20maps.%20The%20second%20idea%20is%20to%20improve%20the%0Avessel%20quality%20in%20the%20translated%20OCTA%20projection%20maps.%20As%20a%20result%2C%20we%20propose%0Aa%20novel%20Vessel%20Promoted%20Guidance%20%28VPG%29%20module%20to%20enhance%20the%20attention%20of%0Anetwork%20on%20retinal%20vessels.%20Experimental%20results%20on%20two%20datasets%20demonstrate%0Athat%20our%20TransPro%20outperforms%20state-of-the-art%20approaches%2C%20with%20relative%0Aimprovements%20around%2011.4%25%20in%20MAE%2C%202.7%25%20in%20PSNR%2C%202%25%20in%20SSIM%2C%2040%25%20in%20VDE%2C%20and%0A9.1%25%20in%20VDC%20compared%20to%20the%20baseline%20method.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ustlsh/TransPro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06807v2&entry.124074799=Read"},
{"title": "Freehand Sketch Generation from Mechanical Components", "author": "Zhichao Liao and Di Huang and Heming Fang and Yue Ma and Fengyuan Piao and Xinghui Li and Long Zeng and Pingfa Feng", "abstract": "  Drawing freehand sketches of mechanical components on multimedia devices for\nAI-based engineering modeling has become a new trend. However, its development\nis being impeded because existing works cannot produce suitable sketches for\ndata-driven research. These works either generate sketches lacking a freehand\nstyle or utilize generative models not originally designed for this task\nresulting in poor effectiveness. To address this issue, we design a two-stage\ngenerative framework mimicking the human sketching behavior pattern, called\nMSFormer, which is the first time to produce humanoid freehand sketches\ntailored for mechanical components. The first stage employs Open CASCADE\ntechnology to obtain multi-view contour sketches from mechanical components,\nfiltering perturbing signals for the ensuing generation process. Meanwhile, we\ndesign a view selector to simulate viewpoint selection tasks during human\nsketching for picking out information-rich sketches. The second stage\ntranslates contour sketches into freehand sketches by a transformer-based\ngenerator. To retain essential modeling features as much as possible and\nrationalize stroke distribution, we introduce a novel edge-constraint stroke\ninitialization. Furthermore, we utilize a CLIP vision encoder and a new loss\nfunction incorporating the Hausdorff distance to enhance the generalizability\nand robustness of the model. Extensive experiments demonstrate that our\napproach achieves state-of-the-art performance for generating freehand sketches\nin the mechanical domain. Project page: https://mcfreeskegen.github.io .\n", "link": "http://arxiv.org/abs/2408.05966v2", "date": "2024-08-21", "relevancy": 2.7604, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5726}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5418}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Freehand%20Sketch%20Generation%20from%20Mechanical%20Components&body=Title%3A%20Freehand%20Sketch%20Generation%20from%20Mechanical%20Components%0AAuthor%3A%20Zhichao%20Liao%20and%20Di%20Huang%20and%20Heming%20Fang%20and%20Yue%20Ma%20and%20Fengyuan%20Piao%20and%20Xinghui%20Li%20and%20Long%20Zeng%20and%20Pingfa%20Feng%0AAbstract%3A%20%20%20Drawing%20freehand%20sketches%20of%20mechanical%20components%20on%20multimedia%20devices%20for%0AAI-based%20engineering%20modeling%20has%20become%20a%20new%20trend.%20However%2C%20its%20development%0Ais%20being%20impeded%20because%20existing%20works%20cannot%20produce%20suitable%20sketches%20for%0Adata-driven%20research.%20These%20works%20either%20generate%20sketches%20lacking%20a%20freehand%0Astyle%20or%20utilize%20generative%20models%20not%20originally%20designed%20for%20this%20task%0Aresulting%20in%20poor%20effectiveness.%20To%20address%20this%20issue%2C%20we%20design%20a%20two-stage%0Agenerative%20framework%20mimicking%20the%20human%20sketching%20behavior%20pattern%2C%20called%0AMSFormer%2C%20which%20is%20the%20first%20time%20to%20produce%20humanoid%20freehand%20sketches%0Atailored%20for%20mechanical%20components.%20The%20first%20stage%20employs%20Open%20CASCADE%0Atechnology%20to%20obtain%20multi-view%20contour%20sketches%20from%20mechanical%20components%2C%0Afiltering%20perturbing%20signals%20for%20the%20ensuing%20generation%20process.%20Meanwhile%2C%20we%0Adesign%20a%20view%20selector%20to%20simulate%20viewpoint%20selection%20tasks%20during%20human%0Asketching%20for%20picking%20out%20information-rich%20sketches.%20The%20second%20stage%0Atranslates%20contour%20sketches%20into%20freehand%20sketches%20by%20a%20transformer-based%0Agenerator.%20To%20retain%20essential%20modeling%20features%20as%20much%20as%20possible%20and%0Arationalize%20stroke%20distribution%2C%20we%20introduce%20a%20novel%20edge-constraint%20stroke%0Ainitialization.%20Furthermore%2C%20we%20utilize%20a%20CLIP%20vision%20encoder%20and%20a%20new%20loss%0Afunction%20incorporating%20the%20Hausdorff%20distance%20to%20enhance%20the%20generalizability%0Aand%20robustness%20of%20the%20model.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20for%20generating%20freehand%20sketches%0Ain%20the%20mechanical%20domain.%20Project%20page%3A%20https%3A//mcfreeskegen.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05966v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreehand%2520Sketch%2520Generation%2520from%2520Mechanical%2520Components%26entry.906535625%3DZhichao%2520Liao%2520and%2520Di%2520Huang%2520and%2520Heming%2520Fang%2520and%2520Yue%2520Ma%2520and%2520Fengyuan%2520Piao%2520and%2520Xinghui%2520Li%2520and%2520Long%2520Zeng%2520and%2520Pingfa%2520Feng%26entry.1292438233%3D%2520%2520Drawing%2520freehand%2520sketches%2520of%2520mechanical%2520components%2520on%2520multimedia%2520devices%2520for%250AAI-based%2520engineering%2520modeling%2520has%2520become%2520a%2520new%2520trend.%2520However%252C%2520its%2520development%250Ais%2520being%2520impeded%2520because%2520existing%2520works%2520cannot%2520produce%2520suitable%2520sketches%2520for%250Adata-driven%2520research.%2520These%2520works%2520either%2520generate%2520sketches%2520lacking%2520a%2520freehand%250Astyle%2520or%2520utilize%2520generative%2520models%2520not%2520originally%2520designed%2520for%2520this%2520task%250Aresulting%2520in%2520poor%2520effectiveness.%2520To%2520address%2520this%2520issue%252C%2520we%2520design%2520a%2520two-stage%250Agenerative%2520framework%2520mimicking%2520the%2520human%2520sketching%2520behavior%2520pattern%252C%2520called%250AMSFormer%252C%2520which%2520is%2520the%2520first%2520time%2520to%2520produce%2520humanoid%2520freehand%2520sketches%250Atailored%2520for%2520mechanical%2520components.%2520The%2520first%2520stage%2520employs%2520Open%2520CASCADE%250Atechnology%2520to%2520obtain%2520multi-view%2520contour%2520sketches%2520from%2520mechanical%2520components%252C%250Afiltering%2520perturbing%2520signals%2520for%2520the%2520ensuing%2520generation%2520process.%2520Meanwhile%252C%2520we%250Adesign%2520a%2520view%2520selector%2520to%2520simulate%2520viewpoint%2520selection%2520tasks%2520during%2520human%250Asketching%2520for%2520picking%2520out%2520information-rich%2520sketches.%2520The%2520second%2520stage%250Atranslates%2520contour%2520sketches%2520into%2520freehand%2520sketches%2520by%2520a%2520transformer-based%250Agenerator.%2520To%2520retain%2520essential%2520modeling%2520features%2520as%2520much%2520as%2520possible%2520and%250Arationalize%2520stroke%2520distribution%252C%2520we%2520introduce%2520a%2520novel%2520edge-constraint%2520stroke%250Ainitialization.%2520Furthermore%252C%2520we%2520utilize%2520a%2520CLIP%2520vision%2520encoder%2520and%2520a%2520new%2520loss%250Afunction%2520incorporating%2520the%2520Hausdorff%2520distance%2520to%2520enhance%2520the%2520generalizability%250Aand%2520robustness%2520of%2520the%2520model.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520state-of-the-art%2520performance%2520for%2520generating%2520freehand%2520sketches%250Ain%2520the%2520mechanical%2520domain.%2520Project%2520page%253A%2520https%253A//mcfreeskegen.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05966v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Freehand%20Sketch%20Generation%20from%20Mechanical%20Components&entry.906535625=Zhichao%20Liao%20and%20Di%20Huang%20and%20Heming%20Fang%20and%20Yue%20Ma%20and%20Fengyuan%20Piao%20and%20Xinghui%20Li%20and%20Long%20Zeng%20and%20Pingfa%20Feng&entry.1292438233=%20%20Drawing%20freehand%20sketches%20of%20mechanical%20components%20on%20multimedia%20devices%20for%0AAI-based%20engineering%20modeling%20has%20become%20a%20new%20trend.%20However%2C%20its%20development%0Ais%20being%20impeded%20because%20existing%20works%20cannot%20produce%20suitable%20sketches%20for%0Adata-driven%20research.%20These%20works%20either%20generate%20sketches%20lacking%20a%20freehand%0Astyle%20or%20utilize%20generative%20models%20not%20originally%20designed%20for%20this%20task%0Aresulting%20in%20poor%20effectiveness.%20To%20address%20this%20issue%2C%20we%20design%20a%20two-stage%0Agenerative%20framework%20mimicking%20the%20human%20sketching%20behavior%20pattern%2C%20called%0AMSFormer%2C%20which%20is%20the%20first%20time%20to%20produce%20humanoid%20freehand%20sketches%0Atailored%20for%20mechanical%20components.%20The%20first%20stage%20employs%20Open%20CASCADE%0Atechnology%20to%20obtain%20multi-view%20contour%20sketches%20from%20mechanical%20components%2C%0Afiltering%20perturbing%20signals%20for%20the%20ensuing%20generation%20process.%20Meanwhile%2C%20we%0Adesign%20a%20view%20selector%20to%20simulate%20viewpoint%20selection%20tasks%20during%20human%0Asketching%20for%20picking%20out%20information-rich%20sketches.%20The%20second%20stage%0Atranslates%20contour%20sketches%20into%20freehand%20sketches%20by%20a%20transformer-based%0Agenerator.%20To%20retain%20essential%20modeling%20features%20as%20much%20as%20possible%20and%0Arationalize%20stroke%20distribution%2C%20we%20introduce%20a%20novel%20edge-constraint%20stroke%0Ainitialization.%20Furthermore%2C%20we%20utilize%20a%20CLIP%20vision%20encoder%20and%20a%20new%20loss%0Afunction%20incorporating%20the%20Hausdorff%20distance%20to%20enhance%20the%20generalizability%0Aand%20robustness%20of%20the%20model.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20for%20generating%20freehand%20sketches%0Ain%20the%20mechanical%20domain.%20Project%20page%3A%20https%3A//mcfreeskegen.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05966v2&entry.124074799=Read"},
{"title": "NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for\n  Domain-Generalized Nuclei Segmentation", "author": "Zhenye Lou and Qing Xu and Zekun Jiang and Xiangjian He and Zhen Chen and Yi Wang and Chenxin Li and Maggie M. He and Wenting Duan", "abstract": "  Domain-generalized nuclei segmentation refers to the generalizability of\nmodels to unseen domains based on knowledge learned from source domains and is\nchallenged by various image conditions, cell types, and stain strategies.\nRecently, the Segment Anything Model (SAM) has made great success in universal\nimage segmentation by interactive prompt modes (e.g., point and box). Despite\nits strengths, the original SAM presents limited adaptation to medical images.\nMoreover, SAM requires providing manual bounding box prompts for each object to\nproduce satisfactory segmentation masks, so it is laborious in nuclei\nsegmentation scenarios. To address these limitations, we propose a\ndomain-generalizable framework for nuclei image segmentation, abbreviated to\nNuSegDG. Specifically, we first devise a Heterogeneous Space Adapter\n(HS-Adapter) to learn multi-dimensional feature representations of different\nnuclei domains by injecting a small number of trainable parameters into the\nimage encoder of SAM. To alleviate the labor-intensive requirement of manual\nprompts, we introduce a Gaussian-Kernel Prompt Encoder (GKP-Encoder) to\ngenerate density maps driven by a single point, which guides segmentation\npredictions by mixing position prompts and semantic prompts. Furthermore, we\npresent a Two-Stage Mask Decoder (TSM-Decoder) to effectively convert semantic\nmasks to instance maps without the manual demand for morphological shape\nrefinement. Based on our experimental evaluations, the proposed NuSegDG\ndemonstrates state-of-the-art performance in nuclei instance segmentation,\nexhibiting superior domain generalization capabilities. The source code is\navailable at https://github.com/xq141839/NuSegDG.\n", "link": "http://arxiv.org/abs/2408.11787v1", "date": "2024-08-21", "relevancy": 2.723, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5527}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NuSegDG%3A%20Integration%20of%20Heterogeneous%20Space%20and%20Gaussian%20Kernel%20for%0A%20%20Domain-Generalized%20Nuclei%20Segmentation&body=Title%3A%20NuSegDG%3A%20Integration%20of%20Heterogeneous%20Space%20and%20Gaussian%20Kernel%20for%0A%20%20Domain-Generalized%20Nuclei%20Segmentation%0AAuthor%3A%20Zhenye%20Lou%20and%20Qing%20Xu%20and%20Zekun%20Jiang%20and%20Xiangjian%20He%20and%20Zhen%20Chen%20and%20Yi%20Wang%20and%20Chenxin%20Li%20and%20Maggie%20M.%20He%20and%20Wenting%20Duan%0AAbstract%3A%20%20%20Domain-generalized%20nuclei%20segmentation%20refers%20to%20the%20generalizability%20of%0Amodels%20to%20unseen%20domains%20based%20on%20knowledge%20learned%20from%20source%20domains%20and%20is%0Achallenged%20by%20various%20image%20conditions%2C%20cell%20types%2C%20and%20stain%20strategies.%0ARecently%2C%20the%20Segment%20Anything%20Model%20%28SAM%29%20has%20made%20great%20success%20in%20universal%0Aimage%20segmentation%20by%20interactive%20prompt%20modes%20%28e.g.%2C%20point%20and%20box%29.%20Despite%0Aits%20strengths%2C%20the%20original%20SAM%20presents%20limited%20adaptation%20to%20medical%20images.%0AMoreover%2C%20SAM%20requires%20providing%20manual%20bounding%20box%20prompts%20for%20each%20object%20to%0Aproduce%20satisfactory%20segmentation%20masks%2C%20so%20it%20is%20laborious%20in%20nuclei%0Asegmentation%20scenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Adomain-generalizable%20framework%20for%20nuclei%20image%20segmentation%2C%20abbreviated%20to%0ANuSegDG.%20Specifically%2C%20we%20first%20devise%20a%20Heterogeneous%20Space%20Adapter%0A%28HS-Adapter%29%20to%20learn%20multi-dimensional%20feature%20representations%20of%20different%0Anuclei%20domains%20by%20injecting%20a%20small%20number%20of%20trainable%20parameters%20into%20the%0Aimage%20encoder%20of%20SAM.%20To%20alleviate%20the%20labor-intensive%20requirement%20of%20manual%0Aprompts%2C%20we%20introduce%20a%20Gaussian-Kernel%20Prompt%20Encoder%20%28GKP-Encoder%29%20to%0Agenerate%20density%20maps%20driven%20by%20a%20single%20point%2C%20which%20guides%20segmentation%0Apredictions%20by%20mixing%20position%20prompts%20and%20semantic%20prompts.%20Furthermore%2C%20we%0Apresent%20a%20Two-Stage%20Mask%20Decoder%20%28TSM-Decoder%29%20to%20effectively%20convert%20semantic%0Amasks%20to%20instance%20maps%20without%20the%20manual%20demand%20for%20morphological%20shape%0Arefinement.%20Based%20on%20our%20experimental%20evaluations%2C%20the%20proposed%20NuSegDG%0Ademonstrates%20state-of-the-art%20performance%20in%20nuclei%20instance%20segmentation%2C%0Aexhibiting%20superior%20domain%20generalization%20capabilities.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/xq141839/NuSegDG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNuSegDG%253A%2520Integration%2520of%2520Heterogeneous%2520Space%2520and%2520Gaussian%2520Kernel%2520for%250A%2520%2520Domain-Generalized%2520Nuclei%2520Segmentation%26entry.906535625%3DZhenye%2520Lou%2520and%2520Qing%2520Xu%2520and%2520Zekun%2520Jiang%2520and%2520Xiangjian%2520He%2520and%2520Zhen%2520Chen%2520and%2520Yi%2520Wang%2520and%2520Chenxin%2520Li%2520and%2520Maggie%2520M.%2520He%2520and%2520Wenting%2520Duan%26entry.1292438233%3D%2520%2520Domain-generalized%2520nuclei%2520segmentation%2520refers%2520to%2520the%2520generalizability%2520of%250Amodels%2520to%2520unseen%2520domains%2520based%2520on%2520knowledge%2520learned%2520from%2520source%2520domains%2520and%2520is%250Achallenged%2520by%2520various%2520image%2520conditions%252C%2520cell%2520types%252C%2520and%2520stain%2520strategies.%250ARecently%252C%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520made%2520great%2520success%2520in%2520universal%250Aimage%2520segmentation%2520by%2520interactive%2520prompt%2520modes%2520%2528e.g.%252C%2520point%2520and%2520box%2529.%2520Despite%250Aits%2520strengths%252C%2520the%2520original%2520SAM%2520presents%2520limited%2520adaptation%2520to%2520medical%2520images.%250AMoreover%252C%2520SAM%2520requires%2520providing%2520manual%2520bounding%2520box%2520prompts%2520for%2520each%2520object%2520to%250Aproduce%2520satisfactory%2520segmentation%2520masks%252C%2520so%2520it%2520is%2520laborious%2520in%2520nuclei%250Asegmentation%2520scenarios.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Adomain-generalizable%2520framework%2520for%2520nuclei%2520image%2520segmentation%252C%2520abbreviated%2520to%250ANuSegDG.%2520Specifically%252C%2520we%2520first%2520devise%2520a%2520Heterogeneous%2520Space%2520Adapter%250A%2528HS-Adapter%2529%2520to%2520learn%2520multi-dimensional%2520feature%2520representations%2520of%2520different%250Anuclei%2520domains%2520by%2520injecting%2520a%2520small%2520number%2520of%2520trainable%2520parameters%2520into%2520the%250Aimage%2520encoder%2520of%2520SAM.%2520To%2520alleviate%2520the%2520labor-intensive%2520requirement%2520of%2520manual%250Aprompts%252C%2520we%2520introduce%2520a%2520Gaussian-Kernel%2520Prompt%2520Encoder%2520%2528GKP-Encoder%2529%2520to%250Agenerate%2520density%2520maps%2520driven%2520by%2520a%2520single%2520point%252C%2520which%2520guides%2520segmentation%250Apredictions%2520by%2520mixing%2520position%2520prompts%2520and%2520semantic%2520prompts.%2520Furthermore%252C%2520we%250Apresent%2520a%2520Two-Stage%2520Mask%2520Decoder%2520%2528TSM-Decoder%2529%2520to%2520effectively%2520convert%2520semantic%250Amasks%2520to%2520instance%2520maps%2520without%2520the%2520manual%2520demand%2520for%2520morphological%2520shape%250Arefinement.%2520Based%2520on%2520our%2520experimental%2520evaluations%252C%2520the%2520proposed%2520NuSegDG%250Ademonstrates%2520state-of-the-art%2520performance%2520in%2520nuclei%2520instance%2520segmentation%252C%250Aexhibiting%2520superior%2520domain%2520generalization%2520capabilities.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/xq141839/NuSegDG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NuSegDG%3A%20Integration%20of%20Heterogeneous%20Space%20and%20Gaussian%20Kernel%20for%0A%20%20Domain-Generalized%20Nuclei%20Segmentation&entry.906535625=Zhenye%20Lou%20and%20Qing%20Xu%20and%20Zekun%20Jiang%20and%20Xiangjian%20He%20and%20Zhen%20Chen%20and%20Yi%20Wang%20and%20Chenxin%20Li%20and%20Maggie%20M.%20He%20and%20Wenting%20Duan&entry.1292438233=%20%20Domain-generalized%20nuclei%20segmentation%20refers%20to%20the%20generalizability%20of%0Amodels%20to%20unseen%20domains%20based%20on%20knowledge%20learned%20from%20source%20domains%20and%20is%0Achallenged%20by%20various%20image%20conditions%2C%20cell%20types%2C%20and%20stain%20strategies.%0ARecently%2C%20the%20Segment%20Anything%20Model%20%28SAM%29%20has%20made%20great%20success%20in%20universal%0Aimage%20segmentation%20by%20interactive%20prompt%20modes%20%28e.g.%2C%20point%20and%20box%29.%20Despite%0Aits%20strengths%2C%20the%20original%20SAM%20presents%20limited%20adaptation%20to%20medical%20images.%0AMoreover%2C%20SAM%20requires%20providing%20manual%20bounding%20box%20prompts%20for%20each%20object%20to%0Aproduce%20satisfactory%20segmentation%20masks%2C%20so%20it%20is%20laborious%20in%20nuclei%0Asegmentation%20scenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Adomain-generalizable%20framework%20for%20nuclei%20image%20segmentation%2C%20abbreviated%20to%0ANuSegDG.%20Specifically%2C%20we%20first%20devise%20a%20Heterogeneous%20Space%20Adapter%0A%28HS-Adapter%29%20to%20learn%20multi-dimensional%20feature%20representations%20of%20different%0Anuclei%20domains%20by%20injecting%20a%20small%20number%20of%20trainable%20parameters%20into%20the%0Aimage%20encoder%20of%20SAM.%20To%20alleviate%20the%20labor-intensive%20requirement%20of%20manual%0Aprompts%2C%20we%20introduce%20a%20Gaussian-Kernel%20Prompt%20Encoder%20%28GKP-Encoder%29%20to%0Agenerate%20density%20maps%20driven%20by%20a%20single%20point%2C%20which%20guides%20segmentation%0Apredictions%20by%20mixing%20position%20prompts%20and%20semantic%20prompts.%20Furthermore%2C%20we%0Apresent%20a%20Two-Stage%20Mask%20Decoder%20%28TSM-Decoder%29%20to%20effectively%20convert%20semantic%0Amasks%20to%20instance%20maps%20without%20the%20manual%20demand%20for%20morphological%20shape%0Arefinement.%20Based%20on%20our%20experimental%20evaluations%2C%20the%20proposed%20NuSegDG%0Ademonstrates%20state-of-the-art%20performance%20in%20nuclei%20instance%20segmentation%2C%0Aexhibiting%20superior%20domain%20generalization%20capabilities.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/xq141839/NuSegDG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11787v1&entry.124074799=Read"},
{"title": "Positional Prompt Tuning for Efficient 3D Representation Learning", "author": "Shaochen Zhang and Zekun Qi and Runpei Dong and Xiuxiu Bai and Xing Wei", "abstract": "  Point cloud analysis has achieved significant development and is\nwell-performed in multiple downstream tasks like point cloud classification and\nsegmentation, etc. Being conscious of the simplicity of the position encoding\nstructure in Transformer-based architectures, we attach importance to the\nposition encoding as a high-dimensional part and the patch encoder to offer\nmulti-scale information. Together with the sequential Transformer, the whole\nmodule with position encoding comprehensively constructs a multi-scale feature\nabstraction module that considers both the local parts from the patch and the\nglobal parts from center points as position encoding. With only a few\nparameters, the position embedding module fits the setting of PEFT\n(Parameter-Efficient Fine-Tuning) tasks pretty well. Thus we unfreeze these\nparameters as a fine-tuning part. At the same time, we review the existing\nprompt and adapter tuning methods, proposing a fresh way of prompts and\nsynthesizing them with adapters as dynamic adjustments. Our Proposed method of\nPEFT tasks, namely PPT, with only 1.05% of parameters for training, gets\nstate-of-the-art results in several mainstream datasets, such as 95.01%\naccuracy in the ScanObjectNN OBJ_BG dataset. Codes will be released at\nhttps://github.com/zsc000722/PPT.\n", "link": "http://arxiv.org/abs/2408.11567v1", "date": "2024-08-21", "relevancy": 2.7082, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5537}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Positional%20Prompt%20Tuning%20for%20Efficient%203D%20Representation%20Learning&body=Title%3A%20Positional%20Prompt%20Tuning%20for%20Efficient%203D%20Representation%20Learning%0AAuthor%3A%20Shaochen%20Zhang%20and%20Zekun%20Qi%20and%20Runpei%20Dong%20and%20Xiuxiu%20Bai%20and%20Xing%20Wei%0AAbstract%3A%20%20%20Point%20cloud%20analysis%20has%20achieved%20significant%20development%20and%20is%0Awell-performed%20in%20multiple%20downstream%20tasks%20like%20point%20cloud%20classification%20and%0Asegmentation%2C%20etc.%20Being%20conscious%20of%20the%20simplicity%20of%20the%20position%20encoding%0Astructure%20in%20Transformer-based%20architectures%2C%20we%20attach%20importance%20to%20the%0Aposition%20encoding%20as%20a%20high-dimensional%20part%20and%20the%20patch%20encoder%20to%20offer%0Amulti-scale%20information.%20Together%20with%20the%20sequential%20Transformer%2C%20the%20whole%0Amodule%20with%20position%20encoding%20comprehensively%20constructs%20a%20multi-scale%20feature%0Aabstraction%20module%20that%20considers%20both%20the%20local%20parts%20from%20the%20patch%20and%20the%0Aglobal%20parts%20from%20center%20points%20as%20position%20encoding.%20With%20only%20a%20few%0Aparameters%2C%20the%20position%20embedding%20module%20fits%20the%20setting%20of%20PEFT%0A%28Parameter-Efficient%20Fine-Tuning%29%20tasks%20pretty%20well.%20Thus%20we%20unfreeze%20these%0Aparameters%20as%20a%20fine-tuning%20part.%20At%20the%20same%20time%2C%20we%20review%20the%20existing%0Aprompt%20and%20adapter%20tuning%20methods%2C%20proposing%20a%20fresh%20way%20of%20prompts%20and%0Asynthesizing%20them%20with%20adapters%20as%20dynamic%20adjustments.%20Our%20Proposed%20method%20of%0APEFT%20tasks%2C%20namely%20PPT%2C%20with%20only%201.05%25%20of%20parameters%20for%20training%2C%20gets%0Astate-of-the-art%20results%20in%20several%20mainstream%20datasets%2C%20such%20as%2095.01%25%0Aaccuracy%20in%20the%20ScanObjectNN%20OBJ_BG%20dataset.%20Codes%20will%20be%20released%20at%0Ahttps%3A//github.com/zsc000722/PPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositional%2520Prompt%2520Tuning%2520for%2520Efficient%25203D%2520Representation%2520Learning%26entry.906535625%3DShaochen%2520Zhang%2520and%2520Zekun%2520Qi%2520and%2520Runpei%2520Dong%2520and%2520Xiuxiu%2520Bai%2520and%2520Xing%2520Wei%26entry.1292438233%3D%2520%2520Point%2520cloud%2520analysis%2520has%2520achieved%2520significant%2520development%2520and%2520is%250Awell-performed%2520in%2520multiple%2520downstream%2520tasks%2520like%2520point%2520cloud%2520classification%2520and%250Asegmentation%252C%2520etc.%2520Being%2520conscious%2520of%2520the%2520simplicity%2520of%2520the%2520position%2520encoding%250Astructure%2520in%2520Transformer-based%2520architectures%252C%2520we%2520attach%2520importance%2520to%2520the%250Aposition%2520encoding%2520as%2520a%2520high-dimensional%2520part%2520and%2520the%2520patch%2520encoder%2520to%2520offer%250Amulti-scale%2520information.%2520Together%2520with%2520the%2520sequential%2520Transformer%252C%2520the%2520whole%250Amodule%2520with%2520position%2520encoding%2520comprehensively%2520constructs%2520a%2520multi-scale%2520feature%250Aabstraction%2520module%2520that%2520considers%2520both%2520the%2520local%2520parts%2520from%2520the%2520patch%2520and%2520the%250Aglobal%2520parts%2520from%2520center%2520points%2520as%2520position%2520encoding.%2520With%2520only%2520a%2520few%250Aparameters%252C%2520the%2520position%2520embedding%2520module%2520fits%2520the%2520setting%2520of%2520PEFT%250A%2528Parameter-Efficient%2520Fine-Tuning%2529%2520tasks%2520pretty%2520well.%2520Thus%2520we%2520unfreeze%2520these%250Aparameters%2520as%2520a%2520fine-tuning%2520part.%2520At%2520the%2520same%2520time%252C%2520we%2520review%2520the%2520existing%250Aprompt%2520and%2520adapter%2520tuning%2520methods%252C%2520proposing%2520a%2520fresh%2520way%2520of%2520prompts%2520and%250Asynthesizing%2520them%2520with%2520adapters%2520as%2520dynamic%2520adjustments.%2520Our%2520Proposed%2520method%2520of%250APEFT%2520tasks%252C%2520namely%2520PPT%252C%2520with%2520only%25201.05%2525%2520of%2520parameters%2520for%2520training%252C%2520gets%250Astate-of-the-art%2520results%2520in%2520several%2520mainstream%2520datasets%252C%2520such%2520as%252095.01%2525%250Aaccuracy%2520in%2520the%2520ScanObjectNN%2520OBJ_BG%2520dataset.%2520Codes%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/zsc000722/PPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Positional%20Prompt%20Tuning%20for%20Efficient%203D%20Representation%20Learning&entry.906535625=Shaochen%20Zhang%20and%20Zekun%20Qi%20and%20Runpei%20Dong%20and%20Xiuxiu%20Bai%20and%20Xing%20Wei&entry.1292438233=%20%20Point%20cloud%20analysis%20has%20achieved%20significant%20development%20and%20is%0Awell-performed%20in%20multiple%20downstream%20tasks%20like%20point%20cloud%20classification%20and%0Asegmentation%2C%20etc.%20Being%20conscious%20of%20the%20simplicity%20of%20the%20position%20encoding%0Astructure%20in%20Transformer-based%20architectures%2C%20we%20attach%20importance%20to%20the%0Aposition%20encoding%20as%20a%20high-dimensional%20part%20and%20the%20patch%20encoder%20to%20offer%0Amulti-scale%20information.%20Together%20with%20the%20sequential%20Transformer%2C%20the%20whole%0Amodule%20with%20position%20encoding%20comprehensively%20constructs%20a%20multi-scale%20feature%0Aabstraction%20module%20that%20considers%20both%20the%20local%20parts%20from%20the%20patch%20and%20the%0Aglobal%20parts%20from%20center%20points%20as%20position%20encoding.%20With%20only%20a%20few%0Aparameters%2C%20the%20position%20embedding%20module%20fits%20the%20setting%20of%20PEFT%0A%28Parameter-Efficient%20Fine-Tuning%29%20tasks%20pretty%20well.%20Thus%20we%20unfreeze%20these%0Aparameters%20as%20a%20fine-tuning%20part.%20At%20the%20same%20time%2C%20we%20review%20the%20existing%0Aprompt%20and%20adapter%20tuning%20methods%2C%20proposing%20a%20fresh%20way%20of%20prompts%20and%0Asynthesizing%20them%20with%20adapters%20as%20dynamic%20adjustments.%20Our%20Proposed%20method%20of%0APEFT%20tasks%2C%20namely%20PPT%2C%20with%20only%201.05%25%20of%20parameters%20for%20training%2C%20gets%0Astate-of-the-art%20results%20in%20several%20mainstream%20datasets%2C%20such%20as%2095.01%25%0Aaccuracy%20in%20the%20ScanObjectNN%20OBJ_BG%20dataset.%20Codes%20will%20be%20released%20at%0Ahttps%3A//github.com/zsc000722/PPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11567v1&entry.124074799=Read"},
{"title": "Unfolded proximal neural networks for robust image Gaussian denoising", "author": "Hoang Trieu Vy Le and Audrey Repetti and Nelly Pustelnik", "abstract": "  A common approach to solve inverse imaging problems relies on finding a\nmaximum a posteriori (MAP) estimate of the original unknown image, by solving a\nminimization problem. In thiscontext, iterative proximal algorithms are widely\nused, enabling to handle non-smooth functions and linear operators. Recently,\nthese algorithms have been paired with deep learning strategies, to further\nimprove the estimate quality. In particular, proximal neural networks (PNNs)\nhave been introduced, obtained by unrolling a proximal algorithm as for finding\na MAP estimate, but over a fixed number of iterations, with learned linear\noperators and parameters. As PNNs are based on optimization theory, they are\nvery flexible, and can be adapted to any image restoration task, as soon as a\nproximal algorithm can solve it. They further have much lighter architectures\nthan traditional networks. In this article we propose a unified framework to\nbuild PNNs for the Gaussian denoising task, based on both the dual-FB and the\nprimal-dual Chambolle-Pock algorithms. We further show that accelerated\ninertial versions of these algorithms enable skip connections in the associated\nNN layers. We propose different learning strategies for our PNN framework, and\ninvestigate their robustness (Lipschitz property) and denoising efficiency.\nFinally, we assess the robustness of our PNNs when plugged in a\nforward-backward algorithm for an image deblurring problem.\n", "link": "http://arxiv.org/abs/2308.03139v2", "date": "2024-08-21", "relevancy": 2.6922, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5402}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unfolded%20proximal%20neural%20networks%20for%20robust%20image%20Gaussian%20denoising&body=Title%3A%20Unfolded%20proximal%20neural%20networks%20for%20robust%20image%20Gaussian%20denoising%0AAuthor%3A%20Hoang%20Trieu%20Vy%20Le%20and%20Audrey%20Repetti%20and%20Nelly%20Pustelnik%0AAbstract%3A%20%20%20A%20common%20approach%20to%20solve%20inverse%20imaging%20problems%20relies%20on%20finding%20a%0Amaximum%20a%20posteriori%20%28MAP%29%20estimate%20of%20the%20original%20unknown%20image%2C%20by%20solving%20a%0Aminimization%20problem.%20In%20thiscontext%2C%20iterative%20proximal%20algorithms%20are%20widely%0Aused%2C%20enabling%20to%20handle%20non-smooth%20functions%20and%20linear%20operators.%20Recently%2C%0Athese%20algorithms%20have%20been%20paired%20with%20deep%20learning%20strategies%2C%20to%20further%0Aimprove%20the%20estimate%20quality.%20In%20particular%2C%20proximal%20neural%20networks%20%28PNNs%29%0Ahave%20been%20introduced%2C%20obtained%20by%20unrolling%20a%20proximal%20algorithm%20as%20for%20finding%0Aa%20MAP%20estimate%2C%20but%20over%20a%20fixed%20number%20of%20iterations%2C%20with%20learned%20linear%0Aoperators%20and%20parameters.%20As%20PNNs%20are%20based%20on%20optimization%20theory%2C%20they%20are%0Avery%20flexible%2C%20and%20can%20be%20adapted%20to%20any%20image%20restoration%20task%2C%20as%20soon%20as%20a%0Aproximal%20algorithm%20can%20solve%20it.%20They%20further%20have%20much%20lighter%20architectures%0Athan%20traditional%20networks.%20In%20this%20article%20we%20propose%20a%20unified%20framework%20to%0Abuild%20PNNs%20for%20the%20Gaussian%20denoising%20task%2C%20based%20on%20both%20the%20dual-FB%20and%20the%0Aprimal-dual%20Chambolle-Pock%20algorithms.%20We%20further%20show%20that%20accelerated%0Ainertial%20versions%20of%20these%20algorithms%20enable%20skip%20connections%20in%20the%20associated%0ANN%20layers.%20We%20propose%20different%20learning%20strategies%20for%20our%20PNN%20framework%2C%20and%0Ainvestigate%20their%20robustness%20%28Lipschitz%20property%29%20and%20denoising%20efficiency.%0AFinally%2C%20we%20assess%20the%20robustness%20of%20our%20PNNs%20when%20plugged%20in%20a%0Aforward-backward%20algorithm%20for%20an%20image%20deblurring%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.03139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnfolded%2520proximal%2520neural%2520networks%2520for%2520robust%2520image%2520Gaussian%2520denoising%26entry.906535625%3DHoang%2520Trieu%2520Vy%2520Le%2520and%2520Audrey%2520Repetti%2520and%2520Nelly%2520Pustelnik%26entry.1292438233%3D%2520%2520A%2520common%2520approach%2520to%2520solve%2520inverse%2520imaging%2520problems%2520relies%2520on%2520finding%2520a%250Amaximum%2520a%2520posteriori%2520%2528MAP%2529%2520estimate%2520of%2520the%2520original%2520unknown%2520image%252C%2520by%2520solving%2520a%250Aminimization%2520problem.%2520In%2520thiscontext%252C%2520iterative%2520proximal%2520algorithms%2520are%2520widely%250Aused%252C%2520enabling%2520to%2520handle%2520non-smooth%2520functions%2520and%2520linear%2520operators.%2520Recently%252C%250Athese%2520algorithms%2520have%2520been%2520paired%2520with%2520deep%2520learning%2520strategies%252C%2520to%2520further%250Aimprove%2520the%2520estimate%2520quality.%2520In%2520particular%252C%2520proximal%2520neural%2520networks%2520%2528PNNs%2529%250Ahave%2520been%2520introduced%252C%2520obtained%2520by%2520unrolling%2520a%2520proximal%2520algorithm%2520as%2520for%2520finding%250Aa%2520MAP%2520estimate%252C%2520but%2520over%2520a%2520fixed%2520number%2520of%2520iterations%252C%2520with%2520learned%2520linear%250Aoperators%2520and%2520parameters.%2520As%2520PNNs%2520are%2520based%2520on%2520optimization%2520theory%252C%2520they%2520are%250Avery%2520flexible%252C%2520and%2520can%2520be%2520adapted%2520to%2520any%2520image%2520restoration%2520task%252C%2520as%2520soon%2520as%2520a%250Aproximal%2520algorithm%2520can%2520solve%2520it.%2520They%2520further%2520have%2520much%2520lighter%2520architectures%250Athan%2520traditional%2520networks.%2520In%2520this%2520article%2520we%2520propose%2520a%2520unified%2520framework%2520to%250Abuild%2520PNNs%2520for%2520the%2520Gaussian%2520denoising%2520task%252C%2520based%2520on%2520both%2520the%2520dual-FB%2520and%2520the%250Aprimal-dual%2520Chambolle-Pock%2520algorithms.%2520We%2520further%2520show%2520that%2520accelerated%250Ainertial%2520versions%2520of%2520these%2520algorithms%2520enable%2520skip%2520connections%2520in%2520the%2520associated%250ANN%2520layers.%2520We%2520propose%2520different%2520learning%2520strategies%2520for%2520our%2520PNN%2520framework%252C%2520and%250Ainvestigate%2520their%2520robustness%2520%2528Lipschitz%2520property%2529%2520and%2520denoising%2520efficiency.%250AFinally%252C%2520we%2520assess%2520the%2520robustness%2520of%2520our%2520PNNs%2520when%2520plugged%2520in%2520a%250Aforward-backward%2520algorithm%2520for%2520an%2520image%2520deblurring%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.03139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unfolded%20proximal%20neural%20networks%20for%20robust%20image%20Gaussian%20denoising&entry.906535625=Hoang%20Trieu%20Vy%20Le%20and%20Audrey%20Repetti%20and%20Nelly%20Pustelnik&entry.1292438233=%20%20A%20common%20approach%20to%20solve%20inverse%20imaging%20problems%20relies%20on%20finding%20a%0Amaximum%20a%20posteriori%20%28MAP%29%20estimate%20of%20the%20original%20unknown%20image%2C%20by%20solving%20a%0Aminimization%20problem.%20In%20thiscontext%2C%20iterative%20proximal%20algorithms%20are%20widely%0Aused%2C%20enabling%20to%20handle%20non-smooth%20functions%20and%20linear%20operators.%20Recently%2C%0Athese%20algorithms%20have%20been%20paired%20with%20deep%20learning%20strategies%2C%20to%20further%0Aimprove%20the%20estimate%20quality.%20In%20particular%2C%20proximal%20neural%20networks%20%28PNNs%29%0Ahave%20been%20introduced%2C%20obtained%20by%20unrolling%20a%20proximal%20algorithm%20as%20for%20finding%0Aa%20MAP%20estimate%2C%20but%20over%20a%20fixed%20number%20of%20iterations%2C%20with%20learned%20linear%0Aoperators%20and%20parameters.%20As%20PNNs%20are%20based%20on%20optimization%20theory%2C%20they%20are%0Avery%20flexible%2C%20and%20can%20be%20adapted%20to%20any%20image%20restoration%20task%2C%20as%20soon%20as%20a%0Aproximal%20algorithm%20can%20solve%20it.%20They%20further%20have%20much%20lighter%20architectures%0Athan%20traditional%20networks.%20In%20this%20article%20we%20propose%20a%20unified%20framework%20to%0Abuild%20PNNs%20for%20the%20Gaussian%20denoising%20task%2C%20based%20on%20both%20the%20dual-FB%20and%20the%0Aprimal-dual%20Chambolle-Pock%20algorithms.%20We%20further%20show%20that%20accelerated%0Ainertial%20versions%20of%20these%20algorithms%20enable%20skip%20connections%20in%20the%20associated%0ANN%20layers.%20We%20propose%20different%20learning%20strategies%20for%20our%20PNN%20framework%2C%20and%0Ainvestigate%20their%20robustness%20%28Lipschitz%20property%29%20and%20denoising%20efficiency.%0AFinally%2C%20we%20assess%20the%20robustness%20of%20our%20PNNs%20when%20plugged%20in%20a%0Aforward-backward%20algorithm%20for%20an%20image%20deblurring%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.03139v2&entry.124074799=Read"},
{"title": "HYVE: Hybrid Vertex Encoder for Neural Distance Fields", "author": "Stefan Rhys Jeske and Jonathan Klein and Dominik L. Michels and Jan Bender", "abstract": "  Neural shape representation generally refers to representing 3D geometry\nusing neural networks, e.g., computing a signed distance or occupancy value at\na specific spatial position. In this paper we present a neural-network\narchitecture suitable for accurate encoding of 3D shapes in a single forward\npass. Our architecture is based on a multi-scale hybrid system incorporating\ngraph-based and voxel-based components, as well as a continuously\ndifferentiable decoder. The hybrid system includes a novel way of voxelizing\npoint-based features in neural networks, which we show can be used in\ncombination with oriented point-clouds to obtain smoother and more detailed\nreconstructions. Furthermore, our network is trained to solve the eikonal\nequation and only requires knowledge of the zero-level set for training and\ninference. This means that in contrast to most previous shape encoder\narchitectures, our network is able to output valid signed distance fields\nwithout explicit prior knowledge of non-zero distance values or shape\noccupancy. It also requires only a single forward-pass, instead of the\nlatent-code optimization used in auto-decoder methods. We further propose a\nmodification to the loss function in case that surface normals are not well\ndefined, e.g., in the context of non-watertight surfaces and non-manifold\ngeometry, resulting in an unsigned distance field. Overall, our system can help\nto reduce the computational overhead of training and evaluating neural distance\nfields, as well as enabling the application to difficult geometry.\n", "link": "http://arxiv.org/abs/2310.06644v3", "date": "2024-08-21", "relevancy": 2.6904, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5676}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HYVE%3A%20Hybrid%20Vertex%20Encoder%20for%20Neural%20Distance%20Fields&body=Title%3A%20HYVE%3A%20Hybrid%20Vertex%20Encoder%20for%20Neural%20Distance%20Fields%0AAuthor%3A%20Stefan%20Rhys%20Jeske%20and%20Jonathan%20Klein%20and%20Dominik%20L.%20Michels%20and%20Jan%20Bender%0AAbstract%3A%20%20%20Neural%20shape%20representation%20generally%20refers%20to%20representing%203D%20geometry%0Ausing%20neural%20networks%2C%20e.g.%2C%20computing%20a%20signed%20distance%20or%20occupancy%20value%20at%0Aa%20specific%20spatial%20position.%20In%20this%20paper%20we%20present%20a%20neural-network%0Aarchitecture%20suitable%20for%20accurate%20encoding%20of%203D%20shapes%20in%20a%20single%20forward%0Apass.%20Our%20architecture%20is%20based%20on%20a%20multi-scale%20hybrid%20system%20incorporating%0Agraph-based%20and%20voxel-based%20components%2C%20as%20well%20as%20a%20continuously%0Adifferentiable%20decoder.%20The%20hybrid%20system%20includes%20a%20novel%20way%20of%20voxelizing%0Apoint-based%20features%20in%20neural%20networks%2C%20which%20we%20show%20can%20be%20used%20in%0Acombination%20with%20oriented%20point-clouds%20to%20obtain%20smoother%20and%20more%20detailed%0Areconstructions.%20Furthermore%2C%20our%20network%20is%20trained%20to%20solve%20the%20eikonal%0Aequation%20and%20only%20requires%20knowledge%20of%20the%20zero-level%20set%20for%20training%20and%0Ainference.%20This%20means%20that%20in%20contrast%20to%20most%20previous%20shape%20encoder%0Aarchitectures%2C%20our%20network%20is%20able%20to%20output%20valid%20signed%20distance%20fields%0Awithout%20explicit%20prior%20knowledge%20of%20non-zero%20distance%20values%20or%20shape%0Aoccupancy.%20It%20also%20requires%20only%20a%20single%20forward-pass%2C%20instead%20of%20the%0Alatent-code%20optimization%20used%20in%20auto-decoder%20methods.%20We%20further%20propose%20a%0Amodification%20to%20the%20loss%20function%20in%20case%20that%20surface%20normals%20are%20not%20well%0Adefined%2C%20e.g.%2C%20in%20the%20context%20of%20non-watertight%20surfaces%20and%20non-manifold%0Ageometry%2C%20resulting%20in%20an%20unsigned%20distance%20field.%20Overall%2C%20our%20system%20can%20help%0Ato%20reduce%20the%20computational%20overhead%20of%20training%20and%20evaluating%20neural%20distance%0Afields%2C%20as%20well%20as%20enabling%20the%20application%20to%20difficult%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06644v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHYVE%253A%2520Hybrid%2520Vertex%2520Encoder%2520for%2520Neural%2520Distance%2520Fields%26entry.906535625%3DStefan%2520Rhys%2520Jeske%2520and%2520Jonathan%2520Klein%2520and%2520Dominik%2520L.%2520Michels%2520and%2520Jan%2520Bender%26entry.1292438233%3D%2520%2520Neural%2520shape%2520representation%2520generally%2520refers%2520to%2520representing%25203D%2520geometry%250Ausing%2520neural%2520networks%252C%2520e.g.%252C%2520computing%2520a%2520signed%2520distance%2520or%2520occupancy%2520value%2520at%250Aa%2520specific%2520spatial%2520position.%2520In%2520this%2520paper%2520we%2520present%2520a%2520neural-network%250Aarchitecture%2520suitable%2520for%2520accurate%2520encoding%2520of%25203D%2520shapes%2520in%2520a%2520single%2520forward%250Apass.%2520Our%2520architecture%2520is%2520based%2520on%2520a%2520multi-scale%2520hybrid%2520system%2520incorporating%250Agraph-based%2520and%2520voxel-based%2520components%252C%2520as%2520well%2520as%2520a%2520continuously%250Adifferentiable%2520decoder.%2520The%2520hybrid%2520system%2520includes%2520a%2520novel%2520way%2520of%2520voxelizing%250Apoint-based%2520features%2520in%2520neural%2520networks%252C%2520which%2520we%2520show%2520can%2520be%2520used%2520in%250Acombination%2520with%2520oriented%2520point-clouds%2520to%2520obtain%2520smoother%2520and%2520more%2520detailed%250Areconstructions.%2520Furthermore%252C%2520our%2520network%2520is%2520trained%2520to%2520solve%2520the%2520eikonal%250Aequation%2520and%2520only%2520requires%2520knowledge%2520of%2520the%2520zero-level%2520set%2520for%2520training%2520and%250Ainference.%2520This%2520means%2520that%2520in%2520contrast%2520to%2520most%2520previous%2520shape%2520encoder%250Aarchitectures%252C%2520our%2520network%2520is%2520able%2520to%2520output%2520valid%2520signed%2520distance%2520fields%250Awithout%2520explicit%2520prior%2520knowledge%2520of%2520non-zero%2520distance%2520values%2520or%2520shape%250Aoccupancy.%2520It%2520also%2520requires%2520only%2520a%2520single%2520forward-pass%252C%2520instead%2520of%2520the%250Alatent-code%2520optimization%2520used%2520in%2520auto-decoder%2520methods.%2520We%2520further%2520propose%2520a%250Amodification%2520to%2520the%2520loss%2520function%2520in%2520case%2520that%2520surface%2520normals%2520are%2520not%2520well%250Adefined%252C%2520e.g.%252C%2520in%2520the%2520context%2520of%2520non-watertight%2520surfaces%2520and%2520non-manifold%250Ageometry%252C%2520resulting%2520in%2520an%2520unsigned%2520distance%2520field.%2520Overall%252C%2520our%2520system%2520can%2520help%250Ato%2520reduce%2520the%2520computational%2520overhead%2520of%2520training%2520and%2520evaluating%2520neural%2520distance%250Afields%252C%2520as%2520well%2520as%2520enabling%2520the%2520application%2520to%2520difficult%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06644v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HYVE%3A%20Hybrid%20Vertex%20Encoder%20for%20Neural%20Distance%20Fields&entry.906535625=Stefan%20Rhys%20Jeske%20and%20Jonathan%20Klein%20and%20Dominik%20L.%20Michels%20and%20Jan%20Bender&entry.1292438233=%20%20Neural%20shape%20representation%20generally%20refers%20to%20representing%203D%20geometry%0Ausing%20neural%20networks%2C%20e.g.%2C%20computing%20a%20signed%20distance%20or%20occupancy%20value%20at%0Aa%20specific%20spatial%20position.%20In%20this%20paper%20we%20present%20a%20neural-network%0Aarchitecture%20suitable%20for%20accurate%20encoding%20of%203D%20shapes%20in%20a%20single%20forward%0Apass.%20Our%20architecture%20is%20based%20on%20a%20multi-scale%20hybrid%20system%20incorporating%0Agraph-based%20and%20voxel-based%20components%2C%20as%20well%20as%20a%20continuously%0Adifferentiable%20decoder.%20The%20hybrid%20system%20includes%20a%20novel%20way%20of%20voxelizing%0Apoint-based%20features%20in%20neural%20networks%2C%20which%20we%20show%20can%20be%20used%20in%0Acombination%20with%20oriented%20point-clouds%20to%20obtain%20smoother%20and%20more%20detailed%0Areconstructions.%20Furthermore%2C%20our%20network%20is%20trained%20to%20solve%20the%20eikonal%0Aequation%20and%20only%20requires%20knowledge%20of%20the%20zero-level%20set%20for%20training%20and%0Ainference.%20This%20means%20that%20in%20contrast%20to%20most%20previous%20shape%20encoder%0Aarchitectures%2C%20our%20network%20is%20able%20to%20output%20valid%20signed%20distance%20fields%0Awithout%20explicit%20prior%20knowledge%20of%20non-zero%20distance%20values%20or%20shape%0Aoccupancy.%20It%20also%20requires%20only%20a%20single%20forward-pass%2C%20instead%20of%20the%0Alatent-code%20optimization%20used%20in%20auto-decoder%20methods.%20We%20further%20propose%20a%0Amodification%20to%20the%20loss%20function%20in%20case%20that%20surface%20normals%20are%20not%20well%0Adefined%2C%20e.g.%2C%20in%20the%20context%20of%20non-watertight%20surfaces%20and%20non-manifold%0Ageometry%2C%20resulting%20in%20an%20unsigned%20distance%20field.%20Overall%2C%20our%20system%20can%20help%0Ato%20reduce%20the%20computational%20overhead%20of%20training%20and%20evaluating%20neural%20distance%0Afields%2C%20as%20well%20as%20enabling%20the%20application%20to%20difficult%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06644v3&entry.124074799=Read"},
{"title": "The Tug-of-War Between Deepfake Generation and Detection", "author": "Hannah Lee and Changyeon Lee and Kevin Farhat and Lin Qiu and Steve Geluso and Aerin Kim and Oren Etzioni", "abstract": "  Multimodal generative models are rapidly evolving, leading to a surge in the\ngeneration of realistic video and audio that offers exciting possibilities but\nalso serious risks. Deepfake videos, which can convincingly impersonate\nindividuals, have particularly garnered attention due to their potential misuse\nin spreading misinformation and creating fraudulent content. This survey paper\nexamines the dual landscape of deepfake video generation and detection,\nemphasizing the need for effective countermeasures against potential abuses. We\nprovide a comprehensive overview of current deepfake generation techniques,\nincluding face swapping, reenactment, and audio-driven animation, which\nleverage cutting-edge technologies like GANs and diffusion models to produce\nhighly realistic fake videos. Additionally, we analyze various detection\napproaches designed to differentiate authentic from altered videos, from\ndetecting visual artifacts to deploying advanced algorithms that pinpoint\ninconsistencies across video and audio signals.\n  The effectiveness of these detection methods heavily relies on the diversity\nand quality of datasets used for training and evaluation. We discuss the\nevolution of deepfake datasets, highlighting the importance of robust, diverse,\nand frequently updated collections to enhance the detection accuracy and\ngeneralizability. As deepfakes become increasingly indistinguishable from\nauthentic content, developing advanced detection techniques that can keep pace\nwith generation technologies is crucial. We advocate for a proactive approach\nin the \"tug-of-war\" between deepfake creators and detectors, emphasizing the\nneed for continuous research collaboration, standardization of evaluation\nmetrics, and the creation of comprehensive benchmarks.\n", "link": "http://arxiv.org/abs/2407.06174v4", "date": "2024-08-21", "relevancy": 2.6792, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5559}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.534}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Tug-of-War%20Between%20Deepfake%20Generation%20and%20Detection&body=Title%3A%20The%20Tug-of-War%20Between%20Deepfake%20Generation%20and%20Detection%0AAuthor%3A%20Hannah%20Lee%20and%20Changyeon%20Lee%20and%20Kevin%20Farhat%20and%20Lin%20Qiu%20and%20Steve%20Geluso%20and%20Aerin%20Kim%20and%20Oren%20Etzioni%0AAbstract%3A%20%20%20Multimodal%20generative%20models%20are%20rapidly%20evolving%2C%20leading%20to%20a%20surge%20in%20the%0Ageneration%20of%20realistic%20video%20and%20audio%20that%20offers%20exciting%20possibilities%20but%0Aalso%20serious%20risks.%20Deepfake%20videos%2C%20which%20can%20convincingly%20impersonate%0Aindividuals%2C%20have%20particularly%20garnered%20attention%20due%20to%20their%20potential%20misuse%0Ain%20spreading%20misinformation%20and%20creating%20fraudulent%20content.%20This%20survey%20paper%0Aexamines%20the%20dual%20landscape%20of%20deepfake%20video%20generation%20and%20detection%2C%0Aemphasizing%20the%20need%20for%20effective%20countermeasures%20against%20potential%20abuses.%20We%0Aprovide%20a%20comprehensive%20overview%20of%20current%20deepfake%20generation%20techniques%2C%0Aincluding%20face%20swapping%2C%20reenactment%2C%20and%20audio-driven%20animation%2C%20which%0Aleverage%20cutting-edge%20technologies%20like%20GANs%20and%20diffusion%20models%20to%20produce%0Ahighly%20realistic%20fake%20videos.%20Additionally%2C%20we%20analyze%20various%20detection%0Aapproaches%20designed%20to%20differentiate%20authentic%20from%20altered%20videos%2C%20from%0Adetecting%20visual%20artifacts%20to%20deploying%20advanced%20algorithms%20that%20pinpoint%0Ainconsistencies%20across%20video%20and%20audio%20signals.%0A%20%20The%20effectiveness%20of%20these%20detection%20methods%20heavily%20relies%20on%20the%20diversity%0Aand%20quality%20of%20datasets%20used%20for%20training%20and%20evaluation.%20We%20discuss%20the%0Aevolution%20of%20deepfake%20datasets%2C%20highlighting%20the%20importance%20of%20robust%2C%20diverse%2C%0Aand%20frequently%20updated%20collections%20to%20enhance%20the%20detection%20accuracy%20and%0Ageneralizability.%20As%20deepfakes%20become%20increasingly%20indistinguishable%20from%0Aauthentic%20content%2C%20developing%20advanced%20detection%20techniques%20that%20can%20keep%20pace%0Awith%20generation%20technologies%20is%20crucial.%20We%20advocate%20for%20a%20proactive%20approach%0Ain%20the%20%22tug-of-war%22%20between%20deepfake%20creators%20and%20detectors%2C%20emphasizing%20the%0Aneed%20for%20continuous%20research%20collaboration%2C%20standardization%20of%20evaluation%0Ametrics%2C%20and%20the%20creation%20of%20comprehensive%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06174v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Tug-of-War%2520Between%2520Deepfake%2520Generation%2520and%2520Detection%26entry.906535625%3DHannah%2520Lee%2520and%2520Changyeon%2520Lee%2520and%2520Kevin%2520Farhat%2520and%2520Lin%2520Qiu%2520and%2520Steve%2520Geluso%2520and%2520Aerin%2520Kim%2520and%2520Oren%2520Etzioni%26entry.1292438233%3D%2520%2520Multimodal%2520generative%2520models%2520are%2520rapidly%2520evolving%252C%2520leading%2520to%2520a%2520surge%2520in%2520the%250Ageneration%2520of%2520realistic%2520video%2520and%2520audio%2520that%2520offers%2520exciting%2520possibilities%2520but%250Aalso%2520serious%2520risks.%2520Deepfake%2520videos%252C%2520which%2520can%2520convincingly%2520impersonate%250Aindividuals%252C%2520have%2520particularly%2520garnered%2520attention%2520due%2520to%2520their%2520potential%2520misuse%250Ain%2520spreading%2520misinformation%2520and%2520creating%2520fraudulent%2520content.%2520This%2520survey%2520paper%250Aexamines%2520the%2520dual%2520landscape%2520of%2520deepfake%2520video%2520generation%2520and%2520detection%252C%250Aemphasizing%2520the%2520need%2520for%2520effective%2520countermeasures%2520against%2520potential%2520abuses.%2520We%250Aprovide%2520a%2520comprehensive%2520overview%2520of%2520current%2520deepfake%2520generation%2520techniques%252C%250Aincluding%2520face%2520swapping%252C%2520reenactment%252C%2520and%2520audio-driven%2520animation%252C%2520which%250Aleverage%2520cutting-edge%2520technologies%2520like%2520GANs%2520and%2520diffusion%2520models%2520to%2520produce%250Ahighly%2520realistic%2520fake%2520videos.%2520Additionally%252C%2520we%2520analyze%2520various%2520detection%250Aapproaches%2520designed%2520to%2520differentiate%2520authentic%2520from%2520altered%2520videos%252C%2520from%250Adetecting%2520visual%2520artifacts%2520to%2520deploying%2520advanced%2520algorithms%2520that%2520pinpoint%250Ainconsistencies%2520across%2520video%2520and%2520audio%2520signals.%250A%2520%2520The%2520effectiveness%2520of%2520these%2520detection%2520methods%2520heavily%2520relies%2520on%2520the%2520diversity%250Aand%2520quality%2520of%2520datasets%2520used%2520for%2520training%2520and%2520evaluation.%2520We%2520discuss%2520the%250Aevolution%2520of%2520deepfake%2520datasets%252C%2520highlighting%2520the%2520importance%2520of%2520robust%252C%2520diverse%252C%250Aand%2520frequently%2520updated%2520collections%2520to%2520enhance%2520the%2520detection%2520accuracy%2520and%250Ageneralizability.%2520As%2520deepfakes%2520become%2520increasingly%2520indistinguishable%2520from%250Aauthentic%2520content%252C%2520developing%2520advanced%2520detection%2520techniques%2520that%2520can%2520keep%2520pace%250Awith%2520generation%2520technologies%2520is%2520crucial.%2520We%2520advocate%2520for%2520a%2520proactive%2520approach%250Ain%2520the%2520%2522tug-of-war%2522%2520between%2520deepfake%2520creators%2520and%2520detectors%252C%2520emphasizing%2520the%250Aneed%2520for%2520continuous%2520research%2520collaboration%252C%2520standardization%2520of%2520evaluation%250Ametrics%252C%2520and%2520the%2520creation%2520of%2520comprehensive%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06174v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Tug-of-War%20Between%20Deepfake%20Generation%20and%20Detection&entry.906535625=Hannah%20Lee%20and%20Changyeon%20Lee%20and%20Kevin%20Farhat%20and%20Lin%20Qiu%20and%20Steve%20Geluso%20and%20Aerin%20Kim%20and%20Oren%20Etzioni&entry.1292438233=%20%20Multimodal%20generative%20models%20are%20rapidly%20evolving%2C%20leading%20to%20a%20surge%20in%20the%0Ageneration%20of%20realistic%20video%20and%20audio%20that%20offers%20exciting%20possibilities%20but%0Aalso%20serious%20risks.%20Deepfake%20videos%2C%20which%20can%20convincingly%20impersonate%0Aindividuals%2C%20have%20particularly%20garnered%20attention%20due%20to%20their%20potential%20misuse%0Ain%20spreading%20misinformation%20and%20creating%20fraudulent%20content.%20This%20survey%20paper%0Aexamines%20the%20dual%20landscape%20of%20deepfake%20video%20generation%20and%20detection%2C%0Aemphasizing%20the%20need%20for%20effective%20countermeasures%20against%20potential%20abuses.%20We%0Aprovide%20a%20comprehensive%20overview%20of%20current%20deepfake%20generation%20techniques%2C%0Aincluding%20face%20swapping%2C%20reenactment%2C%20and%20audio-driven%20animation%2C%20which%0Aleverage%20cutting-edge%20technologies%20like%20GANs%20and%20diffusion%20models%20to%20produce%0Ahighly%20realistic%20fake%20videos.%20Additionally%2C%20we%20analyze%20various%20detection%0Aapproaches%20designed%20to%20differentiate%20authentic%20from%20altered%20videos%2C%20from%0Adetecting%20visual%20artifacts%20to%20deploying%20advanced%20algorithms%20that%20pinpoint%0Ainconsistencies%20across%20video%20and%20audio%20signals.%0A%20%20The%20effectiveness%20of%20these%20detection%20methods%20heavily%20relies%20on%20the%20diversity%0Aand%20quality%20of%20datasets%20used%20for%20training%20and%20evaluation.%20We%20discuss%20the%0Aevolution%20of%20deepfake%20datasets%2C%20highlighting%20the%20importance%20of%20robust%2C%20diverse%2C%0Aand%20frequently%20updated%20collections%20to%20enhance%20the%20detection%20accuracy%20and%0Ageneralizability.%20As%20deepfakes%20become%20increasingly%20indistinguishable%20from%0Aauthentic%20content%2C%20developing%20advanced%20detection%20techniques%20that%20can%20keep%20pace%0Awith%20generation%20technologies%20is%20crucial.%20We%20advocate%20for%20a%20proactive%20approach%0Ain%20the%20%22tug-of-war%22%20between%20deepfake%20creators%20and%20detectors%2C%20emphasizing%20the%0Aneed%20for%20continuous%20research%20collaboration%2C%20standardization%20of%20evaluation%0Ametrics%2C%20and%20the%20creation%20of%20comprehensive%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06174v4&entry.124074799=Read"},
{"title": "Source-Free Domain Adaptation Guided by Vision and Vision-Language\n  Pre-Training", "author": "Wenyu Zhang and Li Shen and Chuan-Sheng Foo", "abstract": "  Source-free domain adaptation (SFDA) aims to adapt a source model trained on\na fully-labeled source domain to a related but unlabeled target domain. While\nthe source model is a key avenue for acquiring target pseudolabels, the\ngenerated pseudolabels may exhibit source bias. In the conventional SFDA\npipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to\ninitialize the source model at the start of source training, and subsequently\ndiscarded. Despite having diverse features important for generalization, the\npre-trained feature extractor can overfit to the source data distribution\nduring source training and forget relevant target domain knowledge. Rather than\ndiscarding this valuable knowledge, we introduce an integrated framework to\nincorporate pre-trained networks into the target adaptation process. The\nproposed framework is flexible and allows us to plug modern pre-trained\nnetworks into the adaptation process to leverage their stronger representation\nlearning capabilities. For adaptation, we propose the Co-learn algorithm to\nimprove target pseudolabel quality collaboratively through the source model and\na pre-trained feature extractor. Building on the recent success of the\nvision-language model CLIP in zero-shot image recognition, we present an\nextension Co-learn++ to further incorporate CLIP's zero-shot classification\ndecisions. We evaluate on 4 benchmark datasets and include more challenging\nscenarios such as open-set, partial-set and open-partial SFDA. Experimental\nresults demonstrate that our proposed strategy improves adaptation performance\nand can be successfully integrated with existing SFDA methods.\n", "link": "http://arxiv.org/abs/2405.02954v2", "date": "2024-08-21", "relevancy": 2.6714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.579}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5165}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source-Free%20Domain%20Adaptation%20Guided%20by%20Vision%20and%20Vision-Language%0A%20%20Pre-Training&body=Title%3A%20Source-Free%20Domain%20Adaptation%20Guided%20by%20Vision%20and%20Vision-Language%0A%20%20Pre-Training%0AAuthor%3A%20Wenyu%20Zhang%20and%20Li%20Shen%20and%20Chuan-Sheng%20Foo%0AAbstract%3A%20%20%20Source-free%20domain%20adaptation%20%28SFDA%29%20aims%20to%20adapt%20a%20source%20model%20trained%20on%0Aa%20fully-labeled%20source%20domain%20to%20a%20related%20but%20unlabeled%20target%20domain.%20While%0Athe%20source%20model%20is%20a%20key%20avenue%20for%20acquiring%20target%20pseudolabels%2C%20the%0Agenerated%20pseudolabels%20may%20exhibit%20source%20bias.%20In%20the%20conventional%20SFDA%0Apipeline%2C%20a%20large%20data%20%28e.g.%20ImageNet%29%20pre-trained%20feature%20extractor%20is%20used%20to%0Ainitialize%20the%20source%20model%20at%20the%20start%20of%20source%20training%2C%20and%20subsequently%0Adiscarded.%20Despite%20having%20diverse%20features%20important%20for%20generalization%2C%20the%0Apre-trained%20feature%20extractor%20can%20overfit%20to%20the%20source%20data%20distribution%0Aduring%20source%20training%20and%20forget%20relevant%20target%20domain%20knowledge.%20Rather%20than%0Adiscarding%20this%20valuable%20knowledge%2C%20we%20introduce%20an%20integrated%20framework%20to%0Aincorporate%20pre-trained%20networks%20into%20the%20target%20adaptation%20process.%20The%0Aproposed%20framework%20is%20flexible%20and%20allows%20us%20to%20plug%20modern%20pre-trained%0Anetworks%20into%20the%20adaptation%20process%20to%20leverage%20their%20stronger%20representation%0Alearning%20capabilities.%20For%20adaptation%2C%20we%20propose%20the%20Co-learn%20algorithm%20to%0Aimprove%20target%20pseudolabel%20quality%20collaboratively%20through%20the%20source%20model%20and%0Aa%20pre-trained%20feature%20extractor.%20Building%20on%20the%20recent%20success%20of%20the%0Avision-language%20model%20CLIP%20in%20zero-shot%20image%20recognition%2C%20we%20present%20an%0Aextension%20Co-learn%2B%2B%20to%20further%20incorporate%20CLIP%27s%20zero-shot%20classification%0Adecisions.%20We%20evaluate%20on%204%20benchmark%20datasets%20and%20include%20more%20challenging%0Ascenarios%20such%20as%20open-set%2C%20partial-set%20and%20open-partial%20SFDA.%20Experimental%0Aresults%20demonstrate%20that%20our%20proposed%20strategy%20improves%20adaptation%20performance%0Aand%20can%20be%20successfully%20integrated%20with%20existing%20SFDA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource-Free%2520Domain%2520Adaptation%2520Guided%2520by%2520Vision%2520and%2520Vision-Language%250A%2520%2520Pre-Training%26entry.906535625%3DWenyu%2520Zhang%2520and%2520Li%2520Shen%2520and%2520Chuan-Sheng%2520Foo%26entry.1292438233%3D%2520%2520Source-free%2520domain%2520adaptation%2520%2528SFDA%2529%2520aims%2520to%2520adapt%2520a%2520source%2520model%2520trained%2520on%250Aa%2520fully-labeled%2520source%2520domain%2520to%2520a%2520related%2520but%2520unlabeled%2520target%2520domain.%2520While%250Athe%2520source%2520model%2520is%2520a%2520key%2520avenue%2520for%2520acquiring%2520target%2520pseudolabels%252C%2520the%250Agenerated%2520pseudolabels%2520may%2520exhibit%2520source%2520bias.%2520In%2520the%2520conventional%2520SFDA%250Apipeline%252C%2520a%2520large%2520data%2520%2528e.g.%2520ImageNet%2529%2520pre-trained%2520feature%2520extractor%2520is%2520used%2520to%250Ainitialize%2520the%2520source%2520model%2520at%2520the%2520start%2520of%2520source%2520training%252C%2520and%2520subsequently%250Adiscarded.%2520Despite%2520having%2520diverse%2520features%2520important%2520for%2520generalization%252C%2520the%250Apre-trained%2520feature%2520extractor%2520can%2520overfit%2520to%2520the%2520source%2520data%2520distribution%250Aduring%2520source%2520training%2520and%2520forget%2520relevant%2520target%2520domain%2520knowledge.%2520Rather%2520than%250Adiscarding%2520this%2520valuable%2520knowledge%252C%2520we%2520introduce%2520an%2520integrated%2520framework%2520to%250Aincorporate%2520pre-trained%2520networks%2520into%2520the%2520target%2520adaptation%2520process.%2520The%250Aproposed%2520framework%2520is%2520flexible%2520and%2520allows%2520us%2520to%2520plug%2520modern%2520pre-trained%250Anetworks%2520into%2520the%2520adaptation%2520process%2520to%2520leverage%2520their%2520stronger%2520representation%250Alearning%2520capabilities.%2520For%2520adaptation%252C%2520we%2520propose%2520the%2520Co-learn%2520algorithm%2520to%250Aimprove%2520target%2520pseudolabel%2520quality%2520collaboratively%2520through%2520the%2520source%2520model%2520and%250Aa%2520pre-trained%2520feature%2520extractor.%2520Building%2520on%2520the%2520recent%2520success%2520of%2520the%250Avision-language%2520model%2520CLIP%2520in%2520zero-shot%2520image%2520recognition%252C%2520we%2520present%2520an%250Aextension%2520Co-learn%252B%252B%2520to%2520further%2520incorporate%2520CLIP%2527s%2520zero-shot%2520classification%250Adecisions.%2520We%2520evaluate%2520on%25204%2520benchmark%2520datasets%2520and%2520include%2520more%2520challenging%250Ascenarios%2520such%2520as%2520open-set%252C%2520partial-set%2520and%2520open-partial%2520SFDA.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520proposed%2520strategy%2520improves%2520adaptation%2520performance%250Aand%2520can%2520be%2520successfully%2520integrated%2520with%2520existing%2520SFDA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source-Free%20Domain%20Adaptation%20Guided%20by%20Vision%20and%20Vision-Language%0A%20%20Pre-Training&entry.906535625=Wenyu%20Zhang%20and%20Li%20Shen%20and%20Chuan-Sheng%20Foo&entry.1292438233=%20%20Source-free%20domain%20adaptation%20%28SFDA%29%20aims%20to%20adapt%20a%20source%20model%20trained%20on%0Aa%20fully-labeled%20source%20domain%20to%20a%20related%20but%20unlabeled%20target%20domain.%20While%0Athe%20source%20model%20is%20a%20key%20avenue%20for%20acquiring%20target%20pseudolabels%2C%20the%0Agenerated%20pseudolabels%20may%20exhibit%20source%20bias.%20In%20the%20conventional%20SFDA%0Apipeline%2C%20a%20large%20data%20%28e.g.%20ImageNet%29%20pre-trained%20feature%20extractor%20is%20used%20to%0Ainitialize%20the%20source%20model%20at%20the%20start%20of%20source%20training%2C%20and%20subsequently%0Adiscarded.%20Despite%20having%20diverse%20features%20important%20for%20generalization%2C%20the%0Apre-trained%20feature%20extractor%20can%20overfit%20to%20the%20source%20data%20distribution%0Aduring%20source%20training%20and%20forget%20relevant%20target%20domain%20knowledge.%20Rather%20than%0Adiscarding%20this%20valuable%20knowledge%2C%20we%20introduce%20an%20integrated%20framework%20to%0Aincorporate%20pre-trained%20networks%20into%20the%20target%20adaptation%20process.%20The%0Aproposed%20framework%20is%20flexible%20and%20allows%20us%20to%20plug%20modern%20pre-trained%0Anetworks%20into%20the%20adaptation%20process%20to%20leverage%20their%20stronger%20representation%0Alearning%20capabilities.%20For%20adaptation%2C%20we%20propose%20the%20Co-learn%20algorithm%20to%0Aimprove%20target%20pseudolabel%20quality%20collaboratively%20through%20the%20source%20model%20and%0Aa%20pre-trained%20feature%20extractor.%20Building%20on%20the%20recent%20success%20of%20the%0Avision-language%20model%20CLIP%20in%20zero-shot%20image%20recognition%2C%20we%20present%20an%0Aextension%20Co-learn%2B%2B%20to%20further%20incorporate%20CLIP%27s%20zero-shot%20classification%0Adecisions.%20We%20evaluate%20on%204%20benchmark%20datasets%20and%20include%20more%20challenging%0Ascenarios%20such%20as%20open-set%2C%20partial-set%20and%20open-partial%20SFDA.%20Experimental%0Aresults%20demonstrate%20that%20our%20proposed%20strategy%20improves%20adaptation%20performance%0Aand%20can%20be%20successfully%20integrated%20with%20existing%20SFDA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02954v2&entry.124074799=Read"},
{"title": "SynPlay: Importing Real-world Diversity for a Synthetic Human Dataset", "author": "Jinsub Yim and Hyungtae Lee and Sungmin Eum and Yi-Ting Shen and Yan Zhang and Heesung Kwon and Shuvra S. Bhattacharyya", "abstract": "  We introduce Synthetic Playground (SynPlay), a new synthetic human dataset\nthat aims to bring out the diversity of human appearance in the real world. We\nfocus on two factors to achieve a level of diversity that has not yet been seen\nin previous works: i) realistic human motions and poses and ii) multiple camera\nviewpoints towards human instances. We first use a game engine and its\nlibrary-provided elementary motions to create games where virtual players can\ntake less-constrained and natural movements while following the game rules\n(i.e., rule-guided motion design as opposed to detail-guided design). We then\naugment the elementary motions with real human motions captured with a motion\ncapture device. To render various human appearances in the games from multiple\nviewpoints, we use seven virtual cameras encompassing the ground and aerial\nviews, capturing abundant aerial-vs-ground and dynamic-vs-static attributes of\nthe scene. Through extensive and carefully-designed experiments, we show that\nusing SynPlay in model training leads to enhanced accuracy over existing\nsynthetic datasets for human detection and segmentation. The benefit of SynPlay\nbecomes even greater for tasks in the data-scarce regime, such as few-shot and\ncross-domain learning tasks. These results clearly demonstrate that SynPlay can\nbe used as an essential dataset with rich attributes of complex human\nappearances and poses suitable for model pretraining. SynPlay dataset\ncomprising over 73k images and 6.5M human instances, is available for download\nat https://synplaydataset.github.io/.\n", "link": "http://arxiv.org/abs/2408.11814v1", "date": "2024-08-21", "relevancy": 2.6566, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5329}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5305}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynPlay%3A%20Importing%20Real-world%20Diversity%20for%20a%20Synthetic%20Human%20Dataset&body=Title%3A%20SynPlay%3A%20Importing%20Real-world%20Diversity%20for%20a%20Synthetic%20Human%20Dataset%0AAuthor%3A%20Jinsub%20Yim%20and%20Hyungtae%20Lee%20and%20Sungmin%20Eum%20and%20Yi-Ting%20Shen%20and%20Yan%20Zhang%20and%20Heesung%20Kwon%20and%20Shuvra%20S.%20Bhattacharyya%0AAbstract%3A%20%20%20We%20introduce%20Synthetic%20Playground%20%28SynPlay%29%2C%20a%20new%20synthetic%20human%20dataset%0Athat%20aims%20to%20bring%20out%20the%20diversity%20of%20human%20appearance%20in%20the%20real%20world.%20We%0Afocus%20on%20two%20factors%20to%20achieve%20a%20level%20of%20diversity%20that%20has%20not%20yet%20been%20seen%0Ain%20previous%20works%3A%20i%29%20realistic%20human%20motions%20and%20poses%20and%20ii%29%20multiple%20camera%0Aviewpoints%20towards%20human%20instances.%20We%20first%20use%20a%20game%20engine%20and%20its%0Alibrary-provided%20elementary%20motions%20to%20create%20games%20where%20virtual%20players%20can%0Atake%20less-constrained%20and%20natural%20movements%20while%20following%20the%20game%20rules%0A%28i.e.%2C%20rule-guided%20motion%20design%20as%20opposed%20to%20detail-guided%20design%29.%20We%20then%0Aaugment%20the%20elementary%20motions%20with%20real%20human%20motions%20captured%20with%20a%20motion%0Acapture%20device.%20To%20render%20various%20human%20appearances%20in%20the%20games%20from%20multiple%0Aviewpoints%2C%20we%20use%20seven%20virtual%20cameras%20encompassing%20the%20ground%20and%20aerial%0Aviews%2C%20capturing%20abundant%20aerial-vs-ground%20and%20dynamic-vs-static%20attributes%20of%0Athe%20scene.%20Through%20extensive%20and%20carefully-designed%20experiments%2C%20we%20show%20that%0Ausing%20SynPlay%20in%20model%20training%20leads%20to%20enhanced%20accuracy%20over%20existing%0Asynthetic%20datasets%20for%20human%20detection%20and%20segmentation.%20The%20benefit%20of%20SynPlay%0Abecomes%20even%20greater%20for%20tasks%20in%20the%20data-scarce%20regime%2C%20such%20as%20few-shot%20and%0Across-domain%20learning%20tasks.%20These%20results%20clearly%20demonstrate%20that%20SynPlay%20can%0Abe%20used%20as%20an%20essential%20dataset%20with%20rich%20attributes%20of%20complex%20human%0Aappearances%20and%20poses%20suitable%20for%20model%20pretraining.%20SynPlay%20dataset%0Acomprising%20over%2073k%20images%20and%206.5M%20human%20instances%2C%20is%20available%20for%20download%0Aat%20https%3A//synplaydataset.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynPlay%253A%2520Importing%2520Real-world%2520Diversity%2520for%2520a%2520Synthetic%2520Human%2520Dataset%26entry.906535625%3DJinsub%2520Yim%2520and%2520Hyungtae%2520Lee%2520and%2520Sungmin%2520Eum%2520and%2520Yi-Ting%2520Shen%2520and%2520Yan%2520Zhang%2520and%2520Heesung%2520Kwon%2520and%2520Shuvra%2520S.%2520Bhattacharyya%26entry.1292438233%3D%2520%2520We%2520introduce%2520Synthetic%2520Playground%2520%2528SynPlay%2529%252C%2520a%2520new%2520synthetic%2520human%2520dataset%250Athat%2520aims%2520to%2520bring%2520out%2520the%2520diversity%2520of%2520human%2520appearance%2520in%2520the%2520real%2520world.%2520We%250Afocus%2520on%2520two%2520factors%2520to%2520achieve%2520a%2520level%2520of%2520diversity%2520that%2520has%2520not%2520yet%2520been%2520seen%250Ain%2520previous%2520works%253A%2520i%2529%2520realistic%2520human%2520motions%2520and%2520poses%2520and%2520ii%2529%2520multiple%2520camera%250Aviewpoints%2520towards%2520human%2520instances.%2520We%2520first%2520use%2520a%2520game%2520engine%2520and%2520its%250Alibrary-provided%2520elementary%2520motions%2520to%2520create%2520games%2520where%2520virtual%2520players%2520can%250Atake%2520less-constrained%2520and%2520natural%2520movements%2520while%2520following%2520the%2520game%2520rules%250A%2528i.e.%252C%2520rule-guided%2520motion%2520design%2520as%2520opposed%2520to%2520detail-guided%2520design%2529.%2520We%2520then%250Aaugment%2520the%2520elementary%2520motions%2520with%2520real%2520human%2520motions%2520captured%2520with%2520a%2520motion%250Acapture%2520device.%2520To%2520render%2520various%2520human%2520appearances%2520in%2520the%2520games%2520from%2520multiple%250Aviewpoints%252C%2520we%2520use%2520seven%2520virtual%2520cameras%2520encompassing%2520the%2520ground%2520and%2520aerial%250Aviews%252C%2520capturing%2520abundant%2520aerial-vs-ground%2520and%2520dynamic-vs-static%2520attributes%2520of%250Athe%2520scene.%2520Through%2520extensive%2520and%2520carefully-designed%2520experiments%252C%2520we%2520show%2520that%250Ausing%2520SynPlay%2520in%2520model%2520training%2520leads%2520to%2520enhanced%2520accuracy%2520over%2520existing%250Asynthetic%2520datasets%2520for%2520human%2520detection%2520and%2520segmentation.%2520The%2520benefit%2520of%2520SynPlay%250Abecomes%2520even%2520greater%2520for%2520tasks%2520in%2520the%2520data-scarce%2520regime%252C%2520such%2520as%2520few-shot%2520and%250Across-domain%2520learning%2520tasks.%2520These%2520results%2520clearly%2520demonstrate%2520that%2520SynPlay%2520can%250Abe%2520used%2520as%2520an%2520essential%2520dataset%2520with%2520rich%2520attributes%2520of%2520complex%2520human%250Aappearances%2520and%2520poses%2520suitable%2520for%2520model%2520pretraining.%2520SynPlay%2520dataset%250Acomprising%2520over%252073k%2520images%2520and%25206.5M%2520human%2520instances%252C%2520is%2520available%2520for%2520download%250Aat%2520https%253A//synplaydataset.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynPlay%3A%20Importing%20Real-world%20Diversity%20for%20a%20Synthetic%20Human%20Dataset&entry.906535625=Jinsub%20Yim%20and%20Hyungtae%20Lee%20and%20Sungmin%20Eum%20and%20Yi-Ting%20Shen%20and%20Yan%20Zhang%20and%20Heesung%20Kwon%20and%20Shuvra%20S.%20Bhattacharyya&entry.1292438233=%20%20We%20introduce%20Synthetic%20Playground%20%28SynPlay%29%2C%20a%20new%20synthetic%20human%20dataset%0Athat%20aims%20to%20bring%20out%20the%20diversity%20of%20human%20appearance%20in%20the%20real%20world.%20We%0Afocus%20on%20two%20factors%20to%20achieve%20a%20level%20of%20diversity%20that%20has%20not%20yet%20been%20seen%0Ain%20previous%20works%3A%20i%29%20realistic%20human%20motions%20and%20poses%20and%20ii%29%20multiple%20camera%0Aviewpoints%20towards%20human%20instances.%20We%20first%20use%20a%20game%20engine%20and%20its%0Alibrary-provided%20elementary%20motions%20to%20create%20games%20where%20virtual%20players%20can%0Atake%20less-constrained%20and%20natural%20movements%20while%20following%20the%20game%20rules%0A%28i.e.%2C%20rule-guided%20motion%20design%20as%20opposed%20to%20detail-guided%20design%29.%20We%20then%0Aaugment%20the%20elementary%20motions%20with%20real%20human%20motions%20captured%20with%20a%20motion%0Acapture%20device.%20To%20render%20various%20human%20appearances%20in%20the%20games%20from%20multiple%0Aviewpoints%2C%20we%20use%20seven%20virtual%20cameras%20encompassing%20the%20ground%20and%20aerial%0Aviews%2C%20capturing%20abundant%20aerial-vs-ground%20and%20dynamic-vs-static%20attributes%20of%0Athe%20scene.%20Through%20extensive%20and%20carefully-designed%20experiments%2C%20we%20show%20that%0Ausing%20SynPlay%20in%20model%20training%20leads%20to%20enhanced%20accuracy%20over%20existing%0Asynthetic%20datasets%20for%20human%20detection%20and%20segmentation.%20The%20benefit%20of%20SynPlay%0Abecomes%20even%20greater%20for%20tasks%20in%20the%20data-scarce%20regime%2C%20such%20as%20few-shot%20and%0Across-domain%20learning%20tasks.%20These%20results%20clearly%20demonstrate%20that%20SynPlay%20can%0Abe%20used%20as%20an%20essential%20dataset%20with%20rich%20attributes%20of%20complex%20human%0Aappearances%20and%20poses%20suitable%20for%20model%20pretraining.%20SynPlay%20dataset%0Acomprising%20over%2073k%20images%20and%206.5M%20human%20instances%2C%20is%20available%20for%20download%0Aat%20https%3A//synplaydataset.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11814v1&entry.124074799=Read"},
{"title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation", "author": "Jianzong Wu and Xiangtai Li and Yanhong Zeng and Jiangning Zhang and Qianyu Zhou and Yining Li and Yunhai Tong and Kai Chen", "abstract": "  In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth\n", "link": "http://arxiv.org/abs/2406.17758v2", "date": "2024-08-21", "relevancy": 2.6494, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6669}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation&body=Title%3A%20MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation%0AAuthor%3A%20Jianzong%20Wu%20and%20Xiangtai%20Li%20and%20Yanhong%20Zeng%20and%20Jiangning%20Zhang%20and%20Qianyu%20Zhou%20and%20Yining%20Li%20and%20Yunhai%20Tong%20and%20Kai%20Chen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20MotionBooth%2C%20an%20innovative%20framework%20designed%20for%0Aanimating%20customized%20subjects%20with%20precise%20control%20over%20both%20object%20and%20camera%0Amovements.%20By%20leveraging%20a%20few%20images%20of%20a%20specific%20object%2C%20we%20efficiently%0Afine-tune%20a%20text-to-video%20model%20to%20capture%20the%20object%27s%20shape%20and%20attributes%0Aaccurately.%20Our%20approach%20presents%20subject%20region%20loss%20and%20video%20preservation%0Aloss%20to%20enhance%20the%20subject%27s%20learning%20performance%2C%20along%20with%20a%20subject%20token%0Across-attention%20loss%20to%20integrate%20the%20customized%20subject%20with%20motion%20control%0Asignals.%20Additionally%2C%20we%20propose%20training-free%20techniques%20for%20managing%20subject%0Aand%20camera%20motions%20during%20inference.%20In%20particular%2C%20we%20utilize%20cross-attention%0Amap%20manipulation%20to%20govern%20subject%20motion%20and%20introduce%20a%20novel%20latent%20shift%0Amodule%20for%20camera%20movement%20control%20as%20well.%20MotionBooth%20excels%20in%20preserving%0Athe%20appearance%20of%20subjects%20while%20simultaneously%20controlling%20the%20motions%20in%0Agenerated%20videos.%20Extensive%20quantitative%20and%20qualitative%20evaluations%0Ademonstrate%20the%20superiority%20and%20effectiveness%20of%20our%20method.%20Our%20project%20page%0Ais%20at%20https%3A//jianzongwu.github.io/projects/motionbooth%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17758v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionBooth%253A%2520Motion-Aware%2520Customized%2520Text-to-Video%2520Generation%26entry.906535625%3DJianzong%2520Wu%2520and%2520Xiangtai%2520Li%2520and%2520Yanhong%2520Zeng%2520and%2520Jiangning%2520Zhang%2520and%2520Qianyu%2520Zhou%2520and%2520Yining%2520Li%2520and%2520Yunhai%2520Tong%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520MotionBooth%252C%2520an%2520innovative%2520framework%2520designed%2520for%250Aanimating%2520customized%2520subjects%2520with%2520precise%2520control%2520over%2520both%2520object%2520and%2520camera%250Amovements.%2520By%2520leveraging%2520a%2520few%2520images%2520of%2520a%2520specific%2520object%252C%2520we%2520efficiently%250Afine-tune%2520a%2520text-to-video%2520model%2520to%2520capture%2520the%2520object%2527s%2520shape%2520and%2520attributes%250Aaccurately.%2520Our%2520approach%2520presents%2520subject%2520region%2520loss%2520and%2520video%2520preservation%250Aloss%2520to%2520enhance%2520the%2520subject%2527s%2520learning%2520performance%252C%2520along%2520with%2520a%2520subject%2520token%250Across-attention%2520loss%2520to%2520integrate%2520the%2520customized%2520subject%2520with%2520motion%2520control%250Asignals.%2520Additionally%252C%2520we%2520propose%2520training-free%2520techniques%2520for%2520managing%2520subject%250Aand%2520camera%2520motions%2520during%2520inference.%2520In%2520particular%252C%2520we%2520utilize%2520cross-attention%250Amap%2520manipulation%2520to%2520govern%2520subject%2520motion%2520and%2520introduce%2520a%2520novel%2520latent%2520shift%250Amodule%2520for%2520camera%2520movement%2520control%2520as%2520well.%2520MotionBooth%2520excels%2520in%2520preserving%250Athe%2520appearance%2520of%2520subjects%2520while%2520simultaneously%2520controlling%2520the%2520motions%2520in%250Agenerated%2520videos.%2520Extensive%2520quantitative%2520and%2520qualitative%2520evaluations%250Ademonstrate%2520the%2520superiority%2520and%2520effectiveness%2520of%2520our%2520method.%2520Our%2520project%2520page%250Ais%2520at%2520https%253A//jianzongwu.github.io/projects/motionbooth%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17758v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation&entry.906535625=Jianzong%20Wu%20and%20Xiangtai%20Li%20and%20Yanhong%20Zeng%20and%20Jiangning%20Zhang%20and%20Qianyu%20Zhou%20and%20Yining%20Li%20and%20Yunhai%20Tong%20and%20Kai%20Chen&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20MotionBooth%2C%20an%20innovative%20framework%20designed%20for%0Aanimating%20customized%20subjects%20with%20precise%20control%20over%20both%20object%20and%20camera%0Amovements.%20By%20leveraging%20a%20few%20images%20of%20a%20specific%20object%2C%20we%20efficiently%0Afine-tune%20a%20text-to-video%20model%20to%20capture%20the%20object%27s%20shape%20and%20attributes%0Aaccurately.%20Our%20approach%20presents%20subject%20region%20loss%20and%20video%20preservation%0Aloss%20to%20enhance%20the%20subject%27s%20learning%20performance%2C%20along%20with%20a%20subject%20token%0Across-attention%20loss%20to%20integrate%20the%20customized%20subject%20with%20motion%20control%0Asignals.%20Additionally%2C%20we%20propose%20training-free%20techniques%20for%20managing%20subject%0Aand%20camera%20motions%20during%20inference.%20In%20particular%2C%20we%20utilize%20cross-attention%0Amap%20manipulation%20to%20govern%20subject%20motion%20and%20introduce%20a%20novel%20latent%20shift%0Amodule%20for%20camera%20movement%20control%20as%20well.%20MotionBooth%20excels%20in%20preserving%0Athe%20appearance%20of%20subjects%20while%20simultaneously%20controlling%20the%20motions%20in%0Agenerated%20videos.%20Extensive%20quantitative%20and%20qualitative%20evaluations%0Ademonstrate%20the%20superiority%20and%20effectiveness%20of%20our%20method.%20Our%20project%20page%0Ais%20at%20https%3A//jianzongwu.github.io/projects/motionbooth%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17758v2&entry.124074799=Read"},
{"title": "FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot\n  Performance", "author": "Jiedong Zhuang and Jiaqi Hu and Lianrui Mu and Rui Hu and Xiaoyu Liang and Jiangnan Ye and Haoji Hu", "abstract": "  CLIP has achieved impressive zero-shot performance after pre-training on a\nlarge-scale dataset consisting of paired image-text data. Previous works have\nutilized CLIP by incorporating manually designed visual prompts like colored\ncircles and blur masks into the images to guide the model's attention, showing\nenhanced zero-shot performance in downstream tasks. Although these methods have\nachieved promising results, they inevitably alter the original information of\nthe images, which can lead to failure in specific tasks. We propose a\ntrain-free method Foveal-Attention CLIP (FALIP), which adjusts the CLIP's\nattention by inserting foveal attention masks into the multi-head\nself-attention module. We demonstrate FALIP effectively boosts CLIP zero-shot\nperformance in tasks such as referring expressions comprehension, image\nclassification, and 3D point cloud recognition. Experimental results further\nshow that FALIP outperforms existing methods on most metrics and can augment\ncurrent methods to enhance their performance.\n", "link": "http://arxiv.org/abs/2407.05578v2", "date": "2024-08-21", "relevancy": 2.6383, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5654}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5184}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FALIP%3A%20Visual%20Prompt%20as%20Foveal%20Attention%20Boosts%20CLIP%20Zero-Shot%0A%20%20Performance&body=Title%3A%20FALIP%3A%20Visual%20Prompt%20as%20Foveal%20Attention%20Boosts%20CLIP%20Zero-Shot%0A%20%20Performance%0AAuthor%3A%20Jiedong%20Zhuang%20and%20Jiaqi%20Hu%20and%20Lianrui%20Mu%20and%20Rui%20Hu%20and%20Xiaoyu%20Liang%20and%20Jiangnan%20Ye%20and%20Haoji%20Hu%0AAbstract%3A%20%20%20CLIP%20has%20achieved%20impressive%20zero-shot%20performance%20after%20pre-training%20on%20a%0Alarge-scale%20dataset%20consisting%20of%20paired%20image-text%20data.%20Previous%20works%20have%0Autilized%20CLIP%20by%20incorporating%20manually%20designed%20visual%20prompts%20like%20colored%0Acircles%20and%20blur%20masks%20into%20the%20images%20to%20guide%20the%20model%27s%20attention%2C%20showing%0Aenhanced%20zero-shot%20performance%20in%20downstream%20tasks.%20Although%20these%20methods%20have%0Aachieved%20promising%20results%2C%20they%20inevitably%20alter%20the%20original%20information%20of%0Athe%20images%2C%20which%20can%20lead%20to%20failure%20in%20specific%20tasks.%20We%20propose%20a%0Atrain-free%20method%20Foveal-Attention%20CLIP%20%28FALIP%29%2C%20which%20adjusts%20the%20CLIP%27s%0Aattention%20by%20inserting%20foveal%20attention%20masks%20into%20the%20multi-head%0Aself-attention%20module.%20We%20demonstrate%20FALIP%20effectively%20boosts%20CLIP%20zero-shot%0Aperformance%20in%20tasks%20such%20as%20referring%20expressions%20comprehension%2C%20image%0Aclassification%2C%20and%203D%20point%20cloud%20recognition.%20Experimental%20results%20further%0Ashow%20that%20FALIP%20outperforms%20existing%20methods%20on%20most%20metrics%20and%20can%20augment%0Acurrent%20methods%20to%20enhance%20their%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFALIP%253A%2520Visual%2520Prompt%2520as%2520Foveal%2520Attention%2520Boosts%2520CLIP%2520Zero-Shot%250A%2520%2520Performance%26entry.906535625%3DJiedong%2520Zhuang%2520and%2520Jiaqi%2520Hu%2520and%2520Lianrui%2520Mu%2520and%2520Rui%2520Hu%2520and%2520Xiaoyu%2520Liang%2520and%2520Jiangnan%2520Ye%2520and%2520Haoji%2520Hu%26entry.1292438233%3D%2520%2520CLIP%2520has%2520achieved%2520impressive%2520zero-shot%2520performance%2520after%2520pre-training%2520on%2520a%250Alarge-scale%2520dataset%2520consisting%2520of%2520paired%2520image-text%2520data.%2520Previous%2520works%2520have%250Autilized%2520CLIP%2520by%2520incorporating%2520manually%2520designed%2520visual%2520prompts%2520like%2520colored%250Acircles%2520and%2520blur%2520masks%2520into%2520the%2520images%2520to%2520guide%2520the%2520model%2527s%2520attention%252C%2520showing%250Aenhanced%2520zero-shot%2520performance%2520in%2520downstream%2520tasks.%2520Although%2520these%2520methods%2520have%250Aachieved%2520promising%2520results%252C%2520they%2520inevitably%2520alter%2520the%2520original%2520information%2520of%250Athe%2520images%252C%2520which%2520can%2520lead%2520to%2520failure%2520in%2520specific%2520tasks.%2520We%2520propose%2520a%250Atrain-free%2520method%2520Foveal-Attention%2520CLIP%2520%2528FALIP%2529%252C%2520which%2520adjusts%2520the%2520CLIP%2527s%250Aattention%2520by%2520inserting%2520foveal%2520attention%2520masks%2520into%2520the%2520multi-head%250Aself-attention%2520module.%2520We%2520demonstrate%2520FALIP%2520effectively%2520boosts%2520CLIP%2520zero-shot%250Aperformance%2520in%2520tasks%2520such%2520as%2520referring%2520expressions%2520comprehension%252C%2520image%250Aclassification%252C%2520and%25203D%2520point%2520cloud%2520recognition.%2520Experimental%2520results%2520further%250Ashow%2520that%2520FALIP%2520outperforms%2520existing%2520methods%2520on%2520most%2520metrics%2520and%2520can%2520augment%250Acurrent%2520methods%2520to%2520enhance%2520their%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALIP%3A%20Visual%20Prompt%20as%20Foveal%20Attention%20Boosts%20CLIP%20Zero-Shot%0A%20%20Performance&entry.906535625=Jiedong%20Zhuang%20and%20Jiaqi%20Hu%20and%20Lianrui%20Mu%20and%20Rui%20Hu%20and%20Xiaoyu%20Liang%20and%20Jiangnan%20Ye%20and%20Haoji%20Hu&entry.1292438233=%20%20CLIP%20has%20achieved%20impressive%20zero-shot%20performance%20after%20pre-training%20on%20a%0Alarge-scale%20dataset%20consisting%20of%20paired%20image-text%20data.%20Previous%20works%20have%0Autilized%20CLIP%20by%20incorporating%20manually%20designed%20visual%20prompts%20like%20colored%0Acircles%20and%20blur%20masks%20into%20the%20images%20to%20guide%20the%20model%27s%20attention%2C%20showing%0Aenhanced%20zero-shot%20performance%20in%20downstream%20tasks.%20Although%20these%20methods%20have%0Aachieved%20promising%20results%2C%20they%20inevitably%20alter%20the%20original%20information%20of%0Athe%20images%2C%20which%20can%20lead%20to%20failure%20in%20specific%20tasks.%20We%20propose%20a%0Atrain-free%20method%20Foveal-Attention%20CLIP%20%28FALIP%29%2C%20which%20adjusts%20the%20CLIP%27s%0Aattention%20by%20inserting%20foveal%20attention%20masks%20into%20the%20multi-head%0Aself-attention%20module.%20We%20demonstrate%20FALIP%20effectively%20boosts%20CLIP%20zero-shot%0Aperformance%20in%20tasks%20such%20as%20referring%20expressions%20comprehension%2C%20image%0Aclassification%2C%20and%203D%20point%20cloud%20recognition.%20Experimental%20results%20further%0Ashow%20that%20FALIP%20outperforms%20existing%20methods%20on%20most%20metrics%20and%20can%20augment%0Acurrent%20methods%20to%20enhance%20their%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05578v2&entry.124074799=Read"},
{"title": "Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for\n  Few-Shot Class-Incremental Learning", "author": "Xiaojie Li and Yibo Yang and Jianlong Wu and Bernard Ghanem and Liqiang Nie and Min Zhang", "abstract": "  Few-shot class-incremental learning (FSCIL) confronts the challenge of\nintegrating new classes into a model with minimal training samples while\npreserving the knowledge of previously learned classes. Traditional methods\nwidely adopt static adaptation relying on a fixed parameter space to learn from\ndata that arrive sequentially, prone to overfitting to the current session.\nExisting dynamic strategies require the expansion of the parameter space\ncontinually, leading to increased complexity. In this study, we explore the\npotential of Selective State Space Models (SSMs) for FSCIL, leveraging its\ndynamic weights and strong ability in sequence modeling to address these\nchallenges. Concretely, we propose a dual selective SSM projector that\ndynamically adjusts the projection parameters based on the intermediate\nfeatures for dynamic adaptation. The dual design enables the model to maintain\nthe robust features of base classes, while adaptively learning distinctive\nfeature shifts for novel classes. Additionally, we develop a class-sensitive\nselective scan mechanism to guide dynamic adaptation. It minimizes the\ndisruption to base-class representations caused by training on novel data, and\nmeanwhile, forces the selective scan to perform in distinct patterns between\nbase and novel classes. Experiments on miniImageNet, CUB-200, and CIFAR-100\ndemonstrate that our framework outperforms the existing state-of-the-art\nmethods. The code is available at\n\\url{https://github.com/xiaojieli0903/Mamba-FSCIL}.\n", "link": "http://arxiv.org/abs/2407.06136v2", "date": "2024-08-21", "relevancy": 2.6377, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5286}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5279}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba-FSCIL%3A%20Dynamic%20Adaptation%20with%20Selective%20State%20Space%20Model%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning&body=Title%3A%20Mamba-FSCIL%3A%20Dynamic%20Adaptation%20with%20Selective%20State%20Space%20Model%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning%0AAuthor%3A%20Xiaojie%20Li%20and%20Yibo%20Yang%20and%20Jianlong%20Wu%20and%20Bernard%20Ghanem%20and%20Liqiang%20Nie%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20confronts%20the%20challenge%20of%0Aintegrating%20new%20classes%20into%20a%20model%20with%20minimal%20training%20samples%20while%0Apreserving%20the%20knowledge%20of%20previously%20learned%20classes.%20Traditional%20methods%0Awidely%20adopt%20static%20adaptation%20relying%20on%20a%20fixed%20parameter%20space%20to%20learn%20from%0Adata%20that%20arrive%20sequentially%2C%20prone%20to%20overfitting%20to%20the%20current%20session.%0AExisting%20dynamic%20strategies%20require%20the%20expansion%20of%20the%20parameter%20space%0Acontinually%2C%20leading%20to%20increased%20complexity.%20In%20this%20study%2C%20we%20explore%20the%0Apotential%20of%20Selective%20State%20Space%20Models%20%28SSMs%29%20for%20FSCIL%2C%20leveraging%20its%0Adynamic%20weights%20and%20strong%20ability%20in%20sequence%20modeling%20to%20address%20these%0Achallenges.%20Concretely%2C%20we%20propose%20a%20dual%20selective%20SSM%20projector%20that%0Adynamically%20adjusts%20the%20projection%20parameters%20based%20on%20the%20intermediate%0Afeatures%20for%20dynamic%20adaptation.%20The%20dual%20design%20enables%20the%20model%20to%20maintain%0Athe%20robust%20features%20of%20base%20classes%2C%20while%20adaptively%20learning%20distinctive%0Afeature%20shifts%20for%20novel%20classes.%20Additionally%2C%20we%20develop%20a%20class-sensitive%0Aselective%20scan%20mechanism%20to%20guide%20dynamic%20adaptation.%20It%20minimizes%20the%0Adisruption%20to%20base-class%20representations%20caused%20by%20training%20on%20novel%20data%2C%20and%0Ameanwhile%2C%20forces%20the%20selective%20scan%20to%20perform%20in%20distinct%20patterns%20between%0Abase%20and%20novel%20classes.%20Experiments%20on%20miniImageNet%2C%20CUB-200%2C%20and%20CIFAR-100%0Ademonstrate%20that%20our%20framework%20outperforms%20the%20existing%20state-of-the-art%0Amethods.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/xiaojieli0903/Mamba-FSCIL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06136v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba-FSCIL%253A%2520Dynamic%2520Adaptation%2520with%2520Selective%2520State%2520Space%2520Model%2520for%250A%2520%2520Few-Shot%2520Class-Incremental%2520Learning%26entry.906535625%3DXiaojie%2520Li%2520and%2520Yibo%2520Yang%2520and%2520Jianlong%2520Wu%2520and%2520Bernard%2520Ghanem%2520and%2520Liqiang%2520Nie%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Few-shot%2520class-incremental%2520learning%2520%2528FSCIL%2529%2520confronts%2520the%2520challenge%2520of%250Aintegrating%2520new%2520classes%2520into%2520a%2520model%2520with%2520minimal%2520training%2520samples%2520while%250Apreserving%2520the%2520knowledge%2520of%2520previously%2520learned%2520classes.%2520Traditional%2520methods%250Awidely%2520adopt%2520static%2520adaptation%2520relying%2520on%2520a%2520fixed%2520parameter%2520space%2520to%2520learn%2520from%250Adata%2520that%2520arrive%2520sequentially%252C%2520prone%2520to%2520overfitting%2520to%2520the%2520current%2520session.%250AExisting%2520dynamic%2520strategies%2520require%2520the%2520expansion%2520of%2520the%2520parameter%2520space%250Acontinually%252C%2520leading%2520to%2520increased%2520complexity.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%250Apotential%2520of%2520Selective%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520for%2520FSCIL%252C%2520leveraging%2520its%250Adynamic%2520weights%2520and%2520strong%2520ability%2520in%2520sequence%2520modeling%2520to%2520address%2520these%250Achallenges.%2520Concretely%252C%2520we%2520propose%2520a%2520dual%2520selective%2520SSM%2520projector%2520that%250Adynamically%2520adjusts%2520the%2520projection%2520parameters%2520based%2520on%2520the%2520intermediate%250Afeatures%2520for%2520dynamic%2520adaptation.%2520The%2520dual%2520design%2520enables%2520the%2520model%2520to%2520maintain%250Athe%2520robust%2520features%2520of%2520base%2520classes%252C%2520while%2520adaptively%2520learning%2520distinctive%250Afeature%2520shifts%2520for%2520novel%2520classes.%2520Additionally%252C%2520we%2520develop%2520a%2520class-sensitive%250Aselective%2520scan%2520mechanism%2520to%2520guide%2520dynamic%2520adaptation.%2520It%2520minimizes%2520the%250Adisruption%2520to%2520base-class%2520representations%2520caused%2520by%2520training%2520on%2520novel%2520data%252C%2520and%250Ameanwhile%252C%2520forces%2520the%2520selective%2520scan%2520to%2520perform%2520in%2520distinct%2520patterns%2520between%250Abase%2520and%2520novel%2520classes.%2520Experiments%2520on%2520miniImageNet%252C%2520CUB-200%252C%2520and%2520CIFAR-100%250Ademonstrate%2520that%2520our%2520framework%2520outperforms%2520the%2520existing%2520state-of-the-art%250Amethods.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/xiaojieli0903/Mamba-FSCIL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06136v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba-FSCIL%3A%20Dynamic%20Adaptation%20with%20Selective%20State%20Space%20Model%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning&entry.906535625=Xiaojie%20Li%20and%20Yibo%20Yang%20and%20Jianlong%20Wu%20and%20Bernard%20Ghanem%20and%20Liqiang%20Nie%20and%20Min%20Zhang&entry.1292438233=%20%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20confronts%20the%20challenge%20of%0Aintegrating%20new%20classes%20into%20a%20model%20with%20minimal%20training%20samples%20while%0Apreserving%20the%20knowledge%20of%20previously%20learned%20classes.%20Traditional%20methods%0Awidely%20adopt%20static%20adaptation%20relying%20on%20a%20fixed%20parameter%20space%20to%20learn%20from%0Adata%20that%20arrive%20sequentially%2C%20prone%20to%20overfitting%20to%20the%20current%20session.%0AExisting%20dynamic%20strategies%20require%20the%20expansion%20of%20the%20parameter%20space%0Acontinually%2C%20leading%20to%20increased%20complexity.%20In%20this%20study%2C%20we%20explore%20the%0Apotential%20of%20Selective%20State%20Space%20Models%20%28SSMs%29%20for%20FSCIL%2C%20leveraging%20its%0Adynamic%20weights%20and%20strong%20ability%20in%20sequence%20modeling%20to%20address%20these%0Achallenges.%20Concretely%2C%20we%20propose%20a%20dual%20selective%20SSM%20projector%20that%0Adynamically%20adjusts%20the%20projection%20parameters%20based%20on%20the%20intermediate%0Afeatures%20for%20dynamic%20adaptation.%20The%20dual%20design%20enables%20the%20model%20to%20maintain%0Athe%20robust%20features%20of%20base%20classes%2C%20while%20adaptively%20learning%20distinctive%0Afeature%20shifts%20for%20novel%20classes.%20Additionally%2C%20we%20develop%20a%20class-sensitive%0Aselective%20scan%20mechanism%20to%20guide%20dynamic%20adaptation.%20It%20minimizes%20the%0Adisruption%20to%20base-class%20representations%20caused%20by%20training%20on%20novel%20data%2C%20and%0Ameanwhile%2C%20forces%20the%20selective%20scan%20to%20perform%20in%20distinct%20patterns%20between%0Abase%20and%20novel%20classes.%20Experiments%20on%20miniImageNet%2C%20CUB-200%2C%20and%20CIFAR-100%0Ademonstrate%20that%20our%20framework%20outperforms%20the%20existing%20state-of-the-art%0Amethods.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/xiaojieli0903/Mamba-FSCIL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06136v2&entry.124074799=Read"},
{"title": "Optimizing Federated Graph Learning with Inherent Structural Knowledge\n  and Dual-Densely Connected GNNs", "author": "Longwen Wang and Jianchun Liu and Zhi Liu and Jinyang Huang", "abstract": "  Federated Graph Learning (FGL) is an emerging technology that enables clients\nto collaboratively train powerful Graph Neural Networks (GNNs) in a distributed\nmanner without exposing their private data. Nevertheless, FGL still faces the\nchallenge of the severe non-Independent and Identically Distributed (non-IID)\nnature of graphs, which possess diverse node and edge structures, especially\nacross varied domains. Thus, exploring the knowledge inherent in these\nstructures becomes significantly crucial. Existing methods, however, either\noverlook the inherent structural knowledge in graph data or capture it at the\ncost of significantly increased resource demands (e.g., FLOPs and communication\nbandwidth), which can be detrimental to distributed paradigms. Inspired by\nthis, we propose FedDense, a novel FGL framework that optimizes the utilization\nefficiency of inherent structural knowledge. To better acquire knowledge of\ndiverse and underexploited structures, FedDense first explicitly encodes the\nstructural knowledge inherent within graph data itself alongside node features.\nBesides, FedDense introduces a Dual-Densely Connected (DDC) GNN architecture\nthat exploits the multi-scale (i.e., one-hop to multi-hop) feature and\nstructure insights embedded in the aggregated feature maps at each layer. In\naddition to the exploitation of inherent structures, we consider resource\nlimitations in FGL, devising exceedingly narrow layers atop the DDC\narchitecture and adopting a selective parameter sharing strategy to reduce\nresource costs substantially. We conduct extensive experiments using 15\ndatasets across 4 different domains, demonstrating that FedDense consistently\nsurpasses baselines by a large margin in training performance, while demanding\nminimal resources.\n", "link": "http://arxiv.org/abs/2408.11662v1", "date": "2024-08-21", "relevancy": 2.623, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.542}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5244}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Federated%20Graph%20Learning%20with%20Inherent%20Structural%20Knowledge%0A%20%20and%20Dual-Densely%20Connected%20GNNs&body=Title%3A%20Optimizing%20Federated%20Graph%20Learning%20with%20Inherent%20Structural%20Knowledge%0A%20%20and%20Dual-Densely%20Connected%20GNNs%0AAuthor%3A%20Longwen%20Wang%20and%20Jianchun%20Liu%20and%20Zhi%20Liu%20and%20Jinyang%20Huang%0AAbstract%3A%20%20%20Federated%20Graph%20Learning%20%28FGL%29%20is%20an%20emerging%20technology%20that%20enables%20clients%0Ato%20collaboratively%20train%20powerful%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20a%20distributed%0Amanner%20without%20exposing%20their%20private%20data.%20Nevertheless%2C%20FGL%20still%20faces%20the%0Achallenge%20of%20the%20severe%20non-Independent%20and%20Identically%20Distributed%20%28non-IID%29%0Anature%20of%20graphs%2C%20which%20possess%20diverse%20node%20and%20edge%20structures%2C%20especially%0Aacross%20varied%20domains.%20Thus%2C%20exploring%20the%20knowledge%20inherent%20in%20these%0Astructures%20becomes%20significantly%20crucial.%20Existing%20methods%2C%20however%2C%20either%0Aoverlook%20the%20inherent%20structural%20knowledge%20in%20graph%20data%20or%20capture%20it%20at%20the%0Acost%20of%20significantly%20increased%20resource%20demands%20%28e.g.%2C%20FLOPs%20and%20communication%0Abandwidth%29%2C%20which%20can%20be%20detrimental%20to%20distributed%20paradigms.%20Inspired%20by%0Athis%2C%20we%20propose%20FedDense%2C%20a%20novel%20FGL%20framework%20that%20optimizes%20the%20utilization%0Aefficiency%20of%20inherent%20structural%20knowledge.%20To%20better%20acquire%20knowledge%20of%0Adiverse%20and%20underexploited%20structures%2C%20FedDense%20first%20explicitly%20encodes%20the%0Astructural%20knowledge%20inherent%20within%20graph%20data%20itself%20alongside%20node%20features.%0ABesides%2C%20FedDense%20introduces%20a%20Dual-Densely%20Connected%20%28DDC%29%20GNN%20architecture%0Athat%20exploits%20the%20multi-scale%20%28i.e.%2C%20one-hop%20to%20multi-hop%29%20feature%20and%0Astructure%20insights%20embedded%20in%20the%20aggregated%20feature%20maps%20at%20each%20layer.%20In%0Aaddition%20to%20the%20exploitation%20of%20inherent%20structures%2C%20we%20consider%20resource%0Alimitations%20in%20FGL%2C%20devising%20exceedingly%20narrow%20layers%20atop%20the%20DDC%0Aarchitecture%20and%20adopting%20a%20selective%20parameter%20sharing%20strategy%20to%20reduce%0Aresource%20costs%20substantially.%20We%20conduct%20extensive%20experiments%20using%2015%0Adatasets%20across%204%20different%20domains%2C%20demonstrating%20that%20FedDense%20consistently%0Asurpasses%20baselines%20by%20a%20large%20margin%20in%20training%20performance%2C%20while%20demanding%0Aminimal%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Federated%2520Graph%2520Learning%2520with%2520Inherent%2520Structural%2520Knowledge%250A%2520%2520and%2520Dual-Densely%2520Connected%2520GNNs%26entry.906535625%3DLongwen%2520Wang%2520and%2520Jianchun%2520Liu%2520and%2520Zhi%2520Liu%2520and%2520Jinyang%2520Huang%26entry.1292438233%3D%2520%2520Federated%2520Graph%2520Learning%2520%2528FGL%2529%2520is%2520an%2520emerging%2520technology%2520that%2520enables%2520clients%250Ato%2520collaboratively%2520train%2520powerful%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520in%2520a%2520distributed%250Amanner%2520without%2520exposing%2520their%2520private%2520data.%2520Nevertheless%252C%2520FGL%2520still%2520faces%2520the%250Achallenge%2520of%2520the%2520severe%2520non-Independent%2520and%2520Identically%2520Distributed%2520%2528non-IID%2529%250Anature%2520of%2520graphs%252C%2520which%2520possess%2520diverse%2520node%2520and%2520edge%2520structures%252C%2520especially%250Aacross%2520varied%2520domains.%2520Thus%252C%2520exploring%2520the%2520knowledge%2520inherent%2520in%2520these%250Astructures%2520becomes%2520significantly%2520crucial.%2520Existing%2520methods%252C%2520however%252C%2520either%250Aoverlook%2520the%2520inherent%2520structural%2520knowledge%2520in%2520graph%2520data%2520or%2520capture%2520it%2520at%2520the%250Acost%2520of%2520significantly%2520increased%2520resource%2520demands%2520%2528e.g.%252C%2520FLOPs%2520and%2520communication%250Abandwidth%2529%252C%2520which%2520can%2520be%2520detrimental%2520to%2520distributed%2520paradigms.%2520Inspired%2520by%250Athis%252C%2520we%2520propose%2520FedDense%252C%2520a%2520novel%2520FGL%2520framework%2520that%2520optimizes%2520the%2520utilization%250Aefficiency%2520of%2520inherent%2520structural%2520knowledge.%2520To%2520better%2520acquire%2520knowledge%2520of%250Adiverse%2520and%2520underexploited%2520structures%252C%2520FedDense%2520first%2520explicitly%2520encodes%2520the%250Astructural%2520knowledge%2520inherent%2520within%2520graph%2520data%2520itself%2520alongside%2520node%2520features.%250ABesides%252C%2520FedDense%2520introduces%2520a%2520Dual-Densely%2520Connected%2520%2528DDC%2529%2520GNN%2520architecture%250Athat%2520exploits%2520the%2520multi-scale%2520%2528i.e.%252C%2520one-hop%2520to%2520multi-hop%2529%2520feature%2520and%250Astructure%2520insights%2520embedded%2520in%2520the%2520aggregated%2520feature%2520maps%2520at%2520each%2520layer.%2520In%250Aaddition%2520to%2520the%2520exploitation%2520of%2520inherent%2520structures%252C%2520we%2520consider%2520resource%250Alimitations%2520in%2520FGL%252C%2520devising%2520exceedingly%2520narrow%2520layers%2520atop%2520the%2520DDC%250Aarchitecture%2520and%2520adopting%2520a%2520selective%2520parameter%2520sharing%2520strategy%2520to%2520reduce%250Aresource%2520costs%2520substantially.%2520We%2520conduct%2520extensive%2520experiments%2520using%252015%250Adatasets%2520across%25204%2520different%2520domains%252C%2520demonstrating%2520that%2520FedDense%2520consistently%250Asurpasses%2520baselines%2520by%2520a%2520large%2520margin%2520in%2520training%2520performance%252C%2520while%2520demanding%250Aminimal%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Federated%20Graph%20Learning%20with%20Inherent%20Structural%20Knowledge%0A%20%20and%20Dual-Densely%20Connected%20GNNs&entry.906535625=Longwen%20Wang%20and%20Jianchun%20Liu%20and%20Zhi%20Liu%20and%20Jinyang%20Huang&entry.1292438233=%20%20Federated%20Graph%20Learning%20%28FGL%29%20is%20an%20emerging%20technology%20that%20enables%20clients%0Ato%20collaboratively%20train%20powerful%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20a%20distributed%0Amanner%20without%20exposing%20their%20private%20data.%20Nevertheless%2C%20FGL%20still%20faces%20the%0Achallenge%20of%20the%20severe%20non-Independent%20and%20Identically%20Distributed%20%28non-IID%29%0Anature%20of%20graphs%2C%20which%20possess%20diverse%20node%20and%20edge%20structures%2C%20especially%0Aacross%20varied%20domains.%20Thus%2C%20exploring%20the%20knowledge%20inherent%20in%20these%0Astructures%20becomes%20significantly%20crucial.%20Existing%20methods%2C%20however%2C%20either%0Aoverlook%20the%20inherent%20structural%20knowledge%20in%20graph%20data%20or%20capture%20it%20at%20the%0Acost%20of%20significantly%20increased%20resource%20demands%20%28e.g.%2C%20FLOPs%20and%20communication%0Abandwidth%29%2C%20which%20can%20be%20detrimental%20to%20distributed%20paradigms.%20Inspired%20by%0Athis%2C%20we%20propose%20FedDense%2C%20a%20novel%20FGL%20framework%20that%20optimizes%20the%20utilization%0Aefficiency%20of%20inherent%20structural%20knowledge.%20To%20better%20acquire%20knowledge%20of%0Adiverse%20and%20underexploited%20structures%2C%20FedDense%20first%20explicitly%20encodes%20the%0Astructural%20knowledge%20inherent%20within%20graph%20data%20itself%20alongside%20node%20features.%0ABesides%2C%20FedDense%20introduces%20a%20Dual-Densely%20Connected%20%28DDC%29%20GNN%20architecture%0Athat%20exploits%20the%20multi-scale%20%28i.e.%2C%20one-hop%20to%20multi-hop%29%20feature%20and%0Astructure%20insights%20embedded%20in%20the%20aggregated%20feature%20maps%20at%20each%20layer.%20In%0Aaddition%20to%20the%20exploitation%20of%20inherent%20structures%2C%20we%20consider%20resource%0Alimitations%20in%20FGL%2C%20devising%20exceedingly%20narrow%20layers%20atop%20the%20DDC%0Aarchitecture%20and%20adopting%20a%20selective%20parameter%20sharing%20strategy%20to%20reduce%0Aresource%20costs%20substantially.%20We%20conduct%20extensive%20experiments%20using%2015%0Adatasets%20across%204%20different%20domains%2C%20demonstrating%20that%20FedDense%20consistently%0Asurpasses%20baselines%20by%20a%20large%20margin%20in%20training%20performance%2C%20while%20demanding%0Aminimal%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11662v1&entry.124074799=Read"},
{"title": "Structure-preserving Planar Simplification for Indoor Environments", "author": "Bishwash Khanal and Sanjay Rijal and Manish Awale and Vaghawan Ojha", "abstract": "  This paper presents a novel approach for structure-preserving planar\nsimplification of indoor scene point clouds for both simulated and real-world\nenvironments. Initially, the scene point cloud undergoes preprocessing steps,\nincluding noise reduction and Manhattan world alignment, to ensure robustness\nand coherence in subsequent analyses. We segment each captured scene into\nstructured (walls-ceiling-floor) and non-structured (indoor objects) scenes.\nLeveraging a RANSAC algorithm, we extract primitive planes from the input point\ncloud, facilitating the segmentation and simplification of the structured\nscene. The best-fitting wall meshes are then generated from the primitives,\nfollowed by adjacent mesh merging with the vertex-translation algorithm which\npreserves the mesh layout. To accurately represent ceilings and floors, we\nemploy the mesh clipping algorithm which clips the ceiling and floor meshes\nwith respect to wall normals. In the case of indoor scenes, we apply a surface\nreconstruction technique to enhance the fidelity. This paper focuses on the\nintricate steps of the proposed scene simplification methodology, addressing\ncomplex scenarios such as multi-story and slanted walls and ceilings. We also\nconduct qualitative and quantitative performance comparisons against popular\nsurface reconstruction, shape approximation, and floorplan generation\napproaches.\n", "link": "http://arxiv.org/abs/2408.06814v2", "date": "2024-08-21", "relevancy": 2.5957, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5377}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5098}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-preserving%20Planar%20Simplification%20for%20Indoor%20Environments&body=Title%3A%20Structure-preserving%20Planar%20Simplification%20for%20Indoor%20Environments%0AAuthor%3A%20Bishwash%20Khanal%20and%20Sanjay%20Rijal%20and%20Manish%20Awale%20and%20Vaghawan%20Ojha%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20structure-preserving%20planar%0Asimplification%20of%20indoor%20scene%20point%20clouds%20for%20both%20simulated%20and%20real-world%0Aenvironments.%20Initially%2C%20the%20scene%20point%20cloud%20undergoes%20preprocessing%20steps%2C%0Aincluding%20noise%20reduction%20and%20Manhattan%20world%20alignment%2C%20to%20ensure%20robustness%0Aand%20coherence%20in%20subsequent%20analyses.%20We%20segment%20each%20captured%20scene%20into%0Astructured%20%28walls-ceiling-floor%29%20and%20non-structured%20%28indoor%20objects%29%20scenes.%0ALeveraging%20a%20RANSAC%20algorithm%2C%20we%20extract%20primitive%20planes%20from%20the%20input%20point%0Acloud%2C%20facilitating%20the%20segmentation%20and%20simplification%20of%20the%20structured%0Ascene.%20The%20best-fitting%20wall%20meshes%20are%20then%20generated%20from%20the%20primitives%2C%0Afollowed%20by%20adjacent%20mesh%20merging%20with%20the%20vertex-translation%20algorithm%20which%0Apreserves%20the%20mesh%20layout.%20To%20accurately%20represent%20ceilings%20and%20floors%2C%20we%0Aemploy%20the%20mesh%20clipping%20algorithm%20which%20clips%20the%20ceiling%20and%20floor%20meshes%0Awith%20respect%20to%20wall%20normals.%20In%20the%20case%20of%20indoor%20scenes%2C%20we%20apply%20a%20surface%0Areconstruction%20technique%20to%20enhance%20the%20fidelity.%20This%20paper%20focuses%20on%20the%0Aintricate%20steps%20of%20the%20proposed%20scene%20simplification%20methodology%2C%20addressing%0Acomplex%20scenarios%20such%20as%20multi-story%20and%20slanted%20walls%20and%20ceilings.%20We%20also%0Aconduct%20qualitative%20and%20quantitative%20performance%20comparisons%20against%20popular%0Asurface%20reconstruction%2C%20shape%20approximation%2C%20and%20floorplan%20generation%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-preserving%2520Planar%2520Simplification%2520for%2520Indoor%2520Environments%26entry.906535625%3DBishwash%2520Khanal%2520and%2520Sanjay%2520Rijal%2520and%2520Manish%2520Awale%2520and%2520Vaghawan%2520Ojha%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520structure-preserving%2520planar%250Asimplification%2520of%2520indoor%2520scene%2520point%2520clouds%2520for%2520both%2520simulated%2520and%2520real-world%250Aenvironments.%2520Initially%252C%2520the%2520scene%2520point%2520cloud%2520undergoes%2520preprocessing%2520steps%252C%250Aincluding%2520noise%2520reduction%2520and%2520Manhattan%2520world%2520alignment%252C%2520to%2520ensure%2520robustness%250Aand%2520coherence%2520in%2520subsequent%2520analyses.%2520We%2520segment%2520each%2520captured%2520scene%2520into%250Astructured%2520%2528walls-ceiling-floor%2529%2520and%2520non-structured%2520%2528indoor%2520objects%2529%2520scenes.%250ALeveraging%2520a%2520RANSAC%2520algorithm%252C%2520we%2520extract%2520primitive%2520planes%2520from%2520the%2520input%2520point%250Acloud%252C%2520facilitating%2520the%2520segmentation%2520and%2520simplification%2520of%2520the%2520structured%250Ascene.%2520The%2520best-fitting%2520wall%2520meshes%2520are%2520then%2520generated%2520from%2520the%2520primitives%252C%250Afollowed%2520by%2520adjacent%2520mesh%2520merging%2520with%2520the%2520vertex-translation%2520algorithm%2520which%250Apreserves%2520the%2520mesh%2520layout.%2520To%2520accurately%2520represent%2520ceilings%2520and%2520floors%252C%2520we%250Aemploy%2520the%2520mesh%2520clipping%2520algorithm%2520which%2520clips%2520the%2520ceiling%2520and%2520floor%2520meshes%250Awith%2520respect%2520to%2520wall%2520normals.%2520In%2520the%2520case%2520of%2520indoor%2520scenes%252C%2520we%2520apply%2520a%2520surface%250Areconstruction%2520technique%2520to%2520enhance%2520the%2520fidelity.%2520This%2520paper%2520focuses%2520on%2520the%250Aintricate%2520steps%2520of%2520the%2520proposed%2520scene%2520simplification%2520methodology%252C%2520addressing%250Acomplex%2520scenarios%2520such%2520as%2520multi-story%2520and%2520slanted%2520walls%2520and%2520ceilings.%2520We%2520also%250Aconduct%2520qualitative%2520and%2520quantitative%2520performance%2520comparisons%2520against%2520popular%250Asurface%2520reconstruction%252C%2520shape%2520approximation%252C%2520and%2520floorplan%2520generation%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-preserving%20Planar%20Simplification%20for%20Indoor%20Environments&entry.906535625=Bishwash%20Khanal%20and%20Sanjay%20Rijal%20and%20Manish%20Awale%20and%20Vaghawan%20Ojha&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20structure-preserving%20planar%0Asimplification%20of%20indoor%20scene%20point%20clouds%20for%20both%20simulated%20and%20real-world%0Aenvironments.%20Initially%2C%20the%20scene%20point%20cloud%20undergoes%20preprocessing%20steps%2C%0Aincluding%20noise%20reduction%20and%20Manhattan%20world%20alignment%2C%20to%20ensure%20robustness%0Aand%20coherence%20in%20subsequent%20analyses.%20We%20segment%20each%20captured%20scene%20into%0Astructured%20%28walls-ceiling-floor%29%20and%20non-structured%20%28indoor%20objects%29%20scenes.%0ALeveraging%20a%20RANSAC%20algorithm%2C%20we%20extract%20primitive%20planes%20from%20the%20input%20point%0Acloud%2C%20facilitating%20the%20segmentation%20and%20simplification%20of%20the%20structured%0Ascene.%20The%20best-fitting%20wall%20meshes%20are%20then%20generated%20from%20the%20primitives%2C%0Afollowed%20by%20adjacent%20mesh%20merging%20with%20the%20vertex-translation%20algorithm%20which%0Apreserves%20the%20mesh%20layout.%20To%20accurately%20represent%20ceilings%20and%20floors%2C%20we%0Aemploy%20the%20mesh%20clipping%20algorithm%20which%20clips%20the%20ceiling%20and%20floor%20meshes%0Awith%20respect%20to%20wall%20normals.%20In%20the%20case%20of%20indoor%20scenes%2C%20we%20apply%20a%20surface%0Areconstruction%20technique%20to%20enhance%20the%20fidelity.%20This%20paper%20focuses%20on%20the%0Aintricate%20steps%20of%20the%20proposed%20scene%20simplification%20methodology%2C%20addressing%0Acomplex%20scenarios%20such%20as%20multi-story%20and%20slanted%20walls%20and%20ceilings.%20We%20also%0Aconduct%20qualitative%20and%20quantitative%20performance%20comparisons%20against%20popular%0Asurface%20reconstruction%2C%20shape%20approximation%2C%20and%20floorplan%20generation%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06814v2&entry.124074799=Read"},
{"title": "KOSMOS-2.5: A Multimodal Literate Model", "author": "Tengchao Lv and Yupan Huang and Jingye Chen and Yuzhong Zhao and Yilin Jia and Lei Cui and Shuming Ma and Yaoyao Chang and Shaohan Huang and Wenhui Wang and Li Dong and Weiyao Luo and Shaoxiang Wu and Guoxin Wang and Cha Zhang and Furu Wei", "abstract": "  The automatic reading of text-intensive images represents a significant\nadvancement toward achieving Artificial General Intelligence (AGI). In this\npaper we present KOSMOS-2.5, a multimodal literate model for machine reading of\ntext-intensive images. Pre-trained on a large-scale corpus of text-intensive\nimages, KOSMOS-2.5 excels in two distinct yet complementary transcription\ntasks: (1) generating spatially-aware text blocks, where each block of text is\nassigned spatial coordinates within the image, and (2) producing structured\ntext output that captures both style and structure in markdown format. This\nunified multimodal literate capability is achieved through a shared\ndecoder-only autoregressive Transformer architecture and task-specific prompts.\nBuilding on this foundation, we fine-tune KOSMOS-2.5 for document understanding\ntasks, resulting in a document understanding generalist named KOSMOS-2.5-CHAT.\nAdditionally, a large corpus of 357.4 million document pages spanning diverse\ndomains was curated for pre-training. We evaluate KOSMOS-2.5 on two newly\nproposed benchmarks, OCREval and MarkdownEval, for document-level text\nrecognition and image-to-markdown generation, demonstrating impressive literate\ncapabilities comparable to GPT-4o. KOSMOS-2.5-CHAT achieves performance\ncomparable to other state-of-the-art generalists that are five times larger\n(1.3B vs. 7B) across nine text-rich visual question answering benchmarks.\nModels and code have been available at \\url{https://aka.ms/kosmos25}.\n", "link": "http://arxiv.org/abs/2309.11419v2", "date": "2024-08-21", "relevancy": 2.587, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5556}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5051}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KOSMOS-2.5%3A%20A%20Multimodal%20Literate%20Model&body=Title%3A%20KOSMOS-2.5%3A%20A%20Multimodal%20Literate%20Model%0AAuthor%3A%20Tengchao%20Lv%20and%20Yupan%20Huang%20and%20Jingye%20Chen%20and%20Yuzhong%20Zhao%20and%20Yilin%20Jia%20and%20Lei%20Cui%20and%20Shuming%20Ma%20and%20Yaoyao%20Chang%20and%20Shaohan%20Huang%20and%20Wenhui%20Wang%20and%20Li%20Dong%20and%20Weiyao%20Luo%20and%20Shaoxiang%20Wu%20and%20Guoxin%20Wang%20and%20Cha%20Zhang%20and%20Furu%20Wei%0AAbstract%3A%20%20%20The%20automatic%20reading%20of%20text-intensive%20images%20represents%20a%20significant%0Aadvancement%20toward%20achieving%20Artificial%20General%20Intelligence%20%28AGI%29.%20In%20this%0Apaper%20we%20present%20KOSMOS-2.5%2C%20a%20multimodal%20literate%20model%20for%20machine%20reading%20of%0Atext-intensive%20images.%20Pre-trained%20on%20a%20large-scale%20corpus%20of%20text-intensive%0Aimages%2C%20KOSMOS-2.5%20excels%20in%20two%20distinct%20yet%20complementary%20transcription%0Atasks%3A%20%281%29%20generating%20spatially-aware%20text%20blocks%2C%20where%20each%20block%20of%20text%20is%0Aassigned%20spatial%20coordinates%20within%20the%20image%2C%20and%20%282%29%20producing%20structured%0Atext%20output%20that%20captures%20both%20style%20and%20structure%20in%20markdown%20format.%20This%0Aunified%20multimodal%20literate%20capability%20is%20achieved%20through%20a%20shared%0Adecoder-only%20autoregressive%20Transformer%20architecture%20and%20task-specific%20prompts.%0ABuilding%20on%20this%20foundation%2C%20we%20fine-tune%20KOSMOS-2.5%20for%20document%20understanding%0Atasks%2C%20resulting%20in%20a%20document%20understanding%20generalist%20named%20KOSMOS-2.5-CHAT.%0AAdditionally%2C%20a%20large%20corpus%20of%20357.4%20million%20document%20pages%20spanning%20diverse%0Adomains%20was%20curated%20for%20pre-training.%20We%20evaluate%20KOSMOS-2.5%20on%20two%20newly%0Aproposed%20benchmarks%2C%20OCREval%20and%20MarkdownEval%2C%20for%20document-level%20text%0Arecognition%20and%20image-to-markdown%20generation%2C%20demonstrating%20impressive%20literate%0Acapabilities%20comparable%20to%20GPT-4o.%20KOSMOS-2.5-CHAT%20achieves%20performance%0Acomparable%20to%20other%20state-of-the-art%20generalists%20that%20are%20five%20times%20larger%0A%281.3B%20vs.%207B%29%20across%20nine%20text-rich%20visual%20question%20answering%20benchmarks.%0AModels%20and%20code%20have%20been%20available%20at%20%5Curl%7Bhttps%3A//aka.ms/kosmos25%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKOSMOS-2.5%253A%2520A%2520Multimodal%2520Literate%2520Model%26entry.906535625%3DTengchao%2520Lv%2520and%2520Yupan%2520Huang%2520and%2520Jingye%2520Chen%2520and%2520Yuzhong%2520Zhao%2520and%2520Yilin%2520Jia%2520and%2520Lei%2520Cui%2520and%2520Shuming%2520Ma%2520and%2520Yaoyao%2520Chang%2520and%2520Shaohan%2520Huang%2520and%2520Wenhui%2520Wang%2520and%2520Li%2520Dong%2520and%2520Weiyao%2520Luo%2520and%2520Shaoxiang%2520Wu%2520and%2520Guoxin%2520Wang%2520and%2520Cha%2520Zhang%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520The%2520automatic%2520reading%2520of%2520text-intensive%2520images%2520represents%2520a%2520significant%250Aadvancement%2520toward%2520achieving%2520Artificial%2520General%2520Intelligence%2520%2528AGI%2529.%2520In%2520this%250Apaper%2520we%2520present%2520KOSMOS-2.5%252C%2520a%2520multimodal%2520literate%2520model%2520for%2520machine%2520reading%2520of%250Atext-intensive%2520images.%2520Pre-trained%2520on%2520a%2520large-scale%2520corpus%2520of%2520text-intensive%250Aimages%252C%2520KOSMOS-2.5%2520excels%2520in%2520two%2520distinct%2520yet%2520complementary%2520transcription%250Atasks%253A%2520%25281%2529%2520generating%2520spatially-aware%2520text%2520blocks%252C%2520where%2520each%2520block%2520of%2520text%2520is%250Aassigned%2520spatial%2520coordinates%2520within%2520the%2520image%252C%2520and%2520%25282%2529%2520producing%2520structured%250Atext%2520output%2520that%2520captures%2520both%2520style%2520and%2520structure%2520in%2520markdown%2520format.%2520This%250Aunified%2520multimodal%2520literate%2520capability%2520is%2520achieved%2520through%2520a%2520shared%250Adecoder-only%2520autoregressive%2520Transformer%2520architecture%2520and%2520task-specific%2520prompts.%250ABuilding%2520on%2520this%2520foundation%252C%2520we%2520fine-tune%2520KOSMOS-2.5%2520for%2520document%2520understanding%250Atasks%252C%2520resulting%2520in%2520a%2520document%2520understanding%2520generalist%2520named%2520KOSMOS-2.5-CHAT.%250AAdditionally%252C%2520a%2520large%2520corpus%2520of%2520357.4%2520million%2520document%2520pages%2520spanning%2520diverse%250Adomains%2520was%2520curated%2520for%2520pre-training.%2520We%2520evaluate%2520KOSMOS-2.5%2520on%2520two%2520newly%250Aproposed%2520benchmarks%252C%2520OCREval%2520and%2520MarkdownEval%252C%2520for%2520document-level%2520text%250Arecognition%2520and%2520image-to-markdown%2520generation%252C%2520demonstrating%2520impressive%2520literate%250Acapabilities%2520comparable%2520to%2520GPT-4o.%2520KOSMOS-2.5-CHAT%2520achieves%2520performance%250Acomparable%2520to%2520other%2520state-of-the-art%2520generalists%2520that%2520are%2520five%2520times%2520larger%250A%25281.3B%2520vs.%25207B%2529%2520across%2520nine%2520text-rich%2520visual%2520question%2520answering%2520benchmarks.%250AModels%2520and%2520code%2520have%2520been%2520available%2520at%2520%255Curl%257Bhttps%253A//aka.ms/kosmos25%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KOSMOS-2.5%3A%20A%20Multimodal%20Literate%20Model&entry.906535625=Tengchao%20Lv%20and%20Yupan%20Huang%20and%20Jingye%20Chen%20and%20Yuzhong%20Zhao%20and%20Yilin%20Jia%20and%20Lei%20Cui%20and%20Shuming%20Ma%20and%20Yaoyao%20Chang%20and%20Shaohan%20Huang%20and%20Wenhui%20Wang%20and%20Li%20Dong%20and%20Weiyao%20Luo%20and%20Shaoxiang%20Wu%20and%20Guoxin%20Wang%20and%20Cha%20Zhang%20and%20Furu%20Wei&entry.1292438233=%20%20The%20automatic%20reading%20of%20text-intensive%20images%20represents%20a%20significant%0Aadvancement%20toward%20achieving%20Artificial%20General%20Intelligence%20%28AGI%29.%20In%20this%0Apaper%20we%20present%20KOSMOS-2.5%2C%20a%20multimodal%20literate%20model%20for%20machine%20reading%20of%0Atext-intensive%20images.%20Pre-trained%20on%20a%20large-scale%20corpus%20of%20text-intensive%0Aimages%2C%20KOSMOS-2.5%20excels%20in%20two%20distinct%20yet%20complementary%20transcription%0Atasks%3A%20%281%29%20generating%20spatially-aware%20text%20blocks%2C%20where%20each%20block%20of%20text%20is%0Aassigned%20spatial%20coordinates%20within%20the%20image%2C%20and%20%282%29%20producing%20structured%0Atext%20output%20that%20captures%20both%20style%20and%20structure%20in%20markdown%20format.%20This%0Aunified%20multimodal%20literate%20capability%20is%20achieved%20through%20a%20shared%0Adecoder-only%20autoregressive%20Transformer%20architecture%20and%20task-specific%20prompts.%0ABuilding%20on%20this%20foundation%2C%20we%20fine-tune%20KOSMOS-2.5%20for%20document%20understanding%0Atasks%2C%20resulting%20in%20a%20document%20understanding%20generalist%20named%20KOSMOS-2.5-CHAT.%0AAdditionally%2C%20a%20large%20corpus%20of%20357.4%20million%20document%20pages%20spanning%20diverse%0Adomains%20was%20curated%20for%20pre-training.%20We%20evaluate%20KOSMOS-2.5%20on%20two%20newly%0Aproposed%20benchmarks%2C%20OCREval%20and%20MarkdownEval%2C%20for%20document-level%20text%0Arecognition%20and%20image-to-markdown%20generation%2C%20demonstrating%20impressive%20literate%0Acapabilities%20comparable%20to%20GPT-4o.%20KOSMOS-2.5-CHAT%20achieves%20performance%0Acomparable%20to%20other%20state-of-the-art%20generalists%20that%20are%20five%20times%20larger%0A%281.3B%20vs.%207B%29%20across%20nine%20text-rich%20visual%20question%20answering%20benchmarks.%0AModels%20and%20code%20have%20been%20available%20at%20%5Curl%7Bhttps%3A//aka.ms/kosmos25%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11419v2&entry.124074799=Read"},
{"title": "Supervised Representation Learning towards Generalizable Assembly State\n  Recognition", "author": "Tim J. Schoonbeek and Goutham Balachandran and Hans Onvlee and Tim Houben and Shao-Hsuan Hung and Jacek Kustra and Peter H. N. de With and Fons van der Sommen", "abstract": "  Assembly state recognition facilitates the execution of assembly procedures,\noffering feedback to enhance efficiency and minimize errors. However,\nrecognizing assembly states poses challenges in scalability, since parts are\nfrequently updated, and the robustness to execution errors remains\nunderexplored. To address these challenges, this paper proposes an approach\nbased on representation learning and the novel intermediate-state informed loss\nfunction modification (ISIL). ISIL leverages unlabeled transitions between\nstates and demonstrates significant improvements in clustering and\nclassification performance for all tested architectures and losses. Despite\nbeing trained exclusively on images without execution errors, thorough analysis\non error states demonstrates that our approach accurately distinguishes between\ncorrect states and states with various types of execution errors. The\nintegration of the proposed algorithm can offer meaningful assistance to\nworkers and mitigate unexpected losses due to procedural mishaps in industrial\nsettings. The code is available at: https://timschoonbeek.github.io/state_rec\n", "link": "http://arxiv.org/abs/2408.11700v1", "date": "2024-08-21", "relevancy": 2.5841, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5609}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4993}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Representation%20Learning%20towards%20Generalizable%20Assembly%20State%0A%20%20Recognition&body=Title%3A%20Supervised%20Representation%20Learning%20towards%20Generalizable%20Assembly%20State%0A%20%20Recognition%0AAuthor%3A%20Tim%20J.%20Schoonbeek%20and%20Goutham%20Balachandran%20and%20Hans%20Onvlee%20and%20Tim%20Houben%20and%20Shao-Hsuan%20Hung%20and%20Jacek%20Kustra%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20%20%20Assembly%20state%20recognition%20facilitates%20the%20execution%20of%20assembly%20procedures%2C%0Aoffering%20feedback%20to%20enhance%20efficiency%20and%20minimize%20errors.%20However%2C%0Arecognizing%20assembly%20states%20poses%20challenges%20in%20scalability%2C%20since%20parts%20are%0Afrequently%20updated%2C%20and%20the%20robustness%20to%20execution%20errors%20remains%0Aunderexplored.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20an%20approach%0Abased%20on%20representation%20learning%20and%20the%20novel%20intermediate-state%20informed%20loss%0Afunction%20modification%20%28ISIL%29.%20ISIL%20leverages%20unlabeled%20transitions%20between%0Astates%20and%20demonstrates%20significant%20improvements%20in%20clustering%20and%0Aclassification%20performance%20for%20all%20tested%20architectures%20and%20losses.%20Despite%0Abeing%20trained%20exclusively%20on%20images%20without%20execution%20errors%2C%20thorough%20analysis%0Aon%20error%20states%20demonstrates%20that%20our%20approach%20accurately%20distinguishes%20between%0Acorrect%20states%20and%20states%20with%20various%20types%20of%20execution%20errors.%20The%0Aintegration%20of%20the%20proposed%20algorithm%20can%20offer%20meaningful%20assistance%20to%0Aworkers%20and%20mitigate%20unexpected%20losses%20due%20to%20procedural%20mishaps%20in%20industrial%0Asettings.%20The%20code%20is%20available%20at%3A%20https%3A//timschoonbeek.github.io/state_rec%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Representation%2520Learning%2520towards%2520Generalizable%2520Assembly%2520State%250A%2520%2520Recognition%26entry.906535625%3DTim%2520J.%2520Schoonbeek%2520and%2520Goutham%2520Balachandran%2520and%2520Hans%2520Onvlee%2520and%2520Tim%2520Houben%2520and%2520Shao-Hsuan%2520Hung%2520and%2520Jacek%2520Kustra%2520and%2520Peter%2520H.%2520N.%2520de%2520With%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3D%2520%2520Assembly%2520state%2520recognition%2520facilitates%2520the%2520execution%2520of%2520assembly%2520procedures%252C%250Aoffering%2520feedback%2520to%2520enhance%2520efficiency%2520and%2520minimize%2520errors.%2520However%252C%250Arecognizing%2520assembly%2520states%2520poses%2520challenges%2520in%2520scalability%252C%2520since%2520parts%2520are%250Afrequently%2520updated%252C%2520and%2520the%2520robustness%2520to%2520execution%2520errors%2520remains%250Aunderexplored.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520an%2520approach%250Abased%2520on%2520representation%2520learning%2520and%2520the%2520novel%2520intermediate-state%2520informed%2520loss%250Afunction%2520modification%2520%2528ISIL%2529.%2520ISIL%2520leverages%2520unlabeled%2520transitions%2520between%250Astates%2520and%2520demonstrates%2520significant%2520improvements%2520in%2520clustering%2520and%250Aclassification%2520performance%2520for%2520all%2520tested%2520architectures%2520and%2520losses.%2520Despite%250Abeing%2520trained%2520exclusively%2520on%2520images%2520without%2520execution%2520errors%252C%2520thorough%2520analysis%250Aon%2520error%2520states%2520demonstrates%2520that%2520our%2520approach%2520accurately%2520distinguishes%2520between%250Acorrect%2520states%2520and%2520states%2520with%2520various%2520types%2520of%2520execution%2520errors.%2520The%250Aintegration%2520of%2520the%2520proposed%2520algorithm%2520can%2520offer%2520meaningful%2520assistance%2520to%250Aworkers%2520and%2520mitigate%2520unexpected%2520losses%2520due%2520to%2520procedural%2520mishaps%2520in%2520industrial%250Asettings.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//timschoonbeek.github.io/state_rec%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Representation%20Learning%20towards%20Generalizable%20Assembly%20State%0A%20%20Recognition&entry.906535625=Tim%20J.%20Schoonbeek%20and%20Goutham%20Balachandran%20and%20Hans%20Onvlee%20and%20Tim%20Houben%20and%20Shao-Hsuan%20Hung%20and%20Jacek%20Kustra%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen&entry.1292438233=%20%20Assembly%20state%20recognition%20facilitates%20the%20execution%20of%20assembly%20procedures%2C%0Aoffering%20feedback%20to%20enhance%20efficiency%20and%20minimize%20errors.%20However%2C%0Arecognizing%20assembly%20states%20poses%20challenges%20in%20scalability%2C%20since%20parts%20are%0Afrequently%20updated%2C%20and%20the%20robustness%20to%20execution%20errors%20remains%0Aunderexplored.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20an%20approach%0Abased%20on%20representation%20learning%20and%20the%20novel%20intermediate-state%20informed%20loss%0Afunction%20modification%20%28ISIL%29.%20ISIL%20leverages%20unlabeled%20transitions%20between%0Astates%20and%20demonstrates%20significant%20improvements%20in%20clustering%20and%0Aclassification%20performance%20for%20all%20tested%20architectures%20and%20losses.%20Despite%0Abeing%20trained%20exclusively%20on%20images%20without%20execution%20errors%2C%20thorough%20analysis%0Aon%20error%20states%20demonstrates%20that%20our%20approach%20accurately%20distinguishes%20between%0Acorrect%20states%20and%20states%20with%20various%20types%20of%20execution%20errors.%20The%0Aintegration%20of%20the%20proposed%20algorithm%20can%20offer%20meaningful%20assistance%20to%0Aworkers%20and%20mitigate%20unexpected%20losses%20due%20to%20procedural%20mishaps%20in%20industrial%0Asettings.%20The%20code%20is%20available%20at%3A%20https%3A//timschoonbeek.github.io/state_rec%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11700v1&entry.124074799=Read"},
{"title": "Mechanistically analyzing the effects of fine-tuning on procedurally\n  defined tasks", "author": "Samyak Jain and Robert Kirk and Ekdeep Singh Lubana and Robert P. Dick and Hidenori Tanaka and Edward Grefenstette and Tim Rockt\u00e4schel and David Scott Krueger", "abstract": "  Fine-tuning large pre-trained models has become the de facto strategy for\ndeveloping both task-specific and general-purpose machine learning systems,\nincluding developing models that are safe to deploy. Despite its clear\nimportance, there has been minimal work that explains how fine-tuning alters\nthe underlying capabilities learned by a model during pretraining: does\nfine-tuning yield entirely novel capabilities or does it just modulate existing\nones? We address this question empirically in synthetic, controlled settings\nwhere we can use mechanistic interpretability tools (e.g., network pruning and\nprobing) to understand how the model's underlying capabilities are changing. We\nperform an extensive analysis of the effects of fine-tuning in these settings,\nand show that: (i) fine-tuning rarely alters the underlying model capabilities;\n(ii) a minimal transformation, which we call a 'wrapper', is typically learned\non top of the underlying model capabilities, creating the illusion that they\nhave been modified; and (iii) further fine-tuning on a task where such hidden\ncapabilities are relevant leads to sample-efficient 'revival' of the\ncapability, i.e., the model begins reusing these capability after only a few\ngradient steps. This indicates that practitioners can unintentionally remove a\nmodel's safety wrapper merely by fine-tuning it on a, e.g., superficially\nunrelated, downstream task. We additionally perform analysis on language models\ntrained on the TinyStories dataset to support our claims in a more realistic\nsetup.\n", "link": "http://arxiv.org/abs/2311.12786v2", "date": "2024-08-21", "relevancy": 2.5605, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5244}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5098}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanistically%20analyzing%20the%20effects%20of%20fine-tuning%20on%20procedurally%0A%20%20defined%20tasks&body=Title%3A%20Mechanistically%20analyzing%20the%20effects%20of%20fine-tuning%20on%20procedurally%0A%20%20defined%20tasks%0AAuthor%3A%20Samyak%20Jain%20and%20Robert%20Kirk%20and%20Ekdeep%20Singh%20Lubana%20and%20Robert%20P.%20Dick%20and%20Hidenori%20Tanaka%20and%20Edward%20Grefenstette%20and%20Tim%20Rockt%C3%A4schel%20and%20David%20Scott%20Krueger%0AAbstract%3A%20%20%20Fine-tuning%20large%20pre-trained%20models%20has%20become%20the%20de%20facto%20strategy%20for%0Adeveloping%20both%20task-specific%20and%20general-purpose%20machine%20learning%20systems%2C%0Aincluding%20developing%20models%20that%20are%20safe%20to%20deploy.%20Despite%20its%20clear%0Aimportance%2C%20there%20has%20been%20minimal%20work%20that%20explains%20how%20fine-tuning%20alters%0Athe%20underlying%20capabilities%20learned%20by%20a%20model%20during%20pretraining%3A%20does%0Afine-tuning%20yield%20entirely%20novel%20capabilities%20or%20does%20it%20just%20modulate%20existing%0Aones%3F%20We%20address%20this%20question%20empirically%20in%20synthetic%2C%20controlled%20settings%0Awhere%20we%20can%20use%20mechanistic%20interpretability%20tools%20%28e.g.%2C%20network%20pruning%20and%0Aprobing%29%20to%20understand%20how%20the%20model%27s%20underlying%20capabilities%20are%20changing.%20We%0Aperform%20an%20extensive%20analysis%20of%20the%20effects%20of%20fine-tuning%20in%20these%20settings%2C%0Aand%20show%20that%3A%20%28i%29%20fine-tuning%20rarely%20alters%20the%20underlying%20model%20capabilities%3B%0A%28ii%29%20a%20minimal%20transformation%2C%20which%20we%20call%20a%20%27wrapper%27%2C%20is%20typically%20learned%0Aon%20top%20of%20the%20underlying%20model%20capabilities%2C%20creating%20the%20illusion%20that%20they%0Ahave%20been%20modified%3B%20and%20%28iii%29%20further%20fine-tuning%20on%20a%20task%20where%20such%20hidden%0Acapabilities%20are%20relevant%20leads%20to%20sample-efficient%20%27revival%27%20of%20the%0Acapability%2C%20i.e.%2C%20the%20model%20begins%20reusing%20these%20capability%20after%20only%20a%20few%0Agradient%20steps.%20This%20indicates%20that%20practitioners%20can%20unintentionally%20remove%20a%0Amodel%27s%20safety%20wrapper%20merely%20by%20fine-tuning%20it%20on%20a%2C%20e.g.%2C%20superficially%0Aunrelated%2C%20downstream%20task.%20We%20additionally%20perform%20analysis%20on%20language%20models%0Atrained%20on%20the%20TinyStories%20dataset%20to%20support%20our%20claims%20in%20a%20more%20realistic%0Asetup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12786v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanistically%2520analyzing%2520the%2520effects%2520of%2520fine-tuning%2520on%2520procedurally%250A%2520%2520defined%2520tasks%26entry.906535625%3DSamyak%2520Jain%2520and%2520Robert%2520Kirk%2520and%2520Ekdeep%2520Singh%2520Lubana%2520and%2520Robert%2520P.%2520Dick%2520and%2520Hidenori%2520Tanaka%2520and%2520Edward%2520Grefenstette%2520and%2520Tim%2520Rockt%25C3%25A4schel%2520and%2520David%2520Scott%2520Krueger%26entry.1292438233%3D%2520%2520Fine-tuning%2520large%2520pre-trained%2520models%2520has%2520become%2520the%2520de%2520facto%2520strategy%2520for%250Adeveloping%2520both%2520task-specific%2520and%2520general-purpose%2520machine%2520learning%2520systems%252C%250Aincluding%2520developing%2520models%2520that%2520are%2520safe%2520to%2520deploy.%2520Despite%2520its%2520clear%250Aimportance%252C%2520there%2520has%2520been%2520minimal%2520work%2520that%2520explains%2520how%2520fine-tuning%2520alters%250Athe%2520underlying%2520capabilities%2520learned%2520by%2520a%2520model%2520during%2520pretraining%253A%2520does%250Afine-tuning%2520yield%2520entirely%2520novel%2520capabilities%2520or%2520does%2520it%2520just%2520modulate%2520existing%250Aones%253F%2520We%2520address%2520this%2520question%2520empirically%2520in%2520synthetic%252C%2520controlled%2520settings%250Awhere%2520we%2520can%2520use%2520mechanistic%2520interpretability%2520tools%2520%2528e.g.%252C%2520network%2520pruning%2520and%250Aprobing%2529%2520to%2520understand%2520how%2520the%2520model%2527s%2520underlying%2520capabilities%2520are%2520changing.%2520We%250Aperform%2520an%2520extensive%2520analysis%2520of%2520the%2520effects%2520of%2520fine-tuning%2520in%2520these%2520settings%252C%250Aand%2520show%2520that%253A%2520%2528i%2529%2520fine-tuning%2520rarely%2520alters%2520the%2520underlying%2520model%2520capabilities%253B%250A%2528ii%2529%2520a%2520minimal%2520transformation%252C%2520which%2520we%2520call%2520a%2520%2527wrapper%2527%252C%2520is%2520typically%2520learned%250Aon%2520top%2520of%2520the%2520underlying%2520model%2520capabilities%252C%2520creating%2520the%2520illusion%2520that%2520they%250Ahave%2520been%2520modified%253B%2520and%2520%2528iii%2529%2520further%2520fine-tuning%2520on%2520a%2520task%2520where%2520such%2520hidden%250Acapabilities%2520are%2520relevant%2520leads%2520to%2520sample-efficient%2520%2527revival%2527%2520of%2520the%250Acapability%252C%2520i.e.%252C%2520the%2520model%2520begins%2520reusing%2520these%2520capability%2520after%2520only%2520a%2520few%250Agradient%2520steps.%2520This%2520indicates%2520that%2520practitioners%2520can%2520unintentionally%2520remove%2520a%250Amodel%2527s%2520safety%2520wrapper%2520merely%2520by%2520fine-tuning%2520it%2520on%2520a%252C%2520e.g.%252C%2520superficially%250Aunrelated%252C%2520downstream%2520task.%2520We%2520additionally%2520perform%2520analysis%2520on%2520language%2520models%250Atrained%2520on%2520the%2520TinyStories%2520dataset%2520to%2520support%2520our%2520claims%2520in%2520a%2520more%2520realistic%250Asetup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12786v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistically%20analyzing%20the%20effects%20of%20fine-tuning%20on%20procedurally%0A%20%20defined%20tasks&entry.906535625=Samyak%20Jain%20and%20Robert%20Kirk%20and%20Ekdeep%20Singh%20Lubana%20and%20Robert%20P.%20Dick%20and%20Hidenori%20Tanaka%20and%20Edward%20Grefenstette%20and%20Tim%20Rockt%C3%A4schel%20and%20David%20Scott%20Krueger&entry.1292438233=%20%20Fine-tuning%20large%20pre-trained%20models%20has%20become%20the%20de%20facto%20strategy%20for%0Adeveloping%20both%20task-specific%20and%20general-purpose%20machine%20learning%20systems%2C%0Aincluding%20developing%20models%20that%20are%20safe%20to%20deploy.%20Despite%20its%20clear%0Aimportance%2C%20there%20has%20been%20minimal%20work%20that%20explains%20how%20fine-tuning%20alters%0Athe%20underlying%20capabilities%20learned%20by%20a%20model%20during%20pretraining%3A%20does%0Afine-tuning%20yield%20entirely%20novel%20capabilities%20or%20does%20it%20just%20modulate%20existing%0Aones%3F%20We%20address%20this%20question%20empirically%20in%20synthetic%2C%20controlled%20settings%0Awhere%20we%20can%20use%20mechanistic%20interpretability%20tools%20%28e.g.%2C%20network%20pruning%20and%0Aprobing%29%20to%20understand%20how%20the%20model%27s%20underlying%20capabilities%20are%20changing.%20We%0Aperform%20an%20extensive%20analysis%20of%20the%20effects%20of%20fine-tuning%20in%20these%20settings%2C%0Aand%20show%20that%3A%20%28i%29%20fine-tuning%20rarely%20alters%20the%20underlying%20model%20capabilities%3B%0A%28ii%29%20a%20minimal%20transformation%2C%20which%20we%20call%20a%20%27wrapper%27%2C%20is%20typically%20learned%0Aon%20top%20of%20the%20underlying%20model%20capabilities%2C%20creating%20the%20illusion%20that%20they%0Ahave%20been%20modified%3B%20and%20%28iii%29%20further%20fine-tuning%20on%20a%20task%20where%20such%20hidden%0Acapabilities%20are%20relevant%20leads%20to%20sample-efficient%20%27revival%27%20of%20the%0Acapability%2C%20i.e.%2C%20the%20model%20begins%20reusing%20these%20capability%20after%20only%20a%20few%0Agradient%20steps.%20This%20indicates%20that%20practitioners%20can%20unintentionally%20remove%20a%0Amodel%27s%20safety%20wrapper%20merely%20by%20fine-tuning%20it%20on%20a%2C%20e.g.%2C%20superficially%0Aunrelated%2C%20downstream%20task.%20We%20additionally%20perform%20analysis%20on%20language%20models%0Atrained%20on%20the%20TinyStories%20dataset%20to%20support%20our%20claims%20in%20a%20more%20realistic%0Asetup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12786v2&entry.124074799=Read"},
{"title": "Predicting Gradient is Better: Exploring Self-Supervised Learning for\n  SAR ATR with a Joint-Embedding Predictive Architecture", "author": "Weijie Li and Yang Wei and Tianpeng Liu and Yuenan Hou and Yuxuan Li and Zhen Liu and Yongxiang Liu and Li Liu", "abstract": "  The growing Synthetic Aperture Radar (SAR) data has the potential to build a\nfoundation model through Self-Supervised Learning (SSL) methods, which can\nachieve various SAR Automatic Target Recognition (ATR) tasks with pre-training\nin large-scale unlabeled data and fine-tuning in small labeled samples. SSL\naims to construct supervision signals directly from the data, which minimizes\nthe need for expensive expert annotation and maximizes the use of the expanding\ndata pool for a foundational model. This study investigates an effective SSL\nmethod for SAR ATR, which can pave the way for a foundation model in SAR ATR.\nThe primary obstacles faced in SSL for SAR ATR are the small targets in remote\nsensing and speckle noise in SAR images, corresponding to the SSL approach and\nsignals. To overcome these challenges, we present a novel Joint-Embedding\nPredictive Architecture for SAR ATR (SAR-JEPA), which leverages local masked\npatches to predict the multi-scale SAR gradient representations of unseen\ncontext. The key aspect of SAR-JEPA is integrating SAR domain features to\nensure high-quality self-supervised signals as target features. Besides, we\nemploy local masks and multi-scale features to accommodate the various small\ntargets in remote sensing. By fine-tuning and evaluating our framework on three\ntarget recognition datasets (vehicle, ship, and aircraft) with four other\ndatasets as pre-training, we demonstrate its outperformance over other SSL\nmethods and its effectiveness with increasing SAR data. This study showcases\nthe potential of SSL for SAR target recognition across diverse targets, scenes,\nand sensors.Our codes and weights are available in\n\\url{https://github.com/waterdisappear/SAR-JEPA.\n", "link": "http://arxiv.org/abs/2311.15153v5", "date": "2024-08-21", "relevancy": 2.5135, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5098}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.501}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Gradient%20is%20Better%3A%20Exploring%20Self-Supervised%20Learning%20for%0A%20%20SAR%20ATR%20with%20a%20Joint-Embedding%20Predictive%20Architecture&body=Title%3A%20Predicting%20Gradient%20is%20Better%3A%20Exploring%20Self-Supervised%20Learning%20for%0A%20%20SAR%20ATR%20with%20a%20Joint-Embedding%20Predictive%20Architecture%0AAuthor%3A%20Weijie%20Li%20and%20Yang%20Wei%20and%20Tianpeng%20Liu%20and%20Yuenan%20Hou%20and%20Yuxuan%20Li%20and%20Zhen%20Liu%20and%20Yongxiang%20Liu%20and%20Li%20Liu%0AAbstract%3A%20%20%20The%20growing%20Synthetic%20Aperture%20Radar%20%28SAR%29%20data%20has%20the%20potential%20to%20build%20a%0Afoundation%20model%20through%20Self-Supervised%20Learning%20%28SSL%29%20methods%2C%20which%20can%0Aachieve%20various%20SAR%20Automatic%20Target%20Recognition%20%28ATR%29%20tasks%20with%20pre-training%0Ain%20large-scale%20unlabeled%20data%20and%20fine-tuning%20in%20small%20labeled%20samples.%20SSL%0Aaims%20to%20construct%20supervision%20signals%20directly%20from%20the%20data%2C%20which%20minimizes%0Athe%20need%20for%20expensive%20expert%20annotation%20and%20maximizes%20the%20use%20of%20the%20expanding%0Adata%20pool%20for%20a%20foundational%20model.%20This%20study%20investigates%20an%20effective%20SSL%0Amethod%20for%20SAR%20ATR%2C%20which%20can%20pave%20the%20way%20for%20a%20foundation%20model%20in%20SAR%20ATR.%0AThe%20primary%20obstacles%20faced%20in%20SSL%20for%20SAR%20ATR%20are%20the%20small%20targets%20in%20remote%0Asensing%20and%20speckle%20noise%20in%20SAR%20images%2C%20corresponding%20to%20the%20SSL%20approach%20and%0Asignals.%20To%20overcome%20these%20challenges%2C%20we%20present%20a%20novel%20Joint-Embedding%0APredictive%20Architecture%20for%20SAR%20ATR%20%28SAR-JEPA%29%2C%20which%20leverages%20local%20masked%0Apatches%20to%20predict%20the%20multi-scale%20SAR%20gradient%20representations%20of%20unseen%0Acontext.%20The%20key%20aspect%20of%20SAR-JEPA%20is%20integrating%20SAR%20domain%20features%20to%0Aensure%20high-quality%20self-supervised%20signals%20as%20target%20features.%20Besides%2C%20we%0Aemploy%20local%20masks%20and%20multi-scale%20features%20to%20accommodate%20the%20various%20small%0Atargets%20in%20remote%20sensing.%20By%20fine-tuning%20and%20evaluating%20our%20framework%20on%20three%0Atarget%20recognition%20datasets%20%28vehicle%2C%20ship%2C%20and%20aircraft%29%20with%20four%20other%0Adatasets%20as%20pre-training%2C%20we%20demonstrate%20its%20outperformance%20over%20other%20SSL%0Amethods%20and%20its%20effectiveness%20with%20increasing%20SAR%20data.%20This%20study%20showcases%0Athe%20potential%20of%20SSL%20for%20SAR%20target%20recognition%20across%20diverse%20targets%2C%20scenes%2C%0Aand%20sensors.Our%20codes%20and%20weights%20are%20available%20in%0A%5Curl%7Bhttps%3A//github.com/waterdisappear/SAR-JEPA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15153v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Gradient%2520is%2520Better%253A%2520Exploring%2520Self-Supervised%2520Learning%2520for%250A%2520%2520SAR%2520ATR%2520with%2520a%2520Joint-Embedding%2520Predictive%2520Architecture%26entry.906535625%3DWeijie%2520Li%2520and%2520Yang%2520Wei%2520and%2520Tianpeng%2520Liu%2520and%2520Yuenan%2520Hou%2520and%2520Yuxuan%2520Li%2520and%2520Zhen%2520Liu%2520and%2520Yongxiang%2520Liu%2520and%2520Li%2520Liu%26entry.1292438233%3D%2520%2520The%2520growing%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520data%2520has%2520the%2520potential%2520to%2520build%2520a%250Afoundation%2520model%2520through%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520methods%252C%2520which%2520can%250Aachieve%2520various%2520SAR%2520Automatic%2520Target%2520Recognition%2520%2528ATR%2529%2520tasks%2520with%2520pre-training%250Ain%2520large-scale%2520unlabeled%2520data%2520and%2520fine-tuning%2520in%2520small%2520labeled%2520samples.%2520SSL%250Aaims%2520to%2520construct%2520supervision%2520signals%2520directly%2520from%2520the%2520data%252C%2520which%2520minimizes%250Athe%2520need%2520for%2520expensive%2520expert%2520annotation%2520and%2520maximizes%2520the%2520use%2520of%2520the%2520expanding%250Adata%2520pool%2520for%2520a%2520foundational%2520model.%2520This%2520study%2520investigates%2520an%2520effective%2520SSL%250Amethod%2520for%2520SAR%2520ATR%252C%2520which%2520can%2520pave%2520the%2520way%2520for%2520a%2520foundation%2520model%2520in%2520SAR%2520ATR.%250AThe%2520primary%2520obstacles%2520faced%2520in%2520SSL%2520for%2520SAR%2520ATR%2520are%2520the%2520small%2520targets%2520in%2520remote%250Asensing%2520and%2520speckle%2520noise%2520in%2520SAR%2520images%252C%2520corresponding%2520to%2520the%2520SSL%2520approach%2520and%250Asignals.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520present%2520a%2520novel%2520Joint-Embedding%250APredictive%2520Architecture%2520for%2520SAR%2520ATR%2520%2528SAR-JEPA%2529%252C%2520which%2520leverages%2520local%2520masked%250Apatches%2520to%2520predict%2520the%2520multi-scale%2520SAR%2520gradient%2520representations%2520of%2520unseen%250Acontext.%2520The%2520key%2520aspect%2520of%2520SAR-JEPA%2520is%2520integrating%2520SAR%2520domain%2520features%2520to%250Aensure%2520high-quality%2520self-supervised%2520signals%2520as%2520target%2520features.%2520Besides%252C%2520we%250Aemploy%2520local%2520masks%2520and%2520multi-scale%2520features%2520to%2520accommodate%2520the%2520various%2520small%250Atargets%2520in%2520remote%2520sensing.%2520By%2520fine-tuning%2520and%2520evaluating%2520our%2520framework%2520on%2520three%250Atarget%2520recognition%2520datasets%2520%2528vehicle%252C%2520ship%252C%2520and%2520aircraft%2529%2520with%2520four%2520other%250Adatasets%2520as%2520pre-training%252C%2520we%2520demonstrate%2520its%2520outperformance%2520over%2520other%2520SSL%250Amethods%2520and%2520its%2520effectiveness%2520with%2520increasing%2520SAR%2520data.%2520This%2520study%2520showcases%250Athe%2520potential%2520of%2520SSL%2520for%2520SAR%2520target%2520recognition%2520across%2520diverse%2520targets%252C%2520scenes%252C%250Aand%2520sensors.Our%2520codes%2520and%2520weights%2520are%2520available%2520in%250A%255Curl%257Bhttps%253A//github.com/waterdisappear/SAR-JEPA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15153v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Gradient%20is%20Better%3A%20Exploring%20Self-Supervised%20Learning%20for%0A%20%20SAR%20ATR%20with%20a%20Joint-Embedding%20Predictive%20Architecture&entry.906535625=Weijie%20Li%20and%20Yang%20Wei%20and%20Tianpeng%20Liu%20and%20Yuenan%20Hou%20and%20Yuxuan%20Li%20and%20Zhen%20Liu%20and%20Yongxiang%20Liu%20and%20Li%20Liu&entry.1292438233=%20%20The%20growing%20Synthetic%20Aperture%20Radar%20%28SAR%29%20data%20has%20the%20potential%20to%20build%20a%0Afoundation%20model%20through%20Self-Supervised%20Learning%20%28SSL%29%20methods%2C%20which%20can%0Aachieve%20various%20SAR%20Automatic%20Target%20Recognition%20%28ATR%29%20tasks%20with%20pre-training%0Ain%20large-scale%20unlabeled%20data%20and%20fine-tuning%20in%20small%20labeled%20samples.%20SSL%0Aaims%20to%20construct%20supervision%20signals%20directly%20from%20the%20data%2C%20which%20minimizes%0Athe%20need%20for%20expensive%20expert%20annotation%20and%20maximizes%20the%20use%20of%20the%20expanding%0Adata%20pool%20for%20a%20foundational%20model.%20This%20study%20investigates%20an%20effective%20SSL%0Amethod%20for%20SAR%20ATR%2C%20which%20can%20pave%20the%20way%20for%20a%20foundation%20model%20in%20SAR%20ATR.%0AThe%20primary%20obstacles%20faced%20in%20SSL%20for%20SAR%20ATR%20are%20the%20small%20targets%20in%20remote%0Asensing%20and%20speckle%20noise%20in%20SAR%20images%2C%20corresponding%20to%20the%20SSL%20approach%20and%0Asignals.%20To%20overcome%20these%20challenges%2C%20we%20present%20a%20novel%20Joint-Embedding%0APredictive%20Architecture%20for%20SAR%20ATR%20%28SAR-JEPA%29%2C%20which%20leverages%20local%20masked%0Apatches%20to%20predict%20the%20multi-scale%20SAR%20gradient%20representations%20of%20unseen%0Acontext.%20The%20key%20aspect%20of%20SAR-JEPA%20is%20integrating%20SAR%20domain%20features%20to%0Aensure%20high-quality%20self-supervised%20signals%20as%20target%20features.%20Besides%2C%20we%0Aemploy%20local%20masks%20and%20multi-scale%20features%20to%20accommodate%20the%20various%20small%0Atargets%20in%20remote%20sensing.%20By%20fine-tuning%20and%20evaluating%20our%20framework%20on%20three%0Atarget%20recognition%20datasets%20%28vehicle%2C%20ship%2C%20and%20aircraft%29%20with%20four%20other%0Adatasets%20as%20pre-training%2C%20we%20demonstrate%20its%20outperformance%20over%20other%20SSL%0Amethods%20and%20its%20effectiveness%20with%20increasing%20SAR%20data.%20This%20study%20showcases%0Athe%20potential%20of%20SSL%20for%20SAR%20target%20recognition%20across%20diverse%20targets%2C%20scenes%2C%0Aand%20sensors.Our%20codes%20and%20weights%20are%20available%20in%0A%5Curl%7Bhttps%3A//github.com/waterdisappear/SAR-JEPA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15153v5&entry.124074799=Read"},
{"title": "Generative AI in Industrial Machine Vision -- A Review", "author": "Hans Aoyang Zhou and Dominik Wolfschl\u00e4ger and Constantinos Florides and Jonas Werheid and Hannes Behnen and Jan-Henrick Woltersmann and Tiago C. Pinto and Marco Kemmerling and Anas Abdelrazeq and Robert H. Schmitt", "abstract": "  Machine vision enhances automation, quality control, and operational\nefficiency in industrial applications by enabling machines to interpret and act\non visual data. While traditional computer vision algorithms and approaches\nremain widely utilized, machine learning has become pivotal in current research\nactivities. In particular, generative AI demonstrates promising potential by\nimproving pattern recognition capabilities, through data augmentation,\nincreasing image resolution, and identifying anomalies for quality control.\nHowever, the application of generative AI in machine vision is still in its\nearly stages due to challenges in data diversity, computational requirements,\nand the necessity for robust validation methods. A comprehensive literature\nreview is essential to understand the current state of generative AI in\nindustrial machine vision, focusing on recent advancements, applications, and\nresearch trends. Thus, a literature review based on the PRISMA guidelines was\nconducted, analyzing over 1,200 papers on generative AI in industrial machine\nvision. Our findings reveal various patterns in current research, with the\nprimary use of generative AI being data augmentation, for machine vision tasks\nsuch as classification and object detection. Furthermore, we gather a\ncollection of application challenges together with data requirements to enable\na successful application of generative AI in industrial machine vision. This\noverview aims to provide researchers with insights into the different areas and\napplications within current research, highlighting significant advancements and\nidentifying opportunities for future work.\n", "link": "http://arxiv.org/abs/2408.10775v2", "date": "2024-08-21", "relevancy": 2.4568, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5088}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4894}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20in%20Industrial%20Machine%20Vision%20--%20A%20Review&body=Title%3A%20Generative%20AI%20in%20Industrial%20Machine%20Vision%20--%20A%20Review%0AAuthor%3A%20Hans%20Aoyang%20Zhou%20and%20Dominik%20Wolfschl%C3%A4ger%20and%20Constantinos%20Florides%20and%20Jonas%20Werheid%20and%20Hannes%20Behnen%20and%20Jan-Henrick%20Woltersmann%20and%20Tiago%20C.%20Pinto%20and%20Marco%20Kemmerling%20and%20Anas%20Abdelrazeq%20and%20Robert%20H.%20Schmitt%0AAbstract%3A%20%20%20Machine%20vision%20enhances%20automation%2C%20quality%20control%2C%20and%20operational%0Aefficiency%20in%20industrial%20applications%20by%20enabling%20machines%20to%20interpret%20and%20act%0Aon%20visual%20data.%20While%20traditional%20computer%20vision%20algorithms%20and%20approaches%0Aremain%20widely%20utilized%2C%20machine%20learning%20has%20become%20pivotal%20in%20current%20research%0Aactivities.%20In%20particular%2C%20generative%20AI%20demonstrates%20promising%20potential%20by%0Aimproving%20pattern%20recognition%20capabilities%2C%20through%20data%20augmentation%2C%0Aincreasing%20image%20resolution%2C%20and%20identifying%20anomalies%20for%20quality%20control.%0AHowever%2C%20the%20application%20of%20generative%20AI%20in%20machine%20vision%20is%20still%20in%20its%0Aearly%20stages%20due%20to%20challenges%20in%20data%20diversity%2C%20computational%20requirements%2C%0Aand%20the%20necessity%20for%20robust%20validation%20methods.%20A%20comprehensive%20literature%0Areview%20is%20essential%20to%20understand%20the%20current%20state%20of%20generative%20AI%20in%0Aindustrial%20machine%20vision%2C%20focusing%20on%20recent%20advancements%2C%20applications%2C%20and%0Aresearch%20trends.%20Thus%2C%20a%20literature%20review%20based%20on%20the%20PRISMA%20guidelines%20was%0Aconducted%2C%20analyzing%20over%201%2C200%20papers%20on%20generative%20AI%20in%20industrial%20machine%0Avision.%20Our%20findings%20reveal%20various%20patterns%20in%20current%20research%2C%20with%20the%0Aprimary%20use%20of%20generative%20AI%20being%20data%20augmentation%2C%20for%20machine%20vision%20tasks%0Asuch%20as%20classification%20and%20object%20detection.%20Furthermore%2C%20we%20gather%20a%0Acollection%20of%20application%20challenges%20together%20with%20data%20requirements%20to%20enable%0Aa%20successful%20application%20of%20generative%20AI%20in%20industrial%20machine%20vision.%20This%0Aoverview%20aims%20to%20provide%20researchers%20with%20insights%20into%20the%20different%20areas%20and%0Aapplications%20within%20current%20research%2C%20highlighting%20significant%20advancements%20and%0Aidentifying%20opportunities%20for%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520in%2520Industrial%2520Machine%2520Vision%2520--%2520A%2520Review%26entry.906535625%3DHans%2520Aoyang%2520Zhou%2520and%2520Dominik%2520Wolfschl%25C3%25A4ger%2520and%2520Constantinos%2520Florides%2520and%2520Jonas%2520Werheid%2520and%2520Hannes%2520Behnen%2520and%2520Jan-Henrick%2520Woltersmann%2520and%2520Tiago%2520C.%2520Pinto%2520and%2520Marco%2520Kemmerling%2520and%2520Anas%2520Abdelrazeq%2520and%2520Robert%2520H.%2520Schmitt%26entry.1292438233%3D%2520%2520Machine%2520vision%2520enhances%2520automation%252C%2520quality%2520control%252C%2520and%2520operational%250Aefficiency%2520in%2520industrial%2520applications%2520by%2520enabling%2520machines%2520to%2520interpret%2520and%2520act%250Aon%2520visual%2520data.%2520While%2520traditional%2520computer%2520vision%2520algorithms%2520and%2520approaches%250Aremain%2520widely%2520utilized%252C%2520machine%2520learning%2520has%2520become%2520pivotal%2520in%2520current%2520research%250Aactivities.%2520In%2520particular%252C%2520generative%2520AI%2520demonstrates%2520promising%2520potential%2520by%250Aimproving%2520pattern%2520recognition%2520capabilities%252C%2520through%2520data%2520augmentation%252C%250Aincreasing%2520image%2520resolution%252C%2520and%2520identifying%2520anomalies%2520for%2520quality%2520control.%250AHowever%252C%2520the%2520application%2520of%2520generative%2520AI%2520in%2520machine%2520vision%2520is%2520still%2520in%2520its%250Aearly%2520stages%2520due%2520to%2520challenges%2520in%2520data%2520diversity%252C%2520computational%2520requirements%252C%250Aand%2520the%2520necessity%2520for%2520robust%2520validation%2520methods.%2520A%2520comprehensive%2520literature%250Areview%2520is%2520essential%2520to%2520understand%2520the%2520current%2520state%2520of%2520generative%2520AI%2520in%250Aindustrial%2520machine%2520vision%252C%2520focusing%2520on%2520recent%2520advancements%252C%2520applications%252C%2520and%250Aresearch%2520trends.%2520Thus%252C%2520a%2520literature%2520review%2520based%2520on%2520the%2520PRISMA%2520guidelines%2520was%250Aconducted%252C%2520analyzing%2520over%25201%252C200%2520papers%2520on%2520generative%2520AI%2520in%2520industrial%2520machine%250Avision.%2520Our%2520findings%2520reveal%2520various%2520patterns%2520in%2520current%2520research%252C%2520with%2520the%250Aprimary%2520use%2520of%2520generative%2520AI%2520being%2520data%2520augmentation%252C%2520for%2520machine%2520vision%2520tasks%250Asuch%2520as%2520classification%2520and%2520object%2520detection.%2520Furthermore%252C%2520we%2520gather%2520a%250Acollection%2520of%2520application%2520challenges%2520together%2520with%2520data%2520requirements%2520to%2520enable%250Aa%2520successful%2520application%2520of%2520generative%2520AI%2520in%2520industrial%2520machine%2520vision.%2520This%250Aoverview%2520aims%2520to%2520provide%2520researchers%2520with%2520insights%2520into%2520the%2520different%2520areas%2520and%250Aapplications%2520within%2520current%2520research%252C%2520highlighting%2520significant%2520advancements%2520and%250Aidentifying%2520opportunities%2520for%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20in%20Industrial%20Machine%20Vision%20--%20A%20Review&entry.906535625=Hans%20Aoyang%20Zhou%20and%20Dominik%20Wolfschl%C3%A4ger%20and%20Constantinos%20Florides%20and%20Jonas%20Werheid%20and%20Hannes%20Behnen%20and%20Jan-Henrick%20Woltersmann%20and%20Tiago%20C.%20Pinto%20and%20Marco%20Kemmerling%20and%20Anas%20Abdelrazeq%20and%20Robert%20H.%20Schmitt&entry.1292438233=%20%20Machine%20vision%20enhances%20automation%2C%20quality%20control%2C%20and%20operational%0Aefficiency%20in%20industrial%20applications%20by%20enabling%20machines%20to%20interpret%20and%20act%0Aon%20visual%20data.%20While%20traditional%20computer%20vision%20algorithms%20and%20approaches%0Aremain%20widely%20utilized%2C%20machine%20learning%20has%20become%20pivotal%20in%20current%20research%0Aactivities.%20In%20particular%2C%20generative%20AI%20demonstrates%20promising%20potential%20by%0Aimproving%20pattern%20recognition%20capabilities%2C%20through%20data%20augmentation%2C%0Aincreasing%20image%20resolution%2C%20and%20identifying%20anomalies%20for%20quality%20control.%0AHowever%2C%20the%20application%20of%20generative%20AI%20in%20machine%20vision%20is%20still%20in%20its%0Aearly%20stages%20due%20to%20challenges%20in%20data%20diversity%2C%20computational%20requirements%2C%0Aand%20the%20necessity%20for%20robust%20validation%20methods.%20A%20comprehensive%20literature%0Areview%20is%20essential%20to%20understand%20the%20current%20state%20of%20generative%20AI%20in%0Aindustrial%20machine%20vision%2C%20focusing%20on%20recent%20advancements%2C%20applications%2C%20and%0Aresearch%20trends.%20Thus%2C%20a%20literature%20review%20based%20on%20the%20PRISMA%20guidelines%20was%0Aconducted%2C%20analyzing%20over%201%2C200%20papers%20on%20generative%20AI%20in%20industrial%20machine%0Avision.%20Our%20findings%20reveal%20various%20patterns%20in%20current%20research%2C%20with%20the%0Aprimary%20use%20of%20generative%20AI%20being%20data%20augmentation%2C%20for%20machine%20vision%20tasks%0Asuch%20as%20classification%20and%20object%20detection.%20Furthermore%2C%20we%20gather%20a%0Acollection%20of%20application%20challenges%20together%20with%20data%20requirements%20to%20enable%0Aa%20successful%20application%20of%20generative%20AI%20in%20industrial%20machine%20vision.%20This%0Aoverview%20aims%20to%20provide%20researchers%20with%20insights%20into%20the%20different%20areas%20and%0Aapplications%20within%20current%20research%2C%20highlighting%20significant%20advancements%20and%0Aidentifying%20opportunities%20for%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10775v2&entry.124074799=Read"},
{"title": "CMAB: A First National-Scale Multi-Attribute Building Dataset in China\n  Derived from Open Source Data and GeoAI", "author": "Yecheng Zhang and Huimin Zhao and Ying Long", "abstract": "  Rapidly acquiring three-dimensional (3D) building data, including geometric\nattributes like rooftop, height and orientations, as well as indicative\nattributes like function, quality, and age, is essential for accurate urban\nanalysis, simulations, and policy updates. Current building datasets suffer\nfrom incomplete coverage of building multi-attributes. This paper introduces a\ngeospatial artificial intelligence (GeoAI) framework for large-scale building\nmodeling, presenting the first national-scale Multi-Attribute Building dataset\n(CMAB), covering 3,667 spatial cities, 29 million buildings, and 21.3 billion\nsquare meters of rooftops with an F1-Score of 89.93% in OCRNet-based\nextraction, totaling 337.7 billion cubic meters of building stock. We trained\nbootstrap aggregated XGBoost models with city administrative classifications,\nincorporating features such as morphology, location, and function. Using\nmulti-source data, including billions of high-resolution Google Earth images\nand 60 million street view images (SVIs), we generated rooftop, height,\nfunction, age, and quality attributes for each building. Accuracy was validated\nthrough model benchmarks, existing similar products, and manual SVI validation,\nmostly above 80%. Our dataset and results are crucial for global SDGs and urban\nplanning.\n", "link": "http://arxiv.org/abs/2408.05891v2", "date": "2024-08-21", "relevancy": 2.4423, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4961}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4961}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMAB%3A%20A%20First%20National-Scale%20Multi-Attribute%20Building%20Dataset%20in%20China%0A%20%20Derived%20from%20Open%20Source%20Data%20and%20GeoAI&body=Title%3A%20CMAB%3A%20A%20First%20National-Scale%20Multi-Attribute%20Building%20Dataset%20in%20China%0A%20%20Derived%20from%20Open%20Source%20Data%20and%20GeoAI%0AAuthor%3A%20Yecheng%20Zhang%20and%20Huimin%20Zhao%20and%20Ying%20Long%0AAbstract%3A%20%20%20Rapidly%20acquiring%20three-dimensional%20%283D%29%20building%20data%2C%20including%20geometric%0Aattributes%20like%20rooftop%2C%20height%20and%20orientations%2C%20as%20well%20as%20indicative%0Aattributes%20like%20function%2C%20quality%2C%20and%20age%2C%20is%20essential%20for%20accurate%20urban%0Aanalysis%2C%20simulations%2C%20and%20policy%20updates.%20Current%20building%20datasets%20suffer%0Afrom%20incomplete%20coverage%20of%20building%20multi-attributes.%20This%20paper%20introduces%20a%0Ageospatial%20artificial%20intelligence%20%28GeoAI%29%20framework%20for%20large-scale%20building%0Amodeling%2C%20presenting%20the%20first%20national-scale%20Multi-Attribute%20Building%20dataset%0A%28CMAB%29%2C%20covering%203%2C667%20spatial%20cities%2C%2029%20million%20buildings%2C%20and%2021.3%20billion%0Asquare%20meters%20of%20rooftops%20with%20an%20F1-Score%20of%2089.93%25%20in%20OCRNet-based%0Aextraction%2C%20totaling%20337.7%20billion%20cubic%20meters%20of%20building%20stock.%20We%20trained%0Abootstrap%20aggregated%20XGBoost%20models%20with%20city%20administrative%20classifications%2C%0Aincorporating%20features%20such%20as%20morphology%2C%20location%2C%20and%20function.%20Using%0Amulti-source%20data%2C%20including%20billions%20of%20high-resolution%20Google%20Earth%20images%0Aand%2060%20million%20street%20view%20images%20%28SVIs%29%2C%20we%20generated%20rooftop%2C%20height%2C%0Afunction%2C%20age%2C%20and%20quality%20attributes%20for%20each%20building.%20Accuracy%20was%20validated%0Athrough%20model%20benchmarks%2C%20existing%20similar%20products%2C%20and%20manual%20SVI%20validation%2C%0Amostly%20above%2080%25.%20Our%20dataset%20and%20results%20are%20crucial%20for%20global%20SDGs%20and%20urban%0Aplanning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMAB%253A%2520A%2520First%2520National-Scale%2520Multi-Attribute%2520Building%2520Dataset%2520in%2520China%250A%2520%2520Derived%2520from%2520Open%2520Source%2520Data%2520and%2520GeoAI%26entry.906535625%3DYecheng%2520Zhang%2520and%2520Huimin%2520Zhao%2520and%2520Ying%2520Long%26entry.1292438233%3D%2520%2520Rapidly%2520acquiring%2520three-dimensional%2520%25283D%2529%2520building%2520data%252C%2520including%2520geometric%250Aattributes%2520like%2520rooftop%252C%2520height%2520and%2520orientations%252C%2520as%2520well%2520as%2520indicative%250Aattributes%2520like%2520function%252C%2520quality%252C%2520and%2520age%252C%2520is%2520essential%2520for%2520accurate%2520urban%250Aanalysis%252C%2520simulations%252C%2520and%2520policy%2520updates.%2520Current%2520building%2520datasets%2520suffer%250Afrom%2520incomplete%2520coverage%2520of%2520building%2520multi-attributes.%2520This%2520paper%2520introduces%2520a%250Ageospatial%2520artificial%2520intelligence%2520%2528GeoAI%2529%2520framework%2520for%2520large-scale%2520building%250Amodeling%252C%2520presenting%2520the%2520first%2520national-scale%2520Multi-Attribute%2520Building%2520dataset%250A%2528CMAB%2529%252C%2520covering%25203%252C667%2520spatial%2520cities%252C%252029%2520million%2520buildings%252C%2520and%252021.3%2520billion%250Asquare%2520meters%2520of%2520rooftops%2520with%2520an%2520F1-Score%2520of%252089.93%2525%2520in%2520OCRNet-based%250Aextraction%252C%2520totaling%2520337.7%2520billion%2520cubic%2520meters%2520of%2520building%2520stock.%2520We%2520trained%250Abootstrap%2520aggregated%2520XGBoost%2520models%2520with%2520city%2520administrative%2520classifications%252C%250Aincorporating%2520features%2520such%2520as%2520morphology%252C%2520location%252C%2520and%2520function.%2520Using%250Amulti-source%2520data%252C%2520including%2520billions%2520of%2520high-resolution%2520Google%2520Earth%2520images%250Aand%252060%2520million%2520street%2520view%2520images%2520%2528SVIs%2529%252C%2520we%2520generated%2520rooftop%252C%2520height%252C%250Afunction%252C%2520age%252C%2520and%2520quality%2520attributes%2520for%2520each%2520building.%2520Accuracy%2520was%2520validated%250Athrough%2520model%2520benchmarks%252C%2520existing%2520similar%2520products%252C%2520and%2520manual%2520SVI%2520validation%252C%250Amostly%2520above%252080%2525.%2520Our%2520dataset%2520and%2520results%2520are%2520crucial%2520for%2520global%2520SDGs%2520and%2520urban%250Aplanning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMAB%3A%20A%20First%20National-Scale%20Multi-Attribute%20Building%20Dataset%20in%20China%0A%20%20Derived%20from%20Open%20Source%20Data%20and%20GeoAI&entry.906535625=Yecheng%20Zhang%20and%20Huimin%20Zhao%20and%20Ying%20Long&entry.1292438233=%20%20Rapidly%20acquiring%20three-dimensional%20%283D%29%20building%20data%2C%20including%20geometric%0Aattributes%20like%20rooftop%2C%20height%20and%20orientations%2C%20as%20well%20as%20indicative%0Aattributes%20like%20function%2C%20quality%2C%20and%20age%2C%20is%20essential%20for%20accurate%20urban%0Aanalysis%2C%20simulations%2C%20and%20policy%20updates.%20Current%20building%20datasets%20suffer%0Afrom%20incomplete%20coverage%20of%20building%20multi-attributes.%20This%20paper%20introduces%20a%0Ageospatial%20artificial%20intelligence%20%28GeoAI%29%20framework%20for%20large-scale%20building%0Amodeling%2C%20presenting%20the%20first%20national-scale%20Multi-Attribute%20Building%20dataset%0A%28CMAB%29%2C%20covering%203%2C667%20spatial%20cities%2C%2029%20million%20buildings%2C%20and%2021.3%20billion%0Asquare%20meters%20of%20rooftops%20with%20an%20F1-Score%20of%2089.93%25%20in%20OCRNet-based%0Aextraction%2C%20totaling%20337.7%20billion%20cubic%20meters%20of%20building%20stock.%20We%20trained%0Abootstrap%20aggregated%20XGBoost%20models%20with%20city%20administrative%20classifications%2C%0Aincorporating%20features%20such%20as%20morphology%2C%20location%2C%20and%20function.%20Using%0Amulti-source%20data%2C%20including%20billions%20of%20high-resolution%20Google%20Earth%20images%0Aand%2060%20million%20street%20view%20images%20%28SVIs%29%2C%20we%20generated%20rooftop%2C%20height%2C%0Afunction%2C%20age%2C%20and%20quality%20attributes%20for%20each%20building.%20Accuracy%20was%20validated%0Athrough%20model%20benchmarks%2C%20existing%20similar%20products%2C%20and%20manual%20SVI%20validation%2C%0Amostly%20above%2080%25.%20Our%20dataset%20and%20results%20are%20crucial%20for%20global%20SDGs%20and%20urban%0Aplanning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05891v2&entry.124074799=Read"},
{"title": "RaNDT SLAM: Radar SLAM Based on Intensity-Augmented Normal Distributions\n  Transform", "author": "Maximilian Hilger and Nils Mandischer and Burkhard Corves", "abstract": "  Rescue robotics sets high requirements to perception algorithms due to the\nunstructured and potentially vision-denied environments. Pivoting\nFrequency-Modulated Continuous Wave radars are an emerging sensing modality for\nSLAM in this kind of environment. However, the complex noise characteristics of\nradar SLAM makes, particularly indoor, applications computationally demanding\nand slow. In this work, we introduce a novel radar SLAM framework, RaNDT SLAM,\nthat operates fast and generates accurate robot trajectories. The method is\nbased on the Normal Distributions Transform augmented by radar intensity\nmeasures. Motion estimation is based on fusion of motion model, IMU data, and\nregistration of the intensity-augmented Normal Distributions Transform. We\nevaluate RaNDT SLAM in a new benchmark dataset and the Oxford Radar RobotCar\ndataset. The new dataset contains indoor and outdoor environments besides\nmultiple sensing modalities (LiDAR, radar, and IMU).\n", "link": "http://arxiv.org/abs/2408.11576v1", "date": "2024-08-21", "relevancy": 2.4359, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6583}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5776}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaNDT%20SLAM%3A%20Radar%20SLAM%20Based%20on%20Intensity-Augmented%20Normal%20Distributions%0A%20%20Transform&body=Title%3A%20RaNDT%20SLAM%3A%20Radar%20SLAM%20Based%20on%20Intensity-Augmented%20Normal%20Distributions%0A%20%20Transform%0AAuthor%3A%20Maximilian%20Hilger%20and%20Nils%20Mandischer%20and%20Burkhard%20Corves%0AAbstract%3A%20%20%20Rescue%20robotics%20sets%20high%20requirements%20to%20perception%20algorithms%20due%20to%20the%0Aunstructured%20and%20potentially%20vision-denied%20environments.%20Pivoting%0AFrequency-Modulated%20Continuous%20Wave%20radars%20are%20an%20emerging%20sensing%20modality%20for%0ASLAM%20in%20this%20kind%20of%20environment.%20However%2C%20the%20complex%20noise%20characteristics%20of%0Aradar%20SLAM%20makes%2C%20particularly%20indoor%2C%20applications%20computationally%20demanding%0Aand%20slow.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20radar%20SLAM%20framework%2C%20RaNDT%20SLAM%2C%0Athat%20operates%20fast%20and%20generates%20accurate%20robot%20trajectories.%20The%20method%20is%0Abased%20on%20the%20Normal%20Distributions%20Transform%20augmented%20by%20radar%20intensity%0Ameasures.%20Motion%20estimation%20is%20based%20on%20fusion%20of%20motion%20model%2C%20IMU%20data%2C%20and%0Aregistration%20of%20the%20intensity-augmented%20Normal%20Distributions%20Transform.%20We%0Aevaluate%20RaNDT%20SLAM%20in%20a%20new%20benchmark%20dataset%20and%20the%20Oxford%20Radar%20RobotCar%0Adataset.%20The%20new%20dataset%20contains%20indoor%20and%20outdoor%20environments%20besides%0Amultiple%20sensing%20modalities%20%28LiDAR%2C%20radar%2C%20and%20IMU%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaNDT%2520SLAM%253A%2520Radar%2520SLAM%2520Based%2520on%2520Intensity-Augmented%2520Normal%2520Distributions%250A%2520%2520Transform%26entry.906535625%3DMaximilian%2520Hilger%2520and%2520Nils%2520Mandischer%2520and%2520Burkhard%2520Corves%26entry.1292438233%3D%2520%2520Rescue%2520robotics%2520sets%2520high%2520requirements%2520to%2520perception%2520algorithms%2520due%2520to%2520the%250Aunstructured%2520and%2520potentially%2520vision-denied%2520environments.%2520Pivoting%250AFrequency-Modulated%2520Continuous%2520Wave%2520radars%2520are%2520an%2520emerging%2520sensing%2520modality%2520for%250ASLAM%2520in%2520this%2520kind%2520of%2520environment.%2520However%252C%2520the%2520complex%2520noise%2520characteristics%2520of%250Aradar%2520SLAM%2520makes%252C%2520particularly%2520indoor%252C%2520applications%2520computationally%2520demanding%250Aand%2520slow.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520radar%2520SLAM%2520framework%252C%2520RaNDT%2520SLAM%252C%250Athat%2520operates%2520fast%2520and%2520generates%2520accurate%2520robot%2520trajectories.%2520The%2520method%2520is%250Abased%2520on%2520the%2520Normal%2520Distributions%2520Transform%2520augmented%2520by%2520radar%2520intensity%250Ameasures.%2520Motion%2520estimation%2520is%2520based%2520on%2520fusion%2520of%2520motion%2520model%252C%2520IMU%2520data%252C%2520and%250Aregistration%2520of%2520the%2520intensity-augmented%2520Normal%2520Distributions%2520Transform.%2520We%250Aevaluate%2520RaNDT%2520SLAM%2520in%2520a%2520new%2520benchmark%2520dataset%2520and%2520the%2520Oxford%2520Radar%2520RobotCar%250Adataset.%2520The%2520new%2520dataset%2520contains%2520indoor%2520and%2520outdoor%2520environments%2520besides%250Amultiple%2520sensing%2520modalities%2520%2528LiDAR%252C%2520radar%252C%2520and%2520IMU%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaNDT%20SLAM%3A%20Radar%20SLAM%20Based%20on%20Intensity-Augmented%20Normal%20Distributions%0A%20%20Transform&entry.906535625=Maximilian%20Hilger%20and%20Nils%20Mandischer%20and%20Burkhard%20Corves&entry.1292438233=%20%20Rescue%20robotics%20sets%20high%20requirements%20to%20perception%20algorithms%20due%20to%20the%0Aunstructured%20and%20potentially%20vision-denied%20environments.%20Pivoting%0AFrequency-Modulated%20Continuous%20Wave%20radars%20are%20an%20emerging%20sensing%20modality%20for%0ASLAM%20in%20this%20kind%20of%20environment.%20However%2C%20the%20complex%20noise%20characteristics%20of%0Aradar%20SLAM%20makes%2C%20particularly%20indoor%2C%20applications%20computationally%20demanding%0Aand%20slow.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20radar%20SLAM%20framework%2C%20RaNDT%20SLAM%2C%0Athat%20operates%20fast%20and%20generates%20accurate%20robot%20trajectories.%20The%20method%20is%0Abased%20on%20the%20Normal%20Distributions%20Transform%20augmented%20by%20radar%20intensity%0Ameasures.%20Motion%20estimation%20is%20based%20on%20fusion%20of%20motion%20model%2C%20IMU%20data%2C%20and%0Aregistration%20of%20the%20intensity-augmented%20Normal%20Distributions%20Transform.%20We%0Aevaluate%20RaNDT%20SLAM%20in%20a%20new%20benchmark%20dataset%20and%20the%20Oxford%20Radar%20RobotCar%0Adataset.%20The%20new%20dataset%20contains%20indoor%20and%20outdoor%20environments%20besides%0Amultiple%20sensing%20modalities%20%28LiDAR%2C%20radar%2C%20and%20IMU%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11576v1&entry.124074799=Read"},
{"title": "Universal Time-Series Representation Learning: A Survey", "author": "Patara Trirat and Yooju Shin and Junhyeok Kang and Youngeun Nam and Jihye Na and Minyoung Bae and Joeun Kim and Byunghyun Kim and Jae-Gil Lee", "abstract": "  Time-series data exists in every corner of real-world systems and services,\nranging from satellites in the sky to wearable devices on human bodies.\nLearning representations by extracting and inferring valuable information from\nthese time series is crucial for understanding the complex dynamics of\nparticular phenomena and enabling informed decisions. With the learned\nrepresentations, we can perform numerous downstream analyses more effectively.\nAmong several approaches, deep learning has demonstrated remarkable performance\nin extracting hidden patterns and features from time-series data without manual\nfeature engineering. This survey first presents a novel taxonomy based on three\nfundamental elements in designing state-of-the-art universal representation\nlearning methods for time series. According to the proposed taxonomy, we\ncomprehensively review existing studies and discuss their intuitions and\ninsights into how these methods enhance the quality of learned representations.\nFinally, as a guideline for future studies, we summarize commonly used\nexperimental setups and datasets and discuss several promising research\ndirections. An up-to-date corresponding resource is available at\nhttps://github.com/itouchz/awesome-deep-time-series-representations.\n", "link": "http://arxiv.org/abs/2401.03717v2", "date": "2024-08-21", "relevancy": 2.4298, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5137}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4738}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Time-Series%20Representation%20Learning%3A%20A%20Survey&body=Title%3A%20Universal%20Time-Series%20Representation%20Learning%3A%20A%20Survey%0AAuthor%3A%20Patara%20Trirat%20and%20Yooju%20Shin%20and%20Junhyeok%20Kang%20and%20Youngeun%20Nam%20and%20Jihye%20Na%20and%20Minyoung%20Bae%20and%20Joeun%20Kim%20and%20Byunghyun%20Kim%20and%20Jae-Gil%20Lee%0AAbstract%3A%20%20%20Time-series%20data%20exists%20in%20every%20corner%20of%20real-world%20systems%20and%20services%2C%0Aranging%20from%20satellites%20in%20the%20sky%20to%20wearable%20devices%20on%20human%20bodies.%0ALearning%20representations%20by%20extracting%20and%20inferring%20valuable%20information%20from%0Athese%20time%20series%20is%20crucial%20for%20understanding%20the%20complex%20dynamics%20of%0Aparticular%20phenomena%20and%20enabling%20informed%20decisions.%20With%20the%20learned%0Arepresentations%2C%20we%20can%20perform%20numerous%20downstream%20analyses%20more%20effectively.%0AAmong%20several%20approaches%2C%20deep%20learning%20has%20demonstrated%20remarkable%20performance%0Ain%20extracting%20hidden%20patterns%20and%20features%20from%20time-series%20data%20without%20manual%0Afeature%20engineering.%20This%20survey%20first%20presents%20a%20novel%20taxonomy%20based%20on%20three%0Afundamental%20elements%20in%20designing%20state-of-the-art%20universal%20representation%0Alearning%20methods%20for%20time%20series.%20According%20to%20the%20proposed%20taxonomy%2C%20we%0Acomprehensively%20review%20existing%20studies%20and%20discuss%20their%20intuitions%20and%0Ainsights%20into%20how%20these%20methods%20enhance%20the%20quality%20of%20learned%20representations.%0AFinally%2C%20as%20a%20guideline%20for%20future%20studies%2C%20we%20summarize%20commonly%20used%0Aexperimental%20setups%20and%20datasets%20and%20discuss%20several%20promising%20research%0Adirections.%20An%20up-to-date%20corresponding%20resource%20is%20available%20at%0Ahttps%3A//github.com/itouchz/awesome-deep-time-series-representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Time-Series%2520Representation%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DPatara%2520Trirat%2520and%2520Yooju%2520Shin%2520and%2520Junhyeok%2520Kang%2520and%2520Youngeun%2520Nam%2520and%2520Jihye%2520Na%2520and%2520Minyoung%2520Bae%2520and%2520Joeun%2520Kim%2520and%2520Byunghyun%2520Kim%2520and%2520Jae-Gil%2520Lee%26entry.1292438233%3D%2520%2520Time-series%2520data%2520exists%2520in%2520every%2520corner%2520of%2520real-world%2520systems%2520and%2520services%252C%250Aranging%2520from%2520satellites%2520in%2520the%2520sky%2520to%2520wearable%2520devices%2520on%2520human%2520bodies.%250ALearning%2520representations%2520by%2520extracting%2520and%2520inferring%2520valuable%2520information%2520from%250Athese%2520time%2520series%2520is%2520crucial%2520for%2520understanding%2520the%2520complex%2520dynamics%2520of%250Aparticular%2520phenomena%2520and%2520enabling%2520informed%2520decisions.%2520With%2520the%2520learned%250Arepresentations%252C%2520we%2520can%2520perform%2520numerous%2520downstream%2520analyses%2520more%2520effectively.%250AAmong%2520several%2520approaches%252C%2520deep%2520learning%2520has%2520demonstrated%2520remarkable%2520performance%250Ain%2520extracting%2520hidden%2520patterns%2520and%2520features%2520from%2520time-series%2520data%2520without%2520manual%250Afeature%2520engineering.%2520This%2520survey%2520first%2520presents%2520a%2520novel%2520taxonomy%2520based%2520on%2520three%250Afundamental%2520elements%2520in%2520designing%2520state-of-the-art%2520universal%2520representation%250Alearning%2520methods%2520for%2520time%2520series.%2520According%2520to%2520the%2520proposed%2520taxonomy%252C%2520we%250Acomprehensively%2520review%2520existing%2520studies%2520and%2520discuss%2520their%2520intuitions%2520and%250Ainsights%2520into%2520how%2520these%2520methods%2520enhance%2520the%2520quality%2520of%2520learned%2520representations.%250AFinally%252C%2520as%2520a%2520guideline%2520for%2520future%2520studies%252C%2520we%2520summarize%2520commonly%2520used%250Aexperimental%2520setups%2520and%2520datasets%2520and%2520discuss%2520several%2520promising%2520research%250Adirections.%2520An%2520up-to-date%2520corresponding%2520resource%2520is%2520available%2520at%250Ahttps%253A//github.com/itouchz/awesome-deep-time-series-representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Time-Series%20Representation%20Learning%3A%20A%20Survey&entry.906535625=Patara%20Trirat%20and%20Yooju%20Shin%20and%20Junhyeok%20Kang%20and%20Youngeun%20Nam%20and%20Jihye%20Na%20and%20Minyoung%20Bae%20and%20Joeun%20Kim%20and%20Byunghyun%20Kim%20and%20Jae-Gil%20Lee&entry.1292438233=%20%20Time-series%20data%20exists%20in%20every%20corner%20of%20real-world%20systems%20and%20services%2C%0Aranging%20from%20satellites%20in%20the%20sky%20to%20wearable%20devices%20on%20human%20bodies.%0ALearning%20representations%20by%20extracting%20and%20inferring%20valuable%20information%20from%0Athese%20time%20series%20is%20crucial%20for%20understanding%20the%20complex%20dynamics%20of%0Aparticular%20phenomena%20and%20enabling%20informed%20decisions.%20With%20the%20learned%0Arepresentations%2C%20we%20can%20perform%20numerous%20downstream%20analyses%20more%20effectively.%0AAmong%20several%20approaches%2C%20deep%20learning%20has%20demonstrated%20remarkable%20performance%0Ain%20extracting%20hidden%20patterns%20and%20features%20from%20time-series%20data%20without%20manual%0Afeature%20engineering.%20This%20survey%20first%20presents%20a%20novel%20taxonomy%20based%20on%20three%0Afundamental%20elements%20in%20designing%20state-of-the-art%20universal%20representation%0Alearning%20methods%20for%20time%20series.%20According%20to%20the%20proposed%20taxonomy%2C%20we%0Acomprehensively%20review%20existing%20studies%20and%20discuss%20their%20intuitions%20and%0Ainsights%20into%20how%20these%20methods%20enhance%20the%20quality%20of%20learned%20representations.%0AFinally%2C%20as%20a%20guideline%20for%20future%20studies%2C%20we%20summarize%20commonly%20used%0Aexperimental%20setups%20and%20datasets%20and%20discuss%20several%20promising%20research%0Adirections.%20An%20up-to-date%20corresponding%20resource%20is%20available%20at%0Ahttps%3A//github.com/itouchz/awesome-deep-time-series-representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03717v2&entry.124074799=Read"},
{"title": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models", "author": "Jiaming He and Wenbo Jiang and Guanyu Hou and Wenshu Fan and Rui Zhang and Hongwei Li", "abstract": "  Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo\n", "link": "http://arxiv.org/abs/2404.14795v4", "date": "2024-08-21", "relevancy": 2.4035, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5183}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Watch%20Out%20for%20Your%20Guidance%20on%20Generation%21%20Exploring%20Conditional%0A%20%20Backdoor%20Attacks%20against%20Large%20Language%20Models&body=Title%3A%20Watch%20Out%20for%20Your%20Guidance%20on%20Generation%21%20Exploring%20Conditional%0A%20%20Backdoor%20Attacks%20against%20Large%20Language%20Models%0AAuthor%3A%20Jiaming%20He%20and%20Wenbo%20Jiang%20and%20Guanyu%20Hou%20and%20Wenshu%20Fan%20and%20Rui%20Zhang%20and%20Hongwei%20Li%0AAbstract%3A%20%20%20Mainstream%20backdoor%20attacks%20on%20large%20language%20models%20%28LLMs%29%20typically%20set%20a%0Afixed%20trigger%20in%20the%20input%20instance%20and%20specific%20responses%20for%20triggered%0Aqueries.%20However%2C%20the%20fixed%20trigger%20setting%20%28e.g.%2C%20unusual%20words%29%20may%20be%20easily%0Adetected%20by%20human%20detection%2C%20limiting%20the%20effectiveness%20and%20practicality%20in%0Areal-world%20scenarios.%20To%20enhance%20the%20stealthiness%20of%20backdoor%20activation%2C%20we%0Apresent%20a%20new%20poisoning%20paradigm%20against%20LLMs%20triggered%20by%20specifying%0Ageneration%20conditions%2C%20which%20are%20commonly%20adopted%20strategies%20by%20users%20during%0Amodel%20inference.%20The%20poisoned%20model%20performs%20normally%20for%20output%20under%0Anormal/other%20generation%20conditions%2C%20while%20becomes%20harmful%20for%20output%20under%0Atarget%20generation%20conditions.%20To%20achieve%20this%20objective%2C%20we%20introduce%20BrieFool%2C%0Aan%20efficient%20attack%20framework.%20It%20leverages%20the%20characteristics%20of%20generation%0Aconditions%20by%20efficient%20instruction%20sampling%20and%20poisoning%20data%20generation%2C%0Athereby%20influencing%20the%20behavior%20of%20LLMs%20under%20target%20conditions.%20Our%20attack%0Acan%20be%20generally%20divided%20into%20two%20types%20with%20different%20targets%3A%20Safety%0Aunalignment%20attack%20and%20Ability%20degradation%20attack.%20Our%20extensive%20experiments%0Ademonstrate%20that%20BrieFool%20is%20effective%20across%20safety%20domains%20and%20ability%0Adomains%2C%20achieving%20higher%20success%20rates%20than%20baseline%20methods%2C%20with%2094.3%20%25%20on%0AGPT-3.5-turbo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14795v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWatch%2520Out%2520for%2520Your%2520Guidance%2520on%2520Generation%2521%2520Exploring%2520Conditional%250A%2520%2520Backdoor%2520Attacks%2520against%2520Large%2520Language%2520Models%26entry.906535625%3DJiaming%2520He%2520and%2520Wenbo%2520Jiang%2520and%2520Guanyu%2520Hou%2520and%2520Wenshu%2520Fan%2520and%2520Rui%2520Zhang%2520and%2520Hongwei%2520Li%26entry.1292438233%3D%2520%2520Mainstream%2520backdoor%2520attacks%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520typically%2520set%2520a%250Afixed%2520trigger%2520in%2520the%2520input%2520instance%2520and%2520specific%2520responses%2520for%2520triggered%250Aqueries.%2520However%252C%2520the%2520fixed%2520trigger%2520setting%2520%2528e.g.%252C%2520unusual%2520words%2529%2520may%2520be%2520easily%250Adetected%2520by%2520human%2520detection%252C%2520limiting%2520the%2520effectiveness%2520and%2520practicality%2520in%250Areal-world%2520scenarios.%2520To%2520enhance%2520the%2520stealthiness%2520of%2520backdoor%2520activation%252C%2520we%250Apresent%2520a%2520new%2520poisoning%2520paradigm%2520against%2520LLMs%2520triggered%2520by%2520specifying%250Ageneration%2520conditions%252C%2520which%2520are%2520commonly%2520adopted%2520strategies%2520by%2520users%2520during%250Amodel%2520inference.%2520The%2520poisoned%2520model%2520performs%2520normally%2520for%2520output%2520under%250Anormal/other%2520generation%2520conditions%252C%2520while%2520becomes%2520harmful%2520for%2520output%2520under%250Atarget%2520generation%2520conditions.%2520To%2520achieve%2520this%2520objective%252C%2520we%2520introduce%2520BrieFool%252C%250Aan%2520efficient%2520attack%2520framework.%2520It%2520leverages%2520the%2520characteristics%2520of%2520generation%250Aconditions%2520by%2520efficient%2520instruction%2520sampling%2520and%2520poisoning%2520data%2520generation%252C%250Athereby%2520influencing%2520the%2520behavior%2520of%2520LLMs%2520under%2520target%2520conditions.%2520Our%2520attack%250Acan%2520be%2520generally%2520divided%2520into%2520two%2520types%2520with%2520different%2520targets%253A%2520Safety%250Aunalignment%2520attack%2520and%2520Ability%2520degradation%2520attack.%2520Our%2520extensive%2520experiments%250Ademonstrate%2520that%2520BrieFool%2520is%2520effective%2520across%2520safety%2520domains%2520and%2520ability%250Adomains%252C%2520achieving%2520higher%2520success%2520rates%2520than%2520baseline%2520methods%252C%2520with%252094.3%2520%2525%2520on%250AGPT-3.5-turbo%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14795v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Watch%20Out%20for%20Your%20Guidance%20on%20Generation%21%20Exploring%20Conditional%0A%20%20Backdoor%20Attacks%20against%20Large%20Language%20Models&entry.906535625=Jiaming%20He%20and%20Wenbo%20Jiang%20and%20Guanyu%20Hou%20and%20Wenshu%20Fan%20and%20Rui%20Zhang%20and%20Hongwei%20Li&entry.1292438233=%20%20Mainstream%20backdoor%20attacks%20on%20large%20language%20models%20%28LLMs%29%20typically%20set%20a%0Afixed%20trigger%20in%20the%20input%20instance%20and%20specific%20responses%20for%20triggered%0Aqueries.%20However%2C%20the%20fixed%20trigger%20setting%20%28e.g.%2C%20unusual%20words%29%20may%20be%20easily%0Adetected%20by%20human%20detection%2C%20limiting%20the%20effectiveness%20and%20practicality%20in%0Areal-world%20scenarios.%20To%20enhance%20the%20stealthiness%20of%20backdoor%20activation%2C%20we%0Apresent%20a%20new%20poisoning%20paradigm%20against%20LLMs%20triggered%20by%20specifying%0Ageneration%20conditions%2C%20which%20are%20commonly%20adopted%20strategies%20by%20users%20during%0Amodel%20inference.%20The%20poisoned%20model%20performs%20normally%20for%20output%20under%0Anormal/other%20generation%20conditions%2C%20while%20becomes%20harmful%20for%20output%20under%0Atarget%20generation%20conditions.%20To%20achieve%20this%20objective%2C%20we%20introduce%20BrieFool%2C%0Aan%20efficient%20attack%20framework.%20It%20leverages%20the%20characteristics%20of%20generation%0Aconditions%20by%20efficient%20instruction%20sampling%20and%20poisoning%20data%20generation%2C%0Athereby%20influencing%20the%20behavior%20of%20LLMs%20under%20target%20conditions.%20Our%20attack%0Acan%20be%20generally%20divided%20into%20two%20types%20with%20different%20targets%3A%20Safety%0Aunalignment%20attack%20and%20Ability%20degradation%20attack.%20Our%20extensive%20experiments%0Ademonstrate%20that%20BrieFool%20is%20effective%20across%20safety%20domains%20and%20ability%0Adomains%2C%20achieving%20higher%20success%20rates%20than%20baseline%20methods%2C%20with%2094.3%20%25%20on%0AGPT-3.5-turbo%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14795v4&entry.124074799=Read"},
{"title": "Enhanced Visual SLAM for Collision-free Driving with Lightweight\n  Autonomous Cars", "author": "Zhihao Lin and Zhen Tian and Qi Zhang and Hanyang Zhuang and Jianglin Lan", "abstract": "  The paper presents a vision-based obstacle avoidance strategy for lightweight\nself-driving cars that can be run on a CPU-only device using a single RGB-D\ncamera. The method consists of two steps: visual perception and path planning.\nThe visual perception part uses ORBSLAM3 enhanced with optical flow to estimate\nthe car's poses and extract rich texture information from the scene. In the\npath planning phase, we employ a method combining a control Lyapunov function\nand control barrier function in the form of quadratic program (CLF-CBF-QP)\ntogether with an obstacle shape reconstruction process (SRP) to plan safe and\nstable trajectories. To validate the performance and robustness of the proposed\nmethod, simulation experiments were conducted with a car in various complex\nindoor environments using the Gazebo simulation environment. Our method can\neffectively avoid obstacles in the scenes. The proposed algorithm outperforms\nbenchmark algorithms in achieving more stable and shorter trajectories across\nmultiple simulated scenes.\n", "link": "http://arxiv.org/abs/2408.11582v1", "date": "2024-08-21", "relevancy": 2.4007, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6102}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Visual%20SLAM%20for%20Collision-free%20Driving%20with%20Lightweight%0A%20%20Autonomous%20Cars&body=Title%3A%20Enhanced%20Visual%20SLAM%20for%20Collision-free%20Driving%20with%20Lightweight%0A%20%20Autonomous%20Cars%0AAuthor%3A%20Zhihao%20Lin%20and%20Zhen%20Tian%20and%20Qi%20Zhang%20and%20Hanyang%20Zhuang%20and%20Jianglin%20Lan%0AAbstract%3A%20%20%20The%20paper%20presents%20a%20vision-based%20obstacle%20avoidance%20strategy%20for%20lightweight%0Aself-driving%20cars%20that%20can%20be%20run%20on%20a%20CPU-only%20device%20using%20a%20single%20RGB-D%0Acamera.%20The%20method%20consists%20of%20two%20steps%3A%20visual%20perception%20and%20path%20planning.%0AThe%20visual%20perception%20part%20uses%20ORBSLAM3%20enhanced%20with%20optical%20flow%20to%20estimate%0Athe%20car%27s%20poses%20and%20extract%20rich%20texture%20information%20from%20the%20scene.%20In%20the%0Apath%20planning%20phase%2C%20we%20employ%20a%20method%20combining%20a%20control%20Lyapunov%20function%0Aand%20control%20barrier%20function%20in%20the%20form%20of%20quadratic%20program%20%28CLF-CBF-QP%29%0Atogether%20with%20an%20obstacle%20shape%20reconstruction%20process%20%28SRP%29%20to%20plan%20safe%20and%0Astable%20trajectories.%20To%20validate%20the%20performance%20and%20robustness%20of%20the%20proposed%0Amethod%2C%20simulation%20experiments%20were%20conducted%20with%20a%20car%20in%20various%20complex%0Aindoor%20environments%20using%20the%20Gazebo%20simulation%20environment.%20Our%20method%20can%0Aeffectively%20avoid%20obstacles%20in%20the%20scenes.%20The%20proposed%20algorithm%20outperforms%0Abenchmark%20algorithms%20in%20achieving%20more%20stable%20and%20shorter%20trajectories%20across%0Amultiple%20simulated%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Visual%2520SLAM%2520for%2520Collision-free%2520Driving%2520with%2520Lightweight%250A%2520%2520Autonomous%2520Cars%26entry.906535625%3DZhihao%2520Lin%2520and%2520Zhen%2520Tian%2520and%2520Qi%2520Zhang%2520and%2520Hanyang%2520Zhuang%2520and%2520Jianglin%2520Lan%26entry.1292438233%3D%2520%2520The%2520paper%2520presents%2520a%2520vision-based%2520obstacle%2520avoidance%2520strategy%2520for%2520lightweight%250Aself-driving%2520cars%2520that%2520can%2520be%2520run%2520on%2520a%2520CPU-only%2520device%2520using%2520a%2520single%2520RGB-D%250Acamera.%2520The%2520method%2520consists%2520of%2520two%2520steps%253A%2520visual%2520perception%2520and%2520path%2520planning.%250AThe%2520visual%2520perception%2520part%2520uses%2520ORBSLAM3%2520enhanced%2520with%2520optical%2520flow%2520to%2520estimate%250Athe%2520car%2527s%2520poses%2520and%2520extract%2520rich%2520texture%2520information%2520from%2520the%2520scene.%2520In%2520the%250Apath%2520planning%2520phase%252C%2520we%2520employ%2520a%2520method%2520combining%2520a%2520control%2520Lyapunov%2520function%250Aand%2520control%2520barrier%2520function%2520in%2520the%2520form%2520of%2520quadratic%2520program%2520%2528CLF-CBF-QP%2529%250Atogether%2520with%2520an%2520obstacle%2520shape%2520reconstruction%2520process%2520%2528SRP%2529%2520to%2520plan%2520safe%2520and%250Astable%2520trajectories.%2520To%2520validate%2520the%2520performance%2520and%2520robustness%2520of%2520the%2520proposed%250Amethod%252C%2520simulation%2520experiments%2520were%2520conducted%2520with%2520a%2520car%2520in%2520various%2520complex%250Aindoor%2520environments%2520using%2520the%2520Gazebo%2520simulation%2520environment.%2520Our%2520method%2520can%250Aeffectively%2520avoid%2520obstacles%2520in%2520the%2520scenes.%2520The%2520proposed%2520algorithm%2520outperforms%250Abenchmark%2520algorithms%2520in%2520achieving%2520more%2520stable%2520and%2520shorter%2520trajectories%2520across%250Amultiple%2520simulated%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Visual%20SLAM%20for%20Collision-free%20Driving%20with%20Lightweight%0A%20%20Autonomous%20Cars&entry.906535625=Zhihao%20Lin%20and%20Zhen%20Tian%20and%20Qi%20Zhang%20and%20Hanyang%20Zhuang%20and%20Jianglin%20Lan&entry.1292438233=%20%20The%20paper%20presents%20a%20vision-based%20obstacle%20avoidance%20strategy%20for%20lightweight%0Aself-driving%20cars%20that%20can%20be%20run%20on%20a%20CPU-only%20device%20using%20a%20single%20RGB-D%0Acamera.%20The%20method%20consists%20of%20two%20steps%3A%20visual%20perception%20and%20path%20planning.%0AThe%20visual%20perception%20part%20uses%20ORBSLAM3%20enhanced%20with%20optical%20flow%20to%20estimate%0Athe%20car%27s%20poses%20and%20extract%20rich%20texture%20information%20from%20the%20scene.%20In%20the%0Apath%20planning%20phase%2C%20we%20employ%20a%20method%20combining%20a%20control%20Lyapunov%20function%0Aand%20control%20barrier%20function%20in%20the%20form%20of%20quadratic%20program%20%28CLF-CBF-QP%29%0Atogether%20with%20an%20obstacle%20shape%20reconstruction%20process%20%28SRP%29%20to%20plan%20safe%20and%0Astable%20trajectories.%20To%20validate%20the%20performance%20and%20robustness%20of%20the%20proposed%0Amethod%2C%20simulation%20experiments%20were%20conducted%20with%20a%20car%20in%20various%20complex%0Aindoor%20environments%20using%20the%20Gazebo%20simulation%20environment.%20Our%20method%20can%0Aeffectively%20avoid%20obstacles%20in%20the%20scenes.%20The%20proposed%20algorithm%20outperforms%0Abenchmark%20algorithms%20in%20achieving%20more%20stable%20and%20shorter%20trajectories%20across%0Amultiple%20simulated%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11582v1&entry.124074799=Read"},
{"title": "DTN: Deep Multiple Task-specific Feature Interactions Network for\n  Multi-Task Recommendation", "author": "Yaowen Bi and Yuteng Lian and Jie Cui and Jun Liu and Peijian Wang and Guanghui Li and Xuejun Chen and Jinglin Zhao and Hao Wen and Jing Zhang and Zhaoqi Zhang and Wenzhuo Song and Yang Sun and Weiwei Zhang and Mingchen Cai and Guanxing Zhang", "abstract": "  Neural-based multi-task learning (MTL) has been successfully applied to many\nrecommendation applications. However, these MTL models (e.g., MMoE, PLE) did\nnot consider feature interaction during the optimization, which is crucial for\ncapturing complex high-order features and has been widely used in ranking\nmodels for real-world recommender systems. Moreover, through feature importance\nanalysis across various tasks in MTL, we have observed an interesting\ndivergence phenomenon that the same feature can have significantly different\nimportance across different tasks in MTL. To address these issues, we propose\nDeep Multiple Task-specific Feature Interactions Network (DTN) with a novel\nmodel structure design. DTN introduces multiple diversified task-specific\nfeature interaction methods and task-sensitive network in MTL networks,\nenabling the model to learn task-specific diversified feature interaction\nrepresentations, which improves the efficiency of joint representation learning\nin a general setup. We applied DTN to our company's real-world E-commerce\nrecommendation dataset, which consisted of over 6.3 billion samples, the\nresults demonstrated that DTN significantly outperformed state-of-the-art MTL\nmodels. Moreover, during online evaluation of DTN in a large-scale E-commerce\nrecommender system, we observed a 3.28% in clicks, a 3.10% increase in orders\nand a 2.70% increase in GMV (Gross Merchandise Value) compared to the\nstate-of-the-art MTL models. Finally, extensive offline experiments conducted\non public benchmark datasets demonstrate that DTN can be applied to various\nscenarios beyond recommendations, enhancing the performance of ranking models.\n", "link": "http://arxiv.org/abs/2408.11611v1", "date": "2024-08-21", "relevancy": 2.3988, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.508}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4666}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DTN%3A%20Deep%20Multiple%20Task-specific%20Feature%20Interactions%20Network%20for%0A%20%20Multi-Task%20Recommendation&body=Title%3A%20DTN%3A%20Deep%20Multiple%20Task-specific%20Feature%20Interactions%20Network%20for%0A%20%20Multi-Task%20Recommendation%0AAuthor%3A%20Yaowen%20Bi%20and%20Yuteng%20Lian%20and%20Jie%20Cui%20and%20Jun%20Liu%20and%20Peijian%20Wang%20and%20Guanghui%20Li%20and%20Xuejun%20Chen%20and%20Jinglin%20Zhao%20and%20Hao%20Wen%20and%20Jing%20Zhang%20and%20Zhaoqi%20Zhang%20and%20Wenzhuo%20Song%20and%20Yang%20Sun%20and%20Weiwei%20Zhang%20and%20Mingchen%20Cai%20and%20Guanxing%20Zhang%0AAbstract%3A%20%20%20Neural-based%20multi-task%20learning%20%28MTL%29%20has%20been%20successfully%20applied%20to%20many%0Arecommendation%20applications.%20However%2C%20these%20MTL%20models%20%28e.g.%2C%20MMoE%2C%20PLE%29%20did%0Anot%20consider%20feature%20interaction%20during%20the%20optimization%2C%20which%20is%20crucial%20for%0Acapturing%20complex%20high-order%20features%20and%20has%20been%20widely%20used%20in%20ranking%0Amodels%20for%20real-world%20recommender%20systems.%20Moreover%2C%20through%20feature%20importance%0Aanalysis%20across%20various%20tasks%20in%20MTL%2C%20we%20have%20observed%20an%20interesting%0Adivergence%20phenomenon%20that%20the%20same%20feature%20can%20have%20significantly%20different%0Aimportance%20across%20different%20tasks%20in%20MTL.%20To%20address%20these%20issues%2C%20we%20propose%0ADeep%20Multiple%20Task-specific%20Feature%20Interactions%20Network%20%28DTN%29%20with%20a%20novel%0Amodel%20structure%20design.%20DTN%20introduces%20multiple%20diversified%20task-specific%0Afeature%20interaction%20methods%20and%20task-sensitive%20network%20in%20MTL%20networks%2C%0Aenabling%20the%20model%20to%20learn%20task-specific%20diversified%20feature%20interaction%0Arepresentations%2C%20which%20improves%20the%20efficiency%20of%20joint%20representation%20learning%0Ain%20a%20general%20setup.%20We%20applied%20DTN%20to%20our%20company%27s%20real-world%20E-commerce%0Arecommendation%20dataset%2C%20which%20consisted%20of%20over%206.3%20billion%20samples%2C%20the%0Aresults%20demonstrated%20that%20DTN%20significantly%20outperformed%20state-of-the-art%20MTL%0Amodels.%20Moreover%2C%20during%20online%20evaluation%20of%20DTN%20in%20a%20large-scale%20E-commerce%0Arecommender%20system%2C%20we%20observed%20a%203.28%25%20in%20clicks%2C%20a%203.10%25%20increase%20in%20orders%0Aand%20a%202.70%25%20increase%20in%20GMV%20%28Gross%20Merchandise%20Value%29%20compared%20to%20the%0Astate-of-the-art%20MTL%20models.%20Finally%2C%20extensive%20offline%20experiments%20conducted%0Aon%20public%20benchmark%20datasets%20demonstrate%20that%20DTN%20can%20be%20applied%20to%20various%0Ascenarios%20beyond%20recommendations%2C%20enhancing%20the%20performance%20of%20ranking%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDTN%253A%2520Deep%2520Multiple%2520Task-specific%2520Feature%2520Interactions%2520Network%2520for%250A%2520%2520Multi-Task%2520Recommendation%26entry.906535625%3DYaowen%2520Bi%2520and%2520Yuteng%2520Lian%2520and%2520Jie%2520Cui%2520and%2520Jun%2520Liu%2520and%2520Peijian%2520Wang%2520and%2520Guanghui%2520Li%2520and%2520Xuejun%2520Chen%2520and%2520Jinglin%2520Zhao%2520and%2520Hao%2520Wen%2520and%2520Jing%2520Zhang%2520and%2520Zhaoqi%2520Zhang%2520and%2520Wenzhuo%2520Song%2520and%2520Yang%2520Sun%2520and%2520Weiwei%2520Zhang%2520and%2520Mingchen%2520Cai%2520and%2520Guanxing%2520Zhang%26entry.1292438233%3D%2520%2520Neural-based%2520multi-task%2520learning%2520%2528MTL%2529%2520has%2520been%2520successfully%2520applied%2520to%2520many%250Arecommendation%2520applications.%2520However%252C%2520these%2520MTL%2520models%2520%2528e.g.%252C%2520MMoE%252C%2520PLE%2529%2520did%250Anot%2520consider%2520feature%2520interaction%2520during%2520the%2520optimization%252C%2520which%2520is%2520crucial%2520for%250Acapturing%2520complex%2520high-order%2520features%2520and%2520has%2520been%2520widely%2520used%2520in%2520ranking%250Amodels%2520for%2520real-world%2520recommender%2520systems.%2520Moreover%252C%2520through%2520feature%2520importance%250Aanalysis%2520across%2520various%2520tasks%2520in%2520MTL%252C%2520we%2520have%2520observed%2520an%2520interesting%250Adivergence%2520phenomenon%2520that%2520the%2520same%2520feature%2520can%2520have%2520significantly%2520different%250Aimportance%2520across%2520different%2520tasks%2520in%2520MTL.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ADeep%2520Multiple%2520Task-specific%2520Feature%2520Interactions%2520Network%2520%2528DTN%2529%2520with%2520a%2520novel%250Amodel%2520structure%2520design.%2520DTN%2520introduces%2520multiple%2520diversified%2520task-specific%250Afeature%2520interaction%2520methods%2520and%2520task-sensitive%2520network%2520in%2520MTL%2520networks%252C%250Aenabling%2520the%2520model%2520to%2520learn%2520task-specific%2520diversified%2520feature%2520interaction%250Arepresentations%252C%2520which%2520improves%2520the%2520efficiency%2520of%2520joint%2520representation%2520learning%250Ain%2520a%2520general%2520setup.%2520We%2520applied%2520DTN%2520to%2520our%2520company%2527s%2520real-world%2520E-commerce%250Arecommendation%2520dataset%252C%2520which%2520consisted%2520of%2520over%25206.3%2520billion%2520samples%252C%2520the%250Aresults%2520demonstrated%2520that%2520DTN%2520significantly%2520outperformed%2520state-of-the-art%2520MTL%250Amodels.%2520Moreover%252C%2520during%2520online%2520evaluation%2520of%2520DTN%2520in%2520a%2520large-scale%2520E-commerce%250Arecommender%2520system%252C%2520we%2520observed%2520a%25203.28%2525%2520in%2520clicks%252C%2520a%25203.10%2525%2520increase%2520in%2520orders%250Aand%2520a%25202.70%2525%2520increase%2520in%2520GMV%2520%2528Gross%2520Merchandise%2520Value%2529%2520compared%2520to%2520the%250Astate-of-the-art%2520MTL%2520models.%2520Finally%252C%2520extensive%2520offline%2520experiments%2520conducted%250Aon%2520public%2520benchmark%2520datasets%2520demonstrate%2520that%2520DTN%2520can%2520be%2520applied%2520to%2520various%250Ascenarios%2520beyond%2520recommendations%252C%2520enhancing%2520the%2520performance%2520of%2520ranking%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DTN%3A%20Deep%20Multiple%20Task-specific%20Feature%20Interactions%20Network%20for%0A%20%20Multi-Task%20Recommendation&entry.906535625=Yaowen%20Bi%20and%20Yuteng%20Lian%20and%20Jie%20Cui%20and%20Jun%20Liu%20and%20Peijian%20Wang%20and%20Guanghui%20Li%20and%20Xuejun%20Chen%20and%20Jinglin%20Zhao%20and%20Hao%20Wen%20and%20Jing%20Zhang%20and%20Zhaoqi%20Zhang%20and%20Wenzhuo%20Song%20and%20Yang%20Sun%20and%20Weiwei%20Zhang%20and%20Mingchen%20Cai%20and%20Guanxing%20Zhang&entry.1292438233=%20%20Neural-based%20multi-task%20learning%20%28MTL%29%20has%20been%20successfully%20applied%20to%20many%0Arecommendation%20applications.%20However%2C%20these%20MTL%20models%20%28e.g.%2C%20MMoE%2C%20PLE%29%20did%0Anot%20consider%20feature%20interaction%20during%20the%20optimization%2C%20which%20is%20crucial%20for%0Acapturing%20complex%20high-order%20features%20and%20has%20been%20widely%20used%20in%20ranking%0Amodels%20for%20real-world%20recommender%20systems.%20Moreover%2C%20through%20feature%20importance%0Aanalysis%20across%20various%20tasks%20in%20MTL%2C%20we%20have%20observed%20an%20interesting%0Adivergence%20phenomenon%20that%20the%20same%20feature%20can%20have%20significantly%20different%0Aimportance%20across%20different%20tasks%20in%20MTL.%20To%20address%20these%20issues%2C%20we%20propose%0ADeep%20Multiple%20Task-specific%20Feature%20Interactions%20Network%20%28DTN%29%20with%20a%20novel%0Amodel%20structure%20design.%20DTN%20introduces%20multiple%20diversified%20task-specific%0Afeature%20interaction%20methods%20and%20task-sensitive%20network%20in%20MTL%20networks%2C%0Aenabling%20the%20model%20to%20learn%20task-specific%20diversified%20feature%20interaction%0Arepresentations%2C%20which%20improves%20the%20efficiency%20of%20joint%20representation%20learning%0Ain%20a%20general%20setup.%20We%20applied%20DTN%20to%20our%20company%27s%20real-world%20E-commerce%0Arecommendation%20dataset%2C%20which%20consisted%20of%20over%206.3%20billion%20samples%2C%20the%0Aresults%20demonstrated%20that%20DTN%20significantly%20outperformed%20state-of-the-art%20MTL%0Amodels.%20Moreover%2C%20during%20online%20evaluation%20of%20DTN%20in%20a%20large-scale%20E-commerce%0Arecommender%20system%2C%20we%20observed%20a%203.28%25%20in%20clicks%2C%20a%203.10%25%20increase%20in%20orders%0Aand%20a%202.70%25%20increase%20in%20GMV%20%28Gross%20Merchandise%20Value%29%20compared%20to%20the%0Astate-of-the-art%20MTL%20models.%20Finally%2C%20extensive%20offline%20experiments%20conducted%0Aon%20public%20benchmark%20datasets%20demonstrate%20that%20DTN%20can%20be%20applied%20to%20various%0Ascenarios%20beyond%20recommendations%2C%20enhancing%20the%20performance%20of%20ranking%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11611v1&entry.124074799=Read"},
{"title": "GNN-SKAN: Harnessing the Power of SwallowKAN to Advance Molecular\n  Representation Learning with GNNs", "author": "Ruifeng Li and Mingqian Li and Wei Liu and Hongyang Chen", "abstract": "  Effective molecular representation learning is crucial for advancing\nmolecular property prediction and drug design. Mainstream molecular\nrepresentation learning approaches are based on Graph Neural Networks (GNNs).\nHowever, these approaches struggle with three significant challenges:\ninsufficient annotations, molecular diversity, and architectural limitations\nsuch as over-squashing, which leads to the loss of critical structural details.\nTo address these challenges, we introduce a new class of GNNs that integrates\nthe Kolmogorov-Arnold Networks (KANs), known for their robust data-fitting\ncapabilities and high accuracy in small-scale AI + Science tasks. By\nincorporating KANs into GNNs, our model enhances the representation of\nmolecular structures. We further advance this approach with a variant called\nSwallowKAN (SKAN), which employs adaptive Radial Basis Functions (RBFs) as the\ncore of the non-linear neurons. This innovation improves both computational\nefficiency and adaptability to diverse molecular structures. Building on the\nstrengths of SKAN, we propose a new class of GNNs, GNN-SKAN, and its augmented\nvariant, GNN-SKAN+, which incorporates a SKAN-based classifier to further boost\nperformance. To our knowledge, this is the first work to integrate KANs into\nGNN architectures tailored for molecular representation learning. Experiments\nacross 6 classification datasets, 6 regression datasets, and 4 few-shot\nlearning datasets demonstrate that our approach achieves new state-of-the-art\nperformance in terms of accuracy and computational cost.\n", "link": "http://arxiv.org/abs/2408.01018v2", "date": "2024-08-21", "relevancy": 2.3877, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4858}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4802}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNN-SKAN%3A%20Harnessing%20the%20Power%20of%20SwallowKAN%20to%20Advance%20Molecular%0A%20%20Representation%20Learning%20with%20GNNs&body=Title%3A%20GNN-SKAN%3A%20Harnessing%20the%20Power%20of%20SwallowKAN%20to%20Advance%20Molecular%0A%20%20Representation%20Learning%20with%20GNNs%0AAuthor%3A%20Ruifeng%20Li%20and%20Mingqian%20Li%20and%20Wei%20Liu%20and%20Hongyang%20Chen%0AAbstract%3A%20%20%20Effective%20molecular%20representation%20learning%20is%20crucial%20for%20advancing%0Amolecular%20property%20prediction%20and%20drug%20design.%20Mainstream%20molecular%0Arepresentation%20learning%20approaches%20are%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29.%0AHowever%2C%20these%20approaches%20struggle%20with%20three%20significant%20challenges%3A%0Ainsufficient%20annotations%2C%20molecular%20diversity%2C%20and%20architectural%20limitations%0Asuch%20as%20over-squashing%2C%20which%20leads%20to%20the%20loss%20of%20critical%20structural%20details.%0ATo%20address%20these%20challenges%2C%20we%20introduce%20a%20new%20class%20of%20GNNs%20that%20integrates%0Athe%20Kolmogorov-Arnold%20Networks%20%28KANs%29%2C%20known%20for%20their%20robust%20data-fitting%0Acapabilities%20and%20high%20accuracy%20in%20small-scale%20AI%20%2B%20Science%20tasks.%20By%0Aincorporating%20KANs%20into%20GNNs%2C%20our%20model%20enhances%20the%20representation%20of%0Amolecular%20structures.%20We%20further%20advance%20this%20approach%20with%20a%20variant%20called%0ASwallowKAN%20%28SKAN%29%2C%20which%20employs%20adaptive%20Radial%20Basis%20Functions%20%28RBFs%29%20as%20the%0Acore%20of%20the%20non-linear%20neurons.%20This%20innovation%20improves%20both%20computational%0Aefficiency%20and%20adaptability%20to%20diverse%20molecular%20structures.%20Building%20on%20the%0Astrengths%20of%20SKAN%2C%20we%20propose%20a%20new%20class%20of%20GNNs%2C%20GNN-SKAN%2C%20and%20its%20augmented%0Avariant%2C%20GNN-SKAN%2B%2C%20which%20incorporates%20a%20SKAN-based%20classifier%20to%20further%20boost%0Aperformance.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20integrate%20KANs%20into%0AGNN%20architectures%20tailored%20for%20molecular%20representation%20learning.%20Experiments%0Aacross%206%20classification%20datasets%2C%206%20regression%20datasets%2C%20and%204%20few-shot%0Alearning%20datasets%20demonstrate%20that%20our%20approach%20achieves%20new%20state-of-the-art%0Aperformance%20in%20terms%20of%20accuracy%20and%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNN-SKAN%253A%2520Harnessing%2520the%2520Power%2520of%2520SwallowKAN%2520to%2520Advance%2520Molecular%250A%2520%2520Representation%2520Learning%2520with%2520GNNs%26entry.906535625%3DRuifeng%2520Li%2520and%2520Mingqian%2520Li%2520and%2520Wei%2520Liu%2520and%2520Hongyang%2520Chen%26entry.1292438233%3D%2520%2520Effective%2520molecular%2520representation%2520learning%2520is%2520crucial%2520for%2520advancing%250Amolecular%2520property%2520prediction%2520and%2520drug%2520design.%2520Mainstream%2520molecular%250Arepresentation%2520learning%2520approaches%2520are%2520based%2520on%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%250AHowever%252C%2520these%2520approaches%2520struggle%2520with%2520three%2520significant%2520challenges%253A%250Ainsufficient%2520annotations%252C%2520molecular%2520diversity%252C%2520and%2520architectural%2520limitations%250Asuch%2520as%2520over-squashing%252C%2520which%2520leads%2520to%2520the%2520loss%2520of%2520critical%2520structural%2520details.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520new%2520class%2520of%2520GNNs%2520that%2520integrates%250Athe%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%252C%2520known%2520for%2520their%2520robust%2520data-fitting%250Acapabilities%2520and%2520high%2520accuracy%2520in%2520small-scale%2520AI%2520%252B%2520Science%2520tasks.%2520By%250Aincorporating%2520KANs%2520into%2520GNNs%252C%2520our%2520model%2520enhances%2520the%2520representation%2520of%250Amolecular%2520structures.%2520We%2520further%2520advance%2520this%2520approach%2520with%2520a%2520variant%2520called%250ASwallowKAN%2520%2528SKAN%2529%252C%2520which%2520employs%2520adaptive%2520Radial%2520Basis%2520Functions%2520%2528RBFs%2529%2520as%2520the%250Acore%2520of%2520the%2520non-linear%2520neurons.%2520This%2520innovation%2520improves%2520both%2520computational%250Aefficiency%2520and%2520adaptability%2520to%2520diverse%2520molecular%2520structures.%2520Building%2520on%2520the%250Astrengths%2520of%2520SKAN%252C%2520we%2520propose%2520a%2520new%2520class%2520of%2520GNNs%252C%2520GNN-SKAN%252C%2520and%2520its%2520augmented%250Avariant%252C%2520GNN-SKAN%252B%252C%2520which%2520incorporates%2520a%2520SKAN-based%2520classifier%2520to%2520further%2520boost%250Aperformance.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520integrate%2520KANs%2520into%250AGNN%2520architectures%2520tailored%2520for%2520molecular%2520representation%2520learning.%2520Experiments%250Aacross%25206%2520classification%2520datasets%252C%25206%2520regression%2520datasets%252C%2520and%25204%2520few-shot%250Alearning%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520new%2520state-of-the-art%250Aperformance%2520in%2520terms%2520of%2520accuracy%2520and%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNN-SKAN%3A%20Harnessing%20the%20Power%20of%20SwallowKAN%20to%20Advance%20Molecular%0A%20%20Representation%20Learning%20with%20GNNs&entry.906535625=Ruifeng%20Li%20and%20Mingqian%20Li%20and%20Wei%20Liu%20and%20Hongyang%20Chen&entry.1292438233=%20%20Effective%20molecular%20representation%20learning%20is%20crucial%20for%20advancing%0Amolecular%20property%20prediction%20and%20drug%20design.%20Mainstream%20molecular%0Arepresentation%20learning%20approaches%20are%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29.%0AHowever%2C%20these%20approaches%20struggle%20with%20three%20significant%20challenges%3A%0Ainsufficient%20annotations%2C%20molecular%20diversity%2C%20and%20architectural%20limitations%0Asuch%20as%20over-squashing%2C%20which%20leads%20to%20the%20loss%20of%20critical%20structural%20details.%0ATo%20address%20these%20challenges%2C%20we%20introduce%20a%20new%20class%20of%20GNNs%20that%20integrates%0Athe%20Kolmogorov-Arnold%20Networks%20%28KANs%29%2C%20known%20for%20their%20robust%20data-fitting%0Acapabilities%20and%20high%20accuracy%20in%20small-scale%20AI%20%2B%20Science%20tasks.%20By%0Aincorporating%20KANs%20into%20GNNs%2C%20our%20model%20enhances%20the%20representation%20of%0Amolecular%20structures.%20We%20further%20advance%20this%20approach%20with%20a%20variant%20called%0ASwallowKAN%20%28SKAN%29%2C%20which%20employs%20adaptive%20Radial%20Basis%20Functions%20%28RBFs%29%20as%20the%0Acore%20of%20the%20non-linear%20neurons.%20This%20innovation%20improves%20both%20computational%0Aefficiency%20and%20adaptability%20to%20diverse%20molecular%20structures.%20Building%20on%20the%0Astrengths%20of%20SKAN%2C%20we%20propose%20a%20new%20class%20of%20GNNs%2C%20GNN-SKAN%2C%20and%20its%20augmented%0Avariant%2C%20GNN-SKAN%2B%2C%20which%20incorporates%20a%20SKAN-based%20classifier%20to%20further%20boost%0Aperformance.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20integrate%20KANs%20into%0AGNN%20architectures%20tailored%20for%20molecular%20representation%20learning.%20Experiments%0Aacross%206%20classification%20datasets%2C%206%20regression%20datasets%2C%20and%204%20few-shot%0Alearning%20datasets%20demonstrate%20that%20our%20approach%20achieves%20new%20state-of-the-art%0Aperformance%20in%20terms%20of%20accuracy%20and%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01018v2&entry.124074799=Read"},
{"title": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization", "author": "Kangjun Noh and Baekryun Seong and Hoyoon Byun and Youngjun Choi and Sungjin Song and Kyungwoo Song", "abstract": "  Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at https://github.com/sksmssh/LBCforOOVGen\n", "link": "http://arxiv.org/abs/2408.10923v2", "date": "2024-08-21", "relevancy": 2.3815, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5049}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4649}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LBC%3A%20Language-Based-Classifier%20for%20Out-Of-Variable%20Generalization&body=Title%3A%20LBC%3A%20Language-Based-Classifier%20for%20Out-Of-Variable%20Generalization%0AAuthor%3A%20Kangjun%20Noh%20and%20Baekryun%20Seong%20and%20Hoyoon%20Byun%20and%20Youngjun%20Choi%20and%20Sungjin%20Song%20and%20Kyungwoo%20Song%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20great%20success%20in%20natural%20language%0Aprocessing%20tasks%20such%20as%20response%20generation.%20However%2C%20their%20use%20in%20tabular%0Adata%20has%20been%20limited%20due%20to%20their%20inferior%20performance%20compared%20to%20traditional%0Amachine%20learning%20models%20%28TMLs%29%20such%20as%20XGBoost.%20We%20find%20that%20the%20pre-trained%0Aknowledge%20of%20LLMs%20enables%20them%20to%20interpret%20new%20variables%20that%20appear%20in%20a%20test%0Awithout%20additional%20training%2C%20a%20capability%20central%20to%20the%20concept%20of%0AOut-of-Variable%20%28OOV%29.%20From%20the%20findings%2C%20we%20propose%20a%0ALanguage-Based-Classifier%20%28LBC%29%2C%20a%20classifier%20that%20maximizes%20the%20benefits%20of%0ALLMs%20to%20outperform%20TMLs%20on%20OOV%20tasks.%20LBC%20employs%20three%20key%20methodological%0Astrategies%3A%201%29%20Categorical%20changes%20to%20adjust%20data%20to%20better%20fit%20the%20model%27s%0Aunderstanding%2C%202%29%20Advanced%20order%20and%20indicator%20to%20enhance%20data%20representation%0Ato%20the%20model%2C%20and%203%29%20Using%20verbalizer%20to%20map%20logit%20scores%20to%20classes%20during%0Ainference%20to%20generate%20model%20predictions.%20These%20strategies%2C%20combined%20with%20the%0Apre-trained%20knowledge%20of%20LBC%2C%20emphasize%20the%20model%27s%20ability%20to%20effectively%0Ahandle%20OOV%20tasks.%20We%20empirically%20and%20theoretically%20validate%20the%20superiority%20of%0ALBC.%20LBC%20is%20the%20first%20study%20to%20apply%20an%20LLM-based%20model%20to%20OOV%20tasks.%20The%0Asource%20code%20is%20at%20https%3A//github.com/sksmssh/LBCforOOVGen%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10923v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLBC%253A%2520Language-Based-Classifier%2520for%2520Out-Of-Variable%2520Generalization%26entry.906535625%3DKangjun%2520Noh%2520and%2520Baekryun%2520Seong%2520and%2520Hoyoon%2520Byun%2520and%2520Youngjun%2520Choi%2520and%2520Sungjin%2520Song%2520and%2520Kyungwoo%2520Song%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520great%2520success%2520in%2520natural%2520language%250Aprocessing%2520tasks%2520such%2520as%2520response%2520generation.%2520However%252C%2520their%2520use%2520in%2520tabular%250Adata%2520has%2520been%2520limited%2520due%2520to%2520their%2520inferior%2520performance%2520compared%2520to%2520traditional%250Amachine%2520learning%2520models%2520%2528TMLs%2529%2520such%2520as%2520XGBoost.%2520We%2520find%2520that%2520the%2520pre-trained%250Aknowledge%2520of%2520LLMs%2520enables%2520them%2520to%2520interpret%2520new%2520variables%2520that%2520appear%2520in%2520a%2520test%250Awithout%2520additional%2520training%252C%2520a%2520capability%2520central%2520to%2520the%2520concept%2520of%250AOut-of-Variable%2520%2528OOV%2529.%2520From%2520the%2520findings%252C%2520we%2520propose%2520a%250ALanguage-Based-Classifier%2520%2528LBC%2529%252C%2520a%2520classifier%2520that%2520maximizes%2520the%2520benefits%2520of%250ALLMs%2520to%2520outperform%2520TMLs%2520on%2520OOV%2520tasks.%2520LBC%2520employs%2520three%2520key%2520methodological%250Astrategies%253A%25201%2529%2520Categorical%2520changes%2520to%2520adjust%2520data%2520to%2520better%2520fit%2520the%2520model%2527s%250Aunderstanding%252C%25202%2529%2520Advanced%2520order%2520and%2520indicator%2520to%2520enhance%2520data%2520representation%250Ato%2520the%2520model%252C%2520and%25203%2529%2520Using%2520verbalizer%2520to%2520map%2520logit%2520scores%2520to%2520classes%2520during%250Ainference%2520to%2520generate%2520model%2520predictions.%2520These%2520strategies%252C%2520combined%2520with%2520the%250Apre-trained%2520knowledge%2520of%2520LBC%252C%2520emphasize%2520the%2520model%2527s%2520ability%2520to%2520effectively%250Ahandle%2520OOV%2520tasks.%2520We%2520empirically%2520and%2520theoretically%2520validate%2520the%2520superiority%2520of%250ALBC.%2520LBC%2520is%2520the%2520first%2520study%2520to%2520apply%2520an%2520LLM-based%2520model%2520to%2520OOV%2520tasks.%2520The%250Asource%2520code%2520is%2520at%2520https%253A//github.com/sksmssh/LBCforOOVGen%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10923v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LBC%3A%20Language-Based-Classifier%20for%20Out-Of-Variable%20Generalization&entry.906535625=Kangjun%20Noh%20and%20Baekryun%20Seong%20and%20Hoyoon%20Byun%20and%20Youngjun%20Choi%20and%20Sungjin%20Song%20and%20Kyungwoo%20Song&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20great%20success%20in%20natural%20language%0Aprocessing%20tasks%20such%20as%20response%20generation.%20However%2C%20their%20use%20in%20tabular%0Adata%20has%20been%20limited%20due%20to%20their%20inferior%20performance%20compared%20to%20traditional%0Amachine%20learning%20models%20%28TMLs%29%20such%20as%20XGBoost.%20We%20find%20that%20the%20pre-trained%0Aknowledge%20of%20LLMs%20enables%20them%20to%20interpret%20new%20variables%20that%20appear%20in%20a%20test%0Awithout%20additional%20training%2C%20a%20capability%20central%20to%20the%20concept%20of%0AOut-of-Variable%20%28OOV%29.%20From%20the%20findings%2C%20we%20propose%20a%0ALanguage-Based-Classifier%20%28LBC%29%2C%20a%20classifier%20that%20maximizes%20the%20benefits%20of%0ALLMs%20to%20outperform%20TMLs%20on%20OOV%20tasks.%20LBC%20employs%20three%20key%20methodological%0Astrategies%3A%201%29%20Categorical%20changes%20to%20adjust%20data%20to%20better%20fit%20the%20model%27s%0Aunderstanding%2C%202%29%20Advanced%20order%20and%20indicator%20to%20enhance%20data%20representation%0Ato%20the%20model%2C%20and%203%29%20Using%20verbalizer%20to%20map%20logit%20scores%20to%20classes%20during%0Ainference%20to%20generate%20model%20predictions.%20These%20strategies%2C%20combined%20with%20the%0Apre-trained%20knowledge%20of%20LBC%2C%20emphasize%20the%20model%27s%20ability%20to%20effectively%0Ahandle%20OOV%20tasks.%20We%20empirically%20and%20theoretically%20validate%20the%20superiority%20of%0ALBC.%20LBC%20is%20the%20first%20study%20to%20apply%20an%20LLM-based%20model%20to%20OOV%20tasks.%20The%0Asource%20code%20is%20at%20https%3A//github.com/sksmssh/LBCforOOVGen%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10923v2&entry.124074799=Read"},
{"title": "Optimizing Interpretable Decision Tree Policies for Reinforcement\n  Learning", "author": "Dani\u00ebl Vos and Sicco Verwer", "abstract": "  Reinforcement learning techniques leveraging deep learning have made\ntremendous progress in recent years. However, the complexity of neural networks\nprevents practitioners from understanding their behavior. Decision trees have\ngained increased attention in supervised learning for their inherent\ninterpretability, enabling modelers to understand the exact prediction process\nafter learning. This paper considers the problem of optimizing interpretable\ndecision tree policies to replace neural networks in reinforcement learning\nsettings. Previous works have relaxed the tree structure, restricted to\noptimizing only tree leaves, or applied imitation learning techniques to\napproximately copy the behavior of a neural network policy with a decision\ntree. We propose the Decision Tree Policy Optimization (DTPO) algorithm that\ndirectly optimizes the complete decision tree using policy gradients. Our\ntechnique uses established decision tree heuristics for regression to perform\npolicy optimization. We empirically show that DTPO is a competitive algorithm\ncompared to imitation learning algorithms for optimizing decision tree policies\nin reinforcement learning.\n", "link": "http://arxiv.org/abs/2408.11632v1", "date": "2024-08-21", "relevancy": 2.3798, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.489}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4825}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Interpretable%20Decision%20Tree%20Policies%20for%20Reinforcement%0A%20%20Learning&body=Title%3A%20Optimizing%20Interpretable%20Decision%20Tree%20Policies%20for%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Dani%C3%ABl%20Vos%20and%20Sicco%20Verwer%0AAbstract%3A%20%20%20Reinforcement%20learning%20techniques%20leveraging%20deep%20learning%20have%20made%0Atremendous%20progress%20in%20recent%20years.%20However%2C%20the%20complexity%20of%20neural%20networks%0Aprevents%20practitioners%20from%20understanding%20their%20behavior.%20Decision%20trees%20have%0Agained%20increased%20attention%20in%20supervised%20learning%20for%20their%20inherent%0Ainterpretability%2C%20enabling%20modelers%20to%20understand%20the%20exact%20prediction%20process%0Aafter%20learning.%20This%20paper%20considers%20the%20problem%20of%20optimizing%20interpretable%0Adecision%20tree%20policies%20to%20replace%20neural%20networks%20in%20reinforcement%20learning%0Asettings.%20Previous%20works%20have%20relaxed%20the%20tree%20structure%2C%20restricted%20to%0Aoptimizing%20only%20tree%20leaves%2C%20or%20applied%20imitation%20learning%20techniques%20to%0Aapproximately%20copy%20the%20behavior%20of%20a%20neural%20network%20policy%20with%20a%20decision%0Atree.%20We%20propose%20the%20Decision%20Tree%20Policy%20Optimization%20%28DTPO%29%20algorithm%20that%0Adirectly%20optimizes%20the%20complete%20decision%20tree%20using%20policy%20gradients.%20Our%0Atechnique%20uses%20established%20decision%20tree%20heuristics%20for%20regression%20to%20perform%0Apolicy%20optimization.%20We%20empirically%20show%20that%20DTPO%20is%20a%20competitive%20algorithm%0Acompared%20to%20imitation%20learning%20algorithms%20for%20optimizing%20decision%20tree%20policies%0Ain%20reinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Interpretable%2520Decision%2520Tree%2520Policies%2520for%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DDani%25C3%25ABl%2520Vos%2520and%2520Sicco%2520Verwer%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520techniques%2520leveraging%2520deep%2520learning%2520have%2520made%250Atremendous%2520progress%2520in%2520recent%2520years.%2520However%252C%2520the%2520complexity%2520of%2520neural%2520networks%250Aprevents%2520practitioners%2520from%2520understanding%2520their%2520behavior.%2520Decision%2520trees%2520have%250Agained%2520increased%2520attention%2520in%2520supervised%2520learning%2520for%2520their%2520inherent%250Ainterpretability%252C%2520enabling%2520modelers%2520to%2520understand%2520the%2520exact%2520prediction%2520process%250Aafter%2520learning.%2520This%2520paper%2520considers%2520the%2520problem%2520of%2520optimizing%2520interpretable%250Adecision%2520tree%2520policies%2520to%2520replace%2520neural%2520networks%2520in%2520reinforcement%2520learning%250Asettings.%2520Previous%2520works%2520have%2520relaxed%2520the%2520tree%2520structure%252C%2520restricted%2520to%250Aoptimizing%2520only%2520tree%2520leaves%252C%2520or%2520applied%2520imitation%2520learning%2520techniques%2520to%250Aapproximately%2520copy%2520the%2520behavior%2520of%2520a%2520neural%2520network%2520policy%2520with%2520a%2520decision%250Atree.%2520We%2520propose%2520the%2520Decision%2520Tree%2520Policy%2520Optimization%2520%2528DTPO%2529%2520algorithm%2520that%250Adirectly%2520optimizes%2520the%2520complete%2520decision%2520tree%2520using%2520policy%2520gradients.%2520Our%250Atechnique%2520uses%2520established%2520decision%2520tree%2520heuristics%2520for%2520regression%2520to%2520perform%250Apolicy%2520optimization.%2520We%2520empirically%2520show%2520that%2520DTPO%2520is%2520a%2520competitive%2520algorithm%250Acompared%2520to%2520imitation%2520learning%2520algorithms%2520for%2520optimizing%2520decision%2520tree%2520policies%250Ain%2520reinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Interpretable%20Decision%20Tree%20Policies%20for%20Reinforcement%0A%20%20Learning&entry.906535625=Dani%C3%ABl%20Vos%20and%20Sicco%20Verwer&entry.1292438233=%20%20Reinforcement%20learning%20techniques%20leveraging%20deep%20learning%20have%20made%0Atremendous%20progress%20in%20recent%20years.%20However%2C%20the%20complexity%20of%20neural%20networks%0Aprevents%20practitioners%20from%20understanding%20their%20behavior.%20Decision%20trees%20have%0Agained%20increased%20attention%20in%20supervised%20learning%20for%20their%20inherent%0Ainterpretability%2C%20enabling%20modelers%20to%20understand%20the%20exact%20prediction%20process%0Aafter%20learning.%20This%20paper%20considers%20the%20problem%20of%20optimizing%20interpretable%0Adecision%20tree%20policies%20to%20replace%20neural%20networks%20in%20reinforcement%20learning%0Asettings.%20Previous%20works%20have%20relaxed%20the%20tree%20structure%2C%20restricted%20to%0Aoptimizing%20only%20tree%20leaves%2C%20or%20applied%20imitation%20learning%20techniques%20to%0Aapproximately%20copy%20the%20behavior%20of%20a%20neural%20network%20policy%20with%20a%20decision%0Atree.%20We%20propose%20the%20Decision%20Tree%20Policy%20Optimization%20%28DTPO%29%20algorithm%20that%0Adirectly%20optimizes%20the%20complete%20decision%20tree%20using%20policy%20gradients.%20Our%0Atechnique%20uses%20established%20decision%20tree%20heuristics%20for%20regression%20to%20perform%0Apolicy%20optimization.%20We%20empirically%20show%20that%20DTPO%20is%20a%20competitive%20algorithm%0Acompared%20to%20imitation%20learning%20algorithms%20for%20optimizing%20decision%20tree%20policies%0Ain%20reinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11632v1&entry.124074799=Read"},
{"title": "Memorization In In-Context Learning", "author": "Shahriar Golchin and Mihai Surdeanu and Steven Bethard and Eduardo Blanco and Ellen Riloff", "abstract": "  In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind these performance improvements\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance across various ICL regimes: zero-shot, few-shot, and many-shot. Our\nmost notable findings include: (1) ICL significantly surfaces memorization\ncompared to zero-shot learning in most cases; (2) demonstrations, without their\nlabels, are the most effective element in surfacing memorization; (3) ICL\nimproves performance when the surfaced memorization in few-shot regimes reaches\na high level (about 40%); and (4) there is a very strong correlation between\nperformance and memorization in ICL when it outperforms zero-shot learning.\nOverall, our study uncovers a hidden phenomenon -- memorization -- at the core\nof ICL, raising an important question: to what extent do LLMs truly generalize\nfrom demonstrations in ICL, and how much of their success is due to\nmemorization?\n", "link": "http://arxiv.org/abs/2408.11546v1", "date": "2024-08-21", "relevancy": 2.377, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5025}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4639}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorization%20In%20In-Context%20Learning&body=Title%3A%20Memorization%20In%20In-Context%20Learning%0AAuthor%3A%20Shahriar%20Golchin%20and%20Mihai%20Surdeanu%20and%20Steven%20Bethard%20and%20Eduardo%20Blanco%20and%20Ellen%20Riloff%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20has%20proven%20to%20be%20an%20effective%20strategy%20for%0Aimproving%20the%20performance%20of%20large%20language%20models%20%28LLMs%29%20with%20no%20additional%0Atraining.%20However%2C%20the%20exact%20mechanism%20behind%20these%20performance%20improvements%0Aremains%20unclear.%20This%20study%20is%20the%20first%20to%20show%20how%20ICL%20surfaces%20memorized%0Atraining%20data%20and%20to%20explore%20the%20correlation%20between%20this%20memorization%20and%0Aperformance%20across%20various%20ICL%20regimes%3A%20zero-shot%2C%20few-shot%2C%20and%20many-shot.%20Our%0Amost%20notable%20findings%20include%3A%20%281%29%20ICL%20significantly%20surfaces%20memorization%0Acompared%20to%20zero-shot%20learning%20in%20most%20cases%3B%20%282%29%20demonstrations%2C%20without%20their%0Alabels%2C%20are%20the%20most%20effective%20element%20in%20surfacing%20memorization%3B%20%283%29%20ICL%0Aimproves%20performance%20when%20the%20surfaced%20memorization%20in%20few-shot%20regimes%20reaches%0Aa%20high%20level%20%28about%2040%25%29%3B%20and%20%284%29%20there%20is%20a%20very%20strong%20correlation%20between%0Aperformance%20and%20memorization%20in%20ICL%20when%20it%20outperforms%20zero-shot%20learning.%0AOverall%2C%20our%20study%20uncovers%20a%20hidden%20phenomenon%20--%20memorization%20--%20at%20the%20core%0Aof%20ICL%2C%20raising%20an%20important%20question%3A%20to%20what%20extent%20do%20LLMs%20truly%20generalize%0Afrom%20demonstrations%20in%20ICL%2C%20and%20how%20much%20of%20their%20success%20is%20due%20to%0Amemorization%3F%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorization%2520In%2520In-Context%2520Learning%26entry.906535625%3DShahriar%2520Golchin%2520and%2520Mihai%2520Surdeanu%2520and%2520Steven%2520Bethard%2520and%2520Eduardo%2520Blanco%2520and%2520Ellen%2520Riloff%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520has%2520proven%2520to%2520be%2520an%2520effective%2520strategy%2520for%250Aimproving%2520the%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520no%2520additional%250Atraining.%2520However%252C%2520the%2520exact%2520mechanism%2520behind%2520these%2520performance%2520improvements%250Aremains%2520unclear.%2520This%2520study%2520is%2520the%2520first%2520to%2520show%2520how%2520ICL%2520surfaces%2520memorized%250Atraining%2520data%2520and%2520to%2520explore%2520the%2520correlation%2520between%2520this%2520memorization%2520and%250Aperformance%2520across%2520various%2520ICL%2520regimes%253A%2520zero-shot%252C%2520few-shot%252C%2520and%2520many-shot.%2520Our%250Amost%2520notable%2520findings%2520include%253A%2520%25281%2529%2520ICL%2520significantly%2520surfaces%2520memorization%250Acompared%2520to%2520zero-shot%2520learning%2520in%2520most%2520cases%253B%2520%25282%2529%2520demonstrations%252C%2520without%2520their%250Alabels%252C%2520are%2520the%2520most%2520effective%2520element%2520in%2520surfacing%2520memorization%253B%2520%25283%2529%2520ICL%250Aimproves%2520performance%2520when%2520the%2520surfaced%2520memorization%2520in%2520few-shot%2520regimes%2520reaches%250Aa%2520high%2520level%2520%2528about%252040%2525%2529%253B%2520and%2520%25284%2529%2520there%2520is%2520a%2520very%2520strong%2520correlation%2520between%250Aperformance%2520and%2520memorization%2520in%2520ICL%2520when%2520it%2520outperforms%2520zero-shot%2520learning.%250AOverall%252C%2520our%2520study%2520uncovers%2520a%2520hidden%2520phenomenon%2520--%2520memorization%2520--%2520at%2520the%2520core%250Aof%2520ICL%252C%2520raising%2520an%2520important%2520question%253A%2520to%2520what%2520extent%2520do%2520LLMs%2520truly%2520generalize%250Afrom%2520demonstrations%2520in%2520ICL%252C%2520and%2520how%2520much%2520of%2520their%2520success%2520is%2520due%2520to%250Amemorization%253F%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorization%20In%20In-Context%20Learning&entry.906535625=Shahriar%20Golchin%20and%20Mihai%20Surdeanu%20and%20Steven%20Bethard%20and%20Eduardo%20Blanco%20and%20Ellen%20Riloff&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20has%20proven%20to%20be%20an%20effective%20strategy%20for%0Aimproving%20the%20performance%20of%20large%20language%20models%20%28LLMs%29%20with%20no%20additional%0Atraining.%20However%2C%20the%20exact%20mechanism%20behind%20these%20performance%20improvements%0Aremains%20unclear.%20This%20study%20is%20the%20first%20to%20show%20how%20ICL%20surfaces%20memorized%0Atraining%20data%20and%20to%20explore%20the%20correlation%20between%20this%20memorization%20and%0Aperformance%20across%20various%20ICL%20regimes%3A%20zero-shot%2C%20few-shot%2C%20and%20many-shot.%20Our%0Amost%20notable%20findings%20include%3A%20%281%29%20ICL%20significantly%20surfaces%20memorization%0Acompared%20to%20zero-shot%20learning%20in%20most%20cases%3B%20%282%29%20demonstrations%2C%20without%20their%0Alabels%2C%20are%20the%20most%20effective%20element%20in%20surfacing%20memorization%3B%20%283%29%20ICL%0Aimproves%20performance%20when%20the%20surfaced%20memorization%20in%20few-shot%20regimes%20reaches%0Aa%20high%20level%20%28about%2040%25%29%3B%20and%20%284%29%20there%20is%20a%20very%20strong%20correlation%20between%0Aperformance%20and%20memorization%20in%20ICL%20when%20it%20outperforms%20zero-shot%20learning.%0AOverall%2C%20our%20study%20uncovers%20a%20hidden%20phenomenon%20--%20memorization%20--%20at%20the%20core%0Aof%20ICL%2C%20raising%20an%20important%20question%3A%20to%20what%20extent%20do%20LLMs%20truly%20generalize%0Afrom%20demonstrations%20in%20ICL%2C%20and%20how%20much%20of%20their%20success%20is%20due%20to%0Amemorization%3F%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11546v1&entry.124074799=Read"},
{"title": "Suppressing unknown disturbances to dynamical systems using machine\n  learning", "author": "Juan G. Restrepo and Clayton P. Byers and Per Sebastian Skardal", "abstract": "  Identifying and suppressing unknown disturbances to dynamical systems is a\nproblem with applications in many different fields. Here we present a\nmodel-free method to identify and suppress an unknown disturbance to an unknown\nsystem based only on previous observations of the system under the influence of\na known forcing function. We find that, under very mild restrictions on the\ntraining function, our method is able to robustly identify and suppress a large\nclass of unknown disturbances. We illustrate our scheme with the identification\nof both deterministic and stochastic unknown disturbances to an analog electric\nchaotic circuit and with numerical examples where a chaotic disturbance to\nvarious chaotic dynamical systems is identified and suppressed.\n", "link": "http://arxiv.org/abs/2307.03690v5", "date": "2024-08-21", "relevancy": 2.3629, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4894}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4644}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20using%20machine%0A%20%20learning&body=Title%3A%20Suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20using%20machine%0A%20%20learning%0AAuthor%3A%20Juan%20G.%20Restrepo%20and%20Clayton%20P.%20Byers%20and%20Per%20Sebastian%20Skardal%0AAbstract%3A%20%20%20Identifying%20and%20suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20is%20a%0Aproblem%20with%20applications%20in%20many%20different%20fields.%20Here%20we%20present%20a%0Amodel-free%20method%20to%20identify%20and%20suppress%20an%20unknown%20disturbance%20to%20an%20unknown%0Asystem%20based%20only%20on%20previous%20observations%20of%20the%20system%20under%20the%20influence%20of%0Aa%20known%20forcing%20function.%20We%20find%20that%2C%20under%20very%20mild%20restrictions%20on%20the%0Atraining%20function%2C%20our%20method%20is%20able%20to%20robustly%20identify%20and%20suppress%20a%20large%0Aclass%20of%20unknown%20disturbances.%20We%20illustrate%20our%20scheme%20with%20the%20identification%0Aof%20both%20deterministic%20and%20stochastic%20unknown%20disturbances%20to%20an%20analog%20electric%0Achaotic%20circuit%20and%20with%20numerical%20examples%20where%20a%20chaotic%20disturbance%20to%0Avarious%20chaotic%20dynamical%20systems%20is%20identified%20and%20suppressed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.03690v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuppressing%2520unknown%2520disturbances%2520to%2520dynamical%2520systems%2520using%2520machine%250A%2520%2520learning%26entry.906535625%3DJuan%2520G.%2520Restrepo%2520and%2520Clayton%2520P.%2520Byers%2520and%2520Per%2520Sebastian%2520Skardal%26entry.1292438233%3D%2520%2520Identifying%2520and%2520suppressing%2520unknown%2520disturbances%2520to%2520dynamical%2520systems%2520is%2520a%250Aproblem%2520with%2520applications%2520in%2520many%2520different%2520fields.%2520Here%2520we%2520present%2520a%250Amodel-free%2520method%2520to%2520identify%2520and%2520suppress%2520an%2520unknown%2520disturbance%2520to%2520an%2520unknown%250Asystem%2520based%2520only%2520on%2520previous%2520observations%2520of%2520the%2520system%2520under%2520the%2520influence%2520of%250Aa%2520known%2520forcing%2520function.%2520We%2520find%2520that%252C%2520under%2520very%2520mild%2520restrictions%2520on%2520the%250Atraining%2520function%252C%2520our%2520method%2520is%2520able%2520to%2520robustly%2520identify%2520and%2520suppress%2520a%2520large%250Aclass%2520of%2520unknown%2520disturbances.%2520We%2520illustrate%2520our%2520scheme%2520with%2520the%2520identification%250Aof%2520both%2520deterministic%2520and%2520stochastic%2520unknown%2520disturbances%2520to%2520an%2520analog%2520electric%250Achaotic%2520circuit%2520and%2520with%2520numerical%2520examples%2520where%2520a%2520chaotic%2520disturbance%2520to%250Avarious%2520chaotic%2520dynamical%2520systems%2520is%2520identified%2520and%2520suppressed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.03690v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20using%20machine%0A%20%20learning&entry.906535625=Juan%20G.%20Restrepo%20and%20Clayton%20P.%20Byers%20and%20Per%20Sebastian%20Skardal&entry.1292438233=%20%20Identifying%20and%20suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20is%20a%0Aproblem%20with%20applications%20in%20many%20different%20fields.%20Here%20we%20present%20a%0Amodel-free%20method%20to%20identify%20and%20suppress%20an%20unknown%20disturbance%20to%20an%20unknown%0Asystem%20based%20only%20on%20previous%20observations%20of%20the%20system%20under%20the%20influence%20of%0Aa%20known%20forcing%20function.%20We%20find%20that%2C%20under%20very%20mild%20restrictions%20on%20the%0Atraining%20function%2C%20our%20method%20is%20able%20to%20robustly%20identify%20and%20suppress%20a%20large%0Aclass%20of%20unknown%20disturbances.%20We%20illustrate%20our%20scheme%20with%20the%20identification%0Aof%20both%20deterministic%20and%20stochastic%20unknown%20disturbances%20to%20an%20analog%20electric%0Achaotic%20circuit%20and%20with%20numerical%20examples%20where%20a%20chaotic%20disturbance%20to%0Avarious%20chaotic%20dynamical%20systems%20is%20identified%20and%20suppressed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.03690v5&entry.124074799=Read"},
{"title": "OccNeRF: Advancing 3D Occupancy Prediction in LiDAR-Free Environments", "author": "Chubin Zhang and Juncheng Yan and Yi Wei and Jiaxin Li and Li Liu and Yansong Tang and Yueqi Duan and Jiwen Lu", "abstract": "  Occupancy prediction reconstructs 3D structures of surrounding environments.\nIt provides detailed information for autonomous driving planning and\nnavigation. However, most existing methods heavily rely on the LiDAR point\nclouds to generate occupancy ground truth, which is not available in the\nvision-based system. In this paper, we propose an OccNeRF method for training\noccupancy networks without 3D supervision. Different from previous works which\nconsider a bounded scene, we parameterize the reconstructed occupancy fields\nand reorganize the sampling strategy to align with the cameras' infinite\nperceptive range. The neural rendering is adopted to convert occupancy fields\nto multi-camera depth maps, supervised by multi-frame photometric consistency.\nMoreover, for semantic occupancy prediction, we design several strategies to\npolish the prompts and filter the outputs of a pretrained open-vocabulary 2D\nsegmentation model. Extensive experiments for both self-supervised depth\nestimation and 3D occupancy prediction tasks on nuScenes and SemanticKITTI\ndatasets demonstrate the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2312.09243v3", "date": "2024-08-21", "relevancy": 2.3507, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5956}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5919}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OccNeRF%3A%20Advancing%203D%20Occupancy%20Prediction%20in%20LiDAR-Free%20Environments&body=Title%3A%20OccNeRF%3A%20Advancing%203D%20Occupancy%20Prediction%20in%20LiDAR-Free%20Environments%0AAuthor%3A%20Chubin%20Zhang%20and%20Juncheng%20Yan%20and%20Yi%20Wei%20and%20Jiaxin%20Li%20and%20Li%20Liu%20and%20Yansong%20Tang%20and%20Yueqi%20Duan%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Occupancy%20prediction%20reconstructs%203D%20structures%20of%20surrounding%20environments.%0AIt%20provides%20detailed%20information%20for%20autonomous%20driving%20planning%20and%0Anavigation.%20However%2C%20most%20existing%20methods%20heavily%20rely%20on%20the%20LiDAR%20point%0Aclouds%20to%20generate%20occupancy%20ground%20truth%2C%20which%20is%20not%20available%20in%20the%0Avision-based%20system.%20In%20this%20paper%2C%20we%20propose%20an%20OccNeRF%20method%20for%20training%0Aoccupancy%20networks%20without%203D%20supervision.%20Different%20from%20previous%20works%20which%0Aconsider%20a%20bounded%20scene%2C%20we%20parameterize%20the%20reconstructed%20occupancy%20fields%0Aand%20reorganize%20the%20sampling%20strategy%20to%20align%20with%20the%20cameras%27%20infinite%0Aperceptive%20range.%20The%20neural%20rendering%20is%20adopted%20to%20convert%20occupancy%20fields%0Ato%20multi-camera%20depth%20maps%2C%20supervised%20by%20multi-frame%20photometric%20consistency.%0AMoreover%2C%20for%20semantic%20occupancy%20prediction%2C%20we%20design%20several%20strategies%20to%0Apolish%20the%20prompts%20and%20filter%20the%20outputs%20of%20a%20pretrained%20open-vocabulary%202D%0Asegmentation%20model.%20Extensive%20experiments%20for%20both%20self-supervised%20depth%0Aestimation%20and%203D%20occupancy%20prediction%20tasks%20on%20nuScenes%20and%20SemanticKITTI%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09243v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccNeRF%253A%2520Advancing%25203D%2520Occupancy%2520Prediction%2520in%2520LiDAR-Free%2520Environments%26entry.906535625%3DChubin%2520Zhang%2520and%2520Juncheng%2520Yan%2520and%2520Yi%2520Wei%2520and%2520Jiaxin%2520Li%2520and%2520Li%2520Liu%2520and%2520Yansong%2520Tang%2520and%2520Yueqi%2520Duan%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Occupancy%2520prediction%2520reconstructs%25203D%2520structures%2520of%2520surrounding%2520environments.%250AIt%2520provides%2520detailed%2520information%2520for%2520autonomous%2520driving%2520planning%2520and%250Anavigation.%2520However%252C%2520most%2520existing%2520methods%2520heavily%2520rely%2520on%2520the%2520LiDAR%2520point%250Aclouds%2520to%2520generate%2520occupancy%2520ground%2520truth%252C%2520which%2520is%2520not%2520available%2520in%2520the%250Avision-based%2520system.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520OccNeRF%2520method%2520for%2520training%250Aoccupancy%2520networks%2520without%25203D%2520supervision.%2520Different%2520from%2520previous%2520works%2520which%250Aconsider%2520a%2520bounded%2520scene%252C%2520we%2520parameterize%2520the%2520reconstructed%2520occupancy%2520fields%250Aand%2520reorganize%2520the%2520sampling%2520strategy%2520to%2520align%2520with%2520the%2520cameras%2527%2520infinite%250Aperceptive%2520range.%2520The%2520neural%2520rendering%2520is%2520adopted%2520to%2520convert%2520occupancy%2520fields%250Ato%2520multi-camera%2520depth%2520maps%252C%2520supervised%2520by%2520multi-frame%2520photometric%2520consistency.%250AMoreover%252C%2520for%2520semantic%2520occupancy%2520prediction%252C%2520we%2520design%2520several%2520strategies%2520to%250Apolish%2520the%2520prompts%2520and%2520filter%2520the%2520outputs%2520of%2520a%2520pretrained%2520open-vocabulary%25202D%250Asegmentation%2520model.%2520Extensive%2520experiments%2520for%2520both%2520self-supervised%2520depth%250Aestimation%2520and%25203D%2520occupancy%2520prediction%2520tasks%2520on%2520nuScenes%2520and%2520SemanticKITTI%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09243v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccNeRF%3A%20Advancing%203D%20Occupancy%20Prediction%20in%20LiDAR-Free%20Environments&entry.906535625=Chubin%20Zhang%20and%20Juncheng%20Yan%20and%20Yi%20Wei%20and%20Jiaxin%20Li%20and%20Li%20Liu%20and%20Yansong%20Tang%20and%20Yueqi%20Duan%20and%20Jiwen%20Lu&entry.1292438233=%20%20Occupancy%20prediction%20reconstructs%203D%20structures%20of%20surrounding%20environments.%0AIt%20provides%20detailed%20information%20for%20autonomous%20driving%20planning%20and%0Anavigation.%20However%2C%20most%20existing%20methods%20heavily%20rely%20on%20the%20LiDAR%20point%0Aclouds%20to%20generate%20occupancy%20ground%20truth%2C%20which%20is%20not%20available%20in%20the%0Avision-based%20system.%20In%20this%20paper%2C%20we%20propose%20an%20OccNeRF%20method%20for%20training%0Aoccupancy%20networks%20without%203D%20supervision.%20Different%20from%20previous%20works%20which%0Aconsider%20a%20bounded%20scene%2C%20we%20parameterize%20the%20reconstructed%20occupancy%20fields%0Aand%20reorganize%20the%20sampling%20strategy%20to%20align%20with%20the%20cameras%27%20infinite%0Aperceptive%20range.%20The%20neural%20rendering%20is%20adopted%20to%20convert%20occupancy%20fields%0Ato%20multi-camera%20depth%20maps%2C%20supervised%20by%20multi-frame%20photometric%20consistency.%0AMoreover%2C%20for%20semantic%20occupancy%20prediction%2C%20we%20design%20several%20strategies%20to%0Apolish%20the%20prompts%20and%20filter%20the%20outputs%20of%20a%20pretrained%20open-vocabulary%202D%0Asegmentation%20model.%20Extensive%20experiments%20for%20both%20self-supervised%20depth%0Aestimation%20and%203D%20occupancy%20prediction%20tasks%20on%20nuScenes%20and%20SemanticKITTI%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09243v3&entry.124074799=Read"},
{"title": "Deep Generative Models in Robotics: A Survey on Learning from Multimodal\n  Demonstrations", "author": "Julen Urain and Ajay Mandlekar and Yilun Du and Mahi Shafiullah and Danfei Xu and Katerina Fragkiadaki and Georgia Chalvatzaki and Jan Peters", "abstract": "  Learning from Demonstrations, the field that proposes to learn robot behavior\nmodels from data, is gaining popularity with the emergence of deep generative\nmodels. Although the problem has been studied for years under names such as\nImitation Learning, Behavioral Cloning, or Inverse Reinforcement Learning,\nclassical methods have relied on models that don't capture complex data\ndistributions well or don't scale well to large numbers of demonstrations. In\nrecent years, the robot learning community has shown increasing interest in\nusing deep generative models to capture the complexity of large datasets. In\nthis survey, we aim to provide a unified and comprehensive review of the last\nyear's progress in the use of deep generative models in robotics. We present\nthe different types of models that the community has explored, such as\nenergy-based models, diffusion models, action value maps, or generative\nadversarial networks. We also present the different types of applications in\nwhich deep generative models have been used, from grasp generation to\ntrajectory generation or cost learning. One of the most important elements of\ngenerative models is the generalization out of distributions. In our survey, we\nreview the different decisions the community has made to improve the\ngeneralization of the learned models. Finally, we highlight the research\nchallenges and propose a number of future directions for learning deep\ngenerative models in robotics.\n", "link": "http://arxiv.org/abs/2408.04380v3", "date": "2024-08-21", "relevancy": 2.3503, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5966}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5864}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations&body=Title%3A%20Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations%0AAuthor%3A%20Julen%20Urain%20and%20Ajay%20Mandlekar%20and%20Yilun%20Du%20and%20Mahi%20Shafiullah%20and%20Danfei%20Xu%20and%20Katerina%20Fragkiadaki%20and%20Georgia%20Chalvatzaki%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Learning%20from%20Demonstrations%2C%20the%20field%20that%20proposes%20to%20learn%20robot%20behavior%0Amodels%20from%20data%2C%20is%20gaining%20popularity%20with%20the%20emergence%20of%20deep%20generative%0Amodels.%20Although%20the%20problem%20has%20been%20studied%20for%20years%20under%20names%20such%20as%0AImitation%20Learning%2C%20Behavioral%20Cloning%2C%20or%20Inverse%20Reinforcement%20Learning%2C%0Aclassical%20methods%20have%20relied%20on%20models%20that%20don%27t%20capture%20complex%20data%0Adistributions%20well%20or%20don%27t%20scale%20well%20to%20large%20numbers%20of%20demonstrations.%20In%0Arecent%20years%2C%20the%20robot%20learning%20community%20has%20shown%20increasing%20interest%20in%0Ausing%20deep%20generative%20models%20to%20capture%20the%20complexity%20of%20large%20datasets.%20In%0Athis%20survey%2C%20we%20aim%20to%20provide%20a%20unified%20and%20comprehensive%20review%20of%20the%20last%0Ayear%27s%20progress%20in%20the%20use%20of%20deep%20generative%20models%20in%20robotics.%20We%20present%0Athe%20different%20types%20of%20models%20that%20the%20community%20has%20explored%2C%20such%20as%0Aenergy-based%20models%2C%20diffusion%20models%2C%20action%20value%20maps%2C%20or%20generative%0Aadversarial%20networks.%20We%20also%20present%20the%20different%20types%20of%20applications%20in%0Awhich%20deep%20generative%20models%20have%20been%20used%2C%20from%20grasp%20generation%20to%0Atrajectory%20generation%20or%20cost%20learning.%20One%20of%20the%20most%20important%20elements%20of%0Agenerative%20models%20is%20the%20generalization%20out%20of%20distributions.%20In%20our%20survey%2C%20we%0Areview%20the%20different%20decisions%20the%20community%20has%20made%20to%20improve%20the%0Ageneralization%20of%20the%20learned%20models.%20Finally%2C%20we%20highlight%20the%20research%0Achallenges%20and%20propose%20a%20number%20of%20future%20directions%20for%20learning%20deep%0Agenerative%20models%20in%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04380v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Generative%2520Models%2520in%2520Robotics%253A%2520A%2520Survey%2520on%2520Learning%2520from%2520Multimodal%250A%2520%2520Demonstrations%26entry.906535625%3DJulen%2520Urain%2520and%2520Ajay%2520Mandlekar%2520and%2520Yilun%2520Du%2520and%2520Mahi%2520Shafiullah%2520and%2520Danfei%2520Xu%2520and%2520Katerina%2520Fragkiadaki%2520and%2520Georgia%2520Chalvatzaki%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Learning%2520from%2520Demonstrations%252C%2520the%2520field%2520that%2520proposes%2520to%2520learn%2520robot%2520behavior%250Amodels%2520from%2520data%252C%2520is%2520gaining%2520popularity%2520with%2520the%2520emergence%2520of%2520deep%2520generative%250Amodels.%2520Although%2520the%2520problem%2520has%2520been%2520studied%2520for%2520years%2520under%2520names%2520such%2520as%250AImitation%2520Learning%252C%2520Behavioral%2520Cloning%252C%2520or%2520Inverse%2520Reinforcement%2520Learning%252C%250Aclassical%2520methods%2520have%2520relied%2520on%2520models%2520that%2520don%2527t%2520capture%2520complex%2520data%250Adistributions%2520well%2520or%2520don%2527t%2520scale%2520well%2520to%2520large%2520numbers%2520of%2520demonstrations.%2520In%250Arecent%2520years%252C%2520the%2520robot%2520learning%2520community%2520has%2520shown%2520increasing%2520interest%2520in%250Ausing%2520deep%2520generative%2520models%2520to%2520capture%2520the%2520complexity%2520of%2520large%2520datasets.%2520In%250Athis%2520survey%252C%2520we%2520aim%2520to%2520provide%2520a%2520unified%2520and%2520comprehensive%2520review%2520of%2520the%2520last%250Ayear%2527s%2520progress%2520in%2520the%2520use%2520of%2520deep%2520generative%2520models%2520in%2520robotics.%2520We%2520present%250Athe%2520different%2520types%2520of%2520models%2520that%2520the%2520community%2520has%2520explored%252C%2520such%2520as%250Aenergy-based%2520models%252C%2520diffusion%2520models%252C%2520action%2520value%2520maps%252C%2520or%2520generative%250Aadversarial%2520networks.%2520We%2520also%2520present%2520the%2520different%2520types%2520of%2520applications%2520in%250Awhich%2520deep%2520generative%2520models%2520have%2520been%2520used%252C%2520from%2520grasp%2520generation%2520to%250Atrajectory%2520generation%2520or%2520cost%2520learning.%2520One%2520of%2520the%2520most%2520important%2520elements%2520of%250Agenerative%2520models%2520is%2520the%2520generalization%2520out%2520of%2520distributions.%2520In%2520our%2520survey%252C%2520we%250Areview%2520the%2520different%2520decisions%2520the%2520community%2520has%2520made%2520to%2520improve%2520the%250Ageneralization%2520of%2520the%2520learned%2520models.%2520Finally%252C%2520we%2520highlight%2520the%2520research%250Achallenges%2520and%2520propose%2520a%2520number%2520of%2520future%2520directions%2520for%2520learning%2520deep%250Agenerative%2520models%2520in%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04380v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations&entry.906535625=Julen%20Urain%20and%20Ajay%20Mandlekar%20and%20Yilun%20Du%20and%20Mahi%20Shafiullah%20and%20Danfei%20Xu%20and%20Katerina%20Fragkiadaki%20and%20Georgia%20Chalvatzaki%20and%20Jan%20Peters&entry.1292438233=%20%20Learning%20from%20Demonstrations%2C%20the%20field%20that%20proposes%20to%20learn%20robot%20behavior%0Amodels%20from%20data%2C%20is%20gaining%20popularity%20with%20the%20emergence%20of%20deep%20generative%0Amodels.%20Although%20the%20problem%20has%20been%20studied%20for%20years%20under%20names%20such%20as%0AImitation%20Learning%2C%20Behavioral%20Cloning%2C%20or%20Inverse%20Reinforcement%20Learning%2C%0Aclassical%20methods%20have%20relied%20on%20models%20that%20don%27t%20capture%20complex%20data%0Adistributions%20well%20or%20don%27t%20scale%20well%20to%20large%20numbers%20of%20demonstrations.%20In%0Arecent%20years%2C%20the%20robot%20learning%20community%20has%20shown%20increasing%20interest%20in%0Ausing%20deep%20generative%20models%20to%20capture%20the%20complexity%20of%20large%20datasets.%20In%0Athis%20survey%2C%20we%20aim%20to%20provide%20a%20unified%20and%20comprehensive%20review%20of%20the%20last%0Ayear%27s%20progress%20in%20the%20use%20of%20deep%20generative%20models%20in%20robotics.%20We%20present%0Athe%20different%20types%20of%20models%20that%20the%20community%20has%20explored%2C%20such%20as%0Aenergy-based%20models%2C%20diffusion%20models%2C%20action%20value%20maps%2C%20or%20generative%0Aadversarial%20networks.%20We%20also%20present%20the%20different%20types%20of%20applications%20in%0Awhich%20deep%20generative%20models%20have%20been%20used%2C%20from%20grasp%20generation%20to%0Atrajectory%20generation%20or%20cost%20learning.%20One%20of%20the%20most%20important%20elements%20of%0Agenerative%20models%20is%20the%20generalization%20out%20of%20distributions.%20In%20our%20survey%2C%20we%0Areview%20the%20different%20decisions%20the%20community%20has%20made%20to%20improve%20the%0Ageneralization%20of%20the%20learned%20models.%20Finally%2C%20we%20highlight%20the%20research%0Achallenges%20and%20propose%20a%20number%20of%20future%20directions%20for%20learning%20deep%0Agenerative%20models%20in%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04380v3&entry.124074799=Read"},
{"title": "Online Learning-Based Inertial Parameter Identification of Unknown\n  Object for Model-Based Control of Wheeled Humanoids", "author": "Donghoon Baek and Bo Peng and Saurabh Gupta and Joao Ramos", "abstract": "  Identifying the dynamic properties of manipulated objects is essential for\nsafe and accurate robot control. Most methods rely on low noise force torque\nsensors, long exciting signals, and solving nonlinear optimization problems,\nmaking the estimation process slow. In this work, we propose a fast, online\nlearning based inertial parameter estimation framework that enhances model\nbased control. We aim to quickly and accurately estimate the parameters of an\nunknown object using only the robot's proprioception through end to end\nlearning, which is applicable for real-time system. To effectively capture\nfeatures in robot proprioception affected by object dynamics and address the\nchallenge of obtaining ground truth inertial parameters in the real world, we\ndeveloped a high fidelity simulation that uses more accurate robot dynamics\nthrough real-to-sim adaptation. Since our adaptation focuses solely on the\nrobot, task-relevant data (e.g., holding an object) is not required from the\nreal world, simplifying the data collection process. Moreover, we address both\nparametric and non-parametric modeling errors independently using Robot System\nIdentification and Gaussian Processes. We validate our estimator to assess how\nquickly and accurately it can estimate physically feasible parameters of an\nmanipulated object given a specific trajectory obtained from a wheeled humanoid\nrobot. Our estimator achieves faster estimation speeds (around 0.1 seconds)\nwhile maintaining accuracy comparable to other methods. Additionally, our\nestimator further highlight its benefits in improving the performance of model\nbased control by compensating object's dynamics and reinitializing new\nequilibrium point of wheeled humanoid\n", "link": "http://arxiv.org/abs/2309.09810v3", "date": "2024-08-21", "relevancy": 2.331, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.641}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6135}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Learning-Based%20Inertial%20Parameter%20Identification%20of%20Unknown%0A%20%20Object%20for%20Model-Based%20Control%20of%20Wheeled%20Humanoids&body=Title%3A%20Online%20Learning-Based%20Inertial%20Parameter%20Identification%20of%20Unknown%0A%20%20Object%20for%20Model-Based%20Control%20of%20Wheeled%20Humanoids%0AAuthor%3A%20Donghoon%20Baek%20and%20Bo%20Peng%20and%20Saurabh%20Gupta%20and%20Joao%20Ramos%0AAbstract%3A%20%20%20Identifying%20the%20dynamic%20properties%20of%20manipulated%20objects%20is%20essential%20for%0Asafe%20and%20accurate%20robot%20control.%20Most%20methods%20rely%20on%20low%20noise%20force%20torque%0Asensors%2C%20long%20exciting%20signals%2C%20and%20solving%20nonlinear%20optimization%20problems%2C%0Amaking%20the%20estimation%20process%20slow.%20In%20this%20work%2C%20we%20propose%20a%20fast%2C%20online%0Alearning%20based%20inertial%20parameter%20estimation%20framework%20that%20enhances%20model%0Abased%20control.%20We%20aim%20to%20quickly%20and%20accurately%20estimate%20the%20parameters%20of%20an%0Aunknown%20object%20using%20only%20the%20robot%27s%20proprioception%20through%20end%20to%20end%0Alearning%2C%20which%20is%20applicable%20for%20real-time%20system.%20To%20effectively%20capture%0Afeatures%20in%20robot%20proprioception%20affected%20by%20object%20dynamics%20and%20address%20the%0Achallenge%20of%20obtaining%20ground%20truth%20inertial%20parameters%20in%20the%20real%20world%2C%20we%0Adeveloped%20a%20high%20fidelity%20simulation%20that%20uses%20more%20accurate%20robot%20dynamics%0Athrough%20real-to-sim%20adaptation.%20Since%20our%20adaptation%20focuses%20solely%20on%20the%0Arobot%2C%20task-relevant%20data%20%28e.g.%2C%20holding%20an%20object%29%20is%20not%20required%20from%20the%0Areal%20world%2C%20simplifying%20the%20data%20collection%20process.%20Moreover%2C%20we%20address%20both%0Aparametric%20and%20non-parametric%20modeling%20errors%20independently%20using%20Robot%20System%0AIdentification%20and%20Gaussian%20Processes.%20We%20validate%20our%20estimator%20to%20assess%20how%0Aquickly%20and%20accurately%20it%20can%20estimate%20physically%20feasible%20parameters%20of%20an%0Amanipulated%20object%20given%20a%20specific%20trajectory%20obtained%20from%20a%20wheeled%20humanoid%0Arobot.%20Our%20estimator%20achieves%20faster%20estimation%20speeds%20%28around%200.1%20seconds%29%0Awhile%20maintaining%20accuracy%20comparable%20to%20other%20methods.%20Additionally%2C%20our%0Aestimator%20further%20highlight%20its%20benefits%20in%20improving%20the%20performance%20of%20model%0Abased%20control%20by%20compensating%20object%27s%20dynamics%20and%20reinitializing%20new%0Aequilibrium%20point%20of%20wheeled%20humanoid%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09810v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Learning-Based%2520Inertial%2520Parameter%2520Identification%2520of%2520Unknown%250A%2520%2520Object%2520for%2520Model-Based%2520Control%2520of%2520Wheeled%2520Humanoids%26entry.906535625%3DDonghoon%2520Baek%2520and%2520Bo%2520Peng%2520and%2520Saurabh%2520Gupta%2520and%2520Joao%2520Ramos%26entry.1292438233%3D%2520%2520Identifying%2520the%2520dynamic%2520properties%2520of%2520manipulated%2520objects%2520is%2520essential%2520for%250Asafe%2520and%2520accurate%2520robot%2520control.%2520Most%2520methods%2520rely%2520on%2520low%2520noise%2520force%2520torque%250Asensors%252C%2520long%2520exciting%2520signals%252C%2520and%2520solving%2520nonlinear%2520optimization%2520problems%252C%250Amaking%2520the%2520estimation%2520process%2520slow.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520fast%252C%2520online%250Alearning%2520based%2520inertial%2520parameter%2520estimation%2520framework%2520that%2520enhances%2520model%250Abased%2520control.%2520We%2520aim%2520to%2520quickly%2520and%2520accurately%2520estimate%2520the%2520parameters%2520of%2520an%250Aunknown%2520object%2520using%2520only%2520the%2520robot%2527s%2520proprioception%2520through%2520end%2520to%2520end%250Alearning%252C%2520which%2520is%2520applicable%2520for%2520real-time%2520system.%2520To%2520effectively%2520capture%250Afeatures%2520in%2520robot%2520proprioception%2520affected%2520by%2520object%2520dynamics%2520and%2520address%2520the%250Achallenge%2520of%2520obtaining%2520ground%2520truth%2520inertial%2520parameters%2520in%2520the%2520real%2520world%252C%2520we%250Adeveloped%2520a%2520high%2520fidelity%2520simulation%2520that%2520uses%2520more%2520accurate%2520robot%2520dynamics%250Athrough%2520real-to-sim%2520adaptation.%2520Since%2520our%2520adaptation%2520focuses%2520solely%2520on%2520the%250Arobot%252C%2520task-relevant%2520data%2520%2528e.g.%252C%2520holding%2520an%2520object%2529%2520is%2520not%2520required%2520from%2520the%250Areal%2520world%252C%2520simplifying%2520the%2520data%2520collection%2520process.%2520Moreover%252C%2520we%2520address%2520both%250Aparametric%2520and%2520non-parametric%2520modeling%2520errors%2520independently%2520using%2520Robot%2520System%250AIdentification%2520and%2520Gaussian%2520Processes.%2520We%2520validate%2520our%2520estimator%2520to%2520assess%2520how%250Aquickly%2520and%2520accurately%2520it%2520can%2520estimate%2520physically%2520feasible%2520parameters%2520of%2520an%250Amanipulated%2520object%2520given%2520a%2520specific%2520trajectory%2520obtained%2520from%2520a%2520wheeled%2520humanoid%250Arobot.%2520Our%2520estimator%2520achieves%2520faster%2520estimation%2520speeds%2520%2528around%25200.1%2520seconds%2529%250Awhile%2520maintaining%2520accuracy%2520comparable%2520to%2520other%2520methods.%2520Additionally%252C%2520our%250Aestimator%2520further%2520highlight%2520its%2520benefits%2520in%2520improving%2520the%2520performance%2520of%2520model%250Abased%2520control%2520by%2520compensating%2520object%2527s%2520dynamics%2520and%2520reinitializing%2520new%250Aequilibrium%2520point%2520of%2520wheeled%2520humanoid%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09810v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Learning-Based%20Inertial%20Parameter%20Identification%20of%20Unknown%0A%20%20Object%20for%20Model-Based%20Control%20of%20Wheeled%20Humanoids&entry.906535625=Donghoon%20Baek%20and%20Bo%20Peng%20and%20Saurabh%20Gupta%20and%20Joao%20Ramos&entry.1292438233=%20%20Identifying%20the%20dynamic%20properties%20of%20manipulated%20objects%20is%20essential%20for%0Asafe%20and%20accurate%20robot%20control.%20Most%20methods%20rely%20on%20low%20noise%20force%20torque%0Asensors%2C%20long%20exciting%20signals%2C%20and%20solving%20nonlinear%20optimization%20problems%2C%0Amaking%20the%20estimation%20process%20slow.%20In%20this%20work%2C%20we%20propose%20a%20fast%2C%20online%0Alearning%20based%20inertial%20parameter%20estimation%20framework%20that%20enhances%20model%0Abased%20control.%20We%20aim%20to%20quickly%20and%20accurately%20estimate%20the%20parameters%20of%20an%0Aunknown%20object%20using%20only%20the%20robot%27s%20proprioception%20through%20end%20to%20end%0Alearning%2C%20which%20is%20applicable%20for%20real-time%20system.%20To%20effectively%20capture%0Afeatures%20in%20robot%20proprioception%20affected%20by%20object%20dynamics%20and%20address%20the%0Achallenge%20of%20obtaining%20ground%20truth%20inertial%20parameters%20in%20the%20real%20world%2C%20we%0Adeveloped%20a%20high%20fidelity%20simulation%20that%20uses%20more%20accurate%20robot%20dynamics%0Athrough%20real-to-sim%20adaptation.%20Since%20our%20adaptation%20focuses%20solely%20on%20the%0Arobot%2C%20task-relevant%20data%20%28e.g.%2C%20holding%20an%20object%29%20is%20not%20required%20from%20the%0Areal%20world%2C%20simplifying%20the%20data%20collection%20process.%20Moreover%2C%20we%20address%20both%0Aparametric%20and%20non-parametric%20modeling%20errors%20independently%20using%20Robot%20System%0AIdentification%20and%20Gaussian%20Processes.%20We%20validate%20our%20estimator%20to%20assess%20how%0Aquickly%20and%20accurately%20it%20can%20estimate%20physically%20feasible%20parameters%20of%20an%0Amanipulated%20object%20given%20a%20specific%20trajectory%20obtained%20from%20a%20wheeled%20humanoid%0Arobot.%20Our%20estimator%20achieves%20faster%20estimation%20speeds%20%28around%200.1%20seconds%29%0Awhile%20maintaining%20accuracy%20comparable%20to%20other%20methods.%20Additionally%2C%20our%0Aestimator%20further%20highlight%20its%20benefits%20in%20improving%20the%20performance%20of%20model%0Abased%20control%20by%20compensating%20object%27s%20dynamics%20and%20reinitializing%20new%0Aequilibrium%20point%20of%20wheeled%20humanoid%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09810v3&entry.124074799=Read"},
{"title": "Open-Ended 3D Point Cloud Instance Segmentation", "author": "Phuc D. A. Nguyen and Minh Luu and Anh Tran and Cuong Pham and Khoi Nguyen", "abstract": "  Open-Vocab 3D Instance Segmentation methods (OV-3DIS) have recently\ndemonstrated their ability to generalize to unseen objects. However, these\nmethods still depend on predefined class names during testing, restricting the\nautonomy of agents. To mitigate this constraint, we propose a novel problem\ntermed Open-Ended 3D Instance Segmentation (OE-3DIS), which eliminates the\nnecessity for predefined class names during testing. Moreover, we contribute a\ncomprehensive set of strong baselines, derived from OV-3DIS approaches and\nleveraging 2D Multimodal Large Language Models. To assess the performance of\nour OE-3DIS system, we introduce a novel Open-Ended score, evaluating both the\nsemantic and geometric quality of predicted masks and their associated class\nnames, alongside the standard AP score. Our approach demonstrates significant\nperformance improvements over the baselines on the ScanNet200 and ScanNet++\ndatasets. Remarkably, our method surpasses the performance of Open3DIS, the\ncurrent state-of-the-art method in OV-3DIS, even in the absence of ground-truth\nobject class names.\n", "link": "http://arxiv.org/abs/2408.11747v1", "date": "2024-08-21", "relevancy": 2.3207, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5831}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Ended%203D%20Point%20Cloud%20Instance%20Segmentation&body=Title%3A%20Open-Ended%203D%20Point%20Cloud%20Instance%20Segmentation%0AAuthor%3A%20Phuc%20D.%20A.%20Nguyen%20and%20Minh%20Luu%20and%20Anh%20Tran%20and%20Cuong%20Pham%20and%20Khoi%20Nguyen%0AAbstract%3A%20%20%20Open-Vocab%203D%20Instance%20Segmentation%20methods%20%28OV-3DIS%29%20have%20recently%0Ademonstrated%20their%20ability%20to%20generalize%20to%20unseen%20objects.%20However%2C%20these%0Amethods%20still%20depend%20on%20predefined%20class%20names%20during%20testing%2C%20restricting%20the%0Aautonomy%20of%20agents.%20To%20mitigate%20this%20constraint%2C%20we%20propose%20a%20novel%20problem%0Atermed%20Open-Ended%203D%20Instance%20Segmentation%20%28OE-3DIS%29%2C%20which%20eliminates%20the%0Anecessity%20for%20predefined%20class%20names%20during%20testing.%20Moreover%2C%20we%20contribute%20a%0Acomprehensive%20set%20of%20strong%20baselines%2C%20derived%20from%20OV-3DIS%20approaches%20and%0Aleveraging%202D%20Multimodal%20Large%20Language%20Models.%20To%20assess%20the%20performance%20of%0Aour%20OE-3DIS%20system%2C%20we%20introduce%20a%20novel%20Open-Ended%20score%2C%20evaluating%20both%20the%0Asemantic%20and%20geometric%20quality%20of%20predicted%20masks%20and%20their%20associated%20class%0Anames%2C%20alongside%20the%20standard%20AP%20score.%20Our%20approach%20demonstrates%20significant%0Aperformance%20improvements%20over%20the%20baselines%20on%20the%20ScanNet200%20and%20ScanNet%2B%2B%0Adatasets.%20Remarkably%2C%20our%20method%20surpasses%20the%20performance%20of%20Open3DIS%2C%20the%0Acurrent%20state-of-the-art%20method%20in%20OV-3DIS%2C%20even%20in%20the%20absence%20of%20ground-truth%0Aobject%20class%20names.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Ended%25203D%2520Point%2520Cloud%2520Instance%2520Segmentation%26entry.906535625%3DPhuc%2520D.%2520A.%2520Nguyen%2520and%2520Minh%2520Luu%2520and%2520Anh%2520Tran%2520and%2520Cuong%2520Pham%2520and%2520Khoi%2520Nguyen%26entry.1292438233%3D%2520%2520Open-Vocab%25203D%2520Instance%2520Segmentation%2520methods%2520%2528OV-3DIS%2529%2520have%2520recently%250Ademonstrated%2520their%2520ability%2520to%2520generalize%2520to%2520unseen%2520objects.%2520However%252C%2520these%250Amethods%2520still%2520depend%2520on%2520predefined%2520class%2520names%2520during%2520testing%252C%2520restricting%2520the%250Aautonomy%2520of%2520agents.%2520To%2520mitigate%2520this%2520constraint%252C%2520we%2520propose%2520a%2520novel%2520problem%250Atermed%2520Open-Ended%25203D%2520Instance%2520Segmentation%2520%2528OE-3DIS%2529%252C%2520which%2520eliminates%2520the%250Anecessity%2520for%2520predefined%2520class%2520names%2520during%2520testing.%2520Moreover%252C%2520we%2520contribute%2520a%250Acomprehensive%2520set%2520of%2520strong%2520baselines%252C%2520derived%2520from%2520OV-3DIS%2520approaches%2520and%250Aleveraging%25202D%2520Multimodal%2520Large%2520Language%2520Models.%2520To%2520assess%2520the%2520performance%2520of%250Aour%2520OE-3DIS%2520system%252C%2520we%2520introduce%2520a%2520novel%2520Open-Ended%2520score%252C%2520evaluating%2520both%2520the%250Asemantic%2520and%2520geometric%2520quality%2520of%2520predicted%2520masks%2520and%2520their%2520associated%2520class%250Anames%252C%2520alongside%2520the%2520standard%2520AP%2520score.%2520Our%2520approach%2520demonstrates%2520significant%250Aperformance%2520improvements%2520over%2520the%2520baselines%2520on%2520the%2520ScanNet200%2520and%2520ScanNet%252B%252B%250Adatasets.%2520Remarkably%252C%2520our%2520method%2520surpasses%2520the%2520performance%2520of%2520Open3DIS%252C%2520the%250Acurrent%2520state-of-the-art%2520method%2520in%2520OV-3DIS%252C%2520even%2520in%2520the%2520absence%2520of%2520ground-truth%250Aobject%2520class%2520names.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Ended%203D%20Point%20Cloud%20Instance%20Segmentation&entry.906535625=Phuc%20D.%20A.%20Nguyen%20and%20Minh%20Luu%20and%20Anh%20Tran%20and%20Cuong%20Pham%20and%20Khoi%20Nguyen&entry.1292438233=%20%20Open-Vocab%203D%20Instance%20Segmentation%20methods%20%28OV-3DIS%29%20have%20recently%0Ademonstrated%20their%20ability%20to%20generalize%20to%20unseen%20objects.%20However%2C%20these%0Amethods%20still%20depend%20on%20predefined%20class%20names%20during%20testing%2C%20restricting%20the%0Aautonomy%20of%20agents.%20To%20mitigate%20this%20constraint%2C%20we%20propose%20a%20novel%20problem%0Atermed%20Open-Ended%203D%20Instance%20Segmentation%20%28OE-3DIS%29%2C%20which%20eliminates%20the%0Anecessity%20for%20predefined%20class%20names%20during%20testing.%20Moreover%2C%20we%20contribute%20a%0Acomprehensive%20set%20of%20strong%20baselines%2C%20derived%20from%20OV-3DIS%20approaches%20and%0Aleveraging%202D%20Multimodal%20Large%20Language%20Models.%20To%20assess%20the%20performance%20of%0Aour%20OE-3DIS%20system%2C%20we%20introduce%20a%20novel%20Open-Ended%20score%2C%20evaluating%20both%20the%0Asemantic%20and%20geometric%20quality%20of%20predicted%20masks%20and%20their%20associated%20class%0Anames%2C%20alongside%20the%20standard%20AP%20score.%20Our%20approach%20demonstrates%20significant%0Aperformance%20improvements%20over%20the%20baselines%20on%20the%20ScanNet200%20and%20ScanNet%2B%2B%0Adatasets.%20Remarkably%2C%20our%20method%20surpasses%20the%20performance%20of%20Open3DIS%2C%20the%0Acurrent%20state-of-the-art%20method%20in%20OV-3DIS%2C%20even%20in%20the%20absence%20of%20ground-truth%0Aobject%20class%20names.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11747v1&entry.124074799=Read"},
{"title": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a\n  Multi-Agent Framework", "author": "Zhifei Xie and Daniel Tang and Dingwei Tan and Jacques Klein and Tegawend F. Bissyand and Saad Ezzini", "abstract": "  Current video generation models excel at creating short, realistic clips, but\nstruggle with longer, multi-scene videos. We introduce \\texttt{DreamFactory},\nan LLM-based framework that tackles this challenge. \\texttt{DreamFactory}\nleverages multi-agent collaboration principles and a Key Frames Iteration\nDesign Method to ensure consistency and style across long videos. It utilizes\nChain of Thought (COT) to address uncertainties inherent in large language\nmodels. \\texttt{DreamFactory} generates long, stylistically coherent, and\ncomplex videos. Evaluating these long-form videos presents a challenge. We\npropose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene\nStyle Consistency Score. To further research in this area, we contribute the\nMulti-Scene Videos Dataset containing over 150 human-rated videos.\n", "link": "http://arxiv.org/abs/2408.11788v1", "date": "2024-08-21", "relevancy": 2.2786, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5784}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5648}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamFactory%3A%20Pioneering%20Multi-Scene%20Long%20Video%20Generation%20with%20a%0A%20%20Multi-Agent%20Framework&body=Title%3A%20DreamFactory%3A%20Pioneering%20Multi-Scene%20Long%20Video%20Generation%20with%20a%0A%20%20Multi-Agent%20Framework%0AAuthor%3A%20Zhifei%20Xie%20and%20Daniel%20Tang%20and%20Dingwei%20Tan%20and%20Jacques%20Klein%20and%20Tegawend%20F.%20Bissyand%20and%20Saad%20Ezzini%0AAbstract%3A%20%20%20Current%20video%20generation%20models%20excel%20at%20creating%20short%2C%20realistic%20clips%2C%20but%0Astruggle%20with%20longer%2C%20multi-scene%20videos.%20We%20introduce%20%5Ctexttt%7BDreamFactory%7D%2C%0Aan%20LLM-based%20framework%20that%20tackles%20this%20challenge.%20%5Ctexttt%7BDreamFactory%7D%0Aleverages%20multi-agent%20collaboration%20principles%20and%20a%20Key%20Frames%20Iteration%0ADesign%20Method%20to%20ensure%20consistency%20and%20style%20across%20long%20videos.%20It%20utilizes%0AChain%20of%20Thought%20%28COT%29%20to%20address%20uncertainties%20inherent%20in%20large%20language%0Amodels.%20%5Ctexttt%7BDreamFactory%7D%20generates%20long%2C%20stylistically%20coherent%2C%20and%0Acomplex%20videos.%20Evaluating%20these%20long-form%20videos%20presents%20a%20challenge.%20We%0Apropose%20novel%20metrics%20such%20as%20Cross-Scene%20Face%20Distance%20Score%20and%20Cross-Scene%0AStyle%20Consistency%20Score.%20To%20further%20research%20in%20this%20area%2C%20we%20contribute%20the%0AMulti-Scene%20Videos%20Dataset%20containing%20over%20150%20human-rated%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamFactory%253A%2520Pioneering%2520Multi-Scene%2520Long%2520Video%2520Generation%2520with%2520a%250A%2520%2520Multi-Agent%2520Framework%26entry.906535625%3DZhifei%2520Xie%2520and%2520Daniel%2520Tang%2520and%2520Dingwei%2520Tan%2520and%2520Jacques%2520Klein%2520and%2520Tegawend%2520F.%2520Bissyand%2520and%2520Saad%2520Ezzini%26entry.1292438233%3D%2520%2520Current%2520video%2520generation%2520models%2520excel%2520at%2520creating%2520short%252C%2520realistic%2520clips%252C%2520but%250Astruggle%2520with%2520longer%252C%2520multi-scene%2520videos.%2520We%2520introduce%2520%255Ctexttt%257BDreamFactory%257D%252C%250Aan%2520LLM-based%2520framework%2520that%2520tackles%2520this%2520challenge.%2520%255Ctexttt%257BDreamFactory%257D%250Aleverages%2520multi-agent%2520collaboration%2520principles%2520and%2520a%2520Key%2520Frames%2520Iteration%250ADesign%2520Method%2520to%2520ensure%2520consistency%2520and%2520style%2520across%2520long%2520videos.%2520It%2520utilizes%250AChain%2520of%2520Thought%2520%2528COT%2529%2520to%2520address%2520uncertainties%2520inherent%2520in%2520large%2520language%250Amodels.%2520%255Ctexttt%257BDreamFactory%257D%2520generates%2520long%252C%2520stylistically%2520coherent%252C%2520and%250Acomplex%2520videos.%2520Evaluating%2520these%2520long-form%2520videos%2520presents%2520a%2520challenge.%2520We%250Apropose%2520novel%2520metrics%2520such%2520as%2520Cross-Scene%2520Face%2520Distance%2520Score%2520and%2520Cross-Scene%250AStyle%2520Consistency%2520Score.%2520To%2520further%2520research%2520in%2520this%2520area%252C%2520we%2520contribute%2520the%250AMulti-Scene%2520Videos%2520Dataset%2520containing%2520over%2520150%2520human-rated%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamFactory%3A%20Pioneering%20Multi-Scene%20Long%20Video%20Generation%20with%20a%0A%20%20Multi-Agent%20Framework&entry.906535625=Zhifei%20Xie%20and%20Daniel%20Tang%20and%20Dingwei%20Tan%20and%20Jacques%20Klein%20and%20Tegawend%20F.%20Bissyand%20and%20Saad%20Ezzini&entry.1292438233=%20%20Current%20video%20generation%20models%20excel%20at%20creating%20short%2C%20realistic%20clips%2C%20but%0Astruggle%20with%20longer%2C%20multi-scene%20videos.%20We%20introduce%20%5Ctexttt%7BDreamFactory%7D%2C%0Aan%20LLM-based%20framework%20that%20tackles%20this%20challenge.%20%5Ctexttt%7BDreamFactory%7D%0Aleverages%20multi-agent%20collaboration%20principles%20and%20a%20Key%20Frames%20Iteration%0ADesign%20Method%20to%20ensure%20consistency%20and%20style%20across%20long%20videos.%20It%20utilizes%0AChain%20of%20Thought%20%28COT%29%20to%20address%20uncertainties%20inherent%20in%20large%20language%0Amodels.%20%5Ctexttt%7BDreamFactory%7D%20generates%20long%2C%20stylistically%20coherent%2C%20and%0Acomplex%20videos.%20Evaluating%20these%20long-form%20videos%20presents%20a%20challenge.%20We%0Apropose%20novel%20metrics%20such%20as%20Cross-Scene%20Face%20Distance%20Score%20and%20Cross-Scene%0AStyle%20Consistency%20Score.%20To%20further%20research%20in%20this%20area%2C%20we%20contribute%20the%0AMulti-Scene%20Videos%20Dataset%20containing%20over%20150%20human-rated%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11788v1&entry.124074799=Read"},
{"title": "ACE: A Cross-Platform Visual-Exoskeletons System for Low-Cost Dexterous\n  Teleoperation", "author": "Shiqi Yang and Minghuan Liu and Yuzhe Qin and Runyu Ding and Jialong Li and Xuxin Cheng and Ruihan Yang and Sha Yi and Xiaolong Wang", "abstract": "  Learning from demonstrations has shown to be an effective approach to robotic\nmanipulation, especially with the recently collected large-scale robot data\nwith teleoperation systems. Building an efficient teleoperation system across\ndiverse robot platforms has become more crucial than ever. However, there is a\nnotable lack of cost-effective and user-friendly teleoperation systems for\ndifferent end-effectors, e.g., anthropomorphic robot hands and grippers, that\ncan operate across multiple platforms. To address this issue, we develop ACE, a\ncross-platform visual-exoskeleton system for low-cost dexterous teleoperation.\nOur system utilizes a hand-facing camera to capture 3D hand poses and an\nexoskeleton mounted on a portable base, enabling accurate real-time capture of\nboth finger and wrist poses. Compared to previous systems, which often require\nhardware customization according to different robots, our single system can\ngeneralize to humanoid hands, arm-hands, arm-gripper, and quadruped-gripper\nsystems with high-precision teleoperation. This enables imitation learning for\ncomplex manipulation tasks on diverse platforms.\n", "link": "http://arxiv.org/abs/2408.11805v1", "date": "2024-08-21", "relevancy": 2.2459, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6049}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5635}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACE%3A%20A%20Cross-Platform%20Visual-Exoskeletons%20System%20for%20Low-Cost%20Dexterous%0A%20%20Teleoperation&body=Title%3A%20ACE%3A%20A%20Cross-Platform%20Visual-Exoskeletons%20System%20for%20Low-Cost%20Dexterous%0A%20%20Teleoperation%0AAuthor%3A%20Shiqi%20Yang%20and%20Minghuan%20Liu%20and%20Yuzhe%20Qin%20and%20Runyu%20Ding%20and%20Jialong%20Li%20and%20Xuxin%20Cheng%20and%20Ruihan%20Yang%20and%20Sha%20Yi%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20Learning%20from%20demonstrations%20has%20shown%20to%20be%20an%20effective%20approach%20to%20robotic%0Amanipulation%2C%20especially%20with%20the%20recently%20collected%20large-scale%20robot%20data%0Awith%20teleoperation%20systems.%20Building%20an%20efficient%20teleoperation%20system%20across%0Adiverse%20robot%20platforms%20has%20become%20more%20crucial%20than%20ever.%20However%2C%20there%20is%20a%0Anotable%20lack%20of%20cost-effective%20and%20user-friendly%20teleoperation%20systems%20for%0Adifferent%20end-effectors%2C%20e.g.%2C%20anthropomorphic%20robot%20hands%20and%20grippers%2C%20that%0Acan%20operate%20across%20multiple%20platforms.%20To%20address%20this%20issue%2C%20we%20develop%20ACE%2C%20a%0Across-platform%20visual-exoskeleton%20system%20for%20low-cost%20dexterous%20teleoperation.%0AOur%20system%20utilizes%20a%20hand-facing%20camera%20to%20capture%203D%20hand%20poses%20and%20an%0Aexoskeleton%20mounted%20on%20a%20portable%20base%2C%20enabling%20accurate%20real-time%20capture%20of%0Aboth%20finger%20and%20wrist%20poses.%20Compared%20to%20previous%20systems%2C%20which%20often%20require%0Ahardware%20customization%20according%20to%20different%20robots%2C%20our%20single%20system%20can%0Ageneralize%20to%20humanoid%20hands%2C%20arm-hands%2C%20arm-gripper%2C%20and%20quadruped-gripper%0Asystems%20with%20high-precision%20teleoperation.%20This%20enables%20imitation%20learning%20for%0Acomplex%20manipulation%20tasks%20on%20diverse%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACE%253A%2520A%2520Cross-Platform%2520Visual-Exoskeletons%2520System%2520for%2520Low-Cost%2520Dexterous%250A%2520%2520Teleoperation%26entry.906535625%3DShiqi%2520Yang%2520and%2520Minghuan%2520Liu%2520and%2520Yuzhe%2520Qin%2520and%2520Runyu%2520Ding%2520and%2520Jialong%2520Li%2520and%2520Xuxin%2520Cheng%2520and%2520Ruihan%2520Yang%2520and%2520Sha%2520Yi%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3D%2520%2520Learning%2520from%2520demonstrations%2520has%2520shown%2520to%2520be%2520an%2520effective%2520approach%2520to%2520robotic%250Amanipulation%252C%2520especially%2520with%2520the%2520recently%2520collected%2520large-scale%2520robot%2520data%250Awith%2520teleoperation%2520systems.%2520Building%2520an%2520efficient%2520teleoperation%2520system%2520across%250Adiverse%2520robot%2520platforms%2520has%2520become%2520more%2520crucial%2520than%2520ever.%2520However%252C%2520there%2520is%2520a%250Anotable%2520lack%2520of%2520cost-effective%2520and%2520user-friendly%2520teleoperation%2520systems%2520for%250Adifferent%2520end-effectors%252C%2520e.g.%252C%2520anthropomorphic%2520robot%2520hands%2520and%2520grippers%252C%2520that%250Acan%2520operate%2520across%2520multiple%2520platforms.%2520To%2520address%2520this%2520issue%252C%2520we%2520develop%2520ACE%252C%2520a%250Across-platform%2520visual-exoskeleton%2520system%2520for%2520low-cost%2520dexterous%2520teleoperation.%250AOur%2520system%2520utilizes%2520a%2520hand-facing%2520camera%2520to%2520capture%25203D%2520hand%2520poses%2520and%2520an%250Aexoskeleton%2520mounted%2520on%2520a%2520portable%2520base%252C%2520enabling%2520accurate%2520real-time%2520capture%2520of%250Aboth%2520finger%2520and%2520wrist%2520poses.%2520Compared%2520to%2520previous%2520systems%252C%2520which%2520often%2520require%250Ahardware%2520customization%2520according%2520to%2520different%2520robots%252C%2520our%2520single%2520system%2520can%250Ageneralize%2520to%2520humanoid%2520hands%252C%2520arm-hands%252C%2520arm-gripper%252C%2520and%2520quadruped-gripper%250Asystems%2520with%2520high-precision%2520teleoperation.%2520This%2520enables%2520imitation%2520learning%2520for%250Acomplex%2520manipulation%2520tasks%2520on%2520diverse%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACE%3A%20A%20Cross-Platform%20Visual-Exoskeletons%20System%20for%20Low-Cost%20Dexterous%0A%20%20Teleoperation&entry.906535625=Shiqi%20Yang%20and%20Minghuan%20Liu%20and%20Yuzhe%20Qin%20and%20Runyu%20Ding%20and%20Jialong%20Li%20and%20Xuxin%20Cheng%20and%20Ruihan%20Yang%20and%20Sha%20Yi%20and%20Xiaolong%20Wang&entry.1292438233=%20%20Learning%20from%20demonstrations%20has%20shown%20to%20be%20an%20effective%20approach%20to%20robotic%0Amanipulation%2C%20especially%20with%20the%20recently%20collected%20large-scale%20robot%20data%0Awith%20teleoperation%20systems.%20Building%20an%20efficient%20teleoperation%20system%20across%0Adiverse%20robot%20platforms%20has%20become%20more%20crucial%20than%20ever.%20However%2C%20there%20is%20a%0Anotable%20lack%20of%20cost-effective%20and%20user-friendly%20teleoperation%20systems%20for%0Adifferent%20end-effectors%2C%20e.g.%2C%20anthropomorphic%20robot%20hands%20and%20grippers%2C%20that%0Acan%20operate%20across%20multiple%20platforms.%20To%20address%20this%20issue%2C%20we%20develop%20ACE%2C%20a%0Across-platform%20visual-exoskeleton%20system%20for%20low-cost%20dexterous%20teleoperation.%0AOur%20system%20utilizes%20a%20hand-facing%20camera%20to%20capture%203D%20hand%20poses%20and%20an%0Aexoskeleton%20mounted%20on%20a%20portable%20base%2C%20enabling%20accurate%20real-time%20capture%20of%0Aboth%20finger%20and%20wrist%20poses.%20Compared%20to%20previous%20systems%2C%20which%20often%20require%0Ahardware%20customization%20according%20to%20different%20robots%2C%20our%20single%20system%20can%0Ageneralize%20to%20humanoid%20hands%2C%20arm-hands%2C%20arm-gripper%2C%20and%20quadruped-gripper%0Asystems%20with%20high-precision%20teleoperation.%20This%20enables%20imitation%20learning%20for%0Acomplex%20manipulation%20tasks%20on%20diverse%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11805v1&entry.124074799=Read"},
{"title": "Toward Control of Wheeled Humanoid Robots with Unknown Payloads:\n  Equilibrium Point Estimation via Real-to-Sim Adaptation", "author": "Donghoon Baek and Youngwoo Sim and Amartya Purushottam and Saurabh Gupta and Joao Ramos", "abstract": "  Model-based controllers using a linearized model around the system's\nequilibrium point is a common approach in the control of a wheeled humanoid due\nto their less computational load and ease of stability analysis. However,\ncontrolling a wheeled humanoid robot while it lifts an unknown object presents\nsignificant challenges, primarily due to the lack of knowledge in object\ndynamics. This paper presents a framework designed for predicting the new\nequilibrium point explicitly to control a wheeled-legged robot with unknown\ndynamics. We estimated the total mass and center of mass of the system from its\nresponse to initially unknown dynamics, then calculated the new equilibrium\npoint accordingly. To avoid using additional sensors (e.g., force torque\nsensor) and reduce the effort of obtaining expensive real data, a data-driven\napproach is utilized with a novel real-to-sim adaptation. A more accurate\nnonlinear dynamics model, offering a closer representation of real-world\nphysics, is injected into a rigid-body simulation for real-to-sim adaptation.\nThe nonlinear dynamics model parameters were optimized using Particle Swarm\nOptimization. The efficacy of this framework was validated on a physical\nwheeled inverted pendulum, a simplified model of a wheeled-legged robot. The\nexperimental results indicate that employing a more precise analytical model\nwith optimized parameters significantly reduces the gap between simulation and\nreality, thus improving the efficiency of a model-based controller in\ncontrolling a wheeled robot with unknown dynamics\n", "link": "http://arxiv.org/abs/2403.10948v2", "date": "2024-08-21", "relevancy": 2.2265, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6148}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5743}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Control%20of%20Wheeled%20Humanoid%20Robots%20with%20Unknown%20Payloads%3A%0A%20%20Equilibrium%20Point%20Estimation%20via%20Real-to-Sim%20Adaptation&body=Title%3A%20Toward%20Control%20of%20Wheeled%20Humanoid%20Robots%20with%20Unknown%20Payloads%3A%0A%20%20Equilibrium%20Point%20Estimation%20via%20Real-to-Sim%20Adaptation%0AAuthor%3A%20Donghoon%20Baek%20and%20Youngwoo%20Sim%20and%20Amartya%20Purushottam%20and%20Saurabh%20Gupta%20and%20Joao%20Ramos%0AAbstract%3A%20%20%20Model-based%20controllers%20using%20a%20linearized%20model%20around%20the%20system%27s%0Aequilibrium%20point%20is%20a%20common%20approach%20in%20the%20control%20of%20a%20wheeled%20humanoid%20due%0Ato%20their%20less%20computational%20load%20and%20ease%20of%20stability%20analysis.%20However%2C%0Acontrolling%20a%20wheeled%20humanoid%20robot%20while%20it%20lifts%20an%20unknown%20object%20presents%0Asignificant%20challenges%2C%20primarily%20due%20to%20the%20lack%20of%20knowledge%20in%20object%0Adynamics.%20This%20paper%20presents%20a%20framework%20designed%20for%20predicting%20the%20new%0Aequilibrium%20point%20explicitly%20to%20control%20a%20wheeled-legged%20robot%20with%20unknown%0Adynamics.%20We%20estimated%20the%20total%20mass%20and%20center%20of%20mass%20of%20the%20system%20from%20its%0Aresponse%20to%20initially%20unknown%20dynamics%2C%20then%20calculated%20the%20new%20equilibrium%0Apoint%20accordingly.%20To%20avoid%20using%20additional%20sensors%20%28e.g.%2C%20force%20torque%0Asensor%29%20and%20reduce%20the%20effort%20of%20obtaining%20expensive%20real%20data%2C%20a%20data-driven%0Aapproach%20is%20utilized%20with%20a%20novel%20real-to-sim%20adaptation.%20A%20more%20accurate%0Anonlinear%20dynamics%20model%2C%20offering%20a%20closer%20representation%20of%20real-world%0Aphysics%2C%20is%20injected%20into%20a%20rigid-body%20simulation%20for%20real-to-sim%20adaptation.%0AThe%20nonlinear%20dynamics%20model%20parameters%20were%20optimized%20using%20Particle%20Swarm%0AOptimization.%20The%20efficacy%20of%20this%20framework%20was%20validated%20on%20a%20physical%0Awheeled%20inverted%20pendulum%2C%20a%20simplified%20model%20of%20a%20wheeled-legged%20robot.%20The%0Aexperimental%20results%20indicate%20that%20employing%20a%20more%20precise%20analytical%20model%0Awith%20optimized%20parameters%20significantly%20reduces%20the%20gap%20between%20simulation%20and%0Areality%2C%20thus%20improving%20the%20efficiency%20of%20a%20model-based%20controller%20in%0Acontrolling%20a%20wheeled%20robot%20with%20unknown%20dynamics%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10948v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Control%2520of%2520Wheeled%2520Humanoid%2520Robots%2520with%2520Unknown%2520Payloads%253A%250A%2520%2520Equilibrium%2520Point%2520Estimation%2520via%2520Real-to-Sim%2520Adaptation%26entry.906535625%3DDonghoon%2520Baek%2520and%2520Youngwoo%2520Sim%2520and%2520Amartya%2520Purushottam%2520and%2520Saurabh%2520Gupta%2520and%2520Joao%2520Ramos%26entry.1292438233%3D%2520%2520Model-based%2520controllers%2520using%2520a%2520linearized%2520model%2520around%2520the%2520system%2527s%250Aequilibrium%2520point%2520is%2520a%2520common%2520approach%2520in%2520the%2520control%2520of%2520a%2520wheeled%2520humanoid%2520due%250Ato%2520their%2520less%2520computational%2520load%2520and%2520ease%2520of%2520stability%2520analysis.%2520However%252C%250Acontrolling%2520a%2520wheeled%2520humanoid%2520robot%2520while%2520it%2520lifts%2520an%2520unknown%2520object%2520presents%250Asignificant%2520challenges%252C%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520knowledge%2520in%2520object%250Adynamics.%2520This%2520paper%2520presents%2520a%2520framework%2520designed%2520for%2520predicting%2520the%2520new%250Aequilibrium%2520point%2520explicitly%2520to%2520control%2520a%2520wheeled-legged%2520robot%2520with%2520unknown%250Adynamics.%2520We%2520estimated%2520the%2520total%2520mass%2520and%2520center%2520of%2520mass%2520of%2520the%2520system%2520from%2520its%250Aresponse%2520to%2520initially%2520unknown%2520dynamics%252C%2520then%2520calculated%2520the%2520new%2520equilibrium%250Apoint%2520accordingly.%2520To%2520avoid%2520using%2520additional%2520sensors%2520%2528e.g.%252C%2520force%2520torque%250Asensor%2529%2520and%2520reduce%2520the%2520effort%2520of%2520obtaining%2520expensive%2520real%2520data%252C%2520a%2520data-driven%250Aapproach%2520is%2520utilized%2520with%2520a%2520novel%2520real-to-sim%2520adaptation.%2520A%2520more%2520accurate%250Anonlinear%2520dynamics%2520model%252C%2520offering%2520a%2520closer%2520representation%2520of%2520real-world%250Aphysics%252C%2520is%2520injected%2520into%2520a%2520rigid-body%2520simulation%2520for%2520real-to-sim%2520adaptation.%250AThe%2520nonlinear%2520dynamics%2520model%2520parameters%2520were%2520optimized%2520using%2520Particle%2520Swarm%250AOptimization.%2520The%2520efficacy%2520of%2520this%2520framework%2520was%2520validated%2520on%2520a%2520physical%250Awheeled%2520inverted%2520pendulum%252C%2520a%2520simplified%2520model%2520of%2520a%2520wheeled-legged%2520robot.%2520The%250Aexperimental%2520results%2520indicate%2520that%2520employing%2520a%2520more%2520precise%2520analytical%2520model%250Awith%2520optimized%2520parameters%2520significantly%2520reduces%2520the%2520gap%2520between%2520simulation%2520and%250Areality%252C%2520thus%2520improving%2520the%2520efficiency%2520of%2520a%2520model-based%2520controller%2520in%250Acontrolling%2520a%2520wheeled%2520robot%2520with%2520unknown%2520dynamics%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10948v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Control%20of%20Wheeled%20Humanoid%20Robots%20with%20Unknown%20Payloads%3A%0A%20%20Equilibrium%20Point%20Estimation%20via%20Real-to-Sim%20Adaptation&entry.906535625=Donghoon%20Baek%20and%20Youngwoo%20Sim%20and%20Amartya%20Purushottam%20and%20Saurabh%20Gupta%20and%20Joao%20Ramos&entry.1292438233=%20%20Model-based%20controllers%20using%20a%20linearized%20model%20around%20the%20system%27s%0Aequilibrium%20point%20is%20a%20common%20approach%20in%20the%20control%20of%20a%20wheeled%20humanoid%20due%0Ato%20their%20less%20computational%20load%20and%20ease%20of%20stability%20analysis.%20However%2C%0Acontrolling%20a%20wheeled%20humanoid%20robot%20while%20it%20lifts%20an%20unknown%20object%20presents%0Asignificant%20challenges%2C%20primarily%20due%20to%20the%20lack%20of%20knowledge%20in%20object%0Adynamics.%20This%20paper%20presents%20a%20framework%20designed%20for%20predicting%20the%20new%0Aequilibrium%20point%20explicitly%20to%20control%20a%20wheeled-legged%20robot%20with%20unknown%0Adynamics.%20We%20estimated%20the%20total%20mass%20and%20center%20of%20mass%20of%20the%20system%20from%20its%0Aresponse%20to%20initially%20unknown%20dynamics%2C%20then%20calculated%20the%20new%20equilibrium%0Apoint%20accordingly.%20To%20avoid%20using%20additional%20sensors%20%28e.g.%2C%20force%20torque%0Asensor%29%20and%20reduce%20the%20effort%20of%20obtaining%20expensive%20real%20data%2C%20a%20data-driven%0Aapproach%20is%20utilized%20with%20a%20novel%20real-to-sim%20adaptation.%20A%20more%20accurate%0Anonlinear%20dynamics%20model%2C%20offering%20a%20closer%20representation%20of%20real-world%0Aphysics%2C%20is%20injected%20into%20a%20rigid-body%20simulation%20for%20real-to-sim%20adaptation.%0AThe%20nonlinear%20dynamics%20model%20parameters%20were%20optimized%20using%20Particle%20Swarm%0AOptimization.%20The%20efficacy%20of%20this%20framework%20was%20validated%20on%20a%20physical%0Awheeled%20inverted%20pendulum%2C%20a%20simplified%20model%20of%20a%20wheeled-legged%20robot.%20The%0Aexperimental%20results%20indicate%20that%20employing%20a%20more%20precise%20analytical%20model%0Awith%20optimized%20parameters%20significantly%20reduces%20the%20gap%20between%20simulation%20and%0Areality%2C%20thus%20improving%20the%20efficiency%20of%20a%20model-based%20controller%20in%0Acontrolling%20a%20wheeled%20robot%20with%20unknown%20dynamics%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10948v2&entry.124074799=Read"},
{"title": "MCDubber: Multimodal Context-Aware Expressive Video Dubbing", "author": "Yuan Zhao and Zhenqi Jia and Rui Liu and De Hu and Feilong Bao and Guanglai Gao", "abstract": "  Automatic Video Dubbing (AVD) aims to take the given script and generate\nspeech that aligns with lip motion and prosody expressiveness. Current AVD\nmodels mainly utilize visual information of the current sentence to enhance the\nprosody of synthesized speech. However, it is crucial to consider whether the\nprosody of the generated dubbing aligns with the multimodal context, as the\ndubbing will be combined with the original context in the final video. This\naspect has been overlooked in previous studies. To address this issue, we\npropose a Multimodal Context-aware video Dubbing model, termed\n\\textbf{MCDubber}, to convert the modeling object from a single sentence to a\nlonger sequence with context information to ensure the consistency of the\nglobal context prosody. MCDubber comprises three main components: (1) A context\nduration aligner aims to learn the context-aware alignment between the text and\nlip frames; (2) A context prosody predictor seeks to read the global context\nvisual sequence and predict the context-aware global energy and pitch; (3) A\ncontext acoustic decoder ultimately predicts the global context mel-spectrogram\nwith the assistance of adjacent ground-truth mel-spectrograms of the target\nsentence. Through this process, MCDubber fully considers the influence of\nmultimodal context on the prosody expressiveness of the current sentence when\ndubbing. The extracted mel-spectrogram belonging to the target sentence from\nthe output context mel-spectrograms is the final required dubbing audio.\nExtensive experiments on the Chem benchmark dataset demonstrate that our\nMCDubber significantly improves dubbing expressiveness compared to all advanced\nbaselines. The code and demos are available at\nhttps://github.com/XiaoYuanJun-zy/MCDubber.\n", "link": "http://arxiv.org/abs/2408.11593v1", "date": "2024-08-21", "relevancy": 2.2186, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5717}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCDubber%3A%20Multimodal%20Context-Aware%20Expressive%20Video%20Dubbing&body=Title%3A%20MCDubber%3A%20Multimodal%20Context-Aware%20Expressive%20Video%20Dubbing%0AAuthor%3A%20Yuan%20Zhao%20and%20Zhenqi%20Jia%20and%20Rui%20Liu%20and%20De%20Hu%20and%20Feilong%20Bao%20and%20Guanglai%20Gao%0AAbstract%3A%20%20%20Automatic%20Video%20Dubbing%20%28AVD%29%20aims%20to%20take%20the%20given%20script%20and%20generate%0Aspeech%20that%20aligns%20with%20lip%20motion%20and%20prosody%20expressiveness.%20Current%20AVD%0Amodels%20mainly%20utilize%20visual%20information%20of%20the%20current%20sentence%20to%20enhance%20the%0Aprosody%20of%20synthesized%20speech.%20However%2C%20it%20is%20crucial%20to%20consider%20whether%20the%0Aprosody%20of%20the%20generated%20dubbing%20aligns%20with%20the%20multimodal%20context%2C%20as%20the%0Adubbing%20will%20be%20combined%20with%20the%20original%20context%20in%20the%20final%20video.%20This%0Aaspect%20has%20been%20overlooked%20in%20previous%20studies.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20Multimodal%20Context-aware%20video%20Dubbing%20model%2C%20termed%0A%5Ctextbf%7BMCDubber%7D%2C%20to%20convert%20the%20modeling%20object%20from%20a%20single%20sentence%20to%20a%0Alonger%20sequence%20with%20context%20information%20to%20ensure%20the%20consistency%20of%20the%0Aglobal%20context%20prosody.%20MCDubber%20comprises%20three%20main%20components%3A%20%281%29%20A%20context%0Aduration%20aligner%20aims%20to%20learn%20the%20context-aware%20alignment%20between%20the%20text%20and%0Alip%20frames%3B%20%282%29%20A%20context%20prosody%20predictor%20seeks%20to%20read%20the%20global%20context%0Avisual%20sequence%20and%20predict%20the%20context-aware%20global%20energy%20and%20pitch%3B%20%283%29%20A%0Acontext%20acoustic%20decoder%20ultimately%20predicts%20the%20global%20context%20mel-spectrogram%0Awith%20the%20assistance%20of%20adjacent%20ground-truth%20mel-spectrograms%20of%20the%20target%0Asentence.%20Through%20this%20process%2C%20MCDubber%20fully%20considers%20the%20influence%20of%0Amultimodal%20context%20on%20the%20prosody%20expressiveness%20of%20the%20current%20sentence%20when%0Adubbing.%20The%20extracted%20mel-spectrogram%20belonging%20to%20the%20target%20sentence%20from%0Athe%20output%20context%20mel-spectrograms%20is%20the%20final%20required%20dubbing%20audio.%0AExtensive%20experiments%20on%20the%20Chem%20benchmark%20dataset%20demonstrate%20that%20our%0AMCDubber%20significantly%20improves%20dubbing%20expressiveness%20compared%20to%20all%20advanced%0Abaselines.%20The%20code%20and%20demos%20are%20available%20at%0Ahttps%3A//github.com/XiaoYuanJun-zy/MCDubber.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCDubber%253A%2520Multimodal%2520Context-Aware%2520Expressive%2520Video%2520Dubbing%26entry.906535625%3DYuan%2520Zhao%2520and%2520Zhenqi%2520Jia%2520and%2520Rui%2520Liu%2520and%2520De%2520Hu%2520and%2520Feilong%2520Bao%2520and%2520Guanglai%2520Gao%26entry.1292438233%3D%2520%2520Automatic%2520Video%2520Dubbing%2520%2528AVD%2529%2520aims%2520to%2520take%2520the%2520given%2520script%2520and%2520generate%250Aspeech%2520that%2520aligns%2520with%2520lip%2520motion%2520and%2520prosody%2520expressiveness.%2520Current%2520AVD%250Amodels%2520mainly%2520utilize%2520visual%2520information%2520of%2520the%2520current%2520sentence%2520to%2520enhance%2520the%250Aprosody%2520of%2520synthesized%2520speech.%2520However%252C%2520it%2520is%2520crucial%2520to%2520consider%2520whether%2520the%250Aprosody%2520of%2520the%2520generated%2520dubbing%2520aligns%2520with%2520the%2520multimodal%2520context%252C%2520as%2520the%250Adubbing%2520will%2520be%2520combined%2520with%2520the%2520original%2520context%2520in%2520the%2520final%2520video.%2520This%250Aaspect%2520has%2520been%2520overlooked%2520in%2520previous%2520studies.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520Multimodal%2520Context-aware%2520video%2520Dubbing%2520model%252C%2520termed%250A%255Ctextbf%257BMCDubber%257D%252C%2520to%2520convert%2520the%2520modeling%2520object%2520from%2520a%2520single%2520sentence%2520to%2520a%250Alonger%2520sequence%2520with%2520context%2520information%2520to%2520ensure%2520the%2520consistency%2520of%2520the%250Aglobal%2520context%2520prosody.%2520MCDubber%2520comprises%2520three%2520main%2520components%253A%2520%25281%2529%2520A%2520context%250Aduration%2520aligner%2520aims%2520to%2520learn%2520the%2520context-aware%2520alignment%2520between%2520the%2520text%2520and%250Alip%2520frames%253B%2520%25282%2529%2520A%2520context%2520prosody%2520predictor%2520seeks%2520to%2520read%2520the%2520global%2520context%250Avisual%2520sequence%2520and%2520predict%2520the%2520context-aware%2520global%2520energy%2520and%2520pitch%253B%2520%25283%2529%2520A%250Acontext%2520acoustic%2520decoder%2520ultimately%2520predicts%2520the%2520global%2520context%2520mel-spectrogram%250Awith%2520the%2520assistance%2520of%2520adjacent%2520ground-truth%2520mel-spectrograms%2520of%2520the%2520target%250Asentence.%2520Through%2520this%2520process%252C%2520MCDubber%2520fully%2520considers%2520the%2520influence%2520of%250Amultimodal%2520context%2520on%2520the%2520prosody%2520expressiveness%2520of%2520the%2520current%2520sentence%2520when%250Adubbing.%2520The%2520extracted%2520mel-spectrogram%2520belonging%2520to%2520the%2520target%2520sentence%2520from%250Athe%2520output%2520context%2520mel-spectrograms%2520is%2520the%2520final%2520required%2520dubbing%2520audio.%250AExtensive%2520experiments%2520on%2520the%2520Chem%2520benchmark%2520dataset%2520demonstrate%2520that%2520our%250AMCDubber%2520significantly%2520improves%2520dubbing%2520expressiveness%2520compared%2520to%2520all%2520advanced%250Abaselines.%2520The%2520code%2520and%2520demos%2520are%2520available%2520at%250Ahttps%253A//github.com/XiaoYuanJun-zy/MCDubber.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCDubber%3A%20Multimodal%20Context-Aware%20Expressive%20Video%20Dubbing&entry.906535625=Yuan%20Zhao%20and%20Zhenqi%20Jia%20and%20Rui%20Liu%20and%20De%20Hu%20and%20Feilong%20Bao%20and%20Guanglai%20Gao&entry.1292438233=%20%20Automatic%20Video%20Dubbing%20%28AVD%29%20aims%20to%20take%20the%20given%20script%20and%20generate%0Aspeech%20that%20aligns%20with%20lip%20motion%20and%20prosody%20expressiveness.%20Current%20AVD%0Amodels%20mainly%20utilize%20visual%20information%20of%20the%20current%20sentence%20to%20enhance%20the%0Aprosody%20of%20synthesized%20speech.%20However%2C%20it%20is%20crucial%20to%20consider%20whether%20the%0Aprosody%20of%20the%20generated%20dubbing%20aligns%20with%20the%20multimodal%20context%2C%20as%20the%0Adubbing%20will%20be%20combined%20with%20the%20original%20context%20in%20the%20final%20video.%20This%0Aaspect%20has%20been%20overlooked%20in%20previous%20studies.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20Multimodal%20Context-aware%20video%20Dubbing%20model%2C%20termed%0A%5Ctextbf%7BMCDubber%7D%2C%20to%20convert%20the%20modeling%20object%20from%20a%20single%20sentence%20to%20a%0Alonger%20sequence%20with%20context%20information%20to%20ensure%20the%20consistency%20of%20the%0Aglobal%20context%20prosody.%20MCDubber%20comprises%20three%20main%20components%3A%20%281%29%20A%20context%0Aduration%20aligner%20aims%20to%20learn%20the%20context-aware%20alignment%20between%20the%20text%20and%0Alip%20frames%3B%20%282%29%20A%20context%20prosody%20predictor%20seeks%20to%20read%20the%20global%20context%0Avisual%20sequence%20and%20predict%20the%20context-aware%20global%20energy%20and%20pitch%3B%20%283%29%20A%0Acontext%20acoustic%20decoder%20ultimately%20predicts%20the%20global%20context%20mel-spectrogram%0Awith%20the%20assistance%20of%20adjacent%20ground-truth%20mel-spectrograms%20of%20the%20target%0Asentence.%20Through%20this%20process%2C%20MCDubber%20fully%20considers%20the%20influence%20of%0Amultimodal%20context%20on%20the%20prosody%20expressiveness%20of%20the%20current%20sentence%20when%0Adubbing.%20The%20extracted%20mel-spectrogram%20belonging%20to%20the%20target%20sentence%20from%0Athe%20output%20context%20mel-spectrograms%20is%20the%20final%20required%20dubbing%20audio.%0AExtensive%20experiments%20on%20the%20Chem%20benchmark%20dataset%20demonstrate%20that%20our%0AMCDubber%20significantly%20improves%20dubbing%20expressiveness%20compared%20to%20all%20advanced%0Abaselines.%20The%20code%20and%20demos%20are%20available%20at%0Ahttps%3A//github.com/XiaoYuanJun-zy/MCDubber.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11593v1&entry.124074799=Read"},
{"title": "DH-Bench: Probing Depth and Height Perception of Large Visual-Language\n  Models", "author": "Shehreen Azad and Yash Jain and Rishit Garg and Yogesh S Rawat and Vibhav Vineet", "abstract": "  Geometric understanding is crucial for navigating and interacting with our\nenvironment. While large Vision Language Models (VLMs) demonstrate impressive\ncapabilities, deploying them in real-world scenarios necessitates a comparable\ngeometric understanding in visual perception. In this work, we focus on the\ngeometric comprehension of these models; specifically targeting the depths and\nheights of objects within a scene. Our observations reveal that, although VLMs\nexcel in basic geometric properties perception such as shape and size, they\nencounter significant challenges in reasoning about the depth and height of\nobjects. To address this, we introduce a suite of benchmark datasets\nencompassing Synthetic 2D, Synthetic 3D, and Real-World scenarios to rigorously\nevaluate these aspects. We benchmark 17 state-of-the-art VLMs using these\ndatasets and find that they consistently struggle with both depth and height\nperception. Our key insights include detailed analyses of the shortcomings in\ndepth and height reasoning capabilities of VLMs and the inherent bias present\nin these models. This study aims to pave the way for the development of VLMs\nwith enhanced geometric understanding, crucial for real-world applications. The\ncode and datasets for our benchmarks will be available at\n\\url{https://tinyurl.com/DH-Bench1}.\n", "link": "http://arxiv.org/abs/2408.11748v1", "date": "2024-08-21", "relevancy": 2.2105, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5605}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DH-Bench%3A%20Probing%20Depth%20and%20Height%20Perception%20of%20Large%20Visual-Language%0A%20%20Models&body=Title%3A%20DH-Bench%3A%20Probing%20Depth%20and%20Height%20Perception%20of%20Large%20Visual-Language%0A%20%20Models%0AAuthor%3A%20Shehreen%20Azad%20and%20Yash%20Jain%20and%20Rishit%20Garg%20and%20Yogesh%20S%20Rawat%20and%20Vibhav%20Vineet%0AAbstract%3A%20%20%20Geometric%20understanding%20is%20crucial%20for%20navigating%20and%20interacting%20with%20our%0Aenvironment.%20While%20large%20Vision%20Language%20Models%20%28VLMs%29%20demonstrate%20impressive%0Acapabilities%2C%20deploying%20them%20in%20real-world%20scenarios%20necessitates%20a%20comparable%0Ageometric%20understanding%20in%20visual%20perception.%20In%20this%20work%2C%20we%20focus%20on%20the%0Ageometric%20comprehension%20of%20these%20models%3B%20specifically%20targeting%20the%20depths%20and%0Aheights%20of%20objects%20within%20a%20scene.%20Our%20observations%20reveal%20that%2C%20although%20VLMs%0Aexcel%20in%20basic%20geometric%20properties%20perception%20such%20as%20shape%20and%20size%2C%20they%0Aencounter%20significant%20challenges%20in%20reasoning%20about%20the%20depth%20and%20height%20of%0Aobjects.%20To%20address%20this%2C%20we%20introduce%20a%20suite%20of%20benchmark%20datasets%0Aencompassing%20Synthetic%202D%2C%20Synthetic%203D%2C%20and%20Real-World%20scenarios%20to%20rigorously%0Aevaluate%20these%20aspects.%20We%20benchmark%2017%20state-of-the-art%20VLMs%20using%20these%0Adatasets%20and%20find%20that%20they%20consistently%20struggle%20with%20both%20depth%20and%20height%0Aperception.%20Our%20key%20insights%20include%20detailed%20analyses%20of%20the%20shortcomings%20in%0Adepth%20and%20height%20reasoning%20capabilities%20of%20VLMs%20and%20the%20inherent%20bias%20present%0Ain%20these%20models.%20This%20study%20aims%20to%20pave%20the%20way%20for%20the%20development%20of%20VLMs%0Awith%20enhanced%20geometric%20understanding%2C%20crucial%20for%20real-world%20applications.%20The%0Acode%20and%20datasets%20for%20our%20benchmarks%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//tinyurl.com/DH-Bench1%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDH-Bench%253A%2520Probing%2520Depth%2520and%2520Height%2520Perception%2520of%2520Large%2520Visual-Language%250A%2520%2520Models%26entry.906535625%3DShehreen%2520Azad%2520and%2520Yash%2520Jain%2520and%2520Rishit%2520Garg%2520and%2520Yogesh%2520S%2520Rawat%2520and%2520Vibhav%2520Vineet%26entry.1292438233%3D%2520%2520Geometric%2520understanding%2520is%2520crucial%2520for%2520navigating%2520and%2520interacting%2520with%2520our%250Aenvironment.%2520While%2520large%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520impressive%250Acapabilities%252C%2520deploying%2520them%2520in%2520real-world%2520scenarios%2520necessitates%2520a%2520comparable%250Ageometric%2520understanding%2520in%2520visual%2520perception.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%250Ageometric%2520comprehension%2520of%2520these%2520models%253B%2520specifically%2520targeting%2520the%2520depths%2520and%250Aheights%2520of%2520objects%2520within%2520a%2520scene.%2520Our%2520observations%2520reveal%2520that%252C%2520although%2520VLMs%250Aexcel%2520in%2520basic%2520geometric%2520properties%2520perception%2520such%2520as%2520shape%2520and%2520size%252C%2520they%250Aencounter%2520significant%2520challenges%2520in%2520reasoning%2520about%2520the%2520depth%2520and%2520height%2520of%250Aobjects.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520suite%2520of%2520benchmark%2520datasets%250Aencompassing%2520Synthetic%25202D%252C%2520Synthetic%25203D%252C%2520and%2520Real-World%2520scenarios%2520to%2520rigorously%250Aevaluate%2520these%2520aspects.%2520We%2520benchmark%252017%2520state-of-the-art%2520VLMs%2520using%2520these%250Adatasets%2520and%2520find%2520that%2520they%2520consistently%2520struggle%2520with%2520both%2520depth%2520and%2520height%250Aperception.%2520Our%2520key%2520insights%2520include%2520detailed%2520analyses%2520of%2520the%2520shortcomings%2520in%250Adepth%2520and%2520height%2520reasoning%2520capabilities%2520of%2520VLMs%2520and%2520the%2520inherent%2520bias%2520present%250Ain%2520these%2520models.%2520This%2520study%2520aims%2520to%2520pave%2520the%2520way%2520for%2520the%2520development%2520of%2520VLMs%250Awith%2520enhanced%2520geometric%2520understanding%252C%2520crucial%2520for%2520real-world%2520applications.%2520The%250Acode%2520and%2520datasets%2520for%2520our%2520benchmarks%2520will%2520be%2520available%2520at%250A%255Curl%257Bhttps%253A//tinyurl.com/DH-Bench1%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DH-Bench%3A%20Probing%20Depth%20and%20Height%20Perception%20of%20Large%20Visual-Language%0A%20%20Models&entry.906535625=Shehreen%20Azad%20and%20Yash%20Jain%20and%20Rishit%20Garg%20and%20Yogesh%20S%20Rawat%20and%20Vibhav%20Vineet&entry.1292438233=%20%20Geometric%20understanding%20is%20crucial%20for%20navigating%20and%20interacting%20with%20our%0Aenvironment.%20While%20large%20Vision%20Language%20Models%20%28VLMs%29%20demonstrate%20impressive%0Acapabilities%2C%20deploying%20them%20in%20real-world%20scenarios%20necessitates%20a%20comparable%0Ageometric%20understanding%20in%20visual%20perception.%20In%20this%20work%2C%20we%20focus%20on%20the%0Ageometric%20comprehension%20of%20these%20models%3B%20specifically%20targeting%20the%20depths%20and%0Aheights%20of%20objects%20within%20a%20scene.%20Our%20observations%20reveal%20that%2C%20although%20VLMs%0Aexcel%20in%20basic%20geometric%20properties%20perception%20such%20as%20shape%20and%20size%2C%20they%0Aencounter%20significant%20challenges%20in%20reasoning%20about%20the%20depth%20and%20height%20of%0Aobjects.%20To%20address%20this%2C%20we%20introduce%20a%20suite%20of%20benchmark%20datasets%0Aencompassing%20Synthetic%202D%2C%20Synthetic%203D%2C%20and%20Real-World%20scenarios%20to%20rigorously%0Aevaluate%20these%20aspects.%20We%20benchmark%2017%20state-of-the-art%20VLMs%20using%20these%0Adatasets%20and%20find%20that%20they%20consistently%20struggle%20with%20both%20depth%20and%20height%0Aperception.%20Our%20key%20insights%20include%20detailed%20analyses%20of%20the%20shortcomings%20in%0Adepth%20and%20height%20reasoning%20capabilities%20of%20VLMs%20and%20the%20inherent%20bias%20present%0Ain%20these%20models.%20This%20study%20aims%20to%20pave%20the%20way%20for%20the%20development%20of%20VLMs%0Awith%20enhanced%20geometric%20understanding%2C%20crucial%20for%20real-world%20applications.%20The%0Acode%20and%20datasets%20for%20our%20benchmarks%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//tinyurl.com/DH-Bench1%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11748v1&entry.124074799=Read"},
{"title": "ControlCol: Controllability in Automatic Speaker Video Colorization", "author": "Rory Ward and John G. Breslin and Peter Corcoran", "abstract": "  Adding color to black-and-white speaker videos automatically is a highly\ndesirable technique. It is an artistic process that requires interactivity with\nhumans for the best results. Many existing automatic video colorization systems\nprovide little opportunity for the user to guide the colorization process. In\nthis work, we introduce a novel automatic speaker video colorization system\nwhich provides controllability to the user while also maintaining high\ncolorization quality relative to state-of-the-art techniques. We name this\nsystem ControlCol. ControlCol performs 3.5% better than the previous\nstate-of-the-art DeOldify on the Grid and Lombard Grid datasets when PSNR,\nSSIM, FID and FVD are used as metrics. This result is also supported by our\nhuman evaluation, where in a head-to-head comparison, ControlCol is preferred\n90% of the time to DeOldify. Example videos can be seen in the supplementary\nmaterial.\n", "link": "http://arxiv.org/abs/2408.11711v1", "date": "2024-08-21", "relevancy": 2.1998, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5528}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5514}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ControlCol%3A%20Controllability%20in%20Automatic%20Speaker%20Video%20Colorization&body=Title%3A%20ControlCol%3A%20Controllability%20in%20Automatic%20Speaker%20Video%20Colorization%0AAuthor%3A%20Rory%20Ward%20and%20John%20G.%20Breslin%20and%20Peter%20Corcoran%0AAbstract%3A%20%20%20Adding%20color%20to%20black-and-white%20speaker%20videos%20automatically%20is%20a%20highly%0Adesirable%20technique.%20It%20is%20an%20artistic%20process%20that%20requires%20interactivity%20with%0Ahumans%20for%20the%20best%20results.%20Many%20existing%20automatic%20video%20colorization%20systems%0Aprovide%20little%20opportunity%20for%20the%20user%20to%20guide%20the%20colorization%20process.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20automatic%20speaker%20video%20colorization%20system%0Awhich%20provides%20controllability%20to%20the%20user%20while%20also%20maintaining%20high%0Acolorization%20quality%20relative%20to%20state-of-the-art%20techniques.%20We%20name%20this%0Asystem%20ControlCol.%20ControlCol%20performs%203.5%25%20better%20than%20the%20previous%0Astate-of-the-art%20DeOldify%20on%20the%20Grid%20and%20Lombard%20Grid%20datasets%20when%20PSNR%2C%0ASSIM%2C%20FID%20and%20FVD%20are%20used%20as%20metrics.%20This%20result%20is%20also%20supported%20by%20our%0Ahuman%20evaluation%2C%20where%20in%20a%20head-to-head%20comparison%2C%20ControlCol%20is%20preferred%0A90%25%20of%20the%20time%20to%20DeOldify.%20Example%20videos%20can%20be%20seen%20in%20the%20supplementary%0Amaterial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlCol%253A%2520Controllability%2520in%2520Automatic%2520Speaker%2520Video%2520Colorization%26entry.906535625%3DRory%2520Ward%2520and%2520John%2520G.%2520Breslin%2520and%2520Peter%2520Corcoran%26entry.1292438233%3D%2520%2520Adding%2520color%2520to%2520black-and-white%2520speaker%2520videos%2520automatically%2520is%2520a%2520highly%250Adesirable%2520technique.%2520It%2520is%2520an%2520artistic%2520process%2520that%2520requires%2520interactivity%2520with%250Ahumans%2520for%2520the%2520best%2520results.%2520Many%2520existing%2520automatic%2520video%2520colorization%2520systems%250Aprovide%2520little%2520opportunity%2520for%2520the%2520user%2520to%2520guide%2520the%2520colorization%2520process.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520a%2520novel%2520automatic%2520speaker%2520video%2520colorization%2520system%250Awhich%2520provides%2520controllability%2520to%2520the%2520user%2520while%2520also%2520maintaining%2520high%250Acolorization%2520quality%2520relative%2520to%2520state-of-the-art%2520techniques.%2520We%2520name%2520this%250Asystem%2520ControlCol.%2520ControlCol%2520performs%25203.5%2525%2520better%2520than%2520the%2520previous%250Astate-of-the-art%2520DeOldify%2520on%2520the%2520Grid%2520and%2520Lombard%2520Grid%2520datasets%2520when%2520PSNR%252C%250ASSIM%252C%2520FID%2520and%2520FVD%2520are%2520used%2520as%2520metrics.%2520This%2520result%2520is%2520also%2520supported%2520by%2520our%250Ahuman%2520evaluation%252C%2520where%2520in%2520a%2520head-to-head%2520comparison%252C%2520ControlCol%2520is%2520preferred%250A90%2525%2520of%2520the%2520time%2520to%2520DeOldify.%2520Example%2520videos%2520can%2520be%2520seen%2520in%2520the%2520supplementary%250Amaterial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ControlCol%3A%20Controllability%20in%20Automatic%20Speaker%20Video%20Colorization&entry.906535625=Rory%20Ward%20and%20John%20G.%20Breslin%20and%20Peter%20Corcoran&entry.1292438233=%20%20Adding%20color%20to%20black-and-white%20speaker%20videos%20automatically%20is%20a%20highly%0Adesirable%20technique.%20It%20is%20an%20artistic%20process%20that%20requires%20interactivity%20with%0Ahumans%20for%20the%20best%20results.%20Many%20existing%20automatic%20video%20colorization%20systems%0Aprovide%20little%20opportunity%20for%20the%20user%20to%20guide%20the%20colorization%20process.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20automatic%20speaker%20video%20colorization%20system%0Awhich%20provides%20controllability%20to%20the%20user%20while%20also%20maintaining%20high%0Acolorization%20quality%20relative%20to%20state-of-the-art%20techniques.%20We%20name%20this%0Asystem%20ControlCol.%20ControlCol%20performs%203.5%25%20better%20than%20the%20previous%0Astate-of-the-art%20DeOldify%20on%20the%20Grid%20and%20Lombard%20Grid%20datasets%20when%20PSNR%2C%0ASSIM%2C%20FID%20and%20FVD%20are%20used%20as%20metrics.%20This%20result%20is%20also%20supported%20by%20our%0Ahuman%20evaluation%2C%20where%20in%20a%20head-to-head%20comparison%2C%20ControlCol%20is%20preferred%0A90%25%20of%20the%20time%20to%20DeOldify.%20Example%20videos%20can%20be%20seen%20in%20the%20supplementary%0Amaterial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11711v1&entry.124074799=Read"},
{"title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos", "author": "Fuzhao Xue and Yukang Chen and Dacheng Li and Qinghao Hu and Ligeng Zhu and Xiuyu Li and Yunhao Fang and Haotian Tang and Shang Yang and Zhijian Liu and Ethan He and Hongxu Yin and Pavlo Molchanov and Jan Kautz and Linxi Fan and Yuke Zhu and Yao Lu and Song Han", "abstract": "  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long supervised fine-tuning. However, training on long video is\ncomputationally and memory intensive. We introduce the long-context Multi-Modal\nSequence Parallelism (MM-SP) system that efficiently parallelizes long video\ntraining and inference, enabling 2M context length training on 256 GPUs without\nany gradient checkpointing. LongVILA efficiently extends the number of video\nframes of VILA from 8 to 1024, improving the long video captioning score from\n2.00 to 3.26 (out of 5), achieving 99.5% accuracy in 1400-frame (274k context\nlength) video needle-in-a-haystack. LongVILA-8B demonstrates consistent\naccuracy improvements on long videos in the VideoMME benchmark as the number of\nframes increases. Besides, MM-SP is 2.1x - 5.7x faster than ring sequence\nparallelism and 1.1x - 1.4x faster than Megatron with context parallelism +\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.\n", "link": "http://arxiv.org/abs/2408.10188v3", "date": "2024-08-21", "relevancy": 2.1844, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.56}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5404}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos&body=Title%3A%20LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos%0AAuthor%3A%20Fuzhao%20Xue%20and%20Yukang%20Chen%20and%20Dacheng%20Li%20and%20Qinghao%20Hu%20and%20Ligeng%20Zhu%20and%20Xiuyu%20Li%20and%20Yunhao%20Fang%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhijian%20Liu%20and%20Ethan%20He%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20Long-context%20capability%20is%20critical%20for%20multi-modal%20foundation%20models%2C%0Aespecially%20for%20long%20video%20understanding.%20We%20introduce%20LongVILA%2C%20a%20full-stack%0Asolution%20for%20long-context%20visual-language%20models%20by%20co-designing%20the%20algorithm%0Aand%20system.%20For%20model%20training%2C%20we%20upgrade%20existing%20VLMs%20to%20support%20long%20video%0Aunderstanding%20by%20incorporating%20two%20additional%20stages%2C%20i.e.%2C%20long%20context%0Aextension%20and%20long%20supervised%20fine-tuning.%20However%2C%20training%20on%20long%20video%20is%0Acomputationally%20and%20memory%20intensive.%20We%20introduce%20the%20long-context%20Multi-Modal%0ASequence%20Parallelism%20%28MM-SP%29%20system%20that%20efficiently%20parallelizes%20long%20video%0Atraining%20and%20inference%2C%20enabling%202M%20context%20length%20training%20on%20256%20GPUs%20without%0Aany%20gradient%20checkpointing.%20LongVILA%20efficiently%20extends%20the%20number%20of%20video%0Aframes%20of%20VILA%20from%208%20to%201024%2C%20improving%20the%20long%20video%20captioning%20score%20from%0A2.00%20to%203.26%20%28out%20of%205%29%2C%20achieving%2099.5%25%20accuracy%20in%201400-frame%20%28274k%20context%0Alength%29%20video%20needle-in-a-haystack.%20LongVILA-8B%20demonstrates%20consistent%0Aaccuracy%20improvements%20on%20long%20videos%20in%20the%20VideoMME%20benchmark%20as%20the%20number%20of%0Aframes%20increases.%20Besides%2C%20MM-SP%20is%202.1x%20-%205.7x%20faster%20than%20ring%20sequence%0Aparallelism%20and%201.1x%20-%201.4x%20faster%20than%20Megatron%20with%20context%20parallelism%20%2B%0Atensor%20parallelism.%20Moreover%2C%20it%20seamlessly%20integrates%20with%20Hugging%20Face%0ATransformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10188v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVILA%253A%2520Scaling%2520Long-Context%2520Visual%2520Language%2520Models%2520for%2520Long%2520Videos%26entry.906535625%3DFuzhao%2520Xue%2520and%2520Yukang%2520Chen%2520and%2520Dacheng%2520Li%2520and%2520Qinghao%2520Hu%2520and%2520Ligeng%2520Zhu%2520and%2520Xiuyu%2520Li%2520and%2520Yunhao%2520Fang%2520and%2520Haotian%2520Tang%2520and%2520Shang%2520Yang%2520and%2520Zhijian%2520Liu%2520and%2520Ethan%2520He%2520and%2520Hongxu%2520Yin%2520and%2520Pavlo%2520Molchanov%2520and%2520Jan%2520Kautz%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Long-context%2520capability%2520is%2520critical%2520for%2520multi-modal%2520foundation%2520models%252C%250Aespecially%2520for%2520long%2520video%2520understanding.%2520We%2520introduce%2520LongVILA%252C%2520a%2520full-stack%250Asolution%2520for%2520long-context%2520visual-language%2520models%2520by%2520co-designing%2520the%2520algorithm%250Aand%2520system.%2520For%2520model%2520training%252C%2520we%2520upgrade%2520existing%2520VLMs%2520to%2520support%2520long%2520video%250Aunderstanding%2520by%2520incorporating%2520two%2520additional%2520stages%252C%2520i.e.%252C%2520long%2520context%250Aextension%2520and%2520long%2520supervised%2520fine-tuning.%2520However%252C%2520training%2520on%2520long%2520video%2520is%250Acomputationally%2520and%2520memory%2520intensive.%2520We%2520introduce%2520the%2520long-context%2520Multi-Modal%250ASequence%2520Parallelism%2520%2528MM-SP%2529%2520system%2520that%2520efficiently%2520parallelizes%2520long%2520video%250Atraining%2520and%2520inference%252C%2520enabling%25202M%2520context%2520length%2520training%2520on%2520256%2520GPUs%2520without%250Aany%2520gradient%2520checkpointing.%2520LongVILA%2520efficiently%2520extends%2520the%2520number%2520of%2520video%250Aframes%2520of%2520VILA%2520from%25208%2520to%25201024%252C%2520improving%2520the%2520long%2520video%2520captioning%2520score%2520from%250A2.00%2520to%25203.26%2520%2528out%2520of%25205%2529%252C%2520achieving%252099.5%2525%2520accuracy%2520in%25201400-frame%2520%2528274k%2520context%250Alength%2529%2520video%2520needle-in-a-haystack.%2520LongVILA-8B%2520demonstrates%2520consistent%250Aaccuracy%2520improvements%2520on%2520long%2520videos%2520in%2520the%2520VideoMME%2520benchmark%2520as%2520the%2520number%2520of%250Aframes%2520increases.%2520Besides%252C%2520MM-SP%2520is%25202.1x%2520-%25205.7x%2520faster%2520than%2520ring%2520sequence%250Aparallelism%2520and%25201.1x%2520-%25201.4x%2520faster%2520than%2520Megatron%2520with%2520context%2520parallelism%2520%252B%250Atensor%2520parallelism.%2520Moreover%252C%2520it%2520seamlessly%2520integrates%2520with%2520Hugging%2520Face%250ATransformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10188v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos&entry.906535625=Fuzhao%20Xue%20and%20Yukang%20Chen%20and%20Dacheng%20Li%20and%20Qinghao%20Hu%20and%20Ligeng%20Zhu%20and%20Xiuyu%20Li%20and%20Yunhao%20Fang%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhijian%20Liu%20and%20Ethan%20He%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20Long-context%20capability%20is%20critical%20for%20multi-modal%20foundation%20models%2C%0Aespecially%20for%20long%20video%20understanding.%20We%20introduce%20LongVILA%2C%20a%20full-stack%0Asolution%20for%20long-context%20visual-language%20models%20by%20co-designing%20the%20algorithm%0Aand%20system.%20For%20model%20training%2C%20we%20upgrade%20existing%20VLMs%20to%20support%20long%20video%0Aunderstanding%20by%20incorporating%20two%20additional%20stages%2C%20i.e.%2C%20long%20context%0Aextension%20and%20long%20supervised%20fine-tuning.%20However%2C%20training%20on%20long%20video%20is%0Acomputationally%20and%20memory%20intensive.%20We%20introduce%20the%20long-context%20Multi-Modal%0ASequence%20Parallelism%20%28MM-SP%29%20system%20that%20efficiently%20parallelizes%20long%20video%0Atraining%20and%20inference%2C%20enabling%202M%20context%20length%20training%20on%20256%20GPUs%20without%0Aany%20gradient%20checkpointing.%20LongVILA%20efficiently%20extends%20the%20number%20of%20video%0Aframes%20of%20VILA%20from%208%20to%201024%2C%20improving%20the%20long%20video%20captioning%20score%20from%0A2.00%20to%203.26%20%28out%20of%205%29%2C%20achieving%2099.5%25%20accuracy%20in%201400-frame%20%28274k%20context%0Alength%29%20video%20needle-in-a-haystack.%20LongVILA-8B%20demonstrates%20consistent%0Aaccuracy%20improvements%20on%20long%20videos%20in%20the%20VideoMME%20benchmark%20as%20the%20number%20of%0Aframes%20increases.%20Besides%2C%20MM-SP%20is%202.1x%20-%205.7x%20faster%20than%20ring%20sequence%0Aparallelism%20and%201.1x%20-%201.4x%20faster%20than%20Megatron%20with%20context%20parallelism%20%2B%0Atensor%20parallelism.%20Moreover%2C%20it%20seamlessly%20integrates%20with%20Hugging%20Face%0ATransformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10188v3&entry.124074799=Read"},
{"title": "Story3D-Agent: Exploring 3D Storytelling Visualization with Large\n  Language Models", "author": "Yuzhou Huang and Yiran Qin and Shunlin Lu and Xintao Wang and Rui Huang and Ying Shan and Ruimao Zhang", "abstract": "  Traditional visual storytelling is complex, requiring specialized knowledge\nand substantial resources, yet often constrained by human creativity and\ncreation precision. While Large Language Models (LLMs) enhance visual\nstorytelling, current approaches often limit themselves to 2D visuals or\noversimplify stories through motion synthesis and behavioral simulation,\nfailing to create comprehensive, multi-dimensional narratives. To this end, we\npresent Story3D-Agent, a pioneering approach that leverages the capabilities of\nLLMs to transform provided narratives into 3D-rendered visualizations. By\nintegrating procedural modeling, our approach enables precise control over\nmulti-character actions and motions, as well as diverse decorative elements,\nensuring the long-range and dynamic 3D representation. Furthermore, our method\nsupports narrative extension through logical reasoning, ensuring that generated\ncontent remains consistent with existing conditions. We have thoroughly\nevaluated our Story3D-Agent to validate its effectiveness, offering a basic\nframework to advance 3D story representation.\n", "link": "http://arxiv.org/abs/2408.11801v1", "date": "2024-08-21", "relevancy": 2.1771, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5647}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5402}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Story3D-Agent%3A%20Exploring%203D%20Storytelling%20Visualization%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Story3D-Agent%3A%20Exploring%203D%20Storytelling%20Visualization%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yuzhou%20Huang%20and%20Yiran%20Qin%20and%20Shunlin%20Lu%20and%20Xintao%20Wang%20and%20Rui%20Huang%20and%20Ying%20Shan%20and%20Ruimao%20Zhang%0AAbstract%3A%20%20%20Traditional%20visual%20storytelling%20is%20complex%2C%20requiring%20specialized%20knowledge%0Aand%20substantial%20resources%2C%20yet%20often%20constrained%20by%20human%20creativity%20and%0Acreation%20precision.%20While%20Large%20Language%20Models%20%28LLMs%29%20enhance%20visual%0Astorytelling%2C%20current%20approaches%20often%20limit%20themselves%20to%202D%20visuals%20or%0Aoversimplify%20stories%20through%20motion%20synthesis%20and%20behavioral%20simulation%2C%0Afailing%20to%20create%20comprehensive%2C%20multi-dimensional%20narratives.%20To%20this%20end%2C%20we%0Apresent%20Story3D-Agent%2C%20a%20pioneering%20approach%20that%20leverages%20the%20capabilities%20of%0ALLMs%20to%20transform%20provided%20narratives%20into%203D-rendered%20visualizations.%20By%0Aintegrating%20procedural%20modeling%2C%20our%20approach%20enables%20precise%20control%20over%0Amulti-character%20actions%20and%20motions%2C%20as%20well%20as%20diverse%20decorative%20elements%2C%0Aensuring%20the%20long-range%20and%20dynamic%203D%20representation.%20Furthermore%2C%20our%20method%0Asupports%20narrative%20extension%20through%20logical%20reasoning%2C%20ensuring%20that%20generated%0Acontent%20remains%20consistent%20with%20existing%20conditions.%20We%20have%20thoroughly%0Aevaluated%20our%20Story3D-Agent%20to%20validate%20its%20effectiveness%2C%20offering%20a%20basic%0Aframework%20to%20advance%203D%20story%20representation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStory3D-Agent%253A%2520Exploring%25203D%2520Storytelling%2520Visualization%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYuzhou%2520Huang%2520and%2520Yiran%2520Qin%2520and%2520Shunlin%2520Lu%2520and%2520Xintao%2520Wang%2520and%2520Rui%2520Huang%2520and%2520Ying%2520Shan%2520and%2520Ruimao%2520Zhang%26entry.1292438233%3D%2520%2520Traditional%2520visual%2520storytelling%2520is%2520complex%252C%2520requiring%2520specialized%2520knowledge%250Aand%2520substantial%2520resources%252C%2520yet%2520often%2520constrained%2520by%2520human%2520creativity%2520and%250Acreation%2520precision.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520enhance%2520visual%250Astorytelling%252C%2520current%2520approaches%2520often%2520limit%2520themselves%2520to%25202D%2520visuals%2520or%250Aoversimplify%2520stories%2520through%2520motion%2520synthesis%2520and%2520behavioral%2520simulation%252C%250Afailing%2520to%2520create%2520comprehensive%252C%2520multi-dimensional%2520narratives.%2520To%2520this%2520end%252C%2520we%250Apresent%2520Story3D-Agent%252C%2520a%2520pioneering%2520approach%2520that%2520leverages%2520the%2520capabilities%2520of%250ALLMs%2520to%2520transform%2520provided%2520narratives%2520into%25203D-rendered%2520visualizations.%2520By%250Aintegrating%2520procedural%2520modeling%252C%2520our%2520approach%2520enables%2520precise%2520control%2520over%250Amulti-character%2520actions%2520and%2520motions%252C%2520as%2520well%2520as%2520diverse%2520decorative%2520elements%252C%250Aensuring%2520the%2520long-range%2520and%2520dynamic%25203D%2520representation.%2520Furthermore%252C%2520our%2520method%250Asupports%2520narrative%2520extension%2520through%2520logical%2520reasoning%252C%2520ensuring%2520that%2520generated%250Acontent%2520remains%2520consistent%2520with%2520existing%2520conditions.%2520We%2520have%2520thoroughly%250Aevaluated%2520our%2520Story3D-Agent%2520to%2520validate%2520its%2520effectiveness%252C%2520offering%2520a%2520basic%250Aframework%2520to%2520advance%25203D%2520story%2520representation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Story3D-Agent%3A%20Exploring%203D%20Storytelling%20Visualization%20with%20Large%0A%20%20Language%20Models&entry.906535625=Yuzhou%20Huang%20and%20Yiran%20Qin%20and%20Shunlin%20Lu%20and%20Xintao%20Wang%20and%20Rui%20Huang%20and%20Ying%20Shan%20and%20Ruimao%20Zhang&entry.1292438233=%20%20Traditional%20visual%20storytelling%20is%20complex%2C%20requiring%20specialized%20knowledge%0Aand%20substantial%20resources%2C%20yet%20often%20constrained%20by%20human%20creativity%20and%0Acreation%20precision.%20While%20Large%20Language%20Models%20%28LLMs%29%20enhance%20visual%0Astorytelling%2C%20current%20approaches%20often%20limit%20themselves%20to%202D%20visuals%20or%0Aoversimplify%20stories%20through%20motion%20synthesis%20and%20behavioral%20simulation%2C%0Afailing%20to%20create%20comprehensive%2C%20multi-dimensional%20narratives.%20To%20this%20end%2C%20we%0Apresent%20Story3D-Agent%2C%20a%20pioneering%20approach%20that%20leverages%20the%20capabilities%20of%0ALLMs%20to%20transform%20provided%20narratives%20into%203D-rendered%20visualizations.%20By%0Aintegrating%20procedural%20modeling%2C%20our%20approach%20enables%20precise%20control%20over%0Amulti-character%20actions%20and%20motions%2C%20as%20well%20as%20diverse%20decorative%20elements%2C%0Aensuring%20the%20long-range%20and%20dynamic%203D%20representation.%20Furthermore%2C%20our%20method%0Asupports%20narrative%20extension%20through%20logical%20reasoning%2C%20ensuring%20that%20generated%0Acontent%20remains%20consistent%20with%20existing%20conditions.%20We%20have%20thoroughly%0Aevaluated%20our%20Story3D-Agent%20to%20validate%20its%20effectiveness%2C%20offering%20a%20basic%0Aframework%20to%20advance%203D%20story%20representation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11801v1&entry.124074799=Read"},
{"title": "Hierarchical Salient Patch Identification for Interpretable Fundus\n  Disease Localization", "author": "Yitao Peng and Lianghua He and Die Hu", "abstract": "  With the widespread application of deep learning technology in medical image\nanalysis, the effective explanation of model predictions and improvement of\ndiagnostic accuracy have become urgent problems that need to be solved.\nAttribution methods have become key tools to help doctors better understand the\ndiagnostic basis of models, and are used to explain and localize diseases in\nmedical images. However, previous methods suffer from inaccurate and incomplete\nlocalization problems for fundus diseases with complex and diverse structures.\nTo solve these problems, we propose a weakly supervised interpretable fundus\ndisease localization method called hierarchical salient patch identification\n(HSPI) that can achieve interpretable disease localization using only\nimage-level labels and a neural network classifier (NNC). First, we propose\nsalient patch identification (SPI), which divides the image into several\npatches and optimizes consistency loss to identify which patch in the input\nimage is most important for the network's prediction, in order to locate the\ndisease. Second, we propose a hierarchical identification strategy to force SPI\nto analyze the importance of different areas to neural network classifier's\nprediction to comprehensively locate disease areas. Conditional peak focusing\nis then introduced to ensure that the mask vector can accurately locate the\ndisease area. Finally, we propose patch selection based on multi-sized\nintersections to filter out incorrectly or additionally identified non-disease\nregions. We conduct disease localization experiments on fundus image datasets\nand achieve the best performance on multiple evaluation metrics compared to\nprevious interpretable attribution methods. Additional ablation studies are\nconducted to verify the effectiveness of each method.\n", "link": "http://arxiv.org/abs/2405.14334v2", "date": "2024-08-21", "relevancy": 2.1689, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5382}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Salient%20Patch%20Identification%20for%20Interpretable%20Fundus%0A%20%20Disease%20Localization&body=Title%3A%20Hierarchical%20Salient%20Patch%20Identification%20for%20Interpretable%20Fundus%0A%20%20Disease%20Localization%0AAuthor%3A%20Yitao%20Peng%20and%20Lianghua%20He%20and%20Die%20Hu%0AAbstract%3A%20%20%20With%20the%20widespread%20application%20of%20deep%20learning%20technology%20in%20medical%20image%0Aanalysis%2C%20the%20effective%20explanation%20of%20model%20predictions%20and%20improvement%20of%0Adiagnostic%20accuracy%20have%20become%20urgent%20problems%20that%20need%20to%20be%20solved.%0AAttribution%20methods%20have%20become%20key%20tools%20to%20help%20doctors%20better%20understand%20the%0Adiagnostic%20basis%20of%20models%2C%20and%20are%20used%20to%20explain%20and%20localize%20diseases%20in%0Amedical%20images.%20However%2C%20previous%20methods%20suffer%20from%20inaccurate%20and%20incomplete%0Alocalization%20problems%20for%20fundus%20diseases%20with%20complex%20and%20diverse%20structures.%0ATo%20solve%20these%20problems%2C%20we%20propose%20a%20weakly%20supervised%20interpretable%20fundus%0Adisease%20localization%20method%20called%20hierarchical%20salient%20patch%20identification%0A%28HSPI%29%20that%20can%20achieve%20interpretable%20disease%20localization%20using%20only%0Aimage-level%20labels%20and%20a%20neural%20network%20classifier%20%28NNC%29.%20First%2C%20we%20propose%0Asalient%20patch%20identification%20%28SPI%29%2C%20which%20divides%20the%20image%20into%20several%0Apatches%20and%20optimizes%20consistency%20loss%20to%20identify%20which%20patch%20in%20the%20input%0Aimage%20is%20most%20important%20for%20the%20network%27s%20prediction%2C%20in%20order%20to%20locate%20the%0Adisease.%20Second%2C%20we%20propose%20a%20hierarchical%20identification%20strategy%20to%20force%20SPI%0Ato%20analyze%20the%20importance%20of%20different%20areas%20to%20neural%20network%20classifier%27s%0Aprediction%20to%20comprehensively%20locate%20disease%20areas.%20Conditional%20peak%20focusing%0Ais%20then%20introduced%20to%20ensure%20that%20the%20mask%20vector%20can%20accurately%20locate%20the%0Adisease%20area.%20Finally%2C%20we%20propose%20patch%20selection%20based%20on%20multi-sized%0Aintersections%20to%20filter%20out%20incorrectly%20or%20additionally%20identified%20non-disease%0Aregions.%20We%20conduct%20disease%20localization%20experiments%20on%20fundus%20image%20datasets%0Aand%20achieve%20the%20best%20performance%20on%20multiple%20evaluation%20metrics%20compared%20to%0Aprevious%20interpretable%20attribution%20methods.%20Additional%20ablation%20studies%20are%0Aconducted%20to%20verify%20the%20effectiveness%20of%20each%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Salient%2520Patch%2520Identification%2520for%2520Interpretable%2520Fundus%250A%2520%2520Disease%2520Localization%26entry.906535625%3DYitao%2520Peng%2520and%2520Lianghua%2520He%2520and%2520Die%2520Hu%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520application%2520of%2520deep%2520learning%2520technology%2520in%2520medical%2520image%250Aanalysis%252C%2520the%2520effective%2520explanation%2520of%2520model%2520predictions%2520and%2520improvement%2520of%250Adiagnostic%2520accuracy%2520have%2520become%2520urgent%2520problems%2520that%2520need%2520to%2520be%2520solved.%250AAttribution%2520methods%2520have%2520become%2520key%2520tools%2520to%2520help%2520doctors%2520better%2520understand%2520the%250Adiagnostic%2520basis%2520of%2520models%252C%2520and%2520are%2520used%2520to%2520explain%2520and%2520localize%2520diseases%2520in%250Amedical%2520images.%2520However%252C%2520previous%2520methods%2520suffer%2520from%2520inaccurate%2520and%2520incomplete%250Alocalization%2520problems%2520for%2520fundus%2520diseases%2520with%2520complex%2520and%2520diverse%2520structures.%250ATo%2520solve%2520these%2520problems%252C%2520we%2520propose%2520a%2520weakly%2520supervised%2520interpretable%2520fundus%250Adisease%2520localization%2520method%2520called%2520hierarchical%2520salient%2520patch%2520identification%250A%2528HSPI%2529%2520that%2520can%2520achieve%2520interpretable%2520disease%2520localization%2520using%2520only%250Aimage-level%2520labels%2520and%2520a%2520neural%2520network%2520classifier%2520%2528NNC%2529.%2520First%252C%2520we%2520propose%250Asalient%2520patch%2520identification%2520%2528SPI%2529%252C%2520which%2520divides%2520the%2520image%2520into%2520several%250Apatches%2520and%2520optimizes%2520consistency%2520loss%2520to%2520identify%2520which%2520patch%2520in%2520the%2520input%250Aimage%2520is%2520most%2520important%2520for%2520the%2520network%2527s%2520prediction%252C%2520in%2520order%2520to%2520locate%2520the%250Adisease.%2520Second%252C%2520we%2520propose%2520a%2520hierarchical%2520identification%2520strategy%2520to%2520force%2520SPI%250Ato%2520analyze%2520the%2520importance%2520of%2520different%2520areas%2520to%2520neural%2520network%2520classifier%2527s%250Aprediction%2520to%2520comprehensively%2520locate%2520disease%2520areas.%2520Conditional%2520peak%2520focusing%250Ais%2520then%2520introduced%2520to%2520ensure%2520that%2520the%2520mask%2520vector%2520can%2520accurately%2520locate%2520the%250Adisease%2520area.%2520Finally%252C%2520we%2520propose%2520patch%2520selection%2520based%2520on%2520multi-sized%250Aintersections%2520to%2520filter%2520out%2520incorrectly%2520or%2520additionally%2520identified%2520non-disease%250Aregions.%2520We%2520conduct%2520disease%2520localization%2520experiments%2520on%2520fundus%2520image%2520datasets%250Aand%2520achieve%2520the%2520best%2520performance%2520on%2520multiple%2520evaluation%2520metrics%2520compared%2520to%250Aprevious%2520interpretable%2520attribution%2520methods.%2520Additional%2520ablation%2520studies%2520are%250Aconducted%2520to%2520verify%2520the%2520effectiveness%2520of%2520each%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Salient%20Patch%20Identification%20for%20Interpretable%20Fundus%0A%20%20Disease%20Localization&entry.906535625=Yitao%20Peng%20and%20Lianghua%20He%20and%20Die%20Hu&entry.1292438233=%20%20With%20the%20widespread%20application%20of%20deep%20learning%20technology%20in%20medical%20image%0Aanalysis%2C%20the%20effective%20explanation%20of%20model%20predictions%20and%20improvement%20of%0Adiagnostic%20accuracy%20have%20become%20urgent%20problems%20that%20need%20to%20be%20solved.%0AAttribution%20methods%20have%20become%20key%20tools%20to%20help%20doctors%20better%20understand%20the%0Adiagnostic%20basis%20of%20models%2C%20and%20are%20used%20to%20explain%20and%20localize%20diseases%20in%0Amedical%20images.%20However%2C%20previous%20methods%20suffer%20from%20inaccurate%20and%20incomplete%0Alocalization%20problems%20for%20fundus%20diseases%20with%20complex%20and%20diverse%20structures.%0ATo%20solve%20these%20problems%2C%20we%20propose%20a%20weakly%20supervised%20interpretable%20fundus%0Adisease%20localization%20method%20called%20hierarchical%20salient%20patch%20identification%0A%28HSPI%29%20that%20can%20achieve%20interpretable%20disease%20localization%20using%20only%0Aimage-level%20labels%20and%20a%20neural%20network%20classifier%20%28NNC%29.%20First%2C%20we%20propose%0Asalient%20patch%20identification%20%28SPI%29%2C%20which%20divides%20the%20image%20into%20several%0Apatches%20and%20optimizes%20consistency%20loss%20to%20identify%20which%20patch%20in%20the%20input%0Aimage%20is%20most%20important%20for%20the%20network%27s%20prediction%2C%20in%20order%20to%20locate%20the%0Adisease.%20Second%2C%20we%20propose%20a%20hierarchical%20identification%20strategy%20to%20force%20SPI%0Ato%20analyze%20the%20importance%20of%20different%20areas%20to%20neural%20network%20classifier%27s%0Aprediction%20to%20comprehensively%20locate%20disease%20areas.%20Conditional%20peak%20focusing%0Ais%20then%20introduced%20to%20ensure%20that%20the%20mask%20vector%20can%20accurately%20locate%20the%0Adisease%20area.%20Finally%2C%20we%20propose%20patch%20selection%20based%20on%20multi-sized%0Aintersections%20to%20filter%20out%20incorrectly%20or%20additionally%20identified%20non-disease%0Aregions.%20We%20conduct%20disease%20localization%20experiments%20on%20fundus%20image%20datasets%0Aand%20achieve%20the%20best%20performance%20on%20multiple%20evaluation%20metrics%20compared%20to%0Aprevious%20interpretable%20attribution%20methods.%20Additional%20ablation%20studies%20are%0Aconducted%20to%20verify%20the%20effectiveness%20of%20each%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14334v2&entry.124074799=Read"},
{"title": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs", "author": "Yuanyang Yin and Yaqi Zhao and Yajie Zhang and Ke Lin and Jiahao Wang and Xin Tao and Pengfei Wan and Di Zhang and Baoqun Yin and Wentao Zhang", "abstract": "  Multimodal Large Language Models (MLLMs) have recently demonstrated\nremarkable perceptual and reasoning abilities, typically comprising a Vision\nEncoder, an Adapter, and a Large Language Model (LLM). The adapter serves as\nthe critical bridge between the visual and language components. However,\ntraining adapters with image-level supervision often results in significant\nmisalignment, undermining the LLMs' capabilities and limiting the potential of\nMultimodal LLMs. To address this, we introduce Supervised Embedding Alignment\n(SEA), a token-level alignment method that leverages vision-language\npre-trained models, such as CLIP, to align visual tokens with the LLM's\nembedding space through contrastive learning. This approach ensures a more\ncoherent integration of visual and language representations, enhancing the\nperformance and interpretability of multimodal LLMs while preserving their\ninherent capabilities. Extensive experiments show that SEA effectively improves\nMLLMs, particularly for smaller models, without adding extra data or inference\ncomputation. SEA also lays the groundwork for developing more general and\nadaptable solutions to enhance multimodal systems.\n", "link": "http://arxiv.org/abs/2408.11813v1", "date": "2024-08-21", "relevancy": 2.1601, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5974}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5052}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEA%3A%20Supervised%20Embedding%20Alignment%20for%20Token-Level%20Visual-Textual%0A%20%20Integration%20in%20MLLMs&body=Title%3A%20SEA%3A%20Supervised%20Embedding%20Alignment%20for%20Token-Level%20Visual-Textual%0A%20%20Integration%20in%20MLLMs%0AAuthor%3A%20Yuanyang%20Yin%20and%20Yaqi%20Zhao%20and%20Yajie%20Zhang%20and%20Ke%20Lin%20and%20Jiahao%20Wang%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Baoqun%20Yin%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20demonstrated%0Aremarkable%20perceptual%20and%20reasoning%20abilities%2C%20typically%20comprising%20a%20Vision%0AEncoder%2C%20an%20Adapter%2C%20and%20a%20Large%20Language%20Model%20%28LLM%29.%20The%20adapter%20serves%20as%0Athe%20critical%20bridge%20between%20the%20visual%20and%20language%20components.%20However%2C%0Atraining%20adapters%20with%20image-level%20supervision%20often%20results%20in%20significant%0Amisalignment%2C%20undermining%20the%20LLMs%27%20capabilities%20and%20limiting%20the%20potential%20of%0AMultimodal%20LLMs.%20To%20address%20this%2C%20we%20introduce%20Supervised%20Embedding%20Alignment%0A%28SEA%29%2C%20a%20token-level%20alignment%20method%20that%20leverages%20vision-language%0Apre-trained%20models%2C%20such%20as%20CLIP%2C%20to%20align%20visual%20tokens%20with%20the%20LLM%27s%0Aembedding%20space%20through%20contrastive%20learning.%20This%20approach%20ensures%20a%20more%0Acoherent%20integration%20of%20visual%20and%20language%20representations%2C%20enhancing%20the%0Aperformance%20and%20interpretability%20of%20multimodal%20LLMs%20while%20preserving%20their%0Ainherent%20capabilities.%20Extensive%20experiments%20show%20that%20SEA%20effectively%20improves%0AMLLMs%2C%20particularly%20for%20smaller%20models%2C%20without%20adding%20extra%20data%20or%20inference%0Acomputation.%20SEA%20also%20lays%20the%20groundwork%20for%20developing%20more%20general%20and%0Aadaptable%20solutions%20to%20enhance%20multimodal%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEA%253A%2520Supervised%2520Embedding%2520Alignment%2520for%2520Token-Level%2520Visual-Textual%250A%2520%2520Integration%2520in%2520MLLMs%26entry.906535625%3DYuanyang%2520Yin%2520and%2520Yaqi%2520Zhao%2520and%2520Yajie%2520Zhang%2520and%2520Ke%2520Lin%2520and%2520Jiahao%2520Wang%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Baoqun%2520Yin%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520recently%2520demonstrated%250Aremarkable%2520perceptual%2520and%2520reasoning%2520abilities%252C%2520typically%2520comprising%2520a%2520Vision%250AEncoder%252C%2520an%2520Adapter%252C%2520and%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529.%2520The%2520adapter%2520serves%2520as%250Athe%2520critical%2520bridge%2520between%2520the%2520visual%2520and%2520language%2520components.%2520However%252C%250Atraining%2520adapters%2520with%2520image-level%2520supervision%2520often%2520results%2520in%2520significant%250Amisalignment%252C%2520undermining%2520the%2520LLMs%2527%2520capabilities%2520and%2520limiting%2520the%2520potential%2520of%250AMultimodal%2520LLMs.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Supervised%2520Embedding%2520Alignment%250A%2528SEA%2529%252C%2520a%2520token-level%2520alignment%2520method%2520that%2520leverages%2520vision-language%250Apre-trained%2520models%252C%2520such%2520as%2520CLIP%252C%2520to%2520align%2520visual%2520tokens%2520with%2520the%2520LLM%2527s%250Aembedding%2520space%2520through%2520contrastive%2520learning.%2520This%2520approach%2520ensures%2520a%2520more%250Acoherent%2520integration%2520of%2520visual%2520and%2520language%2520representations%252C%2520enhancing%2520the%250Aperformance%2520and%2520interpretability%2520of%2520multimodal%2520LLMs%2520while%2520preserving%2520their%250Ainherent%2520capabilities.%2520Extensive%2520experiments%2520show%2520that%2520SEA%2520effectively%2520improves%250AMLLMs%252C%2520particularly%2520for%2520smaller%2520models%252C%2520without%2520adding%2520extra%2520data%2520or%2520inference%250Acomputation.%2520SEA%2520also%2520lays%2520the%2520groundwork%2520for%2520developing%2520more%2520general%2520and%250Aadaptable%2520solutions%2520to%2520enhance%2520multimodal%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEA%3A%20Supervised%20Embedding%20Alignment%20for%20Token-Level%20Visual-Textual%0A%20%20Integration%20in%20MLLMs&entry.906535625=Yuanyang%20Yin%20and%20Yaqi%20Zhao%20and%20Yajie%20Zhang%20and%20Ke%20Lin%20and%20Jiahao%20Wang%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Baoqun%20Yin%20and%20Wentao%20Zhang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20demonstrated%0Aremarkable%20perceptual%20and%20reasoning%20abilities%2C%20typically%20comprising%20a%20Vision%0AEncoder%2C%20an%20Adapter%2C%20and%20a%20Large%20Language%20Model%20%28LLM%29.%20The%20adapter%20serves%20as%0Athe%20critical%20bridge%20between%20the%20visual%20and%20language%20components.%20However%2C%0Atraining%20adapters%20with%20image-level%20supervision%20often%20results%20in%20significant%0Amisalignment%2C%20undermining%20the%20LLMs%27%20capabilities%20and%20limiting%20the%20potential%20of%0AMultimodal%20LLMs.%20To%20address%20this%2C%20we%20introduce%20Supervised%20Embedding%20Alignment%0A%28SEA%29%2C%20a%20token-level%20alignment%20method%20that%20leverages%20vision-language%0Apre-trained%20models%2C%20such%20as%20CLIP%2C%20to%20align%20visual%20tokens%20with%20the%20LLM%27s%0Aembedding%20space%20through%20contrastive%20learning.%20This%20approach%20ensures%20a%20more%0Acoherent%20integration%20of%20visual%20and%20language%20representations%2C%20enhancing%20the%0Aperformance%20and%20interpretability%20of%20multimodal%20LLMs%20while%20preserving%20their%0Ainherent%20capabilities.%20Extensive%20experiments%20show%20that%20SEA%20effectively%20improves%0AMLLMs%2C%20particularly%20for%20smaller%20models%2C%20without%20adding%20extra%20data%20or%20inference%0Acomputation.%20SEA%20also%20lays%20the%20groundwork%20for%20developing%20more%20general%20and%0Aadaptable%20solutions%20to%20enhance%20multimodal%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11813v1&entry.124074799=Read"},
{"title": "Multi-Grained Query-Guided Set Prediction Network for Grounded\n  Multimodal Named Entity Recognition", "author": "Jielong Tang and Zhenxing Wang and Ziyang Gong and Jianxing Yu and Xiangwei Zhu and Jian Yin", "abstract": "  Grounded Multimodal Named Entity Recognition (GMNER) is an emerging\ninformation extraction (IE) task, aiming to simultaneously extract entity\nspans, types, and corresponding visual regions of entities from given\nsentence-image pairs data. Recent unified methods employing machine reading\ncomprehension or sequence generation-based frameworks show limitations in this\ndifficult task. The former, utilizing human-designed queries, struggles to\ndifferentiate ambiguous entities, such as Jordan (Person) and off-White x\nJordan (Shoes). The latter, following the one-by-one decoding order, suffers\nfrom exposure bias issues. We maintain that these works misunderstand the\nrelationships of multimodal entities. To tackle these, we propose a novel\nunified framework named Multi-grained Query-guided Set Prediction Network\n(MQSPN) to learn appropriate relationships at intra-entity and inter-entity\nlevels. Specifically, MQSPN consists of a Multi-grained Query Set (MQS) and a\nMultimodal Set Prediction Network (MSP). MQS explicitly aligns entity regions\nwith entity spans by employing a set of learnable queries to strengthen\nintra-entity connections. Based on distinct intra-entity modeling, MSP\nreformulates GMNER as a set prediction, guiding models to establish appropriate\ninter-entity relationships from a global matching perspective. Additionally, we\nincorporate a query-guided Fusion Net (QFNet) to work as a glue network between\nMQS and MSP. Extensive experiments demonstrate that our approach achieves\nstate-of-the-art performances in widely used benchmarks.\n", "link": "http://arxiv.org/abs/2407.21033v2", "date": "2024-08-21", "relevancy": 2.1497, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5443}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Grained%20Query-Guided%20Set%20Prediction%20Network%20for%20Grounded%0A%20%20Multimodal%20Named%20Entity%20Recognition&body=Title%3A%20Multi-Grained%20Query-Guided%20Set%20Prediction%20Network%20for%20Grounded%0A%20%20Multimodal%20Named%20Entity%20Recognition%0AAuthor%3A%20Jielong%20Tang%20and%20Zhenxing%20Wang%20and%20Ziyang%20Gong%20and%20Jianxing%20Yu%20and%20Xiangwei%20Zhu%20and%20Jian%20Yin%0AAbstract%3A%20%20%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20%28GMNER%29%20is%20an%20emerging%0Ainformation%20extraction%20%28IE%29%20task%2C%20aiming%20to%20simultaneously%20extract%20entity%0Aspans%2C%20types%2C%20and%20corresponding%20visual%20regions%20of%20entities%20from%20given%0Asentence-image%20pairs%20data.%20Recent%20unified%20methods%20employing%20machine%20reading%0Acomprehension%20or%20sequence%20generation-based%20frameworks%20show%20limitations%20in%20this%0Adifficult%20task.%20The%20former%2C%20utilizing%20human-designed%20queries%2C%20struggles%20to%0Adifferentiate%20ambiguous%20entities%2C%20such%20as%20Jordan%20%28Person%29%20and%20off-White%20x%0AJordan%20%28Shoes%29.%20The%20latter%2C%20following%20the%20one-by-one%20decoding%20order%2C%20suffers%0Afrom%20exposure%20bias%20issues.%20We%20maintain%20that%20these%20works%20misunderstand%20the%0Arelationships%20of%20multimodal%20entities.%20To%20tackle%20these%2C%20we%20propose%20a%20novel%0Aunified%20framework%20named%20Multi-grained%20Query-guided%20Set%20Prediction%20Network%0A%28MQSPN%29%20to%20learn%20appropriate%20relationships%20at%20intra-entity%20and%20inter-entity%0Alevels.%20Specifically%2C%20MQSPN%20consists%20of%20a%20Multi-grained%20Query%20Set%20%28MQS%29%20and%20a%0AMultimodal%20Set%20Prediction%20Network%20%28MSP%29.%20MQS%20explicitly%20aligns%20entity%20regions%0Awith%20entity%20spans%20by%20employing%20a%20set%20of%20learnable%20queries%20to%20strengthen%0Aintra-entity%20connections.%20Based%20on%20distinct%20intra-entity%20modeling%2C%20MSP%0Areformulates%20GMNER%20as%20a%20set%20prediction%2C%20guiding%20models%20to%20establish%20appropriate%0Ainter-entity%20relationships%20from%20a%20global%20matching%20perspective.%20Additionally%2C%20we%0Aincorporate%20a%20query-guided%20Fusion%20Net%20%28QFNet%29%20to%20work%20as%20a%20glue%20network%20between%0AMQS%20and%20MSP.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%0Astate-of-the-art%20performances%20in%20widely%20used%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21033v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Grained%2520Query-Guided%2520Set%2520Prediction%2520Network%2520for%2520Grounded%250A%2520%2520Multimodal%2520Named%2520Entity%2520Recognition%26entry.906535625%3DJielong%2520Tang%2520and%2520Zhenxing%2520Wang%2520and%2520Ziyang%2520Gong%2520and%2520Jianxing%2520Yu%2520and%2520Xiangwei%2520Zhu%2520and%2520Jian%2520Yin%26entry.1292438233%3D%2520%2520Grounded%2520Multimodal%2520Named%2520Entity%2520Recognition%2520%2528GMNER%2529%2520is%2520an%2520emerging%250Ainformation%2520extraction%2520%2528IE%2529%2520task%252C%2520aiming%2520to%2520simultaneously%2520extract%2520entity%250Aspans%252C%2520types%252C%2520and%2520corresponding%2520visual%2520regions%2520of%2520entities%2520from%2520given%250Asentence-image%2520pairs%2520data.%2520Recent%2520unified%2520methods%2520employing%2520machine%2520reading%250Acomprehension%2520or%2520sequence%2520generation-based%2520frameworks%2520show%2520limitations%2520in%2520this%250Adifficult%2520task.%2520The%2520former%252C%2520utilizing%2520human-designed%2520queries%252C%2520struggles%2520to%250Adifferentiate%2520ambiguous%2520entities%252C%2520such%2520as%2520Jordan%2520%2528Person%2529%2520and%2520off-White%2520x%250AJordan%2520%2528Shoes%2529.%2520The%2520latter%252C%2520following%2520the%2520one-by-one%2520decoding%2520order%252C%2520suffers%250Afrom%2520exposure%2520bias%2520issues.%2520We%2520maintain%2520that%2520these%2520works%2520misunderstand%2520the%250Arelationships%2520of%2520multimodal%2520entities.%2520To%2520tackle%2520these%252C%2520we%2520propose%2520a%2520novel%250Aunified%2520framework%2520named%2520Multi-grained%2520Query-guided%2520Set%2520Prediction%2520Network%250A%2528MQSPN%2529%2520to%2520learn%2520appropriate%2520relationships%2520at%2520intra-entity%2520and%2520inter-entity%250Alevels.%2520Specifically%252C%2520MQSPN%2520consists%2520of%2520a%2520Multi-grained%2520Query%2520Set%2520%2528MQS%2529%2520and%2520a%250AMultimodal%2520Set%2520Prediction%2520Network%2520%2528MSP%2529.%2520MQS%2520explicitly%2520aligns%2520entity%2520regions%250Awith%2520entity%2520spans%2520by%2520employing%2520a%2520set%2520of%2520learnable%2520queries%2520to%2520strengthen%250Aintra-entity%2520connections.%2520Based%2520on%2520distinct%2520intra-entity%2520modeling%252C%2520MSP%250Areformulates%2520GMNER%2520as%2520a%2520set%2520prediction%252C%2520guiding%2520models%2520to%2520establish%2520appropriate%250Ainter-entity%2520relationships%2520from%2520a%2520global%2520matching%2520perspective.%2520Additionally%252C%2520we%250Aincorporate%2520a%2520query-guided%2520Fusion%2520Net%2520%2528QFNet%2529%2520to%2520work%2520as%2520a%2520glue%2520network%2520between%250AMQS%2520and%2520MSP.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%250Astate-of-the-art%2520performances%2520in%2520widely%2520used%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21033v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Grained%20Query-Guided%20Set%20Prediction%20Network%20for%20Grounded%0A%20%20Multimodal%20Named%20Entity%20Recognition&entry.906535625=Jielong%20Tang%20and%20Zhenxing%20Wang%20and%20Ziyang%20Gong%20and%20Jianxing%20Yu%20and%20Xiangwei%20Zhu%20and%20Jian%20Yin&entry.1292438233=%20%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20%28GMNER%29%20is%20an%20emerging%0Ainformation%20extraction%20%28IE%29%20task%2C%20aiming%20to%20simultaneously%20extract%20entity%0Aspans%2C%20types%2C%20and%20corresponding%20visual%20regions%20of%20entities%20from%20given%0Asentence-image%20pairs%20data.%20Recent%20unified%20methods%20employing%20machine%20reading%0Acomprehension%20or%20sequence%20generation-based%20frameworks%20show%20limitations%20in%20this%0Adifficult%20task.%20The%20former%2C%20utilizing%20human-designed%20queries%2C%20struggles%20to%0Adifferentiate%20ambiguous%20entities%2C%20such%20as%20Jordan%20%28Person%29%20and%20off-White%20x%0AJordan%20%28Shoes%29.%20The%20latter%2C%20following%20the%20one-by-one%20decoding%20order%2C%20suffers%0Afrom%20exposure%20bias%20issues.%20We%20maintain%20that%20these%20works%20misunderstand%20the%0Arelationships%20of%20multimodal%20entities.%20To%20tackle%20these%2C%20we%20propose%20a%20novel%0Aunified%20framework%20named%20Multi-grained%20Query-guided%20Set%20Prediction%20Network%0A%28MQSPN%29%20to%20learn%20appropriate%20relationships%20at%20intra-entity%20and%20inter-entity%0Alevels.%20Specifically%2C%20MQSPN%20consists%20of%20a%20Multi-grained%20Query%20Set%20%28MQS%29%20and%20a%0AMultimodal%20Set%20Prediction%20Network%20%28MSP%29.%20MQS%20explicitly%20aligns%20entity%20regions%0Awith%20entity%20spans%20by%20employing%20a%20set%20of%20learnable%20queries%20to%20strengthen%0Aintra-entity%20connections.%20Based%20on%20distinct%20intra-entity%20modeling%2C%20MSP%0Areformulates%20GMNER%20as%20a%20set%20prediction%2C%20guiding%20models%20to%20establish%20appropriate%0Ainter-entity%20relationships%20from%20a%20global%20matching%20perspective.%20Additionally%2C%20we%0Aincorporate%20a%20query-guided%20Fusion%20Net%20%28QFNet%29%20to%20work%20as%20a%20glue%20network%20between%0AMQS%20and%20MSP.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%0Astate-of-the-art%20performances%20in%20widely%20used%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21033v2&entry.124074799=Read"},
{"title": "Approaching Deep Learning through the Spectral Dynamics of Weights", "author": "David Yunis and Kumar Kshitij Patel and Samuel Wheeler and Pedro Savarese and Gal Vardi and Karen Livescu and Michael Maire and Matthew R. Walter", "abstract": "  We propose an empirical approach centered on the spectral dynamics of weights\n-- the behavior of singular values and vectors during optimization -- to unify\nand clarify several phenomena in deep learning. We identify a consistent bias\nin optimization across various experiments, from small-scale ``grokking'' to\nlarge-scale tasks like image classification with ConvNets, image generation\nwith UNets, speech recognition with LSTMs, and language modeling with\nTransformers. We also demonstrate that weight decay enhances this bias beyond\nits role as a norm regularizer, even in practical systems. Moreover, we show\nthat these spectral dynamics distinguish memorizing networks from generalizing\nones, offering a novel perspective on this longstanding conundrum.\nAdditionally, we leverage spectral dynamics to explore the emergence of\nwell-performing sparse subnetworks (lottery tickets) and the structure of the\nloss surface through linear mode connectivity. Our findings suggest that\nspectral dynamics provide a coherent framework to better understand the\nbehavior of neural networks across diverse settings.\n", "link": "http://arxiv.org/abs/2408.11804v1", "date": "2024-08-21", "relevancy": 2.1488, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5543}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5262}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approaching%20Deep%20Learning%20through%20the%20Spectral%20Dynamics%20of%20Weights&body=Title%3A%20Approaching%20Deep%20Learning%20through%20the%20Spectral%20Dynamics%20of%20Weights%0AAuthor%3A%20David%20Yunis%20and%20Kumar%20Kshitij%20Patel%20and%20Samuel%20Wheeler%20and%20Pedro%20Savarese%20and%20Gal%20Vardi%20and%20Karen%20Livescu%20and%20Michael%20Maire%20and%20Matthew%20R.%20Walter%0AAbstract%3A%20%20%20We%20propose%20an%20empirical%20approach%20centered%20on%20the%20spectral%20dynamics%20of%20weights%0A--%20the%20behavior%20of%20singular%20values%20and%20vectors%20during%20optimization%20--%20to%20unify%0Aand%20clarify%20several%20phenomena%20in%20deep%20learning.%20We%20identify%20a%20consistent%20bias%0Ain%20optimization%20across%20various%20experiments%2C%20from%20small-scale%20%60%60grokking%27%27%20to%0Alarge-scale%20tasks%20like%20image%20classification%20with%20ConvNets%2C%20image%20generation%0Awith%20UNets%2C%20speech%20recognition%20with%20LSTMs%2C%20and%20language%20modeling%20with%0ATransformers.%20We%20also%20demonstrate%20that%20weight%20decay%20enhances%20this%20bias%20beyond%0Aits%20role%20as%20a%20norm%20regularizer%2C%20even%20in%20practical%20systems.%20Moreover%2C%20we%20show%0Athat%20these%20spectral%20dynamics%20distinguish%20memorizing%20networks%20from%20generalizing%0Aones%2C%20offering%20a%20novel%20perspective%20on%20this%20longstanding%20conundrum.%0AAdditionally%2C%20we%20leverage%20spectral%20dynamics%20to%20explore%20the%20emergence%20of%0Awell-performing%20sparse%20subnetworks%20%28lottery%20tickets%29%20and%20the%20structure%20of%20the%0Aloss%20surface%20through%20linear%20mode%20connectivity.%20Our%20findings%20suggest%20that%0Aspectral%20dynamics%20provide%20a%20coherent%20framework%20to%20better%20understand%20the%0Abehavior%20of%20neural%20networks%20across%20diverse%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproaching%2520Deep%2520Learning%2520through%2520the%2520Spectral%2520Dynamics%2520of%2520Weights%26entry.906535625%3DDavid%2520Yunis%2520and%2520Kumar%2520Kshitij%2520Patel%2520and%2520Samuel%2520Wheeler%2520and%2520Pedro%2520Savarese%2520and%2520Gal%2520Vardi%2520and%2520Karen%2520Livescu%2520and%2520Michael%2520Maire%2520and%2520Matthew%2520R.%2520Walter%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520empirical%2520approach%2520centered%2520on%2520the%2520spectral%2520dynamics%2520of%2520weights%250A--%2520the%2520behavior%2520of%2520singular%2520values%2520and%2520vectors%2520during%2520optimization%2520--%2520to%2520unify%250Aand%2520clarify%2520several%2520phenomena%2520in%2520deep%2520learning.%2520We%2520identify%2520a%2520consistent%2520bias%250Ain%2520optimization%2520across%2520various%2520experiments%252C%2520from%2520small-scale%2520%2560%2560grokking%2527%2527%2520to%250Alarge-scale%2520tasks%2520like%2520image%2520classification%2520with%2520ConvNets%252C%2520image%2520generation%250Awith%2520UNets%252C%2520speech%2520recognition%2520with%2520LSTMs%252C%2520and%2520language%2520modeling%2520with%250ATransformers.%2520We%2520also%2520demonstrate%2520that%2520weight%2520decay%2520enhances%2520this%2520bias%2520beyond%250Aits%2520role%2520as%2520a%2520norm%2520regularizer%252C%2520even%2520in%2520practical%2520systems.%2520Moreover%252C%2520we%2520show%250Athat%2520these%2520spectral%2520dynamics%2520distinguish%2520memorizing%2520networks%2520from%2520generalizing%250Aones%252C%2520offering%2520a%2520novel%2520perspective%2520on%2520this%2520longstanding%2520conundrum.%250AAdditionally%252C%2520we%2520leverage%2520spectral%2520dynamics%2520to%2520explore%2520the%2520emergence%2520of%250Awell-performing%2520sparse%2520subnetworks%2520%2528lottery%2520tickets%2529%2520and%2520the%2520structure%2520of%2520the%250Aloss%2520surface%2520through%2520linear%2520mode%2520connectivity.%2520Our%2520findings%2520suggest%2520that%250Aspectral%2520dynamics%2520provide%2520a%2520coherent%2520framework%2520to%2520better%2520understand%2520the%250Abehavior%2520of%2520neural%2520networks%2520across%2520diverse%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approaching%20Deep%20Learning%20through%20the%20Spectral%20Dynamics%20of%20Weights&entry.906535625=David%20Yunis%20and%20Kumar%20Kshitij%20Patel%20and%20Samuel%20Wheeler%20and%20Pedro%20Savarese%20and%20Gal%20Vardi%20and%20Karen%20Livescu%20and%20Michael%20Maire%20and%20Matthew%20R.%20Walter&entry.1292438233=%20%20We%20propose%20an%20empirical%20approach%20centered%20on%20the%20spectral%20dynamics%20of%20weights%0A--%20the%20behavior%20of%20singular%20values%20and%20vectors%20during%20optimization%20--%20to%20unify%0Aand%20clarify%20several%20phenomena%20in%20deep%20learning.%20We%20identify%20a%20consistent%20bias%0Ain%20optimization%20across%20various%20experiments%2C%20from%20small-scale%20%60%60grokking%27%27%20to%0Alarge-scale%20tasks%20like%20image%20classification%20with%20ConvNets%2C%20image%20generation%0Awith%20UNets%2C%20speech%20recognition%20with%20LSTMs%2C%20and%20language%20modeling%20with%0ATransformers.%20We%20also%20demonstrate%20that%20weight%20decay%20enhances%20this%20bias%20beyond%0Aits%20role%20as%20a%20norm%20regularizer%2C%20even%20in%20practical%20systems.%20Moreover%2C%20we%20show%0Athat%20these%20spectral%20dynamics%20distinguish%20memorizing%20networks%20from%20generalizing%0Aones%2C%20offering%20a%20novel%20perspective%20on%20this%20longstanding%20conundrum.%0AAdditionally%2C%20we%20leverage%20spectral%20dynamics%20to%20explore%20the%20emergence%20of%0Awell-performing%20sparse%20subnetworks%20%28lottery%20tickets%29%20and%20the%20structure%20of%20the%0Aloss%20surface%20through%20linear%20mode%20connectivity.%20Our%20findings%20suggest%20that%0Aspectral%20dynamics%20provide%20a%20coherent%20framework%20to%20better%20understand%20the%0Abehavior%20of%20neural%20networks%20across%20diverse%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11804v1&entry.124074799=Read"},
{"title": "Self-Supervised Visual Preference Alignment", "author": "Ke Zhu and Zheng Ge and Liang Zhao and Xiangyu Zhang", "abstract": "  This paper makes the first attempt towards unsupervised preference alignment\nin Vision-Language Models (VLMs). We generate chosen and rejected responses\nwith regard to the original and augmented image pairs, and conduct preference\nalignment with direct preference optimization. It is based on a core idea:\nproperly designed augmentation to the image input will induce VLM to generate\nfalse but hard negative responses, which helps the model to learn from and\nproduce more robust and powerful answers. The whole pipeline no longer hinges\non supervision from GPT-4 or human involvement during alignment, and is highly\nefficient with few lines of code. With only 8k randomly sampled unsupervised\ndata, it achieves 90\\% relative score to GPT-4 on complex reasoning in\nLLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex\nmulti-modal benchmark MM-Vet. Visualizations shows its improved ability to\nalign with user-intentions. A series of ablations are firmly conducted to\nreveal the latent mechanism of the approach, which also indicates its potential\ntowards further scaling. Code are available in\nhttps://github.com/Kevinz-code/SeVa.\n", "link": "http://arxiv.org/abs/2404.10501v2", "date": "2024-08-21", "relevancy": 2.1403, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Visual%20Preference%20Alignment&body=Title%3A%20Self-Supervised%20Visual%20Preference%20Alignment%0AAuthor%3A%20Ke%20Zhu%20and%20Zheng%20Ge%20and%20Liang%20Zhao%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20This%20paper%20makes%20the%20first%20attempt%20towards%20unsupervised%20preference%20alignment%0Ain%20Vision-Language%20Models%20%28VLMs%29.%20We%20generate%20chosen%20and%20rejected%20responses%0Awith%20regard%20to%20the%20original%20and%20augmented%20image%20pairs%2C%20and%20conduct%20preference%0Aalignment%20with%20direct%20preference%20optimization.%20It%20is%20based%20on%20a%20core%20idea%3A%0Aproperly%20designed%20augmentation%20to%20the%20image%20input%20will%20induce%20VLM%20to%20generate%0Afalse%20but%20hard%20negative%20responses%2C%20which%20helps%20the%20model%20to%20learn%20from%20and%0Aproduce%20more%20robust%20and%20powerful%20answers.%20The%20whole%20pipeline%20no%20longer%20hinges%0Aon%20supervision%20from%20GPT-4%20or%20human%20involvement%20during%20alignment%2C%20and%20is%20highly%0Aefficient%20with%20few%20lines%20of%20code.%20With%20only%208k%20randomly%20sampled%20unsupervised%0Adata%2C%20it%20achieves%2090%5C%25%20relative%20score%20to%20GPT-4%20on%20complex%20reasoning%20in%0ALLaVA-Bench%2C%20and%20improves%20LLaVA-7B/13B%20by%206.7%5C%25/5.6%5C%25%20score%20on%20complex%0Amulti-modal%20benchmark%20MM-Vet.%20Visualizations%20shows%20its%20improved%20ability%20to%0Aalign%20with%20user-intentions.%20A%20series%20of%20ablations%20are%20firmly%20conducted%20to%0Areveal%20the%20latent%20mechanism%20of%20the%20approach%2C%20which%20also%20indicates%20its%20potential%0Atowards%20further%20scaling.%20Code%20are%20available%20in%0Ahttps%3A//github.com/Kevinz-code/SeVa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Visual%2520Preference%2520Alignment%26entry.906535625%3DKe%2520Zhu%2520and%2520Zheng%2520Ge%2520and%2520Liang%2520Zhao%2520and%2520Xiangyu%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520makes%2520the%2520first%2520attempt%2520towards%2520unsupervised%2520preference%2520alignment%250Ain%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520We%2520generate%2520chosen%2520and%2520rejected%2520responses%250Awith%2520regard%2520to%2520the%2520original%2520and%2520augmented%2520image%2520pairs%252C%2520and%2520conduct%2520preference%250Aalignment%2520with%2520direct%2520preference%2520optimization.%2520It%2520is%2520based%2520on%2520a%2520core%2520idea%253A%250Aproperly%2520designed%2520augmentation%2520to%2520the%2520image%2520input%2520will%2520induce%2520VLM%2520to%2520generate%250Afalse%2520but%2520hard%2520negative%2520responses%252C%2520which%2520helps%2520the%2520model%2520to%2520learn%2520from%2520and%250Aproduce%2520more%2520robust%2520and%2520powerful%2520answers.%2520The%2520whole%2520pipeline%2520no%2520longer%2520hinges%250Aon%2520supervision%2520from%2520GPT-4%2520or%2520human%2520involvement%2520during%2520alignment%252C%2520and%2520is%2520highly%250Aefficient%2520with%2520few%2520lines%2520of%2520code.%2520With%2520only%25208k%2520randomly%2520sampled%2520unsupervised%250Adata%252C%2520it%2520achieves%252090%255C%2525%2520relative%2520score%2520to%2520GPT-4%2520on%2520complex%2520reasoning%2520in%250ALLaVA-Bench%252C%2520and%2520improves%2520LLaVA-7B/13B%2520by%25206.7%255C%2525/5.6%255C%2525%2520score%2520on%2520complex%250Amulti-modal%2520benchmark%2520MM-Vet.%2520Visualizations%2520shows%2520its%2520improved%2520ability%2520to%250Aalign%2520with%2520user-intentions.%2520A%2520series%2520of%2520ablations%2520are%2520firmly%2520conducted%2520to%250Areveal%2520the%2520latent%2520mechanism%2520of%2520the%2520approach%252C%2520which%2520also%2520indicates%2520its%2520potential%250Atowards%2520further%2520scaling.%2520Code%2520are%2520available%2520in%250Ahttps%253A//github.com/Kevinz-code/SeVa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Visual%20Preference%20Alignment&entry.906535625=Ke%20Zhu%20and%20Zheng%20Ge%20and%20Liang%20Zhao%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20This%20paper%20makes%20the%20first%20attempt%20towards%20unsupervised%20preference%20alignment%0Ain%20Vision-Language%20Models%20%28VLMs%29.%20We%20generate%20chosen%20and%20rejected%20responses%0Awith%20regard%20to%20the%20original%20and%20augmented%20image%20pairs%2C%20and%20conduct%20preference%0Aalignment%20with%20direct%20preference%20optimization.%20It%20is%20based%20on%20a%20core%20idea%3A%0Aproperly%20designed%20augmentation%20to%20the%20image%20input%20will%20induce%20VLM%20to%20generate%0Afalse%20but%20hard%20negative%20responses%2C%20which%20helps%20the%20model%20to%20learn%20from%20and%0Aproduce%20more%20robust%20and%20powerful%20answers.%20The%20whole%20pipeline%20no%20longer%20hinges%0Aon%20supervision%20from%20GPT-4%20or%20human%20involvement%20during%20alignment%2C%20and%20is%20highly%0Aefficient%20with%20few%20lines%20of%20code.%20With%20only%208k%20randomly%20sampled%20unsupervised%0Adata%2C%20it%20achieves%2090%5C%25%20relative%20score%20to%20GPT-4%20on%20complex%20reasoning%20in%0ALLaVA-Bench%2C%20and%20improves%20LLaVA-7B/13B%20by%206.7%5C%25/5.6%5C%25%20score%20on%20complex%0Amulti-modal%20benchmark%20MM-Vet.%20Visualizations%20shows%20its%20improved%20ability%20to%0Aalign%20with%20user-intentions.%20A%20series%20of%20ablations%20are%20firmly%20conducted%20to%0Areveal%20the%20latent%20mechanism%20of%20the%20approach%2C%20which%20also%20indicates%20its%20potential%0Atowards%20further%20scaling.%20Code%20are%20available%20in%0Ahttps%3A//github.com/Kevinz-code/SeVa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10501v2&entry.124074799=Read"},
{"title": "CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in\n  Visual Question Answering", "author": "Yuliang Cai and Mohammad Rostami", "abstract": "  Large vision-language models (VLMs) have shown significant performance boost\nin various application domains. However, adopting them to deal with several\nsequentially encountered tasks has been challenging because finetuning a VLM on\na task normally leads to reducing its generalization power and the capacity of\nlearning new tasks as well as causing catastrophic forgetting on previously\nlearned tasks. Enabling using VLMs in multimodal continual learning (CL)\nsettings can help to address such scenarios. To improve generalization capacity\nand prevent catastrophic forgetting, we propose a novel prompt-based CL method\nfor VLMs, namely $\\textbf{Clu}$ster-based $\\textbf{Mo}$dality Fusion Prompt\n(\\textbf{CluMo}). We design a novel \\textbf{Key-Key-Prompt} pair, where each\nprompt is associated with a visual prompt key and a textual prompt key. We\nadopt a two-stage training strategy. During the first stage, the single-modal\nkeys are trained via $K$-means clustering algorithm to help select the best\nsemantically matched prompt. During the second stage, the prompt keys are\nfrozen, the selected prompt is attached to the input for training the VLM in\nthe CL scenario. Experiments on two benchmarks demonstrate that our method\nachieves SOTA performance.\n", "link": "http://arxiv.org/abs/2408.11742v1", "date": "2024-08-21", "relevancy": 2.1331, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5806}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5216}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CluMo%3A%20Cluster-based%20Modality%20Fusion%20Prompt%20for%20Continual%20Learning%20in%0A%20%20Visual%20Question%20Answering&body=Title%3A%20CluMo%3A%20Cluster-based%20Modality%20Fusion%20Prompt%20for%20Continual%20Learning%20in%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Yuliang%20Cai%20and%20Mohammad%20Rostami%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28VLMs%29%20have%20shown%20significant%20performance%20boost%0Ain%20various%20application%20domains.%20However%2C%20adopting%20them%20to%20deal%20with%20several%0Asequentially%20encountered%20tasks%20has%20been%20challenging%20because%20finetuning%20a%20VLM%20on%0Aa%20task%20normally%20leads%20to%20reducing%20its%20generalization%20power%20and%20the%20capacity%20of%0Alearning%20new%20tasks%20as%20well%20as%20causing%20catastrophic%20forgetting%20on%20previously%0Alearned%20tasks.%20Enabling%20using%20VLMs%20in%20multimodal%20continual%20learning%20%28CL%29%0Asettings%20can%20help%20to%20address%20such%20scenarios.%20To%20improve%20generalization%20capacity%0Aand%20prevent%20catastrophic%20forgetting%2C%20we%20propose%20a%20novel%20prompt-based%20CL%20method%0Afor%20VLMs%2C%20namely%20%24%5Ctextbf%7BClu%7D%24ster-based%20%24%5Ctextbf%7BMo%7D%24dality%20Fusion%20Prompt%0A%28%5Ctextbf%7BCluMo%7D%29.%20We%20design%20a%20novel%20%5Ctextbf%7BKey-Key-Prompt%7D%20pair%2C%20where%20each%0Aprompt%20is%20associated%20with%20a%20visual%20prompt%20key%20and%20a%20textual%20prompt%20key.%20We%0Aadopt%20a%20two-stage%20training%20strategy.%20During%20the%20first%20stage%2C%20the%20single-modal%0Akeys%20are%20trained%20via%20%24K%24-means%20clustering%20algorithm%20to%20help%20select%20the%20best%0Asemantically%20matched%20prompt.%20During%20the%20second%20stage%2C%20the%20prompt%20keys%20are%0Afrozen%2C%20the%20selected%20prompt%20is%20attached%20to%20the%20input%20for%20training%20the%20VLM%20in%0Athe%20CL%20scenario.%20Experiments%20on%20two%20benchmarks%20demonstrate%20that%20our%20method%0Aachieves%20SOTA%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluMo%253A%2520Cluster-based%2520Modality%2520Fusion%2520Prompt%2520for%2520Continual%2520Learning%2520in%250A%2520%2520Visual%2520Question%2520Answering%26entry.906535625%3DYuliang%2520Cai%2520and%2520Mohammad%2520Rostami%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520significant%2520performance%2520boost%250Ain%2520various%2520application%2520domains.%2520However%252C%2520adopting%2520them%2520to%2520deal%2520with%2520several%250Asequentially%2520encountered%2520tasks%2520has%2520been%2520challenging%2520because%2520finetuning%2520a%2520VLM%2520on%250Aa%2520task%2520normally%2520leads%2520to%2520reducing%2520its%2520generalization%2520power%2520and%2520the%2520capacity%2520of%250Alearning%2520new%2520tasks%2520as%2520well%2520as%2520causing%2520catastrophic%2520forgetting%2520on%2520previously%250Alearned%2520tasks.%2520Enabling%2520using%2520VLMs%2520in%2520multimodal%2520continual%2520learning%2520%2528CL%2529%250Asettings%2520can%2520help%2520to%2520address%2520such%2520scenarios.%2520To%2520improve%2520generalization%2520capacity%250Aand%2520prevent%2520catastrophic%2520forgetting%252C%2520we%2520propose%2520a%2520novel%2520prompt-based%2520CL%2520method%250Afor%2520VLMs%252C%2520namely%2520%2524%255Ctextbf%257BClu%257D%2524ster-based%2520%2524%255Ctextbf%257BMo%257D%2524dality%2520Fusion%2520Prompt%250A%2528%255Ctextbf%257BCluMo%257D%2529.%2520We%2520design%2520a%2520novel%2520%255Ctextbf%257BKey-Key-Prompt%257D%2520pair%252C%2520where%2520each%250Aprompt%2520is%2520associated%2520with%2520a%2520visual%2520prompt%2520key%2520and%2520a%2520textual%2520prompt%2520key.%2520We%250Aadopt%2520a%2520two-stage%2520training%2520strategy.%2520During%2520the%2520first%2520stage%252C%2520the%2520single-modal%250Akeys%2520are%2520trained%2520via%2520%2524K%2524-means%2520clustering%2520algorithm%2520to%2520help%2520select%2520the%2520best%250Asemantically%2520matched%2520prompt.%2520During%2520the%2520second%2520stage%252C%2520the%2520prompt%2520keys%2520are%250Afrozen%252C%2520the%2520selected%2520prompt%2520is%2520attached%2520to%2520the%2520input%2520for%2520training%2520the%2520VLM%2520in%250Athe%2520CL%2520scenario.%2520Experiments%2520on%2520two%2520benchmarks%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520SOTA%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CluMo%3A%20Cluster-based%20Modality%20Fusion%20Prompt%20for%20Continual%20Learning%20in%0A%20%20Visual%20Question%20Answering&entry.906535625=Yuliang%20Cai%20and%20Mohammad%20Rostami&entry.1292438233=%20%20Large%20vision-language%20models%20%28VLMs%29%20have%20shown%20significant%20performance%20boost%0Ain%20various%20application%20domains.%20However%2C%20adopting%20them%20to%20deal%20with%20several%0Asequentially%20encountered%20tasks%20has%20been%20challenging%20because%20finetuning%20a%20VLM%20on%0Aa%20task%20normally%20leads%20to%20reducing%20its%20generalization%20power%20and%20the%20capacity%20of%0Alearning%20new%20tasks%20as%20well%20as%20causing%20catastrophic%20forgetting%20on%20previously%0Alearned%20tasks.%20Enabling%20using%20VLMs%20in%20multimodal%20continual%20learning%20%28CL%29%0Asettings%20can%20help%20to%20address%20such%20scenarios.%20To%20improve%20generalization%20capacity%0Aand%20prevent%20catastrophic%20forgetting%2C%20we%20propose%20a%20novel%20prompt-based%20CL%20method%0Afor%20VLMs%2C%20namely%20%24%5Ctextbf%7BClu%7D%24ster-based%20%24%5Ctextbf%7BMo%7D%24dality%20Fusion%20Prompt%0A%28%5Ctextbf%7BCluMo%7D%29.%20We%20design%20a%20novel%20%5Ctextbf%7BKey-Key-Prompt%7D%20pair%2C%20where%20each%0Aprompt%20is%20associated%20with%20a%20visual%20prompt%20key%20and%20a%20textual%20prompt%20key.%20We%0Aadopt%20a%20two-stage%20training%20strategy.%20During%20the%20first%20stage%2C%20the%20single-modal%0Akeys%20are%20trained%20via%20%24K%24-means%20clustering%20algorithm%20to%20help%20select%20the%20best%0Asemantically%20matched%20prompt.%20During%20the%20second%20stage%2C%20the%20prompt%20keys%20are%0Afrozen%2C%20the%20selected%20prompt%20is%20attached%20to%20the%20input%20for%20training%20the%20VLM%20in%0Athe%20CL%20scenario.%20Experiments%20on%20two%20benchmarks%20demonstrate%20that%20our%20method%0Aachieves%20SOTA%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11742v1&entry.124074799=Read"},
{"title": "SAM-REF: Rethinking Image-Prompt Synergy for Refinement in Segment\n  Anything", "author": "Chongkai Yu and Anqi Li and Xiaochao Qu and Luoqi Liu and Ting Liu", "abstract": "  The advent of the Segment Anything Model (SAM) marks a significant milestone\nfor interactive segmentation using generalist models. As a late fusion model,\nSAM extracts image embeddings once and merges them with prompts in later\ninteractions. This strategy limits the models ability to extract detailed\ninformation from the prompted target zone. Current specialist models utilize\nthe early fusion strategy that encodes the combination of images and prompts to\ntarget the prompted objects, yet repetitive complex computations on the images\nresult in high latency. The key to these issues is efficiently synergizing the\nimages and prompts. We propose SAM-REF, a two-stage refinement framework that\nfully integrates images and prompts globally and locally while maintaining the\naccuracy of early fusion and the efficiency of late fusion. The first-stage\nGlobalDiff Refiner is a lightweight early fusion network that combines the\nwhole image and prompts, focusing on capturing detailed information for the\nentire object. The second-stage PatchDiff Refiner locates the object detail\nwindow according to the mask and prompts, then refines the local details of the\nobject. Experimentally, we demonstrated the high effectiveness and efficiency\nof our method in tackling complex cases with multiple interactions. Our SAM-REF\nmodel outperforms the current state-of-the-art method in most metrics on\nsegmentation quality without compromising efficiency.\n", "link": "http://arxiv.org/abs/2408.11535v1", "date": "2024-08-21", "relevancy": 2.1249, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-REF%3A%20Rethinking%20Image-Prompt%20Synergy%20for%20Refinement%20in%20Segment%0A%20%20Anything&body=Title%3A%20SAM-REF%3A%20Rethinking%20Image-Prompt%20Synergy%20for%20Refinement%20in%20Segment%0A%20%20Anything%0AAuthor%3A%20Chongkai%20Yu%20and%20Anqi%20Li%20and%20Xiaochao%20Qu%20and%20Luoqi%20Liu%20and%20Ting%20Liu%0AAbstract%3A%20%20%20The%20advent%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20marks%20a%20significant%20milestone%0Afor%20interactive%20segmentation%20using%20generalist%20models.%20As%20a%20late%20fusion%20model%2C%0ASAM%20extracts%20image%20embeddings%20once%20and%20merges%20them%20with%20prompts%20in%20later%0Ainteractions.%20This%20strategy%20limits%20the%20models%20ability%20to%20extract%20detailed%0Ainformation%20from%20the%20prompted%20target%20zone.%20Current%20specialist%20models%20utilize%0Athe%20early%20fusion%20strategy%20that%20encodes%20the%20combination%20of%20images%20and%20prompts%20to%0Atarget%20the%20prompted%20objects%2C%20yet%20repetitive%20complex%20computations%20on%20the%20images%0Aresult%20in%20high%20latency.%20The%20key%20to%20these%20issues%20is%20efficiently%20synergizing%20the%0Aimages%20and%20prompts.%20We%20propose%20SAM-REF%2C%20a%20two-stage%20refinement%20framework%20that%0Afully%20integrates%20images%20and%20prompts%20globally%20and%20locally%20while%20maintaining%20the%0Aaccuracy%20of%20early%20fusion%20and%20the%20efficiency%20of%20late%20fusion.%20The%20first-stage%0AGlobalDiff%20Refiner%20is%20a%20lightweight%20early%20fusion%20network%20that%20combines%20the%0Awhole%20image%20and%20prompts%2C%20focusing%20on%20capturing%20detailed%20information%20for%20the%0Aentire%20object.%20The%20second-stage%20PatchDiff%20Refiner%20locates%20the%20object%20detail%0Awindow%20according%20to%20the%20mask%20and%20prompts%2C%20then%20refines%20the%20local%20details%20of%20the%0Aobject.%20Experimentally%2C%20we%20demonstrated%20the%20high%20effectiveness%20and%20efficiency%0Aof%20our%20method%20in%20tackling%20complex%20cases%20with%20multiple%20interactions.%20Our%20SAM-REF%0Amodel%20outperforms%20the%20current%20state-of-the-art%20method%20in%20most%20metrics%20on%0Asegmentation%20quality%20without%20compromising%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-REF%253A%2520Rethinking%2520Image-Prompt%2520Synergy%2520for%2520Refinement%2520in%2520Segment%250A%2520%2520Anything%26entry.906535625%3DChongkai%2520Yu%2520and%2520Anqi%2520Li%2520and%2520Xiaochao%2520Qu%2520and%2520Luoqi%2520Liu%2520and%2520Ting%2520Liu%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520marks%2520a%2520significant%2520milestone%250Afor%2520interactive%2520segmentation%2520using%2520generalist%2520models.%2520As%2520a%2520late%2520fusion%2520model%252C%250ASAM%2520extracts%2520image%2520embeddings%2520once%2520and%2520merges%2520them%2520with%2520prompts%2520in%2520later%250Ainteractions.%2520This%2520strategy%2520limits%2520the%2520models%2520ability%2520to%2520extract%2520detailed%250Ainformation%2520from%2520the%2520prompted%2520target%2520zone.%2520Current%2520specialist%2520models%2520utilize%250Athe%2520early%2520fusion%2520strategy%2520that%2520encodes%2520the%2520combination%2520of%2520images%2520and%2520prompts%2520to%250Atarget%2520the%2520prompted%2520objects%252C%2520yet%2520repetitive%2520complex%2520computations%2520on%2520the%2520images%250Aresult%2520in%2520high%2520latency.%2520The%2520key%2520to%2520these%2520issues%2520is%2520efficiently%2520synergizing%2520the%250Aimages%2520and%2520prompts.%2520We%2520propose%2520SAM-REF%252C%2520a%2520two-stage%2520refinement%2520framework%2520that%250Afully%2520integrates%2520images%2520and%2520prompts%2520globally%2520and%2520locally%2520while%2520maintaining%2520the%250Aaccuracy%2520of%2520early%2520fusion%2520and%2520the%2520efficiency%2520of%2520late%2520fusion.%2520The%2520first-stage%250AGlobalDiff%2520Refiner%2520is%2520a%2520lightweight%2520early%2520fusion%2520network%2520that%2520combines%2520the%250Awhole%2520image%2520and%2520prompts%252C%2520focusing%2520on%2520capturing%2520detailed%2520information%2520for%2520the%250Aentire%2520object.%2520The%2520second-stage%2520PatchDiff%2520Refiner%2520locates%2520the%2520object%2520detail%250Awindow%2520according%2520to%2520the%2520mask%2520and%2520prompts%252C%2520then%2520refines%2520the%2520local%2520details%2520of%2520the%250Aobject.%2520Experimentally%252C%2520we%2520demonstrated%2520the%2520high%2520effectiveness%2520and%2520efficiency%250Aof%2520our%2520method%2520in%2520tackling%2520complex%2520cases%2520with%2520multiple%2520interactions.%2520Our%2520SAM-REF%250Amodel%2520outperforms%2520the%2520current%2520state-of-the-art%2520method%2520in%2520most%2520metrics%2520on%250Asegmentation%2520quality%2520without%2520compromising%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-REF%3A%20Rethinking%20Image-Prompt%20Synergy%20for%20Refinement%20in%20Segment%0A%20%20Anything&entry.906535625=Chongkai%20Yu%20and%20Anqi%20Li%20and%20Xiaochao%20Qu%20and%20Luoqi%20Liu%20and%20Ting%20Liu&entry.1292438233=%20%20The%20advent%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20marks%20a%20significant%20milestone%0Afor%20interactive%20segmentation%20using%20generalist%20models.%20As%20a%20late%20fusion%20model%2C%0ASAM%20extracts%20image%20embeddings%20once%20and%20merges%20them%20with%20prompts%20in%20later%0Ainteractions.%20This%20strategy%20limits%20the%20models%20ability%20to%20extract%20detailed%0Ainformation%20from%20the%20prompted%20target%20zone.%20Current%20specialist%20models%20utilize%0Athe%20early%20fusion%20strategy%20that%20encodes%20the%20combination%20of%20images%20and%20prompts%20to%0Atarget%20the%20prompted%20objects%2C%20yet%20repetitive%20complex%20computations%20on%20the%20images%0Aresult%20in%20high%20latency.%20The%20key%20to%20these%20issues%20is%20efficiently%20synergizing%20the%0Aimages%20and%20prompts.%20We%20propose%20SAM-REF%2C%20a%20two-stage%20refinement%20framework%20that%0Afully%20integrates%20images%20and%20prompts%20globally%20and%20locally%20while%20maintaining%20the%0Aaccuracy%20of%20early%20fusion%20and%20the%20efficiency%20of%20late%20fusion.%20The%20first-stage%0AGlobalDiff%20Refiner%20is%20a%20lightweight%20early%20fusion%20network%20that%20combines%20the%0Awhole%20image%20and%20prompts%2C%20focusing%20on%20capturing%20detailed%20information%20for%20the%0Aentire%20object.%20The%20second-stage%20PatchDiff%20Refiner%20locates%20the%20object%20detail%0Awindow%20according%20to%20the%20mask%20and%20prompts%2C%20then%20refines%20the%20local%20details%20of%20the%0Aobject.%20Experimentally%2C%20we%20demonstrated%20the%20high%20effectiveness%20and%20efficiency%0Aof%20our%20method%20in%20tackling%20complex%20cases%20with%20multiple%20interactions.%20Our%20SAM-REF%0Amodel%20outperforms%20the%20current%20state-of-the-art%20method%20in%20most%20metrics%20on%0Asegmentation%20quality%20without%20compromising%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11535v1&entry.124074799=Read"},
{"title": "Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision\n  and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring\n  at Intersections", "author": "Ahmed S. Abdelrahman and Mohamed Abdel-Aty and Dongdong Wang", "abstract": "  Computer vision has advanced research methodologies, enhancing system\nservices across various fields. It is a core component in traffic monitoring\nsystems for improving road safety; however, these monitoring systems don't\npreserve the privacy of pedestrians who appear in the videos, potentially\nrevealing their identities. Addressing this issue, our paper introduces\nVideo-to-Text Pedestrian Monitoring (VTPM), which monitors pedestrian movements\nat intersections and generates real-time textual reports, including traffic\nsignal and weather information. VTPM uses computer vision models for pedestrian\ndetection and tracking, achieving a latency of 0.05 seconds per video frame.\nAdditionally, it detects crossing violations with 90.2% accuracy by\nincorporating traffic signal data. The proposed framework is equipped with\nPhi-3 mini-4k to generate real-time textual reports of pedestrian activity\nwhile stating safety concerns like crossing violations, conflicts, and the\nimpact of weather on their behavior with latency of 0.33 seconds. To enhance\ncomprehensive analysis of the generated textual reports, Phi-3 medium is\nfine-tuned for historical analysis of these generated textual reports. This\nfine-tuning enables more reliable analysis about the pedestrian safety at\nintersections, effectively detecting patterns and safety critical events. The\nproposed VTPM offers a more efficient alternative to video footage by using\ntextual reports reducing memory usage, saving up to 253 million percent,\neliminating privacy issues, and enabling comprehensive interactive historical\nanalysis.\n", "link": "http://arxiv.org/abs/2408.11649v1", "date": "2024-08-21", "relevancy": 2.1065, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5398}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5244}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-to-Text%20Pedestrian%20Monitoring%20%28VTPM%29%3A%20Leveraging%20Computer%20Vision%0A%20%20and%20Large%20Language%20Models%20for%20Privacy-Preserve%20Pedestrian%20Activity%20Monitoring%0A%20%20at%20Intersections&body=Title%3A%20Video-to-Text%20Pedestrian%20Monitoring%20%28VTPM%29%3A%20Leveraging%20Computer%20Vision%0A%20%20and%20Large%20Language%20Models%20for%20Privacy-Preserve%20Pedestrian%20Activity%20Monitoring%0A%20%20at%20Intersections%0AAuthor%3A%20Ahmed%20S.%20Abdelrahman%20and%20Mohamed%20Abdel-Aty%20and%20Dongdong%20Wang%0AAbstract%3A%20%20%20Computer%20vision%20has%20advanced%20research%20methodologies%2C%20enhancing%20system%0Aservices%20across%20various%20fields.%20It%20is%20a%20core%20component%20in%20traffic%20monitoring%0Asystems%20for%20improving%20road%20safety%3B%20however%2C%20these%20monitoring%20systems%20don%27t%0Apreserve%20the%20privacy%20of%20pedestrians%20who%20appear%20in%20the%20videos%2C%20potentially%0Arevealing%20their%20identities.%20Addressing%20this%20issue%2C%20our%20paper%20introduces%0AVideo-to-Text%20Pedestrian%20Monitoring%20%28VTPM%29%2C%20which%20monitors%20pedestrian%20movements%0Aat%20intersections%20and%20generates%20real-time%20textual%20reports%2C%20including%20traffic%0Asignal%20and%20weather%20information.%20VTPM%20uses%20computer%20vision%20models%20for%20pedestrian%0Adetection%20and%20tracking%2C%20achieving%20a%20latency%20of%200.05%20seconds%20per%20video%20frame.%0AAdditionally%2C%20it%20detects%20crossing%20violations%20with%2090.2%25%20accuracy%20by%0Aincorporating%20traffic%20signal%20data.%20The%20proposed%20framework%20is%20equipped%20with%0APhi-3%20mini-4k%20to%20generate%20real-time%20textual%20reports%20of%20pedestrian%20activity%0Awhile%20stating%20safety%20concerns%20like%20crossing%20violations%2C%20conflicts%2C%20and%20the%0Aimpact%20of%20weather%20on%20their%20behavior%20with%20latency%20of%200.33%20seconds.%20To%20enhance%0Acomprehensive%20analysis%20of%20the%20generated%20textual%20reports%2C%20Phi-3%20medium%20is%0Afine-tuned%20for%20historical%20analysis%20of%20these%20generated%20textual%20reports.%20This%0Afine-tuning%20enables%20more%20reliable%20analysis%20about%20the%20pedestrian%20safety%20at%0Aintersections%2C%20effectively%20detecting%20patterns%20and%20safety%20critical%20events.%20The%0Aproposed%20VTPM%20offers%20a%20more%20efficient%20alternative%20to%20video%20footage%20by%20using%0Atextual%20reports%20reducing%20memory%20usage%2C%20saving%20up%20to%20253%20million%20percent%2C%0Aeliminating%20privacy%20issues%2C%20and%20enabling%20comprehensive%20interactive%20historical%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-to-Text%2520Pedestrian%2520Monitoring%2520%2528VTPM%2529%253A%2520Leveraging%2520Computer%2520Vision%250A%2520%2520and%2520Large%2520Language%2520Models%2520for%2520Privacy-Preserve%2520Pedestrian%2520Activity%2520Monitoring%250A%2520%2520at%2520Intersections%26entry.906535625%3DAhmed%2520S.%2520Abdelrahman%2520and%2520Mohamed%2520Abdel-Aty%2520and%2520Dongdong%2520Wang%26entry.1292438233%3D%2520%2520Computer%2520vision%2520has%2520advanced%2520research%2520methodologies%252C%2520enhancing%2520system%250Aservices%2520across%2520various%2520fields.%2520It%2520is%2520a%2520core%2520component%2520in%2520traffic%2520monitoring%250Asystems%2520for%2520improving%2520road%2520safety%253B%2520however%252C%2520these%2520monitoring%2520systems%2520don%2527t%250Apreserve%2520the%2520privacy%2520of%2520pedestrians%2520who%2520appear%2520in%2520the%2520videos%252C%2520potentially%250Arevealing%2520their%2520identities.%2520Addressing%2520this%2520issue%252C%2520our%2520paper%2520introduces%250AVideo-to-Text%2520Pedestrian%2520Monitoring%2520%2528VTPM%2529%252C%2520which%2520monitors%2520pedestrian%2520movements%250Aat%2520intersections%2520and%2520generates%2520real-time%2520textual%2520reports%252C%2520including%2520traffic%250Asignal%2520and%2520weather%2520information.%2520VTPM%2520uses%2520computer%2520vision%2520models%2520for%2520pedestrian%250Adetection%2520and%2520tracking%252C%2520achieving%2520a%2520latency%2520of%25200.05%2520seconds%2520per%2520video%2520frame.%250AAdditionally%252C%2520it%2520detects%2520crossing%2520violations%2520with%252090.2%2525%2520accuracy%2520by%250Aincorporating%2520traffic%2520signal%2520data.%2520The%2520proposed%2520framework%2520is%2520equipped%2520with%250APhi-3%2520mini-4k%2520to%2520generate%2520real-time%2520textual%2520reports%2520of%2520pedestrian%2520activity%250Awhile%2520stating%2520safety%2520concerns%2520like%2520crossing%2520violations%252C%2520conflicts%252C%2520and%2520the%250Aimpact%2520of%2520weather%2520on%2520their%2520behavior%2520with%2520latency%2520of%25200.33%2520seconds.%2520To%2520enhance%250Acomprehensive%2520analysis%2520of%2520the%2520generated%2520textual%2520reports%252C%2520Phi-3%2520medium%2520is%250Afine-tuned%2520for%2520historical%2520analysis%2520of%2520these%2520generated%2520textual%2520reports.%2520This%250Afine-tuning%2520enables%2520more%2520reliable%2520analysis%2520about%2520the%2520pedestrian%2520safety%2520at%250Aintersections%252C%2520effectively%2520detecting%2520patterns%2520and%2520safety%2520critical%2520events.%2520The%250Aproposed%2520VTPM%2520offers%2520a%2520more%2520efficient%2520alternative%2520to%2520video%2520footage%2520by%2520using%250Atextual%2520reports%2520reducing%2520memory%2520usage%252C%2520saving%2520up%2520to%2520253%2520million%2520percent%252C%250Aeliminating%2520privacy%2520issues%252C%2520and%2520enabling%2520comprehensive%2520interactive%2520historical%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-to-Text%20Pedestrian%20Monitoring%20%28VTPM%29%3A%20Leveraging%20Computer%20Vision%0A%20%20and%20Large%20Language%20Models%20for%20Privacy-Preserve%20Pedestrian%20Activity%20Monitoring%0A%20%20at%20Intersections&entry.906535625=Ahmed%20S.%20Abdelrahman%20and%20Mohamed%20Abdel-Aty%20and%20Dongdong%20Wang&entry.1292438233=%20%20Computer%20vision%20has%20advanced%20research%20methodologies%2C%20enhancing%20system%0Aservices%20across%20various%20fields.%20It%20is%20a%20core%20component%20in%20traffic%20monitoring%0Asystems%20for%20improving%20road%20safety%3B%20however%2C%20these%20monitoring%20systems%20don%27t%0Apreserve%20the%20privacy%20of%20pedestrians%20who%20appear%20in%20the%20videos%2C%20potentially%0Arevealing%20their%20identities.%20Addressing%20this%20issue%2C%20our%20paper%20introduces%0AVideo-to-Text%20Pedestrian%20Monitoring%20%28VTPM%29%2C%20which%20monitors%20pedestrian%20movements%0Aat%20intersections%20and%20generates%20real-time%20textual%20reports%2C%20including%20traffic%0Asignal%20and%20weather%20information.%20VTPM%20uses%20computer%20vision%20models%20for%20pedestrian%0Adetection%20and%20tracking%2C%20achieving%20a%20latency%20of%200.05%20seconds%20per%20video%20frame.%0AAdditionally%2C%20it%20detects%20crossing%20violations%20with%2090.2%25%20accuracy%20by%0Aincorporating%20traffic%20signal%20data.%20The%20proposed%20framework%20is%20equipped%20with%0APhi-3%20mini-4k%20to%20generate%20real-time%20textual%20reports%20of%20pedestrian%20activity%0Awhile%20stating%20safety%20concerns%20like%20crossing%20violations%2C%20conflicts%2C%20and%20the%0Aimpact%20of%20weather%20on%20their%20behavior%20with%20latency%20of%200.33%20seconds.%20To%20enhance%0Acomprehensive%20analysis%20of%20the%20generated%20textual%20reports%2C%20Phi-3%20medium%20is%0Afine-tuned%20for%20historical%20analysis%20of%20these%20generated%20textual%20reports.%20This%0Afine-tuning%20enables%20more%20reliable%20analysis%20about%20the%20pedestrian%20safety%20at%0Aintersections%2C%20effectively%20detecting%20patterns%20and%20safety%20critical%20events.%20The%0Aproposed%20VTPM%20offers%20a%20more%20efficient%20alternative%20to%20video%20footage%20by%20using%0Atextual%20reports%20reducing%20memory%20usage%2C%20saving%20up%20to%20253%20million%20percent%2C%0Aeliminating%20privacy%20issues%2C%20and%20enabling%20comprehensive%20interactive%20historical%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11649v1&entry.124074799=Read"},
{"title": "Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a\n  DeltaHand", "author": "Zilin Si and Kevin Lee Zhang and Zeynep Temel and Oliver Kroemer", "abstract": "  Dexterous robotic manipulation remains a challenging domain due to its strict\ndemands for precision and robustness on both hardware and software. While\ndexterous robotic hands have demonstrated remarkable capabilities in complex\ntasks, efficiently learning adaptive control policies for hands still presents\na significant hurdle given the high dimensionalities of hands and tasks. To\nbridge this gap, we propose Tilde, an imitation learning-based in-hand\nmanipulation system on a dexterous DeltaHand. It leverages 1) a low-cost,\nconfigurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a\nuser-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an\nefficient and generalizable imitation learning approach with diffusion\npolicies. Our proposed TeleHand has a kinematic twin design to the DeltaHand\nthat enables precise one-to-one joint control of the DeltaHand during\nteleoperation. This facilitates efficient high-quality data collection of human\ndemonstrations in the real world. To evaluate the effectiveness of our system,\nwe demonstrate the fully autonomous closed-loop deployment of diffusion\npolicies learned from demonstrations across seven dexterous manipulation tasks\nwith an average 90% success rate.\n", "link": "http://arxiv.org/abs/2405.18804v2", "date": "2024-08-21", "relevancy": 2.1029, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5507}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5117}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tilde%3A%20Teleoperation%20for%20Dexterous%20In-Hand%20Manipulation%20Learning%20with%20a%0A%20%20DeltaHand&body=Title%3A%20Tilde%3A%20Teleoperation%20for%20Dexterous%20In-Hand%20Manipulation%20Learning%20with%20a%0A%20%20DeltaHand%0AAuthor%3A%20Zilin%20Si%20and%20Kevin%20Lee%20Zhang%20and%20Zeynep%20Temel%20and%20Oliver%20Kroemer%0AAbstract%3A%20%20%20Dexterous%20robotic%20manipulation%20remains%20a%20challenging%20domain%20due%20to%20its%20strict%0Ademands%20for%20precision%20and%20robustness%20on%20both%20hardware%20and%20software.%20While%0Adexterous%20robotic%20hands%20have%20demonstrated%20remarkable%20capabilities%20in%20complex%0Atasks%2C%20efficiently%20learning%20adaptive%20control%20policies%20for%20hands%20still%20presents%0Aa%20significant%20hurdle%20given%20the%20high%20dimensionalities%20of%20hands%20and%20tasks.%20To%0Abridge%20this%20gap%2C%20we%20propose%20Tilde%2C%20an%20imitation%20learning-based%20in-hand%0Amanipulation%20system%20on%20a%20dexterous%20DeltaHand.%20It%20leverages%201%29%20a%20low-cost%2C%0Aconfigurable%2C%20simple-to-control%2C%20soft%20dexterous%20robotic%20hand%2C%20DeltaHand%2C%202%29%20a%0Auser-friendly%2C%20precise%2C%20real-time%20teleoperation%20interface%2C%20TeleHand%2C%20and%203%29%20an%0Aefficient%20and%20generalizable%20imitation%20learning%20approach%20with%20diffusion%0Apolicies.%20Our%20proposed%20TeleHand%20has%20a%20kinematic%20twin%20design%20to%20the%20DeltaHand%0Athat%20enables%20precise%20one-to-one%20joint%20control%20of%20the%20DeltaHand%20during%0Ateleoperation.%20This%20facilitates%20efficient%20high-quality%20data%20collection%20of%20human%0Ademonstrations%20in%20the%20real%20world.%20To%20evaluate%20the%20effectiveness%20of%20our%20system%2C%0Awe%20demonstrate%20the%20fully%20autonomous%20closed-loop%20deployment%20of%20diffusion%0Apolicies%20learned%20from%20demonstrations%20across%20seven%20dexterous%20manipulation%20tasks%0Awith%20an%20average%2090%25%20success%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18804v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTilde%253A%2520Teleoperation%2520for%2520Dexterous%2520In-Hand%2520Manipulation%2520Learning%2520with%2520a%250A%2520%2520DeltaHand%26entry.906535625%3DZilin%2520Si%2520and%2520Kevin%2520Lee%2520Zhang%2520and%2520Zeynep%2520Temel%2520and%2520Oliver%2520Kroemer%26entry.1292438233%3D%2520%2520Dexterous%2520robotic%2520manipulation%2520remains%2520a%2520challenging%2520domain%2520due%2520to%2520its%2520strict%250Ademands%2520for%2520precision%2520and%2520robustness%2520on%2520both%2520hardware%2520and%2520software.%2520While%250Adexterous%2520robotic%2520hands%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520complex%250Atasks%252C%2520efficiently%2520learning%2520adaptive%2520control%2520policies%2520for%2520hands%2520still%2520presents%250Aa%2520significant%2520hurdle%2520given%2520the%2520high%2520dimensionalities%2520of%2520hands%2520and%2520tasks.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520Tilde%252C%2520an%2520imitation%2520learning-based%2520in-hand%250Amanipulation%2520system%2520on%2520a%2520dexterous%2520DeltaHand.%2520It%2520leverages%25201%2529%2520a%2520low-cost%252C%250Aconfigurable%252C%2520simple-to-control%252C%2520soft%2520dexterous%2520robotic%2520hand%252C%2520DeltaHand%252C%25202%2529%2520a%250Auser-friendly%252C%2520precise%252C%2520real-time%2520teleoperation%2520interface%252C%2520TeleHand%252C%2520and%25203%2529%2520an%250Aefficient%2520and%2520generalizable%2520imitation%2520learning%2520approach%2520with%2520diffusion%250Apolicies.%2520Our%2520proposed%2520TeleHand%2520has%2520a%2520kinematic%2520twin%2520design%2520to%2520the%2520DeltaHand%250Athat%2520enables%2520precise%2520one-to-one%2520joint%2520control%2520of%2520the%2520DeltaHand%2520during%250Ateleoperation.%2520This%2520facilitates%2520efficient%2520high-quality%2520data%2520collection%2520of%2520human%250Ademonstrations%2520in%2520the%2520real%2520world.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520system%252C%250Awe%2520demonstrate%2520the%2520fully%2520autonomous%2520closed-loop%2520deployment%2520of%2520diffusion%250Apolicies%2520learned%2520from%2520demonstrations%2520across%2520seven%2520dexterous%2520manipulation%2520tasks%250Awith%2520an%2520average%252090%2525%2520success%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18804v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tilde%3A%20Teleoperation%20for%20Dexterous%20In-Hand%20Manipulation%20Learning%20with%20a%0A%20%20DeltaHand&entry.906535625=Zilin%20Si%20and%20Kevin%20Lee%20Zhang%20and%20Zeynep%20Temel%20and%20Oliver%20Kroemer&entry.1292438233=%20%20Dexterous%20robotic%20manipulation%20remains%20a%20challenging%20domain%20due%20to%20its%20strict%0Ademands%20for%20precision%20and%20robustness%20on%20both%20hardware%20and%20software.%20While%0Adexterous%20robotic%20hands%20have%20demonstrated%20remarkable%20capabilities%20in%20complex%0Atasks%2C%20efficiently%20learning%20adaptive%20control%20policies%20for%20hands%20still%20presents%0Aa%20significant%20hurdle%20given%20the%20high%20dimensionalities%20of%20hands%20and%20tasks.%20To%0Abridge%20this%20gap%2C%20we%20propose%20Tilde%2C%20an%20imitation%20learning-based%20in-hand%0Amanipulation%20system%20on%20a%20dexterous%20DeltaHand.%20It%20leverages%201%29%20a%20low-cost%2C%0Aconfigurable%2C%20simple-to-control%2C%20soft%20dexterous%20robotic%20hand%2C%20DeltaHand%2C%202%29%20a%0Auser-friendly%2C%20precise%2C%20real-time%20teleoperation%20interface%2C%20TeleHand%2C%20and%203%29%20an%0Aefficient%20and%20generalizable%20imitation%20learning%20approach%20with%20diffusion%0Apolicies.%20Our%20proposed%20TeleHand%20has%20a%20kinematic%20twin%20design%20to%20the%20DeltaHand%0Athat%20enables%20precise%20one-to-one%20joint%20control%20of%20the%20DeltaHand%20during%0Ateleoperation.%20This%20facilitates%20efficient%20high-quality%20data%20collection%20of%20human%0Ademonstrations%20in%20the%20real%20world.%20To%20evaluate%20the%20effectiveness%20of%20our%20system%2C%0Awe%20demonstrate%20the%20fully%20autonomous%20closed-loop%20deployment%20of%20diffusion%0Apolicies%20learned%20from%20demonstrations%20across%20seven%20dexterous%20manipulation%20tasks%0Awith%20an%20average%2090%25%20success%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18804v2&entry.124074799=Read"},
{"title": "SOAP: Enhancing Spatio-Temporal Relation and Motion Information\n  Capturing for Few-Shot Action Recognition", "author": "Wenbo Huang and Jinghui Zhang and Xuwei Qian and Zhen Wu and Meng Wang and Lei Zhang", "abstract": "  High frame-rate (HFR) videos of action recognition improve fine-grained\nexpression while reducing the spatio-temporal relation and motion information\ndensity. Thus, large amounts of video samples are continuously required for\ntraditional data-driven training. However, samples are not always sufficient in\nreal-world scenarios, promoting few-shot action recognition (FSAR) research. We\nobserve that most recent FSAR works build spatio-temporal relation of video\nsamples via temporal alignment after spatial feature extraction, cutting apart\nspatial and temporal features within samples. They also capture motion\ninformation via narrow perspectives between adjacent frames without considering\ndensity, leading to insufficient motion information capturing. Therefore, we\npropose a novel plug-and-play architecture for FSAR called Spatio-tempOral\nfrAme tuPle enhancer (SOAP) in this paper. The model we designed with such\narchitecture refers to SOAP-Net. Temporal connections between different feature\nchannels and spatio-temporal relation of features are considered instead of\nsimple feature extraction. Comprehensive motion information is also captured,\nusing frame tuples with multiple frames containing more motion information than\nadjacent frames. Combining frame tuples of diverse frame counts further\nprovides a broader perspective. SOAP-Net achieves new state-of-the-art\nperformance across well-known benchmarks such as SthSthV2, Kinetics, UCF101,\nand HMDB51. Extensive empirical evaluations underscore the competitiveness,\npluggability, generalization, and robustness of SOAP. The code is released at\nhttps://github.com/wenbohuang1002/SOAP.\n", "link": "http://arxiv.org/abs/2407.16344v3", "date": "2024-08-21", "relevancy": 2.1024, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5388}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5184}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOAP%3A%20Enhancing%20Spatio-Temporal%20Relation%20and%20Motion%20Information%0A%20%20Capturing%20for%20Few-Shot%20Action%20Recognition&body=Title%3A%20SOAP%3A%20Enhancing%20Spatio-Temporal%20Relation%20and%20Motion%20Information%0A%20%20Capturing%20for%20Few-Shot%20Action%20Recognition%0AAuthor%3A%20Wenbo%20Huang%20and%20Jinghui%20Zhang%20and%20Xuwei%20Qian%20and%20Zhen%20Wu%20and%20Meng%20Wang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20High%20frame-rate%20%28HFR%29%20videos%20of%20action%20recognition%20improve%20fine-grained%0Aexpression%20while%20reducing%20the%20spatio-temporal%20relation%20and%20motion%20information%0Adensity.%20Thus%2C%20large%20amounts%20of%20video%20samples%20are%20continuously%20required%20for%0Atraditional%20data-driven%20training.%20However%2C%20samples%20are%20not%20always%20sufficient%20in%0Areal-world%20scenarios%2C%20promoting%20few-shot%20action%20recognition%20%28FSAR%29%20research.%20We%0Aobserve%20that%20most%20recent%20FSAR%20works%20build%20spatio-temporal%20relation%20of%20video%0Asamples%20via%20temporal%20alignment%20after%20spatial%20feature%20extraction%2C%20cutting%20apart%0Aspatial%20and%20temporal%20features%20within%20samples.%20They%20also%20capture%20motion%0Ainformation%20via%20narrow%20perspectives%20between%20adjacent%20frames%20without%20considering%0Adensity%2C%20leading%20to%20insufficient%20motion%20information%20capturing.%20Therefore%2C%20we%0Apropose%20a%20novel%20plug-and-play%20architecture%20for%20FSAR%20called%20Spatio-tempOral%0AfrAme%20tuPle%20enhancer%20%28SOAP%29%20in%20this%20paper.%20The%20model%20we%20designed%20with%20such%0Aarchitecture%20refers%20to%20SOAP-Net.%20Temporal%20connections%20between%20different%20feature%0Achannels%20and%20spatio-temporal%20relation%20of%20features%20are%20considered%20instead%20of%0Asimple%20feature%20extraction.%20Comprehensive%20motion%20information%20is%20also%20captured%2C%0Ausing%20frame%20tuples%20with%20multiple%20frames%20containing%20more%20motion%20information%20than%0Aadjacent%20frames.%20Combining%20frame%20tuples%20of%20diverse%20frame%20counts%20further%0Aprovides%20a%20broader%20perspective.%20SOAP-Net%20achieves%20new%20state-of-the-art%0Aperformance%20across%20well-known%20benchmarks%20such%20as%20SthSthV2%2C%20Kinetics%2C%20UCF101%2C%0Aand%20HMDB51.%20Extensive%20empirical%20evaluations%20underscore%20the%20competitiveness%2C%0Apluggability%2C%20generalization%2C%20and%20robustness%20of%20SOAP.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/wenbohuang1002/SOAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOAP%253A%2520Enhancing%2520Spatio-Temporal%2520Relation%2520and%2520Motion%2520Information%250A%2520%2520Capturing%2520for%2520Few-Shot%2520Action%2520Recognition%26entry.906535625%3DWenbo%2520Huang%2520and%2520Jinghui%2520Zhang%2520and%2520Xuwei%2520Qian%2520and%2520Zhen%2520Wu%2520and%2520Meng%2520Wang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520High%2520frame-rate%2520%2528HFR%2529%2520videos%2520of%2520action%2520recognition%2520improve%2520fine-grained%250Aexpression%2520while%2520reducing%2520the%2520spatio-temporal%2520relation%2520and%2520motion%2520information%250Adensity.%2520Thus%252C%2520large%2520amounts%2520of%2520video%2520samples%2520are%2520continuously%2520required%2520for%250Atraditional%2520data-driven%2520training.%2520However%252C%2520samples%2520are%2520not%2520always%2520sufficient%2520in%250Areal-world%2520scenarios%252C%2520promoting%2520few-shot%2520action%2520recognition%2520%2528FSAR%2529%2520research.%2520We%250Aobserve%2520that%2520most%2520recent%2520FSAR%2520works%2520build%2520spatio-temporal%2520relation%2520of%2520video%250Asamples%2520via%2520temporal%2520alignment%2520after%2520spatial%2520feature%2520extraction%252C%2520cutting%2520apart%250Aspatial%2520and%2520temporal%2520features%2520within%2520samples.%2520They%2520also%2520capture%2520motion%250Ainformation%2520via%2520narrow%2520perspectives%2520between%2520adjacent%2520frames%2520without%2520considering%250Adensity%252C%2520leading%2520to%2520insufficient%2520motion%2520information%2520capturing.%2520Therefore%252C%2520we%250Apropose%2520a%2520novel%2520plug-and-play%2520architecture%2520for%2520FSAR%2520called%2520Spatio-tempOral%250AfrAme%2520tuPle%2520enhancer%2520%2528SOAP%2529%2520in%2520this%2520paper.%2520The%2520model%2520we%2520designed%2520with%2520such%250Aarchitecture%2520refers%2520to%2520SOAP-Net.%2520Temporal%2520connections%2520between%2520different%2520feature%250Achannels%2520and%2520spatio-temporal%2520relation%2520of%2520features%2520are%2520considered%2520instead%2520of%250Asimple%2520feature%2520extraction.%2520Comprehensive%2520motion%2520information%2520is%2520also%2520captured%252C%250Ausing%2520frame%2520tuples%2520with%2520multiple%2520frames%2520containing%2520more%2520motion%2520information%2520than%250Aadjacent%2520frames.%2520Combining%2520frame%2520tuples%2520of%2520diverse%2520frame%2520counts%2520further%250Aprovides%2520a%2520broader%2520perspective.%2520SOAP-Net%2520achieves%2520new%2520state-of-the-art%250Aperformance%2520across%2520well-known%2520benchmarks%2520such%2520as%2520SthSthV2%252C%2520Kinetics%252C%2520UCF101%252C%250Aand%2520HMDB51.%2520Extensive%2520empirical%2520evaluations%2520underscore%2520the%2520competitiveness%252C%250Apluggability%252C%2520generalization%252C%2520and%2520robustness%2520of%2520SOAP.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/wenbohuang1002/SOAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOAP%3A%20Enhancing%20Spatio-Temporal%20Relation%20and%20Motion%20Information%0A%20%20Capturing%20for%20Few-Shot%20Action%20Recognition&entry.906535625=Wenbo%20Huang%20and%20Jinghui%20Zhang%20and%20Xuwei%20Qian%20and%20Zhen%20Wu%20and%20Meng%20Wang%20and%20Lei%20Zhang&entry.1292438233=%20%20High%20frame-rate%20%28HFR%29%20videos%20of%20action%20recognition%20improve%20fine-grained%0Aexpression%20while%20reducing%20the%20spatio-temporal%20relation%20and%20motion%20information%0Adensity.%20Thus%2C%20large%20amounts%20of%20video%20samples%20are%20continuously%20required%20for%0Atraditional%20data-driven%20training.%20However%2C%20samples%20are%20not%20always%20sufficient%20in%0Areal-world%20scenarios%2C%20promoting%20few-shot%20action%20recognition%20%28FSAR%29%20research.%20We%0Aobserve%20that%20most%20recent%20FSAR%20works%20build%20spatio-temporal%20relation%20of%20video%0Asamples%20via%20temporal%20alignment%20after%20spatial%20feature%20extraction%2C%20cutting%20apart%0Aspatial%20and%20temporal%20features%20within%20samples.%20They%20also%20capture%20motion%0Ainformation%20via%20narrow%20perspectives%20between%20adjacent%20frames%20without%20considering%0Adensity%2C%20leading%20to%20insufficient%20motion%20information%20capturing.%20Therefore%2C%20we%0Apropose%20a%20novel%20plug-and-play%20architecture%20for%20FSAR%20called%20Spatio-tempOral%0AfrAme%20tuPle%20enhancer%20%28SOAP%29%20in%20this%20paper.%20The%20model%20we%20designed%20with%20such%0Aarchitecture%20refers%20to%20SOAP-Net.%20Temporal%20connections%20between%20different%20feature%0Achannels%20and%20spatio-temporal%20relation%20of%20features%20are%20considered%20instead%20of%0Asimple%20feature%20extraction.%20Comprehensive%20motion%20information%20is%20also%20captured%2C%0Ausing%20frame%20tuples%20with%20multiple%20frames%20containing%20more%20motion%20information%20than%0Aadjacent%20frames.%20Combining%20frame%20tuples%20of%20diverse%20frame%20counts%20further%0Aprovides%20a%20broader%20perspective.%20SOAP-Net%20achieves%20new%20state-of-the-art%0Aperformance%20across%20well-known%20benchmarks%20such%20as%20SthSthV2%2C%20Kinetics%2C%20UCF101%2C%0Aand%20HMDB51.%20Extensive%20empirical%20evaluations%20underscore%20the%20competitiveness%2C%0Apluggability%2C%20generalization%2C%20and%20robustness%20of%20SOAP.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/wenbohuang1002/SOAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16344v3&entry.124074799=Read"},
{"title": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for\n  Transformer Pretraining", "author": "Pihe Hu and Shaolong Li and Longbo Huang", "abstract": "  Large language models (LLMs) have made significant strides in complex tasks,\nyet their widespread adoption is impeded by substantial computational demands.\nWith hundreds of billion parameters, transformer-based LLMs necessitate months\nof pretraining across a high-end GPU cluster. However, this paper reveals a\ncompelling finding: transformers exhibit considerable redundancy in pretraining\ncomputations, which motivates our proposed solution, Mixed Sparsity Training\n(MST), an efficient pretraining method that can reduce about $75\\%$ of Floating\nPoint Operations (FLOPs) while maintaining performance. MST integrates dynamic\nsparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention\n(HSA) during pretraining, involving three distinct phases: warm-up,\nultra-sparsification, and restoration. The warm-up phase transforms the dense\nmodel into a sparse one, and the restoration phase reinstates connections.\nThroughout these phases, the model is trained with a dynamically evolving\nsparse topology and an HSA mechanism to maintain performance and minimize\ntraining FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reduction\nof $4\\times$ without compromising performance.\n", "link": "http://arxiv.org/abs/2408.11746v1", "date": "2024-08-21", "relevancy": 2.1006, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.553}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5411}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixed%20Sparsity%20Training%3A%20Achieving%204%24%5Ctimes%24%20FLOP%20Reduction%20for%0A%20%20Transformer%20Pretraining&body=Title%3A%20Mixed%20Sparsity%20Training%3A%20Achieving%204%24%5Ctimes%24%20FLOP%20Reduction%20for%0A%20%20Transformer%20Pretraining%0AAuthor%3A%20Pihe%20Hu%20and%20Shaolong%20Li%20and%20Longbo%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20complex%20tasks%2C%0Ayet%20their%20widespread%20adoption%20is%20impeded%20by%20substantial%20computational%20demands.%0AWith%20hundreds%20of%20billion%20parameters%2C%20transformer-based%20LLMs%20necessitate%20months%0Aof%20pretraining%20across%20a%20high-end%20GPU%20cluster.%20However%2C%20this%20paper%20reveals%20a%0Acompelling%20finding%3A%20transformers%20exhibit%20considerable%20redundancy%20in%20pretraining%0Acomputations%2C%20which%20motivates%20our%20proposed%20solution%2C%20Mixed%20Sparsity%20Training%0A%28MST%29%2C%20an%20efficient%20pretraining%20method%20that%20can%20reduce%20about%20%2475%5C%25%24%20of%20Floating%0APoint%20Operations%20%28FLOPs%29%20while%20maintaining%20performance.%20MST%20integrates%20dynamic%0Asparse%20training%20%28DST%29%20with%20Sparsity%20Variation%20%28SV%29%20and%20Hybrid%20Sparse%20Attention%0A%28HSA%29%20during%20pretraining%2C%20involving%20three%20distinct%20phases%3A%20warm-up%2C%0Aultra-sparsification%2C%20and%20restoration.%20The%20warm-up%20phase%20transforms%20the%20dense%0Amodel%20into%20a%20sparse%20one%2C%20and%20the%20restoration%20phase%20reinstates%20connections.%0AThroughout%20these%20phases%2C%20the%20model%20is%20trained%20with%20a%20dynamically%20evolving%0Asparse%20topology%20and%20an%20HSA%20mechanism%20to%20maintain%20performance%20and%20minimize%0Atraining%20FLOPs%20concurrently.%20Our%20experiment%20on%20GPT-2%20showcases%20a%20FLOP%20reduction%0Aof%20%244%5Ctimes%24%20without%20compromising%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixed%2520Sparsity%2520Training%253A%2520Achieving%25204%2524%255Ctimes%2524%2520FLOP%2520Reduction%2520for%250A%2520%2520Transformer%2520Pretraining%26entry.906535625%3DPihe%2520Hu%2520and%2520Shaolong%2520Li%2520and%2520Longbo%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520strides%2520in%2520complex%2520tasks%252C%250Ayet%2520their%2520widespread%2520adoption%2520is%2520impeded%2520by%2520substantial%2520computational%2520demands.%250AWith%2520hundreds%2520of%2520billion%2520parameters%252C%2520transformer-based%2520LLMs%2520necessitate%2520months%250Aof%2520pretraining%2520across%2520a%2520high-end%2520GPU%2520cluster.%2520However%252C%2520this%2520paper%2520reveals%2520a%250Acompelling%2520finding%253A%2520transformers%2520exhibit%2520considerable%2520redundancy%2520in%2520pretraining%250Acomputations%252C%2520which%2520motivates%2520our%2520proposed%2520solution%252C%2520Mixed%2520Sparsity%2520Training%250A%2528MST%2529%252C%2520an%2520efficient%2520pretraining%2520method%2520that%2520can%2520reduce%2520about%2520%252475%255C%2525%2524%2520of%2520Floating%250APoint%2520Operations%2520%2528FLOPs%2529%2520while%2520maintaining%2520performance.%2520MST%2520integrates%2520dynamic%250Asparse%2520training%2520%2528DST%2529%2520with%2520Sparsity%2520Variation%2520%2528SV%2529%2520and%2520Hybrid%2520Sparse%2520Attention%250A%2528HSA%2529%2520during%2520pretraining%252C%2520involving%2520three%2520distinct%2520phases%253A%2520warm-up%252C%250Aultra-sparsification%252C%2520and%2520restoration.%2520The%2520warm-up%2520phase%2520transforms%2520the%2520dense%250Amodel%2520into%2520a%2520sparse%2520one%252C%2520and%2520the%2520restoration%2520phase%2520reinstates%2520connections.%250AThroughout%2520these%2520phases%252C%2520the%2520model%2520is%2520trained%2520with%2520a%2520dynamically%2520evolving%250Asparse%2520topology%2520and%2520an%2520HSA%2520mechanism%2520to%2520maintain%2520performance%2520and%2520minimize%250Atraining%2520FLOPs%2520concurrently.%2520Our%2520experiment%2520on%2520GPT-2%2520showcases%2520a%2520FLOP%2520reduction%250Aof%2520%25244%255Ctimes%2524%2520without%2520compromising%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed%20Sparsity%20Training%3A%20Achieving%204%24%5Ctimes%24%20FLOP%20Reduction%20for%0A%20%20Transformer%20Pretraining&entry.906535625=Pihe%20Hu%20and%20Shaolong%20Li%20and%20Longbo%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20complex%20tasks%2C%0Ayet%20their%20widespread%20adoption%20is%20impeded%20by%20substantial%20computational%20demands.%0AWith%20hundreds%20of%20billion%20parameters%2C%20transformer-based%20LLMs%20necessitate%20months%0Aof%20pretraining%20across%20a%20high-end%20GPU%20cluster.%20However%2C%20this%20paper%20reveals%20a%0Acompelling%20finding%3A%20transformers%20exhibit%20considerable%20redundancy%20in%20pretraining%0Acomputations%2C%20which%20motivates%20our%20proposed%20solution%2C%20Mixed%20Sparsity%20Training%0A%28MST%29%2C%20an%20efficient%20pretraining%20method%20that%20can%20reduce%20about%20%2475%5C%25%24%20of%20Floating%0APoint%20Operations%20%28FLOPs%29%20while%20maintaining%20performance.%20MST%20integrates%20dynamic%0Asparse%20training%20%28DST%29%20with%20Sparsity%20Variation%20%28SV%29%20and%20Hybrid%20Sparse%20Attention%0A%28HSA%29%20during%20pretraining%2C%20involving%20three%20distinct%20phases%3A%20warm-up%2C%0Aultra-sparsification%2C%20and%20restoration.%20The%20warm-up%20phase%20transforms%20the%20dense%0Amodel%20into%20a%20sparse%20one%2C%20and%20the%20restoration%20phase%20reinstates%20connections.%0AThroughout%20these%20phases%2C%20the%20model%20is%20trained%20with%20a%20dynamically%20evolving%0Asparse%20topology%20and%20an%20HSA%20mechanism%20to%20maintain%20performance%20and%20minimize%0Atraining%20FLOPs%20concurrently.%20Our%20experiment%20on%20GPT-2%20showcases%20a%20FLOP%20reduction%0Aof%20%244%5Ctimes%24%20without%20compromising%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11746v1&entry.124074799=Read"},
{"title": "Improving global awareness of linkset predictions using Cross-Attentive\n  Modulation tokens", "author": "F\u00e9lix Marcoccia and C\u00e9dric Adjih and Paul M\u00fchlethaler", "abstract": "  Most of multiple link prediction or graph generation techniques rely on the\nattention mechanism or on Graph Neural Networks (GNNs), which consist in\nleveraging node-level information exchanges in order to form proper link\npredictions. Such node-level interactions do not process nodes as an ordered\nsequence, which would imply some kind of natural ordering of the nodes: they\nare said to be permutation invariant mechanisms. They are well suited for graph\nproblems, but struggle at providing a global orchestration of the predicted\nlinks, which can result in a loss of performance. Some typical issues can be\nthe difficulty to ensure high-level properties such as global connectedness,\nfixed diameter or to avoid information bottleneck effects such as oversmoothing\nand oversquashing, which respectively consist in abundant smoothing in dense\nareas leading to a loss of information and a tendency to exclude isolated nodes\nfrom the message passing scheme, and often result in irrelevant, unbalanced\nlink predictions. To tackle this problem, we hereby present Cross-Attentive\nModulation (CAM) tokens, which introduce cross-attentive units used to\ncondition node and edge-level modulations in order to enable context-aware\ncomputations that improve the global consistency of the prediction links. We\nwill implement it on a few permutation invariant architectures, and showcase\nbenchmarks that prove the merits of our work.\n", "link": "http://arxiv.org/abs/2405.19375v3", "date": "2024-08-21", "relevancy": 2.0783, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5348}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20global%20awareness%20of%20linkset%20predictions%20using%20Cross-Attentive%0A%20%20Modulation%20tokens&body=Title%3A%20Improving%20global%20awareness%20of%20linkset%20predictions%20using%20Cross-Attentive%0A%20%20Modulation%20tokens%0AAuthor%3A%20F%C3%A9lix%20Marcoccia%20and%20C%C3%A9dric%20Adjih%20and%20Paul%20M%C3%BChlethaler%0AAbstract%3A%20%20%20Most%20of%20multiple%20link%20prediction%20or%20graph%20generation%20techniques%20rely%20on%20the%0Aattention%20mechanism%20or%20on%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20consist%20in%0Aleveraging%20node-level%20information%20exchanges%20in%20order%20to%20form%20proper%20link%0Apredictions.%20Such%20node-level%20interactions%20do%20not%20process%20nodes%20as%20an%20ordered%0Asequence%2C%20which%20would%20imply%20some%20kind%20of%20natural%20ordering%20of%20the%20nodes%3A%20they%0Aare%20said%20to%20be%20permutation%20invariant%20mechanisms.%20They%20are%20well%20suited%20for%20graph%0Aproblems%2C%20but%20struggle%20at%20providing%20a%20global%20orchestration%20of%20the%20predicted%0Alinks%2C%20which%20can%20result%20in%20a%20loss%20of%20performance.%20Some%20typical%20issues%20can%20be%0Athe%20difficulty%20to%20ensure%20high-level%20properties%20such%20as%20global%20connectedness%2C%0Afixed%20diameter%20or%20to%20avoid%20information%20bottleneck%20effects%20such%20as%20oversmoothing%0Aand%20oversquashing%2C%20which%20respectively%20consist%20in%20abundant%20smoothing%20in%20dense%0Aareas%20leading%20to%20a%20loss%20of%20information%20and%20a%20tendency%20to%20exclude%20isolated%20nodes%0Afrom%20the%20message%20passing%20scheme%2C%20and%20often%20result%20in%20irrelevant%2C%20unbalanced%0Alink%20predictions.%20To%20tackle%20this%20problem%2C%20we%20hereby%20present%20Cross-Attentive%0AModulation%20%28CAM%29%20tokens%2C%20which%20introduce%20cross-attentive%20units%20used%20to%0Acondition%20node%20and%20edge-level%20modulations%20in%20order%20to%20enable%20context-aware%0Acomputations%20that%20improve%20the%20global%20consistency%20of%20the%20prediction%20links.%20We%0Awill%20implement%20it%20on%20a%20few%20permutation%20invariant%20architectures%2C%20and%20showcase%0Abenchmarks%20that%20prove%20the%20merits%20of%20our%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19375v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520global%2520awareness%2520of%2520linkset%2520predictions%2520using%2520Cross-Attentive%250A%2520%2520Modulation%2520tokens%26entry.906535625%3DF%25C3%25A9lix%2520Marcoccia%2520and%2520C%25C3%25A9dric%2520Adjih%2520and%2520Paul%2520M%25C3%25BChlethaler%26entry.1292438233%3D%2520%2520Most%2520of%2520multiple%2520link%2520prediction%2520or%2520graph%2520generation%2520techniques%2520rely%2520on%2520the%250Aattention%2520mechanism%2520or%2520on%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520which%2520consist%2520in%250Aleveraging%2520node-level%2520information%2520exchanges%2520in%2520order%2520to%2520form%2520proper%2520link%250Apredictions.%2520Such%2520node-level%2520interactions%2520do%2520not%2520process%2520nodes%2520as%2520an%2520ordered%250Asequence%252C%2520which%2520would%2520imply%2520some%2520kind%2520of%2520natural%2520ordering%2520of%2520the%2520nodes%253A%2520they%250Aare%2520said%2520to%2520be%2520permutation%2520invariant%2520mechanisms.%2520They%2520are%2520well%2520suited%2520for%2520graph%250Aproblems%252C%2520but%2520struggle%2520at%2520providing%2520a%2520global%2520orchestration%2520of%2520the%2520predicted%250Alinks%252C%2520which%2520can%2520result%2520in%2520a%2520loss%2520of%2520performance.%2520Some%2520typical%2520issues%2520can%2520be%250Athe%2520difficulty%2520to%2520ensure%2520high-level%2520properties%2520such%2520as%2520global%2520connectedness%252C%250Afixed%2520diameter%2520or%2520to%2520avoid%2520information%2520bottleneck%2520effects%2520such%2520as%2520oversmoothing%250Aand%2520oversquashing%252C%2520which%2520respectively%2520consist%2520in%2520abundant%2520smoothing%2520in%2520dense%250Aareas%2520leading%2520to%2520a%2520loss%2520of%2520information%2520and%2520a%2520tendency%2520to%2520exclude%2520isolated%2520nodes%250Afrom%2520the%2520message%2520passing%2520scheme%252C%2520and%2520often%2520result%2520in%2520irrelevant%252C%2520unbalanced%250Alink%2520predictions.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520hereby%2520present%2520Cross-Attentive%250AModulation%2520%2528CAM%2529%2520tokens%252C%2520which%2520introduce%2520cross-attentive%2520units%2520used%2520to%250Acondition%2520node%2520and%2520edge-level%2520modulations%2520in%2520order%2520to%2520enable%2520context-aware%250Acomputations%2520that%2520improve%2520the%2520global%2520consistency%2520of%2520the%2520prediction%2520links.%2520We%250Awill%2520implement%2520it%2520on%2520a%2520few%2520permutation%2520invariant%2520architectures%252C%2520and%2520showcase%250Abenchmarks%2520that%2520prove%2520the%2520merits%2520of%2520our%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19375v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20global%20awareness%20of%20linkset%20predictions%20using%20Cross-Attentive%0A%20%20Modulation%20tokens&entry.906535625=F%C3%A9lix%20Marcoccia%20and%20C%C3%A9dric%20Adjih%20and%20Paul%20M%C3%BChlethaler&entry.1292438233=%20%20Most%20of%20multiple%20link%20prediction%20or%20graph%20generation%20techniques%20rely%20on%20the%0Aattention%20mechanism%20or%20on%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20consist%20in%0Aleveraging%20node-level%20information%20exchanges%20in%20order%20to%20form%20proper%20link%0Apredictions.%20Such%20node-level%20interactions%20do%20not%20process%20nodes%20as%20an%20ordered%0Asequence%2C%20which%20would%20imply%20some%20kind%20of%20natural%20ordering%20of%20the%20nodes%3A%20they%0Aare%20said%20to%20be%20permutation%20invariant%20mechanisms.%20They%20are%20well%20suited%20for%20graph%0Aproblems%2C%20but%20struggle%20at%20providing%20a%20global%20orchestration%20of%20the%20predicted%0Alinks%2C%20which%20can%20result%20in%20a%20loss%20of%20performance.%20Some%20typical%20issues%20can%20be%0Athe%20difficulty%20to%20ensure%20high-level%20properties%20such%20as%20global%20connectedness%2C%0Afixed%20diameter%20or%20to%20avoid%20information%20bottleneck%20effects%20such%20as%20oversmoothing%0Aand%20oversquashing%2C%20which%20respectively%20consist%20in%20abundant%20smoothing%20in%20dense%0Aareas%20leading%20to%20a%20loss%20of%20information%20and%20a%20tendency%20to%20exclude%20isolated%20nodes%0Afrom%20the%20message%20passing%20scheme%2C%20and%20often%20result%20in%20irrelevant%2C%20unbalanced%0Alink%20predictions.%20To%20tackle%20this%20problem%2C%20we%20hereby%20present%20Cross-Attentive%0AModulation%20%28CAM%29%20tokens%2C%20which%20introduce%20cross-attentive%20units%20used%20to%0Acondition%20node%20and%20edge-level%20modulations%20in%20order%20to%20enable%20context-aware%0Acomputations%20that%20improve%20the%20global%20consistency%20of%20the%20prediction%20links.%20We%0Awill%20implement%20it%20on%20a%20few%20permutation%20invariant%20architectures%2C%20and%20showcase%0Abenchmarks%20that%20prove%20the%20merits%20of%20our%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19375v3&entry.124074799=Read"},
{"title": "Interpretable Long-term Action Quality Assessment", "author": "Xu Dong and Xinran Liu and Wanqing Li and Anthony Adeyemi-Ejeye and Andrew Gilbert", "abstract": "  Long-term Action Quality Assessment (AQA) evaluates the execution of\nactivities in videos. However, the length presents challenges in fine-grained\ninterpretability, with current AQA methods typically producing a single score\nby averaging clip features, lacking detailed semantic meanings of individual\nclips. Long-term videos pose additional difficulty due to the complexity and\ndiversity of actions, exacerbating interpretability challenges. While\nquery-based transformer networks offer promising long-term modeling\ncapabilities, their interpretability in AQA remains unsatisfactory due to a\nphenomenon we term Temporal Skipping, where the model skips self-attention\nlayers to prevent output degradation. To address this, we propose an attention\nloss function and a query initialization method to enhance performance and\ninterpretability. Additionally, we introduce a weight-score regression module\ndesigned to approximate the scoring patterns observed in human judgments and\nreplace conventional single-score regression, improving the rationality of\ninterpretability. Our approach achieves state-of-the-art results on three\nreal-world, long-term AQA benchmarks. Our code is available at:\nhttps://github.com/dx199771/Interpretability-AQA\n", "link": "http://arxiv.org/abs/2408.11687v1", "date": "2024-08-21", "relevancy": 2.0757, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5486}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5206}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Long-term%20Action%20Quality%20Assessment&body=Title%3A%20Interpretable%20Long-term%20Action%20Quality%20Assessment%0AAuthor%3A%20Xu%20Dong%20and%20Xinran%20Liu%20and%20Wanqing%20Li%20and%20Anthony%20Adeyemi-Ejeye%20and%20Andrew%20Gilbert%0AAbstract%3A%20%20%20Long-term%20Action%20Quality%20Assessment%20%28AQA%29%20evaluates%20the%20execution%20of%0Aactivities%20in%20videos.%20However%2C%20the%20length%20presents%20challenges%20in%20fine-grained%0Ainterpretability%2C%20with%20current%20AQA%20methods%20typically%20producing%20a%20single%20score%0Aby%20averaging%20clip%20features%2C%20lacking%20detailed%20semantic%20meanings%20of%20individual%0Aclips.%20Long-term%20videos%20pose%20additional%20difficulty%20due%20to%20the%20complexity%20and%0Adiversity%20of%20actions%2C%20exacerbating%20interpretability%20challenges.%20While%0Aquery-based%20transformer%20networks%20offer%20promising%20long-term%20modeling%0Acapabilities%2C%20their%20interpretability%20in%20AQA%20remains%20unsatisfactory%20due%20to%20a%0Aphenomenon%20we%20term%20Temporal%20Skipping%2C%20where%20the%20model%20skips%20self-attention%0Alayers%20to%20prevent%20output%20degradation.%20To%20address%20this%2C%20we%20propose%20an%20attention%0Aloss%20function%20and%20a%20query%20initialization%20method%20to%20enhance%20performance%20and%0Ainterpretability.%20Additionally%2C%20we%20introduce%20a%20weight-score%20regression%20module%0Adesigned%20to%20approximate%20the%20scoring%20patterns%20observed%20in%20human%20judgments%20and%0Areplace%20conventional%20single-score%20regression%2C%20improving%20the%20rationality%20of%0Ainterpretability.%20Our%20approach%20achieves%20state-of-the-art%20results%20on%20three%0Areal-world%2C%20long-term%20AQA%20benchmarks.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/dx199771/Interpretability-AQA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Long-term%2520Action%2520Quality%2520Assessment%26entry.906535625%3DXu%2520Dong%2520and%2520Xinran%2520Liu%2520and%2520Wanqing%2520Li%2520and%2520Anthony%2520Adeyemi-Ejeye%2520and%2520Andrew%2520Gilbert%26entry.1292438233%3D%2520%2520Long-term%2520Action%2520Quality%2520Assessment%2520%2528AQA%2529%2520evaluates%2520the%2520execution%2520of%250Aactivities%2520in%2520videos.%2520However%252C%2520the%2520length%2520presents%2520challenges%2520in%2520fine-grained%250Ainterpretability%252C%2520with%2520current%2520AQA%2520methods%2520typically%2520producing%2520a%2520single%2520score%250Aby%2520averaging%2520clip%2520features%252C%2520lacking%2520detailed%2520semantic%2520meanings%2520of%2520individual%250Aclips.%2520Long-term%2520videos%2520pose%2520additional%2520difficulty%2520due%2520to%2520the%2520complexity%2520and%250Adiversity%2520of%2520actions%252C%2520exacerbating%2520interpretability%2520challenges.%2520While%250Aquery-based%2520transformer%2520networks%2520offer%2520promising%2520long-term%2520modeling%250Acapabilities%252C%2520their%2520interpretability%2520in%2520AQA%2520remains%2520unsatisfactory%2520due%2520to%2520a%250Aphenomenon%2520we%2520term%2520Temporal%2520Skipping%252C%2520where%2520the%2520model%2520skips%2520self-attention%250Alayers%2520to%2520prevent%2520output%2520degradation.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520attention%250Aloss%2520function%2520and%2520a%2520query%2520initialization%2520method%2520to%2520enhance%2520performance%2520and%250Ainterpretability.%2520Additionally%252C%2520we%2520introduce%2520a%2520weight-score%2520regression%2520module%250Adesigned%2520to%2520approximate%2520the%2520scoring%2520patterns%2520observed%2520in%2520human%2520judgments%2520and%250Areplace%2520conventional%2520single-score%2520regression%252C%2520improving%2520the%2520rationality%2520of%250Ainterpretability.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520results%2520on%2520three%250Areal-world%252C%2520long-term%2520AQA%2520benchmarks.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/dx199771/Interpretability-AQA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Long-term%20Action%20Quality%20Assessment&entry.906535625=Xu%20Dong%20and%20Xinran%20Liu%20and%20Wanqing%20Li%20and%20Anthony%20Adeyemi-Ejeye%20and%20Andrew%20Gilbert&entry.1292438233=%20%20Long-term%20Action%20Quality%20Assessment%20%28AQA%29%20evaluates%20the%20execution%20of%0Aactivities%20in%20videos.%20However%2C%20the%20length%20presents%20challenges%20in%20fine-grained%0Ainterpretability%2C%20with%20current%20AQA%20methods%20typically%20producing%20a%20single%20score%0Aby%20averaging%20clip%20features%2C%20lacking%20detailed%20semantic%20meanings%20of%20individual%0Aclips.%20Long-term%20videos%20pose%20additional%20difficulty%20due%20to%20the%20complexity%20and%0Adiversity%20of%20actions%2C%20exacerbating%20interpretability%20challenges.%20While%0Aquery-based%20transformer%20networks%20offer%20promising%20long-term%20modeling%0Acapabilities%2C%20their%20interpretability%20in%20AQA%20remains%20unsatisfactory%20due%20to%20a%0Aphenomenon%20we%20term%20Temporal%20Skipping%2C%20where%20the%20model%20skips%20self-attention%0Alayers%20to%20prevent%20output%20degradation.%20To%20address%20this%2C%20we%20propose%20an%20attention%0Aloss%20function%20and%20a%20query%20initialization%20method%20to%20enhance%20performance%20and%0Ainterpretability.%20Additionally%2C%20we%20introduce%20a%20weight-score%20regression%20module%0Adesigned%20to%20approximate%20the%20scoring%20patterns%20observed%20in%20human%20judgments%20and%0Areplace%20conventional%20single-score%20regression%2C%20improving%20the%20rationality%20of%0Ainterpretability.%20Our%20approach%20achieves%20state-of-the-art%20results%20on%20three%0Areal-world%2C%20long-term%20AQA%20benchmarks.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/dx199771/Interpretability-AQA%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11687v1&entry.124074799=Read"},
{"title": "V-RoAst: A New Dataset for Visual Road Assessment", "author": "Natchapon Jongwiriyanurak and Zichao Zeng and June Moh Goo and Xinglei Wang and Ilya Ilyankou and Kerkritt Srirrongvikrai and Meihui Wang and James Haworth", "abstract": "  Road traffic crashes cause millions of deaths annually and have a significant\neconomic impact, particularly in low- and middle-income countries (LMICs). This\npaper presents an approach using Vision Language Models (VLMs) for road safety\nassessment, overcoming the limitations of traditional Convolutional Neural\nNetworks (CNNs). We introduce a new task ,V-RoAst (Visual question answering\nfor Road Assessment), with a real-world dataset. Our approach optimizes prompt\nengineering and evaluates advanced VLMs, including Gemini-1.5-flash and\nGPT-4o-mini. The models effectively examine attributes for road assessment.\nUsing crowdsourced imagery from Mapillary, our scalable solution influentially\nestimates road safety levels. In addition, this approach is designed for local\nstakeholders who lack resources, as it does not require training data. It\noffers a cost-effective and automated methods for global road safety\nassessments, potentially saving lives and reducing economic burdens.\n", "link": "http://arxiv.org/abs/2408.10872v2", "date": "2024-08-21", "relevancy": 2.0634, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5398}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5057}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-RoAst%3A%20A%20New%20Dataset%20for%20Visual%20Road%20Assessment&body=Title%3A%20V-RoAst%3A%20A%20New%20Dataset%20for%20Visual%20Road%20Assessment%0AAuthor%3A%20Natchapon%20Jongwiriyanurak%20and%20Zichao%20Zeng%20and%20June%20Moh%20Goo%20and%20Xinglei%20Wang%20and%20Ilya%20Ilyankou%20and%20Kerkritt%20Srirrongvikrai%20and%20Meihui%20Wang%20and%20James%20Haworth%0AAbstract%3A%20%20%20Road%20traffic%20crashes%20cause%20millions%20of%20deaths%20annually%20and%20have%20a%20significant%0Aeconomic%20impact%2C%20particularly%20in%20low-%20and%20middle-income%20countries%20%28LMICs%29.%20This%0Apaper%20presents%20an%20approach%20using%20Vision%20Language%20Models%20%28VLMs%29%20for%20road%20safety%0Aassessment%2C%20overcoming%20the%20limitations%20of%20traditional%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20We%20introduce%20a%20new%20task%20%2CV-RoAst%20%28Visual%20question%20answering%0Afor%20Road%20Assessment%29%2C%20with%20a%20real-world%20dataset.%20Our%20approach%20optimizes%20prompt%0Aengineering%20and%20evaluates%20advanced%20VLMs%2C%20including%20Gemini-1.5-flash%20and%0AGPT-4o-mini.%20The%20models%20effectively%20examine%20attributes%20for%20road%20assessment.%0AUsing%20crowdsourced%20imagery%20from%20Mapillary%2C%20our%20scalable%20solution%20influentially%0Aestimates%20road%20safety%20levels.%20In%20addition%2C%20this%20approach%20is%20designed%20for%20local%0Astakeholders%20who%20lack%20resources%2C%20as%20it%20does%20not%20require%20training%20data.%20It%0Aoffers%20a%20cost-effective%20and%20automated%20methods%20for%20global%20road%20safety%0Aassessments%2C%20potentially%20saving%20lives%20and%20reducing%20economic%20burdens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10872v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-RoAst%253A%2520A%2520New%2520Dataset%2520for%2520Visual%2520Road%2520Assessment%26entry.906535625%3DNatchapon%2520Jongwiriyanurak%2520and%2520Zichao%2520Zeng%2520and%2520June%2520Moh%2520Goo%2520and%2520Xinglei%2520Wang%2520and%2520Ilya%2520Ilyankou%2520and%2520Kerkritt%2520Srirrongvikrai%2520and%2520Meihui%2520Wang%2520and%2520James%2520Haworth%26entry.1292438233%3D%2520%2520Road%2520traffic%2520crashes%2520cause%2520millions%2520of%2520deaths%2520annually%2520and%2520have%2520a%2520significant%250Aeconomic%2520impact%252C%2520particularly%2520in%2520low-%2520and%2520middle-income%2520countries%2520%2528LMICs%2529.%2520This%250Apaper%2520presents%2520an%2520approach%2520using%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520for%2520road%2520safety%250Aassessment%252C%2520overcoming%2520the%2520limitations%2520of%2520traditional%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529.%2520We%2520introduce%2520a%2520new%2520task%2520%252CV-RoAst%2520%2528Visual%2520question%2520answering%250Afor%2520Road%2520Assessment%2529%252C%2520with%2520a%2520real-world%2520dataset.%2520Our%2520approach%2520optimizes%2520prompt%250Aengineering%2520and%2520evaluates%2520advanced%2520VLMs%252C%2520including%2520Gemini-1.5-flash%2520and%250AGPT-4o-mini.%2520The%2520models%2520effectively%2520examine%2520attributes%2520for%2520road%2520assessment.%250AUsing%2520crowdsourced%2520imagery%2520from%2520Mapillary%252C%2520our%2520scalable%2520solution%2520influentially%250Aestimates%2520road%2520safety%2520levels.%2520In%2520addition%252C%2520this%2520approach%2520is%2520designed%2520for%2520local%250Astakeholders%2520who%2520lack%2520resources%252C%2520as%2520it%2520does%2520not%2520require%2520training%2520data.%2520It%250Aoffers%2520a%2520cost-effective%2520and%2520automated%2520methods%2520for%2520global%2520road%2520safety%250Aassessments%252C%2520potentially%2520saving%2520lives%2520and%2520reducing%2520economic%2520burdens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10872v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-RoAst%3A%20A%20New%20Dataset%20for%20Visual%20Road%20Assessment&entry.906535625=Natchapon%20Jongwiriyanurak%20and%20Zichao%20Zeng%20and%20June%20Moh%20Goo%20and%20Xinglei%20Wang%20and%20Ilya%20Ilyankou%20and%20Kerkritt%20Srirrongvikrai%20and%20Meihui%20Wang%20and%20James%20Haworth&entry.1292438233=%20%20Road%20traffic%20crashes%20cause%20millions%20of%20deaths%20annually%20and%20have%20a%20significant%0Aeconomic%20impact%2C%20particularly%20in%20low-%20and%20middle-income%20countries%20%28LMICs%29.%20This%0Apaper%20presents%20an%20approach%20using%20Vision%20Language%20Models%20%28VLMs%29%20for%20road%20safety%0Aassessment%2C%20overcoming%20the%20limitations%20of%20traditional%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20We%20introduce%20a%20new%20task%20%2CV-RoAst%20%28Visual%20question%20answering%0Afor%20Road%20Assessment%29%2C%20with%20a%20real-world%20dataset.%20Our%20approach%20optimizes%20prompt%0Aengineering%20and%20evaluates%20advanced%20VLMs%2C%20including%20Gemini-1.5-flash%20and%0AGPT-4o-mini.%20The%20models%20effectively%20examine%20attributes%20for%20road%20assessment.%0AUsing%20crowdsourced%20imagery%20from%20Mapillary%2C%20our%20scalable%20solution%20influentially%0Aestimates%20road%20safety%20levels.%20In%20addition%2C%20this%20approach%20is%20designed%20for%20local%0Astakeholders%20who%20lack%20resources%2C%20as%20it%20does%20not%20require%20training%20data.%20It%0Aoffers%20a%20cost-effective%20and%20automated%20methods%20for%20global%20road%20safety%0Aassessments%2C%20potentially%20saving%20lives%20and%20reducing%20economic%20burdens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10872v2&entry.124074799=Read"},
{"title": "Edge AI as a Service with Coordinated Deep Neural Networks", "author": "Alireza Maleki and Hamed Shah-Mansouri and Babak H. Khalaj", "abstract": "  As artificial intelligence (AI) applications continue to expand in\nnext-generation networks, there is a growing need for deep neural network (DNN)\nmodels. Although DNN models deployed at the edge are promising for providing AI\nas a service with low latency, their cooperation is yet to be explored. In this\npaper, we consider that DNN service providers share their computing resources\nas well as their models' parameters and allow other DNNs to offload their\ncomputations without mirroring. We propose a novel algorithm called coordinated\nDNNs on edge (\\textbf{CoDE}) that facilitates coordination among DNN services\nby establishing new inference paths. CoDE aims to find the optimal path, which\nis the path with the highest possible reward, by creating multi-task DNNs from\nindividual models. The reward reflects the inference throughput and model\naccuracy. With CoDE, DNN models can make new paths for inference by using their\nown or other models' parameters. We then evaluate the performance of CoDE\nthrough numerical experiments. The results demonstrate a $40\\%$ increase in the\ninference throughput while degrading the average accuracy by only $2.3\\%$.\nExperiments show that CoDE enhances the inference throughput and, achieves\nhigher precision compared to a state-of-the-art existing method.\n", "link": "http://arxiv.org/abs/2401.00631v2", "date": "2024-08-21", "relevancy": 2.062, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5715}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4899}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge%20AI%20as%20a%20Service%20with%20Coordinated%20Deep%20Neural%20Networks&body=Title%3A%20Edge%20AI%20as%20a%20Service%20with%20Coordinated%20Deep%20Neural%20Networks%0AAuthor%3A%20Alireza%20Maleki%20and%20Hamed%20Shah-Mansouri%20and%20Babak%20H.%20Khalaj%0AAbstract%3A%20%20%20As%20artificial%20intelligence%20%28AI%29%20applications%20continue%20to%20expand%20in%0Anext-generation%20networks%2C%20there%20is%20a%20growing%20need%20for%20deep%20neural%20network%20%28DNN%29%0Amodels.%20Although%20DNN%20models%20deployed%20at%20the%20edge%20are%20promising%20for%20providing%20AI%0Aas%20a%20service%20with%20low%20latency%2C%20their%20cooperation%20is%20yet%20to%20be%20explored.%20In%20this%0Apaper%2C%20we%20consider%20that%20DNN%20service%20providers%20share%20their%20computing%20resources%0Aas%20well%20as%20their%20models%27%20parameters%20and%20allow%20other%20DNNs%20to%20offload%20their%0Acomputations%20without%20mirroring.%20We%20propose%20a%20novel%20algorithm%20called%20coordinated%0ADNNs%20on%20edge%20%28%5Ctextbf%7BCoDE%7D%29%20that%20facilitates%20coordination%20among%20DNN%20services%0Aby%20establishing%20new%20inference%20paths.%20CoDE%20aims%20to%20find%20the%20optimal%20path%2C%20which%0Ais%20the%20path%20with%20the%20highest%20possible%20reward%2C%20by%20creating%20multi-task%20DNNs%20from%0Aindividual%20models.%20The%20reward%20reflects%20the%20inference%20throughput%20and%20model%0Aaccuracy.%20With%20CoDE%2C%20DNN%20models%20can%20make%20new%20paths%20for%20inference%20by%20using%20their%0Aown%20or%20other%20models%27%20parameters.%20We%20then%20evaluate%20the%20performance%20of%20CoDE%0Athrough%20numerical%20experiments.%20The%20results%20demonstrate%20a%20%2440%5C%25%24%20increase%20in%20the%0Ainference%20throughput%20while%20degrading%20the%20average%20accuracy%20by%20only%20%242.3%5C%25%24.%0AExperiments%20show%20that%20CoDE%20enhances%20the%20inference%20throughput%20and%2C%20achieves%0Ahigher%20precision%20compared%20to%20a%20state-of-the-art%20existing%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge%2520AI%2520as%2520a%2520Service%2520with%2520Coordinated%2520Deep%2520Neural%2520Networks%26entry.906535625%3DAlireza%2520Maleki%2520and%2520Hamed%2520Shah-Mansouri%2520and%2520Babak%2520H.%2520Khalaj%26entry.1292438233%3D%2520%2520As%2520artificial%2520intelligence%2520%2528AI%2529%2520applications%2520continue%2520to%2520expand%2520in%250Anext-generation%2520networks%252C%2520there%2520is%2520a%2520growing%2520need%2520for%2520deep%2520neural%2520network%2520%2528DNN%2529%250Amodels.%2520Although%2520DNN%2520models%2520deployed%2520at%2520the%2520edge%2520are%2520promising%2520for%2520providing%2520AI%250Aas%2520a%2520service%2520with%2520low%2520latency%252C%2520their%2520cooperation%2520is%2520yet%2520to%2520be%2520explored.%2520In%2520this%250Apaper%252C%2520we%2520consider%2520that%2520DNN%2520service%2520providers%2520share%2520their%2520computing%2520resources%250Aas%2520well%2520as%2520their%2520models%2527%2520parameters%2520and%2520allow%2520other%2520DNNs%2520to%2520offload%2520their%250Acomputations%2520without%2520mirroring.%2520We%2520propose%2520a%2520novel%2520algorithm%2520called%2520coordinated%250ADNNs%2520on%2520edge%2520%2528%255Ctextbf%257BCoDE%257D%2529%2520that%2520facilitates%2520coordination%2520among%2520DNN%2520services%250Aby%2520establishing%2520new%2520inference%2520paths.%2520CoDE%2520aims%2520to%2520find%2520the%2520optimal%2520path%252C%2520which%250Ais%2520the%2520path%2520with%2520the%2520highest%2520possible%2520reward%252C%2520by%2520creating%2520multi-task%2520DNNs%2520from%250Aindividual%2520models.%2520The%2520reward%2520reflects%2520the%2520inference%2520throughput%2520and%2520model%250Aaccuracy.%2520With%2520CoDE%252C%2520DNN%2520models%2520can%2520make%2520new%2520paths%2520for%2520inference%2520by%2520using%2520their%250Aown%2520or%2520other%2520models%2527%2520parameters.%2520We%2520then%2520evaluate%2520the%2520performance%2520of%2520CoDE%250Athrough%2520numerical%2520experiments.%2520The%2520results%2520demonstrate%2520a%2520%252440%255C%2525%2524%2520increase%2520in%2520the%250Ainference%2520throughput%2520while%2520degrading%2520the%2520average%2520accuracy%2520by%2520only%2520%25242.3%255C%2525%2524.%250AExperiments%2520show%2520that%2520CoDE%2520enhances%2520the%2520inference%2520throughput%2520and%252C%2520achieves%250Ahigher%2520precision%2520compared%2520to%2520a%2520state-of-the-art%2520existing%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20AI%20as%20a%20Service%20with%20Coordinated%20Deep%20Neural%20Networks&entry.906535625=Alireza%20Maleki%20and%20Hamed%20Shah-Mansouri%20and%20Babak%20H.%20Khalaj&entry.1292438233=%20%20As%20artificial%20intelligence%20%28AI%29%20applications%20continue%20to%20expand%20in%0Anext-generation%20networks%2C%20there%20is%20a%20growing%20need%20for%20deep%20neural%20network%20%28DNN%29%0Amodels.%20Although%20DNN%20models%20deployed%20at%20the%20edge%20are%20promising%20for%20providing%20AI%0Aas%20a%20service%20with%20low%20latency%2C%20their%20cooperation%20is%20yet%20to%20be%20explored.%20In%20this%0Apaper%2C%20we%20consider%20that%20DNN%20service%20providers%20share%20their%20computing%20resources%0Aas%20well%20as%20their%20models%27%20parameters%20and%20allow%20other%20DNNs%20to%20offload%20their%0Acomputations%20without%20mirroring.%20We%20propose%20a%20novel%20algorithm%20called%20coordinated%0ADNNs%20on%20edge%20%28%5Ctextbf%7BCoDE%7D%29%20that%20facilitates%20coordination%20among%20DNN%20services%0Aby%20establishing%20new%20inference%20paths.%20CoDE%20aims%20to%20find%20the%20optimal%20path%2C%20which%0Ais%20the%20path%20with%20the%20highest%20possible%20reward%2C%20by%20creating%20multi-task%20DNNs%20from%0Aindividual%20models.%20The%20reward%20reflects%20the%20inference%20throughput%20and%20model%0Aaccuracy.%20With%20CoDE%2C%20DNN%20models%20can%20make%20new%20paths%20for%20inference%20by%20using%20their%0Aown%20or%20other%20models%27%20parameters.%20We%20then%20evaluate%20the%20performance%20of%20CoDE%0Athrough%20numerical%20experiments.%20The%20results%20demonstrate%20a%20%2440%5C%25%24%20increase%20in%20the%0Ainference%20throughput%20while%20degrading%20the%20average%20accuracy%20by%20only%20%242.3%5C%25%24.%0AExperiments%20show%20that%20CoDE%20enhances%20the%20inference%20throughput%20and%2C%20achieves%0Ahigher%20precision%20compared%20to%20a%20state-of-the-art%20existing%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00631v2&entry.124074799=Read"},
{"title": "Online state vector reduction during model predictive control with\n  gradient-based trajectory optimisation", "author": "David Russell and Rafael Papallas and Mehmet Dogar", "abstract": "  Non-prehensile manipulation in high-dimensional systems is challenging for a\nvariety of reasons, one of the main reasons is the computationally long\nplanning times that come with a large state space. Trajectory optimisation\nalgorithms have proved their utility in a wide variety of tasks, but, like most\nmethods struggle scaling to the high dimensional systems ubiquitous to\nnon-prehensile manipulation in clutter as well as deformable object\nmanipulation. We reason that, during manipulation, different degrees of freedom\nwill become more or less important to the task over time as the system evolves.\nWe leverage this idea to reduce the number of degrees of freedom considered in\na trajectory optimisation problem, to reduce planning times. This idea is\nparticularly relevant in the context of model predictive control (MPC) where\nthe cost landscape of the optimisation problem is constantly evolving. We\nprovide simulation results under asynchronous MPC and show our methods are\ncapable of achieving better overall performance due to the decreased policy lag\nwhilst still being able to optimise trajectories effectively.\n", "link": "http://arxiv.org/abs/2408.11665v1", "date": "2024-08-21", "relevancy": 2.0619, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5588}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5265}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20state%20vector%20reduction%20during%20model%20predictive%20control%20with%0A%20%20gradient-based%20trajectory%20optimisation&body=Title%3A%20Online%20state%20vector%20reduction%20during%20model%20predictive%20control%20with%0A%20%20gradient-based%20trajectory%20optimisation%0AAuthor%3A%20David%20Russell%20and%20Rafael%20Papallas%20and%20Mehmet%20Dogar%0AAbstract%3A%20%20%20Non-prehensile%20manipulation%20in%20high-dimensional%20systems%20is%20challenging%20for%20a%0Avariety%20of%20reasons%2C%20one%20of%20the%20main%20reasons%20is%20the%20computationally%20long%0Aplanning%20times%20that%20come%20with%20a%20large%20state%20space.%20Trajectory%20optimisation%0Aalgorithms%20have%20proved%20their%20utility%20in%20a%20wide%20variety%20of%20tasks%2C%20but%2C%20like%20most%0Amethods%20struggle%20scaling%20to%20the%20high%20dimensional%20systems%20ubiquitous%20to%0Anon-prehensile%20manipulation%20in%20clutter%20as%20well%20as%20deformable%20object%0Amanipulation.%20We%20reason%20that%2C%20during%20manipulation%2C%20different%20degrees%20of%20freedom%0Awill%20become%20more%20or%20less%20important%20to%20the%20task%20over%20time%20as%20the%20system%20evolves.%0AWe%20leverage%20this%20idea%20to%20reduce%20the%20number%20of%20degrees%20of%20freedom%20considered%20in%0Aa%20trajectory%20optimisation%20problem%2C%20to%20reduce%20planning%20times.%20This%20idea%20is%0Aparticularly%20relevant%20in%20the%20context%20of%20model%20predictive%20control%20%28MPC%29%20where%0Athe%20cost%20landscape%20of%20the%20optimisation%20problem%20is%20constantly%20evolving.%20We%0Aprovide%20simulation%20results%20under%20asynchronous%20MPC%20and%20show%20our%20methods%20are%0Acapable%20of%20achieving%20better%20overall%20performance%20due%20to%20the%20decreased%20policy%20lag%0Awhilst%20still%20being%20able%20to%20optimise%20trajectories%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520state%2520vector%2520reduction%2520during%2520model%2520predictive%2520control%2520with%250A%2520%2520gradient-based%2520trajectory%2520optimisation%26entry.906535625%3DDavid%2520Russell%2520and%2520Rafael%2520Papallas%2520and%2520Mehmet%2520Dogar%26entry.1292438233%3D%2520%2520Non-prehensile%2520manipulation%2520in%2520high-dimensional%2520systems%2520is%2520challenging%2520for%2520a%250Avariety%2520of%2520reasons%252C%2520one%2520of%2520the%2520main%2520reasons%2520is%2520the%2520computationally%2520long%250Aplanning%2520times%2520that%2520come%2520with%2520a%2520large%2520state%2520space.%2520Trajectory%2520optimisation%250Aalgorithms%2520have%2520proved%2520their%2520utility%2520in%2520a%2520wide%2520variety%2520of%2520tasks%252C%2520but%252C%2520like%2520most%250Amethods%2520struggle%2520scaling%2520to%2520the%2520high%2520dimensional%2520systems%2520ubiquitous%2520to%250Anon-prehensile%2520manipulation%2520in%2520clutter%2520as%2520well%2520as%2520deformable%2520object%250Amanipulation.%2520We%2520reason%2520that%252C%2520during%2520manipulation%252C%2520different%2520degrees%2520of%2520freedom%250Awill%2520become%2520more%2520or%2520less%2520important%2520to%2520the%2520task%2520over%2520time%2520as%2520the%2520system%2520evolves.%250AWe%2520leverage%2520this%2520idea%2520to%2520reduce%2520the%2520number%2520of%2520degrees%2520of%2520freedom%2520considered%2520in%250Aa%2520trajectory%2520optimisation%2520problem%252C%2520to%2520reduce%2520planning%2520times.%2520This%2520idea%2520is%250Aparticularly%2520relevant%2520in%2520the%2520context%2520of%2520model%2520predictive%2520control%2520%2528MPC%2529%2520where%250Athe%2520cost%2520landscape%2520of%2520the%2520optimisation%2520problem%2520is%2520constantly%2520evolving.%2520We%250Aprovide%2520simulation%2520results%2520under%2520asynchronous%2520MPC%2520and%2520show%2520our%2520methods%2520are%250Acapable%2520of%2520achieving%2520better%2520overall%2520performance%2520due%2520to%2520the%2520decreased%2520policy%2520lag%250Awhilst%2520still%2520being%2520able%2520to%2520optimise%2520trajectories%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20state%20vector%20reduction%20during%20model%20predictive%20control%20with%0A%20%20gradient-based%20trajectory%20optimisation&entry.906535625=David%20Russell%20and%20Rafael%20Papallas%20and%20Mehmet%20Dogar&entry.1292438233=%20%20Non-prehensile%20manipulation%20in%20high-dimensional%20systems%20is%20challenging%20for%20a%0Avariety%20of%20reasons%2C%20one%20of%20the%20main%20reasons%20is%20the%20computationally%20long%0Aplanning%20times%20that%20come%20with%20a%20large%20state%20space.%20Trajectory%20optimisation%0Aalgorithms%20have%20proved%20their%20utility%20in%20a%20wide%20variety%20of%20tasks%2C%20but%2C%20like%20most%0Amethods%20struggle%20scaling%20to%20the%20high%20dimensional%20systems%20ubiquitous%20to%0Anon-prehensile%20manipulation%20in%20clutter%20as%20well%20as%20deformable%20object%0Amanipulation.%20We%20reason%20that%2C%20during%20manipulation%2C%20different%20degrees%20of%20freedom%0Awill%20become%20more%20or%20less%20important%20to%20the%20task%20over%20time%20as%20the%20system%20evolves.%0AWe%20leverage%20this%20idea%20to%20reduce%20the%20number%20of%20degrees%20of%20freedom%20considered%20in%0Aa%20trajectory%20optimisation%20problem%2C%20to%20reduce%20planning%20times.%20This%20idea%20is%0Aparticularly%20relevant%20in%20the%20context%20of%20model%20predictive%20control%20%28MPC%29%20where%0Athe%20cost%20landscape%20of%20the%20optimisation%20problem%20is%20constantly%20evolving.%20We%0Aprovide%20simulation%20results%20under%20asynchronous%20MPC%20and%20show%20our%20methods%20are%0Acapable%20of%20achieving%20better%20overall%20performance%20due%20to%20the%20decreased%20policy%20lag%0Awhilst%20still%20being%20able%20to%20optimise%20trajectories%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11665v1&entry.124074799=Read"},
{"title": "Toward Enhancing Vehicle Color Recognition in Adverse Conditions: A\n  Dataset and Benchmark", "author": "Gabriel E. Lima and Rayson Laroca and Eduardo Santos and Eduil Nascimento Jr. and David Menotti", "abstract": "  Vehicle information recognition is crucial in various practical domains,\nparticularly in criminal investigations. Vehicle Color Recognition (VCR) has\ngarnered significant research interest because color is a visually\ndistinguishable attribute of vehicles and is less affected by partial occlusion\nand changes in viewpoint. Despite the success of existing methods for this\ntask, the relatively low complexity of the datasets used in the literature has\nbeen largely overlooked. This research addresses this gap by compiling a new\ndataset representing a more challenging VCR scenario. The images - sourced from\nsix license plate recognition datasets - are categorized into eleven colors,\nand their annotations were validated using official vehicle registration\ninformation. We evaluate the performance of four deep learning models on a\nwidely adopted dataset and our proposed dataset to establish a benchmark. The\nresults demonstrate that our dataset poses greater difficulty for the tested\nmodels and highlights scenarios that require further exploration in VCR.\nRemarkably, nighttime scenes account for a significant portion of the errors\nmade by the best-performing model. This research provides a foundation for\nfuture studies on VCR, while also offering valuable insights for the field of\nfine-grained vehicle classification.\n", "link": "http://arxiv.org/abs/2408.11589v1", "date": "2024-08-21", "relevancy": 2.0403, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5142}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5072}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Enhancing%20Vehicle%20Color%20Recognition%20in%20Adverse%20Conditions%3A%20A%0A%20%20Dataset%20and%20Benchmark&body=Title%3A%20Toward%20Enhancing%20Vehicle%20Color%20Recognition%20in%20Adverse%20Conditions%3A%20A%0A%20%20Dataset%20and%20Benchmark%0AAuthor%3A%20Gabriel%20E.%20Lima%20and%20Rayson%20Laroca%20and%20Eduardo%20Santos%20and%20Eduil%20Nascimento%20Jr.%20and%20David%20Menotti%0AAbstract%3A%20%20%20Vehicle%20information%20recognition%20is%20crucial%20in%20various%20practical%20domains%2C%0Aparticularly%20in%20criminal%20investigations.%20Vehicle%20Color%20Recognition%20%28VCR%29%20has%0Agarnered%20significant%20research%20interest%20because%20color%20is%20a%20visually%0Adistinguishable%20attribute%20of%20vehicles%20and%20is%20less%20affected%20by%20partial%20occlusion%0Aand%20changes%20in%20viewpoint.%20Despite%20the%20success%20of%20existing%20methods%20for%20this%0Atask%2C%20the%20relatively%20low%20complexity%20of%20the%20datasets%20used%20in%20the%20literature%20has%0Abeen%20largely%20overlooked.%20This%20research%20addresses%20this%20gap%20by%20compiling%20a%20new%0Adataset%20representing%20a%20more%20challenging%20VCR%20scenario.%20The%20images%20-%20sourced%20from%0Asix%20license%20plate%20recognition%20datasets%20-%20are%20categorized%20into%20eleven%20colors%2C%0Aand%20their%20annotations%20were%20validated%20using%20official%20vehicle%20registration%0Ainformation.%20We%20evaluate%20the%20performance%20of%20four%20deep%20learning%20models%20on%20a%0Awidely%20adopted%20dataset%20and%20our%20proposed%20dataset%20to%20establish%20a%20benchmark.%20The%0Aresults%20demonstrate%20that%20our%20dataset%20poses%20greater%20difficulty%20for%20the%20tested%0Amodels%20and%20highlights%20scenarios%20that%20require%20further%20exploration%20in%20VCR.%0ARemarkably%2C%20nighttime%20scenes%20account%20for%20a%20significant%20portion%20of%20the%20errors%0Amade%20by%20the%20best-performing%20model.%20This%20research%20provides%20a%20foundation%20for%0Afuture%20studies%20on%20VCR%2C%20while%20also%20offering%20valuable%20insights%20for%20the%20field%20of%0Afine-grained%20vehicle%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Enhancing%2520Vehicle%2520Color%2520Recognition%2520in%2520Adverse%2520Conditions%253A%2520A%250A%2520%2520Dataset%2520and%2520Benchmark%26entry.906535625%3DGabriel%2520E.%2520Lima%2520and%2520Rayson%2520Laroca%2520and%2520Eduardo%2520Santos%2520and%2520Eduil%2520Nascimento%2520Jr.%2520and%2520David%2520Menotti%26entry.1292438233%3D%2520%2520Vehicle%2520information%2520recognition%2520is%2520crucial%2520in%2520various%2520practical%2520domains%252C%250Aparticularly%2520in%2520criminal%2520investigations.%2520Vehicle%2520Color%2520Recognition%2520%2528VCR%2529%2520has%250Agarnered%2520significant%2520research%2520interest%2520because%2520color%2520is%2520a%2520visually%250Adistinguishable%2520attribute%2520of%2520vehicles%2520and%2520is%2520less%2520affected%2520by%2520partial%2520occlusion%250Aand%2520changes%2520in%2520viewpoint.%2520Despite%2520the%2520success%2520of%2520existing%2520methods%2520for%2520this%250Atask%252C%2520the%2520relatively%2520low%2520complexity%2520of%2520the%2520datasets%2520used%2520in%2520the%2520literature%2520has%250Abeen%2520largely%2520overlooked.%2520This%2520research%2520addresses%2520this%2520gap%2520by%2520compiling%2520a%2520new%250Adataset%2520representing%2520a%2520more%2520challenging%2520VCR%2520scenario.%2520The%2520images%2520-%2520sourced%2520from%250Asix%2520license%2520plate%2520recognition%2520datasets%2520-%2520are%2520categorized%2520into%2520eleven%2520colors%252C%250Aand%2520their%2520annotations%2520were%2520validated%2520using%2520official%2520vehicle%2520registration%250Ainformation.%2520We%2520evaluate%2520the%2520performance%2520of%2520four%2520deep%2520learning%2520models%2520on%2520a%250Awidely%2520adopted%2520dataset%2520and%2520our%2520proposed%2520dataset%2520to%2520establish%2520a%2520benchmark.%2520The%250Aresults%2520demonstrate%2520that%2520our%2520dataset%2520poses%2520greater%2520difficulty%2520for%2520the%2520tested%250Amodels%2520and%2520highlights%2520scenarios%2520that%2520require%2520further%2520exploration%2520in%2520VCR.%250ARemarkably%252C%2520nighttime%2520scenes%2520account%2520for%2520a%2520significant%2520portion%2520of%2520the%2520errors%250Amade%2520by%2520the%2520best-performing%2520model.%2520This%2520research%2520provides%2520a%2520foundation%2520for%250Afuture%2520studies%2520on%2520VCR%252C%2520while%2520also%2520offering%2520valuable%2520insights%2520for%2520the%2520field%2520of%250Afine-grained%2520vehicle%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Enhancing%20Vehicle%20Color%20Recognition%20in%20Adverse%20Conditions%3A%20A%0A%20%20Dataset%20and%20Benchmark&entry.906535625=Gabriel%20E.%20Lima%20and%20Rayson%20Laroca%20and%20Eduardo%20Santos%20and%20Eduil%20Nascimento%20Jr.%20and%20David%20Menotti&entry.1292438233=%20%20Vehicle%20information%20recognition%20is%20crucial%20in%20various%20practical%20domains%2C%0Aparticularly%20in%20criminal%20investigations.%20Vehicle%20Color%20Recognition%20%28VCR%29%20has%0Agarnered%20significant%20research%20interest%20because%20color%20is%20a%20visually%0Adistinguishable%20attribute%20of%20vehicles%20and%20is%20less%20affected%20by%20partial%20occlusion%0Aand%20changes%20in%20viewpoint.%20Despite%20the%20success%20of%20existing%20methods%20for%20this%0Atask%2C%20the%20relatively%20low%20complexity%20of%20the%20datasets%20used%20in%20the%20literature%20has%0Abeen%20largely%20overlooked.%20This%20research%20addresses%20this%20gap%20by%20compiling%20a%20new%0Adataset%20representing%20a%20more%20challenging%20VCR%20scenario.%20The%20images%20-%20sourced%20from%0Asix%20license%20plate%20recognition%20datasets%20-%20are%20categorized%20into%20eleven%20colors%2C%0Aand%20their%20annotations%20were%20validated%20using%20official%20vehicle%20registration%0Ainformation.%20We%20evaluate%20the%20performance%20of%20four%20deep%20learning%20models%20on%20a%0Awidely%20adopted%20dataset%20and%20our%20proposed%20dataset%20to%20establish%20a%20benchmark.%20The%0Aresults%20demonstrate%20that%20our%20dataset%20poses%20greater%20difficulty%20for%20the%20tested%0Amodels%20and%20highlights%20scenarios%20that%20require%20further%20exploration%20in%20VCR.%0ARemarkably%2C%20nighttime%20scenes%20account%20for%20a%20significant%20portion%20of%20the%20errors%0Amade%20by%20the%20best-performing%20model.%20This%20research%20provides%20a%20foundation%20for%0Afuture%20studies%20on%20VCR%2C%20while%20also%20offering%20valuable%20insights%20for%20the%20field%20of%0Afine-grained%20vehicle%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11589v1&entry.124074799=Read"},
{"title": "FedGS: Federated Gradient Scaling for Heterogeneous Medical Image\n  Segmentation", "author": "Philip Schutte and Valentina Corbetta and Regina Beets-Tan and Wilson Silva", "abstract": "  Federated Learning (FL) in Deep Learning (DL)-automated medical image\nsegmentation helps preserving privacy by enabling collaborative model training\nwithout sharing patient data. However, FL faces challenges with data\nheterogeneity among institutions, leading to suboptimal global models.\nIntegrating Disentangled Representation Learning (DRL) in FL can enhance\nrobustness by separating data into distinct representations. Existing DRL\nmethods assume heterogeneity lies solely in style features, overlooking\ncontent-based variability like lesion size and shape. We propose FedGS, a novel\nFL aggregation method, to improve segmentation performance on small,\nunder-represented targets while maintaining overall efficacy. FedGS\ndemonstrates superior performance over FedAvg, particularly for small lesions,\nacross PolypGen and LiTS datasets. The code and pre-trained checkpoints are\navailable at the following link:\nhttps://github.com/Trustworthy-AI-UU-NKI/Federated-Learning-Disentanglement\n", "link": "http://arxiv.org/abs/2408.11701v1", "date": "2024-08-21", "relevancy": 2.0397, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5193}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5101}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedGS%3A%20Federated%20Gradient%20Scaling%20for%20Heterogeneous%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20FedGS%3A%20Federated%20Gradient%20Scaling%20for%20Heterogeneous%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Philip%20Schutte%20and%20Valentina%20Corbetta%20and%20Regina%20Beets-Tan%20and%20Wilson%20Silva%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20in%20Deep%20Learning%20%28DL%29-automated%20medical%20image%0Asegmentation%20helps%20preserving%20privacy%20by%20enabling%20collaborative%20model%20training%0Awithout%20sharing%20patient%20data.%20However%2C%20FL%20faces%20challenges%20with%20data%0Aheterogeneity%20among%20institutions%2C%20leading%20to%20suboptimal%20global%20models.%0AIntegrating%20Disentangled%20Representation%20Learning%20%28DRL%29%20in%20FL%20can%20enhance%0Arobustness%20by%20separating%20data%20into%20distinct%20representations.%20Existing%20DRL%0Amethods%20assume%20heterogeneity%20lies%20solely%20in%20style%20features%2C%20overlooking%0Acontent-based%20variability%20like%20lesion%20size%20and%20shape.%20We%20propose%20FedGS%2C%20a%20novel%0AFL%20aggregation%20method%2C%20to%20improve%20segmentation%20performance%20on%20small%2C%0Aunder-represented%20targets%20while%20maintaining%20overall%20efficacy.%20FedGS%0Ademonstrates%20superior%20performance%20over%20FedAvg%2C%20particularly%20for%20small%20lesions%2C%0Aacross%20PolypGen%20and%20LiTS%20datasets.%20The%20code%20and%20pre-trained%20checkpoints%20are%0Aavailable%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/Trustworthy-AI-UU-NKI/Federated-Learning-Disentanglement%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedGS%253A%2520Federated%2520Gradient%2520Scaling%2520for%2520Heterogeneous%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DPhilip%2520Schutte%2520and%2520Valentina%2520Corbetta%2520and%2520Regina%2520Beets-Tan%2520and%2520Wilson%2520Silva%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520in%2520Deep%2520Learning%2520%2528DL%2529-automated%2520medical%2520image%250Asegmentation%2520helps%2520preserving%2520privacy%2520by%2520enabling%2520collaborative%2520model%2520training%250Awithout%2520sharing%2520patient%2520data.%2520However%252C%2520FL%2520faces%2520challenges%2520with%2520data%250Aheterogeneity%2520among%2520institutions%252C%2520leading%2520to%2520suboptimal%2520global%2520models.%250AIntegrating%2520Disentangled%2520Representation%2520Learning%2520%2528DRL%2529%2520in%2520FL%2520can%2520enhance%250Arobustness%2520by%2520separating%2520data%2520into%2520distinct%2520representations.%2520Existing%2520DRL%250Amethods%2520assume%2520heterogeneity%2520lies%2520solely%2520in%2520style%2520features%252C%2520overlooking%250Acontent-based%2520variability%2520like%2520lesion%2520size%2520and%2520shape.%2520We%2520propose%2520FedGS%252C%2520a%2520novel%250AFL%2520aggregation%2520method%252C%2520to%2520improve%2520segmentation%2520performance%2520on%2520small%252C%250Aunder-represented%2520targets%2520while%2520maintaining%2520overall%2520efficacy.%2520FedGS%250Ademonstrates%2520superior%2520performance%2520over%2520FedAvg%252C%2520particularly%2520for%2520small%2520lesions%252C%250Aacross%2520PolypGen%2520and%2520LiTS%2520datasets.%2520The%2520code%2520and%2520pre-trained%2520checkpoints%2520are%250Aavailable%2520at%2520the%2520following%2520link%253A%250Ahttps%253A//github.com/Trustworthy-AI-UU-NKI/Federated-Learning-Disentanglement%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedGS%3A%20Federated%20Gradient%20Scaling%20for%20Heterogeneous%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Philip%20Schutte%20and%20Valentina%20Corbetta%20and%20Regina%20Beets-Tan%20and%20Wilson%20Silva&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20in%20Deep%20Learning%20%28DL%29-automated%20medical%20image%0Asegmentation%20helps%20preserving%20privacy%20by%20enabling%20collaborative%20model%20training%0Awithout%20sharing%20patient%20data.%20However%2C%20FL%20faces%20challenges%20with%20data%0Aheterogeneity%20among%20institutions%2C%20leading%20to%20suboptimal%20global%20models.%0AIntegrating%20Disentangled%20Representation%20Learning%20%28DRL%29%20in%20FL%20can%20enhance%0Arobustness%20by%20separating%20data%20into%20distinct%20representations.%20Existing%20DRL%0Amethods%20assume%20heterogeneity%20lies%20solely%20in%20style%20features%2C%20overlooking%0Acontent-based%20variability%20like%20lesion%20size%20and%20shape.%20We%20propose%20FedGS%2C%20a%20novel%0AFL%20aggregation%20method%2C%20to%20improve%20segmentation%20performance%20on%20small%2C%0Aunder-represented%20targets%20while%20maintaining%20overall%20efficacy.%20FedGS%0Ademonstrates%20superior%20performance%20over%20FedAvg%2C%20particularly%20for%20small%20lesions%2C%0Aacross%20PolypGen%20and%20LiTS%20datasets.%20The%20code%20and%20pre-trained%20checkpoints%20are%0Aavailable%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/Trustworthy-AI-UU-NKI/Federated-Learning-Disentanglement%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11701v1&entry.124074799=Read"},
{"title": "On Learnable Parameters of Optimal and Suboptimal Deep Learning Models", "author": "Ziwei Zheng and Huizhi Liang and Vaclav Snasel and Vito Latora and Panos Pardalos and Giuseppe Nicosia and Varun Ojha", "abstract": "  We scrutinize the structural and operational aspects of deep learning models,\nparticularly focusing on the nuances of learnable parameters (weight)\nstatistics, distribution, node interaction, and visualization. By establishing\ncorrelations between variance in weight patterns and overall network\nperformance, we investigate the varying (optimal and suboptimal) performances\nof various deep-learning models. Our empirical analysis extends across widely\nrecognized datasets such as MNIST, Fashion-MNIST, and CIFAR-10, and various\ndeep learning models such as deep neural networks (DNNs), convolutional neural\nnetworks (CNNs), and vision transformer (ViT), enabling us to pinpoint\ncharacteristics of learnable parameters that correlate with successful\nnetworks. Through extensive experiments on the diverse architectures of deep\nlearning models, we shed light on the critical factors that influence the\nfunctionality and efficiency of DNNs. Our findings reveal that successful\nnetworks, irrespective of datasets or models, are invariably similar to other\nsuccessful networks in their converged weights statistics and distribution,\nwhile poor-performing networks vary in their weights. In addition, our research\nshows that the learnable parameters of widely varied deep learning models such\nas DNN, CNN, and ViT exhibit similar learning characteristics.\n", "link": "http://arxiv.org/abs/2408.11720v1", "date": "2024-08-21", "relevancy": 2.0358, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5273}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4998}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Learnable%20Parameters%20of%20Optimal%20and%20Suboptimal%20Deep%20Learning%20Models&body=Title%3A%20On%20Learnable%20Parameters%20of%20Optimal%20and%20Suboptimal%20Deep%20Learning%20Models%0AAuthor%3A%20Ziwei%20Zheng%20and%20Huizhi%20Liang%20and%20Vaclav%20Snasel%20and%20Vito%20Latora%20and%20Panos%20Pardalos%20and%20Giuseppe%20Nicosia%20and%20Varun%20Ojha%0AAbstract%3A%20%20%20We%20scrutinize%20the%20structural%20and%20operational%20aspects%20of%20deep%20learning%20models%2C%0Aparticularly%20focusing%20on%20the%20nuances%20of%20learnable%20parameters%20%28weight%29%0Astatistics%2C%20distribution%2C%20node%20interaction%2C%20and%20visualization.%20By%20establishing%0Acorrelations%20between%20variance%20in%20weight%20patterns%20and%20overall%20network%0Aperformance%2C%20we%20investigate%20the%20varying%20%28optimal%20and%20suboptimal%29%20performances%0Aof%20various%20deep-learning%20models.%20Our%20empirical%20analysis%20extends%20across%20widely%0Arecognized%20datasets%20such%20as%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR-10%2C%20and%20various%0Adeep%20learning%20models%20such%20as%20deep%20neural%20networks%20%28DNNs%29%2C%20convolutional%20neural%0Anetworks%20%28CNNs%29%2C%20and%20vision%20transformer%20%28ViT%29%2C%20enabling%20us%20to%20pinpoint%0Acharacteristics%20of%20learnable%20parameters%20that%20correlate%20with%20successful%0Anetworks.%20Through%20extensive%20experiments%20on%20the%20diverse%20architectures%20of%20deep%0Alearning%20models%2C%20we%20shed%20light%20on%20the%20critical%20factors%20that%20influence%20the%0Afunctionality%20and%20efficiency%20of%20DNNs.%20Our%20findings%20reveal%20that%20successful%0Anetworks%2C%20irrespective%20of%20datasets%20or%20models%2C%20are%20invariably%20similar%20to%20other%0Asuccessful%20networks%20in%20their%20converged%20weights%20statistics%20and%20distribution%2C%0Awhile%20poor-performing%20networks%20vary%20in%20their%20weights.%20In%20addition%2C%20our%20research%0Ashows%20that%20the%20learnable%20parameters%20of%20widely%20varied%20deep%20learning%20models%20such%0Aas%20DNN%2C%20CNN%2C%20and%20ViT%20exhibit%20similar%20learning%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Learnable%2520Parameters%2520of%2520Optimal%2520and%2520Suboptimal%2520Deep%2520Learning%2520Models%26entry.906535625%3DZiwei%2520Zheng%2520and%2520Huizhi%2520Liang%2520and%2520Vaclav%2520Snasel%2520and%2520Vito%2520Latora%2520and%2520Panos%2520Pardalos%2520and%2520Giuseppe%2520Nicosia%2520and%2520Varun%2520Ojha%26entry.1292438233%3D%2520%2520We%2520scrutinize%2520the%2520structural%2520and%2520operational%2520aspects%2520of%2520deep%2520learning%2520models%252C%250Aparticularly%2520focusing%2520on%2520the%2520nuances%2520of%2520learnable%2520parameters%2520%2528weight%2529%250Astatistics%252C%2520distribution%252C%2520node%2520interaction%252C%2520and%2520visualization.%2520By%2520establishing%250Acorrelations%2520between%2520variance%2520in%2520weight%2520patterns%2520and%2520overall%2520network%250Aperformance%252C%2520we%2520investigate%2520the%2520varying%2520%2528optimal%2520and%2520suboptimal%2529%2520performances%250Aof%2520various%2520deep-learning%2520models.%2520Our%2520empirical%2520analysis%2520extends%2520across%2520widely%250Arecognized%2520datasets%2520such%2520as%2520MNIST%252C%2520Fashion-MNIST%252C%2520and%2520CIFAR-10%252C%2520and%2520various%250Adeep%2520learning%2520models%2520such%2520as%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%252C%2520and%2520vision%2520transformer%2520%2528ViT%2529%252C%2520enabling%2520us%2520to%2520pinpoint%250Acharacteristics%2520of%2520learnable%2520parameters%2520that%2520correlate%2520with%2520successful%250Anetworks.%2520Through%2520extensive%2520experiments%2520on%2520the%2520diverse%2520architectures%2520of%2520deep%250Alearning%2520models%252C%2520we%2520shed%2520light%2520on%2520the%2520critical%2520factors%2520that%2520influence%2520the%250Afunctionality%2520and%2520efficiency%2520of%2520DNNs.%2520Our%2520findings%2520reveal%2520that%2520successful%250Anetworks%252C%2520irrespective%2520of%2520datasets%2520or%2520models%252C%2520are%2520invariably%2520similar%2520to%2520other%250Asuccessful%2520networks%2520in%2520their%2520converged%2520weights%2520statistics%2520and%2520distribution%252C%250Awhile%2520poor-performing%2520networks%2520vary%2520in%2520their%2520weights.%2520In%2520addition%252C%2520our%2520research%250Ashows%2520that%2520the%2520learnable%2520parameters%2520of%2520widely%2520varied%2520deep%2520learning%2520models%2520such%250Aas%2520DNN%252C%2520CNN%252C%2520and%2520ViT%2520exhibit%2520similar%2520learning%2520characteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Learnable%20Parameters%20of%20Optimal%20and%20Suboptimal%20Deep%20Learning%20Models&entry.906535625=Ziwei%20Zheng%20and%20Huizhi%20Liang%20and%20Vaclav%20Snasel%20and%20Vito%20Latora%20and%20Panos%20Pardalos%20and%20Giuseppe%20Nicosia%20and%20Varun%20Ojha&entry.1292438233=%20%20We%20scrutinize%20the%20structural%20and%20operational%20aspects%20of%20deep%20learning%20models%2C%0Aparticularly%20focusing%20on%20the%20nuances%20of%20learnable%20parameters%20%28weight%29%0Astatistics%2C%20distribution%2C%20node%20interaction%2C%20and%20visualization.%20By%20establishing%0Acorrelations%20between%20variance%20in%20weight%20patterns%20and%20overall%20network%0Aperformance%2C%20we%20investigate%20the%20varying%20%28optimal%20and%20suboptimal%29%20performances%0Aof%20various%20deep-learning%20models.%20Our%20empirical%20analysis%20extends%20across%20widely%0Arecognized%20datasets%20such%20as%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR-10%2C%20and%20various%0Adeep%20learning%20models%20such%20as%20deep%20neural%20networks%20%28DNNs%29%2C%20convolutional%20neural%0Anetworks%20%28CNNs%29%2C%20and%20vision%20transformer%20%28ViT%29%2C%20enabling%20us%20to%20pinpoint%0Acharacteristics%20of%20learnable%20parameters%20that%20correlate%20with%20successful%0Anetworks.%20Through%20extensive%20experiments%20on%20the%20diverse%20architectures%20of%20deep%0Alearning%20models%2C%20we%20shed%20light%20on%20the%20critical%20factors%20that%20influence%20the%0Afunctionality%20and%20efficiency%20of%20DNNs.%20Our%20findings%20reveal%20that%20successful%0Anetworks%2C%20irrespective%20of%20datasets%20or%20models%2C%20are%20invariably%20similar%20to%20other%0Asuccessful%20networks%20in%20their%20converged%20weights%20statistics%20and%20distribution%2C%0Awhile%20poor-performing%20networks%20vary%20in%20their%20weights.%20In%20addition%2C%20our%20research%0Ashows%20that%20the%20learnable%20parameters%20of%20widely%20varied%20deep%20learning%20models%20such%0Aas%20DNN%2C%20CNN%2C%20and%20ViT%20exhibit%20similar%20learning%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11720v1&entry.124074799=Read"},
{"title": "5G NR PRACH Detection with Convolutional Neural Networks (CNN):\n  Overcoming Cell Interference Challenges", "author": "Desire Guel and Arsene Kabore and Didier Bassole", "abstract": "  In this paper, we present a novel approach to interference detection in 5G\nNew Radio (5G-NR) networks using Convolutional Neural Networks (CNN).\nInterference in 5G networks challenges high-quality service due to dense user\nequipment deployment and increased wireless environment complexity. Our\nCNN-based model is designed to detect Physical Random Access Channel (PRACH)\nsequences amidst various interference scenarios, leveraging the spatial and\ntemporal characteristics of PRACH signals to enhance detection accuracy and\nrobustness. Comprehensive datasets of simulated PRACH signals under controlled\ninterference conditions were generated to train and validate the model.\nExperimental results show that our CNN-based approach outperforms traditional\nPRACH detection methods in accuracy, precision, recall and F1-score. This study\ndemonstrates the potential of AI/ML techniques in advancing interference\nmanagement in 5G networks, providing a foundation for future research and\npractical applications in optimizing network performance and reliability.\n", "link": "http://arxiv.org/abs/2408.11659v1", "date": "2024-08-21", "relevancy": 2.0351, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4226}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4017}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%205G%20NR%20PRACH%20Detection%20with%20Convolutional%20Neural%20Networks%20%28CNN%29%3A%0A%20%20Overcoming%20Cell%20Interference%20Challenges&body=Title%3A%205G%20NR%20PRACH%20Detection%20with%20Convolutional%20Neural%20Networks%20%28CNN%29%3A%0A%20%20Overcoming%20Cell%20Interference%20Challenges%0AAuthor%3A%20Desire%20Guel%20and%20Arsene%20Kabore%20and%20Didier%20Bassole%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20to%20interference%20detection%20in%205G%0ANew%20Radio%20%285G-NR%29%20networks%20using%20Convolutional%20Neural%20Networks%20%28CNN%29.%0AInterference%20in%205G%20networks%20challenges%20high-quality%20service%20due%20to%20dense%20user%0Aequipment%20deployment%20and%20increased%20wireless%20environment%20complexity.%20Our%0ACNN-based%20model%20is%20designed%20to%20detect%20Physical%20Random%20Access%20Channel%20%28PRACH%29%0Asequences%20amidst%20various%20interference%20scenarios%2C%20leveraging%20the%20spatial%20and%0Atemporal%20characteristics%20of%20PRACH%20signals%20to%20enhance%20detection%20accuracy%20and%0Arobustness.%20Comprehensive%20datasets%20of%20simulated%20PRACH%20signals%20under%20controlled%0Ainterference%20conditions%20were%20generated%20to%20train%20and%20validate%20the%20model.%0AExperimental%20results%20show%20that%20our%20CNN-based%20approach%20outperforms%20traditional%0APRACH%20detection%20methods%20in%20accuracy%2C%20precision%2C%20recall%20and%20F1-score.%20This%20study%0Ademonstrates%20the%20potential%20of%20AI/ML%20techniques%20in%20advancing%20interference%0Amanagement%20in%205G%20networks%2C%20providing%20a%20foundation%20for%20future%20research%20and%0Apractical%20applications%20in%20optimizing%20network%20performance%20and%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D5G%2520NR%2520PRACH%2520Detection%2520with%2520Convolutional%2520Neural%2520Networks%2520%2528CNN%2529%253A%250A%2520%2520Overcoming%2520Cell%2520Interference%2520Challenges%26entry.906535625%3DDesire%2520Guel%2520and%2520Arsene%2520Kabore%2520and%2520Didier%2520Bassole%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520to%2520interference%2520detection%2520in%25205G%250ANew%2520Radio%2520%25285G-NR%2529%2520networks%2520using%2520Convolutional%2520Neural%2520Networks%2520%2528CNN%2529.%250AInterference%2520in%25205G%2520networks%2520challenges%2520high-quality%2520service%2520due%2520to%2520dense%2520user%250Aequipment%2520deployment%2520and%2520increased%2520wireless%2520environment%2520complexity.%2520Our%250ACNN-based%2520model%2520is%2520designed%2520to%2520detect%2520Physical%2520Random%2520Access%2520Channel%2520%2528PRACH%2529%250Asequences%2520amidst%2520various%2520interference%2520scenarios%252C%2520leveraging%2520the%2520spatial%2520and%250Atemporal%2520characteristics%2520of%2520PRACH%2520signals%2520to%2520enhance%2520detection%2520accuracy%2520and%250Arobustness.%2520Comprehensive%2520datasets%2520of%2520simulated%2520PRACH%2520signals%2520under%2520controlled%250Ainterference%2520conditions%2520were%2520generated%2520to%2520train%2520and%2520validate%2520the%2520model.%250AExperimental%2520results%2520show%2520that%2520our%2520CNN-based%2520approach%2520outperforms%2520traditional%250APRACH%2520detection%2520methods%2520in%2520accuracy%252C%2520precision%252C%2520recall%2520and%2520F1-score.%2520This%2520study%250Ademonstrates%2520the%2520potential%2520of%2520AI/ML%2520techniques%2520in%2520advancing%2520interference%250Amanagement%2520in%25205G%2520networks%252C%2520providing%2520a%2520foundation%2520for%2520future%2520research%2520and%250Apractical%2520applications%2520in%2520optimizing%2520network%2520performance%2520and%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=5G%20NR%20PRACH%20Detection%20with%20Convolutional%20Neural%20Networks%20%28CNN%29%3A%0A%20%20Overcoming%20Cell%20Interference%20Challenges&entry.906535625=Desire%20Guel%20and%20Arsene%20Kabore%20and%20Didier%20Bassole&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20to%20interference%20detection%20in%205G%0ANew%20Radio%20%285G-NR%29%20networks%20using%20Convolutional%20Neural%20Networks%20%28CNN%29.%0AInterference%20in%205G%20networks%20challenges%20high-quality%20service%20due%20to%20dense%20user%0Aequipment%20deployment%20and%20increased%20wireless%20environment%20complexity.%20Our%0ACNN-based%20model%20is%20designed%20to%20detect%20Physical%20Random%20Access%20Channel%20%28PRACH%29%0Asequences%20amidst%20various%20interference%20scenarios%2C%20leveraging%20the%20spatial%20and%0Atemporal%20characteristics%20of%20PRACH%20signals%20to%20enhance%20detection%20accuracy%20and%0Arobustness.%20Comprehensive%20datasets%20of%20simulated%20PRACH%20signals%20under%20controlled%0Ainterference%20conditions%20were%20generated%20to%20train%20and%20validate%20the%20model.%0AExperimental%20results%20show%20that%20our%20CNN-based%20approach%20outperforms%20traditional%0APRACH%20detection%20methods%20in%20accuracy%2C%20precision%2C%20recall%20and%20F1-score.%20This%20study%0Ademonstrates%20the%20potential%20of%20AI/ML%20techniques%20in%20advancing%20interference%0Amanagement%20in%205G%20networks%2C%20providing%20a%20foundation%20for%20future%20research%20and%0Apractical%20applications%20in%20optimizing%20network%20performance%20and%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11659v1&entry.124074799=Read"},
{"title": "A Survey for Foundation Models in Autonomous Driving", "author": "Haoxiang Gao and Zhongruo Wang and Yaqian Li and Kaiwen Long and Ming Yang and Yiqing Shen", "abstract": "  The advent of foundation models has revolutionized the fields of natural\nlanguage processing and computer vision, paving the way for their application\nin autonomous driving (AD). This survey presents a comprehensive review of more\nthan 40 research papers, demonstrating the role of foundation models in\nenhancing AD. Large language models contribute to planning and simulation in\nAD, particularly through their proficiency in reasoning, code generation and\ntranslation. In parallel, vision foundation models are increasingly adapted for\ncritical tasks such as 3D object detection and tracking, as well as creating\nrealistic driving scenarios for simulation and testing. Multi-modal foundation\nmodels, integrating diverse inputs, exhibit exceptional visual understanding\nand spatial reasoning, crucial for end-to-end AD. This survey not only provides\na structured taxonomy, categorizing foundation models based on their modalities\nand functionalities within the AD domain but also delves into the methods\nemployed in current research. It identifies the gaps between existing\nfoundation models and cutting-edge AD approaches, thereby charting future\nresearch directions and proposing a roadmap for bridging these gaps.\n", "link": "http://arxiv.org/abs/2402.01105v2", "date": "2024-08-21", "relevancy": 2.0284, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5446}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4998}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20for%20Foundation%20Models%20in%20Autonomous%20Driving&body=Title%3A%20A%20Survey%20for%20Foundation%20Models%20in%20Autonomous%20Driving%0AAuthor%3A%20Haoxiang%20Gao%20and%20Zhongruo%20Wang%20and%20Yaqian%20Li%20and%20Kaiwen%20Long%20and%20Ming%20Yang%20and%20Yiqing%20Shen%0AAbstract%3A%20%20%20The%20advent%20of%20foundation%20models%20has%20revolutionized%20the%20fields%20of%20natural%0Alanguage%20processing%20and%20computer%20vision%2C%20paving%20the%20way%20for%20their%20application%0Ain%20autonomous%20driving%20%28AD%29.%20This%20survey%20presents%20a%20comprehensive%20review%20of%20more%0Athan%2040%20research%20papers%2C%20demonstrating%20the%20role%20of%20foundation%20models%20in%0Aenhancing%20AD.%20Large%20language%20models%20contribute%20to%20planning%20and%20simulation%20in%0AAD%2C%20particularly%20through%20their%20proficiency%20in%20reasoning%2C%20code%20generation%20and%0Atranslation.%20In%20parallel%2C%20vision%20foundation%20models%20are%20increasingly%20adapted%20for%0Acritical%20tasks%20such%20as%203D%20object%20detection%20and%20tracking%2C%20as%20well%20as%20creating%0Arealistic%20driving%20scenarios%20for%20simulation%20and%20testing.%20Multi-modal%20foundation%0Amodels%2C%20integrating%20diverse%20inputs%2C%20exhibit%20exceptional%20visual%20understanding%0Aand%20spatial%20reasoning%2C%20crucial%20for%20end-to-end%20AD.%20This%20survey%20not%20only%20provides%0Aa%20structured%20taxonomy%2C%20categorizing%20foundation%20models%20based%20on%20their%20modalities%0Aand%20functionalities%20within%20the%20AD%20domain%20but%20also%20delves%20into%20the%20methods%0Aemployed%20in%20current%20research.%20It%20identifies%20the%20gaps%20between%20existing%0Afoundation%20models%20and%20cutting-edge%20AD%20approaches%2C%20thereby%20charting%20future%0Aresearch%20directions%20and%20proposing%20a%20roadmap%20for%20bridging%20these%20gaps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520for%2520Foundation%2520Models%2520in%2520Autonomous%2520Driving%26entry.906535625%3DHaoxiang%2520Gao%2520and%2520Zhongruo%2520Wang%2520and%2520Yaqian%2520Li%2520and%2520Kaiwen%2520Long%2520and%2520Ming%2520Yang%2520and%2520Yiqing%2520Shen%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520foundation%2520models%2520has%2520revolutionized%2520the%2520fields%2520of%2520natural%250Alanguage%2520processing%2520and%2520computer%2520vision%252C%2520paving%2520the%2520way%2520for%2520their%2520application%250Ain%2520autonomous%2520driving%2520%2528AD%2529.%2520This%2520survey%2520presents%2520a%2520comprehensive%2520review%2520of%2520more%250Athan%252040%2520research%2520papers%252C%2520demonstrating%2520the%2520role%2520of%2520foundation%2520models%2520in%250Aenhancing%2520AD.%2520Large%2520language%2520models%2520contribute%2520to%2520planning%2520and%2520simulation%2520in%250AAD%252C%2520particularly%2520through%2520their%2520proficiency%2520in%2520reasoning%252C%2520code%2520generation%2520and%250Atranslation.%2520In%2520parallel%252C%2520vision%2520foundation%2520models%2520are%2520increasingly%2520adapted%2520for%250Acritical%2520tasks%2520such%2520as%25203D%2520object%2520detection%2520and%2520tracking%252C%2520as%2520well%2520as%2520creating%250Arealistic%2520driving%2520scenarios%2520for%2520simulation%2520and%2520testing.%2520Multi-modal%2520foundation%250Amodels%252C%2520integrating%2520diverse%2520inputs%252C%2520exhibit%2520exceptional%2520visual%2520understanding%250Aand%2520spatial%2520reasoning%252C%2520crucial%2520for%2520end-to-end%2520AD.%2520This%2520survey%2520not%2520only%2520provides%250Aa%2520structured%2520taxonomy%252C%2520categorizing%2520foundation%2520models%2520based%2520on%2520their%2520modalities%250Aand%2520functionalities%2520within%2520the%2520AD%2520domain%2520but%2520also%2520delves%2520into%2520the%2520methods%250Aemployed%2520in%2520current%2520research.%2520It%2520identifies%2520the%2520gaps%2520between%2520existing%250Afoundation%2520models%2520and%2520cutting-edge%2520AD%2520approaches%252C%2520thereby%2520charting%2520future%250Aresearch%2520directions%2520and%2520proposing%2520a%2520roadmap%2520for%2520bridging%2520these%2520gaps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20for%20Foundation%20Models%20in%20Autonomous%20Driving&entry.906535625=Haoxiang%20Gao%20and%20Zhongruo%20Wang%20and%20Yaqian%20Li%20and%20Kaiwen%20Long%20and%20Ming%20Yang%20and%20Yiqing%20Shen&entry.1292438233=%20%20The%20advent%20of%20foundation%20models%20has%20revolutionized%20the%20fields%20of%20natural%0Alanguage%20processing%20and%20computer%20vision%2C%20paving%20the%20way%20for%20their%20application%0Ain%20autonomous%20driving%20%28AD%29.%20This%20survey%20presents%20a%20comprehensive%20review%20of%20more%0Athan%2040%20research%20papers%2C%20demonstrating%20the%20role%20of%20foundation%20models%20in%0Aenhancing%20AD.%20Large%20language%20models%20contribute%20to%20planning%20and%20simulation%20in%0AAD%2C%20particularly%20through%20their%20proficiency%20in%20reasoning%2C%20code%20generation%20and%0Atranslation.%20In%20parallel%2C%20vision%20foundation%20models%20are%20increasingly%20adapted%20for%0Acritical%20tasks%20such%20as%203D%20object%20detection%20and%20tracking%2C%20as%20well%20as%20creating%0Arealistic%20driving%20scenarios%20for%20simulation%20and%20testing.%20Multi-modal%20foundation%0Amodels%2C%20integrating%20diverse%20inputs%2C%20exhibit%20exceptional%20visual%20understanding%0Aand%20spatial%20reasoning%2C%20crucial%20for%20end-to-end%20AD.%20This%20survey%20not%20only%20provides%0Aa%20structured%20taxonomy%2C%20categorizing%20foundation%20models%20based%20on%20their%20modalities%0Aand%20functionalities%20within%20the%20AD%20domain%20but%20also%20delves%20into%20the%20methods%0Aemployed%20in%20current%20research.%20It%20identifies%20the%20gaps%20between%20existing%0Afoundation%20models%20and%20cutting-edge%20AD%20approaches%2C%20thereby%20charting%20future%0Aresearch%20directions%20and%20proposing%20a%20roadmap%20for%20bridging%20these%20gaps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01105v2&entry.124074799=Read"},
{"title": "Slicing Input Features to Accelerate Deep Learning: A Case Study with\n  Graph Neural Networks", "author": "Zhengjia Xu and Dingyang Lyu and Jinghui Zhang", "abstract": "  As graphs grow larger, full-batch GNN training becomes hard for single GPU\nmemory. Therefore, to enhance the scalability of GNN training, some studies\nhave proposed sampling-based mini-batch training and distributed graph\nlearning. However, these methods still have drawbacks, such as performance\ndegradation and heavy communication. This paper introduces SliceGCN, a\nfeature-sliced distributed large-scale graph learning method. SliceGCN slices\nthe node features, with each computing device, i.e., GPU, handling partial\nfeatures. After each GPU processes its share, partial representations are\nobtained and concatenated to form complete representations, enabling a single\nGPU's memory to handle the entire graph structure. This aims to avoid the\naccuracy loss typically associated with mini-batch training (due to incomplete\ngraph structures) and to reduce inter-GPU communication during message passing\n(the forward propagation process of GNNs). To study and mitigate potential\naccuracy reductions due to slicing features, this paper proposes feature fusion\nand slice encoding. Experiments were conducted on six node classification\ndatasets, yielding some interesting analytical results. These results indicate\nthat while SliceGCN does not enhance efficiency on smaller datasets, it does\nimprove efficiency on larger datasets. Additionally, we found that SliceGCN and\nits variants have better convergence, feature fusion and slice encoding can\nmake training more stable, reduce accuracy fluctuations, and this study also\ndiscovered that the design of SliceGCN has a potentially parameter-efficient\nnature.\n", "link": "http://arxiv.org/abs/2408.11500v1", "date": "2024-08-21", "relevancy": 2.0177, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5109}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slicing%20Input%20Features%20to%20Accelerate%20Deep%20Learning%3A%20A%20Case%20Study%20with%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20Slicing%20Input%20Features%20to%20Accelerate%20Deep%20Learning%3A%20A%20Case%20Study%20with%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20Zhengjia%20Xu%20and%20Dingyang%20Lyu%20and%20Jinghui%20Zhang%0AAbstract%3A%20%20%20As%20graphs%20grow%20larger%2C%20full-batch%20GNN%20training%20becomes%20hard%20for%20single%20GPU%0Amemory.%20Therefore%2C%20to%20enhance%20the%20scalability%20of%20GNN%20training%2C%20some%20studies%0Ahave%20proposed%20sampling-based%20mini-batch%20training%20and%20distributed%20graph%0Alearning.%20However%2C%20these%20methods%20still%20have%20drawbacks%2C%20such%20as%20performance%0Adegradation%20and%20heavy%20communication.%20This%20paper%20introduces%20SliceGCN%2C%20a%0Afeature-sliced%20distributed%20large-scale%20graph%20learning%20method.%20SliceGCN%20slices%0Athe%20node%20features%2C%20with%20each%20computing%20device%2C%20i.e.%2C%20GPU%2C%20handling%20partial%0Afeatures.%20After%20each%20GPU%20processes%20its%20share%2C%20partial%20representations%20are%0Aobtained%20and%20concatenated%20to%20form%20complete%20representations%2C%20enabling%20a%20single%0AGPU%27s%20memory%20to%20handle%20the%20entire%20graph%20structure.%20This%20aims%20to%20avoid%20the%0Aaccuracy%20loss%20typically%20associated%20with%20mini-batch%20training%20%28due%20to%20incomplete%0Agraph%20structures%29%20and%20to%20reduce%20inter-GPU%20communication%20during%20message%20passing%0A%28the%20forward%20propagation%20process%20of%20GNNs%29.%20To%20study%20and%20mitigate%20potential%0Aaccuracy%20reductions%20due%20to%20slicing%20features%2C%20this%20paper%20proposes%20feature%20fusion%0Aand%20slice%20encoding.%20Experiments%20were%20conducted%20on%20six%20node%20classification%0Adatasets%2C%20yielding%20some%20interesting%20analytical%20results.%20These%20results%20indicate%0Athat%20while%20SliceGCN%20does%20not%20enhance%20efficiency%20on%20smaller%20datasets%2C%20it%20does%0Aimprove%20efficiency%20on%20larger%20datasets.%20Additionally%2C%20we%20found%20that%20SliceGCN%20and%0Aits%20variants%20have%20better%20convergence%2C%20feature%20fusion%20and%20slice%20encoding%20can%0Amake%20training%20more%20stable%2C%20reduce%20accuracy%20fluctuations%2C%20and%20this%20study%20also%0Adiscovered%20that%20the%20design%20of%20SliceGCN%20has%20a%20potentially%20parameter-efficient%0Anature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlicing%2520Input%2520Features%2520to%2520Accelerate%2520Deep%2520Learning%253A%2520A%2520Case%2520Study%2520with%250A%2520%2520Graph%2520Neural%2520Networks%26entry.906535625%3DZhengjia%2520Xu%2520and%2520Dingyang%2520Lyu%2520and%2520Jinghui%2520Zhang%26entry.1292438233%3D%2520%2520As%2520graphs%2520grow%2520larger%252C%2520full-batch%2520GNN%2520training%2520becomes%2520hard%2520for%2520single%2520GPU%250Amemory.%2520Therefore%252C%2520to%2520enhance%2520the%2520scalability%2520of%2520GNN%2520training%252C%2520some%2520studies%250Ahave%2520proposed%2520sampling-based%2520mini-batch%2520training%2520and%2520distributed%2520graph%250Alearning.%2520However%252C%2520these%2520methods%2520still%2520have%2520drawbacks%252C%2520such%2520as%2520performance%250Adegradation%2520and%2520heavy%2520communication.%2520This%2520paper%2520introduces%2520SliceGCN%252C%2520a%250Afeature-sliced%2520distributed%2520large-scale%2520graph%2520learning%2520method.%2520SliceGCN%2520slices%250Athe%2520node%2520features%252C%2520with%2520each%2520computing%2520device%252C%2520i.e.%252C%2520GPU%252C%2520handling%2520partial%250Afeatures.%2520After%2520each%2520GPU%2520processes%2520its%2520share%252C%2520partial%2520representations%2520are%250Aobtained%2520and%2520concatenated%2520to%2520form%2520complete%2520representations%252C%2520enabling%2520a%2520single%250AGPU%2527s%2520memory%2520to%2520handle%2520the%2520entire%2520graph%2520structure.%2520This%2520aims%2520to%2520avoid%2520the%250Aaccuracy%2520loss%2520typically%2520associated%2520with%2520mini-batch%2520training%2520%2528due%2520to%2520incomplete%250Agraph%2520structures%2529%2520and%2520to%2520reduce%2520inter-GPU%2520communication%2520during%2520message%2520passing%250A%2528the%2520forward%2520propagation%2520process%2520of%2520GNNs%2529.%2520To%2520study%2520and%2520mitigate%2520potential%250Aaccuracy%2520reductions%2520due%2520to%2520slicing%2520features%252C%2520this%2520paper%2520proposes%2520feature%2520fusion%250Aand%2520slice%2520encoding.%2520Experiments%2520were%2520conducted%2520on%2520six%2520node%2520classification%250Adatasets%252C%2520yielding%2520some%2520interesting%2520analytical%2520results.%2520These%2520results%2520indicate%250Athat%2520while%2520SliceGCN%2520does%2520not%2520enhance%2520efficiency%2520on%2520smaller%2520datasets%252C%2520it%2520does%250Aimprove%2520efficiency%2520on%2520larger%2520datasets.%2520Additionally%252C%2520we%2520found%2520that%2520SliceGCN%2520and%250Aits%2520variants%2520have%2520better%2520convergence%252C%2520feature%2520fusion%2520and%2520slice%2520encoding%2520can%250Amake%2520training%2520more%2520stable%252C%2520reduce%2520accuracy%2520fluctuations%252C%2520and%2520this%2520study%2520also%250Adiscovered%2520that%2520the%2520design%2520of%2520SliceGCN%2520has%2520a%2520potentially%2520parameter-efficient%250Anature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slicing%20Input%20Features%20to%20Accelerate%20Deep%20Learning%3A%20A%20Case%20Study%20with%0A%20%20Graph%20Neural%20Networks&entry.906535625=Zhengjia%20Xu%20and%20Dingyang%20Lyu%20and%20Jinghui%20Zhang&entry.1292438233=%20%20As%20graphs%20grow%20larger%2C%20full-batch%20GNN%20training%20becomes%20hard%20for%20single%20GPU%0Amemory.%20Therefore%2C%20to%20enhance%20the%20scalability%20of%20GNN%20training%2C%20some%20studies%0Ahave%20proposed%20sampling-based%20mini-batch%20training%20and%20distributed%20graph%0Alearning.%20However%2C%20these%20methods%20still%20have%20drawbacks%2C%20such%20as%20performance%0Adegradation%20and%20heavy%20communication.%20This%20paper%20introduces%20SliceGCN%2C%20a%0Afeature-sliced%20distributed%20large-scale%20graph%20learning%20method.%20SliceGCN%20slices%0Athe%20node%20features%2C%20with%20each%20computing%20device%2C%20i.e.%2C%20GPU%2C%20handling%20partial%0Afeatures.%20After%20each%20GPU%20processes%20its%20share%2C%20partial%20representations%20are%0Aobtained%20and%20concatenated%20to%20form%20complete%20representations%2C%20enabling%20a%20single%0AGPU%27s%20memory%20to%20handle%20the%20entire%20graph%20structure.%20This%20aims%20to%20avoid%20the%0Aaccuracy%20loss%20typically%20associated%20with%20mini-batch%20training%20%28due%20to%20incomplete%0Agraph%20structures%29%20and%20to%20reduce%20inter-GPU%20communication%20during%20message%20passing%0A%28the%20forward%20propagation%20process%20of%20GNNs%29.%20To%20study%20and%20mitigate%20potential%0Aaccuracy%20reductions%20due%20to%20slicing%20features%2C%20this%20paper%20proposes%20feature%20fusion%0Aand%20slice%20encoding.%20Experiments%20were%20conducted%20on%20six%20node%20classification%0Adatasets%2C%20yielding%20some%20interesting%20analytical%20results.%20These%20results%20indicate%0Athat%20while%20SliceGCN%20does%20not%20enhance%20efficiency%20on%20smaller%20datasets%2C%20it%20does%0Aimprove%20efficiency%20on%20larger%20datasets.%20Additionally%2C%20we%20found%20that%20SliceGCN%20and%0Aits%20variants%20have%20better%20convergence%2C%20feature%20fusion%20and%20slice%20encoding%20can%0Amake%20training%20more%20stable%2C%20reduce%20accuracy%20fluctuations%2C%20and%20this%20study%20also%0Adiscovered%20that%20the%20design%20of%20SliceGCN%20has%20a%20potentially%20parameter-efficient%0Anature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11500v1&entry.124074799=Read"},
{"title": "Accelerating Hopfield Network Dynamics: Beyond Synchronous Updates and\n  Forward Euler", "author": "C\u00e9dric Goemaere and Johannes Deleu and Thomas Demeester", "abstract": "  The Hopfield network serves as a fundamental energy-based model in machine\nlearning, capturing memory retrieval dynamics through an ordinary differential\nequation (ODE). The model's output, the equilibrium point of the ODE, is\ntraditionally computed via synchronous updates using the forward Euler method.\nThis paper aims to overcome some of the disadvantages of this approach. We\npropose a conceptual shift, viewing Hopfield networks as instances of Deep\nEquilibrium Models (DEQs). The DEQ framework not only allows for the use of\nspecialized solvers, but also leads to new insights on an empirical inference\ntechnique that we will refer to as 'even-odd splitting'. Our theoretical\nanalysis of the method uncovers a parallelizable asynchronous update scheme,\nwhich should converge roughly twice as fast as the conventional synchronous\nupdates. Empirical evaluations validate these findings, showcasing the\nadvantages of both the DEQ framework and even-odd splitting in digitally\nsimulating energy minimization in Hopfield networks. The code is available at\nhttps://github.com/cgoemaere/hopdeq\n", "link": "http://arxiv.org/abs/2311.15673v2", "date": "2024-08-21", "relevancy": 2.0172, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5315}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Hopfield%20Network%20Dynamics%3A%20Beyond%20Synchronous%20Updates%20and%0A%20%20Forward%20Euler&body=Title%3A%20Accelerating%20Hopfield%20Network%20Dynamics%3A%20Beyond%20Synchronous%20Updates%20and%0A%20%20Forward%20Euler%0AAuthor%3A%20C%C3%A9dric%20Goemaere%20and%20Johannes%20Deleu%20and%20Thomas%20Demeester%0AAbstract%3A%20%20%20The%20Hopfield%20network%20serves%20as%20a%20fundamental%20energy-based%20model%20in%20machine%0Alearning%2C%20capturing%20memory%20retrieval%20dynamics%20through%20an%20ordinary%20differential%0Aequation%20%28ODE%29.%20The%20model%27s%20output%2C%20the%20equilibrium%20point%20of%20the%20ODE%2C%20is%0Atraditionally%20computed%20via%20synchronous%20updates%20using%20the%20forward%20Euler%20method.%0AThis%20paper%20aims%20to%20overcome%20some%20of%20the%20disadvantages%20of%20this%20approach.%20We%0Apropose%20a%20conceptual%20shift%2C%20viewing%20Hopfield%20networks%20as%20instances%20of%20Deep%0AEquilibrium%20Models%20%28DEQs%29.%20The%20DEQ%20framework%20not%20only%20allows%20for%20the%20use%20of%0Aspecialized%20solvers%2C%20but%20also%20leads%20to%20new%20insights%20on%20an%20empirical%20inference%0Atechnique%20that%20we%20will%20refer%20to%20as%20%27even-odd%20splitting%27.%20Our%20theoretical%0Aanalysis%20of%20the%20method%20uncovers%20a%20parallelizable%20asynchronous%20update%20scheme%2C%0Awhich%20should%20converge%20roughly%20twice%20as%20fast%20as%20the%20conventional%20synchronous%0Aupdates.%20Empirical%20evaluations%20validate%20these%20findings%2C%20showcasing%20the%0Aadvantages%20of%20both%20the%20DEQ%20framework%20and%20even-odd%20splitting%20in%20digitally%0Asimulating%20energy%20minimization%20in%20Hopfield%20networks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/cgoemaere/hopdeq%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Hopfield%2520Network%2520Dynamics%253A%2520Beyond%2520Synchronous%2520Updates%2520and%250A%2520%2520Forward%2520Euler%26entry.906535625%3DC%25C3%25A9dric%2520Goemaere%2520and%2520Johannes%2520Deleu%2520and%2520Thomas%2520Demeester%26entry.1292438233%3D%2520%2520The%2520Hopfield%2520network%2520serves%2520as%2520a%2520fundamental%2520energy-based%2520model%2520in%2520machine%250Alearning%252C%2520capturing%2520memory%2520retrieval%2520dynamics%2520through%2520an%2520ordinary%2520differential%250Aequation%2520%2528ODE%2529.%2520The%2520model%2527s%2520output%252C%2520the%2520equilibrium%2520point%2520of%2520the%2520ODE%252C%2520is%250Atraditionally%2520computed%2520via%2520synchronous%2520updates%2520using%2520the%2520forward%2520Euler%2520method.%250AThis%2520paper%2520aims%2520to%2520overcome%2520some%2520of%2520the%2520disadvantages%2520of%2520this%2520approach.%2520We%250Apropose%2520a%2520conceptual%2520shift%252C%2520viewing%2520Hopfield%2520networks%2520as%2520instances%2520of%2520Deep%250AEquilibrium%2520Models%2520%2528DEQs%2529.%2520The%2520DEQ%2520framework%2520not%2520only%2520allows%2520for%2520the%2520use%2520of%250Aspecialized%2520solvers%252C%2520but%2520also%2520leads%2520to%2520new%2520insights%2520on%2520an%2520empirical%2520inference%250Atechnique%2520that%2520we%2520will%2520refer%2520to%2520as%2520%2527even-odd%2520splitting%2527.%2520Our%2520theoretical%250Aanalysis%2520of%2520the%2520method%2520uncovers%2520a%2520parallelizable%2520asynchronous%2520update%2520scheme%252C%250Awhich%2520should%2520converge%2520roughly%2520twice%2520as%2520fast%2520as%2520the%2520conventional%2520synchronous%250Aupdates.%2520Empirical%2520evaluations%2520validate%2520these%2520findings%252C%2520showcasing%2520the%250Aadvantages%2520of%2520both%2520the%2520DEQ%2520framework%2520and%2520even-odd%2520splitting%2520in%2520digitally%250Asimulating%2520energy%2520minimization%2520in%2520Hopfield%2520networks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/cgoemaere/hopdeq%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Hopfield%20Network%20Dynamics%3A%20Beyond%20Synchronous%20Updates%20and%0A%20%20Forward%20Euler&entry.906535625=C%C3%A9dric%20Goemaere%20and%20Johannes%20Deleu%20and%20Thomas%20Demeester&entry.1292438233=%20%20The%20Hopfield%20network%20serves%20as%20a%20fundamental%20energy-based%20model%20in%20machine%0Alearning%2C%20capturing%20memory%20retrieval%20dynamics%20through%20an%20ordinary%20differential%0Aequation%20%28ODE%29.%20The%20model%27s%20output%2C%20the%20equilibrium%20point%20of%20the%20ODE%2C%20is%0Atraditionally%20computed%20via%20synchronous%20updates%20using%20the%20forward%20Euler%20method.%0AThis%20paper%20aims%20to%20overcome%20some%20of%20the%20disadvantages%20of%20this%20approach.%20We%0Apropose%20a%20conceptual%20shift%2C%20viewing%20Hopfield%20networks%20as%20instances%20of%20Deep%0AEquilibrium%20Models%20%28DEQs%29.%20The%20DEQ%20framework%20not%20only%20allows%20for%20the%20use%20of%0Aspecialized%20solvers%2C%20but%20also%20leads%20to%20new%20insights%20on%20an%20empirical%20inference%0Atechnique%20that%20we%20will%20refer%20to%20as%20%27even-odd%20splitting%27.%20Our%20theoretical%0Aanalysis%20of%20the%20method%20uncovers%20a%20parallelizable%20asynchronous%20update%20scheme%2C%0Awhich%20should%20converge%20roughly%20twice%20as%20fast%20as%20the%20conventional%20synchronous%0Aupdates.%20Empirical%20evaluations%20validate%20these%20findings%2C%20showcasing%20the%0Aadvantages%20of%20both%20the%20DEQ%20framework%20and%20even-odd%20splitting%20in%20digitally%0Asimulating%20energy%20minimization%20in%20Hopfield%20networks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/cgoemaere/hopdeq%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15673v2&entry.124074799=Read"},
{"title": "2-Level Reinforcement Learning for Ships on Inland Waterways: Path\n  Planning and Following", "author": "Martin Waltz and Niklas Paulig and Ostap Okhrin", "abstract": "  This paper proposes a realistic modularized framework for controlling\nautonomous surface vehicles (ASVs) on inland waterways (IWs) based on deep\nreinforcement learning (DRL). The framework improves operational safety and\ncomprises two levels: a high-level local path planning (LPP) unit and a\nlow-level path following (PF) unit, each consisting of a DRL agent. The LPP\nagent is responsible for planning a path under consideration of dynamic\nvessels, closing a gap in the current research landscape. In addition, the LPP\nagent adequately considers traffic rules and the geometry of the waterway. We\nthereby introduce a novel application of a spatial-temporal recurrent neural\nnetwork architecture to continuous action spaces. The LPP agent outperforms a\nstate-of-the-art artificial potential field (APF) method by increasing the\nminimum distance to other vessels by 65% on average. The PF agent performs\nlow-level actuator control while accounting for shallow water influences and\nthe environmental forces winds, waves, and currents. Compared with a\nproportional-integral-derivative (PID) controller, the PF agent yields only 61%\nof the mean cross-track error (MCTE) while significantly reducing control\neffort (CE) in terms of the required absolute rudder angle. Lastly, both agents\nare jointly validated in simulation, employing the lower Elbe in northern\nGermany as an example case and using real automatic identification system (AIS)\ntrajectories to model the behavior of other ships.\n", "link": "http://arxiv.org/abs/2307.16769v3", "date": "2024-08-21", "relevancy": 2.0166, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5194}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5136}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202-Level%20Reinforcement%20Learning%20for%20Ships%20on%20Inland%20Waterways%3A%20Path%0A%20%20Planning%20and%20Following&body=Title%3A%202-Level%20Reinforcement%20Learning%20for%20Ships%20on%20Inland%20Waterways%3A%20Path%0A%20%20Planning%20and%20Following%0AAuthor%3A%20Martin%20Waltz%20and%20Niklas%20Paulig%20and%20Ostap%20Okhrin%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20realistic%20modularized%20framework%20for%20controlling%0Aautonomous%20surface%20vehicles%20%28ASVs%29%20on%20inland%20waterways%20%28IWs%29%20based%20on%20deep%0Areinforcement%20learning%20%28DRL%29.%20The%20framework%20improves%20operational%20safety%20and%0Acomprises%20two%20levels%3A%20a%20high-level%20local%20path%20planning%20%28LPP%29%20unit%20and%20a%0Alow-level%20path%20following%20%28PF%29%20unit%2C%20each%20consisting%20of%20a%20DRL%20agent.%20The%20LPP%0Aagent%20is%20responsible%20for%20planning%20a%20path%20under%20consideration%20of%20dynamic%0Avessels%2C%20closing%20a%20gap%20in%20the%20current%20research%20landscape.%20In%20addition%2C%20the%20LPP%0Aagent%20adequately%20considers%20traffic%20rules%20and%20the%20geometry%20of%20the%20waterway.%20We%0Athereby%20introduce%20a%20novel%20application%20of%20a%20spatial-temporal%20recurrent%20neural%0Anetwork%20architecture%20to%20continuous%20action%20spaces.%20The%20LPP%20agent%20outperforms%20a%0Astate-of-the-art%20artificial%20potential%20field%20%28APF%29%20method%20by%20increasing%20the%0Aminimum%20distance%20to%20other%20vessels%20by%2065%25%20on%20average.%20The%20PF%20agent%20performs%0Alow-level%20actuator%20control%20while%20accounting%20for%20shallow%20water%20influences%20and%0Athe%20environmental%20forces%20winds%2C%20waves%2C%20and%20currents.%20Compared%20with%20a%0Aproportional-integral-derivative%20%28PID%29%20controller%2C%20the%20PF%20agent%20yields%20only%2061%25%0Aof%20the%20mean%20cross-track%20error%20%28MCTE%29%20while%20significantly%20reducing%20control%0Aeffort%20%28CE%29%20in%20terms%20of%20the%20required%20absolute%20rudder%20angle.%20Lastly%2C%20both%20agents%0Aare%20jointly%20validated%20in%20simulation%2C%20employing%20the%20lower%20Elbe%20in%20northern%0AGermany%20as%20an%20example%20case%20and%20using%20real%20automatic%20identification%20system%20%28AIS%29%0Atrajectories%20to%20model%20the%20behavior%20of%20other%20ships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16769v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2-Level%2520Reinforcement%2520Learning%2520for%2520Ships%2520on%2520Inland%2520Waterways%253A%2520Path%250A%2520%2520Planning%2520and%2520Following%26entry.906535625%3DMartin%2520Waltz%2520and%2520Niklas%2520Paulig%2520and%2520Ostap%2520Okhrin%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520realistic%2520modularized%2520framework%2520for%2520controlling%250Aautonomous%2520surface%2520vehicles%2520%2528ASVs%2529%2520on%2520inland%2520waterways%2520%2528IWs%2529%2520based%2520on%2520deep%250Areinforcement%2520learning%2520%2528DRL%2529.%2520The%2520framework%2520improves%2520operational%2520safety%2520and%250Acomprises%2520two%2520levels%253A%2520a%2520high-level%2520local%2520path%2520planning%2520%2528LPP%2529%2520unit%2520and%2520a%250Alow-level%2520path%2520following%2520%2528PF%2529%2520unit%252C%2520each%2520consisting%2520of%2520a%2520DRL%2520agent.%2520The%2520LPP%250Aagent%2520is%2520responsible%2520for%2520planning%2520a%2520path%2520under%2520consideration%2520of%2520dynamic%250Avessels%252C%2520closing%2520a%2520gap%2520in%2520the%2520current%2520research%2520landscape.%2520In%2520addition%252C%2520the%2520LPP%250Aagent%2520adequately%2520considers%2520traffic%2520rules%2520and%2520the%2520geometry%2520of%2520the%2520waterway.%2520We%250Athereby%2520introduce%2520a%2520novel%2520application%2520of%2520a%2520spatial-temporal%2520recurrent%2520neural%250Anetwork%2520architecture%2520to%2520continuous%2520action%2520spaces.%2520The%2520LPP%2520agent%2520outperforms%2520a%250Astate-of-the-art%2520artificial%2520potential%2520field%2520%2528APF%2529%2520method%2520by%2520increasing%2520the%250Aminimum%2520distance%2520to%2520other%2520vessels%2520by%252065%2525%2520on%2520average.%2520The%2520PF%2520agent%2520performs%250Alow-level%2520actuator%2520control%2520while%2520accounting%2520for%2520shallow%2520water%2520influences%2520and%250Athe%2520environmental%2520forces%2520winds%252C%2520waves%252C%2520and%2520currents.%2520Compared%2520with%2520a%250Aproportional-integral-derivative%2520%2528PID%2529%2520controller%252C%2520the%2520PF%2520agent%2520yields%2520only%252061%2525%250Aof%2520the%2520mean%2520cross-track%2520error%2520%2528MCTE%2529%2520while%2520significantly%2520reducing%2520control%250Aeffort%2520%2528CE%2529%2520in%2520terms%2520of%2520the%2520required%2520absolute%2520rudder%2520angle.%2520Lastly%252C%2520both%2520agents%250Aare%2520jointly%2520validated%2520in%2520simulation%252C%2520employing%2520the%2520lower%2520Elbe%2520in%2520northern%250AGermany%2520as%2520an%2520example%2520case%2520and%2520using%2520real%2520automatic%2520identification%2520system%2520%2528AIS%2529%250Atrajectories%2520to%2520model%2520the%2520behavior%2520of%2520other%2520ships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.16769v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2-Level%20Reinforcement%20Learning%20for%20Ships%20on%20Inland%20Waterways%3A%20Path%0A%20%20Planning%20and%20Following&entry.906535625=Martin%20Waltz%20and%20Niklas%20Paulig%20and%20Ostap%20Okhrin&entry.1292438233=%20%20This%20paper%20proposes%20a%20realistic%20modularized%20framework%20for%20controlling%0Aautonomous%20surface%20vehicles%20%28ASVs%29%20on%20inland%20waterways%20%28IWs%29%20based%20on%20deep%0Areinforcement%20learning%20%28DRL%29.%20The%20framework%20improves%20operational%20safety%20and%0Acomprises%20two%20levels%3A%20a%20high-level%20local%20path%20planning%20%28LPP%29%20unit%20and%20a%0Alow-level%20path%20following%20%28PF%29%20unit%2C%20each%20consisting%20of%20a%20DRL%20agent.%20The%20LPP%0Aagent%20is%20responsible%20for%20planning%20a%20path%20under%20consideration%20of%20dynamic%0Avessels%2C%20closing%20a%20gap%20in%20the%20current%20research%20landscape.%20In%20addition%2C%20the%20LPP%0Aagent%20adequately%20considers%20traffic%20rules%20and%20the%20geometry%20of%20the%20waterway.%20We%0Athereby%20introduce%20a%20novel%20application%20of%20a%20spatial-temporal%20recurrent%20neural%0Anetwork%20architecture%20to%20continuous%20action%20spaces.%20The%20LPP%20agent%20outperforms%20a%0Astate-of-the-art%20artificial%20potential%20field%20%28APF%29%20method%20by%20increasing%20the%0Aminimum%20distance%20to%20other%20vessels%20by%2065%25%20on%20average.%20The%20PF%20agent%20performs%0Alow-level%20actuator%20control%20while%20accounting%20for%20shallow%20water%20influences%20and%0Athe%20environmental%20forces%20winds%2C%20waves%2C%20and%20currents.%20Compared%20with%20a%0Aproportional-integral-derivative%20%28PID%29%20controller%2C%20the%20PF%20agent%20yields%20only%2061%25%0Aof%20the%20mean%20cross-track%20error%20%28MCTE%29%20while%20significantly%20reducing%20control%0Aeffort%20%28CE%29%20in%20terms%20of%20the%20required%20absolute%20rudder%20angle.%20Lastly%2C%20both%20agents%0Aare%20jointly%20validated%20in%20simulation%2C%20employing%20the%20lower%20Elbe%20in%20northern%0AGermany%20as%20an%20example%20case%20and%20using%20real%20automatic%20identification%20system%20%28AIS%29%0Atrajectories%20to%20model%20the%20behavior%20of%20other%20ships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16769v3&entry.124074799=Read"},
{"title": "AnyDesign: Versatile Area Fashion Editing via Mask-Free Diffusion", "author": "Yunfang Niu and Lingxiang Wu and Dong Yi and Jie Peng and Ning Jiang and Haiying Wu and Jinqiao Wang", "abstract": "  Fashion image editing aims to modify a person's appearance based on a given\ninstruction. Existing methods require auxiliary tools like segmenters and\nkeypoint extractors, lacking a flexible and unified framework. Moreover, these\nmethods are limited in the variety of clothing types they can handle, as most\ndatasets focus on people in clean backgrounds and only include generic garments\nsuch as tops, pants, and dresses. These limitations restrict their\napplicability in real-world scenarios. In this paper, we first extend an\nexisting dataset for human generation to include a wider range of apparel and\nmore complex backgrounds. This extended dataset features people wearing diverse\nitems such as tops, pants, dresses, skirts, headwear, scarves, shoes, socks,\nand bags. Additionally, we propose AnyDesign, a diffusion-based method that\nenables mask-free editing on versatile areas. Users can simply input a human\nimage along with a corresponding prompt in either text or image format. Our\napproach incorporates Fashion DiT, equipped with a Fashion-Guidance Attention\n(FGA) module designed to fuse explicit apparel types and CLIP-encoded apparel\nfeatures. Both Qualitative and quantitative experiments demonstrate that our\nmethod delivers high-quality fashion editing and outperforms contemporary\ntext-guided fashion editing methods.\n", "link": "http://arxiv.org/abs/2408.11553v1", "date": "2024-08-21", "relevancy": 2.0133, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7707}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6594}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyDesign%3A%20Versatile%20Area%20Fashion%20Editing%20via%20Mask-Free%20Diffusion&body=Title%3A%20AnyDesign%3A%20Versatile%20Area%20Fashion%20Editing%20via%20Mask-Free%20Diffusion%0AAuthor%3A%20Yunfang%20Niu%20and%20Lingxiang%20Wu%20and%20Dong%20Yi%20and%20Jie%20Peng%20and%20Ning%20Jiang%20and%20Haiying%20Wu%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Fashion%20image%20editing%20aims%20to%20modify%20a%20person%27s%20appearance%20based%20on%20a%20given%0Ainstruction.%20Existing%20methods%20require%20auxiliary%20tools%20like%20segmenters%20and%0Akeypoint%20extractors%2C%20lacking%20a%20flexible%20and%20unified%20framework.%20Moreover%2C%20these%0Amethods%20are%20limited%20in%20the%20variety%20of%20clothing%20types%20they%20can%20handle%2C%20as%20most%0Adatasets%20focus%20on%20people%20in%20clean%20backgrounds%20and%20only%20include%20generic%20garments%0Asuch%20as%20tops%2C%20pants%2C%20and%20dresses.%20These%20limitations%20restrict%20their%0Aapplicability%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20first%20extend%20an%0Aexisting%20dataset%20for%20human%20generation%20to%20include%20a%20wider%20range%20of%20apparel%20and%0Amore%20complex%20backgrounds.%20This%20extended%20dataset%20features%20people%20wearing%20diverse%0Aitems%20such%20as%20tops%2C%20pants%2C%20dresses%2C%20skirts%2C%20headwear%2C%20scarves%2C%20shoes%2C%20socks%2C%0Aand%20bags.%20Additionally%2C%20we%20propose%20AnyDesign%2C%20a%20diffusion-based%20method%20that%0Aenables%20mask-free%20editing%20on%20versatile%20areas.%20Users%20can%20simply%20input%20a%20human%0Aimage%20along%20with%20a%20corresponding%20prompt%20in%20either%20text%20or%20image%20format.%20Our%0Aapproach%20incorporates%20Fashion%20DiT%2C%20equipped%20with%20a%20Fashion-Guidance%20Attention%0A%28FGA%29%20module%20designed%20to%20fuse%20explicit%20apparel%20types%20and%20CLIP-encoded%20apparel%0Afeatures.%20Both%20Qualitative%20and%20quantitative%20experiments%20demonstrate%20that%20our%0Amethod%20delivers%20high-quality%20fashion%20editing%20and%20outperforms%20contemporary%0Atext-guided%20fashion%20editing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyDesign%253A%2520Versatile%2520Area%2520Fashion%2520Editing%2520via%2520Mask-Free%2520Diffusion%26entry.906535625%3DYunfang%2520Niu%2520and%2520Lingxiang%2520Wu%2520and%2520Dong%2520Yi%2520and%2520Jie%2520Peng%2520and%2520Ning%2520Jiang%2520and%2520Haiying%2520Wu%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Fashion%2520image%2520editing%2520aims%2520to%2520modify%2520a%2520person%2527s%2520appearance%2520based%2520on%2520a%2520given%250Ainstruction.%2520Existing%2520methods%2520require%2520auxiliary%2520tools%2520like%2520segmenters%2520and%250Akeypoint%2520extractors%252C%2520lacking%2520a%2520flexible%2520and%2520unified%2520framework.%2520Moreover%252C%2520these%250Amethods%2520are%2520limited%2520in%2520the%2520variety%2520of%2520clothing%2520types%2520they%2520can%2520handle%252C%2520as%2520most%250Adatasets%2520focus%2520on%2520people%2520in%2520clean%2520backgrounds%2520and%2520only%2520include%2520generic%2520garments%250Asuch%2520as%2520tops%252C%2520pants%252C%2520and%2520dresses.%2520These%2520limitations%2520restrict%2520their%250Aapplicability%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520first%2520extend%2520an%250Aexisting%2520dataset%2520for%2520human%2520generation%2520to%2520include%2520a%2520wider%2520range%2520of%2520apparel%2520and%250Amore%2520complex%2520backgrounds.%2520This%2520extended%2520dataset%2520features%2520people%2520wearing%2520diverse%250Aitems%2520such%2520as%2520tops%252C%2520pants%252C%2520dresses%252C%2520skirts%252C%2520headwear%252C%2520scarves%252C%2520shoes%252C%2520socks%252C%250Aand%2520bags.%2520Additionally%252C%2520we%2520propose%2520AnyDesign%252C%2520a%2520diffusion-based%2520method%2520that%250Aenables%2520mask-free%2520editing%2520on%2520versatile%2520areas.%2520Users%2520can%2520simply%2520input%2520a%2520human%250Aimage%2520along%2520with%2520a%2520corresponding%2520prompt%2520in%2520either%2520text%2520or%2520image%2520format.%2520Our%250Aapproach%2520incorporates%2520Fashion%2520DiT%252C%2520equipped%2520with%2520a%2520Fashion-Guidance%2520Attention%250A%2528FGA%2529%2520module%2520designed%2520to%2520fuse%2520explicit%2520apparel%2520types%2520and%2520CLIP-encoded%2520apparel%250Afeatures.%2520Both%2520Qualitative%2520and%2520quantitative%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520delivers%2520high-quality%2520fashion%2520editing%2520and%2520outperforms%2520contemporary%250Atext-guided%2520fashion%2520editing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyDesign%3A%20Versatile%20Area%20Fashion%20Editing%20via%20Mask-Free%20Diffusion&entry.906535625=Yunfang%20Niu%20and%20Lingxiang%20Wu%20and%20Dong%20Yi%20and%20Jie%20Peng%20and%20Ning%20Jiang%20and%20Haiying%20Wu%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Fashion%20image%20editing%20aims%20to%20modify%20a%20person%27s%20appearance%20based%20on%20a%20given%0Ainstruction.%20Existing%20methods%20require%20auxiliary%20tools%20like%20segmenters%20and%0Akeypoint%20extractors%2C%20lacking%20a%20flexible%20and%20unified%20framework.%20Moreover%2C%20these%0Amethods%20are%20limited%20in%20the%20variety%20of%20clothing%20types%20they%20can%20handle%2C%20as%20most%0Adatasets%20focus%20on%20people%20in%20clean%20backgrounds%20and%20only%20include%20generic%20garments%0Asuch%20as%20tops%2C%20pants%2C%20and%20dresses.%20These%20limitations%20restrict%20their%0Aapplicability%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20first%20extend%20an%0Aexisting%20dataset%20for%20human%20generation%20to%20include%20a%20wider%20range%20of%20apparel%20and%0Amore%20complex%20backgrounds.%20This%20extended%20dataset%20features%20people%20wearing%20diverse%0Aitems%20such%20as%20tops%2C%20pants%2C%20dresses%2C%20skirts%2C%20headwear%2C%20scarves%2C%20shoes%2C%20socks%2C%0Aand%20bags.%20Additionally%2C%20we%20propose%20AnyDesign%2C%20a%20diffusion-based%20method%20that%0Aenables%20mask-free%20editing%20on%20versatile%20areas.%20Users%20can%20simply%20input%20a%20human%0Aimage%20along%20with%20a%20corresponding%20prompt%20in%20either%20text%20or%20image%20format.%20Our%0Aapproach%20incorporates%20Fashion%20DiT%2C%20equipped%20with%20a%20Fashion-Guidance%20Attention%0A%28FGA%29%20module%20designed%20to%20fuse%20explicit%20apparel%20types%20and%20CLIP-encoded%20apparel%0Afeatures.%20Both%20Qualitative%20and%20quantitative%20experiments%20demonstrate%20that%20our%0Amethod%20delivers%20high-quality%20fashion%20editing%20and%20outperforms%20contemporary%0Atext-guided%20fashion%20editing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11553v1&entry.124074799=Read"},
{"title": "GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models", "author": "Jonathan Roberts and Kai Han and Samuel Albanie", "abstract": "  Large multimodal models (LMMs) have exhibited proficiencies across many\nvisual tasks. Although numerous well-known benchmarks exist to evaluate model\nperformance, they increasingly have insufficient headroom. As such, there is a\npressing need for a new generation of benchmarks challenging enough for the\nnext generation of LMMs. One area that LMMs show potential is graph analysis,\nspecifically, the tasks an analyst might typically perform when interpreting\nfigures such as estimating the mean, intercepts or correlations of functions\nand data series. In this work, we introduce GRAB, a graph analysis benchmark,\nfit for current and future frontier LMMs. Our benchmark is entirely synthetic,\nensuring high-quality, noise-free questions. GRAB is comprised of 2170\nquestions, covering four tasks and 23 graph properties. We evaluate 20 LMMs on\nGRAB, finding it to be a challenging benchmark, with the highest performing\nmodel attaining a score of just 21.7%. Finally, we conduct various ablations to\ninvestigate where the models succeed and struggle. We release GRAB to encourage\nprogress in this important, growing domain.\n", "link": "http://arxiv.org/abs/2408.11817v1", "date": "2024-08-21", "relevancy": 2.0016, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5071}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4959}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models&body=Title%3A%20GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models%0AAuthor%3A%20Jonathan%20Roberts%20and%20Kai%20Han%20and%20Samuel%20Albanie%0AAbstract%3A%20%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20exhibited%20proficiencies%20across%20many%0Avisual%20tasks.%20Although%20numerous%20well-known%20benchmarks%20exist%20to%20evaluate%20model%0Aperformance%2C%20they%20increasingly%20have%20insufficient%20headroom.%20As%20such%2C%20there%20is%20a%0Apressing%20need%20for%20a%20new%20generation%20of%20benchmarks%20challenging%20enough%20for%20the%0Anext%20generation%20of%20LMMs.%20One%20area%20that%20LMMs%20show%20potential%20is%20graph%20analysis%2C%0Aspecifically%2C%20the%20tasks%20an%20analyst%20might%20typically%20perform%20when%20interpreting%0Afigures%20such%20as%20estimating%20the%20mean%2C%20intercepts%20or%20correlations%20of%20functions%0Aand%20data%20series.%20In%20this%20work%2C%20we%20introduce%20GRAB%2C%20a%20graph%20analysis%20benchmark%2C%0Afit%20for%20current%20and%20future%20frontier%20LMMs.%20Our%20benchmark%20is%20entirely%20synthetic%2C%0Aensuring%20high-quality%2C%20noise-free%20questions.%20GRAB%20is%20comprised%20of%202170%0Aquestions%2C%20covering%20four%20tasks%20and%2023%20graph%20properties.%20We%20evaluate%2020%20LMMs%20on%0AGRAB%2C%20finding%20it%20to%20be%20a%20challenging%20benchmark%2C%20with%20the%20highest%20performing%0Amodel%20attaining%20a%20score%20of%20just%2021.7%25.%20Finally%2C%20we%20conduct%20various%20ablations%20to%0Ainvestigate%20where%20the%20models%20succeed%20and%20struggle.%20We%20release%20GRAB%20to%20encourage%0Aprogress%20in%20this%20important%2C%20growing%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAB%253A%2520A%2520Challenging%2520GRaph%2520Analysis%2520Benchmark%2520for%2520Large%2520Multimodal%2520Models%26entry.906535625%3DJonathan%2520Roberts%2520and%2520Kai%2520Han%2520and%2520Samuel%2520Albanie%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520exhibited%2520proficiencies%2520across%2520many%250Avisual%2520tasks.%2520Although%2520numerous%2520well-known%2520benchmarks%2520exist%2520to%2520evaluate%2520model%250Aperformance%252C%2520they%2520increasingly%2520have%2520insufficient%2520headroom.%2520As%2520such%252C%2520there%2520is%2520a%250Apressing%2520need%2520for%2520a%2520new%2520generation%2520of%2520benchmarks%2520challenging%2520enough%2520for%2520the%250Anext%2520generation%2520of%2520LMMs.%2520One%2520area%2520that%2520LMMs%2520show%2520potential%2520is%2520graph%2520analysis%252C%250Aspecifically%252C%2520the%2520tasks%2520an%2520analyst%2520might%2520typically%2520perform%2520when%2520interpreting%250Afigures%2520such%2520as%2520estimating%2520the%2520mean%252C%2520intercepts%2520or%2520correlations%2520of%2520functions%250Aand%2520data%2520series.%2520In%2520this%2520work%252C%2520we%2520introduce%2520GRAB%252C%2520a%2520graph%2520analysis%2520benchmark%252C%250Afit%2520for%2520current%2520and%2520future%2520frontier%2520LMMs.%2520Our%2520benchmark%2520is%2520entirely%2520synthetic%252C%250Aensuring%2520high-quality%252C%2520noise-free%2520questions.%2520GRAB%2520is%2520comprised%2520of%25202170%250Aquestions%252C%2520covering%2520four%2520tasks%2520and%252023%2520graph%2520properties.%2520We%2520evaluate%252020%2520LMMs%2520on%250AGRAB%252C%2520finding%2520it%2520to%2520be%2520a%2520challenging%2520benchmark%252C%2520with%2520the%2520highest%2520performing%250Amodel%2520attaining%2520a%2520score%2520of%2520just%252021.7%2525.%2520Finally%252C%2520we%2520conduct%2520various%2520ablations%2520to%250Ainvestigate%2520where%2520the%2520models%2520succeed%2520and%2520struggle.%2520We%2520release%2520GRAB%2520to%2520encourage%250Aprogress%2520in%2520this%2520important%252C%2520growing%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAB%3A%20A%20Challenging%20GRaph%20Analysis%20Benchmark%20for%20Large%20Multimodal%20Models&entry.906535625=Jonathan%20Roberts%20and%20Kai%20Han%20and%20Samuel%20Albanie&entry.1292438233=%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20exhibited%20proficiencies%20across%20many%0Avisual%20tasks.%20Although%20numerous%20well-known%20benchmarks%20exist%20to%20evaluate%20model%0Aperformance%2C%20they%20increasingly%20have%20insufficient%20headroom.%20As%20such%2C%20there%20is%20a%0Apressing%20need%20for%20a%20new%20generation%20of%20benchmarks%20challenging%20enough%20for%20the%0Anext%20generation%20of%20LMMs.%20One%20area%20that%20LMMs%20show%20potential%20is%20graph%20analysis%2C%0Aspecifically%2C%20the%20tasks%20an%20analyst%20might%20typically%20perform%20when%20interpreting%0Afigures%20such%20as%20estimating%20the%20mean%2C%20intercepts%20or%20correlations%20of%20functions%0Aand%20data%20series.%20In%20this%20work%2C%20we%20introduce%20GRAB%2C%20a%20graph%20analysis%20benchmark%2C%0Afit%20for%20current%20and%20future%20frontier%20LMMs.%20Our%20benchmark%20is%20entirely%20synthetic%2C%0Aensuring%20high-quality%2C%20noise-free%20questions.%20GRAB%20is%20comprised%20of%202170%0Aquestions%2C%20covering%20four%20tasks%20and%2023%20graph%20properties.%20We%20evaluate%2020%20LMMs%20on%0AGRAB%2C%20finding%20it%20to%20be%20a%20challenging%20benchmark%2C%20with%20the%20highest%20performing%0Amodel%20attaining%20a%20score%20of%20just%2021.7%25.%20Finally%2C%20we%20conduct%20various%20ablations%20to%0Ainvestigate%20where%20the%20models%20succeed%20and%20struggle.%20We%20release%20GRAB%20to%20encourage%0Aprogress%20in%20this%20important%2C%20growing%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11817v1&entry.124074799=Read"},
{"title": "MambaCSR: Dual-Interleaved Scanning for Compressed Image\n  Super-Resolution With SSMs", "author": "Yulin Ren and Xin Li and Mengxi Guo and Bingchen Li and Shijie Zhao and Zhibo Chen", "abstract": "  We present MambaCSR, a simple but effective framework based on Mamba for the\nchallenging compressed image super-resolution (CSR) task. Particularly, the\nscanning strategies of Mamba are crucial for effective contextual knowledge\nmodeling in the restoration process despite it relying on selective state space\nmodeling for all tokens. In this work, we propose an efficient dual-interleaved\nscanning paradigm (DIS) for CSR, which is composed of two scanning strategies:\n(i) hierarchical interleaved scanning is designed to comprehensively capture\nand utilize the most potential contextual information within an image by\nsimultaneously taking advantage of the local window-based and sequential\nscanning methods; (ii) horizontal-to-vertical interleaved scanning is proposed\nto reduce the computational cost by leaving the redundancy between the scanning\nof different directions. To overcome the non-uniform compression artifacts, we\nalso propose position-aligned cross-scale scanning to model multi-scale\ncontextual information. Experimental results on multiple benchmarks have shown\nthe great performance of our MambaCSR in the compressed image super-resolution\ntask. The code will be soon available\nin~\\textcolor{magenta}{\\url{https://github.com/renyulin-f/MambaCSR}}.\n", "link": "http://arxiv.org/abs/2408.11758v1", "date": "2024-08-21", "relevancy": 1.985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5089}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaCSR%3A%20Dual-Interleaved%20Scanning%20for%20Compressed%20Image%0A%20%20Super-Resolution%20With%20SSMs&body=Title%3A%20MambaCSR%3A%20Dual-Interleaved%20Scanning%20for%20Compressed%20Image%0A%20%20Super-Resolution%20With%20SSMs%0AAuthor%3A%20Yulin%20Ren%20and%20Xin%20Li%20and%20Mengxi%20Guo%20and%20Bingchen%20Li%20and%20Shijie%20Zhao%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20We%20present%20MambaCSR%2C%20a%20simple%20but%20effective%20framework%20based%20on%20Mamba%20for%20the%0Achallenging%20compressed%20image%20super-resolution%20%28CSR%29%20task.%20Particularly%2C%20the%0Ascanning%20strategies%20of%20Mamba%20are%20crucial%20for%20effective%20contextual%20knowledge%0Amodeling%20in%20the%20restoration%20process%20despite%20it%20relying%20on%20selective%20state%20space%0Amodeling%20for%20all%20tokens.%20In%20this%20work%2C%20we%20propose%20an%20efficient%20dual-interleaved%0Ascanning%20paradigm%20%28DIS%29%20for%20CSR%2C%20which%20is%20composed%20of%20two%20scanning%20strategies%3A%0A%28i%29%20hierarchical%20interleaved%20scanning%20is%20designed%20to%20comprehensively%20capture%0Aand%20utilize%20the%20most%20potential%20contextual%20information%20within%20an%20image%20by%0Asimultaneously%20taking%20advantage%20of%20the%20local%20window-based%20and%20sequential%0Ascanning%20methods%3B%20%28ii%29%20horizontal-to-vertical%20interleaved%20scanning%20is%20proposed%0Ato%20reduce%20the%20computational%20cost%20by%20leaving%20the%20redundancy%20between%20the%20scanning%0Aof%20different%20directions.%20To%20overcome%20the%20non-uniform%20compression%20artifacts%2C%20we%0Aalso%20propose%20position-aligned%20cross-scale%20scanning%20to%20model%20multi-scale%0Acontextual%20information.%20Experimental%20results%20on%20multiple%20benchmarks%20have%20shown%0Athe%20great%20performance%20of%20our%20MambaCSR%20in%20the%20compressed%20image%20super-resolution%0Atask.%20The%20code%20will%20be%20soon%20available%0Ain~%5Ctextcolor%7Bmagenta%7D%7B%5Curl%7Bhttps%3A//github.com/renyulin-f/MambaCSR%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaCSR%253A%2520Dual-Interleaved%2520Scanning%2520for%2520Compressed%2520Image%250A%2520%2520Super-Resolution%2520With%2520SSMs%26entry.906535625%3DYulin%2520Ren%2520and%2520Xin%2520Li%2520and%2520Mengxi%2520Guo%2520and%2520Bingchen%2520Li%2520and%2520Shijie%2520Zhao%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520MambaCSR%252C%2520a%2520simple%2520but%2520effective%2520framework%2520based%2520on%2520Mamba%2520for%2520the%250Achallenging%2520compressed%2520image%2520super-resolution%2520%2528CSR%2529%2520task.%2520Particularly%252C%2520the%250Ascanning%2520strategies%2520of%2520Mamba%2520are%2520crucial%2520for%2520effective%2520contextual%2520knowledge%250Amodeling%2520in%2520the%2520restoration%2520process%2520despite%2520it%2520relying%2520on%2520selective%2520state%2520space%250Amodeling%2520for%2520all%2520tokens.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520efficient%2520dual-interleaved%250Ascanning%2520paradigm%2520%2528DIS%2529%2520for%2520CSR%252C%2520which%2520is%2520composed%2520of%2520two%2520scanning%2520strategies%253A%250A%2528i%2529%2520hierarchical%2520interleaved%2520scanning%2520is%2520designed%2520to%2520comprehensively%2520capture%250Aand%2520utilize%2520the%2520most%2520potential%2520contextual%2520information%2520within%2520an%2520image%2520by%250Asimultaneously%2520taking%2520advantage%2520of%2520the%2520local%2520window-based%2520and%2520sequential%250Ascanning%2520methods%253B%2520%2528ii%2529%2520horizontal-to-vertical%2520interleaved%2520scanning%2520is%2520proposed%250Ato%2520reduce%2520the%2520computational%2520cost%2520by%2520leaving%2520the%2520redundancy%2520between%2520the%2520scanning%250Aof%2520different%2520directions.%2520To%2520overcome%2520the%2520non-uniform%2520compression%2520artifacts%252C%2520we%250Aalso%2520propose%2520position-aligned%2520cross-scale%2520scanning%2520to%2520model%2520multi-scale%250Acontextual%2520information.%2520Experimental%2520results%2520on%2520multiple%2520benchmarks%2520have%2520shown%250Athe%2520great%2520performance%2520of%2520our%2520MambaCSR%2520in%2520the%2520compressed%2520image%2520super-resolution%250Atask.%2520The%2520code%2520will%2520be%2520soon%2520available%250Ain~%255Ctextcolor%257Bmagenta%257D%257B%255Curl%257Bhttps%253A//github.com/renyulin-f/MambaCSR%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaCSR%3A%20Dual-Interleaved%20Scanning%20for%20Compressed%20Image%0A%20%20Super-Resolution%20With%20SSMs&entry.906535625=Yulin%20Ren%20and%20Xin%20Li%20and%20Mengxi%20Guo%20and%20Bingchen%20Li%20and%20Shijie%20Zhao%20and%20Zhibo%20Chen&entry.1292438233=%20%20We%20present%20MambaCSR%2C%20a%20simple%20but%20effective%20framework%20based%20on%20Mamba%20for%20the%0Achallenging%20compressed%20image%20super-resolution%20%28CSR%29%20task.%20Particularly%2C%20the%0Ascanning%20strategies%20of%20Mamba%20are%20crucial%20for%20effective%20contextual%20knowledge%0Amodeling%20in%20the%20restoration%20process%20despite%20it%20relying%20on%20selective%20state%20space%0Amodeling%20for%20all%20tokens.%20In%20this%20work%2C%20we%20propose%20an%20efficient%20dual-interleaved%0Ascanning%20paradigm%20%28DIS%29%20for%20CSR%2C%20which%20is%20composed%20of%20two%20scanning%20strategies%3A%0A%28i%29%20hierarchical%20interleaved%20scanning%20is%20designed%20to%20comprehensively%20capture%0Aand%20utilize%20the%20most%20potential%20contextual%20information%20within%20an%20image%20by%0Asimultaneously%20taking%20advantage%20of%20the%20local%20window-based%20and%20sequential%0Ascanning%20methods%3B%20%28ii%29%20horizontal-to-vertical%20interleaved%20scanning%20is%20proposed%0Ato%20reduce%20the%20computational%20cost%20by%20leaving%20the%20redundancy%20between%20the%20scanning%0Aof%20different%20directions.%20To%20overcome%20the%20non-uniform%20compression%20artifacts%2C%20we%0Aalso%20propose%20position-aligned%20cross-scale%20scanning%20to%20model%20multi-scale%0Acontextual%20information.%20Experimental%20results%20on%20multiple%20benchmarks%20have%20shown%0Athe%20great%20performance%20of%20our%20MambaCSR%20in%20the%20compressed%20image%20super-resolution%0Atask.%20The%20code%20will%20be%20soon%20available%0Ain~%5Ctextcolor%7Bmagenta%7D%7B%5Curl%7Bhttps%3A//github.com/renyulin-f/MambaCSR%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11758v1&entry.124074799=Read"},
{"title": "S4Sleep: Elucidating the design space of deep-learning-based sleep stage\n  classification models", "author": "Tiezhi Wang and Nils Strodthoff", "abstract": "  Scoring sleep stages in polysomnography recordings is a time-consuming task\nplagued by significant inter-rater variability. Therefore, it stands to benefit\nfrom the application of machine learning algorithms. While many algorithms have\nbeen proposed for this purpose, certain critical architectural decisions have\nnot received systematic exploration. In this study, we meticulously investigate\nthese design choices within the broad category of encoder-predictor\narchitectures. We identify robust architectures applicable to both time series\nand spectrogram input representations. These architectures incorporate\nstructured state space models as integral components and achieve statistically\nsignificant performance improvements compared to state-of-the-art approaches on\nthe extensive Sleep Heart Health Study dataset. We anticipate that the\narchitectural insights gained from this study along with the refined\nmethodology for architecture search demonstrated herein will not only prove\nvaluable for future research in sleep staging but also hold relevance for other\ntime series annotation tasks.\n", "link": "http://arxiv.org/abs/2310.06715v2", "date": "2024-08-21", "relevancy": 1.9846, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5071}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4885}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S4Sleep%3A%20Elucidating%20the%20design%20space%20of%20deep-learning-based%20sleep%20stage%0A%20%20classification%20models&body=Title%3A%20S4Sleep%3A%20Elucidating%20the%20design%20space%20of%20deep-learning-based%20sleep%20stage%0A%20%20classification%20models%0AAuthor%3A%20Tiezhi%20Wang%20and%20Nils%20Strodthoff%0AAbstract%3A%20%20%20Scoring%20sleep%20stages%20in%20polysomnography%20recordings%20is%20a%20time-consuming%20task%0Aplagued%20by%20significant%20inter-rater%20variability.%20Therefore%2C%20it%20stands%20to%20benefit%0Afrom%20the%20application%20of%20machine%20learning%20algorithms.%20While%20many%20algorithms%20have%0Abeen%20proposed%20for%20this%20purpose%2C%20certain%20critical%20architectural%20decisions%20have%0Anot%20received%20systematic%20exploration.%20In%20this%20study%2C%20we%20meticulously%20investigate%0Athese%20design%20choices%20within%20the%20broad%20category%20of%20encoder-predictor%0Aarchitectures.%20We%20identify%20robust%20architectures%20applicable%20to%20both%20time%20series%0Aand%20spectrogram%20input%20representations.%20These%20architectures%20incorporate%0Astructured%20state%20space%20models%20as%20integral%20components%20and%20achieve%20statistically%0Asignificant%20performance%20improvements%20compared%20to%20state-of-the-art%20approaches%20on%0Athe%20extensive%20Sleep%20Heart%20Health%20Study%20dataset.%20We%20anticipate%20that%20the%0Aarchitectural%20insights%20gained%20from%20this%20study%20along%20with%20the%20refined%0Amethodology%20for%20architecture%20search%20demonstrated%20herein%20will%20not%20only%20prove%0Avaluable%20for%20future%20research%20in%20sleep%20staging%20but%20also%20hold%20relevance%20for%20other%0Atime%20series%20annotation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS4Sleep%253A%2520Elucidating%2520the%2520design%2520space%2520of%2520deep-learning-based%2520sleep%2520stage%250A%2520%2520classification%2520models%26entry.906535625%3DTiezhi%2520Wang%2520and%2520Nils%2520Strodthoff%26entry.1292438233%3D%2520%2520Scoring%2520sleep%2520stages%2520in%2520polysomnography%2520recordings%2520is%2520a%2520time-consuming%2520task%250Aplagued%2520by%2520significant%2520inter-rater%2520variability.%2520Therefore%252C%2520it%2520stands%2520to%2520benefit%250Afrom%2520the%2520application%2520of%2520machine%2520learning%2520algorithms.%2520While%2520many%2520algorithms%2520have%250Abeen%2520proposed%2520for%2520this%2520purpose%252C%2520certain%2520critical%2520architectural%2520decisions%2520have%250Anot%2520received%2520systematic%2520exploration.%2520In%2520this%2520study%252C%2520we%2520meticulously%2520investigate%250Athese%2520design%2520choices%2520within%2520the%2520broad%2520category%2520of%2520encoder-predictor%250Aarchitectures.%2520We%2520identify%2520robust%2520architectures%2520applicable%2520to%2520both%2520time%2520series%250Aand%2520spectrogram%2520input%2520representations.%2520These%2520architectures%2520incorporate%250Astructured%2520state%2520space%2520models%2520as%2520integral%2520components%2520and%2520achieve%2520statistically%250Asignificant%2520performance%2520improvements%2520compared%2520to%2520state-of-the-art%2520approaches%2520on%250Athe%2520extensive%2520Sleep%2520Heart%2520Health%2520Study%2520dataset.%2520We%2520anticipate%2520that%2520the%250Aarchitectural%2520insights%2520gained%2520from%2520this%2520study%2520along%2520with%2520the%2520refined%250Amethodology%2520for%2520architecture%2520search%2520demonstrated%2520herein%2520will%2520not%2520only%2520prove%250Avaluable%2520for%2520future%2520research%2520in%2520sleep%2520staging%2520but%2520also%2520hold%2520relevance%2520for%2520other%250Atime%2520series%2520annotation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S4Sleep%3A%20Elucidating%20the%20design%20space%20of%20deep-learning-based%20sleep%20stage%0A%20%20classification%20models&entry.906535625=Tiezhi%20Wang%20and%20Nils%20Strodthoff&entry.1292438233=%20%20Scoring%20sleep%20stages%20in%20polysomnography%20recordings%20is%20a%20time-consuming%20task%0Aplagued%20by%20significant%20inter-rater%20variability.%20Therefore%2C%20it%20stands%20to%20benefit%0Afrom%20the%20application%20of%20machine%20learning%20algorithms.%20While%20many%20algorithms%20have%0Abeen%20proposed%20for%20this%20purpose%2C%20certain%20critical%20architectural%20decisions%20have%0Anot%20received%20systematic%20exploration.%20In%20this%20study%2C%20we%20meticulously%20investigate%0Athese%20design%20choices%20within%20the%20broad%20category%20of%20encoder-predictor%0Aarchitectures.%20We%20identify%20robust%20architectures%20applicable%20to%20both%20time%20series%0Aand%20spectrogram%20input%20representations.%20These%20architectures%20incorporate%0Astructured%20state%20space%20models%20as%20integral%20components%20and%20achieve%20statistically%0Asignificant%20performance%20improvements%20compared%20to%20state-of-the-art%20approaches%20on%0Athe%20extensive%20Sleep%20Heart%20Health%20Study%20dataset.%20We%20anticipate%20that%20the%0Aarchitectural%20insights%20gained%20from%20this%20study%20along%20with%20the%20refined%0Amethodology%20for%20architecture%20search%20demonstrated%20herein%20will%20not%20only%20prove%0Avaluable%20for%20future%20research%20in%20sleep%20staging%20but%20also%20hold%20relevance%20for%20other%0Atime%20series%20annotation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06715v2&entry.124074799=Read"},
{"title": "LARR: Large Language Model Aided Real-time Scene Recommendation with\n  Semantic Understanding", "author": "Zhizhong Wan and Bin Yin and Junjie Xie and Fei Jiang and Xiang Li and Wei Lin", "abstract": "  Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS),\naiming to provide personalized recommendation services for users in many\naspects such as food delivery, e-commerce and so on. However, traditional RS\nrelies on collaborative signals, which lacks semantic understanding to\nreal-time scenes. We also noticed that a major challenge in utilizing Large\nLanguage Models (LLMs) for practical recommendation purposes is their\nefficiency in dealing with long text input. To break through the problems\nabove, we propose Large Language Model Aided Real-time Scene\nRecommendation(LARR), adopt LLMs for semantic understanding, utilizing\nreal-time scene information in RS without requiring LLM to process the entire\nreal-time scene text directly, thereby enhancing the efficiency of LLM-based\nCTR modeling. Specifically, recommendation domain-specific knowledge is\ninjected into LLM and then RS employs an aggregation encoder to build real-time\nscene information from separate LLM's outputs. Firstly, a LLM is continual\npretrained on corpus built from recommendation data with the aid of special\ntokens. Subsequently, the LLM is fine-tuned via contrastive learning on three\nkinds of sample construction strategies. Through this step, LLM is transformed\ninto a text embedding model. Finally, LLM's separate outputs for different\nscene features are aggregated by an encoder, aligning to collaborative signals\nin RS, enhancing the performance of recommendation model.\n", "link": "http://arxiv.org/abs/2408.11523v1", "date": "2024-08-21", "relevancy": 1.9739, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4994}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4957}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LARR%3A%20Large%20Language%20Model%20Aided%20Real-time%20Scene%20Recommendation%20with%0A%20%20Semantic%20Understanding&body=Title%3A%20LARR%3A%20Large%20Language%20Model%20Aided%20Real-time%20Scene%20Recommendation%20with%0A%20%20Semantic%20Understanding%0AAuthor%3A%20Zhizhong%20Wan%20and%20Bin%20Yin%20and%20Junjie%20Xie%20and%20Fei%20Jiang%20and%20Xiang%20Li%20and%20Wei%20Lin%0AAbstract%3A%20%20%20Click-Through%20Rate%20%28CTR%29%20prediction%20is%20crucial%20for%20Recommendation%20System%28RS%29%2C%0Aaiming%20to%20provide%20personalized%20recommendation%20services%20for%20users%20in%20many%0Aaspects%20such%20as%20food%20delivery%2C%20e-commerce%20and%20so%20on.%20However%2C%20traditional%20RS%0Arelies%20on%20collaborative%20signals%2C%20which%20lacks%20semantic%20understanding%20to%0Areal-time%20scenes.%20We%20also%20noticed%20that%20a%20major%20challenge%20in%20utilizing%20Large%0ALanguage%20Models%20%28LLMs%29%20for%20practical%20recommendation%20purposes%20is%20their%0Aefficiency%20in%20dealing%20with%20long%20text%20input.%20To%20break%20through%20the%20problems%0Aabove%2C%20we%20propose%20Large%20Language%20Model%20Aided%20Real-time%20Scene%0ARecommendation%28LARR%29%2C%20adopt%20LLMs%20for%20semantic%20understanding%2C%20utilizing%0Areal-time%20scene%20information%20in%20RS%20without%20requiring%20LLM%20to%20process%20the%20entire%0Areal-time%20scene%20text%20directly%2C%20thereby%20enhancing%20the%20efficiency%20of%20LLM-based%0ACTR%20modeling.%20Specifically%2C%20recommendation%20domain-specific%20knowledge%20is%0Ainjected%20into%20LLM%20and%20then%20RS%20employs%20an%20aggregation%20encoder%20to%20build%20real-time%0Ascene%20information%20from%20separate%20LLM%27s%20outputs.%20Firstly%2C%20a%20LLM%20is%20continual%0Apretrained%20on%20corpus%20built%20from%20recommendation%20data%20with%20the%20aid%20of%20special%0Atokens.%20Subsequently%2C%20the%20LLM%20is%20fine-tuned%20via%20contrastive%20learning%20on%20three%0Akinds%20of%20sample%20construction%20strategies.%20Through%20this%20step%2C%20LLM%20is%20transformed%0Ainto%20a%20text%20embedding%20model.%20Finally%2C%20LLM%27s%20separate%20outputs%20for%20different%0Ascene%20features%20are%20aggregated%20by%20an%20encoder%2C%20aligning%20to%20collaborative%20signals%0Ain%20RS%2C%20enhancing%20the%20performance%20of%20recommendation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLARR%253A%2520Large%2520Language%2520Model%2520Aided%2520Real-time%2520Scene%2520Recommendation%2520with%250A%2520%2520Semantic%2520Understanding%26entry.906535625%3DZhizhong%2520Wan%2520and%2520Bin%2520Yin%2520and%2520Junjie%2520Xie%2520and%2520Fei%2520Jiang%2520and%2520Xiang%2520Li%2520and%2520Wei%2520Lin%26entry.1292438233%3D%2520%2520Click-Through%2520Rate%2520%2528CTR%2529%2520prediction%2520is%2520crucial%2520for%2520Recommendation%2520System%2528RS%2529%252C%250Aaiming%2520to%2520provide%2520personalized%2520recommendation%2520services%2520for%2520users%2520in%2520many%250Aaspects%2520such%2520as%2520food%2520delivery%252C%2520e-commerce%2520and%2520so%2520on.%2520However%252C%2520traditional%2520RS%250Arelies%2520on%2520collaborative%2520signals%252C%2520which%2520lacks%2520semantic%2520understanding%2520to%250Areal-time%2520scenes.%2520We%2520also%2520noticed%2520that%2520a%2520major%2520challenge%2520in%2520utilizing%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520for%2520practical%2520recommendation%2520purposes%2520is%2520their%250Aefficiency%2520in%2520dealing%2520with%2520long%2520text%2520input.%2520To%2520break%2520through%2520the%2520problems%250Aabove%252C%2520we%2520propose%2520Large%2520Language%2520Model%2520Aided%2520Real-time%2520Scene%250ARecommendation%2528LARR%2529%252C%2520adopt%2520LLMs%2520for%2520semantic%2520understanding%252C%2520utilizing%250Areal-time%2520scene%2520information%2520in%2520RS%2520without%2520requiring%2520LLM%2520to%2520process%2520the%2520entire%250Areal-time%2520scene%2520text%2520directly%252C%2520thereby%2520enhancing%2520the%2520efficiency%2520of%2520LLM-based%250ACTR%2520modeling.%2520Specifically%252C%2520recommendation%2520domain-specific%2520knowledge%2520is%250Ainjected%2520into%2520LLM%2520and%2520then%2520RS%2520employs%2520an%2520aggregation%2520encoder%2520to%2520build%2520real-time%250Ascene%2520information%2520from%2520separate%2520LLM%2527s%2520outputs.%2520Firstly%252C%2520a%2520LLM%2520is%2520continual%250Apretrained%2520on%2520corpus%2520built%2520from%2520recommendation%2520data%2520with%2520the%2520aid%2520of%2520special%250Atokens.%2520Subsequently%252C%2520the%2520LLM%2520is%2520fine-tuned%2520via%2520contrastive%2520learning%2520on%2520three%250Akinds%2520of%2520sample%2520construction%2520strategies.%2520Through%2520this%2520step%252C%2520LLM%2520is%2520transformed%250Ainto%2520a%2520text%2520embedding%2520model.%2520Finally%252C%2520LLM%2527s%2520separate%2520outputs%2520for%2520different%250Ascene%2520features%2520are%2520aggregated%2520by%2520an%2520encoder%252C%2520aligning%2520to%2520collaborative%2520signals%250Ain%2520RS%252C%2520enhancing%2520the%2520performance%2520of%2520recommendation%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LARR%3A%20Large%20Language%20Model%20Aided%20Real-time%20Scene%20Recommendation%20with%0A%20%20Semantic%20Understanding&entry.906535625=Zhizhong%20Wan%20and%20Bin%20Yin%20and%20Junjie%20Xie%20and%20Fei%20Jiang%20and%20Xiang%20Li%20and%20Wei%20Lin&entry.1292438233=%20%20Click-Through%20Rate%20%28CTR%29%20prediction%20is%20crucial%20for%20Recommendation%20System%28RS%29%2C%0Aaiming%20to%20provide%20personalized%20recommendation%20services%20for%20users%20in%20many%0Aaspects%20such%20as%20food%20delivery%2C%20e-commerce%20and%20so%20on.%20However%2C%20traditional%20RS%0Arelies%20on%20collaborative%20signals%2C%20which%20lacks%20semantic%20understanding%20to%0Areal-time%20scenes.%20We%20also%20noticed%20that%20a%20major%20challenge%20in%20utilizing%20Large%0ALanguage%20Models%20%28LLMs%29%20for%20practical%20recommendation%20purposes%20is%20their%0Aefficiency%20in%20dealing%20with%20long%20text%20input.%20To%20break%20through%20the%20problems%0Aabove%2C%20we%20propose%20Large%20Language%20Model%20Aided%20Real-time%20Scene%0ARecommendation%28LARR%29%2C%20adopt%20LLMs%20for%20semantic%20understanding%2C%20utilizing%0Areal-time%20scene%20information%20in%20RS%20without%20requiring%20LLM%20to%20process%20the%20entire%0Areal-time%20scene%20text%20directly%2C%20thereby%20enhancing%20the%20efficiency%20of%20LLM-based%0ACTR%20modeling.%20Specifically%2C%20recommendation%20domain-specific%20knowledge%20is%0Ainjected%20into%20LLM%20and%20then%20RS%20employs%20an%20aggregation%20encoder%20to%20build%20real-time%0Ascene%20information%20from%20separate%20LLM%27s%20outputs.%20Firstly%2C%20a%20LLM%20is%20continual%0Apretrained%20on%20corpus%20built%20from%20recommendation%20data%20with%20the%20aid%20of%20special%0Atokens.%20Subsequently%2C%20the%20LLM%20is%20fine-tuned%20via%20contrastive%20learning%20on%20three%0Akinds%20of%20sample%20construction%20strategies.%20Through%20this%20step%2C%20LLM%20is%20transformed%0Ainto%20a%20text%20embedding%20model.%20Finally%2C%20LLM%27s%20separate%20outputs%20for%20different%0Ascene%20features%20are%20aggregated%20by%20an%20encoder%2C%20aligning%20to%20collaborative%20signals%0Ain%20RS%2C%20enhancing%20the%20performance%20of%20recommendation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11523v1&entry.124074799=Read"},
{"title": "Proximal Policy Optimization with Graph Neural Networks for Optimal\n  Power Flow", "author": "\u00c1ngela L\u00f3pez-Cardona and Guillermo Bern\u00e1rdez and Pere Barlet-Ros and Albert Cabellos-Aparicio", "abstract": "  Optimal Power Flow (OPF) is a very traditional research area within the power\nsystems field that seeks for the optimal operation point of electric power\nplants, and which needs to be solved every few minutes in real-world scenarios.\nHowever, due to the nonconvexities that arise in power generation systems,\nthere is not yet a fast, robust solution technique for the full Alternating\nCurrent Optimal Power Flow (ACOPF). In the last decades, power grids have\nevolved into a typical dynamic, non-linear and large-scale control system,\nknown as the power system, so searching for better and faster ACOPF solutions\nis becoming crucial. Appearance of Graph Neural Networks (GNN) has allowed the\nnatural use of Machine Learning (ML) algorithms on graph data, such as power\nnetworks. On the other hand, Deep Reinforcement Learning (DRL) is known for its\npowerful capability to solve complex decision-making problems. Although\nsolutions that use these two methods separately are beginning to appear in the\nliterature, none has yet combined the advantages of both. We propose a novel\narchitecture based on the Proximal Policy Optimization algorithm with Graph\nNeural Networks to solve the Optimal Power Flow. The objective is to design an\narchitecture that learns how to solve the optimization problem and that is at\nthe same time able to generalize to unseen scenarios. We compare our solution\nwith the DCOPF in terms of cost after having trained our DRL agent on IEEE 30\nbus system and then computing the OPF on that base network with topology\nchanges\n", "link": "http://arxiv.org/abs/2212.12470v2", "date": "2024-08-21", "relevancy": 1.9731, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5063}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4882}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proximal%20Policy%20Optimization%20with%20Graph%20Neural%20Networks%20for%20Optimal%0A%20%20Power%20Flow&body=Title%3A%20Proximal%20Policy%20Optimization%20with%20Graph%20Neural%20Networks%20for%20Optimal%0A%20%20Power%20Flow%0AAuthor%3A%20%C3%81ngela%20L%C3%B3pez-Cardona%20and%20Guillermo%20Bern%C3%A1rdez%20and%20Pere%20Barlet-Ros%20and%20Albert%20Cabellos-Aparicio%0AAbstract%3A%20%20%20Optimal%20Power%20Flow%20%28OPF%29%20is%20a%20very%20traditional%20research%20area%20within%20the%20power%0Asystems%20field%20that%20seeks%20for%20the%20optimal%20operation%20point%20of%20electric%20power%0Aplants%2C%20and%20which%20needs%20to%20be%20solved%20every%20few%20minutes%20in%20real-world%20scenarios.%0AHowever%2C%20due%20to%20the%20nonconvexities%20that%20arise%20in%20power%20generation%20systems%2C%0Athere%20is%20not%20yet%20a%20fast%2C%20robust%20solution%20technique%20for%20the%20full%20Alternating%0ACurrent%20Optimal%20Power%20Flow%20%28ACOPF%29.%20In%20the%20last%20decades%2C%20power%20grids%20have%0Aevolved%20into%20a%20typical%20dynamic%2C%20non-linear%20and%20large-scale%20control%20system%2C%0Aknown%20as%20the%20power%20system%2C%20so%20searching%20for%20better%20and%20faster%20ACOPF%20solutions%0Ais%20becoming%20crucial.%20Appearance%20of%20Graph%20Neural%20Networks%20%28GNN%29%20has%20allowed%20the%0Anatural%20use%20of%20Machine%20Learning%20%28ML%29%20algorithms%20on%20graph%20data%2C%20such%20as%20power%0Anetworks.%20On%20the%20other%20hand%2C%20Deep%20Reinforcement%20Learning%20%28DRL%29%20is%20known%20for%20its%0Apowerful%20capability%20to%20solve%20complex%20decision-making%20problems.%20Although%0Asolutions%20that%20use%20these%20two%20methods%20separately%20are%20beginning%20to%20appear%20in%20the%0Aliterature%2C%20none%20has%20yet%20combined%20the%20advantages%20of%20both.%20We%20propose%20a%20novel%0Aarchitecture%20based%20on%20the%20Proximal%20Policy%20Optimization%20algorithm%20with%20Graph%0ANeural%20Networks%20to%20solve%20the%20Optimal%20Power%20Flow.%20The%20objective%20is%20to%20design%20an%0Aarchitecture%20that%20learns%20how%20to%20solve%20the%20optimization%20problem%20and%20that%20is%20at%0Athe%20same%20time%20able%20to%20generalize%20to%20unseen%20scenarios.%20We%20compare%20our%20solution%0Awith%20the%20DCOPF%20in%20terms%20of%20cost%20after%20having%20trained%20our%20DRL%20agent%20on%20IEEE%2030%0Abus%20system%20and%20then%20computing%20the%20OPF%20on%20that%20base%20network%20with%20topology%0Achanges%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.12470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProximal%2520Policy%2520Optimization%2520with%2520Graph%2520Neural%2520Networks%2520for%2520Optimal%250A%2520%2520Power%2520Flow%26entry.906535625%3D%25C3%2581ngela%2520L%25C3%25B3pez-Cardona%2520and%2520Guillermo%2520Bern%25C3%25A1rdez%2520and%2520Pere%2520Barlet-Ros%2520and%2520Albert%2520Cabellos-Aparicio%26entry.1292438233%3D%2520%2520Optimal%2520Power%2520Flow%2520%2528OPF%2529%2520is%2520a%2520very%2520traditional%2520research%2520area%2520within%2520the%2520power%250Asystems%2520field%2520that%2520seeks%2520for%2520the%2520optimal%2520operation%2520point%2520of%2520electric%2520power%250Aplants%252C%2520and%2520which%2520needs%2520to%2520be%2520solved%2520every%2520few%2520minutes%2520in%2520real-world%2520scenarios.%250AHowever%252C%2520due%2520to%2520the%2520nonconvexities%2520that%2520arise%2520in%2520power%2520generation%2520systems%252C%250Athere%2520is%2520not%2520yet%2520a%2520fast%252C%2520robust%2520solution%2520technique%2520for%2520the%2520full%2520Alternating%250ACurrent%2520Optimal%2520Power%2520Flow%2520%2528ACOPF%2529.%2520In%2520the%2520last%2520decades%252C%2520power%2520grids%2520have%250Aevolved%2520into%2520a%2520typical%2520dynamic%252C%2520non-linear%2520and%2520large-scale%2520control%2520system%252C%250Aknown%2520as%2520the%2520power%2520system%252C%2520so%2520searching%2520for%2520better%2520and%2520faster%2520ACOPF%2520solutions%250Ais%2520becoming%2520crucial.%2520Appearance%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNN%2529%2520has%2520allowed%2520the%250Anatural%2520use%2520of%2520Machine%2520Learning%2520%2528ML%2529%2520algorithms%2520on%2520graph%2520data%252C%2520such%2520as%2520power%250Anetworks.%2520On%2520the%2520other%2520hand%252C%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520is%2520known%2520for%2520its%250Apowerful%2520capability%2520to%2520solve%2520complex%2520decision-making%2520problems.%2520Although%250Asolutions%2520that%2520use%2520these%2520two%2520methods%2520separately%2520are%2520beginning%2520to%2520appear%2520in%2520the%250Aliterature%252C%2520none%2520has%2520yet%2520combined%2520the%2520advantages%2520of%2520both.%2520We%2520propose%2520a%2520novel%250Aarchitecture%2520based%2520on%2520the%2520Proximal%2520Policy%2520Optimization%2520algorithm%2520with%2520Graph%250ANeural%2520Networks%2520to%2520solve%2520the%2520Optimal%2520Power%2520Flow.%2520The%2520objective%2520is%2520to%2520design%2520an%250Aarchitecture%2520that%2520learns%2520how%2520to%2520solve%2520the%2520optimization%2520problem%2520and%2520that%2520is%2520at%250Athe%2520same%2520time%2520able%2520to%2520generalize%2520to%2520unseen%2520scenarios.%2520We%2520compare%2520our%2520solution%250Awith%2520the%2520DCOPF%2520in%2520terms%2520of%2520cost%2520after%2520having%2520trained%2520our%2520DRL%2520agent%2520on%2520IEEE%252030%250Abus%2520system%2520and%2520then%2520computing%2520the%2520OPF%2520on%2520that%2520base%2520network%2520with%2520topology%250Achanges%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.12470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proximal%20Policy%20Optimization%20with%20Graph%20Neural%20Networks%20for%20Optimal%0A%20%20Power%20Flow&entry.906535625=%C3%81ngela%20L%C3%B3pez-Cardona%20and%20Guillermo%20Bern%C3%A1rdez%20and%20Pere%20Barlet-Ros%20and%20Albert%20Cabellos-Aparicio&entry.1292438233=%20%20Optimal%20Power%20Flow%20%28OPF%29%20is%20a%20very%20traditional%20research%20area%20within%20the%20power%0Asystems%20field%20that%20seeks%20for%20the%20optimal%20operation%20point%20of%20electric%20power%0Aplants%2C%20and%20which%20needs%20to%20be%20solved%20every%20few%20minutes%20in%20real-world%20scenarios.%0AHowever%2C%20due%20to%20the%20nonconvexities%20that%20arise%20in%20power%20generation%20systems%2C%0Athere%20is%20not%20yet%20a%20fast%2C%20robust%20solution%20technique%20for%20the%20full%20Alternating%0ACurrent%20Optimal%20Power%20Flow%20%28ACOPF%29.%20In%20the%20last%20decades%2C%20power%20grids%20have%0Aevolved%20into%20a%20typical%20dynamic%2C%20non-linear%20and%20large-scale%20control%20system%2C%0Aknown%20as%20the%20power%20system%2C%20so%20searching%20for%20better%20and%20faster%20ACOPF%20solutions%0Ais%20becoming%20crucial.%20Appearance%20of%20Graph%20Neural%20Networks%20%28GNN%29%20has%20allowed%20the%0Anatural%20use%20of%20Machine%20Learning%20%28ML%29%20algorithms%20on%20graph%20data%2C%20such%20as%20power%0Anetworks.%20On%20the%20other%20hand%2C%20Deep%20Reinforcement%20Learning%20%28DRL%29%20is%20known%20for%20its%0Apowerful%20capability%20to%20solve%20complex%20decision-making%20problems.%20Although%0Asolutions%20that%20use%20these%20two%20methods%20separately%20are%20beginning%20to%20appear%20in%20the%0Aliterature%2C%20none%20has%20yet%20combined%20the%20advantages%20of%20both.%20We%20propose%20a%20novel%0Aarchitecture%20based%20on%20the%20Proximal%20Policy%20Optimization%20algorithm%20with%20Graph%0ANeural%20Networks%20to%20solve%20the%20Optimal%20Power%20Flow.%20The%20objective%20is%20to%20design%20an%0Aarchitecture%20that%20learns%20how%20to%20solve%20the%20optimization%20problem%20and%20that%20is%20at%0Athe%20same%20time%20able%20to%20generalize%20to%20unseen%20scenarios.%20We%20compare%20our%20solution%0Awith%20the%20DCOPF%20in%20terms%20of%20cost%20after%20having%20trained%20our%20DRL%20agent%20on%20IEEE%2030%0Abus%20system%20and%20then%20computing%20the%20OPF%20on%20that%20base%20network%20with%20topology%0Achanges%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.12470v2&entry.124074799=Read"},
{"title": "Drama Engine: A Framework for Narrative Agents", "author": "Martin Pichlmair and Riddhi Raj and Charlene Putney", "abstract": "  This technical report presents the Drama Engine, a novel framework for\nagentic interaction with large language models designed for narrative purposes.\nThe framework adapts multi-agent system principles to create dynamic,\ncontext-aware companions that can develop over time and interact with users and\neach other. Key features include multi-agent workflows with delegation, dynamic\nprompt assembly, and model-agnostic design. The Drama Engine introduces unique\nelements such as companion development, mood systems, and automatic context\nsummarising. It is implemented in TypeScript. The framework's applications\ninclude multi-agent chats and virtual co-workers for creative writing. The\npaper discusses the system's architecture, prompt assembly process, delegation\nmechanisms, and moderation techniques, as well as potential ethical\nconsiderations and future extensions.\n", "link": "http://arxiv.org/abs/2408.11574v1", "date": "2024-08-21", "relevancy": 1.9671, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5097}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5095}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drama%20Engine%3A%20A%20Framework%20for%20Narrative%20Agents&body=Title%3A%20Drama%20Engine%3A%20A%20Framework%20for%20Narrative%20Agents%0AAuthor%3A%20Martin%20Pichlmair%20and%20Riddhi%20Raj%20and%20Charlene%20Putney%0AAbstract%3A%20%20%20This%20technical%20report%20presents%20the%20Drama%20Engine%2C%20a%20novel%20framework%20for%0Aagentic%20interaction%20with%20large%20language%20models%20designed%20for%20narrative%20purposes.%0AThe%20framework%20adapts%20multi-agent%20system%20principles%20to%20create%20dynamic%2C%0Acontext-aware%20companions%20that%20can%20develop%20over%20time%20and%20interact%20with%20users%20and%0Aeach%20other.%20Key%20features%20include%20multi-agent%20workflows%20with%20delegation%2C%20dynamic%0Aprompt%20assembly%2C%20and%20model-agnostic%20design.%20The%20Drama%20Engine%20introduces%20unique%0Aelements%20such%20as%20companion%20development%2C%20mood%20systems%2C%20and%20automatic%20context%0Asummarising.%20It%20is%20implemented%20in%20TypeScript.%20The%20framework%27s%20applications%0Ainclude%20multi-agent%20chats%20and%20virtual%20co-workers%20for%20creative%20writing.%20The%0Apaper%20discusses%20the%20system%27s%20architecture%2C%20prompt%20assembly%20process%2C%20delegation%0Amechanisms%2C%20and%20moderation%20techniques%2C%20as%20well%20as%20potential%20ethical%0Aconsiderations%20and%20future%20extensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrama%2520Engine%253A%2520A%2520Framework%2520for%2520Narrative%2520Agents%26entry.906535625%3DMartin%2520Pichlmair%2520and%2520Riddhi%2520Raj%2520and%2520Charlene%2520Putney%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520presents%2520the%2520Drama%2520Engine%252C%2520a%2520novel%2520framework%2520for%250Aagentic%2520interaction%2520with%2520large%2520language%2520models%2520designed%2520for%2520narrative%2520purposes.%250AThe%2520framework%2520adapts%2520multi-agent%2520system%2520principles%2520to%2520create%2520dynamic%252C%250Acontext-aware%2520companions%2520that%2520can%2520develop%2520over%2520time%2520and%2520interact%2520with%2520users%2520and%250Aeach%2520other.%2520Key%2520features%2520include%2520multi-agent%2520workflows%2520with%2520delegation%252C%2520dynamic%250Aprompt%2520assembly%252C%2520and%2520model-agnostic%2520design.%2520The%2520Drama%2520Engine%2520introduces%2520unique%250Aelements%2520such%2520as%2520companion%2520development%252C%2520mood%2520systems%252C%2520and%2520automatic%2520context%250Asummarising.%2520It%2520is%2520implemented%2520in%2520TypeScript.%2520The%2520framework%2527s%2520applications%250Ainclude%2520multi-agent%2520chats%2520and%2520virtual%2520co-workers%2520for%2520creative%2520writing.%2520The%250Apaper%2520discusses%2520the%2520system%2527s%2520architecture%252C%2520prompt%2520assembly%2520process%252C%2520delegation%250Amechanisms%252C%2520and%2520moderation%2520techniques%252C%2520as%2520well%2520as%2520potential%2520ethical%250Aconsiderations%2520and%2520future%2520extensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drama%20Engine%3A%20A%20Framework%20for%20Narrative%20Agents&entry.906535625=Martin%20Pichlmair%20and%20Riddhi%20Raj%20and%20Charlene%20Putney&entry.1292438233=%20%20This%20technical%20report%20presents%20the%20Drama%20Engine%2C%20a%20novel%20framework%20for%0Aagentic%20interaction%20with%20large%20language%20models%20designed%20for%20narrative%20purposes.%0AThe%20framework%20adapts%20multi-agent%20system%20principles%20to%20create%20dynamic%2C%0Acontext-aware%20companions%20that%20can%20develop%20over%20time%20and%20interact%20with%20users%20and%0Aeach%20other.%20Key%20features%20include%20multi-agent%20workflows%20with%20delegation%2C%20dynamic%0Aprompt%20assembly%2C%20and%20model-agnostic%20design.%20The%20Drama%20Engine%20introduces%20unique%0Aelements%20such%20as%20companion%20development%2C%20mood%20systems%2C%20and%20automatic%20context%0Asummarising.%20It%20is%20implemented%20in%20TypeScript.%20The%20framework%27s%20applications%0Ainclude%20multi-agent%20chats%20and%20virtual%20co-workers%20for%20creative%20writing.%20The%0Apaper%20discusses%20the%20system%27s%20architecture%2C%20prompt%20assembly%20process%2C%20delegation%0Amechanisms%2C%20and%20moderation%20techniques%2C%20as%20well%20as%20potential%20ethical%0Aconsiderations%20and%20future%20extensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11574v1&entry.124074799=Read"},
{"title": "Comparative Analysis of NMPC and Fuzzy PID Controllers for Trajectory\n  Tracking in Omni-Drive Robots: Design, Simulation, and Performance Evaluation", "author": "Love Panta", "abstract": "  Trajectory tracking for an Omni-drive robot presents a challenging task that\ndemands an efficient controller design. This paper introduces a self-optimizing\ncontroller, Type-1 fuzzyPID, which leverages dynamic and static system response\nanalysis to overcome the limitations of manual tuning. To account for system\nuncertainties, an Interval Type-2 fuzzyPID controller is also developed. Both\ncontrollers are designed using Matlab/Simulink and tested through trajectory\ntracking simulations in the CoppeliaSim environment. Additionally, a non-linear\nmodel predictive controller(NMPC) is proposed and compared against the fuzzyPID\ncontrollers. The impact of tunable parameters on NMPC tracking accuracy is\nthoroughly examined. We also present plots of the step-response characteristics\nand noise rejection experiments for each controller. Simulation results\nvalidate the precision and effectiveness of NMPC over fuzzyPID controllers\nwhile trading computational complexity. Access to code and simulation\nenvironment is available in the following link:\nhttps://github.com/love481/Omni-drive-robot-Simulation.git.\n", "link": "http://arxiv.org/abs/2403.06744v2", "date": "2024-08-21", "relevancy": 1.9662, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5053}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4911}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20NMPC%20and%20Fuzzy%20PID%20Controllers%20for%20Trajectory%0A%20%20Tracking%20in%20Omni-Drive%20Robots%3A%20Design%2C%20Simulation%2C%20and%20Performance%20Evaluation&body=Title%3A%20Comparative%20Analysis%20of%20NMPC%20and%20Fuzzy%20PID%20Controllers%20for%20Trajectory%0A%20%20Tracking%20in%20Omni-Drive%20Robots%3A%20Design%2C%20Simulation%2C%20and%20Performance%20Evaluation%0AAuthor%3A%20Love%20Panta%0AAbstract%3A%20%20%20Trajectory%20tracking%20for%20an%20Omni-drive%20robot%20presents%20a%20challenging%20task%20that%0Ademands%20an%20efficient%20controller%20design.%20This%20paper%20introduces%20a%20self-optimizing%0Acontroller%2C%20Type-1%20fuzzyPID%2C%20which%20leverages%20dynamic%20and%20static%20system%20response%0Aanalysis%20to%20overcome%20the%20limitations%20of%20manual%20tuning.%20To%20account%20for%20system%0Auncertainties%2C%20an%20Interval%20Type-2%20fuzzyPID%20controller%20is%20also%20developed.%20Both%0Acontrollers%20are%20designed%20using%20Matlab/Simulink%20and%20tested%20through%20trajectory%0Atracking%20simulations%20in%20the%20CoppeliaSim%20environment.%20Additionally%2C%20a%20non-linear%0Amodel%20predictive%20controller%28NMPC%29%20is%20proposed%20and%20compared%20against%20the%20fuzzyPID%0Acontrollers.%20The%20impact%20of%20tunable%20parameters%20on%20NMPC%20tracking%20accuracy%20is%0Athoroughly%20examined.%20We%20also%20present%20plots%20of%20the%20step-response%20characteristics%0Aand%20noise%20rejection%20experiments%20for%20each%20controller.%20Simulation%20results%0Avalidate%20the%20precision%20and%20effectiveness%20of%20NMPC%20over%20fuzzyPID%20controllers%0Awhile%20trading%20computational%20complexity.%20Access%20to%20code%20and%20simulation%0Aenvironment%20is%20available%20in%20the%20following%20link%3A%0Ahttps%3A//github.com/love481/Omni-drive-robot-Simulation.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06744v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520NMPC%2520and%2520Fuzzy%2520PID%2520Controllers%2520for%2520Trajectory%250A%2520%2520Tracking%2520in%2520Omni-Drive%2520Robots%253A%2520Design%252C%2520Simulation%252C%2520and%2520Performance%2520Evaluation%26entry.906535625%3DLove%2520Panta%26entry.1292438233%3D%2520%2520Trajectory%2520tracking%2520for%2520an%2520Omni-drive%2520robot%2520presents%2520a%2520challenging%2520task%2520that%250Ademands%2520an%2520efficient%2520controller%2520design.%2520This%2520paper%2520introduces%2520a%2520self-optimizing%250Acontroller%252C%2520Type-1%2520fuzzyPID%252C%2520which%2520leverages%2520dynamic%2520and%2520static%2520system%2520response%250Aanalysis%2520to%2520overcome%2520the%2520limitations%2520of%2520manual%2520tuning.%2520To%2520account%2520for%2520system%250Auncertainties%252C%2520an%2520Interval%2520Type-2%2520fuzzyPID%2520controller%2520is%2520also%2520developed.%2520Both%250Acontrollers%2520are%2520designed%2520using%2520Matlab/Simulink%2520and%2520tested%2520through%2520trajectory%250Atracking%2520simulations%2520in%2520the%2520CoppeliaSim%2520environment.%2520Additionally%252C%2520a%2520non-linear%250Amodel%2520predictive%2520controller%2528NMPC%2529%2520is%2520proposed%2520and%2520compared%2520against%2520the%2520fuzzyPID%250Acontrollers.%2520The%2520impact%2520of%2520tunable%2520parameters%2520on%2520NMPC%2520tracking%2520accuracy%2520is%250Athoroughly%2520examined.%2520We%2520also%2520present%2520plots%2520of%2520the%2520step-response%2520characteristics%250Aand%2520noise%2520rejection%2520experiments%2520for%2520each%2520controller.%2520Simulation%2520results%250Avalidate%2520the%2520precision%2520and%2520effectiveness%2520of%2520NMPC%2520over%2520fuzzyPID%2520controllers%250Awhile%2520trading%2520computational%2520complexity.%2520Access%2520to%2520code%2520and%2520simulation%250Aenvironment%2520is%2520available%2520in%2520the%2520following%2520link%253A%250Ahttps%253A//github.com/love481/Omni-drive-robot-Simulation.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06744v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20NMPC%20and%20Fuzzy%20PID%20Controllers%20for%20Trajectory%0A%20%20Tracking%20in%20Omni-Drive%20Robots%3A%20Design%2C%20Simulation%2C%20and%20Performance%20Evaluation&entry.906535625=Love%20Panta&entry.1292438233=%20%20Trajectory%20tracking%20for%20an%20Omni-drive%20robot%20presents%20a%20challenging%20task%20that%0Ademands%20an%20efficient%20controller%20design.%20This%20paper%20introduces%20a%20self-optimizing%0Acontroller%2C%20Type-1%20fuzzyPID%2C%20which%20leverages%20dynamic%20and%20static%20system%20response%0Aanalysis%20to%20overcome%20the%20limitations%20of%20manual%20tuning.%20To%20account%20for%20system%0Auncertainties%2C%20an%20Interval%20Type-2%20fuzzyPID%20controller%20is%20also%20developed.%20Both%0Acontrollers%20are%20designed%20using%20Matlab/Simulink%20and%20tested%20through%20trajectory%0Atracking%20simulations%20in%20the%20CoppeliaSim%20environment.%20Additionally%2C%20a%20non-linear%0Amodel%20predictive%20controller%28NMPC%29%20is%20proposed%20and%20compared%20against%20the%20fuzzyPID%0Acontrollers.%20The%20impact%20of%20tunable%20parameters%20on%20NMPC%20tracking%20accuracy%20is%0Athoroughly%20examined.%20We%20also%20present%20plots%20of%20the%20step-response%20characteristics%0Aand%20noise%20rejection%20experiments%20for%20each%20controller.%20Simulation%20results%0Avalidate%20the%20precision%20and%20effectiveness%20of%20NMPC%20over%20fuzzyPID%20controllers%0Awhile%20trading%20computational%20complexity.%20Access%20to%20code%20and%20simulation%0Aenvironment%20is%20available%20in%20the%20following%20link%3A%0Ahttps%3A//github.com/love481/Omni-drive-robot-Simulation.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06744v2&entry.124074799=Read"},
{"title": "Improving the Scan-rescan Precision of AI-based CMR Biomarker Estimation", "author": "Dewmini Hasara Wickremasinghe and Yiyang Xu and Esther Puyol-Ant\u00f3n and Paul Aljabar and Reza Razavi and Andrew P. King", "abstract": "  Quantification of cardiac biomarkers from cine cardiovascular magnetic\nresonance (CMR) data using deep learning (DL) methods offers many advantages,\nsuch as increased accuracy and faster analysis. However, only a few studies\nhave focused on the scan-rescan precision of the biomarker estimates, which is\nimportant for reproducibility and longitudinal analysis. Here, we propose a\ncardiac biomarker estimation pipeline that not only focuses on achieving high\nsegmentation accuracy but also on improving the scan-rescan precision of the\ncomputed biomarkers, namely left and right ventricular ejection fraction, and\nleft ventricular myocardial mass. We evaluate two approaches to improve the\napical-basal resolution of the segmentations used for estimating the\nbiomarkers: one based on image interpolation and one based on segmentation\ninterpolation. Using a database comprising scan-rescan cine CMR data acquired\nfrom 92 subjects, we compare the performance of these two methods against\nground truth (GT) segmentations and DL segmentations obtained before\ninterpolation (baseline). The results demonstrate that both the image-based and\nsegmentation-based interpolation methods were able to narrow Bland-Altman\nscan-rescan confidence intervals for all biomarkers compared to the GT and\nbaseline performances. Our findings highlight the importance of focusing not\nonly on segmentation accuracy but also on the consistency of biomarkers across\nrepeated scans, which is crucial for longitudinal analysis of cardiac function.\n", "link": "http://arxiv.org/abs/2408.11754v1", "date": "2024-08-21", "relevancy": 1.9509, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5067}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4891}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Scan-rescan%20Precision%20of%20AI-based%20CMR%20Biomarker%20Estimation&body=Title%3A%20Improving%20the%20Scan-rescan%20Precision%20of%20AI-based%20CMR%20Biomarker%20Estimation%0AAuthor%3A%20Dewmini%20Hasara%20Wickremasinghe%20and%20Yiyang%20Xu%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Paul%20Aljabar%20and%20Reza%20Razavi%20and%20Andrew%20P.%20King%0AAbstract%3A%20%20%20Quantification%20of%20cardiac%20biomarkers%20from%20cine%20cardiovascular%20magnetic%0Aresonance%20%28CMR%29%20data%20using%20deep%20learning%20%28DL%29%20methods%20offers%20many%20advantages%2C%0Asuch%20as%20increased%20accuracy%20and%20faster%20analysis.%20However%2C%20only%20a%20few%20studies%0Ahave%20focused%20on%20the%20scan-rescan%20precision%20of%20the%20biomarker%20estimates%2C%20which%20is%0Aimportant%20for%20reproducibility%20and%20longitudinal%20analysis.%20Here%2C%20we%20propose%20a%0Acardiac%20biomarker%20estimation%20pipeline%20that%20not%20only%20focuses%20on%20achieving%20high%0Asegmentation%20accuracy%20but%20also%20on%20improving%20the%20scan-rescan%20precision%20of%20the%0Acomputed%20biomarkers%2C%20namely%20left%20and%20right%20ventricular%20ejection%20fraction%2C%20and%0Aleft%20ventricular%20myocardial%20mass.%20We%20evaluate%20two%20approaches%20to%20improve%20the%0Aapical-basal%20resolution%20of%20the%20segmentations%20used%20for%20estimating%20the%0Abiomarkers%3A%20one%20based%20on%20image%20interpolation%20and%20one%20based%20on%20segmentation%0Ainterpolation.%20Using%20a%20database%20comprising%20scan-rescan%20cine%20CMR%20data%20acquired%0Afrom%2092%20subjects%2C%20we%20compare%20the%20performance%20of%20these%20two%20methods%20against%0Aground%20truth%20%28GT%29%20segmentations%20and%20DL%20segmentations%20obtained%20before%0Ainterpolation%20%28baseline%29.%20The%20results%20demonstrate%20that%20both%20the%20image-based%20and%0Asegmentation-based%20interpolation%20methods%20were%20able%20to%20narrow%20Bland-Altman%0Ascan-rescan%20confidence%20intervals%20for%20all%20biomarkers%20compared%20to%20the%20GT%20and%0Abaseline%20performances.%20Our%20findings%20highlight%20the%20importance%20of%20focusing%20not%0Aonly%20on%20segmentation%20accuracy%20but%20also%20on%20the%20consistency%20of%20biomarkers%20across%0Arepeated%20scans%2C%20which%20is%20crucial%20for%20longitudinal%20analysis%20of%20cardiac%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Scan-rescan%2520Precision%2520of%2520AI-based%2520CMR%2520Biomarker%2520Estimation%26entry.906535625%3DDewmini%2520Hasara%2520Wickremasinghe%2520and%2520Yiyang%2520Xu%2520and%2520Esther%2520Puyol-Ant%25C3%25B3n%2520and%2520Paul%2520Aljabar%2520and%2520Reza%2520Razavi%2520and%2520Andrew%2520P.%2520King%26entry.1292438233%3D%2520%2520Quantification%2520of%2520cardiac%2520biomarkers%2520from%2520cine%2520cardiovascular%2520magnetic%250Aresonance%2520%2528CMR%2529%2520data%2520using%2520deep%2520learning%2520%2528DL%2529%2520methods%2520offers%2520many%2520advantages%252C%250Asuch%2520as%2520increased%2520accuracy%2520and%2520faster%2520analysis.%2520However%252C%2520only%2520a%2520few%2520studies%250Ahave%2520focused%2520on%2520the%2520scan-rescan%2520precision%2520of%2520the%2520biomarker%2520estimates%252C%2520which%2520is%250Aimportant%2520for%2520reproducibility%2520and%2520longitudinal%2520analysis.%2520Here%252C%2520we%2520propose%2520a%250Acardiac%2520biomarker%2520estimation%2520pipeline%2520that%2520not%2520only%2520focuses%2520on%2520achieving%2520high%250Asegmentation%2520accuracy%2520but%2520also%2520on%2520improving%2520the%2520scan-rescan%2520precision%2520of%2520the%250Acomputed%2520biomarkers%252C%2520namely%2520left%2520and%2520right%2520ventricular%2520ejection%2520fraction%252C%2520and%250Aleft%2520ventricular%2520myocardial%2520mass.%2520We%2520evaluate%2520two%2520approaches%2520to%2520improve%2520the%250Aapical-basal%2520resolution%2520of%2520the%2520segmentations%2520used%2520for%2520estimating%2520the%250Abiomarkers%253A%2520one%2520based%2520on%2520image%2520interpolation%2520and%2520one%2520based%2520on%2520segmentation%250Ainterpolation.%2520Using%2520a%2520database%2520comprising%2520scan-rescan%2520cine%2520CMR%2520data%2520acquired%250Afrom%252092%2520subjects%252C%2520we%2520compare%2520the%2520performance%2520of%2520these%2520two%2520methods%2520against%250Aground%2520truth%2520%2528GT%2529%2520segmentations%2520and%2520DL%2520segmentations%2520obtained%2520before%250Ainterpolation%2520%2528baseline%2529.%2520The%2520results%2520demonstrate%2520that%2520both%2520the%2520image-based%2520and%250Asegmentation-based%2520interpolation%2520methods%2520were%2520able%2520to%2520narrow%2520Bland-Altman%250Ascan-rescan%2520confidence%2520intervals%2520for%2520all%2520biomarkers%2520compared%2520to%2520the%2520GT%2520and%250Abaseline%2520performances.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%2520focusing%2520not%250Aonly%2520on%2520segmentation%2520accuracy%2520but%2520also%2520on%2520the%2520consistency%2520of%2520biomarkers%2520across%250Arepeated%2520scans%252C%2520which%2520is%2520crucial%2520for%2520longitudinal%2520analysis%2520of%2520cardiac%2520function.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Scan-rescan%20Precision%20of%20AI-based%20CMR%20Biomarker%20Estimation&entry.906535625=Dewmini%20Hasara%20Wickremasinghe%20and%20Yiyang%20Xu%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Paul%20Aljabar%20and%20Reza%20Razavi%20and%20Andrew%20P.%20King&entry.1292438233=%20%20Quantification%20of%20cardiac%20biomarkers%20from%20cine%20cardiovascular%20magnetic%0Aresonance%20%28CMR%29%20data%20using%20deep%20learning%20%28DL%29%20methods%20offers%20many%20advantages%2C%0Asuch%20as%20increased%20accuracy%20and%20faster%20analysis.%20However%2C%20only%20a%20few%20studies%0Ahave%20focused%20on%20the%20scan-rescan%20precision%20of%20the%20biomarker%20estimates%2C%20which%20is%0Aimportant%20for%20reproducibility%20and%20longitudinal%20analysis.%20Here%2C%20we%20propose%20a%0Acardiac%20biomarker%20estimation%20pipeline%20that%20not%20only%20focuses%20on%20achieving%20high%0Asegmentation%20accuracy%20but%20also%20on%20improving%20the%20scan-rescan%20precision%20of%20the%0Acomputed%20biomarkers%2C%20namely%20left%20and%20right%20ventricular%20ejection%20fraction%2C%20and%0Aleft%20ventricular%20myocardial%20mass.%20We%20evaluate%20two%20approaches%20to%20improve%20the%0Aapical-basal%20resolution%20of%20the%20segmentations%20used%20for%20estimating%20the%0Abiomarkers%3A%20one%20based%20on%20image%20interpolation%20and%20one%20based%20on%20segmentation%0Ainterpolation.%20Using%20a%20database%20comprising%20scan-rescan%20cine%20CMR%20data%20acquired%0Afrom%2092%20subjects%2C%20we%20compare%20the%20performance%20of%20these%20two%20methods%20against%0Aground%20truth%20%28GT%29%20segmentations%20and%20DL%20segmentations%20obtained%20before%0Ainterpolation%20%28baseline%29.%20The%20results%20demonstrate%20that%20both%20the%20image-based%20and%0Asegmentation-based%20interpolation%20methods%20were%20able%20to%20narrow%20Bland-Altman%0Ascan-rescan%20confidence%20intervals%20for%20all%20biomarkers%20compared%20to%20the%20GT%20and%0Abaseline%20performances.%20Our%20findings%20highlight%20the%20importance%20of%20focusing%20not%0Aonly%20on%20segmentation%20accuracy%20but%20also%20on%20the%20consistency%20of%20biomarkers%20across%0Arepeated%20scans%2C%20which%20is%20crucial%20for%20longitudinal%20analysis%20of%20cardiac%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11754v1&entry.124074799=Read"},
{"title": "XDT-CXR: Investigating Cross-Disease Transferability in Zero-Shot Binary\n  Classification of Chest X-Rays", "author": "Umaima Rahman and Abhishek Basu and Muhammad Uzair Khattak and Aniq Ur Rahman", "abstract": "  This study explores the concept of cross-disease transferability (XDT) in\nmedical imaging, focusing on the potential of binary classifiers trained on one\ndisease to perform zero-shot classification on another disease affecting the\nsame organ. Utilizing chest X-rays (CXR) as the primary modality, we\ninvestigate whether a model trained on one pulmonary disease can make\npredictions about another novel pulmonary disease, a scenario with significant\nimplications for medical settings with limited data on emerging diseases. The\nXDT framework leverages the embedding space of a vision encoder, which, through\nkernel transformation, aids in distinguishing between diseased and non-diseased\nclasses in the latent space. This capability is especially beneficial in\nresource-limited environments or in regions with low prevalence of certain\ndiseases, where conventional diagnostic practices may fail. However, the XDT\nframework is currently limited to binary classification, determining only the\npresence or absence of a disease rather than differentiating among multiple\ndiseases. This limitation underscores the supplementary role of XDT to\ntraditional diagnostic tests in clinical settings. Furthermore, results show\nthat XDT-CXR as a framework is able to make better predictions compared to\nother zero-shot learning (ZSL) baselines.\n", "link": "http://arxiv.org/abs/2408.11493v1", "date": "2024-08-21", "relevancy": 1.9436, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4858}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XDT-CXR%3A%20Investigating%20Cross-Disease%20Transferability%20in%20Zero-Shot%20Binary%0A%20%20Classification%20of%20Chest%20X-Rays&body=Title%3A%20XDT-CXR%3A%20Investigating%20Cross-Disease%20Transferability%20in%20Zero-Shot%20Binary%0A%20%20Classification%20of%20Chest%20X-Rays%0AAuthor%3A%20Umaima%20Rahman%20and%20Abhishek%20Basu%20and%20Muhammad%20Uzair%20Khattak%20and%20Aniq%20Ur%20Rahman%0AAbstract%3A%20%20%20This%20study%20explores%20the%20concept%20of%20cross-disease%20transferability%20%28XDT%29%20in%0Amedical%20imaging%2C%20focusing%20on%20the%20potential%20of%20binary%20classifiers%20trained%20on%20one%0Adisease%20to%20perform%20zero-shot%20classification%20on%20another%20disease%20affecting%20the%0Asame%20organ.%20Utilizing%20chest%20X-rays%20%28CXR%29%20as%20the%20primary%20modality%2C%20we%0Ainvestigate%20whether%20a%20model%20trained%20on%20one%20pulmonary%20disease%20can%20make%0Apredictions%20about%20another%20novel%20pulmonary%20disease%2C%20a%20scenario%20with%20significant%0Aimplications%20for%20medical%20settings%20with%20limited%20data%20on%20emerging%20diseases.%20The%0AXDT%20framework%20leverages%20the%20embedding%20space%20of%20a%20vision%20encoder%2C%20which%2C%20through%0Akernel%20transformation%2C%20aids%20in%20distinguishing%20between%20diseased%20and%20non-diseased%0Aclasses%20in%20the%20latent%20space.%20This%20capability%20is%20especially%20beneficial%20in%0Aresource-limited%20environments%20or%20in%20regions%20with%20low%20prevalence%20of%20certain%0Adiseases%2C%20where%20conventional%20diagnostic%20practices%20may%20fail.%20However%2C%20the%20XDT%0Aframework%20is%20currently%20limited%20to%20binary%20classification%2C%20determining%20only%20the%0Apresence%20or%20absence%20of%20a%20disease%20rather%20than%20differentiating%20among%20multiple%0Adiseases.%20This%20limitation%20underscores%20the%20supplementary%20role%20of%20XDT%20to%0Atraditional%20diagnostic%20tests%20in%20clinical%20settings.%20Furthermore%2C%20results%20show%0Athat%20XDT-CXR%20as%20a%20framework%20is%20able%20to%20make%20better%20predictions%20compared%20to%0Aother%20zero-shot%20learning%20%28ZSL%29%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXDT-CXR%253A%2520Investigating%2520Cross-Disease%2520Transferability%2520in%2520Zero-Shot%2520Binary%250A%2520%2520Classification%2520of%2520Chest%2520X-Rays%26entry.906535625%3DUmaima%2520Rahman%2520and%2520Abhishek%2520Basu%2520and%2520Muhammad%2520Uzair%2520Khattak%2520and%2520Aniq%2520Ur%2520Rahman%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520concept%2520of%2520cross-disease%2520transferability%2520%2528XDT%2529%2520in%250Amedical%2520imaging%252C%2520focusing%2520on%2520the%2520potential%2520of%2520binary%2520classifiers%2520trained%2520on%2520one%250Adisease%2520to%2520perform%2520zero-shot%2520classification%2520on%2520another%2520disease%2520affecting%2520the%250Asame%2520organ.%2520Utilizing%2520chest%2520X-rays%2520%2528CXR%2529%2520as%2520the%2520primary%2520modality%252C%2520we%250Ainvestigate%2520whether%2520a%2520model%2520trained%2520on%2520one%2520pulmonary%2520disease%2520can%2520make%250Apredictions%2520about%2520another%2520novel%2520pulmonary%2520disease%252C%2520a%2520scenario%2520with%2520significant%250Aimplications%2520for%2520medical%2520settings%2520with%2520limited%2520data%2520on%2520emerging%2520diseases.%2520The%250AXDT%2520framework%2520leverages%2520the%2520embedding%2520space%2520of%2520a%2520vision%2520encoder%252C%2520which%252C%2520through%250Akernel%2520transformation%252C%2520aids%2520in%2520distinguishing%2520between%2520diseased%2520and%2520non-diseased%250Aclasses%2520in%2520the%2520latent%2520space.%2520This%2520capability%2520is%2520especially%2520beneficial%2520in%250Aresource-limited%2520environments%2520or%2520in%2520regions%2520with%2520low%2520prevalence%2520of%2520certain%250Adiseases%252C%2520where%2520conventional%2520diagnostic%2520practices%2520may%2520fail.%2520However%252C%2520the%2520XDT%250Aframework%2520is%2520currently%2520limited%2520to%2520binary%2520classification%252C%2520determining%2520only%2520the%250Apresence%2520or%2520absence%2520of%2520a%2520disease%2520rather%2520than%2520differentiating%2520among%2520multiple%250Adiseases.%2520This%2520limitation%2520underscores%2520the%2520supplementary%2520role%2520of%2520XDT%2520to%250Atraditional%2520diagnostic%2520tests%2520in%2520clinical%2520settings.%2520Furthermore%252C%2520results%2520show%250Athat%2520XDT-CXR%2520as%2520a%2520framework%2520is%2520able%2520to%2520make%2520better%2520predictions%2520compared%2520to%250Aother%2520zero-shot%2520learning%2520%2528ZSL%2529%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XDT-CXR%3A%20Investigating%20Cross-Disease%20Transferability%20in%20Zero-Shot%20Binary%0A%20%20Classification%20of%20Chest%20X-Rays&entry.906535625=Umaima%20Rahman%20and%20Abhishek%20Basu%20and%20Muhammad%20Uzair%20Khattak%20and%20Aniq%20Ur%20Rahman&entry.1292438233=%20%20This%20study%20explores%20the%20concept%20of%20cross-disease%20transferability%20%28XDT%29%20in%0Amedical%20imaging%2C%20focusing%20on%20the%20potential%20of%20binary%20classifiers%20trained%20on%20one%0Adisease%20to%20perform%20zero-shot%20classification%20on%20another%20disease%20affecting%20the%0Asame%20organ.%20Utilizing%20chest%20X-rays%20%28CXR%29%20as%20the%20primary%20modality%2C%20we%0Ainvestigate%20whether%20a%20model%20trained%20on%20one%20pulmonary%20disease%20can%20make%0Apredictions%20about%20another%20novel%20pulmonary%20disease%2C%20a%20scenario%20with%20significant%0Aimplications%20for%20medical%20settings%20with%20limited%20data%20on%20emerging%20diseases.%20The%0AXDT%20framework%20leverages%20the%20embedding%20space%20of%20a%20vision%20encoder%2C%20which%2C%20through%0Akernel%20transformation%2C%20aids%20in%20distinguishing%20between%20diseased%20and%20non-diseased%0Aclasses%20in%20the%20latent%20space.%20This%20capability%20is%20especially%20beneficial%20in%0Aresource-limited%20environments%20or%20in%20regions%20with%20low%20prevalence%20of%20certain%0Adiseases%2C%20where%20conventional%20diagnostic%20practices%20may%20fail.%20However%2C%20the%20XDT%0Aframework%20is%20currently%20limited%20to%20binary%20classification%2C%20determining%20only%20the%0Apresence%20or%20absence%20of%20a%20disease%20rather%20than%20differentiating%20among%20multiple%0Adiseases.%20This%20limitation%20underscores%20the%20supplementary%20role%20of%20XDT%20to%0Atraditional%20diagnostic%20tests%20in%20clinical%20settings.%20Furthermore%2C%20results%20show%0Athat%20XDT-CXR%20as%20a%20framework%20is%20able%20to%20make%20better%20predictions%20compared%20to%0Aother%20zero-shot%20learning%20%28ZSL%29%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11493v1&entry.124074799=Read"},
{"title": "Improving Calibration by Relating Focal Loss, Temperature Scaling, and\n  Properness", "author": "Viacheslav Komisarenko and Meelis Kull", "abstract": "  Proper losses such as cross-entropy incentivize classifiers to produce class\nprobabilities that are well-calibrated on the training data. Due to the\ngeneralization gap, these classifiers tend to become overconfident on the test\ndata, mandating calibration methods such as temperature scaling. The focal loss\nis not proper, but training with it has been shown to often result in\nclassifiers that are better calibrated on test data. Our first contribution is\na simple explanation about why focal loss training often leads to better\ncalibration than cross-entropy training. For this, we prove that focal loss can\nbe decomposed into a confidence-raising transformation and a proper loss. This\nis why focal loss pushes the model to provide under-confident predictions on\nthe training data, resulting in being better calibrated on the test data, due\nto the generalization gap. Secondly, we reveal a strong connection between\ntemperature scaling and focal loss through its confidence-raising\ntransformation, which we refer to as the focal calibration map. Thirdly, we\npropose focal temperature scaling - a new post-hoc calibration method combining\nfocal calibration and temperature scaling. Our experiments on three image\nclassification datasets demonstrate that focal temperature scaling outperforms\nstandard temperature scaling.\n", "link": "http://arxiv.org/abs/2408.11598v1", "date": "2024-08-21", "relevancy": 1.9421, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.497}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4911}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Calibration%20by%20Relating%20Focal%20Loss%2C%20Temperature%20Scaling%2C%20and%0A%20%20Properness&body=Title%3A%20Improving%20Calibration%20by%20Relating%20Focal%20Loss%2C%20Temperature%20Scaling%2C%20and%0A%20%20Properness%0AAuthor%3A%20Viacheslav%20Komisarenko%20and%20Meelis%20Kull%0AAbstract%3A%20%20%20Proper%20losses%20such%20as%20cross-entropy%20incentivize%20classifiers%20to%20produce%20class%0Aprobabilities%20that%20are%20well-calibrated%20on%20the%20training%20data.%20Due%20to%20the%0Ageneralization%20gap%2C%20these%20classifiers%20tend%20to%20become%20overconfident%20on%20the%20test%0Adata%2C%20mandating%20calibration%20methods%20such%20as%20temperature%20scaling.%20The%20focal%20loss%0Ais%20not%20proper%2C%20but%20training%20with%20it%20has%20been%20shown%20to%20often%20result%20in%0Aclassifiers%20that%20are%20better%20calibrated%20on%20test%20data.%20Our%20first%20contribution%20is%0Aa%20simple%20explanation%20about%20why%20focal%20loss%20training%20often%20leads%20to%20better%0Acalibration%20than%20cross-entropy%20training.%20For%20this%2C%20we%20prove%20that%20focal%20loss%20can%0Abe%20decomposed%20into%20a%20confidence-raising%20transformation%20and%20a%20proper%20loss.%20This%0Ais%20why%20focal%20loss%20pushes%20the%20model%20to%20provide%20under-confident%20predictions%20on%0Athe%20training%20data%2C%20resulting%20in%20being%20better%20calibrated%20on%20the%20test%20data%2C%20due%0Ato%20the%20generalization%20gap.%20Secondly%2C%20we%20reveal%20a%20strong%20connection%20between%0Atemperature%20scaling%20and%20focal%20loss%20through%20its%20confidence-raising%0Atransformation%2C%20which%20we%20refer%20to%20as%20the%20focal%20calibration%20map.%20Thirdly%2C%20we%0Apropose%20focal%20temperature%20scaling%20-%20a%20new%20post-hoc%20calibration%20method%20combining%0Afocal%20calibration%20and%20temperature%20scaling.%20Our%20experiments%20on%20three%20image%0Aclassification%20datasets%20demonstrate%20that%20focal%20temperature%20scaling%20outperforms%0Astandard%20temperature%20scaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Calibration%2520by%2520Relating%2520Focal%2520Loss%252C%2520Temperature%2520Scaling%252C%2520and%250A%2520%2520Properness%26entry.906535625%3DViacheslav%2520Komisarenko%2520and%2520Meelis%2520Kull%26entry.1292438233%3D%2520%2520Proper%2520losses%2520such%2520as%2520cross-entropy%2520incentivize%2520classifiers%2520to%2520produce%2520class%250Aprobabilities%2520that%2520are%2520well-calibrated%2520on%2520the%2520training%2520data.%2520Due%2520to%2520the%250Ageneralization%2520gap%252C%2520these%2520classifiers%2520tend%2520to%2520become%2520overconfident%2520on%2520the%2520test%250Adata%252C%2520mandating%2520calibration%2520methods%2520such%2520as%2520temperature%2520scaling.%2520The%2520focal%2520loss%250Ais%2520not%2520proper%252C%2520but%2520training%2520with%2520it%2520has%2520been%2520shown%2520to%2520often%2520result%2520in%250Aclassifiers%2520that%2520are%2520better%2520calibrated%2520on%2520test%2520data.%2520Our%2520first%2520contribution%2520is%250Aa%2520simple%2520explanation%2520about%2520why%2520focal%2520loss%2520training%2520often%2520leads%2520to%2520better%250Acalibration%2520than%2520cross-entropy%2520training.%2520For%2520this%252C%2520we%2520prove%2520that%2520focal%2520loss%2520can%250Abe%2520decomposed%2520into%2520a%2520confidence-raising%2520transformation%2520and%2520a%2520proper%2520loss.%2520This%250Ais%2520why%2520focal%2520loss%2520pushes%2520the%2520model%2520to%2520provide%2520under-confident%2520predictions%2520on%250Athe%2520training%2520data%252C%2520resulting%2520in%2520being%2520better%2520calibrated%2520on%2520the%2520test%2520data%252C%2520due%250Ato%2520the%2520generalization%2520gap.%2520Secondly%252C%2520we%2520reveal%2520a%2520strong%2520connection%2520between%250Atemperature%2520scaling%2520and%2520focal%2520loss%2520through%2520its%2520confidence-raising%250Atransformation%252C%2520which%2520we%2520refer%2520to%2520as%2520the%2520focal%2520calibration%2520map.%2520Thirdly%252C%2520we%250Apropose%2520focal%2520temperature%2520scaling%2520-%2520a%2520new%2520post-hoc%2520calibration%2520method%2520combining%250Afocal%2520calibration%2520and%2520temperature%2520scaling.%2520Our%2520experiments%2520on%2520three%2520image%250Aclassification%2520datasets%2520demonstrate%2520that%2520focal%2520temperature%2520scaling%2520outperforms%250Astandard%2520temperature%2520scaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Calibration%20by%20Relating%20Focal%20Loss%2C%20Temperature%20Scaling%2C%20and%0A%20%20Properness&entry.906535625=Viacheslav%20Komisarenko%20and%20Meelis%20Kull&entry.1292438233=%20%20Proper%20losses%20such%20as%20cross-entropy%20incentivize%20classifiers%20to%20produce%20class%0Aprobabilities%20that%20are%20well-calibrated%20on%20the%20training%20data.%20Due%20to%20the%0Ageneralization%20gap%2C%20these%20classifiers%20tend%20to%20become%20overconfident%20on%20the%20test%0Adata%2C%20mandating%20calibration%20methods%20such%20as%20temperature%20scaling.%20The%20focal%20loss%0Ais%20not%20proper%2C%20but%20training%20with%20it%20has%20been%20shown%20to%20often%20result%20in%0Aclassifiers%20that%20are%20better%20calibrated%20on%20test%20data.%20Our%20first%20contribution%20is%0Aa%20simple%20explanation%20about%20why%20focal%20loss%20training%20often%20leads%20to%20better%0Acalibration%20than%20cross-entropy%20training.%20For%20this%2C%20we%20prove%20that%20focal%20loss%20can%0Abe%20decomposed%20into%20a%20confidence-raising%20transformation%20and%20a%20proper%20loss.%20This%0Ais%20why%20focal%20loss%20pushes%20the%20model%20to%20provide%20under-confident%20predictions%20on%0Athe%20training%20data%2C%20resulting%20in%20being%20better%20calibrated%20on%20the%20test%20data%2C%20due%0Ato%20the%20generalization%20gap.%20Secondly%2C%20we%20reveal%20a%20strong%20connection%20between%0Atemperature%20scaling%20and%20focal%20loss%20through%20its%20confidence-raising%0Atransformation%2C%20which%20we%20refer%20to%20as%20the%20focal%20calibration%20map.%20Thirdly%2C%20we%0Apropose%20focal%20temperature%20scaling%20-%20a%20new%20post-hoc%20calibration%20method%20combining%0Afocal%20calibration%20and%20temperature%20scaling.%20Our%20experiments%20on%20three%20image%0Aclassification%20datasets%20demonstrate%20that%20focal%20temperature%20scaling%20outperforms%0Astandard%20temperature%20scaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11598v1&entry.124074799=Read"},
{"title": "PathMLP: Smooth Path Towards High-order Homophily", "author": "Jiajun Zhou and Chenxuan Xie and Shengbo Gong and Jiaxu Qian and Shanqing Yu and Qi Xuan and Xiaoniu Yang", "abstract": "  Real-world graphs exhibit increasing heterophily, where nodes no longer tend\nto be connected to nodes with the same label, challenging the homophily\nassumption of classical graph neural networks (GNNs) and impeding their\nperformance. Intriguingly, from the observation of heterophilous data, we\nnotice that certain high-order information exhibits higher homophily, which\nmotivates us to involve high-order information in node representation learning.\nHowever, common practices in GNNs to acquire high-order information mainly\nthrough increasing model depth and altering message-passing mechanisms, which,\nalbeit effective to a certain extent, suffer from three shortcomings: 1)\nover-smoothing due to excessive model depth and propagation times; 2)\nhigh-order information is not fully utilized; 3) low computational efficiency.\nIn this regard, we design a similarity-based path sampling strategy to capture\nsmooth paths containing high-order homophily. Then we propose a lightweight\nmodel based on multi-layer perceptrons (MLP), named PathMLP, which can encode\nmessages carried by paths via simple transformation and concatenation\noperations, and effectively learn node representations in heterophilous graphs\nthrough adaptive path aggregation. Extensive experiments demonstrate that our\nmethod outperforms baselines on 16 out of 20 datasets, underlining its\neffectiveness and superiority in alleviating the heterophily problem. In\naddition, our method is immune to over-smoothing and has high computational\nefficiency. The source code will be available in\nhttps://github.com/Graph4Sec-Team/PathMLP.\n", "link": "http://arxiv.org/abs/2306.13532v2", "date": "2024-08-21", "relevancy": 1.9326, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4859}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4831}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PathMLP%3A%20Smooth%20Path%20Towards%20High-order%20Homophily&body=Title%3A%20PathMLP%3A%20Smooth%20Path%20Towards%20High-order%20Homophily%0AAuthor%3A%20Jiajun%20Zhou%20and%20Chenxuan%20Xie%20and%20Shengbo%20Gong%20and%20Jiaxu%20Qian%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang%0AAbstract%3A%20%20%20Real-world%20graphs%20exhibit%20increasing%20heterophily%2C%20where%20nodes%20no%20longer%20tend%0Ato%20be%20connected%20to%20nodes%20with%20the%20same%20label%2C%20challenging%20the%20homophily%0Aassumption%20of%20classical%20graph%20neural%20networks%20%28GNNs%29%20and%20impeding%20their%0Aperformance.%20Intriguingly%2C%20from%20the%20observation%20of%20heterophilous%20data%2C%20we%0Anotice%20that%20certain%20high-order%20information%20exhibits%20higher%20homophily%2C%20which%0Amotivates%20us%20to%20involve%20high-order%20information%20in%20node%20representation%20learning.%0AHowever%2C%20common%20practices%20in%20GNNs%20to%20acquire%20high-order%20information%20mainly%0Athrough%20increasing%20model%20depth%20and%20altering%20message-passing%20mechanisms%2C%20which%2C%0Aalbeit%20effective%20to%20a%20certain%20extent%2C%20suffer%20from%20three%20shortcomings%3A%201%29%0Aover-smoothing%20due%20to%20excessive%20model%20depth%20and%20propagation%20times%3B%202%29%0Ahigh-order%20information%20is%20not%20fully%20utilized%3B%203%29%20low%20computational%20efficiency.%0AIn%20this%20regard%2C%20we%20design%20a%20similarity-based%20path%20sampling%20strategy%20to%20capture%0Asmooth%20paths%20containing%20high-order%20homophily.%20Then%20we%20propose%20a%20lightweight%0Amodel%20based%20on%20multi-layer%20perceptrons%20%28MLP%29%2C%20named%20PathMLP%2C%20which%20can%20encode%0Amessages%20carried%20by%20paths%20via%20simple%20transformation%20and%20concatenation%0Aoperations%2C%20and%20effectively%20learn%20node%20representations%20in%20heterophilous%20graphs%0Athrough%20adaptive%20path%20aggregation.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20baselines%20on%2016%20out%20of%2020%20datasets%2C%20underlining%20its%0Aeffectiveness%20and%20superiority%20in%20alleviating%20the%20heterophily%20problem.%20In%0Aaddition%2C%20our%20method%20is%20immune%20to%20over-smoothing%20and%20has%20high%20computational%0Aefficiency.%20The%20source%20code%20will%20be%20available%20in%0Ahttps%3A//github.com/Graph4Sec-Team/PathMLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13532v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathMLP%253A%2520Smooth%2520Path%2520Towards%2520High-order%2520Homophily%26entry.906535625%3DJiajun%2520Zhou%2520and%2520Chenxuan%2520Xie%2520and%2520Shengbo%2520Gong%2520and%2520Jiaxu%2520Qian%2520and%2520Shanqing%2520Yu%2520and%2520Qi%2520Xuan%2520and%2520Xiaoniu%2520Yang%26entry.1292438233%3D%2520%2520Real-world%2520graphs%2520exhibit%2520increasing%2520heterophily%252C%2520where%2520nodes%2520no%2520longer%2520tend%250Ato%2520be%2520connected%2520to%2520nodes%2520with%2520the%2520same%2520label%252C%2520challenging%2520the%2520homophily%250Aassumption%2520of%2520classical%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520and%2520impeding%2520their%250Aperformance.%2520Intriguingly%252C%2520from%2520the%2520observation%2520of%2520heterophilous%2520data%252C%2520we%250Anotice%2520that%2520certain%2520high-order%2520information%2520exhibits%2520higher%2520homophily%252C%2520which%250Amotivates%2520us%2520to%2520involve%2520high-order%2520information%2520in%2520node%2520representation%2520learning.%250AHowever%252C%2520common%2520practices%2520in%2520GNNs%2520to%2520acquire%2520high-order%2520information%2520mainly%250Athrough%2520increasing%2520model%2520depth%2520and%2520altering%2520message-passing%2520mechanisms%252C%2520which%252C%250Aalbeit%2520effective%2520to%2520a%2520certain%2520extent%252C%2520suffer%2520from%2520three%2520shortcomings%253A%25201%2529%250Aover-smoothing%2520due%2520to%2520excessive%2520model%2520depth%2520and%2520propagation%2520times%253B%25202%2529%250Ahigh-order%2520information%2520is%2520not%2520fully%2520utilized%253B%25203%2529%2520low%2520computational%2520efficiency.%250AIn%2520this%2520regard%252C%2520we%2520design%2520a%2520similarity-based%2520path%2520sampling%2520strategy%2520to%2520capture%250Asmooth%2520paths%2520containing%2520high-order%2520homophily.%2520Then%2520we%2520propose%2520a%2520lightweight%250Amodel%2520based%2520on%2520multi-layer%2520perceptrons%2520%2528MLP%2529%252C%2520named%2520PathMLP%252C%2520which%2520can%2520encode%250Amessages%2520carried%2520by%2520paths%2520via%2520simple%2520transformation%2520and%2520concatenation%250Aoperations%252C%2520and%2520effectively%2520learn%2520node%2520representations%2520in%2520heterophilous%2520graphs%250Athrough%2520adaptive%2520path%2520aggregation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520baselines%2520on%252016%2520out%2520of%252020%2520datasets%252C%2520underlining%2520its%250Aeffectiveness%2520and%2520superiority%2520in%2520alleviating%2520the%2520heterophily%2520problem.%2520In%250Aaddition%252C%2520our%2520method%2520is%2520immune%2520to%2520over-smoothing%2520and%2520has%2520high%2520computational%250Aefficiency.%2520The%2520source%2520code%2520will%2520be%2520available%2520in%250Ahttps%253A//github.com/Graph4Sec-Team/PathMLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13532v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PathMLP%3A%20Smooth%20Path%20Towards%20High-order%20Homophily&entry.906535625=Jiajun%20Zhou%20and%20Chenxuan%20Xie%20and%20Shengbo%20Gong%20and%20Jiaxu%20Qian%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang&entry.1292438233=%20%20Real-world%20graphs%20exhibit%20increasing%20heterophily%2C%20where%20nodes%20no%20longer%20tend%0Ato%20be%20connected%20to%20nodes%20with%20the%20same%20label%2C%20challenging%20the%20homophily%0Aassumption%20of%20classical%20graph%20neural%20networks%20%28GNNs%29%20and%20impeding%20their%0Aperformance.%20Intriguingly%2C%20from%20the%20observation%20of%20heterophilous%20data%2C%20we%0Anotice%20that%20certain%20high-order%20information%20exhibits%20higher%20homophily%2C%20which%0Amotivates%20us%20to%20involve%20high-order%20information%20in%20node%20representation%20learning.%0AHowever%2C%20common%20practices%20in%20GNNs%20to%20acquire%20high-order%20information%20mainly%0Athrough%20increasing%20model%20depth%20and%20altering%20message-passing%20mechanisms%2C%20which%2C%0Aalbeit%20effective%20to%20a%20certain%20extent%2C%20suffer%20from%20three%20shortcomings%3A%201%29%0Aover-smoothing%20due%20to%20excessive%20model%20depth%20and%20propagation%20times%3B%202%29%0Ahigh-order%20information%20is%20not%20fully%20utilized%3B%203%29%20low%20computational%20efficiency.%0AIn%20this%20regard%2C%20we%20design%20a%20similarity-based%20path%20sampling%20strategy%20to%20capture%0Asmooth%20paths%20containing%20high-order%20homophily.%20Then%20we%20propose%20a%20lightweight%0Amodel%20based%20on%20multi-layer%20perceptrons%20%28MLP%29%2C%20named%20PathMLP%2C%20which%20can%20encode%0Amessages%20carried%20by%20paths%20via%20simple%20transformation%20and%20concatenation%0Aoperations%2C%20and%20effectively%20learn%20node%20representations%20in%20heterophilous%20graphs%0Athrough%20adaptive%20path%20aggregation.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20outperforms%20baselines%20on%2016%20out%20of%2020%20datasets%2C%20underlining%20its%0Aeffectiveness%20and%20superiority%20in%20alleviating%20the%20heterophily%20problem.%20In%0Aaddition%2C%20our%20method%20is%20immune%20to%20over-smoothing%20and%20has%20high%20computational%0Aefficiency.%20The%20source%20code%20will%20be%20available%20in%0Ahttps%3A//github.com/Graph4Sec-Team/PathMLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13532v2&entry.124074799=Read"},
{"title": "SBDet: A Symmetry-Breaking Object Detector via Relaxed\n  Rotation-Equivariance", "author": "Zhiqiang Wu and Yingjie Liu and Hanlin Dong and Xuan Tang and Jian Yang and Bo Jin and Mingsong Chen and Xian Wei", "abstract": "  Introducing Group Equivariant Convolution (GConv) empowers models to explore\nsymmetries hidden in visual data, improving their performance. However, in\nreal-world scenarios, objects or scenes often exhibit perturbations of a\nsymmetric system, specifically a deviation from a symmetric architecture, which\ncan be characterized by a non-trivial action of a symmetry group, known as\nSymmetry-Breaking. Traditional GConv methods are limited by the strict\noperation rules in the group space, only ensuring features remain strictly\nequivariant under limited group transformations, making it difficult to adapt\nto Symmetry-Breaking or non-rigid transformations. Motivated by this, we\nintroduce a novel Relaxed Rotation GConv (R2GConv) with our defined Relaxed\nRotation-Equivariant group $\\mathbf{R}_4$. Furthermore, we propose a Relaxed\nRotation-Equivariant Network (R2Net) as the backbone and further develop the\nSymmetry-Breaking Object Detector (SBDet) for 2D object detection built upon\nit. Experiments demonstrate the effectiveness of our proposed R2GConv in\nnatural image classification tasks, and SBDet achieves excellent performance in\nobject detection tasks with improved generalization capabilities and\nrobustness.\n", "link": "http://arxiv.org/abs/2408.11760v1", "date": "2024-08-21", "relevancy": 1.9289, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4841}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SBDet%3A%20A%20Symmetry-Breaking%20Object%20Detector%20via%20Relaxed%0A%20%20Rotation-Equivariance&body=Title%3A%20SBDet%3A%20A%20Symmetry-Breaking%20Object%20Detector%20via%20Relaxed%0A%20%20Rotation-Equivariance%0AAuthor%3A%20Zhiqiang%20Wu%20and%20Yingjie%20Liu%20and%20Hanlin%20Dong%20and%20Xuan%20Tang%20and%20Jian%20Yang%20and%20Bo%20Jin%20and%20Mingsong%20Chen%20and%20Xian%20Wei%0AAbstract%3A%20%20%20Introducing%20Group%20Equivariant%20Convolution%20%28GConv%29%20empowers%20models%20to%20explore%0Asymmetries%20hidden%20in%20visual%20data%2C%20improving%20their%20performance.%20However%2C%20in%0Areal-world%20scenarios%2C%20objects%20or%20scenes%20often%20exhibit%20perturbations%20of%20a%0Asymmetric%20system%2C%20specifically%20a%20deviation%20from%20a%20symmetric%20architecture%2C%20which%0Acan%20be%20characterized%20by%20a%20non-trivial%20action%20of%20a%20symmetry%20group%2C%20known%20as%0ASymmetry-Breaking.%20Traditional%20GConv%20methods%20are%20limited%20by%20the%20strict%0Aoperation%20rules%20in%20the%20group%20space%2C%20only%20ensuring%20features%20remain%20strictly%0Aequivariant%20under%20limited%20group%20transformations%2C%20making%20it%20difficult%20to%20adapt%0Ato%20Symmetry-Breaking%20or%20non-rigid%20transformations.%20Motivated%20by%20this%2C%20we%0Aintroduce%20a%20novel%20Relaxed%20Rotation%20GConv%20%28R2GConv%29%20with%20our%20defined%20Relaxed%0ARotation-Equivariant%20group%20%24%5Cmathbf%7BR%7D_4%24.%20Furthermore%2C%20we%20propose%20a%20Relaxed%0ARotation-Equivariant%20Network%20%28R2Net%29%20as%20the%20backbone%20and%20further%20develop%20the%0ASymmetry-Breaking%20Object%20Detector%20%28SBDet%29%20for%202D%20object%20detection%20built%20upon%0Ait.%20Experiments%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20R2GConv%20in%0Anatural%20image%20classification%20tasks%2C%20and%20SBDet%20achieves%20excellent%20performance%20in%0Aobject%20detection%20tasks%20with%20improved%20generalization%20capabilities%20and%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSBDet%253A%2520A%2520Symmetry-Breaking%2520Object%2520Detector%2520via%2520Relaxed%250A%2520%2520Rotation-Equivariance%26entry.906535625%3DZhiqiang%2520Wu%2520and%2520Yingjie%2520Liu%2520and%2520Hanlin%2520Dong%2520and%2520Xuan%2520Tang%2520and%2520Jian%2520Yang%2520and%2520Bo%2520Jin%2520and%2520Mingsong%2520Chen%2520and%2520Xian%2520Wei%26entry.1292438233%3D%2520%2520Introducing%2520Group%2520Equivariant%2520Convolution%2520%2528GConv%2529%2520empowers%2520models%2520to%2520explore%250Asymmetries%2520hidden%2520in%2520visual%2520data%252C%2520improving%2520their%2520performance.%2520However%252C%2520in%250Areal-world%2520scenarios%252C%2520objects%2520or%2520scenes%2520often%2520exhibit%2520perturbations%2520of%2520a%250Asymmetric%2520system%252C%2520specifically%2520a%2520deviation%2520from%2520a%2520symmetric%2520architecture%252C%2520which%250Acan%2520be%2520characterized%2520by%2520a%2520non-trivial%2520action%2520of%2520a%2520symmetry%2520group%252C%2520known%2520as%250ASymmetry-Breaking.%2520Traditional%2520GConv%2520methods%2520are%2520limited%2520by%2520the%2520strict%250Aoperation%2520rules%2520in%2520the%2520group%2520space%252C%2520only%2520ensuring%2520features%2520remain%2520strictly%250Aequivariant%2520under%2520limited%2520group%2520transformations%252C%2520making%2520it%2520difficult%2520to%2520adapt%250Ato%2520Symmetry-Breaking%2520or%2520non-rigid%2520transformations.%2520Motivated%2520by%2520this%252C%2520we%250Aintroduce%2520a%2520novel%2520Relaxed%2520Rotation%2520GConv%2520%2528R2GConv%2529%2520with%2520our%2520defined%2520Relaxed%250ARotation-Equivariant%2520group%2520%2524%255Cmathbf%257BR%257D_4%2524.%2520Furthermore%252C%2520we%2520propose%2520a%2520Relaxed%250ARotation-Equivariant%2520Network%2520%2528R2Net%2529%2520as%2520the%2520backbone%2520and%2520further%2520develop%2520the%250ASymmetry-Breaking%2520Object%2520Detector%2520%2528SBDet%2529%2520for%25202D%2520object%2520detection%2520built%2520upon%250Ait.%2520Experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520R2GConv%2520in%250Anatural%2520image%2520classification%2520tasks%252C%2520and%2520SBDet%2520achieves%2520excellent%2520performance%2520in%250Aobject%2520detection%2520tasks%2520with%2520improved%2520generalization%2520capabilities%2520and%250Arobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SBDet%3A%20A%20Symmetry-Breaking%20Object%20Detector%20via%20Relaxed%0A%20%20Rotation-Equivariance&entry.906535625=Zhiqiang%20Wu%20and%20Yingjie%20Liu%20and%20Hanlin%20Dong%20and%20Xuan%20Tang%20and%20Jian%20Yang%20and%20Bo%20Jin%20and%20Mingsong%20Chen%20and%20Xian%20Wei&entry.1292438233=%20%20Introducing%20Group%20Equivariant%20Convolution%20%28GConv%29%20empowers%20models%20to%20explore%0Asymmetries%20hidden%20in%20visual%20data%2C%20improving%20their%20performance.%20However%2C%20in%0Areal-world%20scenarios%2C%20objects%20or%20scenes%20often%20exhibit%20perturbations%20of%20a%0Asymmetric%20system%2C%20specifically%20a%20deviation%20from%20a%20symmetric%20architecture%2C%20which%0Acan%20be%20characterized%20by%20a%20non-trivial%20action%20of%20a%20symmetry%20group%2C%20known%20as%0ASymmetry-Breaking.%20Traditional%20GConv%20methods%20are%20limited%20by%20the%20strict%0Aoperation%20rules%20in%20the%20group%20space%2C%20only%20ensuring%20features%20remain%20strictly%0Aequivariant%20under%20limited%20group%20transformations%2C%20making%20it%20difficult%20to%20adapt%0Ato%20Symmetry-Breaking%20or%20non-rigid%20transformations.%20Motivated%20by%20this%2C%20we%0Aintroduce%20a%20novel%20Relaxed%20Rotation%20GConv%20%28R2GConv%29%20with%20our%20defined%20Relaxed%0ARotation-Equivariant%20group%20%24%5Cmathbf%7BR%7D_4%24.%20Furthermore%2C%20we%20propose%20a%20Relaxed%0ARotation-Equivariant%20Network%20%28R2Net%29%20as%20the%20backbone%20and%20further%20develop%20the%0ASymmetry-Breaking%20Object%20Detector%20%28SBDet%29%20for%202D%20object%20detection%20built%20upon%0Ait.%20Experiments%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20R2GConv%20in%0Anatural%20image%20classification%20tasks%2C%20and%20SBDet%20achieves%20excellent%20performance%20in%0Aobject%20detection%20tasks%20with%20improved%20generalization%20capabilities%20and%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11760v1&entry.124074799=Read"},
{"title": "Diversity and stylization of the contemporary user-generated visual arts\n  in the complexity-entropy plane", "author": "Seunghwan Kim and Byunghwee Lee and Wonjae Lee", "abstract": "  The advent of computational and numerical methods in recent times has\nprovided new avenues for analyzing art historiographical narratives and tracing\nthe evolution of art styles therein. Here, we investigate an evolutionary\nprocess underpinning the emergence and stylization of contemporary\nuser-generated visual art styles using the complexity-entropy (C-H) plane,\nwhich quantifies local structures in paintings. Informatizing 149,780 images\ncurated in DeviantArt and Behance platforms from 2010 to 2020, we analyze the\nrelationship between local information of the C-H space and multi-level image\nfeatures generated by a deep neural network and a feature extraction algorithm.\nThe results reveal significant statistical relationships between the C-H\ninformation of visual artistic styles and the dissimilarities of the\nmulti-level image features over time within groups of artworks. By disclosing a\nparticular C-H region where the diversity of image representations is\nnoticeably manifested, our analyses reveal an empirical condition of emerging\nstyles that are both novel in the C-H plane and characterized by greater\nstylistic diversity. Our research shows that visual art analyses combined with\nphysics-inspired methodologies and machine learning, can provide macroscopic\ninsights into quantitatively mapping relevant characteristics of an\nevolutionary process underpinning the creative stylization of uncharted visual\narts of given groups and time.\n", "link": "http://arxiv.org/abs/2408.10356v2", "date": "2024-08-21", "relevancy": 1.9226, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5344}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4699}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversity%20and%20stylization%20of%20the%20contemporary%20user-generated%20visual%20arts%0A%20%20in%20the%20complexity-entropy%20plane&body=Title%3A%20Diversity%20and%20stylization%20of%20the%20contemporary%20user-generated%20visual%20arts%0A%20%20in%20the%20complexity-entropy%20plane%0AAuthor%3A%20Seunghwan%20Kim%20and%20Byunghwee%20Lee%20and%20Wonjae%20Lee%0AAbstract%3A%20%20%20The%20advent%20of%20computational%20and%20numerical%20methods%20in%20recent%20times%20has%0Aprovided%20new%20avenues%20for%20analyzing%20art%20historiographical%20narratives%20and%20tracing%0Athe%20evolution%20of%20art%20styles%20therein.%20Here%2C%20we%20investigate%20an%20evolutionary%0Aprocess%20underpinning%20the%20emergence%20and%20stylization%20of%20contemporary%0Auser-generated%20visual%20art%20styles%20using%20the%20complexity-entropy%20%28C-H%29%20plane%2C%0Awhich%20quantifies%20local%20structures%20in%20paintings.%20Informatizing%20149%2C780%20images%0Acurated%20in%20DeviantArt%20and%20Behance%20platforms%20from%202010%20to%202020%2C%20we%20analyze%20the%0Arelationship%20between%20local%20information%20of%20the%20C-H%20space%20and%20multi-level%20image%0Afeatures%20generated%20by%20a%20deep%20neural%20network%20and%20a%20feature%20extraction%20algorithm.%0AThe%20results%20reveal%20significant%20statistical%20relationships%20between%20the%20C-H%0Ainformation%20of%20visual%20artistic%20styles%20and%20the%20dissimilarities%20of%20the%0Amulti-level%20image%20features%20over%20time%20within%20groups%20of%20artworks.%20By%20disclosing%20a%0Aparticular%20C-H%20region%20where%20the%20diversity%20of%20image%20representations%20is%0Anoticeably%20manifested%2C%20our%20analyses%20reveal%20an%20empirical%20condition%20of%20emerging%0Astyles%20that%20are%20both%20novel%20in%20the%20C-H%20plane%20and%20characterized%20by%20greater%0Astylistic%20diversity.%20Our%20research%20shows%20that%20visual%20art%20analyses%20combined%20with%0Aphysics-inspired%20methodologies%20and%20machine%20learning%2C%20can%20provide%20macroscopic%0Ainsights%20into%20quantitatively%20mapping%20relevant%20characteristics%20of%20an%0Aevolutionary%20process%20underpinning%20the%20creative%20stylization%20of%20uncharted%20visual%0Aarts%20of%20given%20groups%20and%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10356v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversity%2520and%2520stylization%2520of%2520the%2520contemporary%2520user-generated%2520visual%2520arts%250A%2520%2520in%2520the%2520complexity-entropy%2520plane%26entry.906535625%3DSeunghwan%2520Kim%2520and%2520Byunghwee%2520Lee%2520and%2520Wonjae%2520Lee%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520computational%2520and%2520numerical%2520methods%2520in%2520recent%2520times%2520has%250Aprovided%2520new%2520avenues%2520for%2520analyzing%2520art%2520historiographical%2520narratives%2520and%2520tracing%250Athe%2520evolution%2520of%2520art%2520styles%2520therein.%2520Here%252C%2520we%2520investigate%2520an%2520evolutionary%250Aprocess%2520underpinning%2520the%2520emergence%2520and%2520stylization%2520of%2520contemporary%250Auser-generated%2520visual%2520art%2520styles%2520using%2520the%2520complexity-entropy%2520%2528C-H%2529%2520plane%252C%250Awhich%2520quantifies%2520local%2520structures%2520in%2520paintings.%2520Informatizing%2520149%252C780%2520images%250Acurated%2520in%2520DeviantArt%2520and%2520Behance%2520platforms%2520from%25202010%2520to%25202020%252C%2520we%2520analyze%2520the%250Arelationship%2520between%2520local%2520information%2520of%2520the%2520C-H%2520space%2520and%2520multi-level%2520image%250Afeatures%2520generated%2520by%2520a%2520deep%2520neural%2520network%2520and%2520a%2520feature%2520extraction%2520algorithm.%250AThe%2520results%2520reveal%2520significant%2520statistical%2520relationships%2520between%2520the%2520C-H%250Ainformation%2520of%2520visual%2520artistic%2520styles%2520and%2520the%2520dissimilarities%2520of%2520the%250Amulti-level%2520image%2520features%2520over%2520time%2520within%2520groups%2520of%2520artworks.%2520By%2520disclosing%2520a%250Aparticular%2520C-H%2520region%2520where%2520the%2520diversity%2520of%2520image%2520representations%2520is%250Anoticeably%2520manifested%252C%2520our%2520analyses%2520reveal%2520an%2520empirical%2520condition%2520of%2520emerging%250Astyles%2520that%2520are%2520both%2520novel%2520in%2520the%2520C-H%2520plane%2520and%2520characterized%2520by%2520greater%250Astylistic%2520diversity.%2520Our%2520research%2520shows%2520that%2520visual%2520art%2520analyses%2520combined%2520with%250Aphysics-inspired%2520methodologies%2520and%2520machine%2520learning%252C%2520can%2520provide%2520macroscopic%250Ainsights%2520into%2520quantitatively%2520mapping%2520relevant%2520characteristics%2520of%2520an%250Aevolutionary%2520process%2520underpinning%2520the%2520creative%2520stylization%2520of%2520uncharted%2520visual%250Aarts%2520of%2520given%2520groups%2520and%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10356v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversity%20and%20stylization%20of%20the%20contemporary%20user-generated%20visual%20arts%0A%20%20in%20the%20complexity-entropy%20plane&entry.906535625=Seunghwan%20Kim%20and%20Byunghwee%20Lee%20and%20Wonjae%20Lee&entry.1292438233=%20%20The%20advent%20of%20computational%20and%20numerical%20methods%20in%20recent%20times%20has%0Aprovided%20new%20avenues%20for%20analyzing%20art%20historiographical%20narratives%20and%20tracing%0Athe%20evolution%20of%20art%20styles%20therein.%20Here%2C%20we%20investigate%20an%20evolutionary%0Aprocess%20underpinning%20the%20emergence%20and%20stylization%20of%20contemporary%0Auser-generated%20visual%20art%20styles%20using%20the%20complexity-entropy%20%28C-H%29%20plane%2C%0Awhich%20quantifies%20local%20structures%20in%20paintings.%20Informatizing%20149%2C780%20images%0Acurated%20in%20DeviantArt%20and%20Behance%20platforms%20from%202010%20to%202020%2C%20we%20analyze%20the%0Arelationship%20between%20local%20information%20of%20the%20C-H%20space%20and%20multi-level%20image%0Afeatures%20generated%20by%20a%20deep%20neural%20network%20and%20a%20feature%20extraction%20algorithm.%0AThe%20results%20reveal%20significant%20statistical%20relationships%20between%20the%20C-H%0Ainformation%20of%20visual%20artistic%20styles%20and%20the%20dissimilarities%20of%20the%0Amulti-level%20image%20features%20over%20time%20within%20groups%20of%20artworks.%20By%20disclosing%20a%0Aparticular%20C-H%20region%20where%20the%20diversity%20of%20image%20representations%20is%0Anoticeably%20manifested%2C%20our%20analyses%20reveal%20an%20empirical%20condition%20of%20emerging%0Astyles%20that%20are%20both%20novel%20in%20the%20C-H%20plane%20and%20characterized%20by%20greater%0Astylistic%20diversity.%20Our%20research%20shows%20that%20visual%20art%20analyses%20combined%20with%0Aphysics-inspired%20methodologies%20and%20machine%20learning%2C%20can%20provide%20macroscopic%0Ainsights%20into%20quantitatively%20mapping%20relevant%20characteristics%20of%20an%0Aevolutionary%20process%20underpinning%20the%20creative%20stylization%20of%20uncharted%20visual%0Aarts%20of%20given%20groups%20and%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10356v2&entry.124074799=Read"},
{"title": "Spike-and-slab shrinkage priors for structurally sparse Bayesian neural\n  networks", "author": "Sanket Jantre and Shrijita Bhattacharya and Tapabrata Maiti", "abstract": "  Network complexity and computational efficiency have become increasingly\nsignificant aspects of deep learning. Sparse deep learning addresses these\nchallenges by recovering a sparse representation of the underlying target\nfunction by reducing heavily over-parameterized deep neural networks.\nSpecifically, deep neural architectures compressed via structured sparsity\n(e.g. node sparsity) provide low latency inference, higher data throughput, and\nreduced energy consumption. In this paper, we explore two well-established\nshrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian\nneural networks. To this end, we propose structurally sparse Bayesian neural\nnetworks which systematically prune excessive nodes with (i) Spike-and-Slab\nGroup Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors,\nand develop computationally tractable variational inference including\ncontinuous relaxation of Bernoulli variables. We establish the contraction\nrates of the variational posterior of our proposed models as a function of the\nnetwork topology, layer-wise node cardinalities, and bounds on the network\nweights. We empirically demonstrate the competitive performance of our models\ncompared to the baseline models in prediction accuracy, model compression, and\ninference latency.\n", "link": "http://arxiv.org/abs/2308.09104v2", "date": "2024-08-21", "relevancy": 1.9199, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4852}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spike-and-slab%20shrinkage%20priors%20for%20structurally%20sparse%20Bayesian%20neural%0A%20%20networks&body=Title%3A%20Spike-and-slab%20shrinkage%20priors%20for%20structurally%20sparse%20Bayesian%20neural%0A%20%20networks%0AAuthor%3A%20Sanket%20Jantre%20and%20Shrijita%20Bhattacharya%20and%20Tapabrata%20Maiti%0AAbstract%3A%20%20%20Network%20complexity%20and%20computational%20efficiency%20have%20become%20increasingly%0Asignificant%20aspects%20of%20deep%20learning.%20Sparse%20deep%20learning%20addresses%20these%0Achallenges%20by%20recovering%20a%20sparse%20representation%20of%20the%20underlying%20target%0Afunction%20by%20reducing%20heavily%20over-parameterized%20deep%20neural%20networks.%0ASpecifically%2C%20deep%20neural%20architectures%20compressed%20via%20structured%20sparsity%0A%28e.g.%20node%20sparsity%29%20provide%20low%20latency%20inference%2C%20higher%20data%20throughput%2C%20and%0Areduced%20energy%20consumption.%20In%20this%20paper%2C%20we%20explore%20two%20well-established%0Ashrinkage%20techniques%2C%20Lasso%20and%20Horseshoe%2C%20for%20model%20compression%20in%20Bayesian%0Aneural%20networks.%20To%20this%20end%2C%20we%20propose%20structurally%20sparse%20Bayesian%20neural%0Anetworks%20which%20systematically%20prune%20excessive%20nodes%20with%20%28i%29%20Spike-and-Slab%0AGroup%20Lasso%20%28SS-GL%29%2C%20and%20%28ii%29%20Spike-and-Slab%20Group%20Horseshoe%20%28SS-GHS%29%20priors%2C%0Aand%20develop%20computationally%20tractable%20variational%20inference%20including%0Acontinuous%20relaxation%20of%20Bernoulli%20variables.%20We%20establish%20the%20contraction%0Arates%20of%20the%20variational%20posterior%20of%20our%20proposed%20models%20as%20a%20function%20of%20the%0Anetwork%20topology%2C%20layer-wise%20node%20cardinalities%2C%20and%20bounds%20on%20the%20network%0Aweights.%20We%20empirically%20demonstrate%20the%20competitive%20performance%20of%20our%20models%0Acompared%20to%20the%20baseline%20models%20in%20prediction%20accuracy%2C%20model%20compression%2C%20and%0Ainference%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09104v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpike-and-slab%2520shrinkage%2520priors%2520for%2520structurally%2520sparse%2520Bayesian%2520neural%250A%2520%2520networks%26entry.906535625%3DSanket%2520Jantre%2520and%2520Shrijita%2520Bhattacharya%2520and%2520Tapabrata%2520Maiti%26entry.1292438233%3D%2520%2520Network%2520complexity%2520and%2520computational%2520efficiency%2520have%2520become%2520increasingly%250Asignificant%2520aspects%2520of%2520deep%2520learning.%2520Sparse%2520deep%2520learning%2520addresses%2520these%250Achallenges%2520by%2520recovering%2520a%2520sparse%2520representation%2520of%2520the%2520underlying%2520target%250Afunction%2520by%2520reducing%2520heavily%2520over-parameterized%2520deep%2520neural%2520networks.%250ASpecifically%252C%2520deep%2520neural%2520architectures%2520compressed%2520via%2520structured%2520sparsity%250A%2528e.g.%2520node%2520sparsity%2529%2520provide%2520low%2520latency%2520inference%252C%2520higher%2520data%2520throughput%252C%2520and%250Areduced%2520energy%2520consumption.%2520In%2520this%2520paper%252C%2520we%2520explore%2520two%2520well-established%250Ashrinkage%2520techniques%252C%2520Lasso%2520and%2520Horseshoe%252C%2520for%2520model%2520compression%2520in%2520Bayesian%250Aneural%2520networks.%2520To%2520this%2520end%252C%2520we%2520propose%2520structurally%2520sparse%2520Bayesian%2520neural%250Anetworks%2520which%2520systematically%2520prune%2520excessive%2520nodes%2520with%2520%2528i%2529%2520Spike-and-Slab%250AGroup%2520Lasso%2520%2528SS-GL%2529%252C%2520and%2520%2528ii%2529%2520Spike-and-Slab%2520Group%2520Horseshoe%2520%2528SS-GHS%2529%2520priors%252C%250Aand%2520develop%2520computationally%2520tractable%2520variational%2520inference%2520including%250Acontinuous%2520relaxation%2520of%2520Bernoulli%2520variables.%2520We%2520establish%2520the%2520contraction%250Arates%2520of%2520the%2520variational%2520posterior%2520of%2520our%2520proposed%2520models%2520as%2520a%2520function%2520of%2520the%250Anetwork%2520topology%252C%2520layer-wise%2520node%2520cardinalities%252C%2520and%2520bounds%2520on%2520the%2520network%250Aweights.%2520We%2520empirically%2520demonstrate%2520the%2520competitive%2520performance%2520of%2520our%2520models%250Acompared%2520to%2520the%2520baseline%2520models%2520in%2520prediction%2520accuracy%252C%2520model%2520compression%252C%2520and%250Ainference%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09104v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spike-and-slab%20shrinkage%20priors%20for%20structurally%20sparse%20Bayesian%20neural%0A%20%20networks&entry.906535625=Sanket%20Jantre%20and%20Shrijita%20Bhattacharya%20and%20Tapabrata%20Maiti&entry.1292438233=%20%20Network%20complexity%20and%20computational%20efficiency%20have%20become%20increasingly%0Asignificant%20aspects%20of%20deep%20learning.%20Sparse%20deep%20learning%20addresses%20these%0Achallenges%20by%20recovering%20a%20sparse%20representation%20of%20the%20underlying%20target%0Afunction%20by%20reducing%20heavily%20over-parameterized%20deep%20neural%20networks.%0ASpecifically%2C%20deep%20neural%20architectures%20compressed%20via%20structured%20sparsity%0A%28e.g.%20node%20sparsity%29%20provide%20low%20latency%20inference%2C%20higher%20data%20throughput%2C%20and%0Areduced%20energy%20consumption.%20In%20this%20paper%2C%20we%20explore%20two%20well-established%0Ashrinkage%20techniques%2C%20Lasso%20and%20Horseshoe%2C%20for%20model%20compression%20in%20Bayesian%0Aneural%20networks.%20To%20this%20end%2C%20we%20propose%20structurally%20sparse%20Bayesian%20neural%0Anetworks%20which%20systematically%20prune%20excessive%20nodes%20with%20%28i%29%20Spike-and-Slab%0AGroup%20Lasso%20%28SS-GL%29%2C%20and%20%28ii%29%20Spike-and-Slab%20Group%20Horseshoe%20%28SS-GHS%29%20priors%2C%0Aand%20develop%20computationally%20tractable%20variational%20inference%20including%0Acontinuous%20relaxation%20of%20Bernoulli%20variables.%20We%20establish%20the%20contraction%0Arates%20of%20the%20variational%20posterior%20of%20our%20proposed%20models%20as%20a%20function%20of%20the%0Anetwork%20topology%2C%20layer-wise%20node%20cardinalities%2C%20and%20bounds%20on%20the%20network%0Aweights.%20We%20empirically%20demonstrate%20the%20competitive%20performance%20of%20our%20models%0Acompared%20to%20the%20baseline%20models%20in%20prediction%20accuracy%2C%20model%20compression%2C%20and%0Ainference%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09104v2&entry.124074799=Read"},
{"title": "Just Project! Multi-Channel Despeckling, the Easy Way", "author": "Lo\u00efc Denis and Emanuele Dalsasso and Florence Tupin", "abstract": "  Reducing speckle fluctuations in multi-channel SAR images is essential in\nmany applications of SAR imaging such as polarimetric classification or\ninterferometric height estimation. While single-channel despeckling has widely\nbenefited from the application of deep learning techniques, extensions to\nmulti-channel SAR images are much more challenging.This paper introduces\nMuChaPro, a generic framework that exploits existing single-channel despeckling\nmethods. The key idea is to generate numerous single-channel projections,\nrestore these projections, and recombine them into the final multi-channel\nestimate. This simple approach is shown to be effective in polarimetric and/or\ninterferometric modalities. A special appeal of MuChaPro is the possibility to\napply a self-supervised training strategy to learn sensor-specific networks for\nsingle-channel despeckling.\n", "link": "http://arxiv.org/abs/2408.11531v1", "date": "2024-08-21", "relevancy": 1.9157, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4883}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Just%20Project%21%20Multi-Channel%20Despeckling%2C%20the%20Easy%20Way&body=Title%3A%20Just%20Project%21%20Multi-Channel%20Despeckling%2C%20the%20Easy%20Way%0AAuthor%3A%20Lo%C3%AFc%20Denis%20and%20Emanuele%20Dalsasso%20and%20Florence%20Tupin%0AAbstract%3A%20%20%20Reducing%20speckle%20fluctuations%20in%20multi-channel%20SAR%20images%20is%20essential%20in%0Amany%20applications%20of%20SAR%20imaging%20such%20as%20polarimetric%20classification%20or%0Ainterferometric%20height%20estimation.%20While%20single-channel%20despeckling%20has%20widely%0Abenefited%20from%20the%20application%20of%20deep%20learning%20techniques%2C%20extensions%20to%0Amulti-channel%20SAR%20images%20are%20much%20more%20challenging.This%20paper%20introduces%0AMuChaPro%2C%20a%20generic%20framework%20that%20exploits%20existing%20single-channel%20despeckling%0Amethods.%20The%20key%20idea%20is%20to%20generate%20numerous%20single-channel%20projections%2C%0Arestore%20these%20projections%2C%20and%20recombine%20them%20into%20the%20final%20multi-channel%0Aestimate.%20This%20simple%20approach%20is%20shown%20to%20be%20effective%20in%20polarimetric%20and/or%0Ainterferometric%20modalities.%20A%20special%20appeal%20of%20MuChaPro%20is%20the%20possibility%20to%0Aapply%20a%20self-supervised%20training%20strategy%20to%20learn%20sensor-specific%20networks%20for%0Asingle-channel%20despeckling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJust%2520Project%2521%2520Multi-Channel%2520Despeckling%252C%2520the%2520Easy%2520Way%26entry.906535625%3DLo%25C3%25AFc%2520Denis%2520and%2520Emanuele%2520Dalsasso%2520and%2520Florence%2520Tupin%26entry.1292438233%3D%2520%2520Reducing%2520speckle%2520fluctuations%2520in%2520multi-channel%2520SAR%2520images%2520is%2520essential%2520in%250Amany%2520applications%2520of%2520SAR%2520imaging%2520such%2520as%2520polarimetric%2520classification%2520or%250Ainterferometric%2520height%2520estimation.%2520While%2520single-channel%2520despeckling%2520has%2520widely%250Abenefited%2520from%2520the%2520application%2520of%2520deep%2520learning%2520techniques%252C%2520extensions%2520to%250Amulti-channel%2520SAR%2520images%2520are%2520much%2520more%2520challenging.This%2520paper%2520introduces%250AMuChaPro%252C%2520a%2520generic%2520framework%2520that%2520exploits%2520existing%2520single-channel%2520despeckling%250Amethods.%2520The%2520key%2520idea%2520is%2520to%2520generate%2520numerous%2520single-channel%2520projections%252C%250Arestore%2520these%2520projections%252C%2520and%2520recombine%2520them%2520into%2520the%2520final%2520multi-channel%250Aestimate.%2520This%2520simple%2520approach%2520is%2520shown%2520to%2520be%2520effective%2520in%2520polarimetric%2520and/or%250Ainterferometric%2520modalities.%2520A%2520special%2520appeal%2520of%2520MuChaPro%2520is%2520the%2520possibility%2520to%250Aapply%2520a%2520self-supervised%2520training%2520strategy%2520to%2520learn%2520sensor-specific%2520networks%2520for%250Asingle-channel%2520despeckling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Just%20Project%21%20Multi-Channel%20Despeckling%2C%20the%20Easy%20Way&entry.906535625=Lo%C3%AFc%20Denis%20and%20Emanuele%20Dalsasso%20and%20Florence%20Tupin&entry.1292438233=%20%20Reducing%20speckle%20fluctuations%20in%20multi-channel%20SAR%20images%20is%20essential%20in%0Amany%20applications%20of%20SAR%20imaging%20such%20as%20polarimetric%20classification%20or%0Ainterferometric%20height%20estimation.%20While%20single-channel%20despeckling%20has%20widely%0Abenefited%20from%20the%20application%20of%20deep%20learning%20techniques%2C%20extensions%20to%0Amulti-channel%20SAR%20images%20are%20much%20more%20challenging.This%20paper%20introduces%0AMuChaPro%2C%20a%20generic%20framework%20that%20exploits%20existing%20single-channel%20despeckling%0Amethods.%20The%20key%20idea%20is%20to%20generate%20numerous%20single-channel%20projections%2C%0Arestore%20these%20projections%2C%20and%20recombine%20them%20into%20the%20final%20multi-channel%0Aestimate.%20This%20simple%20approach%20is%20shown%20to%20be%20effective%20in%20polarimetric%20and/or%0Ainterferometric%20modalities.%20A%20special%20appeal%20of%20MuChaPro%20is%20the%20possibility%20to%0Aapply%20a%20self-supervised%20training%20strategy%20to%20learn%20sensor-specific%20networks%20for%0Asingle-channel%20despeckling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11531v1&entry.124074799=Read"},
{"title": "Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain\n  Diffusion Models", "author": "Chun-Yen Shih and Li-Xuan Peng and Jia-Wei Liao and Ernie Chu and Cheng-Fu Chou and Jun-Cheng Chen", "abstract": "  Diffusion Models have emerged as powerful generative models for high-quality\nimage synthesis, with many subsequent image editing techniques based on them.\nHowever, the ease of text-based image editing introduces significant risks,\nsuch as malicious editing for scams or intellectual property infringement.\nPrevious works have attempted to safeguard images from diffusion-based editing\nby adding imperceptible perturbations. These methods are costly and\nspecifically target prevalent Latent Diffusion Models (LDMs), while\nPixel-domain Diffusion Models (PDMs) remain largely unexplored and robust\nagainst such attacks. Our work addresses this gap by proposing a novel\nattacking framework with a feature representation attack loss that exploits\nvulnerabilities in denoising UNets and a latent optimization strategy to\nenhance the naturalness of protected images. Extensive experiments demonstrate\nthe effectiveness of our approach in attacking dominant PDM-based editing\nmethods (e.g., SDEdit) while maintaining reasonable protection fidelity and\nrobustness against common defense methods. Additionally, our framework is\nextensible to LDMs, achieving comparable performance to existing approaches.\n", "link": "http://arxiv.org/abs/2408.11810v1", "date": "2024-08-21", "relevancy": 1.7361, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6413}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5954}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel%20Is%20Not%20A%20Barrier%3A%20An%20Effective%20Evasion%20Attack%20for%20Pixel-Domain%0A%20%20Diffusion%20Models&body=Title%3A%20Pixel%20Is%20Not%20A%20Barrier%3A%20An%20Effective%20Evasion%20Attack%20for%20Pixel-Domain%0A%20%20Diffusion%20Models%0AAuthor%3A%20Chun-Yen%20Shih%20and%20Li-Xuan%20Peng%20and%20Jia-Wei%20Liao%20and%20Ernie%20Chu%20and%20Cheng-Fu%20Chou%20and%20Jun-Cheng%20Chen%0AAbstract%3A%20%20%20Diffusion%20Models%20have%20emerged%20as%20powerful%20generative%20models%20for%20high-quality%0Aimage%20synthesis%2C%20with%20many%20subsequent%20image%20editing%20techniques%20based%20on%20them.%0AHowever%2C%20the%20ease%20of%20text-based%20image%20editing%20introduces%20significant%20risks%2C%0Asuch%20as%20malicious%20editing%20for%20scams%20or%20intellectual%20property%20infringement.%0APrevious%20works%20have%20attempted%20to%20safeguard%20images%20from%20diffusion-based%20editing%0Aby%20adding%20imperceptible%20perturbations.%20These%20methods%20are%20costly%20and%0Aspecifically%20target%20prevalent%20Latent%20Diffusion%20Models%20%28LDMs%29%2C%20while%0APixel-domain%20Diffusion%20Models%20%28PDMs%29%20remain%20largely%20unexplored%20and%20robust%0Aagainst%20such%20attacks.%20Our%20work%20addresses%20this%20gap%20by%20proposing%20a%20novel%0Aattacking%20framework%20with%20a%20feature%20representation%20attack%20loss%20that%20exploits%0Avulnerabilities%20in%20denoising%20UNets%20and%20a%20latent%20optimization%20strategy%20to%0Aenhance%20the%20naturalness%20of%20protected%20images.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20of%20our%20approach%20in%20attacking%20dominant%20PDM-based%20editing%0Amethods%20%28e.g.%2C%20SDEdit%29%20while%20maintaining%20reasonable%20protection%20fidelity%20and%0Arobustness%20against%20common%20defense%20methods.%20Additionally%2C%20our%20framework%20is%0Aextensible%20to%20LDMs%2C%20achieving%20comparable%20performance%20to%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel%2520Is%2520Not%2520A%2520Barrier%253A%2520An%2520Effective%2520Evasion%2520Attack%2520for%2520Pixel-Domain%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DChun-Yen%2520Shih%2520and%2520Li-Xuan%2520Peng%2520and%2520Jia-Wei%2520Liao%2520and%2520Ernie%2520Chu%2520and%2520Cheng-Fu%2520Chou%2520and%2520Jun-Cheng%2520Chen%26entry.1292438233%3D%2520%2520Diffusion%2520Models%2520have%2520emerged%2520as%2520powerful%2520generative%2520models%2520for%2520high-quality%250Aimage%2520synthesis%252C%2520with%2520many%2520subsequent%2520image%2520editing%2520techniques%2520based%2520on%2520them.%250AHowever%252C%2520the%2520ease%2520of%2520text-based%2520image%2520editing%2520introduces%2520significant%2520risks%252C%250Asuch%2520as%2520malicious%2520editing%2520for%2520scams%2520or%2520intellectual%2520property%2520infringement.%250APrevious%2520works%2520have%2520attempted%2520to%2520safeguard%2520images%2520from%2520diffusion-based%2520editing%250Aby%2520adding%2520imperceptible%2520perturbations.%2520These%2520methods%2520are%2520costly%2520and%250Aspecifically%2520target%2520prevalent%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529%252C%2520while%250APixel-domain%2520Diffusion%2520Models%2520%2528PDMs%2529%2520remain%2520largely%2520unexplored%2520and%2520robust%250Aagainst%2520such%2520attacks.%2520Our%2520work%2520addresses%2520this%2520gap%2520by%2520proposing%2520a%2520novel%250Aattacking%2520framework%2520with%2520a%2520feature%2520representation%2520attack%2520loss%2520that%2520exploits%250Avulnerabilities%2520in%2520denoising%2520UNets%2520and%2520a%2520latent%2520optimization%2520strategy%2520to%250Aenhance%2520the%2520naturalness%2520of%2520protected%2520images.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520approach%2520in%2520attacking%2520dominant%2520PDM-based%2520editing%250Amethods%2520%2528e.g.%252C%2520SDEdit%2529%2520while%2520maintaining%2520reasonable%2520protection%2520fidelity%2520and%250Arobustness%2520against%2520common%2520defense%2520methods.%2520Additionally%252C%2520our%2520framework%2520is%250Aextensible%2520to%2520LDMs%252C%2520achieving%2520comparable%2520performance%2520to%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel%20Is%20Not%20A%20Barrier%3A%20An%20Effective%20Evasion%20Attack%20for%20Pixel-Domain%0A%20%20Diffusion%20Models&entry.906535625=Chun-Yen%20Shih%20and%20Li-Xuan%20Peng%20and%20Jia-Wei%20Liao%20and%20Ernie%20Chu%20and%20Cheng-Fu%20Chou%20and%20Jun-Cheng%20Chen&entry.1292438233=%20%20Diffusion%20Models%20have%20emerged%20as%20powerful%20generative%20models%20for%20high-quality%0Aimage%20synthesis%2C%20with%20many%20subsequent%20image%20editing%20techniques%20based%20on%20them.%0AHowever%2C%20the%20ease%20of%20text-based%20image%20editing%20introduces%20significant%20risks%2C%0Asuch%20as%20malicious%20editing%20for%20scams%20or%20intellectual%20property%20infringement.%0APrevious%20works%20have%20attempted%20to%20safeguard%20images%20from%20diffusion-based%20editing%0Aby%20adding%20imperceptible%20perturbations.%20These%20methods%20are%20costly%20and%0Aspecifically%20target%20prevalent%20Latent%20Diffusion%20Models%20%28LDMs%29%2C%20while%0APixel-domain%20Diffusion%20Models%20%28PDMs%29%20remain%20largely%20unexplored%20and%20robust%0Aagainst%20such%20attacks.%20Our%20work%20addresses%20this%20gap%20by%20proposing%20a%20novel%0Aattacking%20framework%20with%20a%20feature%20representation%20attack%20loss%20that%20exploits%0Avulnerabilities%20in%20denoising%20UNets%20and%20a%20latent%20optimization%20strategy%20to%0Aenhance%20the%20naturalness%20of%20protected%20images.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20of%20our%20approach%20in%20attacking%20dominant%20PDM-based%20editing%0Amethods%20%28e.g.%2C%20SDEdit%29%20while%20maintaining%20reasonable%20protection%20fidelity%20and%0Arobustness%20against%20common%20defense%20methods.%20Additionally%2C%20our%20framework%20is%0Aextensible%20to%20LDMs%2C%20achieving%20comparable%20performance%20to%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11810v1&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Enhancing the Understandability of\n  Generated Unit Tests", "author": "Amirhossein Deljouyi and Roham Koohestani and Maliheh Izadi and Andy Zaidman", "abstract": "  Automated unit test generators, particularly search-based software testing\ntools like EvoSuite, are capable of generating tests with high coverage.\nAlthough these generators alleviate the burden of writing unit tests, they\noften pose challenges for software engineers in terms of understanding the\ngenerated tests. To address this, we introduce UTGen, which combines\nsearch-based software testing and large language models to enhance the\nunderstandability of automatically generated test cases. We achieve this\nenhancement through contextualizing test data, improving identifier naming, and\nadding descriptive comments. Through a controlled experiment with 32\nparticipants from both academia and industry, we investigate how the\nunderstandability of unit tests affects a software engineer's ability to\nperform bug-fixing tasks. We selected bug-fixing to simulate a real-world\nscenario that emphasizes the importance of understandable test cases. We\nobserve that participants working on assignments with UTGen test cases fix up\nto 33% more bugs and use up to 20% less time when compared to baseline test\ncases. From the post-test questionnaire, we gathered that participants found\nthat enhanced test names, test data, and variable names improved their\nbug-fixing process.\n", "link": "http://arxiv.org/abs/2408.11710v1", "date": "2024-08-21", "relevancy": 1.8329, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4785}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4694}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Enhancing%20the%20Understandability%20of%0A%20%20Generated%20Unit%20Tests&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Enhancing%20the%20Understandability%20of%0A%20%20Generated%20Unit%20Tests%0AAuthor%3A%20Amirhossein%20Deljouyi%20and%20Roham%20Koohestani%20and%20Maliheh%20Izadi%20and%20Andy%20Zaidman%0AAbstract%3A%20%20%20Automated%20unit%20test%20generators%2C%20particularly%20search-based%20software%20testing%0Atools%20like%20EvoSuite%2C%20are%20capable%20of%20generating%20tests%20with%20high%20coverage.%0AAlthough%20these%20generators%20alleviate%20the%20burden%20of%20writing%20unit%20tests%2C%20they%0Aoften%20pose%20challenges%20for%20software%20engineers%20in%20terms%20of%20understanding%20the%0Agenerated%20tests.%20To%20address%20this%2C%20we%20introduce%20UTGen%2C%20which%20combines%0Asearch-based%20software%20testing%20and%20large%20language%20models%20to%20enhance%20the%0Aunderstandability%20of%20automatically%20generated%20test%20cases.%20We%20achieve%20this%0Aenhancement%20through%20contextualizing%20test%20data%2C%20improving%20identifier%20naming%2C%20and%0Aadding%20descriptive%20comments.%20Through%20a%20controlled%20experiment%20with%2032%0Aparticipants%20from%20both%20academia%20and%20industry%2C%20we%20investigate%20how%20the%0Aunderstandability%20of%20unit%20tests%20affects%20a%20software%20engineer%27s%20ability%20to%0Aperform%20bug-fixing%20tasks.%20We%20selected%20bug-fixing%20to%20simulate%20a%20real-world%0Ascenario%20that%20emphasizes%20the%20importance%20of%20understandable%20test%20cases.%20We%0Aobserve%20that%20participants%20working%20on%20assignments%20with%20UTGen%20test%20cases%20fix%20up%0Ato%2033%25%20more%20bugs%20and%20use%20up%20to%2020%25%20less%20time%20when%20compared%20to%20baseline%20test%0Acases.%20From%20the%20post-test%20questionnaire%2C%20we%20gathered%20that%20participants%20found%0Athat%20enhanced%20test%20names%2C%20test%20data%2C%20and%20variable%20names%20improved%20their%0Abug-fixing%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Enhancing%2520the%2520Understandability%2520of%250A%2520%2520Generated%2520Unit%2520Tests%26entry.906535625%3DAmirhossein%2520Deljouyi%2520and%2520Roham%2520Koohestani%2520and%2520Maliheh%2520Izadi%2520and%2520Andy%2520Zaidman%26entry.1292438233%3D%2520%2520Automated%2520unit%2520test%2520generators%252C%2520particularly%2520search-based%2520software%2520testing%250Atools%2520like%2520EvoSuite%252C%2520are%2520capable%2520of%2520generating%2520tests%2520with%2520high%2520coverage.%250AAlthough%2520these%2520generators%2520alleviate%2520the%2520burden%2520of%2520writing%2520unit%2520tests%252C%2520they%250Aoften%2520pose%2520challenges%2520for%2520software%2520engineers%2520in%2520terms%2520of%2520understanding%2520the%250Agenerated%2520tests.%2520To%2520address%2520this%252C%2520we%2520introduce%2520UTGen%252C%2520which%2520combines%250Asearch-based%2520software%2520testing%2520and%2520large%2520language%2520models%2520to%2520enhance%2520the%250Aunderstandability%2520of%2520automatically%2520generated%2520test%2520cases.%2520We%2520achieve%2520this%250Aenhancement%2520through%2520contextualizing%2520test%2520data%252C%2520improving%2520identifier%2520naming%252C%2520and%250Aadding%2520descriptive%2520comments.%2520Through%2520a%2520controlled%2520experiment%2520with%252032%250Aparticipants%2520from%2520both%2520academia%2520and%2520industry%252C%2520we%2520investigate%2520how%2520the%250Aunderstandability%2520of%2520unit%2520tests%2520affects%2520a%2520software%2520engineer%2527s%2520ability%2520to%250Aperform%2520bug-fixing%2520tasks.%2520We%2520selected%2520bug-fixing%2520to%2520simulate%2520a%2520real-world%250Ascenario%2520that%2520emphasizes%2520the%2520importance%2520of%2520understandable%2520test%2520cases.%2520We%250Aobserve%2520that%2520participants%2520working%2520on%2520assignments%2520with%2520UTGen%2520test%2520cases%2520fix%2520up%250Ato%252033%2525%2520more%2520bugs%2520and%2520use%2520up%2520to%252020%2525%2520less%2520time%2520when%2520compared%2520to%2520baseline%2520test%250Acases.%2520From%2520the%2520post-test%2520questionnaire%252C%2520we%2520gathered%2520that%2520participants%2520found%250Athat%2520enhanced%2520test%2520names%252C%2520test%2520data%252C%2520and%2520variable%2520names%2520improved%2520their%250Abug-fixing%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Enhancing%20the%20Understandability%20of%0A%20%20Generated%20Unit%20Tests&entry.906535625=Amirhossein%20Deljouyi%20and%20Roham%20Koohestani%20and%20Maliheh%20Izadi%20and%20Andy%20Zaidman&entry.1292438233=%20%20Automated%20unit%20test%20generators%2C%20particularly%20search-based%20software%20testing%0Atools%20like%20EvoSuite%2C%20are%20capable%20of%20generating%20tests%20with%20high%20coverage.%0AAlthough%20these%20generators%20alleviate%20the%20burden%20of%20writing%20unit%20tests%2C%20they%0Aoften%20pose%20challenges%20for%20software%20engineers%20in%20terms%20of%20understanding%20the%0Agenerated%20tests.%20To%20address%20this%2C%20we%20introduce%20UTGen%2C%20which%20combines%0Asearch-based%20software%20testing%20and%20large%20language%20models%20to%20enhance%20the%0Aunderstandability%20of%20automatically%20generated%20test%20cases.%20We%20achieve%20this%0Aenhancement%20through%20contextualizing%20test%20data%2C%20improving%20identifier%20naming%2C%20and%0Aadding%20descriptive%20comments.%20Through%20a%20controlled%20experiment%20with%2032%0Aparticipants%20from%20both%20academia%20and%20industry%2C%20we%20investigate%20how%20the%0Aunderstandability%20of%20unit%20tests%20affects%20a%20software%20engineer%27s%20ability%20to%0Aperform%20bug-fixing%20tasks.%20We%20selected%20bug-fixing%20to%20simulate%20a%20real-world%0Ascenario%20that%20emphasizes%20the%20importance%20of%20understandable%20test%20cases.%20We%0Aobserve%20that%20participants%20working%20on%20assignments%20with%20UTGen%20test%20cases%20fix%20up%0Ato%2033%25%20more%20bugs%20and%20use%20up%20to%2020%25%20less%20time%20when%20compared%20to%20baseline%20test%0Acases.%20From%20the%20post-test%20questionnaire%2C%20we%20gathered%20that%20participants%20found%0Athat%20enhanced%20test%20names%2C%20test%20data%2C%20and%20variable%20names%20improved%20their%0Abug-fixing%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11710v1&entry.124074799=Read"},
{"title": "Efficient Detection of Toxic Prompts in Large Language Models", "author": "Yi Liu and Junzhe Yu and Huijia Sun and Ling Shi and Gelei Deng and Yuqi Chen and Yang Liu", "abstract": "  Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs.\n", "link": "http://arxiv.org/abs/2408.11727v1", "date": "2024-08-21", "relevancy": 1.7623, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4558}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Detection%20of%20Toxic%20Prompts%20in%20Large%20Language%20Models&body=Title%3A%20Efficient%20Detection%20of%20Toxic%20Prompts%20in%20Large%20Language%20Models%0AAuthor%3A%20Yi%20Liu%20and%20Junzhe%20Yu%20and%20Huijia%20Sun%20and%20Ling%20Shi%20and%20Gelei%20Deng%20and%20Yuqi%20Chen%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20and%20Gemini%20have%20significantly%0Aadvanced%20natural%20language%20processing%2C%20enabling%20various%20applications%20such%20as%0Achatbots%20and%20automated%20content%20generation.%20However%2C%20these%20models%20can%20be%0Aexploited%20by%20malicious%20individuals%20who%20craft%20toxic%20prompts%20to%20elicit%20harmful%20or%0Aunethical%20responses.%20These%20individuals%20often%20employ%20jailbreaking%20techniques%20to%0Abypass%20safety%20mechanisms%2C%20highlighting%20the%20need%20for%20robust%20toxic%20prompt%0Adetection%20methods.%20Existing%20detection%20techniques%2C%20both%20blackbox%20and%20whitebox%2C%0Aface%20challenges%20related%20to%20the%20diversity%20of%20toxic%20prompts%2C%20scalability%2C%20and%0Acomputational%20efficiency.%20In%20response%2C%20we%20propose%20ToxicDetector%2C%20a%20lightweight%0Agreybox%20method%20designed%20to%20efficiently%20detect%20toxic%20prompts%20in%20LLMs.%0AToxicDetector%20leverages%20LLMs%20to%20create%20toxic%20concept%20prompts%2C%20uses%20embedding%0Avectors%20to%20form%20feature%20vectors%2C%20and%20employs%20a%20Multi-Layer%20Perceptron%20%28MLP%29%0Aclassifier%20for%20prompt%20classification.%20Our%20evaluation%20on%20various%20versions%20of%20the%0ALLama%20models%2C%20Gemma-2%2C%20and%20multiple%20datasets%20demonstrates%20that%20ToxicDetector%0Aachieves%20a%20high%20accuracy%20of%2096.39%5C%25%20and%20a%20low%20false%20positive%20rate%20of%202.00%5C%25%2C%0Aoutperforming%20state-of-the-art%20methods.%20Additionally%2C%20ToxicDetector%27s%0Aprocessing%20time%20of%200.0780%20seconds%20per%20prompt%20makes%20it%20highly%20suitable%20for%0Areal-time%20applications.%20ToxicDetector%20achieves%20high%20accuracy%2C%20efficiency%2C%20and%0Ascalability%2C%20making%20it%20a%20practical%20method%20for%20toxic%20prompt%20detection%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Detection%2520of%2520Toxic%2520Prompts%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DYi%2520Liu%2520and%2520Junzhe%2520Yu%2520and%2520Huijia%2520Sun%2520and%2520Ling%2520Shi%2520and%2520Gelei%2520Deng%2520and%2520Yuqi%2520Chen%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520like%2520ChatGPT%2520and%2520Gemini%2520have%2520significantly%250Aadvanced%2520natural%2520language%2520processing%252C%2520enabling%2520various%2520applications%2520such%2520as%250Achatbots%2520and%2520automated%2520content%2520generation.%2520However%252C%2520these%2520models%2520can%2520be%250Aexploited%2520by%2520malicious%2520individuals%2520who%2520craft%2520toxic%2520prompts%2520to%2520elicit%2520harmful%2520or%250Aunethical%2520responses.%2520These%2520individuals%2520often%2520employ%2520jailbreaking%2520techniques%2520to%250Abypass%2520safety%2520mechanisms%252C%2520highlighting%2520the%2520need%2520for%2520robust%2520toxic%2520prompt%250Adetection%2520methods.%2520Existing%2520detection%2520techniques%252C%2520both%2520blackbox%2520and%2520whitebox%252C%250Aface%2520challenges%2520related%2520to%2520the%2520diversity%2520of%2520toxic%2520prompts%252C%2520scalability%252C%2520and%250Acomputational%2520efficiency.%2520In%2520response%252C%2520we%2520propose%2520ToxicDetector%252C%2520a%2520lightweight%250Agreybox%2520method%2520designed%2520to%2520efficiently%2520detect%2520toxic%2520prompts%2520in%2520LLMs.%250AToxicDetector%2520leverages%2520LLMs%2520to%2520create%2520toxic%2520concept%2520prompts%252C%2520uses%2520embedding%250Avectors%2520to%2520form%2520feature%2520vectors%252C%2520and%2520employs%2520a%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%250Aclassifier%2520for%2520prompt%2520classification.%2520Our%2520evaluation%2520on%2520various%2520versions%2520of%2520the%250ALLama%2520models%252C%2520Gemma-2%252C%2520and%2520multiple%2520datasets%2520demonstrates%2520that%2520ToxicDetector%250Aachieves%2520a%2520high%2520accuracy%2520of%252096.39%255C%2525%2520and%2520a%2520low%2520false%2520positive%2520rate%2520of%25202.00%255C%2525%252C%250Aoutperforming%2520state-of-the-art%2520methods.%2520Additionally%252C%2520ToxicDetector%2527s%250Aprocessing%2520time%2520of%25200.0780%2520seconds%2520per%2520prompt%2520makes%2520it%2520highly%2520suitable%2520for%250Areal-time%2520applications.%2520ToxicDetector%2520achieves%2520high%2520accuracy%252C%2520efficiency%252C%2520and%250Ascalability%252C%2520making%2520it%2520a%2520practical%2520method%2520for%2520toxic%2520prompt%2520detection%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Detection%20of%20Toxic%20Prompts%20in%20Large%20Language%20Models&entry.906535625=Yi%20Liu%20and%20Junzhe%20Yu%20and%20Huijia%20Sun%20and%20Ling%20Shi%20and%20Gelei%20Deng%20and%20Yuqi%20Chen%20and%20Yang%20Liu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20and%20Gemini%20have%20significantly%0Aadvanced%20natural%20language%20processing%2C%20enabling%20various%20applications%20such%20as%0Achatbots%20and%20automated%20content%20generation.%20However%2C%20these%20models%20can%20be%0Aexploited%20by%20malicious%20individuals%20who%20craft%20toxic%20prompts%20to%20elicit%20harmful%20or%0Aunethical%20responses.%20These%20individuals%20often%20employ%20jailbreaking%20techniques%20to%0Abypass%20safety%20mechanisms%2C%20highlighting%20the%20need%20for%20robust%20toxic%20prompt%0Adetection%20methods.%20Existing%20detection%20techniques%2C%20both%20blackbox%20and%20whitebox%2C%0Aface%20challenges%20related%20to%20the%20diversity%20of%20toxic%20prompts%2C%20scalability%2C%20and%0Acomputational%20efficiency.%20In%20response%2C%20we%20propose%20ToxicDetector%2C%20a%20lightweight%0Agreybox%20method%20designed%20to%20efficiently%20detect%20toxic%20prompts%20in%20LLMs.%0AToxicDetector%20leverages%20LLMs%20to%20create%20toxic%20concept%20prompts%2C%20uses%20embedding%0Avectors%20to%20form%20feature%20vectors%2C%20and%20employs%20a%20Multi-Layer%20Perceptron%20%28MLP%29%0Aclassifier%20for%20prompt%20classification.%20Our%20evaluation%20on%20various%20versions%20of%20the%0ALLama%20models%2C%20Gemma-2%2C%20and%20multiple%20datasets%20demonstrates%20that%20ToxicDetector%0Aachieves%20a%20high%20accuracy%20of%2096.39%5C%25%20and%20a%20low%20false%20positive%20rate%20of%202.00%5C%25%2C%0Aoutperforming%20state-of-the-art%20methods.%20Additionally%2C%20ToxicDetector%27s%0Aprocessing%20time%20of%200.0780%20seconds%20per%20prompt%20makes%20it%20highly%20suitable%20for%0Areal-time%20applications.%20ToxicDetector%20achieves%20high%20accuracy%2C%20efficiency%2C%20and%0Ascalability%2C%20making%20it%20a%20practical%20method%20for%20toxic%20prompt%20detection%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11727v1&entry.124074799=Read"},
{"title": "Networked Communication for Mean-Field Games with Function Approximation\n  and Empirical Mean-Field Estimation", "author": "Patrick Benjamin and Alessandro Abate", "abstract": "  Recent works have provided algorithms by which decentralised agents, which\nmay be connected via a communication network, can learn equilibria in\nMean-Field Games from a single, non-episodic run of the empirical system.\nHowever, these algorithms are given for tabular settings: this computationally\nlimits the size of players' observation space, meaning that the algorithms are\nnot able to handle anything but small state spaces, nor to generalise beyond\npolicies depending on the ego player's state to so-called\n'population-dependent' policies. We address this limitation by introducing\nfunction approximation to the existing setting, drawing on the Munchausen\nOnline Mirror Descent method that has previously been employed only in\nfinite-horizon, episodic, centralised settings. While this permits us to\ninclude the population's mean-field distribution in the observation for each\nplayer's policy, it is arguably unrealistic to assume that decentralised agents\nwould have access to this global information: we therefore additionally provide\nnew algorithms that allow agents to estimate the global empirical distribution\nbased on a local neighbourhood, and to improve this estimate via communication\nover a given network. Our experiments showcase how the communication network\nallows decentralised agents to estimate the mean-field distribution for\npopulation-dependent policies, and that exchanging policy information helps\nnetworked agents to outperform both independent and even centralised agents in\nfunction-approximation settings, by an even greater margin than in tabular\nsettings.\n", "link": "http://arxiv.org/abs/2408.11607v1", "date": "2024-08-21", "relevancy": 1.2961, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4749}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4204}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Networked%20Communication%20for%20Mean-Field%20Games%20with%20Function%20Approximation%0A%20%20and%20Empirical%20Mean-Field%20Estimation&body=Title%3A%20Networked%20Communication%20for%20Mean-Field%20Games%20with%20Function%20Approximation%0A%20%20and%20Empirical%20Mean-Field%20Estimation%0AAuthor%3A%20Patrick%20Benjamin%20and%20Alessandro%20Abate%0AAbstract%3A%20%20%20Recent%20works%20have%20provided%20algorithms%20by%20which%20decentralised%20agents%2C%20which%0Amay%20be%20connected%20via%20a%20communication%20network%2C%20can%20learn%20equilibria%20in%0AMean-Field%20Games%20from%20a%20single%2C%20non-episodic%20run%20of%20the%20empirical%20system.%0AHowever%2C%20these%20algorithms%20are%20given%20for%20tabular%20settings%3A%20this%20computationally%0Alimits%20the%20size%20of%20players%27%20observation%20space%2C%20meaning%20that%20the%20algorithms%20are%0Anot%20able%20to%20handle%20anything%20but%20small%20state%20spaces%2C%20nor%20to%20generalise%20beyond%0Apolicies%20depending%20on%20the%20ego%20player%27s%20state%20to%20so-called%0A%27population-dependent%27%20policies.%20We%20address%20this%20limitation%20by%20introducing%0Afunction%20approximation%20to%20the%20existing%20setting%2C%20drawing%20on%20the%20Munchausen%0AOnline%20Mirror%20Descent%20method%20that%20has%20previously%20been%20employed%20only%20in%0Afinite-horizon%2C%20episodic%2C%20centralised%20settings.%20While%20this%20permits%20us%20to%0Ainclude%20the%20population%27s%20mean-field%20distribution%20in%20the%20observation%20for%20each%0Aplayer%27s%20policy%2C%20it%20is%20arguably%20unrealistic%20to%20assume%20that%20decentralised%20agents%0Awould%20have%20access%20to%20this%20global%20information%3A%20we%20therefore%20additionally%20provide%0Anew%20algorithms%20that%20allow%20agents%20to%20estimate%20the%20global%20empirical%20distribution%0Abased%20on%20a%20local%20neighbourhood%2C%20and%20to%20improve%20this%20estimate%20via%20communication%0Aover%20a%20given%20network.%20Our%20experiments%20showcase%20how%20the%20communication%20network%0Aallows%20decentralised%20agents%20to%20estimate%20the%20mean-field%20distribution%20for%0Apopulation-dependent%20policies%2C%20and%20that%20exchanging%20policy%20information%20helps%0Anetworked%20agents%20to%20outperform%20both%20independent%20and%20even%20centralised%20agents%20in%0Afunction-approximation%20settings%2C%20by%20an%20even%20greater%20margin%20than%20in%20tabular%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetworked%2520Communication%2520for%2520Mean-Field%2520Games%2520with%2520Function%2520Approximation%250A%2520%2520and%2520Empirical%2520Mean-Field%2520Estimation%26entry.906535625%3DPatrick%2520Benjamin%2520and%2520Alessandro%2520Abate%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520provided%2520algorithms%2520by%2520which%2520decentralised%2520agents%252C%2520which%250Amay%2520be%2520connected%2520via%2520a%2520communication%2520network%252C%2520can%2520learn%2520equilibria%2520in%250AMean-Field%2520Games%2520from%2520a%2520single%252C%2520non-episodic%2520run%2520of%2520the%2520empirical%2520system.%250AHowever%252C%2520these%2520algorithms%2520are%2520given%2520for%2520tabular%2520settings%253A%2520this%2520computationally%250Alimits%2520the%2520size%2520of%2520players%2527%2520observation%2520space%252C%2520meaning%2520that%2520the%2520algorithms%2520are%250Anot%2520able%2520to%2520handle%2520anything%2520but%2520small%2520state%2520spaces%252C%2520nor%2520to%2520generalise%2520beyond%250Apolicies%2520depending%2520on%2520the%2520ego%2520player%2527s%2520state%2520to%2520so-called%250A%2527population-dependent%2527%2520policies.%2520We%2520address%2520this%2520limitation%2520by%2520introducing%250Afunction%2520approximation%2520to%2520the%2520existing%2520setting%252C%2520drawing%2520on%2520the%2520Munchausen%250AOnline%2520Mirror%2520Descent%2520method%2520that%2520has%2520previously%2520been%2520employed%2520only%2520in%250Afinite-horizon%252C%2520episodic%252C%2520centralised%2520settings.%2520While%2520this%2520permits%2520us%2520to%250Ainclude%2520the%2520population%2527s%2520mean-field%2520distribution%2520in%2520the%2520observation%2520for%2520each%250Aplayer%2527s%2520policy%252C%2520it%2520is%2520arguably%2520unrealistic%2520to%2520assume%2520that%2520decentralised%2520agents%250Awould%2520have%2520access%2520to%2520this%2520global%2520information%253A%2520we%2520therefore%2520additionally%2520provide%250Anew%2520algorithms%2520that%2520allow%2520agents%2520to%2520estimate%2520the%2520global%2520empirical%2520distribution%250Abased%2520on%2520a%2520local%2520neighbourhood%252C%2520and%2520to%2520improve%2520this%2520estimate%2520via%2520communication%250Aover%2520a%2520given%2520network.%2520Our%2520experiments%2520showcase%2520how%2520the%2520communication%2520network%250Aallows%2520decentralised%2520agents%2520to%2520estimate%2520the%2520mean-field%2520distribution%2520for%250Apopulation-dependent%2520policies%252C%2520and%2520that%2520exchanging%2520policy%2520information%2520helps%250Anetworked%2520agents%2520to%2520outperform%2520both%2520independent%2520and%2520even%2520centralised%2520agents%2520in%250Afunction-approximation%2520settings%252C%2520by%2520an%2520even%2520greater%2520margin%2520than%2520in%2520tabular%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Networked%20Communication%20for%20Mean-Field%20Games%20with%20Function%20Approximation%0A%20%20and%20Empirical%20Mean-Field%20Estimation&entry.906535625=Patrick%20Benjamin%20and%20Alessandro%20Abate&entry.1292438233=%20%20Recent%20works%20have%20provided%20algorithms%20by%20which%20decentralised%20agents%2C%20which%0Amay%20be%20connected%20via%20a%20communication%20network%2C%20can%20learn%20equilibria%20in%0AMean-Field%20Games%20from%20a%20single%2C%20non-episodic%20run%20of%20the%20empirical%20system.%0AHowever%2C%20these%20algorithms%20are%20given%20for%20tabular%20settings%3A%20this%20computationally%0Alimits%20the%20size%20of%20players%27%20observation%20space%2C%20meaning%20that%20the%20algorithms%20are%0Anot%20able%20to%20handle%20anything%20but%20small%20state%20spaces%2C%20nor%20to%20generalise%20beyond%0Apolicies%20depending%20on%20the%20ego%20player%27s%20state%20to%20so-called%0A%27population-dependent%27%20policies.%20We%20address%20this%20limitation%20by%20introducing%0Afunction%20approximation%20to%20the%20existing%20setting%2C%20drawing%20on%20the%20Munchausen%0AOnline%20Mirror%20Descent%20method%20that%20has%20previously%20been%20employed%20only%20in%0Afinite-horizon%2C%20episodic%2C%20centralised%20settings.%20While%20this%20permits%20us%20to%0Ainclude%20the%20population%27s%20mean-field%20distribution%20in%20the%20observation%20for%20each%0Aplayer%27s%20policy%2C%20it%20is%20arguably%20unrealistic%20to%20assume%20that%20decentralised%20agents%0Awould%20have%20access%20to%20this%20global%20information%3A%20we%20therefore%20additionally%20provide%0Anew%20algorithms%20that%20allow%20agents%20to%20estimate%20the%20global%20empirical%20distribution%0Abased%20on%20a%20local%20neighbourhood%2C%20and%20to%20improve%20this%20estimate%20via%20communication%0Aover%20a%20given%20network.%20Our%20experiments%20showcase%20how%20the%20communication%20network%0Aallows%20decentralised%20agents%20to%20estimate%20the%20mean-field%20distribution%20for%0Apopulation-dependent%20policies%2C%20and%20that%20exchanging%20policy%20information%20helps%0Anetworked%20agents%20to%20outperform%20both%20independent%20and%20even%20centralised%20agents%20in%0Afunction-approximation%20settings%2C%20by%20an%20even%20greater%20margin%20than%20in%20tabular%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11607v1&entry.124074799=Read"},
{"title": "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code\n  using Copilot", "author": "Beiqi Zhang and Peng Liang and Qiong Feng and Yujia Fu and Zengyang Li", "abstract": "  As one of the most popular dynamic languages, Python experiences a decrease\nin readability and maintainability when code smells are present. Recent\nadvancements in Large Language Models have sparked growing interest in\nAI-enabled tools for both code generation and refactoring. GitHub Copilot is\none such tool that has gained widespread usage. Copilot Chat, released in\nSeptember 2023, functions as an interactive tool aimed at facilitating natural\nlanguage-powered coding. However, limited attention has been given to\nunderstanding code smells in Copilot-generated Python code and Copilot Chat's\nability to fix the code smells. To this end, we built a dataset comprising 102\ncode smells in Copilot-generated Python code. Our aim is to first explore the\noccurrence of code smells in Copilot-generated Python code and then evaluate\nthe effectiveness of Copilot Chat in fixing these code smells employing\ndifferent prompts. The results show that 8 out of 10 types of code smells can\nbe detected in Copilot-generated Python code, among which Multiply-Nested\nContainer is the most common one. For these code smells, Copilot Chat achieves\na highest fixing rate of 87.1%, showing promise in fixing Python code smells\ngenerated by Copilot itself. In addition, the effectiveness of Copilot Chat in\nfixing these smells can be improved by providing more detailed prompts.\n", "link": "http://arxiv.org/abs/2401.14176v2", "date": "2024-08-21", "relevancy": 1.2419, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4335}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4003}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Copilot-in-the-Loop%3A%20Fixing%20Code%20Smells%20in%20Copilot-Generated%20Python%20Code%0A%20%20using%20Copilot&body=Title%3A%20Copilot-in-the-Loop%3A%20Fixing%20Code%20Smells%20in%20Copilot-Generated%20Python%20Code%0A%20%20using%20Copilot%0AAuthor%3A%20Beiqi%20Zhang%20and%20Peng%20Liang%20and%20Qiong%20Feng%20and%20Yujia%20Fu%20and%20Zengyang%20Li%0AAbstract%3A%20%20%20As%20one%20of%20the%20most%20popular%20dynamic%20languages%2C%20Python%20experiences%20a%20decrease%0Ain%20readability%20and%20maintainability%20when%20code%20smells%20are%20present.%20Recent%0Aadvancements%20in%20Large%20Language%20Models%20have%20sparked%20growing%20interest%20in%0AAI-enabled%20tools%20for%20both%20code%20generation%20and%20refactoring.%20GitHub%20Copilot%20is%0Aone%20such%20tool%20that%20has%20gained%20widespread%20usage.%20Copilot%20Chat%2C%20released%20in%0ASeptember%202023%2C%20functions%20as%20an%20interactive%20tool%20aimed%20at%20facilitating%20natural%0Alanguage-powered%20coding.%20However%2C%20limited%20attention%20has%20been%20given%20to%0Aunderstanding%20code%20smells%20in%20Copilot-generated%20Python%20code%20and%20Copilot%20Chat%27s%0Aability%20to%20fix%20the%20code%20smells.%20To%20this%20end%2C%20we%20built%20a%20dataset%20comprising%20102%0Acode%20smells%20in%20Copilot-generated%20Python%20code.%20Our%20aim%20is%20to%20first%20explore%20the%0Aoccurrence%20of%20code%20smells%20in%20Copilot-generated%20Python%20code%20and%20then%20evaluate%0Athe%20effectiveness%20of%20Copilot%20Chat%20in%20fixing%20these%20code%20smells%20employing%0Adifferent%20prompts.%20The%20results%20show%20that%208%20out%20of%2010%20types%20of%20code%20smells%20can%0Abe%20detected%20in%20Copilot-generated%20Python%20code%2C%20among%20which%20Multiply-Nested%0AContainer%20is%20the%20most%20common%20one.%20For%20these%20code%20smells%2C%20Copilot%20Chat%20achieves%0Aa%20highest%20fixing%20rate%20of%2087.1%25%2C%20showing%20promise%20in%20fixing%20Python%20code%20smells%0Agenerated%20by%20Copilot%20itself.%20In%20addition%2C%20the%20effectiveness%20of%20Copilot%20Chat%20in%0Afixing%20these%20smells%20can%20be%20improved%20by%20providing%20more%20detailed%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCopilot-in-the-Loop%253A%2520Fixing%2520Code%2520Smells%2520in%2520Copilot-Generated%2520Python%2520Code%250A%2520%2520using%2520Copilot%26entry.906535625%3DBeiqi%2520Zhang%2520and%2520Peng%2520Liang%2520and%2520Qiong%2520Feng%2520and%2520Yujia%2520Fu%2520and%2520Zengyang%2520Li%26entry.1292438233%3D%2520%2520As%2520one%2520of%2520the%2520most%2520popular%2520dynamic%2520languages%252C%2520Python%2520experiences%2520a%2520decrease%250Ain%2520readability%2520and%2520maintainability%2520when%2520code%2520smells%2520are%2520present.%2520Recent%250Aadvancements%2520in%2520Large%2520Language%2520Models%2520have%2520sparked%2520growing%2520interest%2520in%250AAI-enabled%2520tools%2520for%2520both%2520code%2520generation%2520and%2520refactoring.%2520GitHub%2520Copilot%2520is%250Aone%2520such%2520tool%2520that%2520has%2520gained%2520widespread%2520usage.%2520Copilot%2520Chat%252C%2520released%2520in%250ASeptember%25202023%252C%2520functions%2520as%2520an%2520interactive%2520tool%2520aimed%2520at%2520facilitating%2520natural%250Alanguage-powered%2520coding.%2520However%252C%2520limited%2520attention%2520has%2520been%2520given%2520to%250Aunderstanding%2520code%2520smells%2520in%2520Copilot-generated%2520Python%2520code%2520and%2520Copilot%2520Chat%2527s%250Aability%2520to%2520fix%2520the%2520code%2520smells.%2520To%2520this%2520end%252C%2520we%2520built%2520a%2520dataset%2520comprising%2520102%250Acode%2520smells%2520in%2520Copilot-generated%2520Python%2520code.%2520Our%2520aim%2520is%2520to%2520first%2520explore%2520the%250Aoccurrence%2520of%2520code%2520smells%2520in%2520Copilot-generated%2520Python%2520code%2520and%2520then%2520evaluate%250Athe%2520effectiveness%2520of%2520Copilot%2520Chat%2520in%2520fixing%2520these%2520code%2520smells%2520employing%250Adifferent%2520prompts.%2520The%2520results%2520show%2520that%25208%2520out%2520of%252010%2520types%2520of%2520code%2520smells%2520can%250Abe%2520detected%2520in%2520Copilot-generated%2520Python%2520code%252C%2520among%2520which%2520Multiply-Nested%250AContainer%2520is%2520the%2520most%2520common%2520one.%2520For%2520these%2520code%2520smells%252C%2520Copilot%2520Chat%2520achieves%250Aa%2520highest%2520fixing%2520rate%2520of%252087.1%2525%252C%2520showing%2520promise%2520in%2520fixing%2520Python%2520code%2520smells%250Agenerated%2520by%2520Copilot%2520itself.%2520In%2520addition%252C%2520the%2520effectiveness%2520of%2520Copilot%2520Chat%2520in%250Afixing%2520these%2520smells%2520can%2520be%2520improved%2520by%2520providing%2520more%2520detailed%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Copilot-in-the-Loop%3A%20Fixing%20Code%20Smells%20in%20Copilot-Generated%20Python%20Code%0A%20%20using%20Copilot&entry.906535625=Beiqi%20Zhang%20and%20Peng%20Liang%20and%20Qiong%20Feng%20and%20Yujia%20Fu%20and%20Zengyang%20Li&entry.1292438233=%20%20As%20one%20of%20the%20most%20popular%20dynamic%20languages%2C%20Python%20experiences%20a%20decrease%0Ain%20readability%20and%20maintainability%20when%20code%20smells%20are%20present.%20Recent%0Aadvancements%20in%20Large%20Language%20Models%20have%20sparked%20growing%20interest%20in%0AAI-enabled%20tools%20for%20both%20code%20generation%20and%20refactoring.%20GitHub%20Copilot%20is%0Aone%20such%20tool%20that%20has%20gained%20widespread%20usage.%20Copilot%20Chat%2C%20released%20in%0ASeptember%202023%2C%20functions%20as%20an%20interactive%20tool%20aimed%20at%20facilitating%20natural%0Alanguage-powered%20coding.%20However%2C%20limited%20attention%20has%20been%20given%20to%0Aunderstanding%20code%20smells%20in%20Copilot-generated%20Python%20code%20and%20Copilot%20Chat%27s%0Aability%20to%20fix%20the%20code%20smells.%20To%20this%20end%2C%20we%20built%20a%20dataset%20comprising%20102%0Acode%20smells%20in%20Copilot-generated%20Python%20code.%20Our%20aim%20is%20to%20first%20explore%20the%0Aoccurrence%20of%20code%20smells%20in%20Copilot-generated%20Python%20code%20and%20then%20evaluate%0Athe%20effectiveness%20of%20Copilot%20Chat%20in%20fixing%20these%20code%20smells%20employing%0Adifferent%20prompts.%20The%20results%20show%20that%208%20out%20of%2010%20types%20of%20code%20smells%20can%0Abe%20detected%20in%20Copilot-generated%20Python%20code%2C%20among%20which%20Multiply-Nested%0AContainer%20is%20the%20most%20common%20one.%20For%20these%20code%20smells%2C%20Copilot%20Chat%20achieves%0Aa%20highest%20fixing%20rate%20of%2087.1%25%2C%20showing%20promise%20in%20fixing%20Python%20code%20smells%0Agenerated%20by%20Copilot%20itself.%20In%20addition%2C%20the%20effectiveness%20of%20Copilot%20Chat%20in%0Afixing%20these%20smells%20can%20be%20improved%20by%20providing%20more%20detailed%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14176v2&entry.124074799=Read"},
{"title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in\n  the LLM Era", "author": "Sunhao Dai and Chen Xu and Shicheng Xu and Liang Pang and Zhenhua Dong and Jun Xu", "abstract": "  With the rapid advancements of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.\n", "link": "http://arxiv.org/abs/2404.11457v2", "date": "2024-08-21", "relevancy": 1.4006, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4838}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4652}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias%20and%20Unfairness%20in%20Information%20Retrieval%20Systems%3A%20New%20Challenges%20in%0A%20%20the%20LLM%20Era&body=Title%3A%20Bias%20and%20Unfairness%20in%20Information%20Retrieval%20Systems%3A%20New%20Challenges%20in%0A%20%20the%20LLM%20Era%0AAuthor%3A%20Sunhao%20Dai%20and%20Chen%20Xu%20and%20Shicheng%20Xu%20and%20Liang%20Pang%20and%20Zhenhua%20Dong%20and%20Jun%20Xu%0AAbstract%3A%20%20%20With%20the%20rapid%20advancements%20of%20large%20language%20models%20%28LLMs%29%2C%20information%0Aretrieval%20%28IR%29%20systems%2C%20such%20as%20search%20engines%20and%20recommender%20systems%2C%20have%0Aundergone%20a%20significant%20paradigm%20shift.%20This%20evolution%2C%20while%20heralding%20new%0Aopportunities%2C%20introduces%20emerging%20challenges%2C%20particularly%20in%20terms%20of%20biases%0Aand%20unfairness%2C%20which%20may%20threaten%20the%20information%20ecosystem.%20In%20this%20paper%2C%20we%0Apresent%20a%20comprehensive%20survey%20of%20existing%20works%20on%20emerging%20and%20pressing%20bias%0Aand%20unfairness%20issues%20in%20IR%20systems%20when%20the%20integration%20of%20LLMs.%20We%20first%0Aunify%20bias%20and%20unfairness%20issues%20as%20distribution%20mismatch%20problems%2C%20providing%20a%0Agroundwork%20for%20categorizing%20various%20mitigation%20strategies%20through%20distribution%0Aalignment.%20Subsequently%2C%20we%20systematically%20delve%20into%20the%20specific%20bias%20and%0Aunfairness%20issues%20arising%20from%20three%20critical%20stages%20of%20LLMs%20integration%20into%0AIR%20systems%3A%20data%20collection%2C%20model%20development%2C%20and%20result%20evaluation.%20In%20doing%0Aso%2C%20we%20meticulously%20review%20and%20analyze%20recent%20literature%2C%20focusing%20on%20the%0Adefinitions%2C%20characteristics%2C%20and%20corresponding%20mitigation%20strategies%0Aassociated%20with%20these%20issues.%20Finally%2C%20we%20identify%20and%20highlight%20some%20open%0Aproblems%20and%20challenges%20for%20future%20work%2C%20aiming%20to%20inspire%20researchers%20and%0Astakeholders%20in%20the%20IR%20field%20and%20beyond%20to%20better%20understand%20and%20mitigate%20bias%0Aand%20unfairness%20issues%20of%20IR%20in%20this%20LLM%20era.%20We%20also%20consistently%20maintain%20a%0AGitHub%20repository%20for%20the%20relevant%20papers%20and%20resources%20in%20this%20rising%0Adirection%20at%20https%3A//github.com/KID-22/LLM-IR-Bias-Fairness-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11457v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias%2520and%2520Unfairness%2520in%2520Information%2520Retrieval%2520Systems%253A%2520New%2520Challenges%2520in%250A%2520%2520the%2520LLM%2520Era%26entry.906535625%3DSunhao%2520Dai%2520and%2520Chen%2520Xu%2520and%2520Shicheng%2520Xu%2520and%2520Liang%2520Pang%2520and%2520Zhenhua%2520Dong%2520and%2520Jun%2520Xu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancements%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520information%250Aretrieval%2520%2528IR%2529%2520systems%252C%2520such%2520as%2520search%2520engines%2520and%2520recommender%2520systems%252C%2520have%250Aundergone%2520a%2520significant%2520paradigm%2520shift.%2520This%2520evolution%252C%2520while%2520heralding%2520new%250Aopportunities%252C%2520introduces%2520emerging%2520challenges%252C%2520particularly%2520in%2520terms%2520of%2520biases%250Aand%2520unfairness%252C%2520which%2520may%2520threaten%2520the%2520information%2520ecosystem.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520comprehensive%2520survey%2520of%2520existing%2520works%2520on%2520emerging%2520and%2520pressing%2520bias%250Aand%2520unfairness%2520issues%2520in%2520IR%2520systems%2520when%2520the%2520integration%2520of%2520LLMs.%2520We%2520first%250Aunify%2520bias%2520and%2520unfairness%2520issues%2520as%2520distribution%2520mismatch%2520problems%252C%2520providing%2520a%250Agroundwork%2520for%2520categorizing%2520various%2520mitigation%2520strategies%2520through%2520distribution%250Aalignment.%2520Subsequently%252C%2520we%2520systematically%2520delve%2520into%2520the%2520specific%2520bias%2520and%250Aunfairness%2520issues%2520arising%2520from%2520three%2520critical%2520stages%2520of%2520LLMs%2520integration%2520into%250AIR%2520systems%253A%2520data%2520collection%252C%2520model%2520development%252C%2520and%2520result%2520evaluation.%2520In%2520doing%250Aso%252C%2520we%2520meticulously%2520review%2520and%2520analyze%2520recent%2520literature%252C%2520focusing%2520on%2520the%250Adefinitions%252C%2520characteristics%252C%2520and%2520corresponding%2520mitigation%2520strategies%250Aassociated%2520with%2520these%2520issues.%2520Finally%252C%2520we%2520identify%2520and%2520highlight%2520some%2520open%250Aproblems%2520and%2520challenges%2520for%2520future%2520work%252C%2520aiming%2520to%2520inspire%2520researchers%2520and%250Astakeholders%2520in%2520the%2520IR%2520field%2520and%2520beyond%2520to%2520better%2520understand%2520and%2520mitigate%2520bias%250Aand%2520unfairness%2520issues%2520of%2520IR%2520in%2520this%2520LLM%2520era.%2520We%2520also%2520consistently%2520maintain%2520a%250AGitHub%2520repository%2520for%2520the%2520relevant%2520papers%2520and%2520resources%2520in%2520this%2520rising%250Adirection%2520at%2520https%253A//github.com/KID-22/LLM-IR-Bias-Fairness-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11457v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias%20and%20Unfairness%20in%20Information%20Retrieval%20Systems%3A%20New%20Challenges%20in%0A%20%20the%20LLM%20Era&entry.906535625=Sunhao%20Dai%20and%20Chen%20Xu%20and%20Shicheng%20Xu%20and%20Liang%20Pang%20and%20Zhenhua%20Dong%20and%20Jun%20Xu&entry.1292438233=%20%20With%20the%20rapid%20advancements%20of%20large%20language%20models%20%28LLMs%29%2C%20information%0Aretrieval%20%28IR%29%20systems%2C%20such%20as%20search%20engines%20and%20recommender%20systems%2C%20have%0Aundergone%20a%20significant%20paradigm%20shift.%20This%20evolution%2C%20while%20heralding%20new%0Aopportunities%2C%20introduces%20emerging%20challenges%2C%20particularly%20in%20terms%20of%20biases%0Aand%20unfairness%2C%20which%20may%20threaten%20the%20information%20ecosystem.%20In%20this%20paper%2C%20we%0Apresent%20a%20comprehensive%20survey%20of%20existing%20works%20on%20emerging%20and%20pressing%20bias%0Aand%20unfairness%20issues%20in%20IR%20systems%20when%20the%20integration%20of%20LLMs.%20We%20first%0Aunify%20bias%20and%20unfairness%20issues%20as%20distribution%20mismatch%20problems%2C%20providing%20a%0Agroundwork%20for%20categorizing%20various%20mitigation%20strategies%20through%20distribution%0Aalignment.%20Subsequently%2C%20we%20systematically%20delve%20into%20the%20specific%20bias%20and%0Aunfairness%20issues%20arising%20from%20three%20critical%20stages%20of%20LLMs%20integration%20into%0AIR%20systems%3A%20data%20collection%2C%20model%20development%2C%20and%20result%20evaluation.%20In%20doing%0Aso%2C%20we%20meticulously%20review%20and%20analyze%20recent%20literature%2C%20focusing%20on%20the%0Adefinitions%2C%20characteristics%2C%20and%20corresponding%20mitigation%20strategies%0Aassociated%20with%20these%20issues.%20Finally%2C%20we%20identify%20and%20highlight%20some%20open%0Aproblems%20and%20challenges%20for%20future%20work%2C%20aiming%20to%20inspire%20researchers%20and%0Astakeholders%20in%20the%20IR%20field%20and%20beyond%20to%20better%20understand%20and%20mitigate%20bias%0Aand%20unfairness%20issues%20of%20IR%20in%20this%20LLM%20era.%20We%20also%20consistently%20maintain%20a%0AGitHub%20repository%20for%20the%20relevant%20papers%20and%20resources%20in%20this%20rising%0Adirection%20at%20https%3A//github.com/KID-22/LLM-IR-Bias-Fairness-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11457v2&entry.124074799=Read"},
{"title": "Don't Kill the Baby: The Case for AI in Arbitration", "author": "Michael Broyde and Yiyang Mei", "abstract": "  Since the introduction of Generative AI (GenAI) in 2022, its ability to\nsimulate human intelligence and generate content has sparked both enthusiasm\nand concern. While much criticism focuses on AI's potential to perpetuate bias,\ncreate emotional dissonance, displace jobs, and raise ethical questions, these\nconcerns often overlook the practical benefits of AI, particularly in legal\ncontexts.\n  This article examines the integration of AI into arbitration, arguing that\nthe Federal Arbitration Act (FAA) allows parties to contractually choose\nAI-driven arbitration, despite traditional reservations. The article makes\nthree key contributions: (1) It shifts the focus from debates over AI's\npersonhood to the practical aspects of incorporating AI into arbitration,\nasserting that AI can effectively serve as an arbitrator if both parties agree;\n(2) It positions arbitration as an ideal starting point for broader AI adoption\nin the legal field, given its flexibility and the autonomy it grants parties to\ndefine their standards of fairness; and (3) It outlines future research\ndirections, emphasizing the importance of empirically comparing AI and human\narbitration, which could lead to the development of distinct systems.\n  By advocating for the use of AI in arbitration, this article underscores the\nimportance of respecting contractual autonomy and creating an environment that\nallows AI's potential to be fully realized. Drawing on the insights of Judge\nRichard Posner, the article argues that the ethical obligations of AI in\narbitration should be understood within the context of its technological\nstrengths and the voluntary nature of arbitration agreements. Ultimately, it\ncalls for a balanced, open-minded approach to AI in arbitration, recognizing\nits potential to enhance the efficiency, fairness, and flexibility of dispute\nresolution\n", "link": "http://arxiv.org/abs/2408.11608v1", "date": "2024-08-21", "relevancy": 1.5375, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3884}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Kill%20the%20Baby%3A%20The%20Case%20for%20AI%20in%20Arbitration&body=Title%3A%20Don%27t%20Kill%20the%20Baby%3A%20The%20Case%20for%20AI%20in%20Arbitration%0AAuthor%3A%20Michael%20Broyde%20and%20Yiyang%20Mei%0AAbstract%3A%20%20%20Since%20the%20introduction%20of%20Generative%20AI%20%28GenAI%29%20in%202022%2C%20its%20ability%20to%0Asimulate%20human%20intelligence%20and%20generate%20content%20has%20sparked%20both%20enthusiasm%0Aand%20concern.%20While%20much%20criticism%20focuses%20on%20AI%27s%20potential%20to%20perpetuate%20bias%2C%0Acreate%20emotional%20dissonance%2C%20displace%20jobs%2C%20and%20raise%20ethical%20questions%2C%20these%0Aconcerns%20often%20overlook%20the%20practical%20benefits%20of%20AI%2C%20particularly%20in%20legal%0Acontexts.%0A%20%20This%20article%20examines%20the%20integration%20of%20AI%20into%20arbitration%2C%20arguing%20that%0Athe%20Federal%20Arbitration%20Act%20%28FAA%29%20allows%20parties%20to%20contractually%20choose%0AAI-driven%20arbitration%2C%20despite%20traditional%20reservations.%20The%20article%20makes%0Athree%20key%20contributions%3A%20%281%29%20It%20shifts%20the%20focus%20from%20debates%20over%20AI%27s%0Apersonhood%20to%20the%20practical%20aspects%20of%20incorporating%20AI%20into%20arbitration%2C%0Aasserting%20that%20AI%20can%20effectively%20serve%20as%20an%20arbitrator%20if%20both%20parties%20agree%3B%0A%282%29%20It%20positions%20arbitration%20as%20an%20ideal%20starting%20point%20for%20broader%20AI%20adoption%0Ain%20the%20legal%20field%2C%20given%20its%20flexibility%20and%20the%20autonomy%20it%20grants%20parties%20to%0Adefine%20their%20standards%20of%20fairness%3B%20and%20%283%29%20It%20outlines%20future%20research%0Adirections%2C%20emphasizing%20the%20importance%20of%20empirically%20comparing%20AI%20and%20human%0Aarbitration%2C%20which%20could%20lead%20to%20the%20development%20of%20distinct%20systems.%0A%20%20By%20advocating%20for%20the%20use%20of%20AI%20in%20arbitration%2C%20this%20article%20underscores%20the%0Aimportance%20of%20respecting%20contractual%20autonomy%20and%20creating%20an%20environment%20that%0Aallows%20AI%27s%20potential%20to%20be%20fully%20realized.%20Drawing%20on%20the%20insights%20of%20Judge%0ARichard%20Posner%2C%20the%20article%20argues%20that%20the%20ethical%20obligations%20of%20AI%20in%0Aarbitration%20should%20be%20understood%20within%20the%20context%20of%20its%20technological%0Astrengths%20and%20the%20voluntary%20nature%20of%20arbitration%20agreements.%20Ultimately%2C%20it%0Acalls%20for%20a%20balanced%2C%20open-minded%20approach%20to%20AI%20in%20arbitration%2C%20recognizing%0Aits%20potential%20to%20enhance%20the%20efficiency%2C%20fairness%2C%20and%20flexibility%20of%20dispute%0Aresolution%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Kill%2520the%2520Baby%253A%2520The%2520Case%2520for%2520AI%2520in%2520Arbitration%26entry.906535625%3DMichael%2520Broyde%2520and%2520Yiyang%2520Mei%26entry.1292438233%3D%2520%2520Since%2520the%2520introduction%2520of%2520Generative%2520AI%2520%2528GenAI%2529%2520in%25202022%252C%2520its%2520ability%2520to%250Asimulate%2520human%2520intelligence%2520and%2520generate%2520content%2520has%2520sparked%2520both%2520enthusiasm%250Aand%2520concern.%2520While%2520much%2520criticism%2520focuses%2520on%2520AI%2527s%2520potential%2520to%2520perpetuate%2520bias%252C%250Acreate%2520emotional%2520dissonance%252C%2520displace%2520jobs%252C%2520and%2520raise%2520ethical%2520questions%252C%2520these%250Aconcerns%2520often%2520overlook%2520the%2520practical%2520benefits%2520of%2520AI%252C%2520particularly%2520in%2520legal%250Acontexts.%250A%2520%2520This%2520article%2520examines%2520the%2520integration%2520of%2520AI%2520into%2520arbitration%252C%2520arguing%2520that%250Athe%2520Federal%2520Arbitration%2520Act%2520%2528FAA%2529%2520allows%2520parties%2520to%2520contractually%2520choose%250AAI-driven%2520arbitration%252C%2520despite%2520traditional%2520reservations.%2520The%2520article%2520makes%250Athree%2520key%2520contributions%253A%2520%25281%2529%2520It%2520shifts%2520the%2520focus%2520from%2520debates%2520over%2520AI%2527s%250Apersonhood%2520to%2520the%2520practical%2520aspects%2520of%2520incorporating%2520AI%2520into%2520arbitration%252C%250Aasserting%2520that%2520AI%2520can%2520effectively%2520serve%2520as%2520an%2520arbitrator%2520if%2520both%2520parties%2520agree%253B%250A%25282%2529%2520It%2520positions%2520arbitration%2520as%2520an%2520ideal%2520starting%2520point%2520for%2520broader%2520AI%2520adoption%250Ain%2520the%2520legal%2520field%252C%2520given%2520its%2520flexibility%2520and%2520the%2520autonomy%2520it%2520grants%2520parties%2520to%250Adefine%2520their%2520standards%2520of%2520fairness%253B%2520and%2520%25283%2529%2520It%2520outlines%2520future%2520research%250Adirections%252C%2520emphasizing%2520the%2520importance%2520of%2520empirically%2520comparing%2520AI%2520and%2520human%250Aarbitration%252C%2520which%2520could%2520lead%2520to%2520the%2520development%2520of%2520distinct%2520systems.%250A%2520%2520By%2520advocating%2520for%2520the%2520use%2520of%2520AI%2520in%2520arbitration%252C%2520this%2520article%2520underscores%2520the%250Aimportance%2520of%2520respecting%2520contractual%2520autonomy%2520and%2520creating%2520an%2520environment%2520that%250Aallows%2520AI%2527s%2520potential%2520to%2520be%2520fully%2520realized.%2520Drawing%2520on%2520the%2520insights%2520of%2520Judge%250ARichard%2520Posner%252C%2520the%2520article%2520argues%2520that%2520the%2520ethical%2520obligations%2520of%2520AI%2520in%250Aarbitration%2520should%2520be%2520understood%2520within%2520the%2520context%2520of%2520its%2520technological%250Astrengths%2520and%2520the%2520voluntary%2520nature%2520of%2520arbitration%2520agreements.%2520Ultimately%252C%2520it%250Acalls%2520for%2520a%2520balanced%252C%2520open-minded%2520approach%2520to%2520AI%2520in%2520arbitration%252C%2520recognizing%250Aits%2520potential%2520to%2520enhance%2520the%2520efficiency%252C%2520fairness%252C%2520and%2520flexibility%2520of%2520dispute%250Aresolution%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Kill%20the%20Baby%3A%20The%20Case%20for%20AI%20in%20Arbitration&entry.906535625=Michael%20Broyde%20and%20Yiyang%20Mei&entry.1292438233=%20%20Since%20the%20introduction%20of%20Generative%20AI%20%28GenAI%29%20in%202022%2C%20its%20ability%20to%0Asimulate%20human%20intelligence%20and%20generate%20content%20has%20sparked%20both%20enthusiasm%0Aand%20concern.%20While%20much%20criticism%20focuses%20on%20AI%27s%20potential%20to%20perpetuate%20bias%2C%0Acreate%20emotional%20dissonance%2C%20displace%20jobs%2C%20and%20raise%20ethical%20questions%2C%20these%0Aconcerns%20often%20overlook%20the%20practical%20benefits%20of%20AI%2C%20particularly%20in%20legal%0Acontexts.%0A%20%20This%20article%20examines%20the%20integration%20of%20AI%20into%20arbitration%2C%20arguing%20that%0Athe%20Federal%20Arbitration%20Act%20%28FAA%29%20allows%20parties%20to%20contractually%20choose%0AAI-driven%20arbitration%2C%20despite%20traditional%20reservations.%20The%20article%20makes%0Athree%20key%20contributions%3A%20%281%29%20It%20shifts%20the%20focus%20from%20debates%20over%20AI%27s%0Apersonhood%20to%20the%20practical%20aspects%20of%20incorporating%20AI%20into%20arbitration%2C%0Aasserting%20that%20AI%20can%20effectively%20serve%20as%20an%20arbitrator%20if%20both%20parties%20agree%3B%0A%282%29%20It%20positions%20arbitration%20as%20an%20ideal%20starting%20point%20for%20broader%20AI%20adoption%0Ain%20the%20legal%20field%2C%20given%20its%20flexibility%20and%20the%20autonomy%20it%20grants%20parties%20to%0Adefine%20their%20standards%20of%20fairness%3B%20and%20%283%29%20It%20outlines%20future%20research%0Adirections%2C%20emphasizing%20the%20importance%20of%20empirically%20comparing%20AI%20and%20human%0Aarbitration%2C%20which%20could%20lead%20to%20the%20development%20of%20distinct%20systems.%0A%20%20By%20advocating%20for%20the%20use%20of%20AI%20in%20arbitration%2C%20this%20article%20underscores%20the%0Aimportance%20of%20respecting%20contractual%20autonomy%20and%20creating%20an%20environment%20that%0Aallows%20AI%27s%20potential%20to%20be%20fully%20realized.%20Drawing%20on%20the%20insights%20of%20Judge%0ARichard%20Posner%2C%20the%20article%20argues%20that%20the%20ethical%20obligations%20of%20AI%20in%0Aarbitration%20should%20be%20understood%20within%20the%20context%20of%20its%20technological%0Astrengths%20and%20the%20voluntary%20nature%20of%20arbitration%20agreements.%20Ultimately%2C%20it%0Acalls%20for%20a%20balanced%2C%20open-minded%20approach%20to%20AI%20in%20arbitration%2C%20recognizing%0Aits%20potential%20to%20enhance%20the%20efficiency%2C%20fairness%2C%20and%20flexibility%20of%20dispute%0Aresolution%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11608v1&entry.124074799=Read"},
{"title": "Efficient Exploration and Discriminative World Model Learning with an\n  Object-Centric Abstraction", "author": "Anthony GX-Chen and Kenneth Marino and Rob Fergus", "abstract": "  In the face of difficult exploration problems in reinforcement learning, we\nstudy whether giving an agent an object-centric mapping (describing a set of\nitems and their attributes) allow for more efficient learning. We found this\nproblem is best solved hierarchically by modelling items at a higher level of\nstate abstraction to pixels, and attribute change at a higher level of temporal\nabstraction to primitive actions. This abstraction simplifies the transition\ndynamic by making specific future states easier to predict. We make use of this\nto propose a fully model-based algorithm that learns a discriminative world\nmodel, plans to explore efficiently with only a count-based intrinsic reward,\nand can subsequently plan to reach any discovered (abstract) states.\n  We demonstrate the model's ability to (i) efficiently solve single tasks,\n(ii) transfer zero-shot and few-shot across item types and environments, and\n(iii) plan across long horizons. Across a suite of 2D crafting and MiniHack\nenvironments, we empirically show our model significantly out-performs\nstate-of-the-art low-level methods (without abstraction), as well as performant\nmodel-free and model-based methods using the same abstraction. Finally, we show\nhow to reinforce learn low level object-perturbing policies, as well as\nsupervise learn the object mapping itself.\n", "link": "http://arxiv.org/abs/2408.11816v1", "date": "2024-08-21", "relevancy": 1.7221, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6414}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5877}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Exploration%20and%20Discriminative%20World%20Model%20Learning%20with%20an%0A%20%20Object-Centric%20Abstraction&body=Title%3A%20Efficient%20Exploration%20and%20Discriminative%20World%20Model%20Learning%20with%20an%0A%20%20Object-Centric%20Abstraction%0AAuthor%3A%20Anthony%20GX-Chen%20and%20Kenneth%20Marino%20and%20Rob%20Fergus%0AAbstract%3A%20%20%20In%20the%20face%20of%20difficult%20exploration%20problems%20in%20reinforcement%20learning%2C%20we%0Astudy%20whether%20giving%20an%20agent%20an%20object-centric%20mapping%20%28describing%20a%20set%20of%0Aitems%20and%20their%20attributes%29%20allow%20for%20more%20efficient%20learning.%20We%20found%20this%0Aproblem%20is%20best%20solved%20hierarchically%20by%20modelling%20items%20at%20a%20higher%20level%20of%0Astate%20abstraction%20to%20pixels%2C%20and%20attribute%20change%20at%20a%20higher%20level%20of%20temporal%0Aabstraction%20to%20primitive%20actions.%20This%20abstraction%20simplifies%20the%20transition%0Adynamic%20by%20making%20specific%20future%20states%20easier%20to%20predict.%20We%20make%20use%20of%20this%0Ato%20propose%20a%20fully%20model-based%20algorithm%20that%20learns%20a%20discriminative%20world%0Amodel%2C%20plans%20to%20explore%20efficiently%20with%20only%20a%20count-based%20intrinsic%20reward%2C%0Aand%20can%20subsequently%20plan%20to%20reach%20any%20discovered%20%28abstract%29%20states.%0A%20%20We%20demonstrate%20the%20model%27s%20ability%20to%20%28i%29%20efficiently%20solve%20single%20tasks%2C%0A%28ii%29%20transfer%20zero-shot%20and%20few-shot%20across%20item%20types%20and%20environments%2C%20and%0A%28iii%29%20plan%20across%20long%20horizons.%20Across%20a%20suite%20of%202D%20crafting%20and%20MiniHack%0Aenvironments%2C%20we%20empirically%20show%20our%20model%20significantly%20out-performs%0Astate-of-the-art%20low-level%20methods%20%28without%20abstraction%29%2C%20as%20well%20as%20performant%0Amodel-free%20and%20model-based%20methods%20using%20the%20same%20abstraction.%20Finally%2C%20we%20show%0Ahow%20to%20reinforce%20learn%20low%20level%20object-perturbing%20policies%2C%20as%20well%20as%0Asupervise%20learn%20the%20object%20mapping%20itself.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Exploration%2520and%2520Discriminative%2520World%2520Model%2520Learning%2520with%2520an%250A%2520%2520Object-Centric%2520Abstraction%26entry.906535625%3DAnthony%2520GX-Chen%2520and%2520Kenneth%2520Marino%2520and%2520Rob%2520Fergus%26entry.1292438233%3D%2520%2520In%2520the%2520face%2520of%2520difficult%2520exploration%2520problems%2520in%2520reinforcement%2520learning%252C%2520we%250Astudy%2520whether%2520giving%2520an%2520agent%2520an%2520object-centric%2520mapping%2520%2528describing%2520a%2520set%2520of%250Aitems%2520and%2520their%2520attributes%2529%2520allow%2520for%2520more%2520efficient%2520learning.%2520We%2520found%2520this%250Aproblem%2520is%2520best%2520solved%2520hierarchically%2520by%2520modelling%2520items%2520at%2520a%2520higher%2520level%2520of%250Astate%2520abstraction%2520to%2520pixels%252C%2520and%2520attribute%2520change%2520at%2520a%2520higher%2520level%2520of%2520temporal%250Aabstraction%2520to%2520primitive%2520actions.%2520This%2520abstraction%2520simplifies%2520the%2520transition%250Adynamic%2520by%2520making%2520specific%2520future%2520states%2520easier%2520to%2520predict.%2520We%2520make%2520use%2520of%2520this%250Ato%2520propose%2520a%2520fully%2520model-based%2520algorithm%2520that%2520learns%2520a%2520discriminative%2520world%250Amodel%252C%2520plans%2520to%2520explore%2520efficiently%2520with%2520only%2520a%2520count-based%2520intrinsic%2520reward%252C%250Aand%2520can%2520subsequently%2520plan%2520to%2520reach%2520any%2520discovered%2520%2528abstract%2529%2520states.%250A%2520%2520We%2520demonstrate%2520the%2520model%2527s%2520ability%2520to%2520%2528i%2529%2520efficiently%2520solve%2520single%2520tasks%252C%250A%2528ii%2529%2520transfer%2520zero-shot%2520and%2520few-shot%2520across%2520item%2520types%2520and%2520environments%252C%2520and%250A%2528iii%2529%2520plan%2520across%2520long%2520horizons.%2520Across%2520a%2520suite%2520of%25202D%2520crafting%2520and%2520MiniHack%250Aenvironments%252C%2520we%2520empirically%2520show%2520our%2520model%2520significantly%2520out-performs%250Astate-of-the-art%2520low-level%2520methods%2520%2528without%2520abstraction%2529%252C%2520as%2520well%2520as%2520performant%250Amodel-free%2520and%2520model-based%2520methods%2520using%2520the%2520same%2520abstraction.%2520Finally%252C%2520we%2520show%250Ahow%2520to%2520reinforce%2520learn%2520low%2520level%2520object-perturbing%2520policies%252C%2520as%2520well%2520as%250Asupervise%2520learn%2520the%2520object%2520mapping%2520itself.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Exploration%20and%20Discriminative%20World%20Model%20Learning%20with%20an%0A%20%20Object-Centric%20Abstraction&entry.906535625=Anthony%20GX-Chen%20and%20Kenneth%20Marino%20and%20Rob%20Fergus&entry.1292438233=%20%20In%20the%20face%20of%20difficult%20exploration%20problems%20in%20reinforcement%20learning%2C%20we%0Astudy%20whether%20giving%20an%20agent%20an%20object-centric%20mapping%20%28describing%20a%20set%20of%0Aitems%20and%20their%20attributes%29%20allow%20for%20more%20efficient%20learning.%20We%20found%20this%0Aproblem%20is%20best%20solved%20hierarchically%20by%20modelling%20items%20at%20a%20higher%20level%20of%0Astate%20abstraction%20to%20pixels%2C%20and%20attribute%20change%20at%20a%20higher%20level%20of%20temporal%0Aabstraction%20to%20primitive%20actions.%20This%20abstraction%20simplifies%20the%20transition%0Adynamic%20by%20making%20specific%20future%20states%20easier%20to%20predict.%20We%20make%20use%20of%20this%0Ato%20propose%20a%20fully%20model-based%20algorithm%20that%20learns%20a%20discriminative%20world%0Amodel%2C%20plans%20to%20explore%20efficiently%20with%20only%20a%20count-based%20intrinsic%20reward%2C%0Aand%20can%20subsequently%20plan%20to%20reach%20any%20discovered%20%28abstract%29%20states.%0A%20%20We%20demonstrate%20the%20model%27s%20ability%20to%20%28i%29%20efficiently%20solve%20single%20tasks%2C%0A%28ii%29%20transfer%20zero-shot%20and%20few-shot%20across%20item%20types%20and%20environments%2C%20and%0A%28iii%29%20plan%20across%20long%20horizons.%20Across%20a%20suite%20of%202D%20crafting%20and%20MiniHack%0Aenvironments%2C%20we%20empirically%20show%20our%20model%20significantly%20out-performs%0Astate-of-the-art%20low-level%20methods%20%28without%20abstraction%29%2C%20as%20well%20as%20performant%0Amodel-free%20and%20model-based%20methods%20using%20the%20same%20abstraction.%20Finally%2C%20we%20show%0Ahow%20to%20reinforce%20learn%20low%20level%20object-perturbing%20policies%2C%20as%20well%20as%0Asupervise%20learn%20the%20object%20mapping%20itself.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11816v1&entry.124074799=Read"},
{"title": "Mutagenesis screen to map the functionals of parameters of Large\n  Language Models", "author": "Yue Hu and Kai Hu and Patrick X. Zhao and Javed Khan and Chengming Xu", "abstract": "  Large Language Models (LLMs) have significantly advanced artificial\nintelligence, excelling in numerous tasks. Although the functionality of a\nmodel is inherently tied to its parameters, a systematic method for exploring\nthe connections between the parameters and the functionality are lacking.\nModels sharing similar structure and parameter counts exhibit significant\nperformance disparities across various tasks, prompting investigations into the\nvarying patterns that govern their performance. We adopted a mutagenesis screen\napproach inspired by the methods used in biological studies, to investigate\nLlama2-7b and Zephyr. This technique involved mutating elements within the\nmodels' matrices to their maximum or minimum values to examine the relationship\nbetween model parameters and their functionalities. Our research uncovered\nmultiple levels of fine structures within both models. Many matrices showed a\nmixture of maximum and minimum mutations following mutagenesis, but others were\npredominantly sensitive to one type. Notably, mutations that produced\nphenotypes, especially those with severe outcomes, tended to cluster along\naxes. Additionally, the location of maximum and minimum mutations often\ndisplayed a complementary pattern on matrix in both models, with the Gate\nmatrix showing a unique two-dimensional asymmetry after rearrangement. In\nZephyr, certain mutations consistently resulted in poetic or conversational\nrather than descriptive outputs. These \"writer\" mutations grouped according to\nthe high-frequency initial word of the output, with a marked tendency to share\nthe row coordinate even when they are in different matrices. Our findings\naffirm that the mutagenesis screen is an effective tool for deciphering the\ncomplexities of large language models and identifying unexpected ways to expand\ntheir potential, providing deeper insights into the foundational aspects of AI\nsystems.\n", "link": "http://arxiv.org/abs/2408.11494v1", "date": "2024-08-21", "relevancy": 1.3737, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4606}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4555}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mutagenesis%20screen%20to%20map%20the%20functionals%20of%20parameters%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20Mutagenesis%20screen%20to%20map%20the%20functionals%20of%20parameters%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yue%20Hu%20and%20Kai%20Hu%20and%20Patrick%20X.%20Zhao%20and%20Javed%20Khan%20and%20Chengming%20Xu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%20advanced%20artificial%0Aintelligence%2C%20excelling%20in%20numerous%20tasks.%20Although%20the%20functionality%20of%20a%0Amodel%20is%20inherently%20tied%20to%20its%20parameters%2C%20a%20systematic%20method%20for%20exploring%0Athe%20connections%20between%20the%20parameters%20and%20the%20functionality%20are%20lacking.%0AModels%20sharing%20similar%20structure%20and%20parameter%20counts%20exhibit%20significant%0Aperformance%20disparities%20across%20various%20tasks%2C%20prompting%20investigations%20into%20the%0Avarying%20patterns%20that%20govern%20their%20performance.%20We%20adopted%20a%20mutagenesis%20screen%0Aapproach%20inspired%20by%20the%20methods%20used%20in%20biological%20studies%2C%20to%20investigate%0ALlama2-7b%20and%20Zephyr.%20This%20technique%20involved%20mutating%20elements%20within%20the%0Amodels%27%20matrices%20to%20their%20maximum%20or%20minimum%20values%20to%20examine%20the%20relationship%0Abetween%20model%20parameters%20and%20their%20functionalities.%20Our%20research%20uncovered%0Amultiple%20levels%20of%20fine%20structures%20within%20both%20models.%20Many%20matrices%20showed%20a%0Amixture%20of%20maximum%20and%20minimum%20mutations%20following%20mutagenesis%2C%20but%20others%20were%0Apredominantly%20sensitive%20to%20one%20type.%20Notably%2C%20mutations%20that%20produced%0Aphenotypes%2C%20especially%20those%20with%20severe%20outcomes%2C%20tended%20to%20cluster%20along%0Aaxes.%20Additionally%2C%20the%20location%20of%20maximum%20and%20minimum%20mutations%20often%0Adisplayed%20a%20complementary%20pattern%20on%20matrix%20in%20both%20models%2C%20with%20the%20Gate%0Amatrix%20showing%20a%20unique%20two-dimensional%20asymmetry%20after%20rearrangement.%20In%0AZephyr%2C%20certain%20mutations%20consistently%20resulted%20in%20poetic%20or%20conversational%0Arather%20than%20descriptive%20outputs.%20These%20%22writer%22%20mutations%20grouped%20according%20to%0Athe%20high-frequency%20initial%20word%20of%20the%20output%2C%20with%20a%20marked%20tendency%20to%20share%0Athe%20row%20coordinate%20even%20when%20they%20are%20in%20different%20matrices.%20Our%20findings%0Aaffirm%20that%20the%20mutagenesis%20screen%20is%20an%20effective%20tool%20for%20deciphering%20the%0Acomplexities%20of%20large%20language%20models%20and%20identifying%20unexpected%20ways%20to%20expand%0Atheir%20potential%2C%20providing%20deeper%20insights%20into%20the%20foundational%20aspects%20of%20AI%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMutagenesis%2520screen%2520to%2520map%2520the%2520functionals%2520of%2520parameters%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYue%2520Hu%2520and%2520Kai%2520Hu%2520and%2520Patrick%2520X.%2520Zhao%2520and%2520Javed%2520Khan%2520and%2520Chengming%2520Xu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520significantly%2520advanced%2520artificial%250Aintelligence%252C%2520excelling%2520in%2520numerous%2520tasks.%2520Although%2520the%2520functionality%2520of%2520a%250Amodel%2520is%2520inherently%2520tied%2520to%2520its%2520parameters%252C%2520a%2520systematic%2520method%2520for%2520exploring%250Athe%2520connections%2520between%2520the%2520parameters%2520and%2520the%2520functionality%2520are%2520lacking.%250AModels%2520sharing%2520similar%2520structure%2520and%2520parameter%2520counts%2520exhibit%2520significant%250Aperformance%2520disparities%2520across%2520various%2520tasks%252C%2520prompting%2520investigations%2520into%2520the%250Avarying%2520patterns%2520that%2520govern%2520their%2520performance.%2520We%2520adopted%2520a%2520mutagenesis%2520screen%250Aapproach%2520inspired%2520by%2520the%2520methods%2520used%2520in%2520biological%2520studies%252C%2520to%2520investigate%250ALlama2-7b%2520and%2520Zephyr.%2520This%2520technique%2520involved%2520mutating%2520elements%2520within%2520the%250Amodels%2527%2520matrices%2520to%2520their%2520maximum%2520or%2520minimum%2520values%2520to%2520examine%2520the%2520relationship%250Abetween%2520model%2520parameters%2520and%2520their%2520functionalities.%2520Our%2520research%2520uncovered%250Amultiple%2520levels%2520of%2520fine%2520structures%2520within%2520both%2520models.%2520Many%2520matrices%2520showed%2520a%250Amixture%2520of%2520maximum%2520and%2520minimum%2520mutations%2520following%2520mutagenesis%252C%2520but%2520others%2520were%250Apredominantly%2520sensitive%2520to%2520one%2520type.%2520Notably%252C%2520mutations%2520that%2520produced%250Aphenotypes%252C%2520especially%2520those%2520with%2520severe%2520outcomes%252C%2520tended%2520to%2520cluster%2520along%250Aaxes.%2520Additionally%252C%2520the%2520location%2520of%2520maximum%2520and%2520minimum%2520mutations%2520often%250Adisplayed%2520a%2520complementary%2520pattern%2520on%2520matrix%2520in%2520both%2520models%252C%2520with%2520the%2520Gate%250Amatrix%2520showing%2520a%2520unique%2520two-dimensional%2520asymmetry%2520after%2520rearrangement.%2520In%250AZephyr%252C%2520certain%2520mutations%2520consistently%2520resulted%2520in%2520poetic%2520or%2520conversational%250Arather%2520than%2520descriptive%2520outputs.%2520These%2520%2522writer%2522%2520mutations%2520grouped%2520according%2520to%250Athe%2520high-frequency%2520initial%2520word%2520of%2520the%2520output%252C%2520with%2520a%2520marked%2520tendency%2520to%2520share%250Athe%2520row%2520coordinate%2520even%2520when%2520they%2520are%2520in%2520different%2520matrices.%2520Our%2520findings%250Aaffirm%2520that%2520the%2520mutagenesis%2520screen%2520is%2520an%2520effective%2520tool%2520for%2520deciphering%2520the%250Acomplexities%2520of%2520large%2520language%2520models%2520and%2520identifying%2520unexpected%2520ways%2520to%2520expand%250Atheir%2520potential%252C%2520providing%2520deeper%2520insights%2520into%2520the%2520foundational%2520aspects%2520of%2520AI%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mutagenesis%20screen%20to%20map%20the%20functionals%20of%20parameters%20of%20Large%0A%20%20Language%20Models&entry.906535625=Yue%20Hu%20and%20Kai%20Hu%20and%20Patrick%20X.%20Zhao%20and%20Javed%20Khan%20and%20Chengming%20Xu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%20advanced%20artificial%0Aintelligence%2C%20excelling%20in%20numerous%20tasks.%20Although%20the%20functionality%20of%20a%0Amodel%20is%20inherently%20tied%20to%20its%20parameters%2C%20a%20systematic%20method%20for%20exploring%0Athe%20connections%20between%20the%20parameters%20and%20the%20functionality%20are%20lacking.%0AModels%20sharing%20similar%20structure%20and%20parameter%20counts%20exhibit%20significant%0Aperformance%20disparities%20across%20various%20tasks%2C%20prompting%20investigations%20into%20the%0Avarying%20patterns%20that%20govern%20their%20performance.%20We%20adopted%20a%20mutagenesis%20screen%0Aapproach%20inspired%20by%20the%20methods%20used%20in%20biological%20studies%2C%20to%20investigate%0ALlama2-7b%20and%20Zephyr.%20This%20technique%20involved%20mutating%20elements%20within%20the%0Amodels%27%20matrices%20to%20their%20maximum%20or%20minimum%20values%20to%20examine%20the%20relationship%0Abetween%20model%20parameters%20and%20their%20functionalities.%20Our%20research%20uncovered%0Amultiple%20levels%20of%20fine%20structures%20within%20both%20models.%20Many%20matrices%20showed%20a%0Amixture%20of%20maximum%20and%20minimum%20mutations%20following%20mutagenesis%2C%20but%20others%20were%0Apredominantly%20sensitive%20to%20one%20type.%20Notably%2C%20mutations%20that%20produced%0Aphenotypes%2C%20especially%20those%20with%20severe%20outcomes%2C%20tended%20to%20cluster%20along%0Aaxes.%20Additionally%2C%20the%20location%20of%20maximum%20and%20minimum%20mutations%20often%0Adisplayed%20a%20complementary%20pattern%20on%20matrix%20in%20both%20models%2C%20with%20the%20Gate%0Amatrix%20showing%20a%20unique%20two-dimensional%20asymmetry%20after%20rearrangement.%20In%0AZephyr%2C%20certain%20mutations%20consistently%20resulted%20in%20poetic%20or%20conversational%0Arather%20than%20descriptive%20outputs.%20These%20%22writer%22%20mutations%20grouped%20according%20to%0Athe%20high-frequency%20initial%20word%20of%20the%20output%2C%20with%20a%20marked%20tendency%20to%20share%0Athe%20row%20coordinate%20even%20when%20they%20are%20in%20different%20matrices.%20Our%20findings%0Aaffirm%20that%20the%20mutagenesis%20screen%20is%20an%20effective%20tool%20for%20deciphering%20the%0Acomplexities%20of%20large%20language%20models%20and%20identifying%20unexpected%20ways%20to%20expand%0Atheir%20potential%2C%20providing%20deeper%20insights%20into%20the%20foundational%20aspects%20of%20AI%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11494v1&entry.124074799=Read"},
{"title": "Inference-Time Selective Debiasing", "author": "Gleb Kuzmin and Neemesh Yadav and Ivan Smirnov and Timothy Baldwin and Artem Shelmanov", "abstract": "  We propose selective debiasing -- an inference-time safety mechanism that\naims to increase the overall quality of models in terms of prediction\nperformance and fairness in the situation when re-training a model is\nprohibitive. The method is inspired by selective prediction, where some\npredictions that are considered low quality are discarded at inference time. In\nour approach, we identify the potentially biased model predictions and, instead\nof discarding them, we debias them using LEACE -- a post-processing debiasing\nmethod. To select problematic predictions, we propose a bias quantification\napproach based on KL divergence, which achieves better results than standard UQ\nmethods. Experiments with text classification datasets demonstrate that\nselective debiasing helps to close the performance gap between post-processing\nmethods and at-training and pre-processing debiasing techniques.\n", "link": "http://arxiv.org/abs/2407.19345v2", "date": "2024-08-21", "relevancy": 1.5286, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.532}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Selective%20Debiasing&body=Title%3A%20Inference-Time%20Selective%20Debiasing%0AAuthor%3A%20Gleb%20Kuzmin%20and%20Neemesh%20Yadav%20and%20Ivan%20Smirnov%20and%20Timothy%20Baldwin%20and%20Artem%20Shelmanov%0AAbstract%3A%20%20%20We%20propose%20selective%20debiasing%20--%20an%20inference-time%20safety%20mechanism%20that%0Aaims%20to%20increase%20the%20overall%20quality%20of%20models%20in%20terms%20of%20prediction%0Aperformance%20and%20fairness%20in%20the%20situation%20when%20re-training%20a%20model%20is%0Aprohibitive.%20The%20method%20is%20inspired%20by%20selective%20prediction%2C%20where%20some%0Apredictions%20that%20are%20considered%20low%20quality%20are%20discarded%20at%20inference%20time.%20In%0Aour%20approach%2C%20we%20identify%20the%20potentially%20biased%20model%20predictions%20and%2C%20instead%0Aof%20discarding%20them%2C%20we%20debias%20them%20using%20LEACE%20--%20a%20post-processing%20debiasing%0Amethod.%20To%20select%20problematic%20predictions%2C%20we%20propose%20a%20bias%20quantification%0Aapproach%20based%20on%20KL%20divergence%2C%20which%20achieves%20better%20results%20than%20standard%20UQ%0Amethods.%20Experiments%20with%20text%20classification%20datasets%20demonstrate%20that%0Aselective%20debiasing%20helps%20to%20close%20the%20performance%20gap%20between%20post-processing%0Amethods%20and%20at-training%20and%20pre-processing%20debiasing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Selective%2520Debiasing%26entry.906535625%3DGleb%2520Kuzmin%2520and%2520Neemesh%2520Yadav%2520and%2520Ivan%2520Smirnov%2520and%2520Timothy%2520Baldwin%2520and%2520Artem%2520Shelmanov%26entry.1292438233%3D%2520%2520We%2520propose%2520selective%2520debiasing%2520--%2520an%2520inference-time%2520safety%2520mechanism%2520that%250Aaims%2520to%2520increase%2520the%2520overall%2520quality%2520of%2520models%2520in%2520terms%2520of%2520prediction%250Aperformance%2520and%2520fairness%2520in%2520the%2520situation%2520when%2520re-training%2520a%2520model%2520is%250Aprohibitive.%2520The%2520method%2520is%2520inspired%2520by%2520selective%2520prediction%252C%2520where%2520some%250Apredictions%2520that%2520are%2520considered%2520low%2520quality%2520are%2520discarded%2520at%2520inference%2520time.%2520In%250Aour%2520approach%252C%2520we%2520identify%2520the%2520potentially%2520biased%2520model%2520predictions%2520and%252C%2520instead%250Aof%2520discarding%2520them%252C%2520we%2520debias%2520them%2520using%2520LEACE%2520--%2520a%2520post-processing%2520debiasing%250Amethod.%2520To%2520select%2520problematic%2520predictions%252C%2520we%2520propose%2520a%2520bias%2520quantification%250Aapproach%2520based%2520on%2520KL%2520divergence%252C%2520which%2520achieves%2520better%2520results%2520than%2520standard%2520UQ%250Amethods.%2520Experiments%2520with%2520text%2520classification%2520datasets%2520demonstrate%2520that%250Aselective%2520debiasing%2520helps%2520to%2520close%2520the%2520performance%2520gap%2520between%2520post-processing%250Amethods%2520and%2520at-training%2520and%2520pre-processing%2520debiasing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Selective%20Debiasing&entry.906535625=Gleb%20Kuzmin%20and%20Neemesh%20Yadav%20and%20Ivan%20Smirnov%20and%20Timothy%20Baldwin%20and%20Artem%20Shelmanov&entry.1292438233=%20%20We%20propose%20selective%20debiasing%20--%20an%20inference-time%20safety%20mechanism%20that%0Aaims%20to%20increase%20the%20overall%20quality%20of%20models%20in%20terms%20of%20prediction%0Aperformance%20and%20fairness%20in%20the%20situation%20when%20re-training%20a%20model%20is%0Aprohibitive.%20The%20method%20is%20inspired%20by%20selective%20prediction%2C%20where%20some%0Apredictions%20that%20are%20considered%20low%20quality%20are%20discarded%20at%20inference%20time.%20In%0Aour%20approach%2C%20we%20identify%20the%20potentially%20biased%20model%20predictions%20and%2C%20instead%0Aof%20discarding%20them%2C%20we%20debias%20them%20using%20LEACE%20--%20a%20post-processing%20debiasing%0Amethod.%20To%20select%20problematic%20predictions%2C%20we%20propose%20a%20bias%20quantification%0Aapproach%20based%20on%20KL%20divergence%2C%20which%20achieves%20better%20results%20than%20standard%20UQ%0Amethods.%20Experiments%20with%20text%20classification%20datasets%20demonstrate%20that%0Aselective%20debiasing%20helps%20to%20close%20the%20performance%20gap%20between%20post-processing%0Amethods%20and%20at-training%20and%20pre-processing%20debiasing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19345v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


