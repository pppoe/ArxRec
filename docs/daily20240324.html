<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "T-MAE: Temporal Masked Autoencoders for Point Cloud Representation\n  Learning", "author": "Weijie Wei and Fatemeh Karimi Nejadasl and Theo Gevers and Martin R. Oswald", "abstract": "  The scarcity of annotated data in LiDAR point cloud understanding hinders\neffective representation learning. Consequently, scholars have been actively\ninvestigating efficacious self-supervised pre-training paradigms. Nevertheless,\ntemporal information, which is inherent in the LiDAR point cloud sequence, is\nconsistently disregarded. To better utilize this property, we propose an\neffective pre-training strategy, namely Temporal Masked Auto-Encoders (T-MAE),\nwhich takes as input temporally adjacent frames and learns temporal dependency.\nA SiamWCA backbone, containing a Siamese encoder and a windowed cross-attention\n(WCA) module, is established for the two-frame input. Considering that the\nmovement of an ego-vehicle alters the view of the same instance, temporal\nmodeling also serves as a robust and natural data augmentation, enhancing the\ncomprehension of target objects. SiamWCA is a powerful architecture but heavily\nrelies on annotated data. Our T-MAE pre-training strategy alleviates its demand\nfor annotated data. Comprehensive experiments demonstrate that T-MAE achieves\nthe best performance on both Waymo and ONCE datasets among competitive\nself-supervised approaches.\n", "link": "http://arxiv.org/abs/2312.10217v2", "date": "2024-03-21", "relevancy": 3.0896, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6029}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.565}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20T-MAE%3A%20Temporal%20Masked%20Autoencoders%20for%20Point%20Cloud%20Representation%0A%20%20Learning&body=Title%3A%20T-MAE%3A%20Temporal%20Masked%20Autoencoders%20for%20Point%20Cloud%20Representation%0A%20%20Learning%0AAuthor%3A%20Weijie%20Wei%20and%20Fatemeh%20Karimi%20Nejadasl%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20The%20scarcity%20of%20annotated%20data%20in%20LiDAR%20point%20cloud%20understanding%20hinders%0Aeffective%20representation%20learning.%20Consequently%2C%20scholars%20have%20been%20actively%0Ainvestigating%20efficacious%20self-supervised%20pre-training%20paradigms.%20Nevertheless%2C%0Atemporal%20information%2C%20which%20is%20inherent%20in%20the%20LiDAR%20point%20cloud%20sequence%2C%20is%0Aconsistently%20disregarded.%20To%20better%20utilize%20this%20property%2C%20we%20propose%20an%0Aeffective%20pre-training%20strategy%2C%20namely%20Temporal%20Masked%20Auto-Encoders%20%28T-MAE%29%2C%0Awhich%20takes%20as%20input%20temporally%20adjacent%20frames%20and%20learns%20temporal%20dependency.%0AA%20SiamWCA%20backbone%2C%20containing%20a%20Siamese%20encoder%20and%20a%20windowed%20cross-attention%0A%28WCA%29%20module%2C%20is%20established%20for%20the%20two-frame%20input.%20Considering%20that%20the%0Amovement%20of%20an%20ego-vehicle%20alters%20the%20view%20of%20the%20same%20instance%2C%20temporal%0Amodeling%20also%20serves%20as%20a%20robust%20and%20natural%20data%20augmentation%2C%20enhancing%20the%0Acomprehension%20of%20target%20objects.%20SiamWCA%20is%20a%20powerful%20architecture%20but%20heavily%0Arelies%20on%20annotated%20data.%20Our%20T-MAE%20pre-training%20strategy%20alleviates%20its%20demand%0Afor%20annotated%20data.%20Comprehensive%20experiments%20demonstrate%20that%20T-MAE%20achieves%0Athe%20best%20performance%20on%20both%20Waymo%20and%20ONCE%20datasets%20among%20competitive%0Aself-supervised%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10217v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-MAE%3A%20Temporal%20Masked%20Autoencoders%20for%20Point%20Cloud%20Representation%0A%20%20Learning&entry.906535625=Weijie%20Wei%20and%20Fatemeh%20Karimi%20Nejadasl%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20The%20scarcity%20of%20annotated%20data%20in%20LiDAR%20point%20cloud%20understanding%20hinders%0Aeffective%20representation%20learning.%20Consequently%2C%20scholars%20have%20been%20actively%0Ainvestigating%20efficacious%20self-supervised%20pre-training%20paradigms.%20Nevertheless%2C%0Atemporal%20information%2C%20which%20is%20inherent%20in%20the%20LiDAR%20point%20cloud%20sequence%2C%20is%0Aconsistently%20disregarded.%20To%20better%20utilize%20this%20property%2C%20we%20propose%20an%0Aeffective%20pre-training%20strategy%2C%20namely%20Temporal%20Masked%20Auto-Encoders%20%28T-MAE%29%2C%0Awhich%20takes%20as%20input%20temporally%20adjacent%20frames%20and%20learns%20temporal%20dependency.%0AA%20SiamWCA%20backbone%2C%20containing%20a%20Siamese%20encoder%20and%20a%20windowed%20cross-attention%0A%28WCA%29%20module%2C%20is%20established%20for%20the%20two-frame%20input.%20Considering%20that%20the%0Amovement%20of%20an%20ego-vehicle%20alters%20the%20view%20of%20the%20same%20instance%2C%20temporal%0Amodeling%20also%20serves%20as%20a%20robust%20and%20natural%20data%20augmentation%2C%20enhancing%20the%0Acomprehension%20of%20target%20objects.%20SiamWCA%20is%20a%20powerful%20architecture%20but%20heavily%0Arelies%20on%20annotated%20data.%20Our%20T-MAE%20pre-training%20strategy%20alleviates%20its%20demand%0Afor%20annotated%20data.%20Comprehensive%20experiments%20demonstrate%20that%20T-MAE%20achieves%0Athe%20best%20performance%20on%20both%20Waymo%20and%20ONCE%20datasets%20among%20competitive%0Aself-supervised%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10217v2&entry.124074799=Read"},
{"title": "Enabling Visual Composition and Animation in Unsupervised Video\n  Generation", "author": "Aram Davtyan and Sepehr Sameni and Bj\u00f6rn Ommer and Paolo Favaro", "abstract": "  In this work we propose a novel method for unsupervised controllable video\ngeneration. Once trained on a dataset of unannotated videos, at inference our\nmodel is capable of both composing scenes of predefined object parts and\nanimating them in a plausible and controlled way. This is achieved by\nconditioning video generation on a randomly selected subset of local\npre-trained self-supervised features during training. We call our model CAGE\nfor visual Composition and Animation for video GEneration. We conduct a series\nof experiments to demonstrate capabilities of CAGE in various settings. Project\nwebsite: https://araachie.github.io/cage.\n", "link": "http://arxiv.org/abs/2403.14368v1", "date": "2024-03-21", "relevancy": 2.953, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6436}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5854}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5429}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enabling%20Visual%20Composition%20and%20Animation%20in%20Unsupervised%20Video%0A%20%20Generation&body=Title%3A%20Enabling%20Visual%20Composition%20and%20Animation%20in%20Unsupervised%20Video%0A%20%20Generation%0AAuthor%3A%20Aram%20Davtyan%20and%20Sepehr%20Sameni%20and%20Bj%C3%B6rn%20Ommer%20and%20Paolo%20Favaro%0AAbstract%3A%20%20%20In%20this%20work%20we%20propose%20a%20novel%20method%20for%20unsupervised%20controllable%20video%0Ageneration.%20Once%20trained%20on%20a%20dataset%20of%20unannotated%20videos%2C%20at%20inference%20our%0Amodel%20is%20capable%20of%20both%20composing%20scenes%20of%20predefined%20object%20parts%20and%0Aanimating%20them%20in%20a%20plausible%20and%20controlled%20way.%20This%20is%20achieved%20by%0Aconditioning%20video%20generation%20on%20a%20randomly%20selected%20subset%20of%20local%0Apre-trained%20self-supervised%20features%20during%20training.%20We%20call%20our%20model%20CAGE%0Afor%20visual%20Composition%20and%20Animation%20for%20video%20GEneration.%20We%20conduct%20a%20series%0Aof%20experiments%20to%20demonstrate%20capabilities%20of%20CAGE%20in%20various%20settings.%20Project%0Awebsite%3A%20https%3A//araachie.github.io/cage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14368v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Visual%20Composition%20and%20Animation%20in%20Unsupervised%20Video%0A%20%20Generation&entry.906535625=Aram%20Davtyan%20and%20Sepehr%20Sameni%20and%20Bj%C3%B6rn%20Ommer%20and%20Paolo%20Favaro&entry.1292438233=%20%20In%20this%20work%20we%20propose%20a%20novel%20method%20for%20unsupervised%20controllable%20video%0Ageneration.%20Once%20trained%20on%20a%20dataset%20of%20unannotated%20videos%2C%20at%20inference%20our%0Amodel%20is%20capable%20of%20both%20composing%20scenes%20of%20predefined%20object%20parts%20and%0Aanimating%20them%20in%20a%20plausible%20and%20controlled%20way.%20This%20is%20achieved%20by%0Aconditioning%20video%20generation%20on%20a%20randomly%20selected%20subset%20of%20local%0Apre-trained%20self-supervised%20features%20during%20training.%20We%20call%20our%20model%20CAGE%0Afor%20visual%20Composition%20and%20Animation%20for%20video%20GEneration.%20We%20conduct%20a%20series%0Aof%20experiments%20to%20demonstrate%20capabilities%20of%20CAGE%20in%20various%20settings.%20Project%0Awebsite%3A%20https%3A//araachie.github.io/cage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14368v1&entry.124074799=Read"},
{"title": "Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D\n  Pose Estimation", "author": "Francesco Di Felice and Alberto Remus and Stefano Gasperini and Benjamin Busam and Lionel Ott and Federico Tombari and Roland Siegwart and Carlo Alberto Avizzano", "abstract": "  Estimating the pose of objects through vision is essential to make robotic\nplatforms interact with the environment. Yet, it presents many challenges,\noften related to the lack of flexibility and generalizability of\nstate-of-the-art solutions. Diffusion models are a cutting-edge neural\narchitecture transforming 2D and 3D computer vision, outlining remarkable\nperformances in zero-shot novel-view synthesis. Such a use case is particularly\nintriguing for reconstructing 3D objects. However, localizing objects in\nunstructured environments is rather unexplored. To this end, this work presents\nZero123-6D to demonstrate the utility of Diffusion Model-based\nnovel-view-synthesizers in enhancing RGB 6D pose estimation at category-level\nby integrating them with feature extraction techniques. The outlined method\nexploits such a novel view synthesizer to expand a sparse set of RGB-only\nreference views for the zero-shot 6D pose estimation task. Experiments are\nquantitatively analyzed on the CO3D dataset, showcasing increased performance\nover baselines, a substantial reduction in data requirements, and the removal\nof the necessity of depth information.\n", "link": "http://arxiv.org/abs/2403.14279v1", "date": "2024-03-21", "relevancy": 2.8763, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5765}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5764}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5728}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero123-6D%3A%20Zero-shot%20Novel%20View%20Synthesis%20for%20RGB%20Category-level%206D%0A%20%20Pose%20Estimation&body=Title%3A%20Zero123-6D%3A%20Zero-shot%20Novel%20View%20Synthesis%20for%20RGB%20Category-level%206D%0A%20%20Pose%20Estimation%0AAuthor%3A%20Francesco%20Di%20Felice%20and%20Alberto%20Remus%20and%20Stefano%20Gasperini%20and%20Benjamin%20Busam%20and%20Lionel%20Ott%20and%20Federico%20Tombari%20and%20Roland%20Siegwart%20and%20Carlo%20Alberto%20Avizzano%0AAbstract%3A%20%20%20Estimating%20the%20pose%20of%20objects%20through%20vision%20is%20essential%20to%20make%20robotic%0Aplatforms%20interact%20with%20the%20environment.%20Yet%2C%20it%20presents%20many%20challenges%2C%0Aoften%20related%20to%20the%20lack%20of%20flexibility%20and%20generalizability%20of%0Astate-of-the-art%20solutions.%20Diffusion%20models%20are%20a%20cutting-edge%20neural%0Aarchitecture%20transforming%202D%20and%203D%20computer%20vision%2C%20outlining%20remarkable%0Aperformances%20in%20zero-shot%20novel-view%20synthesis.%20Such%20a%20use%20case%20is%20particularly%0Aintriguing%20for%20reconstructing%203D%20objects.%20However%2C%20localizing%20objects%20in%0Aunstructured%20environments%20is%20rather%20unexplored.%20To%20this%20end%2C%20this%20work%20presents%0AZero123-6D%20to%20demonstrate%20the%20utility%20of%20Diffusion%20Model-based%0Anovel-view-synthesizers%20in%20enhancing%20RGB%206D%20pose%20estimation%20at%20category-level%0Aby%20integrating%20them%20with%20feature%20extraction%20techniques.%20The%20outlined%20method%0Aexploits%20such%20a%20novel%20view%20synthesizer%20to%20expand%20a%20sparse%20set%20of%20RGB-only%0Areference%20views%20for%20the%20zero-shot%206D%20pose%20estimation%20task.%20Experiments%20are%0Aquantitatively%20analyzed%20on%20the%20CO3D%20dataset%2C%20showcasing%20increased%20performance%0Aover%20baselines%2C%20a%20substantial%20reduction%20in%20data%20requirements%2C%20and%20the%20removal%0Aof%20the%20necessity%20of%20depth%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14279v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero123-6D%3A%20Zero-shot%20Novel%20View%20Synthesis%20for%20RGB%20Category-level%206D%0A%20%20Pose%20Estimation&entry.906535625=Francesco%20Di%20Felice%20and%20Alberto%20Remus%20and%20Stefano%20Gasperini%20and%20Benjamin%20Busam%20and%20Lionel%20Ott%20and%20Federico%20Tombari%20and%20Roland%20Siegwart%20and%20Carlo%20Alberto%20Avizzano&entry.1292438233=%20%20Estimating%20the%20pose%20of%20objects%20through%20vision%20is%20essential%20to%20make%20robotic%0Aplatforms%20interact%20with%20the%20environment.%20Yet%2C%20it%20presents%20many%20challenges%2C%0Aoften%20related%20to%20the%20lack%20of%20flexibility%20and%20generalizability%20of%0Astate-of-the-art%20solutions.%20Diffusion%20models%20are%20a%20cutting-edge%20neural%0Aarchitecture%20transforming%202D%20and%203D%20computer%20vision%2C%20outlining%20remarkable%0Aperformances%20in%20zero-shot%20novel-view%20synthesis.%20Such%20a%20use%20case%20is%20particularly%0Aintriguing%20for%20reconstructing%203D%20objects.%20However%2C%20localizing%20objects%20in%0Aunstructured%20environments%20is%20rather%20unexplored.%20To%20this%20end%2C%20this%20work%20presents%0AZero123-6D%20to%20demonstrate%20the%20utility%20of%20Diffusion%20Model-based%0Anovel-view-synthesizers%20in%20enhancing%20RGB%206D%20pose%20estimation%20at%20category-level%0Aby%20integrating%20them%20with%20feature%20extraction%20techniques.%20The%20outlined%20method%0Aexploits%20such%20a%20novel%20view%20synthesizer%20to%20expand%20a%20sparse%20set%20of%20RGB-only%0Areference%20views%20for%20the%20zero-shot%206D%20pose%20estimation%20task.%20Experiments%20are%0Aquantitatively%20analyzed%20on%20the%20CO3D%20dataset%2C%20showcasing%20increased%20performance%0Aover%20baselines%2C%20a%20substantial%20reduction%20in%20data%20requirements%2C%20and%20the%20removal%0Aof%20the%20necessity%20of%20depth%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14279v1&entry.124074799=Read"},
{"title": "Exploring Task Unification in Graph Representation Learning via\n  Generative Approach", "author": "Yulan Hu and Sheng Ouyang and Zhirui Yang and Ge Chen and Junchen Wan and Xiao Wang and Yong Liu", "abstract": "  Graphs are ubiquitous in real-world scenarios and encompass a diverse range\nof tasks, from node-, edge-, and graph-level tasks to transfer learning.\nHowever, designing specific tasks for each type of graph data is often costly\nand lacks generalizability. Recent endeavors under the \"Pre-training +\nFine-tuning\" or \"Pre-training + Prompt\" paradigms aim to design a unified\nframework capable of generalizing across multiple graph tasks. Among these,\ngraph autoencoders (GAEs), generative self-supervised models, have demonstrated\ntheir potential in effectively addressing various graph tasks. Nevertheless,\nthese methods typically employ multi-stage training and require adaptive\ndesigns, which on one hand make it difficult to be seamlessly applied to\ndiverse graph tasks and on the other hand overlook the negative impact caused\nby discrepancies in task objectives between the different stages. To address\nthese challenges, we propose GA^2E, a unified adversarially masked autoencoder\ncapable of addressing the above challenges seamlessly. Specifically, GA^2E\nproposes to use the subgraph as the meta-structure, which remains consistent\nacross all graph tasks (ranging from node-, edge-, and graph-level to transfer\nlearning) and all stages (both during training and inference). Further, GA^2E\noperates in a \\textbf{\"Generate then Discriminate\"} manner. It leverages the\nmasked GAE to reconstruct the input subgraph whilst treating it as a generator\nto compel the reconstructed graphs resemble the input subgraph. Furthermore,\nGA^2E introduces an auxiliary discriminator to discern the authenticity between\nthe reconstructed (generated) subgraph and the input subgraph, thus ensuring\nthe robustness of the graph representation through adversarial training\nmechanisms. We validate GA^2E's capabilities through extensive experiments on\n21 datasets across four types of graph tasks.\n", "link": "http://arxiv.org/abs/2403.14340v1", "date": "2024-03-21", "relevancy": 2.8406, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5758}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5696}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5589}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Task%20Unification%20in%20Graph%20Representation%20Learning%20via%0A%20%20Generative%20Approach&body=Title%3A%20Exploring%20Task%20Unification%20in%20Graph%20Representation%20Learning%20via%0A%20%20Generative%20Approach%0AAuthor%3A%20Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Zhirui%20Yang%20and%20Ge%20Chen%20and%20Junchen%20Wan%20and%20Xiao%20Wang%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Graphs%20are%20ubiquitous%20in%20real-world%20scenarios%20and%20encompass%20a%20diverse%20range%0Aof%20tasks%2C%20from%20node-%2C%20edge-%2C%20and%20graph-level%20tasks%20to%20transfer%20learning.%0AHowever%2C%20designing%20specific%20tasks%20for%20each%20type%20of%20graph%20data%20is%20often%20costly%0Aand%20lacks%20generalizability.%20Recent%20endeavors%20under%20the%20%22Pre-training%20%2B%0AFine-tuning%22%20or%20%22Pre-training%20%2B%20Prompt%22%20paradigms%20aim%20to%20design%20a%20unified%0Aframework%20capable%20of%20generalizing%20across%20multiple%20graph%20tasks.%20Among%20these%2C%0Agraph%20autoencoders%20%28GAEs%29%2C%20generative%20self-supervised%20models%2C%20have%20demonstrated%0Atheir%20potential%20in%20effectively%20addressing%20various%20graph%20tasks.%20Nevertheless%2C%0Athese%20methods%20typically%20employ%20multi-stage%20training%20and%20require%20adaptive%0Adesigns%2C%20which%20on%20one%20hand%20make%20it%20difficult%20to%20be%20seamlessly%20applied%20to%0Adiverse%20graph%20tasks%20and%20on%20the%20other%20hand%20overlook%20the%20negative%20impact%20caused%0Aby%20discrepancies%20in%20task%20objectives%20between%20the%20different%20stages.%20To%20address%0Athese%20challenges%2C%20we%20propose%20GA%5E2E%2C%20a%20unified%20adversarially%20masked%20autoencoder%0Acapable%20of%20addressing%20the%20above%20challenges%20seamlessly.%20Specifically%2C%20GA%5E2E%0Aproposes%20to%20use%20the%20subgraph%20as%20the%20meta-structure%2C%20which%20remains%20consistent%0Aacross%20all%20graph%20tasks%20%28ranging%20from%20node-%2C%20edge-%2C%20and%20graph-level%20to%20transfer%0Alearning%29%20and%20all%20stages%20%28both%20during%20training%20and%20inference%29.%20Further%2C%20GA%5E2E%0Aoperates%20in%20a%20%5Ctextbf%7B%22Generate%20then%20Discriminate%22%7D%20manner.%20It%20leverages%20the%0Amasked%20GAE%20to%20reconstruct%20the%20input%20subgraph%20whilst%20treating%20it%20as%20a%20generator%0Ato%20compel%20the%20reconstructed%20graphs%20resemble%20the%20input%20subgraph.%20Furthermore%2C%0AGA%5E2E%20introduces%20an%20auxiliary%20discriminator%20to%20discern%20the%20authenticity%20between%0Athe%20reconstructed%20%28generated%29%20subgraph%20and%20the%20input%20subgraph%2C%20thus%20ensuring%0Athe%20robustness%20of%20the%20graph%20representation%20through%20adversarial%20training%0Amechanisms.%20We%20validate%20GA%5E2E%27s%20capabilities%20through%20extensive%20experiments%20on%0A21%20datasets%20across%20four%20types%20of%20graph%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14340v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Task%20Unification%20in%20Graph%20Representation%20Learning%20via%0A%20%20Generative%20Approach&entry.906535625=Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Zhirui%20Yang%20and%20Ge%20Chen%20and%20Junchen%20Wan%20and%20Xiao%20Wang%20and%20Yong%20Liu&entry.1292438233=%20%20Graphs%20are%20ubiquitous%20in%20real-world%20scenarios%20and%20encompass%20a%20diverse%20range%0Aof%20tasks%2C%20from%20node-%2C%20edge-%2C%20and%20graph-level%20tasks%20to%20transfer%20learning.%0AHowever%2C%20designing%20specific%20tasks%20for%20each%20type%20of%20graph%20data%20is%20often%20costly%0Aand%20lacks%20generalizability.%20Recent%20endeavors%20under%20the%20%22Pre-training%20%2B%0AFine-tuning%22%20or%20%22Pre-training%20%2B%20Prompt%22%20paradigms%20aim%20to%20design%20a%20unified%0Aframework%20capable%20of%20generalizing%20across%20multiple%20graph%20tasks.%20Among%20these%2C%0Agraph%20autoencoders%20%28GAEs%29%2C%20generative%20self-supervised%20models%2C%20have%20demonstrated%0Atheir%20potential%20in%20effectively%20addressing%20various%20graph%20tasks.%20Nevertheless%2C%0Athese%20methods%20typically%20employ%20multi-stage%20training%20and%20require%20adaptive%0Adesigns%2C%20which%20on%20one%20hand%20make%20it%20difficult%20to%20be%20seamlessly%20applied%20to%0Adiverse%20graph%20tasks%20and%20on%20the%20other%20hand%20overlook%20the%20negative%20impact%20caused%0Aby%20discrepancies%20in%20task%20objectives%20between%20the%20different%20stages.%20To%20address%0Athese%20challenges%2C%20we%20propose%20GA%5E2E%2C%20a%20unified%20adversarially%20masked%20autoencoder%0Acapable%20of%20addressing%20the%20above%20challenges%20seamlessly.%20Specifically%2C%20GA%5E2E%0Aproposes%20to%20use%20the%20subgraph%20as%20the%20meta-structure%2C%20which%20remains%20consistent%0Aacross%20all%20graph%20tasks%20%28ranging%20from%20node-%2C%20edge-%2C%20and%20graph-level%20to%20transfer%0Alearning%29%20and%20all%20stages%20%28both%20during%20training%20and%20inference%29.%20Further%2C%20GA%5E2E%0Aoperates%20in%20a%20%5Ctextbf%7B%22Generate%20then%20Discriminate%22%7D%20manner.%20It%20leverages%20the%0Amasked%20GAE%20to%20reconstruct%20the%20input%20subgraph%20whilst%20treating%20it%20as%20a%20generator%0Ato%20compel%20the%20reconstructed%20graphs%20resemble%20the%20input%20subgraph.%20Furthermore%2C%0AGA%5E2E%20introduces%20an%20auxiliary%20discriminator%20to%20discern%20the%20authenticity%20between%0Athe%20reconstructed%20%28generated%29%20subgraph%20and%20the%20input%20subgraph%2C%20thus%20ensuring%0Athe%20robustness%20of%20the%20graph%20representation%20through%20adversarial%20training%0Amechanisms.%20We%20validate%20GA%5E2E%27s%20capabilities%20through%20extensive%20experiments%20on%0A21%20datasets%20across%20four%20types%20of%20graph%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14340v1&entry.124074799=Read"},
{"title": "VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition", "author": "Yun-Jin Li and Mariia Gladkova and Yan Xia and Rui Wang and Daniel Cremers", "abstract": "  Recent works on the global place recognition treat the task as a retrieval\nproblem, where an off-the-shelf global descriptor is commonly designed in\nimage-based and LiDAR-based modalities. However, it is non-trivial to perform\naccurate image-LiDAR global place recognition since extracting consistent and\nrobust global descriptors from different domains (2D images and 3D point\nclouds) is challenging. To address this issue, we propose a novel\nVoxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel\ncorrespondences in a self-supervised manner and brings them into a shared\nfeature space. Specifically, VXP is trained in a two-stage manner that first\nexplicitly exploits local feature correspondences and enforces similarity of\nglobal descriptors. Extensive experiments on the three benchmarks (Oxford\nRobotCar, ViViD++ and KITTI) demonstrate our method surpasses the\nstate-of-the-art cross-modal retrieval by a large margin.\n", "link": "http://arxiv.org/abs/2403.14594v1", "date": "2024-03-21", "relevancy": 2.8125, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.595}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VXP%3A%20Voxel-Cross-Pixel%20Large-scale%20Image-LiDAR%20Place%20Recognition&body=Title%3A%20VXP%3A%20Voxel-Cross-Pixel%20Large-scale%20Image-LiDAR%20Place%20Recognition%0AAuthor%3A%20Yun-Jin%20Li%20and%20Mariia%20Gladkova%20and%20Yan%20Xia%20and%20Rui%20Wang%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Recent%20works%20on%20the%20global%20place%20recognition%20treat%20the%20task%20as%20a%20retrieval%0Aproblem%2C%20where%20an%20off-the-shelf%20global%20descriptor%20is%20commonly%20designed%20in%0Aimage-based%20and%20LiDAR-based%20modalities.%20However%2C%20it%20is%20non-trivial%20to%20perform%0Aaccurate%20image-LiDAR%20global%20place%20recognition%20since%20extracting%20consistent%20and%0Arobust%20global%20descriptors%20from%20different%20domains%20%282D%20images%20and%203D%20point%0Aclouds%29%20is%20challenging.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0AVoxel-Cross-Pixel%20%28VXP%29%20approach%2C%20which%20establishes%20voxel%20and%20pixel%0Acorrespondences%20in%20a%20self-supervised%20manner%20and%20brings%20them%20into%20a%20shared%0Afeature%20space.%20Specifically%2C%20VXP%20is%20trained%20in%20a%20two-stage%20manner%20that%20first%0Aexplicitly%20exploits%20local%20feature%20correspondences%20and%20enforces%20similarity%20of%0Aglobal%20descriptors.%20Extensive%20experiments%20on%20the%20three%20benchmarks%20%28Oxford%0ARobotCar%2C%20ViViD%2B%2B%20and%20KITTI%29%20demonstrate%20our%20method%20surpasses%20the%0Astate-of-the-art%20cross-modal%20retrieval%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14594v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VXP%3A%20Voxel-Cross-Pixel%20Large-scale%20Image-LiDAR%20Place%20Recognition&entry.906535625=Yun-Jin%20Li%20and%20Mariia%20Gladkova%20and%20Yan%20Xia%20and%20Rui%20Wang%20and%20Daniel%20Cremers&entry.1292438233=%20%20Recent%20works%20on%20the%20global%20place%20recognition%20treat%20the%20task%20as%20a%20retrieval%0Aproblem%2C%20where%20an%20off-the-shelf%20global%20descriptor%20is%20commonly%20designed%20in%0Aimage-based%20and%20LiDAR-based%20modalities.%20However%2C%20it%20is%20non-trivial%20to%20perform%0Aaccurate%20image-LiDAR%20global%20place%20recognition%20since%20extracting%20consistent%20and%0Arobust%20global%20descriptors%20from%20different%20domains%20%282D%20images%20and%203D%20point%0Aclouds%29%20is%20challenging.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0AVoxel-Cross-Pixel%20%28VXP%29%20approach%2C%20which%20establishes%20voxel%20and%20pixel%0Acorrespondences%20in%20a%20self-supervised%20manner%20and%20brings%20them%20into%20a%20shared%0Afeature%20space.%20Specifically%2C%20VXP%20is%20trained%20in%20a%20two-stage%20manner%20that%20first%0Aexplicitly%20exploits%20local%20feature%20correspondences%20and%20enforces%20similarity%20of%0Aglobal%20descriptors.%20Extensive%20experiments%20on%20the%20three%20benchmarks%20%28Oxford%0ARobotCar%2C%20ViViD%2B%2B%20and%20KITTI%29%20demonstrate%20our%20method%20surpasses%20the%0Astate-of-the-art%20cross-modal%20retrieval%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14594v1&entry.124074799=Read"},
{"title": "Visually-Aware Context Modeling for News Image Captioning", "author": "Tingyu Qu and Tinne Tuytelaars and Marie-Francine Moens", "abstract": "  News Image Captioning aims to create captions from news articles and images,\nemphasizing the connection between textual context and visual elements.\nRecognizing the significance of human faces in news images and the face-name\nco-occurrence pattern in existing datasets, we propose a face-naming module for\nlearning better name embeddings. Apart from names, which can be directly linked\nto an image area (faces), news image captions mostly contain context\ninformation that can only be found in the article. We design a retrieval\nstrategy using CLIP to retrieve sentences that are semantically close to the\nimage, mimicking human thought process of linking articles to images.\nFurthermore, to tackle the problem of the imbalanced proportion of article\ncontext and image context in captions, we introduce a simple yet effective\nmethod Contrasting with Language Model backbone (CoLaM) to the training\npipeline. We conduct extensive experiments to demonstrate the efficacy of our\nframework. We out-perform the previous state-of-the-art (without external data)\nby 7.97/5.80 CIDEr scores on GoodNews/NYTimes800k. Our code is available at\nhttps://github.com/tingyu215/VACNIC.\n", "link": "http://arxiv.org/abs/2308.08325v2", "date": "2024-03-21", "relevancy": 2.7617, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5649}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5377}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visually-Aware%20Context%20Modeling%20for%20News%20Image%20Captioning&body=Title%3A%20Visually-Aware%20Context%20Modeling%20for%20News%20Image%20Captioning%0AAuthor%3A%20Tingyu%20Qu%20and%20Tinne%20Tuytelaars%20and%20Marie-Francine%20Moens%0AAbstract%3A%20%20%20News%20Image%20Captioning%20aims%20to%20create%20captions%20from%20news%20articles%20and%20images%2C%0Aemphasizing%20the%20connection%20between%20textual%20context%20and%20visual%20elements.%0ARecognizing%20the%20significance%20of%20human%20faces%20in%20news%20images%20and%20the%20face-name%0Aco-occurrence%20pattern%20in%20existing%20datasets%2C%20we%20propose%20a%20face-naming%20module%20for%0Alearning%20better%20name%20embeddings.%20Apart%20from%20names%2C%20which%20can%20be%20directly%20linked%0Ato%20an%20image%20area%20%28faces%29%2C%20news%20image%20captions%20mostly%20contain%20context%0Ainformation%20that%20can%20only%20be%20found%20in%20the%20article.%20We%20design%20a%20retrieval%0Astrategy%20using%20CLIP%20to%20retrieve%20sentences%20that%20are%20semantically%20close%20to%20the%0Aimage%2C%20mimicking%20human%20thought%20process%20of%20linking%20articles%20to%20images.%0AFurthermore%2C%20to%20tackle%20the%20problem%20of%20the%20imbalanced%20proportion%20of%20article%0Acontext%20and%20image%20context%20in%20captions%2C%20we%20introduce%20a%20simple%20yet%20effective%0Amethod%20Contrasting%20with%20Language%20Model%20backbone%20%28CoLaM%29%20to%20the%20training%0Apipeline.%20We%20conduct%20extensive%20experiments%20to%20demonstrate%20the%20efficacy%20of%20our%0Aframework.%20We%20out-perform%20the%20previous%20state-of-the-art%20%28without%20external%20data%29%0Aby%207.97/5.80%20CIDEr%20scores%20on%20GoodNews/NYTimes800k.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tingyu215/VACNIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08325v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visually-Aware%20Context%20Modeling%20for%20News%20Image%20Captioning&entry.906535625=Tingyu%20Qu%20and%20Tinne%20Tuytelaars%20and%20Marie-Francine%20Moens&entry.1292438233=%20%20News%20Image%20Captioning%20aims%20to%20create%20captions%20from%20news%20articles%20and%20images%2C%0Aemphasizing%20the%20connection%20between%20textual%20context%20and%20visual%20elements.%0ARecognizing%20the%20significance%20of%20human%20faces%20in%20news%20images%20and%20the%20face-name%0Aco-occurrence%20pattern%20in%20existing%20datasets%2C%20we%20propose%20a%20face-naming%20module%20for%0Alearning%20better%20name%20embeddings.%20Apart%20from%20names%2C%20which%20can%20be%20directly%20linked%0Ato%20an%20image%20area%20%28faces%29%2C%20news%20image%20captions%20mostly%20contain%20context%0Ainformation%20that%20can%20only%20be%20found%20in%20the%20article.%20We%20design%20a%20retrieval%0Astrategy%20using%20CLIP%20to%20retrieve%20sentences%20that%20are%20semantically%20close%20to%20the%0Aimage%2C%20mimicking%20human%20thought%20process%20of%20linking%20articles%20to%20images.%0AFurthermore%2C%20to%20tackle%20the%20problem%20of%20the%20imbalanced%20proportion%20of%20article%0Acontext%20and%20image%20context%20in%20captions%2C%20we%20introduce%20a%20simple%20yet%20effective%0Amethod%20Contrasting%20with%20Language%20Model%20backbone%20%28CoLaM%29%20to%20the%20training%0Apipeline.%20We%20conduct%20extensive%20experiments%20to%20demonstrate%20the%20efficacy%20of%20our%0Aframework.%20We%20out-perform%20the%20previous%20state-of-the-art%20%28without%20external%20data%29%0Aby%207.97/5.80%20CIDEr%20scores%20on%20GoodNews/NYTimes800k.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tingyu215/VACNIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08325v2&entry.124074799=Read"},
{"title": "Analyzing Local Representations of Self-supervised Vision Transformers", "author": "Ani Vanyan and Alvard Barseghyan and Hakob Tamazyan and Vahan Huroyan and Hrant Khachatrian and Martin Danelljan", "abstract": "  In this paper, we present a comparative analysis of various self-supervised\nVision Transformers (ViTs), focusing on their local representative power.\nInspired by large language models, we examine the abilities of ViTs to perform\nvarious computer vision tasks with little to no fine-tuning. We design\nevaluation framework to analyze the quality of local, i.e.\\ patch-level,\nrepresentations in the context of few-shot semantic segmentation, instance\nidentification, object retrieval and tracking. We discover that contrastive\nlearning based methods like DINO produce more universal patch representations\nthat can be immediately applied for downstream tasks with no parameter tuning,\ncompared to masked image modeling. The embeddings learned using the latter\napproach, e.g. in masked autoencoders, have high variance features that harm\ndistance-based algorithms, such as k-NN, and do not contain useful information\nfor most downstream tasks. Furthermore, we demonstrate that removing these\nhigh-variance features enhances k-NN for MAE, as well as for its recent\nextension Scale-MAE. Finally, we find an object instance retrieval setting\nwhere DINOv2, a model pretrained on two orders of magnitude more data, falls\nshort of its less compute intensive counterpart DINO.\n", "link": "http://arxiv.org/abs/2401.00463v2", "date": "2024-03-21", "relevancy": 2.7545, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5563}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5532}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5432}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Local%20Representations%20of%20Self-supervised%20Vision%20Transformers&body=Title%3A%20Analyzing%20Local%20Representations%20of%20Self-supervised%20Vision%20Transformers%0AAuthor%3A%20Ani%20Vanyan%20and%20Alvard%20Barseghyan%20and%20Hakob%20Tamazyan%20and%20Vahan%20Huroyan%20and%20Hrant%20Khachatrian%20and%20Martin%20Danelljan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20comparative%20analysis%20of%20various%20self-supervised%0AVision%20Transformers%20%28ViTs%29%2C%20focusing%20on%20their%20local%20representative%20power.%0AInspired%20by%20large%20language%20models%2C%20we%20examine%20the%20abilities%20of%20ViTs%20to%20perform%0Avarious%20computer%20vision%20tasks%20with%20little%20to%20no%20fine-tuning.%20We%20design%0Aevaluation%20framework%20to%20analyze%20the%20quality%20of%20local%2C%20i.e.%5C%20patch-level%2C%0Arepresentations%20in%20the%20context%20of%20few-shot%20semantic%20segmentation%2C%20instance%0Aidentification%2C%20object%20retrieval%20and%20tracking.%20We%20discover%20that%20contrastive%0Alearning%20based%20methods%20like%20DINO%20produce%20more%20universal%20patch%20representations%0Athat%20can%20be%20immediately%20applied%20for%20downstream%20tasks%20with%20no%20parameter%20tuning%2C%0Acompared%20to%20masked%20image%20modeling.%20The%20embeddings%20learned%20using%20the%20latter%0Aapproach%2C%20e.g.%20in%20masked%20autoencoders%2C%20have%20high%20variance%20features%20that%20harm%0Adistance-based%20algorithms%2C%20such%20as%20k-NN%2C%20and%20do%20not%20contain%20useful%20information%0Afor%20most%20downstream%20tasks.%20Furthermore%2C%20we%20demonstrate%20that%20removing%20these%0Ahigh-variance%20features%20enhances%20k-NN%20for%20MAE%2C%20as%20well%20as%20for%20its%20recent%0Aextension%20Scale-MAE.%20Finally%2C%20we%20find%20an%20object%20instance%20retrieval%20setting%0Awhere%20DINOv2%2C%20a%20model%20pretrained%20on%20two%20orders%20of%20magnitude%20more%20data%2C%20falls%0Ashort%20of%20its%20less%20compute%20intensive%20counterpart%20DINO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00463v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Local%20Representations%20of%20Self-supervised%20Vision%20Transformers&entry.906535625=Ani%20Vanyan%20and%20Alvard%20Barseghyan%20and%20Hakob%20Tamazyan%20and%20Vahan%20Huroyan%20and%20Hrant%20Khachatrian%20and%20Martin%20Danelljan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20comparative%20analysis%20of%20various%20self-supervised%0AVision%20Transformers%20%28ViTs%29%2C%20focusing%20on%20their%20local%20representative%20power.%0AInspired%20by%20large%20language%20models%2C%20we%20examine%20the%20abilities%20of%20ViTs%20to%20perform%0Avarious%20computer%20vision%20tasks%20with%20little%20to%20no%20fine-tuning.%20We%20design%0Aevaluation%20framework%20to%20analyze%20the%20quality%20of%20local%2C%20i.e.%5C%20patch-level%2C%0Arepresentations%20in%20the%20context%20of%20few-shot%20semantic%20segmentation%2C%20instance%0Aidentification%2C%20object%20retrieval%20and%20tracking.%20We%20discover%20that%20contrastive%0Alearning%20based%20methods%20like%20DINO%20produce%20more%20universal%20patch%20representations%0Athat%20can%20be%20immediately%20applied%20for%20downstream%20tasks%20with%20no%20parameter%20tuning%2C%0Acompared%20to%20masked%20image%20modeling.%20The%20embeddings%20learned%20using%20the%20latter%0Aapproach%2C%20e.g.%20in%20masked%20autoencoders%2C%20have%20high%20variance%20features%20that%20harm%0Adistance-based%20algorithms%2C%20such%20as%20k-NN%2C%20and%20do%20not%20contain%20useful%20information%0Afor%20most%20downstream%20tasks.%20Furthermore%2C%20we%20demonstrate%20that%20removing%20these%0Ahigh-variance%20features%20enhances%20k-NN%20for%20MAE%2C%20as%20well%20as%20for%20its%20recent%0Aextension%20Scale-MAE.%20Finally%2C%20we%20find%20an%20object%20instance%20retrieval%20setting%0Awhere%20DINOv2%2C%20a%20model%20pretrained%20on%20two%20orders%20of%20magnitude%20more%20data%2C%20falls%0Ashort%20of%20its%20less%20compute%20intensive%20counterpart%20DINO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00463v2&entry.124074799=Read"},
{"title": "View-decoupled Transformer for Person Re-identification under\n  Aerial-ground Camera Network", "author": "Quan Zhang and Lei Wang and Vishal M. Patel and Xiaohua Xie and Jianhuang Lai", "abstract": "  Existing person re-identification methods have achieved remarkable advances\nin appearance-based identity association across homogeneous cameras, such as\nground-ground matching. However, as a more practical scenario, aerial-ground\nperson re-identification (AGPReID) among heterogeneous cameras has received\nminimal attention. To alleviate the disruption of discriminative identity\nrepresentation by dramatic view discrepancy as the most significant challenge\nin AGPReID, the view-decoupled transformer (VDT) is proposed as a simple yet\neffective framework. Two major components are designed in VDT to decouple\nview-related and view-unrelated features, namely hierarchical subtractive\nseparation and orthogonal loss, where the former separates these two features\ninside the VDT, and the latter constrains these two to be independent. In\naddition, we contribute a large-scale AGPReID dataset called CARGO, consisting\nof five/eight aerial/ground cameras, 5,000 identities, and 108,563 images.\nExperiments on two datasets show that VDT is a feasible and effective solution\nfor AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on\nCARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational\ncomplexity. Our project is available at https://github.com/LinlyAC/VDT-AGPReID\n", "link": "http://arxiv.org/abs/2403.14513v1", "date": "2024-03-21", "relevancy": 2.7519, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5791}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5322}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20View-decoupled%20Transformer%20for%20Person%20Re-identification%20under%0A%20%20Aerial-ground%20Camera%20Network&body=Title%3A%20View-decoupled%20Transformer%20for%20Person%20Re-identification%20under%0A%20%20Aerial-ground%20Camera%20Network%0AAuthor%3A%20Quan%20Zhang%20and%20Lei%20Wang%20and%20Vishal%20M.%20Patel%20and%20Xiaohua%20Xie%20and%20Jianhuang%20Lai%0AAbstract%3A%20%20%20Existing%20person%20re-identification%20methods%20have%20achieved%20remarkable%20advances%0Ain%20appearance-based%20identity%20association%20across%20homogeneous%20cameras%2C%20such%20as%0Aground-ground%20matching.%20However%2C%20as%20a%20more%20practical%20scenario%2C%20aerial-ground%0Aperson%20re-identification%20%28AGPReID%29%20among%20heterogeneous%20cameras%20has%20received%0Aminimal%20attention.%20To%20alleviate%20the%20disruption%20of%20discriminative%20identity%0Arepresentation%20by%20dramatic%20view%20discrepancy%20as%20the%20most%20significant%20challenge%0Ain%20AGPReID%2C%20the%20view-decoupled%20transformer%20%28VDT%29%20is%20proposed%20as%20a%20simple%20yet%0Aeffective%20framework.%20Two%20major%20components%20are%20designed%20in%20VDT%20to%20decouple%0Aview-related%20and%20view-unrelated%20features%2C%20namely%20hierarchical%20subtractive%0Aseparation%20and%20orthogonal%20loss%2C%20where%20the%20former%20separates%20these%20two%20features%0Ainside%20the%20VDT%2C%20and%20the%20latter%20constrains%20these%20two%20to%20be%20independent.%20In%0Aaddition%2C%20we%20contribute%20a%20large-scale%20AGPReID%20dataset%20called%20CARGO%2C%20consisting%0Aof%20five/eight%20aerial/ground%20cameras%2C%205%2C000%20identities%2C%20and%20108%2C563%20images.%0AExperiments%20on%20two%20datasets%20show%20that%20VDT%20is%20a%20feasible%20and%20effective%20solution%0Afor%20AGPReID%2C%20surpassing%20the%20previous%20method%20on%20mAP/Rank1%20by%20up%20to%205.0%25/2.7%25%20on%0ACARGO%20and%203.7%25/5.2%25%20on%20AG-ReID%2C%20keeping%20the%20same%20magnitude%20of%20computational%0Acomplexity.%20Our%20project%20is%20available%20at%20https%3A//github.com/LinlyAC/VDT-AGPReID%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14513v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=View-decoupled%20Transformer%20for%20Person%20Re-identification%20under%0A%20%20Aerial-ground%20Camera%20Network&entry.906535625=Quan%20Zhang%20and%20Lei%20Wang%20and%20Vishal%20M.%20Patel%20and%20Xiaohua%20Xie%20and%20Jianhuang%20Lai&entry.1292438233=%20%20Existing%20person%20re-identification%20methods%20have%20achieved%20remarkable%20advances%0Ain%20appearance-based%20identity%20association%20across%20homogeneous%20cameras%2C%20such%20as%0Aground-ground%20matching.%20However%2C%20as%20a%20more%20practical%20scenario%2C%20aerial-ground%0Aperson%20re-identification%20%28AGPReID%29%20among%20heterogeneous%20cameras%20has%20received%0Aminimal%20attention.%20To%20alleviate%20the%20disruption%20of%20discriminative%20identity%0Arepresentation%20by%20dramatic%20view%20discrepancy%20as%20the%20most%20significant%20challenge%0Ain%20AGPReID%2C%20the%20view-decoupled%20transformer%20%28VDT%29%20is%20proposed%20as%20a%20simple%20yet%0Aeffective%20framework.%20Two%20major%20components%20are%20designed%20in%20VDT%20to%20decouple%0Aview-related%20and%20view-unrelated%20features%2C%20namely%20hierarchical%20subtractive%0Aseparation%20and%20orthogonal%20loss%2C%20where%20the%20former%20separates%20these%20two%20features%0Ainside%20the%20VDT%2C%20and%20the%20latter%20constrains%20these%20two%20to%20be%20independent.%20In%0Aaddition%2C%20we%20contribute%20a%20large-scale%20AGPReID%20dataset%20called%20CARGO%2C%20consisting%0Aof%20five/eight%20aerial/ground%20cameras%2C%205%2C000%20identities%2C%20and%20108%2C563%20images.%0AExperiments%20on%20two%20datasets%20show%20that%20VDT%20is%20a%20feasible%20and%20effective%20solution%0Afor%20AGPReID%2C%20surpassing%20the%20previous%20method%20on%20mAP/Rank1%20by%20up%20to%205.0%25/2.7%25%20on%0ACARGO%20and%203.7%25/5.2%25%20on%20AG-ReID%2C%20keeping%20the%20same%20magnitude%20of%20computational%0Acomplexity.%20Our%20project%20is%20available%20at%20https%3A//github.com/LinlyAC/VDT-AGPReID%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14513v1&entry.124074799=Read"},
{"title": "Hierarchical Text-to-Vision Self Supervised Alignment for Improved\n  Histopathology Representation Learning", "author": "Hasindri Watawana and Kanchana Ranasinghe and Tariq Mahmood and Muzammal Naseer and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Self-supervised representation learning has been highly promising for\nhistopathology image analysis with numerous approaches leveraging their\npatient-slide-patch hierarchy to learn better representations. In this paper,\nwe explore how the combination of domain specific natural language information\nwith such hierarchical visual representations can benefit rich representation\nlearning for medical image tasks. Building on automated language description\ngeneration for features visible in histopathology images, we present a novel\nlanguage-tied self-supervised learning framework, Hierarchical Language-tied\nSelf-Supervision (HLSS) for histopathology images. We explore contrastive\nobjectives and granular language description based text alignment at multiple\nhierarchies to inject language modality information into the visual\nrepresentations. Our resulting model achieves state-of-the-art performance on\ntwo medical imaging benchmarks, OpenSRH and TCGA datasets. Our framework also\nprovides better interpretability with our language aligned representation\nspace. Code is available at https://github.com/Hasindri/HLSS.\n", "link": "http://arxiv.org/abs/2403.14616v1", "date": "2024-03-21", "relevancy": 2.73, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5815}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5577}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4988}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Text-to-Vision%20Self%20Supervised%20Alignment%20for%20Improved%0A%20%20Histopathology%20Representation%20Learning&body=Title%3A%20Hierarchical%20Text-to-Vision%20Self%20Supervised%20Alignment%20for%20Improved%0A%20%20Histopathology%20Representation%20Learning%0AAuthor%3A%20Hasindri%20Watawana%20and%20Kanchana%20Ranasinghe%20and%20Tariq%20Mahmood%20and%20Muzammal%20Naseer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Self-supervised%20representation%20learning%20has%20been%20highly%20promising%20for%0Ahistopathology%20image%20analysis%20with%20numerous%20approaches%20leveraging%20their%0Apatient-slide-patch%20hierarchy%20to%20learn%20better%20representations.%20In%20this%20paper%2C%0Awe%20explore%20how%20the%20combination%20of%20domain%20specific%20natural%20language%20information%0Awith%20such%20hierarchical%20visual%20representations%20can%20benefit%20rich%20representation%0Alearning%20for%20medical%20image%20tasks.%20Building%20on%20automated%20language%20description%0Ageneration%20for%20features%20visible%20in%20histopathology%20images%2C%20we%20present%20a%20novel%0Alanguage-tied%20self-supervised%20learning%20framework%2C%20Hierarchical%20Language-tied%0ASelf-Supervision%20%28HLSS%29%20for%20histopathology%20images.%20We%20explore%20contrastive%0Aobjectives%20and%20granular%20language%20description%20based%20text%20alignment%20at%20multiple%0Ahierarchies%20to%20inject%20language%20modality%20information%20into%20the%20visual%0Arepresentations.%20Our%20resulting%20model%20achieves%20state-of-the-art%20performance%20on%0Atwo%20medical%20imaging%20benchmarks%2C%20OpenSRH%20and%20TCGA%20datasets.%20Our%20framework%20also%0Aprovides%20better%20interpretability%20with%20our%20language%20aligned%20representation%0Aspace.%20Code%20is%20available%20at%20https%3A//github.com/Hasindri/HLSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14616v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Text-to-Vision%20Self%20Supervised%20Alignment%20for%20Improved%0A%20%20Histopathology%20Representation%20Learning&entry.906535625=Hasindri%20Watawana%20and%20Kanchana%20Ranasinghe%20and%20Tariq%20Mahmood%20and%20Muzammal%20Naseer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Self-supervised%20representation%20learning%20has%20been%20highly%20promising%20for%0Ahistopathology%20image%20analysis%20with%20numerous%20approaches%20leveraging%20their%0Apatient-slide-patch%20hierarchy%20to%20learn%20better%20representations.%20In%20this%20paper%2C%0Awe%20explore%20how%20the%20combination%20of%20domain%20specific%20natural%20language%20information%0Awith%20such%20hierarchical%20visual%20representations%20can%20benefit%20rich%20representation%0Alearning%20for%20medical%20image%20tasks.%20Building%20on%20automated%20language%20description%0Ageneration%20for%20features%20visible%20in%20histopathology%20images%2C%20we%20present%20a%20novel%0Alanguage-tied%20self-supervised%20learning%20framework%2C%20Hierarchical%20Language-tied%0ASelf-Supervision%20%28HLSS%29%20for%20histopathology%20images.%20We%20explore%20contrastive%0Aobjectives%20and%20granular%20language%20description%20based%20text%20alignment%20at%20multiple%0Ahierarchies%20to%20inject%20language%20modality%20information%20into%20the%20visual%0Arepresentations.%20Our%20resulting%20model%20achieves%20state-of-the-art%20performance%20on%0Atwo%20medical%20imaging%20benchmarks%2C%20OpenSRH%20and%20TCGA%20datasets.%20Our%20framework%20also%0Aprovides%20better%20interpretability%20with%20our%20language%20aligned%20representation%0Aspace.%20Code%20is%20available%20at%20https%3A//github.com/Hasindri/HLSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14616v1&entry.124074799=Read"},
{"title": "LDTR: Transformer-based Lane Detection with Anchor-chain Representation", "author": "Zhongyu Yang and Chen Shen and Wei Shao and Tengfei Xing and Runbo Hu and Pengfei Xu and Hua Chai and Ruini Xue", "abstract": "  Despite recent advances in lane detection methods, scenarios with limited- or\nno-visual-clue of lanes due to factors such as lighting conditions and\nocclusion remain challenging and crucial for automated driving. Moreover,\ncurrent lane representations require complex post-processing and struggle with\nspecific instances. Inspired by the DETR architecture, we propose LDTR, a\ntransformer-based model to address these issues. Lanes are modeled with a novel\nanchor-chain, regarding a lane as a whole from the beginning, which enables\nLDTR to handle special lanes inherently. To enhance lane instance perception,\nLDTR incorporates a novel multi-referenced deformable attention module to\ndistribute attention around the object. Additionally, LDTR incorporates two\nline IoU algorithms to improve convergence efficiency and employs a Gaussian\nheatmap auxiliary branch to enhance model representation capability during\ntraining. To evaluate lane detection models, we rely on Frechet distance,\nparameterized F1-score, and additional synthetic metrics. Experimental results\ndemonstrate that LDTR achieves state-of-the-art performance on well-known\ndatasets.\n", "link": "http://arxiv.org/abs/2403.14354v1", "date": "2024-03-21", "relevancy": 2.7283, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5406}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5291}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LDTR%3A%20Transformer-based%20Lane%20Detection%20with%20Anchor-chain%20Representation&body=Title%3A%20LDTR%3A%20Transformer-based%20Lane%20Detection%20with%20Anchor-chain%20Representation%0AAuthor%3A%20Zhongyu%20Yang%20and%20Chen%20Shen%20and%20Wei%20Shao%20and%20Tengfei%20Xing%20and%20Runbo%20Hu%20and%20Pengfei%20Xu%20and%20Hua%20Chai%20and%20Ruini%20Xue%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20lane%20detection%20methods%2C%20scenarios%20with%20limited-%20or%0Ano-visual-clue%20of%20lanes%20due%20to%20factors%20such%20as%20lighting%20conditions%20and%0Aocclusion%20remain%20challenging%20and%20crucial%20for%20automated%20driving.%20Moreover%2C%0Acurrent%20lane%20representations%20require%20complex%20post-processing%20and%20struggle%20with%0Aspecific%20instances.%20Inspired%20by%20the%20DETR%20architecture%2C%20we%20propose%20LDTR%2C%20a%0Atransformer-based%20model%20to%20address%20these%20issues.%20Lanes%20are%20modeled%20with%20a%20novel%0Aanchor-chain%2C%20regarding%20a%20lane%20as%20a%20whole%20from%20the%20beginning%2C%20which%20enables%0ALDTR%20to%20handle%20special%20lanes%20inherently.%20To%20enhance%20lane%20instance%20perception%2C%0ALDTR%20incorporates%20a%20novel%20multi-referenced%20deformable%20attention%20module%20to%0Adistribute%20attention%20around%20the%20object.%20Additionally%2C%20LDTR%20incorporates%20two%0Aline%20IoU%20algorithms%20to%20improve%20convergence%20efficiency%20and%20employs%20a%20Gaussian%0Aheatmap%20auxiliary%20branch%20to%20enhance%20model%20representation%20capability%20during%0Atraining.%20To%20evaluate%20lane%20detection%20models%2C%20we%20rely%20on%20Frechet%20distance%2C%0Aparameterized%20F1-score%2C%20and%20additional%20synthetic%20metrics.%20Experimental%20results%0Ademonstrate%20that%20LDTR%20achieves%20state-of-the-art%20performance%20on%20well-known%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14354v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LDTR%3A%20Transformer-based%20Lane%20Detection%20with%20Anchor-chain%20Representation&entry.906535625=Zhongyu%20Yang%20and%20Chen%20Shen%20and%20Wei%20Shao%20and%20Tengfei%20Xing%20and%20Runbo%20Hu%20and%20Pengfei%20Xu%20and%20Hua%20Chai%20and%20Ruini%20Xue&entry.1292438233=%20%20Despite%20recent%20advances%20in%20lane%20detection%20methods%2C%20scenarios%20with%20limited-%20or%0Ano-visual-clue%20of%20lanes%20due%20to%20factors%20such%20as%20lighting%20conditions%20and%0Aocclusion%20remain%20challenging%20and%20crucial%20for%20automated%20driving.%20Moreover%2C%0Acurrent%20lane%20representations%20require%20complex%20post-processing%20and%20struggle%20with%0Aspecific%20instances.%20Inspired%20by%20the%20DETR%20architecture%2C%20we%20propose%20LDTR%2C%20a%0Atransformer-based%20model%20to%20address%20these%20issues.%20Lanes%20are%20modeled%20with%20a%20novel%0Aanchor-chain%2C%20regarding%20a%20lane%20as%20a%20whole%20from%20the%20beginning%2C%20which%20enables%0ALDTR%20to%20handle%20special%20lanes%20inherently.%20To%20enhance%20lane%20instance%20perception%2C%0ALDTR%20incorporates%20a%20novel%20multi-referenced%20deformable%20attention%20module%20to%0Adistribute%20attention%20around%20the%20object.%20Additionally%2C%20LDTR%20incorporates%20two%0Aline%20IoU%20algorithms%20to%20improve%20convergence%20efficiency%20and%20employs%20a%20Gaussian%0Aheatmap%20auxiliary%20branch%20to%20enhance%20model%20representation%20capability%20during%0Atraining.%20To%20evaluate%20lane%20detection%20models%2C%20we%20rely%20on%20Frechet%20distance%2C%0Aparameterized%20F1-score%2C%20and%20additional%20synthetic%20metrics.%20Experimental%20results%0Ademonstrate%20that%20LDTR%20achieves%20state-of-the-art%20performance%20on%20well-known%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14354v1&entry.124074799=Read"},
{"title": "Point2RBox: Combine Knowledge from Synthetic Visual Patterns for\n  End-to-end Oriented Object Detection with Single Point Supervision", "author": "Yi Yu and Xue Yang and Qingyun Li and Feipeng Da and Jifeng Dai and Yu Qiao and Junchi Yan", "abstract": "  With the rapidly increasing demand for oriented object detection (OOD),\nrecent research involving weakly-supervised detectors for learning rotated box\n(RBox) from the horizontal box (HBox) has attracted more and more attention. In\nthis paper, we explore a more challenging yet label-efficient setting, namely\nsingle point-supervised OOD, and present our approach called Point2RBox.\nSpecifically, we propose to leverage two principles: 1) Synthetic pattern\nknowledge combination: By sampling around each labeled point on the image, we\nspread the object feature to synthetic visual patterns with known boxes to\nprovide the knowledge for box regression. 2) Transform self-supervision: With a\ntransformed input image (e.g. scaled/rotated), the output RBoxes are trained to\nfollow the same transformation so that the network can perceive the relative\nsize/rotation between objects. The detector is further enhanced by a few\ndevised techniques to cope with peripheral issues, e.g. the anchor/layer\nassignment as the size of the object is not available in our point supervision\nsetting. To our best knowledge, Point2RBox is the first end-to-end solution for\npoint-supervised OOD. In particular, our method uses a lightweight paradigm,\nyet it achieves a competitive performance among point-supervised alternatives,\n41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.\n", "link": "http://arxiv.org/abs/2311.14758v2", "date": "2024-03-21", "relevancy": 2.7217, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5807}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5491}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5032}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Point2RBox%3A%20Combine%20Knowledge%20from%20Synthetic%20Visual%20Patterns%20for%0A%20%20End-to-end%20Oriented%20Object%20Detection%20with%20Single%20Point%20Supervision&body=Title%3A%20Point2RBox%3A%20Combine%20Knowledge%20from%20Synthetic%20Visual%20Patterns%20for%0A%20%20End-to-end%20Oriented%20Object%20Detection%20with%20Single%20Point%20Supervision%0AAuthor%3A%20Yi%20Yu%20and%20Xue%20Yang%20and%20Qingyun%20Li%20and%20Feipeng%20Da%20and%20Jifeng%20Dai%20and%20Yu%20Qiao%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20With%20the%20rapidly%20increasing%20demand%20for%20oriented%20object%20detection%20%28OOD%29%2C%0Arecent%20research%20involving%20weakly-supervised%20detectors%20for%20learning%20rotated%20box%0A%28RBox%29%20from%20the%20horizontal%20box%20%28HBox%29%20has%20attracted%20more%20and%20more%20attention.%20In%0Athis%20paper%2C%20we%20explore%20a%20more%20challenging%20yet%20label-efficient%20setting%2C%20namely%0Asingle%20point-supervised%20OOD%2C%20and%20present%20our%20approach%20called%20Point2RBox.%0ASpecifically%2C%20we%20propose%20to%20leverage%20two%20principles%3A%201%29%20Synthetic%20pattern%0Aknowledge%20combination%3A%20By%20sampling%20around%20each%20labeled%20point%20on%20the%20image%2C%20we%0Aspread%20the%20object%20feature%20to%20synthetic%20visual%20patterns%20with%20known%20boxes%20to%0Aprovide%20the%20knowledge%20for%20box%20regression.%202%29%20Transform%20self-supervision%3A%20With%20a%0Atransformed%20input%20image%20%28e.g.%20scaled/rotated%29%2C%20the%20output%20RBoxes%20are%20trained%20to%0Afollow%20the%20same%20transformation%20so%20that%20the%20network%20can%20perceive%20the%20relative%0Asize/rotation%20between%20objects.%20The%20detector%20is%20further%20enhanced%20by%20a%20few%0Adevised%20techniques%20to%20cope%20with%20peripheral%20issues%2C%20e.g.%20the%20anchor/layer%0Aassignment%20as%20the%20size%20of%20the%20object%20is%20not%20available%20in%20our%20point%20supervision%0Asetting.%20To%20our%20best%20knowledge%2C%20Point2RBox%20is%20the%20first%20end-to-end%20solution%20for%0Apoint-supervised%20OOD.%20In%20particular%2C%20our%20method%20uses%20a%20lightweight%20paradigm%2C%0Ayet%20it%20achieves%20a%20competitive%20performance%20among%20point-supervised%20alternatives%2C%0A41.05%25/27.62%25/80.01%25%20on%20DOTA/DIOR/HRSC%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14758v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point2RBox%3A%20Combine%20Knowledge%20from%20Synthetic%20Visual%20Patterns%20for%0A%20%20End-to-end%20Oriented%20Object%20Detection%20with%20Single%20Point%20Supervision&entry.906535625=Yi%20Yu%20and%20Xue%20Yang%20and%20Qingyun%20Li%20and%20Feipeng%20Da%20and%20Jifeng%20Dai%20and%20Yu%20Qiao%20and%20Junchi%20Yan&entry.1292438233=%20%20With%20the%20rapidly%20increasing%20demand%20for%20oriented%20object%20detection%20%28OOD%29%2C%0Arecent%20research%20involving%20weakly-supervised%20detectors%20for%20learning%20rotated%20box%0A%28RBox%29%20from%20the%20horizontal%20box%20%28HBox%29%20has%20attracted%20more%20and%20more%20attention.%20In%0Athis%20paper%2C%20we%20explore%20a%20more%20challenging%20yet%20label-efficient%20setting%2C%20namely%0Asingle%20point-supervised%20OOD%2C%20and%20present%20our%20approach%20called%20Point2RBox.%0ASpecifically%2C%20we%20propose%20to%20leverage%20two%20principles%3A%201%29%20Synthetic%20pattern%0Aknowledge%20combination%3A%20By%20sampling%20around%20each%20labeled%20point%20on%20the%20image%2C%20we%0Aspread%20the%20object%20feature%20to%20synthetic%20visual%20patterns%20with%20known%20boxes%20to%0Aprovide%20the%20knowledge%20for%20box%20regression.%202%29%20Transform%20self-supervision%3A%20With%20a%0Atransformed%20input%20image%20%28e.g.%20scaled/rotated%29%2C%20the%20output%20RBoxes%20are%20trained%20to%0Afollow%20the%20same%20transformation%20so%20that%20the%20network%20can%20perceive%20the%20relative%0Asize/rotation%20between%20objects.%20The%20detector%20is%20further%20enhanced%20by%20a%20few%0Adevised%20techniques%20to%20cope%20with%20peripheral%20issues%2C%20e.g.%20the%20anchor/layer%0Aassignment%20as%20the%20size%20of%20the%20object%20is%20not%20available%20in%20our%20point%20supervision%0Asetting.%20To%20our%20best%20knowledge%2C%20Point2RBox%20is%20the%20first%20end-to-end%20solution%20for%0Apoint-supervised%20OOD.%20In%20particular%2C%20our%20method%20uses%20a%20lightweight%20paradigm%2C%0Ayet%20it%20achieves%20a%20competitive%20performance%20among%20point-supervised%20alternatives%2C%0A41.05%25/27.62%25/80.01%25%20on%20DOTA/DIOR/HRSC%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14758v2&entry.124074799=Read"},
{"title": "DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single\n  Video", "author": "Narek Tumanyan and Assaf Singer and Shai Bagon and Tali Dekel", "abstract": "  We present DINO-Tracker -- a new framework for long-term dense tracking in\nvideo. The pillar of our approach is combining test-time training on a single\nvideo, with the powerful localized semantic features learned by a pre-trained\nDINO-ViT model. Specifically, our framework simultaneously adopts DINO's\nfeatures to fit to the motion observations of the test video, while training a\ntracker that directly leverages the refined features. The entire framework is\ntrained end-to-end using a combination of self-supervised losses, and\nregularization that allows us to retain and benefit from DINO's semantic prior.\nExtensive evaluation demonstrates that our method achieves state-of-the-art\nresults on known benchmarks. DINO-tracker significantly outperforms\nself-supervised methods and is competitive with state-of-the-art supervised\ntrackers, while outperforming them in challenging cases of tracking under\nlong-term occlusions.\n", "link": "http://arxiv.org/abs/2403.14548v1", "date": "2024-03-21", "relevancy": 2.7156, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5781}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5453}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DINO-Tracker%3A%20Taming%20DINO%20for%20Self-Supervised%20Point%20Tracking%20in%20a%20Single%0A%20%20Video&body=Title%3A%20DINO-Tracker%3A%20Taming%20DINO%20for%20Self-Supervised%20Point%20Tracking%20in%20a%20Single%0A%20%20Video%0AAuthor%3A%20Narek%20Tumanyan%20and%20Assaf%20Singer%20and%20Shai%20Bagon%20and%20Tali%20Dekel%0AAbstract%3A%20%20%20We%20present%20DINO-Tracker%20--%20a%20new%20framework%20for%20long-term%20dense%20tracking%20in%0Avideo.%20The%20pillar%20of%20our%20approach%20is%20combining%20test-time%20training%20on%20a%20single%0Avideo%2C%20with%20the%20powerful%20localized%20semantic%20features%20learned%20by%20a%20pre-trained%0ADINO-ViT%20model.%20Specifically%2C%20our%20framework%20simultaneously%20adopts%20DINO%27s%0Afeatures%20to%20fit%20to%20the%20motion%20observations%20of%20the%20test%20video%2C%20while%20training%20a%0Atracker%20that%20directly%20leverages%20the%20refined%20features.%20The%20entire%20framework%20is%0Atrained%20end-to-end%20using%20a%20combination%20of%20self-supervised%20losses%2C%20and%0Aregularization%20that%20allows%20us%20to%20retain%20and%20benefit%20from%20DINO%27s%20semantic%20prior.%0AExtensive%20evaluation%20demonstrates%20that%20our%20method%20achieves%20state-of-the-art%0Aresults%20on%20known%20benchmarks.%20DINO-tracker%20significantly%20outperforms%0Aself-supervised%20methods%20and%20is%20competitive%20with%20state-of-the-art%20supervised%0Atrackers%2C%20while%20outperforming%20them%20in%20challenging%20cases%20of%20tracking%20under%0Along-term%20occlusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14548v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-Tracker%3A%20Taming%20DINO%20for%20Self-Supervised%20Point%20Tracking%20in%20a%20Single%0A%20%20Video&entry.906535625=Narek%20Tumanyan%20and%20Assaf%20Singer%20and%20Shai%20Bagon%20and%20Tali%20Dekel&entry.1292438233=%20%20We%20present%20DINO-Tracker%20--%20a%20new%20framework%20for%20long-term%20dense%20tracking%20in%0Avideo.%20The%20pillar%20of%20our%20approach%20is%20combining%20test-time%20training%20on%20a%20single%0Avideo%2C%20with%20the%20powerful%20localized%20semantic%20features%20learned%20by%20a%20pre-trained%0ADINO-ViT%20model.%20Specifically%2C%20our%20framework%20simultaneously%20adopts%20DINO%27s%0Afeatures%20to%20fit%20to%20the%20motion%20observations%20of%20the%20test%20video%2C%20while%20training%20a%0Atracker%20that%20directly%20leverages%20the%20refined%20features.%20The%20entire%20framework%20is%0Atrained%20end-to-end%20using%20a%20combination%20of%20self-supervised%20losses%2C%20and%0Aregularization%20that%20allows%20us%20to%20retain%20and%20benefit%20from%20DINO%27s%20semantic%20prior.%0AExtensive%20evaluation%20demonstrates%20that%20our%20method%20achieves%20state-of-the-art%0Aresults%20on%20known%20benchmarks.%20DINO-tracker%20significantly%20outperforms%0Aself-supervised%20methods%20and%20is%20competitive%20with%20state-of-the-art%20supervised%0Atrackers%2C%20while%20outperforming%20them%20in%20challenging%20cases%20of%20tracking%20under%0Along-term%20occlusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14548v1&entry.124074799=Read"},
{"title": "ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D\n  Decomposition", "author": "Tianhao Wu and Chuanxia Zheng and Tat-Jen Cham and Qianyi Wu", "abstract": "  3D decomposition/segmentation still remains a challenge as large-scale 3D\nannotated data is not readily available. Contemporary approaches typically\nleverage 2D machine-generated segments, integrating them for 3D consistency.\nWhile the majority of these methods are based on NeRFs, they face a potential\nweakness that the instance/semantic embedding features derive from independent\nMLPs, thus preventing the segmentation network from learning the geometric\ndetails of the objects directly through radiance and density. In this paper, we\npropose ClusteringSDF, a novel approach to achieve both segmentation and\nreconstruction in 3D via the neural implicit surface representation,\nspecifically Signal Distance Function (SDF), where the segmentation rendering\nis directly integrated with the volume rendering of neural implicit surfaces.\nAlthough based on ObjectSDF++, ClusteringSDF no longer requires the\nground-truth segments for supervision while maintaining the capability of\nreconstructing individual object surfaces, but purely with the noisy and\ninconsistent labels from pre-trained models.As the core of ClusteringSDF, we\nintroduce a high-efficient clustering mechanism for lifting the 2D labels to 3D\nand the experimental results on the challenging scenes from ScanNet and Replica\ndatasets show that ClusteringSDF can achieve competitive performance compared\nagainst the state-of-the-art with significantly reduced training time.\n", "link": "http://arxiv.org/abs/2403.14619v1", "date": "2024-03-21", "relevancy": 2.6981, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5366}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5268}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ClusteringSDF%3A%20Self-Organized%20Neural%20Implicit%20Surfaces%20for%203D%0A%20%20Decomposition&body=Title%3A%20ClusteringSDF%3A%20Self-Organized%20Neural%20Implicit%20Surfaces%20for%203D%0A%20%20Decomposition%0AAuthor%3A%20Tianhao%20Wu%20and%20Chuanxia%20Zheng%20and%20Tat-Jen%20Cham%20and%20Qianyi%20Wu%0AAbstract%3A%20%20%203D%20decomposition/segmentation%20still%20remains%20a%20challenge%20as%20large-scale%203D%0Aannotated%20data%20is%20not%20readily%20available.%20Contemporary%20approaches%20typically%0Aleverage%202D%20machine-generated%20segments%2C%20integrating%20them%20for%203D%20consistency.%0AWhile%20the%20majority%20of%20these%20methods%20are%20based%20on%20NeRFs%2C%20they%20face%20a%20potential%0Aweakness%20that%20the%20instance/semantic%20embedding%20features%20derive%20from%20independent%0AMLPs%2C%20thus%20preventing%20the%20segmentation%20network%20from%20learning%20the%20geometric%0Adetails%20of%20the%20objects%20directly%20through%20radiance%20and%20density.%20In%20this%20paper%2C%20we%0Apropose%20ClusteringSDF%2C%20a%20novel%20approach%20to%20achieve%20both%20segmentation%20and%0Areconstruction%20in%203D%20via%20the%20neural%20implicit%20surface%20representation%2C%0Aspecifically%20Signal%20Distance%20Function%20%28SDF%29%2C%20where%20the%20segmentation%20rendering%0Ais%20directly%20integrated%20with%20the%20volume%20rendering%20of%20neural%20implicit%20surfaces.%0AAlthough%20based%20on%20ObjectSDF%2B%2B%2C%20ClusteringSDF%20no%20longer%20requires%20the%0Aground-truth%20segments%20for%20supervision%20while%20maintaining%20the%20capability%20of%0Areconstructing%20individual%20object%20surfaces%2C%20but%20purely%20with%20the%20noisy%20and%0Ainconsistent%20labels%20from%20pre-trained%20models.As%20the%20core%20of%20ClusteringSDF%2C%20we%0Aintroduce%20a%20high-efficient%20clustering%20mechanism%20for%20lifting%20the%202D%20labels%20to%203D%0Aand%20the%20experimental%20results%20on%20the%20challenging%20scenes%20from%20ScanNet%20and%20Replica%0Adatasets%20show%20that%20ClusteringSDF%20can%20achieve%20competitive%20performance%20compared%0Aagainst%20the%20state-of-the-art%20with%20significantly%20reduced%20training%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14619v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClusteringSDF%3A%20Self-Organized%20Neural%20Implicit%20Surfaces%20for%203D%0A%20%20Decomposition&entry.906535625=Tianhao%20Wu%20and%20Chuanxia%20Zheng%20and%20Tat-Jen%20Cham%20and%20Qianyi%20Wu&entry.1292438233=%20%203D%20decomposition/segmentation%20still%20remains%20a%20challenge%20as%20large-scale%203D%0Aannotated%20data%20is%20not%20readily%20available.%20Contemporary%20approaches%20typically%0Aleverage%202D%20machine-generated%20segments%2C%20integrating%20them%20for%203D%20consistency.%0AWhile%20the%20majority%20of%20these%20methods%20are%20based%20on%20NeRFs%2C%20they%20face%20a%20potential%0Aweakness%20that%20the%20instance/semantic%20embedding%20features%20derive%20from%20independent%0AMLPs%2C%20thus%20preventing%20the%20segmentation%20network%20from%20learning%20the%20geometric%0Adetails%20of%20the%20objects%20directly%20through%20radiance%20and%20density.%20In%20this%20paper%2C%20we%0Apropose%20ClusteringSDF%2C%20a%20novel%20approach%20to%20achieve%20both%20segmentation%20and%0Areconstruction%20in%203D%20via%20the%20neural%20implicit%20surface%20representation%2C%0Aspecifically%20Signal%20Distance%20Function%20%28SDF%29%2C%20where%20the%20segmentation%20rendering%0Ais%20directly%20integrated%20with%20the%20volume%20rendering%20of%20neural%20implicit%20surfaces.%0AAlthough%20based%20on%20ObjectSDF%2B%2B%2C%20ClusteringSDF%20no%20longer%20requires%20the%0Aground-truth%20segments%20for%20supervision%20while%20maintaining%20the%20capability%20of%0Areconstructing%20individual%20object%20surfaces%2C%20but%20purely%20with%20the%20noisy%20and%0Ainconsistent%20labels%20from%20pre-trained%20models.As%20the%20core%20of%20ClusteringSDF%2C%20we%0Aintroduce%20a%20high-efficient%20clustering%20mechanism%20for%20lifting%20the%202D%20labels%20to%203D%0Aand%20the%20experimental%20results%20on%20the%20challenging%20scenes%20from%20ScanNet%20and%20Replica%0Adatasets%20show%20that%20ClusteringSDF%20can%20achieve%20competitive%20performance%20compared%0Aagainst%20the%20state-of-the-art%20with%20significantly%20reduced%20training%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14619v1&entry.124074799=Read"},
{"title": "A Lightweight Attention-based Deep Network via Multi-Scale Feature\n  Fusion for Multi-View Facial Expression Recognition", "author": "Ali Ezati and Mohammadreza Dezyani and Rajib Rana and Roozbeh Rajabi and Ahmad Ayatollahi", "abstract": "  Convolutional neural networks (CNNs) and their variations have shown\neffectiveness in facial expression recognition (FER). However, they face\nchallenges when dealing with high computational complexity and multi-view head\nposes in real-world scenarios. We introduce a lightweight attentional network\nincorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For\nthe first challenge, we have carefully designed a lightweight fully\nconvolutional network (FCN). We address the second challenge by presenting two\nnovel components, namely mass attention (MassAtt) and point wise feature\nselection (PWFS) blocks. The MassAtt block simultaneously generates channel and\nspatial attention maps to recalibrate feature maps by emphasizing important\nfeatures while suppressing irrelevant ones. On the other hand, the PWFS block\nemploys a feature selection mechanism that discards less meaningful features\nprior to the fusion process. This mechanism distinguishes it from previous\nmethods that directly fuse multi-scale features. Our proposed approach achieved\nresults comparable to state-of-the-art methods in terms of parameter counts and\nrobustness to pose variation, with accuracy rates of 90.77% on KDEF, 70.44% on\nFER-2013, and 86.96% on FERPlus datasets. The code for LANMSFF is available at\nhttps://github.com/AE-1129/LANMSFF.\n", "link": "http://arxiv.org/abs/2403.14318v1", "date": "2024-03-21", "relevancy": 2.6615, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5396}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5329}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5244}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Attention-based%20Deep%20Network%20via%20Multi-Scale%20Feature%0A%20%20Fusion%20for%20Multi-View%20Facial%20Expression%20Recognition&body=Title%3A%20A%20Lightweight%20Attention-based%20Deep%20Network%20via%20Multi-Scale%20Feature%0A%20%20Fusion%20for%20Multi-View%20Facial%20Expression%20Recognition%0AAuthor%3A%20Ali%20Ezati%20and%20Mohammadreza%20Dezyani%20and%20Rajib%20Rana%20and%20Roozbeh%20Rajabi%20and%20Ahmad%20Ayatollahi%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20and%20their%20variations%20have%20shown%0Aeffectiveness%20in%20facial%20expression%20recognition%20%28FER%29.%20However%2C%20they%20face%0Achallenges%20when%20dealing%20with%20high%20computational%20complexity%20and%20multi-view%20head%0Aposes%20in%20real-world%20scenarios.%20We%20introduce%20a%20lightweight%20attentional%20network%0Aincorporating%20multi-scale%20feature%20fusion%20%28LANMSFF%29%20to%20tackle%20these%20issues.%20For%0Athe%20first%20challenge%2C%20we%20have%20carefully%20designed%20a%20lightweight%20fully%0Aconvolutional%20network%20%28FCN%29.%20We%20address%20the%20second%20challenge%20by%20presenting%20two%0Anovel%20components%2C%20namely%20mass%20attention%20%28MassAtt%29%20and%20point%20wise%20feature%0Aselection%20%28PWFS%29%20blocks.%20The%20MassAtt%20block%20simultaneously%20generates%20channel%20and%0Aspatial%20attention%20maps%20to%20recalibrate%20feature%20maps%20by%20emphasizing%20important%0Afeatures%20while%20suppressing%20irrelevant%20ones.%20On%20the%20other%20hand%2C%20the%20PWFS%20block%0Aemploys%20a%20feature%20selection%20mechanism%20that%20discards%20less%20meaningful%20features%0Aprior%20to%20the%20fusion%20process.%20This%20mechanism%20distinguishes%20it%20from%20previous%0Amethods%20that%20directly%20fuse%20multi-scale%20features.%20Our%20proposed%20approach%20achieved%0Aresults%20comparable%20to%20state-of-the-art%20methods%20in%20terms%20of%20parameter%20counts%20and%0Arobustness%20to%20pose%20variation%2C%20with%20accuracy%20rates%20of%2090.77%25%20on%20KDEF%2C%2070.44%25%20on%0AFER-2013%2C%20and%2086.96%25%20on%20FERPlus%20datasets.%20The%20code%20for%20LANMSFF%20is%20available%20at%0Ahttps%3A//github.com/AE-1129/LANMSFF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14318v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Attention-based%20Deep%20Network%20via%20Multi-Scale%20Feature%0A%20%20Fusion%20for%20Multi-View%20Facial%20Expression%20Recognition&entry.906535625=Ali%20Ezati%20and%20Mohammadreza%20Dezyani%20and%20Rajib%20Rana%20and%20Roozbeh%20Rajabi%20and%20Ahmad%20Ayatollahi&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20and%20their%20variations%20have%20shown%0Aeffectiveness%20in%20facial%20expression%20recognition%20%28FER%29.%20However%2C%20they%20face%0Achallenges%20when%20dealing%20with%20high%20computational%20complexity%20and%20multi-view%20head%0Aposes%20in%20real-world%20scenarios.%20We%20introduce%20a%20lightweight%20attentional%20network%0Aincorporating%20multi-scale%20feature%20fusion%20%28LANMSFF%29%20to%20tackle%20these%20issues.%20For%0Athe%20first%20challenge%2C%20we%20have%20carefully%20designed%20a%20lightweight%20fully%0Aconvolutional%20network%20%28FCN%29.%20We%20address%20the%20second%20challenge%20by%20presenting%20two%0Anovel%20components%2C%20namely%20mass%20attention%20%28MassAtt%29%20and%20point%20wise%20feature%0Aselection%20%28PWFS%29%20blocks.%20The%20MassAtt%20block%20simultaneously%20generates%20channel%20and%0Aspatial%20attention%20maps%20to%20recalibrate%20feature%20maps%20by%20emphasizing%20important%0Afeatures%20while%20suppressing%20irrelevant%20ones.%20On%20the%20other%20hand%2C%20the%20PWFS%20block%0Aemploys%20a%20feature%20selection%20mechanism%20that%20discards%20less%20meaningful%20features%0Aprior%20to%20the%20fusion%20process.%20This%20mechanism%20distinguishes%20it%20from%20previous%0Amethods%20that%20directly%20fuse%20multi-scale%20features.%20Our%20proposed%20approach%20achieved%0Aresults%20comparable%20to%20state-of-the-art%20methods%20in%20terms%20of%20parameter%20counts%20and%0Arobustness%20to%20pose%20variation%2C%20with%20accuracy%20rates%20of%2090.77%25%20on%20KDEF%2C%2070.44%25%20on%0AFER-2013%2C%20and%2086.96%25%20on%20FERPlus%20datasets.%20The%20code%20for%20LANMSFF%20is%20available%20at%0Ahttps%3A//github.com/AE-1129/LANMSFF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14318v1&entry.124074799=Read"},
{"title": "BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image", "author": "Minje Kim and Tae-Kyun Kim", "abstract": "  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1) bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3) the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n", "link": "http://arxiv.org/abs/2403.08262v2", "date": "2024-03-21", "relevancy": 2.6521, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5285}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5105}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BiTT%3A%20Bi-directional%20Texture%20Reconstruction%20of%20Interacting%20Two%20Hands%0A%20%20from%20a%20Single%20Image&body=Title%3A%20BiTT%3A%20Bi-directional%20Texture%20Reconstruction%20of%20Interacting%20Two%20Hands%0A%20%20from%20a%20Single%20Image%0AAuthor%3A%20Minje%20Kim%20and%20Tae-Kyun%20Kim%0AAbstract%3A%20%20%20Creating%20personalized%20hand%20avatars%20is%20important%20to%20offer%20a%20realistic%0Aexperience%20to%20users%20on%20AR%20/%20VR%20platforms.%20While%20most%20prior%20studies%20focused%20on%0Areconstructing%203D%20hand%20shapes%2C%20some%20recent%20work%20has%20tackled%20the%20reconstruction%0Aof%20hand%20textures%20on%20top%20of%20shapes.%20However%2C%20these%20methods%20are%20often%20limited%20to%0Acapturing%20pixels%20on%20the%20visible%20side%20of%20a%20hand%2C%20requiring%20diverse%20views%20of%20the%0Ahand%20in%20a%20video%20or%20multiple%20images%20as%20input.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Amethod%2C%20BiTT%28Bi-directional%20Texture%20reconstruction%20of%20Two%20hands%29%2C%20which%20is%20the%0Afirst%20end-to-end%20trainable%20method%20for%20relightable%2C%20pose-free%20texture%0Areconstruction%20of%20two%20interacting%20hands%20taking%20only%20a%20single%20RGB%20image%2C%20by%0Athree%20novel%20components%3A%201%29%20bi-directional%20%28left%20%24%5Cleftrightarrow%24%20right%29%0Atexture%20reconstruction%20using%20the%20texture%20symmetry%20of%20left%20/%20right%20hands%2C%202%29%0Autilizing%20a%20texture%20parametric%20model%20for%20hand%20texture%20recovery%2C%20and%203%29%20the%0Aoverall%20coarse-to-fine%20stage%20pipeline%20for%20reconstructing%20personalized%20texture%0Aof%20two%20interacting%20hands.%20BiTT%20first%20estimates%20the%20scene%20light%20condition%20and%0Aalbedo%20image%20from%20an%20input%20image%2C%20then%20reconstructs%20the%20texture%20of%20both%20hands%0Athrough%20the%20texture%20parametric%20model%20and%20bi-directional%20texture%20reconstructor.%0AIn%20experiments%20using%20InterHand2.6M%20and%20RGB2Hands%20datasets%2C%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20hand%20texture%20reconstruction%20methods%0Aquantitatively%20and%20qualitatively.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yunminjin2/BiTT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08262v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiTT%3A%20Bi-directional%20Texture%20Reconstruction%20of%20Interacting%20Two%20Hands%0A%20%20from%20a%20Single%20Image&entry.906535625=Minje%20Kim%20and%20Tae-Kyun%20Kim&entry.1292438233=%20%20Creating%20personalized%20hand%20avatars%20is%20important%20to%20offer%20a%20realistic%0Aexperience%20to%20users%20on%20AR%20/%20VR%20platforms.%20While%20most%20prior%20studies%20focused%20on%0Areconstructing%203D%20hand%20shapes%2C%20some%20recent%20work%20has%20tackled%20the%20reconstruction%0Aof%20hand%20textures%20on%20top%20of%20shapes.%20However%2C%20these%20methods%20are%20often%20limited%20to%0Acapturing%20pixels%20on%20the%20visible%20side%20of%20a%20hand%2C%20requiring%20diverse%20views%20of%20the%0Ahand%20in%20a%20video%20or%20multiple%20images%20as%20input.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Amethod%2C%20BiTT%28Bi-directional%20Texture%20reconstruction%20of%20Two%20hands%29%2C%20which%20is%20the%0Afirst%20end-to-end%20trainable%20method%20for%20relightable%2C%20pose-free%20texture%0Areconstruction%20of%20two%20interacting%20hands%20taking%20only%20a%20single%20RGB%20image%2C%20by%0Athree%20novel%20components%3A%201%29%20bi-directional%20%28left%20%24%5Cleftrightarrow%24%20right%29%0Atexture%20reconstruction%20using%20the%20texture%20symmetry%20of%20left%20/%20right%20hands%2C%202%29%0Autilizing%20a%20texture%20parametric%20model%20for%20hand%20texture%20recovery%2C%20and%203%29%20the%0Aoverall%20coarse-to-fine%20stage%20pipeline%20for%20reconstructing%20personalized%20texture%0Aof%20two%20interacting%20hands.%20BiTT%20first%20estimates%20the%20scene%20light%20condition%20and%0Aalbedo%20image%20from%20an%20input%20image%2C%20then%20reconstructs%20the%20texture%20of%20both%20hands%0Athrough%20the%20texture%20parametric%20model%20and%20bi-directional%20texture%20reconstructor.%0AIn%20experiments%20using%20InterHand2.6M%20and%20RGB2Hands%20datasets%2C%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20hand%20texture%20reconstruction%20methods%0Aquantitatively%20and%20qualitatively.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/yunminjin2/BiTT%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08262v2&entry.124074799=Read"},
{"title": "OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation", "author": "Bohao Peng and Xiaoyang Wu and Li Jiang and Yukang Chen and Hengshuang Zhao and Zhuotao Tian and Jiaya Jia", "abstract": "  The booming of 3D recognition in the 2020s began with the introduction of\npoint cloud transformers. They quickly overwhelmed sparse CNNs and became\nstate-of-the-art models, especially in 3D semantic segmentation. However,\nsparse CNNs are still valuable networks, due to their efficiency treasure, and\nease of application. In this work, we reexamine the design distinctions and\ntest the limits of what a sparse CNN can achieve. We discover that the key\ncredit to the performance difference is adaptivity. Specifically, we propose\ntwo key components, i.e., adaptive receptive fields (spatially) and adaptive\nrelation, to bridge the gap. This exploration led to the creation of\nOmni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a\nlightweight module to greatly enhance the adaptivity of sparse CNNs at minimal\ncomputational cost. Without any self-attention modules, OA-CNNs favorably\nsurpass point transformers in terms of accuracy in both indoor and outdoor\nscenes, with much less latency and memory cost. Notably, it achieves 76.1%,\n78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation\nbenchmarks respectively, while maintaining at most 5x better speed than\ntransformer counterparts. This revelation highlights the potential of pure\nsparse CNNs to outperform transformer-related networks.\n", "link": "http://arxiv.org/abs/2403.14418v1", "date": "2024-03-21", "relevancy": 2.6504, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5539}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5187}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5176}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OA-CNNs%3A%20Omni-Adaptive%20Sparse%20CNNs%20for%203D%20Semantic%20Segmentation&body=Title%3A%20OA-CNNs%3A%20Omni-Adaptive%20Sparse%20CNNs%20for%203D%20Semantic%20Segmentation%0AAuthor%3A%20Bohao%20Peng%20and%20Xiaoyang%20Wu%20and%20Li%20Jiang%20and%20Yukang%20Chen%20and%20Hengshuang%20Zhao%20and%20Zhuotao%20Tian%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20The%20booming%20of%203D%20recognition%20in%20the%202020s%20began%20with%20the%20introduction%20of%0Apoint%20cloud%20transformers.%20They%20quickly%20overwhelmed%20sparse%20CNNs%20and%20became%0Astate-of-the-art%20models%2C%20especially%20in%203D%20semantic%20segmentation.%20However%2C%0Asparse%20CNNs%20are%20still%20valuable%20networks%2C%20due%20to%20their%20efficiency%20treasure%2C%20and%0Aease%20of%20application.%20In%20this%20work%2C%20we%20reexamine%20the%20design%20distinctions%20and%0Atest%20the%20limits%20of%20what%20a%20sparse%20CNN%20can%20achieve.%20We%20discover%20that%20the%20key%0Acredit%20to%20the%20performance%20difference%20is%20adaptivity.%20Specifically%2C%20we%20propose%0Atwo%20key%20components%2C%20i.e.%2C%20adaptive%20receptive%20fields%20%28spatially%29%20and%20adaptive%0Arelation%2C%20to%20bridge%20the%20gap.%20This%20exploration%20led%20to%20the%20creation%20of%0AOmni-Adaptive%203D%20CNNs%20%28OA-CNNs%29%2C%20a%20family%20of%20networks%20that%20integrates%20a%0Alightweight%20module%20to%20greatly%20enhance%20the%20adaptivity%20of%20sparse%20CNNs%20at%20minimal%0Acomputational%20cost.%20Without%20any%20self-attention%20modules%2C%20OA-CNNs%20favorably%0Asurpass%20point%20transformers%20in%20terms%20of%20accuracy%20in%20both%20indoor%20and%20outdoor%0Ascenes%2C%20with%20much%20less%20latency%20and%20memory%20cost.%20Notably%2C%20it%20achieves%2076.1%25%2C%0A78.9%25%2C%20and%2070.6%25%20mIoU%20on%20ScanNet%20v2%2C%20nuScenes%2C%20and%20SemanticKITTI%20validation%0Abenchmarks%20respectively%2C%20while%20maintaining%20at%20most%205x%20better%20speed%20than%0Atransformer%20counterparts.%20This%20revelation%20highlights%20the%20potential%20of%20pure%0Asparse%20CNNs%20to%20outperform%20transformer-related%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14418v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OA-CNNs%3A%20Omni-Adaptive%20Sparse%20CNNs%20for%203D%20Semantic%20Segmentation&entry.906535625=Bohao%20Peng%20and%20Xiaoyang%20Wu%20and%20Li%20Jiang%20and%20Yukang%20Chen%20and%20Hengshuang%20Zhao%20and%20Zhuotao%20Tian%20and%20Jiaya%20Jia&entry.1292438233=%20%20The%20booming%20of%203D%20recognition%20in%20the%202020s%20began%20with%20the%20introduction%20of%0Apoint%20cloud%20transformers.%20They%20quickly%20overwhelmed%20sparse%20CNNs%20and%20became%0Astate-of-the-art%20models%2C%20especially%20in%203D%20semantic%20segmentation.%20However%2C%0Asparse%20CNNs%20are%20still%20valuable%20networks%2C%20due%20to%20their%20efficiency%20treasure%2C%20and%0Aease%20of%20application.%20In%20this%20work%2C%20we%20reexamine%20the%20design%20distinctions%20and%0Atest%20the%20limits%20of%20what%20a%20sparse%20CNN%20can%20achieve.%20We%20discover%20that%20the%20key%0Acredit%20to%20the%20performance%20difference%20is%20adaptivity.%20Specifically%2C%20we%20propose%0Atwo%20key%20components%2C%20i.e.%2C%20adaptive%20receptive%20fields%20%28spatially%29%20and%20adaptive%0Arelation%2C%20to%20bridge%20the%20gap.%20This%20exploration%20led%20to%20the%20creation%20of%0AOmni-Adaptive%203D%20CNNs%20%28OA-CNNs%29%2C%20a%20family%20of%20networks%20that%20integrates%20a%0Alightweight%20module%20to%20greatly%20enhance%20the%20adaptivity%20of%20sparse%20CNNs%20at%20minimal%0Acomputational%20cost.%20Without%20any%20self-attention%20modules%2C%20OA-CNNs%20favorably%0Asurpass%20point%20transformers%20in%20terms%20of%20accuracy%20in%20both%20indoor%20and%20outdoor%0Ascenes%2C%20with%20much%20less%20latency%20and%20memory%20cost.%20Notably%2C%20it%20achieves%2076.1%25%2C%0A78.9%25%2C%20and%2070.6%25%20mIoU%20on%20ScanNet%20v2%2C%20nuScenes%2C%20and%20SemanticKITTI%20validation%0Abenchmarks%20respectively%2C%20while%20maintaining%20at%20most%205x%20better%20speed%20than%0Atransformer%20counterparts.%20This%20revelation%20highlights%20the%20potential%20of%20pure%0Asparse%20CNNs%20to%20outperform%20transformer-related%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14418v1&entry.124074799=Read"},
{"title": "A Generative Approach for Wikipedia-Scale Visual Entity Recognition", "author": "Mathilde Caron and Ahmet Iscen and Alireza Fathi and Cordelia Schmid", "abstract": "  In this paper, we address web-scale visual entity recognition, specifically\nthe task of mapping a given query image to one of the 6 million existing\nentities in Wikipedia. One way of approaching a problem of such scale is using\ndual-encoder models (eg CLIP), where all the entity names and query images are\nembedded into a unified space, paving the way for an approximate k-NN search.\nAlternatively, it is also possible to re-purpose a captioning model to directly\ngenerate the entity names for a given image. In contrast, we introduce a novel\nGenerative Entity Recognition (GER) framework, which given an input image\nlearns to auto-regressively decode a semantic and discriminative ``code''\nidentifying the target entity. Our experiments demonstrate the efficacy of this\nGER paradigm, showcasing state-of-the-art performance on the challenging OVEN\nbenchmark. GER surpasses strong captioning, dual-encoder, visual matching and\nhierarchical classification baselines, affirming its advantage in tackling the\ncomplexities of web-scale recognition.\n", "link": "http://arxiv.org/abs/2403.02041v2", "date": "2024-03-21", "relevancy": 2.6321, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5421}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5413}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4958}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Generative%20Approach%20for%20Wikipedia-Scale%20Visual%20Entity%20Recognition&body=Title%3A%20A%20Generative%20Approach%20for%20Wikipedia-Scale%20Visual%20Entity%20Recognition%0AAuthor%3A%20Mathilde%20Caron%20and%20Ahmet%20Iscen%20and%20Alireza%20Fathi%20and%20Cordelia%20Schmid%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20web-scale%20visual%20entity%20recognition%2C%20specifically%0Athe%20task%20of%20mapping%20a%20given%20query%20image%20to%20one%20of%20the%206%20million%20existing%0Aentities%20in%20Wikipedia.%20One%20way%20of%20approaching%20a%20problem%20of%20such%20scale%20is%20using%0Adual-encoder%20models%20%28eg%20CLIP%29%2C%20where%20all%20the%20entity%20names%20and%20query%20images%20are%0Aembedded%20into%20a%20unified%20space%2C%20paving%20the%20way%20for%20an%20approximate%20k-NN%20search.%0AAlternatively%2C%20it%20is%20also%20possible%20to%20re-purpose%20a%20captioning%20model%20to%20directly%0Agenerate%20the%20entity%20names%20for%20a%20given%20image.%20In%20contrast%2C%20we%20introduce%20a%20novel%0AGenerative%20Entity%20Recognition%20%28GER%29%20framework%2C%20which%20given%20an%20input%20image%0Alearns%20to%20auto-regressively%20decode%20a%20semantic%20and%20discriminative%20%60%60code%27%27%0Aidentifying%20the%20target%20entity.%20Our%20experiments%20demonstrate%20the%20efficacy%20of%20this%0AGER%20paradigm%2C%20showcasing%20state-of-the-art%20performance%20on%20the%20challenging%20OVEN%0Abenchmark.%20GER%20surpasses%20strong%20captioning%2C%20dual-encoder%2C%20visual%20matching%20and%0Ahierarchical%20classification%20baselines%2C%20affirming%20its%20advantage%20in%20tackling%20the%0Acomplexities%20of%20web-scale%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02041v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generative%20Approach%20for%20Wikipedia-Scale%20Visual%20Entity%20Recognition&entry.906535625=Mathilde%20Caron%20and%20Ahmet%20Iscen%20and%20Alireza%20Fathi%20and%20Cordelia%20Schmid&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20web-scale%20visual%20entity%20recognition%2C%20specifically%0Athe%20task%20of%20mapping%20a%20given%20query%20image%20to%20one%20of%20the%206%20million%20existing%0Aentities%20in%20Wikipedia.%20One%20way%20of%20approaching%20a%20problem%20of%20such%20scale%20is%20using%0Adual-encoder%20models%20%28eg%20CLIP%29%2C%20where%20all%20the%20entity%20names%20and%20query%20images%20are%0Aembedded%20into%20a%20unified%20space%2C%20paving%20the%20way%20for%20an%20approximate%20k-NN%20search.%0AAlternatively%2C%20it%20is%20also%20possible%20to%20re-purpose%20a%20captioning%20model%20to%20directly%0Agenerate%20the%20entity%20names%20for%20a%20given%20image.%20In%20contrast%2C%20we%20introduce%20a%20novel%0AGenerative%20Entity%20Recognition%20%28GER%29%20framework%2C%20which%20given%20an%20input%20image%0Alearns%20to%20auto-regressively%20decode%20a%20semantic%20and%20discriminative%20%60%60code%27%27%0Aidentifying%20the%20target%20entity.%20Our%20experiments%20demonstrate%20the%20efficacy%20of%20this%0AGER%20paradigm%2C%20showcasing%20state-of-the-art%20performance%20on%20the%20challenging%20OVEN%0Abenchmark.%20GER%20surpasses%20strong%20captioning%2C%20dual-encoder%2C%20visual%20matching%20and%0Ahierarchical%20classification%20baselines%2C%20affirming%20its%20advantage%20in%20tackling%20the%0Acomplexities%20of%20web-scale%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02041v2&entry.124074799=Read"},
{"title": "GLC++: Source-Free Universal Domain Adaptation through Global-Local\n  Clustering and Contrastive Affinity Learning", "author": "Sanqing Qu and Tianpei Zou and Florian R\u00f6hrbein and Cewu Lu and Guang Chen and Dacheng Tao and Changjun Jiang", "abstract": "  Deep neural networks often exhibit sub-optimal performance under covariate\nand category shifts. Source-Free Domain Adaptation (SFDA) presents a promising\nsolution to this dilemma, yet most SFDA approaches are restricted to closed-set\nscenarios. In this paper, we explore Source-Free Universal Domain Adaptation\n(SF-UniDA) aiming to accurately classify \"known\" data belonging to common\ncategories and segregate them from target-private \"unknown\" data. We propose a\nnovel Global and Local Clustering (GLC) technique, which comprises an adaptive\none-vs-all global clustering algorithm to discern between target classes,\ncomplemented by a local k-NN clustering strategy to mitigate negative transfer.\nDespite the effectiveness, the inherent closed-set source architecture leads to\nuniform treatment of \"unknown\" data, impeding the identification of distinct\n\"unknown\" categories. To address this, we evolve GLC to GLC++, integrating a\ncontrastive affinity learning strategy. We examine the superiority of GLC and\nGLC++ across multiple benchmarks and category shift scenarios. Remarkably, in\nthe most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by\n16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel\ncategory clustering accuracy of GLC by 4.3% in open-set scenarios on\nOffice-Home. Furthermore, the introduced contrastive learning strategy not only\nenhances GLC but also significantly facilitates existing methodologies.\n", "link": "http://arxiv.org/abs/2403.14410v1", "date": "2024-03-21", "relevancy": 2.628, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5382}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5188}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GLC%2B%2B%3A%20Source-Free%20Universal%20Domain%20Adaptation%20through%20Global-Local%0A%20%20Clustering%20and%20Contrastive%20Affinity%20Learning&body=Title%3A%20GLC%2B%2B%3A%20Source-Free%20Universal%20Domain%20Adaptation%20through%20Global-Local%0A%20%20Clustering%20and%20Contrastive%20Affinity%20Learning%0AAuthor%3A%20Sanqing%20Qu%20and%20Tianpei%20Zou%20and%20Florian%20R%C3%B6hrbein%20and%20Cewu%20Lu%20and%20Guang%20Chen%20and%20Dacheng%20Tao%20and%20Changjun%20Jiang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20often%20exhibit%20sub-optimal%20performance%20under%20covariate%0Aand%20category%20shifts.%20Source-Free%20Domain%20Adaptation%20%28SFDA%29%20presents%20a%20promising%0Asolution%20to%20this%20dilemma%2C%20yet%20most%20SFDA%20approaches%20are%20restricted%20to%20closed-set%0Ascenarios.%20In%20this%20paper%2C%20we%20explore%20Source-Free%20Universal%20Domain%20Adaptation%0A%28SF-UniDA%29%20aiming%20to%20accurately%20classify%20%22known%22%20data%20belonging%20to%20common%0Acategories%20and%20segregate%20them%20from%20target-private%20%22unknown%22%20data.%20We%20propose%20a%0Anovel%20Global%20and%20Local%20Clustering%20%28GLC%29%20technique%2C%20which%20comprises%20an%20adaptive%0Aone-vs-all%20global%20clustering%20algorithm%20to%20discern%20between%20target%20classes%2C%0Acomplemented%20by%20a%20local%20k-NN%20clustering%20strategy%20to%20mitigate%20negative%20transfer.%0ADespite%20the%20effectiveness%2C%20the%20inherent%20closed-set%20source%20architecture%20leads%20to%0Auniform%20treatment%20of%20%22unknown%22%20data%2C%20impeding%20the%20identification%20of%20distinct%0A%22unknown%22%20categories.%20To%20address%20this%2C%20we%20evolve%20GLC%20to%20GLC%2B%2B%2C%20integrating%20a%0Acontrastive%20affinity%20learning%20strategy.%20We%20examine%20the%20superiority%20of%20GLC%20and%0AGLC%2B%2B%20across%20multiple%20benchmarks%20and%20category%20shift%20scenarios.%20Remarkably%2C%20in%0Athe%20most%20challenging%20open-partial-set%20scenarios%2C%20GLC%20and%20GLC%2B%2B%20surpass%20GATE%20by%0A16.7%25%20and%2018.6%25%20in%20H-score%20on%20VisDA%2C%20respectively.%20GLC%2B%2B%20enhances%20the%20novel%0Acategory%20clustering%20accuracy%20of%20GLC%20by%204.3%25%20in%20open-set%20scenarios%20on%0AOffice-Home.%20Furthermore%2C%20the%20introduced%20contrastive%20learning%20strategy%20not%20only%0Aenhances%20GLC%20but%20also%20significantly%20facilitates%20existing%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14410v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLC%2B%2B%3A%20Source-Free%20Universal%20Domain%20Adaptation%20through%20Global-Local%0A%20%20Clustering%20and%20Contrastive%20Affinity%20Learning&entry.906535625=Sanqing%20Qu%20and%20Tianpei%20Zou%20and%20Florian%20R%C3%B6hrbein%20and%20Cewu%20Lu%20and%20Guang%20Chen%20and%20Dacheng%20Tao%20and%20Changjun%20Jiang&entry.1292438233=%20%20Deep%20neural%20networks%20often%20exhibit%20sub-optimal%20performance%20under%20covariate%0Aand%20category%20shifts.%20Source-Free%20Domain%20Adaptation%20%28SFDA%29%20presents%20a%20promising%0Asolution%20to%20this%20dilemma%2C%20yet%20most%20SFDA%20approaches%20are%20restricted%20to%20closed-set%0Ascenarios.%20In%20this%20paper%2C%20we%20explore%20Source-Free%20Universal%20Domain%20Adaptation%0A%28SF-UniDA%29%20aiming%20to%20accurately%20classify%20%22known%22%20data%20belonging%20to%20common%0Acategories%20and%20segregate%20them%20from%20target-private%20%22unknown%22%20data.%20We%20propose%20a%0Anovel%20Global%20and%20Local%20Clustering%20%28GLC%29%20technique%2C%20which%20comprises%20an%20adaptive%0Aone-vs-all%20global%20clustering%20algorithm%20to%20discern%20between%20target%20classes%2C%0Acomplemented%20by%20a%20local%20k-NN%20clustering%20strategy%20to%20mitigate%20negative%20transfer.%0ADespite%20the%20effectiveness%2C%20the%20inherent%20closed-set%20source%20architecture%20leads%20to%0Auniform%20treatment%20of%20%22unknown%22%20data%2C%20impeding%20the%20identification%20of%20distinct%0A%22unknown%22%20categories.%20To%20address%20this%2C%20we%20evolve%20GLC%20to%20GLC%2B%2B%2C%20integrating%20a%0Acontrastive%20affinity%20learning%20strategy.%20We%20examine%20the%20superiority%20of%20GLC%20and%0AGLC%2B%2B%20across%20multiple%20benchmarks%20and%20category%20shift%20scenarios.%20Remarkably%2C%20in%0Athe%20most%20challenging%20open-partial-set%20scenarios%2C%20GLC%20and%20GLC%2B%2B%20surpass%20GATE%20by%0A16.7%25%20and%2018.6%25%20in%20H-score%20on%20VisDA%2C%20respectively.%20GLC%2B%2B%20enhances%20the%20novel%0Acategory%20clustering%20accuracy%20of%20GLC%20by%204.3%25%20in%20open-set%20scenarios%20on%0AOffice-Home.%20Furthermore%2C%20the%20introduced%20contrastive%20learning%20strategy%20not%20only%0Aenhances%20GLC%20but%20also%20significantly%20facilitates%20existing%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14410v1&entry.124074799=Read"},
{"title": "Neural Network-Based Processing and Reconstruction of Compromised\n  Biophotonic Image Data", "author": "Michael John Fanous and Paloma Casteleiro Costa and Cagatay Isil and Luzhe Huang and Aydogan Ozcan", "abstract": "  The integration of deep learning techniques with biophotonic setups has\nopened new horizons in bioimaging. A compelling trend in this field involves\ndeliberately compromising certain measurement metrics to engineer better\nbioimaging tools in terms of cost, speed, and form-factor, followed by\ncompensating for the resulting defects through the utilization of deep learning\nmodels trained on a large amount of ideal, superior or alternative data. This\nstrategic approach has found increasing popularity due to its potential to\nenhance various aspects of biophotonic imaging. One of the primary motivations\nfor employing this strategy is the pursuit of higher temporal resolution or\nincreased imaging speed, critical for capturing fine dynamic biological\nprocesses. This approach also offers the prospect of simplifying hardware\nrequirements/complexities, thereby making advanced imaging standards more\naccessible in terms of cost and/or size. This article provides an in-depth\nreview of the diverse measurement aspects that researchers intentionally impair\nin their biophotonic setups, including the point spread function,\nsignal-to-noise ratio, sampling density, and pixel resolution. By deliberately\ncompromising these metrics, researchers aim to not only recuperate them through\nthe application of deep learning networks, but also bolster in return other\ncrucial parameters, such as the field-of-view, depth-of-field, and\nspace-bandwidth product. Here, we discuss various biophotonic methods that have\nsuccessfully employed this strategic approach. These techniques span broad\napplications and showcase the versatility and effectiveness of deep learning in\nthe context of compromised biophotonic data. Finally, by offering our\nperspectives on the future possibilities of this rapidly evolving concept, we\nhope to motivate our readers to explore novel ways of balancing hardware\ncompromises with compensation via AI.\n", "link": "http://arxiv.org/abs/2403.14324v1", "date": "2024-03-21", "relevancy": 2.6128, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5344}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5202}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5131}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Network-Based%20Processing%20and%20Reconstruction%20of%20Compromised%0A%20%20Biophotonic%20Image%20Data&body=Title%3A%20Neural%20Network-Based%20Processing%20and%20Reconstruction%20of%20Compromised%0A%20%20Biophotonic%20Image%20Data%0AAuthor%3A%20Michael%20John%20Fanous%20and%20Paloma%20Casteleiro%20Costa%20and%20Cagatay%20Isil%20and%20Luzhe%20Huang%20and%20Aydogan%20Ozcan%0AAbstract%3A%20%20%20The%20integration%20of%20deep%20learning%20techniques%20with%20biophotonic%20setups%20has%0Aopened%20new%20horizons%20in%20bioimaging.%20A%20compelling%20trend%20in%20this%20field%20involves%0Adeliberately%20compromising%20certain%20measurement%20metrics%20to%20engineer%20better%0Abioimaging%20tools%20in%20terms%20of%20cost%2C%20speed%2C%20and%20form-factor%2C%20followed%20by%0Acompensating%20for%20the%20resulting%20defects%20through%20the%20utilization%20of%20deep%20learning%0Amodels%20trained%20on%20a%20large%20amount%20of%20ideal%2C%20superior%20or%20alternative%20data.%20This%0Astrategic%20approach%20has%20found%20increasing%20popularity%20due%20to%20its%20potential%20to%0Aenhance%20various%20aspects%20of%20biophotonic%20imaging.%20One%20of%20the%20primary%20motivations%0Afor%20employing%20this%20strategy%20is%20the%20pursuit%20of%20higher%20temporal%20resolution%20or%0Aincreased%20imaging%20speed%2C%20critical%20for%20capturing%20fine%20dynamic%20biological%0Aprocesses.%20This%20approach%20also%20offers%20the%20prospect%20of%20simplifying%20hardware%0Arequirements/complexities%2C%20thereby%20making%20advanced%20imaging%20standards%20more%0Aaccessible%20in%20terms%20of%20cost%20and/or%20size.%20This%20article%20provides%20an%20in-depth%0Areview%20of%20the%20diverse%20measurement%20aspects%20that%20researchers%20intentionally%20impair%0Ain%20their%20biophotonic%20setups%2C%20including%20the%20point%20spread%20function%2C%0Asignal-to-noise%20ratio%2C%20sampling%20density%2C%20and%20pixel%20resolution.%20By%20deliberately%0Acompromising%20these%20metrics%2C%20researchers%20aim%20to%20not%20only%20recuperate%20them%20through%0Athe%20application%20of%20deep%20learning%20networks%2C%20but%20also%20bolster%20in%20return%20other%0Acrucial%20parameters%2C%20such%20as%20the%20field-of-view%2C%20depth-of-field%2C%20and%0Aspace-bandwidth%20product.%20Here%2C%20we%20discuss%20various%20biophotonic%20methods%20that%20have%0Asuccessfully%20employed%20this%20strategic%20approach.%20These%20techniques%20span%20broad%0Aapplications%20and%20showcase%20the%20versatility%20and%20effectiveness%20of%20deep%20learning%20in%0Athe%20context%20of%20compromised%20biophotonic%20data.%20Finally%2C%20by%20offering%20our%0Aperspectives%20on%20the%20future%20possibilities%20of%20this%20rapidly%20evolving%20concept%2C%20we%0Ahope%20to%20motivate%20our%20readers%20to%20explore%20novel%20ways%20of%20balancing%20hardware%0Acompromises%20with%20compensation%20via%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14324v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network-Based%20Processing%20and%20Reconstruction%20of%20Compromised%0A%20%20Biophotonic%20Image%20Data&entry.906535625=Michael%20John%20Fanous%20and%20Paloma%20Casteleiro%20Costa%20and%20Cagatay%20Isil%20and%20Luzhe%20Huang%20and%20Aydogan%20Ozcan&entry.1292438233=%20%20The%20integration%20of%20deep%20learning%20techniques%20with%20biophotonic%20setups%20has%0Aopened%20new%20horizons%20in%20bioimaging.%20A%20compelling%20trend%20in%20this%20field%20involves%0Adeliberately%20compromising%20certain%20measurement%20metrics%20to%20engineer%20better%0Abioimaging%20tools%20in%20terms%20of%20cost%2C%20speed%2C%20and%20form-factor%2C%20followed%20by%0Acompensating%20for%20the%20resulting%20defects%20through%20the%20utilization%20of%20deep%20learning%0Amodels%20trained%20on%20a%20large%20amount%20of%20ideal%2C%20superior%20or%20alternative%20data.%20This%0Astrategic%20approach%20has%20found%20increasing%20popularity%20due%20to%20its%20potential%20to%0Aenhance%20various%20aspects%20of%20biophotonic%20imaging.%20One%20of%20the%20primary%20motivations%0Afor%20employing%20this%20strategy%20is%20the%20pursuit%20of%20higher%20temporal%20resolution%20or%0Aincreased%20imaging%20speed%2C%20critical%20for%20capturing%20fine%20dynamic%20biological%0Aprocesses.%20This%20approach%20also%20offers%20the%20prospect%20of%20simplifying%20hardware%0Arequirements/complexities%2C%20thereby%20making%20advanced%20imaging%20standards%20more%0Aaccessible%20in%20terms%20of%20cost%20and/or%20size.%20This%20article%20provides%20an%20in-depth%0Areview%20of%20the%20diverse%20measurement%20aspects%20that%20researchers%20intentionally%20impair%0Ain%20their%20biophotonic%20setups%2C%20including%20the%20point%20spread%20function%2C%0Asignal-to-noise%20ratio%2C%20sampling%20density%2C%20and%20pixel%20resolution.%20By%20deliberately%0Acompromising%20these%20metrics%2C%20researchers%20aim%20to%20not%20only%20recuperate%20them%20through%0Athe%20application%20of%20deep%20learning%20networks%2C%20but%20also%20bolster%20in%20return%20other%0Acrucial%20parameters%2C%20such%20as%20the%20field-of-view%2C%20depth-of-field%2C%20and%0Aspace-bandwidth%20product.%20Here%2C%20we%20discuss%20various%20biophotonic%20methods%20that%20have%0Asuccessfully%20employed%20this%20strategic%20approach.%20These%20techniques%20span%20broad%0Aapplications%20and%20showcase%20the%20versatility%20and%20effectiveness%20of%20deep%20learning%20in%0Athe%20context%20of%20compromised%20biophotonic%20data.%20Finally%2C%20by%20offering%20our%0Aperspectives%20on%20the%20future%20possibilities%20of%20this%20rapidly%20evolving%20concept%2C%20we%0Ahope%20to%20motivate%20our%20readers%20to%20explore%20novel%20ways%20of%20balancing%20hardware%0Acompromises%20with%20compensation%20via%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14324v1&entry.124074799=Read"},
{"title": "LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT\n  Descriptors", "author": "Saksham Suri and Matthew Walmer and Kamal Gupta and Abhinav Shrivastava", "abstract": "  We present a simple self-supervised method to enhance the performance of ViT\nfeatures for dense downstream tasks. Our Lightweight Feature Transform (LiFT)\nis a straightforward and compact postprocessing network that can be applied to\nenhance the features of any pre-trained ViT backbone. LiFT is fast and easy to\ntrain with a self-supervised objective, and it boosts the density of ViT\nfeatures for minimal extra inference cost. Furthermore, we demonstrate that\nLiFT can be applied with approaches that use additional task-specific\ndownstream modules, as we integrate LiFT with ViTDet for COCO detection and\nsegmentation. Despite the simplicity of LiFT, we find that it is not simply\nlearning a more complex version of bilinear interpolation. Instead, our LiFT\ntraining protocol leads to several desirable emergent properties that benefit\nViT features in dense downstream tasks. This includes greater scale invariance\nfor features, and better object boundary maps. By simply training LiFT for a\nfew epochs, we show improved performance on keypoint correspondence, detection,\nsegmentation, and object discovery tasks. Overall, LiFT provides an easy way to\nunlock the benefits of denser feature arrays for a fraction of the\ncomputational cost. For more details, refer to our project page at\nhttps://www.cs.umd.edu/~sakshams/LiFT/.\n", "link": "http://arxiv.org/abs/2403.14625v1", "date": "2024-03-21", "relevancy": 2.61, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5128}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LiFT%3A%20A%20Surprisingly%20Simple%20Lightweight%20Feature%20Transform%20for%20Dense%20ViT%0A%20%20Descriptors&body=Title%3A%20LiFT%3A%20A%20Surprisingly%20Simple%20Lightweight%20Feature%20Transform%20for%20Dense%20ViT%0A%20%20Descriptors%0AAuthor%3A%20Saksham%20Suri%20and%20Matthew%20Walmer%20and%20Kamal%20Gupta%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20We%20present%20a%20simple%20self-supervised%20method%20to%20enhance%20the%20performance%20of%20ViT%0Afeatures%20for%20dense%20downstream%20tasks.%20Our%20Lightweight%20Feature%20Transform%20%28LiFT%29%0Ais%20a%20straightforward%20and%20compact%20postprocessing%20network%20that%20can%20be%20applied%20to%0Aenhance%20the%20features%20of%20any%20pre-trained%20ViT%20backbone.%20LiFT%20is%20fast%20and%20easy%20to%0Atrain%20with%20a%20self-supervised%20objective%2C%20and%20it%20boosts%20the%20density%20of%20ViT%0Afeatures%20for%20minimal%20extra%20inference%20cost.%20Furthermore%2C%20we%20demonstrate%20that%0ALiFT%20can%20be%20applied%20with%20approaches%20that%20use%20additional%20task-specific%0Adownstream%20modules%2C%20as%20we%20integrate%20LiFT%20with%20ViTDet%20for%20COCO%20detection%20and%0Asegmentation.%20Despite%20the%20simplicity%20of%20LiFT%2C%20we%20find%20that%20it%20is%20not%20simply%0Alearning%20a%20more%20complex%20version%20of%20bilinear%20interpolation.%20Instead%2C%20our%20LiFT%0Atraining%20protocol%20leads%20to%20several%20desirable%20emergent%20properties%20that%20benefit%0AViT%20features%20in%20dense%20downstream%20tasks.%20This%20includes%20greater%20scale%20invariance%0Afor%20features%2C%20and%20better%20object%20boundary%20maps.%20By%20simply%20training%20LiFT%20for%20a%0Afew%20epochs%2C%20we%20show%20improved%20performance%20on%20keypoint%20correspondence%2C%20detection%2C%0Asegmentation%2C%20and%20object%20discovery%20tasks.%20Overall%2C%20LiFT%20provides%20an%20easy%20way%20to%0Aunlock%20the%20benefits%20of%20denser%20feature%20arrays%20for%20a%20fraction%20of%20the%0Acomputational%20cost.%20For%20more%20details%2C%20refer%20to%20our%20project%20page%20at%0Ahttps%3A//www.cs.umd.edu/~sakshams/LiFT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14625v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiFT%3A%20A%20Surprisingly%20Simple%20Lightweight%20Feature%20Transform%20for%20Dense%20ViT%0A%20%20Descriptors&entry.906535625=Saksham%20Suri%20and%20Matthew%20Walmer%20and%20Kamal%20Gupta%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20We%20present%20a%20simple%20self-supervised%20method%20to%20enhance%20the%20performance%20of%20ViT%0Afeatures%20for%20dense%20downstream%20tasks.%20Our%20Lightweight%20Feature%20Transform%20%28LiFT%29%0Ais%20a%20straightforward%20and%20compact%20postprocessing%20network%20that%20can%20be%20applied%20to%0Aenhance%20the%20features%20of%20any%20pre-trained%20ViT%20backbone.%20LiFT%20is%20fast%20and%20easy%20to%0Atrain%20with%20a%20self-supervised%20objective%2C%20and%20it%20boosts%20the%20density%20of%20ViT%0Afeatures%20for%20minimal%20extra%20inference%20cost.%20Furthermore%2C%20we%20demonstrate%20that%0ALiFT%20can%20be%20applied%20with%20approaches%20that%20use%20additional%20task-specific%0Adownstream%20modules%2C%20as%20we%20integrate%20LiFT%20with%20ViTDet%20for%20COCO%20detection%20and%0Asegmentation.%20Despite%20the%20simplicity%20of%20LiFT%2C%20we%20find%20that%20it%20is%20not%20simply%0Alearning%20a%20more%20complex%20version%20of%20bilinear%20interpolation.%20Instead%2C%20our%20LiFT%0Atraining%20protocol%20leads%20to%20several%20desirable%20emergent%20properties%20that%20benefit%0AViT%20features%20in%20dense%20downstream%20tasks.%20This%20includes%20greater%20scale%20invariance%0Afor%20features%2C%20and%20better%20object%20boundary%20maps.%20By%20simply%20training%20LiFT%20for%20a%0Afew%20epochs%2C%20we%20show%20improved%20performance%20on%20keypoint%20correspondence%2C%20detection%2C%0Asegmentation%2C%20and%20object%20discovery%20tasks.%20Overall%2C%20LiFT%20provides%20an%20easy%20way%20to%0Aunlock%20the%20benefits%20of%20denser%20feature%20arrays%20for%20a%20fraction%20of%20the%0Acomputational%20cost.%20For%20more%20details%2C%20refer%20to%20our%20project%20page%20at%0Ahttps%3A//www.cs.umd.edu/~sakshams/LiFT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14625v1&entry.124074799=Read"},
{"title": "Separate and Conquer: Decoupling Co-occurrence via Decomposition and\n  Representation for Weakly Supervised Semantic Segmentation", "author": "Zhiwei Yang and Kexue Fu and Minghong Duan and Linhao Qu and Shuo Wang and Zhijian Song", "abstract": "  Weakly supervised semantic segmentation (WSSS) with image-level labels aims\nto achieve segmentation tasks without dense annotations. However, attributed to\nthe frequent coupling of co-occurring objects and the limited supervision from\nimage-level labels, the challenging co-occurrence problem is widely present and\nleads to false activation of objects in WSSS. In this work, we devise a\n'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of\nimage space and feature space. In the image space, we propose to 'separate' the\nco-occurring objects with image decomposition by subdividing images into\npatches. Importantly, we assign each patch a category tag from Class Activation\nMaps (CAMs), which spatially helps remove the co-context bias and guide the\nsubsequent representation. In the feature space, we propose to 'conquer' the\nfalse activation by enhancing semantic representation with multi-granularity\nknowledge contrast. To this end, a dual-teacher-single-student architecture is\ndesigned and tag-guided contrast is conducted, which guarantee the correctness\nof knowledge and further facilitate the discrepancy among co-contexts. We\nstreamline the multi-staged WSSS pipeline end-to-end and tackle this issue\nwithout external supervision. Extensive experiments are conducted, validating\nthe efficiency of our method and the superiority over previous single-staged\nand even multi-staged competitors on PASCAL VOC and MS COCO. Code is available\nat https://github.com/zwyang6/SeCo.git.\n", "link": "http://arxiv.org/abs/2402.18467v3", "date": "2024-03-21", "relevancy": 2.5905, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5364}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5034}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Separate%20and%20Conquer%3A%20Decoupling%20Co-occurrence%20via%20Decomposition%20and%0A%20%20Representation%20for%20Weakly%20Supervised%20Semantic%20Segmentation&body=Title%3A%20Separate%20and%20Conquer%3A%20Decoupling%20Co-occurrence%20via%20Decomposition%20and%0A%20%20Representation%20for%20Weakly%20Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Zhiwei%20Yang%20and%20Kexue%20Fu%20and%20Minghong%20Duan%20and%20Linhao%20Qu%20and%20Shuo%20Wang%20and%20Zhijian%20Song%0AAbstract%3A%20%20%20Weakly%20supervised%20semantic%20segmentation%20%28WSSS%29%20with%20image-level%20labels%20aims%0Ato%20achieve%20segmentation%20tasks%20without%20dense%20annotations.%20However%2C%20attributed%20to%0Athe%20frequent%20coupling%20of%20co-occurring%20objects%20and%20the%20limited%20supervision%20from%0Aimage-level%20labels%2C%20the%20challenging%20co-occurrence%20problem%20is%20widely%20present%20and%0Aleads%20to%20false%20activation%20of%20objects%20in%20WSSS.%20In%20this%20work%2C%20we%20devise%20a%0A%27Separate%20and%20Conquer%27%20scheme%20SeCo%20to%20tackle%20this%20issue%20from%20dimensions%20of%0Aimage%20space%20and%20feature%20space.%20In%20the%20image%20space%2C%20we%20propose%20to%20%27separate%27%20the%0Aco-occurring%20objects%20with%20image%20decomposition%20by%20subdividing%20images%20into%0Apatches.%20Importantly%2C%20we%20assign%20each%20patch%20a%20category%20tag%20from%20Class%20Activation%0AMaps%20%28CAMs%29%2C%20which%20spatially%20helps%20remove%20the%20co-context%20bias%20and%20guide%20the%0Asubsequent%20representation.%20In%20the%20feature%20space%2C%20we%20propose%20to%20%27conquer%27%20the%0Afalse%20activation%20by%20enhancing%20semantic%20representation%20with%20multi-granularity%0Aknowledge%20contrast.%20To%20this%20end%2C%20a%20dual-teacher-single-student%20architecture%20is%0Adesigned%20and%20tag-guided%20contrast%20is%20conducted%2C%20which%20guarantee%20the%20correctness%0Aof%20knowledge%20and%20further%20facilitate%20the%20discrepancy%20among%20co-contexts.%20We%0Astreamline%20the%20multi-staged%20WSSS%20pipeline%20end-to-end%20and%20tackle%20this%20issue%0Awithout%20external%20supervision.%20Extensive%20experiments%20are%20conducted%2C%20validating%0Athe%20efficiency%20of%20our%20method%20and%20the%20superiority%20over%20previous%20single-staged%0Aand%20even%20multi-staged%20competitors%20on%20PASCAL%20VOC%20and%20MS%20COCO.%20Code%20is%20available%0Aat%20https%3A//github.com/zwyang6/SeCo.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18467v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separate%20and%20Conquer%3A%20Decoupling%20Co-occurrence%20via%20Decomposition%20and%0A%20%20Representation%20for%20Weakly%20Supervised%20Semantic%20Segmentation&entry.906535625=Zhiwei%20Yang%20and%20Kexue%20Fu%20and%20Minghong%20Duan%20and%20Linhao%20Qu%20and%20Shuo%20Wang%20and%20Zhijian%20Song&entry.1292438233=%20%20Weakly%20supervised%20semantic%20segmentation%20%28WSSS%29%20with%20image-level%20labels%20aims%0Ato%20achieve%20segmentation%20tasks%20without%20dense%20annotations.%20However%2C%20attributed%20to%0Athe%20frequent%20coupling%20of%20co-occurring%20objects%20and%20the%20limited%20supervision%20from%0Aimage-level%20labels%2C%20the%20challenging%20co-occurrence%20problem%20is%20widely%20present%20and%0Aleads%20to%20false%20activation%20of%20objects%20in%20WSSS.%20In%20this%20work%2C%20we%20devise%20a%0A%27Separate%20and%20Conquer%27%20scheme%20SeCo%20to%20tackle%20this%20issue%20from%20dimensions%20of%0Aimage%20space%20and%20feature%20space.%20In%20the%20image%20space%2C%20we%20propose%20to%20%27separate%27%20the%0Aco-occurring%20objects%20with%20image%20decomposition%20by%20subdividing%20images%20into%0Apatches.%20Importantly%2C%20we%20assign%20each%20patch%20a%20category%20tag%20from%20Class%20Activation%0AMaps%20%28CAMs%29%2C%20which%20spatially%20helps%20remove%20the%20co-context%20bias%20and%20guide%20the%0Asubsequent%20representation.%20In%20the%20feature%20space%2C%20we%20propose%20to%20%27conquer%27%20the%0Afalse%20activation%20by%20enhancing%20semantic%20representation%20with%20multi-granularity%0Aknowledge%20contrast.%20To%20this%20end%2C%20a%20dual-teacher-single-student%20architecture%20is%0Adesigned%20and%20tag-guided%20contrast%20is%20conducted%2C%20which%20guarantee%20the%20correctness%0Aof%20knowledge%20and%20further%20facilitate%20the%20discrepancy%20among%20co-contexts.%20We%0Astreamline%20the%20multi-staged%20WSSS%20pipeline%20end-to-end%20and%20tackle%20this%20issue%0Awithout%20external%20supervision.%20Extensive%20experiments%20are%20conducted%2C%20validating%0Athe%20efficiency%20of%20our%20method%20and%20the%20superiority%20over%20previous%20single-staged%0Aand%20even%20multi-staged%20competitors%20on%20PASCAL%20VOC%20and%20MS%20COCO.%20Code%20is%20available%0Aat%20https%3A//github.com/zwyang6/SeCo.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18467v3&entry.124074799=Read"},
{"title": "ColonNeRF: High-Fidelity Neural Reconstruction of Long Colonoscopy", "author": "Yufei Shi and Beijia Lu and Jia-Wei Liu and Ming Li and Mike Zheng Shou", "abstract": "  Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.\nHowever, accurate long-sequence colonoscopy reconstruction faces three major\nchallenges: (1) dissimilarity among segments of the colon due to its meandering\nand convoluted shape; (2) co-existence of simple and intricately folded\ngeometry structures; (3) sparse viewpoints due to constrained camera\ntrajectories. To tackle these challenges, we introduce a new reconstruction\nframework based on neural radiance field (NeRF), named ColonNeRF, which\nleverages neural rendering for novel view synthesis of long-sequence\ncolonoscopy. Specifically, to reconstruct the entire colon in a piecewise\nmanner, our ColonNeRF introduces a region division and integration module,\neffectively reducing shape dissimilarity and ensuring geometric consistency in\neach segment. To learn both the simple and complex geometry in a unified\nframework, our ColonNeRF incorporates a multi-level fusion module that\nprogressively models the colon regions from easy to hard. Additionally, to\novercome the challenges from sparse views, we devise a DensiNet module for\ndensifying camera poses under the guidance of semantic consistency. We conduct\nextensive experiments on both synthetic and real-world datasets to evaluate our\nColonNeRF. Quantitatively, ColonNeRF exhibits a 67%-85% increase in LPIPS-ALEX\nscores. Qualitatively, our reconstruction visualizations show much clearer\ntextures and more accurate geometric details. These sufficiently demonstrate\nour superior performance over the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2312.02015v2", "date": "2024-03-21", "relevancy": 2.5732, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5305}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5081}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5054}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ColonNeRF%3A%20High-Fidelity%20Neural%20Reconstruction%20of%20Long%20Colonoscopy&body=Title%3A%20ColonNeRF%3A%20High-Fidelity%20Neural%20Reconstruction%20of%20Long%20Colonoscopy%0AAuthor%3A%20Yufei%20Shi%20and%20Beijia%20Lu%20and%20Jia-Wei%20Liu%20and%20Ming%20Li%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Colonoscopy%20reconstruction%20is%20pivotal%20for%20diagnosing%20colorectal%20cancer.%0AHowever%2C%20accurate%20long-sequence%20colonoscopy%20reconstruction%20faces%20three%20major%0Achallenges%3A%20%281%29%20dissimilarity%20among%20segments%20of%20the%20colon%20due%20to%20its%20meandering%0Aand%20convoluted%20shape%3B%20%282%29%20co-existence%20of%20simple%20and%20intricately%20folded%0Ageometry%20structures%3B%20%283%29%20sparse%20viewpoints%20due%20to%20constrained%20camera%0Atrajectories.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20a%20new%20reconstruction%0Aframework%20based%20on%20neural%20radiance%20field%20%28NeRF%29%2C%20named%20ColonNeRF%2C%20which%0Aleverages%20neural%20rendering%20for%20novel%20view%20synthesis%20of%20long-sequence%0Acolonoscopy.%20Specifically%2C%20to%20reconstruct%20the%20entire%20colon%20in%20a%20piecewise%0Amanner%2C%20our%20ColonNeRF%20introduces%20a%20region%20division%20and%20integration%20module%2C%0Aeffectively%20reducing%20shape%20dissimilarity%20and%20ensuring%20geometric%20consistency%20in%0Aeach%20segment.%20To%20learn%20both%20the%20simple%20and%20complex%20geometry%20in%20a%20unified%0Aframework%2C%20our%20ColonNeRF%20incorporates%20a%20multi-level%20fusion%20module%20that%0Aprogressively%20models%20the%20colon%20regions%20from%20easy%20to%20hard.%20Additionally%2C%20to%0Aovercome%20the%20challenges%20from%20sparse%20views%2C%20we%20devise%20a%20DensiNet%20module%20for%0Adensifying%20camera%20poses%20under%20the%20guidance%20of%20semantic%20consistency.%20We%20conduct%0Aextensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20to%20evaluate%20our%0AColonNeRF.%20Quantitatively%2C%20ColonNeRF%20exhibits%20a%2067%25-85%25%20increase%20in%20LPIPS-ALEX%0Ascores.%20Qualitatively%2C%20our%20reconstruction%20visualizations%20show%20much%20clearer%0Atextures%20and%20more%20accurate%20geometric%20details.%20These%20sufficiently%20demonstrate%0Aour%20superior%20performance%20over%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02015v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColonNeRF%3A%20High-Fidelity%20Neural%20Reconstruction%20of%20Long%20Colonoscopy&entry.906535625=Yufei%20Shi%20and%20Beijia%20Lu%20and%20Jia-Wei%20Liu%20and%20Ming%20Li%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Colonoscopy%20reconstruction%20is%20pivotal%20for%20diagnosing%20colorectal%20cancer.%0AHowever%2C%20accurate%20long-sequence%20colonoscopy%20reconstruction%20faces%20three%20major%0Achallenges%3A%20%281%29%20dissimilarity%20among%20segments%20of%20the%20colon%20due%20to%20its%20meandering%0Aand%20convoluted%20shape%3B%20%282%29%20co-existence%20of%20simple%20and%20intricately%20folded%0Ageometry%20structures%3B%20%283%29%20sparse%20viewpoints%20due%20to%20constrained%20camera%0Atrajectories.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20a%20new%20reconstruction%0Aframework%20based%20on%20neural%20radiance%20field%20%28NeRF%29%2C%20named%20ColonNeRF%2C%20which%0Aleverages%20neural%20rendering%20for%20novel%20view%20synthesis%20of%20long-sequence%0Acolonoscopy.%20Specifically%2C%20to%20reconstruct%20the%20entire%20colon%20in%20a%20piecewise%0Amanner%2C%20our%20ColonNeRF%20introduces%20a%20region%20division%20and%20integration%20module%2C%0Aeffectively%20reducing%20shape%20dissimilarity%20and%20ensuring%20geometric%20consistency%20in%0Aeach%20segment.%20To%20learn%20both%20the%20simple%20and%20complex%20geometry%20in%20a%20unified%0Aframework%2C%20our%20ColonNeRF%20incorporates%20a%20multi-level%20fusion%20module%20that%0Aprogressively%20models%20the%20colon%20regions%20from%20easy%20to%20hard.%20Additionally%2C%20to%0Aovercome%20the%20challenges%20from%20sparse%20views%2C%20we%20devise%20a%20DensiNet%20module%20for%0Adensifying%20camera%20poses%20under%20the%20guidance%20of%20semantic%20consistency.%20We%20conduct%0Aextensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20to%20evaluate%20our%0AColonNeRF.%20Quantitatively%2C%20ColonNeRF%20exhibits%20a%2067%25-85%25%20increase%20in%20LPIPS-ALEX%0Ascores.%20Qualitatively%2C%20our%20reconstruction%20visualizations%20show%20much%20clearer%0Atextures%20and%20more%20accurate%20geometric%20details.%20These%20sufficiently%20demonstrate%0Aour%20superior%20performance%20over%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02015v2&entry.124074799=Read"},
{"title": "Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against\n  Query-Based Attacks", "author": "Pascal Zimmer and S\u00e9bastien Andreina and Giorgia Azzurra Marson and Ghassan Karame", "abstract": "  Although promising, existing defenses against query-based attacks share a\ncommon limitation: they offer increased robustness against attacks at the price\nof a considerable accuracy drop on clean samples. In this work, we show how to\nefficiently establish, at test-time, a solid tradeoff between robustness and\naccuracy when mitigating query-based attacks. Given that these attacks\nnecessarily explore low-confidence regions, our insight is that activating\ndedicated defenses, such as random noise defense and random image\ntransformations, only for low-confidence inputs is sufficient to prevent them.\nOur approach is independent of training and supported by theory. We verify the\neffectiveness of our approach for various existing defenses by conducting\nextensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm\nthat our proposal can indeed enhance these defenses by providing better\ntradeoffs between robustness and accuracy when compared to state-of-the-art\napproaches while being completely training-free.\n", "link": "http://arxiv.org/abs/2312.10132v2", "date": "2024-03-21", "relevancy": 2.5653, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5245}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5081}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5066}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Gap%3A%20Achieving%20Better%20Accuracy-Robustness%20Tradeoffs%20against%0A%20%20Query-Based%20Attacks&body=Title%3A%20Closing%20the%20Gap%3A%20Achieving%20Better%20Accuracy-Robustness%20Tradeoffs%20against%0A%20%20Query-Based%20Attacks%0AAuthor%3A%20Pascal%20Zimmer%20and%20S%C3%A9bastien%20Andreina%20and%20Giorgia%20Azzurra%20Marson%20and%20Ghassan%20Karame%0AAbstract%3A%20%20%20Although%20promising%2C%20existing%20defenses%20against%20query-based%20attacks%20share%20a%0Acommon%20limitation%3A%20they%20offer%20increased%20robustness%20against%20attacks%20at%20the%20price%0Aof%20a%20considerable%20accuracy%20drop%20on%20clean%20samples.%20In%20this%20work%2C%20we%20show%20how%20to%0Aefficiently%20establish%2C%20at%20test-time%2C%20a%20solid%20tradeoff%20between%20robustness%20and%0Aaccuracy%20when%20mitigating%20query-based%20attacks.%20Given%20that%20these%20attacks%0Anecessarily%20explore%20low-confidence%20regions%2C%20our%20insight%20is%20that%20activating%0Adedicated%20defenses%2C%20such%20as%20random%20noise%20defense%20and%20random%20image%0Atransformations%2C%20only%20for%20low-confidence%20inputs%20is%20sufficient%20to%20prevent%20them.%0AOur%20approach%20is%20independent%20of%20training%20and%20supported%20by%20theory.%20We%20verify%20the%0Aeffectiveness%20of%20our%20approach%20for%20various%20existing%20defenses%20by%20conducting%0Aextensive%20experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet.%20Our%20results%20confirm%0Athat%20our%20proposal%20can%20indeed%20enhance%20these%20defenses%20by%20providing%20better%0Atradeoffs%20between%20robustness%20and%20accuracy%20when%20compared%20to%20state-of-the-art%0Aapproaches%20while%20being%20completely%20training-free.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10132v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Gap%3A%20Achieving%20Better%20Accuracy-Robustness%20Tradeoffs%20against%0A%20%20Query-Based%20Attacks&entry.906535625=Pascal%20Zimmer%20and%20S%C3%A9bastien%20Andreina%20and%20Giorgia%20Azzurra%20Marson%20and%20Ghassan%20Karame&entry.1292438233=%20%20Although%20promising%2C%20existing%20defenses%20against%20query-based%20attacks%20share%20a%0Acommon%20limitation%3A%20they%20offer%20increased%20robustness%20against%20attacks%20at%20the%20price%0Aof%20a%20considerable%20accuracy%20drop%20on%20clean%20samples.%20In%20this%20work%2C%20we%20show%20how%20to%0Aefficiently%20establish%2C%20at%20test-time%2C%20a%20solid%20tradeoff%20between%20robustness%20and%0Aaccuracy%20when%20mitigating%20query-based%20attacks.%20Given%20that%20these%20attacks%0Anecessarily%20explore%20low-confidence%20regions%2C%20our%20insight%20is%20that%20activating%0Adedicated%20defenses%2C%20such%20as%20random%20noise%20defense%20and%20random%20image%0Atransformations%2C%20only%20for%20low-confidence%20inputs%20is%20sufficient%20to%20prevent%20them.%0AOur%20approach%20is%20independent%20of%20training%20and%20supported%20by%20theory.%20We%20verify%20the%0Aeffectiveness%20of%20our%20approach%20for%20various%20existing%20defenses%20by%20conducting%0Aextensive%20experiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet.%20Our%20results%20confirm%0Athat%20our%20proposal%20can%20indeed%20enhance%20these%20defenses%20by%20providing%20better%0Atradeoffs%20between%20robustness%20and%20accuracy%20when%20compared%20to%20state-of-the-art%0Aapproaches%20while%20being%20completely%20training-free.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10132v2&entry.124074799=Read"},
{"title": "Loop Improvement: An Efficient Approach for Extracting Shared Features\n  from Heterogeneous Data without Central Server", "author": "Fei Li and Chu Kiong Loo and Wei Shiung Liew and Xiaofeng Liu", "abstract": "  In federated learning, data heterogeneity significantly impacts performance.\nA typical solution involves segregating these parameters into shared and\npersonalized components, a concept also relevant in multi-task learning.\nAddressing this, we propose \"Loop Improvement\" (LI), a novel method enhancing\nthis separation and feature extraction without necessitating a central server\nor data interchange among participants. Our experiments reveal LI's superiority\nin several aspects: In personalized federated learning environments, LI\nconsistently outperforms the advanced FedALA algorithm in accuracy across\ndiverse scenarios. Additionally, LI's feature extractor closely matches the\nperformance achieved when aggregating data from all clients. In global model\ncontexts, employing LI with stacked personalized layers and an additional\nnetwork also yields comparable results to combined client data scenarios.\nFurthermore, LI's adaptability extends to multi-task learning, streamlining the\nextraction of common features across tasks and obviating the need for\nsimultaneous training. This approach not only enhances individual task\nperformance but also achieves accuracy levels on par with classic multi-task\nlearning methods where all tasks are trained simultaneously. LI integrates a\nloop topology with layer-wise and end-to-end training, compatible with various\nneural network models. This paper also delves into the theoretical\nunderpinnings of LI's effectiveness, offering insights into its potential\napplications. The code is on https://github.com/axedge1983/LI\n", "link": "http://arxiv.org/abs/2403.14371v1", "date": "2024-03-21", "relevancy": 2.5558, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5192}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5097}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5046}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Loop%20Improvement%3A%20An%20Efficient%20Approach%20for%20Extracting%20Shared%20Features%0A%20%20from%20Heterogeneous%20Data%20without%20Central%20Server&body=Title%3A%20Loop%20Improvement%3A%20An%20Efficient%20Approach%20for%20Extracting%20Shared%20Features%0A%20%20from%20Heterogeneous%20Data%20without%20Central%20Server%0AAuthor%3A%20Fei%20Li%20and%20Chu%20Kiong%20Loo%20and%20Wei%20Shiung%20Liew%20and%20Xiaofeng%20Liu%0AAbstract%3A%20%20%20In%20federated%20learning%2C%20data%20heterogeneity%20significantly%20impacts%20performance.%0AA%20typical%20solution%20involves%20segregating%20these%20parameters%20into%20shared%20and%0Apersonalized%20components%2C%20a%20concept%20also%20relevant%20in%20multi-task%20learning.%0AAddressing%20this%2C%20we%20propose%20%22Loop%20Improvement%22%20%28LI%29%2C%20a%20novel%20method%20enhancing%0Athis%20separation%20and%20feature%20extraction%20without%20necessitating%20a%20central%20server%0Aor%20data%20interchange%20among%20participants.%20Our%20experiments%20reveal%20LI%27s%20superiority%0Ain%20several%20aspects%3A%20In%20personalized%20federated%20learning%20environments%2C%20LI%0Aconsistently%20outperforms%20the%20advanced%20FedALA%20algorithm%20in%20accuracy%20across%0Adiverse%20scenarios.%20Additionally%2C%20LI%27s%20feature%20extractor%20closely%20matches%20the%0Aperformance%20achieved%20when%20aggregating%20data%20from%20all%20clients.%20In%20global%20model%0Acontexts%2C%20employing%20LI%20with%20stacked%20personalized%20layers%20and%20an%20additional%0Anetwork%20also%20yields%20comparable%20results%20to%20combined%20client%20data%20scenarios.%0AFurthermore%2C%20LI%27s%20adaptability%20extends%20to%20multi-task%20learning%2C%20streamlining%20the%0Aextraction%20of%20common%20features%20across%20tasks%20and%20obviating%20the%20need%20for%0Asimultaneous%20training.%20This%20approach%20not%20only%20enhances%20individual%20task%0Aperformance%20but%20also%20achieves%20accuracy%20levels%20on%20par%20with%20classic%20multi-task%0Alearning%20methods%20where%20all%20tasks%20are%20trained%20simultaneously.%20LI%20integrates%20a%0Aloop%20topology%20with%20layer-wise%20and%20end-to-end%20training%2C%20compatible%20with%20various%0Aneural%20network%20models.%20This%20paper%20also%20delves%20into%20the%20theoretical%0Aunderpinnings%20of%20LI%27s%20effectiveness%2C%20offering%20insights%20into%20its%20potential%0Aapplications.%20The%20code%20is%20on%20https%3A//github.com/axedge1983/LI%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14371v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loop%20Improvement%3A%20An%20Efficient%20Approach%20for%20Extracting%20Shared%20Features%0A%20%20from%20Heterogeneous%20Data%20without%20Central%20Server&entry.906535625=Fei%20Li%20and%20Chu%20Kiong%20Loo%20and%20Wei%20Shiung%20Liew%20and%20Xiaofeng%20Liu&entry.1292438233=%20%20In%20federated%20learning%2C%20data%20heterogeneity%20significantly%20impacts%20performance.%0AA%20typical%20solution%20involves%20segregating%20these%20parameters%20into%20shared%20and%0Apersonalized%20components%2C%20a%20concept%20also%20relevant%20in%20multi-task%20learning.%0AAddressing%20this%2C%20we%20propose%20%22Loop%20Improvement%22%20%28LI%29%2C%20a%20novel%20method%20enhancing%0Athis%20separation%20and%20feature%20extraction%20without%20necessitating%20a%20central%20server%0Aor%20data%20interchange%20among%20participants.%20Our%20experiments%20reveal%20LI%27s%20superiority%0Ain%20several%20aspects%3A%20In%20personalized%20federated%20learning%20environments%2C%20LI%0Aconsistently%20outperforms%20the%20advanced%20FedALA%20algorithm%20in%20accuracy%20across%0Adiverse%20scenarios.%20Additionally%2C%20LI%27s%20feature%20extractor%20closely%20matches%20the%0Aperformance%20achieved%20when%20aggregating%20data%20from%20all%20clients.%20In%20global%20model%0Acontexts%2C%20employing%20LI%20with%20stacked%20personalized%20layers%20and%20an%20additional%0Anetwork%20also%20yields%20comparable%20results%20to%20combined%20client%20data%20scenarios.%0AFurthermore%2C%20LI%27s%20adaptability%20extends%20to%20multi-task%20learning%2C%20streamlining%20the%0Aextraction%20of%20common%20features%20across%20tasks%20and%20obviating%20the%20need%20for%0Asimultaneous%20training.%20This%20approach%20not%20only%20enhances%20individual%20task%0Aperformance%20but%20also%20achieves%20accuracy%20levels%20on%20par%20with%20classic%20multi-task%0Alearning%20methods%20where%20all%20tasks%20are%20trained%20simultaneously.%20LI%20integrates%20a%0Aloop%20topology%20with%20layer-wise%20and%20end-to-end%20training%2C%20compatible%20with%20various%0Aneural%20network%20models.%20This%20paper%20also%20delves%20into%20the%20theoretical%0Aunderpinnings%20of%20LI%27s%20effectiveness%2C%20offering%20insights%20into%20its%20potential%0Aapplications.%20The%20code%20is%20on%20https%3A//github.com/axedge1983/LI%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14371v1&entry.124074799=Read"},
{"title": "A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity\n  Identification", "author": "Seungkwon Kim and Sangyeon Kim and Seung-Hun Nam", "abstract": "  Portrait stylization is a challenging task involving the transformation of an\ninput portrait image into a specific style while preserving its inherent\ncharacteristics. The recent introduction of Stable Diffusion (SD) has\nsignificantly improved the quality of outcomes in this field. However, a\npractical stylization framework that can effectively filter harmful input\ncontent and preserve the distinct characteristics of an input, such as\nskin-tone, while maintaining the quality of stylization remains lacking. These\nchallenges have hindered the wide deployment of such a framework. To address\nthese issues, this study proposes a portrait stylization framework that\nincorporates a nudity content identification module (NCIM) and a\nskin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM\nshowed good performance in enhancing explicit content filtering, and STAPSM\naccurately represented a diverse range of skin tones. Our proposed framework\nhas been successfully deployed in practice, and it has effectively satisfied\ncritical requirements of real-world applications.\n", "link": "http://arxiv.org/abs/2403.14264v1", "date": "2024-03-21", "relevancy": 2.5444, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5277}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5153}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Portrait%20Stylization%20with%20Skin-Tone%20Awareness%20and%20Nudity%0A%20%20Identification&body=Title%3A%20A%20Framework%20for%20Portrait%20Stylization%20with%20Skin-Tone%20Awareness%20and%20Nudity%0A%20%20Identification%0AAuthor%3A%20Seungkwon%20Kim%20and%20Sangyeon%20Kim%20and%20Seung-Hun%20Nam%0AAbstract%3A%20%20%20Portrait%20stylization%20is%20a%20challenging%20task%20involving%20the%20transformation%20of%20an%0Ainput%20portrait%20image%20into%20a%20specific%20style%20while%20preserving%20its%20inherent%0Acharacteristics.%20The%20recent%20introduction%20of%20Stable%20Diffusion%20%28SD%29%20has%0Asignificantly%20improved%20the%20quality%20of%20outcomes%20in%20this%20field.%20However%2C%20a%0Apractical%20stylization%20framework%20that%20can%20effectively%20filter%20harmful%20input%0Acontent%20and%20preserve%20the%20distinct%20characteristics%20of%20an%20input%2C%20such%20as%0Askin-tone%2C%20while%20maintaining%20the%20quality%20of%20stylization%20remains%20lacking.%20These%0Achallenges%20have%20hindered%20the%20wide%20deployment%20of%20such%20a%20framework.%20To%20address%0Athese%20issues%2C%20this%20study%20proposes%20a%20portrait%20stylization%20framework%20that%0Aincorporates%20a%20nudity%20content%20identification%20module%20%28NCIM%29%20and%20a%0Askin-tone-aware%20portrait%20stylization%20module%20%28STAPSM%29.%20In%20experiments%2C%20NCIM%0Ashowed%20good%20performance%20in%20enhancing%20explicit%20content%20filtering%2C%20and%20STAPSM%0Aaccurately%20represented%20a%20diverse%20range%20of%20skin%20tones.%20Our%20proposed%20framework%0Ahas%20been%20successfully%20deployed%20in%20practice%2C%20and%20it%20has%20effectively%20satisfied%0Acritical%20requirements%20of%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14264v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Portrait%20Stylization%20with%20Skin-Tone%20Awareness%20and%20Nudity%0A%20%20Identification&entry.906535625=Seungkwon%20Kim%20and%20Sangyeon%20Kim%20and%20Seung-Hun%20Nam&entry.1292438233=%20%20Portrait%20stylization%20is%20a%20challenging%20task%20involving%20the%20transformation%20of%20an%0Ainput%20portrait%20image%20into%20a%20specific%20style%20while%20preserving%20its%20inherent%0Acharacteristics.%20The%20recent%20introduction%20of%20Stable%20Diffusion%20%28SD%29%20has%0Asignificantly%20improved%20the%20quality%20of%20outcomes%20in%20this%20field.%20However%2C%20a%0Apractical%20stylization%20framework%20that%20can%20effectively%20filter%20harmful%20input%0Acontent%20and%20preserve%20the%20distinct%20characteristics%20of%20an%20input%2C%20such%20as%0Askin-tone%2C%20while%20maintaining%20the%20quality%20of%20stylization%20remains%20lacking.%20These%0Achallenges%20have%20hindered%20the%20wide%20deployment%20of%20such%20a%20framework.%20To%20address%0Athese%20issues%2C%20this%20study%20proposes%20a%20portrait%20stylization%20framework%20that%0Aincorporates%20a%20nudity%20content%20identification%20module%20%28NCIM%29%20and%20a%0Askin-tone-aware%20portrait%20stylization%20module%20%28STAPSM%29.%20In%20experiments%2C%20NCIM%0Ashowed%20good%20performance%20in%20enhancing%20explicit%20content%20filtering%2C%20and%20STAPSM%0Aaccurately%20represented%20a%20diverse%20range%20of%20skin%20tones.%20Our%20proposed%20framework%0Ahas%20been%20successfully%20deployed%20in%20practice%2C%20and%20it%20has%20effectively%20satisfied%0Acritical%20requirements%20of%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14264v1&entry.124074799=Read"},
{"title": "Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion", "author": "Yuanxun Lu and Jingyang Zhang and Shiwei Li and Tian Fang and David McKinnon and Yanghai Tsin and Long Quan and Xun Cao and Yao Yao", "abstract": "  Recent advances in generative AI have unveiled significant potential for the\ncreation of 3D content. However, current methods either apply a pre-trained 2D\ndiffusion model with the time-consuming score distillation sampling (SDS), or a\ndirect 3D diffusion model trained on limited 3D data losing generation\ndiversity. In this work, we approach the problem by employing a multi-view 2.5D\ndiffusion fine-tuned from a pre-trained 2D diffusion model. The multi-view 2.5D\ndiffusion directly models the structural distribution of 3D data, while still\nmaintaining the strong generalization ability of the original 2D diffusion\nmodel, filling the gap between 2D diffusion-based and direct 3D diffusion-based\nmethods for 3D content generation. During inference, multi-view normal maps are\ngenerated using the 2.5D diffusion, and a novel differentiable rasterization\nscheme is introduced to fuse the almost consistent multi-view normal maps into\na consistent 3D model. We further design a normal-conditioned multi-view image\ngeneration module for fast appearance generation given the 3D geometry. Our\nmethod is a one-pass diffusion process and does not require any SDS\noptimization as post-processing. We demonstrate through extensive experiments\nthat, our direct 2.5D generation with the specially-designed fusion scheme can\nachieve diverse, mode-seeking-free, and high-fidelity 3D content generation in\nonly 10 seconds. Project page: https://nju-3dv.github.io/projects/direct25.\n", "link": "http://arxiv.org/abs/2311.15980v2", "date": "2024-03-21", "relevancy": 2.5408, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6503}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6273}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6232}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Direct2.5%3A%20Diverse%20Text-to-3D%20Generation%20via%20Multi-view%202.5D%20Diffusion&body=Title%3A%20Direct2.5%3A%20Diverse%20Text-to-3D%20Generation%20via%20Multi-view%202.5D%20Diffusion%0AAuthor%3A%20Yuanxun%20Lu%20and%20Jingyang%20Zhang%20and%20Shiwei%20Li%20and%20Tian%20Fang%20and%20David%20McKinnon%20and%20Yanghai%20Tsin%20and%20Long%20Quan%20and%20Xun%20Cao%20and%20Yao%20Yao%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20AI%20have%20unveiled%20significant%20potential%20for%20the%0Acreation%20of%203D%20content.%20However%2C%20current%20methods%20either%20apply%20a%20pre-trained%202D%0Adiffusion%20model%20with%20the%20time-consuming%20score%20distillation%20sampling%20%28SDS%29%2C%20or%20a%0Adirect%203D%20diffusion%20model%20trained%20on%20limited%203D%20data%20losing%20generation%0Adiversity.%20In%20this%20work%2C%20we%20approach%20the%20problem%20by%20employing%20a%20multi-view%202.5D%0Adiffusion%20fine-tuned%20from%20a%20pre-trained%202D%20diffusion%20model.%20The%20multi-view%202.5D%0Adiffusion%20directly%20models%20the%20structural%20distribution%20of%203D%20data%2C%20while%20still%0Amaintaining%20the%20strong%20generalization%20ability%20of%20the%20original%202D%20diffusion%0Amodel%2C%20filling%20the%20gap%20between%202D%20diffusion-based%20and%20direct%203D%20diffusion-based%0Amethods%20for%203D%20content%20generation.%20During%20inference%2C%20multi-view%20normal%20maps%20are%0Agenerated%20using%20the%202.5D%20diffusion%2C%20and%20a%20novel%20differentiable%20rasterization%0Ascheme%20is%20introduced%20to%20fuse%20the%20almost%20consistent%20multi-view%20normal%20maps%20into%0Aa%20consistent%203D%20model.%20We%20further%20design%20a%20normal-conditioned%20multi-view%20image%0Ageneration%20module%20for%20fast%20appearance%20generation%20given%20the%203D%20geometry.%20Our%0Amethod%20is%20a%20one-pass%20diffusion%20process%20and%20does%20not%20require%20any%20SDS%0Aoptimization%20as%20post-processing.%20We%20demonstrate%20through%20extensive%20experiments%0Athat%2C%20our%20direct%202.5D%20generation%20with%20the%20specially-designed%20fusion%20scheme%20can%0Aachieve%20diverse%2C%20mode-seeking-free%2C%20and%20high-fidelity%203D%20content%20generation%20in%0Aonly%2010%20seconds.%20Project%20page%3A%20https%3A//nju-3dv.github.io/projects/direct25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15980v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct2.5%3A%20Diverse%20Text-to-3D%20Generation%20via%20Multi-view%202.5D%20Diffusion&entry.906535625=Yuanxun%20Lu%20and%20Jingyang%20Zhang%20and%20Shiwei%20Li%20and%20Tian%20Fang%20and%20David%20McKinnon%20and%20Yanghai%20Tsin%20and%20Long%20Quan%20and%20Xun%20Cao%20and%20Yao%20Yao&entry.1292438233=%20%20Recent%20advances%20in%20generative%20AI%20have%20unveiled%20significant%20potential%20for%20the%0Acreation%20of%203D%20content.%20However%2C%20current%20methods%20either%20apply%20a%20pre-trained%202D%0Adiffusion%20model%20with%20the%20time-consuming%20score%20distillation%20sampling%20%28SDS%29%2C%20or%20a%0Adirect%203D%20diffusion%20model%20trained%20on%20limited%203D%20data%20losing%20generation%0Adiversity.%20In%20this%20work%2C%20we%20approach%20the%20problem%20by%20employing%20a%20multi-view%202.5D%0Adiffusion%20fine-tuned%20from%20a%20pre-trained%202D%20diffusion%20model.%20The%20multi-view%202.5D%0Adiffusion%20directly%20models%20the%20structural%20distribution%20of%203D%20data%2C%20while%20still%0Amaintaining%20the%20strong%20generalization%20ability%20of%20the%20original%202D%20diffusion%0Amodel%2C%20filling%20the%20gap%20between%202D%20diffusion-based%20and%20direct%203D%20diffusion-based%0Amethods%20for%203D%20content%20generation.%20During%20inference%2C%20multi-view%20normal%20maps%20are%0Agenerated%20using%20the%202.5D%20diffusion%2C%20and%20a%20novel%20differentiable%20rasterization%0Ascheme%20is%20introduced%20to%20fuse%20the%20almost%20consistent%20multi-view%20normal%20maps%20into%0Aa%20consistent%203D%20model.%20We%20further%20design%20a%20normal-conditioned%20multi-view%20image%0Ageneration%20module%20for%20fast%20appearance%20generation%20given%20the%203D%20geometry.%20Our%0Amethod%20is%20a%20one-pass%20diffusion%20process%20and%20does%20not%20require%20any%20SDS%0Aoptimization%20as%20post-processing.%20We%20demonstrate%20through%20extensive%20experiments%0Athat%2C%20our%20direct%202.5D%20generation%20with%20the%20specially-designed%20fusion%20scheme%20can%0Aachieve%20diverse%2C%20mode-seeking-free%2C%20and%20high-fidelity%203D%20content%20generation%20in%0Aonly%2010%20seconds.%20Project%20page%3A%20https%3A//nju-3dv.github.io/projects/direct25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15980v2&entry.124074799=Read"},
{"title": "CFPL-FAS: Class Free Prompt Learning for Generalizable Face\n  Anti-spoofing", "author": "Ajian Liu and Shuai Xue and Jianwen Gan and Jun Wan and Yanyan Liang and Jiankang Deng and Sergio Escalera and Zhen Lei", "abstract": "  Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the\nmodel's performance on unseen domains. Existing methods either rely on domain\nlabels to align domain-invariant feature spaces, or disentangle generalizable\nfeatures from the whole sample, which inevitably lead to the distortion of\nsemantic feature structures and achieve limited generalization. In this work,\nwe make use of large-scale VLMs like CLIP and leverage the textual feature to\ndynamically adjust the classifier's weights for exploring generalizable visual\nfeatures. Specifically, we propose a novel Class Free Prompt Learning (CFPL)\nparadigm for DG FAS, which utilizes two lightweight transformers, namely\nContent Q-Former (CQF) and Style Q-Former (SQF), to learn the different\nsemantic prompts conditioned on content and style features by using a set of\nlearnable query vectors, respectively. Thus, the generalizable prompt can be\nlearned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is\nintroduced to ensure CQF learns visual representation that is most informative\nof the content description. (2) A Diversified Style Prompt (DSP) technology is\nproposed to diversify the learning of style prompts by mixing feature\nstatistics between instance-specific styles. Finally, the learned text features\nmodulate visual features to generalization through the designed Prompt\nModulation (PM). Extensive experiments show that the CFPL is effective and\noutperforms the state-of-the-art methods on several cross-domain datasets.\n", "link": "http://arxiv.org/abs/2403.14333v1", "date": "2024-03-21", "relevancy": 2.5282, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5011}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4893}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CFPL-FAS%3A%20Class%20Free%20Prompt%20Learning%20for%20Generalizable%20Face%0A%20%20Anti-spoofing&body=Title%3A%20CFPL-FAS%3A%20Class%20Free%20Prompt%20Learning%20for%20Generalizable%20Face%0A%20%20Anti-spoofing%0AAuthor%3A%20Ajian%20Liu%20and%20Shuai%20Xue%20and%20Jianwen%20Gan%20and%20Jun%20Wan%20and%20Yanyan%20Liang%20and%20Jiankang%20Deng%20and%20Sergio%20Escalera%20and%20Zhen%20Lei%0AAbstract%3A%20%20%20Domain%20generalization%20%28DG%29%20based%20Face%20Anti-Spoofing%20%28FAS%29%20aims%20to%20improve%20the%0Amodel%27s%20performance%20on%20unseen%20domains.%20Existing%20methods%20either%20rely%20on%20domain%0Alabels%20to%20align%20domain-invariant%20feature%20spaces%2C%20or%20disentangle%20generalizable%0Afeatures%20from%20the%20whole%20sample%2C%20which%20inevitably%20lead%20to%20the%20distortion%20of%0Asemantic%20feature%20structures%20and%20achieve%20limited%20generalization.%20In%20this%20work%2C%0Awe%20make%20use%20of%20large-scale%20VLMs%20like%20CLIP%20and%20leverage%20the%20textual%20feature%20to%0Adynamically%20adjust%20the%20classifier%27s%20weights%20for%20exploring%20generalizable%20visual%0Afeatures.%20Specifically%2C%20we%20propose%20a%20novel%20Class%20Free%20Prompt%20Learning%20%28CFPL%29%0Aparadigm%20for%20DG%20FAS%2C%20which%20utilizes%20two%20lightweight%20transformers%2C%20namely%0AContent%20Q-Former%20%28CQF%29%20and%20Style%20Q-Former%20%28SQF%29%2C%20to%20learn%20the%20different%0Asemantic%20prompts%20conditioned%20on%20content%20and%20style%20features%20by%20using%20a%20set%20of%0Alearnable%20query%20vectors%2C%20respectively.%20Thus%2C%20the%20generalizable%20prompt%20can%20be%0Alearned%20by%20two%20improvements%3A%20%281%29%20A%20Prompt-Text%20Matched%20%28PTM%29%20supervision%20is%0Aintroduced%20to%20ensure%20CQF%20learns%20visual%20representation%20that%20is%20most%20informative%0Aof%20the%20content%20description.%20%282%29%20A%20Diversified%20Style%20Prompt%20%28DSP%29%20technology%20is%0Aproposed%20to%20diversify%20the%20learning%20of%20style%20prompts%20by%20mixing%20feature%0Astatistics%20between%20instance-specific%20styles.%20Finally%2C%20the%20learned%20text%20features%0Amodulate%20visual%20features%20to%20generalization%20through%20the%20designed%20Prompt%0AModulation%20%28PM%29.%20Extensive%20experiments%20show%20that%20the%20CFPL%20is%20effective%20and%0Aoutperforms%20the%20state-of-the-art%20methods%20on%20several%20cross-domain%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14333v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CFPL-FAS%3A%20Class%20Free%20Prompt%20Learning%20for%20Generalizable%20Face%0A%20%20Anti-spoofing&entry.906535625=Ajian%20Liu%20and%20Shuai%20Xue%20and%20Jianwen%20Gan%20and%20Jun%20Wan%20and%20Yanyan%20Liang%20and%20Jiankang%20Deng%20and%20Sergio%20Escalera%20and%20Zhen%20Lei&entry.1292438233=%20%20Domain%20generalization%20%28DG%29%20based%20Face%20Anti-Spoofing%20%28FAS%29%20aims%20to%20improve%20the%0Amodel%27s%20performance%20on%20unseen%20domains.%20Existing%20methods%20either%20rely%20on%20domain%0Alabels%20to%20align%20domain-invariant%20feature%20spaces%2C%20or%20disentangle%20generalizable%0Afeatures%20from%20the%20whole%20sample%2C%20which%20inevitably%20lead%20to%20the%20distortion%20of%0Asemantic%20feature%20structures%20and%20achieve%20limited%20generalization.%20In%20this%20work%2C%0Awe%20make%20use%20of%20large-scale%20VLMs%20like%20CLIP%20and%20leverage%20the%20textual%20feature%20to%0Adynamically%20adjust%20the%20classifier%27s%20weights%20for%20exploring%20generalizable%20visual%0Afeatures.%20Specifically%2C%20we%20propose%20a%20novel%20Class%20Free%20Prompt%20Learning%20%28CFPL%29%0Aparadigm%20for%20DG%20FAS%2C%20which%20utilizes%20two%20lightweight%20transformers%2C%20namely%0AContent%20Q-Former%20%28CQF%29%20and%20Style%20Q-Former%20%28SQF%29%2C%20to%20learn%20the%20different%0Asemantic%20prompts%20conditioned%20on%20content%20and%20style%20features%20by%20using%20a%20set%20of%0Alearnable%20query%20vectors%2C%20respectively.%20Thus%2C%20the%20generalizable%20prompt%20can%20be%0Alearned%20by%20two%20improvements%3A%20%281%29%20A%20Prompt-Text%20Matched%20%28PTM%29%20supervision%20is%0Aintroduced%20to%20ensure%20CQF%20learns%20visual%20representation%20that%20is%20most%20informative%0Aof%20the%20content%20description.%20%282%29%20A%20Diversified%20Style%20Prompt%20%28DSP%29%20technology%20is%0Aproposed%20to%20diversify%20the%20learning%20of%20style%20prompts%20by%20mixing%20feature%0Astatistics%20between%20instance-specific%20styles.%20Finally%2C%20the%20learned%20text%20features%0Amodulate%20visual%20features%20to%20generalization%20through%20the%20designed%20Prompt%0AModulation%20%28PM%29.%20Extensive%20experiments%20show%20that%20the%20CFPL%20is%20effective%20and%0Aoutperforms%20the%20state-of-the-art%20methods%20on%20several%20cross-domain%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14333v1&entry.124074799=Read"},
{"title": "ReNoise: Real Image Inversion Through Iterative Noising", "author": "Daniel Garibi and Or Patashnik and Andrey Voynov and Hadar Averbuch-Elor and Daniel Cohen-Or", "abstract": "  Recent advancements in text-guided diffusion models have unlocked powerful\nimage manipulation capabilities. However, applying these methods to real images\nnecessitates the inversion of the images into the domain of the pretrained\ndiffusion model. Achieving faithful inversion remains a challenge, particularly\nfor more recent models trained to generate images with a small number of\ndenoising steps. In this work, we introduce an inversion method with a high\nquality-to-operation ratio, enhancing reconstruction accuracy without\nincreasing the number of operations. Building on reversing the diffusion\nsampling process, our method employs an iterative renoising mechanism at each\ninversion sampling step. This mechanism refines the approximation of a\npredicted point along the forward diffusion trajectory, by iteratively applying\nthe pretrained diffusion model, and averaging these predictions. We evaluate\nthe performance of our ReNoise technique using various sampling algorithms and\nmodels, including recent accelerated diffusion models. Through comprehensive\nevaluations and comparisons, we show its effectiveness in terms of both\naccuracy and speed. Furthermore, we confirm that our method preserves\neditability by demonstrating text-driven image editing on real images.\n", "link": "http://arxiv.org/abs/2403.14602v1", "date": "2024-03-21", "relevancy": 2.5163, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6684}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6525}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5804}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ReNoise%3A%20Real%20Image%20Inversion%20Through%20Iterative%20Noising&body=Title%3A%20ReNoise%3A%20Real%20Image%20Inversion%20Through%20Iterative%20Noising%0AAuthor%3A%20Daniel%20Garibi%20and%20Or%20Patashnik%20and%20Andrey%20Voynov%20and%20Hadar%20Averbuch-Elor%20and%20Daniel%20Cohen-Or%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-guided%20diffusion%20models%20have%20unlocked%20powerful%0Aimage%20manipulation%20capabilities.%20However%2C%20applying%20these%20methods%20to%20real%20images%0Anecessitates%20the%20inversion%20of%20the%20images%20into%20the%20domain%20of%20the%20pretrained%0Adiffusion%20model.%20Achieving%20faithful%20inversion%20remains%20a%20challenge%2C%20particularly%0Afor%20more%20recent%20models%20trained%20to%20generate%20images%20with%20a%20small%20number%20of%0Adenoising%20steps.%20In%20this%20work%2C%20we%20introduce%20an%20inversion%20method%20with%20a%20high%0Aquality-to-operation%20ratio%2C%20enhancing%20reconstruction%20accuracy%20without%0Aincreasing%20the%20number%20of%20operations.%20Building%20on%20reversing%20the%20diffusion%0Asampling%20process%2C%20our%20method%20employs%20an%20iterative%20renoising%20mechanism%20at%20each%0Ainversion%20sampling%20step.%20This%20mechanism%20refines%20the%20approximation%20of%20a%0Apredicted%20point%20along%20the%20forward%20diffusion%20trajectory%2C%20by%20iteratively%20applying%0Athe%20pretrained%20diffusion%20model%2C%20and%20averaging%20these%20predictions.%20We%20evaluate%0Athe%20performance%20of%20our%20ReNoise%20technique%20using%20various%20sampling%20algorithms%20and%0Amodels%2C%20including%20recent%20accelerated%20diffusion%20models.%20Through%20comprehensive%0Aevaluations%20and%20comparisons%2C%20we%20show%20its%20effectiveness%20in%20terms%20of%20both%0Aaccuracy%20and%20speed.%20Furthermore%2C%20we%20confirm%20that%20our%20method%20preserves%0Aeditability%20by%20demonstrating%20text-driven%20image%20editing%20on%20real%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14602v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReNoise%3A%20Real%20Image%20Inversion%20Through%20Iterative%20Noising&entry.906535625=Daniel%20Garibi%20and%20Or%20Patashnik%20and%20Andrey%20Voynov%20and%20Hadar%20Averbuch-Elor%20and%20Daniel%20Cohen-Or&entry.1292438233=%20%20Recent%20advancements%20in%20text-guided%20diffusion%20models%20have%20unlocked%20powerful%0Aimage%20manipulation%20capabilities.%20However%2C%20applying%20these%20methods%20to%20real%20images%0Anecessitates%20the%20inversion%20of%20the%20images%20into%20the%20domain%20of%20the%20pretrained%0Adiffusion%20model.%20Achieving%20faithful%20inversion%20remains%20a%20challenge%2C%20particularly%0Afor%20more%20recent%20models%20trained%20to%20generate%20images%20with%20a%20small%20number%20of%0Adenoising%20steps.%20In%20this%20work%2C%20we%20introduce%20an%20inversion%20method%20with%20a%20high%0Aquality-to-operation%20ratio%2C%20enhancing%20reconstruction%20accuracy%20without%0Aincreasing%20the%20number%20of%20operations.%20Building%20on%20reversing%20the%20diffusion%0Asampling%20process%2C%20our%20method%20employs%20an%20iterative%20renoising%20mechanism%20at%20each%0Ainversion%20sampling%20step.%20This%20mechanism%20refines%20the%20approximation%20of%20a%0Apredicted%20point%20along%20the%20forward%20diffusion%20trajectory%2C%20by%20iteratively%20applying%0Athe%20pretrained%20diffusion%20model%2C%20and%20averaging%20these%20predictions.%20We%20evaluate%0Athe%20performance%20of%20our%20ReNoise%20technique%20using%20various%20sampling%20algorithms%20and%0Amodels%2C%20including%20recent%20accelerated%20diffusion%20models.%20Through%20comprehensive%0Aevaluations%20and%20comparisons%2C%20we%20show%20its%20effectiveness%20in%20terms%20of%20both%0Aaccuracy%20and%20speed.%20Furthermore%2C%20we%20confirm%20that%20our%20method%20preserves%0Aeditability%20by%20demonstrating%20text-driven%20image%20editing%20on%20real%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14602v1&entry.124074799=Read"},
{"title": "Enhancing Historical Image Retrieval with Compositional Cues", "author": "Tingyu Lin and Robert Sablatnig", "abstract": "  In analyzing vast amounts of digitally stored historical image data, existing\ncontent-based retrieval methods often overlook significant non-semantic\ninformation, limiting their effectiveness for flexible exploration across\nvaried themes. To broaden the applicability of image retrieval methods for\ndiverse purposes and uncover more general patterns, we innovatively introduce a\ncrucial factor from computational aesthetics, namely image composition, into\nthis topic. By explicitly integrating composition-related information extracted\nby CNN into the designed retrieval model, our method considers both the image's\ncomposition rules and semantic information. Qualitative and quantitative\nexperiments demonstrate that the image retrieval network guided by composition\ninformation outperforms those relying solely on content information,\nfacilitating the identification of images in databases closer to the target\nimage in human perception. Please visit https://github.com/linty5/CCBIR to try\nour codes.\n", "link": "http://arxiv.org/abs/2403.14287v1", "date": "2024-03-21", "relevancy": 2.4942, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5688}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4639}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4638}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Historical%20Image%20Retrieval%20with%20Compositional%20Cues&body=Title%3A%20Enhancing%20Historical%20Image%20Retrieval%20with%20Compositional%20Cues%0AAuthor%3A%20Tingyu%20Lin%20and%20Robert%20Sablatnig%0AAbstract%3A%20%20%20In%20analyzing%20vast%20amounts%20of%20digitally%20stored%20historical%20image%20data%2C%20existing%0Acontent-based%20retrieval%20methods%20often%20overlook%20significant%20non-semantic%0Ainformation%2C%20limiting%20their%20effectiveness%20for%20flexible%20exploration%20across%0Avaried%20themes.%20To%20broaden%20the%20applicability%20of%20image%20retrieval%20methods%20for%0Adiverse%20purposes%20and%20uncover%20more%20general%20patterns%2C%20we%20innovatively%20introduce%20a%0Acrucial%20factor%20from%20computational%20aesthetics%2C%20namely%20image%20composition%2C%20into%0Athis%20topic.%20By%20explicitly%20integrating%20composition-related%20information%20extracted%0Aby%20CNN%20into%20the%20designed%20retrieval%20model%2C%20our%20method%20considers%20both%20the%20image%27s%0Acomposition%20rules%20and%20semantic%20information.%20Qualitative%20and%20quantitative%0Aexperiments%20demonstrate%20that%20the%20image%20retrieval%20network%20guided%20by%20composition%0Ainformation%20outperforms%20those%20relying%20solely%20on%20content%20information%2C%0Afacilitating%20the%20identification%20of%20images%20in%20databases%20closer%20to%20the%20target%0Aimage%20in%20human%20perception.%20Please%20visit%20https%3A//github.com/linty5/CCBIR%20to%20try%0Aour%20codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14287v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Historical%20Image%20Retrieval%20with%20Compositional%20Cues&entry.906535625=Tingyu%20Lin%20and%20Robert%20Sablatnig&entry.1292438233=%20%20In%20analyzing%20vast%20amounts%20of%20digitally%20stored%20historical%20image%20data%2C%20existing%0Acontent-based%20retrieval%20methods%20often%20overlook%20significant%20non-semantic%0Ainformation%2C%20limiting%20their%20effectiveness%20for%20flexible%20exploration%20across%0Avaried%20themes.%20To%20broaden%20the%20applicability%20of%20image%20retrieval%20methods%20for%0Adiverse%20purposes%20and%20uncover%20more%20general%20patterns%2C%20we%20innovatively%20introduce%20a%0Acrucial%20factor%20from%20computational%20aesthetics%2C%20namely%20image%20composition%2C%20into%0Athis%20topic.%20By%20explicitly%20integrating%20composition-related%20information%20extracted%0Aby%20CNN%20into%20the%20designed%20retrieval%20model%2C%20our%20method%20considers%20both%20the%20image%27s%0Acomposition%20rules%20and%20semantic%20information.%20Qualitative%20and%20quantitative%0Aexperiments%20demonstrate%20that%20the%20image%20retrieval%20network%20guided%20by%20composition%0Ainformation%20outperforms%20those%20relying%20solely%20on%20content%20information%2C%0Afacilitating%20the%20identification%20of%20images%20in%20databases%20closer%20to%20the%20target%0Aimage%20in%20human%20perception.%20Please%20visit%20https%3A//github.com/linty5/CCBIR%20to%20try%0Aour%20codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14287v1&entry.124074799=Read"},
{"title": "Deep Classifier Mimicry without Data Access", "author": "Steven Braun and Martin Mundt and Kristian Kersting", "abstract": "  Access to pre-trained models has recently emerged as a standard across\nnumerous machine learning domains. Unfortunately, access to the original data\nthe models were trained on may not equally be granted. This makes it\ntremendously challenging to fine-tune, compress models, adapt continually, or\nto do any other type of data-driven update. We posit that original data access\nmay however not be required. Specifically, we propose Contrastive Abductive\nKnowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure\nthat mimics deep classifiers without access to the original data. To this end,\nCAKE generates pairs of noisy synthetic samples and diffuses them contrastively\ntoward a model's decision boundary. We empirically corroborate CAKE's\neffectiveness using several benchmark datasets and various architectural\nchoices, paving the way for broad application.\n", "link": "http://arxiv.org/abs/2306.02090v3", "date": "2024-03-21", "relevancy": 2.4892, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5062}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Classifier%20Mimicry%20without%20Data%20Access&body=Title%3A%20Deep%20Classifier%20Mimicry%20without%20Data%20Access%0AAuthor%3A%20Steven%20Braun%20and%20Martin%20Mundt%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Access%20to%20pre-trained%20models%20has%20recently%20emerged%20as%20a%20standard%20across%0Anumerous%20machine%20learning%20domains.%20Unfortunately%2C%20access%20to%20the%20original%20data%0Athe%20models%20were%20trained%20on%20may%20not%20equally%20be%20granted.%20This%20makes%20it%0Atremendously%20challenging%20to%20fine-tune%2C%20compress%20models%2C%20adapt%20continually%2C%20or%0Ato%20do%20any%20other%20type%20of%20data-driven%20update.%20We%20posit%20that%20original%20data%20access%0Amay%20however%20not%20be%20required.%20Specifically%2C%20we%20propose%20Contrastive%20Abductive%0AKnowledge%20Extraction%20%28CAKE%29%2C%20a%20model-agnostic%20knowledge%20distillation%20procedure%0Athat%20mimics%20deep%20classifiers%20without%20access%20to%20the%20original%20data.%20To%20this%20end%2C%0ACAKE%20generates%20pairs%20of%20noisy%20synthetic%20samples%20and%20diffuses%20them%20contrastively%0Atoward%20a%20model%27s%20decision%20boundary.%20We%20empirically%20corroborate%20CAKE%27s%0Aeffectiveness%20using%20several%20benchmark%20datasets%20and%20various%20architectural%0Achoices%2C%20paving%20the%20way%20for%20broad%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02090v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Classifier%20Mimicry%20without%20Data%20Access&entry.906535625=Steven%20Braun%20and%20Martin%20Mundt%20and%20Kristian%20Kersting&entry.1292438233=%20%20Access%20to%20pre-trained%20models%20has%20recently%20emerged%20as%20a%20standard%20across%0Anumerous%20machine%20learning%20domains.%20Unfortunately%2C%20access%20to%20the%20original%20data%0Athe%20models%20were%20trained%20on%20may%20not%20equally%20be%20granted.%20This%20makes%20it%0Atremendously%20challenging%20to%20fine-tune%2C%20compress%20models%2C%20adapt%20continually%2C%20or%0Ato%20do%20any%20other%20type%20of%20data-driven%20update.%20We%20posit%20that%20original%20data%20access%0Amay%20however%20not%20be%20required.%20Specifically%2C%20we%20propose%20Contrastive%20Abductive%0AKnowledge%20Extraction%20%28CAKE%29%2C%20a%20model-agnostic%20knowledge%20distillation%20procedure%0Athat%20mimics%20deep%20classifiers%20without%20access%20to%20the%20original%20data.%20To%20this%20end%2C%0ACAKE%20generates%20pairs%20of%20noisy%20synthetic%20samples%20and%20diffuses%20them%20contrastively%0Atoward%20a%20model%27s%20decision%20boundary.%20We%20empirically%20corroborate%20CAKE%27s%0Aeffectiveness%20using%20several%20benchmark%20datasets%20and%20various%20architectural%0Achoices%2C%20paving%20the%20way%20for%20broad%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02090v3&entry.124074799=Read"},
{"title": "AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks", "author": "Max Ku and Cong Wei and Weiming Ren and Huan Yang and Wenhu Chen", "abstract": "  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n", "link": "http://arxiv.org/abs/2403.14468v1", "date": "2024-03-21", "relevancy": 2.4806, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6542}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6346}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5803}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AnyV2V%3A%20A%20Plug-and-Play%20Framework%20For%20Any%20Video-to-Video%20Editing%20Tasks&body=Title%3A%20AnyV2V%3A%20A%20Plug-and-Play%20Framework%20For%20Any%20Video-to-Video%20Editing%20Tasks%0AAuthor%3A%20Max%20Ku%20and%20Cong%20Wei%20and%20Weiming%20Ren%20and%20Huan%20Yang%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Video-to-video%20editing%20involves%20editing%20a%20source%20video%20along%20with%20additional%0Acontrol%20%28such%20as%20text%20prompts%2C%20subjects%2C%20or%20styles%29%20to%20generate%20a%20new%20video%0Athat%20aligns%20with%20the%20source%20video%20and%20the%20provided%20control.%20Traditional%20methods%0Ahave%20been%20constrained%20to%20certain%20editing%20types%2C%20limiting%20their%20ability%20to%20meet%0Athe%20wide%20range%20of%20user%20demands.%20In%20this%20paper%2C%20we%20introduce%20AnyV2V%2C%20a%20novel%0Atraining-free%20framework%20designed%20to%20simplify%20video%20editing%20into%20two%20primary%0Asteps%3A%20%281%29%20employing%20an%20off-the-shelf%20image%20editing%20model%20%28e.g.%0AInstructPix2Pix%2C%20InstantID%2C%20etc%29%20to%20modify%20the%20first%20frame%2C%20%282%29%20utilizing%20an%0Aexisting%20image-to-video%20generation%20model%20%28e.g.%20I2VGen-XL%29%20for%20DDIM%20inversion%0Aand%20feature%20injection.%20In%20the%20first%20stage%2C%20AnyV2V%20can%20plug%20in%20any%20existing%0Aimage%20editing%20tools%20to%20support%20an%20extensive%20array%20of%20video%20editing%20tasks.%0ABeyond%20the%20traditional%20prompt-based%20editing%20methods%2C%20AnyV2V%20also%20can%20support%0Anovel%20video%20editing%20tasks%2C%20including%20reference-based%20style%20transfer%2C%0Asubject-driven%20editing%2C%20and%20identity%20manipulation%2C%20which%20were%20unattainable%20by%0Aprevious%20methods.%20In%20the%20second%20stage%2C%20AnyV2V%20can%20plug%20in%20any%20existing%0Aimage-to-video%20models%20to%20perform%20DDIM%20inversion%20and%20intermediate%20feature%0Ainjection%20to%20maintain%20the%20appearance%20and%20motion%20consistency%20with%20the%20source%0Avideo.%20On%20the%20prompt-based%20editing%2C%20we%20show%20that%20AnyV2V%20can%20outperform%20the%0Aprevious%20best%20approach%20by%2035%5C%25%20on%20prompt%20alignment%2C%20and%2025%5C%25%20on%20human%0Apreference.%20On%20the%20three%20novel%20tasks%2C%20we%20show%20that%20AnyV2V%20also%20achieves%20a%20high%0Asuccess%20rate.%20We%20believe%20AnyV2V%20will%20continue%20to%20thrive%20due%20to%20its%20ability%20to%0Aseamlessly%20integrate%20the%20fast-evolving%20image%20editing%20methods.%20Such%0Acompatibility%20can%20help%20AnyV2V%20to%20increase%20its%20versatility%20to%20cater%20to%20diverse%0Auser%20demands.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14468v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyV2V%3A%20A%20Plug-and-Play%20Framework%20For%20Any%20Video-to-Video%20Editing%20Tasks&entry.906535625=Max%20Ku%20and%20Cong%20Wei%20and%20Weiming%20Ren%20and%20Huan%20Yang%20and%20Wenhu%20Chen&entry.1292438233=%20%20Video-to-video%20editing%20involves%20editing%20a%20source%20video%20along%20with%20additional%0Acontrol%20%28such%20as%20text%20prompts%2C%20subjects%2C%20or%20styles%29%20to%20generate%20a%20new%20video%0Athat%20aligns%20with%20the%20source%20video%20and%20the%20provided%20control.%20Traditional%20methods%0Ahave%20been%20constrained%20to%20certain%20editing%20types%2C%20limiting%20their%20ability%20to%20meet%0Athe%20wide%20range%20of%20user%20demands.%20In%20this%20paper%2C%20we%20introduce%20AnyV2V%2C%20a%20novel%0Atraining-free%20framework%20designed%20to%20simplify%20video%20editing%20into%20two%20primary%0Asteps%3A%20%281%29%20employing%20an%20off-the-shelf%20image%20editing%20model%20%28e.g.%0AInstructPix2Pix%2C%20InstantID%2C%20etc%29%20to%20modify%20the%20first%20frame%2C%20%282%29%20utilizing%20an%0Aexisting%20image-to-video%20generation%20model%20%28e.g.%20I2VGen-XL%29%20for%20DDIM%20inversion%0Aand%20feature%20injection.%20In%20the%20first%20stage%2C%20AnyV2V%20can%20plug%20in%20any%20existing%0Aimage%20editing%20tools%20to%20support%20an%20extensive%20array%20of%20video%20editing%20tasks.%0ABeyond%20the%20traditional%20prompt-based%20editing%20methods%2C%20AnyV2V%20also%20can%20support%0Anovel%20video%20editing%20tasks%2C%20including%20reference-based%20style%20transfer%2C%0Asubject-driven%20editing%2C%20and%20identity%20manipulation%2C%20which%20were%20unattainable%20by%0Aprevious%20methods.%20In%20the%20second%20stage%2C%20AnyV2V%20can%20plug%20in%20any%20existing%0Aimage-to-video%20models%20to%20perform%20DDIM%20inversion%20and%20intermediate%20feature%0Ainjection%20to%20maintain%20the%20appearance%20and%20motion%20consistency%20with%20the%20source%0Avideo.%20On%20the%20prompt-based%20editing%2C%20we%20show%20that%20AnyV2V%20can%20outperform%20the%0Aprevious%20best%20approach%20by%2035%5C%25%20on%20prompt%20alignment%2C%20and%2025%5C%25%20on%20human%0Apreference.%20On%20the%20three%20novel%20tasks%2C%20we%20show%20that%20AnyV2V%20also%20achieves%20a%20high%0Asuccess%20rate.%20We%20believe%20AnyV2V%20will%20continue%20to%20thrive%20due%20to%20its%20ability%20to%0Aseamlessly%20integrate%20the%20fast-evolving%20image%20editing%20methods.%20Such%0Acompatibility%20can%20help%20AnyV2V%20to%20increase%20its%20versatility%20to%20cater%20to%20diverse%0Auser%20demands.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14468v1&entry.124074799=Read"},
{"title": "Learning to Project for Cross-Task Knowledge Distillation", "author": "Dylan Auty and Roy Miles and Benedikt Kolbeinsson and Krystian Mikolajczyk", "abstract": "  Traditional knowledge distillation (KD) relies on a proficient teacher\ntrained on the target task, which is not always available. In this setting,\ncross-task distillation can be used, enabling the use of any teacher model\ntrained on a different task. However, many KD methods prove ineffective when\napplied to this cross-task setting. To address this limitation, we propose a\nsimple modification: the use of an inverted projection. We show that this\ndrop-in replacement for a standard projector is effective by learning to\ndisregard any task-specific features which might degrade the student's\nperformance. We find that this simple modification is sufficient for extending\nmany KD methods to the cross-task setting, where the teacher and student tasks\ncan be very different. In doing so, we obtain up to a 1.9% improvement in the\ncross-task setting compared to the traditional projection, at no additional\ncost. Our method can obtain significant performance improvements (up to 7%)\nwhen using even a randomly-initialised teacher on various tasks such as depth\nestimation, image translation, and semantic segmentation, despite the lack of\nany learned knowledge to transfer. To provide conceptual and analytical\ninsights into this result, we show that using an inverted projection allows the\ndistillation loss to be decomposed into a knowledge transfer and a spectral\nregularisation component. Through this analysis we are additionally able to\npropose a novel regularisation loss that allows teacher-free distillation,\nenabling performance improvements of up to 8.57% on ImageNet with no additional\ntraining costs.\n", "link": "http://arxiv.org/abs/2403.14494v1", "date": "2024-03-21", "relevancy": 2.4793, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.497}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Project%20for%20Cross-Task%20Knowledge%20Distillation&body=Title%3A%20Learning%20to%20Project%20for%20Cross-Task%20Knowledge%20Distillation%0AAuthor%3A%20Dylan%20Auty%20and%20Roy%20Miles%20and%20Benedikt%20Kolbeinsson%20and%20Krystian%20Mikolajczyk%0AAbstract%3A%20%20%20Traditional%20knowledge%20distillation%20%28KD%29%20relies%20on%20a%20proficient%20teacher%0Atrained%20on%20the%20target%20task%2C%20which%20is%20not%20always%20available.%20In%20this%20setting%2C%0Across-task%20distillation%20can%20be%20used%2C%20enabling%20the%20use%20of%20any%20teacher%20model%0Atrained%20on%20a%20different%20task.%20However%2C%20many%20KD%20methods%20prove%20ineffective%20when%0Aapplied%20to%20this%20cross-task%20setting.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Asimple%20modification%3A%20the%20use%20of%20an%20inverted%20projection.%20We%20show%20that%20this%0Adrop-in%20replacement%20for%20a%20standard%20projector%20is%20effective%20by%20learning%20to%0Adisregard%20any%20task-specific%20features%20which%20might%20degrade%20the%20student%27s%0Aperformance.%20We%20find%20that%20this%20simple%20modification%20is%20sufficient%20for%20extending%0Amany%20KD%20methods%20to%20the%20cross-task%20setting%2C%20where%20the%20teacher%20and%20student%20tasks%0Acan%20be%20very%20different.%20In%20doing%20so%2C%20we%20obtain%20up%20to%20a%201.9%25%20improvement%20in%20the%0Across-task%20setting%20compared%20to%20the%20traditional%20projection%2C%20at%20no%20additional%0Acost.%20Our%20method%20can%20obtain%20significant%20performance%20improvements%20%28up%20to%207%25%29%0Awhen%20using%20even%20a%20randomly-initialised%20teacher%20on%20various%20tasks%20such%20as%20depth%0Aestimation%2C%20image%20translation%2C%20and%20semantic%20segmentation%2C%20despite%20the%20lack%20of%0Aany%20learned%20knowledge%20to%20transfer.%20To%20provide%20conceptual%20and%20analytical%0Ainsights%20into%20this%20result%2C%20we%20show%20that%20using%20an%20inverted%20projection%20allows%20the%0Adistillation%20loss%20to%20be%20decomposed%20into%20a%20knowledge%20transfer%20and%20a%20spectral%0Aregularisation%20component.%20Through%20this%20analysis%20we%20are%20additionally%20able%20to%0Apropose%20a%20novel%20regularisation%20loss%20that%20allows%20teacher-free%20distillation%2C%0Aenabling%20performance%20improvements%20of%20up%20to%208.57%25%20on%20ImageNet%20with%20no%20additional%0Atraining%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14494v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Project%20for%20Cross-Task%20Knowledge%20Distillation&entry.906535625=Dylan%20Auty%20and%20Roy%20Miles%20and%20Benedikt%20Kolbeinsson%20and%20Krystian%20Mikolajczyk&entry.1292438233=%20%20Traditional%20knowledge%20distillation%20%28KD%29%20relies%20on%20a%20proficient%20teacher%0Atrained%20on%20the%20target%20task%2C%20which%20is%20not%20always%20available.%20In%20this%20setting%2C%0Across-task%20distillation%20can%20be%20used%2C%20enabling%20the%20use%20of%20any%20teacher%20model%0Atrained%20on%20a%20different%20task.%20However%2C%20many%20KD%20methods%20prove%20ineffective%20when%0Aapplied%20to%20this%20cross-task%20setting.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Asimple%20modification%3A%20the%20use%20of%20an%20inverted%20projection.%20We%20show%20that%20this%0Adrop-in%20replacement%20for%20a%20standard%20projector%20is%20effective%20by%20learning%20to%0Adisregard%20any%20task-specific%20features%20which%20might%20degrade%20the%20student%27s%0Aperformance.%20We%20find%20that%20this%20simple%20modification%20is%20sufficient%20for%20extending%0Amany%20KD%20methods%20to%20the%20cross-task%20setting%2C%20where%20the%20teacher%20and%20student%20tasks%0Acan%20be%20very%20different.%20In%20doing%20so%2C%20we%20obtain%20up%20to%20a%201.9%25%20improvement%20in%20the%0Across-task%20setting%20compared%20to%20the%20traditional%20projection%2C%20at%20no%20additional%0Acost.%20Our%20method%20can%20obtain%20significant%20performance%20improvements%20%28up%20to%207%25%29%0Awhen%20using%20even%20a%20randomly-initialised%20teacher%20on%20various%20tasks%20such%20as%20depth%0Aestimation%2C%20image%20translation%2C%20and%20semantic%20segmentation%2C%20despite%20the%20lack%20of%0Aany%20learned%20knowledge%20to%20transfer.%20To%20provide%20conceptual%20and%20analytical%0Ainsights%20into%20this%20result%2C%20we%20show%20that%20using%20an%20inverted%20projection%20allows%20the%0Adistillation%20loss%20to%20be%20decomposed%20into%20a%20knowledge%20transfer%20and%20a%20spectral%0Aregularisation%20component.%20Through%20this%20analysis%20we%20are%20additionally%20able%20to%0Apropose%20a%20novel%20regularisation%20loss%20that%20allows%20teacher-free%20distillation%2C%0Aenabling%20performance%20improvements%20of%20up%20to%208.57%25%20on%20ImageNet%20with%20no%20additional%0Atraining%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14494v1&entry.124074799=Read"},
{"title": "Enhanced Few-Shot Class-Incremental Learning via Ensemble Models", "author": "Mingli Zhu and Zihao Zhu and Sihong Chen and Chen Chen and Baoyuan Wu", "abstract": "  Few-shot class-incremental learning (FSCIL) aims to continually fit new\nclasses with limited training data, while maintaining the performance of\npreviously learned classes. The main challenges are overfitting the rare new\ntraining samples and forgetting old classes. While catastrophic forgetting has\nbeen extensively studied, the overfitting problem has attracted less attention\nin FSCIL. To tackle overfitting challenge, we design a new ensemble model\nframework cooperated with data augmentation to boost generalization. In this\nway, the enhanced model works as a library storing abundant features to\nguarantee fast adaptation to downstream tasks. Specifically, the multi-input\nmulti-output ensemble structure is applied with a spatial-aware data\naugmentation strategy, aiming at diversifying the feature extractor and\nalleviating overfitting in incremental sessions. Moreover, self-supervised\nlearning is also integrated to further improve the model generalization.\nComprehensive experimental results show that the proposed method can indeed\nmitigate the overfitting problem in FSCIL, and outperform the state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2401.07208v2", "date": "2024-03-21", "relevancy": 2.4791, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5089}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4911}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4874}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Few-Shot%20Class-Incremental%20Learning%20via%20Ensemble%20Models&body=Title%3A%20Enhanced%20Few-Shot%20Class-Incremental%20Learning%20via%20Ensemble%20Models%0AAuthor%3A%20Mingli%20Zhu%20and%20Zihao%20Zhu%20and%20Sihong%20Chen%20and%20Chen%20Chen%20and%20Baoyuan%20Wu%0AAbstract%3A%20%20%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20aims%20to%20continually%20fit%20new%0Aclasses%20with%20limited%20training%20data%2C%20while%20maintaining%20the%20performance%20of%0Apreviously%20learned%20classes.%20The%20main%20challenges%20are%20overfitting%20the%20rare%20new%0Atraining%20samples%20and%20forgetting%20old%20classes.%20While%20catastrophic%20forgetting%20has%0Abeen%20extensively%20studied%2C%20the%20overfitting%20problem%20has%20attracted%20less%20attention%0Ain%20FSCIL.%20To%20tackle%20overfitting%20challenge%2C%20we%20design%20a%20new%20ensemble%20model%0Aframework%20cooperated%20with%20data%20augmentation%20to%20boost%20generalization.%20In%20this%0Away%2C%20the%20enhanced%20model%20works%20as%20a%20library%20storing%20abundant%20features%20to%0Aguarantee%20fast%20adaptation%20to%20downstream%20tasks.%20Specifically%2C%20the%20multi-input%0Amulti-output%20ensemble%20structure%20is%20applied%20with%20a%20spatial-aware%20data%0Aaugmentation%20strategy%2C%20aiming%20at%20diversifying%20the%20feature%20extractor%20and%0Aalleviating%20overfitting%20in%20incremental%20sessions.%20Moreover%2C%20self-supervised%0Alearning%20is%20also%20integrated%20to%20further%20improve%20the%20model%20generalization.%0AComprehensive%20experimental%20results%20show%20that%20the%20proposed%20method%20can%20indeed%0Amitigate%20the%20overfitting%20problem%20in%20FSCIL%2C%20and%20outperform%20the%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07208v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Few-Shot%20Class-Incremental%20Learning%20via%20Ensemble%20Models&entry.906535625=Mingli%20Zhu%20and%20Zihao%20Zhu%20and%20Sihong%20Chen%20and%20Chen%20Chen%20and%20Baoyuan%20Wu&entry.1292438233=%20%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20aims%20to%20continually%20fit%20new%0Aclasses%20with%20limited%20training%20data%2C%20while%20maintaining%20the%20performance%20of%0Apreviously%20learned%20classes.%20The%20main%20challenges%20are%20overfitting%20the%20rare%20new%0Atraining%20samples%20and%20forgetting%20old%20classes.%20While%20catastrophic%20forgetting%20has%0Abeen%20extensively%20studied%2C%20the%20overfitting%20problem%20has%20attracted%20less%20attention%0Ain%20FSCIL.%20To%20tackle%20overfitting%20challenge%2C%20we%20design%20a%20new%20ensemble%20model%0Aframework%20cooperated%20with%20data%20augmentation%20to%20boost%20generalization.%20In%20this%0Away%2C%20the%20enhanced%20model%20works%20as%20a%20library%20storing%20abundant%20features%20to%0Aguarantee%20fast%20adaptation%20to%20downstream%20tasks.%20Specifically%2C%20the%20multi-input%0Amulti-output%20ensemble%20structure%20is%20applied%20with%20a%20spatial-aware%20data%0Aaugmentation%20strategy%2C%20aiming%20at%20diversifying%20the%20feature%20extractor%20and%0Aalleviating%20overfitting%20in%20incremental%20sessions.%20Moreover%2C%20self-supervised%0Alearning%20is%20also%20integrated%20to%20further%20improve%20the%20model%20generalization.%0AComprehensive%20experimental%20results%20show%20that%20the%20proposed%20method%20can%20indeed%0Amitigate%20the%20overfitting%20problem%20in%20FSCIL%2C%20and%20outperform%20the%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07208v2&entry.124074799=Read"},
{"title": "A Bag of Tricks for Few-Shot Class-Incremental Learning", "author": "Shuvendu Roy and Chunjong Park and Aldi Fahrezi and Ali Etemad", "abstract": "  We present a bag of tricks framework for few-shot class-incremental learning\n(FSCIL), which is a challenging form of continual learning that involves\ncontinuous adaptation to new tasks with limited samples. FSCIL requires both\nstability and adaptability, i.e., preserving proficiency in previously learned\ntasks while learning new ones. Our proposed bag of tricks brings together eight\nkey and highly influential techniques that improve stability, adaptability, and\noverall performance under a unified framework for FSCIL. We organize these\ntricks into three categories: stability tricks, adaptability tricks, and\ntraining tricks. Stability tricks aim to mitigate the forgetting of previously\nlearned classes by enhancing the separation between the embeddings of learned\nclasses and minimizing interference when learning new ones. On the other hand,\nadaptability tricks focus on the effective learning of new classes. Finally,\ntraining tricks improve the overall performance without compromising stability\nor adaptability. We perform extensive experiments on three benchmark datasets,\nCIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed\nframework. Our detailed analysis shows that our approach substantially improves\nboth stability and adaptability, establishing a new state-of-the-art by\noutperforming prior works in the area. We believe our method provides a go-to\nsolution and establishes a robust baseline for future research in this area.\n", "link": "http://arxiv.org/abs/2403.14392v1", "date": "2024-03-21", "relevancy": 2.4731, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4968}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4961}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4909}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Bag%20of%20Tricks%20for%20Few-Shot%20Class-Incremental%20Learning&body=Title%3A%20A%20Bag%20of%20Tricks%20for%20Few-Shot%20Class-Incremental%20Learning%0AAuthor%3A%20Shuvendu%20Roy%20and%20Chunjong%20Park%20and%20Aldi%20Fahrezi%20and%20Ali%20Etemad%0AAbstract%3A%20%20%20We%20present%20a%20bag%20of%20tricks%20framework%20for%20few-shot%20class-incremental%20learning%0A%28FSCIL%29%2C%20which%20is%20a%20challenging%20form%20of%20continual%20learning%20that%20involves%0Acontinuous%20adaptation%20to%20new%20tasks%20with%20limited%20samples.%20FSCIL%20requires%20both%0Astability%20and%20adaptability%2C%20i.e.%2C%20preserving%20proficiency%20in%20previously%20learned%0Atasks%20while%20learning%20new%20ones.%20Our%20proposed%20bag%20of%20tricks%20brings%20together%20eight%0Akey%20and%20highly%20influential%20techniques%20that%20improve%20stability%2C%20adaptability%2C%20and%0Aoverall%20performance%20under%20a%20unified%20framework%20for%20FSCIL.%20We%20organize%20these%0Atricks%20into%20three%20categories%3A%20stability%20tricks%2C%20adaptability%20tricks%2C%20and%0Atraining%20tricks.%20Stability%20tricks%20aim%20to%20mitigate%20the%20forgetting%20of%20previously%0Alearned%20classes%20by%20enhancing%20the%20separation%20between%20the%20embeddings%20of%20learned%0Aclasses%20and%20minimizing%20interference%20when%20learning%20new%20ones.%20On%20the%20other%20hand%2C%0Aadaptability%20tricks%20focus%20on%20the%20effective%20learning%20of%20new%20classes.%20Finally%2C%0Atraining%20tricks%20improve%20the%20overall%20performance%20without%20compromising%20stability%0Aor%20adaptability.%20We%20perform%20extensive%20experiments%20on%20three%20benchmark%20datasets%2C%0ACIFAR-100%2C%20CUB-200%2C%20and%20miniIMageNet%2C%20to%20evaluate%20the%20impact%20of%20our%20proposed%0Aframework.%20Our%20detailed%20analysis%20shows%20that%20our%20approach%20substantially%20improves%0Aboth%20stability%20and%20adaptability%2C%20establishing%20a%20new%20state-of-the-art%20by%0Aoutperforming%20prior%20works%20in%20the%20area.%20We%20believe%20our%20method%20provides%20a%20go-to%0Asolution%20and%20establishes%20a%20robust%20baseline%20for%20future%20research%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14392v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bag%20of%20Tricks%20for%20Few-Shot%20Class-Incremental%20Learning&entry.906535625=Shuvendu%20Roy%20and%20Chunjong%20Park%20and%20Aldi%20Fahrezi%20and%20Ali%20Etemad&entry.1292438233=%20%20We%20present%20a%20bag%20of%20tricks%20framework%20for%20few-shot%20class-incremental%20learning%0A%28FSCIL%29%2C%20which%20is%20a%20challenging%20form%20of%20continual%20learning%20that%20involves%0Acontinuous%20adaptation%20to%20new%20tasks%20with%20limited%20samples.%20FSCIL%20requires%20both%0Astability%20and%20adaptability%2C%20i.e.%2C%20preserving%20proficiency%20in%20previously%20learned%0Atasks%20while%20learning%20new%20ones.%20Our%20proposed%20bag%20of%20tricks%20brings%20together%20eight%0Akey%20and%20highly%20influential%20techniques%20that%20improve%20stability%2C%20adaptability%2C%20and%0Aoverall%20performance%20under%20a%20unified%20framework%20for%20FSCIL.%20We%20organize%20these%0Atricks%20into%20three%20categories%3A%20stability%20tricks%2C%20adaptability%20tricks%2C%20and%0Atraining%20tricks.%20Stability%20tricks%20aim%20to%20mitigate%20the%20forgetting%20of%20previously%0Alearned%20classes%20by%20enhancing%20the%20separation%20between%20the%20embeddings%20of%20learned%0Aclasses%20and%20minimizing%20interference%20when%20learning%20new%20ones.%20On%20the%20other%20hand%2C%0Aadaptability%20tricks%20focus%20on%20the%20effective%20learning%20of%20new%20classes.%20Finally%2C%0Atraining%20tricks%20improve%20the%20overall%20performance%20without%20compromising%20stability%0Aor%20adaptability.%20We%20perform%20extensive%20experiments%20on%20three%20benchmark%20datasets%2C%0ACIFAR-100%2C%20CUB-200%2C%20and%20miniIMageNet%2C%20to%20evaluate%20the%20impact%20of%20our%20proposed%0Aframework.%20Our%20detailed%20analysis%20shows%20that%20our%20approach%20substantially%20improves%0Aboth%20stability%20and%20adaptability%2C%20establishing%20a%20new%20state-of-the-art%20by%0Aoutperforming%20prior%20works%20in%20the%20area.%20We%20believe%20our%20method%20provides%20a%20go-to%0Asolution%20and%20establishes%20a%20robust%20baseline%20for%20future%20research%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14392v1&entry.124074799=Read"},
{"title": "Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient\n  Method", "author": "Yulan Hu and Sheng Ouyang and Jingyu Liu and Ge Chen and Zhirui Yang and Junchen Wan and Fuzheng Zhang and Zhongyuan Wang and Yong Liu", "abstract": "  Graph contrastive learning (GCL) has emerged as a representative graph\nself-supervised method, achieving significant success. The currently prevalent\noptimization objective for GCL is InfoNCE. Typically, it employs augmentation\ntechniques to obtain two views, where a node in one view acts as the anchor,\nthe corresponding node in the other view serves as the positive sample, and all\nother nodes are regarded as negative samples. The goal is to minimize the\ndistance between the anchor node and positive samples and maximize the distance\nto negative samples. However, due to the lack of label information during\ntraining, InfoNCE inevitably treats samples from the same class as negative\nsamples, leading to the issue of false negative samples. This can impair the\nlearned node representations and subsequently hinder performance in downstream\ntasks. While numerous methods have been proposed to mitigate the impact of\nfalse negatives, they still face various challenges. For instance, while\nincreasing the number of negative samples can dilute the impact of false\nnegatives, it concurrently increases computational burden. Thus, we propose\nGraphRank, a simple yet efficient graph contrastive learning method that\naddresses the problem of false negative samples by redefining the concept of\nnegative samples to a certain extent, thereby avoiding the issue of false\nnegative samples. The effectiveness of GraphRank is empirically validated\nthrough experiments on the node, edge, and graph level tasks.\n", "link": "http://arxiv.org/abs/2310.14525v2", "date": "2024-03-21", "relevancy": 2.456, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5493}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4633}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.461}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Ranking%20Contrastive%20Learning%3A%20A%20Extremely%20Simple%20yet%20Efficient%0A%20%20Method&body=Title%3A%20Graph%20Ranking%20Contrastive%20Learning%3A%20A%20Extremely%20Simple%20yet%20Efficient%0A%20%20Method%0AAuthor%3A%20Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Jingyu%20Liu%20and%20Ge%20Chen%20and%20Zhirui%20Yang%20and%20Junchen%20Wan%20and%20Fuzheng%20Zhang%20and%20Zhongyuan%20Wang%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Graph%20contrastive%20learning%20%28GCL%29%20has%20emerged%20as%20a%20representative%20graph%0Aself-supervised%20method%2C%20achieving%20significant%20success.%20The%20currently%20prevalent%0Aoptimization%20objective%20for%20GCL%20is%20InfoNCE.%20Typically%2C%20it%20employs%20augmentation%0Atechniques%20to%20obtain%20two%20views%2C%20where%20a%20node%20in%20one%20view%20acts%20as%20the%20anchor%2C%0Athe%20corresponding%20node%20in%20the%20other%20view%20serves%20as%20the%20positive%20sample%2C%20and%20all%0Aother%20nodes%20are%20regarded%20as%20negative%20samples.%20The%20goal%20is%20to%20minimize%20the%0Adistance%20between%20the%20anchor%20node%20and%20positive%20samples%20and%20maximize%20the%20distance%0Ato%20negative%20samples.%20However%2C%20due%20to%20the%20lack%20of%20label%20information%20during%0Atraining%2C%20InfoNCE%20inevitably%20treats%20samples%20from%20the%20same%20class%20as%20negative%0Asamples%2C%20leading%20to%20the%20issue%20of%20false%20negative%20samples.%20This%20can%20impair%20the%0Alearned%20node%20representations%20and%20subsequently%20hinder%20performance%20in%20downstream%0Atasks.%20While%20numerous%20methods%20have%20been%20proposed%20to%20mitigate%20the%20impact%20of%0Afalse%20negatives%2C%20they%20still%20face%20various%20challenges.%20For%20instance%2C%20while%0Aincreasing%20the%20number%20of%20negative%20samples%20can%20dilute%20the%20impact%20of%20false%0Anegatives%2C%20it%20concurrently%20increases%20computational%20burden.%20Thus%2C%20we%20propose%0AGraphRank%2C%20a%20simple%20yet%20efficient%20graph%20contrastive%20learning%20method%20that%0Aaddresses%20the%20problem%20of%20false%20negative%20samples%20by%20redefining%20the%20concept%20of%0Anegative%20samples%20to%20a%20certain%20extent%2C%20thereby%20avoiding%20the%20issue%20of%20false%0Anegative%20samples.%20The%20effectiveness%20of%20GraphRank%20is%20empirically%20validated%0Athrough%20experiments%20on%20the%20node%2C%20edge%2C%20and%20graph%20level%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14525v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Ranking%20Contrastive%20Learning%3A%20A%20Extremely%20Simple%20yet%20Efficient%0A%20%20Method&entry.906535625=Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Jingyu%20Liu%20and%20Ge%20Chen%20and%20Zhirui%20Yang%20and%20Junchen%20Wan%20and%20Fuzheng%20Zhang%20and%20Zhongyuan%20Wang%20and%20Yong%20Liu&entry.1292438233=%20%20Graph%20contrastive%20learning%20%28GCL%29%20has%20emerged%20as%20a%20representative%20graph%0Aself-supervised%20method%2C%20achieving%20significant%20success.%20The%20currently%20prevalent%0Aoptimization%20objective%20for%20GCL%20is%20InfoNCE.%20Typically%2C%20it%20employs%20augmentation%0Atechniques%20to%20obtain%20two%20views%2C%20where%20a%20node%20in%20one%20view%20acts%20as%20the%20anchor%2C%0Athe%20corresponding%20node%20in%20the%20other%20view%20serves%20as%20the%20positive%20sample%2C%20and%20all%0Aother%20nodes%20are%20regarded%20as%20negative%20samples.%20The%20goal%20is%20to%20minimize%20the%0Adistance%20between%20the%20anchor%20node%20and%20positive%20samples%20and%20maximize%20the%20distance%0Ato%20negative%20samples.%20However%2C%20due%20to%20the%20lack%20of%20label%20information%20during%0Atraining%2C%20InfoNCE%20inevitably%20treats%20samples%20from%20the%20same%20class%20as%20negative%0Asamples%2C%20leading%20to%20the%20issue%20of%20false%20negative%20samples.%20This%20can%20impair%20the%0Alearned%20node%20representations%20and%20subsequently%20hinder%20performance%20in%20downstream%0Atasks.%20While%20numerous%20methods%20have%20been%20proposed%20to%20mitigate%20the%20impact%20of%0Afalse%20negatives%2C%20they%20still%20face%20various%20challenges.%20For%20instance%2C%20while%0Aincreasing%20the%20number%20of%20negative%20samples%20can%20dilute%20the%20impact%20of%20false%0Anegatives%2C%20it%20concurrently%20increases%20computational%20burden.%20Thus%2C%20we%20propose%0AGraphRank%2C%20a%20simple%20yet%20efficient%20graph%20contrastive%20learning%20method%20that%0Aaddresses%20the%20problem%20of%20false%20negative%20samples%20by%20redefining%20the%20concept%20of%0Anegative%20samples%20to%20a%20certain%20extent%2C%20thereby%20avoiding%20the%20issue%20of%20false%0Anegative%20samples.%20The%20effectiveness%20of%20GraphRank%20is%20empirically%20validated%0Athrough%20experiments%20on%20the%20node%2C%20edge%2C%20and%20graph%20level%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14525v2&entry.124074799=Read"},
{"title": "Transfer Learning for Cross-dataset Isolated Sign Language Recognition\n  in Under-Resourced Datasets", "author": "Ahmet Alp Kindiroglu and Ozgur Kara and Ogulcan Ozdemir and Lale Akarun", "abstract": "  Sign language recognition (SLR) has recently achieved a breakthrough in\nperformance thanks to deep neural networks trained on large annotated sign\ndatasets. Of the many different sign languages, these annotated datasets are\nonly available for a select few. Since acquiring gloss-level labels on sign\nlanguage videos is difficult, learning by transferring knowledge from existing\nannotated sources is useful for recognition in under-resourced sign languages.\nThis study provides a publicly available cross-dataset transfer learning\nbenchmark from two existing public Turkish SLR datasets. We use a temporal\ngraph convolution-based sign language recognition approach to evaluate five\nsupervised transfer learning approaches and experiment with closed-set and\npartial-set cross-dataset transfer learning. Experiments demonstrate that\nimprovement over finetuning based transfer learning is possible with\nspecialized supervised transfer learning methods.\n", "link": "http://arxiv.org/abs/2403.14534v1", "date": "2024-03-21", "relevancy": 2.446, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5153}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4862}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4661}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20for%20Cross-dataset%20Isolated%20Sign%20Language%20Recognition%0A%20%20in%20Under-Resourced%20Datasets&body=Title%3A%20Transfer%20Learning%20for%20Cross-dataset%20Isolated%20Sign%20Language%20Recognition%0A%20%20in%20Under-Resourced%20Datasets%0AAuthor%3A%20Ahmet%20Alp%20Kindiroglu%20and%20Ozgur%20Kara%20and%20Ogulcan%20Ozdemir%20and%20Lale%20Akarun%0AAbstract%3A%20%20%20Sign%20language%20recognition%20%28SLR%29%20has%20recently%20achieved%20a%20breakthrough%20in%0Aperformance%20thanks%20to%20deep%20neural%20networks%20trained%20on%20large%20annotated%20sign%0Adatasets.%20Of%20the%20many%20different%20sign%20languages%2C%20these%20annotated%20datasets%20are%0Aonly%20available%20for%20a%20select%20few.%20Since%20acquiring%20gloss-level%20labels%20on%20sign%0Alanguage%20videos%20is%20difficult%2C%20learning%20by%20transferring%20knowledge%20from%20existing%0Aannotated%20sources%20is%20useful%20for%20recognition%20in%20under-resourced%20sign%20languages.%0AThis%20study%20provides%20a%20publicly%20available%20cross-dataset%20transfer%20learning%0Abenchmark%20from%20two%20existing%20public%20Turkish%20SLR%20datasets.%20We%20use%20a%20temporal%0Agraph%20convolution-based%20sign%20language%20recognition%20approach%20to%20evaluate%20five%0Asupervised%20transfer%20learning%20approaches%20and%20experiment%20with%20closed-set%20and%0Apartial-set%20cross-dataset%20transfer%20learning.%20Experiments%20demonstrate%20that%0Aimprovement%20over%20finetuning%20based%20transfer%20learning%20is%20possible%20with%0Aspecialized%20supervised%20transfer%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14534v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20for%20Cross-dataset%20Isolated%20Sign%20Language%20Recognition%0A%20%20in%20Under-Resourced%20Datasets&entry.906535625=Ahmet%20Alp%20Kindiroglu%20and%20Ozgur%20Kara%20and%20Ogulcan%20Ozdemir%20and%20Lale%20Akarun&entry.1292438233=%20%20Sign%20language%20recognition%20%28SLR%29%20has%20recently%20achieved%20a%20breakthrough%20in%0Aperformance%20thanks%20to%20deep%20neural%20networks%20trained%20on%20large%20annotated%20sign%0Adatasets.%20Of%20the%20many%20different%20sign%20languages%2C%20these%20annotated%20datasets%20are%0Aonly%20available%20for%20a%20select%20few.%20Since%20acquiring%20gloss-level%20labels%20on%20sign%0Alanguage%20videos%20is%20difficult%2C%20learning%20by%20transferring%20knowledge%20from%20existing%0Aannotated%20sources%20is%20useful%20for%20recognition%20in%20under-resourced%20sign%20languages.%0AThis%20study%20provides%20a%20publicly%20available%20cross-dataset%20transfer%20learning%0Abenchmark%20from%20two%20existing%20public%20Turkish%20SLR%20datasets.%20We%20use%20a%20temporal%0Agraph%20convolution-based%20sign%20language%20recognition%20approach%20to%20evaluate%20five%0Asupervised%20transfer%20learning%20approaches%20and%20experiment%20with%20closed-set%20and%0Apartial-set%20cross-dataset%20transfer%20learning.%20Experiments%20demonstrate%20that%0Aimprovement%20over%20finetuning%20based%20transfer%20learning%20is%20possible%20with%0Aspecialized%20supervised%20transfer%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14534v1&entry.124074799=Read"},
{"title": "GIVT: Generative Infinite-Vocabulary Transformers", "author": "Michael Tschannen and Cian Eastwood and Fabian Mentzer", "abstract": "  We introduce generative infinite-vocabulary transformers (GIVT) which\ngenerate vector sequences with real-valued entries, instead of discrete tokens\nfrom a finite vocabulary. To this end, we propose two surprisingly simple\nmodifications to decoder-only transformers: 1) at the input, we replace the\nfinite-vocabulary lookup table with a linear projection of the input vectors;\nand 2) at the output, we replace the logits prediction (usually mapped to a\ncategorical distribution) with the parameters of a multivariate Gaussian\nmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,\nwhere transformers are used to model the discrete latent sequences of a VQ-VAE,\nwe use GIVT to model the unquantized real-valued latent sequences of a\n$\\beta$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and\nimproved variants thereof) as well as MaskGIT, and achieves performance\ncompetitive with recent latent diffusion models. Finally, we obtain strong\nresults outside of image generation when applying GIVT to panoptic segmentation\nand depth estimation with a VAE variant of the UViM framework\n", "link": "http://arxiv.org/abs/2312.02116v3", "date": "2024-03-21", "relevancy": 2.446, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.621}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6063}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6009}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GIVT%3A%20Generative%20Infinite-Vocabulary%20Transformers&body=Title%3A%20GIVT%3A%20Generative%20Infinite-Vocabulary%20Transformers%0AAuthor%3A%20Michael%20Tschannen%20and%20Cian%20Eastwood%20and%20Fabian%20Mentzer%0AAbstract%3A%20%20%20We%20introduce%20generative%20infinite-vocabulary%20transformers%20%28GIVT%29%20which%0Agenerate%20vector%20sequences%20with%20real-valued%20entries%2C%20instead%20of%20discrete%20tokens%0Afrom%20a%20finite%20vocabulary.%20To%20this%20end%2C%20we%20propose%20two%20surprisingly%20simple%0Amodifications%20to%20decoder-only%20transformers%3A%201%29%20at%20the%20input%2C%20we%20replace%20the%0Afinite-vocabulary%20lookup%20table%20with%20a%20linear%20projection%20of%20the%20input%20vectors%3B%0Aand%202%29%20at%20the%20output%2C%20we%20replace%20the%20logits%20prediction%20%28usually%20mapped%20to%20a%0Acategorical%20distribution%29%20with%20the%20parameters%20of%20a%20multivariate%20Gaussian%0Amixture%20model.%20Inspired%20by%20the%20image-generation%20paradigm%20of%20VQ-GAN%20and%20MaskGIT%2C%0Awhere%20transformers%20are%20used%20to%20model%20the%20discrete%20latent%20sequences%20of%20a%20VQ-VAE%2C%0Awe%20use%20GIVT%20to%20model%20the%20unquantized%20real-valued%20latent%20sequences%20of%20a%0A%24%5Cbeta%24-VAE.%20In%20class-conditional%20image%20generation%20GIVT%20outperforms%20VQ-GAN%20%28and%0Aimproved%20variants%20thereof%29%20as%20well%20as%20MaskGIT%2C%20and%20achieves%20performance%0Acompetitive%20with%20recent%20latent%20diffusion%20models.%20Finally%2C%20we%20obtain%20strong%0Aresults%20outside%20of%20image%20generation%20when%20applying%20GIVT%20to%20panoptic%20segmentation%0Aand%20depth%20estimation%20with%20a%20VAE%20variant%20of%20the%20UViM%20framework%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02116v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIVT%3A%20Generative%20Infinite-Vocabulary%20Transformers&entry.906535625=Michael%20Tschannen%20and%20Cian%20Eastwood%20and%20Fabian%20Mentzer&entry.1292438233=%20%20We%20introduce%20generative%20infinite-vocabulary%20transformers%20%28GIVT%29%20which%0Agenerate%20vector%20sequences%20with%20real-valued%20entries%2C%20instead%20of%20discrete%20tokens%0Afrom%20a%20finite%20vocabulary.%20To%20this%20end%2C%20we%20propose%20two%20surprisingly%20simple%0Amodifications%20to%20decoder-only%20transformers%3A%201%29%20at%20the%20input%2C%20we%20replace%20the%0Afinite-vocabulary%20lookup%20table%20with%20a%20linear%20projection%20of%20the%20input%20vectors%3B%0Aand%202%29%20at%20the%20output%2C%20we%20replace%20the%20logits%20prediction%20%28usually%20mapped%20to%20a%0Acategorical%20distribution%29%20with%20the%20parameters%20of%20a%20multivariate%20Gaussian%0Amixture%20model.%20Inspired%20by%20the%20image-generation%20paradigm%20of%20VQ-GAN%20and%20MaskGIT%2C%0Awhere%20transformers%20are%20used%20to%20model%20the%20discrete%20latent%20sequences%20of%20a%20VQ-VAE%2C%0Awe%20use%20GIVT%20to%20model%20the%20unquantized%20real-valued%20latent%20sequences%20of%20a%0A%24%5Cbeta%24-VAE.%20In%20class-conditional%20image%20generation%20GIVT%20outperforms%20VQ-GAN%20%28and%0Aimproved%20variants%20thereof%29%20as%20well%20as%20MaskGIT%2C%20and%20achieves%20performance%0Acompetitive%20with%20recent%20latent%20diffusion%20models.%20Finally%2C%20we%20obtain%20strong%0Aresults%20outside%20of%20image%20generation%20when%20applying%20GIVT%20to%20panoptic%20segmentation%0Aand%20depth%20estimation%20with%20a%20VAE%20variant%20of%20the%20UViM%20framework%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02116v3&entry.124074799=Read"},
{"title": "MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images", "author": "Yuedong Chen and Haofei Xu and Chuanxia Zheng and Bohan Zhuang and Marc Pollefeys and Andreas Geiger and Tat-Jen Cham and Jianfei Cai", "abstract": "  We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model\nlearned from sparse multi-view images. To accurately localize the Gaussian\ncenters, we propose to build a cost volume representation via plane sweeping in\nthe 3D space, where the cross-view feature similarities stored in the cost\nvolume can provide valuable geometry cues to the estimation of depth. We learn\nthe Gaussian primitives' opacities, covariances, and spherical harmonics\ncoefficients jointly with the Gaussian centers while only relying on\nphotometric supervision. We demonstrate the importance of the cost volume\nrepresentation in learning feed-forward Gaussian Splatting models via extensive\nexperimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,\nour model achieves state-of-the-art performance with the fastest feed-forward\ninference speed (22 fps). Compared to the latest state-of-the-art method\npixelSplat, our model uses $10\\times $ fewer parameters and infers more than\n$2\\times$ faster while providing higher appearance and geometry quality as well\nas better cross-dataset generalization.\n", "link": "http://arxiv.org/abs/2403.14627v1", "date": "2024-03-21", "relevancy": 2.4353, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4948}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4776}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MVSplat%3A%20Efficient%203D%20Gaussian%20Splatting%20from%20Sparse%20Multi-View%20Images&body=Title%3A%20MVSplat%3A%20Efficient%203D%20Gaussian%20Splatting%20from%20Sparse%20Multi-View%20Images%0AAuthor%3A%20Yuedong%20Chen%20and%20Haofei%20Xu%20and%20Chuanxia%20Zheng%20and%20Bohan%20Zhuang%20and%20Marc%20Pollefeys%20and%20Andreas%20Geiger%20and%20Tat-Jen%20Cham%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%20We%20propose%20MVSplat%2C%20an%20efficient%20feed-forward%203D%20Gaussian%20Splatting%20model%0Alearned%20from%20sparse%20multi-view%20images.%20To%20accurately%20localize%20the%20Gaussian%0Acenters%2C%20we%20propose%20to%20build%20a%20cost%20volume%20representation%20via%20plane%20sweeping%20in%0Athe%203D%20space%2C%20where%20the%20cross-view%20feature%20similarities%20stored%20in%20the%20cost%0Avolume%20can%20provide%20valuable%20geometry%20cues%20to%20the%20estimation%20of%20depth.%20We%20learn%0Athe%20Gaussian%20primitives%27%20opacities%2C%20covariances%2C%20and%20spherical%20harmonics%0Acoefficients%20jointly%20with%20the%20Gaussian%20centers%20while%20only%20relying%20on%0Aphotometric%20supervision.%20We%20demonstrate%20the%20importance%20of%20the%20cost%20volume%0Arepresentation%20in%20learning%20feed-forward%20Gaussian%20Splatting%20models%20via%20extensive%0Aexperimental%20evaluations.%20On%20the%20large-scale%20RealEstate10K%20and%20ACID%20benchmarks%2C%0Aour%20model%20achieves%20state-of-the-art%20performance%20with%20the%20fastest%20feed-forward%0Ainference%20speed%20%2822%20fps%29.%20Compared%20to%20the%20latest%20state-of-the-art%20method%0ApixelSplat%2C%20our%20model%20uses%20%2410%5Ctimes%20%24%20fewer%20parameters%20and%20infers%20more%20than%0A%242%5Ctimes%24%20faster%20while%20providing%20higher%20appearance%20and%20geometry%20quality%20as%20well%0Aas%20better%20cross-dataset%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14627v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVSplat%3A%20Efficient%203D%20Gaussian%20Splatting%20from%20Sparse%20Multi-View%20Images&entry.906535625=Yuedong%20Chen%20and%20Haofei%20Xu%20and%20Chuanxia%20Zheng%20and%20Bohan%20Zhuang%20and%20Marc%20Pollefeys%20and%20Andreas%20Geiger%20and%20Tat-Jen%20Cham%20and%20Jianfei%20Cai&entry.1292438233=%20%20We%20propose%20MVSplat%2C%20an%20efficient%20feed-forward%203D%20Gaussian%20Splatting%20model%0Alearned%20from%20sparse%20multi-view%20images.%20To%20accurately%20localize%20the%20Gaussian%0Acenters%2C%20we%20propose%20to%20build%20a%20cost%20volume%20representation%20via%20plane%20sweeping%20in%0Athe%203D%20space%2C%20where%20the%20cross-view%20feature%20similarities%20stored%20in%20the%20cost%0Avolume%20can%20provide%20valuable%20geometry%20cues%20to%20the%20estimation%20of%20depth.%20We%20learn%0Athe%20Gaussian%20primitives%27%20opacities%2C%20covariances%2C%20and%20spherical%20harmonics%0Acoefficients%20jointly%20with%20the%20Gaussian%20centers%20while%20only%20relying%20on%0Aphotometric%20supervision.%20We%20demonstrate%20the%20importance%20of%20the%20cost%20volume%0Arepresentation%20in%20learning%20feed-forward%20Gaussian%20Splatting%20models%20via%20extensive%0Aexperimental%20evaluations.%20On%20the%20large-scale%20RealEstate10K%20and%20ACID%20benchmarks%2C%0Aour%20model%20achieves%20state-of-the-art%20performance%20with%20the%20fastest%20feed-forward%0Ainference%20speed%20%2822%20fps%29.%20Compared%20to%20the%20latest%20state-of-the-art%20method%0ApixelSplat%2C%20our%20model%20uses%20%2410%5Ctimes%20%24%20fewer%20parameters%20and%20infers%20more%20than%0A%242%5Ctimes%24%20faster%20while%20providing%20higher%20appearance%20and%20geometry%20quality%20as%20well%0Aas%20better%20cross-dataset%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14627v1&entry.124074799=Read"},
{"title": "HySim: An Efficient Hybrid Similarity Measure for Patch Matching in\n  Image Inpainting", "author": "Saad Noufel and Nadir Maaroufi and Mehdi Najib and Mohamed Bakhouya", "abstract": "  Inpainting, for filling missing image regions, is a crucial task in various\napplications, such as medical imaging and remote sensing. Trending data-driven\napproaches efficiency, for image inpainting, often requires extensive data\npreprocessing. In this sense, there is still a need for model-driven approaches\nin case of application constrained with data availability and quality,\nespecially for those related for time series forecasting using image inpainting\ntechniques. This paper proposes an improved modeldriven approach relying on\npatch-based techniques. Our approach deviates from the standard Sum of Squared\nDifferences (SSD) similarity measure by introducing a Hybrid Similarity\n(HySim), which combines both strengths of Chebychev and Minkowski distances.\nThis hybridization enhances patch selection, leading to high-quality inpainting\nresults with reduced mismatch errors. Experimental results proved the\neffectiveness of our approach against other model-driven techniques, such as\ndiffusion or patch-based approaches, showcasing its effectiveness in achieving\nvisually pleasing restorations.\n", "link": "http://arxiv.org/abs/2403.14292v1", "date": "2024-03-21", "relevancy": 2.4206, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.502}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4803}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.47}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HySim%3A%20An%20Efficient%20Hybrid%20Similarity%20Measure%20for%20Patch%20Matching%20in%0A%20%20Image%20Inpainting&body=Title%3A%20HySim%3A%20An%20Efficient%20Hybrid%20Similarity%20Measure%20for%20Patch%20Matching%20in%0A%20%20Image%20Inpainting%0AAuthor%3A%20Saad%20Noufel%20and%20Nadir%20Maaroufi%20and%20Mehdi%20Najib%20and%20Mohamed%20Bakhouya%0AAbstract%3A%20%20%20Inpainting%2C%20for%20filling%20missing%20image%20regions%2C%20is%20a%20crucial%20task%20in%20various%0Aapplications%2C%20such%20as%20medical%20imaging%20and%20remote%20sensing.%20Trending%20data-driven%0Aapproaches%20efficiency%2C%20for%20image%20inpainting%2C%20often%20requires%20extensive%20data%0Apreprocessing.%20In%20this%20sense%2C%20there%20is%20still%20a%20need%20for%20model-driven%20approaches%0Ain%20case%20of%20application%20constrained%20with%20data%20availability%20and%20quality%2C%0Aespecially%20for%20those%20related%20for%20time%20series%20forecasting%20using%20image%20inpainting%0Atechniques.%20This%20paper%20proposes%20an%20improved%20modeldriven%20approach%20relying%20on%0Apatch-based%20techniques.%20Our%20approach%20deviates%20from%20the%20standard%20Sum%20of%20Squared%0ADifferences%20%28SSD%29%20similarity%20measure%20by%20introducing%20a%20Hybrid%20Similarity%0A%28HySim%29%2C%20which%20combines%20both%20strengths%20of%20Chebychev%20and%20Minkowski%20distances.%0AThis%20hybridization%20enhances%20patch%20selection%2C%20leading%20to%20high-quality%20inpainting%0Aresults%20with%20reduced%20mismatch%20errors.%20Experimental%20results%20proved%20the%0Aeffectiveness%20of%20our%20approach%20against%20other%20model-driven%20techniques%2C%20such%20as%0Adiffusion%20or%20patch-based%20approaches%2C%20showcasing%20its%20effectiveness%20in%20achieving%0Avisually%20pleasing%20restorations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14292v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HySim%3A%20An%20Efficient%20Hybrid%20Similarity%20Measure%20for%20Patch%20Matching%20in%0A%20%20Image%20Inpainting&entry.906535625=Saad%20Noufel%20and%20Nadir%20Maaroufi%20and%20Mehdi%20Najib%20and%20Mohamed%20Bakhouya&entry.1292438233=%20%20Inpainting%2C%20for%20filling%20missing%20image%20regions%2C%20is%20a%20crucial%20task%20in%20various%0Aapplications%2C%20such%20as%20medical%20imaging%20and%20remote%20sensing.%20Trending%20data-driven%0Aapproaches%20efficiency%2C%20for%20image%20inpainting%2C%20often%20requires%20extensive%20data%0Apreprocessing.%20In%20this%20sense%2C%20there%20is%20still%20a%20need%20for%20model-driven%20approaches%0Ain%20case%20of%20application%20constrained%20with%20data%20availability%20and%20quality%2C%0Aespecially%20for%20those%20related%20for%20time%20series%20forecasting%20using%20image%20inpainting%0Atechniques.%20This%20paper%20proposes%20an%20improved%20modeldriven%20approach%20relying%20on%0Apatch-based%20techniques.%20Our%20approach%20deviates%20from%20the%20standard%20Sum%20of%20Squared%0ADifferences%20%28SSD%29%20similarity%20measure%20by%20introducing%20a%20Hybrid%20Similarity%0A%28HySim%29%2C%20which%20combines%20both%20strengths%20of%20Chebychev%20and%20Minkowski%20distances.%0AThis%20hybridization%20enhances%20patch%20selection%2C%20leading%20to%20high-quality%20inpainting%0Aresults%20with%20reduced%20mismatch%20errors.%20Experimental%20results%20proved%20the%0Aeffectiveness%20of%20our%20approach%20against%20other%20model-driven%20techniques%2C%20such%20as%0Adiffusion%20or%20patch-based%20approaches%2C%20showcasing%20its%20effectiveness%20in%20achieving%0Avisually%20pleasing%20restorations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14292v1&entry.124074799=Read"},
{"title": "HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression", "author": "Yihang Chen and Qianyi Wu and Jianfei Cai and Mehrtash Harandi and Weiyao Lin", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To address this, we make use of the relations between the\nunorganized anchors and the structured hash grid, leveraging their mutual\ninformation for context modeling, and propose a Hash-grid Assisted Context\n(HAC) framework for highly compact 3DGS representation. Our approach introduces\na binary hash grid to establish continuous spatial consistencies, allowing us\nto unveil the inherent spatial relations of anchors through a carefully\ndesigned context model. To facilitate entropy coding, we utilize Gaussian\ndistributions to accurately estimate the probability of each quantized\nattribute, where an adaptive quantization module is proposed to enable\nhigh-precision quantization of these attributes for improved fidelity\nrestoration. Additionally, we incorporate an adaptive masking strategy to\neliminate invalid Gaussians and anchors. Importantly, our work is the pioneer\nto explore context-based compression for 3DGS representation, resulting in a\nremarkable size reduction of over $75\\times$ compared to vanilla 3DGS, while\nsimultaneously improving fidelity, and achieving over $11\\times$ size reduction\nover SOTA 3DGS compression approach Scaffold-GS. Our code is available here:\nhttps://github.com/YihangChen-ee/HAC\n", "link": "http://arxiv.org/abs/2403.14530v1", "date": "2024-03-21", "relevancy": 2.4135, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5081}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4644}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HAC%3A%20Hash-grid%20Assisted%20Context%20for%203D%20Gaussian%20Splatting%20Compression&body=Title%3A%20HAC%3A%20Hash-grid%20Assisted%20Context%20for%203D%20Gaussian%20Splatting%20Compression%0AAuthor%3A%20Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Jianfei%20Cai%20and%20Mehrtash%20Harandi%20and%20Weiyao%20Lin%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20framework%20for%20novel%0Aview%20synthesis%2C%20boasting%20rapid%20rendering%20speed%20with%20high%20fidelity.%20However%2C%20the%0Asubstantial%20Gaussians%20and%20their%20associated%20attributes%20necessitate%20effective%0Acompression%20techniques.%20Nevertheless%2C%20the%20sparse%20and%20unorganized%20nature%20of%20the%0Apoint%20cloud%20of%20Gaussians%20%28or%20anchors%20in%20our%20paper%29%20presents%20challenges%20for%0Acompression.%20To%20address%20this%2C%20we%20make%20use%20of%20the%20relations%20between%20the%0Aunorganized%20anchors%20and%20the%20structured%20hash%20grid%2C%20leveraging%20their%20mutual%0Ainformation%20for%20context%20modeling%2C%20and%20propose%20a%20Hash-grid%20Assisted%20Context%0A%28HAC%29%20framework%20for%20highly%20compact%203DGS%20representation.%20Our%20approach%20introduces%0Aa%20binary%20hash%20grid%20to%20establish%20continuous%20spatial%20consistencies%2C%20allowing%20us%0Ato%20unveil%20the%20inherent%20spatial%20relations%20of%20anchors%20through%20a%20carefully%0Adesigned%20context%20model.%20To%20facilitate%20entropy%20coding%2C%20we%20utilize%20Gaussian%0Adistributions%20to%20accurately%20estimate%20the%20probability%20of%20each%20quantized%0Aattribute%2C%20where%20an%20adaptive%20quantization%20module%20is%20proposed%20to%20enable%0Ahigh-precision%20quantization%20of%20these%20attributes%20for%20improved%20fidelity%0Arestoration.%20Additionally%2C%20we%20incorporate%20an%20adaptive%20masking%20strategy%20to%0Aeliminate%20invalid%20Gaussians%20and%20anchors.%20Importantly%2C%20our%20work%20is%20the%20pioneer%0Ato%20explore%20context-based%20compression%20for%203DGS%20representation%2C%20resulting%20in%20a%0Aremarkable%20size%20reduction%20of%20over%20%2475%5Ctimes%24%20compared%20to%20vanilla%203DGS%2C%20while%0Asimultaneously%20improving%20fidelity%2C%20and%20achieving%20over%20%2411%5Ctimes%24%20size%20reduction%0Aover%20SOTA%203DGS%20compression%20approach%20Scaffold-GS.%20Our%20code%20is%20available%20here%3A%0Ahttps%3A//github.com/YihangChen-ee/HAC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14530v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAC%3A%20Hash-grid%20Assisted%20Context%20for%203D%20Gaussian%20Splatting%20Compression&entry.906535625=Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Jianfei%20Cai%20and%20Mehrtash%20Harandi%20and%20Weiyao%20Lin&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20framework%20for%20novel%0Aview%20synthesis%2C%20boasting%20rapid%20rendering%20speed%20with%20high%20fidelity.%20However%2C%20the%0Asubstantial%20Gaussians%20and%20their%20associated%20attributes%20necessitate%20effective%0Acompression%20techniques.%20Nevertheless%2C%20the%20sparse%20and%20unorganized%20nature%20of%20the%0Apoint%20cloud%20of%20Gaussians%20%28or%20anchors%20in%20our%20paper%29%20presents%20challenges%20for%0Acompression.%20To%20address%20this%2C%20we%20make%20use%20of%20the%20relations%20between%20the%0Aunorganized%20anchors%20and%20the%20structured%20hash%20grid%2C%20leveraging%20their%20mutual%0Ainformation%20for%20context%20modeling%2C%20and%20propose%20a%20Hash-grid%20Assisted%20Context%0A%28HAC%29%20framework%20for%20highly%20compact%203DGS%20representation.%20Our%20approach%20introduces%0Aa%20binary%20hash%20grid%20to%20establish%20continuous%20spatial%20consistencies%2C%20allowing%20us%0Ato%20unveil%20the%20inherent%20spatial%20relations%20of%20anchors%20through%20a%20carefully%0Adesigned%20context%20model.%20To%20facilitate%20entropy%20coding%2C%20we%20utilize%20Gaussian%0Adistributions%20to%20accurately%20estimate%20the%20probability%20of%20each%20quantized%0Aattribute%2C%20where%20an%20adaptive%20quantization%20module%20is%20proposed%20to%20enable%0Ahigh-precision%20quantization%20of%20these%20attributes%20for%20improved%20fidelity%0Arestoration.%20Additionally%2C%20we%20incorporate%20an%20adaptive%20masking%20strategy%20to%0Aeliminate%20invalid%20Gaussians%20and%20anchors.%20Importantly%2C%20our%20work%20is%20the%20pioneer%0Ato%20explore%20context-based%20compression%20for%203DGS%20representation%2C%20resulting%20in%20a%0Aremarkable%20size%20reduction%20of%20over%20%2475%5Ctimes%24%20compared%20to%20vanilla%203DGS%2C%20while%0Asimultaneously%20improving%20fidelity%2C%20and%20achieving%20over%20%2411%5Ctimes%24%20size%20reduction%0Aover%20SOTA%203DGS%20compression%20approach%20Scaffold-GS.%20Our%20code%20is%20available%20here%3A%0Ahttps%3A//github.com/YihangChen-ee/HAC%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14530v1&entry.124074799=Read"},
{"title": "Self-Supervised Class-Agnostic Motion Prediction with Spatial and\n  Temporal Consistency Regularizations", "author": "Kewei Wang and Yizheng Wu and Jun Cen and Zhiyu Pan and Xingyi Li and Zhe Wang and Zhiguo Cao and Guosheng Lin", "abstract": "  The perception of motion behavior in a dynamic environment holds significant\nimportance for autonomous driving systems, wherein class-agnostic motion\nprediction methods directly predict the motion of the entire point cloud. While\nmost existing methods rely on fully-supervised learning, the manual labeling of\npoint cloud data is laborious and time-consuming. Therefore, several\nannotation-efficient methods have been proposed to address this challenge.\nAlthough effective, these methods rely on weak annotations or additional\nmulti-modal data like images, and the potential benefits inherent in the point\ncloud sequence are still underexplored. To this end, we explore the feasibility\nof self-supervised motion prediction with only unlabeled LiDAR point clouds.\nInitially, we employ an optimal transport solver to establish coarse\ncorrespondences between current and future point clouds as the coarse pseudo\nmotion labels. Training models directly using such coarse labels leads to\nnoticeable spatial and temporal prediction inconsistencies. To mitigate these\nissues, we introduce three simple spatial and temporal regularization losses,\nwhich facilitate the self-supervised training process effectively. Experimental\nresults demonstrate the significant superiority of our approach over the\nstate-of-the-art self-supervised methods.\n", "link": "http://arxiv.org/abs/2403.13261v2", "date": "2024-03-21", "relevancy": 2.4072, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6139}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5997}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5991}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Class-Agnostic%20Motion%20Prediction%20with%20Spatial%20and%0A%20%20Temporal%20Consistency%20Regularizations&body=Title%3A%20Self-Supervised%20Class-Agnostic%20Motion%20Prediction%20with%20Spatial%20and%0A%20%20Temporal%20Consistency%20Regularizations%0AAuthor%3A%20Kewei%20Wang%20and%20Yizheng%20Wu%20and%20Jun%20Cen%20and%20Zhiyu%20Pan%20and%20Xingyi%20Li%20and%20Zhe%20Wang%20and%20Zhiguo%20Cao%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20The%20perception%20of%20motion%20behavior%20in%20a%20dynamic%20environment%20holds%20significant%0Aimportance%20for%20autonomous%20driving%20systems%2C%20wherein%20class-agnostic%20motion%0Aprediction%20methods%20directly%20predict%20the%20motion%20of%20the%20entire%20point%20cloud.%20While%0Amost%20existing%20methods%20rely%20on%20fully-supervised%20learning%2C%20the%20manual%20labeling%20of%0Apoint%20cloud%20data%20is%20laborious%20and%20time-consuming.%20Therefore%2C%20several%0Aannotation-efficient%20methods%20have%20been%20proposed%20to%20address%20this%20challenge.%0AAlthough%20effective%2C%20these%20methods%20rely%20on%20weak%20annotations%20or%20additional%0Amulti-modal%20data%20like%20images%2C%20and%20the%20potential%20benefits%20inherent%20in%20the%20point%0Acloud%20sequence%20are%20still%20underexplored.%20To%20this%20end%2C%20we%20explore%20the%20feasibility%0Aof%20self-supervised%20motion%20prediction%20with%20only%20unlabeled%20LiDAR%20point%20clouds.%0AInitially%2C%20we%20employ%20an%20optimal%20transport%20solver%20to%20establish%20coarse%0Acorrespondences%20between%20current%20and%20future%20point%20clouds%20as%20the%20coarse%20pseudo%0Amotion%20labels.%20Training%20models%20directly%20using%20such%20coarse%20labels%20leads%20to%0Anoticeable%20spatial%20and%20temporal%20prediction%20inconsistencies.%20To%20mitigate%20these%0Aissues%2C%20we%20introduce%20three%20simple%20spatial%20and%20temporal%20regularization%20losses%2C%0Awhich%20facilitate%20the%20self-supervised%20training%20process%20effectively.%20Experimental%0Aresults%20demonstrate%20the%20significant%20superiority%20of%20our%20approach%20over%20the%0Astate-of-the-art%20self-supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13261v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Class-Agnostic%20Motion%20Prediction%20with%20Spatial%20and%0A%20%20Temporal%20Consistency%20Regularizations&entry.906535625=Kewei%20Wang%20and%20Yizheng%20Wu%20and%20Jun%20Cen%20and%20Zhiyu%20Pan%20and%20Xingyi%20Li%20and%20Zhe%20Wang%20and%20Zhiguo%20Cao%20and%20Guosheng%20Lin&entry.1292438233=%20%20The%20perception%20of%20motion%20behavior%20in%20a%20dynamic%20environment%20holds%20significant%0Aimportance%20for%20autonomous%20driving%20systems%2C%20wherein%20class-agnostic%20motion%0Aprediction%20methods%20directly%20predict%20the%20motion%20of%20the%20entire%20point%20cloud.%20While%0Amost%20existing%20methods%20rely%20on%20fully-supervised%20learning%2C%20the%20manual%20labeling%20of%0Apoint%20cloud%20data%20is%20laborious%20and%20time-consuming.%20Therefore%2C%20several%0Aannotation-efficient%20methods%20have%20been%20proposed%20to%20address%20this%20challenge.%0AAlthough%20effective%2C%20these%20methods%20rely%20on%20weak%20annotations%20or%20additional%0Amulti-modal%20data%20like%20images%2C%20and%20the%20potential%20benefits%20inherent%20in%20the%20point%0Acloud%20sequence%20are%20still%20underexplored.%20To%20this%20end%2C%20we%20explore%20the%20feasibility%0Aof%20self-supervised%20motion%20prediction%20with%20only%20unlabeled%20LiDAR%20point%20clouds.%0AInitially%2C%20we%20employ%20an%20optimal%20transport%20solver%20to%20establish%20coarse%0Acorrespondences%20between%20current%20and%20future%20point%20clouds%20as%20the%20coarse%20pseudo%0Amotion%20labels.%20Training%20models%20directly%20using%20such%20coarse%20labels%20leads%20to%0Anoticeable%20spatial%20and%20temporal%20prediction%20inconsistencies.%20To%20mitigate%20these%0Aissues%2C%20we%20introduce%20three%20simple%20spatial%20and%20temporal%20regularization%20losses%2C%0Awhich%20facilitate%20the%20self-supervised%20training%20process%20effectively.%20Experimental%0Aresults%20demonstrate%20the%20significant%20superiority%20of%20our%20approach%20over%20the%0Astate-of-the-art%20self-supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13261v2&entry.124074799=Read"},
{"title": "Explorative Inbetweening of Time and Space", "author": "Haiwen Feng and Zheng Ding and Zhihao Xia and Simon Niklaus and Victoria Abrevaya and Michael J. Black and Xuaner Zhang", "abstract": "  We introduce bounded generation as a generalized task to control video\ngeneration to synthesize arbitrary camera and subject motion based only on a\ngiven start and end frame. Our objective is to fully leverage the inherent\ngeneralization capability of an image-to-video model without additional\ntraining or fine-tuning of the original model. This is achieved through the\nproposed new sampling strategy, which we call Time Reversal Fusion, that fuses\nthe temporally forward and backward denoising paths conditioned on the start\nand end frame, respectively. The fused path results in a video that smoothly\nconnects the two frames, generating inbetweening of faithful subject motion,\nnovel views of static scenes, and seamless video looping when the two bounding\nframes are identical. We curate a diverse evaluation dataset of image pairs and\ncompare against the closest existing methods. We find that Time Reversal Fusion\noutperforms related work on all subtasks, exhibiting the ability to generate\ncomplex motions and 3D-consistent views guided by bounded frames. See project\npage at https://time-reversal.github.io.\n", "link": "http://arxiv.org/abs/2403.14611v1", "date": "2024-03-21", "relevancy": 2.4, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6175}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5958}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5842}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Explorative%20Inbetweening%20of%20Time%20and%20Space&body=Title%3A%20Explorative%20Inbetweening%20of%20Time%20and%20Space%0AAuthor%3A%20Haiwen%20Feng%20and%20Zheng%20Ding%20and%20Zhihao%20Xia%20and%20Simon%20Niklaus%20and%20Victoria%20Abrevaya%20and%20Michael%20J.%20Black%20and%20Xuaner%20Zhang%0AAbstract%3A%20%20%20We%20introduce%20bounded%20generation%20as%20a%20generalized%20task%20to%20control%20video%0Ageneration%20to%20synthesize%20arbitrary%20camera%20and%20subject%20motion%20based%20only%20on%20a%0Agiven%20start%20and%20end%20frame.%20Our%20objective%20is%20to%20fully%20leverage%20the%20inherent%0Ageneralization%20capability%20of%20an%20image-to-video%20model%20without%20additional%0Atraining%20or%20fine-tuning%20of%20the%20original%20model.%20This%20is%20achieved%20through%20the%0Aproposed%20new%20sampling%20strategy%2C%20which%20we%20call%20Time%20Reversal%20Fusion%2C%20that%20fuses%0Athe%20temporally%20forward%20and%20backward%20denoising%20paths%20conditioned%20on%20the%20start%0Aand%20end%20frame%2C%20respectively.%20The%20fused%20path%20results%20in%20a%20video%20that%20smoothly%0Aconnects%20the%20two%20frames%2C%20generating%20inbetweening%20of%20faithful%20subject%20motion%2C%0Anovel%20views%20of%20static%20scenes%2C%20and%20seamless%20video%20looping%20when%20the%20two%20bounding%0Aframes%20are%20identical.%20We%20curate%20a%20diverse%20evaluation%20dataset%20of%20image%20pairs%20and%0Acompare%20against%20the%20closest%20existing%20methods.%20We%20find%20that%20Time%20Reversal%20Fusion%0Aoutperforms%20related%20work%20on%20all%20subtasks%2C%20exhibiting%20the%20ability%20to%20generate%0Acomplex%20motions%20and%203D-consistent%20views%20guided%20by%20bounded%20frames.%20See%20project%0Apage%20at%20https%3A//time-reversal.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14611v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explorative%20Inbetweening%20of%20Time%20and%20Space&entry.906535625=Haiwen%20Feng%20and%20Zheng%20Ding%20and%20Zhihao%20Xia%20and%20Simon%20Niklaus%20and%20Victoria%20Abrevaya%20and%20Michael%20J.%20Black%20and%20Xuaner%20Zhang&entry.1292438233=%20%20We%20introduce%20bounded%20generation%20as%20a%20generalized%20task%20to%20control%20video%0Ageneration%20to%20synthesize%20arbitrary%20camera%20and%20subject%20motion%20based%20only%20on%20a%0Agiven%20start%20and%20end%20frame.%20Our%20objective%20is%20to%20fully%20leverage%20the%20inherent%0Ageneralization%20capability%20of%20an%20image-to-video%20model%20without%20additional%0Atraining%20or%20fine-tuning%20of%20the%20original%20model.%20This%20is%20achieved%20through%20the%0Aproposed%20new%20sampling%20strategy%2C%20which%20we%20call%20Time%20Reversal%20Fusion%2C%20that%20fuses%0Athe%20temporally%20forward%20and%20backward%20denoising%20paths%20conditioned%20on%20the%20start%0Aand%20end%20frame%2C%20respectively.%20The%20fused%20path%20results%20in%20a%20video%20that%20smoothly%0Aconnects%20the%20two%20frames%2C%20generating%20inbetweening%20of%20faithful%20subject%20motion%2C%0Anovel%20views%20of%20static%20scenes%2C%20and%20seamless%20video%20looping%20when%20the%20two%20bounding%0Aframes%20are%20identical.%20We%20curate%20a%20diverse%20evaluation%20dataset%20of%20image%20pairs%20and%0Acompare%20against%20the%20closest%20existing%20methods.%20We%20find%20that%20Time%20Reversal%20Fusion%0Aoutperforms%20related%20work%20on%20all%20subtasks%2C%20exhibiting%20the%20ability%20to%20generate%0Acomplex%20motions%20and%203D-consistent%20views%20guided%20by%20bounded%20frames.%20See%20project%0Apage%20at%20https%3A//time-reversal.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14611v1&entry.124074799=Read"},
{"title": "SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions", "author": "Jaihoon Kim and Juil Koo and Kyeongmin Yeo and Minhyuk Sung", "abstract": "  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n", "link": "http://arxiv.org/abs/2403.14370v1", "date": "2024-03-21", "relevancy": 2.3792, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6102}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.592}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5805}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SyncTweedies%3A%20A%20General%20Generative%20Framework%20Based%20on%20Synchronized%0A%20%20Diffusions&body=Title%3A%20SyncTweedies%3A%20A%20General%20Generative%20Framework%20Based%20on%20Synchronized%0A%20%20Diffusions%0AAuthor%3A%20Jaihoon%20Kim%20and%20Juil%20Koo%20and%20Kyeongmin%20Yeo%20and%20Minhyuk%20Sung%0AAbstract%3A%20%20%20We%20introduce%20a%20general%20framework%20for%20generating%20diverse%20visual%20content%2C%0Aincluding%20ambiguous%20images%2C%20panorama%20images%2C%20mesh%20textures%2C%20and%20Gaussian%20splat%0Atextures%2C%20by%20synchronizing%20multiple%20diffusion%20processes.%20We%20present%20exhaustive%0Ainvestigation%20into%20all%20possible%20scenarios%20for%20synchronizing%20multiple%20diffusion%0Aprocesses%20through%20a%20canonical%20space%20and%20analyze%20their%20characteristics%20across%0Aapplications.%20In%20doing%20so%2C%20we%20reveal%20a%20previously%20unexplored%20case%3A%20averaging%0Athe%20outputs%20of%20Tweedie%27s%20formula%20while%20conducting%20denoising%20in%20multiple%0Ainstance%20spaces.%20This%20case%20also%20provides%20the%20best%20quality%20with%20the%20widest%0Aapplicability%20to%20downstream%20tasks.%20We%20name%20this%20case%20SyncTweedies.%20In%20our%0Aexperiments%20generating%20visual%20content%20aforementioned%2C%20we%20demonstrate%20the%0Asuperior%20quality%20of%20generation%20by%20SyncTweedies%20compared%20to%20other%0Asynchronization%20methods%2C%20optimization-based%20and%20iterative-update-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14370v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyncTweedies%3A%20A%20General%20Generative%20Framework%20Based%20on%20Synchronized%0A%20%20Diffusions&entry.906535625=Jaihoon%20Kim%20and%20Juil%20Koo%20and%20Kyeongmin%20Yeo%20and%20Minhyuk%20Sung&entry.1292438233=%20%20We%20introduce%20a%20general%20framework%20for%20generating%20diverse%20visual%20content%2C%0Aincluding%20ambiguous%20images%2C%20panorama%20images%2C%20mesh%20textures%2C%20and%20Gaussian%20splat%0Atextures%2C%20by%20synchronizing%20multiple%20diffusion%20processes.%20We%20present%20exhaustive%0Ainvestigation%20into%20all%20possible%20scenarios%20for%20synchronizing%20multiple%20diffusion%0Aprocesses%20through%20a%20canonical%20space%20and%20analyze%20their%20characteristics%20across%0Aapplications.%20In%20doing%20so%2C%20we%20reveal%20a%20previously%20unexplored%20case%3A%20averaging%0Athe%20outputs%20of%20Tweedie%27s%20formula%20while%20conducting%20denoising%20in%20multiple%0Ainstance%20spaces.%20This%20case%20also%20provides%20the%20best%20quality%20with%20the%20widest%0Aapplicability%20to%20downstream%20tasks.%20We%20name%20this%20case%20SyncTweedies.%20In%20our%0Aexperiments%20generating%20visual%20content%20aforementioned%2C%20we%20demonstrate%20the%0Asuperior%20quality%20of%20generation%20by%20SyncTweedies%20compared%20to%20other%0Asynchronization%20methods%2C%20optimization-based%20and%20iterative-update-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14370v1&entry.124074799=Read"},
{"title": "Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion", "author": "Xiang Fan and Anand Bhattad and Ranjay Krishna", "abstract": "  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n", "link": "http://arxiv.org/abs/2403.14617v1", "date": "2024-03-21", "relevancy": 2.3534, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6332}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5918}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5669}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Videoshop%3A%20Localized%20Semantic%20Video%20Editing%20with%20Noise-Extrapolated%0A%20%20Diffusion%20Inversion&body=Title%3A%20Videoshop%3A%20Localized%20Semantic%20Video%20Editing%20with%20Noise-Extrapolated%0A%20%20Diffusion%20Inversion%0AAuthor%3A%20Xiang%20Fan%20and%20Anand%20Bhattad%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20We%20introduce%20Videoshop%2C%20a%20training-free%20video%20editing%20algorithm%20for%20localized%0Asemantic%20edits.%20Videoshop%20allows%20users%20to%20use%20any%20editing%20software%2C%20including%0APhotoshop%20and%20generative%20inpainting%2C%20to%20modify%20the%20first%20frame%3B%20it%0Aautomatically%20propagates%20those%20changes%2C%20with%20semantic%2C%20spatial%2C%20and%20temporally%0Aconsistent%20motion%2C%20to%20the%20remaining%20frames.%20Unlike%20existing%20methods%20that%20enable%0Aedits%20only%20through%20imprecise%20textual%20instructions%2C%20Videoshop%20allows%20users%20to%0Aadd%20or%20remove%20objects%2C%20semantically%20change%20objects%2C%20insert%20stock%20photos%20into%0Avideos%2C%20etc.%20with%20fine-grained%20control%20over%20locations%20and%20appearance.%20We%0Aachieve%20this%20through%20image-based%20video%20editing%20by%20inverting%20latents%20with%20noise%0Aextrapolation%2C%20from%20which%20we%20generate%20videos%20conditioned%20on%20the%20edited%20image.%0AVideoshop%20produces%20higher%20quality%20edits%20against%206%20baselines%20on%202%20editing%0Abenchmarks%20using%2010%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14617v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Videoshop%3A%20Localized%20Semantic%20Video%20Editing%20with%20Noise-Extrapolated%0A%20%20Diffusion%20Inversion&entry.906535625=Xiang%20Fan%20and%20Anand%20Bhattad%20and%20Ranjay%20Krishna&entry.1292438233=%20%20We%20introduce%20Videoshop%2C%20a%20training-free%20video%20editing%20algorithm%20for%20localized%0Asemantic%20edits.%20Videoshop%20allows%20users%20to%20use%20any%20editing%20software%2C%20including%0APhotoshop%20and%20generative%20inpainting%2C%20to%20modify%20the%20first%20frame%3B%20it%0Aautomatically%20propagates%20those%20changes%2C%20with%20semantic%2C%20spatial%2C%20and%20temporally%0Aconsistent%20motion%2C%20to%20the%20remaining%20frames.%20Unlike%20existing%20methods%20that%20enable%0Aedits%20only%20through%20imprecise%20textual%20instructions%2C%20Videoshop%20allows%20users%20to%0Aadd%20or%20remove%20objects%2C%20semantically%20change%20objects%2C%20insert%20stock%20photos%20into%0Avideos%2C%20etc.%20with%20fine-grained%20control%20over%20locations%20and%20appearance.%20We%0Aachieve%20this%20through%20image-based%20video%20editing%20by%20inverting%20latents%20with%20noise%0Aextrapolation%2C%20from%20which%20we%20generate%20videos%20conditioned%20on%20the%20edited%20image.%0AVideoshop%20produces%20higher%20quality%20edits%20against%206%20baselines%20on%202%20editing%0Abenchmarks%20using%2010%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14617v1&entry.124074799=Read"},
{"title": "Zero-Shot Multi-Object Shape Completion", "author": "Shun Iwase and Katherine Liu and Vitor Guizilini and Adrien Gaidon and Kris Kitani and Rares Ambrus and Sergey Zakharov", "abstract": "  We present a 3D shape completion method that recovers the complete geometry\nof multiple objects in complex scenes from a single RGB-D image. Despite\nnotable advancements in single object 3D shape completion, high-quality\nreconstructions in highly cluttered real-world multi-object scenes remains a\nchallenge. To address this issue, we propose OctMAE, an architecture that\nleverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near\nreal-time multi-object shape completion through both local and global geometric\nreasoning. Because a na\\\"ive 3D MAE can be computationally intractable and\nmemory intensive even in the latent space, we introduce a novel occlusion\nmasking strategy and adopt 3D rotary embeddings, which significantly improves\nthe runtime and shape completion quality. To generalize to a wide range of\nobjects in diverse scenes, we create a large-scale photorealistic dataset,\nfeaturing a diverse set of 12K 3D object models from the Objaverse dataset\nwhich are rendered in multi-object scenes with physics-based positioning. Our\nmethod outperforms the current state-of-the-art on both synthetic and\nreal-world datasets and demonstrates a strong zero-shot capability.\n", "link": "http://arxiv.org/abs/2403.14628v1", "date": "2024-03-21", "relevancy": 2.3388, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6194}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.568}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5568}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Multi-Object%20Shape%20Completion&body=Title%3A%20Zero-Shot%20Multi-Object%20Shape%20Completion%0AAuthor%3A%20Shun%20Iwase%20and%20Katherine%20Liu%20and%20Vitor%20Guizilini%20and%20Adrien%20Gaidon%20and%20Kris%20Kitani%20and%20Rares%20Ambrus%20and%20Sergey%20Zakharov%0AAbstract%3A%20%20%20We%20present%20a%203D%20shape%20completion%20method%20that%20recovers%20the%20complete%20geometry%0Aof%20multiple%20objects%20in%20complex%20scenes%20from%20a%20single%20RGB-D%20image.%20Despite%0Anotable%20advancements%20in%20single%20object%203D%20shape%20completion%2C%20high-quality%0Areconstructions%20in%20highly%20cluttered%20real-world%20multi-object%20scenes%20remains%20a%0Achallenge.%20To%20address%20this%20issue%2C%20we%20propose%20OctMAE%2C%20an%20architecture%20that%0Aleverages%20an%20Octree%20U-Net%20and%20a%20latent%203D%20MAE%20to%20achieve%20high-quality%20and%20near%0Areal-time%20multi-object%20shape%20completion%20through%20both%20local%20and%20global%20geometric%0Areasoning.%20Because%20a%20na%5C%22ive%203D%20MAE%20can%20be%20computationally%20intractable%20and%0Amemory%20intensive%20even%20in%20the%20latent%20space%2C%20we%20introduce%20a%20novel%20occlusion%0Amasking%20strategy%20and%20adopt%203D%20rotary%20embeddings%2C%20which%20significantly%20improves%0Athe%20runtime%20and%20shape%20completion%20quality.%20To%20generalize%20to%20a%20wide%20range%20of%0Aobjects%20in%20diverse%20scenes%2C%20we%20create%20a%20large-scale%20photorealistic%20dataset%2C%0Afeaturing%20a%20diverse%20set%20of%2012K%203D%20object%20models%20from%20the%20Objaverse%20dataset%0Awhich%20are%20rendered%20in%20multi-object%20scenes%20with%20physics-based%20positioning.%20Our%0Amethod%20outperforms%20the%20current%20state-of-the-art%20on%20both%20synthetic%20and%0Areal-world%20datasets%20and%20demonstrates%20a%20strong%20zero-shot%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14628v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Multi-Object%20Shape%20Completion&entry.906535625=Shun%20Iwase%20and%20Katherine%20Liu%20and%20Vitor%20Guizilini%20and%20Adrien%20Gaidon%20and%20Kris%20Kitani%20and%20Rares%20Ambrus%20and%20Sergey%20Zakharov&entry.1292438233=%20%20We%20present%20a%203D%20shape%20completion%20method%20that%20recovers%20the%20complete%20geometry%0Aof%20multiple%20objects%20in%20complex%20scenes%20from%20a%20single%20RGB-D%20image.%20Despite%0Anotable%20advancements%20in%20single%20object%203D%20shape%20completion%2C%20high-quality%0Areconstructions%20in%20highly%20cluttered%20real-world%20multi-object%20scenes%20remains%20a%0Achallenge.%20To%20address%20this%20issue%2C%20we%20propose%20OctMAE%2C%20an%20architecture%20that%0Aleverages%20an%20Octree%20U-Net%20and%20a%20latent%203D%20MAE%20to%20achieve%20high-quality%20and%20near%0Areal-time%20multi-object%20shape%20completion%20through%20both%20local%20and%20global%20geometric%0Areasoning.%20Because%20a%20na%5C%22ive%203D%20MAE%20can%20be%20computationally%20intractable%20and%0Amemory%20intensive%20even%20in%20the%20latent%20space%2C%20we%20introduce%20a%20novel%20occlusion%0Amasking%20strategy%20and%20adopt%203D%20rotary%20embeddings%2C%20which%20significantly%20improves%0Athe%20runtime%20and%20shape%20completion%20quality.%20To%20generalize%20to%20a%20wide%20range%20of%0Aobjects%20in%20diverse%20scenes%2C%20we%20create%20a%20large-scale%20photorealistic%20dataset%2C%0Afeaturing%20a%20diverse%20set%20of%2012K%203D%20object%20models%20from%20the%20Objaverse%20dataset%0Awhich%20are%20rendered%20in%20multi-object%20scenes%20with%20physics-based%20positioning.%20Our%0Amethod%20outperforms%20the%20current%20state-of-the-art%20on%20both%20synthetic%20and%0Areal-world%20datasets%20and%20demonstrates%20a%20strong%20zero-shot%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14628v1&entry.124074799=Read"},
{"title": "Style-Extracting Diffusion Models for Semi-Supervised Histopathology\n  Segmentation", "author": "Mathias \u00d6ttl and Frauke Wilm and Jana Steenpass and Jingna Qiu and Matthias R\u00fcbner and Arndt Hartmann and Matthias Beckmann and Peter Fasching and Andreas Maier and Ramona Erber and Bernhard Kainz and Katharina Breininger", "abstract": "  Deep learning-based image generation has seen significant advancements with\ndiffusion models, notably improving the quality of generated images. Despite\nthese developments, generating images with unseen characteristics beneficial\nfor downstream tasks has received limited attention. To bridge this gap, we\npropose Style-Extracting Diffusion Models, featuring two conditioning\nmechanisms. Specifically, we utilize 1) a style conditioning mechanism which\nallows to inject style information of previously unseen images during image\ngeneration and 2) a content conditioning which can be targeted to a downstream\ntask, e.g., layout for segmentation. We introduce a trainable style encoder to\nextract style information from images, and an aggregation block that merges\nstyle information from multiple style inputs. This architecture enables the\ngeneration of images with unseen styles in a zero-shot manner, by leveraging\nstyles from unseen images, resulting in more diverse generations. In this work,\nwe use the image layout as target condition and first show the capability of\nour method on a natural image dataset as a proof-of-concept. We further\ndemonstrate its versatility in histopathology, where we combine prior knowledge\nabout tissue composition and unannotated data to create diverse synthetic\nimages with known layouts. This allows us to generate additional synthetic data\nto train a segmentation network in a semi-supervised fashion. We verify the\nadded value of the generated images by showing improved segmentation results\nand lower performance variability between patients when synthetic images are\nincluded during segmentation training. Our code will be made publicly available\nat [LINK].\n", "link": "http://arxiv.org/abs/2403.14429v1", "date": "2024-03-21", "relevancy": 2.3369, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5942}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5779}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5753}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Style-Extracting%20Diffusion%20Models%20for%20Semi-Supervised%20Histopathology%0A%20%20Segmentation&body=Title%3A%20Style-Extracting%20Diffusion%20Models%20for%20Semi-Supervised%20Histopathology%0A%20%20Segmentation%0AAuthor%3A%20Mathias%20%C3%96ttl%20and%20Frauke%20Wilm%20and%20Jana%20Steenpass%20and%20Jingna%20Qiu%20and%20Matthias%20R%C3%BCbner%20and%20Arndt%20Hartmann%20and%20Matthias%20Beckmann%20and%20Peter%20Fasching%20and%20Andreas%20Maier%20and%20Ramona%20Erber%20and%20Bernhard%20Kainz%20and%20Katharina%20Breininger%0AAbstract%3A%20%20%20Deep%20learning-based%20image%20generation%20has%20seen%20significant%20advancements%20with%0Adiffusion%20models%2C%20notably%20improving%20the%20quality%20of%20generated%20images.%20Despite%0Athese%20developments%2C%20generating%20images%20with%20unseen%20characteristics%20beneficial%0Afor%20downstream%20tasks%20has%20received%20limited%20attention.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20Style-Extracting%20Diffusion%20Models%2C%20featuring%20two%20conditioning%0Amechanisms.%20Specifically%2C%20we%20utilize%201%29%20a%20style%20conditioning%20mechanism%20which%0Aallows%20to%20inject%20style%20information%20of%20previously%20unseen%20images%20during%20image%0Ageneration%20and%202%29%20a%20content%20conditioning%20which%20can%20be%20targeted%20to%20a%20downstream%0Atask%2C%20e.g.%2C%20layout%20for%20segmentation.%20We%20introduce%20a%20trainable%20style%20encoder%20to%0Aextract%20style%20information%20from%20images%2C%20and%20an%20aggregation%20block%20that%20merges%0Astyle%20information%20from%20multiple%20style%20inputs.%20This%20architecture%20enables%20the%0Ageneration%20of%20images%20with%20unseen%20styles%20in%20a%20zero-shot%20manner%2C%20by%20leveraging%0Astyles%20from%20unseen%20images%2C%20resulting%20in%20more%20diverse%20generations.%20In%20this%20work%2C%0Awe%20use%20the%20image%20layout%20as%20target%20condition%20and%20first%20show%20the%20capability%20of%0Aour%20method%20on%20a%20natural%20image%20dataset%20as%20a%20proof-of-concept.%20We%20further%0Ademonstrate%20its%20versatility%20in%20histopathology%2C%20where%20we%20combine%20prior%20knowledge%0Aabout%20tissue%20composition%20and%20unannotated%20data%20to%20create%20diverse%20synthetic%0Aimages%20with%20known%20layouts.%20This%20allows%20us%20to%20generate%20additional%20synthetic%20data%0Ato%20train%20a%20segmentation%20network%20in%20a%20semi-supervised%20fashion.%20We%20verify%20the%0Aadded%20value%20of%20the%20generated%20images%20by%20showing%20improved%20segmentation%20results%0Aand%20lower%20performance%20variability%20between%20patients%20when%20synthetic%20images%20are%0Aincluded%20during%20segmentation%20training.%20Our%20code%20will%20be%20made%20publicly%20available%0Aat%20%5BLINK%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14429v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Style-Extracting%20Diffusion%20Models%20for%20Semi-Supervised%20Histopathology%0A%20%20Segmentation&entry.906535625=Mathias%20%C3%96ttl%20and%20Frauke%20Wilm%20and%20Jana%20Steenpass%20and%20Jingna%20Qiu%20and%20Matthias%20R%C3%BCbner%20and%20Arndt%20Hartmann%20and%20Matthias%20Beckmann%20and%20Peter%20Fasching%20and%20Andreas%20Maier%20and%20Ramona%20Erber%20and%20Bernhard%20Kainz%20and%20Katharina%20Breininger&entry.1292438233=%20%20Deep%20learning-based%20image%20generation%20has%20seen%20significant%20advancements%20with%0Adiffusion%20models%2C%20notably%20improving%20the%20quality%20of%20generated%20images.%20Despite%0Athese%20developments%2C%20generating%20images%20with%20unseen%20characteristics%20beneficial%0Afor%20downstream%20tasks%20has%20received%20limited%20attention.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20Style-Extracting%20Diffusion%20Models%2C%20featuring%20two%20conditioning%0Amechanisms.%20Specifically%2C%20we%20utilize%201%29%20a%20style%20conditioning%20mechanism%20which%0Aallows%20to%20inject%20style%20information%20of%20previously%20unseen%20images%20during%20image%0Ageneration%20and%202%29%20a%20content%20conditioning%20which%20can%20be%20targeted%20to%20a%20downstream%0Atask%2C%20e.g.%2C%20layout%20for%20segmentation.%20We%20introduce%20a%20trainable%20style%20encoder%20to%0Aextract%20style%20information%20from%20images%2C%20and%20an%20aggregation%20block%20that%20merges%0Astyle%20information%20from%20multiple%20style%20inputs.%20This%20architecture%20enables%20the%0Ageneration%20of%20images%20with%20unseen%20styles%20in%20a%20zero-shot%20manner%2C%20by%20leveraging%0Astyles%20from%20unseen%20images%2C%20resulting%20in%20more%20diverse%20generations.%20In%20this%20work%2C%0Awe%20use%20the%20image%20layout%20as%20target%20condition%20and%20first%20show%20the%20capability%20of%0Aour%20method%20on%20a%20natural%20image%20dataset%20as%20a%20proof-of-concept.%20We%20further%0Ademonstrate%20its%20versatility%20in%20histopathology%2C%20where%20we%20combine%20prior%20knowledge%0Aabout%20tissue%20composition%20and%20unannotated%20data%20to%20create%20diverse%20synthetic%0Aimages%20with%20known%20layouts.%20This%20allows%20us%20to%20generate%20additional%20synthetic%20data%0Ato%20train%20a%20segmentation%20network%20in%20a%20semi-supervised%20fashion.%20We%20verify%20the%0Aadded%20value%20of%20the%20generated%20images%20by%20showing%20improved%20segmentation%20results%0Aand%20lower%20performance%20variability%20between%20patients%20when%20synthetic%20images%20are%0Aincluded%20during%20segmentation%20training.%20Our%20code%20will%20be%20made%20publicly%20available%0Aat%20%5BLINK%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14429v1&entry.124074799=Read"},
{"title": "Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation", "author": "Ruyi Lian and Haibin Ling", "abstract": "  Localizing predefined 3D keypoints in a 2D image is an effective way to\nestablish 3D-2D correspondences for 6DoF object pose estimation. However,\nunreliable localization results of invisible keypoints degrade the quality of\ncorrespondences. In this paper, we address this issue by localizing the\nimportant keypoints in terms of visibility. Since keypoint visibility\ninformation is currently missing in dataset collection process, we propose an\nefficient way to generate binary visibility labels from available object-level\nannotations, for keypoints of both asymmetric objects and symmetric objects. We\nfurther derive real-valued visibility-aware importance from binary labels based\non PageRank algorithm. Taking advantage of the flexibility of our\nvisibility-aware importance, we construct VAPO (Visibility-Aware POse\nestimator) by integrating the visibility-aware importance with a\nstate-of-the-art pose estimation algorithm, along with additional positional\nencoding. Extensive experiments are conducted on popular pose estimation\nbenchmarks including Linemod, Linemod-Occlusion, and YCB-V. The results show\nthat, VAPO improves both the keypoint correspondences and final estimated\nposes, and clearly achieves state-of-the-art performances.\n", "link": "http://arxiv.org/abs/2403.14559v1", "date": "2024-03-21", "relevancy": 2.325, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6242}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5607}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5251}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visibility-Aware%20Keypoint%20Localization%20for%206DoF%20Object%20Pose%20Estimation&body=Title%3A%20Visibility-Aware%20Keypoint%20Localization%20for%206DoF%20Object%20Pose%20Estimation%0AAuthor%3A%20Ruyi%20Lian%20and%20Haibin%20Ling%0AAbstract%3A%20%20%20Localizing%20predefined%203D%20keypoints%20in%20a%202D%20image%20is%20an%20effective%20way%20to%0Aestablish%203D-2D%20correspondences%20for%206DoF%20object%20pose%20estimation.%20However%2C%0Aunreliable%20localization%20results%20of%20invisible%20keypoints%20degrade%20the%20quality%20of%0Acorrespondences.%20In%20this%20paper%2C%20we%20address%20this%20issue%20by%20localizing%20the%0Aimportant%20keypoints%20in%20terms%20of%20visibility.%20Since%20keypoint%20visibility%0Ainformation%20is%20currently%20missing%20in%20dataset%20collection%20process%2C%20we%20propose%20an%0Aefficient%20way%20to%20generate%20binary%20visibility%20labels%20from%20available%20object-level%0Aannotations%2C%20for%20keypoints%20of%20both%20asymmetric%20objects%20and%20symmetric%20objects.%20We%0Afurther%20derive%20real-valued%20visibility-aware%20importance%20from%20binary%20labels%20based%0Aon%20PageRank%20algorithm.%20Taking%20advantage%20of%20the%20flexibility%20of%20our%0Avisibility-aware%20importance%2C%20we%20construct%20VAPO%20%28Visibility-Aware%20POse%0Aestimator%29%20by%20integrating%20the%20visibility-aware%20importance%20with%20a%0Astate-of-the-art%20pose%20estimation%20algorithm%2C%20along%20with%20additional%20positional%0Aencoding.%20Extensive%20experiments%20are%20conducted%20on%20popular%20pose%20estimation%0Abenchmarks%20including%20Linemod%2C%20Linemod-Occlusion%2C%20and%20YCB-V.%20The%20results%20show%0Athat%2C%20VAPO%20improves%20both%20the%20keypoint%20correspondences%20and%20final%20estimated%0Aposes%2C%20and%20clearly%20achieves%20state-of-the-art%20performances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14559v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visibility-Aware%20Keypoint%20Localization%20for%206DoF%20Object%20Pose%20Estimation&entry.906535625=Ruyi%20Lian%20and%20Haibin%20Ling&entry.1292438233=%20%20Localizing%20predefined%203D%20keypoints%20in%20a%202D%20image%20is%20an%20effective%20way%20to%0Aestablish%203D-2D%20correspondences%20for%206DoF%20object%20pose%20estimation.%20However%2C%0Aunreliable%20localization%20results%20of%20invisible%20keypoints%20degrade%20the%20quality%20of%0Acorrespondences.%20In%20this%20paper%2C%20we%20address%20this%20issue%20by%20localizing%20the%0Aimportant%20keypoints%20in%20terms%20of%20visibility.%20Since%20keypoint%20visibility%0Ainformation%20is%20currently%20missing%20in%20dataset%20collection%20process%2C%20we%20propose%20an%0Aefficient%20way%20to%20generate%20binary%20visibility%20labels%20from%20available%20object-level%0Aannotations%2C%20for%20keypoints%20of%20both%20asymmetric%20objects%20and%20symmetric%20objects.%20We%0Afurther%20derive%20real-valued%20visibility-aware%20importance%20from%20binary%20labels%20based%0Aon%20PageRank%20algorithm.%20Taking%20advantage%20of%20the%20flexibility%20of%20our%0Avisibility-aware%20importance%2C%20we%20construct%20VAPO%20%28Visibility-Aware%20POse%0Aestimator%29%20by%20integrating%20the%20visibility-aware%20importance%20with%20a%0Astate-of-the-art%20pose%20estimation%20algorithm%2C%20along%20with%20additional%20positional%0Aencoding.%20Extensive%20experiments%20are%20conducted%20on%20popular%20pose%20estimation%0Abenchmarks%20including%20Linemod%2C%20Linemod-Occlusion%2C%20and%20YCB-V.%20The%20results%20show%0Athat%2C%20VAPO%20improves%20both%20the%20keypoint%20correspondences%20and%20final%20estimated%0Aposes%2C%20and%20clearly%20achieves%20state-of-the-art%20performances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14559v1&entry.124074799=Read"},
{"title": "Exploring Green AI for Audio Deepfake Detection", "author": "Subhajit Saha and Md Sahidullah and Swagatam Das", "abstract": "  The state-of-the-art audio deepfake detectors leveraging deep neural networks\nexhibit impressive recognition performance. Nonetheless, this advantage is\naccompanied by a significant carbon footprint. This is mainly due to the use of\nhigh-performance computing with accelerators and high training time. Studies\nshow that average deep NLP model produces around 626k lbs of\nCO\\textsubscript{2} which is equivalent to five times of average US car\nemission at its lifetime. This is certainly a massive threat to the\nenvironment. To tackle this challenge, this study presents a novel framework\nfor audio deepfake detection that can be seamlessly trained using standard CPU\nresources. Our proposed framework utilizes off-the-shelve self-supervised\nlearning (SSL) based models which are pre-trained and available in public\nrepositories. In contrast to existing methods that fine-tune SSL models and\nemploy additional deep neural networks for downstream tasks, we exploit\nclassical machine learning algorithms such as logistic regression and shallow\nneural networks using the SSL embeddings extracted using the pre-trained model.\nOur approach shows competitive results compared to the commonly used\nhigh-carbon footprint approaches. In experiments with the ASVspoof 2019 LA\ndataset, we achieve a 0.90\\% equal error rate (EER) with less than 1k trainable\nmodel parameters. To encourage further research in this direction and support\nreproducible results, the Python code will be made publicly accessible\nfollowing acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-\n", "link": "http://arxiv.org/abs/2403.14290v1", "date": "2024-03-21", "relevancy": 2.3191, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4731}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4625}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4559}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Green%20AI%20for%20Audio%20Deepfake%20Detection&body=Title%3A%20Exploring%20Green%20AI%20for%20Audio%20Deepfake%20Detection%0AAuthor%3A%20Subhajit%20Saha%20and%20Md%20Sahidullah%20and%20Swagatam%20Das%0AAbstract%3A%20%20%20The%20state-of-the-art%20audio%20deepfake%20detectors%20leveraging%20deep%20neural%20networks%0Aexhibit%20impressive%20recognition%20performance.%20Nonetheless%2C%20this%20advantage%20is%0Aaccompanied%20by%20a%20significant%20carbon%20footprint.%20This%20is%20mainly%20due%20to%20the%20use%20of%0Ahigh-performance%20computing%20with%20accelerators%20and%20high%20training%20time.%20Studies%0Ashow%20that%20average%20deep%20NLP%20model%20produces%20around%20626k%20lbs%20of%0ACO%5Ctextsubscript%7B2%7D%20which%20is%20equivalent%20to%20five%20times%20of%20average%20US%20car%0Aemission%20at%20its%20lifetime.%20This%20is%20certainly%20a%20massive%20threat%20to%20the%0Aenvironment.%20To%20tackle%20this%20challenge%2C%20this%20study%20presents%20a%20novel%20framework%0Afor%20audio%20deepfake%20detection%20that%20can%20be%20seamlessly%20trained%20using%20standard%20CPU%0Aresources.%20Our%20proposed%20framework%20utilizes%20off-the-shelve%20self-supervised%0Alearning%20%28SSL%29%20based%20models%20which%20are%20pre-trained%20and%20available%20in%20public%0Arepositories.%20In%20contrast%20to%20existing%20methods%20that%20fine-tune%20SSL%20models%20and%0Aemploy%20additional%20deep%20neural%20networks%20for%20downstream%20tasks%2C%20we%20exploit%0Aclassical%20machine%20learning%20algorithms%20such%20as%20logistic%20regression%20and%20shallow%0Aneural%20networks%20using%20the%20SSL%20embeddings%20extracted%20using%20the%20pre-trained%20model.%0AOur%20approach%20shows%20competitive%20results%20compared%20to%20the%20commonly%20used%0Ahigh-carbon%20footprint%20approaches.%20In%20experiments%20with%20the%20ASVspoof%202019%20LA%0Adataset%2C%20we%20achieve%20a%200.90%5C%25%20equal%20error%20rate%20%28EER%29%20with%20less%20than%201k%20trainable%0Amodel%20parameters.%20To%20encourage%20further%20research%20in%20this%20direction%20and%20support%0Areproducible%20results%2C%20the%20Python%20code%20will%20be%20made%20publicly%20accessible%0Afollowing%20acceptance.%20Github%3A%20https%3A//github.com/sahasubhajit/Speech-Spoofing-%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14290v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Green%20AI%20for%20Audio%20Deepfake%20Detection&entry.906535625=Subhajit%20Saha%20and%20Md%20Sahidullah%20and%20Swagatam%20Das&entry.1292438233=%20%20The%20state-of-the-art%20audio%20deepfake%20detectors%20leveraging%20deep%20neural%20networks%0Aexhibit%20impressive%20recognition%20performance.%20Nonetheless%2C%20this%20advantage%20is%0Aaccompanied%20by%20a%20significant%20carbon%20footprint.%20This%20is%20mainly%20due%20to%20the%20use%20of%0Ahigh-performance%20computing%20with%20accelerators%20and%20high%20training%20time.%20Studies%0Ashow%20that%20average%20deep%20NLP%20model%20produces%20around%20626k%20lbs%20of%0ACO%5Ctextsubscript%7B2%7D%20which%20is%20equivalent%20to%20five%20times%20of%20average%20US%20car%0Aemission%20at%20its%20lifetime.%20This%20is%20certainly%20a%20massive%20threat%20to%20the%0Aenvironment.%20To%20tackle%20this%20challenge%2C%20this%20study%20presents%20a%20novel%20framework%0Afor%20audio%20deepfake%20detection%20that%20can%20be%20seamlessly%20trained%20using%20standard%20CPU%0Aresources.%20Our%20proposed%20framework%20utilizes%20off-the-shelve%20self-supervised%0Alearning%20%28SSL%29%20based%20models%20which%20are%20pre-trained%20and%20available%20in%20public%0Arepositories.%20In%20contrast%20to%20existing%20methods%20that%20fine-tune%20SSL%20models%20and%0Aemploy%20additional%20deep%20neural%20networks%20for%20downstream%20tasks%2C%20we%20exploit%0Aclassical%20machine%20learning%20algorithms%20such%20as%20logistic%20regression%20and%20shallow%0Aneural%20networks%20using%20the%20SSL%20embeddings%20extracted%20using%20the%20pre-trained%20model.%0AOur%20approach%20shows%20competitive%20results%20compared%20to%20the%20commonly%20used%0Ahigh-carbon%20footprint%20approaches.%20In%20experiments%20with%20the%20ASVspoof%202019%20LA%0Adataset%2C%20we%20achieve%20a%200.90%5C%25%20equal%20error%20rate%20%28EER%29%20with%20less%20than%201k%20trainable%0Amodel%20parameters.%20To%20encourage%20further%20research%20in%20this%20direction%20and%20support%0Areproducible%20results%2C%20the%20Python%20code%20will%20be%20made%20publicly%20accessible%0Afollowing%20acceptance.%20Github%3A%20https%3A//github.com/sahasubhajit/Speech-Spoofing-%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14290v1&entry.124074799=Read"},
{"title": "DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified &\n  Accurate Image Editing", "author": "Yueru Jia and Yuhui Yuan and Aosong Cheng and Chuke Wang and Ji Li and Huizhu Jia and Shanghang Zhang", "abstract": "  Recently, how to achieve precise image editing has attracted increasing\nattention, especially given the remarkable success of text-to-image generation\nmodels. To unify various spatial-aware image editing abilities into one\nframework, we adopt the concept of layers from the design domain to manipulate\nobjects flexibly with various operations. The key insight is to transform the\nspatial-aware image editing task into a combination of two sub-tasks:\nmulti-layered latent decomposition and multi-layered latent fusion. First, we\nsegment the latent representations of the source images into multiple layers,\nwhich include several object layers and one incomplete background layer that\nnecessitates reliable inpainting. To avoid extra tuning, we further explore the\ninner inpainting ability within the self-attention mechanism. We introduce a\nkey-masking self-attention scheme that can propagate the surrounding context\ninformation into the masked region while mitigating its impact on the regions\noutside the mask. Second, we propose an instruction-guided latent fusion that\npastes the multi-layered latent representations onto a canvas latent. We also\nintroduce an artifact suppression scheme in the latent space to enhance the\ninpainting quality. Due to the inherent modular advantages of such\nmulti-layered representations, we can achieve accurate image editing, and we\ndemonstrate that our approach consistently surpasses the latest spatial editing\nmethods, including Self-Guidance and DiffEditor. Last, we show that our\napproach is a unified framework that supports various accurate image editing\ntasks on more than six different editing tasks.\n", "link": "http://arxiv.org/abs/2403.14487v1", "date": "2024-03-21", "relevancy": 2.3026, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6003}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5584}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5573}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DesignEdit%3A%20Multi-Layered%20Latent%20Decomposition%20and%20Fusion%20for%20Unified%20%26%0A%20%20Accurate%20Image%20Editing&body=Title%3A%20DesignEdit%3A%20Multi-Layered%20Latent%20Decomposition%20and%20Fusion%20for%20Unified%20%26%0A%20%20Accurate%20Image%20Editing%0AAuthor%3A%20Yueru%20Jia%20and%20Yuhui%20Yuan%20and%20Aosong%20Cheng%20and%20Chuke%20Wang%20and%20Ji%20Li%20and%20Huizhu%20Jia%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20how%20to%20achieve%20precise%20image%20editing%20has%20attracted%20increasing%0Aattention%2C%20especially%20given%20the%20remarkable%20success%20of%20text-to-image%20generation%0Amodels.%20To%20unify%20various%20spatial-aware%20image%20editing%20abilities%20into%20one%0Aframework%2C%20we%20adopt%20the%20concept%20of%20layers%20from%20the%20design%20domain%20to%20manipulate%0Aobjects%20flexibly%20with%20various%20operations.%20The%20key%20insight%20is%20to%20transform%20the%0Aspatial-aware%20image%20editing%20task%20into%20a%20combination%20of%20two%20sub-tasks%3A%0Amulti-layered%20latent%20decomposition%20and%20multi-layered%20latent%20fusion.%20First%2C%20we%0Asegment%20the%20latent%20representations%20of%20the%20source%20images%20into%20multiple%20layers%2C%0Awhich%20include%20several%20object%20layers%20and%20one%20incomplete%20background%20layer%20that%0Anecessitates%20reliable%20inpainting.%20To%20avoid%20extra%20tuning%2C%20we%20further%20explore%20the%0Ainner%20inpainting%20ability%20within%20the%20self-attention%20mechanism.%20We%20introduce%20a%0Akey-masking%20self-attention%20scheme%20that%20can%20propagate%20the%20surrounding%20context%0Ainformation%20into%20the%20masked%20region%20while%20mitigating%20its%20impact%20on%20the%20regions%0Aoutside%20the%20mask.%20Second%2C%20we%20propose%20an%20instruction-guided%20latent%20fusion%20that%0Apastes%20the%20multi-layered%20latent%20representations%20onto%20a%20canvas%20latent.%20We%20also%0Aintroduce%20an%20artifact%20suppression%20scheme%20in%20the%20latent%20space%20to%20enhance%20the%0Ainpainting%20quality.%20Due%20to%20the%20inherent%20modular%20advantages%20of%20such%0Amulti-layered%20representations%2C%20we%20can%20achieve%20accurate%20image%20editing%2C%20and%20we%0Ademonstrate%20that%20our%20approach%20consistently%20surpasses%20the%20latest%20spatial%20editing%0Amethods%2C%20including%20Self-Guidance%20and%20DiffEditor.%20Last%2C%20we%20show%20that%20our%0Aapproach%20is%20a%20unified%20framework%20that%20supports%20various%20accurate%20image%20editing%0Atasks%20on%20more%20than%20six%20different%20editing%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14487v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DesignEdit%3A%20Multi-Layered%20Latent%20Decomposition%20and%20Fusion%20for%20Unified%20%26%0A%20%20Accurate%20Image%20Editing&entry.906535625=Yueru%20Jia%20and%20Yuhui%20Yuan%20and%20Aosong%20Cheng%20and%20Chuke%20Wang%20and%20Ji%20Li%20and%20Huizhu%20Jia%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Recently%2C%20how%20to%20achieve%20precise%20image%20editing%20has%20attracted%20increasing%0Aattention%2C%20especially%20given%20the%20remarkable%20success%20of%20text-to-image%20generation%0Amodels.%20To%20unify%20various%20spatial-aware%20image%20editing%20abilities%20into%20one%0Aframework%2C%20we%20adopt%20the%20concept%20of%20layers%20from%20the%20design%20domain%20to%20manipulate%0Aobjects%20flexibly%20with%20various%20operations.%20The%20key%20insight%20is%20to%20transform%20the%0Aspatial-aware%20image%20editing%20task%20into%20a%20combination%20of%20two%20sub-tasks%3A%0Amulti-layered%20latent%20decomposition%20and%20multi-layered%20latent%20fusion.%20First%2C%20we%0Asegment%20the%20latent%20representations%20of%20the%20source%20images%20into%20multiple%20layers%2C%0Awhich%20include%20several%20object%20layers%20and%20one%20incomplete%20background%20layer%20that%0Anecessitates%20reliable%20inpainting.%20To%20avoid%20extra%20tuning%2C%20we%20further%20explore%20the%0Ainner%20inpainting%20ability%20within%20the%20self-attention%20mechanism.%20We%20introduce%20a%0Akey-masking%20self-attention%20scheme%20that%20can%20propagate%20the%20surrounding%20context%0Ainformation%20into%20the%20masked%20region%20while%20mitigating%20its%20impact%20on%20the%20regions%0Aoutside%20the%20mask.%20Second%2C%20we%20propose%20an%20instruction-guided%20latent%20fusion%20that%0Apastes%20the%20multi-layered%20latent%20representations%20onto%20a%20canvas%20latent.%20We%20also%0Aintroduce%20an%20artifact%20suppression%20scheme%20in%20the%20latent%20space%20to%20enhance%20the%0Ainpainting%20quality.%20Due%20to%20the%20inherent%20modular%20advantages%20of%20such%0Amulti-layered%20representations%2C%20we%20can%20achieve%20accurate%20image%20editing%2C%20and%20we%0Ademonstrate%20that%20our%20approach%20consistently%20surpasses%20the%20latest%20spatial%20editing%0Amethods%2C%20including%20Self-Guidance%20and%20DiffEditor.%20Last%2C%20we%20show%20that%20our%0Aapproach%20is%20a%20unified%20framework%20that%20supports%20various%20accurate%20image%20editing%0Atasks%20on%20more%20than%20six%20different%20editing%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14487v1&entry.124074799=Read"},
{"title": "DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning", "author": "Jonathan Lebensold and Maziar Sanjabi and Pietro Astolfi and Adriana Romero-Soriano and Kamalika Chaudhuri and Mike Rabbat and Chuan Guo", "abstract": "  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n", "link": "http://arxiv.org/abs/2403.14421v1", "date": "2024-03-21", "relevancy": 2.2903, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5788}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5701}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5632}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning&body=Title%3A%20DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning%0AAuthor%3A%20Jonathan%20Lebensold%20and%20Maziar%20Sanjabi%20and%20Pietro%20Astolfi%20and%20Adriana%20Romero-Soriano%20and%20Kamalika%20Chaudhuri%20and%20Mike%20Rabbat%20and%20Chuan%20Guo%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20been%20shown%20to%20suffer%20from%20sample-level%0Amemorization%2C%20possibly%20reproducing%20near-perfect%20replica%20of%20images%20that%20they%20are%0Atrained%20on%2C%20which%20may%20be%20undesirable.%20To%20remedy%20this%20issue%2C%20we%20develop%20the%0Afirst%20differentially%20private%20%28DP%29%20retrieval-augmented%20generation%20algorithm%20that%0Ais%20capable%20of%20generating%20high-quality%20image%20samples%20while%20providing%20provable%0Aprivacy%20guarantees.%20Specifically%2C%20we%20assume%20access%20to%20a%20text-to-image%20diffusion%0Amodel%20trained%20on%20a%20small%20amount%20of%20public%20data%2C%20and%20design%20a%20DP%20retrieval%0Amechanism%20to%20augment%20the%20text%20prompt%20with%20samples%20retrieved%20from%20a%20private%0Aretrieval%20dataset.%20Our%20%5Cemph%7Bdifferentially%20private%20retrieval-augmented%0Adiffusion%20model%7D%20%28DP-RDM%29%20requires%20no%20fine-tuning%20on%20the%20retrieval%20dataset%20to%0Aadapt%20to%20another%20domain%2C%20and%20can%20use%20state-of-the-art%20generative%20models%20to%0Agenerate%20high-quality%20image%20samples%20while%20satisfying%20rigorous%20DP%20guarantees.%0AFor%20instance%2C%20when%20evaluated%20on%20MS-COCO%2C%20our%20DP-RDM%20can%20generate%20samples%20with%20a%0Aprivacy%20budget%20of%20%24%5Cepsilon%3D10%24%2C%20while%20providing%20a%20%243.5%24%20point%20improvement%20in%0AFID%20compared%20to%20public-only%20retrieval%20for%20up%20to%20%2410%2C000%24%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14421v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning&entry.906535625=Jonathan%20Lebensold%20and%20Maziar%20Sanjabi%20and%20Pietro%20Astolfi%20and%20Adriana%20Romero-Soriano%20and%20Kamalika%20Chaudhuri%20and%20Mike%20Rabbat%20and%20Chuan%20Guo&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20been%20shown%20to%20suffer%20from%20sample-level%0Amemorization%2C%20possibly%20reproducing%20near-perfect%20replica%20of%20images%20that%20they%20are%0Atrained%20on%2C%20which%20may%20be%20undesirable.%20To%20remedy%20this%20issue%2C%20we%20develop%20the%0Afirst%20differentially%20private%20%28DP%29%20retrieval-augmented%20generation%20algorithm%20that%0Ais%20capable%20of%20generating%20high-quality%20image%20samples%20while%20providing%20provable%0Aprivacy%20guarantees.%20Specifically%2C%20we%20assume%20access%20to%20a%20text-to-image%20diffusion%0Amodel%20trained%20on%20a%20small%20amount%20of%20public%20data%2C%20and%20design%20a%20DP%20retrieval%0Amechanism%20to%20augment%20the%20text%20prompt%20with%20samples%20retrieved%20from%20a%20private%0Aretrieval%20dataset.%20Our%20%5Cemph%7Bdifferentially%20private%20retrieval-augmented%0Adiffusion%20model%7D%20%28DP-RDM%29%20requires%20no%20fine-tuning%20on%20the%20retrieval%20dataset%20to%0Aadapt%20to%20another%20domain%2C%20and%20can%20use%20state-of-the-art%20generative%20models%20to%0Agenerate%20high-quality%20image%20samples%20while%20satisfying%20rigorous%20DP%20guarantees.%0AFor%20instance%2C%20when%20evaluated%20on%20MS-COCO%2C%20our%20DP-RDM%20can%20generate%20samples%20with%20a%0Aprivacy%20budget%20of%20%24%5Cepsilon%3D10%24%2C%20while%20providing%20a%20%243.5%24%20point%20improvement%20in%0AFID%20compared%20to%20public-only%20retrieval%20for%20up%20to%20%2410%2C000%24%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14421v1&entry.124074799=Read"},
{"title": "HyperGALE: ASD Classification via Hypergraph Gated Attention with\n  Learnable Hyperedges", "author": "Mehul Arora and Chirag Shantilal Jain and Lalith Bharadwaj Baru and Kamalaker Dadi and Bapi Raju Surampudi", "abstract": "  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition\ncharacterized by varied social cognitive challenges and repetitive behavioral\npatterns. Identifying reliable brain imaging-based biomarkers for ASD has been\na persistent challenge due to the spectrum's diverse symptomatology. Existing\nbaselines in the field have made significant strides in this direction, yet\nthere remains room for improvement in both performance and interpretability. We\npropose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating\nlearned hyperedges and gated attention mechanisms. This approach has led to\nsubstantial improvements in the model's ability to interpret complex brain\ngraph data, offering deeper insights into ASD biomarker characterization.\nEvaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves\ninterpretability but also demonstrates statistically significant enhancements\nin key performance metrics compared to both previous baselines and the\nfoundational hypergraph model. The advancement \\emph{HyperGALE} brings to ASD\nresearch highlights the potential of sophisticated graph-based techniques in\nneurodevelopmental studies. The source code and implementation instructions are\navailable at GitHub:https://github.com/mehular0ra/HyperGALE.\n", "link": "http://arxiv.org/abs/2403.14484v1", "date": "2024-03-21", "relevancy": 2.2747, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4532}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4529}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HyperGALE%3A%20ASD%20Classification%20via%20Hypergraph%20Gated%20Attention%20with%0A%20%20Learnable%20Hyperedges&body=Title%3A%20HyperGALE%3A%20ASD%20Classification%20via%20Hypergraph%20Gated%20Attention%20with%0A%20%20Learnable%20Hyperedges%0AAuthor%3A%20Mehul%20Arora%20and%20Chirag%20Shantilal%20Jain%20and%20Lalith%20Bharadwaj%20Baru%20and%20Kamalaker%20Dadi%20and%20Bapi%20Raju%20Surampudi%0AAbstract%3A%20%20%20Autism%20Spectrum%20Disorder%20%28ASD%29%20is%20a%20neurodevelopmental%20condition%0Acharacterized%20by%20varied%20social%20cognitive%20challenges%20and%20repetitive%20behavioral%0Apatterns.%20Identifying%20reliable%20brain%20imaging-based%20biomarkers%20for%20ASD%20has%20been%0Aa%20persistent%20challenge%20due%20to%20the%20spectrum%27s%20diverse%20symptomatology.%20Existing%0Abaselines%20in%20the%20field%20have%20made%20significant%20strides%20in%20this%20direction%2C%20yet%0Athere%20remains%20room%20for%20improvement%20in%20both%20performance%20and%20interpretability.%20We%0Apropose%20%5Cemph%7BHyperGALE%7D%2C%20which%20builds%20upon%20the%20hypergraph%20by%20incorporating%0Alearned%20hyperedges%20and%20gated%20attention%20mechanisms.%20This%20approach%20has%20led%20to%0Asubstantial%20improvements%20in%20the%20model%27s%20ability%20to%20interpret%20complex%20brain%0Agraph%20data%2C%20offering%20deeper%20insights%20into%20ASD%20biomarker%20characterization.%0AEvaluated%20on%20the%20extensive%20ABIDE%20II%20dataset%2C%20%5Cemph%7BHyperGALE%7D%20not%20only%20improves%0Ainterpretability%20but%20also%20demonstrates%20statistically%20significant%20enhancements%0Ain%20key%20performance%20metrics%20compared%20to%20both%20previous%20baselines%20and%20the%0Afoundational%20hypergraph%20model.%20The%20advancement%20%5Cemph%7BHyperGALE%7D%20brings%20to%20ASD%0Aresearch%20highlights%20the%20potential%20of%20sophisticated%20graph-based%20techniques%20in%0Aneurodevelopmental%20studies.%20The%20source%20code%20and%20implementation%20instructions%20are%0Aavailable%20at%20GitHub%3Ahttps%3A//github.com/mehular0ra/HyperGALE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14484v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperGALE%3A%20ASD%20Classification%20via%20Hypergraph%20Gated%20Attention%20with%0A%20%20Learnable%20Hyperedges&entry.906535625=Mehul%20Arora%20and%20Chirag%20Shantilal%20Jain%20and%20Lalith%20Bharadwaj%20Baru%20and%20Kamalaker%20Dadi%20and%20Bapi%20Raju%20Surampudi&entry.1292438233=%20%20Autism%20Spectrum%20Disorder%20%28ASD%29%20is%20a%20neurodevelopmental%20condition%0Acharacterized%20by%20varied%20social%20cognitive%20challenges%20and%20repetitive%20behavioral%0Apatterns.%20Identifying%20reliable%20brain%20imaging-based%20biomarkers%20for%20ASD%20has%20been%0Aa%20persistent%20challenge%20due%20to%20the%20spectrum%27s%20diverse%20symptomatology.%20Existing%0Abaselines%20in%20the%20field%20have%20made%20significant%20strides%20in%20this%20direction%2C%20yet%0Athere%20remains%20room%20for%20improvement%20in%20both%20performance%20and%20interpretability.%20We%0Apropose%20%5Cemph%7BHyperGALE%7D%2C%20which%20builds%20upon%20the%20hypergraph%20by%20incorporating%0Alearned%20hyperedges%20and%20gated%20attention%20mechanisms.%20This%20approach%20has%20led%20to%0Asubstantial%20improvements%20in%20the%20model%27s%20ability%20to%20interpret%20complex%20brain%0Agraph%20data%2C%20offering%20deeper%20insights%20into%20ASD%20biomarker%20characterization.%0AEvaluated%20on%20the%20extensive%20ABIDE%20II%20dataset%2C%20%5Cemph%7BHyperGALE%7D%20not%20only%20improves%0Ainterpretability%20but%20also%20demonstrates%20statistically%20significant%20enhancements%0Ain%20key%20performance%20metrics%20compared%20to%20both%20previous%20baselines%20and%20the%0Afoundational%20hypergraph%20model.%20The%20advancement%20%5Cemph%7BHyperGALE%7D%20brings%20to%20ASD%0Aresearch%20highlights%20the%20potential%20of%20sophisticated%20graph-based%20techniques%20in%0Aneurodevelopmental%20studies.%20The%20source%20code%20and%20implementation%20instructions%20are%0Aavailable%20at%20GitHub%3Ahttps%3A//github.com/mehular0ra/HyperGALE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14484v1&entry.124074799=Read"},
{"title": "An explainable three dimension framework to uncover learning patterns: A\n  unified look in variable sulci recognition", "author": "Michail Mamalakis and Heloise de Vareilles and Atheer AI-Manea and Samantha C. Mitchell and Ingrid Arartz and Lynn Egeland Morch-Johnsen and Jane Garrison and Jon Simons and Pietro Lio and John Suckling and Graham Murray", "abstract": "  Explainable AI is crucial in medical imaging. In the challenging field of\nneuroscience, visual topics present a high level of complexity, particularly\nwithin three-dimensional space. The application of neuroscience, which involves\nidentifying brain sulcal features from MRI, faces significant hurdles due to\nvarying annotation protocols among experts and the intricate three-dimension\nfunctionality of the brain. Consequently, traditional explainability approaches\nfall short in effectively validating and evaluating these networks. To address\nthis, we first present a mathematical formulation delineating various\ncategories of explanation needs across diverse computer vision tasks,\ncategorized into self-explanatory, semi-explanatory, non-explanatory, and\nnew-pattern learning applications based on the reliability of the validation\nprotocol. With respect to this mathematical formulation, we propose a 3D\nexplainability framework aimed at validating the outputs of deep learning\nnetworks in detecting the paracingulate sulcus an essential brain anatomical\nfeature. The framework integrates local 3D explanations, global explanations\nthrough dimensionality reduction, concatenated global explanations, and\nstatistical shape features, unveiling new insights into pattern learning. We\ntrained and tested two advanced 3D deep learning networks on the challenging\nTOP-OSLO dataset, significantly improving sulcus detection accuracy,\nparticularly on the left hemisphere. During evaluation with diverse annotation\nprotocols for this dataset, we highlighted the crucial role of an unbiased\nannotation process in achieving precise predictions and effective pattern\nlearning within our proposed 3D framework. The proposed framework not only\nannotates the variable sulcus but also uncovers hidden AI knowledge, promising\nto advance our understanding of brain anatomy and function.\n", "link": "http://arxiv.org/abs/2309.00903v2", "date": "2024-03-21", "relevancy": 2.2717, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5873}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20explainable%20three%20dimension%20framework%20to%20uncover%20learning%20patterns%3A%20A%0A%20%20unified%20look%20in%20variable%20sulci%20recognition&body=Title%3A%20An%20explainable%20three%20dimension%20framework%20to%20uncover%20learning%20patterns%3A%20A%0A%20%20unified%20look%20in%20variable%20sulci%20recognition%0AAuthor%3A%20Michail%20Mamalakis%20and%20Heloise%20de%20Vareilles%20and%20Atheer%20AI-Manea%20and%20Samantha%20C.%20Mitchell%20and%20Ingrid%20Arartz%20and%20Lynn%20Egeland%20Morch-Johnsen%20and%20Jane%20Garrison%20and%20Jon%20Simons%20and%20Pietro%20Lio%20and%20John%20Suckling%20and%20Graham%20Murray%0AAbstract%3A%20%20%20Explainable%20AI%20is%20crucial%20in%20medical%20imaging.%20In%20the%20challenging%20field%20of%0Aneuroscience%2C%20visual%20topics%20present%20a%20high%20level%20of%20complexity%2C%20particularly%0Awithin%20three-dimensional%20space.%20The%20application%20of%20neuroscience%2C%20which%20involves%0Aidentifying%20brain%20sulcal%20features%20from%20MRI%2C%20faces%20significant%20hurdles%20due%20to%0Avarying%20annotation%20protocols%20among%20experts%20and%20the%20intricate%20three-dimension%0Afunctionality%20of%20the%20brain.%20Consequently%2C%20traditional%20explainability%20approaches%0Afall%20short%20in%20effectively%20validating%20and%20evaluating%20these%20networks.%20To%20address%0Athis%2C%20we%20first%20present%20a%20mathematical%20formulation%20delineating%20various%0Acategories%20of%20explanation%20needs%20across%20diverse%20computer%20vision%20tasks%2C%0Acategorized%20into%20self-explanatory%2C%20semi-explanatory%2C%20non-explanatory%2C%20and%0Anew-pattern%20learning%20applications%20based%20on%20the%20reliability%20of%20the%20validation%0Aprotocol.%20With%20respect%20to%20this%20mathematical%20formulation%2C%20we%20propose%20a%203D%0Aexplainability%20framework%20aimed%20at%20validating%20the%20outputs%20of%20deep%20learning%0Anetworks%20in%20detecting%20the%20paracingulate%20sulcus%20an%20essential%20brain%20anatomical%0Afeature.%20The%20framework%20integrates%20local%203D%20explanations%2C%20global%20explanations%0Athrough%20dimensionality%20reduction%2C%20concatenated%20global%20explanations%2C%20and%0Astatistical%20shape%20features%2C%20unveiling%20new%20insights%20into%20pattern%20learning.%20We%0Atrained%20and%20tested%20two%20advanced%203D%20deep%20learning%20networks%20on%20the%20challenging%0ATOP-OSLO%20dataset%2C%20significantly%20improving%20sulcus%20detection%20accuracy%2C%0Aparticularly%20on%20the%20left%20hemisphere.%20During%20evaluation%20with%20diverse%20annotation%0Aprotocols%20for%20this%20dataset%2C%20we%20highlighted%20the%20crucial%20role%20of%20an%20unbiased%0Aannotation%20process%20in%20achieving%20precise%20predictions%20and%20effective%20pattern%0Alearning%20within%20our%20proposed%203D%20framework.%20The%20proposed%20framework%20not%20only%0Aannotates%20the%20variable%20sulcus%20but%20also%20uncovers%20hidden%20AI%20knowledge%2C%20promising%0Ato%20advance%20our%20understanding%20of%20brain%20anatomy%20and%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00903v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20explainable%20three%20dimension%20framework%20to%20uncover%20learning%20patterns%3A%20A%0A%20%20unified%20look%20in%20variable%20sulci%20recognition&entry.906535625=Michail%20Mamalakis%20and%20Heloise%20de%20Vareilles%20and%20Atheer%20AI-Manea%20and%20Samantha%20C.%20Mitchell%20and%20Ingrid%20Arartz%20and%20Lynn%20Egeland%20Morch-Johnsen%20and%20Jane%20Garrison%20and%20Jon%20Simons%20and%20Pietro%20Lio%20and%20John%20Suckling%20and%20Graham%20Murray&entry.1292438233=%20%20Explainable%20AI%20is%20crucial%20in%20medical%20imaging.%20In%20the%20challenging%20field%20of%0Aneuroscience%2C%20visual%20topics%20present%20a%20high%20level%20of%20complexity%2C%20particularly%0Awithin%20three-dimensional%20space.%20The%20application%20of%20neuroscience%2C%20which%20involves%0Aidentifying%20brain%20sulcal%20features%20from%20MRI%2C%20faces%20significant%20hurdles%20due%20to%0Avarying%20annotation%20protocols%20among%20experts%20and%20the%20intricate%20three-dimension%0Afunctionality%20of%20the%20brain.%20Consequently%2C%20traditional%20explainability%20approaches%0Afall%20short%20in%20effectively%20validating%20and%20evaluating%20these%20networks.%20To%20address%0Athis%2C%20we%20first%20present%20a%20mathematical%20formulation%20delineating%20various%0Acategories%20of%20explanation%20needs%20across%20diverse%20computer%20vision%20tasks%2C%0Acategorized%20into%20self-explanatory%2C%20semi-explanatory%2C%20non-explanatory%2C%20and%0Anew-pattern%20learning%20applications%20based%20on%20the%20reliability%20of%20the%20validation%0Aprotocol.%20With%20respect%20to%20this%20mathematical%20formulation%2C%20we%20propose%20a%203D%0Aexplainability%20framework%20aimed%20at%20validating%20the%20outputs%20of%20deep%20learning%0Anetworks%20in%20detecting%20the%20paracingulate%20sulcus%20an%20essential%20brain%20anatomical%0Afeature.%20The%20framework%20integrates%20local%203D%20explanations%2C%20global%20explanations%0Athrough%20dimensionality%20reduction%2C%20concatenated%20global%20explanations%2C%20and%0Astatistical%20shape%20features%2C%20unveiling%20new%20insights%20into%20pattern%20learning.%20We%0Atrained%20and%20tested%20two%20advanced%203D%20deep%20learning%20networks%20on%20the%20challenging%0ATOP-OSLO%20dataset%2C%20significantly%20improving%20sulcus%20detection%20accuracy%2C%0Aparticularly%20on%20the%20left%20hemisphere.%20During%20evaluation%20with%20diverse%20annotation%0Aprotocols%20for%20this%20dataset%2C%20we%20highlighted%20the%20crucial%20role%20of%20an%20unbiased%0Aannotation%20process%20in%20achieving%20precise%20predictions%20and%20effective%20pattern%0Alearning%20within%20our%20proposed%203D%20framework.%20The%20proposed%20framework%20not%20only%0Aannotates%20the%20variable%20sulcus%20but%20also%20uncovers%20hidden%20AI%20knowledge%2C%20promising%0Ato%20advance%20our%20understanding%20of%20brain%20anatomy%20and%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00903v2&entry.124074799=Read"},
{"title": "Object-Centric Domain Randomization for 3D Shape Reconstruction in the\n  Wild", "author": "Junhyeong Cho and Kim Youwang and Hunmin Yang and Tae-Hyun Oh", "abstract": "  One of the biggest challenges in single-view 3D shape reconstruction in the\nwild is the scarcity of <3D shape, 2D image>-paired data from real-world\nenvironments. Inspired by remarkable achievements via domain randomization, we\npropose ObjectDR which synthesizes such paired data via a random simulation of\nvisual variations in object appearances and backgrounds. Our data synthesis\nframework exploits a conditional generative model (e.g., ControlNet) to\ngenerate images conforming to spatial conditions such as 2.5D sketches, which\nare obtainable through a rendering process of 3D shapes from object collections\n(e.g., Objaverse-XL). To simulate diverse variations while preserving object\nsilhouettes embedded in spatial conditions, we also introduce a disentangled\nframework which leverages an initial object guidance. After synthesizing a wide\nrange of data, we pre-train a model on them so that it learns to capture a\ndomain-invariant geometry prior which is consistent across various domains. We\nvalidate its effectiveness by substantially improving 3D shape reconstruction\nmodels on a real-world benchmark. In a scale-up evaluation, our pre-training\nachieves 23.6% superior results compared with the pre-training on high-quality\ncomputer graphics renderings.\n", "link": "http://arxiv.org/abs/2403.14539v1", "date": "2024-03-21", "relevancy": 2.2702, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5897}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5551}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5433}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Object-Centric%20Domain%20Randomization%20for%203D%20Shape%20Reconstruction%20in%20the%0A%20%20Wild&body=Title%3A%20Object-Centric%20Domain%20Randomization%20for%203D%20Shape%20Reconstruction%20in%20the%0A%20%20Wild%0AAuthor%3A%20Junhyeong%20Cho%20and%20Kim%20Youwang%20and%20Hunmin%20Yang%20and%20Tae-Hyun%20Oh%0AAbstract%3A%20%20%20One%20of%20the%20biggest%20challenges%20in%20single-view%203D%20shape%20reconstruction%20in%20the%0Awild%20is%20the%20scarcity%20of%20%3C3D%20shape%2C%202D%20image%3E-paired%20data%20from%20real-world%0Aenvironments.%20Inspired%20by%20remarkable%20achievements%20via%20domain%20randomization%2C%20we%0Apropose%20ObjectDR%20which%20synthesizes%20such%20paired%20data%20via%20a%20random%20simulation%20of%0Avisual%20variations%20in%20object%20appearances%20and%20backgrounds.%20Our%20data%20synthesis%0Aframework%20exploits%20a%20conditional%20generative%20model%20%28e.g.%2C%20ControlNet%29%20to%0Agenerate%20images%20conforming%20to%20spatial%20conditions%20such%20as%202.5D%20sketches%2C%20which%0Aare%20obtainable%20through%20a%20rendering%20process%20of%203D%20shapes%20from%20object%20collections%0A%28e.g.%2C%20Objaverse-XL%29.%20To%20simulate%20diverse%20variations%20while%20preserving%20object%0Asilhouettes%20embedded%20in%20spatial%20conditions%2C%20we%20also%20introduce%20a%20disentangled%0Aframework%20which%20leverages%20an%20initial%20object%20guidance.%20After%20synthesizing%20a%20wide%0Arange%20of%20data%2C%20we%20pre-train%20a%20model%20on%20them%20so%20that%20it%20learns%20to%20capture%20a%0Adomain-invariant%20geometry%20prior%20which%20is%20consistent%20across%20various%20domains.%20We%0Avalidate%20its%20effectiveness%20by%20substantially%20improving%203D%20shape%20reconstruction%0Amodels%20on%20a%20real-world%20benchmark.%20In%20a%20scale-up%20evaluation%2C%20our%20pre-training%0Aachieves%2023.6%25%20superior%20results%20compared%20with%20the%20pre-training%20on%20high-quality%0Acomputer%20graphics%20renderings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14539v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Centric%20Domain%20Randomization%20for%203D%20Shape%20Reconstruction%20in%20the%0A%20%20Wild&entry.906535625=Junhyeong%20Cho%20and%20Kim%20Youwang%20and%20Hunmin%20Yang%20and%20Tae-Hyun%20Oh&entry.1292438233=%20%20One%20of%20the%20biggest%20challenges%20in%20single-view%203D%20shape%20reconstruction%20in%20the%0Awild%20is%20the%20scarcity%20of%20%3C3D%20shape%2C%202D%20image%3E-paired%20data%20from%20real-world%0Aenvironments.%20Inspired%20by%20remarkable%20achievements%20via%20domain%20randomization%2C%20we%0Apropose%20ObjectDR%20which%20synthesizes%20such%20paired%20data%20via%20a%20random%20simulation%20of%0Avisual%20variations%20in%20object%20appearances%20and%20backgrounds.%20Our%20data%20synthesis%0Aframework%20exploits%20a%20conditional%20generative%20model%20%28e.g.%2C%20ControlNet%29%20to%0Agenerate%20images%20conforming%20to%20spatial%20conditions%20such%20as%202.5D%20sketches%2C%20which%0Aare%20obtainable%20through%20a%20rendering%20process%20of%203D%20shapes%20from%20object%20collections%0A%28e.g.%2C%20Objaverse-XL%29.%20To%20simulate%20diverse%20variations%20while%20preserving%20object%0Asilhouettes%20embedded%20in%20spatial%20conditions%2C%20we%20also%20introduce%20a%20disentangled%0Aframework%20which%20leverages%20an%20initial%20object%20guidance.%20After%20synthesizing%20a%20wide%0Arange%20of%20data%2C%20we%20pre-train%20a%20model%20on%20them%20so%20that%20it%20learns%20to%20capture%20a%0Adomain-invariant%20geometry%20prior%20which%20is%20consistent%20across%20various%20domains.%20We%0Avalidate%20its%20effectiveness%20by%20substantially%20improving%203D%20shape%20reconstruction%0Amodels%20on%20a%20real-world%20benchmark.%20In%20a%20scale-up%20evaluation%2C%20our%20pre-training%0Aachieves%2023.6%25%20superior%20results%20compared%20with%20the%20pre-training%20on%20high-quality%0Acomputer%20graphics%20renderings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14539v1&entry.124074799=Read"},
{"title": "LLM4SGG: Large Language Model for Weakly Supervised Scene Graph\n  Generation", "author": "Kibum Kim and Kanghoon Yoon and Jaehyeong Jeon and Yeonjun In and Jinyoung Moon and Donghyun Kim and Chanyoung Park", "abstract": "  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently\nemerged as an alternative to the fully-supervised approach that heavily relies\non costly annotations. In this regard, studies on WSSGG have utilized image\ncaptions to obtain unlocalized triplets while primarily focusing on grounding\nthe unlocalized triplets over image regions. However, they have overlooked the\ntwo issues involved in the triplet formation process from the captions: 1)\nSemantic over-simplification issue arises when extracting triplets from\ncaptions, where fine-grained predicates in captions are undesirably converted\ninto coarse-grained predicates, resulting in a long-tailed predicate\ndistribution, and 2) Low-density scene graph issue arises when aligning the\ntriplets in the caption with entity/predicate classes of interest, where many\ntriplets are discarded and not used in training, leading to insufficient\nsupervision. To tackle the two issues, we propose a new approach, i.e., Large\nLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two\nissues by leveraging the LLM's in-depth understanding of language and reasoning\nability during the extraction of triplets from captions and alignment of\nentity/predicate classes with target data. To further engage the LLM in these\nprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shot\nlearning strategy. To validate the effectiveness of LLM4SGG, we conduct\nextensive experiments on Visual Genome and GQA datasets, showing significant\nimprovements in both Recall@K and mean Recall@K compared to the\nstate-of-the-art WSSGG methods. A further appeal is that LLM4SGG is\ndata-efficient, enabling effective model training with a small amount of\ntraining images.\n", "link": "http://arxiv.org/abs/2310.10404v6", "date": "2024-03-21", "relevancy": 2.2681, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5914}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5554}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5351}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLM4SGG%3A%20Large%20Language%20Model%20for%20Weakly%20Supervised%20Scene%20Graph%0A%20%20Generation&body=Title%3A%20LLM4SGG%3A%20Large%20Language%20Model%20for%20Weakly%20Supervised%20Scene%20Graph%0A%20%20Generation%0AAuthor%3A%20Kibum%20Kim%20and%20Kanghoon%20Yoon%20and%20Jaehyeong%20Jeon%20and%20Yeonjun%20In%20and%20Jinyoung%20Moon%20and%20Donghyun%20Kim%20and%20Chanyoung%20Park%0AAbstract%3A%20%20%20Weakly-Supervised%20Scene%20Graph%20Generation%20%28WSSGG%29%20research%20has%20recently%0Aemerged%20as%20an%20alternative%20to%20the%20fully-supervised%20approach%20that%20heavily%20relies%0Aon%20costly%20annotations.%20In%20this%20regard%2C%20studies%20on%20WSSGG%20have%20utilized%20image%0Acaptions%20to%20obtain%20unlocalized%20triplets%20while%20primarily%20focusing%20on%20grounding%0Athe%20unlocalized%20triplets%20over%20image%20regions.%20However%2C%20they%20have%20overlooked%20the%0Atwo%20issues%20involved%20in%20the%20triplet%20formation%20process%20from%20the%20captions%3A%201%29%0ASemantic%20over-simplification%20issue%20arises%20when%20extracting%20triplets%20from%0Acaptions%2C%20where%20fine-grained%20predicates%20in%20captions%20are%20undesirably%20converted%0Ainto%20coarse-grained%20predicates%2C%20resulting%20in%20a%20long-tailed%20predicate%0Adistribution%2C%20and%202%29%20Low-density%20scene%20graph%20issue%20arises%20when%20aligning%20the%0Atriplets%20in%20the%20caption%20with%20entity/predicate%20classes%20of%20interest%2C%20where%20many%0Atriplets%20are%20discarded%20and%20not%20used%20in%20training%2C%20leading%20to%20insufficient%0Asupervision.%20To%20tackle%20the%20two%20issues%2C%20we%20propose%20a%20new%20approach%2C%20i.e.%2C%20Large%0ALanguage%20Model%20for%20weakly-supervised%20SGG%20%28LLM4SGG%29%2C%20where%20we%20mitigate%20the%20two%0Aissues%20by%20leveraging%20the%20LLM%27s%20in-depth%20understanding%20of%20language%20and%20reasoning%0Aability%20during%20the%20extraction%20of%20triplets%20from%20captions%20and%20alignment%20of%0Aentity/predicate%20classes%20with%20target%20data.%20To%20further%20engage%20the%20LLM%20in%20these%0Aprocesses%2C%20we%20adopt%20the%20idea%20of%20Chain-of-Thought%20and%20the%20in-context%20few-shot%0Alearning%20strategy.%20To%20validate%20the%20effectiveness%20of%20LLM4SGG%2C%20we%20conduct%0Aextensive%20experiments%20on%20Visual%20Genome%20and%20GQA%20datasets%2C%20showing%20significant%0Aimprovements%20in%20both%20Recall%40K%20and%20mean%20Recall%40K%20compared%20to%20the%0Astate-of-the-art%20WSSGG%20methods.%20A%20further%20appeal%20is%20that%20LLM4SGG%20is%0Adata-efficient%2C%20enabling%20effective%20model%20training%20with%20a%20small%20amount%20of%0Atraining%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10404v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM4SGG%3A%20Large%20Language%20Model%20for%20Weakly%20Supervised%20Scene%20Graph%0A%20%20Generation&entry.906535625=Kibum%20Kim%20and%20Kanghoon%20Yoon%20and%20Jaehyeong%20Jeon%20and%20Yeonjun%20In%20and%20Jinyoung%20Moon%20and%20Donghyun%20Kim%20and%20Chanyoung%20Park&entry.1292438233=%20%20Weakly-Supervised%20Scene%20Graph%20Generation%20%28WSSGG%29%20research%20has%20recently%0Aemerged%20as%20an%20alternative%20to%20the%20fully-supervised%20approach%20that%20heavily%20relies%0Aon%20costly%20annotations.%20In%20this%20regard%2C%20studies%20on%20WSSGG%20have%20utilized%20image%0Acaptions%20to%20obtain%20unlocalized%20triplets%20while%20primarily%20focusing%20on%20grounding%0Athe%20unlocalized%20triplets%20over%20image%20regions.%20However%2C%20they%20have%20overlooked%20the%0Atwo%20issues%20involved%20in%20the%20triplet%20formation%20process%20from%20the%20captions%3A%201%29%0ASemantic%20over-simplification%20issue%20arises%20when%20extracting%20triplets%20from%0Acaptions%2C%20where%20fine-grained%20predicates%20in%20captions%20are%20undesirably%20converted%0Ainto%20coarse-grained%20predicates%2C%20resulting%20in%20a%20long-tailed%20predicate%0Adistribution%2C%20and%202%29%20Low-density%20scene%20graph%20issue%20arises%20when%20aligning%20the%0Atriplets%20in%20the%20caption%20with%20entity/predicate%20classes%20of%20interest%2C%20where%20many%0Atriplets%20are%20discarded%20and%20not%20used%20in%20training%2C%20leading%20to%20insufficient%0Asupervision.%20To%20tackle%20the%20two%20issues%2C%20we%20propose%20a%20new%20approach%2C%20i.e.%2C%20Large%0ALanguage%20Model%20for%20weakly-supervised%20SGG%20%28LLM4SGG%29%2C%20where%20we%20mitigate%20the%20two%0Aissues%20by%20leveraging%20the%20LLM%27s%20in-depth%20understanding%20of%20language%20and%20reasoning%0Aability%20during%20the%20extraction%20of%20triplets%20from%20captions%20and%20alignment%20of%0Aentity/predicate%20classes%20with%20target%20data.%20To%20further%20engage%20the%20LLM%20in%20these%0Aprocesses%2C%20we%20adopt%20the%20idea%20of%20Chain-of-Thought%20and%20the%20in-context%20few-shot%0Alearning%20strategy.%20To%20validate%20the%20effectiveness%20of%20LLM4SGG%2C%20we%20conduct%0Aextensive%20experiments%20on%20Visual%20Genome%20and%20GQA%20datasets%2C%20showing%20significant%0Aimprovements%20in%20both%20Recall%40K%20and%20mean%20Recall%40K%20compared%20to%20the%0Astate-of-the-art%20WSSGG%20methods.%20A%20further%20appeal%20is%20that%20LLM4SGG%20is%0Adata-efficient%2C%20enabling%20effective%20model%20training%20with%20a%20small%20amount%20of%0Atraining%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10404v6&entry.124074799=Read"},
{"title": "Learning to Solve Integer Linear Programs with Davis-Yin Splitting", "author": "Daniel McKenzie and Samy Wu Fung and Howard Heaton", "abstract": "  In many applications, a combinatorial problem must be repeatedly solved with\nsimilar, but distinct parameters. Yet, the parameters $w$ are not directly\nobserved; only contextual data $d$ that correlates with $w$ is available. It is\ntempting to use a neural network to predict $w$ given $d$. However, training\nsuch a model requires reconciling the discrete nature of combinatorial\noptimization with the gradient-based frameworks used to train neural networks.\nWhen the problem in question is an Integer Linear Program (ILP), one approach\nto overcome this training issue is to consider a continuous relaxation of the\ncombinatorial problem. While existing methods utilizing this approach have\nshown to be highly effective on small problems, they do not always scale well\nto large problems. In this work, we draw on ideas from modern convex\noptimization to design a network and training scheme which scales effortlessly\nto problems with thousands of variables. Our experiments verify the\ncomputational advantage our proposed method enjoys on two representative\nproblems, namely the shortest path problem and the knapsack problem.\n", "link": "http://arxiv.org/abs/2301.13395v3", "date": "2024-03-21", "relevancy": 2.2583, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.471}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4516}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4323}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Solve%20Integer%20Linear%20Programs%20with%20Davis-Yin%20Splitting&body=Title%3A%20Learning%20to%20Solve%20Integer%20Linear%20Programs%20with%20Davis-Yin%20Splitting%0AAuthor%3A%20Daniel%20McKenzie%20and%20Samy%20Wu%20Fung%20and%20Howard%20Heaton%0AAbstract%3A%20%20%20In%20many%20applications%2C%20a%20combinatorial%20problem%20must%20be%20repeatedly%20solved%20with%0Asimilar%2C%20but%20distinct%20parameters.%20Yet%2C%20the%20parameters%20%24w%24%20are%20not%20directly%0Aobserved%3B%20only%20contextual%20data%20%24d%24%20that%20correlates%20with%20%24w%24%20is%20available.%20It%20is%0Atempting%20to%20use%20a%20neural%20network%20to%20predict%20%24w%24%20given%20%24d%24.%20However%2C%20training%0Asuch%20a%20model%20requires%20reconciling%20the%20discrete%20nature%20of%20combinatorial%0Aoptimization%20with%20the%20gradient-based%20frameworks%20used%20to%20train%20neural%20networks.%0AWhen%20the%20problem%20in%20question%20is%20an%20Integer%20Linear%20Program%20%28ILP%29%2C%20one%20approach%0Ato%20overcome%20this%20training%20issue%20is%20to%20consider%20a%20continuous%20relaxation%20of%20the%0Acombinatorial%20problem.%20While%20existing%20methods%20utilizing%20this%20approach%20have%0Ashown%20to%20be%20highly%20effective%20on%20small%20problems%2C%20they%20do%20not%20always%20scale%20well%0Ato%20large%20problems.%20In%20this%20work%2C%20we%20draw%20on%20ideas%20from%20modern%20convex%0Aoptimization%20to%20design%20a%20network%20and%20training%20scheme%20which%20scales%20effortlessly%0Ato%20problems%20with%20thousands%20of%20variables.%20Our%20experiments%20verify%20the%0Acomputational%20advantage%20our%20proposed%20method%20enjoys%20on%20two%20representative%0Aproblems%2C%20namely%20the%20shortest%20path%20problem%20and%20the%20knapsack%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13395v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Solve%20Integer%20Linear%20Programs%20with%20Davis-Yin%20Splitting&entry.906535625=Daniel%20McKenzie%20and%20Samy%20Wu%20Fung%20and%20Howard%20Heaton&entry.1292438233=%20%20In%20many%20applications%2C%20a%20combinatorial%20problem%20must%20be%20repeatedly%20solved%20with%0Asimilar%2C%20but%20distinct%20parameters.%20Yet%2C%20the%20parameters%20%24w%24%20are%20not%20directly%0Aobserved%3B%20only%20contextual%20data%20%24d%24%20that%20correlates%20with%20%24w%24%20is%20available.%20It%20is%0Atempting%20to%20use%20a%20neural%20network%20to%20predict%20%24w%24%20given%20%24d%24.%20However%2C%20training%0Asuch%20a%20model%20requires%20reconciling%20the%20discrete%20nature%20of%20combinatorial%0Aoptimization%20with%20the%20gradient-based%20frameworks%20used%20to%20train%20neural%20networks.%0AWhen%20the%20problem%20in%20question%20is%20an%20Integer%20Linear%20Program%20%28ILP%29%2C%20one%20approach%0Ato%20overcome%20this%20training%20issue%20is%20to%20consider%20a%20continuous%20relaxation%20of%20the%0Acombinatorial%20problem.%20While%20existing%20methods%20utilizing%20this%20approach%20have%0Ashown%20to%20be%20highly%20effective%20on%20small%20problems%2C%20they%20do%20not%20always%20scale%20well%0Ato%20large%20problems.%20In%20this%20work%2C%20we%20draw%20on%20ideas%20from%20modern%20convex%0Aoptimization%20to%20design%20a%20network%20and%20training%20scheme%20which%20scales%20effortlessly%0Ato%20problems%20with%20thousands%20of%20variables.%20Our%20experiments%20verify%20the%0Acomputational%20advantage%20our%20proposed%20method%20enjoys%20on%20two%20representative%0Aproblems%2C%20namely%20the%20shortest%20path%20problem%20and%20the%20knapsack%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13395v3&entry.124074799=Read"},
{"title": "PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model", "author": "Zheng Zhang and Yeyao Ma and Enming Zhang and Xiang Bai", "abstract": "  PSALM is a powerful extension of the Large Multi-modal Model (LMM) to address\nthe segmentation task challenges. To overcome the limitation of the LMM being\nlimited to textual output, PSALM incorporates a mask decoder and a\nwell-designed input schema to handle a variety of segmentation tasks. This\nschema includes images, task instructions, conditional prompts, and mask\ntokens, which enable the model to generate and classify segmentation masks\neffectively. The flexible design of PSALM supports joint training across\nmultiple datasets and tasks, leading to improved performance and task\ngeneralization. PSALM achieves superior results on several benchmarks, such as\nRefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive,\nand further exhibits zero-shot capabilities on unseen tasks, such as\nopen-vocabulary segmentation, generalized referring expression segmentation and\nvideo object segmentation, making a significant step towards a GPT moment in\ncomputer vision. Through extensive experiments, PSALM demonstrates its\npotential to transform the domain of image segmentation, leveraging the robust\nvisual understanding capabilities of LMMs as seen in natural language\nprocessing. Code and models are available at https://github.com/zamling/PSALM.\n", "link": "http://arxiv.org/abs/2403.14598v1", "date": "2024-03-21", "relevancy": 2.2537, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5983}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PSALM%3A%20Pixelwise%20SegmentAtion%20with%20Large%20Multi-Modal%20Model&body=Title%3A%20PSALM%3A%20Pixelwise%20SegmentAtion%20with%20Large%20Multi-Modal%20Model%0AAuthor%3A%20Zheng%20Zhang%20and%20Yeyao%20Ma%20and%20Enming%20Zhang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20PSALM%20is%20a%20powerful%20extension%20of%20the%20Large%20Multi-modal%20Model%20%28LMM%29%20to%20address%0Athe%20segmentation%20task%20challenges.%20To%20overcome%20the%20limitation%20of%20the%20LMM%20being%0Alimited%20to%20textual%20output%2C%20PSALM%20incorporates%20a%20mask%20decoder%20and%20a%0Awell-designed%20input%20schema%20to%20handle%20a%20variety%20of%20segmentation%20tasks.%20This%0Aschema%20includes%20images%2C%20task%20instructions%2C%20conditional%20prompts%2C%20and%20mask%0Atokens%2C%20which%20enable%20the%20model%20to%20generate%20and%20classify%20segmentation%20masks%0Aeffectively.%20The%20flexible%20design%20of%20PSALM%20supports%20joint%20training%20across%0Amultiple%20datasets%20and%20tasks%2C%20leading%20to%20improved%20performance%20and%20task%0Ageneralization.%20PSALM%20achieves%20superior%20results%20on%20several%20benchmarks%2C%20such%20as%0ARefCOCO/RefCOCO%2B/RefCOCOg%2C%20COCO%20Panoptic%20Segmentation%2C%20and%20COCO-Interactive%2C%0Aand%20further%20exhibits%20zero-shot%20capabilities%20on%20unseen%20tasks%2C%20such%20as%0Aopen-vocabulary%20segmentation%2C%20generalized%20referring%20expression%20segmentation%20and%0Avideo%20object%20segmentation%2C%20making%20a%20significant%20step%20towards%20a%20GPT%20moment%20in%0Acomputer%20vision.%20Through%20extensive%20experiments%2C%20PSALM%20demonstrates%20its%0Apotential%20to%20transform%20the%20domain%20of%20image%20segmentation%2C%20leveraging%20the%20robust%0Avisual%20understanding%20capabilities%20of%20LMMs%20as%20seen%20in%20natural%20language%0Aprocessing.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/zamling/PSALM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14598v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSALM%3A%20Pixelwise%20SegmentAtion%20with%20Large%20Multi-Modal%20Model&entry.906535625=Zheng%20Zhang%20and%20Yeyao%20Ma%20and%20Enming%20Zhang%20and%20Xiang%20Bai&entry.1292438233=%20%20PSALM%20is%20a%20powerful%20extension%20of%20the%20Large%20Multi-modal%20Model%20%28LMM%29%20to%20address%0Athe%20segmentation%20task%20challenges.%20To%20overcome%20the%20limitation%20of%20the%20LMM%20being%0Alimited%20to%20textual%20output%2C%20PSALM%20incorporates%20a%20mask%20decoder%20and%20a%0Awell-designed%20input%20schema%20to%20handle%20a%20variety%20of%20segmentation%20tasks.%20This%0Aschema%20includes%20images%2C%20task%20instructions%2C%20conditional%20prompts%2C%20and%20mask%0Atokens%2C%20which%20enable%20the%20model%20to%20generate%20and%20classify%20segmentation%20masks%0Aeffectively.%20The%20flexible%20design%20of%20PSALM%20supports%20joint%20training%20across%0Amultiple%20datasets%20and%20tasks%2C%20leading%20to%20improved%20performance%20and%20task%0Ageneralization.%20PSALM%20achieves%20superior%20results%20on%20several%20benchmarks%2C%20such%20as%0ARefCOCO/RefCOCO%2B/RefCOCOg%2C%20COCO%20Panoptic%20Segmentation%2C%20and%20COCO-Interactive%2C%0Aand%20further%20exhibits%20zero-shot%20capabilities%20on%20unseen%20tasks%2C%20such%20as%0Aopen-vocabulary%20segmentation%2C%20generalized%20referring%20expression%20segmentation%20and%0Avideo%20object%20segmentation%2C%20making%20a%20significant%20step%20towards%20a%20GPT%20moment%20in%0Acomputer%20vision.%20Through%20extensive%20experiments%2C%20PSALM%20demonstrates%20its%0Apotential%20to%20transform%20the%20domain%20of%20image%20segmentation%2C%20leveraging%20the%20robust%0Avisual%20understanding%20capabilities%20of%20LMMs%20as%20seen%20in%20natural%20language%0Aprocessing.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/zamling/PSALM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14598v1&entry.124074799=Read"},
{"title": "Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face\n  Synthesis", "author": "Jingjing Ren and Cheng Xu and Haoyu Chen and Xinran Qin and Lei Zhu", "abstract": "  Recent progress in multi-modal conditioned face synthesis has enabled the\ncreation of visually striking and accurately aligned facial images. Yet,\ncurrent methods still face issues with scalability, limited flexibility, and a\none-size-fits-all approach to control strength, not accounting for the\ndiffering levels of conditional entropy, a measure of unpredictability in data\ngiven some condition, across modalities. To address these challenges, we\nintroduce a novel uni-modal training approach with modal surrogates, coupled\nwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,\nand scalable multi-modal conditioned face synthesis network. Our uni-modal\ntraining with modal surrogate that only leverage uni-modal data, use modal\nsurrogate to decorate condition with modal-specific characteristic and serve as\nlinker for inter-modal collaboration , fully learns each modality control in\nface synthesis process as well as inter-modal collaboration. The entropy-aware\nmodal-adaptive modulation finely adjust diffusion noise according to\nmodal-specific characteristics and given conditions, enabling well-informed\nstep along denoising trajectory and ultimately leading to synthesis results of\nhigh fidelity and quality. Our framework improves multi-modal face synthesis\nunder various conditions, surpassing current methods in image quality and\nfidelity, as demonstrated by our thorough experimental results.\n", "link": "http://arxiv.org/abs/2312.16274v2", "date": "2024-03-21", "relevancy": 2.2516, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5664}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5412}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Flexible%2C%20Scalable%2C%20and%20Adaptive%20Multi-Modal%20Conditioned%20Face%0A%20%20Synthesis&body=Title%3A%20Towards%20Flexible%2C%20Scalable%2C%20and%20Adaptive%20Multi-Modal%20Conditioned%20Face%0A%20%20Synthesis%0AAuthor%3A%20Jingjing%20Ren%20and%20Cheng%20Xu%20and%20Haoyu%20Chen%20and%20Xinran%20Qin%20and%20Lei%20Zhu%0AAbstract%3A%20%20%20Recent%20progress%20in%20multi-modal%20conditioned%20face%20synthesis%20has%20enabled%20the%0Acreation%20of%20visually%20striking%20and%20accurately%20aligned%20facial%20images.%20Yet%2C%0Acurrent%20methods%20still%20face%20issues%20with%20scalability%2C%20limited%20flexibility%2C%20and%20a%0Aone-size-fits-all%20approach%20to%20control%20strength%2C%20not%20accounting%20for%20the%0Adiffering%20levels%20of%20conditional%20entropy%2C%20a%20measure%20of%20unpredictability%20in%20data%0Agiven%20some%20condition%2C%20across%20modalities.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20novel%20uni-modal%20training%20approach%20with%20modal%20surrogates%2C%20coupled%0Awith%20an%20entropy-aware%20modal-adaptive%20modulation%2C%20to%20support%20flexible%2C%20scalable%2C%0Aand%20scalable%20multi-modal%20conditioned%20face%20synthesis%20network.%20Our%20uni-modal%0Atraining%20with%20modal%20surrogate%20that%20only%20leverage%20uni-modal%20data%2C%20use%20modal%0Asurrogate%20to%20decorate%20condition%20with%20modal-specific%20characteristic%20and%20serve%20as%0Alinker%20for%20inter-modal%20collaboration%20%2C%20fully%20learns%20each%20modality%20control%20in%0Aface%20synthesis%20process%20as%20well%20as%20inter-modal%20collaboration.%20The%20entropy-aware%0Amodal-adaptive%20modulation%20finely%20adjust%20diffusion%20noise%20according%20to%0Amodal-specific%20characteristics%20and%20given%20conditions%2C%20enabling%20well-informed%0Astep%20along%20denoising%20trajectory%20and%20ultimately%20leading%20to%20synthesis%20results%20of%0Ahigh%20fidelity%20and%20quality.%20Our%20framework%20improves%20multi-modal%20face%20synthesis%0Aunder%20various%20conditions%2C%20surpassing%20current%20methods%20in%20image%20quality%20and%0Afidelity%2C%20as%20demonstrated%20by%20our%20thorough%20experimental%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16274v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Flexible%2C%20Scalable%2C%20and%20Adaptive%20Multi-Modal%20Conditioned%20Face%0A%20%20Synthesis&entry.906535625=Jingjing%20Ren%20and%20Cheng%20Xu%20and%20Haoyu%20Chen%20and%20Xinran%20Qin%20and%20Lei%20Zhu&entry.1292438233=%20%20Recent%20progress%20in%20multi-modal%20conditioned%20face%20synthesis%20has%20enabled%20the%0Acreation%20of%20visually%20striking%20and%20accurately%20aligned%20facial%20images.%20Yet%2C%0Acurrent%20methods%20still%20face%20issues%20with%20scalability%2C%20limited%20flexibility%2C%20and%20a%0Aone-size-fits-all%20approach%20to%20control%20strength%2C%20not%20accounting%20for%20the%0Adiffering%20levels%20of%20conditional%20entropy%2C%20a%20measure%20of%20unpredictability%20in%20data%0Agiven%20some%20condition%2C%20across%20modalities.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20novel%20uni-modal%20training%20approach%20with%20modal%20surrogates%2C%20coupled%0Awith%20an%20entropy-aware%20modal-adaptive%20modulation%2C%20to%20support%20flexible%2C%20scalable%2C%0Aand%20scalable%20multi-modal%20conditioned%20face%20synthesis%20network.%20Our%20uni-modal%0Atraining%20with%20modal%20surrogate%20that%20only%20leverage%20uni-modal%20data%2C%20use%20modal%0Asurrogate%20to%20decorate%20condition%20with%20modal-specific%20characteristic%20and%20serve%20as%0Alinker%20for%20inter-modal%20collaboration%20%2C%20fully%20learns%20each%20modality%20control%20in%0Aface%20synthesis%20process%20as%20well%20as%20inter-modal%20collaboration.%20The%20entropy-aware%0Amodal-adaptive%20modulation%20finely%20adjust%20diffusion%20noise%20according%20to%0Amodal-specific%20characteristics%20and%20given%20conditions%2C%20enabling%20well-informed%0Astep%20along%20denoising%20trajectory%20and%20ultimately%20leading%20to%20synthesis%20results%20of%0Ahigh%20fidelity%20and%20quality.%20Our%20framework%20improves%20multi-modal%20face%20synthesis%0Aunder%20various%20conditions%2C%20surpassing%20current%20methods%20in%20image%20quality%20and%0Afidelity%2C%20as%20demonstrated%20by%20our%20thorough%20experimental%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16274v2&entry.124074799=Read"},
{"title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction\n  and Generation", "author": "Yinghao Xu and Zifan Shi and Wang Yifan and Hansheng Chen and Ceyuan Yang and Sida Peng and Yujun Shen and Gordon Wetzstein", "abstract": "  We introduce GRM, a large-scale reconstructor capable of recovering a 3D\nasset from sparse-view images in around 0.1s. GRM is a feed-forward\ntransformer-based model that efficiently incorporates multi-view information to\ntranslate the input pixels into pixel-aligned Gaussians, which are unprojected\nto create a set of densely distributed 3D Gaussians representing a scene.\nTogether, our transformer architecture and the use of 3D Gaussians unlock a\nscalable and efficient reconstruction framework. Extensive experimental results\ndemonstrate the superiority of our method over alternatives regarding both\nreconstruction quality and efficiency. We also showcase the potential of GRM in\ngenerative tasks, i.e., text-to-3D and image-to-3D, by integrating it with\nexisting multi-view diffusion models. Our project website is at:\nhttps://justimyhxu.github.io/projects/grm/.\n", "link": "http://arxiv.org/abs/2403.14621v1", "date": "2024-03-21", "relevancy": 2.2411, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5956}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5843}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5153}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GRM%3A%20Large%20Gaussian%20Reconstruction%20Model%20for%20Efficient%203D%20Reconstruction%0A%20%20and%20Generation&body=Title%3A%20GRM%3A%20Large%20Gaussian%20Reconstruction%20Model%20for%20Efficient%203D%20Reconstruction%0A%20%20and%20Generation%0AAuthor%3A%20Yinghao%20Xu%20and%20Zifan%20Shi%20and%20Wang%20Yifan%20and%20Hansheng%20Chen%20and%20Ceyuan%20Yang%20and%20Sida%20Peng%20and%20Yujun%20Shen%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20We%20introduce%20GRM%2C%20a%20large-scale%20reconstructor%20capable%20of%20recovering%20a%203D%0Aasset%20from%20sparse-view%20images%20in%20around%200.1s.%20GRM%20is%20a%20feed-forward%0Atransformer-based%20model%20that%20efficiently%20incorporates%20multi-view%20information%20to%0Atranslate%20the%20input%20pixels%20into%20pixel-aligned%20Gaussians%2C%20which%20are%20unprojected%0Ato%20create%20a%20set%20of%20densely%20distributed%203D%20Gaussians%20representing%20a%20scene.%0ATogether%2C%20our%20transformer%20architecture%20and%20the%20use%20of%203D%20Gaussians%20unlock%20a%0Ascalable%20and%20efficient%20reconstruction%20framework.%20Extensive%20experimental%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20alternatives%20regarding%20both%0Areconstruction%20quality%20and%20efficiency.%20We%20also%20showcase%20the%20potential%20of%20GRM%20in%0Agenerative%20tasks%2C%20i.e.%2C%20text-to-3D%20and%20image-to-3D%2C%20by%20integrating%20it%20with%0Aexisting%20multi-view%20diffusion%20models.%20Our%20project%20website%20is%20at%3A%0Ahttps%3A//justimyhxu.github.io/projects/grm/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14621v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRM%3A%20Large%20Gaussian%20Reconstruction%20Model%20for%20Efficient%203D%20Reconstruction%0A%20%20and%20Generation&entry.906535625=Yinghao%20Xu%20and%20Zifan%20Shi%20and%20Wang%20Yifan%20and%20Hansheng%20Chen%20and%20Ceyuan%20Yang%20and%20Sida%20Peng%20and%20Yujun%20Shen%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20We%20introduce%20GRM%2C%20a%20large-scale%20reconstructor%20capable%20of%20recovering%20a%203D%0Aasset%20from%20sparse-view%20images%20in%20around%200.1s.%20GRM%20is%20a%20feed-forward%0Atransformer-based%20model%20that%20efficiently%20incorporates%20multi-view%20information%20to%0Atranslate%20the%20input%20pixels%20into%20pixel-aligned%20Gaussians%2C%20which%20are%20unprojected%0Ato%20create%20a%20set%20of%20densely%20distributed%203D%20Gaussians%20representing%20a%20scene.%0ATogether%2C%20our%20transformer%20architecture%20and%20the%20use%20of%203D%20Gaussians%20unlock%20a%0Ascalable%20and%20efficient%20reconstruction%20framework.%20Extensive%20experimental%20results%0Ademonstrate%20the%20superiority%20of%20our%20method%20over%20alternatives%20regarding%20both%0Areconstruction%20quality%20and%20efficiency.%20We%20also%20showcase%20the%20potential%20of%20GRM%20in%0Agenerative%20tasks%2C%20i.e.%2C%20text-to-3D%20and%20image-to-3D%2C%20by%20integrating%20it%20with%0Aexisting%20multi-view%20diffusion%20models.%20Our%20project%20website%20is%20at%3A%0Ahttps%3A//justimyhxu.github.io/projects/grm/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14621v1&entry.124074799=Read"},
{"title": "MULDE: Multiscale Log-Density Estimation via Denoising Score Matching\n  for Video Anomaly Detection", "author": "Jakub Micorek and Horst Possegger and Dominik Narnhofer and Horst Bischof and Mateusz Kozinski", "abstract": "  We propose a novel approach to video anomaly detection: we treat feature\nvectors extracted from videos as realizations of a random variable with a fixed\ndistribution and model this distribution with a neural network. This lets us\nestimate the likelihood of test videos and detect video anomalies by\nthresholding the likelihood estimates. We train our video anomaly detector\nusing a modification of denoising score matching, a method that injects\ntraining data with noise to facilitate modeling its distribution. To eliminate\nhyperparameter selection, we model the distribution of noisy video features\nacross a range of noise levels and introduce a regularizer that tends to align\nthe models for different levels of noise. At test time, we combine anomaly\nindications at multiple noise scales with a Gaussian mixture model. Running our\nvideo anomaly detector induces minimal delays as inference requires merely\nextracting the features and forward-propagating them through a shallow neural\nnetwork and a Gaussian mixture model. Our experiments on five popular video\nanomaly detection benchmarks demonstrate state-of-the-art performance, both in\nthe object-centric and in the frame-centric setup.\n", "link": "http://arxiv.org/abs/2403.14497v1", "date": "2024-03-21", "relevancy": 2.2382, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.574}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5508}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5454}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MULDE%3A%20Multiscale%20Log-Density%20Estimation%20via%20Denoising%20Score%20Matching%0A%20%20for%20Video%20Anomaly%20Detection&body=Title%3A%20MULDE%3A%20Multiscale%20Log-Density%20Estimation%20via%20Denoising%20Score%20Matching%0A%20%20for%20Video%20Anomaly%20Detection%0AAuthor%3A%20Jakub%20Micorek%20and%20Horst%20Possegger%20and%20Dominik%20Narnhofer%20and%20Horst%20Bischof%20and%20Mateusz%20Kozinski%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20to%20video%20anomaly%20detection%3A%20we%20treat%20feature%0Avectors%20extracted%20from%20videos%20as%20realizations%20of%20a%20random%20variable%20with%20a%20fixed%0Adistribution%20and%20model%20this%20distribution%20with%20a%20neural%20network.%20This%20lets%20us%0Aestimate%20the%20likelihood%20of%20test%20videos%20and%20detect%20video%20anomalies%20by%0Athresholding%20the%20likelihood%20estimates.%20We%20train%20our%20video%20anomaly%20detector%0Ausing%20a%20modification%20of%20denoising%20score%20matching%2C%20a%20method%20that%20injects%0Atraining%20data%20with%20noise%20to%20facilitate%20modeling%20its%20distribution.%20To%20eliminate%0Ahyperparameter%20selection%2C%20we%20model%20the%20distribution%20of%20noisy%20video%20features%0Aacross%20a%20range%20of%20noise%20levels%20and%20introduce%20a%20regularizer%20that%20tends%20to%20align%0Athe%20models%20for%20different%20levels%20of%20noise.%20At%20test%20time%2C%20we%20combine%20anomaly%0Aindications%20at%20multiple%20noise%20scales%20with%20a%20Gaussian%20mixture%20model.%20Running%20our%0Avideo%20anomaly%20detector%20induces%20minimal%20delays%20as%20inference%20requires%20merely%0Aextracting%20the%20features%20and%20forward-propagating%20them%20through%20a%20shallow%20neural%0Anetwork%20and%20a%20Gaussian%20mixture%20model.%20Our%20experiments%20on%20five%20popular%20video%0Aanomaly%20detection%20benchmarks%20demonstrate%20state-of-the-art%20performance%2C%20both%20in%0Athe%20object-centric%20and%20in%20the%20frame-centric%20setup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14497v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MULDE%3A%20Multiscale%20Log-Density%20Estimation%20via%20Denoising%20Score%20Matching%0A%20%20for%20Video%20Anomaly%20Detection&entry.906535625=Jakub%20Micorek%20and%20Horst%20Possegger%20and%20Dominik%20Narnhofer%20and%20Horst%20Bischof%20and%20Mateusz%20Kozinski&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20to%20video%20anomaly%20detection%3A%20we%20treat%20feature%0Avectors%20extracted%20from%20videos%20as%20realizations%20of%20a%20random%20variable%20with%20a%20fixed%0Adistribution%20and%20model%20this%20distribution%20with%20a%20neural%20network.%20This%20lets%20us%0Aestimate%20the%20likelihood%20of%20test%20videos%20and%20detect%20video%20anomalies%20by%0Athresholding%20the%20likelihood%20estimates.%20We%20train%20our%20video%20anomaly%20detector%0Ausing%20a%20modification%20of%20denoising%20score%20matching%2C%20a%20method%20that%20injects%0Atraining%20data%20with%20noise%20to%20facilitate%20modeling%20its%20distribution.%20To%20eliminate%0Ahyperparameter%20selection%2C%20we%20model%20the%20distribution%20of%20noisy%20video%20features%0Aacross%20a%20range%20of%20noise%20levels%20and%20introduce%20a%20regularizer%20that%20tends%20to%20align%0Athe%20models%20for%20different%20levels%20of%20noise.%20At%20test%20time%2C%20we%20combine%20anomaly%0Aindications%20at%20multiple%20noise%20scales%20with%20a%20Gaussian%20mixture%20model.%20Running%20our%0Avideo%20anomaly%20detector%20induces%20minimal%20delays%20as%20inference%20requires%20merely%0Aextracting%20the%20features%20and%20forward-propagating%20them%20through%20a%20shallow%20neural%0Anetwork%20and%20a%20Gaussian%20mixture%20model.%20Our%20experiments%20on%20five%20popular%20video%0Aanomaly%20detection%20benchmarks%20demonstrate%20state-of-the-art%20performance%2C%20both%20in%0Athe%20object-centric%20and%20in%20the%20frame-centric%20setup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14497v1&entry.124074799=Read"},
{"title": "Evaluation and Deployment of LiDAR-based Place Recognition in Dense\n  Forests", "author": "Haedam Oh and Nived Chebrolu and Matias Mattamala and Leonard Frei\u00dfmuth and Maurice Fallon", "abstract": "  Many LiDAR place recognition systems have been developed and tested\nspecifically for urban driving scenarios. Their performance in natural\nenvironments such as forests and woodlands have been studied less closely. In\nthis paper, we analyzed the capabilities of four different LiDAR place\nrecognition systems, both handcrafted and learning-based methods, using LiDAR\ndata collected with a handheld device and legged robot within dense forest\nenvironments. In particular, we focused on evaluating localization where there\nis significant translational and orientation difference between corresponding\nLiDAR scan pairs. This is particularly important for forest survey systems\nwhere the sensor or robot does not follow a defined road or path. Extending our\nanalysis we then incorporated the best performing approach, Logg3dNet, into a\nfull 6-DoF pose estimation system -- introducing several verification layers\nfor precise registration. We demonstrated the performance of our methods in\nthree operational modes: online SLAM, offline multi-mission SLAM map merging,\nand relocalization into a prior map. We evaluated these modes using data\ncaptured in forests from three different countries, achieving 80% of correct\nloop closures candidates with baseline distances up to 5m, and 60% up to 10m.\n", "link": "http://arxiv.org/abs/2403.14326v1", "date": "2024-03-21", "relevancy": 2.2337, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5339}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluation%20and%20Deployment%20of%20LiDAR-based%20Place%20Recognition%20in%20Dense%0A%20%20Forests&body=Title%3A%20Evaluation%20and%20Deployment%20of%20LiDAR-based%20Place%20Recognition%20in%20Dense%0A%20%20Forests%0AAuthor%3A%20Haedam%20Oh%20and%20Nived%20Chebrolu%20and%20Matias%20Mattamala%20and%20Leonard%20Frei%C3%9Fmuth%20and%20Maurice%20Fallon%0AAbstract%3A%20%20%20Many%20LiDAR%20place%20recognition%20systems%20have%20been%20developed%20and%20tested%0Aspecifically%20for%20urban%20driving%20scenarios.%20Their%20performance%20in%20natural%0Aenvironments%20such%20as%20forests%20and%20woodlands%20have%20been%20studied%20less%20closely.%20In%0Athis%20paper%2C%20we%20analyzed%20the%20capabilities%20of%20four%20different%20LiDAR%20place%0Arecognition%20systems%2C%20both%20handcrafted%20and%20learning-based%20methods%2C%20using%20LiDAR%0Adata%20collected%20with%20a%20handheld%20device%20and%20legged%20robot%20within%20dense%20forest%0Aenvironments.%20In%20particular%2C%20we%20focused%20on%20evaluating%20localization%20where%20there%0Ais%20significant%20translational%20and%20orientation%20difference%20between%20corresponding%0ALiDAR%20scan%20pairs.%20This%20is%20particularly%20important%20for%20forest%20survey%20systems%0Awhere%20the%20sensor%20or%20robot%20does%20not%20follow%20a%20defined%20road%20or%20path.%20Extending%20our%0Aanalysis%20we%20then%20incorporated%20the%20best%20performing%20approach%2C%20Logg3dNet%2C%20into%20a%0Afull%206-DoF%20pose%20estimation%20system%20--%20introducing%20several%20verification%20layers%0Afor%20precise%20registration.%20We%20demonstrated%20the%20performance%20of%20our%20methods%20in%0Athree%20operational%20modes%3A%20online%20SLAM%2C%20offline%20multi-mission%20SLAM%20map%20merging%2C%0Aand%20relocalization%20into%20a%20prior%20map.%20We%20evaluated%20these%20modes%20using%20data%0Acaptured%20in%20forests%20from%20three%20different%20countries%2C%20achieving%2080%25%20of%20correct%0Aloop%20closures%20candidates%20with%20baseline%20distances%20up%20to%205m%2C%20and%2060%25%20up%20to%2010m.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14326v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20and%20Deployment%20of%20LiDAR-based%20Place%20Recognition%20in%20Dense%0A%20%20Forests&entry.906535625=Haedam%20Oh%20and%20Nived%20Chebrolu%20and%20Matias%20Mattamala%20and%20Leonard%20Frei%C3%9Fmuth%20and%20Maurice%20Fallon&entry.1292438233=%20%20Many%20LiDAR%20place%20recognition%20systems%20have%20been%20developed%20and%20tested%0Aspecifically%20for%20urban%20driving%20scenarios.%20Their%20performance%20in%20natural%0Aenvironments%20such%20as%20forests%20and%20woodlands%20have%20been%20studied%20less%20closely.%20In%0Athis%20paper%2C%20we%20analyzed%20the%20capabilities%20of%20four%20different%20LiDAR%20place%0Arecognition%20systems%2C%20both%20handcrafted%20and%20learning-based%20methods%2C%20using%20LiDAR%0Adata%20collected%20with%20a%20handheld%20device%20and%20legged%20robot%20within%20dense%20forest%0Aenvironments.%20In%20particular%2C%20we%20focused%20on%20evaluating%20localization%20where%20there%0Ais%20significant%20translational%20and%20orientation%20difference%20between%20corresponding%0ALiDAR%20scan%20pairs.%20This%20is%20particularly%20important%20for%20forest%20survey%20systems%0Awhere%20the%20sensor%20or%20robot%20does%20not%20follow%20a%20defined%20road%20or%20path.%20Extending%20our%0Aanalysis%20we%20then%20incorporated%20the%20best%20performing%20approach%2C%20Logg3dNet%2C%20into%20a%0Afull%206-DoF%20pose%20estimation%20system%20--%20introducing%20several%20verification%20layers%0Afor%20precise%20registration.%20We%20demonstrated%20the%20performance%20of%20our%20methods%20in%0Athree%20operational%20modes%3A%20online%20SLAM%2C%20offline%20multi-mission%20SLAM%20map%20merging%2C%0Aand%20relocalization%20into%20a%20prior%20map.%20We%20evaluated%20these%20modes%20using%20data%0Acaptured%20in%20forests%20from%20three%20different%20countries%2C%20achieving%2080%25%20of%20correct%0Aloop%20closures%20candidates%20with%20baseline%20distances%20up%20to%205m%2C%20and%2060%25%20up%20to%2010m.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14326v1&entry.124074799=Read"},
{"title": "Knowledge-Enhanced Recommendation with User-Centric Subgraph Network", "author": "Guangyi Liu and Quanming Yao and Yongqi Zhang and Lei Chen", "abstract": "  Recommendation systems, as widely implemented nowadays on various platforms,\nrecommend relevant items to users based on their preferences. The classical\nmethods which rely on user-item interaction matrices has limitations,\nespecially in scenarios where there is a lack of interaction data for new\nitems. Knowledge graph (KG)-based recommendation systems have emerged as a\npromising solution. However, most KG-based methods adopt node embeddings, which\ndo not provide personalized recommendations for different users and cannot\ngeneralize well to the new items. To address these limitations, we propose\nKnowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning\napproach with graph neural network (GNN) for effective recommendation. KUCNet\nconstructs a U-I subgraph for each user-item pair that captures both the\nhistorical information of user-item interactions and the side information\nprovided in KG. An attention-based GNN is designed to encode the U-I subgraphs\nfor recommendation. Considering efficiency, the pruned user-centric computation\ngraph is further introduced such that multiple U-I subgraphs can be\nsimultaneously computed and that the size can be pruned by Personalized\nPageRank. Our proposed method achieves accurate, efficient, and interpretable\nrecommendations especially for new items. Experimental results demonstrate the\nsuperiority of KUCNet over state-of-the-art KG-based and collaborative\nfiltering (CF)-based methods.\n", "link": "http://arxiv.org/abs/2403.14377v1", "date": "2024-03-21", "relevancy": 2.2039, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4638}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4344}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4241}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Knowledge-Enhanced%20Recommendation%20with%20User-Centric%20Subgraph%20Network&body=Title%3A%20Knowledge-Enhanced%20Recommendation%20with%20User-Centric%20Subgraph%20Network%0AAuthor%3A%20Guangyi%20Liu%20and%20Quanming%20Yao%20and%20Yongqi%20Zhang%20and%20Lei%20Chen%0AAbstract%3A%20%20%20Recommendation%20systems%2C%20as%20widely%20implemented%20nowadays%20on%20various%20platforms%2C%0Arecommend%20relevant%20items%20to%20users%20based%20on%20their%20preferences.%20The%20classical%0Amethods%20which%20rely%20on%20user-item%20interaction%20matrices%20has%20limitations%2C%0Aespecially%20in%20scenarios%20where%20there%20is%20a%20lack%20of%20interaction%20data%20for%20new%0Aitems.%20Knowledge%20graph%20%28KG%29-based%20recommendation%20systems%20have%20emerged%20as%20a%0Apromising%20solution.%20However%2C%20most%20KG-based%20methods%20adopt%20node%20embeddings%2C%20which%0Ado%20not%20provide%20personalized%20recommendations%20for%20different%20users%20and%20cannot%0Ageneralize%20well%20to%20the%20new%20items.%20To%20address%20these%20limitations%2C%20we%20propose%0AKnowledge-enhanced%20User-Centric%20subgraph%20Network%20%28KUCNet%29%2C%20a%20subgraph%20learning%0Aapproach%20with%20graph%20neural%20network%20%28GNN%29%20for%20effective%20recommendation.%20KUCNet%0Aconstructs%20a%20U-I%20subgraph%20for%20each%20user-item%20pair%20that%20captures%20both%20the%0Ahistorical%20information%20of%20user-item%20interactions%20and%20the%20side%20information%0Aprovided%20in%20KG.%20An%20attention-based%20GNN%20is%20designed%20to%20encode%20the%20U-I%20subgraphs%0Afor%20recommendation.%20Considering%20efficiency%2C%20the%20pruned%20user-centric%20computation%0Agraph%20is%20further%20introduced%20such%20that%20multiple%20U-I%20subgraphs%20can%20be%0Asimultaneously%20computed%20and%20that%20the%20size%20can%20be%20pruned%20by%20Personalized%0APageRank.%20Our%20proposed%20method%20achieves%20accurate%2C%20efficient%2C%20and%20interpretable%0Arecommendations%20especially%20for%20new%20items.%20Experimental%20results%20demonstrate%20the%0Asuperiority%20of%20KUCNet%20over%20state-of-the-art%20KG-based%20and%20collaborative%0Afiltering%20%28CF%29-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14377v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Enhanced%20Recommendation%20with%20User-Centric%20Subgraph%20Network&entry.906535625=Guangyi%20Liu%20and%20Quanming%20Yao%20and%20Yongqi%20Zhang%20and%20Lei%20Chen&entry.1292438233=%20%20Recommendation%20systems%2C%20as%20widely%20implemented%20nowadays%20on%20various%20platforms%2C%0Arecommend%20relevant%20items%20to%20users%20based%20on%20their%20preferences.%20The%20classical%0Amethods%20which%20rely%20on%20user-item%20interaction%20matrices%20has%20limitations%2C%0Aespecially%20in%20scenarios%20where%20there%20is%20a%20lack%20of%20interaction%20data%20for%20new%0Aitems.%20Knowledge%20graph%20%28KG%29-based%20recommendation%20systems%20have%20emerged%20as%20a%0Apromising%20solution.%20However%2C%20most%20KG-based%20methods%20adopt%20node%20embeddings%2C%20which%0Ado%20not%20provide%20personalized%20recommendations%20for%20different%20users%20and%20cannot%0Ageneralize%20well%20to%20the%20new%20items.%20To%20address%20these%20limitations%2C%20we%20propose%0AKnowledge-enhanced%20User-Centric%20subgraph%20Network%20%28KUCNet%29%2C%20a%20subgraph%20learning%0Aapproach%20with%20graph%20neural%20network%20%28GNN%29%20for%20effective%20recommendation.%20KUCNet%0Aconstructs%20a%20U-I%20subgraph%20for%20each%20user-item%20pair%20that%20captures%20both%20the%0Ahistorical%20information%20of%20user-item%20interactions%20and%20the%20side%20information%0Aprovided%20in%20KG.%20An%20attention-based%20GNN%20is%20designed%20to%20encode%20the%20U-I%20subgraphs%0Afor%20recommendation.%20Considering%20efficiency%2C%20the%20pruned%20user-centric%20computation%0Agraph%20is%20further%20introduced%20such%20that%20multiple%20U-I%20subgraphs%20can%20be%0Asimultaneously%20computed%20and%20that%20the%20size%20can%20be%20pruned%20by%20Personalized%0APageRank.%20Our%20proposed%20method%20achieves%20accurate%2C%20efficient%2C%20and%20interpretable%0Arecommendations%20especially%20for%20new%20items.%20Experimental%20results%20demonstrate%20the%0Asuperiority%20of%20KUCNet%20over%20state-of-the-art%20KG-based%20and%20collaborative%0Afiltering%20%28CF%29-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14377v1&entry.124074799=Read"},
{"title": "AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and\n  Modulation", "author": "Yuning Cui and Syed Waqas Zamir and Salman Khan and Alois Knoll and Mubarak Shah and Fahad Shahbaz Khan", "abstract": "  In the image acquisition process, various forms of degradation, including\nnoise, haze, and rain, are frequently introduced. These degradations typically\narise from the inherent limitations of cameras or unfavorable ambient\nconditions. To recover clean images from degraded versions, numerous\nspecialized restoration methods have been developed, each targeting a specific\ntype of degradation. Recently, all-in-one algorithms have garnered significant\nattention by addressing different types of degradations within a single model\nwithout requiring prior information of the input degradation type. However,\nthese methods purely operate in the spatial domain and do not delve into the\ndistinct frequency variations inherent to different degradation types. To\naddress this gap, we propose an adaptive all-in-one image restoration network\nbased on frequency mining and modulation. Our approach is motivated by the\nobservation that different degradation types impact the image content on\ndifferent frequency subbands, thereby requiring different treatments for each\nrestoration task. Specifically, we first mine low- and high-frequency\ninformation from the input features, guided by the adaptively decoupled spectra\nof the degraded image. The extracted features are then modulated by a\nbidirectional operator to facilitate interactions between different frequency\ncomponents. Finally, the modulated features are merged into the original input\nfor a progressively guided restoration. With this approach, the model achieves\nadaptive reconstruction by accentuating the informative frequency subbands\naccording to different input degradations. Extensive experiments demonstrate\nthat the proposed method achieves state-of-the-art performance on different\nimage restoration tasks, including denoising, dehazing, deraining, motion\ndeblurring, and low-light image enhancement. Our code is available at\nhttps://github.com/c-yn/AdaIR.\n", "link": "http://arxiv.org/abs/2403.14614v1", "date": "2024-03-21", "relevancy": 2.193, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6045}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5121}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4981}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AdaIR%3A%20Adaptive%20All-in-One%20Image%20Restoration%20via%20Frequency%20Mining%20and%0A%20%20Modulation&body=Title%3A%20AdaIR%3A%20Adaptive%20All-in-One%20Image%20Restoration%20via%20Frequency%20Mining%20and%0A%20%20Modulation%0AAuthor%3A%20Yuning%20Cui%20and%20Syed%20Waqas%20Zamir%20and%20Salman%20Khan%20and%20Alois%20Knoll%20and%20Mubarak%20Shah%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20In%20the%20image%20acquisition%20process%2C%20various%20forms%20of%20degradation%2C%20including%0Anoise%2C%20haze%2C%20and%20rain%2C%20are%20frequently%20introduced.%20These%20degradations%20typically%0Aarise%20from%20the%20inherent%20limitations%20of%20cameras%20or%20unfavorable%20ambient%0Aconditions.%20To%20recover%20clean%20images%20from%20degraded%20versions%2C%20numerous%0Aspecialized%20restoration%20methods%20have%20been%20developed%2C%20each%20targeting%20a%20specific%0Atype%20of%20degradation.%20Recently%2C%20all-in-one%20algorithms%20have%20garnered%20significant%0Aattention%20by%20addressing%20different%20types%20of%20degradations%20within%20a%20single%20model%0Awithout%20requiring%20prior%20information%20of%20the%20input%20degradation%20type.%20However%2C%0Athese%20methods%20purely%20operate%20in%20the%20spatial%20domain%20and%20do%20not%20delve%20into%20the%0Adistinct%20frequency%20variations%20inherent%20to%20different%20degradation%20types.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20an%20adaptive%20all-in-one%20image%20restoration%20network%0Abased%20on%20frequency%20mining%20and%20modulation.%20Our%20approach%20is%20motivated%20by%20the%0Aobservation%20that%20different%20degradation%20types%20impact%20the%20image%20content%20on%0Adifferent%20frequency%20subbands%2C%20thereby%20requiring%20different%20treatments%20for%20each%0Arestoration%20task.%20Specifically%2C%20we%20first%20mine%20low-%20and%20high-frequency%0Ainformation%20from%20the%20input%20features%2C%20guided%20by%20the%20adaptively%20decoupled%20spectra%0Aof%20the%20degraded%20image.%20The%20extracted%20features%20are%20then%20modulated%20by%20a%0Abidirectional%20operator%20to%20facilitate%20interactions%20between%20different%20frequency%0Acomponents.%20Finally%2C%20the%20modulated%20features%20are%20merged%20into%20the%20original%20input%0Afor%20a%20progressively%20guided%20restoration.%20With%20this%20approach%2C%20the%20model%20achieves%0Aadaptive%20reconstruction%20by%20accentuating%20the%20informative%20frequency%20subbands%0Aaccording%20to%20different%20input%20degradations.%20Extensive%20experiments%20demonstrate%0Athat%20the%20proposed%20method%20achieves%20state-of-the-art%20performance%20on%20different%0Aimage%20restoration%20tasks%2C%20including%20denoising%2C%20dehazing%2C%20deraining%2C%20motion%0Adeblurring%2C%20and%20low-light%20image%20enhancement.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/c-yn/AdaIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14614v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaIR%3A%20Adaptive%20All-in-One%20Image%20Restoration%20via%20Frequency%20Mining%20and%0A%20%20Modulation&entry.906535625=Yuning%20Cui%20and%20Syed%20Waqas%20Zamir%20and%20Salman%20Khan%20and%20Alois%20Knoll%20and%20Mubarak%20Shah%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20In%20the%20image%20acquisition%20process%2C%20various%20forms%20of%20degradation%2C%20including%0Anoise%2C%20haze%2C%20and%20rain%2C%20are%20frequently%20introduced.%20These%20degradations%20typically%0Aarise%20from%20the%20inherent%20limitations%20of%20cameras%20or%20unfavorable%20ambient%0Aconditions.%20To%20recover%20clean%20images%20from%20degraded%20versions%2C%20numerous%0Aspecialized%20restoration%20methods%20have%20been%20developed%2C%20each%20targeting%20a%20specific%0Atype%20of%20degradation.%20Recently%2C%20all-in-one%20algorithms%20have%20garnered%20significant%0Aattention%20by%20addressing%20different%20types%20of%20degradations%20within%20a%20single%20model%0Awithout%20requiring%20prior%20information%20of%20the%20input%20degradation%20type.%20However%2C%0Athese%20methods%20purely%20operate%20in%20the%20spatial%20domain%20and%20do%20not%20delve%20into%20the%0Adistinct%20frequency%20variations%20inherent%20to%20different%20degradation%20types.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20an%20adaptive%20all-in-one%20image%20restoration%20network%0Abased%20on%20frequency%20mining%20and%20modulation.%20Our%20approach%20is%20motivated%20by%20the%0Aobservation%20that%20different%20degradation%20types%20impact%20the%20image%20content%20on%0Adifferent%20frequency%20subbands%2C%20thereby%20requiring%20different%20treatments%20for%20each%0Arestoration%20task.%20Specifically%2C%20we%20first%20mine%20low-%20and%20high-frequency%0Ainformation%20from%20the%20input%20features%2C%20guided%20by%20the%20adaptively%20decoupled%20spectra%0Aof%20the%20degraded%20image.%20The%20extracted%20features%20are%20then%20modulated%20by%20a%0Abidirectional%20operator%20to%20facilitate%20interactions%20between%20different%20frequency%0Acomponents.%20Finally%2C%20the%20modulated%20features%20are%20merged%20into%20the%20original%20input%0Afor%20a%20progressively%20guided%20restoration.%20With%20this%20approach%2C%20the%20model%20achieves%0Aadaptive%20reconstruction%20by%20accentuating%20the%20informative%20frequency%20subbands%0Aaccording%20to%20different%20input%20degradations.%20Extensive%20experiments%20demonstrate%0Athat%20the%20proposed%20method%20achieves%20state-of-the-art%20performance%20on%20different%0Aimage%20restoration%20tasks%2C%20including%20denoising%2C%20dehazing%2C%20deraining%2C%20motion%0Adeblurring%2C%20and%20low-light%20image%20enhancement.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/c-yn/AdaIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14614v1&entry.124074799=Read"},
{"title": "Token Transformation Matters: Towards Faithful Post-hoc Explanation for\n  Vision Transformer", "author": "Junyi Wu and Bin Duan and Weitai Kang and Hao Tang and Yan Yan", "abstract": "  While Transformers have rapidly gained popularity in various computer vision\napplications, post-hoc explanations of their internal mechanisms remain largely\nunexplored. Vision Transformers extract visual information by representing\nimage regions as transformed tokens and integrating them via attention weights.\nHowever, existing post-hoc explanation methods merely consider these attention\nweights, neglecting crucial information from the transformed tokens, which\nfails to accurately illustrate the rationales behind the models' predictions.\nTo incorporate the influence of token transformation into interpretation, we\npropose TokenTM, a novel post-hoc explanation method that utilizes our\nintroduced measurement of token transformation effects. Specifically, we\nquantify token transformation effects by measuring changes in token lengths and\ncorrelations in their directions pre- and post-transformation. Moreover, we\ndevelop initialization and aggregation rules to integrate both attention\nweights and token transformation effects across all layers, capturing holistic\ntoken contributions throughout the model. Experimental results on segmentation\nand perturbation tests demonstrate the superiority of our proposed TokenTM\ncompared to state-of-the-art Vision Transformer explanation methods.\n", "link": "http://arxiv.org/abs/2403.14552v1", "date": "2024-03-21", "relevancy": 2.1872, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5833}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5477}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.51}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Token%20Transformation%20Matters%3A%20Towards%20Faithful%20Post-hoc%20Explanation%20for%0A%20%20Vision%20Transformer&body=Title%3A%20Token%20Transformation%20Matters%3A%20Towards%20Faithful%20Post-hoc%20Explanation%20for%0A%20%20Vision%20Transformer%0AAuthor%3A%20Junyi%20Wu%20and%20Bin%20Duan%20and%20Weitai%20Kang%20and%20Hao%20Tang%20and%20Yan%20Yan%0AAbstract%3A%20%20%20While%20Transformers%20have%20rapidly%20gained%20popularity%20in%20various%20computer%20vision%0Aapplications%2C%20post-hoc%20explanations%20of%20their%20internal%20mechanisms%20remain%20largely%0Aunexplored.%20Vision%20Transformers%20extract%20visual%20information%20by%20representing%0Aimage%20regions%20as%20transformed%20tokens%20and%20integrating%20them%20via%20attention%20weights.%0AHowever%2C%20existing%20post-hoc%20explanation%20methods%20merely%20consider%20these%20attention%0Aweights%2C%20neglecting%20crucial%20information%20from%20the%20transformed%20tokens%2C%20which%0Afails%20to%20accurately%20illustrate%20the%20rationales%20behind%20the%20models%27%20predictions.%0ATo%20incorporate%20the%20influence%20of%20token%20transformation%20into%20interpretation%2C%20we%0Apropose%20TokenTM%2C%20a%20novel%20post-hoc%20explanation%20method%20that%20utilizes%20our%0Aintroduced%20measurement%20of%20token%20transformation%20effects.%20Specifically%2C%20we%0Aquantify%20token%20transformation%20effects%20by%20measuring%20changes%20in%20token%20lengths%20and%0Acorrelations%20in%20their%20directions%20pre-%20and%20post-transformation.%20Moreover%2C%20we%0Adevelop%20initialization%20and%20aggregation%20rules%20to%20integrate%20both%20attention%0Aweights%20and%20token%20transformation%20effects%20across%20all%20layers%2C%20capturing%20holistic%0Atoken%20contributions%20throughout%20the%20model.%20Experimental%20results%20on%20segmentation%0Aand%20perturbation%20tests%20demonstrate%20the%20superiority%20of%20our%20proposed%20TokenTM%0Acompared%20to%20state-of-the-art%20Vision%20Transformer%20explanation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14552v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Transformation%20Matters%3A%20Towards%20Faithful%20Post-hoc%20Explanation%20for%0A%20%20Vision%20Transformer&entry.906535625=Junyi%20Wu%20and%20Bin%20Duan%20and%20Weitai%20Kang%20and%20Hao%20Tang%20and%20Yan%20Yan&entry.1292438233=%20%20While%20Transformers%20have%20rapidly%20gained%20popularity%20in%20various%20computer%20vision%0Aapplications%2C%20post-hoc%20explanations%20of%20their%20internal%20mechanisms%20remain%20largely%0Aunexplored.%20Vision%20Transformers%20extract%20visual%20information%20by%20representing%0Aimage%20regions%20as%20transformed%20tokens%20and%20integrating%20them%20via%20attention%20weights.%0AHowever%2C%20existing%20post-hoc%20explanation%20methods%20merely%20consider%20these%20attention%0Aweights%2C%20neglecting%20crucial%20information%20from%20the%20transformed%20tokens%2C%20which%0Afails%20to%20accurately%20illustrate%20the%20rationales%20behind%20the%20models%27%20predictions.%0ATo%20incorporate%20the%20influence%20of%20token%20transformation%20into%20interpretation%2C%20we%0Apropose%20TokenTM%2C%20a%20novel%20post-hoc%20explanation%20method%20that%20utilizes%20our%0Aintroduced%20measurement%20of%20token%20transformation%20effects.%20Specifically%2C%20we%0Aquantify%20token%20transformation%20effects%20by%20measuring%20changes%20in%20token%20lengths%20and%0Acorrelations%20in%20their%20directions%20pre-%20and%20post-transformation.%20Moreover%2C%20we%0Adevelop%20initialization%20and%20aggregation%20rules%20to%20integrate%20both%20attention%0Aweights%20and%20token%20transformation%20effects%20across%20all%20layers%2C%20capturing%20holistic%0Atoken%20contributions%20throughout%20the%20model.%20Experimental%20results%20on%20segmentation%0Aand%20perturbation%20tests%20demonstrate%20the%20superiority%20of%20our%20proposed%20TokenTM%0Acompared%20to%20state-of-the-art%20Vision%20Transformer%20explanation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14552v1&entry.124074799=Read"},
{"title": "Physics-Informed Diffusion Models", "author": "Jan-Hendrik Bastek and WaiChing Sun and Dennis M. Kochmann", "abstract": "  Generative models such as denoising diffusion models are quickly advancing\ntheir ability to approximate highly complex data distributions. They are also\nincreasingly leveraged in scientific machine learning, where samples from the\nimplied data distribution are expected to adhere to specific governing\nequations. We present a framework to inform denoising diffusion models on\nunderlying constraints on such generated samples during model training. Our\napproach improves the alignment of the generated samples with the imposed\nconstraints and significantly outperforms existing methods without affecting\ninference speed. Additionally, our findings suggest that incorporating such\nconstraints during training provides a natural regularization against\noverfitting. Our framework is easy to implement and versatile in its\napplicability for imposing equality and inequality constraints as well as\nauxiliary optimization objectives.\n", "link": "http://arxiv.org/abs/2403.14404v1", "date": "2024-03-21", "relevancy": 2.1865, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.593}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5579}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5168}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Diffusion%20Models&body=Title%3A%20Physics-Informed%20Diffusion%20Models%0AAuthor%3A%20Jan-Hendrik%20Bastek%20and%20WaiChing%20Sun%20and%20Dennis%20M.%20Kochmann%0AAbstract%3A%20%20%20Generative%20models%20such%20as%20denoising%20diffusion%20models%20are%20quickly%20advancing%0Atheir%20ability%20to%20approximate%20highly%20complex%20data%20distributions.%20They%20are%20also%0Aincreasingly%20leveraged%20in%20scientific%20machine%20learning%2C%20where%20samples%20from%20the%0Aimplied%20data%20distribution%20are%20expected%20to%20adhere%20to%20specific%20governing%0Aequations.%20We%20present%20a%20framework%20to%20inform%20denoising%20diffusion%20models%20on%0Aunderlying%20constraints%20on%20such%20generated%20samples%20during%20model%20training.%20Our%0Aapproach%20improves%20the%20alignment%20of%20the%20generated%20samples%20with%20the%20imposed%0Aconstraints%20and%20significantly%20outperforms%20existing%20methods%20without%20affecting%0Ainference%20speed.%20Additionally%2C%20our%20findings%20suggest%20that%20incorporating%20such%0Aconstraints%20during%20training%20provides%20a%20natural%20regularization%20against%0Aoverfitting.%20Our%20framework%20is%20easy%20to%20implement%20and%20versatile%20in%20its%0Aapplicability%20for%20imposing%20equality%20and%20inequality%20constraints%20as%20well%20as%0Aauxiliary%20optimization%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14404v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Diffusion%20Models&entry.906535625=Jan-Hendrik%20Bastek%20and%20WaiChing%20Sun%20and%20Dennis%20M.%20Kochmann&entry.1292438233=%20%20Generative%20models%20such%20as%20denoising%20diffusion%20models%20are%20quickly%20advancing%0Atheir%20ability%20to%20approximate%20highly%20complex%20data%20distributions.%20They%20are%20also%0Aincreasingly%20leveraged%20in%20scientific%20machine%20learning%2C%20where%20samples%20from%20the%0Aimplied%20data%20distribution%20are%20expected%20to%20adhere%20to%20specific%20governing%0Aequations.%20We%20present%20a%20framework%20to%20inform%20denoising%20diffusion%20models%20on%0Aunderlying%20constraints%20on%20such%20generated%20samples%20during%20model%20training.%20Our%0Aapproach%20improves%20the%20alignment%20of%20the%20generated%20samples%20with%20the%20imposed%0Aconstraints%20and%20significantly%20outperforms%20existing%20methods%20without%20affecting%0Ainference%20speed.%20Additionally%2C%20our%20findings%20suggest%20that%20incorporating%20such%0Aconstraints%20during%20training%20provides%20a%20natural%20regularization%20against%0Aoverfitting.%20Our%20framework%20is%20easy%20to%20implement%20and%20versatile%20in%20its%0Aapplicability%20for%20imposing%20equality%20and%20inequality%20constraints%20as%20well%20as%0Aauxiliary%20optimization%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14404v1&entry.124074799=Read"},
{"title": "SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance\n  Field", "author": "Lizhe Liu and Bohua Wang and Hongwei Xie and Daqi Liu and Li Liu and Zhiqiang Tian and Kuiyuan Yang and Bing Wang", "abstract": "  Vision-centric 3D environment understanding is both vital and challenging for\nautonomous driving systems. Recently, object-free methods have attracted\nconsiderable attention. Such methods perceive the world by predicting the\nsemantics of discrete voxel grids but fail to construct continuous and accurate\nobstacle surfaces. To this end, in this paper, we propose SurroundSDF to\nimplicitly predict the signed distance field (SDF) and semantic field for the\ncontinuous perception from surround images. Specifically, we introduce a\nquery-based approach and utilize SDF constrained by the Eikonal formulation to\naccurately describe the surfaces of obstacles. Furthermore, considering the\nabsence of precise SDF ground truth, we propose a novel weakly supervised\nparadigm for SDF, referred to as the Sandwich Eikonal formulation, which\nemphasizes applying correct and dense constraints on both sides of the surface,\nthereby enhancing the perceptual accuracy of the surface. Experiments suggest\nthat our method achieves SOTA for both occupancy prediction and 3D scene\nreconstruction tasks on the nuScenes dataset.\n", "link": "http://arxiv.org/abs/2403.14366v1", "date": "2024-03-21", "relevancy": 2.1833, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5616}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5476}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5377}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SurroundSDF%3A%20Implicit%203D%20Scene%20Understanding%20Based%20on%20Signed%20Distance%0A%20%20Field&body=Title%3A%20SurroundSDF%3A%20Implicit%203D%20Scene%20Understanding%20Based%20on%20Signed%20Distance%0A%20%20Field%0AAuthor%3A%20Lizhe%20Liu%20and%20Bohua%20Wang%20and%20Hongwei%20Xie%20and%20Daqi%20Liu%20and%20Li%20Liu%20and%20Zhiqiang%20Tian%20and%20Kuiyuan%20Yang%20and%20Bing%20Wang%0AAbstract%3A%20%20%20Vision-centric%203D%20environment%20understanding%20is%20both%20vital%20and%20challenging%20for%0Aautonomous%20driving%20systems.%20Recently%2C%20object-free%20methods%20have%20attracted%0Aconsiderable%20attention.%20Such%20methods%20perceive%20the%20world%20by%20predicting%20the%0Asemantics%20of%20discrete%20voxel%20grids%20but%20fail%20to%20construct%20continuous%20and%20accurate%0Aobstacle%20surfaces.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20propose%20SurroundSDF%20to%0Aimplicitly%20predict%20the%20signed%20distance%20field%20%28SDF%29%20and%20semantic%20field%20for%20the%0Acontinuous%20perception%20from%20surround%20images.%20Specifically%2C%20we%20introduce%20a%0Aquery-based%20approach%20and%20utilize%20SDF%20constrained%20by%20the%20Eikonal%20formulation%20to%0Aaccurately%20describe%20the%20surfaces%20of%20obstacles.%20Furthermore%2C%20considering%20the%0Aabsence%20of%20precise%20SDF%20ground%20truth%2C%20we%20propose%20a%20novel%20weakly%20supervised%0Aparadigm%20for%20SDF%2C%20referred%20to%20as%20the%20Sandwich%20Eikonal%20formulation%2C%20which%0Aemphasizes%20applying%20correct%20and%20dense%20constraints%20on%20both%20sides%20of%20the%20surface%2C%0Athereby%20enhancing%20the%20perceptual%20accuracy%20of%20the%20surface.%20Experiments%20suggest%0Athat%20our%20method%20achieves%20SOTA%20for%20both%20occupancy%20prediction%20and%203D%20scene%0Areconstruction%20tasks%20on%20the%20nuScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14366v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurroundSDF%3A%20Implicit%203D%20Scene%20Understanding%20Based%20on%20Signed%20Distance%0A%20%20Field&entry.906535625=Lizhe%20Liu%20and%20Bohua%20Wang%20and%20Hongwei%20Xie%20and%20Daqi%20Liu%20and%20Li%20Liu%20and%20Zhiqiang%20Tian%20and%20Kuiyuan%20Yang%20and%20Bing%20Wang&entry.1292438233=%20%20Vision-centric%203D%20environment%20understanding%20is%20both%20vital%20and%20challenging%20for%0Aautonomous%20driving%20systems.%20Recently%2C%20object-free%20methods%20have%20attracted%0Aconsiderable%20attention.%20Such%20methods%20perceive%20the%20world%20by%20predicting%20the%0Asemantics%20of%20discrete%20voxel%20grids%20but%20fail%20to%20construct%20continuous%20and%20accurate%0Aobstacle%20surfaces.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20propose%20SurroundSDF%20to%0Aimplicitly%20predict%20the%20signed%20distance%20field%20%28SDF%29%20and%20semantic%20field%20for%20the%0Acontinuous%20perception%20from%20surround%20images.%20Specifically%2C%20we%20introduce%20a%0Aquery-based%20approach%20and%20utilize%20SDF%20constrained%20by%20the%20Eikonal%20formulation%20to%0Aaccurately%20describe%20the%20surfaces%20of%20obstacles.%20Furthermore%2C%20considering%20the%0Aabsence%20of%20precise%20SDF%20ground%20truth%2C%20we%20propose%20a%20novel%20weakly%20supervised%0Aparadigm%20for%20SDF%2C%20referred%20to%20as%20the%20Sandwich%20Eikonal%20formulation%2C%20which%0Aemphasizes%20applying%20correct%20and%20dense%20constraints%20on%20both%20sides%20of%20the%20surface%2C%0Athereby%20enhancing%20the%20perceptual%20accuracy%20of%20the%20surface.%20Experiments%20suggest%0Athat%20our%20method%20achieves%20SOTA%20for%20both%20occupancy%20prediction%20and%203D%20scene%0Areconstruction%20tasks%20on%20the%20nuScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14366v1&entry.124074799=Read"},
{"title": "MyVLM: Personalizing VLMs for User-Specific Queries", "author": "Yuval Alaluf and Elad Richardson and Sergey Tulyakov and Kfir Aberman and Daniel Cohen-Or", "abstract": "  Recent large-scale vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding and generating textual descriptions for visual\ncontent. However, these models lack an understanding of user-specific concepts.\nIn this work, we take a first step toward the personalization of VLMs, enabling\nthem to learn and reason over user-provided concepts. For example, we explore\nwhether these models can learn to recognize you in an image and communicate\nwhat you are doing, tailoring the model to reflect your personal experiences\nand relationships. To effectively recognize a variety of user-specific\nconcepts, we augment the VLM with external concept heads that function as\ntoggles for the model, enabling the VLM to identify the presence of specific\ntarget concepts in a given image. Having recognized the concept, we learn a new\nconcept embedding in the intermediate feature space of the VLM. This embedding\nis tasked with guiding the language model to naturally integrate the target\nconcept in its generated response. We apply our technique to BLIP-2 and LLaVA\nfor personalized image captioning and further show its applicability for\npersonalized visual question-answering. Our experiments demonstrate our ability\nto generalize to unseen images of learned concepts while preserving the model\nbehavior on unrelated inputs.\n", "link": "http://arxiv.org/abs/2403.14599v1", "date": "2024-03-21", "relevancy": 2.1825, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5529}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MyVLM%3A%20Personalizing%20VLMs%20for%20User-Specific%20Queries&body=Title%3A%20MyVLM%3A%20Personalizing%20VLMs%20for%20User-Specific%20Queries%0AAuthor%3A%20Yuval%20Alaluf%20and%20Elad%20Richardson%20and%20Sergey%20Tulyakov%20and%20Kfir%20Aberman%20and%20Daniel%20Cohen-Or%0AAbstract%3A%20%20%20Recent%20large-scale%20vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20understanding%20and%20generating%20textual%20descriptions%20for%20visual%0Acontent.%20However%2C%20these%20models%20lack%20an%20understanding%20of%20user-specific%20concepts.%0AIn%20this%20work%2C%20we%20take%20a%20first%20step%20toward%20the%20personalization%20of%20VLMs%2C%20enabling%0Athem%20to%20learn%20and%20reason%20over%20user-provided%20concepts.%20For%20example%2C%20we%20explore%0Awhether%20these%20models%20can%20learn%20to%20recognize%20you%20in%20an%20image%20and%20communicate%0Awhat%20you%20are%20doing%2C%20tailoring%20the%20model%20to%20reflect%20your%20personal%20experiences%0Aand%20relationships.%20To%20effectively%20recognize%20a%20variety%20of%20user-specific%0Aconcepts%2C%20we%20augment%20the%20VLM%20with%20external%20concept%20heads%20that%20function%20as%0Atoggles%20for%20the%20model%2C%20enabling%20the%20VLM%20to%20identify%20the%20presence%20of%20specific%0Atarget%20concepts%20in%20a%20given%20image.%20Having%20recognized%20the%20concept%2C%20we%20learn%20a%20new%0Aconcept%20embedding%20in%20the%20intermediate%20feature%20space%20of%20the%20VLM.%20This%20embedding%0Ais%20tasked%20with%20guiding%20the%20language%20model%20to%20naturally%20integrate%20the%20target%0Aconcept%20in%20its%20generated%20response.%20We%20apply%20our%20technique%20to%20BLIP-2%20and%20LLaVA%0Afor%20personalized%20image%20captioning%20and%20further%20show%20its%20applicability%20for%0Apersonalized%20visual%20question-answering.%20Our%20experiments%20demonstrate%20our%20ability%0Ato%20generalize%20to%20unseen%20images%20of%20learned%20concepts%20while%20preserving%20the%20model%0Abehavior%20on%20unrelated%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14599v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MyVLM%3A%20Personalizing%20VLMs%20for%20User-Specific%20Queries&entry.906535625=Yuval%20Alaluf%20and%20Elad%20Richardson%20and%20Sergey%20Tulyakov%20and%20Kfir%20Aberman%20and%20Daniel%20Cohen-Or&entry.1292438233=%20%20Recent%20large-scale%20vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20understanding%20and%20generating%20textual%20descriptions%20for%20visual%0Acontent.%20However%2C%20these%20models%20lack%20an%20understanding%20of%20user-specific%20concepts.%0AIn%20this%20work%2C%20we%20take%20a%20first%20step%20toward%20the%20personalization%20of%20VLMs%2C%20enabling%0Athem%20to%20learn%20and%20reason%20over%20user-provided%20concepts.%20For%20example%2C%20we%20explore%0Awhether%20these%20models%20can%20learn%20to%20recognize%20you%20in%20an%20image%20and%20communicate%0Awhat%20you%20are%20doing%2C%20tailoring%20the%20model%20to%20reflect%20your%20personal%20experiences%0Aand%20relationships.%20To%20effectively%20recognize%20a%20variety%20of%20user-specific%0Aconcepts%2C%20we%20augment%20the%20VLM%20with%20external%20concept%20heads%20that%20function%20as%0Atoggles%20for%20the%20model%2C%20enabling%20the%20VLM%20to%20identify%20the%20presence%20of%20specific%0Atarget%20concepts%20in%20a%20given%20image.%20Having%20recognized%20the%20concept%2C%20we%20learn%20a%20new%0Aconcept%20embedding%20in%20the%20intermediate%20feature%20space%20of%20the%20VLM.%20This%20embedding%0Ais%20tasked%20with%20guiding%20the%20language%20model%20to%20naturally%20integrate%20the%20target%0Aconcept%20in%20its%20generated%20response.%20We%20apply%20our%20technique%20to%20BLIP-2%20and%20LLaVA%0Afor%20personalized%20image%20captioning%20and%20further%20show%20its%20applicability%20for%0Apersonalized%20visual%20question-answering.%20Our%20experiments%20demonstrate%20our%20ability%0Ato%20generalize%20to%20unseen%20images%20of%20learned%20concepts%20while%20preserving%20the%20model%0Abehavior%20on%20unrelated%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14599v1&entry.124074799=Read"},
{"title": "Annotation-Efficient Polyp Segmentation via Active Learning", "author": "Duojun Huang and Xinyu Xiong and De-Jun Fan and Feng Gao and Xiao-Jian Wu and Guanbin Li", "abstract": "  Deep learning-based techniques have proven effective in polyp segmentation\ntasks when provided with sufficient pixel-wise labeled data. However, the high\ncost of manual annotation has created a bottleneck for model generalization. To\nminimize annotation costs, we propose a deep active learning framework for\nannotation-efficient polyp segmentation. In practice, we measure the\nuncertainty of each sample by examining the similarity between features masked\nby the prediction map of the polyp and the background area. Since the\nsegmentation model tends to perform weak in samples with indistinguishable\nfeatures of foreground and background areas, uncertainty sampling facilitates\nthe fitting of under-learning data. Furthermore, clustering image-level\nfeatures weighted by uncertainty identify samples that are both uncertain and\nrepresentative. To enhance the selectivity of the active selection strategy, we\npropose a novel unsupervised feature discrepancy learning mechanism. The\nselection strategy and feature optimization work in tandem to achieve optimal\nperformance with a limited annotation budget. Extensive experimental results\nhave demonstrated that our proposed method achieved state-of-the-art\nperformance compared to other competitors on both a public dataset and a\nlarge-scale in-house dataset.\n", "link": "http://arxiv.org/abs/2403.14350v1", "date": "2024-03-21", "relevancy": 2.18, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6207}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5374}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5224}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Annotation-Efficient%20Polyp%20Segmentation%20via%20Active%20Learning&body=Title%3A%20Annotation-Efficient%20Polyp%20Segmentation%20via%20Active%20Learning%0AAuthor%3A%20Duojun%20Huang%20and%20Xinyu%20Xiong%20and%20De-Jun%20Fan%20and%20Feng%20Gao%20and%20Xiao-Jian%20Wu%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20Deep%20learning-based%20techniques%20have%20proven%20effective%20in%20polyp%20segmentation%0Atasks%20when%20provided%20with%20sufficient%20pixel-wise%20labeled%20data.%20However%2C%20the%20high%0Acost%20of%20manual%20annotation%20has%20created%20a%20bottleneck%20for%20model%20generalization.%20To%0Aminimize%20annotation%20costs%2C%20we%20propose%20a%20deep%20active%20learning%20framework%20for%0Aannotation-efficient%20polyp%20segmentation.%20In%20practice%2C%20we%20measure%20the%0Auncertainty%20of%20each%20sample%20by%20examining%20the%20similarity%20between%20features%20masked%0Aby%20the%20prediction%20map%20of%20the%20polyp%20and%20the%20background%20area.%20Since%20the%0Asegmentation%20model%20tends%20to%20perform%20weak%20in%20samples%20with%20indistinguishable%0Afeatures%20of%20foreground%20and%20background%20areas%2C%20uncertainty%20sampling%20facilitates%0Athe%20fitting%20of%20under-learning%20data.%20Furthermore%2C%20clustering%20image-level%0Afeatures%20weighted%20by%20uncertainty%20identify%20samples%20that%20are%20both%20uncertain%20and%0Arepresentative.%20To%20enhance%20the%20selectivity%20of%20the%20active%20selection%20strategy%2C%20we%0Apropose%20a%20novel%20unsupervised%20feature%20discrepancy%20learning%20mechanism.%20The%0Aselection%20strategy%20and%20feature%20optimization%20work%20in%20tandem%20to%20achieve%20optimal%0Aperformance%20with%20a%20limited%20annotation%20budget.%20Extensive%20experimental%20results%0Ahave%20demonstrated%20that%20our%20proposed%20method%20achieved%20state-of-the-art%0Aperformance%20compared%20to%20other%20competitors%20on%20both%20a%20public%20dataset%20and%20a%0Alarge-scale%20in-house%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14350v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotation-Efficient%20Polyp%20Segmentation%20via%20Active%20Learning&entry.906535625=Duojun%20Huang%20and%20Xinyu%20Xiong%20and%20De-Jun%20Fan%20and%20Feng%20Gao%20and%20Xiao-Jian%20Wu%20and%20Guanbin%20Li&entry.1292438233=%20%20Deep%20learning-based%20techniques%20have%20proven%20effective%20in%20polyp%20segmentation%0Atasks%20when%20provided%20with%20sufficient%20pixel-wise%20labeled%20data.%20However%2C%20the%20high%0Acost%20of%20manual%20annotation%20has%20created%20a%20bottleneck%20for%20model%20generalization.%20To%0Aminimize%20annotation%20costs%2C%20we%20propose%20a%20deep%20active%20learning%20framework%20for%0Aannotation-efficient%20polyp%20segmentation.%20In%20practice%2C%20we%20measure%20the%0Auncertainty%20of%20each%20sample%20by%20examining%20the%20similarity%20between%20features%20masked%0Aby%20the%20prediction%20map%20of%20the%20polyp%20and%20the%20background%20area.%20Since%20the%0Asegmentation%20model%20tends%20to%20perform%20weak%20in%20samples%20with%20indistinguishable%0Afeatures%20of%20foreground%20and%20background%20areas%2C%20uncertainty%20sampling%20facilitates%0Athe%20fitting%20of%20under-learning%20data.%20Furthermore%2C%20clustering%20image-level%0Afeatures%20weighted%20by%20uncertainty%20identify%20samples%20that%20are%20both%20uncertain%20and%0Arepresentative.%20To%20enhance%20the%20selectivity%20of%20the%20active%20selection%20strategy%2C%20we%0Apropose%20a%20novel%20unsupervised%20feature%20discrepancy%20learning%20mechanism.%20The%0Aselection%20strategy%20and%20feature%20optimization%20work%20in%20tandem%20to%20achieve%20optimal%0Aperformance%20with%20a%20limited%20annotation%20budget.%20Extensive%20experimental%20results%0Ahave%20demonstrated%20that%20our%20proposed%20method%20achieved%20state-of-the-art%0Aperformance%20compared%20to%20other%20competitors%20on%20both%20a%20public%20dataset%20and%20a%0Alarge-scale%20in-house%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14350v1&entry.124074799=Read"},
{"title": "Unsupervised Video Domain Adaptation with Masked Pre-Training and\n  Collaborative Self-Training", "author": "Arun Reddy and William Paul and Corban Rivera and Ketul Shah and Celso M. de Melo and Rama Chellappa", "abstract": "  In this work, we tackle the problem of unsupervised domain adaptation (UDA)\nfor video action recognition. Our approach, which we call UNITE, uses an image\nteacher model to adapt a video student model to the target domain. UNITE first\nemploys self-supervised pre-training to promote discriminative feature learning\non target domain videos using a teacher-guided masked distillation objective.\nWe then perform self-training on masked target data, using the video student\nmodel and image teacher model together to generate improved pseudolabels for\nunlabeled target videos. Our self-training process successfully leverages the\nstrengths of both models to achieve strong transfer performance across domains.\nWe evaluate our approach on multiple video domain adaptation benchmarks and\nobserve significant improvements upon previously reported results.\n", "link": "http://arxiv.org/abs/2312.02914v3", "date": "2024-03-21", "relevancy": 2.1684, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5587}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Video%20Domain%20Adaptation%20with%20Masked%20Pre-Training%20and%0A%20%20Collaborative%20Self-Training&body=Title%3A%20Unsupervised%20Video%20Domain%20Adaptation%20with%20Masked%20Pre-Training%20and%0A%20%20Collaborative%20Self-Training%0AAuthor%3A%20Arun%20Reddy%20and%20William%20Paul%20and%20Corban%20Rivera%20and%20Ketul%20Shah%20and%20Celso%20M.%20de%20Melo%20and%20Rama%20Chellappa%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20tackle%20the%20problem%20of%20unsupervised%20domain%20adaptation%20%28UDA%29%0Afor%20video%20action%20recognition.%20Our%20approach%2C%20which%20we%20call%20UNITE%2C%20uses%20an%20image%0Ateacher%20model%20to%20adapt%20a%20video%20student%20model%20to%20the%20target%20domain.%20UNITE%20first%0Aemploys%20self-supervised%20pre-training%20to%20promote%20discriminative%20feature%20learning%0Aon%20target%20domain%20videos%20using%20a%20teacher-guided%20masked%20distillation%20objective.%0AWe%20then%20perform%20self-training%20on%20masked%20target%20data%2C%20using%20the%20video%20student%0Amodel%20and%20image%20teacher%20model%20together%20to%20generate%20improved%20pseudolabels%20for%0Aunlabeled%20target%20videos.%20Our%20self-training%20process%20successfully%20leverages%20the%0Astrengths%20of%20both%20models%20to%20achieve%20strong%20transfer%20performance%20across%20domains.%0AWe%20evaluate%20our%20approach%20on%20multiple%20video%20domain%20adaptation%20benchmarks%20and%0Aobserve%20significant%20improvements%20upon%20previously%20reported%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02914v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Video%20Domain%20Adaptation%20with%20Masked%20Pre-Training%20and%0A%20%20Collaborative%20Self-Training&entry.906535625=Arun%20Reddy%20and%20William%20Paul%20and%20Corban%20Rivera%20and%20Ketul%20Shah%20and%20Celso%20M.%20de%20Melo%20and%20Rama%20Chellappa&entry.1292438233=%20%20In%20this%20work%2C%20we%20tackle%20the%20problem%20of%20unsupervised%20domain%20adaptation%20%28UDA%29%0Afor%20video%20action%20recognition.%20Our%20approach%2C%20which%20we%20call%20UNITE%2C%20uses%20an%20image%0Ateacher%20model%20to%20adapt%20a%20video%20student%20model%20to%20the%20target%20domain.%20UNITE%20first%0Aemploys%20self-supervised%20pre-training%20to%20promote%20discriminative%20feature%20learning%0Aon%20target%20domain%20videos%20using%20a%20teacher-guided%20masked%20distillation%20objective.%0AWe%20then%20perform%20self-training%20on%20masked%20target%20data%2C%20using%20the%20video%20student%0Amodel%20and%20image%20teacher%20model%20together%20to%20generate%20improved%20pseudolabels%20for%0Aunlabeled%20target%20videos.%20Our%20self-training%20process%20successfully%20leverages%20the%0Astrengths%20of%20both%20models%20to%20achieve%20strong%20transfer%20performance%20across%20domains.%0AWe%20evaluate%20our%20approach%20on%20multiple%20video%20domain%20adaptation%20benchmarks%20and%0Aobserve%20significant%20improvements%20upon%20previously%20reported%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02914v3&entry.124074799=Read"},
{"title": "Learning Hierarchical Control For Constrained Dynamic Task Assignment", "author": "Charlott Vallon and Alessandro Pinto and Bartolomeo Stellato and Francesco Borrelli", "abstract": "  This paper introduces a novel data-driven hierarchical control scheme for\nmanaging a fleet of nonlinear, capacity-constrained autonomous agents in an\niterative environment. We propose a control framework consisting of a\nhigh-level dynamic task assignment and routing layer and low-level motion\nplanning and tracking layer. Each layer of the control hierarchy uses a\ndata-driven MPC policy, maintaining bounded computational complexity at each\ncalculation of a new task assignment or actuation input. We utilize collected\ndata to iteratively refine estimates of agent capacity usage, and update MPC\npolicy parameters accordingly. Our approach leverages tools from iterative\nlearning control to integrate learning at both levels of the hierarchy, and\ncoordinates learning between levels in order to maintain closed-loop\nfeasibility and performance improvement of the connected architecture.\n", "link": "http://arxiv.org/abs/2403.14545v1", "date": "2024-03-21", "relevancy": 2.1631, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6055}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5157}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.486}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Hierarchical%20Control%20For%20Constrained%20Dynamic%20Task%20Assignment&body=Title%3A%20Learning%20Hierarchical%20Control%20For%20Constrained%20Dynamic%20Task%20Assignment%0AAuthor%3A%20Charlott%20Vallon%20and%20Alessandro%20Pinto%20and%20Bartolomeo%20Stellato%20and%20Francesco%20Borrelli%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20data-driven%20hierarchical%20control%20scheme%20for%0Amanaging%20a%20fleet%20of%20nonlinear%2C%20capacity-constrained%20autonomous%20agents%20in%20an%0Aiterative%20environment.%20We%20propose%20a%20control%20framework%20consisting%20of%20a%0Ahigh-level%20dynamic%20task%20assignment%20and%20routing%20layer%20and%20low-level%20motion%0Aplanning%20and%20tracking%20layer.%20Each%20layer%20of%20the%20control%20hierarchy%20uses%20a%0Adata-driven%20MPC%20policy%2C%20maintaining%20bounded%20computational%20complexity%20at%20each%0Acalculation%20of%20a%20new%20task%20assignment%20or%20actuation%20input.%20We%20utilize%20collected%0Adata%20to%20iteratively%20refine%20estimates%20of%20agent%20capacity%20usage%2C%20and%20update%20MPC%0Apolicy%20parameters%20accordingly.%20Our%20approach%20leverages%20tools%20from%20iterative%0Alearning%20control%20to%20integrate%20learning%20at%20both%20levels%20of%20the%20hierarchy%2C%20and%0Acoordinates%20learning%20between%20levels%20in%20order%20to%20maintain%20closed-loop%0Afeasibility%20and%20performance%20improvement%20of%20the%20connected%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14545v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Hierarchical%20Control%20For%20Constrained%20Dynamic%20Task%20Assignment&entry.906535625=Charlott%20Vallon%20and%20Alessandro%20Pinto%20and%20Bartolomeo%20Stellato%20and%20Francesco%20Borrelli&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20data-driven%20hierarchical%20control%20scheme%20for%0Amanaging%20a%20fleet%20of%20nonlinear%2C%20capacity-constrained%20autonomous%20agents%20in%20an%0Aiterative%20environment.%20We%20propose%20a%20control%20framework%20consisting%20of%20a%0Ahigh-level%20dynamic%20task%20assignment%20and%20routing%20layer%20and%20low-level%20motion%0Aplanning%20and%20tracking%20layer.%20Each%20layer%20of%20the%20control%20hierarchy%20uses%20a%0Adata-driven%20MPC%20policy%2C%20maintaining%20bounded%20computational%20complexity%20at%20each%0Acalculation%20of%20a%20new%20task%20assignment%20or%20actuation%20input.%20We%20utilize%20collected%0Adata%20to%20iteratively%20refine%20estimates%20of%20agent%20capacity%20usage%2C%20and%20update%20MPC%0Apolicy%20parameters%20accordingly.%20Our%20approach%20leverages%20tools%20from%20iterative%0Alearning%20control%20to%20integrate%20learning%20at%20both%20levels%20of%20the%20hierarchy%2C%20and%0Acoordinates%20learning%20between%20levels%20in%20order%20to%20maintain%20closed-loop%0Afeasibility%20and%20performance%20improvement%20of%20the%20connected%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14545v1&entry.124074799=Read"},
{"title": "Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering", "author": "Yihao Huang and Felix Juefei-Xu and Qing Guo and Yang Liu and Geguang Pu", "abstract": "  The current high-fidelity generation and high-precision detection of DeepFake\nimages are at an arms race. We believe that producing DeepFakes that are highly\nrealistic and 'detection evasive' can serve the ultimate goal of improving\nfuture generation DeepFake detection capabilities. In this paper, we propose a\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\nwithout hurting image quality by performing implicit spatial-domain notch\nfiltering. We first demonstrate that frequency-domain notch filtering, although\nfamously shown to be effective in removing periodic noise in the spatial\ndomain, is infeasible for our task at hand due to the manual designs required\nfor the notch filters. We, therefore, resort to a learning-based approach to\nreproduce the notch filtering effects, but solely in the spatial domain. We\nadopt a combination of adding overwhelming spatial noise for breaking the\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\nfake images, and we name our method DeepNotch. Deep image filtering provides a\nspecialized filter for each pixel in the noisy image, producing filtered images\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\nuse the semantic information of the image to generate an adversarial guidance\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\nhas demonstrated that our technique significantly reduces the accuracy of these\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\ncase.\n", "link": "http://arxiv.org/abs/2009.09213v6", "date": "2024-03-21", "relevancy": 2.1621, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5662}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5322}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4971}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dodging%20DeepFake%20Detection%20via%20Implicit%20Spatial-Domain%20Notch%20Filtering&body=Title%3A%20Dodging%20DeepFake%20Detection%20via%20Implicit%20Spatial-Domain%20Notch%20Filtering%0AAuthor%3A%20Yihao%20Huang%20and%20Felix%20Juefei-Xu%20and%20Qing%20Guo%20and%20Yang%20Liu%20and%20Geguang%20Pu%0AAbstract%3A%20%20%20The%20current%20high-fidelity%20generation%20and%20high-precision%20detection%20of%20DeepFake%0Aimages%20are%20at%20an%20arms%20race.%20We%20believe%20that%20producing%20DeepFakes%20that%20are%20highly%0Arealistic%20and%20%27detection%20evasive%27%20can%20serve%20the%20ultimate%20goal%20of%20improving%0Afuture%20generation%20DeepFake%20detection%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%0Asimple%20yet%20powerful%20pipeline%20to%20reduce%20the%20artifact%20patterns%20of%20fake%20images%0Awithout%20hurting%20image%20quality%20by%20performing%20implicit%20spatial-domain%20notch%0Afiltering.%20We%20first%20demonstrate%20that%20frequency-domain%20notch%20filtering%2C%20although%0Afamously%20shown%20to%20be%20effective%20in%20removing%20periodic%20noise%20in%20the%20spatial%0Adomain%2C%20is%20infeasible%20for%20our%20task%20at%20hand%20due%20to%20the%20manual%20designs%20required%0Afor%20the%20notch%20filters.%20We%2C%20therefore%2C%20resort%20to%20a%20learning-based%20approach%20to%0Areproduce%20the%20notch%20filtering%20effects%2C%20but%20solely%20in%20the%20spatial%20domain.%20We%0Aadopt%20a%20combination%20of%20adding%20overwhelming%20spatial%20noise%20for%20breaking%20the%0Aperiodic%20noise%20pattern%20and%20deep%20image%20filtering%20to%20reconstruct%20the%20noise-free%0Afake%20images%2C%20and%20we%20name%20our%20method%20DeepNotch.%20Deep%20image%20filtering%20provides%20a%0Aspecialized%20filter%20for%20each%20pixel%20in%20the%20noisy%20image%2C%20producing%20filtered%20images%0Awith%20high%20fidelity%20compared%20to%20their%20DeepFake%20counterparts.%20Moreover%2C%20we%20also%0Ause%20the%20semantic%20information%20of%20the%20image%20to%20generate%20an%20adversarial%20guidance%0Amap%20to%20add%20noise%20intelligently.%20Our%20large-scale%20evaluation%20on%203%20representative%0Astate-of-the-art%20DeepFake%20detection%20methods%20%28tested%20on%2016%20types%20of%20DeepFakes%29%0Ahas%20demonstrated%20that%20our%20technique%20significantly%20reduces%20the%20accuracy%20of%20these%0A3%20fake%20image%20detection%20methods%2C%2036.79%25%20on%20average%20and%20up%20to%2097.02%25%20in%20the%20best%0Acase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2009.09213v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dodging%20DeepFake%20Detection%20via%20Implicit%20Spatial-Domain%20Notch%20Filtering&entry.906535625=Yihao%20Huang%20and%20Felix%20Juefei-Xu%20and%20Qing%20Guo%20and%20Yang%20Liu%20and%20Geguang%20Pu&entry.1292438233=%20%20The%20current%20high-fidelity%20generation%20and%20high-precision%20detection%20of%20DeepFake%0Aimages%20are%20at%20an%20arms%20race.%20We%20believe%20that%20producing%20DeepFakes%20that%20are%20highly%0Arealistic%20and%20%27detection%20evasive%27%20can%20serve%20the%20ultimate%20goal%20of%20improving%0Afuture%20generation%20DeepFake%20detection%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%0Asimple%20yet%20powerful%20pipeline%20to%20reduce%20the%20artifact%20patterns%20of%20fake%20images%0Awithout%20hurting%20image%20quality%20by%20performing%20implicit%20spatial-domain%20notch%0Afiltering.%20We%20first%20demonstrate%20that%20frequency-domain%20notch%20filtering%2C%20although%0Afamously%20shown%20to%20be%20effective%20in%20removing%20periodic%20noise%20in%20the%20spatial%0Adomain%2C%20is%20infeasible%20for%20our%20task%20at%20hand%20due%20to%20the%20manual%20designs%20required%0Afor%20the%20notch%20filters.%20We%2C%20therefore%2C%20resort%20to%20a%20learning-based%20approach%20to%0Areproduce%20the%20notch%20filtering%20effects%2C%20but%20solely%20in%20the%20spatial%20domain.%20We%0Aadopt%20a%20combination%20of%20adding%20overwhelming%20spatial%20noise%20for%20breaking%20the%0Aperiodic%20noise%20pattern%20and%20deep%20image%20filtering%20to%20reconstruct%20the%20noise-free%0Afake%20images%2C%20and%20we%20name%20our%20method%20DeepNotch.%20Deep%20image%20filtering%20provides%20a%0Aspecialized%20filter%20for%20each%20pixel%20in%20the%20noisy%20image%2C%20producing%20filtered%20images%0Awith%20high%20fidelity%20compared%20to%20their%20DeepFake%20counterparts.%20Moreover%2C%20we%20also%0Ause%20the%20semantic%20information%20of%20the%20image%20to%20generate%20an%20adversarial%20guidance%0Amap%20to%20add%20noise%20intelligently.%20Our%20large-scale%20evaluation%20on%203%20representative%0Astate-of-the-art%20DeepFake%20detection%20methods%20%28tested%20on%2016%20types%20of%20DeepFakes%29%0Ahas%20demonstrated%20that%20our%20technique%20significantly%20reduces%20the%20accuracy%20of%20these%0A3%20fake%20image%20detection%20methods%2C%2036.79%25%20on%20average%20and%20up%20to%2097.02%25%20in%20the%20best%0Acase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2009.09213v6&entry.124074799=Read"},
{"title": "DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video\n  Analytics", "author": "Yoonsung Kim and Changhun Oh and Jinwoo Hwang and Wonung Kim and Seongryong Oh and Yubin Lee and Hardik Sharma and Amir Yazdanbakhsh and Jongse Park", "abstract": "  Deep neural network (DNN) video analytics is crucial for autonomous systems\nsuch as self-driving vehicles, unmanned aerial vehicles (UAVs), and security\nrobots. However, real-world deployment faces challenges due to their limited\ncomputational resources and battery power. To tackle these challenges,\ncontinuous learning exploits a lightweight \"student\" model at deployment\n(inference), leverages a larger \"teacher\" model for labeling sampled data\n(labeling), and continuously retrains the student model to adapt to changing\nscenarios (retraining). This paper highlights the limitations in\nstate-of-the-art continuous learning systems: (1) they focus on computations\nfor retraining, while overlooking the compute needs for inference and labeling,\n(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous\nsystems, and (3) they are located on a remote centralized server, intended for\nmulti-tenant scenarios, again unsuitable for autonomous systems due to privacy,\nnetwork availability, and latency concerns. We propose a hardware-algorithm\nco-designed solution for continuous learning, DaCapo, that enables autonomous\nsystems to perform concurrent executions of inference, labeling, and training\nin a performant and energy-efficient manner. DaCapo comprises (1) a\nspatially-partitionable and precision-flexible accelerator enabling parallel\nexecution of kernels on sub-accelerators at their respective precisions, and\n(2) a spatiotemporal resource allocation algorithm that strategically navigates\nthe resource-accuracy tradeoff space, facilitating optimal decisions for\nresource allocation to achieve maximal accuracy. Our evaluation shows that\nDaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based\ncontinuous learning systems, Ekya and EOMU, respectively, while consuming 254x\nless power.\n", "link": "http://arxiv.org/abs/2403.14353v1", "date": "2024-03-21", "relevancy": 2.1618, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5492}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5431}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5307}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DaCapo%3A%20Accelerating%20Continuous%20Learning%20in%20Autonomous%20Systems%20for%20Video%0A%20%20Analytics&body=Title%3A%20DaCapo%3A%20Accelerating%20Continuous%20Learning%20in%20Autonomous%20Systems%20for%20Video%0A%20%20Analytics%0AAuthor%3A%20Yoonsung%20Kim%20and%20Changhun%20Oh%20and%20Jinwoo%20Hwang%20and%20Wonung%20Kim%20and%20Seongryong%20Oh%20and%20Yubin%20Lee%20and%20Hardik%20Sharma%20and%20Amir%20Yazdanbakhsh%20and%20Jongse%20Park%0AAbstract%3A%20%20%20Deep%20neural%20network%20%28DNN%29%20video%20analytics%20is%20crucial%20for%20autonomous%20systems%0Asuch%20as%20self-driving%20vehicles%2C%20unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20and%20security%0Arobots.%20However%2C%20real-world%20deployment%20faces%20challenges%20due%20to%20their%20limited%0Acomputational%20resources%20and%20battery%20power.%20To%20tackle%20these%20challenges%2C%0Acontinuous%20learning%20exploits%20a%20lightweight%20%22student%22%20model%20at%20deployment%0A%28inference%29%2C%20leverages%20a%20larger%20%22teacher%22%20model%20for%20labeling%20sampled%20data%0A%28labeling%29%2C%20and%20continuously%20retrains%20the%20student%20model%20to%20adapt%20to%20changing%0Ascenarios%20%28retraining%29.%20This%20paper%20highlights%20the%20limitations%20in%0Astate-of-the-art%20continuous%20learning%20systems%3A%20%281%29%20they%20focus%20on%20computations%0Afor%20retraining%2C%20while%20overlooking%20the%20compute%20needs%20for%20inference%20and%20labeling%2C%0A%282%29%20they%20rely%20on%20power-hungry%20GPUs%2C%20unsuitable%20for%20battery-operated%20autonomous%0Asystems%2C%20and%20%283%29%20they%20are%20located%20on%20a%20remote%20centralized%20server%2C%20intended%20for%0Amulti-tenant%20scenarios%2C%20again%20unsuitable%20for%20autonomous%20systems%20due%20to%20privacy%2C%0Anetwork%20availability%2C%20and%20latency%20concerns.%20We%20propose%20a%20hardware-algorithm%0Aco-designed%20solution%20for%20continuous%20learning%2C%20DaCapo%2C%20that%20enables%20autonomous%0Asystems%20to%20perform%20concurrent%20executions%20of%20inference%2C%20labeling%2C%20and%20training%0Ain%20a%20performant%20and%20energy-efficient%20manner.%20DaCapo%20comprises%20%281%29%20a%0Aspatially-partitionable%20and%20precision-flexible%20accelerator%20enabling%20parallel%0Aexecution%20of%20kernels%20on%20sub-accelerators%20at%20their%20respective%20precisions%2C%20and%0A%282%29%20a%20spatiotemporal%20resource%20allocation%20algorithm%20that%20strategically%20navigates%0Athe%20resource-accuracy%20tradeoff%20space%2C%20facilitating%20optimal%20decisions%20for%0Aresource%20allocation%20to%20achieve%20maximal%20accuracy.%20Our%20evaluation%20shows%20that%0ADaCapo%20achieves%206.5%25%20and%205.5%25%20higher%20accuracy%20than%20a%20state-of-the-art%20GPU-based%0Acontinuous%20learning%20systems%2C%20Ekya%20and%20EOMU%2C%20respectively%2C%20while%20consuming%20254x%0Aless%20power.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14353v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DaCapo%3A%20Accelerating%20Continuous%20Learning%20in%20Autonomous%20Systems%20for%20Video%0A%20%20Analytics&entry.906535625=Yoonsung%20Kim%20and%20Changhun%20Oh%20and%20Jinwoo%20Hwang%20and%20Wonung%20Kim%20and%20Seongryong%20Oh%20and%20Yubin%20Lee%20and%20Hardik%20Sharma%20and%20Amir%20Yazdanbakhsh%20and%20Jongse%20Park&entry.1292438233=%20%20Deep%20neural%20network%20%28DNN%29%20video%20analytics%20is%20crucial%20for%20autonomous%20systems%0Asuch%20as%20self-driving%20vehicles%2C%20unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20and%20security%0Arobots.%20However%2C%20real-world%20deployment%20faces%20challenges%20due%20to%20their%20limited%0Acomputational%20resources%20and%20battery%20power.%20To%20tackle%20these%20challenges%2C%0Acontinuous%20learning%20exploits%20a%20lightweight%20%22student%22%20model%20at%20deployment%0A%28inference%29%2C%20leverages%20a%20larger%20%22teacher%22%20model%20for%20labeling%20sampled%20data%0A%28labeling%29%2C%20and%20continuously%20retrains%20the%20student%20model%20to%20adapt%20to%20changing%0Ascenarios%20%28retraining%29.%20This%20paper%20highlights%20the%20limitations%20in%0Astate-of-the-art%20continuous%20learning%20systems%3A%20%281%29%20they%20focus%20on%20computations%0Afor%20retraining%2C%20while%20overlooking%20the%20compute%20needs%20for%20inference%20and%20labeling%2C%0A%282%29%20they%20rely%20on%20power-hungry%20GPUs%2C%20unsuitable%20for%20battery-operated%20autonomous%0Asystems%2C%20and%20%283%29%20they%20are%20located%20on%20a%20remote%20centralized%20server%2C%20intended%20for%0Amulti-tenant%20scenarios%2C%20again%20unsuitable%20for%20autonomous%20systems%20due%20to%20privacy%2C%0Anetwork%20availability%2C%20and%20latency%20concerns.%20We%20propose%20a%20hardware-algorithm%0Aco-designed%20solution%20for%20continuous%20learning%2C%20DaCapo%2C%20that%20enables%20autonomous%0Asystems%20to%20perform%20concurrent%20executions%20of%20inference%2C%20labeling%2C%20and%20training%0Ain%20a%20performant%20and%20energy-efficient%20manner.%20DaCapo%20comprises%20%281%29%20a%0Aspatially-partitionable%20and%20precision-flexible%20accelerator%20enabling%20parallel%0Aexecution%20of%20kernels%20on%20sub-accelerators%20at%20their%20respective%20precisions%2C%20and%0A%282%29%20a%20spatiotemporal%20resource%20allocation%20algorithm%20that%20strategically%20navigates%0Athe%20resource-accuracy%20tradeoff%20space%2C%20facilitating%20optimal%20decisions%20for%0Aresource%20allocation%20to%20achieve%20maximal%20accuracy.%20Our%20evaluation%20shows%20that%0ADaCapo%20achieves%206.5%25%20and%205.5%25%20higher%20accuracy%20than%20a%20state-of-the-art%20GPU-based%0Acontinuous%20learning%20systems%2C%20Ekya%20and%20EOMU%2C%20respectively%2C%20while%20consuming%20254x%0Aless%20power.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14353v1&entry.124074799=Read"},
{"title": "T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy", "author": "Qing Jiang and Feng Li and Zhaoyang Zeng and Tianhe Ren and Shilong Liu and Lei Zhang", "abstract": "  We present T-Rex2, a highly practical model for open-set object detection.\nPrevious open-set object detection methods relying on text prompts effectively\nencapsulate the abstract concept of common objects, but struggle with rare or\ncomplex object representation due to data scarcity and descriptive limitations.\nConversely, visual prompts excel in depicting novel objects through concrete\nvisual examples, but fall short in conveying the abstract concept of objects as\neffectively as text prompts. Recognizing the complementary strengths and\nweaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes\nboth prompts within a single model through contrastive learning. T-Rex2 accepts\ninputs in diverse formats, including text prompts, visual prompts, and the\ncombination of both, so that it can handle different scenarios by switching\nbetween the two prompt modalities. Comprehensive experiments demonstrate that\nT-Rex2 exhibits remarkable zero-shot object detection capabilities across a\nwide spectrum of scenarios. We show that text prompts and visual prompts can\nbenefit from each other within the synergy, which is essential to cover massive\nand complicated real-world scenarios and pave the way towards generic object\ndetection. Model API is now available at\n\\url{https://github.com/IDEA-Research/T-Rex}.\n", "link": "http://arxiv.org/abs/2403.14610v1", "date": "2024-03-21", "relevancy": 2.1542, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5607}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5244}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5186}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20T-Rex2%3A%20Towards%20Generic%20Object%20Detection%20via%20Text-Visual%20Prompt%20Synergy&body=Title%3A%20T-Rex2%3A%20Towards%20Generic%20Object%20Detection%20via%20Text-Visual%20Prompt%20Synergy%0AAuthor%3A%20Qing%20Jiang%20and%20Feng%20Li%20and%20Zhaoyang%20Zeng%20and%20Tianhe%20Ren%20and%20Shilong%20Liu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20We%20present%20T-Rex2%2C%20a%20highly%20practical%20model%20for%20open-set%20object%20detection.%0APrevious%20open-set%20object%20detection%20methods%20relying%20on%20text%20prompts%20effectively%0Aencapsulate%20the%20abstract%20concept%20of%20common%20objects%2C%20but%20struggle%20with%20rare%20or%0Acomplex%20object%20representation%20due%20to%20data%20scarcity%20and%20descriptive%20limitations.%0AConversely%2C%20visual%20prompts%20excel%20in%20depicting%20novel%20objects%20through%20concrete%0Avisual%20examples%2C%20but%20fall%20short%20in%20conveying%20the%20abstract%20concept%20of%20objects%20as%0Aeffectively%20as%20text%20prompts.%20Recognizing%20the%20complementary%20strengths%20and%0Aweaknesses%20of%20both%20text%20and%20visual%20prompts%2C%20we%20introduce%20T-Rex2%20that%20synergizes%0Aboth%20prompts%20within%20a%20single%20model%20through%20contrastive%20learning.%20T-Rex2%20accepts%0Ainputs%20in%20diverse%20formats%2C%20including%20text%20prompts%2C%20visual%20prompts%2C%20and%20the%0Acombination%20of%20both%2C%20so%20that%20it%20can%20handle%20different%20scenarios%20by%20switching%0Abetween%20the%20two%20prompt%20modalities.%20Comprehensive%20experiments%20demonstrate%20that%0AT-Rex2%20exhibits%20remarkable%20zero-shot%20object%20detection%20capabilities%20across%20a%0Awide%20spectrum%20of%20scenarios.%20We%20show%20that%20text%20prompts%20and%20visual%20prompts%20can%0Abenefit%20from%20each%20other%20within%20the%20synergy%2C%20which%20is%20essential%20to%20cover%20massive%0Aand%20complicated%20real-world%20scenarios%20and%20pave%20the%20way%20towards%20generic%20object%0Adetection.%20Model%20API%20is%20now%20available%20at%0A%5Curl%7Bhttps%3A//github.com/IDEA-Research/T-Rex%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14610v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-Rex2%3A%20Towards%20Generic%20Object%20Detection%20via%20Text-Visual%20Prompt%20Synergy&entry.906535625=Qing%20Jiang%20and%20Feng%20Li%20and%20Zhaoyang%20Zeng%20and%20Tianhe%20Ren%20and%20Shilong%20Liu%20and%20Lei%20Zhang&entry.1292438233=%20%20We%20present%20T-Rex2%2C%20a%20highly%20practical%20model%20for%20open-set%20object%20detection.%0APrevious%20open-set%20object%20detection%20methods%20relying%20on%20text%20prompts%20effectively%0Aencapsulate%20the%20abstract%20concept%20of%20common%20objects%2C%20but%20struggle%20with%20rare%20or%0Acomplex%20object%20representation%20due%20to%20data%20scarcity%20and%20descriptive%20limitations.%0AConversely%2C%20visual%20prompts%20excel%20in%20depicting%20novel%20objects%20through%20concrete%0Avisual%20examples%2C%20but%20fall%20short%20in%20conveying%20the%20abstract%20concept%20of%20objects%20as%0Aeffectively%20as%20text%20prompts.%20Recognizing%20the%20complementary%20strengths%20and%0Aweaknesses%20of%20both%20text%20and%20visual%20prompts%2C%20we%20introduce%20T-Rex2%20that%20synergizes%0Aboth%20prompts%20within%20a%20single%20model%20through%20contrastive%20learning.%20T-Rex2%20accepts%0Ainputs%20in%20diverse%20formats%2C%20including%20text%20prompts%2C%20visual%20prompts%2C%20and%20the%0Acombination%20of%20both%2C%20so%20that%20it%20can%20handle%20different%20scenarios%20by%20switching%0Abetween%20the%20two%20prompt%20modalities.%20Comprehensive%20experiments%20demonstrate%20that%0AT-Rex2%20exhibits%20remarkable%20zero-shot%20object%20detection%20capabilities%20across%20a%0Awide%20spectrum%20of%20scenarios.%20We%20show%20that%20text%20prompts%20and%20visual%20prompts%20can%0Abenefit%20from%20each%20other%20within%20the%20synergy%2C%20which%20is%20essential%20to%20cover%20massive%0Aand%20complicated%20real-world%20scenarios%20and%20pave%20the%20way%20towards%20generic%20object%0Adetection.%20Model%20API%20is%20now%20available%20at%0A%5Curl%7Bhttps%3A//github.com/IDEA-Research/T-Rex%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14610v1&entry.124074799=Read"},
{"title": "Intrinsic Image Diffusion for Indoor Single-view Material Estimation", "author": "Peter Kocsis and Vincent Sitzmann and Matthias Nie\u00dfner", "abstract": "  We present Intrinsic Image Diffusion, a generative model for appearance\ndecomposition of indoor scenes. Given a single input view, we sample multiple\npossible material explanations represented as albedo, roughness, and metallic\nmaps. Appearance decomposition poses a considerable challenge in computer\nvision due to the inherent ambiguity between lighting and material properties\nand the lack of real datasets. To address this issue, we advocate for a\nprobabilistic formulation, where instead of attempting to directly predict the\ntrue material properties, we employ a conditional generative model to sample\nfrom the solution space. Furthermore, we show that utilizing the strong learned\nprior of recent diffusion models trained on large-scale real-world images can\nbe adapted to material estimation and highly improves the generalization to\nreal images. Our method produces significantly sharper, more consistent, and\nmore detailed materials, outperforming state-of-the-art methods by $1.5dB$ on\nPSNR and by $45\\%$ better FID score on albedo prediction. We demonstrate the\neffectiveness of our approach through experiments on both synthetic and\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2312.12274v2", "date": "2024-03-21", "relevancy": 2.153, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5438}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5411}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5316}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Image%20Diffusion%20for%20Indoor%20Single-view%20Material%20Estimation&body=Title%3A%20Intrinsic%20Image%20Diffusion%20for%20Indoor%20Single-view%20Material%20Estimation%0AAuthor%3A%20Peter%20Kocsis%20and%20Vincent%20Sitzmann%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20We%20present%20Intrinsic%20Image%20Diffusion%2C%20a%20generative%20model%20for%20appearance%0Adecomposition%20of%20indoor%20scenes.%20Given%20a%20single%20input%20view%2C%20we%20sample%20multiple%0Apossible%20material%20explanations%20represented%20as%20albedo%2C%20roughness%2C%20and%20metallic%0Amaps.%20Appearance%20decomposition%20poses%20a%20considerable%20challenge%20in%20computer%0Avision%20due%20to%20the%20inherent%20ambiguity%20between%20lighting%20and%20material%20properties%0Aand%20the%20lack%20of%20real%20datasets.%20To%20address%20this%20issue%2C%20we%20advocate%20for%20a%0Aprobabilistic%20formulation%2C%20where%20instead%20of%20attempting%20to%20directly%20predict%20the%0Atrue%20material%20properties%2C%20we%20employ%20a%20conditional%20generative%20model%20to%20sample%0Afrom%20the%20solution%20space.%20Furthermore%2C%20we%20show%20that%20utilizing%20the%20strong%20learned%0Aprior%20of%20recent%20diffusion%20models%20trained%20on%20large-scale%20real-world%20images%20can%0Abe%20adapted%20to%20material%20estimation%20and%20highly%20improves%20the%20generalization%20to%0Areal%20images.%20Our%20method%20produces%20significantly%20sharper%2C%20more%20consistent%2C%20and%0Amore%20detailed%20materials%2C%20outperforming%20state-of-the-art%20methods%20by%20%241.5dB%24%20on%0APSNR%20and%20by%20%2445%5C%25%24%20better%20FID%20score%20on%20albedo%20prediction.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20through%20experiments%20on%20both%20synthetic%20and%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12274v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Image%20Diffusion%20for%20Indoor%20Single-view%20Material%20Estimation&entry.906535625=Peter%20Kocsis%20and%20Vincent%20Sitzmann%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20We%20present%20Intrinsic%20Image%20Diffusion%2C%20a%20generative%20model%20for%20appearance%0Adecomposition%20of%20indoor%20scenes.%20Given%20a%20single%20input%20view%2C%20we%20sample%20multiple%0Apossible%20material%20explanations%20represented%20as%20albedo%2C%20roughness%2C%20and%20metallic%0Amaps.%20Appearance%20decomposition%20poses%20a%20considerable%20challenge%20in%20computer%0Avision%20due%20to%20the%20inherent%20ambiguity%20between%20lighting%20and%20material%20properties%0Aand%20the%20lack%20of%20real%20datasets.%20To%20address%20this%20issue%2C%20we%20advocate%20for%20a%0Aprobabilistic%20formulation%2C%20where%20instead%20of%20attempting%20to%20directly%20predict%20the%0Atrue%20material%20properties%2C%20we%20employ%20a%20conditional%20generative%20model%20to%20sample%0Afrom%20the%20solution%20space.%20Furthermore%2C%20we%20show%20that%20utilizing%20the%20strong%20learned%0Aprior%20of%20recent%20diffusion%20models%20trained%20on%20large-scale%20real-world%20images%20can%0Abe%20adapted%20to%20material%20estimation%20and%20highly%20improves%20the%20generalization%20to%0Areal%20images.%20Our%20method%20produces%20significantly%20sharper%2C%20more%20consistent%2C%20and%0Amore%20detailed%20materials%2C%20outperforming%20state-of-the-art%20methods%20by%20%241.5dB%24%20on%0APSNR%20and%20by%20%2445%5C%25%24%20better%20FID%20score%20on%20albedo%20prediction.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20through%20experiments%20on%20both%20synthetic%20and%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12274v2&entry.124074799=Read"},
{"title": "Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics", "author": "Jiaqi Yue and Jiancheng Zhao and Chunhui Zhao", "abstract": "  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.\n", "link": "http://arxiv.org/abs/2403.14362v1", "date": "2024-03-21", "relevancy": 2.1475, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5364}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5198}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Less%20but%20Better%3A%20Enabling%20Generalized%20Zero-shot%20Learning%20Towards%20Unseen%0A%20%20Domains%20by%20Intrinsic%20Learning%20from%20Redundant%20LLM%20Semantics&body=Title%3A%20Less%20but%20Better%3A%20Enabling%20Generalized%20Zero-shot%20Learning%20Towards%20Unseen%0A%20%20Domains%20by%20Intrinsic%20Learning%20from%20Redundant%20LLM%20Semantics%0AAuthor%3A%20Jiaqi%20Yue%20and%20Jiancheng%20Zhao%20and%20Chunhui%20Zhao%0AAbstract%3A%20%20%20Generalized%20zero-shot%20learning%20%28GZSL%29%20focuses%20on%20recognizing%20seen%20and%20unseen%0Aclasses%20against%20domain%20shift%20problem%20%28DSP%29%20where%20data%20of%20unseen%20classes%20may%20be%0Amisclassified%20as%20seen%20classes.%20However%2C%20existing%20GZSL%20is%20still%20limited%20to%20seen%0Adomains.%20In%20the%20current%20work%2C%20we%20pioneer%20cross-domain%20GZSL%20%28CDGZSL%29%20which%0Aaddresses%20GZSL%20towards%20unseen%20domains.%20Different%20from%20existing%20GZSL%20methods%0Awhich%20alleviate%20DSP%20by%20generating%20features%20of%20unseen%20classes%20with%20semantics%2C%0ACDGZSL%20needs%20to%20construct%20a%20common%20feature%20space%20across%20domains%20and%20acquire%20the%0Acorresponding%20intrinsic%20semantics%20shared%20among%20domains%20to%20transfer%20from%20seen%20to%0Aunseen%20domains.%20Considering%20the%20information%20asymmetry%20problem%20caused%20by%0Aredundant%20class%20semantics%20annotated%20with%20large%20language%20models%20%28LLMs%29%2C%20we%0Apresent%20Meta%20Domain%20Alignment%20Semantic%20Refinement%20%28MDASR%29.%20Technically%2C%20MDASR%0Aconsists%20of%20two%20parts%3A%20Inter-class%20Similarity%20Alignment%20%28ISA%29%2C%20which%20eliminates%0Athe%20non-intrinsic%20semantics%20not%20shared%20across%20all%20domains%20under%20the%20guidance%20of%0Ainter-class%20feature%20relationships%2C%20and%20Unseen-class%20Meta%20Generation%20%28UMG%29%2C%0Awhich%20preserves%20intrinsic%20semantics%20to%20maintain%20connectivity%20between%20seen%20and%0Aunseen%20classes%20by%20simulating%20feature%20generation.%20MDASR%20effectively%20aligns%20the%0Aredundant%20semantic%20space%20with%20the%20common%20feature%20space%2C%20mitigating%20the%0Ainformation%20asymmetry%20in%20CDGZSL.%20The%20effectiveness%20of%20MDASR%20is%20demonstrated%20on%0Athe%20Office-Home%20and%20Mini-DomainNet%2C%20and%20we%20have%20shared%20the%20LLM-based%20semantics%0Afor%20these%20datasets%20as%20the%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14362v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20but%20Better%3A%20Enabling%20Generalized%20Zero-shot%20Learning%20Towards%20Unseen%0A%20%20Domains%20by%20Intrinsic%20Learning%20from%20Redundant%20LLM%20Semantics&entry.906535625=Jiaqi%20Yue%20and%20Jiancheng%20Zhao%20and%20Chunhui%20Zhao&entry.1292438233=%20%20Generalized%20zero-shot%20learning%20%28GZSL%29%20focuses%20on%20recognizing%20seen%20and%20unseen%0Aclasses%20against%20domain%20shift%20problem%20%28DSP%29%20where%20data%20of%20unseen%20classes%20may%20be%0Amisclassified%20as%20seen%20classes.%20However%2C%20existing%20GZSL%20is%20still%20limited%20to%20seen%0Adomains.%20In%20the%20current%20work%2C%20we%20pioneer%20cross-domain%20GZSL%20%28CDGZSL%29%20which%0Aaddresses%20GZSL%20towards%20unseen%20domains.%20Different%20from%20existing%20GZSL%20methods%0Awhich%20alleviate%20DSP%20by%20generating%20features%20of%20unseen%20classes%20with%20semantics%2C%0ACDGZSL%20needs%20to%20construct%20a%20common%20feature%20space%20across%20domains%20and%20acquire%20the%0Acorresponding%20intrinsic%20semantics%20shared%20among%20domains%20to%20transfer%20from%20seen%20to%0Aunseen%20domains.%20Considering%20the%20information%20asymmetry%20problem%20caused%20by%0Aredundant%20class%20semantics%20annotated%20with%20large%20language%20models%20%28LLMs%29%2C%20we%0Apresent%20Meta%20Domain%20Alignment%20Semantic%20Refinement%20%28MDASR%29.%20Technically%2C%20MDASR%0Aconsists%20of%20two%20parts%3A%20Inter-class%20Similarity%20Alignment%20%28ISA%29%2C%20which%20eliminates%0Athe%20non-intrinsic%20semantics%20not%20shared%20across%20all%20domains%20under%20the%20guidance%20of%0Ainter-class%20feature%20relationships%2C%20and%20Unseen-class%20Meta%20Generation%20%28UMG%29%2C%0Awhich%20preserves%20intrinsic%20semantics%20to%20maintain%20connectivity%20between%20seen%20and%0Aunseen%20classes%20by%20simulating%20feature%20generation.%20MDASR%20effectively%20aligns%20the%0Aredundant%20semantic%20space%20with%20the%20common%20feature%20space%2C%20mitigating%20the%0Ainformation%20asymmetry%20in%20CDGZSL.%20The%20effectiveness%20of%20MDASR%20is%20demonstrated%20on%0Athe%20Office-Home%20and%20Mini-DomainNet%2C%20and%20we%20have%20shared%20the%20LLM-based%20semantics%0Afor%20these%20datasets%20as%20the%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14362v1&entry.124074799=Read"},
{"title": "ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for\n  Contrastive Self-Training", "author": "Zonghan Yang and Peng Li and Ming Yan and Ji Zhang and Fei Huang and Yang Liu", "abstract": "  Language agents have demonstrated autonomous decision-making abilities by\nreasoning with foundation models. Recently, efforts have been made to train\nlanguage agents for performance improvement, with multi-step reasoning and\naction trajectories as the training data. However, collecting such trajectories\nstill requires considerable human effort, by either artificial annotations or\nimplementations of diverse prompting frameworks. In this work, we propose\nA$^3$T, a framework that enables the Autonomous Annotation of Agent\nTrajectories in the style of ReAct. The central role is an ActRe prompting\nagent, which explains the reason for an arbitrary action. When randomly\nsampling an external action, the ReAct-style agent could query the ActRe agent\nwith the action to obtain its textual rationales. Novel trajectories are then\nsynthesized by prepending the posterior reasoning from ActRe to the sampled\naction. In this way, the ReAct-style agent executes multiple trajectories for\nthe failed tasks, and selects the successful ones to supplement its failed\ntrajectory for contrastive self-training. Realized by policy gradient methods\nwith binarized rewards, the contrastive self-training with accumulated\ntrajectories facilitates a closed loop for multiple rounds of language agent\nself-improvement. We conduct experiments using QLoRA fine-tuning with the\nopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with\nA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative\nrounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human\naverage, and 4 rounds of iterative refinement lead to the performance\napproaching human experts. A$^3$T agents significantly outperform existing\ntechniques, including prompting with GPT-4, advanced agent frameworks, and\nfully fine-tuned LLMs.\n", "link": "http://arxiv.org/abs/2403.14589v1", "date": "2024-03-21", "relevancy": 2.145, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5277}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ReAct%20Meets%20ActRe%3A%20Autonomous%20Annotations%20of%20Agent%20Trajectories%20for%0A%20%20Contrastive%20Self-Training&body=Title%3A%20ReAct%20Meets%20ActRe%3A%20Autonomous%20Annotations%20of%20Agent%20Trajectories%20for%0A%20%20Contrastive%20Self-Training%0AAuthor%3A%20Zonghan%20Yang%20and%20Peng%20Li%20and%20Ming%20Yan%20and%20Ji%20Zhang%20and%20Fei%20Huang%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Language%20agents%20have%20demonstrated%20autonomous%20decision-making%20abilities%20by%0Areasoning%20with%20foundation%20models.%20Recently%2C%20efforts%20have%20been%20made%20to%20train%0Alanguage%20agents%20for%20performance%20improvement%2C%20with%20multi-step%20reasoning%20and%0Aaction%20trajectories%20as%20the%20training%20data.%20However%2C%20collecting%20such%20trajectories%0Astill%20requires%20considerable%20human%20effort%2C%20by%20either%20artificial%20annotations%20or%0Aimplementations%20of%20diverse%20prompting%20frameworks.%20In%20this%20work%2C%20we%20propose%0AA%24%5E3%24T%2C%20a%20framework%20that%20enables%20the%20Autonomous%20Annotation%20of%20Agent%0ATrajectories%20in%20the%20style%20of%20ReAct.%20The%20central%20role%20is%20an%20ActRe%20prompting%0Aagent%2C%20which%20explains%20the%20reason%20for%20an%20arbitrary%20action.%20When%20randomly%0Asampling%20an%20external%20action%2C%20the%20ReAct-style%20agent%20could%20query%20the%20ActRe%20agent%0Awith%20the%20action%20to%20obtain%20its%20textual%20rationales.%20Novel%20trajectories%20are%20then%0Asynthesized%20by%20prepending%20the%20posterior%20reasoning%20from%20ActRe%20to%20the%20sampled%0Aaction.%20In%20this%20way%2C%20the%20ReAct-style%20agent%20executes%20multiple%20trajectories%20for%0Athe%20failed%20tasks%2C%20and%20selects%20the%20successful%20ones%20to%20supplement%20its%20failed%0Atrajectory%20for%20contrastive%20self-training.%20Realized%20by%20policy%20gradient%20methods%0Awith%20binarized%20rewards%2C%20the%20contrastive%20self-training%20with%20accumulated%0Atrajectories%20facilitates%20a%20closed%20loop%20for%20multiple%20rounds%20of%20language%20agent%0Aself-improvement.%20We%20conduct%20experiments%20using%20QLoRA%20fine-tuning%20with%20the%0Aopen-sourced%20Mistral-7B-Instruct-v0.2.%20In%20AlfWorld%2C%20the%20agent%20trained%20with%0AA%24%5E3%24T%20obtains%20a%201-shot%20success%20rate%20of%2096%25%2C%20and%20100%25%20success%20with%204%20iterative%0Arounds.%20In%20WebShop%2C%20the%201-shot%20performance%20of%20the%20A%24%5E3%24T%20agent%20matches%20human%0Aaverage%2C%20and%204%20rounds%20of%20iterative%20refinement%20lead%20to%20the%20performance%0Aapproaching%20human%20experts.%20A%24%5E3%24T%20agents%20significantly%20outperform%20existing%0Atechniques%2C%20including%20prompting%20with%20GPT-4%2C%20advanced%20agent%20frameworks%2C%20and%0Afully%20fine-tuned%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14589v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReAct%20Meets%20ActRe%3A%20Autonomous%20Annotations%20of%20Agent%20Trajectories%20for%0A%20%20Contrastive%20Self-Training&entry.906535625=Zonghan%20Yang%20and%20Peng%20Li%20and%20Ming%20Yan%20and%20Ji%20Zhang%20and%20Fei%20Huang%20and%20Yang%20Liu&entry.1292438233=%20%20Language%20agents%20have%20demonstrated%20autonomous%20decision-making%20abilities%20by%0Areasoning%20with%20foundation%20models.%20Recently%2C%20efforts%20have%20been%20made%20to%20train%0Alanguage%20agents%20for%20performance%20improvement%2C%20with%20multi-step%20reasoning%20and%0Aaction%20trajectories%20as%20the%20training%20data.%20However%2C%20collecting%20such%20trajectories%0Astill%20requires%20considerable%20human%20effort%2C%20by%20either%20artificial%20annotations%20or%0Aimplementations%20of%20diverse%20prompting%20frameworks.%20In%20this%20work%2C%20we%20propose%0AA%24%5E3%24T%2C%20a%20framework%20that%20enables%20the%20Autonomous%20Annotation%20of%20Agent%0ATrajectories%20in%20the%20style%20of%20ReAct.%20The%20central%20role%20is%20an%20ActRe%20prompting%0Aagent%2C%20which%20explains%20the%20reason%20for%20an%20arbitrary%20action.%20When%20randomly%0Asampling%20an%20external%20action%2C%20the%20ReAct-style%20agent%20could%20query%20the%20ActRe%20agent%0Awith%20the%20action%20to%20obtain%20its%20textual%20rationales.%20Novel%20trajectories%20are%20then%0Asynthesized%20by%20prepending%20the%20posterior%20reasoning%20from%20ActRe%20to%20the%20sampled%0Aaction.%20In%20this%20way%2C%20the%20ReAct-style%20agent%20executes%20multiple%20trajectories%20for%0Athe%20failed%20tasks%2C%20and%20selects%20the%20successful%20ones%20to%20supplement%20its%20failed%0Atrajectory%20for%20contrastive%20self-training.%20Realized%20by%20policy%20gradient%20methods%0Awith%20binarized%20rewards%2C%20the%20contrastive%20self-training%20with%20accumulated%0Atrajectories%20facilitates%20a%20closed%20loop%20for%20multiple%20rounds%20of%20language%20agent%0Aself-improvement.%20We%20conduct%20experiments%20using%20QLoRA%20fine-tuning%20with%20the%0Aopen-sourced%20Mistral-7B-Instruct-v0.2.%20In%20AlfWorld%2C%20the%20agent%20trained%20with%0AA%24%5E3%24T%20obtains%20a%201-shot%20success%20rate%20of%2096%25%2C%20and%20100%25%20success%20with%204%20iterative%0Arounds.%20In%20WebShop%2C%20the%201-shot%20performance%20of%20the%20A%24%5E3%24T%20agent%20matches%20human%0Aaverage%2C%20and%204%20rounds%20of%20iterative%20refinement%20lead%20to%20the%20performance%0Aapproaching%20human%20experts.%20A%24%5E3%24T%20agents%20significantly%20outperform%20existing%0Atechniques%2C%20including%20prompting%20with%20GPT-4%2C%20advanced%20agent%20frameworks%2C%20and%0Afully%20fine-tuned%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14589v1&entry.124074799=Read"},
{"title": "Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling", "author": "Chengxu Zhuang and Evelina Fedorenko and Jacob Andreas", "abstract": "  Today's most accurate language models are trained on orders of magnitude more\nlanguage data than human language learners receive - but with no supervision\nfrom other sensory modalities that play a crucial role in human learning. Can\nwe make LMs' representations and predictions more accurate (and more\nhuman-like) with more ecologically plausible supervision? This paper describes\nLexiContrastive Grounding (LCG), a grounded language learning procedure that\nleverages visual supervision to improve textual representations.\nLexiContrastive Grounding combines a next token prediction strategy with a\ncontrastive visual grounding objective, focusing on early-layer representations\nthat encode lexical information. Across multiple word-learning and\nsentence-understanding benchmarks, LexiContrastive Grounding not only\noutperforms standard language-only models in learning efficiency, but also\nimproves upon vision-and-language learning procedures including CLIP, GIT,\nFlamingo, and Vokenization. Moreover, LexiContrastive Grounding improves\nperplexity by around 5% on multiple language modeling tasks. This work\nunderscores the potential of incorporating visual grounding into language\nmodels, aligning more closely with the multimodal nature of human language\nacquisition.\n", "link": "http://arxiv.org/abs/2403.14551v1", "date": "2024-03-21", "relevancy": 2.1357, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5428}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5365}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5055}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lexicon-Level%20Contrastive%20Visual-Grounding%20Improves%20Language%20Modeling&body=Title%3A%20Lexicon-Level%20Contrastive%20Visual-Grounding%20Improves%20Language%20Modeling%0AAuthor%3A%20Chengxu%20Zhuang%20and%20Evelina%20Fedorenko%20and%20Jacob%20Andreas%0AAbstract%3A%20%20%20Today%27s%20most%20accurate%20language%20models%20are%20trained%20on%20orders%20of%20magnitude%20more%0Alanguage%20data%20than%20human%20language%20learners%20receive%20-%20but%20with%20no%20supervision%0Afrom%20other%20sensory%20modalities%20that%20play%20a%20crucial%20role%20in%20human%20learning.%20Can%0Awe%20make%20LMs%27%20representations%20and%20predictions%20more%20accurate%20%28and%20more%0Ahuman-like%29%20with%20more%20ecologically%20plausible%20supervision%3F%20This%20paper%20describes%0ALexiContrastive%20Grounding%20%28LCG%29%2C%20a%20grounded%20language%20learning%20procedure%20that%0Aleverages%20visual%20supervision%20to%20improve%20textual%20representations.%0ALexiContrastive%20Grounding%20combines%20a%20next%20token%20prediction%20strategy%20with%20a%0Acontrastive%20visual%20grounding%20objective%2C%20focusing%20on%20early-layer%20representations%0Athat%20encode%20lexical%20information.%20Across%20multiple%20word-learning%20and%0Asentence-understanding%20benchmarks%2C%20LexiContrastive%20Grounding%20not%20only%0Aoutperforms%20standard%20language-only%20models%20in%20learning%20efficiency%2C%20but%20also%0Aimproves%20upon%20vision-and-language%20learning%20procedures%20including%20CLIP%2C%20GIT%2C%0AFlamingo%2C%20and%20Vokenization.%20Moreover%2C%20LexiContrastive%20Grounding%20improves%0Aperplexity%20by%20around%205%25%20on%20multiple%20language%20modeling%20tasks.%20This%20work%0Aunderscores%20the%20potential%20of%20incorporating%20visual%20grounding%20into%20language%0Amodels%2C%20aligning%20more%20closely%20with%20the%20multimodal%20nature%20of%20human%20language%0Aacquisition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14551v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lexicon-Level%20Contrastive%20Visual-Grounding%20Improves%20Language%20Modeling&entry.906535625=Chengxu%20Zhuang%20and%20Evelina%20Fedorenko%20and%20Jacob%20Andreas&entry.1292438233=%20%20Today%27s%20most%20accurate%20language%20models%20are%20trained%20on%20orders%20of%20magnitude%20more%0Alanguage%20data%20than%20human%20language%20learners%20receive%20-%20but%20with%20no%20supervision%0Afrom%20other%20sensory%20modalities%20that%20play%20a%20crucial%20role%20in%20human%20learning.%20Can%0Awe%20make%20LMs%27%20representations%20and%20predictions%20more%20accurate%20%28and%20more%0Ahuman-like%29%20with%20more%20ecologically%20plausible%20supervision%3F%20This%20paper%20describes%0ALexiContrastive%20Grounding%20%28LCG%29%2C%20a%20grounded%20language%20learning%20procedure%20that%0Aleverages%20visual%20supervision%20to%20improve%20textual%20representations.%0ALexiContrastive%20Grounding%20combines%20a%20next%20token%20prediction%20strategy%20with%20a%0Acontrastive%20visual%20grounding%20objective%2C%20focusing%20on%20early-layer%20representations%0Athat%20encode%20lexical%20information.%20Across%20multiple%20word-learning%20and%0Asentence-understanding%20benchmarks%2C%20LexiContrastive%20Grounding%20not%20only%0Aoutperforms%20standard%20language-only%20models%20in%20learning%20efficiency%2C%20but%20also%0Aimproves%20upon%20vision-and-language%20learning%20procedures%20including%20CLIP%2C%20GIT%2C%0AFlamingo%2C%20and%20Vokenization.%20Moreover%2C%20LexiContrastive%20Grounding%20improves%0Aperplexity%20by%20around%205%25%20on%20multiple%20language%20modeling%20tasks.%20This%20work%0Aunderscores%20the%20potential%20of%20incorporating%20visual%20grounding%20into%20language%0Amodels%2C%20aligning%20more%20closely%20with%20the%20multimodal%20nature%20of%20human%20language%0Aacquisition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14551v1&entry.124074799=Read"},
{"title": "Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact\n  Subproblem Solver for Training Structured Neural Network", "author": "Zih-Syuan Huang and Ching-pei Lee", "abstract": "  We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm\nfor training structured neural networks. Similar to existing regularized\nadaptive methods, the subproblem for computing the update direction of RAMDA\ninvolves a nonsmooth regularizer and a diagonal preconditioner, and therefore\ndoes not possess a closed-form solution in general. We thus also carefully\ndevise an implementable inexactness condition that retains convergence\nguarantees similar to the exact versions, and propose a companion efficient\nsolver for the subproblems of both RAMDA and existing methods to make them\npractically feasible. We leverage the theory of manifold identification in\nvariational analysis to show that, even in the presence of such inexactness,\nthe iterates of RAMDA attain the ideal structure induced by the regularizer at\nthe stationary point of asymptotic convergence. This structure is locally\noptimal near the point of convergence, so RAMDA is guaranteed to obtain the\nbest structure possible among all methods converging to the same point, making\nit the first regularized adaptive method outputting models that possess\noutstanding predictive performance while being (locally) optimally structured.\nExtensive numerical experiments in large-scale modern computer vision, language\nmodeling, and speech tasks show that the proposed RAMDA is efficient and\nconsistently outperforms state of the art for training structured neural\nnetwork. Implementation of our algorithm is available at\nhttp://www.github.com/ismoptgroup/RAMDA/.\n", "link": "http://arxiv.org/abs/2403.14398v1", "date": "2024-03-21", "relevancy": 2.1268, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5487}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Regularized%20Adaptive%20Momentum%20Dual%20Averaging%20with%20an%20Efficient%20Inexact%0A%20%20Subproblem%20Solver%20for%20Training%20Structured%20Neural%20Network&body=Title%3A%20Regularized%20Adaptive%20Momentum%20Dual%20Averaging%20with%20an%20Efficient%20Inexact%0A%20%20Subproblem%20Solver%20for%20Training%20Structured%20Neural%20Network%0AAuthor%3A%20Zih-Syuan%20Huang%20and%20Ching-pei%20Lee%0AAbstract%3A%20%20%20We%20propose%20a%20Regularized%20Adaptive%20Momentum%20Dual%20Averaging%20%28RAMDA%29%20algorithm%0Afor%20training%20structured%20neural%20networks.%20Similar%20to%20existing%20regularized%0Aadaptive%20methods%2C%20the%20subproblem%20for%20computing%20the%20update%20direction%20of%20RAMDA%0Ainvolves%20a%20nonsmooth%20regularizer%20and%20a%20diagonal%20preconditioner%2C%20and%20therefore%0Adoes%20not%20possess%20a%20closed-form%20solution%20in%20general.%20We%20thus%20also%20carefully%0Adevise%20an%20implementable%20inexactness%20condition%20that%20retains%20convergence%0Aguarantees%20similar%20to%20the%20exact%20versions%2C%20and%20propose%20a%20companion%20efficient%0Asolver%20for%20the%20subproblems%20of%20both%20RAMDA%20and%20existing%20methods%20to%20make%20them%0Apractically%20feasible.%20We%20leverage%20the%20theory%20of%20manifold%20identification%20in%0Avariational%20analysis%20to%20show%20that%2C%20even%20in%20the%20presence%20of%20such%20inexactness%2C%0Athe%20iterates%20of%20RAMDA%20attain%20the%20ideal%20structure%20induced%20by%20the%20regularizer%20at%0Athe%20stationary%20point%20of%20asymptotic%20convergence.%20This%20structure%20is%20locally%0Aoptimal%20near%20the%20point%20of%20convergence%2C%20so%20RAMDA%20is%20guaranteed%20to%20obtain%20the%0Abest%20structure%20possible%20among%20all%20methods%20converging%20to%20the%20same%20point%2C%20making%0Ait%20the%20first%20regularized%20adaptive%20method%20outputting%20models%20that%20possess%0Aoutstanding%20predictive%20performance%20while%20being%20%28locally%29%20optimally%20structured.%0AExtensive%20numerical%20experiments%20in%20large-scale%20modern%20computer%20vision%2C%20language%0Amodeling%2C%20and%20speech%20tasks%20show%20that%20the%20proposed%20RAMDA%20is%20efficient%20and%0Aconsistently%20outperforms%20state%20of%20the%20art%20for%20training%20structured%20neural%0Anetwork.%20Implementation%20of%20our%20algorithm%20is%20available%20at%0Ahttp%3A//www.github.com/ismoptgroup/RAMDA/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14398v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Adaptive%20Momentum%20Dual%20Averaging%20with%20an%20Efficient%20Inexact%0A%20%20Subproblem%20Solver%20for%20Training%20Structured%20Neural%20Network&entry.906535625=Zih-Syuan%20Huang%20and%20Ching-pei%20Lee&entry.1292438233=%20%20We%20propose%20a%20Regularized%20Adaptive%20Momentum%20Dual%20Averaging%20%28RAMDA%29%20algorithm%0Afor%20training%20structured%20neural%20networks.%20Similar%20to%20existing%20regularized%0Aadaptive%20methods%2C%20the%20subproblem%20for%20computing%20the%20update%20direction%20of%20RAMDA%0Ainvolves%20a%20nonsmooth%20regularizer%20and%20a%20diagonal%20preconditioner%2C%20and%20therefore%0Adoes%20not%20possess%20a%20closed-form%20solution%20in%20general.%20We%20thus%20also%20carefully%0Adevise%20an%20implementable%20inexactness%20condition%20that%20retains%20convergence%0Aguarantees%20similar%20to%20the%20exact%20versions%2C%20and%20propose%20a%20companion%20efficient%0Asolver%20for%20the%20subproblems%20of%20both%20RAMDA%20and%20existing%20methods%20to%20make%20them%0Apractically%20feasible.%20We%20leverage%20the%20theory%20of%20manifold%20identification%20in%0Avariational%20analysis%20to%20show%20that%2C%20even%20in%20the%20presence%20of%20such%20inexactness%2C%0Athe%20iterates%20of%20RAMDA%20attain%20the%20ideal%20structure%20induced%20by%20the%20regularizer%20at%0Athe%20stationary%20point%20of%20asymptotic%20convergence.%20This%20structure%20is%20locally%0Aoptimal%20near%20the%20point%20of%20convergence%2C%20so%20RAMDA%20is%20guaranteed%20to%20obtain%20the%0Abest%20structure%20possible%20among%20all%20methods%20converging%20to%20the%20same%20point%2C%20making%0Ait%20the%20first%20regularized%20adaptive%20method%20outputting%20models%20that%20possess%0Aoutstanding%20predictive%20performance%20while%20being%20%28locally%29%20optimally%20structured.%0AExtensive%20numerical%20experiments%20in%20large-scale%20modern%20computer%20vision%2C%20language%0Amodeling%2C%20and%20speech%20tasks%20show%20that%20the%20proposed%20RAMDA%20is%20efficient%20and%0Aconsistently%20outperforms%20state%20of%20the%20art%20for%20training%20structured%20neural%0Anetwork.%20Implementation%20of%20our%20algorithm%20is%20available%20at%0Ahttp%3A//www.github.com/ismoptgroup/RAMDA/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14398v1&entry.124074799=Read"},
{"title": "CombiNeRF: A Combination of Regularization Techniques for Few-Shot\n  Neural Radiance Field View Synthesis", "author": "Matteo Bonotto and Luigi Sarrocco and Daniele Evangelista and Marco Imperoli and Alberto Pretto", "abstract": "  Neural Radiance Fields (NeRFs) have shown impressive results for novel view\nsynthesis when a sufficiently large amount of views are available. When dealing\nwith few-shot settings, i.e. with a small set of input views, the training\ncould overfit those views, leading to artifacts and geometric and chromatic\ninconsistencies in the resulting rendering. Regularization is a valid solution\nthat helps NeRF generalization. On the other hand, each of the most recent NeRF\nregularization techniques aim to mitigate a specific rendering problem.\nStarting from this observation, in this paper we propose CombiNeRF, a framework\nthat synergically combines several regularization techniques, some of them\nnovel, in order to unify the benefits of each. In particular, we regularize\nsingle and neighboring rays distributions and we add a smoothness term to\nregularize near geometries. After these geometric approaches, we propose to\nexploit Lipschitz regularization to both NeRF density and color networks and to\nuse encoding masks for input features regularization. We show that CombiNeRF\noutperforms the state-of-the-art methods with few-shot settings in several\npublicly available datasets. We also present an ablation study on the LLFF and\nNeRF-Synthetic datasets that support the choices made. We release with this\npaper the open-source implementation of our framework.\n", "link": "http://arxiv.org/abs/2403.14412v1", "date": "2024-03-21", "relevancy": 2.1156, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5593}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5152}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.504}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CombiNeRF%3A%20A%20Combination%20of%20Regularization%20Techniques%20for%20Few-Shot%0A%20%20Neural%20Radiance%20Field%20View%20Synthesis&body=Title%3A%20CombiNeRF%3A%20A%20Combination%20of%20Regularization%20Techniques%20for%20Few-Shot%0A%20%20Neural%20Radiance%20Field%20View%20Synthesis%0AAuthor%3A%20Matteo%20Bonotto%20and%20Luigi%20Sarrocco%20and%20Daniele%20Evangelista%20and%20Marco%20Imperoli%20and%20Alberto%20Pretto%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20shown%20impressive%20results%20for%20novel%20view%0Asynthesis%20when%20a%20sufficiently%20large%20amount%20of%20views%20are%20available.%20When%20dealing%0Awith%20few-shot%20settings%2C%20i.e.%20with%20a%20small%20set%20of%20input%20views%2C%20the%20training%0Acould%20overfit%20those%20views%2C%20leading%20to%20artifacts%20and%20geometric%20and%20chromatic%0Ainconsistencies%20in%20the%20resulting%20rendering.%20Regularization%20is%20a%20valid%20solution%0Athat%20helps%20NeRF%20generalization.%20On%20the%20other%20hand%2C%20each%20of%20the%20most%20recent%20NeRF%0Aregularization%20techniques%20aim%20to%20mitigate%20a%20specific%20rendering%20problem.%0AStarting%20from%20this%20observation%2C%20in%20this%20paper%20we%20propose%20CombiNeRF%2C%20a%20framework%0Athat%20synergically%20combines%20several%20regularization%20techniques%2C%20some%20of%20them%0Anovel%2C%20in%20order%20to%20unify%20the%20benefits%20of%20each.%20In%20particular%2C%20we%20regularize%0Asingle%20and%20neighboring%20rays%20distributions%20and%20we%20add%20a%20smoothness%20term%20to%0Aregularize%20near%20geometries.%20After%20these%20geometric%20approaches%2C%20we%20propose%20to%0Aexploit%20Lipschitz%20regularization%20to%20both%20NeRF%20density%20and%20color%20networks%20and%20to%0Ause%20encoding%20masks%20for%20input%20features%20regularization.%20We%20show%20that%20CombiNeRF%0Aoutperforms%20the%20state-of-the-art%20methods%20with%20few-shot%20settings%20in%20several%0Apublicly%20available%20datasets.%20We%20also%20present%20an%20ablation%20study%20on%20the%20LLFF%20and%0ANeRF-Synthetic%20datasets%20that%20support%20the%20choices%20made.%20We%20release%20with%20this%0Apaper%20the%20open-source%20implementation%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14412v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CombiNeRF%3A%20A%20Combination%20of%20Regularization%20Techniques%20for%20Few-Shot%0A%20%20Neural%20Radiance%20Field%20View%20Synthesis&entry.906535625=Matteo%20Bonotto%20and%20Luigi%20Sarrocco%20and%20Daniele%20Evangelista%20and%20Marco%20Imperoli%20and%20Alberto%20Pretto&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20shown%20impressive%20results%20for%20novel%20view%0Asynthesis%20when%20a%20sufficiently%20large%20amount%20of%20views%20are%20available.%20When%20dealing%0Awith%20few-shot%20settings%2C%20i.e.%20with%20a%20small%20set%20of%20input%20views%2C%20the%20training%0Acould%20overfit%20those%20views%2C%20leading%20to%20artifacts%20and%20geometric%20and%20chromatic%0Ainconsistencies%20in%20the%20resulting%20rendering.%20Regularization%20is%20a%20valid%20solution%0Athat%20helps%20NeRF%20generalization.%20On%20the%20other%20hand%2C%20each%20of%20the%20most%20recent%20NeRF%0Aregularization%20techniques%20aim%20to%20mitigate%20a%20specific%20rendering%20problem.%0AStarting%20from%20this%20observation%2C%20in%20this%20paper%20we%20propose%20CombiNeRF%2C%20a%20framework%0Athat%20synergically%20combines%20several%20regularization%20techniques%2C%20some%20of%20them%0Anovel%2C%20in%20order%20to%20unify%20the%20benefits%20of%20each.%20In%20particular%2C%20we%20regularize%0Asingle%20and%20neighboring%20rays%20distributions%20and%20we%20add%20a%20smoothness%20term%20to%0Aregularize%20near%20geometries.%20After%20these%20geometric%20approaches%2C%20we%20propose%20to%0Aexploit%20Lipschitz%20regularization%20to%20both%20NeRF%20density%20and%20color%20networks%20and%20to%0Ause%20encoding%20masks%20for%20input%20features%20regularization.%20We%20show%20that%20CombiNeRF%0Aoutperforms%20the%20state-of-the-art%20methods%20with%20few-shot%20settings%20in%20several%0Apublicly%20available%20datasets.%20We%20also%20present%20an%20ablation%20study%20on%20the%20LLFF%20and%0ANeRF-Synthetic%20datasets%20that%20support%20the%20choices%20made.%20We%20release%20with%20this%0Apaper%20the%20open-source%20implementation%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14412v1&entry.124074799=Read"},
{"title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference", "author": "Han Zhao and Min Zhang and Wei Zhao and Pengxiang Ding and Siteng Huang and Donglin Wang", "abstract": "  In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, \\textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster\nspeed due to Cobra's linear sequential modeling. (2) Interestingly, the results\nof closed-set challenging prediction benchmarks show that Cobra performs well\nin overcoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.\n", "link": "http://arxiv.org/abs/2403.14520v1", "date": "2024-03-21", "relevancy": 2.1122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5061}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference&body=Title%3A%20Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference%0AAuthor%3A%20Han%20Zhao%20and%20Min%20Zhang%20and%20Wei%20Zhao%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20application%20of%20multimodal%20large%20language%20models%20%28MLLM%29%0Ain%20various%20fields%20has%20achieved%20remarkable%20success.%20However%2C%20as%20the%20foundation%0Amodel%20for%20many%20downstream%20tasks%2C%20current%20MLLMs%20are%20composed%20of%20the%20well-known%0ATransformer%20network%2C%20which%20has%20a%20less%20efficient%20quadratic%20computation%0Acomplexity.%20To%20improve%20the%20efficiency%20of%20such%20basic%20models%2C%20we%20propose%20Cobra%2C%20a%0Alinear%20computational%20complexity%20MLLM.%20Specifically%2C%20Cobra%20integrates%20the%0Aefficient%20Mamba%20language%20model%20into%20the%20visual%20modality.%20Moreover%2C%20we%20explore%0Aand%20study%20various%20modal%20fusion%20schemes%20to%20create%20an%20effective%20multi-modal%0AMamba.%20Extensive%20experiments%20demonstrate%20that%20%281%29%20Cobra%20achieves%20extremely%0Acompetitive%20performance%20with%20current%20computationally%20efficient%20state-of-the-art%0Amethods%2C%20%5Ctextit%7Be.g.%7D%2C%20LLaVA-Phi%2C%20TinyLLaVA%2C%20and%20MobileVLM%20v2%2C%20and%20has%20faster%0Aspeed%20due%20to%20Cobra%27s%20linear%20sequential%20modeling.%20%282%29%20Interestingly%2C%20the%20results%0Aof%20closed-set%20challenging%20prediction%20benchmarks%20show%20that%20Cobra%20performs%20well%0Ain%20overcoming%20visual%20illusions%20and%20spatial%20relationship%20judgments.%20%283%29%20Notably%2C%0ACobra%20even%20achieves%20comparable%20performance%20to%20LLaVA%20with%20about%2043%25%20of%20the%0Anumber%20of%20parameters.%20We%20will%20make%20all%20codes%20of%20Cobra%20open-source%20and%20hope%20that%0Athe%20proposed%20method%20can%20facilitate%20future%20research%20on%20complexity%20problems%20in%0AMLLM.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//sites.google.com/view/cobravlm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14520v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference&entry.906535625=Han%20Zhao%20and%20Min%20Zhang%20and%20Wei%20Zhao%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Donglin%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20application%20of%20multimodal%20large%20language%20models%20%28MLLM%29%0Ain%20various%20fields%20has%20achieved%20remarkable%20success.%20However%2C%20as%20the%20foundation%0Amodel%20for%20many%20downstream%20tasks%2C%20current%20MLLMs%20are%20composed%20of%20the%20well-known%0ATransformer%20network%2C%20which%20has%20a%20less%20efficient%20quadratic%20computation%0Acomplexity.%20To%20improve%20the%20efficiency%20of%20such%20basic%20models%2C%20we%20propose%20Cobra%2C%20a%0Alinear%20computational%20complexity%20MLLM.%20Specifically%2C%20Cobra%20integrates%20the%0Aefficient%20Mamba%20language%20model%20into%20the%20visual%20modality.%20Moreover%2C%20we%20explore%0Aand%20study%20various%20modal%20fusion%20schemes%20to%20create%20an%20effective%20multi-modal%0AMamba.%20Extensive%20experiments%20demonstrate%20that%20%281%29%20Cobra%20achieves%20extremely%0Acompetitive%20performance%20with%20current%20computationally%20efficient%20state-of-the-art%0Amethods%2C%20%5Ctextit%7Be.g.%7D%2C%20LLaVA-Phi%2C%20TinyLLaVA%2C%20and%20MobileVLM%20v2%2C%20and%20has%20faster%0Aspeed%20due%20to%20Cobra%27s%20linear%20sequential%20modeling.%20%282%29%20Interestingly%2C%20the%20results%0Aof%20closed-set%20challenging%20prediction%20benchmarks%20show%20that%20Cobra%20performs%20well%0Ain%20overcoming%20visual%20illusions%20and%20spatial%20relationship%20judgments.%20%283%29%20Notably%2C%0ACobra%20even%20achieves%20comparable%20performance%20to%20LLaVA%20with%20about%2043%25%20of%20the%0Anumber%20of%20parameters.%20We%20will%20make%20all%20codes%20of%20Cobra%20open-source%20and%20hope%20that%0Athe%20proposed%20method%20can%20facilitate%20future%20research%20on%20complexity%20problems%20in%0AMLLM.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//sites.google.com/view/cobravlm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14520v1&entry.124074799=Read"},
{"title": "Analysing Diffusion Segmentation for Medical Images", "author": "Mathias \u00d6ttl and Siyuan Mei and Frauke Wilm and Jana Steenpass and Matthias R\u00fcbner and Arndt Hartmann and Matthias Beckmann and Peter Fasching and Andreas Maier and Ramona Erber and Katharina Breininger", "abstract": "  Denoising Diffusion Probabilistic models have become increasingly popular due\nto their ability to offer probabilistic modeling and generate diverse outputs.\nThis versatility inspired their adaptation for image segmentation, where\nmultiple predictions of the model can produce segmentation results that not\nonly achieve high quality but also capture the uncertainty inherent in the\nmodel. Here, powerful architectures were proposed for improving diffusion\nsegmentation performance. However, there is a notable lack of analysis and\ndiscussions on the differences between diffusion segmentation and image\ngeneration, and thorough evaluations are missing that distinguish the\nimprovements these architectures provide for segmentation in general from their\nbenefit for diffusion segmentation specifically. In this work, we critically\nanalyse and discuss how diffusion segmentation for medical images differs from\ndiffusion image generation, with a particular focus on the training behavior.\nFurthermore, we conduct an assessment how proposed diffusion segmentation\narchitectures perform when trained directly for segmentation. Lastly, we\nexplore how different medical segmentation tasks influence the diffusion\nsegmentation behavior and the diffusion process could be adapted accordingly.\nWith these analyses, we aim to provide in-depth insights into the behavior of\ndiffusion segmentation that allow for a better design and evaluation of\ndiffusion segmentation methods in the future.\n", "link": "http://arxiv.org/abs/2403.14440v1", "date": "2024-03-21", "relevancy": 2.0992, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5375}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5347}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5098}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Analysing%20Diffusion%20Segmentation%20for%20Medical%20Images&body=Title%3A%20Analysing%20Diffusion%20Segmentation%20for%20Medical%20Images%0AAuthor%3A%20Mathias%20%C3%96ttl%20and%20Siyuan%20Mei%20and%20Frauke%20Wilm%20and%20Jana%20Steenpass%20and%20Matthias%20R%C3%BCbner%20and%20Arndt%20Hartmann%20and%20Matthias%20Beckmann%20and%20Peter%20Fasching%20and%20Andreas%20Maier%20and%20Ramona%20Erber%20and%20Katharina%20Breininger%0AAbstract%3A%20%20%20Denoising%20Diffusion%20Probabilistic%20models%20have%20become%20increasingly%20popular%20due%0Ato%20their%20ability%20to%20offer%20probabilistic%20modeling%20and%20generate%20diverse%20outputs.%0AThis%20versatility%20inspired%20their%20adaptation%20for%20image%20segmentation%2C%20where%0Amultiple%20predictions%20of%20the%20model%20can%20produce%20segmentation%20results%20that%20not%0Aonly%20achieve%20high%20quality%20but%20also%20capture%20the%20uncertainty%20inherent%20in%20the%0Amodel.%20Here%2C%20powerful%20architectures%20were%20proposed%20for%20improving%20diffusion%0Asegmentation%20performance.%20However%2C%20there%20is%20a%20notable%20lack%20of%20analysis%20and%0Adiscussions%20on%20the%20differences%20between%20diffusion%20segmentation%20and%20image%0Ageneration%2C%20and%20thorough%20evaluations%20are%20missing%20that%20distinguish%20the%0Aimprovements%20these%20architectures%20provide%20for%20segmentation%20in%20general%20from%20their%0Abenefit%20for%20diffusion%20segmentation%20specifically.%20In%20this%20work%2C%20we%20critically%0Aanalyse%20and%20discuss%20how%20diffusion%20segmentation%20for%20medical%20images%20differs%20from%0Adiffusion%20image%20generation%2C%20with%20a%20particular%20focus%20on%20the%20training%20behavior.%0AFurthermore%2C%20we%20conduct%20an%20assessment%20how%20proposed%20diffusion%20segmentation%0Aarchitectures%20perform%20when%20trained%20directly%20for%20segmentation.%20Lastly%2C%20we%0Aexplore%20how%20different%20medical%20segmentation%20tasks%20influence%20the%20diffusion%0Asegmentation%20behavior%20and%20the%20diffusion%20process%20could%20be%20adapted%20accordingly.%0AWith%20these%20analyses%2C%20we%20aim%20to%20provide%20in-depth%20insights%20into%20the%20behavior%20of%0Adiffusion%20segmentation%20that%20allow%20for%20a%20better%20design%20and%20evaluation%20of%0Adiffusion%20segmentation%20methods%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14440v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysing%20Diffusion%20Segmentation%20for%20Medical%20Images&entry.906535625=Mathias%20%C3%96ttl%20and%20Siyuan%20Mei%20and%20Frauke%20Wilm%20and%20Jana%20Steenpass%20and%20Matthias%20R%C3%BCbner%20and%20Arndt%20Hartmann%20and%20Matthias%20Beckmann%20and%20Peter%20Fasching%20and%20Andreas%20Maier%20and%20Ramona%20Erber%20and%20Katharina%20Breininger&entry.1292438233=%20%20Denoising%20Diffusion%20Probabilistic%20models%20have%20become%20increasingly%20popular%20due%0Ato%20their%20ability%20to%20offer%20probabilistic%20modeling%20and%20generate%20diverse%20outputs.%0AThis%20versatility%20inspired%20their%20adaptation%20for%20image%20segmentation%2C%20where%0Amultiple%20predictions%20of%20the%20model%20can%20produce%20segmentation%20results%20that%20not%0Aonly%20achieve%20high%20quality%20but%20also%20capture%20the%20uncertainty%20inherent%20in%20the%0Amodel.%20Here%2C%20powerful%20architectures%20were%20proposed%20for%20improving%20diffusion%0Asegmentation%20performance.%20However%2C%20there%20is%20a%20notable%20lack%20of%20analysis%20and%0Adiscussions%20on%20the%20differences%20between%20diffusion%20segmentation%20and%20image%0Ageneration%2C%20and%20thorough%20evaluations%20are%20missing%20that%20distinguish%20the%0Aimprovements%20these%20architectures%20provide%20for%20segmentation%20in%20general%20from%20their%0Abenefit%20for%20diffusion%20segmentation%20specifically.%20In%20this%20work%2C%20we%20critically%0Aanalyse%20and%20discuss%20how%20diffusion%20segmentation%20for%20medical%20images%20differs%20from%0Adiffusion%20image%20generation%2C%20with%20a%20particular%20focus%20on%20the%20training%20behavior.%0AFurthermore%2C%20we%20conduct%20an%20assessment%20how%20proposed%20diffusion%20segmentation%0Aarchitectures%20perform%20when%20trained%20directly%20for%20segmentation.%20Lastly%2C%20we%0Aexplore%20how%20different%20medical%20segmentation%20tasks%20influence%20the%20diffusion%0Asegmentation%20behavior%20and%20the%20diffusion%20process%20could%20be%20adapted%20accordingly.%0AWith%20these%20analyses%2C%20we%20aim%20to%20provide%20in-depth%20insights%20into%20the%20behavior%20of%0Adiffusion%20segmentation%20that%20allow%20for%20a%20better%20design%20and%20evaluation%20of%0Adiffusion%20segmentation%20methods%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14440v1&entry.124074799=Read"},
{"title": "Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting", "author": "Alicia Durrer and Julia Wolleb and Florentin Bieder and Paul Friedrich and Lester Melie-Garcia and Mario Ocampo-Pineda and Cosmin I. Bercea and Ibrahim E. Hamamci and Benedikt Wiestler and Marie Piraud and \u00d6zg\u00fcr Yaldizli and Cristina Granziera and Bjoern H. Menze and Philippe C. Cattin and Florian Kofler", "abstract": "  Monitoring diseases that affect the brain's structural integrity requires\nautomated analysis of magnetic resonance (MR) images, e.g., for the evaluation\nof volumetric changes. However, many of the evaluation tools are optimized for\nanalyzing healthy tissue. To enable the evaluation of scans containing\npathological tissue, it is therefore required to restore healthy tissue in the\npathological areas. In this work, we explore and extend denoising diffusion\nmodels for consistent inpainting of healthy 3D brain tissue. We modify\nstate-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, as\nwell as 3D latent and 3D wavelet diffusion models, and train them to synthesize\nhealthy brain tissue. Our evaluation shows that the pseudo-3D model performs\nbest regarding the structural-similarity index, peak signal-to-noise ratio, and\nmean squared error. To emphasize the clinical relevance, we fine-tune this\nmodel on data containing synthetic MS lesions and evaluate it on a downstream\nbrain tissue segmentation task, whereby it outperforms the established FMRIB\nSoftware Library (FSL) lesion-filling method.\n", "link": "http://arxiv.org/abs/2403.14499v1", "date": "2024-03-21", "relevancy": 2.0976, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5446}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5154}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5078}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Denoising%20Diffusion%20Models%20for%203D%20Healthy%20Brain%20Tissue%20Inpainting&body=Title%3A%20Denoising%20Diffusion%20Models%20for%203D%20Healthy%20Brain%20Tissue%20Inpainting%0AAuthor%3A%20Alicia%20Durrer%20and%20Julia%20Wolleb%20and%20Florentin%20Bieder%20and%20Paul%20Friedrich%20and%20Lester%20Melie-Garcia%20and%20Mario%20Ocampo-Pineda%20and%20Cosmin%20I.%20Bercea%20and%20Ibrahim%20E.%20Hamamci%20and%20Benedikt%20Wiestler%20and%20Marie%20Piraud%20and%20%C3%96zg%C3%BCr%20Yaldizli%20and%20Cristina%20Granziera%20and%20Bjoern%20H.%20Menze%20and%20Philippe%20C.%20Cattin%20and%20Florian%20Kofler%0AAbstract%3A%20%20%20Monitoring%20diseases%20that%20affect%20the%20brain%27s%20structural%20integrity%20requires%0Aautomated%20analysis%20of%20magnetic%20resonance%20%28MR%29%20images%2C%20e.g.%2C%20for%20the%20evaluation%0Aof%20volumetric%20changes.%20However%2C%20many%20of%20the%20evaluation%20tools%20are%20optimized%20for%0Aanalyzing%20healthy%20tissue.%20To%20enable%20the%20evaluation%20of%20scans%20containing%0Apathological%20tissue%2C%20it%20is%20therefore%20required%20to%20restore%20healthy%20tissue%20in%20the%0Apathological%20areas.%20In%20this%20work%2C%20we%20explore%20and%20extend%20denoising%20diffusion%0Amodels%20for%20consistent%20inpainting%20of%20healthy%203D%20brain%20tissue.%20We%20modify%0Astate-of-the-art%202D%2C%20pseudo-3D%2C%20and%203D%20methods%20working%20in%20the%20image%20space%2C%20as%0Awell%20as%203D%20latent%20and%203D%20wavelet%20diffusion%20models%2C%20and%20train%20them%20to%20synthesize%0Ahealthy%20brain%20tissue.%20Our%20evaluation%20shows%20that%20the%20pseudo-3D%20model%20performs%0Abest%20regarding%20the%20structural-similarity%20index%2C%20peak%20signal-to-noise%20ratio%2C%20and%0Amean%20squared%20error.%20To%20emphasize%20the%20clinical%20relevance%2C%20we%20fine-tune%20this%0Amodel%20on%20data%20containing%20synthetic%20MS%20lesions%20and%20evaluate%20it%20on%20a%20downstream%0Abrain%20tissue%20segmentation%20task%2C%20whereby%20it%20outperforms%20the%20established%20FMRIB%0ASoftware%20Library%20%28FSL%29%20lesion-filling%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14499v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20Diffusion%20Models%20for%203D%20Healthy%20Brain%20Tissue%20Inpainting&entry.906535625=Alicia%20Durrer%20and%20Julia%20Wolleb%20and%20Florentin%20Bieder%20and%20Paul%20Friedrich%20and%20Lester%20Melie-Garcia%20and%20Mario%20Ocampo-Pineda%20and%20Cosmin%20I.%20Bercea%20and%20Ibrahim%20E.%20Hamamci%20and%20Benedikt%20Wiestler%20and%20Marie%20Piraud%20and%20%C3%96zg%C3%BCr%20Yaldizli%20and%20Cristina%20Granziera%20and%20Bjoern%20H.%20Menze%20and%20Philippe%20C.%20Cattin%20and%20Florian%20Kofler&entry.1292438233=%20%20Monitoring%20diseases%20that%20affect%20the%20brain%27s%20structural%20integrity%20requires%0Aautomated%20analysis%20of%20magnetic%20resonance%20%28MR%29%20images%2C%20e.g.%2C%20for%20the%20evaluation%0Aof%20volumetric%20changes.%20However%2C%20many%20of%20the%20evaluation%20tools%20are%20optimized%20for%0Aanalyzing%20healthy%20tissue.%20To%20enable%20the%20evaluation%20of%20scans%20containing%0Apathological%20tissue%2C%20it%20is%20therefore%20required%20to%20restore%20healthy%20tissue%20in%20the%0Apathological%20areas.%20In%20this%20work%2C%20we%20explore%20and%20extend%20denoising%20diffusion%0Amodels%20for%20consistent%20inpainting%20of%20healthy%203D%20brain%20tissue.%20We%20modify%0Astate-of-the-art%202D%2C%20pseudo-3D%2C%20and%203D%20methods%20working%20in%20the%20image%20space%2C%20as%0Awell%20as%203D%20latent%20and%203D%20wavelet%20diffusion%20models%2C%20and%20train%20them%20to%20synthesize%0Ahealthy%20brain%20tissue.%20Our%20evaluation%20shows%20that%20the%20pseudo-3D%20model%20performs%0Abest%20regarding%20the%20structural-similarity%20index%2C%20peak%20signal-to-noise%20ratio%2C%20and%0Amean%20squared%20error.%20To%20emphasize%20the%20clinical%20relevance%2C%20we%20fine-tune%20this%0Amodel%20on%20data%20containing%20synthetic%20MS%20lesions%20and%20evaluate%20it%20on%20a%20downstream%0Abrain%20tissue%20segmentation%20task%2C%20whereby%20it%20outperforms%20the%20established%20FMRIB%0ASoftware%20Library%20%28FSL%29%20lesion-filling%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14499v1&entry.124074799=Read"},
{"title": "Task-optimal data-driven surrogate models for eNMPC via differentiable\n  simulation and optimization", "author": "Daniel Mayfrank and Na Young Ahn and Alexander Mitsos and Manuel Dahmen", "abstract": "  We present a method for end-to-end learning of Koopman surrogate models for\noptimal performance in control. In contrast to previous contributions that\nemploy standard reinforcement learning (RL) algorithms, we use a training\nalgorithm that exploits the potential differentiability of environments based\non mechanistic simulation models. We evaluate the performance of our method by\ncomparing it to that of other controller type and training algorithm\ncombinations on a literature known eNMPC case study. Our method exhibits\nsuperior performance on this problem, thereby constituting a promising avenue\ntowards more capable controllers that employ dynamic surrogate models.\n", "link": "http://arxiv.org/abs/2403.14425v1", "date": "2024-03-21", "relevancy": 2.0896, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5515}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5183}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5148}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Task-optimal%20data-driven%20surrogate%20models%20for%20eNMPC%20via%20differentiable%0A%20%20simulation%20and%20optimization&body=Title%3A%20Task-optimal%20data-driven%20surrogate%20models%20for%20eNMPC%20via%20differentiable%0A%20%20simulation%20and%20optimization%0AAuthor%3A%20Daniel%20Mayfrank%20and%20Na%20Young%20Ahn%20and%20Alexander%20Mitsos%20and%20Manuel%20Dahmen%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20end-to-end%20learning%20of%20Koopman%20surrogate%20models%20for%0Aoptimal%20performance%20in%20control.%20In%20contrast%20to%20previous%20contributions%20that%0Aemploy%20standard%20reinforcement%20learning%20%28RL%29%20algorithms%2C%20we%20use%20a%20training%0Aalgorithm%20that%20exploits%20the%20potential%20differentiability%20of%20environments%20based%0Aon%20mechanistic%20simulation%20models.%20We%20evaluate%20the%20performance%20of%20our%20method%20by%0Acomparing%20it%20to%20that%20of%20other%20controller%20type%20and%20training%20algorithm%0Acombinations%20on%20a%20literature%20known%20eNMPC%20case%20study.%20Our%20method%20exhibits%0Asuperior%20performance%20on%20this%20problem%2C%20thereby%20constituting%20a%20promising%20avenue%0Atowards%20more%20capable%20controllers%20that%20employ%20dynamic%20surrogate%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14425v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-optimal%20data-driven%20surrogate%20models%20for%20eNMPC%20via%20differentiable%0A%20%20simulation%20and%20optimization&entry.906535625=Daniel%20Mayfrank%20and%20Na%20Young%20Ahn%20and%20Alexander%20Mitsos%20and%20Manuel%20Dahmen&entry.1292438233=%20%20We%20present%20a%20method%20for%20end-to-end%20learning%20of%20Koopman%20surrogate%20models%20for%0Aoptimal%20performance%20in%20control.%20In%20contrast%20to%20previous%20contributions%20that%0Aemploy%20standard%20reinforcement%20learning%20%28RL%29%20algorithms%2C%20we%20use%20a%20training%0Aalgorithm%20that%20exploits%20the%20potential%20differentiability%20of%20environments%20based%0Aon%20mechanistic%20simulation%20models.%20We%20evaluate%20the%20performance%20of%20our%20method%20by%0Acomparing%20it%20to%20that%20of%20other%20controller%20type%20and%20training%20algorithm%0Acombinations%20on%20a%20literature%20known%20eNMPC%20case%20study.%20Our%20method%20exhibits%0Asuperior%20performance%20on%20this%20problem%2C%20thereby%20constituting%20a%20promising%20avenue%0Atowards%20more%20capable%20controllers%20that%20employ%20dynamic%20surrogate%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14425v1&entry.124074799=Read"},
{"title": "ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras\n  Based on Transformer", "author": "Tianye Ding and Hongyu Li and Huaizu Jiang", "abstract": "  Obstacle detection and tracking represent a critical component in robot\nautonomous navigation. In this paper, we propose ODTFormer, a Transformer-based\nmodel to address both obstacle detection and tracking problems. For the\ndetection task, our approach leverages deformable attention to construct a 3D\ncost volume, which is decoded progressively in the form of voxel occupancy\ngrids. We further track the obstacles by matching the voxels between\nconsecutive frames. The entire model can be optimized in an end-to-end manner.\nThrough extensive experiments on DrivingStereo and KITTI benchmarks, our model\nachieves state-of-the-art performance in the obstacle detection task. We also\nreport comparable accuracy to state-of-the-art obstacle tracking models while\nrequiring only a fraction of their computation cost, typically ten-fold to\ntwenty-fold less. The code and model weights will be publicly released.\n", "link": "http://arxiv.org/abs/2403.14626v1", "date": "2024-03-21", "relevancy": 2.0882, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5275}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5167}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ODTFormer%3A%20Efficient%20Obstacle%20Detection%20and%20Tracking%20with%20Stereo%20Cameras%0A%20%20Based%20on%20Transformer&body=Title%3A%20ODTFormer%3A%20Efficient%20Obstacle%20Detection%20and%20Tracking%20with%20Stereo%20Cameras%0A%20%20Based%20on%20Transformer%0AAuthor%3A%20Tianye%20Ding%20and%20Hongyu%20Li%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Obstacle%20detection%20and%20tracking%20represent%20a%20critical%20component%20in%20robot%0Aautonomous%20navigation.%20In%20this%20paper%2C%20we%20propose%20ODTFormer%2C%20a%20Transformer-based%0Amodel%20to%20address%20both%20obstacle%20detection%20and%20tracking%20problems.%20For%20the%0Adetection%20task%2C%20our%20approach%20leverages%20deformable%20attention%20to%20construct%20a%203D%0Acost%20volume%2C%20which%20is%20decoded%20progressively%20in%20the%20form%20of%20voxel%20occupancy%0Agrids.%20We%20further%20track%20the%20obstacles%20by%20matching%20the%20voxels%20between%0Aconsecutive%20frames.%20The%20entire%20model%20can%20be%20optimized%20in%20an%20end-to-end%20manner.%0AThrough%20extensive%20experiments%20on%20DrivingStereo%20and%20KITTI%20benchmarks%2C%20our%20model%0Aachieves%20state-of-the-art%20performance%20in%20the%20obstacle%20detection%20task.%20We%20also%0Areport%20comparable%20accuracy%20to%20state-of-the-art%20obstacle%20tracking%20models%20while%0Arequiring%20only%20a%20fraction%20of%20their%20computation%20cost%2C%20typically%20ten-fold%20to%0Atwenty-fold%20less.%20The%20code%20and%20model%20weights%20will%20be%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14626v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODTFormer%3A%20Efficient%20Obstacle%20Detection%20and%20Tracking%20with%20Stereo%20Cameras%0A%20%20Based%20on%20Transformer&entry.906535625=Tianye%20Ding%20and%20Hongyu%20Li%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Obstacle%20detection%20and%20tracking%20represent%20a%20critical%20component%20in%20robot%0Aautonomous%20navigation.%20In%20this%20paper%2C%20we%20propose%20ODTFormer%2C%20a%20Transformer-based%0Amodel%20to%20address%20both%20obstacle%20detection%20and%20tracking%20problems.%20For%20the%0Adetection%20task%2C%20our%20approach%20leverages%20deformable%20attention%20to%20construct%20a%203D%0Acost%20volume%2C%20which%20is%20decoded%20progressively%20in%20the%20form%20of%20voxel%20occupancy%0Agrids.%20We%20further%20track%20the%20obstacles%20by%20matching%20the%20voxels%20between%0Aconsecutive%20frames.%20The%20entire%20model%20can%20be%20optimized%20in%20an%20end-to-end%20manner.%0AThrough%20extensive%20experiments%20on%20DrivingStereo%20and%20KITTI%20benchmarks%2C%20our%20model%0Aachieves%20state-of-the-art%20performance%20in%20the%20obstacle%20detection%20task.%20We%20also%0Areport%20comparable%20accuracy%20to%20state-of-the-art%20obstacle%20tracking%20models%20while%0Arequiring%20only%20a%20fraction%20of%20their%20computation%20cost%2C%20typically%20ten-fold%20to%0Atwenty-fold%20less.%20The%20code%20and%20model%20weights%20will%20be%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14626v1&entry.124074799=Read"},
{"title": "Towards Efficient Information Fusion: Concentric Dual Fusion Attention\n  Based Multiple Instance Learning for Whole Slide Images", "author": "Yujian Liu and Ruoxuan Wu and Xinjie Shen and Zihuang Lu and Lingyu Liang and Haiyu Zhou and Shipu Xu and Shaoai Cai and Shidang Xu", "abstract": "  In the realm of digital pathology, multi-magnification Multiple Instance\nLearning (multi-mag MIL) has proven effective in leveraging the hierarchical\nstructure of Whole Slide Images (WSIs) to reduce information loss and redundant\ndata. However, current methods fall short in bridging the domain gap between\npretrained models and medical imaging, and often fail to account for spatial\nrelationships across different magnifications. Addressing these challenges, we\nintroduce the Concentric Dual Fusion Attention-MIL (CDFA-MIL) framework,which\ninnovatively combines point-to-area feature-colum attention and point-to-point\nconcentric-row attention using concentric patch. This approach is designed to\neffectively fuse correlated information, enhancing feature representation and\nproviding stronger correlation guidance for WSI analysis. CDFA-MIL\ndistinguishes itself by offering a robust fusion strategy that leads to\nsuperior WSI recognition. Its application has demonstrated exceptional\nperformance, significantly surpassing existing MIL methods in accuracy and F1\nscores on prominent datasets like Camelyon16 and TCGA-NSCLC. Specifically,\nCDFA-MIL achieved an average accuracy and F1-score of 93.7\\% and 94.1\\%\nrespectively on these datasets, marking a notable advancement over traditional\nMIL approaches.\n", "link": "http://arxiv.org/abs/2403.14346v1", "date": "2024-03-21", "relevancy": 2.086, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5061}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4962}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Information%20Fusion%3A%20Concentric%20Dual%20Fusion%20Attention%0A%20%20Based%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Images&body=Title%3A%20Towards%20Efficient%20Information%20Fusion%3A%20Concentric%20Dual%20Fusion%20Attention%0A%20%20Based%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Images%0AAuthor%3A%20Yujian%20Liu%20and%20Ruoxuan%20Wu%20and%20Xinjie%20Shen%20and%20Zihuang%20Lu%20and%20Lingyu%20Liang%20and%20Haiyu%20Zhou%20and%20Shipu%20Xu%20and%20Shaoai%20Cai%20and%20Shidang%20Xu%0AAbstract%3A%20%20%20In%20the%20realm%20of%20digital%20pathology%2C%20multi-magnification%20Multiple%20Instance%0ALearning%20%28multi-mag%20MIL%29%20has%20proven%20effective%20in%20leveraging%20the%20hierarchical%0Astructure%20of%20Whole%20Slide%20Images%20%28WSIs%29%20to%20reduce%20information%20loss%20and%20redundant%0Adata.%20However%2C%20current%20methods%20fall%20short%20in%20bridging%20the%20domain%20gap%20between%0Apretrained%20models%20and%20medical%20imaging%2C%20and%20often%20fail%20to%20account%20for%20spatial%0Arelationships%20across%20different%20magnifications.%20Addressing%20these%20challenges%2C%20we%0Aintroduce%20the%20Concentric%20Dual%20Fusion%20Attention-MIL%20%28CDFA-MIL%29%20framework%2Cwhich%0Ainnovatively%20combines%20point-to-area%20feature-colum%20attention%20and%20point-to-point%0Aconcentric-row%20attention%20using%20concentric%20patch.%20This%20approach%20is%20designed%20to%0Aeffectively%20fuse%20correlated%20information%2C%20enhancing%20feature%20representation%20and%0Aproviding%20stronger%20correlation%20guidance%20for%20WSI%20analysis.%20CDFA-MIL%0Adistinguishes%20itself%20by%20offering%20a%20robust%20fusion%20strategy%20that%20leads%20to%0Asuperior%20WSI%20recognition.%20Its%20application%20has%20demonstrated%20exceptional%0Aperformance%2C%20significantly%20surpassing%20existing%20MIL%20methods%20in%20accuracy%20and%20F1%0Ascores%20on%20prominent%20datasets%20like%20Camelyon16%20and%20TCGA-NSCLC.%20Specifically%2C%0ACDFA-MIL%20achieved%20an%20average%20accuracy%20and%20F1-score%20of%2093.7%5C%25%20and%2094.1%5C%25%0Arespectively%20on%20these%20datasets%2C%20marking%20a%20notable%20advancement%20over%20traditional%0AMIL%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14346v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Information%20Fusion%3A%20Concentric%20Dual%20Fusion%20Attention%0A%20%20Based%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Images&entry.906535625=Yujian%20Liu%20and%20Ruoxuan%20Wu%20and%20Xinjie%20Shen%20and%20Zihuang%20Lu%20and%20Lingyu%20Liang%20and%20Haiyu%20Zhou%20and%20Shipu%20Xu%20and%20Shaoai%20Cai%20and%20Shidang%20Xu&entry.1292438233=%20%20In%20the%20realm%20of%20digital%20pathology%2C%20multi-magnification%20Multiple%20Instance%0ALearning%20%28multi-mag%20MIL%29%20has%20proven%20effective%20in%20leveraging%20the%20hierarchical%0Astructure%20of%20Whole%20Slide%20Images%20%28WSIs%29%20to%20reduce%20information%20loss%20and%20redundant%0Adata.%20However%2C%20current%20methods%20fall%20short%20in%20bridging%20the%20domain%20gap%20between%0Apretrained%20models%20and%20medical%20imaging%2C%20and%20often%20fail%20to%20account%20for%20spatial%0Arelationships%20across%20different%20magnifications.%20Addressing%20these%20challenges%2C%20we%0Aintroduce%20the%20Concentric%20Dual%20Fusion%20Attention-MIL%20%28CDFA-MIL%29%20framework%2Cwhich%0Ainnovatively%20combines%20point-to-area%20feature-colum%20attention%20and%20point-to-point%0Aconcentric-row%20attention%20using%20concentric%20patch.%20This%20approach%20is%20designed%20to%0Aeffectively%20fuse%20correlated%20information%2C%20enhancing%20feature%20representation%20and%0Aproviding%20stronger%20correlation%20guidance%20for%20WSI%20analysis.%20CDFA-MIL%0Adistinguishes%20itself%20by%20offering%20a%20robust%20fusion%20strategy%20that%20leads%20to%0Asuperior%20WSI%20recognition.%20Its%20application%20has%20demonstrated%20exceptional%0Aperformance%2C%20significantly%20surpassing%20existing%20MIL%20methods%20in%20accuracy%20and%20F1%0Ascores%20on%20prominent%20datasets%20like%20Camelyon16%20and%20TCGA-NSCLC.%20Specifically%2C%0ACDFA-MIL%20achieved%20an%20average%20accuracy%20and%20F1-score%20of%2093.7%5C%25%20and%2094.1%5C%25%0Arespectively%20on%20these%20datasets%2C%20marking%20a%20notable%20advancement%20over%20traditional%0AMIL%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14346v1&entry.124074799=Read"},
{"title": "FFT-based Selection and Optimization of Statistics for Robust\n  Recognition of Severely Corrupted Images", "author": "Elena Camuffo and Umberto Michieli and Jijoong Moon and Daehyun Kim and Mete Ozay", "abstract": "  Improving model robustness in case of corrupted images is among the key\nchallenges to enable robust vision systems on smart devices, such as robotic\nagents. Particularly, robust test-time performance is imperative for most of\nthe applications. This paper presents a novel approach to improve robustness of\nany classification model, especially on severely corrupted images. Our method\n(FROST) employs high-frequency features to detect input image corruption type,\nand select layer-wise feature normalization statistics. FROST provides the\nstate-of-the-art results for different models and datasets, outperforming\ncompetitors on ImageNet-C by up to 37.1% relative gain, improving baseline of\n40.9% mCE on severe corruptions.\n", "link": "http://arxiv.org/abs/2403.14335v1", "date": "2024-03-21", "relevancy": 2.0857, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5339}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.516}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5111}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FFT-based%20Selection%20and%20Optimization%20of%20Statistics%20for%20Robust%0A%20%20Recognition%20of%20Severely%20Corrupted%20Images&body=Title%3A%20FFT-based%20Selection%20and%20Optimization%20of%20Statistics%20for%20Robust%0A%20%20Recognition%20of%20Severely%20Corrupted%20Images%0AAuthor%3A%20Elena%20Camuffo%20and%20Umberto%20Michieli%20and%20Jijoong%20Moon%20and%20Daehyun%20Kim%20and%20Mete%20Ozay%0AAbstract%3A%20%20%20Improving%20model%20robustness%20in%20case%20of%20corrupted%20images%20is%20among%20the%20key%0Achallenges%20to%20enable%20robust%20vision%20systems%20on%20smart%20devices%2C%20such%20as%20robotic%0Aagents.%20Particularly%2C%20robust%20test-time%20performance%20is%20imperative%20for%20most%20of%0Athe%20applications.%20This%20paper%20presents%20a%20novel%20approach%20to%20improve%20robustness%20of%0Aany%20classification%20model%2C%20especially%20on%20severely%20corrupted%20images.%20Our%20method%0A%28FROST%29%20employs%20high-frequency%20features%20to%20detect%20input%20image%20corruption%20type%2C%0Aand%20select%20layer-wise%20feature%20normalization%20statistics.%20FROST%20provides%20the%0Astate-of-the-art%20results%20for%20different%20models%20and%20datasets%2C%20outperforming%0Acompetitors%20on%20ImageNet-C%20by%20up%20to%2037.1%25%20relative%20gain%2C%20improving%20baseline%20of%0A40.9%25%20mCE%20on%20severe%20corruptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14335v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FFT-based%20Selection%20and%20Optimization%20of%20Statistics%20for%20Robust%0A%20%20Recognition%20of%20Severely%20Corrupted%20Images&entry.906535625=Elena%20Camuffo%20and%20Umberto%20Michieli%20and%20Jijoong%20Moon%20and%20Daehyun%20Kim%20and%20Mete%20Ozay&entry.1292438233=%20%20Improving%20model%20robustness%20in%20case%20of%20corrupted%20images%20is%20among%20the%20key%0Achallenges%20to%20enable%20robust%20vision%20systems%20on%20smart%20devices%2C%20such%20as%20robotic%0Aagents.%20Particularly%2C%20robust%20test-time%20performance%20is%20imperative%20for%20most%20of%0Athe%20applications.%20This%20paper%20presents%20a%20novel%20approach%20to%20improve%20robustness%20of%0Aany%20classification%20model%2C%20especially%20on%20severely%20corrupted%20images.%20Our%20method%0A%28FROST%29%20employs%20high-frequency%20features%20to%20detect%20input%20image%20corruption%20type%2C%0Aand%20select%20layer-wise%20feature%20normalization%20statistics.%20FROST%20provides%20the%0Astate-of-the-art%20results%20for%20different%20models%20and%20datasets%2C%20outperforming%0Acompetitors%20on%20ImageNet-C%20by%20up%20to%2037.1%25%20relative%20gain%2C%20improving%20baseline%20of%0A40.9%25%20mCE%20on%20severe%20corruptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14335v1&entry.124074799=Read"},
{"title": "FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware\n  Graph Transformer", "author": "Dongyeong Hwang and Hyunju Kim and Sunwoo Kim and Kijung Shin", "abstract": "  The success of a specific neural network architecture is closely tied to the\ndataset and task it tackles; there is no one-size-fits-all solution. Thus,\nconsiderable efforts have been made to quickly and accurately estimate the\nperformances of neural architectures, without full training or evaluation, for\ngiven tasks and datasets. Neural architecture encoding has played a crucial\nrole in the estimation, and graphbased methods, which treat an architecture as\na graph, have shown prominent performance. For enhanced representation learning\nof neural architectures, we introduce FlowerFormer, a powerful graph\ntransformer that incorporates the information flows within a neural\narchitecture. FlowerFormer consists of two key components: (a) bidirectional\nasynchronous message passing, inspired by the flows; (b) global attention built\non flow-based masking. Our extensive experiments demonstrate the superiority of\nFlowerFormer over existing neural encoding methods, and its effectiveness\nextends beyond computer vision models to include graph neural networks and auto\nspeech recognition models. Our code is available at\nhttp://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.\n", "link": "http://arxiv.org/abs/2403.12821v2", "date": "2024-03-21", "relevancy": 2.0797, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6211}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5145}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4849}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FlowerFormer%3A%20Empowering%20Neural%20Architecture%20Encoding%20using%20a%20Flow-aware%0A%20%20Graph%20Transformer&body=Title%3A%20FlowerFormer%3A%20Empowering%20Neural%20Architecture%20Encoding%20using%20a%20Flow-aware%0A%20%20Graph%20Transformer%0AAuthor%3A%20Dongyeong%20Hwang%20and%20Hyunju%20Kim%20and%20Sunwoo%20Kim%20and%20Kijung%20Shin%0AAbstract%3A%20%20%20The%20success%20of%20a%20specific%20neural%20network%20architecture%20is%20closely%20tied%20to%20the%0Adataset%20and%20task%20it%20tackles%3B%20there%20is%20no%20one-size-fits-all%20solution.%20Thus%2C%0Aconsiderable%20efforts%20have%20been%20made%20to%20quickly%20and%20accurately%20estimate%20the%0Aperformances%20of%20neural%20architectures%2C%20without%20full%20training%20or%20evaluation%2C%20for%0Agiven%20tasks%20and%20datasets.%20Neural%20architecture%20encoding%20has%20played%20a%20crucial%0Arole%20in%20the%20estimation%2C%20and%20graphbased%20methods%2C%20which%20treat%20an%20architecture%20as%0Aa%20graph%2C%20have%20shown%20prominent%20performance.%20For%20enhanced%20representation%20learning%0Aof%20neural%20architectures%2C%20we%20introduce%20FlowerFormer%2C%20a%20powerful%20graph%0Atransformer%20that%20incorporates%20the%20information%20flows%20within%20a%20neural%0Aarchitecture.%20FlowerFormer%20consists%20of%20two%20key%20components%3A%20%28a%29%20bidirectional%0Aasynchronous%20message%20passing%2C%20inspired%20by%20the%20flows%3B%20%28b%29%20global%20attention%20built%0Aon%20flow-based%20masking.%20Our%20extensive%20experiments%20demonstrate%20the%20superiority%20of%0AFlowerFormer%20over%20existing%20neural%20encoding%20methods%2C%20and%20its%20effectiveness%0Aextends%20beyond%20computer%20vision%20models%20to%20include%20graph%20neural%20networks%20and%20auto%0Aspeech%20recognition%20models.%20Our%20code%20is%20available%20at%0Ahttp%3A//github.com/y0ngjaenius/CVPR2024_FLOWERFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12821v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowerFormer%3A%20Empowering%20Neural%20Architecture%20Encoding%20using%20a%20Flow-aware%0A%20%20Graph%20Transformer&entry.906535625=Dongyeong%20Hwang%20and%20Hyunju%20Kim%20and%20Sunwoo%20Kim%20and%20Kijung%20Shin&entry.1292438233=%20%20The%20success%20of%20a%20specific%20neural%20network%20architecture%20is%20closely%20tied%20to%20the%0Adataset%20and%20task%20it%20tackles%3B%20there%20is%20no%20one-size-fits-all%20solution.%20Thus%2C%0Aconsiderable%20efforts%20have%20been%20made%20to%20quickly%20and%20accurately%20estimate%20the%0Aperformances%20of%20neural%20architectures%2C%20without%20full%20training%20or%20evaluation%2C%20for%0Agiven%20tasks%20and%20datasets.%20Neural%20architecture%20encoding%20has%20played%20a%20crucial%0Arole%20in%20the%20estimation%2C%20and%20graphbased%20methods%2C%20which%20treat%20an%20architecture%20as%0Aa%20graph%2C%20have%20shown%20prominent%20performance.%20For%20enhanced%20representation%20learning%0Aof%20neural%20architectures%2C%20we%20introduce%20FlowerFormer%2C%20a%20powerful%20graph%0Atransformer%20that%20incorporates%20the%20information%20flows%20within%20a%20neural%0Aarchitecture.%20FlowerFormer%20consists%20of%20two%20key%20components%3A%20%28a%29%20bidirectional%0Aasynchronous%20message%20passing%2C%20inspired%20by%20the%20flows%3B%20%28b%29%20global%20attention%20built%0Aon%20flow-based%20masking.%20Our%20extensive%20experiments%20demonstrate%20the%20superiority%20of%0AFlowerFormer%20over%20existing%20neural%20encoding%20methods%2C%20and%20its%20effectiveness%0Aextends%20beyond%20computer%20vision%20models%20to%20include%20graph%20neural%20networks%20and%20auto%0Aspeech%20recognition%20models.%20Our%20code%20is%20available%20at%0Ahttp%3A//github.com/y0ngjaenius/CVPR2024_FLOWERFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12821v2&entry.124074799=Read"},
{"title": "Simplified Diffusion Schr\u00f6dinger Bridge", "author": "Zhicong Tang and Tiankai Hang and Shuyang Gu and Dong Chen and Baining Guo", "abstract": "  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.\n", "link": "http://arxiv.org/abs/2403.14623v1", "date": "2024-03-21", "relevancy": 2.0766, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5478}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5162}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5106}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simplified%20Diffusion%20Schr%C3%B6dinger%20Bridge&body=Title%3A%20Simplified%20Diffusion%20Schr%C3%B6dinger%20Bridge%0AAuthor%3A%20Zhicong%20Tang%20and%20Tiankai%20Hang%20and%20Shuyang%20Gu%20and%20Dong%20Chen%20and%20Baining%20Guo%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20theoretical%20simplification%20of%20the%20Diffusion%0ASchr%5C%22odinger%20Bridge%20%28DSB%29%20that%20facilitates%20its%20unification%20with%20Score-based%0AGenerative%20Models%20%28SGMs%29%2C%20addressing%20the%20limitations%20of%20DSB%20in%20complex%20data%0Ageneration%20and%20enabling%20faster%20convergence%20and%20enhanced%20performance.%20By%0Aemploying%20SGMs%20as%20an%20initial%20solution%20for%20DSB%2C%20our%20approach%20capitalizes%20on%20the%0Astrengths%20of%20both%20frameworks%2C%20ensuring%20a%20more%20efficient%20training%20process%20and%0Aimproving%20the%20performance%20of%20SGM.%20We%20also%20propose%20a%20reparameterization%0Atechnique%20that%2C%20despite%20theoretical%20approximations%2C%20practically%20improves%20the%0Anetwork%27s%20fitting%20capabilities.%20Our%20extensive%20experimental%20evaluations%20confirm%0Athe%20effectiveness%20of%20the%20simplified%20DSB%2C%20demonstrating%20its%20significant%0Aimprovements.%20We%20believe%20the%20contributions%20of%20this%20work%20pave%20the%20way%20for%0Aadvanced%20generative%20modeling.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14623v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplified%20Diffusion%20Schr%C3%B6dinger%20Bridge&entry.906535625=Zhicong%20Tang%20and%20Tiankai%20Hang%20and%20Shuyang%20Gu%20and%20Dong%20Chen%20and%20Baining%20Guo&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20theoretical%20simplification%20of%20the%20Diffusion%0ASchr%5C%22odinger%20Bridge%20%28DSB%29%20that%20facilitates%20its%20unification%20with%20Score-based%0AGenerative%20Models%20%28SGMs%29%2C%20addressing%20the%20limitations%20of%20DSB%20in%20complex%20data%0Ageneration%20and%20enabling%20faster%20convergence%20and%20enhanced%20performance.%20By%0Aemploying%20SGMs%20as%20an%20initial%20solution%20for%20DSB%2C%20our%20approach%20capitalizes%20on%20the%0Astrengths%20of%20both%20frameworks%2C%20ensuring%20a%20more%20efficient%20training%20process%20and%0Aimproving%20the%20performance%20of%20SGM.%20We%20also%20propose%20a%20reparameterization%0Atechnique%20that%2C%20despite%20theoretical%20approximations%2C%20practically%20improves%20the%0Anetwork%27s%20fitting%20capabilities.%20Our%20extensive%20experimental%20evaluations%20confirm%0Athe%20effectiveness%20of%20the%20simplified%20DSB%2C%20demonstrating%20its%20significant%0Aimprovements.%20We%20believe%20the%20contributions%20of%20this%20work%20pave%20the%20way%20for%0Aadvanced%20generative%20modeling.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14623v1&entry.124074799=Read"},
{"title": "Alleviating Exposure Bias in Diffusion Models through Sampling with\n  Shifted Time Steps", "author": "Mingxiao Li and Tingyu Qu and Ruicong Yao and Wei Sun and Marie-Francine Moens", "abstract": "  Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the\nsynthesis of high-quality images. However, their inference process\ncharacteristically requires numerous, potentially hundreds, of iterative steps,\nwhich could exaggerate the problem of exposure bias due to the training and\ninference discrepancy. Previous work has attempted to mitigate this issue by\nperturbing inputs during training, which consequently mandates the retraining\nof the DPM. In this work, we conduct a systematic study of exposure bias in DPM\nand, intriguingly, we find that the exposure bias could be alleviated with a\nnovel sampling method that we propose, without retraining the model. We\nempirically and theoretically show that, during inference, for each backward\ntime step $t$ and corresponding state $\\hat{x}_t$, there might exist another\ntime step $t_s$ which exhibits superior coupling with $\\hat{x}_t$. Based on\nthis finding, we introduce a sampling method named Time-Shift Sampler. Our\nframework can be seamlessly integrated to existing sampling algorithms, such as\nDDPM, DDIM and other high-order solvers, inducing merely minimal additional\ncomputations. Experimental results show our method brings significant and\nconsistent improvements in FID scores on different datasets and sampling\nmethods. For example, integrating Time-Shift Sampler to F-PNDM yields a\nFID=3.88, achieving 44.49\\% improvements as compared to F-PNDM, on CIFAR-10\nwith 10 sampling steps, which is more performant than the vanilla DDIM with 100\nsampling steps. Our code is available at https://github.com/Mingxiao-Li/TS-DPM.\n", "link": "http://arxiv.org/abs/2305.15583v7", "date": "2024-03-21", "relevancy": 2.0765, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.547}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5036}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4882}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Alleviating%20Exposure%20Bias%20in%20Diffusion%20Models%20through%20Sampling%20with%0A%20%20Shifted%20Time%20Steps&body=Title%3A%20Alleviating%20Exposure%20Bias%20in%20Diffusion%20Models%20through%20Sampling%20with%0A%20%20Shifted%20Time%20Steps%0AAuthor%3A%20Mingxiao%20Li%20and%20Tingyu%20Qu%20and%20Ruicong%20Yao%20and%20Wei%20Sun%20and%20Marie-Francine%20Moens%0AAbstract%3A%20%20%20Diffusion%20Probabilistic%20Models%20%28DPM%29%20have%20shown%20remarkable%20efficacy%20in%20the%0Asynthesis%20of%20high-quality%20images.%20However%2C%20their%20inference%20process%0Acharacteristically%20requires%20numerous%2C%20potentially%20hundreds%2C%20of%20iterative%20steps%2C%0Awhich%20could%20exaggerate%20the%20problem%20of%20exposure%20bias%20due%20to%20the%20training%20and%0Ainference%20discrepancy.%20Previous%20work%20has%20attempted%20to%20mitigate%20this%20issue%20by%0Aperturbing%20inputs%20during%20training%2C%20which%20consequently%20mandates%20the%20retraining%0Aof%20the%20DPM.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20study%20of%20exposure%20bias%20in%20DPM%0Aand%2C%20intriguingly%2C%20we%20find%20that%20the%20exposure%20bias%20could%20be%20alleviated%20with%20a%0Anovel%20sampling%20method%20that%20we%20propose%2C%20without%20retraining%20the%20model.%20We%0Aempirically%20and%20theoretically%20show%20that%2C%20during%20inference%2C%20for%20each%20backward%0Atime%20step%20%24t%24%20and%20corresponding%20state%20%24%5Chat%7Bx%7D_t%24%2C%20there%20might%20exist%20another%0Atime%20step%20%24t_s%24%20which%20exhibits%20superior%20coupling%20with%20%24%5Chat%7Bx%7D_t%24.%20Based%20on%0Athis%20finding%2C%20we%20introduce%20a%20sampling%20method%20named%20Time-Shift%20Sampler.%20Our%0Aframework%20can%20be%20seamlessly%20integrated%20to%20existing%20sampling%20algorithms%2C%20such%20as%0ADDPM%2C%20DDIM%20and%20other%20high-order%20solvers%2C%20inducing%20merely%20minimal%20additional%0Acomputations.%20Experimental%20results%20show%20our%20method%20brings%20significant%20and%0Aconsistent%20improvements%20in%20FID%20scores%20on%20different%20datasets%20and%20sampling%0Amethods.%20For%20example%2C%20integrating%20Time-Shift%20Sampler%20to%20F-PNDM%20yields%20a%0AFID%3D3.88%2C%20achieving%2044.49%5C%25%20improvements%20as%20compared%20to%20F-PNDM%2C%20on%20CIFAR-10%0Awith%2010%20sampling%20steps%2C%20which%20is%20more%20performant%20than%20the%20vanilla%20DDIM%20with%20100%0Asampling%20steps.%20Our%20code%20is%20available%20at%20https%3A//github.com/Mingxiao-Li/TS-DPM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15583v7", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alleviating%20Exposure%20Bias%20in%20Diffusion%20Models%20through%20Sampling%20with%0A%20%20Shifted%20Time%20Steps&entry.906535625=Mingxiao%20Li%20and%20Tingyu%20Qu%20and%20Ruicong%20Yao%20and%20Wei%20Sun%20and%20Marie-Francine%20Moens&entry.1292438233=%20%20Diffusion%20Probabilistic%20Models%20%28DPM%29%20have%20shown%20remarkable%20efficacy%20in%20the%0Asynthesis%20of%20high-quality%20images.%20However%2C%20their%20inference%20process%0Acharacteristically%20requires%20numerous%2C%20potentially%20hundreds%2C%20of%20iterative%20steps%2C%0Awhich%20could%20exaggerate%20the%20problem%20of%20exposure%20bias%20due%20to%20the%20training%20and%0Ainference%20discrepancy.%20Previous%20work%20has%20attempted%20to%20mitigate%20this%20issue%20by%0Aperturbing%20inputs%20during%20training%2C%20which%20consequently%20mandates%20the%20retraining%0Aof%20the%20DPM.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20study%20of%20exposure%20bias%20in%20DPM%0Aand%2C%20intriguingly%2C%20we%20find%20that%20the%20exposure%20bias%20could%20be%20alleviated%20with%20a%0Anovel%20sampling%20method%20that%20we%20propose%2C%20without%20retraining%20the%20model.%20We%0Aempirically%20and%20theoretically%20show%20that%2C%20during%20inference%2C%20for%20each%20backward%0Atime%20step%20%24t%24%20and%20corresponding%20state%20%24%5Chat%7Bx%7D_t%24%2C%20there%20might%20exist%20another%0Atime%20step%20%24t_s%24%20which%20exhibits%20superior%20coupling%20with%20%24%5Chat%7Bx%7D_t%24.%20Based%20on%0Athis%20finding%2C%20we%20introduce%20a%20sampling%20method%20named%20Time-Shift%20Sampler.%20Our%0Aframework%20can%20be%20seamlessly%20integrated%20to%20existing%20sampling%20algorithms%2C%20such%20as%0ADDPM%2C%20DDIM%20and%20other%20high-order%20solvers%2C%20inducing%20merely%20minimal%20additional%0Acomputations.%20Experimental%20results%20show%20our%20method%20brings%20significant%20and%0Aconsistent%20improvements%20in%20FID%20scores%20on%20different%20datasets%20and%20sampling%0Amethods.%20For%20example%2C%20integrating%20Time-Shift%20Sampler%20to%20F-PNDM%20yields%20a%0AFID%3D3.88%2C%20achieving%2044.49%5C%25%20improvements%20as%20compared%20to%20F-PNDM%2C%20on%20CIFAR-10%0Awith%2010%20sampling%20steps%2C%20which%20is%20more%20performant%20than%20the%20vanilla%20DDIM%20with%20100%0Asampling%20steps.%20Our%20code%20is%20available%20at%20https%3A//github.com/Mingxiao-Li/TS-DPM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15583v7&entry.124074799=Read"},
{"title": "DenseFormer: Enhancing Information Flow in Transformers via Depth\n  Weighted Averaging", "author": "Matteo Pagliardini and Amirkeivan Mohtashami and Francois Fleuret and Martin Jaggi", "abstract": "  The transformer architecture by Vaswani et al. (2017) is now ubiquitous\nacross application domains, from natural language processing to speech\nprocessing and image understanding. We propose DenseFormer, a simple\nmodification to the standard architecture that improves the perplexity of the\nmodel without increasing its size -- adding a few thousand parameters for\nlarge-scale models in the 100B parameters range. Our approach relies on an\nadditional averaging step after each transformer block, which computes a\nweighted average of current and past representations -- we refer to this\noperation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit\ncoherent patterns of information flow, revealing the strong and structured\nreuse of activations from distant layers. Experiments demonstrate that\nDenseFormer is more data efficient, reaching the same perplexity of much deeper\ntransformer models, and that for the same perplexity, these new models\noutperform transformer baselines in terms of memory efficiency and inference\ntime.\n", "link": "http://arxiv.org/abs/2402.02622v2", "date": "2024-03-21", "relevancy": 2.0725, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6185}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5209}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4752}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DenseFormer%3A%20Enhancing%20Information%20Flow%20in%20Transformers%20via%20Depth%0A%20%20Weighted%20Averaging&body=Title%3A%20DenseFormer%3A%20Enhancing%20Information%20Flow%20in%20Transformers%20via%20Depth%0A%20%20Weighted%20Averaging%0AAuthor%3A%20Matteo%20Pagliardini%20and%20Amirkeivan%20Mohtashami%20and%20Francois%20Fleuret%20and%20Martin%20Jaggi%0AAbstract%3A%20%20%20The%20transformer%20architecture%20by%20Vaswani%20et%20al.%20%282017%29%20is%20now%20ubiquitous%0Aacross%20application%20domains%2C%20from%20natural%20language%20processing%20to%20speech%0Aprocessing%20and%20image%20understanding.%20We%20propose%20DenseFormer%2C%20a%20simple%0Amodification%20to%20the%20standard%20architecture%20that%20improves%20the%20perplexity%20of%20the%0Amodel%20without%20increasing%20its%20size%20--%20adding%20a%20few%20thousand%20parameters%20for%0Alarge-scale%20models%20in%20the%20100B%20parameters%20range.%20Our%20approach%20relies%20on%20an%0Aadditional%20averaging%20step%20after%20each%20transformer%20block%2C%20which%20computes%20a%0Aweighted%20average%20of%20current%20and%20past%20representations%20--%20we%20refer%20to%20this%0Aoperation%20as%20Depth-Weighted-Average%20%28DWA%29.%20The%20learned%20DWA%20weights%20exhibit%0Acoherent%20patterns%20of%20information%20flow%2C%20revealing%20the%20strong%20and%20structured%0Areuse%20of%20activations%20from%20distant%20layers.%20Experiments%20demonstrate%20that%0ADenseFormer%20is%20more%20data%20efficient%2C%20reaching%20the%20same%20perplexity%20of%20much%20deeper%0Atransformer%20models%2C%20and%20that%20for%20the%20same%20perplexity%2C%20these%20new%20models%0Aoutperform%20transformer%20baselines%20in%20terms%20of%20memory%20efficiency%20and%20inference%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02622v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenseFormer%3A%20Enhancing%20Information%20Flow%20in%20Transformers%20via%20Depth%0A%20%20Weighted%20Averaging&entry.906535625=Matteo%20Pagliardini%20and%20Amirkeivan%20Mohtashami%20and%20Francois%20Fleuret%20and%20Martin%20Jaggi&entry.1292438233=%20%20The%20transformer%20architecture%20by%20Vaswani%20et%20al.%20%282017%29%20is%20now%20ubiquitous%0Aacross%20application%20domains%2C%20from%20natural%20language%20processing%20to%20speech%0Aprocessing%20and%20image%20understanding.%20We%20propose%20DenseFormer%2C%20a%20simple%0Amodification%20to%20the%20standard%20architecture%20that%20improves%20the%20perplexity%20of%20the%0Amodel%20without%20increasing%20its%20size%20--%20adding%20a%20few%20thousand%20parameters%20for%0Alarge-scale%20models%20in%20the%20100B%20parameters%20range.%20Our%20approach%20relies%20on%20an%0Aadditional%20averaging%20step%20after%20each%20transformer%20block%2C%20which%20computes%20a%0Aweighted%20average%20of%20current%20and%20past%20representations%20--%20we%20refer%20to%20this%0Aoperation%20as%20Depth-Weighted-Average%20%28DWA%29.%20The%20learned%20DWA%20weights%20exhibit%0Acoherent%20patterns%20of%20information%20flow%2C%20revealing%20the%20strong%20and%20structured%0Areuse%20of%20activations%20from%20distant%20layers.%20Experiments%20demonstrate%20that%0ADenseFormer%20is%20more%20data%20efficient%2C%20reaching%20the%20same%20perplexity%20of%20much%20deeper%0Atransformer%20models%2C%20and%20that%20for%20the%20same%20perplexity%2C%20these%20new%20models%0Aoutperform%20transformer%20baselines%20in%20terms%20of%20memory%20efficiency%20and%20inference%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02622v2&entry.124074799=Read"},
{"title": "Intelligent Canvas: Enabling Design-Like Exploratory Visual Data\n  Analysis with Generative AI through Rapid Prototyping, Iteration and Curation", "author": "Zijian Ding and Joel Chan", "abstract": "  Complex data analysis inherently seeks unexpected insights through\nexploratory visual analysis methods, transcending logical, step-by-step\nprocessing. However, existing interfaces such as notebooks and dashboards have\nlimitations in exploration and comparison for visual data analysis. Addressing\nthese limitations, we introduce a \"design-like\" intelligent canvas environment\nintegrating generative AI into data analysis, offering rapid prototyping,\niteration, and comparative visualization management. Our dual contributions\ninclude the integration of generative AI components into a canvas interface,\nand empirical findings from a user study (N=10) evaluating the effectiveness of\nthe canvas interface.\n", "link": "http://arxiv.org/abs/2402.08812v3", "date": "2024-03-21", "relevancy": 2.0589, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5519}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5341}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4805}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Canvas%3A%20Enabling%20Design-Like%20Exploratory%20Visual%20Data%0A%20%20Analysis%20with%20Generative%20AI%20through%20Rapid%20Prototyping%2C%20Iteration%20and%20Curation&body=Title%3A%20Intelligent%20Canvas%3A%20Enabling%20Design-Like%20Exploratory%20Visual%20Data%0A%20%20Analysis%20with%20Generative%20AI%20through%20Rapid%20Prototyping%2C%20Iteration%20and%20Curation%0AAuthor%3A%20Zijian%20Ding%20and%20Joel%20Chan%0AAbstract%3A%20%20%20Complex%20data%20analysis%20inherently%20seeks%20unexpected%20insights%20through%0Aexploratory%20visual%20analysis%20methods%2C%20transcending%20logical%2C%20step-by-step%0Aprocessing.%20However%2C%20existing%20interfaces%20such%20as%20notebooks%20and%20dashboards%20have%0Alimitations%20in%20exploration%20and%20comparison%20for%20visual%20data%20analysis.%20Addressing%0Athese%20limitations%2C%20we%20introduce%20a%20%22design-like%22%20intelligent%20canvas%20environment%0Aintegrating%20generative%20AI%20into%20data%20analysis%2C%20offering%20rapid%20prototyping%2C%0Aiteration%2C%20and%20comparative%20visualization%20management.%20Our%20dual%20contributions%0Ainclude%20the%20integration%20of%20generative%20AI%20components%20into%20a%20canvas%20interface%2C%0Aand%20empirical%20findings%20from%20a%20user%20study%20%28N%3D10%29%20evaluating%20the%20effectiveness%20of%0Athe%20canvas%20interface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08812v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Canvas%3A%20Enabling%20Design-Like%20Exploratory%20Visual%20Data%0A%20%20Analysis%20with%20Generative%20AI%20through%20Rapid%20Prototyping%2C%20Iteration%20and%20Curation&entry.906535625=Zijian%20Ding%20and%20Joel%20Chan&entry.1292438233=%20%20Complex%20data%20analysis%20inherently%20seeks%20unexpected%20insights%20through%0Aexploratory%20visual%20analysis%20methods%2C%20transcending%20logical%2C%20step-by-step%0Aprocessing.%20However%2C%20existing%20interfaces%20such%20as%20notebooks%20and%20dashboards%20have%0Alimitations%20in%20exploration%20and%20comparison%20for%20visual%20data%20analysis.%20Addressing%0Athese%20limitations%2C%20we%20introduce%20a%20%22design-like%22%20intelligent%20canvas%20environment%0Aintegrating%20generative%20AI%20into%20data%20analysis%2C%20offering%20rapid%20prototyping%2C%0Aiteration%2C%20and%20comparative%20visualization%20management.%20Our%20dual%20contributions%0Ainclude%20the%20integration%20of%20generative%20AI%20components%20into%20a%20canvas%20interface%2C%0Aand%20empirical%20findings%20from%20a%20user%20study%20%28N%3D10%29%20evaluating%20the%20effectiveness%20of%0Athe%20canvas%20interface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08812v3&entry.124074799=Read"},
{"title": "Star-Searcher: A Complete and Efficient Aerial System for Autonomous\n  Target Search in Complex Unknown Environments", "author": "Yiming Luo and Zixuan Zhuang and Neng Pan and Chen Feng and Shaojie Shen and Fei Gao and Hui Cheng and Boyu Zhou", "abstract": "  This paper tackles the challenge of autonomous target search using unmanned\naerial vehicles (UAVs) in complex unknown environments. To fill the gap in\nsystematic approaches for this task, we introduce Star-Searcher, an aerial\nsystem featuring specialized sensor suites, mapping, and planning modules to\noptimize searching. Path planning challenges due to increased inspection\nrequirements are addressed through a hierarchical planner with a\nvisibility-based viewpoint clustering method. This simplifies planning by\nbreaking it into global and local sub-problems, ensuring efficient global and\nlocal path coverage in real-time. Furthermore, our global path planning employs\na history-aware mechanism to reduce motion inconsistency from frequent map\nchanges, significantly enhancing search efficiency. We conduct comparisons with\nstate-of-the-art methods in both simulation and the real world, demonstrating\nshorter flight paths, reduced time, and higher target search completeness. Our\napproach will be open-sourced for community benefit at\nhttps://github.com/SYSU-STAR/STAR-Searcher.\n", "link": "http://arxiv.org/abs/2402.16348v2", "date": "2024-03-21", "relevancy": 2.0523, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5396}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5293}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.48}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Star-Searcher%3A%20A%20Complete%20and%20Efficient%20Aerial%20System%20for%20Autonomous%0A%20%20Target%20Search%20in%20Complex%20Unknown%20Environments&body=Title%3A%20Star-Searcher%3A%20A%20Complete%20and%20Efficient%20Aerial%20System%20for%20Autonomous%0A%20%20Target%20Search%20in%20Complex%20Unknown%20Environments%0AAuthor%3A%20Yiming%20Luo%20and%20Zixuan%20Zhuang%20and%20Neng%20Pan%20and%20Chen%20Feng%20and%20Shaojie%20Shen%20and%20Fei%20Gao%20and%20Hui%20Cheng%20and%20Boyu%20Zhou%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20challenge%20of%20autonomous%20target%20search%20using%20unmanned%0Aaerial%20vehicles%20%28UAVs%29%20in%20complex%20unknown%20environments.%20To%20fill%20the%20gap%20in%0Asystematic%20approaches%20for%20this%20task%2C%20we%20introduce%20Star-Searcher%2C%20an%20aerial%0Asystem%20featuring%20specialized%20sensor%20suites%2C%20mapping%2C%20and%20planning%20modules%20to%0Aoptimize%20searching.%20Path%20planning%20challenges%20due%20to%20increased%20inspection%0Arequirements%20are%20addressed%20through%20a%20hierarchical%20planner%20with%20a%0Avisibility-based%20viewpoint%20clustering%20method.%20This%20simplifies%20planning%20by%0Abreaking%20it%20into%20global%20and%20local%20sub-problems%2C%20ensuring%20efficient%20global%20and%0Alocal%20path%20coverage%20in%20real-time.%20Furthermore%2C%20our%20global%20path%20planning%20employs%0Aa%20history-aware%20mechanism%20to%20reduce%20motion%20inconsistency%20from%20frequent%20map%0Achanges%2C%20significantly%20enhancing%20search%20efficiency.%20We%20conduct%20comparisons%20with%0Astate-of-the-art%20methods%20in%20both%20simulation%20and%20the%20real%20world%2C%20demonstrating%0Ashorter%20flight%20paths%2C%20reduced%20time%2C%20and%20higher%20target%20search%20completeness.%20Our%0Aapproach%20will%20be%20open-sourced%20for%20community%20benefit%20at%0Ahttps%3A//github.com/SYSU-STAR/STAR-Searcher.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16348v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Star-Searcher%3A%20A%20Complete%20and%20Efficient%20Aerial%20System%20for%20Autonomous%0A%20%20Target%20Search%20in%20Complex%20Unknown%20Environments&entry.906535625=Yiming%20Luo%20and%20Zixuan%20Zhuang%20and%20Neng%20Pan%20and%20Chen%20Feng%20and%20Shaojie%20Shen%20and%20Fei%20Gao%20and%20Hui%20Cheng%20and%20Boyu%20Zhou&entry.1292438233=%20%20This%20paper%20tackles%20the%20challenge%20of%20autonomous%20target%20search%20using%20unmanned%0Aaerial%20vehicles%20%28UAVs%29%20in%20complex%20unknown%20environments.%20To%20fill%20the%20gap%20in%0Asystematic%20approaches%20for%20this%20task%2C%20we%20introduce%20Star-Searcher%2C%20an%20aerial%0Asystem%20featuring%20specialized%20sensor%20suites%2C%20mapping%2C%20and%20planning%20modules%20to%0Aoptimize%20searching.%20Path%20planning%20challenges%20due%20to%20increased%20inspection%0Arequirements%20are%20addressed%20through%20a%20hierarchical%20planner%20with%20a%0Avisibility-based%20viewpoint%20clustering%20method.%20This%20simplifies%20planning%20by%0Abreaking%20it%20into%20global%20and%20local%20sub-problems%2C%20ensuring%20efficient%20global%20and%0Alocal%20path%20coverage%20in%20real-time.%20Furthermore%2C%20our%20global%20path%20planning%20employs%0Aa%20history-aware%20mechanism%20to%20reduce%20motion%20inconsistency%20from%20frequent%20map%0Achanges%2C%20significantly%20enhancing%20search%20efficiency.%20We%20conduct%20comparisons%20with%0Astate-of-the-art%20methods%20in%20both%20simulation%20and%20the%20real%20world%2C%20demonstrating%0Ashorter%20flight%20paths%2C%20reduced%20time%2C%20and%20higher%20target%20search%20completeness.%20Our%0Aapproach%20will%20be%20open-sourced%20for%20community%20benefit%20at%0Ahttps%3A//github.com/SYSU-STAR/STAR-Searcher.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16348v2&entry.124074799=Read"},
{"title": "Machine-learning invariant foliations in forced systems for reduced\n  order modelling", "author": "Robert Szalai", "abstract": "  We identify reduced order models (ROM) of forced systems from data using\ninvariant foliations. The forcing can be external, parametric, periodic or\nquasi-periodic. The process has four steps: 1. identify an approximate\ninvariant torus and the linear dynamics about the torus; 2. identify a globally\ndefined invariant foliation about the torus; 3. identify a local foliation\nabout an invariant manifold that complements the global foliation 4. extract\nthe invariant manifold as the leaf going through the torus and interpret the\nresult. We combine steps 2 and 3, so that we can track the location of the\ninvariant torus and scale the invariance equations appropriately. We highlight\nsome fundamental limitations of invariant manifolds and foliations when fitting\nthem to data, that require further mathematics to resolve.\n", "link": "http://arxiv.org/abs/2403.14514v1", "date": "2024-03-21", "relevancy": 1.2561, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4531}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3947}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Machine-learning%20invariant%20foliations%20in%20forced%20systems%20for%20reduced%0A%20%20order%20modelling&body=Title%3A%20Machine-learning%20invariant%20foliations%20in%20forced%20systems%20for%20reduced%0A%20%20order%20modelling%0AAuthor%3A%20Robert%20Szalai%0AAbstract%3A%20%20%20We%20identify%20reduced%20order%20models%20%28ROM%29%20of%20forced%20systems%20from%20data%20using%0Ainvariant%20foliations.%20The%20forcing%20can%20be%20external%2C%20parametric%2C%20periodic%20or%0Aquasi-periodic.%20The%20process%20has%20four%20steps%3A%201.%20identify%20an%20approximate%0Ainvariant%20torus%20and%20the%20linear%20dynamics%20about%20the%20torus%3B%202.%20identify%20a%20globally%0Adefined%20invariant%20foliation%20about%20the%20torus%3B%203.%20identify%20a%20local%20foliation%0Aabout%20an%20invariant%20manifold%20that%20complements%20the%20global%20foliation%204.%20extract%0Athe%20invariant%20manifold%20as%20the%20leaf%20going%20through%20the%20torus%20and%20interpret%20the%0Aresult.%20We%20combine%20steps%202%20and%203%2C%20so%20that%20we%20can%20track%20the%20location%20of%20the%0Ainvariant%20torus%20and%20scale%20the%20invariance%20equations%20appropriately.%20We%20highlight%0Asome%20fundamental%20limitations%20of%20invariant%20manifolds%20and%20foliations%20when%20fitting%0Athem%20to%20data%2C%20that%20require%20further%20mathematics%20to%20resolve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14514v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine-learning%20invariant%20foliations%20in%20forced%20systems%20for%20reduced%0A%20%20order%20modelling&entry.906535625=Robert%20Szalai&entry.1292438233=%20%20We%20identify%20reduced%20order%20models%20%28ROM%29%20of%20forced%20systems%20from%20data%20using%0Ainvariant%20foliations.%20The%20forcing%20can%20be%20external%2C%20parametric%2C%20periodic%20or%0Aquasi-periodic.%20The%20process%20has%20four%20steps%3A%201.%20identify%20an%20approximate%0Ainvariant%20torus%20and%20the%20linear%20dynamics%20about%20the%20torus%3B%202.%20identify%20a%20globally%0Adefined%20invariant%20foliation%20about%20the%20torus%3B%203.%20identify%20a%20local%20foliation%0Aabout%20an%20invariant%20manifold%20that%20complements%20the%20global%20foliation%204.%20extract%0Athe%20invariant%20manifold%20as%20the%20leaf%20going%20through%20the%20torus%20and%20interpret%20the%0Aresult.%20We%20combine%20steps%202%20and%203%2C%20so%20that%20we%20can%20track%20the%20location%20of%20the%0Ainvariant%20torus%20and%20scale%20the%20invariance%20equations%20appropriately.%20We%20highlight%0Asome%20fundamental%20limitations%20of%20invariant%20manifolds%20and%20foliations%20when%20fitting%0Athem%20to%20data%2C%20that%20require%20further%20mathematics%20to%20resolve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14514v1&entry.124074799=Read"},
{"title": "A Differentially Private Clustering Algorithm for Well-Clustered Graphs", "author": "Weiqiang He and Hendrik Fichtenberger and Pan Peng", "abstract": "  We study differentially private (DP) algorithms for recovering clusters in\nwell-clustered graphs, which are graphs whose vertex set can be partitioned\ninto a small number of sets, each inducing a subgraph of high inner conductance\nand small outer conductance. Such graphs have widespread application as a\nbenchmark in the theoretical analysis of spectral clustering. We provide an\nefficient ($\\epsilon$,$\\delta$)-DP algorithm tailored specifically for such\ngraphs. Our algorithm draws inspiration from the recent work of Chen et al.,\nwho developed DP algorithms for recovery of stochastic block models in cases\nwhere the graph comprises exactly two nearly-balanced clusters. Our algorithm\nworks for well-clustered graphs with $k$ nearly-balanced clusters, and the\nmisclassification ratio almost matches the one of the best-known non-private\nalgorithms. We conduct experimental evaluations on datasets with known ground\ntruth clusters to substantiate the prowess of our algorithm. We also show that\nany (pure) $\\epsilon$-DP algorithm would result in substantial error.\n", "link": "http://arxiv.org/abs/2403.14332v1", "date": "2024-03-21", "relevancy": 1.8807, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3983}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3729}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3573}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Differentially%20Private%20Clustering%20Algorithm%20for%20Well-Clustered%20Graphs&body=Title%3A%20A%20Differentially%20Private%20Clustering%20Algorithm%20for%20Well-Clustered%20Graphs%0AAuthor%3A%20Weiqiang%20He%20and%20Hendrik%20Fichtenberger%20and%20Pan%20Peng%0AAbstract%3A%20%20%20We%20study%20differentially%20private%20%28DP%29%20algorithms%20for%20recovering%20clusters%20in%0Awell-clustered%20graphs%2C%20which%20are%20graphs%20whose%20vertex%20set%20can%20be%20partitioned%0Ainto%20a%20small%20number%20of%20sets%2C%20each%20inducing%20a%20subgraph%20of%20high%20inner%20conductance%0Aand%20small%20outer%20conductance.%20Such%20graphs%20have%20widespread%20application%20as%20a%0Abenchmark%20in%20the%20theoretical%20analysis%20of%20spectral%20clustering.%20We%20provide%20an%0Aefficient%20%28%24%5Cepsilon%24%2C%24%5Cdelta%24%29-DP%20algorithm%20tailored%20specifically%20for%20such%0Agraphs.%20Our%20algorithm%20draws%20inspiration%20from%20the%20recent%20work%20of%20Chen%20et%20al.%2C%0Awho%20developed%20DP%20algorithms%20for%20recovery%20of%20stochastic%20block%20models%20in%20cases%0Awhere%20the%20graph%20comprises%20exactly%20two%20nearly-balanced%20clusters.%20Our%20algorithm%0Aworks%20for%20well-clustered%20graphs%20with%20%24k%24%20nearly-balanced%20clusters%2C%20and%20the%0Amisclassification%20ratio%20almost%20matches%20the%20one%20of%20the%20best-known%20non-private%0Aalgorithms.%20We%20conduct%20experimental%20evaluations%20on%20datasets%20with%20known%20ground%0Atruth%20clusters%20to%20substantiate%20the%20prowess%20of%20our%20algorithm.%20We%20also%20show%20that%0Aany%20%28pure%29%20%24%5Cepsilon%24-DP%20algorithm%20would%20result%20in%20substantial%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14332v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Differentially%20Private%20Clustering%20Algorithm%20for%20Well-Clustered%20Graphs&entry.906535625=Weiqiang%20He%20and%20Hendrik%20Fichtenberger%20and%20Pan%20Peng&entry.1292438233=%20%20We%20study%20differentially%20private%20%28DP%29%20algorithms%20for%20recovering%20clusters%20in%0Awell-clustered%20graphs%2C%20which%20are%20graphs%20whose%20vertex%20set%20can%20be%20partitioned%0Ainto%20a%20small%20number%20of%20sets%2C%20each%20inducing%20a%20subgraph%20of%20high%20inner%20conductance%0Aand%20small%20outer%20conductance.%20Such%20graphs%20have%20widespread%20application%20as%20a%0Abenchmark%20in%20the%20theoretical%20analysis%20of%20spectral%20clustering.%20We%20provide%20an%0Aefficient%20%28%24%5Cepsilon%24%2C%24%5Cdelta%24%29-DP%20algorithm%20tailored%20specifically%20for%20such%0Agraphs.%20Our%20algorithm%20draws%20inspiration%20from%20the%20recent%20work%20of%20Chen%20et%20al.%2C%0Awho%20developed%20DP%20algorithms%20for%20recovery%20of%20stochastic%20block%20models%20in%20cases%0Awhere%20the%20graph%20comprises%20exactly%20two%20nearly-balanced%20clusters.%20Our%20algorithm%0Aworks%20for%20well-clustered%20graphs%20with%20%24k%24%20nearly-balanced%20clusters%2C%20and%20the%0Amisclassification%20ratio%20almost%20matches%20the%20one%20of%20the%20best-known%20non-private%0Aalgorithms.%20We%20conduct%20experimental%20evaluations%20on%20datasets%20with%20known%20ground%0Atruth%20clusters%20to%20substantiate%20the%20prowess%20of%20our%20algorithm.%20We%20also%20show%20that%0Aany%20%28pure%29%20%24%5Cepsilon%24-DP%20algorithm%20would%20result%20in%20substantial%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14332v1&entry.124074799=Read"},
{"title": "The Elements of Differentiable Programming", "author": "Mathieu Blondel and Vincent Roulet", "abstract": "  Artificial intelligence has recently experienced remarkable advances, fueled\nby large models, vast datasets, accelerated hardware, and, last but not least,\nthe transformative power of differentiable programming. This new programming\nparadigm enables end-to-end differentiation of complex computer programs\n(including those with control flows and data structures), making gradient-based\noptimization of program parameters possible. As an emerging paradigm,\ndifferentiable programming builds upon several areas of computer science and\napplied mathematics, including automatic differentiation, graphical models,\noptimization and statistics. This book presents a comprehensive review of the\nfundamental concepts useful for differentiable programming. We adopt two main\nperspectives, that of optimization and that of probability, with clear\nanalogies between the two. Differentiable programming is not merely the\ndifferentiation of programs, but also the thoughtful design of programs\nintended for differentiation. By making programs differentiable, we inherently\nintroduce probability distributions over their execution, providing a means to\nquantify the uncertainty associated with program outputs.\n", "link": "http://arxiv.org/abs/2403.14606v1", "date": "2024-03-21", "relevancy": 1.6888, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4322}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4263}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4106}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Elements%20of%20Differentiable%20Programming&body=Title%3A%20The%20Elements%20of%20Differentiable%20Programming%0AAuthor%3A%20Mathieu%20Blondel%20and%20Vincent%20Roulet%0AAbstract%3A%20%20%20Artificial%20intelligence%20has%20recently%20experienced%20remarkable%20advances%2C%20fueled%0Aby%20large%20models%2C%20vast%20datasets%2C%20accelerated%20hardware%2C%20and%2C%20last%20but%20not%20least%2C%0Athe%20transformative%20power%20of%20differentiable%20programming.%20This%20new%20programming%0Aparadigm%20enables%20end-to-end%20differentiation%20of%20complex%20computer%20programs%0A%28including%20those%20with%20control%20flows%20and%20data%20structures%29%2C%20making%20gradient-based%0Aoptimization%20of%20program%20parameters%20possible.%20As%20an%20emerging%20paradigm%2C%0Adifferentiable%20programming%20builds%20upon%20several%20areas%20of%20computer%20science%20and%0Aapplied%20mathematics%2C%20including%20automatic%20differentiation%2C%20graphical%20models%2C%0Aoptimization%20and%20statistics.%20This%20book%20presents%20a%20comprehensive%20review%20of%20the%0Afundamental%20concepts%20useful%20for%20differentiable%20programming.%20We%20adopt%20two%20main%0Aperspectives%2C%20that%20of%20optimization%20and%20that%20of%20probability%2C%20with%20clear%0Aanalogies%20between%20the%20two.%20Differentiable%20programming%20is%20not%20merely%20the%0Adifferentiation%20of%20programs%2C%20but%20also%20the%20thoughtful%20design%20of%20programs%0Aintended%20for%20differentiation.%20By%20making%20programs%20differentiable%2C%20we%20inherently%0Aintroduce%20probability%20distributions%20over%20their%20execution%2C%20providing%20a%20means%20to%0Aquantify%20the%20uncertainty%20associated%20with%20program%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14606v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Elements%20of%20Differentiable%20Programming&entry.906535625=Mathieu%20Blondel%20and%20Vincent%20Roulet&entry.1292438233=%20%20Artificial%20intelligence%20has%20recently%20experienced%20remarkable%20advances%2C%20fueled%0Aby%20large%20models%2C%20vast%20datasets%2C%20accelerated%20hardware%2C%20and%2C%20last%20but%20not%20least%2C%0Athe%20transformative%20power%20of%20differentiable%20programming.%20This%20new%20programming%0Aparadigm%20enables%20end-to-end%20differentiation%20of%20complex%20computer%20programs%0A%28including%20those%20with%20control%20flows%20and%20data%20structures%29%2C%20making%20gradient-based%0Aoptimization%20of%20program%20parameters%20possible.%20As%20an%20emerging%20paradigm%2C%0Adifferentiable%20programming%20builds%20upon%20several%20areas%20of%20computer%20science%20and%0Aapplied%20mathematics%2C%20including%20automatic%20differentiation%2C%20graphical%20models%2C%0Aoptimization%20and%20statistics.%20This%20book%20presents%20a%20comprehensive%20review%20of%20the%0Afundamental%20concepts%20useful%20for%20differentiable%20programming.%20We%20adopt%20two%20main%0Aperspectives%2C%20that%20of%20optimization%20and%20that%20of%20probability%2C%20with%20clear%0Aanalogies%20between%20the%20two.%20Differentiable%20programming%20is%20not%20merely%20the%0Adifferentiation%20of%20programs%2C%20but%20also%20the%20thoughtful%20design%20of%20programs%0Aintended%20for%20differentiation.%20By%20making%20programs%20differentiable%2C%20we%20inherently%0Aintroduce%20probability%20distributions%20over%20their%20execution%2C%20providing%20a%20means%20to%0Aquantify%20the%20uncertainty%20associated%20with%20program%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14606v1&entry.124074799=Read"},
{"title": "An Analysis of Linear Time Series Forecasting Models", "author": "William Toner and Luke Darlow", "abstract": "  Despite their simplicity, linear models perform well at time series\nforecasting, even when pitted against deeper and more expensive models. A\nnumber of variations to the linear model have been proposed, often including\nsome form of feature normalisation that improves model generalisation. In this\npaper we analyse the sets of functions expressible using these linear model\narchitectures. In so doing we show that several popular variants of linear\nmodels for time series forecasting are equivalent and functionally\nindistinguishable from standard, unconstrained linear regression. We\ncharacterise the model classes for each linear variant. We demonstrate that\neach model can be reinterpreted as unconstrained linear regression over a\nsuitably augmented feature set, and therefore admit closed-form solutions when\nusing a mean-squared loss function. We provide experimental evidence that the\nmodels under inspection learn nearly identical solutions, and finally\ndemonstrate that the simpler closed form solutions are superior forecasters\nacross 72% of test settings.\n", "link": "http://arxiv.org/abs/2403.14587v1", "date": "2024-03-21", "relevancy": 1.1846, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4155}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3893}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Analysis%20of%20Linear%20Time%20Series%20Forecasting%20Models&body=Title%3A%20An%20Analysis%20of%20Linear%20Time%20Series%20Forecasting%20Models%0AAuthor%3A%20William%20Toner%20and%20Luke%20Darlow%0AAbstract%3A%20%20%20Despite%20their%20simplicity%2C%20linear%20models%20perform%20well%20at%20time%20series%0Aforecasting%2C%20even%20when%20pitted%20against%20deeper%20and%20more%20expensive%20models.%20A%0Anumber%20of%20variations%20to%20the%20linear%20model%20have%20been%20proposed%2C%20often%20including%0Asome%20form%20of%20feature%20normalisation%20that%20improves%20model%20generalisation.%20In%20this%0Apaper%20we%20analyse%20the%20sets%20of%20functions%20expressible%20using%20these%20linear%20model%0Aarchitectures.%20In%20so%20doing%20we%20show%20that%20several%20popular%20variants%20of%20linear%0Amodels%20for%20time%20series%20forecasting%20are%20equivalent%20and%20functionally%0Aindistinguishable%20from%20standard%2C%20unconstrained%20linear%20regression.%20We%0Acharacterise%20the%20model%20classes%20for%20each%20linear%20variant.%20We%20demonstrate%20that%0Aeach%20model%20can%20be%20reinterpreted%20as%20unconstrained%20linear%20regression%20over%20a%0Asuitably%20augmented%20feature%20set%2C%20and%20therefore%20admit%20closed-form%20solutions%20when%0Ausing%20a%20mean-squared%20loss%20function.%20We%20provide%20experimental%20evidence%20that%20the%0Amodels%20under%20inspection%20learn%20nearly%20identical%20solutions%2C%20and%20finally%0Ademonstrate%20that%20the%20simpler%20closed%20form%20solutions%20are%20superior%20forecasters%0Aacross%2072%25%20of%20test%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14587v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Analysis%20of%20Linear%20Time%20Series%20Forecasting%20Models&entry.906535625=William%20Toner%20and%20Luke%20Darlow&entry.1292438233=%20%20Despite%20their%20simplicity%2C%20linear%20models%20perform%20well%20at%20time%20series%0Aforecasting%2C%20even%20when%20pitted%20against%20deeper%20and%20more%20expensive%20models.%20A%0Anumber%20of%20variations%20to%20the%20linear%20model%20have%20been%20proposed%2C%20often%20including%0Asome%20form%20of%20feature%20normalisation%20that%20improves%20model%20generalisation.%20In%20this%0Apaper%20we%20analyse%20the%20sets%20of%20functions%20expressible%20using%20these%20linear%20model%0Aarchitectures.%20In%20so%20doing%20we%20show%20that%20several%20popular%20variants%20of%20linear%0Amodels%20for%20time%20series%20forecasting%20are%20equivalent%20and%20functionally%0Aindistinguishable%20from%20standard%2C%20unconstrained%20linear%20regression.%20We%0Acharacterise%20the%20model%20classes%20for%20each%20linear%20variant.%20We%20demonstrate%20that%0Aeach%20model%20can%20be%20reinterpreted%20as%20unconstrained%20linear%20regression%20over%20a%0Asuitably%20augmented%20feature%20set%2C%20and%20therefore%20admit%20closed-form%20solutions%20when%0Ausing%20a%20mean-squared%20loss%20function.%20We%20provide%20experimental%20evidence%20that%20the%0Amodels%20under%20inspection%20learn%20nearly%20identical%20solutions%2C%20and%20finally%0Ademonstrate%20that%20the%20simpler%20closed%20form%20solutions%20are%20superior%20forecasters%0Aacross%2072%25%20of%20test%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14587v1&entry.124074799=Read"},
{"title": "LaserHuman: Language-guided Scene-aware Human Motion Generation in Free\n  Environment", "author": "Peishan Cong and Ziyi Wang and Zhiyang Dou and Yiming Ren and Wei Yin and Kai Cheng and Yujing Sun and Xiaoxiao Long and Xinge Zhu and Yuexin Ma", "abstract": "  Language-guided scene-aware human motion generation has great significance\nfor entertainment and robotics. In response to the limitations of existing\ndatasets, we introduce LaserHuman, a pioneering dataset engineered to\nrevolutionize Scene-Text-to-Motion research. LaserHuman stands out with its\ninclusion of genuine human motions within 3D environments, unbounded free-form\nnatural language descriptions, a blend of indoor and outdoor scenarios, and\ndynamic, ever-changing scenes. Diverse modalities of capture data and rich\nannotations present great opportunities for the research of conditional motion\ngeneration, and can also facilitate the development of real-life applications.\nMoreover, to generate semantically consistent and physically plausible human\nmotions, we propose a multi-conditional diffusion model, which is simple but\neffective, achieving state-of-the-art performance on existing datasets.\n", "link": "http://arxiv.org/abs/2403.13307v2", "date": "2024-03-21", "relevancy": 1.6659, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5623}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.553}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LaserHuman%3A%20Language-guided%20Scene-aware%20Human%20Motion%20Generation%20in%20Free%0A%20%20Environment&body=Title%3A%20LaserHuman%3A%20Language-guided%20Scene-aware%20Human%20Motion%20Generation%20in%20Free%0A%20%20Environment%0AAuthor%3A%20Peishan%20Cong%20and%20Ziyi%20Wang%20and%20Zhiyang%20Dou%20and%20Yiming%20Ren%20and%20Wei%20Yin%20and%20Kai%20Cheng%20and%20Yujing%20Sun%20and%20Xiaoxiao%20Long%20and%20Xinge%20Zhu%20and%20Yuexin%20Ma%0AAbstract%3A%20%20%20Language-guided%20scene-aware%20human%20motion%20generation%20has%20great%20significance%0Afor%20entertainment%20and%20robotics.%20In%20response%20to%20the%20limitations%20of%20existing%0Adatasets%2C%20we%20introduce%20LaserHuman%2C%20a%20pioneering%20dataset%20engineered%20to%0Arevolutionize%20Scene-Text-to-Motion%20research.%20LaserHuman%20stands%20out%20with%20its%0Ainclusion%20of%20genuine%20human%20motions%20within%203D%20environments%2C%20unbounded%20free-form%0Anatural%20language%20descriptions%2C%20a%20blend%20of%20indoor%20and%20outdoor%20scenarios%2C%20and%0Adynamic%2C%20ever-changing%20scenes.%20Diverse%20modalities%20of%20capture%20data%20and%20rich%0Aannotations%20present%20great%20opportunities%20for%20the%20research%20of%20conditional%20motion%0Ageneration%2C%20and%20can%20also%20facilitate%20the%20development%20of%20real-life%20applications.%0AMoreover%2C%20to%20generate%20semantically%20consistent%20and%20physically%20plausible%20human%0Amotions%2C%20we%20propose%20a%20multi-conditional%20diffusion%20model%2C%20which%20is%20simple%20but%0Aeffective%2C%20achieving%20state-of-the-art%20performance%20on%20existing%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13307v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaserHuman%3A%20Language-guided%20Scene-aware%20Human%20Motion%20Generation%20in%20Free%0A%20%20Environment&entry.906535625=Peishan%20Cong%20and%20Ziyi%20Wang%20and%20Zhiyang%20Dou%20and%20Yiming%20Ren%20and%20Wei%20Yin%20and%20Kai%20Cheng%20and%20Yujing%20Sun%20and%20Xiaoxiao%20Long%20and%20Xinge%20Zhu%20and%20Yuexin%20Ma&entry.1292438233=%20%20Language-guided%20scene-aware%20human%20motion%20generation%20has%20great%20significance%0Afor%20entertainment%20and%20robotics.%20In%20response%20to%20the%20limitations%20of%20existing%0Adatasets%2C%20we%20introduce%20LaserHuman%2C%20a%20pioneering%20dataset%20engineered%20to%0Arevolutionize%20Scene-Text-to-Motion%20research.%20LaserHuman%20stands%20out%20with%20its%0Ainclusion%20of%20genuine%20human%20motions%20within%203D%20environments%2C%20unbounded%20free-form%0Anatural%20language%20descriptions%2C%20a%20blend%20of%20indoor%20and%20outdoor%20scenarios%2C%20and%0Adynamic%2C%20ever-changing%20scenes.%20Diverse%20modalities%20of%20capture%20data%20and%20rich%0Aannotations%20present%20great%20opportunities%20for%20the%20research%20of%20conditional%20motion%0Ageneration%2C%20and%20can%20also%20facilitate%20the%20development%20of%20real-life%20applications.%0AMoreover%2C%20to%20generate%20semantically%20consistent%20and%20physically%20plausible%20human%0Amotions%2C%20we%20propose%20a%20multi-conditional%20diffusion%20model%2C%20which%20is%20simple%20but%0Aeffective%2C%20achieving%20state-of-the-art%20performance%20on%20existing%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13307v2&entry.124074799=Read"},
{"title": "On the continuity and smoothness of the value function in reinforcement\n  learning and optimal control", "author": "Hans Harder and Sebastian Peitz", "abstract": "  The value function plays a crucial role as a measure for the cumulative\nfuture reward an agent receives in both reinforcement learning and optimal\ncontrol. It is therefore of interest to study how similar the values of\nneighboring states are, i.e., to investigate the continuity of the value\nfunction. We do so by providing and verifying upper bounds on the value\nfunction's modulus of continuity. Additionally, we show that the value function\nis always H\\\"older continuous under relatively weak assumptions on the\nunderlying system and that non-differentiable value functions can be made\ndifferentiable by slightly \"disturbing\" the system.\n", "link": "http://arxiv.org/abs/2403.14432v1", "date": "2024-03-21", "relevancy": 1.7035, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4453}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4349}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4091}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20continuity%20and%20smoothness%20of%20the%20value%20function%20in%20reinforcement%0A%20%20learning%20and%20optimal%20control&body=Title%3A%20On%20the%20continuity%20and%20smoothness%20of%20the%20value%20function%20in%20reinforcement%0A%20%20learning%20and%20optimal%20control%0AAuthor%3A%20Hans%20Harder%20and%20Sebastian%20Peitz%0AAbstract%3A%20%20%20The%20value%20function%20plays%20a%20crucial%20role%20as%20a%20measure%20for%20the%20cumulative%0Afuture%20reward%20an%20agent%20receives%20in%20both%20reinforcement%20learning%20and%20optimal%0Acontrol.%20It%20is%20therefore%20of%20interest%20to%20study%20how%20similar%20the%20values%20of%0Aneighboring%20states%20are%2C%20i.e.%2C%20to%20investigate%20the%20continuity%20of%20the%20value%0Afunction.%20We%20do%20so%20by%20providing%20and%20verifying%20upper%20bounds%20on%20the%20value%0Afunction%27s%20modulus%20of%20continuity.%20Additionally%2C%20we%20show%20that%20the%20value%20function%0Ais%20always%20H%5C%22older%20continuous%20under%20relatively%20weak%20assumptions%20on%20the%0Aunderlying%20system%20and%20that%20non-differentiable%20value%20functions%20can%20be%20made%0Adifferentiable%20by%20slightly%20%22disturbing%22%20the%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14432v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20continuity%20and%20smoothness%20of%20the%20value%20function%20in%20reinforcement%0A%20%20learning%20and%20optimal%20control&entry.906535625=Hans%20Harder%20and%20Sebastian%20Peitz&entry.1292438233=%20%20The%20value%20function%20plays%20a%20crucial%20role%20as%20a%20measure%20for%20the%20cumulative%0Afuture%20reward%20an%20agent%20receives%20in%20both%20reinforcement%20learning%20and%20optimal%0Acontrol.%20It%20is%20therefore%20of%20interest%20to%20study%20how%20similar%20the%20values%20of%0Aneighboring%20states%20are%2C%20i.e.%2C%20to%20investigate%20the%20continuity%20of%20the%20value%0Afunction.%20We%20do%20so%20by%20providing%20and%20verifying%20upper%20bounds%20on%20the%20value%0Afunction%27s%20modulus%20of%20continuity.%20Additionally%2C%20we%20show%20that%20the%20value%20function%0Ais%20always%20H%5C%22older%20continuous%20under%20relatively%20weak%20assumptions%20on%20the%0Aunderlying%20system%20and%20that%20non-differentiable%20value%20functions%20can%20be%20made%0Adifferentiable%20by%20slightly%20%22disturbing%22%20the%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14432v1&entry.124074799=Read"},
{"title": "Multi-role Consensus through LLMs Discussions for Vulnerability\n  Detection", "author": "Zhenyu Mao and Jialong Li and Munan Li and Kenji Tei", "abstract": "  Recent advancements in large language models (LLMs) have highlighted the\npotential for vulnerability detection, a crucial component of software quality\nassurance. Despite this progress, most studies have been limited to the\nperspective of a single role, usually testers, lacking diverse viewpoints from\ndifferent roles in a typical software development life-cycle, including both\ndevelopers and testers. To this end, this paper introduces an approach to\nemploy LLMs to act as different roles to simulate real-life code review\nprocess, engaging in discussions towards a consensus on the existence and\nclassification of vulnerabilities in the code. Preliminary evaluation of the\nproposed approach indicates a 4.73% increase in the precision rate, 58.9%\nincrease in the recall rate, and a 28.1% increase in the F1 score.\n", "link": "http://arxiv.org/abs/2403.14274v1", "date": "2024-03-21", "relevancy": 1.8917, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5023}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4659}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-role%20Consensus%20through%20LLMs%20Discussions%20for%20Vulnerability%0A%20%20Detection&body=Title%3A%20Multi-role%20Consensus%20through%20LLMs%20Discussions%20for%20Vulnerability%0A%20%20Detection%0AAuthor%3A%20Zhenyu%20Mao%20and%20Jialong%20Li%20and%20Munan%20Li%20and%20Kenji%20Tei%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20highlighted%20the%0Apotential%20for%20vulnerability%20detection%2C%20a%20crucial%20component%20of%20software%20quality%0Aassurance.%20Despite%20this%20progress%2C%20most%20studies%20have%20been%20limited%20to%20the%0Aperspective%20of%20a%20single%20role%2C%20usually%20testers%2C%20lacking%20diverse%20viewpoints%20from%0Adifferent%20roles%20in%20a%20typical%20software%20development%20life-cycle%2C%20including%20both%0Adevelopers%20and%20testers.%20To%20this%20end%2C%20this%20paper%20introduces%20an%20approach%20to%0Aemploy%20LLMs%20to%20act%20as%20different%20roles%20to%20simulate%20real-life%20code%20review%0Aprocess%2C%20engaging%20in%20discussions%20towards%20a%20consensus%20on%20the%20existence%20and%0Aclassification%20of%20vulnerabilities%20in%20the%20code.%20Preliminary%20evaluation%20of%20the%0Aproposed%20approach%20indicates%20a%204.73%25%20increase%20in%20the%20precision%20rate%2C%2058.9%25%0Aincrease%20in%20the%20recall%20rate%2C%20and%20a%2028.1%25%20increase%20in%20the%20F1%20score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14274v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-role%20Consensus%20through%20LLMs%20Discussions%20for%20Vulnerability%0A%20%20Detection&entry.906535625=Zhenyu%20Mao%20and%20Jialong%20Li%20and%20Munan%20Li%20and%20Kenji%20Tei&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20highlighted%20the%0Apotential%20for%20vulnerability%20detection%2C%20a%20crucial%20component%20of%20software%20quality%0Aassurance.%20Despite%20this%20progress%2C%20most%20studies%20have%20been%20limited%20to%20the%0Aperspective%20of%20a%20single%20role%2C%20usually%20testers%2C%20lacking%20diverse%20viewpoints%20from%0Adifferent%20roles%20in%20a%20typical%20software%20development%20life-cycle%2C%20including%20both%0Adevelopers%20and%20testers.%20To%20this%20end%2C%20this%20paper%20introduces%20an%20approach%20to%0Aemploy%20LLMs%20to%20act%20as%20different%20roles%20to%20simulate%20real-life%20code%20review%0Aprocess%2C%20engaging%20in%20discussions%20towards%20a%20consensus%20on%20the%20existence%20and%0Aclassification%20of%20vulnerabilities%20in%20the%20code.%20Preliminary%20evaluation%20of%20the%0Aproposed%20approach%20indicates%20a%204.73%25%20increase%20in%20the%20precision%20rate%2C%2058.9%25%0Aincrease%20in%20the%20recall%20rate%2C%20and%20a%2028.1%25%20increase%20in%20the%20F1%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14274v1&entry.124074799=Read"},
{"title": "The Role of Transparency in Repeated First-Price Auctions with Unknown\n  Valuations", "author": "Nicol\u00f2 Cesa-Bianchi and Tommaso Cesari and Roberto Colomboni and Federico Fusco and Stefano Leonardi", "abstract": "  We study the problem of regret minimization for a single bidder in a sequence\nof first-price auctions where the bidder discovers the item's value only if the\nauction is won. Our main contribution is a complete characterization, up to\nlogarithmic factors, of the minimax regret in terms of the auction's\n\\emph{transparency}, which controls the amount of information on competing bids\ndisclosed by the auctioneer at the end of each auction. Our results hold under\ndifferent assumptions (stochastic, adversarial, and their smoothed variants) on\nthe environment generating the bidder's valuations and competing bids. These\nminimax rates reveal how the interplay between transparency and the nature of\nthe environment affects how fast one can learn to bid optimally in first-price\nauctions.\n", "link": "http://arxiv.org/abs/2307.09478v2", "date": "2024-03-21", "relevancy": 1.5924, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4197}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3833}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.381}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Transparency%20in%20Repeated%20First-Price%20Auctions%20with%20Unknown%0A%20%20Valuations&body=Title%3A%20The%20Role%20of%20Transparency%20in%20Repeated%20First-Price%20Auctions%20with%20Unknown%0A%20%20Valuations%0AAuthor%3A%20Nicol%C3%B2%20Cesa-Bianchi%20and%20Tommaso%20Cesari%20and%20Roberto%20Colomboni%20and%20Federico%20Fusco%20and%20Stefano%20Leonardi%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20regret%20minimization%20for%20a%20single%20bidder%20in%20a%20sequence%0Aof%20first-price%20auctions%20where%20the%20bidder%20discovers%20the%20item%27s%20value%20only%20if%20the%0Aauction%20is%20won.%20Our%20main%20contribution%20is%20a%20complete%20characterization%2C%20up%20to%0Alogarithmic%20factors%2C%20of%20the%20minimax%20regret%20in%20terms%20of%20the%20auction%27s%0A%5Cemph%7Btransparency%7D%2C%20which%20controls%20the%20amount%20of%20information%20on%20competing%20bids%0Adisclosed%20by%20the%20auctioneer%20at%20the%20end%20of%20each%20auction.%20Our%20results%20hold%20under%0Adifferent%20assumptions%20%28stochastic%2C%20adversarial%2C%20and%20their%20smoothed%20variants%29%20on%0Athe%20environment%20generating%20the%20bidder%27s%20valuations%20and%20competing%20bids.%20These%0Aminimax%20rates%20reveal%20how%20the%20interplay%20between%20transparency%20and%20the%20nature%20of%0Athe%20environment%20affects%20how%20fast%20one%20can%20learn%20to%20bid%20optimally%20in%20first-price%0Aauctions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09478v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Transparency%20in%20Repeated%20First-Price%20Auctions%20with%20Unknown%0A%20%20Valuations&entry.906535625=Nicol%C3%B2%20Cesa-Bianchi%20and%20Tommaso%20Cesari%20and%20Roberto%20Colomboni%20and%20Federico%20Fusco%20and%20Stefano%20Leonardi&entry.1292438233=%20%20We%20study%20the%20problem%20of%20regret%20minimization%20for%20a%20single%20bidder%20in%20a%20sequence%0Aof%20first-price%20auctions%20where%20the%20bidder%20discovers%20the%20item%27s%20value%20only%20if%20the%0Aauction%20is%20won.%20Our%20main%20contribution%20is%20a%20complete%20characterization%2C%20up%20to%0Alogarithmic%20factors%2C%20of%20the%20minimax%20regret%20in%20terms%20of%20the%20auction%27s%0A%5Cemph%7Btransparency%7D%2C%20which%20controls%20the%20amount%20of%20information%20on%20competing%20bids%0Adisclosed%20by%20the%20auctioneer%20at%20the%20end%20of%20each%20auction.%20Our%20results%20hold%20under%0Adifferent%20assumptions%20%28stochastic%2C%20adversarial%2C%20and%20their%20smoothed%20variants%29%20on%0Athe%20environment%20generating%20the%20bidder%27s%20valuations%20and%20competing%20bids.%20These%0Aminimax%20rates%20reveal%20how%20the%20interplay%20between%20transparency%20and%20the%20nature%20of%0Athe%20environment%20affects%20how%20fast%20one%20can%20learn%20to%20bid%20optimally%20in%20first-price%0Aauctions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09478v2&entry.124074799=Read"},
{"title": "Tell Me What You Want (What You Really, Really Want): Addressing the\n  Expectation Gap for Goal Conveyance from Humans to Robots", "author": "Kevin Leahy and Ho Chit Siu", "abstract": "  Conveying human goals to autonomous systems (AS) occurs both when the system\nis being designed and when it is being operated. The design-step conveyance is\ntypically mediated by robotics and AI engineers, who must appropriately capture\nend-user requirements and concepts of operations, while the operation-step\nconveyance is mediated by the design, interfaces, and behavior of the AI.\nHowever, communication can be difficult during both these periods because of\nmismatches in the expectations and expertise of the end-user and the\nroboticist, necessitating more design cycles to resolve. We examine some of the\nbarriers in communicating system design requirements, and develop an\naugmentation for applied cognitive task analysis (ACTA) methods, that we call\nrobot task analysis (RTA), pertaining specifically to the development of\nautonomous systems. Further, we introduce a top-down view of an underexplored\narea of friction between requirements communication -- implied human\nexpectations -- utilizing a collection of work primarily from experimental\npsychology and social sciences. We show how such expectations can be used in\nconjunction with task-specific expectations and the system design process for\nAS to improve design team communication, alleviate barriers to user rejection,\nand reduce the number of design cycles.\n", "link": "http://arxiv.org/abs/2403.14344v1", "date": "2024-03-21", "relevancy": 1.4558, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4789}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4646}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tell%20Me%20What%20You%20Want%20%28What%20You%20Really%2C%20Really%20Want%29%3A%20Addressing%20the%0A%20%20Expectation%20Gap%20for%20Goal%20Conveyance%20from%20Humans%20to%20Robots&body=Title%3A%20Tell%20Me%20What%20You%20Want%20%28What%20You%20Really%2C%20Really%20Want%29%3A%20Addressing%20the%0A%20%20Expectation%20Gap%20for%20Goal%20Conveyance%20from%20Humans%20to%20Robots%0AAuthor%3A%20Kevin%20Leahy%20and%20Ho%20Chit%20Siu%0AAbstract%3A%20%20%20Conveying%20human%20goals%20to%20autonomous%20systems%20%28AS%29%20occurs%20both%20when%20the%20system%0Ais%20being%20designed%20and%20when%20it%20is%20being%20operated.%20The%20design-step%20conveyance%20is%0Atypically%20mediated%20by%20robotics%20and%20AI%20engineers%2C%20who%20must%20appropriately%20capture%0Aend-user%20requirements%20and%20concepts%20of%20operations%2C%20while%20the%20operation-step%0Aconveyance%20is%20mediated%20by%20the%20design%2C%20interfaces%2C%20and%20behavior%20of%20the%20AI.%0AHowever%2C%20communication%20can%20be%20difficult%20during%20both%20these%20periods%20because%20of%0Amismatches%20in%20the%20expectations%20and%20expertise%20of%20the%20end-user%20and%20the%0Aroboticist%2C%20necessitating%20more%20design%20cycles%20to%20resolve.%20We%20examine%20some%20of%20the%0Abarriers%20in%20communicating%20system%20design%20requirements%2C%20and%20develop%20an%0Aaugmentation%20for%20applied%20cognitive%20task%20analysis%20%28ACTA%29%20methods%2C%20that%20we%20call%0Arobot%20task%20analysis%20%28RTA%29%2C%20pertaining%20specifically%20to%20the%20development%20of%0Aautonomous%20systems.%20Further%2C%20we%20introduce%20a%20top-down%20view%20of%20an%20underexplored%0Aarea%20of%20friction%20between%20requirements%20communication%20--%20implied%20human%0Aexpectations%20--%20utilizing%20a%20collection%20of%20work%20primarily%20from%20experimental%0Apsychology%20and%20social%20sciences.%20We%20show%20how%20such%20expectations%20can%20be%20used%20in%0Aconjunction%20with%20task-specific%20expectations%20and%20the%20system%20design%20process%20for%0AAS%20to%20improve%20design%20team%20communication%2C%20alleviate%20barriers%20to%20user%20rejection%2C%0Aand%20reduce%20the%20number%20of%20design%20cycles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14344v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tell%20Me%20What%20You%20Want%20%28What%20You%20Really%2C%20Really%20Want%29%3A%20Addressing%20the%0A%20%20Expectation%20Gap%20for%20Goal%20Conveyance%20from%20Humans%20to%20Robots&entry.906535625=Kevin%20Leahy%20and%20Ho%20Chit%20Siu&entry.1292438233=%20%20Conveying%20human%20goals%20to%20autonomous%20systems%20%28AS%29%20occurs%20both%20when%20the%20system%0Ais%20being%20designed%20and%20when%20it%20is%20being%20operated.%20The%20design-step%20conveyance%20is%0Atypically%20mediated%20by%20robotics%20and%20AI%20engineers%2C%20who%20must%20appropriately%20capture%0Aend-user%20requirements%20and%20concepts%20of%20operations%2C%20while%20the%20operation-step%0Aconveyance%20is%20mediated%20by%20the%20design%2C%20interfaces%2C%20and%20behavior%20of%20the%20AI.%0AHowever%2C%20communication%20can%20be%20difficult%20during%20both%20these%20periods%20because%20of%0Amismatches%20in%20the%20expectations%20and%20expertise%20of%20the%20end-user%20and%20the%0Aroboticist%2C%20necessitating%20more%20design%20cycles%20to%20resolve.%20We%20examine%20some%20of%20the%0Abarriers%20in%20communicating%20system%20design%20requirements%2C%20and%20develop%20an%0Aaugmentation%20for%20applied%20cognitive%20task%20analysis%20%28ACTA%29%20methods%2C%20that%20we%20call%0Arobot%20task%20analysis%20%28RTA%29%2C%20pertaining%20specifically%20to%20the%20development%20of%0Aautonomous%20systems.%20Further%2C%20we%20introduce%20a%20top-down%20view%20of%20an%20underexplored%0Aarea%20of%20friction%20between%20requirements%20communication%20--%20implied%20human%0Aexpectations%20--%20utilizing%20a%20collection%20of%20work%20primarily%20from%20experimental%0Apsychology%20and%20social%20sciences.%20We%20show%20how%20such%20expectations%20can%20be%20used%20in%0Aconjunction%20with%20task-specific%20expectations%20and%20the%20system%20design%20process%20for%0AAS%20to%20improve%20design%20team%20communication%2C%20alleviate%20barriers%20to%20user%20rejection%2C%0Aand%20reduce%20the%20number%20of%20design%20cycles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14344v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


