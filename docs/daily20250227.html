<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250226.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Does 3D Gaussian Splatting Need Accurate Volumetric Rendering?", "author": "Adam Celarek and George Kopanas and George Drettakis and Michael Wimmer and Bernhard Kerbl", "abstract": "  Since its introduction, 3D Gaussian Splatting (3DGS) has become an important\nreference method for learning 3D representations of a captured scene, allowing\nreal-time novel-view synthesis with high visual quality and fast training\ntimes. Neural Radiance Fields (NeRFs), which preceded 3DGS, are based on a\nprincipled ray-marching approach for volumetric rendering. In contrast, while\nsharing a similar image formation model with NeRF, 3DGS uses a hybrid rendering\nsolution that builds on the strengths of volume rendering and primitive\nrasterization. A crucial benefit of 3DGS is its performance, achieved through a\nset of approximations, in many cases with respect to volumetric rendering\ntheory. A naturally arising question is whether replacing these approximations\nwith more principled volumetric rendering solutions can improve the quality of\n3DGS. In this paper, we present an in-depth analysis of the various\napproximations and assumptions used by the original 3DGS solution. We\ndemonstrate that, while more accurate volumetric rendering can help for low\nnumbers of primitives, the power of efficient optimization and the large number\nof Gaussians allows 3DGS to outperform volumetric rendering despite its\napproximations.\n", "link": "http://arxiv.org/abs/2502.19318v1", "date": "2025-02-26", "relevancy": 3.2725, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6846}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6512}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%203D%20Gaussian%20Splatting%20Need%20Accurate%20Volumetric%20Rendering%3F&body=Title%3A%20Does%203D%20Gaussian%20Splatting%20Need%20Accurate%20Volumetric%20Rendering%3F%0AAuthor%3A%20Adam%20Celarek%20and%20George%20Kopanas%20and%20George%20Drettakis%20and%20Michael%20Wimmer%20and%20Bernhard%20Kerbl%0AAbstract%3A%20%20%20Since%20its%20introduction%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20an%20important%0Areference%20method%20for%20learning%203D%20representations%20of%20a%20captured%20scene%2C%20allowing%0Areal-time%20novel-view%20synthesis%20with%20high%20visual%20quality%20and%20fast%20training%0Atimes.%20Neural%20Radiance%20Fields%20%28NeRFs%29%2C%20which%20preceded%203DGS%2C%20are%20based%20on%20a%0Aprincipled%20ray-marching%20approach%20for%20volumetric%20rendering.%20In%20contrast%2C%20while%0Asharing%20a%20similar%20image%20formation%20model%20with%20NeRF%2C%203DGS%20uses%20a%20hybrid%20rendering%0Asolution%20that%20builds%20on%20the%20strengths%20of%20volume%20rendering%20and%20primitive%0Arasterization.%20A%20crucial%20benefit%20of%203DGS%20is%20its%20performance%2C%20achieved%20through%20a%0Aset%20of%20approximations%2C%20in%20many%20cases%20with%20respect%20to%20volumetric%20rendering%0Atheory.%20A%20naturally%20arising%20question%20is%20whether%20replacing%20these%20approximations%0Awith%20more%20principled%20volumetric%20rendering%20solutions%20can%20improve%20the%20quality%20of%0A3DGS.%20In%20this%20paper%2C%20we%20present%20an%20in-depth%20analysis%20of%20the%20various%0Aapproximations%20and%20assumptions%20used%20by%20the%20original%203DGS%20solution.%20We%0Ademonstrate%20that%2C%20while%20more%20accurate%20volumetric%20rendering%20can%20help%20for%20low%0Anumbers%20of%20primitives%2C%20the%20power%20of%20efficient%20optimization%20and%20the%20large%20number%0Aof%20Gaussians%20allows%203DGS%20to%20outperform%20volumetric%20rendering%20despite%20its%0Aapproximations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%25203D%2520Gaussian%2520Splatting%2520Need%2520Accurate%2520Volumetric%2520Rendering%253F%26entry.906535625%3DAdam%2520Celarek%2520and%2520George%2520Kopanas%2520and%2520George%2520Drettakis%2520and%2520Michael%2520Wimmer%2520and%2520Bernhard%2520Kerbl%26entry.1292438233%3D%2520%2520Since%2520its%2520introduction%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520become%2520an%2520important%250Areference%2520method%2520for%2520learning%25203D%2520representations%2520of%2520a%2520captured%2520scene%252C%2520allowing%250Areal-time%2520novel-view%2520synthesis%2520with%2520high%2520visual%2520quality%2520and%2520fast%2520training%250Atimes.%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%252C%2520which%2520preceded%25203DGS%252C%2520are%2520based%2520on%2520a%250Aprincipled%2520ray-marching%2520approach%2520for%2520volumetric%2520rendering.%2520In%2520contrast%252C%2520while%250Asharing%2520a%2520similar%2520image%2520formation%2520model%2520with%2520NeRF%252C%25203DGS%2520uses%2520a%2520hybrid%2520rendering%250Asolution%2520that%2520builds%2520on%2520the%2520strengths%2520of%2520volume%2520rendering%2520and%2520primitive%250Arasterization.%2520A%2520crucial%2520benefit%2520of%25203DGS%2520is%2520its%2520performance%252C%2520achieved%2520through%2520a%250Aset%2520of%2520approximations%252C%2520in%2520many%2520cases%2520with%2520respect%2520to%2520volumetric%2520rendering%250Atheory.%2520A%2520naturally%2520arising%2520question%2520is%2520whether%2520replacing%2520these%2520approximations%250Awith%2520more%2520principled%2520volumetric%2520rendering%2520solutions%2520can%2520improve%2520the%2520quality%2520of%250A3DGS.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520in-depth%2520analysis%2520of%2520the%2520various%250Aapproximations%2520and%2520assumptions%2520used%2520by%2520the%2520original%25203DGS%2520solution.%2520We%250Ademonstrate%2520that%252C%2520while%2520more%2520accurate%2520volumetric%2520rendering%2520can%2520help%2520for%2520low%250Anumbers%2520of%2520primitives%252C%2520the%2520power%2520of%2520efficient%2520optimization%2520and%2520the%2520large%2520number%250Aof%2520Gaussians%2520allows%25203DGS%2520to%2520outperform%2520volumetric%2520rendering%2520despite%2520its%250Aapproximations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%203D%20Gaussian%20Splatting%20Need%20Accurate%20Volumetric%20Rendering%3F&entry.906535625=Adam%20Celarek%20and%20George%20Kopanas%20and%20George%20Drettakis%20and%20Michael%20Wimmer%20and%20Bernhard%20Kerbl&entry.1292438233=%20%20Since%20its%20introduction%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20an%20important%0Areference%20method%20for%20learning%203D%20representations%20of%20a%20captured%20scene%2C%20allowing%0Areal-time%20novel-view%20synthesis%20with%20high%20visual%20quality%20and%20fast%20training%0Atimes.%20Neural%20Radiance%20Fields%20%28NeRFs%29%2C%20which%20preceded%203DGS%2C%20are%20based%20on%20a%0Aprincipled%20ray-marching%20approach%20for%20volumetric%20rendering.%20In%20contrast%2C%20while%0Asharing%20a%20similar%20image%20formation%20model%20with%20NeRF%2C%203DGS%20uses%20a%20hybrid%20rendering%0Asolution%20that%20builds%20on%20the%20strengths%20of%20volume%20rendering%20and%20primitive%0Arasterization.%20A%20crucial%20benefit%20of%203DGS%20is%20its%20performance%2C%20achieved%20through%20a%0Aset%20of%20approximations%2C%20in%20many%20cases%20with%20respect%20to%20volumetric%20rendering%0Atheory.%20A%20naturally%20arising%20question%20is%20whether%20replacing%20these%20approximations%0Awith%20more%20principled%20volumetric%20rendering%20solutions%20can%20improve%20the%20quality%20of%0A3DGS.%20In%20this%20paper%2C%20we%20present%20an%20in-depth%20analysis%20of%20the%20various%0Aapproximations%20and%20assumptions%20used%20by%20the%20original%203DGS%20solution.%20We%0Ademonstrate%20that%2C%20while%20more%20accurate%20volumetric%20rendering%20can%20help%20for%20low%0Anumbers%20of%20primitives%2C%20the%20power%20of%20efficient%20optimization%20and%20the%20large%20number%0Aof%20Gaussians%20allows%203DGS%20to%20outperform%20volumetric%20rendering%20despite%20its%0Aapproximations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19318v1&entry.124074799=Read"},
{"title": "GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled\n  Appearance and Geometry Modeling", "author": "Victor Rong and Jingxiang Chen and Sherwin Bahmani and Kiriakos N. Kutulakos and David B. Lindell", "abstract": "  Gaussian splatting has demonstrated excellent performance for view synthesis\nand scene reconstruction. The representation achieves photorealistic quality by\noptimizing the position, scale, color, and opacity of thousands to millions of\n2D or 3D Gaussian primitives within a scene. However, since each Gaussian\nprimitive encodes both appearance and geometry, these attributes are strongly\ncoupled--thus, high-fidelity appearance modeling requires a large number of\nGaussian primitives, even when the scene geometry is simple (e.g., for a\ntextured planar surface). We propose to texture each 2D Gaussian primitive so\nthat even a single Gaussian can be used to capture appearance details. By\nemploying per-primitive texturing, our appearance representation is agnostic to\nthe topology and complexity of the scene's geometry. We show that our approach,\nGStex, yields improved visual quality over prior work in texturing Gaussian\nsplats. Furthermore, we demonstrate that our decoupling enables improved novel\nview synthesis performance compared to 2D Gaussian splatting when reducing the\nnumber of Gaussian primitives, and that GStex can be used for scene appearance\nediting and re-texturing.\n", "link": "http://arxiv.org/abs/2409.12954v3", "date": "2025-02-26", "relevancy": 3.2178, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6607}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6597}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GStex%3A%20Per-Primitive%20Texturing%20of%202D%20Gaussian%20Splatting%20for%20Decoupled%0A%20%20Appearance%20and%20Geometry%20Modeling&body=Title%3A%20GStex%3A%20Per-Primitive%20Texturing%20of%202D%20Gaussian%20Splatting%20for%20Decoupled%0A%20%20Appearance%20and%20Geometry%20Modeling%0AAuthor%3A%20Victor%20Rong%20and%20Jingxiang%20Chen%20and%20Sherwin%20Bahmani%20and%20Kiriakos%20N.%20Kutulakos%20and%20David%20B.%20Lindell%0AAbstract%3A%20%20%20Gaussian%20splatting%20has%20demonstrated%20excellent%20performance%20for%20view%20synthesis%0Aand%20scene%20reconstruction.%20The%20representation%20achieves%20photorealistic%20quality%20by%0Aoptimizing%20the%20position%2C%20scale%2C%20color%2C%20and%20opacity%20of%20thousands%20to%20millions%20of%0A2D%20or%203D%20Gaussian%20primitives%20within%20a%20scene.%20However%2C%20since%20each%20Gaussian%0Aprimitive%20encodes%20both%20appearance%20and%20geometry%2C%20these%20attributes%20are%20strongly%0Acoupled--thus%2C%20high-fidelity%20appearance%20modeling%20requires%20a%20large%20number%20of%0AGaussian%20primitives%2C%20even%20when%20the%20scene%20geometry%20is%20simple%20%28e.g.%2C%20for%20a%0Atextured%20planar%20surface%29.%20We%20propose%20to%20texture%20each%202D%20Gaussian%20primitive%20so%0Athat%20even%20a%20single%20Gaussian%20can%20be%20used%20to%20capture%20appearance%20details.%20By%0Aemploying%20per-primitive%20texturing%2C%20our%20appearance%20representation%20is%20agnostic%20to%0Athe%20topology%20and%20complexity%20of%20the%20scene%27s%20geometry.%20We%20show%20that%20our%20approach%2C%0AGStex%2C%20yields%20improved%20visual%20quality%20over%20prior%20work%20in%20texturing%20Gaussian%0Asplats.%20Furthermore%2C%20we%20demonstrate%20that%20our%20decoupling%20enables%20improved%20novel%0Aview%20synthesis%20performance%20compared%20to%202D%20Gaussian%20splatting%20when%20reducing%20the%0Anumber%20of%20Gaussian%20primitives%2C%20and%20that%20GStex%20can%20be%20used%20for%20scene%20appearance%0Aediting%20and%20re-texturing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12954v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGStex%253A%2520Per-Primitive%2520Texturing%2520of%25202D%2520Gaussian%2520Splatting%2520for%2520Decoupled%250A%2520%2520Appearance%2520and%2520Geometry%2520Modeling%26entry.906535625%3DVictor%2520Rong%2520and%2520Jingxiang%2520Chen%2520and%2520Sherwin%2520Bahmani%2520and%2520Kiriakos%2520N.%2520Kutulakos%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3D%2520%2520Gaussian%2520splatting%2520has%2520demonstrated%2520excellent%2520performance%2520for%2520view%2520synthesis%250Aand%2520scene%2520reconstruction.%2520The%2520representation%2520achieves%2520photorealistic%2520quality%2520by%250Aoptimizing%2520the%2520position%252C%2520scale%252C%2520color%252C%2520and%2520opacity%2520of%2520thousands%2520to%2520millions%2520of%250A2D%2520or%25203D%2520Gaussian%2520primitives%2520within%2520a%2520scene.%2520However%252C%2520since%2520each%2520Gaussian%250Aprimitive%2520encodes%2520both%2520appearance%2520and%2520geometry%252C%2520these%2520attributes%2520are%2520strongly%250Acoupled--thus%252C%2520high-fidelity%2520appearance%2520modeling%2520requires%2520a%2520large%2520number%2520of%250AGaussian%2520primitives%252C%2520even%2520when%2520the%2520scene%2520geometry%2520is%2520simple%2520%2528e.g.%252C%2520for%2520a%250Atextured%2520planar%2520surface%2529.%2520We%2520propose%2520to%2520texture%2520each%25202D%2520Gaussian%2520primitive%2520so%250Athat%2520even%2520a%2520single%2520Gaussian%2520can%2520be%2520used%2520to%2520capture%2520appearance%2520details.%2520By%250Aemploying%2520per-primitive%2520texturing%252C%2520our%2520appearance%2520representation%2520is%2520agnostic%2520to%250Athe%2520topology%2520and%2520complexity%2520of%2520the%2520scene%2527s%2520geometry.%2520We%2520show%2520that%2520our%2520approach%252C%250AGStex%252C%2520yields%2520improved%2520visual%2520quality%2520over%2520prior%2520work%2520in%2520texturing%2520Gaussian%250Asplats.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520our%2520decoupling%2520enables%2520improved%2520novel%250Aview%2520synthesis%2520performance%2520compared%2520to%25202D%2520Gaussian%2520splatting%2520when%2520reducing%2520the%250Anumber%2520of%2520Gaussian%2520primitives%252C%2520and%2520that%2520GStex%2520can%2520be%2520used%2520for%2520scene%2520appearance%250Aediting%2520and%2520re-texturing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12954v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GStex%3A%20Per-Primitive%20Texturing%20of%202D%20Gaussian%20Splatting%20for%20Decoupled%0A%20%20Appearance%20and%20Geometry%20Modeling&entry.906535625=Victor%20Rong%20and%20Jingxiang%20Chen%20and%20Sherwin%20Bahmani%20and%20Kiriakos%20N.%20Kutulakos%20and%20David%20B.%20Lindell&entry.1292438233=%20%20Gaussian%20splatting%20has%20demonstrated%20excellent%20performance%20for%20view%20synthesis%0Aand%20scene%20reconstruction.%20The%20representation%20achieves%20photorealistic%20quality%20by%0Aoptimizing%20the%20position%2C%20scale%2C%20color%2C%20and%20opacity%20of%20thousands%20to%20millions%20of%0A2D%20or%203D%20Gaussian%20primitives%20within%20a%20scene.%20However%2C%20since%20each%20Gaussian%0Aprimitive%20encodes%20both%20appearance%20and%20geometry%2C%20these%20attributes%20are%20strongly%0Acoupled--thus%2C%20high-fidelity%20appearance%20modeling%20requires%20a%20large%20number%20of%0AGaussian%20primitives%2C%20even%20when%20the%20scene%20geometry%20is%20simple%20%28e.g.%2C%20for%20a%0Atextured%20planar%20surface%29.%20We%20propose%20to%20texture%20each%202D%20Gaussian%20primitive%20so%0Athat%20even%20a%20single%20Gaussian%20can%20be%20used%20to%20capture%20appearance%20details.%20By%0Aemploying%20per-primitive%20texturing%2C%20our%20appearance%20representation%20is%20agnostic%20to%0Athe%20topology%20and%20complexity%20of%20the%20scene%27s%20geometry.%20We%20show%20that%20our%20approach%2C%0AGStex%2C%20yields%20improved%20visual%20quality%20over%20prior%20work%20in%20texturing%20Gaussian%0Asplats.%20Furthermore%2C%20we%20demonstrate%20that%20our%20decoupling%20enables%20improved%20novel%0Aview%20synthesis%20performance%20compared%20to%202D%20Gaussian%20splatting%20when%20reducing%20the%0Anumber%20of%20Gaussian%20primitives%2C%20and%20that%20GStex%20can%20be%20used%20for%20scene%20appearance%0Aediting%20and%20re-texturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12954v3&entry.124074799=Read"},
{"title": "Movie Gen: A Cast of Media Foundation Models", "author": "Adam Polyak and Amit Zohar and Andrew Brown and Andros Tjandra and Animesh Sinha and Ann Lee and Apoorv Vyas and Bowen Shi and Chih-Yao Ma and Ching-Yao Chuang and David Yan and Dhruv Choudhary and Dingkang Wang and Geet Sethi and Guan Pang and Haoyu Ma and Ishan Misra and Ji Hou and Jialiang Wang and Kiran Jagadeesh and Kunpeng Li and Luxin Zhang and Mannat Singh and Mary Williamson and Matt Le and Matthew Yu and Mitesh Kumar Singh and Peizhao Zhang and Peter Vajda and Quentin Duval and Rohit Girdhar and Roshan Sumbaly and Sai Saketh Rambhatla and Sam Tsai and Samaneh Azadi and Samyak Datta and Sanyuan Chen and Sean Bell and Sharadh Ramaswamy and Shelly Sheynin and Siddharth Bhattacharya and Simran Motwani and Tao Xu and Tianhe Li and Tingbo Hou and Wei-Ning Hsu and Xi Yin and Xiaoliang Dai and Yaniv Taigman and Yaqiao Luo and Yen-Cheng Liu and Yi-Chiao Wu and Yue Zhao and Yuval Kirstain and Zecheng He and Zijian He and Albert Pumarola and Ali Thabet and Artsiom Sanakoyeu and Arun Mallya and Baishan Guo and Boris Araya and Breena Kerr and Carleigh Wood and Ce Liu and Cen Peng and Dimitry Vengertsev and Edgar Schonfeld and Elliot Blanchard and Felix Juefei-Xu and Fraylie Nord and Jeff Liang and John Hoffman and Jonas Kohler and Kaolin Fire and Karthik Sivakumar and Lawrence Chen and Licheng Yu and Luya Gao and Markos Georgopoulos and Rashel Moritz and Sara K. Sampson and Shikai Li and Simone Parmeggiani and Steve Fine and Tara Fowler and Vladan Petrovic and Yuming Du", "abstract": "  We present Movie Gen, a cast of foundation models that generates\nhigh-quality, 1080p HD videos with different aspect ratios and synchronized\naudio. We also show additional capabilities such as precise instruction-based\nvideo editing and generation of personalized videos based on a user's image.\nOur models set a new state-of-the-art on multiple tasks: text-to-video\nsynthesis, video personalization, video editing, video-to-audio generation, and\ntext-to-audio generation. Our largest video generation model is a 30B parameter\ntransformer trained with a maximum context length of 73K video tokens,\ncorresponding to a generated video of 16 seconds at 16 frames-per-second. We\nshow multiple technical innovations and simplifications on the architecture,\nlatent spaces, training objectives and recipes, data curation, evaluation\nprotocols, parallelization techniques, and inference optimizations that allow\nus to reap the benefits of scaling pre-training data, model size, and training\ncompute for training large scale media generation models. We hope this paper\nhelps the research community to accelerate progress and innovation in media\ngeneration models. All videos from this paper are available at\nhttps://go.fb.me/MovieGenResearchVideos.\n", "link": "http://arxiv.org/abs/2410.13720v2", "date": "2025-02-26", "relevancy": 3.1434, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6386}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6268}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Movie%20Gen%3A%20A%20Cast%20of%20Media%20Foundation%20Models&body=Title%3A%20Movie%20Gen%3A%20A%20Cast%20of%20Media%20Foundation%20Models%0AAuthor%3A%20Adam%20Polyak%20and%20Amit%20Zohar%20and%20Andrew%20Brown%20and%20Andros%20Tjandra%20and%20Animesh%20Sinha%20and%20Ann%20Lee%20and%20Apoorv%20Vyas%20and%20Bowen%20Shi%20and%20Chih-Yao%20Ma%20and%20Ching-Yao%20Chuang%20and%20David%20Yan%20and%20Dhruv%20Choudhary%20and%20Dingkang%20Wang%20and%20Geet%20Sethi%20and%20Guan%20Pang%20and%20Haoyu%20Ma%20and%20Ishan%20Misra%20and%20Ji%20Hou%20and%20Jialiang%20Wang%20and%20Kiran%20Jagadeesh%20and%20Kunpeng%20Li%20and%20Luxin%20Zhang%20and%20Mannat%20Singh%20and%20Mary%20Williamson%20and%20Matt%20Le%20and%20Matthew%20Yu%20and%20Mitesh%20Kumar%20Singh%20and%20Peizhao%20Zhang%20and%20Peter%20Vajda%20and%20Quentin%20Duval%20and%20Rohit%20Girdhar%20and%20Roshan%20Sumbaly%20and%20Sai%20Saketh%20Rambhatla%20and%20Sam%20Tsai%20and%20Samaneh%20Azadi%20and%20Samyak%20Datta%20and%20Sanyuan%20Chen%20and%20Sean%20Bell%20and%20Sharadh%20Ramaswamy%20and%20Shelly%20Sheynin%20and%20Siddharth%20Bhattacharya%20and%20Simran%20Motwani%20and%20Tao%20Xu%20and%20Tianhe%20Li%20and%20Tingbo%20Hou%20and%20Wei-Ning%20Hsu%20and%20Xi%20Yin%20and%20Xiaoliang%20Dai%20and%20Yaniv%20Taigman%20and%20Yaqiao%20Luo%20and%20Yen-Cheng%20Liu%20and%20Yi-Chiao%20Wu%20and%20Yue%20Zhao%20and%20Yuval%20Kirstain%20and%20Zecheng%20He%20and%20Zijian%20He%20and%20Albert%20Pumarola%20and%20Ali%20Thabet%20and%20Artsiom%20Sanakoyeu%20and%20Arun%20Mallya%20and%20Baishan%20Guo%20and%20Boris%20Araya%20and%20Breena%20Kerr%20and%20Carleigh%20Wood%20and%20Ce%20Liu%20and%20Cen%20Peng%20and%20Dimitry%20Vengertsev%20and%20Edgar%20Schonfeld%20and%20Elliot%20Blanchard%20and%20Felix%20Juefei-Xu%20and%20Fraylie%20Nord%20and%20Jeff%20Liang%20and%20John%20Hoffman%20and%20Jonas%20Kohler%20and%20Kaolin%20Fire%20and%20Karthik%20Sivakumar%20and%20Lawrence%20Chen%20and%20Licheng%20Yu%20and%20Luya%20Gao%20and%20Markos%20Georgopoulos%20and%20Rashel%20Moritz%20and%20Sara%20K.%20Sampson%20and%20Shikai%20Li%20and%20Simone%20Parmeggiani%20and%20Steve%20Fine%20and%20Tara%20Fowler%20and%20Vladan%20Petrovic%20and%20Yuming%20Du%0AAbstract%3A%20%20%20We%20present%20Movie%20Gen%2C%20a%20cast%20of%20foundation%20models%20that%20generates%0Ahigh-quality%2C%201080p%20HD%20videos%20with%20different%20aspect%20ratios%20and%20synchronized%0Aaudio.%20We%20also%20show%20additional%20capabilities%20such%20as%20precise%20instruction-based%0Avideo%20editing%20and%20generation%20of%20personalized%20videos%20based%20on%20a%20user%27s%20image.%0AOur%20models%20set%20a%20new%20state-of-the-art%20on%20multiple%20tasks%3A%20text-to-video%0Asynthesis%2C%20video%20personalization%2C%20video%20editing%2C%20video-to-audio%20generation%2C%20and%0Atext-to-audio%20generation.%20Our%20largest%20video%20generation%20model%20is%20a%2030B%20parameter%0Atransformer%20trained%20with%20a%20maximum%20context%20length%20of%2073K%20video%20tokens%2C%0Acorresponding%20to%20a%20generated%20video%20of%2016%20seconds%20at%2016%20frames-per-second.%20We%0Ashow%20multiple%20technical%20innovations%20and%20simplifications%20on%20the%20architecture%2C%0Alatent%20spaces%2C%20training%20objectives%20and%20recipes%2C%20data%20curation%2C%20evaluation%0Aprotocols%2C%20parallelization%20techniques%2C%20and%20inference%20optimizations%20that%20allow%0Aus%20to%20reap%20the%20benefits%20of%20scaling%20pre-training%20data%2C%20model%20size%2C%20and%20training%0Acompute%20for%20training%20large%20scale%20media%20generation%20models.%20We%20hope%20this%20paper%0Ahelps%20the%20research%20community%20to%20accelerate%20progress%20and%20innovation%20in%20media%0Ageneration%20models.%20All%20videos%20from%20this%20paper%20are%20available%20at%0Ahttps%3A//go.fb.me/MovieGenResearchVideos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13720v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovie%2520Gen%253A%2520A%2520Cast%2520of%2520Media%2520Foundation%2520Models%26entry.906535625%3DAdam%2520Polyak%2520and%2520Amit%2520Zohar%2520and%2520Andrew%2520Brown%2520and%2520Andros%2520Tjandra%2520and%2520Animesh%2520Sinha%2520and%2520Ann%2520Lee%2520and%2520Apoorv%2520Vyas%2520and%2520Bowen%2520Shi%2520and%2520Chih-Yao%2520Ma%2520and%2520Ching-Yao%2520Chuang%2520and%2520David%2520Yan%2520and%2520Dhruv%2520Choudhary%2520and%2520Dingkang%2520Wang%2520and%2520Geet%2520Sethi%2520and%2520Guan%2520Pang%2520and%2520Haoyu%2520Ma%2520and%2520Ishan%2520Misra%2520and%2520Ji%2520Hou%2520and%2520Jialiang%2520Wang%2520and%2520Kiran%2520Jagadeesh%2520and%2520Kunpeng%2520Li%2520and%2520Luxin%2520Zhang%2520and%2520Mannat%2520Singh%2520and%2520Mary%2520Williamson%2520and%2520Matt%2520Le%2520and%2520Matthew%2520Yu%2520and%2520Mitesh%2520Kumar%2520Singh%2520and%2520Peizhao%2520Zhang%2520and%2520Peter%2520Vajda%2520and%2520Quentin%2520Duval%2520and%2520Rohit%2520Girdhar%2520and%2520Roshan%2520Sumbaly%2520and%2520Sai%2520Saketh%2520Rambhatla%2520and%2520Sam%2520Tsai%2520and%2520Samaneh%2520Azadi%2520and%2520Samyak%2520Datta%2520and%2520Sanyuan%2520Chen%2520and%2520Sean%2520Bell%2520and%2520Sharadh%2520Ramaswamy%2520and%2520Shelly%2520Sheynin%2520and%2520Siddharth%2520Bhattacharya%2520and%2520Simran%2520Motwani%2520and%2520Tao%2520Xu%2520and%2520Tianhe%2520Li%2520and%2520Tingbo%2520Hou%2520and%2520Wei-Ning%2520Hsu%2520and%2520Xi%2520Yin%2520and%2520Xiaoliang%2520Dai%2520and%2520Yaniv%2520Taigman%2520and%2520Yaqiao%2520Luo%2520and%2520Yen-Cheng%2520Liu%2520and%2520Yi-Chiao%2520Wu%2520and%2520Yue%2520Zhao%2520and%2520Yuval%2520Kirstain%2520and%2520Zecheng%2520He%2520and%2520Zijian%2520He%2520and%2520Albert%2520Pumarola%2520and%2520Ali%2520Thabet%2520and%2520Artsiom%2520Sanakoyeu%2520and%2520Arun%2520Mallya%2520and%2520Baishan%2520Guo%2520and%2520Boris%2520Araya%2520and%2520Breena%2520Kerr%2520and%2520Carleigh%2520Wood%2520and%2520Ce%2520Liu%2520and%2520Cen%2520Peng%2520and%2520Dimitry%2520Vengertsev%2520and%2520Edgar%2520Schonfeld%2520and%2520Elliot%2520Blanchard%2520and%2520Felix%2520Juefei-Xu%2520and%2520Fraylie%2520Nord%2520and%2520Jeff%2520Liang%2520and%2520John%2520Hoffman%2520and%2520Jonas%2520Kohler%2520and%2520Kaolin%2520Fire%2520and%2520Karthik%2520Sivakumar%2520and%2520Lawrence%2520Chen%2520and%2520Licheng%2520Yu%2520and%2520Luya%2520Gao%2520and%2520Markos%2520Georgopoulos%2520and%2520Rashel%2520Moritz%2520and%2520Sara%2520K.%2520Sampson%2520and%2520Shikai%2520Li%2520and%2520Simone%2520Parmeggiani%2520and%2520Steve%2520Fine%2520and%2520Tara%2520Fowler%2520and%2520Vladan%2520Petrovic%2520and%2520Yuming%2520Du%26entry.1292438233%3D%2520%2520We%2520present%2520Movie%2520Gen%252C%2520a%2520cast%2520of%2520foundation%2520models%2520that%2520generates%250Ahigh-quality%252C%25201080p%2520HD%2520videos%2520with%2520different%2520aspect%2520ratios%2520and%2520synchronized%250Aaudio.%2520We%2520also%2520show%2520additional%2520capabilities%2520such%2520as%2520precise%2520instruction-based%250Avideo%2520editing%2520and%2520generation%2520of%2520personalized%2520videos%2520based%2520on%2520a%2520user%2527s%2520image.%250AOur%2520models%2520set%2520a%2520new%2520state-of-the-art%2520on%2520multiple%2520tasks%253A%2520text-to-video%250Asynthesis%252C%2520video%2520personalization%252C%2520video%2520editing%252C%2520video-to-audio%2520generation%252C%2520and%250Atext-to-audio%2520generation.%2520Our%2520largest%2520video%2520generation%2520model%2520is%2520a%252030B%2520parameter%250Atransformer%2520trained%2520with%2520a%2520maximum%2520context%2520length%2520of%252073K%2520video%2520tokens%252C%250Acorresponding%2520to%2520a%2520generated%2520video%2520of%252016%2520seconds%2520at%252016%2520frames-per-second.%2520We%250Ashow%2520multiple%2520technical%2520innovations%2520and%2520simplifications%2520on%2520the%2520architecture%252C%250Alatent%2520spaces%252C%2520training%2520objectives%2520and%2520recipes%252C%2520data%2520curation%252C%2520evaluation%250Aprotocols%252C%2520parallelization%2520techniques%252C%2520and%2520inference%2520optimizations%2520that%2520allow%250Aus%2520to%2520reap%2520the%2520benefits%2520of%2520scaling%2520pre-training%2520data%252C%2520model%2520size%252C%2520and%2520training%250Acompute%2520for%2520training%2520large%2520scale%2520media%2520generation%2520models.%2520We%2520hope%2520this%2520paper%250Ahelps%2520the%2520research%2520community%2520to%2520accelerate%2520progress%2520and%2520innovation%2520in%2520media%250Ageneration%2520models.%2520All%2520videos%2520from%2520this%2520paper%2520are%2520available%2520at%250Ahttps%253A//go.fb.me/MovieGenResearchVideos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13720v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Movie%20Gen%3A%20A%20Cast%20of%20Media%20Foundation%20Models&entry.906535625=Adam%20Polyak%20and%20Amit%20Zohar%20and%20Andrew%20Brown%20and%20Andros%20Tjandra%20and%20Animesh%20Sinha%20and%20Ann%20Lee%20and%20Apoorv%20Vyas%20and%20Bowen%20Shi%20and%20Chih-Yao%20Ma%20and%20Ching-Yao%20Chuang%20and%20David%20Yan%20and%20Dhruv%20Choudhary%20and%20Dingkang%20Wang%20and%20Geet%20Sethi%20and%20Guan%20Pang%20and%20Haoyu%20Ma%20and%20Ishan%20Misra%20and%20Ji%20Hou%20and%20Jialiang%20Wang%20and%20Kiran%20Jagadeesh%20and%20Kunpeng%20Li%20and%20Luxin%20Zhang%20and%20Mannat%20Singh%20and%20Mary%20Williamson%20and%20Matt%20Le%20and%20Matthew%20Yu%20and%20Mitesh%20Kumar%20Singh%20and%20Peizhao%20Zhang%20and%20Peter%20Vajda%20and%20Quentin%20Duval%20and%20Rohit%20Girdhar%20and%20Roshan%20Sumbaly%20and%20Sai%20Saketh%20Rambhatla%20and%20Sam%20Tsai%20and%20Samaneh%20Azadi%20and%20Samyak%20Datta%20and%20Sanyuan%20Chen%20and%20Sean%20Bell%20and%20Sharadh%20Ramaswamy%20and%20Shelly%20Sheynin%20and%20Siddharth%20Bhattacharya%20and%20Simran%20Motwani%20and%20Tao%20Xu%20and%20Tianhe%20Li%20and%20Tingbo%20Hou%20and%20Wei-Ning%20Hsu%20and%20Xi%20Yin%20and%20Xiaoliang%20Dai%20and%20Yaniv%20Taigman%20and%20Yaqiao%20Luo%20and%20Yen-Cheng%20Liu%20and%20Yi-Chiao%20Wu%20and%20Yue%20Zhao%20and%20Yuval%20Kirstain%20and%20Zecheng%20He%20and%20Zijian%20He%20and%20Albert%20Pumarola%20and%20Ali%20Thabet%20and%20Artsiom%20Sanakoyeu%20and%20Arun%20Mallya%20and%20Baishan%20Guo%20and%20Boris%20Araya%20and%20Breena%20Kerr%20and%20Carleigh%20Wood%20and%20Ce%20Liu%20and%20Cen%20Peng%20and%20Dimitry%20Vengertsev%20and%20Edgar%20Schonfeld%20and%20Elliot%20Blanchard%20and%20Felix%20Juefei-Xu%20and%20Fraylie%20Nord%20and%20Jeff%20Liang%20and%20John%20Hoffman%20and%20Jonas%20Kohler%20and%20Kaolin%20Fire%20and%20Karthik%20Sivakumar%20and%20Lawrence%20Chen%20and%20Licheng%20Yu%20and%20Luya%20Gao%20and%20Markos%20Georgopoulos%20and%20Rashel%20Moritz%20and%20Sara%20K.%20Sampson%20and%20Shikai%20Li%20and%20Simone%20Parmeggiani%20and%20Steve%20Fine%20and%20Tara%20Fowler%20and%20Vladan%20Petrovic%20and%20Yuming%20Du&entry.1292438233=%20%20We%20present%20Movie%20Gen%2C%20a%20cast%20of%20foundation%20models%20that%20generates%0Ahigh-quality%2C%201080p%20HD%20videos%20with%20different%20aspect%20ratios%20and%20synchronized%0Aaudio.%20We%20also%20show%20additional%20capabilities%20such%20as%20precise%20instruction-based%0Avideo%20editing%20and%20generation%20of%20personalized%20videos%20based%20on%20a%20user%27s%20image.%0AOur%20models%20set%20a%20new%20state-of-the-art%20on%20multiple%20tasks%3A%20text-to-video%0Asynthesis%2C%20video%20personalization%2C%20video%20editing%2C%20video-to-audio%20generation%2C%20and%0Atext-to-audio%20generation.%20Our%20largest%20video%20generation%20model%20is%20a%2030B%20parameter%0Atransformer%20trained%20with%20a%20maximum%20context%20length%20of%2073K%20video%20tokens%2C%0Acorresponding%20to%20a%20generated%20video%20of%2016%20seconds%20at%2016%20frames-per-second.%20We%0Ashow%20multiple%20technical%20innovations%20and%20simplifications%20on%20the%20architecture%2C%0Alatent%20spaces%2C%20training%20objectives%20and%20recipes%2C%20data%20curation%2C%20evaluation%0Aprotocols%2C%20parallelization%20techniques%2C%20and%20inference%20optimizations%20that%20allow%0Aus%20to%20reap%20the%20benefits%20of%20scaling%20pre-training%20data%2C%20model%20size%2C%20and%20training%0Acompute%20for%20training%20large%20scale%20media%20generation%20models.%20We%20hope%20this%20paper%0Ahelps%20the%20research%20community%20to%20accelerate%20progress%20and%20innovation%20in%20media%0Ageneration%20models.%20All%20videos%20from%20this%20paper%20are%20available%20at%0Ahttps%3A//go.fb.me/MovieGenResearchVideos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13720v2&entry.124074799=Read"},
{"title": "LiDAR Registration with Visual Foundation Models", "author": "Niclas V\u00f6disch and Giovanni Cioffi and Marco Cannici and Wolfram Burgard and Davide Scaramuzza", "abstract": "  LiDAR registration is a fundamental task in robotic mapping and localization.\nA critical component of aligning two point clouds is identifying robust point\ncorrespondences using point descriptors. This step becomes particularly\nchallenging in scenarios involving domain shifts, seasonal changes, and\nvariations in point cloud structures. These factors substantially impact both\nhandcrafted and learning-based approaches. In this paper, we address these\nproblems by proposing to use DINOv2 features, obtained from surround-view\nimages, as point descriptors. We demonstrate that coupling these descriptors\nwith traditional registration algorithms, such as RANSAC or ICP, facilitates\nrobust 6DoF alignment of LiDAR scans with 3D maps, even when the map was\nrecorded more than a year before. Although conceptually straightforward, our\nmethod substantially outperforms more complex baseline techniques. In contrast\nto previous learning-based point descriptors, our method does not require\ndomain-specific retraining and is agnostic to the point cloud structure,\neffectively handling both sparse LiDAR scans and dense 3D maps. We show that\nleveraging the additional camera data enables our method to outperform the best\nbaseline by +24.8 and +17.3 registration recall on the NCLT and Oxford RobotCar\ndatasets. We publicly release the registration benchmark and the code of our\nwork on https://vfm-registration.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2502.19374v1", "date": "2025-02-26", "relevancy": 3.1334, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR%20Registration%20with%20Visual%20Foundation%20Models&body=Title%3A%20LiDAR%20Registration%20with%20Visual%20Foundation%20Models%0AAuthor%3A%20Niclas%20V%C3%B6disch%20and%20Giovanni%20Cioffi%20and%20Marco%20Cannici%20and%20Wolfram%20Burgard%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20LiDAR%20registration%20is%20a%20fundamental%20task%20in%20robotic%20mapping%20and%20localization.%0AA%20critical%20component%20of%20aligning%20two%20point%20clouds%20is%20identifying%20robust%20point%0Acorrespondences%20using%20point%20descriptors.%20This%20step%20becomes%20particularly%0Achallenging%20in%20scenarios%20involving%20domain%20shifts%2C%20seasonal%20changes%2C%20and%0Avariations%20in%20point%20cloud%20structures.%20These%20factors%20substantially%20impact%20both%0Ahandcrafted%20and%20learning-based%20approaches.%20In%20this%20paper%2C%20we%20address%20these%0Aproblems%20by%20proposing%20to%20use%20DINOv2%20features%2C%20obtained%20from%20surround-view%0Aimages%2C%20as%20point%20descriptors.%20We%20demonstrate%20that%20coupling%20these%20descriptors%0Awith%20traditional%20registration%20algorithms%2C%20such%20as%20RANSAC%20or%20ICP%2C%20facilitates%0Arobust%206DoF%20alignment%20of%20LiDAR%20scans%20with%203D%20maps%2C%20even%20when%20the%20map%20was%0Arecorded%20more%20than%20a%20year%20before.%20Although%20conceptually%20straightforward%2C%20our%0Amethod%20substantially%20outperforms%20more%20complex%20baseline%20techniques.%20In%20contrast%0Ato%20previous%20learning-based%20point%20descriptors%2C%20our%20method%20does%20not%20require%0Adomain-specific%20retraining%20and%20is%20agnostic%20to%20the%20point%20cloud%20structure%2C%0Aeffectively%20handling%20both%20sparse%20LiDAR%20scans%20and%20dense%203D%20maps.%20We%20show%20that%0Aleveraging%20the%20additional%20camera%20data%20enables%20our%20method%20to%20outperform%20the%20best%0Abaseline%20by%20%2B24.8%20and%20%2B17.3%20registration%20recall%20on%20the%20NCLT%20and%20Oxford%20RobotCar%0Adatasets.%20We%20publicly%20release%20the%20registration%20benchmark%20and%20the%20code%20of%20our%0Awork%20on%20https%3A//vfm-registration.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR%2520Registration%2520with%2520Visual%2520Foundation%2520Models%26entry.906535625%3DNiclas%2520V%25C3%25B6disch%2520and%2520Giovanni%2520Cioffi%2520and%2520Marco%2520Cannici%2520and%2520Wolfram%2520Burgard%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520LiDAR%2520registration%2520is%2520a%2520fundamental%2520task%2520in%2520robotic%2520mapping%2520and%2520localization.%250AA%2520critical%2520component%2520of%2520aligning%2520two%2520point%2520clouds%2520is%2520identifying%2520robust%2520point%250Acorrespondences%2520using%2520point%2520descriptors.%2520This%2520step%2520becomes%2520particularly%250Achallenging%2520in%2520scenarios%2520involving%2520domain%2520shifts%252C%2520seasonal%2520changes%252C%2520and%250Avariations%2520in%2520point%2520cloud%2520structures.%2520These%2520factors%2520substantially%2520impact%2520both%250Ahandcrafted%2520and%2520learning-based%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%250Aproblems%2520by%2520proposing%2520to%2520use%2520DINOv2%2520features%252C%2520obtained%2520from%2520surround-view%250Aimages%252C%2520as%2520point%2520descriptors.%2520We%2520demonstrate%2520that%2520coupling%2520these%2520descriptors%250Awith%2520traditional%2520registration%2520algorithms%252C%2520such%2520as%2520RANSAC%2520or%2520ICP%252C%2520facilitates%250Arobust%25206DoF%2520alignment%2520of%2520LiDAR%2520scans%2520with%25203D%2520maps%252C%2520even%2520when%2520the%2520map%2520was%250Arecorded%2520more%2520than%2520a%2520year%2520before.%2520Although%2520conceptually%2520straightforward%252C%2520our%250Amethod%2520substantially%2520outperforms%2520more%2520complex%2520baseline%2520techniques.%2520In%2520contrast%250Ato%2520previous%2520learning-based%2520point%2520descriptors%252C%2520our%2520method%2520does%2520not%2520require%250Adomain-specific%2520retraining%2520and%2520is%2520agnostic%2520to%2520the%2520point%2520cloud%2520structure%252C%250Aeffectively%2520handling%2520both%2520sparse%2520LiDAR%2520scans%2520and%2520dense%25203D%2520maps.%2520We%2520show%2520that%250Aleveraging%2520the%2520additional%2520camera%2520data%2520enables%2520our%2520method%2520to%2520outperform%2520the%2520best%250Abaseline%2520by%2520%252B24.8%2520and%2520%252B17.3%2520registration%2520recall%2520on%2520the%2520NCLT%2520and%2520Oxford%2520RobotCar%250Adatasets.%2520We%2520publicly%2520release%2520the%2520registration%2520benchmark%2520and%2520the%2520code%2520of%2520our%250Awork%2520on%2520https%253A//vfm-registration.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR%20Registration%20with%20Visual%20Foundation%20Models&entry.906535625=Niclas%20V%C3%B6disch%20and%20Giovanni%20Cioffi%20and%20Marco%20Cannici%20and%20Wolfram%20Burgard%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20LiDAR%20registration%20is%20a%20fundamental%20task%20in%20robotic%20mapping%20and%20localization.%0AA%20critical%20component%20of%20aligning%20two%20point%20clouds%20is%20identifying%20robust%20point%0Acorrespondences%20using%20point%20descriptors.%20This%20step%20becomes%20particularly%0Achallenging%20in%20scenarios%20involving%20domain%20shifts%2C%20seasonal%20changes%2C%20and%0Avariations%20in%20point%20cloud%20structures.%20These%20factors%20substantially%20impact%20both%0Ahandcrafted%20and%20learning-based%20approaches.%20In%20this%20paper%2C%20we%20address%20these%0Aproblems%20by%20proposing%20to%20use%20DINOv2%20features%2C%20obtained%20from%20surround-view%0Aimages%2C%20as%20point%20descriptors.%20We%20demonstrate%20that%20coupling%20these%20descriptors%0Awith%20traditional%20registration%20algorithms%2C%20such%20as%20RANSAC%20or%20ICP%2C%20facilitates%0Arobust%206DoF%20alignment%20of%20LiDAR%20scans%20with%203D%20maps%2C%20even%20when%20the%20map%20was%0Arecorded%20more%20than%20a%20year%20before.%20Although%20conceptually%20straightforward%2C%20our%0Amethod%20substantially%20outperforms%20more%20complex%20baseline%20techniques.%20In%20contrast%0Ato%20previous%20learning-based%20point%20descriptors%2C%20our%20method%20does%20not%20require%0Adomain-specific%20retraining%20and%20is%20agnostic%20to%20the%20point%20cloud%20structure%2C%0Aeffectively%20handling%20both%20sparse%20LiDAR%20scans%20and%20dense%203D%20maps.%20We%20show%20that%0Aleveraging%20the%20additional%20camera%20data%20enables%20our%20method%20to%20outperform%20the%20best%0Abaseline%20by%20%2B24.8%20and%20%2B17.3%20registration%20recall%20on%20the%20NCLT%20and%20Oxford%20RobotCar%0Adatasets.%20We%20publicly%20release%20the%20registration%20benchmark%20and%20the%20code%20of%20our%0Awork%20on%20https%3A//vfm-registration.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19374v1&entry.124074799=Read"},
{"title": "SCA3D: Enhancing Cross-modal 3D Retrieval via 3D Shape and Caption\n  Paired Data Augmentation", "author": "Junlong Ren and Hao Wu and Hui Xiong and Hao Wang", "abstract": "  The cross-modal 3D retrieval task aims to achieve mutual matching between\ntext descriptions and 3D shapes. This has the potential to enhance the\ninteraction between natural language and the 3D environment, especially within\nthe realms of robotics and embodied artificial intelligence (AI) applications.\nHowever, the scarcity and expensiveness of 3D data constrain the performance of\nexisting cross-modal 3D retrieval methods. These methods heavily rely on\nfeatures derived from the limited number of 3D shapes, resulting in poor\ngeneralization ability across diverse scenarios. To address this challenge, we\nintroduce SCA3D, a novel 3D shape and caption online data augmentation method\nfor cross-modal 3D retrieval. Our approach uses the LLaVA model to create a\ncomponent library, captioning each segmented part of every 3D shape within the\ndataset. Notably, it facilitates the generation of extensive new 3D-text pairs\ncontaining new semantic features. We employ both inter and intra distances to\nalign various components into a new 3D shape, ensuring that the components do\nnot overlap and are closely fitted. Further, text templates are utilized to\nprocess the captions of each component and generate new text descriptions.\nBesides, we use unimodal encoders to extract embeddings for 3D shapes and texts\nbased on the enriched dataset. We then calculate fine-grained cross-modal\nsimilarity using Earth Mover's Distance (EMD) and enhance cross-modal matching\nwith contrastive learning, enabling bidirectional retrieval between texts and\n3D shapes. Extensive experiments show our SCA3D outperforms previous works on\nthe Text2Shape dataset, raising the Shape-to-Text RR@1 score from 20.03 to\n27.22 and the Text-to-Shape RR@1 score from 13.12 to 16.67. Codes can be found\nin https://github.com/3DAgentWorld/SCA3D.\n", "link": "http://arxiv.org/abs/2502.19128v1", "date": "2025-02-26", "relevancy": 3.0849, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6908}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.58}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCA3D%3A%20Enhancing%20Cross-modal%203D%20Retrieval%20via%203D%20Shape%20and%20Caption%0A%20%20Paired%20Data%20Augmentation&body=Title%3A%20SCA3D%3A%20Enhancing%20Cross-modal%203D%20Retrieval%20via%203D%20Shape%20and%20Caption%0A%20%20Paired%20Data%20Augmentation%0AAuthor%3A%20Junlong%20Ren%20and%20Hao%20Wu%20and%20Hui%20Xiong%20and%20Hao%20Wang%0AAbstract%3A%20%20%20The%20cross-modal%203D%20retrieval%20task%20aims%20to%20achieve%20mutual%20matching%20between%0Atext%20descriptions%20and%203D%20shapes.%20This%20has%20the%20potential%20to%20enhance%20the%0Ainteraction%20between%20natural%20language%20and%20the%203D%20environment%2C%20especially%20within%0Athe%20realms%20of%20robotics%20and%20embodied%20artificial%20intelligence%20%28AI%29%20applications.%0AHowever%2C%20the%20scarcity%20and%20expensiveness%20of%203D%20data%20constrain%20the%20performance%20of%0Aexisting%20cross-modal%203D%20retrieval%20methods.%20These%20methods%20heavily%20rely%20on%0Afeatures%20derived%20from%20the%20limited%20number%20of%203D%20shapes%2C%20resulting%20in%20poor%0Ageneralization%20ability%20across%20diverse%20scenarios.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20SCA3D%2C%20a%20novel%203D%20shape%20and%20caption%20online%20data%20augmentation%20method%0Afor%20cross-modal%203D%20retrieval.%20Our%20approach%20uses%20the%20LLaVA%20model%20to%20create%20a%0Acomponent%20library%2C%20captioning%20each%20segmented%20part%20of%20every%203D%20shape%20within%20the%0Adataset.%20Notably%2C%20it%20facilitates%20the%20generation%20of%20extensive%20new%203D-text%20pairs%0Acontaining%20new%20semantic%20features.%20We%20employ%20both%20inter%20and%20intra%20distances%20to%0Aalign%20various%20components%20into%20a%20new%203D%20shape%2C%20ensuring%20that%20the%20components%20do%0Anot%20overlap%20and%20are%20closely%20fitted.%20Further%2C%20text%20templates%20are%20utilized%20to%0Aprocess%20the%20captions%20of%20each%20component%20and%20generate%20new%20text%20descriptions.%0ABesides%2C%20we%20use%20unimodal%20encoders%20to%20extract%20embeddings%20for%203D%20shapes%20and%20texts%0Abased%20on%20the%20enriched%20dataset.%20We%20then%20calculate%20fine-grained%20cross-modal%0Asimilarity%20using%20Earth%20Mover%27s%20Distance%20%28EMD%29%20and%20enhance%20cross-modal%20matching%0Awith%20contrastive%20learning%2C%20enabling%20bidirectional%20retrieval%20between%20texts%20and%0A3D%20shapes.%20Extensive%20experiments%20show%20our%20SCA3D%20outperforms%20previous%20works%20on%0Athe%20Text2Shape%20dataset%2C%20raising%20the%20Shape-to-Text%20RR%401%20score%20from%2020.03%20to%0A27.22%20and%20the%20Text-to-Shape%20RR%401%20score%20from%2013.12%20to%2016.67.%20Codes%20can%20be%20found%0Ain%20https%3A//github.com/3DAgentWorld/SCA3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCA3D%253A%2520Enhancing%2520Cross-modal%25203D%2520Retrieval%2520via%25203D%2520Shape%2520and%2520Caption%250A%2520%2520Paired%2520Data%2520Augmentation%26entry.906535625%3DJunlong%2520Ren%2520and%2520Hao%2520Wu%2520and%2520Hui%2520Xiong%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520The%2520cross-modal%25203D%2520retrieval%2520task%2520aims%2520to%2520achieve%2520mutual%2520matching%2520between%250Atext%2520descriptions%2520and%25203D%2520shapes.%2520This%2520has%2520the%2520potential%2520to%2520enhance%2520the%250Ainteraction%2520between%2520natural%2520language%2520and%2520the%25203D%2520environment%252C%2520especially%2520within%250Athe%2520realms%2520of%2520robotics%2520and%2520embodied%2520artificial%2520intelligence%2520%2528AI%2529%2520applications.%250AHowever%252C%2520the%2520scarcity%2520and%2520expensiveness%2520of%25203D%2520data%2520constrain%2520the%2520performance%2520of%250Aexisting%2520cross-modal%25203D%2520retrieval%2520methods.%2520These%2520methods%2520heavily%2520rely%2520on%250Afeatures%2520derived%2520from%2520the%2520limited%2520number%2520of%25203D%2520shapes%252C%2520resulting%2520in%2520poor%250Ageneralization%2520ability%2520across%2520diverse%2520scenarios.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520SCA3D%252C%2520a%2520novel%25203D%2520shape%2520and%2520caption%2520online%2520data%2520augmentation%2520method%250Afor%2520cross-modal%25203D%2520retrieval.%2520Our%2520approach%2520uses%2520the%2520LLaVA%2520model%2520to%2520create%2520a%250Acomponent%2520library%252C%2520captioning%2520each%2520segmented%2520part%2520of%2520every%25203D%2520shape%2520within%2520the%250Adataset.%2520Notably%252C%2520it%2520facilitates%2520the%2520generation%2520of%2520extensive%2520new%25203D-text%2520pairs%250Acontaining%2520new%2520semantic%2520features.%2520We%2520employ%2520both%2520inter%2520and%2520intra%2520distances%2520to%250Aalign%2520various%2520components%2520into%2520a%2520new%25203D%2520shape%252C%2520ensuring%2520that%2520the%2520components%2520do%250Anot%2520overlap%2520and%2520are%2520closely%2520fitted.%2520Further%252C%2520text%2520templates%2520are%2520utilized%2520to%250Aprocess%2520the%2520captions%2520of%2520each%2520component%2520and%2520generate%2520new%2520text%2520descriptions.%250ABesides%252C%2520we%2520use%2520unimodal%2520encoders%2520to%2520extract%2520embeddings%2520for%25203D%2520shapes%2520and%2520texts%250Abased%2520on%2520the%2520enriched%2520dataset.%2520We%2520then%2520calculate%2520fine-grained%2520cross-modal%250Asimilarity%2520using%2520Earth%2520Mover%2527s%2520Distance%2520%2528EMD%2529%2520and%2520enhance%2520cross-modal%2520matching%250Awith%2520contrastive%2520learning%252C%2520enabling%2520bidirectional%2520retrieval%2520between%2520texts%2520and%250A3D%2520shapes.%2520Extensive%2520experiments%2520show%2520our%2520SCA3D%2520outperforms%2520previous%2520works%2520on%250Athe%2520Text2Shape%2520dataset%252C%2520raising%2520the%2520Shape-to-Text%2520RR%25401%2520score%2520from%252020.03%2520to%250A27.22%2520and%2520the%2520Text-to-Shape%2520RR%25401%2520score%2520from%252013.12%2520to%252016.67.%2520Codes%2520can%2520be%2520found%250Ain%2520https%253A//github.com/3DAgentWorld/SCA3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCA3D%3A%20Enhancing%20Cross-modal%203D%20Retrieval%20via%203D%20Shape%20and%20Caption%0A%20%20Paired%20Data%20Augmentation&entry.906535625=Junlong%20Ren%20and%20Hao%20Wu%20and%20Hui%20Xiong%20and%20Hao%20Wang&entry.1292438233=%20%20The%20cross-modal%203D%20retrieval%20task%20aims%20to%20achieve%20mutual%20matching%20between%0Atext%20descriptions%20and%203D%20shapes.%20This%20has%20the%20potential%20to%20enhance%20the%0Ainteraction%20between%20natural%20language%20and%20the%203D%20environment%2C%20especially%20within%0Athe%20realms%20of%20robotics%20and%20embodied%20artificial%20intelligence%20%28AI%29%20applications.%0AHowever%2C%20the%20scarcity%20and%20expensiveness%20of%203D%20data%20constrain%20the%20performance%20of%0Aexisting%20cross-modal%203D%20retrieval%20methods.%20These%20methods%20heavily%20rely%20on%0Afeatures%20derived%20from%20the%20limited%20number%20of%203D%20shapes%2C%20resulting%20in%20poor%0Ageneralization%20ability%20across%20diverse%20scenarios.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20SCA3D%2C%20a%20novel%203D%20shape%20and%20caption%20online%20data%20augmentation%20method%0Afor%20cross-modal%203D%20retrieval.%20Our%20approach%20uses%20the%20LLaVA%20model%20to%20create%20a%0Acomponent%20library%2C%20captioning%20each%20segmented%20part%20of%20every%203D%20shape%20within%20the%0Adataset.%20Notably%2C%20it%20facilitates%20the%20generation%20of%20extensive%20new%203D-text%20pairs%0Acontaining%20new%20semantic%20features.%20We%20employ%20both%20inter%20and%20intra%20distances%20to%0Aalign%20various%20components%20into%20a%20new%203D%20shape%2C%20ensuring%20that%20the%20components%20do%0Anot%20overlap%20and%20are%20closely%20fitted.%20Further%2C%20text%20templates%20are%20utilized%20to%0Aprocess%20the%20captions%20of%20each%20component%20and%20generate%20new%20text%20descriptions.%0ABesides%2C%20we%20use%20unimodal%20encoders%20to%20extract%20embeddings%20for%203D%20shapes%20and%20texts%0Abased%20on%20the%20enriched%20dataset.%20We%20then%20calculate%20fine-grained%20cross-modal%0Asimilarity%20using%20Earth%20Mover%27s%20Distance%20%28EMD%29%20and%20enhance%20cross-modal%20matching%0Awith%20contrastive%20learning%2C%20enabling%20bidirectional%20retrieval%20between%20texts%20and%0A3D%20shapes.%20Extensive%20experiments%20show%20our%20SCA3D%20outperforms%20previous%20works%20on%0Athe%20Text2Shape%20dataset%2C%20raising%20the%20Shape-to-Text%20RR%401%20score%20from%2020.03%20to%0A27.22%20and%20the%20Text-to-Shape%20RR%401%20score%20from%2013.12%20to%2016.67.%20Codes%20can%20be%20found%0Ain%20https%3A//github.com/3DAgentWorld/SCA3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19128v1&entry.124074799=Read"},
{"title": "Latte++: Spatial-Temporal Voxel-based Test-Time Adaptation for\n  Multi-Modal Segmentation", "author": "Haozhi Cao and Yuecong Xu and Pengyu Yin and Xingyu Ji and Shenghai Yuan and Jianfei Yang and Lihua Xie", "abstract": "  Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an\nunlabeled target domain by leveraging the complementary multi-modal inputs in\nan online manner. Previous MM-TTA methods for 3D segmentation rely on\npredictions of cross-modal information in each input frame, while they ignore\nthe fact that predictions of geometric neighborhoods within consecutive frames\nare highly correlated, leading to unstable predictions across time. To fulfill\nthis gap, we propose ReLiable Spatial-temporal Voxels (Latte), an MM-TTA method\nthat leverages reliable cross-modal spatial-temporal correspondences for\nmulti-modal 3D segmentation. Motivated by the fact that reliable predictions\nshould be consistent with their spatial-temporal correspondences, Latte\naggregates consecutive frames in a sliding-window manner and constructs\nSpatial-Temporal (ST) voxels to capture temporally local prediction consistency\nfor each modality. After filtering out ST voxels with high ST entropy, Latte\nconducts cross-modal learning for each point and pixel by attending to those\nwith reliable and consistent predictions among both spatial and temporal\nneighborhoods. Considering the prediction consistency might vary under\ndifferent sliding windows, we further propose Latte++ which leverages ST voxels\ngenerated under various sliding windows to more thoroughly evaluate intra-modal\nprediction consistency before the cross-modal fusion. Experimental results show\nthat both Latte and Latte++ achieve state-of-the-art performance on five MM-TTA\nbenchmarks compared to previous MM-TTA or TTA methods. Code will be available\nat https://github.com/AronCao49/Latte-plusplus.\n", "link": "http://arxiv.org/abs/2403.06461v4", "date": "2025-02-26", "relevancy": 3.0214, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6817}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5733}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latte%2B%2B%3A%20Spatial-Temporal%20Voxel-based%20Test-Time%20Adaptation%20for%0A%20%20Multi-Modal%20Segmentation&body=Title%3A%20Latte%2B%2B%3A%20Spatial-Temporal%20Voxel-based%20Test-Time%20Adaptation%20for%0A%20%20Multi-Modal%20Segmentation%0AAuthor%3A%20Haozhi%20Cao%20and%20Yuecong%20Xu%20and%20Pengyu%20Yin%20and%20Xingyu%20Ji%20and%20Shenghai%20Yuan%20and%20Jianfei%20Yang%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20Multi-modal%20test-time%20adaptation%20%28MM-TTA%29%20is%20proposed%20to%20adapt%20models%20to%20an%0Aunlabeled%20target%20domain%20by%20leveraging%20the%20complementary%20multi-modal%20inputs%20in%0Aan%20online%20manner.%20Previous%20MM-TTA%20methods%20for%203D%20segmentation%20rely%20on%0Apredictions%20of%20cross-modal%20information%20in%20each%20input%20frame%2C%20while%20they%20ignore%0Athe%20fact%20that%20predictions%20of%20geometric%20neighborhoods%20within%20consecutive%20frames%0Aare%20highly%20correlated%2C%20leading%20to%20unstable%20predictions%20across%20time.%20To%20fulfill%0Athis%20gap%2C%20we%20propose%20ReLiable%20Spatial-temporal%20Voxels%20%28Latte%29%2C%20an%20MM-TTA%20method%0Athat%20leverages%20reliable%20cross-modal%20spatial-temporal%20correspondences%20for%0Amulti-modal%203D%20segmentation.%20Motivated%20by%20the%20fact%20that%20reliable%20predictions%0Ashould%20be%20consistent%20with%20their%20spatial-temporal%20correspondences%2C%20Latte%0Aaggregates%20consecutive%20frames%20in%20a%20sliding-window%20manner%20and%20constructs%0ASpatial-Temporal%20%28ST%29%20voxels%20to%20capture%20temporally%20local%20prediction%20consistency%0Afor%20each%20modality.%20After%20filtering%20out%20ST%20voxels%20with%20high%20ST%20entropy%2C%20Latte%0Aconducts%20cross-modal%20learning%20for%20each%20point%20and%20pixel%20by%20attending%20to%20those%0Awith%20reliable%20and%20consistent%20predictions%20among%20both%20spatial%20and%20temporal%0Aneighborhoods.%20Considering%20the%20prediction%20consistency%20might%20vary%20under%0Adifferent%20sliding%20windows%2C%20we%20further%20propose%20Latte%2B%2B%20which%20leverages%20ST%20voxels%0Agenerated%20under%20various%20sliding%20windows%20to%20more%20thoroughly%20evaluate%20intra-modal%0Aprediction%20consistency%20before%20the%20cross-modal%20fusion.%20Experimental%20results%20show%0Athat%20both%20Latte%20and%20Latte%2B%2B%20achieve%20state-of-the-art%20performance%20on%20five%20MM-TTA%0Abenchmarks%20compared%20to%20previous%20MM-TTA%20or%20TTA%20methods.%20Code%20will%20be%20available%0Aat%20https%3A//github.com/AronCao49/Latte-plusplus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06461v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatte%252B%252B%253A%2520Spatial-Temporal%2520Voxel-based%2520Test-Time%2520Adaptation%2520for%250A%2520%2520Multi-Modal%2520Segmentation%26entry.906535625%3DHaozhi%2520Cao%2520and%2520Yuecong%2520Xu%2520and%2520Pengyu%2520Yin%2520and%2520Xingyu%2520Ji%2520and%2520Shenghai%2520Yuan%2520and%2520Jianfei%2520Yang%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520Multi-modal%2520test-time%2520adaptation%2520%2528MM-TTA%2529%2520is%2520proposed%2520to%2520adapt%2520models%2520to%2520an%250Aunlabeled%2520target%2520domain%2520by%2520leveraging%2520the%2520complementary%2520multi-modal%2520inputs%2520in%250Aan%2520online%2520manner.%2520Previous%2520MM-TTA%2520methods%2520for%25203D%2520segmentation%2520rely%2520on%250Apredictions%2520of%2520cross-modal%2520information%2520in%2520each%2520input%2520frame%252C%2520while%2520they%2520ignore%250Athe%2520fact%2520that%2520predictions%2520of%2520geometric%2520neighborhoods%2520within%2520consecutive%2520frames%250Aare%2520highly%2520correlated%252C%2520leading%2520to%2520unstable%2520predictions%2520across%2520time.%2520To%2520fulfill%250Athis%2520gap%252C%2520we%2520propose%2520ReLiable%2520Spatial-temporal%2520Voxels%2520%2528Latte%2529%252C%2520an%2520MM-TTA%2520method%250Athat%2520leverages%2520reliable%2520cross-modal%2520spatial-temporal%2520correspondences%2520for%250Amulti-modal%25203D%2520segmentation.%2520Motivated%2520by%2520the%2520fact%2520that%2520reliable%2520predictions%250Ashould%2520be%2520consistent%2520with%2520their%2520spatial-temporal%2520correspondences%252C%2520Latte%250Aaggregates%2520consecutive%2520frames%2520in%2520a%2520sliding-window%2520manner%2520and%2520constructs%250ASpatial-Temporal%2520%2528ST%2529%2520voxels%2520to%2520capture%2520temporally%2520local%2520prediction%2520consistency%250Afor%2520each%2520modality.%2520After%2520filtering%2520out%2520ST%2520voxels%2520with%2520high%2520ST%2520entropy%252C%2520Latte%250Aconducts%2520cross-modal%2520learning%2520for%2520each%2520point%2520and%2520pixel%2520by%2520attending%2520to%2520those%250Awith%2520reliable%2520and%2520consistent%2520predictions%2520among%2520both%2520spatial%2520and%2520temporal%250Aneighborhoods.%2520Considering%2520the%2520prediction%2520consistency%2520might%2520vary%2520under%250Adifferent%2520sliding%2520windows%252C%2520we%2520further%2520propose%2520Latte%252B%252B%2520which%2520leverages%2520ST%2520voxels%250Agenerated%2520under%2520various%2520sliding%2520windows%2520to%2520more%2520thoroughly%2520evaluate%2520intra-modal%250Aprediction%2520consistency%2520before%2520the%2520cross-modal%2520fusion.%2520Experimental%2520results%2520show%250Athat%2520both%2520Latte%2520and%2520Latte%252B%252B%2520achieve%2520state-of-the-art%2520performance%2520on%2520five%2520MM-TTA%250Abenchmarks%2520compared%2520to%2520previous%2520MM-TTA%2520or%2520TTA%2520methods.%2520Code%2520will%2520be%2520available%250Aat%2520https%253A//github.com/AronCao49/Latte-plusplus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06461v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latte%2B%2B%3A%20Spatial-Temporal%20Voxel-based%20Test-Time%20Adaptation%20for%0A%20%20Multi-Modal%20Segmentation&entry.906535625=Haozhi%20Cao%20and%20Yuecong%20Xu%20and%20Pengyu%20Yin%20and%20Xingyu%20Ji%20and%20Shenghai%20Yuan%20and%20Jianfei%20Yang%20and%20Lihua%20Xie&entry.1292438233=%20%20Multi-modal%20test-time%20adaptation%20%28MM-TTA%29%20is%20proposed%20to%20adapt%20models%20to%20an%0Aunlabeled%20target%20domain%20by%20leveraging%20the%20complementary%20multi-modal%20inputs%20in%0Aan%20online%20manner.%20Previous%20MM-TTA%20methods%20for%203D%20segmentation%20rely%20on%0Apredictions%20of%20cross-modal%20information%20in%20each%20input%20frame%2C%20while%20they%20ignore%0Athe%20fact%20that%20predictions%20of%20geometric%20neighborhoods%20within%20consecutive%20frames%0Aare%20highly%20correlated%2C%20leading%20to%20unstable%20predictions%20across%20time.%20To%20fulfill%0Athis%20gap%2C%20we%20propose%20ReLiable%20Spatial-temporal%20Voxels%20%28Latte%29%2C%20an%20MM-TTA%20method%0Athat%20leverages%20reliable%20cross-modal%20spatial-temporal%20correspondences%20for%0Amulti-modal%203D%20segmentation.%20Motivated%20by%20the%20fact%20that%20reliable%20predictions%0Ashould%20be%20consistent%20with%20their%20spatial-temporal%20correspondences%2C%20Latte%0Aaggregates%20consecutive%20frames%20in%20a%20sliding-window%20manner%20and%20constructs%0ASpatial-Temporal%20%28ST%29%20voxels%20to%20capture%20temporally%20local%20prediction%20consistency%0Afor%20each%20modality.%20After%20filtering%20out%20ST%20voxels%20with%20high%20ST%20entropy%2C%20Latte%0Aconducts%20cross-modal%20learning%20for%20each%20point%20and%20pixel%20by%20attending%20to%20those%0Awith%20reliable%20and%20consistent%20predictions%20among%20both%20spatial%20and%20temporal%0Aneighborhoods.%20Considering%20the%20prediction%20consistency%20might%20vary%20under%0Adifferent%20sliding%20windows%2C%20we%20further%20propose%20Latte%2B%2B%20which%20leverages%20ST%20voxels%0Agenerated%20under%20various%20sliding%20windows%20to%20more%20thoroughly%20evaluate%20intra-modal%0Aprediction%20consistency%20before%20the%20cross-modal%20fusion.%20Experimental%20results%20show%0Athat%20both%20Latte%20and%20Latte%2B%2B%20achieve%20state-of-the-art%20performance%20on%20five%20MM-TTA%0Abenchmarks%20compared%20to%20previous%20MM-TTA%20or%20TTA%20methods.%20Code%20will%20be%20available%0Aat%20https%3A//github.com/AronCao49/Latte-plusplus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06461v4&entry.124074799=Read"},
{"title": "ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal\n  Large Language Models", "author": "Danae S\u00e1nchez Villegas and Ingo Ziegler and Desmond Elliott", "abstract": "  Reasoning over sequences of images remains a challenge for multimodal large\nlanguage models (MLLMs). While recent models incorporate multi-image data\nduring pre-training, they still struggle to recognize sequential structures,\noften treating images independently. This work introduces ImageChain, a\nframework that enhances MLLMs with sequential reasoning capabilities over image\ndata by modeling visual sequences as a multi-turn conversation. In ImageChain,\nimages are interleaved with corresponding textual descriptions to form a\ncontrolled dialogue that explicitly captures temporal dependencies and\nnarrative progression. Our method optimizes for the task of next-scene\ndescription, where the model generates a context-aware description of an\nupcoming scene based on preceding visual and textual cues. We demonstrate that\nour approach improves performance on the next-scene description task --\nachieving an average improvement from 3.7% to 19% in SimRate, a metric that\nquantifies semantic similarity to human-annotated ground truths. Moreover,\nImageChain achieves robust zero-shot out-of-domain performance in applications\nranging from comics to robotics. Extensive experiments validate that\ninstruction-tuning in a multimodal, multi-turn conversation design is key to\nbridging the gap between static image understanding and temporally-aware\nreasoning.\n", "link": "http://arxiv.org/abs/2502.19409v1", "date": "2025-02-26", "relevancy": 2.9783, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImageChain%3A%20Advancing%20Sequential%20Image-to-Text%20Reasoning%20in%20Multimodal%0A%20%20Large%20Language%20Models&body=Title%3A%20ImageChain%3A%20Advancing%20Sequential%20Image-to-Text%20Reasoning%20in%20Multimodal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Danae%20S%C3%A1nchez%20Villegas%20and%20Ingo%20Ziegler%20and%20Desmond%20Elliott%0AAbstract%3A%20%20%20Reasoning%20over%20sequences%20of%20images%20remains%20a%20challenge%20for%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20While%20recent%20models%20incorporate%20multi-image%20data%0Aduring%20pre-training%2C%20they%20still%20struggle%20to%20recognize%20sequential%20structures%2C%0Aoften%20treating%20images%20independently.%20This%20work%20introduces%20ImageChain%2C%20a%0Aframework%20that%20enhances%20MLLMs%20with%20sequential%20reasoning%20capabilities%20over%20image%0Adata%20by%20modeling%20visual%20sequences%20as%20a%20multi-turn%20conversation.%20In%20ImageChain%2C%0Aimages%20are%20interleaved%20with%20corresponding%20textual%20descriptions%20to%20form%20a%0Acontrolled%20dialogue%20that%20explicitly%20captures%20temporal%20dependencies%20and%0Anarrative%20progression.%20Our%20method%20optimizes%20for%20the%20task%20of%20next-scene%0Adescription%2C%20where%20the%20model%20generates%20a%20context-aware%20description%20of%20an%0Aupcoming%20scene%20based%20on%20preceding%20visual%20and%20textual%20cues.%20We%20demonstrate%20that%0Aour%20approach%20improves%20performance%20on%20the%20next-scene%20description%20task%20--%0Aachieving%20an%20average%20improvement%20from%203.7%25%20to%2019%25%20in%20SimRate%2C%20a%20metric%20that%0Aquantifies%20semantic%20similarity%20to%20human-annotated%20ground%20truths.%20Moreover%2C%0AImageChain%20achieves%20robust%20zero-shot%20out-of-domain%20performance%20in%20applications%0Aranging%20from%20comics%20to%20robotics.%20Extensive%20experiments%20validate%20that%0Ainstruction-tuning%20in%20a%20multimodal%2C%20multi-turn%20conversation%20design%20is%20key%20to%0Abridging%20the%20gap%20between%20static%20image%20understanding%20and%20temporally-aware%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImageChain%253A%2520Advancing%2520Sequential%2520Image-to-Text%2520Reasoning%2520in%2520Multimodal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DDanae%2520S%25C3%25A1nchez%2520Villegas%2520and%2520Ingo%2520Ziegler%2520and%2520Desmond%2520Elliott%26entry.1292438233%3D%2520%2520Reasoning%2520over%2520sequences%2520of%2520images%2520remains%2520a%2520challenge%2520for%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529.%2520While%2520recent%2520models%2520incorporate%2520multi-image%2520data%250Aduring%2520pre-training%252C%2520they%2520still%2520struggle%2520to%2520recognize%2520sequential%2520structures%252C%250Aoften%2520treating%2520images%2520independently.%2520This%2520work%2520introduces%2520ImageChain%252C%2520a%250Aframework%2520that%2520enhances%2520MLLMs%2520with%2520sequential%2520reasoning%2520capabilities%2520over%2520image%250Adata%2520by%2520modeling%2520visual%2520sequences%2520as%2520a%2520multi-turn%2520conversation.%2520In%2520ImageChain%252C%250Aimages%2520are%2520interleaved%2520with%2520corresponding%2520textual%2520descriptions%2520to%2520form%2520a%250Acontrolled%2520dialogue%2520that%2520explicitly%2520captures%2520temporal%2520dependencies%2520and%250Anarrative%2520progression.%2520Our%2520method%2520optimizes%2520for%2520the%2520task%2520of%2520next-scene%250Adescription%252C%2520where%2520the%2520model%2520generates%2520a%2520context-aware%2520description%2520of%2520an%250Aupcoming%2520scene%2520based%2520on%2520preceding%2520visual%2520and%2520textual%2520cues.%2520We%2520demonstrate%2520that%250Aour%2520approach%2520improves%2520performance%2520on%2520the%2520next-scene%2520description%2520task%2520--%250Aachieving%2520an%2520average%2520improvement%2520from%25203.7%2525%2520to%252019%2525%2520in%2520SimRate%252C%2520a%2520metric%2520that%250Aquantifies%2520semantic%2520similarity%2520to%2520human-annotated%2520ground%2520truths.%2520Moreover%252C%250AImageChain%2520achieves%2520robust%2520zero-shot%2520out-of-domain%2520performance%2520in%2520applications%250Aranging%2520from%2520comics%2520to%2520robotics.%2520Extensive%2520experiments%2520validate%2520that%250Ainstruction-tuning%2520in%2520a%2520multimodal%252C%2520multi-turn%2520conversation%2520design%2520is%2520key%2520to%250Abridging%2520the%2520gap%2520between%2520static%2520image%2520understanding%2520and%2520temporally-aware%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageChain%3A%20Advancing%20Sequential%20Image-to-Text%20Reasoning%20in%20Multimodal%0A%20%20Large%20Language%20Models&entry.906535625=Danae%20S%C3%A1nchez%20Villegas%20and%20Ingo%20Ziegler%20and%20Desmond%20Elliott&entry.1292438233=%20%20Reasoning%20over%20sequences%20of%20images%20remains%20a%20challenge%20for%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20While%20recent%20models%20incorporate%20multi-image%20data%0Aduring%20pre-training%2C%20they%20still%20struggle%20to%20recognize%20sequential%20structures%2C%0Aoften%20treating%20images%20independently.%20This%20work%20introduces%20ImageChain%2C%20a%0Aframework%20that%20enhances%20MLLMs%20with%20sequential%20reasoning%20capabilities%20over%20image%0Adata%20by%20modeling%20visual%20sequences%20as%20a%20multi-turn%20conversation.%20In%20ImageChain%2C%0Aimages%20are%20interleaved%20with%20corresponding%20textual%20descriptions%20to%20form%20a%0Acontrolled%20dialogue%20that%20explicitly%20captures%20temporal%20dependencies%20and%0Anarrative%20progression.%20Our%20method%20optimizes%20for%20the%20task%20of%20next-scene%0Adescription%2C%20where%20the%20model%20generates%20a%20context-aware%20description%20of%20an%0Aupcoming%20scene%20based%20on%20preceding%20visual%20and%20textual%20cues.%20We%20demonstrate%20that%0Aour%20approach%20improves%20performance%20on%20the%20next-scene%20description%20task%20--%0Aachieving%20an%20average%20improvement%20from%203.7%25%20to%2019%25%20in%20SimRate%2C%20a%20metric%20that%0Aquantifies%20semantic%20similarity%20to%20human-annotated%20ground%20truths.%20Moreover%2C%0AImageChain%20achieves%20robust%20zero-shot%20out-of-domain%20performance%20in%20applications%0Aranging%20from%20comics%20to%20robotics.%20Extensive%20experiments%20validate%20that%0Ainstruction-tuning%20in%20a%20multimodal%2C%20multi-turn%20conversation%20design%20is%20key%20to%0Abridging%20the%20gap%20between%20static%20image%20understanding%20and%20temporally-aware%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19409v1&entry.124074799=Read"},
{"title": "GHOST 2.0: generative high-fidelity one shot transfer of heads", "author": "Alexander Groshev and Anastasiia Iashchenko and Pavel Paramonov and Denis Dimitrov and Andrey Kuznetsov", "abstract": "  While the task of face swapping has recently gained attention in the research\ncommunity, a related problem of head swapping remains largely unexplored. In\naddition to skin color transfer, head swap poses extra challenges, such as the\nneed to preserve structural information of the whole head during synthesis and\ninpaint gaps between swapped head and background. In this paper, we address\nthese concerns with GHOST 2.0, which consists of two problem-specific modules.\nFirst, we introduce enhanced Aligner model for head reenactment, which\npreserves identity information at multiple scales and is robust to extreme pose\nvariations. Secondly, we use a Blender module that seamlessly integrates the\nreenacted head into the target background by transferring skin color and\ninpainting mismatched regions. Both modules outperform the baselines on the\ncorresponding tasks, allowing to achieve state of the art results in head\nswapping. We also tackle complex cases, such as large difference in hair styles\nof source and target. Code is available at\nhttps://github.com/ai-forever/ghost-2.0\n", "link": "http://arxiv.org/abs/2502.18417v2", "date": "2025-02-26", "relevancy": 2.9614, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5981}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5981}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GHOST%202.0%3A%20generative%20high-fidelity%20one%20shot%20transfer%20of%20heads&body=Title%3A%20GHOST%202.0%3A%20generative%20high-fidelity%20one%20shot%20transfer%20of%20heads%0AAuthor%3A%20Alexander%20Groshev%20and%20Anastasiia%20Iashchenko%20and%20Pavel%20Paramonov%20and%20Denis%20Dimitrov%20and%20Andrey%20Kuznetsov%0AAbstract%3A%20%20%20While%20the%20task%20of%20face%20swapping%20has%20recently%20gained%20attention%20in%20the%20research%0Acommunity%2C%20a%20related%20problem%20of%20head%20swapping%20remains%20largely%20unexplored.%20In%0Aaddition%20to%20skin%20color%20transfer%2C%20head%20swap%20poses%20extra%20challenges%2C%20such%20as%20the%0Aneed%20to%20preserve%20structural%20information%20of%20the%20whole%20head%20during%20synthesis%20and%0Ainpaint%20gaps%20between%20swapped%20head%20and%20background.%20In%20this%20paper%2C%20we%20address%0Athese%20concerns%20with%20GHOST%202.0%2C%20which%20consists%20of%20two%20problem-specific%20modules.%0AFirst%2C%20we%20introduce%20enhanced%20Aligner%20model%20for%20head%20reenactment%2C%20which%0Apreserves%20identity%20information%20at%20multiple%20scales%20and%20is%20robust%20to%20extreme%20pose%0Avariations.%20Secondly%2C%20we%20use%20a%20Blender%20module%20that%20seamlessly%20integrates%20the%0Areenacted%20head%20into%20the%20target%20background%20by%20transferring%20skin%20color%20and%0Ainpainting%20mismatched%20regions.%20Both%20modules%20outperform%20the%20baselines%20on%20the%0Acorresponding%20tasks%2C%20allowing%20to%20achieve%20state%20of%20the%20art%20results%20in%20head%0Aswapping.%20We%20also%20tackle%20complex%20cases%2C%20such%20as%20large%20difference%20in%20hair%20styles%0Aof%20source%20and%20target.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ai-forever/ghost-2.0%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGHOST%25202.0%253A%2520generative%2520high-fidelity%2520one%2520shot%2520transfer%2520of%2520heads%26entry.906535625%3DAlexander%2520Groshev%2520and%2520Anastasiia%2520Iashchenko%2520and%2520Pavel%2520Paramonov%2520and%2520Denis%2520Dimitrov%2520and%2520Andrey%2520Kuznetsov%26entry.1292438233%3D%2520%2520While%2520the%2520task%2520of%2520face%2520swapping%2520has%2520recently%2520gained%2520attention%2520in%2520the%2520research%250Acommunity%252C%2520a%2520related%2520problem%2520of%2520head%2520swapping%2520remains%2520largely%2520unexplored.%2520In%250Aaddition%2520to%2520skin%2520color%2520transfer%252C%2520head%2520swap%2520poses%2520extra%2520challenges%252C%2520such%2520as%2520the%250Aneed%2520to%2520preserve%2520structural%2520information%2520of%2520the%2520whole%2520head%2520during%2520synthesis%2520and%250Ainpaint%2520gaps%2520between%2520swapped%2520head%2520and%2520background.%2520In%2520this%2520paper%252C%2520we%2520address%250Athese%2520concerns%2520with%2520GHOST%25202.0%252C%2520which%2520consists%2520of%2520two%2520problem-specific%2520modules.%250AFirst%252C%2520we%2520introduce%2520enhanced%2520Aligner%2520model%2520for%2520head%2520reenactment%252C%2520which%250Apreserves%2520identity%2520information%2520at%2520multiple%2520scales%2520and%2520is%2520robust%2520to%2520extreme%2520pose%250Avariations.%2520Secondly%252C%2520we%2520use%2520a%2520Blender%2520module%2520that%2520seamlessly%2520integrates%2520the%250Areenacted%2520head%2520into%2520the%2520target%2520background%2520by%2520transferring%2520skin%2520color%2520and%250Ainpainting%2520mismatched%2520regions.%2520Both%2520modules%2520outperform%2520the%2520baselines%2520on%2520the%250Acorresponding%2520tasks%252C%2520allowing%2520to%2520achieve%2520state%2520of%2520the%2520art%2520results%2520in%2520head%250Aswapping.%2520We%2520also%2520tackle%2520complex%2520cases%252C%2520such%2520as%2520large%2520difference%2520in%2520hair%2520styles%250Aof%2520source%2520and%2520target.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ai-forever/ghost-2.0%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GHOST%202.0%3A%20generative%20high-fidelity%20one%20shot%20transfer%20of%20heads&entry.906535625=Alexander%20Groshev%20and%20Anastasiia%20Iashchenko%20and%20Pavel%20Paramonov%20and%20Denis%20Dimitrov%20and%20Andrey%20Kuznetsov&entry.1292438233=%20%20While%20the%20task%20of%20face%20swapping%20has%20recently%20gained%20attention%20in%20the%20research%0Acommunity%2C%20a%20related%20problem%20of%20head%20swapping%20remains%20largely%20unexplored.%20In%0Aaddition%20to%20skin%20color%20transfer%2C%20head%20swap%20poses%20extra%20challenges%2C%20such%20as%20the%0Aneed%20to%20preserve%20structural%20information%20of%20the%20whole%20head%20during%20synthesis%20and%0Ainpaint%20gaps%20between%20swapped%20head%20and%20background.%20In%20this%20paper%2C%20we%20address%0Athese%20concerns%20with%20GHOST%202.0%2C%20which%20consists%20of%20two%20problem-specific%20modules.%0AFirst%2C%20we%20introduce%20enhanced%20Aligner%20model%20for%20head%20reenactment%2C%20which%0Apreserves%20identity%20information%20at%20multiple%20scales%20and%20is%20robust%20to%20extreme%20pose%0Avariations.%20Secondly%2C%20we%20use%20a%20Blender%20module%20that%20seamlessly%20integrates%20the%0Areenacted%20head%20into%20the%20target%20background%20by%20transferring%20skin%20color%20and%0Ainpainting%20mismatched%20regions.%20Both%20modules%20outperform%20the%20baselines%20on%20the%0Acorresponding%20tasks%2C%20allowing%20to%20achieve%20state%20of%20the%20art%20results%20in%20head%0Aswapping.%20We%20also%20tackle%20complex%20cases%2C%20such%20as%20large%20difference%20in%20hair%20styles%0Aof%20source%20and%20target.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ai-forever/ghost-2.0%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18417v2&entry.124074799=Read"},
{"title": "Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation", "author": "Zhaochong An and Guolei Sun and Yun Liu and Runjia Li and Min Wu and Ming-Ming Cheng and Ender Konukoglu and Serge Belongie", "abstract": "  Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to\nsegment novel categories with minimal annotated support samples. While existing\nFS-PCS methods have shown promise, they primarily focus on unimodal point cloud\ninputs, overlooking the potential benefits of leveraging multimodal\ninformation. In this paper, we address this gap by introducing a multimodal\nFS-PCS setup, utilizing textual labels and the potentially available 2D image\nmodality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot\nSegNet (MM-FSS), a model effectively harnessing complementary information from\nmultiple modalities. MM-FSS employs a shared backbone with two heads to extract\nintermodal and unimodal visual features, and a pretrained text encoder to\ngenerate text embeddings. To fully exploit the multimodal information, we\npropose a Multimodal Correlation Fusion (MCF) module to generate multimodal\ncorrelations, and a Multimodal Semantic Fusion (MSF) module to refine the\ncorrelations using text-aware semantic guidance. Additionally, we propose a\nsimple yet effective Test-time Adaptive Cross-modal Calibration (TACC)\ntechnique to mitigate training bias, further improving generalization.\nExperimental results on S3DIS and ScanNet datasets demonstrate significant\nperformance improvements achieved by our method. The efficacy of our approach\nindicates the benefits of leveraging commonly-ignored free modalities for\nFS-PCS, providing valuable insights for future research. The code is available\nat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot\n", "link": "http://arxiv.org/abs/2410.22489v4", "date": "2025-02-26", "relevancy": 2.9405, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6178}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodality%20Helps%20Few-shot%203D%20Point%20Cloud%20Semantic%20Segmentation&body=Title%3A%20Multimodality%20Helps%20Few-shot%203D%20Point%20Cloud%20Semantic%20Segmentation%0AAuthor%3A%20Zhaochong%20An%20and%20Guolei%20Sun%20and%20Yun%20Liu%20and%20Runjia%20Li%20and%20Min%20Wu%20and%20Ming-Ming%20Cheng%20and%20Ender%20Konukoglu%20and%20Serge%20Belongie%0AAbstract%3A%20%20%20Few-shot%203D%20point%20cloud%20segmentation%20%28FS-PCS%29%20aims%20at%20generalizing%20models%20to%0Asegment%20novel%20categories%20with%20minimal%20annotated%20support%20samples.%20While%20existing%0AFS-PCS%20methods%20have%20shown%20promise%2C%20they%20primarily%20focus%20on%20unimodal%20point%20cloud%0Ainputs%2C%20overlooking%20the%20potential%20benefits%20of%20leveraging%20multimodal%0Ainformation.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20introducing%20a%20multimodal%0AFS-PCS%20setup%2C%20utilizing%20textual%20labels%20and%20the%20potentially%20available%202D%20image%0Amodality.%20Under%20this%20easy-to-achieve%20setup%2C%20we%20present%20the%20MultiModal%20Few-Shot%0ASegNet%20%28MM-FSS%29%2C%20a%20model%20effectively%20harnessing%20complementary%20information%20from%0Amultiple%20modalities.%20MM-FSS%20employs%20a%20shared%20backbone%20with%20two%20heads%20to%20extract%0Aintermodal%20and%20unimodal%20visual%20features%2C%20and%20a%20pretrained%20text%20encoder%20to%0Agenerate%20text%20embeddings.%20To%20fully%20exploit%20the%20multimodal%20information%2C%20we%0Apropose%20a%20Multimodal%20Correlation%20Fusion%20%28MCF%29%20module%20to%20generate%20multimodal%0Acorrelations%2C%20and%20a%20Multimodal%20Semantic%20Fusion%20%28MSF%29%20module%20to%20refine%20the%0Acorrelations%20using%20text-aware%20semantic%20guidance.%20Additionally%2C%20we%20propose%20a%0Asimple%20yet%20effective%20Test-time%20Adaptive%20Cross-modal%20Calibration%20%28TACC%29%0Atechnique%20to%20mitigate%20training%20bias%2C%20further%20improving%20generalization.%0AExperimental%20results%20on%20S3DIS%20and%20ScanNet%20datasets%20demonstrate%20significant%0Aperformance%20improvements%20achieved%20by%20our%20method.%20The%20efficacy%20of%20our%20approach%0Aindicates%20the%20benefits%20of%20leveraging%20commonly-ignored%20free%20modalities%20for%0AFS-PCS%2C%20providing%20valuable%20insights%20for%20future%20research.%20The%20code%20is%20available%0Aat%20https%3A//github.com/ZhaochongAn/Multimodality-3D-Few-Shot%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22489v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodality%2520Helps%2520Few-shot%25203D%2520Point%2520Cloud%2520Semantic%2520Segmentation%26entry.906535625%3DZhaochong%2520An%2520and%2520Guolei%2520Sun%2520and%2520Yun%2520Liu%2520and%2520Runjia%2520Li%2520and%2520Min%2520Wu%2520and%2520Ming-Ming%2520Cheng%2520and%2520Ender%2520Konukoglu%2520and%2520Serge%2520Belongie%26entry.1292438233%3D%2520%2520Few-shot%25203D%2520point%2520cloud%2520segmentation%2520%2528FS-PCS%2529%2520aims%2520at%2520generalizing%2520models%2520to%250Asegment%2520novel%2520categories%2520with%2520minimal%2520annotated%2520support%2520samples.%2520While%2520existing%250AFS-PCS%2520methods%2520have%2520shown%2520promise%252C%2520they%2520primarily%2520focus%2520on%2520unimodal%2520point%2520cloud%250Ainputs%252C%2520overlooking%2520the%2520potential%2520benefits%2520of%2520leveraging%2520multimodal%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520a%2520multimodal%250AFS-PCS%2520setup%252C%2520utilizing%2520textual%2520labels%2520and%2520the%2520potentially%2520available%25202D%2520image%250Amodality.%2520Under%2520this%2520easy-to-achieve%2520setup%252C%2520we%2520present%2520the%2520MultiModal%2520Few-Shot%250ASegNet%2520%2528MM-FSS%2529%252C%2520a%2520model%2520effectively%2520harnessing%2520complementary%2520information%2520from%250Amultiple%2520modalities.%2520MM-FSS%2520employs%2520a%2520shared%2520backbone%2520with%2520two%2520heads%2520to%2520extract%250Aintermodal%2520and%2520unimodal%2520visual%2520features%252C%2520and%2520a%2520pretrained%2520text%2520encoder%2520to%250Agenerate%2520text%2520embeddings.%2520To%2520fully%2520exploit%2520the%2520multimodal%2520information%252C%2520we%250Apropose%2520a%2520Multimodal%2520Correlation%2520Fusion%2520%2528MCF%2529%2520module%2520to%2520generate%2520multimodal%250Acorrelations%252C%2520and%2520a%2520Multimodal%2520Semantic%2520Fusion%2520%2528MSF%2529%2520module%2520to%2520refine%2520the%250Acorrelations%2520using%2520text-aware%2520semantic%2520guidance.%2520Additionally%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520Test-time%2520Adaptive%2520Cross-modal%2520Calibration%2520%2528TACC%2529%250Atechnique%2520to%2520mitigate%2520training%2520bias%252C%2520further%2520improving%2520generalization.%250AExperimental%2520results%2520on%2520S3DIS%2520and%2520ScanNet%2520datasets%2520demonstrate%2520significant%250Aperformance%2520improvements%2520achieved%2520by%2520our%2520method.%2520The%2520efficacy%2520of%2520our%2520approach%250Aindicates%2520the%2520benefits%2520of%2520leveraging%2520commonly-ignored%2520free%2520modalities%2520for%250AFS-PCS%252C%2520providing%2520valuable%2520insights%2520for%2520future%2520research.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/ZhaochongAn/Multimodality-3D-Few-Shot%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22489v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodality%20Helps%20Few-shot%203D%20Point%20Cloud%20Semantic%20Segmentation&entry.906535625=Zhaochong%20An%20and%20Guolei%20Sun%20and%20Yun%20Liu%20and%20Runjia%20Li%20and%20Min%20Wu%20and%20Ming-Ming%20Cheng%20and%20Ender%20Konukoglu%20and%20Serge%20Belongie&entry.1292438233=%20%20Few-shot%203D%20point%20cloud%20segmentation%20%28FS-PCS%29%20aims%20at%20generalizing%20models%20to%0Asegment%20novel%20categories%20with%20minimal%20annotated%20support%20samples.%20While%20existing%0AFS-PCS%20methods%20have%20shown%20promise%2C%20they%20primarily%20focus%20on%20unimodal%20point%20cloud%0Ainputs%2C%20overlooking%20the%20potential%20benefits%20of%20leveraging%20multimodal%0Ainformation.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20introducing%20a%20multimodal%0AFS-PCS%20setup%2C%20utilizing%20textual%20labels%20and%20the%20potentially%20available%202D%20image%0Amodality.%20Under%20this%20easy-to-achieve%20setup%2C%20we%20present%20the%20MultiModal%20Few-Shot%0ASegNet%20%28MM-FSS%29%2C%20a%20model%20effectively%20harnessing%20complementary%20information%20from%0Amultiple%20modalities.%20MM-FSS%20employs%20a%20shared%20backbone%20with%20two%20heads%20to%20extract%0Aintermodal%20and%20unimodal%20visual%20features%2C%20and%20a%20pretrained%20text%20encoder%20to%0Agenerate%20text%20embeddings.%20To%20fully%20exploit%20the%20multimodal%20information%2C%20we%0Apropose%20a%20Multimodal%20Correlation%20Fusion%20%28MCF%29%20module%20to%20generate%20multimodal%0Acorrelations%2C%20and%20a%20Multimodal%20Semantic%20Fusion%20%28MSF%29%20module%20to%20refine%20the%0Acorrelations%20using%20text-aware%20semantic%20guidance.%20Additionally%2C%20we%20propose%20a%0Asimple%20yet%20effective%20Test-time%20Adaptive%20Cross-modal%20Calibration%20%28TACC%29%0Atechnique%20to%20mitigate%20training%20bias%2C%20further%20improving%20generalization.%0AExperimental%20results%20on%20S3DIS%20and%20ScanNet%20datasets%20demonstrate%20significant%0Aperformance%20improvements%20achieved%20by%20our%20method.%20The%20efficacy%20of%20our%20approach%0Aindicates%20the%20benefits%20of%20leveraging%20commonly-ignored%20free%20modalities%20for%0AFS-PCS%2C%20providing%20valuable%20insights%20for%20future%20research.%20The%20code%20is%20available%0Aat%20https%3A//github.com/ZhaochongAn/Multimodality-3D-Few-Shot%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22489v4&entry.124074799=Read"},
{"title": "InternVQA: Advancing Compressed Video Quality Assessment with Distilling\n  Large Foundation Model", "author": "Fengbin Guan and Zihao Yu and Yiting Lu and Xin Li and Zhibo Chen", "abstract": "  Video quality assessment tasks rely heavily on the rich features required for\nvideo understanding, such as semantic information, texture, and temporal\nmotion. The existing video foundational model, InternVideo2, has demonstrated\nstrong potential in video understanding tasks due to its large parameter size\nand large-scale multimodal data pertaining. Building on this, we explored the\ntransferability of InternVideo2 to video quality assessment under compression\nscenarios. To design a lightweight model suitable for this task, we proposed a\ndistillation method to equip the smaller model with rich compression quality\npriors. Additionally, we examined the performance of different backbones during\nthe distillation process. The results showed that, compared to other methods,\nour lightweight model distilled from InternVideo2 achieved excellent\nperformance in compression video quality assessment.\n", "link": "http://arxiv.org/abs/2502.19026v1", "date": "2025-02-26", "relevancy": 2.8978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternVQA%3A%20Advancing%20Compressed%20Video%20Quality%20Assessment%20with%20Distilling%0A%20%20Large%20Foundation%20Model&body=Title%3A%20InternVQA%3A%20Advancing%20Compressed%20Video%20Quality%20Assessment%20with%20Distilling%0A%20%20Large%20Foundation%20Model%0AAuthor%3A%20Fengbin%20Guan%20and%20Zihao%20Yu%20and%20Yiting%20Lu%20and%20Xin%20Li%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20Video%20quality%20assessment%20tasks%20rely%20heavily%20on%20the%20rich%20features%20required%20for%0Avideo%20understanding%2C%20such%20as%20semantic%20information%2C%20texture%2C%20and%20temporal%0Amotion.%20The%20existing%20video%20foundational%20model%2C%20InternVideo2%2C%20has%20demonstrated%0Astrong%20potential%20in%20video%20understanding%20tasks%20due%20to%20its%20large%20parameter%20size%0Aand%20large-scale%20multimodal%20data%20pertaining.%20Building%20on%20this%2C%20we%20explored%20the%0Atransferability%20of%20InternVideo2%20to%20video%20quality%20assessment%20under%20compression%0Ascenarios.%20To%20design%20a%20lightweight%20model%20suitable%20for%20this%20task%2C%20we%20proposed%20a%0Adistillation%20method%20to%20equip%20the%20smaller%20model%20with%20rich%20compression%20quality%0Apriors.%20Additionally%2C%20we%20examined%20the%20performance%20of%20different%20backbones%20during%0Athe%20distillation%20process.%20The%20results%20showed%20that%2C%20compared%20to%20other%20methods%2C%0Aour%20lightweight%20model%20distilled%20from%20InternVideo2%20achieved%20excellent%0Aperformance%20in%20compression%20video%20quality%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternVQA%253A%2520Advancing%2520Compressed%2520Video%2520Quality%2520Assessment%2520with%2520Distilling%250A%2520%2520Large%2520Foundation%2520Model%26entry.906535625%3DFengbin%2520Guan%2520and%2520Zihao%2520Yu%2520and%2520Yiting%2520Lu%2520and%2520Xin%2520Li%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520Video%2520quality%2520assessment%2520tasks%2520rely%2520heavily%2520on%2520the%2520rich%2520features%2520required%2520for%250Avideo%2520understanding%252C%2520such%2520as%2520semantic%2520information%252C%2520texture%252C%2520and%2520temporal%250Amotion.%2520The%2520existing%2520video%2520foundational%2520model%252C%2520InternVideo2%252C%2520has%2520demonstrated%250Astrong%2520potential%2520in%2520video%2520understanding%2520tasks%2520due%2520to%2520its%2520large%2520parameter%2520size%250Aand%2520large-scale%2520multimodal%2520data%2520pertaining.%2520Building%2520on%2520this%252C%2520we%2520explored%2520the%250Atransferability%2520of%2520InternVideo2%2520to%2520video%2520quality%2520assessment%2520under%2520compression%250Ascenarios.%2520To%2520design%2520a%2520lightweight%2520model%2520suitable%2520for%2520this%2520task%252C%2520we%2520proposed%2520a%250Adistillation%2520method%2520to%2520equip%2520the%2520smaller%2520model%2520with%2520rich%2520compression%2520quality%250Apriors.%2520Additionally%252C%2520we%2520examined%2520the%2520performance%2520of%2520different%2520backbones%2520during%250Athe%2520distillation%2520process.%2520The%2520results%2520showed%2520that%252C%2520compared%2520to%2520other%2520methods%252C%250Aour%2520lightweight%2520model%2520distilled%2520from%2520InternVideo2%2520achieved%2520excellent%250Aperformance%2520in%2520compression%2520video%2520quality%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternVQA%3A%20Advancing%20Compressed%20Video%20Quality%20Assessment%20with%20Distilling%0A%20%20Large%20Foundation%20Model&entry.906535625=Fengbin%20Guan%20and%20Zihao%20Yu%20and%20Yiting%20Lu%20and%20Xin%20Li%20and%20Zhibo%20Chen&entry.1292438233=%20%20Video%20quality%20assessment%20tasks%20rely%20heavily%20on%20the%20rich%20features%20required%20for%0Avideo%20understanding%2C%20such%20as%20semantic%20information%2C%20texture%2C%20and%20temporal%0Amotion.%20The%20existing%20video%20foundational%20model%2C%20InternVideo2%2C%20has%20demonstrated%0Astrong%20potential%20in%20video%20understanding%20tasks%20due%20to%20its%20large%20parameter%20size%0Aand%20large-scale%20multimodal%20data%20pertaining.%20Building%20on%20this%2C%20we%20explored%20the%0Atransferability%20of%20InternVideo2%20to%20video%20quality%20assessment%20under%20compression%0Ascenarios.%20To%20design%20a%20lightweight%20model%20suitable%20for%20this%20task%2C%20we%20proposed%20a%0Adistillation%20method%20to%20equip%20the%20smaller%20model%20with%20rich%20compression%20quality%0Apriors.%20Additionally%2C%20we%20examined%20the%20performance%20of%20different%20backbones%20during%0Athe%20distillation%20process.%20The%20results%20showed%20that%2C%20compared%20to%20other%20methods%2C%0Aour%20lightweight%20model%20distilled%20from%20InternVideo2%20achieved%20excellent%0Aperformance%20in%20compression%20video%20quality%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19026v1&entry.124074799=Read"},
{"title": "EndoMamba: An Efficient Foundation Model for Endoscopic Videos", "author": "Qingyao Tian and Huai Liao and Xinyan Huang and Bingyu Yang and Dongdong Lei and Sebastien Ourselin and Hongbin Liu", "abstract": "  Endoscopic video-based tasks, such as visual navigation and surgical phase\nrecognition, play a crucial role in minimally invasive surgeries by providing\nreal-time assistance. While recent video foundation models have shown promise,\ntheir applications are hindered by (1) computational inefficiencies and (2)\nsuboptimal performance caused by limited data for pre-training in endoscopy. To\naddress these issues, we present EndoMamba, a foundation model designed for\nreal-time inference while learning generalized spatiotemporal representations.\nFirst, to mitigate computational inefficiencies, we propose the EndoMamba\nbackbone, optimized for real-time inference. Inspired by recent advancements in\nstate space models, EndoMamba integrates Bidirectional Mamba blocks for spatial\nmodeling within individual frames and vanilla Mamba blocks for past-to-present\nreasoning across the temporal domain. This design enables both strong\nspatiotemporal modeling and efficient inference in online video streams.\nSecond, we propose a self-supervised hierarchical pre-training diagram to\nenhance EndoMamba's representation learning using endoscopic videos and\nincorporating general video domain knowledge. Specifically, our approach\ncombines masked reconstruction with auxiliary supervision, leveraging low-level\nreconstruction to capture spatial-temporal structures and high-level alignment\nto transfer broader knowledge from a pretrained general-video domain foundation\nmodel. Extensive experiments on four downstream tasks--classification,\nsegmentation, surgical phase recognition, and localization--demonstrate that\nEndoMamba outperforms existing foundation models and task-specific methods\nwhile maintaining real-time inference speed. The source code will be released\nupon acceptance.\n", "link": "http://arxiv.org/abs/2502.19090v1", "date": "2025-02-26", "relevancy": 2.8825, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5803}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EndoMamba%3A%20An%20Efficient%20Foundation%20Model%20for%20Endoscopic%20Videos&body=Title%3A%20EndoMamba%3A%20An%20Efficient%20Foundation%20Model%20for%20Endoscopic%20Videos%0AAuthor%3A%20Qingyao%20Tian%20and%20Huai%20Liao%20and%20Xinyan%20Huang%20and%20Bingyu%20Yang%20and%20Dongdong%20Lei%20and%20Sebastien%20Ourselin%20and%20Hongbin%20Liu%0AAbstract%3A%20%20%20Endoscopic%20video-based%20tasks%2C%20such%20as%20visual%20navigation%20and%20surgical%20phase%0Arecognition%2C%20play%20a%20crucial%20role%20in%20minimally%20invasive%20surgeries%20by%20providing%0Areal-time%20assistance.%20While%20recent%20video%20foundation%20models%20have%20shown%20promise%2C%0Atheir%20applications%20are%20hindered%20by%20%281%29%20computational%20inefficiencies%20and%20%282%29%0Asuboptimal%20performance%20caused%20by%20limited%20data%20for%20pre-training%20in%20endoscopy.%20To%0Aaddress%20these%20issues%2C%20we%20present%20EndoMamba%2C%20a%20foundation%20model%20designed%20for%0Areal-time%20inference%20while%20learning%20generalized%20spatiotemporal%20representations.%0AFirst%2C%20to%20mitigate%20computational%20inefficiencies%2C%20we%20propose%20the%20EndoMamba%0Abackbone%2C%20optimized%20for%20real-time%20inference.%20Inspired%20by%20recent%20advancements%20in%0Astate%20space%20models%2C%20EndoMamba%20integrates%20Bidirectional%20Mamba%20blocks%20for%20spatial%0Amodeling%20within%20individual%20frames%20and%20vanilla%20Mamba%20blocks%20for%20past-to-present%0Areasoning%20across%20the%20temporal%20domain.%20This%20design%20enables%20both%20strong%0Aspatiotemporal%20modeling%20and%20efficient%20inference%20in%20online%20video%20streams.%0ASecond%2C%20we%20propose%20a%20self-supervised%20hierarchical%20pre-training%20diagram%20to%0Aenhance%20EndoMamba%27s%20representation%20learning%20using%20endoscopic%20videos%20and%0Aincorporating%20general%20video%20domain%20knowledge.%20Specifically%2C%20our%20approach%0Acombines%20masked%20reconstruction%20with%20auxiliary%20supervision%2C%20leveraging%20low-level%0Areconstruction%20to%20capture%20spatial-temporal%20structures%20and%20high-level%20alignment%0Ato%20transfer%20broader%20knowledge%20from%20a%20pretrained%20general-video%20domain%20foundation%0Amodel.%20Extensive%20experiments%20on%20four%20downstream%20tasks--classification%2C%0Asegmentation%2C%20surgical%20phase%20recognition%2C%20and%20localization--demonstrate%20that%0AEndoMamba%20outperforms%20existing%20foundation%20models%20and%20task-specific%20methods%0Awhile%20maintaining%20real-time%20inference%20speed.%20The%20source%20code%20will%20be%20released%0Aupon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndoMamba%253A%2520An%2520Efficient%2520Foundation%2520Model%2520for%2520Endoscopic%2520Videos%26entry.906535625%3DQingyao%2520Tian%2520and%2520Huai%2520Liao%2520and%2520Xinyan%2520Huang%2520and%2520Bingyu%2520Yang%2520and%2520Dongdong%2520Lei%2520and%2520Sebastien%2520Ourselin%2520and%2520Hongbin%2520Liu%26entry.1292438233%3D%2520%2520Endoscopic%2520video-based%2520tasks%252C%2520such%2520as%2520visual%2520navigation%2520and%2520surgical%2520phase%250Arecognition%252C%2520play%2520a%2520crucial%2520role%2520in%2520minimally%2520invasive%2520surgeries%2520by%2520providing%250Areal-time%2520assistance.%2520While%2520recent%2520video%2520foundation%2520models%2520have%2520shown%2520promise%252C%250Atheir%2520applications%2520are%2520hindered%2520by%2520%25281%2529%2520computational%2520inefficiencies%2520and%2520%25282%2529%250Asuboptimal%2520performance%2520caused%2520by%2520limited%2520data%2520for%2520pre-training%2520in%2520endoscopy.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520present%2520EndoMamba%252C%2520a%2520foundation%2520model%2520designed%2520for%250Areal-time%2520inference%2520while%2520learning%2520generalized%2520spatiotemporal%2520representations.%250AFirst%252C%2520to%2520mitigate%2520computational%2520inefficiencies%252C%2520we%2520propose%2520the%2520EndoMamba%250Abackbone%252C%2520optimized%2520for%2520real-time%2520inference.%2520Inspired%2520by%2520recent%2520advancements%2520in%250Astate%2520space%2520models%252C%2520EndoMamba%2520integrates%2520Bidirectional%2520Mamba%2520blocks%2520for%2520spatial%250Amodeling%2520within%2520individual%2520frames%2520and%2520vanilla%2520Mamba%2520blocks%2520for%2520past-to-present%250Areasoning%2520across%2520the%2520temporal%2520domain.%2520This%2520design%2520enables%2520both%2520strong%250Aspatiotemporal%2520modeling%2520and%2520efficient%2520inference%2520in%2520online%2520video%2520streams.%250ASecond%252C%2520we%2520propose%2520a%2520self-supervised%2520hierarchical%2520pre-training%2520diagram%2520to%250Aenhance%2520EndoMamba%2527s%2520representation%2520learning%2520using%2520endoscopic%2520videos%2520and%250Aincorporating%2520general%2520video%2520domain%2520knowledge.%2520Specifically%252C%2520our%2520approach%250Acombines%2520masked%2520reconstruction%2520with%2520auxiliary%2520supervision%252C%2520leveraging%2520low-level%250Areconstruction%2520to%2520capture%2520spatial-temporal%2520structures%2520and%2520high-level%2520alignment%250Ato%2520transfer%2520broader%2520knowledge%2520from%2520a%2520pretrained%2520general-video%2520domain%2520foundation%250Amodel.%2520Extensive%2520experiments%2520on%2520four%2520downstream%2520tasks--classification%252C%250Asegmentation%252C%2520surgical%2520phase%2520recognition%252C%2520and%2520localization--demonstrate%2520that%250AEndoMamba%2520outperforms%2520existing%2520foundation%2520models%2520and%2520task-specific%2520methods%250Awhile%2520maintaining%2520real-time%2520inference%2520speed.%2520The%2520source%2520code%2520will%2520be%2520released%250Aupon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EndoMamba%3A%20An%20Efficient%20Foundation%20Model%20for%20Endoscopic%20Videos&entry.906535625=Qingyao%20Tian%20and%20Huai%20Liao%20and%20Xinyan%20Huang%20and%20Bingyu%20Yang%20and%20Dongdong%20Lei%20and%20Sebastien%20Ourselin%20and%20Hongbin%20Liu&entry.1292438233=%20%20Endoscopic%20video-based%20tasks%2C%20such%20as%20visual%20navigation%20and%20surgical%20phase%0Arecognition%2C%20play%20a%20crucial%20role%20in%20minimally%20invasive%20surgeries%20by%20providing%0Areal-time%20assistance.%20While%20recent%20video%20foundation%20models%20have%20shown%20promise%2C%0Atheir%20applications%20are%20hindered%20by%20%281%29%20computational%20inefficiencies%20and%20%282%29%0Asuboptimal%20performance%20caused%20by%20limited%20data%20for%20pre-training%20in%20endoscopy.%20To%0Aaddress%20these%20issues%2C%20we%20present%20EndoMamba%2C%20a%20foundation%20model%20designed%20for%0Areal-time%20inference%20while%20learning%20generalized%20spatiotemporal%20representations.%0AFirst%2C%20to%20mitigate%20computational%20inefficiencies%2C%20we%20propose%20the%20EndoMamba%0Abackbone%2C%20optimized%20for%20real-time%20inference.%20Inspired%20by%20recent%20advancements%20in%0Astate%20space%20models%2C%20EndoMamba%20integrates%20Bidirectional%20Mamba%20blocks%20for%20spatial%0Amodeling%20within%20individual%20frames%20and%20vanilla%20Mamba%20blocks%20for%20past-to-present%0Areasoning%20across%20the%20temporal%20domain.%20This%20design%20enables%20both%20strong%0Aspatiotemporal%20modeling%20and%20efficient%20inference%20in%20online%20video%20streams.%0ASecond%2C%20we%20propose%20a%20self-supervised%20hierarchical%20pre-training%20diagram%20to%0Aenhance%20EndoMamba%27s%20representation%20learning%20using%20endoscopic%20videos%20and%0Aincorporating%20general%20video%20domain%20knowledge.%20Specifically%2C%20our%20approach%0Acombines%20masked%20reconstruction%20with%20auxiliary%20supervision%2C%20leveraging%20low-level%0Areconstruction%20to%20capture%20spatial-temporal%20structures%20and%20high-level%20alignment%0Ato%20transfer%20broader%20knowledge%20from%20a%20pretrained%20general-video%20domain%20foundation%0Amodel.%20Extensive%20experiments%20on%20four%20downstream%20tasks--classification%2C%0Asegmentation%2C%20surgical%20phase%20recognition%2C%20and%20localization--demonstrate%20that%0AEndoMamba%20outperforms%20existing%20foundation%20models%20and%20task-specific%20methods%0Awhile%20maintaining%20real-time%20inference%20speed.%20The%20source%20code%20will%20be%20released%0Aupon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19090v1&entry.124074799=Read"},
{"title": "Vision Foundation Models for Computed Tomography", "author": "Suraj Pai and Ibrahim Hadzic and Dennis Bontempi and Keno Bressem and Benjamin H. Kann and Andriy Fedorov and Raymond H. Mak and Hugo J. W. L. Aerts", "abstract": "  Foundation models (FMs) have shown transformative potential in radiology by\nperforming diverse, complex tasks across imaging modalities. Here, we developed\nCT-FM, a large-scale 3D image-based pre-trained model designed explicitly for\nvarious radiological tasks. CT-FM was pre-trained using 148,000 computed\ntomography (CT) scans from the Imaging Data Commons through label-agnostic\ncontrastive learning. We evaluated CT-FM across four categories of tasks,\nnamely, whole-body and tumor segmentation, head CT triage, medical image\nretrieval, and semantic understanding, showing superior performance against\nstate-of-the-art models. Beyond quantitative success, CT-FM demonstrated the\nability to cluster regions anatomically and identify similar anatomical and\nstructural concepts across scans. Furthermore, it remained robust across\ntest-retest settings and indicated reasonable salient regions attached to its\nembeddings. This study demonstrates the value of large-scale medical imaging\nfoundation models and by open-sourcing the model weights, code, and data, aims\nto support more adaptable, reliable, and interpretable AI solutions in\nradiology.\n", "link": "http://arxiv.org/abs/2501.09001v2", "date": "2025-02-26", "relevancy": 2.8796, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Foundation%20Models%20for%20Computed%20Tomography&body=Title%3A%20Vision%20Foundation%20Models%20for%20Computed%20Tomography%0AAuthor%3A%20Suraj%20Pai%20and%20Ibrahim%20Hadzic%20and%20Dennis%20Bontempi%20and%20Keno%20Bressem%20and%20Benjamin%20H.%20Kann%20and%20Andriy%20Fedorov%20and%20Raymond%20H.%20Mak%20and%20Hugo%20J.%20W.%20L.%20Aerts%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20have%20shown%20transformative%20potential%20in%20radiology%20by%0Aperforming%20diverse%2C%20complex%20tasks%20across%20imaging%20modalities.%20Here%2C%20we%20developed%0ACT-FM%2C%20a%20large-scale%203D%20image-based%20pre-trained%20model%20designed%20explicitly%20for%0Avarious%20radiological%20tasks.%20CT-FM%20was%20pre-trained%20using%20148%2C000%20computed%0Atomography%20%28CT%29%20scans%20from%20the%20Imaging%20Data%20Commons%20through%20label-agnostic%0Acontrastive%20learning.%20We%20evaluated%20CT-FM%20across%20four%20categories%20of%20tasks%2C%0Anamely%2C%20whole-body%20and%20tumor%20segmentation%2C%20head%20CT%20triage%2C%20medical%20image%0Aretrieval%2C%20and%20semantic%20understanding%2C%20showing%20superior%20performance%20against%0Astate-of-the-art%20models.%20Beyond%20quantitative%20success%2C%20CT-FM%20demonstrated%20the%0Aability%20to%20cluster%20regions%20anatomically%20and%20identify%20similar%20anatomical%20and%0Astructural%20concepts%20across%20scans.%20Furthermore%2C%20it%20remained%20robust%20across%0Atest-retest%20settings%20and%20indicated%20reasonable%20salient%20regions%20attached%20to%20its%0Aembeddings.%20This%20study%20demonstrates%20the%20value%20of%20large-scale%20medical%20imaging%0Afoundation%20models%20and%20by%20open-sourcing%20the%20model%20weights%2C%20code%2C%20and%20data%2C%20aims%0Ato%20support%20more%20adaptable%2C%20reliable%2C%20and%20interpretable%20AI%20solutions%20in%0Aradiology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Foundation%2520Models%2520for%2520Computed%2520Tomography%26entry.906535625%3DSuraj%2520Pai%2520and%2520Ibrahim%2520Hadzic%2520and%2520Dennis%2520Bontempi%2520and%2520Keno%2520Bressem%2520and%2520Benjamin%2520H.%2520Kann%2520and%2520Andriy%2520Fedorov%2520and%2520Raymond%2520H.%2520Mak%2520and%2520Hugo%2520J.%2520W.%2520L.%2520Aerts%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520have%2520shown%2520transformative%2520potential%2520in%2520radiology%2520by%250Aperforming%2520diverse%252C%2520complex%2520tasks%2520across%2520imaging%2520modalities.%2520Here%252C%2520we%2520developed%250ACT-FM%252C%2520a%2520large-scale%25203D%2520image-based%2520pre-trained%2520model%2520designed%2520explicitly%2520for%250Avarious%2520radiological%2520tasks.%2520CT-FM%2520was%2520pre-trained%2520using%2520148%252C000%2520computed%250Atomography%2520%2528CT%2529%2520scans%2520from%2520the%2520Imaging%2520Data%2520Commons%2520through%2520label-agnostic%250Acontrastive%2520learning.%2520We%2520evaluated%2520CT-FM%2520across%2520four%2520categories%2520of%2520tasks%252C%250Anamely%252C%2520whole-body%2520and%2520tumor%2520segmentation%252C%2520head%2520CT%2520triage%252C%2520medical%2520image%250Aretrieval%252C%2520and%2520semantic%2520understanding%252C%2520showing%2520superior%2520performance%2520against%250Astate-of-the-art%2520models.%2520Beyond%2520quantitative%2520success%252C%2520CT-FM%2520demonstrated%2520the%250Aability%2520to%2520cluster%2520regions%2520anatomically%2520and%2520identify%2520similar%2520anatomical%2520and%250Astructural%2520concepts%2520across%2520scans.%2520Furthermore%252C%2520it%2520remained%2520robust%2520across%250Atest-retest%2520settings%2520and%2520indicated%2520reasonable%2520salient%2520regions%2520attached%2520to%2520its%250Aembeddings.%2520This%2520study%2520demonstrates%2520the%2520value%2520of%2520large-scale%2520medical%2520imaging%250Afoundation%2520models%2520and%2520by%2520open-sourcing%2520the%2520model%2520weights%252C%2520code%252C%2520and%2520data%252C%2520aims%250Ato%2520support%2520more%2520adaptable%252C%2520reliable%252C%2520and%2520interpretable%2520AI%2520solutions%2520in%250Aradiology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Foundation%20Models%20for%20Computed%20Tomography&entry.906535625=Suraj%20Pai%20and%20Ibrahim%20Hadzic%20and%20Dennis%20Bontempi%20and%20Keno%20Bressem%20and%20Benjamin%20H.%20Kann%20and%20Andriy%20Fedorov%20and%20Raymond%20H.%20Mak%20and%20Hugo%20J.%20W.%20L.%20Aerts&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20have%20shown%20transformative%20potential%20in%20radiology%20by%0Aperforming%20diverse%2C%20complex%20tasks%20across%20imaging%20modalities.%20Here%2C%20we%20developed%0ACT-FM%2C%20a%20large-scale%203D%20image-based%20pre-trained%20model%20designed%20explicitly%20for%0Avarious%20radiological%20tasks.%20CT-FM%20was%20pre-trained%20using%20148%2C000%20computed%0Atomography%20%28CT%29%20scans%20from%20the%20Imaging%20Data%20Commons%20through%20label-agnostic%0Acontrastive%20learning.%20We%20evaluated%20CT-FM%20across%20four%20categories%20of%20tasks%2C%0Anamely%2C%20whole-body%20and%20tumor%20segmentation%2C%20head%20CT%20triage%2C%20medical%20image%0Aretrieval%2C%20and%20semantic%20understanding%2C%20showing%20superior%20performance%20against%0Astate-of-the-art%20models.%20Beyond%20quantitative%20success%2C%20CT-FM%20demonstrated%20the%0Aability%20to%20cluster%20regions%20anatomically%20and%20identify%20similar%20anatomical%20and%0Astructural%20concepts%20across%20scans.%20Furthermore%2C%20it%20remained%20robust%20across%0Atest-retest%20settings%20and%20indicated%20reasonable%20salient%20regions%20attached%20to%20its%0Aembeddings.%20This%20study%20demonstrates%20the%20value%20of%20large-scale%20medical%20imaging%0Afoundation%20models%20and%20by%20open-sourcing%20the%20model%20weights%2C%20code%2C%20and%20data%2C%20aims%0Ato%20support%20more%20adaptable%2C%20reliable%2C%20and%20interpretable%20AI%20solutions%20in%0Aradiology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09001v2&entry.124074799=Read"},
{"title": "ChineseSimpleVQA -- \"See the World, Discover Knowledge\": A Chinese\n  Factuality Evaluation for Large Vision Language Models", "author": "Jihao Gu and Yingyao Wang and Pi Bu and Chen Wang and Ziming Wang and Tengtao Song and Donglai Wei and Jiale Yuan and Yingxiu Zhao and Yancheng He and Shilong Li and Jiaheng Liu and Meng Cao and Jun Song and Yingshui Tan and Xiang Li and Wenbo Su and Zhicheng Zheng and Xiaoyong Zhu and Bo Zheng", "abstract": "  The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.\n", "link": "http://arxiv.org/abs/2502.11718v3", "date": "2025-02-26", "relevancy": 2.8077, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChineseSimpleVQA%20--%20%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%0A%20%20Factuality%20Evaluation%20for%20Large%20Vision%20Language%20Models&body=Title%3A%20ChineseSimpleVQA%20--%20%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%0A%20%20Factuality%20Evaluation%20for%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Jihao%20Gu%20and%20Yingyao%20Wang%20and%20Pi%20Bu%20and%20Chen%20Wang%20and%20Ziming%20Wang%20and%20Tengtao%20Song%20and%20Donglai%20Wei%20and%20Jiale%20Yuan%20and%20Yingxiu%20Zhao%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Jiaheng%20Liu%20and%20Meng%20Cao%20and%20Jun%20Song%20and%20Yingshui%20Tan%20and%20Xiang%20Li%20and%20Wenbo%20Su%20and%20Zhicheng%20Zheng%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20The%20evaluation%20of%20factual%20accuracy%20in%20large%20vision%20language%20models%20%28LVLMs%29%0Ahas%20lagged%20behind%20their%20rapid%20development%2C%20making%20it%20challenging%20to%20fully%0Areflect%20these%20models%27%20knowledge%20capacity%20and%20reliability.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20first%20factuality-based%20visual%20question-answering%20benchmark%20in%0AChinese%2C%20named%20ChineseSimpleVQA%2C%20aimed%20at%20assessing%20the%20visual%20factuality%20of%0ALVLMs%20across%208%20major%20topics%20and%2056%20subtopics.%20The%20key%20features%20of%20this%0Abenchmark%20include%20a%20focus%20on%20the%20Chinese%20language%2C%20diverse%20knowledge%20types%2C%20a%0Amulti-hop%20question%20construction%2C%20high-quality%20data%2C%20static%20consistency%2C%20and%0Aeasy-to-evaluate%20through%20short%20answers.%20Moreover%2C%20we%20contribute%20a%20rigorous%20data%0Aconstruction%20pipeline%20and%20decouple%20the%20visual%20factuality%20into%20two%20parts%3A%20seeing%0Athe%20world%20%28i.e.%2C%20object%20recognition%29%20and%20discovering%20knowledge.%20This%20decoupling%0Aallows%20us%20to%20analyze%20the%20capability%20boundaries%20and%20execution%20mechanisms%20of%0ALVLMs.%20Subsequently%2C%20we%20evaluate%2034%20advanced%20open-source%20and%20closed-source%0Amodels%2C%20revealing%20critical%20performance%20gaps%20within%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11718v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChineseSimpleVQA%2520--%2520%2522See%2520the%2520World%252C%2520Discover%2520Knowledge%2522%253A%2520A%2520Chinese%250A%2520%2520Factuality%2520Evaluation%2520for%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DJihao%2520Gu%2520and%2520Yingyao%2520Wang%2520and%2520Pi%2520Bu%2520and%2520Chen%2520Wang%2520and%2520Ziming%2520Wang%2520and%2520Tengtao%2520Song%2520and%2520Donglai%2520Wei%2520and%2520Jiale%2520Yuan%2520and%2520Yingxiu%2520Zhao%2520and%2520Yancheng%2520He%2520and%2520Shilong%2520Li%2520and%2520Jiaheng%2520Liu%2520and%2520Meng%2520Cao%2520and%2520Jun%2520Song%2520and%2520Yingshui%2520Tan%2520and%2520Xiang%2520Li%2520and%2520Wenbo%2520Su%2520and%2520Zhicheng%2520Zheng%2520and%2520Xiaoyong%2520Zhu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520factual%2520accuracy%2520in%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%250Ahas%2520lagged%2520behind%2520their%2520rapid%2520development%252C%2520making%2520it%2520challenging%2520to%2520fully%250Areflect%2520these%2520models%2527%2520knowledge%2520capacity%2520and%2520reliability.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520first%2520factuality-based%2520visual%2520question-answering%2520benchmark%2520in%250AChinese%252C%2520named%2520ChineseSimpleVQA%252C%2520aimed%2520at%2520assessing%2520the%2520visual%2520factuality%2520of%250ALVLMs%2520across%25208%2520major%2520topics%2520and%252056%2520subtopics.%2520The%2520key%2520features%2520of%2520this%250Abenchmark%2520include%2520a%2520focus%2520on%2520the%2520Chinese%2520language%252C%2520diverse%2520knowledge%2520types%252C%2520a%250Amulti-hop%2520question%2520construction%252C%2520high-quality%2520data%252C%2520static%2520consistency%252C%2520and%250Aeasy-to-evaluate%2520through%2520short%2520answers.%2520Moreover%252C%2520we%2520contribute%2520a%2520rigorous%2520data%250Aconstruction%2520pipeline%2520and%2520decouple%2520the%2520visual%2520factuality%2520into%2520two%2520parts%253A%2520seeing%250Athe%2520world%2520%2528i.e.%252C%2520object%2520recognition%2529%2520and%2520discovering%2520knowledge.%2520This%2520decoupling%250Aallows%2520us%2520to%2520analyze%2520the%2520capability%2520boundaries%2520and%2520execution%2520mechanisms%2520of%250ALVLMs.%2520Subsequently%252C%2520we%2520evaluate%252034%2520advanced%2520open-source%2520and%2520closed-source%250Amodels%252C%2520revealing%2520critical%2520performance%2520gaps%2520within%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11718v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChineseSimpleVQA%20--%20%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%0A%20%20Factuality%20Evaluation%20for%20Large%20Vision%20Language%20Models&entry.906535625=Jihao%20Gu%20and%20Yingyao%20Wang%20and%20Pi%20Bu%20and%20Chen%20Wang%20and%20Ziming%20Wang%20and%20Tengtao%20Song%20and%20Donglai%20Wei%20and%20Jiale%20Yuan%20and%20Yingxiu%20Zhao%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Jiaheng%20Liu%20and%20Meng%20Cao%20and%20Jun%20Song%20and%20Yingshui%20Tan%20and%20Xiang%20Li%20and%20Wenbo%20Su%20and%20Zhicheng%20Zheng%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng&entry.1292438233=%20%20The%20evaluation%20of%20factual%20accuracy%20in%20large%20vision%20language%20models%20%28LVLMs%29%0Ahas%20lagged%20behind%20their%20rapid%20development%2C%20making%20it%20challenging%20to%20fully%0Areflect%20these%20models%27%20knowledge%20capacity%20and%20reliability.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20first%20factuality-based%20visual%20question-answering%20benchmark%20in%0AChinese%2C%20named%20ChineseSimpleVQA%2C%20aimed%20at%20assessing%20the%20visual%20factuality%20of%0ALVLMs%20across%208%20major%20topics%20and%2056%20subtopics.%20The%20key%20features%20of%20this%0Abenchmark%20include%20a%20focus%20on%20the%20Chinese%20language%2C%20diverse%20knowledge%20types%2C%20a%0Amulti-hop%20question%20construction%2C%20high-quality%20data%2C%20static%20consistency%2C%20and%0Aeasy-to-evaluate%20through%20short%20answers.%20Moreover%2C%20we%20contribute%20a%20rigorous%20data%0Aconstruction%20pipeline%20and%20decouple%20the%20visual%20factuality%20into%20two%20parts%3A%20seeing%0Athe%20world%20%28i.e.%2C%20object%20recognition%29%20and%20discovering%20knowledge.%20This%20decoupling%0Aallows%20us%20to%20analyze%20the%20capability%20boundaries%20and%20execution%20mechanisms%20of%0ALVLMs.%20Subsequently%2C%20we%20evaluate%2034%20advanced%20open-source%20and%20closed-source%0Amodels%2C%20revealing%20critical%20performance%20gaps%20within%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11718v3&entry.124074799=Read"},
{"title": "BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop\n  Closure", "author": "Haoxin Cai and Shenghai Yuan and Xinyi Li and Junfeng Guo and Jianqi Liu", "abstract": "  This work introduces BEV-LIO(LC), a novel LiDAR-Inertial Odometry (LIO)\nframework that combines Bird's Eye View (BEV) image representations of LiDAR\ndata with geometry-based point cloud registration and incorporates loop closure\n(LC) through BEV image features. By normalizing point density, we project LiDAR\npoint clouds into BEV images, thereby enabling efficient feature extraction and\nmatching. A lightweight convolutional neural network (CNN) based feature\nextractor is employed to extract distinctive local and global descriptors from\nthe BEV images. Local descriptors are used to match BEV images with FAST\nkeypoints for reprojection error construction, while global descriptors\nfacilitate loop closure detection. Reprojection error minimization is then\nintegrated with point-to-plane registration within an iterated Extended Kalman\nFilter (iEKF). In the back-end, global descriptors are used to create a\nKD-tree-indexed keyframe database for accurate loop closure detection. When a\nloop closure is detected, Random Sample Consensus (RANSAC) computes a coarse\ntransform from BEV image matching, which serves as the initial estimate for\nIterative Closest Point (ICP). The refined transform is subsequently\nincorporated into a factor graph along with odometry factors, improving the\nglobal consistency of localization. Extensive experiments conducted in various\nscenarios with different LiDAR types demonstrate that BEV-LIO(LC) outperforms\nstate-of-the-art methods, achieving competitive localization accuracy. Our\ncode, video and supplementary materials can be found at\nhttps://github.com/HxCa1/BEV-LIO-LC.\n", "link": "http://arxiv.org/abs/2502.19242v1", "date": "2025-02-26", "relevancy": 2.774, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5755}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5523}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEV-LIO%28LC%29%3A%20BEV%20Image%20Assisted%20LiDAR-Inertial%20Odometry%20with%20Loop%0A%20%20Closure&body=Title%3A%20BEV-LIO%28LC%29%3A%20BEV%20Image%20Assisted%20LiDAR-Inertial%20Odometry%20with%20Loop%0A%20%20Closure%0AAuthor%3A%20Haoxin%20Cai%20and%20Shenghai%20Yuan%20and%20Xinyi%20Li%20and%20Junfeng%20Guo%20and%20Jianqi%20Liu%0AAbstract%3A%20%20%20This%20work%20introduces%20BEV-LIO%28LC%29%2C%20a%20novel%20LiDAR-Inertial%20Odometry%20%28LIO%29%0Aframework%20that%20combines%20Bird%27s%20Eye%20View%20%28BEV%29%20image%20representations%20of%20LiDAR%0Adata%20with%20geometry-based%20point%20cloud%20registration%20and%20incorporates%20loop%20closure%0A%28LC%29%20through%20BEV%20image%20features.%20By%20normalizing%20point%20density%2C%20we%20project%20LiDAR%0Apoint%20clouds%20into%20BEV%20images%2C%20thereby%20enabling%20efficient%20feature%20extraction%20and%0Amatching.%20A%20lightweight%20convolutional%20neural%20network%20%28CNN%29%20based%20feature%0Aextractor%20is%20employed%20to%20extract%20distinctive%20local%20and%20global%20descriptors%20from%0Athe%20BEV%20images.%20Local%20descriptors%20are%20used%20to%20match%20BEV%20images%20with%20FAST%0Akeypoints%20for%20reprojection%20error%20construction%2C%20while%20global%20descriptors%0Afacilitate%20loop%20closure%20detection.%20Reprojection%20error%20minimization%20is%20then%0Aintegrated%20with%20point-to-plane%20registration%20within%20an%20iterated%20Extended%20Kalman%0AFilter%20%28iEKF%29.%20In%20the%20back-end%2C%20global%20descriptors%20are%20used%20to%20create%20a%0AKD-tree-indexed%20keyframe%20database%20for%20accurate%20loop%20closure%20detection.%20When%20a%0Aloop%20closure%20is%20detected%2C%20Random%20Sample%20Consensus%20%28RANSAC%29%20computes%20a%20coarse%0Atransform%20from%20BEV%20image%20matching%2C%20which%20serves%20as%20the%20initial%20estimate%20for%0AIterative%20Closest%20Point%20%28ICP%29.%20The%20refined%20transform%20is%20subsequently%0Aincorporated%20into%20a%20factor%20graph%20along%20with%20odometry%20factors%2C%20improving%20the%0Aglobal%20consistency%20of%20localization.%20Extensive%20experiments%20conducted%20in%20various%0Ascenarios%20with%20different%20LiDAR%20types%20demonstrate%20that%20BEV-LIO%28LC%29%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%20competitive%20localization%20accuracy.%20Our%0Acode%2C%20video%20and%20supplementary%20materials%20can%20be%20found%20at%0Ahttps%3A//github.com/HxCa1/BEV-LIO-LC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEV-LIO%2528LC%2529%253A%2520BEV%2520Image%2520Assisted%2520LiDAR-Inertial%2520Odometry%2520with%2520Loop%250A%2520%2520Closure%26entry.906535625%3DHaoxin%2520Cai%2520and%2520Shenghai%2520Yuan%2520and%2520Xinyi%2520Li%2520and%2520Junfeng%2520Guo%2520and%2520Jianqi%2520Liu%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520BEV-LIO%2528LC%2529%252C%2520a%2520novel%2520LiDAR-Inertial%2520Odometry%2520%2528LIO%2529%250Aframework%2520that%2520combines%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520image%2520representations%2520of%2520LiDAR%250Adata%2520with%2520geometry-based%2520point%2520cloud%2520registration%2520and%2520incorporates%2520loop%2520closure%250A%2528LC%2529%2520through%2520BEV%2520image%2520features.%2520By%2520normalizing%2520point%2520density%252C%2520we%2520project%2520LiDAR%250Apoint%2520clouds%2520into%2520BEV%2520images%252C%2520thereby%2520enabling%2520efficient%2520feature%2520extraction%2520and%250Amatching.%2520A%2520lightweight%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520based%2520feature%250Aextractor%2520is%2520employed%2520to%2520extract%2520distinctive%2520local%2520and%2520global%2520descriptors%2520from%250Athe%2520BEV%2520images.%2520Local%2520descriptors%2520are%2520used%2520to%2520match%2520BEV%2520images%2520with%2520FAST%250Akeypoints%2520for%2520reprojection%2520error%2520construction%252C%2520while%2520global%2520descriptors%250Afacilitate%2520loop%2520closure%2520detection.%2520Reprojection%2520error%2520minimization%2520is%2520then%250Aintegrated%2520with%2520point-to-plane%2520registration%2520within%2520an%2520iterated%2520Extended%2520Kalman%250AFilter%2520%2528iEKF%2529.%2520In%2520the%2520back-end%252C%2520global%2520descriptors%2520are%2520used%2520to%2520create%2520a%250AKD-tree-indexed%2520keyframe%2520database%2520for%2520accurate%2520loop%2520closure%2520detection.%2520When%2520a%250Aloop%2520closure%2520is%2520detected%252C%2520Random%2520Sample%2520Consensus%2520%2528RANSAC%2529%2520computes%2520a%2520coarse%250Atransform%2520from%2520BEV%2520image%2520matching%252C%2520which%2520serves%2520as%2520the%2520initial%2520estimate%2520for%250AIterative%2520Closest%2520Point%2520%2528ICP%2529.%2520The%2520refined%2520transform%2520is%2520subsequently%250Aincorporated%2520into%2520a%2520factor%2520graph%2520along%2520with%2520odometry%2520factors%252C%2520improving%2520the%250Aglobal%2520consistency%2520of%2520localization.%2520Extensive%2520experiments%2520conducted%2520in%2520various%250Ascenarios%2520with%2520different%2520LiDAR%2520types%2520demonstrate%2520that%2520BEV-LIO%2528LC%2529%2520outperforms%250Astate-of-the-art%2520methods%252C%2520achieving%2520competitive%2520localization%2520accuracy.%2520Our%250Acode%252C%2520video%2520and%2520supplementary%2520materials%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/HxCa1/BEV-LIO-LC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEV-LIO%28LC%29%3A%20BEV%20Image%20Assisted%20LiDAR-Inertial%20Odometry%20with%20Loop%0A%20%20Closure&entry.906535625=Haoxin%20Cai%20and%20Shenghai%20Yuan%20and%20Xinyi%20Li%20and%20Junfeng%20Guo%20and%20Jianqi%20Liu&entry.1292438233=%20%20This%20work%20introduces%20BEV-LIO%28LC%29%2C%20a%20novel%20LiDAR-Inertial%20Odometry%20%28LIO%29%0Aframework%20that%20combines%20Bird%27s%20Eye%20View%20%28BEV%29%20image%20representations%20of%20LiDAR%0Adata%20with%20geometry-based%20point%20cloud%20registration%20and%20incorporates%20loop%20closure%0A%28LC%29%20through%20BEV%20image%20features.%20By%20normalizing%20point%20density%2C%20we%20project%20LiDAR%0Apoint%20clouds%20into%20BEV%20images%2C%20thereby%20enabling%20efficient%20feature%20extraction%20and%0Amatching.%20A%20lightweight%20convolutional%20neural%20network%20%28CNN%29%20based%20feature%0Aextractor%20is%20employed%20to%20extract%20distinctive%20local%20and%20global%20descriptors%20from%0Athe%20BEV%20images.%20Local%20descriptors%20are%20used%20to%20match%20BEV%20images%20with%20FAST%0Akeypoints%20for%20reprojection%20error%20construction%2C%20while%20global%20descriptors%0Afacilitate%20loop%20closure%20detection.%20Reprojection%20error%20minimization%20is%20then%0Aintegrated%20with%20point-to-plane%20registration%20within%20an%20iterated%20Extended%20Kalman%0AFilter%20%28iEKF%29.%20In%20the%20back-end%2C%20global%20descriptors%20are%20used%20to%20create%20a%0AKD-tree-indexed%20keyframe%20database%20for%20accurate%20loop%20closure%20detection.%20When%20a%0Aloop%20closure%20is%20detected%2C%20Random%20Sample%20Consensus%20%28RANSAC%29%20computes%20a%20coarse%0Atransform%20from%20BEV%20image%20matching%2C%20which%20serves%20as%20the%20initial%20estimate%20for%0AIterative%20Closest%20Point%20%28ICP%29.%20The%20refined%20transform%20is%20subsequently%0Aincorporated%20into%20a%20factor%20graph%20along%20with%20odometry%20factors%2C%20improving%20the%0Aglobal%20consistency%20of%20localization.%20Extensive%20experiments%20conducted%20in%20various%0Ascenarios%20with%20different%20LiDAR%20types%20demonstrate%20that%20BEV-LIO%28LC%29%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%20competitive%20localization%20accuracy.%20Our%0Acode%2C%20video%20and%20supplementary%20materials%20can%20be%20found%20at%0Ahttps%3A//github.com/HxCa1/BEV-LIO-LC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19242v1&entry.124074799=Read"},
{"title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding", "author": "Max Ku and Thomas Chong and Jonathan Leung and Krish Shah and Alvin Yu and Wenhu Chen", "abstract": "  Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations.\n", "link": "http://arxiv.org/abs/2502.19400v1", "date": "2025-02-26", "relevancy": 2.7291, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TheoremExplainAgent%3A%20Towards%20Multimodal%20Explanations%20for%20LLM%20Theorem%0A%20%20Understanding&body=Title%3A%20TheoremExplainAgent%3A%20Towards%20Multimodal%20Explanations%20for%20LLM%20Theorem%0A%20%20Understanding%0AAuthor%3A%20Max%20Ku%20and%20Thomas%20Chong%20and%20Jonathan%20Leung%20and%20Krish%20Shah%20and%20Alvin%20Yu%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Understanding%20domain-specific%20theorems%20often%20requires%20more%20than%20just%0Atext-based%20reasoning%3B%20effective%20communication%20through%20structured%20visual%0Aexplanations%20is%20crucial%20for%20deeper%20comprehension.%20While%20large%20language%20models%0A%28LLMs%29%20demonstrate%20strong%20performance%20in%20text-based%20theorem%20reasoning%2C%20their%0Aability%20to%20generate%20coherent%20and%20pedagogically%20meaningful%20visual%20explanations%0Aremains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20introduce%20TheoremExplainAgent%2C%20an%0Aagentic%20approach%20for%20generating%20long-form%20theorem%20explanation%20videos%20%28over%205%0Aminutes%29%20using%20Manim%20animations.%20To%20systematically%20evaluate%20multimodal%20theorem%0Aexplanations%2C%20we%20propose%20TheoremExplainBench%2C%20a%20benchmark%20covering%20240%20theorems%0Aacross%20multiple%20STEM%20disciplines%2C%20along%20with%205%20automated%20evaluation%20metrics.%0AOur%20results%20reveal%20that%20agentic%20planning%20is%20essential%20for%20generating%20detailed%0Along-form%20videos%2C%20and%20the%20o3-mini%20agent%20achieves%20a%20success%20rate%20of%2093.8%25%20and%20an%0Aoverall%20score%20of%200.77.%20However%2C%20our%20quantitative%20and%20qualitative%20studies%20show%0Athat%20most%20of%20the%20videos%20produced%20exhibit%20minor%20issues%20with%20visual%20element%0Alayout.%20Furthermore%2C%20multimodal%20explanations%20expose%20deeper%20reasoning%20flaws%20that%0Atext-based%20explanations%20fail%20to%20reveal%2C%20highlighting%20the%20importance%20of%0Amultimodal%20explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoremExplainAgent%253A%2520Towards%2520Multimodal%2520Explanations%2520for%2520LLM%2520Theorem%250A%2520%2520Understanding%26entry.906535625%3DMax%2520Ku%2520and%2520Thomas%2520Chong%2520and%2520Jonathan%2520Leung%2520and%2520Krish%2520Shah%2520and%2520Alvin%2520Yu%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Understanding%2520domain-specific%2520theorems%2520often%2520requires%2520more%2520than%2520just%250Atext-based%2520reasoning%253B%2520effective%2520communication%2520through%2520structured%2520visual%250Aexplanations%2520is%2520crucial%2520for%2520deeper%2520comprehension.%2520While%2520large%2520language%2520models%250A%2528LLMs%2529%2520demonstrate%2520strong%2520performance%2520in%2520text-based%2520theorem%2520reasoning%252C%2520their%250Aability%2520to%2520generate%2520coherent%2520and%2520pedagogically%2520meaningful%2520visual%2520explanations%250Aremains%2520an%2520open%2520challenge.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TheoremExplainAgent%252C%2520an%250Aagentic%2520approach%2520for%2520generating%2520long-form%2520theorem%2520explanation%2520videos%2520%2528over%25205%250Aminutes%2529%2520using%2520Manim%2520animations.%2520To%2520systematically%2520evaluate%2520multimodal%2520theorem%250Aexplanations%252C%2520we%2520propose%2520TheoremExplainBench%252C%2520a%2520benchmark%2520covering%2520240%2520theorems%250Aacross%2520multiple%2520STEM%2520disciplines%252C%2520along%2520with%25205%2520automated%2520evaluation%2520metrics.%250AOur%2520results%2520reveal%2520that%2520agentic%2520planning%2520is%2520essential%2520for%2520generating%2520detailed%250Along-form%2520videos%252C%2520and%2520the%2520o3-mini%2520agent%2520achieves%2520a%2520success%2520rate%2520of%252093.8%2525%2520and%2520an%250Aoverall%2520score%2520of%25200.77.%2520However%252C%2520our%2520quantitative%2520and%2520qualitative%2520studies%2520show%250Athat%2520most%2520of%2520the%2520videos%2520produced%2520exhibit%2520minor%2520issues%2520with%2520visual%2520element%250Alayout.%2520Furthermore%252C%2520multimodal%2520explanations%2520expose%2520deeper%2520reasoning%2520flaws%2520that%250Atext-based%2520explanations%2520fail%2520to%2520reveal%252C%2520highlighting%2520the%2520importance%2520of%250Amultimodal%2520explanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TheoremExplainAgent%3A%20Towards%20Multimodal%20Explanations%20for%20LLM%20Theorem%0A%20%20Understanding&entry.906535625=Max%20Ku%20and%20Thomas%20Chong%20and%20Jonathan%20Leung%20and%20Krish%20Shah%20and%20Alvin%20Yu%20and%20Wenhu%20Chen&entry.1292438233=%20%20Understanding%20domain-specific%20theorems%20often%20requires%20more%20than%20just%0Atext-based%20reasoning%3B%20effective%20communication%20through%20structured%20visual%0Aexplanations%20is%20crucial%20for%20deeper%20comprehension.%20While%20large%20language%20models%0A%28LLMs%29%20demonstrate%20strong%20performance%20in%20text-based%20theorem%20reasoning%2C%20their%0Aability%20to%20generate%20coherent%20and%20pedagogically%20meaningful%20visual%20explanations%0Aremains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20introduce%20TheoremExplainAgent%2C%20an%0Aagentic%20approach%20for%20generating%20long-form%20theorem%20explanation%20videos%20%28over%205%0Aminutes%29%20using%20Manim%20animations.%20To%20systematically%20evaluate%20multimodal%20theorem%0Aexplanations%2C%20we%20propose%20TheoremExplainBench%2C%20a%20benchmark%20covering%20240%20theorems%0Aacross%20multiple%20STEM%20disciplines%2C%20along%20with%205%20automated%20evaluation%20metrics.%0AOur%20results%20reveal%20that%20agentic%20planning%20is%20essential%20for%20generating%20detailed%0Along-form%20videos%2C%20and%20the%20o3-mini%20agent%20achieves%20a%20success%20rate%20of%2093.8%25%20and%20an%0Aoverall%20score%20of%200.77.%20However%2C%20our%20quantitative%20and%20qualitative%20studies%20show%0Athat%20most%20of%20the%20videos%20produced%20exhibit%20minor%20issues%20with%20visual%20element%0Alayout.%20Furthermore%2C%20multimodal%20explanations%20expose%20deeper%20reasoning%20flaws%20that%0Atext-based%20explanations%20fail%20to%20reveal%2C%20highlighting%20the%20importance%20of%0Amultimodal%20explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19400v1&entry.124074799=Read"},
{"title": "HyperGCL: Multi-Modal Graph Contrastive Learning via Learnable\n  Hypergraph Views", "author": "Khaled Mohammed Saifuddin and Shihao Ji and Esra Akbas", "abstract": "  Recent advancements in Graph Contrastive Learning (GCL) have demonstrated\nremarkable effectiveness in improving graph representations. However, relying\non predefined augmentations (e.g., node dropping, edge perturbation, attribute\nmasking) may result in the loss of task-relevant information and a lack of\nadaptability to diverse input data. Furthermore, the selection of negative\nsamples remains rarely explored. In this paper, we introduce HyperGCL, a novel\nmultimodal GCL framework from a hypergraph perspective. HyperGCL constructs\nthree distinct hypergraph views by jointly utilizing the input graph's\nstructure and attributes, enabling a comprehensive integration of multiple\nmodalities in contrastive learning. A learnable adaptive topology augmentation\ntechnique enhances these views by preserving important relations and filtering\nout noise. View-specific encoders capture essential characteristics from each\nview, while a network-aware contrastive loss leverages the underlying topology\nto define positive and negative samples effectively. Extensive experiments on\nbenchmark datasets demonstrate that HyperGCL achieves state-of-the-art node\nclassification performance.\n", "link": "http://arxiv.org/abs/2502.13277v2", "date": "2025-02-26", "relevancy": 2.7053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5709}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.535}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperGCL%3A%20Multi-Modal%20Graph%20Contrastive%20Learning%20via%20Learnable%0A%20%20Hypergraph%20Views&body=Title%3A%20HyperGCL%3A%20Multi-Modal%20Graph%20Contrastive%20Learning%20via%20Learnable%0A%20%20Hypergraph%20Views%0AAuthor%3A%20Khaled%20Mohammed%20Saifuddin%20and%20Shihao%20Ji%20and%20Esra%20Akbas%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Graph%20Contrastive%20Learning%20%28GCL%29%20have%20demonstrated%0Aremarkable%20effectiveness%20in%20improving%20graph%20representations.%20However%2C%20relying%0Aon%20predefined%20augmentations%20%28e.g.%2C%20node%20dropping%2C%20edge%20perturbation%2C%20attribute%0Amasking%29%20may%20result%20in%20the%20loss%20of%20task-relevant%20information%20and%20a%20lack%20of%0Aadaptability%20to%20diverse%20input%20data.%20Furthermore%2C%20the%20selection%20of%20negative%0Asamples%20remains%20rarely%20explored.%20In%20this%20paper%2C%20we%20introduce%20HyperGCL%2C%20a%20novel%0Amultimodal%20GCL%20framework%20from%20a%20hypergraph%20perspective.%20HyperGCL%20constructs%0Athree%20distinct%20hypergraph%20views%20by%20jointly%20utilizing%20the%20input%20graph%27s%0Astructure%20and%20attributes%2C%20enabling%20a%20comprehensive%20integration%20of%20multiple%0Amodalities%20in%20contrastive%20learning.%20A%20learnable%20adaptive%20topology%20augmentation%0Atechnique%20enhances%20these%20views%20by%20preserving%20important%20relations%20and%20filtering%0Aout%20noise.%20View-specific%20encoders%20capture%20essential%20characteristics%20from%20each%0Aview%2C%20while%20a%20network-aware%20contrastive%20loss%20leverages%20the%20underlying%20topology%0Ato%20define%20positive%20and%20negative%20samples%20effectively.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20that%20HyperGCL%20achieves%20state-of-the-art%20node%0Aclassification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperGCL%253A%2520Multi-Modal%2520Graph%2520Contrastive%2520Learning%2520via%2520Learnable%250A%2520%2520Hypergraph%2520Views%26entry.906535625%3DKhaled%2520Mohammed%2520Saifuddin%2520and%2520Shihao%2520Ji%2520and%2520Esra%2520Akbas%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Graph%2520Contrastive%2520Learning%2520%2528GCL%2529%2520have%2520demonstrated%250Aremarkable%2520effectiveness%2520in%2520improving%2520graph%2520representations.%2520However%252C%2520relying%250Aon%2520predefined%2520augmentations%2520%2528e.g.%252C%2520node%2520dropping%252C%2520edge%2520perturbation%252C%2520attribute%250Amasking%2529%2520may%2520result%2520in%2520the%2520loss%2520of%2520task-relevant%2520information%2520and%2520a%2520lack%2520of%250Aadaptability%2520to%2520diverse%2520input%2520data.%2520Furthermore%252C%2520the%2520selection%2520of%2520negative%250Asamples%2520remains%2520rarely%2520explored.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520HyperGCL%252C%2520a%2520novel%250Amultimodal%2520GCL%2520framework%2520from%2520a%2520hypergraph%2520perspective.%2520HyperGCL%2520constructs%250Athree%2520distinct%2520hypergraph%2520views%2520by%2520jointly%2520utilizing%2520the%2520input%2520graph%2527s%250Astructure%2520and%2520attributes%252C%2520enabling%2520a%2520comprehensive%2520integration%2520of%2520multiple%250Amodalities%2520in%2520contrastive%2520learning.%2520A%2520learnable%2520adaptive%2520topology%2520augmentation%250Atechnique%2520enhances%2520these%2520views%2520by%2520preserving%2520important%2520relations%2520and%2520filtering%250Aout%2520noise.%2520View-specific%2520encoders%2520capture%2520essential%2520characteristics%2520from%2520each%250Aview%252C%2520while%2520a%2520network-aware%2520contrastive%2520loss%2520leverages%2520the%2520underlying%2520topology%250Ato%2520define%2520positive%2520and%2520negative%2520samples%2520effectively.%2520Extensive%2520experiments%2520on%250Abenchmark%2520datasets%2520demonstrate%2520that%2520HyperGCL%2520achieves%2520state-of-the-art%2520node%250Aclassification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperGCL%3A%20Multi-Modal%20Graph%20Contrastive%20Learning%20via%20Learnable%0A%20%20Hypergraph%20Views&entry.906535625=Khaled%20Mohammed%20Saifuddin%20and%20Shihao%20Ji%20and%20Esra%20Akbas&entry.1292438233=%20%20Recent%20advancements%20in%20Graph%20Contrastive%20Learning%20%28GCL%29%20have%20demonstrated%0Aremarkable%20effectiveness%20in%20improving%20graph%20representations.%20However%2C%20relying%0Aon%20predefined%20augmentations%20%28e.g.%2C%20node%20dropping%2C%20edge%20perturbation%2C%20attribute%0Amasking%29%20may%20result%20in%20the%20loss%20of%20task-relevant%20information%20and%20a%20lack%20of%0Aadaptability%20to%20diverse%20input%20data.%20Furthermore%2C%20the%20selection%20of%20negative%0Asamples%20remains%20rarely%20explored.%20In%20this%20paper%2C%20we%20introduce%20HyperGCL%2C%20a%20novel%0Amultimodal%20GCL%20framework%20from%20a%20hypergraph%20perspective.%20HyperGCL%20constructs%0Athree%20distinct%20hypergraph%20views%20by%20jointly%20utilizing%20the%20input%20graph%27s%0Astructure%20and%20attributes%2C%20enabling%20a%20comprehensive%20integration%20of%20multiple%0Amodalities%20in%20contrastive%20learning.%20A%20learnable%20adaptive%20topology%20augmentation%0Atechnique%20enhances%20these%20views%20by%20preserving%20important%20relations%20and%20filtering%0Aout%20noise.%20View-specific%20encoders%20capture%20essential%20characteristics%20from%20each%0Aview%2C%20while%20a%20network-aware%20contrastive%20loss%20leverages%20the%20underlying%20topology%0Ato%20define%20positive%20and%20negative%20samples%20effectively.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20that%20HyperGCL%20achieves%20state-of-the-art%20node%0Aclassification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13277v2&entry.124074799=Read"},
{"title": "On the Importance of Text Preprocessing for Multimodal Representation\n  Learning and Pathology Report Generation", "author": "Ruben T. Lucassen and Tijn van de Luijtgaarden and Sander P. J. Moonemans and Gerben E. Breimer and Willeke A. M. Blokx and Mitko Veta", "abstract": "  Vision-language models in pathology enable multimodal case retrieval and\nautomated report generation. Many of the models developed so far, however, have\nbeen trained on pathology reports that include information which cannot be\ninferred from paired whole slide images (e.g., patient history), potentially\nleading to hallucinated sentences in generated reports. To this end, we\ninvestigate how the selection of information from pathology reports for\nvision-language modeling affects the quality of the multimodal representations\nand generated reports. More concretely, we compare a model trained on full\nreports against a model trained on preprocessed reports that only include\nsentences describing the cell and tissue appearances based on the H&E-stained\nslides. For the experiments, we built upon the BLIP-2 framework and used a\ncutaneous melanocytic lesion dataset of 42,433 H&E-stained whole slide images\nand 19,636 corresponding pathology reports. Model performance was assessed\nusing image-to-text and text-to-image retrieval, as well as qualitative\nevaluation of the generated reports by an expert pathologist. Our results\ndemonstrate that text preprocessing prevents hallucination in report\ngeneration. Despite the improvement in the quality of the generated reports,\ntraining the vision-language model on full reports showed better cross-modal\nretrieval performance.\n", "link": "http://arxiv.org/abs/2502.19285v1", "date": "2025-02-26", "relevancy": 2.6949, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Importance%20of%20Text%20Preprocessing%20for%20Multimodal%20Representation%0A%20%20Learning%20and%20Pathology%20Report%20Generation&body=Title%3A%20On%20the%20Importance%20of%20Text%20Preprocessing%20for%20Multimodal%20Representation%0A%20%20Learning%20and%20Pathology%20Report%20Generation%0AAuthor%3A%20Ruben%20T.%20Lucassen%20and%20Tijn%20van%20de%20Luijtgaarden%20and%20Sander%20P.%20J.%20Moonemans%20and%20Gerben%20E.%20Breimer%20and%20Willeke%20A.%20M.%20Blokx%20and%20Mitko%20Veta%0AAbstract%3A%20%20%20Vision-language%20models%20in%20pathology%20enable%20multimodal%20case%20retrieval%20and%0Aautomated%20report%20generation.%20Many%20of%20the%20models%20developed%20so%20far%2C%20however%2C%20have%0Abeen%20trained%20on%20pathology%20reports%20that%20include%20information%20which%20cannot%20be%0Ainferred%20from%20paired%20whole%20slide%20images%20%28e.g.%2C%20patient%20history%29%2C%20potentially%0Aleading%20to%20hallucinated%20sentences%20in%20generated%20reports.%20To%20this%20end%2C%20we%0Ainvestigate%20how%20the%20selection%20of%20information%20from%20pathology%20reports%20for%0Avision-language%20modeling%20affects%20the%20quality%20of%20the%20multimodal%20representations%0Aand%20generated%20reports.%20More%20concretely%2C%20we%20compare%20a%20model%20trained%20on%20full%0Areports%20against%20a%20model%20trained%20on%20preprocessed%20reports%20that%20only%20include%0Asentences%20describing%20the%20cell%20and%20tissue%20appearances%20based%20on%20the%20H%26E-stained%0Aslides.%20For%20the%20experiments%2C%20we%20built%20upon%20the%20BLIP-2%20framework%20and%20used%20a%0Acutaneous%20melanocytic%20lesion%20dataset%20of%2042%2C433%20H%26E-stained%20whole%20slide%20images%0Aand%2019%2C636%20corresponding%20pathology%20reports.%20Model%20performance%20was%20assessed%0Ausing%20image-to-text%20and%20text-to-image%20retrieval%2C%20as%20well%20as%20qualitative%0Aevaluation%20of%20the%20generated%20reports%20by%20an%20expert%20pathologist.%20Our%20results%0Ademonstrate%20that%20text%20preprocessing%20prevents%20hallucination%20in%20report%0Ageneration.%20Despite%20the%20improvement%20in%20the%20quality%20of%20the%20generated%20reports%2C%0Atraining%20the%20vision-language%20model%20on%20full%20reports%20showed%20better%20cross-modal%0Aretrieval%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Importance%2520of%2520Text%2520Preprocessing%2520for%2520Multimodal%2520Representation%250A%2520%2520Learning%2520and%2520Pathology%2520Report%2520Generation%26entry.906535625%3DRuben%2520T.%2520Lucassen%2520and%2520Tijn%2520van%2520de%2520Luijtgaarden%2520and%2520Sander%2520P.%2520J.%2520Moonemans%2520and%2520Gerben%2520E.%2520Breimer%2520and%2520Willeke%2520A.%2520M.%2520Blokx%2520and%2520Mitko%2520Veta%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520in%2520pathology%2520enable%2520multimodal%2520case%2520retrieval%2520and%250Aautomated%2520report%2520generation.%2520Many%2520of%2520the%2520models%2520developed%2520so%2520far%252C%2520however%252C%2520have%250Abeen%2520trained%2520on%2520pathology%2520reports%2520that%2520include%2520information%2520which%2520cannot%2520be%250Ainferred%2520from%2520paired%2520whole%2520slide%2520images%2520%2528e.g.%252C%2520patient%2520history%2529%252C%2520potentially%250Aleading%2520to%2520hallucinated%2520sentences%2520in%2520generated%2520reports.%2520To%2520this%2520end%252C%2520we%250Ainvestigate%2520how%2520the%2520selection%2520of%2520information%2520from%2520pathology%2520reports%2520for%250Avision-language%2520modeling%2520affects%2520the%2520quality%2520of%2520the%2520multimodal%2520representations%250Aand%2520generated%2520reports.%2520More%2520concretely%252C%2520we%2520compare%2520a%2520model%2520trained%2520on%2520full%250Areports%2520against%2520a%2520model%2520trained%2520on%2520preprocessed%2520reports%2520that%2520only%2520include%250Asentences%2520describing%2520the%2520cell%2520and%2520tissue%2520appearances%2520based%2520on%2520the%2520H%2526E-stained%250Aslides.%2520For%2520the%2520experiments%252C%2520we%2520built%2520upon%2520the%2520BLIP-2%2520framework%2520and%2520used%2520a%250Acutaneous%2520melanocytic%2520lesion%2520dataset%2520of%252042%252C433%2520H%2526E-stained%2520whole%2520slide%2520images%250Aand%252019%252C636%2520corresponding%2520pathology%2520reports.%2520Model%2520performance%2520was%2520assessed%250Ausing%2520image-to-text%2520and%2520text-to-image%2520retrieval%252C%2520as%2520well%2520as%2520qualitative%250Aevaluation%2520of%2520the%2520generated%2520reports%2520by%2520an%2520expert%2520pathologist.%2520Our%2520results%250Ademonstrate%2520that%2520text%2520preprocessing%2520prevents%2520hallucination%2520in%2520report%250Ageneration.%2520Despite%2520the%2520improvement%2520in%2520the%2520quality%2520of%2520the%2520generated%2520reports%252C%250Atraining%2520the%2520vision-language%2520model%2520on%2520full%2520reports%2520showed%2520better%2520cross-modal%250Aretrieval%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Importance%20of%20Text%20Preprocessing%20for%20Multimodal%20Representation%0A%20%20Learning%20and%20Pathology%20Report%20Generation&entry.906535625=Ruben%20T.%20Lucassen%20and%20Tijn%20van%20de%20Luijtgaarden%20and%20Sander%20P.%20J.%20Moonemans%20and%20Gerben%20E.%20Breimer%20and%20Willeke%20A.%20M.%20Blokx%20and%20Mitko%20Veta&entry.1292438233=%20%20Vision-language%20models%20in%20pathology%20enable%20multimodal%20case%20retrieval%20and%0Aautomated%20report%20generation.%20Many%20of%20the%20models%20developed%20so%20far%2C%20however%2C%20have%0Abeen%20trained%20on%20pathology%20reports%20that%20include%20information%20which%20cannot%20be%0Ainferred%20from%20paired%20whole%20slide%20images%20%28e.g.%2C%20patient%20history%29%2C%20potentially%0Aleading%20to%20hallucinated%20sentences%20in%20generated%20reports.%20To%20this%20end%2C%20we%0Ainvestigate%20how%20the%20selection%20of%20information%20from%20pathology%20reports%20for%0Avision-language%20modeling%20affects%20the%20quality%20of%20the%20multimodal%20representations%0Aand%20generated%20reports.%20More%20concretely%2C%20we%20compare%20a%20model%20trained%20on%20full%0Areports%20against%20a%20model%20trained%20on%20preprocessed%20reports%20that%20only%20include%0Asentences%20describing%20the%20cell%20and%20tissue%20appearances%20based%20on%20the%20H%26E-stained%0Aslides.%20For%20the%20experiments%2C%20we%20built%20upon%20the%20BLIP-2%20framework%20and%20used%20a%0Acutaneous%20melanocytic%20lesion%20dataset%20of%2042%2C433%20H%26E-stained%20whole%20slide%20images%0Aand%2019%2C636%20corresponding%20pathology%20reports.%20Model%20performance%20was%20assessed%0Ausing%20image-to-text%20and%20text-to-image%20retrieval%2C%20as%20well%20as%20qualitative%0Aevaluation%20of%20the%20generated%20reports%20by%20an%20expert%20pathologist.%20Our%20results%0Ademonstrate%20that%20text%20preprocessing%20prevents%20hallucination%20in%20report%0Ageneration.%20Despite%20the%20improvement%20in%20the%20quality%20of%20the%20generated%20reports%2C%0Atraining%20the%20vision-language%20model%20on%20full%20reports%20showed%20better%20cross-modal%0Aretrieval%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19285v1&entry.124074799=Read"},
{"title": "CoopDETR: A Unified Cooperative Perception Framework for 3D Detection\n  via Object Query", "author": "Zhe Wang and Shaocong Xu and Xucai Zhuang and Tongda Xu and Yan Wang and Jingjing Liu and Yilun Chen and Ya-Qin Zhang", "abstract": "  Cooperative perception enhances the individual perception capabilities of\nautonomous vehicles (AVs) by providing a comprehensive view of the environment.\nHowever, balancing perception performance and transmission costs remains a\nsignificant challenge. Current approaches that transmit region-level features\nacross agents are limited in interpretability and demand substantial bandwidth,\nmaking them unsuitable for practical applications. In this work, we propose\nCoopDETR, a novel cooperative perception framework that introduces object-level\nfeature cooperation via object query. Our framework consists of two key\nmodules: single-agent query generation, which efficiently encodes raw sensor\ndata into object queries, reducing transmission cost while preserving essential\ninformation for detection; and cross-agent query fusion, which includes Spatial\nQuery Matching (SQM) and Object Query Aggregation (OQA) to enable effective\ninteraction between queries. Our experiments on the OPV2V and V2XSet datasets\ndemonstrate that CoopDETR achieves state-of-the-art performance and\nsignificantly reduces transmission costs to 1/782 of previous methods.\n", "link": "http://arxiv.org/abs/2502.19313v1", "date": "2025-02-26", "relevancy": 2.6899, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoopDETR%3A%20A%20Unified%20Cooperative%20Perception%20Framework%20for%203D%20Detection%0A%20%20via%20Object%20Query&body=Title%3A%20CoopDETR%3A%20A%20Unified%20Cooperative%20Perception%20Framework%20for%203D%20Detection%0A%20%20via%20Object%20Query%0AAuthor%3A%20Zhe%20Wang%20and%20Shaocong%20Xu%20and%20Xucai%20Zhuang%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Jingjing%20Liu%20and%20Yilun%20Chen%20and%20Ya-Qin%20Zhang%0AAbstract%3A%20%20%20Cooperative%20perception%20enhances%20the%20individual%20perception%20capabilities%20of%0Aautonomous%20vehicles%20%28AVs%29%20by%20providing%20a%20comprehensive%20view%20of%20the%20environment.%0AHowever%2C%20balancing%20perception%20performance%20and%20transmission%20costs%20remains%20a%0Asignificant%20challenge.%20Current%20approaches%20that%20transmit%20region-level%20features%0Aacross%20agents%20are%20limited%20in%20interpretability%20and%20demand%20substantial%20bandwidth%2C%0Amaking%20them%20unsuitable%20for%20practical%20applications.%20In%20this%20work%2C%20we%20propose%0ACoopDETR%2C%20a%20novel%20cooperative%20perception%20framework%20that%20introduces%20object-level%0Afeature%20cooperation%20via%20object%20query.%20Our%20framework%20consists%20of%20two%20key%0Amodules%3A%20single-agent%20query%20generation%2C%20which%20efficiently%20encodes%20raw%20sensor%0Adata%20into%20object%20queries%2C%20reducing%20transmission%20cost%20while%20preserving%20essential%0Ainformation%20for%20detection%3B%20and%20cross-agent%20query%20fusion%2C%20which%20includes%20Spatial%0AQuery%20Matching%20%28SQM%29%20and%20Object%20Query%20Aggregation%20%28OQA%29%20to%20enable%20effective%0Ainteraction%20between%20queries.%20Our%20experiments%20on%20the%20OPV2V%20and%20V2XSet%20datasets%0Ademonstrate%20that%20CoopDETR%20achieves%20state-of-the-art%20performance%20and%0Asignificantly%20reduces%20transmission%20costs%20to%201/782%20of%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoopDETR%253A%2520A%2520Unified%2520Cooperative%2520Perception%2520Framework%2520for%25203D%2520Detection%250A%2520%2520via%2520Object%2520Query%26entry.906535625%3DZhe%2520Wang%2520and%2520Shaocong%2520Xu%2520and%2520Xucai%2520Zhuang%2520and%2520Tongda%2520Xu%2520and%2520Yan%2520Wang%2520and%2520Jingjing%2520Liu%2520and%2520Yilun%2520Chen%2520and%2520Ya-Qin%2520Zhang%26entry.1292438233%3D%2520%2520Cooperative%2520perception%2520enhances%2520the%2520individual%2520perception%2520capabilities%2520of%250Aautonomous%2520vehicles%2520%2528AVs%2529%2520by%2520providing%2520a%2520comprehensive%2520view%2520of%2520the%2520environment.%250AHowever%252C%2520balancing%2520perception%2520performance%2520and%2520transmission%2520costs%2520remains%2520a%250Asignificant%2520challenge.%2520Current%2520approaches%2520that%2520transmit%2520region-level%2520features%250Aacross%2520agents%2520are%2520limited%2520in%2520interpretability%2520and%2520demand%2520substantial%2520bandwidth%252C%250Amaking%2520them%2520unsuitable%2520for%2520practical%2520applications.%2520In%2520this%2520work%252C%2520we%2520propose%250ACoopDETR%252C%2520a%2520novel%2520cooperative%2520perception%2520framework%2520that%2520introduces%2520object-level%250Afeature%2520cooperation%2520via%2520object%2520query.%2520Our%2520framework%2520consists%2520of%2520two%2520key%250Amodules%253A%2520single-agent%2520query%2520generation%252C%2520which%2520efficiently%2520encodes%2520raw%2520sensor%250Adata%2520into%2520object%2520queries%252C%2520reducing%2520transmission%2520cost%2520while%2520preserving%2520essential%250Ainformation%2520for%2520detection%253B%2520and%2520cross-agent%2520query%2520fusion%252C%2520which%2520includes%2520Spatial%250AQuery%2520Matching%2520%2528SQM%2529%2520and%2520Object%2520Query%2520Aggregation%2520%2528OQA%2529%2520to%2520enable%2520effective%250Ainteraction%2520between%2520queries.%2520Our%2520experiments%2520on%2520the%2520OPV2V%2520and%2520V2XSet%2520datasets%250Ademonstrate%2520that%2520CoopDETR%2520achieves%2520state-of-the-art%2520performance%2520and%250Asignificantly%2520reduces%2520transmission%2520costs%2520to%25201/782%2520of%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoopDETR%3A%20A%20Unified%20Cooperative%20Perception%20Framework%20for%203D%20Detection%0A%20%20via%20Object%20Query&entry.906535625=Zhe%20Wang%20and%20Shaocong%20Xu%20and%20Xucai%20Zhuang%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Jingjing%20Liu%20and%20Yilun%20Chen%20and%20Ya-Qin%20Zhang&entry.1292438233=%20%20Cooperative%20perception%20enhances%20the%20individual%20perception%20capabilities%20of%0Aautonomous%20vehicles%20%28AVs%29%20by%20providing%20a%20comprehensive%20view%20of%20the%20environment.%0AHowever%2C%20balancing%20perception%20performance%20and%20transmission%20costs%20remains%20a%0Asignificant%20challenge.%20Current%20approaches%20that%20transmit%20region-level%20features%0Aacross%20agents%20are%20limited%20in%20interpretability%20and%20demand%20substantial%20bandwidth%2C%0Amaking%20them%20unsuitable%20for%20practical%20applications.%20In%20this%20work%2C%20we%20propose%0ACoopDETR%2C%20a%20novel%20cooperative%20perception%20framework%20that%20introduces%20object-level%0Afeature%20cooperation%20via%20object%20query.%20Our%20framework%20consists%20of%20two%20key%0Amodules%3A%20single-agent%20query%20generation%2C%20which%20efficiently%20encodes%20raw%20sensor%0Adata%20into%20object%20queries%2C%20reducing%20transmission%20cost%20while%20preserving%20essential%0Ainformation%20for%20detection%3B%20and%20cross-agent%20query%20fusion%2C%20which%20includes%20Spatial%0AQuery%20Matching%20%28SQM%29%20and%20Object%20Query%20Aggregation%20%28OQA%29%20to%20enable%20effective%0Ainteraction%20between%20queries.%20Our%20experiments%20on%20the%20OPV2V%20and%20V2XSet%20datasets%0Ademonstrate%20that%20CoopDETR%20achieves%20state-of-the-art%20performance%20and%0Asignificantly%20reduces%20transmission%20costs%20to%201/782%20of%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19313v1&entry.124074799=Read"},
{"title": "Multi-level Attention-guided Graph Neural Network for Image Restoration", "author": "Jiatao Jiang and Zhen Cui and Chunyan Xu and Jian Yang", "abstract": "  In recent years, deep learning has achieved remarkable success in the field\nof image restoration. However, most convolutional neural network-based methods\ntypically focus on a single scale, neglecting the incorporation of multi-scale\ninformation. In image restoration tasks, local features of an image are often\ninsufficient, necessitating the integration of global features to complement\nthem. Although recent neural network algorithms have made significant strides\nin feature extraction, many models do not explicitly model global features or\nconsider the relationship between global and local features. This paper\nproposes multi-level attention-guided graph neural network. The proposed\nnetwork explicitly constructs element block graphs and element graphs within\nfeature maps using multi-attention mechanisms to extract both local structural\nfeatures and global representation information of the image. Since the network\nstruggles to effectively extract global information during image degradation,\nthe structural information of local feature blocks can be used to correct and\nsupplement the global information. Similarly, when element block information in\nthe feature map is missing, it can be refined using global element\nrepresentation information. The graph within the network learns real-time\ndynamic connections through the multi-attention mechanism, and information is\npropagated and aggregated via graph convolution algorithms. By combining local\nelement block information and global element representation information from\nthe feature map, the algorithm can more effectively restore missing information\nin the image. Experimental results on several classic image restoration tasks\ndemonstrate the effectiveness of the proposed method, achieving\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2502.19181v1", "date": "2025-02-26", "relevancy": 2.624, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5381}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5203}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-level%20Attention-guided%20Graph%20Neural%20Network%20for%20Image%20Restoration&body=Title%3A%20Multi-level%20Attention-guided%20Graph%20Neural%20Network%20for%20Image%20Restoration%0AAuthor%3A%20Jiatao%20Jiang%20and%20Zhen%20Cui%20and%20Chunyan%20Xu%20and%20Jian%20Yang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning%20has%20achieved%20remarkable%20success%20in%20the%20field%0Aof%20image%20restoration.%20However%2C%20most%20convolutional%20neural%20network-based%20methods%0Atypically%20focus%20on%20a%20single%20scale%2C%20neglecting%20the%20incorporation%20of%20multi-scale%0Ainformation.%20In%20image%20restoration%20tasks%2C%20local%20features%20of%20an%20image%20are%20often%0Ainsufficient%2C%20necessitating%20the%20integration%20of%20global%20features%20to%20complement%0Athem.%20Although%20recent%20neural%20network%20algorithms%20have%20made%20significant%20strides%0Ain%20feature%20extraction%2C%20many%20models%20do%20not%20explicitly%20model%20global%20features%20or%0Aconsider%20the%20relationship%20between%20global%20and%20local%20features.%20This%20paper%0Aproposes%20multi-level%20attention-guided%20graph%20neural%20network.%20The%20proposed%0Anetwork%20explicitly%20constructs%20element%20block%20graphs%20and%20element%20graphs%20within%0Afeature%20maps%20using%20multi-attention%20mechanisms%20to%20extract%20both%20local%20structural%0Afeatures%20and%20global%20representation%20information%20of%20the%20image.%20Since%20the%20network%0Astruggles%20to%20effectively%20extract%20global%20information%20during%20image%20degradation%2C%0Athe%20structural%20information%20of%20local%20feature%20blocks%20can%20be%20used%20to%20correct%20and%0Asupplement%20the%20global%20information.%20Similarly%2C%20when%20element%20block%20information%20in%0Athe%20feature%20map%20is%20missing%2C%20it%20can%20be%20refined%20using%20global%20element%0Arepresentation%20information.%20The%20graph%20within%20the%20network%20learns%20real-time%0Adynamic%20connections%20through%20the%20multi-attention%20mechanism%2C%20and%20information%20is%0Apropagated%20and%20aggregated%20via%20graph%20convolution%20algorithms.%20By%20combining%20local%0Aelement%20block%20information%20and%20global%20element%20representation%20information%20from%0Athe%20feature%20map%2C%20the%20algorithm%20can%20more%20effectively%20restore%20missing%20information%0Ain%20the%20image.%20Experimental%20results%20on%20several%20classic%20image%20restoration%20tasks%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20achieving%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-level%2520Attention-guided%2520Graph%2520Neural%2520Network%2520for%2520Image%2520Restoration%26entry.906535625%3DJiatao%2520Jiang%2520and%2520Zhen%2520Cui%2520and%2520Chunyan%2520Xu%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520learning%2520has%2520achieved%2520remarkable%2520success%2520in%2520the%2520field%250Aof%2520image%2520restoration.%2520However%252C%2520most%2520convolutional%2520neural%2520network-based%2520methods%250Atypically%2520focus%2520on%2520a%2520single%2520scale%252C%2520neglecting%2520the%2520incorporation%2520of%2520multi-scale%250Ainformation.%2520In%2520image%2520restoration%2520tasks%252C%2520local%2520features%2520of%2520an%2520image%2520are%2520often%250Ainsufficient%252C%2520necessitating%2520the%2520integration%2520of%2520global%2520features%2520to%2520complement%250Athem.%2520Although%2520recent%2520neural%2520network%2520algorithms%2520have%2520made%2520significant%2520strides%250Ain%2520feature%2520extraction%252C%2520many%2520models%2520do%2520not%2520explicitly%2520model%2520global%2520features%2520or%250Aconsider%2520the%2520relationship%2520between%2520global%2520and%2520local%2520features.%2520This%2520paper%250Aproposes%2520multi-level%2520attention-guided%2520graph%2520neural%2520network.%2520The%2520proposed%250Anetwork%2520explicitly%2520constructs%2520element%2520block%2520graphs%2520and%2520element%2520graphs%2520within%250Afeature%2520maps%2520using%2520multi-attention%2520mechanisms%2520to%2520extract%2520both%2520local%2520structural%250Afeatures%2520and%2520global%2520representation%2520information%2520of%2520the%2520image.%2520Since%2520the%2520network%250Astruggles%2520to%2520effectively%2520extract%2520global%2520information%2520during%2520image%2520degradation%252C%250Athe%2520structural%2520information%2520of%2520local%2520feature%2520blocks%2520can%2520be%2520used%2520to%2520correct%2520and%250Asupplement%2520the%2520global%2520information.%2520Similarly%252C%2520when%2520element%2520block%2520information%2520in%250Athe%2520feature%2520map%2520is%2520missing%252C%2520it%2520can%2520be%2520refined%2520using%2520global%2520element%250Arepresentation%2520information.%2520The%2520graph%2520within%2520the%2520network%2520learns%2520real-time%250Adynamic%2520connections%2520through%2520the%2520multi-attention%2520mechanism%252C%2520and%2520information%2520is%250Apropagated%2520and%2520aggregated%2520via%2520graph%2520convolution%2520algorithms.%2520By%2520combining%2520local%250Aelement%2520block%2520information%2520and%2520global%2520element%2520representation%2520information%2520from%250Athe%2520feature%2520map%252C%2520the%2520algorithm%2520can%2520more%2520effectively%2520restore%2520missing%2520information%250Ain%2520the%2520image.%2520Experimental%2520results%2520on%2520several%2520classic%2520image%2520restoration%2520tasks%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520achieving%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-level%20Attention-guided%20Graph%20Neural%20Network%20for%20Image%20Restoration&entry.906535625=Jiatao%20Jiang%20and%20Zhen%20Cui%20and%20Chunyan%20Xu%20and%20Jian%20Yang&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning%20has%20achieved%20remarkable%20success%20in%20the%20field%0Aof%20image%20restoration.%20However%2C%20most%20convolutional%20neural%20network-based%20methods%0Atypically%20focus%20on%20a%20single%20scale%2C%20neglecting%20the%20incorporation%20of%20multi-scale%0Ainformation.%20In%20image%20restoration%20tasks%2C%20local%20features%20of%20an%20image%20are%20often%0Ainsufficient%2C%20necessitating%20the%20integration%20of%20global%20features%20to%20complement%0Athem.%20Although%20recent%20neural%20network%20algorithms%20have%20made%20significant%20strides%0Ain%20feature%20extraction%2C%20many%20models%20do%20not%20explicitly%20model%20global%20features%20or%0Aconsider%20the%20relationship%20between%20global%20and%20local%20features.%20This%20paper%0Aproposes%20multi-level%20attention-guided%20graph%20neural%20network.%20The%20proposed%0Anetwork%20explicitly%20constructs%20element%20block%20graphs%20and%20element%20graphs%20within%0Afeature%20maps%20using%20multi-attention%20mechanisms%20to%20extract%20both%20local%20structural%0Afeatures%20and%20global%20representation%20information%20of%20the%20image.%20Since%20the%20network%0Astruggles%20to%20effectively%20extract%20global%20information%20during%20image%20degradation%2C%0Athe%20structural%20information%20of%20local%20feature%20blocks%20can%20be%20used%20to%20correct%20and%0Asupplement%20the%20global%20information.%20Similarly%2C%20when%20element%20block%20information%20in%0Athe%20feature%20map%20is%20missing%2C%20it%20can%20be%20refined%20using%20global%20element%0Arepresentation%20information.%20The%20graph%20within%20the%20network%20learns%20real-time%0Adynamic%20connections%20through%20the%20multi-attention%20mechanism%2C%20and%20information%20is%0Apropagated%20and%20aggregated%20via%20graph%20convolution%20algorithms.%20By%20combining%20local%0Aelement%20block%20information%20and%20global%20element%20representation%20information%20from%0Athe%20feature%20map%2C%20the%20algorithm%20can%20more%20effectively%20restore%20missing%20information%0Ain%20the%20image.%20Experimental%20results%20on%20several%20classic%20image%20restoration%20tasks%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20achieving%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19181v1&entry.124074799=Read"},
{"title": "MCLRL: A Multi-Domain Contrastive Learning with Reinforcement Learning\n  Framework for Few-Shot Modulation Recognition", "author": "Dongwei Xu and Yutao Zhu and Yao Lu and Youpeng Feng and Yun Lin and Qi Xuan", "abstract": "  With the rapid advancements in wireless communication technology, automatic\nmodulation recognition (AMR) plays a critical role in ensuring communication\nsecurity and reliability. However, numerous challenges, including higher\nperformance demands, difficulty in data acquisition under specific scenarios,\nlimited sample size, and low-quality labeled data, hinder its development.\nFew-shot learning (FSL) offers an effective solution by enabling models to\nachieve satisfactory performance with only a limited number of labeled samples.\nWhile most FSL techniques are applied in the field of computer vision, they are\nnot directly applicable to wireless signal processing. This study does not\npropose a new FSL-specific signal model but introduces a framework called\nMCLRL. This framework combines multi-domain contrastive learning with\nreinforcement learning. Multi-domain representations of signals enhance feature\nrichness, while integrating contrastive learning and reinforcement learning\narchitectures enables the extraction of deep features for classification. In\ndownstream tasks, the model achieves excellent performance using only a few\nsamples and minimal training cycles. Experimental results show that the MCLRL\nframework effectively extracts key features from signals, performs well in FSL\ntasks, and maintains flexibility in signal model selection.\n", "link": "http://arxiv.org/abs/2502.19071v1", "date": "2025-02-26", "relevancy": 2.6143, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCLRL%3A%20A%20Multi-Domain%20Contrastive%20Learning%20with%20Reinforcement%20Learning%0A%20%20Framework%20for%20Few-Shot%20Modulation%20Recognition&body=Title%3A%20MCLRL%3A%20A%20Multi-Domain%20Contrastive%20Learning%20with%20Reinforcement%20Learning%0A%20%20Framework%20for%20Few-Shot%20Modulation%20Recognition%0AAuthor%3A%20Dongwei%20Xu%20and%20Yutao%20Zhu%20and%20Yao%20Lu%20and%20Youpeng%20Feng%20and%20Yun%20Lin%20and%20Qi%20Xuan%0AAbstract%3A%20%20%20With%20the%20rapid%20advancements%20in%20wireless%20communication%20technology%2C%20automatic%0Amodulation%20recognition%20%28AMR%29%20plays%20a%20critical%20role%20in%20ensuring%20communication%0Asecurity%20and%20reliability.%20However%2C%20numerous%20challenges%2C%20including%20higher%0Aperformance%20demands%2C%20difficulty%20in%20data%20acquisition%20under%20specific%20scenarios%2C%0Alimited%20sample%20size%2C%20and%20low-quality%20labeled%20data%2C%20hinder%20its%20development.%0AFew-shot%20learning%20%28FSL%29%20offers%20an%20effective%20solution%20by%20enabling%20models%20to%0Aachieve%20satisfactory%20performance%20with%20only%20a%20limited%20number%20of%20labeled%20samples.%0AWhile%20most%20FSL%20techniques%20are%20applied%20in%20the%20field%20of%20computer%20vision%2C%20they%20are%0Anot%20directly%20applicable%20to%20wireless%20signal%20processing.%20This%20study%20does%20not%0Apropose%20a%20new%20FSL-specific%20signal%20model%20but%20introduces%20a%20framework%20called%0AMCLRL.%20This%20framework%20combines%20multi-domain%20contrastive%20learning%20with%0Areinforcement%20learning.%20Multi-domain%20representations%20of%20signals%20enhance%20feature%0Arichness%2C%20while%20integrating%20contrastive%20learning%20and%20reinforcement%20learning%0Aarchitectures%20enables%20the%20extraction%20of%20deep%20features%20for%20classification.%20In%0Adownstream%20tasks%2C%20the%20model%20achieves%20excellent%20performance%20using%20only%20a%20few%0Asamples%20and%20minimal%20training%20cycles.%20Experimental%20results%20show%20that%20the%20MCLRL%0Aframework%20effectively%20extracts%20key%20features%20from%20signals%2C%20performs%20well%20in%20FSL%0Atasks%2C%20and%20maintains%20flexibility%20in%20signal%20model%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCLRL%253A%2520A%2520Multi-Domain%2520Contrastive%2520Learning%2520with%2520Reinforcement%2520Learning%250A%2520%2520Framework%2520for%2520Few-Shot%2520Modulation%2520Recognition%26entry.906535625%3DDongwei%2520Xu%2520and%2520Yutao%2520Zhu%2520and%2520Yao%2520Lu%2520and%2520Youpeng%2520Feng%2520and%2520Yun%2520Lin%2520and%2520Qi%2520Xuan%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancements%2520in%2520wireless%2520communication%2520technology%252C%2520automatic%250Amodulation%2520recognition%2520%2528AMR%2529%2520plays%2520a%2520critical%2520role%2520in%2520ensuring%2520communication%250Asecurity%2520and%2520reliability.%2520However%252C%2520numerous%2520challenges%252C%2520including%2520higher%250Aperformance%2520demands%252C%2520difficulty%2520in%2520data%2520acquisition%2520under%2520specific%2520scenarios%252C%250Alimited%2520sample%2520size%252C%2520and%2520low-quality%2520labeled%2520data%252C%2520hinder%2520its%2520development.%250AFew-shot%2520learning%2520%2528FSL%2529%2520offers%2520an%2520effective%2520solution%2520by%2520enabling%2520models%2520to%250Aachieve%2520satisfactory%2520performance%2520with%2520only%2520a%2520limited%2520number%2520of%2520labeled%2520samples.%250AWhile%2520most%2520FSL%2520techniques%2520are%2520applied%2520in%2520the%2520field%2520of%2520computer%2520vision%252C%2520they%2520are%250Anot%2520directly%2520applicable%2520to%2520wireless%2520signal%2520processing.%2520This%2520study%2520does%2520not%250Apropose%2520a%2520new%2520FSL-specific%2520signal%2520model%2520but%2520introduces%2520a%2520framework%2520called%250AMCLRL.%2520This%2520framework%2520combines%2520multi-domain%2520contrastive%2520learning%2520with%250Areinforcement%2520learning.%2520Multi-domain%2520representations%2520of%2520signals%2520enhance%2520feature%250Arichness%252C%2520while%2520integrating%2520contrastive%2520learning%2520and%2520reinforcement%2520learning%250Aarchitectures%2520enables%2520the%2520extraction%2520of%2520deep%2520features%2520for%2520classification.%2520In%250Adownstream%2520tasks%252C%2520the%2520model%2520achieves%2520excellent%2520performance%2520using%2520only%2520a%2520few%250Asamples%2520and%2520minimal%2520training%2520cycles.%2520Experimental%2520results%2520show%2520that%2520the%2520MCLRL%250Aframework%2520effectively%2520extracts%2520key%2520features%2520from%2520signals%252C%2520performs%2520well%2520in%2520FSL%250Atasks%252C%2520and%2520maintains%2520flexibility%2520in%2520signal%2520model%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCLRL%3A%20A%20Multi-Domain%20Contrastive%20Learning%20with%20Reinforcement%20Learning%0A%20%20Framework%20for%20Few-Shot%20Modulation%20Recognition&entry.906535625=Dongwei%20Xu%20and%20Yutao%20Zhu%20and%20Yao%20Lu%20and%20Youpeng%20Feng%20and%20Yun%20Lin%20and%20Qi%20Xuan&entry.1292438233=%20%20With%20the%20rapid%20advancements%20in%20wireless%20communication%20technology%2C%20automatic%0Amodulation%20recognition%20%28AMR%29%20plays%20a%20critical%20role%20in%20ensuring%20communication%0Asecurity%20and%20reliability.%20However%2C%20numerous%20challenges%2C%20including%20higher%0Aperformance%20demands%2C%20difficulty%20in%20data%20acquisition%20under%20specific%20scenarios%2C%0Alimited%20sample%20size%2C%20and%20low-quality%20labeled%20data%2C%20hinder%20its%20development.%0AFew-shot%20learning%20%28FSL%29%20offers%20an%20effective%20solution%20by%20enabling%20models%20to%0Aachieve%20satisfactory%20performance%20with%20only%20a%20limited%20number%20of%20labeled%20samples.%0AWhile%20most%20FSL%20techniques%20are%20applied%20in%20the%20field%20of%20computer%20vision%2C%20they%20are%0Anot%20directly%20applicable%20to%20wireless%20signal%20processing.%20This%20study%20does%20not%0Apropose%20a%20new%20FSL-specific%20signal%20model%20but%20introduces%20a%20framework%20called%0AMCLRL.%20This%20framework%20combines%20multi-domain%20contrastive%20learning%20with%0Areinforcement%20learning.%20Multi-domain%20representations%20of%20signals%20enhance%20feature%0Arichness%2C%20while%20integrating%20contrastive%20learning%20and%20reinforcement%20learning%0Aarchitectures%20enables%20the%20extraction%20of%20deep%20features%20for%20classification.%20In%0Adownstream%20tasks%2C%20the%20model%20achieves%20excellent%20performance%20using%20only%20a%20few%0Asamples%20and%20minimal%20training%20cycles.%20Experimental%20results%20show%20that%20the%20MCLRL%0Aframework%20effectively%20extracts%20key%20features%20from%20signals%2C%20performs%20well%20in%20FSL%0Atasks%2C%20and%20maintains%20flexibility%20in%20signal%20model%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19071v1&entry.124074799=Read"},
{"title": "FungalZSL: Zero-Shot Fungal Classification with Image Captioning Using a\n  Synthetic Data Approach", "author": "Anju Rani and Daniel O. Arroyo and Petar Durdevic", "abstract": "  The effectiveness of zero-shot classification in large vision-language models\n(VLMs), such as Contrastive Language-Image Pre-training (CLIP), depends on\naccess to extensive, well-aligned text-image datasets. In this work, we\nintroduce two complementary data sources, one generated by large language\nmodels (LLMs) to describe the stages of fungal growth and another comprising a\ndiverse set of synthetic fungi images. These datasets are designed to enhance\nCLIPs zero-shot classification capabilities for fungi-related tasks. To ensure\neffective alignment between text and image data, we project them into CLIPs\nshared representation space, focusing on different fungal growth stages. We\ngenerate text using LLaMA3.2 to bridge modality gaps and synthetically create\nfungi images. Furthermore, we investigate knowledge transfer by comparing text\noutputs from different LLM techniques to refine classification across growth\nstages.\n", "link": "http://arxiv.org/abs/2502.19038v1", "date": "2025-02-26", "relevancy": 2.5911, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FungalZSL%3A%20Zero-Shot%20Fungal%20Classification%20with%20Image%20Captioning%20Using%20a%0A%20%20Synthetic%20Data%20Approach&body=Title%3A%20FungalZSL%3A%20Zero-Shot%20Fungal%20Classification%20with%20Image%20Captioning%20Using%20a%0A%20%20Synthetic%20Data%20Approach%0AAuthor%3A%20Anju%20Rani%20and%20Daniel%20O.%20Arroyo%20and%20Petar%20Durdevic%0AAbstract%3A%20%20%20The%20effectiveness%20of%20zero-shot%20classification%20in%20large%20vision-language%20models%0A%28VLMs%29%2C%20such%20as%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%2C%20depends%20on%0Aaccess%20to%20extensive%2C%20well-aligned%20text-image%20datasets.%20In%20this%20work%2C%20we%0Aintroduce%20two%20complementary%20data%20sources%2C%20one%20generated%20by%20large%20language%0Amodels%20%28LLMs%29%20to%20describe%20the%20stages%20of%20fungal%20growth%20and%20another%20comprising%20a%0Adiverse%20set%20of%20synthetic%20fungi%20images.%20These%20datasets%20are%20designed%20to%20enhance%0ACLIPs%20zero-shot%20classification%20capabilities%20for%20fungi-related%20tasks.%20To%20ensure%0Aeffective%20alignment%20between%20text%20and%20image%20data%2C%20we%20project%20them%20into%20CLIPs%0Ashared%20representation%20space%2C%20focusing%20on%20different%20fungal%20growth%20stages.%20We%0Agenerate%20text%20using%20LLaMA3.2%20to%20bridge%20modality%20gaps%20and%20synthetically%20create%0Afungi%20images.%20Furthermore%2C%20we%20investigate%20knowledge%20transfer%20by%20comparing%20text%0Aoutputs%20from%20different%20LLM%20techniques%20to%20refine%20classification%20across%20growth%0Astages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFungalZSL%253A%2520Zero-Shot%2520Fungal%2520Classification%2520with%2520Image%2520Captioning%2520Using%2520a%250A%2520%2520Synthetic%2520Data%2520Approach%26entry.906535625%3DAnju%2520Rani%2520and%2520Daniel%2520O.%2520Arroyo%2520and%2520Petar%2520Durdevic%26entry.1292438233%3D%2520%2520The%2520effectiveness%2520of%2520zero-shot%2520classification%2520in%2520large%2520vision-language%2520models%250A%2528VLMs%2529%252C%2520such%2520as%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%252C%2520depends%2520on%250Aaccess%2520to%2520extensive%252C%2520well-aligned%2520text-image%2520datasets.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520two%2520complementary%2520data%2520sources%252C%2520one%2520generated%2520by%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520describe%2520the%2520stages%2520of%2520fungal%2520growth%2520and%2520another%2520comprising%2520a%250Adiverse%2520set%2520of%2520synthetic%2520fungi%2520images.%2520These%2520datasets%2520are%2520designed%2520to%2520enhance%250ACLIPs%2520zero-shot%2520classification%2520capabilities%2520for%2520fungi-related%2520tasks.%2520To%2520ensure%250Aeffective%2520alignment%2520between%2520text%2520and%2520image%2520data%252C%2520we%2520project%2520them%2520into%2520CLIPs%250Ashared%2520representation%2520space%252C%2520focusing%2520on%2520different%2520fungal%2520growth%2520stages.%2520We%250Agenerate%2520text%2520using%2520LLaMA3.2%2520to%2520bridge%2520modality%2520gaps%2520and%2520synthetically%2520create%250Afungi%2520images.%2520Furthermore%252C%2520we%2520investigate%2520knowledge%2520transfer%2520by%2520comparing%2520text%250Aoutputs%2520from%2520different%2520LLM%2520techniques%2520to%2520refine%2520classification%2520across%2520growth%250Astages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FungalZSL%3A%20Zero-Shot%20Fungal%20Classification%20with%20Image%20Captioning%20Using%20a%0A%20%20Synthetic%20Data%20Approach&entry.906535625=Anju%20Rani%20and%20Daniel%20O.%20Arroyo%20and%20Petar%20Durdevic&entry.1292438233=%20%20The%20effectiveness%20of%20zero-shot%20classification%20in%20large%20vision-language%20models%0A%28VLMs%29%2C%20such%20as%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%2C%20depends%20on%0Aaccess%20to%20extensive%2C%20well-aligned%20text-image%20datasets.%20In%20this%20work%2C%20we%0Aintroduce%20two%20complementary%20data%20sources%2C%20one%20generated%20by%20large%20language%0Amodels%20%28LLMs%29%20to%20describe%20the%20stages%20of%20fungal%20growth%20and%20another%20comprising%20a%0Adiverse%20set%20of%20synthetic%20fungi%20images.%20These%20datasets%20are%20designed%20to%20enhance%0ACLIPs%20zero-shot%20classification%20capabilities%20for%20fungi-related%20tasks.%20To%20ensure%0Aeffective%20alignment%20between%20text%20and%20image%20data%2C%20we%20project%20them%20into%20CLIPs%0Ashared%20representation%20space%2C%20focusing%20on%20different%20fungal%20growth%20stages.%20We%0Agenerate%20text%20using%20LLaMA3.2%20to%20bridge%20modality%20gaps%20and%20synthetically%20create%0Afungi%20images.%20Furthermore%2C%20we%20investigate%20knowledge%20transfer%20by%20comparing%20text%0Aoutputs%20from%20different%20LLM%20techniques%20to%20refine%20classification%20across%20growth%0Astages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19038v1&entry.124074799=Read"},
{"title": "Global Graph Propagation with Hierarchical Information Transfer for\n  Incomplete Contrastive Multi-view Clustering", "author": "Guoqing Chao and Kaixin Xu and Xijiong Xie and Yongyong Chen", "abstract": "  Incomplete multi-view clustering has become one of the important research\nproblems due to the extensive missing multi-view data in the real world.\nAlthough the existing methods have made great progress, there are still some\nproblems: 1) most methods cannot effectively mine the information hidden in the\nmissing data; 2) most methods typically divide representation learning and\nclustering into two separate stages, but this may affect the clustering\nperformance as the clustering results directly depend on the learned\nrepresentation. To address these problems, we propose a novel incomplete\nmulti-view clustering method with hierarchical information transfer. Firstly,\nwe design the view-specific Graph Convolutional Networks (GCN) to obtain the\nrepresentation encoding the graph structure, which is then fused into the\nconsensus representation. Secondly, considering that one layer of GCN transfers\none-order neighbor node information, the global graph propagation with the\nconsensus representation is proposed to handle the missing data and learn deep\nrepresentation. Finally, we design a weight-sharing pseudo-classifier with\ncontrastive learning to obtain an end-to-end framework that combines\nview-specific representation learning, global graph propagation with\nhierarchical information transfer, and contrastive clustering for joint\noptimization. Extensive experiments conducted on several commonly-used datasets\ndemonstrate the effectiveness and superiority of our method in comparison with\nother state-of-the-art approaches. The code is available at\nhttps://github.com/KelvinXuu/GHICMC.\n", "link": "http://arxiv.org/abs/2502.19291v1", "date": "2025-02-26", "relevancy": 2.5536, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5092}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Graph%20Propagation%20with%20Hierarchical%20Information%20Transfer%20for%0A%20%20Incomplete%20Contrastive%20Multi-view%20Clustering&body=Title%3A%20Global%20Graph%20Propagation%20with%20Hierarchical%20Information%20Transfer%20for%0A%20%20Incomplete%20Contrastive%20Multi-view%20Clustering%0AAuthor%3A%20Guoqing%20Chao%20and%20Kaixin%20Xu%20and%20Xijiong%20Xie%20and%20Yongyong%20Chen%0AAbstract%3A%20%20%20Incomplete%20multi-view%20clustering%20has%20become%20one%20of%20the%20important%20research%0Aproblems%20due%20to%20the%20extensive%20missing%20multi-view%20data%20in%20the%20real%20world.%0AAlthough%20the%20existing%20methods%20have%20made%20great%20progress%2C%20there%20are%20still%20some%0Aproblems%3A%201%29%20most%20methods%20cannot%20effectively%20mine%20the%20information%20hidden%20in%20the%0Amissing%20data%3B%202%29%20most%20methods%20typically%20divide%20representation%20learning%20and%0Aclustering%20into%20two%20separate%20stages%2C%20but%20this%20may%20affect%20the%20clustering%0Aperformance%20as%20the%20clustering%20results%20directly%20depend%20on%20the%20learned%0Arepresentation.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%20incomplete%0Amulti-view%20clustering%20method%20with%20hierarchical%20information%20transfer.%20Firstly%2C%0Awe%20design%20the%20view-specific%20Graph%20Convolutional%20Networks%20%28GCN%29%20to%20obtain%20the%0Arepresentation%20encoding%20the%20graph%20structure%2C%20which%20is%20then%20fused%20into%20the%0Aconsensus%20representation.%20Secondly%2C%20considering%20that%20one%20layer%20of%20GCN%20transfers%0Aone-order%20neighbor%20node%20information%2C%20the%20global%20graph%20propagation%20with%20the%0Aconsensus%20representation%20is%20proposed%20to%20handle%20the%20missing%20data%20and%20learn%20deep%0Arepresentation.%20Finally%2C%20we%20design%20a%20weight-sharing%20pseudo-classifier%20with%0Acontrastive%20learning%20to%20obtain%20an%20end-to-end%20framework%20that%20combines%0Aview-specific%20representation%20learning%2C%20global%20graph%20propagation%20with%0Ahierarchical%20information%20transfer%2C%20and%20contrastive%20clustering%20for%20joint%0Aoptimization.%20Extensive%20experiments%20conducted%20on%20several%20commonly-used%20datasets%0Ademonstrate%20the%20effectiveness%20and%20superiority%20of%20our%20method%20in%20comparison%20with%0Aother%20state-of-the-art%20approaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/KelvinXuu/GHICMC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Graph%2520Propagation%2520with%2520Hierarchical%2520Information%2520Transfer%2520for%250A%2520%2520Incomplete%2520Contrastive%2520Multi-view%2520Clustering%26entry.906535625%3DGuoqing%2520Chao%2520and%2520Kaixin%2520Xu%2520and%2520Xijiong%2520Xie%2520and%2520Yongyong%2520Chen%26entry.1292438233%3D%2520%2520Incomplete%2520multi-view%2520clustering%2520has%2520become%2520one%2520of%2520the%2520important%2520research%250Aproblems%2520due%2520to%2520the%2520extensive%2520missing%2520multi-view%2520data%2520in%2520the%2520real%2520world.%250AAlthough%2520the%2520existing%2520methods%2520have%2520made%2520great%2520progress%252C%2520there%2520are%2520still%2520some%250Aproblems%253A%25201%2529%2520most%2520methods%2520cannot%2520effectively%2520mine%2520the%2520information%2520hidden%2520in%2520the%250Amissing%2520data%253B%25202%2529%2520most%2520methods%2520typically%2520divide%2520representation%2520learning%2520and%250Aclustering%2520into%2520two%2520separate%2520stages%252C%2520but%2520this%2520may%2520affect%2520the%2520clustering%250Aperformance%2520as%2520the%2520clustering%2520results%2520directly%2520depend%2520on%2520the%2520learned%250Arepresentation.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%2520incomplete%250Amulti-view%2520clustering%2520method%2520with%2520hierarchical%2520information%2520transfer.%2520Firstly%252C%250Awe%2520design%2520the%2520view-specific%2520Graph%2520Convolutional%2520Networks%2520%2528GCN%2529%2520to%2520obtain%2520the%250Arepresentation%2520encoding%2520the%2520graph%2520structure%252C%2520which%2520is%2520then%2520fused%2520into%2520the%250Aconsensus%2520representation.%2520Secondly%252C%2520considering%2520that%2520one%2520layer%2520of%2520GCN%2520transfers%250Aone-order%2520neighbor%2520node%2520information%252C%2520the%2520global%2520graph%2520propagation%2520with%2520the%250Aconsensus%2520representation%2520is%2520proposed%2520to%2520handle%2520the%2520missing%2520data%2520and%2520learn%2520deep%250Arepresentation.%2520Finally%252C%2520we%2520design%2520a%2520weight-sharing%2520pseudo-classifier%2520with%250Acontrastive%2520learning%2520to%2520obtain%2520an%2520end-to-end%2520framework%2520that%2520combines%250Aview-specific%2520representation%2520learning%252C%2520global%2520graph%2520propagation%2520with%250Ahierarchical%2520information%2520transfer%252C%2520and%2520contrastive%2520clustering%2520for%2520joint%250Aoptimization.%2520Extensive%2520experiments%2520conducted%2520on%2520several%2520commonly-used%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520our%2520method%2520in%2520comparison%2520with%250Aother%2520state-of-the-art%2520approaches.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/KelvinXuu/GHICMC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Graph%20Propagation%20with%20Hierarchical%20Information%20Transfer%20for%0A%20%20Incomplete%20Contrastive%20Multi-view%20Clustering&entry.906535625=Guoqing%20Chao%20and%20Kaixin%20Xu%20and%20Xijiong%20Xie%20and%20Yongyong%20Chen&entry.1292438233=%20%20Incomplete%20multi-view%20clustering%20has%20become%20one%20of%20the%20important%20research%0Aproblems%20due%20to%20the%20extensive%20missing%20multi-view%20data%20in%20the%20real%20world.%0AAlthough%20the%20existing%20methods%20have%20made%20great%20progress%2C%20there%20are%20still%20some%0Aproblems%3A%201%29%20most%20methods%20cannot%20effectively%20mine%20the%20information%20hidden%20in%20the%0Amissing%20data%3B%202%29%20most%20methods%20typically%20divide%20representation%20learning%20and%0Aclustering%20into%20two%20separate%20stages%2C%20but%20this%20may%20affect%20the%20clustering%0Aperformance%20as%20the%20clustering%20results%20directly%20depend%20on%20the%20learned%0Arepresentation.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%20incomplete%0Amulti-view%20clustering%20method%20with%20hierarchical%20information%20transfer.%20Firstly%2C%0Awe%20design%20the%20view-specific%20Graph%20Convolutional%20Networks%20%28GCN%29%20to%20obtain%20the%0Arepresentation%20encoding%20the%20graph%20structure%2C%20which%20is%20then%20fused%20into%20the%0Aconsensus%20representation.%20Secondly%2C%20considering%20that%20one%20layer%20of%20GCN%20transfers%0Aone-order%20neighbor%20node%20information%2C%20the%20global%20graph%20propagation%20with%20the%0Aconsensus%20representation%20is%20proposed%20to%20handle%20the%20missing%20data%20and%20learn%20deep%0Arepresentation.%20Finally%2C%20we%20design%20a%20weight-sharing%20pseudo-classifier%20with%0Acontrastive%20learning%20to%20obtain%20an%20end-to-end%20framework%20that%20combines%0Aview-specific%20representation%20learning%2C%20global%20graph%20propagation%20with%0Ahierarchical%20information%20transfer%2C%20and%20contrastive%20clustering%20for%20joint%0Aoptimization.%20Extensive%20experiments%20conducted%20on%20several%20commonly-used%20datasets%0Ademonstrate%20the%20effectiveness%20and%20superiority%20of%20our%20method%20in%20comparison%20with%0Aother%20state-of-the-art%20approaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/KelvinXuu/GHICMC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19291v1&entry.124074799=Read"},
{"title": "Adaptive debiased SGD in high-dimensional GLMs with streaming data", "author": "Ruijian Han and Lan Luo and Yuanhang Luo and Yuanyuan Lin and Jian Huang", "abstract": "  Online statistical inference facilitates real-time analysis of sequentially\ncollected data, making it different from traditional methods that rely on\nstatic datasets. This paper introduces a novel approach to online inference in\nhigh-dimensional generalized linear models, where we update regression\ncoefficient estimates and their standard errors upon each new data arrival. In\ncontrast to existing methods that either require full dataset access or\nlarge-dimensional summary statistics storage, our method operates in a\nsingle-pass mode, significantly reducing both time and space complexity. The\ncore of our methodological innovation lies in an adaptive stochastic gradient\ndescent algorithm tailored for dynamic objective functions, coupled with a\nnovel online debiasing procedure. This allows us to maintain low-dimensional\nsummary statistics while effectively controlling the optimization error\nintroduced by the dynamically changing loss functions. We establish the\nasymptotic normality of our proposed Adaptive Debiased Lasso (ADL) estimator.\nWe conduct extensive simulation experiments to show the statistical validity\nand computational efficiency of our ADL estimator across various settings. Its\ncomputational efficiency is further demonstrated via a real data application to\nthe spam email classification.\n", "link": "http://arxiv.org/abs/2405.18284v3", "date": "2025-02-26", "relevancy": 2.5403, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5224}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5044}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20debiased%20SGD%20in%20high-dimensional%20GLMs%20with%20streaming%20data&body=Title%3A%20Adaptive%20debiased%20SGD%20in%20high-dimensional%20GLMs%20with%20streaming%20data%0AAuthor%3A%20Ruijian%20Han%20and%20Lan%20Luo%20and%20Yuanhang%20Luo%20and%20Yuanyuan%20Lin%20and%20Jian%20Huang%0AAbstract%3A%20%20%20Online%20statistical%20inference%20facilitates%20real-time%20analysis%20of%20sequentially%0Acollected%20data%2C%20making%20it%20different%20from%20traditional%20methods%20that%20rely%20on%0Astatic%20datasets.%20This%20paper%20introduces%20a%20novel%20approach%20to%20online%20inference%20in%0Ahigh-dimensional%20generalized%20linear%20models%2C%20where%20we%20update%20regression%0Acoefficient%20estimates%20and%20their%20standard%20errors%20upon%20each%20new%20data%20arrival.%20In%0Acontrast%20to%20existing%20methods%20that%20either%20require%20full%20dataset%20access%20or%0Alarge-dimensional%20summary%20statistics%20storage%2C%20our%20method%20operates%20in%20a%0Asingle-pass%20mode%2C%20significantly%20reducing%20both%20time%20and%20space%20complexity.%20The%0Acore%20of%20our%20methodological%20innovation%20lies%20in%20an%20adaptive%20stochastic%20gradient%0Adescent%20algorithm%20tailored%20for%20dynamic%20objective%20functions%2C%20coupled%20with%20a%0Anovel%20online%20debiasing%20procedure.%20This%20allows%20us%20to%20maintain%20low-dimensional%0Asummary%20statistics%20while%20effectively%20controlling%20the%20optimization%20error%0Aintroduced%20by%20the%20dynamically%20changing%20loss%20functions.%20We%20establish%20the%0Aasymptotic%20normality%20of%20our%20proposed%20Adaptive%20Debiased%20Lasso%20%28ADL%29%20estimator.%0AWe%20conduct%20extensive%20simulation%20experiments%20to%20show%20the%20statistical%20validity%0Aand%20computational%20efficiency%20of%20our%20ADL%20estimator%20across%20various%20settings.%20Its%0Acomputational%20efficiency%20is%20further%20demonstrated%20via%20a%20real%20data%20application%20to%0Athe%20spam%20email%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18284v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520debiased%2520SGD%2520in%2520high-dimensional%2520GLMs%2520with%2520streaming%2520data%26entry.906535625%3DRuijian%2520Han%2520and%2520Lan%2520Luo%2520and%2520Yuanhang%2520Luo%2520and%2520Yuanyuan%2520Lin%2520and%2520Jian%2520Huang%26entry.1292438233%3D%2520%2520Online%2520statistical%2520inference%2520facilitates%2520real-time%2520analysis%2520of%2520sequentially%250Acollected%2520data%252C%2520making%2520it%2520different%2520from%2520traditional%2520methods%2520that%2520rely%2520on%250Astatic%2520datasets.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520online%2520inference%2520in%250Ahigh-dimensional%2520generalized%2520linear%2520models%252C%2520where%2520we%2520update%2520regression%250Acoefficient%2520estimates%2520and%2520their%2520standard%2520errors%2520upon%2520each%2520new%2520data%2520arrival.%2520In%250Acontrast%2520to%2520existing%2520methods%2520that%2520either%2520require%2520full%2520dataset%2520access%2520or%250Alarge-dimensional%2520summary%2520statistics%2520storage%252C%2520our%2520method%2520operates%2520in%2520a%250Asingle-pass%2520mode%252C%2520significantly%2520reducing%2520both%2520time%2520and%2520space%2520complexity.%2520The%250Acore%2520of%2520our%2520methodological%2520innovation%2520lies%2520in%2520an%2520adaptive%2520stochastic%2520gradient%250Adescent%2520algorithm%2520tailored%2520for%2520dynamic%2520objective%2520functions%252C%2520coupled%2520with%2520a%250Anovel%2520online%2520debiasing%2520procedure.%2520This%2520allows%2520us%2520to%2520maintain%2520low-dimensional%250Asummary%2520statistics%2520while%2520effectively%2520controlling%2520the%2520optimization%2520error%250Aintroduced%2520by%2520the%2520dynamically%2520changing%2520loss%2520functions.%2520We%2520establish%2520the%250Aasymptotic%2520normality%2520of%2520our%2520proposed%2520Adaptive%2520Debiased%2520Lasso%2520%2528ADL%2529%2520estimator.%250AWe%2520conduct%2520extensive%2520simulation%2520experiments%2520to%2520show%2520the%2520statistical%2520validity%250Aand%2520computational%2520efficiency%2520of%2520our%2520ADL%2520estimator%2520across%2520various%2520settings.%2520Its%250Acomputational%2520efficiency%2520is%2520further%2520demonstrated%2520via%2520a%2520real%2520data%2520application%2520to%250Athe%2520spam%2520email%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18284v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20debiased%20SGD%20in%20high-dimensional%20GLMs%20with%20streaming%20data&entry.906535625=Ruijian%20Han%20and%20Lan%20Luo%20and%20Yuanhang%20Luo%20and%20Yuanyuan%20Lin%20and%20Jian%20Huang&entry.1292438233=%20%20Online%20statistical%20inference%20facilitates%20real-time%20analysis%20of%20sequentially%0Acollected%20data%2C%20making%20it%20different%20from%20traditional%20methods%20that%20rely%20on%0Astatic%20datasets.%20This%20paper%20introduces%20a%20novel%20approach%20to%20online%20inference%20in%0Ahigh-dimensional%20generalized%20linear%20models%2C%20where%20we%20update%20regression%0Acoefficient%20estimates%20and%20their%20standard%20errors%20upon%20each%20new%20data%20arrival.%20In%0Acontrast%20to%20existing%20methods%20that%20either%20require%20full%20dataset%20access%20or%0Alarge-dimensional%20summary%20statistics%20storage%2C%20our%20method%20operates%20in%20a%0Asingle-pass%20mode%2C%20significantly%20reducing%20both%20time%20and%20space%20complexity.%20The%0Acore%20of%20our%20methodological%20innovation%20lies%20in%20an%20adaptive%20stochastic%20gradient%0Adescent%20algorithm%20tailored%20for%20dynamic%20objective%20functions%2C%20coupled%20with%20a%0Anovel%20online%20debiasing%20procedure.%20This%20allows%20us%20to%20maintain%20low-dimensional%0Asummary%20statistics%20while%20effectively%20controlling%20the%20optimization%20error%0Aintroduced%20by%20the%20dynamically%20changing%20loss%20functions.%20We%20establish%20the%0Aasymptotic%20normality%20of%20our%20proposed%20Adaptive%20Debiased%20Lasso%20%28ADL%29%20estimator.%0AWe%20conduct%20extensive%20simulation%20experiments%20to%20show%20the%20statistical%20validity%0Aand%20computational%20efficiency%20of%20our%20ADL%20estimator%20across%20various%20settings.%20Its%0Acomputational%20efficiency%20is%20further%20demonstrated%20via%20a%20real%20data%20application%20to%0Athe%20spam%20email%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18284v3&entry.124074799=Read"},
{"title": "Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in\n  Pre-trained Vision-Language Models", "author": "Jiawei Kong and Hao Fang and Sihang Guo and Chenxi Qing and Bin Chen and Bin Wang and Shu-Tao Xia", "abstract": "  While pre-trained Vision-Language Models (VLMs) such as CLIP exhibit\nexcellent representational capabilities for multimodal data, recent studies\nhave shown that they are vulnerable to backdoor attacks. To alleviate the\nthreat, existing defense strategies primarily focus on fine-tuning the entire\nsuspicious model, yet offer only marginal resistance to state-of-the-art\nattacks and often result in a decrease in clean accuracy, particularly in\ndata-limited scenarios. Their failure may be attributed to the mismatch between\ninsufficient fine-tuning data and massive parameters in VLMs. To address this\nchallenge, we propose Class-wise Backdoor Prompt Tuning (CBPT) defense, an\nefficient and effective method that operates on the text prompts to indirectly\npurify the poisoned VLMs. Specifically, we first employ the advanced\ncontrastive learning via our carefully crafted positive and negative samples,\nto effectively invert the backdoor triggers that are potentially adopted by the\nattacker. Once the dummy trigger is established, we utilize the efficient\nprompt tuning technique to optimize these class-wise text prompts for modifying\nthe model's decision boundary to further reclassify the feature regions of\nbackdoor triggers. Extensive experiments demonstrate that CBPT significantly\nmitigates backdoor threats while preserving model utility, e.g. an average\nClean Accuracy (CA) of 58.86\\% and an Attack Success Rate (ASR) of 0.39\\%\nacross seven mainstream backdoor attacks. These results underscore the\nsuperiority of our prompt purifying design to strengthen model robustness\nagainst backdoor attacks.\n", "link": "http://arxiv.org/abs/2502.19269v1", "date": "2025-02-26", "relevancy": 2.519, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.507}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Antidote%3A%20Class-Wise%20Prompt%20Tuning%20for%20Purifying%20Backdoors%20in%0A%20%20Pre-trained%20Vision-Language%20Models&body=Title%3A%20Neural%20Antidote%3A%20Class-Wise%20Prompt%20Tuning%20for%20Purifying%20Backdoors%20in%0A%20%20Pre-trained%20Vision-Language%20Models%0AAuthor%3A%20Jiawei%20Kong%20and%20Hao%20Fang%20and%20Sihang%20Guo%20and%20Chenxi%20Qing%20and%20Bin%20Chen%20and%20Bin%20Wang%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20While%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20such%20as%20CLIP%20exhibit%0Aexcellent%20representational%20capabilities%20for%20multimodal%20data%2C%20recent%20studies%0Ahave%20shown%20that%20they%20are%20vulnerable%20to%20backdoor%20attacks.%20To%20alleviate%20the%0Athreat%2C%20existing%20defense%20strategies%20primarily%20focus%20on%20fine-tuning%20the%20entire%0Asuspicious%20model%2C%20yet%20offer%20only%20marginal%20resistance%20to%20state-of-the-art%0Aattacks%20and%20often%20result%20in%20a%20decrease%20in%20clean%20accuracy%2C%20particularly%20in%0Adata-limited%20scenarios.%20Their%20failure%20may%20be%20attributed%20to%20the%20mismatch%20between%0Ainsufficient%20fine-tuning%20data%20and%20massive%20parameters%20in%20VLMs.%20To%20address%20this%0Achallenge%2C%20we%20propose%20Class-wise%20Backdoor%20Prompt%20Tuning%20%28CBPT%29%20defense%2C%20an%0Aefficient%20and%20effective%20method%20that%20operates%20on%20the%20text%20prompts%20to%20indirectly%0Apurify%20the%20poisoned%20VLMs.%20Specifically%2C%20we%20first%20employ%20the%20advanced%0Acontrastive%20learning%20via%20our%20carefully%20crafted%20positive%20and%20negative%20samples%2C%0Ato%20effectively%20invert%20the%20backdoor%20triggers%20that%20are%20potentially%20adopted%20by%20the%0Aattacker.%20Once%20the%20dummy%20trigger%20is%20established%2C%20we%20utilize%20the%20efficient%0Aprompt%20tuning%20technique%20to%20optimize%20these%20class-wise%20text%20prompts%20for%20modifying%0Athe%20model%27s%20decision%20boundary%20to%20further%20reclassify%20the%20feature%20regions%20of%0Abackdoor%20triggers.%20Extensive%20experiments%20demonstrate%20that%20CBPT%20significantly%0Amitigates%20backdoor%20threats%20while%20preserving%20model%20utility%2C%20e.g.%20an%20average%0AClean%20Accuracy%20%28CA%29%20of%2058.86%5C%25%20and%20an%20Attack%20Success%20Rate%20%28ASR%29%20of%200.39%5C%25%0Aacross%20seven%20mainstream%20backdoor%20attacks.%20These%20results%20underscore%20the%0Asuperiority%20of%20our%20prompt%20purifying%20design%20to%20strengthen%20model%20robustness%0Aagainst%20backdoor%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Antidote%253A%2520Class-Wise%2520Prompt%2520Tuning%2520for%2520Purifying%2520Backdoors%2520in%250A%2520%2520Pre-trained%2520Vision-Language%2520Models%26entry.906535625%3DJiawei%2520Kong%2520and%2520Hao%2520Fang%2520and%2520Sihang%2520Guo%2520and%2520Chenxi%2520Qing%2520and%2520Bin%2520Chen%2520and%2520Bin%2520Wang%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520While%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520exhibit%250Aexcellent%2520representational%2520capabilities%2520for%2520multimodal%2520data%252C%2520recent%2520studies%250Ahave%2520shown%2520that%2520they%2520are%2520vulnerable%2520to%2520backdoor%2520attacks.%2520To%2520alleviate%2520the%250Athreat%252C%2520existing%2520defense%2520strategies%2520primarily%2520focus%2520on%2520fine-tuning%2520the%2520entire%250Asuspicious%2520model%252C%2520yet%2520offer%2520only%2520marginal%2520resistance%2520to%2520state-of-the-art%250Aattacks%2520and%2520often%2520result%2520in%2520a%2520decrease%2520in%2520clean%2520accuracy%252C%2520particularly%2520in%250Adata-limited%2520scenarios.%2520Their%2520failure%2520may%2520be%2520attributed%2520to%2520the%2520mismatch%2520between%250Ainsufficient%2520fine-tuning%2520data%2520and%2520massive%2520parameters%2520in%2520VLMs.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520Class-wise%2520Backdoor%2520Prompt%2520Tuning%2520%2528CBPT%2529%2520defense%252C%2520an%250Aefficient%2520and%2520effective%2520method%2520that%2520operates%2520on%2520the%2520text%2520prompts%2520to%2520indirectly%250Apurify%2520the%2520poisoned%2520VLMs.%2520Specifically%252C%2520we%2520first%2520employ%2520the%2520advanced%250Acontrastive%2520learning%2520via%2520our%2520carefully%2520crafted%2520positive%2520and%2520negative%2520samples%252C%250Ato%2520effectively%2520invert%2520the%2520backdoor%2520triggers%2520that%2520are%2520potentially%2520adopted%2520by%2520the%250Aattacker.%2520Once%2520the%2520dummy%2520trigger%2520is%2520established%252C%2520we%2520utilize%2520the%2520efficient%250Aprompt%2520tuning%2520technique%2520to%2520optimize%2520these%2520class-wise%2520text%2520prompts%2520for%2520modifying%250Athe%2520model%2527s%2520decision%2520boundary%2520to%2520further%2520reclassify%2520the%2520feature%2520regions%2520of%250Abackdoor%2520triggers.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CBPT%2520significantly%250Amitigates%2520backdoor%2520threats%2520while%2520preserving%2520model%2520utility%252C%2520e.g.%2520an%2520average%250AClean%2520Accuracy%2520%2528CA%2529%2520of%252058.86%255C%2525%2520and%2520an%2520Attack%2520Success%2520Rate%2520%2528ASR%2529%2520of%25200.39%255C%2525%250Aacross%2520seven%2520mainstream%2520backdoor%2520attacks.%2520These%2520results%2520underscore%2520the%250Asuperiority%2520of%2520our%2520prompt%2520purifying%2520design%2520to%2520strengthen%2520model%2520robustness%250Aagainst%2520backdoor%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Antidote%3A%20Class-Wise%20Prompt%20Tuning%20for%20Purifying%20Backdoors%20in%0A%20%20Pre-trained%20Vision-Language%20Models&entry.906535625=Jiawei%20Kong%20and%20Hao%20Fang%20and%20Sihang%20Guo%20and%20Chenxi%20Qing%20and%20Bin%20Chen%20and%20Bin%20Wang%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20While%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20such%20as%20CLIP%20exhibit%0Aexcellent%20representational%20capabilities%20for%20multimodal%20data%2C%20recent%20studies%0Ahave%20shown%20that%20they%20are%20vulnerable%20to%20backdoor%20attacks.%20To%20alleviate%20the%0Athreat%2C%20existing%20defense%20strategies%20primarily%20focus%20on%20fine-tuning%20the%20entire%0Asuspicious%20model%2C%20yet%20offer%20only%20marginal%20resistance%20to%20state-of-the-art%0Aattacks%20and%20often%20result%20in%20a%20decrease%20in%20clean%20accuracy%2C%20particularly%20in%0Adata-limited%20scenarios.%20Their%20failure%20may%20be%20attributed%20to%20the%20mismatch%20between%0Ainsufficient%20fine-tuning%20data%20and%20massive%20parameters%20in%20VLMs.%20To%20address%20this%0Achallenge%2C%20we%20propose%20Class-wise%20Backdoor%20Prompt%20Tuning%20%28CBPT%29%20defense%2C%20an%0Aefficient%20and%20effective%20method%20that%20operates%20on%20the%20text%20prompts%20to%20indirectly%0Apurify%20the%20poisoned%20VLMs.%20Specifically%2C%20we%20first%20employ%20the%20advanced%0Acontrastive%20learning%20via%20our%20carefully%20crafted%20positive%20and%20negative%20samples%2C%0Ato%20effectively%20invert%20the%20backdoor%20triggers%20that%20are%20potentially%20adopted%20by%20the%0Aattacker.%20Once%20the%20dummy%20trigger%20is%20established%2C%20we%20utilize%20the%20efficient%0Aprompt%20tuning%20technique%20to%20optimize%20these%20class-wise%20text%20prompts%20for%20modifying%0Athe%20model%27s%20decision%20boundary%20to%20further%20reclassify%20the%20feature%20regions%20of%0Abackdoor%20triggers.%20Extensive%20experiments%20demonstrate%20that%20CBPT%20significantly%0Amitigates%20backdoor%20threats%20while%20preserving%20model%20utility%2C%20e.g.%20an%20average%0AClean%20Accuracy%20%28CA%29%20of%2058.86%5C%25%20and%20an%20Attack%20Success%20Rate%20%28ASR%29%20of%200.39%5C%25%0Aacross%20seven%20mainstream%20backdoor%20attacks.%20These%20results%20underscore%20the%0Asuperiority%20of%20our%20prompt%20purifying%20design%20to%20strengthen%20model%20robustness%0Aagainst%20backdoor%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19269v1&entry.124074799=Read"},
{"title": "TotalSegmentator MRI: Robust Sequence-independent Segmentation of\n  Multiple Anatomic Structures in MRI", "author": "Tugba Akinci D'Antonoli and Lucas K. Berger and Ashraya K. Indrakanti and Nathan Vishwanathan and Jakob Wei\u00df and Matthias Jung and Zeynep Berkarda and Alexander Rau and Marco Reisert and Thomas K\u00fcstner and Alexandra Walter and Elmar M. Merkle and Daniel Boll and Hanns-Christian Breit and Andrew Phillip Nicoli and Martin Segeroth and Joshy Cyriac and Shan Yang and Jakob Wasserthal", "abstract": "  Since the introduction of TotalSegmentator CT, there is demand for a similar\nrobust automated MRI segmentation tool that can be applied across all MRI\nsequences and anatomic structures. In this retrospective study, a nnU-Net model\n(TotalSegmentator) was trained on MRI and CT examinations to segment 80\nanatomic structures relevant for use cases such as organ volumetry, disease\ncharacterization, surgical planning and opportunistic screening. Examinations\nwere randomly sampled from routine clinical studies to represent real-world\nexamples. Dice scores were calculated between the predicted segmentations and\nexpert radiologist reference standard segmentations to evaluate model\nperformance on an internal test set, two external test sets and against two\npublicly available models, and TotalSegmentator CT. The model was applied to an\ninternal dataset containing abdominal MRIs to investigate age-dependent volume\nchanges. A total of 1143 examinations (616 MRIs, 527 CTs) (median age 61 years,\nIQR 50-72) were split into training (n=1088, CT and MRI) and an internal test\nset (n=55; only MRI), two external test sets (AMOS, n=20; CHAOS, n=20; only\nMRI), and an internal aging-study dataset of 8672 abdominal MRIs (median age 59\nyears, IQR 45-70) were included. The model showed a Dice Score of 0.839 on the\ninternal test set and outperformed two other models (Dice Score, 0.862 versus\n0.759; and 0.838 versus 0.560; p<.001 for both). The proposed open-source,\neasy-to-use model allows for automatic, robust segmentation of 80 structures,\nextending the capabilities of TotalSegmentator to MRIs of any sequence. The\nready-to-use online tool is available at https://totalsegmentator.com, the\nmodel at https://github.com/wasserth/TotalSegmentator, and the dataset at\nhttps://zenodo.org/records/14710732.\n", "link": "http://arxiv.org/abs/2405.19492v2", "date": "2025-02-26", "relevancy": 2.5166, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5043}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5043}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TotalSegmentator%20MRI%3A%20Robust%20Sequence-independent%20Segmentation%20of%0A%20%20Multiple%20Anatomic%20Structures%20in%20MRI&body=Title%3A%20TotalSegmentator%20MRI%3A%20Robust%20Sequence-independent%20Segmentation%20of%0A%20%20Multiple%20Anatomic%20Structures%20in%20MRI%0AAuthor%3A%20Tugba%20Akinci%20D%27Antonoli%20and%20Lucas%20K.%20Berger%20and%20Ashraya%20K.%20Indrakanti%20and%20Nathan%20Vishwanathan%20and%20Jakob%20Wei%C3%9F%20and%20Matthias%20Jung%20and%20Zeynep%20Berkarda%20and%20Alexander%20Rau%20and%20Marco%20Reisert%20and%20Thomas%20K%C3%BCstner%20and%20Alexandra%20Walter%20and%20Elmar%20M.%20Merkle%20and%20Daniel%20Boll%20and%20Hanns-Christian%20Breit%20and%20Andrew%20Phillip%20Nicoli%20and%20Martin%20Segeroth%20and%20Joshy%20Cyriac%20and%20Shan%20Yang%20and%20Jakob%20Wasserthal%0AAbstract%3A%20%20%20Since%20the%20introduction%20of%20TotalSegmentator%20CT%2C%20there%20is%20demand%20for%20a%20similar%0Arobust%20automated%20MRI%20segmentation%20tool%20that%20can%20be%20applied%20across%20all%20MRI%0Asequences%20and%20anatomic%20structures.%20In%20this%20retrospective%20study%2C%20a%20nnU-Net%20model%0A%28TotalSegmentator%29%20was%20trained%20on%20MRI%20and%20CT%20examinations%20to%20segment%2080%0Aanatomic%20structures%20relevant%20for%20use%20cases%20such%20as%20organ%20volumetry%2C%20disease%0Acharacterization%2C%20surgical%20planning%20and%20opportunistic%20screening.%20Examinations%0Awere%20randomly%20sampled%20from%20routine%20clinical%20studies%20to%20represent%20real-world%0Aexamples.%20Dice%20scores%20were%20calculated%20between%20the%20predicted%20segmentations%20and%0Aexpert%20radiologist%20reference%20standard%20segmentations%20to%20evaluate%20model%0Aperformance%20on%20an%20internal%20test%20set%2C%20two%20external%20test%20sets%20and%20against%20two%0Apublicly%20available%20models%2C%20and%20TotalSegmentator%20CT.%20The%20model%20was%20applied%20to%20an%0Ainternal%20dataset%20containing%20abdominal%20MRIs%20to%20investigate%20age-dependent%20volume%0Achanges.%20A%20total%20of%201143%20examinations%20%28616%20MRIs%2C%20527%20CTs%29%20%28median%20age%2061%20years%2C%0AIQR%2050-72%29%20were%20split%20into%20training%20%28n%3D1088%2C%20CT%20and%20MRI%29%20and%20an%20internal%20test%0Aset%20%28n%3D55%3B%20only%20MRI%29%2C%20two%20external%20test%20sets%20%28AMOS%2C%20n%3D20%3B%20CHAOS%2C%20n%3D20%3B%20only%0AMRI%29%2C%20and%20an%20internal%20aging-study%20dataset%20of%208672%20abdominal%20MRIs%20%28median%20age%2059%0Ayears%2C%20IQR%2045-70%29%20were%20included.%20The%20model%20showed%20a%20Dice%20Score%20of%200.839%20on%20the%0Ainternal%20test%20set%20and%20outperformed%20two%20other%20models%20%28Dice%20Score%2C%200.862%20versus%0A0.759%3B%20and%200.838%20versus%200.560%3B%20p%3C.001%20for%20both%29.%20The%20proposed%20open-source%2C%0Aeasy-to-use%20model%20allows%20for%20automatic%2C%20robust%20segmentation%20of%2080%20structures%2C%0Aextending%20the%20capabilities%20of%20TotalSegmentator%20to%20MRIs%20of%20any%20sequence.%20The%0Aready-to-use%20online%20tool%20is%20available%20at%20https%3A//totalsegmentator.com%2C%20the%0Amodel%20at%20https%3A//github.com/wasserth/TotalSegmentator%2C%20and%20the%20dataset%20at%0Ahttps%3A//zenodo.org/records/14710732.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTotalSegmentator%2520MRI%253A%2520Robust%2520Sequence-independent%2520Segmentation%2520of%250A%2520%2520Multiple%2520Anatomic%2520Structures%2520in%2520MRI%26entry.906535625%3DTugba%2520Akinci%2520D%2527Antonoli%2520and%2520Lucas%2520K.%2520Berger%2520and%2520Ashraya%2520K.%2520Indrakanti%2520and%2520Nathan%2520Vishwanathan%2520and%2520Jakob%2520Wei%25C3%259F%2520and%2520Matthias%2520Jung%2520and%2520Zeynep%2520Berkarda%2520and%2520Alexander%2520Rau%2520and%2520Marco%2520Reisert%2520and%2520Thomas%2520K%25C3%25BCstner%2520and%2520Alexandra%2520Walter%2520and%2520Elmar%2520M.%2520Merkle%2520and%2520Daniel%2520Boll%2520and%2520Hanns-Christian%2520Breit%2520and%2520Andrew%2520Phillip%2520Nicoli%2520and%2520Martin%2520Segeroth%2520and%2520Joshy%2520Cyriac%2520and%2520Shan%2520Yang%2520and%2520Jakob%2520Wasserthal%26entry.1292438233%3D%2520%2520Since%2520the%2520introduction%2520of%2520TotalSegmentator%2520CT%252C%2520there%2520is%2520demand%2520for%2520a%2520similar%250Arobust%2520automated%2520MRI%2520segmentation%2520tool%2520that%2520can%2520be%2520applied%2520across%2520all%2520MRI%250Asequences%2520and%2520anatomic%2520structures.%2520In%2520this%2520retrospective%2520study%252C%2520a%2520nnU-Net%2520model%250A%2528TotalSegmentator%2529%2520was%2520trained%2520on%2520MRI%2520and%2520CT%2520examinations%2520to%2520segment%252080%250Aanatomic%2520structures%2520relevant%2520for%2520use%2520cases%2520such%2520as%2520organ%2520volumetry%252C%2520disease%250Acharacterization%252C%2520surgical%2520planning%2520and%2520opportunistic%2520screening.%2520Examinations%250Awere%2520randomly%2520sampled%2520from%2520routine%2520clinical%2520studies%2520to%2520represent%2520real-world%250Aexamples.%2520Dice%2520scores%2520were%2520calculated%2520between%2520the%2520predicted%2520segmentations%2520and%250Aexpert%2520radiologist%2520reference%2520standard%2520segmentations%2520to%2520evaluate%2520model%250Aperformance%2520on%2520an%2520internal%2520test%2520set%252C%2520two%2520external%2520test%2520sets%2520and%2520against%2520two%250Apublicly%2520available%2520models%252C%2520and%2520TotalSegmentator%2520CT.%2520The%2520model%2520was%2520applied%2520to%2520an%250Ainternal%2520dataset%2520containing%2520abdominal%2520MRIs%2520to%2520investigate%2520age-dependent%2520volume%250Achanges.%2520A%2520total%2520of%25201143%2520examinations%2520%2528616%2520MRIs%252C%2520527%2520CTs%2529%2520%2528median%2520age%252061%2520years%252C%250AIQR%252050-72%2529%2520were%2520split%2520into%2520training%2520%2528n%253D1088%252C%2520CT%2520and%2520MRI%2529%2520and%2520an%2520internal%2520test%250Aset%2520%2528n%253D55%253B%2520only%2520MRI%2529%252C%2520two%2520external%2520test%2520sets%2520%2528AMOS%252C%2520n%253D20%253B%2520CHAOS%252C%2520n%253D20%253B%2520only%250AMRI%2529%252C%2520and%2520an%2520internal%2520aging-study%2520dataset%2520of%25208672%2520abdominal%2520MRIs%2520%2528median%2520age%252059%250Ayears%252C%2520IQR%252045-70%2529%2520were%2520included.%2520The%2520model%2520showed%2520a%2520Dice%2520Score%2520of%25200.839%2520on%2520the%250Ainternal%2520test%2520set%2520and%2520outperformed%2520two%2520other%2520models%2520%2528Dice%2520Score%252C%25200.862%2520versus%250A0.759%253B%2520and%25200.838%2520versus%25200.560%253B%2520p%253C.001%2520for%2520both%2529.%2520The%2520proposed%2520open-source%252C%250Aeasy-to-use%2520model%2520allows%2520for%2520automatic%252C%2520robust%2520segmentation%2520of%252080%2520structures%252C%250Aextending%2520the%2520capabilities%2520of%2520TotalSegmentator%2520to%2520MRIs%2520of%2520any%2520sequence.%2520The%250Aready-to-use%2520online%2520tool%2520is%2520available%2520at%2520https%253A//totalsegmentator.com%252C%2520the%250Amodel%2520at%2520https%253A//github.com/wasserth/TotalSegmentator%252C%2520and%2520the%2520dataset%2520at%250Ahttps%253A//zenodo.org/records/14710732.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TotalSegmentator%20MRI%3A%20Robust%20Sequence-independent%20Segmentation%20of%0A%20%20Multiple%20Anatomic%20Structures%20in%20MRI&entry.906535625=Tugba%20Akinci%20D%27Antonoli%20and%20Lucas%20K.%20Berger%20and%20Ashraya%20K.%20Indrakanti%20and%20Nathan%20Vishwanathan%20and%20Jakob%20Wei%C3%9F%20and%20Matthias%20Jung%20and%20Zeynep%20Berkarda%20and%20Alexander%20Rau%20and%20Marco%20Reisert%20and%20Thomas%20K%C3%BCstner%20and%20Alexandra%20Walter%20and%20Elmar%20M.%20Merkle%20and%20Daniel%20Boll%20and%20Hanns-Christian%20Breit%20and%20Andrew%20Phillip%20Nicoli%20and%20Martin%20Segeroth%20and%20Joshy%20Cyriac%20and%20Shan%20Yang%20and%20Jakob%20Wasserthal&entry.1292438233=%20%20Since%20the%20introduction%20of%20TotalSegmentator%20CT%2C%20there%20is%20demand%20for%20a%20similar%0Arobust%20automated%20MRI%20segmentation%20tool%20that%20can%20be%20applied%20across%20all%20MRI%0Asequences%20and%20anatomic%20structures.%20In%20this%20retrospective%20study%2C%20a%20nnU-Net%20model%0A%28TotalSegmentator%29%20was%20trained%20on%20MRI%20and%20CT%20examinations%20to%20segment%2080%0Aanatomic%20structures%20relevant%20for%20use%20cases%20such%20as%20organ%20volumetry%2C%20disease%0Acharacterization%2C%20surgical%20planning%20and%20opportunistic%20screening.%20Examinations%0Awere%20randomly%20sampled%20from%20routine%20clinical%20studies%20to%20represent%20real-world%0Aexamples.%20Dice%20scores%20were%20calculated%20between%20the%20predicted%20segmentations%20and%0Aexpert%20radiologist%20reference%20standard%20segmentations%20to%20evaluate%20model%0Aperformance%20on%20an%20internal%20test%20set%2C%20two%20external%20test%20sets%20and%20against%20two%0Apublicly%20available%20models%2C%20and%20TotalSegmentator%20CT.%20The%20model%20was%20applied%20to%20an%0Ainternal%20dataset%20containing%20abdominal%20MRIs%20to%20investigate%20age-dependent%20volume%0Achanges.%20A%20total%20of%201143%20examinations%20%28616%20MRIs%2C%20527%20CTs%29%20%28median%20age%2061%20years%2C%0AIQR%2050-72%29%20were%20split%20into%20training%20%28n%3D1088%2C%20CT%20and%20MRI%29%20and%20an%20internal%20test%0Aset%20%28n%3D55%3B%20only%20MRI%29%2C%20two%20external%20test%20sets%20%28AMOS%2C%20n%3D20%3B%20CHAOS%2C%20n%3D20%3B%20only%0AMRI%29%2C%20and%20an%20internal%20aging-study%20dataset%20of%208672%20abdominal%20MRIs%20%28median%20age%2059%0Ayears%2C%20IQR%2045-70%29%20were%20included.%20The%20model%20showed%20a%20Dice%20Score%20of%200.839%20on%20the%0Ainternal%20test%20set%20and%20outperformed%20two%20other%20models%20%28Dice%20Score%2C%200.862%20versus%0A0.759%3B%20and%200.838%20versus%200.560%3B%20p%3C.001%20for%20both%29.%20The%20proposed%20open-source%2C%0Aeasy-to-use%20model%20allows%20for%20automatic%2C%20robust%20segmentation%20of%2080%20structures%2C%0Aextending%20the%20capabilities%20of%20TotalSegmentator%20to%20MRIs%20of%20any%20sequence.%20The%0Aready-to-use%20online%20tool%20is%20available%20at%20https%3A//totalsegmentator.com%2C%20the%0Amodel%20at%20https%3A//github.com/wasserth/TotalSegmentator%2C%20and%20the%20dataset%20at%0Ahttps%3A//zenodo.org/records/14710732.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19492v2&entry.124074799=Read"},
{"title": "Interpreting Language Reward Models via Contrastive Explanations", "author": "Junqi Jiang and Tom Bewley and Saumitra Mishra and Freddy Lecue and Manuela Veloso", "abstract": "  Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment.\n", "link": "http://arxiv.org/abs/2411.16502v2", "date": "2025-02-26", "relevancy": 2.506, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Language%20Reward%20Models%20via%20Contrastive%20Explanations&body=Title%3A%20Interpreting%20Language%20Reward%20Models%20via%20Contrastive%20Explanations%0AAuthor%3A%20Junqi%20Jiang%20and%20Tom%20Bewley%20and%20Saumitra%20Mishra%20and%20Freddy%20Lecue%20and%20Manuela%20Veloso%0AAbstract%3A%20%20%20Reward%20models%20%28RMs%29%20are%20a%20crucial%20component%20in%20the%20alignment%20of%20large%0Alanguage%20models%27%20%28LLMs%29%20outputs%20with%20human%20values.%20RMs%20approximate%20human%0Apreferences%20over%20possible%20LLM%20responses%20to%20the%20same%20prompt%20by%20predicting%20and%0Acomparing%20reward%20scores.%20However%2C%20as%20they%20are%20typically%20modified%20versions%20of%0ALLMs%20with%20scalar%20output%20heads%2C%20RMs%20are%20large%20black%20boxes%20whose%20predictions%20are%0Anot%20explainable.%20More%20transparent%20RMs%20would%20enable%20improved%20trust%20in%20the%0Aalignment%20of%20LLMs.%20In%20this%20work%2C%20we%20propose%20to%20use%20contrastive%20explanations%20to%0Aexplain%20any%20binary%20response%20comparison%20made%20by%20an%20RM.%20Specifically%2C%20we%20generate%0Aa%20diverse%20set%20of%20new%20comparisons%20similar%20to%20the%20original%20one%20to%20characterise%0Athe%20RM%27s%20local%20behaviour.%20The%20perturbed%20responses%20forming%20the%20new%20comparisons%0Aare%20generated%20to%20explicitly%20modify%20manually%20specified%20high-level%20evaluation%0Aattributes%2C%20on%20which%20analyses%20of%20RM%20behaviour%20are%20grounded.%20In%20quantitative%0Aexperiments%2C%20we%20validate%20the%20effectiveness%20of%20our%20method%20for%20finding%0Ahigh-quality%20contrastive%20explanations.%20We%20then%20showcase%20the%20qualitative%0Ausefulness%20of%20our%20method%20for%20investigating%20global%20sensitivity%20of%20RMs%20to%20each%0Aevaluation%20attribute%2C%20and%20demonstrate%20how%20representative%20examples%20can%20be%0Aautomatically%20extracted%20to%20explain%20and%20compare%20behaviours%20of%20different%20RMs.%20We%0Asee%20our%20method%20as%20a%20flexible%20framework%20for%20RM%20explanation%2C%20providing%20a%20basis%0Afor%20more%20interpretable%20and%20trustworthy%20LLM%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Language%2520Reward%2520Models%2520via%2520Contrastive%2520Explanations%26entry.906535625%3DJunqi%2520Jiang%2520and%2520Tom%2520Bewley%2520and%2520Saumitra%2520Mishra%2520and%2520Freddy%2520Lecue%2520and%2520Manuela%2520Veloso%26entry.1292438233%3D%2520%2520Reward%2520models%2520%2528RMs%2529%2520are%2520a%2520crucial%2520component%2520in%2520the%2520alignment%2520of%2520large%250Alanguage%2520models%2527%2520%2528LLMs%2529%2520outputs%2520with%2520human%2520values.%2520RMs%2520approximate%2520human%250Apreferences%2520over%2520possible%2520LLM%2520responses%2520to%2520the%2520same%2520prompt%2520by%2520predicting%2520and%250Acomparing%2520reward%2520scores.%2520However%252C%2520as%2520they%2520are%2520typically%2520modified%2520versions%2520of%250ALLMs%2520with%2520scalar%2520output%2520heads%252C%2520RMs%2520are%2520large%2520black%2520boxes%2520whose%2520predictions%2520are%250Anot%2520explainable.%2520More%2520transparent%2520RMs%2520would%2520enable%2520improved%2520trust%2520in%2520the%250Aalignment%2520of%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520use%2520contrastive%2520explanations%2520to%250Aexplain%2520any%2520binary%2520response%2520comparison%2520made%2520by%2520an%2520RM.%2520Specifically%252C%2520we%2520generate%250Aa%2520diverse%2520set%2520of%2520new%2520comparisons%2520similar%2520to%2520the%2520original%2520one%2520to%2520characterise%250Athe%2520RM%2527s%2520local%2520behaviour.%2520The%2520perturbed%2520responses%2520forming%2520the%2520new%2520comparisons%250Aare%2520generated%2520to%2520explicitly%2520modify%2520manually%2520specified%2520high-level%2520evaluation%250Aattributes%252C%2520on%2520which%2520analyses%2520of%2520RM%2520behaviour%2520are%2520grounded.%2520In%2520quantitative%250Aexperiments%252C%2520we%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520for%2520finding%250Ahigh-quality%2520contrastive%2520explanations.%2520We%2520then%2520showcase%2520the%2520qualitative%250Ausefulness%2520of%2520our%2520method%2520for%2520investigating%2520global%2520sensitivity%2520of%2520RMs%2520to%2520each%250Aevaluation%2520attribute%252C%2520and%2520demonstrate%2520how%2520representative%2520examples%2520can%2520be%250Aautomatically%2520extracted%2520to%2520explain%2520and%2520compare%2520behaviours%2520of%2520different%2520RMs.%2520We%250Asee%2520our%2520method%2520as%2520a%2520flexible%2520framework%2520for%2520RM%2520explanation%252C%2520providing%2520a%2520basis%250Afor%2520more%2520interpretable%2520and%2520trustworthy%2520LLM%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Language%20Reward%20Models%20via%20Contrastive%20Explanations&entry.906535625=Junqi%20Jiang%20and%20Tom%20Bewley%20and%20Saumitra%20Mishra%20and%20Freddy%20Lecue%20and%20Manuela%20Veloso&entry.1292438233=%20%20Reward%20models%20%28RMs%29%20are%20a%20crucial%20component%20in%20the%20alignment%20of%20large%0Alanguage%20models%27%20%28LLMs%29%20outputs%20with%20human%20values.%20RMs%20approximate%20human%0Apreferences%20over%20possible%20LLM%20responses%20to%20the%20same%20prompt%20by%20predicting%20and%0Acomparing%20reward%20scores.%20However%2C%20as%20they%20are%20typically%20modified%20versions%20of%0ALLMs%20with%20scalar%20output%20heads%2C%20RMs%20are%20large%20black%20boxes%20whose%20predictions%20are%0Anot%20explainable.%20More%20transparent%20RMs%20would%20enable%20improved%20trust%20in%20the%0Aalignment%20of%20LLMs.%20In%20this%20work%2C%20we%20propose%20to%20use%20contrastive%20explanations%20to%0Aexplain%20any%20binary%20response%20comparison%20made%20by%20an%20RM.%20Specifically%2C%20we%20generate%0Aa%20diverse%20set%20of%20new%20comparisons%20similar%20to%20the%20original%20one%20to%20characterise%0Athe%20RM%27s%20local%20behaviour.%20The%20perturbed%20responses%20forming%20the%20new%20comparisons%0Aare%20generated%20to%20explicitly%20modify%20manually%20specified%20high-level%20evaluation%0Aattributes%2C%20on%20which%20analyses%20of%20RM%20behaviour%20are%20grounded.%20In%20quantitative%0Aexperiments%2C%20we%20validate%20the%20effectiveness%20of%20our%20method%20for%20finding%0Ahigh-quality%20contrastive%20explanations.%20We%20then%20showcase%20the%20qualitative%0Ausefulness%20of%20our%20method%20for%20investigating%20global%20sensitivity%20of%20RMs%20to%20each%0Aevaluation%20attribute%2C%20and%20demonstrate%20how%20representative%20examples%20can%20be%0Aautomatically%20extracted%20to%20explain%20and%20compare%20behaviours%20of%20different%20RMs.%20We%0Asee%20our%20method%20as%20a%20flexible%20framework%20for%20RM%20explanation%2C%20providing%20a%20basis%0Afor%20more%20interpretable%20and%20trustworthy%20LLM%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16502v2&entry.124074799=Read"},
{"title": "Detecting Linguistic Indicators for Stereotype Assessment with Large\n  Language Models", "author": "Rebekka G\u00f6rge and Michael Mock and H\u00e9ctor Allende-Cid", "abstract": "  Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.\n", "link": "http://arxiv.org/abs/2502.19160v1", "date": "2025-02-26", "relevancy": 2.4908, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Linguistic%20Indicators%20for%20Stereotype%20Assessment%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Detecting%20Linguistic%20Indicators%20for%20Stereotype%20Assessment%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Rebekka%20G%C3%B6rge%20and%20Michael%20Mock%20and%20H%C3%A9ctor%20Allende-Cid%0AAbstract%3A%20%20%20Social%20categories%20and%20stereotypes%20are%20embedded%20in%20language%20and%20can%20introduce%0Adata%20bias%20into%20Large%20Language%20Models%20%28LLMs%29.%20Despite%20safeguards%2C%20these%20biases%0Aoften%20persist%20in%20model%20behavior%2C%20potentially%20leading%20to%20representational%20harm%0Ain%20outputs.%20While%20sociolinguistic%20research%20provides%20valuable%20insights%20into%20the%0Aformation%20of%20stereotypes%2C%20NLP%20approaches%20for%20stereotype%20detection%20rarely%20draw%0Aon%20this%20foundation%20and%20often%20lack%20objectivity%2C%20precision%2C%20and%20interpretability.%0ATo%20fill%20this%20gap%2C%20in%20this%20work%20we%20propose%20a%20new%20approach%20that%20detects%20and%0Aquantifies%20the%20linguistic%20indicators%20of%20stereotypes%20in%20a%20sentence.%20We%20derive%0Alinguistic%20indicators%20from%20the%20Social%20Category%20and%20Stereotype%20Communication%0A%28SCSC%29%20framework%20which%20indicate%20strong%20social%20category%20formulation%20and%0Astereotyping%20in%20language%2C%20and%20use%20them%20to%20build%20a%20categorization%20scheme.%20To%0Aautomate%20this%20approach%2C%20we%20instruct%20different%20LLMs%20using%20in-context%20learning%20to%0Aapply%20the%20approach%20to%20a%20sentence%2C%20where%20the%20LLM%20examines%20the%20linguistic%0Aproperties%20and%20provides%20a%20basis%20for%20a%20fine-grained%20assessment.%20Based%20on%20an%0Aempirical%20evaluation%20of%20the%20importance%20of%20different%20linguistic%20indicators%2C%20we%0Alearn%20a%20scoring%20function%20that%20measures%20the%20linguistic%20indicators%20of%20a%0Astereotype.%20Our%20annotations%20of%20stereotyped%20sentences%20show%20that%20these%20indicators%0Aare%20present%20in%20these%20sentences%20and%20explain%20the%20strength%20of%20a%20stereotype.%20In%0Aterms%20of%20model%20performance%2C%20our%20results%20show%20that%20the%20models%20generally%20perform%0Awell%20in%20detecting%20and%20classifying%20linguistic%20indicators%20of%20category%20labels%20used%0Ato%20denote%20a%20category%2C%20but%20sometimes%20struggle%20to%20correctly%20evaluate%20the%0Aassociated%20behaviors%20and%20characteristics.%20Using%20more%20few-shot%20examples%20within%0Athe%20prompts%2C%20significantly%20improves%20performance.%20Model%20performance%20increases%0Awith%20size%2C%20as%20Llama-3.3-70B-Instruct%20and%20GPT-4%20achieve%20comparable%20results%20that%0Asurpass%20those%20of%20Mixtral-8x7B-Instruct%2C%20GPT-4-mini%20and%20Llama-3.1-8B-Instruct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Linguistic%2520Indicators%2520for%2520Stereotype%2520Assessment%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DRebekka%2520G%25C3%25B6rge%2520and%2520Michael%2520Mock%2520and%2520H%25C3%25A9ctor%2520Allende-Cid%26entry.1292438233%3D%2520%2520Social%2520categories%2520and%2520stereotypes%2520are%2520embedded%2520in%2520language%2520and%2520can%2520introduce%250Adata%2520bias%2520into%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Despite%2520safeguards%252C%2520these%2520biases%250Aoften%2520persist%2520in%2520model%2520behavior%252C%2520potentially%2520leading%2520to%2520representational%2520harm%250Ain%2520outputs.%2520While%2520sociolinguistic%2520research%2520provides%2520valuable%2520insights%2520into%2520the%250Aformation%2520of%2520stereotypes%252C%2520NLP%2520approaches%2520for%2520stereotype%2520detection%2520rarely%2520draw%250Aon%2520this%2520foundation%2520and%2520often%2520lack%2520objectivity%252C%2520precision%252C%2520and%2520interpretability.%250ATo%2520fill%2520this%2520gap%252C%2520in%2520this%2520work%2520we%2520propose%2520a%2520new%2520approach%2520that%2520detects%2520and%250Aquantifies%2520the%2520linguistic%2520indicators%2520of%2520stereotypes%2520in%2520a%2520sentence.%2520We%2520derive%250Alinguistic%2520indicators%2520from%2520the%2520Social%2520Category%2520and%2520Stereotype%2520Communication%250A%2528SCSC%2529%2520framework%2520which%2520indicate%2520strong%2520social%2520category%2520formulation%2520and%250Astereotyping%2520in%2520language%252C%2520and%2520use%2520them%2520to%2520build%2520a%2520categorization%2520scheme.%2520To%250Aautomate%2520this%2520approach%252C%2520we%2520instruct%2520different%2520LLMs%2520using%2520in-context%2520learning%2520to%250Aapply%2520the%2520approach%2520to%2520a%2520sentence%252C%2520where%2520the%2520LLM%2520examines%2520the%2520linguistic%250Aproperties%2520and%2520provides%2520a%2520basis%2520for%2520a%2520fine-grained%2520assessment.%2520Based%2520on%2520an%250Aempirical%2520evaluation%2520of%2520the%2520importance%2520of%2520different%2520linguistic%2520indicators%252C%2520we%250Alearn%2520a%2520scoring%2520function%2520that%2520measures%2520the%2520linguistic%2520indicators%2520of%2520a%250Astereotype.%2520Our%2520annotations%2520of%2520stereotyped%2520sentences%2520show%2520that%2520these%2520indicators%250Aare%2520present%2520in%2520these%2520sentences%2520and%2520explain%2520the%2520strength%2520of%2520a%2520stereotype.%2520In%250Aterms%2520of%2520model%2520performance%252C%2520our%2520results%2520show%2520that%2520the%2520models%2520generally%2520perform%250Awell%2520in%2520detecting%2520and%2520classifying%2520linguistic%2520indicators%2520of%2520category%2520labels%2520used%250Ato%2520denote%2520a%2520category%252C%2520but%2520sometimes%2520struggle%2520to%2520correctly%2520evaluate%2520the%250Aassociated%2520behaviors%2520and%2520characteristics.%2520Using%2520more%2520few-shot%2520examples%2520within%250Athe%2520prompts%252C%2520significantly%2520improves%2520performance.%2520Model%2520performance%2520increases%250Awith%2520size%252C%2520as%2520Llama-3.3-70B-Instruct%2520and%2520GPT-4%2520achieve%2520comparable%2520results%2520that%250Asurpass%2520those%2520of%2520Mixtral-8x7B-Instruct%252C%2520GPT-4-mini%2520and%2520Llama-3.1-8B-Instruct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Linguistic%20Indicators%20for%20Stereotype%20Assessment%20with%20Large%0A%20%20Language%20Models&entry.906535625=Rebekka%20G%C3%B6rge%20and%20Michael%20Mock%20and%20H%C3%A9ctor%20Allende-Cid&entry.1292438233=%20%20Social%20categories%20and%20stereotypes%20are%20embedded%20in%20language%20and%20can%20introduce%0Adata%20bias%20into%20Large%20Language%20Models%20%28LLMs%29.%20Despite%20safeguards%2C%20these%20biases%0Aoften%20persist%20in%20model%20behavior%2C%20potentially%20leading%20to%20representational%20harm%0Ain%20outputs.%20While%20sociolinguistic%20research%20provides%20valuable%20insights%20into%20the%0Aformation%20of%20stereotypes%2C%20NLP%20approaches%20for%20stereotype%20detection%20rarely%20draw%0Aon%20this%20foundation%20and%20often%20lack%20objectivity%2C%20precision%2C%20and%20interpretability.%0ATo%20fill%20this%20gap%2C%20in%20this%20work%20we%20propose%20a%20new%20approach%20that%20detects%20and%0Aquantifies%20the%20linguistic%20indicators%20of%20stereotypes%20in%20a%20sentence.%20We%20derive%0Alinguistic%20indicators%20from%20the%20Social%20Category%20and%20Stereotype%20Communication%0A%28SCSC%29%20framework%20which%20indicate%20strong%20social%20category%20formulation%20and%0Astereotyping%20in%20language%2C%20and%20use%20them%20to%20build%20a%20categorization%20scheme.%20To%0Aautomate%20this%20approach%2C%20we%20instruct%20different%20LLMs%20using%20in-context%20learning%20to%0Aapply%20the%20approach%20to%20a%20sentence%2C%20where%20the%20LLM%20examines%20the%20linguistic%0Aproperties%20and%20provides%20a%20basis%20for%20a%20fine-grained%20assessment.%20Based%20on%20an%0Aempirical%20evaluation%20of%20the%20importance%20of%20different%20linguistic%20indicators%2C%20we%0Alearn%20a%20scoring%20function%20that%20measures%20the%20linguistic%20indicators%20of%20a%0Astereotype.%20Our%20annotations%20of%20stereotyped%20sentences%20show%20that%20these%20indicators%0Aare%20present%20in%20these%20sentences%20and%20explain%20the%20strength%20of%20a%20stereotype.%20In%0Aterms%20of%20model%20performance%2C%20our%20results%20show%20that%20the%20models%20generally%20perform%0Awell%20in%20detecting%20and%20classifying%20linguistic%20indicators%20of%20category%20labels%20used%0Ato%20denote%20a%20category%2C%20but%20sometimes%20struggle%20to%20correctly%20evaluate%20the%0Aassociated%20behaviors%20and%20characteristics.%20Using%20more%20few-shot%20examples%20within%0Athe%20prompts%2C%20significantly%20improves%20performance.%20Model%20performance%20increases%0Awith%20size%2C%20as%20Llama-3.3-70B-Instruct%20and%20GPT-4%20achieve%20comparable%20results%20that%0Asurpass%20those%20of%20Mixtral-8x7B-Instruct%2C%20GPT-4-mini%20and%20Llama-3.1-8B-Instruct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19160v1&entry.124074799=Read"},
{"title": "GraphBridge: Towards Arbitrary Transfer Learning in GNNs", "author": "Li Ju and Xingyi Yang and Qi Li and Xinchao Wang", "abstract": "  Graph neural networks (GNNs) are conventionally trained on a per-domain,\nper-task basis. It creates a significant barrier in transferring the acquired\nknowledge to different, heterogeneous data setups. This paper introduces\nGraphBridge, a novel framework to enable knowledge transfer across disparate\ntasks and domains in GNNs, circumventing the need for modifications to task\nconfigurations or graph structures. Specifically, GraphBridge allows for the\naugmentation of any pre-trained GNN with prediction heads and a bridging\nnetwork that connects the input to the output layer. This architecture not only\npreserves the intrinsic knowledge of the original model but also supports\noutputs of arbitrary dimensions. To mitigate the negative transfer problem,\nGraphBridg merges the source model with a concurrently trained model, thereby\nreducing the source bias when applied to the target domain. Our method is\nthoroughly evaluated across diverse transfer learning scenarios, including\nGraph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empirical\nvalidation, conducted over 16 datasets representative of these scenarios,\nconfirms the framework's capacity for task- and domain-agnostic transfer\nlearning within graph-like data, marking a significant advancement in the field\nof GNNs.\n", "link": "http://arxiv.org/abs/2502.19252v1", "date": "2025-02-26", "relevancy": 2.4804, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5093}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4937}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphBridge%3A%20Towards%20Arbitrary%20Transfer%20Learning%20in%20GNNs&body=Title%3A%20GraphBridge%3A%20Towards%20Arbitrary%20Transfer%20Learning%20in%20GNNs%0AAuthor%3A%20Li%20Ju%20and%20Xingyi%20Yang%20and%20Qi%20Li%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20conventionally%20trained%20on%20a%20per-domain%2C%0Aper-task%20basis.%20It%20creates%20a%20significant%20barrier%20in%20transferring%20the%20acquired%0Aknowledge%20to%20different%2C%20heterogeneous%20data%20setups.%20This%20paper%20introduces%0AGraphBridge%2C%20a%20novel%20framework%20to%20enable%20knowledge%20transfer%20across%20disparate%0Atasks%20and%20domains%20in%20GNNs%2C%20circumventing%20the%20need%20for%20modifications%20to%20task%0Aconfigurations%20or%20graph%20structures.%20Specifically%2C%20GraphBridge%20allows%20for%20the%0Aaugmentation%20of%20any%20pre-trained%20GNN%20with%20prediction%20heads%20and%20a%20bridging%0Anetwork%20that%20connects%20the%20input%20to%20the%20output%20layer.%20This%20architecture%20not%20only%0Apreserves%20the%20intrinsic%20knowledge%20of%20the%20original%20model%20but%20also%20supports%0Aoutputs%20of%20arbitrary%20dimensions.%20To%20mitigate%20the%20negative%20transfer%20problem%2C%0AGraphBridg%20merges%20the%20source%20model%20with%20a%20concurrently%20trained%20model%2C%20thereby%0Areducing%20the%20source%20bias%20when%20applied%20to%20the%20target%20domain.%20Our%20method%20is%0Athoroughly%20evaluated%20across%20diverse%20transfer%20learning%20scenarios%2C%20including%0AGraph2Graph%2C%20Node2Node%2C%20Graph2Node%2C%20and%20graph2point-cloud.%20Empirical%0Avalidation%2C%20conducted%20over%2016%20datasets%20representative%20of%20these%20scenarios%2C%0Aconfirms%20the%20framework%27s%20capacity%20for%20task-%20and%20domain-agnostic%20transfer%0Alearning%20within%20graph-like%20data%2C%20marking%20a%20significant%20advancement%20in%20the%20field%0Aof%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphBridge%253A%2520Towards%2520Arbitrary%2520Transfer%2520Learning%2520in%2520GNNs%26entry.906535625%3DLi%2520Ju%2520and%2520Xingyi%2520Yang%2520and%2520Qi%2520Li%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520conventionally%2520trained%2520on%2520a%2520per-domain%252C%250Aper-task%2520basis.%2520It%2520creates%2520a%2520significant%2520barrier%2520in%2520transferring%2520the%2520acquired%250Aknowledge%2520to%2520different%252C%2520heterogeneous%2520data%2520setups.%2520This%2520paper%2520introduces%250AGraphBridge%252C%2520a%2520novel%2520framework%2520to%2520enable%2520knowledge%2520transfer%2520across%2520disparate%250Atasks%2520and%2520domains%2520in%2520GNNs%252C%2520circumventing%2520the%2520need%2520for%2520modifications%2520to%2520task%250Aconfigurations%2520or%2520graph%2520structures.%2520Specifically%252C%2520GraphBridge%2520allows%2520for%2520the%250Aaugmentation%2520of%2520any%2520pre-trained%2520GNN%2520with%2520prediction%2520heads%2520and%2520a%2520bridging%250Anetwork%2520that%2520connects%2520the%2520input%2520to%2520the%2520output%2520layer.%2520This%2520architecture%2520not%2520only%250Apreserves%2520the%2520intrinsic%2520knowledge%2520of%2520the%2520original%2520model%2520but%2520also%2520supports%250Aoutputs%2520of%2520arbitrary%2520dimensions.%2520To%2520mitigate%2520the%2520negative%2520transfer%2520problem%252C%250AGraphBridg%2520merges%2520the%2520source%2520model%2520with%2520a%2520concurrently%2520trained%2520model%252C%2520thereby%250Areducing%2520the%2520source%2520bias%2520when%2520applied%2520to%2520the%2520target%2520domain.%2520Our%2520method%2520is%250Athoroughly%2520evaluated%2520across%2520diverse%2520transfer%2520learning%2520scenarios%252C%2520including%250AGraph2Graph%252C%2520Node2Node%252C%2520Graph2Node%252C%2520and%2520graph2point-cloud.%2520Empirical%250Avalidation%252C%2520conducted%2520over%252016%2520datasets%2520representative%2520of%2520these%2520scenarios%252C%250Aconfirms%2520the%2520framework%2527s%2520capacity%2520for%2520task-%2520and%2520domain-agnostic%2520transfer%250Alearning%2520within%2520graph-like%2520data%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520the%2520field%250Aof%2520GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphBridge%3A%20Towards%20Arbitrary%20Transfer%20Learning%20in%20GNNs&entry.906535625=Li%20Ju%20and%20Xingyi%20Yang%20and%20Qi%20Li%20and%20Xinchao%20Wang&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20conventionally%20trained%20on%20a%20per-domain%2C%0Aper-task%20basis.%20It%20creates%20a%20significant%20barrier%20in%20transferring%20the%20acquired%0Aknowledge%20to%20different%2C%20heterogeneous%20data%20setups.%20This%20paper%20introduces%0AGraphBridge%2C%20a%20novel%20framework%20to%20enable%20knowledge%20transfer%20across%20disparate%0Atasks%20and%20domains%20in%20GNNs%2C%20circumventing%20the%20need%20for%20modifications%20to%20task%0Aconfigurations%20or%20graph%20structures.%20Specifically%2C%20GraphBridge%20allows%20for%20the%0Aaugmentation%20of%20any%20pre-trained%20GNN%20with%20prediction%20heads%20and%20a%20bridging%0Anetwork%20that%20connects%20the%20input%20to%20the%20output%20layer.%20This%20architecture%20not%20only%0Apreserves%20the%20intrinsic%20knowledge%20of%20the%20original%20model%20but%20also%20supports%0Aoutputs%20of%20arbitrary%20dimensions.%20To%20mitigate%20the%20negative%20transfer%20problem%2C%0AGraphBridg%20merges%20the%20source%20model%20with%20a%20concurrently%20trained%20model%2C%20thereby%0Areducing%20the%20source%20bias%20when%20applied%20to%20the%20target%20domain.%20Our%20method%20is%0Athoroughly%20evaluated%20across%20diverse%20transfer%20learning%20scenarios%2C%20including%0AGraph2Graph%2C%20Node2Node%2C%20Graph2Node%2C%20and%20graph2point-cloud.%20Empirical%0Avalidation%2C%20conducted%20over%2016%20datasets%20representative%20of%20these%20scenarios%2C%0Aconfirms%20the%20framework%27s%20capacity%20for%20task-%20and%20domain-agnostic%20transfer%0Alearning%20within%20graph-like%20data%2C%20marking%20a%20significant%20advancement%20in%20the%20field%0Aof%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19252v1&entry.124074799=Read"},
{"title": "Learning Decentralized Swarms Using Rotation Equivariant Graph Neural\n  Networks", "author": "Taos Transue and Bao Wang", "abstract": "  The orchestration of agents to optimize a collective objective without\ncentralized control is challenging yet crucial for applications such as\ncontrolling autonomous fleets, and surveillance and reconnaissance using sensor\nnetworks. Decentralized controller design has been inspired by\nself-organization found in nature, with a prominent source of inspiration being\nflocking; however, decentralized controllers struggle to maintain flock\ncohesion. The graph neural network (GNN) architecture has emerged as an\nindispensable machine learning tool for developing decentralized controllers\ncapable of maintaining flock cohesion, but they fail to exploit the symmetries\npresent in flocking dynamics, hindering their generalizability. We enforce\nrotation equivariance and translation invariance symmetries in decentralized\nflocking GNN controllers and achieve comparable flocking control with 70% less\ntraining data and 75% fewer trainable weights than existing GNN controllers\nwithout these symmetries enforced. We also show that our symmetry-aware\ncontroller generalizes better than existing GNN controllers. Code and\nanimations are available at\nhttp://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.\n", "link": "http://arxiv.org/abs/2502.17612v2", "date": "2025-02-26", "relevancy": 2.4314, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5191}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4709}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Decentralized%20Swarms%20Using%20Rotation%20Equivariant%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20Learning%20Decentralized%20Swarms%20Using%20Rotation%20Equivariant%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Taos%20Transue%20and%20Bao%20Wang%0AAbstract%3A%20%20%20The%20orchestration%20of%20agents%20to%20optimize%20a%20collective%20objective%20without%0Acentralized%20control%20is%20challenging%20yet%20crucial%20for%20applications%20such%20as%0Acontrolling%20autonomous%20fleets%2C%20and%20surveillance%20and%20reconnaissance%20using%20sensor%0Anetworks.%20Decentralized%20controller%20design%20has%20been%20inspired%20by%0Aself-organization%20found%20in%20nature%2C%20with%20a%20prominent%20source%20of%20inspiration%20being%0Aflocking%3B%20however%2C%20decentralized%20controllers%20struggle%20to%20maintain%20flock%0Acohesion.%20The%20graph%20neural%20network%20%28GNN%29%20architecture%20has%20emerged%20as%20an%0Aindispensable%20machine%20learning%20tool%20for%20developing%20decentralized%20controllers%0Acapable%20of%20maintaining%20flock%20cohesion%2C%20but%20they%20fail%20to%20exploit%20the%20symmetries%0Apresent%20in%20flocking%20dynamics%2C%20hindering%20their%20generalizability.%20We%20enforce%0Arotation%20equivariance%20and%20translation%20invariance%20symmetries%20in%20decentralized%0Aflocking%20GNN%20controllers%20and%20achieve%20comparable%20flocking%20control%20with%2070%25%20less%0Atraining%20data%20and%2075%25%20fewer%20trainable%20weights%20than%20existing%20GNN%20controllers%0Awithout%20these%20symmetries%20enforced.%20We%20also%20show%20that%20our%20symmetry-aware%0Acontroller%20generalizes%20better%20than%20existing%20GNN%20controllers.%20Code%20and%0Aanimations%20are%20available%20at%0Ahttp%3A//github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Decentralized%2520Swarms%2520Using%2520Rotation%2520Equivariant%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DTaos%2520Transue%2520and%2520Bao%2520Wang%26entry.1292438233%3D%2520%2520The%2520orchestration%2520of%2520agents%2520to%2520optimize%2520a%2520collective%2520objective%2520without%250Acentralized%2520control%2520is%2520challenging%2520yet%2520crucial%2520for%2520applications%2520such%2520as%250Acontrolling%2520autonomous%2520fleets%252C%2520and%2520surveillance%2520and%2520reconnaissance%2520using%2520sensor%250Anetworks.%2520Decentralized%2520controller%2520design%2520has%2520been%2520inspired%2520by%250Aself-organization%2520found%2520in%2520nature%252C%2520with%2520a%2520prominent%2520source%2520of%2520inspiration%2520being%250Aflocking%253B%2520however%252C%2520decentralized%2520controllers%2520struggle%2520to%2520maintain%2520flock%250Acohesion.%2520The%2520graph%2520neural%2520network%2520%2528GNN%2529%2520architecture%2520has%2520emerged%2520as%2520an%250Aindispensable%2520machine%2520learning%2520tool%2520for%2520developing%2520decentralized%2520controllers%250Acapable%2520of%2520maintaining%2520flock%2520cohesion%252C%2520but%2520they%2520fail%2520to%2520exploit%2520the%2520symmetries%250Apresent%2520in%2520flocking%2520dynamics%252C%2520hindering%2520their%2520generalizability.%2520We%2520enforce%250Arotation%2520equivariance%2520and%2520translation%2520invariance%2520symmetries%2520in%2520decentralized%250Aflocking%2520GNN%2520controllers%2520and%2520achieve%2520comparable%2520flocking%2520control%2520with%252070%2525%2520less%250Atraining%2520data%2520and%252075%2525%2520fewer%2520trainable%2520weights%2520than%2520existing%2520GNN%2520controllers%250Awithout%2520these%2520symmetries%2520enforced.%2520We%2520also%2520show%2520that%2520our%2520symmetry-aware%250Acontroller%2520generalizes%2520better%2520than%2520existing%2520GNN%2520controllers.%2520Code%2520and%250Aanimations%2520are%2520available%2520at%250Ahttp%253A//github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Decentralized%20Swarms%20Using%20Rotation%20Equivariant%20Graph%20Neural%0A%20%20Networks&entry.906535625=Taos%20Transue%20and%20Bao%20Wang&entry.1292438233=%20%20The%20orchestration%20of%20agents%20to%20optimize%20a%20collective%20objective%20without%0Acentralized%20control%20is%20challenging%20yet%20crucial%20for%20applications%20such%20as%0Acontrolling%20autonomous%20fleets%2C%20and%20surveillance%20and%20reconnaissance%20using%20sensor%0Anetworks.%20Decentralized%20controller%20design%20has%20been%20inspired%20by%0Aself-organization%20found%20in%20nature%2C%20with%20a%20prominent%20source%20of%20inspiration%20being%0Aflocking%3B%20however%2C%20decentralized%20controllers%20struggle%20to%20maintain%20flock%0Acohesion.%20The%20graph%20neural%20network%20%28GNN%29%20architecture%20has%20emerged%20as%20an%0Aindispensable%20machine%20learning%20tool%20for%20developing%20decentralized%20controllers%0Acapable%20of%20maintaining%20flock%20cohesion%2C%20but%20they%20fail%20to%20exploit%20the%20symmetries%0Apresent%20in%20flocking%20dynamics%2C%20hindering%20their%20generalizability.%20We%20enforce%0Arotation%20equivariance%20and%20translation%20invariance%20symmetries%20in%20decentralized%0Aflocking%20GNN%20controllers%20and%20achieve%20comparable%20flocking%20control%20with%2070%25%20less%0Atraining%20data%20and%2075%25%20fewer%20trainable%20weights%20than%20existing%20GNN%20controllers%0Awithout%20these%20symmetries%20enforced.%20We%20also%20show%20that%20our%20symmetry-aware%0Acontroller%20generalizes%20better%20than%20existing%20GNN%20controllers.%20Code%20and%0Aanimations%20are%20available%20at%0Ahttp%3A//github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17612v2&entry.124074799=Read"},
{"title": "Text-driven 3D Human Generation via Contrastive Preference Optimization", "author": "Pengfei Zhou and Xukun Shen and Yong Hu", "abstract": "  Recent advances in Score Distillation Sampling (SDS) have improved 3D human\ngeneration from textual descriptions. However, existing methods still face\nchallenges in accurately aligning 3D models with long and complex textual\ninputs. To address this challenge, we propose a novel framework that introduces\ncontrastive preferences, where human-level preference models, guided by both\npositive and negative prompts, assist SDS for improved alignment. Specifically,\nwe design a preference optimization module that integrates multiple models to\ncomprehensively capture the full range of textual features. Furthermore, we\nintroduce a negation preference module to mitigate over-optimization of\nirrelevant details by leveraging static-dynamic negation prompts, effectively\npreventing ``reward hacking\". Extensive experiments demonstrate that our method\nachieves state-of-the-art results, significantly enhancing texture realism and\nvisual alignment with textual descriptions, particularly for long and complex\ninputs.\n", "link": "http://arxiv.org/abs/2502.08977v2", "date": "2025-02-26", "relevancy": 2.4288, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.613}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6126}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-driven%203D%20Human%20Generation%20via%20Contrastive%20Preference%20Optimization&body=Title%3A%20Text-driven%203D%20Human%20Generation%20via%20Contrastive%20Preference%20Optimization%0AAuthor%3A%20Pengfei%20Zhou%20and%20Xukun%20Shen%20and%20Yong%20Hu%0AAbstract%3A%20%20%20Recent%20advances%20in%20Score%20Distillation%20Sampling%20%28SDS%29%20have%20improved%203D%20human%0Ageneration%20from%20textual%20descriptions.%20However%2C%20existing%20methods%20still%20face%0Achallenges%20in%20accurately%20aligning%203D%20models%20with%20long%20and%20complex%20textual%0Ainputs.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%20that%20introduces%0Acontrastive%20preferences%2C%20where%20human-level%20preference%20models%2C%20guided%20by%20both%0Apositive%20and%20negative%20prompts%2C%20assist%20SDS%20for%20improved%20alignment.%20Specifically%2C%0Awe%20design%20a%20preference%20optimization%20module%20that%20integrates%20multiple%20models%20to%0Acomprehensively%20capture%20the%20full%20range%20of%20textual%20features.%20Furthermore%2C%20we%0Aintroduce%20a%20negation%20preference%20module%20to%20mitigate%20over-optimization%20of%0Airrelevant%20details%20by%20leveraging%20static-dynamic%20negation%20prompts%2C%20effectively%0Apreventing%20%60%60reward%20hacking%22.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aachieves%20state-of-the-art%20results%2C%20significantly%20enhancing%20texture%20realism%20and%0Avisual%20alignment%20with%20textual%20descriptions%2C%20particularly%20for%20long%20and%20complex%0Ainputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-driven%25203D%2520Human%2520Generation%2520via%2520Contrastive%2520Preference%2520Optimization%26entry.906535625%3DPengfei%2520Zhou%2520and%2520Xukun%2520Shen%2520and%2520Yong%2520Hu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520have%2520improved%25203D%2520human%250Ageneration%2520from%2520textual%2520descriptions.%2520However%252C%2520existing%2520methods%2520still%2520face%250Achallenges%2520in%2520accurately%2520aligning%25203D%2520models%2520with%2520long%2520and%2520complex%2520textual%250Ainputs.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520introduces%250Acontrastive%2520preferences%252C%2520where%2520human-level%2520preference%2520models%252C%2520guided%2520by%2520both%250Apositive%2520and%2520negative%2520prompts%252C%2520assist%2520SDS%2520for%2520improved%2520alignment.%2520Specifically%252C%250Awe%2520design%2520a%2520preference%2520optimization%2520module%2520that%2520integrates%2520multiple%2520models%2520to%250Acomprehensively%2520capture%2520the%2520full%2520range%2520of%2520textual%2520features.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520negation%2520preference%2520module%2520to%2520mitigate%2520over-optimization%2520of%250Airrelevant%2520details%2520by%2520leveraging%2520static-dynamic%2520negation%2520prompts%252C%2520effectively%250Apreventing%2520%2560%2560reward%2520hacking%2522.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520state-of-the-art%2520results%252C%2520significantly%2520enhancing%2520texture%2520realism%2520and%250Avisual%2520alignment%2520with%2520textual%2520descriptions%252C%2520particularly%2520for%2520long%2520and%2520complex%250Ainputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-driven%203D%20Human%20Generation%20via%20Contrastive%20Preference%20Optimization&entry.906535625=Pengfei%20Zhou%20and%20Xukun%20Shen%20and%20Yong%20Hu&entry.1292438233=%20%20Recent%20advances%20in%20Score%20Distillation%20Sampling%20%28SDS%29%20have%20improved%203D%20human%0Ageneration%20from%20textual%20descriptions.%20However%2C%20existing%20methods%20still%20face%0Achallenges%20in%20accurately%20aligning%203D%20models%20with%20long%20and%20complex%20textual%0Ainputs.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%20that%20introduces%0Acontrastive%20preferences%2C%20where%20human-level%20preference%20models%2C%20guided%20by%20both%0Apositive%20and%20negative%20prompts%2C%20assist%20SDS%20for%20improved%20alignment.%20Specifically%2C%0Awe%20design%20a%20preference%20optimization%20module%20that%20integrates%20multiple%20models%20to%0Acomprehensively%20capture%20the%20full%20range%20of%20textual%20features.%20Furthermore%2C%20we%0Aintroduce%20a%20negation%20preference%20module%20to%20mitigate%20over-optimization%20of%0Airrelevant%20details%20by%20leveraging%20static-dynamic%20negation%20prompts%2C%20effectively%0Apreventing%20%60%60reward%20hacking%22.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aachieves%20state-of-the-art%20results%2C%20significantly%20enhancing%20texture%20realism%20and%0Avisual%20alignment%20with%20textual%20descriptions%2C%20particularly%20for%20long%20and%20complex%0Ainputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08977v2&entry.124074799=Read"},
{"title": "From Traditional to Deep Learning Approaches in Whole Slide Image\n  Registration: A Methodological Review", "author": "Behnaz Elhaminia and Abdullah Alsalemi and Esha Nasir and Mostafa Jahanifar and Ruqayya Awan and Lawrence S. Young and Nasir M. Rajpoot and Fayyaz Minhas and Shan E Ahmed Raza", "abstract": "  Whole slide image (WSI) registration is an essential task for analysing the\ntumour microenvironment (TME) in histopathology. It involves the alignment of\nspatial information between WSIs of the same section or serial sections of a\ntissue sample. The tissue sections are usually stained with single or multiple\nbiomarkers before imaging, and the goal is to identify neighbouring nuclei\nalong the Z-axis for creating a 3D image or identifying subclasses of cells in\nthe TME. This task is considerably more challenging compared to radiology image\nregistration, such as magnetic resonance imaging or computed tomography, due to\nvarious factors. These include gigapixel size of images, variations in\nappearance between differently stained tissues, changes in structure and\nmorphology between non-consecutive sections, and the presence of artefacts,\ntears, and deformations. Currently, there is a noticeable gap in the literature\nregarding a review of the current approaches and their limitations, as well as\nthe challenges and opportunities they present. We aim to provide a\ncomprehensive understanding of the available approaches and their application\nfor various purposes. Furthermore, we investigate current deep learning methods\nused for WSI registration, emphasising their diverse methodologies. We examine\nthe available datasets and explore tools and software employed in the field.\nFinally, we identify open challenges and potential future trends in this area\nof research.\n", "link": "http://arxiv.org/abs/2502.19123v1", "date": "2025-02-26", "relevancy": 2.4237, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4885}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.487}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Traditional%20to%20Deep%20Learning%20Approaches%20in%20Whole%20Slide%20Image%0A%20%20Registration%3A%20A%20Methodological%20Review&body=Title%3A%20From%20Traditional%20to%20Deep%20Learning%20Approaches%20in%20Whole%20Slide%20Image%0A%20%20Registration%3A%20A%20Methodological%20Review%0AAuthor%3A%20Behnaz%20Elhaminia%20and%20Abdullah%20Alsalemi%20and%20Esha%20Nasir%20and%20Mostafa%20Jahanifar%20and%20Ruqayya%20Awan%20and%20Lawrence%20S.%20Young%20and%20Nasir%20M.%20Rajpoot%20and%20Fayyaz%20Minhas%20and%20Shan%20E%20Ahmed%20Raza%0AAbstract%3A%20%20%20Whole%20slide%20image%20%28WSI%29%20registration%20is%20an%20essential%20task%20for%20analysing%20the%0Atumour%20microenvironment%20%28TME%29%20in%20histopathology.%20It%20involves%20the%20alignment%20of%0Aspatial%20information%20between%20WSIs%20of%20the%20same%20section%20or%20serial%20sections%20of%20a%0Atissue%20sample.%20The%20tissue%20sections%20are%20usually%20stained%20with%20single%20or%20multiple%0Abiomarkers%20before%20imaging%2C%20and%20the%20goal%20is%20to%20identify%20neighbouring%20nuclei%0Aalong%20the%20Z-axis%20for%20creating%20a%203D%20image%20or%20identifying%20subclasses%20of%20cells%20in%0Athe%20TME.%20This%20task%20is%20considerably%20more%20challenging%20compared%20to%20radiology%20image%0Aregistration%2C%20such%20as%20magnetic%20resonance%20imaging%20or%20computed%20tomography%2C%20due%20to%0Avarious%20factors.%20These%20include%20gigapixel%20size%20of%20images%2C%20variations%20in%0Aappearance%20between%20differently%20stained%20tissues%2C%20changes%20in%20structure%20and%0Amorphology%20between%20non-consecutive%20sections%2C%20and%20the%20presence%20of%20artefacts%2C%0Atears%2C%20and%20deformations.%20Currently%2C%20there%20is%20a%20noticeable%20gap%20in%20the%20literature%0Aregarding%20a%20review%20of%20the%20current%20approaches%20and%20their%20limitations%2C%20as%20well%20as%0Athe%20challenges%20and%20opportunities%20they%20present.%20We%20aim%20to%20provide%20a%0Acomprehensive%20understanding%20of%20the%20available%20approaches%20and%20their%20application%0Afor%20various%20purposes.%20Furthermore%2C%20we%20investigate%20current%20deep%20learning%20methods%0Aused%20for%20WSI%20registration%2C%20emphasising%20their%20diverse%20methodologies.%20We%20examine%0Athe%20available%20datasets%20and%20explore%20tools%20and%20software%20employed%20in%20the%20field.%0AFinally%2C%20we%20identify%20open%20challenges%20and%20potential%20future%20trends%20in%20this%20area%0Aof%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Traditional%2520to%2520Deep%2520Learning%2520Approaches%2520in%2520Whole%2520Slide%2520Image%250A%2520%2520Registration%253A%2520A%2520Methodological%2520Review%26entry.906535625%3DBehnaz%2520Elhaminia%2520and%2520Abdullah%2520Alsalemi%2520and%2520Esha%2520Nasir%2520and%2520Mostafa%2520Jahanifar%2520and%2520Ruqayya%2520Awan%2520and%2520Lawrence%2520S.%2520Young%2520and%2520Nasir%2520M.%2520Rajpoot%2520and%2520Fayyaz%2520Minhas%2520and%2520Shan%2520E%2520Ahmed%2520Raza%26entry.1292438233%3D%2520%2520Whole%2520slide%2520image%2520%2528WSI%2529%2520registration%2520is%2520an%2520essential%2520task%2520for%2520analysing%2520the%250Atumour%2520microenvironment%2520%2528TME%2529%2520in%2520histopathology.%2520It%2520involves%2520the%2520alignment%2520of%250Aspatial%2520information%2520between%2520WSIs%2520of%2520the%2520same%2520section%2520or%2520serial%2520sections%2520of%2520a%250Atissue%2520sample.%2520The%2520tissue%2520sections%2520are%2520usually%2520stained%2520with%2520single%2520or%2520multiple%250Abiomarkers%2520before%2520imaging%252C%2520and%2520the%2520goal%2520is%2520to%2520identify%2520neighbouring%2520nuclei%250Aalong%2520the%2520Z-axis%2520for%2520creating%2520a%25203D%2520image%2520or%2520identifying%2520subclasses%2520of%2520cells%2520in%250Athe%2520TME.%2520This%2520task%2520is%2520considerably%2520more%2520challenging%2520compared%2520to%2520radiology%2520image%250Aregistration%252C%2520such%2520as%2520magnetic%2520resonance%2520imaging%2520or%2520computed%2520tomography%252C%2520due%2520to%250Avarious%2520factors.%2520These%2520include%2520gigapixel%2520size%2520of%2520images%252C%2520variations%2520in%250Aappearance%2520between%2520differently%2520stained%2520tissues%252C%2520changes%2520in%2520structure%2520and%250Amorphology%2520between%2520non-consecutive%2520sections%252C%2520and%2520the%2520presence%2520of%2520artefacts%252C%250Atears%252C%2520and%2520deformations.%2520Currently%252C%2520there%2520is%2520a%2520noticeable%2520gap%2520in%2520the%2520literature%250Aregarding%2520a%2520review%2520of%2520the%2520current%2520approaches%2520and%2520their%2520limitations%252C%2520as%2520well%2520as%250Athe%2520challenges%2520and%2520opportunities%2520they%2520present.%2520We%2520aim%2520to%2520provide%2520a%250Acomprehensive%2520understanding%2520of%2520the%2520available%2520approaches%2520and%2520their%2520application%250Afor%2520various%2520purposes.%2520Furthermore%252C%2520we%2520investigate%2520current%2520deep%2520learning%2520methods%250Aused%2520for%2520WSI%2520registration%252C%2520emphasising%2520their%2520diverse%2520methodologies.%2520We%2520examine%250Athe%2520available%2520datasets%2520and%2520explore%2520tools%2520and%2520software%2520employed%2520in%2520the%2520field.%250AFinally%252C%2520we%2520identify%2520open%2520challenges%2520and%2520potential%2520future%2520trends%2520in%2520this%2520area%250Aof%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Traditional%20to%20Deep%20Learning%20Approaches%20in%20Whole%20Slide%20Image%0A%20%20Registration%3A%20A%20Methodological%20Review&entry.906535625=Behnaz%20Elhaminia%20and%20Abdullah%20Alsalemi%20and%20Esha%20Nasir%20and%20Mostafa%20Jahanifar%20and%20Ruqayya%20Awan%20and%20Lawrence%20S.%20Young%20and%20Nasir%20M.%20Rajpoot%20and%20Fayyaz%20Minhas%20and%20Shan%20E%20Ahmed%20Raza&entry.1292438233=%20%20Whole%20slide%20image%20%28WSI%29%20registration%20is%20an%20essential%20task%20for%20analysing%20the%0Atumour%20microenvironment%20%28TME%29%20in%20histopathology.%20It%20involves%20the%20alignment%20of%0Aspatial%20information%20between%20WSIs%20of%20the%20same%20section%20or%20serial%20sections%20of%20a%0Atissue%20sample.%20The%20tissue%20sections%20are%20usually%20stained%20with%20single%20or%20multiple%0Abiomarkers%20before%20imaging%2C%20and%20the%20goal%20is%20to%20identify%20neighbouring%20nuclei%0Aalong%20the%20Z-axis%20for%20creating%20a%203D%20image%20or%20identifying%20subclasses%20of%20cells%20in%0Athe%20TME.%20This%20task%20is%20considerably%20more%20challenging%20compared%20to%20radiology%20image%0Aregistration%2C%20such%20as%20magnetic%20resonance%20imaging%20or%20computed%20tomography%2C%20due%20to%0Avarious%20factors.%20These%20include%20gigapixel%20size%20of%20images%2C%20variations%20in%0Aappearance%20between%20differently%20stained%20tissues%2C%20changes%20in%20structure%20and%0Amorphology%20between%20non-consecutive%20sections%2C%20and%20the%20presence%20of%20artefacts%2C%0Atears%2C%20and%20deformations.%20Currently%2C%20there%20is%20a%20noticeable%20gap%20in%20the%20literature%0Aregarding%20a%20review%20of%20the%20current%20approaches%20and%20their%20limitations%2C%20as%20well%20as%0Athe%20challenges%20and%20opportunities%20they%20present.%20We%20aim%20to%20provide%20a%0Acomprehensive%20understanding%20of%20the%20available%20approaches%20and%20their%20application%0Afor%20various%20purposes.%20Furthermore%2C%20we%20investigate%20current%20deep%20learning%20methods%0Aused%20for%20WSI%20registration%2C%20emphasising%20their%20diverse%20methodologies.%20We%20examine%0Athe%20available%20datasets%20and%20explore%20tools%20and%20software%20employed%20in%20the%20field.%0AFinally%2C%20we%20identify%20open%20challenges%20and%20potential%20future%20trends%20in%20this%20area%0Aof%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19123v1&entry.124074799=Read"},
{"title": "Binary Neural Networks for Large Language Model: A Survey", "author": "Liangdong Liu and Zhitong Zheng and Cong Wang and Tianhuang Su and Zhenyu Yang", "abstract": "  Large language models (LLMs) have wide applications in the field of natural\nlanguage processing(NLP), such as GPT-4 and Llama. However, with the\nexponential growth of model parameter sizes, LLMs bring significant resource\noverheads. Low-bit quantization, as a key technique, reduces memory usage and\ncomputational demands by decreasing the bit-width of model parameters,\nactivations, and gradients. Previous quantization methods for LLMs have largely\nemployed Post-Training Quantization (PTQ) and Quantization-Aware Training\n(QAT). PTQ does not require any retraining of the original model, while QAT\ninvolves optimizing precision during training to achieve the best quantization\nparameters. The BitNet team proposed a radically different approach, where\nquantization is performed from the start of model training, utilizing\nlow-precision binary weights during the training process. This approach has led\nto the emergence of many binary quantization techniques for large language\nmodels. This paper provides a comprehensive review of these binary quantization\ntechniques. Specifically, we will introduce binary quantization techniques in\ndeep neural networks and further explore their application to LLMs, reviewing\ntheir various contributions, implementations, and applications.\n", "link": "http://arxiv.org/abs/2502.19008v1", "date": "2025-02-26", "relevancy": 2.4138, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Binary%20Neural%20Networks%20for%20Large%20Language%20Model%3A%20A%20Survey&body=Title%3A%20Binary%20Neural%20Networks%20for%20Large%20Language%20Model%3A%20A%20Survey%0AAuthor%3A%20Liangdong%20Liu%20and%20Zhitong%20Zheng%20and%20Cong%20Wang%20and%20Tianhuang%20Su%20and%20Zhenyu%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20wide%20applications%20in%20the%20field%20of%20natural%0Alanguage%20processing%28NLP%29%2C%20such%20as%20GPT-4%20and%20Llama.%20However%2C%20with%20the%0Aexponential%20growth%20of%20model%20parameter%20sizes%2C%20LLMs%20bring%20significant%20resource%0Aoverheads.%20Low-bit%20quantization%2C%20as%20a%20key%20technique%2C%20reduces%20memory%20usage%20and%0Acomputational%20demands%20by%20decreasing%20the%20bit-width%20of%20model%20parameters%2C%0Aactivations%2C%20and%20gradients.%20Previous%20quantization%20methods%20for%20LLMs%20have%20largely%0Aemployed%20Post-Training%20Quantization%20%28PTQ%29%20and%20Quantization-Aware%20Training%0A%28QAT%29.%20PTQ%20does%20not%20require%20any%20retraining%20of%20the%20original%20model%2C%20while%20QAT%0Ainvolves%20optimizing%20precision%20during%20training%20to%20achieve%20the%20best%20quantization%0Aparameters.%20The%20BitNet%20team%20proposed%20a%20radically%20different%20approach%2C%20where%0Aquantization%20is%20performed%20from%20the%20start%20of%20model%20training%2C%20utilizing%0Alow-precision%20binary%20weights%20during%20the%20training%20process.%20This%20approach%20has%20led%0Ato%20the%20emergence%20of%20many%20binary%20quantization%20techniques%20for%20large%20language%0Amodels.%20This%20paper%20provides%20a%20comprehensive%20review%20of%20these%20binary%20quantization%0Atechniques.%20Specifically%2C%20we%20will%20introduce%20binary%20quantization%20techniques%20in%0Adeep%20neural%20networks%20and%20further%20explore%20their%20application%20to%20LLMs%2C%20reviewing%0Atheir%20various%20contributions%2C%20implementations%2C%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBinary%2520Neural%2520Networks%2520for%2520Large%2520Language%2520Model%253A%2520A%2520Survey%26entry.906535625%3DLiangdong%2520Liu%2520and%2520Zhitong%2520Zheng%2520and%2520Cong%2520Wang%2520and%2520Tianhuang%2520Su%2520and%2520Zhenyu%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520wide%2520applications%2520in%2520the%2520field%2520of%2520natural%250Alanguage%2520processing%2528NLP%2529%252C%2520such%2520as%2520GPT-4%2520and%2520Llama.%2520However%252C%2520with%2520the%250Aexponential%2520growth%2520of%2520model%2520parameter%2520sizes%252C%2520LLMs%2520bring%2520significant%2520resource%250Aoverheads.%2520Low-bit%2520quantization%252C%2520as%2520a%2520key%2520technique%252C%2520reduces%2520memory%2520usage%2520and%250Acomputational%2520demands%2520by%2520decreasing%2520the%2520bit-width%2520of%2520model%2520parameters%252C%250Aactivations%252C%2520and%2520gradients.%2520Previous%2520quantization%2520methods%2520for%2520LLMs%2520have%2520largely%250Aemployed%2520Post-Training%2520Quantization%2520%2528PTQ%2529%2520and%2520Quantization-Aware%2520Training%250A%2528QAT%2529.%2520PTQ%2520does%2520not%2520require%2520any%2520retraining%2520of%2520the%2520original%2520model%252C%2520while%2520QAT%250Ainvolves%2520optimizing%2520precision%2520during%2520training%2520to%2520achieve%2520the%2520best%2520quantization%250Aparameters.%2520The%2520BitNet%2520team%2520proposed%2520a%2520radically%2520different%2520approach%252C%2520where%250Aquantization%2520is%2520performed%2520from%2520the%2520start%2520of%2520model%2520training%252C%2520utilizing%250Alow-precision%2520binary%2520weights%2520during%2520the%2520training%2520process.%2520This%2520approach%2520has%2520led%250Ato%2520the%2520emergence%2520of%2520many%2520binary%2520quantization%2520techniques%2520for%2520large%2520language%250Amodels.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520review%2520of%2520these%2520binary%2520quantization%250Atechniques.%2520Specifically%252C%2520we%2520will%2520introduce%2520binary%2520quantization%2520techniques%2520in%250Adeep%2520neural%2520networks%2520and%2520further%2520explore%2520their%2520application%2520to%2520LLMs%252C%2520reviewing%250Atheir%2520various%2520contributions%252C%2520implementations%252C%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Binary%20Neural%20Networks%20for%20Large%20Language%20Model%3A%20A%20Survey&entry.906535625=Liangdong%20Liu%20and%20Zhitong%20Zheng%20and%20Cong%20Wang%20and%20Tianhuang%20Su%20and%20Zhenyu%20Yang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20wide%20applications%20in%20the%20field%20of%20natural%0Alanguage%20processing%28NLP%29%2C%20such%20as%20GPT-4%20and%20Llama.%20However%2C%20with%20the%0Aexponential%20growth%20of%20model%20parameter%20sizes%2C%20LLMs%20bring%20significant%20resource%0Aoverheads.%20Low-bit%20quantization%2C%20as%20a%20key%20technique%2C%20reduces%20memory%20usage%20and%0Acomputational%20demands%20by%20decreasing%20the%20bit-width%20of%20model%20parameters%2C%0Aactivations%2C%20and%20gradients.%20Previous%20quantization%20methods%20for%20LLMs%20have%20largely%0Aemployed%20Post-Training%20Quantization%20%28PTQ%29%20and%20Quantization-Aware%20Training%0A%28QAT%29.%20PTQ%20does%20not%20require%20any%20retraining%20of%20the%20original%20model%2C%20while%20QAT%0Ainvolves%20optimizing%20precision%20during%20training%20to%20achieve%20the%20best%20quantization%0Aparameters.%20The%20BitNet%20team%20proposed%20a%20radically%20different%20approach%2C%20where%0Aquantization%20is%20performed%20from%20the%20start%20of%20model%20training%2C%20utilizing%0Alow-precision%20binary%20weights%20during%20the%20training%20process.%20This%20approach%20has%20led%0Ato%20the%20emergence%20of%20many%20binary%20quantization%20techniques%20for%20large%20language%0Amodels.%20This%20paper%20provides%20a%20comprehensive%20review%20of%20these%20binary%20quantization%0Atechniques.%20Specifically%2C%20we%20will%20introduce%20binary%20quantization%20techniques%20in%0Adeep%20neural%20networks%20and%20further%20explore%20their%20application%20to%20LLMs%2C%20reviewing%0Atheir%20various%20contributions%2C%20implementations%2C%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19008v1&entry.124074799=Read"},
{"title": "A Model-Centric Review of Deep Learning for Protein Design", "author": "Gregory W. Kyro and Tianyin Qiu and Victor S. Batista", "abstract": "  Deep learning has transformed protein design, enabling accurate structure\nprediction, sequence optimization, and de novo protein generation. Advances in\nsingle-chain protein structure prediction via AlphaFold2, RoseTTAFold, ESMFold,\nand others have achieved near-experimental accuracy, inspiring successive work\nextended to biomolecular complexes via AlphaFold Multimer, RoseTTAFold\nAll-Atom, AlphaFold 3, Chai-1, Boltz-1 and others. Generative models such as\nProtGPT2, ProteinMPNN, and RFdiffusion have enabled sequence and backbone\ndesign beyond natural evolution-based limitations. More recently, joint\nsequence-structure co-design models, including ESM3, have integrated both\nmodalities into a unified framework, resulting in improved designability.\nDespite these advances, challenges still exist pertaining to modeling\nsequence-structure-function relationships and ensuring robust generalization\nbeyond the regions of protein space spanned by the training data. Future\nadvances will likely focus on joint sequence-structure-function co-design\nframeworks that are able to model the fitness landscape more effectively than\nmodels that treat these modalities independently. Current capabilities, coupled\nwith the dizzying rate of progress, suggest that the field will soon enable\nrapid, rational design of proteins with tailored structures and functions that\ntranscend the limitations imposed by natural evolution. In this review, we\ndiscuss the current capabilities of deep learning methods for protein design,\nfocusing on some of the most revolutionary and capable models with respect to\ntheir functionality and the applications that they enable, leading up to the\ncurrent challenges of the field and the optimal path forward.\n", "link": "http://arxiv.org/abs/2502.19173v1", "date": "2025-02-26", "relevancy": 2.404, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Model-Centric%20Review%20of%20Deep%20Learning%20for%20Protein%20Design&body=Title%3A%20A%20Model-Centric%20Review%20of%20Deep%20Learning%20for%20Protein%20Design%0AAuthor%3A%20Gregory%20W.%20Kyro%20and%20Tianyin%20Qiu%20and%20Victor%20S.%20Batista%0AAbstract%3A%20%20%20Deep%20learning%20has%20transformed%20protein%20design%2C%20enabling%20accurate%20structure%0Aprediction%2C%20sequence%20optimization%2C%20and%20de%20novo%20protein%20generation.%20Advances%20in%0Asingle-chain%20protein%20structure%20prediction%20via%20AlphaFold2%2C%20RoseTTAFold%2C%20ESMFold%2C%0Aand%20others%20have%20achieved%20near-experimental%20accuracy%2C%20inspiring%20successive%20work%0Aextended%20to%20biomolecular%20complexes%20via%20AlphaFold%20Multimer%2C%20RoseTTAFold%0AAll-Atom%2C%20AlphaFold%203%2C%20Chai-1%2C%20Boltz-1%20and%20others.%20Generative%20models%20such%20as%0AProtGPT2%2C%20ProteinMPNN%2C%20and%20RFdiffusion%20have%20enabled%20sequence%20and%20backbone%0Adesign%20beyond%20natural%20evolution-based%20limitations.%20More%20recently%2C%20joint%0Asequence-structure%20co-design%20models%2C%20including%20ESM3%2C%20have%20integrated%20both%0Amodalities%20into%20a%20unified%20framework%2C%20resulting%20in%20improved%20designability.%0ADespite%20these%20advances%2C%20challenges%20still%20exist%20pertaining%20to%20modeling%0Asequence-structure-function%20relationships%20and%20ensuring%20robust%20generalization%0Abeyond%20the%20regions%20of%20protein%20space%20spanned%20by%20the%20training%20data.%20Future%0Aadvances%20will%20likely%20focus%20on%20joint%20sequence-structure-function%20co-design%0Aframeworks%20that%20are%20able%20to%20model%20the%20fitness%20landscape%20more%20effectively%20than%0Amodels%20that%20treat%20these%20modalities%20independently.%20Current%20capabilities%2C%20coupled%0Awith%20the%20dizzying%20rate%20of%20progress%2C%20suggest%20that%20the%20field%20will%20soon%20enable%0Arapid%2C%20rational%20design%20of%20proteins%20with%20tailored%20structures%20and%20functions%20that%0Atranscend%20the%20limitations%20imposed%20by%20natural%20evolution.%20In%20this%20review%2C%20we%0Adiscuss%20the%20current%20capabilities%20of%20deep%20learning%20methods%20for%20protein%20design%2C%0Afocusing%20on%20some%20of%20the%20most%20revolutionary%20and%20capable%20models%20with%20respect%20to%0Atheir%20functionality%20and%20the%20applications%20that%20they%20enable%2C%20leading%20up%20to%20the%0Acurrent%20challenges%20of%20the%20field%20and%20the%20optimal%20path%20forward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Model-Centric%2520Review%2520of%2520Deep%2520Learning%2520for%2520Protein%2520Design%26entry.906535625%3DGregory%2520W.%2520Kyro%2520and%2520Tianyin%2520Qiu%2520and%2520Victor%2520S.%2520Batista%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520transformed%2520protein%2520design%252C%2520enabling%2520accurate%2520structure%250Aprediction%252C%2520sequence%2520optimization%252C%2520and%2520de%2520novo%2520protein%2520generation.%2520Advances%2520in%250Asingle-chain%2520protein%2520structure%2520prediction%2520via%2520AlphaFold2%252C%2520RoseTTAFold%252C%2520ESMFold%252C%250Aand%2520others%2520have%2520achieved%2520near-experimental%2520accuracy%252C%2520inspiring%2520successive%2520work%250Aextended%2520to%2520biomolecular%2520complexes%2520via%2520AlphaFold%2520Multimer%252C%2520RoseTTAFold%250AAll-Atom%252C%2520AlphaFold%25203%252C%2520Chai-1%252C%2520Boltz-1%2520and%2520others.%2520Generative%2520models%2520such%2520as%250AProtGPT2%252C%2520ProteinMPNN%252C%2520and%2520RFdiffusion%2520have%2520enabled%2520sequence%2520and%2520backbone%250Adesign%2520beyond%2520natural%2520evolution-based%2520limitations.%2520More%2520recently%252C%2520joint%250Asequence-structure%2520co-design%2520models%252C%2520including%2520ESM3%252C%2520have%2520integrated%2520both%250Amodalities%2520into%2520a%2520unified%2520framework%252C%2520resulting%2520in%2520improved%2520designability.%250ADespite%2520these%2520advances%252C%2520challenges%2520still%2520exist%2520pertaining%2520to%2520modeling%250Asequence-structure-function%2520relationships%2520and%2520ensuring%2520robust%2520generalization%250Abeyond%2520the%2520regions%2520of%2520protein%2520space%2520spanned%2520by%2520the%2520training%2520data.%2520Future%250Aadvances%2520will%2520likely%2520focus%2520on%2520joint%2520sequence-structure-function%2520co-design%250Aframeworks%2520that%2520are%2520able%2520to%2520model%2520the%2520fitness%2520landscape%2520more%2520effectively%2520than%250Amodels%2520that%2520treat%2520these%2520modalities%2520independently.%2520Current%2520capabilities%252C%2520coupled%250Awith%2520the%2520dizzying%2520rate%2520of%2520progress%252C%2520suggest%2520that%2520the%2520field%2520will%2520soon%2520enable%250Arapid%252C%2520rational%2520design%2520of%2520proteins%2520with%2520tailored%2520structures%2520and%2520functions%2520that%250Atranscend%2520the%2520limitations%2520imposed%2520by%2520natural%2520evolution.%2520In%2520this%2520review%252C%2520we%250Adiscuss%2520the%2520current%2520capabilities%2520of%2520deep%2520learning%2520methods%2520for%2520protein%2520design%252C%250Afocusing%2520on%2520some%2520of%2520the%2520most%2520revolutionary%2520and%2520capable%2520models%2520with%2520respect%2520to%250Atheir%2520functionality%2520and%2520the%2520applications%2520that%2520they%2520enable%252C%2520leading%2520up%2520to%2520the%250Acurrent%2520challenges%2520of%2520the%2520field%2520and%2520the%2520optimal%2520path%2520forward.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Model-Centric%20Review%20of%20Deep%20Learning%20for%20Protein%20Design&entry.906535625=Gregory%20W.%20Kyro%20and%20Tianyin%20Qiu%20and%20Victor%20S.%20Batista&entry.1292438233=%20%20Deep%20learning%20has%20transformed%20protein%20design%2C%20enabling%20accurate%20structure%0Aprediction%2C%20sequence%20optimization%2C%20and%20de%20novo%20protein%20generation.%20Advances%20in%0Asingle-chain%20protein%20structure%20prediction%20via%20AlphaFold2%2C%20RoseTTAFold%2C%20ESMFold%2C%0Aand%20others%20have%20achieved%20near-experimental%20accuracy%2C%20inspiring%20successive%20work%0Aextended%20to%20biomolecular%20complexes%20via%20AlphaFold%20Multimer%2C%20RoseTTAFold%0AAll-Atom%2C%20AlphaFold%203%2C%20Chai-1%2C%20Boltz-1%20and%20others.%20Generative%20models%20such%20as%0AProtGPT2%2C%20ProteinMPNN%2C%20and%20RFdiffusion%20have%20enabled%20sequence%20and%20backbone%0Adesign%20beyond%20natural%20evolution-based%20limitations.%20More%20recently%2C%20joint%0Asequence-structure%20co-design%20models%2C%20including%20ESM3%2C%20have%20integrated%20both%0Amodalities%20into%20a%20unified%20framework%2C%20resulting%20in%20improved%20designability.%0ADespite%20these%20advances%2C%20challenges%20still%20exist%20pertaining%20to%20modeling%0Asequence-structure-function%20relationships%20and%20ensuring%20robust%20generalization%0Abeyond%20the%20regions%20of%20protein%20space%20spanned%20by%20the%20training%20data.%20Future%0Aadvances%20will%20likely%20focus%20on%20joint%20sequence-structure-function%20co-design%0Aframeworks%20that%20are%20able%20to%20model%20the%20fitness%20landscape%20more%20effectively%20than%0Amodels%20that%20treat%20these%20modalities%20independently.%20Current%20capabilities%2C%20coupled%0Awith%20the%20dizzying%20rate%20of%20progress%2C%20suggest%20that%20the%20field%20will%20soon%20enable%0Arapid%2C%20rational%20design%20of%20proteins%20with%20tailored%20structures%20and%20functions%20that%0Atranscend%20the%20limitations%20imposed%20by%20natural%20evolution.%20In%20this%20review%2C%20we%0Adiscuss%20the%20current%20capabilities%20of%20deep%20learning%20methods%20for%20protein%20design%2C%0Afocusing%20on%20some%20of%20the%20most%20revolutionary%20and%20capable%20models%20with%20respect%20to%0Atheir%20functionality%20and%20the%20applications%20that%20they%20enable%2C%20leading%20up%20to%20the%0Acurrent%20challenges%20of%20the%20field%20and%20the%20optimal%20path%20forward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19173v1&entry.124074799=Read"},
{"title": "Joint Optimal Transport and Embedding for Network Alignment", "author": "Qi Yu and Zhichen Zeng and Yuchen Yan and Lei Ying and R. Srikant and Hanghang Tong", "abstract": "  Network alignment, which aims to find node correspondence across different\nnetworks, is the cornerstone of various downstream multi-network and Web mining\ntasks. Most of the embedding-based methods indirectly model cross-network node\nrelationships by contrasting positive and negative node pairs sampled from\nhand-crafted strategies, which are vulnerable to graph noises and lead to\npotential misalignment of nodes. Another line of work based on the optimal\ntransport (OT) theory directly models cross-network node relationships and\ngenerates noise-reduced alignments. However, OT methods heavily rely on fixed,\npre-defined cost functions that prohibit end-to-end training and are hard to\ngeneralize. In this paper, we aim to unify the embedding and OT-based methods\nin a mutually beneficial manner and propose a joint optimal transport and\nembedding framework for network alignment named JOENA. For one thing (OT for\nembedding), through a simple yet effective transformation, the noise-reduced OT\nmapping serves as an adaptive sampling strategy directly modeling all\ncross-network node pairs for robust embedding learning.For another (embedding\nfor OT), on top of the learned embeddings, the OT cost can be gradually trained\nin an end-to-end fashion, which further enhances the alignment quality. With a\nunified objective, the mutual benefits of both methods can be achieved by an\nalternating optimization schema with guaranteed convergence. Extensive\nexperiments on real-world networks validate the effectiveness and scalability\nof JOENA, achieving up to 16% improvement in MRR and 20x speedup compared with\nthe state-of-the-art alignment methods.\n", "link": "http://arxiv.org/abs/2502.19334v1", "date": "2025-02-26", "relevancy": 2.3957, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5142}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.467}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Optimal%20Transport%20and%20Embedding%20for%20Network%20Alignment&body=Title%3A%20Joint%20Optimal%20Transport%20and%20Embedding%20for%20Network%20Alignment%0AAuthor%3A%20Qi%20Yu%20and%20Zhichen%20Zeng%20and%20Yuchen%20Yan%20and%20Lei%20Ying%20and%20R.%20Srikant%20and%20Hanghang%20Tong%0AAbstract%3A%20%20%20Network%20alignment%2C%20which%20aims%20to%20find%20node%20correspondence%20across%20different%0Anetworks%2C%20is%20the%20cornerstone%20of%20various%20downstream%20multi-network%20and%20Web%20mining%0Atasks.%20Most%20of%20the%20embedding-based%20methods%20indirectly%20model%20cross-network%20node%0Arelationships%20by%20contrasting%20positive%20and%20negative%20node%20pairs%20sampled%20from%0Ahand-crafted%20strategies%2C%20which%20are%20vulnerable%20to%20graph%20noises%20and%20lead%20to%0Apotential%20misalignment%20of%20nodes.%20Another%20line%20of%20work%20based%20on%20the%20optimal%0Atransport%20%28OT%29%20theory%20directly%20models%20cross-network%20node%20relationships%20and%0Agenerates%20noise-reduced%20alignments.%20However%2C%20OT%20methods%20heavily%20rely%20on%20fixed%2C%0Apre-defined%20cost%20functions%20that%20prohibit%20end-to-end%20training%20and%20are%20hard%20to%0Ageneralize.%20In%20this%20paper%2C%20we%20aim%20to%20unify%20the%20embedding%20and%20OT-based%20methods%0Ain%20a%20mutually%20beneficial%20manner%20and%20propose%20a%20joint%20optimal%20transport%20and%0Aembedding%20framework%20for%20network%20alignment%20named%20JOENA.%20For%20one%20thing%20%28OT%20for%0Aembedding%29%2C%20through%20a%20simple%20yet%20effective%20transformation%2C%20the%20noise-reduced%20OT%0Amapping%20serves%20as%20an%20adaptive%20sampling%20strategy%20directly%20modeling%20all%0Across-network%20node%20pairs%20for%20robust%20embedding%20learning.For%20another%20%28embedding%0Afor%20OT%29%2C%20on%20top%20of%20the%20learned%20embeddings%2C%20the%20OT%20cost%20can%20be%20gradually%20trained%0Ain%20an%20end-to-end%20fashion%2C%20which%20further%20enhances%20the%20alignment%20quality.%20With%20a%0Aunified%20objective%2C%20the%20mutual%20benefits%20of%20both%20methods%20can%20be%20achieved%20by%20an%0Aalternating%20optimization%20schema%20with%20guaranteed%20convergence.%20Extensive%0Aexperiments%20on%20real-world%20networks%20validate%20the%20effectiveness%20and%20scalability%0Aof%20JOENA%2C%20achieving%20up%20to%2016%25%20improvement%20in%20MRR%20and%2020x%20speedup%20compared%20with%0Athe%20state-of-the-art%20alignment%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Optimal%2520Transport%2520and%2520Embedding%2520for%2520Network%2520Alignment%26entry.906535625%3DQi%2520Yu%2520and%2520Zhichen%2520Zeng%2520and%2520Yuchen%2520Yan%2520and%2520Lei%2520Ying%2520and%2520R.%2520Srikant%2520and%2520Hanghang%2520Tong%26entry.1292438233%3D%2520%2520Network%2520alignment%252C%2520which%2520aims%2520to%2520find%2520node%2520correspondence%2520across%2520different%250Anetworks%252C%2520is%2520the%2520cornerstone%2520of%2520various%2520downstream%2520multi-network%2520and%2520Web%2520mining%250Atasks.%2520Most%2520of%2520the%2520embedding-based%2520methods%2520indirectly%2520model%2520cross-network%2520node%250Arelationships%2520by%2520contrasting%2520positive%2520and%2520negative%2520node%2520pairs%2520sampled%2520from%250Ahand-crafted%2520strategies%252C%2520which%2520are%2520vulnerable%2520to%2520graph%2520noises%2520and%2520lead%2520to%250Apotential%2520misalignment%2520of%2520nodes.%2520Another%2520line%2520of%2520work%2520based%2520on%2520the%2520optimal%250Atransport%2520%2528OT%2529%2520theory%2520directly%2520models%2520cross-network%2520node%2520relationships%2520and%250Agenerates%2520noise-reduced%2520alignments.%2520However%252C%2520OT%2520methods%2520heavily%2520rely%2520on%2520fixed%252C%250Apre-defined%2520cost%2520functions%2520that%2520prohibit%2520end-to-end%2520training%2520and%2520are%2520hard%2520to%250Ageneralize.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520unify%2520the%2520embedding%2520and%2520OT-based%2520methods%250Ain%2520a%2520mutually%2520beneficial%2520manner%2520and%2520propose%2520a%2520joint%2520optimal%2520transport%2520and%250Aembedding%2520framework%2520for%2520network%2520alignment%2520named%2520JOENA.%2520For%2520one%2520thing%2520%2528OT%2520for%250Aembedding%2529%252C%2520through%2520a%2520simple%2520yet%2520effective%2520transformation%252C%2520the%2520noise-reduced%2520OT%250Amapping%2520serves%2520as%2520an%2520adaptive%2520sampling%2520strategy%2520directly%2520modeling%2520all%250Across-network%2520node%2520pairs%2520for%2520robust%2520embedding%2520learning.For%2520another%2520%2528embedding%250Afor%2520OT%2529%252C%2520on%2520top%2520of%2520the%2520learned%2520embeddings%252C%2520the%2520OT%2520cost%2520can%2520be%2520gradually%2520trained%250Ain%2520an%2520end-to-end%2520fashion%252C%2520which%2520further%2520enhances%2520the%2520alignment%2520quality.%2520With%2520a%250Aunified%2520objective%252C%2520the%2520mutual%2520benefits%2520of%2520both%2520methods%2520can%2520be%2520achieved%2520by%2520an%250Aalternating%2520optimization%2520schema%2520with%2520guaranteed%2520convergence.%2520Extensive%250Aexperiments%2520on%2520real-world%2520networks%2520validate%2520the%2520effectiveness%2520and%2520scalability%250Aof%2520JOENA%252C%2520achieving%2520up%2520to%252016%2525%2520improvement%2520in%2520MRR%2520and%252020x%2520speedup%2520compared%2520with%250Athe%2520state-of-the-art%2520alignment%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Optimal%20Transport%20and%20Embedding%20for%20Network%20Alignment&entry.906535625=Qi%20Yu%20and%20Zhichen%20Zeng%20and%20Yuchen%20Yan%20and%20Lei%20Ying%20and%20R.%20Srikant%20and%20Hanghang%20Tong&entry.1292438233=%20%20Network%20alignment%2C%20which%20aims%20to%20find%20node%20correspondence%20across%20different%0Anetworks%2C%20is%20the%20cornerstone%20of%20various%20downstream%20multi-network%20and%20Web%20mining%0Atasks.%20Most%20of%20the%20embedding-based%20methods%20indirectly%20model%20cross-network%20node%0Arelationships%20by%20contrasting%20positive%20and%20negative%20node%20pairs%20sampled%20from%0Ahand-crafted%20strategies%2C%20which%20are%20vulnerable%20to%20graph%20noises%20and%20lead%20to%0Apotential%20misalignment%20of%20nodes.%20Another%20line%20of%20work%20based%20on%20the%20optimal%0Atransport%20%28OT%29%20theory%20directly%20models%20cross-network%20node%20relationships%20and%0Agenerates%20noise-reduced%20alignments.%20However%2C%20OT%20methods%20heavily%20rely%20on%20fixed%2C%0Apre-defined%20cost%20functions%20that%20prohibit%20end-to-end%20training%20and%20are%20hard%20to%0Ageneralize.%20In%20this%20paper%2C%20we%20aim%20to%20unify%20the%20embedding%20and%20OT-based%20methods%0Ain%20a%20mutually%20beneficial%20manner%20and%20propose%20a%20joint%20optimal%20transport%20and%0Aembedding%20framework%20for%20network%20alignment%20named%20JOENA.%20For%20one%20thing%20%28OT%20for%0Aembedding%29%2C%20through%20a%20simple%20yet%20effective%20transformation%2C%20the%20noise-reduced%20OT%0Amapping%20serves%20as%20an%20adaptive%20sampling%20strategy%20directly%20modeling%20all%0Across-network%20node%20pairs%20for%20robust%20embedding%20learning.For%20another%20%28embedding%0Afor%20OT%29%2C%20on%20top%20of%20the%20learned%20embeddings%2C%20the%20OT%20cost%20can%20be%20gradually%20trained%0Ain%20an%20end-to-end%20fashion%2C%20which%20further%20enhances%20the%20alignment%20quality.%20With%20a%0Aunified%20objective%2C%20the%20mutual%20benefits%20of%20both%20methods%20can%20be%20achieved%20by%20an%0Aalternating%20optimization%20schema%20with%20guaranteed%20convergence.%20Extensive%0Aexperiments%20on%20real-world%20networks%20validate%20the%20effectiveness%20and%20scalability%0Aof%20JOENA%2C%20achieving%20up%20to%2016%25%20improvement%20in%20MRR%20and%2020x%20speedup%20compared%20with%0Athe%20state-of-the-art%20alignment%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19334v1&entry.124074799=Read"},
{"title": "Can Language Models Falsify? Evaluating Algorithmic Reasoning with\n  Counterexample Creation", "author": "Shiven Sinha and Shashwat Goel and Ponnurangam Kumaraguru and Jonas Geiping and Matthias Bethge and Ameya Prabhu", "abstract": "  There is growing excitement about the potential of Language Models (LMs) to\naccelerate scientific discovery. Falsifying hypotheses is key to scientific\nprogress, as it allows claims to be iteratively refined over time. This process\nrequires significant researcher effort, reasoning, and ingenuity. Yet current\nbenchmarks for LMs predominantly assess their ability to generate solutions\nrather than challenge them. We advocate for developing benchmarks that evaluate\nthis inverse capability - creating counterexamples for subtly incorrect\nsolutions. To demonstrate this approach, we start with the domain of\nalgorithmic problem solving, where counterexamples can be evaluated\nautomatically using code execution. Specifically, we introduce REFUTE, a\ndynamically updating benchmark that includes recent problems and incorrect\nsubmissions from programming competitions, where human experts successfully\nidentified counterexamples. Our analysis finds that the best reasoning agents,\neven OpenAI o3-mini (high) with code execution feedback, can create\ncounterexamples for only <9% of incorrect solutions in REFUTE, even though\nratings indicate its ability to solve up to 48% of these problems from scratch.\nWe hope our work spurs progress in evaluating and enhancing LMs' ability to\nfalsify incorrect solutions - a capability that is crucial for both\naccelerating research and making models self-improve through reliable\nreflective reasoning.\n", "link": "http://arxiv.org/abs/2502.19414v1", "date": "2025-02-26", "relevancy": 2.3881, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Language%20Models%20Falsify%3F%20Evaluating%20Algorithmic%20Reasoning%20with%0A%20%20Counterexample%20Creation&body=Title%3A%20Can%20Language%20Models%20Falsify%3F%20Evaluating%20Algorithmic%20Reasoning%20with%0A%20%20Counterexample%20Creation%0AAuthor%3A%20Shiven%20Sinha%20and%20Shashwat%20Goel%20and%20Ponnurangam%20Kumaraguru%20and%20Jonas%20Geiping%20and%20Matthias%20Bethge%20and%20Ameya%20Prabhu%0AAbstract%3A%20%20%20There%20is%20growing%20excitement%20about%20the%20potential%20of%20Language%20Models%20%28LMs%29%20to%0Aaccelerate%20scientific%20discovery.%20Falsifying%20hypotheses%20is%20key%20to%20scientific%0Aprogress%2C%20as%20it%20allows%20claims%20to%20be%20iteratively%20refined%20over%20time.%20This%20process%0Arequires%20significant%20researcher%20effort%2C%20reasoning%2C%20and%20ingenuity.%20Yet%20current%0Abenchmarks%20for%20LMs%20predominantly%20assess%20their%20ability%20to%20generate%20solutions%0Arather%20than%20challenge%20them.%20We%20advocate%20for%20developing%20benchmarks%20that%20evaluate%0Athis%20inverse%20capability%20-%20creating%20counterexamples%20for%20subtly%20incorrect%0Asolutions.%20To%20demonstrate%20this%20approach%2C%20we%20start%20with%20the%20domain%20of%0Aalgorithmic%20problem%20solving%2C%20where%20counterexamples%20can%20be%20evaluated%0Aautomatically%20using%20code%20execution.%20Specifically%2C%20we%20introduce%20REFUTE%2C%20a%0Adynamically%20updating%20benchmark%20that%20includes%20recent%20problems%20and%20incorrect%0Asubmissions%20from%20programming%20competitions%2C%20where%20human%20experts%20successfully%0Aidentified%20counterexamples.%20Our%20analysis%20finds%20that%20the%20best%20reasoning%20agents%2C%0Aeven%20OpenAI%20o3-mini%20%28high%29%20with%20code%20execution%20feedback%2C%20can%20create%0Acounterexamples%20for%20only%20%3C9%25%20of%20incorrect%20solutions%20in%20REFUTE%2C%20even%20though%0Aratings%20indicate%20its%20ability%20to%20solve%20up%20to%2048%25%20of%20these%20problems%20from%20scratch.%0AWe%20hope%20our%20work%20spurs%20progress%20in%20evaluating%20and%20enhancing%20LMs%27%20ability%20to%0Afalsify%20incorrect%20solutions%20-%20a%20capability%20that%20is%20crucial%20for%20both%0Aaccelerating%20research%20and%20making%20models%20self-improve%20through%20reliable%0Areflective%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Language%2520Models%2520Falsify%253F%2520Evaluating%2520Algorithmic%2520Reasoning%2520with%250A%2520%2520Counterexample%2520Creation%26entry.906535625%3DShiven%2520Sinha%2520and%2520Shashwat%2520Goel%2520and%2520Ponnurangam%2520Kumaraguru%2520and%2520Jonas%2520Geiping%2520and%2520Matthias%2520Bethge%2520and%2520Ameya%2520Prabhu%26entry.1292438233%3D%2520%2520There%2520is%2520growing%2520excitement%2520about%2520the%2520potential%2520of%2520Language%2520Models%2520%2528LMs%2529%2520to%250Aaccelerate%2520scientific%2520discovery.%2520Falsifying%2520hypotheses%2520is%2520key%2520to%2520scientific%250Aprogress%252C%2520as%2520it%2520allows%2520claims%2520to%2520be%2520iteratively%2520refined%2520over%2520time.%2520This%2520process%250Arequires%2520significant%2520researcher%2520effort%252C%2520reasoning%252C%2520and%2520ingenuity.%2520Yet%2520current%250Abenchmarks%2520for%2520LMs%2520predominantly%2520assess%2520their%2520ability%2520to%2520generate%2520solutions%250Arather%2520than%2520challenge%2520them.%2520We%2520advocate%2520for%2520developing%2520benchmarks%2520that%2520evaluate%250Athis%2520inverse%2520capability%2520-%2520creating%2520counterexamples%2520for%2520subtly%2520incorrect%250Asolutions.%2520To%2520demonstrate%2520this%2520approach%252C%2520we%2520start%2520with%2520the%2520domain%2520of%250Aalgorithmic%2520problem%2520solving%252C%2520where%2520counterexamples%2520can%2520be%2520evaluated%250Aautomatically%2520using%2520code%2520execution.%2520Specifically%252C%2520we%2520introduce%2520REFUTE%252C%2520a%250Adynamically%2520updating%2520benchmark%2520that%2520includes%2520recent%2520problems%2520and%2520incorrect%250Asubmissions%2520from%2520programming%2520competitions%252C%2520where%2520human%2520experts%2520successfully%250Aidentified%2520counterexamples.%2520Our%2520analysis%2520finds%2520that%2520the%2520best%2520reasoning%2520agents%252C%250Aeven%2520OpenAI%2520o3-mini%2520%2528high%2529%2520with%2520code%2520execution%2520feedback%252C%2520can%2520create%250Acounterexamples%2520for%2520only%2520%253C9%2525%2520of%2520incorrect%2520solutions%2520in%2520REFUTE%252C%2520even%2520though%250Aratings%2520indicate%2520its%2520ability%2520to%2520solve%2520up%2520to%252048%2525%2520of%2520these%2520problems%2520from%2520scratch.%250AWe%2520hope%2520our%2520work%2520spurs%2520progress%2520in%2520evaluating%2520and%2520enhancing%2520LMs%2527%2520ability%2520to%250Afalsify%2520incorrect%2520solutions%2520-%2520a%2520capability%2520that%2520is%2520crucial%2520for%2520both%250Aaccelerating%2520research%2520and%2520making%2520models%2520self-improve%2520through%2520reliable%250Areflective%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Language%20Models%20Falsify%3F%20Evaluating%20Algorithmic%20Reasoning%20with%0A%20%20Counterexample%20Creation&entry.906535625=Shiven%20Sinha%20and%20Shashwat%20Goel%20and%20Ponnurangam%20Kumaraguru%20and%20Jonas%20Geiping%20and%20Matthias%20Bethge%20and%20Ameya%20Prabhu&entry.1292438233=%20%20There%20is%20growing%20excitement%20about%20the%20potential%20of%20Language%20Models%20%28LMs%29%20to%0Aaccelerate%20scientific%20discovery.%20Falsifying%20hypotheses%20is%20key%20to%20scientific%0Aprogress%2C%20as%20it%20allows%20claims%20to%20be%20iteratively%20refined%20over%20time.%20This%20process%0Arequires%20significant%20researcher%20effort%2C%20reasoning%2C%20and%20ingenuity.%20Yet%20current%0Abenchmarks%20for%20LMs%20predominantly%20assess%20their%20ability%20to%20generate%20solutions%0Arather%20than%20challenge%20them.%20We%20advocate%20for%20developing%20benchmarks%20that%20evaluate%0Athis%20inverse%20capability%20-%20creating%20counterexamples%20for%20subtly%20incorrect%0Asolutions.%20To%20demonstrate%20this%20approach%2C%20we%20start%20with%20the%20domain%20of%0Aalgorithmic%20problem%20solving%2C%20where%20counterexamples%20can%20be%20evaluated%0Aautomatically%20using%20code%20execution.%20Specifically%2C%20we%20introduce%20REFUTE%2C%20a%0Adynamically%20updating%20benchmark%20that%20includes%20recent%20problems%20and%20incorrect%0Asubmissions%20from%20programming%20competitions%2C%20where%20human%20experts%20successfully%0Aidentified%20counterexamples.%20Our%20analysis%20finds%20that%20the%20best%20reasoning%20agents%2C%0Aeven%20OpenAI%20o3-mini%20%28high%29%20with%20code%20execution%20feedback%2C%20can%20create%0Acounterexamples%20for%20only%20%3C9%25%20of%20incorrect%20solutions%20in%20REFUTE%2C%20even%20though%0Aratings%20indicate%20its%20ability%20to%20solve%20up%20to%2048%25%20of%20these%20problems%20from%20scratch.%0AWe%20hope%20our%20work%20spurs%20progress%20in%20evaluating%20and%20enhancing%20LMs%27%20ability%20to%0Afalsify%20incorrect%20solutions%20-%20a%20capability%20that%20is%20crucial%20for%20both%0Aaccelerating%20research%20and%20making%20models%20self-improve%20through%20reliable%0Areflective%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19414v1&entry.124074799=Read"},
{"title": "LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace", "author": "Yan Yang and Bin Gao and Ya-xiang Yuan", "abstract": "  Bilevel optimization, with broad applications in machine learning, has an\nintricate hierarchical structure. Gradient-based methods have emerged as a\ncommon approach to large-scale bilevel problems. However, the computation of\nthe hyper-gradient, which involves a Hessian inverse vector product, confines\nthe efficiency and is regarded as a bottleneck. To circumvent the inverse, we\nconstruct a sequence of low-dimensional approximate Krylov subspaces with the\naid of the Lanczos process. As a result, the constructed subspace is able to\ndynamically and incrementally approximate the Hessian inverse vector product\nwith less effort and thus leads to a favorable estimate of the hyper-gradient.\nMoreover, we propose a provable subspace-based framework for bilevel problems\nwhere one central step is to solve a small-size tridiagonal linear system. To\nthe best of our knowledge, this is the first time that subspace techniques are\nincorporated into bilevel optimization. This successful trial not only enjoys\n$\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency\nin a synthetic problem and two deep learning tasks.\n", "link": "http://arxiv.org/abs/2404.03331v2", "date": "2025-02-26", "relevancy": 2.3735, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4827}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4713}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LancBiO%3A%20dynamic%20Lanczos-aided%20bilevel%20optimization%20via%20Krylov%20subspace&body=Title%3A%20LancBiO%3A%20dynamic%20Lanczos-aided%20bilevel%20optimization%20via%20Krylov%20subspace%0AAuthor%3A%20Yan%20Yang%20and%20Bin%20Gao%20and%20Ya-xiang%20Yuan%0AAbstract%3A%20%20%20Bilevel%20optimization%2C%20with%20broad%20applications%20in%20machine%20learning%2C%20has%20an%0Aintricate%20hierarchical%20structure.%20Gradient-based%20methods%20have%20emerged%20as%20a%0Acommon%20approach%20to%20large-scale%20bilevel%20problems.%20However%2C%20the%20computation%20of%0Athe%20hyper-gradient%2C%20which%20involves%20a%20Hessian%20inverse%20vector%20product%2C%20confines%0Athe%20efficiency%20and%20is%20regarded%20as%20a%20bottleneck.%20To%20circumvent%20the%20inverse%2C%20we%0Aconstruct%20a%20sequence%20of%20low-dimensional%20approximate%20Krylov%20subspaces%20with%20the%0Aaid%20of%20the%20Lanczos%20process.%20As%20a%20result%2C%20the%20constructed%20subspace%20is%20able%20to%0Adynamically%20and%20incrementally%20approximate%20the%20Hessian%20inverse%20vector%20product%0Awith%20less%20effort%20and%20thus%20leads%20to%20a%20favorable%20estimate%20of%20the%20hyper-gradient.%0AMoreover%2C%20we%20propose%20a%20provable%20subspace-based%20framework%20for%20bilevel%20problems%0Awhere%20one%20central%20step%20is%20to%20solve%20a%20small-size%20tridiagonal%20linear%20system.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%20that%20subspace%20techniques%20are%0Aincorporated%20into%20bilevel%20optimization.%20This%20successful%20trial%20not%20only%20enjoys%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-1%7D%29%24%20convergence%20rate%20but%20also%20demonstrates%20efficiency%0Ain%20a%20synthetic%20problem%20and%20two%20deep%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03331v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLancBiO%253A%2520dynamic%2520Lanczos-aided%2520bilevel%2520optimization%2520via%2520Krylov%2520subspace%26entry.906535625%3DYan%2520Yang%2520and%2520Bin%2520Gao%2520and%2520Ya-xiang%2520Yuan%26entry.1292438233%3D%2520%2520Bilevel%2520optimization%252C%2520with%2520broad%2520applications%2520in%2520machine%2520learning%252C%2520has%2520an%250Aintricate%2520hierarchical%2520structure.%2520Gradient-based%2520methods%2520have%2520emerged%2520as%2520a%250Acommon%2520approach%2520to%2520large-scale%2520bilevel%2520problems.%2520However%252C%2520the%2520computation%2520of%250Athe%2520hyper-gradient%252C%2520which%2520involves%2520a%2520Hessian%2520inverse%2520vector%2520product%252C%2520confines%250Athe%2520efficiency%2520and%2520is%2520regarded%2520as%2520a%2520bottleneck.%2520To%2520circumvent%2520the%2520inverse%252C%2520we%250Aconstruct%2520a%2520sequence%2520of%2520low-dimensional%2520approximate%2520Krylov%2520subspaces%2520with%2520the%250Aaid%2520of%2520the%2520Lanczos%2520process.%2520As%2520a%2520result%252C%2520the%2520constructed%2520subspace%2520is%2520able%2520to%250Adynamically%2520and%2520incrementally%2520approximate%2520the%2520Hessian%2520inverse%2520vector%2520product%250Awith%2520less%2520effort%2520and%2520thus%2520leads%2520to%2520a%2520favorable%2520estimate%2520of%2520the%2520hyper-gradient.%250AMoreover%252C%2520we%2520propose%2520a%2520provable%2520subspace-based%2520framework%2520for%2520bilevel%2520problems%250Awhere%2520one%2520central%2520step%2520is%2520to%2520solve%2520a%2520small-size%2520tridiagonal%2520linear%2520system.%2520To%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520time%2520that%2520subspace%2520techniques%2520are%250Aincorporated%2520into%2520bilevel%2520optimization.%2520This%2520successful%2520trial%2520not%2520only%2520enjoys%250A%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-1%257D%2529%2524%2520convergence%2520rate%2520but%2520also%2520demonstrates%2520efficiency%250Ain%2520a%2520synthetic%2520problem%2520and%2520two%2520deep%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03331v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LancBiO%3A%20dynamic%20Lanczos-aided%20bilevel%20optimization%20via%20Krylov%20subspace&entry.906535625=Yan%20Yang%20and%20Bin%20Gao%20and%20Ya-xiang%20Yuan&entry.1292438233=%20%20Bilevel%20optimization%2C%20with%20broad%20applications%20in%20machine%20learning%2C%20has%20an%0Aintricate%20hierarchical%20structure.%20Gradient-based%20methods%20have%20emerged%20as%20a%0Acommon%20approach%20to%20large-scale%20bilevel%20problems.%20However%2C%20the%20computation%20of%0Athe%20hyper-gradient%2C%20which%20involves%20a%20Hessian%20inverse%20vector%20product%2C%20confines%0Athe%20efficiency%20and%20is%20regarded%20as%20a%20bottleneck.%20To%20circumvent%20the%20inverse%2C%20we%0Aconstruct%20a%20sequence%20of%20low-dimensional%20approximate%20Krylov%20subspaces%20with%20the%0Aaid%20of%20the%20Lanczos%20process.%20As%20a%20result%2C%20the%20constructed%20subspace%20is%20able%20to%0Adynamically%20and%20incrementally%20approximate%20the%20Hessian%20inverse%20vector%20product%0Awith%20less%20effort%20and%20thus%20leads%20to%20a%20favorable%20estimate%20of%20the%20hyper-gradient.%0AMoreover%2C%20we%20propose%20a%20provable%20subspace-based%20framework%20for%20bilevel%20problems%0Awhere%20one%20central%20step%20is%20to%20solve%20a%20small-size%20tridiagonal%20linear%20system.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20time%20that%20subspace%20techniques%20are%0Aincorporated%20into%20bilevel%20optimization.%20This%20successful%20trial%20not%20only%20enjoys%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-1%7D%29%24%20convergence%20rate%20but%20also%20demonstrates%20efficiency%0Ain%20a%20synthetic%20problem%20and%20two%20deep%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03331v2&entry.124074799=Read"},
{"title": "Large Language Model Agent for Hyper-Parameter Optimization", "author": "Siyi Liu and Chen Gao and Yong Li", "abstract": "  Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios.\n", "link": "http://arxiv.org/abs/2402.01881v3", "date": "2025-02-26", "relevancy": 2.366, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4943}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4636}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Agent%20for%20Hyper-Parameter%20Optimization&body=Title%3A%20Large%20Language%20Model%20Agent%20for%20Hyper-Parameter%20Optimization%0AAuthor%3A%20Siyi%20Liu%20and%20Chen%20Gao%20and%20Yong%20Li%0AAbstract%3A%20%20%20Hyperparameter%20optimization%20is%20critical%20in%20modern%20machine%20learning%2C%20requiring%0Aexpert%20knowledge%2C%20numerous%20trials%2C%20and%20high%20computational%20and%20human%20resources.%0ADespite%20the%20advancements%20in%20Automated%20Machine%20Learning%20%28AutoML%29%2C%20challenges%20in%0Aterms%20of%20trial%20efficiency%2C%20setup%20complexity%2C%20and%20interoperability%20still%0Apersist.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20novel%20paradigm%20leveraging%0ALarge%20Language%20Models%20%28LLMs%29%20to%20automate%20hyperparameter%20optimization%20across%0Adiverse%20machine%20learning%20tasks%2C%20which%20is%20named%20AgentHPO%20%28short%20for%20LLM%0AAgent-based%20Hyperparameter%20Optimization%29.%20Specifically%2C%20AgentHPO%20processes%20the%0Atask%20information%20autonomously%2C%20conducts%20experiments%20with%20specific%0Ahyperparameters%20%28HPs%29%2C%20and%20iteratively%20optimizes%20them%20based%20on%20historical%0Atrials.%20This%20human-like%20optimization%20process%20largely%20reduces%20the%20number%20of%0Arequired%20trials%2C%20simplifies%20the%20setup%20process%2C%20and%20enhances%20interpretability%0Aand%20user%20trust%2C%20compared%20to%20traditional%20AutoML%20methods.%20Extensive%20empirical%0Aexperiments%20conducted%20on%2012%20representative%20machine-learning%20tasks%20indicate%20that%0AAgentHPO%20not%20only%20matches%20but%20also%20often%20surpasses%20the%20best%20human%20trials%20in%0Aterms%20of%20performance%20while%20simultaneously%20providing%20explainable%20results.%0AFurther%20analysis%20sheds%20light%20on%20the%20strategies%20employed%20by%20the%20LLM%20in%0Aoptimizing%20these%20tasks%2C%20highlighting%20its%20effectiveness%20and%20adaptability%20in%0Avarious%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01881v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Agent%2520for%2520Hyper-Parameter%2520Optimization%26entry.906535625%3DSiyi%2520Liu%2520and%2520Chen%2520Gao%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Hyperparameter%2520optimization%2520is%2520critical%2520in%2520modern%2520machine%2520learning%252C%2520requiring%250Aexpert%2520knowledge%252C%2520numerous%2520trials%252C%2520and%2520high%2520computational%2520and%2520human%2520resources.%250ADespite%2520the%2520advancements%2520in%2520Automated%2520Machine%2520Learning%2520%2528AutoML%2529%252C%2520challenges%2520in%250Aterms%2520of%2520trial%2520efficiency%252C%2520setup%2520complexity%252C%2520and%2520interoperability%2520still%250Apersist.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520novel%2520paradigm%2520leveraging%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520automate%2520hyperparameter%2520optimization%2520across%250Adiverse%2520machine%2520learning%2520tasks%252C%2520which%2520is%2520named%2520AgentHPO%2520%2528short%2520for%2520LLM%250AAgent-based%2520Hyperparameter%2520Optimization%2529.%2520Specifically%252C%2520AgentHPO%2520processes%2520the%250Atask%2520information%2520autonomously%252C%2520conducts%2520experiments%2520with%2520specific%250Ahyperparameters%2520%2528HPs%2529%252C%2520and%2520iteratively%2520optimizes%2520them%2520based%2520on%2520historical%250Atrials.%2520This%2520human-like%2520optimization%2520process%2520largely%2520reduces%2520the%2520number%2520of%250Arequired%2520trials%252C%2520simplifies%2520the%2520setup%2520process%252C%2520and%2520enhances%2520interpretability%250Aand%2520user%2520trust%252C%2520compared%2520to%2520traditional%2520AutoML%2520methods.%2520Extensive%2520empirical%250Aexperiments%2520conducted%2520on%252012%2520representative%2520machine-learning%2520tasks%2520indicate%2520that%250AAgentHPO%2520not%2520only%2520matches%2520but%2520also%2520often%2520surpasses%2520the%2520best%2520human%2520trials%2520in%250Aterms%2520of%2520performance%2520while%2520simultaneously%2520providing%2520explainable%2520results.%250AFurther%2520analysis%2520sheds%2520light%2520on%2520the%2520strategies%2520employed%2520by%2520the%2520LLM%2520in%250Aoptimizing%2520these%2520tasks%252C%2520highlighting%2520its%2520effectiveness%2520and%2520adaptability%2520in%250Avarious%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01881v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Agent%20for%20Hyper-Parameter%20Optimization&entry.906535625=Siyi%20Liu%20and%20Chen%20Gao%20and%20Yong%20Li&entry.1292438233=%20%20Hyperparameter%20optimization%20is%20critical%20in%20modern%20machine%20learning%2C%20requiring%0Aexpert%20knowledge%2C%20numerous%20trials%2C%20and%20high%20computational%20and%20human%20resources.%0ADespite%20the%20advancements%20in%20Automated%20Machine%20Learning%20%28AutoML%29%2C%20challenges%20in%0Aterms%20of%20trial%20efficiency%2C%20setup%20complexity%2C%20and%20interoperability%20still%0Apersist.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20novel%20paradigm%20leveraging%0ALarge%20Language%20Models%20%28LLMs%29%20to%20automate%20hyperparameter%20optimization%20across%0Adiverse%20machine%20learning%20tasks%2C%20which%20is%20named%20AgentHPO%20%28short%20for%20LLM%0AAgent-based%20Hyperparameter%20Optimization%29.%20Specifically%2C%20AgentHPO%20processes%20the%0Atask%20information%20autonomously%2C%20conducts%20experiments%20with%20specific%0Ahyperparameters%20%28HPs%29%2C%20and%20iteratively%20optimizes%20them%20based%20on%20historical%0Atrials.%20This%20human-like%20optimization%20process%20largely%20reduces%20the%20number%20of%0Arequired%20trials%2C%20simplifies%20the%20setup%20process%2C%20and%20enhances%20interpretability%0Aand%20user%20trust%2C%20compared%20to%20traditional%20AutoML%20methods.%20Extensive%20empirical%0Aexperiments%20conducted%20on%2012%20representative%20machine-learning%20tasks%20indicate%20that%0AAgentHPO%20not%20only%20matches%20but%20also%20often%20surpasses%20the%20best%20human%20trials%20in%0Aterms%20of%20performance%20while%20simultaneously%20providing%20explainable%20results.%0AFurther%20analysis%20sheds%20light%20on%20the%20strategies%20employed%20by%20the%20LLM%20in%0Aoptimizing%20these%20tasks%2C%20highlighting%20its%20effectiveness%20and%20adaptability%20in%0Avarious%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01881v3&entry.124074799=Read"},
{"title": "Language Imbalance Driven Rewarding for Multilingual Self-improving", "author": "Wen Yang and Junhong Wu and Chen Wang and Chengqing Zong and Jiajun Zhang", "abstract": "  Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs. The code is available\nat https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding\n", "link": "http://arxiv.org/abs/2410.08964v3", "date": "2025-02-26", "relevancy": 2.3569, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4803}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Imbalance%20Driven%20Rewarding%20for%20Multilingual%20Self-improving&body=Title%3A%20Language%20Imbalance%20Driven%20Rewarding%20for%20Multilingual%20Self-improving%0AAuthor%3A%20Wen%20Yang%20and%20Junhong%20Wu%20and%20Chen%20Wang%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20state-of-the-art%20performance%0Aacross%20numerous%20tasks.%20However%2C%20these%20advancements%20have%20predominantly%20benefited%0A%22first-class%22%20languages%20such%20as%20English%20and%20Chinese%2C%20leaving%20many%20other%0Alanguages%20underrepresented.%20This%20imbalance%2C%20while%20limiting%20broader%0Aapplications%2C%20generates%20a%20natural%20preference%20ranking%20between%20languages%2C%0Aoffering%20an%20opportunity%20to%20bootstrap%20the%20multilingual%20capabilities%20of%20LLM%20in%20a%0Aself-improving%20manner.%20Thus%2C%20we%20propose%20%24%5Ctextit%7BLanguage%20Imbalance%20Driven%0ARewarding%7D%24%2C%20where%20the%20inherent%20imbalance%20between%20dominant%20and%20non-dominant%0Alanguages%20within%20LLMs%20is%20leveraged%20as%20a%20reward%20signal.%20Iterative%20DPO%20training%0Ademonstrates%20that%20this%20approach%20not%20only%20enhances%20LLM%20performance%20in%0Anon-dominant%20languages%20but%20also%20improves%20the%20dominant%20language%27s%20capacity%2C%0Athereby%20yielding%20an%20iterative%20reward%20signal.%20Fine-tuning%0AMeta-Llama-3-8B-Instruct%20over%20two%20iterations%20of%20this%20approach%20results%20in%0Acontinuous%20improvements%20in%20multilingual%20performance%20across%0Ainstruction-following%20and%20arithmetic%20reasoning%20tasks%2C%20evidenced%20by%20an%20average%0Aimprovement%20of%207.46%25%20win%20rate%20on%20the%20X-AlpacaEval%20leaderboard%20and%2013.9%25%0Aaccuracy%20on%20the%20MGSM%20benchmark.%20This%20work%20serves%20as%20an%20initial%20exploration%2C%0Apaving%20the%20way%20for%20multilingual%20self-improvement%20of%20LLMs.%20The%20code%20is%20available%0Aat%20https%3A//github.com/ZNLP/Language-Imbalance-Driven-Rewarding%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08964v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Imbalance%2520Driven%2520Rewarding%2520for%2520Multilingual%2520Self-improving%26entry.906535625%3DWen%2520Yang%2520and%2520Junhong%2520Wu%2520and%2520Chen%2520Wang%2520and%2520Chengqing%2520Zong%2520and%2520Jiajun%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520state-of-the-art%2520performance%250Aacross%2520numerous%2520tasks.%2520However%252C%2520these%2520advancements%2520have%2520predominantly%2520benefited%250A%2522first-class%2522%2520languages%2520such%2520as%2520English%2520and%2520Chinese%252C%2520leaving%2520many%2520other%250Alanguages%2520underrepresented.%2520This%2520imbalance%252C%2520while%2520limiting%2520broader%250Aapplications%252C%2520generates%2520a%2520natural%2520preference%2520ranking%2520between%2520languages%252C%250Aoffering%2520an%2520opportunity%2520to%2520bootstrap%2520the%2520multilingual%2520capabilities%2520of%2520LLM%2520in%2520a%250Aself-improving%2520manner.%2520Thus%252C%2520we%2520propose%2520%2524%255Ctextit%257BLanguage%2520Imbalance%2520Driven%250ARewarding%257D%2524%252C%2520where%2520the%2520inherent%2520imbalance%2520between%2520dominant%2520and%2520non-dominant%250Alanguages%2520within%2520LLMs%2520is%2520leveraged%2520as%2520a%2520reward%2520signal.%2520Iterative%2520DPO%2520training%250Ademonstrates%2520that%2520this%2520approach%2520not%2520only%2520enhances%2520LLM%2520performance%2520in%250Anon-dominant%2520languages%2520but%2520also%2520improves%2520the%2520dominant%2520language%2527s%2520capacity%252C%250Athereby%2520yielding%2520an%2520iterative%2520reward%2520signal.%2520Fine-tuning%250AMeta-Llama-3-8B-Instruct%2520over%2520two%2520iterations%2520of%2520this%2520approach%2520results%2520in%250Acontinuous%2520improvements%2520in%2520multilingual%2520performance%2520across%250Ainstruction-following%2520and%2520arithmetic%2520reasoning%2520tasks%252C%2520evidenced%2520by%2520an%2520average%250Aimprovement%2520of%25207.46%2525%2520win%2520rate%2520on%2520the%2520X-AlpacaEval%2520leaderboard%2520and%252013.9%2525%250Aaccuracy%2520on%2520the%2520MGSM%2520benchmark.%2520This%2520work%2520serves%2520as%2520an%2520initial%2520exploration%252C%250Apaving%2520the%2520way%2520for%2520multilingual%2520self-improvement%2520of%2520LLMs.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/ZNLP/Language-Imbalance-Driven-Rewarding%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08964v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Imbalance%20Driven%20Rewarding%20for%20Multilingual%20Self-improving&entry.906535625=Wen%20Yang%20and%20Junhong%20Wu%20and%20Chen%20Wang%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20state-of-the-art%20performance%0Aacross%20numerous%20tasks.%20However%2C%20these%20advancements%20have%20predominantly%20benefited%0A%22first-class%22%20languages%20such%20as%20English%20and%20Chinese%2C%20leaving%20many%20other%0Alanguages%20underrepresented.%20This%20imbalance%2C%20while%20limiting%20broader%0Aapplications%2C%20generates%20a%20natural%20preference%20ranking%20between%20languages%2C%0Aoffering%20an%20opportunity%20to%20bootstrap%20the%20multilingual%20capabilities%20of%20LLM%20in%20a%0Aself-improving%20manner.%20Thus%2C%20we%20propose%20%24%5Ctextit%7BLanguage%20Imbalance%20Driven%0ARewarding%7D%24%2C%20where%20the%20inherent%20imbalance%20between%20dominant%20and%20non-dominant%0Alanguages%20within%20LLMs%20is%20leveraged%20as%20a%20reward%20signal.%20Iterative%20DPO%20training%0Ademonstrates%20that%20this%20approach%20not%20only%20enhances%20LLM%20performance%20in%0Anon-dominant%20languages%20but%20also%20improves%20the%20dominant%20language%27s%20capacity%2C%0Athereby%20yielding%20an%20iterative%20reward%20signal.%20Fine-tuning%0AMeta-Llama-3-8B-Instruct%20over%20two%20iterations%20of%20this%20approach%20results%20in%0Acontinuous%20improvements%20in%20multilingual%20performance%20across%0Ainstruction-following%20and%20arithmetic%20reasoning%20tasks%2C%20evidenced%20by%20an%20average%0Aimprovement%20of%207.46%25%20win%20rate%20on%20the%20X-AlpacaEval%20leaderboard%20and%2013.9%25%0Aaccuracy%20on%20the%20MGSM%20benchmark.%20This%20work%20serves%20as%20an%20initial%20exploration%2C%0Apaving%20the%20way%20for%20multilingual%20self-improvement%20of%20LLMs.%20The%20code%20is%20available%0Aat%20https%3A//github.com/ZNLP/Language-Imbalance-Driven-Rewarding%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08964v3&entry.124074799=Read"},
{"title": "A Survey on Foundation-Model-Based Industrial Defect Detection", "author": "Tianle Yang and Luyao Chang and Jiadong Yan and Juntao Li and Zhi Wang and Ke Zhang", "abstract": "  As industrial products become abundant and sophisticated, visual industrial\ndefect detection receives much attention, including two-dimensional and\nthree-dimensional visual feature modeling. Traditional methods use statistical\nanalysis, abnormal data synthesis modeling, and generation-based models to\nseparate product defect features and complete defect detection. Recently, the\nemergence of foundation models has brought visual and textual semantic prior\nknowledge. Many methods are based on foundation models (FM) to improve the\naccuracy of detection, but at the same time, increase model complexity and slow\ndown inference speed. Some FM-based methods have begun to explore lightweight\nmodeling ways, which have gradually attracted attention and deserve to be\nsystematically analyzed. In this paper, we conduct a systematic survey with\ncomparisons and discussions of foundation model methods from different aspects\nand briefly review non-foundation model (NFM) methods recently published.\nFurthermore, we discuss the differences between FM and NFM methods from\ntraining objectives, model structure and scale, model performance, and\npotential directions for future exploration. Through comparison, we find FM\nmethods are more suitable for few-shot and zero-shot learning, which are more\nin line with actual industrial application scenarios and worthy of in-depth\nresearch.\n", "link": "http://arxiv.org/abs/2502.19106v1", "date": "2025-02-26", "relevancy": 2.343, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Foundation-Model-Based%20Industrial%20Defect%20Detection&body=Title%3A%20A%20Survey%20on%20Foundation-Model-Based%20Industrial%20Defect%20Detection%0AAuthor%3A%20Tianle%20Yang%20and%20Luyao%20Chang%20and%20Jiadong%20Yan%20and%20Juntao%20Li%20and%20Zhi%20Wang%20and%20Ke%20Zhang%0AAbstract%3A%20%20%20As%20industrial%20products%20become%20abundant%20and%20sophisticated%2C%20visual%20industrial%0Adefect%20detection%20receives%20much%20attention%2C%20including%20two-dimensional%20and%0Athree-dimensional%20visual%20feature%20modeling.%20Traditional%20methods%20use%20statistical%0Aanalysis%2C%20abnormal%20data%20synthesis%20modeling%2C%20and%20generation-based%20models%20to%0Aseparate%20product%20defect%20features%20and%20complete%20defect%20detection.%20Recently%2C%20the%0Aemergence%20of%20foundation%20models%20has%20brought%20visual%20and%20textual%20semantic%20prior%0Aknowledge.%20Many%20methods%20are%20based%20on%20foundation%20models%20%28FM%29%20to%20improve%20the%0Aaccuracy%20of%20detection%2C%20but%20at%20the%20same%20time%2C%20increase%20model%20complexity%20and%20slow%0Adown%20inference%20speed.%20Some%20FM-based%20methods%20have%20begun%20to%20explore%20lightweight%0Amodeling%20ways%2C%20which%20have%20gradually%20attracted%20attention%20and%20deserve%20to%20be%0Asystematically%20analyzed.%20In%20this%20paper%2C%20we%20conduct%20a%20systematic%20survey%20with%0Acomparisons%20and%20discussions%20of%20foundation%20model%20methods%20from%20different%20aspects%0Aand%20briefly%20review%20non-foundation%20model%20%28NFM%29%20methods%20recently%20published.%0AFurthermore%2C%20we%20discuss%20the%20differences%20between%20FM%20and%20NFM%20methods%20from%0Atraining%20objectives%2C%20model%20structure%20and%20scale%2C%20model%20performance%2C%20and%0Apotential%20directions%20for%20future%20exploration.%20Through%20comparison%2C%20we%20find%20FM%0Amethods%20are%20more%20suitable%20for%20few-shot%20and%20zero-shot%20learning%2C%20which%20are%20more%0Ain%20line%20with%20actual%20industrial%20application%20scenarios%20and%20worthy%20of%20in-depth%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Foundation-Model-Based%2520Industrial%2520Defect%2520Detection%26entry.906535625%3DTianle%2520Yang%2520and%2520Luyao%2520Chang%2520and%2520Jiadong%2520Yan%2520and%2520Juntao%2520Li%2520and%2520Zhi%2520Wang%2520and%2520Ke%2520Zhang%26entry.1292438233%3D%2520%2520As%2520industrial%2520products%2520become%2520abundant%2520and%2520sophisticated%252C%2520visual%2520industrial%250Adefect%2520detection%2520receives%2520much%2520attention%252C%2520including%2520two-dimensional%2520and%250Athree-dimensional%2520visual%2520feature%2520modeling.%2520Traditional%2520methods%2520use%2520statistical%250Aanalysis%252C%2520abnormal%2520data%2520synthesis%2520modeling%252C%2520and%2520generation-based%2520models%2520to%250Aseparate%2520product%2520defect%2520features%2520and%2520complete%2520defect%2520detection.%2520Recently%252C%2520the%250Aemergence%2520of%2520foundation%2520models%2520has%2520brought%2520visual%2520and%2520textual%2520semantic%2520prior%250Aknowledge.%2520Many%2520methods%2520are%2520based%2520on%2520foundation%2520models%2520%2528FM%2529%2520to%2520improve%2520the%250Aaccuracy%2520of%2520detection%252C%2520but%2520at%2520the%2520same%2520time%252C%2520increase%2520model%2520complexity%2520and%2520slow%250Adown%2520inference%2520speed.%2520Some%2520FM-based%2520methods%2520have%2520begun%2520to%2520explore%2520lightweight%250Amodeling%2520ways%252C%2520which%2520have%2520gradually%2520attracted%2520attention%2520and%2520deserve%2520to%2520be%250Asystematically%2520analyzed.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520systematic%2520survey%2520with%250Acomparisons%2520and%2520discussions%2520of%2520foundation%2520model%2520methods%2520from%2520different%2520aspects%250Aand%2520briefly%2520review%2520non-foundation%2520model%2520%2528NFM%2529%2520methods%2520recently%2520published.%250AFurthermore%252C%2520we%2520discuss%2520the%2520differences%2520between%2520FM%2520and%2520NFM%2520methods%2520from%250Atraining%2520objectives%252C%2520model%2520structure%2520and%2520scale%252C%2520model%2520performance%252C%2520and%250Apotential%2520directions%2520for%2520future%2520exploration.%2520Through%2520comparison%252C%2520we%2520find%2520FM%250Amethods%2520are%2520more%2520suitable%2520for%2520few-shot%2520and%2520zero-shot%2520learning%252C%2520which%2520are%2520more%250Ain%2520line%2520with%2520actual%2520industrial%2520application%2520scenarios%2520and%2520worthy%2520of%2520in-depth%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Foundation-Model-Based%20Industrial%20Defect%20Detection&entry.906535625=Tianle%20Yang%20and%20Luyao%20Chang%20and%20Jiadong%20Yan%20and%20Juntao%20Li%20and%20Zhi%20Wang%20and%20Ke%20Zhang&entry.1292438233=%20%20As%20industrial%20products%20become%20abundant%20and%20sophisticated%2C%20visual%20industrial%0Adefect%20detection%20receives%20much%20attention%2C%20including%20two-dimensional%20and%0Athree-dimensional%20visual%20feature%20modeling.%20Traditional%20methods%20use%20statistical%0Aanalysis%2C%20abnormal%20data%20synthesis%20modeling%2C%20and%20generation-based%20models%20to%0Aseparate%20product%20defect%20features%20and%20complete%20defect%20detection.%20Recently%2C%20the%0Aemergence%20of%20foundation%20models%20has%20brought%20visual%20and%20textual%20semantic%20prior%0Aknowledge.%20Many%20methods%20are%20based%20on%20foundation%20models%20%28FM%29%20to%20improve%20the%0Aaccuracy%20of%20detection%2C%20but%20at%20the%20same%20time%2C%20increase%20model%20complexity%20and%20slow%0Adown%20inference%20speed.%20Some%20FM-based%20methods%20have%20begun%20to%20explore%20lightweight%0Amodeling%20ways%2C%20which%20have%20gradually%20attracted%20attention%20and%20deserve%20to%20be%0Asystematically%20analyzed.%20In%20this%20paper%2C%20we%20conduct%20a%20systematic%20survey%20with%0Acomparisons%20and%20discussions%20of%20foundation%20model%20methods%20from%20different%20aspects%0Aand%20briefly%20review%20non-foundation%20model%20%28NFM%29%20methods%20recently%20published.%0AFurthermore%2C%20we%20discuss%20the%20differences%20between%20FM%20and%20NFM%20methods%20from%0Atraining%20objectives%2C%20model%20structure%20and%20scale%2C%20model%20performance%2C%20and%0Apotential%20directions%20for%20future%20exploration.%20Through%20comparison%2C%20we%20find%20FM%0Amethods%20are%20more%20suitable%20for%20few-shot%20and%20zero-shot%20learning%2C%20which%20are%20more%0Ain%20line%20with%20actual%20industrial%20application%20scenarios%20and%20worthy%20of%20in-depth%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19106v1&entry.124074799=Read"},
{"title": "PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in\n  Semi-Structured Environments", "author": "Yueting Liu and Hanshi Wang and Zhengjun Zha and Weiming Hu and Jin Gao", "abstract": "  Recent advancements in autonomous driving perception have revealed\nexceptional capabilities within structured environments dominated by vehicular\ntraffic. However, current perception models exhibit significant limitations in\nsemi-structured environments, where dynamic pedestrians with more diverse\nirregular movement and occlusion prevail. We attribute this shortcoming to the\nscarcity of high-quality datasets in semi-structured scenes, particularly\nconcerning pedestrian perception and prediction. In this work, we present the\nmulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated in\nsemi-structured scenes with the format of nuScenes. PFSD provides comprehensive\nmulti-modal data annotations with point cloud segmentation, detection, and\nobject IDs for tracking. It encompasses over 130,000 pedestrian instances\ncaptured across various scenarios with varying densities, movement patterns,\nand occlusions. Furthermore, to demonstrate the importance of addressing the\nchallenges posed by more diverse and complex semi-structured environments, we\npropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, to\ndetect pedestrians in densely populated and occluded scenarios, our method\neffectively captures and fuses multi-scale features using a meticulously\ndesigned hybrid framework that integrates sparse and vanilla convolutions.\nExtensive experiments on PFSD demonstrate that HMFN attains improvement in mean\nAverage Precision (mAP) over existing methods, thereby underscoring its\nefficacy in addressing the challenges of 3D pedestrian detection in complex\nsemi-structured environments. Coding and benchmark are available.\n", "link": "http://arxiv.org/abs/2502.15342v3", "date": "2025-02-26", "relevancy": 2.3363, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PFSD%3A%20A%20Multi-Modal%20Pedestrian-Focus%20Scene%20Dataset%20for%20Rich%20Tasks%20in%0A%20%20Semi-Structured%20Environments&body=Title%3A%20PFSD%3A%20A%20Multi-Modal%20Pedestrian-Focus%20Scene%20Dataset%20for%20Rich%20Tasks%20in%0A%20%20Semi-Structured%20Environments%0AAuthor%3A%20Yueting%20Liu%20and%20Hanshi%20Wang%20and%20Zhengjun%20Zha%20and%20Weiming%20Hu%20and%20Jin%20Gao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20autonomous%20driving%20perception%20have%20revealed%0Aexceptional%20capabilities%20within%20structured%20environments%20dominated%20by%20vehicular%0Atraffic.%20However%2C%20current%20perception%20models%20exhibit%20significant%20limitations%20in%0Asemi-structured%20environments%2C%20where%20dynamic%20pedestrians%20with%20more%20diverse%0Airregular%20movement%20and%20occlusion%20prevail.%20We%20attribute%20this%20shortcoming%20to%20the%0Ascarcity%20of%20high-quality%20datasets%20in%20semi-structured%20scenes%2C%20particularly%0Aconcerning%20pedestrian%20perception%20and%20prediction.%20In%20this%20work%2C%20we%20present%20the%0Amulti-modal%20Pedestrian-Focused%20Scene%20Dataset%28PFSD%29%2C%20rigorously%20annotated%20in%0Asemi-structured%20scenes%20with%20the%20format%20of%20nuScenes.%20PFSD%20provides%20comprehensive%0Amulti-modal%20data%20annotations%20with%20point%20cloud%20segmentation%2C%20detection%2C%20and%0Aobject%20IDs%20for%20tracking.%20It%20encompasses%20over%20130%2C000%20pedestrian%20instances%0Acaptured%20across%20various%20scenarios%20with%20varying%20densities%2C%20movement%20patterns%2C%0Aand%20occlusions.%20Furthermore%2C%20to%20demonstrate%20the%20importance%20of%20addressing%20the%0Achallenges%20posed%20by%20more%20diverse%20and%20complex%20semi-structured%20environments%2C%20we%0Apropose%20a%20novel%20Hybrid%20Multi-Scale%20Fusion%20Network%20%28HMFN%29.%20Specifically%2C%20to%0Adetect%20pedestrians%20in%20densely%20populated%20and%20occluded%20scenarios%2C%20our%20method%0Aeffectively%20captures%20and%20fuses%20multi-scale%20features%20using%20a%20meticulously%0Adesigned%20hybrid%20framework%20that%20integrates%20sparse%20and%20vanilla%20convolutions.%0AExtensive%20experiments%20on%20PFSD%20demonstrate%20that%20HMFN%20attains%20improvement%20in%20mean%0AAverage%20Precision%20%28mAP%29%20over%20existing%20methods%2C%20thereby%20underscoring%20its%0Aefficacy%20in%20addressing%20the%20challenges%20of%203D%20pedestrian%20detection%20in%20complex%0Asemi-structured%20environments.%20Coding%20and%20benchmark%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15342v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPFSD%253A%2520A%2520Multi-Modal%2520Pedestrian-Focus%2520Scene%2520Dataset%2520for%2520Rich%2520Tasks%2520in%250A%2520%2520Semi-Structured%2520Environments%26entry.906535625%3DYueting%2520Liu%2520and%2520Hanshi%2520Wang%2520and%2520Zhengjun%2520Zha%2520and%2520Weiming%2520Hu%2520and%2520Jin%2520Gao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520autonomous%2520driving%2520perception%2520have%2520revealed%250Aexceptional%2520capabilities%2520within%2520structured%2520environments%2520dominated%2520by%2520vehicular%250Atraffic.%2520However%252C%2520current%2520perception%2520models%2520exhibit%2520significant%2520limitations%2520in%250Asemi-structured%2520environments%252C%2520where%2520dynamic%2520pedestrians%2520with%2520more%2520diverse%250Airregular%2520movement%2520and%2520occlusion%2520prevail.%2520We%2520attribute%2520this%2520shortcoming%2520to%2520the%250Ascarcity%2520of%2520high-quality%2520datasets%2520in%2520semi-structured%2520scenes%252C%2520particularly%250Aconcerning%2520pedestrian%2520perception%2520and%2520prediction.%2520In%2520this%2520work%252C%2520we%2520present%2520the%250Amulti-modal%2520Pedestrian-Focused%2520Scene%2520Dataset%2528PFSD%2529%252C%2520rigorously%2520annotated%2520in%250Asemi-structured%2520scenes%2520with%2520the%2520format%2520of%2520nuScenes.%2520PFSD%2520provides%2520comprehensive%250Amulti-modal%2520data%2520annotations%2520with%2520point%2520cloud%2520segmentation%252C%2520detection%252C%2520and%250Aobject%2520IDs%2520for%2520tracking.%2520It%2520encompasses%2520over%2520130%252C000%2520pedestrian%2520instances%250Acaptured%2520across%2520various%2520scenarios%2520with%2520varying%2520densities%252C%2520movement%2520patterns%252C%250Aand%2520occlusions.%2520Furthermore%252C%2520to%2520demonstrate%2520the%2520importance%2520of%2520addressing%2520the%250Achallenges%2520posed%2520by%2520more%2520diverse%2520and%2520complex%2520semi-structured%2520environments%252C%2520we%250Apropose%2520a%2520novel%2520Hybrid%2520Multi-Scale%2520Fusion%2520Network%2520%2528HMFN%2529.%2520Specifically%252C%2520to%250Adetect%2520pedestrians%2520in%2520densely%2520populated%2520and%2520occluded%2520scenarios%252C%2520our%2520method%250Aeffectively%2520captures%2520and%2520fuses%2520multi-scale%2520features%2520using%2520a%2520meticulously%250Adesigned%2520hybrid%2520framework%2520that%2520integrates%2520sparse%2520and%2520vanilla%2520convolutions.%250AExtensive%2520experiments%2520on%2520PFSD%2520demonstrate%2520that%2520HMFN%2520attains%2520improvement%2520in%2520mean%250AAverage%2520Precision%2520%2528mAP%2529%2520over%2520existing%2520methods%252C%2520thereby%2520underscoring%2520its%250Aefficacy%2520in%2520addressing%2520the%2520challenges%2520of%25203D%2520pedestrian%2520detection%2520in%2520complex%250Asemi-structured%2520environments.%2520Coding%2520and%2520benchmark%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15342v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PFSD%3A%20A%20Multi-Modal%20Pedestrian-Focus%20Scene%20Dataset%20for%20Rich%20Tasks%20in%0A%20%20Semi-Structured%20Environments&entry.906535625=Yueting%20Liu%20and%20Hanshi%20Wang%20and%20Zhengjun%20Zha%20and%20Weiming%20Hu%20and%20Jin%20Gao&entry.1292438233=%20%20Recent%20advancements%20in%20autonomous%20driving%20perception%20have%20revealed%0Aexceptional%20capabilities%20within%20structured%20environments%20dominated%20by%20vehicular%0Atraffic.%20However%2C%20current%20perception%20models%20exhibit%20significant%20limitations%20in%0Asemi-structured%20environments%2C%20where%20dynamic%20pedestrians%20with%20more%20diverse%0Airregular%20movement%20and%20occlusion%20prevail.%20We%20attribute%20this%20shortcoming%20to%20the%0Ascarcity%20of%20high-quality%20datasets%20in%20semi-structured%20scenes%2C%20particularly%0Aconcerning%20pedestrian%20perception%20and%20prediction.%20In%20this%20work%2C%20we%20present%20the%0Amulti-modal%20Pedestrian-Focused%20Scene%20Dataset%28PFSD%29%2C%20rigorously%20annotated%20in%0Asemi-structured%20scenes%20with%20the%20format%20of%20nuScenes.%20PFSD%20provides%20comprehensive%0Amulti-modal%20data%20annotations%20with%20point%20cloud%20segmentation%2C%20detection%2C%20and%0Aobject%20IDs%20for%20tracking.%20It%20encompasses%20over%20130%2C000%20pedestrian%20instances%0Acaptured%20across%20various%20scenarios%20with%20varying%20densities%2C%20movement%20patterns%2C%0Aand%20occlusions.%20Furthermore%2C%20to%20demonstrate%20the%20importance%20of%20addressing%20the%0Achallenges%20posed%20by%20more%20diverse%20and%20complex%20semi-structured%20environments%2C%20we%0Apropose%20a%20novel%20Hybrid%20Multi-Scale%20Fusion%20Network%20%28HMFN%29.%20Specifically%2C%20to%0Adetect%20pedestrians%20in%20densely%20populated%20and%20occluded%20scenarios%2C%20our%20method%0Aeffectively%20captures%20and%20fuses%20multi-scale%20features%20using%20a%20meticulously%0Adesigned%20hybrid%20framework%20that%20integrates%20sparse%20and%20vanilla%20convolutions.%0AExtensive%20experiments%20on%20PFSD%20demonstrate%20that%20HMFN%20attains%20improvement%20in%20mean%0AAverage%20Precision%20%28mAP%29%20over%20existing%20methods%2C%20thereby%20underscoring%20its%0Aefficacy%20in%20addressing%20the%20challenges%20of%203D%20pedestrian%20detection%20in%20complex%0Asemi-structured%20environments.%20Coding%20and%20benchmark%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15342v3&entry.124074799=Read"},
{"title": "Increasing the Task Flexibility of Heavy-Duty Manipulators Using Visual\n  6D Pose Estimation of Objects", "author": "Petri M\u00e4kinen and Pauli Mustalahti and Tuomo Kivel\u00e4 and Jouni Mattila", "abstract": "  Recent advances in visual 6D pose estimation of objects using deep neural\nnetworks have enabled novel ways of vision-based control for heavy-duty robotic\napplications. In this study, we present a pipeline for the precise tool\npositioning of heavy-duty, long-reach (HDLR) manipulators using advanced\nmachine vision. A camera is utilized in the so-called eye-in-hand configuration\nto estimate directly the poses of a tool and a target object of interest (OOI).\nBased on the pose error between the tool and the target, along with\nmotion-based calibration between the camera and the robot, precise tool\npositioning can be reliably achieved using conventional robotic modeling and\ncontrol methods prevalent in the industry. The proposed methodology comprises\norientation and position alignment based on the visually estimated OOI poses,\nwhereas camera-to-robot calibration is conducted based on motion utilizing\nvisual SLAM. The methods seek to avert the inaccuracies resulting from\nrigid-body--based kinematics of structurally flexible HDLR manipulators via\nimage-based algorithms. To train deep neural networks for OOI pose estimation,\nonly synthetic data are utilized. The methods are validated in a real-world\nsetting using an HDLR manipulator with a 5 m reach. The experimental results\ndemonstrate that an image-based average tool positioning error of less than 2\nmm along the non-depth axes is achieved, which facilitates a new way to\nincrease the task flexibility and automation level of non-rigid HDLR\nmanipulators.\n", "link": "http://arxiv.org/abs/2502.19169v1", "date": "2025-02-26", "relevancy": 2.3203, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5846}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5826}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Increasing%20the%20Task%20Flexibility%20of%20Heavy-Duty%20Manipulators%20Using%20Visual%0A%20%206D%20Pose%20Estimation%20of%20Objects&body=Title%3A%20Increasing%20the%20Task%20Flexibility%20of%20Heavy-Duty%20Manipulators%20Using%20Visual%0A%20%206D%20Pose%20Estimation%20of%20Objects%0AAuthor%3A%20Petri%20M%C3%A4kinen%20and%20Pauli%20Mustalahti%20and%20Tuomo%20Kivel%C3%A4%20and%20Jouni%20Mattila%0AAbstract%3A%20%20%20Recent%20advances%20in%20visual%206D%20pose%20estimation%20of%20objects%20using%20deep%20neural%0Anetworks%20have%20enabled%20novel%20ways%20of%20vision-based%20control%20for%20heavy-duty%20robotic%0Aapplications.%20In%20this%20study%2C%20we%20present%20a%20pipeline%20for%20the%20precise%20tool%0Apositioning%20of%20heavy-duty%2C%20long-reach%20%28HDLR%29%20manipulators%20using%20advanced%0Amachine%20vision.%20A%20camera%20is%20utilized%20in%20the%20so-called%20eye-in-hand%20configuration%0Ato%20estimate%20directly%20the%20poses%20of%20a%20tool%20and%20a%20target%20object%20of%20interest%20%28OOI%29.%0ABased%20on%20the%20pose%20error%20between%20the%20tool%20and%20the%20target%2C%20along%20with%0Amotion-based%20calibration%20between%20the%20camera%20and%20the%20robot%2C%20precise%20tool%0Apositioning%20can%20be%20reliably%20achieved%20using%20conventional%20robotic%20modeling%20and%0Acontrol%20methods%20prevalent%20in%20the%20industry.%20The%20proposed%20methodology%20comprises%0Aorientation%20and%20position%20alignment%20based%20on%20the%20visually%20estimated%20OOI%20poses%2C%0Awhereas%20camera-to-robot%20calibration%20is%20conducted%20based%20on%20motion%20utilizing%0Avisual%20SLAM.%20The%20methods%20seek%20to%20avert%20the%20inaccuracies%20resulting%20from%0Arigid-body--based%20kinematics%20of%20structurally%20flexible%20HDLR%20manipulators%20via%0Aimage-based%20algorithms.%20To%20train%20deep%20neural%20networks%20for%20OOI%20pose%20estimation%2C%0Aonly%20synthetic%20data%20are%20utilized.%20The%20methods%20are%20validated%20in%20a%20real-world%0Asetting%20using%20an%20HDLR%20manipulator%20with%20a%205%20m%20reach.%20The%20experimental%20results%0Ademonstrate%20that%20an%20image-based%20average%20tool%20positioning%20error%20of%20less%20than%202%0Amm%20along%20the%20non-depth%20axes%20is%20achieved%2C%20which%20facilitates%20a%20new%20way%20to%0Aincrease%20the%20task%20flexibility%20and%20automation%20level%20of%20non-rigid%20HDLR%0Amanipulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncreasing%2520the%2520Task%2520Flexibility%2520of%2520Heavy-Duty%2520Manipulators%2520Using%2520Visual%250A%2520%25206D%2520Pose%2520Estimation%2520of%2520Objects%26entry.906535625%3DPetri%2520M%25C3%25A4kinen%2520and%2520Pauli%2520Mustalahti%2520and%2520Tuomo%2520Kivel%25C3%25A4%2520and%2520Jouni%2520Mattila%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520visual%25206D%2520pose%2520estimation%2520of%2520objects%2520using%2520deep%2520neural%250Anetworks%2520have%2520enabled%2520novel%2520ways%2520of%2520vision-based%2520control%2520for%2520heavy-duty%2520robotic%250Aapplications.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520pipeline%2520for%2520the%2520precise%2520tool%250Apositioning%2520of%2520heavy-duty%252C%2520long-reach%2520%2528HDLR%2529%2520manipulators%2520using%2520advanced%250Amachine%2520vision.%2520A%2520camera%2520is%2520utilized%2520in%2520the%2520so-called%2520eye-in-hand%2520configuration%250Ato%2520estimate%2520directly%2520the%2520poses%2520of%2520a%2520tool%2520and%2520a%2520target%2520object%2520of%2520interest%2520%2528OOI%2529.%250ABased%2520on%2520the%2520pose%2520error%2520between%2520the%2520tool%2520and%2520the%2520target%252C%2520along%2520with%250Amotion-based%2520calibration%2520between%2520the%2520camera%2520and%2520the%2520robot%252C%2520precise%2520tool%250Apositioning%2520can%2520be%2520reliably%2520achieved%2520using%2520conventional%2520robotic%2520modeling%2520and%250Acontrol%2520methods%2520prevalent%2520in%2520the%2520industry.%2520The%2520proposed%2520methodology%2520comprises%250Aorientation%2520and%2520position%2520alignment%2520based%2520on%2520the%2520visually%2520estimated%2520OOI%2520poses%252C%250Awhereas%2520camera-to-robot%2520calibration%2520is%2520conducted%2520based%2520on%2520motion%2520utilizing%250Avisual%2520SLAM.%2520The%2520methods%2520seek%2520to%2520avert%2520the%2520inaccuracies%2520resulting%2520from%250Arigid-body--based%2520kinematics%2520of%2520structurally%2520flexible%2520HDLR%2520manipulators%2520via%250Aimage-based%2520algorithms.%2520To%2520train%2520deep%2520neural%2520networks%2520for%2520OOI%2520pose%2520estimation%252C%250Aonly%2520synthetic%2520data%2520are%2520utilized.%2520The%2520methods%2520are%2520validated%2520in%2520a%2520real-world%250Asetting%2520using%2520an%2520HDLR%2520manipulator%2520with%2520a%25205%2520m%2520reach.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520an%2520image-based%2520average%2520tool%2520positioning%2520error%2520of%2520less%2520than%25202%250Amm%2520along%2520the%2520non-depth%2520axes%2520is%2520achieved%252C%2520which%2520facilitates%2520a%2520new%2520way%2520to%250Aincrease%2520the%2520task%2520flexibility%2520and%2520automation%2520level%2520of%2520non-rigid%2520HDLR%250Amanipulators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Increasing%20the%20Task%20Flexibility%20of%20Heavy-Duty%20Manipulators%20Using%20Visual%0A%20%206D%20Pose%20Estimation%20of%20Objects&entry.906535625=Petri%20M%C3%A4kinen%20and%20Pauli%20Mustalahti%20and%20Tuomo%20Kivel%C3%A4%20and%20Jouni%20Mattila&entry.1292438233=%20%20Recent%20advances%20in%20visual%206D%20pose%20estimation%20of%20objects%20using%20deep%20neural%0Anetworks%20have%20enabled%20novel%20ways%20of%20vision-based%20control%20for%20heavy-duty%20robotic%0Aapplications.%20In%20this%20study%2C%20we%20present%20a%20pipeline%20for%20the%20precise%20tool%0Apositioning%20of%20heavy-duty%2C%20long-reach%20%28HDLR%29%20manipulators%20using%20advanced%0Amachine%20vision.%20A%20camera%20is%20utilized%20in%20the%20so-called%20eye-in-hand%20configuration%0Ato%20estimate%20directly%20the%20poses%20of%20a%20tool%20and%20a%20target%20object%20of%20interest%20%28OOI%29.%0ABased%20on%20the%20pose%20error%20between%20the%20tool%20and%20the%20target%2C%20along%20with%0Amotion-based%20calibration%20between%20the%20camera%20and%20the%20robot%2C%20precise%20tool%0Apositioning%20can%20be%20reliably%20achieved%20using%20conventional%20robotic%20modeling%20and%0Acontrol%20methods%20prevalent%20in%20the%20industry.%20The%20proposed%20methodology%20comprises%0Aorientation%20and%20position%20alignment%20based%20on%20the%20visually%20estimated%20OOI%20poses%2C%0Awhereas%20camera-to-robot%20calibration%20is%20conducted%20based%20on%20motion%20utilizing%0Avisual%20SLAM.%20The%20methods%20seek%20to%20avert%20the%20inaccuracies%20resulting%20from%0Arigid-body--based%20kinematics%20of%20structurally%20flexible%20HDLR%20manipulators%20via%0Aimage-based%20algorithms.%20To%20train%20deep%20neural%20networks%20for%20OOI%20pose%20estimation%2C%0Aonly%20synthetic%20data%20are%20utilized.%20The%20methods%20are%20validated%20in%20a%20real-world%0Asetting%20using%20an%20HDLR%20manipulator%20with%20a%205%20m%20reach.%20The%20experimental%20results%0Ademonstrate%20that%20an%20image-based%20average%20tool%20positioning%20error%20of%20less%20than%202%0Amm%20along%20the%20non-depth%20axes%20is%20achieved%2C%20which%20facilitates%20a%20new%20way%20to%0Aincrease%20the%20task%20flexibility%20and%20automation%20level%20of%20non-rigid%20HDLR%0Amanipulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19169v1&entry.124074799=Read"},
{"title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation", "author": "Kaiwen Yan and Hongcheng Guo and Xuanqing Shi and Jingyi Xu and Yaonan Gu and Zhoujun Li", "abstract": "  With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation.\n", "link": "http://arxiv.org/abs/2502.19166v1", "date": "2025-02-26", "relevancy": 2.3082, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4715}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodeIF%3A%20Benchmarking%20the%20Instruction-Following%20Capabilities%20of%20Large%0A%20%20Language%20Models%20for%20Code%20Generation&body=Title%3A%20CodeIF%3A%20Benchmarking%20the%20Instruction-Following%20Capabilities%20of%20Large%0A%20%20Language%20Models%20for%20Code%20Generation%0AAuthor%3A%20Kaiwen%20Yan%20and%20Hongcheng%20Guo%20and%20Xuanqing%20Shi%20and%20Jingyi%20Xu%20and%20Yaonan%20Gu%20and%20Zhoujun%20Li%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20the%20demand%20for%0Arobust%20instruction-following%20capabilities%20in%20code%20generation%20tasks%20has%20grown%0Asignificantly.%20Code%20generation%20not%20only%20facilitates%20faster%20prototyping%20and%0Aautomated%20testing%2C%20but%20also%20augments%20developer%20efficiency%20through%20improved%0Amaintainability%20and%20reusability%20of%20code.%20In%20this%20paper%2C%20we%20introduce%20CodeIF%2C%0Athe%20first%20benchmark%20specifically%20designed%20to%20assess%20the%20abilities%20of%20LLMs%20to%0Aadhere%20to%20task-oriented%20instructions%20within%20diverse%20code%20generation%20scenarios.%0ACodeIF%20encompasses%20a%20broad%20range%20of%20tasks%2C%20including%20function%20synthesis%2C%20error%0Adebugging%2C%20algorithmic%20refactoring%2C%20and%20code%20explanation%2C%20thereby%20providing%20a%0Acomprehensive%20suite%20to%20evaluate%20model%20performance%20across%20varying%20complexity%0Alevels%20and%20programming%20domains.%20We%20conduct%20extensive%20experiments%20with%20LLMs%2C%0Aanalyzing%20their%20strengths%20and%20limitations%20in%20meeting%20the%20demands%20of%20these%0Atasks.%20The%20experimental%20results%20offer%20valuable%20insights%20into%20how%20well%20current%0Amodels%20align%20with%20human%20instructions%2C%20as%20well%20as%20the%20extent%20to%20which%20they%20can%0Agenerate%20consistent%2C%20maintainable%2C%20and%20contextually%20relevant%20code.%20Our%20findings%0Anot%20only%20underscore%20the%20critical%20role%20that%20instruction-following%20LLMs%20can%20play%0Ain%20modern%20software%20development%2C%20but%20also%20illuminate%20pathways%20for%20future%0Aresearch%20aimed%20at%20enhancing%20their%20adaptability%2C%20reliability%2C%20and%20overall%0Aeffectiveness%20in%20automated%20code%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodeIF%253A%2520Benchmarking%2520the%2520Instruction-Following%2520Capabilities%2520of%2520Large%250A%2520%2520Language%2520Models%2520for%2520Code%2520Generation%26entry.906535625%3DKaiwen%2520Yan%2520and%2520Hongcheng%2520Guo%2520and%2520Xuanqing%2520Shi%2520and%2520Jingyi%2520Xu%2520and%2520Yaonan%2520Gu%2520and%2520Zhoujun%2520Li%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520the%2520demand%2520for%250Arobust%2520instruction-following%2520capabilities%2520in%2520code%2520generation%2520tasks%2520has%2520grown%250Asignificantly.%2520Code%2520generation%2520not%2520only%2520facilitates%2520faster%2520prototyping%2520and%250Aautomated%2520testing%252C%2520but%2520also%2520augments%2520developer%2520efficiency%2520through%2520improved%250Amaintainability%2520and%2520reusability%2520of%2520code.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CodeIF%252C%250Athe%2520first%2520benchmark%2520specifically%2520designed%2520to%2520assess%2520the%2520abilities%2520of%2520LLMs%2520to%250Aadhere%2520to%2520task-oriented%2520instructions%2520within%2520diverse%2520code%2520generation%2520scenarios.%250ACodeIF%2520encompasses%2520a%2520broad%2520range%2520of%2520tasks%252C%2520including%2520function%2520synthesis%252C%2520error%250Adebugging%252C%2520algorithmic%2520refactoring%252C%2520and%2520code%2520explanation%252C%2520thereby%2520providing%2520a%250Acomprehensive%2520suite%2520to%2520evaluate%2520model%2520performance%2520across%2520varying%2520complexity%250Alevels%2520and%2520programming%2520domains.%2520We%2520conduct%2520extensive%2520experiments%2520with%2520LLMs%252C%250Aanalyzing%2520their%2520strengths%2520and%2520limitations%2520in%2520meeting%2520the%2520demands%2520of%2520these%250Atasks.%2520The%2520experimental%2520results%2520offer%2520valuable%2520insights%2520into%2520how%2520well%2520current%250Amodels%2520align%2520with%2520human%2520instructions%252C%2520as%2520well%2520as%2520the%2520extent%2520to%2520which%2520they%2520can%250Agenerate%2520consistent%252C%2520maintainable%252C%2520and%2520contextually%2520relevant%2520code.%2520Our%2520findings%250Anot%2520only%2520underscore%2520the%2520critical%2520role%2520that%2520instruction-following%2520LLMs%2520can%2520play%250Ain%2520modern%2520software%2520development%252C%2520but%2520also%2520illuminate%2520pathways%2520for%2520future%250Aresearch%2520aimed%2520at%2520enhancing%2520their%2520adaptability%252C%2520reliability%252C%2520and%2520overall%250Aeffectiveness%2520in%2520automated%2520code%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodeIF%3A%20Benchmarking%20the%20Instruction-Following%20Capabilities%20of%20Large%0A%20%20Language%20Models%20for%20Code%20Generation&entry.906535625=Kaiwen%20Yan%20and%20Hongcheng%20Guo%20and%20Xuanqing%20Shi%20and%20Jingyi%20Xu%20and%20Yaonan%20Gu%20and%20Zhoujun%20Li&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20the%20demand%20for%0Arobust%20instruction-following%20capabilities%20in%20code%20generation%20tasks%20has%20grown%0Asignificantly.%20Code%20generation%20not%20only%20facilitates%20faster%20prototyping%20and%0Aautomated%20testing%2C%20but%20also%20augments%20developer%20efficiency%20through%20improved%0Amaintainability%20and%20reusability%20of%20code.%20In%20this%20paper%2C%20we%20introduce%20CodeIF%2C%0Athe%20first%20benchmark%20specifically%20designed%20to%20assess%20the%20abilities%20of%20LLMs%20to%0Aadhere%20to%20task-oriented%20instructions%20within%20diverse%20code%20generation%20scenarios.%0ACodeIF%20encompasses%20a%20broad%20range%20of%20tasks%2C%20including%20function%20synthesis%2C%20error%0Adebugging%2C%20algorithmic%20refactoring%2C%20and%20code%20explanation%2C%20thereby%20providing%20a%0Acomprehensive%20suite%20to%20evaluate%20model%20performance%20across%20varying%20complexity%0Alevels%20and%20programming%20domains.%20We%20conduct%20extensive%20experiments%20with%20LLMs%2C%0Aanalyzing%20their%20strengths%20and%20limitations%20in%20meeting%20the%20demands%20of%20these%0Atasks.%20The%20experimental%20results%20offer%20valuable%20insights%20into%20how%20well%20current%0Amodels%20align%20with%20human%20instructions%2C%20as%20well%20as%20the%20extent%20to%20which%20they%20can%0Agenerate%20consistent%2C%20maintainable%2C%20and%20contextually%20relevant%20code.%20Our%20findings%0Anot%20only%20underscore%20the%20critical%20role%20that%20instruction-following%20LLMs%20can%20play%0Ain%20modern%20software%20development%2C%20but%20also%20illuminate%20pathways%20for%20future%0Aresearch%20aimed%20at%20enhancing%20their%20adaptability%2C%20reliability%2C%20and%20overall%0Aeffectiveness%20in%20automated%20code%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19166v1&entry.124074799=Read"},
{"title": "Revisiting Prefix-tuning: Statistical Benefits of Reparameterization\n  among Prompts", "author": "Minh Le and Chau Nguyen and Huy Nguyen and Quyen Tran and Trung Le and Nhat Ho", "abstract": "  Prompt-based techniques, such as prompt-tuning and prefix-tuning, have gained\nprominence for their efficiency in fine-tuning large pre-trained models.\nDespite their widespread adoption, the theoretical foundations of these methods\nremain limited. For instance, in prefix-tuning, we observe that a key factor in\nachieving performance parity with full fine-tuning lies in the\nreparameterization strategy. However, the theoretical principles underpinning\nthe effectiveness of this approach have yet to be thoroughly examined. Our\nstudy demonstrates that reparameterization is not merely an engineering trick\nbut is grounded in deep theoretical foundations. Specifically, we show that the\nreparameterization strategy implicitly encodes a shared structure between\nprefix key and value vectors. Building on recent insights into the connection\nbetween prefix-tuning and mixture of experts models, we further illustrate that\nthis shared structure significantly improves sample efficiency in parameter\nestimation compared to non-shared alternatives. The effectiveness of\nprefix-tuning across diverse tasks is empirically confirmed to be enhanced by\nthe shared structure, through extensive experiments in both visual and language\ndomains. Additionally, we uncover similar structural benefits in prompt-tuning,\noffering new perspectives on its success. Our findings provide theoretical and\nempirical contributions, advancing the understanding of prompt-based methods\nand their underlying mechanisms. Our code is publicly available at\nhttps://github.com/Minhchuyentoancbn/ReparamPrefix\n", "link": "http://arxiv.org/abs/2410.02200v3", "date": "2025-02-26", "relevancy": 2.3019, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4638}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Prefix-tuning%3A%20Statistical%20Benefits%20of%20Reparameterization%0A%20%20among%20Prompts&body=Title%3A%20Revisiting%20Prefix-tuning%3A%20Statistical%20Benefits%20of%20Reparameterization%0A%20%20among%20Prompts%0AAuthor%3A%20Minh%20Le%20and%20Chau%20Nguyen%20and%20Huy%20Nguyen%20and%20Quyen%20Tran%20and%20Trung%20Le%20and%20Nhat%20Ho%0AAbstract%3A%20%20%20Prompt-based%20techniques%2C%20such%20as%20prompt-tuning%20and%20prefix-tuning%2C%20have%20gained%0Aprominence%20for%20their%20efficiency%20in%20fine-tuning%20large%20pre-trained%20models.%0ADespite%20their%20widespread%20adoption%2C%20the%20theoretical%20foundations%20of%20these%20methods%0Aremain%20limited.%20For%20instance%2C%20in%20prefix-tuning%2C%20we%20observe%20that%20a%20key%20factor%20in%0Aachieving%20performance%20parity%20with%20full%20fine-tuning%20lies%20in%20the%0Areparameterization%20strategy.%20However%2C%20the%20theoretical%20principles%20underpinning%0Athe%20effectiveness%20of%20this%20approach%20have%20yet%20to%20be%20thoroughly%20examined.%20Our%0Astudy%20demonstrates%20that%20reparameterization%20is%20not%20merely%20an%20engineering%20trick%0Abut%20is%20grounded%20in%20deep%20theoretical%20foundations.%20Specifically%2C%20we%20show%20that%20the%0Areparameterization%20strategy%20implicitly%20encodes%20a%20shared%20structure%20between%0Aprefix%20key%20and%20value%20vectors.%20Building%20on%20recent%20insights%20into%20the%20connection%0Abetween%20prefix-tuning%20and%20mixture%20of%20experts%20models%2C%20we%20further%20illustrate%20that%0Athis%20shared%20structure%20significantly%20improves%20sample%20efficiency%20in%20parameter%0Aestimation%20compared%20to%20non-shared%20alternatives.%20The%20effectiveness%20of%0Aprefix-tuning%20across%20diverse%20tasks%20is%20empirically%20confirmed%20to%20be%20enhanced%20by%0Athe%20shared%20structure%2C%20through%20extensive%20experiments%20in%20both%20visual%20and%20language%0Adomains.%20Additionally%2C%20we%20uncover%20similar%20structural%20benefits%20in%20prompt-tuning%2C%0Aoffering%20new%20perspectives%20on%20its%20success.%20Our%20findings%20provide%20theoretical%20and%0Aempirical%20contributions%2C%20advancing%20the%20understanding%20of%20prompt-based%20methods%0Aand%20their%20underlying%20mechanisms.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Minhchuyentoancbn/ReparamPrefix%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02200v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Prefix-tuning%253A%2520Statistical%2520Benefits%2520of%2520Reparameterization%250A%2520%2520among%2520Prompts%26entry.906535625%3DMinh%2520Le%2520and%2520Chau%2520Nguyen%2520and%2520Huy%2520Nguyen%2520and%2520Quyen%2520Tran%2520and%2520Trung%2520Le%2520and%2520Nhat%2520Ho%26entry.1292438233%3D%2520%2520Prompt-based%2520techniques%252C%2520such%2520as%2520prompt-tuning%2520and%2520prefix-tuning%252C%2520have%2520gained%250Aprominence%2520for%2520their%2520efficiency%2520in%2520fine-tuning%2520large%2520pre-trained%2520models.%250ADespite%2520their%2520widespread%2520adoption%252C%2520the%2520theoretical%2520foundations%2520of%2520these%2520methods%250Aremain%2520limited.%2520For%2520instance%252C%2520in%2520prefix-tuning%252C%2520we%2520observe%2520that%2520a%2520key%2520factor%2520in%250Aachieving%2520performance%2520parity%2520with%2520full%2520fine-tuning%2520lies%2520in%2520the%250Areparameterization%2520strategy.%2520However%252C%2520the%2520theoretical%2520principles%2520underpinning%250Athe%2520effectiveness%2520of%2520this%2520approach%2520have%2520yet%2520to%2520be%2520thoroughly%2520examined.%2520Our%250Astudy%2520demonstrates%2520that%2520reparameterization%2520is%2520not%2520merely%2520an%2520engineering%2520trick%250Abut%2520is%2520grounded%2520in%2520deep%2520theoretical%2520foundations.%2520Specifically%252C%2520we%2520show%2520that%2520the%250Areparameterization%2520strategy%2520implicitly%2520encodes%2520a%2520shared%2520structure%2520between%250Aprefix%2520key%2520and%2520value%2520vectors.%2520Building%2520on%2520recent%2520insights%2520into%2520the%2520connection%250Abetween%2520prefix-tuning%2520and%2520mixture%2520of%2520experts%2520models%252C%2520we%2520further%2520illustrate%2520that%250Athis%2520shared%2520structure%2520significantly%2520improves%2520sample%2520efficiency%2520in%2520parameter%250Aestimation%2520compared%2520to%2520non-shared%2520alternatives.%2520The%2520effectiveness%2520of%250Aprefix-tuning%2520across%2520diverse%2520tasks%2520is%2520empirically%2520confirmed%2520to%2520be%2520enhanced%2520by%250Athe%2520shared%2520structure%252C%2520through%2520extensive%2520experiments%2520in%2520both%2520visual%2520and%2520language%250Adomains.%2520Additionally%252C%2520we%2520uncover%2520similar%2520structural%2520benefits%2520in%2520prompt-tuning%252C%250Aoffering%2520new%2520perspectives%2520on%2520its%2520success.%2520Our%2520findings%2520provide%2520theoretical%2520and%250Aempirical%2520contributions%252C%2520advancing%2520the%2520understanding%2520of%2520prompt-based%2520methods%250Aand%2520their%2520underlying%2520mechanisms.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Minhchuyentoancbn/ReparamPrefix%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02200v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Prefix-tuning%3A%20Statistical%20Benefits%20of%20Reparameterization%0A%20%20among%20Prompts&entry.906535625=Minh%20Le%20and%20Chau%20Nguyen%20and%20Huy%20Nguyen%20and%20Quyen%20Tran%20and%20Trung%20Le%20and%20Nhat%20Ho&entry.1292438233=%20%20Prompt-based%20techniques%2C%20such%20as%20prompt-tuning%20and%20prefix-tuning%2C%20have%20gained%0Aprominence%20for%20their%20efficiency%20in%20fine-tuning%20large%20pre-trained%20models.%0ADespite%20their%20widespread%20adoption%2C%20the%20theoretical%20foundations%20of%20these%20methods%0Aremain%20limited.%20For%20instance%2C%20in%20prefix-tuning%2C%20we%20observe%20that%20a%20key%20factor%20in%0Aachieving%20performance%20parity%20with%20full%20fine-tuning%20lies%20in%20the%0Areparameterization%20strategy.%20However%2C%20the%20theoretical%20principles%20underpinning%0Athe%20effectiveness%20of%20this%20approach%20have%20yet%20to%20be%20thoroughly%20examined.%20Our%0Astudy%20demonstrates%20that%20reparameterization%20is%20not%20merely%20an%20engineering%20trick%0Abut%20is%20grounded%20in%20deep%20theoretical%20foundations.%20Specifically%2C%20we%20show%20that%20the%0Areparameterization%20strategy%20implicitly%20encodes%20a%20shared%20structure%20between%0Aprefix%20key%20and%20value%20vectors.%20Building%20on%20recent%20insights%20into%20the%20connection%0Abetween%20prefix-tuning%20and%20mixture%20of%20experts%20models%2C%20we%20further%20illustrate%20that%0Athis%20shared%20structure%20significantly%20improves%20sample%20efficiency%20in%20parameter%0Aestimation%20compared%20to%20non-shared%20alternatives.%20The%20effectiveness%20of%0Aprefix-tuning%20across%20diverse%20tasks%20is%20empirically%20confirmed%20to%20be%20enhanced%20by%0Athe%20shared%20structure%2C%20through%20extensive%20experiments%20in%20both%20visual%20and%20language%0Adomains.%20Additionally%2C%20we%20uncover%20similar%20structural%20benefits%20in%20prompt-tuning%2C%0Aoffering%20new%20perspectives%20on%20its%20success.%20Our%20findings%20provide%20theoretical%20and%0Aempirical%20contributions%2C%20advancing%20the%20understanding%20of%20prompt-based%20methods%0Aand%20their%20underlying%20mechanisms.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Minhchuyentoancbn/ReparamPrefix%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02200v3&entry.124074799=Read"},
{"title": "An anatomically-informed correspondence initialisation method to improve\n  learning-based registration for radiotherapy", "author": "Edward G. A. Henderson and Marcel van Herk and Andrew F. Green and Eliana M. Vasquez Osorio", "abstract": "  We propose an anatomically-informed initialisation method for interpatient CT\nnon-rigid registration (NRR), using a learning-based model to estimate\ncorrespondences between organ structures. A thin plate spline (TPS)\ndeformation, set up using the correspondence predictions, is used to initialise\nthe scans before a second NRR step. We compare two established NRR methods for\nthe second step: a B-spline iterative optimisation-based algorithm and a deep\nlearning-based approach. Registration performance is evaluated with and without\nthe initialisation by assessing the similarity of propagated structures. Our\nproposed initialisation improved the registration performance of the\nlearning-based method to more closely match the traditional iterative\nalgorithm, with the mean distance-to-agreement reduced by 1.8mm for structures\nincluded in the TPS and 0.6mm for structures not included, while maintaining a\nsubstantial speed advantage (5 vs. 72 seconds).\n", "link": "http://arxiv.org/abs/2502.19101v1", "date": "2025-02-26", "relevancy": 2.3009, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4778}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4678}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20anatomically-informed%20correspondence%20initialisation%20method%20to%20improve%0A%20%20learning-based%20registration%20for%20radiotherapy&body=Title%3A%20An%20anatomically-informed%20correspondence%20initialisation%20method%20to%20improve%0A%20%20learning-based%20registration%20for%20radiotherapy%0AAuthor%3A%20Edward%20G.%20A.%20Henderson%20and%20Marcel%20van%20Herk%20and%20Andrew%20F.%20Green%20and%20Eliana%20M.%20Vasquez%20Osorio%0AAbstract%3A%20%20%20We%20propose%20an%20anatomically-informed%20initialisation%20method%20for%20interpatient%20CT%0Anon-rigid%20registration%20%28NRR%29%2C%20using%20a%20learning-based%20model%20to%20estimate%0Acorrespondences%20between%20organ%20structures.%20A%20thin%20plate%20spline%20%28TPS%29%0Adeformation%2C%20set%20up%20using%20the%20correspondence%20predictions%2C%20is%20used%20to%20initialise%0Athe%20scans%20before%20a%20second%20NRR%20step.%20We%20compare%20two%20established%20NRR%20methods%20for%0Athe%20second%20step%3A%20a%20B-spline%20iterative%20optimisation-based%20algorithm%20and%20a%20deep%0Alearning-based%20approach.%20Registration%20performance%20is%20evaluated%20with%20and%20without%0Athe%20initialisation%20by%20assessing%20the%20similarity%20of%20propagated%20structures.%20Our%0Aproposed%20initialisation%20improved%20the%20registration%20performance%20of%20the%0Alearning-based%20method%20to%20more%20closely%20match%20the%20traditional%20iterative%0Aalgorithm%2C%20with%20the%20mean%20distance-to-agreement%20reduced%20by%201.8mm%20for%20structures%0Aincluded%20in%20the%20TPS%20and%200.6mm%20for%20structures%20not%20included%2C%20while%20maintaining%20a%0Asubstantial%20speed%20advantage%20%285%20vs.%2072%20seconds%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520anatomically-informed%2520correspondence%2520initialisation%2520method%2520to%2520improve%250A%2520%2520learning-based%2520registration%2520for%2520radiotherapy%26entry.906535625%3DEdward%2520G.%2520A.%2520Henderson%2520and%2520Marcel%2520van%2520Herk%2520and%2520Andrew%2520F.%2520Green%2520and%2520Eliana%2520M.%2520Vasquez%2520Osorio%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520anatomically-informed%2520initialisation%2520method%2520for%2520interpatient%2520CT%250Anon-rigid%2520registration%2520%2528NRR%2529%252C%2520using%2520a%2520learning-based%2520model%2520to%2520estimate%250Acorrespondences%2520between%2520organ%2520structures.%2520A%2520thin%2520plate%2520spline%2520%2528TPS%2529%250Adeformation%252C%2520set%2520up%2520using%2520the%2520correspondence%2520predictions%252C%2520is%2520used%2520to%2520initialise%250Athe%2520scans%2520before%2520a%2520second%2520NRR%2520step.%2520We%2520compare%2520two%2520established%2520NRR%2520methods%2520for%250Athe%2520second%2520step%253A%2520a%2520B-spline%2520iterative%2520optimisation-based%2520algorithm%2520and%2520a%2520deep%250Alearning-based%2520approach.%2520Registration%2520performance%2520is%2520evaluated%2520with%2520and%2520without%250Athe%2520initialisation%2520by%2520assessing%2520the%2520similarity%2520of%2520propagated%2520structures.%2520Our%250Aproposed%2520initialisation%2520improved%2520the%2520registration%2520performance%2520of%2520the%250Alearning-based%2520method%2520to%2520more%2520closely%2520match%2520the%2520traditional%2520iterative%250Aalgorithm%252C%2520with%2520the%2520mean%2520distance-to-agreement%2520reduced%2520by%25201.8mm%2520for%2520structures%250Aincluded%2520in%2520the%2520TPS%2520and%25200.6mm%2520for%2520structures%2520not%2520included%252C%2520while%2520maintaining%2520a%250Asubstantial%2520speed%2520advantage%2520%25285%2520vs.%252072%2520seconds%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20anatomically-informed%20correspondence%20initialisation%20method%20to%20improve%0A%20%20learning-based%20registration%20for%20radiotherapy&entry.906535625=Edward%20G.%20A.%20Henderson%20and%20Marcel%20van%20Herk%20and%20Andrew%20F.%20Green%20and%20Eliana%20M.%20Vasquez%20Osorio&entry.1292438233=%20%20We%20propose%20an%20anatomically-informed%20initialisation%20method%20for%20interpatient%20CT%0Anon-rigid%20registration%20%28NRR%29%2C%20using%20a%20learning-based%20model%20to%20estimate%0Acorrespondences%20between%20organ%20structures.%20A%20thin%20plate%20spline%20%28TPS%29%0Adeformation%2C%20set%20up%20using%20the%20correspondence%20predictions%2C%20is%20used%20to%20initialise%0Athe%20scans%20before%20a%20second%20NRR%20step.%20We%20compare%20two%20established%20NRR%20methods%20for%0Athe%20second%20step%3A%20a%20B-spline%20iterative%20optimisation-based%20algorithm%20and%20a%20deep%0Alearning-based%20approach.%20Registration%20performance%20is%20evaluated%20with%20and%20without%0Athe%20initialisation%20by%20assessing%20the%20similarity%20of%20propagated%20structures.%20Our%0Aproposed%20initialisation%20improved%20the%20registration%20performance%20of%20the%0Alearning-based%20method%20to%20more%20closely%20match%20the%20traditional%20iterative%0Aalgorithm%2C%20with%20the%20mean%20distance-to-agreement%20reduced%20by%201.8mm%20for%20structures%0Aincluded%20in%20the%20TPS%20and%200.6mm%20for%20structures%20not%20included%2C%20while%20maintaining%20a%0Asubstantial%20speed%20advantage%20%285%20vs.%2072%20seconds%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19101v1&entry.124074799=Read"},
{"title": "Residual Speech Embeddings for Tone Classification: Removing Linguistic\n  Content to Enhance Paralinguistic Analysis", "author": "Hamdan Al Ahbabi and Gautier Marti and Saeed AlMarri and Ibrahim Elfadel", "abstract": "  Self-supervised learning models for speech processing, such as wav2vec2,\nHuBERT, WavLM, and Whisper, generate embeddings that capture both linguistic\nand paralinguistic information, making it challenging to analyze tone\nindependently of spoken content. In this work, we introduce a method for\ndisentangling paralinguistic features from linguistic content by regressing\nspeech embeddings onto their corresponding text embeddings and using the\nresiduals as a representation of vocal tone. We evaluate this approach across\nmultiple self-supervised speech embeddings, demonstrating that residual\nembeddings significantly improve tone classification performance compared to\nraw speech embeddings. Our results show that this method enhances linear\nseparability, enabling improved classification even with simple models such as\nlogistic regression. Visualization of the residual embeddings further confirms\nthe successful removal of linguistic information while preserving tone-related\nfeatures. These findings highlight the potential of residual embeddings for\napplications in sentiment analysis, speaker characterization, and\nparalinguistic speech processing.\n", "link": "http://arxiv.org/abs/2502.19387v1", "date": "2025-02-26", "relevancy": 2.2999, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4665}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Speech%20Embeddings%20for%20Tone%20Classification%3A%20Removing%20Linguistic%0A%20%20Content%20to%20Enhance%20Paralinguistic%20Analysis&body=Title%3A%20Residual%20Speech%20Embeddings%20for%20Tone%20Classification%3A%20Removing%20Linguistic%0A%20%20Content%20to%20Enhance%20Paralinguistic%20Analysis%0AAuthor%3A%20Hamdan%20Al%20Ahbabi%20and%20Gautier%20Marti%20and%20Saeed%20AlMarri%20and%20Ibrahim%20Elfadel%0AAbstract%3A%20%20%20Self-supervised%20learning%20models%20for%20speech%20processing%2C%20such%20as%20wav2vec2%2C%0AHuBERT%2C%20WavLM%2C%20and%20Whisper%2C%20generate%20embeddings%20that%20capture%20both%20linguistic%0Aand%20paralinguistic%20information%2C%20making%20it%20challenging%20to%20analyze%20tone%0Aindependently%20of%20spoken%20content.%20In%20this%20work%2C%20we%20introduce%20a%20method%20for%0Adisentangling%20paralinguistic%20features%20from%20linguistic%20content%20by%20regressing%0Aspeech%20embeddings%20onto%20their%20corresponding%20text%20embeddings%20and%20using%20the%0Aresiduals%20as%20a%20representation%20of%20vocal%20tone.%20We%20evaluate%20this%20approach%20across%0Amultiple%20self-supervised%20speech%20embeddings%2C%20demonstrating%20that%20residual%0Aembeddings%20significantly%20improve%20tone%20classification%20performance%20compared%20to%0Araw%20speech%20embeddings.%20Our%20results%20show%20that%20this%20method%20enhances%20linear%0Aseparability%2C%20enabling%20improved%20classification%20even%20with%20simple%20models%20such%20as%0Alogistic%20regression.%20Visualization%20of%20the%20residual%20embeddings%20further%20confirms%0Athe%20successful%20removal%20of%20linguistic%20information%20while%20preserving%20tone-related%0Afeatures.%20These%20findings%20highlight%20the%20potential%20of%20residual%20embeddings%20for%0Aapplications%20in%20sentiment%20analysis%2C%20speaker%20characterization%2C%20and%0Aparalinguistic%20speech%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Speech%2520Embeddings%2520for%2520Tone%2520Classification%253A%2520Removing%2520Linguistic%250A%2520%2520Content%2520to%2520Enhance%2520Paralinguistic%2520Analysis%26entry.906535625%3DHamdan%2520Al%2520Ahbabi%2520and%2520Gautier%2520Marti%2520and%2520Saeed%2520AlMarri%2520and%2520Ibrahim%2520Elfadel%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520models%2520for%2520speech%2520processing%252C%2520such%2520as%2520wav2vec2%252C%250AHuBERT%252C%2520WavLM%252C%2520and%2520Whisper%252C%2520generate%2520embeddings%2520that%2520capture%2520both%2520linguistic%250Aand%2520paralinguistic%2520information%252C%2520making%2520it%2520challenging%2520to%2520analyze%2520tone%250Aindependently%2520of%2520spoken%2520content.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520method%2520for%250Adisentangling%2520paralinguistic%2520features%2520from%2520linguistic%2520content%2520by%2520regressing%250Aspeech%2520embeddings%2520onto%2520their%2520corresponding%2520text%2520embeddings%2520and%2520using%2520the%250Aresiduals%2520as%2520a%2520representation%2520of%2520vocal%2520tone.%2520We%2520evaluate%2520this%2520approach%2520across%250Amultiple%2520self-supervised%2520speech%2520embeddings%252C%2520demonstrating%2520that%2520residual%250Aembeddings%2520significantly%2520improve%2520tone%2520classification%2520performance%2520compared%2520to%250Araw%2520speech%2520embeddings.%2520Our%2520results%2520show%2520that%2520this%2520method%2520enhances%2520linear%250Aseparability%252C%2520enabling%2520improved%2520classification%2520even%2520with%2520simple%2520models%2520such%2520as%250Alogistic%2520regression.%2520Visualization%2520of%2520the%2520residual%2520embeddings%2520further%2520confirms%250Athe%2520successful%2520removal%2520of%2520linguistic%2520information%2520while%2520preserving%2520tone-related%250Afeatures.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520residual%2520embeddings%2520for%250Aapplications%2520in%2520sentiment%2520analysis%252C%2520speaker%2520characterization%252C%2520and%250Aparalinguistic%2520speech%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Speech%20Embeddings%20for%20Tone%20Classification%3A%20Removing%20Linguistic%0A%20%20Content%20to%20Enhance%20Paralinguistic%20Analysis&entry.906535625=Hamdan%20Al%20Ahbabi%20and%20Gautier%20Marti%20and%20Saeed%20AlMarri%20and%20Ibrahim%20Elfadel&entry.1292438233=%20%20Self-supervised%20learning%20models%20for%20speech%20processing%2C%20such%20as%20wav2vec2%2C%0AHuBERT%2C%20WavLM%2C%20and%20Whisper%2C%20generate%20embeddings%20that%20capture%20both%20linguistic%0Aand%20paralinguistic%20information%2C%20making%20it%20challenging%20to%20analyze%20tone%0Aindependently%20of%20spoken%20content.%20In%20this%20work%2C%20we%20introduce%20a%20method%20for%0Adisentangling%20paralinguistic%20features%20from%20linguistic%20content%20by%20regressing%0Aspeech%20embeddings%20onto%20their%20corresponding%20text%20embeddings%20and%20using%20the%0Aresiduals%20as%20a%20representation%20of%20vocal%20tone.%20We%20evaluate%20this%20approach%20across%0Amultiple%20self-supervised%20speech%20embeddings%2C%20demonstrating%20that%20residual%0Aembeddings%20significantly%20improve%20tone%20classification%20performance%20compared%20to%0Araw%20speech%20embeddings.%20Our%20results%20show%20that%20this%20method%20enhances%20linear%0Aseparability%2C%20enabling%20improved%20classification%20even%20with%20simple%20models%20such%20as%0Alogistic%20regression.%20Visualization%20of%20the%20residual%20embeddings%20further%20confirms%0Athe%20successful%20removal%20of%20linguistic%20information%20while%20preserving%20tone-related%0Afeatures.%20These%20findings%20highlight%20the%20potential%20of%20residual%20embeddings%20for%0Aapplications%20in%20sentiment%20analysis%2C%20speaker%20characterization%2C%20and%0Aparalinguistic%20speech%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19387v1&entry.124074799=Read"},
{"title": "ProxyTransformation: Preshaping Point Cloud Manifold With Proxy\n  Attention For 3D Visual Grounding", "author": "Qihang Peng and Henry Zheng and Gao Huang", "abstract": "  Embodied intelligence requires agents to interact with 3D environments in\nreal time based on language instructions. A foundational task in this domain is\nego-centric 3D visual grounding. However, the point clouds rendered from RGB-D\nimages retain a large amount of redundant background data and inherent noise,\nboth of which can interfere with the manifold structure of the target regions.\nExisting point cloud enhancement methods often require a tedious process to\nimprove the manifold, which is not suitable for real-time tasks. We propose\nProxy Transformation suitable for multimodal task to efficiently improve the\npoint cloud manifold. Our method first leverages Deformable Point Clustering to\nidentify the point cloud sub-manifolds in target regions. Then, we propose a\nProxy Attention module that utilizes multimodal proxies to guide point cloud\ntransformation. Built upon Proxy Attention, we design a submanifold\ntransformation generation module where textual information globally guides\ntranslation vectors for different submanifolds, optimizing relative spatial\nrelationships of target regions. Simultaneously, image information guides\nlinear transformations within each submanifold, refining the local point cloud\nmanifold of target regions. Extensive experiments demonstrate that Proxy\nTransformation significantly outperforms all existing methods, achieving an\nimpressive improvement of 7.49% on easy targets and 4.60% on hard targets,\nwhile reducing the computational overhead of attention blocks by 40.6%. These\nresults establish a new SOTA in ego-centric 3D visual grounding, showcasing the\neffectiveness and robustness of our approach.\n", "link": "http://arxiv.org/abs/2502.19247v1", "date": "2025-02-26", "relevancy": 2.2955, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5799}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5746}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProxyTransformation%3A%20Preshaping%20Point%20Cloud%20Manifold%20With%20Proxy%0A%20%20Attention%20For%203D%20Visual%20Grounding&body=Title%3A%20ProxyTransformation%3A%20Preshaping%20Point%20Cloud%20Manifold%20With%20Proxy%0A%20%20Attention%20For%203D%20Visual%20Grounding%0AAuthor%3A%20Qihang%20Peng%20and%20Henry%20Zheng%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Embodied%20intelligence%20requires%20agents%20to%20interact%20with%203D%20environments%20in%0Areal%20time%20based%20on%20language%20instructions.%20A%20foundational%20task%20in%20this%20domain%20is%0Aego-centric%203D%20visual%20grounding.%20However%2C%20the%20point%20clouds%20rendered%20from%20RGB-D%0Aimages%20retain%20a%20large%20amount%20of%20redundant%20background%20data%20and%20inherent%20noise%2C%0Aboth%20of%20which%20can%20interfere%20with%20the%20manifold%20structure%20of%20the%20target%20regions.%0AExisting%20point%20cloud%20enhancement%20methods%20often%20require%20a%20tedious%20process%20to%0Aimprove%20the%20manifold%2C%20which%20is%20not%20suitable%20for%20real-time%20tasks.%20We%20propose%0AProxy%20Transformation%20suitable%20for%20multimodal%20task%20to%20efficiently%20improve%20the%0Apoint%20cloud%20manifold.%20Our%20method%20first%20leverages%20Deformable%20Point%20Clustering%20to%0Aidentify%20the%20point%20cloud%20sub-manifolds%20in%20target%20regions.%20Then%2C%20we%20propose%20a%0AProxy%20Attention%20module%20that%20utilizes%20multimodal%20proxies%20to%20guide%20point%20cloud%0Atransformation.%20Built%20upon%20Proxy%20Attention%2C%20we%20design%20a%20submanifold%0Atransformation%20generation%20module%20where%20textual%20information%20globally%20guides%0Atranslation%20vectors%20for%20different%20submanifolds%2C%20optimizing%20relative%20spatial%0Arelationships%20of%20target%20regions.%20Simultaneously%2C%20image%20information%20guides%0Alinear%20transformations%20within%20each%20submanifold%2C%20refining%20the%20local%20point%20cloud%0Amanifold%20of%20target%20regions.%20Extensive%20experiments%20demonstrate%20that%20Proxy%0ATransformation%20significantly%20outperforms%20all%20existing%20methods%2C%20achieving%20an%0Aimpressive%20improvement%20of%207.49%25%20on%20easy%20targets%20and%204.60%25%20on%20hard%20targets%2C%0Awhile%20reducing%20the%20computational%20overhead%20of%20attention%20blocks%20by%2040.6%25.%20These%0Aresults%20establish%20a%20new%20SOTA%20in%20ego-centric%203D%20visual%20grounding%2C%20showcasing%20the%0Aeffectiveness%20and%20robustness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProxyTransformation%253A%2520Preshaping%2520Point%2520Cloud%2520Manifold%2520With%2520Proxy%250A%2520%2520Attention%2520For%25203D%2520Visual%2520Grounding%26entry.906535625%3DQihang%2520Peng%2520and%2520Henry%2520Zheng%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Embodied%2520intelligence%2520requires%2520agents%2520to%2520interact%2520with%25203D%2520environments%2520in%250Areal%2520time%2520based%2520on%2520language%2520instructions.%2520A%2520foundational%2520task%2520in%2520this%2520domain%2520is%250Aego-centric%25203D%2520visual%2520grounding.%2520However%252C%2520the%2520point%2520clouds%2520rendered%2520from%2520RGB-D%250Aimages%2520retain%2520a%2520large%2520amount%2520of%2520redundant%2520background%2520data%2520and%2520inherent%2520noise%252C%250Aboth%2520of%2520which%2520can%2520interfere%2520with%2520the%2520manifold%2520structure%2520of%2520the%2520target%2520regions.%250AExisting%2520point%2520cloud%2520enhancement%2520methods%2520often%2520require%2520a%2520tedious%2520process%2520to%250Aimprove%2520the%2520manifold%252C%2520which%2520is%2520not%2520suitable%2520for%2520real-time%2520tasks.%2520We%2520propose%250AProxy%2520Transformation%2520suitable%2520for%2520multimodal%2520task%2520to%2520efficiently%2520improve%2520the%250Apoint%2520cloud%2520manifold.%2520Our%2520method%2520first%2520leverages%2520Deformable%2520Point%2520Clustering%2520to%250Aidentify%2520the%2520point%2520cloud%2520sub-manifolds%2520in%2520target%2520regions.%2520Then%252C%2520we%2520propose%2520a%250AProxy%2520Attention%2520module%2520that%2520utilizes%2520multimodal%2520proxies%2520to%2520guide%2520point%2520cloud%250Atransformation.%2520Built%2520upon%2520Proxy%2520Attention%252C%2520we%2520design%2520a%2520submanifold%250Atransformation%2520generation%2520module%2520where%2520textual%2520information%2520globally%2520guides%250Atranslation%2520vectors%2520for%2520different%2520submanifolds%252C%2520optimizing%2520relative%2520spatial%250Arelationships%2520of%2520target%2520regions.%2520Simultaneously%252C%2520image%2520information%2520guides%250Alinear%2520transformations%2520within%2520each%2520submanifold%252C%2520refining%2520the%2520local%2520point%2520cloud%250Amanifold%2520of%2520target%2520regions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Proxy%250ATransformation%2520significantly%2520outperforms%2520all%2520existing%2520methods%252C%2520achieving%2520an%250Aimpressive%2520improvement%2520of%25207.49%2525%2520on%2520easy%2520targets%2520and%25204.60%2525%2520on%2520hard%2520targets%252C%250Awhile%2520reducing%2520the%2520computational%2520overhead%2520of%2520attention%2520blocks%2520by%252040.6%2525.%2520These%250Aresults%2520establish%2520a%2520new%2520SOTA%2520in%2520ego-centric%25203D%2520visual%2520grounding%252C%2520showcasing%2520the%250Aeffectiveness%2520and%2520robustness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProxyTransformation%3A%20Preshaping%20Point%20Cloud%20Manifold%20With%20Proxy%0A%20%20Attention%20For%203D%20Visual%20Grounding&entry.906535625=Qihang%20Peng%20and%20Henry%20Zheng%20and%20Gao%20Huang&entry.1292438233=%20%20Embodied%20intelligence%20requires%20agents%20to%20interact%20with%203D%20environments%20in%0Areal%20time%20based%20on%20language%20instructions.%20A%20foundational%20task%20in%20this%20domain%20is%0Aego-centric%203D%20visual%20grounding.%20However%2C%20the%20point%20clouds%20rendered%20from%20RGB-D%0Aimages%20retain%20a%20large%20amount%20of%20redundant%20background%20data%20and%20inherent%20noise%2C%0Aboth%20of%20which%20can%20interfere%20with%20the%20manifold%20structure%20of%20the%20target%20regions.%0AExisting%20point%20cloud%20enhancement%20methods%20often%20require%20a%20tedious%20process%20to%0Aimprove%20the%20manifold%2C%20which%20is%20not%20suitable%20for%20real-time%20tasks.%20We%20propose%0AProxy%20Transformation%20suitable%20for%20multimodal%20task%20to%20efficiently%20improve%20the%0Apoint%20cloud%20manifold.%20Our%20method%20first%20leverages%20Deformable%20Point%20Clustering%20to%0Aidentify%20the%20point%20cloud%20sub-manifolds%20in%20target%20regions.%20Then%2C%20we%20propose%20a%0AProxy%20Attention%20module%20that%20utilizes%20multimodal%20proxies%20to%20guide%20point%20cloud%0Atransformation.%20Built%20upon%20Proxy%20Attention%2C%20we%20design%20a%20submanifold%0Atransformation%20generation%20module%20where%20textual%20information%20globally%20guides%0Atranslation%20vectors%20for%20different%20submanifolds%2C%20optimizing%20relative%20spatial%0Arelationships%20of%20target%20regions.%20Simultaneously%2C%20image%20information%20guides%0Alinear%20transformations%20within%20each%20submanifold%2C%20refining%20the%20local%20point%20cloud%0Amanifold%20of%20target%20regions.%20Extensive%20experiments%20demonstrate%20that%20Proxy%0ATransformation%20significantly%20outperforms%20all%20existing%20methods%2C%20achieving%20an%0Aimpressive%20improvement%20of%207.49%25%20on%20easy%20targets%20and%204.60%25%20on%20hard%20targets%2C%0Awhile%20reducing%20the%20computational%20overhead%20of%20attention%20blocks%20by%2040.6%25.%20These%0Aresults%20establish%20a%20new%20SOTA%20in%20ego-centric%203D%20visual%20grounding%2C%20showcasing%20the%0Aeffectiveness%20and%20robustness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19247v1&entry.124074799=Read"},
{"title": "Multi-view Structural Convolution Network for Domain-Invariant Point\n  Cloud Recognition of Autonomous Vehicles", "author": "Younggun Kim and Beomsik Cho and Seonghoon Ryoo and Soomok Lee", "abstract": "  Point cloud representation has recently become a research hotspot in the\nfield of computer vision and has been utilized for autonomous vehicles.\nHowever, adapting deep learning networks for point cloud data recognition is\nchallenging due to the variability in datasets and sensor technologies. This\nvariability underscores the necessity for adaptive techniques to maintain\naccuracy under different conditions. In this paper, we present the Multi-View\nStructural Convolution Network (MSCN) designed for domain-invariant point cloud\nrecognition. MSCN comprises Structural Convolution Layers (SCL) that extract\nlocal context geometric features from point clouds and Structural Aggregation\nLayers (SAL) that extract and aggregate both local and overall context features\nfrom point clouds. Additionally, our MSCN enhances feature representation\nrobustness by training with unseen domain point clouds derived from source\ndomain point clouds. This method acquires domain-invariant features and\nexhibits robust, consistent performance across various point cloud datasets,\nensuring compatibility with diverse sensor configurations without the need for\nparameter adjustments. This highlights MSCN's potential to significantly\nimprove the reliability and domain invariant features in different\nenvironments. Our code is available at https://github.com/MLMLab/MSCN.\n", "link": "http://arxiv.org/abs/2501.16289v2", "date": "2025-02-26", "relevancy": 2.2875, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.585}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5683}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles&body=Title%3A%20Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles%0AAuthor%3A%20Younggun%20Kim%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee%0AAbstract%3A%20%20%20Point%20cloud%20representation%20has%20recently%20become%20a%20research%20hotspot%20in%20the%0Afield%20of%20computer%20vision%20and%20has%20been%20utilized%20for%20autonomous%20vehicles.%0AHowever%2C%20adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20is%0Achallenging%20due%20to%20the%20variability%20in%20datasets%20and%20sensor%20technologies.%20This%0Avariability%20underscores%20the%20necessity%20for%20adaptive%20techniques%20to%20maintain%0Aaccuracy%20under%20different%20conditions.%20In%20this%20paper%2C%20we%20present%20the%20Multi-View%0AStructural%20Convolution%20Network%20%28MSCN%29%20designed%20for%20domain-invariant%20point%20cloud%0Arecognition.%20MSCN%20comprises%20Structural%20Convolution%20Layers%20%28SCL%29%20that%20extract%0Alocal%20context%20geometric%20features%20from%20point%20clouds%20and%20Structural%20Aggregation%0ALayers%20%28SAL%29%20that%20extract%20and%20aggregate%20both%20local%20and%20overall%20context%20features%0Afrom%20point%20clouds.%20Additionally%2C%20our%20MSCN%20enhances%20feature%20representation%0Arobustness%20by%20training%20with%20unseen%20domain%20point%20clouds%20derived%20from%20source%0Adomain%20point%20clouds.%20This%20method%20acquires%20domain-invariant%20features%20and%0Aexhibits%20robust%2C%20consistent%20performance%20across%20various%20point%20cloud%20datasets%2C%0Aensuring%20compatibility%20with%20diverse%20sensor%20configurations%20without%20the%20need%20for%0Aparameter%20adjustments.%20This%20highlights%20MSCN%27s%20potential%20to%20significantly%0Aimprove%20the%20reliability%20and%20domain%20invariant%20features%20in%20different%0Aenvironments.%20Our%20code%20is%20available%20at%20https%3A//github.com/MLMLab/MSCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Structural%2520Convolution%2520Network%2520for%2520Domain-Invariant%2520Point%250A%2520%2520Cloud%2520Recognition%2520of%2520Autonomous%2520Vehicles%26entry.906535625%3DYounggun%2520Kim%2520and%2520Beomsik%2520Cho%2520and%2520Seonghoon%2520Ryoo%2520and%2520Soomok%2520Lee%26entry.1292438233%3D%2520%2520Point%2520cloud%2520representation%2520has%2520recently%2520become%2520a%2520research%2520hotspot%2520in%2520the%250Afield%2520of%2520computer%2520vision%2520and%2520has%2520been%2520utilized%2520for%2520autonomous%2520vehicles.%250AHowever%252C%2520adapting%2520deep%2520learning%2520networks%2520for%2520point%2520cloud%2520data%2520recognition%2520is%250Achallenging%2520due%2520to%2520the%2520variability%2520in%2520datasets%2520and%2520sensor%2520technologies.%2520This%250Avariability%2520underscores%2520the%2520necessity%2520for%2520adaptive%2520techniques%2520to%2520maintain%250Aaccuracy%2520under%2520different%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Multi-View%250AStructural%2520Convolution%2520Network%2520%2528MSCN%2529%2520designed%2520for%2520domain-invariant%2520point%2520cloud%250Arecognition.%2520MSCN%2520comprises%2520Structural%2520Convolution%2520Layers%2520%2528SCL%2529%2520that%2520extract%250Alocal%2520context%2520geometric%2520features%2520from%2520point%2520clouds%2520and%2520Structural%2520Aggregation%250ALayers%2520%2528SAL%2529%2520that%2520extract%2520and%2520aggregate%2520both%2520local%2520and%2520overall%2520context%2520features%250Afrom%2520point%2520clouds.%2520Additionally%252C%2520our%2520MSCN%2520enhances%2520feature%2520representation%250Arobustness%2520by%2520training%2520with%2520unseen%2520domain%2520point%2520clouds%2520derived%2520from%2520source%250Adomain%2520point%2520clouds.%2520This%2520method%2520acquires%2520domain-invariant%2520features%2520and%250Aexhibits%2520robust%252C%2520consistent%2520performance%2520across%2520various%2520point%2520cloud%2520datasets%252C%250Aensuring%2520compatibility%2520with%2520diverse%2520sensor%2520configurations%2520without%2520the%2520need%2520for%250Aparameter%2520adjustments.%2520This%2520highlights%2520MSCN%2527s%2520potential%2520to%2520significantly%250Aimprove%2520the%2520reliability%2520and%2520domain%2520invariant%2520features%2520in%2520different%250Aenvironments.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/MLMLab/MSCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%20of%20Autonomous%20Vehicles&entry.906535625=Younggun%20Kim%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee&entry.1292438233=%20%20Point%20cloud%20representation%20has%20recently%20become%20a%20research%20hotspot%20in%20the%0Afield%20of%20computer%20vision%20and%20has%20been%20utilized%20for%20autonomous%20vehicles.%0AHowever%2C%20adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20is%0Achallenging%20due%20to%20the%20variability%20in%20datasets%20and%20sensor%20technologies.%20This%0Avariability%20underscores%20the%20necessity%20for%20adaptive%20techniques%20to%20maintain%0Aaccuracy%20under%20different%20conditions.%20In%20this%20paper%2C%20we%20present%20the%20Multi-View%0AStructural%20Convolution%20Network%20%28MSCN%29%20designed%20for%20domain-invariant%20point%20cloud%0Arecognition.%20MSCN%20comprises%20Structural%20Convolution%20Layers%20%28SCL%29%20that%20extract%0Alocal%20context%20geometric%20features%20from%20point%20clouds%20and%20Structural%20Aggregation%0ALayers%20%28SAL%29%20that%20extract%20and%20aggregate%20both%20local%20and%20overall%20context%20features%0Afrom%20point%20clouds.%20Additionally%2C%20our%20MSCN%20enhances%20feature%20representation%0Arobustness%20by%20training%20with%20unseen%20domain%20point%20clouds%20derived%20from%20source%0Adomain%20point%20clouds.%20This%20method%20acquires%20domain-invariant%20features%20and%0Aexhibits%20robust%2C%20consistent%20performance%20across%20various%20point%20cloud%20datasets%2C%0Aensuring%20compatibility%20with%20diverse%20sensor%20configurations%20without%20the%20need%20for%0Aparameter%20adjustments.%20This%20highlights%20MSCN%27s%20potential%20to%20significantly%0Aimprove%20the%20reliability%20and%20domain%20invariant%20features%20in%20different%0Aenvironments.%20Our%20code%20is%20available%20at%20https%3A//github.com/MLMLab/MSCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16289v2&entry.124074799=Read"},
{"title": "Fatigue-PINN: Physics-Informed Fatigue-Driven Motion Modulation and\n  Synthesis", "author": "Iliana Loi and Konstantinos Moustakas", "abstract": "  Fatigue modeling is essential for motion synthesis tasks to model human\nmotions under fatigued conditions and biomechanical engineering applications,\nsuch as investigating the variations in movement patterns and posture due to\nfatigue, defining injury risk mitigation and prevention strategies, formulating\nfatigue minimization schemes and creating improved ergonomic designs.\nNevertheless, employing data-driven methods for synthesizing the impact of\nfatigue on motion, receives little to no attention in the literature. In this\nwork, we present Fatigue-PINN, a deep learning framework based on\nPhysics-Informed Neural Networks, for modeling fatigued human movements, while\nproviding joint-specific fatigue configurations for adaptation and mitigation\nof motion artifacts on a joint level, resulting in more realistic animations.\nTo account for muscle fatigue, we simulate the fatigue-induced fluctuations in\nthe maximum exerted joint torques by leveraging a PINN adaptation of the\nThree-Compartment Controller model to exploit physics-domain knowledge for\nimproving accuracy. This model also introduces parametric motion alignment with\nrespect to joint-specific fatigue, hence avoiding sharp frame transitions. Our\nresults indicate that Fatigue-PINN accurately simulates the effects of\nexternally perceived fatigue on open-type human movements being consistent with\nfindings from real-world experimental fatigue studies. Since fatigue is\nincorporated in torque space, Fatigue-PINN provides an end-to-end\nencoder-decoder-like architecture, to ensure transforming joint angles to joint\ntorques and vice-versa, thus, being compatible with motion synthesis frameworks\noperating on joint angles.\n", "link": "http://arxiv.org/abs/2502.19056v1", "date": "2025-02-26", "relevancy": 2.2844, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.614}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5482}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fatigue-PINN%3A%20Physics-Informed%20Fatigue-Driven%20Motion%20Modulation%20and%0A%20%20Synthesis&body=Title%3A%20Fatigue-PINN%3A%20Physics-Informed%20Fatigue-Driven%20Motion%20Modulation%20and%0A%20%20Synthesis%0AAuthor%3A%20Iliana%20Loi%20and%20Konstantinos%20Moustakas%0AAbstract%3A%20%20%20Fatigue%20modeling%20is%20essential%20for%20motion%20synthesis%20tasks%20to%20model%20human%0Amotions%20under%20fatigued%20conditions%20and%20biomechanical%20engineering%20applications%2C%0Asuch%20as%20investigating%20the%20variations%20in%20movement%20patterns%20and%20posture%20due%20to%0Afatigue%2C%20defining%20injury%20risk%20mitigation%20and%20prevention%20strategies%2C%20formulating%0Afatigue%20minimization%20schemes%20and%20creating%20improved%20ergonomic%20designs.%0ANevertheless%2C%20employing%20data-driven%20methods%20for%20synthesizing%20the%20impact%20of%0Afatigue%20on%20motion%2C%20receives%20little%20to%20no%20attention%20in%20the%20literature.%20In%20this%0Awork%2C%20we%20present%20Fatigue-PINN%2C%20a%20deep%20learning%20framework%20based%20on%0APhysics-Informed%20Neural%20Networks%2C%20for%20modeling%20fatigued%20human%20movements%2C%20while%0Aproviding%20joint-specific%20fatigue%20configurations%20for%20adaptation%20and%20mitigation%0Aof%20motion%20artifacts%20on%20a%20joint%20level%2C%20resulting%20in%20more%20realistic%20animations.%0ATo%20account%20for%20muscle%20fatigue%2C%20we%20simulate%20the%20fatigue-induced%20fluctuations%20in%0Athe%20maximum%20exerted%20joint%20torques%20by%20leveraging%20a%20PINN%20adaptation%20of%20the%0AThree-Compartment%20Controller%20model%20to%20exploit%20physics-domain%20knowledge%20for%0Aimproving%20accuracy.%20This%20model%20also%20introduces%20parametric%20motion%20alignment%20with%0Arespect%20to%20joint-specific%20fatigue%2C%20hence%20avoiding%20sharp%20frame%20transitions.%20Our%0Aresults%20indicate%20that%20Fatigue-PINN%20accurately%20simulates%20the%20effects%20of%0Aexternally%20perceived%20fatigue%20on%20open-type%20human%20movements%20being%20consistent%20with%0Afindings%20from%20real-world%20experimental%20fatigue%20studies.%20Since%20fatigue%20is%0Aincorporated%20in%20torque%20space%2C%20Fatigue-PINN%20provides%20an%20end-to-end%0Aencoder-decoder-like%20architecture%2C%20to%20ensure%20transforming%20joint%20angles%20to%20joint%0Atorques%20and%20vice-versa%2C%20thus%2C%20being%20compatible%20with%20motion%20synthesis%20frameworks%0Aoperating%20on%20joint%20angles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFatigue-PINN%253A%2520Physics-Informed%2520Fatigue-Driven%2520Motion%2520Modulation%2520and%250A%2520%2520Synthesis%26entry.906535625%3DIliana%2520Loi%2520and%2520Konstantinos%2520Moustakas%26entry.1292438233%3D%2520%2520Fatigue%2520modeling%2520is%2520essential%2520for%2520motion%2520synthesis%2520tasks%2520to%2520model%2520human%250Amotions%2520under%2520fatigued%2520conditions%2520and%2520biomechanical%2520engineering%2520applications%252C%250Asuch%2520as%2520investigating%2520the%2520variations%2520in%2520movement%2520patterns%2520and%2520posture%2520due%2520to%250Afatigue%252C%2520defining%2520injury%2520risk%2520mitigation%2520and%2520prevention%2520strategies%252C%2520formulating%250Afatigue%2520minimization%2520schemes%2520and%2520creating%2520improved%2520ergonomic%2520designs.%250ANevertheless%252C%2520employing%2520data-driven%2520methods%2520for%2520synthesizing%2520the%2520impact%2520of%250Afatigue%2520on%2520motion%252C%2520receives%2520little%2520to%2520no%2520attention%2520in%2520the%2520literature.%2520In%2520this%250Awork%252C%2520we%2520present%2520Fatigue-PINN%252C%2520a%2520deep%2520learning%2520framework%2520based%2520on%250APhysics-Informed%2520Neural%2520Networks%252C%2520for%2520modeling%2520fatigued%2520human%2520movements%252C%2520while%250Aproviding%2520joint-specific%2520fatigue%2520configurations%2520for%2520adaptation%2520and%2520mitigation%250Aof%2520motion%2520artifacts%2520on%2520a%2520joint%2520level%252C%2520resulting%2520in%2520more%2520realistic%2520animations.%250ATo%2520account%2520for%2520muscle%2520fatigue%252C%2520we%2520simulate%2520the%2520fatigue-induced%2520fluctuations%2520in%250Athe%2520maximum%2520exerted%2520joint%2520torques%2520by%2520leveraging%2520a%2520PINN%2520adaptation%2520of%2520the%250AThree-Compartment%2520Controller%2520model%2520to%2520exploit%2520physics-domain%2520knowledge%2520for%250Aimproving%2520accuracy.%2520This%2520model%2520also%2520introduces%2520parametric%2520motion%2520alignment%2520with%250Arespect%2520to%2520joint-specific%2520fatigue%252C%2520hence%2520avoiding%2520sharp%2520frame%2520transitions.%2520Our%250Aresults%2520indicate%2520that%2520Fatigue-PINN%2520accurately%2520simulates%2520the%2520effects%2520of%250Aexternally%2520perceived%2520fatigue%2520on%2520open-type%2520human%2520movements%2520being%2520consistent%2520with%250Afindings%2520from%2520real-world%2520experimental%2520fatigue%2520studies.%2520Since%2520fatigue%2520is%250Aincorporated%2520in%2520torque%2520space%252C%2520Fatigue-PINN%2520provides%2520an%2520end-to-end%250Aencoder-decoder-like%2520architecture%252C%2520to%2520ensure%2520transforming%2520joint%2520angles%2520to%2520joint%250Atorques%2520and%2520vice-versa%252C%2520thus%252C%2520being%2520compatible%2520with%2520motion%2520synthesis%2520frameworks%250Aoperating%2520on%2520joint%2520angles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fatigue-PINN%3A%20Physics-Informed%20Fatigue-Driven%20Motion%20Modulation%20and%0A%20%20Synthesis&entry.906535625=Iliana%20Loi%20and%20Konstantinos%20Moustakas&entry.1292438233=%20%20Fatigue%20modeling%20is%20essential%20for%20motion%20synthesis%20tasks%20to%20model%20human%0Amotions%20under%20fatigued%20conditions%20and%20biomechanical%20engineering%20applications%2C%0Asuch%20as%20investigating%20the%20variations%20in%20movement%20patterns%20and%20posture%20due%20to%0Afatigue%2C%20defining%20injury%20risk%20mitigation%20and%20prevention%20strategies%2C%20formulating%0Afatigue%20minimization%20schemes%20and%20creating%20improved%20ergonomic%20designs.%0ANevertheless%2C%20employing%20data-driven%20methods%20for%20synthesizing%20the%20impact%20of%0Afatigue%20on%20motion%2C%20receives%20little%20to%20no%20attention%20in%20the%20literature.%20In%20this%0Awork%2C%20we%20present%20Fatigue-PINN%2C%20a%20deep%20learning%20framework%20based%20on%0APhysics-Informed%20Neural%20Networks%2C%20for%20modeling%20fatigued%20human%20movements%2C%20while%0Aproviding%20joint-specific%20fatigue%20configurations%20for%20adaptation%20and%20mitigation%0Aof%20motion%20artifacts%20on%20a%20joint%20level%2C%20resulting%20in%20more%20realistic%20animations.%0ATo%20account%20for%20muscle%20fatigue%2C%20we%20simulate%20the%20fatigue-induced%20fluctuations%20in%0Athe%20maximum%20exerted%20joint%20torques%20by%20leveraging%20a%20PINN%20adaptation%20of%20the%0AThree-Compartment%20Controller%20model%20to%20exploit%20physics-domain%20knowledge%20for%0Aimproving%20accuracy.%20This%20model%20also%20introduces%20parametric%20motion%20alignment%20with%0Arespect%20to%20joint-specific%20fatigue%2C%20hence%20avoiding%20sharp%20frame%20transitions.%20Our%0Aresults%20indicate%20that%20Fatigue-PINN%20accurately%20simulates%20the%20effects%20of%0Aexternally%20perceived%20fatigue%20on%20open-type%20human%20movements%20being%20consistent%20with%0Afindings%20from%20real-world%20experimental%20fatigue%20studies.%20Since%20fatigue%20is%0Aincorporated%20in%20torque%20space%2C%20Fatigue-PINN%20provides%20an%20end-to-end%0Aencoder-decoder-like%20architecture%2C%20to%20ensure%20transforming%20joint%20angles%20to%20joint%0Atorques%20and%20vice-versa%2C%20thus%2C%20being%20compatible%20with%20motion%20synthesis%20frameworks%0Aoperating%20on%20joint%20angles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19056v1&entry.124074799=Read"},
{"title": "Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret\n  in Noise-Free Gaussian Process Bandits", "author": "Shogo Iwazaki", "abstract": "  We study the noise-free Gaussian Process (GP) bandits problem, in which the\nlearner seeks to minimize regret through noise-free observations of the\nblack-box objective function lying on the known reproducing kernel Hilbert\nspace (RKHS). Gaussian process upper confidence bound (GP-UCB) is the\nwell-known GP-bandits algorithm whose query points are adaptively chosen based\non the GP-based upper confidence bound score. Although several existing works\nhave reported the practical success of GP-UCB, the current theoretical results\nindicate its suboptimal performance. However, GP-UCB tends to perform well\nempirically compared with other nearly optimal noise-free algorithms that rely\non a non-adaptive sampling scheme of query points. This paper resolves this gap\nbetween theoretical and empirical performance by showing the nearly optimal\nregret upper bound of noise-free GP-UCB. Specifically, our analysis shows the\nfirst constant cumulative regret in the noise-free settings for the squared\nexponential kernel and Mat\\'ern kernel with some degree of smoothness.\n", "link": "http://arxiv.org/abs/2502.19006v1", "date": "2025-02-26", "relevancy": 2.2783, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4658}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4528}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Upper%20Confidence%20Bound%20Achieves%20Nearly-Optimal%20Regret%0A%20%20in%20Noise-Free%20Gaussian%20Process%20Bandits&body=Title%3A%20Gaussian%20Process%20Upper%20Confidence%20Bound%20Achieves%20Nearly-Optimal%20Regret%0A%20%20in%20Noise-Free%20Gaussian%20Process%20Bandits%0AAuthor%3A%20Shogo%20Iwazaki%0AAbstract%3A%20%20%20We%20study%20the%20noise-free%20Gaussian%20Process%20%28GP%29%20bandits%20problem%2C%20in%20which%20the%0Alearner%20seeks%20to%20minimize%20regret%20through%20noise-free%20observations%20of%20the%0Ablack-box%20objective%20function%20lying%20on%20the%20known%20reproducing%20kernel%20Hilbert%0Aspace%20%28RKHS%29.%20Gaussian%20process%20upper%20confidence%20bound%20%28GP-UCB%29%20is%20the%0Awell-known%20GP-bandits%20algorithm%20whose%20query%20points%20are%20adaptively%20chosen%20based%0Aon%20the%20GP-based%20upper%20confidence%20bound%20score.%20Although%20several%20existing%20works%0Ahave%20reported%20the%20practical%20success%20of%20GP-UCB%2C%20the%20current%20theoretical%20results%0Aindicate%20its%20suboptimal%20performance.%20However%2C%20GP-UCB%20tends%20to%20perform%20well%0Aempirically%20compared%20with%20other%20nearly%20optimal%20noise-free%20algorithms%20that%20rely%0Aon%20a%20non-adaptive%20sampling%20scheme%20of%20query%20points.%20This%20paper%20resolves%20this%20gap%0Abetween%20theoretical%20and%20empirical%20performance%20by%20showing%20the%20nearly%20optimal%0Aregret%20upper%20bound%20of%20noise-free%20GP-UCB.%20Specifically%2C%20our%20analysis%20shows%20the%0Afirst%20constant%20cumulative%20regret%20in%20the%20noise-free%20settings%20for%20the%20squared%0Aexponential%20kernel%20and%20Mat%5C%27ern%20kernel%20with%20some%20degree%20of%20smoothness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Upper%2520Confidence%2520Bound%2520Achieves%2520Nearly-Optimal%2520Regret%250A%2520%2520in%2520Noise-Free%2520Gaussian%2520Process%2520Bandits%26entry.906535625%3DShogo%2520Iwazaki%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520noise-free%2520Gaussian%2520Process%2520%2528GP%2529%2520bandits%2520problem%252C%2520in%2520which%2520the%250Alearner%2520seeks%2520to%2520minimize%2520regret%2520through%2520noise-free%2520observations%2520of%2520the%250Ablack-box%2520objective%2520function%2520lying%2520on%2520the%2520known%2520reproducing%2520kernel%2520Hilbert%250Aspace%2520%2528RKHS%2529.%2520Gaussian%2520process%2520upper%2520confidence%2520bound%2520%2528GP-UCB%2529%2520is%2520the%250Awell-known%2520GP-bandits%2520algorithm%2520whose%2520query%2520points%2520are%2520adaptively%2520chosen%2520based%250Aon%2520the%2520GP-based%2520upper%2520confidence%2520bound%2520score.%2520Although%2520several%2520existing%2520works%250Ahave%2520reported%2520the%2520practical%2520success%2520of%2520GP-UCB%252C%2520the%2520current%2520theoretical%2520results%250Aindicate%2520its%2520suboptimal%2520performance.%2520However%252C%2520GP-UCB%2520tends%2520to%2520perform%2520well%250Aempirically%2520compared%2520with%2520other%2520nearly%2520optimal%2520noise-free%2520algorithms%2520that%2520rely%250Aon%2520a%2520non-adaptive%2520sampling%2520scheme%2520of%2520query%2520points.%2520This%2520paper%2520resolves%2520this%2520gap%250Abetween%2520theoretical%2520and%2520empirical%2520performance%2520by%2520showing%2520the%2520nearly%2520optimal%250Aregret%2520upper%2520bound%2520of%2520noise-free%2520GP-UCB.%2520Specifically%252C%2520our%2520analysis%2520shows%2520the%250Afirst%2520constant%2520cumulative%2520regret%2520in%2520the%2520noise-free%2520settings%2520for%2520the%2520squared%250Aexponential%2520kernel%2520and%2520Mat%255C%2527ern%2520kernel%2520with%2520some%2520degree%2520of%2520smoothness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Upper%20Confidence%20Bound%20Achieves%20Nearly-Optimal%20Regret%0A%20%20in%20Noise-Free%20Gaussian%20Process%20Bandits&entry.906535625=Shogo%20Iwazaki&entry.1292438233=%20%20We%20study%20the%20noise-free%20Gaussian%20Process%20%28GP%29%20bandits%20problem%2C%20in%20which%20the%0Alearner%20seeks%20to%20minimize%20regret%20through%20noise-free%20observations%20of%20the%0Ablack-box%20objective%20function%20lying%20on%20the%20known%20reproducing%20kernel%20Hilbert%0Aspace%20%28RKHS%29.%20Gaussian%20process%20upper%20confidence%20bound%20%28GP-UCB%29%20is%20the%0Awell-known%20GP-bandits%20algorithm%20whose%20query%20points%20are%20adaptively%20chosen%20based%0Aon%20the%20GP-based%20upper%20confidence%20bound%20score.%20Although%20several%20existing%20works%0Ahave%20reported%20the%20practical%20success%20of%20GP-UCB%2C%20the%20current%20theoretical%20results%0Aindicate%20its%20suboptimal%20performance.%20However%2C%20GP-UCB%20tends%20to%20perform%20well%0Aempirically%20compared%20with%20other%20nearly%20optimal%20noise-free%20algorithms%20that%20rely%0Aon%20a%20non-adaptive%20sampling%20scheme%20of%20query%20points.%20This%20paper%20resolves%20this%20gap%0Abetween%20theoretical%20and%20empirical%20performance%20by%20showing%20the%20nearly%20optimal%0Aregret%20upper%20bound%20of%20noise-free%20GP-UCB.%20Specifically%2C%20our%20analysis%20shows%20the%0Afirst%20constant%20cumulative%20regret%20in%20the%20noise-free%20settings%20for%20the%20squared%0Aexponential%20kernel%20and%20Mat%5C%27ern%20kernel%20with%20some%20degree%20of%20smoothness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19006v1&entry.124074799=Read"},
{"title": "Leg Exoskeleton Odometry using a Limited FOV Depth Sensor", "author": "Fabio Elnecave Xavier and Matis Viozelange and Guillaume Burger and Marine P\u00e9triaux and Jean-Emmanuel Deschaud and Fran\u00e7ois Goulette", "abstract": "  For leg exoskeletons to operate effectively in real-world environments, they\nmust be able to perceive and understand the terrain around them. However,\nunlike other legged robots, exoskeletons face specific constraints on where\ndepth sensors can be mounted due to the presence of a human user. These\nconstraints lead to a limited Field Of View (FOV) and greater sensor motion,\nmaking odometry particularly challenging. To address this, we propose a novel\nodometry algorithm that integrates proprioceptive data from the exoskeleton\nwith point clouds from a depth camera to produce accurate elevation maps\ndespite these limitations. Our method builds on an extended Kalman filter (EKF)\nto fuse kinematic and inertial measurements, while incorporating a tailored\niterative closest point (ICP) algorithm to register new point clouds with the\nelevation map. Experimental validation with a leg exoskeleton demonstrates that\nour approach reduces drift and enhances the quality of elevation maps compared\nto a purely proprioceptive baseline, while also outperforming a more\ntraditional point cloud map-based variant.\n", "link": "http://arxiv.org/abs/2502.19237v1", "date": "2025-02-26", "relevancy": 2.2606, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5981}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leg%20Exoskeleton%20Odometry%20using%20a%20Limited%20FOV%20Depth%20Sensor&body=Title%3A%20Leg%20Exoskeleton%20Odometry%20using%20a%20Limited%20FOV%20Depth%20Sensor%0AAuthor%3A%20Fabio%20Elnecave%20Xavier%20and%20Matis%20Viozelange%20and%20Guillaume%20Burger%20and%20Marine%20P%C3%A9triaux%20and%20Jean-Emmanuel%20Deschaud%20and%20Fran%C3%A7ois%20Goulette%0AAbstract%3A%20%20%20For%20leg%20exoskeletons%20to%20operate%20effectively%20in%20real-world%20environments%2C%20they%0Amust%20be%20able%20to%20perceive%20and%20understand%20the%20terrain%20around%20them.%20However%2C%0Aunlike%20other%20legged%20robots%2C%20exoskeletons%20face%20specific%20constraints%20on%20where%0Adepth%20sensors%20can%20be%20mounted%20due%20to%20the%20presence%20of%20a%20human%20user.%20These%0Aconstraints%20lead%20to%20a%20limited%20Field%20Of%20View%20%28FOV%29%20and%20greater%20sensor%20motion%2C%0Amaking%20odometry%20particularly%20challenging.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Aodometry%20algorithm%20that%20integrates%20proprioceptive%20data%20from%20the%20exoskeleton%0Awith%20point%20clouds%20from%20a%20depth%20camera%20to%20produce%20accurate%20elevation%20maps%0Adespite%20these%20limitations.%20Our%20method%20builds%20on%20an%20extended%20Kalman%20filter%20%28EKF%29%0Ato%20fuse%20kinematic%20and%20inertial%20measurements%2C%20while%20incorporating%20a%20tailored%0Aiterative%20closest%20point%20%28ICP%29%20algorithm%20to%20register%20new%20point%20clouds%20with%20the%0Aelevation%20map.%20Experimental%20validation%20with%20a%20leg%20exoskeleton%20demonstrates%20that%0Aour%20approach%20reduces%20drift%20and%20enhances%20the%20quality%20of%20elevation%20maps%20compared%0Ato%20a%20purely%20proprioceptive%20baseline%2C%20while%20also%20outperforming%20a%20more%0Atraditional%20point%20cloud%20map-based%20variant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeg%2520Exoskeleton%2520Odometry%2520using%2520a%2520Limited%2520FOV%2520Depth%2520Sensor%26entry.906535625%3DFabio%2520Elnecave%2520Xavier%2520and%2520Matis%2520Viozelange%2520and%2520Guillaume%2520Burger%2520and%2520Marine%2520P%25C3%25A9triaux%2520and%2520Jean-Emmanuel%2520Deschaud%2520and%2520Fran%25C3%25A7ois%2520Goulette%26entry.1292438233%3D%2520%2520For%2520leg%2520exoskeletons%2520to%2520operate%2520effectively%2520in%2520real-world%2520environments%252C%2520they%250Amust%2520be%2520able%2520to%2520perceive%2520and%2520understand%2520the%2520terrain%2520around%2520them.%2520However%252C%250Aunlike%2520other%2520legged%2520robots%252C%2520exoskeletons%2520face%2520specific%2520constraints%2520on%2520where%250Adepth%2520sensors%2520can%2520be%2520mounted%2520due%2520to%2520the%2520presence%2520of%2520a%2520human%2520user.%2520These%250Aconstraints%2520lead%2520to%2520a%2520limited%2520Field%2520Of%2520View%2520%2528FOV%2529%2520and%2520greater%2520sensor%2520motion%252C%250Amaking%2520odometry%2520particularly%2520challenging.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%250Aodometry%2520algorithm%2520that%2520integrates%2520proprioceptive%2520data%2520from%2520the%2520exoskeleton%250Awith%2520point%2520clouds%2520from%2520a%2520depth%2520camera%2520to%2520produce%2520accurate%2520elevation%2520maps%250Adespite%2520these%2520limitations.%2520Our%2520method%2520builds%2520on%2520an%2520extended%2520Kalman%2520filter%2520%2528EKF%2529%250Ato%2520fuse%2520kinematic%2520and%2520inertial%2520measurements%252C%2520while%2520incorporating%2520a%2520tailored%250Aiterative%2520closest%2520point%2520%2528ICP%2529%2520algorithm%2520to%2520register%2520new%2520point%2520clouds%2520with%2520the%250Aelevation%2520map.%2520Experimental%2520validation%2520with%2520a%2520leg%2520exoskeleton%2520demonstrates%2520that%250Aour%2520approach%2520reduces%2520drift%2520and%2520enhances%2520the%2520quality%2520of%2520elevation%2520maps%2520compared%250Ato%2520a%2520purely%2520proprioceptive%2520baseline%252C%2520while%2520also%2520outperforming%2520a%2520more%250Atraditional%2520point%2520cloud%2520map-based%2520variant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leg%20Exoskeleton%20Odometry%20using%20a%20Limited%20FOV%20Depth%20Sensor&entry.906535625=Fabio%20Elnecave%20Xavier%20and%20Matis%20Viozelange%20and%20Guillaume%20Burger%20and%20Marine%20P%C3%A9triaux%20and%20Jean-Emmanuel%20Deschaud%20and%20Fran%C3%A7ois%20Goulette&entry.1292438233=%20%20For%20leg%20exoskeletons%20to%20operate%20effectively%20in%20real-world%20environments%2C%20they%0Amust%20be%20able%20to%20perceive%20and%20understand%20the%20terrain%20around%20them.%20However%2C%0Aunlike%20other%20legged%20robots%2C%20exoskeletons%20face%20specific%20constraints%20on%20where%0Adepth%20sensors%20can%20be%20mounted%20due%20to%20the%20presence%20of%20a%20human%20user.%20These%0Aconstraints%20lead%20to%20a%20limited%20Field%20Of%20View%20%28FOV%29%20and%20greater%20sensor%20motion%2C%0Amaking%20odometry%20particularly%20challenging.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Aodometry%20algorithm%20that%20integrates%20proprioceptive%20data%20from%20the%20exoskeleton%0Awith%20point%20clouds%20from%20a%20depth%20camera%20to%20produce%20accurate%20elevation%20maps%0Adespite%20these%20limitations.%20Our%20method%20builds%20on%20an%20extended%20Kalman%20filter%20%28EKF%29%0Ato%20fuse%20kinematic%20and%20inertial%20measurements%2C%20while%20incorporating%20a%20tailored%0Aiterative%20closest%20point%20%28ICP%29%20algorithm%20to%20register%20new%20point%20clouds%20with%20the%0Aelevation%20map.%20Experimental%20validation%20with%20a%20leg%20exoskeleton%20demonstrates%20that%0Aour%20approach%20reduces%20drift%20and%20enhances%20the%20quality%20of%20elevation%20maps%20compared%0Ato%20a%20purely%20proprioceptive%20baseline%2C%20while%20also%20outperforming%20a%20more%0Atraditional%20point%20cloud%20map-based%20variant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19237v1&entry.124074799=Read"},
{"title": "Dynamic Degradation Decomposition Network for All-in-One Image\n  Restoration", "author": "Huiqiang Wang and Mingchen Song and Guoqiang Zhong", "abstract": "  Currently, restoring clean images from a variety of degradation types using a\nsingle model is still a challenging task. Existing all-in-one image restoration\napproaches struggle with addressing complex and ambiguously defined degradation\ntypes. In this paper, we introduce a dynamic degradation decomposition network\nfor all-in-one image restoration, named D$^3$Net. D$^3$Net achieves\ndegradation-adaptive image restoration with guided prompt through cross-domain\ninteraction and dynamic degradation decomposition. Concretely, in D$^3$Net, the\nproposed Cross-Domain Degradation Analyzer (CDDA) engages in deep interaction\nbetween frequency domain degradation characteristics and spatial domain image\nfeatures to identify and model variations of different degradation types on the\nimage manifold, generating degradation correction prompt and strategy prompt,\nwhich guide the following decomposition process. Furthermore, the prompt-based\nDynamic Decomposition Mechanism (DDM) for progressive degradation\ndecomposition, that encourages the network to adaptively select restoration\nstrategies utilizing the two-level prompt generated by CDDA. Thanks to the\nsynergistic cooperation between CDDA and DDM, D$^3$Net achieves superior\nflexibility and scalability in handling unknown degradation, while effectively\nreducing unnecessary computational overhead. Extensive experiments on multiple\nimage restoration tasks demonstrate that D$^3$Net significantly outperforms the\nstate-of-the-art approaches, especially improving PSNR by 5.47dB and 3.30dB on\nthe SOTS-Outdoor and GoPro datasets, respectively.\n", "link": "http://arxiv.org/abs/2502.19068v1", "date": "2025-02-26", "relevancy": 2.2605, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5777}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5641}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Degradation%20Decomposition%20Network%20for%20All-in-One%20Image%0A%20%20Restoration&body=Title%3A%20Dynamic%20Degradation%20Decomposition%20Network%20for%20All-in-One%20Image%0A%20%20Restoration%0AAuthor%3A%20Huiqiang%20Wang%20and%20Mingchen%20Song%20and%20Guoqiang%20Zhong%0AAbstract%3A%20%20%20Currently%2C%20restoring%20clean%20images%20from%20a%20variety%20of%20degradation%20types%20using%20a%0Asingle%20model%20is%20still%20a%20challenging%20task.%20Existing%20all-in-one%20image%20restoration%0Aapproaches%20struggle%20with%20addressing%20complex%20and%20ambiguously%20defined%20degradation%0Atypes.%20In%20this%20paper%2C%20we%20introduce%20a%20dynamic%20degradation%20decomposition%20network%0Afor%20all-in-one%20image%20restoration%2C%20named%20D%24%5E3%24Net.%20D%24%5E3%24Net%20achieves%0Adegradation-adaptive%20image%20restoration%20with%20guided%20prompt%20through%20cross-domain%0Ainteraction%20and%20dynamic%20degradation%20decomposition.%20Concretely%2C%20in%20D%24%5E3%24Net%2C%20the%0Aproposed%20Cross-Domain%20Degradation%20Analyzer%20%28CDDA%29%20engages%20in%20deep%20interaction%0Abetween%20frequency%20domain%20degradation%20characteristics%20and%20spatial%20domain%20image%0Afeatures%20to%20identify%20and%20model%20variations%20of%20different%20degradation%20types%20on%20the%0Aimage%20manifold%2C%20generating%20degradation%20correction%20prompt%20and%20strategy%20prompt%2C%0Awhich%20guide%20the%20following%20decomposition%20process.%20Furthermore%2C%20the%20prompt-based%0ADynamic%20Decomposition%20Mechanism%20%28DDM%29%20for%20progressive%20degradation%0Adecomposition%2C%20that%20encourages%20the%20network%20to%20adaptively%20select%20restoration%0Astrategies%20utilizing%20the%20two-level%20prompt%20generated%20by%20CDDA.%20Thanks%20to%20the%0Asynergistic%20cooperation%20between%20CDDA%20and%20DDM%2C%20D%24%5E3%24Net%20achieves%20superior%0Aflexibility%20and%20scalability%20in%20handling%20unknown%20degradation%2C%20while%20effectively%0Areducing%20unnecessary%20computational%20overhead.%20Extensive%20experiments%20on%20multiple%0Aimage%20restoration%20tasks%20demonstrate%20that%20D%24%5E3%24Net%20significantly%20outperforms%20the%0Astate-of-the-art%20approaches%2C%20especially%20improving%20PSNR%20by%205.47dB%20and%203.30dB%20on%0Athe%20SOTS-Outdoor%20and%20GoPro%20datasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Degradation%2520Decomposition%2520Network%2520for%2520All-in-One%2520Image%250A%2520%2520Restoration%26entry.906535625%3DHuiqiang%2520Wang%2520and%2520Mingchen%2520Song%2520and%2520Guoqiang%2520Zhong%26entry.1292438233%3D%2520%2520Currently%252C%2520restoring%2520clean%2520images%2520from%2520a%2520variety%2520of%2520degradation%2520types%2520using%2520a%250Asingle%2520model%2520is%2520still%2520a%2520challenging%2520task.%2520Existing%2520all-in-one%2520image%2520restoration%250Aapproaches%2520struggle%2520with%2520addressing%2520complex%2520and%2520ambiguously%2520defined%2520degradation%250Atypes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520dynamic%2520degradation%2520decomposition%2520network%250Afor%2520all-in-one%2520image%2520restoration%252C%2520named%2520D%2524%255E3%2524Net.%2520D%2524%255E3%2524Net%2520achieves%250Adegradation-adaptive%2520image%2520restoration%2520with%2520guided%2520prompt%2520through%2520cross-domain%250Ainteraction%2520and%2520dynamic%2520degradation%2520decomposition.%2520Concretely%252C%2520in%2520D%2524%255E3%2524Net%252C%2520the%250Aproposed%2520Cross-Domain%2520Degradation%2520Analyzer%2520%2528CDDA%2529%2520engages%2520in%2520deep%2520interaction%250Abetween%2520frequency%2520domain%2520degradation%2520characteristics%2520and%2520spatial%2520domain%2520image%250Afeatures%2520to%2520identify%2520and%2520model%2520variations%2520of%2520different%2520degradation%2520types%2520on%2520the%250Aimage%2520manifold%252C%2520generating%2520degradation%2520correction%2520prompt%2520and%2520strategy%2520prompt%252C%250Awhich%2520guide%2520the%2520following%2520decomposition%2520process.%2520Furthermore%252C%2520the%2520prompt-based%250ADynamic%2520Decomposition%2520Mechanism%2520%2528DDM%2529%2520for%2520progressive%2520degradation%250Adecomposition%252C%2520that%2520encourages%2520the%2520network%2520to%2520adaptively%2520select%2520restoration%250Astrategies%2520utilizing%2520the%2520two-level%2520prompt%2520generated%2520by%2520CDDA.%2520Thanks%2520to%2520the%250Asynergistic%2520cooperation%2520between%2520CDDA%2520and%2520DDM%252C%2520D%2524%255E3%2524Net%2520achieves%2520superior%250Aflexibility%2520and%2520scalability%2520in%2520handling%2520unknown%2520degradation%252C%2520while%2520effectively%250Areducing%2520unnecessary%2520computational%2520overhead.%2520Extensive%2520experiments%2520on%2520multiple%250Aimage%2520restoration%2520tasks%2520demonstrate%2520that%2520D%2524%255E3%2524Net%2520significantly%2520outperforms%2520the%250Astate-of-the-art%2520approaches%252C%2520especially%2520improving%2520PSNR%2520by%25205.47dB%2520and%25203.30dB%2520on%250Athe%2520SOTS-Outdoor%2520and%2520GoPro%2520datasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Degradation%20Decomposition%20Network%20for%20All-in-One%20Image%0A%20%20Restoration&entry.906535625=Huiqiang%20Wang%20and%20Mingchen%20Song%20and%20Guoqiang%20Zhong&entry.1292438233=%20%20Currently%2C%20restoring%20clean%20images%20from%20a%20variety%20of%20degradation%20types%20using%20a%0Asingle%20model%20is%20still%20a%20challenging%20task.%20Existing%20all-in-one%20image%20restoration%0Aapproaches%20struggle%20with%20addressing%20complex%20and%20ambiguously%20defined%20degradation%0Atypes.%20In%20this%20paper%2C%20we%20introduce%20a%20dynamic%20degradation%20decomposition%20network%0Afor%20all-in-one%20image%20restoration%2C%20named%20D%24%5E3%24Net.%20D%24%5E3%24Net%20achieves%0Adegradation-adaptive%20image%20restoration%20with%20guided%20prompt%20through%20cross-domain%0Ainteraction%20and%20dynamic%20degradation%20decomposition.%20Concretely%2C%20in%20D%24%5E3%24Net%2C%20the%0Aproposed%20Cross-Domain%20Degradation%20Analyzer%20%28CDDA%29%20engages%20in%20deep%20interaction%0Abetween%20frequency%20domain%20degradation%20characteristics%20and%20spatial%20domain%20image%0Afeatures%20to%20identify%20and%20model%20variations%20of%20different%20degradation%20types%20on%20the%0Aimage%20manifold%2C%20generating%20degradation%20correction%20prompt%20and%20strategy%20prompt%2C%0Awhich%20guide%20the%20following%20decomposition%20process.%20Furthermore%2C%20the%20prompt-based%0ADynamic%20Decomposition%20Mechanism%20%28DDM%29%20for%20progressive%20degradation%0Adecomposition%2C%20that%20encourages%20the%20network%20to%20adaptively%20select%20restoration%0Astrategies%20utilizing%20the%20two-level%20prompt%20generated%20by%20CDDA.%20Thanks%20to%20the%0Asynergistic%20cooperation%20between%20CDDA%20and%20DDM%2C%20D%24%5E3%24Net%20achieves%20superior%0Aflexibility%20and%20scalability%20in%20handling%20unknown%20degradation%2C%20while%20effectively%0Areducing%20unnecessary%20computational%20overhead.%20Extensive%20experiments%20on%20multiple%0Aimage%20restoration%20tasks%20demonstrate%20that%20D%24%5E3%24Net%20significantly%20outperforms%20the%0Astate-of-the-art%20approaches%2C%20especially%20improving%20PSNR%20by%205.47dB%20and%203.30dB%20on%0Athe%20SOTS-Outdoor%20and%20GoPro%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19068v1&entry.124074799=Read"},
{"title": "Corporate Fraud Detection in Rich-yet-Noisy Financial Graph", "author": "Shiqi Wang and Zhibo Zhang and Libing Fang and Cam-Tu Nguyen and Wenzhon Li", "abstract": "  Corporate fraud detection aims to automatically recognize companies that\nconduct wrongful activities such as fraudulent financial statements or illegal\ninsider trading. Previous learning-based methods fail to effectively integrate\nrich interactions in the company network. To close this gap, we collect 18-year\nfinancial records in China to form three graph datasets with fraud labels. We\nanalyze the characteristics of the financial graphs, highlighting two\npronounced issues: (1) information overload: the dominance of (noisy)\nnon-company nodes over company nodes hinders the message-passing process in\nGraph Convolution Networks (GCN); and (2) hidden fraud: there exists a large\npercentage of possible undetected violations in the collected data. The hidden\nfraud problem will introduce noisy labels in the training dataset and\ncompromise fraud detection results. To handle such challenges, we propose a\nnovel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage\nLearning (${\\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to\nmitigate the information overload and effectively learns rich representations.\nThe proposed model adopts a two-stage learning method to enhance robustness\nagainst hidden frauds. Extensive experimental results not only confirm the\nimportance of interactions but also show the superiority of ${\\rm KeGCN}_{R}$\nover a number of strong baselines in terms of fraud detection effectiveness and\nrobustness.\n", "link": "http://arxiv.org/abs/2502.19305v1", "date": "2025-02-26", "relevancy": 2.2581, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4574}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4529}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Corporate%20Fraud%20Detection%20in%20Rich-yet-Noisy%20Financial%20Graph&body=Title%3A%20Corporate%20Fraud%20Detection%20in%20Rich-yet-Noisy%20Financial%20Graph%0AAuthor%3A%20Shiqi%20Wang%20and%20Zhibo%20Zhang%20and%20Libing%20Fang%20and%20Cam-Tu%20Nguyen%20and%20Wenzhon%20Li%0AAbstract%3A%20%20%20Corporate%20fraud%20detection%20aims%20to%20automatically%20recognize%20companies%20that%0Aconduct%20wrongful%20activities%20such%20as%20fraudulent%20financial%20statements%20or%20illegal%0Ainsider%20trading.%20Previous%20learning-based%20methods%20fail%20to%20effectively%20integrate%0Arich%20interactions%20in%20the%20company%20network.%20To%20close%20this%20gap%2C%20we%20collect%2018-year%0Afinancial%20records%20in%20China%20to%20form%20three%20graph%20datasets%20with%20fraud%20labels.%20We%0Aanalyze%20the%20characteristics%20of%20the%20financial%20graphs%2C%20highlighting%20two%0Apronounced%20issues%3A%20%281%29%20information%20overload%3A%20the%20dominance%20of%20%28noisy%29%0Anon-company%20nodes%20over%20company%20nodes%20hinders%20the%20message-passing%20process%20in%0AGraph%20Convolution%20Networks%20%28GCN%29%3B%20and%20%282%29%20hidden%20fraud%3A%20there%20exists%20a%20large%0Apercentage%20of%20possible%20undetected%20violations%20in%20the%20collected%20data.%20The%20hidden%0Afraud%20problem%20will%20introduce%20noisy%20labels%20in%20the%20training%20dataset%20and%0Acompromise%20fraud%20detection%20results.%20To%20handle%20such%20challenges%2C%20we%20propose%20a%0Anovel%20graph-based%20method%2C%20namely%2C%20Knowledge-enhanced%20GCN%20with%20Robust%20Two-stage%0ALearning%20%28%24%7B%5Crm%20KeGCN%7D_%7BR%7D%24%29%2C%20which%20leverages%20Knowledge%20Graph%20Embeddings%20to%0Amitigate%20the%20information%20overload%20and%20effectively%20learns%20rich%20representations.%0AThe%20proposed%20model%20adopts%20a%20two-stage%20learning%20method%20to%20enhance%20robustness%0Aagainst%20hidden%20frauds.%20Extensive%20experimental%20results%20not%20only%20confirm%20the%0Aimportance%20of%20interactions%20but%20also%20show%20the%20superiority%20of%20%24%7B%5Crm%20KeGCN%7D_%7BR%7D%24%0Aover%20a%20number%20of%20strong%20baselines%20in%20terms%20of%20fraud%20detection%20effectiveness%20and%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorporate%2520Fraud%2520Detection%2520in%2520Rich-yet-Noisy%2520Financial%2520Graph%26entry.906535625%3DShiqi%2520Wang%2520and%2520Zhibo%2520Zhang%2520and%2520Libing%2520Fang%2520and%2520Cam-Tu%2520Nguyen%2520and%2520Wenzhon%2520Li%26entry.1292438233%3D%2520%2520Corporate%2520fraud%2520detection%2520aims%2520to%2520automatically%2520recognize%2520companies%2520that%250Aconduct%2520wrongful%2520activities%2520such%2520as%2520fraudulent%2520financial%2520statements%2520or%2520illegal%250Ainsider%2520trading.%2520Previous%2520learning-based%2520methods%2520fail%2520to%2520effectively%2520integrate%250Arich%2520interactions%2520in%2520the%2520company%2520network.%2520To%2520close%2520this%2520gap%252C%2520we%2520collect%252018-year%250Afinancial%2520records%2520in%2520China%2520to%2520form%2520three%2520graph%2520datasets%2520with%2520fraud%2520labels.%2520We%250Aanalyze%2520the%2520characteristics%2520of%2520the%2520financial%2520graphs%252C%2520highlighting%2520two%250Apronounced%2520issues%253A%2520%25281%2529%2520information%2520overload%253A%2520the%2520dominance%2520of%2520%2528noisy%2529%250Anon-company%2520nodes%2520over%2520company%2520nodes%2520hinders%2520the%2520message-passing%2520process%2520in%250AGraph%2520Convolution%2520Networks%2520%2528GCN%2529%253B%2520and%2520%25282%2529%2520hidden%2520fraud%253A%2520there%2520exists%2520a%2520large%250Apercentage%2520of%2520possible%2520undetected%2520violations%2520in%2520the%2520collected%2520data.%2520The%2520hidden%250Afraud%2520problem%2520will%2520introduce%2520noisy%2520labels%2520in%2520the%2520training%2520dataset%2520and%250Acompromise%2520fraud%2520detection%2520results.%2520To%2520handle%2520such%2520challenges%252C%2520we%2520propose%2520a%250Anovel%2520graph-based%2520method%252C%2520namely%252C%2520Knowledge-enhanced%2520GCN%2520with%2520Robust%2520Two-stage%250ALearning%2520%2528%2524%257B%255Crm%2520KeGCN%257D_%257BR%257D%2524%2529%252C%2520which%2520leverages%2520Knowledge%2520Graph%2520Embeddings%2520to%250Amitigate%2520the%2520information%2520overload%2520and%2520effectively%2520learns%2520rich%2520representations.%250AThe%2520proposed%2520model%2520adopts%2520a%2520two-stage%2520learning%2520method%2520to%2520enhance%2520robustness%250Aagainst%2520hidden%2520frauds.%2520Extensive%2520experimental%2520results%2520not%2520only%2520confirm%2520the%250Aimportance%2520of%2520interactions%2520but%2520also%2520show%2520the%2520superiority%2520of%2520%2524%257B%255Crm%2520KeGCN%257D_%257BR%257D%2524%250Aover%2520a%2520number%2520of%2520strong%2520baselines%2520in%2520terms%2520of%2520fraud%2520detection%2520effectiveness%2520and%250Arobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Corporate%20Fraud%20Detection%20in%20Rich-yet-Noisy%20Financial%20Graph&entry.906535625=Shiqi%20Wang%20and%20Zhibo%20Zhang%20and%20Libing%20Fang%20and%20Cam-Tu%20Nguyen%20and%20Wenzhon%20Li&entry.1292438233=%20%20Corporate%20fraud%20detection%20aims%20to%20automatically%20recognize%20companies%20that%0Aconduct%20wrongful%20activities%20such%20as%20fraudulent%20financial%20statements%20or%20illegal%0Ainsider%20trading.%20Previous%20learning-based%20methods%20fail%20to%20effectively%20integrate%0Arich%20interactions%20in%20the%20company%20network.%20To%20close%20this%20gap%2C%20we%20collect%2018-year%0Afinancial%20records%20in%20China%20to%20form%20three%20graph%20datasets%20with%20fraud%20labels.%20We%0Aanalyze%20the%20characteristics%20of%20the%20financial%20graphs%2C%20highlighting%20two%0Apronounced%20issues%3A%20%281%29%20information%20overload%3A%20the%20dominance%20of%20%28noisy%29%0Anon-company%20nodes%20over%20company%20nodes%20hinders%20the%20message-passing%20process%20in%0AGraph%20Convolution%20Networks%20%28GCN%29%3B%20and%20%282%29%20hidden%20fraud%3A%20there%20exists%20a%20large%0Apercentage%20of%20possible%20undetected%20violations%20in%20the%20collected%20data.%20The%20hidden%0Afraud%20problem%20will%20introduce%20noisy%20labels%20in%20the%20training%20dataset%20and%0Acompromise%20fraud%20detection%20results.%20To%20handle%20such%20challenges%2C%20we%20propose%20a%0Anovel%20graph-based%20method%2C%20namely%2C%20Knowledge-enhanced%20GCN%20with%20Robust%20Two-stage%0ALearning%20%28%24%7B%5Crm%20KeGCN%7D_%7BR%7D%24%29%2C%20which%20leverages%20Knowledge%20Graph%20Embeddings%20to%0Amitigate%20the%20information%20overload%20and%20effectively%20learns%20rich%20representations.%0AThe%20proposed%20model%20adopts%20a%20two-stage%20learning%20method%20to%20enhance%20robustness%0Aagainst%20hidden%20frauds.%20Extensive%20experimental%20results%20not%20only%20confirm%20the%0Aimportance%20of%20interactions%20but%20also%20show%20the%20superiority%20of%20%24%7B%5Crm%20KeGCN%7D_%7BR%7D%24%0Aover%20a%20number%20of%20strong%20baselines%20in%20terms%20of%20fraud%20detection%20effectiveness%20and%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19305v1&entry.124074799=Read"},
{"title": "Prompting Techniques for Secure Code Generation: A Systematic\n  Investigation", "author": "Catherine Tony and Nicol\u00e1s E. D\u00edaz Ferreyra and Markus Mutas and Salem Dhiff and Riccardo Scandariato", "abstract": "  Large Language Models (LLMs) are gaining momentum in software development\nwith prompt-driven programming enabling developers to create code from natural\nlanguage (NL) instructions. However, studies have questioned their ability to\nproduce secure code and, thereby, the quality of prompt-generated software.\nAlongside, various prompting techniques that carefully tailor prompts have\nemerged to elicit optimal responses from LLMs. Still, the interplay between\nsuch prompting strategies and secure code generation remains under-explored and\ncalls for further investigations. OBJECTIVE: In this study, we investigate the\nimpact of different prompting techniques on the security of code generated from\nNL instructions by LLMs. METHOD: First we perform a systematic literature\nreview to identify the existing prompting techniques that can be used for code\ngeneration tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5,\nand GPT-4 models for secure code generation. For this, we used an existing\ndataset consisting of 150 NL security-relevant code-generation prompts.\nRESULTS: Our work (i) classifies potential prompting techniques for code\ngeneration (ii) adapts and evaluates a subset of the identified techniques for\nsecure code generation tasks and (iii) observes a reduction in security\nweaknesses across the tested LLMs, especially after using an existing technique\ncalled Recursive Criticism and Improvement (RCI), contributing valuable\ninsights to the ongoing discourse on LLM-generated code security.\n", "link": "http://arxiv.org/abs/2407.07064v2", "date": "2025-02-26", "relevancy": 2.2469, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.47}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20Techniques%20for%20Secure%20Code%20Generation%3A%20A%20Systematic%0A%20%20Investigation&body=Title%3A%20Prompting%20Techniques%20for%20Secure%20Code%20Generation%3A%20A%20Systematic%0A%20%20Investigation%0AAuthor%3A%20Catherine%20Tony%20and%20Nicol%C3%A1s%20E.%20D%C3%ADaz%20Ferreyra%20and%20Markus%20Mutas%20and%20Salem%20Dhiff%20and%20Riccardo%20Scandariato%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20gaining%20momentum%20in%20software%20development%0Awith%20prompt-driven%20programming%20enabling%20developers%20to%20create%20code%20from%20natural%0Alanguage%20%28NL%29%20instructions.%20However%2C%20studies%20have%20questioned%20their%20ability%20to%0Aproduce%20secure%20code%20and%2C%20thereby%2C%20the%20quality%20of%20prompt-generated%20software.%0AAlongside%2C%20various%20prompting%20techniques%20that%20carefully%20tailor%20prompts%20have%0Aemerged%20to%20elicit%20optimal%20responses%20from%20LLMs.%20Still%2C%20the%20interplay%20between%0Asuch%20prompting%20strategies%20and%20secure%20code%20generation%20remains%20under-explored%20and%0Acalls%20for%20further%20investigations.%20OBJECTIVE%3A%20In%20this%20study%2C%20we%20investigate%20the%0Aimpact%20of%20different%20prompting%20techniques%20on%20the%20security%20of%20code%20generated%20from%0ANL%20instructions%20by%20LLMs.%20METHOD%3A%20First%20we%20perform%20a%20systematic%20literature%0Areview%20to%20identify%20the%20existing%20prompting%20techniques%20that%20can%20be%20used%20for%20code%0Ageneration%20tasks.%20A%20subset%20of%20these%20techniques%20are%20evaluated%20on%20GPT-3%2C%20GPT-3.5%2C%0Aand%20GPT-4%20models%20for%20secure%20code%20generation.%20For%20this%2C%20we%20used%20an%20existing%0Adataset%20consisting%20of%20150%20NL%20security-relevant%20code-generation%20prompts.%0ARESULTS%3A%20Our%20work%20%28i%29%20classifies%20potential%20prompting%20techniques%20for%20code%0Ageneration%20%28ii%29%20adapts%20and%20evaluates%20a%20subset%20of%20the%20identified%20techniques%20for%0Asecure%20code%20generation%20tasks%20and%20%28iii%29%20observes%20a%20reduction%20in%20security%0Aweaknesses%20across%20the%20tested%20LLMs%2C%20especially%20after%20using%20an%20existing%20technique%0Acalled%20Recursive%20Criticism%20and%20Improvement%20%28RCI%29%2C%20contributing%20valuable%0Ainsights%20to%20the%20ongoing%20discourse%20on%20LLM-generated%20code%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07064v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520Techniques%2520for%2520Secure%2520Code%2520Generation%253A%2520A%2520Systematic%250A%2520%2520Investigation%26entry.906535625%3DCatherine%2520Tony%2520and%2520Nicol%25C3%25A1s%2520E.%2520D%25C3%25ADaz%2520Ferreyra%2520and%2520Markus%2520Mutas%2520and%2520Salem%2520Dhiff%2520and%2520Riccardo%2520Scandariato%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520gaining%2520momentum%2520in%2520software%2520development%250Awith%2520prompt-driven%2520programming%2520enabling%2520developers%2520to%2520create%2520code%2520from%2520natural%250Alanguage%2520%2528NL%2529%2520instructions.%2520However%252C%2520studies%2520have%2520questioned%2520their%2520ability%2520to%250Aproduce%2520secure%2520code%2520and%252C%2520thereby%252C%2520the%2520quality%2520of%2520prompt-generated%2520software.%250AAlongside%252C%2520various%2520prompting%2520techniques%2520that%2520carefully%2520tailor%2520prompts%2520have%250Aemerged%2520to%2520elicit%2520optimal%2520responses%2520from%2520LLMs.%2520Still%252C%2520the%2520interplay%2520between%250Asuch%2520prompting%2520strategies%2520and%2520secure%2520code%2520generation%2520remains%2520under-explored%2520and%250Acalls%2520for%2520further%2520investigations.%2520OBJECTIVE%253A%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520different%2520prompting%2520techniques%2520on%2520the%2520security%2520of%2520code%2520generated%2520from%250ANL%2520instructions%2520by%2520LLMs.%2520METHOD%253A%2520First%2520we%2520perform%2520a%2520systematic%2520literature%250Areview%2520to%2520identify%2520the%2520existing%2520prompting%2520techniques%2520that%2520can%2520be%2520used%2520for%2520code%250Ageneration%2520tasks.%2520A%2520subset%2520of%2520these%2520techniques%2520are%2520evaluated%2520on%2520GPT-3%252C%2520GPT-3.5%252C%250Aand%2520GPT-4%2520models%2520for%2520secure%2520code%2520generation.%2520For%2520this%252C%2520we%2520used%2520an%2520existing%250Adataset%2520consisting%2520of%2520150%2520NL%2520security-relevant%2520code-generation%2520prompts.%250ARESULTS%253A%2520Our%2520work%2520%2528i%2529%2520classifies%2520potential%2520prompting%2520techniques%2520for%2520code%250Ageneration%2520%2528ii%2529%2520adapts%2520and%2520evaluates%2520a%2520subset%2520of%2520the%2520identified%2520techniques%2520for%250Asecure%2520code%2520generation%2520tasks%2520and%2520%2528iii%2529%2520observes%2520a%2520reduction%2520in%2520security%250Aweaknesses%2520across%2520the%2520tested%2520LLMs%252C%2520especially%2520after%2520using%2520an%2520existing%2520technique%250Acalled%2520Recursive%2520Criticism%2520and%2520Improvement%2520%2528RCI%2529%252C%2520contributing%2520valuable%250Ainsights%2520to%2520the%2520ongoing%2520discourse%2520on%2520LLM-generated%2520code%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07064v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20Techniques%20for%20Secure%20Code%20Generation%3A%20A%20Systematic%0A%20%20Investigation&entry.906535625=Catherine%20Tony%20and%20Nicol%C3%A1s%20E.%20D%C3%ADaz%20Ferreyra%20and%20Markus%20Mutas%20and%20Salem%20Dhiff%20and%20Riccardo%20Scandariato&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20gaining%20momentum%20in%20software%20development%0Awith%20prompt-driven%20programming%20enabling%20developers%20to%20create%20code%20from%20natural%0Alanguage%20%28NL%29%20instructions.%20However%2C%20studies%20have%20questioned%20their%20ability%20to%0Aproduce%20secure%20code%20and%2C%20thereby%2C%20the%20quality%20of%20prompt-generated%20software.%0AAlongside%2C%20various%20prompting%20techniques%20that%20carefully%20tailor%20prompts%20have%0Aemerged%20to%20elicit%20optimal%20responses%20from%20LLMs.%20Still%2C%20the%20interplay%20between%0Asuch%20prompting%20strategies%20and%20secure%20code%20generation%20remains%20under-explored%20and%0Acalls%20for%20further%20investigations.%20OBJECTIVE%3A%20In%20this%20study%2C%20we%20investigate%20the%0Aimpact%20of%20different%20prompting%20techniques%20on%20the%20security%20of%20code%20generated%20from%0ANL%20instructions%20by%20LLMs.%20METHOD%3A%20First%20we%20perform%20a%20systematic%20literature%0Areview%20to%20identify%20the%20existing%20prompting%20techniques%20that%20can%20be%20used%20for%20code%0Ageneration%20tasks.%20A%20subset%20of%20these%20techniques%20are%20evaluated%20on%20GPT-3%2C%20GPT-3.5%2C%0Aand%20GPT-4%20models%20for%20secure%20code%20generation.%20For%20this%2C%20we%20used%20an%20existing%0Adataset%20consisting%20of%20150%20NL%20security-relevant%20code-generation%20prompts.%0ARESULTS%3A%20Our%20work%20%28i%29%20classifies%20potential%20prompting%20techniques%20for%20code%0Ageneration%20%28ii%29%20adapts%20and%20evaluates%20a%20subset%20of%20the%20identified%20techniques%20for%0Asecure%20code%20generation%20tasks%20and%20%28iii%29%20observes%20a%20reduction%20in%20security%0Aweaknesses%20across%20the%20tested%20LLMs%2C%20especially%20after%20using%20an%20existing%20technique%0Acalled%20Recursive%20Criticism%20and%20Improvement%20%28RCI%29%2C%20contributing%20valuable%0Ainsights%20to%20the%20ongoing%20discourse%20on%20LLM-generated%20code%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07064v2&entry.124074799=Read"},
{"title": "Deep Learning For Time Series Analysis With Application On Human Motion", "author": "Ali Ismail-Fawaz", "abstract": "  Time series data, defined by equally spaced points over time, is essential in\nfields like medicine, telecommunications, and energy. Analyzing it involves\ntasks such as classification, clustering, prototyping, and regression.\nClassification identifies normal vs. abnormal movements in skeleton-based\nmotion sequences, clustering detects stock market behavior patterns,\nprototyping expands physical therapy datasets, and regression predicts patient\nrecovery. Deep learning has recently gained traction in time series analysis\ndue to its success in other domains. This thesis leverages deep learning to\nenhance classification with feature engineering, introduce foundation models,\nand develop a compact yet state-of-the-art architecture. We also address\nlimited labeled data with self-supervised learning. Our contributions apply to\nreal-world tasks, including human motion analysis for action recognition and\nrehabilitation. We introduce a generative model for human motion data, valuable\nfor cinematic production and gaming. For prototyping, we propose a shape-based\nsynthetic sample generation method to support regression models when data is\nscarce. Lastly, we critically evaluate discriminative and generative models,\nidentifying limitations in current methodologies and advocating for a robust,\nstandardized evaluation framework. Our experiments on public datasets provide\nnovel insights and methodologies, advancing time series analysis with practical\napplications.\n", "link": "http://arxiv.org/abs/2502.19364v1", "date": "2025-02-26", "relevancy": 2.242, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.593}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.542}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20For%20Time%20Series%20Analysis%20With%20Application%20On%20Human%20Motion&body=Title%3A%20Deep%20Learning%20For%20Time%20Series%20Analysis%20With%20Application%20On%20Human%20Motion%0AAuthor%3A%20Ali%20Ismail-Fawaz%0AAbstract%3A%20%20%20Time%20series%20data%2C%20defined%20by%20equally%20spaced%20points%20over%20time%2C%20is%20essential%20in%0Afields%20like%20medicine%2C%20telecommunications%2C%20and%20energy.%20Analyzing%20it%20involves%0Atasks%20such%20as%20classification%2C%20clustering%2C%20prototyping%2C%20and%20regression.%0AClassification%20identifies%20normal%20vs.%20abnormal%20movements%20in%20skeleton-based%0Amotion%20sequences%2C%20clustering%20detects%20stock%20market%20behavior%20patterns%2C%0Aprototyping%20expands%20physical%20therapy%20datasets%2C%20and%20regression%20predicts%20patient%0Arecovery.%20Deep%20learning%20has%20recently%20gained%20traction%20in%20time%20series%20analysis%0Adue%20to%20its%20success%20in%20other%20domains.%20This%20thesis%20leverages%20deep%20learning%20to%0Aenhance%20classification%20with%20feature%20engineering%2C%20introduce%20foundation%20models%2C%0Aand%20develop%20a%20compact%20yet%20state-of-the-art%20architecture.%20We%20also%20address%0Alimited%20labeled%20data%20with%20self-supervised%20learning.%20Our%20contributions%20apply%20to%0Areal-world%20tasks%2C%20including%20human%20motion%20analysis%20for%20action%20recognition%20and%0Arehabilitation.%20We%20introduce%20a%20generative%20model%20for%20human%20motion%20data%2C%20valuable%0Afor%20cinematic%20production%20and%20gaming.%20For%20prototyping%2C%20we%20propose%20a%20shape-based%0Asynthetic%20sample%20generation%20method%20to%20support%20regression%20models%20when%20data%20is%0Ascarce.%20Lastly%2C%20we%20critically%20evaluate%20discriminative%20and%20generative%20models%2C%0Aidentifying%20limitations%20in%20current%20methodologies%20and%20advocating%20for%20a%20robust%2C%0Astandardized%20evaluation%20framework.%20Our%20experiments%20on%20public%20datasets%20provide%0Anovel%20insights%20and%20methodologies%2C%20advancing%20time%20series%20analysis%20with%20practical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520For%2520Time%2520Series%2520Analysis%2520With%2520Application%2520On%2520Human%2520Motion%26entry.906535625%3DAli%2520Ismail-Fawaz%26entry.1292438233%3D%2520%2520Time%2520series%2520data%252C%2520defined%2520by%2520equally%2520spaced%2520points%2520over%2520time%252C%2520is%2520essential%2520in%250Afields%2520like%2520medicine%252C%2520telecommunications%252C%2520and%2520energy.%2520Analyzing%2520it%2520involves%250Atasks%2520such%2520as%2520classification%252C%2520clustering%252C%2520prototyping%252C%2520and%2520regression.%250AClassification%2520identifies%2520normal%2520vs.%2520abnormal%2520movements%2520in%2520skeleton-based%250Amotion%2520sequences%252C%2520clustering%2520detects%2520stock%2520market%2520behavior%2520patterns%252C%250Aprototyping%2520expands%2520physical%2520therapy%2520datasets%252C%2520and%2520regression%2520predicts%2520patient%250Arecovery.%2520Deep%2520learning%2520has%2520recently%2520gained%2520traction%2520in%2520time%2520series%2520analysis%250Adue%2520to%2520its%2520success%2520in%2520other%2520domains.%2520This%2520thesis%2520leverages%2520deep%2520learning%2520to%250Aenhance%2520classification%2520with%2520feature%2520engineering%252C%2520introduce%2520foundation%2520models%252C%250Aand%2520develop%2520a%2520compact%2520yet%2520state-of-the-art%2520architecture.%2520We%2520also%2520address%250Alimited%2520labeled%2520data%2520with%2520self-supervised%2520learning.%2520Our%2520contributions%2520apply%2520to%250Areal-world%2520tasks%252C%2520including%2520human%2520motion%2520analysis%2520for%2520action%2520recognition%2520and%250Arehabilitation.%2520We%2520introduce%2520a%2520generative%2520model%2520for%2520human%2520motion%2520data%252C%2520valuable%250Afor%2520cinematic%2520production%2520and%2520gaming.%2520For%2520prototyping%252C%2520we%2520propose%2520a%2520shape-based%250Asynthetic%2520sample%2520generation%2520method%2520to%2520support%2520regression%2520models%2520when%2520data%2520is%250Ascarce.%2520Lastly%252C%2520we%2520critically%2520evaluate%2520discriminative%2520and%2520generative%2520models%252C%250Aidentifying%2520limitations%2520in%2520current%2520methodologies%2520and%2520advocating%2520for%2520a%2520robust%252C%250Astandardized%2520evaluation%2520framework.%2520Our%2520experiments%2520on%2520public%2520datasets%2520provide%250Anovel%2520insights%2520and%2520methodologies%252C%2520advancing%2520time%2520series%2520analysis%2520with%2520practical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20For%20Time%20Series%20Analysis%20With%20Application%20On%20Human%20Motion&entry.906535625=Ali%20Ismail-Fawaz&entry.1292438233=%20%20Time%20series%20data%2C%20defined%20by%20equally%20spaced%20points%20over%20time%2C%20is%20essential%20in%0Afields%20like%20medicine%2C%20telecommunications%2C%20and%20energy.%20Analyzing%20it%20involves%0Atasks%20such%20as%20classification%2C%20clustering%2C%20prototyping%2C%20and%20regression.%0AClassification%20identifies%20normal%20vs.%20abnormal%20movements%20in%20skeleton-based%0Amotion%20sequences%2C%20clustering%20detects%20stock%20market%20behavior%20patterns%2C%0Aprototyping%20expands%20physical%20therapy%20datasets%2C%20and%20regression%20predicts%20patient%0Arecovery.%20Deep%20learning%20has%20recently%20gained%20traction%20in%20time%20series%20analysis%0Adue%20to%20its%20success%20in%20other%20domains.%20This%20thesis%20leverages%20deep%20learning%20to%0Aenhance%20classification%20with%20feature%20engineering%2C%20introduce%20foundation%20models%2C%0Aand%20develop%20a%20compact%20yet%20state-of-the-art%20architecture.%20We%20also%20address%0Alimited%20labeled%20data%20with%20self-supervised%20learning.%20Our%20contributions%20apply%20to%0Areal-world%20tasks%2C%20including%20human%20motion%20analysis%20for%20action%20recognition%20and%0Arehabilitation.%20We%20introduce%20a%20generative%20model%20for%20human%20motion%20data%2C%20valuable%0Afor%20cinematic%20production%20and%20gaming.%20For%20prototyping%2C%20we%20propose%20a%20shape-based%0Asynthetic%20sample%20generation%20method%20to%20support%20regression%20models%20when%20data%20is%0Ascarce.%20Lastly%2C%20we%20critically%20evaluate%20discriminative%20and%20generative%20models%2C%0Aidentifying%20limitations%20in%20current%20methodologies%20and%20advocating%20for%20a%20robust%2C%0Astandardized%20evaluation%20framework.%20Our%20experiments%20on%20public%20datasets%20provide%0Anovel%20insights%20and%20methodologies%2C%20advancing%20time%20series%20analysis%20with%20practical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19364v1&entry.124074799=Read"},
{"title": "Spatial-Mamba: Effective Visual State Space Models via Structure-aware\n  State Fusion", "author": "Chaodong Xiao and Minghan Li and Zhengqiang Zhang and Deyu Meng and Lei Zhang", "abstract": "  Selective state space models (SSMs), such as Mamba, highly excel at capturing\nlong-range dependencies in 1D sequential data, while their applications to 2D\nvision tasks still face challenges. Current visual SSMs often convert images\ninto 1D sequences and employ various scanning patterns to incorporate local\nspatial dependencies. However, these methods are limited in effectively\ncapturing the complex image spatial structures and the increased computational\ncost caused by the lengthened scanning paths. To address these limitations, we\npropose Spatial-Mamba, a novel approach that establishes neighborhood\nconnectivity directly in the state space. Instead of relying solely on\nsequential state transitions, we introduce a structure-aware state fusion\nequation, which leverages dilated convolutions to capture image spatial\nstructural dependencies, significantly enhancing the flow of visual contextual\ninformation. Spatial-Mamba proceeds in three stages: initial state computation\nin a unidirectional scan, spatial context acquisition through structure-aware\nstate fusion, and final state computation using the observation equation. Our\ntheoretical analysis shows that Spatial-Mamba unifies the original Mamba and\nlinear attention under the same matrix multiplication framework, providing a\ndeeper understanding of our method. Experimental results demonstrate that\nSpatial-Mamba, even with a single scan, attains or surpasses the\nstate-of-the-art SSM-based models in image classification, detection and\nsegmentation. Source codes and trained models can be found at\nhttps://github.com/EdwardChasel/Spatial-Mamba.\n", "link": "http://arxiv.org/abs/2410.15091v2", "date": "2025-02-26", "relevancy": 2.2282, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5629}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Mamba%3A%20Effective%20Visual%20State%20Space%20Models%20via%20Structure-aware%0A%20%20State%20Fusion&body=Title%3A%20Spatial-Mamba%3A%20Effective%20Visual%20State%20Space%20Models%20via%20Structure-aware%0A%20%20State%20Fusion%0AAuthor%3A%20Chaodong%20Xiao%20and%20Minghan%20Li%20and%20Zhengqiang%20Zhang%20and%20Deyu%20Meng%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Selective%20state%20space%20models%20%28SSMs%29%2C%20such%20as%20Mamba%2C%20highly%20excel%20at%20capturing%0Along-range%20dependencies%20in%201D%20sequential%20data%2C%20while%20their%20applications%20to%202D%0Avision%20tasks%20still%20face%20challenges.%20Current%20visual%20SSMs%20often%20convert%20images%0Ainto%201D%20sequences%20and%20employ%20various%20scanning%20patterns%20to%20incorporate%20local%0Aspatial%20dependencies.%20However%2C%20these%20methods%20are%20limited%20in%20effectively%0Acapturing%20the%20complex%20image%20spatial%20structures%20and%20the%20increased%20computational%0Acost%20caused%20by%20the%20lengthened%20scanning%20paths.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Spatial-Mamba%2C%20a%20novel%20approach%20that%20establishes%20neighborhood%0Aconnectivity%20directly%20in%20the%20state%20space.%20Instead%20of%20relying%20solely%20on%0Asequential%20state%20transitions%2C%20we%20introduce%20a%20structure-aware%20state%20fusion%0Aequation%2C%20which%20leverages%20dilated%20convolutions%20to%20capture%20image%20spatial%0Astructural%20dependencies%2C%20significantly%20enhancing%20the%20flow%20of%20visual%20contextual%0Ainformation.%20Spatial-Mamba%20proceeds%20in%20three%20stages%3A%20initial%20state%20computation%0Ain%20a%20unidirectional%20scan%2C%20spatial%20context%20acquisition%20through%20structure-aware%0Astate%20fusion%2C%20and%20final%20state%20computation%20using%20the%20observation%20equation.%20Our%0Atheoretical%20analysis%20shows%20that%20Spatial-Mamba%20unifies%20the%20original%20Mamba%20and%0Alinear%20attention%20under%20the%20same%20matrix%20multiplication%20framework%2C%20providing%20a%0Adeeper%20understanding%20of%20our%20method.%20Experimental%20results%20demonstrate%20that%0ASpatial-Mamba%2C%20even%20with%20a%20single%20scan%2C%20attains%20or%20surpasses%20the%0Astate-of-the-art%20SSM-based%20models%20in%20image%20classification%2C%20detection%20and%0Asegmentation.%20Source%20codes%20and%20trained%20models%20can%20be%20found%20at%0Ahttps%3A//github.com/EdwardChasel/Spatial-Mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15091v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Mamba%253A%2520Effective%2520Visual%2520State%2520Space%2520Models%2520via%2520Structure-aware%250A%2520%2520State%2520Fusion%26entry.906535625%3DChaodong%2520Xiao%2520and%2520Minghan%2520Li%2520and%2520Zhengqiang%2520Zhang%2520and%2520Deyu%2520Meng%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Selective%2520state%2520space%2520models%2520%2528SSMs%2529%252C%2520such%2520as%2520Mamba%252C%2520highly%2520excel%2520at%2520capturing%250Along-range%2520dependencies%2520in%25201D%2520sequential%2520data%252C%2520while%2520their%2520applications%2520to%25202D%250Avision%2520tasks%2520still%2520face%2520challenges.%2520Current%2520visual%2520SSMs%2520often%2520convert%2520images%250Ainto%25201D%2520sequences%2520and%2520employ%2520various%2520scanning%2520patterns%2520to%2520incorporate%2520local%250Aspatial%2520dependencies.%2520However%252C%2520these%2520methods%2520are%2520limited%2520in%2520effectively%250Acapturing%2520the%2520complex%2520image%2520spatial%2520structures%2520and%2520the%2520increased%2520computational%250Acost%2520caused%2520by%2520the%2520lengthened%2520scanning%2520paths.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520Spatial-Mamba%252C%2520a%2520novel%2520approach%2520that%2520establishes%2520neighborhood%250Aconnectivity%2520directly%2520in%2520the%2520state%2520space.%2520Instead%2520of%2520relying%2520solely%2520on%250Asequential%2520state%2520transitions%252C%2520we%2520introduce%2520a%2520structure-aware%2520state%2520fusion%250Aequation%252C%2520which%2520leverages%2520dilated%2520convolutions%2520to%2520capture%2520image%2520spatial%250Astructural%2520dependencies%252C%2520significantly%2520enhancing%2520the%2520flow%2520of%2520visual%2520contextual%250Ainformation.%2520Spatial-Mamba%2520proceeds%2520in%2520three%2520stages%253A%2520initial%2520state%2520computation%250Ain%2520a%2520unidirectional%2520scan%252C%2520spatial%2520context%2520acquisition%2520through%2520structure-aware%250Astate%2520fusion%252C%2520and%2520final%2520state%2520computation%2520using%2520the%2520observation%2520equation.%2520Our%250Atheoretical%2520analysis%2520shows%2520that%2520Spatial-Mamba%2520unifies%2520the%2520original%2520Mamba%2520and%250Alinear%2520attention%2520under%2520the%2520same%2520matrix%2520multiplication%2520framework%252C%2520providing%2520a%250Adeeper%2520understanding%2520of%2520our%2520method.%2520Experimental%2520results%2520demonstrate%2520that%250ASpatial-Mamba%252C%2520even%2520with%2520a%2520single%2520scan%252C%2520attains%2520or%2520surpasses%2520the%250Astate-of-the-art%2520SSM-based%2520models%2520in%2520image%2520classification%252C%2520detection%2520and%250Asegmentation.%2520Source%2520codes%2520and%2520trained%2520models%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/EdwardChasel/Spatial-Mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15091v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Mamba%3A%20Effective%20Visual%20State%20Space%20Models%20via%20Structure-aware%0A%20%20State%20Fusion&entry.906535625=Chaodong%20Xiao%20and%20Minghan%20Li%20and%20Zhengqiang%20Zhang%20and%20Deyu%20Meng%20and%20Lei%20Zhang&entry.1292438233=%20%20Selective%20state%20space%20models%20%28SSMs%29%2C%20such%20as%20Mamba%2C%20highly%20excel%20at%20capturing%0Along-range%20dependencies%20in%201D%20sequential%20data%2C%20while%20their%20applications%20to%202D%0Avision%20tasks%20still%20face%20challenges.%20Current%20visual%20SSMs%20often%20convert%20images%0Ainto%201D%20sequences%20and%20employ%20various%20scanning%20patterns%20to%20incorporate%20local%0Aspatial%20dependencies.%20However%2C%20these%20methods%20are%20limited%20in%20effectively%0Acapturing%20the%20complex%20image%20spatial%20structures%20and%20the%20increased%20computational%0Acost%20caused%20by%20the%20lengthened%20scanning%20paths.%20To%20address%20these%20limitations%2C%20we%0Apropose%20Spatial-Mamba%2C%20a%20novel%20approach%20that%20establishes%20neighborhood%0Aconnectivity%20directly%20in%20the%20state%20space.%20Instead%20of%20relying%20solely%20on%0Asequential%20state%20transitions%2C%20we%20introduce%20a%20structure-aware%20state%20fusion%0Aequation%2C%20which%20leverages%20dilated%20convolutions%20to%20capture%20image%20spatial%0Astructural%20dependencies%2C%20significantly%20enhancing%20the%20flow%20of%20visual%20contextual%0Ainformation.%20Spatial-Mamba%20proceeds%20in%20three%20stages%3A%20initial%20state%20computation%0Ain%20a%20unidirectional%20scan%2C%20spatial%20context%20acquisition%20through%20structure-aware%0Astate%20fusion%2C%20and%20final%20state%20computation%20using%20the%20observation%20equation.%20Our%0Atheoretical%20analysis%20shows%20that%20Spatial-Mamba%20unifies%20the%20original%20Mamba%20and%0Alinear%20attention%20under%20the%20same%20matrix%20multiplication%20framework%2C%20providing%20a%0Adeeper%20understanding%20of%20our%20method.%20Experimental%20results%20demonstrate%20that%0ASpatial-Mamba%2C%20even%20with%20a%20single%20scan%2C%20attains%20or%20surpasses%20the%0Astate-of-the-art%20SSM-based%20models%20in%20image%20classification%2C%20detection%20and%0Asegmentation.%20Source%20codes%20and%20trained%20models%20can%20be%20found%20at%0Ahttps%3A//github.com/EdwardChasel/Spatial-Mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15091v2&entry.124074799=Read"},
{"title": "Potential Field as Scene Affordance for Behavior Change-Based Visual\n  Risk Object Identification", "author": "Pang-Yuan Pao and Shu-Wei Lu and Ze-Yan Lu and Yi-Ting Chen", "abstract": "  We study behavior change-based visual risk object identification\n(Visual-ROI), a critical framework designed to detect potential hazards for\nintelligent driving systems. Existing methods often show significant\nlimitations in spatial accuracy and temporal consistency, stemming from an\nincomplete understanding of scene affordance. For example, these methods\nfrequently misidentify vehicles that do not impact the ego vehicle as risk\nobjects. Furthermore, existing behavior change-based methods are inefficient\nbecause they implement causal inference in the perspective image space. We\npropose a new framework with a Bird's Eye View (BEV) representation to overcome\nthe above challenges. Specifically, we utilize potential fields as scene\naffordance, involving repulsive forces derived from road infrastructure and\ntraffic participants, along with attractive forces sourced from target\ndestinations. In this work, we compute potential fields by assigning different\nenergy levels according to the semantic labels obtained from BEV semantic\nsegmentation. We conduct thorough experiments and ablation studies, comparing\nthe proposed method with various state-of-the-art algorithms on both synthetic\nand real-world datasets. Our results show a notable increase in spatial and\ntemporal consistency, with enhancements of 20.3% and 11.6% on the RiskBench\ndataset, respectively. Additionally, we can improve computational efficiency by\n88%. We achieve improvements of 5.4% in spatial accuracy and 7.2% in temporal\nconsistency on the nuScenes dataset.\n", "link": "http://arxiv.org/abs/2409.15846v2", "date": "2025-02-26", "relevancy": 2.2109, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Potential%20Field%20as%20Scene%20Affordance%20for%20Behavior%20Change-Based%20Visual%0A%20%20Risk%20Object%20Identification&body=Title%3A%20Potential%20Field%20as%20Scene%20Affordance%20for%20Behavior%20Change-Based%20Visual%0A%20%20Risk%20Object%20Identification%0AAuthor%3A%20Pang-Yuan%20Pao%20and%20Shu-Wei%20Lu%20and%20Ze-Yan%20Lu%20and%20Yi-Ting%20Chen%0AAbstract%3A%20%20%20We%20study%20behavior%20change-based%20visual%20risk%20object%20identification%0A%28Visual-ROI%29%2C%20a%20critical%20framework%20designed%20to%20detect%20potential%20hazards%20for%0Aintelligent%20driving%20systems.%20Existing%20methods%20often%20show%20significant%0Alimitations%20in%20spatial%20accuracy%20and%20temporal%20consistency%2C%20stemming%20from%20an%0Aincomplete%20understanding%20of%20scene%20affordance.%20For%20example%2C%20these%20methods%0Afrequently%20misidentify%20vehicles%20that%20do%20not%20impact%20the%20ego%20vehicle%20as%20risk%0Aobjects.%20Furthermore%2C%20existing%20behavior%20change-based%20methods%20are%20inefficient%0Abecause%20they%20implement%20causal%20inference%20in%20the%20perspective%20image%20space.%20We%0Apropose%20a%20new%20framework%20with%20a%20Bird%27s%20Eye%20View%20%28BEV%29%20representation%20to%20overcome%0Athe%20above%20challenges.%20Specifically%2C%20we%20utilize%20potential%20fields%20as%20scene%0Aaffordance%2C%20involving%20repulsive%20forces%20derived%20from%20road%20infrastructure%20and%0Atraffic%20participants%2C%20along%20with%20attractive%20forces%20sourced%20from%20target%0Adestinations.%20In%20this%20work%2C%20we%20compute%20potential%20fields%20by%20assigning%20different%0Aenergy%20levels%20according%20to%20the%20semantic%20labels%20obtained%20from%20BEV%20semantic%0Asegmentation.%20We%20conduct%20thorough%20experiments%20and%20ablation%20studies%2C%20comparing%0Athe%20proposed%20method%20with%20various%20state-of-the-art%20algorithms%20on%20both%20synthetic%0Aand%20real-world%20datasets.%20Our%20results%20show%20a%20notable%20increase%20in%20spatial%20and%0Atemporal%20consistency%2C%20with%20enhancements%20of%2020.3%25%20and%2011.6%25%20on%20the%20RiskBench%0Adataset%2C%20respectively.%20Additionally%2C%20we%20can%20improve%20computational%20efficiency%20by%0A88%25.%20We%20achieve%20improvements%20of%205.4%25%20in%20spatial%20accuracy%20and%207.2%25%20in%20temporal%0Aconsistency%20on%20the%20nuScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPotential%2520Field%2520as%2520Scene%2520Affordance%2520for%2520Behavior%2520Change-Based%2520Visual%250A%2520%2520Risk%2520Object%2520Identification%26entry.906535625%3DPang-Yuan%2520Pao%2520and%2520Shu-Wei%2520Lu%2520and%2520Ze-Yan%2520Lu%2520and%2520Yi-Ting%2520Chen%26entry.1292438233%3D%2520%2520We%2520study%2520behavior%2520change-based%2520visual%2520risk%2520object%2520identification%250A%2528Visual-ROI%2529%252C%2520a%2520critical%2520framework%2520designed%2520to%2520detect%2520potential%2520hazards%2520for%250Aintelligent%2520driving%2520systems.%2520Existing%2520methods%2520often%2520show%2520significant%250Alimitations%2520in%2520spatial%2520accuracy%2520and%2520temporal%2520consistency%252C%2520stemming%2520from%2520an%250Aincomplete%2520understanding%2520of%2520scene%2520affordance.%2520For%2520example%252C%2520these%2520methods%250Afrequently%2520misidentify%2520vehicles%2520that%2520do%2520not%2520impact%2520the%2520ego%2520vehicle%2520as%2520risk%250Aobjects.%2520Furthermore%252C%2520existing%2520behavior%2520change-based%2520methods%2520are%2520inefficient%250Abecause%2520they%2520implement%2520causal%2520inference%2520in%2520the%2520perspective%2520image%2520space.%2520We%250Apropose%2520a%2520new%2520framework%2520with%2520a%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520representation%2520to%2520overcome%250Athe%2520above%2520challenges.%2520Specifically%252C%2520we%2520utilize%2520potential%2520fields%2520as%2520scene%250Aaffordance%252C%2520involving%2520repulsive%2520forces%2520derived%2520from%2520road%2520infrastructure%2520and%250Atraffic%2520participants%252C%2520along%2520with%2520attractive%2520forces%2520sourced%2520from%2520target%250Adestinations.%2520In%2520this%2520work%252C%2520we%2520compute%2520potential%2520fields%2520by%2520assigning%2520different%250Aenergy%2520levels%2520according%2520to%2520the%2520semantic%2520labels%2520obtained%2520from%2520BEV%2520semantic%250Asegmentation.%2520We%2520conduct%2520thorough%2520experiments%2520and%2520ablation%2520studies%252C%2520comparing%250Athe%2520proposed%2520method%2520with%2520various%2520state-of-the-art%2520algorithms%2520on%2520both%2520synthetic%250Aand%2520real-world%2520datasets.%2520Our%2520results%2520show%2520a%2520notable%2520increase%2520in%2520spatial%2520and%250Atemporal%2520consistency%252C%2520with%2520enhancements%2520of%252020.3%2525%2520and%252011.6%2525%2520on%2520the%2520RiskBench%250Adataset%252C%2520respectively.%2520Additionally%252C%2520we%2520can%2520improve%2520computational%2520efficiency%2520by%250A88%2525.%2520We%2520achieve%2520improvements%2520of%25205.4%2525%2520in%2520spatial%2520accuracy%2520and%25207.2%2525%2520in%2520temporal%250Aconsistency%2520on%2520the%2520nuScenes%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Potential%20Field%20as%20Scene%20Affordance%20for%20Behavior%20Change-Based%20Visual%0A%20%20Risk%20Object%20Identification&entry.906535625=Pang-Yuan%20Pao%20and%20Shu-Wei%20Lu%20and%20Ze-Yan%20Lu%20and%20Yi-Ting%20Chen&entry.1292438233=%20%20We%20study%20behavior%20change-based%20visual%20risk%20object%20identification%0A%28Visual-ROI%29%2C%20a%20critical%20framework%20designed%20to%20detect%20potential%20hazards%20for%0Aintelligent%20driving%20systems.%20Existing%20methods%20often%20show%20significant%0Alimitations%20in%20spatial%20accuracy%20and%20temporal%20consistency%2C%20stemming%20from%20an%0Aincomplete%20understanding%20of%20scene%20affordance.%20For%20example%2C%20these%20methods%0Afrequently%20misidentify%20vehicles%20that%20do%20not%20impact%20the%20ego%20vehicle%20as%20risk%0Aobjects.%20Furthermore%2C%20existing%20behavior%20change-based%20methods%20are%20inefficient%0Abecause%20they%20implement%20causal%20inference%20in%20the%20perspective%20image%20space.%20We%0Apropose%20a%20new%20framework%20with%20a%20Bird%27s%20Eye%20View%20%28BEV%29%20representation%20to%20overcome%0Athe%20above%20challenges.%20Specifically%2C%20we%20utilize%20potential%20fields%20as%20scene%0Aaffordance%2C%20involving%20repulsive%20forces%20derived%20from%20road%20infrastructure%20and%0Atraffic%20participants%2C%20along%20with%20attractive%20forces%20sourced%20from%20target%0Adestinations.%20In%20this%20work%2C%20we%20compute%20potential%20fields%20by%20assigning%20different%0Aenergy%20levels%20according%20to%20the%20semantic%20labels%20obtained%20from%20BEV%20semantic%0Asegmentation.%20We%20conduct%20thorough%20experiments%20and%20ablation%20studies%2C%20comparing%0Athe%20proposed%20method%20with%20various%20state-of-the-art%20algorithms%20on%20both%20synthetic%0Aand%20real-world%20datasets.%20Our%20results%20show%20a%20notable%20increase%20in%20spatial%20and%0Atemporal%20consistency%2C%20with%20enhancements%20of%2020.3%25%20and%2011.6%25%20on%20the%20RiskBench%0Adataset%2C%20respectively.%20Additionally%2C%20we%20can%20improve%20computational%20efficiency%20by%0A88%25.%20We%20achieve%20improvements%20of%205.4%25%20in%20spatial%20accuracy%20and%207.2%25%20in%20temporal%0Aconsistency%20on%20the%20nuScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15846v2&entry.124074799=Read"},
{"title": "Recurrent Auto-Encoders for Enhanced Deep Reinforcement Learning in\n  Wilderness Search and Rescue Planning", "author": "Jan-Hendrik Ewers and David Anderson and Douglas Thomson", "abstract": "  Wilderness search and rescue operations are often carried out over vast\nlandscapes. The search efforts, however, must be undertaken in minimum time to\nmaximize the chance of survival of the victim. Whilst the advent of cheap\nmulticopters in recent years has changed the way search operations are handled,\nit has not solved the challenges of the massive areas at hand. The problem\ntherefore is not one of complete coverage, but one of maximizing the\ninformation gathered in the limited time available. In this work we propose\nthat a combination of a recurrent autoencoder and deep reinforcement learning\nis a more efficient solution to the search problem than previous pure deep\nreinforcement learning or optimisation approaches. The autoencoder training\nparadigm efficiently maximizes the information throughput of the encoder into\nits latent space representation which deep reinforcement learning is primed to\nleverage. Without the overhead of independently solving the problem that the\nrecurrent autoencoder is designed for, it is more efficient in learning the\ncontrol task. We further implement three additional architectures for a\ncomprehensive comparison of the main proposed architecture. Similarly, we apply\nboth soft actor-critic and proximal policy optimisation to provide an insight\ninto the performance of both in a highly non-linear and complex application\nwith a large observation Results show that the proposed architecture is vastly\nsuperior to the benchmarks, with soft actor-critic achieving the best\nperformance. This model further outperformed work from the literature whilst\nhaving below a fifth of the total learnable parameters and training in a\nquarter of the time.\n", "link": "http://arxiv.org/abs/2502.19356v1", "date": "2025-02-26", "relevancy": 2.1884, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5515}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5446}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Auto-Encoders%20for%20Enhanced%20Deep%20Reinforcement%20Learning%20in%0A%20%20Wilderness%20Search%20and%20Rescue%20Planning&body=Title%3A%20Recurrent%20Auto-Encoders%20for%20Enhanced%20Deep%20Reinforcement%20Learning%20in%0A%20%20Wilderness%20Search%20and%20Rescue%20Planning%0AAuthor%3A%20Jan-Hendrik%20Ewers%20and%20David%20Anderson%20and%20Douglas%20Thomson%0AAbstract%3A%20%20%20Wilderness%20search%20and%20rescue%20operations%20are%20often%20carried%20out%20over%20vast%0Alandscapes.%20The%20search%20efforts%2C%20however%2C%20must%20be%20undertaken%20in%20minimum%20time%20to%0Amaximize%20the%20chance%20of%20survival%20of%20the%20victim.%20Whilst%20the%20advent%20of%20cheap%0Amulticopters%20in%20recent%20years%20has%20changed%20the%20way%20search%20operations%20are%20handled%2C%0Ait%20has%20not%20solved%20the%20challenges%20of%20the%20massive%20areas%20at%20hand.%20The%20problem%0Atherefore%20is%20not%20one%20of%20complete%20coverage%2C%20but%20one%20of%20maximizing%20the%0Ainformation%20gathered%20in%20the%20limited%20time%20available.%20In%20this%20work%20we%20propose%0Athat%20a%20combination%20of%20a%20recurrent%20autoencoder%20and%20deep%20reinforcement%20learning%0Ais%20a%20more%20efficient%20solution%20to%20the%20search%20problem%20than%20previous%20pure%20deep%0Areinforcement%20learning%20or%20optimisation%20approaches.%20The%20autoencoder%20training%0Aparadigm%20efficiently%20maximizes%20the%20information%20throughput%20of%20the%20encoder%20into%0Aits%20latent%20space%20representation%20which%20deep%20reinforcement%20learning%20is%20primed%20to%0Aleverage.%20Without%20the%20overhead%20of%20independently%20solving%20the%20problem%20that%20the%0Arecurrent%20autoencoder%20is%20designed%20for%2C%20it%20is%20more%20efficient%20in%20learning%20the%0Acontrol%20task.%20We%20further%20implement%20three%20additional%20architectures%20for%20a%0Acomprehensive%20comparison%20of%20the%20main%20proposed%20architecture.%20Similarly%2C%20we%20apply%0Aboth%20soft%20actor-critic%20and%20proximal%20policy%20optimisation%20to%20provide%20an%20insight%0Ainto%20the%20performance%20of%20both%20in%20a%20highly%20non-linear%20and%20complex%20application%0Awith%20a%20large%20observation%20Results%20show%20that%20the%20proposed%20architecture%20is%20vastly%0Asuperior%20to%20the%20benchmarks%2C%20with%20soft%20actor-critic%20achieving%20the%20best%0Aperformance.%20This%20model%20further%20outperformed%20work%20from%20the%20literature%20whilst%0Ahaving%20below%20a%20fifth%20of%20the%20total%20learnable%20parameters%20and%20training%20in%20a%0Aquarter%20of%20the%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Auto-Encoders%2520for%2520Enhanced%2520Deep%2520Reinforcement%2520Learning%2520in%250A%2520%2520Wilderness%2520Search%2520and%2520Rescue%2520Planning%26entry.906535625%3DJan-Hendrik%2520Ewers%2520and%2520David%2520Anderson%2520and%2520Douglas%2520Thomson%26entry.1292438233%3D%2520%2520Wilderness%2520search%2520and%2520rescue%2520operations%2520are%2520often%2520carried%2520out%2520over%2520vast%250Alandscapes.%2520The%2520search%2520efforts%252C%2520however%252C%2520must%2520be%2520undertaken%2520in%2520minimum%2520time%2520to%250Amaximize%2520the%2520chance%2520of%2520survival%2520of%2520the%2520victim.%2520Whilst%2520the%2520advent%2520of%2520cheap%250Amulticopters%2520in%2520recent%2520years%2520has%2520changed%2520the%2520way%2520search%2520operations%2520are%2520handled%252C%250Ait%2520has%2520not%2520solved%2520the%2520challenges%2520of%2520the%2520massive%2520areas%2520at%2520hand.%2520The%2520problem%250Atherefore%2520is%2520not%2520one%2520of%2520complete%2520coverage%252C%2520but%2520one%2520of%2520maximizing%2520the%250Ainformation%2520gathered%2520in%2520the%2520limited%2520time%2520available.%2520In%2520this%2520work%2520we%2520propose%250Athat%2520a%2520combination%2520of%2520a%2520recurrent%2520autoencoder%2520and%2520deep%2520reinforcement%2520learning%250Ais%2520a%2520more%2520efficient%2520solution%2520to%2520the%2520search%2520problem%2520than%2520previous%2520pure%2520deep%250Areinforcement%2520learning%2520or%2520optimisation%2520approaches.%2520The%2520autoencoder%2520training%250Aparadigm%2520efficiently%2520maximizes%2520the%2520information%2520throughput%2520of%2520the%2520encoder%2520into%250Aits%2520latent%2520space%2520representation%2520which%2520deep%2520reinforcement%2520learning%2520is%2520primed%2520to%250Aleverage.%2520Without%2520the%2520overhead%2520of%2520independently%2520solving%2520the%2520problem%2520that%2520the%250Arecurrent%2520autoencoder%2520is%2520designed%2520for%252C%2520it%2520is%2520more%2520efficient%2520in%2520learning%2520the%250Acontrol%2520task.%2520We%2520further%2520implement%2520three%2520additional%2520architectures%2520for%2520a%250Acomprehensive%2520comparison%2520of%2520the%2520main%2520proposed%2520architecture.%2520Similarly%252C%2520we%2520apply%250Aboth%2520soft%2520actor-critic%2520and%2520proximal%2520policy%2520optimisation%2520to%2520provide%2520an%2520insight%250Ainto%2520the%2520performance%2520of%2520both%2520in%2520a%2520highly%2520non-linear%2520and%2520complex%2520application%250Awith%2520a%2520large%2520observation%2520Results%2520show%2520that%2520the%2520proposed%2520architecture%2520is%2520vastly%250Asuperior%2520to%2520the%2520benchmarks%252C%2520with%2520soft%2520actor-critic%2520achieving%2520the%2520best%250Aperformance.%2520This%2520model%2520further%2520outperformed%2520work%2520from%2520the%2520literature%2520whilst%250Ahaving%2520below%2520a%2520fifth%2520of%2520the%2520total%2520learnable%2520parameters%2520and%2520training%2520in%2520a%250Aquarter%2520of%2520the%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Auto-Encoders%20for%20Enhanced%20Deep%20Reinforcement%20Learning%20in%0A%20%20Wilderness%20Search%20and%20Rescue%20Planning&entry.906535625=Jan-Hendrik%20Ewers%20and%20David%20Anderson%20and%20Douglas%20Thomson&entry.1292438233=%20%20Wilderness%20search%20and%20rescue%20operations%20are%20often%20carried%20out%20over%20vast%0Alandscapes.%20The%20search%20efforts%2C%20however%2C%20must%20be%20undertaken%20in%20minimum%20time%20to%0Amaximize%20the%20chance%20of%20survival%20of%20the%20victim.%20Whilst%20the%20advent%20of%20cheap%0Amulticopters%20in%20recent%20years%20has%20changed%20the%20way%20search%20operations%20are%20handled%2C%0Ait%20has%20not%20solved%20the%20challenges%20of%20the%20massive%20areas%20at%20hand.%20The%20problem%0Atherefore%20is%20not%20one%20of%20complete%20coverage%2C%20but%20one%20of%20maximizing%20the%0Ainformation%20gathered%20in%20the%20limited%20time%20available.%20In%20this%20work%20we%20propose%0Athat%20a%20combination%20of%20a%20recurrent%20autoencoder%20and%20deep%20reinforcement%20learning%0Ais%20a%20more%20efficient%20solution%20to%20the%20search%20problem%20than%20previous%20pure%20deep%0Areinforcement%20learning%20or%20optimisation%20approaches.%20The%20autoencoder%20training%0Aparadigm%20efficiently%20maximizes%20the%20information%20throughput%20of%20the%20encoder%20into%0Aits%20latent%20space%20representation%20which%20deep%20reinforcement%20learning%20is%20primed%20to%0Aleverage.%20Without%20the%20overhead%20of%20independently%20solving%20the%20problem%20that%20the%0Arecurrent%20autoencoder%20is%20designed%20for%2C%20it%20is%20more%20efficient%20in%20learning%20the%0Acontrol%20task.%20We%20further%20implement%20three%20additional%20architectures%20for%20a%0Acomprehensive%20comparison%20of%20the%20main%20proposed%20architecture.%20Similarly%2C%20we%20apply%0Aboth%20soft%20actor-critic%20and%20proximal%20policy%20optimisation%20to%20provide%20an%20insight%0Ainto%20the%20performance%20of%20both%20in%20a%20highly%20non-linear%20and%20complex%20application%0Awith%20a%20large%20observation%20Results%20show%20that%20the%20proposed%20architecture%20is%20vastly%0Asuperior%20to%20the%20benchmarks%2C%20with%20soft%20actor-critic%20achieving%20the%20best%0Aperformance.%20This%20model%20further%20outperformed%20work%20from%20the%20literature%20whilst%0Ahaving%20below%20a%20fifth%20of%20the%20total%20learnable%20parameters%20and%20training%20in%20a%0Aquarter%20of%20the%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19356v1&entry.124074799=Read"},
{"title": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time", "author": "Yizhou Chen and Xiaoyue Wu and Yeheng Zong and Anran Li and Yuzhen Chen and Julie Wu and Bohao Zhang and Ram Vasudevan", "abstract": "  Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.\n", "link": "http://arxiv.org/abs/2502.15037v2", "date": "2025-02-26", "relevancy": 2.1815, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5537}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5461}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEFT%3A%20Differentiable%20Branched%20Discrete%20Elastic%20Rods%20for%20Modeling%0A%20%20Furcated%20DLOs%20in%20Real-Time&body=Title%3A%20DEFT%3A%20Differentiable%20Branched%20Discrete%20Elastic%20Rods%20for%20Modeling%0A%20%20Furcated%20DLOs%20in%20Real-Time%0AAuthor%3A%20Yizhou%20Chen%20and%20Xiaoyue%20Wu%20and%20Yeheng%20Zong%20and%20Anran%20Li%20and%20Yuzhen%20Chen%20and%20Julie%20Wu%20and%20Bohao%20Zhang%20and%20Ram%20Vasudevan%0AAbstract%3A%20%20%20Autonomous%20wire%20harness%20assembly%20requires%20robots%20to%20manipulate%20complex%0Abranched%20cables%20with%20high%20precision%20and%20reliability.%20A%20key%20challenge%20in%0Aautomating%20this%20process%20is%20predicting%20how%20these%20flexible%20and%20branched%0Astructures%20behave%20under%20manipulation.%20Without%20accurate%20predictions%2C%20it%20is%0Adifficult%20for%20robots%20to%20reliably%20plan%20or%20execute%20assembly%20operations.%20While%0Aexisting%20research%20has%20made%20progress%20in%20modeling%20single-threaded%20Deformable%0ALinear%20Objects%20%28DLOs%29%2C%20extending%20these%20approaches%20to%20Branched%20Deformable%20Linear%0AObjects%20%28BDLOs%29%20presents%20fundamental%20challenges.%20The%20junction%20points%20in%20BDLOs%0Acreate%20complex%20force%20interactions%20and%20strain%20propagation%20patterns%20that%20cannot%0Abe%20adequately%20captured%20by%20simply%20connecting%20multiple%20single-DLO%20models.%20To%0Aaddress%20these%20challenges%2C%20this%20paper%20presents%20Differentiable%20discrete%20branched%0AElastic%20rods%20for%20modeling%20Furcated%20DLOs%20in%20real-Time%20%28DEFT%29%2C%20a%20novel%20framework%0Athat%20combines%20a%20differentiable%20physics-based%20model%20with%20a%20learning%20framework%0Ato%3A%201%29%20accurately%20model%20BDLO%20dynamics%2C%20including%20dynamic%20propagation%20at%0Ajunction%20points%20and%20grasping%20in%20the%20middle%20of%20a%20BDLO%2C%202%29%20achieve%20efficient%0Acomputation%20for%20real-time%20inference%2C%20and%203%29%20enable%20planning%20to%20demonstrate%0Adexterous%20BDLO%20manipulation.%20A%20comprehensive%20series%20of%20real-world%20experiments%0Ademonstrates%20DEFT%27s%20efficacy%20in%20terms%20of%20accuracy%2C%20computational%20speed%2C%20and%0Ageneralizability%20compared%20to%20state-of-the-art%20alternatives.%20Project%0Apage%3Ahttps%3A//roahmlab.github.io/DEFT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEFT%253A%2520Differentiable%2520Branched%2520Discrete%2520Elastic%2520Rods%2520for%2520Modeling%250A%2520%2520Furcated%2520DLOs%2520in%2520Real-Time%26entry.906535625%3DYizhou%2520Chen%2520and%2520Xiaoyue%2520Wu%2520and%2520Yeheng%2520Zong%2520and%2520Anran%2520Li%2520and%2520Yuzhen%2520Chen%2520and%2520Julie%2520Wu%2520and%2520Bohao%2520Zhang%2520and%2520Ram%2520Vasudevan%26entry.1292438233%3D%2520%2520Autonomous%2520wire%2520harness%2520assembly%2520requires%2520robots%2520to%2520manipulate%2520complex%250Abranched%2520cables%2520with%2520high%2520precision%2520and%2520reliability.%2520A%2520key%2520challenge%2520in%250Aautomating%2520this%2520process%2520is%2520predicting%2520how%2520these%2520flexible%2520and%2520branched%250Astructures%2520behave%2520under%2520manipulation.%2520Without%2520accurate%2520predictions%252C%2520it%2520is%250Adifficult%2520for%2520robots%2520to%2520reliably%2520plan%2520or%2520execute%2520assembly%2520operations.%2520While%250Aexisting%2520research%2520has%2520made%2520progress%2520in%2520modeling%2520single-threaded%2520Deformable%250ALinear%2520Objects%2520%2528DLOs%2529%252C%2520extending%2520these%2520approaches%2520to%2520Branched%2520Deformable%2520Linear%250AObjects%2520%2528BDLOs%2529%2520presents%2520fundamental%2520challenges.%2520The%2520junction%2520points%2520in%2520BDLOs%250Acreate%2520complex%2520force%2520interactions%2520and%2520strain%2520propagation%2520patterns%2520that%2520cannot%250Abe%2520adequately%2520captured%2520by%2520simply%2520connecting%2520multiple%2520single-DLO%2520models.%2520To%250Aaddress%2520these%2520challenges%252C%2520this%2520paper%2520presents%2520Differentiable%2520discrete%2520branched%250AElastic%2520rods%2520for%2520modeling%2520Furcated%2520DLOs%2520in%2520real-Time%2520%2528DEFT%2529%252C%2520a%2520novel%2520framework%250Athat%2520combines%2520a%2520differentiable%2520physics-based%2520model%2520with%2520a%2520learning%2520framework%250Ato%253A%25201%2529%2520accurately%2520model%2520BDLO%2520dynamics%252C%2520including%2520dynamic%2520propagation%2520at%250Ajunction%2520points%2520and%2520grasping%2520in%2520the%2520middle%2520of%2520a%2520BDLO%252C%25202%2529%2520achieve%2520efficient%250Acomputation%2520for%2520real-time%2520inference%252C%2520and%25203%2529%2520enable%2520planning%2520to%2520demonstrate%250Adexterous%2520BDLO%2520manipulation.%2520A%2520comprehensive%2520series%2520of%2520real-world%2520experiments%250Ademonstrates%2520DEFT%2527s%2520efficacy%2520in%2520terms%2520of%2520accuracy%252C%2520computational%2520speed%252C%2520and%250Ageneralizability%2520compared%2520to%2520state-of-the-art%2520alternatives.%2520Project%250Apage%253Ahttps%253A//roahmlab.github.io/DEFT/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEFT%3A%20Differentiable%20Branched%20Discrete%20Elastic%20Rods%20for%20Modeling%0A%20%20Furcated%20DLOs%20in%20Real-Time&entry.906535625=Yizhou%20Chen%20and%20Xiaoyue%20Wu%20and%20Yeheng%20Zong%20and%20Anran%20Li%20and%20Yuzhen%20Chen%20and%20Julie%20Wu%20and%20Bohao%20Zhang%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Autonomous%20wire%20harness%20assembly%20requires%20robots%20to%20manipulate%20complex%0Abranched%20cables%20with%20high%20precision%20and%20reliability.%20A%20key%20challenge%20in%0Aautomating%20this%20process%20is%20predicting%20how%20these%20flexible%20and%20branched%0Astructures%20behave%20under%20manipulation.%20Without%20accurate%20predictions%2C%20it%20is%0Adifficult%20for%20robots%20to%20reliably%20plan%20or%20execute%20assembly%20operations.%20While%0Aexisting%20research%20has%20made%20progress%20in%20modeling%20single-threaded%20Deformable%0ALinear%20Objects%20%28DLOs%29%2C%20extending%20these%20approaches%20to%20Branched%20Deformable%20Linear%0AObjects%20%28BDLOs%29%20presents%20fundamental%20challenges.%20The%20junction%20points%20in%20BDLOs%0Acreate%20complex%20force%20interactions%20and%20strain%20propagation%20patterns%20that%20cannot%0Abe%20adequately%20captured%20by%20simply%20connecting%20multiple%20single-DLO%20models.%20To%0Aaddress%20these%20challenges%2C%20this%20paper%20presents%20Differentiable%20discrete%20branched%0AElastic%20rods%20for%20modeling%20Furcated%20DLOs%20in%20real-Time%20%28DEFT%29%2C%20a%20novel%20framework%0Athat%20combines%20a%20differentiable%20physics-based%20model%20with%20a%20learning%20framework%0Ato%3A%201%29%20accurately%20model%20BDLO%20dynamics%2C%20including%20dynamic%20propagation%20at%0Ajunction%20points%20and%20grasping%20in%20the%20middle%20of%20a%20BDLO%2C%202%29%20achieve%20efficient%0Acomputation%20for%20real-time%20inference%2C%20and%203%29%20enable%20planning%20to%20demonstrate%0Adexterous%20BDLO%20manipulation.%20A%20comprehensive%20series%20of%20real-world%20experiments%0Ademonstrates%20DEFT%27s%20efficacy%20in%20terms%20of%20accuracy%2C%20computational%20speed%2C%20and%0Ageneralizability%20compared%20to%20state-of-the-art%20alternatives.%20Project%0Apage%3Ahttps%3A//roahmlab.github.io/DEFT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15037v2&entry.124074799=Read"},
{"title": "Linear Combination of Saved Checkpoints Makes Consistency and Diffusion\n  Models Better", "author": "Enshu Liu and Junyi Zhu and Zinan Lin and Xuefei Ning and Shuaiqi Wang and Matthew B. Blaschko and Sergey Yekhanin and Shengen Yan and Guohao Dai and Huazhong Yang and Yu Wang", "abstract": "  Diffusion Models (DM) and Consistency Models (CM) are two types of popular\ngenerative models with good generation quality on various tasks. When training\nDM and CM, intermediate weight checkpoints are not fully utilized and only the\nlast converged checkpoint is used. In this work, we find that high-quality\nmodel weights often lie in a basin which cannot be reached by SGD but can be\nobtained by proper checkpoint averaging. Based on these observations, we\npropose LCSC, a simple but effective and efficient method to enhance the\nperformance of DM and CM, by combining checkpoints along the training\ntrajectory with coefficients deduced from evolutionary search. We demonstrate\nthe value of LCSC through two use cases: $\\textbf{(a) Reducing training cost.}$\nWith LCSC, we only need to train DM/CM with fewer number of iterations and/or\nlower batch sizes to obtain comparable sample quality with the fully trained\nmodel. For example, LCSC achieves considerable training speedups for CM\n(23$\\times$ on CIFAR-10 and 15$\\times$ on ImageNet-64). $\\textbf{(b) Enhancing\npre-trained models.}$ Assuming full training is already done, LCSC can further\nimprove the generation quality or speed of the final converged models. For\nexample, LCSC achieves better performance using 1 number of function evaluation\n(NFE) than the base model with 2 NFE on consistency distillation, and decreases\nthe NFE of DM from 15 to 9 while maintaining the generation quality on\nCIFAR-10. Our code is available at\nhttps://github.com/imagination-research/LCSC.\n", "link": "http://arxiv.org/abs/2404.02241v3", "date": "2025-02-26", "relevancy": 2.176, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5643}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5598}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Combination%20of%20Saved%20Checkpoints%20Makes%20Consistency%20and%20Diffusion%0A%20%20Models%20Better&body=Title%3A%20Linear%20Combination%20of%20Saved%20Checkpoints%20Makes%20Consistency%20and%20Diffusion%0A%20%20Models%20Better%0AAuthor%3A%20Enshu%20Liu%20and%20Junyi%20Zhu%20and%20Zinan%20Lin%20and%20Xuefei%20Ning%20and%20Shuaiqi%20Wang%20and%20Matthew%20B.%20Blaschko%20and%20Sergey%20Yekhanin%20and%20Shengen%20Yan%20and%20Guohao%20Dai%20and%20Huazhong%20Yang%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Diffusion%20Models%20%28DM%29%20and%20Consistency%20Models%20%28CM%29%20are%20two%20types%20of%20popular%0Agenerative%20models%20with%20good%20generation%20quality%20on%20various%20tasks.%20When%20training%0ADM%20and%20CM%2C%20intermediate%20weight%20checkpoints%20are%20not%20fully%20utilized%20and%20only%20the%0Alast%20converged%20checkpoint%20is%20used.%20In%20this%20work%2C%20we%20find%20that%20high-quality%0Amodel%20weights%20often%20lie%20in%20a%20basin%20which%20cannot%20be%20reached%20by%20SGD%20but%20can%20be%0Aobtained%20by%20proper%20checkpoint%20averaging.%20Based%20on%20these%20observations%2C%20we%0Apropose%20LCSC%2C%20a%20simple%20but%20effective%20and%20efficient%20method%20to%20enhance%20the%0Aperformance%20of%20DM%20and%20CM%2C%20by%20combining%20checkpoints%20along%20the%20training%0Atrajectory%20with%20coefficients%20deduced%20from%20evolutionary%20search.%20We%20demonstrate%0Athe%20value%20of%20LCSC%20through%20two%20use%20cases%3A%20%24%5Ctextbf%7B%28a%29%20Reducing%20training%20cost.%7D%24%0AWith%20LCSC%2C%20we%20only%20need%20to%20train%20DM/CM%20with%20fewer%20number%20of%20iterations%20and/or%0Alower%20batch%20sizes%20to%20obtain%20comparable%20sample%20quality%20with%20the%20fully%20trained%0Amodel.%20For%20example%2C%20LCSC%20achieves%20considerable%20training%20speedups%20for%20CM%0A%2823%24%5Ctimes%24%20on%20CIFAR-10%20and%2015%24%5Ctimes%24%20on%20ImageNet-64%29.%20%24%5Ctextbf%7B%28b%29%20Enhancing%0Apre-trained%20models.%7D%24%20Assuming%20full%20training%20is%20already%20done%2C%20LCSC%20can%20further%0Aimprove%20the%20generation%20quality%20or%20speed%20of%20the%20final%20converged%20models.%20For%0Aexample%2C%20LCSC%20achieves%20better%20performance%20using%201%20number%20of%20function%20evaluation%0A%28NFE%29%20than%20the%20base%20model%20with%202%20NFE%20on%20consistency%20distillation%2C%20and%20decreases%0Athe%20NFE%20of%20DM%20from%2015%20to%209%20while%20maintaining%20the%20generation%20quality%20on%0ACIFAR-10.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/imagination-research/LCSC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02241v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Combination%2520of%2520Saved%2520Checkpoints%2520Makes%2520Consistency%2520and%2520Diffusion%250A%2520%2520Models%2520Better%26entry.906535625%3DEnshu%2520Liu%2520and%2520Junyi%2520Zhu%2520and%2520Zinan%2520Lin%2520and%2520Xuefei%2520Ning%2520and%2520Shuaiqi%2520Wang%2520and%2520Matthew%2520B.%2520Blaschko%2520and%2520Sergey%2520Yekhanin%2520and%2520Shengen%2520Yan%2520and%2520Guohao%2520Dai%2520and%2520Huazhong%2520Yang%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520Models%2520%2528DM%2529%2520and%2520Consistency%2520Models%2520%2528CM%2529%2520are%2520two%2520types%2520of%2520popular%250Agenerative%2520models%2520with%2520good%2520generation%2520quality%2520on%2520various%2520tasks.%2520When%2520training%250ADM%2520and%2520CM%252C%2520intermediate%2520weight%2520checkpoints%2520are%2520not%2520fully%2520utilized%2520and%2520only%2520the%250Alast%2520converged%2520checkpoint%2520is%2520used.%2520In%2520this%2520work%252C%2520we%2520find%2520that%2520high-quality%250Amodel%2520weights%2520often%2520lie%2520in%2520a%2520basin%2520which%2520cannot%2520be%2520reached%2520by%2520SGD%2520but%2520can%2520be%250Aobtained%2520by%2520proper%2520checkpoint%2520averaging.%2520Based%2520on%2520these%2520observations%252C%2520we%250Apropose%2520LCSC%252C%2520a%2520simple%2520but%2520effective%2520and%2520efficient%2520method%2520to%2520enhance%2520the%250Aperformance%2520of%2520DM%2520and%2520CM%252C%2520by%2520combining%2520checkpoints%2520along%2520the%2520training%250Atrajectory%2520with%2520coefficients%2520deduced%2520from%2520evolutionary%2520search.%2520We%2520demonstrate%250Athe%2520value%2520of%2520LCSC%2520through%2520two%2520use%2520cases%253A%2520%2524%255Ctextbf%257B%2528a%2529%2520Reducing%2520training%2520cost.%257D%2524%250AWith%2520LCSC%252C%2520we%2520only%2520need%2520to%2520train%2520DM/CM%2520with%2520fewer%2520number%2520of%2520iterations%2520and/or%250Alower%2520batch%2520sizes%2520to%2520obtain%2520comparable%2520sample%2520quality%2520with%2520the%2520fully%2520trained%250Amodel.%2520For%2520example%252C%2520LCSC%2520achieves%2520considerable%2520training%2520speedups%2520for%2520CM%250A%252823%2524%255Ctimes%2524%2520on%2520CIFAR-10%2520and%252015%2524%255Ctimes%2524%2520on%2520ImageNet-64%2529.%2520%2524%255Ctextbf%257B%2528b%2529%2520Enhancing%250Apre-trained%2520models.%257D%2524%2520Assuming%2520full%2520training%2520is%2520already%2520done%252C%2520LCSC%2520can%2520further%250Aimprove%2520the%2520generation%2520quality%2520or%2520speed%2520of%2520the%2520final%2520converged%2520models.%2520For%250Aexample%252C%2520LCSC%2520achieves%2520better%2520performance%2520using%25201%2520number%2520of%2520function%2520evaluation%250A%2528NFE%2529%2520than%2520the%2520base%2520model%2520with%25202%2520NFE%2520on%2520consistency%2520distillation%252C%2520and%2520decreases%250Athe%2520NFE%2520of%2520DM%2520from%252015%2520to%25209%2520while%2520maintaining%2520the%2520generation%2520quality%2520on%250ACIFAR-10.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/imagination-research/LCSC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02241v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Combination%20of%20Saved%20Checkpoints%20Makes%20Consistency%20and%20Diffusion%0A%20%20Models%20Better&entry.906535625=Enshu%20Liu%20and%20Junyi%20Zhu%20and%20Zinan%20Lin%20and%20Xuefei%20Ning%20and%20Shuaiqi%20Wang%20and%20Matthew%20B.%20Blaschko%20and%20Sergey%20Yekhanin%20and%20Shengen%20Yan%20and%20Guohao%20Dai%20and%20Huazhong%20Yang%20and%20Yu%20Wang&entry.1292438233=%20%20Diffusion%20Models%20%28DM%29%20and%20Consistency%20Models%20%28CM%29%20are%20two%20types%20of%20popular%0Agenerative%20models%20with%20good%20generation%20quality%20on%20various%20tasks.%20When%20training%0ADM%20and%20CM%2C%20intermediate%20weight%20checkpoints%20are%20not%20fully%20utilized%20and%20only%20the%0Alast%20converged%20checkpoint%20is%20used.%20In%20this%20work%2C%20we%20find%20that%20high-quality%0Amodel%20weights%20often%20lie%20in%20a%20basin%20which%20cannot%20be%20reached%20by%20SGD%20but%20can%20be%0Aobtained%20by%20proper%20checkpoint%20averaging.%20Based%20on%20these%20observations%2C%20we%0Apropose%20LCSC%2C%20a%20simple%20but%20effective%20and%20efficient%20method%20to%20enhance%20the%0Aperformance%20of%20DM%20and%20CM%2C%20by%20combining%20checkpoints%20along%20the%20training%0Atrajectory%20with%20coefficients%20deduced%20from%20evolutionary%20search.%20We%20demonstrate%0Athe%20value%20of%20LCSC%20through%20two%20use%20cases%3A%20%24%5Ctextbf%7B%28a%29%20Reducing%20training%20cost.%7D%24%0AWith%20LCSC%2C%20we%20only%20need%20to%20train%20DM/CM%20with%20fewer%20number%20of%20iterations%20and/or%0Alower%20batch%20sizes%20to%20obtain%20comparable%20sample%20quality%20with%20the%20fully%20trained%0Amodel.%20For%20example%2C%20LCSC%20achieves%20considerable%20training%20speedups%20for%20CM%0A%2823%24%5Ctimes%24%20on%20CIFAR-10%20and%2015%24%5Ctimes%24%20on%20ImageNet-64%29.%20%24%5Ctextbf%7B%28b%29%20Enhancing%0Apre-trained%20models.%7D%24%20Assuming%20full%20training%20is%20already%20done%2C%20LCSC%20can%20further%0Aimprove%20the%20generation%20quality%20or%20speed%20of%20the%20final%20converged%20models.%20For%0Aexample%2C%20LCSC%20achieves%20better%20performance%20using%201%20number%20of%20function%20evaluation%0A%28NFE%29%20than%20the%20base%20model%20with%202%20NFE%20on%20consistency%20distillation%2C%20and%20decreases%0Athe%20NFE%20of%20DM%20from%2015%20to%209%20while%20maintaining%20the%20generation%20quality%20on%0ACIFAR-10.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/imagination-research/LCSC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02241v3&entry.124074799=Read"},
{"title": "The FFT Strikes Back: An Efficient Alternative to Self-Attention", "author": "Jacob Fein-Ashley", "abstract": "  Conventional self-attention mechanisms incur quadratic complexity, limiting\ntheir scalability on long sequences. We introduce FFTNet, an adaptive spectral\nfiltering framework that leverages the Fast Fourier Transform (FFT) to achieve\nglobal token mixing in $\\mathcal{O}(n\\log n)$ time. By transforming inputs into\nthe frequency domain, FFTNet exploits the orthogonality and energy preservation\nguaranteed by Parseval's theorem to capture long-range dependencies\nefficiently. A learnable spectral filter and modReLU activation dynamically\nemphasize salient frequency components, providing a rigorous and adaptive\nalternative to traditional self-attention. Experiments on the Long Range Arena\nand ImageNet benchmarks validate our theoretical insights and demonstrate\nsuperior performance over fixed Fourier and standard attention models.\n", "link": "http://arxiv.org/abs/2502.18394v2", "date": "2025-02-26", "relevancy": 2.1738, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.593}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5143}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20FFT%20Strikes%20Back%3A%20An%20Efficient%20Alternative%20to%20Self-Attention&body=Title%3A%20The%20FFT%20Strikes%20Back%3A%20An%20Efficient%20Alternative%20to%20Self-Attention%0AAuthor%3A%20Jacob%20Fein-Ashley%0AAbstract%3A%20%20%20Conventional%20self-attention%20mechanisms%20incur%20quadratic%20complexity%2C%20limiting%0Atheir%20scalability%20on%20long%20sequences.%20We%20introduce%20FFTNet%2C%20an%20adaptive%20spectral%0Afiltering%20framework%20that%20leverages%20the%20Fast%20Fourier%20Transform%20%28FFT%29%20to%20achieve%0Aglobal%20token%20mixing%20in%20%24%5Cmathcal%7BO%7D%28n%5Clog%20n%29%24%20time.%20By%20transforming%20inputs%20into%0Athe%20frequency%20domain%2C%20FFTNet%20exploits%20the%20orthogonality%20and%20energy%20preservation%0Aguaranteed%20by%20Parseval%27s%20theorem%20to%20capture%20long-range%20dependencies%0Aefficiently.%20A%20learnable%20spectral%20filter%20and%20modReLU%20activation%20dynamically%0Aemphasize%20salient%20frequency%20components%2C%20providing%20a%20rigorous%20and%20adaptive%0Aalternative%20to%20traditional%20self-attention.%20Experiments%20on%20the%20Long%20Range%20Arena%0Aand%20ImageNet%20benchmarks%20validate%20our%20theoretical%20insights%20and%20demonstrate%0Asuperior%20performance%20over%20fixed%20Fourier%20and%20standard%20attention%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520FFT%2520Strikes%2520Back%253A%2520An%2520Efficient%2520Alternative%2520to%2520Self-Attention%26entry.906535625%3DJacob%2520Fein-Ashley%26entry.1292438233%3D%2520%2520Conventional%2520self-attention%2520mechanisms%2520incur%2520quadratic%2520complexity%252C%2520limiting%250Atheir%2520scalability%2520on%2520long%2520sequences.%2520We%2520introduce%2520FFTNet%252C%2520an%2520adaptive%2520spectral%250Afiltering%2520framework%2520that%2520leverages%2520the%2520Fast%2520Fourier%2520Transform%2520%2528FFT%2529%2520to%2520achieve%250Aglobal%2520token%2520mixing%2520in%2520%2524%255Cmathcal%257BO%257D%2528n%255Clog%2520n%2529%2524%2520time.%2520By%2520transforming%2520inputs%2520into%250Athe%2520frequency%2520domain%252C%2520FFTNet%2520exploits%2520the%2520orthogonality%2520and%2520energy%2520preservation%250Aguaranteed%2520by%2520Parseval%2527s%2520theorem%2520to%2520capture%2520long-range%2520dependencies%250Aefficiently.%2520A%2520learnable%2520spectral%2520filter%2520and%2520modReLU%2520activation%2520dynamically%250Aemphasize%2520salient%2520frequency%2520components%252C%2520providing%2520a%2520rigorous%2520and%2520adaptive%250Aalternative%2520to%2520traditional%2520self-attention.%2520Experiments%2520on%2520the%2520Long%2520Range%2520Arena%250Aand%2520ImageNet%2520benchmarks%2520validate%2520our%2520theoretical%2520insights%2520and%2520demonstrate%250Asuperior%2520performance%2520over%2520fixed%2520Fourier%2520and%2520standard%2520attention%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20FFT%20Strikes%20Back%3A%20An%20Efficient%20Alternative%20to%20Self-Attention&entry.906535625=Jacob%20Fein-Ashley&entry.1292438233=%20%20Conventional%20self-attention%20mechanisms%20incur%20quadratic%20complexity%2C%20limiting%0Atheir%20scalability%20on%20long%20sequences.%20We%20introduce%20FFTNet%2C%20an%20adaptive%20spectral%0Afiltering%20framework%20that%20leverages%20the%20Fast%20Fourier%20Transform%20%28FFT%29%20to%20achieve%0Aglobal%20token%20mixing%20in%20%24%5Cmathcal%7BO%7D%28n%5Clog%20n%29%24%20time.%20By%20transforming%20inputs%20into%0Athe%20frequency%20domain%2C%20FFTNet%20exploits%20the%20orthogonality%20and%20energy%20preservation%0Aguaranteed%20by%20Parseval%27s%20theorem%20to%20capture%20long-range%20dependencies%0Aefficiently.%20A%20learnable%20spectral%20filter%20and%20modReLU%20activation%20dynamically%0Aemphasize%20salient%20frequency%20components%2C%20providing%20a%20rigorous%20and%20adaptive%0Aalternative%20to%20traditional%20self-attention.%20Experiments%20on%20the%20Long%20Range%20Arena%0Aand%20ImageNet%20benchmarks%20validate%20our%20theoretical%20insights%20and%20demonstrate%0Asuperior%20performance%20over%20fixed%20Fourier%20and%20standard%20attention%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18394v2&entry.124074799=Read"},
{"title": "Max360IQ: Blind Omnidirectional Image Quality Assessment with Multi-axis\n  Attention", "author": "Jiebin Yan and Ziwen Tan and Yuming Fang and Jiale Rao and Yifan Zuo", "abstract": "  Omnidirectional image, also called 360-degree image, is able to capture the\nentire 360-degree scene, thereby providing more realistic immersive feelings\nfor users than general 2D image and stereoscopic image. Meanwhile, this feature\nbrings great challenges to measuring the perceptual quality of omnidirectional\nimages, which is closely related to users' quality of experience, especially\nwhen the omnidirectional images suffer from non-uniform distortion. In this\npaper, we propose a novel and effective blind omnidirectional image quality\nassessment (BOIQA) model with multi-axis attention (Max360IQ), which can\nproficiently measure not only the quality of uniformly distorted\nomnidirectional images but also the quality of non-uniformly distorted\nomnidirectional images. Specifically, the proposed Max360IQ is mainly composed\nof a backbone with stacked multi-axis attention modules for capturing both\nglobal and local spatial interactions of extracted viewports, a multi-scale\nfeature integration (MSFI) module to fuse multi-scale features and a quality\nregression module with deep semantic guidance for predicting the quality of\nomnidirectional images. Experimental results demonstrate that the proposed\nMax360IQ outperforms the state-of-the-art Assessor360 by 3.6\\% in terms of SRCC\non the JUFE database with non-uniform distortion, and gains improvement of\n0.4\\% and 0.8\\% in terms of SRCC on the OIQA and CVIQ databases, respectively.\nThe source code is available at https://github.com/WenJuing/Max360IQ.\n", "link": "http://arxiv.org/abs/2502.19046v1", "date": "2025-02-26", "relevancy": 2.1685, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Max360IQ%3A%20Blind%20Omnidirectional%20Image%20Quality%20Assessment%20with%20Multi-axis%0A%20%20Attention&body=Title%3A%20Max360IQ%3A%20Blind%20Omnidirectional%20Image%20Quality%20Assessment%20with%20Multi-axis%0A%20%20Attention%0AAuthor%3A%20Jiebin%20Yan%20and%20Ziwen%20Tan%20and%20Yuming%20Fang%20and%20Jiale%20Rao%20and%20Yifan%20Zuo%0AAbstract%3A%20%20%20Omnidirectional%20image%2C%20also%20called%20360-degree%20image%2C%20is%20able%20to%20capture%20the%0Aentire%20360-degree%20scene%2C%20thereby%20providing%20more%20realistic%20immersive%20feelings%0Afor%20users%20than%20general%202D%20image%20and%20stereoscopic%20image.%20Meanwhile%2C%20this%20feature%0Abrings%20great%20challenges%20to%20measuring%20the%20perceptual%20quality%20of%20omnidirectional%0Aimages%2C%20which%20is%20closely%20related%20to%20users%27%20quality%20of%20experience%2C%20especially%0Awhen%20the%20omnidirectional%20images%20suffer%20from%20non-uniform%20distortion.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20and%20effective%20blind%20omnidirectional%20image%20quality%0Aassessment%20%28BOIQA%29%20model%20with%20multi-axis%20attention%20%28Max360IQ%29%2C%20which%20can%0Aproficiently%20measure%20not%20only%20the%20quality%20of%20uniformly%20distorted%0Aomnidirectional%20images%20but%20also%20the%20quality%20of%20non-uniformly%20distorted%0Aomnidirectional%20images.%20Specifically%2C%20the%20proposed%20Max360IQ%20is%20mainly%20composed%0Aof%20a%20backbone%20with%20stacked%20multi-axis%20attention%20modules%20for%20capturing%20both%0Aglobal%20and%20local%20spatial%20interactions%20of%20extracted%20viewports%2C%20a%20multi-scale%0Afeature%20integration%20%28MSFI%29%20module%20to%20fuse%20multi-scale%20features%20and%20a%20quality%0Aregression%20module%20with%20deep%20semantic%20guidance%20for%20predicting%20the%20quality%20of%0Aomnidirectional%20images.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0AMax360IQ%20outperforms%20the%20state-of-the-art%20Assessor360%20by%203.6%5C%25%20in%20terms%20of%20SRCC%0Aon%20the%20JUFE%20database%20with%20non-uniform%20distortion%2C%20and%20gains%20improvement%20of%0A0.4%5C%25%20and%200.8%5C%25%20in%20terms%20of%20SRCC%20on%20the%20OIQA%20and%20CVIQ%20databases%2C%20respectively.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/WenJuing/Max360IQ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMax360IQ%253A%2520Blind%2520Omnidirectional%2520Image%2520Quality%2520Assessment%2520with%2520Multi-axis%250A%2520%2520Attention%26entry.906535625%3DJiebin%2520Yan%2520and%2520Ziwen%2520Tan%2520and%2520Yuming%2520Fang%2520and%2520Jiale%2520Rao%2520and%2520Yifan%2520Zuo%26entry.1292438233%3D%2520%2520Omnidirectional%2520image%252C%2520also%2520called%2520360-degree%2520image%252C%2520is%2520able%2520to%2520capture%2520the%250Aentire%2520360-degree%2520scene%252C%2520thereby%2520providing%2520more%2520realistic%2520immersive%2520feelings%250Afor%2520users%2520than%2520general%25202D%2520image%2520and%2520stereoscopic%2520image.%2520Meanwhile%252C%2520this%2520feature%250Abrings%2520great%2520challenges%2520to%2520measuring%2520the%2520perceptual%2520quality%2520of%2520omnidirectional%250Aimages%252C%2520which%2520is%2520closely%2520related%2520to%2520users%2527%2520quality%2520of%2520experience%252C%2520especially%250Awhen%2520the%2520omnidirectional%2520images%2520suffer%2520from%2520non-uniform%2520distortion.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520and%2520effective%2520blind%2520omnidirectional%2520image%2520quality%250Aassessment%2520%2528BOIQA%2529%2520model%2520with%2520multi-axis%2520attention%2520%2528Max360IQ%2529%252C%2520which%2520can%250Aproficiently%2520measure%2520not%2520only%2520the%2520quality%2520of%2520uniformly%2520distorted%250Aomnidirectional%2520images%2520but%2520also%2520the%2520quality%2520of%2520non-uniformly%2520distorted%250Aomnidirectional%2520images.%2520Specifically%252C%2520the%2520proposed%2520Max360IQ%2520is%2520mainly%2520composed%250Aof%2520a%2520backbone%2520with%2520stacked%2520multi-axis%2520attention%2520modules%2520for%2520capturing%2520both%250Aglobal%2520and%2520local%2520spatial%2520interactions%2520of%2520extracted%2520viewports%252C%2520a%2520multi-scale%250Afeature%2520integration%2520%2528MSFI%2529%2520module%2520to%2520fuse%2520multi-scale%2520features%2520and%2520a%2520quality%250Aregression%2520module%2520with%2520deep%2520semantic%2520guidance%2520for%2520predicting%2520the%2520quality%2520of%250Aomnidirectional%2520images.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%250AMax360IQ%2520outperforms%2520the%2520state-of-the-art%2520Assessor360%2520by%25203.6%255C%2525%2520in%2520terms%2520of%2520SRCC%250Aon%2520the%2520JUFE%2520database%2520with%2520non-uniform%2520distortion%252C%2520and%2520gains%2520improvement%2520of%250A0.4%255C%2525%2520and%25200.8%255C%2525%2520in%2520terms%2520of%2520SRCC%2520on%2520the%2520OIQA%2520and%2520CVIQ%2520databases%252C%2520respectively.%250AThe%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/WenJuing/Max360IQ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Max360IQ%3A%20Blind%20Omnidirectional%20Image%20Quality%20Assessment%20with%20Multi-axis%0A%20%20Attention&entry.906535625=Jiebin%20Yan%20and%20Ziwen%20Tan%20and%20Yuming%20Fang%20and%20Jiale%20Rao%20and%20Yifan%20Zuo&entry.1292438233=%20%20Omnidirectional%20image%2C%20also%20called%20360-degree%20image%2C%20is%20able%20to%20capture%20the%0Aentire%20360-degree%20scene%2C%20thereby%20providing%20more%20realistic%20immersive%20feelings%0Afor%20users%20than%20general%202D%20image%20and%20stereoscopic%20image.%20Meanwhile%2C%20this%20feature%0Abrings%20great%20challenges%20to%20measuring%20the%20perceptual%20quality%20of%20omnidirectional%0Aimages%2C%20which%20is%20closely%20related%20to%20users%27%20quality%20of%20experience%2C%20especially%0Awhen%20the%20omnidirectional%20images%20suffer%20from%20non-uniform%20distortion.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20and%20effective%20blind%20omnidirectional%20image%20quality%0Aassessment%20%28BOIQA%29%20model%20with%20multi-axis%20attention%20%28Max360IQ%29%2C%20which%20can%0Aproficiently%20measure%20not%20only%20the%20quality%20of%20uniformly%20distorted%0Aomnidirectional%20images%20but%20also%20the%20quality%20of%20non-uniformly%20distorted%0Aomnidirectional%20images.%20Specifically%2C%20the%20proposed%20Max360IQ%20is%20mainly%20composed%0Aof%20a%20backbone%20with%20stacked%20multi-axis%20attention%20modules%20for%20capturing%20both%0Aglobal%20and%20local%20spatial%20interactions%20of%20extracted%20viewports%2C%20a%20multi-scale%0Afeature%20integration%20%28MSFI%29%20module%20to%20fuse%20multi-scale%20features%20and%20a%20quality%0Aregression%20module%20with%20deep%20semantic%20guidance%20for%20predicting%20the%20quality%20of%0Aomnidirectional%20images.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0AMax360IQ%20outperforms%20the%20state-of-the-art%20Assessor360%20by%203.6%5C%25%20in%20terms%20of%20SRCC%0Aon%20the%20JUFE%20database%20with%20non-uniform%20distortion%2C%20and%20gains%20improvement%20of%0A0.4%5C%25%20and%200.8%5C%25%20in%20terms%20of%20SRCC%20on%20the%20OIQA%20and%20CVIQ%20databases%2C%20respectively.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/WenJuing/Max360IQ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19046v1&entry.124074799=Read"},
{"title": "ARENA: Adaptive Risk-aware and Energy-efficient NAvigation for\n  Multi-Objective 3D Infrastructure Inspection with a UAV", "author": "David-Alexandre Poissant and Alexis Lussier Desbiens and Fran\u00e7ois Ferland and Louis Petit", "abstract": "  Autonomous robotic inspection missions require balancing multiple conflicting\nobjectives while navigating near costly obstacles. Current multi-objective path\nplanning (MOPP) methods struggle to adapt to evolving risks like localization\nerrors, weather, battery state, and communication issues. This letter presents\nan Adaptive Risk-aware and Energy-efficient NAvigation (ARENA) MOPP approach\nfor UAVs in complex 3D environments. Our method enables online trajectory\nadaptation by optimizing safety, time, and energy using 4D NURBS representation\nand a genetic-based algorithm to generate the Pareto front. A novel risk-aware\nvoting algorithm ensures adaptivity. Simulations and real-world tests\ndemonstrate the planner's ability to produce diverse, optimized trajectories\ncovering 95% or more of the range defined by single-objective benchmarks and\nits ability to estimate power consumption with a mean error representing 14% of\nthe full power range. The ARENA framework enhances UAV autonomy and reliability\nin critical, evolving 3D missions.\n", "link": "http://arxiv.org/abs/2502.19401v1", "date": "2025-02-26", "relevancy": 2.1675, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5807}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5439}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARENA%3A%20Adaptive%20Risk-aware%20and%20Energy-efficient%20NAvigation%20for%0A%20%20Multi-Objective%203D%20Infrastructure%20Inspection%20with%20a%20UAV&body=Title%3A%20ARENA%3A%20Adaptive%20Risk-aware%20and%20Energy-efficient%20NAvigation%20for%0A%20%20Multi-Objective%203D%20Infrastructure%20Inspection%20with%20a%20UAV%0AAuthor%3A%20David-Alexandre%20Poissant%20and%20Alexis%20Lussier%20Desbiens%20and%20Fran%C3%A7ois%20Ferland%20and%20Louis%20Petit%0AAbstract%3A%20%20%20Autonomous%20robotic%20inspection%20missions%20require%20balancing%20multiple%20conflicting%0Aobjectives%20while%20navigating%20near%20costly%20obstacles.%20Current%20multi-objective%20path%0Aplanning%20%28MOPP%29%20methods%20struggle%20to%20adapt%20to%20evolving%20risks%20like%20localization%0Aerrors%2C%20weather%2C%20battery%20state%2C%20and%20communication%20issues.%20This%20letter%20presents%0Aan%20Adaptive%20Risk-aware%20and%20Energy-efficient%20NAvigation%20%28ARENA%29%20MOPP%20approach%0Afor%20UAVs%20in%20complex%203D%20environments.%20Our%20method%20enables%20online%20trajectory%0Aadaptation%20by%20optimizing%20safety%2C%20time%2C%20and%20energy%20using%204D%20NURBS%20representation%0Aand%20a%20genetic-based%20algorithm%20to%20generate%20the%20Pareto%20front.%20A%20novel%20risk-aware%0Avoting%20algorithm%20ensures%20adaptivity.%20Simulations%20and%20real-world%20tests%0Ademonstrate%20the%20planner%27s%20ability%20to%20produce%20diverse%2C%20optimized%20trajectories%0Acovering%2095%25%20or%20more%20of%20the%20range%20defined%20by%20single-objective%20benchmarks%20and%0Aits%20ability%20to%20estimate%20power%20consumption%20with%20a%20mean%20error%20representing%2014%25%20of%0Athe%20full%20power%20range.%20The%20ARENA%20framework%20enhances%20UAV%20autonomy%20and%20reliability%0Ain%20critical%2C%20evolving%203D%20missions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARENA%253A%2520Adaptive%2520Risk-aware%2520and%2520Energy-efficient%2520NAvigation%2520for%250A%2520%2520Multi-Objective%25203D%2520Infrastructure%2520Inspection%2520with%2520a%2520UAV%26entry.906535625%3DDavid-Alexandre%2520Poissant%2520and%2520Alexis%2520Lussier%2520Desbiens%2520and%2520Fran%25C3%25A7ois%2520Ferland%2520and%2520Louis%2520Petit%26entry.1292438233%3D%2520%2520Autonomous%2520robotic%2520inspection%2520missions%2520require%2520balancing%2520multiple%2520conflicting%250Aobjectives%2520while%2520navigating%2520near%2520costly%2520obstacles.%2520Current%2520multi-objective%2520path%250Aplanning%2520%2528MOPP%2529%2520methods%2520struggle%2520to%2520adapt%2520to%2520evolving%2520risks%2520like%2520localization%250Aerrors%252C%2520weather%252C%2520battery%2520state%252C%2520and%2520communication%2520issues.%2520This%2520letter%2520presents%250Aan%2520Adaptive%2520Risk-aware%2520and%2520Energy-efficient%2520NAvigation%2520%2528ARENA%2529%2520MOPP%2520approach%250Afor%2520UAVs%2520in%2520complex%25203D%2520environments.%2520Our%2520method%2520enables%2520online%2520trajectory%250Aadaptation%2520by%2520optimizing%2520safety%252C%2520time%252C%2520and%2520energy%2520using%25204D%2520NURBS%2520representation%250Aand%2520a%2520genetic-based%2520algorithm%2520to%2520generate%2520the%2520Pareto%2520front.%2520A%2520novel%2520risk-aware%250Avoting%2520algorithm%2520ensures%2520adaptivity.%2520Simulations%2520and%2520real-world%2520tests%250Ademonstrate%2520the%2520planner%2527s%2520ability%2520to%2520produce%2520diverse%252C%2520optimized%2520trajectories%250Acovering%252095%2525%2520or%2520more%2520of%2520the%2520range%2520defined%2520by%2520single-objective%2520benchmarks%2520and%250Aits%2520ability%2520to%2520estimate%2520power%2520consumption%2520with%2520a%2520mean%2520error%2520representing%252014%2525%2520of%250Athe%2520full%2520power%2520range.%2520The%2520ARENA%2520framework%2520enhances%2520UAV%2520autonomy%2520and%2520reliability%250Ain%2520critical%252C%2520evolving%25203D%2520missions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARENA%3A%20Adaptive%20Risk-aware%20and%20Energy-efficient%20NAvigation%20for%0A%20%20Multi-Objective%203D%20Infrastructure%20Inspection%20with%20a%20UAV&entry.906535625=David-Alexandre%20Poissant%20and%20Alexis%20Lussier%20Desbiens%20and%20Fran%C3%A7ois%20Ferland%20and%20Louis%20Petit&entry.1292438233=%20%20Autonomous%20robotic%20inspection%20missions%20require%20balancing%20multiple%20conflicting%0Aobjectives%20while%20navigating%20near%20costly%20obstacles.%20Current%20multi-objective%20path%0Aplanning%20%28MOPP%29%20methods%20struggle%20to%20adapt%20to%20evolving%20risks%20like%20localization%0Aerrors%2C%20weather%2C%20battery%20state%2C%20and%20communication%20issues.%20This%20letter%20presents%0Aan%20Adaptive%20Risk-aware%20and%20Energy-efficient%20NAvigation%20%28ARENA%29%20MOPP%20approach%0Afor%20UAVs%20in%20complex%203D%20environments.%20Our%20method%20enables%20online%20trajectory%0Aadaptation%20by%20optimizing%20safety%2C%20time%2C%20and%20energy%20using%204D%20NURBS%20representation%0Aand%20a%20genetic-based%20algorithm%20to%20generate%20the%20Pareto%20front.%20A%20novel%20risk-aware%0Avoting%20algorithm%20ensures%20adaptivity.%20Simulations%20and%20real-world%20tests%0Ademonstrate%20the%20planner%27s%20ability%20to%20produce%20diverse%2C%20optimized%20trajectories%0Acovering%2095%25%20or%20more%20of%20the%20range%20defined%20by%20single-objective%20benchmarks%20and%0Aits%20ability%20to%20estimate%20power%20consumption%20with%20a%20mean%20error%20representing%2014%25%20of%0Athe%20full%20power%20range.%20The%20ARENA%20framework%20enhances%20UAV%20autonomy%20and%20reliability%0Ain%20critical%2C%20evolving%203D%20missions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19401v1&entry.124074799=Read"},
{"title": "ChemScraper: Leveraging PDF Graphics Instructions for Molecular Diagram\n  Parsing", "author": "Ayush Kumar Shah and Bryan Manrique Amador and Abhisek Dey and Ming Creekmore and Blake Ocampo and Scott Denmark and Richard Zanibbi", "abstract": "  Most molecular diagram parsers recover chemical structure from raster images\n(e.g., PNGs). However, many PDFs include commands giving explicit locations and\nshapes for characters, lines, and polygons. We present a new parser that uses\nthese born-digital PDF primitives as input. The parsing model is fast and\naccurate, and does not require GPUs, Optical Character Recognition (OCR), or\nvectorization. We use the parser to annotate raster images and then train a new\nmulti-task neural network for recognizing molecules in raster images. We\nevaluate our parsers using SMILES and standard benchmarks, along with a novel\nevaluation protocol comparing molecular graphs directly that supports automatic\nerror compilation and reveals errors missed by SMILES-based evaluation. On the\nsynthetic USPTO benchmark, our born-digital parser obtains a recognition rate\nof 98.4% (1% higher than previous models) and our relatively simple neural\nparser for raster images obtains a rate of 85% using less training data than\nexisting neural approaches (thousands vs. millions of molecules).\n", "link": "http://arxiv.org/abs/2311.12161v5", "date": "2025-02-26", "relevancy": 2.1491, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4343}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4276}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChemScraper%3A%20Leveraging%20PDF%20Graphics%20Instructions%20for%20Molecular%20Diagram%0A%20%20Parsing&body=Title%3A%20ChemScraper%3A%20Leveraging%20PDF%20Graphics%20Instructions%20for%20Molecular%20Diagram%0A%20%20Parsing%0AAuthor%3A%20Ayush%20Kumar%20Shah%20and%20Bryan%20Manrique%20Amador%20and%20Abhisek%20Dey%20and%20Ming%20Creekmore%20and%20Blake%20Ocampo%20and%20Scott%20Denmark%20and%20Richard%20Zanibbi%0AAbstract%3A%20%20%20Most%20molecular%20diagram%20parsers%20recover%20chemical%20structure%20from%20raster%20images%0A%28e.g.%2C%20PNGs%29.%20However%2C%20many%20PDFs%20include%20commands%20giving%20explicit%20locations%20and%0Ashapes%20for%20characters%2C%20lines%2C%20and%20polygons.%20We%20present%20a%20new%20parser%20that%20uses%0Athese%20born-digital%20PDF%20primitives%20as%20input.%20The%20parsing%20model%20is%20fast%20and%0Aaccurate%2C%20and%20does%20not%20require%20GPUs%2C%20Optical%20Character%20Recognition%20%28OCR%29%2C%20or%0Avectorization.%20We%20use%20the%20parser%20to%20annotate%20raster%20images%20and%20then%20train%20a%20new%0Amulti-task%20neural%20network%20for%20recognizing%20molecules%20in%20raster%20images.%20We%0Aevaluate%20our%20parsers%20using%20SMILES%20and%20standard%20benchmarks%2C%20along%20with%20a%20novel%0Aevaluation%20protocol%20comparing%20molecular%20graphs%20directly%20that%20supports%20automatic%0Aerror%20compilation%20and%20reveals%20errors%20missed%20by%20SMILES-based%20evaluation.%20On%20the%0Asynthetic%20USPTO%20benchmark%2C%20our%20born-digital%20parser%20obtains%20a%20recognition%20rate%0Aof%2098.4%25%20%281%25%20higher%20than%20previous%20models%29%20and%20our%20relatively%20simple%20neural%0Aparser%20for%20raster%20images%20obtains%20a%20rate%20of%2085%25%20using%20less%20training%20data%20than%0Aexisting%20neural%20approaches%20%28thousands%20vs.%20millions%20of%20molecules%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12161v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChemScraper%253A%2520Leveraging%2520PDF%2520Graphics%2520Instructions%2520for%2520Molecular%2520Diagram%250A%2520%2520Parsing%26entry.906535625%3DAyush%2520Kumar%2520Shah%2520and%2520Bryan%2520Manrique%2520Amador%2520and%2520Abhisek%2520Dey%2520and%2520Ming%2520Creekmore%2520and%2520Blake%2520Ocampo%2520and%2520Scott%2520Denmark%2520and%2520Richard%2520Zanibbi%26entry.1292438233%3D%2520%2520Most%2520molecular%2520diagram%2520parsers%2520recover%2520chemical%2520structure%2520from%2520raster%2520images%250A%2528e.g.%252C%2520PNGs%2529.%2520However%252C%2520many%2520PDFs%2520include%2520commands%2520giving%2520explicit%2520locations%2520and%250Ashapes%2520for%2520characters%252C%2520lines%252C%2520and%2520polygons.%2520We%2520present%2520a%2520new%2520parser%2520that%2520uses%250Athese%2520born-digital%2520PDF%2520primitives%2520as%2520input.%2520The%2520parsing%2520model%2520is%2520fast%2520and%250Aaccurate%252C%2520and%2520does%2520not%2520require%2520GPUs%252C%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%252C%2520or%250Avectorization.%2520We%2520use%2520the%2520parser%2520to%2520annotate%2520raster%2520images%2520and%2520then%2520train%2520a%2520new%250Amulti-task%2520neural%2520network%2520for%2520recognizing%2520molecules%2520in%2520raster%2520images.%2520We%250Aevaluate%2520our%2520parsers%2520using%2520SMILES%2520and%2520standard%2520benchmarks%252C%2520along%2520with%2520a%2520novel%250Aevaluation%2520protocol%2520comparing%2520molecular%2520graphs%2520directly%2520that%2520supports%2520automatic%250Aerror%2520compilation%2520and%2520reveals%2520errors%2520missed%2520by%2520SMILES-based%2520evaluation.%2520On%2520the%250Asynthetic%2520USPTO%2520benchmark%252C%2520our%2520born-digital%2520parser%2520obtains%2520a%2520recognition%2520rate%250Aof%252098.4%2525%2520%25281%2525%2520higher%2520than%2520previous%2520models%2529%2520and%2520our%2520relatively%2520simple%2520neural%250Aparser%2520for%2520raster%2520images%2520obtains%2520a%2520rate%2520of%252085%2525%2520using%2520less%2520training%2520data%2520than%250Aexisting%2520neural%2520approaches%2520%2528thousands%2520vs.%2520millions%2520of%2520molecules%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12161v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChemScraper%3A%20Leveraging%20PDF%20Graphics%20Instructions%20for%20Molecular%20Diagram%0A%20%20Parsing&entry.906535625=Ayush%20Kumar%20Shah%20and%20Bryan%20Manrique%20Amador%20and%20Abhisek%20Dey%20and%20Ming%20Creekmore%20and%20Blake%20Ocampo%20and%20Scott%20Denmark%20and%20Richard%20Zanibbi&entry.1292438233=%20%20Most%20molecular%20diagram%20parsers%20recover%20chemical%20structure%20from%20raster%20images%0A%28e.g.%2C%20PNGs%29.%20However%2C%20many%20PDFs%20include%20commands%20giving%20explicit%20locations%20and%0Ashapes%20for%20characters%2C%20lines%2C%20and%20polygons.%20We%20present%20a%20new%20parser%20that%20uses%0Athese%20born-digital%20PDF%20primitives%20as%20input.%20The%20parsing%20model%20is%20fast%20and%0Aaccurate%2C%20and%20does%20not%20require%20GPUs%2C%20Optical%20Character%20Recognition%20%28OCR%29%2C%20or%0Avectorization.%20We%20use%20the%20parser%20to%20annotate%20raster%20images%20and%20then%20train%20a%20new%0Amulti-task%20neural%20network%20for%20recognizing%20molecules%20in%20raster%20images.%20We%0Aevaluate%20our%20parsers%20using%20SMILES%20and%20standard%20benchmarks%2C%20along%20with%20a%20novel%0Aevaluation%20protocol%20comparing%20molecular%20graphs%20directly%20that%20supports%20automatic%0Aerror%20compilation%20and%20reveals%20errors%20missed%20by%20SMILES-based%20evaluation.%20On%20the%0Asynthetic%20USPTO%20benchmark%2C%20our%20born-digital%20parser%20obtains%20a%20recognition%20rate%0Aof%2098.4%25%20%281%25%20higher%20than%20previous%20models%29%20and%20our%20relatively%20simple%20neural%0Aparser%20for%20raster%20images%20obtains%20a%20rate%20of%2085%25%20using%20less%20training%20data%20than%0Aexisting%20neural%20approaches%20%28thousands%20vs.%20millions%20of%20molecules%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12161v5&entry.124074799=Read"},
{"title": "Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences", "author": "Nikolaos Dimitriadis and Pascal Frossard and Francois Fleuret", "abstract": "  Multi-task trade-offs in machine learning can be addressed via Pareto Front\nLearning (PFL) methods that parameterize the Pareto Front (PF) with a single\nmodel. PFL permits to select the desired operational point during inference,\ncontrary to traditional Multi-Task Learning (MTL) that optimizes for a single\ntrade-off decided prior to training. However, recent PFL methodologies suffer\nfrom limited scalability, slow convergence, and excessive memory requirements,\nwhile exhibiting inconsistent mappings from preference to objective space. We\nintroduce PaLoRA, a novel parameter-efficient method that addresses these\nlimitations in two ways. First, we augment any neural network architecture with\ntask-specific low-rank adapters and continuously parameterize the PF in their\nconvex hull. Our approach steers the original model and the adapters towards\nlearning general and task-specific features, respectively. Second, we propose a\ndeterministic sampling schedule of preference vectors that reinforces this\ndivision of labor, enabling faster convergence and strengthening the validity\nof the mapping from preference to objective space throughout training. Our\nexperiments show that PaLoRA outperforms state-of-the-art MTL and PFL baselines\nacross various datasets, scales to large networks, reducing the memory overhead\n$23.8-31.7$ times compared with competing PFL baselines in scene understanding\nbenchmarks.\n", "link": "http://arxiv.org/abs/2407.08056v2", "date": "2025-02-26", "relevancy": 2.1416, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pareto%20Low-Rank%20Adapters%3A%20Efficient%20Multi-Task%20Learning%20with%20Preferences&body=Title%3A%20Pareto%20Low-Rank%20Adapters%3A%20Efficient%20Multi-Task%20Learning%20with%20Preferences%0AAuthor%3A%20Nikolaos%20Dimitriadis%20and%20Pascal%20Frossard%20and%20Francois%20Fleuret%0AAbstract%3A%20%20%20Multi-task%20trade-offs%20in%20machine%20learning%20can%20be%20addressed%20via%20Pareto%20Front%0ALearning%20%28PFL%29%20methods%20that%20parameterize%20the%20Pareto%20Front%20%28PF%29%20with%20a%20single%0Amodel.%20PFL%20permits%20to%20select%20the%20desired%20operational%20point%20during%20inference%2C%0Acontrary%20to%20traditional%20Multi-Task%20Learning%20%28MTL%29%20that%20optimizes%20for%20a%20single%0Atrade-off%20decided%20prior%20to%20training.%20However%2C%20recent%20PFL%20methodologies%20suffer%0Afrom%20limited%20scalability%2C%20slow%20convergence%2C%20and%20excessive%20memory%20requirements%2C%0Awhile%20exhibiting%20inconsistent%20mappings%20from%20preference%20to%20objective%20space.%20We%0Aintroduce%20PaLoRA%2C%20a%20novel%20parameter-efficient%20method%20that%20addresses%20these%0Alimitations%20in%20two%20ways.%20First%2C%20we%20augment%20any%20neural%20network%20architecture%20with%0Atask-specific%20low-rank%20adapters%20and%20continuously%20parameterize%20the%20PF%20in%20their%0Aconvex%20hull.%20Our%20approach%20steers%20the%20original%20model%20and%20the%20adapters%20towards%0Alearning%20general%20and%20task-specific%20features%2C%20respectively.%20Second%2C%20we%20propose%20a%0Adeterministic%20sampling%20schedule%20of%20preference%20vectors%20that%20reinforces%20this%0Adivision%20of%20labor%2C%20enabling%20faster%20convergence%20and%20strengthening%20the%20validity%0Aof%20the%20mapping%20from%20preference%20to%20objective%20space%20throughout%20training.%20Our%0Aexperiments%20show%20that%20PaLoRA%20outperforms%20state-of-the-art%20MTL%20and%20PFL%20baselines%0Aacross%20various%20datasets%2C%20scales%20to%20large%20networks%2C%20reducing%20the%20memory%20overhead%0A%2423.8-31.7%24%20times%20compared%20with%20competing%20PFL%20baselines%20in%20scene%20understanding%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08056v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPareto%2520Low-Rank%2520Adapters%253A%2520Efficient%2520Multi-Task%2520Learning%2520with%2520Preferences%26entry.906535625%3DNikolaos%2520Dimitriadis%2520and%2520Pascal%2520Frossard%2520and%2520Francois%2520Fleuret%26entry.1292438233%3D%2520%2520Multi-task%2520trade-offs%2520in%2520machine%2520learning%2520can%2520be%2520addressed%2520via%2520Pareto%2520Front%250ALearning%2520%2528PFL%2529%2520methods%2520that%2520parameterize%2520the%2520Pareto%2520Front%2520%2528PF%2529%2520with%2520a%2520single%250Amodel.%2520PFL%2520permits%2520to%2520select%2520the%2520desired%2520operational%2520point%2520during%2520inference%252C%250Acontrary%2520to%2520traditional%2520Multi-Task%2520Learning%2520%2528MTL%2529%2520that%2520optimizes%2520for%2520a%2520single%250Atrade-off%2520decided%2520prior%2520to%2520training.%2520However%252C%2520recent%2520PFL%2520methodologies%2520suffer%250Afrom%2520limited%2520scalability%252C%2520slow%2520convergence%252C%2520and%2520excessive%2520memory%2520requirements%252C%250Awhile%2520exhibiting%2520inconsistent%2520mappings%2520from%2520preference%2520to%2520objective%2520space.%2520We%250Aintroduce%2520PaLoRA%252C%2520a%2520novel%2520parameter-efficient%2520method%2520that%2520addresses%2520these%250Alimitations%2520in%2520two%2520ways.%2520First%252C%2520we%2520augment%2520any%2520neural%2520network%2520architecture%2520with%250Atask-specific%2520low-rank%2520adapters%2520and%2520continuously%2520parameterize%2520the%2520PF%2520in%2520their%250Aconvex%2520hull.%2520Our%2520approach%2520steers%2520the%2520original%2520model%2520and%2520the%2520adapters%2520towards%250Alearning%2520general%2520and%2520task-specific%2520features%252C%2520respectively.%2520Second%252C%2520we%2520propose%2520a%250Adeterministic%2520sampling%2520schedule%2520of%2520preference%2520vectors%2520that%2520reinforces%2520this%250Adivision%2520of%2520labor%252C%2520enabling%2520faster%2520convergence%2520and%2520strengthening%2520the%2520validity%250Aof%2520the%2520mapping%2520from%2520preference%2520to%2520objective%2520space%2520throughout%2520training.%2520Our%250Aexperiments%2520show%2520that%2520PaLoRA%2520outperforms%2520state-of-the-art%2520MTL%2520and%2520PFL%2520baselines%250Aacross%2520various%2520datasets%252C%2520scales%2520to%2520large%2520networks%252C%2520reducing%2520the%2520memory%2520overhead%250A%252423.8-31.7%2524%2520times%2520compared%2520with%2520competing%2520PFL%2520baselines%2520in%2520scene%2520understanding%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08056v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pareto%20Low-Rank%20Adapters%3A%20Efficient%20Multi-Task%20Learning%20with%20Preferences&entry.906535625=Nikolaos%20Dimitriadis%20and%20Pascal%20Frossard%20and%20Francois%20Fleuret&entry.1292438233=%20%20Multi-task%20trade-offs%20in%20machine%20learning%20can%20be%20addressed%20via%20Pareto%20Front%0ALearning%20%28PFL%29%20methods%20that%20parameterize%20the%20Pareto%20Front%20%28PF%29%20with%20a%20single%0Amodel.%20PFL%20permits%20to%20select%20the%20desired%20operational%20point%20during%20inference%2C%0Acontrary%20to%20traditional%20Multi-Task%20Learning%20%28MTL%29%20that%20optimizes%20for%20a%20single%0Atrade-off%20decided%20prior%20to%20training.%20However%2C%20recent%20PFL%20methodologies%20suffer%0Afrom%20limited%20scalability%2C%20slow%20convergence%2C%20and%20excessive%20memory%20requirements%2C%0Awhile%20exhibiting%20inconsistent%20mappings%20from%20preference%20to%20objective%20space.%20We%0Aintroduce%20PaLoRA%2C%20a%20novel%20parameter-efficient%20method%20that%20addresses%20these%0Alimitations%20in%20two%20ways.%20First%2C%20we%20augment%20any%20neural%20network%20architecture%20with%0Atask-specific%20low-rank%20adapters%20and%20continuously%20parameterize%20the%20PF%20in%20their%0Aconvex%20hull.%20Our%20approach%20steers%20the%20original%20model%20and%20the%20adapters%20towards%0Alearning%20general%20and%20task-specific%20features%2C%20respectively.%20Second%2C%20we%20propose%20a%0Adeterministic%20sampling%20schedule%20of%20preference%20vectors%20that%20reinforces%20this%0Adivision%20of%20labor%2C%20enabling%20faster%20convergence%20and%20strengthening%20the%20validity%0Aof%20the%20mapping%20from%20preference%20to%20objective%20space%20throughout%20training.%20Our%0Aexperiments%20show%20that%20PaLoRA%20outperforms%20state-of-the-art%20MTL%20and%20PFL%20baselines%0Aacross%20various%20datasets%2C%20scales%20to%20large%20networks%2C%20reducing%20the%20memory%20overhead%0A%2423.8-31.7%24%20times%20compared%20with%20competing%20PFL%20baselines%20in%20scene%20understanding%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08056v2&entry.124074799=Read"},
{"title": "DataMan: Data Manager for Pre-training Large Language Models", "author": "Ru Peng and Kexin Yang and Yawen Zeng and Junyang Lin and Dayiheng Liu and Junbo Zhao", "abstract": "  The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.\n", "link": "http://arxiv.org/abs/2502.19363v1", "date": "2025-02-26", "relevancy": 2.1386, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5339}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataMan%3A%20Data%20Manager%20for%20Pre-training%20Large%20Language%20Models&body=Title%3A%20DataMan%3A%20Data%20Manager%20for%20Pre-training%20Large%20Language%20Models%0AAuthor%3A%20Ru%20Peng%20and%20Kexin%20Yang%20and%20Yawen%20Zeng%20and%20Junyang%20Lin%20and%20Dayiheng%20Liu%20and%20Junbo%20Zhao%0AAbstract%3A%20%20%20The%20performance%20emergence%20of%20large%20language%20models%20%28LLMs%29%20driven%20by%20data%0Ascaling%20laws%20makes%20the%20selection%20of%20pre-training%20data%20increasingly%20important.%0AHowever%2C%20existing%20methods%20rely%20on%20limited%20heuristics%20and%20human%20intuition%2C%0Alacking%20comprehensive%20and%20clear%20guidelines.%20To%20address%20this%2C%20we%20are%20inspired%20by%0A%60%60reverse%20thinking%27%27%20--%20prompting%20LLMs%20to%20self-identify%20which%20criteria%20benefit%0Aits%20performance.%20As%20its%20pre-training%20capabilities%20are%20related%20to%20perplexity%0A%28PPL%29%2C%20we%20derive%2014%20quality%20criteria%20from%20the%20causes%20of%20text%20perplexity%0Aanomalies%20and%20introduce%2015%20common%20application%20domains%20to%20support%20domain%20mixing.%0AIn%20this%20paper%2C%20we%20train%20a%20Data%20Manager%20%28DataMan%29%20to%20learn%20quality%20ratings%20and%0Adomain%20recognition%20from%20pointwise%20rating%2C%20and%20use%20it%20to%20annotate%20a%20447B%20token%0Apre-training%20corpus%20with%2014%20quality%20ratings%20and%20domain%20type.%20Our%20experiments%0Avalidate%20our%20approach%2C%20using%20DataMan%20to%20select%2030B%20tokens%20to%20train%20a%0A1.3B-parameter%20language%20model%2C%20demonstrating%20significant%20improvements%20in%0Ain-context%20learning%20%28ICL%29%2C%20perplexity%2C%20and%20instruction-following%20ability%20over%0Athe%20state-of-the-art%20baseline.%20The%20best-performing%20model%2C%20based%20on%20the%20Overall%0AScore%20l%3D5%20surpasses%20a%20model%20trained%20with%2050%25%20more%20data%20using%20uniform%20sampling.%0AWe%20continue%20pre-training%20with%20high-rated%2C%20domain-specific%20data%20annotated%20by%0ADataMan%20to%20enhance%20domain-specific%20ICL%20performance%20and%20thus%20verify%20DataMan%27s%0Adomain%20mixing%20ability.%20Our%20findings%20emphasize%20the%20importance%20of%20quality%0Aranking%2C%20the%20complementary%20nature%20of%20quality%20criteria%2C%20and%20their%20low%0Acorrelation%20with%20perplexity%2C%20analyzing%20misalignment%20between%20PPL%20and%20ICL%0Aperformance.%20We%20also%20thoroughly%20analyzed%20our%20pre-training%20dataset%2C%20examining%0Aits%20composition%2C%20the%20distribution%20of%20quality%20ratings%2C%20and%20the%20original%20document%0Asources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataMan%253A%2520Data%2520Manager%2520for%2520Pre-training%2520Large%2520Language%2520Models%26entry.906535625%3DRu%2520Peng%2520and%2520Kexin%2520Yang%2520and%2520Yawen%2520Zeng%2520and%2520Junyang%2520Lin%2520and%2520Dayiheng%2520Liu%2520and%2520Junbo%2520Zhao%26entry.1292438233%3D%2520%2520The%2520performance%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520driven%2520by%2520data%250Ascaling%2520laws%2520makes%2520the%2520selection%2520of%2520pre-training%2520data%2520increasingly%2520important.%250AHowever%252C%2520existing%2520methods%2520rely%2520on%2520limited%2520heuristics%2520and%2520human%2520intuition%252C%250Alacking%2520comprehensive%2520and%2520clear%2520guidelines.%2520To%2520address%2520this%252C%2520we%2520are%2520inspired%2520by%250A%2560%2560reverse%2520thinking%2527%2527%2520--%2520prompting%2520LLMs%2520to%2520self-identify%2520which%2520criteria%2520benefit%250Aits%2520performance.%2520As%2520its%2520pre-training%2520capabilities%2520are%2520related%2520to%2520perplexity%250A%2528PPL%2529%252C%2520we%2520derive%252014%2520quality%2520criteria%2520from%2520the%2520causes%2520of%2520text%2520perplexity%250Aanomalies%2520and%2520introduce%252015%2520common%2520application%2520domains%2520to%2520support%2520domain%2520mixing.%250AIn%2520this%2520paper%252C%2520we%2520train%2520a%2520Data%2520Manager%2520%2528DataMan%2529%2520to%2520learn%2520quality%2520ratings%2520and%250Adomain%2520recognition%2520from%2520pointwise%2520rating%252C%2520and%2520use%2520it%2520to%2520annotate%2520a%2520447B%2520token%250Apre-training%2520corpus%2520with%252014%2520quality%2520ratings%2520and%2520domain%2520type.%2520Our%2520experiments%250Avalidate%2520our%2520approach%252C%2520using%2520DataMan%2520to%2520select%252030B%2520tokens%2520to%2520train%2520a%250A1.3B-parameter%2520language%2520model%252C%2520demonstrating%2520significant%2520improvements%2520in%250Ain-context%2520learning%2520%2528ICL%2529%252C%2520perplexity%252C%2520and%2520instruction-following%2520ability%2520over%250Athe%2520state-of-the-art%2520baseline.%2520The%2520best-performing%2520model%252C%2520based%2520on%2520the%2520Overall%250AScore%2520l%253D5%2520surpasses%2520a%2520model%2520trained%2520with%252050%2525%2520more%2520data%2520using%2520uniform%2520sampling.%250AWe%2520continue%2520pre-training%2520with%2520high-rated%252C%2520domain-specific%2520data%2520annotated%2520by%250ADataMan%2520to%2520enhance%2520domain-specific%2520ICL%2520performance%2520and%2520thus%2520verify%2520DataMan%2527s%250Adomain%2520mixing%2520ability.%2520Our%2520findings%2520emphasize%2520the%2520importance%2520of%2520quality%250Aranking%252C%2520the%2520complementary%2520nature%2520of%2520quality%2520criteria%252C%2520and%2520their%2520low%250Acorrelation%2520with%2520perplexity%252C%2520analyzing%2520misalignment%2520between%2520PPL%2520and%2520ICL%250Aperformance.%2520We%2520also%2520thoroughly%2520analyzed%2520our%2520pre-training%2520dataset%252C%2520examining%250Aits%2520composition%252C%2520the%2520distribution%2520of%2520quality%2520ratings%252C%2520and%2520the%2520original%2520document%250Asources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataMan%3A%20Data%20Manager%20for%20Pre-training%20Large%20Language%20Models&entry.906535625=Ru%20Peng%20and%20Kexin%20Yang%20and%20Yawen%20Zeng%20and%20Junyang%20Lin%20and%20Dayiheng%20Liu%20and%20Junbo%20Zhao&entry.1292438233=%20%20The%20performance%20emergence%20of%20large%20language%20models%20%28LLMs%29%20driven%20by%20data%0Ascaling%20laws%20makes%20the%20selection%20of%20pre-training%20data%20increasingly%20important.%0AHowever%2C%20existing%20methods%20rely%20on%20limited%20heuristics%20and%20human%20intuition%2C%0Alacking%20comprehensive%20and%20clear%20guidelines.%20To%20address%20this%2C%20we%20are%20inspired%20by%0A%60%60reverse%20thinking%27%27%20--%20prompting%20LLMs%20to%20self-identify%20which%20criteria%20benefit%0Aits%20performance.%20As%20its%20pre-training%20capabilities%20are%20related%20to%20perplexity%0A%28PPL%29%2C%20we%20derive%2014%20quality%20criteria%20from%20the%20causes%20of%20text%20perplexity%0Aanomalies%20and%20introduce%2015%20common%20application%20domains%20to%20support%20domain%20mixing.%0AIn%20this%20paper%2C%20we%20train%20a%20Data%20Manager%20%28DataMan%29%20to%20learn%20quality%20ratings%20and%0Adomain%20recognition%20from%20pointwise%20rating%2C%20and%20use%20it%20to%20annotate%20a%20447B%20token%0Apre-training%20corpus%20with%2014%20quality%20ratings%20and%20domain%20type.%20Our%20experiments%0Avalidate%20our%20approach%2C%20using%20DataMan%20to%20select%2030B%20tokens%20to%20train%20a%0A1.3B-parameter%20language%20model%2C%20demonstrating%20significant%20improvements%20in%0Ain-context%20learning%20%28ICL%29%2C%20perplexity%2C%20and%20instruction-following%20ability%20over%0Athe%20state-of-the-art%20baseline.%20The%20best-performing%20model%2C%20based%20on%20the%20Overall%0AScore%20l%3D5%20surpasses%20a%20model%20trained%20with%2050%25%20more%20data%20using%20uniform%20sampling.%0AWe%20continue%20pre-training%20with%20high-rated%2C%20domain-specific%20data%20annotated%20by%0ADataMan%20to%20enhance%20domain-specific%20ICL%20performance%20and%20thus%20verify%20DataMan%27s%0Adomain%20mixing%20ability.%20Our%20findings%20emphasize%20the%20importance%20of%20quality%0Aranking%2C%20the%20complementary%20nature%20of%20quality%20criteria%2C%20and%20their%20low%0Acorrelation%20with%20perplexity%2C%20analyzing%20misalignment%20between%20PPL%20and%20ICL%0Aperformance.%20We%20also%20thoroughly%20analyzed%20our%20pre-training%20dataset%2C%20examining%0Aits%20composition%2C%20the%20distribution%20of%20quality%20ratings%2C%20and%20the%20original%20document%0Asources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19363v1&entry.124074799=Read"},
{"title": "Arbitrary Volumetric Refocusing of Dense and Sparse Light Fields", "author": "Tharindu Samarakoon and Kalana Abeywardena and Chamira U. S. Edussooriya", "abstract": "  A four-dimensional light field (LF) captures both textural and geometrical\ninformation of a scene in contrast to a two-dimensional image that captures\nonly the textural information of a scene. Post-capture refocusing is an\nexciting application of LFs enabled by the geometric information captured.\nPreviously proposed LF refocusing methods are mostly limited to the refocusing\nof single planar or volumetric region of a scene corresponding to a depth range\nand cannot simultaneously generate in-focus and out-of-focus regions having the\nsame depth range. In this paper, we propose an end-to-end pipeline to\nsimultaneously refocus multiple arbitrary planar or volumetric regions of a\ndense or a sparse LF. We employ pixel-dependent shifts with the typical\nshift-and-sum method to refocus an LF. The pixel-dependent shifts enables to\nrefocus each pixel of an LF independently. For sparse LFs, the shift-and-sum\nmethod introduces ghosting artifacts due to the spatial undersampling. We\nemploy a deep learning model based on U-Net architecture to almost completely\neliminate the ghosting artifacts. The experimental results obtained with\nseveral LF datasets confirm the effectiveness of the proposed method. In\nparticular, sparse LFs refocused with the proposed method archive structural\nsimilarity index higher than 0.9 despite having only 20% of data compared to\ndense LFs.\n", "link": "http://arxiv.org/abs/2502.19238v1", "date": "2025-02-26", "relevancy": 2.1342, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arbitrary%20Volumetric%20Refocusing%20of%20Dense%20and%20Sparse%20Light%20Fields&body=Title%3A%20Arbitrary%20Volumetric%20Refocusing%20of%20Dense%20and%20Sparse%20Light%20Fields%0AAuthor%3A%20Tharindu%20Samarakoon%20and%20Kalana%20Abeywardena%20and%20Chamira%20U.%20S.%20Edussooriya%0AAbstract%3A%20%20%20A%20four-dimensional%20light%20field%20%28LF%29%20captures%20both%20textural%20and%20geometrical%0Ainformation%20of%20a%20scene%20in%20contrast%20to%20a%20two-dimensional%20image%20that%20captures%0Aonly%20the%20textural%20information%20of%20a%20scene.%20Post-capture%20refocusing%20is%20an%0Aexciting%20application%20of%20LFs%20enabled%20by%20the%20geometric%20information%20captured.%0APreviously%20proposed%20LF%20refocusing%20methods%20are%20mostly%20limited%20to%20the%20refocusing%0Aof%20single%20planar%20or%20volumetric%20region%20of%20a%20scene%20corresponding%20to%20a%20depth%20range%0Aand%20cannot%20simultaneously%20generate%20in-focus%20and%20out-of-focus%20regions%20having%20the%0Asame%20depth%20range.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%20pipeline%20to%0Asimultaneously%20refocus%20multiple%20arbitrary%20planar%20or%20volumetric%20regions%20of%20a%0Adense%20or%20a%20sparse%20LF.%20We%20employ%20pixel-dependent%20shifts%20with%20the%20typical%0Ashift-and-sum%20method%20to%20refocus%20an%20LF.%20The%20pixel-dependent%20shifts%20enables%20to%0Arefocus%20each%20pixel%20of%20an%20LF%20independently.%20For%20sparse%20LFs%2C%20the%20shift-and-sum%0Amethod%20introduces%20ghosting%20artifacts%20due%20to%20the%20spatial%20undersampling.%20We%0Aemploy%20a%20deep%20learning%20model%20based%20on%20U-Net%20architecture%20to%20almost%20completely%0Aeliminate%20the%20ghosting%20artifacts.%20The%20experimental%20results%20obtained%20with%0Aseveral%20LF%20datasets%20confirm%20the%20effectiveness%20of%20the%20proposed%20method.%20In%0Aparticular%2C%20sparse%20LFs%20refocused%20with%20the%20proposed%20method%20archive%20structural%0Asimilarity%20index%20higher%20than%200.9%20despite%20having%20only%2020%25%20of%20data%20compared%20to%0Adense%20LFs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArbitrary%2520Volumetric%2520Refocusing%2520of%2520Dense%2520and%2520Sparse%2520Light%2520Fields%26entry.906535625%3DTharindu%2520Samarakoon%2520and%2520Kalana%2520Abeywardena%2520and%2520Chamira%2520U.%2520S.%2520Edussooriya%26entry.1292438233%3D%2520%2520A%2520four-dimensional%2520light%2520field%2520%2528LF%2529%2520captures%2520both%2520textural%2520and%2520geometrical%250Ainformation%2520of%2520a%2520scene%2520in%2520contrast%2520to%2520a%2520two-dimensional%2520image%2520that%2520captures%250Aonly%2520the%2520textural%2520information%2520of%2520a%2520scene.%2520Post-capture%2520refocusing%2520is%2520an%250Aexciting%2520application%2520of%2520LFs%2520enabled%2520by%2520the%2520geometric%2520information%2520captured.%250APreviously%2520proposed%2520LF%2520refocusing%2520methods%2520are%2520mostly%2520limited%2520to%2520the%2520refocusing%250Aof%2520single%2520planar%2520or%2520volumetric%2520region%2520of%2520a%2520scene%2520corresponding%2520to%2520a%2520depth%2520range%250Aand%2520cannot%2520simultaneously%2520generate%2520in-focus%2520and%2520out-of-focus%2520regions%2520having%2520the%250Asame%2520depth%2520range.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520end-to-end%2520pipeline%2520to%250Asimultaneously%2520refocus%2520multiple%2520arbitrary%2520planar%2520or%2520volumetric%2520regions%2520of%2520a%250Adense%2520or%2520a%2520sparse%2520LF.%2520We%2520employ%2520pixel-dependent%2520shifts%2520with%2520the%2520typical%250Ashift-and-sum%2520method%2520to%2520refocus%2520an%2520LF.%2520The%2520pixel-dependent%2520shifts%2520enables%2520to%250Arefocus%2520each%2520pixel%2520of%2520an%2520LF%2520independently.%2520For%2520sparse%2520LFs%252C%2520the%2520shift-and-sum%250Amethod%2520introduces%2520ghosting%2520artifacts%2520due%2520to%2520the%2520spatial%2520undersampling.%2520We%250Aemploy%2520a%2520deep%2520learning%2520model%2520based%2520on%2520U-Net%2520architecture%2520to%2520almost%2520completely%250Aeliminate%2520the%2520ghosting%2520artifacts.%2520The%2520experimental%2520results%2520obtained%2520with%250Aseveral%2520LF%2520datasets%2520confirm%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%2520In%250Aparticular%252C%2520sparse%2520LFs%2520refocused%2520with%2520the%2520proposed%2520method%2520archive%2520structural%250Asimilarity%2520index%2520higher%2520than%25200.9%2520despite%2520having%2520only%252020%2525%2520of%2520data%2520compared%2520to%250Adense%2520LFs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arbitrary%20Volumetric%20Refocusing%20of%20Dense%20and%20Sparse%20Light%20Fields&entry.906535625=Tharindu%20Samarakoon%20and%20Kalana%20Abeywardena%20and%20Chamira%20U.%20S.%20Edussooriya&entry.1292438233=%20%20A%20four-dimensional%20light%20field%20%28LF%29%20captures%20both%20textural%20and%20geometrical%0Ainformation%20of%20a%20scene%20in%20contrast%20to%20a%20two-dimensional%20image%20that%20captures%0Aonly%20the%20textural%20information%20of%20a%20scene.%20Post-capture%20refocusing%20is%20an%0Aexciting%20application%20of%20LFs%20enabled%20by%20the%20geometric%20information%20captured.%0APreviously%20proposed%20LF%20refocusing%20methods%20are%20mostly%20limited%20to%20the%20refocusing%0Aof%20single%20planar%20or%20volumetric%20region%20of%20a%20scene%20corresponding%20to%20a%20depth%20range%0Aand%20cannot%20simultaneously%20generate%20in-focus%20and%20out-of-focus%20regions%20having%20the%0Asame%20depth%20range.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%20pipeline%20to%0Asimultaneously%20refocus%20multiple%20arbitrary%20planar%20or%20volumetric%20regions%20of%20a%0Adense%20or%20a%20sparse%20LF.%20We%20employ%20pixel-dependent%20shifts%20with%20the%20typical%0Ashift-and-sum%20method%20to%20refocus%20an%20LF.%20The%20pixel-dependent%20shifts%20enables%20to%0Arefocus%20each%20pixel%20of%20an%20LF%20independently.%20For%20sparse%20LFs%2C%20the%20shift-and-sum%0Amethod%20introduces%20ghosting%20artifacts%20due%20to%20the%20spatial%20undersampling.%20We%0Aemploy%20a%20deep%20learning%20model%20based%20on%20U-Net%20architecture%20to%20almost%20completely%0Aeliminate%20the%20ghosting%20artifacts.%20The%20experimental%20results%20obtained%20with%0Aseveral%20LF%20datasets%20confirm%20the%20effectiveness%20of%20the%20proposed%20method.%20In%0Aparticular%2C%20sparse%20LFs%20refocused%20with%20the%20proposed%20method%20archive%20structural%0Asimilarity%20index%20higher%20than%200.9%20despite%20having%20only%2020%25%20of%20data%20compared%20to%0Adense%20LFs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19238v1&entry.124074799=Read"},
{"title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models", "author": "Yuxuan Zhang", "abstract": "  With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.\n", "link": "http://arxiv.org/abs/2502.18168v2", "date": "2025-02-26", "relevancy": 2.1306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5331}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SECURA%3A%20Sigmoid-Enhanced%20CUR%20Decomposition%20with%20Uninterrupted%20Retention%0A%20%20and%20Low-Rank%20Adaptation%20in%20Large%20Language%20Models&body=Title%3A%20SECURA%3A%20Sigmoid-Enhanced%20CUR%20Decomposition%20with%20Uninterrupted%20Retention%0A%20%20and%20Low-Rank%20Adaptation%20in%20Large%20Language%20Models%0AAuthor%3A%20Yuxuan%20Zhang%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20fully%20fine-tuning%0A%28FT%29%20these%20models%20has%20become%20increasingly%20impractical%20due%20to%20the%20high%0Acomputational%20demands.%20Additionally%2C%20FT%20can%20lead%20to%20catastrophic%20forgetting.%20As%0Aan%20alternative%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20been%20proposed%2C%20which%20fine-tunes%0Aonly%20a%20small%20subset%20of%20parameters%2C%20achieving%20similar%20performance%20to%20FT%20while%0Asignificantly%20reducing%20resource%20requirements.%20However%2C%20since%20LoRA%20inherits%20FT%27s%0Adesign%2C%20the%20issue%20of%20catastrophic%20forgetting%20remains.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20SECURA%3A%20Sigmoid-Enhanced%20CUR%0ADecomposition%20LoRA%2C%20a%20novel%20parameter-efficient%20fine-tuning%20%28PEFT%29%20variant%20that%0Amitigates%20catastrophic%20forgetting%20while%20improving%20fine-tuning%20performance.%20Our%0Amethod%20introduces%20a%20new%20normalization%20technique%2C%20SigNorm%2C%20to%20enhance%20parameter%0Aretention%20and%20overall%20performance.%0A%20%20SECURA%20has%20been%20evaluated%20on%20a%20variety%20of%20tasks%2C%20including%20mathematical%0Aproblem-solving%20%28GSM8K%29%2C%20challenging%20question-answering%20%28CNNDM%29%2C%20translation%0A%28NewsDE%29%2C%20and%20complex%20multiple-choice%20reasoning%20%28LogiQA%29.%20Experimental%20results%0Ashow%20that%20SECURA%20achieves%20an%20average%20fine-tuning%20improvement%20of%203.59%25%20across%0Afour%20multiple-choice%20question%20%28MCQ%29%20tasks%20and%20a%202.51%25%20improvement%20across%20five%0Aquestion-answering%20%28QA%29%20tasks%20on%20models%20such%20as%20Gemma2%202b%2C%20Qwen2%201.5b%2C%20Qwen%202%0A7b%2C%20Llama3%208b%2C%20and%20Llama3.1%208b%2C%20compared%20to%20DoRA.%20Moreover%2C%20SECURA%20demonstrates%0Asuperior%20knowledge%20retention%20capabilities%2C%20maintaining%20more%20than%2070%25%20accuracy%0Aon%20basic%20LLM%20knowledge%20across%2016%20continual%20learning%20tests%2C%20outperforming%0AExperience%20Replay%20%28ER%29%2C%20Sequential%20Learning%20%28SEQ%29%2C%20EWC%2C%20I-LoRA%2C%20and%20CUR-LoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18168v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSECURA%253A%2520Sigmoid-Enhanced%2520CUR%2520Decomposition%2520with%2520Uninterrupted%2520Retention%250A%2520%2520and%2520Low-Rank%2520Adaptation%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DYuxuan%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520fully%2520fine-tuning%250A%2528FT%2529%2520these%2520models%2520has%2520become%2520increasingly%2520impractical%2520due%2520to%2520the%2520high%250Acomputational%2520demands.%2520Additionally%252C%2520FT%2520can%2520lead%2520to%2520catastrophic%2520forgetting.%2520As%250Aan%2520alternative%252C%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520been%2520proposed%252C%2520which%2520fine-tunes%250Aonly%2520a%2520small%2520subset%2520of%2520parameters%252C%2520achieving%2520similar%2520performance%2520to%2520FT%2520while%250Asignificantly%2520reducing%2520resource%2520requirements.%2520However%252C%2520since%2520LoRA%2520inherits%2520FT%2527s%250Adesign%252C%2520the%2520issue%2520of%2520catastrophic%2520forgetting%2520remains.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520SECURA%253A%2520Sigmoid-Enhanced%2520CUR%250ADecomposition%2520LoRA%252C%2520a%2520novel%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520variant%2520that%250Amitigates%2520catastrophic%2520forgetting%2520while%2520improving%2520fine-tuning%2520performance.%2520Our%250Amethod%2520introduces%2520a%2520new%2520normalization%2520technique%252C%2520SigNorm%252C%2520to%2520enhance%2520parameter%250Aretention%2520and%2520overall%2520performance.%250A%2520%2520SECURA%2520has%2520been%2520evaluated%2520on%2520a%2520variety%2520of%2520tasks%252C%2520including%2520mathematical%250Aproblem-solving%2520%2528GSM8K%2529%252C%2520challenging%2520question-answering%2520%2528CNNDM%2529%252C%2520translation%250A%2528NewsDE%2529%252C%2520and%2520complex%2520multiple-choice%2520reasoning%2520%2528LogiQA%2529.%2520Experimental%2520results%250Ashow%2520that%2520SECURA%2520achieves%2520an%2520average%2520fine-tuning%2520improvement%2520of%25203.59%2525%2520across%250Afour%2520multiple-choice%2520question%2520%2528MCQ%2529%2520tasks%2520and%2520a%25202.51%2525%2520improvement%2520across%2520five%250Aquestion-answering%2520%2528QA%2529%2520tasks%2520on%2520models%2520such%2520as%2520Gemma2%25202b%252C%2520Qwen2%25201.5b%252C%2520Qwen%25202%250A7b%252C%2520Llama3%25208b%252C%2520and%2520Llama3.1%25208b%252C%2520compared%2520to%2520DoRA.%2520Moreover%252C%2520SECURA%2520demonstrates%250Asuperior%2520knowledge%2520retention%2520capabilities%252C%2520maintaining%2520more%2520than%252070%2525%2520accuracy%250Aon%2520basic%2520LLM%2520knowledge%2520across%252016%2520continual%2520learning%2520tests%252C%2520outperforming%250AExperience%2520Replay%2520%2528ER%2529%252C%2520Sequential%2520Learning%2520%2528SEQ%2529%252C%2520EWC%252C%2520I-LoRA%252C%2520and%2520CUR-LoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18168v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SECURA%3A%20Sigmoid-Enhanced%20CUR%20Decomposition%20with%20Uninterrupted%20Retention%0A%20%20and%20Low-Rank%20Adaptation%20in%20Large%20Language%20Models&entry.906535625=Yuxuan%20Zhang&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20fully%20fine-tuning%0A%28FT%29%20these%20models%20has%20become%20increasingly%20impractical%20due%20to%20the%20high%0Acomputational%20demands.%20Additionally%2C%20FT%20can%20lead%20to%20catastrophic%20forgetting.%20As%0Aan%20alternative%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20been%20proposed%2C%20which%20fine-tunes%0Aonly%20a%20small%20subset%20of%20parameters%2C%20achieving%20similar%20performance%20to%20FT%20while%0Asignificantly%20reducing%20resource%20requirements.%20However%2C%20since%20LoRA%20inherits%20FT%27s%0Adesign%2C%20the%20issue%20of%20catastrophic%20forgetting%20remains.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20SECURA%3A%20Sigmoid-Enhanced%20CUR%0ADecomposition%20LoRA%2C%20a%20novel%20parameter-efficient%20fine-tuning%20%28PEFT%29%20variant%20that%0Amitigates%20catastrophic%20forgetting%20while%20improving%20fine-tuning%20performance.%20Our%0Amethod%20introduces%20a%20new%20normalization%20technique%2C%20SigNorm%2C%20to%20enhance%20parameter%0Aretention%20and%20overall%20performance.%0A%20%20SECURA%20has%20been%20evaluated%20on%20a%20variety%20of%20tasks%2C%20including%20mathematical%0Aproblem-solving%20%28GSM8K%29%2C%20challenging%20question-answering%20%28CNNDM%29%2C%20translation%0A%28NewsDE%29%2C%20and%20complex%20multiple-choice%20reasoning%20%28LogiQA%29.%20Experimental%20results%0Ashow%20that%20SECURA%20achieves%20an%20average%20fine-tuning%20improvement%20of%203.59%25%20across%0Afour%20multiple-choice%20question%20%28MCQ%29%20tasks%20and%20a%202.51%25%20improvement%20across%20five%0Aquestion-answering%20%28QA%29%20tasks%20on%20models%20such%20as%20Gemma2%202b%2C%20Qwen2%201.5b%2C%20Qwen%202%0A7b%2C%20Llama3%208b%2C%20and%20Llama3.1%208b%2C%20compared%20to%20DoRA.%20Moreover%2C%20SECURA%20demonstrates%0Asuperior%20knowledge%20retention%20capabilities%2C%20maintaining%20more%20than%2070%25%20accuracy%0Aon%20basic%20LLM%20knowledge%20across%2016%20continual%20learning%20tests%2C%20outperforming%0AExperience%20Replay%20%28ER%29%2C%20Sequential%20Learning%20%28SEQ%29%2C%20EWC%2C%20I-LoRA%2C%20and%20CUR-LoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18168v2&entry.124074799=Read"},
{"title": "Self-supervised conformal prediction for uncertainty quantification in\n  Poisson imaging problems", "author": "Bernardin Tamo Amougou and Marcelo Pereyra and Barbara Pascal", "abstract": "  Image restoration problems are often ill-posed, leading to significant\nuncertainty in reconstructed images. Accurately quantifying this uncertainty is\nessential for the reliable interpretation of reconstructed images. However,\nimage restoration methods often lack uncertainty quantification capabilities.\nConformal prediction offers a rigorous framework to augment image restoration\nmethods with accurate uncertainty quantification estimates, but it typically\nrequires abundant ground truth data for calibration. This paper presents a\nself-supervised conformal prediction method for Poisson imaging problems which\nleverages Poisson Unbiased Risk Estimator to eliminate the need for ground\ntruth data. The resulting self-calibrating conformal prediction approach is\napplicable to any Poisson linear imaging problem that is ill-conditioned, and\nis particularly effective when combined with modern self-supervised image\nrestoration techniques trained directly on measurement data. The proposed\nmethod is demonstrated through numerical experiments on image denoising and\ndeblurring; its performance are comparable to supervised conformal prediction\nmethods relying on ground truth data.\n", "link": "http://arxiv.org/abs/2502.19194v1", "date": "2025-02-26", "relevancy": 2.1287, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5479}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5235}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20conformal%20prediction%20for%20uncertainty%20quantification%20in%0A%20%20Poisson%20imaging%20problems&body=Title%3A%20Self-supervised%20conformal%20prediction%20for%20uncertainty%20quantification%20in%0A%20%20Poisson%20imaging%20problems%0AAuthor%3A%20Bernardin%20Tamo%20Amougou%20and%20Marcelo%20Pereyra%20and%20Barbara%20Pascal%0AAbstract%3A%20%20%20Image%20restoration%20problems%20are%20often%20ill-posed%2C%20leading%20to%20significant%0Auncertainty%20in%20reconstructed%20images.%20Accurately%20quantifying%20this%20uncertainty%20is%0Aessential%20for%20the%20reliable%20interpretation%20of%20reconstructed%20images.%20However%2C%0Aimage%20restoration%20methods%20often%20lack%20uncertainty%20quantification%20capabilities.%0AConformal%20prediction%20offers%20a%20rigorous%20framework%20to%20augment%20image%20restoration%0Amethods%20with%20accurate%20uncertainty%20quantification%20estimates%2C%20but%20it%20typically%0Arequires%20abundant%20ground%20truth%20data%20for%20calibration.%20This%20paper%20presents%20a%0Aself-supervised%20conformal%20prediction%20method%20for%20Poisson%20imaging%20problems%20which%0Aleverages%20Poisson%20Unbiased%20Risk%20Estimator%20to%20eliminate%20the%20need%20for%20ground%0Atruth%20data.%20The%20resulting%20self-calibrating%20conformal%20prediction%20approach%20is%0Aapplicable%20to%20any%20Poisson%20linear%20imaging%20problem%20that%20is%20ill-conditioned%2C%20and%0Ais%20particularly%20effective%20when%20combined%20with%20modern%20self-supervised%20image%0Arestoration%20techniques%20trained%20directly%20on%20measurement%20data.%20The%20proposed%0Amethod%20is%20demonstrated%20through%20numerical%20experiments%20on%20image%20denoising%20and%0Adeblurring%3B%20its%20performance%20are%20comparable%20to%20supervised%20conformal%20prediction%0Amethods%20relying%20on%20ground%20truth%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520conformal%2520prediction%2520for%2520uncertainty%2520quantification%2520in%250A%2520%2520Poisson%2520imaging%2520problems%26entry.906535625%3DBernardin%2520Tamo%2520Amougou%2520and%2520Marcelo%2520Pereyra%2520and%2520Barbara%2520Pascal%26entry.1292438233%3D%2520%2520Image%2520restoration%2520problems%2520are%2520often%2520ill-posed%252C%2520leading%2520to%2520significant%250Auncertainty%2520in%2520reconstructed%2520images.%2520Accurately%2520quantifying%2520this%2520uncertainty%2520is%250Aessential%2520for%2520the%2520reliable%2520interpretation%2520of%2520reconstructed%2520images.%2520However%252C%250Aimage%2520restoration%2520methods%2520often%2520lack%2520uncertainty%2520quantification%2520capabilities.%250AConformal%2520prediction%2520offers%2520a%2520rigorous%2520framework%2520to%2520augment%2520image%2520restoration%250Amethods%2520with%2520accurate%2520uncertainty%2520quantification%2520estimates%252C%2520but%2520it%2520typically%250Arequires%2520abundant%2520ground%2520truth%2520data%2520for%2520calibration.%2520This%2520paper%2520presents%2520a%250Aself-supervised%2520conformal%2520prediction%2520method%2520for%2520Poisson%2520imaging%2520problems%2520which%250Aleverages%2520Poisson%2520Unbiased%2520Risk%2520Estimator%2520to%2520eliminate%2520the%2520need%2520for%2520ground%250Atruth%2520data.%2520The%2520resulting%2520self-calibrating%2520conformal%2520prediction%2520approach%2520is%250Aapplicable%2520to%2520any%2520Poisson%2520linear%2520imaging%2520problem%2520that%2520is%2520ill-conditioned%252C%2520and%250Ais%2520particularly%2520effective%2520when%2520combined%2520with%2520modern%2520self-supervised%2520image%250Arestoration%2520techniques%2520trained%2520directly%2520on%2520measurement%2520data.%2520The%2520proposed%250Amethod%2520is%2520demonstrated%2520through%2520numerical%2520experiments%2520on%2520image%2520denoising%2520and%250Adeblurring%253B%2520its%2520performance%2520are%2520comparable%2520to%2520supervised%2520conformal%2520prediction%250Amethods%2520relying%2520on%2520ground%2520truth%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20conformal%20prediction%20for%20uncertainty%20quantification%20in%0A%20%20Poisson%20imaging%20problems&entry.906535625=Bernardin%20Tamo%20Amougou%20and%20Marcelo%20Pereyra%20and%20Barbara%20Pascal&entry.1292438233=%20%20Image%20restoration%20problems%20are%20often%20ill-posed%2C%20leading%20to%20significant%0Auncertainty%20in%20reconstructed%20images.%20Accurately%20quantifying%20this%20uncertainty%20is%0Aessential%20for%20the%20reliable%20interpretation%20of%20reconstructed%20images.%20However%2C%0Aimage%20restoration%20methods%20often%20lack%20uncertainty%20quantification%20capabilities.%0AConformal%20prediction%20offers%20a%20rigorous%20framework%20to%20augment%20image%20restoration%0Amethods%20with%20accurate%20uncertainty%20quantification%20estimates%2C%20but%20it%20typically%0Arequires%20abundant%20ground%20truth%20data%20for%20calibration.%20This%20paper%20presents%20a%0Aself-supervised%20conformal%20prediction%20method%20for%20Poisson%20imaging%20problems%20which%0Aleverages%20Poisson%20Unbiased%20Risk%20Estimator%20to%20eliminate%20the%20need%20for%20ground%0Atruth%20data.%20The%20resulting%20self-calibrating%20conformal%20prediction%20approach%20is%0Aapplicable%20to%20any%20Poisson%20linear%20imaging%20problem%20that%20is%20ill-conditioned%2C%20and%0Ais%20particularly%20effective%20when%20combined%20with%20modern%20self-supervised%20image%0Arestoration%20techniques%20trained%20directly%20on%20measurement%20data.%20The%20proposed%0Amethod%20is%20demonstrated%20through%20numerical%20experiments%20on%20image%20denoising%20and%0Adeblurring%3B%20its%20performance%20are%20comparable%20to%20supervised%20conformal%20prediction%0Amethods%20relying%20on%20ground%20truth%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19194v1&entry.124074799=Read"},
{"title": "The Sharpness Disparity Principle in Transformers for Accelerating\n  Language Model Pre-Training", "author": "Jinbo Wang and Mingze Wang and Zhanpeng Zhou and Junchi Yan and Weinan E and Lei Wu", "abstract": "  Transformers consist of diverse building blocks, such as embedding layers,\nnormalization layers, self-attention mechanisms, and point-wise feedforward\nnetworks. Thus, understanding the differences and interactions among these\nblocks is important. In this paper, we uncover a clear Sharpness Disparity\nacross these blocks, which emerges early in training and intriguingly persists\nthroughout the training process. Motivated by this finding, we propose\nBlockwise Learning Rate (LR), a strategy that tailors the LR to each block's\nsharpness, accelerating large language model (LLM) pre-training. By integrating\nBlockwise LR into AdamW, we consistently achieve lower terminal loss and nearly\n$2\\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration\nacross GPT-2 and LLaMA, with model sizes ranging from 0.12B to 1.1B and\ndatasets of OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into\nAdam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of\nAdam, achieving a combined $2\\times$ speedup and $2\\times$ memory saving. These\nresults underscore the potential of exploiting the sharpness disparity to\nimprove LLM training.\n", "link": "http://arxiv.org/abs/2502.19002v1", "date": "2025-02-26", "relevancy": 2.1287, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5799}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5237}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Sharpness%20Disparity%20Principle%20in%20Transformers%20for%20Accelerating%0A%20%20Language%20Model%20Pre-Training&body=Title%3A%20The%20Sharpness%20Disparity%20Principle%20in%20Transformers%20for%20Accelerating%0A%20%20Language%20Model%20Pre-Training%0AAuthor%3A%20Jinbo%20Wang%20and%20Mingze%20Wang%20and%20Zhanpeng%20Zhou%20and%20Junchi%20Yan%20and%20Weinan%20E%20and%20Lei%20Wu%0AAbstract%3A%20%20%20Transformers%20consist%20of%20diverse%20building%20blocks%2C%20such%20as%20embedding%20layers%2C%0Anormalization%20layers%2C%20self-attention%20mechanisms%2C%20and%20point-wise%20feedforward%0Anetworks.%20Thus%2C%20understanding%20the%20differences%20and%20interactions%20among%20these%0Ablocks%20is%20important.%20In%20this%20paper%2C%20we%20uncover%20a%20clear%20Sharpness%20Disparity%0Aacross%20these%20blocks%2C%20which%20emerges%20early%20in%20training%20and%20intriguingly%20persists%0Athroughout%20the%20training%20process.%20Motivated%20by%20this%20finding%2C%20we%20propose%0ABlockwise%20Learning%20Rate%20%28LR%29%2C%20a%20strategy%20that%20tailors%20the%20LR%20to%20each%20block%27s%0Asharpness%2C%20accelerating%20large%20language%20model%20%28LLM%29%20pre-training.%20By%20integrating%0ABlockwise%20LR%20into%20AdamW%2C%20we%20consistently%20achieve%20lower%20terminal%20loss%20and%20nearly%0A%242%5Ctimes%24%20speedup%20compared%20to%20vanilla%20AdamW.%20We%20demonstrate%20this%20acceleration%0Aacross%20GPT-2%20and%20LLaMA%2C%20with%20model%20sizes%20ranging%20from%200.12B%20to%201.1B%20and%0Adatasets%20of%20OpenWebText%20and%20MiniPile.%20Finally%2C%20we%20incorporate%20Blockwise%20LR%20into%0AAdam-mini%20%28Zhang%20et%20al.%2C%202024%29%2C%20a%20recently%20proposed%20memory-efficient%20variant%20of%0AAdam%2C%20achieving%20a%20combined%20%242%5Ctimes%24%20speedup%20and%20%242%5Ctimes%24%20memory%20saving.%20These%0Aresults%20underscore%20the%20potential%20of%20exploiting%20the%20sharpness%20disparity%20to%0Aimprove%20LLM%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Sharpness%2520Disparity%2520Principle%2520in%2520Transformers%2520for%2520Accelerating%250A%2520%2520Language%2520Model%2520Pre-Training%26entry.906535625%3DJinbo%2520Wang%2520and%2520Mingze%2520Wang%2520and%2520Zhanpeng%2520Zhou%2520and%2520Junchi%2520Yan%2520and%2520Weinan%2520E%2520and%2520Lei%2520Wu%26entry.1292438233%3D%2520%2520Transformers%2520consist%2520of%2520diverse%2520building%2520blocks%252C%2520such%2520as%2520embedding%2520layers%252C%250Anormalization%2520layers%252C%2520self-attention%2520mechanisms%252C%2520and%2520point-wise%2520feedforward%250Anetworks.%2520Thus%252C%2520understanding%2520the%2520differences%2520and%2520interactions%2520among%2520these%250Ablocks%2520is%2520important.%2520In%2520this%2520paper%252C%2520we%2520uncover%2520a%2520clear%2520Sharpness%2520Disparity%250Aacross%2520these%2520blocks%252C%2520which%2520emerges%2520early%2520in%2520training%2520and%2520intriguingly%2520persists%250Athroughout%2520the%2520training%2520process.%2520Motivated%2520by%2520this%2520finding%252C%2520we%2520propose%250ABlockwise%2520Learning%2520Rate%2520%2528LR%2529%252C%2520a%2520strategy%2520that%2520tailors%2520the%2520LR%2520to%2520each%2520block%2527s%250Asharpness%252C%2520accelerating%2520large%2520language%2520model%2520%2528LLM%2529%2520pre-training.%2520By%2520integrating%250ABlockwise%2520LR%2520into%2520AdamW%252C%2520we%2520consistently%2520achieve%2520lower%2520terminal%2520loss%2520and%2520nearly%250A%25242%255Ctimes%2524%2520speedup%2520compared%2520to%2520vanilla%2520AdamW.%2520We%2520demonstrate%2520this%2520acceleration%250Aacross%2520GPT-2%2520and%2520LLaMA%252C%2520with%2520model%2520sizes%2520ranging%2520from%25200.12B%2520to%25201.1B%2520and%250Adatasets%2520of%2520OpenWebText%2520and%2520MiniPile.%2520Finally%252C%2520we%2520incorporate%2520Blockwise%2520LR%2520into%250AAdam-mini%2520%2528Zhang%2520et%2520al.%252C%25202024%2529%252C%2520a%2520recently%2520proposed%2520memory-efficient%2520variant%2520of%250AAdam%252C%2520achieving%2520a%2520combined%2520%25242%255Ctimes%2524%2520speedup%2520and%2520%25242%255Ctimes%2524%2520memory%2520saving.%2520These%250Aresults%2520underscore%2520the%2520potential%2520of%2520exploiting%2520the%2520sharpness%2520disparity%2520to%250Aimprove%2520LLM%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Sharpness%20Disparity%20Principle%20in%20Transformers%20for%20Accelerating%0A%20%20Language%20Model%20Pre-Training&entry.906535625=Jinbo%20Wang%20and%20Mingze%20Wang%20and%20Zhanpeng%20Zhou%20and%20Junchi%20Yan%20and%20Weinan%20E%20and%20Lei%20Wu&entry.1292438233=%20%20Transformers%20consist%20of%20diverse%20building%20blocks%2C%20such%20as%20embedding%20layers%2C%0Anormalization%20layers%2C%20self-attention%20mechanisms%2C%20and%20point-wise%20feedforward%0Anetworks.%20Thus%2C%20understanding%20the%20differences%20and%20interactions%20among%20these%0Ablocks%20is%20important.%20In%20this%20paper%2C%20we%20uncover%20a%20clear%20Sharpness%20Disparity%0Aacross%20these%20blocks%2C%20which%20emerges%20early%20in%20training%20and%20intriguingly%20persists%0Athroughout%20the%20training%20process.%20Motivated%20by%20this%20finding%2C%20we%20propose%0ABlockwise%20Learning%20Rate%20%28LR%29%2C%20a%20strategy%20that%20tailors%20the%20LR%20to%20each%20block%27s%0Asharpness%2C%20accelerating%20large%20language%20model%20%28LLM%29%20pre-training.%20By%20integrating%0ABlockwise%20LR%20into%20AdamW%2C%20we%20consistently%20achieve%20lower%20terminal%20loss%20and%20nearly%0A%242%5Ctimes%24%20speedup%20compared%20to%20vanilla%20AdamW.%20We%20demonstrate%20this%20acceleration%0Aacross%20GPT-2%20and%20LLaMA%2C%20with%20model%20sizes%20ranging%20from%200.12B%20to%201.1B%20and%0Adatasets%20of%20OpenWebText%20and%20MiniPile.%20Finally%2C%20we%20incorporate%20Blockwise%20LR%20into%0AAdam-mini%20%28Zhang%20et%20al.%2C%202024%29%2C%20a%20recently%20proposed%20memory-efficient%20variant%20of%0AAdam%2C%20achieving%20a%20combined%20%242%5Ctimes%24%20speedup%20and%20%242%5Ctimes%24%20memory%20saving.%20These%0Aresults%20underscore%20the%20potential%20of%20exploiting%20the%20sharpness%20disparity%20to%0Aimprove%20LLM%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19002v1&entry.124074799=Read"},
{"title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation", "author": "Fredrik Carlsson and Fangyu Liu and Daniel Ward and Murathan Kurfali and Joakim Nivre", "abstract": "  This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.\n", "link": "http://arxiv.org/abs/2412.04318v2", "date": "2025-02-26", "relevancy": 2.1275, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5377}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.536}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Hyperfitting%20Phenomenon%3A%20Sharpening%20and%20Stabilizing%20LLMs%20for%0A%20%20Open-Ended%20Text%20Generation&body=Title%3A%20The%20Hyperfitting%20Phenomenon%3A%20Sharpening%20and%20Stabilizing%20LLMs%20for%0A%20%20Open-Ended%20Text%20Generation%0AAuthor%3A%20Fredrik%20Carlsson%20and%20Fangyu%20Liu%20and%20Daniel%20Ward%20and%20Murathan%20Kurfali%20and%20Joakim%20Nivre%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20counter-intuitive%20generalization%20results%20of%0Aoverfitting%20pre-trained%20large%20language%20models%20%28LLMs%29%20on%20very%20small%20datasets.%20In%0Athe%20setting%20of%20open-ended%20text%20generation%2C%20it%20is%20well-documented%20that%20LLMs%20tend%0Ato%20generate%20repetitive%20and%20dull%20sequences%2C%20a%20phenomenon%20that%20is%20especially%0Aapparent%20when%20generating%20using%20greedy%20decoding.%20This%20issue%20persists%20even%20with%0Astate-of-the-art%20LLMs%20containing%20billions%20of%20parameters%2C%20trained%20via%20next-token%0Aprediction%20on%20large%20datasets.%20We%20find%20that%20by%20further%20fine-tuning%20these%20models%0Ato%20achieve%20a%20near-zero%20training%20loss%20on%20a%20small%20set%20of%20samples%20--%20a%20process%20we%0Arefer%20to%20as%20hyperfitting%20--%20the%20long-sequence%20generative%20capabilities%20are%0Agreatly%20enhanced.%20Greedy%20decoding%20with%20these%20Hyperfitted%20models%20even%20outperform%0ATop-P%20sampling%20over%20long-sequences%2C%20both%20in%20terms%20of%20diversity%20and%20human%0Apreferences.%20This%20phenomenon%20extends%20to%20LLMs%20of%20various%20sizes%2C%20different%0Adomains%2C%20and%20even%20autoregressive%20image%20generation.%20We%20further%20find%20this%0Aphenomena%20to%20be%20distinctly%20different%20from%20that%20of%20Grokking%20and%20double%20descent.%0ASurprisingly%2C%20our%20experiments%20indicate%20that%20hyperfitted%20models%20rarely%20fall%20into%0Arepeating%20sequences%20they%20were%20trained%20on%2C%20and%20even%20explicitly%20blocking%20these%0Asequences%20results%20in%20high-quality%20output.%20All%20hyperfitted%20models%20produce%0Aextremely%20low-entropy%20predictions%2C%20often%20allocating%20nearly%20all%20probability%20to%20a%0Asingle%20token.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Hyperfitting%2520Phenomenon%253A%2520Sharpening%2520and%2520Stabilizing%2520LLMs%2520for%250A%2520%2520Open-Ended%2520Text%2520Generation%26entry.906535625%3DFredrik%2520Carlsson%2520and%2520Fangyu%2520Liu%2520and%2520Daniel%2520Ward%2520and%2520Murathan%2520Kurfali%2520and%2520Joakim%2520Nivre%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520counter-intuitive%2520generalization%2520results%2520of%250Aoverfitting%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520very%2520small%2520datasets.%2520In%250Athe%2520setting%2520of%2520open-ended%2520text%2520generation%252C%2520it%2520is%2520well-documented%2520that%2520LLMs%2520tend%250Ato%2520generate%2520repetitive%2520and%2520dull%2520sequences%252C%2520a%2520phenomenon%2520that%2520is%2520especially%250Aapparent%2520when%2520generating%2520using%2520greedy%2520decoding.%2520This%2520issue%2520persists%2520even%2520with%250Astate-of-the-art%2520LLMs%2520containing%2520billions%2520of%2520parameters%252C%2520trained%2520via%2520next-token%250Aprediction%2520on%2520large%2520datasets.%2520We%2520find%2520that%2520by%2520further%2520fine-tuning%2520these%2520models%250Ato%2520achieve%2520a%2520near-zero%2520training%2520loss%2520on%2520a%2520small%2520set%2520of%2520samples%2520--%2520a%2520process%2520we%250Arefer%2520to%2520as%2520hyperfitting%2520--%2520the%2520long-sequence%2520generative%2520capabilities%2520are%250Agreatly%2520enhanced.%2520Greedy%2520decoding%2520with%2520these%2520Hyperfitted%2520models%2520even%2520outperform%250ATop-P%2520sampling%2520over%2520long-sequences%252C%2520both%2520in%2520terms%2520of%2520diversity%2520and%2520human%250Apreferences.%2520This%2520phenomenon%2520extends%2520to%2520LLMs%2520of%2520various%2520sizes%252C%2520different%250Adomains%252C%2520and%2520even%2520autoregressive%2520image%2520generation.%2520We%2520further%2520find%2520this%250Aphenomena%2520to%2520be%2520distinctly%2520different%2520from%2520that%2520of%2520Grokking%2520and%2520double%2520descent.%250ASurprisingly%252C%2520our%2520experiments%2520indicate%2520that%2520hyperfitted%2520models%2520rarely%2520fall%2520into%250Arepeating%2520sequences%2520they%2520were%2520trained%2520on%252C%2520and%2520even%2520explicitly%2520blocking%2520these%250Asequences%2520results%2520in%2520high-quality%2520output.%2520All%2520hyperfitted%2520models%2520produce%250Aextremely%2520low-entropy%2520predictions%252C%2520often%2520allocating%2520nearly%2520all%2520probability%2520to%2520a%250Asingle%2520token.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Hyperfitting%20Phenomenon%3A%20Sharpening%20and%20Stabilizing%20LLMs%20for%0A%20%20Open-Ended%20Text%20Generation&entry.906535625=Fredrik%20Carlsson%20and%20Fangyu%20Liu%20and%20Daniel%20Ward%20and%20Murathan%20Kurfali%20and%20Joakim%20Nivre&entry.1292438233=%20%20This%20paper%20introduces%20the%20counter-intuitive%20generalization%20results%20of%0Aoverfitting%20pre-trained%20large%20language%20models%20%28LLMs%29%20on%20very%20small%20datasets.%20In%0Athe%20setting%20of%20open-ended%20text%20generation%2C%20it%20is%20well-documented%20that%20LLMs%20tend%0Ato%20generate%20repetitive%20and%20dull%20sequences%2C%20a%20phenomenon%20that%20is%20especially%0Aapparent%20when%20generating%20using%20greedy%20decoding.%20This%20issue%20persists%20even%20with%0Astate-of-the-art%20LLMs%20containing%20billions%20of%20parameters%2C%20trained%20via%20next-token%0Aprediction%20on%20large%20datasets.%20We%20find%20that%20by%20further%20fine-tuning%20these%20models%0Ato%20achieve%20a%20near-zero%20training%20loss%20on%20a%20small%20set%20of%20samples%20--%20a%20process%20we%0Arefer%20to%20as%20hyperfitting%20--%20the%20long-sequence%20generative%20capabilities%20are%0Agreatly%20enhanced.%20Greedy%20decoding%20with%20these%20Hyperfitted%20models%20even%20outperform%0ATop-P%20sampling%20over%20long-sequences%2C%20both%20in%20terms%20of%20diversity%20and%20human%0Apreferences.%20This%20phenomenon%20extends%20to%20LLMs%20of%20various%20sizes%2C%20different%0Adomains%2C%20and%20even%20autoregressive%20image%20generation.%20We%20further%20find%20this%0Aphenomena%20to%20be%20distinctly%20different%20from%20that%20of%20Grokking%20and%20double%20descent.%0ASurprisingly%2C%20our%20experiments%20indicate%20that%20hyperfitted%20models%20rarely%20fall%20into%0Arepeating%20sequences%20they%20were%20trained%20on%2C%20and%20even%20explicitly%20blocking%20these%0Asequences%20results%20in%20high-quality%20output.%20All%20hyperfitted%20models%20produce%0Aextremely%20low-entropy%20predictions%2C%20often%20allocating%20nearly%20all%20probability%20to%20a%0Asingle%20token.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04318v2&entry.124074799=Read"},
{"title": "Blending Optimal Control and Biologically Plausible Learning for\n  Noise-Robust Physical Neural Networks", "author": "Satoshi Sunada and Tomoaki Niiyama and Kazutaka Kanno and Rin Nogami and Andr\u00e9 R\u00f6hm and Takato Awano and Atsushi Uchida", "abstract": "  The rapidly increasing computational demands for artificial intelligence (AI)\nhave spurred the exploration of computing principles beyond conventional\ndigital computers. Physical neural networks (PNNs) offer efficient neuromorphic\ninformation processing by harnessing the innate computational power of physical\nprocesses; however, training their weight parameters is computationally\nexpensive. We propose a training approach for substantially reducing this\ntraining cost. Our training approach merges an optimal control method for\ncontinuous-time dynamical systems with a biologically plausible training\nmethod--direct feedback alignment. In addition to the reduction of training\ntime, this approach achieves robust processing even under measurement errors\nand noise without requiring detailed system information. The effectiveness was\nnumerically and experimentally verified in an optoelectronic delay system. Our\napproach significantly extends the range of physical systems practically usable\nas PNNs.\n", "link": "http://arxiv.org/abs/2502.19053v1", "date": "2025-02-26", "relevancy": 2.1263, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.532}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5313}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blending%20Optimal%20Control%20and%20Biologically%20Plausible%20Learning%20for%0A%20%20Noise-Robust%20Physical%20Neural%20Networks&body=Title%3A%20Blending%20Optimal%20Control%20and%20Biologically%20Plausible%20Learning%20for%0A%20%20Noise-Robust%20Physical%20Neural%20Networks%0AAuthor%3A%20Satoshi%20Sunada%20and%20Tomoaki%20Niiyama%20and%20Kazutaka%20Kanno%20and%20Rin%20Nogami%20and%20Andr%C3%A9%20R%C3%B6hm%20and%20Takato%20Awano%20and%20Atsushi%20Uchida%0AAbstract%3A%20%20%20The%20rapidly%20increasing%20computational%20demands%20for%20artificial%20intelligence%20%28AI%29%0Ahave%20spurred%20the%20exploration%20of%20computing%20principles%20beyond%20conventional%0Adigital%20computers.%20Physical%20neural%20networks%20%28PNNs%29%20offer%20efficient%20neuromorphic%0Ainformation%20processing%20by%20harnessing%20the%20innate%20computational%20power%20of%20physical%0Aprocesses%3B%20however%2C%20training%20their%20weight%20parameters%20is%20computationally%0Aexpensive.%20We%20propose%20a%20training%20approach%20for%20substantially%20reducing%20this%0Atraining%20cost.%20Our%20training%20approach%20merges%20an%20optimal%20control%20method%20for%0Acontinuous-time%20dynamical%20systems%20with%20a%20biologically%20plausible%20training%0Amethod--direct%20feedback%20alignment.%20In%20addition%20to%20the%20reduction%20of%20training%0Atime%2C%20this%20approach%20achieves%20robust%20processing%20even%20under%20measurement%20errors%0Aand%20noise%20without%20requiring%20detailed%20system%20information.%20The%20effectiveness%20was%0Anumerically%20and%20experimentally%20verified%20in%20an%20optoelectronic%20delay%20system.%20Our%0Aapproach%20significantly%20extends%20the%20range%20of%20physical%20systems%20practically%20usable%0Aas%20PNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlending%2520Optimal%2520Control%2520and%2520Biologically%2520Plausible%2520Learning%2520for%250A%2520%2520Noise-Robust%2520Physical%2520Neural%2520Networks%26entry.906535625%3DSatoshi%2520Sunada%2520and%2520Tomoaki%2520Niiyama%2520and%2520Kazutaka%2520Kanno%2520and%2520Rin%2520Nogami%2520and%2520Andr%25C3%25A9%2520R%25C3%25B6hm%2520and%2520Takato%2520Awano%2520and%2520Atsushi%2520Uchida%26entry.1292438233%3D%2520%2520The%2520rapidly%2520increasing%2520computational%2520demands%2520for%2520artificial%2520intelligence%2520%2528AI%2529%250Ahave%2520spurred%2520the%2520exploration%2520of%2520computing%2520principles%2520beyond%2520conventional%250Adigital%2520computers.%2520Physical%2520neural%2520networks%2520%2528PNNs%2529%2520offer%2520efficient%2520neuromorphic%250Ainformation%2520processing%2520by%2520harnessing%2520the%2520innate%2520computational%2520power%2520of%2520physical%250Aprocesses%253B%2520however%252C%2520training%2520their%2520weight%2520parameters%2520is%2520computationally%250Aexpensive.%2520We%2520propose%2520a%2520training%2520approach%2520for%2520substantially%2520reducing%2520this%250Atraining%2520cost.%2520Our%2520training%2520approach%2520merges%2520an%2520optimal%2520control%2520method%2520for%250Acontinuous-time%2520dynamical%2520systems%2520with%2520a%2520biologically%2520plausible%2520training%250Amethod--direct%2520feedback%2520alignment.%2520In%2520addition%2520to%2520the%2520reduction%2520of%2520training%250Atime%252C%2520this%2520approach%2520achieves%2520robust%2520processing%2520even%2520under%2520measurement%2520errors%250Aand%2520noise%2520without%2520requiring%2520detailed%2520system%2520information.%2520The%2520effectiveness%2520was%250Anumerically%2520and%2520experimentally%2520verified%2520in%2520an%2520optoelectronic%2520delay%2520system.%2520Our%250Aapproach%2520significantly%2520extends%2520the%2520range%2520of%2520physical%2520systems%2520practically%2520usable%250Aas%2520PNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blending%20Optimal%20Control%20and%20Biologically%20Plausible%20Learning%20for%0A%20%20Noise-Robust%20Physical%20Neural%20Networks&entry.906535625=Satoshi%20Sunada%20and%20Tomoaki%20Niiyama%20and%20Kazutaka%20Kanno%20and%20Rin%20Nogami%20and%20Andr%C3%A9%20R%C3%B6hm%20and%20Takato%20Awano%20and%20Atsushi%20Uchida&entry.1292438233=%20%20The%20rapidly%20increasing%20computational%20demands%20for%20artificial%20intelligence%20%28AI%29%0Ahave%20spurred%20the%20exploration%20of%20computing%20principles%20beyond%20conventional%0Adigital%20computers.%20Physical%20neural%20networks%20%28PNNs%29%20offer%20efficient%20neuromorphic%0Ainformation%20processing%20by%20harnessing%20the%20innate%20computational%20power%20of%20physical%0Aprocesses%3B%20however%2C%20training%20their%20weight%20parameters%20is%20computationally%0Aexpensive.%20We%20propose%20a%20training%20approach%20for%20substantially%20reducing%20this%0Atraining%20cost.%20Our%20training%20approach%20merges%20an%20optimal%20control%20method%20for%0Acontinuous-time%20dynamical%20systems%20with%20a%20biologically%20plausible%20training%0Amethod--direct%20feedback%20alignment.%20In%20addition%20to%20the%20reduction%20of%20training%0Atime%2C%20this%20approach%20achieves%20robust%20processing%20even%20under%20measurement%20errors%0Aand%20noise%20without%20requiring%20detailed%20system%20information.%20The%20effectiveness%20was%0Anumerically%20and%20experimentally%20verified%20in%20an%20optoelectronic%20delay%20system.%20Our%0Aapproach%20significantly%20extends%20the%20range%20of%20physical%20systems%20practically%20usable%0Aas%20PNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19053v1&entry.124074799=Read"},
{"title": "Learning Harmonized Representations for Speculative Sampling", "author": "Lefan Zhang and Xiaodan Wang and Yanhua Huang and Ruiwen Xu", "abstract": "  Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.\n", "link": "http://arxiv.org/abs/2408.15766v3", "date": "2025-02-26", "relevancy": 2.1191, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.571}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Harmonized%20Representations%20for%20Speculative%20Sampling&body=Title%3A%20Learning%20Harmonized%20Representations%20for%20Speculative%20Sampling%0AAuthor%3A%20Lefan%20Zhang%20and%20Xiaodan%20Wang%20and%20Yanhua%20Huang%20and%20Ruiwen%20Xu%0AAbstract%3A%20%20%20Speculative%20sampling%20is%20a%20promising%20approach%20to%20accelerate%20the%20decoding%20stage%0Afor%20Large%20Language%20Models%20%28LLMs%29.%20Recent%20advancements%20that%20leverage%20target%0ALLM%27s%20contextual%20information%2C%20such%20as%20hidden%20states%20and%20KV%20cache%2C%20have%20shown%0Asignificant%20practical%20improvements.%20However%2C%20these%20approaches%20suffer%20from%0Ainconsistent%20context%20between%20training%20and%20decoding.%20We%20also%20observe%20another%0Adiscrepancy%20between%20the%20training%20and%20decoding%20objectives%20in%20existing%0Aspeculative%20sampling%20methods.%20In%20this%20work%2C%20we%20propose%20a%20solution%20named%0AHArmonized%20Speculative%20Sampling%20%28HASS%29%20that%20learns%20harmonized%20representations%0Ato%20address%20these%20issues.%20HASS%20accelerates%20the%20decoding%20stage%20without%20adding%0Ainference%20overhead%20through%20harmonized%20objective%20distillation%20and%20harmonized%0Acontext%20alignment.%20Experiments%20on%20four%20LLaMA%20models%20demonstrate%20that%20HASS%0Aachieves%202.81x-4.05x%20wall-clock%20time%20speedup%20ratio%20averaging%20across%20three%0Adatasets%2C%20surpassing%20EAGLE-2%20by%208%25-20%25.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/HArmonizedSS/HASS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15766v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Harmonized%2520Representations%2520for%2520Speculative%2520Sampling%26entry.906535625%3DLefan%2520Zhang%2520and%2520Xiaodan%2520Wang%2520and%2520Yanhua%2520Huang%2520and%2520Ruiwen%2520Xu%26entry.1292438233%3D%2520%2520Speculative%2520sampling%2520is%2520a%2520promising%2520approach%2520to%2520accelerate%2520the%2520decoding%2520stage%250Afor%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Recent%2520advancements%2520that%2520leverage%2520target%250ALLM%2527s%2520contextual%2520information%252C%2520such%2520as%2520hidden%2520states%2520and%2520KV%2520cache%252C%2520have%2520shown%250Asignificant%2520practical%2520improvements.%2520However%252C%2520these%2520approaches%2520suffer%2520from%250Ainconsistent%2520context%2520between%2520training%2520and%2520decoding.%2520We%2520also%2520observe%2520another%250Adiscrepancy%2520between%2520the%2520training%2520and%2520decoding%2520objectives%2520in%2520existing%250Aspeculative%2520sampling%2520methods.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520solution%2520named%250AHArmonized%2520Speculative%2520Sampling%2520%2528HASS%2529%2520that%2520learns%2520harmonized%2520representations%250Ato%2520address%2520these%2520issues.%2520HASS%2520accelerates%2520the%2520decoding%2520stage%2520without%2520adding%250Ainference%2520overhead%2520through%2520harmonized%2520objective%2520distillation%2520and%2520harmonized%250Acontext%2520alignment.%2520Experiments%2520on%2520four%2520LLaMA%2520models%2520demonstrate%2520that%2520HASS%250Aachieves%25202.81x-4.05x%2520wall-clock%2520time%2520speedup%2520ratio%2520averaging%2520across%2520three%250Adatasets%252C%2520surpassing%2520EAGLE-2%2520by%25208%2525-20%2525.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/HArmonizedSS/HASS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15766v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Harmonized%20Representations%20for%20Speculative%20Sampling&entry.906535625=Lefan%20Zhang%20and%20Xiaodan%20Wang%20and%20Yanhua%20Huang%20and%20Ruiwen%20Xu&entry.1292438233=%20%20Speculative%20sampling%20is%20a%20promising%20approach%20to%20accelerate%20the%20decoding%20stage%0Afor%20Large%20Language%20Models%20%28LLMs%29.%20Recent%20advancements%20that%20leverage%20target%0ALLM%27s%20contextual%20information%2C%20such%20as%20hidden%20states%20and%20KV%20cache%2C%20have%20shown%0Asignificant%20practical%20improvements.%20However%2C%20these%20approaches%20suffer%20from%0Ainconsistent%20context%20between%20training%20and%20decoding.%20We%20also%20observe%20another%0Adiscrepancy%20between%20the%20training%20and%20decoding%20objectives%20in%20existing%0Aspeculative%20sampling%20methods.%20In%20this%20work%2C%20we%20propose%20a%20solution%20named%0AHArmonized%20Speculative%20Sampling%20%28HASS%29%20that%20learns%20harmonized%20representations%0Ato%20address%20these%20issues.%20HASS%20accelerates%20the%20decoding%20stage%20without%20adding%0Ainference%20overhead%20through%20harmonized%20objective%20distillation%20and%20harmonized%0Acontext%20alignment.%20Experiments%20on%20four%20LLaMA%20models%20demonstrate%20that%20HASS%0Aachieves%202.81x-4.05x%20wall-clock%20time%20speedup%20ratio%20averaging%20across%20three%0Adatasets%2C%20surpassing%20EAGLE-2%20by%208%25-20%25.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/HArmonizedSS/HASS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15766v3&entry.124074799=Read"},
{"title": "dCMF: Learning interpretable evolving patterns from temporal multiway\n  data", "author": "Christos Chatzis and Carla Schenker and J\u00e9r\u00e9my E. Cohen and Evrim Acar", "abstract": "  Multiway datasets are commonly analyzed using unsupervised matrix and tensor\nfactorization methods to reveal underlying patterns. Frequently, such datasets\ninclude timestamps and could correspond to, for example, health-related\nmeasurements of subjects collected over time. The temporal dimension is\ninherently different from the other dimensions, requiring methods that account\nfor its intrinsic properties. Linear Dynamical Systems (LDS) are specifically\ndesigned to capture sequential dependencies in the observed data. In this work,\nwe bridge the gap between tensor factorizations and dynamical modeling by\nexploring the relationship between LDS, Coupled Matrix Factorizations (CMF) and\nthe PARAFAC2 model. We propose a time-aware coupled factorization model called\nd(ynamical)CMF that constrains the temporal evolution of the latent factors to\nadhere to a specific LDS structure. Using synthetic datasets, we compare the\nperformance of dCMF with PARAFAC2 and t(emporal)PARAFAC2 which incorporates\ntemporal smoothness. Our results show that dCMF and PARAFAC2-based approaches\nperform similarly when capturing smoothly evolving patterns that adhere to the\nPARAFAC2 structure. However, dCMF outperforms alternatives when the patterns\nevolve smoothly but deviate from the PARAFAC2 structure. Furthermore, we\ndemonstrate that the proposed dCMF method enables to capture more complex\ndynamics when additional prior information about the temporal evolution is\nincorporated.\n", "link": "http://arxiv.org/abs/2502.19367v1", "date": "2025-02-26", "relevancy": 2.1183, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5502}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5329}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20dCMF%3A%20Learning%20interpretable%20evolving%20patterns%20from%20temporal%20multiway%0A%20%20data&body=Title%3A%20dCMF%3A%20Learning%20interpretable%20evolving%20patterns%20from%20temporal%20multiway%0A%20%20data%0AAuthor%3A%20Christos%20Chatzis%20and%20Carla%20Schenker%20and%20J%C3%A9r%C3%A9my%20E.%20Cohen%20and%20Evrim%20Acar%0AAbstract%3A%20%20%20Multiway%20datasets%20are%20commonly%20analyzed%20using%20unsupervised%20matrix%20and%20tensor%0Afactorization%20methods%20to%20reveal%20underlying%20patterns.%20Frequently%2C%20such%20datasets%0Ainclude%20timestamps%20and%20could%20correspond%20to%2C%20for%20example%2C%20health-related%0Ameasurements%20of%20subjects%20collected%20over%20time.%20The%20temporal%20dimension%20is%0Ainherently%20different%20from%20the%20other%20dimensions%2C%20requiring%20methods%20that%20account%0Afor%20its%20intrinsic%20properties.%20Linear%20Dynamical%20Systems%20%28LDS%29%20are%20specifically%0Adesigned%20to%20capture%20sequential%20dependencies%20in%20the%20observed%20data.%20In%20this%20work%2C%0Awe%20bridge%20the%20gap%20between%20tensor%20factorizations%20and%20dynamical%20modeling%20by%0Aexploring%20the%20relationship%20between%20LDS%2C%20Coupled%20Matrix%20Factorizations%20%28CMF%29%20and%0Athe%20PARAFAC2%20model.%20We%20propose%20a%20time-aware%20coupled%20factorization%20model%20called%0Ad%28ynamical%29CMF%20that%20constrains%20the%20temporal%20evolution%20of%20the%20latent%20factors%20to%0Aadhere%20to%20a%20specific%20LDS%20structure.%20Using%20synthetic%20datasets%2C%20we%20compare%20the%0Aperformance%20of%20dCMF%20with%20PARAFAC2%20and%20t%28emporal%29PARAFAC2%20which%20incorporates%0Atemporal%20smoothness.%20Our%20results%20show%20that%20dCMF%20and%20PARAFAC2-based%20approaches%0Aperform%20similarly%20when%20capturing%20smoothly%20evolving%20patterns%20that%20adhere%20to%20the%0APARAFAC2%20structure.%20However%2C%20dCMF%20outperforms%20alternatives%20when%20the%20patterns%0Aevolve%20smoothly%20but%20deviate%20from%20the%20PARAFAC2%20structure.%20Furthermore%2C%20we%0Ademonstrate%20that%20the%20proposed%20dCMF%20method%20enables%20to%20capture%20more%20complex%0Adynamics%20when%20additional%20prior%20information%20about%20the%20temporal%20evolution%20is%0Aincorporated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DdCMF%253A%2520Learning%2520interpretable%2520evolving%2520patterns%2520from%2520temporal%2520multiway%250A%2520%2520data%26entry.906535625%3DChristos%2520Chatzis%2520and%2520Carla%2520Schenker%2520and%2520J%25C3%25A9r%25C3%25A9my%2520E.%2520Cohen%2520and%2520Evrim%2520Acar%26entry.1292438233%3D%2520%2520Multiway%2520datasets%2520are%2520commonly%2520analyzed%2520using%2520unsupervised%2520matrix%2520and%2520tensor%250Afactorization%2520methods%2520to%2520reveal%2520underlying%2520patterns.%2520Frequently%252C%2520such%2520datasets%250Ainclude%2520timestamps%2520and%2520could%2520correspond%2520to%252C%2520for%2520example%252C%2520health-related%250Ameasurements%2520of%2520subjects%2520collected%2520over%2520time.%2520The%2520temporal%2520dimension%2520is%250Ainherently%2520different%2520from%2520the%2520other%2520dimensions%252C%2520requiring%2520methods%2520that%2520account%250Afor%2520its%2520intrinsic%2520properties.%2520Linear%2520Dynamical%2520Systems%2520%2528LDS%2529%2520are%2520specifically%250Adesigned%2520to%2520capture%2520sequential%2520dependencies%2520in%2520the%2520observed%2520data.%2520In%2520this%2520work%252C%250Awe%2520bridge%2520the%2520gap%2520between%2520tensor%2520factorizations%2520and%2520dynamical%2520modeling%2520by%250Aexploring%2520the%2520relationship%2520between%2520LDS%252C%2520Coupled%2520Matrix%2520Factorizations%2520%2528CMF%2529%2520and%250Athe%2520PARAFAC2%2520model.%2520We%2520propose%2520a%2520time-aware%2520coupled%2520factorization%2520model%2520called%250Ad%2528ynamical%2529CMF%2520that%2520constrains%2520the%2520temporal%2520evolution%2520of%2520the%2520latent%2520factors%2520to%250Aadhere%2520to%2520a%2520specific%2520LDS%2520structure.%2520Using%2520synthetic%2520datasets%252C%2520we%2520compare%2520the%250Aperformance%2520of%2520dCMF%2520with%2520PARAFAC2%2520and%2520t%2528emporal%2529PARAFAC2%2520which%2520incorporates%250Atemporal%2520smoothness.%2520Our%2520results%2520show%2520that%2520dCMF%2520and%2520PARAFAC2-based%2520approaches%250Aperform%2520similarly%2520when%2520capturing%2520smoothly%2520evolving%2520patterns%2520that%2520adhere%2520to%2520the%250APARAFAC2%2520structure.%2520However%252C%2520dCMF%2520outperforms%2520alternatives%2520when%2520the%2520patterns%250Aevolve%2520smoothly%2520but%2520deviate%2520from%2520the%2520PARAFAC2%2520structure.%2520Furthermore%252C%2520we%250Ademonstrate%2520that%2520the%2520proposed%2520dCMF%2520method%2520enables%2520to%2520capture%2520more%2520complex%250Adynamics%2520when%2520additional%2520prior%2520information%2520about%2520the%2520temporal%2520evolution%2520is%250Aincorporated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=dCMF%3A%20Learning%20interpretable%20evolving%20patterns%20from%20temporal%20multiway%0A%20%20data&entry.906535625=Christos%20Chatzis%20and%20Carla%20Schenker%20and%20J%C3%A9r%C3%A9my%20E.%20Cohen%20and%20Evrim%20Acar&entry.1292438233=%20%20Multiway%20datasets%20are%20commonly%20analyzed%20using%20unsupervised%20matrix%20and%20tensor%0Afactorization%20methods%20to%20reveal%20underlying%20patterns.%20Frequently%2C%20such%20datasets%0Ainclude%20timestamps%20and%20could%20correspond%20to%2C%20for%20example%2C%20health-related%0Ameasurements%20of%20subjects%20collected%20over%20time.%20The%20temporal%20dimension%20is%0Ainherently%20different%20from%20the%20other%20dimensions%2C%20requiring%20methods%20that%20account%0Afor%20its%20intrinsic%20properties.%20Linear%20Dynamical%20Systems%20%28LDS%29%20are%20specifically%0Adesigned%20to%20capture%20sequential%20dependencies%20in%20the%20observed%20data.%20In%20this%20work%2C%0Awe%20bridge%20the%20gap%20between%20tensor%20factorizations%20and%20dynamical%20modeling%20by%0Aexploring%20the%20relationship%20between%20LDS%2C%20Coupled%20Matrix%20Factorizations%20%28CMF%29%20and%0Athe%20PARAFAC2%20model.%20We%20propose%20a%20time-aware%20coupled%20factorization%20model%20called%0Ad%28ynamical%29CMF%20that%20constrains%20the%20temporal%20evolution%20of%20the%20latent%20factors%20to%0Aadhere%20to%20a%20specific%20LDS%20structure.%20Using%20synthetic%20datasets%2C%20we%20compare%20the%0Aperformance%20of%20dCMF%20with%20PARAFAC2%20and%20t%28emporal%29PARAFAC2%20which%20incorporates%0Atemporal%20smoothness.%20Our%20results%20show%20that%20dCMF%20and%20PARAFAC2-based%20approaches%0Aperform%20similarly%20when%20capturing%20smoothly%20evolving%20patterns%20that%20adhere%20to%20the%0APARAFAC2%20structure.%20However%2C%20dCMF%20outperforms%20alternatives%20when%20the%20patterns%0Aevolve%20smoothly%20but%20deviate%20from%20the%20PARAFAC2%20structure.%20Furthermore%2C%20we%0Ademonstrate%20that%20the%20proposed%20dCMF%20method%20enables%20to%20capture%20more%20complex%0Adynamics%20when%20additional%20prior%20information%20about%20the%20temporal%20evolution%20is%0Aincorporated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19367v1&entry.124074799=Read"},
{"title": "Verbalized Probabilistic Graphical Modeling", "author": "Hengguan Huang and Xing Shen and Songtao Wang and Lingfa Meng and Dianbo Liu and Hao Wang and Samir Bhatt", "abstract": "  Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.\n", "link": "http://arxiv.org/abs/2406.05516v2", "date": "2025-02-26", "relevancy": 2.1165, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5278}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verbalized%20Probabilistic%20Graphical%20Modeling&body=Title%3A%20Verbalized%20Probabilistic%20Graphical%20Modeling%0AAuthor%3A%20Hengguan%20Huang%20and%20Xing%20Shen%20and%20Songtao%20Wang%20and%20Lingfa%20Meng%20and%20Dianbo%20Liu%20and%20Hao%20Wang%20and%20Samir%20Bhatt%0AAbstract%3A%20%20%20Human%20cognition%20excels%20at%20transcending%20sensory%20input%20and%20forming%20latent%0Arepresentations%20that%20structure%20our%20understanding%20of%20the%20world.%20Although%20Large%0ALanguage%20Models%20%28LLMs%29%20can%20produce%20chain-of-thought%20reasoning%2C%20they%20lack%20a%0Aprincipled%20framework%20to%20capture%20latent%20structures%20and%20model%20uncertainty%2C%0Aespecially%20in%20compositional%20reasoning%20tasks.%20We%20propose%20Verbalized%0AProbabilistic%20Graphical%20Modeling%20%28vPGM%29%2C%20a%20Bayesian%20prompting%20framework%20that%0Aguides%20LLMs%20to%20simulate%20key%20principles%20of%20Probabilistic%20Graphical%20Models%20%28PGMs%29%0Ain%20natural%20language.%20Unlike%20many%20traditional%20probabilistic%20methods%20requiring%0Asubstantial%20domain%20expertise%20or%20specialized%20training%2C%20vPGM%20bypasses%0Aexpert-driven%20model%20design%2C%20making%20it%20well-suited%20for%20scenarios%20with%20limited%0Aassumptions%20or%20scarce%20data.%20We%20evaluated%20our%20model%20on%20several%20compositional%0Areasoning%20tasks%2C%20both%20close-ended%20and%20open-ended.%20Our%20results%20indicate%20that%20the%0Amodel%20effectively%20enhances%20confidence%20calibration%20and%20text%20generation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerbalized%2520Probabilistic%2520Graphical%2520Modeling%26entry.906535625%3DHengguan%2520Huang%2520and%2520Xing%2520Shen%2520and%2520Songtao%2520Wang%2520and%2520Lingfa%2520Meng%2520and%2520Dianbo%2520Liu%2520and%2520Hao%2520Wang%2520and%2520Samir%2520Bhatt%26entry.1292438233%3D%2520%2520Human%2520cognition%2520excels%2520at%2520transcending%2520sensory%2520input%2520and%2520forming%2520latent%250Arepresentations%2520that%2520structure%2520our%2520understanding%2520of%2520the%2520world.%2520Although%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520can%2520produce%2520chain-of-thought%2520reasoning%252C%2520they%2520lack%2520a%250Aprincipled%2520framework%2520to%2520capture%2520latent%2520structures%2520and%2520model%2520uncertainty%252C%250Aespecially%2520in%2520compositional%2520reasoning%2520tasks.%2520We%2520propose%2520Verbalized%250AProbabilistic%2520Graphical%2520Modeling%2520%2528vPGM%2529%252C%2520a%2520Bayesian%2520prompting%2520framework%2520that%250Aguides%2520LLMs%2520to%2520simulate%2520key%2520principles%2520of%2520Probabilistic%2520Graphical%2520Models%2520%2528PGMs%2529%250Ain%2520natural%2520language.%2520Unlike%2520many%2520traditional%2520probabilistic%2520methods%2520requiring%250Asubstantial%2520domain%2520expertise%2520or%2520specialized%2520training%252C%2520vPGM%2520bypasses%250Aexpert-driven%2520model%2520design%252C%2520making%2520it%2520well-suited%2520for%2520scenarios%2520with%2520limited%250Aassumptions%2520or%2520scarce%2520data.%2520We%2520evaluated%2520our%2520model%2520on%2520several%2520compositional%250Areasoning%2520tasks%252C%2520both%2520close-ended%2520and%2520open-ended.%2520Our%2520results%2520indicate%2520that%2520the%250Amodel%2520effectively%2520enhances%2520confidence%2520calibration%2520and%2520text%2520generation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verbalized%20Probabilistic%20Graphical%20Modeling&entry.906535625=Hengguan%20Huang%20and%20Xing%20Shen%20and%20Songtao%20Wang%20and%20Lingfa%20Meng%20and%20Dianbo%20Liu%20and%20Hao%20Wang%20and%20Samir%20Bhatt&entry.1292438233=%20%20Human%20cognition%20excels%20at%20transcending%20sensory%20input%20and%20forming%20latent%0Arepresentations%20that%20structure%20our%20understanding%20of%20the%20world.%20Although%20Large%0ALanguage%20Models%20%28LLMs%29%20can%20produce%20chain-of-thought%20reasoning%2C%20they%20lack%20a%0Aprincipled%20framework%20to%20capture%20latent%20structures%20and%20model%20uncertainty%2C%0Aespecially%20in%20compositional%20reasoning%20tasks.%20We%20propose%20Verbalized%0AProbabilistic%20Graphical%20Modeling%20%28vPGM%29%2C%20a%20Bayesian%20prompting%20framework%20that%0Aguides%20LLMs%20to%20simulate%20key%20principles%20of%20Probabilistic%20Graphical%20Models%20%28PGMs%29%0Ain%20natural%20language.%20Unlike%20many%20traditional%20probabilistic%20methods%20requiring%0Asubstantial%20domain%20expertise%20or%20specialized%20training%2C%20vPGM%20bypasses%0Aexpert-driven%20model%20design%2C%20making%20it%20well-suited%20for%20scenarios%20with%20limited%0Aassumptions%20or%20scarce%20data.%20We%20evaluated%20our%20model%20on%20several%20compositional%0Areasoning%20tasks%2C%20both%20close-ended%20and%20open-ended.%20Our%20results%20indicate%20that%20the%0Amodel%20effectively%20enhances%20confidence%20calibration%20and%20text%20generation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05516v2&entry.124074799=Read"},
{"title": "Physics-informed Variational Autoencoders for Improved Robustness to\n  Environmental Factors of Variation", "author": "Romain Thoreau and Laurent Risser and V\u00e9ronique Achard and B\u00e9atrice Berthelot and Xavier Briottet", "abstract": "  The combination of machine learning models with physical models is a recent\nresearch path to learn robust data representations. In this paper, we introduce\np$^3$VAE, a variational autoencoder that integrates prior physical knowledge\nabout the latent factors of variation that are related to the data acquisition\nconditions. p$^3$VAE combines standard neural network layers with non-trainable\nphysics layers in order to partially ground the latent space to physical\nvariables. We introduce a semi-supervised learning algorithm that strikes a\nbalance between the machine learning part and the physics part. Experiments on\nsimulated and real data sets demonstrate the benefits of our framework against\ncompeting physics-informed and conventional machine learning models, in terms\nof extrapolation capabilities and interpretability. In particular, we show that\np$^3$VAE naturally has interesting disentanglement capabilities. Our code and\ndata have been made publicly available at\nhttps://github.com/Romain3Ch216/p3VAE.\n", "link": "http://arxiv.org/abs/2210.10418v5", "date": "2025-02-26", "relevancy": 2.1138, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5375}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-informed%20Variational%20Autoencoders%20for%20Improved%20Robustness%20to%0A%20%20Environmental%20Factors%20of%20Variation&body=Title%3A%20Physics-informed%20Variational%20Autoencoders%20for%20Improved%20Robustness%20to%0A%20%20Environmental%20Factors%20of%20Variation%0AAuthor%3A%20Romain%20Thoreau%20and%20Laurent%20Risser%20and%20V%C3%A9ronique%20Achard%20and%20B%C3%A9atrice%20Berthelot%20and%20Xavier%20Briottet%0AAbstract%3A%20%20%20The%20combination%20of%20machine%20learning%20models%20with%20physical%20models%20is%20a%20recent%0Aresearch%20path%20to%20learn%20robust%20data%20representations.%20In%20this%20paper%2C%20we%20introduce%0Ap%24%5E3%24VAE%2C%20a%20variational%20autoencoder%20that%20integrates%20prior%20physical%20knowledge%0Aabout%20the%20latent%20factors%20of%20variation%20that%20are%20related%20to%20the%20data%20acquisition%0Aconditions.%20p%24%5E3%24VAE%20combines%20standard%20neural%20network%20layers%20with%20non-trainable%0Aphysics%20layers%20in%20order%20to%20partially%20ground%20the%20latent%20space%20to%20physical%0Avariables.%20We%20introduce%20a%20semi-supervised%20learning%20algorithm%20that%20strikes%20a%0Abalance%20between%20the%20machine%20learning%20part%20and%20the%20physics%20part.%20Experiments%20on%0Asimulated%20and%20real%20data%20sets%20demonstrate%20the%20benefits%20of%20our%20framework%20against%0Acompeting%20physics-informed%20and%20conventional%20machine%20learning%20models%2C%20in%20terms%0Aof%20extrapolation%20capabilities%20and%20interpretability.%20In%20particular%2C%20we%20show%20that%0Ap%24%5E3%24VAE%20naturally%20has%20interesting%20disentanglement%20capabilities.%20Our%20code%20and%0Adata%20have%20been%20made%20publicly%20available%20at%0Ahttps%3A//github.com/Romain3Ch216/p3VAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.10418v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-informed%2520Variational%2520Autoencoders%2520for%2520Improved%2520Robustness%2520to%250A%2520%2520Environmental%2520Factors%2520of%2520Variation%26entry.906535625%3DRomain%2520Thoreau%2520and%2520Laurent%2520Risser%2520and%2520V%25C3%25A9ronique%2520Achard%2520and%2520B%25C3%25A9atrice%2520Berthelot%2520and%2520Xavier%2520Briottet%26entry.1292438233%3D%2520%2520The%2520combination%2520of%2520machine%2520learning%2520models%2520with%2520physical%2520models%2520is%2520a%2520recent%250Aresearch%2520path%2520to%2520learn%2520robust%2520data%2520representations.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Ap%2524%255E3%2524VAE%252C%2520a%2520variational%2520autoencoder%2520that%2520integrates%2520prior%2520physical%2520knowledge%250Aabout%2520the%2520latent%2520factors%2520of%2520variation%2520that%2520are%2520related%2520to%2520the%2520data%2520acquisition%250Aconditions.%2520p%2524%255E3%2524VAE%2520combines%2520standard%2520neural%2520network%2520layers%2520with%2520non-trainable%250Aphysics%2520layers%2520in%2520order%2520to%2520partially%2520ground%2520the%2520latent%2520space%2520to%2520physical%250Avariables.%2520We%2520introduce%2520a%2520semi-supervised%2520learning%2520algorithm%2520that%2520strikes%2520a%250Abalance%2520between%2520the%2520machine%2520learning%2520part%2520and%2520the%2520physics%2520part.%2520Experiments%2520on%250Asimulated%2520and%2520real%2520data%2520sets%2520demonstrate%2520the%2520benefits%2520of%2520our%2520framework%2520against%250Acompeting%2520physics-informed%2520and%2520conventional%2520machine%2520learning%2520models%252C%2520in%2520terms%250Aof%2520extrapolation%2520capabilities%2520and%2520interpretability.%2520In%2520particular%252C%2520we%2520show%2520that%250Ap%2524%255E3%2524VAE%2520naturally%2520has%2520interesting%2520disentanglement%2520capabilities.%2520Our%2520code%2520and%250Adata%2520have%2520been%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Romain3Ch216/p3VAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.10418v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-informed%20Variational%20Autoencoders%20for%20Improved%20Robustness%20to%0A%20%20Environmental%20Factors%20of%20Variation&entry.906535625=Romain%20Thoreau%20and%20Laurent%20Risser%20and%20V%C3%A9ronique%20Achard%20and%20B%C3%A9atrice%20Berthelot%20and%20Xavier%20Briottet&entry.1292438233=%20%20The%20combination%20of%20machine%20learning%20models%20with%20physical%20models%20is%20a%20recent%0Aresearch%20path%20to%20learn%20robust%20data%20representations.%20In%20this%20paper%2C%20we%20introduce%0Ap%24%5E3%24VAE%2C%20a%20variational%20autoencoder%20that%20integrates%20prior%20physical%20knowledge%0Aabout%20the%20latent%20factors%20of%20variation%20that%20are%20related%20to%20the%20data%20acquisition%0Aconditions.%20p%24%5E3%24VAE%20combines%20standard%20neural%20network%20layers%20with%20non-trainable%0Aphysics%20layers%20in%20order%20to%20partially%20ground%20the%20latent%20space%20to%20physical%0Avariables.%20We%20introduce%20a%20semi-supervised%20learning%20algorithm%20that%20strikes%20a%0Abalance%20between%20the%20machine%20learning%20part%20and%20the%20physics%20part.%20Experiments%20on%0Asimulated%20and%20real%20data%20sets%20demonstrate%20the%20benefits%20of%20our%20framework%20against%0Acompeting%20physics-informed%20and%20conventional%20machine%20learning%20models%2C%20in%20terms%0Aof%20extrapolation%20capabilities%20and%20interpretability.%20In%20particular%2C%20we%20show%20that%0Ap%24%5E3%24VAE%20naturally%20has%20interesting%20disentanglement%20capabilities.%20Our%20code%20and%0Adata%20have%20been%20made%20publicly%20available%20at%0Ahttps%3A//github.com/Romain3Ch216/p3VAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.10418v5&entry.124074799=Read"},
{"title": "AI-Powered Bayesian Inference", "author": "Veronika Ro\u010dkov\u00e1 and Sean O'Hagan", "abstract": "  The advent of Generative Artificial Intelligence (GAI) has heralded an\ninflection point that changed how society thinks about knowledge acquisition.\nWhile GAI cannot be fully trusted for decision-making, it may still provide\nvaluable information that can be integrated into a decision pipeline. Rather\nthan seeing the lack of certitude and inherent randomness of GAI as a problem,\nwe view it as an opportunity. Indeed, variable answers to given prompts can be\nleveraged to construct a prior distribution which reflects assuredness of AI\npredictions. This prior distribution may be combined with tailored datasets for\na fully Bayesian analysis with an AI-driven prior. In this paper, we explore\nsuch a possibility within a non-parametric Bayesian framework. The basic idea\nconsists of assigning a Dirichlet process prior distribution on the\ndata-generating distribution with AI generative model as its baseline.\nHyper-parameters of the prior can be tuned out-of-sample to assess the\ninformativeness of the AI prior. Posterior simulation is achieved by computing\na suitably randomized functional on an augmented data that consists of observed\n(labeled) data as well as fake data whose labels have been imputed using AI.\nThis strategy can be parallelized and rapidly produces iid samples from the\nposterior by optimization as opposed to sampling from conditionals. Our method\nenables (predictive) inference and uncertainty quantification leveraging AI\npredictions in a coherent probabilistic manner.\n", "link": "http://arxiv.org/abs/2502.19231v1", "date": "2025-02-26", "relevancy": 2.1084, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5584}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.548}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Powered%20Bayesian%20Inference&body=Title%3A%20AI-Powered%20Bayesian%20Inference%0AAuthor%3A%20Veronika%20Ro%C4%8Dkov%C3%A1%20and%20Sean%20O%27Hagan%0AAbstract%3A%20%20%20The%20advent%20of%20Generative%20Artificial%20Intelligence%20%28GAI%29%20has%20heralded%20an%0Ainflection%20point%20that%20changed%20how%20society%20thinks%20about%20knowledge%20acquisition.%0AWhile%20GAI%20cannot%20be%20fully%20trusted%20for%20decision-making%2C%20it%20may%20still%20provide%0Avaluable%20information%20that%20can%20be%20integrated%20into%20a%20decision%20pipeline.%20Rather%0Athan%20seeing%20the%20lack%20of%20certitude%20and%20inherent%20randomness%20of%20GAI%20as%20a%20problem%2C%0Awe%20view%20it%20as%20an%20opportunity.%20Indeed%2C%20variable%20answers%20to%20given%20prompts%20can%20be%0Aleveraged%20to%20construct%20a%20prior%20distribution%20which%20reflects%20assuredness%20of%20AI%0Apredictions.%20This%20prior%20distribution%20may%20be%20combined%20with%20tailored%20datasets%20for%0Aa%20fully%20Bayesian%20analysis%20with%20an%20AI-driven%20prior.%20In%20this%20paper%2C%20we%20explore%0Asuch%20a%20possibility%20within%20a%20non-parametric%20Bayesian%20framework.%20The%20basic%20idea%0Aconsists%20of%20assigning%20a%20Dirichlet%20process%20prior%20distribution%20on%20the%0Adata-generating%20distribution%20with%20AI%20generative%20model%20as%20its%20baseline.%0AHyper-parameters%20of%20the%20prior%20can%20be%20tuned%20out-of-sample%20to%20assess%20the%0Ainformativeness%20of%20the%20AI%20prior.%20Posterior%20simulation%20is%20achieved%20by%20computing%0Aa%20suitably%20randomized%20functional%20on%20an%20augmented%20data%20that%20consists%20of%20observed%0A%28labeled%29%20data%20as%20well%20as%20fake%20data%20whose%20labels%20have%20been%20imputed%20using%20AI.%0AThis%20strategy%20can%20be%20parallelized%20and%20rapidly%20produces%20iid%20samples%20from%20the%0Aposterior%20by%20optimization%20as%20opposed%20to%20sampling%20from%20conditionals.%20Our%20method%0Aenables%20%28predictive%29%20inference%20and%20uncertainty%20quantification%20leveraging%20AI%0Apredictions%20in%20a%20coherent%20probabilistic%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Powered%2520Bayesian%2520Inference%26entry.906535625%3DVeronika%2520Ro%25C4%258Dkov%25C3%25A1%2520and%2520Sean%2520O%2527Hagan%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Generative%2520Artificial%2520Intelligence%2520%2528GAI%2529%2520has%2520heralded%2520an%250Ainflection%2520point%2520that%2520changed%2520how%2520society%2520thinks%2520about%2520knowledge%2520acquisition.%250AWhile%2520GAI%2520cannot%2520be%2520fully%2520trusted%2520for%2520decision-making%252C%2520it%2520may%2520still%2520provide%250Avaluable%2520information%2520that%2520can%2520be%2520integrated%2520into%2520a%2520decision%2520pipeline.%2520Rather%250Athan%2520seeing%2520the%2520lack%2520of%2520certitude%2520and%2520inherent%2520randomness%2520of%2520GAI%2520as%2520a%2520problem%252C%250Awe%2520view%2520it%2520as%2520an%2520opportunity.%2520Indeed%252C%2520variable%2520answers%2520to%2520given%2520prompts%2520can%2520be%250Aleveraged%2520to%2520construct%2520a%2520prior%2520distribution%2520which%2520reflects%2520assuredness%2520of%2520AI%250Apredictions.%2520This%2520prior%2520distribution%2520may%2520be%2520combined%2520with%2520tailored%2520datasets%2520for%250Aa%2520fully%2520Bayesian%2520analysis%2520with%2520an%2520AI-driven%2520prior.%2520In%2520this%2520paper%252C%2520we%2520explore%250Asuch%2520a%2520possibility%2520within%2520a%2520non-parametric%2520Bayesian%2520framework.%2520The%2520basic%2520idea%250Aconsists%2520of%2520assigning%2520a%2520Dirichlet%2520process%2520prior%2520distribution%2520on%2520the%250Adata-generating%2520distribution%2520with%2520AI%2520generative%2520model%2520as%2520its%2520baseline.%250AHyper-parameters%2520of%2520the%2520prior%2520can%2520be%2520tuned%2520out-of-sample%2520to%2520assess%2520the%250Ainformativeness%2520of%2520the%2520AI%2520prior.%2520Posterior%2520simulation%2520is%2520achieved%2520by%2520computing%250Aa%2520suitably%2520randomized%2520functional%2520on%2520an%2520augmented%2520data%2520that%2520consists%2520of%2520observed%250A%2528labeled%2529%2520data%2520as%2520well%2520as%2520fake%2520data%2520whose%2520labels%2520have%2520been%2520imputed%2520using%2520AI.%250AThis%2520strategy%2520can%2520be%2520parallelized%2520and%2520rapidly%2520produces%2520iid%2520samples%2520from%2520the%250Aposterior%2520by%2520optimization%2520as%2520opposed%2520to%2520sampling%2520from%2520conditionals.%2520Our%2520method%250Aenables%2520%2528predictive%2529%2520inference%2520and%2520uncertainty%2520quantification%2520leveraging%2520AI%250Apredictions%2520in%2520a%2520coherent%2520probabilistic%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Powered%20Bayesian%20Inference&entry.906535625=Veronika%20Ro%C4%8Dkov%C3%A1%20and%20Sean%20O%27Hagan&entry.1292438233=%20%20The%20advent%20of%20Generative%20Artificial%20Intelligence%20%28GAI%29%20has%20heralded%20an%0Ainflection%20point%20that%20changed%20how%20society%20thinks%20about%20knowledge%20acquisition.%0AWhile%20GAI%20cannot%20be%20fully%20trusted%20for%20decision-making%2C%20it%20may%20still%20provide%0Avaluable%20information%20that%20can%20be%20integrated%20into%20a%20decision%20pipeline.%20Rather%0Athan%20seeing%20the%20lack%20of%20certitude%20and%20inherent%20randomness%20of%20GAI%20as%20a%20problem%2C%0Awe%20view%20it%20as%20an%20opportunity.%20Indeed%2C%20variable%20answers%20to%20given%20prompts%20can%20be%0Aleveraged%20to%20construct%20a%20prior%20distribution%20which%20reflects%20assuredness%20of%20AI%0Apredictions.%20This%20prior%20distribution%20may%20be%20combined%20with%20tailored%20datasets%20for%0Aa%20fully%20Bayesian%20analysis%20with%20an%20AI-driven%20prior.%20In%20this%20paper%2C%20we%20explore%0Asuch%20a%20possibility%20within%20a%20non-parametric%20Bayesian%20framework.%20The%20basic%20idea%0Aconsists%20of%20assigning%20a%20Dirichlet%20process%20prior%20distribution%20on%20the%0Adata-generating%20distribution%20with%20AI%20generative%20model%20as%20its%20baseline.%0AHyper-parameters%20of%20the%20prior%20can%20be%20tuned%20out-of-sample%20to%20assess%20the%0Ainformativeness%20of%20the%20AI%20prior.%20Posterior%20simulation%20is%20achieved%20by%20computing%0Aa%20suitably%20randomized%20functional%20on%20an%20augmented%20data%20that%20consists%20of%20observed%0A%28labeled%29%20data%20as%20well%20as%20fake%20data%20whose%20labels%20have%20been%20imputed%20using%20AI.%0AThis%20strategy%20can%20be%20parallelized%20and%20rapidly%20produces%20iid%20samples%20from%20the%0Aposterior%20by%20optimization%20as%20opposed%20to%20sampling%20from%20conditionals.%20Our%20method%0Aenables%20%28predictive%29%20inference%20and%20uncertainty%20quantification%20leveraging%20AI%0Apredictions%20in%20a%20coherent%20probabilistic%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19231v1&entry.124074799=Read"},
{"title": "PolypFlow: Reinforcing Polyp Segmentation with Flow-Driven Dynamics", "author": "Pu Wang and Huaizhi Ma and Zhihua Zhang and Zhuoran Zheng", "abstract": "  Accurate polyp segmentation remains challenging due to irregular lesion\nmorphologies, ambiguous boundaries, and heterogeneous imaging conditions. While\nU-Net variants excel at local feature fusion, they often lack explicit\nmechanisms to model the dynamic evolution of segmentation confidence under\nuncertainty. Inspired by the interpretable nature of flow-based models, we\npresent \\textbf{PolypFLow}, a flow-matching enhanced architecture that injects\nphysics-inspired optimization dynamics into segmentation refinement. Unlike\nconventional cascaded networks, our framework solves an ordinary differential\nequation (ODE) to progressively align coarse initial predictions with ground\ntruth masks through learned velocity fields. This trajectory-based refinement\noffers two key advantages: 1) Interpretable Optimization: Intermediate flow\nsteps visualize how the model corrects under-segmented regions and sharpens\nboundaries at each ODE-solver iteration, demystifying the ``black-box\"\nrefinement process; 2) Boundary-Aware Robustness: The flow dynamics explicitly\nmodel gradient directions along polyp edges, enhancing resilience to\nlow-contrast regions and motion artifacts. Numerous experimental results show\nthat PolypFLow achieves a state-of-the-art while maintaining consistent\nperformance in different lighting scenarios.\n", "link": "http://arxiv.org/abs/2502.19037v1", "date": "2025-02-26", "relevancy": 2.1056, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5756}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5275}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolypFlow%3A%20Reinforcing%20Polyp%20Segmentation%20with%20Flow-Driven%20Dynamics&body=Title%3A%20PolypFlow%3A%20Reinforcing%20Polyp%20Segmentation%20with%20Flow-Driven%20Dynamics%0AAuthor%3A%20Pu%20Wang%20and%20Huaizhi%20Ma%20and%20Zhihua%20Zhang%20and%20Zhuoran%20Zheng%0AAbstract%3A%20%20%20Accurate%20polyp%20segmentation%20remains%20challenging%20due%20to%20irregular%20lesion%0Amorphologies%2C%20ambiguous%20boundaries%2C%20and%20heterogeneous%20imaging%20conditions.%20While%0AU-Net%20variants%20excel%20at%20local%20feature%20fusion%2C%20they%20often%20lack%20explicit%0Amechanisms%20to%20model%20the%20dynamic%20evolution%20of%20segmentation%20confidence%20under%0Auncertainty.%20Inspired%20by%20the%20interpretable%20nature%20of%20flow-based%20models%2C%20we%0Apresent%20%5Ctextbf%7BPolypFLow%7D%2C%20a%20flow-matching%20enhanced%20architecture%20that%20injects%0Aphysics-inspired%20optimization%20dynamics%20into%20segmentation%20refinement.%20Unlike%0Aconventional%20cascaded%20networks%2C%20our%20framework%20solves%20an%20ordinary%20differential%0Aequation%20%28ODE%29%20to%20progressively%20align%20coarse%20initial%20predictions%20with%20ground%0Atruth%20masks%20through%20learned%20velocity%20fields.%20This%20trajectory-based%20refinement%0Aoffers%20two%20key%20advantages%3A%201%29%20Interpretable%20Optimization%3A%20Intermediate%20flow%0Asteps%20visualize%20how%20the%20model%20corrects%20under-segmented%20regions%20and%20sharpens%0Aboundaries%20at%20each%20ODE-solver%20iteration%2C%20demystifying%20the%20%60%60black-box%22%0Arefinement%20process%3B%202%29%20Boundary-Aware%20Robustness%3A%20The%20flow%20dynamics%20explicitly%0Amodel%20gradient%20directions%20along%20polyp%20edges%2C%20enhancing%20resilience%20to%0Alow-contrast%20regions%20and%20motion%20artifacts.%20Numerous%20experimental%20results%20show%0Athat%20PolypFLow%20achieves%20a%20state-of-the-art%20while%20maintaining%20consistent%0Aperformance%20in%20different%20lighting%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolypFlow%253A%2520Reinforcing%2520Polyp%2520Segmentation%2520with%2520Flow-Driven%2520Dynamics%26entry.906535625%3DPu%2520Wang%2520and%2520Huaizhi%2520Ma%2520and%2520Zhihua%2520Zhang%2520and%2520Zhuoran%2520Zheng%26entry.1292438233%3D%2520%2520Accurate%2520polyp%2520segmentation%2520remains%2520challenging%2520due%2520to%2520irregular%2520lesion%250Amorphologies%252C%2520ambiguous%2520boundaries%252C%2520and%2520heterogeneous%2520imaging%2520conditions.%2520While%250AU-Net%2520variants%2520excel%2520at%2520local%2520feature%2520fusion%252C%2520they%2520often%2520lack%2520explicit%250Amechanisms%2520to%2520model%2520the%2520dynamic%2520evolution%2520of%2520segmentation%2520confidence%2520under%250Auncertainty.%2520Inspired%2520by%2520the%2520interpretable%2520nature%2520of%2520flow-based%2520models%252C%2520we%250Apresent%2520%255Ctextbf%257BPolypFLow%257D%252C%2520a%2520flow-matching%2520enhanced%2520architecture%2520that%2520injects%250Aphysics-inspired%2520optimization%2520dynamics%2520into%2520segmentation%2520refinement.%2520Unlike%250Aconventional%2520cascaded%2520networks%252C%2520our%2520framework%2520solves%2520an%2520ordinary%2520differential%250Aequation%2520%2528ODE%2529%2520to%2520progressively%2520align%2520coarse%2520initial%2520predictions%2520with%2520ground%250Atruth%2520masks%2520through%2520learned%2520velocity%2520fields.%2520This%2520trajectory-based%2520refinement%250Aoffers%2520two%2520key%2520advantages%253A%25201%2529%2520Interpretable%2520Optimization%253A%2520Intermediate%2520flow%250Asteps%2520visualize%2520how%2520the%2520model%2520corrects%2520under-segmented%2520regions%2520and%2520sharpens%250Aboundaries%2520at%2520each%2520ODE-solver%2520iteration%252C%2520demystifying%2520the%2520%2560%2560black-box%2522%250Arefinement%2520process%253B%25202%2529%2520Boundary-Aware%2520Robustness%253A%2520The%2520flow%2520dynamics%2520explicitly%250Amodel%2520gradient%2520directions%2520along%2520polyp%2520edges%252C%2520enhancing%2520resilience%2520to%250Alow-contrast%2520regions%2520and%2520motion%2520artifacts.%2520Numerous%2520experimental%2520results%2520show%250Athat%2520PolypFLow%2520achieves%2520a%2520state-of-the-art%2520while%2520maintaining%2520consistent%250Aperformance%2520in%2520different%2520lighting%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolypFlow%3A%20Reinforcing%20Polyp%20Segmentation%20with%20Flow-Driven%20Dynamics&entry.906535625=Pu%20Wang%20and%20Huaizhi%20Ma%20and%20Zhihua%20Zhang%20and%20Zhuoran%20Zheng&entry.1292438233=%20%20Accurate%20polyp%20segmentation%20remains%20challenging%20due%20to%20irregular%20lesion%0Amorphologies%2C%20ambiguous%20boundaries%2C%20and%20heterogeneous%20imaging%20conditions.%20While%0AU-Net%20variants%20excel%20at%20local%20feature%20fusion%2C%20they%20often%20lack%20explicit%0Amechanisms%20to%20model%20the%20dynamic%20evolution%20of%20segmentation%20confidence%20under%0Auncertainty.%20Inspired%20by%20the%20interpretable%20nature%20of%20flow-based%20models%2C%20we%0Apresent%20%5Ctextbf%7BPolypFLow%7D%2C%20a%20flow-matching%20enhanced%20architecture%20that%20injects%0Aphysics-inspired%20optimization%20dynamics%20into%20segmentation%20refinement.%20Unlike%0Aconventional%20cascaded%20networks%2C%20our%20framework%20solves%20an%20ordinary%20differential%0Aequation%20%28ODE%29%20to%20progressively%20align%20coarse%20initial%20predictions%20with%20ground%0Atruth%20masks%20through%20learned%20velocity%20fields.%20This%20trajectory-based%20refinement%0Aoffers%20two%20key%20advantages%3A%201%29%20Interpretable%20Optimization%3A%20Intermediate%20flow%0Asteps%20visualize%20how%20the%20model%20corrects%20under-segmented%20regions%20and%20sharpens%0Aboundaries%20at%20each%20ODE-solver%20iteration%2C%20demystifying%20the%20%60%60black-box%22%0Arefinement%20process%3B%202%29%20Boundary-Aware%20Robustness%3A%20The%20flow%20dynamics%20explicitly%0Amodel%20gradient%20directions%20along%20polyp%20edges%2C%20enhancing%20resilience%20to%0Alow-contrast%20regions%20and%20motion%20artifacts.%20Numerous%20experimental%20results%20show%0Athat%20PolypFLow%20achieves%20a%20state-of-the-art%20while%20maintaining%20consistent%0Aperformance%20in%20different%20lighting%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19037v1&entry.124074799=Read"},
{"title": "Distill Any Depth: Distillation Creates a Stronger Monocular Depth\n  Estimator", "author": "Xiankang He and Dongyan Guo and Hongji Li and Ruibo Li and Ying Cui and Chi Zhang", "abstract": "  Monocular depth estimation (MDE) aims to predict scene depth from a single\nRGB image and plays a crucial role in 3D scene understanding. Recent advances\nin zero-shot MDE leverage normalized depth representations and\ndistillation-based learning to improve generalization across diverse scenes.\nHowever, current depth normalization methods for distillation, relying on\nglobal normalization, can amplify noisy pseudo-labels, reducing distillation\neffectiveness. In this paper, we systematically analyze the impact of different\ndepth normalization strategies on pseudo-label distillation. Based on our\nfindings, we propose Cross-Context Distillation, which integrates global and\nlocal depth cues to enhance pseudo-label quality. Additionally, we introduce a\nmulti-teacher distillation framework that leverages complementary strengths of\ndifferent depth estimation models, leading to more robust and accurate depth\npredictions. Extensive experiments on benchmark datasets demonstrate that our\napproach significantly outperforms state-of-the-art methods, both\nquantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2502.19204v1", "date": "2025-02-26", "relevancy": 2.093, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5722}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distill%20Any%20Depth%3A%20Distillation%20Creates%20a%20Stronger%20Monocular%20Depth%0A%20%20Estimator&body=Title%3A%20Distill%20Any%20Depth%3A%20Distillation%20Creates%20a%20Stronger%20Monocular%20Depth%0A%20%20Estimator%0AAuthor%3A%20Xiankang%20He%20and%20Dongyan%20Guo%20and%20Hongji%20Li%20and%20Ruibo%20Li%20and%20Ying%20Cui%20and%20Chi%20Zhang%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20%28MDE%29%20aims%20to%20predict%20scene%20depth%20from%20a%20single%0ARGB%20image%20and%20plays%20a%20crucial%20role%20in%203D%20scene%20understanding.%20Recent%20advances%0Ain%20zero-shot%20MDE%20leverage%20normalized%20depth%20representations%20and%0Adistillation-based%20learning%20to%20improve%20generalization%20across%20diverse%20scenes.%0AHowever%2C%20current%20depth%20normalization%20methods%20for%20distillation%2C%20relying%20on%0Aglobal%20normalization%2C%20can%20amplify%20noisy%20pseudo-labels%2C%20reducing%20distillation%0Aeffectiveness.%20In%20this%20paper%2C%20we%20systematically%20analyze%20the%20impact%20of%20different%0Adepth%20normalization%20strategies%20on%20pseudo-label%20distillation.%20Based%20on%20our%0Afindings%2C%20we%20propose%20Cross-Context%20Distillation%2C%20which%20integrates%20global%20and%0Alocal%20depth%20cues%20to%20enhance%20pseudo-label%20quality.%20Additionally%2C%20we%20introduce%20a%0Amulti-teacher%20distillation%20framework%20that%20leverages%20complementary%20strengths%20of%0Adifferent%20depth%20estimation%20models%2C%20leading%20to%20more%20robust%20and%20accurate%20depth%0Apredictions.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20our%0Aapproach%20significantly%20outperforms%20state-of-the-art%20methods%2C%20both%0Aquantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistill%2520Any%2520Depth%253A%2520Distillation%2520Creates%2520a%2520Stronger%2520Monocular%2520Depth%250A%2520%2520Estimator%26entry.906535625%3DXiankang%2520He%2520and%2520Dongyan%2520Guo%2520and%2520Hongji%2520Li%2520and%2520Ruibo%2520Li%2520and%2520Ying%2520Cui%2520and%2520Chi%2520Zhang%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520%2528MDE%2529%2520aims%2520to%2520predict%2520scene%2520depth%2520from%2520a%2520single%250ARGB%2520image%2520and%2520plays%2520a%2520crucial%2520role%2520in%25203D%2520scene%2520understanding.%2520Recent%2520advances%250Ain%2520zero-shot%2520MDE%2520leverage%2520normalized%2520depth%2520representations%2520and%250Adistillation-based%2520learning%2520to%2520improve%2520generalization%2520across%2520diverse%2520scenes.%250AHowever%252C%2520current%2520depth%2520normalization%2520methods%2520for%2520distillation%252C%2520relying%2520on%250Aglobal%2520normalization%252C%2520can%2520amplify%2520noisy%2520pseudo-labels%252C%2520reducing%2520distillation%250Aeffectiveness.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520analyze%2520the%2520impact%2520of%2520different%250Adepth%2520normalization%2520strategies%2520on%2520pseudo-label%2520distillation.%2520Based%2520on%2520our%250Afindings%252C%2520we%2520propose%2520Cross-Context%2520Distillation%252C%2520which%2520integrates%2520global%2520and%250Alocal%2520depth%2520cues%2520to%2520enhance%2520pseudo-label%2520quality.%2520Additionally%252C%2520we%2520introduce%2520a%250Amulti-teacher%2520distillation%2520framework%2520that%2520leverages%2520complementary%2520strengths%2520of%250Adifferent%2520depth%2520estimation%2520models%252C%2520leading%2520to%2520more%2520robust%2520and%2520accurate%2520depth%250Apredictions.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%250Aapproach%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520both%250Aquantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distill%20Any%20Depth%3A%20Distillation%20Creates%20a%20Stronger%20Monocular%20Depth%0A%20%20Estimator&entry.906535625=Xiankang%20He%20and%20Dongyan%20Guo%20and%20Hongji%20Li%20and%20Ruibo%20Li%20and%20Ying%20Cui%20and%20Chi%20Zhang&entry.1292438233=%20%20Monocular%20depth%20estimation%20%28MDE%29%20aims%20to%20predict%20scene%20depth%20from%20a%20single%0ARGB%20image%20and%20plays%20a%20crucial%20role%20in%203D%20scene%20understanding.%20Recent%20advances%0Ain%20zero-shot%20MDE%20leverage%20normalized%20depth%20representations%20and%0Adistillation-based%20learning%20to%20improve%20generalization%20across%20diverse%20scenes.%0AHowever%2C%20current%20depth%20normalization%20methods%20for%20distillation%2C%20relying%20on%0Aglobal%20normalization%2C%20can%20amplify%20noisy%20pseudo-labels%2C%20reducing%20distillation%0Aeffectiveness.%20In%20this%20paper%2C%20we%20systematically%20analyze%20the%20impact%20of%20different%0Adepth%20normalization%20strategies%20on%20pseudo-label%20distillation.%20Based%20on%20our%0Afindings%2C%20we%20propose%20Cross-Context%20Distillation%2C%20which%20integrates%20global%20and%0Alocal%20depth%20cues%20to%20enhance%20pseudo-label%20quality.%20Additionally%2C%20we%20introduce%20a%0Amulti-teacher%20distillation%20framework%20that%20leverages%20complementary%20strengths%20of%0Adifferent%20depth%20estimation%20models%2C%20leading%20to%20more%20robust%20and%20accurate%20depth%0Apredictions.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20our%0Aapproach%20significantly%20outperforms%20state-of-the-art%20methods%2C%20both%0Aquantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19204v1&entry.124074799=Read"},
{"title": "EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the\n  Arab Gulf Region", "author": "Nadya Abdel Madjid and Murad Mebrahtu and Abdelmoamen Nasser and Bilal Hassan and Naoufel Werghi and Jorge Dias and Majid Khonji", "abstract": "  This paper introduces the Emirates Multi-Task (EMT) dataset - the first\npublicly available dataset for autonomous driving collected in the Arab Gulf\nregion. The EMT dataset captures the unique road topology, high traffic\ncongestion, and distinctive characteristics of the Gulf region, including\nvariations in pedestrian clothing and weather conditions. It contains over\n30,000 frames from a dash-camera perspective, along with 570,000 annotated\nbounding boxes, covering approximately 150 kilometers of driving routes. The\nEMT dataset supports three primary tasks: tracking, trajectory forecasting and\nintention prediction. Each benchmark dataset is complemented with corresponding\nevaluations: (1) multi-agent tracking experiments, focusing on multi-class\nscenarios and occlusion handling; (2) trajectory forecasting evaluation using\ndeep sequential and interaction-aware models; and (3) intention benchmark\nexperiments conducted for predicting agents intentions from observed\ntrajectories. The dataset is publicly available at\nhttps://avlab.io/emt-dataset, and pre-processing scripts along with evaluation\nmodels can be accessed at https://github.com/AV-Lab/emt-dataset.\n", "link": "http://arxiv.org/abs/2502.19260v1", "date": "2025-02-26", "relevancy": 2.0871, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5321}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5237}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMT%3A%20A%20Visual%20Multi-Task%20Benchmark%20Dataset%20for%20Autonomous%20Driving%20in%20the%0A%20%20Arab%20Gulf%20Region&body=Title%3A%20EMT%3A%20A%20Visual%20Multi-Task%20Benchmark%20Dataset%20for%20Autonomous%20Driving%20in%20the%0A%20%20Arab%20Gulf%20Region%0AAuthor%3A%20Nadya%20Abdel%20Madjid%20and%20Murad%20Mebrahtu%20and%20Abdelmoamen%20Nasser%20and%20Bilal%20Hassan%20and%20Naoufel%20Werghi%20and%20Jorge%20Dias%20and%20Majid%20Khonji%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Emirates%20Multi-Task%20%28EMT%29%20dataset%20-%20the%20first%0Apublicly%20available%20dataset%20for%20autonomous%20driving%20collected%20in%20the%20Arab%20Gulf%0Aregion.%20The%20EMT%20dataset%20captures%20the%20unique%20road%20topology%2C%20high%20traffic%0Acongestion%2C%20and%20distinctive%20characteristics%20of%20the%20Gulf%20region%2C%20including%0Avariations%20in%20pedestrian%20clothing%20and%20weather%20conditions.%20It%20contains%20over%0A30%2C000%20frames%20from%20a%20dash-camera%20perspective%2C%20along%20with%20570%2C000%20annotated%0Abounding%20boxes%2C%20covering%20approximately%20150%20kilometers%20of%20driving%20routes.%20The%0AEMT%20dataset%20supports%20three%20primary%20tasks%3A%20tracking%2C%20trajectory%20forecasting%20and%0Aintention%20prediction.%20Each%20benchmark%20dataset%20is%20complemented%20with%20corresponding%0Aevaluations%3A%20%281%29%20multi-agent%20tracking%20experiments%2C%20focusing%20on%20multi-class%0Ascenarios%20and%20occlusion%20handling%3B%20%282%29%20trajectory%20forecasting%20evaluation%20using%0Adeep%20sequential%20and%20interaction-aware%20models%3B%20and%20%283%29%20intention%20benchmark%0Aexperiments%20conducted%20for%20predicting%20agents%20intentions%20from%20observed%0Atrajectories.%20The%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//avlab.io/emt-dataset%2C%20and%20pre-processing%20scripts%20along%20with%20evaluation%0Amodels%20can%20be%20accessed%20at%20https%3A//github.com/AV-Lab/emt-dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMT%253A%2520A%2520Visual%2520Multi-Task%2520Benchmark%2520Dataset%2520for%2520Autonomous%2520Driving%2520in%2520the%250A%2520%2520Arab%2520Gulf%2520Region%26entry.906535625%3DNadya%2520Abdel%2520Madjid%2520and%2520Murad%2520Mebrahtu%2520and%2520Abdelmoamen%2520Nasser%2520and%2520Bilal%2520Hassan%2520and%2520Naoufel%2520Werghi%2520and%2520Jorge%2520Dias%2520and%2520Majid%2520Khonji%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Emirates%2520Multi-Task%2520%2528EMT%2529%2520dataset%2520-%2520the%2520first%250Apublicly%2520available%2520dataset%2520for%2520autonomous%2520driving%2520collected%2520in%2520the%2520Arab%2520Gulf%250Aregion.%2520The%2520EMT%2520dataset%2520captures%2520the%2520unique%2520road%2520topology%252C%2520high%2520traffic%250Acongestion%252C%2520and%2520distinctive%2520characteristics%2520of%2520the%2520Gulf%2520region%252C%2520including%250Avariations%2520in%2520pedestrian%2520clothing%2520and%2520weather%2520conditions.%2520It%2520contains%2520over%250A30%252C000%2520frames%2520from%2520a%2520dash-camera%2520perspective%252C%2520along%2520with%2520570%252C000%2520annotated%250Abounding%2520boxes%252C%2520covering%2520approximately%2520150%2520kilometers%2520of%2520driving%2520routes.%2520The%250AEMT%2520dataset%2520supports%2520three%2520primary%2520tasks%253A%2520tracking%252C%2520trajectory%2520forecasting%2520and%250Aintention%2520prediction.%2520Each%2520benchmark%2520dataset%2520is%2520complemented%2520with%2520corresponding%250Aevaluations%253A%2520%25281%2529%2520multi-agent%2520tracking%2520experiments%252C%2520focusing%2520on%2520multi-class%250Ascenarios%2520and%2520occlusion%2520handling%253B%2520%25282%2529%2520trajectory%2520forecasting%2520evaluation%2520using%250Adeep%2520sequential%2520and%2520interaction-aware%2520models%253B%2520and%2520%25283%2529%2520intention%2520benchmark%250Aexperiments%2520conducted%2520for%2520predicting%2520agents%2520intentions%2520from%2520observed%250Atrajectories.%2520The%2520dataset%2520is%2520publicly%2520available%2520at%250Ahttps%253A//avlab.io/emt-dataset%252C%2520and%2520pre-processing%2520scripts%2520along%2520with%2520evaluation%250Amodels%2520can%2520be%2520accessed%2520at%2520https%253A//github.com/AV-Lab/emt-dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMT%3A%20A%20Visual%20Multi-Task%20Benchmark%20Dataset%20for%20Autonomous%20Driving%20in%20the%0A%20%20Arab%20Gulf%20Region&entry.906535625=Nadya%20Abdel%20Madjid%20and%20Murad%20Mebrahtu%20and%20Abdelmoamen%20Nasser%20and%20Bilal%20Hassan%20and%20Naoufel%20Werghi%20and%20Jorge%20Dias%20and%20Majid%20Khonji&entry.1292438233=%20%20This%20paper%20introduces%20the%20Emirates%20Multi-Task%20%28EMT%29%20dataset%20-%20the%20first%0Apublicly%20available%20dataset%20for%20autonomous%20driving%20collected%20in%20the%20Arab%20Gulf%0Aregion.%20The%20EMT%20dataset%20captures%20the%20unique%20road%20topology%2C%20high%20traffic%0Acongestion%2C%20and%20distinctive%20characteristics%20of%20the%20Gulf%20region%2C%20including%0Avariations%20in%20pedestrian%20clothing%20and%20weather%20conditions.%20It%20contains%20over%0A30%2C000%20frames%20from%20a%20dash-camera%20perspective%2C%20along%20with%20570%2C000%20annotated%0Abounding%20boxes%2C%20covering%20approximately%20150%20kilometers%20of%20driving%20routes.%20The%0AEMT%20dataset%20supports%20three%20primary%20tasks%3A%20tracking%2C%20trajectory%20forecasting%20and%0Aintention%20prediction.%20Each%20benchmark%20dataset%20is%20complemented%20with%20corresponding%0Aevaluations%3A%20%281%29%20multi-agent%20tracking%20experiments%2C%20focusing%20on%20multi-class%0Ascenarios%20and%20occlusion%20handling%3B%20%282%29%20trajectory%20forecasting%20evaluation%20using%0Adeep%20sequential%20and%20interaction-aware%20models%3B%20and%20%283%29%20intention%20benchmark%0Aexperiments%20conducted%20for%20predicting%20agents%20intentions%20from%20observed%0Atrajectories.%20The%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//avlab.io/emt-dataset%2C%20and%20pre-processing%20scripts%20along%20with%20evaluation%0Amodels%20can%20be%20accessed%20at%20https%3A//github.com/AV-Lab/emt-dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19260v1&entry.124074799=Read"},
{"title": "A HEART for the environment: Transformer-Based Spatiotemporal Modeling\n  for Air Quality Prediction", "author": "Norbert Bodendorfer", "abstract": "  Accurate and reliable air pollution forecasting is crucial for effective\nenvironmental management and policy-making. llull-environment is a\nsophisticated and scalable forecasting system for air pollution, inspired by\nprevious models currently operational in Madrid and Valladolid (Spain). It\ncontains (among other key components) an encoder-decoder convolutional neural\nnetwork to forecast mean pollution levels for four key pollutants (NO$_2$,\nO$_3$, PM$_{10}$, PM$_{2.5}$) using historical data, external forecasts, and\nother contextual features. This paper investigates the augmentation of this\nneural network with an attention mechanism to improve predictive accuracy. The\nproposed attention mechanism pre-processes tensors containing the input\nfeatures before passing them to the existing mean forecasting model. The\nresulting model is a combination of several architectures and ideas and can be\ndescribed as a \"Hybrid Enhanced Autoregressive Transformer\", or HEART. The\neffectiveness of the approach is evaluated by comparing the mean square error\n(MSE) across different attention layouts against the system without such a\nmechanism. We observe a significant reduction in MSE of up to 22%, with an\naverage of 7.5% across tested cities and pollutants. The performance of a given\nattention mechanism turns out to depend on the pollutant, highlighting the\ndifferences in their creation and dissipation processes. Our findings are not\nrestricted to optimizing air quality prediction models, but are applicable\ngenerally to (fixed length) time series forecasting.\n", "link": "http://arxiv.org/abs/2502.19042v1", "date": "2025-02-26", "relevancy": 2.0819, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5635}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5303}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20HEART%20for%20the%20environment%3A%20Transformer-Based%20Spatiotemporal%20Modeling%0A%20%20for%20Air%20Quality%20Prediction&body=Title%3A%20A%20HEART%20for%20the%20environment%3A%20Transformer-Based%20Spatiotemporal%20Modeling%0A%20%20for%20Air%20Quality%20Prediction%0AAuthor%3A%20Norbert%20Bodendorfer%0AAbstract%3A%20%20%20Accurate%20and%20reliable%20air%20pollution%20forecasting%20is%20crucial%20for%20effective%0Aenvironmental%20management%20and%20policy-making.%20llull-environment%20is%20a%0Asophisticated%20and%20scalable%20forecasting%20system%20for%20air%20pollution%2C%20inspired%20by%0Aprevious%20models%20currently%20operational%20in%20Madrid%20and%20Valladolid%20%28Spain%29.%20It%0Acontains%20%28among%20other%20key%20components%29%20an%20encoder-decoder%20convolutional%20neural%0Anetwork%20to%20forecast%20mean%20pollution%20levels%20for%20four%20key%20pollutants%20%28NO%24_2%24%2C%0AO%24_3%24%2C%20PM%24_%7B10%7D%24%2C%20PM%24_%7B2.5%7D%24%29%20using%20historical%20data%2C%20external%20forecasts%2C%20and%0Aother%20contextual%20features.%20This%20paper%20investigates%20the%20augmentation%20of%20this%0Aneural%20network%20with%20an%20attention%20mechanism%20to%20improve%20predictive%20accuracy.%20The%0Aproposed%20attention%20mechanism%20pre-processes%20tensors%20containing%20the%20input%0Afeatures%20before%20passing%20them%20to%20the%20existing%20mean%20forecasting%20model.%20The%0Aresulting%20model%20is%20a%20combination%20of%20several%20architectures%20and%20ideas%20and%20can%20be%0Adescribed%20as%20a%20%22Hybrid%20Enhanced%20Autoregressive%20Transformer%22%2C%20or%20HEART.%20The%0Aeffectiveness%20of%20the%20approach%20is%20evaluated%20by%20comparing%20the%20mean%20square%20error%0A%28MSE%29%20across%20different%20attention%20layouts%20against%20the%20system%20without%20such%20a%0Amechanism.%20We%20observe%20a%20significant%20reduction%20in%20MSE%20of%20up%20to%2022%25%2C%20with%20an%0Aaverage%20of%207.5%25%20across%20tested%20cities%20and%20pollutants.%20The%20performance%20of%20a%20given%0Aattention%20mechanism%20turns%20out%20to%20depend%20on%20the%20pollutant%2C%20highlighting%20the%0Adifferences%20in%20their%20creation%20and%20dissipation%20processes.%20Our%20findings%20are%20not%0Arestricted%20to%20optimizing%20air%20quality%20prediction%20models%2C%20but%20are%20applicable%0Agenerally%20to%20%28fixed%20length%29%20time%20series%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520HEART%2520for%2520the%2520environment%253A%2520Transformer-Based%2520Spatiotemporal%2520Modeling%250A%2520%2520for%2520Air%2520Quality%2520Prediction%26entry.906535625%3DNorbert%2520Bodendorfer%26entry.1292438233%3D%2520%2520Accurate%2520and%2520reliable%2520air%2520pollution%2520forecasting%2520is%2520crucial%2520for%2520effective%250Aenvironmental%2520management%2520and%2520policy-making.%2520llull-environment%2520is%2520a%250Asophisticated%2520and%2520scalable%2520forecasting%2520system%2520for%2520air%2520pollution%252C%2520inspired%2520by%250Aprevious%2520models%2520currently%2520operational%2520in%2520Madrid%2520and%2520Valladolid%2520%2528Spain%2529.%2520It%250Acontains%2520%2528among%2520other%2520key%2520components%2529%2520an%2520encoder-decoder%2520convolutional%2520neural%250Anetwork%2520to%2520forecast%2520mean%2520pollution%2520levels%2520for%2520four%2520key%2520pollutants%2520%2528NO%2524_2%2524%252C%250AO%2524_3%2524%252C%2520PM%2524_%257B10%257D%2524%252C%2520PM%2524_%257B2.5%257D%2524%2529%2520using%2520historical%2520data%252C%2520external%2520forecasts%252C%2520and%250Aother%2520contextual%2520features.%2520This%2520paper%2520investigates%2520the%2520augmentation%2520of%2520this%250Aneural%2520network%2520with%2520an%2520attention%2520mechanism%2520to%2520improve%2520predictive%2520accuracy.%2520The%250Aproposed%2520attention%2520mechanism%2520pre-processes%2520tensors%2520containing%2520the%2520input%250Afeatures%2520before%2520passing%2520them%2520to%2520the%2520existing%2520mean%2520forecasting%2520model.%2520The%250Aresulting%2520model%2520is%2520a%2520combination%2520of%2520several%2520architectures%2520and%2520ideas%2520and%2520can%2520be%250Adescribed%2520as%2520a%2520%2522Hybrid%2520Enhanced%2520Autoregressive%2520Transformer%2522%252C%2520or%2520HEART.%2520The%250Aeffectiveness%2520of%2520the%2520approach%2520is%2520evaluated%2520by%2520comparing%2520the%2520mean%2520square%2520error%250A%2528MSE%2529%2520across%2520different%2520attention%2520layouts%2520against%2520the%2520system%2520without%2520such%2520a%250Amechanism.%2520We%2520observe%2520a%2520significant%2520reduction%2520in%2520MSE%2520of%2520up%2520to%252022%2525%252C%2520with%2520an%250Aaverage%2520of%25207.5%2525%2520across%2520tested%2520cities%2520and%2520pollutants.%2520The%2520performance%2520of%2520a%2520given%250Aattention%2520mechanism%2520turns%2520out%2520to%2520depend%2520on%2520the%2520pollutant%252C%2520highlighting%2520the%250Adifferences%2520in%2520their%2520creation%2520and%2520dissipation%2520processes.%2520Our%2520findings%2520are%2520not%250Arestricted%2520to%2520optimizing%2520air%2520quality%2520prediction%2520models%252C%2520but%2520are%2520applicable%250Agenerally%2520to%2520%2528fixed%2520length%2529%2520time%2520series%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20HEART%20for%20the%20environment%3A%20Transformer-Based%20Spatiotemporal%20Modeling%0A%20%20for%20Air%20Quality%20Prediction&entry.906535625=Norbert%20Bodendorfer&entry.1292438233=%20%20Accurate%20and%20reliable%20air%20pollution%20forecasting%20is%20crucial%20for%20effective%0Aenvironmental%20management%20and%20policy-making.%20llull-environment%20is%20a%0Asophisticated%20and%20scalable%20forecasting%20system%20for%20air%20pollution%2C%20inspired%20by%0Aprevious%20models%20currently%20operational%20in%20Madrid%20and%20Valladolid%20%28Spain%29.%20It%0Acontains%20%28among%20other%20key%20components%29%20an%20encoder-decoder%20convolutional%20neural%0Anetwork%20to%20forecast%20mean%20pollution%20levels%20for%20four%20key%20pollutants%20%28NO%24_2%24%2C%0AO%24_3%24%2C%20PM%24_%7B10%7D%24%2C%20PM%24_%7B2.5%7D%24%29%20using%20historical%20data%2C%20external%20forecasts%2C%20and%0Aother%20contextual%20features.%20This%20paper%20investigates%20the%20augmentation%20of%20this%0Aneural%20network%20with%20an%20attention%20mechanism%20to%20improve%20predictive%20accuracy.%20The%0Aproposed%20attention%20mechanism%20pre-processes%20tensors%20containing%20the%20input%0Afeatures%20before%20passing%20them%20to%20the%20existing%20mean%20forecasting%20model.%20The%0Aresulting%20model%20is%20a%20combination%20of%20several%20architectures%20and%20ideas%20and%20can%20be%0Adescribed%20as%20a%20%22Hybrid%20Enhanced%20Autoregressive%20Transformer%22%2C%20or%20HEART.%20The%0Aeffectiveness%20of%20the%20approach%20is%20evaluated%20by%20comparing%20the%20mean%20square%20error%0A%28MSE%29%20across%20different%20attention%20layouts%20against%20the%20system%20without%20such%20a%0Amechanism.%20We%20observe%20a%20significant%20reduction%20in%20MSE%20of%20up%20to%2022%25%2C%20with%20an%0Aaverage%20of%207.5%25%20across%20tested%20cities%20and%20pollutants.%20The%20performance%20of%20a%20given%0Aattention%20mechanism%20turns%20out%20to%20depend%20on%20the%20pollutant%2C%20highlighting%20the%0Adifferences%20in%20their%20creation%20and%20dissipation%20processes.%20Our%20findings%20are%20not%0Arestricted%20to%20optimizing%20air%20quality%20prediction%20models%2C%20but%20are%20applicable%0Agenerally%20to%20%28fixed%20length%29%20time%20series%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19042v1&entry.124074799=Read"},
{"title": "Wasserstein Distances, Neuronal Entanglement, and Sparsity", "author": "Shashata Sawmya and Linghao Kong and Ilia Markov and Dan Alistarh and Nir Shavit", "abstract": "  Disentangling polysemantic neurons is at the core of many current approaches\nto interpretability of large language models. Here we attempt to study how\ndisentanglement can be used to understand performance, particularly under\nweight sparsity, a leading post-training optimization technique. We suggest a\nnovel measure for estimating neuronal entanglement: the Wasserstein distance of\na neuron's output distribution to a Gaussian. Moreover, we show the existence\nof a small number of highly entangled \"Wasserstein Neurons\" in each linear\nlayer of an LLM, characterized by their highly non-Gaussian output\ndistributions, their role in mapping similar inputs to dissimilar outputs, and\ntheir significant impact on model accuracy. To study these phenomena, we\npropose a new experimental framework for disentangling polysemantic neurons.\nOur framework separates each layer's inputs to create a mixture of experts\nwhere each neuron's output is computed by a mixture of neurons of lower\nWasserstein distance, each better at maintaining accuracy when sparsified\nwithout retraining. We provide strong evidence that this is because the mixture\nof sparse experts is effectively disentangling the input-output relationship of\nindividual neurons, in particular the difficult Wasserstein neurons.\n", "link": "http://arxiv.org/abs/2405.15756v4", "date": "2025-02-26", "relevancy": 2.0798, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wasserstein%20Distances%2C%20Neuronal%20Entanglement%2C%20and%20Sparsity&body=Title%3A%20Wasserstein%20Distances%2C%20Neuronal%20Entanglement%2C%20and%20Sparsity%0AAuthor%3A%20Shashata%20Sawmya%20and%20Linghao%20Kong%20and%20Ilia%20Markov%20and%20Dan%20Alistarh%20and%20Nir%20Shavit%0AAbstract%3A%20%20%20Disentangling%20polysemantic%20neurons%20is%20at%20the%20core%20of%20many%20current%20approaches%0Ato%20interpretability%20of%20large%20language%20models.%20Here%20we%20attempt%20to%20study%20how%0Adisentanglement%20can%20be%20used%20to%20understand%20performance%2C%20particularly%20under%0Aweight%20sparsity%2C%20a%20leading%20post-training%20optimization%20technique.%20We%20suggest%20a%0Anovel%20measure%20for%20estimating%20neuronal%20entanglement%3A%20the%20Wasserstein%20distance%20of%0Aa%20neuron%27s%20output%20distribution%20to%20a%20Gaussian.%20Moreover%2C%20we%20show%20the%20existence%0Aof%20a%20small%20number%20of%20highly%20entangled%20%22Wasserstein%20Neurons%22%20in%20each%20linear%0Alayer%20of%20an%20LLM%2C%20characterized%20by%20their%20highly%20non-Gaussian%20output%0Adistributions%2C%20their%20role%20in%20mapping%20similar%20inputs%20to%20dissimilar%20outputs%2C%20and%0Atheir%20significant%20impact%20on%20model%20accuracy.%20To%20study%20these%20phenomena%2C%20we%0Apropose%20a%20new%20experimental%20framework%20for%20disentangling%20polysemantic%20neurons.%0AOur%20framework%20separates%20each%20layer%27s%20inputs%20to%20create%20a%20mixture%20of%20experts%0Awhere%20each%20neuron%27s%20output%20is%20computed%20by%20a%20mixture%20of%20neurons%20of%20lower%0AWasserstein%20distance%2C%20each%20better%20at%20maintaining%20accuracy%20when%20sparsified%0Awithout%20retraining.%20We%20provide%20strong%20evidence%20that%20this%20is%20because%20the%20mixture%0Aof%20sparse%20experts%20is%20effectively%20disentangling%20the%20input-output%20relationship%20of%0Aindividual%20neurons%2C%20in%20particular%20the%20difficult%20Wasserstein%20neurons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15756v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWasserstein%2520Distances%252C%2520Neuronal%2520Entanglement%252C%2520and%2520Sparsity%26entry.906535625%3DShashata%2520Sawmya%2520and%2520Linghao%2520Kong%2520and%2520Ilia%2520Markov%2520and%2520Dan%2520Alistarh%2520and%2520Nir%2520Shavit%26entry.1292438233%3D%2520%2520Disentangling%2520polysemantic%2520neurons%2520is%2520at%2520the%2520core%2520of%2520many%2520current%2520approaches%250Ato%2520interpretability%2520of%2520large%2520language%2520models.%2520Here%2520we%2520attempt%2520to%2520study%2520how%250Adisentanglement%2520can%2520be%2520used%2520to%2520understand%2520performance%252C%2520particularly%2520under%250Aweight%2520sparsity%252C%2520a%2520leading%2520post-training%2520optimization%2520technique.%2520We%2520suggest%2520a%250Anovel%2520measure%2520for%2520estimating%2520neuronal%2520entanglement%253A%2520the%2520Wasserstein%2520distance%2520of%250Aa%2520neuron%2527s%2520output%2520distribution%2520to%2520a%2520Gaussian.%2520Moreover%252C%2520we%2520show%2520the%2520existence%250Aof%2520a%2520small%2520number%2520of%2520highly%2520entangled%2520%2522Wasserstein%2520Neurons%2522%2520in%2520each%2520linear%250Alayer%2520of%2520an%2520LLM%252C%2520characterized%2520by%2520their%2520highly%2520non-Gaussian%2520output%250Adistributions%252C%2520their%2520role%2520in%2520mapping%2520similar%2520inputs%2520to%2520dissimilar%2520outputs%252C%2520and%250Atheir%2520significant%2520impact%2520on%2520model%2520accuracy.%2520To%2520study%2520these%2520phenomena%252C%2520we%250Apropose%2520a%2520new%2520experimental%2520framework%2520for%2520disentangling%2520polysemantic%2520neurons.%250AOur%2520framework%2520separates%2520each%2520layer%2527s%2520inputs%2520to%2520create%2520a%2520mixture%2520of%2520experts%250Awhere%2520each%2520neuron%2527s%2520output%2520is%2520computed%2520by%2520a%2520mixture%2520of%2520neurons%2520of%2520lower%250AWasserstein%2520distance%252C%2520each%2520better%2520at%2520maintaining%2520accuracy%2520when%2520sparsified%250Awithout%2520retraining.%2520We%2520provide%2520strong%2520evidence%2520that%2520this%2520is%2520because%2520the%2520mixture%250Aof%2520sparse%2520experts%2520is%2520effectively%2520disentangling%2520the%2520input-output%2520relationship%2520of%250Aindividual%2520neurons%252C%2520in%2520particular%2520the%2520difficult%2520Wasserstein%2520neurons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15756v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wasserstein%20Distances%2C%20Neuronal%20Entanglement%2C%20and%20Sparsity&entry.906535625=Shashata%20Sawmya%20and%20Linghao%20Kong%20and%20Ilia%20Markov%20and%20Dan%20Alistarh%20and%20Nir%20Shavit&entry.1292438233=%20%20Disentangling%20polysemantic%20neurons%20is%20at%20the%20core%20of%20many%20current%20approaches%0Ato%20interpretability%20of%20large%20language%20models.%20Here%20we%20attempt%20to%20study%20how%0Adisentanglement%20can%20be%20used%20to%20understand%20performance%2C%20particularly%20under%0Aweight%20sparsity%2C%20a%20leading%20post-training%20optimization%20technique.%20We%20suggest%20a%0Anovel%20measure%20for%20estimating%20neuronal%20entanglement%3A%20the%20Wasserstein%20distance%20of%0Aa%20neuron%27s%20output%20distribution%20to%20a%20Gaussian.%20Moreover%2C%20we%20show%20the%20existence%0Aof%20a%20small%20number%20of%20highly%20entangled%20%22Wasserstein%20Neurons%22%20in%20each%20linear%0Alayer%20of%20an%20LLM%2C%20characterized%20by%20their%20highly%20non-Gaussian%20output%0Adistributions%2C%20their%20role%20in%20mapping%20similar%20inputs%20to%20dissimilar%20outputs%2C%20and%0Atheir%20significant%20impact%20on%20model%20accuracy.%20To%20study%20these%20phenomena%2C%20we%0Apropose%20a%20new%20experimental%20framework%20for%20disentangling%20polysemantic%20neurons.%0AOur%20framework%20separates%20each%20layer%27s%20inputs%20to%20create%20a%20mixture%20of%20experts%0Awhere%20each%20neuron%27s%20output%20is%20computed%20by%20a%20mixture%20of%20neurons%20of%20lower%0AWasserstein%20distance%2C%20each%20better%20at%20maintaining%20accuracy%20when%20sparsified%0Awithout%20retraining.%20We%20provide%20strong%20evidence%20that%20this%20is%20because%20the%20mixture%0Aof%20sparse%20experts%20is%20effectively%20disentangling%20the%20input-output%20relationship%20of%0Aindividual%20neurons%2C%20in%20particular%20the%20difficult%20Wasserstein%20neurons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15756v4&entry.124074799=Read"},
{"title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention\n  Matrix", "author": "Yingyu Liang and Jiangxuan Long and Zhenmei Shi and Zhao Song and Yufa Zhou", "abstract": "  Large Language Models (LLMs) have shown immense potential in enhancing\nvarious aspects of our daily lives, from conversational AI to search and AI\nassistants. However, their growing capabilities come at the cost of extremely\nlarge model sizes, making deployment on edge devices challenging due to memory\nand computational constraints. This paper introduces a novel approach to LLM\nweight pruning that directly optimizes for approximating the attention matrix,\na core component of transformer architectures. Unlike existing methods that\nfocus on linear approximations, our approach accounts for the non-linear nature\nof the Softmax attention mechanism. We provide theoretical guarantees for the\nconvergence of our Gradient Descent-based optimization method to a near-optimal\npruning mask solution. Our empirical results demonstrate the effectiveness of\nour non-linear pruning approach in maintaining model performance while\nsignificantly reducing computational costs, which is beyond the current\nstate-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This\nwork establishes a new theoretical foundation for pruning algorithm design in\nLLMs, potentially paving the way for more efficient LLM inference on\nresource-constrained devices.\n", "link": "http://arxiv.org/abs/2410.11261v2", "date": "2025-02-26", "relevancy": 2.0714, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5482}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5135}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Linear%20Approximations%3A%20A%20Novel%20Pruning%20Approach%20for%20Attention%0A%20%20Matrix&body=Title%3A%20Beyond%20Linear%20Approximations%3A%20A%20Novel%20Pruning%20Approach%20for%20Attention%0A%20%20Matrix%0AAuthor%3A%20Yingyu%20Liang%20and%20Jiangxuan%20Long%20and%20Zhenmei%20Shi%20and%20Zhao%20Song%20and%20Yufa%20Zhou%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20immense%20potential%20in%20enhancing%0Avarious%20aspects%20of%20our%20daily%20lives%2C%20from%20conversational%20AI%20to%20search%20and%20AI%0Aassistants.%20However%2C%20their%20growing%20capabilities%20come%20at%20the%20cost%20of%20extremely%0Alarge%20model%20sizes%2C%20making%20deployment%20on%20edge%20devices%20challenging%20due%20to%20memory%0Aand%20computational%20constraints.%20This%20paper%20introduces%20a%20novel%20approach%20to%20LLM%0Aweight%20pruning%20that%20directly%20optimizes%20for%20approximating%20the%20attention%20matrix%2C%0Aa%20core%20component%20of%20transformer%20architectures.%20Unlike%20existing%20methods%20that%0Afocus%20on%20linear%20approximations%2C%20our%20approach%20accounts%20for%20the%20non-linear%20nature%0Aof%20the%20Softmax%20attention%20mechanism.%20We%20provide%20theoretical%20guarantees%20for%20the%0Aconvergence%20of%20our%20Gradient%20Descent-based%20optimization%20method%20to%20a%20near-optimal%0Apruning%20mask%20solution.%20Our%20empirical%20results%20demonstrate%20the%20effectiveness%20of%0Aour%20non-linear%20pruning%20approach%20in%20maintaining%20model%20performance%20while%0Asignificantly%20reducing%20computational%20costs%2C%20which%20is%20beyond%20the%20current%0Astate-of-the-art%20methods%2C%20i.e.%2C%20SparseGPT%20and%20Wanda%2C%20by%20a%20large%20margin.%20This%0Awork%20establishes%20a%20new%20theoretical%20foundation%20for%20pruning%20algorithm%20design%20in%0ALLMs%2C%20potentially%20paving%20the%20way%20for%20more%20efficient%20LLM%20inference%20on%0Aresource-constrained%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11261v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Linear%2520Approximations%253A%2520A%2520Novel%2520Pruning%2520Approach%2520for%2520Attention%250A%2520%2520Matrix%26entry.906535625%3DYingyu%2520Liang%2520and%2520Jiangxuan%2520Long%2520and%2520Zhenmei%2520Shi%2520and%2520Zhao%2520Song%2520and%2520Yufa%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520immense%2520potential%2520in%2520enhancing%250Avarious%2520aspects%2520of%2520our%2520daily%2520lives%252C%2520from%2520conversational%2520AI%2520to%2520search%2520and%2520AI%250Aassistants.%2520However%252C%2520their%2520growing%2520capabilities%2520come%2520at%2520the%2520cost%2520of%2520extremely%250Alarge%2520model%2520sizes%252C%2520making%2520deployment%2520on%2520edge%2520devices%2520challenging%2520due%2520to%2520memory%250Aand%2520computational%2520constraints.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520LLM%250Aweight%2520pruning%2520that%2520directly%2520optimizes%2520for%2520approximating%2520the%2520attention%2520matrix%252C%250Aa%2520core%2520component%2520of%2520transformer%2520architectures.%2520Unlike%2520existing%2520methods%2520that%250Afocus%2520on%2520linear%2520approximations%252C%2520our%2520approach%2520accounts%2520for%2520the%2520non-linear%2520nature%250Aof%2520the%2520Softmax%2520attention%2520mechanism.%2520We%2520provide%2520theoretical%2520guarantees%2520for%2520the%250Aconvergence%2520of%2520our%2520Gradient%2520Descent-based%2520optimization%2520method%2520to%2520a%2520near-optimal%250Apruning%2520mask%2520solution.%2520Our%2520empirical%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520non-linear%2520pruning%2520approach%2520in%2520maintaining%2520model%2520performance%2520while%250Asignificantly%2520reducing%2520computational%2520costs%252C%2520which%2520is%2520beyond%2520the%2520current%250Astate-of-the-art%2520methods%252C%2520i.e.%252C%2520SparseGPT%2520and%2520Wanda%252C%2520by%2520a%2520large%2520margin.%2520This%250Awork%2520establishes%2520a%2520new%2520theoretical%2520foundation%2520for%2520pruning%2520algorithm%2520design%2520in%250ALLMs%252C%2520potentially%2520paving%2520the%2520way%2520for%2520more%2520efficient%2520LLM%2520inference%2520on%250Aresource-constrained%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11261v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Linear%20Approximations%3A%20A%20Novel%20Pruning%20Approach%20for%20Attention%0A%20%20Matrix&entry.906535625=Yingyu%20Liang%20and%20Jiangxuan%20Long%20and%20Zhenmei%20Shi%20and%20Zhao%20Song%20and%20Yufa%20Zhou&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20immense%20potential%20in%20enhancing%0Avarious%20aspects%20of%20our%20daily%20lives%2C%20from%20conversational%20AI%20to%20search%20and%20AI%0Aassistants.%20However%2C%20their%20growing%20capabilities%20come%20at%20the%20cost%20of%20extremely%0Alarge%20model%20sizes%2C%20making%20deployment%20on%20edge%20devices%20challenging%20due%20to%20memory%0Aand%20computational%20constraints.%20This%20paper%20introduces%20a%20novel%20approach%20to%20LLM%0Aweight%20pruning%20that%20directly%20optimizes%20for%20approximating%20the%20attention%20matrix%2C%0Aa%20core%20component%20of%20transformer%20architectures.%20Unlike%20existing%20methods%20that%0Afocus%20on%20linear%20approximations%2C%20our%20approach%20accounts%20for%20the%20non-linear%20nature%0Aof%20the%20Softmax%20attention%20mechanism.%20We%20provide%20theoretical%20guarantees%20for%20the%0Aconvergence%20of%20our%20Gradient%20Descent-based%20optimization%20method%20to%20a%20near-optimal%0Apruning%20mask%20solution.%20Our%20empirical%20results%20demonstrate%20the%20effectiveness%20of%0Aour%20non-linear%20pruning%20approach%20in%20maintaining%20model%20performance%20while%0Asignificantly%20reducing%20computational%20costs%2C%20which%20is%20beyond%20the%20current%0Astate-of-the-art%20methods%2C%20i.e.%2C%20SparseGPT%20and%20Wanda%2C%20by%20a%20large%20margin.%20This%0Awork%20establishes%20a%20new%20theoretical%20foundation%20for%20pruning%20algorithm%20design%20in%0ALLMs%2C%20potentially%20paving%20the%20way%20for%20more%20efficient%20LLM%20inference%20on%0Aresource-constrained%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11261v2&entry.124074799=Read"},
{"title": "HDEE: Heterogeneous Domain Expert Ensemble", "author": "O\u011fuzhan Ersoy and Jari Kolehmainen and Gabriel Passamani Andrade", "abstract": "  Training dense LLMs requires enormous amounts of data and centralized\ncompute, which introduces fundamental bottlenecks and ever-growing costs for\nlarge models. Several studies aim to reduce this dependency on centralization\nby reducing the communication overhead of training dense models. Taking this\nidea of reducing communication overhead to a natural extreme, by training\nembarrassingly parallelizable ensembles of small independent experts, has been\nshown to outperform large dense models trained in traditional centralized\nsettings. However, existing studies do not take into account underlying\ndifferences amongst data domains and treat them as monolithic, regardless of\ntheir underlying complexity, size, or distribution. In this paper, we explore\nthe effects of introducing heterogeneity to these ensembles of domain expert\nmodels. Specifically, by allowing models within the ensemble to vary in\nsize--as well as the number of training steps taken depending on the training\ndata's domain--we study the effect heterogeneity has on these ensembles when\nevaluated against domains included in, and excluded from, the training set. We\nuse the same compute budget to train heterogeneous ensembles and homogeneous\nbaselines for comparison. We show that the heterogeneous ensembles achieve the\nlowest perplexity scores in $20$ out of the $21$ data domains used in the\nevaluation. Our code is available at https://github.com/gensyn-ai/hdee.\n", "link": "http://arxiv.org/abs/2502.19385v1", "date": "2025-02-26", "relevancy": 2.0629, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5328}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HDEE%3A%20Heterogeneous%20Domain%20Expert%20Ensemble&body=Title%3A%20HDEE%3A%20Heterogeneous%20Domain%20Expert%20Ensemble%0AAuthor%3A%20O%C4%9Fuzhan%20Ersoy%20and%20Jari%20Kolehmainen%20and%20Gabriel%20Passamani%20Andrade%0AAbstract%3A%20%20%20Training%20dense%20LLMs%20requires%20enormous%20amounts%20of%20data%20and%20centralized%0Acompute%2C%20which%20introduces%20fundamental%20bottlenecks%20and%20ever-growing%20costs%20for%0Alarge%20models.%20Several%20studies%20aim%20to%20reduce%20this%20dependency%20on%20centralization%0Aby%20reducing%20the%20communication%20overhead%20of%20training%20dense%20models.%20Taking%20this%0Aidea%20of%20reducing%20communication%20overhead%20to%20a%20natural%20extreme%2C%20by%20training%0Aembarrassingly%20parallelizable%20ensembles%20of%20small%20independent%20experts%2C%20has%20been%0Ashown%20to%20outperform%20large%20dense%20models%20trained%20in%20traditional%20centralized%0Asettings.%20However%2C%20existing%20studies%20do%20not%20take%20into%20account%20underlying%0Adifferences%20amongst%20data%20domains%20and%20treat%20them%20as%20monolithic%2C%20regardless%20of%0Atheir%20underlying%20complexity%2C%20size%2C%20or%20distribution.%20In%20this%20paper%2C%20we%20explore%0Athe%20effects%20of%20introducing%20heterogeneity%20to%20these%20ensembles%20of%20domain%20expert%0Amodels.%20Specifically%2C%20by%20allowing%20models%20within%20the%20ensemble%20to%20vary%20in%0Asize--as%20well%20as%20the%20number%20of%20training%20steps%20taken%20depending%20on%20the%20training%0Adata%27s%20domain--we%20study%20the%20effect%20heterogeneity%20has%20on%20these%20ensembles%20when%0Aevaluated%20against%20domains%20included%20in%2C%20and%20excluded%20from%2C%20the%20training%20set.%20We%0Ause%20the%20same%20compute%20budget%20to%20train%20heterogeneous%20ensembles%20and%20homogeneous%0Abaselines%20for%20comparison.%20We%20show%20that%20the%20heterogeneous%20ensembles%20achieve%20the%0Alowest%20perplexity%20scores%20in%20%2420%24%20out%20of%20the%20%2421%24%20data%20domains%20used%20in%20the%0Aevaluation.%20Our%20code%20is%20available%20at%20https%3A//github.com/gensyn-ai/hdee.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHDEE%253A%2520Heterogeneous%2520Domain%2520Expert%2520Ensemble%26entry.906535625%3DO%25C4%259Fuzhan%2520Ersoy%2520and%2520Jari%2520Kolehmainen%2520and%2520Gabriel%2520Passamani%2520Andrade%26entry.1292438233%3D%2520%2520Training%2520dense%2520LLMs%2520requires%2520enormous%2520amounts%2520of%2520data%2520and%2520centralized%250Acompute%252C%2520which%2520introduces%2520fundamental%2520bottlenecks%2520and%2520ever-growing%2520costs%2520for%250Alarge%2520models.%2520Several%2520studies%2520aim%2520to%2520reduce%2520this%2520dependency%2520on%2520centralization%250Aby%2520reducing%2520the%2520communication%2520overhead%2520of%2520training%2520dense%2520models.%2520Taking%2520this%250Aidea%2520of%2520reducing%2520communication%2520overhead%2520to%2520a%2520natural%2520extreme%252C%2520by%2520training%250Aembarrassingly%2520parallelizable%2520ensembles%2520of%2520small%2520independent%2520experts%252C%2520has%2520been%250Ashown%2520to%2520outperform%2520large%2520dense%2520models%2520trained%2520in%2520traditional%2520centralized%250Asettings.%2520However%252C%2520existing%2520studies%2520do%2520not%2520take%2520into%2520account%2520underlying%250Adifferences%2520amongst%2520data%2520domains%2520and%2520treat%2520them%2520as%2520monolithic%252C%2520regardless%2520of%250Atheir%2520underlying%2520complexity%252C%2520size%252C%2520or%2520distribution.%2520In%2520this%2520paper%252C%2520we%2520explore%250Athe%2520effects%2520of%2520introducing%2520heterogeneity%2520to%2520these%2520ensembles%2520of%2520domain%2520expert%250Amodels.%2520Specifically%252C%2520by%2520allowing%2520models%2520within%2520the%2520ensemble%2520to%2520vary%2520in%250Asize--as%2520well%2520as%2520the%2520number%2520of%2520training%2520steps%2520taken%2520depending%2520on%2520the%2520training%250Adata%2527s%2520domain--we%2520study%2520the%2520effect%2520heterogeneity%2520has%2520on%2520these%2520ensembles%2520when%250Aevaluated%2520against%2520domains%2520included%2520in%252C%2520and%2520excluded%2520from%252C%2520the%2520training%2520set.%2520We%250Ause%2520the%2520same%2520compute%2520budget%2520to%2520train%2520heterogeneous%2520ensembles%2520and%2520homogeneous%250Abaselines%2520for%2520comparison.%2520We%2520show%2520that%2520the%2520heterogeneous%2520ensembles%2520achieve%2520the%250Alowest%2520perplexity%2520scores%2520in%2520%252420%2524%2520out%2520of%2520the%2520%252421%2524%2520data%2520domains%2520used%2520in%2520the%250Aevaluation.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/gensyn-ai/hdee.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HDEE%3A%20Heterogeneous%20Domain%20Expert%20Ensemble&entry.906535625=O%C4%9Fuzhan%20Ersoy%20and%20Jari%20Kolehmainen%20and%20Gabriel%20Passamani%20Andrade&entry.1292438233=%20%20Training%20dense%20LLMs%20requires%20enormous%20amounts%20of%20data%20and%20centralized%0Acompute%2C%20which%20introduces%20fundamental%20bottlenecks%20and%20ever-growing%20costs%20for%0Alarge%20models.%20Several%20studies%20aim%20to%20reduce%20this%20dependency%20on%20centralization%0Aby%20reducing%20the%20communication%20overhead%20of%20training%20dense%20models.%20Taking%20this%0Aidea%20of%20reducing%20communication%20overhead%20to%20a%20natural%20extreme%2C%20by%20training%0Aembarrassingly%20parallelizable%20ensembles%20of%20small%20independent%20experts%2C%20has%20been%0Ashown%20to%20outperform%20large%20dense%20models%20trained%20in%20traditional%20centralized%0Asettings.%20However%2C%20existing%20studies%20do%20not%20take%20into%20account%20underlying%0Adifferences%20amongst%20data%20domains%20and%20treat%20them%20as%20monolithic%2C%20regardless%20of%0Atheir%20underlying%20complexity%2C%20size%2C%20or%20distribution.%20In%20this%20paper%2C%20we%20explore%0Athe%20effects%20of%20introducing%20heterogeneity%20to%20these%20ensembles%20of%20domain%20expert%0Amodels.%20Specifically%2C%20by%20allowing%20models%20within%20the%20ensemble%20to%20vary%20in%0Asize--as%20well%20as%20the%20number%20of%20training%20steps%20taken%20depending%20on%20the%20training%0Adata%27s%20domain--we%20study%20the%20effect%20heterogeneity%20has%20on%20these%20ensembles%20when%0Aevaluated%20against%20domains%20included%20in%2C%20and%20excluded%20from%2C%20the%20training%20set.%20We%0Ause%20the%20same%20compute%20budget%20to%20train%20heterogeneous%20ensembles%20and%20homogeneous%0Abaselines%20for%20comparison.%20We%20show%20that%20the%20heterogeneous%20ensembles%20achieve%20the%0Alowest%20perplexity%20scores%20in%20%2420%24%20out%20of%20the%20%2421%24%20data%20domains%20used%20in%20the%0Aevaluation.%20Our%20code%20is%20available%20at%20https%3A//github.com/gensyn-ai/hdee.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19385v1&entry.124074799=Read"},
{"title": "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model", "author": "Vijay Kumar Sutrakar and Anjana PK and Rohit Bisariya and Soumya KK and Gopal Chawan M", "abstract": "  In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.\n", "link": "http://arxiv.org/abs/2502.19164v1", "date": "2025-02-26", "relevancy": 2.0591, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4118}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20of%20Cavity%20Backed%20Slotted%20Antenna%20using%20Machine%20Learning%0A%20%20Regression%20Model&body=Title%3A%20Design%20of%20Cavity%20Backed%20Slotted%20Antenna%20using%20Machine%20Learning%0A%20%20Regression%20Model%0AAuthor%3A%20Vijay%20Kumar%20Sutrakar%20and%20Anjana%20PK%20and%20Rohit%20Bisariya%20and%20Soumya%20KK%20and%20Gopal%20Chawan%20M%0AAbstract%3A%20%20%20In%20this%20paper%2C%20a%20regression-based%20machine%20learning%20model%20is%20used%20for%20the%0Adesign%20of%20cavity%20backed%20slotted%20antenna.%20This%20type%20of%20antenna%20is%20commonly%20used%0Ain%20military%20and%20aviation%20communication%20systems.%20Initial%20reflection%20coefficient%0Adata%20of%20cavity%20backed%20slotted%20antenna%20is%20generated%20using%20electromagnetic%0Asolver.%20These%20reflection%20coefficient%20data%20is%20then%20used%20as%20input%20for%20training%0Aregression-based%20machine%20learning%20model.%20The%20model%20is%20trained%20to%20predict%20the%0Adimensions%20of%20cavity%20backed%20slotted%20antenna%20based%20on%20the%20input%20reflection%0Acoefficient%20for%20a%20wide%20frequency%20band%20varying%20from%201%20GHz%20to%208%20GHz.%20This%0Aapproach%20allows%20for%20rapid%20prediction%20of%20optimal%20antenna%20configurations%2C%0Areducing%20the%20need%20for%20repeated%20physical%20testing%20and%20manual%20adjustments%2C%20may%0Alead%20to%20significant%20amount%20of%20design%20and%20development%20cost%20saving.%20The%20proposed%0Amodel%20also%20demonstrates%20its%20versatility%20in%20predicting%20multi%20frequency%20resonance%0Aacross%201%20GHz%20to%208%20GHz.%20Also%2C%20the%20proposed%20approach%20demonstrates%20the%20potential%0Afor%20leveraging%20machine%20learning%20in%20advanced%20antenna%20design%2C%20enhancing%0Aefficiency%20and%20accuracy%20in%20practical%20applications%20such%20as%20radar%2C%20military%0Aidentification%20systems%20and%20secure%20communication%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520of%2520Cavity%2520Backed%2520Slotted%2520Antenna%2520using%2520Machine%2520Learning%250A%2520%2520Regression%2520Model%26entry.906535625%3DVijay%2520Kumar%2520Sutrakar%2520and%2520Anjana%2520PK%2520and%2520Rohit%2520Bisariya%2520and%2520Soumya%2520KK%2520and%2520Gopal%2520Chawan%2520M%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520a%2520regression-based%2520machine%2520learning%2520model%2520is%2520used%2520for%2520the%250Adesign%2520of%2520cavity%2520backed%2520slotted%2520antenna.%2520This%2520type%2520of%2520antenna%2520is%2520commonly%2520used%250Ain%2520military%2520and%2520aviation%2520communication%2520systems.%2520Initial%2520reflection%2520coefficient%250Adata%2520of%2520cavity%2520backed%2520slotted%2520antenna%2520is%2520generated%2520using%2520electromagnetic%250Asolver.%2520These%2520reflection%2520coefficient%2520data%2520is%2520then%2520used%2520as%2520input%2520for%2520training%250Aregression-based%2520machine%2520learning%2520model.%2520The%2520model%2520is%2520trained%2520to%2520predict%2520the%250Adimensions%2520of%2520cavity%2520backed%2520slotted%2520antenna%2520based%2520on%2520the%2520input%2520reflection%250Acoefficient%2520for%2520a%2520wide%2520frequency%2520band%2520varying%2520from%25201%2520GHz%2520to%25208%2520GHz.%2520This%250Aapproach%2520allows%2520for%2520rapid%2520prediction%2520of%2520optimal%2520antenna%2520configurations%252C%250Areducing%2520the%2520need%2520for%2520repeated%2520physical%2520testing%2520and%2520manual%2520adjustments%252C%2520may%250Alead%2520to%2520significant%2520amount%2520of%2520design%2520and%2520development%2520cost%2520saving.%2520The%2520proposed%250Amodel%2520also%2520demonstrates%2520its%2520versatility%2520in%2520predicting%2520multi%2520frequency%2520resonance%250Aacross%25201%2520GHz%2520to%25208%2520GHz.%2520Also%252C%2520the%2520proposed%2520approach%2520demonstrates%2520the%2520potential%250Afor%2520leveraging%2520machine%2520learning%2520in%2520advanced%2520antenna%2520design%252C%2520enhancing%250Aefficiency%2520and%2520accuracy%2520in%2520practical%2520applications%2520such%2520as%2520radar%252C%2520military%250Aidentification%2520systems%2520and%2520secure%2520communication%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20of%20Cavity%20Backed%20Slotted%20Antenna%20using%20Machine%20Learning%0A%20%20Regression%20Model&entry.906535625=Vijay%20Kumar%20Sutrakar%20and%20Anjana%20PK%20and%20Rohit%20Bisariya%20and%20Soumya%20KK%20and%20Gopal%20Chawan%20M&entry.1292438233=%20%20In%20this%20paper%2C%20a%20regression-based%20machine%20learning%20model%20is%20used%20for%20the%0Adesign%20of%20cavity%20backed%20slotted%20antenna.%20This%20type%20of%20antenna%20is%20commonly%20used%0Ain%20military%20and%20aviation%20communication%20systems.%20Initial%20reflection%20coefficient%0Adata%20of%20cavity%20backed%20slotted%20antenna%20is%20generated%20using%20electromagnetic%0Asolver.%20These%20reflection%20coefficient%20data%20is%20then%20used%20as%20input%20for%20training%0Aregression-based%20machine%20learning%20model.%20The%20model%20is%20trained%20to%20predict%20the%0Adimensions%20of%20cavity%20backed%20slotted%20antenna%20based%20on%20the%20input%20reflection%0Acoefficient%20for%20a%20wide%20frequency%20band%20varying%20from%201%20GHz%20to%208%20GHz.%20This%0Aapproach%20allows%20for%20rapid%20prediction%20of%20optimal%20antenna%20configurations%2C%0Areducing%20the%20need%20for%20repeated%20physical%20testing%20and%20manual%20adjustments%2C%20may%0Alead%20to%20significant%20amount%20of%20design%20and%20development%20cost%20saving.%20The%20proposed%0Amodel%20also%20demonstrates%20its%20versatility%20in%20predicting%20multi%20frequency%20resonance%0Aacross%201%20GHz%20to%208%20GHz.%20Also%2C%20the%20proposed%20approach%20demonstrates%20the%20potential%0Afor%20leveraging%20machine%20learning%20in%20advanced%20antenna%20design%2C%20enhancing%0Aefficiency%20and%20accuracy%20in%20practical%20applications%20such%20as%20radar%2C%20military%0Aidentification%20systems%20and%20secure%20communication%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19164v1&entry.124074799=Read"},
{"title": "Enhancing Gradient-based Discrete Sampling via Parallel Tempering", "author": "Luxu Liang and Yuhang Jia and Feng Zhou", "abstract": "  While gradient-based discrete samplers are effective in sampling from complex\ndistributions, they are susceptible to getting trapped in local minima,\nparticularly in high-dimensional, multimodal discrete distributions, owing to\nthe discontinuities inherent in these landscapes. To circumvent this issue, we\ncombine parallel tempering, also known as replica exchange, with the discrete\nLangevin proposal and develop the Parallel Tempering enhanced Discrete Langevin\nProposal (PTDLP), which are simulated at a series of temperatures. Significant\nenergy differences prompt sample swaps, which are governed by a Metropolis\ncriterion specifically designed for discrete sampling to ensure detailed\nbalance is maintained. Additionally, we introduce an automatic scheme to\ndetermine the optimal temperature schedule and the number of chains, ensuring\nadaptability across diverse tasks with minimal tuning. Theoretically, we\nestablish that our algorithm converges non-asymptotically to the target energy\nand exhibits faster mixing compared to a single chain. Empirical results\nfurther emphasize the superiority of our method in sampling from complex,\nmultimodal discrete distributions, including synthetic problems, restricted\nBoltzmann machines, and deep energy-based models.\n", "link": "http://arxiv.org/abs/2502.19240v1", "date": "2025-02-26", "relevancy": 2.0582, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5203}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5171}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Gradient-based%20Discrete%20Sampling%20via%20Parallel%20Tempering&body=Title%3A%20Enhancing%20Gradient-based%20Discrete%20Sampling%20via%20Parallel%20Tempering%0AAuthor%3A%20Luxu%20Liang%20and%20Yuhang%20Jia%20and%20Feng%20Zhou%0AAbstract%3A%20%20%20While%20gradient-based%20discrete%20samplers%20are%20effective%20in%20sampling%20from%20complex%0Adistributions%2C%20they%20are%20susceptible%20to%20getting%20trapped%20in%20local%20minima%2C%0Aparticularly%20in%20high-dimensional%2C%20multimodal%20discrete%20distributions%2C%20owing%20to%0Athe%20discontinuities%20inherent%20in%20these%20landscapes.%20To%20circumvent%20this%20issue%2C%20we%0Acombine%20parallel%20tempering%2C%20also%20known%20as%20replica%20exchange%2C%20with%20the%20discrete%0ALangevin%20proposal%20and%20develop%20the%20Parallel%20Tempering%20enhanced%20Discrete%20Langevin%0AProposal%20%28PTDLP%29%2C%20which%20are%20simulated%20at%20a%20series%20of%20temperatures.%20Significant%0Aenergy%20differences%20prompt%20sample%20swaps%2C%20which%20are%20governed%20by%20a%20Metropolis%0Acriterion%20specifically%20designed%20for%20discrete%20sampling%20to%20ensure%20detailed%0Abalance%20is%20maintained.%20Additionally%2C%20we%20introduce%20an%20automatic%20scheme%20to%0Adetermine%20the%20optimal%20temperature%20schedule%20and%20the%20number%20of%20chains%2C%20ensuring%0Aadaptability%20across%20diverse%20tasks%20with%20minimal%20tuning.%20Theoretically%2C%20we%0Aestablish%20that%20our%20algorithm%20converges%20non-asymptotically%20to%20the%20target%20energy%0Aand%20exhibits%20faster%20mixing%20compared%20to%20a%20single%20chain.%20Empirical%20results%0Afurther%20emphasize%20the%20superiority%20of%20our%20method%20in%20sampling%20from%20complex%2C%0Amultimodal%20discrete%20distributions%2C%20including%20synthetic%20problems%2C%20restricted%0ABoltzmann%20machines%2C%20and%20deep%20energy-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Gradient-based%2520Discrete%2520Sampling%2520via%2520Parallel%2520Tempering%26entry.906535625%3DLuxu%2520Liang%2520and%2520Yuhang%2520Jia%2520and%2520Feng%2520Zhou%26entry.1292438233%3D%2520%2520While%2520gradient-based%2520discrete%2520samplers%2520are%2520effective%2520in%2520sampling%2520from%2520complex%250Adistributions%252C%2520they%2520are%2520susceptible%2520to%2520getting%2520trapped%2520in%2520local%2520minima%252C%250Aparticularly%2520in%2520high-dimensional%252C%2520multimodal%2520discrete%2520distributions%252C%2520owing%2520to%250Athe%2520discontinuities%2520inherent%2520in%2520these%2520landscapes.%2520To%2520circumvent%2520this%2520issue%252C%2520we%250Acombine%2520parallel%2520tempering%252C%2520also%2520known%2520as%2520replica%2520exchange%252C%2520with%2520the%2520discrete%250ALangevin%2520proposal%2520and%2520develop%2520the%2520Parallel%2520Tempering%2520enhanced%2520Discrete%2520Langevin%250AProposal%2520%2528PTDLP%2529%252C%2520which%2520are%2520simulated%2520at%2520a%2520series%2520of%2520temperatures.%2520Significant%250Aenergy%2520differences%2520prompt%2520sample%2520swaps%252C%2520which%2520are%2520governed%2520by%2520a%2520Metropolis%250Acriterion%2520specifically%2520designed%2520for%2520discrete%2520sampling%2520to%2520ensure%2520detailed%250Abalance%2520is%2520maintained.%2520Additionally%252C%2520we%2520introduce%2520an%2520automatic%2520scheme%2520to%250Adetermine%2520the%2520optimal%2520temperature%2520schedule%2520and%2520the%2520number%2520of%2520chains%252C%2520ensuring%250Aadaptability%2520across%2520diverse%2520tasks%2520with%2520minimal%2520tuning.%2520Theoretically%252C%2520we%250Aestablish%2520that%2520our%2520algorithm%2520converges%2520non-asymptotically%2520to%2520the%2520target%2520energy%250Aand%2520exhibits%2520faster%2520mixing%2520compared%2520to%2520a%2520single%2520chain.%2520Empirical%2520results%250Afurther%2520emphasize%2520the%2520superiority%2520of%2520our%2520method%2520in%2520sampling%2520from%2520complex%252C%250Amultimodal%2520discrete%2520distributions%252C%2520including%2520synthetic%2520problems%252C%2520restricted%250ABoltzmann%2520machines%252C%2520and%2520deep%2520energy-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Gradient-based%20Discrete%20Sampling%20via%20Parallel%20Tempering&entry.906535625=Luxu%20Liang%20and%20Yuhang%20Jia%20and%20Feng%20Zhou&entry.1292438233=%20%20While%20gradient-based%20discrete%20samplers%20are%20effective%20in%20sampling%20from%20complex%0Adistributions%2C%20they%20are%20susceptible%20to%20getting%20trapped%20in%20local%20minima%2C%0Aparticularly%20in%20high-dimensional%2C%20multimodal%20discrete%20distributions%2C%20owing%20to%0Athe%20discontinuities%20inherent%20in%20these%20landscapes.%20To%20circumvent%20this%20issue%2C%20we%0Acombine%20parallel%20tempering%2C%20also%20known%20as%20replica%20exchange%2C%20with%20the%20discrete%0ALangevin%20proposal%20and%20develop%20the%20Parallel%20Tempering%20enhanced%20Discrete%20Langevin%0AProposal%20%28PTDLP%29%2C%20which%20are%20simulated%20at%20a%20series%20of%20temperatures.%20Significant%0Aenergy%20differences%20prompt%20sample%20swaps%2C%20which%20are%20governed%20by%20a%20Metropolis%0Acriterion%20specifically%20designed%20for%20discrete%20sampling%20to%20ensure%20detailed%0Abalance%20is%20maintained.%20Additionally%2C%20we%20introduce%20an%20automatic%20scheme%20to%0Adetermine%20the%20optimal%20temperature%20schedule%20and%20the%20number%20of%20chains%2C%20ensuring%0Aadaptability%20across%20diverse%20tasks%20with%20minimal%20tuning.%20Theoretically%2C%20we%0Aestablish%20that%20our%20algorithm%20converges%20non-asymptotically%20to%20the%20target%20energy%0Aand%20exhibits%20faster%20mixing%20compared%20to%20a%20single%20chain.%20Empirical%20results%0Afurther%20emphasize%20the%20superiority%20of%20our%20method%20in%20sampling%20from%20complex%2C%0Amultimodal%20discrete%20distributions%2C%20including%20synthetic%20problems%2C%20restricted%0ABoltzmann%20machines%2C%20and%20deep%20energy-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19240v1&entry.124074799=Read"},
{"title": "TestNUC: Enhancing Test-Time Computing Approaches through Neighboring\n  Unlabeled Data Consistency", "author": "Henry Peng Zou and Zhengyao Gu and Yue Zhou and Yankai Chen and Weizhi Zhang and Liancheng Fang and Yibo Wang and Yangning Li and Kay Liu and Philip S. Yu", "abstract": "  Test-time computing approaches, which leverage additional computational\nresources during inference, have been proven effective in enhancing large\nlanguage model performance. This work introduces a novel, linearly scaling\napproach, TestNUC, that improves test-time predictions by leveraging the local\nconsistency of neighboring unlabeled data-it classifies an input instance by\nconsidering not only the model's prediction on that instance but also on\nneighboring unlabeled instances. We evaluate TestNUC across eight diverse\ndatasets, spanning intent classification, topic mining, domain discovery, and\nemotion detection, demonstrating its consistent superiority over baseline\nmethods such as standard prompting and self-consistency. Furthermore, TestNUC\ncan be seamlessly integrated with existing test-time computing approaches,\nsubstantially boosting their performance. Our analysis reveals that TestNUC\nscales effectively with increasing amounts of unlabeled data and performs\nrobustly across different embedding models, making it practical for real-world\napplications. Our code is available at https://github.com/HenryPengZou/TestNUC.\n", "link": "http://arxiv.org/abs/2502.19163v1", "date": "2025-02-26", "relevancy": 2.0549, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5345}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5153}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TestNUC%3A%20Enhancing%20Test-Time%20Computing%20Approaches%20through%20Neighboring%0A%20%20Unlabeled%20Data%20Consistency&body=Title%3A%20TestNUC%3A%20Enhancing%20Test-Time%20Computing%20Approaches%20through%20Neighboring%0A%20%20Unlabeled%20Data%20Consistency%0AAuthor%3A%20Henry%20Peng%20Zou%20and%20Zhengyao%20Gu%20and%20Yue%20Zhou%20and%20Yankai%20Chen%20and%20Weizhi%20Zhang%20and%20Liancheng%20Fang%20and%20Yibo%20Wang%20and%20Yangning%20Li%20and%20Kay%20Liu%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20Test-time%20computing%20approaches%2C%20which%20leverage%20additional%20computational%0Aresources%20during%20inference%2C%20have%20been%20proven%20effective%20in%20enhancing%20large%0Alanguage%20model%20performance.%20This%20work%20introduces%20a%20novel%2C%20linearly%20scaling%0Aapproach%2C%20TestNUC%2C%20that%20improves%20test-time%20predictions%20by%20leveraging%20the%20local%0Aconsistency%20of%20neighboring%20unlabeled%20data-it%20classifies%20an%20input%20instance%20by%0Aconsidering%20not%20only%20the%20model%27s%20prediction%20on%20that%20instance%20but%20also%20on%0Aneighboring%20unlabeled%20instances.%20We%20evaluate%20TestNUC%20across%20eight%20diverse%0Adatasets%2C%20spanning%20intent%20classification%2C%20topic%20mining%2C%20domain%20discovery%2C%20and%0Aemotion%20detection%2C%20demonstrating%20its%20consistent%20superiority%20over%20baseline%0Amethods%20such%20as%20standard%20prompting%20and%20self-consistency.%20Furthermore%2C%20TestNUC%0Acan%20be%20seamlessly%20integrated%20with%20existing%20test-time%20computing%20approaches%2C%0Asubstantially%20boosting%20their%20performance.%20Our%20analysis%20reveals%20that%20TestNUC%0Ascales%20effectively%20with%20increasing%20amounts%20of%20unlabeled%20data%20and%20performs%0Arobustly%20across%20different%20embedding%20models%2C%20making%20it%20practical%20for%20real-world%0Aapplications.%20Our%20code%20is%20available%20at%20https%3A//github.com/HenryPengZou/TestNUC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTestNUC%253A%2520Enhancing%2520Test-Time%2520Computing%2520Approaches%2520through%2520Neighboring%250A%2520%2520Unlabeled%2520Data%2520Consistency%26entry.906535625%3DHenry%2520Peng%2520Zou%2520and%2520Zhengyao%2520Gu%2520and%2520Yue%2520Zhou%2520and%2520Yankai%2520Chen%2520and%2520Weizhi%2520Zhang%2520and%2520Liancheng%2520Fang%2520and%2520Yibo%2520Wang%2520and%2520Yangning%2520Li%2520and%2520Kay%2520Liu%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520Test-time%2520computing%2520approaches%252C%2520which%2520leverage%2520additional%2520computational%250Aresources%2520during%2520inference%252C%2520have%2520been%2520proven%2520effective%2520in%2520enhancing%2520large%250Alanguage%2520model%2520performance.%2520This%2520work%2520introduces%2520a%2520novel%252C%2520linearly%2520scaling%250Aapproach%252C%2520TestNUC%252C%2520that%2520improves%2520test-time%2520predictions%2520by%2520leveraging%2520the%2520local%250Aconsistency%2520of%2520neighboring%2520unlabeled%2520data-it%2520classifies%2520an%2520input%2520instance%2520by%250Aconsidering%2520not%2520only%2520the%2520model%2527s%2520prediction%2520on%2520that%2520instance%2520but%2520also%2520on%250Aneighboring%2520unlabeled%2520instances.%2520We%2520evaluate%2520TestNUC%2520across%2520eight%2520diverse%250Adatasets%252C%2520spanning%2520intent%2520classification%252C%2520topic%2520mining%252C%2520domain%2520discovery%252C%2520and%250Aemotion%2520detection%252C%2520demonstrating%2520its%2520consistent%2520superiority%2520over%2520baseline%250Amethods%2520such%2520as%2520standard%2520prompting%2520and%2520self-consistency.%2520Furthermore%252C%2520TestNUC%250Acan%2520be%2520seamlessly%2520integrated%2520with%2520existing%2520test-time%2520computing%2520approaches%252C%250Asubstantially%2520boosting%2520their%2520performance.%2520Our%2520analysis%2520reveals%2520that%2520TestNUC%250Ascales%2520effectively%2520with%2520increasing%2520amounts%2520of%2520unlabeled%2520data%2520and%2520performs%250Arobustly%2520across%2520different%2520embedding%2520models%252C%2520making%2520it%2520practical%2520for%2520real-world%250Aapplications.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/HenryPengZou/TestNUC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TestNUC%3A%20Enhancing%20Test-Time%20Computing%20Approaches%20through%20Neighboring%0A%20%20Unlabeled%20Data%20Consistency&entry.906535625=Henry%20Peng%20Zou%20and%20Zhengyao%20Gu%20and%20Yue%20Zhou%20and%20Yankai%20Chen%20and%20Weizhi%20Zhang%20and%20Liancheng%20Fang%20and%20Yibo%20Wang%20and%20Yangning%20Li%20and%20Kay%20Liu%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20Test-time%20computing%20approaches%2C%20which%20leverage%20additional%20computational%0Aresources%20during%20inference%2C%20have%20been%20proven%20effective%20in%20enhancing%20large%0Alanguage%20model%20performance.%20This%20work%20introduces%20a%20novel%2C%20linearly%20scaling%0Aapproach%2C%20TestNUC%2C%20that%20improves%20test-time%20predictions%20by%20leveraging%20the%20local%0Aconsistency%20of%20neighboring%20unlabeled%20data-it%20classifies%20an%20input%20instance%20by%0Aconsidering%20not%20only%20the%20model%27s%20prediction%20on%20that%20instance%20but%20also%20on%0Aneighboring%20unlabeled%20instances.%20We%20evaluate%20TestNUC%20across%20eight%20diverse%0Adatasets%2C%20spanning%20intent%20classification%2C%20topic%20mining%2C%20domain%20discovery%2C%20and%0Aemotion%20detection%2C%20demonstrating%20its%20consistent%20superiority%20over%20baseline%0Amethods%20such%20as%20standard%20prompting%20and%20self-consistency.%20Furthermore%2C%20TestNUC%0Acan%20be%20seamlessly%20integrated%20with%20existing%20test-time%20computing%20approaches%2C%0Asubstantially%20boosting%20their%20performance.%20Our%20analysis%20reveals%20that%20TestNUC%0Ascales%20effectively%20with%20increasing%20amounts%20of%20unlabeled%20data%20and%20performs%0Arobustly%20across%20different%20embedding%20models%2C%20making%20it%20practical%20for%20real-world%0Aapplications.%20Our%20code%20is%20available%20at%20https%3A//github.com/HenryPengZou/TestNUC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19163v1&entry.124074799=Read"},
{"title": "Espresso: Robust Concept Filtering in Text-to-Image Models", "author": "Anudeep Das and Vasisht Duddu and Rui Zhang and N. Asokan", "abstract": "  Diffusion based text-to-image models are trained on large datasets scraped\nfrom the Internet, potentially containing unacceptable concepts (e.g.,\ncopyright-infringing or unsafe). We need concept removal techniques (CRTs)\nwhich are i) effective in preventing the generation of images with unacceptable\nconcepts, ii) utility-preserving on acceptable concepts, and, iii) robust\nagainst evasion with adversarial prompts. No prior CRT satisfies all these\nrequirements simultaneously. We introduce Espresso, the first robust concept\nfilter based on Contrastive Language-Image Pre-Training (CLIP). We identify\nunacceptable concepts by using the distance between the embedding of a\ngenerated image to the text embeddings of both unacceptable and acceptable\nconcepts. This lets us fine-tune for robustness by separating the text\nembeddings of unacceptable and acceptable concepts while preserving utility. We\npresent a pipeline to evaluate various CRTs to show that Espresso is more\neffective and robust than prior CRTs, while retaining utility.\n", "link": "http://arxiv.org/abs/2404.19227v7", "date": "2025-02-26", "relevancy": 2.0499, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Espresso%3A%20Robust%20Concept%20Filtering%20in%20Text-to-Image%20Models&body=Title%3A%20Espresso%3A%20Robust%20Concept%20Filtering%20in%20Text-to-Image%20Models%0AAuthor%3A%20Anudeep%20Das%20and%20Vasisht%20Duddu%20and%20Rui%20Zhang%20and%20N.%20Asokan%0AAbstract%3A%20%20%20Diffusion%20based%20text-to-image%20models%20are%20trained%20on%20large%20datasets%20scraped%0Afrom%20the%20Internet%2C%20potentially%20containing%20unacceptable%20concepts%20%28e.g.%2C%0Acopyright-infringing%20or%20unsafe%29.%20We%20need%20concept%20removal%20techniques%20%28CRTs%29%0Awhich%20are%20i%29%20effective%20in%20preventing%20the%20generation%20of%20images%20with%20unacceptable%0Aconcepts%2C%20ii%29%20utility-preserving%20on%20acceptable%20concepts%2C%20and%2C%20iii%29%20robust%0Aagainst%20evasion%20with%20adversarial%20prompts.%20No%20prior%20CRT%20satisfies%20all%20these%0Arequirements%20simultaneously.%20We%20introduce%20Espresso%2C%20the%20first%20robust%20concept%0Afilter%20based%20on%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29.%20We%20identify%0Aunacceptable%20concepts%20by%20using%20the%20distance%20between%20the%20embedding%20of%20a%0Agenerated%20image%20to%20the%20text%20embeddings%20of%20both%20unacceptable%20and%20acceptable%0Aconcepts.%20This%20lets%20us%20fine-tune%20for%20robustness%20by%20separating%20the%20text%0Aembeddings%20of%20unacceptable%20and%20acceptable%20concepts%20while%20preserving%20utility.%20We%0Apresent%20a%20pipeline%20to%20evaluate%20various%20CRTs%20to%20show%20that%20Espresso%20is%20more%0Aeffective%20and%20robust%20than%20prior%20CRTs%2C%20while%20retaining%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19227v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEspresso%253A%2520Robust%2520Concept%2520Filtering%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DAnudeep%2520Das%2520and%2520Vasisht%2520Duddu%2520and%2520Rui%2520Zhang%2520and%2520N.%2520Asokan%26entry.1292438233%3D%2520%2520Diffusion%2520based%2520text-to-image%2520models%2520are%2520trained%2520on%2520large%2520datasets%2520scraped%250Afrom%2520the%2520Internet%252C%2520potentially%2520containing%2520unacceptable%2520concepts%2520%2528e.g.%252C%250Acopyright-infringing%2520or%2520unsafe%2529.%2520We%2520need%2520concept%2520removal%2520techniques%2520%2528CRTs%2529%250Awhich%2520are%2520i%2529%2520effective%2520in%2520preventing%2520the%2520generation%2520of%2520images%2520with%2520unacceptable%250Aconcepts%252C%2520ii%2529%2520utility-preserving%2520on%2520acceptable%2520concepts%252C%2520and%252C%2520iii%2529%2520robust%250Aagainst%2520evasion%2520with%2520adversarial%2520prompts.%2520No%2520prior%2520CRT%2520satisfies%2520all%2520these%250Arequirements%2520simultaneously.%2520We%2520introduce%2520Espresso%252C%2520the%2520first%2520robust%2520concept%250Afilter%2520based%2520on%2520Contrastive%2520Language-Image%2520Pre-Training%2520%2528CLIP%2529.%2520We%2520identify%250Aunacceptable%2520concepts%2520by%2520using%2520the%2520distance%2520between%2520the%2520embedding%2520of%2520a%250Agenerated%2520image%2520to%2520the%2520text%2520embeddings%2520of%2520both%2520unacceptable%2520and%2520acceptable%250Aconcepts.%2520This%2520lets%2520us%2520fine-tune%2520for%2520robustness%2520by%2520separating%2520the%2520text%250Aembeddings%2520of%2520unacceptable%2520and%2520acceptable%2520concepts%2520while%2520preserving%2520utility.%2520We%250Apresent%2520a%2520pipeline%2520to%2520evaluate%2520various%2520CRTs%2520to%2520show%2520that%2520Espresso%2520is%2520more%250Aeffective%2520and%2520robust%2520than%2520prior%2520CRTs%252C%2520while%2520retaining%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19227v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Espresso%3A%20Robust%20Concept%20Filtering%20in%20Text-to-Image%20Models&entry.906535625=Anudeep%20Das%20and%20Vasisht%20Duddu%20and%20Rui%20Zhang%20and%20N.%20Asokan&entry.1292438233=%20%20Diffusion%20based%20text-to-image%20models%20are%20trained%20on%20large%20datasets%20scraped%0Afrom%20the%20Internet%2C%20potentially%20containing%20unacceptable%20concepts%20%28e.g.%2C%0Acopyright-infringing%20or%20unsafe%29.%20We%20need%20concept%20removal%20techniques%20%28CRTs%29%0Awhich%20are%20i%29%20effective%20in%20preventing%20the%20generation%20of%20images%20with%20unacceptable%0Aconcepts%2C%20ii%29%20utility-preserving%20on%20acceptable%20concepts%2C%20and%2C%20iii%29%20robust%0Aagainst%20evasion%20with%20adversarial%20prompts.%20No%20prior%20CRT%20satisfies%20all%20these%0Arequirements%20simultaneously.%20We%20introduce%20Espresso%2C%20the%20first%20robust%20concept%0Afilter%20based%20on%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29.%20We%20identify%0Aunacceptable%20concepts%20by%20using%20the%20distance%20between%20the%20embedding%20of%20a%0Agenerated%20image%20to%20the%20text%20embeddings%20of%20both%20unacceptable%20and%20acceptable%0Aconcepts.%20This%20lets%20us%20fine-tune%20for%20robustness%20by%20separating%20the%20text%0Aembeddings%20of%20unacceptable%20and%20acceptable%20concepts%20while%20preserving%20utility.%20We%0Apresent%20a%20pipeline%20to%20evaluate%20various%20CRTs%20to%20show%20that%20Espresso%20is%20more%0Aeffective%20and%20robust%20than%20prior%20CRTs%2C%20while%20retaining%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19227v7&entry.124074799=Read"},
{"title": "On How Iterative Magnitude Pruning Discovers Local Receptive Fields in\n  Fully Connected Neural Networks", "author": "William T. Redman and Zhangyang Wang and Alessandro Ingrosso and Sebastian Goldt", "abstract": "  Since its use in the Lottery Ticket Hypothesis, iterative magnitude pruning\n(IMP) has become a popular method for extracting sparse subnetworks that can be\ntrained to high performance. Despite this, the underlying nature of IMP's\ngeneral success remains unclear. One possibility is that IMP is especially\ncapable of extracting and maintaining strong inductive biases. In support of\nthis, recent work has shown that applying IMP to fully connected neural\nnetworks (FCNs) leads to the emergence of local receptive fields (RFs), an\narchitectural feature present in mammalian visual cortex and convolutional\nneural networks. The question of how IMP is able to do this remains unanswered.\nInspired by results showing that training FCNs on synthetic images with highly\nnon-Gaussian statistics (e.g., sharp edges) is sufficient to drive the\nformation of local RFs, we hypothesize that IMP iteratively increases the\nnon-Gaussian statistics present in the representations of FCNs, creating a\nfeedback loop that enhances localization. We develop a new method for measuring\nthe effect of individual weights on the statistics of the FCN representations\n(\"cavity method\"), which allows us to find evidence in support of this\nhypothesis. Our work, which is the first to study the effect IMP has on the\nstatistics of the representations of neural networks, sheds parsimonious light\non one way in which IMP can drive the formation of strong inductive biases.\n", "link": "http://arxiv.org/abs/2412.06545v2", "date": "2025-02-26", "relevancy": 2.0462, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.522}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.505}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20How%20Iterative%20Magnitude%20Pruning%20Discovers%20Local%20Receptive%20Fields%20in%0A%20%20Fully%20Connected%20Neural%20Networks&body=Title%3A%20On%20How%20Iterative%20Magnitude%20Pruning%20Discovers%20Local%20Receptive%20Fields%20in%0A%20%20Fully%20Connected%20Neural%20Networks%0AAuthor%3A%20William%20T.%20Redman%20and%20Zhangyang%20Wang%20and%20Alessandro%20Ingrosso%20and%20Sebastian%20Goldt%0AAbstract%3A%20%20%20Since%20its%20use%20in%20the%20Lottery%20Ticket%20Hypothesis%2C%20iterative%20magnitude%20pruning%0A%28IMP%29%20has%20become%20a%20popular%20method%20for%20extracting%20sparse%20subnetworks%20that%20can%20be%0Atrained%20to%20high%20performance.%20Despite%20this%2C%20the%20underlying%20nature%20of%20IMP%27s%0Ageneral%20success%20remains%20unclear.%20One%20possibility%20is%20that%20IMP%20is%20especially%0Acapable%20of%20extracting%20and%20maintaining%20strong%20inductive%20biases.%20In%20support%20of%0Athis%2C%20recent%20work%20has%20shown%20that%20applying%20IMP%20to%20fully%20connected%20neural%0Anetworks%20%28FCNs%29%20leads%20to%20the%20emergence%20of%20local%20receptive%20fields%20%28RFs%29%2C%20an%0Aarchitectural%20feature%20present%20in%20mammalian%20visual%20cortex%20and%20convolutional%0Aneural%20networks.%20The%20question%20of%20how%20IMP%20is%20able%20to%20do%20this%20remains%20unanswered.%0AInspired%20by%20results%20showing%20that%20training%20FCNs%20on%20synthetic%20images%20with%20highly%0Anon-Gaussian%20statistics%20%28e.g.%2C%20sharp%20edges%29%20is%20sufficient%20to%20drive%20the%0Aformation%20of%20local%20RFs%2C%20we%20hypothesize%20that%20IMP%20iteratively%20increases%20the%0Anon-Gaussian%20statistics%20present%20in%20the%20representations%20of%20FCNs%2C%20creating%20a%0Afeedback%20loop%20that%20enhances%20localization.%20We%20develop%20a%20new%20method%20for%20measuring%0Athe%20effect%20of%20individual%20weights%20on%20the%20statistics%20of%20the%20FCN%20representations%0A%28%22cavity%20method%22%29%2C%20which%20allows%20us%20to%20find%20evidence%20in%20support%20of%20this%0Ahypothesis.%20Our%20work%2C%20which%20is%20the%20first%20to%20study%20the%20effect%20IMP%20has%20on%20the%0Astatistics%20of%20the%20representations%20of%20neural%20networks%2C%20sheds%20parsimonious%20light%0Aon%20one%20way%20in%20which%20IMP%20can%20drive%20the%20formation%20of%20strong%20inductive%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520How%2520Iterative%2520Magnitude%2520Pruning%2520Discovers%2520Local%2520Receptive%2520Fields%2520in%250A%2520%2520Fully%2520Connected%2520Neural%2520Networks%26entry.906535625%3DWilliam%2520T.%2520Redman%2520and%2520Zhangyang%2520Wang%2520and%2520Alessandro%2520Ingrosso%2520and%2520Sebastian%2520Goldt%26entry.1292438233%3D%2520%2520Since%2520its%2520use%2520in%2520the%2520Lottery%2520Ticket%2520Hypothesis%252C%2520iterative%2520magnitude%2520pruning%250A%2528IMP%2529%2520has%2520become%2520a%2520popular%2520method%2520for%2520extracting%2520sparse%2520subnetworks%2520that%2520can%2520be%250Atrained%2520to%2520high%2520performance.%2520Despite%2520this%252C%2520the%2520underlying%2520nature%2520of%2520IMP%2527s%250Ageneral%2520success%2520remains%2520unclear.%2520One%2520possibility%2520is%2520that%2520IMP%2520is%2520especially%250Acapable%2520of%2520extracting%2520and%2520maintaining%2520strong%2520inductive%2520biases.%2520In%2520support%2520of%250Athis%252C%2520recent%2520work%2520has%2520shown%2520that%2520applying%2520IMP%2520to%2520fully%2520connected%2520neural%250Anetworks%2520%2528FCNs%2529%2520leads%2520to%2520the%2520emergence%2520of%2520local%2520receptive%2520fields%2520%2528RFs%2529%252C%2520an%250Aarchitectural%2520feature%2520present%2520in%2520mammalian%2520visual%2520cortex%2520and%2520convolutional%250Aneural%2520networks.%2520The%2520question%2520of%2520how%2520IMP%2520is%2520able%2520to%2520do%2520this%2520remains%2520unanswered.%250AInspired%2520by%2520results%2520showing%2520that%2520training%2520FCNs%2520on%2520synthetic%2520images%2520with%2520highly%250Anon-Gaussian%2520statistics%2520%2528e.g.%252C%2520sharp%2520edges%2529%2520is%2520sufficient%2520to%2520drive%2520the%250Aformation%2520of%2520local%2520RFs%252C%2520we%2520hypothesize%2520that%2520IMP%2520iteratively%2520increases%2520the%250Anon-Gaussian%2520statistics%2520present%2520in%2520the%2520representations%2520of%2520FCNs%252C%2520creating%2520a%250Afeedback%2520loop%2520that%2520enhances%2520localization.%2520We%2520develop%2520a%2520new%2520method%2520for%2520measuring%250Athe%2520effect%2520of%2520individual%2520weights%2520on%2520the%2520statistics%2520of%2520the%2520FCN%2520representations%250A%2528%2522cavity%2520method%2522%2529%252C%2520which%2520allows%2520us%2520to%2520find%2520evidence%2520in%2520support%2520of%2520this%250Ahypothesis.%2520Our%2520work%252C%2520which%2520is%2520the%2520first%2520to%2520study%2520the%2520effect%2520IMP%2520has%2520on%2520the%250Astatistics%2520of%2520the%2520representations%2520of%2520neural%2520networks%252C%2520sheds%2520parsimonious%2520light%250Aon%2520one%2520way%2520in%2520which%2520IMP%2520can%2520drive%2520the%2520formation%2520of%2520strong%2520inductive%2520biases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20How%20Iterative%20Magnitude%20Pruning%20Discovers%20Local%20Receptive%20Fields%20in%0A%20%20Fully%20Connected%20Neural%20Networks&entry.906535625=William%20T.%20Redman%20and%20Zhangyang%20Wang%20and%20Alessandro%20Ingrosso%20and%20Sebastian%20Goldt&entry.1292438233=%20%20Since%20its%20use%20in%20the%20Lottery%20Ticket%20Hypothesis%2C%20iterative%20magnitude%20pruning%0A%28IMP%29%20has%20become%20a%20popular%20method%20for%20extracting%20sparse%20subnetworks%20that%20can%20be%0Atrained%20to%20high%20performance.%20Despite%20this%2C%20the%20underlying%20nature%20of%20IMP%27s%0Ageneral%20success%20remains%20unclear.%20One%20possibility%20is%20that%20IMP%20is%20especially%0Acapable%20of%20extracting%20and%20maintaining%20strong%20inductive%20biases.%20In%20support%20of%0Athis%2C%20recent%20work%20has%20shown%20that%20applying%20IMP%20to%20fully%20connected%20neural%0Anetworks%20%28FCNs%29%20leads%20to%20the%20emergence%20of%20local%20receptive%20fields%20%28RFs%29%2C%20an%0Aarchitectural%20feature%20present%20in%20mammalian%20visual%20cortex%20and%20convolutional%0Aneural%20networks.%20The%20question%20of%20how%20IMP%20is%20able%20to%20do%20this%20remains%20unanswered.%0AInspired%20by%20results%20showing%20that%20training%20FCNs%20on%20synthetic%20images%20with%20highly%0Anon-Gaussian%20statistics%20%28e.g.%2C%20sharp%20edges%29%20is%20sufficient%20to%20drive%20the%0Aformation%20of%20local%20RFs%2C%20we%20hypothesize%20that%20IMP%20iteratively%20increases%20the%0Anon-Gaussian%20statistics%20present%20in%20the%20representations%20of%20FCNs%2C%20creating%20a%0Afeedback%20loop%20that%20enhances%20localization.%20We%20develop%20a%20new%20method%20for%20measuring%0Athe%20effect%20of%20individual%20weights%20on%20the%20statistics%20of%20the%20FCN%20representations%0A%28%22cavity%20method%22%29%2C%20which%20allows%20us%20to%20find%20evidence%20in%20support%20of%20this%0Ahypothesis.%20Our%20work%2C%20which%20is%20the%20first%20to%20study%20the%20effect%20IMP%20has%20on%20the%0Astatistics%20of%20the%20representations%20of%20neural%20networks%2C%20sheds%20parsimonious%20light%0Aon%20one%20way%20in%20which%20IMP%20can%20drive%20the%20formation%20of%20strong%20inductive%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06545v2&entry.124074799=Read"},
{"title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and\n  Reasoning-Driven Code Intelligence in LLMs", "author": "Dayu Yang and Tianyang Liu and Daoan Zhang and Antoine Simoulin and Xiaoyi Liu and Yuwei Cao and Zhaopu Teng and Xin Qian and Grey Yang and Jiebo Luo and Julian McAuley", "abstract": "  In large language models (LLMs), code and reasoning reinforce each other:\ncode offers an abstract, modular, and logic-driven structure that supports\nreasoning, while reasoning translates high-level goals into smaller, executable\nsteps that drive more advanced code intelligence. In this study, we examine how\ncode serves as a structured medium for enhancing reasoning: it provides\nverifiable execution paths, enforces logical decomposition, and enables runtime\nvalidation. We also explore how improvements in reasoning have transformed code\nintelligence from basic completion to advanced capabilities, enabling models to\naddress complex software engineering tasks through planning and debugging.\nFinally, we identify key challenges and propose future research directions to\nstrengthen this synergy, ultimately improving LLM's performance in both areas.\n", "link": "http://arxiv.org/abs/2502.19411v1", "date": "2025-02-26", "relevancy": 2.0402, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Code%20to%20Think%2C%20Think%20to%20Code%3A%20A%20Survey%20on%20Code-Enhanced%20Reasoning%20and%0A%20%20Reasoning-Driven%20Code%20Intelligence%20in%20LLMs&body=Title%3A%20Code%20to%20Think%2C%20Think%20to%20Code%3A%20A%20Survey%20on%20Code-Enhanced%20Reasoning%20and%0A%20%20Reasoning-Driven%20Code%20Intelligence%20in%20LLMs%0AAuthor%3A%20Dayu%20Yang%20and%20Tianyang%20Liu%20and%20Daoan%20Zhang%20and%20Antoine%20Simoulin%20and%20Xiaoyi%20Liu%20and%20Yuwei%20Cao%20and%20Zhaopu%20Teng%20and%20Xin%20Qian%20and%20Grey%20Yang%20and%20Jiebo%20Luo%20and%20Julian%20McAuley%0AAbstract%3A%20%20%20In%20large%20language%20models%20%28LLMs%29%2C%20code%20and%20reasoning%20reinforce%20each%20other%3A%0Acode%20offers%20an%20abstract%2C%20modular%2C%20and%20logic-driven%20structure%20that%20supports%0Areasoning%2C%20while%20reasoning%20translates%20high-level%20goals%20into%20smaller%2C%20executable%0Asteps%20that%20drive%20more%20advanced%20code%20intelligence.%20In%20this%20study%2C%20we%20examine%20how%0Acode%20serves%20as%20a%20structured%20medium%20for%20enhancing%20reasoning%3A%20it%20provides%0Averifiable%20execution%20paths%2C%20enforces%20logical%20decomposition%2C%20and%20enables%20runtime%0Avalidation.%20We%20also%20explore%20how%20improvements%20in%20reasoning%20have%20transformed%20code%0Aintelligence%20from%20basic%20completion%20to%20advanced%20capabilities%2C%20enabling%20models%20to%0Aaddress%20complex%20software%20engineering%20tasks%20through%20planning%20and%20debugging.%0AFinally%2C%20we%20identify%20key%20challenges%20and%20propose%20future%20research%20directions%20to%0Astrengthen%20this%20synergy%2C%20ultimately%20improving%20LLM%27s%20performance%20in%20both%20areas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCode%2520to%2520Think%252C%2520Think%2520to%2520Code%253A%2520A%2520Survey%2520on%2520Code-Enhanced%2520Reasoning%2520and%250A%2520%2520Reasoning-Driven%2520Code%2520Intelligence%2520in%2520LLMs%26entry.906535625%3DDayu%2520Yang%2520and%2520Tianyang%2520Liu%2520and%2520Daoan%2520Zhang%2520and%2520Antoine%2520Simoulin%2520and%2520Xiaoyi%2520Liu%2520and%2520Yuwei%2520Cao%2520and%2520Zhaopu%2520Teng%2520and%2520Xin%2520Qian%2520and%2520Grey%2520Yang%2520and%2520Jiebo%2520Luo%2520and%2520Julian%2520McAuley%26entry.1292438233%3D%2520%2520In%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520code%2520and%2520reasoning%2520reinforce%2520each%2520other%253A%250Acode%2520offers%2520an%2520abstract%252C%2520modular%252C%2520and%2520logic-driven%2520structure%2520that%2520supports%250Areasoning%252C%2520while%2520reasoning%2520translates%2520high-level%2520goals%2520into%2520smaller%252C%2520executable%250Asteps%2520that%2520drive%2520more%2520advanced%2520code%2520intelligence.%2520In%2520this%2520study%252C%2520we%2520examine%2520how%250Acode%2520serves%2520as%2520a%2520structured%2520medium%2520for%2520enhancing%2520reasoning%253A%2520it%2520provides%250Averifiable%2520execution%2520paths%252C%2520enforces%2520logical%2520decomposition%252C%2520and%2520enables%2520runtime%250Avalidation.%2520We%2520also%2520explore%2520how%2520improvements%2520in%2520reasoning%2520have%2520transformed%2520code%250Aintelligence%2520from%2520basic%2520completion%2520to%2520advanced%2520capabilities%252C%2520enabling%2520models%2520to%250Aaddress%2520complex%2520software%2520engineering%2520tasks%2520through%2520planning%2520and%2520debugging.%250AFinally%252C%2520we%2520identify%2520key%2520challenges%2520and%2520propose%2520future%2520research%2520directions%2520to%250Astrengthen%2520this%2520synergy%252C%2520ultimately%2520improving%2520LLM%2527s%2520performance%2520in%2520both%2520areas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Code%20to%20Think%2C%20Think%20to%20Code%3A%20A%20Survey%20on%20Code-Enhanced%20Reasoning%20and%0A%20%20Reasoning-Driven%20Code%20Intelligence%20in%20LLMs&entry.906535625=Dayu%20Yang%20and%20Tianyang%20Liu%20and%20Daoan%20Zhang%20and%20Antoine%20Simoulin%20and%20Xiaoyi%20Liu%20and%20Yuwei%20Cao%20and%20Zhaopu%20Teng%20and%20Xin%20Qian%20and%20Grey%20Yang%20and%20Jiebo%20Luo%20and%20Julian%20McAuley&entry.1292438233=%20%20In%20large%20language%20models%20%28LLMs%29%2C%20code%20and%20reasoning%20reinforce%20each%20other%3A%0Acode%20offers%20an%20abstract%2C%20modular%2C%20and%20logic-driven%20structure%20that%20supports%0Areasoning%2C%20while%20reasoning%20translates%20high-level%20goals%20into%20smaller%2C%20executable%0Asteps%20that%20drive%20more%20advanced%20code%20intelligence.%20In%20this%20study%2C%20we%20examine%20how%0Acode%20serves%20as%20a%20structured%20medium%20for%20enhancing%20reasoning%3A%20it%20provides%0Averifiable%20execution%20paths%2C%20enforces%20logical%20decomposition%2C%20and%20enables%20runtime%0Avalidation.%20We%20also%20explore%20how%20improvements%20in%20reasoning%20have%20transformed%20code%0Aintelligence%20from%20basic%20completion%20to%20advanced%20capabilities%2C%20enabling%20models%20to%0Aaddress%20complex%20software%20engineering%20tasks%20through%20planning%20and%20debugging.%0AFinally%2C%20we%20identify%20key%20challenges%20and%20propose%20future%20research%20directions%20to%0Astrengthen%20this%20synergy%2C%20ultimately%20improving%20LLM%27s%20performance%20in%20both%20areas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19411v1&entry.124074799=Read"},
{"title": "Multiview graph dual-attention deep learning and contrastive learning\n  for multi-criteria recommender systems", "author": "Saman Forouzandeh and Pavel N. Krivitsky and Rohitash Chandra", "abstract": "  Recommender systems leveraging deep learning models have been crucial for\nassisting users in selecting items aligned with their preferences and\ninterests. However, a significant challenge persists in single-criteria\nrecommender systems, which often overlook the diverse attributes of items that\nhave been addressed by Multi-Criteria Recommender Systems (MCRS). Shared\nembedding vector for multi-criteria item ratings but have struggled to capture\nthe nuanced relationships between users and items based on specific criteria.\nIn this study, we present a novel representation for Multi-Criteria Recommender\nSystems (MCRS) based on a multi-edge bipartite graph, where each edge\nrepresents one criterion rating of items by users, and Multiview Dual Graph\nAttention Networks (MDGAT). Employing MDGAT is beneficial and important for\nadequately considering all relations between users and items, given the\npresence of both local (criterion-based) and global (multi-criteria) relations.\nAdditionally, we define anchor points in each view based on similarity and\nemploy local and global contrastive learning to distinguish between positive\nand negative samples across each view and the entire graph. We evaluate our\nmethod on two real-world datasets and assess its performance based on item\nrating predictions. The results demonstrate that our method achieves higher\naccuracy compared to the baseline method for predicting item ratings on the\nsame datasets. MDGAT effectively capture the local and global impact of\nneighbours and the similarity between nodes.\n", "link": "http://arxiv.org/abs/2502.19271v1", "date": "2025-02-26", "relevancy": 2.0384, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5157}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiview%20graph%20dual-attention%20deep%20learning%20and%20contrastive%20learning%0A%20%20for%20multi-criteria%20recommender%20systems&body=Title%3A%20Multiview%20graph%20dual-attention%20deep%20learning%20and%20contrastive%20learning%0A%20%20for%20multi-criteria%20recommender%20systems%0AAuthor%3A%20Saman%20Forouzandeh%20and%20Pavel%20N.%20Krivitsky%20and%20Rohitash%20Chandra%0AAbstract%3A%20%20%20Recommender%20systems%20leveraging%20deep%20learning%20models%20have%20been%20crucial%20for%0Aassisting%20users%20in%20selecting%20items%20aligned%20with%20their%20preferences%20and%0Ainterests.%20However%2C%20a%20significant%20challenge%20persists%20in%20single-criteria%0Arecommender%20systems%2C%20which%20often%20overlook%20the%20diverse%20attributes%20of%20items%20that%0Ahave%20been%20addressed%20by%20Multi-Criteria%20Recommender%20Systems%20%28MCRS%29.%20Shared%0Aembedding%20vector%20for%20multi-criteria%20item%20ratings%20but%20have%20struggled%20to%20capture%0Athe%20nuanced%20relationships%20between%20users%20and%20items%20based%20on%20specific%20criteria.%0AIn%20this%20study%2C%20we%20present%20a%20novel%20representation%20for%20Multi-Criteria%20Recommender%0ASystems%20%28MCRS%29%20based%20on%20a%20multi-edge%20bipartite%20graph%2C%20where%20each%20edge%0Arepresents%20one%20criterion%20rating%20of%20items%20by%20users%2C%20and%20Multiview%20Dual%20Graph%0AAttention%20Networks%20%28MDGAT%29.%20Employing%20MDGAT%20is%20beneficial%20and%20important%20for%0Aadequately%20considering%20all%20relations%20between%20users%20and%20items%2C%20given%20the%0Apresence%20of%20both%20local%20%28criterion-based%29%20and%20global%20%28multi-criteria%29%20relations.%0AAdditionally%2C%20we%20define%20anchor%20points%20in%20each%20view%20based%20on%20similarity%20and%0Aemploy%20local%20and%20global%20contrastive%20learning%20to%20distinguish%20between%20positive%0Aand%20negative%20samples%20across%20each%20view%20and%20the%20entire%20graph.%20We%20evaluate%20our%0Amethod%20on%20two%20real-world%20datasets%20and%20assess%20its%20performance%20based%20on%20item%0Arating%20predictions.%20The%20results%20demonstrate%20that%20our%20method%20achieves%20higher%0Aaccuracy%20compared%20to%20the%20baseline%20method%20for%20predicting%20item%20ratings%20on%20the%0Asame%20datasets.%20MDGAT%20effectively%20capture%20the%20local%20and%20global%20impact%20of%0Aneighbours%20and%20the%20similarity%20between%20nodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiview%2520graph%2520dual-attention%2520deep%2520learning%2520and%2520contrastive%2520learning%250A%2520%2520for%2520multi-criteria%2520recommender%2520systems%26entry.906535625%3DSaman%2520Forouzandeh%2520and%2520Pavel%2520N.%2520Krivitsky%2520and%2520Rohitash%2520Chandra%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520leveraging%2520deep%2520learning%2520models%2520have%2520been%2520crucial%2520for%250Aassisting%2520users%2520in%2520selecting%2520items%2520aligned%2520with%2520their%2520preferences%2520and%250Ainterests.%2520However%252C%2520a%2520significant%2520challenge%2520persists%2520in%2520single-criteria%250Arecommender%2520systems%252C%2520which%2520often%2520overlook%2520the%2520diverse%2520attributes%2520of%2520items%2520that%250Ahave%2520been%2520addressed%2520by%2520Multi-Criteria%2520Recommender%2520Systems%2520%2528MCRS%2529.%2520Shared%250Aembedding%2520vector%2520for%2520multi-criteria%2520item%2520ratings%2520but%2520have%2520struggled%2520to%2520capture%250Athe%2520nuanced%2520relationships%2520between%2520users%2520and%2520items%2520based%2520on%2520specific%2520criteria.%250AIn%2520this%2520study%252C%2520we%2520present%2520a%2520novel%2520representation%2520for%2520Multi-Criteria%2520Recommender%250ASystems%2520%2528MCRS%2529%2520based%2520on%2520a%2520multi-edge%2520bipartite%2520graph%252C%2520where%2520each%2520edge%250Arepresents%2520one%2520criterion%2520rating%2520of%2520items%2520by%2520users%252C%2520and%2520Multiview%2520Dual%2520Graph%250AAttention%2520Networks%2520%2528MDGAT%2529.%2520Employing%2520MDGAT%2520is%2520beneficial%2520and%2520important%2520for%250Aadequately%2520considering%2520all%2520relations%2520between%2520users%2520and%2520items%252C%2520given%2520the%250Apresence%2520of%2520both%2520local%2520%2528criterion-based%2529%2520and%2520global%2520%2528multi-criteria%2529%2520relations.%250AAdditionally%252C%2520we%2520define%2520anchor%2520points%2520in%2520each%2520view%2520based%2520on%2520similarity%2520and%250Aemploy%2520local%2520and%2520global%2520contrastive%2520learning%2520to%2520distinguish%2520between%2520positive%250Aand%2520negative%2520samples%2520across%2520each%2520view%2520and%2520the%2520entire%2520graph.%2520We%2520evaluate%2520our%250Amethod%2520on%2520two%2520real-world%2520datasets%2520and%2520assess%2520its%2520performance%2520based%2520on%2520item%250Arating%2520predictions.%2520The%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520higher%250Aaccuracy%2520compared%2520to%2520the%2520baseline%2520method%2520for%2520predicting%2520item%2520ratings%2520on%2520the%250Asame%2520datasets.%2520MDGAT%2520effectively%2520capture%2520the%2520local%2520and%2520global%2520impact%2520of%250Aneighbours%2520and%2520the%2520similarity%2520between%2520nodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiview%20graph%20dual-attention%20deep%20learning%20and%20contrastive%20learning%0A%20%20for%20multi-criteria%20recommender%20systems&entry.906535625=Saman%20Forouzandeh%20and%20Pavel%20N.%20Krivitsky%20and%20Rohitash%20Chandra&entry.1292438233=%20%20Recommender%20systems%20leveraging%20deep%20learning%20models%20have%20been%20crucial%20for%0Aassisting%20users%20in%20selecting%20items%20aligned%20with%20their%20preferences%20and%0Ainterests.%20However%2C%20a%20significant%20challenge%20persists%20in%20single-criteria%0Arecommender%20systems%2C%20which%20often%20overlook%20the%20diverse%20attributes%20of%20items%20that%0Ahave%20been%20addressed%20by%20Multi-Criteria%20Recommender%20Systems%20%28MCRS%29.%20Shared%0Aembedding%20vector%20for%20multi-criteria%20item%20ratings%20but%20have%20struggled%20to%20capture%0Athe%20nuanced%20relationships%20between%20users%20and%20items%20based%20on%20specific%20criteria.%0AIn%20this%20study%2C%20we%20present%20a%20novel%20representation%20for%20Multi-Criteria%20Recommender%0ASystems%20%28MCRS%29%20based%20on%20a%20multi-edge%20bipartite%20graph%2C%20where%20each%20edge%0Arepresents%20one%20criterion%20rating%20of%20items%20by%20users%2C%20and%20Multiview%20Dual%20Graph%0AAttention%20Networks%20%28MDGAT%29.%20Employing%20MDGAT%20is%20beneficial%20and%20important%20for%0Aadequately%20considering%20all%20relations%20between%20users%20and%20items%2C%20given%20the%0Apresence%20of%20both%20local%20%28criterion-based%29%20and%20global%20%28multi-criteria%29%20relations.%0AAdditionally%2C%20we%20define%20anchor%20points%20in%20each%20view%20based%20on%20similarity%20and%0Aemploy%20local%20and%20global%20contrastive%20learning%20to%20distinguish%20between%20positive%0Aand%20negative%20samples%20across%20each%20view%20and%20the%20entire%20graph.%20We%20evaluate%20our%0Amethod%20on%20two%20real-world%20datasets%20and%20assess%20its%20performance%20based%20on%20item%0Arating%20predictions.%20The%20results%20demonstrate%20that%20our%20method%20achieves%20higher%0Aaccuracy%20compared%20to%20the%20baseline%20method%20for%20predicting%20item%20ratings%20on%20the%0Asame%20datasets.%20MDGAT%20effectively%20capture%20the%20local%20and%20global%20impact%20of%0Aneighbours%20and%20the%20similarity%20between%20nodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19271v1&entry.124074799=Read"},
{"title": "PlantPal: Leveraging Precision Agriculture Robots to Facilitate Remote\n  Engagement in Urban Gardening", "author": "Albin Zeqiri and Julian Britten and Clara Schramm and Pascal Jansen and Michael Rietzler and Enrico Rukzio", "abstract": "  Urban gardening is widely recognized for its numerous health and\nenvironmental benefits. However, the lack of suitable garden spaces, demanding\ndaily schedules and limited gardening expertise present major roadblocks for\ncitizens looking to engage in urban gardening. While prior research has\nexplored smart home solutions to support urban gardeners, these approaches\ncurrently do not fully address these practical barriers. In this paper, we\npresent PlantPal, a system that enables the cultivation of garden spaces\nirrespective of one's location, expertise level, or time constraints. PlantPal\nenables the shared operation of a precision agriculture robot (PAR) that is\nequipped with garden tools and a multi-camera system. Insights from a 3-week\ndeployment (N=18) indicate that PlantPal facilitated the integration of\ngardening tasks into daily routines, fostered a sense of connection with one's\nfield, and provided an engaging experience despite the remote setting. We\ncontribute design considerations for future robot-assisted urban gardening\nconcepts.\n", "link": "http://arxiv.org/abs/2502.19171v1", "date": "2025-02-26", "relevancy": 1.9863, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.498}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlantPal%3A%20Leveraging%20Precision%20Agriculture%20Robots%20to%20Facilitate%20Remote%0A%20%20Engagement%20in%20Urban%20Gardening&body=Title%3A%20PlantPal%3A%20Leveraging%20Precision%20Agriculture%20Robots%20to%20Facilitate%20Remote%0A%20%20Engagement%20in%20Urban%20Gardening%0AAuthor%3A%20Albin%20Zeqiri%20and%20Julian%20Britten%20and%20Clara%20Schramm%20and%20Pascal%20Jansen%20and%20Michael%20Rietzler%20and%20Enrico%20Rukzio%0AAbstract%3A%20%20%20Urban%20gardening%20is%20widely%20recognized%20for%20its%20numerous%20health%20and%0Aenvironmental%20benefits.%20However%2C%20the%20lack%20of%20suitable%20garden%20spaces%2C%20demanding%0Adaily%20schedules%20and%20limited%20gardening%20expertise%20present%20major%20roadblocks%20for%0Acitizens%20looking%20to%20engage%20in%20urban%20gardening.%20While%20prior%20research%20has%0Aexplored%20smart%20home%20solutions%20to%20support%20urban%20gardeners%2C%20these%20approaches%0Acurrently%20do%20not%20fully%20address%20these%20practical%20barriers.%20In%20this%20paper%2C%20we%0Apresent%20PlantPal%2C%20a%20system%20that%20enables%20the%20cultivation%20of%20garden%20spaces%0Airrespective%20of%20one%27s%20location%2C%20expertise%20level%2C%20or%20time%20constraints.%20PlantPal%0Aenables%20the%20shared%20operation%20of%20a%20precision%20agriculture%20robot%20%28PAR%29%20that%20is%0Aequipped%20with%20garden%20tools%20and%20a%20multi-camera%20system.%20Insights%20from%20a%203-week%0Adeployment%20%28N%3D18%29%20indicate%20that%20PlantPal%20facilitated%20the%20integration%20of%0Agardening%20tasks%20into%20daily%20routines%2C%20fostered%20a%20sense%20of%20connection%20with%20one%27s%0Afield%2C%20and%20provided%20an%20engaging%20experience%20despite%20the%20remote%20setting.%20We%0Acontribute%20design%20considerations%20for%20future%20robot-assisted%20urban%20gardening%0Aconcepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlantPal%253A%2520Leveraging%2520Precision%2520Agriculture%2520Robots%2520to%2520Facilitate%2520Remote%250A%2520%2520Engagement%2520in%2520Urban%2520Gardening%26entry.906535625%3DAlbin%2520Zeqiri%2520and%2520Julian%2520Britten%2520and%2520Clara%2520Schramm%2520and%2520Pascal%2520Jansen%2520and%2520Michael%2520Rietzler%2520and%2520Enrico%2520Rukzio%26entry.1292438233%3D%2520%2520Urban%2520gardening%2520is%2520widely%2520recognized%2520for%2520its%2520numerous%2520health%2520and%250Aenvironmental%2520benefits.%2520However%252C%2520the%2520lack%2520of%2520suitable%2520garden%2520spaces%252C%2520demanding%250Adaily%2520schedules%2520and%2520limited%2520gardening%2520expertise%2520present%2520major%2520roadblocks%2520for%250Acitizens%2520looking%2520to%2520engage%2520in%2520urban%2520gardening.%2520While%2520prior%2520research%2520has%250Aexplored%2520smart%2520home%2520solutions%2520to%2520support%2520urban%2520gardeners%252C%2520these%2520approaches%250Acurrently%2520do%2520not%2520fully%2520address%2520these%2520practical%2520barriers.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520PlantPal%252C%2520a%2520system%2520that%2520enables%2520the%2520cultivation%2520of%2520garden%2520spaces%250Airrespective%2520of%2520one%2527s%2520location%252C%2520expertise%2520level%252C%2520or%2520time%2520constraints.%2520PlantPal%250Aenables%2520the%2520shared%2520operation%2520of%2520a%2520precision%2520agriculture%2520robot%2520%2528PAR%2529%2520that%2520is%250Aequipped%2520with%2520garden%2520tools%2520and%2520a%2520multi-camera%2520system.%2520Insights%2520from%2520a%25203-week%250Adeployment%2520%2528N%253D18%2529%2520indicate%2520that%2520PlantPal%2520facilitated%2520the%2520integration%2520of%250Agardening%2520tasks%2520into%2520daily%2520routines%252C%2520fostered%2520a%2520sense%2520of%2520connection%2520with%2520one%2527s%250Afield%252C%2520and%2520provided%2520an%2520engaging%2520experience%2520despite%2520the%2520remote%2520setting.%2520We%250Acontribute%2520design%2520considerations%2520for%2520future%2520robot-assisted%2520urban%2520gardening%250Aconcepts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlantPal%3A%20Leveraging%20Precision%20Agriculture%20Robots%20to%20Facilitate%20Remote%0A%20%20Engagement%20in%20Urban%20Gardening&entry.906535625=Albin%20Zeqiri%20and%20Julian%20Britten%20and%20Clara%20Schramm%20and%20Pascal%20Jansen%20and%20Michael%20Rietzler%20and%20Enrico%20Rukzio&entry.1292438233=%20%20Urban%20gardening%20is%20widely%20recognized%20for%20its%20numerous%20health%20and%0Aenvironmental%20benefits.%20However%2C%20the%20lack%20of%20suitable%20garden%20spaces%2C%20demanding%0Adaily%20schedules%20and%20limited%20gardening%20expertise%20present%20major%20roadblocks%20for%0Acitizens%20looking%20to%20engage%20in%20urban%20gardening.%20While%20prior%20research%20has%0Aexplored%20smart%20home%20solutions%20to%20support%20urban%20gardeners%2C%20these%20approaches%0Acurrently%20do%20not%20fully%20address%20these%20practical%20barriers.%20In%20this%20paper%2C%20we%0Apresent%20PlantPal%2C%20a%20system%20that%20enables%20the%20cultivation%20of%20garden%20spaces%0Airrespective%20of%20one%27s%20location%2C%20expertise%20level%2C%20or%20time%20constraints.%20PlantPal%0Aenables%20the%20shared%20operation%20of%20a%20precision%20agriculture%20robot%20%28PAR%29%20that%20is%0Aequipped%20with%20garden%20tools%20and%20a%20multi-camera%20system.%20Insights%20from%20a%203-week%0Adeployment%20%28N%3D18%29%20indicate%20that%20PlantPal%20facilitated%20the%20integration%20of%0Agardening%20tasks%20into%20daily%20routines%2C%20fostered%20a%20sense%20of%20connection%20with%20one%27s%0Afield%2C%20and%20provided%20an%20engaging%20experience%20despite%20the%20remote%20setting.%20We%0Acontribute%20design%20considerations%20for%20future%20robot-assisted%20urban%20gardening%0Aconcepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19171v1&entry.124074799=Read"},
{"title": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in\n  LLMs Elicits Effective Personalization to Real Users", "author": "Anikait Singh and Sheryl Hsu and Kyle Hsu and Eric Mitchell and Stefano Ermon and Tatsunori Hashimoto and Archit Sharma and Chelsea Finn", "abstract": "  Effective personalization of LLMs is critical for a broad range of\nuser-interfacing applications such as virtual assistants and content curation.\nInspired by the strong in-context learning capabilities of LLMs, we propose\nFew-Shot Preference Optimization (FSPO), which reframes reward modeling as a\nmeta-learning problem. Under this framework, an LLM learns to quickly adapt to\na user via a few labeled preferences from that user, constructing a\npersonalized reward function for them. Additionally, since real-world\npreference data is scarce and challenging to collect at scale, we propose\ncareful design choices to construct synthetic preference datasets for\npersonalization, generating over 1M synthetic personalized preferences using\npublicly available LLMs. In particular, to successfully transfer from synthetic\ndata to real users, we find it crucial for the data to exhibit both high\ndiversity and coherent, self-consistent structure. We evaluate FSPO on\npersonalized open-ended generation for up to 1,500 synthetic users across\nacross three domains: movie reviews, pedagogical adaptation based on\neducational background, and general question answering, along with a controlled\nhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in\ngenerating responses that are personalized to synthetic users and a 72% winrate\nwith real human users in open-ended question answering.\n", "link": "http://arxiv.org/abs/2502.19312v1", "date": "2025-02-26", "relevancy": 1.4115, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4737}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4699}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FSPO%3A%20Few-Shot%20Preference%20Optimization%20of%20Synthetic%20Preference%20Data%20in%0A%20%20LLMs%20Elicits%20Effective%20Personalization%20to%20Real%20Users&body=Title%3A%20FSPO%3A%20Few-Shot%20Preference%20Optimization%20of%20Synthetic%20Preference%20Data%20in%0A%20%20LLMs%20Elicits%20Effective%20Personalization%20to%20Real%20Users%0AAuthor%3A%20Anikait%20Singh%20and%20Sheryl%20Hsu%20and%20Kyle%20Hsu%20and%20Eric%20Mitchell%20and%20Stefano%20Ermon%20and%20Tatsunori%20Hashimoto%20and%20Archit%20Sharma%20and%20Chelsea%20Finn%0AAbstract%3A%20%20%20Effective%20personalization%20of%20LLMs%20is%20critical%20for%20a%20broad%20range%20of%0Auser-interfacing%20applications%20such%20as%20virtual%20assistants%20and%20content%20curation.%0AInspired%20by%20the%20strong%20in-context%20learning%20capabilities%20of%20LLMs%2C%20we%20propose%0AFew-Shot%20Preference%20Optimization%20%28FSPO%29%2C%20which%20reframes%20reward%20modeling%20as%20a%0Ameta-learning%20problem.%20Under%20this%20framework%2C%20an%20LLM%20learns%20to%20quickly%20adapt%20to%0Aa%20user%20via%20a%20few%20labeled%20preferences%20from%20that%20user%2C%20constructing%20a%0Apersonalized%20reward%20function%20for%20them.%20Additionally%2C%20since%20real-world%0Apreference%20data%20is%20scarce%20and%20challenging%20to%20collect%20at%20scale%2C%20we%20propose%0Acareful%20design%20choices%20to%20construct%20synthetic%20preference%20datasets%20for%0Apersonalization%2C%20generating%20over%201M%20synthetic%20personalized%20preferences%20using%0Apublicly%20available%20LLMs.%20In%20particular%2C%20to%20successfully%20transfer%20from%20synthetic%0Adata%20to%20real%20users%2C%20we%20find%20it%20crucial%20for%20the%20data%20to%20exhibit%20both%20high%0Adiversity%20and%20coherent%2C%20self-consistent%20structure.%20We%20evaluate%20FSPO%20on%0Apersonalized%20open-ended%20generation%20for%20up%20to%201%2C500%20synthetic%20users%20across%0Aacross%20three%20domains%3A%20movie%20reviews%2C%20pedagogical%20adaptation%20based%20on%0Aeducational%20background%2C%20and%20general%20question%20answering%2C%20along%20with%20a%20controlled%0Ahuman%20study.%20Overall%2C%20FSPO%20achieves%20an%2087%25%20Alpaca%20Eval%20winrate%20on%20average%20in%0Agenerating%20responses%20that%20are%20personalized%20to%20synthetic%20users%20and%20a%2072%25%20winrate%0Awith%20real%20human%20users%20in%20open-ended%20question%20answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFSPO%253A%2520Few-Shot%2520Preference%2520Optimization%2520of%2520Synthetic%2520Preference%2520Data%2520in%250A%2520%2520LLMs%2520Elicits%2520Effective%2520Personalization%2520to%2520Real%2520Users%26entry.906535625%3DAnikait%2520Singh%2520and%2520Sheryl%2520Hsu%2520and%2520Kyle%2520Hsu%2520and%2520Eric%2520Mitchell%2520and%2520Stefano%2520Ermon%2520and%2520Tatsunori%2520Hashimoto%2520and%2520Archit%2520Sharma%2520and%2520Chelsea%2520Finn%26entry.1292438233%3D%2520%2520Effective%2520personalization%2520of%2520LLMs%2520is%2520critical%2520for%2520a%2520broad%2520range%2520of%250Auser-interfacing%2520applications%2520such%2520as%2520virtual%2520assistants%2520and%2520content%2520curation.%250AInspired%2520by%2520the%2520strong%2520in-context%2520learning%2520capabilities%2520of%2520LLMs%252C%2520we%2520propose%250AFew-Shot%2520Preference%2520Optimization%2520%2528FSPO%2529%252C%2520which%2520reframes%2520reward%2520modeling%2520as%2520a%250Ameta-learning%2520problem.%2520Under%2520this%2520framework%252C%2520an%2520LLM%2520learns%2520to%2520quickly%2520adapt%2520to%250Aa%2520user%2520via%2520a%2520few%2520labeled%2520preferences%2520from%2520that%2520user%252C%2520constructing%2520a%250Apersonalized%2520reward%2520function%2520for%2520them.%2520Additionally%252C%2520since%2520real-world%250Apreference%2520data%2520is%2520scarce%2520and%2520challenging%2520to%2520collect%2520at%2520scale%252C%2520we%2520propose%250Acareful%2520design%2520choices%2520to%2520construct%2520synthetic%2520preference%2520datasets%2520for%250Apersonalization%252C%2520generating%2520over%25201M%2520synthetic%2520personalized%2520preferences%2520using%250Apublicly%2520available%2520LLMs.%2520In%2520particular%252C%2520to%2520successfully%2520transfer%2520from%2520synthetic%250Adata%2520to%2520real%2520users%252C%2520we%2520find%2520it%2520crucial%2520for%2520the%2520data%2520to%2520exhibit%2520both%2520high%250Adiversity%2520and%2520coherent%252C%2520self-consistent%2520structure.%2520We%2520evaluate%2520FSPO%2520on%250Apersonalized%2520open-ended%2520generation%2520for%2520up%2520to%25201%252C500%2520synthetic%2520users%2520across%250Aacross%2520three%2520domains%253A%2520movie%2520reviews%252C%2520pedagogical%2520adaptation%2520based%2520on%250Aeducational%2520background%252C%2520and%2520general%2520question%2520answering%252C%2520along%2520with%2520a%2520controlled%250Ahuman%2520study.%2520Overall%252C%2520FSPO%2520achieves%2520an%252087%2525%2520Alpaca%2520Eval%2520winrate%2520on%2520average%2520in%250Agenerating%2520responses%2520that%2520are%2520personalized%2520to%2520synthetic%2520users%2520and%2520a%252072%2525%2520winrate%250Awith%2520real%2520human%2520users%2520in%2520open-ended%2520question%2520answering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FSPO%3A%20Few-Shot%20Preference%20Optimization%20of%20Synthetic%20Preference%20Data%20in%0A%20%20LLMs%20Elicits%20Effective%20Personalization%20to%20Real%20Users&entry.906535625=Anikait%20Singh%20and%20Sheryl%20Hsu%20and%20Kyle%20Hsu%20and%20Eric%20Mitchell%20and%20Stefano%20Ermon%20and%20Tatsunori%20Hashimoto%20and%20Archit%20Sharma%20and%20Chelsea%20Finn&entry.1292438233=%20%20Effective%20personalization%20of%20LLMs%20is%20critical%20for%20a%20broad%20range%20of%0Auser-interfacing%20applications%20such%20as%20virtual%20assistants%20and%20content%20curation.%0AInspired%20by%20the%20strong%20in-context%20learning%20capabilities%20of%20LLMs%2C%20we%20propose%0AFew-Shot%20Preference%20Optimization%20%28FSPO%29%2C%20which%20reframes%20reward%20modeling%20as%20a%0Ameta-learning%20problem.%20Under%20this%20framework%2C%20an%20LLM%20learns%20to%20quickly%20adapt%20to%0Aa%20user%20via%20a%20few%20labeled%20preferences%20from%20that%20user%2C%20constructing%20a%0Apersonalized%20reward%20function%20for%20them.%20Additionally%2C%20since%20real-world%0Apreference%20data%20is%20scarce%20and%20challenging%20to%20collect%20at%20scale%2C%20we%20propose%0Acareful%20design%20choices%20to%20construct%20synthetic%20preference%20datasets%20for%0Apersonalization%2C%20generating%20over%201M%20synthetic%20personalized%20preferences%20using%0Apublicly%20available%20LLMs.%20In%20particular%2C%20to%20successfully%20transfer%20from%20synthetic%0Adata%20to%20real%20users%2C%20we%20find%20it%20crucial%20for%20the%20data%20to%20exhibit%20both%20high%0Adiversity%20and%20coherent%2C%20self-consistent%20structure.%20We%20evaluate%20FSPO%20on%0Apersonalized%20open-ended%20generation%20for%20up%20to%201%2C500%20synthetic%20users%20across%0Aacross%20three%20domains%3A%20movie%20reviews%2C%20pedagogical%20adaptation%20based%20on%0Aeducational%20background%2C%20and%20general%20question%20answering%2C%20along%20with%20a%20controlled%0Ahuman%20study.%20Overall%2C%20FSPO%20achieves%20an%2087%25%20Alpaca%20Eval%20winrate%20on%20average%20in%0Agenerating%20responses%20that%20are%20personalized%20to%20synthetic%20users%20and%20a%2072%25%20winrate%0Awith%20real%20human%20users%20in%20open-ended%20question%20answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19312v1&entry.124074799=Read"},
{"title": "Shh, don't say that! Domain Certification in LLMs", "author": "Cornelius Emde and Alasdair Paren and Preetham Arvind and Maxime Kayser and Tom Rainforth and Thomas Lukasiewicz and Bernard Ghanem and Philip H. S. Torr and Adel Bibi", "abstract": "  Large language models (LLMs) are often deployed to perform constrained tasks,\nwith narrow domains. For example, customer support bots can be built on top of\nLLMs, relying on their broad language understanding and capabilities to enhance\nperformance. However, these LLMs are adversarially susceptible, potentially\ngenerating outputs outside the intended domain. To formalize, assess, and\nmitigate this risk, we introduce domain certification; a guarantee that\naccurately characterizes the out-of-domain behavior of language models. We then\npropose a simple yet effective approach, which we call VALID that provides\nadversarial bounds as a certificate. Finally, we evaluate our method across a\ndiverse set of datasets, demonstrating that it yields meaningful certificates,\nwhich bound the probability of out-of-domain samples tightly with minimum\npenalty to refusal behavior.\n", "link": "http://arxiv.org/abs/2502.19320v1", "date": "2025-02-26", "relevancy": 1.9214, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5107}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shh%2C%20don%27t%20say%20that%21%20Domain%20Certification%20in%20LLMs&body=Title%3A%20Shh%2C%20don%27t%20say%20that%21%20Domain%20Certification%20in%20LLMs%0AAuthor%3A%20Cornelius%20Emde%20and%20Alasdair%20Paren%20and%20Preetham%20Arvind%20and%20Maxime%20Kayser%20and%20Tom%20Rainforth%20and%20Thomas%20Lukasiewicz%20and%20Bernard%20Ghanem%20and%20Philip%20H.%20S.%20Torr%20and%20Adel%20Bibi%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20often%20deployed%20to%20perform%20constrained%20tasks%2C%0Awith%20narrow%20domains.%20For%20example%2C%20customer%20support%20bots%20can%20be%20built%20on%20top%20of%0ALLMs%2C%20relying%20on%20their%20broad%20language%20understanding%20and%20capabilities%20to%20enhance%0Aperformance.%20However%2C%20these%20LLMs%20are%20adversarially%20susceptible%2C%20potentially%0Agenerating%20outputs%20outside%20the%20intended%20domain.%20To%20formalize%2C%20assess%2C%20and%0Amitigate%20this%20risk%2C%20we%20introduce%20domain%20certification%3B%20a%20guarantee%20that%0Aaccurately%20characterizes%20the%20out-of-domain%20behavior%20of%20language%20models.%20We%20then%0Apropose%20a%20simple%20yet%20effective%20approach%2C%20which%20we%20call%20VALID%20that%20provides%0Aadversarial%20bounds%20as%20a%20certificate.%20Finally%2C%20we%20evaluate%20our%20method%20across%20a%0Adiverse%20set%20of%20datasets%2C%20demonstrating%20that%20it%20yields%20meaningful%20certificates%2C%0Awhich%20bound%20the%20probability%20of%20out-of-domain%20samples%20tightly%20with%20minimum%0Apenalty%20to%20refusal%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShh%252C%2520don%2527t%2520say%2520that%2521%2520Domain%2520Certification%2520in%2520LLMs%26entry.906535625%3DCornelius%2520Emde%2520and%2520Alasdair%2520Paren%2520and%2520Preetham%2520Arvind%2520and%2520Maxime%2520Kayser%2520and%2520Tom%2520Rainforth%2520and%2520Thomas%2520Lukasiewicz%2520and%2520Bernard%2520Ghanem%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Adel%2520Bibi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520often%2520deployed%2520to%2520perform%2520constrained%2520tasks%252C%250Awith%2520narrow%2520domains.%2520For%2520example%252C%2520customer%2520support%2520bots%2520can%2520be%2520built%2520on%2520top%2520of%250ALLMs%252C%2520relying%2520on%2520their%2520broad%2520language%2520understanding%2520and%2520capabilities%2520to%2520enhance%250Aperformance.%2520However%252C%2520these%2520LLMs%2520are%2520adversarially%2520susceptible%252C%2520potentially%250Agenerating%2520outputs%2520outside%2520the%2520intended%2520domain.%2520To%2520formalize%252C%2520assess%252C%2520and%250Amitigate%2520this%2520risk%252C%2520we%2520introduce%2520domain%2520certification%253B%2520a%2520guarantee%2520that%250Aaccurately%2520characterizes%2520the%2520out-of-domain%2520behavior%2520of%2520language%2520models.%2520We%2520then%250Apropose%2520a%2520simple%2520yet%2520effective%2520approach%252C%2520which%2520we%2520call%2520VALID%2520that%2520provides%250Aadversarial%2520bounds%2520as%2520a%2520certificate.%2520Finally%252C%2520we%2520evaluate%2520our%2520method%2520across%2520a%250Adiverse%2520set%2520of%2520datasets%252C%2520demonstrating%2520that%2520it%2520yields%2520meaningful%2520certificates%252C%250Awhich%2520bound%2520the%2520probability%2520of%2520out-of-domain%2520samples%2520tightly%2520with%2520minimum%250Apenalty%2520to%2520refusal%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shh%2C%20don%27t%20say%20that%21%20Domain%20Certification%20in%20LLMs&entry.906535625=Cornelius%20Emde%20and%20Alasdair%20Paren%20and%20Preetham%20Arvind%20and%20Maxime%20Kayser%20and%20Tom%20Rainforth%20and%20Thomas%20Lukasiewicz%20and%20Bernard%20Ghanem%20and%20Philip%20H.%20S.%20Torr%20and%20Adel%20Bibi&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20often%20deployed%20to%20perform%20constrained%20tasks%2C%0Awith%20narrow%20domains.%20For%20example%2C%20customer%20support%20bots%20can%20be%20built%20on%20top%20of%0ALLMs%2C%20relying%20on%20their%20broad%20language%20understanding%20and%20capabilities%20to%20enhance%0Aperformance.%20However%2C%20these%20LLMs%20are%20adversarially%20susceptible%2C%20potentially%0Agenerating%20outputs%20outside%20the%20intended%20domain.%20To%20formalize%2C%20assess%2C%20and%0Amitigate%20this%20risk%2C%20we%20introduce%20domain%20certification%3B%20a%20guarantee%20that%0Aaccurately%20characterizes%20the%20out-of-domain%20behavior%20of%20language%20models.%20We%20then%0Apropose%20a%20simple%20yet%20effective%20approach%2C%20which%20we%20call%20VALID%20that%20provides%0Aadversarial%20bounds%20as%20a%20certificate.%20Finally%2C%20we%20evaluate%20our%20method%20across%20a%0Adiverse%20set%20of%20datasets%2C%20demonstrating%20that%20it%20yields%20meaningful%20certificates%2C%0Awhich%20bound%20the%20probability%20of%20out-of-domain%20samples%20tightly%20with%20minimum%0Apenalty%20to%20refusal%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19320v1&entry.124074799=Read"},
{"title": "Partition Tree Weighting for Non-Stationary Stochastic Bandits", "author": "Joel Veness and Marcus Hutter and Andras Gyorgy and Jordi Grau-Moya", "abstract": "  This paper considers a generalisation of universal source coding for\ninteraction data, namely data streams that have actions interleaved with\nobservations. Our goal will be to construct a coding distribution that is both\nuniversal \\emph{and} can be used as a control policy. Allowing for action\ngeneration needs careful treatment, as naive approaches which do not\ndistinguish between actions and observations run into the self-delusion problem\nin universal settings. We showcase our perspective in the context of the\nchallenging non-stationary stochastic Bernoulli bandit problem. Our main\ncontribution is an efficient and high performing algorithm for this problem\nthat generalises the Partition Tree Weighting universal source coding technique\nfor passive prediction to the control setting.\n", "link": "http://arxiv.org/abs/2502.19325v1", "date": "2025-02-26", "relevancy": 1.8085, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4545}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partition%20Tree%20Weighting%20for%20Non-Stationary%20Stochastic%20Bandits&body=Title%3A%20Partition%20Tree%20Weighting%20for%20Non-Stationary%20Stochastic%20Bandits%0AAuthor%3A%20Joel%20Veness%20and%20Marcus%20Hutter%20and%20Andras%20Gyorgy%20and%20Jordi%20Grau-Moya%0AAbstract%3A%20%20%20This%20paper%20considers%20a%20generalisation%20of%20universal%20source%20coding%20for%0Ainteraction%20data%2C%20namely%20data%20streams%20that%20have%20actions%20interleaved%20with%0Aobservations.%20Our%20goal%20will%20be%20to%20construct%20a%20coding%20distribution%20that%20is%20both%0Auniversal%20%5Cemph%7Band%7D%20can%20be%20used%20as%20a%20control%20policy.%20Allowing%20for%20action%0Ageneration%20needs%20careful%20treatment%2C%20as%20naive%20approaches%20which%20do%20not%0Adistinguish%20between%20actions%20and%20observations%20run%20into%20the%20self-delusion%20problem%0Ain%20universal%20settings.%20We%20showcase%20our%20perspective%20in%20the%20context%20of%20the%0Achallenging%20non-stationary%20stochastic%20Bernoulli%20bandit%20problem.%20Our%20main%0Acontribution%20is%20an%20efficient%20and%20high%20performing%20algorithm%20for%20this%20problem%0Athat%20generalises%20the%20Partition%20Tree%20Weighting%20universal%20source%20coding%20technique%0Afor%20passive%20prediction%20to%20the%20control%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartition%2520Tree%2520Weighting%2520for%2520Non-Stationary%2520Stochastic%2520Bandits%26entry.906535625%3DJoel%2520Veness%2520and%2520Marcus%2520Hutter%2520and%2520Andras%2520Gyorgy%2520and%2520Jordi%2520Grau-Moya%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520a%2520generalisation%2520of%2520universal%2520source%2520coding%2520for%250Ainteraction%2520data%252C%2520namely%2520data%2520streams%2520that%2520have%2520actions%2520interleaved%2520with%250Aobservations.%2520Our%2520goal%2520will%2520be%2520to%2520construct%2520a%2520coding%2520distribution%2520that%2520is%2520both%250Auniversal%2520%255Cemph%257Band%257D%2520can%2520be%2520used%2520as%2520a%2520control%2520policy.%2520Allowing%2520for%2520action%250Ageneration%2520needs%2520careful%2520treatment%252C%2520as%2520naive%2520approaches%2520which%2520do%2520not%250Adistinguish%2520between%2520actions%2520and%2520observations%2520run%2520into%2520the%2520self-delusion%2520problem%250Ain%2520universal%2520settings.%2520We%2520showcase%2520our%2520perspective%2520in%2520the%2520context%2520of%2520the%250Achallenging%2520non-stationary%2520stochastic%2520Bernoulli%2520bandit%2520problem.%2520Our%2520main%250Acontribution%2520is%2520an%2520efficient%2520and%2520high%2520performing%2520algorithm%2520for%2520this%2520problem%250Athat%2520generalises%2520the%2520Partition%2520Tree%2520Weighting%2520universal%2520source%2520coding%2520technique%250Afor%2520passive%2520prediction%2520to%2520the%2520control%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partition%20Tree%20Weighting%20for%20Non-Stationary%20Stochastic%20Bandits&entry.906535625=Joel%20Veness%20and%20Marcus%20Hutter%20and%20Andras%20Gyorgy%20and%20Jordi%20Grau-Moya&entry.1292438233=%20%20This%20paper%20considers%20a%20generalisation%20of%20universal%20source%20coding%20for%0Ainteraction%20data%2C%20namely%20data%20streams%20that%20have%20actions%20interleaved%20with%0Aobservations.%20Our%20goal%20will%20be%20to%20construct%20a%20coding%20distribution%20that%20is%20both%0Auniversal%20%5Cemph%7Band%7D%20can%20be%20used%20as%20a%20control%20policy.%20Allowing%20for%20action%0Ageneration%20needs%20careful%20treatment%2C%20as%20naive%20approaches%20which%20do%20not%0Adistinguish%20between%20actions%20and%20observations%20run%20into%20the%20self-delusion%20problem%0Ain%20universal%20settings.%20We%20showcase%20our%20perspective%20in%20the%20context%20of%20the%0Achallenging%20non-stationary%20stochastic%20Bernoulli%20bandit%20problem.%20Our%20main%0Acontribution%20is%20an%20efficient%20and%20high%20performing%20algorithm%20for%20this%20problem%0Athat%20generalises%20the%20Partition%20Tree%20Weighting%20universal%20source%20coding%20technique%0Afor%20passive%20prediction%20to%20the%20control%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19325v1&entry.124074799=Read"},
{"title": "Reproducibility in Machine Learning-based Research: Overview, Barriers\n  and Drivers", "author": "Harald Semmelrock and Tony Ross-Hellauer and Simone Kopeinik and Dieter Theiler and Armin Haberl and Stefan Thalmann and Dominik Kowald", "abstract": "  Many research fields are currently reckoning with issues of poor levels of\nreproducibility. Some label it a \"crisis\", and research employing or building\nMachine Learning (ML) models is no exception. Issues including lack of\ntransparency, data or code, poor adherence to standards, and the sensitivity of\nML training conditions mean that many papers are not even reproducible in\nprinciple. Where they are, though, reproducibility experiments have found\nworryingly low degrees of similarity with original results. Despite previous\nappeals from ML researchers on this topic and various initiatives from\nconference reproducibility tracks to the ACM's new Emerging Interest Group on\nReproducibility and Replicability, we contend that the general community\ncontinues to take this issue too lightly. Poor reproducibility threatens trust\nin and integrity of research results. Therefore, in this article, we lay out a\nnew perspective on the key barriers and drivers (both procedural and technical)\nto increased reproducibility at various levels (methods, code, data, and\nexperiments). We then map the drivers to the barriers to give concrete advice\nfor strategies for researchers to mitigate reproducibility issues in their own\nwork, to lay out key areas where further research is needed in specific areas,\nand to further ignite discussion on the threat presented by these urgent\nissues.\n", "link": "http://arxiv.org/abs/2406.14325v3", "date": "2025-02-26", "relevancy": 1.6294, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4208}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reproducibility%20in%20Machine%20Learning-based%20Research%3A%20Overview%2C%20Barriers%0A%20%20and%20Drivers&body=Title%3A%20Reproducibility%20in%20Machine%20Learning-based%20Research%3A%20Overview%2C%20Barriers%0A%20%20and%20Drivers%0AAuthor%3A%20Harald%20Semmelrock%20and%20Tony%20Ross-Hellauer%20and%20Simone%20Kopeinik%20and%20Dieter%20Theiler%20and%20Armin%20Haberl%20and%20Stefan%20Thalmann%20and%20Dominik%20Kowald%0AAbstract%3A%20%20%20Many%20research%20fields%20are%20currently%20reckoning%20with%20issues%20of%20poor%20levels%20of%0Areproducibility.%20Some%20label%20it%20a%20%22crisis%22%2C%20and%20research%20employing%20or%20building%0AMachine%20Learning%20%28ML%29%20models%20is%20no%20exception.%20Issues%20including%20lack%20of%0Atransparency%2C%20data%20or%20code%2C%20poor%20adherence%20to%20standards%2C%20and%20the%20sensitivity%20of%0AML%20training%20conditions%20mean%20that%20many%20papers%20are%20not%20even%20reproducible%20in%0Aprinciple.%20Where%20they%20are%2C%20though%2C%20reproducibility%20experiments%20have%20found%0Aworryingly%20low%20degrees%20of%20similarity%20with%20original%20results.%20Despite%20previous%0Aappeals%20from%20ML%20researchers%20on%20this%20topic%20and%20various%20initiatives%20from%0Aconference%20reproducibility%20tracks%20to%20the%20ACM%27s%20new%20Emerging%20Interest%20Group%20on%0AReproducibility%20and%20Replicability%2C%20we%20contend%20that%20the%20general%20community%0Acontinues%20to%20take%20this%20issue%20too%20lightly.%20Poor%20reproducibility%20threatens%20trust%0Ain%20and%20integrity%20of%20research%20results.%20Therefore%2C%20in%20this%20article%2C%20we%20lay%20out%20a%0Anew%20perspective%20on%20the%20key%20barriers%20and%20drivers%20%28both%20procedural%20and%20technical%29%0Ato%20increased%20reproducibility%20at%20various%20levels%20%28methods%2C%20code%2C%20data%2C%20and%0Aexperiments%29.%20We%20then%20map%20the%20drivers%20to%20the%20barriers%20to%20give%20concrete%20advice%0Afor%20strategies%20for%20researchers%20to%20mitigate%20reproducibility%20issues%20in%20their%20own%0Awork%2C%20to%20lay%20out%20key%20areas%20where%20further%20research%20is%20needed%20in%20specific%20areas%2C%0Aand%20to%20further%20ignite%20discussion%20on%20the%20threat%20presented%20by%20these%20urgent%0Aissues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14325v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReproducibility%2520in%2520Machine%2520Learning-based%2520Research%253A%2520Overview%252C%2520Barriers%250A%2520%2520and%2520Drivers%26entry.906535625%3DHarald%2520Semmelrock%2520and%2520Tony%2520Ross-Hellauer%2520and%2520Simone%2520Kopeinik%2520and%2520Dieter%2520Theiler%2520and%2520Armin%2520Haberl%2520and%2520Stefan%2520Thalmann%2520and%2520Dominik%2520Kowald%26entry.1292438233%3D%2520%2520Many%2520research%2520fields%2520are%2520currently%2520reckoning%2520with%2520issues%2520of%2520poor%2520levels%2520of%250Areproducibility.%2520Some%2520label%2520it%2520a%2520%2522crisis%2522%252C%2520and%2520research%2520employing%2520or%2520building%250AMachine%2520Learning%2520%2528ML%2529%2520models%2520is%2520no%2520exception.%2520Issues%2520including%2520lack%2520of%250Atransparency%252C%2520data%2520or%2520code%252C%2520poor%2520adherence%2520to%2520standards%252C%2520and%2520the%2520sensitivity%2520of%250AML%2520training%2520conditions%2520mean%2520that%2520many%2520papers%2520are%2520not%2520even%2520reproducible%2520in%250Aprinciple.%2520Where%2520they%2520are%252C%2520though%252C%2520reproducibility%2520experiments%2520have%2520found%250Aworryingly%2520low%2520degrees%2520of%2520similarity%2520with%2520original%2520results.%2520Despite%2520previous%250Aappeals%2520from%2520ML%2520researchers%2520on%2520this%2520topic%2520and%2520various%2520initiatives%2520from%250Aconference%2520reproducibility%2520tracks%2520to%2520the%2520ACM%2527s%2520new%2520Emerging%2520Interest%2520Group%2520on%250AReproducibility%2520and%2520Replicability%252C%2520we%2520contend%2520that%2520the%2520general%2520community%250Acontinues%2520to%2520take%2520this%2520issue%2520too%2520lightly.%2520Poor%2520reproducibility%2520threatens%2520trust%250Ain%2520and%2520integrity%2520of%2520research%2520results.%2520Therefore%252C%2520in%2520this%2520article%252C%2520we%2520lay%2520out%2520a%250Anew%2520perspective%2520on%2520the%2520key%2520barriers%2520and%2520drivers%2520%2528both%2520procedural%2520and%2520technical%2529%250Ato%2520increased%2520reproducibility%2520at%2520various%2520levels%2520%2528methods%252C%2520code%252C%2520data%252C%2520and%250Aexperiments%2529.%2520We%2520then%2520map%2520the%2520drivers%2520to%2520the%2520barriers%2520to%2520give%2520concrete%2520advice%250Afor%2520strategies%2520for%2520researchers%2520to%2520mitigate%2520reproducibility%2520issues%2520in%2520their%2520own%250Awork%252C%2520to%2520lay%2520out%2520key%2520areas%2520where%2520further%2520research%2520is%2520needed%2520in%2520specific%2520areas%252C%250Aand%2520to%2520further%2520ignite%2520discussion%2520on%2520the%2520threat%2520presented%2520by%2520these%2520urgent%250Aissues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14325v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reproducibility%20in%20Machine%20Learning-based%20Research%3A%20Overview%2C%20Barriers%0A%20%20and%20Drivers&entry.906535625=Harald%20Semmelrock%20and%20Tony%20Ross-Hellauer%20and%20Simone%20Kopeinik%20and%20Dieter%20Theiler%20and%20Armin%20Haberl%20and%20Stefan%20Thalmann%20and%20Dominik%20Kowald&entry.1292438233=%20%20Many%20research%20fields%20are%20currently%20reckoning%20with%20issues%20of%20poor%20levels%20of%0Areproducibility.%20Some%20label%20it%20a%20%22crisis%22%2C%20and%20research%20employing%20or%20building%0AMachine%20Learning%20%28ML%29%20models%20is%20no%20exception.%20Issues%20including%20lack%20of%0Atransparency%2C%20data%20or%20code%2C%20poor%20adherence%20to%20standards%2C%20and%20the%20sensitivity%20of%0AML%20training%20conditions%20mean%20that%20many%20papers%20are%20not%20even%20reproducible%20in%0Aprinciple.%20Where%20they%20are%2C%20though%2C%20reproducibility%20experiments%20have%20found%0Aworryingly%20low%20degrees%20of%20similarity%20with%20original%20results.%20Despite%20previous%0Aappeals%20from%20ML%20researchers%20on%20this%20topic%20and%20various%20initiatives%20from%0Aconference%20reproducibility%20tracks%20to%20the%20ACM%27s%20new%20Emerging%20Interest%20Group%20on%0AReproducibility%20and%20Replicability%2C%20we%20contend%20that%20the%20general%20community%0Acontinues%20to%20take%20this%20issue%20too%20lightly.%20Poor%20reproducibility%20threatens%20trust%0Ain%20and%20integrity%20of%20research%20results.%20Therefore%2C%20in%20this%20article%2C%20we%20lay%20out%20a%0Anew%20perspective%20on%20the%20key%20barriers%20and%20drivers%20%28both%20procedural%20and%20technical%29%0Ato%20increased%20reproducibility%20at%20various%20levels%20%28methods%2C%20code%2C%20data%2C%20and%0Aexperiments%29.%20We%20then%20map%20the%20drivers%20to%20the%20barriers%20to%20give%20concrete%20advice%0Afor%20strategies%20for%20researchers%20to%20mitigate%20reproducibility%20issues%20in%20their%20own%0Awork%2C%20to%20lay%20out%20key%20areas%20where%20further%20research%20is%20needed%20in%20specific%20areas%2C%0Aand%20to%20further%20ignite%20discussion%20on%20the%20threat%20presented%20by%20these%20urgent%0Aissues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14325v3&entry.124074799=Read"},
{"title": "Diagrammatization and Abduction to Improve AI Interpretability With\n  Domain-Aligned Explanations for Medical Diagnosis", "author": "Brian Y. Lim and Joseph P. Cahaly and Chester Y. F. Sng and Adam Chew", "abstract": "  Many visualizations have been developed for explainable AI (XAI), but they\noften require further reasoning by users to interpret. Investigating XAI for\nhigh-stakes medical diagnosis, we propose improving domain alignment with\ndiagrammatic and abductive reasoning to reduce the interpretability gap. We\ndeveloped DiagramNet to predict cardiac diagnoses from heart auscultation,\nselect the best-fitting hypothesis based on criteria evaluation, and explain\nwith clinically-relevant murmur diagrams. The ante-hoc interpretable model\nleverages domain-relevant ontology, representation, and reasoning process to\nincrease trust in expert users. In modeling studies, we found that DiagramNet\nnot only provides faithful murmur shape explanations, but also has better\nperformance than baseline models. We demonstrate the interpretability and\ntrustworthiness of diagrammatic, abductive explanations in a qualitative user\nstudy with medical students, showing that clinically-relevant, diagrammatic\nexplanations are preferred over technical saliency map explanations. This work\ncontributes insights into providing domain-aligned explanations for\nuser-centric XAI in complex domains.\n", "link": "http://arxiv.org/abs/2302.01241v3", "date": "2025-02-26", "relevancy": 1.5051, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5096}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5032}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagrammatization%20and%20Abduction%20to%20Improve%20AI%20Interpretability%20With%0A%20%20Domain-Aligned%20Explanations%20for%20Medical%20Diagnosis&body=Title%3A%20Diagrammatization%20and%20Abduction%20to%20Improve%20AI%20Interpretability%20With%0A%20%20Domain-Aligned%20Explanations%20for%20Medical%20Diagnosis%0AAuthor%3A%20Brian%20Y.%20Lim%20and%20Joseph%20P.%20Cahaly%20and%20Chester%20Y.%20F.%20Sng%20and%20Adam%20Chew%0AAbstract%3A%20%20%20Many%20visualizations%20have%20been%20developed%20for%20explainable%20AI%20%28XAI%29%2C%20but%20they%0Aoften%20require%20further%20reasoning%20by%20users%20to%20interpret.%20Investigating%20XAI%20for%0Ahigh-stakes%20medical%20diagnosis%2C%20we%20propose%20improving%20domain%20alignment%20with%0Adiagrammatic%20and%20abductive%20reasoning%20to%20reduce%20the%20interpretability%20gap.%20We%0Adeveloped%20DiagramNet%20to%20predict%20cardiac%20diagnoses%20from%20heart%20auscultation%2C%0Aselect%20the%20best-fitting%20hypothesis%20based%20on%20criteria%20evaluation%2C%20and%20explain%0Awith%20clinically-relevant%20murmur%20diagrams.%20The%20ante-hoc%20interpretable%20model%0Aleverages%20domain-relevant%20ontology%2C%20representation%2C%20and%20reasoning%20process%20to%0Aincrease%20trust%20in%20expert%20users.%20In%20modeling%20studies%2C%20we%20found%20that%20DiagramNet%0Anot%20only%20provides%20faithful%20murmur%20shape%20explanations%2C%20but%20also%20has%20better%0Aperformance%20than%20baseline%20models.%20We%20demonstrate%20the%20interpretability%20and%0Atrustworthiness%20of%20diagrammatic%2C%20abductive%20explanations%20in%20a%20qualitative%20user%0Astudy%20with%20medical%20students%2C%20showing%20that%20clinically-relevant%2C%20diagrammatic%0Aexplanations%20are%20preferred%20over%20technical%20saliency%20map%20explanations.%20This%20work%0Acontributes%20insights%20into%20providing%20domain-aligned%20explanations%20for%0Auser-centric%20XAI%20in%20complex%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.01241v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagrammatization%2520and%2520Abduction%2520to%2520Improve%2520AI%2520Interpretability%2520With%250A%2520%2520Domain-Aligned%2520Explanations%2520for%2520Medical%2520Diagnosis%26entry.906535625%3DBrian%2520Y.%2520Lim%2520and%2520Joseph%2520P.%2520Cahaly%2520and%2520Chester%2520Y.%2520F.%2520Sng%2520and%2520Adam%2520Chew%26entry.1292438233%3D%2520%2520Many%2520visualizations%2520have%2520been%2520developed%2520for%2520explainable%2520AI%2520%2528XAI%2529%252C%2520but%2520they%250Aoften%2520require%2520further%2520reasoning%2520by%2520users%2520to%2520interpret.%2520Investigating%2520XAI%2520for%250Ahigh-stakes%2520medical%2520diagnosis%252C%2520we%2520propose%2520improving%2520domain%2520alignment%2520with%250Adiagrammatic%2520and%2520abductive%2520reasoning%2520to%2520reduce%2520the%2520interpretability%2520gap.%2520We%250Adeveloped%2520DiagramNet%2520to%2520predict%2520cardiac%2520diagnoses%2520from%2520heart%2520auscultation%252C%250Aselect%2520the%2520best-fitting%2520hypothesis%2520based%2520on%2520criteria%2520evaluation%252C%2520and%2520explain%250Awith%2520clinically-relevant%2520murmur%2520diagrams.%2520The%2520ante-hoc%2520interpretable%2520model%250Aleverages%2520domain-relevant%2520ontology%252C%2520representation%252C%2520and%2520reasoning%2520process%2520to%250Aincrease%2520trust%2520in%2520expert%2520users.%2520In%2520modeling%2520studies%252C%2520we%2520found%2520that%2520DiagramNet%250Anot%2520only%2520provides%2520faithful%2520murmur%2520shape%2520explanations%252C%2520but%2520also%2520has%2520better%250Aperformance%2520than%2520baseline%2520models.%2520We%2520demonstrate%2520the%2520interpretability%2520and%250Atrustworthiness%2520of%2520diagrammatic%252C%2520abductive%2520explanations%2520in%2520a%2520qualitative%2520user%250Astudy%2520with%2520medical%2520students%252C%2520showing%2520that%2520clinically-relevant%252C%2520diagrammatic%250Aexplanations%2520are%2520preferred%2520over%2520technical%2520saliency%2520map%2520explanations.%2520This%2520work%250Acontributes%2520insights%2520into%2520providing%2520domain-aligned%2520explanations%2520for%250Auser-centric%2520XAI%2520in%2520complex%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.01241v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagrammatization%20and%20Abduction%20to%20Improve%20AI%20Interpretability%20With%0A%20%20Domain-Aligned%20Explanations%20for%20Medical%20Diagnosis&entry.906535625=Brian%20Y.%20Lim%20and%20Joseph%20P.%20Cahaly%20and%20Chester%20Y.%20F.%20Sng%20and%20Adam%20Chew&entry.1292438233=%20%20Many%20visualizations%20have%20been%20developed%20for%20explainable%20AI%20%28XAI%29%2C%20but%20they%0Aoften%20require%20further%20reasoning%20by%20users%20to%20interpret.%20Investigating%20XAI%20for%0Ahigh-stakes%20medical%20diagnosis%2C%20we%20propose%20improving%20domain%20alignment%20with%0Adiagrammatic%20and%20abductive%20reasoning%20to%20reduce%20the%20interpretability%20gap.%20We%0Adeveloped%20DiagramNet%20to%20predict%20cardiac%20diagnoses%20from%20heart%20auscultation%2C%0Aselect%20the%20best-fitting%20hypothesis%20based%20on%20criteria%20evaluation%2C%20and%20explain%0Awith%20clinically-relevant%20murmur%20diagrams.%20The%20ante-hoc%20interpretable%20model%0Aleverages%20domain-relevant%20ontology%2C%20representation%2C%20and%20reasoning%20process%20to%0Aincrease%20trust%20in%20expert%20users.%20In%20modeling%20studies%2C%20we%20found%20that%20DiagramNet%0Anot%20only%20provides%20faithful%20murmur%20shape%20explanations%2C%20but%20also%20has%20better%0Aperformance%20than%20baseline%20models.%20We%20demonstrate%20the%20interpretability%20and%0Atrustworthiness%20of%20diagrammatic%2C%20abductive%20explanations%20in%20a%20qualitative%20user%0Astudy%20with%20medical%20students%2C%20showing%20that%20clinically-relevant%2C%20diagrammatic%0Aexplanations%20are%20preferred%20over%20technical%20saliency%20map%20explanations.%20This%20work%0Acontributes%20insights%20into%20providing%20domain-aligned%20explanations%20for%0Auser-centric%20XAI%20in%20complex%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.01241v3&entry.124074799=Read"},
{"title": "When Personalization Meets Reality: A Multi-Faceted Analysis of\n  Personalized Preference Learning", "author": "Yijiang River Dong and Tiancheng Hu and Yinhong Liu and Ahmet \u00dcst\u00fcn and Nigel Collier", "abstract": "  While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems.\n", "link": "http://arxiv.org/abs/2502.19158v1", "date": "2025-02-26", "relevancy": 1.4735, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4871}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Personalization%20Meets%20Reality%3A%20A%20Multi-Faceted%20Analysis%20of%0A%20%20Personalized%20Preference%20Learning&body=Title%3A%20When%20Personalization%20Meets%20Reality%3A%20A%20Multi-Faceted%20Analysis%20of%0A%20%20Personalized%20Preference%20Learning%0AAuthor%3A%20Yijiang%20River%20Dong%20and%20Tiancheng%20Hu%20and%20Yinhong%20Liu%20and%20Ahmet%20%C3%9Cst%C3%BCn%20and%20Nigel%20Collier%0AAbstract%3A%20%20%20While%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20widely%20used%20to%0Aalign%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20preferences%2C%20it%20typically%20assumes%0Ahomogeneous%20preferences%20across%20users%2C%20overlooking%20diverse%20human%20values%20and%0Aminority%20viewpoints.%20Although%20personalized%20preference%20learning%20addresses%20this%0Aby%20tailoring%20separate%20preferences%20for%20individual%20users%2C%20the%20field%20lacks%0Astandardized%20methods%20to%20assess%20its%20effectiveness.%20We%20present%20a%20multi-faceted%0Aevaluation%20framework%20that%20measures%20not%20only%20performance%20but%20also%20fairness%2C%0Aunintended%20effects%2C%20and%20adaptability%20across%20varying%20levels%20of%20preference%0Adivergence.%20Through%20extensive%20experiments%20comparing%20eight%20personalization%0Amethods%20across%20three%20preference%20datasets%2C%20we%20demonstrate%20that%20performance%0Adifferences%20between%20methods%20could%20reach%2036%25%20when%20users%20strongly%20disagree%2C%20and%0Apersonalization%20can%20introduce%20up%20to%2020%25%20safety%20misalignment.%20These%20findings%0Ahighlight%20the%20critical%20need%20for%20holistic%20evaluation%20approaches%20to%20advance%20the%0Adevelopment%20of%20more%20effective%20and%20inclusive%20preference%20learning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Personalization%2520Meets%2520Reality%253A%2520A%2520Multi-Faceted%2520Analysis%2520of%250A%2520%2520Personalized%2520Preference%2520Learning%26entry.906535625%3DYijiang%2520River%2520Dong%2520and%2520Tiancheng%2520Hu%2520and%2520Yinhong%2520Liu%2520and%2520Ahmet%2520%25C3%259Cst%25C3%25BCn%2520and%2520Nigel%2520Collier%26entry.1292438233%3D%2520%2520While%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520is%2520widely%2520used%2520to%250Aalign%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520human%2520preferences%252C%2520it%2520typically%2520assumes%250Ahomogeneous%2520preferences%2520across%2520users%252C%2520overlooking%2520diverse%2520human%2520values%2520and%250Aminority%2520viewpoints.%2520Although%2520personalized%2520preference%2520learning%2520addresses%2520this%250Aby%2520tailoring%2520separate%2520preferences%2520for%2520individual%2520users%252C%2520the%2520field%2520lacks%250Astandardized%2520methods%2520to%2520assess%2520its%2520effectiveness.%2520We%2520present%2520a%2520multi-faceted%250Aevaluation%2520framework%2520that%2520measures%2520not%2520only%2520performance%2520but%2520also%2520fairness%252C%250Aunintended%2520effects%252C%2520and%2520adaptability%2520across%2520varying%2520levels%2520of%2520preference%250Adivergence.%2520Through%2520extensive%2520experiments%2520comparing%2520eight%2520personalization%250Amethods%2520across%2520three%2520preference%2520datasets%252C%2520we%2520demonstrate%2520that%2520performance%250Adifferences%2520between%2520methods%2520could%2520reach%252036%2525%2520when%2520users%2520strongly%2520disagree%252C%2520and%250Apersonalization%2520can%2520introduce%2520up%2520to%252020%2525%2520safety%2520misalignment.%2520These%2520findings%250Ahighlight%2520the%2520critical%2520need%2520for%2520holistic%2520evaluation%2520approaches%2520to%2520advance%2520the%250Adevelopment%2520of%2520more%2520effective%2520and%2520inclusive%2520preference%2520learning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Personalization%20Meets%20Reality%3A%20A%20Multi-Faceted%20Analysis%20of%0A%20%20Personalized%20Preference%20Learning&entry.906535625=Yijiang%20River%20Dong%20and%20Tiancheng%20Hu%20and%20Yinhong%20Liu%20and%20Ahmet%20%C3%9Cst%C3%BCn%20and%20Nigel%20Collier&entry.1292438233=%20%20While%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20is%20widely%20used%20to%0Aalign%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20preferences%2C%20it%20typically%20assumes%0Ahomogeneous%20preferences%20across%20users%2C%20overlooking%20diverse%20human%20values%20and%0Aminority%20viewpoints.%20Although%20personalized%20preference%20learning%20addresses%20this%0Aby%20tailoring%20separate%20preferences%20for%20individual%20users%2C%20the%20field%20lacks%0Astandardized%20methods%20to%20assess%20its%20effectiveness.%20We%20present%20a%20multi-faceted%0Aevaluation%20framework%20that%20measures%20not%20only%20performance%20but%20also%20fairness%2C%0Aunintended%20effects%2C%20and%20adaptability%20across%20varying%20levels%20of%20preference%0Adivergence.%20Through%20extensive%20experiments%20comparing%20eight%20personalization%0Amethods%20across%20three%20preference%20datasets%2C%20we%20demonstrate%20that%20performance%0Adifferences%20between%20methods%20could%20reach%2036%25%20when%20users%20strongly%20disagree%2C%20and%0Apersonalization%20can%20introduce%20up%20to%2020%25%20safety%20misalignment.%20These%20findings%0Ahighlight%20the%20critical%20need%20for%20holistic%20evaluation%20approaches%20to%20advance%20the%0Adevelopment%20of%20more%20effective%20and%20inclusive%20preference%20learning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19158v1&entry.124074799=Read"},
{"title": "Probabilistic Multi-Layer Perceptrons for Wind Farm Condition Monitoring", "author": "Filippo Fiocchi and Domna Ladopoulou and Petros Dellaportas", "abstract": "  We provide a condition monitoring system for wind farms, based on normal\nbehaviour modelling using a probabilistic multi-layer perceptron with transfer\nlearning via fine-tuning. The model predicts the output power of the wind\nturbine under normal behaviour based on features retrieved from supervisory\ncontrol and data acquisition (SCADA) systems. Its advantages are that (i) it\ncan be trained with SCADA data of at least a few years, (ii) it can incorporate\nall SCADA data of all wind turbines in a wind farm as features, (iii) it\nassumes that the output power follows a normal density with heteroscedastic\nvariance and (iv) it can predict the output of one wind turbine by borrowing\nstrength from the data of all other wind turbines in a farm. Probabilistic\nguidelines for condition monitoring are given via a cumulative sum (CUSUM)\ncontrol chart, which is specifically designed based on a real-data\nclassification exercise and, hence, is adapted to the needs of a wind farm. We\nillustrate the performance of our model in a real SCADA data example which\nprovides evidence that it outperforms other probabilistic prediction models.\n", "link": "http://arxiv.org/abs/2404.16496v2", "date": "2025-02-26", "relevancy": 1.4186, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.467}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Multi-Layer%20Perceptrons%20for%20Wind%20Farm%20Condition%20Monitoring&body=Title%3A%20Probabilistic%20Multi-Layer%20Perceptrons%20for%20Wind%20Farm%20Condition%20Monitoring%0AAuthor%3A%20Filippo%20Fiocchi%20and%20Domna%20Ladopoulou%20and%20Petros%20Dellaportas%0AAbstract%3A%20%20%20We%20provide%20a%20condition%20monitoring%20system%20for%20wind%20farms%2C%20based%20on%20normal%0Abehaviour%20modelling%20using%20a%20probabilistic%20multi-layer%20perceptron%20with%20transfer%0Alearning%20via%20fine-tuning.%20The%20model%20predicts%20the%20output%20power%20of%20the%20wind%0Aturbine%20under%20normal%20behaviour%20based%20on%20features%20retrieved%20from%20supervisory%0Acontrol%20and%20data%20acquisition%20%28SCADA%29%20systems.%20Its%20advantages%20are%20that%20%28i%29%20it%0Acan%20be%20trained%20with%20SCADA%20data%20of%20at%20least%20a%20few%20years%2C%20%28ii%29%20it%20can%20incorporate%0Aall%20SCADA%20data%20of%20all%20wind%20turbines%20in%20a%20wind%20farm%20as%20features%2C%20%28iii%29%20it%0Aassumes%20that%20the%20output%20power%20follows%20a%20normal%20density%20with%20heteroscedastic%0Avariance%20and%20%28iv%29%20it%20can%20predict%20the%20output%20of%20one%20wind%20turbine%20by%20borrowing%0Astrength%20from%20the%20data%20of%20all%20other%20wind%20turbines%20in%20a%20farm.%20Probabilistic%0Aguidelines%20for%20condition%20monitoring%20are%20given%20via%20a%20cumulative%20sum%20%28CUSUM%29%0Acontrol%20chart%2C%20which%20is%20specifically%20designed%20based%20on%20a%20real-data%0Aclassification%20exercise%20and%2C%20hence%2C%20is%20adapted%20to%20the%20needs%20of%20a%20wind%20farm.%20We%0Aillustrate%20the%20performance%20of%20our%20model%20in%20a%20real%20SCADA%20data%20example%20which%0Aprovides%20evidence%20that%20it%20outperforms%20other%20probabilistic%20prediction%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Multi-Layer%2520Perceptrons%2520for%2520Wind%2520Farm%2520Condition%2520Monitoring%26entry.906535625%3DFilippo%2520Fiocchi%2520and%2520Domna%2520Ladopoulou%2520and%2520Petros%2520Dellaportas%26entry.1292438233%3D%2520%2520We%2520provide%2520a%2520condition%2520monitoring%2520system%2520for%2520wind%2520farms%252C%2520based%2520on%2520normal%250Abehaviour%2520modelling%2520using%2520a%2520probabilistic%2520multi-layer%2520perceptron%2520with%2520transfer%250Alearning%2520via%2520fine-tuning.%2520The%2520model%2520predicts%2520the%2520output%2520power%2520of%2520the%2520wind%250Aturbine%2520under%2520normal%2520behaviour%2520based%2520on%2520features%2520retrieved%2520from%2520supervisory%250Acontrol%2520and%2520data%2520acquisition%2520%2528SCADA%2529%2520systems.%2520Its%2520advantages%2520are%2520that%2520%2528i%2529%2520it%250Acan%2520be%2520trained%2520with%2520SCADA%2520data%2520of%2520at%2520least%2520a%2520few%2520years%252C%2520%2528ii%2529%2520it%2520can%2520incorporate%250Aall%2520SCADA%2520data%2520of%2520all%2520wind%2520turbines%2520in%2520a%2520wind%2520farm%2520as%2520features%252C%2520%2528iii%2529%2520it%250Aassumes%2520that%2520the%2520output%2520power%2520follows%2520a%2520normal%2520density%2520with%2520heteroscedastic%250Avariance%2520and%2520%2528iv%2529%2520it%2520can%2520predict%2520the%2520output%2520of%2520one%2520wind%2520turbine%2520by%2520borrowing%250Astrength%2520from%2520the%2520data%2520of%2520all%2520other%2520wind%2520turbines%2520in%2520a%2520farm.%2520Probabilistic%250Aguidelines%2520for%2520condition%2520monitoring%2520are%2520given%2520via%2520a%2520cumulative%2520sum%2520%2528CUSUM%2529%250Acontrol%2520chart%252C%2520which%2520is%2520specifically%2520designed%2520based%2520on%2520a%2520real-data%250Aclassification%2520exercise%2520and%252C%2520hence%252C%2520is%2520adapted%2520to%2520the%2520needs%2520of%2520a%2520wind%2520farm.%2520We%250Aillustrate%2520the%2520performance%2520of%2520our%2520model%2520in%2520a%2520real%2520SCADA%2520data%2520example%2520which%250Aprovides%2520evidence%2520that%2520it%2520outperforms%2520other%2520probabilistic%2520prediction%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Multi-Layer%20Perceptrons%20for%20Wind%20Farm%20Condition%20Monitoring&entry.906535625=Filippo%20Fiocchi%20and%20Domna%20Ladopoulou%20and%20Petros%20Dellaportas&entry.1292438233=%20%20We%20provide%20a%20condition%20monitoring%20system%20for%20wind%20farms%2C%20based%20on%20normal%0Abehaviour%20modelling%20using%20a%20probabilistic%20multi-layer%20perceptron%20with%20transfer%0Alearning%20via%20fine-tuning.%20The%20model%20predicts%20the%20output%20power%20of%20the%20wind%0Aturbine%20under%20normal%20behaviour%20based%20on%20features%20retrieved%20from%20supervisory%0Acontrol%20and%20data%20acquisition%20%28SCADA%29%20systems.%20Its%20advantages%20are%20that%20%28i%29%20it%0Acan%20be%20trained%20with%20SCADA%20data%20of%20at%20least%20a%20few%20years%2C%20%28ii%29%20it%20can%20incorporate%0Aall%20SCADA%20data%20of%20all%20wind%20turbines%20in%20a%20wind%20farm%20as%20features%2C%20%28iii%29%20it%0Aassumes%20that%20the%20output%20power%20follows%20a%20normal%20density%20with%20heteroscedastic%0Avariance%20and%20%28iv%29%20it%20can%20predict%20the%20output%20of%20one%20wind%20turbine%20by%20borrowing%0Astrength%20from%20the%20data%20of%20all%20other%20wind%20turbines%20in%20a%20farm.%20Probabilistic%0Aguidelines%20for%20condition%20monitoring%20are%20given%20via%20a%20cumulative%20sum%20%28CUSUM%29%0Acontrol%20chart%2C%20which%20is%20specifically%20designed%20based%20on%20a%20real-data%0Aclassification%20exercise%20and%2C%20hence%2C%20is%20adapted%20to%20the%20needs%20of%20a%20wind%20farm.%20We%0Aillustrate%20the%20performance%20of%20our%20model%20in%20a%20real%20SCADA%20data%20example%20which%0Aprovides%20evidence%20that%20it%20outperforms%20other%20probabilistic%20prediction%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16496v2&entry.124074799=Read"},
{"title": "Task Graph Maximum Likelihood Estimation for Procedural Activity\n  Understanding in Egocentric Videos", "author": "Luigi Seminara and Giovanni Maria Farinella and Antonino Furnari", "abstract": "  We introduce a gradient-based approach for learning task graphs from\nprocedural activities, improving over hand-crafted methods. Our method directly\noptimizes edge weights via maximum likelihood, enabling integration into neural\narchitectures. We validate our approach on CaptainCook4D, EgoPER, and\nEgoProceL, achieving +14.5%, +10.2%, and +13.6% F1-score improvements. Our\nfeature-based approach for predicting task graphs from textual/video embeddings\ndemonstrates emerging video understanding abilities. We also achieved top\nperformance on the procedure understanding benchmark on Ego-Exo4D and\nsignificantly improved online mistake detection (+19.8% on Assembly101-O, +6.4%\non EPIC-Tent-O). Code:\nhttps://github.com/fpv-iplab/Differentiable-Task-Graph-Learning.\n", "link": "http://arxiv.org/abs/2502.17753v2", "date": "2025-02-26", "relevancy": 1.6827, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Graph%20Maximum%20Likelihood%20Estimation%20for%20Procedural%20Activity%0A%20%20Understanding%20in%20Egocentric%20Videos&body=Title%3A%20Task%20Graph%20Maximum%20Likelihood%20Estimation%20for%20Procedural%20Activity%0A%20%20Understanding%20in%20Egocentric%20Videos%0AAuthor%3A%20Luigi%20Seminara%20and%20Giovanni%20Maria%20Farinella%20and%20Antonino%20Furnari%0AAbstract%3A%20%20%20We%20introduce%20a%20gradient-based%20approach%20for%20learning%20task%20graphs%20from%0Aprocedural%20activities%2C%20improving%20over%20hand-crafted%20methods.%20Our%20method%20directly%0Aoptimizes%20edge%20weights%20via%20maximum%20likelihood%2C%20enabling%20integration%20into%20neural%0Aarchitectures.%20We%20validate%20our%20approach%20on%20CaptainCook4D%2C%20EgoPER%2C%20and%0AEgoProceL%2C%20achieving%20%2B14.5%25%2C%20%2B10.2%25%2C%20and%20%2B13.6%25%20F1-score%20improvements.%20Our%0Afeature-based%20approach%20for%20predicting%20task%20graphs%20from%20textual/video%20embeddings%0Ademonstrates%20emerging%20video%20understanding%20abilities.%20We%20also%20achieved%20top%0Aperformance%20on%20the%20procedure%20understanding%20benchmark%20on%20Ego-Exo4D%20and%0Asignificantly%20improved%20online%20mistake%20detection%20%28%2B19.8%25%20on%20Assembly101-O%2C%20%2B6.4%25%0Aon%20EPIC-Tent-O%29.%20Code%3A%0Ahttps%3A//github.com/fpv-iplab/Differentiable-Task-Graph-Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Graph%2520Maximum%2520Likelihood%2520Estimation%2520for%2520Procedural%2520Activity%250A%2520%2520Understanding%2520in%2520Egocentric%2520Videos%26entry.906535625%3DLuigi%2520Seminara%2520and%2520Giovanni%2520Maria%2520Farinella%2520and%2520Antonino%2520Furnari%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520gradient-based%2520approach%2520for%2520learning%2520task%2520graphs%2520from%250Aprocedural%2520activities%252C%2520improving%2520over%2520hand-crafted%2520methods.%2520Our%2520method%2520directly%250Aoptimizes%2520edge%2520weights%2520via%2520maximum%2520likelihood%252C%2520enabling%2520integration%2520into%2520neural%250Aarchitectures.%2520We%2520validate%2520our%2520approach%2520on%2520CaptainCook4D%252C%2520EgoPER%252C%2520and%250AEgoProceL%252C%2520achieving%2520%252B14.5%2525%252C%2520%252B10.2%2525%252C%2520and%2520%252B13.6%2525%2520F1-score%2520improvements.%2520Our%250Afeature-based%2520approach%2520for%2520predicting%2520task%2520graphs%2520from%2520textual/video%2520embeddings%250Ademonstrates%2520emerging%2520video%2520understanding%2520abilities.%2520We%2520also%2520achieved%2520top%250Aperformance%2520on%2520the%2520procedure%2520understanding%2520benchmark%2520on%2520Ego-Exo4D%2520and%250Asignificantly%2520improved%2520online%2520mistake%2520detection%2520%2528%252B19.8%2525%2520on%2520Assembly101-O%252C%2520%252B6.4%2525%250Aon%2520EPIC-Tent-O%2529.%2520Code%253A%250Ahttps%253A//github.com/fpv-iplab/Differentiable-Task-Graph-Learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Graph%20Maximum%20Likelihood%20Estimation%20for%20Procedural%20Activity%0A%20%20Understanding%20in%20Egocentric%20Videos&entry.906535625=Luigi%20Seminara%20and%20Giovanni%20Maria%20Farinella%20and%20Antonino%20Furnari&entry.1292438233=%20%20We%20introduce%20a%20gradient-based%20approach%20for%20learning%20task%20graphs%20from%0Aprocedural%20activities%2C%20improving%20over%20hand-crafted%20methods.%20Our%20method%20directly%0Aoptimizes%20edge%20weights%20via%20maximum%20likelihood%2C%20enabling%20integration%20into%20neural%0Aarchitectures.%20We%20validate%20our%20approach%20on%20CaptainCook4D%2C%20EgoPER%2C%20and%0AEgoProceL%2C%20achieving%20%2B14.5%25%2C%20%2B10.2%25%2C%20and%20%2B13.6%25%20F1-score%20improvements.%20Our%0Afeature-based%20approach%20for%20predicting%20task%20graphs%20from%20textual/video%20embeddings%0Ademonstrates%20emerging%20video%20understanding%20abilities.%20We%20also%20achieved%20top%0Aperformance%20on%20the%20procedure%20understanding%20benchmark%20on%20Ego-Exo4D%20and%0Asignificantly%20improved%20online%20mistake%20detection%20%28%2B19.8%25%20on%20Assembly101-O%2C%20%2B6.4%25%0Aon%20EPIC-Tent-O%29.%20Code%3A%0Ahttps%3A//github.com/fpv-iplab/Differentiable-Task-Graph-Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17753v2&entry.124074799=Read"},
{"title": "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes", "author": "Sepehr Dehdashtian and Gautam Sreekumar and Vishnu Naresh Boddeti", "abstract": "  Images generated by text-to-image (T2I) models often exhibit visual biases\nand stereotypes of concepts such as culture and profession. Existing\nquantitative measures of stereotypes are based on statistical parity that does\nnot align with the sociological definition of stereotypes and, therefore,\nincorrectly categorizes biases as stereotypes. Instead of oversimplifying\nstereotypes as biases, we propose a quantitative measure of stereotypes that\naligns with its sociological definition. We then propose OASIS to measure the\nstereotypes in a generated dataset and understand their origins within the T2I\nmodel. OASIS includes two scores to measure stereotypes from a generated image\ndataset: (M1) Stereotype Score to measure the distributional violation of\nstereotypical attributes, and (M2) WALS to measure spectral variance in the\nimages along a stereotypical attribute. OASIS also includes two methods to\nunderstand the origins of stereotypes in T2I models: (U1) StOP to discover\nattributes that the T2I model internally associates with a given concept, and\n(U2) SPI to quantify the emergence of stereotypical attributes in the latent\nspace of the T2I model during image generation. Despite the considerable\nprogress in image fidelity, using OASIS, we conclude that newer T2I models such\nas FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts\nand still generate images with widespread stereotypical attributes.\nAdditionally, the quantity of stereotypes worsens for nationalities with lower\nInternet footprints.\n", "link": "http://arxiv.org/abs/2501.00962v2", "date": "2025-02-26", "relevancy": 1.8665, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4854}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OASIS%20Uncovers%3A%20High-Quality%20T2I%20Models%2C%20Same%20Old%20Stereotypes&body=Title%3A%20OASIS%20Uncovers%3A%20High-Quality%20T2I%20Models%2C%20Same%20Old%20Stereotypes%0AAuthor%3A%20Sepehr%20Dehdashtian%20and%20Gautam%20Sreekumar%20and%20Vishnu%20Naresh%20Boddeti%0AAbstract%3A%20%20%20Images%20generated%20by%20text-to-image%20%28T2I%29%20models%20often%20exhibit%20visual%20biases%0Aand%20stereotypes%20of%20concepts%20such%20as%20culture%20and%20profession.%20Existing%0Aquantitative%20measures%20of%20stereotypes%20are%20based%20on%20statistical%20parity%20that%20does%0Anot%20align%20with%20the%20sociological%20definition%20of%20stereotypes%20and%2C%20therefore%2C%0Aincorrectly%20categorizes%20biases%20as%20stereotypes.%20Instead%20of%20oversimplifying%0Astereotypes%20as%20biases%2C%20we%20propose%20a%20quantitative%20measure%20of%20stereotypes%20that%0Aaligns%20with%20its%20sociological%20definition.%20We%20then%20propose%20OASIS%20to%20measure%20the%0Astereotypes%20in%20a%20generated%20dataset%20and%20understand%20their%20origins%20within%20the%20T2I%0Amodel.%20OASIS%20includes%20two%20scores%20to%20measure%20stereotypes%20from%20a%20generated%20image%0Adataset%3A%20%28M1%29%20Stereotype%20Score%20to%20measure%20the%20distributional%20violation%20of%0Astereotypical%20attributes%2C%20and%20%28M2%29%20WALS%20to%20measure%20spectral%20variance%20in%20the%0Aimages%20along%20a%20stereotypical%20attribute.%20OASIS%20also%20includes%20two%20methods%20to%0Aunderstand%20the%20origins%20of%20stereotypes%20in%20T2I%20models%3A%20%28U1%29%20StOP%20to%20discover%0Aattributes%20that%20the%20T2I%20model%20internally%20associates%20with%20a%20given%20concept%2C%20and%0A%28U2%29%20SPI%20to%20quantify%20the%20emergence%20of%20stereotypical%20attributes%20in%20the%20latent%0Aspace%20of%20the%20T2I%20model%20during%20image%20generation.%20Despite%20the%20considerable%0Aprogress%20in%20image%20fidelity%2C%20using%20OASIS%2C%20we%20conclude%20that%20newer%20T2I%20models%20such%0Aas%20FLUX.1%20and%20SDv3%20contain%20strong%20stereotypical%20predispositions%20about%20concepts%0Aand%20still%20generate%20images%20with%20widespread%20stereotypical%20attributes.%0AAdditionally%2C%20the%20quantity%20of%20stereotypes%20worsens%20for%20nationalities%20with%20lower%0AInternet%20footprints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00962v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOASIS%2520Uncovers%253A%2520High-Quality%2520T2I%2520Models%252C%2520Same%2520Old%2520Stereotypes%26entry.906535625%3DSepehr%2520Dehdashtian%2520and%2520Gautam%2520Sreekumar%2520and%2520Vishnu%2520Naresh%2520Boddeti%26entry.1292438233%3D%2520%2520Images%2520generated%2520by%2520text-to-image%2520%2528T2I%2529%2520models%2520often%2520exhibit%2520visual%2520biases%250Aand%2520stereotypes%2520of%2520concepts%2520such%2520as%2520culture%2520and%2520profession.%2520Existing%250Aquantitative%2520measures%2520of%2520stereotypes%2520are%2520based%2520on%2520statistical%2520parity%2520that%2520does%250Anot%2520align%2520with%2520the%2520sociological%2520definition%2520of%2520stereotypes%2520and%252C%2520therefore%252C%250Aincorrectly%2520categorizes%2520biases%2520as%2520stereotypes.%2520Instead%2520of%2520oversimplifying%250Astereotypes%2520as%2520biases%252C%2520we%2520propose%2520a%2520quantitative%2520measure%2520of%2520stereotypes%2520that%250Aaligns%2520with%2520its%2520sociological%2520definition.%2520We%2520then%2520propose%2520OASIS%2520to%2520measure%2520the%250Astereotypes%2520in%2520a%2520generated%2520dataset%2520and%2520understand%2520their%2520origins%2520within%2520the%2520T2I%250Amodel.%2520OASIS%2520includes%2520two%2520scores%2520to%2520measure%2520stereotypes%2520from%2520a%2520generated%2520image%250Adataset%253A%2520%2528M1%2529%2520Stereotype%2520Score%2520to%2520measure%2520the%2520distributional%2520violation%2520of%250Astereotypical%2520attributes%252C%2520and%2520%2528M2%2529%2520WALS%2520to%2520measure%2520spectral%2520variance%2520in%2520the%250Aimages%2520along%2520a%2520stereotypical%2520attribute.%2520OASIS%2520also%2520includes%2520two%2520methods%2520to%250Aunderstand%2520the%2520origins%2520of%2520stereotypes%2520in%2520T2I%2520models%253A%2520%2528U1%2529%2520StOP%2520to%2520discover%250Aattributes%2520that%2520the%2520T2I%2520model%2520internally%2520associates%2520with%2520a%2520given%2520concept%252C%2520and%250A%2528U2%2529%2520SPI%2520to%2520quantify%2520the%2520emergence%2520of%2520stereotypical%2520attributes%2520in%2520the%2520latent%250Aspace%2520of%2520the%2520T2I%2520model%2520during%2520image%2520generation.%2520Despite%2520the%2520considerable%250Aprogress%2520in%2520image%2520fidelity%252C%2520using%2520OASIS%252C%2520we%2520conclude%2520that%2520newer%2520T2I%2520models%2520such%250Aas%2520FLUX.1%2520and%2520SDv3%2520contain%2520strong%2520stereotypical%2520predispositions%2520about%2520concepts%250Aand%2520still%2520generate%2520images%2520with%2520widespread%2520stereotypical%2520attributes.%250AAdditionally%252C%2520the%2520quantity%2520of%2520stereotypes%2520worsens%2520for%2520nationalities%2520with%2520lower%250AInternet%2520footprints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00962v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OASIS%20Uncovers%3A%20High-Quality%20T2I%20Models%2C%20Same%20Old%20Stereotypes&entry.906535625=Sepehr%20Dehdashtian%20and%20Gautam%20Sreekumar%20and%20Vishnu%20Naresh%20Boddeti&entry.1292438233=%20%20Images%20generated%20by%20text-to-image%20%28T2I%29%20models%20often%20exhibit%20visual%20biases%0Aand%20stereotypes%20of%20concepts%20such%20as%20culture%20and%20profession.%20Existing%0Aquantitative%20measures%20of%20stereotypes%20are%20based%20on%20statistical%20parity%20that%20does%0Anot%20align%20with%20the%20sociological%20definition%20of%20stereotypes%20and%2C%20therefore%2C%0Aincorrectly%20categorizes%20biases%20as%20stereotypes.%20Instead%20of%20oversimplifying%0Astereotypes%20as%20biases%2C%20we%20propose%20a%20quantitative%20measure%20of%20stereotypes%20that%0Aaligns%20with%20its%20sociological%20definition.%20We%20then%20propose%20OASIS%20to%20measure%20the%0Astereotypes%20in%20a%20generated%20dataset%20and%20understand%20their%20origins%20within%20the%20T2I%0Amodel.%20OASIS%20includes%20two%20scores%20to%20measure%20stereotypes%20from%20a%20generated%20image%0Adataset%3A%20%28M1%29%20Stereotype%20Score%20to%20measure%20the%20distributional%20violation%20of%0Astereotypical%20attributes%2C%20and%20%28M2%29%20WALS%20to%20measure%20spectral%20variance%20in%20the%0Aimages%20along%20a%20stereotypical%20attribute.%20OASIS%20also%20includes%20two%20methods%20to%0Aunderstand%20the%20origins%20of%20stereotypes%20in%20T2I%20models%3A%20%28U1%29%20StOP%20to%20discover%0Aattributes%20that%20the%20T2I%20model%20internally%20associates%20with%20a%20given%20concept%2C%20and%0A%28U2%29%20SPI%20to%20quantify%20the%20emergence%20of%20stereotypical%20attributes%20in%20the%20latent%0Aspace%20of%20the%20T2I%20model%20during%20image%20generation.%20Despite%20the%20considerable%0Aprogress%20in%20image%20fidelity%2C%20using%20OASIS%2C%20we%20conclude%20that%20newer%20T2I%20models%20such%0Aas%20FLUX.1%20and%20SDv3%20contain%20strong%20stereotypical%20predispositions%20about%20concepts%0Aand%20still%20generate%20images%20with%20widespread%20stereotypical%20attributes.%0AAdditionally%2C%20the%20quantity%20of%20stereotypes%20worsens%20for%20nationalities%20with%20lower%0AInternet%20footprints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00962v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


