<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250213.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Self-Calibrating Gaussian Splatting for Large Field of View\n  Reconstruction", "author": "Youming Deng and Wenqi Xian and Guandao Yang and Leonidas Guibas and Gordon Wetzstein and Steve Marschner and Paul Debevec", "abstract": "  In this paper, we present a self-calibrating framework that jointly optimizes\ncamera parameters, lens distortion and 3D Gaussian representations, enabling\naccurate and efficient scene reconstruction. In particular, our technique\nenables high-quality scene reconstruction from Large field-of-view (FOV)\nimagery taken with wide-angle lenses, allowing the scene to be modeled from a\nsmaller number of images. Our approach introduces a novel method for modeling\ncomplex lens distortions using a hybrid network that combines invertible\nresidual networks with explicit grids. This design effectively regularizes the\noptimization process, achieving greater accuracy than conventional camera\nmodels. Additionally, we propose a cubemap-based resampling strategy to support\nlarge FOV images without sacrificing resolution or introducing distortion\nartifacts. Our method is compatible with the fast rasterization of Gaussian\nSplatting, adaptable to a wide variety of camera lens distortion, and\ndemonstrates state-of-the-art performance on both synthetic and real-world\ndatasets.\n", "link": "http://arxiv.org/abs/2502.09563v1", "date": "2025-02-13", "relevancy": 3.4247, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7517}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6517}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Calibrating%20Gaussian%20Splatting%20for%20Large%20Field%20of%20View%0A%20%20Reconstruction&body=Title%3A%20Self-Calibrating%20Gaussian%20Splatting%20for%20Large%20Field%20of%20View%0A%20%20Reconstruction%0AAuthor%3A%20Youming%20Deng%20and%20Wenqi%20Xian%20and%20Guandao%20Yang%20and%20Leonidas%20Guibas%20and%20Gordon%20Wetzstein%20and%20Steve%20Marschner%20and%20Paul%20Debevec%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20self-calibrating%20framework%20that%20jointly%20optimizes%0Acamera%20parameters%2C%20lens%20distortion%20and%203D%20Gaussian%20representations%2C%20enabling%0Aaccurate%20and%20efficient%20scene%20reconstruction.%20In%20particular%2C%20our%20technique%0Aenables%20high-quality%20scene%20reconstruction%20from%20Large%20field-of-view%20%28FOV%29%0Aimagery%20taken%20with%20wide-angle%20lenses%2C%20allowing%20the%20scene%20to%20be%20modeled%20from%20a%0Asmaller%20number%20of%20images.%20Our%20approach%20introduces%20a%20novel%20method%20for%20modeling%0Acomplex%20lens%20distortions%20using%20a%20hybrid%20network%20that%20combines%20invertible%0Aresidual%20networks%20with%20explicit%20grids.%20This%20design%20effectively%20regularizes%20the%0Aoptimization%20process%2C%20achieving%20greater%20accuracy%20than%20conventional%20camera%0Amodels.%20Additionally%2C%20we%20propose%20a%20cubemap-based%20resampling%20strategy%20to%20support%0Alarge%20FOV%20images%20without%20sacrificing%20resolution%20or%20introducing%20distortion%0Aartifacts.%20Our%20method%20is%20compatible%20with%20the%20fast%20rasterization%20of%20Gaussian%0ASplatting%2C%20adaptable%20to%20a%20wide%20variety%20of%20camera%20lens%20distortion%2C%20and%0Ademonstrates%20state-of-the-art%20performance%20on%20both%20synthetic%20and%20real-world%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Calibrating%2520Gaussian%2520Splatting%2520for%2520Large%2520Field%2520of%2520View%250A%2520%2520Reconstruction%26entry.906535625%3DYouming%2520Deng%2520and%2520Wenqi%2520Xian%2520and%2520Guandao%2520Yang%2520and%2520Leonidas%2520Guibas%2520and%2520Gordon%2520Wetzstein%2520and%2520Steve%2520Marschner%2520and%2520Paul%2520Debevec%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520self-calibrating%2520framework%2520that%2520jointly%2520optimizes%250Acamera%2520parameters%252C%2520lens%2520distortion%2520and%25203D%2520Gaussian%2520representations%252C%2520enabling%250Aaccurate%2520and%2520efficient%2520scene%2520reconstruction.%2520In%2520particular%252C%2520our%2520technique%250Aenables%2520high-quality%2520scene%2520reconstruction%2520from%2520Large%2520field-of-view%2520%2528FOV%2529%250Aimagery%2520taken%2520with%2520wide-angle%2520lenses%252C%2520allowing%2520the%2520scene%2520to%2520be%2520modeled%2520from%2520a%250Asmaller%2520number%2520of%2520images.%2520Our%2520approach%2520introduces%2520a%2520novel%2520method%2520for%2520modeling%250Acomplex%2520lens%2520distortions%2520using%2520a%2520hybrid%2520network%2520that%2520combines%2520invertible%250Aresidual%2520networks%2520with%2520explicit%2520grids.%2520This%2520design%2520effectively%2520regularizes%2520the%250Aoptimization%2520process%252C%2520achieving%2520greater%2520accuracy%2520than%2520conventional%2520camera%250Amodels.%2520Additionally%252C%2520we%2520propose%2520a%2520cubemap-based%2520resampling%2520strategy%2520to%2520support%250Alarge%2520FOV%2520images%2520without%2520sacrificing%2520resolution%2520or%2520introducing%2520distortion%250Aartifacts.%2520Our%2520method%2520is%2520compatible%2520with%2520the%2520fast%2520rasterization%2520of%2520Gaussian%250ASplatting%252C%2520adaptable%2520to%2520a%2520wide%2520variety%2520of%2520camera%2520lens%2520distortion%252C%2520and%250Ademonstrates%2520state-of-the-art%2520performance%2520on%2520both%2520synthetic%2520and%2520real-world%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Calibrating%20Gaussian%20Splatting%20for%20Large%20Field%20of%20View%0A%20%20Reconstruction&entry.906535625=Youming%20Deng%20and%20Wenqi%20Xian%20and%20Guandao%20Yang%20and%20Leonidas%20Guibas%20and%20Gordon%20Wetzstein%20and%20Steve%20Marschner%20and%20Paul%20Debevec&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20self-calibrating%20framework%20that%20jointly%20optimizes%0Acamera%20parameters%2C%20lens%20distortion%20and%203D%20Gaussian%20representations%2C%20enabling%0Aaccurate%20and%20efficient%20scene%20reconstruction.%20In%20particular%2C%20our%20technique%0Aenables%20high-quality%20scene%20reconstruction%20from%20Large%20field-of-view%20%28FOV%29%0Aimagery%20taken%20with%20wide-angle%20lenses%2C%20allowing%20the%20scene%20to%20be%20modeled%20from%20a%0Asmaller%20number%20of%20images.%20Our%20approach%20introduces%20a%20novel%20method%20for%20modeling%0Acomplex%20lens%20distortions%20using%20a%20hybrid%20network%20that%20combines%20invertible%0Aresidual%20networks%20with%20explicit%20grids.%20This%20design%20effectively%20regularizes%20the%0Aoptimization%20process%2C%20achieving%20greater%20accuracy%20than%20conventional%20camera%0Amodels.%20Additionally%2C%20we%20propose%20a%20cubemap-based%20resampling%20strategy%20to%20support%0Alarge%20FOV%20images%20without%20sacrificing%20resolution%20or%20introducing%20distortion%0Aartifacts.%20Our%20method%20is%20compatible%20with%20the%20fast%20rasterization%20of%20Gaussian%0ASplatting%2C%20adaptable%20to%20a%20wide%20variety%20of%20camera%20lens%20distortion%2C%20and%0Ademonstrates%20state-of-the-art%20performance%20on%20both%20synthetic%20and%20real-world%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09563v1&entry.124074799=Read"},
{"title": "4-LEGS: 4D Language Embedded Gaussian Splatting", "author": "Gal Fiebelman and Tamir Cohen and Ayellet Morgenstern and Peter Hedman and Hadar Averbuch-Elor", "abstract": "  The emergence of neural representations has revolutionized our means for\ndigitally viewing a wide range of 3D scenes, enabling the synthesis of\nphotorealistic images rendered from novel views. Recently, several techniques\nhave been proposed for connecting these low-level representations with the\nhigh-level semantics understanding embodied within the scene. These methods\nelevate the rich semantic understanding from 2D imagery to 3D representations,\ndistilling high-dimensional spatial features onto 3D space. In our work, we are\ninterested in connecting language with a dynamic modeling of the world. We show\nhow to lift spatio-temporal features to a 4D representation based on 3D\nGaussian Splatting. This enables an interactive interface where the user can\nspatiotemporally localize events in the video from text prompts. We demonstrate\nour system on public 3D video datasets of people and animals performing various\nactions.\n", "link": "http://arxiv.org/abs/2410.10719v3", "date": "2025-02-13", "relevancy": 3.2972, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.679}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6639}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204-LEGS%3A%204D%20Language%20Embedded%20Gaussian%20Splatting&body=Title%3A%204-LEGS%3A%204D%20Language%20Embedded%20Gaussian%20Splatting%0AAuthor%3A%20Gal%20Fiebelman%20and%20Tamir%20Cohen%20and%20Ayellet%20Morgenstern%20and%20Peter%20Hedman%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20%20%20The%20emergence%20of%20neural%20representations%20has%20revolutionized%20our%20means%20for%0Adigitally%20viewing%20a%20wide%20range%20of%203D%20scenes%2C%20enabling%20the%20synthesis%20of%0Aphotorealistic%20images%20rendered%20from%20novel%20views.%20Recently%2C%20several%20techniques%0Ahave%20been%20proposed%20for%20connecting%20these%20low-level%20representations%20with%20the%0Ahigh-level%20semantics%20understanding%20embodied%20within%20the%20scene.%20These%20methods%0Aelevate%20the%20rich%20semantic%20understanding%20from%202D%20imagery%20to%203D%20representations%2C%0Adistilling%20high-dimensional%20spatial%20features%20onto%203D%20space.%20In%20our%20work%2C%20we%20are%0Ainterested%20in%20connecting%20language%20with%20a%20dynamic%20modeling%20of%20the%20world.%20We%20show%0Ahow%20to%20lift%20spatio-temporal%20features%20to%20a%204D%20representation%20based%20on%203D%0AGaussian%20Splatting.%20This%20enables%20an%20interactive%20interface%20where%20the%20user%20can%0Aspatiotemporally%20localize%20events%20in%20the%20video%20from%20text%20prompts.%20We%20demonstrate%0Aour%20system%20on%20public%203D%20video%20datasets%20of%20people%20and%20animals%20performing%20various%0Aactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10719v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4-LEGS%253A%25204D%2520Language%2520Embedded%2520Gaussian%2520Splatting%26entry.906535625%3DGal%2520Fiebelman%2520and%2520Tamir%2520Cohen%2520and%2520Ayellet%2520Morgenstern%2520and%2520Peter%2520Hedman%2520and%2520Hadar%2520Averbuch-Elor%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520neural%2520representations%2520has%2520revolutionized%2520our%2520means%2520for%250Adigitally%2520viewing%2520a%2520wide%2520range%2520of%25203D%2520scenes%252C%2520enabling%2520the%2520synthesis%2520of%250Aphotorealistic%2520images%2520rendered%2520from%2520novel%2520views.%2520Recently%252C%2520several%2520techniques%250Ahave%2520been%2520proposed%2520for%2520connecting%2520these%2520low-level%2520representations%2520with%2520the%250Ahigh-level%2520semantics%2520understanding%2520embodied%2520within%2520the%2520scene.%2520These%2520methods%250Aelevate%2520the%2520rich%2520semantic%2520understanding%2520from%25202D%2520imagery%2520to%25203D%2520representations%252C%250Adistilling%2520high-dimensional%2520spatial%2520features%2520onto%25203D%2520space.%2520In%2520our%2520work%252C%2520we%2520are%250Ainterested%2520in%2520connecting%2520language%2520with%2520a%2520dynamic%2520modeling%2520of%2520the%2520world.%2520We%2520show%250Ahow%2520to%2520lift%2520spatio-temporal%2520features%2520to%2520a%25204D%2520representation%2520based%2520on%25203D%250AGaussian%2520Splatting.%2520This%2520enables%2520an%2520interactive%2520interface%2520where%2520the%2520user%2520can%250Aspatiotemporally%2520localize%2520events%2520in%2520the%2520video%2520from%2520text%2520prompts.%2520We%2520demonstrate%250Aour%2520system%2520on%2520public%25203D%2520video%2520datasets%2520of%2520people%2520and%2520animals%2520performing%2520various%250Aactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10719v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4-LEGS%3A%204D%20Language%20Embedded%20Gaussian%20Splatting&entry.906535625=Gal%20Fiebelman%20and%20Tamir%20Cohen%20and%20Ayellet%20Morgenstern%20and%20Peter%20Hedman%20and%20Hadar%20Averbuch-Elor&entry.1292438233=%20%20The%20emergence%20of%20neural%20representations%20has%20revolutionized%20our%20means%20for%0Adigitally%20viewing%20a%20wide%20range%20of%203D%20scenes%2C%20enabling%20the%20synthesis%20of%0Aphotorealistic%20images%20rendered%20from%20novel%20views.%20Recently%2C%20several%20techniques%0Ahave%20been%20proposed%20for%20connecting%20these%20low-level%20representations%20with%20the%0Ahigh-level%20semantics%20understanding%20embodied%20within%20the%20scene.%20These%20methods%0Aelevate%20the%20rich%20semantic%20understanding%20from%202D%20imagery%20to%203D%20representations%2C%0Adistilling%20high-dimensional%20spatial%20features%20onto%203D%20space.%20In%20our%20work%2C%20we%20are%0Ainterested%20in%20connecting%20language%20with%20a%20dynamic%20modeling%20of%20the%20world.%20We%20show%0Ahow%20to%20lift%20spatio-temporal%20features%20to%20a%204D%20representation%20based%20on%203D%0AGaussian%20Splatting.%20This%20enables%20an%20interactive%20interface%20where%20the%20user%20can%0Aspatiotemporally%20localize%20events%20in%20the%20video%20from%20text%20prompts.%20We%20demonstrate%0Aour%20system%20on%20public%203D%20video%20datasets%20of%20people%20and%20animals%20performing%20various%0Aactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10719v3&entry.124074799=Read"},
{"title": "LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback\n  Over Multi-Resolution Gaussians-on-Mesh", "author": "Jing Wen and Alexander G. Schwing and Shenlong Wang", "abstract": "  Generalizable rendering of an animatable human avatar from sparse inputs\nrelies on data priors and inductive biases extracted from training on large\ndata to avoid scene-specific optimization and to enable fast reconstruction.\nThis raises two main challenges: First, unlike iterative gradient-based\nadjustment in scene-specific optimization, generalizable methods must\nreconstruct the human shape representation in a single pass at inference time.\nSecond, rendering is preferably computationally efficient yet of high\nresolution. To address both challenges we augment the recently proposed dual\nshape representation, which combines the benefits of a mesh and Gaussian\npoints, in two ways. To improve reconstruction, we propose an iterative\nfeedback update framework, which successively improves the canonical human\nshape representation during reconstruction. To achieve computationally\nefficient yet high-resolution rendering, we study a coupled-multi-resolution\nGaussians-on-Mesh representation. We evaluate the proposed approach on the\nchallenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an\nanimatable representation from sparse inputs in less than 1s, renders views\nwith 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of\n24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in\nrendering quality.\n", "link": "http://arxiv.org/abs/2502.09617v1", "date": "2025-02-13", "relevancy": 3.2699, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6633}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.651}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIFe-GoM%3A%20Generalizable%20Human%20Rendering%20with%20Learned%20Iterative%20Feedback%0A%20%20Over%20Multi-Resolution%20Gaussians-on-Mesh&body=Title%3A%20LIFe-GoM%3A%20Generalizable%20Human%20Rendering%20with%20Learned%20Iterative%20Feedback%0A%20%20Over%20Multi-Resolution%20Gaussians-on-Mesh%0AAuthor%3A%20Jing%20Wen%20and%20Alexander%20G.%20Schwing%20and%20Shenlong%20Wang%0AAbstract%3A%20%20%20Generalizable%20rendering%20of%20an%20animatable%20human%20avatar%20from%20sparse%20inputs%0Arelies%20on%20data%20priors%20and%20inductive%20biases%20extracted%20from%20training%20on%20large%0Adata%20to%20avoid%20scene-specific%20optimization%20and%20to%20enable%20fast%20reconstruction.%0AThis%20raises%20two%20main%20challenges%3A%20First%2C%20unlike%20iterative%20gradient-based%0Aadjustment%20in%20scene-specific%20optimization%2C%20generalizable%20methods%20must%0Areconstruct%20the%20human%20shape%20representation%20in%20a%20single%20pass%20at%20inference%20time.%0ASecond%2C%20rendering%20is%20preferably%20computationally%20efficient%20yet%20of%20high%0Aresolution.%20To%20address%20both%20challenges%20we%20augment%20the%20recently%20proposed%20dual%0Ashape%20representation%2C%20which%20combines%20the%20benefits%20of%20a%20mesh%20and%20Gaussian%0Apoints%2C%20in%20two%20ways.%20To%20improve%20reconstruction%2C%20we%20propose%20an%20iterative%0Afeedback%20update%20framework%2C%20which%20successively%20improves%20the%20canonical%20human%0Ashape%20representation%20during%20reconstruction.%20To%20achieve%20computationally%0Aefficient%20yet%20high-resolution%20rendering%2C%20we%20study%20a%20coupled-multi-resolution%0AGaussians-on-Mesh%20representation.%20We%20evaluate%20the%20proposed%20approach%20on%20the%0Achallenging%20THuman2.0%2C%20XHuman%20and%20AIST%2B%2B%20data.%20Our%20approach%20reconstructs%20an%0Aanimatable%20representation%20from%20sparse%20inputs%20in%20less%20than%201s%2C%20renders%20views%0Awith%2095.1FPS%20at%20%241024%20%5Ctimes%201024%24%2C%20and%20achieves%20PSNR/LPIPS%2A/FID%20of%0A24.65/110.82/51.27%20on%20THuman2.0%2C%20outperforming%20the%20state-of-the-art%20in%0Arendering%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIFe-GoM%253A%2520Generalizable%2520Human%2520Rendering%2520with%2520Learned%2520Iterative%2520Feedback%250A%2520%2520Over%2520Multi-Resolution%2520Gaussians-on-Mesh%26entry.906535625%3DJing%2520Wen%2520and%2520Alexander%2520G.%2520Schwing%2520and%2520Shenlong%2520Wang%26entry.1292438233%3D%2520%2520Generalizable%2520rendering%2520of%2520an%2520animatable%2520human%2520avatar%2520from%2520sparse%2520inputs%250Arelies%2520on%2520data%2520priors%2520and%2520inductive%2520biases%2520extracted%2520from%2520training%2520on%2520large%250Adata%2520to%2520avoid%2520scene-specific%2520optimization%2520and%2520to%2520enable%2520fast%2520reconstruction.%250AThis%2520raises%2520two%2520main%2520challenges%253A%2520First%252C%2520unlike%2520iterative%2520gradient-based%250Aadjustment%2520in%2520scene-specific%2520optimization%252C%2520generalizable%2520methods%2520must%250Areconstruct%2520the%2520human%2520shape%2520representation%2520in%2520a%2520single%2520pass%2520at%2520inference%2520time.%250ASecond%252C%2520rendering%2520is%2520preferably%2520computationally%2520efficient%2520yet%2520of%2520high%250Aresolution.%2520To%2520address%2520both%2520challenges%2520we%2520augment%2520the%2520recently%2520proposed%2520dual%250Ashape%2520representation%252C%2520which%2520combines%2520the%2520benefits%2520of%2520a%2520mesh%2520and%2520Gaussian%250Apoints%252C%2520in%2520two%2520ways.%2520To%2520improve%2520reconstruction%252C%2520we%2520propose%2520an%2520iterative%250Afeedback%2520update%2520framework%252C%2520which%2520successively%2520improves%2520the%2520canonical%2520human%250Ashape%2520representation%2520during%2520reconstruction.%2520To%2520achieve%2520computationally%250Aefficient%2520yet%2520high-resolution%2520rendering%252C%2520we%2520study%2520a%2520coupled-multi-resolution%250AGaussians-on-Mesh%2520representation.%2520We%2520evaluate%2520the%2520proposed%2520approach%2520on%2520the%250Achallenging%2520THuman2.0%252C%2520XHuman%2520and%2520AIST%252B%252B%2520data.%2520Our%2520approach%2520reconstructs%2520an%250Aanimatable%2520representation%2520from%2520sparse%2520inputs%2520in%2520less%2520than%25201s%252C%2520renders%2520views%250Awith%252095.1FPS%2520at%2520%25241024%2520%255Ctimes%25201024%2524%252C%2520and%2520achieves%2520PSNR/LPIPS%252A/FID%2520of%250A24.65/110.82/51.27%2520on%2520THuman2.0%252C%2520outperforming%2520the%2520state-of-the-art%2520in%250Arendering%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIFe-GoM%3A%20Generalizable%20Human%20Rendering%20with%20Learned%20Iterative%20Feedback%0A%20%20Over%20Multi-Resolution%20Gaussians-on-Mesh&entry.906535625=Jing%20Wen%20and%20Alexander%20G.%20Schwing%20and%20Shenlong%20Wang&entry.1292438233=%20%20Generalizable%20rendering%20of%20an%20animatable%20human%20avatar%20from%20sparse%20inputs%0Arelies%20on%20data%20priors%20and%20inductive%20biases%20extracted%20from%20training%20on%20large%0Adata%20to%20avoid%20scene-specific%20optimization%20and%20to%20enable%20fast%20reconstruction.%0AThis%20raises%20two%20main%20challenges%3A%20First%2C%20unlike%20iterative%20gradient-based%0Aadjustment%20in%20scene-specific%20optimization%2C%20generalizable%20methods%20must%0Areconstruct%20the%20human%20shape%20representation%20in%20a%20single%20pass%20at%20inference%20time.%0ASecond%2C%20rendering%20is%20preferably%20computationally%20efficient%20yet%20of%20high%0Aresolution.%20To%20address%20both%20challenges%20we%20augment%20the%20recently%20proposed%20dual%0Ashape%20representation%2C%20which%20combines%20the%20benefits%20of%20a%20mesh%20and%20Gaussian%0Apoints%2C%20in%20two%20ways.%20To%20improve%20reconstruction%2C%20we%20propose%20an%20iterative%0Afeedback%20update%20framework%2C%20which%20successively%20improves%20the%20canonical%20human%0Ashape%20representation%20during%20reconstruction.%20To%20achieve%20computationally%0Aefficient%20yet%20high-resolution%20rendering%2C%20we%20study%20a%20coupled-multi-resolution%0AGaussians-on-Mesh%20representation.%20We%20evaluate%20the%20proposed%20approach%20on%20the%0Achallenging%20THuman2.0%2C%20XHuman%20and%20AIST%2B%2B%20data.%20Our%20approach%20reconstructs%20an%0Aanimatable%20representation%20from%20sparse%20inputs%20in%20less%20than%201s%2C%20renders%20views%0Awith%2095.1FPS%20at%20%241024%20%5Ctimes%201024%24%2C%20and%20achieves%20PSNR/LPIPS%2A/FID%20of%0A24.65/110.82/51.27%20on%20THuman2.0%2C%20outperforming%20the%20state-of-the-art%20in%0Arendering%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09617v1&entry.124074799=Read"},
{"title": "Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection", "author": "Hongru Yan and Yu Zheng and Yueqi Duan", "abstract": "  Skins wrapping around our bodies, leathers covering over the sofa, sheet\nmetal coating the car - it suggests that objects are enclosed by a series of\ncontinuous surfaces, which provides us with informative geometry prior for\nobjectness deduction. In this paper, we propose Gaussian-Det which leverages\nGaussian Splatting as surface representation for multi-view based 3D object\ndetection. Unlike existing monocular or NeRF-based methods which depict the\nobjects via discrete positional data, Gaussian-Det models the objects in a\ncontinuous manner by formulating the input Gaussians as feature descriptors on\na mass of partial surfaces. Furthermore, to address the numerous outliers\ninherently introduced by Gaussian splatting, we accordingly devise a Closure\nInferring Module (CIM) for the comprehensive surface-based objectness\ndeduction. CIM firstly estimates the probabilistic feature residuals for\npartial surfaces given the underdetermined nature of Gaussian Splatting, which\nare then coalesced into a holistic representation on the overall surface\nclosure of the object proposal. In this way, the surface information\nGaussian-Det exploits serves as the prior on the quality and reliability of\nobjectness and the information basis of proposal refinement. Experiments on\nboth synthetic and real-world datasets demonstrate that Gaussian-Det\noutperforms various existing approaches, in terms of both average precision and\nrecall.\n", "link": "http://arxiv.org/abs/2410.01404v2", "date": "2025-02-13", "relevancy": 3.2071, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.664}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6354}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian-Det%3A%20Learning%20Closed-Surface%20Gaussians%20for%203D%20Object%20Detection&body=Title%3A%20Gaussian-Det%3A%20Learning%20Closed-Surface%20Gaussians%20for%203D%20Object%20Detection%0AAuthor%3A%20Hongru%20Yan%20and%20Yu%20Zheng%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20Skins%20wrapping%20around%20our%20bodies%2C%20leathers%20covering%20over%20the%20sofa%2C%20sheet%0Ametal%20coating%20the%20car%20-%20it%20suggests%20that%20objects%20are%20enclosed%20by%20a%20series%20of%0Acontinuous%20surfaces%2C%20which%20provides%20us%20with%20informative%20geometry%20prior%20for%0Aobjectness%20deduction.%20In%20this%20paper%2C%20we%20propose%20Gaussian-Det%20which%20leverages%0AGaussian%20Splatting%20as%20surface%20representation%20for%20multi-view%20based%203D%20object%0Adetection.%20Unlike%20existing%20monocular%20or%20NeRF-based%20methods%20which%20depict%20the%0Aobjects%20via%20discrete%20positional%20data%2C%20Gaussian-Det%20models%20the%20objects%20in%20a%0Acontinuous%20manner%20by%20formulating%20the%20input%20Gaussians%20as%20feature%20descriptors%20on%0Aa%20mass%20of%20partial%20surfaces.%20Furthermore%2C%20to%20address%20the%20numerous%20outliers%0Ainherently%20introduced%20by%20Gaussian%20splatting%2C%20we%20accordingly%20devise%20a%20Closure%0AInferring%20Module%20%28CIM%29%20for%20the%20comprehensive%20surface-based%20objectness%0Adeduction.%20CIM%20firstly%20estimates%20the%20probabilistic%20feature%20residuals%20for%0Apartial%20surfaces%20given%20the%20underdetermined%20nature%20of%20Gaussian%20Splatting%2C%20which%0Aare%20then%20coalesced%20into%20a%20holistic%20representation%20on%20the%20overall%20surface%0Aclosure%20of%20the%20object%20proposal.%20In%20this%20way%2C%20the%20surface%20information%0AGaussian-Det%20exploits%20serves%20as%20the%20prior%20on%20the%20quality%20and%20reliability%20of%0Aobjectness%20and%20the%20information%20basis%20of%20proposal%20refinement.%20Experiments%20on%0Aboth%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20Gaussian-Det%0Aoutperforms%20various%20existing%20approaches%2C%20in%20terms%20of%20both%20average%20precision%20and%0Arecall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian-Det%253A%2520Learning%2520Closed-Surface%2520Gaussians%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DHongru%2520Yan%2520and%2520Yu%2520Zheng%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520Skins%2520wrapping%2520around%2520our%2520bodies%252C%2520leathers%2520covering%2520over%2520the%2520sofa%252C%2520sheet%250Ametal%2520coating%2520the%2520car%2520-%2520it%2520suggests%2520that%2520objects%2520are%2520enclosed%2520by%2520a%2520series%2520of%250Acontinuous%2520surfaces%252C%2520which%2520provides%2520us%2520with%2520informative%2520geometry%2520prior%2520for%250Aobjectness%2520deduction.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Gaussian-Det%2520which%2520leverages%250AGaussian%2520Splatting%2520as%2520surface%2520representation%2520for%2520multi-view%2520based%25203D%2520object%250Adetection.%2520Unlike%2520existing%2520monocular%2520or%2520NeRF-based%2520methods%2520which%2520depict%2520the%250Aobjects%2520via%2520discrete%2520positional%2520data%252C%2520Gaussian-Det%2520models%2520the%2520objects%2520in%2520a%250Acontinuous%2520manner%2520by%2520formulating%2520the%2520input%2520Gaussians%2520as%2520feature%2520descriptors%2520on%250Aa%2520mass%2520of%2520partial%2520surfaces.%2520Furthermore%252C%2520to%2520address%2520the%2520numerous%2520outliers%250Ainherently%2520introduced%2520by%2520Gaussian%2520splatting%252C%2520we%2520accordingly%2520devise%2520a%2520Closure%250AInferring%2520Module%2520%2528CIM%2529%2520for%2520the%2520comprehensive%2520surface-based%2520objectness%250Adeduction.%2520CIM%2520firstly%2520estimates%2520the%2520probabilistic%2520feature%2520residuals%2520for%250Apartial%2520surfaces%2520given%2520the%2520underdetermined%2520nature%2520of%2520Gaussian%2520Splatting%252C%2520which%250Aare%2520then%2520coalesced%2520into%2520a%2520holistic%2520representation%2520on%2520the%2520overall%2520surface%250Aclosure%2520of%2520the%2520object%2520proposal.%2520In%2520this%2520way%252C%2520the%2520surface%2520information%250AGaussian-Det%2520exploits%2520serves%2520as%2520the%2520prior%2520on%2520the%2520quality%2520and%2520reliability%2520of%250Aobjectness%2520and%2520the%2520information%2520basis%2520of%2520proposal%2520refinement.%2520Experiments%2520on%250Aboth%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520Gaussian-Det%250Aoutperforms%2520various%2520existing%2520approaches%252C%2520in%2520terms%2520of%2520both%2520average%2520precision%2520and%250Arecall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian-Det%3A%20Learning%20Closed-Surface%20Gaussians%20for%203D%20Object%20Detection&entry.906535625=Hongru%20Yan%20and%20Yu%20Zheng%20and%20Yueqi%20Duan&entry.1292438233=%20%20Skins%20wrapping%20around%20our%20bodies%2C%20leathers%20covering%20over%20the%20sofa%2C%20sheet%0Ametal%20coating%20the%20car%20-%20it%20suggests%20that%20objects%20are%20enclosed%20by%20a%20series%20of%0Acontinuous%20surfaces%2C%20which%20provides%20us%20with%20informative%20geometry%20prior%20for%0Aobjectness%20deduction.%20In%20this%20paper%2C%20we%20propose%20Gaussian-Det%20which%20leverages%0AGaussian%20Splatting%20as%20surface%20representation%20for%20multi-view%20based%203D%20object%0Adetection.%20Unlike%20existing%20monocular%20or%20NeRF-based%20methods%20which%20depict%20the%0Aobjects%20via%20discrete%20positional%20data%2C%20Gaussian-Det%20models%20the%20objects%20in%20a%0Acontinuous%20manner%20by%20formulating%20the%20input%20Gaussians%20as%20feature%20descriptors%20on%0Aa%20mass%20of%20partial%20surfaces.%20Furthermore%2C%20to%20address%20the%20numerous%20outliers%0Ainherently%20introduced%20by%20Gaussian%20splatting%2C%20we%20accordingly%20devise%20a%20Closure%0AInferring%20Module%20%28CIM%29%20for%20the%20comprehensive%20surface-based%20objectness%0Adeduction.%20CIM%20firstly%20estimates%20the%20probabilistic%20feature%20residuals%20for%0Apartial%20surfaces%20given%20the%20underdetermined%20nature%20of%20Gaussian%20Splatting%2C%20which%0Aare%20then%20coalesced%20into%20a%20holistic%20representation%20on%20the%20overall%20surface%0Aclosure%20of%20the%20object%20proposal.%20In%20this%20way%2C%20the%20surface%20information%0AGaussian-Det%20exploits%20serves%20as%20the%20prior%20on%20the%20quality%20and%20reliability%20of%0Aobjectness%20and%20the%20information%20basis%20of%20proposal%20refinement.%20Experiments%20on%0Aboth%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20Gaussian-Det%0Aoutperforms%20various%20existing%20approaches%2C%20in%20terms%20of%20both%20average%20precision%20and%0Arecall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01404v2&entry.124074799=Read"},
{"title": "Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D\n  Scenes", "author": "Jianqi Chen and Panwen Hu and Xiaojun Chang and Zhenwei Shi and Michael Kampffmeyer and Xiaodan Liang", "abstract": "  Recent advancements in human motion synthesis have focused on specific types\nof motions, such as human-scene interaction, locomotion or human-human\ninteraction, however, there is a lack of a unified system capable of generating\na diverse combination of motion types. In response, we introduce\nSitcom-Crafter, a comprehensive and extendable system for human motion\ngeneration in 3D space, which can be guided by extensive plot contexts to\nenhance workflow efficiency for anime and game designers. The system is\ncomprised of eight modules, three of which are dedicated to motion generation,\nwhile the remaining five are augmentation modules that ensure consistent fusion\nof motion sequences and system functionality. Central to the generation modules\nis our novel 3D scene-aware human-human interaction module, which addresses\ncollision issues by synthesizing implicit 3D Signed Distance Function (SDF)\npoints around motion spaces, thereby minimizing human-scene collisions without\nadditional data collection costs. Complementing this, our locomotion and\nhuman-scene interaction modules leverage existing methods to enrich the\nsystem's motion generation capabilities. Augmentation modules encompass plot\ncomprehension for command generation, motion synchronization for seamless\nintegration of different motion types, hand pose retrieval to enhance motion\nrealism, motion collision revision to prevent human collisions, and 3D\nretargeting to ensure visual fidelity. Experimental evaluations validate the\nsystem's ability to generate high-quality, diverse, and physically realistic\nmotions, underscoring its potential for advancing creative workflows. Project\npage: https://windvchen.github.io/Sitcom-Crafter.\n", "link": "http://arxiv.org/abs/2410.10790v2", "date": "2025-02-13", "relevancy": 3.0641, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6924}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5747}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sitcom-Crafter%3A%20A%20Plot-Driven%20Human%20Motion%20Generation%20System%20in%203D%0A%20%20Scenes&body=Title%3A%20Sitcom-Crafter%3A%20A%20Plot-Driven%20Human%20Motion%20Generation%20System%20in%203D%0A%20%20Scenes%0AAuthor%3A%20Jianqi%20Chen%20and%20Panwen%20Hu%20and%20Xiaojun%20Chang%20and%20Zhenwei%20Shi%20and%20Michael%20Kampffmeyer%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20human%20motion%20synthesis%20have%20focused%20on%20specific%20types%0Aof%20motions%2C%20such%20as%20human-scene%20interaction%2C%20locomotion%20or%20human-human%0Ainteraction%2C%20however%2C%20there%20is%20a%20lack%20of%20a%20unified%20system%20capable%20of%20generating%0Aa%20diverse%20combination%20of%20motion%20types.%20In%20response%2C%20we%20introduce%0ASitcom-Crafter%2C%20a%20comprehensive%20and%20extendable%20system%20for%20human%20motion%0Ageneration%20in%203D%20space%2C%20which%20can%20be%20guided%20by%20extensive%20plot%20contexts%20to%0Aenhance%20workflow%20efficiency%20for%20anime%20and%20game%20designers.%20The%20system%20is%0Acomprised%20of%20eight%20modules%2C%20three%20of%20which%20are%20dedicated%20to%20motion%20generation%2C%0Awhile%20the%20remaining%20five%20are%20augmentation%20modules%20that%20ensure%20consistent%20fusion%0Aof%20motion%20sequences%20and%20system%20functionality.%20Central%20to%20the%20generation%20modules%0Ais%20our%20novel%203D%20scene-aware%20human-human%20interaction%20module%2C%20which%20addresses%0Acollision%20issues%20by%20synthesizing%20implicit%203D%20Signed%20Distance%20Function%20%28SDF%29%0Apoints%20around%20motion%20spaces%2C%20thereby%20minimizing%20human-scene%20collisions%20without%0Aadditional%20data%20collection%20costs.%20Complementing%20this%2C%20our%20locomotion%20and%0Ahuman-scene%20interaction%20modules%20leverage%20existing%20methods%20to%20enrich%20the%0Asystem%27s%20motion%20generation%20capabilities.%20Augmentation%20modules%20encompass%20plot%0Acomprehension%20for%20command%20generation%2C%20motion%20synchronization%20for%20seamless%0Aintegration%20of%20different%20motion%20types%2C%20hand%20pose%20retrieval%20to%20enhance%20motion%0Arealism%2C%20motion%20collision%20revision%20to%20prevent%20human%20collisions%2C%20and%203D%0Aretargeting%20to%20ensure%20visual%20fidelity.%20Experimental%20evaluations%20validate%20the%0Asystem%27s%20ability%20to%20generate%20high-quality%2C%20diverse%2C%20and%20physically%20realistic%0Amotions%2C%20underscoring%20its%20potential%20for%20advancing%20creative%20workflows.%20Project%0Apage%3A%20https%3A//windvchen.github.io/Sitcom-Crafter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10790v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSitcom-Crafter%253A%2520A%2520Plot-Driven%2520Human%2520Motion%2520Generation%2520System%2520in%25203D%250A%2520%2520Scenes%26entry.906535625%3DJianqi%2520Chen%2520and%2520Panwen%2520Hu%2520and%2520Xiaojun%2520Chang%2520and%2520Zhenwei%2520Shi%2520and%2520Michael%2520Kampffmeyer%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520human%2520motion%2520synthesis%2520have%2520focused%2520on%2520specific%2520types%250Aof%2520motions%252C%2520such%2520as%2520human-scene%2520interaction%252C%2520locomotion%2520or%2520human-human%250Ainteraction%252C%2520however%252C%2520there%2520is%2520a%2520lack%2520of%2520a%2520unified%2520system%2520capable%2520of%2520generating%250Aa%2520diverse%2520combination%2520of%2520motion%2520types.%2520In%2520response%252C%2520we%2520introduce%250ASitcom-Crafter%252C%2520a%2520comprehensive%2520and%2520extendable%2520system%2520for%2520human%2520motion%250Ageneration%2520in%25203D%2520space%252C%2520which%2520can%2520be%2520guided%2520by%2520extensive%2520plot%2520contexts%2520to%250Aenhance%2520workflow%2520efficiency%2520for%2520anime%2520and%2520game%2520designers.%2520The%2520system%2520is%250Acomprised%2520of%2520eight%2520modules%252C%2520three%2520of%2520which%2520are%2520dedicated%2520to%2520motion%2520generation%252C%250Awhile%2520the%2520remaining%2520five%2520are%2520augmentation%2520modules%2520that%2520ensure%2520consistent%2520fusion%250Aof%2520motion%2520sequences%2520and%2520system%2520functionality.%2520Central%2520to%2520the%2520generation%2520modules%250Ais%2520our%2520novel%25203D%2520scene-aware%2520human-human%2520interaction%2520module%252C%2520which%2520addresses%250Acollision%2520issues%2520by%2520synthesizing%2520implicit%25203D%2520Signed%2520Distance%2520Function%2520%2528SDF%2529%250Apoints%2520around%2520motion%2520spaces%252C%2520thereby%2520minimizing%2520human-scene%2520collisions%2520without%250Aadditional%2520data%2520collection%2520costs.%2520Complementing%2520this%252C%2520our%2520locomotion%2520and%250Ahuman-scene%2520interaction%2520modules%2520leverage%2520existing%2520methods%2520to%2520enrich%2520the%250Asystem%2527s%2520motion%2520generation%2520capabilities.%2520Augmentation%2520modules%2520encompass%2520plot%250Acomprehension%2520for%2520command%2520generation%252C%2520motion%2520synchronization%2520for%2520seamless%250Aintegration%2520of%2520different%2520motion%2520types%252C%2520hand%2520pose%2520retrieval%2520to%2520enhance%2520motion%250Arealism%252C%2520motion%2520collision%2520revision%2520to%2520prevent%2520human%2520collisions%252C%2520and%25203D%250Aretargeting%2520to%2520ensure%2520visual%2520fidelity.%2520Experimental%2520evaluations%2520validate%2520the%250Asystem%2527s%2520ability%2520to%2520generate%2520high-quality%252C%2520diverse%252C%2520and%2520physically%2520realistic%250Amotions%252C%2520underscoring%2520its%2520potential%2520for%2520advancing%2520creative%2520workflows.%2520Project%250Apage%253A%2520https%253A//windvchen.github.io/Sitcom-Crafter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10790v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sitcom-Crafter%3A%20A%20Plot-Driven%20Human%20Motion%20Generation%20System%20in%203D%0A%20%20Scenes&entry.906535625=Jianqi%20Chen%20and%20Panwen%20Hu%20and%20Xiaojun%20Chang%20and%20Zhenwei%20Shi%20and%20Michael%20Kampffmeyer%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Recent%20advancements%20in%20human%20motion%20synthesis%20have%20focused%20on%20specific%20types%0Aof%20motions%2C%20such%20as%20human-scene%20interaction%2C%20locomotion%20or%20human-human%0Ainteraction%2C%20however%2C%20there%20is%20a%20lack%20of%20a%20unified%20system%20capable%20of%20generating%0Aa%20diverse%20combination%20of%20motion%20types.%20In%20response%2C%20we%20introduce%0ASitcom-Crafter%2C%20a%20comprehensive%20and%20extendable%20system%20for%20human%20motion%0Ageneration%20in%203D%20space%2C%20which%20can%20be%20guided%20by%20extensive%20plot%20contexts%20to%0Aenhance%20workflow%20efficiency%20for%20anime%20and%20game%20designers.%20The%20system%20is%0Acomprised%20of%20eight%20modules%2C%20three%20of%20which%20are%20dedicated%20to%20motion%20generation%2C%0Awhile%20the%20remaining%20five%20are%20augmentation%20modules%20that%20ensure%20consistent%20fusion%0Aof%20motion%20sequences%20and%20system%20functionality.%20Central%20to%20the%20generation%20modules%0Ais%20our%20novel%203D%20scene-aware%20human-human%20interaction%20module%2C%20which%20addresses%0Acollision%20issues%20by%20synthesizing%20implicit%203D%20Signed%20Distance%20Function%20%28SDF%29%0Apoints%20around%20motion%20spaces%2C%20thereby%20minimizing%20human-scene%20collisions%20without%0Aadditional%20data%20collection%20costs.%20Complementing%20this%2C%20our%20locomotion%20and%0Ahuman-scene%20interaction%20modules%20leverage%20existing%20methods%20to%20enrich%20the%0Asystem%27s%20motion%20generation%20capabilities.%20Augmentation%20modules%20encompass%20plot%0Acomprehension%20for%20command%20generation%2C%20motion%20synchronization%20for%20seamless%0Aintegration%20of%20different%20motion%20types%2C%20hand%20pose%20retrieval%20to%20enhance%20motion%0Arealism%2C%20motion%20collision%20revision%20to%20prevent%20human%20collisions%2C%20and%203D%0Aretargeting%20to%20ensure%20visual%20fidelity.%20Experimental%20evaluations%20validate%20the%0Asystem%27s%20ability%20to%20generate%20high-quality%2C%20diverse%2C%20and%20physically%20realistic%0Amotions%2C%20underscoring%20its%20potential%20for%20advancing%20creative%20workflows.%20Project%0Apage%3A%20https%3A//windvchen.github.io/Sitcom-Crafter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10790v2&entry.124074799=Read"},
{"title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos", "author": "Haobo Yuan and Xiangtai Li and Tao Zhang and Zilong Huang and Shilin Xu and Shunping Ji and Yunhai Tong and Lu Qi and Jiashi Feng and Ming-Hsuan Yang", "abstract": "  This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.\n", "link": "http://arxiv.org/abs/2501.04001v2", "date": "2025-02-13", "relevancy": 2.9548, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sa2VA%3A%20Marrying%20SAM2%20with%20LLaVA%20for%20Dense%20Grounded%20Understanding%20of%0A%20%20Images%20and%20Videos&body=Title%3A%20Sa2VA%3A%20Marrying%20SAM2%20with%20LLaVA%20for%20Dense%20Grounded%20Understanding%20of%0A%20%20Images%20and%20Videos%0AAuthor%3A%20Haobo%20Yuan%20and%20Xiangtai%20Li%20and%20Tao%20Zhang%20and%20Zilong%20Huang%20and%20Shilin%20Xu%20and%20Shunping%20Ji%20and%20Yunhai%20Tong%20and%20Lu%20Qi%20and%20Jiashi%20Feng%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20This%20work%20presents%20Sa2VA%2C%20the%20first%20unified%20model%20for%20dense%20grounded%0Aunderstanding%20of%20both%20images%20and%20videos.%20Unlike%20existing%20multi-modal%20large%0Alanguage%20models%2C%20which%20are%20often%20limited%20to%20specific%20modalities%20and%20tasks%2C%0ASa2VA%20supports%20a%20wide%20range%20of%20image%20and%20video%20tasks%2C%20including%20referring%0Asegmentation%20and%20conversation%2C%20with%20minimal%20one-shot%20instruction%20tuning.%20Sa2VA%0Acombines%20SAM-2%2C%20a%20foundation%20video%20segmentation%20model%2C%20with%20LLaVA%2C%20an%20advanced%0Avision-language%20model%2C%20and%20unifies%20text%2C%20image%2C%20and%20video%20into%20a%20shared%20LLM%0Atoken%20space.%20Using%20the%20LLM%2C%20Sa2VA%20generates%20instruction%20tokens%20that%20guide%20SAM-2%0Ain%20producing%20precise%20masks%2C%20enabling%20a%20grounded%2C%20multi-modal%20understanding%20of%0Aboth%20static%20and%20dynamic%20visual%20content.%20Additionally%2C%20we%20introduce%20Ref-SAV%2C%20an%0Aauto-labeled%20dataset%20containing%20over%2072k%20object%20expressions%20in%20complex%20video%0Ascenes%2C%20designed%20to%20boost%20model%20performance.%20We%20also%20manually%20validate%202k%20video%0Aobjects%20in%20the%20Ref-SAV%20datasets%20to%20benchmark%20referring%20video%20object%0Asegmentation%20in%20complex%20environments.%20Experiments%20show%20that%20Sa2VA%20achieves%0Astate-of-the-art%20across%20multiple%20tasks%2C%20particularly%20in%20referring%20video%20object%0Asegmentation%2C%20highlighting%20its%20potential%20for%20complex%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSa2VA%253A%2520Marrying%2520SAM2%2520with%2520LLaVA%2520for%2520Dense%2520Grounded%2520Understanding%2520of%250A%2520%2520Images%2520and%2520Videos%26entry.906535625%3DHaobo%2520Yuan%2520and%2520Xiangtai%2520Li%2520and%2520Tao%2520Zhang%2520and%2520Zilong%2520Huang%2520and%2520Shilin%2520Xu%2520and%2520Shunping%2520Ji%2520and%2520Yunhai%2520Tong%2520and%2520Lu%2520Qi%2520and%2520Jiashi%2520Feng%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520Sa2VA%252C%2520the%2520first%2520unified%2520model%2520for%2520dense%2520grounded%250Aunderstanding%2520of%2520both%2520images%2520and%2520videos.%2520Unlike%2520existing%2520multi-modal%2520large%250Alanguage%2520models%252C%2520which%2520are%2520often%2520limited%2520to%2520specific%2520modalities%2520and%2520tasks%252C%250ASa2VA%2520supports%2520a%2520wide%2520range%2520of%2520image%2520and%2520video%2520tasks%252C%2520including%2520referring%250Asegmentation%2520and%2520conversation%252C%2520with%2520minimal%2520one-shot%2520instruction%2520tuning.%2520Sa2VA%250Acombines%2520SAM-2%252C%2520a%2520foundation%2520video%2520segmentation%2520model%252C%2520with%2520LLaVA%252C%2520an%2520advanced%250Avision-language%2520model%252C%2520and%2520unifies%2520text%252C%2520image%252C%2520and%2520video%2520into%2520a%2520shared%2520LLM%250Atoken%2520space.%2520Using%2520the%2520LLM%252C%2520Sa2VA%2520generates%2520instruction%2520tokens%2520that%2520guide%2520SAM-2%250Ain%2520producing%2520precise%2520masks%252C%2520enabling%2520a%2520grounded%252C%2520multi-modal%2520understanding%2520of%250Aboth%2520static%2520and%2520dynamic%2520visual%2520content.%2520Additionally%252C%2520we%2520introduce%2520Ref-SAV%252C%2520an%250Aauto-labeled%2520dataset%2520containing%2520over%252072k%2520object%2520expressions%2520in%2520complex%2520video%250Ascenes%252C%2520designed%2520to%2520boost%2520model%2520performance.%2520We%2520also%2520manually%2520validate%25202k%2520video%250Aobjects%2520in%2520the%2520Ref-SAV%2520datasets%2520to%2520benchmark%2520referring%2520video%2520object%250Asegmentation%2520in%2520complex%2520environments.%2520Experiments%2520show%2520that%2520Sa2VA%2520achieves%250Astate-of-the-art%2520across%2520multiple%2520tasks%252C%2520particularly%2520in%2520referring%2520video%2520object%250Asegmentation%252C%2520highlighting%2520its%2520potential%2520for%2520complex%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sa2VA%3A%20Marrying%20SAM2%20with%20LLaVA%20for%20Dense%20Grounded%20Understanding%20of%0A%20%20Images%20and%20Videos&entry.906535625=Haobo%20Yuan%20and%20Xiangtai%20Li%20and%20Tao%20Zhang%20and%20Zilong%20Huang%20and%20Shilin%20Xu%20and%20Shunping%20Ji%20and%20Yunhai%20Tong%20and%20Lu%20Qi%20and%20Jiashi%20Feng%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20This%20work%20presents%20Sa2VA%2C%20the%20first%20unified%20model%20for%20dense%20grounded%0Aunderstanding%20of%20both%20images%20and%20videos.%20Unlike%20existing%20multi-modal%20large%0Alanguage%20models%2C%20which%20are%20often%20limited%20to%20specific%20modalities%20and%20tasks%2C%0ASa2VA%20supports%20a%20wide%20range%20of%20image%20and%20video%20tasks%2C%20including%20referring%0Asegmentation%20and%20conversation%2C%20with%20minimal%20one-shot%20instruction%20tuning.%20Sa2VA%0Acombines%20SAM-2%2C%20a%20foundation%20video%20segmentation%20model%2C%20with%20LLaVA%2C%20an%20advanced%0Avision-language%20model%2C%20and%20unifies%20text%2C%20image%2C%20and%20video%20into%20a%20shared%20LLM%0Atoken%20space.%20Using%20the%20LLM%2C%20Sa2VA%20generates%20instruction%20tokens%20that%20guide%20SAM-2%0Ain%20producing%20precise%20masks%2C%20enabling%20a%20grounded%2C%20multi-modal%20understanding%20of%0Aboth%20static%20and%20dynamic%20visual%20content.%20Additionally%2C%20we%20introduce%20Ref-SAV%2C%20an%0Aauto-labeled%20dataset%20containing%20over%2072k%20object%20expressions%20in%20complex%20video%0Ascenes%2C%20designed%20to%20boost%20model%20performance.%20We%20also%20manually%20validate%202k%20video%0Aobjects%20in%20the%20Ref-SAV%20datasets%20to%20benchmark%20referring%20video%20object%0Asegmentation%20in%20complex%20environments.%20Experiments%20show%20that%20Sa2VA%20achieves%0Astate-of-the-art%20across%20multiple%20tasks%2C%20particularly%20in%20referring%20video%20object%0Asegmentation%2C%20highlighting%20its%20potential%20for%20complex%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04001v2&entry.124074799=Read"},
{"title": "Latent Radiance Fields with 3D-aware 2D Representations", "author": "Chaoyi Zhou and Xi Liu and Feng Luo and Siyu Huang", "abstract": "  Latent 3D reconstruction has shown great promise in empowering 3D semantic\nunderstanding and 3D generation by distilling 2D features into the 3D space.\nHowever, existing approaches struggle with the domain gap between 2D feature\nspace and 3D representations, resulting in degraded rendering performance. To\naddress this challenge, we propose a novel framework that integrates 3D\nawareness into the 2D latent space. The framework consists of three stages: (1)\na correspondence-aware autoencoding method that enhances the 3D consistency of\n2D latent representations, (2) a latent radiance field (LRF) that lifts these\n3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field\n(VAE-RF) alignment strategy that improves image decoding from the rendered 2D\nrepresentations. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art latent 3D reconstruction approaches in terms of synthesis\nperformance and cross-dataset generalizability across diverse indoor and\noutdoor scenes. To our knowledge, this is the first work showing the radiance\nfield representations constructed from 2D latent representations can yield\nphotorealistic 3D reconstruction performance.\n", "link": "http://arxiv.org/abs/2502.09613v1", "date": "2025-02-13", "relevancy": 2.9503, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.59}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.59}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Radiance%20Fields%20with%203D-aware%202D%20Representations&body=Title%3A%20Latent%20Radiance%20Fields%20with%203D-aware%202D%20Representations%0AAuthor%3A%20Chaoyi%20Zhou%20and%20Xi%20Liu%20and%20Feng%20Luo%20and%20Siyu%20Huang%0AAbstract%3A%20%20%20Latent%203D%20reconstruction%20has%20shown%20great%20promise%20in%20empowering%203D%20semantic%0Aunderstanding%20and%203D%20generation%20by%20distilling%202D%20features%20into%20the%203D%20space.%0AHowever%2C%20existing%20approaches%20struggle%20with%20the%20domain%20gap%20between%202D%20feature%0Aspace%20and%203D%20representations%2C%20resulting%20in%20degraded%20rendering%20performance.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%20that%20integrates%203D%0Aawareness%20into%20the%202D%20latent%20space.%20The%20framework%20consists%20of%20three%20stages%3A%20%281%29%0Aa%20correspondence-aware%20autoencoding%20method%20that%20enhances%20the%203D%20consistency%20of%0A2D%20latent%20representations%2C%20%282%29%20a%20latent%20radiance%20field%20%28LRF%29%20that%20lifts%20these%0A3D-aware%202D%20representations%20into%203D%20space%2C%20and%20%283%29%20a%20VAE-Radiance%20Field%0A%28VAE-RF%29%20alignment%20strategy%20that%20improves%20image%20decoding%20from%20the%20rendered%202D%0Arepresentations.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Athe%20state-of-the-art%20latent%203D%20reconstruction%20approaches%20in%20terms%20of%20synthesis%0Aperformance%20and%20cross-dataset%20generalizability%20across%20diverse%20indoor%20and%0Aoutdoor%20scenes.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20showing%20the%20radiance%0Afield%20representations%20constructed%20from%202D%20latent%20representations%20can%20yield%0Aphotorealistic%203D%20reconstruction%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Radiance%2520Fields%2520with%25203D-aware%25202D%2520Representations%26entry.906535625%3DChaoyi%2520Zhou%2520and%2520Xi%2520Liu%2520and%2520Feng%2520Luo%2520and%2520Siyu%2520Huang%26entry.1292438233%3D%2520%2520Latent%25203D%2520reconstruction%2520has%2520shown%2520great%2520promise%2520in%2520empowering%25203D%2520semantic%250Aunderstanding%2520and%25203D%2520generation%2520by%2520distilling%25202D%2520features%2520into%2520the%25203D%2520space.%250AHowever%252C%2520existing%2520approaches%2520struggle%2520with%2520the%2520domain%2520gap%2520between%25202D%2520feature%250Aspace%2520and%25203D%2520representations%252C%2520resulting%2520in%2520degraded%2520rendering%2520performance.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520integrates%25203D%250Aawareness%2520into%2520the%25202D%2520latent%2520space.%2520The%2520framework%2520consists%2520of%2520three%2520stages%253A%2520%25281%2529%250Aa%2520correspondence-aware%2520autoencoding%2520method%2520that%2520enhances%2520the%25203D%2520consistency%2520of%250A2D%2520latent%2520representations%252C%2520%25282%2529%2520a%2520latent%2520radiance%2520field%2520%2528LRF%2529%2520that%2520lifts%2520these%250A3D-aware%25202D%2520representations%2520into%25203D%2520space%252C%2520and%2520%25283%2529%2520a%2520VAE-Radiance%2520Field%250A%2528VAE-RF%2529%2520alignment%2520strategy%2520that%2520improves%2520image%2520decoding%2520from%2520the%2520rendered%25202D%250Arepresentations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Athe%2520state-of-the-art%2520latent%25203D%2520reconstruction%2520approaches%2520in%2520terms%2520of%2520synthesis%250Aperformance%2520and%2520cross-dataset%2520generalizability%2520across%2520diverse%2520indoor%2520and%250Aoutdoor%2520scenes.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520showing%2520the%2520radiance%250Afield%2520representations%2520constructed%2520from%25202D%2520latent%2520representations%2520can%2520yield%250Aphotorealistic%25203D%2520reconstruction%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Radiance%20Fields%20with%203D-aware%202D%20Representations&entry.906535625=Chaoyi%20Zhou%20and%20Xi%20Liu%20and%20Feng%20Luo%20and%20Siyu%20Huang&entry.1292438233=%20%20Latent%203D%20reconstruction%20has%20shown%20great%20promise%20in%20empowering%203D%20semantic%0Aunderstanding%20and%203D%20generation%20by%20distilling%202D%20features%20into%20the%203D%20space.%0AHowever%2C%20existing%20approaches%20struggle%20with%20the%20domain%20gap%20between%202D%20feature%0Aspace%20and%203D%20representations%2C%20resulting%20in%20degraded%20rendering%20performance.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%20that%20integrates%203D%0Aawareness%20into%20the%202D%20latent%20space.%20The%20framework%20consists%20of%20three%20stages%3A%20%281%29%0Aa%20correspondence-aware%20autoencoding%20method%20that%20enhances%20the%203D%20consistency%20of%0A2D%20latent%20representations%2C%20%282%29%20a%20latent%20radiance%20field%20%28LRF%29%20that%20lifts%20these%0A3D-aware%202D%20representations%20into%203D%20space%2C%20and%20%283%29%20a%20VAE-Radiance%20Field%0A%28VAE-RF%29%20alignment%20strategy%20that%20improves%20image%20decoding%20from%20the%20rendered%202D%0Arepresentations.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Athe%20state-of-the-art%20latent%203D%20reconstruction%20approaches%20in%20terms%20of%20synthesis%0Aperformance%20and%20cross-dataset%20generalizability%20across%20diverse%20indoor%20and%0Aoutdoor%20scenes.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20showing%20the%20radiance%0Afield%20representations%20constructed%20from%202D%20latent%20representations%20can%20yield%0Aphotorealistic%203D%20reconstruction%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09613v1&entry.124074799=Read"},
{"title": "ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation", "author": "Rotem Shalev-Arkushin and Rinon Gal and Amit H. Bermano and Ohad Fried", "abstract": "  Diffusion models enable high-quality and diverse visual content synthesis.\nHowever, they struggle to generate rare or unseen concepts. To address this\nchallenge, we explore the usage of Retrieval-Augmented Generation (RAG) with\nimage generation models. We propose ImageRAG, a method that dynamically\nretrieves relevant images based on a given text prompt, and uses them as\ncontext to guide the generation process. Prior approaches that used retrieved\nimages to improve generation, trained models specifically for retrieval-based\ngeneration. In contrast, ImageRAG leverages the capabilities of existing image\nconditioning models, and does not require RAG-specific training. Our approach\nis highly adaptable and can be applied across different model types, showing\nsignificant improvement in generating rare and fine-grained concepts using\ndifferent base models.\n  Our project page is available at: https://rotem-shalev.github.io/ImageRAG\n", "link": "http://arxiv.org/abs/2502.09411v1", "date": "2025-02-13", "relevancy": 2.9498, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5957}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5888}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImageRAG%3A%20Dynamic%20Image%20Retrieval%20for%20Reference-Guided%20Image%20Generation&body=Title%3A%20ImageRAG%3A%20Dynamic%20Image%20Retrieval%20for%20Reference-Guided%20Image%20Generation%0AAuthor%3A%20Rotem%20Shalev-Arkushin%20and%20Rinon%20Gal%20and%20Amit%20H.%20Bermano%20and%20Ohad%20Fried%0AAbstract%3A%20%20%20Diffusion%20models%20enable%20high-quality%20and%20diverse%20visual%20content%20synthesis.%0AHowever%2C%20they%20struggle%20to%20generate%20rare%20or%20unseen%20concepts.%20To%20address%20this%0Achallenge%2C%20we%20explore%20the%20usage%20of%20Retrieval-Augmented%20Generation%20%28RAG%29%20with%0Aimage%20generation%20models.%20We%20propose%20ImageRAG%2C%20a%20method%20that%20dynamically%0Aretrieves%20relevant%20images%20based%20on%20a%20given%20text%20prompt%2C%20and%20uses%20them%20as%0Acontext%20to%20guide%20the%20generation%20process.%20Prior%20approaches%20that%20used%20retrieved%0Aimages%20to%20improve%20generation%2C%20trained%20models%20specifically%20for%20retrieval-based%0Ageneration.%20In%20contrast%2C%20ImageRAG%20leverages%20the%20capabilities%20of%20existing%20image%0Aconditioning%20models%2C%20and%20does%20not%20require%20RAG-specific%20training.%20Our%20approach%0Ais%20highly%20adaptable%20and%20can%20be%20applied%20across%20different%20model%20types%2C%20showing%0Asignificant%20improvement%20in%20generating%20rare%20and%20fine-grained%20concepts%20using%0Adifferent%20base%20models.%0A%20%20Our%20project%20page%20is%20available%20at%3A%20https%3A//rotem-shalev.github.io/ImageRAG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImageRAG%253A%2520Dynamic%2520Image%2520Retrieval%2520for%2520Reference-Guided%2520Image%2520Generation%26entry.906535625%3DRotem%2520Shalev-Arkushin%2520and%2520Rinon%2520Gal%2520and%2520Amit%2520H.%2520Bermano%2520and%2520Ohad%2520Fried%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520enable%2520high-quality%2520and%2520diverse%2520visual%2520content%2520synthesis.%250AHowever%252C%2520they%2520struggle%2520to%2520generate%2520rare%2520or%2520unseen%2520concepts.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520explore%2520the%2520usage%2520of%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520with%250Aimage%2520generation%2520models.%2520We%2520propose%2520ImageRAG%252C%2520a%2520method%2520that%2520dynamically%250Aretrieves%2520relevant%2520images%2520based%2520on%2520a%2520given%2520text%2520prompt%252C%2520and%2520uses%2520them%2520as%250Acontext%2520to%2520guide%2520the%2520generation%2520process.%2520Prior%2520approaches%2520that%2520used%2520retrieved%250Aimages%2520to%2520improve%2520generation%252C%2520trained%2520models%2520specifically%2520for%2520retrieval-based%250Ageneration.%2520In%2520contrast%252C%2520ImageRAG%2520leverages%2520the%2520capabilities%2520of%2520existing%2520image%250Aconditioning%2520models%252C%2520and%2520does%2520not%2520require%2520RAG-specific%2520training.%2520Our%2520approach%250Ais%2520highly%2520adaptable%2520and%2520can%2520be%2520applied%2520across%2520different%2520model%2520types%252C%2520showing%250Asignificant%2520improvement%2520in%2520generating%2520rare%2520and%2520fine-grained%2520concepts%2520using%250Adifferent%2520base%2520models.%250A%2520%2520Our%2520project%2520page%2520is%2520available%2520at%253A%2520https%253A//rotem-shalev.github.io/ImageRAG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageRAG%3A%20Dynamic%20Image%20Retrieval%20for%20Reference-Guided%20Image%20Generation&entry.906535625=Rotem%20Shalev-Arkushin%20and%20Rinon%20Gal%20and%20Amit%20H.%20Bermano%20and%20Ohad%20Fried&entry.1292438233=%20%20Diffusion%20models%20enable%20high-quality%20and%20diverse%20visual%20content%20synthesis.%0AHowever%2C%20they%20struggle%20to%20generate%20rare%20or%20unseen%20concepts.%20To%20address%20this%0Achallenge%2C%20we%20explore%20the%20usage%20of%20Retrieval-Augmented%20Generation%20%28RAG%29%20with%0Aimage%20generation%20models.%20We%20propose%20ImageRAG%2C%20a%20method%20that%20dynamically%0Aretrieves%20relevant%20images%20based%20on%20a%20given%20text%20prompt%2C%20and%20uses%20them%20as%0Acontext%20to%20guide%20the%20generation%20process.%20Prior%20approaches%20that%20used%20retrieved%0Aimages%20to%20improve%20generation%2C%20trained%20models%20specifically%20for%20retrieval-based%0Ageneration.%20In%20contrast%2C%20ImageRAG%20leverages%20the%20capabilities%20of%20existing%20image%0Aconditioning%20models%2C%20and%20does%20not%20require%20RAG-specific%20training.%20Our%20approach%0Ais%20highly%20adaptable%20and%20can%20be%20applied%20across%20different%20model%20types%2C%20showing%0Asignificant%20improvement%20in%20generating%20rare%20and%20fine-grained%20concepts%20using%0Adifferent%20base%20models.%0A%20%20Our%20project%20page%20is%20available%20at%3A%20https%3A//rotem-shalev.github.io/ImageRAG%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09411v1&entry.124074799=Read"},
{"title": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output", "author": "Eason Chen and Chenyu Lin and Xinyi Tang and Aprille Xi and Canwen Wang and Jionghao Lin and Kenneth R Koedinger", "abstract": "  The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases.\n", "link": "http://arxiv.org/abs/2502.04103v2", "date": "2025-02-13", "relevancy": 2.8725, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5969}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5716}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VTutor%3A%20An%20Open-Source%20SDK%20for%20Generative%20AI-Powered%20Animated%0A%20%20Pedagogical%20Agents%20with%20Multi-Media%20Output&body=Title%3A%20VTutor%3A%20An%20Open-Source%20SDK%20for%20Generative%20AI-Powered%20Animated%0A%20%20Pedagogical%20Agents%20with%20Multi-Media%20Output%0AAuthor%3A%20Eason%20Chen%20and%20Chenyu%20Lin%20and%20Xinyi%20Tang%20and%20Aprille%20Xi%20and%20Canwen%20Wang%20and%20Jionghao%20Lin%20and%20Kenneth%20R%20Koedinger%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20large%20language%20models%20%28LLMs%29%20has%20transformed%0Ahuman-computer%20interaction%20%28HCI%29%2C%20but%20the%20interaction%20with%20LLMs%20is%20currently%0Amainly%20focused%20on%20text-based%20interactions%2C%20while%20other%20multi-model%20approaches%0Aremain%20under-explored.%20This%20paper%20introduces%20VTutor%2C%20an%20open-source%20Software%0ADevelopment%20Kit%20%28SDK%29%20that%20combines%20generative%20AI%20with%20advanced%20animation%0Atechnologies%20to%20create%20engaging%2C%20adaptable%2C%20and%20realistic%20APAs%20for%20human-AI%0Amulti-media%20interactions.%20VTutor%20leverages%20LLMs%20for%20real-time%20personalized%0Afeedback%2C%20advanced%20lip%20synchronization%20for%20natural%20speech%20alignment%2C%20and%20WebGL%0Arendering%20for%20seamless%20web%20integration.%20Supporting%20various%202D%20and%203D%20character%0Amodels%2C%20VTutor%20enables%20researchers%20and%20developers%20to%20design%20emotionally%0Aresonant%2C%20contextually%20adaptive%20learning%20agents.%20This%20toolkit%20enhances%20learner%0Aengagement%2C%20feedback%20receptivity%2C%20and%20human-AI%20interaction%20while%20promoting%0Atrustworthy%20AI%20principles%20in%20education.%20VTutor%20sets%20a%20new%20standard%20for%0Anext-generation%20APAs%2C%20offering%20an%20accessible%2C%20scalable%20solution%20for%20fostering%0Ameaningful%20and%20immersive%20human-AI%20interaction%20experiences.%20The%20VTutor%20project%0Ais%20open-sourced%20and%20welcomes%20community-driven%20contributions%20and%20showcases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVTutor%253A%2520An%2520Open-Source%2520SDK%2520for%2520Generative%2520AI-Powered%2520Animated%250A%2520%2520Pedagogical%2520Agents%2520with%2520Multi-Media%2520Output%26entry.906535625%3DEason%2520Chen%2520and%2520Chenyu%2520Lin%2520and%2520Xinyi%2520Tang%2520and%2520Aprille%2520Xi%2520and%2520Canwen%2520Wang%2520and%2520Jionghao%2520Lin%2520and%2520Kenneth%2520R%2520Koedinger%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520transformed%250Ahuman-computer%2520interaction%2520%2528HCI%2529%252C%2520but%2520the%2520interaction%2520with%2520LLMs%2520is%2520currently%250Amainly%2520focused%2520on%2520text-based%2520interactions%252C%2520while%2520other%2520multi-model%2520approaches%250Aremain%2520under-explored.%2520This%2520paper%2520introduces%2520VTutor%252C%2520an%2520open-source%2520Software%250ADevelopment%2520Kit%2520%2528SDK%2529%2520that%2520combines%2520generative%2520AI%2520with%2520advanced%2520animation%250Atechnologies%2520to%2520create%2520engaging%252C%2520adaptable%252C%2520and%2520realistic%2520APAs%2520for%2520human-AI%250Amulti-media%2520interactions.%2520VTutor%2520leverages%2520LLMs%2520for%2520real-time%2520personalized%250Afeedback%252C%2520advanced%2520lip%2520synchronization%2520for%2520natural%2520speech%2520alignment%252C%2520and%2520WebGL%250Arendering%2520for%2520seamless%2520web%2520integration.%2520Supporting%2520various%25202D%2520and%25203D%2520character%250Amodels%252C%2520VTutor%2520enables%2520researchers%2520and%2520developers%2520to%2520design%2520emotionally%250Aresonant%252C%2520contextually%2520adaptive%2520learning%2520agents.%2520This%2520toolkit%2520enhances%2520learner%250Aengagement%252C%2520feedback%2520receptivity%252C%2520and%2520human-AI%2520interaction%2520while%2520promoting%250Atrustworthy%2520AI%2520principles%2520in%2520education.%2520VTutor%2520sets%2520a%2520new%2520standard%2520for%250Anext-generation%2520APAs%252C%2520offering%2520an%2520accessible%252C%2520scalable%2520solution%2520for%2520fostering%250Ameaningful%2520and%2520immersive%2520human-AI%2520interaction%2520experiences.%2520The%2520VTutor%2520project%250Ais%2520open-sourced%2520and%2520welcomes%2520community-driven%2520contributions%2520and%2520showcases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VTutor%3A%20An%20Open-Source%20SDK%20for%20Generative%20AI-Powered%20Animated%0A%20%20Pedagogical%20Agents%20with%20Multi-Media%20Output&entry.906535625=Eason%20Chen%20and%20Chenyu%20Lin%20and%20Xinyi%20Tang%20and%20Aprille%20Xi%20and%20Canwen%20Wang%20and%20Jionghao%20Lin%20and%20Kenneth%20R%20Koedinger&entry.1292438233=%20%20The%20rapid%20evolution%20of%20large%20language%20models%20%28LLMs%29%20has%20transformed%0Ahuman-computer%20interaction%20%28HCI%29%2C%20but%20the%20interaction%20with%20LLMs%20is%20currently%0Amainly%20focused%20on%20text-based%20interactions%2C%20while%20other%20multi-model%20approaches%0Aremain%20under-explored.%20This%20paper%20introduces%20VTutor%2C%20an%20open-source%20Software%0ADevelopment%20Kit%20%28SDK%29%20that%20combines%20generative%20AI%20with%20advanced%20animation%0Atechnologies%20to%20create%20engaging%2C%20adaptable%2C%20and%20realistic%20APAs%20for%20human-AI%0Amulti-media%20interactions.%20VTutor%20leverages%20LLMs%20for%20real-time%20personalized%0Afeedback%2C%20advanced%20lip%20synchronization%20for%20natural%20speech%20alignment%2C%20and%20WebGL%0Arendering%20for%20seamless%20web%20integration.%20Supporting%20various%202D%20and%203D%20character%0Amodels%2C%20VTutor%20enables%20researchers%20and%20developers%20to%20design%20emotionally%0Aresonant%2C%20contextually%20adaptive%20learning%20agents.%20This%20toolkit%20enhances%20learner%0Aengagement%2C%20feedback%20receptivity%2C%20and%20human-AI%20interaction%20while%20promoting%0Atrustworthy%20AI%20principles%20in%20education.%20VTutor%20sets%20a%20new%20standard%20for%0Anext-generation%20APAs%2C%20offering%20an%20accessible%2C%20scalable%20solution%20for%20fostering%0Ameaningful%20and%20immersive%20human-AI%20interaction%20experiences.%20The%20VTutor%20project%0Ais%20open-sourced%20and%20welcomes%20community-driven%20contributions%20and%20showcases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04103v2&entry.124074799=Read"},
{"title": "RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets", "author": "Isabella Liu and Zhan Xu and Wang Yifan and Hao Tan and Zexiang Xu and Xiaolong Wang and Hao Su and Zifan Shi", "abstract": "  We present RigAnything, a novel autoregressive transformer-based model, which\nmakes 3D assets rig-ready by probabilistically generating joints, skeleton\ntopologies, and assigning skinning weights in a template-free manner. Unlike\nmost existing auto-rigging methods, which rely on predefined skeleton template\nand are limited to specific categories like humanoid, RigAnything approaches\nthe rigging problem in an autoregressive manner, iteratively predicting the\nnext joint based on the global input shape and the previous prediction. While\nautoregressive models are typically used to generate sequential data,\nRigAnything extends their application to effectively learn and represent\nskeletons, which are inherently tree structures. To achieve this, we organize\nthe joints in a breadth-first search (BFS) order, enabling the skeleton to be\ndefined as a sequence of 3D locations and the parent index. Furthermore, our\nmodel improves the accuracy of position prediction by leveraging diffusion\nmodeling, ensuring precise and consistent placement of joints within the\nhierarchy. This formulation allows the autoregressive model to efficiently\ncapture both spatial and hierarchical relationships within the skeleton.\nTrained end-to-end on both RigNet and Objaverse datasets, RigAnything\ndemonstrates state-of-the-art performance across diverse object types,\nincluding humanoids, quadrupeds, marine creatures, insects, and many more,\nsurpassing prior methods in quality, robustness, generalizability, and\nefficiency. Please check our website for more details:\nhttps://www.liuisabella.com/RigAnything.\n", "link": "http://arxiv.org/abs/2502.09615v1", "date": "2025-02-13", "relevancy": 2.842, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6032}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5534}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RigAnything%3A%20Template-Free%20Autoregressive%20Rigging%20for%20Diverse%203D%20Assets&body=Title%3A%20RigAnything%3A%20Template-Free%20Autoregressive%20Rigging%20for%20Diverse%203D%20Assets%0AAuthor%3A%20Isabella%20Liu%20and%20Zhan%20Xu%20and%20Wang%20Yifan%20and%20Hao%20Tan%20and%20Zexiang%20Xu%20and%20Xiaolong%20Wang%20and%20Hao%20Su%20and%20Zifan%20Shi%0AAbstract%3A%20%20%20We%20present%20RigAnything%2C%20a%20novel%20autoregressive%20transformer-based%20model%2C%20which%0Amakes%203D%20assets%20rig-ready%20by%20probabilistically%20generating%20joints%2C%20skeleton%0Atopologies%2C%20and%20assigning%20skinning%20weights%20in%20a%20template-free%20manner.%20Unlike%0Amost%20existing%20auto-rigging%20methods%2C%20which%20rely%20on%20predefined%20skeleton%20template%0Aand%20are%20limited%20to%20specific%20categories%20like%20humanoid%2C%20RigAnything%20approaches%0Athe%20rigging%20problem%20in%20an%20autoregressive%20manner%2C%20iteratively%20predicting%20the%0Anext%20joint%20based%20on%20the%20global%20input%20shape%20and%20the%20previous%20prediction.%20While%0Aautoregressive%20models%20are%20typically%20used%20to%20generate%20sequential%20data%2C%0ARigAnything%20extends%20their%20application%20to%20effectively%20learn%20and%20represent%0Askeletons%2C%20which%20are%20inherently%20tree%20structures.%20To%20achieve%20this%2C%20we%20organize%0Athe%20joints%20in%20a%20breadth-first%20search%20%28BFS%29%20order%2C%20enabling%20the%20skeleton%20to%20be%0Adefined%20as%20a%20sequence%20of%203D%20locations%20and%20the%20parent%20index.%20Furthermore%2C%20our%0Amodel%20improves%20the%20accuracy%20of%20position%20prediction%20by%20leveraging%20diffusion%0Amodeling%2C%20ensuring%20precise%20and%20consistent%20placement%20of%20joints%20within%20the%0Ahierarchy.%20This%20formulation%20allows%20the%20autoregressive%20model%20to%20efficiently%0Acapture%20both%20spatial%20and%20hierarchical%20relationships%20within%20the%20skeleton.%0ATrained%20end-to-end%20on%20both%20RigNet%20and%20Objaverse%20datasets%2C%20RigAnything%0Ademonstrates%20state-of-the-art%20performance%20across%20diverse%20object%20types%2C%0Aincluding%20humanoids%2C%20quadrupeds%2C%20marine%20creatures%2C%20insects%2C%20and%20many%20more%2C%0Asurpassing%20prior%20methods%20in%20quality%2C%20robustness%2C%20generalizability%2C%20and%0Aefficiency.%20Please%20check%20our%20website%20for%20more%20details%3A%0Ahttps%3A//www.liuisabella.com/RigAnything.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRigAnything%253A%2520Template-Free%2520Autoregressive%2520Rigging%2520for%2520Diverse%25203D%2520Assets%26entry.906535625%3DIsabella%2520Liu%2520and%2520Zhan%2520Xu%2520and%2520Wang%2520Yifan%2520and%2520Hao%2520Tan%2520and%2520Zexiang%2520Xu%2520and%2520Xiaolong%2520Wang%2520and%2520Hao%2520Su%2520and%2520Zifan%2520Shi%26entry.1292438233%3D%2520%2520We%2520present%2520RigAnything%252C%2520a%2520novel%2520autoregressive%2520transformer-based%2520model%252C%2520which%250Amakes%25203D%2520assets%2520rig-ready%2520by%2520probabilistically%2520generating%2520joints%252C%2520skeleton%250Atopologies%252C%2520and%2520assigning%2520skinning%2520weights%2520in%2520a%2520template-free%2520manner.%2520Unlike%250Amost%2520existing%2520auto-rigging%2520methods%252C%2520which%2520rely%2520on%2520predefined%2520skeleton%2520template%250Aand%2520are%2520limited%2520to%2520specific%2520categories%2520like%2520humanoid%252C%2520RigAnything%2520approaches%250Athe%2520rigging%2520problem%2520in%2520an%2520autoregressive%2520manner%252C%2520iteratively%2520predicting%2520the%250Anext%2520joint%2520based%2520on%2520the%2520global%2520input%2520shape%2520and%2520the%2520previous%2520prediction.%2520While%250Aautoregressive%2520models%2520are%2520typically%2520used%2520to%2520generate%2520sequential%2520data%252C%250ARigAnything%2520extends%2520their%2520application%2520to%2520effectively%2520learn%2520and%2520represent%250Askeletons%252C%2520which%2520are%2520inherently%2520tree%2520structures.%2520To%2520achieve%2520this%252C%2520we%2520organize%250Athe%2520joints%2520in%2520a%2520breadth-first%2520search%2520%2528BFS%2529%2520order%252C%2520enabling%2520the%2520skeleton%2520to%2520be%250Adefined%2520as%2520a%2520sequence%2520of%25203D%2520locations%2520and%2520the%2520parent%2520index.%2520Furthermore%252C%2520our%250Amodel%2520improves%2520the%2520accuracy%2520of%2520position%2520prediction%2520by%2520leveraging%2520diffusion%250Amodeling%252C%2520ensuring%2520precise%2520and%2520consistent%2520placement%2520of%2520joints%2520within%2520the%250Ahierarchy.%2520This%2520formulation%2520allows%2520the%2520autoregressive%2520model%2520to%2520efficiently%250Acapture%2520both%2520spatial%2520and%2520hierarchical%2520relationships%2520within%2520the%2520skeleton.%250ATrained%2520end-to-end%2520on%2520both%2520RigNet%2520and%2520Objaverse%2520datasets%252C%2520RigAnything%250Ademonstrates%2520state-of-the-art%2520performance%2520across%2520diverse%2520object%2520types%252C%250Aincluding%2520humanoids%252C%2520quadrupeds%252C%2520marine%2520creatures%252C%2520insects%252C%2520and%2520many%2520more%252C%250Asurpassing%2520prior%2520methods%2520in%2520quality%252C%2520robustness%252C%2520generalizability%252C%2520and%250Aefficiency.%2520Please%2520check%2520our%2520website%2520for%2520more%2520details%253A%250Ahttps%253A//www.liuisabella.com/RigAnything.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RigAnything%3A%20Template-Free%20Autoregressive%20Rigging%20for%20Diverse%203D%20Assets&entry.906535625=Isabella%20Liu%20and%20Zhan%20Xu%20and%20Wang%20Yifan%20and%20Hao%20Tan%20and%20Zexiang%20Xu%20and%20Xiaolong%20Wang%20and%20Hao%20Su%20and%20Zifan%20Shi&entry.1292438233=%20%20We%20present%20RigAnything%2C%20a%20novel%20autoregressive%20transformer-based%20model%2C%20which%0Amakes%203D%20assets%20rig-ready%20by%20probabilistically%20generating%20joints%2C%20skeleton%0Atopologies%2C%20and%20assigning%20skinning%20weights%20in%20a%20template-free%20manner.%20Unlike%0Amost%20existing%20auto-rigging%20methods%2C%20which%20rely%20on%20predefined%20skeleton%20template%0Aand%20are%20limited%20to%20specific%20categories%20like%20humanoid%2C%20RigAnything%20approaches%0Athe%20rigging%20problem%20in%20an%20autoregressive%20manner%2C%20iteratively%20predicting%20the%0Anext%20joint%20based%20on%20the%20global%20input%20shape%20and%20the%20previous%20prediction.%20While%0Aautoregressive%20models%20are%20typically%20used%20to%20generate%20sequential%20data%2C%0ARigAnything%20extends%20their%20application%20to%20effectively%20learn%20and%20represent%0Askeletons%2C%20which%20are%20inherently%20tree%20structures.%20To%20achieve%20this%2C%20we%20organize%0Athe%20joints%20in%20a%20breadth-first%20search%20%28BFS%29%20order%2C%20enabling%20the%20skeleton%20to%20be%0Adefined%20as%20a%20sequence%20of%203D%20locations%20and%20the%20parent%20index.%20Furthermore%2C%20our%0Amodel%20improves%20the%20accuracy%20of%20position%20prediction%20by%20leveraging%20diffusion%0Amodeling%2C%20ensuring%20precise%20and%20consistent%20placement%20of%20joints%20within%20the%0Ahierarchy.%20This%20formulation%20allows%20the%20autoregressive%20model%20to%20efficiently%0Acapture%20both%20spatial%20and%20hierarchical%20relationships%20within%20the%20skeleton.%0ATrained%20end-to-end%20on%20both%20RigNet%20and%20Objaverse%20datasets%2C%20RigAnything%0Ademonstrates%20state-of-the-art%20performance%20across%20diverse%20object%20types%2C%0Aincluding%20humanoids%2C%20quadrupeds%2C%20marine%20creatures%2C%20insects%2C%20and%20many%20more%2C%0Asurpassing%20prior%20methods%20in%20quality%2C%20robustness%2C%20generalizability%2C%20and%0Aefficiency.%20Please%20check%20our%20website%20for%20more%20details%3A%0Ahttps%3A//www.liuisabella.com/RigAnything.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09615v1&entry.124074799=Read"},
{"title": "When and How Does CLIP Enable Domain and Compositional Generalization?", "author": "Elias Kempf and Simon Schrodi and Max Argus and Thomas Brox", "abstract": "  The remarkable generalization performance of contrastive vision-language\nmodels like CLIP is often attributed to the diversity of their training\ndistributions. However, key questions remain unanswered: Can CLIP generalize to\nan entirely unseen domain when trained on a diverse mixture of domains (domain\ngeneralization)? Can it generalize to unseen classes within partially seen\ndomains (compositional generalization)? What factors affect such\ngeneralization? To answer these questions, we trained CLIP models on\nsystematically constructed training distributions with controlled domain\ndiversity and object class exposure. Our experiments show that domain diversity\nis essential for both domain and compositional generalization, yet\ncompositional generalization can be surprisingly weaker than domain\ngeneralization when the training distribution contains a suboptimal subset of\nthe test domain. Through data-centric and mechanistic analyses, we find that\nsuccessful generalization requires learning of shared representations already\nin intermediate layers and shared circuitry.\n", "link": "http://arxiv.org/abs/2502.09507v1", "date": "2025-02-13", "relevancy": 2.7295, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20and%20How%20Does%20CLIP%20Enable%20Domain%20and%20Compositional%20Generalization%3F&body=Title%3A%20When%20and%20How%20Does%20CLIP%20Enable%20Domain%20and%20Compositional%20Generalization%3F%0AAuthor%3A%20Elias%20Kempf%20and%20Simon%20Schrodi%20and%20Max%20Argus%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20The%20remarkable%20generalization%20performance%20of%20contrastive%20vision-language%0Amodels%20like%20CLIP%20is%20often%20attributed%20to%20the%20diversity%20of%20their%20training%0Adistributions.%20However%2C%20key%20questions%20remain%20unanswered%3A%20Can%20CLIP%20generalize%20to%0Aan%20entirely%20unseen%20domain%20when%20trained%20on%20a%20diverse%20mixture%20of%20domains%20%28domain%0Ageneralization%29%3F%20Can%20it%20generalize%20to%20unseen%20classes%20within%20partially%20seen%0Adomains%20%28compositional%20generalization%29%3F%20What%20factors%20affect%20such%0Ageneralization%3F%20To%20answer%20these%20questions%2C%20we%20trained%20CLIP%20models%20on%0Asystematically%20constructed%20training%20distributions%20with%20controlled%20domain%0Adiversity%20and%20object%20class%20exposure.%20Our%20experiments%20show%20that%20domain%20diversity%0Ais%20essential%20for%20both%20domain%20and%20compositional%20generalization%2C%20yet%0Acompositional%20generalization%20can%20be%20surprisingly%20weaker%20than%20domain%0Ageneralization%20when%20the%20training%20distribution%20contains%20a%20suboptimal%20subset%20of%0Athe%20test%20domain.%20Through%20data-centric%20and%20mechanistic%20analyses%2C%20we%20find%20that%0Asuccessful%20generalization%20requires%20learning%20of%20shared%20representations%20already%0Ain%20intermediate%20layers%20and%20shared%20circuitry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520and%2520How%2520Does%2520CLIP%2520Enable%2520Domain%2520and%2520Compositional%2520Generalization%253F%26entry.906535625%3DElias%2520Kempf%2520and%2520Simon%2520Schrodi%2520and%2520Max%2520Argus%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520The%2520remarkable%2520generalization%2520performance%2520of%2520contrastive%2520vision-language%250Amodels%2520like%2520CLIP%2520is%2520often%2520attributed%2520to%2520the%2520diversity%2520of%2520their%2520training%250Adistributions.%2520However%252C%2520key%2520questions%2520remain%2520unanswered%253A%2520Can%2520CLIP%2520generalize%2520to%250Aan%2520entirely%2520unseen%2520domain%2520when%2520trained%2520on%2520a%2520diverse%2520mixture%2520of%2520domains%2520%2528domain%250Ageneralization%2529%253F%2520Can%2520it%2520generalize%2520to%2520unseen%2520classes%2520within%2520partially%2520seen%250Adomains%2520%2528compositional%2520generalization%2529%253F%2520What%2520factors%2520affect%2520such%250Ageneralization%253F%2520To%2520answer%2520these%2520questions%252C%2520we%2520trained%2520CLIP%2520models%2520on%250Asystematically%2520constructed%2520training%2520distributions%2520with%2520controlled%2520domain%250Adiversity%2520and%2520object%2520class%2520exposure.%2520Our%2520experiments%2520show%2520that%2520domain%2520diversity%250Ais%2520essential%2520for%2520both%2520domain%2520and%2520compositional%2520generalization%252C%2520yet%250Acompositional%2520generalization%2520can%2520be%2520surprisingly%2520weaker%2520than%2520domain%250Ageneralization%2520when%2520the%2520training%2520distribution%2520contains%2520a%2520suboptimal%2520subset%2520of%250Athe%2520test%2520domain.%2520Through%2520data-centric%2520and%2520mechanistic%2520analyses%252C%2520we%2520find%2520that%250Asuccessful%2520generalization%2520requires%2520learning%2520of%2520shared%2520representations%2520already%250Ain%2520intermediate%2520layers%2520and%2520shared%2520circuitry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20and%20How%20Does%20CLIP%20Enable%20Domain%20and%20Compositional%20Generalization%3F&entry.906535625=Elias%20Kempf%20and%20Simon%20Schrodi%20and%20Max%20Argus%20and%20Thomas%20Brox&entry.1292438233=%20%20The%20remarkable%20generalization%20performance%20of%20contrastive%20vision-language%0Amodels%20like%20CLIP%20is%20often%20attributed%20to%20the%20diversity%20of%20their%20training%0Adistributions.%20However%2C%20key%20questions%20remain%20unanswered%3A%20Can%20CLIP%20generalize%20to%0Aan%20entirely%20unseen%20domain%20when%20trained%20on%20a%20diverse%20mixture%20of%20domains%20%28domain%0Ageneralization%29%3F%20Can%20it%20generalize%20to%20unseen%20classes%20within%20partially%20seen%0Adomains%20%28compositional%20generalization%29%3F%20What%20factors%20affect%20such%0Ageneralization%3F%20To%20answer%20these%20questions%2C%20we%20trained%20CLIP%20models%20on%0Asystematically%20constructed%20training%20distributions%20with%20controlled%20domain%0Adiversity%20and%20object%20class%20exposure.%20Our%20experiments%20show%20that%20domain%20diversity%0Ais%20essential%20for%20both%20domain%20and%20compositional%20generalization%2C%20yet%0Acompositional%20generalization%20can%20be%20surprisingly%20weaker%20than%20domain%0Ageneralization%20when%20the%20training%20distribution%20contains%20a%20suboptimal%20subset%20of%0Athe%20test%20domain.%20Through%20data-centric%20and%20mechanistic%20analyses%2C%20we%20find%20that%0Asuccessful%20generalization%20requires%20learning%20of%20shared%20representations%20already%0Ain%20intermediate%20layers%20and%20shared%20circuitry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09507v1&entry.124074799=Read"},
{"title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs", "author": "Yiwen Tang and Zoey Guo and Zhuhao Wang and Ray Zhang and Qizhi Chen and Junli Liu and Delin Qu and Zhigang Wang and Dong Wang and Xuelong Li and Bin Zhao", "abstract": "  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n", "link": "http://arxiv.org/abs/2502.09620v1", "date": "2025-02-13", "relevancy": 2.6949, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6832}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs&body=Title%3A%20Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs%0AAuthor%3A%20Yiwen%20Tang%20and%20Zoey%20Guo%20and%20Zhuhao%20Wang%20and%20Ray%20Zhang%20and%20Qizhi%20Chen%20and%20Junli%20Liu%20and%20Delin%20Qu%20and%20Zhigang%20Wang%20and%20Dong%20Wang%20and%20Xuelong%20Li%20and%20Bin%20Zhao%0AAbstract%3A%20%20%20Encoder-free%20architectures%20have%20been%20preliminarily%20explored%20in%20the%202D%20visual%0Adomain%2C%20yet%20it%20remains%20an%20open%20question%20whether%20they%20can%20be%20effectively%20applied%0Ato%203D%20understanding%20scenarios.%20In%20this%20paper%2C%20we%20present%20the%20first%0Acomprehensive%20investigation%20into%20the%20potential%20of%20encoder-free%20architectures%20to%0Aovercome%20the%20challenges%20of%20encoder-based%203D%20Large%20Multimodal%20Models%20%28LMMs%29.%0AThese%20challenges%20include%20the%20failure%20to%20adapt%20to%20varying%20point%20cloud%0Aresolutions%20and%20the%20point%20features%20from%20the%20encoder%20not%20meeting%20the%20semantic%0Aneeds%20of%20Large%20Language%20Models%20%28LLMs%29.%20We%20identify%20key%20aspects%20for%203D%20LMMs%20to%0Aremove%20the%20encoder%20and%20enable%20the%20LLM%20to%20assume%20the%20role%20of%20the%203D%20encoder%3A%201%29%0AWe%20propose%20the%20LLM-embedded%20Semantic%20Encoding%20strategy%20in%20the%20pre-training%0Astage%2C%20exploring%20the%20effects%20of%20various%20point%20cloud%20self-supervised%20losses.%20And%0Awe%20present%20the%20Hybrid%20Semantic%20Loss%20to%20extract%20high-level%20semantics.%202%29%20We%0Aintroduce%20the%20Hierarchical%20Geometry%20Aggregation%20strategy%20in%20the%20instruction%0Atuning%20stage.%20This%20incorporates%20inductive%20bias%20into%20the%20LLM%20early%20layers%20to%0Afocus%20on%20the%20local%20details%20of%20the%20point%20clouds.%20To%20the%20end%2C%20we%20present%20the%0Afirst%20Encoder-free%203D%20LMM%2C%20ENEL.%20Our%207B%20model%20rivals%20the%20current%0Astate-of-the-art%20model%2C%20ShapeLLM-13B%2C%20achieving%2055.0%25%2C%2050.92%25%2C%20and%2042.7%25%20on%20the%0Aclassification%2C%20captioning%2C%20and%20VQA%20tasks%2C%20respectively.%20Our%20results%0Ademonstrate%20that%20the%20encoder-free%20architecture%20is%20highly%20promising%20for%0Areplacing%20encoder-based%20architectures%20in%20the%20field%20of%203D%20understanding.%20The%0Acode%20is%20released%20at%20https%3A//github.com/Ivan-Tang-3D/ENEL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Potential%2520of%2520Encoder-free%2520Architectures%2520in%25203D%2520LMMs%26entry.906535625%3DYiwen%2520Tang%2520and%2520Zoey%2520Guo%2520and%2520Zhuhao%2520Wang%2520and%2520Ray%2520Zhang%2520and%2520Qizhi%2520Chen%2520and%2520Junli%2520Liu%2520and%2520Delin%2520Qu%2520and%2520Zhigang%2520Wang%2520and%2520Dong%2520Wang%2520and%2520Xuelong%2520Li%2520and%2520Bin%2520Zhao%26entry.1292438233%3D%2520%2520Encoder-free%2520architectures%2520have%2520been%2520preliminarily%2520explored%2520in%2520the%25202D%2520visual%250Adomain%252C%2520yet%2520it%2520remains%2520an%2520open%2520question%2520whether%2520they%2520can%2520be%2520effectively%2520applied%250Ato%25203D%2520understanding%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%250Acomprehensive%2520investigation%2520into%2520the%2520potential%2520of%2520encoder-free%2520architectures%2520to%250Aovercome%2520the%2520challenges%2520of%2520encoder-based%25203D%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529.%250AThese%2520challenges%2520include%2520the%2520failure%2520to%2520adapt%2520to%2520varying%2520point%2520cloud%250Aresolutions%2520and%2520the%2520point%2520features%2520from%2520the%2520encoder%2520not%2520meeting%2520the%2520semantic%250Aneeds%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520We%2520identify%2520key%2520aspects%2520for%25203D%2520LMMs%2520to%250Aremove%2520the%2520encoder%2520and%2520enable%2520the%2520LLM%2520to%2520assume%2520the%2520role%2520of%2520the%25203D%2520encoder%253A%25201%2529%250AWe%2520propose%2520the%2520LLM-embedded%2520Semantic%2520Encoding%2520strategy%2520in%2520the%2520pre-training%250Astage%252C%2520exploring%2520the%2520effects%2520of%2520various%2520point%2520cloud%2520self-supervised%2520losses.%2520And%250Awe%2520present%2520the%2520Hybrid%2520Semantic%2520Loss%2520to%2520extract%2520high-level%2520semantics.%25202%2529%2520We%250Aintroduce%2520the%2520Hierarchical%2520Geometry%2520Aggregation%2520strategy%2520in%2520the%2520instruction%250Atuning%2520stage.%2520This%2520incorporates%2520inductive%2520bias%2520into%2520the%2520LLM%2520early%2520layers%2520to%250Afocus%2520on%2520the%2520local%2520details%2520of%2520the%2520point%2520clouds.%2520To%2520the%2520end%252C%2520we%2520present%2520the%250Afirst%2520Encoder-free%25203D%2520LMM%252C%2520ENEL.%2520Our%25207B%2520model%2520rivals%2520the%2520current%250Astate-of-the-art%2520model%252C%2520ShapeLLM-13B%252C%2520achieving%252055.0%2525%252C%252050.92%2525%252C%2520and%252042.7%2525%2520on%2520the%250Aclassification%252C%2520captioning%252C%2520and%2520VQA%2520tasks%252C%2520respectively.%2520Our%2520results%250Ademonstrate%2520that%2520the%2520encoder-free%2520architecture%2520is%2520highly%2520promising%2520for%250Areplacing%2520encoder-based%2520architectures%2520in%2520the%2520field%2520of%25203D%2520understanding.%2520The%250Acode%2520is%2520released%2520at%2520https%253A//github.com/Ivan-Tang-3D/ENEL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs&entry.906535625=Yiwen%20Tang%20and%20Zoey%20Guo%20and%20Zhuhao%20Wang%20and%20Ray%20Zhang%20and%20Qizhi%20Chen%20and%20Junli%20Liu%20and%20Delin%20Qu%20and%20Zhigang%20Wang%20and%20Dong%20Wang%20and%20Xuelong%20Li%20and%20Bin%20Zhao&entry.1292438233=%20%20Encoder-free%20architectures%20have%20been%20preliminarily%20explored%20in%20the%202D%20visual%0Adomain%2C%20yet%20it%20remains%20an%20open%20question%20whether%20they%20can%20be%20effectively%20applied%0Ato%203D%20understanding%20scenarios.%20In%20this%20paper%2C%20we%20present%20the%20first%0Acomprehensive%20investigation%20into%20the%20potential%20of%20encoder-free%20architectures%20to%0Aovercome%20the%20challenges%20of%20encoder-based%203D%20Large%20Multimodal%20Models%20%28LMMs%29.%0AThese%20challenges%20include%20the%20failure%20to%20adapt%20to%20varying%20point%20cloud%0Aresolutions%20and%20the%20point%20features%20from%20the%20encoder%20not%20meeting%20the%20semantic%0Aneeds%20of%20Large%20Language%20Models%20%28LLMs%29.%20We%20identify%20key%20aspects%20for%203D%20LMMs%20to%0Aremove%20the%20encoder%20and%20enable%20the%20LLM%20to%20assume%20the%20role%20of%20the%203D%20encoder%3A%201%29%0AWe%20propose%20the%20LLM-embedded%20Semantic%20Encoding%20strategy%20in%20the%20pre-training%0Astage%2C%20exploring%20the%20effects%20of%20various%20point%20cloud%20self-supervised%20losses.%20And%0Awe%20present%20the%20Hybrid%20Semantic%20Loss%20to%20extract%20high-level%20semantics.%202%29%20We%0Aintroduce%20the%20Hierarchical%20Geometry%20Aggregation%20strategy%20in%20the%20instruction%0Atuning%20stage.%20This%20incorporates%20inductive%20bias%20into%20the%20LLM%20early%20layers%20to%0Afocus%20on%20the%20local%20details%20of%20the%20point%20clouds.%20To%20the%20end%2C%20we%20present%20the%0Afirst%20Encoder-free%203D%20LMM%2C%20ENEL.%20Our%207B%20model%20rivals%20the%20current%0Astate-of-the-art%20model%2C%20ShapeLLM-13B%2C%20achieving%2055.0%25%2C%2050.92%25%2C%20and%2042.7%25%20on%20the%0Aclassification%2C%20captioning%2C%20and%20VQA%20tasks%2C%20respectively.%20Our%20results%0Ademonstrate%20that%20the%20encoder-free%20architecture%20is%20highly%20promising%20for%0Areplacing%20encoder-based%20architectures%20in%20the%20field%20of%203D%20understanding.%20The%0Acode%20is%20released%20at%20https%3A//github.com/Ivan-Tang-3D/ENEL%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09620v1&entry.124074799=Read"},
{"title": "GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for\n  Remote Sensing Image Analysis", "author": "Angelos Zavras and Dimitrios Michail and Xiao Xiang Zhu and Beg\u00fcm Demir and Ioannis Papoutsis", "abstract": "  The continuous operation of Earth-orbiting satellites generates vast and\never-growing archives of Remote Sensing (RS) images. Natural language presents\nan intuitive interface for accessing, querying, and interpreting the data from\nsuch archives. However, existing Vision-Language Models (VLMs) are\npredominantly trained on web-scraped, noisy image-text data, exhibiting limited\nexposure to the specialized domain of RS. This deficiency results in poor\nperformance on RS-specific tasks, as commonly used datasets often lack\ndetailed, scientifically accurate textual descriptions and instead emphasize\nsolely on attributes like date and location. To bridge this critical gap, we\nintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and\nmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curated\nRS image-text pairs, representing a diverse range of RS modalities associated\nto different spatial resolutions. Unlike existing vision-language datasets in\nRS, GAIA specifically focuses on capturing a diverse range of RS applications,\nproviding unique information about environmental changes, natural disasters,\nand various other dynamic phenomena. The dataset provides a spatially and\ntemporally balanced distribution, spanning across the globe, covering the last\n25 years with a balanced temporal distribution of observations. GAIA's\nconstruction involved a two-stage process: (1) targeted web-scraping of images\nand accompanying text from reputable RS-related sources, and (2) generation of\nfive high-quality, scientifically grounded synthetic captions for each image\nusing carefully crafted prompts that leverage the advanced vision-language\ncapabilities of GPT-4o. Our extensive experiments, including fine-tuning of\nCLIP and BLIP2 models, demonstrate that GAIA significantly improves performance\non RS image classification, cross-modal retrieval and image captioning tasks.\n", "link": "http://arxiv.org/abs/2502.09598v1", "date": "2025-02-13", "relevancy": 2.6878, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAIA%3A%20A%20Global%2C%20Multi-modal%2C%20Multi-scale%20Vision-Language%20Dataset%20for%0A%20%20Remote%20Sensing%20Image%20Analysis&body=Title%3A%20GAIA%3A%20A%20Global%2C%20Multi-modal%2C%20Multi-scale%20Vision-Language%20Dataset%20for%0A%20%20Remote%20Sensing%20Image%20Analysis%0AAuthor%3A%20Angelos%20Zavras%20and%20Dimitrios%20Michail%20and%20Xiao%20Xiang%20Zhu%20and%20Beg%C3%BCm%20Demir%20and%20Ioannis%20Papoutsis%0AAbstract%3A%20%20%20The%20continuous%20operation%20of%20Earth-orbiting%20satellites%20generates%20vast%20and%0Aever-growing%20archives%20of%20Remote%20Sensing%20%28RS%29%20images.%20Natural%20language%20presents%0Aan%20intuitive%20interface%20for%20accessing%2C%20querying%2C%20and%20interpreting%20the%20data%20from%0Asuch%20archives.%20However%2C%20existing%20Vision-Language%20Models%20%28VLMs%29%20are%0Apredominantly%20trained%20on%20web-scraped%2C%20noisy%20image-text%20data%2C%20exhibiting%20limited%0Aexposure%20to%20the%20specialized%20domain%20of%20RS.%20This%20deficiency%20results%20in%20poor%0Aperformance%20on%20RS-specific%20tasks%2C%20as%20commonly%20used%20datasets%20often%20lack%0Adetailed%2C%20scientifically%20accurate%20textual%20descriptions%20and%20instead%20emphasize%0Asolely%20on%20attributes%20like%20date%20and%20location.%20To%20bridge%20this%20critical%20gap%2C%20we%0Aintroduce%20GAIA%2C%20a%20novel%20dataset%20designed%20for%20multi-scale%2C%20multi-sensor%2C%20and%0Amulti-modal%20RS%20image%20analysis.%20GAIA%20comprises%20of%20205%2C150%20meticulously%20curated%0ARS%20image-text%20pairs%2C%20representing%20a%20diverse%20range%20of%20RS%20modalities%20associated%0Ato%20different%20spatial%20resolutions.%20Unlike%20existing%20vision-language%20datasets%20in%0ARS%2C%20GAIA%20specifically%20focuses%20on%20capturing%20a%20diverse%20range%20of%20RS%20applications%2C%0Aproviding%20unique%20information%20about%20environmental%20changes%2C%20natural%20disasters%2C%0Aand%20various%20other%20dynamic%20phenomena.%20The%20dataset%20provides%20a%20spatially%20and%0Atemporally%20balanced%20distribution%2C%20spanning%20across%20the%20globe%2C%20covering%20the%20last%0A25%20years%20with%20a%20balanced%20temporal%20distribution%20of%20observations.%20GAIA%27s%0Aconstruction%20involved%20a%20two-stage%20process%3A%20%281%29%20targeted%20web-scraping%20of%20images%0Aand%20accompanying%20text%20from%20reputable%20RS-related%20sources%2C%20and%20%282%29%20generation%20of%0Afive%20high-quality%2C%20scientifically%20grounded%20synthetic%20captions%20for%20each%20image%0Ausing%20carefully%20crafted%20prompts%20that%20leverage%20the%20advanced%20vision-language%0Acapabilities%20of%20GPT-4o.%20Our%20extensive%20experiments%2C%20including%20fine-tuning%20of%0ACLIP%20and%20BLIP2%20models%2C%20demonstrate%20that%20GAIA%20significantly%20improves%20performance%0Aon%20RS%20image%20classification%2C%20cross-modal%20retrieval%20and%20image%20captioning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAIA%253A%2520A%2520Global%252C%2520Multi-modal%252C%2520Multi-scale%2520Vision-Language%2520Dataset%2520for%250A%2520%2520Remote%2520Sensing%2520Image%2520Analysis%26entry.906535625%3DAngelos%2520Zavras%2520and%2520Dimitrios%2520Michail%2520and%2520Xiao%2520Xiang%2520Zhu%2520and%2520Beg%25C3%25BCm%2520Demir%2520and%2520Ioannis%2520Papoutsis%26entry.1292438233%3D%2520%2520The%2520continuous%2520operation%2520of%2520Earth-orbiting%2520satellites%2520generates%2520vast%2520and%250Aever-growing%2520archives%2520of%2520Remote%2520Sensing%2520%2528RS%2529%2520images.%2520Natural%2520language%2520presents%250Aan%2520intuitive%2520interface%2520for%2520accessing%252C%2520querying%252C%2520and%2520interpreting%2520the%2520data%2520from%250Asuch%2520archives.%2520However%252C%2520existing%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%250Apredominantly%2520trained%2520on%2520web-scraped%252C%2520noisy%2520image-text%2520data%252C%2520exhibiting%2520limited%250Aexposure%2520to%2520the%2520specialized%2520domain%2520of%2520RS.%2520This%2520deficiency%2520results%2520in%2520poor%250Aperformance%2520on%2520RS-specific%2520tasks%252C%2520as%2520commonly%2520used%2520datasets%2520often%2520lack%250Adetailed%252C%2520scientifically%2520accurate%2520textual%2520descriptions%2520and%2520instead%2520emphasize%250Asolely%2520on%2520attributes%2520like%2520date%2520and%2520location.%2520To%2520bridge%2520this%2520critical%2520gap%252C%2520we%250Aintroduce%2520GAIA%252C%2520a%2520novel%2520dataset%2520designed%2520for%2520multi-scale%252C%2520multi-sensor%252C%2520and%250Amulti-modal%2520RS%2520image%2520analysis.%2520GAIA%2520comprises%2520of%2520205%252C150%2520meticulously%2520curated%250ARS%2520image-text%2520pairs%252C%2520representing%2520a%2520diverse%2520range%2520of%2520RS%2520modalities%2520associated%250Ato%2520different%2520spatial%2520resolutions.%2520Unlike%2520existing%2520vision-language%2520datasets%2520in%250ARS%252C%2520GAIA%2520specifically%2520focuses%2520on%2520capturing%2520a%2520diverse%2520range%2520of%2520RS%2520applications%252C%250Aproviding%2520unique%2520information%2520about%2520environmental%2520changes%252C%2520natural%2520disasters%252C%250Aand%2520various%2520other%2520dynamic%2520phenomena.%2520The%2520dataset%2520provides%2520a%2520spatially%2520and%250Atemporally%2520balanced%2520distribution%252C%2520spanning%2520across%2520the%2520globe%252C%2520covering%2520the%2520last%250A25%2520years%2520with%2520a%2520balanced%2520temporal%2520distribution%2520of%2520observations.%2520GAIA%2527s%250Aconstruction%2520involved%2520a%2520two-stage%2520process%253A%2520%25281%2529%2520targeted%2520web-scraping%2520of%2520images%250Aand%2520accompanying%2520text%2520from%2520reputable%2520RS-related%2520sources%252C%2520and%2520%25282%2529%2520generation%2520of%250Afive%2520high-quality%252C%2520scientifically%2520grounded%2520synthetic%2520captions%2520for%2520each%2520image%250Ausing%2520carefully%2520crafted%2520prompts%2520that%2520leverage%2520the%2520advanced%2520vision-language%250Acapabilities%2520of%2520GPT-4o.%2520Our%2520extensive%2520experiments%252C%2520including%2520fine-tuning%2520of%250ACLIP%2520and%2520BLIP2%2520models%252C%2520demonstrate%2520that%2520GAIA%2520significantly%2520improves%2520performance%250Aon%2520RS%2520image%2520classification%252C%2520cross-modal%2520retrieval%2520and%2520image%2520captioning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAIA%3A%20A%20Global%2C%20Multi-modal%2C%20Multi-scale%20Vision-Language%20Dataset%20for%0A%20%20Remote%20Sensing%20Image%20Analysis&entry.906535625=Angelos%20Zavras%20and%20Dimitrios%20Michail%20and%20Xiao%20Xiang%20Zhu%20and%20Beg%C3%BCm%20Demir%20and%20Ioannis%20Papoutsis&entry.1292438233=%20%20The%20continuous%20operation%20of%20Earth-orbiting%20satellites%20generates%20vast%20and%0Aever-growing%20archives%20of%20Remote%20Sensing%20%28RS%29%20images.%20Natural%20language%20presents%0Aan%20intuitive%20interface%20for%20accessing%2C%20querying%2C%20and%20interpreting%20the%20data%20from%0Asuch%20archives.%20However%2C%20existing%20Vision-Language%20Models%20%28VLMs%29%20are%0Apredominantly%20trained%20on%20web-scraped%2C%20noisy%20image-text%20data%2C%20exhibiting%20limited%0Aexposure%20to%20the%20specialized%20domain%20of%20RS.%20This%20deficiency%20results%20in%20poor%0Aperformance%20on%20RS-specific%20tasks%2C%20as%20commonly%20used%20datasets%20often%20lack%0Adetailed%2C%20scientifically%20accurate%20textual%20descriptions%20and%20instead%20emphasize%0Asolely%20on%20attributes%20like%20date%20and%20location.%20To%20bridge%20this%20critical%20gap%2C%20we%0Aintroduce%20GAIA%2C%20a%20novel%20dataset%20designed%20for%20multi-scale%2C%20multi-sensor%2C%20and%0Amulti-modal%20RS%20image%20analysis.%20GAIA%20comprises%20of%20205%2C150%20meticulously%20curated%0ARS%20image-text%20pairs%2C%20representing%20a%20diverse%20range%20of%20RS%20modalities%20associated%0Ato%20different%20spatial%20resolutions.%20Unlike%20existing%20vision-language%20datasets%20in%0ARS%2C%20GAIA%20specifically%20focuses%20on%20capturing%20a%20diverse%20range%20of%20RS%20applications%2C%0Aproviding%20unique%20information%20about%20environmental%20changes%2C%20natural%20disasters%2C%0Aand%20various%20other%20dynamic%20phenomena.%20The%20dataset%20provides%20a%20spatially%20and%0Atemporally%20balanced%20distribution%2C%20spanning%20across%20the%20globe%2C%20covering%20the%20last%0A25%20years%20with%20a%20balanced%20temporal%20distribution%20of%20observations.%20GAIA%27s%0Aconstruction%20involved%20a%20two-stage%20process%3A%20%281%29%20targeted%20web-scraping%20of%20images%0Aand%20accompanying%20text%20from%20reputable%20RS-related%20sources%2C%20and%20%282%29%20generation%20of%0Afive%20high-quality%2C%20scientifically%20grounded%20synthetic%20captions%20for%20each%20image%0Ausing%20carefully%20crafted%20prompts%20that%20leverage%20the%20advanced%20vision-language%0Acapabilities%20of%20GPT-4o.%20Our%20extensive%20experiments%2C%20including%20fine-tuning%20of%0ACLIP%20and%20BLIP2%20models%2C%20demonstrate%20that%20GAIA%20significantly%20improves%20performance%0Aon%20RS%20image%20classification%2C%20cross-modal%20retrieval%20and%20image%20captioning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09598v1&entry.124074799=Read"},
{"title": "Instance Segmentation of Scene Sketches Using Natural Image Priors", "author": "Mia Tang and Yael Vinker and Chuan Yan and Lvmin Zhang and Maneesh Agrawala", "abstract": "  Sketch segmentation involves grouping pixels within a sketch that belong to\nthe same object or instance. It serves as a valuable tool for sketch editing\ntasks, such as moving, scaling, or removing specific components. While image\nsegmentation models have demonstrated remarkable capabilities in recent years,\nsketches present unique challenges for these models due to their sparse nature\nand wide variation in styles. We introduce SketchSeg, a method for instance\nsegmentation of raster scene sketches. Our approach adapts state-of-the-art\nimage segmentation and object detection models to the sketch domain by\nemploying class-agnostic fine-tuning and refining segmentation masks using\ndepth cues. Furthermore, our method organizes sketches into sorted layers,\nwhere occluded instances are inpainted, enabling advanced sketch editing\napplications. As existing datasets in this domain lack variation in sketch\nstyles, we construct a synthetic scene sketch segmentation dataset featuring\nsketches with diverse brush strokes and varying levels of detail. We use this\ndataset to demonstrate the robustness of our approach and will release it to\npromote further research in the field.\n  Project webpage: https://sketchseg.github.io/sketch-seg/\n", "link": "http://arxiv.org/abs/2502.09608v1", "date": "2025-02-13", "relevancy": 2.6411, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance%20Segmentation%20of%20Scene%20Sketches%20Using%20Natural%20Image%20Priors&body=Title%3A%20Instance%20Segmentation%20of%20Scene%20Sketches%20Using%20Natural%20Image%20Priors%0AAuthor%3A%20Mia%20Tang%20and%20Yael%20Vinker%20and%20Chuan%20Yan%20and%20Lvmin%20Zhang%20and%20Maneesh%20Agrawala%0AAbstract%3A%20%20%20Sketch%20segmentation%20involves%20grouping%20pixels%20within%20a%20sketch%20that%20belong%20to%0Athe%20same%20object%20or%20instance.%20It%20serves%20as%20a%20valuable%20tool%20for%20sketch%20editing%0Atasks%2C%20such%20as%20moving%2C%20scaling%2C%20or%20removing%20specific%20components.%20While%20image%0Asegmentation%20models%20have%20demonstrated%20remarkable%20capabilities%20in%20recent%20years%2C%0Asketches%20present%20unique%20challenges%20for%20these%20models%20due%20to%20their%20sparse%20nature%0Aand%20wide%20variation%20in%20styles.%20We%20introduce%20SketchSeg%2C%20a%20method%20for%20instance%0Asegmentation%20of%20raster%20scene%20sketches.%20Our%20approach%20adapts%20state-of-the-art%0Aimage%20segmentation%20and%20object%20detection%20models%20to%20the%20sketch%20domain%20by%0Aemploying%20class-agnostic%20fine-tuning%20and%20refining%20segmentation%20masks%20using%0Adepth%20cues.%20Furthermore%2C%20our%20method%20organizes%20sketches%20into%20sorted%20layers%2C%0Awhere%20occluded%20instances%20are%20inpainted%2C%20enabling%20advanced%20sketch%20editing%0Aapplications.%20As%20existing%20datasets%20in%20this%20domain%20lack%20variation%20in%20sketch%0Astyles%2C%20we%20construct%20a%20synthetic%20scene%20sketch%20segmentation%20dataset%20featuring%0Asketches%20with%20diverse%20brush%20strokes%20and%20varying%20levels%20of%20detail.%20We%20use%20this%0Adataset%20to%20demonstrate%20the%20robustness%20of%20our%20approach%20and%20will%20release%20it%20to%0Apromote%20further%20research%20in%20the%20field.%0A%20%20Project%20webpage%3A%20https%3A//sketchseg.github.io/sketch-seg/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance%2520Segmentation%2520of%2520Scene%2520Sketches%2520Using%2520Natural%2520Image%2520Priors%26entry.906535625%3DMia%2520Tang%2520and%2520Yael%2520Vinker%2520and%2520Chuan%2520Yan%2520and%2520Lvmin%2520Zhang%2520and%2520Maneesh%2520Agrawala%26entry.1292438233%3D%2520%2520Sketch%2520segmentation%2520involves%2520grouping%2520pixels%2520within%2520a%2520sketch%2520that%2520belong%2520to%250Athe%2520same%2520object%2520or%2520instance.%2520It%2520serves%2520as%2520a%2520valuable%2520tool%2520for%2520sketch%2520editing%250Atasks%252C%2520such%2520as%2520moving%252C%2520scaling%252C%2520or%2520removing%2520specific%2520components.%2520While%2520image%250Asegmentation%2520models%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520recent%2520years%252C%250Asketches%2520present%2520unique%2520challenges%2520for%2520these%2520models%2520due%2520to%2520their%2520sparse%2520nature%250Aand%2520wide%2520variation%2520in%2520styles.%2520We%2520introduce%2520SketchSeg%252C%2520a%2520method%2520for%2520instance%250Asegmentation%2520of%2520raster%2520scene%2520sketches.%2520Our%2520approach%2520adapts%2520state-of-the-art%250Aimage%2520segmentation%2520and%2520object%2520detection%2520models%2520to%2520the%2520sketch%2520domain%2520by%250Aemploying%2520class-agnostic%2520fine-tuning%2520and%2520refining%2520segmentation%2520masks%2520using%250Adepth%2520cues.%2520Furthermore%252C%2520our%2520method%2520organizes%2520sketches%2520into%2520sorted%2520layers%252C%250Awhere%2520occluded%2520instances%2520are%2520inpainted%252C%2520enabling%2520advanced%2520sketch%2520editing%250Aapplications.%2520As%2520existing%2520datasets%2520in%2520this%2520domain%2520lack%2520variation%2520in%2520sketch%250Astyles%252C%2520we%2520construct%2520a%2520synthetic%2520scene%2520sketch%2520segmentation%2520dataset%2520featuring%250Asketches%2520with%2520diverse%2520brush%2520strokes%2520and%2520varying%2520levels%2520of%2520detail.%2520We%2520use%2520this%250Adataset%2520to%2520demonstrate%2520the%2520robustness%2520of%2520our%2520approach%2520and%2520will%2520release%2520it%2520to%250Apromote%2520further%2520research%2520in%2520the%2520field.%250A%2520%2520Project%2520webpage%253A%2520https%253A//sketchseg.github.io/sketch-seg/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance%20Segmentation%20of%20Scene%20Sketches%20Using%20Natural%20Image%20Priors&entry.906535625=Mia%20Tang%20and%20Yael%20Vinker%20and%20Chuan%20Yan%20and%20Lvmin%20Zhang%20and%20Maneesh%20Agrawala&entry.1292438233=%20%20Sketch%20segmentation%20involves%20grouping%20pixels%20within%20a%20sketch%20that%20belong%20to%0Athe%20same%20object%20or%20instance.%20It%20serves%20as%20a%20valuable%20tool%20for%20sketch%20editing%0Atasks%2C%20such%20as%20moving%2C%20scaling%2C%20or%20removing%20specific%20components.%20While%20image%0Asegmentation%20models%20have%20demonstrated%20remarkable%20capabilities%20in%20recent%20years%2C%0Asketches%20present%20unique%20challenges%20for%20these%20models%20due%20to%20their%20sparse%20nature%0Aand%20wide%20variation%20in%20styles.%20We%20introduce%20SketchSeg%2C%20a%20method%20for%20instance%0Asegmentation%20of%20raster%20scene%20sketches.%20Our%20approach%20adapts%20state-of-the-art%0Aimage%20segmentation%20and%20object%20detection%20models%20to%20the%20sketch%20domain%20by%0Aemploying%20class-agnostic%20fine-tuning%20and%20refining%20segmentation%20masks%20using%0Adepth%20cues.%20Furthermore%2C%20our%20method%20organizes%20sketches%20into%20sorted%20layers%2C%0Awhere%20occluded%20instances%20are%20inpainted%2C%20enabling%20advanced%20sketch%20editing%0Aapplications.%20As%20existing%20datasets%20in%20this%20domain%20lack%20variation%20in%20sketch%0Astyles%2C%20we%20construct%20a%20synthetic%20scene%20sketch%20segmentation%20dataset%20featuring%0Asketches%20with%20diverse%20brush%20strokes%20and%20varying%20levels%20of%20detail.%20We%20use%20this%0Adataset%20to%20demonstrate%20the%20robustness%20of%20our%20approach%20and%20will%20release%20it%20to%0Apromote%20further%20research%20in%20the%20field.%0A%20%20Project%20webpage%3A%20https%3A//sketchseg.github.io/sketch-seg/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09608v1&entry.124074799=Read"},
{"title": "Enhance-A-Video: Better Generated Video for Free", "author": "Yang Luo and Xuanlei Zhao and Mengzhao Chen and Kaipeng Zhang and Wenqi Shao and Kai Wang and Zhangyang Wang and Yang You", "abstract": "  DiT-based video generation has achieved remarkable results, but research into\nenhancing existing models remains relatively unexplored. In this work, we\nintroduce a training-free approach to enhance the coherence and quality of\nDiT-based generated videos, named Enhance-A-Video. The core idea is enhancing\nthe cross-frame correlations based on non-diagonal temporal attention\ndistributions. Thanks to its simple design, our approach can be easily applied\nto most DiT-based video generation frameworks without any retraining or\nfine-tuning. Across various DiT-based video generation models, our approach\ndemonstrates promising improvements in both temporal consistency and visual\nquality. We hope this research can inspire future explorations in video\ngeneration enhancement.\n", "link": "http://arxiv.org/abs/2502.07508v2", "date": "2025-02-13", "relevancy": 2.6362, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6676}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6601}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhance-A-Video%3A%20Better%20Generated%20Video%20for%20Free&body=Title%3A%20Enhance-A-Video%3A%20Better%20Generated%20Video%20for%20Free%0AAuthor%3A%20Yang%20Luo%20and%20Xuanlei%20Zhao%20and%20Mengzhao%20Chen%20and%20Kaipeng%20Zhang%20and%20Wenqi%20Shao%20and%20Kai%20Wang%20and%20Zhangyang%20Wang%20and%20Yang%20You%0AAbstract%3A%20%20%20DiT-based%20video%20generation%20has%20achieved%20remarkable%20results%2C%20but%20research%20into%0Aenhancing%20existing%20models%20remains%20relatively%20unexplored.%20In%20this%20work%2C%20we%0Aintroduce%20a%20training-free%20approach%20to%20enhance%20the%20coherence%20and%20quality%20of%0ADiT-based%20generated%20videos%2C%20named%20Enhance-A-Video.%20The%20core%20idea%20is%20enhancing%0Athe%20cross-frame%20correlations%20based%20on%20non-diagonal%20temporal%20attention%0Adistributions.%20Thanks%20to%20its%20simple%20design%2C%20our%20approach%20can%20be%20easily%20applied%0Ato%20most%20DiT-based%20video%20generation%20frameworks%20without%20any%20retraining%20or%0Afine-tuning.%20Across%20various%20DiT-based%20video%20generation%20models%2C%20our%20approach%0Ademonstrates%20promising%20improvements%20in%20both%20temporal%20consistency%20and%20visual%0Aquality.%20We%20hope%20this%20research%20can%20inspire%20future%20explorations%20in%20video%0Ageneration%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhance-A-Video%253A%2520Better%2520Generated%2520Video%2520for%2520Free%26entry.906535625%3DYang%2520Luo%2520and%2520Xuanlei%2520Zhao%2520and%2520Mengzhao%2520Chen%2520and%2520Kaipeng%2520Zhang%2520and%2520Wenqi%2520Shao%2520and%2520Kai%2520Wang%2520and%2520Zhangyang%2520Wang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520DiT-based%2520video%2520generation%2520has%2520achieved%2520remarkable%2520results%252C%2520but%2520research%2520into%250Aenhancing%2520existing%2520models%2520remains%2520relatively%2520unexplored.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520training-free%2520approach%2520to%2520enhance%2520the%2520coherence%2520and%2520quality%2520of%250ADiT-based%2520generated%2520videos%252C%2520named%2520Enhance-A-Video.%2520The%2520core%2520idea%2520is%2520enhancing%250Athe%2520cross-frame%2520correlations%2520based%2520on%2520non-diagonal%2520temporal%2520attention%250Adistributions.%2520Thanks%2520to%2520its%2520simple%2520design%252C%2520our%2520approach%2520can%2520be%2520easily%2520applied%250Ato%2520most%2520DiT-based%2520video%2520generation%2520frameworks%2520without%2520any%2520retraining%2520or%250Afine-tuning.%2520Across%2520various%2520DiT-based%2520video%2520generation%2520models%252C%2520our%2520approach%250Ademonstrates%2520promising%2520improvements%2520in%2520both%2520temporal%2520consistency%2520and%2520visual%250Aquality.%2520We%2520hope%2520this%2520research%2520can%2520inspire%2520future%2520explorations%2520in%2520video%250Ageneration%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhance-A-Video%3A%20Better%20Generated%20Video%20for%20Free&entry.906535625=Yang%20Luo%20and%20Xuanlei%20Zhao%20and%20Mengzhao%20Chen%20and%20Kaipeng%20Zhang%20and%20Wenqi%20Shao%20and%20Kai%20Wang%20and%20Zhangyang%20Wang%20and%20Yang%20You&entry.1292438233=%20%20DiT-based%20video%20generation%20has%20achieved%20remarkable%20results%2C%20but%20research%20into%0Aenhancing%20existing%20models%20remains%20relatively%20unexplored.%20In%20this%20work%2C%20we%0Aintroduce%20a%20training-free%20approach%20to%20enhance%20the%20coherence%20and%20quality%20of%0ADiT-based%20generated%20videos%2C%20named%20Enhance-A-Video.%20The%20core%20idea%20is%20enhancing%0Athe%20cross-frame%20correlations%20based%20on%20non-diagonal%20temporal%20attention%0Adistributions.%20Thanks%20to%20its%20simple%20design%2C%20our%20approach%20can%20be%20easily%20applied%0Ato%20most%20DiT-based%20video%20generation%20frameworks%20without%20any%20retraining%20or%0Afine-tuning.%20Across%20various%20DiT-based%20video%20generation%20models%2C%20our%20approach%0Ademonstrates%20promising%20improvements%20in%20both%20temporal%20consistency%20and%20visual%0Aquality.%20We%20hope%20this%20research%20can%20inspire%20future%20explorations%20in%20video%0Ageneration%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07508v2&entry.124074799=Read"},
{"title": "Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures", "author": "Francesco Ballerini and Pierluigi Zama Ramirez and Samuele Salti and Luigi Di Stefano", "abstract": "  Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights.\n", "link": "http://arxiv.org/abs/2502.09623v1", "date": "2025-02-13", "relevancy": 2.6008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5206}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5206}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embed%20Any%20NeRF%3A%20Graph%20Meta-Networks%20for%20Neural%20Tasks%20on%20Arbitrary%20NeRF%0A%20%20Architectures&body=Title%3A%20Embed%20Any%20NeRF%3A%20Graph%20Meta-Networks%20for%20Neural%20Tasks%20on%20Arbitrary%20NeRF%0A%20%20Architectures%0AAuthor%3A%20Francesco%20Ballerini%20and%20Pierluigi%20Zama%20Ramirez%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20a%20groundbreaking%20paradigm%20for%0Arepresenting%203D%20objects%20and%20scenes%20by%20encoding%20shape%20and%20appearance%20information%0Ainto%20the%20weights%20of%20a%20neural%20network.%20Recent%20works%20have%20shown%20how%20such%20weights%0Acan%20be%20used%20as%20input%20to%20frameworks%20processing%20them%20to%20solve%20deep%20learning%0Atasks.%20Yet%2C%20these%20frameworks%20can%20only%20process%20NeRFs%20with%20a%20specific%2C%20predefined%0Aarchitecture.%20In%20this%20paper%2C%20we%20present%20the%20first%20framework%20that%20can%20ingest%0ANeRFs%20with%20multiple%20architectures%20and%20perform%20inference%20on%20architectures%20unseen%0Aat%20training%20time.%20We%20achieve%20this%20goal%20by%20training%20a%20Graph%20Meta-Network%20in%20a%0Arepresentation%20learning%20framework.%20Moreover%2C%20we%20show%20how%20a%20contrastive%0Aobjective%20is%20conducive%20to%20obtaining%20an%20architecture-agnostic%20latent%20space.%20In%0Aexperiments%20on%20both%20MLP-based%20and%20tri-planar%20NeRFs%2C%20our%20approach%20demonstrates%0Arobust%20performance%20in%20classification%20and%20retrieval%20tasks%20that%20either%20matches%20or%0Aexceeds%20that%20of%20existing%20frameworks%20constrained%20to%20single%20architectures%2C%20thus%0Aproviding%20the%20first%20architecture-agnostic%20method%20to%20perform%20tasks%20on%20NeRFs%20by%0Aprocessing%20their%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbed%2520Any%2520NeRF%253A%2520Graph%2520Meta-Networks%2520for%2520Neural%2520Tasks%2520on%2520Arbitrary%2520NeRF%250A%2520%2520Architectures%26entry.906535625%3DFrancesco%2520Ballerini%2520and%2520Pierluigi%2520Zama%2520Ramirez%2520and%2520Samuele%2520Salti%2520and%2520Luigi%2520Di%2520Stefano%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520emerged%2520as%2520a%2520groundbreaking%2520paradigm%2520for%250Arepresenting%25203D%2520objects%2520and%2520scenes%2520by%2520encoding%2520shape%2520and%2520appearance%2520information%250Ainto%2520the%2520weights%2520of%2520a%2520neural%2520network.%2520Recent%2520works%2520have%2520shown%2520how%2520such%2520weights%250Acan%2520be%2520used%2520as%2520input%2520to%2520frameworks%2520processing%2520them%2520to%2520solve%2520deep%2520learning%250Atasks.%2520Yet%252C%2520these%2520frameworks%2520can%2520only%2520process%2520NeRFs%2520with%2520a%2520specific%252C%2520predefined%250Aarchitecture.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520framework%2520that%2520can%2520ingest%250ANeRFs%2520with%2520multiple%2520architectures%2520and%2520perform%2520inference%2520on%2520architectures%2520unseen%250Aat%2520training%2520time.%2520We%2520achieve%2520this%2520goal%2520by%2520training%2520a%2520Graph%2520Meta-Network%2520in%2520a%250Arepresentation%2520learning%2520framework.%2520Moreover%252C%2520we%2520show%2520how%2520a%2520contrastive%250Aobjective%2520is%2520conducive%2520to%2520obtaining%2520an%2520architecture-agnostic%2520latent%2520space.%2520In%250Aexperiments%2520on%2520both%2520MLP-based%2520and%2520tri-planar%2520NeRFs%252C%2520our%2520approach%2520demonstrates%250Arobust%2520performance%2520in%2520classification%2520and%2520retrieval%2520tasks%2520that%2520either%2520matches%2520or%250Aexceeds%2520that%2520of%2520existing%2520frameworks%2520constrained%2520to%2520single%2520architectures%252C%2520thus%250Aproviding%2520the%2520first%2520architecture-agnostic%2520method%2520to%2520perform%2520tasks%2520on%2520NeRFs%2520by%250Aprocessing%2520their%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embed%20Any%20NeRF%3A%20Graph%20Meta-Networks%20for%20Neural%20Tasks%20on%20Arbitrary%20NeRF%0A%20%20Architectures&entry.906535625=Francesco%20Ballerini%20and%20Pierluigi%20Zama%20Ramirez%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20emerged%20as%20a%20groundbreaking%20paradigm%20for%0Arepresenting%203D%20objects%20and%20scenes%20by%20encoding%20shape%20and%20appearance%20information%0Ainto%20the%20weights%20of%20a%20neural%20network.%20Recent%20works%20have%20shown%20how%20such%20weights%0Acan%20be%20used%20as%20input%20to%20frameworks%20processing%20them%20to%20solve%20deep%20learning%0Atasks.%20Yet%2C%20these%20frameworks%20can%20only%20process%20NeRFs%20with%20a%20specific%2C%20predefined%0Aarchitecture.%20In%20this%20paper%2C%20we%20present%20the%20first%20framework%20that%20can%20ingest%0ANeRFs%20with%20multiple%20architectures%20and%20perform%20inference%20on%20architectures%20unseen%0Aat%20training%20time.%20We%20achieve%20this%20goal%20by%20training%20a%20Graph%20Meta-Network%20in%20a%0Arepresentation%20learning%20framework.%20Moreover%2C%20we%20show%20how%20a%20contrastive%0Aobjective%20is%20conducive%20to%20obtaining%20an%20architecture-agnostic%20latent%20space.%20In%0Aexperiments%20on%20both%20MLP-based%20and%20tri-planar%20NeRFs%2C%20our%20approach%20demonstrates%0Arobust%20performance%20in%20classification%20and%20retrieval%20tasks%20that%20either%20matches%20or%0Aexceeds%20that%20of%20existing%20frameworks%20constrained%20to%20single%20architectures%2C%20thus%0Aproviding%20the%20first%20architecture-agnostic%20method%20to%20perform%20tasks%20on%20NeRFs%20by%0Aprocessing%20their%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09623v1&entry.124074799=Read"},
{"title": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units", "author": "Badr AlKhamissi and Greta Tuckute and Antoine Bosselut and Martin Schrimpf", "abstract": "  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n", "link": "http://arxiv.org/abs/2411.02280v2", "date": "2025-02-13", "relevancy": 2.5945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5494}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20LLM%20Language%20Network%3A%20A%20Neuroscientific%20Approach%20for%20Identifying%0A%20%20Causally%20Task-Relevant%20Units&body=Title%3A%20The%20LLM%20Language%20Network%3A%20A%20Neuroscientific%20Approach%20for%20Identifying%0A%20%20Causally%20Task-Relevant%20Units%0AAuthor%3A%20Badr%20AlKhamissi%20and%20Greta%20Tuckute%20and%20Antoine%20Bosselut%20and%20Martin%20Schrimpf%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%20on%20not%20just%0Alanguage%20tasks%2C%20but%20also%20various%20tasks%20that%20are%20not%20linguistic%20in%20nature%2C%20such%0Aas%20logical%20reasoning%20and%20social%20inference.%20In%20the%20human%20brain%2C%20neuroscience%20has%0Aidentified%20a%20core%20language%20system%20that%20selectively%20and%20causally%20supports%0Alanguage%20processing.%20We%20here%20ask%20whether%20similar%20specialization%20for%20language%0Aemerges%20in%20LLMs.%20We%20identify%20language-selective%20units%20within%2018%20popular%20LLMs%2C%0Ausing%20the%20same%20localization%20approach%20that%20is%20used%20in%20neuroscience.%20We%20then%0Aestablish%20the%20causal%20role%20of%20these%20units%20by%20demonstrating%20that%20ablating%20LLM%0Alanguage-selective%20units%20--%20but%20not%20random%20units%20--%20leads%20to%20drastic%20deficits%0Ain%20language%20tasks.%20Correspondingly%2C%20language-selective%20LLM%20units%20are%20more%0Aaligned%20to%20brain%20recordings%20from%20the%20human%20language%20system%20than%20random%20units.%0AFinally%2C%20we%20investigate%20whether%20our%20localization%20method%20extends%20to%20other%0Acognitive%20domains%3A%20while%20we%20find%20specialized%20networks%20in%20some%20LLMs%20for%0Areasoning%20and%20social%20capabilities%2C%20there%20are%20substantial%20differences%20among%0Amodels.%20These%20findings%20provide%20functional%20and%20causal%20evidence%20for%0Aspecialization%20in%20large%20language%20models%2C%20and%20highlight%20parallels%20with%20the%0Afunctional%20organization%20in%20the%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02280v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520LLM%2520Language%2520Network%253A%2520A%2520Neuroscientific%2520Approach%2520for%2520Identifying%250A%2520%2520Causally%2520Task-Relevant%2520Units%26entry.906535625%3DBadr%2520AlKhamissi%2520and%2520Greta%2520Tuckute%2520and%2520Antoine%2520Bosselut%2520and%2520Martin%2520Schrimpf%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520capabilities%2520on%2520not%2520just%250Alanguage%2520tasks%252C%2520but%2520also%2520various%2520tasks%2520that%2520are%2520not%2520linguistic%2520in%2520nature%252C%2520such%250Aas%2520logical%2520reasoning%2520and%2520social%2520inference.%2520In%2520the%2520human%2520brain%252C%2520neuroscience%2520has%250Aidentified%2520a%2520core%2520language%2520system%2520that%2520selectively%2520and%2520causally%2520supports%250Alanguage%2520processing.%2520We%2520here%2520ask%2520whether%2520similar%2520specialization%2520for%2520language%250Aemerges%2520in%2520LLMs.%2520We%2520identify%2520language-selective%2520units%2520within%252018%2520popular%2520LLMs%252C%250Ausing%2520the%2520same%2520localization%2520approach%2520that%2520is%2520used%2520in%2520neuroscience.%2520We%2520then%250Aestablish%2520the%2520causal%2520role%2520of%2520these%2520units%2520by%2520demonstrating%2520that%2520ablating%2520LLM%250Alanguage-selective%2520units%2520--%2520but%2520not%2520random%2520units%2520--%2520leads%2520to%2520drastic%2520deficits%250Ain%2520language%2520tasks.%2520Correspondingly%252C%2520language-selective%2520LLM%2520units%2520are%2520more%250Aaligned%2520to%2520brain%2520recordings%2520from%2520the%2520human%2520language%2520system%2520than%2520random%2520units.%250AFinally%252C%2520we%2520investigate%2520whether%2520our%2520localization%2520method%2520extends%2520to%2520other%250Acognitive%2520domains%253A%2520while%2520we%2520find%2520specialized%2520networks%2520in%2520some%2520LLMs%2520for%250Areasoning%2520and%2520social%2520capabilities%252C%2520there%2520are%2520substantial%2520differences%2520among%250Amodels.%2520These%2520findings%2520provide%2520functional%2520and%2520causal%2520evidence%2520for%250Aspecialization%2520in%2520large%2520language%2520models%252C%2520and%2520highlight%2520parallels%2520with%2520the%250Afunctional%2520organization%2520in%2520the%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02280v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20LLM%20Language%20Network%3A%20A%20Neuroscientific%20Approach%20for%20Identifying%0A%20%20Causally%20Task-Relevant%20Units&entry.906535625=Badr%20AlKhamissi%20and%20Greta%20Tuckute%20and%20Antoine%20Bosselut%20and%20Martin%20Schrimpf&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%20on%20not%20just%0Alanguage%20tasks%2C%20but%20also%20various%20tasks%20that%20are%20not%20linguistic%20in%20nature%2C%20such%0Aas%20logical%20reasoning%20and%20social%20inference.%20In%20the%20human%20brain%2C%20neuroscience%20has%0Aidentified%20a%20core%20language%20system%20that%20selectively%20and%20causally%20supports%0Alanguage%20processing.%20We%20here%20ask%20whether%20similar%20specialization%20for%20language%0Aemerges%20in%20LLMs.%20We%20identify%20language-selective%20units%20within%2018%20popular%20LLMs%2C%0Ausing%20the%20same%20localization%20approach%20that%20is%20used%20in%20neuroscience.%20We%20then%0Aestablish%20the%20causal%20role%20of%20these%20units%20by%20demonstrating%20that%20ablating%20LLM%0Alanguage-selective%20units%20--%20but%20not%20random%20units%20--%20leads%20to%20drastic%20deficits%0Ain%20language%20tasks.%20Correspondingly%2C%20language-selective%20LLM%20units%20are%20more%0Aaligned%20to%20brain%20recordings%20from%20the%20human%20language%20system%20than%20random%20units.%0AFinally%2C%20we%20investigate%20whether%20our%20localization%20method%20extends%20to%20other%0Acognitive%20domains%3A%20while%20we%20find%20specialized%20networks%20in%20some%20LLMs%20for%0Areasoning%20and%20social%20capabilities%2C%20there%20are%20substantial%20differences%20among%0Amodels.%20These%20findings%20provide%20functional%20and%20causal%20evidence%20for%0Aspecialization%20in%20large%20language%20models%2C%20and%20highlight%20parallels%20with%20the%0Afunctional%20organization%20in%20the%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02280v2&entry.124074799=Read"},
{"title": "Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights", "author": "Jonathan Kahana and Or Nathan and Eliahu Horwitz and Yedid Hoshen", "abstract": "  With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.\n", "link": "http://arxiv.org/abs/2502.09619v1", "date": "2025-02-13", "relevancy": 2.5888, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20this%20Model%20Also%20Recognize%20Dogs%3F%20Zero-Shot%20Model%20Search%20from%20Weights&body=Title%3A%20Can%20this%20Model%20Also%20Recognize%20Dogs%3F%20Zero-Shot%20Model%20Search%20from%20Weights%0AAuthor%3A%20Jonathan%20Kahana%20and%20Or%20Nathan%20and%20Eliahu%20Horwitz%20and%20Yedid%20Hoshen%0AAbstract%3A%20%20%20With%20the%20increasing%20numbers%20of%20publicly%20available%20models%2C%20there%20are%20probably%0Apretrained%2C%20online%20models%20for%20most%20tasks%20users%20require.%20However%2C%20current%20model%0Asearch%20methods%20are%20rudimentary%2C%20essentially%20a%20text-based%20search%20in%20the%0Adocumentation%2C%20thus%20users%20cannot%20find%20the%20relevant%20models.%20This%20paper%20presents%0AProbeLog%2C%20a%20method%20for%20retrieving%20classification%20models%20that%20can%20recognize%20a%0Atarget%20concept%2C%20such%20as%20%22Dog%22%2C%20without%20access%20to%20model%20metadata%20or%20training%0Adata.%20Differently%20from%20previous%20probing%20methods%2C%20ProbeLog%20computes%20a%20descriptor%0Afor%20each%20output%20dimension%20%28logit%29%20of%20each%20model%2C%20by%20observing%20its%20responses%20on%0Aa%20fixed%20set%20of%20inputs%20%28probes%29.%20Our%20method%20supports%20both%20logit-based%20retrieval%0A%28%22find%20more%20logits%20like%20this%22%29%20and%20zero-shot%2C%20text-based%20retrieval%20%28%22find%20all%0Alogits%20corresponding%20to%20dogs%22%29.%20As%20probing-based%20representations%20require%0Amultiple%20costly%20feedforward%20passes%20through%20the%20model%2C%20we%20develop%20a%20method%2C%0Abased%20on%20collaborative%20filtering%2C%20that%20reduces%20the%20cost%20of%20encoding%0Arepositories%20by%203x.%20We%20demonstrate%20that%20ProbeLog%20achieves%20high%20retrieval%0Aaccuracy%2C%20both%20in%20real-world%20and%20fine-grained%20search%20tasks%20and%20is%20scalable%20to%0Afull-size%20repositories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520this%2520Model%2520Also%2520Recognize%2520Dogs%253F%2520Zero-Shot%2520Model%2520Search%2520from%2520Weights%26entry.906535625%3DJonathan%2520Kahana%2520and%2520Or%2520Nathan%2520and%2520Eliahu%2520Horwitz%2520and%2520Yedid%2520Hoshen%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520numbers%2520of%2520publicly%2520available%2520models%252C%2520there%2520are%2520probably%250Apretrained%252C%2520online%2520models%2520for%2520most%2520tasks%2520users%2520require.%2520However%252C%2520current%2520model%250Asearch%2520methods%2520are%2520rudimentary%252C%2520essentially%2520a%2520text-based%2520search%2520in%2520the%250Adocumentation%252C%2520thus%2520users%2520cannot%2520find%2520the%2520relevant%2520models.%2520This%2520paper%2520presents%250AProbeLog%252C%2520a%2520method%2520for%2520retrieving%2520classification%2520models%2520that%2520can%2520recognize%2520a%250Atarget%2520concept%252C%2520such%2520as%2520%2522Dog%2522%252C%2520without%2520access%2520to%2520model%2520metadata%2520or%2520training%250Adata.%2520Differently%2520from%2520previous%2520probing%2520methods%252C%2520ProbeLog%2520computes%2520a%2520descriptor%250Afor%2520each%2520output%2520dimension%2520%2528logit%2529%2520of%2520each%2520model%252C%2520by%2520observing%2520its%2520responses%2520on%250Aa%2520fixed%2520set%2520of%2520inputs%2520%2528probes%2529.%2520Our%2520method%2520supports%2520both%2520logit-based%2520retrieval%250A%2528%2522find%2520more%2520logits%2520like%2520this%2522%2529%2520and%2520zero-shot%252C%2520text-based%2520retrieval%2520%2528%2522find%2520all%250Alogits%2520corresponding%2520to%2520dogs%2522%2529.%2520As%2520probing-based%2520representations%2520require%250Amultiple%2520costly%2520feedforward%2520passes%2520through%2520the%2520model%252C%2520we%2520develop%2520a%2520method%252C%250Abased%2520on%2520collaborative%2520filtering%252C%2520that%2520reduces%2520the%2520cost%2520of%2520encoding%250Arepositories%2520by%25203x.%2520We%2520demonstrate%2520that%2520ProbeLog%2520achieves%2520high%2520retrieval%250Aaccuracy%252C%2520both%2520in%2520real-world%2520and%2520fine-grained%2520search%2520tasks%2520and%2520is%2520scalable%2520to%250Afull-size%2520repositories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20this%20Model%20Also%20Recognize%20Dogs%3F%20Zero-Shot%20Model%20Search%20from%20Weights&entry.906535625=Jonathan%20Kahana%20and%20Or%20Nathan%20and%20Eliahu%20Horwitz%20and%20Yedid%20Hoshen&entry.1292438233=%20%20With%20the%20increasing%20numbers%20of%20publicly%20available%20models%2C%20there%20are%20probably%0Apretrained%2C%20online%20models%20for%20most%20tasks%20users%20require.%20However%2C%20current%20model%0Asearch%20methods%20are%20rudimentary%2C%20essentially%20a%20text-based%20search%20in%20the%0Adocumentation%2C%20thus%20users%20cannot%20find%20the%20relevant%20models.%20This%20paper%20presents%0AProbeLog%2C%20a%20method%20for%20retrieving%20classification%20models%20that%20can%20recognize%20a%0Atarget%20concept%2C%20such%20as%20%22Dog%22%2C%20without%20access%20to%20model%20metadata%20or%20training%0Adata.%20Differently%20from%20previous%20probing%20methods%2C%20ProbeLog%20computes%20a%20descriptor%0Afor%20each%20output%20dimension%20%28logit%29%20of%20each%20model%2C%20by%20observing%20its%20responses%20on%0Aa%20fixed%20set%20of%20inputs%20%28probes%29.%20Our%20method%20supports%20both%20logit-based%20retrieval%0A%28%22find%20more%20logits%20like%20this%22%29%20and%20zero-shot%2C%20text-based%20retrieval%20%28%22find%20all%0Alogits%20corresponding%20to%20dogs%22%29.%20As%20probing-based%20representations%20require%0Amultiple%20costly%20feedforward%20passes%20through%20the%20model%2C%20we%20develop%20a%20method%2C%0Abased%20on%20collaborative%20filtering%2C%20that%20reduces%20the%20cost%20of%20encoding%0Arepositories%20by%203x.%20We%20demonstrate%20that%20ProbeLog%20achieves%20high%20retrieval%0Aaccuracy%2C%20both%20in%20real-world%20and%20fine-grained%20search%20tasks%20and%20is%20scalable%20to%0Afull-size%20repositories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09619v1&entry.124074799=Read"},
{"title": "Measuring Human Contribution in AI-Assisted Content Generation", "author": "Yueqi Xie and Tao Qi and Jingwei Yi and Xiyuan Yang and Ryan Whalen and Junming Huang and Qian Ding and Yu Xie and Xing Xie and Fangzhao Wu", "abstract": "  With the growing prevalence of generative artificial intelligence (AI), an\nincreasing amount of content is no longer exclusively generated by humans but\nby generative AI models with human guidance. This shift presents notable\nchallenges for the delineation of originality due to the varying degrees of\nhuman contribution in AI-assisted works. This study raises the research\nquestion of measuring human contribution in AI-assisted content generation and\nintroduces a framework to address this question that is grounded in information\ntheory. By calculating mutual information between human input and AI-assisted\noutput relative to self-information of AI-assisted output, we quantify the\nproportional information contribution of humans in content generation. Our\nexperimental results demonstrate that the proposed measure effectively\ndiscriminates between varying degrees of human contribution across multiple\ncreative domains. We hope that this work lays a foundation for measuring human\ncontributions in AI-assisted content generation in the era of generative AI.\n", "link": "http://arxiv.org/abs/2408.14792v2", "date": "2025-02-13", "relevancy": 2.5802, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5258}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5135}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Human%20Contribution%20in%20AI-Assisted%20Content%20Generation&body=Title%3A%20Measuring%20Human%20Contribution%20in%20AI-Assisted%20Content%20Generation%0AAuthor%3A%20Yueqi%20Xie%20and%20Tao%20Qi%20and%20Jingwei%20Yi%20and%20Xiyuan%20Yang%20and%20Ryan%20Whalen%20and%20Junming%20Huang%20and%20Qian%20Ding%20and%20Yu%20Xie%20and%20Xing%20Xie%20and%20Fangzhao%20Wu%0AAbstract%3A%20%20%20With%20the%20growing%20prevalence%20of%20generative%20artificial%20intelligence%20%28AI%29%2C%20an%0Aincreasing%20amount%20of%20content%20is%20no%20longer%20exclusively%20generated%20by%20humans%20but%0Aby%20generative%20AI%20models%20with%20human%20guidance.%20This%20shift%20presents%20notable%0Achallenges%20for%20the%20delineation%20of%20originality%20due%20to%20the%20varying%20degrees%20of%0Ahuman%20contribution%20in%20AI-assisted%20works.%20This%20study%20raises%20the%20research%0Aquestion%20of%20measuring%20human%20contribution%20in%20AI-assisted%20content%20generation%20and%0Aintroduces%20a%20framework%20to%20address%20this%20question%20that%20is%20grounded%20in%20information%0Atheory.%20By%20calculating%20mutual%20information%20between%20human%20input%20and%20AI-assisted%0Aoutput%20relative%20to%20self-information%20of%20AI-assisted%20output%2C%20we%20quantify%20the%0Aproportional%20information%20contribution%20of%20humans%20in%20content%20generation.%20Our%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20measure%20effectively%0Adiscriminates%20between%20varying%20degrees%20of%20human%20contribution%20across%20multiple%0Acreative%20domains.%20We%20hope%20that%20this%20work%20lays%20a%20foundation%20for%20measuring%20human%0Acontributions%20in%20AI-assisted%20content%20generation%20in%20the%20era%20of%20generative%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Human%2520Contribution%2520in%2520AI-Assisted%2520Content%2520Generation%26entry.906535625%3DYueqi%2520Xie%2520and%2520Tao%2520Qi%2520and%2520Jingwei%2520Yi%2520and%2520Xiyuan%2520Yang%2520and%2520Ryan%2520Whalen%2520and%2520Junming%2520Huang%2520and%2520Qian%2520Ding%2520and%2520Yu%2520Xie%2520and%2520Xing%2520Xie%2520and%2520Fangzhao%2520Wu%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520prevalence%2520of%2520generative%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520an%250Aincreasing%2520amount%2520of%2520content%2520is%2520no%2520longer%2520exclusively%2520generated%2520by%2520humans%2520but%250Aby%2520generative%2520AI%2520models%2520with%2520human%2520guidance.%2520This%2520shift%2520presents%2520notable%250Achallenges%2520for%2520the%2520delineation%2520of%2520originality%2520due%2520to%2520the%2520varying%2520degrees%2520of%250Ahuman%2520contribution%2520in%2520AI-assisted%2520works.%2520This%2520study%2520raises%2520the%2520research%250Aquestion%2520of%2520measuring%2520human%2520contribution%2520in%2520AI-assisted%2520content%2520generation%2520and%250Aintroduces%2520a%2520framework%2520to%2520address%2520this%2520question%2520that%2520is%2520grounded%2520in%2520information%250Atheory.%2520By%2520calculating%2520mutual%2520information%2520between%2520human%2520input%2520and%2520AI-assisted%250Aoutput%2520relative%2520to%2520self-information%2520of%2520AI-assisted%2520output%252C%2520we%2520quantify%2520the%250Aproportional%2520information%2520contribution%2520of%2520humans%2520in%2520content%2520generation.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520measure%2520effectively%250Adiscriminates%2520between%2520varying%2520degrees%2520of%2520human%2520contribution%2520across%2520multiple%250Acreative%2520domains.%2520We%2520hope%2520that%2520this%2520work%2520lays%2520a%2520foundation%2520for%2520measuring%2520human%250Acontributions%2520in%2520AI-assisted%2520content%2520generation%2520in%2520the%2520era%2520of%2520generative%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Human%20Contribution%20in%20AI-Assisted%20Content%20Generation&entry.906535625=Yueqi%20Xie%20and%20Tao%20Qi%20and%20Jingwei%20Yi%20and%20Xiyuan%20Yang%20and%20Ryan%20Whalen%20and%20Junming%20Huang%20and%20Qian%20Ding%20and%20Yu%20Xie%20and%20Xing%20Xie%20and%20Fangzhao%20Wu&entry.1292438233=%20%20With%20the%20growing%20prevalence%20of%20generative%20artificial%20intelligence%20%28AI%29%2C%20an%0Aincreasing%20amount%20of%20content%20is%20no%20longer%20exclusively%20generated%20by%20humans%20but%0Aby%20generative%20AI%20models%20with%20human%20guidance.%20This%20shift%20presents%20notable%0Achallenges%20for%20the%20delineation%20of%20originality%20due%20to%20the%20varying%20degrees%20of%0Ahuman%20contribution%20in%20AI-assisted%20works.%20This%20study%20raises%20the%20research%0Aquestion%20of%20measuring%20human%20contribution%20in%20AI-assisted%20content%20generation%20and%0Aintroduces%20a%20framework%20to%20address%20this%20question%20that%20is%20grounded%20in%20information%0Atheory.%20By%20calculating%20mutual%20information%20between%20human%20input%20and%20AI-assisted%0Aoutput%20relative%20to%20self-information%20of%20AI-assisted%20output%2C%20we%20quantify%20the%0Aproportional%20information%20contribution%20of%20humans%20in%20content%20generation.%20Our%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20measure%20effectively%0Adiscriminates%20between%20varying%20degrees%20of%20human%20contribution%20across%20multiple%0Acreative%20domains.%20We%20hope%20that%20this%20work%20lays%20a%20foundation%20for%20measuring%20human%0Acontributions%20in%20AI-assisted%20content%20generation%20in%20the%20era%20of%20generative%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14792v2&entry.124074799=Read"},
{"title": "Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion\n  Model", "author": "Fei Shen and Cong Wang and Junyao Gao and Qin Guo and Jisheng Dang and Jinhui Tang and Tat-Seng Chua", "abstract": "  Recent advances in conditional diffusion models have shown promise for\ngenerating realistic TalkingFace videos, yet challenges persist in achieving\nconsistent head movement, synchronized facial expressions, and accurate lip\nsynchronization over extended generations. To address these, we introduce the\n\\textbf{M}otion-priors \\textbf{C}onditional \\textbf{D}iffusion \\textbf{M}odel\n(\\textbf{MCDM}), which utilizes both archived and current clip motion priors to\nenhance motion prediction and ensure temporal consistency. The model consists\nof three key elements: (1) an archived-clip motion-prior that incorporates\nhistorical frames and a reference frame to preserve identity and context; (2) a\npresent-clip motion-prior diffusion model that captures multimodal causality\nfor accurate predictions of head movements, lip sync, and expressions; and (3)\na memory-efficient temporal attention mechanism that mitigates error\naccumulation by dynamically storing and updating motion features. We also\nrelease the \\textbf{TalkingFace-Wild} dataset, a multilingual collection of\nover 200 hours of footage across 10 languages. Experimental results demonstrate\nthe effectiveness of MCDM in maintaining identity and motion continuity for\nlong-term TalkingFace generation. Code, models, and datasets will be publicly\navailable.\n", "link": "http://arxiv.org/abs/2502.09533v1", "date": "2025-02-13", "relevancy": 2.5719, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6507}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6443}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Term%20TalkingFace%20Generation%20via%20Motion-Prior%20Conditional%20Diffusion%0A%20%20Model&body=Title%3A%20Long-Term%20TalkingFace%20Generation%20via%20Motion-Prior%20Conditional%20Diffusion%0A%20%20Model%0AAuthor%3A%20Fei%20Shen%20and%20Cong%20Wang%20and%20Junyao%20Gao%20and%20Qin%20Guo%20and%20Jisheng%20Dang%20and%20Jinhui%20Tang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Recent%20advances%20in%20conditional%20diffusion%20models%20have%20shown%20promise%20for%0Agenerating%20realistic%20TalkingFace%20videos%2C%20yet%20challenges%20persist%20in%20achieving%0Aconsistent%20head%20movement%2C%20synchronized%20facial%20expressions%2C%20and%20accurate%20lip%0Asynchronization%20over%20extended%20generations.%20To%20address%20these%2C%20we%20introduce%20the%0A%5Ctextbf%7BM%7Dotion-priors%20%5Ctextbf%7BC%7Donditional%20%5Ctextbf%7BD%7Diffusion%20%5Ctextbf%7BM%7Dodel%0A%28%5Ctextbf%7BMCDM%7D%29%2C%20which%20utilizes%20both%20archived%20and%20current%20clip%20motion%20priors%20to%0Aenhance%20motion%20prediction%20and%20ensure%20temporal%20consistency.%20The%20model%20consists%0Aof%20three%20key%20elements%3A%20%281%29%20an%20archived-clip%20motion-prior%20that%20incorporates%0Ahistorical%20frames%20and%20a%20reference%20frame%20to%20preserve%20identity%20and%20context%3B%20%282%29%20a%0Apresent-clip%20motion-prior%20diffusion%20model%20that%20captures%20multimodal%20causality%0Afor%20accurate%20predictions%20of%20head%20movements%2C%20lip%20sync%2C%20and%20expressions%3B%20and%20%283%29%0Aa%20memory-efficient%20temporal%20attention%20mechanism%20that%20mitigates%20error%0Aaccumulation%20by%20dynamically%20storing%20and%20updating%20motion%20features.%20We%20also%0Arelease%20the%20%5Ctextbf%7BTalkingFace-Wild%7D%20dataset%2C%20a%20multilingual%20collection%20of%0Aover%20200%20hours%20of%20footage%20across%2010%20languages.%20Experimental%20results%20demonstrate%0Athe%20effectiveness%20of%20MCDM%20in%20maintaining%20identity%20and%20motion%20continuity%20for%0Along-term%20TalkingFace%20generation.%20Code%2C%20models%2C%20and%20datasets%20will%20be%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Term%2520TalkingFace%2520Generation%2520via%2520Motion-Prior%2520Conditional%2520Diffusion%250A%2520%2520Model%26entry.906535625%3DFei%2520Shen%2520and%2520Cong%2520Wang%2520and%2520Junyao%2520Gao%2520and%2520Qin%2520Guo%2520and%2520Jisheng%2520Dang%2520and%2520Jinhui%2520Tang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520conditional%2520diffusion%2520models%2520have%2520shown%2520promise%2520for%250Agenerating%2520realistic%2520TalkingFace%2520videos%252C%2520yet%2520challenges%2520persist%2520in%2520achieving%250Aconsistent%2520head%2520movement%252C%2520synchronized%2520facial%2520expressions%252C%2520and%2520accurate%2520lip%250Asynchronization%2520over%2520extended%2520generations.%2520To%2520address%2520these%252C%2520we%2520introduce%2520the%250A%255Ctextbf%257BM%257Dotion-priors%2520%255Ctextbf%257BC%257Donditional%2520%255Ctextbf%257BD%257Diffusion%2520%255Ctextbf%257BM%257Dodel%250A%2528%255Ctextbf%257BMCDM%257D%2529%252C%2520which%2520utilizes%2520both%2520archived%2520and%2520current%2520clip%2520motion%2520priors%2520to%250Aenhance%2520motion%2520prediction%2520and%2520ensure%2520temporal%2520consistency.%2520The%2520model%2520consists%250Aof%2520three%2520key%2520elements%253A%2520%25281%2529%2520an%2520archived-clip%2520motion-prior%2520that%2520incorporates%250Ahistorical%2520frames%2520and%2520a%2520reference%2520frame%2520to%2520preserve%2520identity%2520and%2520context%253B%2520%25282%2529%2520a%250Apresent-clip%2520motion-prior%2520diffusion%2520model%2520that%2520captures%2520multimodal%2520causality%250Afor%2520accurate%2520predictions%2520of%2520head%2520movements%252C%2520lip%2520sync%252C%2520and%2520expressions%253B%2520and%2520%25283%2529%250Aa%2520memory-efficient%2520temporal%2520attention%2520mechanism%2520that%2520mitigates%2520error%250Aaccumulation%2520by%2520dynamically%2520storing%2520and%2520updating%2520motion%2520features.%2520We%2520also%250Arelease%2520the%2520%255Ctextbf%257BTalkingFace-Wild%257D%2520dataset%252C%2520a%2520multilingual%2520collection%2520of%250Aover%2520200%2520hours%2520of%2520footage%2520across%252010%2520languages.%2520Experimental%2520results%2520demonstrate%250Athe%2520effectiveness%2520of%2520MCDM%2520in%2520maintaining%2520identity%2520and%2520motion%2520continuity%2520for%250Along-term%2520TalkingFace%2520generation.%2520Code%252C%2520models%252C%2520and%2520datasets%2520will%2520be%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Term%20TalkingFace%20Generation%20via%20Motion-Prior%20Conditional%20Diffusion%0A%20%20Model&entry.906535625=Fei%20Shen%20and%20Cong%20Wang%20and%20Junyao%20Gao%20and%20Qin%20Guo%20and%20Jisheng%20Dang%20and%20Jinhui%20Tang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Recent%20advances%20in%20conditional%20diffusion%20models%20have%20shown%20promise%20for%0Agenerating%20realistic%20TalkingFace%20videos%2C%20yet%20challenges%20persist%20in%20achieving%0Aconsistent%20head%20movement%2C%20synchronized%20facial%20expressions%2C%20and%20accurate%20lip%0Asynchronization%20over%20extended%20generations.%20To%20address%20these%2C%20we%20introduce%20the%0A%5Ctextbf%7BM%7Dotion-priors%20%5Ctextbf%7BC%7Donditional%20%5Ctextbf%7BD%7Diffusion%20%5Ctextbf%7BM%7Dodel%0A%28%5Ctextbf%7BMCDM%7D%29%2C%20which%20utilizes%20both%20archived%20and%20current%20clip%20motion%20priors%20to%0Aenhance%20motion%20prediction%20and%20ensure%20temporal%20consistency.%20The%20model%20consists%0Aof%20three%20key%20elements%3A%20%281%29%20an%20archived-clip%20motion-prior%20that%20incorporates%0Ahistorical%20frames%20and%20a%20reference%20frame%20to%20preserve%20identity%20and%20context%3B%20%282%29%20a%0Apresent-clip%20motion-prior%20diffusion%20model%20that%20captures%20multimodal%20causality%0Afor%20accurate%20predictions%20of%20head%20movements%2C%20lip%20sync%2C%20and%20expressions%3B%20and%20%283%29%0Aa%20memory-efficient%20temporal%20attention%20mechanism%20that%20mitigates%20error%0Aaccumulation%20by%20dynamically%20storing%20and%20updating%20motion%20features.%20We%20also%0Arelease%20the%20%5Ctextbf%7BTalkingFace-Wild%7D%20dataset%2C%20a%20multilingual%20collection%20of%0Aover%20200%20hours%20of%20footage%20across%2010%20languages.%20Experimental%20results%20demonstrate%0Athe%20effectiveness%20of%20MCDM%20in%20maintaining%20identity%20and%20motion%20continuity%20for%0Along-term%20TalkingFace%20generation.%20Code%2C%20models%2C%20and%20datasets%20will%20be%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09533v1&entry.124074799=Read"},
{"title": "Zero-Shot Offline Imitation Learning via Optimal Transport", "author": "Thomas Rupf and Marco Bagatella and Nico G\u00fcrtler and Jonas Frey and Georg Martius", "abstract": "  Zero-shot imitation learning algorithms hold the promise of reproducing\nunseen behavior from as little as a single demonstration at test time. Existing\npractical approaches view the expert demonstration as a sequence of goals,\nenabling imitation with a high-level goal selector, and a low-level\ngoal-conditioned policy. However, this framework can suffer from myopic\nbehavior: the agent's immediate actions towards achieving individual goals may\nundermine long-term objectives. We introduce a novel method that mitigates this\nissue by directly optimizing the occupancy matching objective that is intrinsic\nto imitation learning. We propose to lift a goal-conditioned value function to\na distance between occupancies, which are in turn approximated via a learned\nworld model. The resulting method can learn from offline, suboptimal data, and\nis capable of non-myopic, zero-shot imitation, as we demonstrate in complex,\ncontinuous benchmarks.\n", "link": "http://arxiv.org/abs/2410.08751v2", "date": "2025-02-13", "relevancy": 2.543, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5138}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5133}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Offline%20Imitation%20Learning%20via%20Optimal%20Transport&body=Title%3A%20Zero-Shot%20Offline%20Imitation%20Learning%20via%20Optimal%20Transport%0AAuthor%3A%20Thomas%20Rupf%20and%20Marco%20Bagatella%20and%20Nico%20G%C3%BCrtler%20and%20Jonas%20Frey%20and%20Georg%20Martius%0AAbstract%3A%20%20%20Zero-shot%20imitation%20learning%20algorithms%20hold%20the%20promise%20of%20reproducing%0Aunseen%20behavior%20from%20as%20little%20as%20a%20single%20demonstration%20at%20test%20time.%20Existing%0Apractical%20approaches%20view%20the%20expert%20demonstration%20as%20a%20sequence%20of%20goals%2C%0Aenabling%20imitation%20with%20a%20high-level%20goal%20selector%2C%20and%20a%20low-level%0Agoal-conditioned%20policy.%20However%2C%20this%20framework%20can%20suffer%20from%20myopic%0Abehavior%3A%20the%20agent%27s%20immediate%20actions%20towards%20achieving%20individual%20goals%20may%0Aundermine%20long-term%20objectives.%20We%20introduce%20a%20novel%20method%20that%20mitigates%20this%0Aissue%20by%20directly%20optimizing%20the%20occupancy%20matching%20objective%20that%20is%20intrinsic%0Ato%20imitation%20learning.%20We%20propose%20to%20lift%20a%20goal-conditioned%20value%20function%20to%0Aa%20distance%20between%20occupancies%2C%20which%20are%20in%20turn%20approximated%20via%20a%20learned%0Aworld%20model.%20The%20resulting%20method%20can%20learn%20from%20offline%2C%20suboptimal%20data%2C%20and%0Ais%20capable%20of%20non-myopic%2C%20zero-shot%20imitation%2C%20as%20we%20demonstrate%20in%20complex%2C%0Acontinuous%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Offline%2520Imitation%2520Learning%2520via%2520Optimal%2520Transport%26entry.906535625%3DThomas%2520Rupf%2520and%2520Marco%2520Bagatella%2520and%2520Nico%2520G%25C3%25BCrtler%2520and%2520Jonas%2520Frey%2520and%2520Georg%2520Martius%26entry.1292438233%3D%2520%2520Zero-shot%2520imitation%2520learning%2520algorithms%2520hold%2520the%2520promise%2520of%2520reproducing%250Aunseen%2520behavior%2520from%2520as%2520little%2520as%2520a%2520single%2520demonstration%2520at%2520test%2520time.%2520Existing%250Apractical%2520approaches%2520view%2520the%2520expert%2520demonstration%2520as%2520a%2520sequence%2520of%2520goals%252C%250Aenabling%2520imitation%2520with%2520a%2520high-level%2520goal%2520selector%252C%2520and%2520a%2520low-level%250Agoal-conditioned%2520policy.%2520However%252C%2520this%2520framework%2520can%2520suffer%2520from%2520myopic%250Abehavior%253A%2520the%2520agent%2527s%2520immediate%2520actions%2520towards%2520achieving%2520individual%2520goals%2520may%250Aundermine%2520long-term%2520objectives.%2520We%2520introduce%2520a%2520novel%2520method%2520that%2520mitigates%2520this%250Aissue%2520by%2520directly%2520optimizing%2520the%2520occupancy%2520matching%2520objective%2520that%2520is%2520intrinsic%250Ato%2520imitation%2520learning.%2520We%2520propose%2520to%2520lift%2520a%2520goal-conditioned%2520value%2520function%2520to%250Aa%2520distance%2520between%2520occupancies%252C%2520which%2520are%2520in%2520turn%2520approximated%2520via%2520a%2520learned%250Aworld%2520model.%2520The%2520resulting%2520method%2520can%2520learn%2520from%2520offline%252C%2520suboptimal%2520data%252C%2520and%250Ais%2520capable%2520of%2520non-myopic%252C%2520zero-shot%2520imitation%252C%2520as%2520we%2520demonstrate%2520in%2520complex%252C%250Acontinuous%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Offline%20Imitation%20Learning%20via%20Optimal%20Transport&entry.906535625=Thomas%20Rupf%20and%20Marco%20Bagatella%20and%20Nico%20G%C3%BCrtler%20and%20Jonas%20Frey%20and%20Georg%20Martius&entry.1292438233=%20%20Zero-shot%20imitation%20learning%20algorithms%20hold%20the%20promise%20of%20reproducing%0Aunseen%20behavior%20from%20as%20little%20as%20a%20single%20demonstration%20at%20test%20time.%20Existing%0Apractical%20approaches%20view%20the%20expert%20demonstration%20as%20a%20sequence%20of%20goals%2C%0Aenabling%20imitation%20with%20a%20high-level%20goal%20selector%2C%20and%20a%20low-level%0Agoal-conditioned%20policy.%20However%2C%20this%20framework%20can%20suffer%20from%20myopic%0Abehavior%3A%20the%20agent%27s%20immediate%20actions%20towards%20achieving%20individual%20goals%20may%0Aundermine%20long-term%20objectives.%20We%20introduce%20a%20novel%20method%20that%20mitigates%20this%0Aissue%20by%20directly%20optimizing%20the%20occupancy%20matching%20objective%20that%20is%20intrinsic%0Ato%20imitation%20learning.%20We%20propose%20to%20lift%20a%20goal-conditioned%20value%20function%20to%0Aa%20distance%20between%20occupancies%2C%20which%20are%20in%20turn%20approximated%20via%20a%20learned%0Aworld%20model.%20The%20resulting%20method%20can%20learn%20from%20offline%2C%20suboptimal%20data%2C%20and%0Ais%20capable%20of%20non-myopic%2C%20zero-shot%20imitation%2C%20as%20we%20demonstrate%20in%20complex%2C%0Acontinuous%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08751v2&entry.124074799=Read"},
{"title": "Enhancing the Utility of Higher-Order Information in Relational Learning", "author": "Raphael Pellegrin and Lukas Fesser and Melanie Weber", "abstract": "  Higher-order information is crucial for relational learning in many domains\nwhere relationships extend beyond pairwise interactions. Hypergraphs provide a\nnatural framework for modeling such relationships, which has motivated recent\nextensions of graph neural net- work architectures to hypergraphs. However,\ncomparisons between hypergraph architectures and standard graph-level models\nremain limited. In this work, we systematically evaluate a selection of\nhypergraph-level and graph-level architectures, to determine their\neffectiveness in leveraging higher-order information in relational learning.\nOur results show that graph-level architectures applied to hypergraph\nexpansions often outperform hypergraph- level ones, even on inputs that are\nnaturally parametrized as hypergraphs. As an alternative approach for\nleveraging higher-order information, we propose hypergraph-level encodings\nbased on classical hypergraph characteristics. While these encodings do not\nsignificantly improve hypergraph architectures, they yield substantial\nperformance gains when combined with graph-level models. Our theoretical\nanalysis shows that hypergraph-level encodings provably increase the\nrepresentational power of message-passing graph neural networks beyond that of\ntheir graph-level counterparts.\n", "link": "http://arxiv.org/abs/2502.09570v1", "date": "2025-02-13", "relevancy": 2.4467, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20the%20Utility%20of%20Higher-Order%20Information%20in%20Relational%20Learning&body=Title%3A%20Enhancing%20the%20Utility%20of%20Higher-Order%20Information%20in%20Relational%20Learning%0AAuthor%3A%20Raphael%20Pellegrin%20and%20Lukas%20Fesser%20and%20Melanie%20Weber%0AAbstract%3A%20%20%20Higher-order%20information%20is%20crucial%20for%20relational%20learning%20in%20many%20domains%0Awhere%20relationships%20extend%20beyond%20pairwise%20interactions.%20Hypergraphs%20provide%20a%0Anatural%20framework%20for%20modeling%20such%20relationships%2C%20which%20has%20motivated%20recent%0Aextensions%20of%20graph%20neural%20net-%20work%20architectures%20to%20hypergraphs.%20However%2C%0Acomparisons%20between%20hypergraph%20architectures%20and%20standard%20graph-level%20models%0Aremain%20limited.%20In%20this%20work%2C%20we%20systematically%20evaluate%20a%20selection%20of%0Ahypergraph-level%20and%20graph-level%20architectures%2C%20to%20determine%20their%0Aeffectiveness%20in%20leveraging%20higher-order%20information%20in%20relational%20learning.%0AOur%20results%20show%20that%20graph-level%20architectures%20applied%20to%20hypergraph%0Aexpansions%20often%20outperform%20hypergraph-%20level%20ones%2C%20even%20on%20inputs%20that%20are%0Anaturally%20parametrized%20as%20hypergraphs.%20As%20an%20alternative%20approach%20for%0Aleveraging%20higher-order%20information%2C%20we%20propose%20hypergraph-level%20encodings%0Abased%20on%20classical%20hypergraph%20characteristics.%20While%20these%20encodings%20do%20not%0Asignificantly%20improve%20hypergraph%20architectures%2C%20they%20yield%20substantial%0Aperformance%20gains%20when%20combined%20with%20graph-level%20models.%20Our%20theoretical%0Aanalysis%20shows%20that%20hypergraph-level%20encodings%20provably%20increase%20the%0Arepresentational%20power%20of%20message-passing%20graph%20neural%20networks%20beyond%20that%20of%0Atheir%20graph-level%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520the%2520Utility%2520of%2520Higher-Order%2520Information%2520in%2520Relational%2520Learning%26entry.906535625%3DRaphael%2520Pellegrin%2520and%2520Lukas%2520Fesser%2520and%2520Melanie%2520Weber%26entry.1292438233%3D%2520%2520Higher-order%2520information%2520is%2520crucial%2520for%2520relational%2520learning%2520in%2520many%2520domains%250Awhere%2520relationships%2520extend%2520beyond%2520pairwise%2520interactions.%2520Hypergraphs%2520provide%2520a%250Anatural%2520framework%2520for%2520modeling%2520such%2520relationships%252C%2520which%2520has%2520motivated%2520recent%250Aextensions%2520of%2520graph%2520neural%2520net-%2520work%2520architectures%2520to%2520hypergraphs.%2520However%252C%250Acomparisons%2520between%2520hypergraph%2520architectures%2520and%2520standard%2520graph-level%2520models%250Aremain%2520limited.%2520In%2520this%2520work%252C%2520we%2520systematically%2520evaluate%2520a%2520selection%2520of%250Ahypergraph-level%2520and%2520graph-level%2520architectures%252C%2520to%2520determine%2520their%250Aeffectiveness%2520in%2520leveraging%2520higher-order%2520information%2520in%2520relational%2520learning.%250AOur%2520results%2520show%2520that%2520graph-level%2520architectures%2520applied%2520to%2520hypergraph%250Aexpansions%2520often%2520outperform%2520hypergraph-%2520level%2520ones%252C%2520even%2520on%2520inputs%2520that%2520are%250Anaturally%2520parametrized%2520as%2520hypergraphs.%2520As%2520an%2520alternative%2520approach%2520for%250Aleveraging%2520higher-order%2520information%252C%2520we%2520propose%2520hypergraph-level%2520encodings%250Abased%2520on%2520classical%2520hypergraph%2520characteristics.%2520While%2520these%2520encodings%2520do%2520not%250Asignificantly%2520improve%2520hypergraph%2520architectures%252C%2520they%2520yield%2520substantial%250Aperformance%2520gains%2520when%2520combined%2520with%2520graph-level%2520models.%2520Our%2520theoretical%250Aanalysis%2520shows%2520that%2520hypergraph-level%2520encodings%2520provably%2520increase%2520the%250Arepresentational%2520power%2520of%2520message-passing%2520graph%2520neural%2520networks%2520beyond%2520that%2520of%250Atheir%2520graph-level%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20the%20Utility%20of%20Higher-Order%20Information%20in%20Relational%20Learning&entry.906535625=Raphael%20Pellegrin%20and%20Lukas%20Fesser%20and%20Melanie%20Weber&entry.1292438233=%20%20Higher-order%20information%20is%20crucial%20for%20relational%20learning%20in%20many%20domains%0Awhere%20relationships%20extend%20beyond%20pairwise%20interactions.%20Hypergraphs%20provide%20a%0Anatural%20framework%20for%20modeling%20such%20relationships%2C%20which%20has%20motivated%20recent%0Aextensions%20of%20graph%20neural%20net-%20work%20architectures%20to%20hypergraphs.%20However%2C%0Acomparisons%20between%20hypergraph%20architectures%20and%20standard%20graph-level%20models%0Aremain%20limited.%20In%20this%20work%2C%20we%20systematically%20evaluate%20a%20selection%20of%0Ahypergraph-level%20and%20graph-level%20architectures%2C%20to%20determine%20their%0Aeffectiveness%20in%20leveraging%20higher-order%20information%20in%20relational%20learning.%0AOur%20results%20show%20that%20graph-level%20architectures%20applied%20to%20hypergraph%0Aexpansions%20often%20outperform%20hypergraph-%20level%20ones%2C%20even%20on%20inputs%20that%20are%0Anaturally%20parametrized%20as%20hypergraphs.%20As%20an%20alternative%20approach%20for%0Aleveraging%20higher-order%20information%2C%20we%20propose%20hypergraph-level%20encodings%0Abased%20on%20classical%20hypergraph%20characteristics.%20While%20these%20encodings%20do%20not%0Asignificantly%20improve%20hypergraph%20architectures%2C%20they%20yield%20substantial%0Aperformance%20gains%20when%20combined%20with%20graph-level%20models.%20Our%20theoretical%0Aanalysis%20shows%20that%20hypergraph-level%20encodings%20provably%20increase%20the%0Arepresentational%20power%20of%20message-passing%20graph%20neural%20networks%20beyond%20that%20of%0Atheir%20graph-level%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09570v1&entry.124074799=Read"},
{"title": "DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References", "author": "Xueyi Liu and Jianibieke Adalibieke and Qianwei Han and Yuzhe Qin and Li Yi", "abstract": "  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n", "link": "http://arxiv.org/abs/2502.09614v1", "date": "2025-02-13", "relevancy": 2.4318, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6242}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6082}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexTrack%3A%20Towards%20Generalizable%20Neural%20Tracking%20Control%20for%20Dexterous%0A%20%20Manipulation%20from%20Human%20References&body=Title%3A%20DexTrack%3A%20Towards%20Generalizable%20Neural%20Tracking%20Control%20for%20Dexterous%0A%20%20Manipulation%20from%20Human%20References%0AAuthor%3A%20Xueyi%20Liu%20and%20Jianibieke%20Adalibieke%20and%20Qianwei%20Han%20and%20Yuzhe%20Qin%20and%20Li%20Yi%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20developing%20a%20generalizable%20neural%20tracking%0Acontroller%20for%20dexterous%20manipulation%20from%20human%20references.%20This%20controller%0Aaims%20to%20manage%20a%20dexterous%20robot%20hand%20to%20manipulate%20diverse%20objects%20for%20various%0Apurposes%20defined%20by%20kinematic%20human-object%20interactions.%20Developing%20such%20a%0Acontroller%20is%20complicated%20by%20the%20intricate%20contact%20dynamics%20of%20dexterous%0Amanipulation%20and%20the%20need%20for%20adaptivity%2C%20generalizability%2C%20and%20robustness.%0ACurrent%20reinforcement%20learning%20and%20trajectory%20optimization%20methods%20often%20fall%0Ashort%20due%20to%20their%20dependence%20on%20task-specific%20rewards%20or%20precise%20system%0Amodels.%20We%20introduce%20an%20approach%20that%20curates%20large-scale%20successful%20robot%0Atracking%20demonstrations%2C%20comprising%20pairs%20of%20human%20references%20and%20robot%0Aactions%2C%20to%20train%20a%20neural%20controller.%20Utilizing%20a%20data%20flywheel%2C%20we%0Aiteratively%20enhance%20the%20controller%27s%20performance%2C%20as%20well%20as%20the%20number%20and%0Aquality%20of%20successful%20tracking%20demonstrations.%20We%20exploit%20available%20tracking%0Ademonstrations%20and%20carefully%20integrate%20reinforcement%20learning%20and%20imitation%0Alearning%20to%20boost%20the%20controller%27s%20performance%20in%20dynamic%20environments.%20At%20the%0Asame%20time%2C%20to%20obtain%20high-quality%20tracking%20demonstrations%2C%20we%20individually%0Aoptimize%20per-trajectory%20tracking%20by%20leveraging%20the%20learned%20tracking%20controller%0Ain%20a%20homotopy%20optimization%20method.%20The%20homotopy%20optimization%2C%20mimicking%0Achain-of-thought%2C%20aids%20in%20solving%20challenging%20trajectory%20tracking%20problems%20to%0Aincrease%20demonstration%20diversity.%20We%20showcase%20our%20success%20by%20training%20a%0Ageneralizable%20neural%20controller%20and%20evaluating%20it%20in%20both%20simulation%20and%20real%0Aworld.%20Our%20method%20achieves%20over%20a%2010%25%20improvement%20in%20success%20rates%20compared%20to%0Aleading%20baselines.%20The%20project%20website%20with%20animated%20results%20is%20available%20at%0Ahttps%3A//meowuu7.github.io/DexTrack/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexTrack%253A%2520Towards%2520Generalizable%2520Neural%2520Tracking%2520Control%2520for%2520Dexterous%250A%2520%2520Manipulation%2520from%2520Human%2520References%26entry.906535625%3DXueyi%2520Liu%2520and%2520Jianibieke%2520Adalibieke%2520and%2520Qianwei%2520Han%2520and%2520Yuzhe%2520Qin%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenge%2520of%2520developing%2520a%2520generalizable%2520neural%2520tracking%250Acontroller%2520for%2520dexterous%2520manipulation%2520from%2520human%2520references.%2520This%2520controller%250Aaims%2520to%2520manage%2520a%2520dexterous%2520robot%2520hand%2520to%2520manipulate%2520diverse%2520objects%2520for%2520various%250Apurposes%2520defined%2520by%2520kinematic%2520human-object%2520interactions.%2520Developing%2520such%2520a%250Acontroller%2520is%2520complicated%2520by%2520the%2520intricate%2520contact%2520dynamics%2520of%2520dexterous%250Amanipulation%2520and%2520the%2520need%2520for%2520adaptivity%252C%2520generalizability%252C%2520and%2520robustness.%250ACurrent%2520reinforcement%2520learning%2520and%2520trajectory%2520optimization%2520methods%2520often%2520fall%250Ashort%2520due%2520to%2520their%2520dependence%2520on%2520task-specific%2520rewards%2520or%2520precise%2520system%250Amodels.%2520We%2520introduce%2520an%2520approach%2520that%2520curates%2520large-scale%2520successful%2520robot%250Atracking%2520demonstrations%252C%2520comprising%2520pairs%2520of%2520human%2520references%2520and%2520robot%250Aactions%252C%2520to%2520train%2520a%2520neural%2520controller.%2520Utilizing%2520a%2520data%2520flywheel%252C%2520we%250Aiteratively%2520enhance%2520the%2520controller%2527s%2520performance%252C%2520as%2520well%2520as%2520the%2520number%2520and%250Aquality%2520of%2520successful%2520tracking%2520demonstrations.%2520We%2520exploit%2520available%2520tracking%250Ademonstrations%2520and%2520carefully%2520integrate%2520reinforcement%2520learning%2520and%2520imitation%250Alearning%2520to%2520boost%2520the%2520controller%2527s%2520performance%2520in%2520dynamic%2520environments.%2520At%2520the%250Asame%2520time%252C%2520to%2520obtain%2520high-quality%2520tracking%2520demonstrations%252C%2520we%2520individually%250Aoptimize%2520per-trajectory%2520tracking%2520by%2520leveraging%2520the%2520learned%2520tracking%2520controller%250Ain%2520a%2520homotopy%2520optimization%2520method.%2520The%2520homotopy%2520optimization%252C%2520mimicking%250Achain-of-thought%252C%2520aids%2520in%2520solving%2520challenging%2520trajectory%2520tracking%2520problems%2520to%250Aincrease%2520demonstration%2520diversity.%2520We%2520showcase%2520our%2520success%2520by%2520training%2520a%250Ageneralizable%2520neural%2520controller%2520and%2520evaluating%2520it%2520in%2520both%2520simulation%2520and%2520real%250Aworld.%2520Our%2520method%2520achieves%2520over%2520a%252010%2525%2520improvement%2520in%2520success%2520rates%2520compared%2520to%250Aleading%2520baselines.%2520The%2520project%2520website%2520with%2520animated%2520results%2520is%2520available%2520at%250Ahttps%253A//meowuu7.github.io/DexTrack/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexTrack%3A%20Towards%20Generalizable%20Neural%20Tracking%20Control%20for%20Dexterous%0A%20%20Manipulation%20from%20Human%20References&entry.906535625=Xueyi%20Liu%20and%20Jianibieke%20Adalibieke%20and%20Qianwei%20Han%20and%20Yuzhe%20Qin%20and%20Li%20Yi&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20developing%20a%20generalizable%20neural%20tracking%0Acontroller%20for%20dexterous%20manipulation%20from%20human%20references.%20This%20controller%0Aaims%20to%20manage%20a%20dexterous%20robot%20hand%20to%20manipulate%20diverse%20objects%20for%20various%0Apurposes%20defined%20by%20kinematic%20human-object%20interactions.%20Developing%20such%20a%0Acontroller%20is%20complicated%20by%20the%20intricate%20contact%20dynamics%20of%20dexterous%0Amanipulation%20and%20the%20need%20for%20adaptivity%2C%20generalizability%2C%20and%20robustness.%0ACurrent%20reinforcement%20learning%20and%20trajectory%20optimization%20methods%20often%20fall%0Ashort%20due%20to%20their%20dependence%20on%20task-specific%20rewards%20or%20precise%20system%0Amodels.%20We%20introduce%20an%20approach%20that%20curates%20large-scale%20successful%20robot%0Atracking%20demonstrations%2C%20comprising%20pairs%20of%20human%20references%20and%20robot%0Aactions%2C%20to%20train%20a%20neural%20controller.%20Utilizing%20a%20data%20flywheel%2C%20we%0Aiteratively%20enhance%20the%20controller%27s%20performance%2C%20as%20well%20as%20the%20number%20and%0Aquality%20of%20successful%20tracking%20demonstrations.%20We%20exploit%20available%20tracking%0Ademonstrations%20and%20carefully%20integrate%20reinforcement%20learning%20and%20imitation%0Alearning%20to%20boost%20the%20controller%27s%20performance%20in%20dynamic%20environments.%20At%20the%0Asame%20time%2C%20to%20obtain%20high-quality%20tracking%20demonstrations%2C%20we%20individually%0Aoptimize%20per-trajectory%20tracking%20by%20leveraging%20the%20learned%20tracking%20controller%0Ain%20a%20homotopy%20optimization%20method.%20The%20homotopy%20optimization%2C%20mimicking%0Achain-of-thought%2C%20aids%20in%20solving%20challenging%20trajectory%20tracking%20problems%20to%0Aincrease%20demonstration%20diversity.%20We%20showcase%20our%20success%20by%20training%20a%0Ageneralizable%20neural%20controller%20and%20evaluating%20it%20in%20both%20simulation%20and%20real%0Aworld.%20Our%20method%20achieves%20over%20a%2010%25%20improvement%20in%20success%20rates%20compared%20to%0Aleading%20baselines.%20The%20project%20website%20with%20animated%20results%20is%20available%20at%0Ahttps%3A//meowuu7.github.io/DexTrack/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09614v1&entry.124074799=Read"},
{"title": "Cracking the Code: Enhancing Development finance understanding with\n  artificial intelligence", "author": "Pierre Beaucoral", "abstract": "  Analyzing development projects is crucial for understanding donors aid\nstrategies, recipients priorities, and to assess development finance capacity\nto adress development issues by on-the-ground actions. In this area, the\nOrganisation for Economic Co-operation and Developments (OECD) Creditor\nReporting System (CRS) dataset is a reference data source. This dataset\nprovides a vast collection of project narratives from various sectors\n(approximately 5 million projects). While the OECD CRS provides a rich source\nof information on development strategies, it falls short in informing project\npurposes due to its reporting process based on donors self-declared main\nobjectives and pre-defined industrial sectors. This research employs a novel\napproach that combines Machine Learning (ML) techniques, specifically Natural\nLanguage Processing (NLP), an innovative Python topic modeling technique called\nBERTopic, to categorise (cluster) and label development projects based on their\nnarrative descriptions. By revealing existing yet hidden topics of development\nfinance, this application of artificial intelligence enables a better\nunderstanding of donor priorities and overall development funding and provides\nmethods to analyse public and private projects narratives.\n", "link": "http://arxiv.org/abs/2502.09495v1", "date": "2025-02-13", "relevancy": 2.4302, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cracking%20the%20Code%3A%20Enhancing%20Development%20finance%20understanding%20with%0A%20%20artificial%20intelligence&body=Title%3A%20Cracking%20the%20Code%3A%20Enhancing%20Development%20finance%20understanding%20with%0A%20%20artificial%20intelligence%0AAuthor%3A%20Pierre%20Beaucoral%0AAbstract%3A%20%20%20Analyzing%20development%20projects%20is%20crucial%20for%20understanding%20donors%20aid%0Astrategies%2C%20recipients%20priorities%2C%20and%20to%20assess%20development%20finance%20capacity%0Ato%20adress%20development%20issues%20by%20on-the-ground%20actions.%20In%20this%20area%2C%20the%0AOrganisation%20for%20Economic%20Co-operation%20and%20Developments%20%28OECD%29%20Creditor%0AReporting%20System%20%28CRS%29%20dataset%20is%20a%20reference%20data%20source.%20This%20dataset%0Aprovides%20a%20vast%20collection%20of%20project%20narratives%20from%20various%20sectors%0A%28approximately%205%20million%20projects%29.%20While%20the%20OECD%20CRS%20provides%20a%20rich%20source%0Aof%20information%20on%20development%20strategies%2C%20it%20falls%20short%20in%20informing%20project%0Apurposes%20due%20to%20its%20reporting%20process%20based%20on%20donors%20self-declared%20main%0Aobjectives%20and%20pre-defined%20industrial%20sectors.%20This%20research%20employs%20a%20novel%0Aapproach%20that%20combines%20Machine%20Learning%20%28ML%29%20techniques%2C%20specifically%20Natural%0ALanguage%20Processing%20%28NLP%29%2C%20an%20innovative%20Python%20topic%20modeling%20technique%20called%0ABERTopic%2C%20to%20categorise%20%28cluster%29%20and%20label%20development%20projects%20based%20on%20their%0Anarrative%20descriptions.%20By%20revealing%20existing%20yet%20hidden%20topics%20of%20development%0Afinance%2C%20this%20application%20of%20artificial%20intelligence%20enables%20a%20better%0Aunderstanding%20of%20donor%20priorities%20and%20overall%20development%20funding%20and%20provides%0Amethods%20to%20analyse%20public%20and%20private%20projects%20narratives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCracking%2520the%2520Code%253A%2520Enhancing%2520Development%2520finance%2520understanding%2520with%250A%2520%2520artificial%2520intelligence%26entry.906535625%3DPierre%2520Beaucoral%26entry.1292438233%3D%2520%2520Analyzing%2520development%2520projects%2520is%2520crucial%2520for%2520understanding%2520donors%2520aid%250Astrategies%252C%2520recipients%2520priorities%252C%2520and%2520to%2520assess%2520development%2520finance%2520capacity%250Ato%2520adress%2520development%2520issues%2520by%2520on-the-ground%2520actions.%2520In%2520this%2520area%252C%2520the%250AOrganisation%2520for%2520Economic%2520Co-operation%2520and%2520Developments%2520%2528OECD%2529%2520Creditor%250AReporting%2520System%2520%2528CRS%2529%2520dataset%2520is%2520a%2520reference%2520data%2520source.%2520This%2520dataset%250Aprovides%2520a%2520vast%2520collection%2520of%2520project%2520narratives%2520from%2520various%2520sectors%250A%2528approximately%25205%2520million%2520projects%2529.%2520While%2520the%2520OECD%2520CRS%2520provides%2520a%2520rich%2520source%250Aof%2520information%2520on%2520development%2520strategies%252C%2520it%2520falls%2520short%2520in%2520informing%2520project%250Apurposes%2520due%2520to%2520its%2520reporting%2520process%2520based%2520on%2520donors%2520self-declared%2520main%250Aobjectives%2520and%2520pre-defined%2520industrial%2520sectors.%2520This%2520research%2520employs%2520a%2520novel%250Aapproach%2520that%2520combines%2520Machine%2520Learning%2520%2528ML%2529%2520techniques%252C%2520specifically%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%252C%2520an%2520innovative%2520Python%2520topic%2520modeling%2520technique%2520called%250ABERTopic%252C%2520to%2520categorise%2520%2528cluster%2529%2520and%2520label%2520development%2520projects%2520based%2520on%2520their%250Anarrative%2520descriptions.%2520By%2520revealing%2520existing%2520yet%2520hidden%2520topics%2520of%2520development%250Afinance%252C%2520this%2520application%2520of%2520artificial%2520intelligence%2520enables%2520a%2520better%250Aunderstanding%2520of%2520donor%2520priorities%2520and%2520overall%2520development%2520funding%2520and%2520provides%250Amethods%2520to%2520analyse%2520public%2520and%2520private%2520projects%2520narratives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cracking%20the%20Code%3A%20Enhancing%20Development%20finance%20understanding%20with%0A%20%20artificial%20intelligence&entry.906535625=Pierre%20Beaucoral&entry.1292438233=%20%20Analyzing%20development%20projects%20is%20crucial%20for%20understanding%20donors%20aid%0Astrategies%2C%20recipients%20priorities%2C%20and%20to%20assess%20development%20finance%20capacity%0Ato%20adress%20development%20issues%20by%20on-the-ground%20actions.%20In%20this%20area%2C%20the%0AOrganisation%20for%20Economic%20Co-operation%20and%20Developments%20%28OECD%29%20Creditor%0AReporting%20System%20%28CRS%29%20dataset%20is%20a%20reference%20data%20source.%20This%20dataset%0Aprovides%20a%20vast%20collection%20of%20project%20narratives%20from%20various%20sectors%0A%28approximately%205%20million%20projects%29.%20While%20the%20OECD%20CRS%20provides%20a%20rich%20source%0Aof%20information%20on%20development%20strategies%2C%20it%20falls%20short%20in%20informing%20project%0Apurposes%20due%20to%20its%20reporting%20process%20based%20on%20donors%20self-declared%20main%0Aobjectives%20and%20pre-defined%20industrial%20sectors.%20This%20research%20employs%20a%20novel%0Aapproach%20that%20combines%20Machine%20Learning%20%28ML%29%20techniques%2C%20specifically%20Natural%0ALanguage%20Processing%20%28NLP%29%2C%20an%20innovative%20Python%20topic%20modeling%20technique%20called%0ABERTopic%2C%20to%20categorise%20%28cluster%29%20and%20label%20development%20projects%20based%20on%20their%0Anarrative%20descriptions.%20By%20revealing%20existing%20yet%20hidden%20topics%20of%20development%0Afinance%2C%20this%20application%20of%20artificial%20intelligence%20enables%20a%20better%0Aunderstanding%20of%20donor%20priorities%20and%20overall%20development%20funding%20and%20provides%0Amethods%20to%20analyse%20public%20and%20private%20projects%20narratives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09495v1&entry.124074799=Read"},
{"title": "Redistribute Ensemble Training for Mitigating Memorization in Diffusion\n  Models", "author": "Xiaoliu Guan and Yu Wu and Huayang Huang and Xiao Liu and Jiaxu Miao and Yi Yang", "abstract": "  Diffusion models, known for their tremendous ability to generate high-quality\nsamples, have recently raised concerns due to their data memorization behavior,\nwhich poses privacy risks. Recent methods for memory mitigation have primarily\naddressed the issue within the context of the text modality in cross-modal\ngeneration tasks, restricting their applicability to specific conditions. In\nthis paper, we propose a novel method for diffusion models from the perspective\nof visual modality, which is more generic and fundamental for mitigating\nmemorization. Directly exposing visual data to the model increases memorization\nrisk, so we design a framework where models learn through proxy model\nparameters instead. Specially, the training dataset is divided into multiple\nshards, with each shard training a proxy model, then aggregated to form the\nfinal model. Additionally, practical analysis of training losses illustrates\nthat the losses for easily memorable images tend to be obviously lower. Thus,\nwe skip the samples with abnormally low loss values from the current mini-batch\nto avoid memorizing. However, balancing the need to skip memorization-prone\nsamples while maintaining sufficient training data for high-quality image\ngeneration presents a key challenge. Thus, we propose IET-AGC+, which\nredistributes highly memorizable samples between shards, to mitigate these\nsamples from over-skipping. Furthermore, we dynamically augment samples based\non their loss values to further reduce memorization. Extensive experiments and\nanalysis on four datasets show that our method successfully reduces memory\ncapacity while maintaining performance. Moreover, we fine-tune the pre-trained\ndiffusion models, e.g., Stable Diffusion, and decrease the memorization score\nby 46.7\\%, demonstrating the effectiveness of our method. Code is available in:\nhttps://github.com/liuxiao-guan/IET_AGC.\n", "link": "http://arxiv.org/abs/2502.09434v1", "date": "2025-02-13", "relevancy": 2.4264, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6275}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6122}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redistribute%20Ensemble%20Training%20for%20Mitigating%20Memorization%20in%20Diffusion%0A%20%20Models&body=Title%3A%20Redistribute%20Ensemble%20Training%20for%20Mitigating%20Memorization%20in%20Diffusion%0A%20%20Models%0AAuthor%3A%20Xiaoliu%20Guan%20and%20Yu%20Wu%20and%20Huayang%20Huang%20and%20Xiao%20Liu%20and%20Jiaxu%20Miao%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Diffusion%20models%2C%20known%20for%20their%20tremendous%20ability%20to%20generate%20high-quality%0Asamples%2C%20have%20recently%20raised%20concerns%20due%20to%20their%20data%20memorization%20behavior%2C%0Awhich%20poses%20privacy%20risks.%20Recent%20methods%20for%20memory%20mitigation%20have%20primarily%0Aaddressed%20the%20issue%20within%20the%20context%20of%20the%20text%20modality%20in%20cross-modal%0Ageneration%20tasks%2C%20restricting%20their%20applicability%20to%20specific%20conditions.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20method%20for%20diffusion%20models%20from%20the%20perspective%0Aof%20visual%20modality%2C%20which%20is%20more%20generic%20and%20fundamental%20for%20mitigating%0Amemorization.%20Directly%20exposing%20visual%20data%20to%20the%20model%20increases%20memorization%0Arisk%2C%20so%20we%20design%20a%20framework%20where%20models%20learn%20through%20proxy%20model%0Aparameters%20instead.%20Specially%2C%20the%20training%20dataset%20is%20divided%20into%20multiple%0Ashards%2C%20with%20each%20shard%20training%20a%20proxy%20model%2C%20then%20aggregated%20to%20form%20the%0Afinal%20model.%20Additionally%2C%20practical%20analysis%20of%20training%20losses%20illustrates%0Athat%20the%20losses%20for%20easily%20memorable%20images%20tend%20to%20be%20obviously%20lower.%20Thus%2C%0Awe%20skip%20the%20samples%20with%20abnormally%20low%20loss%20values%20from%20the%20current%20mini-batch%0Ato%20avoid%20memorizing.%20However%2C%20balancing%20the%20need%20to%20skip%20memorization-prone%0Asamples%20while%20maintaining%20sufficient%20training%20data%20for%20high-quality%20image%0Ageneration%20presents%20a%20key%20challenge.%20Thus%2C%20we%20propose%20IET-AGC%2B%2C%20which%0Aredistributes%20highly%20memorizable%20samples%20between%20shards%2C%20to%20mitigate%20these%0Asamples%20from%20over-skipping.%20Furthermore%2C%20we%20dynamically%20augment%20samples%20based%0Aon%20their%20loss%20values%20to%20further%20reduce%20memorization.%20Extensive%20experiments%20and%0Aanalysis%20on%20four%20datasets%20show%20that%20our%20method%20successfully%20reduces%20memory%0Acapacity%20while%20maintaining%20performance.%20Moreover%2C%20we%20fine-tune%20the%20pre-trained%0Adiffusion%20models%2C%20e.g.%2C%20Stable%20Diffusion%2C%20and%20decrease%20the%20memorization%20score%0Aby%2046.7%5C%25%2C%20demonstrating%20the%20effectiveness%20of%20our%20method.%20Code%20is%20available%20in%3A%0Ahttps%3A//github.com/liuxiao-guan/IET_AGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedistribute%2520Ensemble%2520Training%2520for%2520Mitigating%2520Memorization%2520in%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DXiaoliu%2520Guan%2520and%2520Yu%2520Wu%2520and%2520Huayang%2520Huang%2520and%2520Xiao%2520Liu%2520and%2520Jiaxu%2520Miao%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520models%252C%2520known%2520for%2520their%2520tremendous%2520ability%2520to%2520generate%2520high-quality%250Asamples%252C%2520have%2520recently%2520raised%2520concerns%2520due%2520to%2520their%2520data%2520memorization%2520behavior%252C%250Awhich%2520poses%2520privacy%2520risks.%2520Recent%2520methods%2520for%2520memory%2520mitigation%2520have%2520primarily%250Aaddressed%2520the%2520issue%2520within%2520the%2520context%2520of%2520the%2520text%2520modality%2520in%2520cross-modal%250Ageneration%2520tasks%252C%2520restricting%2520their%2520applicability%2520to%2520specific%2520conditions.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520diffusion%2520models%2520from%2520the%2520perspective%250Aof%2520visual%2520modality%252C%2520which%2520is%2520more%2520generic%2520and%2520fundamental%2520for%2520mitigating%250Amemorization.%2520Directly%2520exposing%2520visual%2520data%2520to%2520the%2520model%2520increases%2520memorization%250Arisk%252C%2520so%2520we%2520design%2520a%2520framework%2520where%2520models%2520learn%2520through%2520proxy%2520model%250Aparameters%2520instead.%2520Specially%252C%2520the%2520training%2520dataset%2520is%2520divided%2520into%2520multiple%250Ashards%252C%2520with%2520each%2520shard%2520training%2520a%2520proxy%2520model%252C%2520then%2520aggregated%2520to%2520form%2520the%250Afinal%2520model.%2520Additionally%252C%2520practical%2520analysis%2520of%2520training%2520losses%2520illustrates%250Athat%2520the%2520losses%2520for%2520easily%2520memorable%2520images%2520tend%2520to%2520be%2520obviously%2520lower.%2520Thus%252C%250Awe%2520skip%2520the%2520samples%2520with%2520abnormally%2520low%2520loss%2520values%2520from%2520the%2520current%2520mini-batch%250Ato%2520avoid%2520memorizing.%2520However%252C%2520balancing%2520the%2520need%2520to%2520skip%2520memorization-prone%250Asamples%2520while%2520maintaining%2520sufficient%2520training%2520data%2520for%2520high-quality%2520image%250Ageneration%2520presents%2520a%2520key%2520challenge.%2520Thus%252C%2520we%2520propose%2520IET-AGC%252B%252C%2520which%250Aredistributes%2520highly%2520memorizable%2520samples%2520between%2520shards%252C%2520to%2520mitigate%2520these%250Asamples%2520from%2520over-skipping.%2520Furthermore%252C%2520we%2520dynamically%2520augment%2520samples%2520based%250Aon%2520their%2520loss%2520values%2520to%2520further%2520reduce%2520memorization.%2520Extensive%2520experiments%2520and%250Aanalysis%2520on%2520four%2520datasets%2520show%2520that%2520our%2520method%2520successfully%2520reduces%2520memory%250Acapacity%2520while%2520maintaining%2520performance.%2520Moreover%252C%2520we%2520fine-tune%2520the%2520pre-trained%250Adiffusion%2520models%252C%2520e.g.%252C%2520Stable%2520Diffusion%252C%2520and%2520decrease%2520the%2520memorization%2520score%250Aby%252046.7%255C%2525%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520method.%2520Code%2520is%2520available%2520in%253A%250Ahttps%253A//github.com/liuxiao-guan/IET_AGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redistribute%20Ensemble%20Training%20for%20Mitigating%20Memorization%20in%20Diffusion%0A%20%20Models&entry.906535625=Xiaoliu%20Guan%20and%20Yu%20Wu%20and%20Huayang%20Huang%20and%20Xiao%20Liu%20and%20Jiaxu%20Miao%20and%20Yi%20Yang&entry.1292438233=%20%20Diffusion%20models%2C%20known%20for%20their%20tremendous%20ability%20to%20generate%20high-quality%0Asamples%2C%20have%20recently%20raised%20concerns%20due%20to%20their%20data%20memorization%20behavior%2C%0Awhich%20poses%20privacy%20risks.%20Recent%20methods%20for%20memory%20mitigation%20have%20primarily%0Aaddressed%20the%20issue%20within%20the%20context%20of%20the%20text%20modality%20in%20cross-modal%0Ageneration%20tasks%2C%20restricting%20their%20applicability%20to%20specific%20conditions.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20method%20for%20diffusion%20models%20from%20the%20perspective%0Aof%20visual%20modality%2C%20which%20is%20more%20generic%20and%20fundamental%20for%20mitigating%0Amemorization.%20Directly%20exposing%20visual%20data%20to%20the%20model%20increases%20memorization%0Arisk%2C%20so%20we%20design%20a%20framework%20where%20models%20learn%20through%20proxy%20model%0Aparameters%20instead.%20Specially%2C%20the%20training%20dataset%20is%20divided%20into%20multiple%0Ashards%2C%20with%20each%20shard%20training%20a%20proxy%20model%2C%20then%20aggregated%20to%20form%20the%0Afinal%20model.%20Additionally%2C%20practical%20analysis%20of%20training%20losses%20illustrates%0Athat%20the%20losses%20for%20easily%20memorable%20images%20tend%20to%20be%20obviously%20lower.%20Thus%2C%0Awe%20skip%20the%20samples%20with%20abnormally%20low%20loss%20values%20from%20the%20current%20mini-batch%0Ato%20avoid%20memorizing.%20However%2C%20balancing%20the%20need%20to%20skip%20memorization-prone%0Asamples%20while%20maintaining%20sufficient%20training%20data%20for%20high-quality%20image%0Ageneration%20presents%20a%20key%20challenge.%20Thus%2C%20we%20propose%20IET-AGC%2B%2C%20which%0Aredistributes%20highly%20memorizable%20samples%20between%20shards%2C%20to%20mitigate%20these%0Asamples%20from%20over-skipping.%20Furthermore%2C%20we%20dynamically%20augment%20samples%20based%0Aon%20their%20loss%20values%20to%20further%20reduce%20memorization.%20Extensive%20experiments%20and%0Aanalysis%20on%20four%20datasets%20show%20that%20our%20method%20successfully%20reduces%20memory%0Acapacity%20while%20maintaining%20performance.%20Moreover%2C%20we%20fine-tune%20the%20pre-trained%0Adiffusion%20models%2C%20e.g.%2C%20Stable%20Diffusion%2C%20and%20decrease%20the%20memorization%20score%0Aby%2046.7%5C%25%2C%20demonstrating%20the%20effectiveness%20of%20our%20method.%20Code%20is%20available%20in%3A%0Ahttps%3A//github.com/liuxiao-guan/IET_AGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09434v1&entry.124074799=Read"},
{"title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling", "author": "Theodoros Kouzelis and Ioannis Kakogeorgiou and Spyros Gidaris and Nikos Komodakis", "abstract": "  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n", "link": "http://arxiv.org/abs/2502.09509v1", "date": "2025-02-13", "relevancy": 2.4093, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6134}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6011}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling&body=Title%3A%20EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling%0AAuthor%3A%20Theodoros%20Kouzelis%20and%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis%0AAbstract%3A%20%20%20Latent%20generative%20models%20have%20emerged%20as%20a%20leading%20approach%20for%20high-quality%0Aimage%20synthesis.%20These%20models%20rely%20on%20an%20autoencoder%20to%20compress%20images%20into%20a%0Alatent%20space%2C%20followed%20by%20a%20generative%20model%20to%20learn%20the%20latent%20distribution.%0AWe%20identify%20that%20existing%20autoencoders%20lack%20equivariance%20to%20semantic-preserving%0Atransformations%20like%20scaling%20and%20rotation%2C%20resulting%20in%20complex%20latent%20spaces%0Athat%20hinder%20generative%20performance.%20To%20address%20this%2C%20we%20propose%20EQ-VAE%2C%20a%0Asimple%20regularization%20approach%20that%20enforces%20equivariance%20in%20the%20latent%20space%2C%0Areducing%20its%20complexity%20without%20degrading%20reconstruction%20quality.%20By%20finetuning%0Apre-trained%20autoencoders%20with%20EQ-VAE%2C%20we%20enhance%20the%20performance%20of%20several%0Astate-of-the-art%20generative%20models%2C%20including%20DiT%2C%20SiT%2C%20REPA%20and%20MaskGIT%2C%0Aachieving%20a%207%20speedup%20on%20DiT-XL/2%20with%20only%20five%20epochs%20of%20SD-VAE%20fine-tuning.%0AEQ-VAE%20is%20compatible%20with%20both%20continuous%20and%20discrete%20autoencoders%2C%20thus%0Aoffering%20a%20versatile%20enhancement%20for%20a%20wide%20range%20of%20latent%20generative%20models.%0AProject%20page%20and%20code%3A%20https%3A//eq-vae.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEQ-VAE%253A%2520Equivariance%2520Regularized%2520Latent%2520Space%2520for%2520Improved%2520Generative%250A%2520%2520Image%2520Modeling%26entry.906535625%3DTheodoros%2520Kouzelis%2520and%2520Ioannis%2520Kakogeorgiou%2520and%2520Spyros%2520Gidaris%2520and%2520Nikos%2520Komodakis%26entry.1292438233%3D%2520%2520Latent%2520generative%2520models%2520have%2520emerged%2520as%2520a%2520leading%2520approach%2520for%2520high-quality%250Aimage%2520synthesis.%2520These%2520models%2520rely%2520on%2520an%2520autoencoder%2520to%2520compress%2520images%2520into%2520a%250Alatent%2520space%252C%2520followed%2520by%2520a%2520generative%2520model%2520to%2520learn%2520the%2520latent%2520distribution.%250AWe%2520identify%2520that%2520existing%2520autoencoders%2520lack%2520equivariance%2520to%2520semantic-preserving%250Atransformations%2520like%2520scaling%2520and%2520rotation%252C%2520resulting%2520in%2520complex%2520latent%2520spaces%250Athat%2520hinder%2520generative%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520EQ-VAE%252C%2520a%250Asimple%2520regularization%2520approach%2520that%2520enforces%2520equivariance%2520in%2520the%2520latent%2520space%252C%250Areducing%2520its%2520complexity%2520without%2520degrading%2520reconstruction%2520quality.%2520By%2520finetuning%250Apre-trained%2520autoencoders%2520with%2520EQ-VAE%252C%2520we%2520enhance%2520the%2520performance%2520of%2520several%250Astate-of-the-art%2520generative%2520models%252C%2520including%2520DiT%252C%2520SiT%252C%2520REPA%2520and%2520MaskGIT%252C%250Aachieving%2520a%25207%2520speedup%2520on%2520DiT-XL/2%2520with%2520only%2520five%2520epochs%2520of%2520SD-VAE%2520fine-tuning.%250AEQ-VAE%2520is%2520compatible%2520with%2520both%2520continuous%2520and%2520discrete%2520autoencoders%252C%2520thus%250Aoffering%2520a%2520versatile%2520enhancement%2520for%2520a%2520wide%2520range%2520of%2520latent%2520generative%2520models.%250AProject%2520page%2520and%2520code%253A%2520https%253A//eq-vae.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling&entry.906535625=Theodoros%20Kouzelis%20and%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis&entry.1292438233=%20%20Latent%20generative%20models%20have%20emerged%20as%20a%20leading%20approach%20for%20high-quality%0Aimage%20synthesis.%20These%20models%20rely%20on%20an%20autoencoder%20to%20compress%20images%20into%20a%0Alatent%20space%2C%20followed%20by%20a%20generative%20model%20to%20learn%20the%20latent%20distribution.%0AWe%20identify%20that%20existing%20autoencoders%20lack%20equivariance%20to%20semantic-preserving%0Atransformations%20like%20scaling%20and%20rotation%2C%20resulting%20in%20complex%20latent%20spaces%0Athat%20hinder%20generative%20performance.%20To%20address%20this%2C%20we%20propose%20EQ-VAE%2C%20a%0Asimple%20regularization%20approach%20that%20enforces%20equivariance%20in%20the%20latent%20space%2C%0Areducing%20its%20complexity%20without%20degrading%20reconstruction%20quality.%20By%20finetuning%0Apre-trained%20autoencoders%20with%20EQ-VAE%2C%20we%20enhance%20the%20performance%20of%20several%0Astate-of-the-art%20generative%20models%2C%20including%20DiT%2C%20SiT%2C%20REPA%20and%20MaskGIT%2C%0Aachieving%20a%207%20speedup%20on%20DiT-XL/2%20with%20only%20five%20epochs%20of%20SD-VAE%20fine-tuning.%0AEQ-VAE%20is%20compatible%20with%20both%20continuous%20and%20discrete%20autoencoders%2C%20thus%0Aoffering%20a%20versatile%20enhancement%20for%20a%20wide%20range%20of%20latent%20generative%20models.%0AProject%20page%20and%20code%3A%20https%3A//eq-vae.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09509v1&entry.124074799=Read"},
{"title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents", "author": "Rui Yang and Hanyang Chen and Junyu Zhang and Mark Zhao and Cheng Qian and Kangrui Wang and Qineng Wang and Teja Venkat Koripella and Marziyeh Movahedi and Manling Li and Heng Ji and Huan Zhang and Tong Zhang", "abstract": "  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n", "link": "http://arxiv.org/abs/2502.09560v1", "date": "2025-02-13", "relevancy": 2.407, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmbodiedBench%3A%20Comprehensive%20Benchmarking%20Multi-modal%20Large%20Language%0A%20%20Models%20for%20Vision-Driven%20Embodied%20Agents&body=Title%3A%20EmbodiedBench%3A%20Comprehensive%20Benchmarking%20Multi-modal%20Large%20Language%0A%20%20Models%20for%20Vision-Driven%20Embodied%20Agents%0AAuthor%3A%20Rui%20Yang%20and%20Hanyang%20Chen%20and%20Junyu%20Zhang%20and%20Mark%20Zhao%20and%20Cheng%20Qian%20and%20Kangrui%20Wang%20and%20Qineng%20Wang%20and%20Teja%20Venkat%20Koripella%20and%20Marziyeh%20Movahedi%20and%20Manling%20Li%20and%20Heng%20Ji%20and%20Huan%20Zhang%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Leveraging%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20to%20create%20embodied%0Aagents%20offers%20a%20promising%20avenue%20for%20tackling%20real-world%20tasks.%20While%0Alanguage-centric%20embodied%20agents%20have%20garnered%20substantial%20attention%2C%0AMLLM-based%20embodied%20agents%20remain%20underexplored%20due%20to%20the%20lack%20of%0Acomprehensive%20evaluation%20frameworks.%20To%20bridge%20this%20gap%2C%20we%20introduce%0AEmbodiedBench%2C%20an%20extensive%20benchmark%20designed%20to%20evaluate%20vision-driven%0Aembodied%20agents.%20EmbodiedBench%20features%3A%20%281%29%20a%20diverse%20set%20of%201%2C128%20testing%0Atasks%20across%20four%20environments%2C%20ranging%20from%20high-level%20semantic%20tasks%20%28e.g.%2C%0Ahousehold%29%20to%20low-level%20tasks%20involving%20atomic%20actions%20%28e.g.%2C%20navigation%20and%0Amanipulation%29%3B%20and%20%282%29%20six%20meticulously%20curated%20subsets%20evaluating%20essential%0Aagent%20capabilities%20like%20commonsense%20reasoning%2C%20complex%20instruction%0Aunderstanding%2C%20spatial%20awareness%2C%20visual%20perception%2C%20and%20long-term%20planning.%0AThrough%20extensive%20experiments%2C%20we%20evaluated%2013%20leading%20proprietary%20and%0Aopen-source%20MLLMs%20within%20EmbodiedBench.%20Our%20findings%20reveal%20that%3A%20MLLMs%20excel%0Aat%20high-level%20tasks%20but%20struggle%20with%20low-level%20manipulation%2C%20with%20the%20best%0Amodel%2C%20GPT-4o%2C%20scoring%20only%2028.9%25%20on%20average.%20EmbodiedBench%20provides%20a%0Amultifaceted%20standardized%20evaluation%20platform%20that%20not%20only%20highlights%20existing%0Achallenges%20but%20also%20offers%20valuable%20insights%20to%20advance%20MLLM-based%20embodied%0Aagents.%20Our%20code%20is%20available%20at%20https%3A//embodiedbench.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodiedBench%253A%2520Comprehensive%2520Benchmarking%2520Multi-modal%2520Large%2520Language%250A%2520%2520Models%2520for%2520Vision-Driven%2520Embodied%2520Agents%26entry.906535625%3DRui%2520Yang%2520and%2520Hanyang%2520Chen%2520and%2520Junyu%2520Zhang%2520and%2520Mark%2520Zhao%2520and%2520Cheng%2520Qian%2520and%2520Kangrui%2520Wang%2520and%2520Qineng%2520Wang%2520and%2520Teja%2520Venkat%2520Koripella%2520and%2520Marziyeh%2520Movahedi%2520and%2520Manling%2520Li%2520and%2520Heng%2520Ji%2520and%2520Huan%2520Zhang%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Leveraging%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520create%2520embodied%250Aagents%2520offers%2520a%2520promising%2520avenue%2520for%2520tackling%2520real-world%2520tasks.%2520While%250Alanguage-centric%2520embodied%2520agents%2520have%2520garnered%2520substantial%2520attention%252C%250AMLLM-based%2520embodied%2520agents%2520remain%2520underexplored%2520due%2520to%2520the%2520lack%2520of%250Acomprehensive%2520evaluation%2520frameworks.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%250AEmbodiedBench%252C%2520an%2520extensive%2520benchmark%2520designed%2520to%2520evaluate%2520vision-driven%250Aembodied%2520agents.%2520EmbodiedBench%2520features%253A%2520%25281%2529%2520a%2520diverse%2520set%2520of%25201%252C128%2520testing%250Atasks%2520across%2520four%2520environments%252C%2520ranging%2520from%2520high-level%2520semantic%2520tasks%2520%2528e.g.%252C%250Ahousehold%2529%2520to%2520low-level%2520tasks%2520involving%2520atomic%2520actions%2520%2528e.g.%252C%2520navigation%2520and%250Amanipulation%2529%253B%2520and%2520%25282%2529%2520six%2520meticulously%2520curated%2520subsets%2520evaluating%2520essential%250Aagent%2520capabilities%2520like%2520commonsense%2520reasoning%252C%2520complex%2520instruction%250Aunderstanding%252C%2520spatial%2520awareness%252C%2520visual%2520perception%252C%2520and%2520long-term%2520planning.%250AThrough%2520extensive%2520experiments%252C%2520we%2520evaluated%252013%2520leading%2520proprietary%2520and%250Aopen-source%2520MLLMs%2520within%2520EmbodiedBench.%2520Our%2520findings%2520reveal%2520that%253A%2520MLLMs%2520excel%250Aat%2520high-level%2520tasks%2520but%2520struggle%2520with%2520low-level%2520manipulation%252C%2520with%2520the%2520best%250Amodel%252C%2520GPT-4o%252C%2520scoring%2520only%252028.9%2525%2520on%2520average.%2520EmbodiedBench%2520provides%2520a%250Amultifaceted%2520standardized%2520evaluation%2520platform%2520that%2520not%2520only%2520highlights%2520existing%250Achallenges%2520but%2520also%2520offers%2520valuable%2520insights%2520to%2520advance%2520MLLM-based%2520embodied%250Aagents.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//embodiedbench.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmbodiedBench%3A%20Comprehensive%20Benchmarking%20Multi-modal%20Large%20Language%0A%20%20Models%20for%20Vision-Driven%20Embodied%20Agents&entry.906535625=Rui%20Yang%20and%20Hanyang%20Chen%20and%20Junyu%20Zhang%20and%20Mark%20Zhao%20and%20Cheng%20Qian%20and%20Kangrui%20Wang%20and%20Qineng%20Wang%20and%20Teja%20Venkat%20Koripella%20and%20Marziyeh%20Movahedi%20and%20Manling%20Li%20and%20Heng%20Ji%20and%20Huan%20Zhang%20and%20Tong%20Zhang&entry.1292438233=%20%20Leveraging%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20to%20create%20embodied%0Aagents%20offers%20a%20promising%20avenue%20for%20tackling%20real-world%20tasks.%20While%0Alanguage-centric%20embodied%20agents%20have%20garnered%20substantial%20attention%2C%0AMLLM-based%20embodied%20agents%20remain%20underexplored%20due%20to%20the%20lack%20of%0Acomprehensive%20evaluation%20frameworks.%20To%20bridge%20this%20gap%2C%20we%20introduce%0AEmbodiedBench%2C%20an%20extensive%20benchmark%20designed%20to%20evaluate%20vision-driven%0Aembodied%20agents.%20EmbodiedBench%20features%3A%20%281%29%20a%20diverse%20set%20of%201%2C128%20testing%0Atasks%20across%20four%20environments%2C%20ranging%20from%20high-level%20semantic%20tasks%20%28e.g.%2C%0Ahousehold%29%20to%20low-level%20tasks%20involving%20atomic%20actions%20%28e.g.%2C%20navigation%20and%0Amanipulation%29%3B%20and%20%282%29%20six%20meticulously%20curated%20subsets%20evaluating%20essential%0Aagent%20capabilities%20like%20commonsense%20reasoning%2C%20complex%20instruction%0Aunderstanding%2C%20spatial%20awareness%2C%20visual%20perception%2C%20and%20long-term%20planning.%0AThrough%20extensive%20experiments%2C%20we%20evaluated%2013%20leading%20proprietary%20and%0Aopen-source%20MLLMs%20within%20EmbodiedBench.%20Our%20findings%20reveal%20that%3A%20MLLMs%20excel%0Aat%20high-level%20tasks%20but%20struggle%20with%20low-level%20manipulation%2C%20with%20the%20best%0Amodel%2C%20GPT-4o%2C%20scoring%20only%2028.9%25%20on%20average.%20EmbodiedBench%20provides%20a%0Amultifaceted%20standardized%20evaluation%20platform%20that%20not%20only%20highlights%20existing%0Achallenges%20but%20also%20offers%20valuable%20insights%20to%20advance%20MLLM-based%20embodied%0Aagents.%20Our%20code%20is%20available%20at%20https%3A//embodiedbench.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09560v1&entry.124074799=Read"},
{"title": "Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for\n  Remote Sensing Community", "author": "Jiancheng Pan and Yanxing Liu and Yuqian Fu and Muyuan Ma and Jiahao Li and Danda Pani Paudel and Luc Van Gool and Xiaomeng Huang", "abstract": "  Object detection, particularly open-vocabulary object detection, plays a\ncrucial role in Earth sciences, such as environmental monitoring, natural\ndisaster assessment, and land-use planning. However, existing open-vocabulary\ndetectors, primarily trained on natural-world images, struggle to generalize to\nremote sensing images due to a significant data domain gap. Thus, this paper\naims to advance the development of open-vocabulary object detection in remote\nsensing community. To achieve this, we first reformulate the task as Locate\nAnything on Earth (LAE) with the goal of detecting any novel concepts on Earth.\nWe then developed the LAE-Label Engine which collects, auto-annotates, and\nunifies up to 10 remote sensing datasets creating the LAE-1M - the first\nlarge-scale remote sensing object detection dataset with broad category\ncoverage. Using the LAE-1M, we further propose and train the novel LAE-DINO\nModel, the first open-vocabulary foundation object detector for the LAE task,\nfeaturing Dynamic Vocabulary Construction (DVC) and Visual-Guided Text Prompt\nLearning (VisGT) modules. DVC dynamically constructs vocabulary for each\ntraining batch, while VisGT maps visual features to semantic space, enhancing\ntext features. We comprehensively conduct experiments on established remote\nsensing benchmark DIOR, DOTAv2.0, as well as our newly introduced 80-class\nLAE-80C benchmark. Results demonstrate the advantages of the LAE-1M dataset and\nthe effectiveness of the LAE-DINO method.\n", "link": "http://arxiv.org/abs/2408.09110v2", "date": "2025-02-13", "relevancy": 2.3311, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5832}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locate%20Anything%20on%20Earth%3A%20Advancing%20Open-Vocabulary%20Object%20Detection%20for%0A%20%20Remote%20Sensing%20Community&body=Title%3A%20Locate%20Anything%20on%20Earth%3A%20Advancing%20Open-Vocabulary%20Object%20Detection%20for%0A%20%20Remote%20Sensing%20Community%0AAuthor%3A%20Jiancheng%20Pan%20and%20Yanxing%20Liu%20and%20Yuqian%20Fu%20and%20Muyuan%20Ma%20and%20Jiahao%20Li%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%20and%20Xiaomeng%20Huang%0AAbstract%3A%20%20%20Object%20detection%2C%20particularly%20open-vocabulary%20object%20detection%2C%20plays%20a%0Acrucial%20role%20in%20Earth%20sciences%2C%20such%20as%20environmental%20monitoring%2C%20natural%0Adisaster%20assessment%2C%20and%20land-use%20planning.%20However%2C%20existing%20open-vocabulary%0Adetectors%2C%20primarily%20trained%20on%20natural-world%20images%2C%20struggle%20to%20generalize%20to%0Aremote%20sensing%20images%20due%20to%20a%20significant%20data%20domain%20gap.%20Thus%2C%20this%20paper%0Aaims%20to%20advance%20the%20development%20of%20open-vocabulary%20object%20detection%20in%20remote%0Asensing%20community.%20To%20achieve%20this%2C%20we%20first%20reformulate%20the%20task%20as%20Locate%0AAnything%20on%20Earth%20%28LAE%29%20with%20the%20goal%20of%20detecting%20any%20novel%20concepts%20on%20Earth.%0AWe%20then%20developed%20the%20LAE-Label%20Engine%20which%20collects%2C%20auto-annotates%2C%20and%0Aunifies%20up%20to%2010%20remote%20sensing%20datasets%20creating%20the%20LAE-1M%20-%20the%20first%0Alarge-scale%20remote%20sensing%20object%20detection%20dataset%20with%20broad%20category%0Acoverage.%20Using%20the%20LAE-1M%2C%20we%20further%20propose%20and%20train%20the%20novel%20LAE-DINO%0AModel%2C%20the%20first%20open-vocabulary%20foundation%20object%20detector%20for%20the%20LAE%20task%2C%0Afeaturing%20Dynamic%20Vocabulary%20Construction%20%28DVC%29%20and%20Visual-Guided%20Text%20Prompt%0ALearning%20%28VisGT%29%20modules.%20DVC%20dynamically%20constructs%20vocabulary%20for%20each%0Atraining%20batch%2C%20while%20VisGT%20maps%20visual%20features%20to%20semantic%20space%2C%20enhancing%0Atext%20features.%20We%20comprehensively%20conduct%20experiments%20on%20established%20remote%0Asensing%20benchmark%20DIOR%2C%20DOTAv2.0%2C%20as%20well%20as%20our%20newly%20introduced%2080-class%0ALAE-80C%20benchmark.%20Results%20demonstrate%20the%20advantages%20of%20the%20LAE-1M%20dataset%20and%0Athe%20effectiveness%20of%20the%20LAE-DINO%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09110v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocate%2520Anything%2520on%2520Earth%253A%2520Advancing%2520Open-Vocabulary%2520Object%2520Detection%2520for%250A%2520%2520Remote%2520Sensing%2520Community%26entry.906535625%3DJiancheng%2520Pan%2520and%2520Yanxing%2520Liu%2520and%2520Yuqian%2520Fu%2520and%2520Muyuan%2520Ma%2520and%2520Jiahao%2520Li%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%2520and%2520Xiaomeng%2520Huang%26entry.1292438233%3D%2520%2520Object%2520detection%252C%2520particularly%2520open-vocabulary%2520object%2520detection%252C%2520plays%2520a%250Acrucial%2520role%2520in%2520Earth%2520sciences%252C%2520such%2520as%2520environmental%2520monitoring%252C%2520natural%250Adisaster%2520assessment%252C%2520and%2520land-use%2520planning.%2520However%252C%2520existing%2520open-vocabulary%250Adetectors%252C%2520primarily%2520trained%2520on%2520natural-world%2520images%252C%2520struggle%2520to%2520generalize%2520to%250Aremote%2520sensing%2520images%2520due%2520to%2520a%2520significant%2520data%2520domain%2520gap.%2520Thus%252C%2520this%2520paper%250Aaims%2520to%2520advance%2520the%2520development%2520of%2520open-vocabulary%2520object%2520detection%2520in%2520remote%250Asensing%2520community.%2520To%2520achieve%2520this%252C%2520we%2520first%2520reformulate%2520the%2520task%2520as%2520Locate%250AAnything%2520on%2520Earth%2520%2528LAE%2529%2520with%2520the%2520goal%2520of%2520detecting%2520any%2520novel%2520concepts%2520on%2520Earth.%250AWe%2520then%2520developed%2520the%2520LAE-Label%2520Engine%2520which%2520collects%252C%2520auto-annotates%252C%2520and%250Aunifies%2520up%2520to%252010%2520remote%2520sensing%2520datasets%2520creating%2520the%2520LAE-1M%2520-%2520the%2520first%250Alarge-scale%2520remote%2520sensing%2520object%2520detection%2520dataset%2520with%2520broad%2520category%250Acoverage.%2520Using%2520the%2520LAE-1M%252C%2520we%2520further%2520propose%2520and%2520train%2520the%2520novel%2520LAE-DINO%250AModel%252C%2520the%2520first%2520open-vocabulary%2520foundation%2520object%2520detector%2520for%2520the%2520LAE%2520task%252C%250Afeaturing%2520Dynamic%2520Vocabulary%2520Construction%2520%2528DVC%2529%2520and%2520Visual-Guided%2520Text%2520Prompt%250ALearning%2520%2528VisGT%2529%2520modules.%2520DVC%2520dynamically%2520constructs%2520vocabulary%2520for%2520each%250Atraining%2520batch%252C%2520while%2520VisGT%2520maps%2520visual%2520features%2520to%2520semantic%2520space%252C%2520enhancing%250Atext%2520features.%2520We%2520comprehensively%2520conduct%2520experiments%2520on%2520established%2520remote%250Asensing%2520benchmark%2520DIOR%252C%2520DOTAv2.0%252C%2520as%2520well%2520as%2520our%2520newly%2520introduced%252080-class%250ALAE-80C%2520benchmark.%2520Results%2520demonstrate%2520the%2520advantages%2520of%2520the%2520LAE-1M%2520dataset%2520and%250Athe%2520effectiveness%2520of%2520the%2520LAE-DINO%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09110v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locate%20Anything%20on%20Earth%3A%20Advancing%20Open-Vocabulary%20Object%20Detection%20for%0A%20%20Remote%20Sensing%20Community&entry.906535625=Jiancheng%20Pan%20and%20Yanxing%20Liu%20and%20Yuqian%20Fu%20and%20Muyuan%20Ma%20and%20Jiahao%20Li%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%20and%20Xiaomeng%20Huang&entry.1292438233=%20%20Object%20detection%2C%20particularly%20open-vocabulary%20object%20detection%2C%20plays%20a%0Acrucial%20role%20in%20Earth%20sciences%2C%20such%20as%20environmental%20monitoring%2C%20natural%0Adisaster%20assessment%2C%20and%20land-use%20planning.%20However%2C%20existing%20open-vocabulary%0Adetectors%2C%20primarily%20trained%20on%20natural-world%20images%2C%20struggle%20to%20generalize%20to%0Aremote%20sensing%20images%20due%20to%20a%20significant%20data%20domain%20gap.%20Thus%2C%20this%20paper%0Aaims%20to%20advance%20the%20development%20of%20open-vocabulary%20object%20detection%20in%20remote%0Asensing%20community.%20To%20achieve%20this%2C%20we%20first%20reformulate%20the%20task%20as%20Locate%0AAnything%20on%20Earth%20%28LAE%29%20with%20the%20goal%20of%20detecting%20any%20novel%20concepts%20on%20Earth.%0AWe%20then%20developed%20the%20LAE-Label%20Engine%20which%20collects%2C%20auto-annotates%2C%20and%0Aunifies%20up%20to%2010%20remote%20sensing%20datasets%20creating%20the%20LAE-1M%20-%20the%20first%0Alarge-scale%20remote%20sensing%20object%20detection%20dataset%20with%20broad%20category%0Acoverage.%20Using%20the%20LAE-1M%2C%20we%20further%20propose%20and%20train%20the%20novel%20LAE-DINO%0AModel%2C%20the%20first%20open-vocabulary%20foundation%20object%20detector%20for%20the%20LAE%20task%2C%0Afeaturing%20Dynamic%20Vocabulary%20Construction%20%28DVC%29%20and%20Visual-Guided%20Text%20Prompt%0ALearning%20%28VisGT%29%20modules.%20DVC%20dynamically%20constructs%20vocabulary%20for%20each%0Atraining%20batch%2C%20while%20VisGT%20maps%20visual%20features%20to%20semantic%20space%2C%20enhancing%0Atext%20features.%20We%20comprehensively%20conduct%20experiments%20on%20established%20remote%0Asensing%20benchmark%20DIOR%2C%20DOTAv2.0%2C%20as%20well%20as%20our%20newly%20introduced%2080-class%0ALAE-80C%20benchmark.%20Results%20demonstrate%20the%20advantages%20of%20the%20LAE-1M%20dataset%20and%0Athe%20effectiveness%20of%20the%20LAE-DINO%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09110v2&entry.124074799=Read"},
{"title": "Improve LLM-based Automatic Essay Scoring with Linguistic Features", "author": "Zhaoyi Joey Hou and Alejandro Ciuba and Xiang Lorraine Li", "abstract": "  Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.\n", "link": "http://arxiv.org/abs/2502.09497v1", "date": "2025-02-13", "relevancy": 2.2544, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4551}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improve%20LLM-based%20Automatic%20Essay%20Scoring%20with%20Linguistic%20Features&body=Title%3A%20Improve%20LLM-based%20Automatic%20Essay%20Scoring%20with%20Linguistic%20Features%0AAuthor%3A%20Zhaoyi%20Joey%20Hou%20and%20Alejandro%20Ciuba%20and%20Xiang%20Lorraine%20Li%0AAbstract%3A%20%20%20Automatic%20Essay%20Scoring%20%28AES%29%20assigns%20scores%20to%20student%20essays%2C%20reducing%20the%0Agrading%20workload%20for%20instructors.%20Developing%20a%20scoring%20system%20capable%20of%0Ahandling%20essays%20across%20diverse%20prompts%20is%20challenging%20due%20to%20the%20flexibility%0Aand%20diverse%20nature%20of%20the%20writing%20task.%20Existing%20methods%20typically%20fall%20into%0Atwo%20categories%3A%20supervised%20feature-based%20approaches%20and%20large%20language%20model%0A%28LLM%29-based%20methods.%20Supervised%20feature-based%20approaches%20often%20achieve%20higher%0Aperformance%20but%20require%20resource-intensive%20training.%20In%20contrast%2C%20LLM-based%0Amethods%20are%20computationally%20efficient%20during%20inference%20but%20tend%20to%20suffer%20from%0Alower%20performance.%20This%20paper%20combines%20these%20approaches%20by%20incorporating%0Alinguistic%20features%20into%20LLM-based%20scoring.%20Experimental%20results%20show%20that%20this%0Ahybrid%20method%20outperforms%20baseline%20models%20for%20both%20in-domain%20and%20out-of-domain%0Awriting%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprove%2520LLM-based%2520Automatic%2520Essay%2520Scoring%2520with%2520Linguistic%2520Features%26entry.906535625%3DZhaoyi%2520Joey%2520Hou%2520and%2520Alejandro%2520Ciuba%2520and%2520Xiang%2520Lorraine%2520Li%26entry.1292438233%3D%2520%2520Automatic%2520Essay%2520Scoring%2520%2528AES%2529%2520assigns%2520scores%2520to%2520student%2520essays%252C%2520reducing%2520the%250Agrading%2520workload%2520for%2520instructors.%2520Developing%2520a%2520scoring%2520system%2520capable%2520of%250Ahandling%2520essays%2520across%2520diverse%2520prompts%2520is%2520challenging%2520due%2520to%2520the%2520flexibility%250Aand%2520diverse%2520nature%2520of%2520the%2520writing%2520task.%2520Existing%2520methods%2520typically%2520fall%2520into%250Atwo%2520categories%253A%2520supervised%2520feature-based%2520approaches%2520and%2520large%2520language%2520model%250A%2528LLM%2529-based%2520methods.%2520Supervised%2520feature-based%2520approaches%2520often%2520achieve%2520higher%250Aperformance%2520but%2520require%2520resource-intensive%2520training.%2520In%2520contrast%252C%2520LLM-based%250Amethods%2520are%2520computationally%2520efficient%2520during%2520inference%2520but%2520tend%2520to%2520suffer%2520from%250Alower%2520performance.%2520This%2520paper%2520combines%2520these%2520approaches%2520by%2520incorporating%250Alinguistic%2520features%2520into%2520LLM-based%2520scoring.%2520Experimental%2520results%2520show%2520that%2520this%250Ahybrid%2520method%2520outperforms%2520baseline%2520models%2520for%2520both%2520in-domain%2520and%2520out-of-domain%250Awriting%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improve%20LLM-based%20Automatic%20Essay%20Scoring%20with%20Linguistic%20Features&entry.906535625=Zhaoyi%20Joey%20Hou%20and%20Alejandro%20Ciuba%20and%20Xiang%20Lorraine%20Li&entry.1292438233=%20%20Automatic%20Essay%20Scoring%20%28AES%29%20assigns%20scores%20to%20student%20essays%2C%20reducing%20the%0Agrading%20workload%20for%20instructors.%20Developing%20a%20scoring%20system%20capable%20of%0Ahandling%20essays%20across%20diverse%20prompts%20is%20challenging%20due%20to%20the%20flexibility%0Aand%20diverse%20nature%20of%20the%20writing%20task.%20Existing%20methods%20typically%20fall%20into%0Atwo%20categories%3A%20supervised%20feature-based%20approaches%20and%20large%20language%20model%0A%28LLM%29-based%20methods.%20Supervised%20feature-based%20approaches%20often%20achieve%20higher%0Aperformance%20but%20require%20resource-intensive%20training.%20In%20contrast%2C%20LLM-based%0Amethods%20are%20computationally%20efficient%20during%20inference%20but%20tend%20to%20suffer%20from%0Alower%20performance.%20This%20paper%20combines%20these%20approaches%20by%20incorporating%0Alinguistic%20features%20into%20LLM-based%20scoring.%20Experimental%20results%20show%20that%20this%0Ahybrid%20method%20outperforms%20baseline%20models%20for%20both%20in-domain%20and%20out-of-domain%0Awriting%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09497v1&entry.124074799=Read"},
{"title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation", "author": "Rongzhao He and Weihao Zheng and Leilei Zhao and Ying Wang and Dalin Zhu and Dan Wu and Bin Hu", "abstract": "  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n", "link": "http://arxiv.org/abs/2501.14679v3", "date": "2025-02-13", "relevancy": 2.2516, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5639}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5635}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation&body=Title%3A%20Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation%0AAuthor%3A%20Rongzhao%20He%20and%20Weihao%20Zheng%20and%20Leilei%20Zhao%20and%20Ying%20Wang%20and%20Dalin%20Zhu%20and%20Dan%20Wu%20and%20Bin%20Hu%0AAbstract%3A%20%20%20Attention-based%20methods%20have%20demonstrated%20exceptional%20performance%20in%0Amodelling%20long-range%20dependencies%20on%20spherical%20cortical%20surfaces%2C%20surpassing%0Atraditional%20Geometric%20Deep%20Learning%20%28GDL%29%20models.%20However%2C%20their%20extensive%0Ainference%20time%20and%20high%20memory%20demands%20pose%20challenges%20for%20application%20to%20large%0Adatasets%20with%20limited%20computing%20resources.%20Inspired%20by%20the%20state%20space%20model%20in%0Acomputer%20vision%2C%20we%20introduce%20the%20attention-free%20Vision%20Mamba%20%28Vim%29%20to%0Aspherical%20surfaces%2C%20presenting%20a%20domain-agnostic%20architecture%20for%20analyzing%0Adata%20on%20spherical%20manifolds.%20Our%20method%20achieves%20surface%20patching%20by%0Arepresenting%20spherical%20data%20as%20a%20sequence%20of%20triangular%20patches%20derived%20from%20a%0Asubdivided%20icosphere.%20The%20proposed%20Surface%20Vision%20Mamba%20%28SiM%29%20is%20evaluated%20on%0Amultiple%20neurodevelopmental%20phenotype%20regression%20tasks%20using%20cortical%20surface%0Ametrics%20from%20neonatal%20brains.%20Experimental%20results%20demonstrate%20that%20SiM%0Aoutperforms%20both%20attention-%20and%20GDL-based%20methods%2C%20delivering%204.8%20times%20faster%0Ainference%20and%20achieving%2091.7%25%20lower%20memory%20consumption%20compared%20to%20the%20Surface%0AVision%20Transformer%20%28SiT%29%20under%20the%20Ico-4%20grid%20partitioning.%20Sensitivity%0Aanalysis%20further%20underscores%20the%20potential%20of%20SiM%20to%20identify%20subtle%20cognitive%0Adevelopmental%20patterns.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Rongzhao-He/surface-vision-mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14679v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurface%2520Vision%2520Mamba%253A%2520Leveraging%2520Bidirectional%2520State%2520Space%2520Model%2520for%250A%2520%2520Efficient%2520Spherical%2520Manifold%2520Representation%26entry.906535625%3DRongzhao%2520He%2520and%2520Weihao%2520Zheng%2520and%2520Leilei%2520Zhao%2520and%2520Ying%2520Wang%2520and%2520Dalin%2520Zhu%2520and%2520Dan%2520Wu%2520and%2520Bin%2520Hu%26entry.1292438233%3D%2520%2520Attention-based%2520methods%2520have%2520demonstrated%2520exceptional%2520performance%2520in%250Amodelling%2520long-range%2520dependencies%2520on%2520spherical%2520cortical%2520surfaces%252C%2520surpassing%250Atraditional%2520Geometric%2520Deep%2520Learning%2520%2528GDL%2529%2520models.%2520However%252C%2520their%2520extensive%250Ainference%2520time%2520and%2520high%2520memory%2520demands%2520pose%2520challenges%2520for%2520application%2520to%2520large%250Adatasets%2520with%2520limited%2520computing%2520resources.%2520Inspired%2520by%2520the%2520state%2520space%2520model%2520in%250Acomputer%2520vision%252C%2520we%2520introduce%2520the%2520attention-free%2520Vision%2520Mamba%2520%2528Vim%2529%2520to%250Aspherical%2520surfaces%252C%2520presenting%2520a%2520domain-agnostic%2520architecture%2520for%2520analyzing%250Adata%2520on%2520spherical%2520manifolds.%2520Our%2520method%2520achieves%2520surface%2520patching%2520by%250Arepresenting%2520spherical%2520data%2520as%2520a%2520sequence%2520of%2520triangular%2520patches%2520derived%2520from%2520a%250Asubdivided%2520icosphere.%2520The%2520proposed%2520Surface%2520Vision%2520Mamba%2520%2528SiM%2529%2520is%2520evaluated%2520on%250Amultiple%2520neurodevelopmental%2520phenotype%2520regression%2520tasks%2520using%2520cortical%2520surface%250Ametrics%2520from%2520neonatal%2520brains.%2520Experimental%2520results%2520demonstrate%2520that%2520SiM%250Aoutperforms%2520both%2520attention-%2520and%2520GDL-based%2520methods%252C%2520delivering%25204.8%2520times%2520faster%250Ainference%2520and%2520achieving%252091.7%2525%2520lower%2520memory%2520consumption%2520compared%2520to%2520the%2520Surface%250AVision%2520Transformer%2520%2528SiT%2529%2520under%2520the%2520Ico-4%2520grid%2520partitioning.%2520Sensitivity%250Aanalysis%2520further%2520underscores%2520the%2520potential%2520of%2520SiM%2520to%2520identify%2520subtle%2520cognitive%250Adevelopmental%2520patterns.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Rongzhao-He/surface-vision-mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14679v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation&entry.906535625=Rongzhao%20He%20and%20Weihao%20Zheng%20and%20Leilei%20Zhao%20and%20Ying%20Wang%20and%20Dalin%20Zhu%20and%20Dan%20Wu%20and%20Bin%20Hu&entry.1292438233=%20%20Attention-based%20methods%20have%20demonstrated%20exceptional%20performance%20in%0Amodelling%20long-range%20dependencies%20on%20spherical%20cortical%20surfaces%2C%20surpassing%0Atraditional%20Geometric%20Deep%20Learning%20%28GDL%29%20models.%20However%2C%20their%20extensive%0Ainference%20time%20and%20high%20memory%20demands%20pose%20challenges%20for%20application%20to%20large%0Adatasets%20with%20limited%20computing%20resources.%20Inspired%20by%20the%20state%20space%20model%20in%0Acomputer%20vision%2C%20we%20introduce%20the%20attention-free%20Vision%20Mamba%20%28Vim%29%20to%0Aspherical%20surfaces%2C%20presenting%20a%20domain-agnostic%20architecture%20for%20analyzing%0Adata%20on%20spherical%20manifolds.%20Our%20method%20achieves%20surface%20patching%20by%0Arepresenting%20spherical%20data%20as%20a%20sequence%20of%20triangular%20patches%20derived%20from%20a%0Asubdivided%20icosphere.%20The%20proposed%20Surface%20Vision%20Mamba%20%28SiM%29%20is%20evaluated%20on%0Amultiple%20neurodevelopmental%20phenotype%20regression%20tasks%20using%20cortical%20surface%0Ametrics%20from%20neonatal%20brains.%20Experimental%20results%20demonstrate%20that%20SiM%0Aoutperforms%20both%20attention-%20and%20GDL-based%20methods%2C%20delivering%204.8%20times%20faster%0Ainference%20and%20achieving%2091.7%25%20lower%20memory%20consumption%20compared%20to%20the%20Surface%0AVision%20Transformer%20%28SiT%29%20under%20the%20Ico-4%20grid%20partitioning.%20Sensitivity%0Aanalysis%20further%20underscores%20the%20potential%20of%20SiM%20to%20identify%20subtle%20cognitive%0Adevelopmental%20patterns.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Rongzhao-He/surface-vision-mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14679v3&entry.124074799=Read"},
{"title": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations", "author": "Dexian Cai and Xiaocui Yang and Yongkang Liu and Daling Wang and Shi Feng and Yifei Zhang and Soujanya Poria", "abstract": "  Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.\n", "link": "http://arxiv.org/abs/2502.09447v1", "date": "2025-02-13", "relevancy": 2.2141, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel-Level%20Reasoning%20Segmentation%20via%20Multi-turn%20Conversations&body=Title%3A%20Pixel-Level%20Reasoning%20Segmentation%20via%20Multi-turn%20Conversations%0AAuthor%3A%20Dexian%20Cai%20and%20Xiaocui%20Yang%20and%20Yongkang%20Liu%20and%20Daling%20Wang%20and%20Shi%20Feng%20and%20Yifei%20Zhang%20and%20Soujanya%20Poria%0AAbstract%3A%20%20%20Existing%20visual%20perception%20systems%20focus%20on%20region-level%20segmentation%20in%0Asingle-turn%20dialogues%2C%20relying%20on%20complex%20and%20explicit%20query%20instructions.%20Such%0Asystems%20cannot%20reason%20at%20the%20pixel%20level%20and%20comprehend%20dynamic%20user%20intent%0Athat%20changes%20over%20interaction.%20Our%20work%20tackles%20this%20issue%20by%20introducing%20a%0Anovel%20task%2C%20Pixel-level%20Reasoning%20Segmentation%20%28Pixel-level%20RS%29%20based%20on%0Amulti-turn%20conversations%2C%20tracking%20evolving%20user%20intent%20via%20multi-turn%0Ainteractions%20for%20fine-grained%20segmentation.%20To%20establish%20a%20benchmark%20for%20this%0Anovel%20task%2C%20we%20build%20a%20Pixel-level%20ReasonIng%20Segmentation%20Dataset%20Based%20on%0AMulti-Turn%20Conversations%20%28PRIST%29%2C%20comprising%2024k%20utterances%20from%208.3k%0Amulti-turn%20conversational%20scenarios%20with%20segmentation%20targets.%20Building%20on%0APRIST%2C%20we%20further%20propose%20MIRAS%2C%20a%20Multi-turn%20Interactive%20ReAsoning%0ASegmentation%20framework%2C%20integrates%20pixel-level%20segmentation%20with%20robust%0Amulti-turn%20conversation%20understanding%2C%20generating%20pixel-grounded%20explanations%0Aaligned%20with%20user%20intent.%20The%20PRIST%20dataset%20and%20MIRSA%20framework%20fill%20the%20gap%20in%0Apixel-level%20reasoning%20segmentation.%20Experimental%20results%20on%20the%20PRIST%20dataset%0Ademonstrate%20that%20our%20method%20outperforms%20current%20segmentation-specific%20baselines%0Ain%20terms%20of%20segmentation%20and%20LLM-based%20reasoning%20metrics.%20The%20code%20and%20data%20are%0Aavailable%20at%3A%20https%3A//github.com/ccccai239/PixelRIST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel-Level%2520Reasoning%2520Segmentation%2520via%2520Multi-turn%2520Conversations%26entry.906535625%3DDexian%2520Cai%2520and%2520Xiaocui%2520Yang%2520and%2520Yongkang%2520Liu%2520and%2520Daling%2520Wang%2520and%2520Shi%2520Feng%2520and%2520Yifei%2520Zhang%2520and%2520Soujanya%2520Poria%26entry.1292438233%3D%2520%2520Existing%2520visual%2520perception%2520systems%2520focus%2520on%2520region-level%2520segmentation%2520in%250Asingle-turn%2520dialogues%252C%2520relying%2520on%2520complex%2520and%2520explicit%2520query%2520instructions.%2520Such%250Asystems%2520cannot%2520reason%2520at%2520the%2520pixel%2520level%2520and%2520comprehend%2520dynamic%2520user%2520intent%250Athat%2520changes%2520over%2520interaction.%2520Our%2520work%2520tackles%2520this%2520issue%2520by%2520introducing%2520a%250Anovel%2520task%252C%2520Pixel-level%2520Reasoning%2520Segmentation%2520%2528Pixel-level%2520RS%2529%2520based%2520on%250Amulti-turn%2520conversations%252C%2520tracking%2520evolving%2520user%2520intent%2520via%2520multi-turn%250Ainteractions%2520for%2520fine-grained%2520segmentation.%2520To%2520establish%2520a%2520benchmark%2520for%2520this%250Anovel%2520task%252C%2520we%2520build%2520a%2520Pixel-level%2520ReasonIng%2520Segmentation%2520Dataset%2520Based%2520on%250AMulti-Turn%2520Conversations%2520%2528PRIST%2529%252C%2520comprising%252024k%2520utterances%2520from%25208.3k%250Amulti-turn%2520conversational%2520scenarios%2520with%2520segmentation%2520targets.%2520Building%2520on%250APRIST%252C%2520we%2520further%2520propose%2520MIRAS%252C%2520a%2520Multi-turn%2520Interactive%2520ReAsoning%250ASegmentation%2520framework%252C%2520integrates%2520pixel-level%2520segmentation%2520with%2520robust%250Amulti-turn%2520conversation%2520understanding%252C%2520generating%2520pixel-grounded%2520explanations%250Aaligned%2520with%2520user%2520intent.%2520The%2520PRIST%2520dataset%2520and%2520MIRSA%2520framework%2520fill%2520the%2520gap%2520in%250Apixel-level%2520reasoning%2520segmentation.%2520Experimental%2520results%2520on%2520the%2520PRIST%2520dataset%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520current%2520segmentation-specific%2520baselines%250Ain%2520terms%2520of%2520segmentation%2520and%2520LLM-based%2520reasoning%2520metrics.%2520The%2520code%2520and%2520data%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/ccccai239/PixelRIST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Level%20Reasoning%20Segmentation%20via%20Multi-turn%20Conversations&entry.906535625=Dexian%20Cai%20and%20Xiaocui%20Yang%20and%20Yongkang%20Liu%20and%20Daling%20Wang%20and%20Shi%20Feng%20and%20Yifei%20Zhang%20and%20Soujanya%20Poria&entry.1292438233=%20%20Existing%20visual%20perception%20systems%20focus%20on%20region-level%20segmentation%20in%0Asingle-turn%20dialogues%2C%20relying%20on%20complex%20and%20explicit%20query%20instructions.%20Such%0Asystems%20cannot%20reason%20at%20the%20pixel%20level%20and%20comprehend%20dynamic%20user%20intent%0Athat%20changes%20over%20interaction.%20Our%20work%20tackles%20this%20issue%20by%20introducing%20a%0Anovel%20task%2C%20Pixel-level%20Reasoning%20Segmentation%20%28Pixel-level%20RS%29%20based%20on%0Amulti-turn%20conversations%2C%20tracking%20evolving%20user%20intent%20via%20multi-turn%0Ainteractions%20for%20fine-grained%20segmentation.%20To%20establish%20a%20benchmark%20for%20this%0Anovel%20task%2C%20we%20build%20a%20Pixel-level%20ReasonIng%20Segmentation%20Dataset%20Based%20on%0AMulti-Turn%20Conversations%20%28PRIST%29%2C%20comprising%2024k%20utterances%20from%208.3k%0Amulti-turn%20conversational%20scenarios%20with%20segmentation%20targets.%20Building%20on%0APRIST%2C%20we%20further%20propose%20MIRAS%2C%20a%20Multi-turn%20Interactive%20ReAsoning%0ASegmentation%20framework%2C%20integrates%20pixel-level%20segmentation%20with%20robust%0Amulti-turn%20conversation%20understanding%2C%20generating%20pixel-grounded%20explanations%0Aaligned%20with%20user%20intent.%20The%20PRIST%20dataset%20and%20MIRSA%20framework%20fill%20the%20gap%20in%0Apixel-level%20reasoning%20segmentation.%20Experimental%20results%20on%20the%20PRIST%20dataset%0Ademonstrate%20that%20our%20method%20outperforms%20current%20segmentation-specific%20baselines%0Ain%20terms%20of%20segmentation%20and%20LLM-based%20reasoning%20metrics.%20The%20code%20and%20data%20are%0Aavailable%20at%3A%20https%3A//github.com/ccccai239/PixelRIST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09447v1&entry.124074799=Read"},
{"title": "A 3D Facial Reconstruction Evaluation Methodology: Comparing Smartphone\n  Scans with Deep Learning Based Methods Using Geometry and Morphometry\n  Criteria", "author": "\u00c1lvaro Heredia-Lid\u00f3n and Alejandro Mo\u00f1ux-Bernal and Alejandro Gonz\u00e1lez and Luis M. Echeverry-Quiceno and Max Rubert and Aroa Casado and Mar\u00eda Esther Esteban and Mireia Andreu-Montoriol and Susanna Gallardo and Cristina Ruffo and Neus Mart\u00ednez-Abad\u00edas and Xavier Sevillano", "abstract": "  Three-dimensional (3D) facial shape analysis has gained interest due to its\npotential clinical applications. However, the high cost of advanced 3D facial\nacquisition systems limits their widespread use, driving the development of\nlow-cost acquisition and reconstruction methods. This study introduces a novel\nevaluation methodology that goes beyond traditional geometry-based benchmarks\nby integrating morphometric shape analysis techniques, providing a statistical\nframework for assessing facial morphology preservation. As a case study, we\ncompare smartphone-based 3D scans with state-of-the-art deep learning\nreconstruction methods from 2D images, using high-end stereophotogrammetry\nmodels as ground truth. This methodology enables a quantitative assessment of\nglobal and local shape differences, offering a biologically meaningful\nvalidation approach for low-cost 3D facial acquisition and reconstruction\ntechniques.\n", "link": "http://arxiv.org/abs/2502.09425v1", "date": "2025-02-13", "relevancy": 2.2055, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.556}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5548}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%203D%20Facial%20Reconstruction%20Evaluation%20Methodology%3A%20Comparing%20Smartphone%0A%20%20Scans%20with%20Deep%20Learning%20Based%20Methods%20Using%20Geometry%20and%20Morphometry%0A%20%20Criteria&body=Title%3A%20A%203D%20Facial%20Reconstruction%20Evaluation%20Methodology%3A%20Comparing%20Smartphone%0A%20%20Scans%20with%20Deep%20Learning%20Based%20Methods%20Using%20Geometry%20and%20Morphometry%0A%20%20Criteria%0AAuthor%3A%20%C3%81lvaro%20Heredia-Lid%C3%B3n%20and%20Alejandro%20Mo%C3%B1ux-Bernal%20and%20Alejandro%20Gonz%C3%A1lez%20and%20Luis%20M.%20Echeverry-Quiceno%20and%20Max%20Rubert%20and%20Aroa%20Casado%20and%20Mar%C3%ADa%20Esther%20Esteban%20and%20Mireia%20Andreu-Montoriol%20and%20Susanna%20Gallardo%20and%20Cristina%20Ruffo%20and%20Neus%20Mart%C3%ADnez-Abad%C3%ADas%20and%20Xavier%20Sevillano%0AAbstract%3A%20%20%20Three-dimensional%20%283D%29%20facial%20shape%20analysis%20has%20gained%20interest%20due%20to%20its%0Apotential%20clinical%20applications.%20However%2C%20the%20high%20cost%20of%20advanced%203D%20facial%0Aacquisition%20systems%20limits%20their%20widespread%20use%2C%20driving%20the%20development%20of%0Alow-cost%20acquisition%20and%20reconstruction%20methods.%20This%20study%20introduces%20a%20novel%0Aevaluation%20methodology%20that%20goes%20beyond%20traditional%20geometry-based%20benchmarks%0Aby%20integrating%20morphometric%20shape%20analysis%20techniques%2C%20providing%20a%20statistical%0Aframework%20for%20assessing%20facial%20morphology%20preservation.%20As%20a%20case%20study%2C%20we%0Acompare%20smartphone-based%203D%20scans%20with%20state-of-the-art%20deep%20learning%0Areconstruction%20methods%20from%202D%20images%2C%20using%20high-end%20stereophotogrammetry%0Amodels%20as%20ground%20truth.%20This%20methodology%20enables%20a%20quantitative%20assessment%20of%0Aglobal%20and%20local%20shape%20differences%2C%20offering%20a%20biologically%20meaningful%0Avalidation%20approach%20for%20low-cost%203D%20facial%20acquisition%20and%20reconstruction%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%25203D%2520Facial%2520Reconstruction%2520Evaluation%2520Methodology%253A%2520Comparing%2520Smartphone%250A%2520%2520Scans%2520with%2520Deep%2520Learning%2520Based%2520Methods%2520Using%2520Geometry%2520and%2520Morphometry%250A%2520%2520Criteria%26entry.906535625%3D%25C3%2581lvaro%2520Heredia-Lid%25C3%25B3n%2520and%2520Alejandro%2520Mo%25C3%25B1ux-Bernal%2520and%2520Alejandro%2520Gonz%25C3%25A1lez%2520and%2520Luis%2520M.%2520Echeverry-Quiceno%2520and%2520Max%2520Rubert%2520and%2520Aroa%2520Casado%2520and%2520Mar%25C3%25ADa%2520Esther%2520Esteban%2520and%2520Mireia%2520Andreu-Montoriol%2520and%2520Susanna%2520Gallardo%2520and%2520Cristina%2520Ruffo%2520and%2520Neus%2520Mart%25C3%25ADnez-Abad%25C3%25ADas%2520and%2520Xavier%2520Sevillano%26entry.1292438233%3D%2520%2520Three-dimensional%2520%25283D%2529%2520facial%2520shape%2520analysis%2520has%2520gained%2520interest%2520due%2520to%2520its%250Apotential%2520clinical%2520applications.%2520However%252C%2520the%2520high%2520cost%2520of%2520advanced%25203D%2520facial%250Aacquisition%2520systems%2520limits%2520their%2520widespread%2520use%252C%2520driving%2520the%2520development%2520of%250Alow-cost%2520acquisition%2520and%2520reconstruction%2520methods.%2520This%2520study%2520introduces%2520a%2520novel%250Aevaluation%2520methodology%2520that%2520goes%2520beyond%2520traditional%2520geometry-based%2520benchmarks%250Aby%2520integrating%2520morphometric%2520shape%2520analysis%2520techniques%252C%2520providing%2520a%2520statistical%250Aframework%2520for%2520assessing%2520facial%2520morphology%2520preservation.%2520As%2520a%2520case%2520study%252C%2520we%250Acompare%2520smartphone-based%25203D%2520scans%2520with%2520state-of-the-art%2520deep%2520learning%250Areconstruction%2520methods%2520from%25202D%2520images%252C%2520using%2520high-end%2520stereophotogrammetry%250Amodels%2520as%2520ground%2520truth.%2520This%2520methodology%2520enables%2520a%2520quantitative%2520assessment%2520of%250Aglobal%2520and%2520local%2520shape%2520differences%252C%2520offering%2520a%2520biologically%2520meaningful%250Avalidation%2520approach%2520for%2520low-cost%25203D%2520facial%2520acquisition%2520and%2520reconstruction%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%203D%20Facial%20Reconstruction%20Evaluation%20Methodology%3A%20Comparing%20Smartphone%0A%20%20Scans%20with%20Deep%20Learning%20Based%20Methods%20Using%20Geometry%20and%20Morphometry%0A%20%20Criteria&entry.906535625=%C3%81lvaro%20Heredia-Lid%C3%B3n%20and%20Alejandro%20Mo%C3%B1ux-Bernal%20and%20Alejandro%20Gonz%C3%A1lez%20and%20Luis%20M.%20Echeverry-Quiceno%20and%20Max%20Rubert%20and%20Aroa%20Casado%20and%20Mar%C3%ADa%20Esther%20Esteban%20and%20Mireia%20Andreu-Montoriol%20and%20Susanna%20Gallardo%20and%20Cristina%20Ruffo%20and%20Neus%20Mart%C3%ADnez-Abad%C3%ADas%20and%20Xavier%20Sevillano&entry.1292438233=%20%20Three-dimensional%20%283D%29%20facial%20shape%20analysis%20has%20gained%20interest%20due%20to%20its%0Apotential%20clinical%20applications.%20However%2C%20the%20high%20cost%20of%20advanced%203D%20facial%0Aacquisition%20systems%20limits%20their%20widespread%20use%2C%20driving%20the%20development%20of%0Alow-cost%20acquisition%20and%20reconstruction%20methods.%20This%20study%20introduces%20a%20novel%0Aevaluation%20methodology%20that%20goes%20beyond%20traditional%20geometry-based%20benchmarks%0Aby%20integrating%20morphometric%20shape%20analysis%20techniques%2C%20providing%20a%20statistical%0Aframework%20for%20assessing%20facial%20morphology%20preservation.%20As%20a%20case%20study%2C%20we%0Acompare%20smartphone-based%203D%20scans%20with%20state-of-the-art%20deep%20learning%0Areconstruction%20methods%20from%202D%20images%2C%20using%20high-end%20stereophotogrammetry%0Amodels%20as%20ground%20truth.%20This%20methodology%20enables%20a%20quantitative%20assessment%20of%0Aglobal%20and%20local%20shape%20differences%2C%20offering%20a%20biologically%20meaningful%0Avalidation%20approach%20for%20low-cost%203D%20facial%20acquisition%20and%20reconstruction%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09425v1&entry.124074799=Read"},
{"title": "Heuristical Comparison of Vision Transformers Against Convolutional\n  Neural Networks for Semantic Segmentation on Remote Sensing Imagery", "author": "Ashim Dahal and Saydul Akbar Murad and Nick Rahimi", "abstract": "  Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.\n", "link": "http://arxiv.org/abs/2411.09101v2", "date": "2025-02-13", "relevancy": 2.1919, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5507}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heuristical%20Comparison%20of%20Vision%20Transformers%20Against%20Convolutional%0A%20%20Neural%20Networks%20for%20Semantic%20Segmentation%20on%20Remote%20Sensing%20Imagery&body=Title%3A%20Heuristical%20Comparison%20of%20Vision%20Transformers%20Against%20Convolutional%0A%20%20Neural%20Networks%20for%20Semantic%20Segmentation%20on%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Ashim%20Dahal%20and%20Saydul%20Akbar%20Murad%20and%20Nick%20Rahimi%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViT%29%20have%20recently%20brought%20a%20new%20wave%20of%20research%20in%20the%0Afield%20of%20computer%20vision.%20These%20models%20have%20performed%20particularly%20well%20in%0Aimage%20classification%20and%20segmentation.%20Research%20on%20semantic%20and%20instance%0Asegmentation%20has%20accelerated%20with%20the%20introduction%20of%20the%20new%20architecture%2C%0Awith%20over%2080%25%20of%20the%20top%2020%20benchmarks%20for%20the%20iSAID%20dataset%20based%20on%20either%0Athe%20ViT%20architecture%20or%20the%20attention%20mechanism%20behind%20its%20success.%20This%20paper%0Afocuses%20on%20the%20heuristic%20comparison%20of%20three%20key%20factors%20of%20using%20%28or%20not%0Ausing%29%20ViT%20for%20semantic%20segmentation%20of%20remote%20sensing%20aerial%20images%20on%20the%0AiSAID%20dataset.%20The%20experimental%20results%20observed%20during%20this%20research%20were%0Aanalyzed%20based%20on%20three%20objectives.%20First%2C%20we%20studied%20the%20use%20of%20a%20weighted%0Afused%20loss%20function%20to%20maximize%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20score%0Aand%20Dice%20score%20while%20minimizing%20entropy%20or%20class%20representation%20loss.%20Second%2C%0Awe%20compared%20transfer%20learning%20on%20Meta%27s%20MaskFormer%2C%20a%20ViT-based%20semantic%0Asegmentation%20model%2C%20against%20a%20generic%20UNet%20Convolutional%20Neural%20Network%20%28CNN%29%0Abased%20on%20mIoU%2C%20Dice%20scores%2C%20training%20efficiency%2C%20and%20inference%20time.%20Third%2C%20we%0Aexamined%20the%20trade-offs%20between%20the%20two%20models%20in%20comparison%20to%20current%0Astate-of-the-art%20segmentation%20models.%20We%20show%20that%20the%20novel%20combined%20weighted%0Aloss%20function%20significantly%20boosts%20the%20CNN%20model%27s%20performance%20compared%20to%0Atransfer%20learning%20with%20ViT.%20The%20code%20for%20this%20implementation%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeuristical%2520Comparison%2520of%2520Vision%2520Transformers%2520Against%2520Convolutional%250A%2520%2520Neural%2520Networks%2520for%2520Semantic%2520Segmentation%2520on%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DAshim%2520Dahal%2520and%2520Saydul%2520Akbar%2520Murad%2520and%2520Nick%2520Rahimi%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViT%2529%2520have%2520recently%2520brought%2520a%2520new%2520wave%2520of%2520research%2520in%2520the%250Afield%2520of%2520computer%2520vision.%2520These%2520models%2520have%2520performed%2520particularly%2520well%2520in%250Aimage%2520classification%2520and%2520segmentation.%2520Research%2520on%2520semantic%2520and%2520instance%250Asegmentation%2520has%2520accelerated%2520with%2520the%2520introduction%2520of%2520the%2520new%2520architecture%252C%250Awith%2520over%252080%2525%2520of%2520the%2520top%252020%2520benchmarks%2520for%2520the%2520iSAID%2520dataset%2520based%2520on%2520either%250Athe%2520ViT%2520architecture%2520or%2520the%2520attention%2520mechanism%2520behind%2520its%2520success.%2520This%2520paper%250Afocuses%2520on%2520the%2520heuristic%2520comparison%2520of%2520three%2520key%2520factors%2520of%2520using%2520%2528or%2520not%250Ausing%2529%2520ViT%2520for%2520semantic%2520segmentation%2520of%2520remote%2520sensing%2520aerial%2520images%2520on%2520the%250AiSAID%2520dataset.%2520The%2520experimental%2520results%2520observed%2520during%2520this%2520research%2520were%250Aanalyzed%2520based%2520on%2520three%2520objectives.%2520First%252C%2520we%2520studied%2520the%2520use%2520of%2520a%2520weighted%250Afused%2520loss%2520function%2520to%2520maximize%2520the%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520score%250Aand%2520Dice%2520score%2520while%2520minimizing%2520entropy%2520or%2520class%2520representation%2520loss.%2520Second%252C%250Awe%2520compared%2520transfer%2520learning%2520on%2520Meta%2527s%2520MaskFormer%252C%2520a%2520ViT-based%2520semantic%250Asegmentation%2520model%252C%2520against%2520a%2520generic%2520UNet%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%250Abased%2520on%2520mIoU%252C%2520Dice%2520scores%252C%2520training%2520efficiency%252C%2520and%2520inference%2520time.%2520Third%252C%2520we%250Aexamined%2520the%2520trade-offs%2520between%2520the%2520two%2520models%2520in%2520comparison%2520to%2520current%250Astate-of-the-art%2520segmentation%2520models.%2520We%2520show%2520that%2520the%2520novel%2520combined%2520weighted%250Aloss%2520function%2520significantly%2520boosts%2520the%2520CNN%2520model%2527s%2520performance%2520compared%2520to%250Atransfer%2520learning%2520with%2520ViT.%2520The%2520code%2520for%2520this%2520implementation%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heuristical%20Comparison%20of%20Vision%20Transformers%20Against%20Convolutional%0A%20%20Neural%20Networks%20for%20Semantic%20Segmentation%20on%20Remote%20Sensing%20Imagery&entry.906535625=Ashim%20Dahal%20and%20Saydul%20Akbar%20Murad%20and%20Nick%20Rahimi&entry.1292438233=%20%20Vision%20Transformers%20%28ViT%29%20have%20recently%20brought%20a%20new%20wave%20of%20research%20in%20the%0Afield%20of%20computer%20vision.%20These%20models%20have%20performed%20particularly%20well%20in%0Aimage%20classification%20and%20segmentation.%20Research%20on%20semantic%20and%20instance%0Asegmentation%20has%20accelerated%20with%20the%20introduction%20of%20the%20new%20architecture%2C%0Awith%20over%2080%25%20of%20the%20top%2020%20benchmarks%20for%20the%20iSAID%20dataset%20based%20on%20either%0Athe%20ViT%20architecture%20or%20the%20attention%20mechanism%20behind%20its%20success.%20This%20paper%0Afocuses%20on%20the%20heuristic%20comparison%20of%20three%20key%20factors%20of%20using%20%28or%20not%0Ausing%29%20ViT%20for%20semantic%20segmentation%20of%20remote%20sensing%20aerial%20images%20on%20the%0AiSAID%20dataset.%20The%20experimental%20results%20observed%20during%20this%20research%20were%0Aanalyzed%20based%20on%20three%20objectives.%20First%2C%20we%20studied%20the%20use%20of%20a%20weighted%0Afused%20loss%20function%20to%20maximize%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20score%0Aand%20Dice%20score%20while%20minimizing%20entropy%20or%20class%20representation%20loss.%20Second%2C%0Awe%20compared%20transfer%20learning%20on%20Meta%27s%20MaskFormer%2C%20a%20ViT-based%20semantic%0Asegmentation%20model%2C%20against%20a%20generic%20UNet%20Convolutional%20Neural%20Network%20%28CNN%29%0Abased%20on%20mIoU%2C%20Dice%20scores%2C%20training%20efficiency%2C%20and%20inference%20time.%20Third%2C%20we%0Aexamined%20the%20trade-offs%20between%20the%20two%20models%20in%20comparison%20to%20current%0Astate-of-the-art%20segmentation%20models.%20We%20show%20that%20the%20novel%20combined%20weighted%0Aloss%20function%20significantly%20boosts%20the%20CNN%20model%27s%20performance%20compared%20to%0Atransfer%20learning%20with%20ViT.%20The%20code%20for%20this%20implementation%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09101v2&entry.124074799=Read"},
{"title": "Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for\n  Weakly-supervised Oriented Object Detection", "author": "Yi Yu and Xue Yang and Yansheng Li and Zhenjun Han and Feipeng Da and Junchi Yan", "abstract": "  Accurately estimating the orientation of visual objects with compact rotated\nbounding boxes (RBoxes) has become a prominent demand, which challenges\nexisting object detection paradigms that only use horizontal bounding boxes\n(HBoxes). To equip the detectors with orientation awareness, supervised\nregression/classification modules have been introduced at the high cost of\nrotation annotation. Meanwhile, some existing datasets with oriented objects\nare already annotated with horizontal boxes or even single points. It becomes\nattractive yet remains open for effectively utilizing weaker single point and\nhorizontal annotations to train an oriented object detector (OOD). We develop\nWholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging\nvarious labeling forms (Points, HBoxes, RBoxes, and their combination) in a\nunified fashion. By only using HBox for training, our Wholly-WOOD achieves\nperformance very close to that of the RBox-trained counterpart on remote\nsensing and other areas, significantly reducing the tedious efforts on\nlabor-intensive annotation for oriented objects. The source codes are available\nat https://github.com/VisionXLab/whollywood (PyTorch-based) and\nhttps://github.com/VisionXLab/whollywood-jittor (Jittor-based).\n", "link": "http://arxiv.org/abs/2502.09471v1", "date": "2025-02-13", "relevancy": 2.1624, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5437}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5425}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wholly-WOOD%3A%20Wholly%20Leveraging%20Diversified-quality%20Labels%20for%0A%20%20Weakly-supervised%20Oriented%20Object%20Detection&body=Title%3A%20Wholly-WOOD%3A%20Wholly%20Leveraging%20Diversified-quality%20Labels%20for%0A%20%20Weakly-supervised%20Oriented%20Object%20Detection%0AAuthor%3A%20Yi%20Yu%20and%20Xue%20Yang%20and%20Yansheng%20Li%20and%20Zhenjun%20Han%20and%20Feipeng%20Da%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20Accurately%20estimating%20the%20orientation%20of%20visual%20objects%20with%20compact%20rotated%0Abounding%20boxes%20%28RBoxes%29%20has%20become%20a%20prominent%20demand%2C%20which%20challenges%0Aexisting%20object%20detection%20paradigms%20that%20only%20use%20horizontal%20bounding%20boxes%0A%28HBoxes%29.%20To%20equip%20the%20detectors%20with%20orientation%20awareness%2C%20supervised%0Aregression/classification%20modules%20have%20been%20introduced%20at%20the%20high%20cost%20of%0Arotation%20annotation.%20Meanwhile%2C%20some%20existing%20datasets%20with%20oriented%20objects%0Aare%20already%20annotated%20with%20horizontal%20boxes%20or%20even%20single%20points.%20It%20becomes%0Aattractive%20yet%20remains%20open%20for%20effectively%20utilizing%20weaker%20single%20point%20and%0Ahorizontal%20annotations%20to%20train%20an%20oriented%20object%20detector%20%28OOD%29.%20We%20develop%0AWholly-WOOD%2C%20a%20weakly-supervised%20OOD%20framework%2C%20capable%20of%20wholly%20leveraging%0Avarious%20labeling%20forms%20%28Points%2C%20HBoxes%2C%20RBoxes%2C%20and%20their%20combination%29%20in%20a%0Aunified%20fashion.%20By%20only%20using%20HBox%20for%20training%2C%20our%20Wholly-WOOD%20achieves%0Aperformance%20very%20close%20to%20that%20of%20the%20RBox-trained%20counterpart%20on%20remote%0Asensing%20and%20other%20areas%2C%20significantly%20reducing%20the%20tedious%20efforts%20on%0Alabor-intensive%20annotation%20for%20oriented%20objects.%20The%20source%20codes%20are%20available%0Aat%20https%3A//github.com/VisionXLab/whollywood%20%28PyTorch-based%29%20and%0Ahttps%3A//github.com/VisionXLab/whollywood-jittor%20%28Jittor-based%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWholly-WOOD%253A%2520Wholly%2520Leveraging%2520Diversified-quality%2520Labels%2520for%250A%2520%2520Weakly-supervised%2520Oriented%2520Object%2520Detection%26entry.906535625%3DYi%2520Yu%2520and%2520Xue%2520Yang%2520and%2520Yansheng%2520Li%2520and%2520Zhenjun%2520Han%2520and%2520Feipeng%2520Da%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520Accurately%2520estimating%2520the%2520orientation%2520of%2520visual%2520objects%2520with%2520compact%2520rotated%250Abounding%2520boxes%2520%2528RBoxes%2529%2520has%2520become%2520a%2520prominent%2520demand%252C%2520which%2520challenges%250Aexisting%2520object%2520detection%2520paradigms%2520that%2520only%2520use%2520horizontal%2520bounding%2520boxes%250A%2528HBoxes%2529.%2520To%2520equip%2520the%2520detectors%2520with%2520orientation%2520awareness%252C%2520supervised%250Aregression/classification%2520modules%2520have%2520been%2520introduced%2520at%2520the%2520high%2520cost%2520of%250Arotation%2520annotation.%2520Meanwhile%252C%2520some%2520existing%2520datasets%2520with%2520oriented%2520objects%250Aare%2520already%2520annotated%2520with%2520horizontal%2520boxes%2520or%2520even%2520single%2520points.%2520It%2520becomes%250Aattractive%2520yet%2520remains%2520open%2520for%2520effectively%2520utilizing%2520weaker%2520single%2520point%2520and%250Ahorizontal%2520annotations%2520to%2520train%2520an%2520oriented%2520object%2520detector%2520%2528OOD%2529.%2520We%2520develop%250AWholly-WOOD%252C%2520a%2520weakly-supervised%2520OOD%2520framework%252C%2520capable%2520of%2520wholly%2520leveraging%250Avarious%2520labeling%2520forms%2520%2528Points%252C%2520HBoxes%252C%2520RBoxes%252C%2520and%2520their%2520combination%2529%2520in%2520a%250Aunified%2520fashion.%2520By%2520only%2520using%2520HBox%2520for%2520training%252C%2520our%2520Wholly-WOOD%2520achieves%250Aperformance%2520very%2520close%2520to%2520that%2520of%2520the%2520RBox-trained%2520counterpart%2520on%2520remote%250Asensing%2520and%2520other%2520areas%252C%2520significantly%2520reducing%2520the%2520tedious%2520efforts%2520on%250Alabor-intensive%2520annotation%2520for%2520oriented%2520objects.%2520The%2520source%2520codes%2520are%2520available%250Aat%2520https%253A//github.com/VisionXLab/whollywood%2520%2528PyTorch-based%2529%2520and%250Ahttps%253A//github.com/VisionXLab/whollywood-jittor%2520%2528Jittor-based%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wholly-WOOD%3A%20Wholly%20Leveraging%20Diversified-quality%20Labels%20for%0A%20%20Weakly-supervised%20Oriented%20Object%20Detection&entry.906535625=Yi%20Yu%20and%20Xue%20Yang%20and%20Yansheng%20Li%20and%20Zhenjun%20Han%20and%20Feipeng%20Da%20and%20Junchi%20Yan&entry.1292438233=%20%20Accurately%20estimating%20the%20orientation%20of%20visual%20objects%20with%20compact%20rotated%0Abounding%20boxes%20%28RBoxes%29%20has%20become%20a%20prominent%20demand%2C%20which%20challenges%0Aexisting%20object%20detection%20paradigms%20that%20only%20use%20horizontal%20bounding%20boxes%0A%28HBoxes%29.%20To%20equip%20the%20detectors%20with%20orientation%20awareness%2C%20supervised%0Aregression/classification%20modules%20have%20been%20introduced%20at%20the%20high%20cost%20of%0Arotation%20annotation.%20Meanwhile%2C%20some%20existing%20datasets%20with%20oriented%20objects%0Aare%20already%20annotated%20with%20horizontal%20boxes%20or%20even%20single%20points.%20It%20becomes%0Aattractive%20yet%20remains%20open%20for%20effectively%20utilizing%20weaker%20single%20point%20and%0Ahorizontal%20annotations%20to%20train%20an%20oriented%20object%20detector%20%28OOD%29.%20We%20develop%0AWholly-WOOD%2C%20a%20weakly-supervised%20OOD%20framework%2C%20capable%20of%20wholly%20leveraging%0Avarious%20labeling%20forms%20%28Points%2C%20HBoxes%2C%20RBoxes%2C%20and%20their%20combination%29%20in%20a%0Aunified%20fashion.%20By%20only%20using%20HBox%20for%20training%2C%20our%20Wholly-WOOD%20achieves%0Aperformance%20very%20close%20to%20that%20of%20the%20RBox-trained%20counterpart%20on%20remote%0Asensing%20and%20other%20areas%2C%20significantly%20reducing%20the%20tedious%20efforts%20on%0Alabor-intensive%20annotation%20for%20oriented%20objects.%20The%20source%20codes%20are%20available%0Aat%20https%3A//github.com/VisionXLab/whollywood%20%28PyTorch-based%29%20and%0Ahttps%3A//github.com/VisionXLab/whollywood-jittor%20%28Jittor-based%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09471v1&entry.124074799=Read"},
{"title": "A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration", "author": "Yuchen Hu and Xi Chen and Weidong Liu and Xiaojun Mao", "abstract": "  Distributed stochastic optimization algorithms can simultaneously process\nlarge-scale datasets, significantly accelerating model training. However, their\neffectiveness is often hindered by the sparsity of distributed networks and\ndata heterogeneity. In this paper, we propose a momentum-accelerated\ndistributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum\n(EDM), which mitigates the bias from data heterogeneity and incorporates\nmomentum techniques commonly used in deep learning to enhance convergence rate.\nOur theoretical analysis demonstrates that the EDM algorithm converges\nsub-linearly to the neighborhood of the optimal solution, the radius of which\nis irrespective of data heterogeneity, when applied to non-convex objective\nfunctions; under the Polyak-Lojasiewicz condition, which is a weaker assumption\nthan strong convexity, it converges linearly to the target region. Our analysis\ntechniques employed to handle momentum in complex distributed parameter update\nstructures yield a sufficiently tight convergence upper bound, offering a new\nperspective for the theoretical analysis of other momentum-based distributed\nalgorithms.\n", "link": "http://arxiv.org/abs/2501.19082v2", "date": "2025-02-13", "relevancy": 2.1427, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5717}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5177}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bias-Correction%20Decentralized%20Stochastic%20Gradient%20Algorithm%20with%0A%20%20Momentum%20Acceleration&body=Title%3A%20A%20Bias-Correction%20Decentralized%20Stochastic%20Gradient%20Algorithm%20with%0A%20%20Momentum%20Acceleration%0AAuthor%3A%20Yuchen%20Hu%20and%20Xi%20Chen%20and%20Weidong%20Liu%20and%20Xiaojun%20Mao%0AAbstract%3A%20%20%20Distributed%20stochastic%20optimization%20algorithms%20can%20simultaneously%20process%0Alarge-scale%20datasets%2C%20significantly%20accelerating%20model%20training.%20However%2C%20their%0Aeffectiveness%20is%20often%20hindered%20by%20the%20sparsity%20of%20distributed%20networks%20and%0Adata%20heterogeneity.%20In%20this%20paper%2C%20we%20propose%20a%20momentum-accelerated%0Adistributed%20stochastic%20gradient%20algorithm%2C%20termed%20Exact-Diffusion%20with%20Momentum%0A%28EDM%29%2C%20which%20mitigates%20the%20bias%20from%20data%20heterogeneity%20and%20incorporates%0Amomentum%20techniques%20commonly%20used%20in%20deep%20learning%20to%20enhance%20convergence%20rate.%0AOur%20theoretical%20analysis%20demonstrates%20that%20the%20EDM%20algorithm%20converges%0Asub-linearly%20to%20the%20neighborhood%20of%20the%20optimal%20solution%2C%20the%20radius%20of%20which%0Ais%20irrespective%20of%20data%20heterogeneity%2C%20when%20applied%20to%20non-convex%20objective%0Afunctions%3B%20under%20the%20Polyak-Lojasiewicz%20condition%2C%20which%20is%20a%20weaker%20assumption%0Athan%20strong%20convexity%2C%20it%20converges%20linearly%20to%20the%20target%20region.%20Our%20analysis%0Atechniques%20employed%20to%20handle%20momentum%20in%20complex%20distributed%20parameter%20update%0Astructures%20yield%20a%20sufficiently%20tight%20convergence%20upper%20bound%2C%20offering%20a%20new%0Aperspective%20for%20the%20theoretical%20analysis%20of%20other%20momentum-based%20distributed%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bias-Correction%2520Decentralized%2520Stochastic%2520Gradient%2520Algorithm%2520with%250A%2520%2520Momentum%2520Acceleration%26entry.906535625%3DYuchen%2520Hu%2520and%2520Xi%2520Chen%2520and%2520Weidong%2520Liu%2520and%2520Xiaojun%2520Mao%26entry.1292438233%3D%2520%2520Distributed%2520stochastic%2520optimization%2520algorithms%2520can%2520simultaneously%2520process%250Alarge-scale%2520datasets%252C%2520significantly%2520accelerating%2520model%2520training.%2520However%252C%2520their%250Aeffectiveness%2520is%2520often%2520hindered%2520by%2520the%2520sparsity%2520of%2520distributed%2520networks%2520and%250Adata%2520heterogeneity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520momentum-accelerated%250Adistributed%2520stochastic%2520gradient%2520algorithm%252C%2520termed%2520Exact-Diffusion%2520with%2520Momentum%250A%2528EDM%2529%252C%2520which%2520mitigates%2520the%2520bias%2520from%2520data%2520heterogeneity%2520and%2520incorporates%250Amomentum%2520techniques%2520commonly%2520used%2520in%2520deep%2520learning%2520to%2520enhance%2520convergence%2520rate.%250AOur%2520theoretical%2520analysis%2520demonstrates%2520that%2520the%2520EDM%2520algorithm%2520converges%250Asub-linearly%2520to%2520the%2520neighborhood%2520of%2520the%2520optimal%2520solution%252C%2520the%2520radius%2520of%2520which%250Ais%2520irrespective%2520of%2520data%2520heterogeneity%252C%2520when%2520applied%2520to%2520non-convex%2520objective%250Afunctions%253B%2520under%2520the%2520Polyak-Lojasiewicz%2520condition%252C%2520which%2520is%2520a%2520weaker%2520assumption%250Athan%2520strong%2520convexity%252C%2520it%2520converges%2520linearly%2520to%2520the%2520target%2520region.%2520Our%2520analysis%250Atechniques%2520employed%2520to%2520handle%2520momentum%2520in%2520complex%2520distributed%2520parameter%2520update%250Astructures%2520yield%2520a%2520sufficiently%2520tight%2520convergence%2520upper%2520bound%252C%2520offering%2520a%2520new%250Aperspective%2520for%2520the%2520theoretical%2520analysis%2520of%2520other%2520momentum-based%2520distributed%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bias-Correction%20Decentralized%20Stochastic%20Gradient%20Algorithm%20with%0A%20%20Momentum%20Acceleration&entry.906535625=Yuchen%20Hu%20and%20Xi%20Chen%20and%20Weidong%20Liu%20and%20Xiaojun%20Mao&entry.1292438233=%20%20Distributed%20stochastic%20optimization%20algorithms%20can%20simultaneously%20process%0Alarge-scale%20datasets%2C%20significantly%20accelerating%20model%20training.%20However%2C%20their%0Aeffectiveness%20is%20often%20hindered%20by%20the%20sparsity%20of%20distributed%20networks%20and%0Adata%20heterogeneity.%20In%20this%20paper%2C%20we%20propose%20a%20momentum-accelerated%0Adistributed%20stochastic%20gradient%20algorithm%2C%20termed%20Exact-Diffusion%20with%20Momentum%0A%28EDM%29%2C%20which%20mitigates%20the%20bias%20from%20data%20heterogeneity%20and%20incorporates%0Amomentum%20techniques%20commonly%20used%20in%20deep%20learning%20to%20enhance%20convergence%20rate.%0AOur%20theoretical%20analysis%20demonstrates%20that%20the%20EDM%20algorithm%20converges%0Asub-linearly%20to%20the%20neighborhood%20of%20the%20optimal%20solution%2C%20the%20radius%20of%20which%0Ais%20irrespective%20of%20data%20heterogeneity%2C%20when%20applied%20to%20non-convex%20objective%0Afunctions%3B%20under%20the%20Polyak-Lojasiewicz%20condition%2C%20which%20is%20a%20weaker%20assumption%0Athan%20strong%20convexity%2C%20it%20converges%20linearly%20to%20the%20target%20region.%20Our%20analysis%0Atechniques%20employed%20to%20handle%20momentum%20in%20complex%20distributed%20parameter%20update%0Astructures%20yield%20a%20sufficiently%20tight%20convergence%20upper%20bound%2C%20offering%20a%20new%0Aperspective%20for%20the%20theoretical%20analysis%20of%20other%20momentum-based%20distributed%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19082v2&entry.124074799=Read"},
{"title": "Rolling Ahead Diffusion for Traffic Scene Simulation", "author": "Yunpeng Liu and Matthew Niedoba and William Harvey and Adam Scibior and Berend Zwartsenberg and Frank Wood", "abstract": "  Realistic driving simulation requires that NPCs not only mimic natural\ndriving behaviors but also react to the behavior of other simulated agents.\nRecent developments in diffusion-based scenario generation focus on creating\ndiverse and realistic traffic scenarios by jointly modelling the motion of all\nthe agents in the scene. However, these traffic scenarios do not react when the\nmotion of agents deviates from their modelled trajectories. For example, the\nego-agent can be controlled by a stand along motion planner. To produce\nreactive scenarios with joint scenario models, the model must regenerate the\nscenario at each timestep based on new observations in a Model Predictive\nControl (MPC) fashion. Although reactive, this method is time-consuming, as one\ncomplete possible future for all NPCs is generated per simulation step.\nAlternatively, one can utilize an autoregressive model (AR) to predict only the\nimmediate next-step future for all NPCs. Although faster, this method lacks the\ncapability for advanced planning. We present a rolling diffusion based traffic\nscene generation model which mixes the benefits of both methods by predicting\nthe next step future and simultaneously predicting partially noised further\nfuture steps at the same time. We show that such model is efficient compared to\ndiffusion model based AR, achieving a beneficial compromise between reactivity\nand computational efficiency.\n", "link": "http://arxiv.org/abs/2502.09587v1", "date": "2025-02-13", "relevancy": 2.139, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5653}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5356}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rolling%20Ahead%20Diffusion%20for%20Traffic%20Scene%20Simulation&body=Title%3A%20Rolling%20Ahead%20Diffusion%20for%20Traffic%20Scene%20Simulation%0AAuthor%3A%20Yunpeng%20Liu%20and%20Matthew%20Niedoba%20and%20William%20Harvey%20and%20Adam%20Scibior%20and%20Berend%20Zwartsenberg%20and%20Frank%20Wood%0AAbstract%3A%20%20%20Realistic%20driving%20simulation%20requires%20that%20NPCs%20not%20only%20mimic%20natural%0Adriving%20behaviors%20but%20also%20react%20to%20the%20behavior%20of%20other%20simulated%20agents.%0ARecent%20developments%20in%20diffusion-based%20scenario%20generation%20focus%20on%20creating%0Adiverse%20and%20realistic%20traffic%20scenarios%20by%20jointly%20modelling%20the%20motion%20of%20all%0Athe%20agents%20in%20the%20scene.%20However%2C%20these%20traffic%20scenarios%20do%20not%20react%20when%20the%0Amotion%20of%20agents%20deviates%20from%20their%20modelled%20trajectories.%20For%20example%2C%20the%0Aego-agent%20can%20be%20controlled%20by%20a%20stand%20along%20motion%20planner.%20To%20produce%0Areactive%20scenarios%20with%20joint%20scenario%20models%2C%20the%20model%20must%20regenerate%20the%0Ascenario%20at%20each%20timestep%20based%20on%20new%20observations%20in%20a%20Model%20Predictive%0AControl%20%28MPC%29%20fashion.%20Although%20reactive%2C%20this%20method%20is%20time-consuming%2C%20as%20one%0Acomplete%20possible%20future%20for%20all%20NPCs%20is%20generated%20per%20simulation%20step.%0AAlternatively%2C%20one%20can%20utilize%20an%20autoregressive%20model%20%28AR%29%20to%20predict%20only%20the%0Aimmediate%20next-step%20future%20for%20all%20NPCs.%20Although%20faster%2C%20this%20method%20lacks%20the%0Acapability%20for%20advanced%20planning.%20We%20present%20a%20rolling%20diffusion%20based%20traffic%0Ascene%20generation%20model%20which%20mixes%20the%20benefits%20of%20both%20methods%20by%20predicting%0Athe%20next%20step%20future%20and%20simultaneously%20predicting%20partially%20noised%20further%0Afuture%20steps%20at%20the%20same%20time.%20We%20show%20that%20such%20model%20is%20efficient%20compared%20to%0Adiffusion%20model%20based%20AR%2C%20achieving%20a%20beneficial%20compromise%20between%20reactivity%0Aand%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRolling%2520Ahead%2520Diffusion%2520for%2520Traffic%2520Scene%2520Simulation%26entry.906535625%3DYunpeng%2520Liu%2520and%2520Matthew%2520Niedoba%2520and%2520William%2520Harvey%2520and%2520Adam%2520Scibior%2520and%2520Berend%2520Zwartsenberg%2520and%2520Frank%2520Wood%26entry.1292438233%3D%2520%2520Realistic%2520driving%2520simulation%2520requires%2520that%2520NPCs%2520not%2520only%2520mimic%2520natural%250Adriving%2520behaviors%2520but%2520also%2520react%2520to%2520the%2520behavior%2520of%2520other%2520simulated%2520agents.%250ARecent%2520developments%2520in%2520diffusion-based%2520scenario%2520generation%2520focus%2520on%2520creating%250Adiverse%2520and%2520realistic%2520traffic%2520scenarios%2520by%2520jointly%2520modelling%2520the%2520motion%2520of%2520all%250Athe%2520agents%2520in%2520the%2520scene.%2520However%252C%2520these%2520traffic%2520scenarios%2520do%2520not%2520react%2520when%2520the%250Amotion%2520of%2520agents%2520deviates%2520from%2520their%2520modelled%2520trajectories.%2520For%2520example%252C%2520the%250Aego-agent%2520can%2520be%2520controlled%2520by%2520a%2520stand%2520along%2520motion%2520planner.%2520To%2520produce%250Areactive%2520scenarios%2520with%2520joint%2520scenario%2520models%252C%2520the%2520model%2520must%2520regenerate%2520the%250Ascenario%2520at%2520each%2520timestep%2520based%2520on%2520new%2520observations%2520in%2520a%2520Model%2520Predictive%250AControl%2520%2528MPC%2529%2520fashion.%2520Although%2520reactive%252C%2520this%2520method%2520is%2520time-consuming%252C%2520as%2520one%250Acomplete%2520possible%2520future%2520for%2520all%2520NPCs%2520is%2520generated%2520per%2520simulation%2520step.%250AAlternatively%252C%2520one%2520can%2520utilize%2520an%2520autoregressive%2520model%2520%2528AR%2529%2520to%2520predict%2520only%2520the%250Aimmediate%2520next-step%2520future%2520for%2520all%2520NPCs.%2520Although%2520faster%252C%2520this%2520method%2520lacks%2520the%250Acapability%2520for%2520advanced%2520planning.%2520We%2520present%2520a%2520rolling%2520diffusion%2520based%2520traffic%250Ascene%2520generation%2520model%2520which%2520mixes%2520the%2520benefits%2520of%2520both%2520methods%2520by%2520predicting%250Athe%2520next%2520step%2520future%2520and%2520simultaneously%2520predicting%2520partially%2520noised%2520further%250Afuture%2520steps%2520at%2520the%2520same%2520time.%2520We%2520show%2520that%2520such%2520model%2520is%2520efficient%2520compared%2520to%250Adiffusion%2520model%2520based%2520AR%252C%2520achieving%2520a%2520beneficial%2520compromise%2520between%2520reactivity%250Aand%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rolling%20Ahead%20Diffusion%20for%20Traffic%20Scene%20Simulation&entry.906535625=Yunpeng%20Liu%20and%20Matthew%20Niedoba%20and%20William%20Harvey%20and%20Adam%20Scibior%20and%20Berend%20Zwartsenberg%20and%20Frank%20Wood&entry.1292438233=%20%20Realistic%20driving%20simulation%20requires%20that%20NPCs%20not%20only%20mimic%20natural%0Adriving%20behaviors%20but%20also%20react%20to%20the%20behavior%20of%20other%20simulated%20agents.%0ARecent%20developments%20in%20diffusion-based%20scenario%20generation%20focus%20on%20creating%0Adiverse%20and%20realistic%20traffic%20scenarios%20by%20jointly%20modelling%20the%20motion%20of%20all%0Athe%20agents%20in%20the%20scene.%20However%2C%20these%20traffic%20scenarios%20do%20not%20react%20when%20the%0Amotion%20of%20agents%20deviates%20from%20their%20modelled%20trajectories.%20For%20example%2C%20the%0Aego-agent%20can%20be%20controlled%20by%20a%20stand%20along%20motion%20planner.%20To%20produce%0Areactive%20scenarios%20with%20joint%20scenario%20models%2C%20the%20model%20must%20regenerate%20the%0Ascenario%20at%20each%20timestep%20based%20on%20new%20observations%20in%20a%20Model%20Predictive%0AControl%20%28MPC%29%20fashion.%20Although%20reactive%2C%20this%20method%20is%20time-consuming%2C%20as%20one%0Acomplete%20possible%20future%20for%20all%20NPCs%20is%20generated%20per%20simulation%20step.%0AAlternatively%2C%20one%20can%20utilize%20an%20autoregressive%20model%20%28AR%29%20to%20predict%20only%20the%0Aimmediate%20next-step%20future%20for%20all%20NPCs.%20Although%20faster%2C%20this%20method%20lacks%20the%0Acapability%20for%20advanced%20planning.%20We%20present%20a%20rolling%20diffusion%20based%20traffic%0Ascene%20generation%20model%20which%20mixes%20the%20benefits%20of%20both%20methods%20by%20predicting%0Athe%20next%20step%20future%20and%20simultaneously%20predicting%20partially%20noised%20further%0Afuture%20steps%20at%20the%20same%20time.%20We%20show%20that%20such%20model%20is%20efficient%20compared%20to%0Adiffusion%20model%20based%20AR%2C%20achieving%20a%20beneficial%20compromise%20between%20reactivity%0Aand%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09587v1&entry.124074799=Read"},
{"title": "On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors", "author": "Xiao Li and Hang Chen and Xiaolin Hu", "abstract": "  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n", "link": "http://arxiv.org/abs/2305.17438v2", "date": "2025-02-13", "relevancy": 2.1276, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5404}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5372}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Importance%20of%20Backbone%20to%20the%20Adversarial%20Robustness%20of%20Object%0A%20%20Detectors&body=Title%3A%20On%20the%20Importance%20of%20Backbone%20to%20the%20Adversarial%20Robustness%20of%20Object%0A%20%20Detectors%0AAuthor%3A%20Xiao%20Li%20and%20Hang%20Chen%20and%20Xiaolin%20Hu%0AAbstract%3A%20%20%20Object%20detection%20is%20a%20critical%20component%20of%20various%20security-sensitive%0Aapplications%2C%20such%20as%20autonomous%20driving%20and%20video%20surveillance.%20However%2C%0Aexisting%20object%20detectors%20are%20vulnerable%20to%20adversarial%20attacks%2C%20which%20poses%20a%0Asignificant%20challenge%20to%20their%20reliability%20and%20security.%20Through%20experiments%2C%0Afirst%2C%20we%20found%20that%20existing%20works%20on%20improving%20the%20adversarial%20robustness%20of%0Aobject%20detectors%20give%20a%20false%20sense%20of%20security.%20Second%2C%20we%20found%20that%0Aadversarially%20pre-trained%20backbone%20networks%20were%20essential%20for%20enhancing%20the%0Aadversarial%20robustness%20of%20object%20detectors.%20We%20then%20proposed%20a%20simple%20yet%0Aeffective%20recipe%20for%20fast%20adversarial%20fine-tuning%20on%20object%20detectors%20with%0Aadversarially%20pre-trained%20backbones.%20Without%20any%20modifications%20to%20the%20structure%0Aof%20object%20detectors%2C%20our%20recipe%20achieved%20significantly%20better%20adversarial%0Arobustness%20than%20previous%20works.%20Finally%2C%20we%20explored%20the%20potential%20of%20different%0Amodern%20object%20detector%20designs%20for%20improving%20adversarial%20robustness%20with%20our%0Arecipe%20and%20demonstrated%20interesting%20findings%2C%20which%20inspired%20us%20to%20design%0Astate-of-the-art%20%28SOTA%29%20robust%20detectors.%20Our%20empirical%20results%20set%20a%20new%0Amilestone%20for%20adversarially%20robust%20object%20detection.%20Code%20and%20trained%0Acheckpoints%20are%20available%20at%20https%3A//github.com/thu-ml/oddefense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Importance%2520of%2520Backbone%2520to%2520the%2520Adversarial%2520Robustness%2520of%2520Object%250A%2520%2520Detectors%26entry.906535625%3DXiao%2520Li%2520and%2520Hang%2520Chen%2520and%2520Xiaolin%2520Hu%26entry.1292438233%3D%2520%2520Object%2520detection%2520is%2520a%2520critical%2520component%2520of%2520various%2520security-sensitive%250Aapplications%252C%2520such%2520as%2520autonomous%2520driving%2520and%2520video%2520surveillance.%2520However%252C%250Aexisting%2520object%2520detectors%2520are%2520vulnerable%2520to%2520adversarial%2520attacks%252C%2520which%2520poses%2520a%250Asignificant%2520challenge%2520to%2520their%2520reliability%2520and%2520security.%2520Through%2520experiments%252C%250Afirst%252C%2520we%2520found%2520that%2520existing%2520works%2520on%2520improving%2520the%2520adversarial%2520robustness%2520of%250Aobject%2520detectors%2520give%2520a%2520false%2520sense%2520of%2520security.%2520Second%252C%2520we%2520found%2520that%250Aadversarially%2520pre-trained%2520backbone%2520networks%2520were%2520essential%2520for%2520enhancing%2520the%250Aadversarial%2520robustness%2520of%2520object%2520detectors.%2520We%2520then%2520proposed%2520a%2520simple%2520yet%250Aeffective%2520recipe%2520for%2520fast%2520adversarial%2520fine-tuning%2520on%2520object%2520detectors%2520with%250Aadversarially%2520pre-trained%2520backbones.%2520Without%2520any%2520modifications%2520to%2520the%2520structure%250Aof%2520object%2520detectors%252C%2520our%2520recipe%2520achieved%2520significantly%2520better%2520adversarial%250Arobustness%2520than%2520previous%2520works.%2520Finally%252C%2520we%2520explored%2520the%2520potential%2520of%2520different%250Amodern%2520object%2520detector%2520designs%2520for%2520improving%2520adversarial%2520robustness%2520with%2520our%250Arecipe%2520and%2520demonstrated%2520interesting%2520findings%252C%2520which%2520inspired%2520us%2520to%2520design%250Astate-of-the-art%2520%2528SOTA%2529%2520robust%2520detectors.%2520Our%2520empirical%2520results%2520set%2520a%2520new%250Amilestone%2520for%2520adversarially%2520robust%2520object%2520detection.%2520Code%2520and%2520trained%250Acheckpoints%2520are%2520available%2520at%2520https%253A//github.com/thu-ml/oddefense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.17438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Importance%20of%20Backbone%20to%20the%20Adversarial%20Robustness%20of%20Object%0A%20%20Detectors&entry.906535625=Xiao%20Li%20and%20Hang%20Chen%20and%20Xiaolin%20Hu&entry.1292438233=%20%20Object%20detection%20is%20a%20critical%20component%20of%20various%20security-sensitive%0Aapplications%2C%20such%20as%20autonomous%20driving%20and%20video%20surveillance.%20However%2C%0Aexisting%20object%20detectors%20are%20vulnerable%20to%20adversarial%20attacks%2C%20which%20poses%20a%0Asignificant%20challenge%20to%20their%20reliability%20and%20security.%20Through%20experiments%2C%0Afirst%2C%20we%20found%20that%20existing%20works%20on%20improving%20the%20adversarial%20robustness%20of%0Aobject%20detectors%20give%20a%20false%20sense%20of%20security.%20Second%2C%20we%20found%20that%0Aadversarially%20pre-trained%20backbone%20networks%20were%20essential%20for%20enhancing%20the%0Aadversarial%20robustness%20of%20object%20detectors.%20We%20then%20proposed%20a%20simple%20yet%0Aeffective%20recipe%20for%20fast%20adversarial%20fine-tuning%20on%20object%20detectors%20with%0Aadversarially%20pre-trained%20backbones.%20Without%20any%20modifications%20to%20the%20structure%0Aof%20object%20detectors%2C%20our%20recipe%20achieved%20significantly%20better%20adversarial%0Arobustness%20than%20previous%20works.%20Finally%2C%20we%20explored%20the%20potential%20of%20different%0Amodern%20object%20detector%20designs%20for%20improving%20adversarial%20robustness%20with%20our%0Arecipe%20and%20demonstrated%20interesting%20findings%2C%20which%20inspired%20us%20to%20design%0Astate-of-the-art%20%28SOTA%29%20robust%20detectors.%20Our%20empirical%20results%20set%20a%20new%0Amilestone%20for%20adversarially%20robust%20object%20detection.%20Code%20and%20trained%0Acheckpoints%20are%20available%20at%20https%3A//github.com/thu-ml/oddefense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17438v2&entry.124074799=Read"},
{"title": "Evaluating Zero-Shot Long-Context LLM Compression", "author": "Chenyu Wang and Yihan Wang and Kai Li", "abstract": "  This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.\n", "link": "http://arxiv.org/abs/2406.06773v2", "date": "2025-02-13", "relevancy": 2.114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Zero-Shot%20Long-Context%20LLM%20Compression&body=Title%3A%20Evaluating%20Zero-Shot%20Long-Context%20LLM%20Compression%0AAuthor%3A%20Chenyu%20Wang%20and%20Yihan%20Wang%20and%20Kai%20Li%0AAbstract%3A%20%20%20This%20study%20evaluates%20the%20effectiveness%20of%20zero-shot%20compression%20techniques%20on%0Alarge%20language%20models%20%28LLMs%29%20under%20long-context.%20We%20identify%20the%20tendency%20for%0Acomputational%20errors%20to%20increase%20under%20long-context%20when%20employing%20certain%0Acompression%20methods.%20We%20propose%20a%20hypothesis%20to%20explain%20the%20varied%20behavior%20of%0Adifferent%20LLM%20compression%20techniques%20and%20explore%20remedies%20to%20mitigate%20the%0Aperformance%20decline%20observed%20in%20some%20techniques%20under%20long-context.%20This%20is%20a%0Acourse%20report%20for%20COS%20598D%20Machine%20Learning%20and%20Systems%20by%20Prof.%20Kai%20Li%20at%0APrinceton%20University.%20Due%20to%20limited%20computational%20resources%2C%20our%20experiments%0Awere%20conducted%20only%20on%20LLaMA-2-7B-32K.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Zero-Shot%2520Long-Context%2520LLM%2520Compression%26entry.906535625%3DChenyu%2520Wang%2520and%2520Yihan%2520Wang%2520and%2520Kai%2520Li%26entry.1292438233%3D%2520%2520This%2520study%2520evaluates%2520the%2520effectiveness%2520of%2520zero-shot%2520compression%2520techniques%2520on%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520under%2520long-context.%2520We%2520identify%2520the%2520tendency%2520for%250Acomputational%2520errors%2520to%2520increase%2520under%2520long-context%2520when%2520employing%2520certain%250Acompression%2520methods.%2520We%2520propose%2520a%2520hypothesis%2520to%2520explain%2520the%2520varied%2520behavior%2520of%250Adifferent%2520LLM%2520compression%2520techniques%2520and%2520explore%2520remedies%2520to%2520mitigate%2520the%250Aperformance%2520decline%2520observed%2520in%2520some%2520techniques%2520under%2520long-context.%2520This%2520is%2520a%250Acourse%2520report%2520for%2520COS%2520598D%2520Machine%2520Learning%2520and%2520Systems%2520by%2520Prof.%2520Kai%2520Li%2520at%250APrinceton%2520University.%2520Due%2520to%2520limited%2520computational%2520resources%252C%2520our%2520experiments%250Awere%2520conducted%2520only%2520on%2520LLaMA-2-7B-32K.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Zero-Shot%20Long-Context%20LLM%20Compression&entry.906535625=Chenyu%20Wang%20and%20Yihan%20Wang%20and%20Kai%20Li&entry.1292438233=%20%20This%20study%20evaluates%20the%20effectiveness%20of%20zero-shot%20compression%20techniques%20on%0Alarge%20language%20models%20%28LLMs%29%20under%20long-context.%20We%20identify%20the%20tendency%20for%0Acomputational%20errors%20to%20increase%20under%20long-context%20when%20employing%20certain%0Acompression%20methods.%20We%20propose%20a%20hypothesis%20to%20explain%20the%20varied%20behavior%20of%0Adifferent%20LLM%20compression%20techniques%20and%20explore%20remedies%20to%20mitigate%20the%0Aperformance%20decline%20observed%20in%20some%20techniques%20under%20long-context.%20This%20is%20a%0Acourse%20report%20for%20COS%20598D%20Machine%20Learning%20and%20Systems%20by%20Prof.%20Kai%20Li%20at%0APrinceton%20University.%20Due%20to%20limited%20computational%20resources%2C%20our%20experiments%0Awere%20conducted%20only%20on%20LLaMA-2-7B-32K.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06773v2&entry.124074799=Read"},
{"title": "Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering", "author": "Mark Beliaev and Victor Yang and Madhura Raju and Jiachen Sun and Xinghai Hu", "abstract": "  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n", "link": "http://arxiv.org/abs/2502.09573v1", "date": "2025-02-13", "relevancy": 2.096, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5416}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5209}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20GPT%20for%20Video%20Understanding%3A%20Zero-Shot%20Performance%20and%20Prompt%0A%20%20Engineering&body=Title%3A%20Optimizing%20GPT%20for%20Video%20Understanding%3A%20Zero-Shot%20Performance%20and%20Prompt%0A%20%20Engineering%0AAuthor%3A%20Mark%20Beliaev%20and%20Victor%20Yang%20and%20Madhura%20Raju%20and%20Jiachen%20Sun%20and%20Xinghai%20Hu%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20tackle%20industry%20challenges%20in%20video%20content%20classification%0Aby%20exploring%20and%20optimizing%20GPT-based%20models%20for%20zero-shot%20classification%0Aacross%20seven%20critical%20categories%20of%20video%20quality.%20We%20contribute%20a%20novel%0Aapproach%20to%20improving%20GPT%27s%20performance%20through%20prompt%20optimization%20and%20policy%0Arefinement%2C%20demonstrating%20that%20simplifying%20complex%20policies%20significantly%0Areduces%20false%20negatives.%20Additionally%2C%20we%20introduce%20a%20new%0Adecomposition-aggregation-based%20prompt%20engineering%20technique%2C%20which%20outperforms%0Atraditional%20single-prompt%20methods.%20These%20experiments%2C%20conducted%20on%20real%0Aindustry%20problems%2C%20show%20that%20thoughtful%20prompt%20design%20can%20substantially%20enhance%0AGPT%27s%20performance%20without%20additional%20finetuning%2C%20offering%20an%20effective%20and%0Ascalable%20solution%20for%20improving%20video%20classification%20systems%20across%20various%0Adomains%20in%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520GPT%2520for%2520Video%2520Understanding%253A%2520Zero-Shot%2520Performance%2520and%2520Prompt%250A%2520%2520Engineering%26entry.906535625%3DMark%2520Beliaev%2520and%2520Victor%2520Yang%2520and%2520Madhura%2520Raju%2520and%2520Jiachen%2520Sun%2520and%2520Xinghai%2520Hu%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520tackle%2520industry%2520challenges%2520in%2520video%2520content%2520classification%250Aby%2520exploring%2520and%2520optimizing%2520GPT-based%2520models%2520for%2520zero-shot%2520classification%250Aacross%2520seven%2520critical%2520categories%2520of%2520video%2520quality.%2520We%2520contribute%2520a%2520novel%250Aapproach%2520to%2520improving%2520GPT%2527s%2520performance%2520through%2520prompt%2520optimization%2520and%2520policy%250Arefinement%252C%2520demonstrating%2520that%2520simplifying%2520complex%2520policies%2520significantly%250Areduces%2520false%2520negatives.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%250Adecomposition-aggregation-based%2520prompt%2520engineering%2520technique%252C%2520which%2520outperforms%250Atraditional%2520single-prompt%2520methods.%2520These%2520experiments%252C%2520conducted%2520on%2520real%250Aindustry%2520problems%252C%2520show%2520that%2520thoughtful%2520prompt%2520design%2520can%2520substantially%2520enhance%250AGPT%2527s%2520performance%2520without%2520additional%2520finetuning%252C%2520offering%2520an%2520effective%2520and%250Ascalable%2520solution%2520for%2520improving%2520video%2520classification%2520systems%2520across%2520various%250Adomains%2520in%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20GPT%20for%20Video%20Understanding%3A%20Zero-Shot%20Performance%20and%20Prompt%0A%20%20Engineering&entry.906535625=Mark%20Beliaev%20and%20Victor%20Yang%20and%20Madhura%20Raju%20and%20Jiachen%20Sun%20and%20Xinghai%20Hu&entry.1292438233=%20%20In%20this%20study%2C%20we%20tackle%20industry%20challenges%20in%20video%20content%20classification%0Aby%20exploring%20and%20optimizing%20GPT-based%20models%20for%20zero-shot%20classification%0Aacross%20seven%20critical%20categories%20of%20video%20quality.%20We%20contribute%20a%20novel%0Aapproach%20to%20improving%20GPT%27s%20performance%20through%20prompt%20optimization%20and%20policy%0Arefinement%2C%20demonstrating%20that%20simplifying%20complex%20policies%20significantly%0Areduces%20false%20negatives.%20Additionally%2C%20we%20introduce%20a%20new%0Adecomposition-aggregation-based%20prompt%20engineering%20technique%2C%20which%20outperforms%0Atraditional%20single-prompt%20methods.%20These%20experiments%2C%20conducted%20on%20real%0Aindustry%20problems%2C%20show%20that%20thoughtful%20prompt%20design%20can%20substantially%20enhance%0AGPT%27s%20performance%20without%20additional%20finetuning%2C%20offering%20an%20effective%20and%0Ascalable%20solution%20for%20improving%20video%20classification%20systems%20across%20various%0Adomains%20in%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09573v1&entry.124074799=Read"},
{"title": "SteROI-D: System Design and Mapping for Stereo Depth Inference on\n  Regions of Interest", "author": "Jack Erhardt and Ziang Li and Reid Pinkham and Andrew Berkovich and Zhengya Zhang", "abstract": "  Machine learning algorithms have enabled high quality stereo depth estimation\nto run on Augmented and Virtual Reality (AR/VR) devices. However, high energy\nconsumption across the full image processing stack prevents stereo depth\nalgorithms from running effectively on battery-limited devices. This paper\nintroduces SteROI-D, a full stereo depth system paired with a mapping\nmethodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity\nat the system level to save energy. SteROI-D's flexible and heterogeneous\ncompute fabric supports diverse ROIs. Importantly, we introduce a systematic\nmapping methodology to effectively handle dynamic ROIs, thereby maximizing\nenergy savings. Using these techniques, our 28nm prototype SteROI-D design\nachieves up to 4.35x reduction in total system energy compared to a baseline\nASIC.\n", "link": "http://arxiv.org/abs/2502.09528v1", "date": "2025-02-13", "relevancy": 2.0928, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5223}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SteROI-D%3A%20System%20Design%20and%20Mapping%20for%20Stereo%20Depth%20Inference%20on%0A%20%20Regions%20of%20Interest&body=Title%3A%20SteROI-D%3A%20System%20Design%20and%20Mapping%20for%20Stereo%20Depth%20Inference%20on%0A%20%20Regions%20of%20Interest%0AAuthor%3A%20Jack%20Erhardt%20and%20Ziang%20Li%20and%20Reid%20Pinkham%20and%20Andrew%20Berkovich%20and%20Zhengya%20Zhang%0AAbstract%3A%20%20%20Machine%20learning%20algorithms%20have%20enabled%20high%20quality%20stereo%20depth%20estimation%0Ato%20run%20on%20Augmented%20and%20Virtual%20Reality%20%28AR/VR%29%20devices.%20However%2C%20high%20energy%0Aconsumption%20across%20the%20full%20image%20processing%20stack%20prevents%20stereo%20depth%0Aalgorithms%20from%20running%20effectively%20on%20battery-limited%20devices.%20This%20paper%0Aintroduces%20SteROI-D%2C%20a%20full%20stereo%20depth%20system%20paired%20with%20a%20mapping%0Amethodology.%20SteROI-D%20exploits%20Region-of-Interest%20%28ROI%29%20and%20temporal%20sparsity%0Aat%20the%20system%20level%20to%20save%20energy.%20SteROI-D%27s%20flexible%20and%20heterogeneous%0Acompute%20fabric%20supports%20diverse%20ROIs.%20Importantly%2C%20we%20introduce%20a%20systematic%0Amapping%20methodology%20to%20effectively%20handle%20dynamic%20ROIs%2C%20thereby%20maximizing%0Aenergy%20savings.%20Using%20these%20techniques%2C%20our%2028nm%20prototype%20SteROI-D%20design%0Aachieves%20up%20to%204.35x%20reduction%20in%20total%20system%20energy%20compared%20to%20a%20baseline%0AASIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteROI-D%253A%2520System%2520Design%2520and%2520Mapping%2520for%2520Stereo%2520Depth%2520Inference%2520on%250A%2520%2520Regions%2520of%2520Interest%26entry.906535625%3DJack%2520Erhardt%2520and%2520Ziang%2520Li%2520and%2520Reid%2520Pinkham%2520and%2520Andrew%2520Berkovich%2520and%2520Zhengya%2520Zhang%26entry.1292438233%3D%2520%2520Machine%2520learning%2520algorithms%2520have%2520enabled%2520high%2520quality%2520stereo%2520depth%2520estimation%250Ato%2520run%2520on%2520Augmented%2520and%2520Virtual%2520Reality%2520%2528AR/VR%2529%2520devices.%2520However%252C%2520high%2520energy%250Aconsumption%2520across%2520the%2520full%2520image%2520processing%2520stack%2520prevents%2520stereo%2520depth%250Aalgorithms%2520from%2520running%2520effectively%2520on%2520battery-limited%2520devices.%2520This%2520paper%250Aintroduces%2520SteROI-D%252C%2520a%2520full%2520stereo%2520depth%2520system%2520paired%2520with%2520a%2520mapping%250Amethodology.%2520SteROI-D%2520exploits%2520Region-of-Interest%2520%2528ROI%2529%2520and%2520temporal%2520sparsity%250Aat%2520the%2520system%2520level%2520to%2520save%2520energy.%2520SteROI-D%2527s%2520flexible%2520and%2520heterogeneous%250Acompute%2520fabric%2520supports%2520diverse%2520ROIs.%2520Importantly%252C%2520we%2520introduce%2520a%2520systematic%250Amapping%2520methodology%2520to%2520effectively%2520handle%2520dynamic%2520ROIs%252C%2520thereby%2520maximizing%250Aenergy%2520savings.%2520Using%2520these%2520techniques%252C%2520our%252028nm%2520prototype%2520SteROI-D%2520design%250Aachieves%2520up%2520to%25204.35x%2520reduction%2520in%2520total%2520system%2520energy%2520compared%2520to%2520a%2520baseline%250AASIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SteROI-D%3A%20System%20Design%20and%20Mapping%20for%20Stereo%20Depth%20Inference%20on%0A%20%20Regions%20of%20Interest&entry.906535625=Jack%20Erhardt%20and%20Ziang%20Li%20and%20Reid%20Pinkham%20and%20Andrew%20Berkovich%20and%20Zhengya%20Zhang&entry.1292438233=%20%20Machine%20learning%20algorithms%20have%20enabled%20high%20quality%20stereo%20depth%20estimation%0Ato%20run%20on%20Augmented%20and%20Virtual%20Reality%20%28AR/VR%29%20devices.%20However%2C%20high%20energy%0Aconsumption%20across%20the%20full%20image%20processing%20stack%20prevents%20stereo%20depth%0Aalgorithms%20from%20running%20effectively%20on%20battery-limited%20devices.%20This%20paper%0Aintroduces%20SteROI-D%2C%20a%20full%20stereo%20depth%20system%20paired%20with%20a%20mapping%0Amethodology.%20SteROI-D%20exploits%20Region-of-Interest%20%28ROI%29%20and%20temporal%20sparsity%0Aat%20the%20system%20level%20to%20save%20energy.%20SteROI-D%27s%20flexible%20and%20heterogeneous%0Acompute%20fabric%20supports%20diverse%20ROIs.%20Importantly%2C%20we%20introduce%20a%20systematic%0Amapping%20methodology%20to%20effectively%20handle%20dynamic%20ROIs%2C%20thereby%20maximizing%0Aenergy%20savings.%20Using%20these%20techniques%2C%20our%2028nm%20prototype%20SteROI-D%20design%0Aachieves%20up%20to%204.35x%20reduction%20in%20total%20system%20energy%20compared%20to%20a%20baseline%0AASIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09528v1&entry.124074799=Read"},
{"title": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models", "author": "Daniel Fleischer and Moshe Berchansky and Gad Markovits and Moshe Wasserblat", "abstract": "  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n", "link": "http://arxiv.org/abs/2502.09390v1", "date": "2025-02-13", "relevancy": 2.0826, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SQuARE%3A%20Sequential%20Question%20Answering%20Reasoning%20Engine%20for%20Enhanced%0A%20%20Chain-of-Thought%20in%20Large%20Language%20Models&body=Title%3A%20SQuARE%3A%20Sequential%20Question%20Answering%20Reasoning%20Engine%20for%20Enhanced%0A%20%20Chain-of-Thought%20in%20Large%20Language%20Models%0AAuthor%3A%20Daniel%20Fleischer%20and%20Moshe%20Berchansky%20and%20Gad%20Markovits%20and%20Moshe%20Wasserblat%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20Natural%20Language%20Processing%2C%20Large%20Language%0AModels%20%28LLMs%29%20are%20tasked%20with%20increasingly%20complex%20reasoning%20challenges.%0ATraditional%20methods%20like%20chain-of-thought%20prompting%20have%20shown%20promise%20but%0Aoften%20fall%20short%20in%20fully%20leveraging%20a%20model%27s%20reasoning%20capabilities.%20This%0Apaper%20introduces%20SQuARE%20%28Sequential%20Question%20Answering%20Reasoning%20Engine%29%2C%20a%0Anovel%20prompting%20technique%20designed%20to%20improve%20reasoning%20through%20a%0Aself-interrogation%20paradigm.%20Building%20upon%20CoT%20frameworks%2C%20SQuARE%20prompts%0Amodels%20to%20generate%20and%20resolve%20multiple%20auxiliary%20questions%20before%20tackling%20the%0Amain%20query%2C%20promoting%20a%20more%20thorough%20exploration%20of%20various%20aspects%20of%20a%0Atopic.%20Our%20expansive%20evaluations%2C%20conducted%20with%20Llama%203%20and%20GPT-4o%20models%0Aacross%20multiple%20question-answering%20datasets%2C%20demonstrate%20that%20SQuARE%0Asignificantly%20surpasses%20traditional%20CoT%20prompts%20and%20existing%0Arephrase-and-respond%20methods.%20By%20systematically%20decomposing%20queries%2C%20SQuARE%0Aadvances%20LLM%20capabilities%20in%20reasoning%20tasks.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/IntelLabs/RAG-FiT/tree/square.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSQuARE%253A%2520Sequential%2520Question%2520Answering%2520Reasoning%2520Engine%2520for%2520Enhanced%250A%2520%2520Chain-of-Thought%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DDaniel%2520Fleischer%2520and%2520Moshe%2520Berchansky%2520and%2520Gad%2520Markovits%2520and%2520Moshe%2520Wasserblat%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520Natural%2520Language%2520Processing%252C%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520are%2520tasked%2520with%2520increasingly%2520complex%2520reasoning%2520challenges.%250ATraditional%2520methods%2520like%2520chain-of-thought%2520prompting%2520have%2520shown%2520promise%2520but%250Aoften%2520fall%2520short%2520in%2520fully%2520leveraging%2520a%2520model%2527s%2520reasoning%2520capabilities.%2520This%250Apaper%2520introduces%2520SQuARE%2520%2528Sequential%2520Question%2520Answering%2520Reasoning%2520Engine%2529%252C%2520a%250Anovel%2520prompting%2520technique%2520designed%2520to%2520improve%2520reasoning%2520through%2520a%250Aself-interrogation%2520paradigm.%2520Building%2520upon%2520CoT%2520frameworks%252C%2520SQuARE%2520prompts%250Amodels%2520to%2520generate%2520and%2520resolve%2520multiple%2520auxiliary%2520questions%2520before%2520tackling%2520the%250Amain%2520query%252C%2520promoting%2520a%2520more%2520thorough%2520exploration%2520of%2520various%2520aspects%2520of%2520a%250Atopic.%2520Our%2520expansive%2520evaluations%252C%2520conducted%2520with%2520Llama%25203%2520and%2520GPT-4o%2520models%250Aacross%2520multiple%2520question-answering%2520datasets%252C%2520demonstrate%2520that%2520SQuARE%250Asignificantly%2520surpasses%2520traditional%2520CoT%2520prompts%2520and%2520existing%250Arephrase-and-respond%2520methods.%2520By%2520systematically%2520decomposing%2520queries%252C%2520SQuARE%250Aadvances%2520LLM%2520capabilities%2520in%2520reasoning%2520tasks.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/IntelLabs/RAG-FiT/tree/square.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SQuARE%3A%20Sequential%20Question%20Answering%20Reasoning%20Engine%20for%20Enhanced%0A%20%20Chain-of-Thought%20in%20Large%20Language%20Models&entry.906535625=Daniel%20Fleischer%20and%20Moshe%20Berchansky%20and%20Gad%20Markovits%20and%20Moshe%20Wasserblat&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20Natural%20Language%20Processing%2C%20Large%20Language%0AModels%20%28LLMs%29%20are%20tasked%20with%20increasingly%20complex%20reasoning%20challenges.%0ATraditional%20methods%20like%20chain-of-thought%20prompting%20have%20shown%20promise%20but%0Aoften%20fall%20short%20in%20fully%20leveraging%20a%20model%27s%20reasoning%20capabilities.%20This%0Apaper%20introduces%20SQuARE%20%28Sequential%20Question%20Answering%20Reasoning%20Engine%29%2C%20a%0Anovel%20prompting%20technique%20designed%20to%20improve%20reasoning%20through%20a%0Aself-interrogation%20paradigm.%20Building%20upon%20CoT%20frameworks%2C%20SQuARE%20prompts%0Amodels%20to%20generate%20and%20resolve%20multiple%20auxiliary%20questions%20before%20tackling%20the%0Amain%20query%2C%20promoting%20a%20more%20thorough%20exploration%20of%20various%20aspects%20of%20a%0Atopic.%20Our%20expansive%20evaluations%2C%20conducted%20with%20Llama%203%20and%20GPT-4o%20models%0Aacross%20multiple%20question-answering%20datasets%2C%20demonstrate%20that%20SQuARE%0Asignificantly%20surpasses%20traditional%20CoT%20prompts%20and%20existing%0Arephrase-and-respond%20methods.%20By%20systematically%20decomposing%20queries%2C%20SQuARE%0Aadvances%20LLM%20capabilities%20in%20reasoning%20tasks.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/IntelLabs/RAG-FiT/tree/square.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09390v1&entry.124074799=Read"},
{"title": "Diffusion Models for Molecules: A Survey of Methods and Tasks", "author": "Liang Wang and Chao Song and Zhiyuan Liu and Yu Rong and Qiang Liu and Shu Wu and Liang Wang", "abstract": "  Generative tasks about molecules, including but not limited to molecule\ngeneration, are crucial for drug discovery and material design, and have\nconsistently attracted significant attention. In recent years, diffusion models\nhave emerged as an impressive class of deep generative models, sparking\nextensive research and leading to numerous studies on their application to\nmolecular generative tasks. Despite the proliferation of related work, there\nremains a notable lack of up-to-date and systematic surveys in this area.\nParticularly, due to the diversity of diffusion model formulations, molecular\ndata modalities, and generative task types, the research landscape is\nchallenging to navigate, hindering understanding and limiting the area's\ngrowth. To address this, this paper conducts a comprehensive survey of\ndiffusion model-based molecular generative methods. We systematically review\nthe research from the perspectives of methodological formulations, data\nmodalities, and task types, offering a novel taxonomy. This survey aims to\nfacilitate understanding and further flourishing development in this area. The\nrelevant papers are summarized at:\nhttps://github.com/AzureLeon1/awesome-molecular-diffusion-models.\n", "link": "http://arxiv.org/abs/2502.09511v1", "date": "2025-02-13", "relevancy": 2.0787, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.54}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5192}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Models%20for%20Molecules%3A%20A%20Survey%20of%20Methods%20and%20Tasks&body=Title%3A%20Diffusion%20Models%20for%20Molecules%3A%20A%20Survey%20of%20Methods%20and%20Tasks%0AAuthor%3A%20Liang%20Wang%20and%20Chao%20Song%20and%20Zhiyuan%20Liu%20and%20Yu%20Rong%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Generative%20tasks%20about%20molecules%2C%20including%20but%20not%20limited%20to%20molecule%0Ageneration%2C%20are%20crucial%20for%20drug%20discovery%20and%20material%20design%2C%20and%20have%0Aconsistently%20attracted%20significant%20attention.%20In%20recent%20years%2C%20diffusion%20models%0Ahave%20emerged%20as%20an%20impressive%20class%20of%20deep%20generative%20models%2C%20sparking%0Aextensive%20research%20and%20leading%20to%20numerous%20studies%20on%20their%20application%20to%0Amolecular%20generative%20tasks.%20Despite%20the%20proliferation%20of%20related%20work%2C%20there%0Aremains%20a%20notable%20lack%20of%20up-to-date%20and%20systematic%20surveys%20in%20this%20area.%0AParticularly%2C%20due%20to%20the%20diversity%20of%20diffusion%20model%20formulations%2C%20molecular%0Adata%20modalities%2C%20and%20generative%20task%20types%2C%20the%20research%20landscape%20is%0Achallenging%20to%20navigate%2C%20hindering%20understanding%20and%20limiting%20the%20area%27s%0Agrowth.%20To%20address%20this%2C%20this%20paper%20conducts%20a%20comprehensive%20survey%20of%0Adiffusion%20model-based%20molecular%20generative%20methods.%20We%20systematically%20review%0Athe%20research%20from%20the%20perspectives%20of%20methodological%20formulations%2C%20data%0Amodalities%2C%20and%20task%20types%2C%20offering%20a%20novel%20taxonomy.%20This%20survey%20aims%20to%0Afacilitate%20understanding%20and%20further%20flourishing%20development%20in%20this%20area.%20The%0Arelevant%20papers%20are%20summarized%20at%3A%0Ahttps%3A//github.com/AzureLeon1/awesome-molecular-diffusion-models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Models%2520for%2520Molecules%253A%2520A%2520Survey%2520of%2520Methods%2520and%2520Tasks%26entry.906535625%3DLiang%2520Wang%2520and%2520Chao%2520Song%2520and%2520Zhiyuan%2520Liu%2520and%2520Yu%2520Rong%2520and%2520Qiang%2520Liu%2520and%2520Shu%2520Wu%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Generative%2520tasks%2520about%2520molecules%252C%2520including%2520but%2520not%2520limited%2520to%2520molecule%250Ageneration%252C%2520are%2520crucial%2520for%2520drug%2520discovery%2520and%2520material%2520design%252C%2520and%2520have%250Aconsistently%2520attracted%2520significant%2520attention.%2520In%2520recent%2520years%252C%2520diffusion%2520models%250Ahave%2520emerged%2520as%2520an%2520impressive%2520class%2520of%2520deep%2520generative%2520models%252C%2520sparking%250Aextensive%2520research%2520and%2520leading%2520to%2520numerous%2520studies%2520on%2520their%2520application%2520to%250Amolecular%2520generative%2520tasks.%2520Despite%2520the%2520proliferation%2520of%2520related%2520work%252C%2520there%250Aremains%2520a%2520notable%2520lack%2520of%2520up-to-date%2520and%2520systematic%2520surveys%2520in%2520this%2520area.%250AParticularly%252C%2520due%2520to%2520the%2520diversity%2520of%2520diffusion%2520model%2520formulations%252C%2520molecular%250Adata%2520modalities%252C%2520and%2520generative%2520task%2520types%252C%2520the%2520research%2520landscape%2520is%250Achallenging%2520to%2520navigate%252C%2520hindering%2520understanding%2520and%2520limiting%2520the%2520area%2527s%250Agrowth.%2520To%2520address%2520this%252C%2520this%2520paper%2520conducts%2520a%2520comprehensive%2520survey%2520of%250Adiffusion%2520model-based%2520molecular%2520generative%2520methods.%2520We%2520systematically%2520review%250Athe%2520research%2520from%2520the%2520perspectives%2520of%2520methodological%2520formulations%252C%2520data%250Amodalities%252C%2520and%2520task%2520types%252C%2520offering%2520a%2520novel%2520taxonomy.%2520This%2520survey%2520aims%2520to%250Afacilitate%2520understanding%2520and%2520further%2520flourishing%2520development%2520in%2520this%2520area.%2520The%250Arelevant%2520papers%2520are%2520summarized%2520at%253A%250Ahttps%253A//github.com/AzureLeon1/awesome-molecular-diffusion-models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Models%20for%20Molecules%3A%20A%20Survey%20of%20Methods%20and%20Tasks&entry.906535625=Liang%20Wang%20and%20Chao%20Song%20and%20Zhiyuan%20Liu%20and%20Yu%20Rong%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang&entry.1292438233=%20%20Generative%20tasks%20about%20molecules%2C%20including%20but%20not%20limited%20to%20molecule%0Ageneration%2C%20are%20crucial%20for%20drug%20discovery%20and%20material%20design%2C%20and%20have%0Aconsistently%20attracted%20significant%20attention.%20In%20recent%20years%2C%20diffusion%20models%0Ahave%20emerged%20as%20an%20impressive%20class%20of%20deep%20generative%20models%2C%20sparking%0Aextensive%20research%20and%20leading%20to%20numerous%20studies%20on%20their%20application%20to%0Amolecular%20generative%20tasks.%20Despite%20the%20proliferation%20of%20related%20work%2C%20there%0Aremains%20a%20notable%20lack%20of%20up-to-date%20and%20systematic%20surveys%20in%20this%20area.%0AParticularly%2C%20due%20to%20the%20diversity%20of%20diffusion%20model%20formulations%2C%20molecular%0Adata%20modalities%2C%20and%20generative%20task%20types%2C%20the%20research%20landscape%20is%0Achallenging%20to%20navigate%2C%20hindering%20understanding%20and%20limiting%20the%20area%27s%0Agrowth.%20To%20address%20this%2C%20this%20paper%20conducts%20a%20comprehensive%20survey%20of%0Adiffusion%20model-based%20molecular%20generative%20methods.%20We%20systematically%20review%0Athe%20research%20from%20the%20perspectives%20of%20methodological%20formulations%2C%20data%0Amodalities%2C%20and%20task%20types%2C%20offering%20a%20novel%20taxonomy.%20This%20survey%20aims%20to%0Afacilitate%20understanding%20and%20further%20flourishing%20development%20in%20this%20area.%20The%0Arelevant%20papers%20are%20summarized%20at%3A%0Ahttps%3A//github.com/AzureLeon1/awesome-molecular-diffusion-models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09511v1&entry.124074799=Read"},
{"title": "Asymptotic Normality of Generalized Low-Rank Matrix Sensing via\n  Riemannian Geometry", "author": "Osbert Bastani", "abstract": "  We prove an asymptotic normality guarantee for generalized low-rank matrix\nsensing -- i.e., matrix sensing under a general convex loss $\\bar\\ell(\\langle\nX,M\\rangle,y^*)$, where $M\\in\\mathbb{R}^{d\\times d}$ is the unknown rank-$k$\nmatrix, $X$ is a measurement matrix, and $y^*$ is the corresponding\nmeasurement. Our analysis relies on tools from Riemannian geometry to handle\ndegeneracy of the Hessian of the loss due to rotational symmetry in the\nparameter space. In particular, we parameterize the manifold of low-rank\nmatrices by $\\bar\\theta\\bar\\theta^\\top$, where\n$\\bar\\theta\\in\\mathbb{R}^{d\\times k}$. Then, assuming the minimizer of the\nempirical loss $\\bar\\theta^0\\in\\mathbb{R}^{d\\times k}$ is in a constant size\nball around the true parameters $\\bar\\theta^*$, we prove\n$\\sqrt{n}(\\phi^0-\\phi^*)\\xrightarrow{D}N(0,(H^*)^{-1})$ as $n\\to\\infty$, where\n$\\phi^0$ and $\\phi^*$ are representations of $\\bar\\theta^*$ and $\\bar\\theta^0$\nin the horizontal space of the Riemannian quotient manifold\n$\\mathbb{R}^{d\\times k}/\\text{O}(k)$, and $H^*$ is the Hessian of the true loss\nin the same representation.\n", "link": "http://arxiv.org/abs/2407.10238v2", "date": "2025-02-13", "relevancy": 2.0481, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4186}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.408}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymptotic%20Normality%20of%20Generalized%20Low-Rank%20Matrix%20Sensing%20via%0A%20%20Riemannian%20Geometry&body=Title%3A%20Asymptotic%20Normality%20of%20Generalized%20Low-Rank%20Matrix%20Sensing%20via%0A%20%20Riemannian%20Geometry%0AAuthor%3A%20Osbert%20Bastani%0AAbstract%3A%20%20%20We%20prove%20an%20asymptotic%20normality%20guarantee%20for%20generalized%20low-rank%20matrix%0Asensing%20--%20i.e.%2C%20matrix%20sensing%20under%20a%20general%20convex%20loss%20%24%5Cbar%5Cell%28%5Clangle%0AX%2CM%5Crangle%2Cy%5E%2A%29%24%2C%20where%20%24M%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20d%7D%24%20is%20the%20unknown%20rank-%24k%24%0Amatrix%2C%20%24X%24%20is%20a%20measurement%20matrix%2C%20and%20%24y%5E%2A%24%20is%20the%20corresponding%0Ameasurement.%20Our%20analysis%20relies%20on%20tools%20from%20Riemannian%20geometry%20to%20handle%0Adegeneracy%20of%20the%20Hessian%20of%20the%20loss%20due%20to%20rotational%20symmetry%20in%20the%0Aparameter%20space.%20In%20particular%2C%20we%20parameterize%20the%20manifold%20of%20low-rank%0Amatrices%20by%20%24%5Cbar%5Ctheta%5Cbar%5Ctheta%5E%5Ctop%24%2C%20where%0A%24%5Cbar%5Ctheta%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20k%7D%24.%20Then%2C%20assuming%20the%20minimizer%20of%20the%0Aempirical%20loss%20%24%5Cbar%5Ctheta%5E0%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20k%7D%24%20is%20in%20a%20constant%20size%0Aball%20around%20the%20true%20parameters%20%24%5Cbar%5Ctheta%5E%2A%24%2C%20we%20prove%0A%24%5Csqrt%7Bn%7D%28%5Cphi%5E0-%5Cphi%5E%2A%29%5Cxrightarrow%7BD%7DN%280%2C%28H%5E%2A%29%5E%7B-1%7D%29%24%20as%20%24n%5Cto%5Cinfty%24%2C%20where%0A%24%5Cphi%5E0%24%20and%20%24%5Cphi%5E%2A%24%20are%20representations%20of%20%24%5Cbar%5Ctheta%5E%2A%24%20and%20%24%5Cbar%5Ctheta%5E0%24%0Ain%20the%20horizontal%20space%20of%20the%20Riemannian%20quotient%20manifold%0A%24%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20k%7D/%5Ctext%7BO%7D%28k%29%24%2C%20and%20%24H%5E%2A%24%20is%20the%20Hessian%20of%20the%20true%20loss%0Ain%20the%20same%20representation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymptotic%2520Normality%2520of%2520Generalized%2520Low-Rank%2520Matrix%2520Sensing%2520via%250A%2520%2520Riemannian%2520Geometry%26entry.906535625%3DOsbert%2520Bastani%26entry.1292438233%3D%2520%2520We%2520prove%2520an%2520asymptotic%2520normality%2520guarantee%2520for%2520generalized%2520low-rank%2520matrix%250Asensing%2520--%2520i.e.%252C%2520matrix%2520sensing%2520under%2520a%2520general%2520convex%2520loss%2520%2524%255Cbar%255Cell%2528%255Clangle%250AX%252CM%255Crangle%252Cy%255E%252A%2529%2524%252C%2520where%2520%2524M%255Cin%255Cmathbb%257BR%257D%255E%257Bd%255Ctimes%2520d%257D%2524%2520is%2520the%2520unknown%2520rank-%2524k%2524%250Amatrix%252C%2520%2524X%2524%2520is%2520a%2520measurement%2520matrix%252C%2520and%2520%2524y%255E%252A%2524%2520is%2520the%2520corresponding%250Ameasurement.%2520Our%2520analysis%2520relies%2520on%2520tools%2520from%2520Riemannian%2520geometry%2520to%2520handle%250Adegeneracy%2520of%2520the%2520Hessian%2520of%2520the%2520loss%2520due%2520to%2520rotational%2520symmetry%2520in%2520the%250Aparameter%2520space.%2520In%2520particular%252C%2520we%2520parameterize%2520the%2520manifold%2520of%2520low-rank%250Amatrices%2520by%2520%2524%255Cbar%255Ctheta%255Cbar%255Ctheta%255E%255Ctop%2524%252C%2520where%250A%2524%255Cbar%255Ctheta%255Cin%255Cmathbb%257BR%257D%255E%257Bd%255Ctimes%2520k%257D%2524.%2520Then%252C%2520assuming%2520the%2520minimizer%2520of%2520the%250Aempirical%2520loss%2520%2524%255Cbar%255Ctheta%255E0%255Cin%255Cmathbb%257BR%257D%255E%257Bd%255Ctimes%2520k%257D%2524%2520is%2520in%2520a%2520constant%2520size%250Aball%2520around%2520the%2520true%2520parameters%2520%2524%255Cbar%255Ctheta%255E%252A%2524%252C%2520we%2520prove%250A%2524%255Csqrt%257Bn%257D%2528%255Cphi%255E0-%255Cphi%255E%252A%2529%255Cxrightarrow%257BD%257DN%25280%252C%2528H%255E%252A%2529%255E%257B-1%257D%2529%2524%2520as%2520%2524n%255Cto%255Cinfty%2524%252C%2520where%250A%2524%255Cphi%255E0%2524%2520and%2520%2524%255Cphi%255E%252A%2524%2520are%2520representations%2520of%2520%2524%255Cbar%255Ctheta%255E%252A%2524%2520and%2520%2524%255Cbar%255Ctheta%255E0%2524%250Ain%2520the%2520horizontal%2520space%2520of%2520the%2520Riemannian%2520quotient%2520manifold%250A%2524%255Cmathbb%257BR%257D%255E%257Bd%255Ctimes%2520k%257D/%255Ctext%257BO%257D%2528k%2529%2524%252C%2520and%2520%2524H%255E%252A%2524%2520is%2520the%2520Hessian%2520of%2520the%2520true%2520loss%250Ain%2520the%2520same%2520representation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymptotic%20Normality%20of%20Generalized%20Low-Rank%20Matrix%20Sensing%20via%0A%20%20Riemannian%20Geometry&entry.906535625=Osbert%20Bastani&entry.1292438233=%20%20We%20prove%20an%20asymptotic%20normality%20guarantee%20for%20generalized%20low-rank%20matrix%0Asensing%20--%20i.e.%2C%20matrix%20sensing%20under%20a%20general%20convex%20loss%20%24%5Cbar%5Cell%28%5Clangle%0AX%2CM%5Crangle%2Cy%5E%2A%29%24%2C%20where%20%24M%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20d%7D%24%20is%20the%20unknown%20rank-%24k%24%0Amatrix%2C%20%24X%24%20is%20a%20measurement%20matrix%2C%20and%20%24y%5E%2A%24%20is%20the%20corresponding%0Ameasurement.%20Our%20analysis%20relies%20on%20tools%20from%20Riemannian%20geometry%20to%20handle%0Adegeneracy%20of%20the%20Hessian%20of%20the%20loss%20due%20to%20rotational%20symmetry%20in%20the%0Aparameter%20space.%20In%20particular%2C%20we%20parameterize%20the%20manifold%20of%20low-rank%0Amatrices%20by%20%24%5Cbar%5Ctheta%5Cbar%5Ctheta%5E%5Ctop%24%2C%20where%0A%24%5Cbar%5Ctheta%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20k%7D%24.%20Then%2C%20assuming%20the%20minimizer%20of%20the%0Aempirical%20loss%20%24%5Cbar%5Ctheta%5E0%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20k%7D%24%20is%20in%20a%20constant%20size%0Aball%20around%20the%20true%20parameters%20%24%5Cbar%5Ctheta%5E%2A%24%2C%20we%20prove%0A%24%5Csqrt%7Bn%7D%28%5Cphi%5E0-%5Cphi%5E%2A%29%5Cxrightarrow%7BD%7DN%280%2C%28H%5E%2A%29%5E%7B-1%7D%29%24%20as%20%24n%5Cto%5Cinfty%24%2C%20where%0A%24%5Cphi%5E0%24%20and%20%24%5Cphi%5E%2A%24%20are%20representations%20of%20%24%5Cbar%5Ctheta%5E%2A%24%20and%20%24%5Cbar%5Ctheta%5E0%24%0Ain%20the%20horizontal%20space%20of%20the%20Riemannian%20quotient%20manifold%0A%24%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20k%7D/%5Ctext%7BO%7D%28k%29%24%2C%20and%20%24H%5E%2A%24%20is%20the%20Hessian%20of%20the%20true%20loss%0Ain%20the%20same%20representation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10238v2&entry.124074799=Read"},
{"title": "Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting", "author": "Nicholas Dronen and Randall Balestriero", "abstract": "  Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\n\\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.\n", "link": "http://arxiv.org/abs/2502.09500v1", "date": "2025-02-13", "relevancy": 2.0378, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5298}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5056}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eidetic%20Learning%3A%20an%20Efficient%20and%20Provable%20Solution%20to%20Catastrophic%0A%20%20Forgetting&body=Title%3A%20Eidetic%20Learning%3A%20an%20Efficient%20and%20Provable%20Solution%20to%20Catastrophic%0A%20%20Forgetting%0AAuthor%3A%20Nicholas%20Dronen%20and%20Randall%20Balestriero%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20--%20the%20phenomenon%20of%20a%20neural%20network%20learning%20a%20task%0At1%20and%20losing%20the%20ability%20to%20perform%20it%20after%20being%20trained%20on%20some%20other%20task%0At2%20--%20is%20a%20long-standing%20problem%20for%20neural%20networks%20%5BMcCloskey%20and%20Cohen%2C%0A1989%5D.%20We%20present%20a%20method%2C%20Eidetic%20Learning%2C%20that%20provably%20solves%20catastrophic%0Aforgetting.%20A%20network%20trained%20with%20Eidetic%20Learning%20--%20here%2C%20an%20EideticNet%20--%0Arequires%20no%20rehearsal%20or%20replay.%20We%20consider%20successive%20discrete%20tasks%20and%20show%0Ahow%20at%20inference%20time%20an%20EideticNet%20automatically%20routes%20new%20instances%20without%0Aauxiliary%20task%20information.%20An%20EideticNet%20bears%20a%20family%20resemblance%20to%20the%0Asparsely-gated%20Mixture-of-Experts%20layer%20Shazeer%20et%20al.%20%5B2016%5D%20in%20that%20network%0Acapacity%20is%20partitioned%20across%20tasks%20and%20the%20network%20itself%20performs%0Adata-conditional%20routing.%20An%20EideticNet%20is%20easy%20to%20implement%20and%20train%2C%20is%0Aefficient%2C%20and%20has%20time%20and%20space%20complexity%20linear%20in%20the%20number%20of%0Aparameters.%20The%20guarantee%20of%20our%20method%20holds%20for%20normalization%20layers%20of%0Amodern%20neural%20networks%20during%20both%20pre-training%20and%20fine-tuning.%20We%20show%20with%20a%0Avariety%20of%20network%20architectures%20and%20sets%20of%20tasks%20that%20EideticNets%20are%20immune%0Ato%20forgetting.%20While%20the%20practical%20benefits%20of%20EideticNets%20are%20substantial%2C%20we%0Abelieve%20they%20can%20be%20benefit%20practitioners%20and%20theorists%20alike.%20The%20code%20for%0Atraining%20EideticNets%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/amazon-science/eideticnet-training%7D%7Bthis%20https%20URL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEidetic%2520Learning%253A%2520an%2520Efficient%2520and%2520Provable%2520Solution%2520to%2520Catastrophic%250A%2520%2520Forgetting%26entry.906535625%3DNicholas%2520Dronen%2520and%2520Randall%2520Balestriero%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520--%2520the%2520phenomenon%2520of%2520a%2520neural%2520network%2520learning%2520a%2520task%250At1%2520and%2520losing%2520the%2520ability%2520to%2520perform%2520it%2520after%2520being%2520trained%2520on%2520some%2520other%2520task%250At2%2520--%2520is%2520a%2520long-standing%2520problem%2520for%2520neural%2520networks%2520%255BMcCloskey%2520and%2520Cohen%252C%250A1989%255D.%2520We%2520present%2520a%2520method%252C%2520Eidetic%2520Learning%252C%2520that%2520provably%2520solves%2520catastrophic%250Aforgetting.%2520A%2520network%2520trained%2520with%2520Eidetic%2520Learning%2520--%2520here%252C%2520an%2520EideticNet%2520--%250Arequires%2520no%2520rehearsal%2520or%2520replay.%2520We%2520consider%2520successive%2520discrete%2520tasks%2520and%2520show%250Ahow%2520at%2520inference%2520time%2520an%2520EideticNet%2520automatically%2520routes%2520new%2520instances%2520without%250Aauxiliary%2520task%2520information.%2520An%2520EideticNet%2520bears%2520a%2520family%2520resemblance%2520to%2520the%250Asparsely-gated%2520Mixture-of-Experts%2520layer%2520Shazeer%2520et%2520al.%2520%255B2016%255D%2520in%2520that%2520network%250Acapacity%2520is%2520partitioned%2520across%2520tasks%2520and%2520the%2520network%2520itself%2520performs%250Adata-conditional%2520routing.%2520An%2520EideticNet%2520is%2520easy%2520to%2520implement%2520and%2520train%252C%2520is%250Aefficient%252C%2520and%2520has%2520time%2520and%2520space%2520complexity%2520linear%2520in%2520the%2520number%2520of%250Aparameters.%2520The%2520guarantee%2520of%2520our%2520method%2520holds%2520for%2520normalization%2520layers%2520of%250Amodern%2520neural%2520networks%2520during%2520both%2520pre-training%2520and%2520fine-tuning.%2520We%2520show%2520with%2520a%250Avariety%2520of%2520network%2520architectures%2520and%2520sets%2520of%2520tasks%2520that%2520EideticNets%2520are%2520immune%250Ato%2520forgetting.%2520While%2520the%2520practical%2520benefits%2520of%2520EideticNets%2520are%2520substantial%252C%2520we%250Abelieve%2520they%2520can%2520be%2520benefit%2520practitioners%2520and%2520theorists%2520alike.%2520The%2520code%2520for%250Atraining%2520EideticNets%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/amazon-science/eideticnet-training%257D%257Bthis%2520https%2520URL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eidetic%20Learning%3A%20an%20Efficient%20and%20Provable%20Solution%20to%20Catastrophic%0A%20%20Forgetting&entry.906535625=Nicholas%20Dronen%20and%20Randall%20Balestriero&entry.1292438233=%20%20Catastrophic%20forgetting%20--%20the%20phenomenon%20of%20a%20neural%20network%20learning%20a%20task%0At1%20and%20losing%20the%20ability%20to%20perform%20it%20after%20being%20trained%20on%20some%20other%20task%0At2%20--%20is%20a%20long-standing%20problem%20for%20neural%20networks%20%5BMcCloskey%20and%20Cohen%2C%0A1989%5D.%20We%20present%20a%20method%2C%20Eidetic%20Learning%2C%20that%20provably%20solves%20catastrophic%0Aforgetting.%20A%20network%20trained%20with%20Eidetic%20Learning%20--%20here%2C%20an%20EideticNet%20--%0Arequires%20no%20rehearsal%20or%20replay.%20We%20consider%20successive%20discrete%20tasks%20and%20show%0Ahow%20at%20inference%20time%20an%20EideticNet%20automatically%20routes%20new%20instances%20without%0Aauxiliary%20task%20information.%20An%20EideticNet%20bears%20a%20family%20resemblance%20to%20the%0Asparsely-gated%20Mixture-of-Experts%20layer%20Shazeer%20et%20al.%20%5B2016%5D%20in%20that%20network%0Acapacity%20is%20partitioned%20across%20tasks%20and%20the%20network%20itself%20performs%0Adata-conditional%20routing.%20An%20EideticNet%20is%20easy%20to%20implement%20and%20train%2C%20is%0Aefficient%2C%20and%20has%20time%20and%20space%20complexity%20linear%20in%20the%20number%20of%0Aparameters.%20The%20guarantee%20of%20our%20method%20holds%20for%20normalization%20layers%20of%0Amodern%20neural%20networks%20during%20both%20pre-training%20and%20fine-tuning.%20We%20show%20with%20a%0Avariety%20of%20network%20architectures%20and%20sets%20of%20tasks%20that%20EideticNets%20are%20immune%0Ato%20forgetting.%20While%20the%20practical%20benefits%20of%20EideticNets%20are%20substantial%2C%20we%0Abelieve%20they%20can%20be%20benefit%20practitioners%20and%20theorists%20alike.%20The%20code%20for%0Atraining%20EideticNets%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/amazon-science/eideticnet-training%7D%7Bthis%20https%20URL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09500v1&entry.124074799=Read"},
{"title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency", "author": "Dongzhi Jiang and Renrui Zhang and Ziyu Guo and Yanwei Li and Yu Qi and Xinyan Chen and Liuhui Wang and Jianhan Jin and Claire Guo and Shen Yan and Bo Zhang and Chaoyou Fu and Peng Gao and Hongsheng Li", "abstract": "  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n", "link": "http://arxiv.org/abs/2502.09621v1", "date": "2025-02-13", "relevancy": 2.016, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5097}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MME-CoT%3A%20Benchmarking%20Chain-of-Thought%20in%20Large%20Multimodal%20Models%20for%0A%20%20Reasoning%20Quality%2C%20Robustness%2C%20and%20Efficiency&body=Title%3A%20MME-CoT%3A%20Benchmarking%20Chain-of-Thought%20in%20Large%20Multimodal%20Models%20for%0A%20%20Reasoning%20Quality%2C%20Robustness%2C%20and%20Efficiency%0AAuthor%3A%20Dongzhi%20Jiang%20and%20Renrui%20Zhang%20and%20Ziyu%20Guo%20and%20Yanwei%20Li%20and%20Yu%20Qi%20and%20Xinyan%20Chen%20and%20Liuhui%20Wang%20and%20Jianhan%20Jin%20and%20Claire%20Guo%20and%20Shen%20Yan%20and%20Bo%20Zhang%20and%20Chaoyou%20Fu%20and%20Peng%20Gao%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Answering%20questions%20with%20Chain-of-Thought%20%28CoT%29%20has%20significantly%20enhanced%0Athe%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20yet%20its%20impact%20on%0ALarge%20Multimodal%20Models%20%28LMMs%29%20still%20lacks%20a%20systematic%20assessment%20and%20in-depth%0Ainvestigation.%20In%20this%20paper%2C%20we%20introduce%20MME-CoT%2C%20a%20specialized%20benchmark%0Aevaluating%20the%20CoT%20reasoning%20performance%20of%20LMMs%2C%20spanning%20six%20domains%3A%20math%2C%0Ascience%2C%20OCR%2C%20logic%2C%20space-time%2C%20and%20general%20scenes.%20As%20the%20first%20comprehensive%0Astudy%20in%20this%20area%2C%20we%20propose%20a%20thorough%20evaluation%20suite%20incorporating%20three%0Anovel%20metrics%20that%20assess%20the%20reasoning%20quality%2C%20robustness%2C%20and%20efficiency%20at%0Aa%20fine-grained%20level.%20Leveraging%20curated%20high-quality%20data%20and%20a%20unique%0Aevaluation%20strategy%2C%20we%20conduct%20an%20in-depth%20analysis%20of%20state-of-the-art%20LMMs%2C%0Auncovering%20several%20key%20insights%3A%201%29%20Models%20with%20reflection%20mechanism%0Ademonstrate%20a%20superior%20CoT%20quality%2C%20with%20Kimi%20k1.5%20outperforming%20GPT-4o%20and%0Ademonstrating%20the%20highest%20quality%20results%3B%202%29%20CoT%20prompting%20often%20degrades%20LMM%0Aperformance%20on%20perception-heavy%20tasks%2C%20suggesting%20a%20potentially%20harmful%0Aoverthinking%20behavior%3B%20and%203%29%20Although%20the%20CoT%20quality%20is%20high%2C%20LMMs%20with%0Areflection%20exhibit%20significant%20inefficiency%20in%20both%20normal%20response%20and%0Aself-correction%20phases.%20We%20hope%20MME-CoT%20serves%20as%20a%20foundation%20for%20advancing%0Amultimodal%20reasoning%20in%20LMMs.%20Project%20Page%3A%20https%3A//mmecot.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMME-CoT%253A%2520Benchmarking%2520Chain-of-Thought%2520in%2520Large%2520Multimodal%2520Models%2520for%250A%2520%2520Reasoning%2520Quality%252C%2520Robustness%252C%2520and%2520Efficiency%26entry.906535625%3DDongzhi%2520Jiang%2520and%2520Renrui%2520Zhang%2520and%2520Ziyu%2520Guo%2520and%2520Yanwei%2520Li%2520and%2520Yu%2520Qi%2520and%2520Xinyan%2520Chen%2520and%2520Liuhui%2520Wang%2520and%2520Jianhan%2520Jin%2520and%2520Claire%2520Guo%2520and%2520Shen%2520Yan%2520and%2520Bo%2520Zhang%2520and%2520Chaoyou%2520Fu%2520and%2520Peng%2520Gao%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Answering%2520questions%2520with%2520Chain-of-Thought%2520%2528CoT%2529%2520has%2520significantly%2520enhanced%250Athe%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520yet%2520its%2520impact%2520on%250ALarge%2520Multimodal%2520Models%2520%2528LMMs%2529%2520still%2520lacks%2520a%2520systematic%2520assessment%2520and%2520in-depth%250Ainvestigation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MME-CoT%252C%2520a%2520specialized%2520benchmark%250Aevaluating%2520the%2520CoT%2520reasoning%2520performance%2520of%2520LMMs%252C%2520spanning%2520six%2520domains%253A%2520math%252C%250Ascience%252C%2520OCR%252C%2520logic%252C%2520space-time%252C%2520and%2520general%2520scenes.%2520As%2520the%2520first%2520comprehensive%250Astudy%2520in%2520this%2520area%252C%2520we%2520propose%2520a%2520thorough%2520evaluation%2520suite%2520incorporating%2520three%250Anovel%2520metrics%2520that%2520assess%2520the%2520reasoning%2520quality%252C%2520robustness%252C%2520and%2520efficiency%2520at%250Aa%2520fine-grained%2520level.%2520Leveraging%2520curated%2520high-quality%2520data%2520and%2520a%2520unique%250Aevaluation%2520strategy%252C%2520we%2520conduct%2520an%2520in-depth%2520analysis%2520of%2520state-of-the-art%2520LMMs%252C%250Auncovering%2520several%2520key%2520insights%253A%25201%2529%2520Models%2520with%2520reflection%2520mechanism%250Ademonstrate%2520a%2520superior%2520CoT%2520quality%252C%2520with%2520Kimi%2520k1.5%2520outperforming%2520GPT-4o%2520and%250Ademonstrating%2520the%2520highest%2520quality%2520results%253B%25202%2529%2520CoT%2520prompting%2520often%2520degrades%2520LMM%250Aperformance%2520on%2520perception-heavy%2520tasks%252C%2520suggesting%2520a%2520potentially%2520harmful%250Aoverthinking%2520behavior%253B%2520and%25203%2529%2520Although%2520the%2520CoT%2520quality%2520is%2520high%252C%2520LMMs%2520with%250Areflection%2520exhibit%2520significant%2520inefficiency%2520in%2520both%2520normal%2520response%2520and%250Aself-correction%2520phases.%2520We%2520hope%2520MME-CoT%2520serves%2520as%2520a%2520foundation%2520for%2520advancing%250Amultimodal%2520reasoning%2520in%2520LMMs.%2520Project%2520Page%253A%2520https%253A//mmecot.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MME-CoT%3A%20Benchmarking%20Chain-of-Thought%20in%20Large%20Multimodal%20Models%20for%0A%20%20Reasoning%20Quality%2C%20Robustness%2C%20and%20Efficiency&entry.906535625=Dongzhi%20Jiang%20and%20Renrui%20Zhang%20and%20Ziyu%20Guo%20and%20Yanwei%20Li%20and%20Yu%20Qi%20and%20Xinyan%20Chen%20and%20Liuhui%20Wang%20and%20Jianhan%20Jin%20and%20Claire%20Guo%20and%20Shen%20Yan%20and%20Bo%20Zhang%20and%20Chaoyou%20Fu%20and%20Peng%20Gao%20and%20Hongsheng%20Li&entry.1292438233=%20%20Answering%20questions%20with%20Chain-of-Thought%20%28CoT%29%20has%20significantly%20enhanced%0Athe%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20yet%20its%20impact%20on%0ALarge%20Multimodal%20Models%20%28LMMs%29%20still%20lacks%20a%20systematic%20assessment%20and%20in-depth%0Ainvestigation.%20In%20this%20paper%2C%20we%20introduce%20MME-CoT%2C%20a%20specialized%20benchmark%0Aevaluating%20the%20CoT%20reasoning%20performance%20of%20LMMs%2C%20spanning%20six%20domains%3A%20math%2C%0Ascience%2C%20OCR%2C%20logic%2C%20space-time%2C%20and%20general%20scenes.%20As%20the%20first%20comprehensive%0Astudy%20in%20this%20area%2C%20we%20propose%20a%20thorough%20evaluation%20suite%20incorporating%20three%0Anovel%20metrics%20that%20assess%20the%20reasoning%20quality%2C%20robustness%2C%20and%20efficiency%20at%0Aa%20fine-grained%20level.%20Leveraging%20curated%20high-quality%20data%20and%20a%20unique%0Aevaluation%20strategy%2C%20we%20conduct%20an%20in-depth%20analysis%20of%20state-of-the-art%20LMMs%2C%0Auncovering%20several%20key%20insights%3A%201%29%20Models%20with%20reflection%20mechanism%0Ademonstrate%20a%20superior%20CoT%20quality%2C%20with%20Kimi%20k1.5%20outperforming%20GPT-4o%20and%0Ademonstrating%20the%20highest%20quality%20results%3B%202%29%20CoT%20prompting%20often%20degrades%20LMM%0Aperformance%20on%20perception-heavy%20tasks%2C%20suggesting%20a%20potentially%20harmful%0Aoverthinking%20behavior%3B%20and%203%29%20Although%20the%20CoT%20quality%20is%20high%2C%20LMMs%20with%0Areflection%20exhibit%20significant%20inefficiency%20in%20both%20normal%20response%20and%0Aself-correction%20phases.%20We%20hope%20MME-CoT%20serves%20as%20a%20foundation%20for%20advancing%0Amultimodal%20reasoning%20in%20LMMs.%20Project%20Page%3A%20https%3A//mmecot.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09621v1&entry.124074799=Read"},
{"title": "Standardisation of Convex Ultrasound Data Through Geometric Analysis and\n  Augmentation", "author": "Alistair Weld and Giovanni Faoro and Luke Dixon and Sophie Camp and Arianna Menciassi and Stamatia Giannarou", "abstract": "  The application of ultrasound in healthcare has seen increased diversity and\nimportance. Unlike other medical imaging modalities, ultrasound research and\ndevelopment has historically lagged, particularly in the case of applications\nwith data-driven algorithms. A significant issue with ultrasound is the extreme\nvariability of the images, due to the number of different machines available\nand the possible combination of parameter settings. One outcome of this is the\nlack of standardised and benchmarking ultrasound datasets. The method proposed\nin this article is an approach to alleviating this issue of disorganisation.\nFor this purpose, the issue of ultrasound data sparsity is examined and a novel\nperspective, approach, and solution is proposed; involving the extraction of\nthe underlying ultrasound plane within the image and representing it using\nannulus sector geometry. An application of this methodology is proposed, which\nis the extraction of scan lines and the linearisation of convex planes.\nValidation of the robustness of the proposed method is performed on both\nprivate and public data. The impact of deformation and the invertibility of\naugmentation using the estimated annulus sector parameters is also studied.\nKeywords: Ultrasound, Annulus Sector, Augmentation, Linearisation.\n", "link": "http://arxiv.org/abs/2502.09482v1", "date": "2025-02-13", "relevancy": 2.0106, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.523}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4895}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Standardisation%20of%20Convex%20Ultrasound%20Data%20Through%20Geometric%20Analysis%20and%0A%20%20Augmentation&body=Title%3A%20Standardisation%20of%20Convex%20Ultrasound%20Data%20Through%20Geometric%20Analysis%20and%0A%20%20Augmentation%0AAuthor%3A%20Alistair%20Weld%20and%20Giovanni%20Faoro%20and%20Luke%20Dixon%20and%20Sophie%20Camp%20and%20Arianna%20Menciassi%20and%20Stamatia%20Giannarou%0AAbstract%3A%20%20%20The%20application%20of%20ultrasound%20in%20healthcare%20has%20seen%20increased%20diversity%20and%0Aimportance.%20Unlike%20other%20medical%20imaging%20modalities%2C%20ultrasound%20research%20and%0Adevelopment%20has%20historically%20lagged%2C%20particularly%20in%20the%20case%20of%20applications%0Awith%20data-driven%20algorithms.%20A%20significant%20issue%20with%20ultrasound%20is%20the%20extreme%0Avariability%20of%20the%20images%2C%20due%20to%20the%20number%20of%20different%20machines%20available%0Aand%20the%20possible%20combination%20of%20parameter%20settings.%20One%20outcome%20of%20this%20is%20the%0Alack%20of%20standardised%20and%20benchmarking%20ultrasound%20datasets.%20The%20method%20proposed%0Ain%20this%20article%20is%20an%20approach%20to%20alleviating%20this%20issue%20of%20disorganisation.%0AFor%20this%20purpose%2C%20the%20issue%20of%20ultrasound%20data%20sparsity%20is%20examined%20and%20a%20novel%0Aperspective%2C%20approach%2C%20and%20solution%20is%20proposed%3B%20involving%20the%20extraction%20of%0Athe%20underlying%20ultrasound%20plane%20within%20the%20image%20and%20representing%20it%20using%0Aannulus%20sector%20geometry.%20An%20application%20of%20this%20methodology%20is%20proposed%2C%20which%0Ais%20the%20extraction%20of%20scan%20lines%20and%20the%20linearisation%20of%20convex%20planes.%0AValidation%20of%20the%20robustness%20of%20the%20proposed%20method%20is%20performed%20on%20both%0Aprivate%20and%20public%20data.%20The%20impact%20of%20deformation%20and%20the%20invertibility%20of%0Aaugmentation%20using%20the%20estimated%20annulus%20sector%20parameters%20is%20also%20studied.%0AKeywords%3A%20Ultrasound%2C%20Annulus%20Sector%2C%20Augmentation%2C%20Linearisation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStandardisation%2520of%2520Convex%2520Ultrasound%2520Data%2520Through%2520Geometric%2520Analysis%2520and%250A%2520%2520Augmentation%26entry.906535625%3DAlistair%2520Weld%2520and%2520Giovanni%2520Faoro%2520and%2520Luke%2520Dixon%2520and%2520Sophie%2520Camp%2520and%2520Arianna%2520Menciassi%2520and%2520Stamatia%2520Giannarou%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520ultrasound%2520in%2520healthcare%2520has%2520seen%2520increased%2520diversity%2520and%250Aimportance.%2520Unlike%2520other%2520medical%2520imaging%2520modalities%252C%2520ultrasound%2520research%2520and%250Adevelopment%2520has%2520historically%2520lagged%252C%2520particularly%2520in%2520the%2520case%2520of%2520applications%250Awith%2520data-driven%2520algorithms.%2520A%2520significant%2520issue%2520with%2520ultrasound%2520is%2520the%2520extreme%250Avariability%2520of%2520the%2520images%252C%2520due%2520to%2520the%2520number%2520of%2520different%2520machines%2520available%250Aand%2520the%2520possible%2520combination%2520of%2520parameter%2520settings.%2520One%2520outcome%2520of%2520this%2520is%2520the%250Alack%2520of%2520standardised%2520and%2520benchmarking%2520ultrasound%2520datasets.%2520The%2520method%2520proposed%250Ain%2520this%2520article%2520is%2520an%2520approach%2520to%2520alleviating%2520this%2520issue%2520of%2520disorganisation.%250AFor%2520this%2520purpose%252C%2520the%2520issue%2520of%2520ultrasound%2520data%2520sparsity%2520is%2520examined%2520and%2520a%2520novel%250Aperspective%252C%2520approach%252C%2520and%2520solution%2520is%2520proposed%253B%2520involving%2520the%2520extraction%2520of%250Athe%2520underlying%2520ultrasound%2520plane%2520within%2520the%2520image%2520and%2520representing%2520it%2520using%250Aannulus%2520sector%2520geometry.%2520An%2520application%2520of%2520this%2520methodology%2520is%2520proposed%252C%2520which%250Ais%2520the%2520extraction%2520of%2520scan%2520lines%2520and%2520the%2520linearisation%2520of%2520convex%2520planes.%250AValidation%2520of%2520the%2520robustness%2520of%2520the%2520proposed%2520method%2520is%2520performed%2520on%2520both%250Aprivate%2520and%2520public%2520data.%2520The%2520impact%2520of%2520deformation%2520and%2520the%2520invertibility%2520of%250Aaugmentation%2520using%2520the%2520estimated%2520annulus%2520sector%2520parameters%2520is%2520also%2520studied.%250AKeywords%253A%2520Ultrasound%252C%2520Annulus%2520Sector%252C%2520Augmentation%252C%2520Linearisation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Standardisation%20of%20Convex%20Ultrasound%20Data%20Through%20Geometric%20Analysis%20and%0A%20%20Augmentation&entry.906535625=Alistair%20Weld%20and%20Giovanni%20Faoro%20and%20Luke%20Dixon%20and%20Sophie%20Camp%20and%20Arianna%20Menciassi%20and%20Stamatia%20Giannarou&entry.1292438233=%20%20The%20application%20of%20ultrasound%20in%20healthcare%20has%20seen%20increased%20diversity%20and%0Aimportance.%20Unlike%20other%20medical%20imaging%20modalities%2C%20ultrasound%20research%20and%0Adevelopment%20has%20historically%20lagged%2C%20particularly%20in%20the%20case%20of%20applications%0Awith%20data-driven%20algorithms.%20A%20significant%20issue%20with%20ultrasound%20is%20the%20extreme%0Avariability%20of%20the%20images%2C%20due%20to%20the%20number%20of%20different%20machines%20available%0Aand%20the%20possible%20combination%20of%20parameter%20settings.%20One%20outcome%20of%20this%20is%20the%0Alack%20of%20standardised%20and%20benchmarking%20ultrasound%20datasets.%20The%20method%20proposed%0Ain%20this%20article%20is%20an%20approach%20to%20alleviating%20this%20issue%20of%20disorganisation.%0AFor%20this%20purpose%2C%20the%20issue%20of%20ultrasound%20data%20sparsity%20is%20examined%20and%20a%20novel%0Aperspective%2C%20approach%2C%20and%20solution%20is%20proposed%3B%20involving%20the%20extraction%20of%0Athe%20underlying%20ultrasound%20plane%20within%20the%20image%20and%20representing%20it%20using%0Aannulus%20sector%20geometry.%20An%20application%20of%20this%20methodology%20is%20proposed%2C%20which%0Ais%20the%20extraction%20of%20scan%20lines%20and%20the%20linearisation%20of%20convex%20planes.%0AValidation%20of%20the%20robustness%20of%20the%20proposed%20method%20is%20performed%20on%20both%0Aprivate%20and%20public%20data.%20The%20impact%20of%20deformation%20and%20the%20invertibility%20of%0Aaugmentation%20using%20the%20estimated%20annulus%20sector%20parameters%20is%20also%20studied.%0AKeywords%3A%20Ultrasound%2C%20Annulus%20Sector%2C%20Augmentation%2C%20Linearisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09482v1&entry.124074799=Read"},
{"title": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue", "author": "Hao Li and Chenghao Yang and An Zhang and Yang Deng and Xiang Wang and Tat-Seng Chua", "abstract": "  Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.\n", "link": "http://arxiv.org/abs/2406.05925v2", "date": "2025-02-13", "relevancy": 2.0052, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5169}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4987}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hello%20Again%21%20LLM-powered%20Personalized%20Agent%20for%20Long-term%20Dialogue&body=Title%3A%20Hello%20Again%21%20LLM-powered%20Personalized%20Agent%20for%20Long-term%20Dialogue%0AAuthor%3A%20Hao%20Li%20and%20Chenghao%20Yang%20and%20An%20Zhang%20and%20Yang%20Deng%20and%20Xiang%20Wang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Open-domain%20dialogue%20systems%20have%20seen%20remarkable%20advancements%20with%20the%0Adevelopment%20of%20large%20language%20models%20%28LLMs%29.%20Nonetheless%2C%20most%20existing%0Adialogue%20systems%20predominantly%20focus%20on%20brief%20single-session%20interactions%2C%0Aneglecting%20the%20real-world%20demands%20for%20long-term%20companionship%20and%20personalized%0Ainteractions%20with%20chatbots.%20Crucial%20to%20addressing%20this%20real-world%20need%20are%0Aevent%20summary%20and%20persona%20management%2C%20which%20enable%20reasoning%20for%20appropriate%0Along-term%20dialogue%20responses.%20Recent%20progress%20in%20the%20human-like%20cognitive%20and%0Areasoning%20capabilities%20of%20LLMs%20suggests%20that%20LLM-based%20agents%20could%0Asignificantly%20enhance%20automated%20perception%2C%20decision-making%2C%20and%0Aproblem-solving.%20In%20response%20to%20this%20potential%2C%20we%20introduce%20a%20model-agnostic%0Aframework%2C%20the%20Long-term%20Dialogue%20Agent%20%28LD-Agent%29%2C%20which%20incorporates%20three%0Aindependently%20tunable%20modules%20dedicated%20to%20event%20perception%2C%20persona%0Aextraction%2C%20and%20response%20generation.%20For%20the%20event%20memory%20module%2C%20long%20and%0Ashort-term%20memory%20banks%20are%20employed%20to%20separately%20focus%20on%20historical%20and%0Aongoing%20sessions%2C%20while%20a%20topic-based%20retrieval%20mechanism%20is%20introduced%20to%0Aenhance%20the%20accuracy%20of%20memory%20retrieval.%20Furthermore%2C%20the%20persona%20module%0Aconducts%20dynamic%20persona%20modeling%20for%20both%20users%20and%20agents.%20The%20integration%20of%0Aretrieved%20memories%20and%20extracted%20personas%20is%20subsequently%20fed%20into%20the%0Agenerator%20to%20induce%20appropriate%20responses.%20The%20effectiveness%2C%20generality%2C%20and%0Across-domain%20capabilities%20of%20LD-Agent%20are%20empirically%20demonstrated%20across%0Avarious%20illustrative%20benchmarks%2C%20models%2C%20and%20tasks.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/leolee99/LD-Agent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHello%2520Again%2521%2520LLM-powered%2520Personalized%2520Agent%2520for%2520Long-term%2520Dialogue%26entry.906535625%3DHao%2520Li%2520and%2520Chenghao%2520Yang%2520and%2520An%2520Zhang%2520and%2520Yang%2520Deng%2520and%2520Xiang%2520Wang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Open-domain%2520dialogue%2520systems%2520have%2520seen%2520remarkable%2520advancements%2520with%2520the%250Adevelopment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Nonetheless%252C%2520most%2520existing%250Adialogue%2520systems%2520predominantly%2520focus%2520on%2520brief%2520single-session%2520interactions%252C%250Aneglecting%2520the%2520real-world%2520demands%2520for%2520long-term%2520companionship%2520and%2520personalized%250Ainteractions%2520with%2520chatbots.%2520Crucial%2520to%2520addressing%2520this%2520real-world%2520need%2520are%250Aevent%2520summary%2520and%2520persona%2520management%252C%2520which%2520enable%2520reasoning%2520for%2520appropriate%250Along-term%2520dialogue%2520responses.%2520Recent%2520progress%2520in%2520the%2520human-like%2520cognitive%2520and%250Areasoning%2520capabilities%2520of%2520LLMs%2520suggests%2520that%2520LLM-based%2520agents%2520could%250Asignificantly%2520enhance%2520automated%2520perception%252C%2520decision-making%252C%2520and%250Aproblem-solving.%2520In%2520response%2520to%2520this%2520potential%252C%2520we%2520introduce%2520a%2520model-agnostic%250Aframework%252C%2520the%2520Long-term%2520Dialogue%2520Agent%2520%2528LD-Agent%2529%252C%2520which%2520incorporates%2520three%250Aindependently%2520tunable%2520modules%2520dedicated%2520to%2520event%2520perception%252C%2520persona%250Aextraction%252C%2520and%2520response%2520generation.%2520For%2520the%2520event%2520memory%2520module%252C%2520long%2520and%250Ashort-term%2520memory%2520banks%2520are%2520employed%2520to%2520separately%2520focus%2520on%2520historical%2520and%250Aongoing%2520sessions%252C%2520while%2520a%2520topic-based%2520retrieval%2520mechanism%2520is%2520introduced%2520to%250Aenhance%2520the%2520accuracy%2520of%2520memory%2520retrieval.%2520Furthermore%252C%2520the%2520persona%2520module%250Aconducts%2520dynamic%2520persona%2520modeling%2520for%2520both%2520users%2520and%2520agents.%2520The%2520integration%2520of%250Aretrieved%2520memories%2520and%2520extracted%2520personas%2520is%2520subsequently%2520fed%2520into%2520the%250Agenerator%2520to%2520induce%2520appropriate%2520responses.%2520The%2520effectiveness%252C%2520generality%252C%2520and%250Across-domain%2520capabilities%2520of%2520LD-Agent%2520are%2520empirically%2520demonstrated%2520across%250Avarious%2520illustrative%2520benchmarks%252C%2520models%252C%2520and%2520tasks.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/leolee99/LD-Agent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hello%20Again%21%20LLM-powered%20Personalized%20Agent%20for%20Long-term%20Dialogue&entry.906535625=Hao%20Li%20and%20Chenghao%20Yang%20and%20An%20Zhang%20and%20Yang%20Deng%20and%20Xiang%20Wang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Open-domain%20dialogue%20systems%20have%20seen%20remarkable%20advancements%20with%20the%0Adevelopment%20of%20large%20language%20models%20%28LLMs%29.%20Nonetheless%2C%20most%20existing%0Adialogue%20systems%20predominantly%20focus%20on%20brief%20single-session%20interactions%2C%0Aneglecting%20the%20real-world%20demands%20for%20long-term%20companionship%20and%20personalized%0Ainteractions%20with%20chatbots.%20Crucial%20to%20addressing%20this%20real-world%20need%20are%0Aevent%20summary%20and%20persona%20management%2C%20which%20enable%20reasoning%20for%20appropriate%0Along-term%20dialogue%20responses.%20Recent%20progress%20in%20the%20human-like%20cognitive%20and%0Areasoning%20capabilities%20of%20LLMs%20suggests%20that%20LLM-based%20agents%20could%0Asignificantly%20enhance%20automated%20perception%2C%20decision-making%2C%20and%0Aproblem-solving.%20In%20response%20to%20this%20potential%2C%20we%20introduce%20a%20model-agnostic%0Aframework%2C%20the%20Long-term%20Dialogue%20Agent%20%28LD-Agent%29%2C%20which%20incorporates%20three%0Aindependently%20tunable%20modules%20dedicated%20to%20event%20perception%2C%20persona%0Aextraction%2C%20and%20response%20generation.%20For%20the%20event%20memory%20module%2C%20long%20and%0Ashort-term%20memory%20banks%20are%20employed%20to%20separately%20focus%20on%20historical%20and%0Aongoing%20sessions%2C%20while%20a%20topic-based%20retrieval%20mechanism%20is%20introduced%20to%0Aenhance%20the%20accuracy%20of%20memory%20retrieval.%20Furthermore%2C%20the%20persona%20module%0Aconducts%20dynamic%20persona%20modeling%20for%20both%20users%20and%20agents.%20The%20integration%20of%0Aretrieved%20memories%20and%20extracted%20personas%20is%20subsequently%20fed%20into%20the%0Agenerator%20to%20induce%20appropriate%20responses.%20The%20effectiveness%2C%20generality%2C%20and%0Across-domain%20capabilities%20of%20LD-Agent%20are%20empirically%20demonstrated%20across%0Avarious%20illustrative%20benchmarks%2C%20models%2C%20and%20tasks.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/leolee99/LD-Agent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05925v2&entry.124074799=Read"},
{"title": "A Galois theorem for machine learning: Functions on symmetric matrices\n  and point clouds via lightweight invariant features", "author": "Ben Blum-Smith and Ningyuan Huang and Marco Cuturi and Soledad Villar", "abstract": "  In this work, we present a mathematical formulation for machine learning of\n(1) functions on symmetric matrices that are invariant with respect to the\naction of permutations by conjugation, and (2) functions on point clouds that\nare invariant with respect to rotations, reflections, and permutations of the\npoints. To achieve this, we provide a general construction of generically\nseparating invariant features using ideas inspired by Galois theory. We\nconstruct $O(n^2)$ invariant features derived from generators for the field of\nrational functions on $n\\times n$ symmetric matrices that are invariant under\njoint permutations of rows and columns. We show that these invariant features\ncan separate all distinct orbits of symmetric matrices except for a measure\nzero set; such features can be used to universally approximate invariant\nfunctions on almost all weighted graphs. For point clouds in a fixed dimension,\nwe prove that the number of invariant features can be reduced, generically\nwithout losing expressivity, to $O(n)$, where $n$ is the number of points. We\ncombine these invariant features with DeepSets to learn functions on symmetric\nmatrices and point clouds with varying sizes. We empirically demonstrate the\nfeasibility of our approach on molecule property regression and point cloud\ndistance prediction.\n", "link": "http://arxiv.org/abs/2405.08097v3", "date": "2025-02-13", "relevancy": 2.0039, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5056}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5046}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Galois%20theorem%20for%20machine%20learning%3A%20Functions%20on%20symmetric%20matrices%0A%20%20and%20point%20clouds%20via%20lightweight%20invariant%20features&body=Title%3A%20A%20Galois%20theorem%20for%20machine%20learning%3A%20Functions%20on%20symmetric%20matrices%0A%20%20and%20point%20clouds%20via%20lightweight%20invariant%20features%0AAuthor%3A%20Ben%20Blum-Smith%20and%20Ningyuan%20Huang%20and%20Marco%20Cuturi%20and%20Soledad%20Villar%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20mathematical%20formulation%20for%20machine%20learning%20of%0A%281%29%20functions%20on%20symmetric%20matrices%20that%20are%20invariant%20with%20respect%20to%20the%0Aaction%20of%20permutations%20by%20conjugation%2C%20and%20%282%29%20functions%20on%20point%20clouds%20that%0Aare%20invariant%20with%20respect%20to%20rotations%2C%20reflections%2C%20and%20permutations%20of%20the%0Apoints.%20To%20achieve%20this%2C%20we%20provide%20a%20general%20construction%20of%20generically%0Aseparating%20invariant%20features%20using%20ideas%20inspired%20by%20Galois%20theory.%20We%0Aconstruct%20%24O%28n%5E2%29%24%20invariant%20features%20derived%20from%20generators%20for%20the%20field%20of%0Arational%20functions%20on%20%24n%5Ctimes%20n%24%20symmetric%20matrices%20that%20are%20invariant%20under%0Ajoint%20permutations%20of%20rows%20and%20columns.%20We%20show%20that%20these%20invariant%20features%0Acan%20separate%20all%20distinct%20orbits%20of%20symmetric%20matrices%20except%20for%20a%20measure%0Azero%20set%3B%20such%20features%20can%20be%20used%20to%20universally%20approximate%20invariant%0Afunctions%20on%20almost%20all%20weighted%20graphs.%20For%20point%20clouds%20in%20a%20fixed%20dimension%2C%0Awe%20prove%20that%20the%20number%20of%20invariant%20features%20can%20be%20reduced%2C%20generically%0Awithout%20losing%20expressivity%2C%20to%20%24O%28n%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20points.%20We%0Acombine%20these%20invariant%20features%20with%20DeepSets%20to%20learn%20functions%20on%20symmetric%0Amatrices%20and%20point%20clouds%20with%20varying%20sizes.%20We%20empirically%20demonstrate%20the%0Afeasibility%20of%20our%20approach%20on%20molecule%20property%20regression%20and%20point%20cloud%0Adistance%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08097v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Galois%2520theorem%2520for%2520machine%2520learning%253A%2520Functions%2520on%2520symmetric%2520matrices%250A%2520%2520and%2520point%2520clouds%2520via%2520lightweight%2520invariant%2520features%26entry.906535625%3DBen%2520Blum-Smith%2520and%2520Ningyuan%2520Huang%2520and%2520Marco%2520Cuturi%2520and%2520Soledad%2520Villar%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520mathematical%2520formulation%2520for%2520machine%2520learning%2520of%250A%25281%2529%2520functions%2520on%2520symmetric%2520matrices%2520that%2520are%2520invariant%2520with%2520respect%2520to%2520the%250Aaction%2520of%2520permutations%2520by%2520conjugation%252C%2520and%2520%25282%2529%2520functions%2520on%2520point%2520clouds%2520that%250Aare%2520invariant%2520with%2520respect%2520to%2520rotations%252C%2520reflections%252C%2520and%2520permutations%2520of%2520the%250Apoints.%2520To%2520achieve%2520this%252C%2520we%2520provide%2520a%2520general%2520construction%2520of%2520generically%250Aseparating%2520invariant%2520features%2520using%2520ideas%2520inspired%2520by%2520Galois%2520theory.%2520We%250Aconstruct%2520%2524O%2528n%255E2%2529%2524%2520invariant%2520features%2520derived%2520from%2520generators%2520for%2520the%2520field%2520of%250Arational%2520functions%2520on%2520%2524n%255Ctimes%2520n%2524%2520symmetric%2520matrices%2520that%2520are%2520invariant%2520under%250Ajoint%2520permutations%2520of%2520rows%2520and%2520columns.%2520We%2520show%2520that%2520these%2520invariant%2520features%250Acan%2520separate%2520all%2520distinct%2520orbits%2520of%2520symmetric%2520matrices%2520except%2520for%2520a%2520measure%250Azero%2520set%253B%2520such%2520features%2520can%2520be%2520used%2520to%2520universally%2520approximate%2520invariant%250Afunctions%2520on%2520almost%2520all%2520weighted%2520graphs.%2520For%2520point%2520clouds%2520in%2520a%2520fixed%2520dimension%252C%250Awe%2520prove%2520that%2520the%2520number%2520of%2520invariant%2520features%2520can%2520be%2520reduced%252C%2520generically%250Awithout%2520losing%2520expressivity%252C%2520to%2520%2524O%2528n%2529%2524%252C%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520points.%2520We%250Acombine%2520these%2520invariant%2520features%2520with%2520DeepSets%2520to%2520learn%2520functions%2520on%2520symmetric%250Amatrices%2520and%2520point%2520clouds%2520with%2520varying%2520sizes.%2520We%2520empirically%2520demonstrate%2520the%250Afeasibility%2520of%2520our%2520approach%2520on%2520molecule%2520property%2520regression%2520and%2520point%2520cloud%250Adistance%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08097v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Galois%20theorem%20for%20machine%20learning%3A%20Functions%20on%20symmetric%20matrices%0A%20%20and%20point%20clouds%20via%20lightweight%20invariant%20features&entry.906535625=Ben%20Blum-Smith%20and%20Ningyuan%20Huang%20and%20Marco%20Cuturi%20and%20Soledad%20Villar&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20mathematical%20formulation%20for%20machine%20learning%20of%0A%281%29%20functions%20on%20symmetric%20matrices%20that%20are%20invariant%20with%20respect%20to%20the%0Aaction%20of%20permutations%20by%20conjugation%2C%20and%20%282%29%20functions%20on%20point%20clouds%20that%0Aare%20invariant%20with%20respect%20to%20rotations%2C%20reflections%2C%20and%20permutations%20of%20the%0Apoints.%20To%20achieve%20this%2C%20we%20provide%20a%20general%20construction%20of%20generically%0Aseparating%20invariant%20features%20using%20ideas%20inspired%20by%20Galois%20theory.%20We%0Aconstruct%20%24O%28n%5E2%29%24%20invariant%20features%20derived%20from%20generators%20for%20the%20field%20of%0Arational%20functions%20on%20%24n%5Ctimes%20n%24%20symmetric%20matrices%20that%20are%20invariant%20under%0Ajoint%20permutations%20of%20rows%20and%20columns.%20We%20show%20that%20these%20invariant%20features%0Acan%20separate%20all%20distinct%20orbits%20of%20symmetric%20matrices%20except%20for%20a%20measure%0Azero%20set%3B%20such%20features%20can%20be%20used%20to%20universally%20approximate%20invariant%0Afunctions%20on%20almost%20all%20weighted%20graphs.%20For%20point%20clouds%20in%20a%20fixed%20dimension%2C%0Awe%20prove%20that%20the%20number%20of%20invariant%20features%20can%20be%20reduced%2C%20generically%0Awithout%20losing%20expressivity%2C%20to%20%24O%28n%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20points.%20We%0Acombine%20these%20invariant%20features%20with%20DeepSets%20to%20learn%20functions%20on%20symmetric%0Amatrices%20and%20point%20clouds%20with%20varying%20sizes.%20We%20empirically%20demonstrate%20the%0Afeasibility%20of%20our%20approach%20on%20molecule%20property%20regression%20and%20point%20cloud%0Adistance%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08097v3&entry.124074799=Read"},
{"title": "Moment of Untruth: Dealing with Negative Queries in Video Moment\n  Retrieval", "author": "Kevin Flanagan and Dima Damen and Michael Wray", "abstract": "  Video Moment Retrieval is a common task to evaluate the performance of\nvisual-language models - it involves localising start and end times of moments\nin videos from query sentences. The current task formulation assumes that the\nqueried moment is present in the video, resulting in false positive moment\npredictions when irrelevant query sentences are provided. In this paper we\npropose the task of Negative-Aware Video Moment Retrieval (NA-VMR), which\nconsiders both moment retrieval accuracy and negative query rejection accuracy.\nWe make the distinction between In-Domain and Out-of-Domain negative queries\nand provide new evaluation benchmarks for two popular video moment retrieval\ndatasets: QVHighlights and Charades-STA. We analyse the ability of current SOTA\nvideo moment retrieval approaches to adapt to Negative-Aware Video Moment\nRetrieval and propose UniVTG-NA, an adaptation of UniVTG designed to tackle\nNA-VMR. UniVTG-NA achieves high negative rejection accuracy (avg. $98.4\\%$)\nscores while retaining moment retrieval scores to within $3.87\\%$ Recall@1.\nDataset splits and code are available at\nhttps://github.com/keflanagan/MomentofUntruth\n", "link": "http://arxiv.org/abs/2502.08544v2", "date": "2025-02-13", "relevancy": 2.0031, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moment%20of%20Untruth%3A%20Dealing%20with%20Negative%20Queries%20in%20Video%20Moment%0A%20%20Retrieval&body=Title%3A%20Moment%20of%20Untruth%3A%20Dealing%20with%20Negative%20Queries%20in%20Video%20Moment%0A%20%20Retrieval%0AAuthor%3A%20Kevin%20Flanagan%20and%20Dima%20Damen%20and%20Michael%20Wray%0AAbstract%3A%20%20%20Video%20Moment%20Retrieval%20is%20a%20common%20task%20to%20evaluate%20the%20performance%20of%0Avisual-language%20models%20-%20it%20involves%20localising%20start%20and%20end%20times%20of%20moments%0Ain%20videos%20from%20query%20sentences.%20The%20current%20task%20formulation%20assumes%20that%20the%0Aqueried%20moment%20is%20present%20in%20the%20video%2C%20resulting%20in%20false%20positive%20moment%0Apredictions%20when%20irrelevant%20query%20sentences%20are%20provided.%20In%20this%20paper%20we%0Apropose%20the%20task%20of%20Negative-Aware%20Video%20Moment%20Retrieval%20%28NA-VMR%29%2C%20which%0Aconsiders%20both%20moment%20retrieval%20accuracy%20and%20negative%20query%20rejection%20accuracy.%0AWe%20make%20the%20distinction%20between%20In-Domain%20and%20Out-of-Domain%20negative%20queries%0Aand%20provide%20new%20evaluation%20benchmarks%20for%20two%20popular%20video%20moment%20retrieval%0Adatasets%3A%20QVHighlights%20and%20Charades-STA.%20We%20analyse%20the%20ability%20of%20current%20SOTA%0Avideo%20moment%20retrieval%20approaches%20to%20adapt%20to%20Negative-Aware%20Video%20Moment%0ARetrieval%20and%20propose%20UniVTG-NA%2C%20an%20adaptation%20of%20UniVTG%20designed%20to%20tackle%0ANA-VMR.%20UniVTG-NA%20achieves%20high%20negative%20rejection%20accuracy%20%28avg.%20%2498.4%5C%25%24%29%0Ascores%20while%20retaining%20moment%20retrieval%20scores%20to%20within%20%243.87%5C%25%24%20Recall%401.%0ADataset%20splits%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/keflanagan/MomentofUntruth%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoment%2520of%2520Untruth%253A%2520Dealing%2520with%2520Negative%2520Queries%2520in%2520Video%2520Moment%250A%2520%2520Retrieval%26entry.906535625%3DKevin%2520Flanagan%2520and%2520Dima%2520Damen%2520and%2520Michael%2520Wray%26entry.1292438233%3D%2520%2520Video%2520Moment%2520Retrieval%2520is%2520a%2520common%2520task%2520to%2520evaluate%2520the%2520performance%2520of%250Avisual-language%2520models%2520-%2520it%2520involves%2520localising%2520start%2520and%2520end%2520times%2520of%2520moments%250Ain%2520videos%2520from%2520query%2520sentences.%2520The%2520current%2520task%2520formulation%2520assumes%2520that%2520the%250Aqueried%2520moment%2520is%2520present%2520in%2520the%2520video%252C%2520resulting%2520in%2520false%2520positive%2520moment%250Apredictions%2520when%2520irrelevant%2520query%2520sentences%2520are%2520provided.%2520In%2520this%2520paper%2520we%250Apropose%2520the%2520task%2520of%2520Negative-Aware%2520Video%2520Moment%2520Retrieval%2520%2528NA-VMR%2529%252C%2520which%250Aconsiders%2520both%2520moment%2520retrieval%2520accuracy%2520and%2520negative%2520query%2520rejection%2520accuracy.%250AWe%2520make%2520the%2520distinction%2520between%2520In-Domain%2520and%2520Out-of-Domain%2520negative%2520queries%250Aand%2520provide%2520new%2520evaluation%2520benchmarks%2520for%2520two%2520popular%2520video%2520moment%2520retrieval%250Adatasets%253A%2520QVHighlights%2520and%2520Charades-STA.%2520We%2520analyse%2520the%2520ability%2520of%2520current%2520SOTA%250Avideo%2520moment%2520retrieval%2520approaches%2520to%2520adapt%2520to%2520Negative-Aware%2520Video%2520Moment%250ARetrieval%2520and%2520propose%2520UniVTG-NA%252C%2520an%2520adaptation%2520of%2520UniVTG%2520designed%2520to%2520tackle%250ANA-VMR.%2520UniVTG-NA%2520achieves%2520high%2520negative%2520rejection%2520accuracy%2520%2528avg.%2520%252498.4%255C%2525%2524%2529%250Ascores%2520while%2520retaining%2520moment%2520retrieval%2520scores%2520to%2520within%2520%25243.87%255C%2525%2524%2520Recall%25401.%250ADataset%2520splits%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/keflanagan/MomentofUntruth%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moment%20of%20Untruth%3A%20Dealing%20with%20Negative%20Queries%20in%20Video%20Moment%0A%20%20Retrieval&entry.906535625=Kevin%20Flanagan%20and%20Dima%20Damen%20and%20Michael%20Wray&entry.1292438233=%20%20Video%20Moment%20Retrieval%20is%20a%20common%20task%20to%20evaluate%20the%20performance%20of%0Avisual-language%20models%20-%20it%20involves%20localising%20start%20and%20end%20times%20of%20moments%0Ain%20videos%20from%20query%20sentences.%20The%20current%20task%20formulation%20assumes%20that%20the%0Aqueried%20moment%20is%20present%20in%20the%20video%2C%20resulting%20in%20false%20positive%20moment%0Apredictions%20when%20irrelevant%20query%20sentences%20are%20provided.%20In%20this%20paper%20we%0Apropose%20the%20task%20of%20Negative-Aware%20Video%20Moment%20Retrieval%20%28NA-VMR%29%2C%20which%0Aconsiders%20both%20moment%20retrieval%20accuracy%20and%20negative%20query%20rejection%20accuracy.%0AWe%20make%20the%20distinction%20between%20In-Domain%20and%20Out-of-Domain%20negative%20queries%0Aand%20provide%20new%20evaluation%20benchmarks%20for%20two%20popular%20video%20moment%20retrieval%0Adatasets%3A%20QVHighlights%20and%20Charades-STA.%20We%20analyse%20the%20ability%20of%20current%20SOTA%0Avideo%20moment%20retrieval%20approaches%20to%20adapt%20to%20Negative-Aware%20Video%20Moment%0ARetrieval%20and%20propose%20UniVTG-NA%2C%20an%20adaptation%20of%20UniVTG%20designed%20to%20tackle%0ANA-VMR.%20UniVTG-NA%20achieves%20high%20negative%20rejection%20accuracy%20%28avg.%20%2498.4%5C%25%24%29%0Ascores%20while%20retaining%20moment%20retrieval%20scores%20to%20within%20%243.87%5C%25%24%20Recall%401.%0ADataset%20splits%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/keflanagan/MomentofUntruth%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08544v2&entry.124074799=Read"},
{"title": "A Differentiable Rank-Based Objective For Better Feature Learning", "author": "Krunoslav Lehman Pavasovic and David Lopez-Paz and Giulio Biroli and Levent Sagun", "abstract": "  In this paper, we leverage existing statistical methods to better understand\nfeature learning from data. We tackle this by modifying the model-free variable\nselection method, Feature Ordering by Conditional Independence (FOCI), which is\nintroduced in \\cite{azadkia2021simple}. While FOCI is based on a non-parametric\ncoefficient of conditional dependence, we introduce its parametric,\ndifferentiable approximation. With this approximate coefficient of correlation,\nwe present a new algorithm called difFOCI, which is applicable to a wider range\nof machine learning problems thanks to its differentiable nature and learnable\nparameters. We present difFOCI in three contexts: (1) as a variable selection\nmethod with baseline comparisons to FOCI, (2) as a trainable model parametrized\nwith a neural network, and (3) as a generic, widely applicable neural network\nregularizer, one that improves feature learning with better management of\nspurious correlations. We evaluate difFOCI on increasingly complex problems\nranging from basic variable selection in toy examples to saliency map\ncomparisons in convolutional networks. We then show how difFOCI can be\nincorporated in the context of fairness to facilitate classifications without\nrelying on sensitive data.\n", "link": "http://arxiv.org/abs/2502.09445v1", "date": "2025-02-13", "relevancy": 1.9989, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5222}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4961}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Differentiable%20Rank-Based%20Objective%20For%20Better%20Feature%20Learning&body=Title%3A%20A%20Differentiable%20Rank-Based%20Objective%20For%20Better%20Feature%20Learning%0AAuthor%3A%20Krunoslav%20Lehman%20Pavasovic%20and%20David%20Lopez-Paz%20and%20Giulio%20Biroli%20and%20Levent%20Sagun%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20leverage%20existing%20statistical%20methods%20to%20better%20understand%0Afeature%20learning%20from%20data.%20We%20tackle%20this%20by%20modifying%20the%20model-free%20variable%0Aselection%20method%2C%20Feature%20Ordering%20by%20Conditional%20Independence%20%28FOCI%29%2C%20which%20is%0Aintroduced%20in%20%5Ccite%7Bazadkia2021simple%7D.%20While%20FOCI%20is%20based%20on%20a%20non-parametric%0Acoefficient%20of%20conditional%20dependence%2C%20we%20introduce%20its%20parametric%2C%0Adifferentiable%20approximation.%20With%20this%20approximate%20coefficient%20of%20correlation%2C%0Awe%20present%20a%20new%20algorithm%20called%20difFOCI%2C%20which%20is%20applicable%20to%20a%20wider%20range%0Aof%20machine%20learning%20problems%20thanks%20to%20its%20differentiable%20nature%20and%20learnable%0Aparameters.%20We%20present%20difFOCI%20in%20three%20contexts%3A%20%281%29%20as%20a%20variable%20selection%0Amethod%20with%20baseline%20comparisons%20to%20FOCI%2C%20%282%29%20as%20a%20trainable%20model%20parametrized%0Awith%20a%20neural%20network%2C%20and%20%283%29%20as%20a%20generic%2C%20widely%20applicable%20neural%20network%0Aregularizer%2C%20one%20that%20improves%20feature%20learning%20with%20better%20management%20of%0Aspurious%20correlations.%20We%20evaluate%20difFOCI%20on%20increasingly%20complex%20problems%0Aranging%20from%20basic%20variable%20selection%20in%20toy%20examples%20to%20saliency%20map%0Acomparisons%20in%20convolutional%20networks.%20We%20then%20show%20how%20difFOCI%20can%20be%0Aincorporated%20in%20the%20context%20of%20fairness%20to%20facilitate%20classifications%20without%0Arelying%20on%20sensitive%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Differentiable%2520Rank-Based%2520Objective%2520For%2520Better%2520Feature%2520Learning%26entry.906535625%3DKrunoslav%2520Lehman%2520Pavasovic%2520and%2520David%2520Lopez-Paz%2520and%2520Giulio%2520Biroli%2520and%2520Levent%2520Sagun%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520leverage%2520existing%2520statistical%2520methods%2520to%2520better%2520understand%250Afeature%2520learning%2520from%2520data.%2520We%2520tackle%2520this%2520by%2520modifying%2520the%2520model-free%2520variable%250Aselection%2520method%252C%2520Feature%2520Ordering%2520by%2520Conditional%2520Independence%2520%2528FOCI%2529%252C%2520which%2520is%250Aintroduced%2520in%2520%255Ccite%257Bazadkia2021simple%257D.%2520While%2520FOCI%2520is%2520based%2520on%2520a%2520non-parametric%250Acoefficient%2520of%2520conditional%2520dependence%252C%2520we%2520introduce%2520its%2520parametric%252C%250Adifferentiable%2520approximation.%2520With%2520this%2520approximate%2520coefficient%2520of%2520correlation%252C%250Awe%2520present%2520a%2520new%2520algorithm%2520called%2520difFOCI%252C%2520which%2520is%2520applicable%2520to%2520a%2520wider%2520range%250Aof%2520machine%2520learning%2520problems%2520thanks%2520to%2520its%2520differentiable%2520nature%2520and%2520learnable%250Aparameters.%2520We%2520present%2520difFOCI%2520in%2520three%2520contexts%253A%2520%25281%2529%2520as%2520a%2520variable%2520selection%250Amethod%2520with%2520baseline%2520comparisons%2520to%2520FOCI%252C%2520%25282%2529%2520as%2520a%2520trainable%2520model%2520parametrized%250Awith%2520a%2520neural%2520network%252C%2520and%2520%25283%2529%2520as%2520a%2520generic%252C%2520widely%2520applicable%2520neural%2520network%250Aregularizer%252C%2520one%2520that%2520improves%2520feature%2520learning%2520with%2520better%2520management%2520of%250Aspurious%2520correlations.%2520We%2520evaluate%2520difFOCI%2520on%2520increasingly%2520complex%2520problems%250Aranging%2520from%2520basic%2520variable%2520selection%2520in%2520toy%2520examples%2520to%2520saliency%2520map%250Acomparisons%2520in%2520convolutional%2520networks.%2520We%2520then%2520show%2520how%2520difFOCI%2520can%2520be%250Aincorporated%2520in%2520the%2520context%2520of%2520fairness%2520to%2520facilitate%2520classifications%2520without%250Arelying%2520on%2520sensitive%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Differentiable%20Rank-Based%20Objective%20For%20Better%20Feature%20Learning&entry.906535625=Krunoslav%20Lehman%20Pavasovic%20and%20David%20Lopez-Paz%20and%20Giulio%20Biroli%20and%20Levent%20Sagun&entry.1292438233=%20%20In%20this%20paper%2C%20we%20leverage%20existing%20statistical%20methods%20to%20better%20understand%0Afeature%20learning%20from%20data.%20We%20tackle%20this%20by%20modifying%20the%20model-free%20variable%0Aselection%20method%2C%20Feature%20Ordering%20by%20Conditional%20Independence%20%28FOCI%29%2C%20which%20is%0Aintroduced%20in%20%5Ccite%7Bazadkia2021simple%7D.%20While%20FOCI%20is%20based%20on%20a%20non-parametric%0Acoefficient%20of%20conditional%20dependence%2C%20we%20introduce%20its%20parametric%2C%0Adifferentiable%20approximation.%20With%20this%20approximate%20coefficient%20of%20correlation%2C%0Awe%20present%20a%20new%20algorithm%20called%20difFOCI%2C%20which%20is%20applicable%20to%20a%20wider%20range%0Aof%20machine%20learning%20problems%20thanks%20to%20its%20differentiable%20nature%20and%20learnable%0Aparameters.%20We%20present%20difFOCI%20in%20three%20contexts%3A%20%281%29%20as%20a%20variable%20selection%0Amethod%20with%20baseline%20comparisons%20to%20FOCI%2C%20%282%29%20as%20a%20trainable%20model%20parametrized%0Awith%20a%20neural%20network%2C%20and%20%283%29%20as%20a%20generic%2C%20widely%20applicable%20neural%20network%0Aregularizer%2C%20one%20that%20improves%20feature%20learning%20with%20better%20management%20of%0Aspurious%20correlations.%20We%20evaluate%20difFOCI%20on%20increasingly%20complex%20problems%0Aranging%20from%20basic%20variable%20selection%20in%20toy%20examples%20to%20saliency%20map%0Acomparisons%20in%20convolutional%20networks.%20We%20then%20show%20how%20difFOCI%20can%20be%0Aincorporated%20in%20the%20context%20of%20fairness%20to%20facilitate%20classifications%20without%0Arelying%20on%20sensitive%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09445v1&entry.124074799=Read"},
{"title": "ArthroPhase: A Novel Dataset and Method for Phase Recognition in\n  Arthroscopic Video", "author": "Ali Bahari Malayeri and Matthias Seibold and Nicola Cavalcanti and Jonas Hein and Sascha Jecklin and Lazaros Vlachopoulos and Sandro Fucentese and Sandro Hodel and Philipp Furnstahl", "abstract": "  This study aims to advance surgical phase recognition in arthroscopic\nprocedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, by\nintroducing the first arthroscopy dataset and developing a novel\ntransformer-based model. We aim to establish a benchmark for arthroscopic\nsurgical phase recognition by leveraging spatio-temporal features to address\nthe specific challenges of arthroscopic videos including limited field of view,\nocclusions, and visual distortions. We developed the ACL27 dataset, comprising\n27 videos of ACL surgeries, each labeled with surgical phases. Our model\nemploys a transformer-based architecture, utilizing temporal-aware frame-wise\nfeature extraction through a ResNet-50 and transformer layers. This approach\nintegrates spatio-temporal features and introduces a Surgical Progress Index\n(SPI) to quantify surgery progression. The model's performance was evaluated\nusing accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80\ndatasets. The proposed model achieved an overall accuracy of 72.91% on the\nACL27 dataset. On the Cholec80 dataset, the model achieved a comparable\nperformance with the state-of-the-art methods with an accuracy of 92.4%. The\nSPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80\ndatasets respectively, indicating reliable surgery progression estimation. This\nstudy introduces a significant advancement in surgical phase recognition for\narthroscopy, providing a comprehensive dataset and a robust transformer-based\nmodel. The results validate the model's effectiveness and generalizability,\nhighlighting its potential to improve surgical training, real-time assistance,\nand operational efficiency in orthopedic surgery. The publicly available\ndataset and code will facilitate future research and development in this\ncritical field.\n", "link": "http://arxiv.org/abs/2502.07431v2", "date": "2025-02-13", "relevancy": 1.9983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4955}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArthroPhase%3A%20A%20Novel%20Dataset%20and%20Method%20for%20Phase%20Recognition%20in%0A%20%20Arthroscopic%20Video&body=Title%3A%20ArthroPhase%3A%20A%20Novel%20Dataset%20and%20Method%20for%20Phase%20Recognition%20in%0A%20%20Arthroscopic%20Video%0AAuthor%3A%20Ali%20Bahari%20Malayeri%20and%20Matthias%20Seibold%20and%20Nicola%20Cavalcanti%20and%20Jonas%20Hein%20and%20Sascha%20Jecklin%20and%20Lazaros%20Vlachopoulos%20and%20Sandro%20Fucentese%20and%20Sandro%20Hodel%20and%20Philipp%20Furnstahl%0AAbstract%3A%20%20%20This%20study%20aims%20to%20advance%20surgical%20phase%20recognition%20in%20arthroscopic%0Aprocedures%2C%20specifically%20Anterior%20Cruciate%20Ligament%20%28ACL%29%20reconstruction%2C%20by%0Aintroducing%20the%20first%20arthroscopy%20dataset%20and%20developing%20a%20novel%0Atransformer-based%20model.%20We%20aim%20to%20establish%20a%20benchmark%20for%20arthroscopic%0Asurgical%20phase%20recognition%20by%20leveraging%20spatio-temporal%20features%20to%20address%0Athe%20specific%20challenges%20of%20arthroscopic%20videos%20including%20limited%20field%20of%20view%2C%0Aocclusions%2C%20and%20visual%20distortions.%20We%20developed%20the%20ACL27%20dataset%2C%20comprising%0A27%20videos%20of%20ACL%20surgeries%2C%20each%20labeled%20with%20surgical%20phases.%20Our%20model%0Aemploys%20a%20transformer-based%20architecture%2C%20utilizing%20temporal-aware%20frame-wise%0Afeature%20extraction%20through%20a%20ResNet-50%20and%20transformer%20layers.%20This%20approach%0Aintegrates%20spatio-temporal%20features%20and%20introduces%20a%20Surgical%20Progress%20Index%0A%28SPI%29%20to%20quantify%20surgery%20progression.%20The%20model%27s%20performance%20was%20evaluated%0Ausing%20accuracy%2C%20precision%2C%20recall%2C%20and%20Jaccard%20Index%20on%20the%20ACL27%20and%20Cholec80%0Adatasets.%20The%20proposed%20model%20achieved%20an%20overall%20accuracy%20of%2072.91%25%20on%20the%0AACL27%20dataset.%20On%20the%20Cholec80%20dataset%2C%20the%20model%20achieved%20a%20comparable%0Aperformance%20with%20the%20state-of-the-art%20methods%20with%20an%20accuracy%20of%2092.4%25.%20The%0ASPI%20demonstrated%20an%20output%20error%20of%2010.6%25%20and%209.86%25%20on%20ACL27%20and%20Cholec80%0Adatasets%20respectively%2C%20indicating%20reliable%20surgery%20progression%20estimation.%20This%0Astudy%20introduces%20a%20significant%20advancement%20in%20surgical%20phase%20recognition%20for%0Aarthroscopy%2C%20providing%20a%20comprehensive%20dataset%20and%20a%20robust%20transformer-based%0Amodel.%20The%20results%20validate%20the%20model%27s%20effectiveness%20and%20generalizability%2C%0Ahighlighting%20its%20potential%20to%20improve%20surgical%20training%2C%20real-time%20assistance%2C%0Aand%20operational%20efficiency%20in%20orthopedic%20surgery.%20The%20publicly%20available%0Adataset%20and%20code%20will%20facilitate%20future%20research%20and%20development%20in%20this%0Acritical%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07431v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArthroPhase%253A%2520A%2520Novel%2520Dataset%2520and%2520Method%2520for%2520Phase%2520Recognition%2520in%250A%2520%2520Arthroscopic%2520Video%26entry.906535625%3DAli%2520Bahari%2520Malayeri%2520and%2520Matthias%2520Seibold%2520and%2520Nicola%2520Cavalcanti%2520and%2520Jonas%2520Hein%2520and%2520Sascha%2520Jecklin%2520and%2520Lazaros%2520Vlachopoulos%2520and%2520Sandro%2520Fucentese%2520and%2520Sandro%2520Hodel%2520and%2520Philipp%2520Furnstahl%26entry.1292438233%3D%2520%2520This%2520study%2520aims%2520to%2520advance%2520surgical%2520phase%2520recognition%2520in%2520arthroscopic%250Aprocedures%252C%2520specifically%2520Anterior%2520Cruciate%2520Ligament%2520%2528ACL%2529%2520reconstruction%252C%2520by%250Aintroducing%2520the%2520first%2520arthroscopy%2520dataset%2520and%2520developing%2520a%2520novel%250Atransformer-based%2520model.%2520We%2520aim%2520to%2520establish%2520a%2520benchmark%2520for%2520arthroscopic%250Asurgical%2520phase%2520recognition%2520by%2520leveraging%2520spatio-temporal%2520features%2520to%2520address%250Athe%2520specific%2520challenges%2520of%2520arthroscopic%2520videos%2520including%2520limited%2520field%2520of%2520view%252C%250Aocclusions%252C%2520and%2520visual%2520distortions.%2520We%2520developed%2520the%2520ACL27%2520dataset%252C%2520comprising%250A27%2520videos%2520of%2520ACL%2520surgeries%252C%2520each%2520labeled%2520with%2520surgical%2520phases.%2520Our%2520model%250Aemploys%2520a%2520transformer-based%2520architecture%252C%2520utilizing%2520temporal-aware%2520frame-wise%250Afeature%2520extraction%2520through%2520a%2520ResNet-50%2520and%2520transformer%2520layers.%2520This%2520approach%250Aintegrates%2520spatio-temporal%2520features%2520and%2520introduces%2520a%2520Surgical%2520Progress%2520Index%250A%2528SPI%2529%2520to%2520quantify%2520surgery%2520progression.%2520The%2520model%2527s%2520performance%2520was%2520evaluated%250Ausing%2520accuracy%252C%2520precision%252C%2520recall%252C%2520and%2520Jaccard%2520Index%2520on%2520the%2520ACL27%2520and%2520Cholec80%250Adatasets.%2520The%2520proposed%2520model%2520achieved%2520an%2520overall%2520accuracy%2520of%252072.91%2525%2520on%2520the%250AACL27%2520dataset.%2520On%2520the%2520Cholec80%2520dataset%252C%2520the%2520model%2520achieved%2520a%2520comparable%250Aperformance%2520with%2520the%2520state-of-the-art%2520methods%2520with%2520an%2520accuracy%2520of%252092.4%2525.%2520The%250ASPI%2520demonstrated%2520an%2520output%2520error%2520of%252010.6%2525%2520and%25209.86%2525%2520on%2520ACL27%2520and%2520Cholec80%250Adatasets%2520respectively%252C%2520indicating%2520reliable%2520surgery%2520progression%2520estimation.%2520This%250Astudy%2520introduces%2520a%2520significant%2520advancement%2520in%2520surgical%2520phase%2520recognition%2520for%250Aarthroscopy%252C%2520providing%2520a%2520comprehensive%2520dataset%2520and%2520a%2520robust%2520transformer-based%250Amodel.%2520The%2520results%2520validate%2520the%2520model%2527s%2520effectiveness%2520and%2520generalizability%252C%250Ahighlighting%2520its%2520potential%2520to%2520improve%2520surgical%2520training%252C%2520real-time%2520assistance%252C%250Aand%2520operational%2520efficiency%2520in%2520orthopedic%2520surgery.%2520The%2520publicly%2520available%250Adataset%2520and%2520code%2520will%2520facilitate%2520future%2520research%2520and%2520development%2520in%2520this%250Acritical%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07431v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArthroPhase%3A%20A%20Novel%20Dataset%20and%20Method%20for%20Phase%20Recognition%20in%0A%20%20Arthroscopic%20Video&entry.906535625=Ali%20Bahari%20Malayeri%20and%20Matthias%20Seibold%20and%20Nicola%20Cavalcanti%20and%20Jonas%20Hein%20and%20Sascha%20Jecklin%20and%20Lazaros%20Vlachopoulos%20and%20Sandro%20Fucentese%20and%20Sandro%20Hodel%20and%20Philipp%20Furnstahl&entry.1292438233=%20%20This%20study%20aims%20to%20advance%20surgical%20phase%20recognition%20in%20arthroscopic%0Aprocedures%2C%20specifically%20Anterior%20Cruciate%20Ligament%20%28ACL%29%20reconstruction%2C%20by%0Aintroducing%20the%20first%20arthroscopy%20dataset%20and%20developing%20a%20novel%0Atransformer-based%20model.%20We%20aim%20to%20establish%20a%20benchmark%20for%20arthroscopic%0Asurgical%20phase%20recognition%20by%20leveraging%20spatio-temporal%20features%20to%20address%0Athe%20specific%20challenges%20of%20arthroscopic%20videos%20including%20limited%20field%20of%20view%2C%0Aocclusions%2C%20and%20visual%20distortions.%20We%20developed%20the%20ACL27%20dataset%2C%20comprising%0A27%20videos%20of%20ACL%20surgeries%2C%20each%20labeled%20with%20surgical%20phases.%20Our%20model%0Aemploys%20a%20transformer-based%20architecture%2C%20utilizing%20temporal-aware%20frame-wise%0Afeature%20extraction%20through%20a%20ResNet-50%20and%20transformer%20layers.%20This%20approach%0Aintegrates%20spatio-temporal%20features%20and%20introduces%20a%20Surgical%20Progress%20Index%0A%28SPI%29%20to%20quantify%20surgery%20progression.%20The%20model%27s%20performance%20was%20evaluated%0Ausing%20accuracy%2C%20precision%2C%20recall%2C%20and%20Jaccard%20Index%20on%20the%20ACL27%20and%20Cholec80%0Adatasets.%20The%20proposed%20model%20achieved%20an%20overall%20accuracy%20of%2072.91%25%20on%20the%0AACL27%20dataset.%20On%20the%20Cholec80%20dataset%2C%20the%20model%20achieved%20a%20comparable%0Aperformance%20with%20the%20state-of-the-art%20methods%20with%20an%20accuracy%20of%2092.4%25.%20The%0ASPI%20demonstrated%20an%20output%20error%20of%2010.6%25%20and%209.86%25%20on%20ACL27%20and%20Cholec80%0Adatasets%20respectively%2C%20indicating%20reliable%20surgery%20progression%20estimation.%20This%0Astudy%20introduces%20a%20significant%20advancement%20in%20surgical%20phase%20recognition%20for%0Aarthroscopy%2C%20providing%20a%20comprehensive%20dataset%20and%20a%20robust%20transformer-based%0Amodel.%20The%20results%20validate%20the%20model%27s%20effectiveness%20and%20generalizability%2C%0Ahighlighting%20its%20potential%20to%20improve%20surgical%20training%2C%20real-time%20assistance%2C%0Aand%20operational%20efficiency%20in%20orthopedic%20surgery.%20The%20publicly%20available%0Adataset%20and%20code%20will%20facilitate%20future%20research%20and%20development%20in%20this%0Acritical%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07431v2&entry.124074799=Read"},
{"title": "Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization", "author": "Huawen Feng and Yan Fan and Xiong Liu and Ting-En Lin and Zekun Yao and Yuchuan Wu and Fei Huang and Yongbin Li and Qianli Ma", "abstract": "  Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.\n", "link": "http://arxiv.org/abs/2310.19347v4", "date": "2025-02-13", "relevancy": 1.9794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4975}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Factual%20Consistency%20of%20News%20Summarization%20by%20Contrastive%0A%20%20Preference%20Optimization&body=Title%3A%20Improving%20Factual%20Consistency%20of%20News%20Summarization%20by%20Contrastive%0A%20%20Preference%20Optimization%0AAuthor%3A%20Huawen%20Feng%20and%20Yan%20Fan%20and%20Xiong%20Liu%20and%20Ting-En%20Lin%20and%20Zekun%20Yao%20and%20Yuchuan%20Wu%20and%20Fei%20Huang%20and%20Yongbin%20Li%20and%20Qianli%20Ma%0AAbstract%3A%20%20%20Despite%20the%20recent%20progress%20in%20news%20summarization%20made%20by%20large%20language%0Amodels%20%28LLMs%29%2C%20they%20often%20generate%20summaries%20that%20are%20factually%20inconsistent%0Awith%20original%20articles%2C%20known%20as%20%22hallucinations%22%20in%20text%20generation.%20Unlike%0Aprevious%20small%20models%20%28e.g.%2C%20BART%2C%20T5%29%2C%20current%20LLMs%20make%20fewer%20silly%20mistakes%0Abut%20more%20sophisticated%20ones%2C%20such%20as%20imposing%20cause%20and%20effect%2C%20adding%20false%0Adetails%2C%20overgeneralizing%2C%20etc.%20These%20hallucinations%20are%20challenging%20to%20detect%0Athrough%20traditional%20methods%2C%20which%20poses%20great%20challenges%20for%20improving%20the%0Afactual%20consistency%20of%20text%20summarization.%20In%20this%20paper%2C%20we%20propose%0AContrastive%20Preference%20Optimization%20%28CPO%29%20to%20disentangle%20the%20LLMs%27%20propensities%0Ato%20generate%20faithful%20and%20fake%20content.%20Furthermore%2C%20we%20adopt%20a%20probing-based%0Aspecific%20training%20method%20to%20improve%20their%20capacity%20of%20distinguishing%20two%20types%0Aof%20propensities.%20In%20this%20way%2C%20LLMs%20can%20execute%20the%20instructions%20more%20accurately%0Aand%20have%20enhanced%20perception%20of%20hallucinations.%20Experimental%20results%20show%20that%0ACPO%20significantly%20improves%20the%20reliability%20of%20summarization%20based%20on%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19347v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Factual%2520Consistency%2520of%2520News%2520Summarization%2520by%2520Contrastive%250A%2520%2520Preference%2520Optimization%26entry.906535625%3DHuawen%2520Feng%2520and%2520Yan%2520Fan%2520and%2520Xiong%2520Liu%2520and%2520Ting-En%2520Lin%2520and%2520Zekun%2520Yao%2520and%2520Yuchuan%2520Wu%2520and%2520Fei%2520Huang%2520and%2520Yongbin%2520Li%2520and%2520Qianli%2520Ma%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520progress%2520in%2520news%2520summarization%2520made%2520by%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520they%2520often%2520generate%2520summaries%2520that%2520are%2520factually%2520inconsistent%250Awith%2520original%2520articles%252C%2520known%2520as%2520%2522hallucinations%2522%2520in%2520text%2520generation.%2520Unlike%250Aprevious%2520small%2520models%2520%2528e.g.%252C%2520BART%252C%2520T5%2529%252C%2520current%2520LLMs%2520make%2520fewer%2520silly%2520mistakes%250Abut%2520more%2520sophisticated%2520ones%252C%2520such%2520as%2520imposing%2520cause%2520and%2520effect%252C%2520adding%2520false%250Adetails%252C%2520overgeneralizing%252C%2520etc.%2520These%2520hallucinations%2520are%2520challenging%2520to%2520detect%250Athrough%2520traditional%2520methods%252C%2520which%2520poses%2520great%2520challenges%2520for%2520improving%2520the%250Afactual%2520consistency%2520of%2520text%2520summarization.%2520In%2520this%2520paper%252C%2520we%2520propose%250AContrastive%2520Preference%2520Optimization%2520%2528CPO%2529%2520to%2520disentangle%2520the%2520LLMs%2527%2520propensities%250Ato%2520generate%2520faithful%2520and%2520fake%2520content.%2520Furthermore%252C%2520we%2520adopt%2520a%2520probing-based%250Aspecific%2520training%2520method%2520to%2520improve%2520their%2520capacity%2520of%2520distinguishing%2520two%2520types%250Aof%2520propensities.%2520In%2520this%2520way%252C%2520LLMs%2520can%2520execute%2520the%2520instructions%2520more%2520accurately%250Aand%2520have%2520enhanced%2520perception%2520of%2520hallucinations.%2520Experimental%2520results%2520show%2520that%250ACPO%2520significantly%2520improves%2520the%2520reliability%2520of%2520summarization%2520based%2520on%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19347v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Factual%20Consistency%20of%20News%20Summarization%20by%20Contrastive%0A%20%20Preference%20Optimization&entry.906535625=Huawen%20Feng%20and%20Yan%20Fan%20and%20Xiong%20Liu%20and%20Ting-En%20Lin%20and%20Zekun%20Yao%20and%20Yuchuan%20Wu%20and%20Fei%20Huang%20and%20Yongbin%20Li%20and%20Qianli%20Ma&entry.1292438233=%20%20Despite%20the%20recent%20progress%20in%20news%20summarization%20made%20by%20large%20language%0Amodels%20%28LLMs%29%2C%20they%20often%20generate%20summaries%20that%20are%20factually%20inconsistent%0Awith%20original%20articles%2C%20known%20as%20%22hallucinations%22%20in%20text%20generation.%20Unlike%0Aprevious%20small%20models%20%28e.g.%2C%20BART%2C%20T5%29%2C%20current%20LLMs%20make%20fewer%20silly%20mistakes%0Abut%20more%20sophisticated%20ones%2C%20such%20as%20imposing%20cause%20and%20effect%2C%20adding%20false%0Adetails%2C%20overgeneralizing%2C%20etc.%20These%20hallucinations%20are%20challenging%20to%20detect%0Athrough%20traditional%20methods%2C%20which%20poses%20great%20challenges%20for%20improving%20the%0Afactual%20consistency%20of%20text%20summarization.%20In%20this%20paper%2C%20we%20propose%0AContrastive%20Preference%20Optimization%20%28CPO%29%20to%20disentangle%20the%20LLMs%27%20propensities%0Ato%20generate%20faithful%20and%20fake%20content.%20Furthermore%2C%20we%20adopt%20a%20probing-based%0Aspecific%20training%20method%20to%20improve%20their%20capacity%20of%20distinguishing%20two%20types%0Aof%20propensities.%20In%20this%20way%2C%20LLMs%20can%20execute%20the%20instructions%20more%20accurately%0Aand%20have%20enhanced%20perception%20of%20hallucinations.%20Experimental%20results%20show%20that%0ACPO%20significantly%20improves%20the%20reliability%20of%20summarization%20based%20on%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19347v4&entry.124074799=Read"},
{"title": "On multi-token prediction for efficient LLM inference", "author": "Somesh Mehra and Javier Alonso Garcia and Lukas Mauch", "abstract": "  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n", "link": "http://arxiv.org/abs/2502.09419v1", "date": "2025-02-13", "relevancy": 1.9761, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20multi-token%20prediction%20for%20efficient%20LLM%20inference&body=Title%3A%20On%20multi-token%20prediction%20for%20efficient%20LLM%20inference%0AAuthor%3A%20Somesh%20Mehra%20and%20Javier%20Alonso%20Garcia%20and%20Lukas%20Mauch%0AAbstract%3A%20%20%20We%20systematically%20investigate%20multi-token%20prediction%20%28MTP%29%20capabilities%0Awithin%20LLMs%20pre-trained%20for%20next-token%20prediction%20%28NTP%29.%20We%20first%20show%20that%0Asuch%20models%20inherently%20possess%20MTP%20capabilities%20via%20numerical%20marginalization%0Aover%20intermediate%20token%20probabilities%2C%20though%20performance%20is%20data-dependent%20and%0Aimproves%20with%20model%20scale.%20Furthermore%2C%20we%20explore%20the%20challenges%20of%0Aintegrating%20MTP%20heads%20into%20frozen%20LLMs%20and%20find%20that%20their%20hidden%20layers%20are%0Astrongly%20specialized%20for%20NTP%2C%20making%20adaptation%20non-trivial.%20Finally%2C%20we%20show%0Athat%20while%20joint%20training%20of%20MTP%20heads%20with%20the%20backbone%20improves%20performance%2C%0Ait%20cannot%20fully%20overcome%20this%20barrier%2C%20prompting%20further%20research%20in%20this%0Adirection.%20Our%20findings%20provide%20a%20deeper%20understanding%20of%20MTP%20applied%20to%0Apretrained%20LLMs%2C%20informing%20strategies%20for%20accelerating%20inference%20through%0Aparallel%20token%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520multi-token%2520prediction%2520for%2520efficient%2520LLM%2520inference%26entry.906535625%3DSomesh%2520Mehra%2520and%2520Javier%2520Alonso%2520Garcia%2520and%2520Lukas%2520Mauch%26entry.1292438233%3D%2520%2520We%2520systematically%2520investigate%2520multi-token%2520prediction%2520%2528MTP%2529%2520capabilities%250Awithin%2520LLMs%2520pre-trained%2520for%2520next-token%2520prediction%2520%2528NTP%2529.%2520We%2520first%2520show%2520that%250Asuch%2520models%2520inherently%2520possess%2520MTP%2520capabilities%2520via%2520numerical%2520marginalization%250Aover%2520intermediate%2520token%2520probabilities%252C%2520though%2520performance%2520is%2520data-dependent%2520and%250Aimproves%2520with%2520model%2520scale.%2520Furthermore%252C%2520we%2520explore%2520the%2520challenges%2520of%250Aintegrating%2520MTP%2520heads%2520into%2520frozen%2520LLMs%2520and%2520find%2520that%2520their%2520hidden%2520layers%2520are%250Astrongly%2520specialized%2520for%2520NTP%252C%2520making%2520adaptation%2520non-trivial.%2520Finally%252C%2520we%2520show%250Athat%2520while%2520joint%2520training%2520of%2520MTP%2520heads%2520with%2520the%2520backbone%2520improves%2520performance%252C%250Ait%2520cannot%2520fully%2520overcome%2520this%2520barrier%252C%2520prompting%2520further%2520research%2520in%2520this%250Adirection.%2520Our%2520findings%2520provide%2520a%2520deeper%2520understanding%2520of%2520MTP%2520applied%2520to%250Apretrained%2520LLMs%252C%2520informing%2520strategies%2520for%2520accelerating%2520inference%2520through%250Aparallel%2520token%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20multi-token%20prediction%20for%20efficient%20LLM%20inference&entry.906535625=Somesh%20Mehra%20and%20Javier%20Alonso%20Garcia%20and%20Lukas%20Mauch&entry.1292438233=%20%20We%20systematically%20investigate%20multi-token%20prediction%20%28MTP%29%20capabilities%0Awithin%20LLMs%20pre-trained%20for%20next-token%20prediction%20%28NTP%29.%20We%20first%20show%20that%0Asuch%20models%20inherently%20possess%20MTP%20capabilities%20via%20numerical%20marginalization%0Aover%20intermediate%20token%20probabilities%2C%20though%20performance%20is%20data-dependent%20and%0Aimproves%20with%20model%20scale.%20Furthermore%2C%20we%20explore%20the%20challenges%20of%0Aintegrating%20MTP%20heads%20into%20frozen%20LLMs%20and%20find%20that%20their%20hidden%20layers%20are%0Astrongly%20specialized%20for%20NTP%2C%20making%20adaptation%20non-trivial.%20Finally%2C%20we%20show%0Athat%20while%20joint%20training%20of%20MTP%20heads%20with%20the%20backbone%20improves%20performance%2C%0Ait%20cannot%20fully%20overcome%20this%20barrier%2C%20prompting%20further%20research%20in%20this%0Adirection.%20Our%20findings%20provide%20a%20deeper%20understanding%20of%20MTP%20applied%20to%0Apretrained%20LLMs%2C%20informing%20strategies%20for%20accelerating%20inference%20through%0Aparallel%20token%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09419v1&entry.124074799=Read"},
{"title": "HorNets: Learning from Discrete and Continuous Signals with Routing\n  Neural Networks", "author": "Boshko Koloski and Nada Lavra\u010d and Bla\u017e \u0160krlj", "abstract": "  Construction of neural network architectures suitable for learning from both\ncontinuous and discrete tabular data is a challenging research endeavor.\nContemporary high-dimensional tabular data sets are often characterized by a\nrelatively small instance count, requiring data-efficient learning. We propose\nHorNets (Horn Networks), a neural network architecture with state-of-the-art\nperformance on synthetic and real-life data sets from scarce-data tabular\ndomains. HorNets are based on a clipped polynomial-like activation function,\nextended by a custom discrete-continuous routing mechanism that decides which\npart of the neural network to optimize based on the input's cardinality. By\nexplicitly modeling parts of the feature combination space or combining whole\nspace in a linear attention-like manner, HorNets dynamically decide which mode\nof operation is the most suitable for a given piece of data with no explicit\nsupervision. This architecture is one of the few approaches that reliably\nretrieves logical clauses (including noisy XNOR) and achieves state-of-the-art\nclassification performance on 14 real-life biomedical high-dimensional data\nsets. HorNets are made freely available under a permissive license alongside a\nsynthetic generator of categorical benchmarks.\n", "link": "http://arxiv.org/abs/2501.14346v2", "date": "2025-02-13", "relevancy": 1.9716, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5191}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4757}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HorNets%3A%20Learning%20from%20Discrete%20and%20Continuous%20Signals%20with%20Routing%0A%20%20Neural%20Networks&body=Title%3A%20HorNets%3A%20Learning%20from%20Discrete%20and%20Continuous%20Signals%20with%20Routing%0A%20%20Neural%20Networks%0AAuthor%3A%20Boshko%20Koloski%20and%20Nada%20Lavra%C4%8D%20and%20Bla%C5%BE%20%C5%A0krlj%0AAbstract%3A%20%20%20Construction%20of%20neural%20network%20architectures%20suitable%20for%20learning%20from%20both%0Acontinuous%20and%20discrete%20tabular%20data%20is%20a%20challenging%20research%20endeavor.%0AContemporary%20high-dimensional%20tabular%20data%20sets%20are%20often%20characterized%20by%20a%0Arelatively%20small%20instance%20count%2C%20requiring%20data-efficient%20learning.%20We%20propose%0AHorNets%20%28Horn%20Networks%29%2C%20a%20neural%20network%20architecture%20with%20state-of-the-art%0Aperformance%20on%20synthetic%20and%20real-life%20data%20sets%20from%20scarce-data%20tabular%0Adomains.%20HorNets%20are%20based%20on%20a%20clipped%20polynomial-like%20activation%20function%2C%0Aextended%20by%20a%20custom%20discrete-continuous%20routing%20mechanism%20that%20decides%20which%0Apart%20of%20the%20neural%20network%20to%20optimize%20based%20on%20the%20input%27s%20cardinality.%20By%0Aexplicitly%20modeling%20parts%20of%20the%20feature%20combination%20space%20or%20combining%20whole%0Aspace%20in%20a%20linear%20attention-like%20manner%2C%20HorNets%20dynamically%20decide%20which%20mode%0Aof%20operation%20is%20the%20most%20suitable%20for%20a%20given%20piece%20of%20data%20with%20no%20explicit%0Asupervision.%20This%20architecture%20is%20one%20of%20the%20few%20approaches%20that%20reliably%0Aretrieves%20logical%20clauses%20%28including%20noisy%20XNOR%29%20and%20achieves%20state-of-the-art%0Aclassification%20performance%20on%2014%20real-life%20biomedical%20high-dimensional%20data%0Asets.%20HorNets%20are%20made%20freely%20available%20under%20a%20permissive%20license%20alongside%20a%0Asynthetic%20generator%20of%20categorical%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHorNets%253A%2520Learning%2520from%2520Discrete%2520and%2520Continuous%2520Signals%2520with%2520Routing%250A%2520%2520Neural%2520Networks%26entry.906535625%3DBoshko%2520Koloski%2520and%2520Nada%2520Lavra%25C4%258D%2520and%2520Bla%25C5%25BE%2520%25C5%25A0krlj%26entry.1292438233%3D%2520%2520Construction%2520of%2520neural%2520network%2520architectures%2520suitable%2520for%2520learning%2520from%2520both%250Acontinuous%2520and%2520discrete%2520tabular%2520data%2520is%2520a%2520challenging%2520research%2520endeavor.%250AContemporary%2520high-dimensional%2520tabular%2520data%2520sets%2520are%2520often%2520characterized%2520by%2520a%250Arelatively%2520small%2520instance%2520count%252C%2520requiring%2520data-efficient%2520learning.%2520We%2520propose%250AHorNets%2520%2528Horn%2520Networks%2529%252C%2520a%2520neural%2520network%2520architecture%2520with%2520state-of-the-art%250Aperformance%2520on%2520synthetic%2520and%2520real-life%2520data%2520sets%2520from%2520scarce-data%2520tabular%250Adomains.%2520HorNets%2520are%2520based%2520on%2520a%2520clipped%2520polynomial-like%2520activation%2520function%252C%250Aextended%2520by%2520a%2520custom%2520discrete-continuous%2520routing%2520mechanism%2520that%2520decides%2520which%250Apart%2520of%2520the%2520neural%2520network%2520to%2520optimize%2520based%2520on%2520the%2520input%2527s%2520cardinality.%2520By%250Aexplicitly%2520modeling%2520parts%2520of%2520the%2520feature%2520combination%2520space%2520or%2520combining%2520whole%250Aspace%2520in%2520a%2520linear%2520attention-like%2520manner%252C%2520HorNets%2520dynamically%2520decide%2520which%2520mode%250Aof%2520operation%2520is%2520the%2520most%2520suitable%2520for%2520a%2520given%2520piece%2520of%2520data%2520with%2520no%2520explicit%250Asupervision.%2520This%2520architecture%2520is%2520one%2520of%2520the%2520few%2520approaches%2520that%2520reliably%250Aretrieves%2520logical%2520clauses%2520%2528including%2520noisy%2520XNOR%2529%2520and%2520achieves%2520state-of-the-art%250Aclassification%2520performance%2520on%252014%2520real-life%2520biomedical%2520high-dimensional%2520data%250Asets.%2520HorNets%2520are%2520made%2520freely%2520available%2520under%2520a%2520permissive%2520license%2520alongside%2520a%250Asynthetic%2520generator%2520of%2520categorical%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HorNets%3A%20Learning%20from%20Discrete%20and%20Continuous%20Signals%20with%20Routing%0A%20%20Neural%20Networks&entry.906535625=Boshko%20Koloski%20and%20Nada%20Lavra%C4%8D%20and%20Bla%C5%BE%20%C5%A0krlj&entry.1292438233=%20%20Construction%20of%20neural%20network%20architectures%20suitable%20for%20learning%20from%20both%0Acontinuous%20and%20discrete%20tabular%20data%20is%20a%20challenging%20research%20endeavor.%0AContemporary%20high-dimensional%20tabular%20data%20sets%20are%20often%20characterized%20by%20a%0Arelatively%20small%20instance%20count%2C%20requiring%20data-efficient%20learning.%20We%20propose%0AHorNets%20%28Horn%20Networks%29%2C%20a%20neural%20network%20architecture%20with%20state-of-the-art%0Aperformance%20on%20synthetic%20and%20real-life%20data%20sets%20from%20scarce-data%20tabular%0Adomains.%20HorNets%20are%20based%20on%20a%20clipped%20polynomial-like%20activation%20function%2C%0Aextended%20by%20a%20custom%20discrete-continuous%20routing%20mechanism%20that%20decides%20which%0Apart%20of%20the%20neural%20network%20to%20optimize%20based%20on%20the%20input%27s%20cardinality.%20By%0Aexplicitly%20modeling%20parts%20of%20the%20feature%20combination%20space%20or%20combining%20whole%0Aspace%20in%20a%20linear%20attention-like%20manner%2C%20HorNets%20dynamically%20decide%20which%20mode%0Aof%20operation%20is%20the%20most%20suitable%20for%20a%20given%20piece%20of%20data%20with%20no%20explicit%0Asupervision.%20This%20architecture%20is%20one%20of%20the%20few%20approaches%20that%20reliably%0Aretrieves%20logical%20clauses%20%28including%20noisy%20XNOR%29%20and%20achieves%20state-of-the-art%0Aclassification%20performance%20on%2014%20real-life%20biomedical%20high-dimensional%20data%0Asets.%20HorNets%20are%20made%20freely%20available%20under%20a%20permissive%20license%20alongside%20a%0Asynthetic%20generator%20of%20categorical%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14346v2&entry.124074799=Read"},
{"title": "Assessing Generative AI value in a public sector context: evidence from\n  a field experiment", "author": "Trevor Fitzpatrick and Seamus Kelly and Patrick Carey and David Walsh and Ruairi Nugent", "abstract": "  The emergence of Generative AI (Gen AI) has motivated an interest in\nunderstanding how it could be used to enhance productivity across various\ntasks. We add to research results for the performance impact of Gen AI on\ncomplex knowledge-based tasks in a public sector setting. In a pre-registered\nexperiment, after establishing a baseline level of performance, we find mixed\nevidence for two types of composite tasks related to document understanding and\ndata analysis. For the Documents task, the treatment group using Gen AI had a\n17% improvement in answer quality scores (as judged by human evaluators) and a\n34% improvement in task completion time compared to a control group. For the\nData task, we find the Gen AI treatment group experienced a 12% reduction in\nquality scores and no significant difference in mean completion time compared\nto the control group. These results suggest that the benefits of Gen AI may be\ntask and potentially respondent dependent. We also discuss field notes and\nlessons learned, as well as supplementary insights from a post-trial survey and\nfeedback workshop with participants.\n", "link": "http://arxiv.org/abs/2502.09479v1", "date": "2025-02-13", "relevancy": 1.9528, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5371}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4661}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Generative%20AI%20value%20in%20a%20public%20sector%20context%3A%20evidence%20from%0A%20%20a%20field%20experiment&body=Title%3A%20Assessing%20Generative%20AI%20value%20in%20a%20public%20sector%20context%3A%20evidence%20from%0A%20%20a%20field%20experiment%0AAuthor%3A%20Trevor%20Fitzpatrick%20and%20Seamus%20Kelly%20and%20Patrick%20Carey%20and%20David%20Walsh%20and%20Ruairi%20Nugent%0AAbstract%3A%20%20%20The%20emergence%20of%20Generative%20AI%20%28Gen%20AI%29%20has%20motivated%20an%20interest%20in%0Aunderstanding%20how%20it%20could%20be%20used%20to%20enhance%20productivity%20across%20various%0Atasks.%20We%20add%20to%20research%20results%20for%20the%20performance%20impact%20of%20Gen%20AI%20on%0Acomplex%20knowledge-based%20tasks%20in%20a%20public%20sector%20setting.%20In%20a%20pre-registered%0Aexperiment%2C%20after%20establishing%20a%20baseline%20level%20of%20performance%2C%20we%20find%20mixed%0Aevidence%20for%20two%20types%20of%20composite%20tasks%20related%20to%20document%20understanding%20and%0Adata%20analysis.%20For%20the%20Documents%20task%2C%20the%20treatment%20group%20using%20Gen%20AI%20had%20a%0A17%25%20improvement%20in%20answer%20quality%20scores%20%28as%20judged%20by%20human%20evaluators%29%20and%20a%0A34%25%20improvement%20in%20task%20completion%20time%20compared%20to%20a%20control%20group.%20For%20the%0AData%20task%2C%20we%20find%20the%20Gen%20AI%20treatment%20group%20experienced%20a%2012%25%20reduction%20in%0Aquality%20scores%20and%20no%20significant%20difference%20in%20mean%20completion%20time%20compared%0Ato%20the%20control%20group.%20These%20results%20suggest%20that%20the%20benefits%20of%20Gen%20AI%20may%20be%0Atask%20and%20potentially%20respondent%20dependent.%20We%20also%20discuss%20field%20notes%20and%0Alessons%20learned%2C%20as%20well%20as%20supplementary%20insights%20from%20a%20post-trial%20survey%20and%0Afeedback%20workshop%20with%20participants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Generative%2520AI%2520value%2520in%2520a%2520public%2520sector%2520context%253A%2520evidence%2520from%250A%2520%2520a%2520field%2520experiment%26entry.906535625%3DTrevor%2520Fitzpatrick%2520and%2520Seamus%2520Kelly%2520and%2520Patrick%2520Carey%2520and%2520David%2520Walsh%2520and%2520Ruairi%2520Nugent%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Generative%2520AI%2520%2528Gen%2520AI%2529%2520has%2520motivated%2520an%2520interest%2520in%250Aunderstanding%2520how%2520it%2520could%2520be%2520used%2520to%2520enhance%2520productivity%2520across%2520various%250Atasks.%2520We%2520add%2520to%2520research%2520results%2520for%2520the%2520performance%2520impact%2520of%2520Gen%2520AI%2520on%250Acomplex%2520knowledge-based%2520tasks%2520in%2520a%2520public%2520sector%2520setting.%2520In%2520a%2520pre-registered%250Aexperiment%252C%2520after%2520establishing%2520a%2520baseline%2520level%2520of%2520performance%252C%2520we%2520find%2520mixed%250Aevidence%2520for%2520two%2520types%2520of%2520composite%2520tasks%2520related%2520to%2520document%2520understanding%2520and%250Adata%2520analysis.%2520For%2520the%2520Documents%2520task%252C%2520the%2520treatment%2520group%2520using%2520Gen%2520AI%2520had%2520a%250A17%2525%2520improvement%2520in%2520answer%2520quality%2520scores%2520%2528as%2520judged%2520by%2520human%2520evaluators%2529%2520and%2520a%250A34%2525%2520improvement%2520in%2520task%2520completion%2520time%2520compared%2520to%2520a%2520control%2520group.%2520For%2520the%250AData%2520task%252C%2520we%2520find%2520the%2520Gen%2520AI%2520treatment%2520group%2520experienced%2520a%252012%2525%2520reduction%2520in%250Aquality%2520scores%2520and%2520no%2520significant%2520difference%2520in%2520mean%2520completion%2520time%2520compared%250Ato%2520the%2520control%2520group.%2520These%2520results%2520suggest%2520that%2520the%2520benefits%2520of%2520Gen%2520AI%2520may%2520be%250Atask%2520and%2520potentially%2520respondent%2520dependent.%2520We%2520also%2520discuss%2520field%2520notes%2520and%250Alessons%2520learned%252C%2520as%2520well%2520as%2520supplementary%2520insights%2520from%2520a%2520post-trial%2520survey%2520and%250Afeedback%2520workshop%2520with%2520participants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Generative%20AI%20value%20in%20a%20public%20sector%20context%3A%20evidence%20from%0A%20%20a%20field%20experiment&entry.906535625=Trevor%20Fitzpatrick%20and%20Seamus%20Kelly%20and%20Patrick%20Carey%20and%20David%20Walsh%20and%20Ruairi%20Nugent&entry.1292438233=%20%20The%20emergence%20of%20Generative%20AI%20%28Gen%20AI%29%20has%20motivated%20an%20interest%20in%0Aunderstanding%20how%20it%20could%20be%20used%20to%20enhance%20productivity%20across%20various%0Atasks.%20We%20add%20to%20research%20results%20for%20the%20performance%20impact%20of%20Gen%20AI%20on%0Acomplex%20knowledge-based%20tasks%20in%20a%20public%20sector%20setting.%20In%20a%20pre-registered%0Aexperiment%2C%20after%20establishing%20a%20baseline%20level%20of%20performance%2C%20we%20find%20mixed%0Aevidence%20for%20two%20types%20of%20composite%20tasks%20related%20to%20document%20understanding%20and%0Adata%20analysis.%20For%20the%20Documents%20task%2C%20the%20treatment%20group%20using%20Gen%20AI%20had%20a%0A17%25%20improvement%20in%20answer%20quality%20scores%20%28as%20judged%20by%20human%20evaluators%29%20and%20a%0A34%25%20improvement%20in%20task%20completion%20time%20compared%20to%20a%20control%20group.%20For%20the%0AData%20task%2C%20we%20find%20the%20Gen%20AI%20treatment%20group%20experienced%20a%2012%25%20reduction%20in%0Aquality%20scores%20and%20no%20significant%20difference%20in%20mean%20completion%20time%20compared%0Ato%20the%20control%20group.%20These%20results%20suggest%20that%20the%20benefits%20of%20Gen%20AI%20may%20be%0Atask%20and%20potentially%20respondent%20dependent.%20We%20also%20discuss%20field%20notes%20and%0Alessons%20learned%2C%20as%20well%20as%20supplementary%20insights%20from%20a%20post-trial%20survey%20and%0Afeedback%20workshop%20with%20participants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09479v1&entry.124074799=Read"},
{"title": "Agent-OM: Leveraging LLM Agents for Ontology Matching", "author": "Zhangcheng Qiang and Weiqing Wang and Kerry Taylor", "abstract": "  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n", "link": "http://arxiv.org/abs/2312.00326v9", "date": "2025-02-13", "relevancy": 1.9401, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent-OM%3A%20Leveraging%20LLM%20Agents%20for%20Ontology%20Matching&body=Title%3A%20Agent-OM%3A%20Leveraging%20LLM%20Agents%20for%20Ontology%20Matching%0AAuthor%3A%20Zhangcheng%20Qiang%20and%20Weiqing%20Wang%20and%20Kerry%20Taylor%0AAbstract%3A%20%20%20Ontology%20matching%20%28OM%29%20enables%20semantic%20interoperability%20between%20different%0Aontologies%20and%20resolves%20their%20conceptual%20heterogeneity%20by%20aligning%20related%0Aentities.%20OM%20systems%20currently%20have%20two%20prevailing%20design%20paradigms%3A%0Aconventional%20knowledge-based%20expert%20systems%20and%20newer%20machine%20learning-based%0Apredictive%20systems.%20While%20large%20language%20models%20%28LLMs%29%20and%20LLM%20agents%20have%0Arevolutionised%20data%20engineering%20and%20have%20been%20applied%20creatively%20in%20many%0Adomains%2C%20their%20potential%20for%20OM%20remains%20underexplored.%20This%20study%20introduces%20a%0Anovel%20agent-powered%20LLM-based%20design%20paradigm%20for%20OM%20systems.%20With%0Aconsideration%20of%20several%20specific%20challenges%20in%20leveraging%20LLM%20agents%20for%20OM%2C%0Awe%20propose%20a%20generic%20framework%2C%20namely%20Agent-OM%20%28Agent%20for%20Ontology%20Matching%29%2C%0Aconsisting%20of%20two%20Siamese%20agents%20for%20retrieval%20and%20matching%2C%20with%20a%20set%20of%20OM%0Atools.%20Our%20framework%20is%20implemented%20in%20a%20proof-of-concept%20system.%20Evaluations%0Aof%20three%20Ontology%20Alignment%20Evaluation%20Initiative%20%28OAEI%29%20tracks%20over%0Astate-of-the-art%20OM%20systems%20show%20that%20our%20system%20can%20achieve%20results%20very%20close%0Ato%20the%20long-standing%20best%20performance%20on%20simple%20OM%20tasks%20and%20can%20significantly%0Aimprove%20the%20performance%20on%20complex%20and%20few-shot%20OM%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00326v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent-OM%253A%2520Leveraging%2520LLM%2520Agents%2520for%2520Ontology%2520Matching%26entry.906535625%3DZhangcheng%2520Qiang%2520and%2520Weiqing%2520Wang%2520and%2520Kerry%2520Taylor%26entry.1292438233%3D%2520%2520Ontology%2520matching%2520%2528OM%2529%2520enables%2520semantic%2520interoperability%2520between%2520different%250Aontologies%2520and%2520resolves%2520their%2520conceptual%2520heterogeneity%2520by%2520aligning%2520related%250Aentities.%2520OM%2520systems%2520currently%2520have%2520two%2520prevailing%2520design%2520paradigms%253A%250Aconventional%2520knowledge-based%2520expert%2520systems%2520and%2520newer%2520machine%2520learning-based%250Apredictive%2520systems.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520LLM%2520agents%2520have%250Arevolutionised%2520data%2520engineering%2520and%2520have%2520been%2520applied%2520creatively%2520in%2520many%250Adomains%252C%2520their%2520potential%2520for%2520OM%2520remains%2520underexplored.%2520This%2520study%2520introduces%2520a%250Anovel%2520agent-powered%2520LLM-based%2520design%2520paradigm%2520for%2520OM%2520systems.%2520With%250Aconsideration%2520of%2520several%2520specific%2520challenges%2520in%2520leveraging%2520LLM%2520agents%2520for%2520OM%252C%250Awe%2520propose%2520a%2520generic%2520framework%252C%2520namely%2520Agent-OM%2520%2528Agent%2520for%2520Ontology%2520Matching%2529%252C%250Aconsisting%2520of%2520two%2520Siamese%2520agents%2520for%2520retrieval%2520and%2520matching%252C%2520with%2520a%2520set%2520of%2520OM%250Atools.%2520Our%2520framework%2520is%2520implemented%2520in%2520a%2520proof-of-concept%2520system.%2520Evaluations%250Aof%2520three%2520Ontology%2520Alignment%2520Evaluation%2520Initiative%2520%2528OAEI%2529%2520tracks%2520over%250Astate-of-the-art%2520OM%2520systems%2520show%2520that%2520our%2520system%2520can%2520achieve%2520results%2520very%2520close%250Ato%2520the%2520long-standing%2520best%2520performance%2520on%2520simple%2520OM%2520tasks%2520and%2520can%2520significantly%250Aimprove%2520the%2520performance%2520on%2520complex%2520and%2520few-shot%2520OM%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00326v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent-OM%3A%20Leveraging%20LLM%20Agents%20for%20Ontology%20Matching&entry.906535625=Zhangcheng%20Qiang%20and%20Weiqing%20Wang%20and%20Kerry%20Taylor&entry.1292438233=%20%20Ontology%20matching%20%28OM%29%20enables%20semantic%20interoperability%20between%20different%0Aontologies%20and%20resolves%20their%20conceptual%20heterogeneity%20by%20aligning%20related%0Aentities.%20OM%20systems%20currently%20have%20two%20prevailing%20design%20paradigms%3A%0Aconventional%20knowledge-based%20expert%20systems%20and%20newer%20machine%20learning-based%0Apredictive%20systems.%20While%20large%20language%20models%20%28LLMs%29%20and%20LLM%20agents%20have%0Arevolutionised%20data%20engineering%20and%20have%20been%20applied%20creatively%20in%20many%0Adomains%2C%20their%20potential%20for%20OM%20remains%20underexplored.%20This%20study%20introduces%20a%0Anovel%20agent-powered%20LLM-based%20design%20paradigm%20for%20OM%20systems.%20With%0Aconsideration%20of%20several%20specific%20challenges%20in%20leveraging%20LLM%20agents%20for%20OM%2C%0Awe%20propose%20a%20generic%20framework%2C%20namely%20Agent-OM%20%28Agent%20for%20Ontology%20Matching%29%2C%0Aconsisting%20of%20two%20Siamese%20agents%20for%20retrieval%20and%20matching%2C%20with%20a%20set%20of%20OM%0Atools.%20Our%20framework%20is%20implemented%20in%20a%20proof-of-concept%20system.%20Evaluations%0Aof%20three%20Ontology%20Alignment%20Evaluation%20Initiative%20%28OAEI%29%20tracks%20over%0Astate-of-the-art%20OM%20systems%20show%20that%20our%20system%20can%20achieve%20results%20very%20close%0Ato%20the%20long-standing%20best%20performance%20on%20simple%20OM%20tasks%20and%20can%20significantly%0Aimprove%20the%20performance%20on%20complex%20and%20few-shot%20OM%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00326v9&entry.124074799=Read"},
{"title": "Robust Learning of Multi-index Models via Iterative Subspace\n  Approximation", "author": "Ilias Diakonikolas and Giannis Iakovidis and Daniel M. Kane and Nikos Zarifis", "abstract": "  We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.\n", "link": "http://arxiv.org/abs/2502.09525v1", "date": "2025-02-13", "relevancy": 1.9355, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Learning%20of%20Multi-index%20Models%20via%20Iterative%20Subspace%0A%20%20Approximation&body=Title%3A%20Robust%20Learning%20of%20Multi-index%20Models%20via%20Iterative%20Subspace%0A%20%20Approximation%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Giannis%20Iakovidis%20and%20Daniel%20M.%20Kane%20and%20Nikos%20Zarifis%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20learning%20Multi-Index%20Models%20%28MIMs%29%20with%20label%20noise%0Aunder%20the%20Gaussian%20distribution.%20A%20%24K%24-MIM%20is%20any%20function%20%24f%24%20that%20only%0Adepends%20on%20a%20%24K%24-dimensional%20subspace.%20We%20focus%20on%20well-behaved%20MIMs%20with%0Afinite%20ranges%20that%20satisfy%20certain%20regularity%20properties.%20Our%20main%20contribution%0Ais%20a%20general%20robust%20learner%20that%20is%20qualitatively%20optimal%20in%20the%20Statistical%0AQuery%20%28SQ%29%20model.%20Our%20algorithm%20iteratively%20constructs%20better%20approximations%20to%0Athe%20defining%20subspace%20by%20computing%20low-degree%20moments%20conditional%20on%20the%0Aprojection%20to%20the%20subspace%20computed%20thus%20far%2C%20and%20adding%20directions%20with%0Arelatively%20large%20empirical%20moments.%20This%20procedure%20efficiently%20finds%20a%20subspace%0A%24V%24%20so%20that%20%24f%28%5Cmathbf%7Bx%7D%29%24%20is%20close%20to%20a%20function%20of%20the%20projection%20of%0A%24%5Cmathbf%7Bx%7D%24%20onto%20%24V%24.%20Conversely%2C%20for%20functions%20for%20which%20these%20conditional%0Amoments%20do%20not%20help%2C%20we%20prove%20an%20SQ%20lower%20bound%20suggesting%20that%20no%20efficient%0Alearner%20exists.%0A%20%20As%20applications%2C%20we%20provide%20faster%20robust%20learners%20for%20the%20following%20concept%0Aclasses%3A%0A%20%20%2A%20%7B%5Cbf%20Multiclass%20Linear%20Classifiers%7D%20We%20give%20a%20constant-factor%20approximate%0Aagnostic%20learner%20with%20sample%20complexity%20%24N%20%3D%20O%28d%29%0A2%5E%7B%5Cmathrm%7Bpoly%7D%28K/%5Cepsilon%29%7D%24%20and%20computational%20complexity%20%24%5Cmathrm%7Bpoly%7D%28N%0A%2Cd%29%24.%20This%20is%20the%20first%20constant-factor%20agnostic%20learner%20for%20this%20class%20whose%0Acomplexity%20is%20a%20fixed-degree%20polynomial%20in%20%24d%24.%0A%20%20%2A%20%7B%5Cbf%20Intersections%20of%20Halfspaces%7D%20We%20give%20an%20approximate%20agnostic%20learner%0Afor%20this%20class%20achieving%200-1%20error%20%24K%20%5Ctilde%7BO%7D%28%5Cmathrm%7BOPT%7D%29%20%2B%20%5Cepsilon%24%20with%0Asample%20complexity%20%24N%3DO%28d%5E2%29%202%5E%7B%5Cmathrm%7Bpoly%7D%28K/%5Cepsilon%29%7D%24%20and%20computational%0Acomplexity%20%24%5Cmathrm%7Bpoly%7D%28N%20%2Cd%29%24.%20This%20is%20the%20first%20agnostic%20learner%20for%20this%0Aclass%20with%20near-linear%20error%20dependence%20and%20complexity%20a%20fixed-degree%0Apolynomial%20in%20%24d%24.%0A%20%20Furthermore%2C%20we%20show%20that%20in%20the%20presence%20of%20random%20classification%20noise%2C%20the%0Acomplexity%20of%20our%20algorithm%20scales%20polynomially%20with%20%241/%5Cepsilon%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Learning%2520of%2520Multi-index%2520Models%2520via%2520Iterative%2520Subspace%250A%2520%2520Approximation%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Giannis%2520Iakovidis%2520and%2520Daniel%2520M.%2520Kane%2520and%2520Nikos%2520Zarifis%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520learning%2520Multi-Index%2520Models%2520%2528MIMs%2529%2520with%2520label%2520noise%250Aunder%2520the%2520Gaussian%2520distribution.%2520A%2520%2524K%2524-MIM%2520is%2520any%2520function%2520%2524f%2524%2520that%2520only%250Adepends%2520on%2520a%2520%2524K%2524-dimensional%2520subspace.%2520We%2520focus%2520on%2520well-behaved%2520MIMs%2520with%250Afinite%2520ranges%2520that%2520satisfy%2520certain%2520regularity%2520properties.%2520Our%2520main%2520contribution%250Ais%2520a%2520general%2520robust%2520learner%2520that%2520is%2520qualitatively%2520optimal%2520in%2520the%2520Statistical%250AQuery%2520%2528SQ%2529%2520model.%2520Our%2520algorithm%2520iteratively%2520constructs%2520better%2520approximations%2520to%250Athe%2520defining%2520subspace%2520by%2520computing%2520low-degree%2520moments%2520conditional%2520on%2520the%250Aprojection%2520to%2520the%2520subspace%2520computed%2520thus%2520far%252C%2520and%2520adding%2520directions%2520with%250Arelatively%2520large%2520empirical%2520moments.%2520This%2520procedure%2520efficiently%2520finds%2520a%2520subspace%250A%2524V%2524%2520so%2520that%2520%2524f%2528%255Cmathbf%257Bx%257D%2529%2524%2520is%2520close%2520to%2520a%2520function%2520of%2520the%2520projection%2520of%250A%2524%255Cmathbf%257Bx%257D%2524%2520onto%2520%2524V%2524.%2520Conversely%252C%2520for%2520functions%2520for%2520which%2520these%2520conditional%250Amoments%2520do%2520not%2520help%252C%2520we%2520prove%2520an%2520SQ%2520lower%2520bound%2520suggesting%2520that%2520no%2520efficient%250Alearner%2520exists.%250A%2520%2520As%2520applications%252C%2520we%2520provide%2520faster%2520robust%2520learners%2520for%2520the%2520following%2520concept%250Aclasses%253A%250A%2520%2520%252A%2520%257B%255Cbf%2520Multiclass%2520Linear%2520Classifiers%257D%2520We%2520give%2520a%2520constant-factor%2520approximate%250Aagnostic%2520learner%2520with%2520sample%2520complexity%2520%2524N%2520%253D%2520O%2528d%2529%250A2%255E%257B%255Cmathrm%257Bpoly%257D%2528K/%255Cepsilon%2529%257D%2524%2520and%2520computational%2520complexity%2520%2524%255Cmathrm%257Bpoly%257D%2528N%250A%252Cd%2529%2524.%2520This%2520is%2520the%2520first%2520constant-factor%2520agnostic%2520learner%2520for%2520this%2520class%2520whose%250Acomplexity%2520is%2520a%2520fixed-degree%2520polynomial%2520in%2520%2524d%2524.%250A%2520%2520%252A%2520%257B%255Cbf%2520Intersections%2520of%2520Halfspaces%257D%2520We%2520give%2520an%2520approximate%2520agnostic%2520learner%250Afor%2520this%2520class%2520achieving%25200-1%2520error%2520%2524K%2520%255Ctilde%257BO%257D%2528%255Cmathrm%257BOPT%257D%2529%2520%252B%2520%255Cepsilon%2524%2520with%250Asample%2520complexity%2520%2524N%253DO%2528d%255E2%2529%25202%255E%257B%255Cmathrm%257Bpoly%257D%2528K/%255Cepsilon%2529%257D%2524%2520and%2520computational%250Acomplexity%2520%2524%255Cmathrm%257Bpoly%257D%2528N%2520%252Cd%2529%2524.%2520This%2520is%2520the%2520first%2520agnostic%2520learner%2520for%2520this%250Aclass%2520with%2520near-linear%2520error%2520dependence%2520and%2520complexity%2520a%2520fixed-degree%250Apolynomial%2520in%2520%2524d%2524.%250A%2520%2520Furthermore%252C%2520we%2520show%2520that%2520in%2520the%2520presence%2520of%2520random%2520classification%2520noise%252C%2520the%250Acomplexity%2520of%2520our%2520algorithm%2520scales%2520polynomially%2520with%2520%25241/%255Cepsilon%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Learning%20of%20Multi-index%20Models%20via%20Iterative%20Subspace%0A%20%20Approximation&entry.906535625=Ilias%20Diakonikolas%20and%20Giannis%20Iakovidis%20and%20Daniel%20M.%20Kane%20and%20Nikos%20Zarifis&entry.1292438233=%20%20We%20study%20the%20task%20of%20learning%20Multi-Index%20Models%20%28MIMs%29%20with%20label%20noise%0Aunder%20the%20Gaussian%20distribution.%20A%20%24K%24-MIM%20is%20any%20function%20%24f%24%20that%20only%0Adepends%20on%20a%20%24K%24-dimensional%20subspace.%20We%20focus%20on%20well-behaved%20MIMs%20with%0Afinite%20ranges%20that%20satisfy%20certain%20regularity%20properties.%20Our%20main%20contribution%0Ais%20a%20general%20robust%20learner%20that%20is%20qualitatively%20optimal%20in%20the%20Statistical%0AQuery%20%28SQ%29%20model.%20Our%20algorithm%20iteratively%20constructs%20better%20approximations%20to%0Athe%20defining%20subspace%20by%20computing%20low-degree%20moments%20conditional%20on%20the%0Aprojection%20to%20the%20subspace%20computed%20thus%20far%2C%20and%20adding%20directions%20with%0Arelatively%20large%20empirical%20moments.%20This%20procedure%20efficiently%20finds%20a%20subspace%0A%24V%24%20so%20that%20%24f%28%5Cmathbf%7Bx%7D%29%24%20is%20close%20to%20a%20function%20of%20the%20projection%20of%0A%24%5Cmathbf%7Bx%7D%24%20onto%20%24V%24.%20Conversely%2C%20for%20functions%20for%20which%20these%20conditional%0Amoments%20do%20not%20help%2C%20we%20prove%20an%20SQ%20lower%20bound%20suggesting%20that%20no%20efficient%0Alearner%20exists.%0A%20%20As%20applications%2C%20we%20provide%20faster%20robust%20learners%20for%20the%20following%20concept%0Aclasses%3A%0A%20%20%2A%20%7B%5Cbf%20Multiclass%20Linear%20Classifiers%7D%20We%20give%20a%20constant-factor%20approximate%0Aagnostic%20learner%20with%20sample%20complexity%20%24N%20%3D%20O%28d%29%0A2%5E%7B%5Cmathrm%7Bpoly%7D%28K/%5Cepsilon%29%7D%24%20and%20computational%20complexity%20%24%5Cmathrm%7Bpoly%7D%28N%0A%2Cd%29%24.%20This%20is%20the%20first%20constant-factor%20agnostic%20learner%20for%20this%20class%20whose%0Acomplexity%20is%20a%20fixed-degree%20polynomial%20in%20%24d%24.%0A%20%20%2A%20%7B%5Cbf%20Intersections%20of%20Halfspaces%7D%20We%20give%20an%20approximate%20agnostic%20learner%0Afor%20this%20class%20achieving%200-1%20error%20%24K%20%5Ctilde%7BO%7D%28%5Cmathrm%7BOPT%7D%29%20%2B%20%5Cepsilon%24%20with%0Asample%20complexity%20%24N%3DO%28d%5E2%29%202%5E%7B%5Cmathrm%7Bpoly%7D%28K/%5Cepsilon%29%7D%24%20and%20computational%0Acomplexity%20%24%5Cmathrm%7Bpoly%7D%28N%20%2Cd%29%24.%20This%20is%20the%20first%20agnostic%20learner%20for%20this%0Aclass%20with%20near-linear%20error%20dependence%20and%20complexity%20a%20fixed-degree%0Apolynomial%20in%20%24d%24.%0A%20%20Furthermore%2C%20we%20show%20that%20in%20the%20presence%20of%20random%20classification%20noise%2C%20the%0Acomplexity%20of%20our%20algorithm%20scales%20polynomially%20with%20%241/%5Cepsilon%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09525v1&entry.124074799=Read"},
{"title": "MorphNLI: A Stepwise Approach to Natural Language Inference Using Text\n  Morphing", "author": "Vlad Andrei Negru and Robert Vacareanu and Camelia Lemnaru and Mihai Surdeanu and Rodica Potolea", "abstract": "  We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label.\n", "link": "http://arxiv.org/abs/2502.09567v1", "date": "2025-02-13", "relevancy": 1.9261, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4889}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MorphNLI%3A%20A%20Stepwise%20Approach%20to%20Natural%20Language%20Inference%20Using%20Text%0A%20%20Morphing&body=Title%3A%20MorphNLI%3A%20A%20Stepwise%20Approach%20to%20Natural%20Language%20Inference%20Using%20Text%0A%20%20Morphing%0AAuthor%3A%20Vlad%20Andrei%20Negru%20and%20Robert%20Vacareanu%20and%20Camelia%20Lemnaru%20and%20Mihai%20Surdeanu%20and%20Rodica%20Potolea%0AAbstract%3A%20%20%20We%20introduce%20MorphNLI%2C%20a%20modular%20step-by-step%20approach%20to%20natural%20language%0Ainference%20%28NLI%29.%20When%20classifying%20the%20premise-hypothesis%20pairs%20into%0A%7Bentailment%2C%20contradiction%2C%20neutral%7D%2C%20we%20use%20a%20language%20model%20to%20generate%20the%0Anecessary%20edits%20to%20incrementally%20transform%20%28i.e.%2C%20morph%29%20the%20premise%20into%20the%0Ahypothesis.%20Then%2C%20using%20an%20off-the-shelf%20NLI%20model%20we%20track%20how%20the%20entailment%0Aprogresses%20with%20these%20atomic%20changes%2C%20aggregating%20these%20intermediate%20labels%0Ainto%20a%20final%20output.%20We%20demonstrate%20the%20advantages%20of%20our%20proposed%20method%0Aparticularly%20in%20realistic%20cross-domain%20settings%2C%20where%20our%20method%20always%0Aoutperforms%20strong%20baselines%20with%20improvements%20up%20to%2012.6%25%20%28relative%29.%20Further%2C%0Aour%20proposed%20approach%20is%20explainable%20as%20the%20atomic%20edits%20can%20be%20used%20to%0Aunderstand%20the%20overall%20NLI%20label.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphNLI%253A%2520A%2520Stepwise%2520Approach%2520to%2520Natural%2520Language%2520Inference%2520Using%2520Text%250A%2520%2520Morphing%26entry.906535625%3DVlad%2520Andrei%2520Negru%2520and%2520Robert%2520Vacareanu%2520and%2520Camelia%2520Lemnaru%2520and%2520Mihai%2520Surdeanu%2520and%2520Rodica%2520Potolea%26entry.1292438233%3D%2520%2520We%2520introduce%2520MorphNLI%252C%2520a%2520modular%2520step-by-step%2520approach%2520to%2520natural%2520language%250Ainference%2520%2528NLI%2529.%2520When%2520classifying%2520the%2520premise-hypothesis%2520pairs%2520into%250A%257Bentailment%252C%2520contradiction%252C%2520neutral%257D%252C%2520we%2520use%2520a%2520language%2520model%2520to%2520generate%2520the%250Anecessary%2520edits%2520to%2520incrementally%2520transform%2520%2528i.e.%252C%2520morph%2529%2520the%2520premise%2520into%2520the%250Ahypothesis.%2520Then%252C%2520using%2520an%2520off-the-shelf%2520NLI%2520model%2520we%2520track%2520how%2520the%2520entailment%250Aprogresses%2520with%2520these%2520atomic%2520changes%252C%2520aggregating%2520these%2520intermediate%2520labels%250Ainto%2520a%2520final%2520output.%2520We%2520demonstrate%2520the%2520advantages%2520of%2520our%2520proposed%2520method%250Aparticularly%2520in%2520realistic%2520cross-domain%2520settings%252C%2520where%2520our%2520method%2520always%250Aoutperforms%2520strong%2520baselines%2520with%2520improvements%2520up%2520to%252012.6%2525%2520%2528relative%2529.%2520Further%252C%250Aour%2520proposed%2520approach%2520is%2520explainable%2520as%2520the%2520atomic%2520edits%2520can%2520be%2520used%2520to%250Aunderstand%2520the%2520overall%2520NLI%2520label.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MorphNLI%3A%20A%20Stepwise%20Approach%20to%20Natural%20Language%20Inference%20Using%20Text%0A%20%20Morphing&entry.906535625=Vlad%20Andrei%20Negru%20and%20Robert%20Vacareanu%20and%20Camelia%20Lemnaru%20and%20Mihai%20Surdeanu%20and%20Rodica%20Potolea&entry.1292438233=%20%20We%20introduce%20MorphNLI%2C%20a%20modular%20step-by-step%20approach%20to%20natural%20language%0Ainference%20%28NLI%29.%20When%20classifying%20the%20premise-hypothesis%20pairs%20into%0A%7Bentailment%2C%20contradiction%2C%20neutral%7D%2C%20we%20use%20a%20language%20model%20to%20generate%20the%0Anecessary%20edits%20to%20incrementally%20transform%20%28i.e.%2C%20morph%29%20the%20premise%20into%20the%0Ahypothesis.%20Then%2C%20using%20an%20off-the-shelf%20NLI%20model%20we%20track%20how%20the%20entailment%0Aprogresses%20with%20these%20atomic%20changes%2C%20aggregating%20these%20intermediate%20labels%0Ainto%20a%20final%20output.%20We%20demonstrate%20the%20advantages%20of%20our%20proposed%20method%0Aparticularly%20in%20realistic%20cross-domain%20settings%2C%20where%20our%20method%20always%0Aoutperforms%20strong%20baselines%20with%20improvements%20up%20to%2012.6%25%20%28relative%29.%20Further%2C%0Aour%20proposed%20approach%20is%20explainable%20as%20the%20atomic%20edits%20can%20be%20used%20to%0Aunderstand%20the%20overall%20NLI%20label.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09567v1&entry.124074799=Read"},
{"title": "On Agnostic PAC Learning in the Small Error Regime", "author": "Julian Asilis and Mikael M\u00f8ller H\u00f8gsgaard and Grigoris Velegkas", "abstract": "  Binary classification in the classic PAC model exhibits a curious phenomenon:\nEmpirical Risk Minimization (ERM) learners are suboptimal in the realizable\ncase yet optimal in the agnostic case. Roughly speaking, this owes itself to\nthe fact that non-realizable distributions $\\mathcal{D}$ are simply more\ndifficult to learn than realizable distributions -- even when one discounts a\nlearner's error by $\\mathrm{err}(h^*_{\\mathcal{D}})$, the error of the best\nhypothesis in $\\mathcal{H}$ for $\\mathcal{D}$. Thus, optimal agnostic learners\nare permitted to incur excess error on (easier-to-learn) distributions\n$\\mathcal{D}$ for which $\\tau = \\mathrm{err}(h^*_{\\mathcal{D}})$ is small.\n  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this\nshortcoming by including $\\tau$ itself as a parameter in the agnostic error\nterm. In this more fine-grained model, they demonstrate tightness of the error\nlower bound $\\tau + \\Omega \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} +\n\\frac{d + \\log(1 / \\delta)}{m} \\right)$ in a regime where $\\tau > d/m$, and\nleave open the question of whether there may be a higher lower bound when $\\tau\n\\approx d/m$, with $d$ denoting $\\mathrm{VC}(\\mathcal{H})$. In this work, we\nresolve this question by exhibiting a learner which achieves error $c \\cdot\n\\tau + O \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} + \\frac{d + \\log(1\n/ \\delta)}{m} \\right)$ for a constant $c \\leq 2.1$, thus matching the lower\nbound when $\\tau \\approx d/m$. Further, our learner is computationally\nefficient and is based upon careful aggregations of ERM classifiers, making\nprogress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS\n`24). We leave open the interesting question of whether our approach can be\nrefined to lower the constant from 2.1 to 1, which would completely settle the\ncomplexity of agnostic learning.\n", "link": "http://arxiv.org/abs/2502.09496v1", "date": "2025-02-13", "relevancy": 1.9257, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4965}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4762}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Agnostic%20PAC%20Learning%20in%20the%20Small%20Error%20Regime&body=Title%3A%20On%20Agnostic%20PAC%20Learning%20in%20the%20Small%20Error%20Regime%0AAuthor%3A%20Julian%20Asilis%20and%20Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Grigoris%20Velegkas%0AAbstract%3A%20%20%20Binary%20classification%20in%20the%20classic%20PAC%20model%20exhibits%20a%20curious%20phenomenon%3A%0AEmpirical%20Risk%20Minimization%20%28ERM%29%20learners%20are%20suboptimal%20in%20the%20realizable%0Acase%20yet%20optimal%20in%20the%20agnostic%20case.%20Roughly%20speaking%2C%20this%20owes%20itself%20to%0Athe%20fact%20that%20non-realizable%20distributions%20%24%5Cmathcal%7BD%7D%24%20are%20simply%20more%0Adifficult%20to%20learn%20than%20realizable%20distributions%20--%20even%20when%20one%20discounts%20a%0Alearner%27s%20error%20by%20%24%5Cmathrm%7Berr%7D%28h%5E%2A_%7B%5Cmathcal%7BD%7D%7D%29%24%2C%20the%20error%20of%20the%20best%0Ahypothesis%20in%20%24%5Cmathcal%7BH%7D%24%20for%20%24%5Cmathcal%7BD%7D%24.%20Thus%2C%20optimal%20agnostic%20learners%0Aare%20permitted%20to%20incur%20excess%20error%20on%20%28easier-to-learn%29%20distributions%0A%24%5Cmathcal%7BD%7D%24%20for%20which%20%24%5Ctau%20%3D%20%5Cmathrm%7Berr%7D%28h%5E%2A_%7B%5Cmathcal%7BD%7D%7D%29%24%20is%20small.%0A%20%20Recent%20work%20of%20Hanneke%2C%20Larsen%2C%20and%20Zhivotovskiy%20%28FOCS%20%6024%29%20addresses%20this%0Ashortcoming%20by%20including%20%24%5Ctau%24%20itself%20as%20a%20parameter%20in%20the%20agnostic%20error%0Aterm.%20In%20this%20more%20fine-grained%20model%2C%20they%20demonstrate%20tightness%20of%20the%20error%0Alower%20bound%20%24%5Ctau%20%2B%20%5COmega%20%5Cleft%28%5Csqrt%7B%5Cfrac%7B%5Ctau%20%28d%20%2B%20%5Clog%281%20/%20%5Cdelta%29%29%7D%7Bm%7D%7D%20%2B%0A%5Cfrac%7Bd%20%2B%20%5Clog%281%20/%20%5Cdelta%29%7D%7Bm%7D%20%5Cright%29%24%20in%20a%20regime%20where%20%24%5Ctau%20%3E%20d/m%24%2C%20and%0Aleave%20open%20the%20question%20of%20whether%20there%20may%20be%20a%20higher%20lower%20bound%20when%20%24%5Ctau%0A%5Capprox%20d/m%24%2C%20with%20%24d%24%20denoting%20%24%5Cmathrm%7BVC%7D%28%5Cmathcal%7BH%7D%29%24.%20In%20this%20work%2C%20we%0Aresolve%20this%20question%20by%20exhibiting%20a%20learner%20which%20achieves%20error%20%24c%20%5Ccdot%0A%5Ctau%20%2B%20O%20%5Cleft%28%5Csqrt%7B%5Cfrac%7B%5Ctau%20%28d%20%2B%20%5Clog%281%20/%20%5Cdelta%29%29%7D%7Bm%7D%7D%20%2B%20%5Cfrac%7Bd%20%2B%20%5Clog%281%0A/%20%5Cdelta%29%7D%7Bm%7D%20%5Cright%29%24%20for%20a%20constant%20%24c%20%5Cleq%202.1%24%2C%20thus%20matching%20the%20lower%0Abound%20when%20%24%5Ctau%20%5Capprox%20d/m%24.%20Further%2C%20our%20learner%20is%20computationally%0Aefficient%20and%20is%20based%20upon%20careful%20aggregations%20of%20ERM%20classifiers%2C%20making%0Aprogress%20on%20two%20other%20questions%20of%20Hanneke%2C%20Larsen%2C%20and%20Zhivotovskiy%20%28FOCS%0A%6024%29.%20We%20leave%20open%20the%20interesting%20question%20of%20whether%20our%20approach%20can%20be%0Arefined%20to%20lower%20the%20constant%20from%202.1%20to%201%2C%20which%20would%20completely%20settle%20the%0Acomplexity%20of%20agnostic%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Agnostic%2520PAC%2520Learning%2520in%2520the%2520Small%2520Error%2520Regime%26entry.906535625%3DJulian%2520Asilis%2520and%2520Mikael%2520M%25C3%25B8ller%2520H%25C3%25B8gsgaard%2520and%2520Grigoris%2520Velegkas%26entry.1292438233%3D%2520%2520Binary%2520classification%2520in%2520the%2520classic%2520PAC%2520model%2520exhibits%2520a%2520curious%2520phenomenon%253A%250AEmpirical%2520Risk%2520Minimization%2520%2528ERM%2529%2520learners%2520are%2520suboptimal%2520in%2520the%2520realizable%250Acase%2520yet%2520optimal%2520in%2520the%2520agnostic%2520case.%2520Roughly%2520speaking%252C%2520this%2520owes%2520itself%2520to%250Athe%2520fact%2520that%2520non-realizable%2520distributions%2520%2524%255Cmathcal%257BD%257D%2524%2520are%2520simply%2520more%250Adifficult%2520to%2520learn%2520than%2520realizable%2520distributions%2520--%2520even%2520when%2520one%2520discounts%2520a%250Alearner%2527s%2520error%2520by%2520%2524%255Cmathrm%257Berr%257D%2528h%255E%252A_%257B%255Cmathcal%257BD%257D%257D%2529%2524%252C%2520the%2520error%2520of%2520the%2520best%250Ahypothesis%2520in%2520%2524%255Cmathcal%257BH%257D%2524%2520for%2520%2524%255Cmathcal%257BD%257D%2524.%2520Thus%252C%2520optimal%2520agnostic%2520learners%250Aare%2520permitted%2520to%2520incur%2520excess%2520error%2520on%2520%2528easier-to-learn%2529%2520distributions%250A%2524%255Cmathcal%257BD%257D%2524%2520for%2520which%2520%2524%255Ctau%2520%253D%2520%255Cmathrm%257Berr%257D%2528h%255E%252A_%257B%255Cmathcal%257BD%257D%257D%2529%2524%2520is%2520small.%250A%2520%2520Recent%2520work%2520of%2520Hanneke%252C%2520Larsen%252C%2520and%2520Zhivotovskiy%2520%2528FOCS%2520%256024%2529%2520addresses%2520this%250Ashortcoming%2520by%2520including%2520%2524%255Ctau%2524%2520itself%2520as%2520a%2520parameter%2520in%2520the%2520agnostic%2520error%250Aterm.%2520In%2520this%2520more%2520fine-grained%2520model%252C%2520they%2520demonstrate%2520tightness%2520of%2520the%2520error%250Alower%2520bound%2520%2524%255Ctau%2520%252B%2520%255COmega%2520%255Cleft%2528%255Csqrt%257B%255Cfrac%257B%255Ctau%2520%2528d%2520%252B%2520%255Clog%25281%2520/%2520%255Cdelta%2529%2529%257D%257Bm%257D%257D%2520%252B%250A%255Cfrac%257Bd%2520%252B%2520%255Clog%25281%2520/%2520%255Cdelta%2529%257D%257Bm%257D%2520%255Cright%2529%2524%2520in%2520a%2520regime%2520where%2520%2524%255Ctau%2520%253E%2520d/m%2524%252C%2520and%250Aleave%2520open%2520the%2520question%2520of%2520whether%2520there%2520may%2520be%2520a%2520higher%2520lower%2520bound%2520when%2520%2524%255Ctau%250A%255Capprox%2520d/m%2524%252C%2520with%2520%2524d%2524%2520denoting%2520%2524%255Cmathrm%257BVC%257D%2528%255Cmathcal%257BH%257D%2529%2524.%2520In%2520this%2520work%252C%2520we%250Aresolve%2520this%2520question%2520by%2520exhibiting%2520a%2520learner%2520which%2520achieves%2520error%2520%2524c%2520%255Ccdot%250A%255Ctau%2520%252B%2520O%2520%255Cleft%2528%255Csqrt%257B%255Cfrac%257B%255Ctau%2520%2528d%2520%252B%2520%255Clog%25281%2520/%2520%255Cdelta%2529%2529%257D%257Bm%257D%257D%2520%252B%2520%255Cfrac%257Bd%2520%252B%2520%255Clog%25281%250A/%2520%255Cdelta%2529%257D%257Bm%257D%2520%255Cright%2529%2524%2520for%2520a%2520constant%2520%2524c%2520%255Cleq%25202.1%2524%252C%2520thus%2520matching%2520the%2520lower%250Abound%2520when%2520%2524%255Ctau%2520%255Capprox%2520d/m%2524.%2520Further%252C%2520our%2520learner%2520is%2520computationally%250Aefficient%2520and%2520is%2520based%2520upon%2520careful%2520aggregations%2520of%2520ERM%2520classifiers%252C%2520making%250Aprogress%2520on%2520two%2520other%2520questions%2520of%2520Hanneke%252C%2520Larsen%252C%2520and%2520Zhivotovskiy%2520%2528FOCS%250A%256024%2529.%2520We%2520leave%2520open%2520the%2520interesting%2520question%2520of%2520whether%2520our%2520approach%2520can%2520be%250Arefined%2520to%2520lower%2520the%2520constant%2520from%25202.1%2520to%25201%252C%2520which%2520would%2520completely%2520settle%2520the%250Acomplexity%2520of%2520agnostic%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Agnostic%20PAC%20Learning%20in%20the%20Small%20Error%20Regime&entry.906535625=Julian%20Asilis%20and%20Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Grigoris%20Velegkas&entry.1292438233=%20%20Binary%20classification%20in%20the%20classic%20PAC%20model%20exhibits%20a%20curious%20phenomenon%3A%0AEmpirical%20Risk%20Minimization%20%28ERM%29%20learners%20are%20suboptimal%20in%20the%20realizable%0Acase%20yet%20optimal%20in%20the%20agnostic%20case.%20Roughly%20speaking%2C%20this%20owes%20itself%20to%0Athe%20fact%20that%20non-realizable%20distributions%20%24%5Cmathcal%7BD%7D%24%20are%20simply%20more%0Adifficult%20to%20learn%20than%20realizable%20distributions%20--%20even%20when%20one%20discounts%20a%0Alearner%27s%20error%20by%20%24%5Cmathrm%7Berr%7D%28h%5E%2A_%7B%5Cmathcal%7BD%7D%7D%29%24%2C%20the%20error%20of%20the%20best%0Ahypothesis%20in%20%24%5Cmathcal%7BH%7D%24%20for%20%24%5Cmathcal%7BD%7D%24.%20Thus%2C%20optimal%20agnostic%20learners%0Aare%20permitted%20to%20incur%20excess%20error%20on%20%28easier-to-learn%29%20distributions%0A%24%5Cmathcal%7BD%7D%24%20for%20which%20%24%5Ctau%20%3D%20%5Cmathrm%7Berr%7D%28h%5E%2A_%7B%5Cmathcal%7BD%7D%7D%29%24%20is%20small.%0A%20%20Recent%20work%20of%20Hanneke%2C%20Larsen%2C%20and%20Zhivotovskiy%20%28FOCS%20%6024%29%20addresses%20this%0Ashortcoming%20by%20including%20%24%5Ctau%24%20itself%20as%20a%20parameter%20in%20the%20agnostic%20error%0Aterm.%20In%20this%20more%20fine-grained%20model%2C%20they%20demonstrate%20tightness%20of%20the%20error%0Alower%20bound%20%24%5Ctau%20%2B%20%5COmega%20%5Cleft%28%5Csqrt%7B%5Cfrac%7B%5Ctau%20%28d%20%2B%20%5Clog%281%20/%20%5Cdelta%29%29%7D%7Bm%7D%7D%20%2B%0A%5Cfrac%7Bd%20%2B%20%5Clog%281%20/%20%5Cdelta%29%7D%7Bm%7D%20%5Cright%29%24%20in%20a%20regime%20where%20%24%5Ctau%20%3E%20d/m%24%2C%20and%0Aleave%20open%20the%20question%20of%20whether%20there%20may%20be%20a%20higher%20lower%20bound%20when%20%24%5Ctau%0A%5Capprox%20d/m%24%2C%20with%20%24d%24%20denoting%20%24%5Cmathrm%7BVC%7D%28%5Cmathcal%7BH%7D%29%24.%20In%20this%20work%2C%20we%0Aresolve%20this%20question%20by%20exhibiting%20a%20learner%20which%20achieves%20error%20%24c%20%5Ccdot%0A%5Ctau%20%2B%20O%20%5Cleft%28%5Csqrt%7B%5Cfrac%7B%5Ctau%20%28d%20%2B%20%5Clog%281%20/%20%5Cdelta%29%29%7D%7Bm%7D%7D%20%2B%20%5Cfrac%7Bd%20%2B%20%5Clog%281%0A/%20%5Cdelta%29%7D%7Bm%7D%20%5Cright%29%24%20for%20a%20constant%20%24c%20%5Cleq%202.1%24%2C%20thus%20matching%20the%20lower%0Abound%20when%20%24%5Ctau%20%5Capprox%20d/m%24.%20Further%2C%20our%20learner%20is%20computationally%0Aefficient%20and%20is%20based%20upon%20careful%20aggregations%20of%20ERM%20classifiers%2C%20making%0Aprogress%20on%20two%20other%20questions%20of%20Hanneke%2C%20Larsen%2C%20and%20Zhivotovskiy%20%28FOCS%0A%6024%29.%20We%20leave%20open%20the%20interesting%20question%20of%20whether%20our%20approach%20can%20be%0Arefined%20to%20lower%20the%20constant%20from%202.1%20to%201%2C%20which%20would%20completely%20settle%20the%0Acomplexity%20of%20agnostic%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09496v1&entry.124074799=Read"},
{"title": "OGBench: Benchmarking Offline Goal-Conditioned RL", "author": "Seohong Park and Kevin Frans and Benjamin Eysenbach and Sergey Levine", "abstract": "  Offline goal-conditioned reinforcement learning (GCRL) is a major problem in\nreinforcement learning (RL) because it provides a simple, unsupervised, and\ndomain-agnostic way to acquire diverse behaviors and representations from\nunlabeled data without rewards. Despite the importance of this setting, we lack\na standard benchmark that can systematically evaluate the capabilities of\noffline GCRL algorithms. In this work, we propose OGBench, a new, high-quality\nbenchmark for algorithms research in offline goal-conditioned RL. OGBench\nconsists of 8 types of environments, 85 datasets, and reference implementations\nof 6 representative offline GCRL algorithms. We have designed these challenging\nand realistic environments and datasets to directly probe different\ncapabilities of algorithms, such as stitching, long-horizon reasoning, and the\nability to handle high-dimensional inputs and stochasticity. While\nrepresentative algorithms may rank similarly on prior benchmarks, our\nexperiments reveal stark strengths and weaknesses in these different\ncapabilities, providing a strong foundation for building new algorithms.\nProject page: https://seohong.me/projects/ogbench\n", "link": "http://arxiv.org/abs/2410.20092v2", "date": "2025-02-13", "relevancy": 1.911, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4937}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4873}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OGBench%3A%20Benchmarking%20Offline%20Goal-Conditioned%20RL&body=Title%3A%20OGBench%3A%20Benchmarking%20Offline%20Goal-Conditioned%20RL%0AAuthor%3A%20Seohong%20Park%20and%20Kevin%20Frans%20and%20Benjamin%20Eysenbach%20and%20Sergey%20Levine%0AAbstract%3A%20%20%20Offline%20goal-conditioned%20reinforcement%20learning%20%28GCRL%29%20is%20a%20major%20problem%20in%0Areinforcement%20learning%20%28RL%29%20because%20it%20provides%20a%20simple%2C%20unsupervised%2C%20and%0Adomain-agnostic%20way%20to%20acquire%20diverse%20behaviors%20and%20representations%20from%0Aunlabeled%20data%20without%20rewards.%20Despite%20the%20importance%20of%20this%20setting%2C%20we%20lack%0Aa%20standard%20benchmark%20that%20can%20systematically%20evaluate%20the%20capabilities%20of%0Aoffline%20GCRL%20algorithms.%20In%20this%20work%2C%20we%20propose%20OGBench%2C%20a%20new%2C%20high-quality%0Abenchmark%20for%20algorithms%20research%20in%20offline%20goal-conditioned%20RL.%20OGBench%0Aconsists%20of%208%20types%20of%20environments%2C%2085%20datasets%2C%20and%20reference%20implementations%0Aof%206%20representative%20offline%20GCRL%20algorithms.%20We%20have%20designed%20these%20challenging%0Aand%20realistic%20environments%20and%20datasets%20to%20directly%20probe%20different%0Acapabilities%20of%20algorithms%2C%20such%20as%20stitching%2C%20long-horizon%20reasoning%2C%20and%20the%0Aability%20to%20handle%20high-dimensional%20inputs%20and%20stochasticity.%20While%0Arepresentative%20algorithms%20may%20rank%20similarly%20on%20prior%20benchmarks%2C%20our%0Aexperiments%20reveal%20stark%20strengths%20and%20weaknesses%20in%20these%20different%0Acapabilities%2C%20providing%20a%20strong%20foundation%20for%20building%20new%20algorithms.%0AProject%20page%3A%20https%3A//seohong.me/projects/ogbench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOGBench%253A%2520Benchmarking%2520Offline%2520Goal-Conditioned%2520RL%26entry.906535625%3DSeohong%2520Park%2520and%2520Kevin%2520Frans%2520and%2520Benjamin%2520Eysenbach%2520and%2520Sergey%2520Levine%26entry.1292438233%3D%2520%2520Offline%2520goal-conditioned%2520reinforcement%2520learning%2520%2528GCRL%2529%2520is%2520a%2520major%2520problem%2520in%250Areinforcement%2520learning%2520%2528RL%2529%2520because%2520it%2520provides%2520a%2520simple%252C%2520unsupervised%252C%2520and%250Adomain-agnostic%2520way%2520to%2520acquire%2520diverse%2520behaviors%2520and%2520representations%2520from%250Aunlabeled%2520data%2520without%2520rewards.%2520Despite%2520the%2520importance%2520of%2520this%2520setting%252C%2520we%2520lack%250Aa%2520standard%2520benchmark%2520that%2520can%2520systematically%2520evaluate%2520the%2520capabilities%2520of%250Aoffline%2520GCRL%2520algorithms.%2520In%2520this%2520work%252C%2520we%2520propose%2520OGBench%252C%2520a%2520new%252C%2520high-quality%250Abenchmark%2520for%2520algorithms%2520research%2520in%2520offline%2520goal-conditioned%2520RL.%2520OGBench%250Aconsists%2520of%25208%2520types%2520of%2520environments%252C%252085%2520datasets%252C%2520and%2520reference%2520implementations%250Aof%25206%2520representative%2520offline%2520GCRL%2520algorithms.%2520We%2520have%2520designed%2520these%2520challenging%250Aand%2520realistic%2520environments%2520and%2520datasets%2520to%2520directly%2520probe%2520different%250Acapabilities%2520of%2520algorithms%252C%2520such%2520as%2520stitching%252C%2520long-horizon%2520reasoning%252C%2520and%2520the%250Aability%2520to%2520handle%2520high-dimensional%2520inputs%2520and%2520stochasticity.%2520While%250Arepresentative%2520algorithms%2520may%2520rank%2520similarly%2520on%2520prior%2520benchmarks%252C%2520our%250Aexperiments%2520reveal%2520stark%2520strengths%2520and%2520weaknesses%2520in%2520these%2520different%250Acapabilities%252C%2520providing%2520a%2520strong%2520foundation%2520for%2520building%2520new%2520algorithms.%250AProject%2520page%253A%2520https%253A//seohong.me/projects/ogbench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OGBench%3A%20Benchmarking%20Offline%20Goal-Conditioned%20RL&entry.906535625=Seohong%20Park%20and%20Kevin%20Frans%20and%20Benjamin%20Eysenbach%20and%20Sergey%20Levine&entry.1292438233=%20%20Offline%20goal-conditioned%20reinforcement%20learning%20%28GCRL%29%20is%20a%20major%20problem%20in%0Areinforcement%20learning%20%28RL%29%20because%20it%20provides%20a%20simple%2C%20unsupervised%2C%20and%0Adomain-agnostic%20way%20to%20acquire%20diverse%20behaviors%20and%20representations%20from%0Aunlabeled%20data%20without%20rewards.%20Despite%20the%20importance%20of%20this%20setting%2C%20we%20lack%0Aa%20standard%20benchmark%20that%20can%20systematically%20evaluate%20the%20capabilities%20of%0Aoffline%20GCRL%20algorithms.%20In%20this%20work%2C%20we%20propose%20OGBench%2C%20a%20new%2C%20high-quality%0Abenchmark%20for%20algorithms%20research%20in%20offline%20goal-conditioned%20RL.%20OGBench%0Aconsists%20of%208%20types%20of%20environments%2C%2085%20datasets%2C%20and%20reference%20implementations%0Aof%206%20representative%20offline%20GCRL%20algorithms.%20We%20have%20designed%20these%20challenging%0Aand%20realistic%20environments%20and%20datasets%20to%20directly%20probe%20different%0Acapabilities%20of%20algorithms%2C%20such%20as%20stitching%2C%20long-horizon%20reasoning%2C%20and%20the%0Aability%20to%20handle%20high-dimensional%20inputs%20and%20stochasticity.%20While%0Arepresentative%20algorithms%20may%20rank%20similarly%20on%20prior%20benchmarks%2C%20our%0Aexperiments%20reveal%20stark%20strengths%20and%20weaknesses%20in%20these%20different%0Acapabilities%2C%20providing%20a%20strong%20foundation%20for%20building%20new%20algorithms.%0AProject%20page%3A%20https%3A//seohong.me/projects/ogbench%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20092v2&entry.124074799=Read"},
{"title": "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models", "author": "Quintina Campbell and Sam Cox and Jorge Medina and Brittany Watterson and Andrew D. White", "abstract": "  Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.\n", "link": "http://arxiv.org/abs/2502.09565v1", "date": "2025-02-13", "relevancy": 1.9092, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4932}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDCrow%3A%20Automating%20Molecular%20Dynamics%20Workflows%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20MDCrow%3A%20Automating%20Molecular%20Dynamics%20Workflows%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Quintina%20Campbell%20and%20Sam%20Cox%20and%20Jorge%20Medina%20and%20Brittany%20Watterson%20and%20Andrew%20D.%20White%0AAbstract%3A%20%20%20Molecular%20dynamics%20%28MD%29%20simulations%20are%20essential%20for%20understanding%0Abiomolecular%20systems%20but%20remain%20challenging%20to%20automate.%20Recent%20advances%20in%0Alarge%20language%20models%20%28LLM%29%20have%20demonstrated%20success%20in%20automating%20complex%0Ascientific%20tasks%20using%20LLM-based%20agents.%20In%20this%20paper%2C%20we%20introduce%20MDCrow%2C%20an%0Aagentic%20LLM%20assistant%20capable%20of%20automating%20MD%20workflows.%20MDCrow%20uses%0Achain-of-thought%20over%2040%20expert-designed%20tools%20for%20handling%20and%20processing%0Afiles%2C%20setting%20up%20simulations%2C%20analyzing%20the%20simulation%20outputs%2C%20and%20retrieving%0Arelevant%20information%20from%20literature%20and%20databases.%20We%20assess%20MDCrow%27s%0Aperformance%20across%2025%20tasks%20of%20varying%20required%20subtasks%20and%20difficulty%2C%20and%20we%0Aevaluate%20the%20agent%27s%20robustness%20to%20both%20difficulty%20and%20prompt%20style.%0A%5Ctexttt%7Bgpt-4o%7D%20is%20able%20to%20complete%20complex%20tasks%20with%20low%20variance%2C%20followed%0Aclosely%20by%20%5Ctexttt%7Bllama3-405b%7D%2C%20a%20compelling%20open-source%20model.%20While%20prompt%0Astyle%20does%20not%20influence%20the%20best%20models%27%20performance%2C%20it%20has%20significant%0Aeffects%20on%20smaller%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDCrow%253A%2520Automating%2520Molecular%2520Dynamics%2520Workflows%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DQuintina%2520Campbell%2520and%2520Sam%2520Cox%2520and%2520Jorge%2520Medina%2520and%2520Brittany%2520Watterson%2520and%2520Andrew%2520D.%2520White%26entry.1292438233%3D%2520%2520Molecular%2520dynamics%2520%2528MD%2529%2520simulations%2520are%2520essential%2520for%2520understanding%250Abiomolecular%2520systems%2520but%2520remain%2520challenging%2520to%2520automate.%2520Recent%2520advances%2520in%250Alarge%2520language%2520models%2520%2528LLM%2529%2520have%2520demonstrated%2520success%2520in%2520automating%2520complex%250Ascientific%2520tasks%2520using%2520LLM-based%2520agents.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MDCrow%252C%2520an%250Aagentic%2520LLM%2520assistant%2520capable%2520of%2520automating%2520MD%2520workflows.%2520MDCrow%2520uses%250Achain-of-thought%2520over%252040%2520expert-designed%2520tools%2520for%2520handling%2520and%2520processing%250Afiles%252C%2520setting%2520up%2520simulations%252C%2520analyzing%2520the%2520simulation%2520outputs%252C%2520and%2520retrieving%250Arelevant%2520information%2520from%2520literature%2520and%2520databases.%2520We%2520assess%2520MDCrow%2527s%250Aperformance%2520across%252025%2520tasks%2520of%2520varying%2520required%2520subtasks%2520and%2520difficulty%252C%2520and%2520we%250Aevaluate%2520the%2520agent%2527s%2520robustness%2520to%2520both%2520difficulty%2520and%2520prompt%2520style.%250A%255Ctexttt%257Bgpt-4o%257D%2520is%2520able%2520to%2520complete%2520complex%2520tasks%2520with%2520low%2520variance%252C%2520followed%250Aclosely%2520by%2520%255Ctexttt%257Bllama3-405b%257D%252C%2520a%2520compelling%2520open-source%2520model.%2520While%2520prompt%250Astyle%2520does%2520not%2520influence%2520the%2520best%2520models%2527%2520performance%252C%2520it%2520has%2520significant%250Aeffects%2520on%2520smaller%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDCrow%3A%20Automating%20Molecular%20Dynamics%20Workflows%20with%20Large%20Language%0A%20%20Models&entry.906535625=Quintina%20Campbell%20and%20Sam%20Cox%20and%20Jorge%20Medina%20and%20Brittany%20Watterson%20and%20Andrew%20D.%20White&entry.1292438233=%20%20Molecular%20dynamics%20%28MD%29%20simulations%20are%20essential%20for%20understanding%0Abiomolecular%20systems%20but%20remain%20challenging%20to%20automate.%20Recent%20advances%20in%0Alarge%20language%20models%20%28LLM%29%20have%20demonstrated%20success%20in%20automating%20complex%0Ascientific%20tasks%20using%20LLM-based%20agents.%20In%20this%20paper%2C%20we%20introduce%20MDCrow%2C%20an%0Aagentic%20LLM%20assistant%20capable%20of%20automating%20MD%20workflows.%20MDCrow%20uses%0Achain-of-thought%20over%2040%20expert-designed%20tools%20for%20handling%20and%20processing%0Afiles%2C%20setting%20up%20simulations%2C%20analyzing%20the%20simulation%20outputs%2C%20and%20retrieving%0Arelevant%20information%20from%20literature%20and%20databases.%20We%20assess%20MDCrow%27s%0Aperformance%20across%2025%20tasks%20of%20varying%20required%20subtasks%20and%20difficulty%2C%20and%20we%0Aevaluate%20the%20agent%27s%20robustness%20to%20both%20difficulty%20and%20prompt%20style.%0A%5Ctexttt%7Bgpt-4o%7D%20is%20able%20to%20complete%20complex%20tasks%20with%20low%20variance%2C%20followed%0Aclosely%20by%20%5Ctexttt%7Bllama3-405b%7D%2C%20a%20compelling%20open-source%20model.%20While%20prompt%0Astyle%20does%20not%20influence%20the%20best%20models%27%20performance%2C%20it%20has%20significant%0Aeffects%20on%20smaller%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09565v1&entry.124074799=Read"},
{"title": "Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits", "author": "Mengmeng Li and Daniel Kuhn and Bahar Ta\u015fkesen", "abstract": "  Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret\nfor adversarial as well as stochastic bandit problems and allow for a\nstreamlined analysis. Nonetheless, FTRL algorithms require the solution of an\noptimization problem in every iteration and are thus computationally\nchallenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve\ncomputational efficiency by perturbing the estimates of the rewards of the\narms, but their regret analysis is cumbersome. We propose a new FTPL algorithm\nthat generates optimal policies for both adversarial and stochastic multi-armed\nbandits. Like FTRL, our algorithm admits a unified regret analysis, and similar\nto FTPL, it offers low computational costs. Unlike existing FTPL algorithms\nthat rely on independent additive disturbances governed by a \\textit{known}\ndistribution, we allow for disturbances governed by an \\textit{ambiguous}\ndistribution that is only known to belong to a given set and propose a\nprinciple of optimism in the face of ambiguity. Consequently, our framework\ngeneralizes existing FTPL algorithms. It also encapsulates a broad range of\nFTRL methods as special cases, including several optimal ones, which appears to\nbe impossible with current FTPL methods. Finally, we use techniques from\ndiscrete choice theory to devise an efficient bisection algorithm for computing\nthe optimistic arm sampling probabilities. This algorithm is up to $10^4$ times\nfaster than standard FTRL algorithms that solve an optimization problem in\nevery iteration. Our results not only settle existing conjectures but also\nprovide new insights into the impact of perturbations by mapping FTRL to FTPL.\n", "link": "http://arxiv.org/abs/2409.20440v2", "date": "2025-02-13", "relevancy": 1.9008, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4867}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4752}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimism%20in%20the%20Face%20of%20Ambiguity%20Principle%20for%20Multi-Armed%20Bandits&body=Title%3A%20Optimism%20in%20the%20Face%20of%20Ambiguity%20Principle%20for%20Multi-Armed%20Bandits%0AAuthor%3A%20Mengmeng%20Li%20and%20Daniel%20Kuhn%20and%20Bahar%20Ta%C5%9Fkesen%0AAbstract%3A%20%20%20Follow-The-Regularized-Leader%20%28FTRL%29%20algorithms%20often%20enjoy%20optimal%20regret%0Afor%20adversarial%20as%20well%20as%20stochastic%20bandit%20problems%20and%20allow%20for%20a%0Astreamlined%20analysis.%20Nonetheless%2C%20FTRL%20algorithms%20require%20the%20solution%20of%20an%0Aoptimization%20problem%20in%20every%20iteration%20and%20are%20thus%20computationally%0Achallenging.%20In%20contrast%2C%20Follow-The-Perturbed-Leader%20%28FTPL%29%20algorithms%20achieve%0Acomputational%20efficiency%20by%20perturbing%20the%20estimates%20of%20the%20rewards%20of%20the%0Aarms%2C%20but%20their%20regret%20analysis%20is%20cumbersome.%20We%20propose%20a%20new%20FTPL%20algorithm%0Athat%20generates%20optimal%20policies%20for%20both%20adversarial%20and%20stochastic%20multi-armed%0Abandits.%20Like%20FTRL%2C%20our%20algorithm%20admits%20a%20unified%20regret%20analysis%2C%20and%20similar%0Ato%20FTPL%2C%20it%20offers%20low%20computational%20costs.%20Unlike%20existing%20FTPL%20algorithms%0Athat%20rely%20on%20independent%20additive%20disturbances%20governed%20by%20a%20%5Ctextit%7Bknown%7D%0Adistribution%2C%20we%20allow%20for%20disturbances%20governed%20by%20an%20%5Ctextit%7Bambiguous%7D%0Adistribution%20that%20is%20only%20known%20to%20belong%20to%20a%20given%20set%20and%20propose%20a%0Aprinciple%20of%20optimism%20in%20the%20face%20of%20ambiguity.%20Consequently%2C%20our%20framework%0Ageneralizes%20existing%20FTPL%20algorithms.%20It%20also%20encapsulates%20a%20broad%20range%20of%0AFTRL%20methods%20as%20special%20cases%2C%20including%20several%20optimal%20ones%2C%20which%20appears%20to%0Abe%20impossible%20with%20current%20FTPL%20methods.%20Finally%2C%20we%20use%20techniques%20from%0Adiscrete%20choice%20theory%20to%20devise%20an%20efficient%20bisection%20algorithm%20for%20computing%0Athe%20optimistic%20arm%20sampling%20probabilities.%20This%20algorithm%20is%20up%20to%20%2410%5E4%24%20times%0Afaster%20than%20standard%20FTRL%20algorithms%20that%20solve%20an%20optimization%20problem%20in%0Aevery%20iteration.%20Our%20results%20not%20only%20settle%20existing%20conjectures%20but%20also%0Aprovide%20new%20insights%20into%20the%20impact%20of%20perturbations%20by%20mapping%20FTRL%20to%20FTPL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20440v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimism%2520in%2520the%2520Face%2520of%2520Ambiguity%2520Principle%2520for%2520Multi-Armed%2520Bandits%26entry.906535625%3DMengmeng%2520Li%2520and%2520Daniel%2520Kuhn%2520and%2520Bahar%2520Ta%25C5%259Fkesen%26entry.1292438233%3D%2520%2520Follow-The-Regularized-Leader%2520%2528FTRL%2529%2520algorithms%2520often%2520enjoy%2520optimal%2520regret%250Afor%2520adversarial%2520as%2520well%2520as%2520stochastic%2520bandit%2520problems%2520and%2520allow%2520for%2520a%250Astreamlined%2520analysis.%2520Nonetheless%252C%2520FTRL%2520algorithms%2520require%2520the%2520solution%2520of%2520an%250Aoptimization%2520problem%2520in%2520every%2520iteration%2520and%2520are%2520thus%2520computationally%250Achallenging.%2520In%2520contrast%252C%2520Follow-The-Perturbed-Leader%2520%2528FTPL%2529%2520algorithms%2520achieve%250Acomputational%2520efficiency%2520by%2520perturbing%2520the%2520estimates%2520of%2520the%2520rewards%2520of%2520the%250Aarms%252C%2520but%2520their%2520regret%2520analysis%2520is%2520cumbersome.%2520We%2520propose%2520a%2520new%2520FTPL%2520algorithm%250Athat%2520generates%2520optimal%2520policies%2520for%2520both%2520adversarial%2520and%2520stochastic%2520multi-armed%250Abandits.%2520Like%2520FTRL%252C%2520our%2520algorithm%2520admits%2520a%2520unified%2520regret%2520analysis%252C%2520and%2520similar%250Ato%2520FTPL%252C%2520it%2520offers%2520low%2520computational%2520costs.%2520Unlike%2520existing%2520FTPL%2520algorithms%250Athat%2520rely%2520on%2520independent%2520additive%2520disturbances%2520governed%2520by%2520a%2520%255Ctextit%257Bknown%257D%250Adistribution%252C%2520we%2520allow%2520for%2520disturbances%2520governed%2520by%2520an%2520%255Ctextit%257Bambiguous%257D%250Adistribution%2520that%2520is%2520only%2520known%2520to%2520belong%2520to%2520a%2520given%2520set%2520and%2520propose%2520a%250Aprinciple%2520of%2520optimism%2520in%2520the%2520face%2520of%2520ambiguity.%2520Consequently%252C%2520our%2520framework%250Ageneralizes%2520existing%2520FTPL%2520algorithms.%2520It%2520also%2520encapsulates%2520a%2520broad%2520range%2520of%250AFTRL%2520methods%2520as%2520special%2520cases%252C%2520including%2520several%2520optimal%2520ones%252C%2520which%2520appears%2520to%250Abe%2520impossible%2520with%2520current%2520FTPL%2520methods.%2520Finally%252C%2520we%2520use%2520techniques%2520from%250Adiscrete%2520choice%2520theory%2520to%2520devise%2520an%2520efficient%2520bisection%2520algorithm%2520for%2520computing%250Athe%2520optimistic%2520arm%2520sampling%2520probabilities.%2520This%2520algorithm%2520is%2520up%2520to%2520%252410%255E4%2524%2520times%250Afaster%2520than%2520standard%2520FTRL%2520algorithms%2520that%2520solve%2520an%2520optimization%2520problem%2520in%250Aevery%2520iteration.%2520Our%2520results%2520not%2520only%2520settle%2520existing%2520conjectures%2520but%2520also%250Aprovide%2520new%2520insights%2520into%2520the%2520impact%2520of%2520perturbations%2520by%2520mapping%2520FTRL%2520to%2520FTPL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20440v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimism%20in%20the%20Face%20of%20Ambiguity%20Principle%20for%20Multi-Armed%20Bandits&entry.906535625=Mengmeng%20Li%20and%20Daniel%20Kuhn%20and%20Bahar%20Ta%C5%9Fkesen&entry.1292438233=%20%20Follow-The-Regularized-Leader%20%28FTRL%29%20algorithms%20often%20enjoy%20optimal%20regret%0Afor%20adversarial%20as%20well%20as%20stochastic%20bandit%20problems%20and%20allow%20for%20a%0Astreamlined%20analysis.%20Nonetheless%2C%20FTRL%20algorithms%20require%20the%20solution%20of%20an%0Aoptimization%20problem%20in%20every%20iteration%20and%20are%20thus%20computationally%0Achallenging.%20In%20contrast%2C%20Follow-The-Perturbed-Leader%20%28FTPL%29%20algorithms%20achieve%0Acomputational%20efficiency%20by%20perturbing%20the%20estimates%20of%20the%20rewards%20of%20the%0Aarms%2C%20but%20their%20regret%20analysis%20is%20cumbersome.%20We%20propose%20a%20new%20FTPL%20algorithm%0Athat%20generates%20optimal%20policies%20for%20both%20adversarial%20and%20stochastic%20multi-armed%0Abandits.%20Like%20FTRL%2C%20our%20algorithm%20admits%20a%20unified%20regret%20analysis%2C%20and%20similar%0Ato%20FTPL%2C%20it%20offers%20low%20computational%20costs.%20Unlike%20existing%20FTPL%20algorithms%0Athat%20rely%20on%20independent%20additive%20disturbances%20governed%20by%20a%20%5Ctextit%7Bknown%7D%0Adistribution%2C%20we%20allow%20for%20disturbances%20governed%20by%20an%20%5Ctextit%7Bambiguous%7D%0Adistribution%20that%20is%20only%20known%20to%20belong%20to%20a%20given%20set%20and%20propose%20a%0Aprinciple%20of%20optimism%20in%20the%20face%20of%20ambiguity.%20Consequently%2C%20our%20framework%0Ageneralizes%20existing%20FTPL%20algorithms.%20It%20also%20encapsulates%20a%20broad%20range%20of%0AFTRL%20methods%20as%20special%20cases%2C%20including%20several%20optimal%20ones%2C%20which%20appears%20to%0Abe%20impossible%20with%20current%20FTPL%20methods.%20Finally%2C%20we%20use%20techniques%20from%0Adiscrete%20choice%20theory%20to%20devise%20an%20efficient%20bisection%20algorithm%20for%20computing%0Athe%20optimistic%20arm%20sampling%20probabilities.%20This%20algorithm%20is%20up%20to%20%2410%5E4%24%20times%0Afaster%20than%20standard%20FTRL%20algorithms%20that%20solve%20an%20optimization%20problem%20in%0Aevery%20iteration.%20Our%20results%20not%20only%20settle%20existing%20conjectures%20but%20also%0Aprovide%20new%20insights%20into%20the%20impact%20of%20perturbations%20by%20mapping%20FTRL%20to%20FTPL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20440v2&entry.124074799=Read"},
{"title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models", "author": "Yung-Sung Chuang and Benjamin Cohen-Wang and Shannon Zejiang Shen and Zhaofeng Wu and Hu Xu and Xi Victoria Lin and James Glass and Shang-Wen Li and Wen-tau Yih", "abstract": "  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n", "link": "http://arxiv.org/abs/2502.09604v1", "date": "2025-02-13", "relevancy": 1.8892, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4863}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4745}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfCite%3A%20Self-Supervised%20Alignment%20for%20Context%20Attribution%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20SelfCite%3A%20Self-Supervised%20Alignment%20for%20Context%20Attribution%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yung-Sung%20Chuang%20and%20Benjamin%20Cohen-Wang%20and%20Shannon%20Zejiang%20Shen%20and%20Zhaofeng%20Wu%20and%20Hu%20Xu%20and%20Xi%20Victoria%20Lin%20and%20James%20Glass%20and%20Shang-Wen%20Li%20and%20Wen-tau%20Yih%0AAbstract%3A%20%20%20We%20introduce%20SelfCite%2C%20a%20novel%20self-supervised%20approach%20that%20aligns%20LLMs%20to%0Agenerate%20high-quality%2C%20fine-grained%2C%20sentence-level%20citations%20for%20the%0Astatements%20in%20their%20generated%20responses.%20Instead%20of%20only%20relying%20on%20costly%20and%0Alabor-intensive%20annotations%2C%20SelfCite%20leverages%20a%20reward%20signal%20provided%20by%20the%0ALLM%20itself%20through%20context%20ablation%3A%20If%20a%20citation%20is%20necessary%2C%20removing%20the%0Acited%20text%20from%20the%20context%20should%20prevent%20the%20same%20response%3B%20if%20sufficient%2C%0Aretaining%20the%20cited%20text%20alone%20should%20preserve%20the%20same%20response.%20This%20reward%0Acan%20guide%20the%20inference-time%20best-of-N%20sampling%20strategy%20to%20improve%20citation%0Aquality%20significantly%2C%20as%20well%20as%20be%20used%20in%20preference%20optimization%20to%0Adirectly%20fine-tune%20the%20models%20for%20generating%20better%20citations.%20The%0Aeffectiveness%20of%20SelfCite%20is%20demonstrated%20by%20increasing%20citation%20F1%20up%20to%205.3%0Apoints%20on%20the%20LongBench-Cite%20benchmark%20across%20five%20long-form%20question%20answering%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfCite%253A%2520Self-Supervised%2520Alignment%2520for%2520Context%2520Attribution%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYung-Sung%2520Chuang%2520and%2520Benjamin%2520Cohen-Wang%2520and%2520Shannon%2520Zejiang%2520Shen%2520and%2520Zhaofeng%2520Wu%2520and%2520Hu%2520Xu%2520and%2520Xi%2520Victoria%2520Lin%2520and%2520James%2520Glass%2520and%2520Shang-Wen%2520Li%2520and%2520Wen-tau%2520Yih%26entry.1292438233%3D%2520%2520We%2520introduce%2520SelfCite%252C%2520a%2520novel%2520self-supervised%2520approach%2520that%2520aligns%2520LLMs%2520to%250Agenerate%2520high-quality%252C%2520fine-grained%252C%2520sentence-level%2520citations%2520for%2520the%250Astatements%2520in%2520their%2520generated%2520responses.%2520Instead%2520of%2520only%2520relying%2520on%2520costly%2520and%250Alabor-intensive%2520annotations%252C%2520SelfCite%2520leverages%2520a%2520reward%2520signal%2520provided%2520by%2520the%250ALLM%2520itself%2520through%2520context%2520ablation%253A%2520If%2520a%2520citation%2520is%2520necessary%252C%2520removing%2520the%250Acited%2520text%2520from%2520the%2520context%2520should%2520prevent%2520the%2520same%2520response%253B%2520if%2520sufficient%252C%250Aretaining%2520the%2520cited%2520text%2520alone%2520should%2520preserve%2520the%2520same%2520response.%2520This%2520reward%250Acan%2520guide%2520the%2520inference-time%2520best-of-N%2520sampling%2520strategy%2520to%2520improve%2520citation%250Aquality%2520significantly%252C%2520as%2520well%2520as%2520be%2520used%2520in%2520preference%2520optimization%2520to%250Adirectly%2520fine-tune%2520the%2520models%2520for%2520generating%2520better%2520citations.%2520The%250Aeffectiveness%2520of%2520SelfCite%2520is%2520demonstrated%2520by%2520increasing%2520citation%2520F1%2520up%2520to%25205.3%250Apoints%2520on%2520the%2520LongBench-Cite%2520benchmark%2520across%2520five%2520long-form%2520question%2520answering%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfCite%3A%20Self-Supervised%20Alignment%20for%20Context%20Attribution%20in%20Large%0A%20%20Language%20Models&entry.906535625=Yung-Sung%20Chuang%20and%20Benjamin%20Cohen-Wang%20and%20Shannon%20Zejiang%20Shen%20and%20Zhaofeng%20Wu%20and%20Hu%20Xu%20and%20Xi%20Victoria%20Lin%20and%20James%20Glass%20and%20Shang-Wen%20Li%20and%20Wen-tau%20Yih&entry.1292438233=%20%20We%20introduce%20SelfCite%2C%20a%20novel%20self-supervised%20approach%20that%20aligns%20LLMs%20to%0Agenerate%20high-quality%2C%20fine-grained%2C%20sentence-level%20citations%20for%20the%0Astatements%20in%20their%20generated%20responses.%20Instead%20of%20only%20relying%20on%20costly%20and%0Alabor-intensive%20annotations%2C%20SelfCite%20leverages%20a%20reward%20signal%20provided%20by%20the%0ALLM%20itself%20through%20context%20ablation%3A%20If%20a%20citation%20is%20necessary%2C%20removing%20the%0Acited%20text%20from%20the%20context%20should%20prevent%20the%20same%20response%3B%20if%20sufficient%2C%0Aretaining%20the%20cited%20text%20alone%20should%20preserve%20the%20same%20response.%20This%20reward%0Acan%20guide%20the%20inference-time%20best-of-N%20sampling%20strategy%20to%20improve%20citation%0Aquality%20significantly%2C%20as%20well%20as%20be%20used%20in%20preference%20optimization%20to%0Adirectly%20fine-tune%20the%20models%20for%20generating%20better%20citations.%20The%0Aeffectiveness%20of%20SelfCite%20is%20demonstrated%20by%20increasing%20citation%20F1%20up%20to%205.3%0Apoints%20on%20the%20LongBench-Cite%20benchmark%20across%20five%20long-form%20question%20answering%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09604v1&entry.124074799=Read"},
{"title": "Better Embeddings with Coupled Adam", "author": "Felix Stollenwerk and Tobias Stollenwerk", "abstract": "  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n", "link": "http://arxiv.org/abs/2502.08441v2", "date": "2025-02-13", "relevancy": 1.887, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4535}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Embeddings%20with%20Coupled%20Adam&body=Title%3A%20Better%20Embeddings%20with%20Coupled%20Adam%0AAuthor%3A%20Felix%20Stollenwerk%20and%20Tobias%20Stollenwerk%0AAbstract%3A%20%20%20Despite%20their%20remarkable%20capabilities%2C%20LLMs%20learn%20word%20representations%20that%0Aexhibit%20the%20undesirable%20yet%20poorly%20understood%20feature%20of%20anisotropy.%20In%20this%0Apaper%2C%20we%20argue%20that%20the%20second%20moment%20in%20Adam%20is%20a%20cause%20of%20anisotropic%0Aembeddings%2C%20and%20suggest%20a%20modified%20optimizer%20called%20Coupled%20Adam%20to%20mitigate%0Athe%20problem.%20Our%20experiments%20demonstrate%20that%20Coupled%20Adam%20significantly%0Aimproves%20the%20quality%20of%20embeddings%2C%20while%20also%20leading%20to%20better%20upstream%20and%0Adownstream%20performance%20on%20large%20enough%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Embeddings%2520with%2520Coupled%2520Adam%26entry.906535625%3DFelix%2520Stollenwerk%2520and%2520Tobias%2520Stollenwerk%26entry.1292438233%3D%2520%2520Despite%2520their%2520remarkable%2520capabilities%252C%2520LLMs%2520learn%2520word%2520representations%2520that%250Aexhibit%2520the%2520undesirable%2520yet%2520poorly%2520understood%2520feature%2520of%2520anisotropy.%2520In%2520this%250Apaper%252C%2520we%2520argue%2520that%2520the%2520second%2520moment%2520in%2520Adam%2520is%2520a%2520cause%2520of%2520anisotropic%250Aembeddings%252C%2520and%2520suggest%2520a%2520modified%2520optimizer%2520called%2520Coupled%2520Adam%2520to%2520mitigate%250Athe%2520problem.%2520Our%2520experiments%2520demonstrate%2520that%2520Coupled%2520Adam%2520significantly%250Aimproves%2520the%2520quality%2520of%2520embeddings%252C%2520while%2520also%2520leading%2520to%2520better%2520upstream%2520and%250Adownstream%2520performance%2520on%2520large%2520enough%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Embeddings%20with%20Coupled%20Adam&entry.906535625=Felix%20Stollenwerk%20and%20Tobias%20Stollenwerk&entry.1292438233=%20%20Despite%20their%20remarkable%20capabilities%2C%20LLMs%20learn%20word%20representations%20that%0Aexhibit%20the%20undesirable%20yet%20poorly%20understood%20feature%20of%20anisotropy.%20In%20this%0Apaper%2C%20we%20argue%20that%20the%20second%20moment%20in%20Adam%20is%20a%20cause%20of%20anisotropic%0Aembeddings%2C%20and%20suggest%20a%20modified%20optimizer%20called%20Coupled%20Adam%20to%20mitigate%0Athe%20problem.%20Our%20experiments%20demonstrate%20that%20Coupled%20Adam%20significantly%0Aimproves%20the%20quality%20of%20embeddings%2C%20while%20also%20leading%20to%20better%20upstream%20and%0Adownstream%20performance%20on%20large%20enough%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08441v2&entry.124074799=Read"},
{"title": "Transformer-Enhanced Variational Autoencoder for Crystal Structure\n  Prediction", "author": "Ziyi Chen and Yang Yuan and Siming Zheng and Jialong Guo and Sihan Liang and Yangang Wang and Zongguo Wang", "abstract": "  Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization.\n", "link": "http://arxiv.org/abs/2502.09423v1", "date": "2025-02-13", "relevancy": 1.8858, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4834}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer-Enhanced%20Variational%20Autoencoder%20for%20Crystal%20Structure%0A%20%20Prediction&body=Title%3A%20Transformer-Enhanced%20Variational%20Autoencoder%20for%20Crystal%20Structure%0A%20%20Prediction%0AAuthor%3A%20Ziyi%20Chen%20and%20Yang%20Yuan%20and%20Siming%20Zheng%20and%20Jialong%20Guo%20and%20Sihan%20Liang%20and%20Yangang%20Wang%20and%20Zongguo%20Wang%0AAbstract%3A%20%20%20Crystal%20structure%20forms%20the%20foundation%20for%20understanding%20the%20physical%20and%0Achemical%20properties%20of%20materials.%20Generative%20models%20have%20emerged%20as%20a%20new%0Aparadigm%20in%20crystal%20structure%20prediction%28CSP%29%2C%20however%2C%20accurately%20capturing%0Akey%20characteristics%20of%20crystal%20structures%2C%20such%20as%20periodicity%20and%20symmetry%2C%0Aremains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%0ATransformer-Enhanced%20Variational%20Autoencoder%20for%20Crystal%20Structure%20Prediction%0A%28TransVAE-CSP%29%2C%20who%20learns%20the%20characteristic%20distribution%20space%20of%20stable%0Amaterials%2C%20enabling%20both%20the%20reconstruction%20and%20generation%20of%20crystal%0Astructures.%20TransVAE-CSP%20integrates%20adaptive%20distance%20expansion%20with%0Airreducible%20representation%20to%20effectively%20capture%20the%20periodicity%20and%20symmetry%0Aof%20crystal%20structures%2C%20and%20the%20encoder%20is%20a%20transformer%20network%20based%20on%20an%0Aequivariant%20dot%20product%20attention%20mechanism.%20Experimental%20results%20on%20the%0Acarbon_24%2C%20perov_5%2C%20and%20mp_20%20datasets%20demonstrate%20that%20TransVAE-CSP%0Aoutperforms%20existing%20methods%20in%20structure%20reconstruction%20and%20generation%20tasks%0Aunder%20various%20modeling%20metrics%2C%20offering%20a%20powerful%20tool%20for%20crystal%20structure%0Adesign%20and%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer-Enhanced%2520Variational%2520Autoencoder%2520for%2520Crystal%2520Structure%250A%2520%2520Prediction%26entry.906535625%3DZiyi%2520Chen%2520and%2520Yang%2520Yuan%2520and%2520Siming%2520Zheng%2520and%2520Jialong%2520Guo%2520and%2520Sihan%2520Liang%2520and%2520Yangang%2520Wang%2520and%2520Zongguo%2520Wang%26entry.1292438233%3D%2520%2520Crystal%2520structure%2520forms%2520the%2520foundation%2520for%2520understanding%2520the%2520physical%2520and%250Achemical%2520properties%2520of%2520materials.%2520Generative%2520models%2520have%2520emerged%2520as%2520a%2520new%250Aparadigm%2520in%2520crystal%2520structure%2520prediction%2528CSP%2529%252C%2520however%252C%2520accurately%2520capturing%250Akey%2520characteristics%2520of%2520crystal%2520structures%252C%2520such%2520as%2520periodicity%2520and%2520symmetry%252C%250Aremains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250ATransformer-Enhanced%2520Variational%2520Autoencoder%2520for%2520Crystal%2520Structure%2520Prediction%250A%2528TransVAE-CSP%2529%252C%2520who%2520learns%2520the%2520characteristic%2520distribution%2520space%2520of%2520stable%250Amaterials%252C%2520enabling%2520both%2520the%2520reconstruction%2520and%2520generation%2520of%2520crystal%250Astructures.%2520TransVAE-CSP%2520integrates%2520adaptive%2520distance%2520expansion%2520with%250Airreducible%2520representation%2520to%2520effectively%2520capture%2520the%2520periodicity%2520and%2520symmetry%250Aof%2520crystal%2520structures%252C%2520and%2520the%2520encoder%2520is%2520a%2520transformer%2520network%2520based%2520on%2520an%250Aequivariant%2520dot%2520product%2520attention%2520mechanism.%2520Experimental%2520results%2520on%2520the%250Acarbon_24%252C%2520perov_5%252C%2520and%2520mp_20%2520datasets%2520demonstrate%2520that%2520TransVAE-CSP%250Aoutperforms%2520existing%2520methods%2520in%2520structure%2520reconstruction%2520and%2520generation%2520tasks%250Aunder%2520various%2520modeling%2520metrics%252C%2520offering%2520a%2520powerful%2520tool%2520for%2520crystal%2520structure%250Adesign%2520and%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-Enhanced%20Variational%20Autoencoder%20for%20Crystal%20Structure%0A%20%20Prediction&entry.906535625=Ziyi%20Chen%20and%20Yang%20Yuan%20and%20Siming%20Zheng%20and%20Jialong%20Guo%20and%20Sihan%20Liang%20and%20Yangang%20Wang%20and%20Zongguo%20Wang&entry.1292438233=%20%20Crystal%20structure%20forms%20the%20foundation%20for%20understanding%20the%20physical%20and%0Achemical%20properties%20of%20materials.%20Generative%20models%20have%20emerged%20as%20a%20new%0Aparadigm%20in%20crystal%20structure%20prediction%28CSP%29%2C%20however%2C%20accurately%20capturing%0Akey%20characteristics%20of%20crystal%20structures%2C%20such%20as%20periodicity%20and%20symmetry%2C%0Aremains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%0ATransformer-Enhanced%20Variational%20Autoencoder%20for%20Crystal%20Structure%20Prediction%0A%28TransVAE-CSP%29%2C%20who%20learns%20the%20characteristic%20distribution%20space%20of%20stable%0Amaterials%2C%20enabling%20both%20the%20reconstruction%20and%20generation%20of%20crystal%0Astructures.%20TransVAE-CSP%20integrates%20adaptive%20distance%20expansion%20with%0Airreducible%20representation%20to%20effectively%20capture%20the%20periodicity%20and%20symmetry%0Aof%20crystal%20structures%2C%20and%20the%20encoder%20is%20a%20transformer%20network%20based%20on%20an%0Aequivariant%20dot%20product%20attention%20mechanism.%20Experimental%20results%20on%20the%0Acarbon_24%2C%20perov_5%2C%20and%20mp_20%20datasets%20demonstrate%20that%20TransVAE-CSP%0Aoutperforms%20existing%20methods%20in%20structure%20reconstruction%20and%20generation%20tasks%0Aunder%20various%20modeling%20metrics%2C%20offering%20a%20powerful%20tool%20for%20crystal%20structure%0Adesign%20and%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09423v1&entry.124074799=Read"},
{"title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs", "author": "Siyan Zhao and Mingyi Hong and Yang Liu and Devamanyu Hazarika and Kaixiang Lin", "abstract": "  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n", "link": "http://arxiv.org/abs/2502.09597v1", "date": "2025-02-13", "relevancy": 1.8755, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Recognize%20Your%20Preferences%3F%20Evaluating%20Personalized%20Preference%0A%20%20Following%20in%20LLMs&body=Title%3A%20Do%20LLMs%20Recognize%20Your%20Preferences%3F%20Evaluating%20Personalized%20Preference%0A%20%20Following%20in%20LLMs%0AAuthor%3A%20Siyan%20Zhao%20and%20Mingyi%20Hong%20and%20Yang%20Liu%20and%20Devamanyu%20Hazarika%20and%20Kaixiang%20Lin%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20as%20chatbots%2C%20yet%20their%0Aability%20to%20personalize%20responses%20to%20user%20preferences%20remains%20limited.%20We%0Aintroduce%20PrefEval%2C%20a%20benchmark%20for%20evaluating%20LLMs%27%20ability%20to%20infer%2C%20memorize%0Aand%20adhere%20to%20user%20preferences%20in%20a%20long-context%20conversational%20setting.%0APrefEval%20comprises%203%2C000%20manually%20curated%20user%20preference%20and%20query%20pairs%0Aspanning%2020%20topics.%20PrefEval%20contains%20user%20personalization%20or%20preference%0Ainformation%20in%20both%20explicit%20and%20implicit%20forms%2C%20and%20evaluates%20LLM%20performance%0Ausing%20a%20generation%20and%20a%20classification%20task.%20With%20PrefEval%2C%20we%20evaluated%20the%0Aaforementioned%20preference%20following%20capabilities%20of%2010%20open-source%20and%0Aproprietary%20LLMs%20in%20multi-session%20conversations%20with%20varying%20context%20lengths%20up%0Ato%20100k%20tokens.%20We%20benchmark%20with%20various%20prompting%2C%20iterative%20feedback%2C%20and%0Aretrieval-augmented%20generation%20methods.%20Our%20benchmarking%20effort%20reveals%20that%0Astate-of-the-art%20LLMs%20face%20significant%20challenges%20in%20proactively%20following%0Ausers%27%20preferences%20during%20conversations.%20In%20particular%2C%20in%20zero-shot%20settings%2C%0Apreference%20following%20accuracy%20falls%20below%2010%25%20at%20merely%2010%20turns%20%28~3k%20tokens%29%0Aacross%20most%20evaluated%20models.%20Even%20with%20advanced%20prompting%20and%20retrieval%0Amethods%2C%20preference%20following%20still%20deteriorates%20in%20long-context%20conversations.%0AFurthermore%2C%20we%20show%20that%20fine-tuning%20on%20PrefEval%20significantly%20improves%0Aperformance.%20We%20believe%20PrefEval%20serves%20as%20a%20valuable%20resource%20for%20measuring%2C%0Aunderstanding%2C%20and%20enhancing%20LLMs%27%20preference%20following%20abilities%2C%20paving%20the%0Away%20for%20personalized%20conversational%20agents.%20Our%20code%20and%20dataset%20are%20available%0Aat%20https%3A//prefeval.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Recognize%2520Your%2520Preferences%253F%2520Evaluating%2520Personalized%2520Preference%250A%2520%2520Following%2520in%2520LLMs%26entry.906535625%3DSiyan%2520Zhao%2520and%2520Mingyi%2520Hong%2520and%2520Yang%2520Liu%2520and%2520Devamanyu%2520Hazarika%2520and%2520Kaixiang%2520Lin%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520as%2520chatbots%252C%2520yet%2520their%250Aability%2520to%2520personalize%2520responses%2520to%2520user%2520preferences%2520remains%2520limited.%2520We%250Aintroduce%2520PrefEval%252C%2520a%2520benchmark%2520for%2520evaluating%2520LLMs%2527%2520ability%2520to%2520infer%252C%2520memorize%250Aand%2520adhere%2520to%2520user%2520preferences%2520in%2520a%2520long-context%2520conversational%2520setting.%250APrefEval%2520comprises%25203%252C000%2520manually%2520curated%2520user%2520preference%2520and%2520query%2520pairs%250Aspanning%252020%2520topics.%2520PrefEval%2520contains%2520user%2520personalization%2520or%2520preference%250Ainformation%2520in%2520both%2520explicit%2520and%2520implicit%2520forms%252C%2520and%2520evaluates%2520LLM%2520performance%250Ausing%2520a%2520generation%2520and%2520a%2520classification%2520task.%2520With%2520PrefEval%252C%2520we%2520evaluated%2520the%250Aaforementioned%2520preference%2520following%2520capabilities%2520of%252010%2520open-source%2520and%250Aproprietary%2520LLMs%2520in%2520multi-session%2520conversations%2520with%2520varying%2520context%2520lengths%2520up%250Ato%2520100k%2520tokens.%2520We%2520benchmark%2520with%2520various%2520prompting%252C%2520iterative%2520feedback%252C%2520and%250Aretrieval-augmented%2520generation%2520methods.%2520Our%2520benchmarking%2520effort%2520reveals%2520that%250Astate-of-the-art%2520LLMs%2520face%2520significant%2520challenges%2520in%2520proactively%2520following%250Ausers%2527%2520preferences%2520during%2520conversations.%2520In%2520particular%252C%2520in%2520zero-shot%2520settings%252C%250Apreference%2520following%2520accuracy%2520falls%2520below%252010%2525%2520at%2520merely%252010%2520turns%2520%2528~3k%2520tokens%2529%250Aacross%2520most%2520evaluated%2520models.%2520Even%2520with%2520advanced%2520prompting%2520and%2520retrieval%250Amethods%252C%2520preference%2520following%2520still%2520deteriorates%2520in%2520long-context%2520conversations.%250AFurthermore%252C%2520we%2520show%2520that%2520fine-tuning%2520on%2520PrefEval%2520significantly%2520improves%250Aperformance.%2520We%2520believe%2520PrefEval%2520serves%2520as%2520a%2520valuable%2520resource%2520for%2520measuring%252C%250Aunderstanding%252C%2520and%2520enhancing%2520LLMs%2527%2520preference%2520following%2520abilities%252C%2520paving%2520the%250Away%2520for%2520personalized%2520conversational%2520agents.%2520Our%2520code%2520and%2520dataset%2520are%2520available%250Aat%2520https%253A//prefeval.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Recognize%20Your%20Preferences%3F%20Evaluating%20Personalized%20Preference%0A%20%20Following%20in%20LLMs&entry.906535625=Siyan%20Zhao%20and%20Mingyi%20Hong%20and%20Yang%20Liu%20and%20Devamanyu%20Hazarika%20and%20Kaixiang%20Lin&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20as%20chatbots%2C%20yet%20their%0Aability%20to%20personalize%20responses%20to%20user%20preferences%20remains%20limited.%20We%0Aintroduce%20PrefEval%2C%20a%20benchmark%20for%20evaluating%20LLMs%27%20ability%20to%20infer%2C%20memorize%0Aand%20adhere%20to%20user%20preferences%20in%20a%20long-context%20conversational%20setting.%0APrefEval%20comprises%203%2C000%20manually%20curated%20user%20preference%20and%20query%20pairs%0Aspanning%2020%20topics.%20PrefEval%20contains%20user%20personalization%20or%20preference%0Ainformation%20in%20both%20explicit%20and%20implicit%20forms%2C%20and%20evaluates%20LLM%20performance%0Ausing%20a%20generation%20and%20a%20classification%20task.%20With%20PrefEval%2C%20we%20evaluated%20the%0Aaforementioned%20preference%20following%20capabilities%20of%2010%20open-source%20and%0Aproprietary%20LLMs%20in%20multi-session%20conversations%20with%20varying%20context%20lengths%20up%0Ato%20100k%20tokens.%20We%20benchmark%20with%20various%20prompting%2C%20iterative%20feedback%2C%20and%0Aretrieval-augmented%20generation%20methods.%20Our%20benchmarking%20effort%20reveals%20that%0Astate-of-the-art%20LLMs%20face%20significant%20challenges%20in%20proactively%20following%0Ausers%27%20preferences%20during%20conversations.%20In%20particular%2C%20in%20zero-shot%20settings%2C%0Apreference%20following%20accuracy%20falls%20below%2010%25%20at%20merely%2010%20turns%20%28~3k%20tokens%29%0Aacross%20most%20evaluated%20models.%20Even%20with%20advanced%20prompting%20and%20retrieval%0Amethods%2C%20preference%20following%20still%20deteriorates%20in%20long-context%20conversations.%0AFurthermore%2C%20we%20show%20that%20fine-tuning%20on%20PrefEval%20significantly%20improves%0Aperformance.%20We%20believe%20PrefEval%20serves%20as%20a%20valuable%20resource%20for%20measuring%2C%0Aunderstanding%2C%20and%20enhancing%20LLMs%27%20preference%20following%20abilities%2C%20paving%20the%0Away%20for%20personalized%20conversational%20agents.%20Our%20code%20and%20dataset%20are%20available%0Aat%20https%3A//prefeval.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09597v1&entry.124074799=Read"},
{"title": "Rationalization Models for Text-to-SQL", "author": "Gaetano Rossiello and Nhan Pham and Michael Glass and Junkyu Lee and Dharmashankar Subramanian", "abstract": "  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n", "link": "http://arxiv.org/abs/2502.06759v2", "date": "2025-02-13", "relevancy": 1.8563, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rationalization%20Models%20for%20Text-to-SQL&body=Title%3A%20Rationalization%20Models%20for%20Text-to-SQL%0AAuthor%3A%20Gaetano%20Rossiello%20and%20Nhan%20Pham%20and%20Michael%20Glass%20and%20Junkyu%20Lee%20and%20Dharmashankar%20Subramanian%0AAbstract%3A%20%20%20We%20introduce%20a%20framework%20for%20generating%20Chain-of-Thought%20%28CoT%29%20rationales%20to%0Aenhance%20text-to-SQL%20model%20fine-tuning.%20These%20rationales%20consist%20of%20intermediate%0ASQL%20statements%20and%20explanations%2C%20serving%20as%20incremental%20steps%20toward%0Aconstructing%20the%20final%20SQL%20query.%20The%20process%20begins%20with%20manually%20annotating%20a%0Asmall%20set%20of%20examples%2C%20which%20are%20then%20used%20to%20prompt%20a%20large%20language%20model%20in%0Aan%20iterative%2C%20dynamic%20few-shot%20knowledge%20distillation%20procedure%20from%20a%20teacher%0Amodel.%20A%20rationalization%20model%20is%20subsequently%20trained%20on%20the%20validated%0Adecomposed%20queries%2C%20enabling%20extensive%20synthetic%20CoT%20annotations%20for%0Atext-to-SQL%20datasets.%20To%20evaluate%20the%20approach%2C%20we%20fine-tune%20small%20language%0Amodels%20with%20and%20without%20these%20rationales%20on%20the%20BIRD%20dataset.%20Results%20indicate%0Athat%20step-by-step%20query%20generation%20improves%20execution%20accuracy%2C%20especially%20for%0Amoderately%20and%20highly%20complex%20queries%2C%20while%20also%20enhancing%20explainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRationalization%2520Models%2520for%2520Text-to-SQL%26entry.906535625%3DGaetano%2520Rossiello%2520and%2520Nhan%2520Pham%2520and%2520Michael%2520Glass%2520and%2520Junkyu%2520Lee%2520and%2520Dharmashankar%2520Subramanian%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520framework%2520for%2520generating%2520Chain-of-Thought%2520%2528CoT%2529%2520rationales%2520to%250Aenhance%2520text-to-SQL%2520model%2520fine-tuning.%2520These%2520rationales%2520consist%2520of%2520intermediate%250ASQL%2520statements%2520and%2520explanations%252C%2520serving%2520as%2520incremental%2520steps%2520toward%250Aconstructing%2520the%2520final%2520SQL%2520query.%2520The%2520process%2520begins%2520with%2520manually%2520annotating%2520a%250Asmall%2520set%2520of%2520examples%252C%2520which%2520are%2520then%2520used%2520to%2520prompt%2520a%2520large%2520language%2520model%2520in%250Aan%2520iterative%252C%2520dynamic%2520few-shot%2520knowledge%2520distillation%2520procedure%2520from%2520a%2520teacher%250Amodel.%2520A%2520rationalization%2520model%2520is%2520subsequently%2520trained%2520on%2520the%2520validated%250Adecomposed%2520queries%252C%2520enabling%2520extensive%2520synthetic%2520CoT%2520annotations%2520for%250Atext-to-SQL%2520datasets.%2520To%2520evaluate%2520the%2520approach%252C%2520we%2520fine-tune%2520small%2520language%250Amodels%2520with%2520and%2520without%2520these%2520rationales%2520on%2520the%2520BIRD%2520dataset.%2520Results%2520indicate%250Athat%2520step-by-step%2520query%2520generation%2520improves%2520execution%2520accuracy%252C%2520especially%2520for%250Amoderately%2520and%2520highly%2520complex%2520queries%252C%2520while%2520also%2520enhancing%2520explainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rationalization%20Models%20for%20Text-to-SQL&entry.906535625=Gaetano%20Rossiello%20and%20Nhan%20Pham%20and%20Michael%20Glass%20and%20Junkyu%20Lee%20and%20Dharmashankar%20Subramanian&entry.1292438233=%20%20We%20introduce%20a%20framework%20for%20generating%20Chain-of-Thought%20%28CoT%29%20rationales%20to%0Aenhance%20text-to-SQL%20model%20fine-tuning.%20These%20rationales%20consist%20of%20intermediate%0ASQL%20statements%20and%20explanations%2C%20serving%20as%20incremental%20steps%20toward%0Aconstructing%20the%20final%20SQL%20query.%20The%20process%20begins%20with%20manually%20annotating%20a%0Asmall%20set%20of%20examples%2C%20which%20are%20then%20used%20to%20prompt%20a%20large%20language%20model%20in%0Aan%20iterative%2C%20dynamic%20few-shot%20knowledge%20distillation%20procedure%20from%20a%20teacher%0Amodel.%20A%20rationalization%20model%20is%20subsequently%20trained%20on%20the%20validated%0Adecomposed%20queries%2C%20enabling%20extensive%20synthetic%20CoT%20annotations%20for%0Atext-to-SQL%20datasets.%20To%20evaluate%20the%20approach%2C%20we%20fine-tune%20small%20language%0Amodels%20with%20and%20without%20these%20rationales%20on%20the%20BIRD%20dataset.%20Results%20indicate%0Athat%20step-by-step%20query%20generation%20improves%20execution%20accuracy%2C%20especially%20for%0Amoderately%20and%20highly%20complex%20queries%2C%20while%20also%20enhancing%20explainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06759v2&entry.124074799=Read"},
{"title": "Learning to Predict Global Atrial Fibrillation Dynamics from Sparse\n  Measurements", "author": "Alexander Jenkins and Andrea Cini and Joseph Barker and Alexander Sharp and Arunashis Sau and Varun Valentine and Srushti Valasang and Xinyang Li and Tom Wong and Timothy Betts and Danilo Mandic and Cesare Alippi and Fu Siong Ng", "abstract": "  Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all\ntreatment with limited success in persistent AF. This may be due to our\ninability to map the dynamics of AF with the limited resolution and coverage\nprovided by sequential contact mapping catheters, preventing effective patient\nphenotyping for personalised, targeted ablation. Here we introduce FibMap, a\ngraph recurrent neural network model that reconstructs global AF dynamics from\nsparse measurements. Trained and validated on 51 non-contact whole atria\nrecordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,\nachieving a 210% lower mean absolute error and an order of magnitude higher\nperformance in tracking phase singularities compared to baseline methods.\nClinical utility of FibMap is demonstrated on real-world contact mapping\nrecordings, achieving reconstruction fidelity comparable to non-contact\nmapping. FibMap's state-spaces and patient-specific parameters offer insights\nfor electrophenotyping AF. Integrating FibMap into clinical practice could\nenable personalised AF care and improve outcomes.\n", "link": "http://arxiv.org/abs/2502.09473v1", "date": "2025-02-13", "relevancy": 1.841, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.482}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4584}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Predict%20Global%20Atrial%20Fibrillation%20Dynamics%20from%20Sparse%0A%20%20Measurements&body=Title%3A%20Learning%20to%20Predict%20Global%20Atrial%20Fibrillation%20Dynamics%20from%20Sparse%0A%20%20Measurements%0AAuthor%3A%20Alexander%20Jenkins%20and%20Andrea%20Cini%20and%20Joseph%20Barker%20and%20Alexander%20Sharp%20and%20Arunashis%20Sau%20and%20Varun%20Valentine%20and%20Srushti%20Valasang%20and%20Xinyang%20Li%20and%20Tom%20Wong%20and%20Timothy%20Betts%20and%20Danilo%20Mandic%20and%20Cesare%20Alippi%20and%20Fu%20Siong%20Ng%0AAbstract%3A%20%20%20Catheter%20ablation%20of%20Atrial%20Fibrillation%20%28AF%29%20consists%20of%20a%20one-size-fits-all%0Atreatment%20with%20limited%20success%20in%20persistent%20AF.%20This%20may%20be%20due%20to%20our%0Ainability%20to%20map%20the%20dynamics%20of%20AF%20with%20the%20limited%20resolution%20and%20coverage%0Aprovided%20by%20sequential%20contact%20mapping%20catheters%2C%20preventing%20effective%20patient%0Aphenotyping%20for%20personalised%2C%20targeted%20ablation.%20Here%20we%20introduce%20FibMap%2C%20a%0Agraph%20recurrent%20neural%20network%20model%20that%20reconstructs%20global%20AF%20dynamics%20from%0Asparse%20measurements.%20Trained%20and%20validated%20on%2051%20non-contact%20whole%20atria%0Arecordings%2C%20FibMap%20reconstructs%20whole%20atria%20dynamics%20from%2010%25%20surface%20coverage%2C%0Aachieving%20a%20210%25%20lower%20mean%20absolute%20error%20and%20an%20order%20of%20magnitude%20higher%0Aperformance%20in%20tracking%20phase%20singularities%20compared%20to%20baseline%20methods.%0AClinical%20utility%20of%20FibMap%20is%20demonstrated%20on%20real-world%20contact%20mapping%0Arecordings%2C%20achieving%20reconstruction%20fidelity%20comparable%20to%20non-contact%0Amapping.%20FibMap%27s%20state-spaces%20and%20patient-specific%20parameters%20offer%20insights%0Afor%20electrophenotyping%20AF.%20Integrating%20FibMap%20into%20clinical%20practice%20could%0Aenable%20personalised%20AF%20care%20and%20improve%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Predict%2520Global%2520Atrial%2520Fibrillation%2520Dynamics%2520from%2520Sparse%250A%2520%2520Measurements%26entry.906535625%3DAlexander%2520Jenkins%2520and%2520Andrea%2520Cini%2520and%2520Joseph%2520Barker%2520and%2520Alexander%2520Sharp%2520and%2520Arunashis%2520Sau%2520and%2520Varun%2520Valentine%2520and%2520Srushti%2520Valasang%2520and%2520Xinyang%2520Li%2520and%2520Tom%2520Wong%2520and%2520Timothy%2520Betts%2520and%2520Danilo%2520Mandic%2520and%2520Cesare%2520Alippi%2520and%2520Fu%2520Siong%2520Ng%26entry.1292438233%3D%2520%2520Catheter%2520ablation%2520of%2520Atrial%2520Fibrillation%2520%2528AF%2529%2520consists%2520of%2520a%2520one-size-fits-all%250Atreatment%2520with%2520limited%2520success%2520in%2520persistent%2520AF.%2520This%2520may%2520be%2520due%2520to%2520our%250Ainability%2520to%2520map%2520the%2520dynamics%2520of%2520AF%2520with%2520the%2520limited%2520resolution%2520and%2520coverage%250Aprovided%2520by%2520sequential%2520contact%2520mapping%2520catheters%252C%2520preventing%2520effective%2520patient%250Aphenotyping%2520for%2520personalised%252C%2520targeted%2520ablation.%2520Here%2520we%2520introduce%2520FibMap%252C%2520a%250Agraph%2520recurrent%2520neural%2520network%2520model%2520that%2520reconstructs%2520global%2520AF%2520dynamics%2520from%250Asparse%2520measurements.%2520Trained%2520and%2520validated%2520on%252051%2520non-contact%2520whole%2520atria%250Arecordings%252C%2520FibMap%2520reconstructs%2520whole%2520atria%2520dynamics%2520from%252010%2525%2520surface%2520coverage%252C%250Aachieving%2520a%2520210%2525%2520lower%2520mean%2520absolute%2520error%2520and%2520an%2520order%2520of%2520magnitude%2520higher%250Aperformance%2520in%2520tracking%2520phase%2520singularities%2520compared%2520to%2520baseline%2520methods.%250AClinical%2520utility%2520of%2520FibMap%2520is%2520demonstrated%2520on%2520real-world%2520contact%2520mapping%250Arecordings%252C%2520achieving%2520reconstruction%2520fidelity%2520comparable%2520to%2520non-contact%250Amapping.%2520FibMap%2527s%2520state-spaces%2520and%2520patient-specific%2520parameters%2520offer%2520insights%250Afor%2520electrophenotyping%2520AF.%2520Integrating%2520FibMap%2520into%2520clinical%2520practice%2520could%250Aenable%2520personalised%2520AF%2520care%2520and%2520improve%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Predict%20Global%20Atrial%20Fibrillation%20Dynamics%20from%20Sparse%0A%20%20Measurements&entry.906535625=Alexander%20Jenkins%20and%20Andrea%20Cini%20and%20Joseph%20Barker%20and%20Alexander%20Sharp%20and%20Arunashis%20Sau%20and%20Varun%20Valentine%20and%20Srushti%20Valasang%20and%20Xinyang%20Li%20and%20Tom%20Wong%20and%20Timothy%20Betts%20and%20Danilo%20Mandic%20and%20Cesare%20Alippi%20and%20Fu%20Siong%20Ng&entry.1292438233=%20%20Catheter%20ablation%20of%20Atrial%20Fibrillation%20%28AF%29%20consists%20of%20a%20one-size-fits-all%0Atreatment%20with%20limited%20success%20in%20persistent%20AF.%20This%20may%20be%20due%20to%20our%0Ainability%20to%20map%20the%20dynamics%20of%20AF%20with%20the%20limited%20resolution%20and%20coverage%0Aprovided%20by%20sequential%20contact%20mapping%20catheters%2C%20preventing%20effective%20patient%0Aphenotyping%20for%20personalised%2C%20targeted%20ablation.%20Here%20we%20introduce%20FibMap%2C%20a%0Agraph%20recurrent%20neural%20network%20model%20that%20reconstructs%20global%20AF%20dynamics%20from%0Asparse%20measurements.%20Trained%20and%20validated%20on%2051%20non-contact%20whole%20atria%0Arecordings%2C%20FibMap%20reconstructs%20whole%20atria%20dynamics%20from%2010%25%20surface%20coverage%2C%0Aachieving%20a%20210%25%20lower%20mean%20absolute%20error%20and%20an%20order%20of%20magnitude%20higher%0Aperformance%20in%20tracking%20phase%20singularities%20compared%20to%20baseline%20methods.%0AClinical%20utility%20of%20FibMap%20is%20demonstrated%20on%20real-world%20contact%20mapping%0Arecordings%2C%20achieving%20reconstruction%20fidelity%20comparable%20to%20non-contact%0Amapping.%20FibMap%27s%20state-spaces%20and%20patient-specific%20parameters%20offer%20insights%0Afor%20electrophenotyping%20AF.%20Integrating%20FibMap%20into%20clinical%20practice%20could%0Aenable%20personalised%20AF%20care%20and%20improve%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09473v1&entry.124074799=Read"},
{"title": "Scalable First-order Method for Certifying Optimal k-Sparse GLMs", "author": "Jiachang Liu and Soroosh Shafiee and Andrea Lodi", "abstract": "  This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.\n", "link": "http://arxiv.org/abs/2502.09502v1", "date": "2025-02-13", "relevancy": 1.8334, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4708}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4587}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20First-order%20Method%20for%20Certifying%20Optimal%20k-Sparse%20GLMs&body=Title%3A%20Scalable%20First-order%20Method%20for%20Certifying%20Optimal%20k-Sparse%20GLMs%0AAuthor%3A%20Jiachang%20Liu%20and%20Soroosh%20Shafiee%20and%20Andrea%20Lodi%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20problem%20of%20certifying%20optimality%20for%20sparse%0Ageneralized%20linear%20models%20%28GLMs%29%2C%20where%20sparsity%20is%20enforced%20through%20an%0A%24%5Cell_0%24%20cardinality%20constraint.%20While%20branch-and-bound%20%28BnB%29%20frameworks%20can%0Acertify%20optimality%20by%20pruning%20nodes%20using%20dual%20bounds%2C%20existing%20methods%20for%0Acomputing%20these%20bounds%20are%20either%20computationally%20intensive%20or%20exhibit%20slow%0Aconvergence%2C%20limiting%20their%20scalability%20to%20large-scale%20problems.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20first-order%20proximal%20gradient%20algorithm%20designed%0Ato%20solve%20the%20perspective%20relaxation%20of%20the%20problem%20within%20a%20BnB%20framework.%0ASpecifically%2C%20we%20formulate%20the%20relaxed%20problem%20as%20a%20composite%20optimization%0Aproblem%20and%20demonstrate%20that%20the%20proximal%20operator%20of%20the%20non-smooth%20component%0Acan%20be%20computed%20exactly%20in%20log-linear%20time%20complexity%2C%20eliminating%20the%20need%20to%0Asolve%20a%20computationally%20expensive%20second-order%20cone%20program.%20Furthermore%2C%20we%0Aintroduce%20a%20simple%20restart%20strategy%20that%20enhances%20convergence%20speed%20while%0Amaintaining%20low%20per-iteration%20complexity.%20Extensive%20experiments%20on%20synthetic%0Aand%20real-world%20datasets%20show%20that%20our%20approach%20significantly%20accelerates%20dual%0Abound%20computations%20and%20is%20highly%20effective%20in%20providing%20optimality%20certificates%0Afor%20large-scale%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520First-order%2520Method%2520for%2520Certifying%2520Optimal%2520k-Sparse%2520GLMs%26entry.906535625%3DJiachang%2520Liu%2520and%2520Soroosh%2520Shafiee%2520and%2520Andrea%2520Lodi%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520problem%2520of%2520certifying%2520optimality%2520for%2520sparse%250Ageneralized%2520linear%2520models%2520%2528GLMs%2529%252C%2520where%2520sparsity%2520is%2520enforced%2520through%2520an%250A%2524%255Cell_0%2524%2520cardinality%2520constraint.%2520While%2520branch-and-bound%2520%2528BnB%2529%2520frameworks%2520can%250Acertify%2520optimality%2520by%2520pruning%2520nodes%2520using%2520dual%2520bounds%252C%2520existing%2520methods%2520for%250Acomputing%2520these%2520bounds%2520are%2520either%2520computationally%2520intensive%2520or%2520exhibit%2520slow%250Aconvergence%252C%2520limiting%2520their%2520scalability%2520to%2520large-scale%2520problems.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520first-order%2520proximal%2520gradient%2520algorithm%2520designed%250Ato%2520solve%2520the%2520perspective%2520relaxation%2520of%2520the%2520problem%2520within%2520a%2520BnB%2520framework.%250ASpecifically%252C%2520we%2520formulate%2520the%2520relaxed%2520problem%2520as%2520a%2520composite%2520optimization%250Aproblem%2520and%2520demonstrate%2520that%2520the%2520proximal%2520operator%2520of%2520the%2520non-smooth%2520component%250Acan%2520be%2520computed%2520exactly%2520in%2520log-linear%2520time%2520complexity%252C%2520eliminating%2520the%2520need%2520to%250Asolve%2520a%2520computationally%2520expensive%2520second-order%2520cone%2520program.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520simple%2520restart%2520strategy%2520that%2520enhances%2520convergence%2520speed%2520while%250Amaintaining%2520low%2520per-iteration%2520complexity.%2520Extensive%2520experiments%2520on%2520synthetic%250Aand%2520real-world%2520datasets%2520show%2520that%2520our%2520approach%2520significantly%2520accelerates%2520dual%250Abound%2520computations%2520and%2520is%2520highly%2520effective%2520in%2520providing%2520optimality%2520certificates%250Afor%2520large-scale%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20First-order%20Method%20for%20Certifying%20Optimal%20k-Sparse%20GLMs&entry.906535625=Jiachang%20Liu%20and%20Soroosh%20Shafiee%20and%20Andrea%20Lodi&entry.1292438233=%20%20This%20paper%20investigates%20the%20problem%20of%20certifying%20optimality%20for%20sparse%0Ageneralized%20linear%20models%20%28GLMs%29%2C%20where%20sparsity%20is%20enforced%20through%20an%0A%24%5Cell_0%24%20cardinality%20constraint.%20While%20branch-and-bound%20%28BnB%29%20frameworks%20can%0Acertify%20optimality%20by%20pruning%20nodes%20using%20dual%20bounds%2C%20existing%20methods%20for%0Acomputing%20these%20bounds%20are%20either%20computationally%20intensive%20or%20exhibit%20slow%0Aconvergence%2C%20limiting%20their%20scalability%20to%20large-scale%20problems.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20first-order%20proximal%20gradient%20algorithm%20designed%0Ato%20solve%20the%20perspective%20relaxation%20of%20the%20problem%20within%20a%20BnB%20framework.%0ASpecifically%2C%20we%20formulate%20the%20relaxed%20problem%20as%20a%20composite%20optimization%0Aproblem%20and%20demonstrate%20that%20the%20proximal%20operator%20of%20the%20non-smooth%20component%0Acan%20be%20computed%20exactly%20in%20log-linear%20time%20complexity%2C%20eliminating%20the%20need%20to%0Asolve%20a%20computationally%20expensive%20second-order%20cone%20program.%20Furthermore%2C%20we%0Aintroduce%20a%20simple%20restart%20strategy%20that%20enhances%20convergence%20speed%20while%0Amaintaining%20low%20per-iteration%20complexity.%20Extensive%20experiments%20on%20synthetic%0Aand%20real-world%20datasets%20show%20that%20our%20approach%20significantly%20accelerates%20dual%0Abound%20computations%20and%20is%20highly%20effective%20in%20providing%20optimality%20certificates%0Afor%20large-scale%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09502v1&entry.124074799=Read"},
{"title": "Fast Tensor Completion via Approximate Richardson Iteration", "author": "Mehrdad Ghadiri and Matthew Fahrbach and Yunbum Kook and Ali Jadbabaie", "abstract": "  We study tensor completion (TC) through the lens of low-rank tensor\ndecomposition (TD). Many TD algorithms use fast alternating minimization\nmethods, which solve highly structured linear regression problems at each step\n(e.g., for CP, Tucker, and tensor-train decompositions). However, such\nalgebraic structure is lost in TC regression problems, making direct extensions\nunclear. To address this, we propose a lifting approach that approximately\nsolves TC regression problems using structured TD regression algorithms as\nblackbox subroutines, enabling sublinear-time methods. We theoretically analyze\nthe convergence rate of our approximate Richardson iteration based algorithm,\nand we demonstrate on real-world tensors that its running time can be 100x\nfaster than direct methods for CP completion.\n", "link": "http://arxiv.org/abs/2502.09534v1", "date": "2025-02-13", "relevancy": 1.8226, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.479}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4482}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Tensor%20Completion%20via%20Approximate%20Richardson%20Iteration&body=Title%3A%20Fast%20Tensor%20Completion%20via%20Approximate%20Richardson%20Iteration%0AAuthor%3A%20Mehrdad%20Ghadiri%20and%20Matthew%20Fahrbach%20and%20Yunbum%20Kook%20and%20Ali%20Jadbabaie%0AAbstract%3A%20%20%20We%20study%20tensor%20completion%20%28TC%29%20through%20the%20lens%20of%20low-rank%20tensor%0Adecomposition%20%28TD%29.%20Many%20TD%20algorithms%20use%20fast%20alternating%20minimization%0Amethods%2C%20which%20solve%20highly%20structured%20linear%20regression%20problems%20at%20each%20step%0A%28e.g.%2C%20for%20CP%2C%20Tucker%2C%20and%20tensor-train%20decompositions%29.%20However%2C%20such%0Aalgebraic%20structure%20is%20lost%20in%20TC%20regression%20problems%2C%20making%20direct%20extensions%0Aunclear.%20To%20address%20this%2C%20we%20propose%20a%20lifting%20approach%20that%20approximately%0Asolves%20TC%20regression%20problems%20using%20structured%20TD%20regression%20algorithms%20as%0Ablackbox%20subroutines%2C%20enabling%20sublinear-time%20methods.%20We%20theoretically%20analyze%0Athe%20convergence%20rate%20of%20our%20approximate%20Richardson%20iteration%20based%20algorithm%2C%0Aand%20we%20demonstrate%20on%20real-world%20tensors%20that%20its%20running%20time%20can%20be%20100x%0Afaster%20than%20direct%20methods%20for%20CP%20completion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Tensor%2520Completion%2520via%2520Approximate%2520Richardson%2520Iteration%26entry.906535625%3DMehrdad%2520Ghadiri%2520and%2520Matthew%2520Fahrbach%2520and%2520Yunbum%2520Kook%2520and%2520Ali%2520Jadbabaie%26entry.1292438233%3D%2520%2520We%2520study%2520tensor%2520completion%2520%2528TC%2529%2520through%2520the%2520lens%2520of%2520low-rank%2520tensor%250Adecomposition%2520%2528TD%2529.%2520Many%2520TD%2520algorithms%2520use%2520fast%2520alternating%2520minimization%250Amethods%252C%2520which%2520solve%2520highly%2520structured%2520linear%2520regression%2520problems%2520at%2520each%2520step%250A%2528e.g.%252C%2520for%2520CP%252C%2520Tucker%252C%2520and%2520tensor-train%2520decompositions%2529.%2520However%252C%2520such%250Aalgebraic%2520structure%2520is%2520lost%2520in%2520TC%2520regression%2520problems%252C%2520making%2520direct%2520extensions%250Aunclear.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520lifting%2520approach%2520that%2520approximately%250Asolves%2520TC%2520regression%2520problems%2520using%2520structured%2520TD%2520regression%2520algorithms%2520as%250Ablackbox%2520subroutines%252C%2520enabling%2520sublinear-time%2520methods.%2520We%2520theoretically%2520analyze%250Athe%2520convergence%2520rate%2520of%2520our%2520approximate%2520Richardson%2520iteration%2520based%2520algorithm%252C%250Aand%2520we%2520demonstrate%2520on%2520real-world%2520tensors%2520that%2520its%2520running%2520time%2520can%2520be%2520100x%250Afaster%2520than%2520direct%2520methods%2520for%2520CP%2520completion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Tensor%20Completion%20via%20Approximate%20Richardson%20Iteration&entry.906535625=Mehrdad%20Ghadiri%20and%20Matthew%20Fahrbach%20and%20Yunbum%20Kook%20and%20Ali%20Jadbabaie&entry.1292438233=%20%20We%20study%20tensor%20completion%20%28TC%29%20through%20the%20lens%20of%20low-rank%20tensor%0Adecomposition%20%28TD%29.%20Many%20TD%20algorithms%20use%20fast%20alternating%20minimization%0Amethods%2C%20which%20solve%20highly%20structured%20linear%20regression%20problems%20at%20each%20step%0A%28e.g.%2C%20for%20CP%2C%20Tucker%2C%20and%20tensor-train%20decompositions%29.%20However%2C%20such%0Aalgebraic%20structure%20is%20lost%20in%20TC%20regression%20problems%2C%20making%20direct%20extensions%0Aunclear.%20To%20address%20this%2C%20we%20propose%20a%20lifting%20approach%20that%20approximately%0Asolves%20TC%20regression%20problems%20using%20structured%20TD%20regression%20algorithms%20as%0Ablackbox%20subroutines%2C%20enabling%20sublinear-time%20methods.%20We%20theoretically%20analyze%0Athe%20convergence%20rate%20of%20our%20approximate%20Richardson%20iteration%20based%20algorithm%2C%0Aand%20we%20demonstrate%20on%20real-world%20tensors%20that%20its%20running%20time%20can%20be%20100x%0Afaster%20than%20direct%20methods%20for%20CP%20completion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09534v1&entry.124074799=Read"},
{"title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning", "author": "Xinyin Ma and Guangnian Wan and Runpeng Yu and Gongfan Fang and Xinchao Wang", "abstract": "  Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.\n", "link": "http://arxiv.org/abs/2502.09601v1", "date": "2025-02-13", "relevancy": 1.7953, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoT-Valve%3A%20Length-Compressible%20Chain-of-Thought%20Tuning&body=Title%3A%20CoT-Valve%3A%20Length-Compressible%20Chain-of-Thought%20Tuning%0AAuthor%3A%20Xinyin%20Ma%20and%20Guangnian%20Wan%20and%20Runpeng%20Yu%20and%20Gongfan%20Fang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Chain-of-Thought%20significantly%20enhances%20a%20model%27s%20reasoning%20capability%2C%20but%0Ait%20also%20comes%20with%20a%20considerable%20increase%20in%20inference%20costs%20due%20to%20long%0Achains.%20With%20the%20observation%20that%20the%20reasoning%20path%20can%20be%20easily%20compressed%0Aunder%20easy%20tasks%20but%20struggle%20on%20hard%20tasks%2C%20we%20explore%20the%20feasibility%20of%0Aelastically%20controlling%20the%20length%20of%20reasoning%20paths%20with%20only%20one%20model%2C%0Athereby%20reducing%20the%20inference%20overhead%20of%20reasoning%20models%20dynamically%20based%0Aon%20task%20difficulty.%20We%20introduce%20a%20new%20tuning%20and%20inference%20strategy%20named%0ACoT-Valve%2C%20designed%20to%20allow%20models%20to%20generate%20reasoning%20chains%20of%20varying%0Alengths.%20To%20achieve%20this%2C%20we%20propose%20to%20identify%20a%20direction%20in%20the%20parameter%0Aspace%20that%2C%20when%20manipulated%2C%20can%20effectively%20control%20the%20length%20of%20generated%0ACoT.%20Moreover%2C%20we%20show%20that%20this%20property%20is%20valuable%20for%20compressing%20the%0Areasoning%20chain.%20We%20construct%20datasets%20with%20chains%20from%20long%20to%20short%20for%20the%0Asame%20questions%20and%20explore%20two%20enhanced%20strategies%20for%20CoT-Valve%3A%20%281%29%20a%20precise%0Alength-compressible%20CoT%20tuning%20method%2C%20and%20%282%29%20a%20progressive%20chain%20length%0Acompression%20approach.%20Our%20experiments%20show%20that%20CoT-Valve%20successfully%20enables%0Acontrollability%20and%20compressibility%20of%20the%20chain%20and%20shows%20better%20performance%0Athan%20the%20prompt-based%20control.%20We%20applied%20this%20method%20to%20QwQ-32B-Preview%2C%0Areducing%20reasoning%20chains%20on%20GSM8K%20from%20741%20to%20225%20tokens%20with%20a%20minor%0Aperformance%20drop%20%2895.07%25%20to%2094.92%25%29%20and%20on%20AIME%20from%206827%20to%204629%20tokens%2C%20with%0Aonly%20one%20additional%20incorrect%20answer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoT-Valve%253A%2520Length-Compressible%2520Chain-of-Thought%2520Tuning%26entry.906535625%3DXinyin%2520Ma%2520and%2520Guangnian%2520Wan%2520and%2520Runpeng%2520Yu%2520and%2520Gongfan%2520Fang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520significantly%2520enhances%2520a%2520model%2527s%2520reasoning%2520capability%252C%2520but%250Ait%2520also%2520comes%2520with%2520a%2520considerable%2520increase%2520in%2520inference%2520costs%2520due%2520to%2520long%250Achains.%2520With%2520the%2520observation%2520that%2520the%2520reasoning%2520path%2520can%2520be%2520easily%2520compressed%250Aunder%2520easy%2520tasks%2520but%2520struggle%2520on%2520hard%2520tasks%252C%2520we%2520explore%2520the%2520feasibility%2520of%250Aelastically%2520controlling%2520the%2520length%2520of%2520reasoning%2520paths%2520with%2520only%2520one%2520model%252C%250Athereby%2520reducing%2520the%2520inference%2520overhead%2520of%2520reasoning%2520models%2520dynamically%2520based%250Aon%2520task%2520difficulty.%2520We%2520introduce%2520a%2520new%2520tuning%2520and%2520inference%2520strategy%2520named%250ACoT-Valve%252C%2520designed%2520to%2520allow%2520models%2520to%2520generate%2520reasoning%2520chains%2520of%2520varying%250Alengths.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520to%2520identify%2520a%2520direction%2520in%2520the%2520parameter%250Aspace%2520that%252C%2520when%2520manipulated%252C%2520can%2520effectively%2520control%2520the%2520length%2520of%2520generated%250ACoT.%2520Moreover%252C%2520we%2520show%2520that%2520this%2520property%2520is%2520valuable%2520for%2520compressing%2520the%250Areasoning%2520chain.%2520We%2520construct%2520datasets%2520with%2520chains%2520from%2520long%2520to%2520short%2520for%2520the%250Asame%2520questions%2520and%2520explore%2520two%2520enhanced%2520strategies%2520for%2520CoT-Valve%253A%2520%25281%2529%2520a%2520precise%250Alength-compressible%2520CoT%2520tuning%2520method%252C%2520and%2520%25282%2529%2520a%2520progressive%2520chain%2520length%250Acompression%2520approach.%2520Our%2520experiments%2520show%2520that%2520CoT-Valve%2520successfully%2520enables%250Acontrollability%2520and%2520compressibility%2520of%2520the%2520chain%2520and%2520shows%2520better%2520performance%250Athan%2520the%2520prompt-based%2520control.%2520We%2520applied%2520this%2520method%2520to%2520QwQ-32B-Preview%252C%250Areducing%2520reasoning%2520chains%2520on%2520GSM8K%2520from%2520741%2520to%2520225%2520tokens%2520with%2520a%2520minor%250Aperformance%2520drop%2520%252895.07%2525%2520to%252094.92%2525%2529%2520and%2520on%2520AIME%2520from%25206827%2520to%25204629%2520tokens%252C%2520with%250Aonly%2520one%2520additional%2520incorrect%2520answer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoT-Valve%3A%20Length-Compressible%20Chain-of-Thought%20Tuning&entry.906535625=Xinyin%20Ma%20and%20Guangnian%20Wan%20and%20Runpeng%20Yu%20and%20Gongfan%20Fang%20and%20Xinchao%20Wang&entry.1292438233=%20%20Chain-of-Thought%20significantly%20enhances%20a%20model%27s%20reasoning%20capability%2C%20but%0Ait%20also%20comes%20with%20a%20considerable%20increase%20in%20inference%20costs%20due%20to%20long%0Achains.%20With%20the%20observation%20that%20the%20reasoning%20path%20can%20be%20easily%20compressed%0Aunder%20easy%20tasks%20but%20struggle%20on%20hard%20tasks%2C%20we%20explore%20the%20feasibility%20of%0Aelastically%20controlling%20the%20length%20of%20reasoning%20paths%20with%20only%20one%20model%2C%0Athereby%20reducing%20the%20inference%20overhead%20of%20reasoning%20models%20dynamically%20based%0Aon%20task%20difficulty.%20We%20introduce%20a%20new%20tuning%20and%20inference%20strategy%20named%0ACoT-Valve%2C%20designed%20to%20allow%20models%20to%20generate%20reasoning%20chains%20of%20varying%0Alengths.%20To%20achieve%20this%2C%20we%20propose%20to%20identify%20a%20direction%20in%20the%20parameter%0Aspace%20that%2C%20when%20manipulated%2C%20can%20effectively%20control%20the%20length%20of%20generated%0ACoT.%20Moreover%2C%20we%20show%20that%20this%20property%20is%20valuable%20for%20compressing%20the%0Areasoning%20chain.%20We%20construct%20datasets%20with%20chains%20from%20long%20to%20short%20for%20the%0Asame%20questions%20and%20explore%20two%20enhanced%20strategies%20for%20CoT-Valve%3A%20%281%29%20a%20precise%0Alength-compressible%20CoT%20tuning%20method%2C%20and%20%282%29%20a%20progressive%20chain%20length%0Acompression%20approach.%20Our%20experiments%20show%20that%20CoT-Valve%20successfully%20enables%0Acontrollability%20and%20compressibility%20of%20the%20chain%20and%20shows%20better%20performance%0Athan%20the%20prompt-based%20control.%20We%20applied%20this%20method%20to%20QwQ-32B-Preview%2C%0Areducing%20reasoning%20chains%20on%20GSM8K%20from%20741%20to%20225%20tokens%20with%20a%20minor%0Aperformance%20drop%20%2895.07%25%20to%2094.92%25%29%20and%20on%20AIME%20from%206827%20to%204629%20tokens%2C%20with%0Aonly%20one%20additional%20incorrect%20answer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09601v1&entry.124074799=Read"},
{"title": "Truth Knows No Language: Evaluating Truthfulness Beyond English", "author": "Blanca Calvo Figueras and Eneko Sagarzazu and Julen Etxaniz and Jeremy Barnes and Pablo Gamallo and Iria De Dios Flores and Rodrigo Agerri", "abstract": "  We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.\n", "link": "http://arxiv.org/abs/2502.09387v1", "date": "2025-02-13", "relevancy": 1.7946, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Truth%20Knows%20No%20Language%3A%20Evaluating%20Truthfulness%20Beyond%20English&body=Title%3A%20Truth%20Knows%20No%20Language%3A%20Evaluating%20Truthfulness%20Beyond%20English%0AAuthor%3A%20Blanca%20Calvo%20Figueras%20and%20Eneko%20Sagarzazu%20and%20Julen%20Etxaniz%20and%20Jeremy%20Barnes%20and%20Pablo%20Gamallo%20and%20Iria%20De%20Dios%20Flores%20and%20Rodrigo%20Agerri%0AAbstract%3A%20%20%20We%20introduce%20a%20professionally%20translated%20extension%20of%20the%20TruthfulQA%0Abenchmark%20designed%20to%20evaluate%20truthfulness%20in%20Basque%2C%20Catalan%2C%20Galician%2C%20and%0ASpanish.%20Truthfulness%20evaluations%20of%20large%20language%20models%20%28LLMs%29%20have%0Aprimarily%20been%20conducted%20in%20English.%20However%2C%20the%20ability%20of%20LLMs%20to%20maintain%0Atruthfulness%20across%20languages%20remains%20under-explored.%20Our%20study%20evaluates%2012%0Astate-of-the-art%20open%20LLMs%2C%20comparing%20base%20and%20instruction-tuned%20models%20using%0Ahuman%20evaluation%2C%20multiple-choice%20metrics%2C%20and%20LLM-as-a-Judge%20scoring.%20Our%0Afindings%20reveal%20that%2C%20while%20LLMs%20perform%20best%20in%20English%20and%20worst%20in%20Basque%0A%28the%20lowest-resourced%20language%29%2C%20overall%20truthfulness%20discrepancies%20across%0Alanguages%20are%20smaller%20than%20anticipated.%20Furthermore%2C%20we%20show%20that%0ALLM-as-a-Judge%20correlates%20more%20closely%20with%20human%20judgments%20than%0Amultiple-choice%20metrics%2C%20and%20that%20informativeness%20plays%20a%20critical%20role%20in%0Atruthfulness%20assessment.%20Our%20results%20also%20indicate%20that%20machine%20translation%0Aprovides%20a%20viable%20approach%20for%20extending%20truthfulness%20benchmarks%20to%20additional%0Alanguages%2C%20offering%20a%20scalable%20alternative%20to%20professional%20translation.%0AFinally%2C%20we%20observe%20that%20universal%20knowledge%20questions%20are%20better%20handled%0Aacross%20languages%20than%20context-%20and%20time-dependent%20ones%2C%20highlighting%20the%20need%0Afor%20truthfulness%20evaluations%20that%20account%20for%20cultural%20and%20temporal%0Avariability.%20Dataset%20and%20code%20are%20publicly%20available%20under%20open%20licenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruth%2520Knows%2520No%2520Language%253A%2520Evaluating%2520Truthfulness%2520Beyond%2520English%26entry.906535625%3DBlanca%2520Calvo%2520Figueras%2520and%2520Eneko%2520Sagarzazu%2520and%2520Julen%2520Etxaniz%2520and%2520Jeremy%2520Barnes%2520and%2520Pablo%2520Gamallo%2520and%2520Iria%2520De%2520Dios%2520Flores%2520and%2520Rodrigo%2520Agerri%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520professionally%2520translated%2520extension%2520of%2520the%2520TruthfulQA%250Abenchmark%2520designed%2520to%2520evaluate%2520truthfulness%2520in%2520Basque%252C%2520Catalan%252C%2520Galician%252C%2520and%250ASpanish.%2520Truthfulness%2520evaluations%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Aprimarily%2520been%2520conducted%2520in%2520English.%2520However%252C%2520the%2520ability%2520of%2520LLMs%2520to%2520maintain%250Atruthfulness%2520across%2520languages%2520remains%2520under-explored.%2520Our%2520study%2520evaluates%252012%250Astate-of-the-art%2520open%2520LLMs%252C%2520comparing%2520base%2520and%2520instruction-tuned%2520models%2520using%250Ahuman%2520evaluation%252C%2520multiple-choice%2520metrics%252C%2520and%2520LLM-as-a-Judge%2520scoring.%2520Our%250Afindings%2520reveal%2520that%252C%2520while%2520LLMs%2520perform%2520best%2520in%2520English%2520and%2520worst%2520in%2520Basque%250A%2528the%2520lowest-resourced%2520language%2529%252C%2520overall%2520truthfulness%2520discrepancies%2520across%250Alanguages%2520are%2520smaller%2520than%2520anticipated.%2520Furthermore%252C%2520we%2520show%2520that%250ALLM-as-a-Judge%2520correlates%2520more%2520closely%2520with%2520human%2520judgments%2520than%250Amultiple-choice%2520metrics%252C%2520and%2520that%2520informativeness%2520plays%2520a%2520critical%2520role%2520in%250Atruthfulness%2520assessment.%2520Our%2520results%2520also%2520indicate%2520that%2520machine%2520translation%250Aprovides%2520a%2520viable%2520approach%2520for%2520extending%2520truthfulness%2520benchmarks%2520to%2520additional%250Alanguages%252C%2520offering%2520a%2520scalable%2520alternative%2520to%2520professional%2520translation.%250AFinally%252C%2520we%2520observe%2520that%2520universal%2520knowledge%2520questions%2520are%2520better%2520handled%250Aacross%2520languages%2520than%2520context-%2520and%2520time-dependent%2520ones%252C%2520highlighting%2520the%2520need%250Afor%2520truthfulness%2520evaluations%2520that%2520account%2520for%2520cultural%2520and%2520temporal%250Avariability.%2520Dataset%2520and%2520code%2520are%2520publicly%2520available%2520under%2520open%2520licenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truth%20Knows%20No%20Language%3A%20Evaluating%20Truthfulness%20Beyond%20English&entry.906535625=Blanca%20Calvo%20Figueras%20and%20Eneko%20Sagarzazu%20and%20Julen%20Etxaniz%20and%20Jeremy%20Barnes%20and%20Pablo%20Gamallo%20and%20Iria%20De%20Dios%20Flores%20and%20Rodrigo%20Agerri&entry.1292438233=%20%20We%20introduce%20a%20professionally%20translated%20extension%20of%20the%20TruthfulQA%0Abenchmark%20designed%20to%20evaluate%20truthfulness%20in%20Basque%2C%20Catalan%2C%20Galician%2C%20and%0ASpanish.%20Truthfulness%20evaluations%20of%20large%20language%20models%20%28LLMs%29%20have%0Aprimarily%20been%20conducted%20in%20English.%20However%2C%20the%20ability%20of%20LLMs%20to%20maintain%0Atruthfulness%20across%20languages%20remains%20under-explored.%20Our%20study%20evaluates%2012%0Astate-of-the-art%20open%20LLMs%2C%20comparing%20base%20and%20instruction-tuned%20models%20using%0Ahuman%20evaluation%2C%20multiple-choice%20metrics%2C%20and%20LLM-as-a-Judge%20scoring.%20Our%0Afindings%20reveal%20that%2C%20while%20LLMs%20perform%20best%20in%20English%20and%20worst%20in%20Basque%0A%28the%20lowest-resourced%20language%29%2C%20overall%20truthfulness%20discrepancies%20across%0Alanguages%20are%20smaller%20than%20anticipated.%20Furthermore%2C%20we%20show%20that%0ALLM-as-a-Judge%20correlates%20more%20closely%20with%20human%20judgments%20than%0Amultiple-choice%20metrics%2C%20and%20that%20informativeness%20plays%20a%20critical%20role%20in%0Atruthfulness%20assessment.%20Our%20results%20also%20indicate%20that%20machine%20translation%0Aprovides%20a%20viable%20approach%20for%20extending%20truthfulness%20benchmarks%20to%20additional%0Alanguages%2C%20offering%20a%20scalable%20alternative%20to%20professional%20translation.%0AFinally%2C%20we%20observe%20that%20universal%20knowledge%20questions%20are%20better%20handled%0Aacross%20languages%20than%20context-%20and%20time-dependent%20ones%2C%20highlighting%20the%20need%0Afor%20truthfulness%20evaluations%20that%20account%20for%20cultural%20and%20temporal%0Avariability.%20Dataset%20and%20code%20are%20publicly%20available%20under%20open%20licenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09387v1&entry.124074799=Read"},
{"title": "Generalizable Reinforcement Learning with Biologically Inspired\n  Hyperdimensional Occupancy Grid Maps for Exploration and Goal-Directed Path\n  Planning", "author": "Shay Snyder and Ryan Shea and Andrew Capodieci and David Gorsich and Maryam Parsa", "abstract": "  Real-time autonomous systems utilize multi-layer computational frameworks to\nperform critical tasks such as perception, goal finding, and path planning.\nTraditional methods implement perception using occupancy grid mapping (OGM),\nsegmenting the environment into discretized cells with probabilistic\ninformation. This classical approach is well-established and provides a\nstructured input for downstream processes like goal finding and path planning\nalgorithms. Recent approaches leverage a biologically inspired mathematical\nframework known as vector symbolic architectures (VSA), commonly known as\nhyperdimensional computing, to perform probabilistic OGM in hyperdimensional\nspace. This approach, VSA-OGM, provides native compatibility with spiking\nneural networks, positioning VSA-OGM as a potential neuromorphic alternative to\nconventional OGM. However, for large-scale integration, it is essential to\nassess the performance implications of VSA-OGM on downstream tasks compared to\nestablished OGM methods. This study examines the efficacy of VSA-OGM against a\ntraditional OGM approach, Bayesian Hilbert Maps (BHM), within reinforcement\nlearning based goal finding and path planning frameworks, across a controlled\nexploration environment and an autonomous driving scenario inspired by the\nF1-Tenth challenge. Our results demonstrate that VSA-OGM maintains comparable\nlearning performance across single and multi-scenario training configurations\nwhile improving performance on unseen environments by approximately 47%. These\nfindings highlight the increased generalizability of policy networks trained\nwith VSA-OGM over BHM, reinforcing its potential for real-world deployment in\ndiverse environments.\n", "link": "http://arxiv.org/abs/2502.09393v1", "date": "2025-02-13", "relevancy": 1.7882, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6201}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Reinforcement%20Learning%20with%20Biologically%20Inspired%0A%20%20Hyperdimensional%20Occupancy%20Grid%20Maps%20for%20Exploration%20and%20Goal-Directed%20Path%0A%20%20Planning&body=Title%3A%20Generalizable%20Reinforcement%20Learning%20with%20Biologically%20Inspired%0A%20%20Hyperdimensional%20Occupancy%20Grid%20Maps%20for%20Exploration%20and%20Goal-Directed%20Path%0A%20%20Planning%0AAuthor%3A%20Shay%20Snyder%20and%20Ryan%20Shea%20and%20Andrew%20Capodieci%20and%20David%20Gorsich%20and%20Maryam%20Parsa%0AAbstract%3A%20%20%20Real-time%20autonomous%20systems%20utilize%20multi-layer%20computational%20frameworks%20to%0Aperform%20critical%20tasks%20such%20as%20perception%2C%20goal%20finding%2C%20and%20path%20planning.%0ATraditional%20methods%20implement%20perception%20using%20occupancy%20grid%20mapping%20%28OGM%29%2C%0Asegmenting%20the%20environment%20into%20discretized%20cells%20with%20probabilistic%0Ainformation.%20This%20classical%20approach%20is%20well-established%20and%20provides%20a%0Astructured%20input%20for%20downstream%20processes%20like%20goal%20finding%20and%20path%20planning%0Aalgorithms.%20Recent%20approaches%20leverage%20a%20biologically%20inspired%20mathematical%0Aframework%20known%20as%20vector%20symbolic%20architectures%20%28VSA%29%2C%20commonly%20known%20as%0Ahyperdimensional%20computing%2C%20to%20perform%20probabilistic%20OGM%20in%20hyperdimensional%0Aspace.%20This%20approach%2C%20VSA-OGM%2C%20provides%20native%20compatibility%20with%20spiking%0Aneural%20networks%2C%20positioning%20VSA-OGM%20as%20a%20potential%20neuromorphic%20alternative%20to%0Aconventional%20OGM.%20However%2C%20for%20large-scale%20integration%2C%20it%20is%20essential%20to%0Aassess%20the%20performance%20implications%20of%20VSA-OGM%20on%20downstream%20tasks%20compared%20to%0Aestablished%20OGM%20methods.%20This%20study%20examines%20the%20efficacy%20of%20VSA-OGM%20against%20a%0Atraditional%20OGM%20approach%2C%20Bayesian%20Hilbert%20Maps%20%28BHM%29%2C%20within%20reinforcement%0Alearning%20based%20goal%20finding%20and%20path%20planning%20frameworks%2C%20across%20a%20controlled%0Aexploration%20environment%20and%20an%20autonomous%20driving%20scenario%20inspired%20by%20the%0AF1-Tenth%20challenge.%20Our%20results%20demonstrate%20that%20VSA-OGM%20maintains%20comparable%0Alearning%20performance%20across%20single%20and%20multi-scenario%20training%20configurations%0Awhile%20improving%20performance%20on%20unseen%20environments%20by%20approximately%2047%25.%20These%0Afindings%20highlight%20the%20increased%20generalizability%20of%20policy%20networks%20trained%0Awith%20VSA-OGM%20over%20BHM%2C%20reinforcing%20its%20potential%20for%20real-world%20deployment%20in%0Adiverse%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Reinforcement%2520Learning%2520with%2520Biologically%2520Inspired%250A%2520%2520Hyperdimensional%2520Occupancy%2520Grid%2520Maps%2520for%2520Exploration%2520and%2520Goal-Directed%2520Path%250A%2520%2520Planning%26entry.906535625%3DShay%2520Snyder%2520and%2520Ryan%2520Shea%2520and%2520Andrew%2520Capodieci%2520and%2520David%2520Gorsich%2520and%2520Maryam%2520Parsa%26entry.1292438233%3D%2520%2520Real-time%2520autonomous%2520systems%2520utilize%2520multi-layer%2520computational%2520frameworks%2520to%250Aperform%2520critical%2520tasks%2520such%2520as%2520perception%252C%2520goal%2520finding%252C%2520and%2520path%2520planning.%250ATraditional%2520methods%2520implement%2520perception%2520using%2520occupancy%2520grid%2520mapping%2520%2528OGM%2529%252C%250Asegmenting%2520the%2520environment%2520into%2520discretized%2520cells%2520with%2520probabilistic%250Ainformation.%2520This%2520classical%2520approach%2520is%2520well-established%2520and%2520provides%2520a%250Astructured%2520input%2520for%2520downstream%2520processes%2520like%2520goal%2520finding%2520and%2520path%2520planning%250Aalgorithms.%2520Recent%2520approaches%2520leverage%2520a%2520biologically%2520inspired%2520mathematical%250Aframework%2520known%2520as%2520vector%2520symbolic%2520architectures%2520%2528VSA%2529%252C%2520commonly%2520known%2520as%250Ahyperdimensional%2520computing%252C%2520to%2520perform%2520probabilistic%2520OGM%2520in%2520hyperdimensional%250Aspace.%2520This%2520approach%252C%2520VSA-OGM%252C%2520provides%2520native%2520compatibility%2520with%2520spiking%250Aneural%2520networks%252C%2520positioning%2520VSA-OGM%2520as%2520a%2520potential%2520neuromorphic%2520alternative%2520to%250Aconventional%2520OGM.%2520However%252C%2520for%2520large-scale%2520integration%252C%2520it%2520is%2520essential%2520to%250Aassess%2520the%2520performance%2520implications%2520of%2520VSA-OGM%2520on%2520downstream%2520tasks%2520compared%2520to%250Aestablished%2520OGM%2520methods.%2520This%2520study%2520examines%2520the%2520efficacy%2520of%2520VSA-OGM%2520against%2520a%250Atraditional%2520OGM%2520approach%252C%2520Bayesian%2520Hilbert%2520Maps%2520%2528BHM%2529%252C%2520within%2520reinforcement%250Alearning%2520based%2520goal%2520finding%2520and%2520path%2520planning%2520frameworks%252C%2520across%2520a%2520controlled%250Aexploration%2520environment%2520and%2520an%2520autonomous%2520driving%2520scenario%2520inspired%2520by%2520the%250AF1-Tenth%2520challenge.%2520Our%2520results%2520demonstrate%2520that%2520VSA-OGM%2520maintains%2520comparable%250Alearning%2520performance%2520across%2520single%2520and%2520multi-scenario%2520training%2520configurations%250Awhile%2520improving%2520performance%2520on%2520unseen%2520environments%2520by%2520approximately%252047%2525.%2520These%250Afindings%2520highlight%2520the%2520increased%2520generalizability%2520of%2520policy%2520networks%2520trained%250Awith%2520VSA-OGM%2520over%2520BHM%252C%2520reinforcing%2520its%2520potential%2520for%2520real-world%2520deployment%2520in%250Adiverse%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Reinforcement%20Learning%20with%20Biologically%20Inspired%0A%20%20Hyperdimensional%20Occupancy%20Grid%20Maps%20for%20Exploration%20and%20Goal-Directed%20Path%0A%20%20Planning&entry.906535625=Shay%20Snyder%20and%20Ryan%20Shea%20and%20Andrew%20Capodieci%20and%20David%20Gorsich%20and%20Maryam%20Parsa&entry.1292438233=%20%20Real-time%20autonomous%20systems%20utilize%20multi-layer%20computational%20frameworks%20to%0Aperform%20critical%20tasks%20such%20as%20perception%2C%20goal%20finding%2C%20and%20path%20planning.%0ATraditional%20methods%20implement%20perception%20using%20occupancy%20grid%20mapping%20%28OGM%29%2C%0Asegmenting%20the%20environment%20into%20discretized%20cells%20with%20probabilistic%0Ainformation.%20This%20classical%20approach%20is%20well-established%20and%20provides%20a%0Astructured%20input%20for%20downstream%20processes%20like%20goal%20finding%20and%20path%20planning%0Aalgorithms.%20Recent%20approaches%20leverage%20a%20biologically%20inspired%20mathematical%0Aframework%20known%20as%20vector%20symbolic%20architectures%20%28VSA%29%2C%20commonly%20known%20as%0Ahyperdimensional%20computing%2C%20to%20perform%20probabilistic%20OGM%20in%20hyperdimensional%0Aspace.%20This%20approach%2C%20VSA-OGM%2C%20provides%20native%20compatibility%20with%20spiking%0Aneural%20networks%2C%20positioning%20VSA-OGM%20as%20a%20potential%20neuromorphic%20alternative%20to%0Aconventional%20OGM.%20However%2C%20for%20large-scale%20integration%2C%20it%20is%20essential%20to%0Aassess%20the%20performance%20implications%20of%20VSA-OGM%20on%20downstream%20tasks%20compared%20to%0Aestablished%20OGM%20methods.%20This%20study%20examines%20the%20efficacy%20of%20VSA-OGM%20against%20a%0Atraditional%20OGM%20approach%2C%20Bayesian%20Hilbert%20Maps%20%28BHM%29%2C%20within%20reinforcement%0Alearning%20based%20goal%20finding%20and%20path%20planning%20frameworks%2C%20across%20a%20controlled%0Aexploration%20environment%20and%20an%20autonomous%20driving%20scenario%20inspired%20by%20the%0AF1-Tenth%20challenge.%20Our%20results%20demonstrate%20that%20VSA-OGM%20maintains%20comparable%0Alearning%20performance%20across%20single%20and%20multi-scenario%20training%20configurations%0Awhile%20improving%20performance%20on%20unseen%20environments%20by%20approximately%2047%25.%20These%0Afindings%20highlight%20the%20increased%20generalizability%20of%20policy%20networks%20trained%0Awith%20VSA-OGM%20over%20BHM%2C%20reinforcing%20its%20potential%20for%20real-world%20deployment%20in%0Adiverse%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09393v1&entry.124074799=Read"},
{"title": "Objective quantification of mood states using large language models", "author": "Jakub Onysk and Quentin Huys", "abstract": "  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n", "link": "http://arxiv.org/abs/2502.09487v1", "date": "2025-02-13", "relevancy": 1.7796, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Objective%20quantification%20of%20mood%20states%20using%20large%20language%20models&body=Title%3A%20Objective%20quantification%20of%20mood%20states%20using%20large%20language%20models%0AAuthor%3A%20Jakub%20Onysk%20and%20Quentin%20Huys%0AAbstract%3A%20%20%20Emotional%20states%20influence%20human%20behaviour%20and%20cognition%2C%20leading%20to%20diverse%0Athought%20trajectories.%20Similarly%2C%20Large%20Language%20Models%20%28LLMs%29%20showcase%20an%0Aexcellent%20level%20of%20response%20consistency%20across%20wide-ranging%20contexts%20%28prompts%29.%0AWe%20leverage%20these%20parallels%20to%20establish%20a%20framework%20for%20quantifying%20mental%0Astates.%20Our%20approach%20utilises%20self-report%20questionnaires%20that%20reliably%20assess%0Athese%20states%20due%20to%20their%20inherent%20sensitivity%20to%20patterns%20of%20co-occurring%0Aresponses.%20Specifically%2C%20we%20recruited%20a%20large%20sample%20of%20participants%20%28N%3D422%29%20to%0Ainvestigate%20how%20well%20an%20LLM%20%28Mistral-7B-OpenOrca%29%20quantifies%20a%20heterogenous%20set%0Aof%20depressive%20mood%20states%20measured%20with%20participants%27%20open-ended%20responses%20to%20a%0Adepression%20questionnaire.%20We%20show%20LLM%20responses%20to%20held-out%20multiple-choice%0Aquestions%2C%20given%20participants%27%20open-ended%20answers%2C%20correlate%20strongly%20%28r%3A%0A0.52-0.84%29%20with%20true%20questionnaire%20scores%2C%20demonstrating%20LLM%27s%20generalisation%0Afrom%20mood%20representations.%20We%20explore%20a%20link%20between%20these%20representations%20and%0Afactor%20analysis.%20Using%20ridge%20regression%2C%20we%20find%20depression-related%20subspaces%0Awithin%20LLM%20hidden%20states.%20We%20show%20these%20subspaces%20to%20be%20predictive%20of%0Aparticipants%27%20%22Depression%22%20and%20%22Somatic%20%26%20Emotional%20Distress%22%20factor%20scores%2C%20as%0Awell%20as%20suicidality%20severity.%20Overall%2C%20LLMs%20can%20provide%20quantitative%20measures%0Aof%20mental%20states.%20The%20reliability%20of%20these%20hinges%20upon%20how%20informative%20the%0Aquestions%20we%20ask%20participants%20are.%20Used%20correctly%2C%20this%20approach%20could%0Asupplement%20mental%20state%20assessment%20in%20a%20variety%20of%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjective%2520quantification%2520of%2520mood%2520states%2520using%2520large%2520language%2520models%26entry.906535625%3DJakub%2520Onysk%2520and%2520Quentin%2520Huys%26entry.1292438233%3D%2520%2520Emotional%2520states%2520influence%2520human%2520behaviour%2520and%2520cognition%252C%2520leading%2520to%2520diverse%250Athought%2520trajectories.%2520Similarly%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520showcase%2520an%250Aexcellent%2520level%2520of%2520response%2520consistency%2520across%2520wide-ranging%2520contexts%2520%2528prompts%2529.%250AWe%2520leverage%2520these%2520parallels%2520to%2520establish%2520a%2520framework%2520for%2520quantifying%2520mental%250Astates.%2520Our%2520approach%2520utilises%2520self-report%2520questionnaires%2520that%2520reliably%2520assess%250Athese%2520states%2520due%2520to%2520their%2520inherent%2520sensitivity%2520to%2520patterns%2520of%2520co-occurring%250Aresponses.%2520Specifically%252C%2520we%2520recruited%2520a%2520large%2520sample%2520of%2520participants%2520%2528N%253D422%2529%2520to%250Ainvestigate%2520how%2520well%2520an%2520LLM%2520%2528Mistral-7B-OpenOrca%2529%2520quantifies%2520a%2520heterogenous%2520set%250Aof%2520depressive%2520mood%2520states%2520measured%2520with%2520participants%2527%2520open-ended%2520responses%2520to%2520a%250Adepression%2520questionnaire.%2520We%2520show%2520LLM%2520responses%2520to%2520held-out%2520multiple-choice%250Aquestions%252C%2520given%2520participants%2527%2520open-ended%2520answers%252C%2520correlate%2520strongly%2520%2528r%253A%250A0.52-0.84%2529%2520with%2520true%2520questionnaire%2520scores%252C%2520demonstrating%2520LLM%2527s%2520generalisation%250Afrom%2520mood%2520representations.%2520We%2520explore%2520a%2520link%2520between%2520these%2520representations%2520and%250Afactor%2520analysis.%2520Using%2520ridge%2520regression%252C%2520we%2520find%2520depression-related%2520subspaces%250Awithin%2520LLM%2520hidden%2520states.%2520We%2520show%2520these%2520subspaces%2520to%2520be%2520predictive%2520of%250Aparticipants%2527%2520%2522Depression%2522%2520and%2520%2522Somatic%2520%2526%2520Emotional%2520Distress%2522%2520factor%2520scores%252C%2520as%250Awell%2520as%2520suicidality%2520severity.%2520Overall%252C%2520LLMs%2520can%2520provide%2520quantitative%2520measures%250Aof%2520mental%2520states.%2520The%2520reliability%2520of%2520these%2520hinges%2520upon%2520how%2520informative%2520the%250Aquestions%2520we%2520ask%2520participants%2520are.%2520Used%2520correctly%252C%2520this%2520approach%2520could%250Asupplement%2520mental%2520state%2520assessment%2520in%2520a%2520variety%2520of%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Objective%20quantification%20of%20mood%20states%20using%20large%20language%20models&entry.906535625=Jakub%20Onysk%20and%20Quentin%20Huys&entry.1292438233=%20%20Emotional%20states%20influence%20human%20behaviour%20and%20cognition%2C%20leading%20to%20diverse%0Athought%20trajectories.%20Similarly%2C%20Large%20Language%20Models%20%28LLMs%29%20showcase%20an%0Aexcellent%20level%20of%20response%20consistency%20across%20wide-ranging%20contexts%20%28prompts%29.%0AWe%20leverage%20these%20parallels%20to%20establish%20a%20framework%20for%20quantifying%20mental%0Astates.%20Our%20approach%20utilises%20self-report%20questionnaires%20that%20reliably%20assess%0Athese%20states%20due%20to%20their%20inherent%20sensitivity%20to%20patterns%20of%20co-occurring%0Aresponses.%20Specifically%2C%20we%20recruited%20a%20large%20sample%20of%20participants%20%28N%3D422%29%20to%0Ainvestigate%20how%20well%20an%20LLM%20%28Mistral-7B-OpenOrca%29%20quantifies%20a%20heterogenous%20set%0Aof%20depressive%20mood%20states%20measured%20with%20participants%27%20open-ended%20responses%20to%20a%0Adepression%20questionnaire.%20We%20show%20LLM%20responses%20to%20held-out%20multiple-choice%0Aquestions%2C%20given%20participants%27%20open-ended%20answers%2C%20correlate%20strongly%20%28r%3A%0A0.52-0.84%29%20with%20true%20questionnaire%20scores%2C%20demonstrating%20LLM%27s%20generalisation%0Afrom%20mood%20representations.%20We%20explore%20a%20link%20between%20these%20representations%20and%0Afactor%20analysis.%20Using%20ridge%20regression%2C%20we%20find%20depression-related%20subspaces%0Awithin%20LLM%20hidden%20states.%20We%20show%20these%20subspaces%20to%20be%20predictive%20of%0Aparticipants%27%20%22Depression%22%20and%20%22Somatic%20%26%20Emotional%20Distress%22%20factor%20scores%2C%20as%0Awell%20as%20suicidality%20severity.%20Overall%2C%20LLMs%20can%20provide%20quantitative%20measures%0Aof%20mental%20states.%20The%20reliability%20of%20these%20hinges%20upon%20how%20informative%20the%0Aquestions%20we%20ask%20participants%20are.%20Used%20correctly%2C%20this%20approach%20could%0Asupplement%20mental%20state%20assessment%20in%20a%20variety%20of%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09487v1&entry.124074799=Read"},
{"title": "Environment-Centric Learning Approach for Gait Synthesis in Terrestrial\n  Soft Robots", "author": "Caitlin Freeman and Arun Niddish Mahendran and Vishesh Vikas", "abstract": "  Locomotion gaits are fundamental for control of soft terrestrial robots.\nHowever, synthesis of these gaits is challenging due to modeling of\nrobot-environment interaction and lack of a mathematical framework. This work\npresents an environment-centric, data-driven and fault-tolerant probabilistic\nModel-Free Control (pMFC) framework that allows for soft multi-limb robots to\nlearn from their environment and synthesize diverse sets of locomotion gaits\nfor realizing open-loop control. Here, discretization of factors dominating\nrobot-environment interactions enables an environment-specific graphical\nrepresentation where the edges encode experimental locomotion data\ncorresponding to the robot motion primitives. In this graph, locomotion gaits\nare defined as simple cycles that are transformation invariant, i.e., the\nlocomotion is independent of the starting vertex of these periodic cycles. Gait\nsynthesis, the problem of finding optimal locomotion gaits for a given\nsubstrate, is formulated as Binary Integer Linear Programming (BILP) problems\nwith a linearized cost function, linear constraints, and iterative simple cycle\ndetection. Experimentally, gaits are synthesized for varying robot-environment\ninteractions. Variables include robot morphology - three-limb and four-limb\nrobots, TerreSoRo-III and TerreSoRo-IV; substrate - rubber mat, whiteboard and\ncarpet; and actuator functionality - simulated loss of robot limb actuation. On\nan average, gait synthesis improves the translation and rotation speeds by 82%\nand 97% respectively. The results highlight that data-driven methods are vital\nto soft robot locomotion control due to the significant influence of unexpected\nasymmetries in the system and the dependence of optimal gait sequences on the\nexperimental robot-environment interaction.\n", "link": "http://arxiv.org/abs/2402.03617v2", "date": "2025-02-13", "relevancy": 1.7613, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6379}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5903}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Environment-Centric%20Learning%20Approach%20for%20Gait%20Synthesis%20in%20Terrestrial%0A%20%20Soft%20Robots&body=Title%3A%20Environment-Centric%20Learning%20Approach%20for%20Gait%20Synthesis%20in%20Terrestrial%0A%20%20Soft%20Robots%0AAuthor%3A%20Caitlin%20Freeman%20and%20Arun%20Niddish%20Mahendran%20and%20Vishesh%20Vikas%0AAbstract%3A%20%20%20Locomotion%20gaits%20are%20fundamental%20for%20control%20of%20soft%20terrestrial%20robots.%0AHowever%2C%20synthesis%20of%20these%20gaits%20is%20challenging%20due%20to%20modeling%20of%0Arobot-environment%20interaction%20and%20lack%20of%20a%20mathematical%20framework.%20This%20work%0Apresents%20an%20environment-centric%2C%20data-driven%20and%20fault-tolerant%20probabilistic%0AModel-Free%20Control%20%28pMFC%29%20framework%20that%20allows%20for%20soft%20multi-limb%20robots%20to%0Alearn%20from%20their%20environment%20and%20synthesize%20diverse%20sets%20of%20locomotion%20gaits%0Afor%20realizing%20open-loop%20control.%20Here%2C%20discretization%20of%20factors%20dominating%0Arobot-environment%20interactions%20enables%20an%20environment-specific%20graphical%0Arepresentation%20where%20the%20edges%20encode%20experimental%20locomotion%20data%0Acorresponding%20to%20the%20robot%20motion%20primitives.%20In%20this%20graph%2C%20locomotion%20gaits%0Aare%20defined%20as%20simple%20cycles%20that%20are%20transformation%20invariant%2C%20i.e.%2C%20the%0Alocomotion%20is%20independent%20of%20the%20starting%20vertex%20of%20these%20periodic%20cycles.%20Gait%0Asynthesis%2C%20the%20problem%20of%20finding%20optimal%20locomotion%20gaits%20for%20a%20given%0Asubstrate%2C%20is%20formulated%20as%20Binary%20Integer%20Linear%20Programming%20%28BILP%29%20problems%0Awith%20a%20linearized%20cost%20function%2C%20linear%20constraints%2C%20and%20iterative%20simple%20cycle%0Adetection.%20Experimentally%2C%20gaits%20are%20synthesized%20for%20varying%20robot-environment%0Ainteractions.%20Variables%20include%20robot%20morphology%20-%20three-limb%20and%20four-limb%0Arobots%2C%20TerreSoRo-III%20and%20TerreSoRo-IV%3B%20substrate%20-%20rubber%20mat%2C%20whiteboard%20and%0Acarpet%3B%20and%20actuator%20functionality%20-%20simulated%20loss%20of%20robot%20limb%20actuation.%20On%0Aan%20average%2C%20gait%20synthesis%20improves%20the%20translation%20and%20rotation%20speeds%20by%2082%25%0Aand%2097%25%20respectively.%20The%20results%20highlight%20that%20data-driven%20methods%20are%20vital%0Ato%20soft%20robot%20locomotion%20control%20due%20to%20the%20significant%20influence%20of%20unexpected%0Aasymmetries%20in%20the%20system%20and%20the%20dependence%20of%20optimal%20gait%20sequences%20on%20the%0Aexperimental%20robot-environment%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03617v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvironment-Centric%2520Learning%2520Approach%2520for%2520Gait%2520Synthesis%2520in%2520Terrestrial%250A%2520%2520Soft%2520Robots%26entry.906535625%3DCaitlin%2520Freeman%2520and%2520Arun%2520Niddish%2520Mahendran%2520and%2520Vishesh%2520Vikas%26entry.1292438233%3D%2520%2520Locomotion%2520gaits%2520are%2520fundamental%2520for%2520control%2520of%2520soft%2520terrestrial%2520robots.%250AHowever%252C%2520synthesis%2520of%2520these%2520gaits%2520is%2520challenging%2520due%2520to%2520modeling%2520of%250Arobot-environment%2520interaction%2520and%2520lack%2520of%2520a%2520mathematical%2520framework.%2520This%2520work%250Apresents%2520an%2520environment-centric%252C%2520data-driven%2520and%2520fault-tolerant%2520probabilistic%250AModel-Free%2520Control%2520%2528pMFC%2529%2520framework%2520that%2520allows%2520for%2520soft%2520multi-limb%2520robots%2520to%250Alearn%2520from%2520their%2520environment%2520and%2520synthesize%2520diverse%2520sets%2520of%2520locomotion%2520gaits%250Afor%2520realizing%2520open-loop%2520control.%2520Here%252C%2520discretization%2520of%2520factors%2520dominating%250Arobot-environment%2520interactions%2520enables%2520an%2520environment-specific%2520graphical%250Arepresentation%2520where%2520the%2520edges%2520encode%2520experimental%2520locomotion%2520data%250Acorresponding%2520to%2520the%2520robot%2520motion%2520primitives.%2520In%2520this%2520graph%252C%2520locomotion%2520gaits%250Aare%2520defined%2520as%2520simple%2520cycles%2520that%2520are%2520transformation%2520invariant%252C%2520i.e.%252C%2520the%250Alocomotion%2520is%2520independent%2520of%2520the%2520starting%2520vertex%2520of%2520these%2520periodic%2520cycles.%2520Gait%250Asynthesis%252C%2520the%2520problem%2520of%2520finding%2520optimal%2520locomotion%2520gaits%2520for%2520a%2520given%250Asubstrate%252C%2520is%2520formulated%2520as%2520Binary%2520Integer%2520Linear%2520Programming%2520%2528BILP%2529%2520problems%250Awith%2520a%2520linearized%2520cost%2520function%252C%2520linear%2520constraints%252C%2520and%2520iterative%2520simple%2520cycle%250Adetection.%2520Experimentally%252C%2520gaits%2520are%2520synthesized%2520for%2520varying%2520robot-environment%250Ainteractions.%2520Variables%2520include%2520robot%2520morphology%2520-%2520three-limb%2520and%2520four-limb%250Arobots%252C%2520TerreSoRo-III%2520and%2520TerreSoRo-IV%253B%2520substrate%2520-%2520rubber%2520mat%252C%2520whiteboard%2520and%250Acarpet%253B%2520and%2520actuator%2520functionality%2520-%2520simulated%2520loss%2520of%2520robot%2520limb%2520actuation.%2520On%250Aan%2520average%252C%2520gait%2520synthesis%2520improves%2520the%2520translation%2520and%2520rotation%2520speeds%2520by%252082%2525%250Aand%252097%2525%2520respectively.%2520The%2520results%2520highlight%2520that%2520data-driven%2520methods%2520are%2520vital%250Ato%2520soft%2520robot%2520locomotion%2520control%2520due%2520to%2520the%2520significant%2520influence%2520of%2520unexpected%250Aasymmetries%2520in%2520the%2520system%2520and%2520the%2520dependence%2520of%2520optimal%2520gait%2520sequences%2520on%2520the%250Aexperimental%2520robot-environment%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03617v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Environment-Centric%20Learning%20Approach%20for%20Gait%20Synthesis%20in%20Terrestrial%0A%20%20Soft%20Robots&entry.906535625=Caitlin%20Freeman%20and%20Arun%20Niddish%20Mahendran%20and%20Vishesh%20Vikas&entry.1292438233=%20%20Locomotion%20gaits%20are%20fundamental%20for%20control%20of%20soft%20terrestrial%20robots.%0AHowever%2C%20synthesis%20of%20these%20gaits%20is%20challenging%20due%20to%20modeling%20of%0Arobot-environment%20interaction%20and%20lack%20of%20a%20mathematical%20framework.%20This%20work%0Apresents%20an%20environment-centric%2C%20data-driven%20and%20fault-tolerant%20probabilistic%0AModel-Free%20Control%20%28pMFC%29%20framework%20that%20allows%20for%20soft%20multi-limb%20robots%20to%0Alearn%20from%20their%20environment%20and%20synthesize%20diverse%20sets%20of%20locomotion%20gaits%0Afor%20realizing%20open-loop%20control.%20Here%2C%20discretization%20of%20factors%20dominating%0Arobot-environment%20interactions%20enables%20an%20environment-specific%20graphical%0Arepresentation%20where%20the%20edges%20encode%20experimental%20locomotion%20data%0Acorresponding%20to%20the%20robot%20motion%20primitives.%20In%20this%20graph%2C%20locomotion%20gaits%0Aare%20defined%20as%20simple%20cycles%20that%20are%20transformation%20invariant%2C%20i.e.%2C%20the%0Alocomotion%20is%20independent%20of%20the%20starting%20vertex%20of%20these%20periodic%20cycles.%20Gait%0Asynthesis%2C%20the%20problem%20of%20finding%20optimal%20locomotion%20gaits%20for%20a%20given%0Asubstrate%2C%20is%20formulated%20as%20Binary%20Integer%20Linear%20Programming%20%28BILP%29%20problems%0Awith%20a%20linearized%20cost%20function%2C%20linear%20constraints%2C%20and%20iterative%20simple%20cycle%0Adetection.%20Experimentally%2C%20gaits%20are%20synthesized%20for%20varying%20robot-environment%0Ainteractions.%20Variables%20include%20robot%20morphology%20-%20three-limb%20and%20four-limb%0Arobots%2C%20TerreSoRo-III%20and%20TerreSoRo-IV%3B%20substrate%20-%20rubber%20mat%2C%20whiteboard%20and%0Acarpet%3B%20and%20actuator%20functionality%20-%20simulated%20loss%20of%20robot%20limb%20actuation.%20On%0Aan%20average%2C%20gait%20synthesis%20improves%20the%20translation%20and%20rotation%20speeds%20by%2082%25%0Aand%2097%25%20respectively.%20The%20results%20highlight%20that%20data-driven%20methods%20are%20vital%0Ato%20soft%20robot%20locomotion%20control%20due%20to%20the%20significant%20influence%20of%20unexpected%0Aasymmetries%20in%20the%20system%20and%20the%20dependence%20of%20optimal%20gait%20sequences%20on%20the%0Aexperimental%20robot-environment%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03617v2&entry.124074799=Read"},
{"title": "PenTest++: Elevating Ethical Hacking with AI and Automation", "author": "Haitham S. Al-Sinani and Chris J. Mitchell", "abstract": "  Traditional ethical hacking relies on skilled professionals and\ntime-intensive command management, which limits its scalability and efficiency.\nTo address these challenges, we introduce PenTest++, an AI-augmented system\nthat integrates automation with generative AI (GenAI) to optimise ethical\nhacking workflows. Developed in a controlled virtual environment, PenTest++\nstreamlines critical penetration testing tasks, including reconnaissance,\nscanning, enumeration, exploitation, and documentation, while maintaining a\nmodular and adaptable design. The system balances automation with human\noversight, ensuring informed decision-making at key stages, and offers\nsignificant benefits such as enhanced efficiency, scalability, and\nadaptability. However, it also raises ethical considerations, including privacy\nconcerns and the risks of AI-generated inaccuracies (hallucinations). This\nresearch underscores the potential of AI-driven systems like PenTest++ to\ncomplement human expertise in cybersecurity by automating routine tasks,\nenabling professionals to focus on strategic decision-making. By incorporating\nrobust ethical safeguards and promoting ongoing refinement, PenTest++\ndemonstrates how AI can be responsibly harnessed to address operational and\nethical challenges in the evolving cybersecurity landscape.\n", "link": "http://arxiv.org/abs/2502.09484v1", "date": "2025-02-13", "relevancy": 1.7321, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4424}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4375}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PenTest%2B%2B%3A%20Elevating%20Ethical%20Hacking%20with%20AI%20and%20Automation&body=Title%3A%20PenTest%2B%2B%3A%20Elevating%20Ethical%20Hacking%20with%20AI%20and%20Automation%0AAuthor%3A%20Haitham%20S.%20Al-Sinani%20and%20Chris%20J.%20Mitchell%0AAbstract%3A%20%20%20Traditional%20ethical%20hacking%20relies%20on%20skilled%20professionals%20and%0Atime-intensive%20command%20management%2C%20which%20limits%20its%20scalability%20and%20efficiency.%0ATo%20address%20these%20challenges%2C%20we%20introduce%20PenTest%2B%2B%2C%20an%20AI-augmented%20system%0Athat%20integrates%20automation%20with%20generative%20AI%20%28GenAI%29%20to%20optimise%20ethical%0Ahacking%20workflows.%20Developed%20in%20a%20controlled%20virtual%20environment%2C%20PenTest%2B%2B%0Astreamlines%20critical%20penetration%20testing%20tasks%2C%20including%20reconnaissance%2C%0Ascanning%2C%20enumeration%2C%20exploitation%2C%20and%20documentation%2C%20while%20maintaining%20a%0Amodular%20and%20adaptable%20design.%20The%20system%20balances%20automation%20with%20human%0Aoversight%2C%20ensuring%20informed%20decision-making%20at%20key%20stages%2C%20and%20offers%0Asignificant%20benefits%20such%20as%20enhanced%20efficiency%2C%20scalability%2C%20and%0Aadaptability.%20However%2C%20it%20also%20raises%20ethical%20considerations%2C%20including%20privacy%0Aconcerns%20and%20the%20risks%20of%20AI-generated%20inaccuracies%20%28hallucinations%29.%20This%0Aresearch%20underscores%20the%20potential%20of%20AI-driven%20systems%20like%20PenTest%2B%2B%20to%0Acomplement%20human%20expertise%20in%20cybersecurity%20by%20automating%20routine%20tasks%2C%0Aenabling%20professionals%20to%20focus%20on%20strategic%20decision-making.%20By%20incorporating%0Arobust%20ethical%20safeguards%20and%20promoting%20ongoing%20refinement%2C%20PenTest%2B%2B%0Ademonstrates%20how%20AI%20can%20be%20responsibly%20harnessed%20to%20address%20operational%20and%0Aethical%20challenges%20in%20the%20evolving%20cybersecurity%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPenTest%252B%252B%253A%2520Elevating%2520Ethical%2520Hacking%2520with%2520AI%2520and%2520Automation%26entry.906535625%3DHaitham%2520S.%2520Al-Sinani%2520and%2520Chris%2520J.%2520Mitchell%26entry.1292438233%3D%2520%2520Traditional%2520ethical%2520hacking%2520relies%2520on%2520skilled%2520professionals%2520and%250Atime-intensive%2520command%2520management%252C%2520which%2520limits%2520its%2520scalability%2520and%2520efficiency.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520PenTest%252B%252B%252C%2520an%2520AI-augmented%2520system%250Athat%2520integrates%2520automation%2520with%2520generative%2520AI%2520%2528GenAI%2529%2520to%2520optimise%2520ethical%250Ahacking%2520workflows.%2520Developed%2520in%2520a%2520controlled%2520virtual%2520environment%252C%2520PenTest%252B%252B%250Astreamlines%2520critical%2520penetration%2520testing%2520tasks%252C%2520including%2520reconnaissance%252C%250Ascanning%252C%2520enumeration%252C%2520exploitation%252C%2520and%2520documentation%252C%2520while%2520maintaining%2520a%250Amodular%2520and%2520adaptable%2520design.%2520The%2520system%2520balances%2520automation%2520with%2520human%250Aoversight%252C%2520ensuring%2520informed%2520decision-making%2520at%2520key%2520stages%252C%2520and%2520offers%250Asignificant%2520benefits%2520such%2520as%2520enhanced%2520efficiency%252C%2520scalability%252C%2520and%250Aadaptability.%2520However%252C%2520it%2520also%2520raises%2520ethical%2520considerations%252C%2520including%2520privacy%250Aconcerns%2520and%2520the%2520risks%2520of%2520AI-generated%2520inaccuracies%2520%2528hallucinations%2529.%2520This%250Aresearch%2520underscores%2520the%2520potential%2520of%2520AI-driven%2520systems%2520like%2520PenTest%252B%252B%2520to%250Acomplement%2520human%2520expertise%2520in%2520cybersecurity%2520by%2520automating%2520routine%2520tasks%252C%250Aenabling%2520professionals%2520to%2520focus%2520on%2520strategic%2520decision-making.%2520By%2520incorporating%250Arobust%2520ethical%2520safeguards%2520and%2520promoting%2520ongoing%2520refinement%252C%2520PenTest%252B%252B%250Ademonstrates%2520how%2520AI%2520can%2520be%2520responsibly%2520harnessed%2520to%2520address%2520operational%2520and%250Aethical%2520challenges%2520in%2520the%2520evolving%2520cybersecurity%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PenTest%2B%2B%3A%20Elevating%20Ethical%20Hacking%20with%20AI%20and%20Automation&entry.906535625=Haitham%20S.%20Al-Sinani%20and%20Chris%20J.%20Mitchell&entry.1292438233=%20%20Traditional%20ethical%20hacking%20relies%20on%20skilled%20professionals%20and%0Atime-intensive%20command%20management%2C%20which%20limits%20its%20scalability%20and%20efficiency.%0ATo%20address%20these%20challenges%2C%20we%20introduce%20PenTest%2B%2B%2C%20an%20AI-augmented%20system%0Athat%20integrates%20automation%20with%20generative%20AI%20%28GenAI%29%20to%20optimise%20ethical%0Ahacking%20workflows.%20Developed%20in%20a%20controlled%20virtual%20environment%2C%20PenTest%2B%2B%0Astreamlines%20critical%20penetration%20testing%20tasks%2C%20including%20reconnaissance%2C%0Ascanning%2C%20enumeration%2C%20exploitation%2C%20and%20documentation%2C%20while%20maintaining%20a%0Amodular%20and%20adaptable%20design.%20The%20system%20balances%20automation%20with%20human%0Aoversight%2C%20ensuring%20informed%20decision-making%20at%20key%20stages%2C%20and%20offers%0Asignificant%20benefits%20such%20as%20enhanced%20efficiency%2C%20scalability%2C%20and%0Aadaptability.%20However%2C%20it%20also%20raises%20ethical%20considerations%2C%20including%20privacy%0Aconcerns%20and%20the%20risks%20of%20AI-generated%20inaccuracies%20%28hallucinations%29.%20This%0Aresearch%20underscores%20the%20potential%20of%20AI-driven%20systems%20like%20PenTest%2B%2B%20to%0Acomplement%20human%20expertise%20in%20cybersecurity%20by%20automating%20routine%20tasks%2C%0Aenabling%20professionals%20to%20focus%20on%20strategic%20decision-making.%20By%20incorporating%0Arobust%20ethical%20safeguards%20and%20promoting%20ongoing%20refinement%2C%20PenTest%2B%2B%0Ademonstrates%20how%20AI%20can%20be%20responsibly%20harnessed%20to%20address%20operational%20and%0Aethical%20challenges%20in%20the%20evolving%20cybersecurity%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09484v1&entry.124074799=Read"},
{"title": "DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation\n  Networks for Quantitative Nanomaterial Analysis through Differentiable\n  Rendering and Generative Modelling", "author": "Dennis Possart and Leonid Mill and Florian Vollnhals and Tor Hildebrand and Peter Suter and Mathis Hoffmann and Jonas Utz and Daniel Augsburger and Mareike Thies and Mingxuan Wu and Fabian Wagner and George Sarau and Silke Christiansen and Katharina Breininger", "abstract": "  Nanomaterials exhibit distinctive properties governed by parameters such as\nsize, shape, and surface characteristics, which critically influence their\napplications and interactions across technological, biological, and\nenvironmental contexts. Accurate quantification and understanding of these\nmaterials are essential for advancing research and innovation. In this regard,\ndeep learning segmentation networks have emerged as powerful tools that enable\nautomated insights and replace subjective methods with precise quantitative\nanalysis. However, their efficacy depends on representative annotated datasets,\nwhich are challenging to obtain due to the costly imaging of nanoparticles and\nthe labor-intensive nature of manual annotations. To overcome these\nlimitations, we introduce DiffRenderGAN, a novel generative model designed to\nproduce annotated synthetic data. By integrating a differentiable renderer into\na Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes\ntextural rendering parameters to generate realistic, annotated nanoparticle\nimages from non-annotated real microscopy images. This approach reduces the\nneed for manual intervention and enhances segmentation performance compared to\nexisting synthetic data methods by generating diverse and realistic data.\nTested on multiple ion and electron microscopy cases, including titanium\ndioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),\nDiffRenderGAN bridges the gap between synthetic and real data, advancing the\nquantification and understanding of complex nanomaterial systems.\n", "link": "http://arxiv.org/abs/2502.09477v1", "date": "2025-02-13", "relevancy": 1.7217, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5839}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5748}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffRenderGAN%3A%20Addressing%20Training%20Data%20Scarcity%20in%20Deep%20Segmentation%0A%20%20Networks%20for%20Quantitative%20Nanomaterial%20Analysis%20through%20Differentiable%0A%20%20Rendering%20and%20Generative%20Modelling&body=Title%3A%20DiffRenderGAN%3A%20Addressing%20Training%20Data%20Scarcity%20in%20Deep%20Segmentation%0A%20%20Networks%20for%20Quantitative%20Nanomaterial%20Analysis%20through%20Differentiable%0A%20%20Rendering%20and%20Generative%20Modelling%0AAuthor%3A%20Dennis%20Possart%20and%20Leonid%20Mill%20and%20Florian%20Vollnhals%20and%20Tor%20Hildebrand%20and%20Peter%20Suter%20and%20Mathis%20Hoffmann%20and%20Jonas%20Utz%20and%20Daniel%20Augsburger%20and%20Mareike%20Thies%20and%20Mingxuan%20Wu%20and%20Fabian%20Wagner%20and%20George%20Sarau%20and%20Silke%20Christiansen%20and%20Katharina%20Breininger%0AAbstract%3A%20%20%20Nanomaterials%20exhibit%20distinctive%20properties%20governed%20by%20parameters%20such%20as%0Asize%2C%20shape%2C%20and%20surface%20characteristics%2C%20which%20critically%20influence%20their%0Aapplications%20and%20interactions%20across%20technological%2C%20biological%2C%20and%0Aenvironmental%20contexts.%20Accurate%20quantification%20and%20understanding%20of%20these%0Amaterials%20are%20essential%20for%20advancing%20research%20and%20innovation.%20In%20this%20regard%2C%0Adeep%20learning%20segmentation%20networks%20have%20emerged%20as%20powerful%20tools%20that%20enable%0Aautomated%20insights%20and%20replace%20subjective%20methods%20with%20precise%20quantitative%0Aanalysis.%20However%2C%20their%20efficacy%20depends%20on%20representative%20annotated%20datasets%2C%0Awhich%20are%20challenging%20to%20obtain%20due%20to%20the%20costly%20imaging%20of%20nanoparticles%20and%0Athe%20labor-intensive%20nature%20of%20manual%20annotations.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20DiffRenderGAN%2C%20a%20novel%20generative%20model%20designed%20to%0Aproduce%20annotated%20synthetic%20data.%20By%20integrating%20a%20differentiable%20renderer%20into%0Aa%20Generative%20Adversarial%20Network%20%28GAN%29%20framework%2C%20DiffRenderGAN%20optimizes%0Atextural%20rendering%20parameters%20to%20generate%20realistic%2C%20annotated%20nanoparticle%0Aimages%20from%20non-annotated%20real%20microscopy%20images.%20This%20approach%20reduces%20the%0Aneed%20for%20manual%20intervention%20and%20enhances%20segmentation%20performance%20compared%20to%0Aexisting%20synthetic%20data%20methods%20by%20generating%20diverse%20and%20realistic%20data.%0ATested%20on%20multiple%20ion%20and%20electron%20microscopy%20cases%2C%20including%20titanium%0Adioxide%20%28TiO%24_2%24%29%2C%20silicon%20dioxide%20%28SiO%24_2%24%29%29%2C%20and%20silver%20nanowires%20%28AgNW%29%2C%0ADiffRenderGAN%20bridges%20the%20gap%20between%20synthetic%20and%20real%20data%2C%20advancing%20the%0Aquantification%20and%20understanding%20of%20complex%20nanomaterial%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffRenderGAN%253A%2520Addressing%2520Training%2520Data%2520Scarcity%2520in%2520Deep%2520Segmentation%250A%2520%2520Networks%2520for%2520Quantitative%2520Nanomaterial%2520Analysis%2520through%2520Differentiable%250A%2520%2520Rendering%2520and%2520Generative%2520Modelling%26entry.906535625%3DDennis%2520Possart%2520and%2520Leonid%2520Mill%2520and%2520Florian%2520Vollnhals%2520and%2520Tor%2520Hildebrand%2520and%2520Peter%2520Suter%2520and%2520Mathis%2520Hoffmann%2520and%2520Jonas%2520Utz%2520and%2520Daniel%2520Augsburger%2520and%2520Mareike%2520Thies%2520and%2520Mingxuan%2520Wu%2520and%2520Fabian%2520Wagner%2520and%2520George%2520Sarau%2520and%2520Silke%2520Christiansen%2520and%2520Katharina%2520Breininger%26entry.1292438233%3D%2520%2520Nanomaterials%2520exhibit%2520distinctive%2520properties%2520governed%2520by%2520parameters%2520such%2520as%250Asize%252C%2520shape%252C%2520and%2520surface%2520characteristics%252C%2520which%2520critically%2520influence%2520their%250Aapplications%2520and%2520interactions%2520across%2520technological%252C%2520biological%252C%2520and%250Aenvironmental%2520contexts.%2520Accurate%2520quantification%2520and%2520understanding%2520of%2520these%250Amaterials%2520are%2520essential%2520for%2520advancing%2520research%2520and%2520innovation.%2520In%2520this%2520regard%252C%250Adeep%2520learning%2520segmentation%2520networks%2520have%2520emerged%2520as%2520powerful%2520tools%2520that%2520enable%250Aautomated%2520insights%2520and%2520replace%2520subjective%2520methods%2520with%2520precise%2520quantitative%250Aanalysis.%2520However%252C%2520their%2520efficacy%2520depends%2520on%2520representative%2520annotated%2520datasets%252C%250Awhich%2520are%2520challenging%2520to%2520obtain%2520due%2520to%2520the%2520costly%2520imaging%2520of%2520nanoparticles%2520and%250Athe%2520labor-intensive%2520nature%2520of%2520manual%2520annotations.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520introduce%2520DiffRenderGAN%252C%2520a%2520novel%2520generative%2520model%2520designed%2520to%250Aproduce%2520annotated%2520synthetic%2520data.%2520By%2520integrating%2520a%2520differentiable%2520renderer%2520into%250Aa%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520framework%252C%2520DiffRenderGAN%2520optimizes%250Atextural%2520rendering%2520parameters%2520to%2520generate%2520realistic%252C%2520annotated%2520nanoparticle%250Aimages%2520from%2520non-annotated%2520real%2520microscopy%2520images.%2520This%2520approach%2520reduces%2520the%250Aneed%2520for%2520manual%2520intervention%2520and%2520enhances%2520segmentation%2520performance%2520compared%2520to%250Aexisting%2520synthetic%2520data%2520methods%2520by%2520generating%2520diverse%2520and%2520realistic%2520data.%250ATested%2520on%2520multiple%2520ion%2520and%2520electron%2520microscopy%2520cases%252C%2520including%2520titanium%250Adioxide%2520%2528TiO%2524_2%2524%2529%252C%2520silicon%2520dioxide%2520%2528SiO%2524_2%2524%2529%2529%252C%2520and%2520silver%2520nanowires%2520%2528AgNW%2529%252C%250ADiffRenderGAN%2520bridges%2520the%2520gap%2520between%2520synthetic%2520and%2520real%2520data%252C%2520advancing%2520the%250Aquantification%2520and%2520understanding%2520of%2520complex%2520nanomaterial%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffRenderGAN%3A%20Addressing%20Training%20Data%20Scarcity%20in%20Deep%20Segmentation%0A%20%20Networks%20for%20Quantitative%20Nanomaterial%20Analysis%20through%20Differentiable%0A%20%20Rendering%20and%20Generative%20Modelling&entry.906535625=Dennis%20Possart%20and%20Leonid%20Mill%20and%20Florian%20Vollnhals%20and%20Tor%20Hildebrand%20and%20Peter%20Suter%20and%20Mathis%20Hoffmann%20and%20Jonas%20Utz%20and%20Daniel%20Augsburger%20and%20Mareike%20Thies%20and%20Mingxuan%20Wu%20and%20Fabian%20Wagner%20and%20George%20Sarau%20and%20Silke%20Christiansen%20and%20Katharina%20Breininger&entry.1292438233=%20%20Nanomaterials%20exhibit%20distinctive%20properties%20governed%20by%20parameters%20such%20as%0Asize%2C%20shape%2C%20and%20surface%20characteristics%2C%20which%20critically%20influence%20their%0Aapplications%20and%20interactions%20across%20technological%2C%20biological%2C%20and%0Aenvironmental%20contexts.%20Accurate%20quantification%20and%20understanding%20of%20these%0Amaterials%20are%20essential%20for%20advancing%20research%20and%20innovation.%20In%20this%20regard%2C%0Adeep%20learning%20segmentation%20networks%20have%20emerged%20as%20powerful%20tools%20that%20enable%0Aautomated%20insights%20and%20replace%20subjective%20methods%20with%20precise%20quantitative%0Aanalysis.%20However%2C%20their%20efficacy%20depends%20on%20representative%20annotated%20datasets%2C%0Awhich%20are%20challenging%20to%20obtain%20due%20to%20the%20costly%20imaging%20of%20nanoparticles%20and%0Athe%20labor-intensive%20nature%20of%20manual%20annotations.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20DiffRenderGAN%2C%20a%20novel%20generative%20model%20designed%20to%0Aproduce%20annotated%20synthetic%20data.%20By%20integrating%20a%20differentiable%20renderer%20into%0Aa%20Generative%20Adversarial%20Network%20%28GAN%29%20framework%2C%20DiffRenderGAN%20optimizes%0Atextural%20rendering%20parameters%20to%20generate%20realistic%2C%20annotated%20nanoparticle%0Aimages%20from%20non-annotated%20real%20microscopy%20images.%20This%20approach%20reduces%20the%0Aneed%20for%20manual%20intervention%20and%20enhances%20segmentation%20performance%20compared%20to%0Aexisting%20synthetic%20data%20methods%20by%20generating%20diverse%20and%20realistic%20data.%0ATested%20on%20multiple%20ion%20and%20electron%20microscopy%20cases%2C%20including%20titanium%0Adioxide%20%28TiO%24_2%24%29%2C%20silicon%20dioxide%20%28SiO%24_2%24%29%29%2C%20and%20silver%20nanowires%20%28AgNW%29%2C%0ADiffRenderGAN%20bridges%20the%20gap%20between%20synthetic%20and%20real%20data%2C%20advancing%20the%0Aquantification%20and%20understanding%20of%20complex%20nanomaterial%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09477v1&entry.124074799=Read"},
{"title": "S$^2$-Diffusion: Generalizing from Instance-level to Category-level\n  Skills in Robot Manipulation", "author": "Quantao Yang and Michael C. Welle and Danica Kragic and Olov Andersson", "abstract": "  Recent advances in skill learning has propelled robot manipulation to new\nheights by enabling it to learn complex manipulation tasks from a practical\nnumber of demonstrations. However, these skills are often limited to the\nparticular action, object, and environment \\textit{instances} that are shown in\nthe training data, and have trouble transferring to other instances of the same\ncategory. In this work we present an open-vocabulary Spatial-Semantic Diffusion\npolicy (S$^2$-Diffusion) which enables generalization from instance-level\ntraining data to category-level, enabling skills to be transferable between\ninstances of the same category. We show that functional aspects of skills can\nbe captured via a promptable semantic module combined with a spatial\nrepresentation. We further propose leveraging depth estimation networks to\nallow the use of only a single RGB camera. Our approach is evaluated and\ncompared on a diverse number of robot manipulation tasks, both in simulation\nand in the real world. Our results show that S$^2$-Diffusion is invariant to\nchanges in category-irrelevant factors as well as enables satisfying\nperformance on other instances within the same category, even if it was not\ntrained on that specific instance. Full videos of all real-world experiments\nare available in the supplementary material.\n", "link": "http://arxiv.org/abs/2502.09389v1", "date": "2025-02-13", "relevancy": 1.7156, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.592}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5828}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S%24%5E2%24-Diffusion%3A%20Generalizing%20from%20Instance-level%20to%20Category-level%0A%20%20Skills%20in%20Robot%20Manipulation&body=Title%3A%20S%24%5E2%24-Diffusion%3A%20Generalizing%20from%20Instance-level%20to%20Category-level%0A%20%20Skills%20in%20Robot%20Manipulation%0AAuthor%3A%20Quantao%20Yang%20and%20Michael%20C.%20Welle%20and%20Danica%20Kragic%20and%20Olov%20Andersson%0AAbstract%3A%20%20%20Recent%20advances%20in%20skill%20learning%20has%20propelled%20robot%20manipulation%20to%20new%0Aheights%20by%20enabling%20it%20to%20learn%20complex%20manipulation%20tasks%20from%20a%20practical%0Anumber%20of%20demonstrations.%20However%2C%20these%20skills%20are%20often%20limited%20to%20the%0Aparticular%20action%2C%20object%2C%20and%20environment%20%5Ctextit%7Binstances%7D%20that%20are%20shown%20in%0Athe%20training%20data%2C%20and%20have%20trouble%20transferring%20to%20other%20instances%20of%20the%20same%0Acategory.%20In%20this%20work%20we%20present%20an%20open-vocabulary%20Spatial-Semantic%20Diffusion%0Apolicy%20%28S%24%5E2%24-Diffusion%29%20which%20enables%20generalization%20from%20instance-level%0Atraining%20data%20to%20category-level%2C%20enabling%20skills%20to%20be%20transferable%20between%0Ainstances%20of%20the%20same%20category.%20We%20show%20that%20functional%20aspects%20of%20skills%20can%0Abe%20captured%20via%20a%20promptable%20semantic%20module%20combined%20with%20a%20spatial%0Arepresentation.%20We%20further%20propose%20leveraging%20depth%20estimation%20networks%20to%0Aallow%20the%20use%20of%20only%20a%20single%20RGB%20camera.%20Our%20approach%20is%20evaluated%20and%0Acompared%20on%20a%20diverse%20number%20of%20robot%20manipulation%20tasks%2C%20both%20in%20simulation%0Aand%20in%20the%20real%20world.%20Our%20results%20show%20that%20S%24%5E2%24-Diffusion%20is%20invariant%20to%0Achanges%20in%20category-irrelevant%20factors%20as%20well%20as%20enables%20satisfying%0Aperformance%20on%20other%20instances%20within%20the%20same%20category%2C%20even%20if%20it%20was%20not%0Atrained%20on%20that%20specific%20instance.%20Full%20videos%20of%20all%20real-world%20experiments%0Aare%20available%20in%20the%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS%2524%255E2%2524-Diffusion%253A%2520Generalizing%2520from%2520Instance-level%2520to%2520Category-level%250A%2520%2520Skills%2520in%2520Robot%2520Manipulation%26entry.906535625%3DQuantao%2520Yang%2520and%2520Michael%2520C.%2520Welle%2520and%2520Danica%2520Kragic%2520and%2520Olov%2520Andersson%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520skill%2520learning%2520has%2520propelled%2520robot%2520manipulation%2520to%2520new%250Aheights%2520by%2520enabling%2520it%2520to%2520learn%2520complex%2520manipulation%2520tasks%2520from%2520a%2520practical%250Anumber%2520of%2520demonstrations.%2520However%252C%2520these%2520skills%2520are%2520often%2520limited%2520to%2520the%250Aparticular%2520action%252C%2520object%252C%2520and%2520environment%2520%255Ctextit%257Binstances%257D%2520that%2520are%2520shown%2520in%250Athe%2520training%2520data%252C%2520and%2520have%2520trouble%2520transferring%2520to%2520other%2520instances%2520of%2520the%2520same%250Acategory.%2520In%2520this%2520work%2520we%2520present%2520an%2520open-vocabulary%2520Spatial-Semantic%2520Diffusion%250Apolicy%2520%2528S%2524%255E2%2524-Diffusion%2529%2520which%2520enables%2520generalization%2520from%2520instance-level%250Atraining%2520data%2520to%2520category-level%252C%2520enabling%2520skills%2520to%2520be%2520transferable%2520between%250Ainstances%2520of%2520the%2520same%2520category.%2520We%2520show%2520that%2520functional%2520aspects%2520of%2520skills%2520can%250Abe%2520captured%2520via%2520a%2520promptable%2520semantic%2520module%2520combined%2520with%2520a%2520spatial%250Arepresentation.%2520We%2520further%2520propose%2520leveraging%2520depth%2520estimation%2520networks%2520to%250Aallow%2520the%2520use%2520of%2520only%2520a%2520single%2520RGB%2520camera.%2520Our%2520approach%2520is%2520evaluated%2520and%250Acompared%2520on%2520a%2520diverse%2520number%2520of%2520robot%2520manipulation%2520tasks%252C%2520both%2520in%2520simulation%250Aand%2520in%2520the%2520real%2520world.%2520Our%2520results%2520show%2520that%2520S%2524%255E2%2524-Diffusion%2520is%2520invariant%2520to%250Achanges%2520in%2520category-irrelevant%2520factors%2520as%2520well%2520as%2520enables%2520satisfying%250Aperformance%2520on%2520other%2520instances%2520within%2520the%2520same%2520category%252C%2520even%2520if%2520it%2520was%2520not%250Atrained%2520on%2520that%2520specific%2520instance.%2520Full%2520videos%2520of%2520all%2520real-world%2520experiments%250Aare%2520available%2520in%2520the%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S%24%5E2%24-Diffusion%3A%20Generalizing%20from%20Instance-level%20to%20Category-level%0A%20%20Skills%20in%20Robot%20Manipulation&entry.906535625=Quantao%20Yang%20and%20Michael%20C.%20Welle%20and%20Danica%20Kragic%20and%20Olov%20Andersson&entry.1292438233=%20%20Recent%20advances%20in%20skill%20learning%20has%20propelled%20robot%20manipulation%20to%20new%0Aheights%20by%20enabling%20it%20to%20learn%20complex%20manipulation%20tasks%20from%20a%20practical%0Anumber%20of%20demonstrations.%20However%2C%20these%20skills%20are%20often%20limited%20to%20the%0Aparticular%20action%2C%20object%2C%20and%20environment%20%5Ctextit%7Binstances%7D%20that%20are%20shown%20in%0Athe%20training%20data%2C%20and%20have%20trouble%20transferring%20to%20other%20instances%20of%20the%20same%0Acategory.%20In%20this%20work%20we%20present%20an%20open-vocabulary%20Spatial-Semantic%20Diffusion%0Apolicy%20%28S%24%5E2%24-Diffusion%29%20which%20enables%20generalization%20from%20instance-level%0Atraining%20data%20to%20category-level%2C%20enabling%20skills%20to%20be%20transferable%20between%0Ainstances%20of%20the%20same%20category.%20We%20show%20that%20functional%20aspects%20of%20skills%20can%0Abe%20captured%20via%20a%20promptable%20semantic%20module%20combined%20with%20a%20spatial%0Arepresentation.%20We%20further%20propose%20leveraging%20depth%20estimation%20networks%20to%0Aallow%20the%20use%20of%20only%20a%20single%20RGB%20camera.%20Our%20approach%20is%20evaluated%20and%0Acompared%20on%20a%20diverse%20number%20of%20robot%20manipulation%20tasks%2C%20both%20in%20simulation%0Aand%20in%20the%20real%20world.%20Our%20results%20show%20that%20S%24%5E2%24-Diffusion%20is%20invariant%20to%0Achanges%20in%20category-irrelevant%20factors%20as%20well%20as%20enables%20satisfying%0Aperformance%20on%20other%20instances%20within%20the%20same%20category%2C%20even%20if%20it%20was%20not%0Atrained%20on%20that%20specific%20instance.%20Full%20videos%20of%20all%20real-world%20experiments%0Aare%20available%20in%20the%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09389v1&entry.124074799=Read"},
{"title": "Opening Articulated Objects in the Real World", "author": "Arjun Gupta and Michelle Zhang and Rishik Sathua and Saurabh Gupta", "abstract": "  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n", "link": "http://arxiv.org/abs/2402.17767v2", "date": "2025-02-13", "relevancy": 1.6901, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6371}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.57}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opening%20Articulated%20Objects%20in%20the%20Real%20World&body=Title%3A%20Opening%20Articulated%20Objects%20in%20the%20Real%20World%0AAuthor%3A%20Arjun%20Gupta%20and%20Michelle%20Zhang%20and%20Rishik%20Sathua%20and%20Saurabh%20Gupta%0AAbstract%3A%20%20%20What%20does%20it%20take%20to%20build%20mobile%20manipulation%20systems%20that%20can%20competently%0Aoperate%20on%20previously%20unseen%20objects%20in%20previously%20unseen%20environments%3F%20This%0Awork%20answers%20this%20question%20using%20opening%20of%20articulated%20objects%20as%20a%20mobile%0Amanipulation%20testbed.%20Specifically%2C%20our%20focus%20is%20on%20the%20end-to-end%20performance%0Aon%20this%20task%20without%20any%20privileged%20information%2C%20i.e.%20the%20robot%20starts%20at%20a%0Alocation%20with%20the%20novel%20target%20articulated%20object%20in%20view%2C%20and%20has%20to%20approach%0Athe%20object%20and%20successfully%20open%20it.%20We%20first%20develop%20a%20system%20for%20this%20task%2C%0Aand%20then%20conduct%20100%2B%20end-to-end%20system%20tests%20across%2013%20real%20world%20test%20sites.%0AOur%20large-scale%20study%20reveals%20a%20number%20of%20surprising%20findings%3A%20a%29%20modular%0Asystems%20outperform%20end-to-end%20learned%20systems%20for%20this%20task%2C%20even%20when%20the%0Aend-to-end%20learned%20systems%20are%20trained%20on%201000%2B%20demonstrations%2C%20b%29%20perception%2C%0Aand%20not%20precise%20end-effector%20control%2C%20is%20the%20primary%20bottleneck%20to%20task%0Asuccess%2C%20and%20c%29%20state-of-the-art%20articulation%20parameter%20estimation%20models%0Adeveloped%20in%20isolation%20struggle%20when%20faced%20with%20robot-centric%20viewpoints.%0AOverall%2C%20our%20findings%20highlight%20the%20limitations%20of%20developing%20components%20of%20the%0Apipeline%20in%20isolation%20and%20underscore%20the%20need%20for%20system-level%20research%2C%0Aproviding%20a%20pragmatic%20roadmap%20for%20building%20generalizable%20mobile%20manipulation%0Asystems.%20Videos%2C%20code%2C%20and%20models%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//arjung128.github.io/opening-articulated-objects/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpening%2520Articulated%2520Objects%2520in%2520the%2520Real%2520World%26entry.906535625%3DArjun%2520Gupta%2520and%2520Michelle%2520Zhang%2520and%2520Rishik%2520Sathua%2520and%2520Saurabh%2520Gupta%26entry.1292438233%3D%2520%2520What%2520does%2520it%2520take%2520to%2520build%2520mobile%2520manipulation%2520systems%2520that%2520can%2520competently%250Aoperate%2520on%2520previously%2520unseen%2520objects%2520in%2520previously%2520unseen%2520environments%253F%2520This%250Awork%2520answers%2520this%2520question%2520using%2520opening%2520of%2520articulated%2520objects%2520as%2520a%2520mobile%250Amanipulation%2520testbed.%2520Specifically%252C%2520our%2520focus%2520is%2520on%2520the%2520end-to-end%2520performance%250Aon%2520this%2520task%2520without%2520any%2520privileged%2520information%252C%2520i.e.%2520the%2520robot%2520starts%2520at%2520a%250Alocation%2520with%2520the%2520novel%2520target%2520articulated%2520object%2520in%2520view%252C%2520and%2520has%2520to%2520approach%250Athe%2520object%2520and%2520successfully%2520open%2520it.%2520We%2520first%2520develop%2520a%2520system%2520for%2520this%2520task%252C%250Aand%2520then%2520conduct%2520100%252B%2520end-to-end%2520system%2520tests%2520across%252013%2520real%2520world%2520test%2520sites.%250AOur%2520large-scale%2520study%2520reveals%2520a%2520number%2520of%2520surprising%2520findings%253A%2520a%2529%2520modular%250Asystems%2520outperform%2520end-to-end%2520learned%2520systems%2520for%2520this%2520task%252C%2520even%2520when%2520the%250Aend-to-end%2520learned%2520systems%2520are%2520trained%2520on%25201000%252B%2520demonstrations%252C%2520b%2529%2520perception%252C%250Aand%2520not%2520precise%2520end-effector%2520control%252C%2520is%2520the%2520primary%2520bottleneck%2520to%2520task%250Asuccess%252C%2520and%2520c%2529%2520state-of-the-art%2520articulation%2520parameter%2520estimation%2520models%250Adeveloped%2520in%2520isolation%2520struggle%2520when%2520faced%2520with%2520robot-centric%2520viewpoints.%250AOverall%252C%2520our%2520findings%2520highlight%2520the%2520limitations%2520of%2520developing%2520components%2520of%2520the%250Apipeline%2520in%2520isolation%2520and%2520underscore%2520the%2520need%2520for%2520system-level%2520research%252C%250Aproviding%2520a%2520pragmatic%2520roadmap%2520for%2520building%2520generalizable%2520mobile%2520manipulation%250Asystems.%2520Videos%252C%2520code%252C%2520and%2520models%2520are%2520available%2520on%2520the%2520project%2520website%253A%250Ahttps%253A//arjung128.github.io/opening-articulated-objects/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opening%20Articulated%20Objects%20in%20the%20Real%20World&entry.906535625=Arjun%20Gupta%20and%20Michelle%20Zhang%20and%20Rishik%20Sathua%20and%20Saurabh%20Gupta&entry.1292438233=%20%20What%20does%20it%20take%20to%20build%20mobile%20manipulation%20systems%20that%20can%20competently%0Aoperate%20on%20previously%20unseen%20objects%20in%20previously%20unseen%20environments%3F%20This%0Awork%20answers%20this%20question%20using%20opening%20of%20articulated%20objects%20as%20a%20mobile%0Amanipulation%20testbed.%20Specifically%2C%20our%20focus%20is%20on%20the%20end-to-end%20performance%0Aon%20this%20task%20without%20any%20privileged%20information%2C%20i.e.%20the%20robot%20starts%20at%20a%0Alocation%20with%20the%20novel%20target%20articulated%20object%20in%20view%2C%20and%20has%20to%20approach%0Athe%20object%20and%20successfully%20open%20it.%20We%20first%20develop%20a%20system%20for%20this%20task%2C%0Aand%20then%20conduct%20100%2B%20end-to-end%20system%20tests%20across%2013%20real%20world%20test%20sites.%0AOur%20large-scale%20study%20reveals%20a%20number%20of%20surprising%20findings%3A%20a%29%20modular%0Asystems%20outperform%20end-to-end%20learned%20systems%20for%20this%20task%2C%20even%20when%20the%0Aend-to-end%20learned%20systems%20are%20trained%20on%201000%2B%20demonstrations%2C%20b%29%20perception%2C%0Aand%20not%20precise%20end-effector%20control%2C%20is%20the%20primary%20bottleneck%20to%20task%0Asuccess%2C%20and%20c%29%20state-of-the-art%20articulation%20parameter%20estimation%20models%0Adeveloped%20in%20isolation%20struggle%20when%20faced%20with%20robot-centric%20viewpoints.%0AOverall%2C%20our%20findings%20highlight%20the%20limitations%20of%20developing%20components%20of%20the%0Apipeline%20in%20isolation%20and%20underscore%20the%20need%20for%20system-level%20research%2C%0Aproviding%20a%20pragmatic%20roadmap%20for%20building%20generalizable%20mobile%20manipulation%0Asystems.%20Videos%2C%20code%2C%20and%20models%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//arjung128.github.io/opening-articulated-objects/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17767v2&entry.124074799=Read"},
{"title": "Toward Universal Laws of Outlier Propagation", "author": "Aram Ebtekar and Yuhao Wang and Dominik Janzing", "abstract": "  We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.\n", "link": "http://arxiv.org/abs/2502.08593v2", "date": "2025-02-13", "relevancy": 1.6772, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4614}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4286}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Universal%20Laws%20of%20Outlier%20Propagation&body=Title%3A%20Toward%20Universal%20Laws%20of%20Outlier%20Propagation%0AAuthor%3A%20Aram%20Ebtekar%20and%20Yuhao%20Wang%20and%20Dominik%20Janzing%0AAbstract%3A%20%20%20We%20argue%20that%20Algorithmic%20Information%20Theory%20%28AIT%29%20admits%20a%20principled%20way%20to%0Aquantify%20outliers%20in%20terms%20of%20so-called%20randomness%20deficiency.%20For%20the%0Aprobability%20distribution%20generated%20by%20a%20causal%20Bayesian%20network%2C%20we%20show%20that%0Athe%20randomness%20deficiency%20of%20the%20joint%20state%20decomposes%20into%20randomness%0Adeficiencies%20of%20each%20causal%20mechanism%2C%20subject%20to%20the%20Independence%20of%0AMechanisms%20Principle.%20Accordingly%2C%20anomalous%20joint%20observations%20can%20be%0Aquantitatively%20attributed%20to%20their%20root%20causes%2C%20i.e.%2C%20the%20mechanisms%20that%0Abehaved%20anomalously.%20As%20an%20extension%20of%20Levin%27s%20law%20of%20randomness%20conservation%2C%0Awe%20show%20that%20weak%20outliers%20cannot%20cause%20strong%20ones%20when%20Independence%20of%0AMechanisms%20holds.%20We%20show%20how%20these%20information%20theoretic%20laws%20provide%20a%20better%0Aunderstanding%20of%20the%20behaviour%20of%20outliers%20defined%20with%20respect%20to%20existing%0Ascores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Universal%2520Laws%2520of%2520Outlier%2520Propagation%26entry.906535625%3DAram%2520Ebtekar%2520and%2520Yuhao%2520Wang%2520and%2520Dominik%2520Janzing%26entry.1292438233%3D%2520%2520We%2520argue%2520that%2520Algorithmic%2520Information%2520Theory%2520%2528AIT%2529%2520admits%2520a%2520principled%2520way%2520to%250Aquantify%2520outliers%2520in%2520terms%2520of%2520so-called%2520randomness%2520deficiency.%2520For%2520the%250Aprobability%2520distribution%2520generated%2520by%2520a%2520causal%2520Bayesian%2520network%252C%2520we%2520show%2520that%250Athe%2520randomness%2520deficiency%2520of%2520the%2520joint%2520state%2520decomposes%2520into%2520randomness%250Adeficiencies%2520of%2520each%2520causal%2520mechanism%252C%2520subject%2520to%2520the%2520Independence%2520of%250AMechanisms%2520Principle.%2520Accordingly%252C%2520anomalous%2520joint%2520observations%2520can%2520be%250Aquantitatively%2520attributed%2520to%2520their%2520root%2520causes%252C%2520i.e.%252C%2520the%2520mechanisms%2520that%250Abehaved%2520anomalously.%2520As%2520an%2520extension%2520of%2520Levin%2527s%2520law%2520of%2520randomness%2520conservation%252C%250Awe%2520show%2520that%2520weak%2520outliers%2520cannot%2520cause%2520strong%2520ones%2520when%2520Independence%2520of%250AMechanisms%2520holds.%2520We%2520show%2520how%2520these%2520information%2520theoretic%2520laws%2520provide%2520a%2520better%250Aunderstanding%2520of%2520the%2520behaviour%2520of%2520outliers%2520defined%2520with%2520respect%2520to%2520existing%250Ascores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Universal%20Laws%20of%20Outlier%20Propagation&entry.906535625=Aram%20Ebtekar%20and%20Yuhao%20Wang%20and%20Dominik%20Janzing&entry.1292438233=%20%20We%20argue%20that%20Algorithmic%20Information%20Theory%20%28AIT%29%20admits%20a%20principled%20way%20to%0Aquantify%20outliers%20in%20terms%20of%20so-called%20randomness%20deficiency.%20For%20the%0Aprobability%20distribution%20generated%20by%20a%20causal%20Bayesian%20network%2C%20we%20show%20that%0Athe%20randomness%20deficiency%20of%20the%20joint%20state%20decomposes%20into%20randomness%0Adeficiencies%20of%20each%20causal%20mechanism%2C%20subject%20to%20the%20Independence%20of%0AMechanisms%20Principle.%20Accordingly%2C%20anomalous%20joint%20observations%20can%20be%0Aquantitatively%20attributed%20to%20their%20root%20causes%2C%20i.e.%2C%20the%20mechanisms%20that%0Abehaved%20anomalously.%20As%20an%20extension%20of%20Levin%27s%20law%20of%20randomness%20conservation%2C%0Awe%20show%20that%20weak%20outliers%20cannot%20cause%20strong%20ones%20when%20Independence%20of%0AMechanisms%20holds.%20We%20show%20how%20these%20information%20theoretic%20laws%20provide%20a%20better%0Aunderstanding%20of%20the%20behaviour%20of%20outliers%20defined%20with%20respect%20to%20existing%0Ascores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08593v2&entry.124074799=Read"},
{"title": "SyntheticPop: Attacking Speaker Verification Systems With Synthetic\n  VoicePops", "author": "Eshaq Jamdar and Amith Kamath Belman", "abstract": "  Voice Authentication (VA), also known as Automatic Speaker Verification\n(ASV), is a widely adopted authentication method, particularly in automated\nsystems like banking services, where it serves as a secondary layer of user\nauthentication. Despite its popularity, VA systems are vulnerable to various\nattacks, including replay, impersonation, and the emerging threat of deepfake\naudio that mimics the voice of legitimate users. To mitigate these risks,\nseveral defense mechanisms have been proposed. One such solution, Voice Pops,\naims to distinguish an individual's unique phoneme pronunciations during the\nenrollment process. While promising, the effectiveness of VA+VoicePop against a\nbroader range of attacks, particularly logical or adversarial attacks, remains\ninsufficiently explored. We propose a novel attack method, which we refer to as\nSyntheticPop, designed to target the phoneme recognition capabilities of the\nVA+VoicePop system. The SyntheticPop attack involves embedding synthetic \"pop\"\nnoises into spoofed audio samples, significantly degrading the model's\nperformance. We achieve an attack success rate of over 95% while poisoning 20%\nof the training dataset. Our experiments demonstrate that VA+VoicePop achieves\n69% accuracy under normal conditions, 37% accuracy when subjected to a baseline\nlabel flipping attack, and just 14% accuracy under our proposed SyntheticPop\nattack, emphasizing the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2502.09553v1", "date": "2025-02-13", "relevancy": 1.6494, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4263}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4095}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyntheticPop%3A%20Attacking%20Speaker%20Verification%20Systems%20With%20Synthetic%0A%20%20VoicePops&body=Title%3A%20SyntheticPop%3A%20Attacking%20Speaker%20Verification%20Systems%20With%20Synthetic%0A%20%20VoicePops%0AAuthor%3A%20Eshaq%20Jamdar%20and%20Amith%20Kamath%20Belman%0AAbstract%3A%20%20%20Voice%20Authentication%20%28VA%29%2C%20also%20known%20as%20Automatic%20Speaker%20Verification%0A%28ASV%29%2C%20is%20a%20widely%20adopted%20authentication%20method%2C%20particularly%20in%20automated%0Asystems%20like%20banking%20services%2C%20where%20it%20serves%20as%20a%20secondary%20layer%20of%20user%0Aauthentication.%20Despite%20its%20popularity%2C%20VA%20systems%20are%20vulnerable%20to%20various%0Aattacks%2C%20including%20replay%2C%20impersonation%2C%20and%20the%20emerging%20threat%20of%20deepfake%0Aaudio%20that%20mimics%20the%20voice%20of%20legitimate%20users.%20To%20mitigate%20these%20risks%2C%0Aseveral%20defense%20mechanisms%20have%20been%20proposed.%20One%20such%20solution%2C%20Voice%20Pops%2C%0Aaims%20to%20distinguish%20an%20individual%27s%20unique%20phoneme%20pronunciations%20during%20the%0Aenrollment%20process.%20While%20promising%2C%20the%20effectiveness%20of%20VA%2BVoicePop%20against%20a%0Abroader%20range%20of%20attacks%2C%20particularly%20logical%20or%20adversarial%20attacks%2C%20remains%0Ainsufficiently%20explored.%20We%20propose%20a%20novel%20attack%20method%2C%20which%20we%20refer%20to%20as%0ASyntheticPop%2C%20designed%20to%20target%20the%20phoneme%20recognition%20capabilities%20of%20the%0AVA%2BVoicePop%20system.%20The%20SyntheticPop%20attack%20involves%20embedding%20synthetic%20%22pop%22%0Anoises%20into%20spoofed%20audio%20samples%2C%20significantly%20degrading%20the%20model%27s%0Aperformance.%20We%20achieve%20an%20attack%20success%20rate%20of%20over%2095%25%20while%20poisoning%2020%25%0Aof%20the%20training%20dataset.%20Our%20experiments%20demonstrate%20that%20VA%2BVoicePop%20achieves%0A69%25%20accuracy%20under%20normal%20conditions%2C%2037%25%20accuracy%20when%20subjected%20to%20a%20baseline%0Alabel%20flipping%20attack%2C%20and%20just%2014%25%20accuracy%20under%20our%20proposed%20SyntheticPop%0Aattack%2C%20emphasizing%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyntheticPop%253A%2520Attacking%2520Speaker%2520Verification%2520Systems%2520With%2520Synthetic%250A%2520%2520VoicePops%26entry.906535625%3DEshaq%2520Jamdar%2520and%2520Amith%2520Kamath%2520Belman%26entry.1292438233%3D%2520%2520Voice%2520Authentication%2520%2528VA%2529%252C%2520also%2520known%2520as%2520Automatic%2520Speaker%2520Verification%250A%2528ASV%2529%252C%2520is%2520a%2520widely%2520adopted%2520authentication%2520method%252C%2520particularly%2520in%2520automated%250Asystems%2520like%2520banking%2520services%252C%2520where%2520it%2520serves%2520as%2520a%2520secondary%2520layer%2520of%2520user%250Aauthentication.%2520Despite%2520its%2520popularity%252C%2520VA%2520systems%2520are%2520vulnerable%2520to%2520various%250Aattacks%252C%2520including%2520replay%252C%2520impersonation%252C%2520and%2520the%2520emerging%2520threat%2520of%2520deepfake%250Aaudio%2520that%2520mimics%2520the%2520voice%2520of%2520legitimate%2520users.%2520To%2520mitigate%2520these%2520risks%252C%250Aseveral%2520defense%2520mechanisms%2520have%2520been%2520proposed.%2520One%2520such%2520solution%252C%2520Voice%2520Pops%252C%250Aaims%2520to%2520distinguish%2520an%2520individual%2527s%2520unique%2520phoneme%2520pronunciations%2520during%2520the%250Aenrollment%2520process.%2520While%2520promising%252C%2520the%2520effectiveness%2520of%2520VA%252BVoicePop%2520against%2520a%250Abroader%2520range%2520of%2520attacks%252C%2520particularly%2520logical%2520or%2520adversarial%2520attacks%252C%2520remains%250Ainsufficiently%2520explored.%2520We%2520propose%2520a%2520novel%2520attack%2520method%252C%2520which%2520we%2520refer%2520to%2520as%250ASyntheticPop%252C%2520designed%2520to%2520target%2520the%2520phoneme%2520recognition%2520capabilities%2520of%2520the%250AVA%252BVoicePop%2520system.%2520The%2520SyntheticPop%2520attack%2520involves%2520embedding%2520synthetic%2520%2522pop%2522%250Anoises%2520into%2520spoofed%2520audio%2520samples%252C%2520significantly%2520degrading%2520the%2520model%2527s%250Aperformance.%2520We%2520achieve%2520an%2520attack%2520success%2520rate%2520of%2520over%252095%2525%2520while%2520poisoning%252020%2525%250Aof%2520the%2520training%2520dataset.%2520Our%2520experiments%2520demonstrate%2520that%2520VA%252BVoicePop%2520achieves%250A69%2525%2520accuracy%2520under%2520normal%2520conditions%252C%252037%2525%2520accuracy%2520when%2520subjected%2520to%2520a%2520baseline%250Alabel%2520flipping%2520attack%252C%2520and%2520just%252014%2525%2520accuracy%2520under%2520our%2520proposed%2520SyntheticPop%250Aattack%252C%2520emphasizing%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyntheticPop%3A%20Attacking%20Speaker%20Verification%20Systems%20With%20Synthetic%0A%20%20VoicePops&entry.906535625=Eshaq%20Jamdar%20and%20Amith%20Kamath%20Belman&entry.1292438233=%20%20Voice%20Authentication%20%28VA%29%2C%20also%20known%20as%20Automatic%20Speaker%20Verification%0A%28ASV%29%2C%20is%20a%20widely%20adopted%20authentication%20method%2C%20particularly%20in%20automated%0Asystems%20like%20banking%20services%2C%20where%20it%20serves%20as%20a%20secondary%20layer%20of%20user%0Aauthentication.%20Despite%20its%20popularity%2C%20VA%20systems%20are%20vulnerable%20to%20various%0Aattacks%2C%20including%20replay%2C%20impersonation%2C%20and%20the%20emerging%20threat%20of%20deepfake%0Aaudio%20that%20mimics%20the%20voice%20of%20legitimate%20users.%20To%20mitigate%20these%20risks%2C%0Aseveral%20defense%20mechanisms%20have%20been%20proposed.%20One%20such%20solution%2C%20Voice%20Pops%2C%0Aaims%20to%20distinguish%20an%20individual%27s%20unique%20phoneme%20pronunciations%20during%20the%0Aenrollment%20process.%20While%20promising%2C%20the%20effectiveness%20of%20VA%2BVoicePop%20against%20a%0Abroader%20range%20of%20attacks%2C%20particularly%20logical%20or%20adversarial%20attacks%2C%20remains%0Ainsufficiently%20explored.%20We%20propose%20a%20novel%20attack%20method%2C%20which%20we%20refer%20to%20as%0ASyntheticPop%2C%20designed%20to%20target%20the%20phoneme%20recognition%20capabilities%20of%20the%0AVA%2BVoicePop%20system.%20The%20SyntheticPop%20attack%20involves%20embedding%20synthetic%20%22pop%22%0Anoises%20into%20spoofed%20audio%20samples%2C%20significantly%20degrading%20the%20model%27s%0Aperformance.%20We%20achieve%20an%20attack%20success%20rate%20of%20over%2095%25%20while%20poisoning%2020%25%0Aof%20the%20training%20dataset.%20Our%20experiments%20demonstrate%20that%20VA%2BVoicePop%20achieves%0A69%25%20accuracy%20under%20normal%20conditions%2C%2037%25%20accuracy%20when%20subjected%20to%20a%20baseline%0Alabel%20flipping%20attack%2C%20and%20just%2014%25%20accuracy%20under%20our%20proposed%20SyntheticPop%0Aattack%2C%20emphasizing%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09553v1&entry.124074799=Read"},
{"title": "Learning to Coordinate with Experts", "author": "Mohamad H. Danesh and Tu Trinh and Benjamin Plaut and Nguyen X. Khanh", "abstract": "  When deployed in dynamic environments, AI agents will inevitably encounter\nchallenges that exceed their individual capabilities. Leveraging assistance\nfrom expert agents-whether human or AI-can significantly enhance safety and\nperformance in such situations. However, querying experts is often costly,\nnecessitating the development of agents that can efficiently request and\nutilize expert guidance. In this paper, we introduce a fundamental coordination\nproblem called Learning to Yield and Request Control (YRC), where the objective\nis to learn a strategy that determines when to act autonomously and when to\nseek expert assistance. We consider a challenging practical setting in which an\nagent does not interact with experts during training but must adapt to novel\nenvironmental changes and expert interventions at test time. To facilitate\nempirical research, we introduce YRC-Bench, an open-source benchmark featuring\ndiverse domains. YRC-Bench provides a standardized Gym-like API, simulated\nexperts, evaluation pipeline, and implementation of competitive baselines.\nTowards tackling the YRC problem, we propose a novel validation approach and\ninvestigate the performance of various learning methods across diverse\nenvironments, yielding insights that can guide future research.\n", "link": "http://arxiv.org/abs/2502.09583v1", "date": "2025-02-13", "relevancy": 1.6458, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5804}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Coordinate%20with%20Experts&body=Title%3A%20Learning%20to%20Coordinate%20with%20Experts%0AAuthor%3A%20Mohamad%20H.%20Danesh%20and%20Tu%20Trinh%20and%20Benjamin%20Plaut%20and%20Nguyen%20X.%20Khanh%0AAbstract%3A%20%20%20When%20deployed%20in%20dynamic%20environments%2C%20AI%20agents%20will%20inevitably%20encounter%0Achallenges%20that%20exceed%20their%20individual%20capabilities.%20Leveraging%20assistance%0Afrom%20expert%20agents-whether%20human%20or%20AI-can%20significantly%20enhance%20safety%20and%0Aperformance%20in%20such%20situations.%20However%2C%20querying%20experts%20is%20often%20costly%2C%0Anecessitating%20the%20development%20of%20agents%20that%20can%20efficiently%20request%20and%0Autilize%20expert%20guidance.%20In%20this%20paper%2C%20we%20introduce%20a%20fundamental%20coordination%0Aproblem%20called%20Learning%20to%20Yield%20and%20Request%20Control%20%28YRC%29%2C%20where%20the%20objective%0Ais%20to%20learn%20a%20strategy%20that%20determines%20when%20to%20act%20autonomously%20and%20when%20to%0Aseek%20expert%20assistance.%20We%20consider%20a%20challenging%20practical%20setting%20in%20which%20an%0Aagent%20does%20not%20interact%20with%20experts%20during%20training%20but%20must%20adapt%20to%20novel%0Aenvironmental%20changes%20and%20expert%20interventions%20at%20test%20time.%20To%20facilitate%0Aempirical%20research%2C%20we%20introduce%20YRC-Bench%2C%20an%20open-source%20benchmark%20featuring%0Adiverse%20domains.%20YRC-Bench%20provides%20a%20standardized%20Gym-like%20API%2C%20simulated%0Aexperts%2C%20evaluation%20pipeline%2C%20and%20implementation%20of%20competitive%20baselines.%0ATowards%20tackling%20the%20YRC%20problem%2C%20we%20propose%20a%20novel%20validation%20approach%20and%0Ainvestigate%20the%20performance%20of%20various%20learning%20methods%20across%20diverse%0Aenvironments%2C%20yielding%20insights%20that%20can%20guide%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Coordinate%2520with%2520Experts%26entry.906535625%3DMohamad%2520H.%2520Danesh%2520and%2520Tu%2520Trinh%2520and%2520Benjamin%2520Plaut%2520and%2520Nguyen%2520X.%2520Khanh%26entry.1292438233%3D%2520%2520When%2520deployed%2520in%2520dynamic%2520environments%252C%2520AI%2520agents%2520will%2520inevitably%2520encounter%250Achallenges%2520that%2520exceed%2520their%2520individual%2520capabilities.%2520Leveraging%2520assistance%250Afrom%2520expert%2520agents-whether%2520human%2520or%2520AI-can%2520significantly%2520enhance%2520safety%2520and%250Aperformance%2520in%2520such%2520situations.%2520However%252C%2520querying%2520experts%2520is%2520often%2520costly%252C%250Anecessitating%2520the%2520development%2520of%2520agents%2520that%2520can%2520efficiently%2520request%2520and%250Autilize%2520expert%2520guidance.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520fundamental%2520coordination%250Aproblem%2520called%2520Learning%2520to%2520Yield%2520and%2520Request%2520Control%2520%2528YRC%2529%252C%2520where%2520the%2520objective%250Ais%2520to%2520learn%2520a%2520strategy%2520that%2520determines%2520when%2520to%2520act%2520autonomously%2520and%2520when%2520to%250Aseek%2520expert%2520assistance.%2520We%2520consider%2520a%2520challenging%2520practical%2520setting%2520in%2520which%2520an%250Aagent%2520does%2520not%2520interact%2520with%2520experts%2520during%2520training%2520but%2520must%2520adapt%2520to%2520novel%250Aenvironmental%2520changes%2520and%2520expert%2520interventions%2520at%2520test%2520time.%2520To%2520facilitate%250Aempirical%2520research%252C%2520we%2520introduce%2520YRC-Bench%252C%2520an%2520open-source%2520benchmark%2520featuring%250Adiverse%2520domains.%2520YRC-Bench%2520provides%2520a%2520standardized%2520Gym-like%2520API%252C%2520simulated%250Aexperts%252C%2520evaluation%2520pipeline%252C%2520and%2520implementation%2520of%2520competitive%2520baselines.%250ATowards%2520tackling%2520the%2520YRC%2520problem%252C%2520we%2520propose%2520a%2520novel%2520validation%2520approach%2520and%250Ainvestigate%2520the%2520performance%2520of%2520various%2520learning%2520methods%2520across%2520diverse%250Aenvironments%252C%2520yielding%2520insights%2520that%2520can%2520guide%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Coordinate%20with%20Experts&entry.906535625=Mohamad%20H.%20Danesh%20and%20Tu%20Trinh%20and%20Benjamin%20Plaut%20and%20Nguyen%20X.%20Khanh&entry.1292438233=%20%20When%20deployed%20in%20dynamic%20environments%2C%20AI%20agents%20will%20inevitably%20encounter%0Achallenges%20that%20exceed%20their%20individual%20capabilities.%20Leveraging%20assistance%0Afrom%20expert%20agents-whether%20human%20or%20AI-can%20significantly%20enhance%20safety%20and%0Aperformance%20in%20such%20situations.%20However%2C%20querying%20experts%20is%20often%20costly%2C%0Anecessitating%20the%20development%20of%20agents%20that%20can%20efficiently%20request%20and%0Autilize%20expert%20guidance.%20In%20this%20paper%2C%20we%20introduce%20a%20fundamental%20coordination%0Aproblem%20called%20Learning%20to%20Yield%20and%20Request%20Control%20%28YRC%29%2C%20where%20the%20objective%0Ais%20to%20learn%20a%20strategy%20that%20determines%20when%20to%20act%20autonomously%20and%20when%20to%0Aseek%20expert%20assistance.%20We%20consider%20a%20challenging%20practical%20setting%20in%20which%20an%0Aagent%20does%20not%20interact%20with%20experts%20during%20training%20but%20must%20adapt%20to%20novel%0Aenvironmental%20changes%20and%20expert%20interventions%20at%20test%20time.%20To%20facilitate%0Aempirical%20research%2C%20we%20introduce%20YRC-Bench%2C%20an%20open-source%20benchmark%20featuring%0Adiverse%20domains.%20YRC-Bench%20provides%20a%20standardized%20Gym-like%20API%2C%20simulated%0Aexperts%2C%20evaluation%20pipeline%2C%20and%20implementation%20of%20competitive%20baselines.%0ATowards%20tackling%20the%20YRC%20problem%2C%20we%20propose%20a%20novel%20validation%20approach%20and%0Ainvestigate%20the%20performance%20of%20various%20learning%20methods%20across%20diverse%0Aenvironments%2C%20yielding%20insights%20that%20can%20guide%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09583v1&entry.124074799=Read"},
{"title": "Theoretical Benefit and Limitation of Diffusion Language Model", "author": "Guhao Feng and Yihan Geng and Jian Guan and Wei Wu and Liwei Wang and Di He", "abstract": "  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n", "link": "http://arxiv.org/abs/2502.09622v1", "date": "2025-02-13", "relevancy": 1.6352, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5837}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Benefit%20and%20Limitation%20of%20Diffusion%20Language%20Model&body=Title%3A%20Theoretical%20Benefit%20and%20Limitation%20of%20Diffusion%20Language%20Model%0AAuthor%3A%20Guhao%20Feng%20and%20Yihan%20Geng%20and%20Jian%20Guan%20and%20Wei%20Wu%20and%20Liwei%20Wang%20and%20Di%20He%0AAbstract%3A%20%20%20Diffusion%20language%20models%20have%20emerged%20as%20a%20promising%20approach%20for%20text%0Ageneration.%20One%20would%20naturally%20expect%20this%20method%20to%20be%20an%20efficient%0Areplacement%20for%20autoregressive%20models%20since%20multiple%20tokens%20can%20be%20sampled%20in%0Aparallel%20during%20each%20diffusion%20step.%20However%2C%20its%20efficiency-accuracy%20trade-off%0Ais%20not%20yet%20well%20understood.%20In%20this%20paper%2C%20we%20present%20a%20rigorous%20theoretical%0Aanalysis%20of%20a%20widely%20used%20type%20of%20diffusion%20language%20model%2C%20the%20Masked%0ADiffusion%20Model%20%28MDM%29%2C%20and%20find%20that%20its%20effectiveness%20heavily%20depends%20on%20the%0Atarget%20evaluation%20metric.%20Under%20mild%20conditions%2C%20we%20prove%20that%20when%20using%0Aperplexity%20as%20the%20metric%2C%20MDMs%20can%20achieve%20near-optimal%20perplexity%20in%20sampling%0Asteps%20regardless%20of%20sequence%20length%2C%20demonstrating%20that%20efficiency%20can%20be%0Aachieved%20without%20sacrificing%20performance.%20However%2C%20when%20using%20the%20sequence%0Aerror%20rate--which%20is%20important%20for%20understanding%20the%20%22correctness%22%20of%20a%0Asequence%2C%20such%20as%20a%20reasoning%20chain--we%20show%20that%20the%20required%20sampling%20steps%0Amust%20scale%20linearly%20with%20sequence%20length%20to%20obtain%20%22correct%22%20sequences%2C%20thereby%0Aeliminating%20MDM%27s%20efficiency%20advantage%20over%20autoregressive%20models.%20Our%20analysis%0Aestablishes%20the%20first%20theoretical%20foundation%20for%20understanding%20the%20benefits%20and%0Alimitations%20of%20MDMs.%20All%20theoretical%20findings%20are%20supported%20by%20empirical%0Astudies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoretical%2520Benefit%2520and%2520Limitation%2520of%2520Diffusion%2520Language%2520Model%26entry.906535625%3DGuhao%2520Feng%2520and%2520Yihan%2520Geng%2520and%2520Jian%2520Guan%2520and%2520Wei%2520Wu%2520and%2520Liwei%2520Wang%2520and%2520Di%2520He%26entry.1292438233%3D%2520%2520Diffusion%2520language%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520text%250Ageneration.%2520One%2520would%2520naturally%2520expect%2520this%2520method%2520to%2520be%2520an%2520efficient%250Areplacement%2520for%2520autoregressive%2520models%2520since%2520multiple%2520tokens%2520can%2520be%2520sampled%2520in%250Aparallel%2520during%2520each%2520diffusion%2520step.%2520However%252C%2520its%2520efficiency-accuracy%2520trade-off%250Ais%2520not%2520yet%2520well%2520understood.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520rigorous%2520theoretical%250Aanalysis%2520of%2520a%2520widely%2520used%2520type%2520of%2520diffusion%2520language%2520model%252C%2520the%2520Masked%250ADiffusion%2520Model%2520%2528MDM%2529%252C%2520and%2520find%2520that%2520its%2520effectiveness%2520heavily%2520depends%2520on%2520the%250Atarget%2520evaluation%2520metric.%2520Under%2520mild%2520conditions%252C%2520we%2520prove%2520that%2520when%2520using%250Aperplexity%2520as%2520the%2520metric%252C%2520MDMs%2520can%2520achieve%2520near-optimal%2520perplexity%2520in%2520sampling%250Asteps%2520regardless%2520of%2520sequence%2520length%252C%2520demonstrating%2520that%2520efficiency%2520can%2520be%250Aachieved%2520without%2520sacrificing%2520performance.%2520However%252C%2520when%2520using%2520the%2520sequence%250Aerror%2520rate--which%2520is%2520important%2520for%2520understanding%2520the%2520%2522correctness%2522%2520of%2520a%250Asequence%252C%2520such%2520as%2520a%2520reasoning%2520chain--we%2520show%2520that%2520the%2520required%2520sampling%2520steps%250Amust%2520scale%2520linearly%2520with%2520sequence%2520length%2520to%2520obtain%2520%2522correct%2522%2520sequences%252C%2520thereby%250Aeliminating%2520MDM%2527s%2520efficiency%2520advantage%2520over%2520autoregressive%2520models.%2520Our%2520analysis%250Aestablishes%2520the%2520first%2520theoretical%2520foundation%2520for%2520understanding%2520the%2520benefits%2520and%250Alimitations%2520of%2520MDMs.%2520All%2520theoretical%2520findings%2520are%2520supported%2520by%2520empirical%250Astudies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Benefit%20and%20Limitation%20of%20Diffusion%20Language%20Model&entry.906535625=Guhao%20Feng%20and%20Yihan%20Geng%20and%20Jian%20Guan%20and%20Wei%20Wu%20and%20Liwei%20Wang%20and%20Di%20He&entry.1292438233=%20%20Diffusion%20language%20models%20have%20emerged%20as%20a%20promising%20approach%20for%20text%0Ageneration.%20One%20would%20naturally%20expect%20this%20method%20to%20be%20an%20efficient%0Areplacement%20for%20autoregressive%20models%20since%20multiple%20tokens%20can%20be%20sampled%20in%0Aparallel%20during%20each%20diffusion%20step.%20However%2C%20its%20efficiency-accuracy%20trade-off%0Ais%20not%20yet%20well%20understood.%20In%20this%20paper%2C%20we%20present%20a%20rigorous%20theoretical%0Aanalysis%20of%20a%20widely%20used%20type%20of%20diffusion%20language%20model%2C%20the%20Masked%0ADiffusion%20Model%20%28MDM%29%2C%20and%20find%20that%20its%20effectiveness%20heavily%20depends%20on%20the%0Atarget%20evaluation%20metric.%20Under%20mild%20conditions%2C%20we%20prove%20that%20when%20using%0Aperplexity%20as%20the%20metric%2C%20MDMs%20can%20achieve%20near-optimal%20perplexity%20in%20sampling%0Asteps%20regardless%20of%20sequence%20length%2C%20demonstrating%20that%20efficiency%20can%20be%0Aachieved%20without%20sacrificing%20performance.%20However%2C%20when%20using%20the%20sequence%0Aerror%20rate--which%20is%20important%20for%20understanding%20the%20%22correctness%22%20of%20a%0Asequence%2C%20such%20as%20a%20reasoning%20chain--we%20show%20that%20the%20required%20sampling%20steps%0Amust%20scale%20linearly%20with%20sequence%20length%20to%20obtain%20%22correct%22%20sequences%2C%20thereby%0Aeliminating%20MDM%27s%20efficiency%20advantage%20over%20autoregressive%20models.%20Our%20analysis%0Aestablishes%20the%20first%20theoretical%20foundation%20for%20understanding%20the%20benefits%20and%0Alimitations%20of%20MDMs.%20All%20theoretical%20findings%20are%20supported%20by%20empirical%0Astudies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09622v1&entry.124074799=Read"},
{"title": "Metamorphic Testing for Pose Estimation Systems", "author": "Matias Duran and Thomas Laurent and Ellen Rushe and Anthony Ventresque", "abstract": "  Pose estimation systems are used in a variety of fields, from sports\nanalytics to livestock care. Given their potential impact, it is paramount to\nsystematically test their behaviour and potential for failure. This is a\ncomplex task due to the oracle problem and the high cost of manual labelling\nnecessary to build ground truth keypoints. This problem is exacerbated by the\nfact that different applications require systems to focus on different subjects\n(e.g., human versus animal) or landmarks (e.g., only extremities versus whole\nbody and face), which makes labelled test data rarely reusable. To combat these\nproblems we propose MET-POSE, a metamorphic testing framework for pose\nestimation systems that bypasses the need for manual annotation while assessing\nthe performance of these systems under different circumstances. MET-POSE thus\nallows users of pose estimation systems to assess the systems in conditions\nthat more closely relate to their application without having to label an ad-hoc\ntest dataset or rely only on available datasets, which may not be adapted to\ntheir application domain. While we define MET-POSE in general terms, we also\npresent a non-exhaustive list of metamorphic rules that represent common\nchallenges in computer vision applications, as well as a specific way to\nevaluate these rules. We then experimentally show the effectiveness of MET-POSE\nby applying it to Mediapipe Holistic, a state of the art human pose estimation\nsystem, with the FLIC and PHOENIX datasets. With these experiments, we outline\nnumerous ways in which the outputs of MET-POSE can uncover faults in pose\nestimation systems at a similar or higher rate than classic testing using hand\nlabelled data, and show that users can tailor the rule set they use to the\nfaults and level of accuracy relevant to their application.\n", "link": "http://arxiv.org/abs/2502.09460v1", "date": "2025-02-13", "relevancy": 1.6308, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5503}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5486}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metamorphic%20Testing%20for%20Pose%20Estimation%20Systems&body=Title%3A%20Metamorphic%20Testing%20for%20Pose%20Estimation%20Systems%0AAuthor%3A%20Matias%20Duran%20and%20Thomas%20Laurent%20and%20Ellen%20Rushe%20and%20Anthony%20Ventresque%0AAbstract%3A%20%20%20Pose%20estimation%20systems%20are%20used%20in%20a%20variety%20of%20fields%2C%20from%20sports%0Aanalytics%20to%20livestock%20care.%20Given%20their%20potential%20impact%2C%20it%20is%20paramount%20to%0Asystematically%20test%20their%20behaviour%20and%20potential%20for%20failure.%20This%20is%20a%0Acomplex%20task%20due%20to%20the%20oracle%20problem%20and%20the%20high%20cost%20of%20manual%20labelling%0Anecessary%20to%20build%20ground%20truth%20keypoints.%20This%20problem%20is%20exacerbated%20by%20the%0Afact%20that%20different%20applications%20require%20systems%20to%20focus%20on%20different%20subjects%0A%28e.g.%2C%20human%20versus%20animal%29%20or%20landmarks%20%28e.g.%2C%20only%20extremities%20versus%20whole%0Abody%20and%20face%29%2C%20which%20makes%20labelled%20test%20data%20rarely%20reusable.%20To%20combat%20these%0Aproblems%20we%20propose%20MET-POSE%2C%20a%20metamorphic%20testing%20framework%20for%20pose%0Aestimation%20systems%20that%20bypasses%20the%20need%20for%20manual%20annotation%20while%20assessing%0Athe%20performance%20of%20these%20systems%20under%20different%20circumstances.%20MET-POSE%20thus%0Aallows%20users%20of%20pose%20estimation%20systems%20to%20assess%20the%20systems%20in%20conditions%0Athat%20more%20closely%20relate%20to%20their%20application%20without%20having%20to%20label%20an%20ad-hoc%0Atest%20dataset%20or%20rely%20only%20on%20available%20datasets%2C%20which%20may%20not%20be%20adapted%20to%0Atheir%20application%20domain.%20While%20we%20define%20MET-POSE%20in%20general%20terms%2C%20we%20also%0Apresent%20a%20non-exhaustive%20list%20of%20metamorphic%20rules%20that%20represent%20common%0Achallenges%20in%20computer%20vision%20applications%2C%20as%20well%20as%20a%20specific%20way%20to%0Aevaluate%20these%20rules.%20We%20then%20experimentally%20show%20the%20effectiveness%20of%20MET-POSE%0Aby%20applying%20it%20to%20Mediapipe%20Holistic%2C%20a%20state%20of%20the%20art%20human%20pose%20estimation%0Asystem%2C%20with%20the%20FLIC%20and%20PHOENIX%20datasets.%20With%20these%20experiments%2C%20we%20outline%0Anumerous%20ways%20in%20which%20the%20outputs%20of%20MET-POSE%20can%20uncover%20faults%20in%20pose%0Aestimation%20systems%20at%20a%20similar%20or%20higher%20rate%20than%20classic%20testing%20using%20hand%0Alabelled%20data%2C%20and%20show%20that%20users%20can%20tailor%20the%20rule%20set%20they%20use%20to%20the%0Afaults%20and%20level%20of%20accuracy%20relevant%20to%20their%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetamorphic%2520Testing%2520for%2520Pose%2520Estimation%2520Systems%26entry.906535625%3DMatias%2520Duran%2520and%2520Thomas%2520Laurent%2520and%2520Ellen%2520Rushe%2520and%2520Anthony%2520Ventresque%26entry.1292438233%3D%2520%2520Pose%2520estimation%2520systems%2520are%2520used%2520in%2520a%2520variety%2520of%2520fields%252C%2520from%2520sports%250Aanalytics%2520to%2520livestock%2520care.%2520Given%2520their%2520potential%2520impact%252C%2520it%2520is%2520paramount%2520to%250Asystematically%2520test%2520their%2520behaviour%2520and%2520potential%2520for%2520failure.%2520This%2520is%2520a%250Acomplex%2520task%2520due%2520to%2520the%2520oracle%2520problem%2520and%2520the%2520high%2520cost%2520of%2520manual%2520labelling%250Anecessary%2520to%2520build%2520ground%2520truth%2520keypoints.%2520This%2520problem%2520is%2520exacerbated%2520by%2520the%250Afact%2520that%2520different%2520applications%2520require%2520systems%2520to%2520focus%2520on%2520different%2520subjects%250A%2528e.g.%252C%2520human%2520versus%2520animal%2529%2520or%2520landmarks%2520%2528e.g.%252C%2520only%2520extremities%2520versus%2520whole%250Abody%2520and%2520face%2529%252C%2520which%2520makes%2520labelled%2520test%2520data%2520rarely%2520reusable.%2520To%2520combat%2520these%250Aproblems%2520we%2520propose%2520MET-POSE%252C%2520a%2520metamorphic%2520testing%2520framework%2520for%2520pose%250Aestimation%2520systems%2520that%2520bypasses%2520the%2520need%2520for%2520manual%2520annotation%2520while%2520assessing%250Athe%2520performance%2520of%2520these%2520systems%2520under%2520different%2520circumstances.%2520MET-POSE%2520thus%250Aallows%2520users%2520of%2520pose%2520estimation%2520systems%2520to%2520assess%2520the%2520systems%2520in%2520conditions%250Athat%2520more%2520closely%2520relate%2520to%2520their%2520application%2520without%2520having%2520to%2520label%2520an%2520ad-hoc%250Atest%2520dataset%2520or%2520rely%2520only%2520on%2520available%2520datasets%252C%2520which%2520may%2520not%2520be%2520adapted%2520to%250Atheir%2520application%2520domain.%2520While%2520we%2520define%2520MET-POSE%2520in%2520general%2520terms%252C%2520we%2520also%250Apresent%2520a%2520non-exhaustive%2520list%2520of%2520metamorphic%2520rules%2520that%2520represent%2520common%250Achallenges%2520in%2520computer%2520vision%2520applications%252C%2520as%2520well%2520as%2520a%2520specific%2520way%2520to%250Aevaluate%2520these%2520rules.%2520We%2520then%2520experimentally%2520show%2520the%2520effectiveness%2520of%2520MET-POSE%250Aby%2520applying%2520it%2520to%2520Mediapipe%2520Holistic%252C%2520a%2520state%2520of%2520the%2520art%2520human%2520pose%2520estimation%250Asystem%252C%2520with%2520the%2520FLIC%2520and%2520PHOENIX%2520datasets.%2520With%2520these%2520experiments%252C%2520we%2520outline%250Anumerous%2520ways%2520in%2520which%2520the%2520outputs%2520of%2520MET-POSE%2520can%2520uncover%2520faults%2520in%2520pose%250Aestimation%2520systems%2520at%2520a%2520similar%2520or%2520higher%2520rate%2520than%2520classic%2520testing%2520using%2520hand%250Alabelled%2520data%252C%2520and%2520show%2520that%2520users%2520can%2520tailor%2520the%2520rule%2520set%2520they%2520use%2520to%2520the%250Afaults%2520and%2520level%2520of%2520accuracy%2520relevant%2520to%2520their%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metamorphic%20Testing%20for%20Pose%20Estimation%20Systems&entry.906535625=Matias%20Duran%20and%20Thomas%20Laurent%20and%20Ellen%20Rushe%20and%20Anthony%20Ventresque&entry.1292438233=%20%20Pose%20estimation%20systems%20are%20used%20in%20a%20variety%20of%20fields%2C%20from%20sports%0Aanalytics%20to%20livestock%20care.%20Given%20their%20potential%20impact%2C%20it%20is%20paramount%20to%0Asystematically%20test%20their%20behaviour%20and%20potential%20for%20failure.%20This%20is%20a%0Acomplex%20task%20due%20to%20the%20oracle%20problem%20and%20the%20high%20cost%20of%20manual%20labelling%0Anecessary%20to%20build%20ground%20truth%20keypoints.%20This%20problem%20is%20exacerbated%20by%20the%0Afact%20that%20different%20applications%20require%20systems%20to%20focus%20on%20different%20subjects%0A%28e.g.%2C%20human%20versus%20animal%29%20or%20landmarks%20%28e.g.%2C%20only%20extremities%20versus%20whole%0Abody%20and%20face%29%2C%20which%20makes%20labelled%20test%20data%20rarely%20reusable.%20To%20combat%20these%0Aproblems%20we%20propose%20MET-POSE%2C%20a%20metamorphic%20testing%20framework%20for%20pose%0Aestimation%20systems%20that%20bypasses%20the%20need%20for%20manual%20annotation%20while%20assessing%0Athe%20performance%20of%20these%20systems%20under%20different%20circumstances.%20MET-POSE%20thus%0Aallows%20users%20of%20pose%20estimation%20systems%20to%20assess%20the%20systems%20in%20conditions%0Athat%20more%20closely%20relate%20to%20their%20application%20without%20having%20to%20label%20an%20ad-hoc%0Atest%20dataset%20or%20rely%20only%20on%20available%20datasets%2C%20which%20may%20not%20be%20adapted%20to%0Atheir%20application%20domain.%20While%20we%20define%20MET-POSE%20in%20general%20terms%2C%20we%20also%0Apresent%20a%20non-exhaustive%20list%20of%20metamorphic%20rules%20that%20represent%20common%0Achallenges%20in%20computer%20vision%20applications%2C%20as%20well%20as%20a%20specific%20way%20to%0Aevaluate%20these%20rules.%20We%20then%20experimentally%20show%20the%20effectiveness%20of%20MET-POSE%0Aby%20applying%20it%20to%20Mediapipe%20Holistic%2C%20a%20state%20of%20the%20art%20human%20pose%20estimation%0Asystem%2C%20with%20the%20FLIC%20and%20PHOENIX%20datasets.%20With%20these%20experiments%2C%20we%20outline%0Anumerous%20ways%20in%20which%20the%20outputs%20of%20MET-POSE%20can%20uncover%20faults%20in%20pose%0Aestimation%20systems%20at%20a%20similar%20or%20higher%20rate%20than%20classic%20testing%20using%20hand%0Alabelled%20data%2C%20and%20show%20that%20users%20can%20tailor%20the%20rule%20set%20they%20use%20to%20the%0Afaults%20and%20level%20of%20accuracy%20relevant%20to%20their%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09460v1&entry.124074799=Read"},
{"title": "AttentionSmithy: A Modular Framework for Rapid Transformer Development\n  and Customization", "author": "Caleb Cranney and Jesse G. Meyer", "abstract": "  Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.\n", "link": "http://arxiv.org/abs/2502.09503v1", "date": "2025-02-13", "relevancy": 1.6125, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5676}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentionSmithy%3A%20A%20Modular%20Framework%20for%20Rapid%20Transformer%20Development%0A%20%20and%20Customization&body=Title%3A%20AttentionSmithy%3A%20A%20Modular%20Framework%20for%20Rapid%20Transformer%20Development%0A%20%20and%20Customization%0AAuthor%3A%20Caleb%20Cranney%20and%20Jesse%20G.%20Meyer%0AAbstract%3A%20%20%20Transformer%20architectures%20have%20transformed%20AI%20applications%20but%20remain%20complex%0Ato%20customize%20for%20domain%20experts%20lacking%20low-level%20implementation%20expertise.%20We%0Aintroduce%20AttentionSmithy%2C%20a%20modular%20software%20package%20that%20simplifies%0Atransformer%20innovation%20by%20breaking%20down%20key%20components%20into%20reusable%20building%0Ablocks%3A%20attention%20modules%2C%20feed-forward%20networks%2C%20normalization%20layers%2C%20and%0Apositional%20encodings.%20Users%20can%20rapidly%20prototype%20and%20evaluate%20transformer%0Avariants%20without%20extensive%20coding.%20Our%20framework%20supports%20four%20positional%0Aencoding%20strategies%20and%20integrates%20with%20neural%20architecture%20search%20for%0Aautomated%20design.%20We%20validate%20AttentionSmithy%20by%20replicating%20the%20original%0Atransformer%20under%20resource%20constraints%20and%20optimizing%20translation%20performance%0Aby%20combining%20positional%20encodings.%20Additionally%2C%20we%20demonstrate%20its%0Aadaptability%20in%20gene-specific%20modeling%2C%20achieving%20over%2095%25%20accuracy%20in%20cell%0Atype%20classification.%20These%20case%20studies%20highlight%20AttentionSmithy%27s%20potential%0Ato%20accelerate%20research%20across%20diverse%20fields%20by%20removing%20framework%0Aimplementation%20barriers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentionSmithy%253A%2520A%2520Modular%2520Framework%2520for%2520Rapid%2520Transformer%2520Development%250A%2520%2520and%2520Customization%26entry.906535625%3DCaleb%2520Cranney%2520and%2520Jesse%2520G.%2520Meyer%26entry.1292438233%3D%2520%2520Transformer%2520architectures%2520have%2520transformed%2520AI%2520applications%2520but%2520remain%2520complex%250Ato%2520customize%2520for%2520domain%2520experts%2520lacking%2520low-level%2520implementation%2520expertise.%2520We%250Aintroduce%2520AttentionSmithy%252C%2520a%2520modular%2520software%2520package%2520that%2520simplifies%250Atransformer%2520innovation%2520by%2520breaking%2520down%2520key%2520components%2520into%2520reusable%2520building%250Ablocks%253A%2520attention%2520modules%252C%2520feed-forward%2520networks%252C%2520normalization%2520layers%252C%2520and%250Apositional%2520encodings.%2520Users%2520can%2520rapidly%2520prototype%2520and%2520evaluate%2520transformer%250Avariants%2520without%2520extensive%2520coding.%2520Our%2520framework%2520supports%2520four%2520positional%250Aencoding%2520strategies%2520and%2520integrates%2520with%2520neural%2520architecture%2520search%2520for%250Aautomated%2520design.%2520We%2520validate%2520AttentionSmithy%2520by%2520replicating%2520the%2520original%250Atransformer%2520under%2520resource%2520constraints%2520and%2520optimizing%2520translation%2520performance%250Aby%2520combining%2520positional%2520encodings.%2520Additionally%252C%2520we%2520demonstrate%2520its%250Aadaptability%2520in%2520gene-specific%2520modeling%252C%2520achieving%2520over%252095%2525%2520accuracy%2520in%2520cell%250Atype%2520classification.%2520These%2520case%2520studies%2520highlight%2520AttentionSmithy%2527s%2520potential%250Ato%2520accelerate%2520research%2520across%2520diverse%2520fields%2520by%2520removing%2520framework%250Aimplementation%2520barriers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentionSmithy%3A%20A%20Modular%20Framework%20for%20Rapid%20Transformer%20Development%0A%20%20and%20Customization&entry.906535625=Caleb%20Cranney%20and%20Jesse%20G.%20Meyer&entry.1292438233=%20%20Transformer%20architectures%20have%20transformed%20AI%20applications%20but%20remain%20complex%0Ato%20customize%20for%20domain%20experts%20lacking%20low-level%20implementation%20expertise.%20We%0Aintroduce%20AttentionSmithy%2C%20a%20modular%20software%20package%20that%20simplifies%0Atransformer%20innovation%20by%20breaking%20down%20key%20components%20into%20reusable%20building%0Ablocks%3A%20attention%20modules%2C%20feed-forward%20networks%2C%20normalization%20layers%2C%20and%0Apositional%20encodings.%20Users%20can%20rapidly%20prototype%20and%20evaluate%20transformer%0Avariants%20without%20extensive%20coding.%20Our%20framework%20supports%20four%20positional%0Aencoding%20strategies%20and%20integrates%20with%20neural%20architecture%20search%20for%0Aautomated%20design.%20We%20validate%20AttentionSmithy%20by%20replicating%20the%20original%0Atransformer%20under%20resource%20constraints%20and%20optimizing%20translation%20performance%0Aby%20combining%20positional%20encodings.%20Additionally%2C%20we%20demonstrate%20its%0Aadaptability%20in%20gene-specific%20modeling%2C%20achieving%20over%2095%25%20accuracy%20in%20cell%0Atype%20classification.%20These%20case%20studies%20highlight%20AttentionSmithy%27s%20potential%0Ato%20accelerate%20research%20across%20diverse%20fields%20by%20removing%20framework%0Aimplementation%20barriers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09503v1&entry.124074799=Read"},
{"title": "TransMLA: Multi-Head Latent Attention Is All You Need", "author": "Fanxu Meng and Zengwei Yao and Muhan Zhang", "abstract": "  Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.\n", "link": "http://arxiv.org/abs/2502.07864v2", "date": "2025-02-13", "relevancy": 1.5878, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5379}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5227}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransMLA%3A%20Multi-Head%20Latent%20Attention%20Is%20All%20You%20Need&body=Title%3A%20TransMLA%3A%20Multi-Head%20Latent%20Attention%20Is%20All%20You%20Need%0AAuthor%3A%20Fanxu%20Meng%20and%20Zengwei%20Yao%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20Modern%20large%20language%20models%20%28LLMs%29%20often%20encounter%20communication%20bottlenecks%0Aon%20current%20hardware%2C%20rather%20than%20purely%20computational%20constraints.%20Multi-head%0ALatent%20Attention%20%28MLA%29%20tackles%20this%20challenge%20by%20using%20low-rank%20matrices%20in%20the%0Akey-value%20%28KV%29%20layers%2C%20thereby%20allowing%20compressed%20latent%20KV%20states%20to%20be%0Acached.%20This%20approach%20significantly%20reduces%20the%20KV%20cache%20size%20relative%20to%0Atraditional%20multi-head%20attention%2C%20leading%20to%20faster%20inference.%20Moreover%2C%20MLA%0Aemploys%20an%20up-projection%20matrix%20to%20increase%20expressiveness%2C%20trading%20additional%0Acomputation%20for%20reduced%20communication%20overhead.%20Although%20MLA%20has%20demonstrated%0Aefficiency%20and%20effectiveness%20in%20Deepseek%20V2/V3/R1%2C%20many%20major%20model%20providers%0Astill%20rely%20on%20Group%20Query%20Attention%20%28GQA%29%20and%20have%20not%20announced%20any%20plans%20to%0Aadopt%20MLA.%20In%20this%20paper%2C%20we%20show%20that%20GQA%20can%20always%20be%20represented%20by%20MLA%0Awhile%20maintaining%20the%20same%20KV%20cache%20overhead%2C%20but%20the%20converse%20does%20not%20hold.%0ATo%20encourage%20broader%20use%20of%20MLA%2C%20we%20introduce%20TransMLA%2C%20a%20post-training%20method%0Athat%20converts%20widely%20used%20GQA-based%20pre-trained%20models%20%28e.g.%2C%20LLaMA%2C%20Qwen%2C%0AMixtral%29%20into%20MLA-based%20models.%20After%20conversion%2C%20the%20model%20can%20undergo%0Aadditional%20training%20to%20boost%20expressiveness%20without%20increasing%20the%20KV%20cache%0Asize.%20Furthermore%2C%20we%20plan%20to%20develop%20MLA-specific%20inference%20acceleration%0Atechniques%20to%20preserve%20low%20latency%20in%20transformed%20models%2C%20thus%20enabling%20more%0Aefficient%20distillation%20of%20Deepseek%20R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransMLA%253A%2520Multi-Head%2520Latent%2520Attention%2520Is%2520All%2520You%2520Need%26entry.906535625%3DFanxu%2520Meng%2520and%2520Zengwei%2520Yao%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520encounter%2520communication%2520bottlenecks%250Aon%2520current%2520hardware%252C%2520rather%2520than%2520purely%2520computational%2520constraints.%2520Multi-head%250ALatent%2520Attention%2520%2528MLA%2529%2520tackles%2520this%2520challenge%2520by%2520using%2520low-rank%2520matrices%2520in%2520the%250Akey-value%2520%2528KV%2529%2520layers%252C%2520thereby%2520allowing%2520compressed%2520latent%2520KV%2520states%2520to%2520be%250Acached.%2520This%2520approach%2520significantly%2520reduces%2520the%2520KV%2520cache%2520size%2520relative%2520to%250Atraditional%2520multi-head%2520attention%252C%2520leading%2520to%2520faster%2520inference.%2520Moreover%252C%2520MLA%250Aemploys%2520an%2520up-projection%2520matrix%2520to%2520increase%2520expressiveness%252C%2520trading%2520additional%250Acomputation%2520for%2520reduced%2520communication%2520overhead.%2520Although%2520MLA%2520has%2520demonstrated%250Aefficiency%2520and%2520effectiveness%2520in%2520Deepseek%2520V2/V3/R1%252C%2520many%2520major%2520model%2520providers%250Astill%2520rely%2520on%2520Group%2520Query%2520Attention%2520%2528GQA%2529%2520and%2520have%2520not%2520announced%2520any%2520plans%2520to%250Aadopt%2520MLA.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520GQA%2520can%2520always%2520be%2520represented%2520by%2520MLA%250Awhile%2520maintaining%2520the%2520same%2520KV%2520cache%2520overhead%252C%2520but%2520the%2520converse%2520does%2520not%2520hold.%250ATo%2520encourage%2520broader%2520use%2520of%2520MLA%252C%2520we%2520introduce%2520TransMLA%252C%2520a%2520post-training%2520method%250Athat%2520converts%2520widely%2520used%2520GQA-based%2520pre-trained%2520models%2520%2528e.g.%252C%2520LLaMA%252C%2520Qwen%252C%250AMixtral%2529%2520into%2520MLA-based%2520models.%2520After%2520conversion%252C%2520the%2520model%2520can%2520undergo%250Aadditional%2520training%2520to%2520boost%2520expressiveness%2520without%2520increasing%2520the%2520KV%2520cache%250Asize.%2520Furthermore%252C%2520we%2520plan%2520to%2520develop%2520MLA-specific%2520inference%2520acceleration%250Atechniques%2520to%2520preserve%2520low%2520latency%2520in%2520transformed%2520models%252C%2520thus%2520enabling%2520more%250Aefficient%2520distillation%2520of%2520Deepseek%2520R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransMLA%3A%20Multi-Head%20Latent%20Attention%20Is%20All%20You%20Need&entry.906535625=Fanxu%20Meng%20and%20Zengwei%20Yao%20and%20Muhan%20Zhang&entry.1292438233=%20%20Modern%20large%20language%20models%20%28LLMs%29%20often%20encounter%20communication%20bottlenecks%0Aon%20current%20hardware%2C%20rather%20than%20purely%20computational%20constraints.%20Multi-head%0ALatent%20Attention%20%28MLA%29%20tackles%20this%20challenge%20by%20using%20low-rank%20matrices%20in%20the%0Akey-value%20%28KV%29%20layers%2C%20thereby%20allowing%20compressed%20latent%20KV%20states%20to%20be%0Acached.%20This%20approach%20significantly%20reduces%20the%20KV%20cache%20size%20relative%20to%0Atraditional%20multi-head%20attention%2C%20leading%20to%20faster%20inference.%20Moreover%2C%20MLA%0Aemploys%20an%20up-projection%20matrix%20to%20increase%20expressiveness%2C%20trading%20additional%0Acomputation%20for%20reduced%20communication%20overhead.%20Although%20MLA%20has%20demonstrated%0Aefficiency%20and%20effectiveness%20in%20Deepseek%20V2/V3/R1%2C%20many%20major%20model%20providers%0Astill%20rely%20on%20Group%20Query%20Attention%20%28GQA%29%20and%20have%20not%20announced%20any%20plans%20to%0Aadopt%20MLA.%20In%20this%20paper%2C%20we%20show%20that%20GQA%20can%20always%20be%20represented%20by%20MLA%0Awhile%20maintaining%20the%20same%20KV%20cache%20overhead%2C%20but%20the%20converse%20does%20not%20hold.%0ATo%20encourage%20broader%20use%20of%20MLA%2C%20we%20introduce%20TransMLA%2C%20a%20post-training%20method%0Athat%20converts%20widely%20used%20GQA-based%20pre-trained%20models%20%28e.g.%2C%20LLaMA%2C%20Qwen%2C%0AMixtral%29%20into%20MLA-based%20models.%20After%20conversion%2C%20the%20model%20can%20undergo%0Aadditional%20training%20to%20boost%20expressiveness%20without%20increasing%20the%20KV%20cache%0Asize.%20Furthermore%2C%20we%20plan%20to%20develop%20MLA-specific%20inference%20acceleration%0Atechniques%20to%20preserve%20low%20latency%20in%20transformed%20models%2C%20thus%20enabling%20more%0Aefficient%20distillation%20of%20Deepseek%20R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07864v2&entry.124074799=Read"},
{"title": "Variational Rectified Flow Matching", "author": "Pengsheng Guo and Alexander G. Schwing", "abstract": "  We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.\n", "link": "http://arxiv.org/abs/2502.09616v1", "date": "2025-02-13", "relevancy": 1.5856, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6102}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Rectified%20Flow%20Matching&body=Title%3A%20Variational%20Rectified%20Flow%20Matching%0AAuthor%3A%20Pengsheng%20Guo%20and%20Alexander%20G.%20Schwing%0AAbstract%3A%20%20%20We%20study%20Variational%20Rectified%20Flow%20Matching%2C%20a%20framework%20that%20enhances%0Aclassic%20rectified%20flow%20matching%20by%20modeling%20multi-modal%20velocity%20vector-fields.%0AAt%20inference%20time%2C%20classic%20rectified%20flow%20matching%20%27moves%27%20samples%20from%20a%0Asource%20distribution%20to%20the%20target%20distribution%20by%20solving%20an%20ordinary%0Adifferential%20equation%20via%20integration%20along%20a%20velocity%20vector-field.%20At%0Atraining%20time%2C%20the%20velocity%20vector-field%20is%20learnt%20by%20linearly%20interpolating%0Abetween%20coupled%20samples%20one%20drawn%20from%20the%20source%20and%20one%20drawn%20from%20the%20target%0Adistribution%20randomly.%20This%20leads%20to%20%27%27ground-truth%27%27%20velocity%20vector-fields%0Athat%20point%20in%20different%20directions%20at%20the%20same%20location%2C%20i.e.%2C%20the%20velocity%0Avector-fields%20are%20multi-modal/ambiguous.%20However%2C%20since%20training%20uses%20a%0Astandard%20mean-squared-error%20loss%2C%20the%20learnt%20velocity%20vector-field%20averages%0A%27%27ground-truth%27%27%20directions%20and%20isn%27t%20multi-modal.%20In%20contrast%2C%20variational%0Arectified%20flow%20matching%20learns%20and%20samples%20from%20multi-modal%20flow%20directions.%20We%0Ashow%20on%20synthetic%20data%2C%20MNIST%2C%20CIFAR-10%2C%20and%20ImageNet%20that%20variational%0Arectified%20flow%20matching%20leads%20to%20compelling%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Rectified%2520Flow%2520Matching%26entry.906535625%3DPengsheng%2520Guo%2520and%2520Alexander%2520G.%2520Schwing%26entry.1292438233%3D%2520%2520We%2520study%2520Variational%2520Rectified%2520Flow%2520Matching%252C%2520a%2520framework%2520that%2520enhances%250Aclassic%2520rectified%2520flow%2520matching%2520by%2520modeling%2520multi-modal%2520velocity%2520vector-fields.%250AAt%2520inference%2520time%252C%2520classic%2520rectified%2520flow%2520matching%2520%2527moves%2527%2520samples%2520from%2520a%250Asource%2520distribution%2520to%2520the%2520target%2520distribution%2520by%2520solving%2520an%2520ordinary%250Adifferential%2520equation%2520via%2520integration%2520along%2520a%2520velocity%2520vector-field.%2520At%250Atraining%2520time%252C%2520the%2520velocity%2520vector-field%2520is%2520learnt%2520by%2520linearly%2520interpolating%250Abetween%2520coupled%2520samples%2520one%2520drawn%2520from%2520the%2520source%2520and%2520one%2520drawn%2520from%2520the%2520target%250Adistribution%2520randomly.%2520This%2520leads%2520to%2520%2527%2527ground-truth%2527%2527%2520velocity%2520vector-fields%250Athat%2520point%2520in%2520different%2520directions%2520at%2520the%2520same%2520location%252C%2520i.e.%252C%2520the%2520velocity%250Avector-fields%2520are%2520multi-modal/ambiguous.%2520However%252C%2520since%2520training%2520uses%2520a%250Astandard%2520mean-squared-error%2520loss%252C%2520the%2520learnt%2520velocity%2520vector-field%2520averages%250A%2527%2527ground-truth%2527%2527%2520directions%2520and%2520isn%2527t%2520multi-modal.%2520In%2520contrast%252C%2520variational%250Arectified%2520flow%2520matching%2520learns%2520and%2520samples%2520from%2520multi-modal%2520flow%2520directions.%2520We%250Ashow%2520on%2520synthetic%2520data%252C%2520MNIST%252C%2520CIFAR-10%252C%2520and%2520ImageNet%2520that%2520variational%250Arectified%2520flow%2520matching%2520leads%2520to%2520compelling%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Rectified%20Flow%20Matching&entry.906535625=Pengsheng%20Guo%20and%20Alexander%20G.%20Schwing&entry.1292438233=%20%20We%20study%20Variational%20Rectified%20Flow%20Matching%2C%20a%20framework%20that%20enhances%0Aclassic%20rectified%20flow%20matching%20by%20modeling%20multi-modal%20velocity%20vector-fields.%0AAt%20inference%20time%2C%20classic%20rectified%20flow%20matching%20%27moves%27%20samples%20from%20a%0Asource%20distribution%20to%20the%20target%20distribution%20by%20solving%20an%20ordinary%0Adifferential%20equation%20via%20integration%20along%20a%20velocity%20vector-field.%20At%0Atraining%20time%2C%20the%20velocity%20vector-field%20is%20learnt%20by%20linearly%20interpolating%0Abetween%20coupled%20samples%20one%20drawn%20from%20the%20source%20and%20one%20drawn%20from%20the%20target%0Adistribution%20randomly.%20This%20leads%20to%20%27%27ground-truth%27%27%20velocity%20vector-fields%0Athat%20point%20in%20different%20directions%20at%20the%20same%20location%2C%20i.e.%2C%20the%20velocity%0Avector-fields%20are%20multi-modal/ambiguous.%20However%2C%20since%20training%20uses%20a%0Astandard%20mean-squared-error%20loss%2C%20the%20learnt%20velocity%20vector-field%20averages%0A%27%27ground-truth%27%27%20directions%20and%20isn%27t%20multi-modal.%20In%20contrast%2C%20variational%0Arectified%20flow%20matching%20learns%20and%20samples%20from%20multi-modal%20flow%20directions.%20We%0Ashow%20on%20synthetic%20data%2C%20MNIST%2C%20CIFAR-10%2C%20and%20ImageNet%20that%20variational%0Arectified%20flow%20matching%20leads%20to%20compelling%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09616v1&entry.124074799=Read"},
{"title": "Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep\n  Graph Networks", "author": "Simon Heilig and Alessio Gravina and Alessandro Trenta and Claudio Gallicchio and Davide Bacciu", "abstract": "  The dynamics of information diffusion within graphs is a critical open issue\nthat heavily influences graph representation learning, especially when\nconsidering long-range propagation. This calls for principled approaches that\ncontrol and regulate the degree of propagation and dissipation of information\nthroughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian\nDeep Graph Networks, a novel framework that models neural information flow in\ngraphs by building on the laws of conservation of Hamiltonian dynamical\nsystems. We reconcile under a single theoretical and practical framework both\nnon-dissipative long-range propagation and non-conservative behaviors,\nintroducing tools from mechanical systems to gauge the equilibrium between the\ntwo components. Our approach can be applied to general message-passing\narchitectures, and it provides theoretical guarantees on information\nconservation in time. Empirical results prove the effectiveness of our\nport-Hamiltonian scheme in pushing simple graph convolutional architectures to\nstate-of-the-art performance in long-range benchmarks.\n", "link": "http://arxiv.org/abs/2405.17163v2", "date": "2025-02-13", "relevancy": 1.5779, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5513}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.519}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Port-Hamiltonian%20Architectural%20Bias%20for%20Long-Range%20Propagation%20in%20Deep%0A%20%20Graph%20Networks&body=Title%3A%20Port-Hamiltonian%20Architectural%20Bias%20for%20Long-Range%20Propagation%20in%20Deep%0A%20%20Graph%20Networks%0AAuthor%3A%20Simon%20Heilig%20and%20Alessio%20Gravina%20and%20Alessandro%20Trenta%20and%20Claudio%20Gallicchio%20and%20Davide%20Bacciu%0AAbstract%3A%20%20%20The%20dynamics%20of%20information%20diffusion%20within%20graphs%20is%20a%20critical%20open%20issue%0Athat%20heavily%20influences%20graph%20representation%20learning%2C%20especially%20when%0Aconsidering%20long-range%20propagation.%20This%20calls%20for%20principled%20approaches%20that%0Acontrol%20and%20regulate%20the%20degree%20of%20propagation%20and%20dissipation%20of%20information%0Athroughout%20the%20neural%20flow.%20Motivated%20by%20this%2C%20we%20introduce%20%28port-%29Hamiltonian%0ADeep%20Graph%20Networks%2C%20a%20novel%20framework%20that%20models%20neural%20information%20flow%20in%0Agraphs%20by%20building%20on%20the%20laws%20of%20conservation%20of%20Hamiltonian%20dynamical%0Asystems.%20We%20reconcile%20under%20a%20single%20theoretical%20and%20practical%20framework%20both%0Anon-dissipative%20long-range%20propagation%20and%20non-conservative%20behaviors%2C%0Aintroducing%20tools%20from%20mechanical%20systems%20to%20gauge%20the%20equilibrium%20between%20the%0Atwo%20components.%20Our%20approach%20can%20be%20applied%20to%20general%20message-passing%0Aarchitectures%2C%20and%20it%20provides%20theoretical%20guarantees%20on%20information%0Aconservation%20in%20time.%20Empirical%20results%20prove%20the%20effectiveness%20of%20our%0Aport-Hamiltonian%20scheme%20in%20pushing%20simple%20graph%20convolutional%20architectures%20to%0Astate-of-the-art%20performance%20in%20long-range%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17163v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPort-Hamiltonian%2520Architectural%2520Bias%2520for%2520Long-Range%2520Propagation%2520in%2520Deep%250A%2520%2520Graph%2520Networks%26entry.906535625%3DSimon%2520Heilig%2520and%2520Alessio%2520Gravina%2520and%2520Alessandro%2520Trenta%2520and%2520Claudio%2520Gallicchio%2520and%2520Davide%2520Bacciu%26entry.1292438233%3D%2520%2520The%2520dynamics%2520of%2520information%2520diffusion%2520within%2520graphs%2520is%2520a%2520critical%2520open%2520issue%250Athat%2520heavily%2520influences%2520graph%2520representation%2520learning%252C%2520especially%2520when%250Aconsidering%2520long-range%2520propagation.%2520This%2520calls%2520for%2520principled%2520approaches%2520that%250Acontrol%2520and%2520regulate%2520the%2520degree%2520of%2520propagation%2520and%2520dissipation%2520of%2520information%250Athroughout%2520the%2520neural%2520flow.%2520Motivated%2520by%2520this%252C%2520we%2520introduce%2520%2528port-%2529Hamiltonian%250ADeep%2520Graph%2520Networks%252C%2520a%2520novel%2520framework%2520that%2520models%2520neural%2520information%2520flow%2520in%250Agraphs%2520by%2520building%2520on%2520the%2520laws%2520of%2520conservation%2520of%2520Hamiltonian%2520dynamical%250Asystems.%2520We%2520reconcile%2520under%2520a%2520single%2520theoretical%2520and%2520practical%2520framework%2520both%250Anon-dissipative%2520long-range%2520propagation%2520and%2520non-conservative%2520behaviors%252C%250Aintroducing%2520tools%2520from%2520mechanical%2520systems%2520to%2520gauge%2520the%2520equilibrium%2520between%2520the%250Atwo%2520components.%2520Our%2520approach%2520can%2520be%2520applied%2520to%2520general%2520message-passing%250Aarchitectures%252C%2520and%2520it%2520provides%2520theoretical%2520guarantees%2520on%2520information%250Aconservation%2520in%2520time.%2520Empirical%2520results%2520prove%2520the%2520effectiveness%2520of%2520our%250Aport-Hamiltonian%2520scheme%2520in%2520pushing%2520simple%2520graph%2520convolutional%2520architectures%2520to%250Astate-of-the-art%2520performance%2520in%2520long-range%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17163v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Port-Hamiltonian%20Architectural%20Bias%20for%20Long-Range%20Propagation%20in%20Deep%0A%20%20Graph%20Networks&entry.906535625=Simon%20Heilig%20and%20Alessio%20Gravina%20and%20Alessandro%20Trenta%20and%20Claudio%20Gallicchio%20and%20Davide%20Bacciu&entry.1292438233=%20%20The%20dynamics%20of%20information%20diffusion%20within%20graphs%20is%20a%20critical%20open%20issue%0Athat%20heavily%20influences%20graph%20representation%20learning%2C%20especially%20when%0Aconsidering%20long-range%20propagation.%20This%20calls%20for%20principled%20approaches%20that%0Acontrol%20and%20regulate%20the%20degree%20of%20propagation%20and%20dissipation%20of%20information%0Athroughout%20the%20neural%20flow.%20Motivated%20by%20this%2C%20we%20introduce%20%28port-%29Hamiltonian%0ADeep%20Graph%20Networks%2C%20a%20novel%20framework%20that%20models%20neural%20information%20flow%20in%0Agraphs%20by%20building%20on%20the%20laws%20of%20conservation%20of%20Hamiltonian%20dynamical%0Asystems.%20We%20reconcile%20under%20a%20single%20theoretical%20and%20practical%20framework%20both%0Anon-dissipative%20long-range%20propagation%20and%20non-conservative%20behaviors%2C%0Aintroducing%20tools%20from%20mechanical%20systems%20to%20gauge%20the%20equilibrium%20between%20the%0Atwo%20components.%20Our%20approach%20can%20be%20applied%20to%20general%20message-passing%0Aarchitectures%2C%20and%20it%20provides%20theoretical%20guarantees%20on%20information%0Aconservation%20in%20time.%20Empirical%20results%20prove%20the%20effectiveness%20of%20our%0Aport-Hamiltonian%20scheme%20in%20pushing%20simple%20graph%20convolutional%20architectures%20to%0Astate-of-the-art%20performance%20in%20long-range%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17163v2&entry.124074799=Read"},
{"title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization", "author": "Francesco Pezone and Sergio Barbarossa and Giuseppe Caire", "abstract": "  This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approach\nintegrating generative models to optimize image compression for\nsemantic/task-oriented communications. SQ-GAN employs off-the-shelf semantic\nsemantic segmentation and a new specifically developed semantic-conditioned\nadaptive mask module (SAMM) to selectively encode semantically significant\nfeatures of the images. SQ-GAN outperforms state-of-the-art image compression\nschemes such as JPEG2000 and BPG across multiple metrics, including perceptual\nquality and semantic segmentation accuracy on the post-decoding reconstructed\nimage, at extreme low compression rates expressed in bits per pixel.\n", "link": "http://arxiv.org/abs/2502.09520v1", "date": "2025-02-13", "relevancy": 1.5748, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5549}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5184}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SQ-GAN%3A%20Semantic%20Image%20Communications%20Using%20Masked%20Vector%20Quantization&body=Title%3A%20SQ-GAN%3A%20Semantic%20Image%20Communications%20Using%20Masked%20Vector%20Quantization%0AAuthor%3A%20Francesco%20Pezone%20and%20Sergio%20Barbarossa%20and%20Giuseppe%20Caire%0AAbstract%3A%20%20%20This%20work%20introduces%20Semantically%20Masked%20VQ-GAN%20%28SQ-GAN%29%2C%20a%20novel%20approach%0Aintegrating%20generative%20models%20to%20optimize%20image%20compression%20for%0Asemantic/task-oriented%20communications.%20SQ-GAN%20employs%20off-the-shelf%20semantic%0Asemantic%20segmentation%20and%20a%20new%20specifically%20developed%20semantic-conditioned%0Aadaptive%20mask%20module%20%28SAMM%29%20to%20selectively%20encode%20semantically%20significant%0Afeatures%20of%20the%20images.%20SQ-GAN%20outperforms%20state-of-the-art%20image%20compression%0Aschemes%20such%20as%20JPEG2000%20and%20BPG%20across%20multiple%20metrics%2C%20including%20perceptual%0Aquality%20and%20semantic%20segmentation%20accuracy%20on%20the%20post-decoding%20reconstructed%0Aimage%2C%20at%20extreme%20low%20compression%20rates%20expressed%20in%20bits%20per%20pixel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSQ-GAN%253A%2520Semantic%2520Image%2520Communications%2520Using%2520Masked%2520Vector%2520Quantization%26entry.906535625%3DFrancesco%2520Pezone%2520and%2520Sergio%2520Barbarossa%2520and%2520Giuseppe%2520Caire%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520Semantically%2520Masked%2520VQ-GAN%2520%2528SQ-GAN%2529%252C%2520a%2520novel%2520approach%250Aintegrating%2520generative%2520models%2520to%2520optimize%2520image%2520compression%2520for%250Asemantic/task-oriented%2520communications.%2520SQ-GAN%2520employs%2520off-the-shelf%2520semantic%250Asemantic%2520segmentation%2520and%2520a%2520new%2520specifically%2520developed%2520semantic-conditioned%250Aadaptive%2520mask%2520module%2520%2528SAMM%2529%2520to%2520selectively%2520encode%2520semantically%2520significant%250Afeatures%2520of%2520the%2520images.%2520SQ-GAN%2520outperforms%2520state-of-the-art%2520image%2520compression%250Aschemes%2520such%2520as%2520JPEG2000%2520and%2520BPG%2520across%2520multiple%2520metrics%252C%2520including%2520perceptual%250Aquality%2520and%2520semantic%2520segmentation%2520accuracy%2520on%2520the%2520post-decoding%2520reconstructed%250Aimage%252C%2520at%2520extreme%2520low%2520compression%2520rates%2520expressed%2520in%2520bits%2520per%2520pixel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SQ-GAN%3A%20Semantic%20Image%20Communications%20Using%20Masked%20Vector%20Quantization&entry.906535625=Francesco%20Pezone%20and%20Sergio%20Barbarossa%20and%20Giuseppe%20Caire&entry.1292438233=%20%20This%20work%20introduces%20Semantically%20Masked%20VQ-GAN%20%28SQ-GAN%29%2C%20a%20novel%20approach%0Aintegrating%20generative%20models%20to%20optimize%20image%20compression%20for%0Asemantic/task-oriented%20communications.%20SQ-GAN%20employs%20off-the-shelf%20semantic%0Asemantic%20segmentation%20and%20a%20new%20specifically%20developed%20semantic-conditioned%0Aadaptive%20mask%20module%20%28SAMM%29%20to%20selectively%20encode%20semantically%20significant%0Afeatures%20of%20the%20images.%20SQ-GAN%20outperforms%20state-of-the-art%20image%20compression%0Aschemes%20such%20as%20JPEG2000%20and%20BPG%20across%20multiple%20metrics%2C%20including%20perceptual%0Aquality%20and%20semantic%20segmentation%20accuracy%20on%20the%20post-decoding%20reconstructed%0Aimage%2C%20at%20extreme%20low%20compression%20rates%20expressed%20in%20bits%20per%20pixel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09520v1&entry.124074799=Read"},
{"title": "DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra", "author": "Montgomery Bohde and Mrunali Manjrekar and Runzhong Wang and Shuiwang Ji and Connor W. Coley", "abstract": "  Mass spectrometry plays a fundamental role in elucidating the structures of\nunknown molecules and subsequent scientific discoveries. One formulation of the\nstructure elucidation task is the conditional $\\textit{de novo}$ generation of\nmolecular structure given a mass spectrum. Toward a more accurate and efficient\nscientific discovery pipeline for small molecules, we present DiffMS, a\nformula-restricted encoder-decoder generative network that achieves\nstate-of-the-art performance on this task. The encoder utilizes a transformer\narchitecture and models mass spectra domain knowledge such as peak formulae and\nneutral losses, and the decoder is a discrete graph diffusion model restricted\nby the heavy-atom composition of a known chemical formula. To develop a robust\ndecoder that bridges latent embeddings and molecular structures, we pretrain\nthe diffusion decoder with fingerprint-structure pairs, which are available in\nvirtually infinite quantities, compared to structure-spectrum pairs that number\nin the tens of thousands. Extensive experiments on established benchmarks show\nthat DiffMS outperforms existing models on $\\textit{de novo}$ molecule\ngeneration. We provide several ablations to demonstrate the effectiveness of\nour diffusion and pretraining approaches and show consistent performance\nscaling with increasing pretraining dataset size. DiffMS code is publicly\navailable at https://github.com/coleygroup/DiffMS.\n", "link": "http://arxiv.org/abs/2502.09571v1", "date": "2025-02-13", "relevancy": 1.5681, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5742}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5171}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffMS%3A%20Diffusion%20Generation%20of%20Molecules%20Conditioned%20on%20Mass%20Spectra&body=Title%3A%20DiffMS%3A%20Diffusion%20Generation%20of%20Molecules%20Conditioned%20on%20Mass%20Spectra%0AAuthor%3A%20Montgomery%20Bohde%20and%20Mrunali%20Manjrekar%20and%20Runzhong%20Wang%20and%20Shuiwang%20Ji%20and%20Connor%20W.%20Coley%0AAbstract%3A%20%20%20Mass%20spectrometry%20plays%20a%20fundamental%20role%20in%20elucidating%20the%20structures%20of%0Aunknown%20molecules%20and%20subsequent%20scientific%20discoveries.%20One%20formulation%20of%20the%0Astructure%20elucidation%20task%20is%20the%20conditional%20%24%5Ctextit%7Bde%20novo%7D%24%20generation%20of%0Amolecular%20structure%20given%20a%20mass%20spectrum.%20Toward%20a%20more%20accurate%20and%20efficient%0Ascientific%20discovery%20pipeline%20for%20small%20molecules%2C%20we%20present%20DiffMS%2C%20a%0Aformula-restricted%20encoder-decoder%20generative%20network%20that%20achieves%0Astate-of-the-art%20performance%20on%20this%20task.%20The%20encoder%20utilizes%20a%20transformer%0Aarchitecture%20and%20models%20mass%20spectra%20domain%20knowledge%20such%20as%20peak%20formulae%20and%0Aneutral%20losses%2C%20and%20the%20decoder%20is%20a%20discrete%20graph%20diffusion%20model%20restricted%0Aby%20the%20heavy-atom%20composition%20of%20a%20known%20chemical%20formula.%20To%20develop%20a%20robust%0Adecoder%20that%20bridges%20latent%20embeddings%20and%20molecular%20structures%2C%20we%20pretrain%0Athe%20diffusion%20decoder%20with%20fingerprint-structure%20pairs%2C%20which%20are%20available%20in%0Avirtually%20infinite%20quantities%2C%20compared%20to%20structure-spectrum%20pairs%20that%20number%0Ain%20the%20tens%20of%20thousands.%20Extensive%20experiments%20on%20established%20benchmarks%20show%0Athat%20DiffMS%20outperforms%20existing%20models%20on%20%24%5Ctextit%7Bde%20novo%7D%24%20molecule%0Ageneration.%20We%20provide%20several%20ablations%20to%20demonstrate%20the%20effectiveness%20of%0Aour%20diffusion%20and%20pretraining%20approaches%20and%20show%20consistent%20performance%0Ascaling%20with%20increasing%20pretraining%20dataset%20size.%20DiffMS%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/coleygroup/DiffMS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffMS%253A%2520Diffusion%2520Generation%2520of%2520Molecules%2520Conditioned%2520on%2520Mass%2520Spectra%26entry.906535625%3DMontgomery%2520Bohde%2520and%2520Mrunali%2520Manjrekar%2520and%2520Runzhong%2520Wang%2520and%2520Shuiwang%2520Ji%2520and%2520Connor%2520W.%2520Coley%26entry.1292438233%3D%2520%2520Mass%2520spectrometry%2520plays%2520a%2520fundamental%2520role%2520in%2520elucidating%2520the%2520structures%2520of%250Aunknown%2520molecules%2520and%2520subsequent%2520scientific%2520discoveries.%2520One%2520formulation%2520of%2520the%250Astructure%2520elucidation%2520task%2520is%2520the%2520conditional%2520%2524%255Ctextit%257Bde%2520novo%257D%2524%2520generation%2520of%250Amolecular%2520structure%2520given%2520a%2520mass%2520spectrum.%2520Toward%2520a%2520more%2520accurate%2520and%2520efficient%250Ascientific%2520discovery%2520pipeline%2520for%2520small%2520molecules%252C%2520we%2520present%2520DiffMS%252C%2520a%250Aformula-restricted%2520encoder-decoder%2520generative%2520network%2520that%2520achieves%250Astate-of-the-art%2520performance%2520on%2520this%2520task.%2520The%2520encoder%2520utilizes%2520a%2520transformer%250Aarchitecture%2520and%2520models%2520mass%2520spectra%2520domain%2520knowledge%2520such%2520as%2520peak%2520formulae%2520and%250Aneutral%2520losses%252C%2520and%2520the%2520decoder%2520is%2520a%2520discrete%2520graph%2520diffusion%2520model%2520restricted%250Aby%2520the%2520heavy-atom%2520composition%2520of%2520a%2520known%2520chemical%2520formula.%2520To%2520develop%2520a%2520robust%250Adecoder%2520that%2520bridges%2520latent%2520embeddings%2520and%2520molecular%2520structures%252C%2520we%2520pretrain%250Athe%2520diffusion%2520decoder%2520with%2520fingerprint-structure%2520pairs%252C%2520which%2520are%2520available%2520in%250Avirtually%2520infinite%2520quantities%252C%2520compared%2520to%2520structure-spectrum%2520pairs%2520that%2520number%250Ain%2520the%2520tens%2520of%2520thousands.%2520Extensive%2520experiments%2520on%2520established%2520benchmarks%2520show%250Athat%2520DiffMS%2520outperforms%2520existing%2520models%2520on%2520%2524%255Ctextit%257Bde%2520novo%257D%2524%2520molecule%250Ageneration.%2520We%2520provide%2520several%2520ablations%2520to%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520diffusion%2520and%2520pretraining%2520approaches%2520and%2520show%2520consistent%2520performance%250Ascaling%2520with%2520increasing%2520pretraining%2520dataset%2520size.%2520DiffMS%2520code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/coleygroup/DiffMS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffMS%3A%20Diffusion%20Generation%20of%20Molecules%20Conditioned%20on%20Mass%20Spectra&entry.906535625=Montgomery%20Bohde%20and%20Mrunali%20Manjrekar%20and%20Runzhong%20Wang%20and%20Shuiwang%20Ji%20and%20Connor%20W.%20Coley&entry.1292438233=%20%20Mass%20spectrometry%20plays%20a%20fundamental%20role%20in%20elucidating%20the%20structures%20of%0Aunknown%20molecules%20and%20subsequent%20scientific%20discoveries.%20One%20formulation%20of%20the%0Astructure%20elucidation%20task%20is%20the%20conditional%20%24%5Ctextit%7Bde%20novo%7D%24%20generation%20of%0Amolecular%20structure%20given%20a%20mass%20spectrum.%20Toward%20a%20more%20accurate%20and%20efficient%0Ascientific%20discovery%20pipeline%20for%20small%20molecules%2C%20we%20present%20DiffMS%2C%20a%0Aformula-restricted%20encoder-decoder%20generative%20network%20that%20achieves%0Astate-of-the-art%20performance%20on%20this%20task.%20The%20encoder%20utilizes%20a%20transformer%0Aarchitecture%20and%20models%20mass%20spectra%20domain%20knowledge%20such%20as%20peak%20formulae%20and%0Aneutral%20losses%2C%20and%20the%20decoder%20is%20a%20discrete%20graph%20diffusion%20model%20restricted%0Aby%20the%20heavy-atom%20composition%20of%20a%20known%20chemical%20formula.%20To%20develop%20a%20robust%0Adecoder%20that%20bridges%20latent%20embeddings%20and%20molecular%20structures%2C%20we%20pretrain%0Athe%20diffusion%20decoder%20with%20fingerprint-structure%20pairs%2C%20which%20are%20available%20in%0Avirtually%20infinite%20quantities%2C%20compared%20to%20structure-spectrum%20pairs%20that%20number%0Ain%20the%20tens%20of%20thousands.%20Extensive%20experiments%20on%20established%20benchmarks%20show%0Athat%20DiffMS%20outperforms%20existing%20models%20on%20%24%5Ctextit%7Bde%20novo%7D%24%20molecule%0Ageneration.%20We%20provide%20several%20ablations%20to%20demonstrate%20the%20effectiveness%20of%0Aour%20diffusion%20and%20pretraining%20approaches%20and%20show%20consistent%20performance%0Ascaling%20with%20increasing%20pretraining%20dataset%20size.%20DiffMS%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/coleygroup/DiffMS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09571v1&entry.124074799=Read"},
{"title": "Score-of-Mixture Training: Training One-Step Generative Models Made\n  Simple", "author": "Tejas Jayashankar and J. Jon Ryu and Gregory Wornell", "abstract": "  We propose Score-of-Mixture Training (SMT), a novel framework for training\none-step generative models by minimizing a class of divergences called the\n$\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score\nof mixture distributions between real and fake samples across multiple noise\nlevels. Similar to consistency models, our approach supports both training from\nscratch (SMT) and distillation using a pretrained diffusion model, which we\ncall Score-of-Mixture Distillation (SMD). It is simple to implement, requires\nminimal hyperparameter tuning, and ensures stable training. Experiments on\nCIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even\noutperform existing methods.\n", "link": "http://arxiv.org/abs/2502.09609v1", "date": "2025-02-13", "relevancy": 1.5613, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5403}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5156}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Score-of-Mixture%20Training%3A%20Training%20One-Step%20Generative%20Models%20Made%0A%20%20Simple&body=Title%3A%20Score-of-Mixture%20Training%3A%20Training%20One-Step%20Generative%20Models%20Made%0A%20%20Simple%0AAuthor%3A%20Tejas%20Jayashankar%20and%20J.%20Jon%20Ryu%20and%20Gregory%20Wornell%0AAbstract%3A%20%20%20We%20propose%20Score-of-Mixture%20Training%20%28SMT%29%2C%20a%20novel%20framework%20for%20training%0Aone-step%20generative%20models%20by%20minimizing%20a%20class%20of%20divergences%20called%20the%0A%24%5Calpha%24-skew%20Jensen-Shannon%20divergence.%20At%20its%20core%2C%20SMT%20estimates%20the%20score%0Aof%20mixture%20distributions%20between%20real%20and%20fake%20samples%20across%20multiple%20noise%0Alevels.%20Similar%20to%20consistency%20models%2C%20our%20approach%20supports%20both%20training%20from%0Ascratch%20%28SMT%29%20and%20distillation%20using%20a%20pretrained%20diffusion%20model%2C%20which%20we%0Acall%20Score-of-Mixture%20Distillation%20%28SMD%29.%20It%20is%20simple%20to%20implement%2C%20requires%0Aminimal%20hyperparameter%20tuning%2C%20and%20ensures%20stable%20training.%20Experiments%20on%0ACIFAR-10%20and%20ImageNet%2064x64%20show%20that%20SMT/SMD%20are%20competitive%20with%20and%20can%20even%0Aoutperform%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScore-of-Mixture%2520Training%253A%2520Training%2520One-Step%2520Generative%2520Models%2520Made%250A%2520%2520Simple%26entry.906535625%3DTejas%2520Jayashankar%2520and%2520J.%2520Jon%2520Ryu%2520and%2520Gregory%2520Wornell%26entry.1292438233%3D%2520%2520We%2520propose%2520Score-of-Mixture%2520Training%2520%2528SMT%2529%252C%2520a%2520novel%2520framework%2520for%2520training%250Aone-step%2520generative%2520models%2520by%2520minimizing%2520a%2520class%2520of%2520divergences%2520called%2520the%250A%2524%255Calpha%2524-skew%2520Jensen-Shannon%2520divergence.%2520At%2520its%2520core%252C%2520SMT%2520estimates%2520the%2520score%250Aof%2520mixture%2520distributions%2520between%2520real%2520and%2520fake%2520samples%2520across%2520multiple%2520noise%250Alevels.%2520Similar%2520to%2520consistency%2520models%252C%2520our%2520approach%2520supports%2520both%2520training%2520from%250Ascratch%2520%2528SMT%2529%2520and%2520distillation%2520using%2520a%2520pretrained%2520diffusion%2520model%252C%2520which%2520we%250Acall%2520Score-of-Mixture%2520Distillation%2520%2528SMD%2529.%2520It%2520is%2520simple%2520to%2520implement%252C%2520requires%250Aminimal%2520hyperparameter%2520tuning%252C%2520and%2520ensures%2520stable%2520training.%2520Experiments%2520on%250ACIFAR-10%2520and%2520ImageNet%252064x64%2520show%2520that%2520SMT/SMD%2520are%2520competitive%2520with%2520and%2520can%2520even%250Aoutperform%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Score-of-Mixture%20Training%3A%20Training%20One-Step%20Generative%20Models%20Made%0A%20%20Simple&entry.906535625=Tejas%20Jayashankar%20and%20J.%20Jon%20Ryu%20and%20Gregory%20Wornell&entry.1292438233=%20%20We%20propose%20Score-of-Mixture%20Training%20%28SMT%29%2C%20a%20novel%20framework%20for%20training%0Aone-step%20generative%20models%20by%20minimizing%20a%20class%20of%20divergences%20called%20the%0A%24%5Calpha%24-skew%20Jensen-Shannon%20divergence.%20At%20its%20core%2C%20SMT%20estimates%20the%20score%0Aof%20mixture%20distributions%20between%20real%20and%20fake%20samples%20across%20multiple%20noise%0Alevels.%20Similar%20to%20consistency%20models%2C%20our%20approach%20supports%20both%20training%20from%0Ascratch%20%28SMT%29%20and%20distillation%20using%20a%20pretrained%20diffusion%20model%2C%20which%20we%0Acall%20Score-of-Mixture%20Distillation%20%28SMD%29.%20It%20is%20simple%20to%20implement%2C%20requires%0Aminimal%20hyperparameter%20tuning%2C%20and%20ensures%20stable%20training.%20Experiments%20on%0ACIFAR-10%20and%20ImageNet%2064x64%20show%20that%20SMT/SMD%20are%20competitive%20with%20and%20can%20even%0Aoutperform%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09609v1&entry.124074799=Read"},
{"title": "Scenario-based assessment of automated driving systems: How (not) to\n  parameterize scenarios?", "author": "Erwin de Gelder and Olaf Op den Camp", "abstract": "  The development of Automated Driving Systems (ADSs) has advanced\nsignificantly. To enable their large-scale deployment, the United Nations\nRegulation 157 (UN R157) concerning the approval of Automated Lane Keeping\nSystems (ALKSs) has been approved in 2021. UN R157 requires an activated ALKS\nto avoid any collisions that are reasonably preventable and proposes a method\nto distinguish reasonably preventable collisions from unpreventable ones using\n\"the simulated performance of a skilled and attentive human driver\". With\ndifferent driver models, benchmarks are set for ALKSs in three types of\nscenarios. The three types of scenarios considered in the proposed method in UN\nR157 assume a certain parameterization without any further consideration.\n  This work investigates the parameterization of these scenarios, showing that\nthe choice of parameterization significantly affects the simulation outcomes.\nBy comparing real-world and parameterized scenarios, we show that the influence\nof parameterization depends on the scenario type, driver model, and evaluation\ncriterion. Alternative parameterizations are proposed, leading to results that\nare closer to the non-parameterized scenarios in terms of recall, precision,\nand F1 score. The study highlights the importance of careful scenario\nparameterization and suggests improvements to the current UN R157 approach.\n", "link": "http://arxiv.org/abs/2409.01117v2", "date": "2025-02-13", "relevancy": 1.4362, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5069}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scenario-based%20assessment%20of%20automated%20driving%20systems%3A%20How%20%28not%29%20to%0A%20%20parameterize%20scenarios%3F&body=Title%3A%20Scenario-based%20assessment%20of%20automated%20driving%20systems%3A%20How%20%28not%29%20to%0A%20%20parameterize%20scenarios%3F%0AAuthor%3A%20Erwin%20de%20Gelder%20and%20Olaf%20Op%20den%20Camp%0AAbstract%3A%20%20%20The%20development%20of%20Automated%20Driving%20Systems%20%28ADSs%29%20has%20advanced%0Asignificantly.%20To%20enable%20their%20large-scale%20deployment%2C%20the%20United%20Nations%0ARegulation%20157%20%28UN%20R157%29%20concerning%20the%20approval%20of%20Automated%20Lane%20Keeping%0ASystems%20%28ALKSs%29%20has%20been%20approved%20in%202021.%20UN%20R157%20requires%20an%20activated%20ALKS%0Ato%20avoid%20any%20collisions%20that%20are%20reasonably%20preventable%20and%20proposes%20a%20method%0Ato%20distinguish%20reasonably%20preventable%20collisions%20from%20unpreventable%20ones%20using%0A%22the%20simulated%20performance%20of%20a%20skilled%20and%20attentive%20human%20driver%22.%20With%0Adifferent%20driver%20models%2C%20benchmarks%20are%20set%20for%20ALKSs%20in%20three%20types%20of%0Ascenarios.%20The%20three%20types%20of%20scenarios%20considered%20in%20the%20proposed%20method%20in%20UN%0AR157%20assume%20a%20certain%20parameterization%20without%20any%20further%20consideration.%0A%20%20This%20work%20investigates%20the%20parameterization%20of%20these%20scenarios%2C%20showing%20that%0Athe%20choice%20of%20parameterization%20significantly%20affects%20the%20simulation%20outcomes.%0ABy%20comparing%20real-world%20and%20parameterized%20scenarios%2C%20we%20show%20that%20the%20influence%0Aof%20parameterization%20depends%20on%20the%20scenario%20type%2C%20driver%20model%2C%20and%20evaluation%0Acriterion.%20Alternative%20parameterizations%20are%20proposed%2C%20leading%20to%20results%20that%0Aare%20closer%20to%20the%20non-parameterized%20scenarios%20in%20terms%20of%20recall%2C%20precision%2C%0Aand%20F1%20score.%20The%20study%20highlights%20the%20importance%20of%20careful%20scenario%0Aparameterization%20and%20suggests%20improvements%20to%20the%20current%20UN%20R157%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01117v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScenario-based%2520assessment%2520of%2520automated%2520driving%2520systems%253A%2520How%2520%2528not%2529%2520to%250A%2520%2520parameterize%2520scenarios%253F%26entry.906535625%3DErwin%2520de%2520Gelder%2520and%2520Olaf%2520Op%2520den%2520Camp%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520Automated%2520Driving%2520Systems%2520%2528ADSs%2529%2520has%2520advanced%250Asignificantly.%2520To%2520enable%2520their%2520large-scale%2520deployment%252C%2520the%2520United%2520Nations%250ARegulation%2520157%2520%2528UN%2520R157%2529%2520concerning%2520the%2520approval%2520of%2520Automated%2520Lane%2520Keeping%250ASystems%2520%2528ALKSs%2529%2520has%2520been%2520approved%2520in%25202021.%2520UN%2520R157%2520requires%2520an%2520activated%2520ALKS%250Ato%2520avoid%2520any%2520collisions%2520that%2520are%2520reasonably%2520preventable%2520and%2520proposes%2520a%2520method%250Ato%2520distinguish%2520reasonably%2520preventable%2520collisions%2520from%2520unpreventable%2520ones%2520using%250A%2522the%2520simulated%2520performance%2520of%2520a%2520skilled%2520and%2520attentive%2520human%2520driver%2522.%2520With%250Adifferent%2520driver%2520models%252C%2520benchmarks%2520are%2520set%2520for%2520ALKSs%2520in%2520three%2520types%2520of%250Ascenarios.%2520The%2520three%2520types%2520of%2520scenarios%2520considered%2520in%2520the%2520proposed%2520method%2520in%2520UN%250AR157%2520assume%2520a%2520certain%2520parameterization%2520without%2520any%2520further%2520consideration.%250A%2520%2520This%2520work%2520investigates%2520the%2520parameterization%2520of%2520these%2520scenarios%252C%2520showing%2520that%250Athe%2520choice%2520of%2520parameterization%2520significantly%2520affects%2520the%2520simulation%2520outcomes.%250ABy%2520comparing%2520real-world%2520and%2520parameterized%2520scenarios%252C%2520we%2520show%2520that%2520the%2520influence%250Aof%2520parameterization%2520depends%2520on%2520the%2520scenario%2520type%252C%2520driver%2520model%252C%2520and%2520evaluation%250Acriterion.%2520Alternative%2520parameterizations%2520are%2520proposed%252C%2520leading%2520to%2520results%2520that%250Aare%2520closer%2520to%2520the%2520non-parameterized%2520scenarios%2520in%2520terms%2520of%2520recall%252C%2520precision%252C%250Aand%2520F1%2520score.%2520The%2520study%2520highlights%2520the%2520importance%2520of%2520careful%2520scenario%250Aparameterization%2520and%2520suggests%2520improvements%2520to%2520the%2520current%2520UN%2520R157%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01117v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scenario-based%20assessment%20of%20automated%20driving%20systems%3A%20How%20%28not%29%20to%0A%20%20parameterize%20scenarios%3F&entry.906535625=Erwin%20de%20Gelder%20and%20Olaf%20Op%20den%20Camp&entry.1292438233=%20%20The%20development%20of%20Automated%20Driving%20Systems%20%28ADSs%29%20has%20advanced%0Asignificantly.%20To%20enable%20their%20large-scale%20deployment%2C%20the%20United%20Nations%0ARegulation%20157%20%28UN%20R157%29%20concerning%20the%20approval%20of%20Automated%20Lane%20Keeping%0ASystems%20%28ALKSs%29%20has%20been%20approved%20in%202021.%20UN%20R157%20requires%20an%20activated%20ALKS%0Ato%20avoid%20any%20collisions%20that%20are%20reasonably%20preventable%20and%20proposes%20a%20method%0Ato%20distinguish%20reasonably%20preventable%20collisions%20from%20unpreventable%20ones%20using%0A%22the%20simulated%20performance%20of%20a%20skilled%20and%20attentive%20human%20driver%22.%20With%0Adifferent%20driver%20models%2C%20benchmarks%20are%20set%20for%20ALKSs%20in%20three%20types%20of%0Ascenarios.%20The%20three%20types%20of%20scenarios%20considered%20in%20the%20proposed%20method%20in%20UN%0AR157%20assume%20a%20certain%20parameterization%20without%20any%20further%20consideration.%0A%20%20This%20work%20investigates%20the%20parameterization%20of%20these%20scenarios%2C%20showing%20that%0Athe%20choice%20of%20parameterization%20significantly%20affects%20the%20simulation%20outcomes.%0ABy%20comparing%20real-world%20and%20parameterized%20scenarios%2C%20we%20show%20that%20the%20influence%0Aof%20parameterization%20depends%20on%20the%20scenario%20type%2C%20driver%20model%2C%20and%20evaluation%0Acriterion.%20Alternative%20parameterizations%20are%20proposed%2C%20leading%20to%20results%20that%0Aare%20closer%20to%20the%20non-parameterized%20scenarios%20in%20terms%20of%20recall%2C%20precision%2C%0Aand%20F1%20score.%20The%20study%20highlights%20the%20importance%20of%20careful%20scenario%0Aparameterization%20and%20suggests%20improvements%20to%20the%20current%20UN%20R157%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01117v2&entry.124074799=Read"},
{"title": "Inverse Design with Dynamic Mode Decomposition", "author": "Yunpeng Zhu and Liangliang Cheng and Anping Jing and Hanyu Huo and Ziqiang Lang and Bo Zhang and J. Nathan Kutz", "abstract": "  We introduce a computationally efficient method for the automation of inverse\ndesign in science and engineering. Based on simple least-square regression, the\nunderlying dynamic mode decomposition algorithm can be used to construct a\nlow-rank subspace spanning multiple experiments in parameter space. The\nproposed inverse design dynamic mode composition (ID-DMD) algorithm leverages\nthe computed low-dimensional subspace to enable fast digital design and\noptimization on laptop-level computing, including the potential to prescribe\nthe dynamics themselves. Moreover, the method is robust to noise, physically\ninterpretable, and can provide uncertainty quantification metrics. The\narchitecture can also efficiently scale to large-scale design problems using\nrandomized algorithms in the ID-DMD. The simplicity of the method and its\nimplementation are highly attractive in practice, and the ID-DMD has been\ndemonstrated to be an order of magnitude more accurate than competing methods\nwhile simultaneously being 3-5 orders faster on challenging engineering design\nproblems ranging from structural vibrations to fluid dynamics. Due to its\nspeed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with\nother leading machine learning methods represents a significant advancement in\ndata-driven methods for inverse design and optimization, promising a paradigm\nshift in how to approach inverse design in practice.\n", "link": "http://arxiv.org/abs/2502.09490v1", "date": "2025-02-13", "relevancy": 1.0013, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5174}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4938}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Design%20with%20Dynamic%20Mode%20Decomposition&body=Title%3A%20Inverse%20Design%20with%20Dynamic%20Mode%20Decomposition%0AAuthor%3A%20Yunpeng%20Zhu%20and%20Liangliang%20Cheng%20and%20Anping%20Jing%20and%20Hanyu%20Huo%20and%20Ziqiang%20Lang%20and%20Bo%20Zhang%20and%20J.%20Nathan%20Kutz%0AAbstract%3A%20%20%20We%20introduce%20a%20computationally%20efficient%20method%20for%20the%20automation%20of%20inverse%0Adesign%20in%20science%20and%20engineering.%20Based%20on%20simple%20least-square%20regression%2C%20the%0Aunderlying%20dynamic%20mode%20decomposition%20algorithm%20can%20be%20used%20to%20construct%20a%0Alow-rank%20subspace%20spanning%20multiple%20experiments%20in%20parameter%20space.%20The%0Aproposed%20inverse%20design%20dynamic%20mode%20composition%20%28ID-DMD%29%20algorithm%20leverages%0Athe%20computed%20low-dimensional%20subspace%20to%20enable%20fast%20digital%20design%20and%0Aoptimization%20on%20laptop-level%20computing%2C%20including%20the%20potential%20to%20prescribe%0Athe%20dynamics%20themselves.%20Moreover%2C%20the%20method%20is%20robust%20to%20noise%2C%20physically%0Ainterpretable%2C%20and%20can%20provide%20uncertainty%20quantification%20metrics.%20The%0Aarchitecture%20can%20also%20efficiently%20scale%20to%20large-scale%20design%20problems%20using%0Arandomized%20algorithms%20in%20the%20ID-DMD.%20The%20simplicity%20of%20the%20method%20and%20its%0Aimplementation%20are%20highly%20attractive%20in%20practice%2C%20and%20the%20ID-DMD%20has%20been%0Ademonstrated%20to%20be%20an%20order%20of%20magnitude%20more%20accurate%20than%20competing%20methods%0Awhile%20simultaneously%20being%203-5%20orders%20faster%20on%20challenging%20engineering%20design%0Aproblems%20ranging%20from%20structural%20vibrations%20to%20fluid%20dynamics.%20Due%20to%20its%0Aspeed%2C%20robustness%2C%20interpretability%2C%20and%20ease-of-use%2C%20ID-DMD%20in%20comparison%20with%0Aother%20leading%20machine%20learning%20methods%20represents%20a%20significant%20advancement%20in%0Adata-driven%20methods%20for%20inverse%20design%20and%20optimization%2C%20promising%20a%20paradigm%0Ashift%20in%20how%20to%20approach%20inverse%20design%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Design%2520with%2520Dynamic%2520Mode%2520Decomposition%26entry.906535625%3DYunpeng%2520Zhu%2520and%2520Liangliang%2520Cheng%2520and%2520Anping%2520Jing%2520and%2520Hanyu%2520Huo%2520and%2520Ziqiang%2520Lang%2520and%2520Bo%2520Zhang%2520and%2520J.%2520Nathan%2520Kutz%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520computationally%2520efficient%2520method%2520for%2520the%2520automation%2520of%2520inverse%250Adesign%2520in%2520science%2520and%2520engineering.%2520Based%2520on%2520simple%2520least-square%2520regression%252C%2520the%250Aunderlying%2520dynamic%2520mode%2520decomposition%2520algorithm%2520can%2520be%2520used%2520to%2520construct%2520a%250Alow-rank%2520subspace%2520spanning%2520multiple%2520experiments%2520in%2520parameter%2520space.%2520The%250Aproposed%2520inverse%2520design%2520dynamic%2520mode%2520composition%2520%2528ID-DMD%2529%2520algorithm%2520leverages%250Athe%2520computed%2520low-dimensional%2520subspace%2520to%2520enable%2520fast%2520digital%2520design%2520and%250Aoptimization%2520on%2520laptop-level%2520computing%252C%2520including%2520the%2520potential%2520to%2520prescribe%250Athe%2520dynamics%2520themselves.%2520Moreover%252C%2520the%2520method%2520is%2520robust%2520to%2520noise%252C%2520physically%250Ainterpretable%252C%2520and%2520can%2520provide%2520uncertainty%2520quantification%2520metrics.%2520The%250Aarchitecture%2520can%2520also%2520efficiently%2520scale%2520to%2520large-scale%2520design%2520problems%2520using%250Arandomized%2520algorithms%2520in%2520the%2520ID-DMD.%2520The%2520simplicity%2520of%2520the%2520method%2520and%2520its%250Aimplementation%2520are%2520highly%2520attractive%2520in%2520practice%252C%2520and%2520the%2520ID-DMD%2520has%2520been%250Ademonstrated%2520to%2520be%2520an%2520order%2520of%2520magnitude%2520more%2520accurate%2520than%2520competing%2520methods%250Awhile%2520simultaneously%2520being%25203-5%2520orders%2520faster%2520on%2520challenging%2520engineering%2520design%250Aproblems%2520ranging%2520from%2520structural%2520vibrations%2520to%2520fluid%2520dynamics.%2520Due%2520to%2520its%250Aspeed%252C%2520robustness%252C%2520interpretability%252C%2520and%2520ease-of-use%252C%2520ID-DMD%2520in%2520comparison%2520with%250Aother%2520leading%2520machine%2520learning%2520methods%2520represents%2520a%2520significant%2520advancement%2520in%250Adata-driven%2520methods%2520for%2520inverse%2520design%2520and%2520optimization%252C%2520promising%2520a%2520paradigm%250Ashift%2520in%2520how%2520to%2520approach%2520inverse%2520design%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Design%20with%20Dynamic%20Mode%20Decomposition&entry.906535625=Yunpeng%20Zhu%20and%20Liangliang%20Cheng%20and%20Anping%20Jing%20and%20Hanyu%20Huo%20and%20Ziqiang%20Lang%20and%20Bo%20Zhang%20and%20J.%20Nathan%20Kutz&entry.1292438233=%20%20We%20introduce%20a%20computationally%20efficient%20method%20for%20the%20automation%20of%20inverse%0Adesign%20in%20science%20and%20engineering.%20Based%20on%20simple%20least-square%20regression%2C%20the%0Aunderlying%20dynamic%20mode%20decomposition%20algorithm%20can%20be%20used%20to%20construct%20a%0Alow-rank%20subspace%20spanning%20multiple%20experiments%20in%20parameter%20space.%20The%0Aproposed%20inverse%20design%20dynamic%20mode%20composition%20%28ID-DMD%29%20algorithm%20leverages%0Athe%20computed%20low-dimensional%20subspace%20to%20enable%20fast%20digital%20design%20and%0Aoptimization%20on%20laptop-level%20computing%2C%20including%20the%20potential%20to%20prescribe%0Athe%20dynamics%20themselves.%20Moreover%2C%20the%20method%20is%20robust%20to%20noise%2C%20physically%0Ainterpretable%2C%20and%20can%20provide%20uncertainty%20quantification%20metrics.%20The%0Aarchitecture%20can%20also%20efficiently%20scale%20to%20large-scale%20design%20problems%20using%0Arandomized%20algorithms%20in%20the%20ID-DMD.%20The%20simplicity%20of%20the%20method%20and%20its%0Aimplementation%20are%20highly%20attractive%20in%20practice%2C%20and%20the%20ID-DMD%20has%20been%0Ademonstrated%20to%20be%20an%20order%20of%20magnitude%20more%20accurate%20than%20competing%20methods%0Awhile%20simultaneously%20being%203-5%20orders%20faster%20on%20challenging%20engineering%20design%0Aproblems%20ranging%20from%20structural%20vibrations%20to%20fluid%20dynamics.%20Due%20to%20its%0Aspeed%2C%20robustness%2C%20interpretability%2C%20and%20ease-of-use%2C%20ID-DMD%20in%20comparison%20with%0Aother%20leading%20machine%20learning%20methods%20represents%20a%20significant%20advancement%20in%0Adata-driven%20methods%20for%20inverse%20design%20and%20optimization%2C%20promising%20a%20paradigm%0Ashift%20in%20how%20to%20approach%20inverse%20design%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09490v1&entry.124074799=Read"},
{"title": "Communicating Likelihoods with Normalising Flows", "author": "Jack Y. Araz and Anja Beck and M\u00e9ril Reboud and Michael Spannowsky and Danny van Dyk", "abstract": "  We present a machine-learning-based workflow to model an unbinned likelihood\nfrom its samples. A key advancement over existing approaches is the validation\nof the learned likelihood using rigorous statistical tests of the joint\ndistribution, such as the Kolmogorov-Smirnov test of the joint distribution.\nOur method enables the reliable communication of experimental and\nphenomenological likelihoods for subsequent analyses. We demonstrate its\neffectiveness through three case studies in high-energy physics. To support\nbroader adoption, we provide an open-source reference implementation, nabu.\n", "link": "http://arxiv.org/abs/2502.09494v1", "date": "2025-02-13", "relevancy": 1.3733, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4827}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4581}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communicating%20Likelihoods%20with%20Normalising%20Flows&body=Title%3A%20Communicating%20Likelihoods%20with%20Normalising%20Flows%0AAuthor%3A%20Jack%20Y.%20Araz%20and%20Anja%20Beck%20and%20M%C3%A9ril%20Reboud%20and%20Michael%20Spannowsky%20and%20Danny%20van%20Dyk%0AAbstract%3A%20%20%20We%20present%20a%20machine-learning-based%20workflow%20to%20model%20an%20unbinned%20likelihood%0Afrom%20its%20samples.%20A%20key%20advancement%20over%20existing%20approaches%20is%20the%20validation%0Aof%20the%20learned%20likelihood%20using%20rigorous%20statistical%20tests%20of%20the%20joint%0Adistribution%2C%20such%20as%20the%20Kolmogorov-Smirnov%20test%20of%20the%20joint%20distribution.%0AOur%20method%20enables%20the%20reliable%20communication%20of%20experimental%20and%0Aphenomenological%20likelihoods%20for%20subsequent%20analyses.%20We%20demonstrate%20its%0Aeffectiveness%20through%20three%20case%20studies%20in%20high-energy%20physics.%20To%20support%0Abroader%20adoption%2C%20we%20provide%20an%20open-source%20reference%20implementation%2C%20nabu.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunicating%2520Likelihoods%2520with%2520Normalising%2520Flows%26entry.906535625%3DJack%2520Y.%2520Araz%2520and%2520Anja%2520Beck%2520and%2520M%25C3%25A9ril%2520Reboud%2520and%2520Michael%2520Spannowsky%2520and%2520Danny%2520van%2520Dyk%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520machine-learning-based%2520workflow%2520to%2520model%2520an%2520unbinned%2520likelihood%250Afrom%2520its%2520samples.%2520A%2520key%2520advancement%2520over%2520existing%2520approaches%2520is%2520the%2520validation%250Aof%2520the%2520learned%2520likelihood%2520using%2520rigorous%2520statistical%2520tests%2520of%2520the%2520joint%250Adistribution%252C%2520such%2520as%2520the%2520Kolmogorov-Smirnov%2520test%2520of%2520the%2520joint%2520distribution.%250AOur%2520method%2520enables%2520the%2520reliable%2520communication%2520of%2520experimental%2520and%250Aphenomenological%2520likelihoods%2520for%2520subsequent%2520analyses.%2520We%2520demonstrate%2520its%250Aeffectiveness%2520through%2520three%2520case%2520studies%2520in%2520high-energy%2520physics.%2520To%2520support%250Abroader%2520adoption%252C%2520we%2520provide%2520an%2520open-source%2520reference%2520implementation%252C%2520nabu.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communicating%20Likelihoods%20with%20Normalising%20Flows&entry.906535625=Jack%20Y.%20Araz%20and%20Anja%20Beck%20and%20M%C3%A9ril%20Reboud%20and%20Michael%20Spannowsky%20and%20Danny%20van%20Dyk&entry.1292438233=%20%20We%20present%20a%20machine-learning-based%20workflow%20to%20model%20an%20unbinned%20likelihood%0Afrom%20its%20samples.%20A%20key%20advancement%20over%20existing%20approaches%20is%20the%20validation%0Aof%20the%20learned%20likelihood%20using%20rigorous%20statistical%20tests%20of%20the%20joint%0Adistribution%2C%20such%20as%20the%20Kolmogorov-Smirnov%20test%20of%20the%20joint%20distribution.%0AOur%20method%20enables%20the%20reliable%20communication%20of%20experimental%20and%0Aphenomenological%20likelihoods%20for%20subsequent%20analyses.%20We%20demonstrate%20its%0Aeffectiveness%20through%20three%20case%20studies%20in%20high-energy%20physics.%20To%20support%0Abroader%20adoption%2C%20we%20provide%20an%20open-source%20reference%20implementation%2C%20nabu.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09494v1&entry.124074799=Read"},
{"title": "Sable: a Performant, Efficient and Scalable Sequence Model for MARL", "author": "Omayma Mahjoub and Sasha Abramowitz and Ruan de Kock and Wiem Khlifi and Simon du Toit and Jemma Daniel and Louay Ben Nessir and Louise Beyers and Claude Formanek and Liam Clark and Arnu Pretorius", "abstract": "  As multi-agent reinforcement learning (MARL) progresses towards solving\nlarger and more complex problems, it becomes increasingly important that\nalgorithms exhibit the key properties of (1) strong performance, (2) memory\nefficiency and (3) scalability. In this work, we introduce Sable, a performant,\nmemory efficient and scalable sequence modeling approach to MARL. Sable works\nby adapting the retention mechanism in Retentive Networks to achieve\ncomputationally efficient processing of multi-agent observations with long\ncontext memory for temporal reasoning. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in a large number of diverse tasks\n(34 out of 45 tested). Furthermore, Sable maintains performance as we scale the\nnumber of agents, handling environments with more than a thousand agents while\nexhibiting a linear increase in memory usage. Finally, we conduct ablation\nstudies to isolate the source of Sable's performance gains and confirm its\nefficient computational memory usage.\n", "link": "http://arxiv.org/abs/2410.01706v2", "date": "2025-02-13", "relevancy": 1.4629, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5045}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sable%3A%20a%20Performant%2C%20Efficient%20and%20Scalable%20Sequence%20Model%20for%20MARL&body=Title%3A%20Sable%3A%20a%20Performant%2C%20Efficient%20and%20Scalable%20Sequence%20Model%20for%20MARL%0AAuthor%3A%20Omayma%20Mahjoub%20and%20Sasha%20Abramowitz%20and%20Ruan%20de%20Kock%20and%20Wiem%20Khlifi%20and%20Simon%20du%20Toit%20and%20Jemma%20Daniel%20and%20Louay%20Ben%20Nessir%20and%20Louise%20Beyers%20and%20Claude%20Formanek%20and%20Liam%20Clark%20and%20Arnu%20Pretorius%0AAbstract%3A%20%20%20As%20multi-agent%20reinforcement%20learning%20%28MARL%29%20progresses%20towards%20solving%0Alarger%20and%20more%20complex%20problems%2C%20it%20becomes%20increasingly%20important%20that%0Aalgorithms%20exhibit%20the%20key%20properties%20of%20%281%29%20strong%20performance%2C%20%282%29%20memory%0Aefficiency%20and%20%283%29%20scalability.%20In%20this%20work%2C%20we%20introduce%20Sable%2C%20a%20performant%2C%0Amemory%20efficient%20and%20scalable%20sequence%20modeling%20approach%20to%20MARL.%20Sable%20works%0Aby%20adapting%20the%20retention%20mechanism%20in%20Retentive%20Networks%20to%20achieve%0Acomputationally%20efficient%20processing%20of%20multi-agent%20observations%20with%20long%0Acontext%20memory%20for%20temporal%20reasoning.%20Through%20extensive%20evaluations%20across%20six%0Adiverse%20environments%2C%20we%20demonstrate%20how%20Sable%20is%20able%20to%20significantly%0Aoutperform%20existing%20state-of-the-art%20methods%20in%20a%20large%20number%20of%20diverse%20tasks%0A%2834%20out%20of%2045%20tested%29.%20Furthermore%2C%20Sable%20maintains%20performance%20as%20we%20scale%20the%0Anumber%20of%20agents%2C%20handling%20environments%20with%20more%20than%20a%20thousand%20agents%20while%0Aexhibiting%20a%20linear%20increase%20in%20memory%20usage.%20Finally%2C%20we%20conduct%20ablation%0Astudies%20to%20isolate%20the%20source%20of%20Sable%27s%20performance%20gains%20and%20confirm%20its%0Aefficient%20computational%20memory%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSable%253A%2520a%2520Performant%252C%2520Efficient%2520and%2520Scalable%2520Sequence%2520Model%2520for%2520MARL%26entry.906535625%3DOmayma%2520Mahjoub%2520and%2520Sasha%2520Abramowitz%2520and%2520Ruan%2520de%2520Kock%2520and%2520Wiem%2520Khlifi%2520and%2520Simon%2520du%2520Toit%2520and%2520Jemma%2520Daniel%2520and%2520Louay%2520Ben%2520Nessir%2520and%2520Louise%2520Beyers%2520and%2520Claude%2520Formanek%2520and%2520Liam%2520Clark%2520and%2520Arnu%2520Pretorius%26entry.1292438233%3D%2520%2520As%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520progresses%2520towards%2520solving%250Alarger%2520and%2520more%2520complex%2520problems%252C%2520it%2520becomes%2520increasingly%2520important%2520that%250Aalgorithms%2520exhibit%2520the%2520key%2520properties%2520of%2520%25281%2529%2520strong%2520performance%252C%2520%25282%2529%2520memory%250Aefficiency%2520and%2520%25283%2529%2520scalability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Sable%252C%2520a%2520performant%252C%250Amemory%2520efficient%2520and%2520scalable%2520sequence%2520modeling%2520approach%2520to%2520MARL.%2520Sable%2520works%250Aby%2520adapting%2520the%2520retention%2520mechanism%2520in%2520Retentive%2520Networks%2520to%2520achieve%250Acomputationally%2520efficient%2520processing%2520of%2520multi-agent%2520observations%2520with%2520long%250Acontext%2520memory%2520for%2520temporal%2520reasoning.%2520Through%2520extensive%2520evaluations%2520across%2520six%250Adiverse%2520environments%252C%2520we%2520demonstrate%2520how%2520Sable%2520is%2520able%2520to%2520significantly%250Aoutperform%2520existing%2520state-of-the-art%2520methods%2520in%2520a%2520large%2520number%2520of%2520diverse%2520tasks%250A%252834%2520out%2520of%252045%2520tested%2529.%2520Furthermore%252C%2520Sable%2520maintains%2520performance%2520as%2520we%2520scale%2520the%250Anumber%2520of%2520agents%252C%2520handling%2520environments%2520with%2520more%2520than%2520a%2520thousand%2520agents%2520while%250Aexhibiting%2520a%2520linear%2520increase%2520in%2520memory%2520usage.%2520Finally%252C%2520we%2520conduct%2520ablation%250Astudies%2520to%2520isolate%2520the%2520source%2520of%2520Sable%2527s%2520performance%2520gains%2520and%2520confirm%2520its%250Aefficient%2520computational%2520memory%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sable%3A%20a%20Performant%2C%20Efficient%20and%20Scalable%20Sequence%20Model%20for%20MARL&entry.906535625=Omayma%20Mahjoub%20and%20Sasha%20Abramowitz%20and%20Ruan%20de%20Kock%20and%20Wiem%20Khlifi%20and%20Simon%20du%20Toit%20and%20Jemma%20Daniel%20and%20Louay%20Ben%20Nessir%20and%20Louise%20Beyers%20and%20Claude%20Formanek%20and%20Liam%20Clark%20and%20Arnu%20Pretorius&entry.1292438233=%20%20As%20multi-agent%20reinforcement%20learning%20%28MARL%29%20progresses%20towards%20solving%0Alarger%20and%20more%20complex%20problems%2C%20it%20becomes%20increasingly%20important%20that%0Aalgorithms%20exhibit%20the%20key%20properties%20of%20%281%29%20strong%20performance%2C%20%282%29%20memory%0Aefficiency%20and%20%283%29%20scalability.%20In%20this%20work%2C%20we%20introduce%20Sable%2C%20a%20performant%2C%0Amemory%20efficient%20and%20scalable%20sequence%20modeling%20approach%20to%20MARL.%20Sable%20works%0Aby%20adapting%20the%20retention%20mechanism%20in%20Retentive%20Networks%20to%20achieve%0Acomputationally%20efficient%20processing%20of%20multi-agent%20observations%20with%20long%0Acontext%20memory%20for%20temporal%20reasoning.%20Through%20extensive%20evaluations%20across%20six%0Adiverse%20environments%2C%20we%20demonstrate%20how%20Sable%20is%20able%20to%20significantly%0Aoutperform%20existing%20state-of-the-art%20methods%20in%20a%20large%20number%20of%20diverse%20tasks%0A%2834%20out%20of%2045%20tested%29.%20Furthermore%2C%20Sable%20maintains%20performance%20as%20we%20scale%20the%0Anumber%20of%20agents%2C%20handling%20environments%20with%20more%20than%20a%20thousand%20agents%20while%0Aexhibiting%20a%20linear%20increase%20in%20memory%20usage.%20Finally%2C%20we%20conduct%20ablation%0Astudies%20to%20isolate%20the%20source%20of%20Sable%27s%20performance%20gains%20and%20confirm%20its%0Aefficient%20computational%20memory%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01706v2&entry.124074799=Read"},
{"title": "Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes", "author": "Navdeep Kumar and Adarsh Gupta and Maxence Mohamed Elfatihi and Giorgia Ramponi and Kfir Yehuda Levy and Shie Mannor", "abstract": "  We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs.\n", "link": "http://arxiv.org/abs/2502.09432v1", "date": "2025-02-13", "relevancy": 1.4853, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4992}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Formulation%20for%20Non-Rectangular%20Lp%20Robust%20Markov%20Decision%20Processes&body=Title%3A%20Dual%20Formulation%20for%20Non-Rectangular%20Lp%20Robust%20Markov%20Decision%20Processes%0AAuthor%3A%20Navdeep%20Kumar%20and%20Adarsh%20Gupta%20and%20Maxence%20Mohamed%20Elfatihi%20and%20Giorgia%20Ramponi%20and%20Kfir%20Yehuda%20Levy%20and%20Shie%20Mannor%0AAbstract%3A%20%20%20We%20study%20robust%20Markov%20decision%20processes%20%28RMDPs%29%20with%20non-rectangular%0Auncertainty%20sets%2C%20which%20capture%20interdependencies%20across%20states%20unlike%0Atraditional%20rectangular%20models.%20While%20non-rectangular%20robust%20policy%20evaluation%0Ais%20generally%20NP-hard%2C%20even%20in%20approximation%2C%20we%20identify%20a%20powerful%20class%20of%0A%24L_p%24-bounded%20uncertainty%20sets%20that%20avoid%20these%20complexity%20barriers%20due%20to%0Atheir%20structural%20simplicity.%20We%20further%20show%20that%20this%20class%20can%20be%20decomposed%0Ainto%20infinitely%20many%20%5Ctexttt%7Bsa%7D-rectangular%20%24L_p%24-bounded%20sets%20and%20leverage%0Aits%20structural%20properties%20to%20derive%20a%20novel%20dual%20formulation%20for%20%24L_p%24%20RMDPs.%0AThis%20formulation%20provides%20key%20insights%20into%20the%20adversary%27s%20strategy%20and%0Aenables%20the%20development%20of%20the%20first%20robust%20policy%20evaluation%20algorithms%20for%0Anon-rectangular%20RMDPs.%20Empirical%20results%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20brute-force%20methods%2C%20establishing%20a%20promising%0Afoundation%20for%20future%20investigation%20into%20non-rectangular%20robust%20MDPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Formulation%2520for%2520Non-Rectangular%2520Lp%2520Robust%2520Markov%2520Decision%2520Processes%26entry.906535625%3DNavdeep%2520Kumar%2520and%2520Adarsh%2520Gupta%2520and%2520Maxence%2520Mohamed%2520Elfatihi%2520and%2520Giorgia%2520Ramponi%2520and%2520Kfir%2520Yehuda%2520Levy%2520and%2520Shie%2520Mannor%26entry.1292438233%3D%2520%2520We%2520study%2520robust%2520Markov%2520decision%2520processes%2520%2528RMDPs%2529%2520with%2520non-rectangular%250Auncertainty%2520sets%252C%2520which%2520capture%2520interdependencies%2520across%2520states%2520unlike%250Atraditional%2520rectangular%2520models.%2520While%2520non-rectangular%2520robust%2520policy%2520evaluation%250Ais%2520generally%2520NP-hard%252C%2520even%2520in%2520approximation%252C%2520we%2520identify%2520a%2520powerful%2520class%2520of%250A%2524L_p%2524-bounded%2520uncertainty%2520sets%2520that%2520avoid%2520these%2520complexity%2520barriers%2520due%2520to%250Atheir%2520structural%2520simplicity.%2520We%2520further%2520show%2520that%2520this%2520class%2520can%2520be%2520decomposed%250Ainto%2520infinitely%2520many%2520%255Ctexttt%257Bsa%257D-rectangular%2520%2524L_p%2524-bounded%2520sets%2520and%2520leverage%250Aits%2520structural%2520properties%2520to%2520derive%2520a%2520novel%2520dual%2520formulation%2520for%2520%2524L_p%2524%2520RMDPs.%250AThis%2520formulation%2520provides%2520key%2520insights%2520into%2520the%2520adversary%2527s%2520strategy%2520and%250Aenables%2520the%2520development%2520of%2520the%2520first%2520robust%2520policy%2520evaluation%2520algorithms%2520for%250Anon-rectangular%2520RMDPs.%2520Empirical%2520results%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520brute-force%2520methods%252C%2520establishing%2520a%2520promising%250Afoundation%2520for%2520future%2520investigation%2520into%2520non-rectangular%2520robust%2520MDPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Formulation%20for%20Non-Rectangular%20Lp%20Robust%20Markov%20Decision%20Processes&entry.906535625=Navdeep%20Kumar%20and%20Adarsh%20Gupta%20and%20Maxence%20Mohamed%20Elfatihi%20and%20Giorgia%20Ramponi%20and%20Kfir%20Yehuda%20Levy%20and%20Shie%20Mannor&entry.1292438233=%20%20We%20study%20robust%20Markov%20decision%20processes%20%28RMDPs%29%20with%20non-rectangular%0Auncertainty%20sets%2C%20which%20capture%20interdependencies%20across%20states%20unlike%0Atraditional%20rectangular%20models.%20While%20non-rectangular%20robust%20policy%20evaluation%0Ais%20generally%20NP-hard%2C%20even%20in%20approximation%2C%20we%20identify%20a%20powerful%20class%20of%0A%24L_p%24-bounded%20uncertainty%20sets%20that%20avoid%20these%20complexity%20barriers%20due%20to%0Atheir%20structural%20simplicity.%20We%20further%20show%20that%20this%20class%20can%20be%20decomposed%0Ainto%20infinitely%20many%20%5Ctexttt%7Bsa%7D-rectangular%20%24L_p%24-bounded%20sets%20and%20leverage%0Aits%20structural%20properties%20to%20derive%20a%20novel%20dual%20formulation%20for%20%24L_p%24%20RMDPs.%0AThis%20formulation%20provides%20key%20insights%20into%20the%20adversary%27s%20strategy%20and%0Aenables%20the%20development%20of%20the%20first%20robust%20policy%20evaluation%20algorithms%20for%0Anon-rectangular%20RMDPs.%20Empirical%20results%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20brute-force%20methods%2C%20establishing%20a%20promising%0Afoundation%20for%20future%20investigation%20into%20non-rectangular%20robust%20MDPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09432v1&entry.124074799=Read"},
{"title": "Real-Time Fast Marching Tree for Mobile Robot Motion Planning in Dynamic\n  Environments", "author": "Jefferson Silveira and Kleber Cabral and Sidney Givigi and Joshua A. Marshall", "abstract": "  This paper proposes the Real-Time Fast Marching Tree (RT-FMT), a real-time\nplanning algorithm that features local and global path generation,\nmultiple-query planning, and dynamic obstacle avoidance. During the search,\nRT-FMT quickly looks for the global solution and, in the meantime, generates\nlocal paths that can be used by the robot to start execution faster. In\naddition, our algorithm constantly rewires the tree to keep branches from\nforming inside the dynamic obstacles and to maintain the tree root near the\nrobot, which allows the tree to be reused multiple times for different goals.\nOur algorithm is based on the planners Fast Marching Tree (FMT*) and Real-time\nRapidly-Exploring Random Tree (RT-RRT*). We show via simulations that RT-FMT\noutperforms RT- RRT* in both execution cost and arrival time, in most cases.\nMoreover, we also demonstrate via simulation that it is worthwhile taking the\nlocal path before the global path is available in order to reduce arrival time,\neven though there is a small possibility of taking an inferior path.\n", "link": "http://arxiv.org/abs/2502.09556v1", "date": "2025-02-13", "relevancy": 1.4602, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4849}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Fast%20Marching%20Tree%20for%20Mobile%20Robot%20Motion%20Planning%20in%20Dynamic%0A%20%20Environments&body=Title%3A%20Real-Time%20Fast%20Marching%20Tree%20for%20Mobile%20Robot%20Motion%20Planning%20in%20Dynamic%0A%20%20Environments%0AAuthor%3A%20Jefferson%20Silveira%20and%20Kleber%20Cabral%20and%20Sidney%20Givigi%20and%20Joshua%20A.%20Marshall%0AAbstract%3A%20%20%20This%20paper%20proposes%20the%20Real-Time%20Fast%20Marching%20Tree%20%28RT-FMT%29%2C%20a%20real-time%0Aplanning%20algorithm%20that%20features%20local%20and%20global%20path%20generation%2C%0Amultiple-query%20planning%2C%20and%20dynamic%20obstacle%20avoidance.%20During%20the%20search%2C%0ART-FMT%20quickly%20looks%20for%20the%20global%20solution%20and%2C%20in%20the%20meantime%2C%20generates%0Alocal%20paths%20that%20can%20be%20used%20by%20the%20robot%20to%20start%20execution%20faster.%20In%0Aaddition%2C%20our%20algorithm%20constantly%20rewires%20the%20tree%20to%20keep%20branches%20from%0Aforming%20inside%20the%20dynamic%20obstacles%20and%20to%20maintain%20the%20tree%20root%20near%20the%0Arobot%2C%20which%20allows%20the%20tree%20to%20be%20reused%20multiple%20times%20for%20different%20goals.%0AOur%20algorithm%20is%20based%20on%20the%20planners%20Fast%20Marching%20Tree%20%28FMT%2A%29%20and%20Real-time%0ARapidly-Exploring%20Random%20Tree%20%28RT-RRT%2A%29.%20We%20show%20via%20simulations%20that%20RT-FMT%0Aoutperforms%20RT-%20RRT%2A%20in%20both%20execution%20cost%20and%20arrival%20time%2C%20in%20most%20cases.%0AMoreover%2C%20we%20also%20demonstrate%20via%20simulation%20that%20it%20is%20worthwhile%20taking%20the%0Alocal%20path%20before%20the%20global%20path%20is%20available%20in%20order%20to%20reduce%20arrival%20time%2C%0Aeven%20though%20there%20is%20a%20small%20possibility%20of%20taking%20an%20inferior%20path.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Fast%2520Marching%2520Tree%2520for%2520Mobile%2520Robot%2520Motion%2520Planning%2520in%2520Dynamic%250A%2520%2520Environments%26entry.906535625%3DJefferson%2520Silveira%2520and%2520Kleber%2520Cabral%2520and%2520Sidney%2520Givigi%2520and%2520Joshua%2520A.%2520Marshall%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520the%2520Real-Time%2520Fast%2520Marching%2520Tree%2520%2528RT-FMT%2529%252C%2520a%2520real-time%250Aplanning%2520algorithm%2520that%2520features%2520local%2520and%2520global%2520path%2520generation%252C%250Amultiple-query%2520planning%252C%2520and%2520dynamic%2520obstacle%2520avoidance.%2520During%2520the%2520search%252C%250ART-FMT%2520quickly%2520looks%2520for%2520the%2520global%2520solution%2520and%252C%2520in%2520the%2520meantime%252C%2520generates%250Alocal%2520paths%2520that%2520can%2520be%2520used%2520by%2520the%2520robot%2520to%2520start%2520execution%2520faster.%2520In%250Aaddition%252C%2520our%2520algorithm%2520constantly%2520rewires%2520the%2520tree%2520to%2520keep%2520branches%2520from%250Aforming%2520inside%2520the%2520dynamic%2520obstacles%2520and%2520to%2520maintain%2520the%2520tree%2520root%2520near%2520the%250Arobot%252C%2520which%2520allows%2520the%2520tree%2520to%2520be%2520reused%2520multiple%2520times%2520for%2520different%2520goals.%250AOur%2520algorithm%2520is%2520based%2520on%2520the%2520planners%2520Fast%2520Marching%2520Tree%2520%2528FMT%252A%2529%2520and%2520Real-time%250ARapidly-Exploring%2520Random%2520Tree%2520%2528RT-RRT%252A%2529.%2520We%2520show%2520via%2520simulations%2520that%2520RT-FMT%250Aoutperforms%2520RT-%2520RRT%252A%2520in%2520both%2520execution%2520cost%2520and%2520arrival%2520time%252C%2520in%2520most%2520cases.%250AMoreover%252C%2520we%2520also%2520demonstrate%2520via%2520simulation%2520that%2520it%2520is%2520worthwhile%2520taking%2520the%250Alocal%2520path%2520before%2520the%2520global%2520path%2520is%2520available%2520in%2520order%2520to%2520reduce%2520arrival%2520time%252C%250Aeven%2520though%2520there%2520is%2520a%2520small%2520possibility%2520of%2520taking%2520an%2520inferior%2520path.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Fast%20Marching%20Tree%20for%20Mobile%20Robot%20Motion%20Planning%20in%20Dynamic%0A%20%20Environments&entry.906535625=Jefferson%20Silveira%20and%20Kleber%20Cabral%20and%20Sidney%20Givigi%20and%20Joshua%20A.%20Marshall&entry.1292438233=%20%20This%20paper%20proposes%20the%20Real-Time%20Fast%20Marching%20Tree%20%28RT-FMT%29%2C%20a%20real-time%0Aplanning%20algorithm%20that%20features%20local%20and%20global%20path%20generation%2C%0Amultiple-query%20planning%2C%20and%20dynamic%20obstacle%20avoidance.%20During%20the%20search%2C%0ART-FMT%20quickly%20looks%20for%20the%20global%20solution%20and%2C%20in%20the%20meantime%2C%20generates%0Alocal%20paths%20that%20can%20be%20used%20by%20the%20robot%20to%20start%20execution%20faster.%20In%0Aaddition%2C%20our%20algorithm%20constantly%20rewires%20the%20tree%20to%20keep%20branches%20from%0Aforming%20inside%20the%20dynamic%20obstacles%20and%20to%20maintain%20the%20tree%20root%20near%20the%0Arobot%2C%20which%20allows%20the%20tree%20to%20be%20reused%20multiple%20times%20for%20different%20goals.%0AOur%20algorithm%20is%20based%20on%20the%20planners%20Fast%20Marching%20Tree%20%28FMT%2A%29%20and%20Real-time%0ARapidly-Exploring%20Random%20Tree%20%28RT-RRT%2A%29.%20We%20show%20via%20simulations%20that%20RT-FMT%0Aoutperforms%20RT-%20RRT%2A%20in%20both%20execution%20cost%20and%20arrival%20time%2C%20in%20most%20cases.%0AMoreover%2C%20we%20also%20demonstrate%20via%20simulation%20that%20it%20is%20worthwhile%20taking%20the%0Alocal%20path%20before%20the%20global%20path%20is%20available%20in%20order%20to%20reduce%20arrival%20time%2C%0Aeven%20though%20there%20is%20a%20small%20possibility%20of%20taking%20an%20inferior%20path.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09556v1&entry.124074799=Read"},
{"title": "Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages", "author": "Shreyan Biswas and Alexander Erlei and Ujwal Gadiraju", "abstract": "  Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.\n", "link": "http://arxiv.org/abs/2502.09532v1", "date": "2025-02-13", "relevancy": 1.3337, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.459}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%21%20Choice%20Independence%20in%20Using%20Multilingual%20LLMs%20for%0A%20%20Persuasive%20Co-Writing%20Tasks%20in%20Different%20Languages&body=Title%3A%20Mind%20the%20Gap%21%20Choice%20Independence%20in%20Using%20Multilingual%20LLMs%20for%0A%20%20Persuasive%20Co-Writing%20Tasks%20in%20Different%20Languages%0AAuthor%3A%20Shreyan%20Biswas%20and%20Alexander%20Erlei%20and%20Ujwal%20Gadiraju%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20AI%20have%20precipitated%20a%20proliferation%20of%20novel%0Awriting%20assistants.%20These%20systems%20typically%20rely%20on%20multilingual%20large%20language%0Amodels%20%28LLMs%29%2C%20providing%20globalized%20workers%20the%20ability%20to%20revise%20or%20create%0Adiverse%20forms%20of%20content%20in%20different%20languages.%20However%2C%20there%20is%20substantial%0Aevidence%20indicating%20that%20the%20performance%20of%20multilingual%20LLMs%20varies%20between%0Alanguages.%20Users%20who%20employ%20writing%20assistance%20for%20multiple%20languages%20are%0Atherefore%20susceptible%20to%20disparate%20output%20quality.%20Importantly%2C%20recent%20research%0Ahas%20shown%20that%20people%20tend%20to%20generalize%20algorithmic%20errors%20across%20independent%0Atasks%2C%20violating%20the%20behavioral%20axiom%20of%20choice%20independence.%20In%20this%20paper%2C%20we%0Aanalyze%20whether%20user%20utilization%20of%20novel%20writing%20assistants%20in%20a%20charity%0Aadvertisement%20writing%20task%20is%20affected%20by%20the%20AI%27s%20performance%20in%20a%20second%0Alanguage.%20Furthermore%2C%20we%20quantify%20the%20extent%20to%20which%20these%20patterns%20translate%0Ainto%20the%20persuasiveness%20of%20generated%20charity%20advertisements%2C%20as%20well%20as%20the%0Arole%20of%20peoples%27%20beliefs%20about%20LLM%20utilization%20in%20their%20donation%20choices.%20Our%0Aresults%20provide%20evidence%20that%20writers%20who%20engage%20with%20an%20LLM-based%20writing%0Aassistant%20violate%20choice%20independence%2C%20as%20prior%20exposure%20to%20a%20Spanish%20LLM%0Areduces%20subsequent%20utilization%20of%20an%20English%20LLM.%20While%20these%20patterns%20do%20not%0Aaffect%20the%20aggregate%20persuasiveness%20of%20the%20generated%20advertisements%2C%20people%27s%0Abeliefs%20about%20the%20source%20of%20an%20advertisement%20%28human%20versus%20AI%29%20do.%20In%0Aparticular%2C%20Spanish-speaking%20female%20participants%20who%20believed%20that%20they%20read%20an%0AAI-generated%20advertisement%20strongly%20adjusted%20their%20donation%20behavior%20downwards.%0AFurthermore%2C%20people%20are%20generally%20not%20able%20to%20adequately%20differentiate%20between%0Ahuman-generated%20and%20LLM-generated%20ads.%20Our%20work%20has%20important%20implications%20for%0Athe%20design%2C%20development%2C%20integration%2C%20and%20adoption%20of%20multilingual%20LLMs%20as%0Aassistive%20agents%20--%20particularly%20in%20writing%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%2521%2520Choice%2520Independence%2520in%2520Using%2520Multilingual%2520LLMs%2520for%250A%2520%2520Persuasive%2520Co-Writing%2520Tasks%2520in%2520Different%2520Languages%26entry.906535625%3DShreyan%2520Biswas%2520and%2520Alexander%2520Erlei%2520and%2520Ujwal%2520Gadiraju%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520AI%2520have%2520precipitated%2520a%2520proliferation%2520of%2520novel%250Awriting%2520assistants.%2520These%2520systems%2520typically%2520rely%2520on%2520multilingual%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520providing%2520globalized%2520workers%2520the%2520ability%2520to%2520revise%2520or%2520create%250Adiverse%2520forms%2520of%2520content%2520in%2520different%2520languages.%2520However%252C%2520there%2520is%2520substantial%250Aevidence%2520indicating%2520that%2520the%2520performance%2520of%2520multilingual%2520LLMs%2520varies%2520between%250Alanguages.%2520Users%2520who%2520employ%2520writing%2520assistance%2520for%2520multiple%2520languages%2520are%250Atherefore%2520susceptible%2520to%2520disparate%2520output%2520quality.%2520Importantly%252C%2520recent%2520research%250Ahas%2520shown%2520that%2520people%2520tend%2520to%2520generalize%2520algorithmic%2520errors%2520across%2520independent%250Atasks%252C%2520violating%2520the%2520behavioral%2520axiom%2520of%2520choice%2520independence.%2520In%2520this%2520paper%252C%2520we%250Aanalyze%2520whether%2520user%2520utilization%2520of%2520novel%2520writing%2520assistants%2520in%2520a%2520charity%250Aadvertisement%2520writing%2520task%2520is%2520affected%2520by%2520the%2520AI%2527s%2520performance%2520in%2520a%2520second%250Alanguage.%2520Furthermore%252C%2520we%2520quantify%2520the%2520extent%2520to%2520which%2520these%2520patterns%2520translate%250Ainto%2520the%2520persuasiveness%2520of%2520generated%2520charity%2520advertisements%252C%2520as%2520well%2520as%2520the%250Arole%2520of%2520peoples%2527%2520beliefs%2520about%2520LLM%2520utilization%2520in%2520their%2520donation%2520choices.%2520Our%250Aresults%2520provide%2520evidence%2520that%2520writers%2520who%2520engage%2520with%2520an%2520LLM-based%2520writing%250Aassistant%2520violate%2520choice%2520independence%252C%2520as%2520prior%2520exposure%2520to%2520a%2520Spanish%2520LLM%250Areduces%2520subsequent%2520utilization%2520of%2520an%2520English%2520LLM.%2520While%2520these%2520patterns%2520do%2520not%250Aaffect%2520the%2520aggregate%2520persuasiveness%2520of%2520the%2520generated%2520advertisements%252C%2520people%2527s%250Abeliefs%2520about%2520the%2520source%2520of%2520an%2520advertisement%2520%2528human%2520versus%2520AI%2529%2520do.%2520In%250Aparticular%252C%2520Spanish-speaking%2520female%2520participants%2520who%2520believed%2520that%2520they%2520read%2520an%250AAI-generated%2520advertisement%2520strongly%2520adjusted%2520their%2520donation%2520behavior%2520downwards.%250AFurthermore%252C%2520people%2520are%2520generally%2520not%2520able%2520to%2520adequately%2520differentiate%2520between%250Ahuman-generated%2520and%2520LLM-generated%2520ads.%2520Our%2520work%2520has%2520important%2520implications%2520for%250Athe%2520design%252C%2520development%252C%2520integration%252C%2520and%2520adoption%2520of%2520multilingual%2520LLMs%2520as%250Aassistive%2520agents%2520--%2520particularly%2520in%2520writing%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%21%20Choice%20Independence%20in%20Using%20Multilingual%20LLMs%20for%0A%20%20Persuasive%20Co-Writing%20Tasks%20in%20Different%20Languages&entry.906535625=Shreyan%20Biswas%20and%20Alexander%20Erlei%20and%20Ujwal%20Gadiraju&entry.1292438233=%20%20Recent%20advances%20in%20generative%20AI%20have%20precipitated%20a%20proliferation%20of%20novel%0Awriting%20assistants.%20These%20systems%20typically%20rely%20on%20multilingual%20large%20language%0Amodels%20%28LLMs%29%2C%20providing%20globalized%20workers%20the%20ability%20to%20revise%20or%20create%0Adiverse%20forms%20of%20content%20in%20different%20languages.%20However%2C%20there%20is%20substantial%0Aevidence%20indicating%20that%20the%20performance%20of%20multilingual%20LLMs%20varies%20between%0Alanguages.%20Users%20who%20employ%20writing%20assistance%20for%20multiple%20languages%20are%0Atherefore%20susceptible%20to%20disparate%20output%20quality.%20Importantly%2C%20recent%20research%0Ahas%20shown%20that%20people%20tend%20to%20generalize%20algorithmic%20errors%20across%20independent%0Atasks%2C%20violating%20the%20behavioral%20axiom%20of%20choice%20independence.%20In%20this%20paper%2C%20we%0Aanalyze%20whether%20user%20utilization%20of%20novel%20writing%20assistants%20in%20a%20charity%0Aadvertisement%20writing%20task%20is%20affected%20by%20the%20AI%27s%20performance%20in%20a%20second%0Alanguage.%20Furthermore%2C%20we%20quantify%20the%20extent%20to%20which%20these%20patterns%20translate%0Ainto%20the%20persuasiveness%20of%20generated%20charity%20advertisements%2C%20as%20well%20as%20the%0Arole%20of%20peoples%27%20beliefs%20about%20LLM%20utilization%20in%20their%20donation%20choices.%20Our%0Aresults%20provide%20evidence%20that%20writers%20who%20engage%20with%20an%20LLM-based%20writing%0Aassistant%20violate%20choice%20independence%2C%20as%20prior%20exposure%20to%20a%20Spanish%20LLM%0Areduces%20subsequent%20utilization%20of%20an%20English%20LLM.%20While%20these%20patterns%20do%20not%0Aaffect%20the%20aggregate%20persuasiveness%20of%20the%20generated%20advertisements%2C%20people%27s%0Abeliefs%20about%20the%20source%20of%20an%20advertisement%20%28human%20versus%20AI%29%20do.%20In%0Aparticular%2C%20Spanish-speaking%20female%20participants%20who%20believed%20that%20they%20read%20an%0AAI-generated%20advertisement%20strongly%20adjusted%20their%20donation%20behavior%20downwards.%0AFurthermore%2C%20people%20are%20generally%20not%20able%20to%20adequately%20differentiate%20between%0Ahuman-generated%20and%20LLM-generated%20ads.%20Our%20work%20has%20important%20implications%20for%0Athe%20design%2C%20development%2C%20integration%2C%20and%20adoption%20of%20multilingual%20LLMs%20as%0Aassistive%20agents%20--%20particularly%20in%20writing%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09532v1&entry.124074799=Read"},
{"title": "Mixed-curvature decision trees and random forests", "author": "Philippe Chlenski and Quentin Chu and Raiyan R. Khan and Kaizhu Du and Antonio Khalil Moretti and Itsik Pe'er", "abstract": "  Decision trees (DTs) and their random forest (RF) extensions are workhorses\nof classification and regression in Euclidean spaces. However, algorithms for\nlearning in non-Euclidean spaces are still limited. We extend DT and RF\nalgorithms to product manifolds: Cartesian products of several hyperbolic,\nhyperspherical, or Euclidean components. Such manifolds handle heterogeneous\ncurvature while still factorizing neatly into simpler components, making them\ncompelling embedding spaces for complex datasets. Our novel angular\nreformulation of DTs respects the geometry of the product manifold, yielding\nsplits that are geodesically convex, maximum-margin, and composable. In the\nspecial cases of single-component manifolds, our method simplifies to its\nEuclidean or hyperbolic counterparts, or introduces hyperspherical DT\nalgorithms, depending on the curvature. We benchmark our method on various\nclassification, regression, and link prediction tasks on synthetic data, graph\nembeddings, mixed-curvature variational autoencoder latent spaces, and\nempirical data. Compared to 7 other classifiers, product RFs ranked first on 25\nout of 57 benchmarks, and placed in the top 2 for 46 out of 57. This highlights\nthe value of product RFs as straightforward yet powerful new tools for data\nanalysis in product manifolds. Code for our paper is available at\nhttps://github.com/pchlenski/manify.\n", "link": "http://arxiv.org/abs/2410.13879v2", "date": "2025-02-13", "relevancy": 1.4018, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5067}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4573}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixed-curvature%20decision%20trees%20and%20random%20forests&body=Title%3A%20Mixed-curvature%20decision%20trees%20and%20random%20forests%0AAuthor%3A%20Philippe%20Chlenski%20and%20Quentin%20Chu%20and%20Raiyan%20R.%20Khan%20and%20Kaizhu%20Du%20and%20Antonio%20Khalil%20Moretti%20and%20Itsik%20Pe%27er%0AAbstract%3A%20%20%20Decision%20trees%20%28DTs%29%20and%20their%20random%20forest%20%28RF%29%20extensions%20are%20workhorses%0Aof%20classification%20and%20regression%20in%20Euclidean%20spaces.%20However%2C%20algorithms%20for%0Alearning%20in%20non-Euclidean%20spaces%20are%20still%20limited.%20We%20extend%20DT%20and%20RF%0Aalgorithms%20to%20product%20manifolds%3A%20Cartesian%20products%20of%20several%20hyperbolic%2C%0Ahyperspherical%2C%20or%20Euclidean%20components.%20Such%20manifolds%20handle%20heterogeneous%0Acurvature%20while%20still%20factorizing%20neatly%20into%20simpler%20components%2C%20making%20them%0Acompelling%20embedding%20spaces%20for%20complex%20datasets.%20Our%20novel%20angular%0Areformulation%20of%20DTs%20respects%20the%20geometry%20of%20the%20product%20manifold%2C%20yielding%0Asplits%20that%20are%20geodesically%20convex%2C%20maximum-margin%2C%20and%20composable.%20In%20the%0Aspecial%20cases%20of%20single-component%20manifolds%2C%20our%20method%20simplifies%20to%20its%0AEuclidean%20or%20hyperbolic%20counterparts%2C%20or%20introduces%20hyperspherical%20DT%0Aalgorithms%2C%20depending%20on%20the%20curvature.%20We%20benchmark%20our%20method%20on%20various%0Aclassification%2C%20regression%2C%20and%20link%20prediction%20tasks%20on%20synthetic%20data%2C%20graph%0Aembeddings%2C%20mixed-curvature%20variational%20autoencoder%20latent%20spaces%2C%20and%0Aempirical%20data.%20Compared%20to%207%20other%20classifiers%2C%20product%20RFs%20ranked%20first%20on%2025%0Aout%20of%2057%20benchmarks%2C%20and%20placed%20in%20the%20top%202%20for%2046%20out%20of%2057.%20This%20highlights%0Athe%20value%20of%20product%20RFs%20as%20straightforward%20yet%20powerful%20new%20tools%20for%20data%0Aanalysis%20in%20product%20manifolds.%20Code%20for%20our%20paper%20is%20available%20at%0Ahttps%3A//github.com/pchlenski/manify.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixed-curvature%2520decision%2520trees%2520and%2520random%2520forests%26entry.906535625%3DPhilippe%2520Chlenski%2520and%2520Quentin%2520Chu%2520and%2520Raiyan%2520R.%2520Khan%2520and%2520Kaizhu%2520Du%2520and%2520Antonio%2520Khalil%2520Moretti%2520and%2520Itsik%2520Pe%2527er%26entry.1292438233%3D%2520%2520Decision%2520trees%2520%2528DTs%2529%2520and%2520their%2520random%2520forest%2520%2528RF%2529%2520extensions%2520are%2520workhorses%250Aof%2520classification%2520and%2520regression%2520in%2520Euclidean%2520spaces.%2520However%252C%2520algorithms%2520for%250Alearning%2520in%2520non-Euclidean%2520spaces%2520are%2520still%2520limited.%2520We%2520extend%2520DT%2520and%2520RF%250Aalgorithms%2520to%2520product%2520manifolds%253A%2520Cartesian%2520products%2520of%2520several%2520hyperbolic%252C%250Ahyperspherical%252C%2520or%2520Euclidean%2520components.%2520Such%2520manifolds%2520handle%2520heterogeneous%250Acurvature%2520while%2520still%2520factorizing%2520neatly%2520into%2520simpler%2520components%252C%2520making%2520them%250Acompelling%2520embedding%2520spaces%2520for%2520complex%2520datasets.%2520Our%2520novel%2520angular%250Areformulation%2520of%2520DTs%2520respects%2520the%2520geometry%2520of%2520the%2520product%2520manifold%252C%2520yielding%250Asplits%2520that%2520are%2520geodesically%2520convex%252C%2520maximum-margin%252C%2520and%2520composable.%2520In%2520the%250Aspecial%2520cases%2520of%2520single-component%2520manifolds%252C%2520our%2520method%2520simplifies%2520to%2520its%250AEuclidean%2520or%2520hyperbolic%2520counterparts%252C%2520or%2520introduces%2520hyperspherical%2520DT%250Aalgorithms%252C%2520depending%2520on%2520the%2520curvature.%2520We%2520benchmark%2520our%2520method%2520on%2520various%250Aclassification%252C%2520regression%252C%2520and%2520link%2520prediction%2520tasks%2520on%2520synthetic%2520data%252C%2520graph%250Aembeddings%252C%2520mixed-curvature%2520variational%2520autoencoder%2520latent%2520spaces%252C%2520and%250Aempirical%2520data.%2520Compared%2520to%25207%2520other%2520classifiers%252C%2520product%2520RFs%2520ranked%2520first%2520on%252025%250Aout%2520of%252057%2520benchmarks%252C%2520and%2520placed%2520in%2520the%2520top%25202%2520for%252046%2520out%2520of%252057.%2520This%2520highlights%250Athe%2520value%2520of%2520product%2520RFs%2520as%2520straightforward%2520yet%2520powerful%2520new%2520tools%2520for%2520data%250Aanalysis%2520in%2520product%2520manifolds.%2520Code%2520for%2520our%2520paper%2520is%2520available%2520at%250Ahttps%253A//github.com/pchlenski/manify.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed-curvature%20decision%20trees%20and%20random%20forests&entry.906535625=Philippe%20Chlenski%20and%20Quentin%20Chu%20and%20Raiyan%20R.%20Khan%20and%20Kaizhu%20Du%20and%20Antonio%20Khalil%20Moretti%20and%20Itsik%20Pe%27er&entry.1292438233=%20%20Decision%20trees%20%28DTs%29%20and%20their%20random%20forest%20%28RF%29%20extensions%20are%20workhorses%0Aof%20classification%20and%20regression%20in%20Euclidean%20spaces.%20However%2C%20algorithms%20for%0Alearning%20in%20non-Euclidean%20spaces%20are%20still%20limited.%20We%20extend%20DT%20and%20RF%0Aalgorithms%20to%20product%20manifolds%3A%20Cartesian%20products%20of%20several%20hyperbolic%2C%0Ahyperspherical%2C%20or%20Euclidean%20components.%20Such%20manifolds%20handle%20heterogeneous%0Acurvature%20while%20still%20factorizing%20neatly%20into%20simpler%20components%2C%20making%20them%0Acompelling%20embedding%20spaces%20for%20complex%20datasets.%20Our%20novel%20angular%0Areformulation%20of%20DTs%20respects%20the%20geometry%20of%20the%20product%20manifold%2C%20yielding%0Asplits%20that%20are%20geodesically%20convex%2C%20maximum-margin%2C%20and%20composable.%20In%20the%0Aspecial%20cases%20of%20single-component%20manifolds%2C%20our%20method%20simplifies%20to%20its%0AEuclidean%20or%20hyperbolic%20counterparts%2C%20or%20introduces%20hyperspherical%20DT%0Aalgorithms%2C%20depending%20on%20the%20curvature.%20We%20benchmark%20our%20method%20on%20various%0Aclassification%2C%20regression%2C%20and%20link%20prediction%20tasks%20on%20synthetic%20data%2C%20graph%0Aembeddings%2C%20mixed-curvature%20variational%20autoencoder%20latent%20spaces%2C%20and%0Aempirical%20data.%20Compared%20to%207%20other%20classifiers%2C%20product%20RFs%20ranked%20first%20on%2025%0Aout%20of%2057%20benchmarks%2C%20and%20placed%20in%20the%20top%202%20for%2046%20out%20of%2057.%20This%20highlights%0Athe%20value%20of%20product%20RFs%20as%20straightforward%20yet%20powerful%20new%20tools%20for%20data%0Aanalysis%20in%20product%20manifolds.%20Code%20for%20our%20paper%20is%20available%20at%0Ahttps%3A//github.com/pchlenski/manify.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13879v2&entry.124074799=Read"},
{"title": "Proxy-informed Bayesian transfer learning with unknown sources", "author": "Sabina J. Sloman and Julien Martinelli and Samuel Kaski", "abstract": "  Generalization outside the scope of one's training data requires leveraging\nprior knowledge about the effects that transfer, and the effects that don't,\nbetween different data sources. Transfer learning is a framework for specifying\nand refining this knowledge about sets of source (training) and target\n(prediction) data. A challenging open problem is addressing the empirical\nphenomenon of negative transfer, whereby the transfer learner performs worse on\nthe target data after taking the source data into account than before. We first\nintroduce a Bayesian perspective on negative transfer, and then a method to\naddress it. The key insight from our formulation is that negative transfer can\nstem from misspecified prior information about non-transferable causes of the\nsource data. Our proposed method, proxy-informed robust method for\nprobabilistic transfer learning (PROMPT), does not require prior knowledge of\nthe source data (the data sources may be \"unknown\"). PROMPT is thus applicable\nwhen differences between tasks are unobserved, such as in the presence of\nlatent confounders. Moreover, the learner need not have access to observations\nin the target task (cannot \"fine-tune\"), and instead makes use of proxy\n(indirect) information. Our theoretical results show that the threat of\nnegative transfer does not depend on the informativeness of the proxy\ninformation, highlighting the usefulness of PROMPT in cases where only noisy\nindirect information, such as human feedback, is available.\n", "link": "http://arxiv.org/abs/2411.03263v2", "date": "2025-02-13", "relevancy": 1.4218, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5202}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4644}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proxy-informed%20Bayesian%20transfer%20learning%20with%20unknown%20sources&body=Title%3A%20Proxy-informed%20Bayesian%20transfer%20learning%20with%20unknown%20sources%0AAuthor%3A%20Sabina%20J.%20Sloman%20and%20Julien%20Martinelli%20and%20Samuel%20Kaski%0AAbstract%3A%20%20%20Generalization%20outside%20the%20scope%20of%20one%27s%20training%20data%20requires%20leveraging%0Aprior%20knowledge%20about%20the%20effects%20that%20transfer%2C%20and%20the%20effects%20that%20don%27t%2C%0Abetween%20different%20data%20sources.%20Transfer%20learning%20is%20a%20framework%20for%20specifying%0Aand%20refining%20this%20knowledge%20about%20sets%20of%20source%20%28training%29%20and%20target%0A%28prediction%29%20data.%20A%20challenging%20open%20problem%20is%20addressing%20the%20empirical%0Aphenomenon%20of%20negative%20transfer%2C%20whereby%20the%20transfer%20learner%20performs%20worse%20on%0Athe%20target%20data%20after%20taking%20the%20source%20data%20into%20account%20than%20before.%20We%20first%0Aintroduce%20a%20Bayesian%20perspective%20on%20negative%20transfer%2C%20and%20then%20a%20method%20to%0Aaddress%20it.%20The%20key%20insight%20from%20our%20formulation%20is%20that%20negative%20transfer%20can%0Astem%20from%20misspecified%20prior%20information%20about%20non-transferable%20causes%20of%20the%0Asource%20data.%20Our%20proposed%20method%2C%20proxy-informed%20robust%20method%20for%0Aprobabilistic%20transfer%20learning%20%28PROMPT%29%2C%20does%20not%20require%20prior%20knowledge%20of%0Athe%20source%20data%20%28the%20data%20sources%20may%20be%20%22unknown%22%29.%20PROMPT%20is%20thus%20applicable%0Awhen%20differences%20between%20tasks%20are%20unobserved%2C%20such%20as%20in%20the%20presence%20of%0Alatent%20confounders.%20Moreover%2C%20the%20learner%20need%20not%20have%20access%20to%20observations%0Ain%20the%20target%20task%20%28cannot%20%22fine-tune%22%29%2C%20and%20instead%20makes%20use%20of%20proxy%0A%28indirect%29%20information.%20Our%20theoretical%20results%20show%20that%20the%20threat%20of%0Anegative%20transfer%20does%20not%20depend%20on%20the%20informativeness%20of%20the%20proxy%0Ainformation%2C%20highlighting%20the%20usefulness%20of%20PROMPT%20in%20cases%20where%20only%20noisy%0Aindirect%20information%2C%20such%20as%20human%20feedback%2C%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProxy-informed%2520Bayesian%2520transfer%2520learning%2520with%2520unknown%2520sources%26entry.906535625%3DSabina%2520J.%2520Sloman%2520and%2520Julien%2520Martinelli%2520and%2520Samuel%2520Kaski%26entry.1292438233%3D%2520%2520Generalization%2520outside%2520the%2520scope%2520of%2520one%2527s%2520training%2520data%2520requires%2520leveraging%250Aprior%2520knowledge%2520about%2520the%2520effects%2520that%2520transfer%252C%2520and%2520the%2520effects%2520that%2520don%2527t%252C%250Abetween%2520different%2520data%2520sources.%2520Transfer%2520learning%2520is%2520a%2520framework%2520for%2520specifying%250Aand%2520refining%2520this%2520knowledge%2520about%2520sets%2520of%2520source%2520%2528training%2529%2520and%2520target%250A%2528prediction%2529%2520data.%2520A%2520challenging%2520open%2520problem%2520is%2520addressing%2520the%2520empirical%250Aphenomenon%2520of%2520negative%2520transfer%252C%2520whereby%2520the%2520transfer%2520learner%2520performs%2520worse%2520on%250Athe%2520target%2520data%2520after%2520taking%2520the%2520source%2520data%2520into%2520account%2520than%2520before.%2520We%2520first%250Aintroduce%2520a%2520Bayesian%2520perspective%2520on%2520negative%2520transfer%252C%2520and%2520then%2520a%2520method%2520to%250Aaddress%2520it.%2520The%2520key%2520insight%2520from%2520our%2520formulation%2520is%2520that%2520negative%2520transfer%2520can%250Astem%2520from%2520misspecified%2520prior%2520information%2520about%2520non-transferable%2520causes%2520of%2520the%250Asource%2520data.%2520Our%2520proposed%2520method%252C%2520proxy-informed%2520robust%2520method%2520for%250Aprobabilistic%2520transfer%2520learning%2520%2528PROMPT%2529%252C%2520does%2520not%2520require%2520prior%2520knowledge%2520of%250Athe%2520source%2520data%2520%2528the%2520data%2520sources%2520may%2520be%2520%2522unknown%2522%2529.%2520PROMPT%2520is%2520thus%2520applicable%250Awhen%2520differences%2520between%2520tasks%2520are%2520unobserved%252C%2520such%2520as%2520in%2520the%2520presence%2520of%250Alatent%2520confounders.%2520Moreover%252C%2520the%2520learner%2520need%2520not%2520have%2520access%2520to%2520observations%250Ain%2520the%2520target%2520task%2520%2528cannot%2520%2522fine-tune%2522%2529%252C%2520and%2520instead%2520makes%2520use%2520of%2520proxy%250A%2528indirect%2529%2520information.%2520Our%2520theoretical%2520results%2520show%2520that%2520the%2520threat%2520of%250Anegative%2520transfer%2520does%2520not%2520depend%2520on%2520the%2520informativeness%2520of%2520the%2520proxy%250Ainformation%252C%2520highlighting%2520the%2520usefulness%2520of%2520PROMPT%2520in%2520cases%2520where%2520only%2520noisy%250Aindirect%2520information%252C%2520such%2520as%2520human%2520feedback%252C%2520is%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proxy-informed%20Bayesian%20transfer%20learning%20with%20unknown%20sources&entry.906535625=Sabina%20J.%20Sloman%20and%20Julien%20Martinelli%20and%20Samuel%20Kaski&entry.1292438233=%20%20Generalization%20outside%20the%20scope%20of%20one%27s%20training%20data%20requires%20leveraging%0Aprior%20knowledge%20about%20the%20effects%20that%20transfer%2C%20and%20the%20effects%20that%20don%27t%2C%0Abetween%20different%20data%20sources.%20Transfer%20learning%20is%20a%20framework%20for%20specifying%0Aand%20refining%20this%20knowledge%20about%20sets%20of%20source%20%28training%29%20and%20target%0A%28prediction%29%20data.%20A%20challenging%20open%20problem%20is%20addressing%20the%20empirical%0Aphenomenon%20of%20negative%20transfer%2C%20whereby%20the%20transfer%20learner%20performs%20worse%20on%0Athe%20target%20data%20after%20taking%20the%20source%20data%20into%20account%20than%20before.%20We%20first%0Aintroduce%20a%20Bayesian%20perspective%20on%20negative%20transfer%2C%20and%20then%20a%20method%20to%0Aaddress%20it.%20The%20key%20insight%20from%20our%20formulation%20is%20that%20negative%20transfer%20can%0Astem%20from%20misspecified%20prior%20information%20about%20non-transferable%20causes%20of%20the%0Asource%20data.%20Our%20proposed%20method%2C%20proxy-informed%20robust%20method%20for%0Aprobabilistic%20transfer%20learning%20%28PROMPT%29%2C%20does%20not%20require%20prior%20knowledge%20of%0Athe%20source%20data%20%28the%20data%20sources%20may%20be%20%22unknown%22%29.%20PROMPT%20is%20thus%20applicable%0Awhen%20differences%20between%20tasks%20are%20unobserved%2C%20such%20as%20in%20the%20presence%20of%0Alatent%20confounders.%20Moreover%2C%20the%20learner%20need%20not%20have%20access%20to%20observations%0Ain%20the%20target%20task%20%28cannot%20%22fine-tune%22%29%2C%20and%20instead%20makes%20use%20of%20proxy%0A%28indirect%29%20information.%20Our%20theoretical%20results%20show%20that%20the%20threat%20of%0Anegative%20transfer%20does%20not%20depend%20on%20the%20informativeness%20of%20the%20proxy%0Ainformation%2C%20highlighting%20the%20usefulness%20of%20PROMPT%20in%20cases%20where%20only%20noisy%0Aindirect%20information%2C%20such%20as%20human%20feedback%2C%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03263v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


